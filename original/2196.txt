In this paper, we propose a novel adaptive deep disturbance-disentangled learning (ADDL) method for effective facial expression recognition (FER). ADDL involves a two-stage learning procedure. First, a disturbance feature extraction model is trained to identify multiple disturbing factors on a large-scale face database involving disturbance label information. Second, an adaptive disturbance-disentangled model, which contains a global shared subnetwork and two task-specific subnetworks, is designed and learned to explicitly disentangle disturbing factors from facial expression images. In particular, the expression subnetwork leverages a multi-level attention mechanism to extract expression-specific features, while the disturbance subnetwork embraces a new adaptive disturbance feature learning module to extract disturbance-specific features based on adversarial transfer learning. Moreover, a mutual information neural estimator is adopted to minimize the correlation between expression-specific and disturbance-specific features. Extensive experimental results on both in-the-lab FER databases (including CK+, MMI, and Oulu-CASIA) and in-the-wild FER databases (including RAF-DB, SFEW, Aff-Wild2, and AffectNet) show that our proposed method consistently outperforms several state-of-the-art FER methods. This clearly demonstrates the great potential of disturbance disentanglement for FER. Our code is available at https://github.com/delian11/ADDL.

Introduction
Facial expression conveys nonverbal cues and plays a fundamental role in understanding emotions in human interaction and communication. During the past few decades, facial expression recognition (FER) has attracted increasing attention in computer vision due to its variety of applications in entertainment, sociable robotics, data-driven animation, and so on (Zhang et al., 2018a, b). Recently, with the considerable development of deep learning, FER has made substantial progress (Chang et al., 2019; Chen et al., 2020; Dapogny et al., 2018; Kollias et al., 2020a; Li & Deng, 2019; Li et al., 2017; Meng et al., 2017; Yang et al., 2018a; Yan et al., 2020; Zhang et al., 2018c).

Fig. 1
figure 1
Some facial expressions of a in-the-lab images [the images are from the CK+ database (Lucey et al., 2010)] and b in-the-wild images [the images are from the SFEW database (Dhall et al., 2011)]

Full size image
Despite great progress, FER is still a challenging task. On the one hand, facial expression images exhibit large inter-class similarities and intra-class variances caused by the existence of multiple disturbing factors. For example, in each row of Fig. 1a, the images of different expressions are visually similar due to the same illumination and identity. Meanwhile, in the two rows of Fig. 1b, the images of the same expression show great differences because of changes in gender, age, race, identity, illumination, and pose. Clearly, these disturbing factors adversely affect the extraction of expression-specific features. On the other hand, different FER databases may involve different types of disturbing factors. For instance, some in-the-lab FER databases only include disturbances caused by variations in identity, age, and gender but not pose, as shown in Fig. 1a, while some in-the-wild FER databases may suffer from severe identity, illumination, and pose variations, as given in Fig. 1b.

It is of great importance to properly disentangle disturbing factors from facial expression images for FER. A variety of deep learning-based FER methods (Rifai et al., 2012; Liu et al., 2018; Mollahosseini et al. 2016; Hu et al., 2017; Wang et al., 2020c) have been proposed to implicitly reduce the disturbance for recognizing facial expressions. The training of these methods typically relies on a large amount of labeled data to achieve satisfactory performance. However, many FER databases contain only limited labeled training data. As a result, it is not a trivial task to learn robust deep models that can effectively alleviate the influence of various disturbing factors in the case of limited training data.

To date, some disturbance-disentangled-based FER methods (Meng et al., 2017; Zhang et al., 2020b; Chen et al., 2018; Zhang et al., 2018b; Yang et al., 2018b), which explicitly disentangle disturbing factors, have been developed. Note that many FER databases only provide labels of facial expression and identity (or pose) since manually labeling various disturbing factors is time-consuming and labor-intensive. As a consequence, these methods are only able to disentangle one or two disturbing factors for FER, leading to suboptimal performance. Moreover, they may not work well when the labels of disturbing factors are not available in the FER databases.

Fortunately, some large-scale face databases provide a large number of facial images together with the label information for different disturbing factors. For example, Multi-PIE (Gross et al., 2010) offers labels of identity, pose, and illumination. RAF-DB (Li et al., 2017) gives labels of gender, race, and age. Therefore, how to effectively exploit these large-scale disturbance-labeled face databases to perform transfer learning for classifying expressions in disturbance-unlabeled FER databases is a significantly rewarding research problem.

To address the above problems, we propose a novel adaptive deep disturbance-disentangled learning (ADDL) method for FER. ADDL adaptively disentangles multiple disturbing factors from facial expression images and effectively extracts expression-specific features, building its success by borrowing the strengths from both multi-task learning and adversarial transfer learning.

The ADDL method involves a two-stage learning procedure: (1) training a disturbance feature extraction model (DFEM) and (2) training an adaptive disturbance-disentangled model (ADDM). Specifically, the DFEM is first trained to identify multiple disturbing factors on a large-scale face database. Second, based on the trained DFEM, the ADDM is learned to remove the disturbance and extract discriminative features for expression recognition.

In summary, the main contributions of our work are as follows:

We propose a novel ADDL method that contains two crucial components (i.e., the DFEM and ADDM) for effective FER. In particular, the knowledge in the DFEM trained on the large-scale face database is effectively transferred to the ADDM to classify expressions in the disturbance-unlabeled FER database. Therefore, the ADDL method is capable of simultaneously disentangling multiple disturbing factors and capturing expression-related information.

We elaborately design two task-specific subnetworks in the ADDM. For the expression subnetwork, we employ a multi-level attention mechanism to extract expression-specific features. For the disturbance subnetwork, we adopt adversarial transfer learning to learn disturbance-specific features. The two subnetworks are jointly trained to exploit both spatial-aware and semantic-aware information.

We extensively evaluate the proposed ADDL on both in-the-lab and in-the-wild FER databases. Experimental results from these databases show that our proposed method performs favorably against several state-of-the-art FER methods.

This paper is a substantial extension of our previous conference work in Ruan et al. (2020). The method in our previous work has two main limitations. First, it cannot adaptively choose the disturbing factors when trained on an FER database. Second, disturbance disentanglement is not explicitly performed. This paper alleviates these limitations in two aspects: (1) an adaptive disturbance feature learning module (ADFL) is designed to learn the importance weights corresponding to different disturbing factors and then perform adversarial transfer learning; (2) a mutual information neural estimator (MINE) is leveraged to minimize the correlation between expression-specific and disturbance-specific features.

To summarize, we have added the following new significant contributions:

We design the ADFL to greatly facilitate the extraction of disturbance-specific features by considering different influences of disturbing factors in the FER training database. In this way, the characteristics of the FER database can be taken into account to choose the disturbing factors, and thus adaptive disturbance-specific features are extracted.

We adopt the MINE during the training of the ADDM. Thus, we are able to effectively enhance the explicit disentanglement between expression-specific and disturbance-specific features for better FER.

Based on the above two extensions, the proposed ADDL method consistently achieves better recognition accuracy than our previous method on both in-the-lab and in-the-wild FER databases.

The remainder of this paper is organized as follows. Section 2 briefly reviews the related work. Section 3 introduces the details of our proposed ADDL method. Section 4 provides experimental results on three popular in-the-lab FER databases (CK+, MMI, and Oulu-CASIA) and four challenging in-the-wild FER databases (RAF-DB, SFEW, AffWild-2, and AffectNet). Finally, Sect. 5 presents the conclusion and future work.

Related Work
In this section, we review state-of-the-art work of convolutional neural network (CNN)-based FER methods in Sect. 2.1, disturbance-disentangled-based FER methods in Sect. 2.2, action unit recognition in Sect. 2.3, and attention mechanisms in Sect. 2.4, which are closely related to our proposed method.

CNN-Based FER Methods
Currently, CNN-based FER methods (Li & Deng, 2020) have achieved promising performance due to the powerful capability of CNNs to capture high-level semantic information. For example, Yu and Zhang (2015) develop an ensemble of CNNs, which shows impressive results in the EmotiW challenge. Mollahosseini et al. (2016) introduce a network consisting of two convolutional layers and four inception layers (Szegedy et al., 2015) to predict facial expressions. Hu et al. (2017) propose a supervised scoring ensemble (SSE) method, where supervision signals are used not only for deep layers but also for intermediate and shallow layers. Liu et al. (2018) design a multi-channel pose-aware CNN (MPCNN) to aggregate multi-scale features, which are fed into a pose-aware recognition network for pose estimation and pose conditioned expression recognition.

These CNN-based methods implicitly alleviate the influence of disturbances involved in facial expression images. Generally, they rely heavily on a large number of labeled data to learn effective feature representations. However, many FER databases do not provide sufficient training data containing diverse variations for different disturbing factors. As a result, the trained CNN models may not be sufficiently robust to handle various disturbing factors.

Disturbance-Disentangled-Based FER Methods
Some methods have been proposed to explicitly perform disturbance disentanglement for FER. For example, Meng et al. (2017) introduce an identity-aware CNN (IACNN) method to alleviate the variations caused by facial identity, where an identity-sensitive contrastive loss is developed to learn identity-related information. Wang et al. (2019) propose an adversarial feature learning method to disentangle the disturbance caused by pose and identity.

Recently, generative adversarial networks (GANs) have been widely used in pose-robust FER (Zhang et al., 2018b; Wang et al., 2020d) and identity-robust FER (Chen et al., 2018; Yang et al., 2018b). Zhang et al. (2018b, 2020a, b) develop a GAN-based pose-invariant method for facial image synthesis and expression recognition by exploiting the relationship between different poses and expressions. Furthermore, the disturbance caused by facial identity is explicitly reduced by adversarial learning. Yang et al. (2018b) propose an identity-adaptive method to learn an identity subspace, which can generate different expressions while preserving identity-related information for each individual.

The above methods require the labels of disturbing factors in the FER training databases. Unfortunately, many FER databases only provide labels of facial expressions and some facial attributes (such as identity and pose) but lack the label information for other disturbing factors. Therefore, these methods are only able to handle one or two disturbing factors. Moreover, they may fail on disturbance-unlabeled FER databases.

Facial expression images are often intertwined with multiple disturbing factors (such as identity, pose, age, gender, and illumination). Hence, it is desirable to simultaneously alleviate the influence of these disturbing factors. In this paper, we capitalize on the disturbance label information available in the large-scale face database to perform adversarial transfer learning for expression recognition on the disturbance-unlabeled FER database. This not only successfully addresses the problems of the lack of disturbance labels and limited training data in the FER database, but also enables the proposed method to effectively disentangle different disturbing factors from facial expression images.

Action Unit Recognition
Ekman and Friesen (1976) develop the facial action coding system (FACS), which encodes atomic nonoverlapping facial muscles called action units (AUs). Based on the FACS, facial expressions can be viewed as combinations of certain AUs.

Some methods have been proposed to learn task-specific representations for AU recognition. For example, Zhang et al. (2018d) design an adversarial training framework (ATF), which is trained by minimizing the AU loss and maximizing the identity loss. In this way, identity-invariant features are extracted for AU detection. Li et al. (2019) propose a twin-cycle autoencoder (TCAE) for AU detection in a self-supervised manner. They factorize the movements into AU-related and pose-related displacements based on a pair of images. Therefore, facial action-related movements can be disentangled from head motion-related movements, which is beneficial for learning discriminative AU-related features. Sankaran et al. (2020) implicitly capture the correlations between two modalities by using an encoder-decoder framework to learn a unified representation for cross-modality AU recognition.

The above methods perform disentanglement based on multi-task learning CNN or an encoder-decoder structure. Nevertheless, they take one or two disturbing factors into account and do not fully consider the explicit disentanglement between the AU-related movements and disturbing factors.

Attention Mechanisms
In recent years, attention mechanism-based CNN methods have been developed in a variety of tasks, such as fine-grained image recognition (Fu et al., 2017; Hu et al., 2018), image captioning (Xu et al., 2015), person re-identification (Wu et al., 2018), and human pose estimation (Chu et al., 2017). Hu et al., (2018) propose a novel architecture unit termed the squeeze-and-excitation (SE) block, which adaptively recalibrates channel-wise feature responses by modeling interdependencies between channels. Chu et al. (2017) design a multi-context attention mechanism-based network for human pose estimation.

Psychological studies have revealed that salient facial regions (such as the mouth, nose, and eyes) play a critical role in FER (Pantic & Rothkrantz, 2000). Meanwhile, attention mechanisms have shown great capability to select salient features. Therefore, attention mechanisms are beneficial to improve the FER performance. For instance, Xie et al. (2019a) propose a deep attentive multi-path CNN (DAM-CNN) method for FER, where a spatial attention mechanism is adopted to obtain salient regions. Wang et al. (2020c) propose a region attention network (RAN) to locate salient facial regions for occlusion-invariant and pose-invariant FER. In general, these methods leverage high-level semantic features of CNNs for FER.

Both high-level and low-level features of CNNs are advantageous for performing FER. Low-level features capture the spatial-aware information of facial images, which can be used to determine the boundaries of salient regions. High-level features encode the semantic-aware information of facial images, which is desirable to locate salient regions (Zhao & Wu, 2019). In this paper, unlike previous methods, we employ a multi-level attention mechanism, which aggregates the attentive features from different layers of the network. This mechanism effectively exploits both spatial-aware and semantic-aware information to extract discriminative features for identifying facial expressions. Moreover, we leverage a self-attention layer to learn the importance weights corresponding to different disturbing factors, enabling the extraction of adaptive disturbance-specific features. Therefore, we can accommodate the different influences of multiple disturbing factors in the FER database.

Proposed Method
In this section, we introduce our proposed ADDL method in detail. First, an overview of the ADDL method is given in Sect. 3.1. Then, the key components (the DFEM and ADDM) of ADDL are described in Sects. 3.2 and 3.3, respectively. Finally, some discussions about ADDL are presented in Sect. 3.4.

Fig. 2
figure 2
The network architecture of our proposed ADDL method. The training phase of ADDL involves a two-stage learning procedure. a Training a DFEM consisting of shared layers and task-specific layers. The DFEM predicts various disturbing factors. b Training an ADDM consisting of a global shared subnetwork (ğ‘†ğ‘”), an expression subnetwork (ğ‘†ğ‘’), and a disturbance subnetwork (ğ‘†ğ‘‘). The ADDM extracts expression-specific features by explicitly disentangling the disturbance

Full size image
Overview
The training phase of the ADDL method involves a two-stage learning procedure: (1) training a DFEM to predict various disturbing factors, and (2) training an ADDM, which adapts to the characteristics of each FER database, to extract expression-specific features by explicitly disentangling multiple disturbing factors from facial expression images. The network architecture of our proposed ADDL method is illustrated in Fig. 2.

Specifically, in the first stage, a DFEM is trained to simultaneously identify various disturbing factors on the disturbance-labeled face database. In this manner, the DFEM effectively captures the prior disturbance information. In the second stage, based on the trained DFEM, an ADDM, consisting of a global shared subnetwork and two task-specific subnetworks (i.e., an expression subnetwork and a disturbance subnetwork), is learned to classify expressions on the disturbance-unlabeled FER database.

In the ADDM, the expression subnetwork leverages a multi-level attention mechanism to comprehensively extract expression-specific features. Meanwhile, by taking advantage of adversarial transfer learning, the disturbance subnetwork capitalizes on the features extracted from the trained DFEM to effectively learn adaptive disturbance-specific features.

During the testing phase, given a facial image, only the global shared subnetwork and expression subnetwork from the trained ADDM are used to extract features and predict facial expressions.

Disturbance Feature Extraction Model (DFEM)
The DFEM is designed to extract discriminative features that capture the information for identifying multiple disturbing factors by taking advantage of multi-task learning on the disturbance-labeled face database. As shown in Fig. 2a, the network architecture of the DFEM consists of shared layers and task-specific layers.

Specifically, facial images are first fed into several shared layers consisting of a cascade of linear and nonlinear transformations to obtain high-level features. In this paper, we adopt ResNet-18 (He et al., 2016), which is widely used in previous works (Wang et al., 2020b), as shared layers. Then, the task-specific layers use a multi-branch architecture to extract features, where each branch containing two cascaded fully-connected (FC) layers classifies a disturbing factor. Note that the features obtained from the first FC layer of each branch encode the information for predicting a disturbing factor, while those from the second FC layer are the predicted outputs.

Given a disturbance-labeled face database, its training set ğ“ğ‘™ with R images is represented as ğ“ğ‘™={ğ±ğ‘™ğ‘–,ğ²ğ‘–}ğ‘…ğ‘–=1, where ğ±ğ‘™ğ‘– denotes the ith training image and ğ²ğ‘–=[ğ‘¦1ğ‘–,â€¦,ğ‘¦ğ‘€ğ‘–]T is an M-dimensional vector representing the labels of disturbing factors corresponding to ğ±ğ‘™ğ‘–. M denotes the number of disturbing factors. The optimization problem of the DFEM is formulated as

argminğ°ğ‘,{ğ°ğ‘—}ğ‘€ğ‘—=1âˆ‘ğ‘–=1ğ‘…âˆ‘ğ‘—=1ğ‘€îˆ¸ğ‘—ğ¶ğ¸(ğ‘¦ğ‘—ğ‘–,îˆ²ğ‘—(ğ±ğ‘™ğ‘–,ğ°ğ‘,ğ°ğ‘—)),
(1)
where the network parameter ğ°ğ‘ controls feature sharing among all the disturbing factors and the network parameter ğ°ğ‘— controls the update of features for the jth disturbing factor; îˆ²ğ‘—(â‹…,â‹…,â‹…) represents the prediction function for the jth disturbing factor, given the input ğ±ğ‘™ğ‘– and the network parameters ğ°ğ‘ and ğ°ğ‘—; ğ‘¦ğ‘—ğ‘– denotes the label of the ğ‘—th disturbing factor corresponding to ğ±ğ‘™ğ‘–; and îˆ¸ğ‘—ğ¶ğ¸(â‹…,â‹…) represents the cross-entropy (CE) loss between the ground-truth label ğ‘¦ğ‘—ğ‘– and the result estimated by îˆ²ğ‘—. Mathematically, the CE loss is defined as

îˆ¸ğ‘—ğ¶ğ¸=âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘—ğŸ™[ğ‘˜=ğ‘¦ğ‘—ğ‘–]log(îˆ²ğ‘—(ğ±ğ‘™ğ‘–,ğ°ğ‘,ğ°ğ‘—)),
(2)
where log(â‹…) represents the logarithm function; ğ¾ğ‘— indicates the category number of the jth disturbing factor; and ğŸ™[ğ‘˜=ğ‘¦ğ‘—ğ‘–] outputs 1 when ğ‘˜=ğ‘¦ğ‘—ğ‘– and 0 otherwise.

Adaptive Disturbance-Disentangled Model (ADDM)
Based on the DFEM trained on the large-scale face database, the ADDM is learned to model the expression-related information and disturbance-related information on the disturbance-unlabeled FER database. As shown in Fig. 2b, the network architecture of the ADDM consists of a global shared subnetwork, an expression subnetwork, and a disturbance subnetwork.

In the following, we introduce the key components of the ADDM.

Global Shared Subnetwork
The global shared subnetwork (denoted ğ‘†ğ‘”) is designed to extract global shared features of input images. In this paper, we employ ResNet-18 (He et al., 2016) as ğ‘†ğ‘”, where the final FC layer is removed.

Task-Specific Subnetworks
ADDM contains two task-specific subnetworks, i.e., an expression subnetwork (denoted ğ‘†ğ‘’) and a disturbance subnetwork (denoted ğ‘†ğ‘‘). Two subnetworks are jointly trained based on ğ‘†ğ‘”.

Expression Subnetwork ğ‘†ğ‘’ is designed to learn expression-specific features by applying attention blocks to ğ‘†ğ‘”. ğ‘†ğ‘’ consists of a set of attention blocks (see Sect. 3.3.3), which are followed by an average pooling layer and two FC layers. Here, the attention block generates a soft attention mask, which indicates the importance of each position in the feature map from ğ‘†ğ‘”.

Considering that the features from different levels of the network in ğ‘†ğ‘’ are complementary, a multi-level attention mechanism is employed to fully exploit these features. Specifically, we first utilize several max pooling layers to ensure the same sizes of feature maps from different attention blocks (except for the last two blocks) since the sizes of feature maps vary from layer to layer. Then, these resized feature maps are concatenated as

ğšğ‘œğ‘¢ğ‘¡=[ğšÌ‚ ğ‘’1;â€¦;ğšÌ‚ ğ‘’ğ¿âˆ’2;ğšğ‘’ğ¿âˆ’1;ğšğ‘’ğ¿],
(3)
where ğšğ‘’ğ‘— indicates the feature map from the jth attention block in ğ‘†ğ‘’; ğšÌ‚ ğ‘’ğ‘— represents the output feature map of the max pooling layer corresponding to ğšğ‘’ğ‘—; L denotes the number of attention blocks; and ğšğ‘œğ‘¢ğ‘¡ is the final combined feature map.

Note that, as shown in Fig. 2, the max pooling layer is not applied to ğšğ‘’ğ¿âˆ’1 and ğšğ‘’ğ¿ to ensure the same sizes of feature maps for concatenation. In this way, both low-level spatial features and high-level semantic features are aggregated to extract expression-specific features.

Given a disturbance-unlabeled FER database, its training set ğ“ğ‘¢ with N images is represented as ğ“ğ‘¢={ğ±ğ‘¢ğ‘–,ğ‘¦ğ‘–}ğ‘ğ‘–=1, where ğ±ğ‘¢ğ‘– denotes the ith training image and ğ‘¦ğ‘– indicates the expression label corresponding to ğ±ğ‘¢ğ‘–. ğ‘†ğ‘’ optimizes the following problem:

argminğ°ğ‘”,ğ°ğ‘’âˆ‘ğ‘–=1ğ‘îˆ¸ğ¶ğ¸(ğ‘¦ğ‘–,îˆ²ğ‘’(ğ±ğ‘¢ğ‘–,ğ°ğ‘”,ğ°ğ‘’)),
(4)
where ğ°ğ‘” and ğ°ğ‘’ denote the network parameters in ğ‘†ğ‘” and ğ‘†ğ‘’, respectively; îˆ²ğ‘’ denotes the prediction function; and îˆ¸ğ¶ğ¸ indicates the CE loss between the ground-truth expression label ğ‘¦ğ‘– and the predicted result by îˆ²ğ‘’, which is expressed as

îˆ¸ğ¶ğ¸=âˆ’âˆ‘ğ‘˜=1ğ¾ğŸ™[ğ‘˜=ğ‘¦ğ‘–]log(îˆ²ğ‘’(ğ±ğ‘¢ğ‘–,ğ°ğ‘”,ğ°ğ‘’)),
(5)
where K is the number of expression categories.

Disturbance Subnetwork ğ‘†ğ‘‘ is designed to learn disturbance-specific features. To achieve this, a straightforward way is to generate pseudo-labels of disturbing factors in the FER database by applying the trained DFEM and then train ğ‘†ğ‘‘ with these pseudo-labels. However, these pseudo-labels unavoidably involve a large number of noisy labels due to the discrepancy between the source domain (the face database used to train the DFEM) and the target domain (the FER database used to train ğ‘†ğ‘‘). As a result, these noisy labels seriously affect the extraction of disturbance-specific features, thereby reducing the final FER performance.

In this paper, we take advantage of adversarial transfer learning to effectively improve the performance of the model in the unlabeled target domain, given the labeled source domain. Mathematically, we constrain the distributions of the output feature maps from ğ‘†ğ‘‘ to be as close as those from the trained DFEM. Such a manner alleviates the domain discrepancy and avoids labeling disturbing factors in the target domain.

The network architecture of ğ‘†ğ‘‘ is given in Fig. 2b. It is comprised of a set of attention blocks, which are followed by an average pooling layer, an FC layer, and an adaptive disturbance feature learning module (ADFL), where the FC layer extracts disturbance-specific features and the ADFL performs adversarial transfer learning between disturbance-specific features and weighted disturbing factor features.

Fig. 3
figure 3
The network architecture of the ADFL

Full size image
The network architecture of the ADFL is shown in Fig. 3. The ADFL consists of a self-attention (SA) layer (including an FC layer and a sigmoid layer), a feature fusion layer, and a discriminator.

We suppose that the extracted disturbance-specific feature is denoted ğŸğ‘‘, given a facial image from the disturbance-unlabeled FER database. First, the SA layer outputs the importance weights (represented as [ğ›¼1,â‹¯,ğ›¼ğ‘€]T) corresponding to M disturbing factors. These importance weights reflect the different influences of disturbing factors in the FER training database. Meanwhile, we also obtain a set of disturbing factor features extracted from the first FC layers of task-specific layers in the trained DFEM, denoted ğ“ğ‘={ğŸğ‘ğ‘—}ğ‘€ğ‘—=1. Here, ğŸğ‘ğ‘— represents the jth disturbing factor feature.

Then, the feature fusion layer combines these disturbing factor features according to their corresponding importance weights, which can be expressed as

ğŸğ‘š=âˆ‘ğ‘—=1ğ‘€ğ›¼ğ‘—ğŸğ‘ğ‘—,
(6)
where ğŸğ‘š represents the weighted disturbing factor feature.

Finally, a discriminator D (consisting of four FC layers and a leaky ReLU function) is introduced to play an adversarial game with a feature extractor F. Here, the feature extractor F refers to the layers used to extract ğŸğ‘‘ in ğ‘†ğ‘‘. F tries to minimize the divergence of the feature distributions between ğŸğ‘‘ and ğŸğ‘š, while D aims to distinguish ğŸğ‘‘ from ğŸğ‘š. The objective of adversarial training is formulated as

minğ·maxğ¹îˆ¸ğ´ğ·(ğ¹,ğ·),
(7)
where the adversarial loss îˆ¸ğ´ğ· is defined as

îˆ¸ğ´ğ·=âˆ’ğ”¼[log(ğ·(ğŸğ‘š))]âˆ’ğ”¼[log(1âˆ’ğ·(ğŸğ‘‘))].
(8)
To facilitate knowledge transfer from the trained DFEM to ğ‘†ğ‘‘, it is natural that the distributions of both the final output features and the intermediate attention maps from ğ‘†ğ‘‘ are close to those from the trained DFEM. Therefore, we also apply attention transfer (Zhang et al., 2018a), which has been proven to be effective in bridging the gap between the source domain and the target domain, by transferring attention knowledge. The attention transfer loss is expressed as

îˆ¸ğ´ğ‘‡=âˆ‘ğ‘—=1ğ¿||ğªğ‘‘ğ‘—||ğªğ‘‘ğ‘—||2âˆ’ğªğ‘ğ‘—||ğªğ‘ğ‘—||2||2,
(9)
where ğªğ‘‘ğ‘— and ğªğ‘ğ‘— are the jth attention maps from ğ‘†ğ‘‘ and the trained DFEM in the vectorized forms, respectively.

As mentioned previously, each FER database involves certain types of disturbing factors. In the ADFL, the SA layer estimates the importance weights corresponding to different disturbing factors based on ğŸğ‘‘, while the feature fusion layer outputs ğŸğ‘š based on these importance weights. Therefore, ğŸğ‘š incorporates the prior disturbance information that considers the characteristics of the FER database. For instance, the prior pose information does not greatly contribute to the extraction of ğŸğ‘š for the in-the-lab FER database (since the importance weight corresponding to pose is low in this case), while it is encoded in ğŸğ‘š for the in-the-wild FER database. By performing adversarial transfer learning, the distributions of ğŸğ‘š and ğŸğ‘‘ are as similar as possible. Hence, we are able to effectively extract adaptive disturbance-specific features, which approximate a linear combination of disturbing factor features from the trained DFEM, by exploiting the knowledge of the FER database.

In the SA layer, the importance weight reflects the influence of a disturbing factor. To explain this, we take a simple example for illustration. Assume that we have an FER training database only involving identity variations and that ğ›¼1 corresponds to the importance weight of the identity. In other words, all the images in this FER database are captured with the same gender, age, race, illumination, and pose. Thus, the first disturbing factor features (i.e., ğŸğ‘1 corresponding to identity) of different images in the FER database significantly vary, while the others (i.e., {ğŸğ‘2,â€¦,ğŸğ‘ğ‘€}) show small variations. By minimizing the differences between ğŸğ‘‘ and ğŸğ‘š, ğ›¼1 and ğ›¼ğ‘— (ğ‘—âˆˆ{2,â‹¯,ğ‘€})) are assigned large and small values, respectively. Accordingly, the joint loss function [see Eq. (14)] can be gradually optimized. Otherwise, (i.e., ğ›¼1 is small while ğ›¼ğ‘— (ğ‘—âˆˆ{2,â€¦,ğ‘€})) is large), the weighted disturbing factor features [see Eq. (6)] are similar for all the images. In such a case, the disturbance subnetwork fails to extract effective information, and thus disturbance disentanglement cannot be properly performed. Therefore, the value of ğ›¼1 reflects the influence of identity.

Note that the multi-level attention mechanism is not used in ğ‘†ğ‘‘. This is because the salient regions for identifying multiple disturbing factors are different. For example, illumination estimation mainly relies on the global facial region, while pose classification focuses on local regions around salient facial landmarks. In other words, it is not appropriate to directly concatenate low-level spatial features and high-level semantic features to extract disturbance-specific features in ğ‘†ğ‘‘.

Attention Block
Inspired by Liu et al. (2019), we develop an attention block for both ğ‘†ğ‘‘ and ğ‘†ğ‘’. The network architecture of the attention block is given in Fig. 4.

Fig. 4
figure 4
The network architecture of the attention block

Full size image
The first attention block in ğ‘†ğ‘’ or ğ‘†ğ‘‘ takes the feature ğ®1 from the first convolution block in ğ‘†ğ‘” as the input. For the subsequent attention block at the jth layer, the element-wise weighted addition between the global feature ğ®ğ‘— from ğ‘†ğ‘” and the task-specific feature ğšğ‘¡ğ‘—âˆ’1 (ğ‘¡âˆˆ{ğ‘’,ğ‘‘}) from the previous layer in ğ‘†ğ‘¡ (ğ‘¡âˆˆ{ğ‘’,ğ‘‘}) is taken as the input, as shown in Fig. 4. Then, the attention mask ğ¦ğ‘¡ğ‘— (ğ‘¡âˆˆ{ğ‘’,ğ‘‘}) generated from the jth layer in ğ‘†ğ‘¡ (ğ‘¡âˆˆ{ğ‘’,ğ‘‘}) is expressed as

ğ¦ğ‘¡ğ‘—={g(ğ®ğ‘—),                         ğ‘—=1,g(ğ›¿1ğ®ğ‘—+ğ›¿2ğšğ‘¡ğ‘—âˆ’1),     ğ‘—â‰¥2,
(10)
where ğ›¿1 and ğ›¿2 are the learnable parameters that, respectively, determine the importance of the global feature ğ®ğ‘— and the task-specific feature ğšğ‘¡ğ‘—âˆ’1; g(â‹…) denotes the aggregation of a batch normalization (BN) layer, a parametric ReLU (PReLU) layer, a 1Ã—1 convolutional layer, another batch normalization layer, and a sigmoid layer that constrains the output within the range of (0, 1).

The output feature map ğšğ‘¡ğ‘— of the jth attention block for ğ‘†ğ‘¡ (ğ‘¡âˆˆ{ğ‘’,ğ‘‘}) is given as

ğšğ‘¡ğ‘—=h(ğ¦ğ‘¡ğ‘—âŠ™ğ®ğ‘—),
(11)
where â€˜âŠ™â€™ denotes the element-wise multiplication; h(â‹…) denotes a convolutional layer with a 3Ã—3 kernel that matches the channels between the attention mask from the (ğ‘—âˆ’1)th layer in ğ‘†ğ‘¡ (ğ‘¡âˆˆ{ğ‘’,ğ‘‘}) and the global shared feature in the jth layer in ğ‘†ğ‘”, followed by a BN layer, a PReLU layer, and a max pooling layer to match the sizes of the feature maps between the above two features.

It is worth noting that our attention block outputs a 3D attention mask, where each attention map in the mask captures salient regions for a feature channel in ğ‘†ğ‘”. This is different from the traditional attention block (Xie et al., 2019a), which applies the same 2D mask to each feature channel. Therefore, the attention block used in our paper takes into account the differences between feature maps and thus can generate more accurate attention weights.

Mutual Information Neural Estimator (MINE)
To perform explicit disentanglement between the disturbance-specific feature ğŸğ‘‘ and the expression-specific feature ğŸğ‘’, the correlation between the two features should be minimized. Generally, the Kullbackâ€“Leibler (Kâ€“L) divergence îˆ°ğ¾ğ¿(â„™ğ¹ğ‘‘||â„™ğ¹ğ‘’) can be used to minimize the discrepancy between two feature distributions. Here, ğ¹ğ‘‘ and ğ¹ğ‘’ denote the random variables of ğŸğ‘‘ and ğŸğ‘’, respectively. â„™ğ¹ğ‘‘ and â„™ğ¹ğ‘’ represent the marginal probability distributions of ğ¹ğ‘‘ and ğ¹ğ‘’, respectively. However, we cannot guarantee that the features with dissimilar distributions are uncorrelated.

Inspired by Belghazi et al. (2018), we leverage mutual information to measure the correlation between ğŸğ‘‘ and ğŸğ‘’ (note that if two variables are independent of each other, their mutual information is zero). Specifically, we employ a mutual information neural estimator (MINE) (Belghazi et al., 2018) to estimate the mutual information between ğŸğ‘‘ and ğŸğ‘’, leading to explicit disentanglement. Based on the Kâ€“L divergence and the Donskerâ€“Varadhan representation (Donsker & Varadhan, 1983), the mutual information can be estimated by the MINE as

ğ¼(ğ¹ğ‘‘;ğ¹ğ‘’)=îˆ°ğ¾ğ¿(â„™ğ¹ğ‘‘ğ¹ğ‘’||â„™ğ¹ğ‘‘âŠ—â„™ğ¹ğ‘’)â‰¥ğ”¼â„™ğ¹ğ‘‘ğ¹ğ‘’[ğ‘‡ğœƒ(ğŸğ‘‘,ğŸğ‘’)]âˆ’log(ğ”¼â„™ğ¹ğ‘‘âŠ—â„™ğ¹ğ‘’[ğ‘’ğ‘‡ğœƒ(ğŸğ‘‘,ğŸğ‘’)]),
(12)
where â€˜âŠ—â€™ is the product function; â„™ğ¹ğ‘‘ğ¹ğ‘’ represents the joint probability distribution of (ğ¹ğ‘‘, ğ¹ğ‘’); and ğ‘‡ğœƒ is a neural network with parameters ğœƒ (the detailed architecture of ğ‘‡ğœƒ is described in Table 1a).

Given n mini-batch samples {ğŸğ‘‘ğ‘–,ğŸğ‘’ğ‘–}ğ‘›ğ‘–=1 from the joint distribution and n samples {ğŸğ‘’ğ‘–~}ğ‘›ğ‘–=1 from the marginal distribution of ğ¹ğ‘’ (which can be estimated by shuffling the samples from the joint distribution along the batch axis), the mutual information loss îˆ¸ğ‘€ğ¼ is approximated as

îˆ¸ğ‘€ğ¼=ğ¼(ğ¹ğ‘‘;ğ¹ğ‘’)â‰ˆ1ğ‘›âˆ‘ğ‘–=1ğ‘›ğ‘‡ğœƒ(ğŸğ‘‘ğ‘–,ğŸğ‘’ğ‘–)âˆ’log(1ğ‘›âˆ‘ğ‘–=1ğ‘›ğ‘’ğ‘‡ğœƒ(ğŸğ‘‘ğ‘–,ğŸğ‘’ğ‘–~)).
(13)
The correlation between ğŸğ‘‘ and ğŸğ‘’ is minimized by optimizing the mutual information loss îˆ¸ğ‘€ğ¼. Therefore, we are able to disentangle the disturbance in an explicit way.

Joint Loss Function
The joint loss function of the ADDM is defined as

îˆ¸=îˆ¸ğ¶ğ¸+ğœ†1îˆ¸ğ´ğ·+ğœ†2îˆ¸ğ´ğ‘‡+ğœ†3îˆ¸ğ‘€ğ¼,
(14)
where ğœ†1, ğœ†2, and ğœ†3 denote the balanced parameters of the adversarial loss, the attention transfer loss, and the mutual information loss, respectively.

By minimizing the joint loss function, the ADDM is able to extract discriminative expression-specific features for FER.

Table 1 The detailed architecture of the MINE and each subnetwork in the ADDM
Full size table
Discussions
A number of CNN-based FER methods (Mollahosseini et al. 2016; Yu & Zhang, 2015) suffer from the problem that the final expression features contain the disturbance because of limited training data. Some disturbance-disentangled-based FER methods (Zhang et al., 2018b; Meng et al., 2017) may not accurately recognize expressions in the disturbance-unlabeled FER database.

Different from traditional FER methods, the ADDL method successfully leverages the available disturbance label information from the large-scale face database to perform adversarial transfer learning on the disturbance-unlabeled FER database. In particular, by designing a disturbance subnetwork and minimizing the mutual information, the disturbance can be effectively and explicitly disentangled from the features used for expression recognition. Such a manner significantly improves the discriminability of expression-specific features. Therefore, the problems due to limited training data and the lack of disturbance labels can be greatly alleviated. Moreover, the ADFL is developed to facilitate the extraction of disturbance-specific features by fully exploiting the different influences of disturbing factors in the FER database.

Experiments
In this section, extensive experiments are conducted to show the superiority of our proposed method. First, we introduce several public FER databases and the implementation details in Sects. 4.1 and 4.2, respectively. Then, we conduct ablation studies to evaluate each component of our proposed method in Sect. 4.3. Next, we compare our method with several state-of-the-art FER methods in Sect. 4.4. Finally, we present the computational complexity of our method and apply our method to valence and arousal estimation in Sects. 4.5 and 4.6, respectively.

Databases
To validate the effectiveness of the proposed method, we evaluate the performance on three in-the-lab FER databases [CK+ (Lucey et al., 2010), MMI (Valstar & Pantic, 2010), and Oulu-CASIA (Zhao et al., 2011)) and four in-the-wild databases (RAF-DB (Li et al., 2017), SFEW (Dhall et al., 2011), Aff-Wild2 (Kollias & Zafeiriou, 2018), and AffectNet (Mollahosseini et al. 2017)].

CK+: The Extended Cohn-Kanade (CK+) database is a commonly used laboratory-controlled database for evaluating the FER performance. It contains 327 video sequences annotated with expression labels, including six basic expressions (i.e., angry, happy, surprise, sad, disgust, and fear) and one nonbasic expression (i.e., contempt). Each sequence shows a shift from a neutral expression to a peak expression. We choose the last three expressional frames from each sequence to construct the training set and the test set, which contain 981 images in total.

MMI: The MMI database is composed of 30 subjects, for which 205 image sequences captured in the frontal view are labeled with six basic facial expressions. Similar to the CK+ database, we select the three peak expressional frames in each sequence to compose the training set and the test set (consisting of 615 images in total).

Oulu-CASIA: The Oulu-CASIA database contains videos of 80 subjects. Each subject contains six basic expressions, where each expression corresponds to a video sequence. The videos are collected with two imaging systems (i.e., near-infrared and visible light) under three different illumination conditions. As done in Yang et al. (2018a), the last three frames in each sequence captured with visible light and strong illumination are used in our experiments, resulting in a total of 1,440 images.

RAF-DB: The Real-world Affective Face database (RAF-DB) is a real-world database that contains 15,331 images labeled with six basic facial expressions and a neutral expression, where 12,271 and 3,068 images are used for training and testing, respectively. In addition to the expression labels, the images in RAF-DB are also labeled with the facial attributes of age, gender, and race.

SFEW: The SFEW database is created by selecting the static frames from the AFEW database, which covers unconstrained facial expressions, varied head poses, large age range, varied focus, different resolutions of faces, and real-world illumination. It provides 958 images for training and 436 images for testing. Each image is labeled with one of six basic expressions or the neutral expression.

Aff-Wild2: The Aff-Wild2 database is extended from the Aff-Wild database (Kollias et al., 2019), which consists of 558 YouTube videos with 2,786,201 frames. The videos involve large variations in age, race, pose, illumination, and so on. In this paper, we use the preprocessed version provided by Zhang et al. (2020c) in the ABAW 2020 competition (Kollias et al., 2020b), which contains 904,825 images for training and 322,080 validation images for testing. All the images are annotated with seven expression categories, as in RAF-DB and SFEW.

AffectNet: The AffectNet database is a large-scale database of facial emotions in the wild. It contains 450,000 facial images from the Internet with both categorical (including seven expressions) and valence-arousal annotations. For FER, we select 283,901 images for training and 3500 validation images for testing, as done in Zeng et al. (2018), Wang et al. (2019), Farzaneh and Qi (2021). For valence and arousal estimation, we use all the images with valence-arousal annotations, resulting in 320,739 images for training and 4,500 images for testing.

For in-the-lab databases, we employ the popular tenfold cross-validation protocol for evaluation, as done in Meng et al. (2017), Yang et al. (2018a), Zhao et al. (2016), Ding et al. (2017). For in-the-wild databases, we follow the default evaluation protocols provided by the databases.

Implementation Details
In this paper, we use ResNet-18 pretrained on the MS-Celeb-1M database as the backbone (Wang et al., 2020b). The dimensionalities of {ğŸğ‘ğ‘—}ğ‘€ğ‘—=1, ğŸğ‘‘, and ğŸğ‘’ are 128. Table 1 illustrates the detailed architecture of the MINE and each subnetwork in the ADDM, where the output dimensionality of each layer is also given.

For all the databases, the face in each image is detected and cropped according to the eye positions. Then, the facial image is resized to the size of 256Ã—256. During training, the facial images are randomly cropped to the size of 224Ã—224, and the cropped images are further processed by using a horizontal flip. For the Aff-Wild2 and AffectNet databases, the oversampling strategy is used, as done in Wang et al. (2020b).

Since five blocks are used in ResNet-18, L is set to five in Eqs. (3) and (9). The values of ğœ†1, ğœ†2, and ğœ†3 in Eq. (14) are empirically set to 1.0, 0.10, and 0.0010, respectively. We train the networks using the Adam algorithm (Kingma & Ba, 2014) with a learning rate of 0.0001, ğ›½1=0.500, and ğ›½2=0.999. The learning rate is further divided by 10 after 10, 18, 25, and 32 epochs. All our models are trained on a single NVIDIA GTX 1080Ti GPU using PyTorch for 40 epochs, with a batch size of 16 for RAF-DB and AffectNet and 8 for the other FER databases (except for Aff-Wild2). For Aff-Wild2, our model is trained on two NVIDIA GTX 1080Ti GPUs for 40 epochs with a batch size of 64. For computational complexity, we evaluate the inference time and speed of our method by using a single NVIDIA GTX 1080Ti GPU.

Table 2 Details of the three baseline methods, six DDL variants, and four ADDL variants
Full size table
The DFEM is trained on both the Multi-PIE face database (Gross et al., 2010) and the RAF-DB database, which provide the labels of multiple disturbing factors. Note that large-scale facial attribute databases (such as CelebA (Liu et al., 2015)) are not used for training. This is because they do not have labels of illumination and pose (which are not facial attributes). Moreover, CelebA only contains binary facial attributes (with and without), and thus, it cannot comprehensively describe the variations of each attribute. In contrast, Multi-PIE has labels of identity (337 individuals), pose (15 viewpoints), and illumination (19 lighting conditions), while RAF-DB gives those of gender (3 classes), age (5 ranges), and race (3 classes). Therefore, Multi-PIE and RAF-DB are more suitable to train the DFEM. During the training of the DFEM, missing labels of some disturbing factors are ignored during back-propagation.

For the Aff-Wild2 database, we use the weighted average of accuracy (33%) and F1 score (67%) as the evaluation metric, as done in the ABAW 2020 competition (Kollias et al., 2020b). For the other databases, we adopt the test accuracy as the evaluation metric.

Ablation Studies
To show the superiority of the proposed method, we perform extensive ablation studies to evaluate the influence of different components on the performance. In this subsection, we use one in-the-lab database (MMI) and one in-the-wild database (RAF-DB) for evaluation.

Specifically, we evaluate the performance of three baseline methods, six DDL variants, and four ADDL variants. DDL refers to our original method (Ruan et al., 2020) that does not involve the ADFL and the MINE, while the ADDL method is developed in this paper.

These methods are described as follows: (1) The baseline method (denoted Baseline) that uses only ğ‘†ğ‘” followed by two FC layers to predict the expression of the input image. (2) The baseline method with attention blocks (denoted Baseline_at) that simultaneously uses ğ‘†ğ‘” and ğ‘†ğ‘’, but does not use the multi-level attention mechanism in ğ‘†ğ‘’. (3) The baseline method with attention blocks (denoted Baseline_mat) that simultaneously uses ğ‘†ğ‘” and ğ‘†ğ‘’, and employs the multi-level attention mechanism in ğ‘†ğ‘’. (4) The method (denoted DDL_g) that simultaneously uses ğ‘†ğ‘”, ğ‘†ğ‘’, and ğ‘†ğ‘‘, where ğ‘†ğ‘‘ is trained based on the gender features extracted by the DFEM. (5) The method (denoted DDL_ga) that is similar to DDL_g, but where ğ‘†ğ‘‘ is trained based on both the gender and age features extracted by the DFEM. (6) The method (denoted DDL_gar) that is similar to DDL_g, but where ğ‘†ğ‘‘ is trained based on the gender, age, and race features extracted by the DFEM. (7) The method (denoted DDL_gar&id) that is similar to DDL_g, but where ğ‘†ğ‘‘ is trained based on the gender, age, race, and identity features extracted by the DFEM. (8) The method (denoted DDL_gar&id&il) that is similar to DDL_g, but where ğ‘†ğ‘‘ is trained based on the gender, age, race, identity, and illumination features extracted by the DFEM. (9) The method (denoted DDL_gar&id&il&p) that is similar to DDL_g, but where ğ‘†ğ‘‘ is trained based on the gender, age, race, identity, illumination, and pose features extracted by the DFEM. (10) The ADDL method (denoted ADDL_ADFL) that only uses the ADFL. (11) The ADDL method (denoted ADDL_MI) that employs only the MINE to perform explicit disentanglement between ğŸğ‘‘ and ğŸğ‘’, where ğŸğ‘‘ is learned using the DFEM as for the DDL_gar&id&il&p. (12) The ADDL method (denoted ADDL_MI-DFEM) that employs the MINE but without using the DFEM. (13) The ADDL method that simultaneously uses the ADFL and the MINE.

The details of these methods are summarized in Table 2. For a fair comparison, the pretrained ResNet-18 is employed for all the methods. Table 3 reports the recognition accuracy obtained by these methods on the MMI and RAF-DB databases.

Posed versus Naturalistic Facial Expressions
Generally, in-the-lab FER databases contain posed facial expressions, while in-the-wild databases are comprised of naturalistic facial expressions. Posed expressions usually have slow and jerky onsets, where facial actions typically do not show peaks simultaneously (Motley & Camden, 1988). In contrast, naturalistic expressions tend to exhibit fast and smooth onsets, where distinct facial movements reach peaks in a short duration. According to Table 3, compared with the baseline, the accuracy gains obtained by the ADDL method are 6.97% and 2.41% on the MMI and RAF-DB databases, respectively. This shows the importance of disentangling disturbance for both the posed and naturalistic FER, which enables the extraction of effective expression-specific features. Note that the recognition accuracy obtained by our method on MMI is lower than that on RAF-DB. This can be ascribed to the limited training set (note that there are 615 images in MMI), thereby increasing the difficulty of learning a robust FER model.

Influence of the Attention Block and Multi-level Attention Mechanism
Table 3 The recognition accuracy (%) obtained by the three baseline methods, six DDL variants, and four ADDL variants on the MMI and RAF-DB databases
Full size table
Fig. 5
figure 5
Visualization of attentive feature maps on the a MMI and b RAF-DB databases

Full size image
As illustrated in Table 3, Baseline_at achieves better recognition performance than the Baseline method on both MMI and RAF-DB. Specifically, compared with Baseline, Baseline_at achieves 2.06% and 0.52% gains in terms of recognition accuracy on the MMI and RAF-DB databases, respectively. The above results show the effectiveness of the attention block.

Baseline_mat obtains higher recognition accuracy than Baseline_at. Specifically, in comparison with Baseline_at, Baseline_mat gets 0.25% improvements in terms of recognition accuracy on MMI. For RAF-DB, its accuracy is further improved by 0.16%. This verifies the effectiveness of the multi-level attention mechanism.

To further show the importance of the multi-level attention mechanism, we add the generated feature maps in ğ‘†ğ‘’ to the input facial images and visualize them in Fig. 5. Specifically, the combined feature maps [see Eq. (3)] before the FC layer are first added along the channel dimension, which generates an attentive feature map with a size of 7Ã—7. Then, this feature map is resized to the same size as the input image. Finally, we add the resized attentive feature map to the input image and obtain the final result.

As given in Fig. 5, the warm-toned parts of an image correspond to the regions with large values in the attentive feature map, while the cold-toned parts correspond to the regions with small values in the attentive feature map. We can observe that the attentive feature map is able to focus on the salient facial regions (especially the regions around the eyes and mouth) that are critical for FER. In particular, for the images in RAF-DB, the corresponding attentive feature maps tend to focus on larger facial patches than those in MMI. This is because the images in RAF-DB involve large pose variations and low quality. A larger facial patch is beneficial to extract more discriminative features for FER on the in-the-wild database.

Fig. 6
figure 6
Visualization of the importance weights (corresponding to various disturbing factors) learned by the proposed ADDL in the training sets of the a MMI and b RAF-DB databases

Full size image
Fig. 7
figure 7
Feature visualization using t-SNE. The features are extracted by using two baseline methods and the proposed ADDL method. The first row shows the feature visualization on the MMI database, and the second row shows the feature visualization on the RAF-DB database. a Feature visualization on the model trained by Baseline. b Feature visualization on the model trained by Baseline_mat. c Feature visualization on the model trained by the ADDL method

Full size image
Influence of the Different Disturbing Factors
As shown in Table 3, all the DDL variants consistently perform better than Baseline_mat, which demonstrates the importance of the disturbance subnetwork ğ‘†ğ‘‘. For the RAF-DB database, the recognition accuracy obtained by DDL tends to be higher when more disturbing factors are considered. DDL achieves the best performance when all the disturbing factors are employed for disturbance-specific feature learning in ğ‘†ğ‘‘. This is because the images in RAF-DB contain severe variations caused by multiple disturbing factors. Disentangling these disturbing factors from facial expression images benefits the extraction of effective expression-specific features. However, for the MMI database, DDL obtains the best accuracy when all the disturbing factors except for the pose are considered. This is because the images in MMI do not involve pose variations (the images are all frontal). Therefore, it is critical to properly choose the disturbing factors by taking into account the characteristics of the FER database.

Influence of the ADFL and MINE
From Table 3, we can make the following observations. First, the ADDL_ADFL method achieves better recognition performance than all the DDL variants. Compared with the DDL_gar&id&il&p method, which does not consider the different influences of disturbing factors (i.e., the importance weights corresponding to all the disturbing factors are the same), the ADDL_ADFL method obtains 1.84% and 0.91% improvements in terms of recognition accuracy on MMI and RAF-DB, respectively. This indicates that adopting the SA layer is effective in learning the importance weights, which can be further beneficial to the extraction of disturbance-specific features in ğ‘†ğ‘‘.

Second, to demonstrate the importance of explicit disentanglement, we jointly train the MINE and the ADDM in the ADDL_MI method. The ADDL_MI method also obtains higher accuracy than all the DDL variants. Therefore, minimizing the mutual information is advantageous to explicitly disentangle disturbance-specific features from expression-specific features and has a positive influence on the final performance.

Third, the ADDL method achieves the best accuracy on both in-the-lab and in-the-wild databases when both the ADFL and MINE are jointly adopted. Specifically, the proposed ADDL method outperforms the DDL_gar&id&il&p method by 2.57% and 1.17% on MMI and RAF-DB, respectively. In summary, the developed ADFL and MINE are effective to improve the FER performance.

Table 4 The NMI values obtained by different methods
Full size table
To illustrate that the importance weights from the AFDL can reflect the different influences of disturbing factors in the FER training database, we visualize the importance weights learned by the ADDL method in the training sets of MMI and RAF-DB, as shown in Fig. 6. In Fig. 6a, the importance weight corresponding to the pose is smaller than those corresponding to the other disturbing factors in MMI. This is because the images from MMI do not contain severe pose variations. In Fig. 6b, the weights corresponding to gender, race, age, and pose are similar and higher than the weight corresponding to identity in RAF-DB. This indicates that RAF-DB suffers from more disturbing factors than MMI. Therefore, the above results validate that the proposed AFDL can adaptively estimate the importance weights corresponding to different disturbing factors according to the characteristics of the FER database.

To demonstrate that our proposed method is able to extract discriminative features for expression recognition, we further use t-SNE (Maaten & Hinton, 2008) to visualize the features in the 2D space. Figure 7 shows the feature visualization obtained by the Baseline, Baseline_mat, and ADDL methods on the MMI and RAF-DB databases.

From Fig. 7, we can see that the proposed ADDL method can effectively reduce intra-class variances and inter-class similarities compared with Baseline and Baseline_mat. Baseline_mat achieves better inter-class separation and intra-class compactness than Baseline, which verifies the superiority of the multi-level attention mechanism and attention blocks. As shown in the second row of Fig. 7, due to the great challenges of the RAF-DB database, the features from different classes severely overlap for the Baseline method. In contrast, for the proposed ADDL method, the features from the same class are more closely clustered, while the inter-class distances are enlarged (especially for surprise, sad, neutral, and disgust expressions). Therefore, our method is capable of effectively disentangling the disturbance, even when some challenging variations occur in facial expression images.

Finally, we adopt the Normalized Mutual Information (NMI) value to quantitatively measure the quality of classification results obtained by different methods, as shown in Table 4. We can observe that the ADDL method gives the highest NMI value among all the competing methods. Moreover, both ADDL_ADFL and ADDL_MI obtain higher NMI values than the three baselines and six DDL variants. This further demonstrates the effectiveness of ADFL and MINE for reducing intra-class differences and inter-class similarities.

Influence of the DFEM
Table 5 Ablation studies for the influence of the different DFEM models
Full size table
We evaluate the influence of the DFEM on the final performance. We compare the ADDL_MI-DFEM with the ADDL and ADDL_ADFL methods.

As shown in Table 3, we can see that the recognition accuracy obtained by the ADDL_MI-DFEM method significantly drops on both in-the-lab and in-the-wild databases compared with that of the ADDL method. When the DFEM is not adopted, ADDL is only optimized by the CE loss and mutual information loss. In this way, the disturbance-specific features are learned in an unsupervised way without using any prior knowledge about disturbing factors. Thus, the disturbance subnetwork cannot effectively capture disturbance-related information, degrading the disentanglement performance of ADDL.

Compared with the ADDL_MI-DFEM, the ADDL_ADFL method also has better performance since it is able to extract more discriminative disturbance-specific features by leveraging prior information based on the trained DFEM. Therefore, the DFEM plays a critical role in the disturbance disentanglement to improve the accuracy of the ADDL method.

Influence of the Different DFEM Models
Table 6 Ablation studies for the influence of backbones pretrained on different databases
Full size table
Table 7 Ablation studies for the influence of the balanced parameters ğœ†1, ğœ†2, and ğœ†3 on the MMI and RAF-DB databases
Full size table
We evaluate the performance of our method with the different DFEM models trained based on three face databases (Multi-PIE, RAF-DB, and Multi-PIE & RAF-DB), as shown in Table 5.

We can see that when the DFEM is trained based on Multi-PIE (including the labels of identity, illumination, and pose), ADDL only achieves 85.00% and 88.89% on MMI and RAF-DB, respectively. When the DFEM is trained based on RAF-DB (including the labels of gender, race, and age), ADDL obtains 85.71% and 88.69% on MMI and RAF-DB, respectively. However, when both RAF-DB and Multi-PIE are used to train the DFEM, the performance of the ADDL method is greatly improved. This further shows the importance of considering different types of disturbing factors for disturbance disentanglement.

Influence of Backbones Pretrained on the Different Databases
Table 8 Comparisons of all the competing methods on in-the-lab databases (CK+, MMI, and Oulu-CASIA)
Full size table
We investigate the influence of backbones pretrained on the different databases (including ImageNet, AffectNet, and MS-Celeb-1M) on the final performance, as shown in Table 6. The performance obtained by our method without pretraining the backbone is also evaluated.

Our method with the pretrained backbone achieves much better FER performance than that without pretraining the backbone. Moreover, our method with the backbone pretrained on MS-Celeb-1M gives better performance than those pretrained on other large-scale databases. This is because MS-Celeb-1M (including 10 M images) contains many more facial images than AffectNet (including 283 K images), which facilitates the backbone network to extract more effective global features for FER. Although there are a great number of images in ImageNet, most samples are natural images rather than facial images. Therefore, our method with the backbone pretrained on ImageNet gives the worst performance among the three pretrained backbones.

Influence of Balanced Parameters
We study the influence of three balanced parameters (i.e., ğœ†1, ğœ†2, and ğœ†3) in the joint loss [Eq. (14)], as shown in Table 7.

Specifically, we first fix ğœ†2=0.10 and ğœ†3=0.0010, and set the values of ğœ†1 from 0.0 to 2.0. The results are shown in Table 7a. When ğœ†1=0.0, adversarial training is not adopted, and thus, the disturbance-specific features cannot be effectively learned, leading to a performance decrease. When ğœ†1=1.0, the proposed method obtains the highest accuracy. Then, we fix ğœ†1=1.0 and ğœ†3=0.0010, and set the values of ğœ†2 from 0.00 to 0.20. The results are shown in Table 7b. The proposed method achieves the top performance when ğœ†2=0.10. Note that when ğœ†2=0.00 (attention transfer is not used in this case), the proposed method achieves worse accuracy than that without using adversarial training on both MMI and RAF-DB. Hence, it is important to bridge the gap between the DFEM and the disturbance subnetwork at the lower layers. Finally, Table 7c illustrates the results obtained by our method by fixing ğœ†1=1.0 and ğœ†2=0.10 and varying the values of ğœ†3 from 0.0000 to 0.1000. We can observe that the proposed method obtains the best accuracy when ğœ†3=0.0010. In this paper, we use ğœ†1=1.0, ğœ†2=0.10, and ğœ†3=0.0010 for all the experiments.

Comparisons with State-of-the-Art FER Methods
In this subsection, we compare our proposed method with several state-of-the-art FER methods.

For in-the-lab databases, we compare the proposed ADDL with fourteen representative FER methods, including LBP-TOP (Zhao & Pietikainen, 2007), PPDN (Zhao et al., 2016), FN2EN (Ding et al., 2017), IACNN (Meng et al., 2017), DLP-CNN (Li & Deng, 2018), DTAGN (Jung et al., 2015), DeRL (Yang et al., 2018a), IPA2LT (Zeng et al., 2018), DAM-CNN (Xie et al., 2019a), PHRNN-MSCNN (Zhang et al., 2017), L2-sparseness (Xie et al., 2019b), FMPN (Chen et al., 2019), TDGAN (Xie et al., 2020), and our previous DDL (Ruan et al., 2020). For in-the-wild databases, we also compare our proposed ADDL with several representative FER methods, including gACNN (Li et al., 2018), IPA2LT (Zeng et al., 2018), SPDNet (Acharya et al., 2018), IPFR (Wang et al., 2019), RAN (Wang et al., 2020c), SCN (Wang et al., 2020b), FMPN (Chen et al., 2019), CPG (Hung et al., 2019b), PAENet (Hung et al., 2019a), PSR (Vo et al., 2020), DACL (Farzaneh & Qi, 2021), EfficientNet-B0 (Savchenko, 2021), CNN (Anas et al., 2020), NISL (Deng et al., 2020), LLAM (Wang et al., 2020a), ICT-VIPL (Zhang et al., 2020c), DMACS (Gera & Balasubramanian, 2020), ResNet101+BLSTM (Liu et al., 2020), ResNet101+BLSTM+CBAM (Liu et al., 2020), SIU (Dresvyanskiy et al., 2020), and TNT (Kuhnke et al., 2020).

Table 8 gives the performance comparisons between the proposed method and several state-of-the-art FER methods on in-the-lab databases (CK+, MMI, and Oulu-CASIA). Tables 9, 10, and 11 give the performance comparisons on two in-the-wild databases (RAF-DB and SFEW), Aff-Wild2, and AffectNet, respectively. The accuracy obtained by each competing method is taken directly from the corresponding paper.

Table 9 Performance comparisons between our method and several state-of-the-art FER methods on the RAF-DB and SFEW databases
Full size table
Results on In-the-Lab Databases
As shown in Table 8, almost all the methods obtain high recognition accuracy in the CK+ database and relatively low classification rates in the MMI database among the three in-the-lab databases. This is because the images from CK+ are of high quality and the intensities of different expressions are strong, while those from MMI are affected by the glasses and the expression intensities are weak.

Among all the competing methods, the top four methods are our proposed ADDL, DDL, FN2EN, and PHRNN-MSCNN. The proposed ADDL method outperforms DDL in all the in-the-lab databases due to the effectiveness of ADFL and MINE, where the ADFL extracts adaptive disturbance-specific features and the MINE performs explicit disentanglement between expression-specific features and disturbance-specific features. Note that the disturbing factors are not explicitly disentangled in DDL, leading to inferior expression-specific features. ADDL also achieves better accuracy than FN2EN on both CK+ and Oulu-CASIA. Note that our test set in CK+ is more challenging (since it contains the images corresponding to the contempt expression apart from the six basic expressions), while FE2EN only considers the six basic expressions. PHRNN-MSCNN is comprised of a recurrent neural network (RNN) and a CNN, where both the facial image and facial landmarks are used as the input. In contrast, our proposed ADDL achieves better performance by using a single image as the input. In particular, although MMI contains more challenging variations than the other two in-the-lab databases, ADDL outperforms PHRNN-MSCNN by a large margin (4.95% improvements) on MMI. This can be ascribed to the effectiveness of our proposed adaptive deep disturbance-disentangled learning.

Table 10 Performance comparisons between our method and several FER state-of-the-art methods on the AffectNet database
Full size table
Results on In-the-Wild Databases
As shown in Table 9, we compare the proposed method with twelve state-of-the-art FER methods on the RAF-DB and SFEW databases. Among all the methods, the proposed ADDL, SCN, DACL, DDL, and SPDNet obtain higher recognition accuracy than the other competing methods on RAF-DB, while the proposed ADDL, DDL, IPA2LT, and SPDNet are the top four methods on SFEW. SCN addresses the uncertainty problem in FER and achieves state-of-the-art performance. IPA2LT deals with the problem of inconsistent annotations in the FER databases. SPDNet introduces the covariance pooling into FER. PSR develops a scaling block to handle facial images at different resolutions. DACL leverages an attention mechanism based on a sparse center loss to enhance the discriminative capability of features. However, the above methods do not explicitly take the disturbing factors into consideration, which may lead to inferior performance in the case of limited training samples.

On the one hand, IACNN and IPFR are disturbance-disentangled-based methods, but they can cope with only one or two disturbing factors. Different from the above methods, ADDL is able to explicitly disentangle multiple disturbing factors by leveraging adversarial transfer learning, even though disturbing factors are not labeled in the FER database. On the other hand, gACNN and RAN address the occlusion problem by combining local learning and global learning. However, these two methods only utilize high-level features to perform FER. Unlike these methods, ADDL exploits both high-level features and low-level features in the expression subnetwork, thereby achieving excellent performance. Finally, compared with DDL, ADDL achieves higher accuracy on RAF-DB and SFEW. It is worth pointing out that DDL cannot adaptively choose the disturbing factors when trained on an FER database. However, ADDL effectively alleviates this problem by designing the ADFL.

From Table 10, our proposed ADDL method performs the best among all the image-based and video-based methods, with an overall score of 49.66%. SIU and TNT outperform our method, because they exploit the additional temporal and audio information for FER. Among all the competing methods, NISL proposes a multi-task model to learn from incomplete labels. LLAM and DMACS resort to attention blocks to extract global and local attention-aware features from facial images. ICT-VIPL, SIU, and TNT simultaneously extract visual features from videos and acoustic features from audio tracks to construct discriminative expression features. ResNet101+BLSTM uses ResNet-101 and BLSTM to extract semantic features and temporal features, respectively. However, the above methods do not fully consider the multiple disturbing factors in facial expression images. In summary, the above results show the effectiveness of our method in the large-scale FER database.

From Table 11, the proposed ADDL outperforms the other competing methods on AffectNet. FMPN designs an additional branch to learn local features from facial muscle moving regions. Then, the local features are combined with holistic features for classifying expressions. CPG and PAENet introduce compact and unforgetting models to progressively learn new tasks. EfficientNet-B0 is trained in a multi-task learning manner, where facial attribute prediction is performed to improve the representation ability of the features (i.e., edges and corners) at the lower CNN layers. However, the above methods along with IPA2LT and gACNN do not perform disturbance disentanglement, leading to inferior FER performance.

Computational Complexity
In this subsection, we briefly analyze the computational complexity of the ADDL method. We also evaluate SCN and the Baseline_mat method for a comparison. Note that the results obtained by other competing methods are not given since their source codes are not publicly available. We use the number of parameters (Params) and Floating Point operations (FLOPs) to evaluate the memory consumption and computational complexity of the model, respectively. Moreover, we adopt the inference time and speed to measure latency. We take the RAF-DB database for performance evaluation.

Table 12 reports the number of parameters and FLOPs obtained by SCN, Baseline_mat, and ADDL. Both ADDL and Baseline_mat have more parameters and higher FLOPs than SCN. This is because the ADDM, which is based on multiple attention blocks, is trained during the two-stage learning procedure.

The inference time and speed obtained by SCN, Baseline_mat, and ADDL are given in Table 13. We can observe that the proposed ADDL obtains an inference time of 5.21 ms, which is similar to Baseline_mat due to the same inference phases. The inference speed of ADDL is lower than that of SCN. Because multiple attention blocks are employed in ADDL to extract discriminative features. This improves the FER accuracy but slows down the inference speed of the model. Although the computational complexity of the training phase of our proposed ADDL method is high, it can still obtain real-time inference speed and be applicable to real-world scenarios.

Table 11 The inference time and speed obtained by different methods on the RAF-DB database
Full size table
Valence and Arousal Estimation
In this subsection, we evaluate the performance of our method for the task of valence and arousal (VA) estimation on the AffectNet database. Similar to previous methods (Mollahosseini et al. 2017; Jang et al., 2019; Kollias et al., 2018), we view the VA estimation as a regression task.

To perform the VA estimation, an FC layer is added after the expression classification layer (i.e., the last layer) in ğ‘†ğ‘’ to regress the valence and arousal values. Then, we fine-tune the classification and regression layers based on a well-trained ADDM that obtains the best validation accuracy for a 7-way FER. The learning rate is set to 0.001 for the last two layers and 0.0001 for the other layers in the ADDM. In this paper, we adopt two commonly used metrics, i.e., root mean square error (RMSE) and concordance correlation coefficient (CCC) (Mollahosseini et al. 2017), to evaluate the performance. Thus, we add the RMSE and CCC losses into Eq. (14) for joint training. The comparison results are reported in Table 14.

As shown in Table 14, the factorized high-order CNN method achieves the best performance on the four evaluation metrics except for the RMSE of valence. The proposed ADDL obtains the best result on the RMSE of valence and the second place on the other three evaluation metrics. Factorized high-order CNN (Kossaifi et al., 2020b) employs a higher-order factorized convolution network, where a single tensor regression layer (Kossaifi et al., 2020a) is dedicated to performing regression of the VA values. In contrast, the proposed ADDL is based on a classification model with an additional regression layer, which may limit the regression performance. The VGG-Face+2M imgs method synthesizes facial images to improve the performance for the VA estimation. Face-SSD jointly performs face detection and face analysis. However, these methods obtain worse performance than ours. These results show the feasibility of our method for the VA estimation.

Conclusion and Future Work
In this paper, we propose a novel ADDL method for FER. ADDL is able to disentangle multiple disturbing factors simultaneously and adaptively (even when the labels of disturbing factors are not available in the FER database) and effectively extract expression-related information. The training of ADDL contains two stages. First, a DFEM is trained to identify multiple disturbing factors in a multi-task learning manner. Then, based on the trained DFEM, an ADDM is learned to classify facial expressions by considering the characteristics of the FER database. In the ADDM, an ADFL is developed to estimate the importance weights corresponding to different disturbing factors and perform adversarial transfer learning. Furthermore, an MINE is employed to enable the explicit disentanglement between expression-specific features and disturbance-specific features. Extensive experiments on both in-the-lab and in-the-wild FER databases have demonstrated the superior performance of ADDL over several state-of-the-art FER methods.

It is widely assumed that facial expressions can infer the emotional state of humans. However, Barrett et al. (2019) show that the way humans express their emotions may significantly vary across different cultures and situations. Moreover, they also reveal that similar configurations of facial movements may belong to different emotion categories. Naturally, human perception of emotions does not rely on one type of information. Instead, it is triggered by a variety of cues from different sources. By investigating such cues, many recent efforts (Lv et al., 2021) have been proposed toward multi-modality (such as facial expressions, body gestures, and voice to physiological signals) emotion recognition by leveraging the strengths of each modality. In the future, we plan to extend our method to multi-modality emotion recognition.