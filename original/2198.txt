In this paper, we propose a novel continuous latent semantic analysis fitting method, to efficiently and effectively estimate the parameters of model instances in data, based on latent semantic analysis and continuous preference analysis. Specifically, we construct a new latent semantic space (LSS): where inliers of different model instances are mapped into several independent directions, while gross outliers are distributed close to the origin of LSS. After that, we analyze the data distribution to effectively remove gross outliers in LSS, and propose an improved clustering algorithm to segment the remaining data points. On the one hand, the proposed fitting method is able to achieve excellent fitting results; due to the effective continuous preference analysis in LSS. On the other hand, the proposed method can efficiently obtain final fitting results due to the dimensionality reduction in LSS. Experimental results on both synthetic data and real images demonstrate that the proposed method achieves significant superiority over several state-of-the-art model fitting methods on both fitting accuracy and computational speed.

Geometric model fitting is one of the most challenging tasks in computer vision, and it has been applied to several types of computer vision tasks, such as, homography/fundamental matrix estimation (Xiao et al. 2019), vanishing point detection (Lee and Yoon 2019), 3D reconstruction (Yang and Meer 2020), and motion segmentation (Xu et al. 2019). One of the main challenges in model fitting is to robustly estimate the model parameters for data involving a large number of outliers and multiple structures.

Background
Several Primary Definitions
We firstly provide some definitions in model fitting to address the model fitting problem.

A geometric model is usually specified before model fitting. Several popular geometric models have been used, such as, line, circle, homography/fundamental matrix, etc. Once a type of geometric model is specified, one can explain data with the model.

A model instance (also called a structure) is an instance of the specified type of model in the data. Multi-structural data contain multiple model instances.

A minimal subset is the minimum number of data points that are required generating a model hypothesis, e.g., 2 data points for line fitting, 4 data points for homography fitting.

A model hypothesis is generated based on each minimal subset. The subset is sampled using different types of sampling strategies. For data with outliers, many model hypotheses are usually required being generated to estimate the model instances in data.

Based on the above definitions, the model fitting problem can be described as: Given a set of input data points and a specified type of model, the goal is to estimate the parameters of model instances that best explain the input data (Wu et al. 2008; Le et al. 2019; Meer 2020; Ma et al. 2021).

Generally, there are two steps included in the model fitting procedure: (1) Sample a number of minimal subsets to generate a set of model hypotheses; (2) Estimate the parameters of model instances according to the generated model hypotheses by model selection.

Some methods have been proposed to improve either the quality of sampling [e.g., Proximity (Kanazawa and Kawakami 2004), PROSAC (Chum and Matas 2005), MultiGS (Chin et al. 2012) and MAGSAC (Barath et al. 2019, 2020)) or the performance of model selection (e.g., HMSS (Tennakoon et al. 2016) and RansaCov (Magri and Fusiello 2016)].

In this paper, we mainly focus on both the effectiveness and computational efficiency of model selection.

Prior Work
Existing fitting methods can be roughly classified into two categories, i.e., the consensus analysis based and the preference analysis based fitting methods. The consensus analysis based fitting methods [e.g., RANSAC (Fischler and Bolles 1981), RansaCov (Magri and Fusiello 2016), RHT (Xu et al. 1990), GPbM (Mittal et al. 2012), AKSWH (Wang et al. 2012) and MSHF (Wang et al. 2019)] directly select model instances from a number of generated model hypotheses according to some evaluation criterions (such as, the number of inliers or the weighting scores of model hypotheses), and then segment data points according to the user-specified or estimated inlier noise scale.

Among theses methods, RANSAC is one of the most popular fitting methods due to its simplicity and robustness to outliers. The main steps of RANSAC are described as follows:

Randomly sample a number of minimal subsets of data points and then generate the corresponding model hypotheses.

Select the model hypothesis that includes the max number of inliers as the estimated model instance.

RANSAC is simple yet robust. However, it suffers from several limitations. For example, it requires randomly sampling a large number of minimal subsets for model hypothesis generation to “hit”Footnote1 a model instance in data with high proportions of outliers. However, most of the sampled minimal subsets are not significant (i.e., most of the sampled minimal subsets include outliers). This will cause high computational complexity, especially for high dimensional models. Moreover, RANSAC sequentially selects the best model hypothesis as the estimated model instance for multi-structural data via a “fit-and-remove” framework, which is not very effective and computationally efficient (Wang et al. 2012).

To improve the fitting performance, RansaCov formulates the model fitting problem as a problem of set coverage and it searches for the best sets that include the maximum number of inliers. RHT (Xu et al. 1990) assigns each generated model hypothesis a score, and then selects the model hypothesis with the maximum score as the estimated model instance. GPbM (Mittal et al. 2012) formulates the model fitting problem as an optimization problem. AKSWH (Wang et al. 2012) directly clusters the generated model hypotheses, and then selects the model hypothesis with the maximum weighting score from each cluster as an estimated model instance. MSHF (Wang et al. 2019) combines hypergraphs and mode-seeking to directly select model hypotheses as the estimated model instances according to their weight scores and a similarity matrix. Although these fitting methods are not very sensitive to data distribution (i.e., their performance is relatively less affected by unbalanced data distribution), their fitting performance largely depends on the quality of the generated model hypotheses.

The preference analysis based fitting methods [e.g., SWS (Purkait et al. 2017), HF (Xiao et al. 2016), RPA (Magri and Fusiello 2017), J-linkage (Toldo and Fusiello 2008), T-linkage (Magri and Fusiello 2014) and MCT (Magri and Fusiello 2019)] estimate model instances by labeling data points based on the generated model hypotheses. Specifically, SWS (Purkait et al. 2017) adopts large hyperedges to represent the relationship between model hypotheses and data points for model fitting. HF (Xiao et al. 2016) introduces a novel hypergraph model to formulate the model fitting problem as a hypergraph partition problem. RPA (Magri and Fusiello 2017) combines principal component analysis and non negative matrix factorization to deal with multi-structure model fitting. J-linkage (Toldo and Fusiello 2008)/T-linkage (Magri and Fusiello 2014) directly cluster data points according to their preference sets. MCT (Magri and Fusiello 2019) extends T-linkage to handle different nested classes of models.

Moreover, there are also some energy-based model fitting methods, e.g., PEaRL (Isack and Boykov 2012), RCMSA (Pham et al. 2014) and CORAL (Amayo et al. 2018). We can see that, most of the currently existing fitting methods estimate the fitting results from the residual matrix, which is derived from model hypotheses and data points. However, to cover all model instances in data, they often require sampling a large number of model hypotheses. Then, the fitting methods may suffer from high computational complexity when they compute the similarities between all data points, especially for large-size data.

Fig. 1
figure 1
Overview of the proposed method for two-view based motion segmentation

Full size image
Motivations and Contributions
In this paper, we propose a continuous latent semantic analysis method (called as CLSA) for robust multi-structure fitting. We aim to exploit the advantage of preference analysis (that is able to achieve good fitting accuracy) and yet reduce the high computational complexity of multi-structural model fitting by applying latent semantic analysis (LSA) (Deerwester et al. 1990) on data points (involving a large number of outliers). Note that LSA is a valuable analysis tool, which was originally used to construct a low-dimensional subspace by analyzing the relationships between a set of documents and terms for word processing, to effectively reduce the dimensions of word-document count vectors. Inspired by that work, we construct a new low-dimensional latent semantic space (LSS), to analyze the relationships among data points, by both LSA and continuous preference analysis (CPA)Footnote2 implemented by T-linkage (Magri and Fusiello 2014), for solving the model fitting problem.

It is worth pointing out that the combination of LSA and CPA is an important improvement over either LSA or CPA. That is, for LSA, CPA can capture more accurate relationships between model hypotheses and data points (note that LSA has no notion of these). For CPA, LSA can be used to reduce the high dimension of a preference matrix generated by CPA while keeping its principal preference information. Thus, both LSA and CPA jointly lead to better fitting performance and higher computational efficiency. To the best of our knowledge, we are the first to exploit the relationships between data points and model hypotheses via both LSA and CPA, for robust multi-structure model fitting.

Moreover, we analyze the constructed LSS to remove gross outliers, and cluster the remaining data points into independent directions in LSS. Note that there usually are two kinds of outliers, i.e., gross outliers and pseudo outliers in data points. Gross outliers are the data points that do not belong to the inliers of any model instance in data; while pseudo outliers are the data points that belong to the inliers of one model instance, but they are the outliers to the other model instances in data. In our work, we aim to effectively remove gross outliers and cluster/segment inliers (belonging to different model instances) in LSS. Firstly, we adaptively set a threshold based on an information theoretic approach (Ferraz et al. 2007) to remove gross outliers in LSS. Then, by analyzing the distribution of the remaining data points in LSS, we cluster these points into different directions via an improved clustering algorithm. Note that, in LSS, the high-order model fitting problem (e.g., homography estimation and two-view based motion segmentation) is formulated as a simple clustering problem, which is much easier to deal with.

We show the overview of the proposed CLSA method applied to two-view based motion segmentation in Fig. 1. In Fig. 1b, a preference matrix, whose entries are the preference values of data points to model hypotheses, and the key step of continuous latent semantic analysis are shown. From Fig. 1c, we can see that, each of the input feature matches is mapped to a point in LSS by the proposed method. In LSS, the points corresponding to the inliers and gross outliers are distributed differently. That is, the points corresponding to the gross outliers are distributed close to the origin of LSS, while the points corresponding to the inliers of different model instances are distributed in their respective directions and they are far from the origin of LSS (see Fig. 1d). After removing the gross outliers from the input data, the complex model fitting problem becomes a much easier clustering problem, which can be solved in an effective and efficient manner (see Fig. 1e).

In this paper we are primarily concerned with proposing a model fitting method to segment data. In model fitting, one tries to jointly determine both the parameters of the underlying models, and the segmentation induced by the assignment of data to the models they fit. Since our main aim is to segment the data, the accuracy of the estimated model instances is only a means to an end. In contrast, for model extraction/estimation, the accuracy of concern is that of the model parameter estimates, not the induced segmentation per se. Despite the focus of this work on segmentation, the ideas could also be used in areas where the model parameters are more of interest but that is beyond the scope of this paper.

The key contributions of this work are summarized as follows:

We propose a way to analyze the complex relationships between data points and model hypotheses by exploiting the advantage of both latent semantic analysis and continuous preference analysis, to improve the performance on both segmentation accuracy and computational efficiency for robust model fitting.

We propose an adaptive gross outlier removal strategy in the new latent semantic space, where inliers and gross outliers are separately distributed. Then we formulate the complex model fitting problem as a simple clustering problem on the remaining data without gross outliers.

We improve a traditional clustering method (MacQueen 1967) by introducing a preference function to effectively seed the initial centers, which achieves more stable clustering results for the task of model fitting.

Our experimental evaluations and comparisons demonstrate the significant superiority of the proposed method over several state-of-the-art geometric model fitting methods for different model fitting tasks.

The rest of the paper is organized as follows: We describe the components of the proposed fitting method in Sect. 2. We summarize the complete method in Sect. 3. We present the experimental results on both synthetic and real data in Sect. 4. We discuss the proposed method in Sect. 5 and draw conclusions in Sect. 6.

The Methodology
In this section, we describe the details of the proposed continuous latent semantic analysis fitting method (called CLSA). We first describe the problem formulation in Sect. 2.1, and then review the background of latent semantic analysis (LSA) (Deerwester et al. 1990) (Sect. 2.2). We then construct a novel latent semantic space by LSA and the continuous preference analysis (CPA) in Sect. 2.3. After that, we analyze the data distribution in the constructed latent semantic space, and then propose an adaptive gross outlier removal strategy (Sect. 2.4). Then we propose an improved clustering algorithm to segment the remaining data points (Sect. 2.5).

Problem Formulation
Given a set of data points 𝑋={𝑥1,…,𝑥𝑛} and a set of model hypotheses 𝛩={𝜃1,…,𝜃𝑛} generated by sampling, we define the preference function (Magri and Fusiello 2014), which represents the preference of a data point 𝑥𝑖 to a model hypothesis 𝜃𝑗, as follows:

𝑓(𝑥𝑖,𝜃𝑗)=exp{−𝑑(𝑥𝑖,𝜃𝑗)𝜓},
(1)
where 𝑑(𝑥𝑖,𝜃𝑗) denotes the residual value measured with the Sampson distance (Torr and Murray 1997) from a data point 𝑥𝑖 to a model hypothesis 𝜃𝑗. 𝜓 is a threshold (and the influence of its value on the proposed CLSA will be discussed with more details in Sect. 4.1).

By using Eq. (1), we compute the preference matrix, which is used to estimate the parameters of model instances in the data. Then, we also segment input data points belonging to different model instances into different groups. The cost function is defined as Magri and Fusiello (2016), Mittal et al. (2012):

𝑆𝐸=# 𝑚𝑖𝑠𝑙𝑎𝑏𝑒𝑙𝑒𝑑 𝑑𝑎𝑡𝑎 𝑝𝑜𝑖𝑛𝑡𝑠# 𝑑𝑎𝑡𝑎 𝑝𝑜𝑖𝑛𝑡𝑠,
(2)
where # 𝑚𝑖𝑠𝑙𝑎𝑏𝑒𝑙𝑒𝑑 𝑑𝑎𝑡𝑎 𝑝𝑜𝑖𝑛𝑡𝑠 is the number of mislabeled data points, and # 𝑑𝑎𝑡𝑎 𝑝𝑜𝑖𝑛𝑡𝑠 is the number of input data points. Note that, if the estimated model instance is close to the true model instance, the value of SE will be small; Otherwise, the value of SE will be large.

Latent Semantic Analysis (LSA)
Latent semantic analysis (LSA) was proposed to analyze a binary matrix of documents/terms, via singular value decomposition (SVD), for document representation.

Specifically, given documents 𝐷𝑜={𝑑𝑜1,…,𝑑𝑜𝑛} with terms 𝑇𝑒={𝑡𝑒1,…,𝑡𝑒𝑚}, generate a binary matrix 𝐓𝐃, which denotes the relationship between documents and terms (i.e., 𝑡𝑑(𝑖,𝑗)=1 if the i-th document belongs to j-th term; Otherwise, 𝑡𝑑(𝑖,𝑗)=0). The key idea of LSA is to map documents to a latent space, where documents are close if they share meaningful association, based on 𝐓𝐃 by SVD.

LSA firstly decomposes 𝐓𝐃 by SVD as follows:

𝐓𝐃𝑛×𝑚=𝐔ΣΣ𝐕𝑇,
(3)
where 𝐔 and 𝐕 are the orthogonal matrices of left and right singular vectors 𝐔𝐔𝑇=𝐕𝐕𝑇=𝐼 (here I is an identity matrix), respectively. ΣΣ=𝑑𝑖𝑎𝑔(𝜎1,…,𝜎𝑟) is a diagonal matrix with singular values on the diagonal and the singular values are listed in non-increasing order, i.e., 𝜎1≥𝜎2≥⋯≥𝜎𝑟>0, 𝑟=𝑟𝑎𝑛𝑘(𝐓𝐃).

Then 𝐓𝐃 is approximately computed by the top k singular values in ΣΣ, i.e.,

𝐓𝐃=𝐓𝐃~≈𝐔𝑛×𝑟𝑘ΣΣ𝑟𝑘×𝑟𝑘𝐕𝑇𝑚×𝑟𝑘,
(4)
where 𝑟𝑘 is the number of considering 𝑟𝑎𝑛𝑘(𝐓𝐃). After that, LSA uses the rows of 𝐔𝑛×𝑟𝑘ΣΣ𝑟𝑘×𝑟𝑘 as the coordinates for documents in the latent space. This is because the document-to-document inner products are given by:

𝐓𝐃~𝐓𝐃~𝑇=𝐔𝑛×𝑟𝑘ΣΣ𝑟𝑘×𝑟𝑘𝐕𝑇𝑚×𝑟𝑘𝐕𝑚×𝑟𝑘𝐼𝐔𝑇𝑛×𝑟𝑘ΣΣ𝑟𝑘×𝑟𝑘.
(5)
Based on the latent space, LSA is able to effectively analyze the relationship between documents.

Latent Semantic Space Construction
Inspired by LSA, we propose to analyze an inlier indexing matrix (generated by considering both data points and model hypotheses) via SVD for the model fitting problem. A binary inlier indexing matrix is used to represent the relationship between data points and model hypotheses. Here, we use a continuous preference matrix (Magri and Fusiello 2014) (derived from a preference function) instead of a binary one to improve its fitting performance.

For n input data points and m generated model hypotheses, a singular value decomposition of a continuous preference matrix 𝐅𝑛×𝑚 (each entry is derived from Eq. (1)) has the following form:

𝐅𝑛×𝑚=𝐔ΣΣ𝐕𝑇,
(6)
i.e.

⎡⎣⎢⎢⎢𝑓(𝑥1,𝜃1)⋮𝑓(𝑥𝑛,𝜃1)⋯⋱⋯𝑓(𝑥1,𝜃𝑚)⋮𝑓(𝑥𝑛,𝜃𝑚)⎤⎦⎥⎥⎥=[[𝑢1]⋯[𝑢𝑟]]⋅⎡⎣⎢⎢⎢𝜎1⋮0⋯⋱⋯0⋮𝜎𝑟⎤⎦⎥⎥⎥⋅⎡⎣⎢⎢⎢[𝑣1]⋮[𝑣𝑟]⎤⎦⎥⎥⎥
(7)
Recall that, a preference matrix denotes the preference between data points and model hypotheses. Specifically, if the value of 𝑓(𝑥𝑖,𝜃𝑗) is large, it means that the data point 𝑥𝑖 prefers the model hypothesis 𝜃𝑗. Thus, after decomposition by SVD, a left singular vector 𝑢𝑖 denotes the preference of all data points to the i-th topic model of model hypotheses. A topic model is a estimated model instance for model fitting. Accordingly, a right singular vector 𝑣𝑗 denotes the preference of all model hypotheses to the j-th topic model of data points.

Each singular value 𝜎𝑖 denotes the strength of correlation between a data point and a model hypothesis. That is, if the 𝜎𝑖 value is large, then it is a high possibility that the i-th topic model of data points and the i-th topic model of model hypotheses correspond to the same model instance in data; Otherwise, the i-th topic model of data points and the i-th topic model of model hypotheses are usually insignificant and thus they can be ignored. This is because an inlier and the corresponding model instance have much stronger correlation than the other cases. Therefore, we only need to analyze the top 𝑟𝑘 data points and model hypotheses instead of analyzing all data points and model hypotheses. Here, 𝑟𝑘 is usually set to a small value, i.e., 𝑟𝑘≪𝑟 (we also further discuss its influence in Sect. 4.1). Thus, Eq. (6) can be rewritten as:

𝐅𝑛×𝑚=𝐅̃ 𝑛×𝑚≈𝐔𝑛×𝑟𝑘ΣΣ𝑟𝑘×𝑟𝑘𝐕𝑇𝑚×𝑟𝑘.
(8)
Eq. (8) is also consistent with the property of SVD that the top 10% (even 1%) of singular values account for over 99% of all singular values.

Then, to simplify the complex model fitting problems, we construct a latent space where the data points belonging to the same model instance (these data points share similar preference to the same model hypothesis) are close to each other. That is, the value of a data point 𝑥𝑖 in the constructed space only relates to the left singular vectors and has nothing with the origin value of 𝑥𝑖. Here we call the constructed space the “latent semantic space (LSS)”, because the space involves the latent semantic information (the inliers belonging to the same model instance in the data usually correspond to the mapped points in one direction of LSS).

Note that the points-to-points inner products based on Eq. (8) are given by:

𝐅̃ ̃ 𝑛×𝑛=𝐅̃ 𝑛×𝑚𝐅̃ 𝑇𝑛×𝑚=𝐔𝑛×𝑟𝑘ΣΣ2𝑟𝑘×𝑟𝑘𝐔𝑇𝑛×𝑟𝑘.
(9)
Now, we have a 𝑛×𝑛 matrix 𝐅̃ ̃  to determine singular values ΣΣ and left singular vectors 𝐔. Thus, we use the rows of 𝐔𝑛×𝑟𝑘ΣΣ𝑟𝑘×𝑟𝑘 as defining coordinates for data points in LSS.

After that, we no longer analyze the preference matrix but LSS, and the dimension is reduced from a large m value of a preference matrix to a much smaller 𝑟𝑘 value of LSS. It is also worth pointing out that, the value of each data point in LSS has its specific physical interpretation. That is, the value of a data point 𝑥𝑖 in the j-th dimension of LSS corresponds to the preference of 𝑥𝑖 to the j-th topic model of model hypotheses. Thus, the complex model fitting problem is reduced to a much easier LSS analysis problem.

Fig. 2
figure 2
An example of outlier removal based on the information theoretic approach for homography estimation. a The input “Elderhalla” image pair. b The proposed outlier removal strategy. c The estimated inliers. d The removed gross outliers. The gross outliers are marked in blue, and the inliers of different model instances are marked in red. (Color figure online)

Full size image
Adaptive Outlier Removal
Based on the constructed LSS, we propose a novel simple but effective outlier removal strategy in this subsection.

In LSS, a data point 𝑥𝑖 corresponding to an inlier is assigned a large value in a direction (which means the data point is distributed far from the origin of LSS), while 𝑥𝑖 corresponding to a gross outlier is assigned a small value. The reason behind this is that, the value of 𝑥𝑖 in the j-th dimension of LSS corresponds to the preference of 𝑥𝑖 to the j-th topic model of model hypotheses. Obviously, a topic model of model hypotheses prefers 𝑥𝑖 if it is an inlier while not any topic model of model hypotheses prefers 𝑥𝑖 if it is a gross outlier. Thus, in LSS, the points corresponding to inliers of model instances are distributed in several independent directions while the points corresponding to gross outliers are distributed close the origin (𝐎) of LSS. That is, the points corresponding to inliers are far from the origin 𝐎 while the points corresponding to gross outliers are close to 𝐎. Thus, we remove the points corresponding to gross outliers by using a threshold, according to the distance between the mapped points and 𝐎 in LSS. However, it is difficult to manually set a proper threshold value for different input data.

In this paper, we propose to use information theory (Ferraz et al. 2007) to adaptively set the threshold value for outlier removal. Firstly, given a set of mapped points 𝐗̂ ={𝑥̂ 𝑖}𝑖=1,…,𝑛 in LSS, we compute the Euclidean distance 𝑑̂ 𝑜𝑥̂ 𝑖 between each mapped point 𝑥̂ 𝑖 and 𝐎 in LSS. Then, we compute the gap 𝑔𝑖 between 𝑑̂ 𝑜𝑥̂ 𝑖 and the maximum distance of the mapped points to 𝐎 as:

𝑔𝑖=max{𝐃𝐨}−𝑑̂ 𝑜𝑥̂ 𝑖,
(10)
where 𝐃𝐨={𝑑̂ 𝑜𝑥̂ 𝑖}𝑖=1,…,𝑛. After that, we measure the quantity of information provided by each mapped point 𝑥̂ 𝑖:

𝑄(𝑥̂ 𝑖)=−log𝑝(𝑥̂ 𝑖),
(11)
where 𝑝(𝑥̂ 𝑖) is the prior probability of each mapped point 𝑥̂ 𝑖 to correspond to an outlier, and it is computed by normalizing the gap:

𝑝(𝑥̂ 𝑖)=𝑔𝑖/∑𝑖=1𝑛𝑔𝑖.
(12)
The corresponding entropy is computed as:

𝐿=−∑𝑖=1𝑛𝑝(𝑥̂ 𝑖)log𝑝(𝑥̂ 𝑖).
(13)
We remove the mapped points with quantity of information lower than the estimated entropy value L and retain the points with quantity of information higher than the entropy value L:

𝐗̂ 𝐼={𝑥𝑖|𝑄(𝑥̂ 𝑖)>𝐿}.
(14)
To illustrate the outlier removal strategy, in Fig. 2, we show an example of outlier removal (based on our information theoretic approach) for homography estimation on the “Elderhalla” image pair. We can see that, inliers are assigned a large quantity of information while gross outliers are assigned a smaller quantity of information (see Fig. 2b). The proposed outlier removal strategy successfully distinguishes the mapped points in LSS (see Fig. 2c, d).

It is worth pointing out that, although Wang et al. (2012) and Wang et al. (2019) and the proposed outlier removal strategy in this paper, use information theory similar to that in Ferraz et al. (2007) to improve the fitting performance, they are significantly different in that: (1) They deal with different objects, i.e., the proposed strategy in this paper uses information theory to remove gross outliers from all data points while (Wang et al. 2012, 2019) use it to remove insignificant model hypotheses from all generated model hypotheses; (2) They work in different spaces, i.e., the proposed strategy works in LSS while (Wang et al. 2012, 2019) work in the parameter space.

Inlier Segmentation
The constructed latent semantic space is well approximated by multiple low-dimensional spaces. Thus, after removing the points corresponding to gross outliers (those with high probabilities in LSS), we propose to cluster the remaining mapped points, where each cluster corresponds to a model instance and the corresponding data points are the inliers of the model instance. From the distribution of the mapped points in LSS (see Fig. 1d), we can see that the points belonging to different clusters have obvious discrimination. The reason behind this is that, the inliers belonging to the same model instance, i.e., the same topic model of model hypotheses, will prefer the same directions in LSS. Thus, we can use a clustering method [e.g., K-means (MacQueen 1967), DBSCAN (Ester et al. 1996) and Clusterdp (Rodriguez and Laio 2014)] to cluster the remaining points in LSS. However, these clustering methods have some limitations, e.g., K-means is sensitive to the selection of the initial centers; for DBSCAN and Clusterdp, it is difficult to set their parameters for different input data.

In this paper, we follow the idea of K-means++ (Arthur and Vassilvitskii 2007), which improves K-means by employing a novel technique for seeding the initial centers. Specifically, we propose a new way of seeding the initial centers for K-means by introducing the preference function (see Eq. (1)). We measure the similarity between two points (𝑥̂ 𝑖, 𝑥̂ 𝑗) in LSS based on their Tanimoto distance (Tanimoto 1957):

𝑠(𝑥̂ 𝑖,𝑥̂ 𝑗)=1−⟨𝑃𝑆𝑃𝑆𝑖,𝑃𝑆𝑃𝑆𝑗⟩‖𝑃𝑆𝑃𝑆𝑖‖2+‖𝑃𝑆𝑃𝑆𝑗‖2−⟨𝑃𝑆𝑃𝑆𝑖,𝑃𝑆𝑃𝑆𝑗⟩,
(15)
where 𝑃𝑆𝑃𝑆𝑖 and 𝑃𝑆𝑃𝑆𝑗 are the preference sets of the data points 𝑥̂ 𝑖 and 𝑥̂ 𝑗 for all model hypothesesFootnote3. ⟨⋅,⋅⟩ and ‖⋅‖ denote the standard inner product and the corresponding induced norm, respectively.

Fig. 3
figure 3
Two examples of the distance distribution between an inlier and the other data points (i.e. the other inliers and the pseudo outliers) based on the Euclidean distance (Left) and the Tanimoto distance (Right) for two model fitting tasks

Full size image
Compared with the Euclidean distance, the Tanimoto distance is much more robust to the distribution of data points, for our case. Note that, if two data points share similar preference to model hypotheses, then their Tanimoto distance will be a small value; Otherwise, their Tanimoto distance will be a large value. In Fig. 3, we show two examples of the distance distribution between an inlier and the other data points (including the other inliers and the pseudo outliers) based on the Euclidean distance and the Tanimoto distance on the “6 lines” data and the “Unihouse” data, respectively. We can see that, many pseudo outliers have the same distance as inliers, when the Euclidean distance is used; while only a few pseudo outliers share the same distance with inliers, when the Tanimoto distance is used. Thus, the Tanimoto distance is able to more effectively capture the distance between inliers from the same model instance than the Euclidean distance, and we employ the Tanimoto distance, rather than the Euclidean distance, in the proposed fitting method.

Then, based on the similarity, we seed the centers from data points, according to their Tanimoto distance to the closest center that is chosen. We summarize the proposed clustering algorithm (called K-means*++) in Algorithm 1. The proposed K-means*++ is an improvement over K-means++ in terms of initial center selection. Compared with the standard K-means (MacQueen 1967), both K-means++ and K-means*++ are able to achieve more stable clustering results, due to their effectiveness for seeding the initial centers. However, it is worth pointing out that K-means*++ is able to achieve better clustering results than K-means++ for model fitting. This is because that K-means*++ uses the preference information to measure the similarity between data points, which is more effective than the Euclidean distance used by K-means++, for robust model fitting.

figure a
The Complete Method and Complexity Analysis
figure b
In this section, based on the components described in the previous sections, we summarize the complete Continuous Latent Semantic Analysis (CLSA) fitting method in Algorithm 2.

The proposed CLSA mainly includes three parts: the latent semantic space (LSS) construction, outlier removal, and inlier segmentation. CLSA firstly constructs LSS by both latent semantic analysis and continuous preference analysis, to map inliers of different model instances to several independent directions while gross outliers are mapped to the region close to 𝐎 in LSS. Then, CLSA analyzes the distribution of the mapped points in LSS and adaptively removes gross outliers by using information theory, which makes the model fitting problem easier to deal with. After that, CLSA labels the remaining points via an improved K-means*++ clustering algorithm, by which the parameters and inliers of model instances are estimated and segmented, respectively. As the experimental results show in Sect. 4, CLSA is able to efficiently and effectively deal with the multi-structure fitting problem.

For the computational complexity of the proposed CLSA, we mainly focus on improving both the effectiveness and computational efficiency of model selection for model fitting in this paper, and thus we do not consider the time for model hypothesis generation (i.e. Step 1). The preference matrix can be directly derived from the residual values between data points and model hypotheses (i.e. Step 2), thus its computational complexity is O(n), where n is the number of data points. For constructing the latent semantic space (i.e. Step 3), the computational complexity is approximately 𝑂(𝑛𝑟𝑘), where 𝑟𝑘 is the dimension of LSS, respectively. For outlier removal (i.e. Step 4), the computational complexity is O(n). For Algorithm 1, the computational complexity is 𝑂(𝑛̂ 𝑚), where 𝑛̂  is the number of estimated inliers. Thus the total complexity of CLSA approximately amounts to 𝑂(𝑛𝑟𝑘+𝑛̂ 𝑚).

Fig. 4
figure 4
The segmentation errors obtained by the proposed CLSA2 fitting method with different parameter values for line fitting based segmentation (a, d), homography based segmentation (b, e), and two-view based motion segmentation (c, f). The horizontal coordinate values of the black points in (d, e, f) denote the groundtruth number of model instances in data

Full size image
Experiments
In this section, we investigate the performance of the proposed CLSA fitting method, and compare it with several state-of-the-art model fitting methodsFootnote4, including the consensus analysis based fitting methods [i.e., MSHF (Wang et al. 2019) and RansaCov (Magri and Fusiello 2016)] and the preference analysis based fitting methods [i.e., T-linkage (Magri and Fusiello 2014) and RPA (Magri and Fusiello 2017)], on both synthetic data and real images. Moreover, to show the effectiveness of the proposed clustering algorithm, we test two versions of CLSA: CLSA1, which uses the traditional K-means algorithm (MacQueen 1967) to cluster points in LSS, and CLSA2, which uses the proposed K-means*++ algorithm to cluster points in LSS. We also run RCMSA (Pham et al. 2014) as a baseline for high-order fitting tasks (i.e., homography based segmentation and two-view based motion segmentation) due to its effectiveness.

All experiments are run on MS Windows 10 with Intel Core i7-3770 CPU 3.4GHz and 16GB RAM. The segmentation error (SE) is measured by the cost function in Eq. (2).

Fig. 5
figure 5
The segmentation errors obtained by the proposed CLSA2 fitting method with different sampling algorithms (i.e., RANSAC, MultiGS and Proximity) for the tasks of a line fitting based segmentation, b homography based segmentation and c two-view based motion segmentation

Full size image
Parameter Analysis and Settings
In this subsection, we analyze the influence of the parameter (i.e., the threshold 𝜓 in Eq. (1)) of the proposed CLSA2 fitting method on its performance. We test different values of the parameters for line fitting based segmentation, homography based segmentation and two-view based motion segmentation (for each task in Sect. 4.4,  4.5 and  4.6, we test three datasets, respectively), and show the segmentation errors obtained by CLSA2 with different parameter values in Fig. 4a–c.

We can see that, CLSA2 is abe to achieve low segmentation errors in the three fitting tasks for most cases. Specifically, when the value of the threshold 𝜓 (Fig. 4a–c) is respectively set to 0.04∼0.10 (for line fitting based segmentation), 0.02∼0.09 (for homography based segmentation) and 0.01∼0.04 (for two-view based motion segmentation), CLSA2 is able to achieve relatively stable results. Thus, we set the value of 𝜓 to 0.04 for all the following experiments.

We have also tested the proposed CLSA2 fitting method with different 𝑟𝑘 values in Eq (9) for line fitting based segmentation, homography based segmentation and two-view based motion segmentation. We show the segmentation errors obtained by CLSA2 with different 𝑟𝑘 values in Fig. 4d–f.

We can see that, when the 𝑟𝑘 value is less than the ground truth number of model instances in data, CLSA2 fails to fit model instances for all three fitting tasks. In contrast, when the 𝑟𝑘 value is equal to or larger than the ground truth number of model instances, CLSA2 is able to achieve low segmentation errors. This is because some inliers of different model instances are wrongly mapped into one direction, if the 𝑟𝑘 value is less than the ground truth number of model instances, which causes K-means*++ (used by CLSA2) to fail to correctly label these points. Thus, we set the 𝑟𝑘 value to a large value, which is obviously larger than the ground truth number of model instances for Eq. (9).

In this paper, we mainly focus on model selection. Thus, we provide the number of model instances to the proposed method as the competing methods (i.e., RansaCov, RPA and T-linkage) do. That is, 𝑟𝑘 is equal to k in Algorithm 1.

Fig. 6
figure 6
The segmentation errors obtained by and the CPU time used by the proposed CLSA2 fitting method with different numbers of model hypotheses for the tasks of line fitting based segmentation (a, d), homography based segmentation (b, e) and two-view based motion segmentation (c, f)

Full size image
Fig. 7
figure 7
The results obtained by the proposed methods (CLSA2) for line fitting based segmentation. The first row of each sub-figure shows the ground truth segmentation results and the second row shows the fitting and segmentation results obtained by CLSA2. The inlier noise scale is 1.5 and each line has 100 inliers. The outliers are labeled in blue, and the inliers of each estimated model instance are labeled in different colors (the following figures have the same settings)

Full size image
Table 1 Quantitative comparison results obtained by the six competing methods for line fitting based segmentation on four synthetic data
Full size table
The Influence of Sampling Algorithms
We also analyze the performance of CLSA2 with three popular sampling algorithms [i.e., RANSAC (Fischler and Bolles 1981), MultiGS (Chin et al. 2012) and Proximity (Kanazawa and Kawakami 2004)] for line fitting based segmentation, homography based segmentation and two-view based motion segmentation, respectively. For each task in Sects. 4.4,  4.5 and  4.6, we test three datasets, respectively, and we also show the segmentation errors obtained by CLSA2 in Fig. 5.

We can see that, for line fitting based segmentation, three versions of CLSA2 using all three sampling algorithms are able to achieve low segmentation errors (see Fig. 5a) compared with the other competing fitting methods. However, when using the random sampling algorithm (which is used in RANSAC), CLSA2 generates a high proportion of bad model hypotheses for high-order fitting tasks. For this case, CLSA2 achieves relatively higher segmentation errors than those achieved by CLSA2 using the guided sampling algorithms (see Fig. 5b, c). Among the three sampling algorithms, Proximity generates a similar quality of model hypotheses as MultiGS does, but it is much faster than MultiGS. Therefore, we select Proximity as the sampling algorithm for the proposed method and all the other competing methods in the following experiments.

The Influence of the Number of Model Hypotheses
In this subsection, we analyze the performance of CLSA2 with different numbers of model hypotheses for line fitting based segmentation, homography based segmentation and two-view based motion segmentation. For each task, we also test the three datasets, and we show the segmentation errors obtained by CLSA2 and the CPU time used by CLSA2 in Fig. 6.

We can see that, CLSA2 is able to achieve low segmentation errors even when we generate a small number of model hypotheses for all three fitting tasks (see Fig. 6a–c), and it also achieves stable fitting results (across different numbers of model hypotheses), which further shows the stable properties of CLSA2. For the CPU time, CLSA2 spends more time with increasing number of model hypotheses for the three fitting tasks (see Fig. 6d–f). This is because that the preference analysis becomes more complex when more model hypotheses are generated. Therefore, for CLSA2, we can generate a small number of model hypotheses for the three fitting tasks. However, to fairly compare with the other competing methods, we use the same setting of the number of model hypotheses, as that used in Wang et al. (2012) and Wang et al. (2019). That is, we generate 5, 000, 10, 000 and 20, 000 model hypotheses for line fitting based segmentation, homography based segmentation and two-view based motion segmentation, respectively. Moreover, all the competing methods select model instances based on the same model hypotheses.

Line Fitting Based Segmentation
We evaluate the performance of the six fitting methods for line fitting based segmentation on four challenging synthetic data. We repeat each experiment 50 times. We report the average segmentation errors (in the format of average errors±standard deviation) obtained by the six fitting methods, and the average CPU timeFootnote5 (in seconds) used by the competing methods in Table 1. We also show the fitting results obtained by the proposed method in Fig. 7.

From Fig. 7 and Table 1, we can see that the proposed CLSA2 fitting method achieves significantly better results than the other five competing fitting methods, with regard to the average segmentation errors. For the CPU time, CLSA2 achieves similar performance to CLSA1 but it is much faster than the other four fitting methods: CLSA2 is about 1.10 to 229.96 times faster than the other four fitting methods. CLSA1/CLSA2 are faster than the other four competing methods since they effectively reduce the high dimension of the preference matrix, by which they are able to greatly improve the computational efficiency. Moreover, they formulate the line fitting problem as a simpler clustering problem, which further improves the computational speed.

Table 2 Quantitative comparison results obtained by the seven competing methods for homography based segmentation on all 19 image pairs from the AdelaideRMF dataset. “k” is the actual number of model instances in each data and “%Outlier” is the percentage of gross outliers
Full size table
Fig. 8
figure 8
The results obtained by CLSA2 for homography based segmentation on some of the AdelaideRMF dataset (two views are shown for each case in (a, b), and only one of the two views is shown for each case in (c–l). The first row in (a, b) shows the original images. The first row in (c–l) shows the ground truth segmentation results, and the second row in (a–l) shows the fitting results obtained by CLSA2

Full size image
For the segmentation accuracy, MSHF, RansaCov and CLSA1 achieve similar low segmentation errors for all the four data. T-linkage and RPA achieve low segmentation errors for the “3 lines”, “4 lines” and “5 lines” data, but both may fail to fit the “6 lines” data among the 50 repetitions because they deal with the crossing model instances ineffectively (note that the “6 lines” data includes many crossing model instances). In contrast to CLSA2, CLSA1 is not very stable. This is because that K-means used by CLSA1 is sensitive to the initialization. Thus, CLSA1 is not able to achieve low standard deviations of error. In contrast, CLSA2 succeeds in fitting all the four data, and achieves the lowest average segmentation errors among the six competing fitting methods, for all the four data. Compared with CLSA1, CLSA2 is also very stable and achieves relatively lower standard deviations due to the effectiveness of the proposed K-means*++ (used by CLSA2).

Homography Based Segmentation
We evaluate the competing methods for homography based segmentation on the 19 real image pairs from the AdelaideRMF dataset (Wong et al. 2011)Footnote6 (the dataset contains 19 image pairs for homography based segmentation, and 19 image pairs for two-view based motion segmentation, which will be evaluated in Sect. 4.6). We repeat each experiment 50 times, and report the average results obtained by the seven competing methods in Table 2. We also show some fitting results obtained by CLSA2 for homography based segmentation in Fig. 8.

From Fig. 8 and Table 2, we can see that CLSA2 is able to achieve good results, obtaining the lowest average segmentation errors for 11 out of the 19 image pairs and obtaining the top three lowest segmentation errors for 7 out of the other 8 image pairs among all the seven competing fitting methods. Considering all the 19 image pairs, CLSA2 also achieves the lowest mean and median values of overall segmentation errors. It is worth pointing out that CLSA1/CLSA2 implement the process of model selection within one second for 16 out of the 19 image pairs, which is much less than the time used by the other five competing methods. In contrast, MSHF only succeeds in fitting 13 out of the 19 image pairs. RansaCov also achieves low segmentation errors for 11 out of the 19 image pairs. However, RansaCov is slower than MSHF for all image pairs and it produces a higher median value of overall segmentation errors than MSHF. This is because MSHF removes most of the ineffective model hypotheses before performing the process of model selection, while RansaCov considers all generated model hypotheses during the model selection procedure. Although T-linkage and RPA are preference analysis based fitting methods (like CLSA2), both are not able to achieve low segmentation errors (compared to CLSA2) for most of the image pairs. This is because that CLSA2 only keeps the principal preference information while removing the redundant information by latent semantic analysis. Moreover, CLSA2 uses an effective clustering algorithm (i.e., K-means*++) to label data points. Note that, CLSA1 achieves the same average segmentation errors as CLSA2 for 7 out of the 19 image pairs, but it produces higher average segmentation errors than CLSA2 for the other 12 image pairs, which further shows the effectiveness of the improved K-means*++ clustering algorithm. RCMSA outperforms the other six competing methods for the image pairs including a large number of model instances (i.e., the Unihouse, Bonhall and Johnsonb image pairs), but it fails in fitting the image pairs with unbalanced data (e.g., the Barrsmith, Elderhallb and Napierb image pairs).

Table 3 Quantitative comparison results obtained by the seven competing methods for two-view based motion segmentation on 19 image pairs from the AdelaideRMF dataset
Full size table
Fig. 9
figure 9
Some results obtained by CLSA2 for two-view based motion segmentation on the AdelaideRMF dataset (two views are shown for each case in (a–b), and only one of the two views is shown for each case in (c–l). The first row in (a, b) shows the original images. The first row in (c–l) shows the ground truth segmentation results, and the second row in (a–l) shows the fitting results obtained by CLSA2

Full size image
Two-View Based Motion Segmentation
For the task of two-view based motion segmentation, we use the other 19 image pairs of the AdelaideRMF dataset to evaluate the performance of the seven competing fitting methods. We show the average results in Table 3, and we also show some fitting results obtained by CLSA2 in Fig. 9.

From Fig. 9 and Table 3, we can see that RansaCov achieves high average segmentation errors and it fails to fit model instances for most cases. This is because that RansaCov considers all the generated model hypotheses, which include a large number of ineffective hypotheses. Note that the minimal subset for the two-view based motion segmentation task consists of 7 or 8 data points, and it is hard to sample an all-inlier minimal subset. Thus, it requires sampling a large number of minimal subsets to increase the probability of covering all model instances in data, but this will generate more ineffective model hypotheses as well. As the results show, the ineffective model hypotheses have significant influence on the fitting results obtained by RansaCov. Both T-linkage and RPA are able to achieve low segmentation errors for most of the image pairs, but their computational costs are relatively high, due to the use of the complicated clustering procedure. MSHF is able to achieve reasonably low average segmentation errors for 12 out of the 19 image pairs, and CLSA1 achieves reasonably low average segmentation errors for 8 out of the 19 image pairs. RCMSA is also able to achieve low average segmentation errors for most cases. Compared with the other competing methods, CLSA2 obtains the lowest segmentation errors for 16 out of the 19 image pairs and it obtains the lowest mean and median values of overall segmentation errors. In addition, CLSA2 is able to achieve very stable segmentation errors for all the 19 image pairs, when we repeat each experiment 50 times (note that each experiment uses different generated model hypotheses). This excellent performance mainly results from the steps of latent semantic space construction, outlier removal and inlier segmentation in CLSA2. Note that, CLSA1/CLSA2 are able to implement the process of model selection within 2 second for all the image pairs, which is much faster than the other five competing methods.

Discussion
The Comparison Between CLSA1 and CLSA2
In this paper, we proposed a basic approach with two identified variants. Both variants perform well - usually being the fastest and the most accurate, something uncommon in accuracy/cost tradeoffs. CLSA1 is the variant one would choose in general, if speed is a high priority, as it is usually faster and generally not so inferior in performance (still usually beating the competitors other than our slower variant). CLSA2 is one that would be recommended if speed is less of an issue and higher performance in segmentation has high priority.

Compared to the other competing methods, the proposed CLSA2 is more generalized, for handing the model fitting problem, and it only requires a few parameters (whose values do not significantly affect the accuracy of CLSA2). CLSA2 is also robust to different guided sampling algorithms and different numbers of the generated model hypotheses.

From the experimental results of line fitting based segmentation, we can see that CLSA2 is able to effectively deal with the data with different numbers of model instances, and a large number of outliers. From the experimental results of homography based segmentation and two-view based motion segmentation, CLSA2 also shows good performance compared to the other competing methods. Note that, CLSA2 shows significant advantages over the other competing methods for two-view based motion segmentation over homography based segmentation. One of the reasons is that, the task for two-view based motion segmentation is more challenging since it requires dealing with more ineffective model hypotheses. Moreover, we cannot ignore the fact that some fitting methods (e.g. MSHF) directly deal with the fitting task in the parameter space, and those methods are able to more effectively handle the data points located at the intersection of two structures. However, the fitting performance of those methods largely depends on the quality of the generated model hypotheses. Thus, the proposed CLSA2 achieves much better performance than those methods for two-view based motion segmentation.

Fig. 10
figure 10
An example showing a unbalanced data and the corresponding ratios of inliers and model hypotheses. a The minimum (in green) and maximum (in red) inlier cases of the “Bonhall” image pair. b The minimum and maximum ratios of inliers to all data points and the ratios of the generated model hypotheses corresponding to the two model instances in all generated model hypotheses

Full size image
Although the proposed CLSA2 significantly improves the performance of several state-of-the-art fitting methods on dealing with unbalanced data (that is, the numbers of inliers belonging to different model instances are unbalanced), it may not be effective to deal with heavily unbalanced data. We show an example of a unbalanced data in Fig. 10. We can see that, the minimum and maximum ratios of inliers belonging different model instances to all data points for the “Bonhall” image pair are significantly different, and the ratios of the generated model hypotheses corresponding to the two model instances to all model hypotheses are much more different. In this case, the preference function (used in CLSA2) is not effective for the inliers of the model instance with the minimum inlier ratio. As a result, the proposed CLSA2 can not fit this data very well. However, all of the other competing fitting methods also have this limitation. Note that, sampling non-minimal subsets may help alleviate the unbalance of the generated model hypotheses, as discussed in Tran et al. (2014). However, it is still a very challenging problem.

Fig. 11
figure 11
Quantitative comparisons of different versions of the proposed method for line fitting based segmentation. In each subfigure, we show the segmentation errors obtained by and the CPU time used by the proposed methods with respect to the cumulative distribution. A point on the curve with coordinate (x, y) represents that there are 100×𝑥 percents of image pairs which have values no more than y

Full size image
The Influence of Different Components of the Proposed Method
It is worth pointing that, in this paper, we argue that the proposed CLSA fitting method with the combination of latent semantic analysis (LSA) and continuous preference analysis (CPA) is an important improvement over that using either LSA or CPA. To verify this, we evaluate three different versions of CLSA, i.e., CLSA with LSA and CPA, CLSA+BPA with LSA and binary preference analysis (BPA), and CLSA+t-SNE with CPA and t-SNE (Maaten and Hinton 2008), for line fitting based segmentation, homography based segmentation and two-view based motion segmentation. We also test different outlier removal strategies (i.e., 1D Mixture Gaussian Model (Chin et al. 2009), RFM (Jiang et al. 2020) and LPM (Ma et al. 2019)), for the proposed method with CLSA+Gaussian, CLSA+RFM and CLSA+LPM, respectively (here, RFM and LPM only work for the two-view model fitting problem, thus, we test CLSA+RFM and CLSA+LPM only for homography based segmentation and two-view based motion segmentation). In addition, to compare the Tanimoto distance, we test a version (i.e. CLSA+Eucldean) with the Eucldean distance. To test different qualities of preference matrices, we generate 20 groups of model hypotheses for all versions of CLSA. We show the quantitative comparisons of different versions of CLSA in Figs. 11, 12 and 13 for line fitting based segmentation, homography based segmentation and two-view based motion segmentation, respectively.

Fig. 12
figure 12
Quantitative comparisons of different versions of the proposed method for homography based segmentation. In each subfigure, we show the segmentation errors obtained by and the CPU time used by the proposed methods with respect to the cumulative distribution

Full size image
Fig. 13
figure 13
Quantitative comparisons of different versions of the proposed method for two-view based motion segmentation. In each subfigure, we show the segmentation errors obtained by and the CPU time used by the proposed methods with respect to the cumulative distribution

Full size image
From Figs. 11, 12 and 13, we can see that, CLSA is able to achieve better performance on the segmentation error than CLSA+BPA for most of the datasets, especially for the task of line fitting based segmentation (see Fig. 11), where the data include a large number of outliers (and they use the similar CPU time for all datasets). This shows the effectiveness of CPA for model fitting. For CLSA and CLSA+t-SNE, which deal with the preference matrix using LSA and t-SNE, respectively, CLSA is able to achieve lower segmentation errors with much less CPU time than CLSA+t-SNE, for all the three tasks (see Figs. 11, 12 and 13). This shows the effectiveness of LSA for model fitting. For different outlier removal strategies, CLSA achieves lower segmentation errors than CLSA+Gaussian, CLSA+RFM and CLSA+LPM for all the three tasks, while they spend the similar CPU time. This further shows the effectiveness of our proposed outlier removal strategy for model fitting. For different distance measures, CLSA is able to show the superiority over CLSA+Eucldean on the performance of the segmentation error for all the three tasks, while they also spend the similar CPU time. Thus, this shows the effectiveness of the Tanimoto distance for model fitting. For different qualities of preference matrices, CLSA is able to achieve stable fitting results for all the three tasks.

Fig. 14
figure 14
An example of estimating the number of model instances on an image pair. a The over-segmented result when we input 𝑘=10. b The segmentation result after estimating the number of model instances by using the proposed solution

Full size image
The Estimation of the Number of Model Instances
The estimation of the number of model instances in data is also an important and challenging task for model fitting. We note that if two model hypotheses correspond to the same model instance, they will share most of their inliers. Thus, we provide a promising solution for adaptively estimating the number of model instances in data by analyzing the inlier distribution and the quality of the estimated model instances. Specifically, we set the value of k in Algorithm 2 to be a reasonable large number so that the maximum number of model instances in data is less than k. In this way, it is much easier to set the k value than to require an user to input the truth number of model instances in data. After obtaining k model hypothesis candidates by Algorithm 2 (note that these model hypothesis candidates are significant model hypotheses since most of outliers have been removed in Step 4 of Algorithm 2, but the result may be over-segmented as k is usually larger than the true number of model instances in data), we compute the ratio of the common inliers shared by any two model hypothesis candidates, and group the two model hypothesis candidates into one cluster if the ratio of the common inliers shared by the two model hypothesis candidates is high (which means they are more likely to correspond to the same model instance). This process will fuse the model hypothesis candidates corresponding to the same model instance. Then, we estimate the number of model instances by counting the number of the model hypothesis groups. In Fig. 14, we show an example for the process of estimating the number of model instances. We can see that, the result in Fig. 14a is over-segmented when we input 𝑘=10 (that is, we assume that the maximum number of model instances in the data is less than 10, which can be satisfied in most common cases). In contrast, we obtain the good segmentation result in Fig. 14b, where we keep the model hypothesis candidate that has the best quality [measured by weighting scores (Wang et al. 2012)] in each cluster as the parameter estimation of the corresponding model instance, after estimating the number of model instances by the proposed solution.

Conclusion
In this paper, we propose a continuous latent semantic analysis based fitting method (called CLSA), which exploits the advantages of both latent semantic analysis (LSA) and continues preference analysis (CPA). LSA is able to significantly increase the efficiency and reduce the computational cost of the proposed CLSA method. CPA can be used by CLSA to effectively represent the relationship between model hypotheses and data points. By exploiting the advantages of both LSA and CPA, we construct a novel latent semantic space (LSS), where we map gross outliers close to the origin of LSS, while mapping inliers of different model instances to several independent directions in LSS. After that, we employ an information theoretic approach to remove the points corresponding to gross outliers in LSS. Then we treat the complex model fitting problem as a simple clustering problem (which is much easier to be dealt with because the data points corresponding to gross outliers are removed). We also propose an effective but simple clustering algorithm for inlier segmentation. Extensive experimental results show that the proposed CLSA method is able to achieve better performance than several state-of-the-art fitting methods on both segmentation accuracy and computational efficiency for different model fitting tasks.

