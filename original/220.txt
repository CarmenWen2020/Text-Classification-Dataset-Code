In this paper, a multichannel EEG emotion recognition method based on a novel dynamical graph convolutional neural networks (DGCNN) is proposed. The basic idea of the proposed EEG emotion recognition method is to use a graph to model the multichannel EEG features and then perform EEG emotion classification based on this model. Different from the traditional graph convolutional neural networks (GCNN) methods, the proposed DGCNN method can dynamically learn the intrinsic relationship between different electroencephalogram (EEG) channels, represented by an adjacency matrix, via training a neural network so as to benefit for more discriminative EEG feature extraction. Then, the learned adjacency matrix is used to learn more discriminative features for improving the EEG emotion recognition. We conduct extensive experiments on the SJTU emotion EEG dataset (SEED) and DREAMER dataset. The experimental results demonstrate that the proposed method achieves better recognition performance than the state-of-the-art methods, in which the average recognition accuracy of 90.4 percent is achieved for subject dependent experiment while 79.95 percent for subject independent cross-validation one on the SEED database, and the average accuracies of 86.23, 84.54 and 85.02 percent are respectively obtained for valence, arousal and dominance classifications on the DREAMER database.

SECTION 1Introduction
Emotion recognition plays an important role in the human-machine interaction [1], which enables machine to perceive the emotional states of human beings so as to make machine more ‘sympathetic’ in the human-machine interaction. Basically, emotion recognition methods can be divided into two categories. The first one is based on non-physiological signals, such as facial expression images [2], [3], [4], [5], [6], body gesture [7], and voice signal [8]. The second one is based on physiological signal, such as electroencephalogram (EEG) [9], electromyogram (EMG) [10], and electrocardiogram (ECG) [11]. Among the various types of physiological signals, EEG signal is one of the most commonly used ones, which is directly captured from the brain cortex and hence it would be advantageous to reflect the mental states of human beings. With the rapid development of dry EEG electrode techniques and the EEG signal processing methods, EEG emotion recognition has received more and more attentions in recent years [12], [13], [14], [15].

Basically, there are two major ways to describe human's emotions [16], i.e., the discrete basic emotion description approach and the dimension approach. For the discrete basic emotion description approach, the emotions are classified into a set of discrete status, e.g., the six basic emotions (i.e., joy, sadness, surprise, fear, anger, and disgust) [17]. Different from the discrete emotion description approach, the dimension approach describes emotions in continuous form, in which the emotions are characterized by three dimensions (valence, arousal and dominance) [18], [19] or simply two dimensions (valence and arousal), in which the valence dimension mainly characterizes how positive or negative the emotions are, whereas the arousal dimension aims to characterize the degree of how excited or apathetic the emotions are [16].

The research of applying EEG signal to the emotion recognition can be traced back to work of Musha et al. in [20]. During the past decades, many machine learning and signal processing methods are proposed to deal with the EEG emotion recognition [21], [22]. A typical EEG emotion recognition method usually consists of two major parts, i.e., discriminative EEG feature extraction and emotion classification. Basically, the EEG features used for emotion recognition can be generally divided into two kinds, i.e., time-domain feature type and frequency-domain feature type. The time domain features, e.g., Hjorth feature [23], fractal dimension feature [24] and higher order crossing feature [25], mainly capture the temporal information of EEG signals. Different from the time-domain feature, the frequency-domain feature aims to capture the EEG emotion information from the frequency point of view. One of the most commonly used frequency-domain feature extraction methods is to decompose the EEG signal into several frequency bands, e.g., δ band (1-3 Hz), θ band (4-7 Hz), α band (8-13 Hz), β band (14-30 Hz) and γ band (31-50 Hz) [21], [26], [27], [28], and then extract EEG features from each frequency band, respectively. The commonly used EEG features include the differential entropy (DE) feature [29], [30], the power spectral density (PSD) feature [31], the differential asymmetry (DASM) feature [24], the rational asymmetry (RASM) feature [32] and the differential caudality (DCAU) feature [19].

To deal with EEG emotion classification problem, there are many methods appeared in the literatures [33], among which the method of using deep neural networks (DNN) [19] had been demonstrated to be one of the most successful one. Convolutional neural networks (CNN) is one of the most famous DNN approaches and had been widely used to cope with various classification problems, such as image classification [34], [35], [36], [37], object detection [38], tracking [39] and segmentation [40]. Although CNN model had been demonstrated to be very powerful in dealing with the classification problems, it is notable that previous applications of CNN focus more on the local feature learning from image, video and speech, in which the data points of the signal are continuously changed. For another feature learning problems, such as the feature learning from transportation network and brain network, the traditional CNN method may not be well suitable because the signals are discrete and discontinuous in the spatial domain. In this case, graph based description methods [41], [42] would provide a more appropriate way.

Graph neural network (GNN) [43] aims to build the neural networks under the graph theory to cope with the data in graph domain. Graph convolutional neural network (GCNN) [44] is an extension of the traditional CNN method by combining CNN with spectral theory [45]. Compared with classical CNN method, GCNN would be more advantageous in dealing with the discriminative feature extraction of signals in the discrete spatial domain [46]. More importantly, the GCNN method provides an effective way to describe the intrinsic relationship between different nodes of the graph, which would provide a potential way to explore the relationships among the multiple EEG channels during the EEG emotion recognition.

Motivated by the success of the GCNN model, in this paper we will investigate the multichannel EEG emotion recognition problem using graph representation approach, in which each EEG channel corresponds to a vertex node whereas the connection between two different vertex nodes corresponds to an edge of the graph. Although GCNN can be used to describe the connections among different nodes according to their spatial positions, we should predetermine the connections among the various EEG channels before applying it to building the emotion recognition model. On the other hand, it is notable that the spatial position connections among the EEG channels are different from the functional connections among them. In other words, a closer spatial relationship may not guarantee a closer functional relationship, whereas the functional relationship would be useful for the discriminative EEG feature extraction in emotion recognition. Consequently, it is not reasonable to predetermine the connections of the graph nodes according to their spatial positions.

To alleviate the limitations of the GCNN method, in this paper we propose a novel dynamical graph convolutional neural networks (DGCNN) model for learning discriminative EEG features as well as the intrinsic relationship, e.g., the functional relationship, among the various EEG channels. Specifically, to learn the relationships among the various EEG channels, we propose a novel method to construct the connections among the various vertex nodes of the graph by learning an adjacency matrix. However, different from the traditional GCNN method that predetermines the adjacency matrix before the model training, the proposed DGCNN method learns the adjacency matrix in a dynamic way, i.e., the entries of the adjacency matrix are adaptively updated with the changes of graph model parameters during the model training. Consequently, in contrast to the GCNN method, the adjacency matrix learned by the DGCNN would be more useful because it captures the intrinsic connections of the EEG channels and hence it would be able to improve the discriminant abilities of the networks.

The remainder of this paper is organized as follows: In Section 2, we briefly review the preliminaries of graph theory. In Section 3, we propose the DGCNN model and the EEG emotion recognition method based on this model. Extensive experiments are conducted in Section 4. Finally, we conclude the paper in Section 5.

SECTION 2Graph Preliminary
In this section, we introduce some preliminary knowledge about the graph representation and the spectral graph filtering, which are the basis for our DGCNN method.

2.1 Graph Representation
A directed and connected graph can be defined as G={V,E,W}, in which V represents the set of nodes with the number of |V|=N and E denotes the set of edges connecting these nodes. Let W∈RN×N denote an adjacency matrix describing the connections between any two nodes in V, in which the entry of W in the ith row and jth column, denoted by wij, measures the importance of the connection between the ith node and the jth one. Fig. 1 illustrates an example of a graph containing six vertex nodes and the edges connecting the nodes of the graph, as well as the adjacency matrix associated with the graph, where the different color arrows in the left-hand side of the figure denote the edges connecting the source nodes to destination nodes, whereas the right-hand side of the figure is the illustration of the corresponding adjacency matrix.


Fig. 1.
Example of a directed graph and the corresponding adjacency matrix, where the left part is the connections of six nodes and right part is the adjacency matrix.

Show All

The commonly used methods to determine the entries wij of the adjacency matrix W include the distance function method [47] and K-nearest neighbor (KNN) rule method [48]. A typical distance function would be the Gaussian kernel function, which can be expressed as
wij=⎧⎩⎨⎪⎪exp(−[dist(i,j)]22θ2),0,if dist(i,j)≤τotherwise ,(1)
View Sourcewhere τ and θ are two fixed parameters and dist(i,j) denotes the distance between the ith node and the jth one.

2.2 Spectral Graph Filtering
The spectral graph theory has been successfully used for building expander graphs [49], spectral clustering [50], graph visualization [51] and other applications [52]. Spectral graph filtering, also called graph convolution, is a popular signal processing method for graph data operation, in which Graph Fourier Transform (GFT) [47] is a typical example.

Let L denote the Laplacian matrix of the graph G. Then, L can be expressed as
L=D−W∈RN×N,(2)
View Sourcewhere D∈RN×N is a diagonal matrix and the ith diagonal element can be calculated by Dii=∑jwij.

For a given spatial signal x∈RN, its GFT is expressed as follows:
x^=UTx,(3)
View SourceRight-click on figure for MathML and additional features.where x^ denotes the transformed signal in the frequency domain, U is an orthonormal matrix that can be obtained via the singular value decomposition (SVD) of the graph Laplacian matrix L [45]
L=UΛUT,(4)
View SourceRight-click on figure for MathML and additional features.in which the columns of U=[u0,…,uN−1]∈RN×N constitute the Fourier basis, and Λ=diag([λ0,…,λN−1]) is a diagonal matrix.

From (3), the inverse of GFT can be expressed as the following form:
x=UUTx=Ux^.(5)
View SourceThen, the convolution of two signals x and y on the graph, denoted by x∗Gy, can be expressed as [44]
x∗Gy=U((UTx)⊙(UTy)),(6)
View Sourcewhere ⊙ denotes the element-wise Hadamard product.

Now let g(⋅) be a filtering function and then a signal x filtered by g(L) can be expressed as
y=g(L)x=g(UΛUT)x=Ug(Λ)UTx,(7)
View Sourcewhere g(Λ) is expressed as
g(Λ)=⎡⎣⎢⎢g(λ0)⋮0⋯⋱⋯0⋮g(λN−1)⎤⎦⎥⎥.(8)
View SourceIt is notable that the filtering operation of (7) is equivalent to the graph convolution of the signal x with the vector of Udiag(g(Λ)) due to the following formulation:
y=g(L)x=Ug(Λ)UTx             =U(diag(g(Λ))⊙(UTx))             =U((UT(Udiag(g(Λ)))⊙(UTx))             =x∗G(Udiag(g(Λ))).(9)
View SourceRight-click on figure for MathML and additional features.

SECTION 3DGCNN for EEG Emotion Recognition
In this section, we first propose the DGCNN model and then apply it to the EEG emotion recognition problem, in which the adjacency matrix W that characterizes the relationships of the various vertex nodes is dynamically learned instead of being predetermined [44].

3.1 DGCNN Model for EEG Emotion Recognition
Let W∗ denote the optimal adjacency matrix to be learned. Then, the graph convolution of the signal x with the vector of U∗diag(g(Λ∗)) defined by the spatial filtering g(L∗) can be expressed as
y=g(L∗)x=U∗g(Λ∗)U∗Tx,(10)
View SourceRight-click on figure for MathML and additional features.where the L∗ can be calculated from W∗ based on (2), and Λ∗=diag([λ∗0,…,λ∗N−1]) is a diagonal matrix.

Since it is difficult to directly calculate the expression of g(Λ∗), we simplify this calculation by using the K order Chebyshev polynomials [44] to replace the polynomial expansion of g(Λ∗), such that the calculation becomes much easier and faster. Specifically, let λ∗max denote the largest element among the diagonal entries of Λ∗ and Λ~∗ denote the normalized Λ∗, i.e., Λ~∗=2Λ∗/λ∗max−IN, such that the diagonal elements of Λ~∗ lie in the interval of [−1,1], where IN is the N×N identity matrix.

Under the K order Chebyshev polynomials framework, we obtain that g(Λ∗) can be approximated by
g(Λ∗)=∑k=0K−1θkTk(Λ~∗),(11)
View SourceRight-click on figure for MathML and additional features.where θk is the coefficient of Chebyshev polynomials, and Tk(x) can be recursively calculated according to the following recursive expressions:
{T0(x)=1, T1(x)=x,Tk(x)=2xTk−1(x)−Tk−2(x),k≥2.(12)
View SourceRight-click on figure for MathML and additional features.

According to (11), we obtain that the graph convolution operation defined in (10) can be rewritten by
y=U∗g(Λ∗)U∗Tx=∑k=0K−1U∗⎡⎣⎢⎢⎢θkTk(λ~∗0)⋮0⋯⋱⋯0⋮θkTk(λ~∗N−1)⎤⎦⎥⎥⎥U∗Tx=∑k=0K−1θkTk(L~∗)x,(13)
View Sourcewhere L~∗=2L∗/λ∗max−IN.

The expression of (13) means that calculating the graph convolution of x can be expressed as the combination of the convolutional results of x with each of the Chebyshev polynomial components. Based on the expression of (13), we propose the DGCNN model for EEG emotion recognition. The framework of the proposed DGCNN model is illustrated in Fig. 2, which consists of four major layers, i.e., the dynamical graph convolutional layer, the 1 × 1 convolutional layer, the Relu activation layer, and the full connection layer.


Fig. 2.
The framework of the DGCNN model for EEG emotion recognition, which consists of the dynamical graph convolutional operation via the learned graph connections, convolution layer with 1×1 kernel, Relu activation and the full connection. The inputs of the model are the EEG features extracted from multiple frequency bands, e.g., five frequency bands (δ band, θ band, α band, β band, and γ band), in which each EEG channel is represented as a node of the graph. The outputs are the predicted labels through softmax.

Show All

Specifically, the input of the DGCNN model corresponds to the EEG features extracted from multiple frequency bands, e.g., five frequency bands (δ band, θ band, α band, β band, and γ band), in which each EEG channel is represented as a node in the DGCNN model. Following the graph filtering operation is a 1×1 convolution layer, which aims to learn the discriminative features among the various frequency domains. Moreover, to realize the nonlinear mapping capability of the network, the Relu activation function [53] is adopted to ensure that the outputs of the graph filtering layer are non-negative. Finally, the outputs of activation function are further sent to a multi-layer full connection network and a softmax function is also used to predict the desired class label information of the input EEG features.

3.2 Algorithm for DGCNN
To optimize the optimal network parameters, we adopt the back propagation (BP) method to iteratively update the network parameters until the optimal or suboptimal solutions are achieved. For this purpose, we define a loss function based on cross entropy cost, which is expressed as the following form:
Loss=cross_entropy(l,lp)+α∥Θ∥,(14)
View Sourcewhere l and lp denote the actual label vector of training data and the predicted one, respectively, Θ denotes all the model parameters and α is the trade-off regularization weight. The cross entropy function cross_entropy(l,lp) aims at measuring the dissimilarity between the actual emotional labels and the desired ones while the regularization α∥Θ∥ aims to prevent over-fitting of the model parameters learning.

When applying the BP method to dynamically learn the optimal adjacency matrix W∗ of the DGCNN model, we have to calculate the partial derivative of the loss function with respect to W∗, which is formulated as
∂Loss∂W∗=⎛⎝⎜⎜⎜⎜∂Loss∂w∗11⋮∂Loss∂w∗N1⋯⋮⋯∂Loss∂w∗1N⋮∂Loss∂w∗NN⎞⎠⎟⎟⎟⎟,(15)
View Sourcewhere w∗ij denotes the ith row and j-th column element of W∗. According to the chain rule, the calculation of ∂Loss∂w∗ij can be expressed as
∂Loss∂w∗ij=∂cross_entropy(l,lp)∂L~∗⋅∂L~∗∂w∗ij+α∂∥Θ∥∂w∗ij.(16)
View Source

After calculating the partial derivative of ∂Loss∂W∗, we can use the following rule to update the optimal adjacency matrix W∗:
W∗=(1−ρ)W∗+ρ∂Loss∂W∗,
View Sourcewhere ρ denotes the learning rate of the network.

Algorithm 1 summarizes the detailed procedures of training the DGCNN model in EEG emotion recognition.

Algorithm 1. Procedures of Training Optimal DGCNN Model for EEG Emotion Recognition
Require: Multichannel EEG features associated with multiple frequency bands, the class labels corresponding to the EEG features, the number of Chebyshev polynomial order K, the learning rate ρ;

Ensure: The desired adjacency matrix W∗ and model parameters of DGCNN;

Initialize the adjacency matrix W∗ and other model parameters;

repeat

Regularizing the elements of the matrix W∗ using Relu operation such that the elements are non-negative;

Calculating the Laplacian matrix L∗;

Calculating the normalized Laplacian matrix L~∗;

Calculating the Chebyshev polynomial items Tk(L~∗) (k=0,1,…,K−1);

Calculating ∑K−1k=0θkTk(L~∗)x;

Calculating the 1×1 convolution results and regularizing the results using the Relu operation;

Calculating the results of the full connection layer;

Calculating the loss function using (14);

Updating the adjacency matrix
W∗=(1−ρ)W∗+ρ∂Loss∂W∗
View SourceRight-click on figure for MathML and additional features.

and other model parameters;

until the iterations satisfy the predefined algorithm convergence condition.


SECTION 4Experiments
In this section, we conduct extensive experiments on two emotional EEG databases that are commonly used in EEG emotion recognition to evaluate the effectiveness of the proposed GDCNN method. The first one is the SJTU Emotion EEG Database (SEED) [19] while the second one is the DREAMER [54].

4.1 Emotional EEG Databases
The SEED database contains EEG data of 15 subjects (7 males and 8 females), which are collected via 62 EEG electrodes from the subjects when they are watching fifteen Chinese film clips with three types of emotions, i.e., negative, positive and neutral. To avoid making subjects fatigue, the whole experiment will not last for a long time and the duration of each film clip is about 4 minutes. The stimulus materials can be understood without explanation. Consequently, all the EEG signals will be categorized into one of three kinds of emotion states (positive, neutral and negative). The data collection lasted for 3 different periods corresponding to 3 sessions, and each session corresponds to 15 trials of EEG data such that there are totally 45 trials of EEG data for each subject. In addition, an additional subjective self-assessment for each subject is also carried out after the subjects watching the film clips in order to guarantee that the collected EEG data share the same emotion states as the film clips presented to the subjects.

The DREAMER database consists of EEG data via 14 EEG electrodes of 23 subjects (14 males and 9 females). To build this database, 18 film clips are used for eliciting 9 different emotions, i.e., amusement, excitement, happiness, calmness, anger, disgust, fear, sadness and surprise. Each film clip lasts for a period time between 65 to 393s, which is thought to be sufficient for eliciting single emotions. The data collection begins with a neutral film clip watching to help the subjects return to the the neutral emotion state in each new trial of data collection and also to serve as the baseline signals. After watching a film clip, the self-assessment manikins (SAM) were used to acquire subjective assessments of valence, arousal and dominance. Finally, all the EEG signals are assigned with three binary states (low/high valence, low/high arousal and low/high dominance).

4.2 EEG Emotion Recognition Experiments on SEED Database
In this part, we conduct two kinds of experiments to evaluate the EEG emotion recognition performance using the proposed DGCNN method. The first kind of experiment is subject-dependent whereas the second one is subject-independent.

4.2.1 Subject-Dependent Experiments on SEED Database
In subject-dependent experiments, we adopt the same experimental protocol as that of [19] to evaluate the proposed DGCNN method. Specifically, for all the 15 trials of EEG data associated with one session of one subject, the first 9 trials of EEG data are used to serve as the training set and remaining 6 ones as the testing set. Then, the recognition accuracy corresponding to each period is obtained for each subject. Finally, the average classification accuracy and standard deviation over two sessions of all the 15 subjects are calculated.

We investigate five kinds of features to evaluate the proposed EEG emotion recognition method, i.e., the differential entropy feature, the power spectral density feature, the differential asymmetry feature, the rational asymmetry feature, and the differential caudality feature. The features are respectively extracted in each of the frequency bands (δ band, θ band, α band, β band, and γ band). Moreover, to extract the EEG feature, each trial of EEG signal flow is partitioned into a set of blocks, where each block contains 1s of EEG signals. In this case, we can extract five kinds of features from each block. The number of EEG features extracted from each frequency band corresponding to the various feature types are summarized in Table 1.

TABLE 1 Number of the Five Types of EEG Features Extracted from Each Frequency Band on SEED Database

Based on the EEG features shown in Table 1, we have trained the DGCNN model shown in Fig. 2 for the EEG emotion recognition. In this case, each vertex node of the graph in the DGCNN model is associated with five EEG features corresponding to the five frequency bands. It is notable that the number of vertex nodes in the graph would be different for different feature types. Specifically, for both PSD and DE features, the number of vertex nodes in the graph is 62 whereas it would be 27 nodes for both DASM and RASM because there are only 27 features for these two feature types. Similarly, the number of vertex nodes in the graph would be 23 for DCAU feature type.

Table 2 summarizes experimental results in terms of the average EEG emotion recognition accuracy and the standard deviation of the DGCNN method under the five different EEG feature types (PSD, DE, DASM, RASM and DCAU) and the five different frequency bands, where the results annotated by “all” mean that the EEG features associated with all of the five frequency bands are combined together in the experiments such that each vertex node of the graph is associated with five EEG features. For the comparison purpose, we also include the experimental results of [19] with deep belief networks (DBN) [55] and support vector machine (SVM) [56] in Table 2. Moreover, we also conduct the same experiments using the GCNN method to serve as a baseline method to evaluate the performance of DGCNN. Here it should be noted that, for the GCNN model, the elements of the adjacency matrix are predetermined according to the spatial relationship of the EEG channels (Fig. 3 illustrates the spatial relationship of the 62 EEG channels used to construct the adjacency matrix for the GCNN model, i.e., there would be a direct connection between two EEG channels if they are closely related.).


Fig. 3.
Illustration of the connections among the 62 EEG channels, which is used for constructing the adjacency matrix of GCNN.

Show All

TABLE 2 Comparisons of the Average Accuracies and Standard Deviations (%%) of Subject Dependent EEG-Based Emotion Recognition Experiments on SEED Database Among the Various Methods

From Table 2, we can observe the following major points:

Among the five kinds of EEG features, the DE feature was demonstrated to be better than most of the other ones in terms of the average recognition accuracy. For each emotion recognition method, the best average recognition accuracy was achieved when all the five frequency bands were used together. Especially, the best average recognition accuracy of DE feature was as high as 90.4 percent (with standard deviation of 8.49 percent) when all the five frequency bands were used.

Among the four EEG emotion recognition methods, both DGCNN and GCNN achieve better average recognition accuracies than the other two methods (SVM and DBN). Compared with GCNN, however, DGCNN demonstrates to be more powerful in classifying the EEG emotion classes in most cases when the same EEG feature is used. This is most likely due to the optimization for determining the entries of the adjacency matrix in the GCNN model, which makes it more accurate to characterize the relationship between the various EEG channels.

Among the five EEG frequency bands, both β and γ frequency bands achieve better recognition results than the other three ones in most cases, which indicates that the higher frequency bands may be more closely related with the emotion activities whereas the lower frequency bands are less related with the emotion activities.

4.2.2 Subject-Independent Experiments on SEED
In the subject-independent experiments, we adopt the leave-one-subject-out (LOSO) cross-validation strategy to evaluate the EEG emotion recognition performance of the proposed DGCNN method. Specifically, in LOSO cross-validation experimental protocol, the EEG data of 14 subjects are used for training the model and the EEG data of the rest one subject is used as testing data. The experiments are repeated such that the EEG data of each subject are used once as the testing data. The average classification accuracies and standard deviations corresponding to the five kinds of EEG features are respectively calculated.

Table 3 summarizes the experimental results in terms of the average EEG emotion recognition accuracy and the standard deviation of DGCNN under the different kinds of EEG features and the different frequency bands. From Table 3, we obtain the following major points:

For each kind of feature, the recognition accuracies associated with higher frequency bands are better than the ones of lower frequency bands.

TABLE 3 The Average Accuracies and Standard Deviations (%%) of Subject Independent LOSO Cross Validation EEG-Based Emotion Recognition Experiments on SEED Database Using DGCNN
Table 3- 
The Average Accuracies and Standard Deviations ($\%$%) of Subject Independent LOSO Cross Validation EEG-Based Emotion Recognition Experiments on SEED Database Using DGCNN
For each kind of feature, the best recognition accuracy is obtained when combining the features extracted from the five frequency bands together for emotion recognition. Especially, when the five frequency bands are used and DE feature is adopted, the best recognition accuracy (=79.95%) is achieved.

Considering that the subject-independent emotion recognition task can be seen as a cross-domain emotion recognition problem, we adopt several popular cross-domain recognition methods, including transfer component analysis (TCA) [57], KPCA [58], transductive SVM (T-SVM) [59] and transductive parameter transfer (TPT) [60], to serve as the baseline methods. Since the use of the DE features combined with the five frequency bands (δ, θ, α, β, and γ) had been demonstrated to be the most effective features in the EEG emotion recognition, we adopt these features to compare the EEG emotion recognition performance among the five methods. Fig. 4 summarizes the experimental results. From the results of Fig. 4, we can see that the proposed DGCNN method achieves the highest recognition accuracy (=79.95%) among the five methods. In addition, we can also see that the standard deviation (=9.02%) of the proposed DGCNN is much lower than that of TCA, KPCA, T-SVM, and TPT, which indicates that DGCNN is much more stable compared with the other four methods.


Fig. 4.
Comparisons of the EEG emotion recognition accuracies and standard deviations among TCA, KPCA, T-SVM, TPT, and DGCNN on SEED database.

Show All

4.3 EEG Emotion Recognition Experiments on DREAMER Database
In this part, we conducted experiments on the DREAMER database to evaluate the EEG emotion recognition performance using the proposed DGCNN method. Before the experiments, we adopted the same feature extraction method as that of [54] to extract a set of PSD features. During the feature extraction, we first cropped the EEG signals corresponding to the last 60 seconds of each film clip and then decomposed the signals into θ (4-8 Hz), α (8-13 Hz), and β (13-20 Hz) frequency bands. For each frequency band, the 60s EEG signals are further segmented into a set of 59 blocks by sliding a window with the size of 256 EEG points, in which there are half overlap between two subsequent blocks. Finally, the PSD features were calculated from the EEG signal of each block. As a result, we obtain 14 features associated with the 14 EEG channels in total from each block and we concatenate them into a 14-dimensional feature vector to represent a EEG data sample. In this way, we totally obtain 59 EEG data samples associated with each frequency band from each session, which are used for our EEG emotion recognition evaluation of the DGCNN method. Table 4 shows the information about the EEG features we extracted from the DREAMER database.

TABLE 4 Number of the PSD EEG Features of Each EEG Sample Associated with Each Frequency Band on DREAMER Database

To evaluate the EEG emotion recognition performance of the proposed DGCNN method, we adopt the subject-dependent leave-one-session-out cross-validation strategy to carry out the experiments. Specifically, for all the 18 sessions (corresponding to the 18 film clips) of experiments for each subject, we choose the EEG data belonging to one session as the testing data and use the ones belonging to the other 17 sessions as the training data. The emotion classification accuracy for the testing data is calculated based on the training data. This procedure is repeated for 18 trials such that the EEG data samples of each session have been used once as the testing data. Then, the overall classification accuracy of this subject is achieved by averaging the recognition accuracies of all the 18 trials. Finally, we use the average emotion classification accuracy obtained by averaging all the 23 subjects to evaluate the emotion recognition performance of the DGCNN method.

Table 5 shows the experimental results of the DGCNN method with respect to different emotion dimensions (i.e., valence, arousal and dominance). For comparison purpose, three state-of-the-art methods, i.e., SVM, graph regularized sparse linear discriminant analysis (GraphSLDA) [61] and group sparse canonical correlation analysis (GSCCA) [9], are also used to conduct the same experiments and the experimental results are included in Table 5.

TABLE 5 The Average Classification Accuracies and Standard Deviations (%%) of Subject-Dependent EEG Emotion Recognition on DREAMER Database Using PSD Feature

From Table 5, we can obtain the following major points:

The proposed DGCNN method achieves much higher classification accuracies than the other three state-of-the-art methods, in which the classification accuracies could be as high as 86.23 percent for valence classification, 84.54 percent for arousal classification, and 85.02 percent for dominance classification, respectively.

The proposed DGCNN achieves more stable results than SVM, GraphSLDA, and GSCCA in terms of the standard deviations, in which the standard deviations of DGCNN are 12.29 percent for valence, 10.18 percent for arousal and 10.25 percent for dominance, respectively.

SECTION 5Conclusions and Discussions
In this paper, we have proposed a novel DGCNN model for EEG emotion recognition and conducted experiments on SEED and DREAMER EEG emotion databases, respectively, to evaluate the effectiveness of the proposed method. Both subject-dependent experiments and subject-independent cross validation experiments on SEED database had been conducted and the experimental results indicated that the DGCNN method achieves better recognition performance than the state-of-the-art methods such as the SVM, DBN, KPCA, TCA, T-SVM and TPT methods. Especially, when the DE features of five frequency bands are combined together, the average recognition accuracy of DGCNN can be as high as 90.40 percent for the case of subject dependent experiments and can be as high as 79.95 percent for the case of subject independent cross validation experiments. On the DREAMER database, the average accuracies of valence, arousal, and dominance using the proposed DGCNN are 86.23, 84.54 and 85.02 percent respectively, which are higher than SVM, GraphSLDA and GSCCA. The better recognition performance of DGCNN is most likely due to the following major points:

The use of nonlinear neural network of DGCNN makes it much more powerful to deal with the nonlinear discriminative feature learning;

The graph representation of DGCNN provides a useful way to characterize the intrinsic relationships among the various EEG channels, which is advantageous for extracting the most discriminative features for the emotion recognition task;

DGCNN adaptively learns the intrinsic relationships of EEG channels through optimization of the adjacency matrix W. In contrast to DGCNN, the GCNN model determines the values of W prior to the model learning stage. Consequently, using DGCNN model would be more accurate than GCNN to characterize the relationships of EEG channels.

Additionally, it is notable that the diagonal elements of the adjacency matrix indicate the contributions of the EEG channels to the EEG emotion recognition. Hence, the adjacency matrix would provide a potential way to find out what are the most contributive EEG channels in the EEG emotion recognition, which would be advantageous to further improve the EEG emotion recognition performance. We leave this interesting topic as our future work.

Another interesting issue that could be investigated in future is about the data scale generalization. Although the proposed DGCNN method had demonstrated to be a good method in dealing with the EEG emotion recognition, it is also notable that the data scales of the EEG databases used in the experiments are still relatively small, which would not be enough for learning more powerful deep neural network models and hence may limit the further performance improvement in this research. Consequently, an EEG database with much larger scales is desired in solving this problem and this is also a major task of our future work.
