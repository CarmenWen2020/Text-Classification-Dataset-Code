Existing scene understanding systems mainly focus on recognizing the visible parts of a scene ignoring the intact appearance of physical objects in the real world. Concurrently image completion has aimed to create plausible appearance for the invisible regions but requires a manual mask as input. In this work we propose a higher level scene understanding system to tackle both visible and invisible parts of objects and backgrounds in a given scene. Particularly we built a system to decompose a scene into individual objects infer their underlying occlusion relationships and even automatically learn which parts of the objects are occluded that need to be completed. In order to disentangle the occluded relationships of all objects in a complex scene we use the fact that the front object without being occluded is easy to be identified detected and segmented. Our system interleaves the two tasks of instance segmentation and scene completion through multiple iterations solving for objects layer by layer. We first provide a thorough experiment using a new realistically rendered dataset with ground truths for all invisible regions. To bridge the domain gap to real imagery where ground truths are unavailable we then train another model with the pseudo ground truths generated from our trained synthesis model. We demonstrate results on a wide variety of datasets and show significant improvement over the state of the art. Access provided by University of Auckland Library Introduction Fig. Example results of scene decomposition and recomposition. a Input. b Our model structurally decomposes a scene into individual completed objects. Red rectangles highlight the original invisible parts. c The inferred pairwise order top graph and edited order bottom graph of the instances. Blue nodes indicate the deleted objects while the red node is the moved object. d The new recomposed scene Color figure online Full size image The vision community has made rapid advances in scene understanding tasks such as object classification and localization Girshick et al. He et al. Ren et al. scene parsing Long et al. Chen et al. Badrinarayanan et al. instance segmentation Pinheiro et al. He et al. Chen et al. and layered scene decomposition Gould et al. Yang et al. Zhang et al. . Despite their impressive performance these systems deal only with visible parts of scenes without trying to exploit invisible regions which results in an uncompleted representation of real objects.In parallel significantly progress for the generation task has been made with the emergence of deep generative networks such as GAN based models Goodfellow et al. 
Existing scene understanding systems mainly focus on recognizing the visible parts of a scene, ignoring the intact appearance of physical objects in the real-world. Concurrently, image completion has aimed to create plausible appearance for the invisible regions, but requires a manual mask as input. In this work, we propose a higher-level scene understanding system to tackle both visible and invisible parts of objects and backgrounds in a given scene. Particularly, we built a system to decompose a scene into individual objects, infer their underlying occlusion relationships, and even automatically learn which parts of the objects are occluded that need to be completed. In order to disentangle the occluded relationships of all objects in a complex scene, we use the fact that the front object without being occluded is easy to be identified, detected, and segmented. Our system interleaves the two tasks of instance segmentation and scene completion through multiple iterations, solving for objects layer-by-layer. We first provide a thorough experiment using a new realistically rendered dataset with ground-truths for all invisible regions. To bridge the domain gap to real imagery where ground-truths are unavailable, we then train another model with the pseudo-ground-truths generated from our trained synthesis model. We demonstrate results on a wide variety of datasets and show significant improvement over the state-of-the-art.

Access provided by University of Auckland Library

Introduction
Fig. 1
figure 1
Example results of scene decomposition and recomposition. a Input. b Our model structurally decomposes a scene into individual completed objects. Red rectangles highlight the original invisible parts. c The inferred pairwise order (top graph) and edited order (bottom graph) of the instances. Blue nodes indicate the deleted objects while the red node is the moved object. d The new recomposed scene (Color figure online)

Full size image
The vision community has made rapid advances in scene understanding tasks, such as object classification and localization (Girshick et al. 2014; He et al., 2015; Ren et al. 2015), scene parsing (Long et al. 2015; Chen et al. 2017; Badrinarayanan et al. 2017), instance segmentation (Pinheiro et al. 2015; He et al. 2017; Chen et al. 2019), and layered scene decomposition (Gould et al. 2009; Yang et al. 2010; Zhang et al. 2015). Despite their impressive performance, these systems deal only with visible parts of scenes without trying to exploit invisible regions, which results in an uncompleted representation of real objects.

In parallel, significantly progress for the generation task has been made with the emergence of deep generative networks, such as GAN-based models (Goodfellow et al. 2014; Gulrajani et al. 2017; Karras et al. 2019), VAE-based models (Kingma and Welling 2014; Van Den Oord and Vinyals 2017; Vahdat and Kautz 2020), and flow-based models (Dinh et al. 2014, 2017; Kingma and Dhariwal 2018). Empowered by these techniques, image completion (Iizuka et al. 2017; Yu et al. 2018; Zheng et al. 2019) and object completion (Ehsani et al. 2018; Zhan et al. 2020; Ling et al. 2020) have made it possible to create the plausible appearances for occluded objects and backgrounds. However, these systems depend on manual masks or visible ground-truth masks as input, rather than automatically understand the full scene.

In this paper, we aim to build a system that has the ability to decompose a scene into individual objects, infer their underlying occlusion relationships, and moreover imagine what occluded objects may look like, while using only an RGB image as input. This novel task involves the classical recognition task of instance segmentation to predict the geometry and category of all objects in a scene, and the generation task of image completion to reconstruct invisible parts of objects and backgrounds.

To decompose a scene into instances with completed appearances in one pass is extremely challenging. This is because realistic natural scenes often consist of a vast collection of physical objects, with complex scene structure and occlusion relationships, especially when one object is occluded by multiple objects, or when instances have deep hierarchical occlusion relationships.

Our core idea is from the observation that it is easier to identify, detect and segment foreground objects than to directly model the complete shape of occluded objects (as shown in Sect. 5.2.2). Motivated by this, we propose a Completed Scene Decomposition Network (CSDNet) that learns to segment and complete each object in a scene layer-by-layer consecutively. As shown in Fig. 1, our layered scene decomposition network only segments the fully visible objects out in each layer (Fig. 1b). If the system is able to properly segment the foreground objects, it will automatically learn which parts of occluded objects are actually invisible that need to be filled in. The completed image is then passed back to the layered scene decomposition network, which can again focus purely on detecting and segmenting fully visible objects. As the interleaving proceeds, a structured instance depth order (Fig. 1c) is progressively derived. The thorough decomposition of a scene along with spatial relationships allows the system to freely recompose a new scene (Fig. 1d).

Another challenge in this novel task is the lack of data: there is no complex, realistic dataset that provides intact ground-truth appearance for originally occluded objects and backgrounds in a scene. While latest works (Li and Malik 2016; Zhan et al. 2020) introduced a self-supervised way to tackle the amodal completion using only visible annotations, they can not do a fair quantitative comparison as no real ground-truths are available. To mitigate this issue, we constructed a high-quality rendered dataset, named Completed Scene Decomposition (CSD), based on more than 2k indoor rooms. Unlike the datasets in (Ehsani et al. 2018; Dhamo et al. 2019), our dataset is designed to have more typical camera viewpoints, with near-realistic appearance.

As elaborated in Sect. 5.2, the proposed system performs well on this rendered dataset, both qualitatively and quantitatively outperforming existing methods in completed scene decomposition, in terms of amodal instance segmentation, instance depth ordering, and content completion. To further demonstrate the generalization of our system, we extend it to real datasets. As there is no ground truth annotations and appearance available for training, we created pseudo-ground-truths for real images using our model that is trained on CSD, and then fine-tuned this model accordingly. This model outperforms state-of-the-art methods (Zhu et al. 2017; Qi et al. 2019; Zhan et al. 2020) on amodal instance segmentation and depth ordering tasks, despite these methods being specialized to their respective tasks rather than our holistic completed scene decomposition task.

In summary, we propose a layer-by-layer scene decomposition network that jointly learns structural scene decomposition and completion, rather than treating them separately as the existing works (Ehsani et al. 2018; Dhamo et al. 2019; Zhan et al. 2020). To our knowledge, it is the first work that proposes to complete objects based on the global context, instead of tackling each object independently. To address this novel task, we render a high-quality rendered dataset with ground-truth for all instances. We then provide a thorough ablation study using this rendered dataset, in which we demonstrate that the method substantially outperforms existing methods that address the task in isolation. On real images, we improve the performance to the recent state-of-the-art methods by using pseudo-ground-truth as weakly-supervised labels. The experimental results show that our CSDNet is able to acquire a full decomposition of a scene, with only an image as input, which conduces to a lot of applications, e.g. object-level image editing.

The rest of the paper is organized as follows. We discuss the related work in Sect. 2, and describe our layer-by-layer CSDNet in detail in Sect. 3. In Sect. 4 we present our rendered dataset. We then show the experiment results on this synthetic dataset as well as the results on real-world images in Sect. 5, followed by a conclusion in Sect. 6.

Table 1 Comparison with related work based on three aspects: output, input and data.
Full size table
Related Work
A variety of scene understanding tasks have previously been proposed, including layered scene decomposition (Yang et al. 2011), instance segmentation (He et al. 2017), amodal segmentation (Li and Malik 2016), and scene parsing (Chen et al. 2017). In order to clarify the relationships of our work to the relevant literature, Table 1 gives a comparison based on three aspects: what the goals are, which information is used, and on which dataset is evaluated.

Layered scene decomposition for inmodal perception The layered scene decomposition for visible regions has been extensively studied in the literature. Nitzberg et al. (Mark and Mumford 1990; Nitzberg et al. 1993) first reasoned about occlusion through the use of a “2.1D” model and aimed to segment the foreground objects and recover the object contour by utilizing the occlusion relationship. Shade et al. (Shade et al. 1998) then extended the representation as a layered depth image (LDI), which contains multiple layers for a complex scene. Based on this image representation, the early works focused on ordering the semantic map as occluded and visible regions. Winn and Shotton (Winn and Shotton 2006) proposed a LayoutCRF to model several occlusions for segmenting partially occluded objects. Gould et al. (Gould et al. 2009) decomposed a scene into semantic regions together with their spatial relationships. Sun et al. (Sun et al. 2010) utilized an MRF to model the layered image motion with occlusion ordering. Yang et al.  (Yang et al. 2010, 2011) formulated a layered object detection and segmentation model, in which occlusion ordering for all detected objects was derived. This inferred order for all objects has been used to improve scene parsing (Tighe et al. 2014) through a CRF. Zhang et al. (Zhang et al. 2015) combined CNN and MRF to predict instance segmentation with depth ordering. While these methods evaluate occlusion ordering, their main goal is to improve the inmodal perception accuracy using the spatial occlusion information. In contrast to these methods, our method not only focuses on visible regions with structural inmodal perception, but also solves for amodal perception. i.e. to learn what is behind the occlusion.

Amodal image/instance perception Some initial steps have been taken toward amodal perception, exploring the invisible regions. Gao et al. (Gao et al. 2007) and Hoiem et al. (Hoiem et al, 2011) first explored contour completion by inferring occlusion relationships. Guo and Hoiem (Guo and Hoiem 2011) then investigated background semantic map completion by learning relationships between occluders and background. Subsequently, (Liu et al. 2016) introduced the Occlusion-CRF to handle occlusions and complete occluded surfaces. Kar et al. (Kar et al. 2015) focused on amodal bounding box completion. The common attribute in these earlier amodal perception works is using piecemeal representations of a scene, rather than a full decomposition that infers the amodal shapes for all objects.

The success of advanced deep networks trained on large-scale annotated datasets has recently led to the ability to get more comprehensive representations of a scene. Instance segmentation (Pinheiro et al. 2015; Dai et al. 2016; Pinheiro et al. 2016; Li et al. 2017) deals with detecting and segmenting all objects of a scene. This task combines the object detection (Girshick et al. 2014; He et al., 2015; Girshick 2015) and semantic segmentation (Long et al. 2015; Chen et al. 2017; Badrinarayanan et al. 2017). However, these methods typically segment the scene only in visible regions, and do not have an explicit structural representation of a scene that includes occluded regions. We believe a key reason is the lack of large-scale datasets with corresponding annotations for amodal perception and occlusion ordering. The widely used datasets, such as Pascal VOC 2012 (Everingham et al. 2010), NYU Depth v2 (Silberman et al. 2012), COCO (Lin et al. 2014), KITTI (Geiger et al. 2012), and CityScapes (Cordts et al. 2016), contain only annotations for the visible instances, purely aiming for 2D inmodal perception.

To steer by the lack of annotated datasets, Li et al. (Li and Malik 2016) presented a self-supervised approach by pasting occluders into an image. Although reasonable amodal segmentation results are shown, a quantitative comparison is unavailable due to the lack of ground-truth annotations for invisible parts. In more recent works, the completed masks for occluded parts are provided in COCOA (Zhu et al. 2017) and KINS (Qi et al. 2019), which are respectively a subset of COCO (Lin et al. 2014) and KITTI (Geiger et al. 2012). However, their annotations for invisible parts are manually labeled, which is highly subjective (Ehsani et al. 2018; Zhan et al. 2020). Furthermore, these datasets are mainly used for the task of inferring amodal semantic maps and are not suitable for the task of RGB appearance completion, since the ground truth RGB appearance for occluded parts are not available. In contrast, we jointly address these two amodal perception tasks using our constructed CSD dataset.

Amodal perception for both mask and appearance The problem of generating the amodal RGB appearance for the occluded parts is highly related to semantic image completion. The latest methods (Pathak et al. 2016; Yang et al. 2017; Iizuka et al. 2017; Yu et al. 2018; Zheng et al. 2019) extend GANs (Goodfellow et al. 2014) and CGANs (Mirza and Osindero 2014) to address this task, generating new imagery to fill in partially erased image regions. However, they mainly focus on object removal, needing users to interactively annotate the objects to be removed.

Fig. 2
figure 2
An illustration of the CSDNet framework. a Overall layer-by-layer completed scene decomposition pipeline. In each step, the layered decomposition network selects out the fully visible objects. The completion network will complete the resultant holes with appropriate imagery. The next step starts again with the completed image. b The layered scene decomposition network estimates instance masks and binary occlusion relationships. c The completion network generates realistic content for the original invisible regions

Full size image
SeGAN (Ehsani et al. 2018) involved an end-to-end network that sequentially infers amodal masks and generates complete RGB appearances for instances. The instance depth order is estimated by comparing the areas of the full and visible masks. PCNet (Zhan et al. 2020) used a self-supervised learning approach to recover masks and content using only visible annotations. However, these works mainly present results in which the ground truth visible mask is used as input, and are sensitive to errors in this visible mask. While the recent work of Yan et al. (Yan et al. 2019) tried to visualize the invisible from a single input image, it only tackles the occluded “vehicle” category, for which there is much less variation in amodal shape and RGB appearance, and thus easier to model.

There are two recent works that attempt to learn structural scene decomposition with amodal perception. MONet (Burgess et al. 2019) combined an attention network and a CVAE (Kingma and Welling 2014) for jointly modeling objects in a scene. While it is nominally able to do object appearance completion, this unsupervised method has only been shown to work on simple toy examples. Dhamo et al. (Dhamo et al. 2019) utilized Mask-RCNN (He et al. 2017) to obtain visible masks, and conducted RGBA-D completion for each object. However, depth values are hard to accurately estimate from a single image, especially in real images without paired depth ground-truths. In practice, even if we use domain transfer learning for depth estimation, the pixel-level depth value for all objects are unlikely to be consistent in a real scene. Therefore, our method uses an instance-level occlusion order, akin to the “2.1D” model (Mark and Mumford 1990), to represent the structural information of a scene, which is easier to be inferred and manipulated.

Method
In this work, we aim to derive a higher-level structural decomposition of a scene. When given a single RGB image 𝐈, our goal is to decompose all objects in it and infer their fully completed RGB appearances, together with their underlying occlusion relationships (As depicted in Fig. 1). Our system is designed to carry out inmodal perception for visible structured instance segmentation, and also solve the amodal perception task of completing shapes and appearances for originally occluded/invisible parts.

Instead of directly predicting the invisible content and decoupling the occlusion relationships of all objects at one pass, we use the fact that foreground objects are more easily identified, detected and segmented without occlusion. Our CSDNet decomposes the scene layer-by-layer. As shown in Fig. 2a, in each step 𝑠𝑘−1, given an image 𝐈(𝑠𝑘−1), the layered segmentation network creates masks as well as occlusion labels for all detected objects. Those instances classified as fully visible are extracted out and the scene completion network generates reasonable content as well as visually realistic appearances for invisible regions. The completed image 𝐈̂ (𝑠𝑘−1) will then be resubmitted for layered instance segmentation in the next step 𝑠𝑘. This differs significantly from previous works (Ehsani et al. 2018; Dhamo et al. 2019; Burgess et al. 2019; Zhan et al. 2020; Ling et al. 2020), which do not adapt the segmentation process based on completion results.

Our key novel insight is that scene completion generates completed shapes for originally occluded objects by leveraging the global scene context, so that the intact objects are subsequently easier to be detected and segmented without occlusion. Conversely, better segmented masks are the cornerstones to complete individual objects by precisely predicting which regions are occluded. Furthermore, this interleaved process enables extensive information sharing between these two networks, to holistically solve for multiple objects, and produces a structured representation for a scene. This contrasts with existing one-pass methods (Ehsani et al. 2018; Dhamo et al. 2019; Burgess et al. 2019; Zhan et al. 2020; Ling et al. 2020), where the segmentation and completion are processed separately and instances are handled independently. Together with the benefit of occlusion reasoning, our system is able to explicitly learn which parts of the objects and background are occluded that need to be completed, instead of freely extending to arbitrary shapes.

Layered Scene Decomposition
Mask R-CNN We begin by briefly reviewing the Mask R-CNN network (He et al. 2017). Mask R-CNN consists of two main stages. In the first stage, the image is passed to a backbone network (e.g. ResNet-50-FPN (Lin et al. 2017)) and next to a region proposal network (RPN (Ren et al. 2015)) to get object proposals. In the second stage, the network extracts features using RoIAlign from each candidate box, for passing to bounding-box regression, object classification and mask prediction. We refer readers to (He et al. 2017; Chen et al. 2019) for details.

Layered Scene Decomposition As shown in Fig. 2 b, our layered scene decomposition network adopts the same two-stage procedure. For each candidate instance, it outputs a class label, a bounding-box offset, an instance mask and a layer label. The new layer classification branch is added because our main goal is to visit the invisible, where it is important to precisely estimate the originally occluded regions that need to be completed.

Our proposed layer branch is a binary occlusion classifier, which determines if an object is fully visible or partially occluded. Combining with the completion network, it consecutively decomposes a scene layer-by-layer, where at each step it is applied to a single RGB image derived from the counterpart scene completion step. While only binary decisions are made in each step, after a full set of iterations, a comprehensive layered occlusion ordering is obtained. The following parts describe how this is done, looking first at the instance depth order representation, followed by how occlusion head is designed.

Fig. 3
figure 3
Instance depth order representation. Top images show absolute layer order (Qi et al. 2019) in different views. Bottom directed graphs give the pairwise order between objects. Note that the same chair (instance #3) has different absolute layer orders in the two views

Full size image
Instance depth order representation Absolute layer order and pairwise occlusion order are two standard representations for occlusion reasoning in a scene (Sun et al. 2010; Mark and Mumford 1990). As shown in Fig. 3, the definition for our absolute layer order  follows (Qi et al. 2019), where fully visible objects are labeled as 0, while other occluded objects have 1 order higher than the highest-order instance occluding them (see top images in Fig. 3). We interpret the pairwise occlusion order matrix as a directed graph 𝐺=(Ω,𝑊) (see bottom graphs in Fig. 3), where Ω is a discrete set of all instances with number N, and W is a 𝑁×𝑁 matrix. 𝑊𝑖,𝑗 is the occlusion relationship of instance i to instance j. We use three numbers to encode the order — {−1: occluded, 0 : no relationship, 1: front}. For example, the chair (instance #3) is occluded by the table (instance #2), so the pairwise order for the chair is 𝑊3,2=−1, while the pairwise order for the table is inversely labeled as 𝑊2,3=1.

Occlusion head In practice we find it hard to directly predict these instance depth orders. The absolute layer order index 𝑙∈ cannot be predicted purely from local features in a bounding box, since it depends on the global layout of all objects in a scene. Furthermore, this index is very sensitive to viewpoints, e.g. in Fig. 3, the desk (instance #5) is occluded by only one object (instance #3) in both views, but the absolute layer order indices of the desk are different: “2” vs “1”. In contrast, pairwise ordering G captures the occlusion relationships between pairs of objects, but all pairs have to be analyzed, leading to scalability issues in the current instance segmentation network. As R-CNN-based system creates 2,000 candidate objects, this pairwise analysis requires building an unwieldy 2k×2k features. Even if we were to restrict these to the 100 highest scoring detection boxes, it will still be very memory intensive.

We circumvent these problems as our occlusion classifier only predicts a binary occlusion label: {0,1} in each step, where 0 is fully visible, and 1 is occluded, following the setting of absolute layer order. During training, each ground-truth binary occlusion label is determined from the pairwise order of the actual objects present in the scene. The occlusion head in our layered scene decomposition network is the stacked fc layers, which receive aligned features from each RoI and predicts the binary occlusion label.

Decomposition Loss The multi-task loss function for layered scene decomposition is defined as follows:

𝐿decomp=∑𝑡=1𝑇𝛼𝑡(𝐿𝑡cls+𝐿𝑡bbox+𝐿𝑡mask+𝐿𝑡occ)+𝛽𝐿seg
(1)
where classification loss 𝐿𝑡cls, bounding-box loss 𝐿𝑡bbox, mask loss 𝐿𝑡mask and semantic segmentation loss 𝐿seg are identical to those defined in HTC (Chen et al. 2019), and 𝐿𝑡occ is the occlusion loss at the cascade refined stage t (three cascade refined blocks in HTC (Chen et al. 2019)), using binary cross-entropy loss (Long et al. 2015) for each RoI.

Visiting the Invisible by Exploring Global Context
In our solution, we treat visiting the invisible as an image completion (Pathak et al. 2016) problem. As illustrated in Fig. 2, in step 𝑠𝑘−1, after removing the fully visible instances, the given image 𝐈(𝑠𝑘−1) is degraded to become 𝐈(𝑠𝑘−1)𝑚. Our main goal is to generate appropriate content to complete these previously invisible regions. Unlike existing methods that complete each object independently (Ehsani et al. 2018; Dhamo et al. 2019; Burgess et al. 2019; Zhan et al. 2020; Ling et al. 2020), our model completes multiple objects in each step layer-by-layer, such that the information from earlier scene completions propagate to later ones. The global scene context is utilized in each step.

PICNet We first briefly review PICNet (Zheng et al. 2019) in our completion network. PICNet has two parallel pipelines. The top reconstructive pipeline combines information from both visible and invisible regions, which is used only for training. The bottom pipeline infers the invisible regions’ distribution based on visible information, which aims to provide multiple and diverse results for this highly subjective problem. The two pipelines share identical weights for the encoder and decoder structures. We refer readers to (Zheng et al. 2019) for details.

To visit the invisible, it is critical to know which parts are invisible and therefore need to be completed. The general image completion methods typically expect masks that have fixed shapes or are interactively drawn by users, which differs from our goal that automatically models occlusion and completion. Recent related works (Ehsani et al. 2018; Zhan et al. 2020; Ling et al. 2020) depend on the ground-truth visible masks as input to indicate which parts are occluded. In contrast, our system selects out fully visible objects and automatically learns which parts are occluded in each step. The holes left behind explicitly define the occluded regions for remaining objects, and thus the completed shapes for remaining objects must be deliberately restricted to these regions, instead of being allowed to grow freely using only the predicted visible masks. While the occluded objects may indeed have diverse shapes and appearances as addressed in PICNet, in this work, to simplify the task and focus on our main objectives, we only want the completed result closest to the ground-truth. Therefore, we use the encoder-decoder architecture, and eschew the random sampling aspect in original PICNet.

Fig. 4
figure 4
Amodal instance segmentation results on the COCOA validation set. Our model trained on synthetic dataset (Ours-syn) achieves visually reasonable results in the similar real indoor scenes (example in top row), but it fails in some dissimilar real scenes (example in bottom row). After training on the real data with “pseudo ground-truths”, the model (Ours-Real) performs much better. Note that, unlike the PCNet (Zhan et al. 2020) that needs visible inmodal ground-truth masks as input, our system decomposes a scene using only an RGB image (Color figure online)

Full size image
Completion Loss The overall scene completion loss function is given as:

𝐿comp=𝛼rec𝐿rec+𝛼ad𝐿ad+𝛼per𝐿per
(2)
where reconstruction loss 𝐿rec and adversarial loss 𝐿ad are identical to those in PICNet (Zheng et al. 2019). The perceptual loss 𝐿per=|𝐅(𝑙)(𝐈̂ (𝑠𝑘))−𝐅(𝑙)(𝐈(𝑠𝑘))| (Johnson et al. 2016), based on a pretrained VGG-19 (Simonyan and Zisserman 2014), is the 𝑙1 distance of features 𝐅 in l-th layer between the generated image 𝐈̂ (𝑠𝑘) and ground-truth 𝐈(𝑠𝑘).

Inferring Instance Pairwise Occlusion Order
As discussed in Sect. 3.1, absolute layer order  is sensitive to errors. If one object is incorrectly selected as a front object, objects behind it will have their absolute layer order incorrectly shifted. Hence in keeping with prior works (Ehsani et al. 2018; Zhan et al. 2020), we use the pairwise occlusion order 𝐺=(Ω,𝑊) for final evaluation.

Given a single image 𝐈, our model decomposes it into instances with completed RGB appearances 𝐴𝑆𝐾Ω. Here, A denotes the amodal perception instance (inclusive of both mask and appearance), Ω specifies instances in the scene, and 𝑆𝐾 indicates which layers are the instances in (selected out in step 𝑠𝑘). When two segmented amodal masks 𝐴𝑠𝑖𝜔𝑖 and 𝐴𝑠𝑗𝜔𝑗 overlap, we infer their occlusion relationship based on the order of the object-removing process, formally:

𝑊𝜔𝑖,𝜔𝑗=⎧⎩⎨⎪⎪01−1 if 𝑂(𝐴𝑠𝑖𝜔𝑖,𝐴𝑠𝑗𝜔𝑗)=0if𝑂(𝐴𝑠𝑖𝜔𝑖,𝐴𝑠𝑗𝜔𝑗)>0and𝑠𝑖<𝑠𝑗if𝑂(𝐴𝑠𝑖𝜔𝑖,𝐴𝑠𝑗𝜔𝑗)>0and𝑠𝑖≥𝑠𝑗
(3)
where 𝑂(𝐴𝑠𝑖𝜔𝑖,𝐴𝑠𝑗𝜔𝑗) is the area of overlap between instances 𝜔𝑖 and 𝜔𝑗. If they do not overlap, they share no pairwise depth-order relationship in a scene. If there is an overlap and the instance 𝜔𝑖 has a smaller layer order, the inferred pairwise order is 𝑊𝜔𝑖,𝜔𝑗 = 1; otherwise it is labeled as 𝑊𝜔𝑖,𝜔𝑗 = -1. Hence the instance occlusion relationship only depends on the order (selected out step) of removal between the two instances, and do not suffer from shift errors.

Training on Real Data with Pseudo Ground-truth
Real-world data appropriate for a completed scene decomposition task is difficult to acquire, because ground truth shapes and RGB appearance for occluded parts are hard to collect without extensive manual interventions, e.g. deliberate physical placement and removal of objects. Although our proposed model trained on the high-quality rendered data achieves visually reasonable results in some real scenes that share similarities to the rendered dataset (e.g. the scene in the top row of Fig. 4), it does not generalize well to dissimilar real scenes (e.g. the scene in the bottom row of Fig. 4). These are caused by: 1) differences in categories between synthetic and real datasets, especially between indoor and outdoor scenes, e.g. motorbikes do not appear in our synthetic dataset; and 2) inconsistencies between synthetically trained image completion of masked regions and fully visible real pixels, e.g. the missing corner of the table in Fig. 6.

One alternative is to simply use an image completion network trained only on real images. From our experience, this performs poorly in a scene decomposition task. The reason is that while real-trained image completion methods are able to create perceptually-pleasing regions and textures for a single completion, they do not appear to have the ability to adhere to consistent object geometry and boundaries when de-occluding, which is crucial for object shape completion.

Fig. 5
figure 5
Pseudo RGB ground-truth. a Input. b Masked image by selecting out the fully visible objects. c Pseudo ground-truth generated from our model trained on synthetic data

Full size image
Fig. 6
figure 6
Training pipeline for real images. We introduce a semi-supervised learning method for real data by providing pseudo RGB ground-truth for originally invisible regions

Full size image
Table 2 Dataset statistic comparison. “Occludees” denotes the occluded objects.
Full size table
Fig. 7
figure 7
Our rendered dataset. a High quality rendered RGB images. b Semantic map and instance annotations with bbox, category, ordering and segmentation map. c Intact RGBA ground-truth for instances and background

Full size image
Our key motivating insight is this: instead of training the model entirely without ground-truth in the completion task, we train it in a semi-supervised learning manner, exploiting the scene structure and object shape knowledge that has been gained in our synthetically-trained CSDNet. As shown in Fig. 5, when the fully visible instances are correctly segmented out by the real decomposition network, this synthetic completion model is able to generate visually adequate appearance, but more importantly it is better able to retain appropriate geometry and shapes. We can use this to guide the main image completion process in real-word data, while allowing a GAN-based loss to increase the realism of the output.

Specifically, for a real image 𝐈, we first train the layered decomposition network using the manual annotated amodal ground truth labels. In a step, after segmenting and selecting out the foreground objects, we obtain 𝐈̂ (𝑠𝑘)𝑠𝑦𝑛=𝐺(𝐈(𝑠𝑘)𝑚;𝜃syn) to serve as “pseudo ground-truth” (green box in Fig. 6) through the completion model trained on synthetic data. We then train the completion network 𝐺(𝐈(𝑠𝑘)𝑚;𝜃real) using the loss function of quation (2) by comparing the output 𝐈̂ (𝑠𝑘)𝑟𝑒𝑎𝑙 to “pseudo ground-truth” 𝐈̂ (𝑠𝑘)𝑠𝑦𝑛. It is worth noticing that the completed image is passed back to the layered decomposition network in the next layer, where the decomposition loss 𝐿decomp in equation (1) will be backpropagated to the completion network. This connection allows the completion network to learn to complete real world objects that might not be learned through the synthetic data.

Synthetic Data Creation
Large datasets with complete ground-truth appearances for all objects are very limited. Burgess et al. (Burgess et al. 2019) created the Objects Room dataset, but only with toy objects. Ehsani et al. (Ehsani et al. 2018) and Dhamo et al. (Dhamo et al. 2019) rendered synthetic datasets. However, the former only includes 11 rooms, with most viewpoints being atypical of real indoor images. The latter’s OpenGL-rendered dataset appears to have more typical viewpoints with rich annotations, but the OpenGL-rendered images have low realism. We refer readers to (Ehsani et al. 2018) and (Dhamo et al. 2019) for details of image quality. Recently, Zhan et al. (Zhan et al. 2020) explored the amodal completion task through self-supervised learning without the need of amodal ground-truth. However, a fair quantitative comparison is not possible as no appearance ground-truth is available for invisible parts.

To mitigate this issue, we rendered a realistic dataset with Maya (Autodesk 2019). In Table 2, we compare our proposed CSD dataset with other amodal datasets, i.e. COCOA (Zhu et al. 2017), COCOA-cls (Follmann et al. 2019), D2S (Follmann et al. 2019), KINS (Qi et al. 2019), DYCE (Ehsani et al. 2018) and OLMD (Dhamo et al. 2019), considering the number of images, instances and “occludees”. Note that for the real scenes, the “ground truth” amodal masks were manually guessed by human annotators, and there is no visually complete RGB ground truth for occluded instances nor the background.

Data Rendering Our rendered data is based on a total of 11.4k views inside over 2k rooms (3D CAD models from SUNCG (Song et al. 2017)) with various room types and lighting environments (see Fig. 7a). To select the typical viewpoints, we first sampled many random positions, orientations and heights for the camera. Only when a view contains at least 5 objects will we render the image and the corresponding ground-truth of each instance. To avoid an excessive rendering workload, we separately rendered each isolated object, as well as the empty room, as shown in Fig. 7c. This allows us to then freely create the ground-truths of each layer by compositing these individual objects and background using the instance occlusion order.

Data Annotation Each rendered scene is accompanied by a global semantic map and dense annotations for all objects. As shown in Table 2, there are 11,434 images encompassing 129,336 labeled instances in our CSD. On average, there are 11 individual objects per view. Among these, 63.58% instances are partially occluded by other objects and the average occlusion ratio (average IoU between two objects) is 26.27%. More importantly, the intact RGB appearances for all instance are provided, as well as categories (the 40 classes in NYUDv2 (Nathan Silberman et al. 2012)), bounding boxes and masks for complete objects, as well as for only the visible regions (shown in Figs. 7b and  7c). Furthermore, the absolute layer order and pairwise occlusion order shown in Fig. 3 are also defined in our rendered dataset.

Experiments
Setup
Datasets We evaluated our system on three datasets: COCOA (Zhu et al. 2017), KINS (Qi et al. 2019) and the rendered CSD (as shown in Table 2) and Fig. 7. COCOA is annotated from COCO (Lin et al. 2014), a large scale natural image datasets, in which 5,073 images are selected to manually label with pairwise occlusion orders and amodal masks. KINS is derived from the outdoor traffic dataset KITTI, in which 14,991 images were labeled with absolute layer orders and amodal masks. CSD is our rendered synthetic dataset, where we conducted thorough experiments and ablation studies to assess the quality of the completed results for invisible appearance estimation (since the in-the-wild datasets lack ground-truth for the occluded parts).

Metrics For amodal instance segmentation, we report the standard COCO metrics (Lin et al. 2014), including AP (average over IoU thresholds), AP50, AP75, and AP𝑆, AP𝑀 and AP𝐿 (AP at different scales), and AR (average recall over IoU). Unless otherwise stated, the AP and AR is for mask IoU. For appearance completion, we use the RMSE, SSIM and PSNR metrics to evaluate the quality of generated images. All images are normalized to the range [0,1].

Since the occlusion order is strongly related to the quality of instance segmentation (higher quality masks are easier to order (Zhu et al. 2017)), previous works (Ehsani et al. 2018; Zhan et al. 2020) directly utilize the ground truth visible mask as input during testing to segment the object for instance depth ordering. However, precise visible instance segmentation remains hard to achieve in real applications. Following (Zhu et al. 2017), we evaluate the pairwise instance depth ordering for those correctly detected instances. Specifically, given a pairwise occlusion order 𝐺=(Ω,𝑊) predicted by the model, we only evaluate the order for valid instances that have IoU with ground-truth masks over a given threshold. For instance, if we set the threshold as 0.5, the predicted instance 𝜔 will only be evaluated when we can identify a matched ground-truth mask with IoU≥0.5. Hence we can measure the occlusion average precision (OAP) across different thresholds. As result, we can analyze the influence of amodal instance segmentation quality on the accuracy of instance depth ordering, as shown in Tables 4 and 9. Note that the originally presented metric in (Zhu et al. 2017) for instance pairwise depth ordering is a special case of our proposed OAP, using only the IoU threshold of 0.5.

Training Except for the newly added weight 𝛼𝑝𝑒𝑟=1.0, we set other hyper-parameters following the baseline works of HTC (Chen et al. 2019) and PICNet (Zheng et al. 2019). Although these choices were based on their respective tasks in isolation, we found that they were also suitable for our jointly optimized system.

Fig. 8
figure 8
Layer-by-Layer Completed Scene Decomposition results on rendered CSD testing set. a Input RGB images. b Final amodal instance segmentation. c Inferred directed graph for pairwise order. d Columns labeled S1-5 show the decomposed instances (top) and completed scene (bottom) based on the predicted non-occluded masks. Note that the originally occluded invisible parts are filled in with realistic appearance

Full size image
The synthetic model is trained in three phases: 1) the layered scene decomposition network (Fig. 2b) is trained with loss 𝐿𝑑𝑒𝑐𝑜𝑚𝑝 for 24 epochs, where at each layer, re-composited layered ground-truths are used as input. 2) Separately, the completion network (Fig. 2c) is trained with loss 𝐿𝑐𝑜𝑚𝑝 for 50 epochs, wherein the ground-truth layer orders and segmented masks are used to designate the invisible regions for completion. 3) Both decomposition and completion networks are trained jointly for 12 epochs, without relying on ground-truths as input at any layer (Fig. 2a). Doing so allows the scene decomposition network to learn to cope with flaws (e.g. texture artifacts) in the scene completion network, and vice versa. For each scene, the iteration ends when no more objects are detected, or a maximum 10 iterations is reached.

The training on real data only involved phases 1 and 3, as no ground-truth appearances are available for the invisible parts. The layered decomposition network is trained only for one layer (original image) in phase 1 due to no re-composed ground-truth images. Since phase 3 does not rely on ground-truth as input, we trained it layer-by-layer on real images by providing the “pseudo ground truth” appearances to calculate the reconstruction loss. To reduce the effect of progressively introduced artifacts in image completion, we used full bounding boxes detected in the first layer as proposals for remaining decomposition steps.

Inference During testing, fully visible instances are selected and assigned an absolute layer order corresponding to the step index 𝑠𝑘. In each layer, the decomposition network selects the highest scoring 100 detected boxes for mask segmentation and non-occlusion predication. As we observed that higher object classification scores provided more accurately segmented boundaries, we only select non-occluded objects with high classification scores and non-occlusion scores (thresholds of 0.5 for synthetic images and 0.3 for real images) among these 100 candidates. We further observed that in some cases we detected multiple objects with high object classification confidences, yet none were classified as fully visible due to low non-occlusion scores, especially in complex scenes with steps larger than 7. We will then choose the instance with the highest non-occlusion score so that at least one object is selected at each layer. When no objects are detected, the iteration stops.

Table 3 Amodal Instance Segmentation on CSD testing sets.
Full size table
Table 4 Instance depth ordering on CSD testing sets.
Full size table
Results on Synthetic CSD Dataset
We first present results that we obtained from our framework when experimenting on our synthetic CSD dataset.

Main Results
Completed scene decomposition We show the qualitative results of CSDNet in Fig. 8. Given a single RGB image, the system has learned to decompose it into semantically complete instances (e.g. counter, table, window) and the background (wall, floor and ceiling), while completing RGB appearance for invisible regions. Columns labeled S1-5 show the completed results layer-by-layer. In each layer, fully visible instances are segmented out, and after scene completion some previously occluded regions become fully visible in the next layer, e.g. the table in the second example. The final amodal instance segmentation results shown in Fig. 8b consist of the fully visible amodal masks in each layer. Note that unlike MONet (Burgess et al. 2019), our model does not need predefined slots. The process will stop when it is unable to detect any more objects.

Amodal instance segmentation We compare CSDNet to the state-of-the-art methods in amodal instance segmentation in Table 3. As the existing works Mask-RCNN (He et al. 2017) and HTC (Chen et al. 2019) are aimed at inmodal perception for visible parts, we retrained their models for amodal perception task, by providing amodal ground-truths. We also retrained MLC (Qi et al. 2019) on our rendered dataset, which are the latest work for amodal perception. For PCNet (Zhan et al. 2020), we used the predicted visible mask as input, rather than the original visible ground-truth annotations. While the HTC (Chen et al. 2019) improves on Mask-RCNN’s (He et al. 2017) bounding box AP by about 1.6 points by refining the bounding box offsets in three cascade steps, the improvement for amodal mask segmentation is quite minor at 0.5 points. We believe this is an inherent limitation of methods that attempt to directly estimate the complete shape of occluded objects without first reasoning about occluding objects and masking their image features, as the features of occluding objects end up being involved in the classification of occluded objects, which will distract the network. In contrast, our CSDNet is able to improve the amodal mask segmentation accuracy by a relative 6.3% with the same backbone segmentation network (HTC), by jointing segmentation and completion with layer-by-layer decomposition.

Table 5 Object Completion.
Full size table
To further demonstrate that better completed images improve amodal segmentation, we consider a scenario with a completion oracle, by using ground-truth appearances to repair the occluded parts in each step. This is denoted as the CSDNet-gt, for which amodal instance segmentation accuracy increases from 47.3% to 56.0% (relative 18.4% improvement). We also note that, while the CSDNet-gt using Mask-RCNN achieves lower bounding box score than our HTC CSDNet (“54.9” vs “56.3”), the mask accuracy is much higher (“53.1” vs “50.3”). This suggests that amodal segmentation benefits from better completed images.

Instance depth ordering We compare CSDNet to the state-of-the-art instance depth ordering in Table 4. The original SeGAN and PCNet used ground-truth visible masks V𝑔𝑡 as input during testing. For a fair comparison, we first retrained them on our synthetic data using the same segmentation network (HTC (Chen et al. 2019)) for all models. After predicting amodal masks, we assessed various instance depth ordering algorithms: two baselines proposed in AmodalMask (Zhu et al. 2017) of ordering by areaFootnote1 and by y-axis (amodal masks closest to image bottom in front), ordering by incremental area defined as the IoU area between visible and amodal masksFootnote2, and our ordering by absolute layer order (Sect. 3.3).

As can be seen in Table 4, all instantiations of our model outperformed baselines as well as previous models. Unlike SeGAN (Ehsani et al. 2018) and PCNet (Zhan et al. 2020), our final model explicitly predicts the occlusion labels of instances, which improved the OAP substantially. While MLC (Qi et al. 2019) predicts the instance occlusion order in a network, it only contains one layer for binary occlusion / non-occlusion labeling. In contrast, our method provides a fully structural decomposition of a scene in multiple steps. Additionally, we observed that our model achieves better performance with a higher IoU threshold for selecting the segmented mask (closer match to the ground-truth masks). This suggests that the instance depth ordering is highly related to the instance segmentation quality, and thus it is important to use our proposed OAP for multi-threshold evaluation. We further observed that the occlusion relationships of small objects are difficult to infer in our method. However, the Y-axis ordering method had similar performance under various metrics as it only depends on the locations of objects. Note that our depth ordering does not rely on any ground-truth labels that are used in (Ehsani et al. 2018; Zhan et al. 2020).

Object completion We finally evaluated the quality of generated appearances. We compared our results to those from SeGAN (Ehsani et al. 2018), Dhamo et al. (Dhamo et al. 2019) (abbrev. as DNT), PCNet (Zhan et al. 2020) and PICNet (Zheng et al. 2019) (original point-attention, without perceptual loss) in Table 5. We evaluated different conditions of: C1) when the ground-truth full mask F𝑔𝑡 is provided to all methods, C2a) when the ground-truth visible mask V𝑔𝑡 is the input to SeGAN and PCNet, and C2b) when an RGB image is the only input to other methods. C2a-V𝑔𝑡 is considered because SeGAN and PCNet assumes that a precisely predefined visible mask is provided as input.

In C1-F𝑔𝑡, CSDNet substantially outperformed the other methods. In C2, even when given only RGB images without ground-truth masks, our method worked better than SeGAN and PCNet with V𝑔𝑡. One important reason for the strong performance of CSDNet is the occlusion reasoning component, which constraints the completed shapes of partly occluded objects based on the global scene context and other objects during testing.

Qualitative results are visualized in Fig. 9. We noted that SeGAN worked well only when ground-truth amodal masks F𝑔𝑡 were available to accurately label which parts were invisible that needed filling in, while DNT generated blurry results from simultaneously predicting RGB appearance and depth maps in one network, which is not an ideal approach (Zamir et al. 2018). The PCNet (Zhan et al. 2020) can not correctly repair the object shape as it trained without ground-truth object shape and appearance. Our CSDNet performed much better on background completion, as it only masked fully visible objects in each step instead of all objects at a go, so that earlier completed information propagates to later steps.

Fig. 9
figure 9
Results for Visiting the Invisible. We show the input image, our amodal instance segmentation results, and the objects and background we try to visit. The red rectangles highlight the previously invisible regions of occluded objects

Full size image
Fig. 10
figure 10
Ablations with different numbers of objects and objects in different layers. Left: Amodal instance segmentation accuracy for the SOTA models presented in Table 3. These models are trained on our synthetic CSD dataset with true ground-truth for originally occluded regions. Right: Amodal instance segmentation recall for the SOTA models presented in Table 8. These models were trained on the COCOA dataset with masks manually guessed by human annotators for occluded regions. Amodal segmentation is more difficult for hierarchically deeper occlusion relationships. The “#” numbers in the X-axis indicate the number of testing instances in each setting

Full size image
Ablation Studies
We ran a number of ablations on different components in CSDNet. Results are shown in Fig. 10 and Tables 6 and 7.

Number of objects per image. We first investigated the performance of state-of-the-art methods on various numbers of objects in each image. As shown in Fig. 10(left), when there are more objects in a scene, the amodal instance segmentation accuracy will decrease gradually, due to the more complex occlusion relationships and larger occlusion regions. The figure demonstrates that our decomposition network benefits from the completion network when there are fewer layers of occlusion, e.g. noting the large improvement when the number of objects is less than 14. However, when there are too many objects in a scene, e.g. leading to more than 7 hierarchical occlusion layers, the improvement will be smaller due to accumulated completion errors over the layers.

Amodal segmentation in different layers. In this setting, we selected the ground-truth instances based on the absolute layer value for the evaluation. As shown in Fig. 10(right), the recall rate dramatically decreases (average 24.0 AR) with the occlusion regions being heavier in the deeper layer, which supports our main claim that in terms of amodal instance segmentation, foreground objects are generally easier to be identified, detected and segmented than occluded objects, even for real objects and scenes. Besides, while the MLC (HTC) (Qi et al. 2019) shows better performance than our CSDNet(Mask R-CNN) in the first layer, its recall value decreases more for subsequent occluded objects. This further highlights the division of labor in our approach, where the layer decomposition network explicitly predicts the occluded regions and the later completion network infers reasonable content.

Maximum iterations Table 6a shows mask AP results over different iterations during testing. We noted that the amodal instance segmentation results benefit from the completed images when occlusion layers are limited (less than 7). However, when we conducted a deeper decomposition, the accumulated errors from wrong segmentation and unrealistic completion in previous steps will lead to worse detection and segmentation results downstream, with some noise patches classified as objects.

Occlusion classifier branch Modeling the full scene structure directly is challenging. Instead, our occlusion classifier solely performs binary classification for each detected instance using features from the full bounding box. In particular, if features from neighboring objects extend into a bounding box, our occlusion branch just needs to classify whether the selected object is being blocked (“occluded”) or not (“front”). In table 6b, we compare FCNs and MLP under the same HTC architecture. The MLP achieves much better accuracy by directly modeling the global relationship in the extracted features.

Does better completion help decomposition? We show quantitative results for a fixed decomposition network (layered HTC (Chen et al. 2019) with two completion methods in Table 7a. Without any completion (“w/o”), segmented results were naturally bad (“36.8” vs “50.3”) as it had to handle empty masked regions. More interestingly, even if advanced methods were used to generate visual completion, the isolated training of the decomposition and completion networks led to degraded performance. This suggests that even when generated imagery looks good visually, there is still a domain or semantic gap to the original visible pixels, and thus flaws and artifacts will affect the next segmentation step. The PICNet with patch attention provides better completed results than the original point attention PICNet (Zheng et al. 2019), resulting in a large improvement (“50.3” vs “47.7”) of amodal instance segmentation.

Does better decomposition help completion? To answer this, we report the results of using different scene segmentation networks with a same completion network (Patch-attention PICNet (Zheng et al. 2019)) in Table 7b. We also first considered the ideal situation that ground-truth segmentation masks were provided in each decomposition step. As shown in Table 7b, the completion quality significantly improved (RMSE: “0.0614”, SSIM: “0.9179” and PSNR: “35.24”) as occluded parts were correctly pointed out and the completion network precisely knows which parts need to be completed. HTC (Chen et al. 2019) provided better instance masks than Mask-RCNN (He et al. 2017), which resulted in more accurately completed scene imagery. The best results were with end-to-end jointly training.

Table 6 Ablations on CSDNet.
Full size table
Table 7 Ablations for joint optimization.
Full size table
Fig. 11
figure 11
Layer-by-layer completed scene decomposition on natural images. a Inputs. b Final amodal instance segmentation. c Inferred directed graph for pairwise occlusion order. d Columns labeled S1-3 show the decomposed instances with completed appearance in each step

Full size image
Table 8 Amodal Instance Segmentation on COCOA and KINS sets.
Full size table
Results on Real Datasets
We now assess our model on real images. Since the ground-truth appearances are unavailable, we only provide the visual scene manipulation results in Sect. 5.4, instead of quantitative results for invisible appearance completion.

Completed scene decomposition. In Fig. 11, we visualize the layer-by-layer completed scene decomposition results on real images. Our CSDNet is able to decompose a scene into completed instances with correct ordering. The originally occluded invisible parts of “suitcase”, for instance, is completed with full shape and realistic appearance. Note that, our system is a fully scene understanding method that only takes an image as input, without requiring the other manual annotations as  (Ehsani et al. 2018; Zhan et al. 2020).

Amodal instance segmentation. Next, we compare with state-of-the-art methods on amodal instance segmentation. Among these, AmodalMask (Zhu et al. 2017) and ORCNN (Follmann et al. 2019) were trained for the COCOA dataset, MLC (Qi et al. 2019) worked for the KINS dataset, and PCNet (Zhan et al. 2020) focused on amodal completion rather than amodal segmentation. For a fair comparison, when these methods do not provide results on a given dataset, we trained their models using publicly released code. We also conducted an ablation on training the system, denoted as “CSDNet-syn”, which uses the completion network trained on the synthetic dataset directly, instead of retraining another completion network with “pseudo ground truth”.

Table 8 shows that our results (34.8 mAP and 32.2 mAP) are 0.4 points and 0.6 points higher than the recent MLC using the same segmentation structure (HTC) in COCOA and KINS, respectively. PCNet (Zhan et al. 2020) considers amodal perception in two steps and assumes that visible masks are available. We note that their mAP scores were very high when the visible ground-truth masks were provided. This is because all initial masks were matched to the annotations (without detection and segmentation errors for instances, as shown in Fig. 12). However, when we used a segmentation network to obtain visible masks V̂ 𝑝𝑟𝑒𝑑 for PCNet, the amodal instance segmentation results became lower than other methods. We also found that without jointly optimizing the completion network, the CSDNet-syn resulted in a severe reduction in amodal mask AP (around 2 points). This suggests that while the pre-trained completion network can copy similar appearance from visible regions, it is inadequate to generate appropriate geometry and shapes for all instances, especially for those categories that do not exist in the synthetic dataset.

Fig. 12
figure 12
Amodal instance segmentation results on natural images. Our CSDNet learns to predict the intact mask for the occluded objects (e.g. animals and human). Note that, unlike PCNet (Zhan et al. 2020), our model does not depend on the visible mask (first row) as input. Hence it can handle some objects without ground-truth annotation, such as two ‘humans’ in the third column and the ‘smartphone’ in the fourth column

Full size image
Table 9 Instance depth ordering on COCOA and KINS sets.
Full size table
Fig. 13
figure 13
Free editing based on the results of our system on images from various datasets. Note that our method is able to automatically detect, segment and complete the objects in the scene, without the need for manual interactive masking, with interactive operations limited to only “delete” and “drag-and-drop”. The blue arrows show object removal, while red arrows show object moving operations. We can observe that the originally invisible regions are fully visible after editing

Full size image
In Fig. 12, we compare our CSDNet and PCNet (Zhan et al. 2020). PCNet only completes the given visible annotated objects which had visible masks. In contrast, our CSDNet produces more contiguous amodal instance segmentation maps even for some unlabeled objects, for instance, the two “humans” in the third column. Furthermore, our model correctly predicts the amodal masks for the “cow” in the first column, while the ground truth labels are definitely wrong. On the other hand, our model can directly create a deep hierarchical representation of a scene, producing a layer order for each instance.

Instance depth ordering Finally, we report the instance depth ordering results in Table 9. In order to compare with existing work, we considered two settings: ground-truths provided (blue rows in Table 9), and only RGB images given. The OrderNet obtained the best results as the ground-truth full masks F𝑔𝑡 were given. We note that PCNet and our model achieved comparable performance when the visible ground-truth masks were used. Note that, we only used V𝑔𝑡 for depth ordering, while PCNet utilized the visible mask as input for both mask completion and depth ordering. Furthermore, when no ground-truth annotation was provided as input, our model performed better than MLC and PCNet.

Applications
We illustrate some image editing / re-composition applications of this novel task, after we learned to decompose a scene into isolated completed objects together with their spatial occlusion relationships. In Fig. 13, we visualize some recomposed scenes on various datasets, including our CSD, real COCOA (Zhu et al. 2017), and KITTI (Qi et al. 2019).

In these cases, we directly modified the positions and occlusion ordering of individual objects. For instance, in the first bedroom example, we deleted the “window”, and moved the “bed” and the “counter”, which resulted in also modifying their occlusion order. Note that all original invisible regions were filled in with reasonable appearance. We also tested our model on real NYUD-v2 (Nathan Silberman et al. 2012) images which do not belong to any of the training sets used. As shown in the last column of Fig. 13, our model was able to detect and segment the object and complete the scene. The “picture”, for instance, is deleted and filled in with background appearance.

Conclusions
Building on previous inmodal and amodal instance perception work, we explored a higher-level structure scene understanding task that aims to decompose a scene into intact semantic instances, with completed RGB appearance and spatial occlusion relationships. We presented a layer-by-layer CSDNet, an iterative method to address this novel task. The main motivation behind our method is that fully visible objects, at each step, can be relatively easily detected and selected out without concern about occlusion. To do this, we simplified this complex task to two subtasks: instance segmentation and scene completion. We analyzed CSDNet and compared it with recent works on various datasets. Experimental results show that our model can handle an arbitrary number of objects and is able to generate the appearance of occluded parts. Our model outperformed current state-of-the-art methods that address this problem in one pass. The thorough ablation studies on synthetic data demonstrate that the two subtasks can contribute to each other through the layer-by-layer processing.

Gulrajani et al. Karras et al. VAE based models Kingma and Welling Van Den Oord and Vinyals Vahdat and Kautz and flow based models Dinh et al. Kingma and Dhariwal . Empowered by these techniques image completion Iizuka et al. Yu et al. Zheng et al. and object completion Ehsani et al. Zhan et al. Ling et al. have made it possible to create the plausible appearances for occluded objects and backgrounds. However these systems depend on manual masks or visible ground truth masks as input rather than automatically understand the full scene.In this paper we aim to build a system that has the ability to decompose a scene into individual objects infer their underlying occlusion relationships and moreover imagine what occluded objects may look like while using only an RGB image as input. This novel task involves the classical recognition task of instance segmentation to predict the geometry and category of all objects in a scene and the generation task of image completion to reconstruct invisible parts of objects and backgrounds.To decompose a scene into instances with completed appearances in one pass is extremely challenging. This is because realistic natural scenes often consist of a vast collection of physical objects with complex scene structure and occlusion relationships especially when one object is occluded by multiple objects or when instances have deep hierarchical occlusion relationships.Our core idea is from the observation that it is easier to identify detect and segment foreground objects than to directly model the complete shape of occluded objects as shown in Sect. .. . Motivated by this we propose a Completed Scene Decomposition Network CSDNet that learns to segment and complete each object in a scene layer by layer consecutively. As shown in Fig.  our layered scene decomposition network only segments the fully visible objects out in each layer Fig. b . If the system is able to properly segment the foreground objects it will automatically learn which parts of occluded objects are actually invisible that need to be filled in. The completed image is then passed back to the layered scene decomposition network which can again focus purely on detecting and segmenting fully visible objects. As the interleaving proceeds a structured instance depth order Fig. c is progressively derived. The thorough decomposition of a scene along with spatial relationships allows the system to freely recompose a new scene Fig. d .Another challenge in this novel task is the lack of data there is no complex realistic dataset that provides intact ground truth appearance for originally occluded objects and backgrounds in a scene. While latest works  Li and Malik Zhan et al. introduced a self supervised way to tackle the amodal completion using only visible annotations they can not do a fair quantitative comparison as no real ground truths are available. To mitigate this issue we constructed a high quality rendered dataset named Completed Scene Decomposition CSD based on more than k indoor rooms. Unlike the datasets in Ehsani et al. Dhamo et al. our dataset is designed to have more typical camera viewpoints with near realistic appearance.As elaborated in Sect. . the proposed system performs well on this rendered dataset both qualitatively and quantitatively outperforming existing methods in completed scene decomposition in terms of amodal instance segmentation instance depth ordering and content completion. To further demonstrate the generalization of our system we extend it to real datasets. As there is no ground truth annotations and appearance available for training we created pseudo ground truths for real images using our model that is trained on CSD and then fine tuned this model accordingly. This model outperforms state of the art methods Zhu et al. Qi et al. Zhan et al. on amodal instance segmentation and depth ordering tasks despite these methods being specialized to their respective tasks rather than our holistic completed scene decomposition task.In summary we propose a layer by layer scene decomposition network that jointly learns structural scene decomposition and completion rather than treating them separately as the existing works Ehsani et al. Dhamo et al. Zhan et al. . To our knowledge it is the first work that proposes to complete objects based on the global context instead of tackling each object independently. To address this novel task we render a high quality rendered dataset with ground truth for all instances. We then provide a thorough ablation study using this rendered dataset in which we demonstrate that the method substantially outperforms existing methods that address the task in isolation. On real images we improve the performance to the recent state of the art methods by using pseudo ground truth as weakly supervised labels. The experimental results show that our CSDNet is able to acquire a full decomposition of a scene with only an image as input which conduces to a lot of applications e.g. object level image editing.The rest of the paper is organized as follows. We discuss the related work in Sect.  and describe our layer by layer CSDNet in detail in Sect. . In Sect.  we present our rendered dataset. We then show the experiment results on this synthetic dataset as well as the results on real world images in Sect.  followed by a conclusion in Sect. .Table Comparison with related work based on three aspects output input and data.Full size tableRelated WorkA variety of scene understanding tasks have previously been proposed including layered scene decomposition  Yang et al. instance segmentation  He et al. amodal segmentation  Li and Malik and scene parsing  Chen et al. . In order to clarify the relationships of our work to the relevant literature Table  gives a comparison based on three aspects what the goals are which information is used and on which dataset is evaluated.Layered scene decomposition for inmodal perception The layered scene decomposition for visible regions has been extensively studied in the literature. Nitzberg et al. Mark and Mumford Nitzberg et al. first reasoned about occlusion through the use of a “.D” model and aimed to segment the foreground objects and recover the object contour by utilizing the occlusion relationship. Shade et al. Shade et al. then extended the representation as a layered depth image LDI which contains multiple layers for a complex scene. Based on this image representation the early works focused on ordering the semantic map as occluded and visible regions. Winn and Shotton Winn and Shotton proposed a LayoutCRF to model several occlusions for segmenting partially occluded objects. Gould et al. Gould et al. decomposed a scene into semantic regions together with their spatial relationships. Sun et al. Sun et al. utilized an MRF to model the layered image motion with occlusion ordering. Yang et al.   Yang et al. formulated a layered object detection and segmentation model in which occlusion ordering for all detected objects was derived. This inferred order for all objects has been used to improve scene parsing  Tighe et al. through a CRF. Zhang et al. Zhang et al. combined CNN and MRF to predict instance segmentation with depth ordering. While these methods evaluate occlusion ordering their main goal is to improve the inmodal perception accuracy using the spatial occlusion information. In contrast to these methods our method not only focuses on visible regions with structural inmodal perception but also solves for amodal perception. i.e. to learn what is behind the occlusion.Amodal image/instance perception Some initial steps have been taken toward amodal perception exploring the invisible regions. Gao et al. Gao et al. and Hoiem et al. Hoiem et al first explored contour completion by inferring occlusion relationships. Guo and Hoiem Guo and Hoiem then investigated background semantic map completion by learning relationships between occluders and background. Subsequently Liu et al. introduced the Occlusion CRF to handle occlusions and complete occluded surfaces. Kar et al. Kar et al. focused on amodal bounding box completion. The common attribute in these earlier amodal perception works is using piecemeal representations of a scene rather than a full decomposition that infers the amodal shapes for all objects.The success of advanced deep networks trained on large scale annotated datasets has recently led to the ability to get more comprehensive representations of a scene. Instance segmentation Pinheiro et al. Dai et al. Pinheiro et al. Li et al. deals with detecting and segmenting all objects of a scene. This task combines the object detection Girshick et al. He et al. Girshick and semantic segmentation Long et al. Chen et al. Badrinarayanan et al. . However these methods typically segment the scene only in visible regions and do not have an explicit structural representation of a scene that includes occluded regions. We believe a key reason is the lack of large scale datasets with corresponding annotations for amodal perception and occlusion ordering. The widely used datasets such as Pascal VOC Everingham et al. NYU Depth v Silberman et al. COCO Lin et al. KITTI Geiger et al. and CityScapes Cordts et al. contain only annotations for the visible instances purely aiming for D inmodal perception.To steer by the lack of annotated datasets Li et al. Li and Malik presented a self supervised approach by pasting occluders into an image. Although reasonable amodal segmentation results are shown a quantitative comparison is unavailable due to the lack of ground truth annotations for invisible parts. In more recent works the completed masks for occluded parts are provided in COCOA  Zhu et al. and KINS  Qi et al. which are respectively a subset of COCO  Lin et al. and KITTI  Geiger et al. . However their annotations for invisible parts are manually labeled which is highly subjective  Ehsani et al. Zhan et al. . Furthermore these datasets are mainly used for the task of inferring amodal semantic maps and are not suitable for the task of RGB appearance completion since the ground truth RGB appearance for occluded parts are not available. In contrast we jointly address these two amodal perception tasks using our constructed CSD dataset.Amodal perception for both mask and appearance The problem of generating the amodal RGB appearance for the occluded parts is highly related to semantic image completion. The latest methods Pathak et al. Yang et al. Iizuka et al. Yu et al. Zheng et al. extend GANs Goodfellow et al. and CGANs Mirza and Osindero to address this task generating new imagery to fill in partially erased image regions. However they mainly focus on object removal needing users to interactively annotate the objects to be removed.Fig. An illustration of the CSDNet framework. a Overall layer by layer completed scene decomposition pipeline. In each step the layered decomposition network selects out the fully visible objects. The completion network will complete the resultant holes with appropriate imagery. The next step starts again with the completed image. b The layered scene decomposition network estimates instance masks and binary occlusion relationships. c The completion network generates realistic content for the original invisible regionsFull size imageSeGAN Ehsani et al. involved an end to end network that sequentially infers amodal masks and generates complete RGB appearances for instances. The instance depth order is estimated by comparing the areas of the full and visible masks. PCNet  Zhan et al. used a self supervised learning approach to recover masks and content using only visible annotations. However these works mainly present results in which the ground truth visible mask is used as input and are sensitive to errors in this visible mask. While the recent work of Yan et al. Yan et al. tried to visualize the invisible from a single input image it only tackles the occluded “vehicle” category for which there is much less variation in amodal shape and RGB appearance and thus easier to model.There are two recent works that attempt to learn structural scene decomposition with amodal perception. MONet Burgess et al. combined an attention network and a CVAE  Kingma and Welling for jointly modeling objects in a scene. While it is nominally able to do object appearance completion this unsupervised method has only been shown to work on simple toy examples. Dhamo et al. Dhamo et al. utilized Mask RCNN He et al. to obtain visible masks and conducted RGBA D completion for each object. However depth values are hard to accurately estimate from a single image especially in real images without paired depth ground truths. In practice even if we use domain transfer learning for depth estimation the pixel level depth value for all objects are unlikely to be consistent in a real scene. Therefore our method uses an instance level occlusion order akin to the “.D” model Mark and Mumford to represent the structural information of a scene which is easier to be inferred and manipulated.MethodIn this work we aim to derive a higher level structural decomposition of a scene. When given a single RGB image \ \mathbf{I }\ our goal is to decompose all objects in it and infer their fully completed RGB appearances together with their underlying occlusion relationships As depicted in Fig.  . Our system is designed to carry out inmodal perception for visible structured instance segmentation and also solve the amodal perception task of completing shapes and appearances for originally occluded/invisible parts.Instead of directly predicting the invisible content and decoupling the occlusion relationships of all objects at one pass we use the fact that foreground objects are more easily identified detected and segmented without occlusion. Our CSDNet decomposes the scene layer by layer. As shown in Fig. a in each step \ s {k }\ given an image \ {\mathbf {I}}^{ s {k } }\ the layered segmentation network creates masks as well as occlusion labels for all detected objects. Those instances classified as fully visible are extracted out and the scene completion network generates reasonable content as well as visually realistic appearances for invisible regions. The completed image \ \mathbf {{\hat{I}}}^{ s {k } }\ will then be resubmitted for layered instance segmentation in the next step \ s {k}\ . This differs significantly from previous works  Ehsani et al. Dhamo et al. Burgess et al. Zhan et al. Ling et al. which do not adapt the segmentation process based on completion results.Our key novel insight is that scene completion generates completed shapes for originally occluded objects by leveraging the global scene context so that the intact objects are subsequently easier to be detected and segmented without occlusion. Conversely better segmented masks are the cornerstones to complete individual objects by precisely predicting which regions are occluded. Furthermore this interleaved process enables extensive information sharing between these two networks to holistically solve for multiple objects and produces a structured representation for a scene. This contrasts with existing one pass methods  Ehsani et al. Dhamo et al. Burgess et al. Zhan et al. Ling et al. where the segmentation and completion are processed separately and instances are handled independently. Together with the benefit of occlusion reasoning our system is able to explicitly learn which parts of the objects and background are occluded that need to be completed instead of freely extending to arbitrary shapes.Layered Scene DecompositionMask R CNN We begin by briefly reviewing the Mask R CNN network He et al. . Mask R CNN consists of two main stages. In the first stage the image is passed to a backbone network e.g. ResNet FPN Lin et al. and next to a region proposal network RPN Ren et al. to get object proposals. In the second stage the network extracts features using RoIAlign from each candidate box for passing to bounding box regression object classification and mask prediction. We refer readers to He et al. Chen et al. for details.Layered Scene Decomposition As shown in Fig.  b our layered scene decomposition network adopts the same two stage procedure. For each candidate instance it outputs a class label a bounding box offset an instance mask and a layer label. The new layer classification branch is added because our main goal is to visit the invisible where it is important to precisely estimate the originally occluded regions that need to be completed.Our proposed layer branch is a binary occlusion classifier which determines if an object is fully visible or partially occluded. Combining with the completion network it consecutively decomposes a scene layer by layer where at each step it is applied to a single RGB image derived from the counterpart scene completion step. While only binary decisions are made in each step after a full set of iterations a comprehensive layered occlusion ordering is obtained. The following parts describe how this is done looking first at the instance depth order representation followed by how occlusion head is designed.Fig. Instance depth order representation. Top images show absolute layer order  Qi et al. in different views. Bottom directed graphs give the pairwise order between objects. Note that the same chair instance has different absolute layer orders in the two viewsFull size imageInstance depth order representation Absolute layer order and pairwise occlusion order are two standard representations for occlusion reasoning in a scene Sun et al. Mark and Mumford . As shown in Fig.  the definition for our absolute layer order \ {\mathcal {L}}\ follows  Qi et al. where fully visible objects are labeled as while other occluded objects have order higher than the highest order instance occluding them see top images in Fig.  . We interpret the pairwise occlusion order matrix as a directed graph \ G \Omega W \ see bottom graphs in Fig.  where \ \Omega \ is a discrete set of all instances with number N and W is a \ N\times N\ matrix. \ W {i j}\ is the occlusion relationship of instance i to instance j. We use three numbers to encode the order — \ \{ \ occluded no relationship front\ \}\ . For example the chair instance is occluded by the table instance so the pairwise order for the chair is \ W { } \ while the pairwise order for the table is inversely labeled as \ W { } \ .Occlusion head In practice we find it hard to directly predict these instance depth orders. The absolute layer order index \ l\in {\mathcal {L}}\ cannot be predicted purely from local features in a bounding box since it depends on the global layout of all objects in a scene. Furthermore this index is very sensitive to viewpoints e.g. in Fig.  the desk instance is occluded by only one object instance in both views but the absolute layer order indices of the desk are different “” vs “”. In contrast pairwise ordering G captures the occlusion relationships between pairs of objects but all pairs have to be analyzed leading to scalability issues in the current instance segmentation network. As R CNN based system creates candidate objects this pairwise analysis requires building an unwieldy k\ \times \ k features. Even if we were to restrict these to the highest scoring detection boxes it will still be very memory intensive.We circumvent these problems as our occlusion classifier only predicts a binary occlusion label \ \{ \}\ in each step where is fully visible and is occluded following the setting of absolute layer order. During training each ground truth binary occlusion label is determined from the pairwise order of the actual objects present in the scene. The occlusion head in our layered scene decomposition network is the stacked fc layers which receive aligned features from each RoI and predicts the binary occlusion label.Decomposition Loss The multi task loss function for layered scene decomposition is defined as follows \begin{aligned} {L} \text {decomp} \sum {t }^{T}\alpha t {L} \text {cls}^t {L} \text {bbox}^t {L} \text {mask}^t {L} \text {occ}^t \beta {L} \text {seg}\nonumber \\ \end{aligned} where classification loss \ {L} \text {cls}^t\ bounding box loss \ {L} \text {bbox}^t\ mask loss \ {L} \text {mask}^t\ and semantic segmentation loss \ {L} \text {seg}\ are identical to those defined in HTC Chen et al. and \ {L} \text {occ}^t\ is the occlusion loss at the cascade refined stage t three cascade refined blocks in HTC  Chen et al. using binary cross entropy loss  Long et al. for each RoI.Visiting the Invisible by Exploring Global ContextIn our solution we treat visiting the invisible as an image completion  Pathak et al. problem. As illustrated in Fig.  in step \ s {k }\ after removing the fully visible instances the given image \ {\mathbf {I}}^{ s {k } }\ is degraded to become \ {\mathbf {I}} m^{ s {k } }\ . Our main goal is to generate appropriate content to complete these previously invisible regions. Unlike existing methods that complete each object independently  Ehsani et al. Dhamo et al. Burgess et al. Zhan et al. Ling et al. our model completes multiple objects in each step layer by layer such that the information from earlier scene completions propagate to later ones. The global scene context is utilized in each step.PICNet We first briefly review PICNet Zheng et al. in our completion network. PICNet has two parallel pipelines. The top reconstructive pipeline combines information from both visible and invisible regions which is used only for training. The bottom pipeline infers the invisible regions’ distribution based on visible information which aims to provide multiple and diverse results for this highly subjective problem. The two pipelines share identical weights for the encoder and decoder structures. We refer readers to Zheng et al. for details.To visit the invisible it is critical to know which parts are invisible and therefore need to be completed. The general image completion methods typically expect masks that have fixed shapes or are interactively drawn by users which differs from our goal that automatically models occlusion and completion. Recent related works  Ehsani et al. Zhan et al. Ling et al. depend on the ground truth visible masks as input to indicate which parts are occluded. In contrast our system selects out fully visible objects and automatically learns which parts are occluded in each step. The holes left behind explicitly define the occluded regions for remaining objects and thus the completed shapes for remaining objects must be deliberately restricted to these regions instead of being allowed to grow freely using only the predicted visible masks. While the occluded objects may indeed have diverse shapes and appearances as addressed in PICNet in this work to simplify the task and focus on our main objectives we only want the completed result closest to the ground truth. Therefore we use the encoder decoder architecture and eschew the random sampling aspect in original PICNet.Fig. Amodal instance segmentation results on the COCOA validation set. Our model trained on synthetic dataset Ours syn achieves visually reasonable results in the similar real indoor scenes example in top row but it fails in some dissimilar real scenes example in bottom row . After training on the real data with “pseudo ground truths” the model Ours Real performs much better. Note that unlike the PCNet  Zhan et al. that needs visible inmodal ground truth masks as input our system decomposes a scene using only an RGB image Color figure online Full size imageCompletion Loss The overall scene completion loss function is given as \begin{aligned} {L} \text {comp} \alpha \text {rec}{L} \text {rec} \alpha \text {ad}{L} \text {ad} \alpha \text {per}{L} \text {per} \end{aligned} where reconstruction loss \ {L} \text {rec}\ and adversarial loss \ {L} \text {ad}\ are identical to those in PICNet Zheng et al. . The perceptual loss \ {L} \text {per} {\mathbf {F}}^{ l } \mathbf {{\hat{I}}}^{ s {k} } {\mathbf {F}}^{ l } {\mathbf {I}}^{ s {k} } \ Johnson et al. based on a pretrained VGG Simonyan and Zisserman is the \ l \ distance of features \ {\mathbf {F}}\ in l th layer between the generated image \ \mathbf {{\hat{I}}}^{ s {k} }\ and ground truth \ {\mathbf {I}}^{ s {k} }\ .Inferring Instance Pairwise Occlusion OrderAs discussed in Sect. . absolute layer order \ {\mathcal {L}}\ is sensitive to errors. If one object is incorrectly selected as a front object objects behind it will have their absolute layer order incorrectly shifted. Hence in keeping with prior works  Ehsani et al. Zhan et al. we use the pairwise occlusion order \ G \Omega W \ for final evaluation.Given a single image \ {\mathbf {I}}\ our model decomposes it into instances with completed RGB appearances \ A \Omega ^{S K}\ . Here A denotes the amodal perception instance inclusive of both mask and appearance \ \Omega \ specifies instances in the scene and \ S K\ indicates which layers are the instances in selected out in step \ s k\ . When two segmented amodal masks \ A {\omega i}^{s i}\ and \ A {\omega j}^{s j}\ overlap we infer their occlusion relationship based on the order of the object removing process formally \begin{aligned} W {\omega i \omega j} \left\{ \begin{array}{ll} {} \hbox { if}\ O A {\omega i}^{s i} A {\omega j}^{s j} \\ {} \hbox {if} O A {\omega i}^{s i} A {\omega j}^{s j} \hbox {and} s i s j\\ {} \hbox {if} O A {\omega i}^{s i} A {\omega j}^{s j} \hbox {and} s i\ge s j\\ \end{array} \right. \end{aligned} where \ O A {\omega i}^{s i} A {\omega j}^{s j} \ is the area of overlap between instances \ \omega i\ and \ \omega j\ . If they do not overlap they share no pairwise depth order relationship in a scene. If there is an overlap and the instance \ \omega i\ has a smaller layer order the inferred pairwise order is \ W {\omega i \omega j}\ otherwise it is labeled as \ W {\omega i \omega j}\ . Hence the instance occlusion relationship only depends on the order selected out step of removal between the two instances and do not suffer from shift errors.Training on Real Data with Pseudo Ground truthReal world data appropriate for a completed scene decomposition task is difficult to acquire because ground truth shapes and RGB appearance for occluded parts are hard to collect without extensive manual interventions e.g. deliberate physical placement and removal of objects. Although our proposed model trained on the high quality rendered data achieves visually reasonable results in some real scenes that share similarities to the rendered dataset e.g. the scene in the top row of Fig.  it does not generalize well to dissimilar real scenes e.g. the scene in the bottom row of Fig.  . These are caused by differences in categories between synthetic and real datasets especially between indoor and outdoor scenes e.g. motorbikes do not appear in our synthetic dataset and inconsistencies between synthetically trained image completion of masked regions and fully visible real pixels e.g. the missing corner of the table in Fig. .One alternative is to simply use an image completion network trained only on real images. From our experience this performs poorly in a scene decomposition task. The reason is that while real trained image completion methods are able to create perceptually pleasing regions and textures for a single completion they do not appear to have the ability to adhere to consistent object geometry and boundaries when de occluding which is crucial for object shape completion.Fig. Pseudo RGB ground truth. a Input. b Masked image by selecting out the fully visible objects. c Pseudo ground truth generated from our model trained on synthetic dataFull size imageFig. Training pipeline for real images. We introduce a semi supervised learning method for real data by providing pseudo RGB ground truth for originally invisible regionsFull size imageTable Dataset statistic comparison. “Occludees” denotes the occluded objects.Full size table Fig. Our rendered dataset. a High quality rendered RGB images. b Semantic map and instance annotations with bbox category ordering and segmentation map. c Intact RGBA ground truth for instances and backgroundFull size image Our key motivating insight is this instead of training the model entirely without ground truth in the completion task we train it in a semi supervised learning manner exploiting the scene structure and object shape knowledge that has been gained in our synthetically trained CSDNet. As shown in Fig.  when the fully visible instances are correctly segmented out by the real decomposition network this synthetic completion model is able to generate visually adequate appearance but more importantly it is better able to retain appropriate geometry and shapes. We can use this to guide the main image completion process in real word data while allowing a GAN based loss to increase the realism of the output.Specifically for a real image \ {\mathbf {I}}\ we first train the layered decomposition network using the manual annotated amodal ground truth labels. In a step after segmenting and selecting out the foreground objects we obtain \ {\hat{\mathbf {I}}} {syn}^{ s k } G {\mathbf {I}} m^{ s k } \theta {\text {syn}} \ to serve as “pseudo ground truth” green box in Fig.  through the completion model trained on synthetic data. We then train the completion network \ G {\mathbf {I}} m^{ s k } \theta {\text {real}} \ using the loss function of quation by comparing the output \ {\hat{\mathbf {I}}} {real}^{ s k }\ to “pseudo ground truth” \ {\hat{\mathbf {I}}} {syn}^{ s k }\ . It is worth noticing that the completed image is passed back to the layered decomposition network in the next layer where the decomposition loss \ {L} \text {decomp}\ in equation will be backpropagated to the completion network. This connection allows the completion network to learn to complete real world objects that might not be learned through the synthetic data.Synthetic Data CreationLarge datasets with complete ground truth appearances for all objects are very limited. Burgess et al. Burgess et al. created the Objects Room dataset but only with toy objects. Ehsani et al. Ehsani et al. and Dhamo et al. Dhamo et al. rendered synthetic datasets. However the former only includes rooms with most viewpoints being atypical of real indoor images. The latter’s OpenGL rendered dataset appears to have more typical viewpoints with rich annotations but the OpenGL rendered images have low realism. We refer readers to Ehsani et al. and Dhamo et al. for details of image quality. Recently Zhan et al. Zhan et al. explored the amodal completion task through self supervised learning without the need of amodal ground truth. However a fair quantitative comparison is not possible as no appearance ground truth is available for invisible parts.To mitigate this issue we rendered a realistic dataset with Maya Autodesk . In Table  we compare our proposed CSD dataset with other amodal datasets i.e. COCOA Zhu et al. COCOA cls Follmann et al. DS Follmann et al. KINS Qi et al. DYCE Ehsani et al. and OLMD Dhamo et al. considering the number of images instances and “occludees”. Note that for the real scenes the “ground truth” amodal masks were manually guessed by human annotators and there is no visually complete RGB ground truth for occluded instances nor the background.Data Rendering Our rendered data is based on a total of .k views inside over k rooms D CAD models from SUNCG Song et al. with various room types and lighting environments see Fig. a . To select the typical viewpoints we first sampled many random positions orientations and heights for the camera. Only when a view contains at least objects will we render the image and the corresponding ground truth of each instance. To avoid an excessive rendering workload we separately rendered each isolated object as well as the empty room as shown in Fig. c. This allows us to then freely create the ground truths of each layer by compositing these individual objects and background using the instance occlusion order.Data Annotation Each rendered scene is accompanied by a global semantic map and dense annotations for all objects. As shown in Table  there are images encompassing labeled instances in our CSD. On average there are individual objects per view. Among these \ .\%\ instances are partially occluded by other objects and the average occlusion ratio average IoU between two objects is \ .\%\ . More importantly the intact RGB appearances for all instance are provided as well as categories the classes in NYUDv Nathan Silberman et al. bounding boxes and masks for complete objects as well as for only the visible regions shown in Figs. b and  c . Furthermore the absolute layer order and pairwise occlusion order shown in Fig.  are also defined in our rendered dataset.ExperimentsSetupDatasets We evaluated our system on three datasets COCOA  Zhu et al. KINS  Qi et al. and the rendered CSD as shown in Table  and Fig. . COCOA is annotated from COCO Lin et al. a large scale natural image datasets in which images are selected to manually label with pairwise occlusion orders and amodal masks. KINS is derived from the outdoor traffic dataset KITTI in which images were labeled with absolute layer orders and amodal masks. CSD is our rendered synthetic dataset where we conducted thorough experiments and ablation studies to assess the quality of the completed results for invisible appearance estimation since the in the wild datasets lack ground truth for the occluded parts .Metrics For amodal instance segmentation we report the standard COCO metrics Lin et al. including AP average over IoU thresholds \ \text {AP} {}\ \ \text {AP} {}\ and \ \text {AP} {S}\ \ \text {AP} {M}\ and \ \text {AP} {L}\ AP at different scales and AR average recall over IoU . Unless otherwise stated the AP and AR is for mask IoU. For appearance completion we use the RMSE SSIM and PSNR metrics to evaluate the quality of generated images. All images are normalized to the range \ \left \right \ .Since the occlusion order is strongly related to the quality of instance segmentation higher quality masks are easier to order Zhu et al. previous works Ehsani et al. Zhan et al. directly utilize the ground truth visible mask as input during testing to segment the object for instance depth ordering. However precise visible instance segmentation remains hard to achieve in real applications. Following Zhu et al. we evaluate the pairwise instance depth ordering for those correctly detected instances. Specifically given a pairwise occlusion order \ G \Omega W \ predicted by the model we only evaluate the order for valid instances that have IoU with ground truth masks over a given threshold. For instance if we set the threshold as . the predicted instance \ \omega \ will only be evaluated when we can identify a matched ground truth mask with \ \text {IoU}\ge .\ . Hence we can measure the occlusion average precision OAP across different thresholds. As result we can analyze the influence of amodal instance segmentation quality on the accuracy of instance depth ordering as shown in Tables  and . Note that the originally presented metric in Zhu et al. for instance pairwise depth ordering is a special case of our proposed OAP using only the IoU threshold of ..Training Except for the newly added weight \ \alpha {per} .\ we set other hyper parameters following the baseline works of HTC Chen et al. and PICNet Zheng et al. . Although these choices were based on their respective tasks in isolation we found that they were also suitable for our jointly optimized system.Fig. Layer by Layer Completed Scene Decomposition results on rendered CSD testing set. a Input RGB images. b Final amodal instance segmentation. c Inferred directed graph for pairwise order. d Columns labeled S show the decomposed instances top and completed scene bottom based on the predicted non occluded masks. Note that the originally occluded invisible parts are filled in with realistic appearanceFull size imageThe synthetic model is trained in three phases the layered scene decomposition network Fig. b is trained with loss \ {L} {decomp}\ for epochs where at each layer re composited layered ground truths are used as input. Separately the completion network Fig. c is trained with loss \ {L} {comp}\ for epochs wherein the ground truth layer orders and segmented masks are used to designate the invisible regions for completion. Both decomposition and completion networks are trained jointly for epochs without relying on ground truths as input at any layer Fig. a . Doing so allows the scene decomposition network to learn to cope with flaws e.g. texture artifacts in the scene completion network and vice versa. For each scene the iteration ends when no more objects are detected or a maximum iterations is reached.The training on real data only involved phases and as no ground truth appearances are available for the invisible parts. The layered decomposition network is trained only for one layer original image in phase due to no re composed ground truth images. Since phase does not rely on ground truth as input we trained it layer by layer on real images by providing the “pseudo ground truth” appearances to calculate the reconstruction loss. To reduce the effect of progressively introduced artifacts in image completion we used full bounding boxes detected in the first layer as proposals for remaining decomposition steps.Inference During testing fully visible instances are selected and assigned an absolute layer order corresponding to the step index \ s k\ . In each layer the decomposition network selects the highest scoring detected boxes for mask segmentation and non occlusion predication. As we observed that higher object classification scores provided more accurately segmented boundaries we only select non occluded objects with high classification scores and non occlusion scores thresholds of . for synthetic images and . for real images among these candidates. We further observed that in some cases we detected multiple objects with high object classification confidences yet none were classified as fully visible due to low non occlusion scores especially in complex scenes with steps larger than . We will then choose the instance with the highest non occlusion score so that at least one object is selected at each layer. When no objects are detected the iteration stops.Table Amodal Instance Segmentation on CSD testing sets.Full size tableTable Instance depth ordering on CSD testing sets.Full size tableResults on Synthetic CSD DatasetWe first present results that we obtained from our framework when experimenting on our synthetic CSD dataset.Main ResultsCompleted scene decomposition We show the qualitative results of CSDNet in Fig. . Given a single RGB image the system has learned to decompose it into semantically complete instances e.g. counter table window and the background wall floor and ceiling while completing RGB appearance for invisible regions. Columns labeled S show the completed results layer by layer. In each layer fully visible instances are segmented out and after scene completion some previously occluded regions become fully visible in the next layer e.g. the table in the second example. The final amodal instance segmentation results shown in Fig. b consist of the fully visible amodal masks in each layer. Note that unlike MONet  Burgess et al. our model does not need predefined slots. The process will stop when it is unable to detect any more objects.Amodal instance segmentation We compare CSDNet to the state of the art methods in amodal instance segmentation in Table . As the existing works Mask RCNN  He et al. and HTC  Chen et al. are aimed at inmodal perception for visible parts we retrained their models for amodal perception task by providing amodal ground truths. We also retrained MLC  Qi et al. on our rendered dataset which are the latest work for amodal perception. For PCNet  Zhan et al. we used the predicted visible mask as input rather than the original visible ground truth annotations. While the HTC  Chen et al. improves on Mask RCNN’s  He et al. bounding box AP by about . points by refining the bounding box offsets in three cascade steps the improvement for amodal mask segmentation is quite minor at . points. We believe this is an inherent limitation of methods that attempt to directly estimate the complete shape of occluded objects without first reasoning about occluding objects and masking their image features as the features of occluding objects end up being involved in the classification of occluded objects which will distract the network. In contrast our CSDNet is able to improve the amodal mask segmentation accuracy by a relative \ .\%\ with the same backbone segmentation network HTC by jointing segmentation and completion with layer by layer decomposition.Table Object Completion.Full size tableTo further demonstrate that better completed images improve amodal segmentation we consider a scenario with a completion oracle by using ground truth appearances to repair the occluded parts in each step. This is denoted as the CSDNet gt for which amodal instance segmentation accuracy increases from \ .\%\ to \ .\%\ relative \ .\%\ improvement . We also note that while the CSDNet gt using Mask RCNN achieves lower bounding box score than our HTC CSDNet “.” vs “.” the mask accuracy is much higher “.” vs “.” . This suggests that amodal segmentation benefits from better completed images.Instance depth ordering We compare CSDNet to the state of the art instance depth ordering in Table . The original SeGAN and PCNet used ground truth visible masks \ \text {V} {gt}\ as input during testing. For a fair comparison we first retrained them on our synthetic data using the same segmentation network HTC Chen et al. for all models. After predicting amodal masks we assessed various instance depth ordering algorithms two baselines proposed in AmodalMask Zhu et al. of ordering by areaFootnote and by y axis amodal masks closest to image bottom in front ordering by incremental area defined as the IoU area between visible and amodal masksFootnote and our ordering by absolute layer order Sect. . .As can be seen in Table  all instantiations of our model outperformed baselines as well as previous models. Unlike SeGAN  Ehsani et al. and PCNet Zhan et al. our final model explicitly predicts the occlusion labels of instances which improved the OAP substantially. While MLC Qi et al. predicts the instance occlusion order in a network it only contains one layer for binary occlusion / non occlusion labeling. In contrast our method provides a fully structural decomposition of a scene in multiple steps. Additionally we observed that our model achieves better performance with a higher IoU threshold for selecting the segmented mask closer match to the ground truth masks . This suggests that the instance depth ordering is highly related to the instance segmentation quality and thus it is important to use our proposed OAP for multi threshold evaluation. We further observed that the occlusion relationships of small objects are difficult to infer in our method. However the Y axis ordering method had similar performance under various metrics as it only depends on the locations of objects. Note that our depth ordering does not rely on any ground truth labels that are used in Ehsani et al. Zhan et al. .Object completion We finally evaluated the quality of generated appearances. We compared our results to those from SeGAN Ehsani et al. Dhamo et al. Dhamo et al. abbrev. as DNT PCNet Zhan et al. and PICNet Zheng et al. original point attention without perceptual loss in Table . We evaluated different conditions of C when the ground truth full mask \ \text {F} {gt}\ is provided to all methods Ca when the ground truth visible mask \ \text {V} {gt}\ is the input to SeGAN and PCNet and Cb when an RGB image is the only input to other methods. Ca \ \text {V} {gt}\ is considered because SeGAN and PCNet assumes that a precisely predefined visible mask is provided as input.In C \ \text {F} {gt}\ CSDNet substantially outperformed the other methods. In C even when given only RGB images without ground truth masks our method worked better than SeGAN and PCNet with \ \text {V} {gt}\ . One important reason for the strong performance of CSDNet is the occlusion reasoning component which constraints the completed shapes of partly occluded objects based on the global scene context and other objects during testing.Qualitative results are visualized in Fig. . We noted that SeGAN worked well only when ground truth amodal masks \ \text {F} {gt}\ were available to accurately label which parts were invisible that needed filling in while DNT generated blurry results from simultaneously predicting RGB appearance and depth maps in one network which is not an ideal approach  Zamir et al. . The PCNet Zhan et al. can not correctly repair the object shape as it trained without ground truth object shape and appearance. Our CSDNet performed much better on background completion as it only masked fully visible objects in each step instead of all objects at a go so that earlier completed information propagates to later steps.Fig. Results for Visiting the Invisible. We show the input image our amodal instance segmentation results and the objects and background we try to visit. The red rectangles highlight the previously invisible regions of occluded objectsFull size imageFig. Ablations with different numbers of objects and objects in different layers. Left Amodal instance segmentation accuracy for the SOTA models presented in Table . These models are trained on our synthetic CSD dataset with true ground truth for originally occluded regions. Right Amodal instance segmentation recall for the SOTA models presented in Table . These models were trained on the COCOA dataset with masks manually guessed by human annotators for occluded regions. Amodal segmentation is more difficult for hierarchically deeper occlusion relationships. The “ ” numbers in the X axis indicate the number of testing instances in each settingFull size imageAblation StudiesWe ran a number of ablations on different components in CSDNet. Results are shown in Fig.  and Tables  and .Number of objects per image. We first investigated the performance of state of the art methods on various numbers of objects in each image. As shown in Fig.  left when there are more objects in a scene the amodal instance segmentation accuracy will decrease gradually due to the more complex occlusion relationships and larger occlusion regions. The figure demonstrates that our decomposition network benefits from the completion network when there are fewer layers of occlusion e.g. noting the large improvement when the number of objects is less than . However when there are too many objects in a scene e.g. leading to more than hierarchical occlusion layers the improvement will be smaller due to accumulated completion errors over the layers.Amodal segmentation in different layers. In this setting we selected the ground truth instances based on the absolute layer value for the evaluation. As shown in Fig.  right the recall rate dramatically decreases average . AR with the occlusion regions being heavier in the deeper layer which supports our main claim that in terms of amodal instance segmentation foreground objects are generally easier to be identified detected and segmented than occluded objects even for real objects and scenes. Besides while the MLC HTC Qi et al. shows better performance than our CSDNet Mask R CNN in the first layer its recall value decreases more for subsequent occluded objects. This further highlights the division of labor in our approach where the layer decomposition network explicitly predicts the occluded regions and the later completion network infers reasonable content.Maximum iterations Table a shows mask AP results over different iterations during testing. We noted that the amodal instance segmentation results benefit from the completed images when occlusion layers are limited less than . However when we conducted a deeper decomposition the accumulated errors from wrong segmentation and unrealistic completion in previous steps will lead to worse detection and segmentation results downstream with some noise patches classified as objects.Occlusion classifier branch Modeling the full scene structure directly is challenging. Instead our occlusion classifier solely performs binary classification for each detected instance using features from the full bounding box. In particular if features from neighboring objects extend into a bounding box our occlusion branch just needs to classify whether the selected object is being blocked “occluded” or not “front” . In table b we compare FCNs and MLP under the same HTC architecture. The MLP achieves much better accuracy by directly modeling the global relationship in the extracted features.Does better completion help decomposition We show quantitative results for a fixed decomposition network layered HTC Chen et al. with two completion methods in Table a. Without any completion “w/o” segmented results were naturally bad “.” vs “.” as it had to handle empty masked regions. More interestingly even if advanced methods were used to generate visual completion the isolated training of the decomposition and completion networks led to degraded performance. This suggests that even when generated imagery looks good visually there is still a domain or semantic gap to the original visible pixels and thus flaws and artifacts will affect the next segmentation step. The PICNet with patch attention provides better completed results than the original point attention PICNet Zheng et al. resulting in a large improvement “.” vs “.” of amodal instance segmentation.Does better decomposition help completion To answer this we report the results of using different scene segmentation networks with a same completion network Patch attention PICNet  Zheng et al. in Table b. We also first considered the ideal situation that ground truth segmentation masks were provided in each decomposition step. As shown in Table b the completion quality significantly improved RMSE “.” SSIM “.” and PSNR “.” as occluded parts were correctly pointed out and the completion network precisely knows which parts need to be completed. HTC  Chen et al. provided better instance masks than Mask RCNN  He et al. which resulted in more accurately completed scene imagery. The best results were with end to end jointly training.Table Ablations on CSDNet.Full size tableTable Ablations for joint optimization.Full size table Fig. Layer by layer completed scene decomposition on natural images. a Inputs. b Final amodal instance segmentation. c Inferred directed graph for pairwise occlusion order. d Columns labeled S show the decomposed instances with completed appearance in each stepFull size image Table Amodal Instance Segmentation on COCOA and KINS sets.Full size table Results on Real DatasetsWe now assess our model on real images. Since the ground truth appearances are unavailable we only provide the visual scene manipulation results in Sect. . instead of quantitative results for invisible appearance completion.Completed scene decomposition. In Fig.  we visualize the layer by layer completed scene decomposition results on real images. Our CSDNet is able to decompose a scene into completed instances with correct ordering. The originally occluded invisible parts of “suitcase” for instance is completed with full shape and realistic appearance. Note that our system is a fully scene understanding method that only takes an image as input without requiring the other manual annotations as   Ehsani et al. Zhan et al. .Amodal instance segmentation. Next we compare with state of the art methods on amodal instance segmentation. Among these AmodalMask  Zhu et al. and ORCNN Follmann et al. were trained for the COCOA dataset MLC  Qi et al. worked for the KINS dataset and PCNet  Zhan et al. focused on amodal completion rather than amodal segmentation. For a fair comparison when these methods do not provide results on a given dataset we trained their models using publicly released code. We also conducted an ablation on training the system denoted as “CSDNet syn” which uses the completion network trained on the synthetic dataset directly instead of retraining another completion network with “pseudo ground truth”.Table  shows that our results . mAP and . mAP are . points and . points higher than the recent MLC using the same segmentation structure HTC in COCOA and KINS respectively. PCNet  Zhan et al. considers amodal perception in two steps and assumes that visible masks are available. We note that their mAP scores were very high when the visible ground truth masks were provided. This is because all initial masks were matched to the annotations without detection and segmentation errors for instances as shown in Fig.  . However when we used a segmentation network to obtain visible masks \ \hat{\text {V}} {pred}\ for PCNet the amodal instance segmentation results became lower than other methods. We also found that without jointly optimizing the completion network the CSDNet syn resulted in a severe reduction in amodal mask AP around points . This suggests that while the pre trained completion network can copy similar appearance from visible regions it is inadequate to generate appropriate geometry and shapes for all instances especially for those categories that do not exist in the synthetic dataset.Fig. Amodal instance segmentation results on natural images. Our CSDNet learns to predict the intact mask for the occluded objects e.g. animals and human . Note that unlike PCNet  Zhan et al. our model does not depend on the visible mask first row as input. Hence it can handle some objects without ground truth annotation such as two ‘humans’ in the third column and the ‘smartphone’ in the fourth columnFull size imageTable Instance depth ordering on COCOA and KINS sets.Full size table Fig. Free editing based on the results of our system on images from various datasets. Note that our method is able to automatically detect segment and complete the objects in the scene without the need for manual interactive masking with interactive operations limited to only “delete” and “drag and drop”. The blue arrows show object removal while red arrows show object moving operations. We can observe that the originally invisible regions are fully visible after editingFull size image In Fig.  we compare our CSDNet and PCNet  Zhan et al. . PCNet only completes the given visible annotated objects which had visible masks. In contrast our CSDNet produces more contiguous amodal instance segmentation maps even for some unlabeled objects for instance the two “humans” in the third column. Furthermore our model correctly predicts the amodal masks for the “cow” in the first column while the ground truth labels are definitely wrong. On the other hand our model can directly create a deep hierarchical representation of a scene producing a layer order for each instance.Instance depth ordering Finally we report the instance depth ordering results in Table . In order to compare with existing work we considered two settings ground truths provided blue rows in Table  and only RGB images given. The OrderNet obtained the best results as the ground truth full masks \ \text {F} {gt}\ were given. We note that PCNet and our model achieved comparable performance when the visible ground truth masks were used. Note that we only used \ \text {V} {gt}\ for depth ordering while PCNet utilized the visible mask as input for both mask completion and depth ordering. Furthermore when no ground truth annotation was provided as input our model performed better than MLC and PCNet.ApplicationsWe illustrate some image editing / re composition applications of this novel task after we learned to decompose a scene into isolated completed objects together with their spatial occlusion relationships. In Fig.  we visualize some recomposed scenes on various datasets including our CSD real COCOA Zhu et al. and KITTI Qi et al. .In these cases we directly modified the positions and occlusion ordering of individual objects. For instance in the first bedroom example we deleted the “window” and moved the “bed” and the “counter” which resulted in also modifying their occlusion order. Note that all original invisible regions were filled in with reasonable appearance. We also tested our model on real NYUD v  Nathan Silberman et al. images which do not belong to any of the training sets used. As shown in the last column of Fig.  our model was able to detect and segment the object and complete the scene. The “picture” for instance is deleted and filled in with background appearance.ConclusionsBuilding on previous inmodal and amodal instance perception work we explored a higher level structure scene understanding task that aims to decompose a scene into intact semantic instances with completed RGB appearance and spatial occlusion relationships. We presented a layer by layer CSDNet an iterative method to address this novel task. The main motivation behind our method is that fully visible objects at each step can be relatively easily detected and selected out without concern about occlusion. To do this we simplified this complex task to two subtasks instance segmentation and scene completion. We analyzed CSDNet and compared it with recent works on various datasets. Experimental results show that our model can handle an arbitrary number of objects and is able to generate the appearance of occluded parts. Our model outperformed current state of the art methods that address this problem in one pass. The thorough ablation studies on synthetic data demonstrate that the two subtasks can contribute to each other through the layer by layer processing. Notes.We used the heuristic in PCNet Zhan et al. — larger masks are ordered in front for KINS and behind for COCOA and CSD..See details in Zhan et al. where the visible ground truth masks \ \text {V} {gt}\ are used for ordering.