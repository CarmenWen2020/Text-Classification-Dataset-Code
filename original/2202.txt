Multiple Object Tracking (MOT) has attracted increasing interests in recent years, which plays a significant role in video analysis. MOT aims to track the specific targets as whole trajectories and locate the positions of the trajectory at different times. These trajectories are usually applied in Action Recognition, Anomaly Detection, Crowd Analysis and Multiple-Camera Tracking, etc. However, existing methods are still a challenge in complex scene. Generating false (impure, incomplete) tracklets directly affects the performance of subsequent tasks. Therefore, we propose a novel architecture, Siamese Bi-directional GRU, to construct Cleaving Network and Re-connection Network as trajectory post-processing. Cleaving Network is able to split the impure tracklets as several pure sub-tracklets, and Re-connection Network aims to re-connect the tracklets which belong to same person as whole trajectory. In addition, our methods are extended to Multiple-Camera Tracking, however, current methods rarely consider the spatial-temporal constraint, which increases redundant trajectory matching. Therefore, we present Position Projection Network (PPN) to convert trajectory position from local camera-coordinate to global world-coodrinate, which provides adequate and accurate temporal-spatial information for trajectory association. The proposed technique is evaluated over two widely used datasets MOT16 and Duke-MTMCT, and experiments demonstrate its superior effectiveness as compared with the state-of-the-arts.

Access provided by University of Auckland Library

Introduction
Multiple Object Tracking (MOT) is an important task in video surveillance analysis, which aims to locate the position of targets and associate specific targets as whole trajectories at all times, whose outputs (trajectories) are commonly utilized for Action Recognition, Anomaly Detection, Human Behavior Analysis, Crowd Analysis and Multiple-Camera Tracking, etc. However, MOT task still has many difficulties, for example, partial or long-term occlusion deteriorates the description of targets, which affects continuity of trajectory, some tracklets are split as several sub-tracklets when the tracklet is occluded by others; Frequent occlusion and cross-motion in crowd scene usually cause the neighboring target occludes the tracked-target, the target is gradually replaced by another target, and then the tracklet contains one more targets which make the tracklet impure. Furthermore, generating false (impure or incomplete) tracklets directly influences the subsequent tracklet-based tasks, such as Action Recognition and Multiple-Camera Tracking. Therefore, we focus on trajectory post-processing strategy to cleave the impure tracklets and re-connect the same tracklets. In addition, our strategy applies not only in Single Camera Tracking, it is also extended to Multiple-Camera Tracking. However, for trajectory matching in multiple-camera tracking, the existing methods take less consideration on the positional relationship between cameras, which causes redundant trajectory matching process. As a result, the redundant matching affects the calculating efficiency and increases mis-matching probability of trajectory. In this paper, we present multiple stages Multiple-Camera Multiple Object Tracking (MCMOT) framework, which are divided into three parts (as shown in Fig. 1): 1. Single Camera Multiple Object Tracking; 2. Trajectory Position Projection; 3. Temporal-Spatial Constraint Trajectory Matching.

Single Camera Multiple Object Tracking (MOT) aims to identify each object and predict their trajectories in a single camera video sequence. MOT based methods address this problem by data association, which jointly optimize the matching process of bounding boxes detected by a detector within the inter-frames of a sequence. The same individual has regular temporal or spatial cues in video. For example, a person has slight appearance, velocity and direction changes. Therefore, MOT usually depends on the combination of multiple cues (e.g. appearance, motion and interactions) to associate the similar bounding boxes. Although the performance is gradually improving at the MOT Challenges Milan et al. (2016), the effectiveness of MOT is still limited by object detection quality, long-term occlusion and scene complexity. To solve this sophisticated problem, we intend to extract discriminative features, and design more effective association metrics for MOT.

Tracking-by-detection is a dominant solution for MOT, which links similar objects into trajectory by associating their feature representation and bounding box position. Tracking-by-detection is to search the optimal assignment from multiple cues within a set of bounding boxes. For example, the appearance and motion of person are discriminative cues for data association. Currently, deep networks have achieved significant performance improvement in MOT Tang et al. (2017); Chu et al. (2017); Sadeghian et al. (2017). However, some difficulties remain unresolved, such as “mis-tracking” and “lost-tracking”. As shown in Fig. 6, a tracked person is gradually occluded by another person, which causes mis-tracking. As shown in Fig. 7 , a tracklet is split into several fragments, long-term occlusion leads to lost-tracking. Thus trajectory post-processing becomes particularly important for multiple-camera tracking.

Fig. 1
figure 1
The Framework of Multiple-Camera Multiple Object Tracking: 1. Single camera multiple object tracking with tracklet cleaving and re-conection. 2. Position Projection for each trajectory. 3. Cross-camera trajectories matching based on temporal-spatial constraint

Full size image
Multiple-Camera Multiple Object Tracking focuses on associating trajectories from different cameras’ tracking results (illustrated as Fig. 2). Duke-MTMCT Ristani et al. (2017) is a typical example for MCMOT task, which includes 8 cameras in different locations and viewpoints. The existing methods of trajectory matching overly depend on extracting tracklet appearance features, which is regarded as Person Re-identification task in general. However, due to the difference of illumination, the angle of camera and body posture, appearance features are not sufficient to match trajectory. It increases huge amount of operations to match each trajectory one by one and improves the probability of mis-matching. Therefore, incorporating temporal-spatial information eliminates redundant amount of matching operations, and improves matching accuracy.

Fig. 2
figure 2
The illustration of Multiple-Camera Multiple Object Tracking (MCMOT) task. The Tracking Dataset is from Duke-MTMCT, which includes 8 cameras in different location and view, the middle bottom image indicates the overall map

Full size image
Temporal-spatial information contains position, velocity, timestamp and camera ID. The information is utilized for predicting the trajectory position in the future and then narrowing the search area. The relative locations between cameras are also conducive to matching trajectories. Due to the differences of camera locations and viewpoints, we have to correlate the trajectory positions between camera and actual scenario. For example, as shown in Fig. 2, both camera No.5 and No.7 are adjacent. When the person disappears from the bottom of the camera No.5, it will most likely appear from the left side of the camera No.7. For previous methods, to correlate cameras, traditional method Jiang et al. (2018) constructed a topological graph based on camera locations and viewpoints, which needs to calibrate each camera. However if some cameras are changed, they have to re-calibrate and re-construct the topological graph. Therefore, it is necessary to convert the trajectory position to the uniform coordinate such as world-coordinate.

In this paper, we construct multi-stage framework to reduce the number of mis-tracking and lost-tracking. Our proposal consists of several independent modules which includes tracking module and post-processing module. We propose a novel architecture, Siamese Bi-directional GRU (SiaBiGRU) for trajectory post-processing. Based on the SiaBiGRU, we design Cleaving Network to check the purity of tracking and split impure tracklets, and address Re-conection Network to link sub-tracklets as trajectory. Additionally, in order to verify whether our post-processing model can improve the effect of subsequent tasks, we select multiple-camera tracking as subsequent task. In trajectory matching phase, we construct a Position Projection Network (PPN) to convert the trajectory location from camera-coordinate to world-coordinate. Finally, we reduce the search range according to the trajectory motion prediction in world-coordinates and associate the trajectories from different cameras by their general appearance features. The proposed method is divided into three steps as illustrated in Fig. 1: (1.Single Camera Multiple Object Tracking, 2.Trajectory Position Projection, 3.Multiple Camera Trajectory Matching), where Single Camera Tracking (the first step) includes three sub-steps: (a.Tracklet Generation, b.Tracklet Cleaving, c.Tracklet Re-connection, as shown in Fig. 3). Our contributions in this paper are shown as follows:

We design a novel multiple-stages framework for Multiple-Camera Multiple Object Tracking (MCMOT) which includes “trajectory processing” in single camera and temporal-spatial based trajectory association in multiple-camera scene.

For post-processing, we propose a novel Siamese Bi-directional GRU (SiaBiGRU) to cleave the impure tracklets into sub-tracklets and re-connect these sub-tracklets according to their similarity.

For cross-camera trajectory matching, we present a Position Projection Network, which effectively leverages temporal-spatial information by converting the trajectories location from camera-coordinate to world-coordinate.

The proposed model greatly reduces the amount of trajectory matching and further decreases the number of mis-matching. Experiments demonstrate its superior effectiveness and robustness over the state-of-the-arts in MOT Benchmark.

Related Work
Single Camera Multiple Object Tracking
Single Camera Multiple Object Tracking in videos has attracted great attention. Single camera MOT generates trajectories corresponding to each object in a video sequence that is captured by a single camera. The main strategy is to guide object tracking by detection. For example, Tang et al. (2017); Xiang et al. (2016); Choi (2015); Kim et al. (2015); Chen et al. (2017) focus on designing an ingenious data association or multiple hypothesis. Schulter et al. (2017); Levinkov et al. (2017); Maksai et al. (2017) rely on network flow and graph optimization which are powerful approaches for tracking. Bergmann et al. (2019) exploited the bounding box regression of an object detector to predict the position of an object to convert a detector into a Tracktor. Liu et al. (2020) proposed a novel graph representation that takes both the feature of individual object and the relations among objects into consideration. Xiang et al. (2020) presented an end-to-end conditional random field within a unified deep networks. The inter-relation of targets has multiple cues in a sequence including appearance, motion and interaction, which are summarized by Sadeghian et al. (2017). Some scholars have carried out research on tracking cluster and post-processing to improve the tracking performance. For example, Zhang et al. (2020) constructed motion evaluation network and appearance evaluation network to learn long-term features of tracklets for association. Peng et al. (2018) adopted a constrained clustering to piece tracklets according to appearance characteristic of tracklet, and Peng et al. (2020) utilized Box-Plane matching strategy to achieve association.

The appearance model aims to extract person features. For example, Le et al. (2016); Yang Hoon et al. (2016) adopt the appearance model of some early traditional algorithms such as color histogram to represent the image features, or that Choi (2015); Bae and Yoon (2014); Yang and Jia (2016) utilize covariance matrix or hand-crafted keypoint features. Henschel et al. (2017) uses a novel multi-object tracking formulation to incorporate several detectors into a tracking system. Kim et al. (2015) extended the multiple hypothesis by enhancing detection model. Ma et al. (2019) presented an end-to-end deep learning framework for MOT. With the development of deep learning model Zhuang et al. (2018, 2018); Hou et al. (2020); Guo et al. (2020); Li et al. (2020), CNN are gradually utilzed in MOT. Tang et al. (2017); Sadeghian et al. (2017) train the CNN on the basis of person re-identification strategy Liu and Zhang (2020, 2021); Liu et al. (2019) to extract the image features, and Son et al. (2017) utilized the quadruplet loss to enhance the feature expression. Chu et al. (2017) builds the CNN model to generate visibility maps to solve the occlusion problem. Wang et al. (2016); Bae and Yoon (2014) are presented to improve the tracklet association and tracklet confidence to perform the tracklet task. Ma et al. (2021) adopted human-interaction model to improve the representation of targets in crowd scene.

The motion model defines the rule of object movement, which is divided into linear position prediction Son et al. (2017) and non-linear position prediction Dicle et al. (2013). Zhu et al. (2018); Gao and Jiang (2016) proposed spatial and temporal attention mechanisms to enhance the performance of MOT. Following the success of RNN models for sequence prediction tasks, Alahi et al. (2016) proposed social-LSTM to predict the position of each person in the scene. The interaction model described the inter-relationship of different pedestrians in the same scene. Yang Hoon et al. (2016) designed the structural constraint by the location of people to optimize assignment. In addition, Henschel et al. (2018) used a novel multi-object tracking formulation to incorporate several detectors into a tracking system. Ma et al. (2018) addressed a sophisticated model to process trajectories.

Multiple-Camera Multiple Object Tracking
Given the trajectories generated by the single camera MOT, cross-camera MOT further associates trajectories corresponding to the same object that are captured by different cameras. Ristani and Tomasi (2018); Jiang et al. (2018); Yoon et al. (2016); Tesfaye et al. (2017); Zhang et al. (2017); Maksai et al. (2017) are evaluated at the Duke-MTMCT Ristani et al. (2017) benchmark. Maksai et al. (2017) presented a Non-Markovian method to impose global consistency by using behavioral patterns to guide the tracking algorithms. Ristani and Tomasi (2018) proposed an adaptive weighted triplet loss for training and a new technique for hard-identity mining on extracting appearance feature. Yoon et al. (2016) applied a multiple hypothesis tracking (MHT) to handle the tracking problem with disjoint views. Jiang et al. (2018) addressed an orientation-driven person ReID and an effective camera topology estimation based on appearance feature for online inter-camera trajectory association. Cai and Medioni (2016) operated by comparing entry/exit rates across pairs of cameras. Berdereck et al. (2012) relied on completely overlapping and unobstructed views. Chen et al. (2011) built an adaptive and unsupervised method for a camera network, it can incrementally refine the clustering results of the entry/exit zones and the transition time probability distributions. Tesfaye et al. (2017) proposed a unified three-layer hierarchical approach for solving tracking problems in multiple non-overlapping cameras. [51] designed a system of multiple interacting targets in a camera network which decides the group state of each trajectory.

Single Camera Multi-Object Tracking
To generate accurate and robust trajectories from every single camera, we design multiple-stage single camera MOT framework, which is divided into three modules (as illustrated in Fig. 3): A. tracklet generation aims to generate tracklet candidates using bounding boxes of appearance and motion features; B. tracklet cleaving aims to estimate suitable split positions for impure tracklets; C. tracklet re-connection aims to associate the sub-tracklets which belong to the same person. The cleaving and re-connection processes are tracklet-to-tracklet based method. Fig. 5 shows the architecture of cleaving network and re-connection network. In this Section, the data association metric which generates tracklets from relatively sparse scenario as the tracklet candidate is described in Sect. 3.1(A). We present the reason why the algorithm mis-tracks the other people and how to estimate the tracklet reliability and split the impure tracklets in Sect. 3.2(B). Section 3.3(C) gives the traclets re-connection and association strategy, moreover, the training method of our network is also discussed.

Fig. 3
figure 3
Single Camera MOT are divide into three steps: 1.Tracklet Generation: Tracklets are generated by appearance and motion feature 2.Tracklet Cleaving: The impure tracklets are split by cleaving network 3.Tracklet Re-connection: tracklets which belong to the same person are linked by re-connection network

Full size image
Tracklet Generation
Tracklet Generation is a online sequential process that associates the bounding boxes of high similarity frame by frame to generate tracklet candidates (as described in Fig. 4). The set of nodes (bounding boxes) are composed of detection  and bounding box candidates . We denote the set of detection bounding boxes 𝑡 (𝑑𝑘𝑡∈𝑡), where 𝑑𝑘𝑡 indicates the k-th detection bounding box in frame t. 𝑡 (𝑐𝑘𝑛∈𝑡;𝑛≤𝑡,𝑡=𝑡−1⋃𝑡−1) denotes the set of tracked object candidates, where 𝑐𝑘𝑡 is the k-th candidate in frame t (all of the red dots which include unassigned dots and previous residual of 𝑡−1 for t-th frame in Fig. 4 are attributable to 𝑡). To connect the candidates and detection within inter-frames, we match the candidates 𝑐𝑘𝑡 and 𝑑𝑘𝑡 in a bipartite graph with Hungarian algorithm Sahbani and Adiprawita (2017). The bipartite graph 𝐺=(,) whose nodes V are divided into left part 𝑡∈𝐿 and right part 𝑡∈𝑅, 𝑒𝑖𝑗∈ is the edge of 𝑐𝑖𝑡 and 𝑑𝑗𝑡. Each node (bounding box) is defined as 10 dimensions [cid, id, t, x, y, w, h, wx, wy, s], which represents the camera id, tracklet id by tracker, the bounding box frame, the left-top position (x, y), width and height of the bounding box, the world-coordinate (wx, wy) and the state of the tracklet, respectively. The state of tracklet includes “tracked”, “lost” and “quit”, which are similar to Markov Decision Processes Xiang et al. (2016). If the node is associated by another node in the next frame, the node statement is labeled “tracked” (the blue and indigo dots in Fig. 4). On the contrary, due to objects “escaping” the sight, the unassigned nodes are labeled “lost” (the red dots in Fig. 4, lost node generally appears at the tail of tracklet). We define search interval length to be 𝜂𝑠 frames, if the “lost” node is found to be associated within 𝜂𝑠 frames, its state changes from “lost” to “tracked”, otherwise, the nodes from whole tracklet states are labeled “quit”. To generate tracklet candidates, the bipartite graph’s edge weights between nodes are defined as 𝑒𝑖𝑗=(𝑐𝑖𝑡,𝑑𝑗𝑡), where (𝑐𝑖𝑡,𝑑𝑗𝑡) estimates the similarity between two nodes. The overall cost function can thus be determined as follows:

(𝑐𝑖𝑡,𝑑𝑗𝑡)=𝜆𝑎𝐹𝑎(𝑐𝑖𝑡,𝑑𝑗𝑡)+𝜆𝑚𝐹𝑚(𝑐𝑖𝑡,𝑑𝑗𝑡)
(1)
𝐹𝑎(𝑐𝑖𝑡,𝑑𝑗𝑡)=1−𝑐𝑜𝑠(𝑓𝑐𝑖𝑡,𝑓𝑑𝑗𝑡)
(2)
𝐹𝑚(𝑐𝑖𝑡,𝑑𝑗𝑡)=∥𝑝̂ 𝑐𝑖𝑡−𝑝𝑑𝑗𝑡∥22
(3)
where 𝐹𝑎(𝑐𝑖𝑡,𝑑𝑗𝑡) denotes the appearance similarity between 𝑐𝑖𝑡 and 𝑑𝑗𝑡. Function cos(A, B) is formulated as 𝐴⋅𝐵|𝐴|⋅|𝐵|. The features 𝑓𝑐𝑖𝑡,𝑓𝑑𝑗𝑡 are extracted by appearance model. 𝜆𝑎,𝜆𝑚 are the weight coefficients of the function. 𝐹𝑚(𝑐𝑖𝑡,𝑑𝑗𝑡) estimates the motion distance between the detection position 𝑝𝑑𝑗𝑡 and candidate prediction position 𝑝̂ 𝑐𝑖𝑡, which is defined in 4 dimensions [𝑥̂ ,𝑦̂ ,𝑤̂ ,ℎ̂ ] that stand for the prediction of x, y-coordinate, width and height, respectively.

Appearance model extracts the pedestrian appearance features (e.g. color, shape and texture). Coherent understanding of the pedestrian appearance and further discriminative feature representation are essential for node matching in MOT. We adopt the Person Re-identification method as the appearance model Luo et al. (2019). The total loss of appearance model is defined as

𝑎𝑝𝑝=𝜆𝑖𝑑𝑖𝑑+𝜆𝑡𝑟𝑖𝑡𝑟𝑖+𝜆𝑐𝑐
(4)
which combines three types of loss (ID Loss: 𝑖𝑑Zheng et al. (2018), Triplet Loss: 𝑡𝑟𝑖 Hermans et al. (2017) and Center Loss: 𝑐 Wen et al. (2016)) together to train the appearance model, where the 𝜆𝑖𝑑, 𝜆𝑡𝑟𝑖 and 𝜆𝑐 are the loss weights to adjust training effectiveness. Appearance Extraction is treated as the multi classification task, which aims to classify the embedding feature of person image in the hyperspace.

The ID loss is formulated as:

𝑖𝑑=∑𝑣=1𝑁−𝑞𝑣𝑙𝑜𝑔(𝑞𝑣^),𝑞𝑣^=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑓𝑐𝑙𝑠,𝑣)
(5)
where 𝑖𝑑(𝑓𝑐𝑙𝑠,𝑣) indicates the cross-entropy loss, 𝑓𝑐𝑙𝑠,𝑣 is the classification feature after the output of the CNN 𝑓𝑣 by fully-connected layer, where v represents the node from 𝐶𝑖𝑡 or 𝐷𝑖𝑡 𝑞𝑣^ denotes the predict probability of classification, which is the output of the softmax. N and 𝑞𝑣 indicate the number of class and ground truth label, respectively.

Fig. 4
figure 4
The process of Tracklet Generation, the tracker aims to associate the bounding box frame by frame. The yellow and red dots for t-th frame belong to 𝑡 and 𝑡, respectively. The blue and indigo dots indicate the ’tracked’ nodes, where blue dot is the initial position of tracklet. For each step, we construct the Bipartite Graph 𝐺𝑡 by 𝑐𝑖𝑡,𝑑𝑗𝑡 and optimize the graph, and then corresponding 𝑐𝑖𝑡,𝑑𝑗𝑡 are updated to 𝑐𝑖𝑡+1,𝑑𝑗𝑡+1 for next step. We show the processing result of the first six frames (omitting the 5-th frame). The last frame corresponding 𝐶6,𝐷6 and 6 are shown at bottom of figure

Full size image
The triplet loss is a metric learning, given an anchor node 𝑣𝑎, and the corresponding embedding feature 𝑓𝑣𝑎, and we select the same class node 𝑣𝑝 and different class node 𝑣𝑛 as the positive and negative nodes, respectively. The triplet loss is defined as:

𝑡𝑟𝑖=[𝑑𝑝−𝑑𝑛+𝛼]+𝑑𝑝=∥𝑓𝑣𝑎−𝑓𝑣𝑝∥22,𝑑𝑛=∥𝑓𝑣𝑎−𝑓𝑣𝑛∥22
(6)
Fig. 5
figure 5
The architecture of tracklet cleaving and re-connection network, a Cleaving the tracklets by bidirectional outputs of GRU, b Re-connecting the tracklets by the features of siamese GRU.

Full size image
where the 𝑑𝑝 and 𝑑𝑛 indicate the Euclidean distance of positive pair and negative pair, 𝛼 is the distance threshold and [ ∗ ]+ is equivalent to function 𝑚𝑎𝑥( ∗ ,0).

The center loss is defined as:

𝑐=12∑𝑖=1𝑚∥𝑓𝑣−𝑐𝑦𝑣∥22
(7)
where m is the number of batch size, and 𝑦𝑣 indicates the label of the v-th image in a mini-batch, 𝑐𝑦𝑣 denotes the 𝑦𝑣-th class center of deep features.

After training appearance model, the output of CNN model 𝑓𝑣 is 𝐿2 normalized and utilized for Eq. 2 to calculate the similarity of appearance cue.

Motion model analyzes the pedestrian movement rule and predict the position in the future. The inputs of motion model include historical location of tracklet and its corresponding timestamp. The architecture of motion model is a LSTM, which is able to learn sequential data. We construct tracklets ground truth position as the LSTM training set. The inputs of LSTM are the tracklet historical position [𝑝𝑢𝑡, 𝑝𝑢𝑡+1, 𝑝𝑢𝑡+2, ...], where 𝑝𝑢𝑡 means the u-th tracklet posotion in frame t. The outputs of LSTM are the predicted positions in the next frame, [𝑝̂ 𝑢𝑡+1, 𝑝̂ 𝑢𝑡+2, 𝑝̂ 𝑢𝑡+3, ...]. We use the actual position of tracklet to supervise the LSTM. The position loss is described as

𝑚𝑜𝑡=∑𝑖=1𝐿𝑢−1∥𝑝̂ 𝑢𝑖−𝑝𝑢𝑖∥22
(8)
where 𝐿𝑢 is the length of u-th tracklet. we compute the distance between the predicted and the actual position. After training motion model, the position of each frame is input into the LSTM to generate the position for future frames.

After bipartite graph construction, we adopt Hungarian Algorithm Sahbani and Adiprawita (2017) to optimize the bipartite graph and obtain association results. To evaluate the performance of optimized results, we define a target function 𝐹𝐺 to calculate the differences between the ground truth graph 𝐺𝑔𝑡𝑡 and the optimized graph result 𝐺𝑡^, which is given by

𝐹𝐺(𝐺𝑡^,𝐺𝑔𝑡𝑡)=∑𝑒𝑔𝑡𝑖𝑗∈𝑔𝑡𝑡(𝑒𝑔𝑡𝑖𝑗−𝑒̂ 𝑖𝑗)+∑𝑒𝑔𝑡𝑖𝑗∉𝑔𝑡𝑡𝜎∗(𝑒̂ 𝑖𝑗−𝑒𝑔𝑡𝑖𝑗)
(9)
𝑒𝑔𝑡𝑖𝑗 indicate the ground truth connection between node i and j in 𝐺𝑔𝑡𝑡, the Eq.9 is divided into two parts by the plus (“+”). The left part 𝑒𝑔𝑡𝑖𝑗∈𝑔𝑡𝑡 means that 𝑐𝑖𝑡 and 𝑑𝑗𝑡 are associated, 𝑒𝑔𝑡𝑖𝑗≡1, and vice versa, 𝑒𝑔𝑡𝑖𝑗≡0 at the right part. 𝑒𝑖𝑗^={0,1} is the optimized edge of 𝐺𝑡. If the 𝑒𝑖𝑗^=0 at the left part, but 𝑒𝑔𝑡𝑖𝑗=1 in ground truth, the same targets are not connected, the tracked target will be lost (lost-tracking). If the 𝑒𝑖𝑗^=1 at the right part, but 𝑒𝑔𝑡𝑖𝑗=0 in ground truth, the different targets are connected, the tracked target will be switched to other target (mis-tracking). Since the negative effects of mis-tracking is greater than lost-tracking, we define a weight 𝜎 to focus more on mis-tracking, 𝜎 is set to 2. Finally, we select the optimal models with minimum score (output of Eq.9) from all of models as Tracklet Generation model.

The generated tracklets 𝜏𝑘∈ have the following attributions: 𝜏𝑘: [𝜏𝑘[𝑖𝑑],𝜏𝑘[𝑡𝑠],𝜏𝑘[𝑡𝑒],𝜏𝑘[𝐯𝑠],𝜏𝑘[𝐯𝑒],𝜏𝑘[𝑙], 𝜏𝑘[𝑟𝑘𝑡]𝑙×10,𝜏𝑘[𝐯𝑘𝑡]𝑙×1,𝜏𝑘[𝑠]], which are tracklet id, start frame, end frame, start velocity, end velocity, tracklet length and all of the nodes, where nodes 𝜏𝑘[𝑟𝑘𝑡] is a 𝑙×10 matrix, which contains all of the node information (each row stands for a node), 𝜏𝑘[𝐯𝑘𝑡]𝑙×1 records the velocity at all times. 𝜏𝑘[𝑠] is state of the tracklet, which includes “tracked”, “lost” and “quit”.

Tracklet Cleaving
After Tracklet Generation, we have a set of tracklet  in sequence. However, the Tracklet Generation may mis-track the wrong person when two persons have cross-motion or occlude each other, which degrades the generated tracklet purity. Fig. 6 shows an example of an impure tracklet, when another person (blue shirt) gradually occludes the target person (white shirt). Even though the two pedestrians present different apperance, when a large area of the target is occluded by the occluder, the bounding boxes related to target and the occluder are indistinguishable. Traditional methods only considers the bounding box in short-term adjacent frames. As the result, the target is replaced by occluder. Meanwhile, due to the two pedestrians being that the same position, the appearance and motion model on tracklet generation are not able to check whether tracker is mis-tracking.

Fig. 6
figure 6
The example of impure tracklet (mis-tracking), cleaving network aims to split the impure tracklet as two pure tracklet. The pink line is the most suitable split position for the example

Full size image
To guarantee the tracklet being the same person, we design a bidirectional output Gated Recurrent Unit to estimate the tracklet purity and cleave the false tracklets. Tracklet Cleaving is an end-to-end training network to check whether the tracklet is pure and search the suitable split position of impure tracklet. We define the pure tracklets + and impure tracklets − as:

{𝜏𝑘∈+𝜏𝑘∈−∀𝑖,𝑗,∃𝑖,𝑗,𝑟𝑘𝑖,𝑟𝑘𝑗∈𝜏𝑘,𝑟𝑘𝑖(𝑖𝑑)≡𝑟𝑘𝑗(𝑖𝑑)𝑟𝑘𝑖,𝑟𝑘𝑗∈𝜏𝑘,𝑟𝑘𝑖(𝑖𝑑)≠𝑟𝑘𝑗(𝑖𝑑)
(10)
where 𝑟𝑘𝑖,𝑟𝑘𝑗 are the i-th, j-th elements on tracklet 𝜏𝑘. All of the tracklets 𝜏𝑘∈,𝑘∈𝐾 are fed into the Cleaving Network to check purity of the tracklets, and search the best split position of impure tracklets. The tracklet Cleaving Network is shown in Fig. 6. First of all, we utilize the CNN to extract the image features 𝜑𝑘𝑐,𝑖, 𝑖∈[1,𝐿𝑘] from the tracklet, 𝐿𝑘 is the k-th length of tracklet. Secondly, all the features 𝜑𝑘𝑐,𝑖 are input into the forward-GRU and backward-GRU, respectively. Both GRUs have the shared weights, and the output is 𝜑𝑘𝑔,𝑖, 𝑖∈[−𝐿𝑘,−1]∪[1,𝐿𝑘], the positive and negative superscript values stand for forward and backward features from GRU. And then, we calculate the adjacent vectors distance between the features from the forward and the backward(e.g. length=10, {𝜑𝑘𝑔,1,𝜑𝑘𝑔,−9},{𝜑𝑘𝑔,2,𝜑𝑘𝑔,−8},... ) as a series of feature distance to concatenate as an 1 ×(𝐿𝑘−1) vector 𝜑𝑘𝑑:

𝜑𝑘𝑑,𝑖=∥𝜑𝑘𝑔,𝑖−𝜑𝑘𝑔,𝑖−𝐿𝑘∥22, 𝑖∈[1,𝐿𝑘−1]𝜑𝑘𝑑=([𝜑𝑘𝑑,1,𝜑𝑘𝑑,2,...,𝜑𝑘𝑑,𝐿𝑘])
(11)
The algorithm calculates the distance 𝜑𝑘𝑑,𝑖 between the features from the left to current position i and the right to corresponding position 𝑖−𝐿𝑘. 𝜑𝑘𝑑,𝑖 is fed into two fully-connected layers separately after normalization. First output of the FC layer 𝜑𝑘𝑠 is used for searching the most suitable split position, which generally appears at the maximum disparity from these distances. The output of other FC layer 𝜑𝑘𝑝 is utilized for checking whether the tracklet is pure. The advantage of GRU is to be able to summarize the general characteristics with the same person and eliminate occlusion in order to obtain preferable feature expression. For training Cleaving Network, the cleaving loss 𝑐𝑙𝑣 is defined as:

𝑐𝑙𝑣=𝜆𝑓𝑐,𝑓𝑡𝑟+𝜆𝑠𝑐,𝑠𝑟ℎ+𝜆𝑝𝑐,𝑝𝑢𝑟
(12)
The feature loss 𝑐,𝑓𝑡𝑟 calculates the difference between each output of GRU 𝜑𝑘𝑓,𝑖, 𝑖∈[−𝐿𝑘,−1]∪[1,𝐿𝑘] and the ground truth. The searching loss 𝑐,𝑠𝑟ℎ measures distance between the predict split position and real split position. The purity loss is a Binary task to differentiate whether the tracket is pure. Eq.12 is expanded as:

𝑐𝑙𝑣=∑𝑘∈𝐾(𝜆𝑓2𝐿𝑘(∑𝑖=−𝐿𝑘−1𝐹𝜉(𝜑𝑘𝑔,𝑖)+∑𝑗=1𝐿𝑘𝐹𝜉(𝜑𝑘𝑔,𝑗))+𝜆𝑠𝐹𝜄(𝜑𝑘𝑠)+𝜆𝑝𝐹2(𝜑𝑘𝑑))
(13)
where 𝜆∗ is the loss weight coefficient, K indicates the number of the tracklets in training set, 𝜉 and 𝜄 denote the number of trakclet class and length of k-th tracklets, respectively. 𝐹𝑁 (𝑁={𝜉, 𝜄, 2}) indicates Cross-Entropy loss:

𝐹𝑁=∑𝑖=1𝑁−𝑞𝑖𝑙𝑜𝑔(𝑞𝑖^),𝑞𝑖^=𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝜑)
(14)
Pure Loss 𝑐,𝑝𝑢𝑟 is a binary classification task, which supervises 𝜑𝑘𝑝 to judge the purity of a tracklet. Search Loss 𝑐,𝑠𝑟ℎ is a multiple classification task, which is utilized for optimizing 𝜑𝑘𝑠 to search the best splitting position. Search Loss 𝑐,𝑠𝑟ℎ can also be defined by L1-Loss to calculate the differences between prediction split position and ground truth.

When the training cleaving network is completed, the network is used for checking the tracklets 𝜏𝑘∈ generated from tracklet generation part. In general, the most suitable split position occurs on the maximum feature distance between left-side and right-side for the position. If the 𝜏𝑘 belongs to impure tracklet, it will be split into two tracklets 𝜏𝑘1 and 𝜏𝑘2. To guarantee the tracklet purity, the tracklet cleaving step is vital for tracklet description and cross-camera trajectory matching.

Tracklet Re-connection
After Tracklet Cleaving step, we obtain some pure tracklets. However, long-term occlusion usually breaks the whole tracklet, frequent occlusion in crowd produces the lost-tracking and id-switch for tracklets, and illumination variation influences the appearance feature description and then affects the tracking performance. Fig. 7 is an example of tracklet fragments, which belong to the same person.

Fig. 7
figure 7
The example of fragmented tracklets (lost-tracking), re-connection network aims to connect the fragmented tracklets as a whole tracklet

Full size image
Re-connection focuses on extracting the general features of tracklets and calculating similarity between tracklets and connecting fragmented tracklets as a whole tracklet. The architecture of re-connection network is shown in Fig. 5. For matching tracklets, each tracklet is extracted the general feature to describe the tracklet appearance characteristics. The tracklets with similar general feature will be re-connected. We combine various losses to reduce the within-class distance and enlarge the between-class distance, simultaneously. Our network is designed with the verification loss and identification loss at each GRU output. The re-connection loss is defined as:

𝑟𝑐𝑛=𝑔𝑙𝑜+𝑙𝑜𝑐
(15)
Where the 𝑔𝑙𝑜 and 𝑙𝑜𝑐 indicate the global loss and local loss of the network, respectively. We use the contrastive loss by Euclidean distance for the verification and the cross-entropy losses in the multi-classification task for the identification. The identification loss 𝐹(∗) is the same as the Eq.14. The details of the contrastive loss 𝐸(∗) are shown as:

𝐸(𝜑𝑘1𝑓,𝜑𝑘2𝑓)=𝑦∥𝜑𝑘1𝑓−𝜑𝑘2𝑓∥22+(1−𝑦)𝑚𝑎𝑥{0,(𝜂−∥𝜑𝑘1𝑓−𝜑𝑘2𝑓∥22)}
(16)
where 𝜑𝑘∗𝑓 indicates the output feature of GRU 𝜑𝑘∗𝑔 after fully-connected (FC) and ReLU. 𝐸(𝜑𝑖,𝜑𝑗) is contrastive function, 𝑦∈{0,1} is the label indicator, 𝜂 is a margin constant. The representation of loss can be formulated as follows:

 𝑔𝑙𝑜=𝜆𝑣𝑣+𝜆𝑖𝑑(𝑖𝑑1+𝑖𝑑2)=𝜆𝑣𝐸(𝜑𝑘1𝑓,𝜑𝑘2𝑓)+𝜆𝑖𝑑(𝐹(𝜑𝑘1𝑓)+𝐹(𝜑𝑘2𝑓))
(17)
𝜑𝑘𝑓=12𝐿𝑘(∑𝑖=−𝐿𝑘−1𝜑𝑘𝑓,𝑖+∑𝑗=1𝐿𝑘𝜑𝑘𝑓,𝑗)
(18)
where 𝜑𝑘𝑓 is the temporal pooling McLaughlin et al. (2016) of each output of GRU.

𝑙𝑜𝑐=𝜆𝑙𝑜𝑐_𝑣𝑙𝑜𝑐_𝑣+𝜆𝑙𝑜𝑐_𝑖𝑑𝑙𝑜𝑐_𝑖𝑑
(19)
𝑙𝑜𝑐_𝑣=∥𝜑𝑘1𝑓,1−𝜑𝑘1𝑓,𝐿𝑘1∥22+∥𝜑𝑘2𝑓,1−𝜑𝑘2𝑓,𝐿𝑘2∥22  −∥𝜑𝑘1𝑓,1−𝜑𝑘2𝑓,1∥22−∥𝜑𝑘1𝑓,𝐿𝑘1−𝜑𝑘2𝑓,𝐿𝑘2∥22+𝛿
(20)
𝑙𝑜𝑐_𝑖𝑑=∑𝑘∈𝑘1,𝑘2(∑𝑖=−𝐿𝑘−1𝐹(𝜑𝑘𝑓,𝑖)+∑𝑗=1𝐿𝑘𝐹(𝜑𝑘𝑓,𝑗))
(21)
𝜆 is the loss weight coefficient. 𝑣, 𝑖𝑑∗, 𝑙𝑜𝑐_𝑣 and 𝑙𝑜𝑐_𝑖𝑑 denote the verification and identification loss of global and local, respectively. 𝑙𝑜𝑐_𝑣 is similar to triplet loss (refer to Son et al. (2017)), including the disparity of head and tail of the tracklet, head between different tracklets and tail between different tracklets. 𝛿 is the threshold of margin. 𝑙𝑜𝑐_𝑖𝑑 is the multi-classification task for each output.

When the training of re-connection network is completed, the network is utilized for extracting tracklet general feature, and the tracklet re-connection step compares several tracklets and connects tracklets {𝜏𝑘1, 𝜏𝑘2, 𝜏𝑘3, ...} which belong to the same person as the whole tracklet 𝜏𝑘.

Multiple-Camera Tracking
For Multiple-Camera Multiple Object Tracking, we aim to associate the trajectories from different cameras. Each trajectory is generated from the single camera tracking. Section 4.1(A) introduces how to train Position Projection Network (PPN) and position conversion of the trajectories from camera-coordinate to world-coordinate. Section 4.2(B) discusses how to eliminate redundant matching operation by temporal-spatial constraint. Section 4.3(C) describes how to associate the trajectories by similarity of general appearance feature.

Fig. 8
figure 8
Annotated point-pairs for training Position Projection Network , the images are from Duke-MTMCT, where (a) is camera 2 and (b) is camera 5

Full size image
Position Projection
Position Projection focuses on transferring each tracklet position from camera-coordinate to world-coordinate. We propose a deep learning method called Position Projection Network (PPN) to project each point position. Given the input position (camera-coordinate) (𝜎𝑥, 𝜎𝑦), the target (output) of the network is the world-coordinate (𝜔𝑥, 𝜔𝑦). We treat the position projection as a fitting task. Compared with the traditional method such as Geometric Camera Calibration Hartley and Zisserman (2003), our method only annotates a few point-pairs between camera image and world map to train PPN instead of calculating the translation matrix and rotation matrix.

Fig. 9
figure 9
The example of Position Projection, the figure shows the tracklets position from camera2 (bottom-right) and camera5 (top-right) transferred to world map (left), a and b are the tracking and projection results at the 246025-th and 247216-th frame (The calibration-frame) respectively

Full size image
Figure 8 shows how to annotate and generate more point-pairs for the training set. The left and right images denote the camera view image and world map. The same point number between camera image and world map indicates the same position. For example Fig. 8a, we only mark 23 points on camera image and world map, respectively, and we choose the representative locations as the point-pairs such as corner-points. Duke-MTMCT dataset has 8-camera sequences, we annotate 8 groups of point-pairs respectively, 8-camera coordinates are projected to the same world map. After point-pair annotation, we get several point-pairs, of which is a vector 𝜚𝑖: [𝑐𝑖𝑑,𝑖,𝜎𝑖𝑥,𝜎𝑖𝑦,𝜔𝑖𝑥,𝜔𝑖𝑦], where cid indicates the camera number and i denotes the point number.

In order to improve the precision of projection, we present an interpolation method to enlarge the point-pairs. We divide the scene into several areas according to plane, for example Fig. 8a, points 𝜚𝑎∈01, a: [1→14,18,19,20] belong to the same plane, points 𝜚𝑏∈02, b: [15,16,18→23] belong to the other plane, points 𝜚𝑐∈03, c: [16, 17, 21, 22, 23] belong to the third plane. For each Area ∗, we interpolate a new point from the midpoint of neighboring points on camera image and world map, respectively, which is defined as:

1∗=0∗∪1∗1∗={𝜚𝑖𝑗| 𝑖,𝑗∈0∗},𝜚𝑖𝑗=𝑚𝑖𝑑𝑝𝑜𝑖𝑛𝑡(𝜚𝑖,𝜚𝑗)
(22)
where 1∗ indicates the generated point-pair set from the midpoint 𝜚𝑖𝑗: [𝑐𝑖𝑑,𝑖𝑗,𝜎𝑖𝑗𝑥,𝜎𝑖𝑗𝑦,𝜔𝑖𝑗𝑥,𝜔𝑖𝑗𝑦] of neighboring points 𝜚𝑖, 𝜚𝑗, the point number ij is numbered in order. 1∗ is the combination of original point-pair set and generated point-pair set. ∗ denotes the area number. Point-pair enlargement is a recursive process, which is computed as:

𝑠+1∗=𝑠∗∪𝑠+1∗,    𝑠+1∗={𝜚𝑖𝑗| 𝑖,𝑗∈𝑠∗}
(23)
For Duke-MTMCT dataset, we have five iterations to generate 30k point-pair for training PPN. The architecture of PPN contains 3 fully-connected (fc) layers for each camera projection, the camera position [𝜎𝑥,𝜎𝑦] are divided by the length and width of the image, respectively, for normalizing the value to 0-1 [𝜎𝑥¯,𝜎𝑦¯] as the PPN input (1×2 vector). The normalized vector is fed into 𝑓𝑐1 (2→128) →𝑓𝑐2 (128→128) →𝑓𝑐3 (128→2), each layer contains fully-connected, batch normalization and ReLU. The output of 𝑓𝑐3 is a 1×2 vector [𝜔𝑥¯^,𝜔𝑦¯^]. We utilize the generated world position [𝜔𝑥,𝜔𝑦] to supervise PPN. The loss is defined as:

𝑝=∣∣𝜔𝑥¯−𝜔𝑥¯^∣∣+∣∣𝜔𝑦¯−𝜔𝑦¯^∣∣
(24)
where 𝑝 is a L1-loss, [𝜔𝑥¯,𝜔𝑦¯] are normalized position by world position. The PPN is an one-to-one mapping between camera coordinate and world coordinate, so 𝑝 aims to narrow the distance between the predicted position and the real position.

Temporal-Spatial Constraint
We adopt the temporal-spatial constraint to reduce the amount of matching operation. The tracklet positions are projected on the uniform world map. Figure 9 shows the camera2 and camera5 tracking results and projection results at the 289752-th and 290160-th frames. A throng in camera2(a) exits the field of view from the right. For each disappearing tracklet 𝜏 (introduced in Sect. 3.1(A)), we compute the velocity 𝜏[𝑣𝑒] at the last frames before it disappears, which is defined as:

𝜏[𝐯𝑒]=𝜏[𝐯𝐿]=12(𝜏[𝐯𝐿−1]+1𝜂𝑣∑𝑡=1𝜂𝑣(𝑝𝐿−𝑝𝐿−𝑡))
(25)
which is a recursion equation, where 𝜏[𝐯𝐿] is the velocity of the last frame for 𝜏, L is the current length of tracklet 𝜏𝑘. 𝜏[𝐯𝐿−1] is the previous frame velocity, and 𝜂𝑣 indicates the velocity computing width parameter, 𝑝𝐿 denotes the position of the last frame for 𝜏. The velocity of the current frame is based on the previous velocity and the average of the vectors between the current position and the previous few positions.

Tracklet association are restricted previously by temporal-spatial constraints. Our proposal eliminates lots of irrelevant trajectories that appear too early or too late as compared with the target duration time, meanwhile, it removes trajectories which are too far away from the target prediction position . The constraint of matching is shown as:

‖‖𝜏𝑘𝑗[𝑝1]−𝜏𝑘𝑖[𝑝̂ 𝐿𝑘𝑖+Δ𝑡𝑖,𝑗]‖‖22<𝜂𝑐
(26)
where

𝜏𝑘𝑖[𝑝̂ 𝐿𝑘𝑖+Δ𝑡𝑖,𝑗]=𝜏𝑘𝑖[𝑝̂ 𝐿𝑘𝑖]+Δ𝑡𝑖,𝑗∗𝜏𝑘𝑖[𝐯𝑒], Δ𝑡𝑖,𝑗=𝜏𝑘𝑗[𝑡𝑠]−𝜏𝑘𝑖[𝑡𝑒],𝑠.𝑡. 0<𝜏𝑘𝑗[𝑡𝑠]−𝜏𝑘𝑖[𝑡𝑒]<𝜂𝑡; 𝜏𝑘𝑖[𝑠],𝜏𝑘𝑗[𝑠]≠𝑞𝑢𝑖𝑡𝑡𝑒𝑑
(27)
𝜏𝑘𝑗[𝑝1] indicates the position of 𝜏𝑘𝑗 at the first frame of tracklet. 𝜏𝑘𝑖[𝑝̂ 𝐿𝑘𝑖+Δ𝑡𝑖,𝑗] is the predicted position of the 𝜏𝑘𝑗 in the future. 𝜂𝑐 is the spatial constraint parameter. Δ𝑡𝑖,𝑗 is the interval between start frame 𝜏𝑘𝑗 and end frame 𝜏𝑘𝑖. 𝜂𝑡 is the temporal constraint parameter. 𝜏[𝑠] are the states of the tracklet, which contains “tracked”, “lost” and “quit”(same as the node states in Section 3.1(A)).

Trajectory Association
Trajectory matching candidates are filtered by temporal-spatial constraint. Trajectory association is based on calculating the general appearance feature distance between Trajectories. The trajectory general feature is extracted by Re-connection Network (Sec.3.3(C)). Given a trajectory 𝜏𝑘∈, and its corresponding trajectory candidate set with constraint is 𝑘:{𝜏1𝑘,𝜏2𝑘,𝜏3𝑘,...}. We calculate the Cosine distance between 𝜏𝑘 and each trajectory candidate 𝜏∗𝑘. We define an association threshold, 𝜃𝑅, and associate the trajectories of which the distance is less than 𝜃𝑅. At last, we utilize “Union-Find-Set” to classify the connected subset. Each subset indicates a whole trajectory.

Table 1 The performance with different steps on MOT16 validation set
Full size table
Experiments
Datasets and Benchmarks
We evaluate our proposal on MOT16 Milan et al. (2016) and Duke-MTMCT Ristani et al. (2017) datasets, which contain large-scale video sequences from different cameras and scenes. Both datasets’ tracking results are evaluated on MOT Challenge Leal-Taix et al. (2015). We additionally use the Person ReID dataset Market-1501 Zheng et al. (2015) and DukeMTMC-reID Ristani et al. (2017) to train Appearance Model, utilize tracking dataset PathTrack Manen et al. (2017), Duke-MTMCT Ristani et al. (2017) and video re-identification dataset MARS Zheng et al. (2016) to train Cleaving and Re-connection Network. The Motion Model and Position Projection Networks are trained on Duke-MTMCT.

Duke-MTMCT is a large-scale dataset for multiple target multiple-camera tracking with the videos captured by 8 surveillance cameras at different viewing angles including 2800 identities (persons) in Duke University. The video duration of each camera is 86 minutes, which is split into training set (0–50 min) and testing set (50-86 min). In addition, the dataset provides DPM Felzenszwalb et al. (2010) and Openpose Cao et al. (2018) detection results for each frame as the tracker input. However, currently the dataset has been removed from the MOT benchmark.

MOT16 is a classical evaluation dataset comparing several tracking methods on MOT Challenge, which includes 14 sequences captured from surveillance, hand-held shooting and driving recorder by static cameras and moving cameras. The length of each video is about 500-1500 frames. And the dataset also provides the detection DPM.

Implementation Details
In our experiments, our networks consist of CNN, LSTM and GRU, where GRU Cho et al. (2014) is a type of RNN with gates and hidden units. For tracklet generation, we train the CNN network of appearance model with SeResNet50 Hu et al. (2018), the images are resized to 128*256 from ReID training set and the output of CNN 𝑓𝑐𝑖𝑡,𝑓𝑑𝑗𝑡 produces a 2048-dimensional vector to describe the image. In addition, the inputs of the LSTM network for motion model is a series of 2-dimensional vector 𝑝𝑐𝑖𝑡: [x, y] with a tracklet, which are divided by image width and height to have the input normalized, and the LSTM output is the prediction of the position and size 𝑝̂ 𝑐𝑖𝑡:[𝑥̂ ,𝑦̂ ]. For tracklet cleaving and re-connection, the model is a deep Siamese Bi-GRU, which includes four hidden-layers and the maximum length of GRU is 120 frames. The input of the GRU is a series of appearance features by CNN. The outputs of the GRU 𝜑𝑘𝑔,𝑖,𝑖∈[−𝐿𝑘,−1]∪[1,𝐿𝑘] are 128-dimensional vectors, which are fed to FC network for verification loss and for comparison of the corresponding features for classification loss. At single-camera tracking, association threshold 𝜃𝐺=0.7, tracklet matching threshold 𝜃𝑅=0.5. The searching interval length 𝜂𝑠 is 100 frames.

For cross-camera, the spatial constraint parameter 𝜂𝑐=300 pixels on world map, the temporal constraint parameter 𝜂𝑡=3000 frames to retrieve tracklets. We associate the tracklets which satisfy the constraint by the Re-connection Network output 𝜑𝑘𝑓. The tracklet association is the tracklet similarity matching task, which compares the distance of feature pair by pair. The closer the distance of feature, the more similar the image. In addition, the temporal constraint makes sure the re-connection candidates appear at different times and the target doesn’t appear twice at the same time. The spatial constraint checks whether two tracklet candidates meet normal movement rule. If the distance of tracklets is less than 𝜃𝑅, the tracklets are connected. However, the matching of tracklets between different cameras might affect the labeling of ID, e.g. 𝜏𝑖 linked by 𝜏𝑗; 𝜏𝑗 connected by 𝜏𝑘; 𝜏𝑖 also associated by 𝜏𝑘. As the result, 𝜏𝑖, 𝜏𝑗, 𝜏𝑘 need to be labeled the same ID number. We adopt “Union-Find-Set” algorithm to solve the tracklet association task, which searches the connected sub-graph as the same trajectory from global matching graph. We use the AdamOptimizer Kingma and Ba (2015) as the training optimizer, our experiment is implemented with Python 3.6 and Pytorch 0.4.1 framework and on Nvidia Tesla K40 GPUs.

Ablation Study
For Single-Camera Tracking, Table 1 describes our performance for each evaluation parameter at different steps. The first row indicates the tracklet generation step, which reaches 52.2 MOTA and 54.4 IDF1. The cleaving step aims to check the tracklet purity and split impure tracklets, and the re-connection step focuses on linking the tracklet fragments. Both Step 2 and Step 3 improve ID types of measures such as IDF1 from 54.4 to 59.1, but they don’t basically affect CLEAR-MOT metrics except ID switch. The last two steps are explained in Fig. 10, the Gap Insertion method fills the disappearing bounding boxes due to occlusion according to the existing bounding boxes, this step can increase the True Positive (TP) bounding boxes, meanwhile the False Positive is (FP) also increased. On the whole, most of the performance measures are improved, especially MOTA, IDF1 and Frag. The last step, tracklet smoothing method adjusts the boxes’ size to optimize the boxes’ “waggle” between frames, which can decrease the frag and slightly reduce ID switch.

Fig. 10
figure 10
The explanation of Gap Insertion and Tracklet Smoothing

Full size image
Table 2 The comparison from different matching strategy on Duke-MTMCT training set
Full size table
For Cross-Camera Tracking, we present a Position Projection matching strategy to associate trajectories from different cameras. Table 2 shows the influence of using temporal and spatial constraints on the quality of trajectory matching and performance. The Global Retrieval indicates traversal search for each trajectory one-by-one in the history trajectory gallery. The Temporal gives the constraint for the start-frame and end-frame of trajectory. We filter out the trajectories where their start-frame is earlier than target trajectory end-frame. Temporal constraint can reduce by half the unavailable matching pairs. The strong temporal constraint additionally sets the upper limit of frame interval, which can decrease the amount of trajectory matching operation from 1577665 to 99870. Topology strategy aims to construct the connected relation between cameras on actual scenario, which only matches the trajectories from neighboring cameras, it removes half of the irrelevant trajectories. Spatial-temporal constraint is our method, which combines strong temporal and position projection, and the constraint details are described in Eq.27. The spatial-temporal can extremely reduce the number of matching-pairs, the remaining pairs meet motion rule between appearing position of candidate trajectory and predicted position of target trajectory on the real scenario. It can largely eliminate redundant amount of matching operation and decrease the probability of mis-matching.

Table 3 shows the performance of single and multiple camera tracking with different models in Duke-MTMCT validation set. “R” includes Re-connection, Gap Insertion and Smoothing processing. From the first and second rows, using the Cleaving Network can improve 3.3% MOTA and 3.2% IDF1 in single camera, and 2.0% IDF1 in multiple-camera, respectively. The third row adopts Position Projection, thus IDF1 in multiple camera is greatly improved. From the last row of the table, the IDR is raised a lot by using Cleaving Network and Position Projection. Hence, the IDF1 is obviously higher than the others in Table 3.

Networks Analysis
Cleaving Network
To train the Cleaving Network, we construct a training dataset from Duke-MTMCT Ristani et al. (2017), PathTrack Manen et al. (2017) and MOT16 Milan et al. (2016). The datasets includes more than 2500 pedestrians’ tracklet from different cameras and scenes, we divide each tracklet into several sub-tracklet of uniform-length (120 frames). Ultimately, we have more than 10K sub-tracklets (1.2M images) for training Cleaving Network.

Before training the Cleaving Network, we randomly select two tracklets 𝜏𝑎 and 𝜏𝑏. We generate a random parameter 𝑙𝑝 from 0 or 1. If 𝑙𝑝=0, we input an impure tracklet (we constructed) into Cleaving Network, so we generate another random parameter 𝑙𝑠 from 0 to 120, which indicates the splitting position. We combine the two tracklets as a joint tracklet, which is formulated as: 𝑙𝑖𝑑=[𝑙𝑖𝑑1[0:𝑙𝑠],[𝑙𝑖𝑑2[𝑙𝑠:120]]. If 𝑙𝑝=1, we input a pure tracklet into Cleaving Network, we set 𝑙𝑠=120.

Table 3 The performance of Multiple-Camera Tracking with different models in Duke-MTMCT validation set
Full size table
Fig. 11
figure 11
The purity and split results of Cleaving Network for pure & impure tracklets on training and testing dataset. a: In training set b: In testing set

Full size image
In training step, we randomly shuffle the order of the impure and the pure tracklets to feed into Cleaving Network. We define three types of losses (Feature Loss 𝑐,𝑓𝑡𝑟, Pure Loss 𝑐,𝑝𝑢𝑟, Search Loss 𝑐,𝑠𝑟ℎ) to supervise the network. Feature Loss 𝑐,𝑓𝑡𝑟 is treated as a multiple classification task, which supervises the outputs of each step 𝜑𝑘𝑔,𝑖 to generate discriminative features. Pure Loss 𝑐,𝑝𝑢𝑟 is a binary classification task, which supervises 𝜑𝑘𝑝 to judge the purity of a tracklet. Search Loss 𝑐,𝑠𝑟ℎ is utilized for optimizing 𝜑𝑘𝑠 to search the best splitting position of impure tracklet. We utilize two kinds of loss (cross-entropy and L1-loss) to train cleaving network, respectively.

To evaluate the performance of cleaving network in training, we define three evaluation indexes: 1.Accuracy of ID classification; 2. Accuracy of purity; 3. Average distance of best splitting position. If we define 𝑐,𝑠𝑟𝑐 as cross-entropy, the task initially is treated as multiple classification task (121 classes), we fix the tracklet length of input (120 frames), each tracklet has 121 gaps between frames. At the 21st epoch, the accuracy of ID: 98.6%; Accuracy of purity: 95.3%; Average distance of best splitting position: 2.57 gaps. If we define 𝑐,𝑠𝑟𝑐 as L1-loss, which is to minimize the distance between the output 𝜑𝑘𝑠 and the ground truth 𝑙𝑠. At the 13rd epoch, the Accuracy of ID: 98.6%; Accuracy of purity: 95.2%; Average distance of best splitting position: 2.43 gaps. Although the performances (𝜑𝑘𝑝 and 𝜑𝑘𝑠) with two kinds of loss are the same, using L1-loss can make earlier convergence than using cross-entropy.

In testing step, we fix the length of Cleaving Network input (120 frames), therefore, we equably sample the over-length tracklets and padding the under-length tracklets to being of a uniform length.

For over-length tracklet 𝜏1:

 𝜏1=[𝐼1,𝐼2,𝐼3,...,𝐼238,𝐼239,𝐼240]1∗240

where 𝐼∗ indicates the i-th croped image of the tracklet 𝜏.

We convert 𝜏1 to 𝜏1′:

 𝜏1′=[𝐼1,𝐼3,𝐼5,...,𝐼235,𝐼237,𝐼239]1∗120

For under-length tracklet 𝜏2:

 𝜏2=[𝐼1,𝐼2,𝐼3,...,𝐼58,𝐼59,𝐼60]1∗60

We convert 𝜏2 to 𝜏2′:

 𝜏2′=[𝐼1,𝐼1,𝐼2,...,𝐼59,𝐼60,𝐼60]1∗120

For under-length tracklet, we can easily get the best splitting position in the original tracklet:

 𝜏2′=[𝐼1,...,𝐼30,𝐼30,|𝐼31,𝐼31,...,𝐼60]1∗120

The splitting position of original tracklet is:

 𝜏2=[𝐼1,...,𝐼29,𝐼30,|𝐼31,𝐼32,...,𝐼60]1∗60

However, for over-length tracklets, we cannot directly get the best splitting positions in the original tracklet. Thus, we select ±60 frames around the splitting position to form a tracklet to input to the network again.

For example:

 𝜏1′=[𝐼1,...,𝐼169,|𝐼171,...,𝐼239]1∗120

Select the sub-tracklet and re-feed to Cleaving Network:

 𝜏1″=[𝐼110,...,𝐼169,𝐼170,𝐼171,...,𝐼229]1∗120

The splitting position of original tracklet is:

 𝜏1=[𝐼1,...,𝐼169,|𝐼170,𝐼171,...,𝐼240]1∗240

An example of searching splitting position recursively for over-length tracklet 𝜏585 is illustrated in Fig. 12.

Figure 11 illustrates tracklets 𝜏𝑘∗ and the corresponding outputs of Cleaving Network, which includes purity results: 𝜑𝑘∗𝑝, splitting results: 𝜑𝑘∗𝑠, and feature visualization results: 𝜑𝑘∗𝑠. If the tracklet is pure, since the change of the appearance of the target being tracked in a short time of period is very little (because of the frame rate assumption), the change of feature of the the tracklet appearance is also slight. Thus, most of the elements of feature 𝜑∗𝑑,𝑖 tend to be zero. On the contrary, if the tracklet is impure, since the target is replaced by the other, the difference of the appearances is obvious before and after the replacement. Thus, most of the elements of feature 𝜑∗𝑑,𝑖 are not zero. The position of the maximum value of the feature (pink dotted line of each impure tracklet in the Fig. 11) indicates that the features at the two sides have the largest difference.

Fig. 12
figure 12
The illustration of searching splitting position recursively for over-length tracklet 𝜏585 by Cleaving Network

Full size image
Fig. 13
figure 13
Experimental results of Re-connecction Network (a): tracklets from multiple cameras (b): Similarity matrix between tracklets

Full size image
Re-connection Network
The training dataset of Re-connection Network is composed of DukeMTMCT Ristani et al. (2017), MOT16 Milan et al. (2016) and MARS Zheng et al. (2016), which includes more than 2800 persons, and each person has several tracklets from different cameras, viewpoint, pose, illumination, etc. The Re-connection Network is finally convergent to the 90th epoch in training. The re-connection network is regarded as a general feature extractor, in post-processing step, re-connection strategy determines whether the tracklets belong to the same person based on appearance similarity between the tracklets and spatial-temporal constraint.

Fig. 13 illustrates the performance of Re-connection Network on Duke-MTMCT testing set, where Fig. 13a shows the tracklets from multiple cameras, each tracklet is marked with their tracklet ID, camera ID and timestamps at the top of the images. Fig. 13b illustrates the similarity between the tracklets. The similarity is calculated by 𝜆𝑎𝐹𝑎(𝑐𝑖𝑡,𝑑𝑗𝑡) (described in Eq.(1)). Generally 𝜆𝑎 is set 0.5, the value of 𝜆𝑎𝐹𝑎(𝑐𝑖𝑡,𝑑𝑗𝑡) is between [0, 1]. The more similar the features are, the smaller the value is. In the Fig. 13b, the similarity value between the tracklets of the same person is generally not more than 0.15, and most of the similarity value between the tracklets of different persons is not less than 0.3. However, for two tracklets which are of highly similar appearance and don’t belong to the same person, their values are less than 0.3, such as No.329 and No.6582. Actually, at the matching step, No.329 is removed initially from the No.6582 matching candidates by the temporal-spatial constraint. Thus, matching tracklets by threshold is able to re-connect the tracklets effectively.

Table 4 Results on the MOT16 testing set
Full size table
Position Projection Network
The proposal of position projection strategy is normalizing the positions from different cameras to the same map, which is utilized for tracklet motion prediction on world map and tracklet matching between cameras. The position projection task is treated as regression task: we assume that the pedestrians move at the surface of the ground, given a camera-coordinate (𝜎𝑥,𝜎𝑦), the output of PPN is a unique position (𝜔𝑥,𝜔𝑦) on world-coordinate.

To improve the performance of position projection, we propose a data expansion method (described in Sect. 4.1(A)). Extending the point-pairs by generating midpoints is implemented within each plane. The function of Position Projection Network (PPN) is the same as Projective Transformations of 2D. As Hartley and Zisserman (2003) describes in Sect. 2.3, Definition 2.11: “A planar projective transformation is a linear transformation on homogeneous 3-vectors represented by a non-singular 3*3 matrix”, which is formulated as:

⎛⎝⎜⎜⎜𝑥′1𝑥′2𝑥′3⎞⎠⎟⎟⎟=⎡⎣⎢⎢ℎ11ℎ21ℎ31ℎ12ℎ22ℎ32ℎ13ℎ23ℎ33⎤⎦⎥⎥⎛⎝⎜⎜𝑥1𝑥1𝑥3⎞⎠⎟⎟
(28)
where 𝑥∗,𝑥′∗ indicate the coordinates of original and transformed plane, respectively. The position transformation between camera image and world map belongs to an affine transformation, which is a non-singular linear transformation as following:

⎛⎝⎜⎜⎜𝜎𝑥𝜎𝑦1⎞⎠⎟⎟⎟=⎡⎣⎢⎢⎢𝑎11𝑎210𝑎12𝑎220𝑡𝑥𝑡𝑦1⎤⎦⎥⎥⎥⎛⎝⎜⎜⎜𝜔𝑥𝜔𝑦1⎞⎠⎟⎟⎟
(29)
Affine transformation ensures that a point (𝜎𝑥,𝜎𝑦) in camera image has a unique point (𝜔𝑥,𝜔𝑦) in world map. Given two points 𝜚1,𝜚2 in a plane, their midpoint 𝜚12 must be also in the plane. Therefore, generated midpoint between points in camera image also corresponds to a midpoint between the corresponding points in the world map.

Compared with affine transformation, PPN learns the projection relation from labeled and generated data by deep learning. The geometry-based method Jiang et al. (2018) matches the trajectory by manually creating a topology association according to the path between cameras. However, if the number of cameras are increased, the manual operation will become more complicated. Additionally, since the view of cameras are different, the velocity and direction calculated by position in camera-coordinate exist errors. On the contrary, PPN needn’t previously make the geometric calibration, such as vanishing point camera calibration. The prediction position based on world map is much easier, because all of trajectories from different cameras are in the same plane, the velocity and direction cannot be affected by the view of each camera.In experimental results, the average distance error 𝑝 between output and labeled position is 0.0032 pixels/point for each camera.

Table 5 Results on the MOT17 testing set
Full size table
Table 6 Results on the Duke-MTMCT easy testing dataset Benchmark
Full size table
MOT Evaluation Metrics
The MOT Challenge Benchmark adopted the standard CLEAR-MOT mapping Bernardin and Stiefelhagen (2008) and ID measures Ristani et al. (2017) for evaluating MOT performance. The main metrics for MOT are MOTA and IDF1. MOTA (Multiple Object Tracking Accuracy) measures the effect of tracking for each tracklet, which depends on True Positives (TP) ,False Positives (FP), False Negatives (FN) and Id Switches (IDSw), 𝑀𝑂𝑇𝐴=1−𝐹𝑃+𝐹𝑁+𝐼𝐷𝑆𝑇𝑃. Total number of Fragment (Frag), Mostly tracked targets (MT), Mostly lost targets (ML) are used for evaluating tracklet integrity as the reference indexes. The IDF1 (ID F1 Score) is the ratio of correctly identified detection over the average number of true and computed detection, which depends on ID True Positives (IDTP), ID False Positives (IDFP) and ID False Negatives (IDFN), 𝐼𝐷𝐹1=2∗𝐼𝐷𝑇𝑃2∗𝐼𝐷𝑇𝑃+𝐼𝐷𝐹𝑁+𝐼𝐷𝐹𝑃. IDP (ID Precision) indicates fraction of computed detections that are correctly identified, 𝐼𝐷𝑃=𝐼𝐷𝑇𝑃𝐼𝐷𝑇𝑃+𝐼𝐷𝐹𝑃. IDR (ID Recall) means fraction of ground-truth detection that are correctly identified, 𝐼𝐷𝑅=𝐼𝐷𝑇𝑃𝐼𝐷𝑇𝑃+𝐼𝐷𝐹𝑁.

Method Comparison
We evaluate our proposal against the other traditional state-of-the-art methods on two public Tracking Benchmark datasets (MOT16, MOT17 and Duke-MTMCT). Table 4 compares the performance of our method with the existing methods on MOT16 testing set. Compared with the others methods, we achieve the higher performance of 55.0% on MOTA, 19.1% on ML and 77829 on FN. Our method outperforms most of the methods on the others metrics. Table 5 shows the performance of different state-of-the-art methods on MOT17 testing set. Trackers are divided into two types: Tracking-by-Detection (TBD) and Joint Detection and Tracking (JDT). TBD-based methods follow the traditional framework to associate bounding boxes according to the given detection results; JDT-based methods combine both of models (detection and tracking) as an end-to-end network Zhou et al. (2020) or tracking module interacts with the detection module in feature layer Peng et al. (2020). Therefore, these methods are slightly higher than TBD-based trackers. Our proposal achieves excellent performance compared to other TBD-based trackers. For multiple-camera tracking, Duke-MTMCT datasets are captured from surveillance on static camera (shown in Table 6). Our proposal effectively computes the trajectory motion rules and accurately associates tracklets from different cameras. Due to cleaving and re-connection step, for Single-Camera we reach 85.6% on MOTA and accomplish the highest scores of MT, ML, FN. For cross-camera, most of the traditional methods Maksai et al. (2017); Ristani et al. (2017); Liang and Zhou (2017); Tesfaye et al. (2017); Yoon et al. (2016); Zhang et al. (2017) neglect the temporal-spatial information, and Jiang et al. (2018) only constructs topological graph for matching trajectory. Our position projection strategy can correctly describes the real tracklet position, we can eliminate irrelevant trajectories by trajectory appearing time and predicted position, therefore we greatly enhance the ID Recall (IDR), which is up to 73.5%, meanwhile our IDF1 gets the highest score on Duke-MTMCT.

Conclusion
In this paper, for single camera tracking, we propose cleaving and re-connection networks to process the tracklets on crowd or long-term occlusion by Deep Siamese Bi-GRU. The method examines each output of bidirectional GRU to search the suitable split position and match the tracklets to reconnect the same person. For training, we extract the tracklet dataset from existing MOT datasets for training our frameworks. Our proposal has better performance for static camera such as surveillance. The algorithm achieves 55.0%, 57.1% and 85.6% in MOTA that approach to the state-of-the-art methods on MOT16, MOT17 and Duke-MTMCT benchmark dataset, respectively, where the visualization tracking results at different phase is shown in Fig. 14, and single camera tracking result is shown in Fig. 15. For multiple-camera tracking, we present a position projection strategy to convert the tracklet position from camera-coordinate to world-coordinate. We only annotate few point-pairs to train the Position Projection Network. Figure 16 illustrates the position projection and multiple-camera tracking results on Duke-MTMCT, where the left of the figure is the world map, and the right of figure shows each camera view. The same numbers between world map and camera image indicate projection areas. For trajectory matching, we predict the tracklet position in the future on the world map, and extract the tracklet general features by the re-connection network, meanwhile we add the temporal-spatial constraint to reduce the unavailable pairs. The final cross-camera tracking results are shown in Fig. 17. The crowd orderly appear on the camera No.1, No.2 and No.5, where the bottom of the images illustrates the person tracklet in different cameras.

Fig. 14
figure 14
The visualization tracking results at different phase

Full size image
Fig. 15
figure 15
Single-camera Tracking results on testing set

Full size image
Fig. 16
figure 16
Multiple-camera Tracking and Position Projection results on Duke-MTMCT. Left: world map Right: 8-cameras tracking results (In order to illustrate the tracking and position projection results more clearly, we mark the ID of corresponding camera in the world map respectively)

Full size image
Fig. 17
figure 17
Cross-camera Tracking results on Duke-MTMCT testing set. Top-right: The 243345-th frame on Camera 1; Top-middle: The 245913-rd frame on Camera 2; Top-left: The 247216-th frame on Camera 5. The bottom 3 lines: No.436, No.449 and No.485 tracklets on different cameras

Full size image
In the future, we will further research on the cleaving phase. To completely cleave the impure tracklets which contain more than two sub-tracklets, we will utilize the “Transformer” to calculate the similarity of any two moments of impure tracking to replace the GRU. In addition, we will explore how to optimize the occlusion problem, i.e., we intend to combine the head-detector and body-detector as collaborative track of pedestrians. To describe the general features effectively, we will consult to the video-based person ReID methods such as 3D CNN model, non-local, and attention strategy. To extract the interaction feature, we will explore the movement relationships between pedestrians. We will also increase the processing efficiency. Lastly, we attempt to utilize transfer learning to improve the model robustness. For the overlapping regions cross-camera tracking task, we firstly determine the overlapping regions between the cameras. We match the tracklets according to the similarity between tracklets’ appearance and calculating the distance between the tracklets’ positions in the world map. And then, we set a threshold to judge whether the tracklets belong to the same person. As for the person making turn, we can properly enlarge the radius 𝜂𝑐 to cover this case. In addition, we need to improve the appearance model in the future to further enhance the matching accuracy.