Multiple Object Tracking (MOT) has attracted increasing interests in recent years, which plays a significant role in video analysis. MOT aims to track the specific targets as whole trajectories and locate the positions of the trajectory at different times. These trajectories are usually applied in Action Recognition, Anomaly Detection, Crowd Analysis and Multiple-Camera Tracking, etc. However, existing methods are still a challenge in complex scene. Generating false (impure, incomplete) tracklets directly affects the performance of subsequent tasks. Therefore, we propose a novel architecture, Siamese Bi-directional GRU, to construct Cleaving Network and Re-connection Network as trajectory post-processing. Cleaving Network is able to split the impure tracklets as several pure sub-tracklets, and Re-connection Network aims to re-connect the tracklets which belong to same person as whole trajectory. In addition, our methods are extended to Multiple-Camera Tracking, however, current methods rarely consider the spatial-temporal constraint, which increases redundant trajectory matching. Therefore, we present Position Projection Network (PPN) to convert trajectory position from local camera-coordinate to global world-coodrinate, which provides adequate and accurate temporal-spatial information for trajectory association. The proposed technique is evaluated over two widely used datasets MOT16 and Duke-MTMCT, and experiments demonstrate its superior effectiveness as compared with the state-of-the-arts.

Access provided by University of Auckland Library

Introduction
Multiple Object Tracking (MOT) is an important task in video surveillance analysis, which aims to locate the position of targets and associate specific targets as whole trajectories at all times, whose outputs (trajectories) are commonly utilized for Action Recognition, Anomaly Detection, Human Behavior Analysis, Crowd Analysis and Multiple-Camera Tracking, etc. However, MOT task still has many difficulties, for example, partial or long-term occlusion deteriorates the description of targets, which affects continuity of trajectory, some tracklets are split as several sub-tracklets when the tracklet is occluded by others; Frequent occlusion and cross-motion in crowd scene usually cause the neighboring target occludes the tracked-target, the target is gradually replaced by another target, and then the tracklet contains one more targets which make the tracklet impure. Furthermore, generating false (impure or incomplete) tracklets directly influences the subsequent tracklet-based tasks, such as Action Recognition and Multiple-Camera Tracking. Therefore, we focus on trajectory post-processing strategy to cleave the impure tracklets and re-connect the same tracklets. In addition, our strategy applies not only in Single Camera Tracking, it is also extended to Multiple-Camera Tracking. However, for trajectory matching in multiple-camera tracking, the existing methods take less consideration on the positional relationship between cameras, which causes redundant trajectory matching process. As a result, the redundant matching affects the calculating efficiency and increases mis-matching probability of trajectory. In this paper, we present multiple stages Multiple-Camera Multiple Object Tracking (MCMOT) framework, which are divided into three parts (as shown in Fig. 1): 1. Single Camera Multiple Object Tracking; 2. Trajectory Position Projection; 3. Temporal-Spatial Constraint Trajectory Matching.

Single Camera Multiple Object Tracking (MOT) aims to identify each object and predict their trajectories in a single camera video sequence. MOT based methods address this problem by data association, which jointly optimize the matching process of bounding boxes detected by a detector within the inter-frames of a sequence. The same individual has regular temporal or spatial cues in video. For example, a person has slight appearance, velocity and direction changes. Therefore, MOT usually depends on the combination of multiple cues (e.g. appearance, motion and interactions) to associate the similar bounding boxes. Although the performance is gradually improving at the MOT Challenges Milan et al. (2016), the effectiveness of MOT is still limited by object detection quality, long-term occlusion and scene complexity. To solve this sophisticated problem, we intend to extract discriminative features, and design more effective association metrics for MOT.

Tracking-by-detection is a dominant solution for MOT, which links similar objects into trajectory by associating their feature representation and bounding box position. Tracking-by-detection is to search the optimal assignment from multiple cues within a set of bounding boxes. For example, the appearance and motion of person are discriminative cues for data association. Currently, deep networks have achieved significant performance improvement in MOT Tang et al. (2017); Chu et al. (2017); Sadeghian et al. (2017). However, some difficulties remain unresolved, such as â€œmis-trackingâ€ and â€œlost-trackingâ€. As shown in Fig. 6, a tracked person is gradually occluded by another person, which causes mis-tracking. As shown in Fig. 7 , a tracklet is split into several fragments, long-term occlusion leads to lost-tracking. Thus trajectory post-processing becomes particularly important for multiple-camera tracking.

Fig. 1
figure 1
The Framework of Multiple-Camera Multiple Object Tracking: 1. Single camera multiple object tracking with tracklet cleaving and re-conection. 2. Position Projection for each trajectory. 3. Cross-camera trajectories matching based on temporal-spatial constraint

Full size image
Multiple-Camera Multiple Object Tracking focuses on associating trajectories from different camerasâ€™ tracking results (illustrated as Fig. 2). Duke-MTMCT Ristani et al. (2017) is a typical example for MCMOT task, which includes 8 cameras in different locations and viewpoints. The existing methods of trajectory matching overly depend on extracting tracklet appearance features, which is regarded as Person Re-identification task in general. However, due to the difference of illumination, the angle of camera and body posture, appearance features are not sufficient to match trajectory. It increases huge amount of operations to match each trajectory one by one and improves the probability of mis-matching. Therefore, incorporating temporal-spatial information eliminates redundant amount of matching operations, and improves matching accuracy.

Fig. 2
figure 2
The illustration of Multiple-Camera Multiple Object Tracking (MCMOT) task. The Tracking Dataset is from Duke-MTMCT, which includes 8 cameras in different location and view, the middle bottom image indicates the overall map

Full size image
Temporal-spatial information contains position, velocity, timestamp and camera ID. The information is utilized for predicting the trajectory position in the future and then narrowing the search area. The relative locations between cameras are also conducive to matching trajectories. Due to the differences of camera locations and viewpoints, we have to correlate the trajectory positions between camera and actual scenario. For example, as shown in Fig. 2, both camera No.5 and No.7 are adjacent. When the person disappears from the bottom of the camera No.5, it will most likely appear from the left side of the camera No.7. For previous methods, to correlate cameras, traditional method Jiang et al. (2018) constructed a topological graph based on camera locations and viewpoints, which needs to calibrate each camera. However if some cameras are changed, they have to re-calibrate and re-construct the topological graph. Therefore, it is necessary to convert the trajectory position to the uniform coordinate such as world-coordinate.

In this paper, we construct multi-stage framework to reduce the number of mis-tracking and lost-tracking. Our proposal consists of several independent modules which includes tracking module and post-processing module. We propose a novel architecture, Siamese Bi-directional GRU (SiaBiGRU) for trajectory post-processing. Based on the SiaBiGRU, we design Cleaving Network to check the purity of tracking and split impure tracklets, and address Re-conection Network to link sub-tracklets as trajectory. Additionally, in order to verify whether our post-processing model can improve the effect of subsequent tasks, we select multiple-camera tracking as subsequent task. In trajectory matching phase, we construct a Position Projection Network (PPN) to convert the trajectory location from camera-coordinate to world-coordinate. Finally, we reduce the search range according to the trajectory motion prediction in world-coordinates and associate the trajectories from different cameras by their general appearance features. The proposed method is divided into three steps as illustrated in Fig. 1: (1.Single Camera Multiple Object Tracking, 2.Trajectory Position Projection, 3.Multiple Camera Trajectory Matching), where Single Camera Tracking (the first step) includes three sub-steps: (a.Tracklet Generation, b.Tracklet Cleaving, c.Tracklet Re-connection, as shown in Fig. 3). Our contributions in this paper are shown as follows:

We design a novel multiple-stages framework for Multiple-Camera Multiple Object Tracking (MCMOT) which includes â€œtrajectory processingâ€ in single camera and temporal-spatial based trajectory association in multiple-camera scene.

For post-processing, we propose a novel Siamese Bi-directional GRU (SiaBiGRU) to cleave the impure tracklets into sub-tracklets and re-connect these sub-tracklets according to their similarity.

For cross-camera trajectory matching, we present a Position Projection Network, which effectively leverages temporal-spatial information by converting the trajectories location from camera-coordinate to world-coordinate.

The proposed model greatly reduces the amount of trajectory matching and further decreases the number of mis-matching. Experiments demonstrate its superior effectiveness and robustness over the state-of-the-arts in MOT Benchmark.

Related Work
Single Camera Multiple Object Tracking
Single Camera Multiple Object Tracking in videos has attracted great attention. Single camera MOT generates trajectories corresponding to each object in a video sequence that is captured by a single camera. The main strategy is to guide object tracking by detection. For example, Tang et al. (2017); Xiang et al. (2016); Choi (2015); Kim et al. (2015); Chen et al. (2017) focus on designing an ingenious data association or multiple hypothesis. Schulter et al. (2017); Levinkov et al. (2017); Maksai et al. (2017) rely on network flow and graph optimization which are powerful approaches for tracking. Bergmann et al. (2019) exploited the bounding box regression of an object detector to predict the position of an object to convert a detector into a Tracktor. Liu et al. (2020) proposed a novel graph representation that takes both the feature of individual object and the relations among objects into consideration. Xiang et al. (2020) presented an end-to-end conditional random field within a unified deep networks. The inter-relation of targets has multiple cues in a sequence including appearance, motion and interaction, which are summarized by Sadeghian et al. (2017). Some scholars have carried out research on tracking cluster and post-processing to improve the tracking performance. For example, Zhang et al. (2020) constructed motion evaluation network and appearance evaluation network to learn long-term features of tracklets for association. Peng et al. (2018) adopted a constrained clustering to piece tracklets according to appearance characteristic of tracklet, and Peng et al. (2020) utilized Box-Plane matching strategy to achieve association.

The appearance model aims to extract person features. For example, Le et al. (2016); Yang Hoon et al. (2016) adopt the appearance model of some early traditional algorithms such as color histogram to represent the image features, or that Choi (2015); Bae and Yoon (2014); Yang and Jia (2016) utilize covariance matrix or hand-crafted keypoint features. Henschel et al. (2017) uses a novel multi-object tracking formulation to incorporate several detectors into a tracking system. Kim et al. (2015) extended the multiple hypothesis by enhancing detection model. Ma et al. (2019) presented an end-to-end deep learning framework for MOT. With the development of deep learning model Zhuang et al. (2018, 2018); Hou et al. (2020); Guo et al. (2020); Li et al. (2020), CNN are gradually utilzed in MOT. Tang et al. (2017); Sadeghian et al. (2017) train the CNN on the basis of person re-identification strategy Liu and Zhang (2020, 2021); Liu et al. (2019) to extract the image features, and Son et al. (2017) utilized the quadruplet loss to enhance the feature expression. Chu et al. (2017) builds the CNN model to generate visibility maps to solve the occlusion problem. Wang et al. (2016); Bae and Yoon (2014) are presented to improve the tracklet association and tracklet confidence to perform the tracklet task. Ma et al. (2021) adopted human-interaction model to improve the representation of targets in crowd scene.

The motion model defines the rule of object movement, which is divided into linear position prediction Son et al. (2017) and non-linear position prediction Dicle et al. (2013). Zhu et al. (2018); Gao and Jiang (2016) proposed spatial and temporal attention mechanisms to enhance the performance of MOT. Following the success of RNN models for sequence prediction tasks, Alahi et al. (2016) proposed social-LSTM to predict the position of each person in the scene. The interaction model described the inter-relationship of different pedestrians in the same scene. Yang Hoon et al. (2016) designed the structural constraint by the location of people to optimize assignment. In addition, Henschel et al. (2018) used a novel multi-object tracking formulation to incorporate several detectors into a tracking system. Ma et al. (2018) addressed a sophisticated model to process trajectories.

Multiple-Camera Multiple Object Tracking
Given the trajectories generated by the single camera MOT, cross-camera MOT further associates trajectories corresponding to the same object that are captured by different cameras. Ristani and Tomasi (2018); Jiang et al. (2018); Yoon et al. (2016); Tesfaye et al. (2017); Zhang et al. (2017); Maksai et al. (2017) are evaluated at the Duke-MTMCT Ristani et al. (2017) benchmark. Maksai et al. (2017) presented a Non-Markovian method to impose global consistency by using behavioral patterns to guide the tracking algorithms. Ristani and Tomasi (2018) proposed an adaptive weighted triplet loss for training and a new technique for hard-identity mining on extracting appearance feature. Yoon et al. (2016) applied a multiple hypothesis tracking (MHT) to handle the tracking problem with disjoint views. Jiang et al. (2018) addressed an orientation-driven person ReID and an effective camera topology estimation based on appearance feature for online inter-camera trajectory association. Cai and Medioni (2016) operated by comparing entry/exit rates across pairs of cameras. Berdereck et al. (2012) relied on completely overlapping and unobstructed views. Chen et al. (2011) built an adaptive and unsupervised method for a camera network, it can incrementally refine the clustering results of the entry/exit zones and the transition time probability distributions. Tesfaye et al. (2017) proposed a unified three-layer hierarchical approach for solving tracking problems in multiple non-overlapping cameras. [51] designed a system of multiple interacting targets in a camera network which decides the group state of each trajectory.

Single Camera Multi-Object Tracking
To generate accurate and robust trajectories from every single camera, we design multiple-stage single camera MOT framework, which is divided into three modules (as illustrated in Fig. 3): A. tracklet generation aims to generate tracklet candidates using bounding boxes of appearance and motion features; B. tracklet cleaving aims to estimate suitable split positions for impure tracklets; C. tracklet re-connection aims to associate the sub-tracklets which belong to the same person. The cleaving and re-connection processes are tracklet-to-tracklet based method. Fig. 5 shows the architecture of cleaving network and re-connection network. In this Section, the data association metric which generates tracklets from relatively sparse scenario as the tracklet candidate is described in Sect. 3.1(A). We present the reason why the algorithm mis-tracks the other people and how to estimate the tracklet reliability and split the impure tracklets in Sect. 3.2(B). Section 3.3(C) gives the traclets re-connection and association strategy, moreover, the training method of our network is also discussed.

Fig. 3
figure 3
Single Camera MOT are divide into three steps: 1.Tracklet Generation: Tracklets are generated by appearance and motion feature 2.Tracklet Cleaving: The impure tracklets are split by cleaving network 3.Tracklet Re-connection: tracklets which belong to the same person are linked by re-connection network

Full size image
Tracklet Generation
Tracklet Generation is a online sequential process that associates the bounding boxes of high similarity frame by frame to generate tracklet candidates (as described in Fig. 4). The set of nodes (bounding boxes) are composed of detection îˆ° and bounding box candidates îˆ¯. We denote the set of detection bounding boxes îˆ°ğ‘¡ (ğ‘‘ğ‘˜ğ‘¡âˆˆîˆ°ğ‘¡), where ğ‘‘ğ‘˜ğ‘¡ indicates the k-th detection bounding box in frame t. îˆ¯ğ‘¡ (ğ‘ğ‘˜ğ‘›âˆˆîˆ¯ğ‘¡;ğ‘›â‰¤ğ‘¡,îˆ¯ğ‘¡=îˆ¯ğ‘¡âˆ’1â‹ƒîˆ°ğ‘¡âˆ’1) denotes the set of tracked object candidates, where ğ‘ğ‘˜ğ‘¡ is the k-th candidate in frame t (all of the red dots which include unassigned dots and previous residual of îˆ°ğ‘¡âˆ’1 for t-th frame in Fig. 4 are attributable to îˆ¯ğ‘¡). To connect the candidates and detection within inter-frames, we match the candidates ğ‘ğ‘˜ğ‘¡ and ğ‘‘ğ‘˜ğ‘¡ in a bipartite graph with Hungarian algorithm Sahbani and Adiprawita (2017). The bipartite graph ğº=(î‰‚,îˆ±) whose nodes V are divided into left part îˆ¯ğ‘¡âˆˆî‰‚ğ¿ and right part îˆ°ğ‘¡âˆˆî‰‚ğ‘…, ğ‘’ğ‘–ğ‘—âˆˆîˆ± is the edge of ğ‘ğ‘–ğ‘¡ and ğ‘‘ğ‘—ğ‘¡. Each node (bounding box) is defined as 10 dimensions [cid, id, t, x, y, w, h, wx, wy, s], which represents the camera id, tracklet id by tracker, the bounding box frame, the left-top position (x, y), width and height of the bounding box, the world-coordinate (wx, wy) and the state of the tracklet, respectively. The state of tracklet includes â€œtrackedâ€, â€œlostâ€ and â€œquitâ€, which are similar to Markov Decision Processes Xiang et al. (2016). If the node is associated by another node in the next frame, the node statement is labeled â€œtrackedâ€ (the blue and indigo dots in Fig. 4). On the contrary, due to objects â€œescapingâ€ the sight, the unassigned nodes are labeled â€œlostâ€ (the red dots in Fig. 4, lost node generally appears at the tail of tracklet). We define search interval length to be ğœ‚ğ‘  frames, if the â€œlostâ€ node is found to be associated within ğœ‚ğ‘  frames, its state changes from â€œlostâ€ to â€œtrackedâ€, otherwise, the nodes from whole tracklet states are labeled â€œquitâ€. To generate tracklet candidates, the bipartite graphâ€™s edge weights between nodes are defined as ğ‘’ğ‘–ğ‘—=îˆ¿(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡), where îˆ¿(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡) estimates the similarity between two nodes. The overall cost function can thus be determined as follows:

îˆ¿(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡)=ğœ†ğ‘ğ¹ğ‘(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡)+ğœ†ğ‘šğ¹ğ‘š(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡)
(1)
ğ¹ğ‘(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡)=1âˆ’ğ‘ğ‘œğ‘ (ğ‘“ğ‘ğ‘–ğ‘¡,ğ‘“ğ‘‘ğ‘—ğ‘¡)
(2)
ğ¹ğ‘š(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡)=âˆ¥ğ‘Ì‚ ğ‘ğ‘–ğ‘¡âˆ’ğ‘ğ‘‘ğ‘—ğ‘¡âˆ¥22
(3)
where ğ¹ğ‘(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡) denotes the appearance similarity between ğ‘ğ‘–ğ‘¡ and ğ‘‘ğ‘—ğ‘¡. Function cos(A, B) is formulated as ğ´â‹…ğµ|ğ´|â‹…|ğµ|. The features ğ‘“ğ‘ğ‘–ğ‘¡,ğ‘“ğ‘‘ğ‘—ğ‘¡ are extracted by appearance model. ğœ†ğ‘,ğœ†ğ‘š are the weight coefficients of the function. ğ¹ğ‘š(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡) estimates the motion distance between the detection position ğ‘ğ‘‘ğ‘—ğ‘¡ and candidate prediction position ğ‘Ì‚ ğ‘ğ‘–ğ‘¡, which is defined in 4 dimensions [ğ‘¥Ì‚ ,ğ‘¦Ì‚ ,ğ‘¤Ì‚ ,â„Ì‚ ] that stand for the prediction of x, y-coordinate, width and height, respectively.

Appearance model extracts the pedestrian appearance features (e.g. color, shape and texture). Coherent understanding of the pedestrian appearance and further discriminative feature representation are essential for node matching in MOT. We adopt the Person Re-identification method as the appearance model Luo et al. (2019). The total loss of appearance model is defined as

îˆ¸ğ‘ğ‘ğ‘=ğœ†ğ‘–ğ‘‘îˆ¸ğ‘–ğ‘‘+ğœ†ğ‘¡ğ‘Ÿğ‘–îˆ¸ğ‘¡ğ‘Ÿğ‘–+ğœ†ğ‘îˆ¸ğ‘
(4)
which combines three types of loss (ID Loss: îˆ¸ğ‘–ğ‘‘Zheng et al. (2018), Triplet Loss: îˆ¸ğ‘¡ğ‘Ÿğ‘– Hermans et al. (2017) and Center Loss: îˆ¸ğ‘ Wen et al. (2016)) together to train the appearance model, where the ğœ†ğ‘–ğ‘‘, ğœ†ğ‘¡ğ‘Ÿğ‘– and ğœ†ğ‘ are the loss weights to adjust training effectiveness. Appearance Extraction is treated as the multi classification task, which aims to classify the embedding feature of person image in the hyperspace.

The ID loss is formulated as:

îˆ¸ğ‘–ğ‘‘=âˆ‘ğ‘£=1ğ‘âˆ’ğ‘ğ‘£ğ‘™ğ‘œğ‘”(ğ‘ğ‘£^),ğ‘ğ‘£^=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘“ğ‘ğ‘™ğ‘ ,ğ‘£)
(5)
where îˆ¸ğ‘–ğ‘‘(ğ‘“ğ‘ğ‘™ğ‘ ,ğ‘£) indicates the cross-entropy loss, ğ‘“ğ‘ğ‘™ğ‘ ,ğ‘£ is the classification feature after the output of the CNN ğ‘“ğ‘£ by fully-connected layer, where v represents the node from ğ¶ğ‘–ğ‘¡ or ğ·ğ‘–ğ‘¡ ğ‘ğ‘£^ denotes the predict probability of classification, which is the output of the softmax. N and ğ‘ğ‘£ indicate the number of class and ground truth label, respectively.

Fig. 4
figure 4
The process of Tracklet Generation, the tracker aims to associate the bounding box frame by frame. The yellow and red dots for t-th frame belong to îˆ°ğ‘¡ and îˆ¯ğ‘¡, respectively. The blue and indigo dots indicate the â€™trackedâ€™ nodes, where blue dot is the initial position of tracklet. For each step, we construct the Bipartite Graph ğºğ‘¡ by ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡ and optimize the graph, and then corresponding ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡ are updated to ğ‘ğ‘–ğ‘¡+1,ğ‘‘ğ‘—ğ‘¡+1 for next step. We show the processing result of the first six frames (omitting the 5-th frame). The last frame corresponding ğ¶6,ğ·6 and î‰€6 are shown at bottom of figure

Full size image
The triplet loss is a metric learning, given an anchor node ğ‘£ğ‘, and the corresponding embedding feature ğ‘“ğ‘£ğ‘, and we select the same class node ğ‘£ğ‘ and different class node ğ‘£ğ‘› as the positive and negative nodes, respectively. The triplet loss is defined as:

îˆ¸ğ‘¡ğ‘Ÿğ‘–=[ğ‘‘ğ‘âˆ’ğ‘‘ğ‘›+ğ›¼]+ğ‘‘ğ‘=âˆ¥ğ‘“ğ‘£ğ‘âˆ’ğ‘“ğ‘£ğ‘âˆ¥22,ğ‘‘ğ‘›=âˆ¥ğ‘“ğ‘£ğ‘âˆ’ğ‘“ğ‘£ğ‘›âˆ¥22
(6)
Fig. 5
figure 5
The architecture of tracklet cleaving and re-connection network, a Cleaving the tracklets by bidirectional outputs of GRU, b Re-connecting the tracklets by the features of siamese GRU.

Full size image
where the ğ‘‘ğ‘ and ğ‘‘ğ‘› indicate the Euclidean distance of positive pair and negative pair, ğ›¼ is the distance threshold and [ âˆ— ]+ is equivalent to function ğ‘šğ‘ğ‘¥( âˆ— ,0).

The center loss is defined as:

îˆ¸ğ‘=12âˆ‘ğ‘–=1ğ‘šâˆ¥ğ‘“ğ‘£âˆ’ğ‘ğ‘¦ğ‘£âˆ¥22
(7)
where m is the number of batch size, and ğ‘¦ğ‘£ indicates the label of the v-th image in a mini-batch, ğ‘ğ‘¦ğ‘£ denotes the ğ‘¦ğ‘£-th class center of deep features.

After training appearance model, the output of CNN model ğ‘“ğ‘£ is ğ¿2 normalized and utilized for Eq. 2 to calculate the similarity of appearance cue.

Motion model analyzes the pedestrian movement rule and predict the position in the future. The inputs of motion model include historical location of tracklet and its corresponding timestamp. The architecture of motion model is a LSTM, which is able to learn sequential data. We construct tracklets ground truth position as the LSTM training set. The inputs of LSTM are the tracklet historical position [ğ‘ğ‘¢ğ‘¡, ğ‘ğ‘¢ğ‘¡+1, ğ‘ğ‘¢ğ‘¡+2, ...], where ğ‘ğ‘¢ğ‘¡ means the u-th tracklet posotion in frame t. The outputs of LSTM are the predicted positions in the next frame, [ğ‘Ì‚ ğ‘¢ğ‘¡+1, ğ‘Ì‚ ğ‘¢ğ‘¡+2, ğ‘Ì‚ ğ‘¢ğ‘¡+3, ...]. We use the actual position of tracklet to supervise the LSTM. The position loss is described as

îˆ¸ğ‘šğ‘œğ‘¡=âˆ‘ğ‘–=1ğ¿ğ‘¢âˆ’1âˆ¥ğ‘Ì‚ ğ‘¢ğ‘–âˆ’ğ‘ğ‘¢ğ‘–âˆ¥22
(8)
where ğ¿ğ‘¢ is the length of u-th tracklet. we compute the distance between the predicted and the actual position. After training motion model, the position of each frame is input into the LSTM to generate the position for future frames.

After bipartite graph construction, we adopt Hungarian Algorithm Sahbani and Adiprawita (2017) to optimize the bipartite graph and obtain association results. To evaluate the performance of optimized results, we define a target function ğ¹ğº to calculate the differences between the ground truth graph ğºğ‘”ğ‘¡ğ‘¡ and the optimized graph result ğºğ‘¡^, which is given by

ğ¹ğº(ğºğ‘¡^,ğºğ‘”ğ‘¡ğ‘¡)=âˆ‘ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—âˆˆîˆ±ğ‘”ğ‘¡ğ‘¡(ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—âˆ’ğ‘’Ì‚ ğ‘–ğ‘—)+âˆ‘ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—âˆ‰îˆ±ğ‘”ğ‘¡ğ‘¡ğœâˆ—(ğ‘’Ì‚ ğ‘–ğ‘—âˆ’ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—)
(9)
ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘— indicate the ground truth connection between node i and j in ğºğ‘”ğ‘¡ğ‘¡, the Eq.9 is divided into two parts by the plus (â€œ+â€). The left part ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—âˆˆîˆ±ğ‘”ğ‘¡ğ‘¡ means that ğ‘ğ‘–ğ‘¡ and ğ‘‘ğ‘—ğ‘¡ are associated, ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—â‰¡1, and vice versa, ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—â‰¡0 at the right part. ğ‘’ğ‘–ğ‘—^={0,1} is the optimized edge of ğºğ‘¡. If the ğ‘’ğ‘–ğ‘—^=0 at the left part, but ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—=1 in ground truth, the same targets are not connected, the tracked target will be lost (lost-tracking). If the ğ‘’ğ‘–ğ‘—^=1 at the right part, but ğ‘’ğ‘”ğ‘¡ğ‘–ğ‘—=0 in ground truth, the different targets are connected, the tracked target will be switched to other target (mis-tracking). Since the negative effects of mis-tracking is greater than lost-tracking, we define a weight ğœ to focus more on mis-tracking, ğœ is set to 2. Finally, we select the optimal models with minimum score (output of Eq.9) from all of models as Tracklet Generation model.

The generated tracklets ğœğ‘˜âˆˆî‰€ have the following attributions: ğœğ‘˜: [ğœğ‘˜[ğ‘–ğ‘‘],ğœğ‘˜[ğ‘¡ğ‘ ],ğœğ‘˜[ğ‘¡ğ‘’],ğœğ‘˜[ğ¯ğ‘ ],ğœğ‘˜[ğ¯ğ‘’],ğœğ‘˜[ğ‘™], ğœğ‘˜[ğ‘Ÿğ‘˜ğ‘¡]ğ‘™Ã—10,ğœğ‘˜[ğ¯ğ‘˜ğ‘¡]ğ‘™Ã—1,ğœğ‘˜[ğ‘ ]], which are tracklet id, start frame, end frame, start velocity, end velocity, tracklet length and all of the nodes, where nodes ğœğ‘˜[ğ‘Ÿğ‘˜ğ‘¡] is a ğ‘™Ã—10 matrix, which contains all of the node information (each row stands for a node), ğœğ‘˜[ğ¯ğ‘˜ğ‘¡]ğ‘™Ã—1 records the velocity at all times. ğœğ‘˜[ğ‘ ] is state of the tracklet, which includes â€œtrackedâ€, â€œlostâ€ and â€œquitâ€.

Tracklet Cleaving
After Tracklet Generation, we have a set of tracklet î‰€ in sequence. However, the Tracklet Generation may mis-track the wrong person when two persons have cross-motion or occlude each other, which degrades the generated tracklet purity. Fig. 6 shows an example of an impure tracklet, when another person (blue shirt) gradually occludes the target person (white shirt). Even though the two pedestrians present different apperance, when a large area of the target is occluded by the occluder, the bounding boxes related to target and the occluder are indistinguishable. Traditional methods only considers the bounding box in short-term adjacent frames. As the result, the target is replaced by occluder. Meanwhile, due to the two pedestrians being that the same position, the appearance and motion model on tracklet generation are not able to check whether tracker is mis-tracking.

Fig. 6
figure 6
The example of impure tracklet (mis-tracking), cleaving network aims to split the impure tracklet as two pure tracklet. The pink line is the most suitable split position for the example

Full size image
To guarantee the tracklet being the same person, we design a bidirectional output Gated Recurrent Unit to estimate the tracklet purity and cleave the false tracklets. Tracklet Cleaving is an end-to-end training network to check whether the tracklet is pure and search the suitable split position of impure tracklet. We define the pure tracklets î‰€+ and impure tracklets î‰€âˆ’ as:

{ğœğ‘˜âˆˆî‰€+ğœğ‘˜âˆˆî‰€âˆ’âˆ€ğ‘–,ğ‘—,âˆƒğ‘–,ğ‘—,ğ‘Ÿğ‘˜ğ‘–,ğ‘Ÿğ‘˜ğ‘—âˆˆğœğ‘˜,ğ‘Ÿğ‘˜ğ‘–(ğ‘–ğ‘‘)â‰¡ğ‘Ÿğ‘˜ğ‘—(ğ‘–ğ‘‘)ğ‘Ÿğ‘˜ğ‘–,ğ‘Ÿğ‘˜ğ‘—âˆˆğœğ‘˜,ğ‘Ÿğ‘˜ğ‘–(ğ‘–ğ‘‘)â‰ ğ‘Ÿğ‘˜ğ‘—(ğ‘–ğ‘‘)
(10)
where ğ‘Ÿğ‘˜ğ‘–,ğ‘Ÿğ‘˜ğ‘— are the i-th, j-th elements on tracklet ğœğ‘˜. All of the tracklets ğœğ‘˜âˆˆî‰€,ğ‘˜âˆˆğ¾ are fed into the Cleaving Network to check purity of the tracklets, and search the best split position of impure tracklets. The tracklet Cleaving Network is shown in Fig. 6. First of all, we utilize the CNN to extract the image features ğœ‘ğ‘˜ğ‘,ğ‘–, ğ‘–âˆˆ[1,ğ¿ğ‘˜] from the tracklet, ğ¿ğ‘˜ is the k-th length of tracklet. Secondly, all the features ğœ‘ğ‘˜ğ‘,ğ‘– are input into the forward-GRU and backward-GRU, respectively. Both GRUs have the shared weights, and the output is ğœ‘ğ‘˜ğ‘”,ğ‘–, ğ‘–âˆˆ[âˆ’ğ¿ğ‘˜,âˆ’1]âˆª[1,ğ¿ğ‘˜], the positive and negative superscript values stand for forward and backward features from GRU. And then, we calculate the adjacent vectors distance between the features from the forward and the backward(e.g. length=10, {ğœ‘ğ‘˜ğ‘”,1,ğœ‘ğ‘˜ğ‘”,âˆ’9},{ğœ‘ğ‘˜ğ‘”,2,ğœ‘ğ‘˜ğ‘”,âˆ’8},... ) as a series of feature distance to concatenate as an 1 Ã—(ğ¿ğ‘˜âˆ’1) vector ğœ‘ğ‘˜ğ‘‘:

ğœ‘ğ‘˜ğ‘‘,ğ‘–=âˆ¥ğœ‘ğ‘˜ğ‘”,ğ‘–âˆ’ğœ‘ğ‘˜ğ‘”,ğ‘–âˆ’ğ¿ğ‘˜âˆ¥22, ğ‘–âˆˆ[1,ğ¿ğ‘˜âˆ’1]ğœ‘ğ‘˜ğ‘‘=([ğœ‘ğ‘˜ğ‘‘,1,ğœ‘ğ‘˜ğ‘‘,2,...,ğœ‘ğ‘˜ğ‘‘,ğ¿ğ‘˜])
(11)
The algorithm calculates the distance ğœ‘ğ‘˜ğ‘‘,ğ‘– between the features from the left to current position i and the right to corresponding position ğ‘–âˆ’ğ¿ğ‘˜. ğœ‘ğ‘˜ğ‘‘,ğ‘– is fed into two fully-connected layers separately after normalization. First output of the FC layer ğœ‘ğ‘˜ğ‘  is used for searching the most suitable split position, which generally appears at the maximum disparity from these distances. The output of other FC layer ğœ‘ğ‘˜ğ‘ is utilized for checking whether the tracklet is pure. The advantage of GRU is to be able to summarize the general characteristics with the same person and eliminate occlusion in order to obtain preferable feature expression. For training Cleaving Network, the cleaving loss îˆ¸ğ‘ğ‘™ğ‘£ is defined as:

îˆ¸ğ‘ğ‘™ğ‘£=ğœ†ğ‘“îˆ¸ğ‘,ğ‘“ğ‘¡ğ‘Ÿ+ğœ†ğ‘ îˆ¸ğ‘,ğ‘ ğ‘Ÿâ„+ğœ†ğ‘îˆ¸ğ‘,ğ‘ğ‘¢ğ‘Ÿ
(12)
The feature loss îˆ¸ğ‘,ğ‘“ğ‘¡ğ‘Ÿ calculates the difference between each output of GRU ğœ‘ğ‘˜ğ‘“,ğ‘–, ğ‘–âˆˆ[âˆ’ğ¿ğ‘˜,âˆ’1]âˆª[1,ğ¿ğ‘˜] and the ground truth. The searching loss îˆ¸ğ‘,ğ‘ ğ‘Ÿâ„ measures distance between the predict split position and real split position. The purity loss is a Binary task to differentiate whether the tracket is pure. Eq.12 is expanded as:

îˆ¸ğ‘ğ‘™ğ‘£=âˆ‘ğ‘˜âˆˆğ¾(ğœ†ğ‘“2ğ¿ğ‘˜(âˆ‘ğ‘–=âˆ’ğ¿ğ‘˜âˆ’1ğ¹ğœ‰(ğœ‘ğ‘˜ğ‘”,ğ‘–)+âˆ‘ğ‘—=1ğ¿ğ‘˜ğ¹ğœ‰(ğœ‘ğ‘˜ğ‘”,ğ‘—))+ğœ†ğ‘ ğ¹ğœ„(ğœ‘ğ‘˜ğ‘ )+ğœ†ğ‘ğ¹2(ğœ‘ğ‘˜ğ‘‘))
(13)
where ğœ†âˆ— is the loss weight coefficient, K indicates the number of the tracklets in training set, ğœ‰ and ğœ„ denote the number of trakclet class and length of k-th tracklets, respectively. ğ¹ğ‘ (ğ‘={ğœ‰, ğœ„, 2}) indicates Cross-Entropy loss:

ğ¹ğ‘=âˆ‘ğ‘–=1ğ‘âˆ’ğ‘ğ‘–ğ‘™ğ‘œğ‘”(ğ‘ğ‘–^),ğ‘ğ‘–^=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğœ‘)
(14)
Pure Loss îˆ¸ğ‘,ğ‘ğ‘¢ğ‘Ÿ is a binary classification task, which supervises ğœ‘ğ‘˜ğ‘ to judge the purity of a tracklet. Search Loss îˆ¸ğ‘,ğ‘ ğ‘Ÿâ„ is a multiple classification task, which is utilized for optimizing ğœ‘ğ‘˜ğ‘  to search the best splitting position. Search Loss îˆ¸ğ‘,ğ‘ ğ‘Ÿâ„ can also be defined by L1-Loss to calculate the differences between prediction split position and ground truth.

When the training cleaving network is completed, the network is used for checking the tracklets ğœğ‘˜âˆˆî‰€ generated from tracklet generation part. In general, the most suitable split position occurs on the maximum feature distance between left-side and right-side for the position. If the ğœğ‘˜ belongs to impure tracklet, it will be split into two tracklets ğœğ‘˜1 and ğœğ‘˜2. To guarantee the tracklet purity, the tracklet cleaving step is vital for tracklet description and cross-camera trajectory matching.

Tracklet Re-connection
After Tracklet Cleaving step, we obtain some pure tracklets. However, long-term occlusion usually breaks the whole tracklet, frequent occlusion in crowd produces the lost-tracking and id-switch for tracklets, and illumination variation influences the appearance feature description and then affects the tracking performance. Fig. 7 is an example of tracklet fragments, which belong to the same person.

Fig. 7
figure 7
The example of fragmented tracklets (lost-tracking), re-connection network aims to connect the fragmented tracklets as a whole tracklet

Full size image
Re-connection focuses on extracting the general features of tracklets and calculating similarity between tracklets and connecting fragmented tracklets as a whole tracklet. The architecture of re-connection network is shown in Fig. 5. For matching tracklets, each tracklet is extracted the general feature to describe the tracklet appearance characteristics. The tracklets with similar general feature will be re-connected. We combine various losses to reduce the within-class distance and enlarge the between-class distance, simultaneously. Our network is designed with the verification loss and identification loss at each GRU output. The re-connection loss is defined as:

îˆ¸ğ‘Ÿğ‘ğ‘›=îˆ¸ğ‘”ğ‘™ğ‘œ+îˆ¸ğ‘™ğ‘œğ‘
(15)
Where the îˆ¸ğ‘”ğ‘™ğ‘œ and îˆ¸ğ‘™ğ‘œğ‘ indicate the global loss and local loss of the network, respectively. We use the contrastive loss by Euclidean distance for the verification and the cross-entropy losses in the multi-classification task for the identification. The identification loss ğ¹(âˆ—) is the same as the Eq.14. The details of the contrastive loss ğ¸(âˆ—) are shown as:

ğ¸(ğœ‘ğ‘˜1ğ‘“,ğœ‘ğ‘˜2ğ‘“)=ğ‘¦âˆ¥ğœ‘ğ‘˜1ğ‘“âˆ’ğœ‘ğ‘˜2ğ‘“âˆ¥22+(1âˆ’ğ‘¦)ğ‘šğ‘ğ‘¥{0,(ğœ‚âˆ’âˆ¥ğœ‘ğ‘˜1ğ‘“âˆ’ğœ‘ğ‘˜2ğ‘“âˆ¥22)}
(16)
where ğœ‘ğ‘˜âˆ—ğ‘“ indicates the output feature of GRU ğœ‘ğ‘˜âˆ—ğ‘” after fully-connected (FC) and ReLU. ğ¸(ğœ‘ğ‘–,ğœ‘ğ‘—) is contrastive function, ğ‘¦âˆˆ{0,1} is the label indicator, ğœ‚ is a margin constant. The representation of loss can be formulated as follows:

 îˆ¸ğ‘”ğ‘™ğ‘œ=ğœ†ğ‘£îˆ¸ğ‘£+ğœ†ğ‘–ğ‘‘(îˆ¸ğ‘–ğ‘‘1+îˆ¸ğ‘–ğ‘‘2)=ğœ†ğ‘£ğ¸(ğœ‘ğ‘˜1ğ‘“,ğœ‘ğ‘˜2ğ‘“)+ğœ†ğ‘–ğ‘‘(ğ¹(ğœ‘ğ‘˜1ğ‘“)+ğ¹(ğœ‘ğ‘˜2ğ‘“))
(17)
ğœ‘ğ‘˜ğ‘“=12ğ¿ğ‘˜(âˆ‘ğ‘–=âˆ’ğ¿ğ‘˜âˆ’1ğœ‘ğ‘˜ğ‘“,ğ‘–+âˆ‘ğ‘—=1ğ¿ğ‘˜ğœ‘ğ‘˜ğ‘“,ğ‘—)
(18)
where ğœ‘ğ‘˜ğ‘“ is the temporal pooling McLaughlin et al. (2016) of each output of GRU.

îˆ¸ğ‘™ğ‘œğ‘=ğœ†ğ‘™ğ‘œğ‘_ğ‘£îˆ¸ğ‘™ğ‘œğ‘_ğ‘£+ğœ†ğ‘™ğ‘œğ‘_ğ‘–ğ‘‘îˆ¸ğ‘™ğ‘œğ‘_ğ‘–ğ‘‘
(19)
îˆ¸ğ‘™ğ‘œğ‘_ğ‘£=âˆ¥ğœ‘ğ‘˜1ğ‘“,1âˆ’ğœ‘ğ‘˜1ğ‘“,ğ¿ğ‘˜1âˆ¥22+âˆ¥ğœ‘ğ‘˜2ğ‘“,1âˆ’ğœ‘ğ‘˜2ğ‘“,ğ¿ğ‘˜2âˆ¥22  âˆ’âˆ¥ğœ‘ğ‘˜1ğ‘“,1âˆ’ğœ‘ğ‘˜2ğ‘“,1âˆ¥22âˆ’âˆ¥ğœ‘ğ‘˜1ğ‘“,ğ¿ğ‘˜1âˆ’ğœ‘ğ‘˜2ğ‘“,ğ¿ğ‘˜2âˆ¥22+ğ›¿
(20)
îˆ¸ğ‘™ğ‘œğ‘_ğ‘–ğ‘‘=âˆ‘ğ‘˜âˆˆğ‘˜1,ğ‘˜2(âˆ‘ğ‘–=âˆ’ğ¿ğ‘˜âˆ’1ğ¹(ğœ‘ğ‘˜ğ‘“,ğ‘–)+âˆ‘ğ‘—=1ğ¿ğ‘˜ğ¹(ğœ‘ğ‘˜ğ‘“,ğ‘—))
(21)
ğœ† is the loss weight coefficient. îˆ¸ğ‘£, îˆ¸ğ‘–ğ‘‘âˆ—, îˆ¸ğ‘™ğ‘œğ‘_ğ‘£ and îˆ¸ğ‘™ğ‘œğ‘_ğ‘–ğ‘‘ denote the verification and identification loss of global and local, respectively. îˆ¸ğ‘™ğ‘œğ‘_ğ‘£ is similar to triplet loss (refer to Son et al. (2017)), including the disparity of head and tail of the tracklet, head between different tracklets and tail between different tracklets. ğ›¿ is the threshold of margin. îˆ¸ğ‘™ğ‘œğ‘_ğ‘–ğ‘‘ is the multi-classification task for each output.

When the training of re-connection network is completed, the network is utilized for extracting tracklet general feature, and the tracklet re-connection step compares several tracklets and connects tracklets {ğœğ‘˜1, ğœğ‘˜2, ğœğ‘˜3, ...} which belong to the same person as the whole tracklet ğœğ‘˜.

Multiple-Camera Tracking
For Multiple-Camera Multiple Object Tracking, we aim to associate the trajectories from different cameras. Each trajectory is generated from the single camera tracking. Section 4.1(A) introduces how to train Position Projection Network (PPN) and position conversion of the trajectories from camera-coordinate to world-coordinate. Section 4.2(B) discusses how to eliminate redundant matching operation by temporal-spatial constraint. Section 4.3(C) describes how to associate the trajectories by similarity of general appearance feature.

Fig. 8
figure 8
Annotated point-pairs for training Position Projection Network , the images are from Duke-MTMCT, where (a) is camera 2 and (b) is camera 5

Full size image
Position Projection
Position Projection focuses on transferring each tracklet position from camera-coordinate to world-coordinate. We propose a deep learning method called Position Projection Network (PPN) to project each point position. Given the input position (camera-coordinate) (ğœğ‘¥, ğœğ‘¦), the target (output) of the network is the world-coordinate (ğœ”ğ‘¥, ğœ”ğ‘¦). We treat the position projection as a fitting task. Compared with the traditional method such as Geometric Camera Calibration Hartley and Zisserman (2003), our method only annotates a few point-pairs between camera image and world map to train PPN instead of calculating the translation matrix and rotation matrix.

Fig. 9
figure 9
The example of Position Projection, the figure shows the tracklets position from camera2 (bottom-right) and camera5 (top-right) transferred to world map (left), a and b are the tracking and projection results at the 246025-th and 247216-th frame (The calibration-frame) respectively

Full size image
Figure 8 shows how to annotate and generate more point-pairs for the training set. The left and right images denote the camera view image and world map. The same point number between camera image and world map indicates the same position. For example Fig. 8a, we only mark 23 points on camera image and world map, respectively, and we choose the representative locations as the point-pairs such as corner-points. Duke-MTMCT dataset has 8-camera sequences, we annotate 8 groups of point-pairs respectively, 8-camera coordinates are projected to the same world map. After point-pair annotation, we get several point-pairs, of which is a vector ğœšğ‘–: [ğ‘ğ‘–ğ‘‘,ğ‘–,ğœğ‘–ğ‘¥,ğœğ‘–ğ‘¦,ğœ”ğ‘–ğ‘¥,ğœ”ğ‘–ğ‘¦], where cid indicates the camera number and i denotes the point number.

In order to improve the precision of projection, we present an interpolation method to enlarge the point-pairs. We divide the scene into several areas according to plane, for example Fig. 8a, points ğœšğ‘âˆˆîˆ­01, a: [1â†’14,18,19,20] belong to the same plane, points ğœšğ‘âˆˆîˆ­02, b: [15,16,18â†’23] belong to the other plane, points ğœšğ‘âˆˆîˆ­03, c: [16, 17, 21, 22, 23] belong to the third plane. For each Area îˆ­âˆ—, we interpolate a new point from the midpoint of neighboring points on camera image and world map, respectively, which is defined as:

îˆ­1âˆ—=îˆ­0âˆ—âˆªîˆ½1âˆ—îˆ½1âˆ—={ğœšğ‘–ğ‘—| ğ‘–,ğ‘—âˆˆîˆ­0âˆ—},ğœšğ‘–ğ‘—=ğ‘šğ‘–ğ‘‘ğ‘ğ‘œğ‘–ğ‘›ğ‘¡(ğœšğ‘–,ğœšğ‘—)
(22)
where îˆ½1âˆ— indicates the generated point-pair set from the midpoint ğœšğ‘–ğ‘—: [ğ‘ğ‘–ğ‘‘,ğ‘–ğ‘—,ğœğ‘–ğ‘—ğ‘¥,ğœğ‘–ğ‘—ğ‘¦,ğœ”ğ‘–ğ‘—ğ‘¥,ğœ”ğ‘–ğ‘—ğ‘¦] of neighboring points ğœšğ‘–, ğœšğ‘—, the point number ij is numbered in order. îˆ½1âˆ— is the combination of original point-pair set and generated point-pair set. âˆ— denotes the area number. Point-pair enlargement is a recursive process, which is computed as:

îˆ­ğ‘ +1âˆ—=îˆ­ğ‘ âˆ—âˆªîˆ½ğ‘ +1âˆ—,    îˆ½ğ‘ +1âˆ—={ğœšğ‘–ğ‘—| ğ‘–,ğ‘—âˆˆîˆ­ğ‘ âˆ—}
(23)
For Duke-MTMCT dataset, we have five iterations to generate 30k point-pair for training PPN. The architecture of PPN contains 3 fully-connected (fc) layers for each camera projection, the camera position [ğœğ‘¥,ğœğ‘¦] are divided by the length and width of the image, respectively, for normalizing the value to 0-1 [ğœğ‘¥Â¯,ğœğ‘¦Â¯] as the PPN input (1Ã—2 vector). The normalized vector is fed into ğ‘“ğ‘1 (2â†’128) â†’ğ‘“ğ‘2 (128â†’128) â†’ğ‘“ğ‘3 (128â†’2), each layer contains fully-connected, batch normalization and ReLU. The output of ğ‘“ğ‘3 is a 1Ã—2 vector [ğœ”ğ‘¥Â¯^,ğœ”ğ‘¦Â¯^]. We utilize the generated world position [ğœ”ğ‘¥,ğœ”ğ‘¦] to supervise PPN. The loss is defined as:

îˆ¸ğ‘=âˆ£âˆ£ğœ”ğ‘¥Â¯âˆ’ğœ”ğ‘¥Â¯^âˆ£âˆ£+âˆ£âˆ£ğœ”ğ‘¦Â¯âˆ’ğœ”ğ‘¦Â¯^âˆ£âˆ£
(24)
where îˆ¸ğ‘ is a L1-loss, [ğœ”ğ‘¥Â¯,ğœ”ğ‘¦Â¯] are normalized position by world position. The PPN is an one-to-one mapping between camera coordinate and world coordinate, so îˆ¸ğ‘ aims to narrow the distance between the predicted position and the real position.

Temporal-Spatial Constraint
We adopt the temporal-spatial constraint to reduce the amount of matching operation. The tracklet positions are projected on the uniform world map. Figure 9 shows the camera2 and camera5 tracking results and projection results at the 289752-th and 290160-th frames. A throng in camera2(a) exits the field of view from the right. For each disappearing tracklet ğœ (introduced in Sect. 3.1(A)), we compute the velocity ğœ[ğ‘£ğ‘’] at the last frames before it disappears, which is defined as:

ğœ[ğ¯ğ‘’]=ğœ[ğ¯ğ¿]=12(ğœ[ğ¯ğ¿âˆ’1]+1ğœ‚ğ‘£âˆ‘ğ‘¡=1ğœ‚ğ‘£(ğ‘ğ¿âˆ’ğ‘ğ¿âˆ’ğ‘¡))
(25)
which is a recursion equation, where ğœ[ğ¯ğ¿] is the velocity of the last frame for ğœ, L is the current length of tracklet ğœğ‘˜. ğœ[ğ¯ğ¿âˆ’1] is the previous frame velocity, and ğœ‚ğ‘£ indicates the velocity computing width parameter, ğ‘ğ¿ denotes the position of the last frame for ğœ. The velocity of the current frame is based on the previous velocity and the average of the vectors between the current position and the previous few positions.

Tracklet association are restricted previously by temporal-spatial constraints. Our proposal eliminates lots of irrelevant trajectories that appear too early or too late as compared with the target duration time, meanwhile, it removes trajectories which are too far away from the target prediction position . The constraint of matching is shown as:

â€–â€–ğœğ‘˜ğ‘—[ğ‘1]âˆ’ğœğ‘˜ğ‘–[ğ‘Ì‚ ğ¿ğ‘˜ğ‘–+Î”ğ‘¡ğ‘–,ğ‘—]â€–â€–22<ğœ‚ğ‘
(26)
where

ğœğ‘˜ğ‘–[ğ‘Ì‚ ğ¿ğ‘˜ğ‘–+Î”ğ‘¡ğ‘–,ğ‘—]=ğœğ‘˜ğ‘–[ğ‘Ì‚ ğ¿ğ‘˜ğ‘–]+Î”ğ‘¡ğ‘–,ğ‘—âˆ—ğœğ‘˜ğ‘–[ğ¯ğ‘’], Î”ğ‘¡ğ‘–,ğ‘—=ğœğ‘˜ğ‘—[ğ‘¡ğ‘ ]âˆ’ğœğ‘˜ğ‘–[ğ‘¡ğ‘’],ğ‘ .ğ‘¡. 0<ğœğ‘˜ğ‘—[ğ‘¡ğ‘ ]âˆ’ğœğ‘˜ğ‘–[ğ‘¡ğ‘’]<ğœ‚ğ‘¡; ğœğ‘˜ğ‘–[ğ‘ ],ğœğ‘˜ğ‘—[ğ‘ ]â‰ ğ‘ğ‘¢ğ‘–ğ‘¡ğ‘¡ğ‘’ğ‘‘
(27)
ğœğ‘˜ğ‘—[ğ‘1] indicates the position of ğœğ‘˜ğ‘— at the first frame of tracklet. ğœğ‘˜ğ‘–[ğ‘Ì‚ ğ¿ğ‘˜ğ‘–+Î”ğ‘¡ğ‘–,ğ‘—] is the predicted position of the ğœğ‘˜ğ‘— in the future. ğœ‚ğ‘ is the spatial constraint parameter. Î”ğ‘¡ğ‘–,ğ‘— is the interval between start frame ğœğ‘˜ğ‘— and end frame ğœğ‘˜ğ‘–. ğœ‚ğ‘¡ is the temporal constraint parameter. ğœ[ğ‘ ] are the states of the tracklet, which contains â€œtrackedâ€, â€œlostâ€ and â€œquitâ€(same as the node states in Section 3.1(A)).

Trajectory Association
Trajectory matching candidates are filtered by temporal-spatial constraint. Trajectory association is based on calculating the general appearance feature distance between Trajectories. The trajectory general feature is extracted by Re-connection Network (Sec.3.3(C)). Given a trajectory ğœğ‘˜âˆˆî‰€, and its corresponding trajectory candidate set with constraint is î‰€ğ‘˜:{ğœ1ğ‘˜,ğœ2ğ‘˜,ğœ3ğ‘˜,...}. We calculate the Cosine distance between ğœğ‘˜ and each trajectory candidate ğœâˆ—ğ‘˜. We define an association threshold, ğœƒğ‘…, and associate the trajectories of which the distance is less than ğœƒğ‘…. At last, we utilize â€œUnion-Find-Setâ€ to classify the connected subset. Each subset indicates a whole trajectory.

Table 1 The performance with different steps on MOT16 validation set
Full size table
Experiments
Datasets and Benchmarks
We evaluate our proposal on MOT16 Milan et al. (2016) and Duke-MTMCT Ristani et al. (2017) datasets, which contain large-scale video sequences from different cameras and scenes. Both datasetsâ€™ tracking results are evaluated on MOT Challenge Leal-Taix et al. (2015). We additionally use the Person ReID dataset Market-1501 Zheng et al. (2015) and DukeMTMC-reID Ristani et al. (2017) to train Appearance Model, utilize tracking dataset PathTrack Manen et al. (2017), Duke-MTMCT Ristani et al. (2017) and video re-identification dataset MARS Zheng et al. (2016) to train Cleaving and Re-connection Network. The Motion Model and Position Projection Networks are trained on Duke-MTMCT.

Duke-MTMCT is a large-scale dataset for multiple target multiple-camera tracking with the videos captured by 8 surveillance cameras at different viewing angles including 2800 identities (persons) in Duke University. The video duration of each camera is 86 minutes, which is split into training set (0â€“50 min) and testing set (50-86 min). In addition, the dataset provides DPM Felzenszwalb et al. (2010) and Openpose Cao et al. (2018) detection results for each frame as the tracker input. However, currently the dataset has been removed from the MOT benchmark.

MOT16 is a classical evaluation dataset comparing several tracking methods on MOT Challenge, which includes 14 sequences captured from surveillance, hand-held shooting and driving recorder by static cameras and moving cameras. The length of each video is about 500-1500 frames. And the dataset also provides the detection DPM.

Implementation Details
In our experiments, our networks consist of CNN, LSTM and GRU, where GRU Cho et al. (2014) is a type of RNN with gates and hidden units. For tracklet generation, we train the CNN network of appearance model with SeResNet50 Hu et al. (2018), the images are resized to 128*256 from ReID training set and the output of CNN ğ‘“ğ‘ğ‘–ğ‘¡,ğ‘“ğ‘‘ğ‘—ğ‘¡ produces a 2048-dimensional vector to describe the image. In addition, the inputs of the LSTM network for motion model is a series of 2-dimensional vector ğ‘ğ‘ğ‘–ğ‘¡: [x, y] with a tracklet, which are divided by image width and height to have the input normalized, and the LSTM output is the prediction of the position and size ğ‘Ì‚ ğ‘ğ‘–ğ‘¡:[ğ‘¥Ì‚ ,ğ‘¦Ì‚ ]. For tracklet cleaving and re-connection, the model is a deep Siamese Bi-GRU, which includes four hidden-layers and the maximum length of GRU is 120 frames. The input of the GRU is a series of appearance features by CNN. The outputs of the GRU ğœ‘ğ‘˜ğ‘”,ğ‘–,ğ‘–âˆˆ[âˆ’ğ¿ğ‘˜,âˆ’1]âˆª[1,ğ¿ğ‘˜] are 128-dimensional vectors, which are fed to FC network for verification loss and for comparison of the corresponding features for classification loss. At single-camera tracking, association threshold ğœƒğº=0.7, tracklet matching threshold ğœƒğ‘…=0.5. The searching interval length ğœ‚ğ‘  is 100 frames.

For cross-camera, the spatial constraint parameter ğœ‚ğ‘=300 pixels on world map, the temporal constraint parameter ğœ‚ğ‘¡=3000 frames to retrieve tracklets. We associate the tracklets which satisfy the constraint by the Re-connection Network output ğœ‘ğ‘˜ğ‘“. The tracklet association is the tracklet similarity matching task, which compares the distance of feature pair by pair. The closer the distance of feature, the more similar the image. In addition, the temporal constraint makes sure the re-connection candidates appear at different times and the target doesnâ€™t appear twice at the same time. The spatial constraint checks whether two tracklet candidates meet normal movement rule. If the distance of tracklets is less than ğœƒğ‘…, the tracklets are connected. However, the matching of tracklets between different cameras might affect the labeling of ID, e.g. ğœğ‘– linked by ğœğ‘—; ğœğ‘— connected by ğœğ‘˜; ğœğ‘– also associated by ğœğ‘˜. As the result, ğœğ‘–, ğœğ‘—, ğœğ‘˜ need to be labeled the same ID number. We adopt â€œUnion-Find-Setâ€ algorithm to solve the tracklet association task, which searches the connected sub-graph as the same trajectory from global matching graph. We use the AdamOptimizer Kingma and Ba (2015) as the training optimizer, our experiment is implemented with Python 3.6 and Pytorch 0.4.1 framework and on Nvidia Tesla K40 GPUs.

Ablation Study
For Single-Camera Tracking, Table 1 describes our performance for each evaluation parameter at different steps. The first row indicates the tracklet generation step, which reaches 52.2 MOTA and 54.4 IDF1. The cleaving step aims to check the tracklet purity and split impure tracklets, and the re-connection step focuses on linking the tracklet fragments. Both Step 2 and Step 3 improve ID types of measures such as IDF1 from 54.4 to 59.1, but they donâ€™t basically affect CLEAR-MOT metrics except ID switch. The last two steps are explained in Fig. 10, the Gap Insertion method fills the disappearing bounding boxes due to occlusion according to the existing bounding boxes, this step can increase the True Positive (TP) bounding boxes, meanwhile the False Positive is (FP) also increased. On the whole, most of the performance measures are improved, especially MOTA, IDF1 and Frag. The last step, tracklet smoothing method adjusts the boxesâ€™ size to optimize the boxesâ€™ â€œwaggleâ€ between frames, which can decrease the frag and slightly reduce ID switch.

Fig. 10
figure 10
The explanation of Gap Insertion and Tracklet Smoothing

Full size image
Table 2 The comparison from different matching strategy on Duke-MTMCT training set
Full size table
For Cross-Camera Tracking, we present a Position Projection matching strategy to associate trajectories from different cameras. Table 2 shows the influence of using temporal and spatial constraints on the quality of trajectory matching and performance. The Global Retrieval indicates traversal search for each trajectory one-by-one in the history trajectory gallery. The Temporal gives the constraint for the start-frame and end-frame of trajectory. We filter out the trajectories where their start-frame is earlier than target trajectory end-frame. Temporal constraint can reduce by half the unavailable matching pairs. The strong temporal constraint additionally sets the upper limit of frame interval, which can decrease the amount of trajectory matching operation from 1577665 to 99870. Topology strategy aims to construct the connected relation between cameras on actual scenario, which only matches the trajectories from neighboring cameras, it removes half of the irrelevant trajectories. Spatial-temporal constraint is our method, which combines strong temporal and position projection, and the constraint details are described in Eq.27. The spatial-temporal can extremely reduce the number of matching-pairs, the remaining pairs meet motion rule between appearing position of candidate trajectory and predicted position of target trajectory on the real scenario. It can largely eliminate redundant amount of matching operation and decrease the probability of mis-matching.

Table 3 shows the performance of single and multiple camera tracking with different models in Duke-MTMCT validation set. â€œRâ€ includes Re-connection, Gap Insertion and Smoothing processing. From the first and second rows, using the Cleaving Network can improve 3.3% MOTA and 3.2% IDF1 in single camera, and 2.0% IDF1 in multiple-camera, respectively. The third row adopts Position Projection, thus IDF1 in multiple camera is greatly improved. From the last row of the table, the IDR is raised a lot by using Cleaving Network and Position Projection. Hence, the IDF1 is obviously higher than the others in Table 3.

Networks Analysis
Cleaving Network
To train the Cleaving Network, we construct a training dataset from Duke-MTMCT Ristani et al. (2017), PathTrack Manen et al. (2017) and MOT16 Milan et al. (2016). The datasets includes more than 2500 pedestriansâ€™ tracklet from different cameras and scenes, we divide each tracklet into several sub-tracklet of uniform-length (120 frames). Ultimately, we have more than 10K sub-tracklets (1.2M images) for training Cleaving Network.

Before training the Cleaving Network, we randomly select two tracklets ğœğ‘ and ğœğ‘. We generate a random parameter ğ‘™ğ‘ from 0 or 1. If ğ‘™ğ‘=0, we input an impure tracklet (we constructed) into Cleaving Network, so we generate another random parameter ğ‘™ğ‘  from 0 to 120, which indicates the splitting position. We combine the two tracklets as a joint tracklet, which is formulated as: ğ‘™ğ‘–ğ‘‘=[ğ‘™ğ‘–ğ‘‘1[0:ğ‘™ğ‘ ],[ğ‘™ğ‘–ğ‘‘2[ğ‘™ğ‘ :120]]. If ğ‘™ğ‘=1, we input a pure tracklet into Cleaving Network, we set ğ‘™ğ‘ =120.

Table 3 The performance of Multiple-Camera Tracking with different models in Duke-MTMCT validation set
Full size table
Fig. 11
figure 11
The purity and split results of Cleaving Network for pure & impure tracklets on training and testing dataset. a: In training set b: In testing set

Full size image
In training step, we randomly shuffle the order of the impure and the pure tracklets to feed into Cleaving Network. We define three types of losses (Feature Loss îˆ¸ğ‘,ğ‘“ğ‘¡ğ‘Ÿ, Pure Loss îˆ¸ğ‘,ğ‘ğ‘¢ğ‘Ÿ, Search Loss îˆ¸ğ‘,ğ‘ ğ‘Ÿâ„) to supervise the network. Feature Loss îˆ¸ğ‘,ğ‘“ğ‘¡ğ‘Ÿ is treated as a multiple classification task, which supervises the outputs of each step ğœ‘ğ‘˜ğ‘”,ğ‘– to generate discriminative features. Pure Loss îˆ¸ğ‘,ğ‘ğ‘¢ğ‘Ÿ is a binary classification task, which supervises ğœ‘ğ‘˜ğ‘ to judge the purity of a tracklet. Search Loss îˆ¸ğ‘,ğ‘ ğ‘Ÿâ„ is utilized for optimizing ğœ‘ğ‘˜ğ‘  to search the best splitting position of impure tracklet. We utilize two kinds of loss (cross-entropy and L1-loss) to train cleaving network, respectively.

To evaluate the performance of cleaving network in training, we define three evaluation indexes: 1.Accuracy of ID classification; 2. Accuracy of purity; 3. Average distance of best splitting position. If we define îˆ¸ğ‘,ğ‘ ğ‘Ÿğ‘ as cross-entropy, the task initially is treated as multiple classification task (121 classes), we fix the tracklet length of input (120 frames), each tracklet has 121 gaps between frames. At the 21st epoch, the accuracy of ID: 98.6%; Accuracy of purity: 95.3%; Average distance of best splitting position: 2.57 gaps. If we define îˆ¸ğ‘,ğ‘ ğ‘Ÿğ‘ as L1-loss, which is to minimize the distance between the output ğœ‘ğ‘˜ğ‘  and the ground truth ğ‘™ğ‘ . At the 13rd epoch, the Accuracy of ID: 98.6%; Accuracy of purity: 95.2%; Average distance of best splitting position: 2.43 gaps. Although the performances (ğœ‘ğ‘˜ğ‘ and ğœ‘ğ‘˜ğ‘ ) with two kinds of loss are the same, using L1-loss can make earlier convergence than using cross-entropy.

In testing step, we fix the length of Cleaving Network input (120 frames), therefore, we equably sample the over-length tracklets and padding the under-length tracklets to being of a uniform length.

For over-length tracklet ğœ1:

 ğœ1=[ğ¼1,ğ¼2,ğ¼3,...,ğ¼238,ğ¼239,ğ¼240]1âˆ—240

where ğ¼âˆ— indicates the i-th croped image of the tracklet ğœ.

We convert ğœ1 to ğœ1â€²:

 ğœ1â€²=[ğ¼1,ğ¼3,ğ¼5,...,ğ¼235,ğ¼237,ğ¼239]1âˆ—120

For under-length tracklet ğœ2:

 ğœ2=[ğ¼1,ğ¼2,ğ¼3,...,ğ¼58,ğ¼59,ğ¼60]1âˆ—60

We convert ğœ2 to ğœ2â€²:

 ğœ2â€²=[ğ¼1,ğ¼1,ğ¼2,...,ğ¼59,ğ¼60,ğ¼60]1âˆ—120

For under-length tracklet, we can easily get the best splitting position in the original tracklet:

 ğœ2â€²=[ğ¼1,...,ğ¼30,ğ¼30,|ğ¼31,ğ¼31,...,ğ¼60]1âˆ—120

The splitting position of original tracklet is:

 ğœ2=[ğ¼1,...,ğ¼29,ğ¼30,|ğ¼31,ğ¼32,...,ğ¼60]1âˆ—60

However, for over-length tracklets, we cannot directly get the best splitting positions in the original tracklet. Thus, we select Â±60 frames around the splitting position to form a tracklet to input to the network again.

For example:

 ğœ1â€²=[ğ¼1,...,ğ¼169,|ğ¼171,...,ğ¼239]1âˆ—120

Select the sub-tracklet and re-feed to Cleaving Network:

 ğœ1â€³=[ğ¼110,...,ğ¼169,ğ¼170,ğ¼171,...,ğ¼229]1âˆ—120

The splitting position of original tracklet is:

 ğœ1=[ğ¼1,...,ğ¼169,|ğ¼170,ğ¼171,...,ğ¼240]1âˆ—240

An example of searching splitting position recursively for over-length tracklet ğœ585 is illustrated in Fig. 12.

Figure 11 illustrates tracklets ğœğ‘˜âˆ— and the corresponding outputs of Cleaving Network, which includes purity results: ğœ‘ğ‘˜âˆ—ğ‘, splitting results: ğœ‘ğ‘˜âˆ—ğ‘ , and feature visualization results: ğœ‘ğ‘˜âˆ—ğ‘ . If the tracklet is pure, since the change of the appearance of the target being tracked in a short time of period is very little (because of the frame rate assumption), the change of feature of the the tracklet appearance is also slight. Thus, most of the elements of feature ğœ‘âˆ—ğ‘‘,ğ‘– tend to be zero. On the contrary, if the tracklet is impure, since the target is replaced by the other, the difference of the appearances is obvious before and after the replacement. Thus, most of the elements of feature ğœ‘âˆ—ğ‘‘,ğ‘– are not zero. The position of the maximum value of the feature (pink dotted line of each impure tracklet in the Fig. 11) indicates that the features at the two sides have the largest difference.

Fig. 12
figure 12
The illustration of searching splitting position recursively for over-length tracklet ğœ585 by Cleaving Network

Full size image
Fig. 13
figure 13
Experimental results of Re-connecction Network (a): tracklets from multiple cameras (b): Similarity matrix between tracklets

Full size image
Re-connection Network
The training dataset of Re-connection Network is composed of DukeMTMCT Ristani et al. (2017), MOT16 Milan et al. (2016) and MARS Zheng et al. (2016), which includes more than 2800 persons, and each person has several tracklets from different cameras, viewpoint, pose, illumination, etc. The Re-connection Network is finally convergent to the 90th epoch in training. The re-connection network is regarded as a general feature extractor, in post-processing step, re-connection strategy determines whether the tracklets belong to the same person based on appearance similarity between the tracklets and spatial-temporal constraint.

Fig. 13 illustrates the performance of Re-connection Network on Duke-MTMCT testing set, where Fig. 13a shows the tracklets from multiple cameras, each tracklet is marked with their tracklet ID, camera ID and timestamps at the top of the images. Fig. 13b illustrates the similarity between the tracklets. The similarity is calculated by ğœ†ğ‘ğ¹ğ‘(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡) (described in Eq.(1)). Generally ğœ†ğ‘ is set 0.5, the value of ğœ†ğ‘ğ¹ğ‘(ğ‘ğ‘–ğ‘¡,ğ‘‘ğ‘—ğ‘¡) is between [0, 1]. The more similar the features are, the smaller the value is. In the Fig. 13b, the similarity value between the tracklets of the same person is generally not more than 0.15, and most of the similarity value between the tracklets of different persons is not less than 0.3. However, for two tracklets which are of highly similar appearance and donâ€™t belong to the same person, their values are less than 0.3, such as No.329 and No.6582. Actually, at the matching step, No.329 is removed initially from the No.6582 matching candidates by the temporal-spatial constraint. Thus, matching tracklets by threshold is able to re-connect the tracklets effectively.

Table 4 Results on the MOT16 testing set
Full size table
Position Projection Network
The proposal of position projection strategy is normalizing the positions from different cameras to the same map, which is utilized for tracklet motion prediction on world map and tracklet matching between cameras. The position projection task is treated as regression task: we assume that the pedestrians move at the surface of the ground, given a camera-coordinate (ğœğ‘¥,ğœğ‘¦), the output of PPN is a unique position (ğœ”ğ‘¥,ğœ”ğ‘¦) on world-coordinate.

To improve the performance of position projection, we propose a data expansion method (described in Sect. 4.1(A)). Extending the point-pairs by generating midpoints is implemented within each plane. The function of Position Projection Network (PPN) is the same as Projective Transformations of 2D. As Hartley and Zisserman (2003) describes in Sect. 2.3, Definition 2.11: â€œA planar projective transformation is a linear transformation on homogeneous 3-vectors represented by a non-singular 3*3 matrixâ€, which is formulated as:

â›ââœâœâœğ‘¥â€²1ğ‘¥â€²2ğ‘¥â€²3ââ âŸâŸâŸ=â¡â£â¢â¢â„11â„21â„31â„12â„22â„32â„13â„23â„33â¤â¦â¥â¥â›ââœâœğ‘¥1ğ‘¥1ğ‘¥3ââ âŸâŸ
(28)
where ğ‘¥âˆ—,ğ‘¥â€²âˆ— indicate the coordinates of original and transformed plane, respectively. The position transformation between camera image and world map belongs to an affine transformation, which is a non-singular linear transformation as following:

â›ââœâœâœğœğ‘¥ğœğ‘¦1ââ âŸâŸâŸ=â¡â£â¢â¢â¢ğ‘11ğ‘210ğ‘12ğ‘220ğ‘¡ğ‘¥ğ‘¡ğ‘¦1â¤â¦â¥â¥â¥â›ââœâœâœğœ”ğ‘¥ğœ”ğ‘¦1ââ âŸâŸâŸ
(29)
Affine transformation ensures that a point (ğœğ‘¥,ğœğ‘¦) in camera image has a unique point (ğœ”ğ‘¥,ğœ”ğ‘¦) in world map. Given two points ğœš1,ğœš2 in a plane, their midpoint ğœš12 must be also in the plane. Therefore, generated midpoint between points in camera image also corresponds to a midpoint between the corresponding points in the world map.

Compared with affine transformation, PPN learns the projection relation from labeled and generated data by deep learning. The geometry-based method Jiang et al. (2018) matches the trajectory by manually creating a topology association according to the path between cameras. However, if the number of cameras are increased, the manual operation will become more complicated. Additionally, since the view of cameras are different, the velocity and direction calculated by position in camera-coordinate exist errors. On the contrary, PPN neednâ€™t previously make the geometric calibration, such as vanishing point camera calibration. The prediction position based on world map is much easier, because all of trajectories from different cameras are in the same plane, the velocity and direction cannot be affected by the view of each camera.In experimental results, the average distance error îˆ¸ğ‘ between output and labeled position is 0.0032 pixels/point for each camera.

Table 5 Results on the MOT17 testing set
Full size table
Table 6 Results on the Duke-MTMCT easy testing dataset Benchmark
Full size table
MOT Evaluation Metrics
The MOT Challenge Benchmark adopted the standard CLEAR-MOT mapping Bernardin and Stiefelhagen (2008) and ID measures Ristani et al. (2017) for evaluating MOT performance. The main metrics for MOT are MOTA and IDF1. MOTA (Multiple Object Tracking Accuracy) measures the effect of tracking for each tracklet, which depends on True Positives (TP) ,False Positives (FP), False Negatives (FN) and Id Switches (IDSw), ğ‘€ğ‘‚ğ‘‡ğ´=1âˆ’ğ¹ğ‘ƒ+ğ¹ğ‘+ğ¼ğ·ğ‘†ğ‘‡ğ‘ƒ. Total number of Fragment (Frag), Mostly tracked targets (MT), Mostly lost targets (ML) are used for evaluating tracklet integrity as the reference indexes. The IDF1 (ID F1 Score) is the ratio of correctly identified detection over the average number of true and computed detection, which depends on ID True Positives (IDTP), ID False Positives (IDFP) and ID False Negatives (IDFN), ğ¼ğ·ğ¹1=2âˆ—ğ¼ğ·ğ‘‡ğ‘ƒ2âˆ—ğ¼ğ·ğ‘‡ğ‘ƒ+ğ¼ğ·ğ¹ğ‘+ğ¼ğ·ğ¹ğ‘ƒ. IDP (ID Precision) indicates fraction of computed detections that are correctly identified, ğ¼ğ·ğ‘ƒ=ğ¼ğ·ğ‘‡ğ‘ƒğ¼ğ·ğ‘‡ğ‘ƒ+ğ¼ğ·ğ¹ğ‘ƒ. IDR (ID Recall) means fraction of ground-truth detection that are correctly identified, ğ¼ğ·ğ‘…=ğ¼ğ·ğ‘‡ğ‘ƒğ¼ğ·ğ‘‡ğ‘ƒ+ğ¼ğ·ğ¹ğ‘.

Method Comparison
We evaluate our proposal against the other traditional state-of-the-art methods on two public Tracking Benchmark datasets (MOT16, MOT17 and Duke-MTMCT). Table 4 compares the performance of our method with the existing methods on MOT16 testing set. Compared with the others methods, we achieve the higher performance of 55.0% on MOTA, 19.1% on ML and 77829 on FN. Our method outperforms most of the methods on the others metrics. Table 5 shows the performance of different state-of-the-art methods on MOT17 testing set. Trackers are divided into two types: Tracking-by-Detection (TBD) and Joint Detection and Tracking (JDT). TBD-based methods follow the traditional framework to associate bounding boxes according to the given detection results; JDT-based methods combine both of models (detection and tracking) as an end-to-end network Zhou et al. (2020) or tracking module interacts with the detection module in feature layer Peng et al. (2020). Therefore, these methods are slightly higher than TBD-based trackers. Our proposal achieves excellent performance compared to other TBD-based trackers. For multiple-camera tracking, Duke-MTMCT datasets are captured from surveillance on static camera (shown in Table 6). Our proposal effectively computes the trajectory motion rules and accurately associates tracklets from different cameras. Due to cleaving and re-connection step, for Single-Camera we reach 85.6% on MOTA and accomplish the highest scores of MT, ML, FN. For cross-camera, most of the traditional methods Maksai et al. (2017); Ristani et al. (2017); Liang and Zhou (2017); Tesfaye et al. (2017); Yoon et al. (2016); Zhang et al. (2017) neglect the temporal-spatial information, and Jiang et al. (2018) only constructs topological graph for matching trajectory. Our position projection strategy can correctly describes the real tracklet position, we can eliminate irrelevant trajectories by trajectory appearing time and predicted position, therefore we greatly enhance the ID Recall (IDR), which is up to 73.5%, meanwhile our IDF1 gets the highest score on Duke-MTMCT.

Conclusion
In this paper, for single camera tracking, we propose cleaving and re-connection networks to process the tracklets on crowd or long-term occlusion by Deep Siamese Bi-GRU. The method examines each output of bidirectional GRU to search the suitable split position and match the tracklets to reconnect the same person. For training, we extract the tracklet dataset from existing MOT datasets for training our frameworks. Our proposal has better performance for static camera such as surveillance. The algorithm achieves 55.0%, 57.1% and 85.6% in MOTA that approach to the state-of-the-art methods on MOT16, MOT17 and Duke-MTMCT benchmark dataset, respectively, where the visualization tracking results at different phase is shown in Fig. 14, and single camera tracking result is shown in Fig. 15. For multiple-camera tracking, we present a position projection strategy to convert the tracklet position from camera-coordinate to world-coordinate. We only annotate few point-pairs to train the Position Projection Network. Figure 16 illustrates the position projection and multiple-camera tracking results on Duke-MTMCT, where the left of the figure is the world map, and the right of figure shows each camera view. The same numbers between world map and camera image indicate projection areas. For trajectory matching, we predict the tracklet position in the future on the world map, and extract the tracklet general features by the re-connection network, meanwhile we add the temporal-spatial constraint to reduce the unavailable pairs. The final cross-camera tracking results are shown in Fig. 17. The crowd orderly appear on the camera No.1, No.2 and No.5, where the bottom of the images illustrates the person tracklet in different cameras.

Fig. 14
figure 14
The visualization tracking results at different phase

Full size image
Fig. 15
figure 15
Single-camera Tracking results on testing set

Full size image
Fig. 16
figure 16
Multiple-camera Tracking and Position Projection results on Duke-MTMCT. Left: world map Right: 8-cameras tracking results (In order to illustrate the tracking and position projection results more clearly, we mark the ID of corresponding camera in the world map respectively)

Full size image
Fig. 17
figure 17
Cross-camera Tracking results on Duke-MTMCT testing set. Top-right: The 243345-th frame on Camera 1; Top-middle: The 245913-rd frame on Camera 2; Top-left: The 247216-th frame on Camera 5. The bottom 3 lines: No.436, No.449 and No.485 tracklets on different cameras

Full size image
In the future, we will further research on the cleaving phase. To completely cleave the impure tracklets which contain more than two sub-tracklets, we will utilize the â€œTransformerâ€ to calculate the similarity of any two moments of impure tracking to replace the GRU. In addition, we will explore how to optimize the occlusion problem, i.e., we intend to combine the head-detector and body-detector as collaborative track of pedestrians. To describe the general features effectively, we will consult to the video-based person ReID methods such as 3D CNN model, non-local, and attention strategy. To extract the interaction feature, we will explore the movement relationships between pedestrians. We will also increase the processing efficiency. Lastly, we attempt to utilize transfer learning to improve the model robustness. For the overlapping regions cross-camera tracking task, we firstly determine the overlapping regions between the cameras. We match the tracklets according to the similarity between trackletsâ€™ appearance and calculating the distance between the trackletsâ€™ positions in the world map. And then, we set a threshold to judge whether the tracklets belong to the same person. As for the person making turn, we can properly enlarge the radius ğœ‚ğ‘ to cover this case. In addition, we need to improve the appearance model in the future to further enhance the matching accuracy.