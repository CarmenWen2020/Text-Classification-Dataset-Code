Deep convolutional neural networks (DCNNs) have dominated as the best performers on almost all computer vision tasks over the past several years. However, it remains a major challenge to deploy these powerful DCNNs in resource-limited environments, such as embedded devices and smartphones. To this end, 1-bit CNNs have emerged as a feasible solution as they are much more resource-efficient. Unfortunately, they often suffer from a significant performance drop compared to their full-precision counterparts. In this paper, we propose a novel Bayesian Optimized compact 1-bit CNNs (BONNs) model, which has the advantage of Bayesian learning, to improve the performance of 1-bit CNNs significantly. BONNs incorporate the prior distributions of full-precision kernels, features, and filters into a Bayesian framework to construct 1-bit CNNs in a comprehensive end-to-end manner. The proposed Bayesian learning algorithms are well-founded and used to optimize the network simultaneously in different kernels, features, and filters, which largely improves the compactness and capacity of 1-bit CNNs. We further introduce a new Bayesian learning-based pruning method for 1-bit CNNs, which significantly increases the model efficiency with very competitive performance. This enables our method to be used in a variety of practical scenarios. Extensive experiments on the ImageNet, CIFAR, and LFW datasets show that BONNs achieve the best in classification performance compared to a variety of state-of-the-art 1-bit CNN models. In particular, BONN achieves a strong generalization performance on the object detection task.

Access provided by University of Auckland Library

Introduction
Deep convolutional neural networks (DCNNs) have exhibited superior performance in low-level (Dong et al. 2014; Liu et al. 2018c) and high-level (He et al. 2016; Liu et al. 2016; Wan et al. 2019) computer vision tasks. However, such superiority is accompanied by significant computation and storage requirements. Most existing powerful DCNNs are composed of extensive amounts of floating-point parameters stored in the form of 32 bits. The convolution operation is implemented as a matrix multiplication between floating-point operands. Such computation is time- and storage-consuming, which prohibits DCNNs from being deployed on resource-limited devices such as smartphones and drones. To tackle this problem, various approaches have been explored to compress DCNNs, which can be coarsely categorized into quantization-based Courbariaux et al. (2015) and pruning-based (Li et al. 2017; Han et al. 2015) methods. Quantization methods approximate full-precision parameters with lower-precision ones, simultaneously accelerating the convolution operation and saving storage. Among them, 1-bit convolutional neural networks (1-bit CNNs) can be regarded as an extreme case of quantization, which binarize convolution kernels and activations as ±1 in Courbariaux et al. (2016) or ±𝛼𝑙 in Rastegari et al. (2016). Recently, DoReFa-Net Zhou et al. (2016) exploits 1-bit convolution kernels with low bit-width parameters and gradients to accelerate both training and inference. ABC-Net Lin et al. (2017a) adopts multiple binary weights and activations to approximate full-precision weights, such that the degradation of prediction accuracy is reduced. Modulated convolutional networks are presented in Wang et al. (2018) to binarize only the kernels. Leng et al. (2018) borrowed the idea from ADMM to compress deep models with network weights represented by only a small number of bits. Bi-Real Net Liu et al. (2020) explores a new variant of residual structure to preserve the real activations before the sign function, with a tight approximation to the derivative of the non-differentiable sign function. Zhuang et al. (2018) presented a 2∼4-bit quantization scheme using a two-stage approach to alternately quantize the weights and activations, which provides an optimal trade-off between memory, efficiency, and performance. Furthermore, WAGE Wu et al. (2018) is proposed to discretize both the training and inference processes, which quantizes weights and activations and gradients and errors. In Gu et al. (2019), a quantization method is introduced based on a discrete backpropagation algorithm to learn better 1-bit CNNs.

Fig. 1
figure 1
The evolution of the prior 𝑝(𝑥𝑥), the distribution of the observation 𝑦𝑦, and the posterior 𝑝(𝑥𝑥|𝑦𝑦) during learning, where 𝑥𝑥 is the latent variable representing the full-precision parameters and 𝑦𝑦 is the quantization error. At the beginning, the parameters 𝑥𝑥 are initialized according to a single-mode Gaussian distribution. When our learning algorithm converges, the ideal case is that (i) 𝑝(𝑦𝑦) becomes a Gaussian distribution (0,𝜈), which corresponds to the minimum reconstruction error, and (ii) 𝑝(𝑥𝑥|𝑦𝑦)=𝑝(𝑥𝑥) is a Gaussian mixture distribution with two modes where the binarized values 𝑥𝑥^ and −𝑥𝑥^ are located

Full size image
Neural network pruning focuses on removing network connections in a non-structured or structured manner. Early works in non-structured pruning LeCun et al. (1990), Hassibi and Stork (1993) propose a saliency-like measurement to remove redundant weights determined by the second-order derivative matrix of the loss function w.r.t. the weights. Han et al. (2015) proposed an iterative thresholding method to remove unimportant weights with small absolute values. Guo et al. (2016) presented a connection splicing to avoid incorrect pruning of weights, which can reduce the accuracy loss of the pruned network. Note that existing non-structured pruning methods typically rely on using specific hardware to accelerate online inference. In contrast, structured pruning can achieve rapid inference without using specialized packages. Li et al. (2017) proposed a magnitude-based pruning to remove filters and their corresponding feature maps by calculating the ℓ1-norm of filters in a layer-wise manner. A Taylor expansion-based criterion is proposed in Molchanov et al. (2017) to prune one filter and then fine-tune the pruned network iteratively. Unlike these multistage and layer-wise pruning methods, Lin et al. (2019a) and Huang and Wang (2018) use an uncontrolled sparse soft mask to prune the network in an end-to-end manner. Recently, kernel sparsity and entropy have been introduced as indicators to prune networks efficiently Li et al. (2019). However, this method is hardware-dependent and therefore restricted for general applications.

Despite the existing progress made in 1-bit quantization or network pruning, little work has combined quantization and pruning in a unified framework to reinforce each other. However, it is clearly necessary to introduce pruning techniques into 1-bit CNNs. Not all filters and kernels are equally important and worth quantizing in the same way, as validated subsequently in our experiments. One potential solution is to prune the network first and then conduct 1-bit quantization over the remaining network to have a more compressed network. However, such a solution fails to consider the difference between the binarized and full-precision parameters during pruning. Instinctively, 1-bit CNNs tend to be easily pruned since CNNs are more redundant before and after binarization Liu et al. (2019b). Thus, one promising alternative is to conduct pruning over BNNs. However, it remains an open problem to design a unified framework to calculate a 1-bit network first and then prune it. In particular, due to the deteriorated representation capability in 1-bit networks, the backpropagation process can be very sensitive to parameter updates, making the existing optimization schemes Gu et al. (2019) fail.

To tackle this problem, in this paper, we investigate the possibility of using Bayesian learning, a well-established global optimization scheme Mockus et al. (1978),Blundell et al. (2015), into the pruning of 1-bit CNNs. First, the Bayesian learning binarizes the full-precision kernels to two quantization values (centers) to obtain 1-bit CNNs. The quantization error is minimized when the full-precision kernels follow a Gaussian mixture model, with each Gaussian centered at its corresponding quantization value. Given two centers for 1-bit CNNs, two Gaussians forming the mixture model are employed to model the full-precision kernels. Subsequently, the Bayesian learning framework establishes a new pruning operation to prune the 1-bit CNNs. In particular, we divide the filters into two groups assuming that those in one group follow the same Gaussian distribution. Their average is then used to replace the weights of the filters in this group. Figure 1 illustrates the overall framework where three innovative elements are introduced to the learning procedure of 1-bit CNNs with compression: (1) minimizing the reconstruction error of parameters before and after quantization,(2) modeling the parameter distribution as a Gaussian mixture with two modes centered at the binarized values, and (3) pruning the quantized network by maximizing a posterior probability. Further analysis led to our three new losses and corresponding learning algorithms, referred to as the Bayesian kernel loss, Bayesian feature loss, and Bayesian pruning loss. These three losses can be jointly applied with the conventional cross-entropy loss within the same back-propagation pipeline. The advantages of Bayesian learning are intrinsically inherited during the model quantization and pruning. The proposed losses can further comprehensively supervise the training process of 1-bit CNNs concerning both the kernel distribution and feature distribution. Finally, a new direction on 1-bit CNNs pruning is explored further to improve the compressed model’s applicability in practical applications.

In summary, the contributions of this paper are threefold:

1)
We present a novel Bayesian learning framework for calculating extremely compact 1-bit CNNs, which integrates both 1-bit quantization and pruning for the first time in a joint Bayesian learning setting. This framework fully investigates the intrinsic relationship between full-precision and 1-bit CNNs in terms of kernels and features and among filters.

2)
CNNs are binarized and pruned end-to-end, which is the first of this kind. Three innovative loss terms are proposed to simultaneously consider the kernel distribution, feature distribution, and filter distribution to supervise the training process, which is more comprehensive and efficient.

3)
Our models achieve the best classification performance compared to other state-of-the-art 1-bit CNNs on the ImageNet Krizhevsky et al. (2012), CIFAR Krizhevsky et al. (2014), and LFW Huang et al. (2008) datasets. In particular, our BONNs consistently achieve higher performance than prior arts on the object detection task on PASCAL VOC Everingham et al. (2010) and COCO Lin et al. (2014).

The work is an extension of our conference paper Gu et al. (2019) with four essential innovations: 1) We improve our method by extending Bayesian learning to the task of pruning 1-bit CNNs, which further validate the effectiveness of our proposed method. 2) We elaborate on the details of asynchronous backpropagation for the optimization. 3) We conduct extensive experiments applying our model on the tasks of classification, face recognition, and pruning. 4) We validate our BONNs on the object detection task and achieve a much better performance than state-of-the-art BNNs on PASCAL VOC and COCO.

The performance of our 1-bit CNNs on large-scale datasets like LFW, CFP-FP, AgeDB-30, and COCO are much better than others, which strongly support the effectiveness of our method.

Related Work
Extensive work has been reported on compressing and accelerating DCNNs through quantization, Rastegari et al. (2016); Courbariaux et al. (2016); Liu et al. (2020), low-rank approximation Zhang et al. (2016); Novikov et al. (2015), network pruning Li et al. (2017); Guo et al. (2016); Han et al. (2015), and more recently, architecture search Liu et al. (2018b); Zoph et al. (2018). Amongst them, network pruning and quantization are most relevant to our work and are reviewed in this section.

Neural Network Quantization
To the best of our knowledge, BNN Courbariaux et al. (2016) is the first attempt to binarize both the weights and activations of convolution layers in CNNs. BNN works well in maintaining the classification accuracy on small datasets like CIFAR-10 and CIFAR-100, Krizhevsky et al. (2014), which is less effective when applied on large datasets like ImageNet Rastegari et al. (2016); Deng et al. (2009). Instead of binarizing the kernel weights into ±1, the work in Rastegari et al. (2016) adds a layer-wise scalar 𝛼𝑙 to reconstruct the binarized kernels and proves that the mean absolute value (MAV) of each layer is the optimal value for 𝛼𝑙. Inspired by using a scalar to reconstruct binarized kernels, HQRQ Li et al. (2017) adopts a high-order binarization scheme to achieve a more accurate approximation while preserving the advantage of the binary operation. ABC-Net Lin et al. (2017a) adopts multiple binary weights and activations to approximate full-precision weights to alleviate the degradation in prediction accuracy. Leng et al. (2018) decoupled the continuous parameters from the discrete constraints of the network using ADMM, which therefore achieves extremely low bit rates. Recently, Bi-Real Net Liu et al. (2020) explores a new variant of residual structure to preserve the real activations before the sign function, with a tight approximation to the derivative of the non-differentiable sign function. SLB Yang et al. (2020) introduces neural architecture search (NAS) Liu et al. (2019c) into the binarization of weights. LWS-Det Xu et al. (2021) fully unitize the angular and amplitude minimization technology to narrow the gap between real-valued convolution and 1-bit convolution. Xu et al. (2021) explores the influence of dead weights, which refer to a group of weights, and introduces the rectified clamp unit (ReCU) to revive the dead weights for updating. McDonnell McDonnell (2018) applied a warm-restart learning-rate schedule to quantize network weights into 1-bit, which achieves about 98%∼99% of peak performance on CIFAR.

Quantizing kernel weights and activations to binary values is an extreme case of neural network quantization, prone to unacceptable accuracy degradation. Accordingly, sufficient attention has been paid to quantize DCNNs with more than 1 bit. Specifically, ternary weights are introduced to reduce the quantization error in TWN Li et al. (2016).

DoReFa-Net Zhou et al. (2016) exploits convolution kernels with low bit-width parameters and gradients to accelerate both the training and inference.

TTQ Zhu et al. (2017) uses two full-precision scaling coefficients to quantize the weights to ternary values.

Zhuang et al. (2018) presented a 2∼4-bit quantization scheme using a two-stage approach to alternately quantize the weights and activations, which provides an optimal tradeoff among memory, efficiency, and performance.

Jung et al. (2019) parameterizes the quantization intervals and obtain their optimal values by directly minimizing the task loss of the network and also the accuracy degeneration with further bit-width reduction. Cai et al. (2020) supports both uniform and mixed-precision quantization by optimizing for a distilled dataset, which is engineered to match the statistics of batch normalization across different layers of the network. Xie et al. (2020) introduce transfer learning into network quantization to obtain an accurate low-precision model by utilizing the Kullback-Leibler (KL) divergence. Fang et al. (2020) enables accurate approximation for tensor values that have bell-shaped distributions with long tails and finds the entire range by minimizing the quantization error. Xu et al. (2020) investigate a data-free quantization method to remove the data dependence burden. Furthermore, WAGE Wu et al. (2018) is proposed to discretize both the training and inference processes, where not only weights/activations but also gradients/errors are quantized.

Despite the excellent efficiency, 1-bit CNNs seriously limit the network representation capability, leading to an inevitable loss of accuracy.

To handle this problem, PCNN Gu et al. (2019) utilizes a projection algorithm to enhance backpropagation, with a corresponding projection loss to improve the accuracy of 1-bit CNNs significantly.

Neural Network Pruning
Methods in network pruning can be further divided into both unstructured pruning and structured pruning. Unstructured pruning is to remove unimportant weights independently. Work in Han et al. (2015) and Han et al. (2015) proposes to prune the weights with small absolute values, where the sparse structure is stored in a compressed sparse row or column format. In Yang et al. (2017), an energy-aware pruning approach is introduced to prune unimportant weights layer-by-layer by minimizing the error reconstruction. These methods need to store the network, and the speedup is achieved by using specific sparse matrix multiplication with special hardware designs.

In contrast, structured pruning directly removes structured kernels, filters, and/or layers to compress simultaneously and speed up CNNs, which various off-the-shelf deep learning libraries well support. A group sparsity regularization is introduced in Yoon and Hwang (2017) to exploit correlations among features in the network. The average percentage of zeros (APoZ) of each filter is exploited as a pruning measure in Hu et al. (2016)

This is equal to the percentage of zero values in the output feature map corresponding to the filter. Recently, Luo et al. (2017) pruned the filters that have a minimal impact on the next convolutional layer. He et al. (2017) proposed a LASSO regression-based channel selection scheme, which uses a least square reconstruction to prune filters. More recently, Lin et al. (2018a) proposed a global and dynamic training algorithm to prune unsalient filters. Ding et al. (2019) proposed a novel optimization method called C-SGD, which trains the filters to collapse into serval single points in the parameter hyperspace, and then prunes the identical filters without performance loss. He et al. (2019) proposed a filter pruning method via geometric median instead of the traditional norm-based criterion. Lemaire et al. (2019) introduced a budget-aware regularization and pruned the network via a learnable mask layer with knowledge distillation. Although filter pruning approaches can reduce the memory footprint, a dimensional mismatch problem is encountered for the popular multi-branch networks, e.g., ResNets He et al. (2016). Our method differs from all the above approaches where we reduce the redundancy of 3D filters and do not modify the output of a convolutional layer to avoid the dimensional mismatch.

Several channel pruning methods are suited for the widely-used ResNet He et al. (2016) and DenseNet Huang et al. (2017). These methods remove unimportant input feature maps of convolutional layers, which can avoid dimensional mismatch. For example, Liu et al. (2017b) imposed an ℓ1-regularization on the scaling factors on the batch normalization to select unimportant feature maps. Weight pruning and group convolution are used to sparsify the network in Huang et al. (2018).

However, these channel pruning methods obtain a sparse network based on a complex training procedure, requiring high offline costs.

Our work is similar to the binarization methods that compress and accelerate DCNNs with pruning and binary operations. However, unlike the previous works, e.g., ReActNet Liu et al. (2020) and IR-Net Qin et al. (2020), we take the latent distributions of kernel weights, features, and filters into consideration and propose two Bayesian losses to improve 1-bit CNNs’ capacity significantly. Furthermore, we design a new Bayesian pruning method with a Bayesian pruning loss for further reducing the storage and computation of 1-bit CNNs.

Bayesian Learning
Bayesian learning is a paradigm for constructing statistical models based on Bayes’ Theorem, providing useful learning algorithms and help us understand other learning algorithms. Bayesian learning shows its major advantages to solve probabilistic graphical models, which can help achieve the information exchange between the perception task and the inference task, conditional dependencies on high-dimensional data, and effective modeling of uncertainty. (Bishop 1997; Lampinen and Vehtari 2001) have been comprehensively studied about Bayesian Neural Networks (BayesNNs). More recent developments which establish the efficacy of BayesNNs can be found in Sun et al. (2017), Liang et al. (2018) and the references therein. The estimation of the posterior distribution is a key part of Bayesian inference and represents the information about the uncertainties for both data and parameters. However, an exact analytical solution for the posterior distribution is intractable as the number of parameters is huge, and the functional form of a neural network does not lend itself to exact integration Blundell et al. (2015). Several approaches have been proposed for solving posterior distribution of weights of BayesNNs, based on both optimization-based techniques such as variational inference (VI) and sampling-based approach, such as Markov Chain Monte Carlo (MCMC). MCMC techniques are typically used to obtain sampling-based estimates of the posterior distribution. Indeed, BayesNNs with MCMC have not seen widespread adoption due to computational cost in terms of both time and storage on a large dataset Kingma et al. (2015). In contrast to MCMC, VI tends to converge faster, and it has been applied to many popular Bayesian models, such as factorial models and topic models Blei and Lafferty (2007). The basic idea of VI is that it first defines a family of variational distributions and then minimizes the Kullback-Leibler (KL) divergence concerning the variational family. Many recent works have discussed the application of variational inference to BayesNNs, , e.g., Blundell et al. (2015); Sun et al. (2019).

Besides, Bayesian learning is widely used for other subareas of deep learning. In principle, the Bayesian approach to learn neural networks does not have problems of tuning a large number of hyper-parameters or over-fitting the training data (MacKay 1992a; Hernández-Lobato and Adams 2015). Recently, Bayesian learning is applied to estimate layer size and network depth in the neural architecture search problem (Dikov and Bayer 2019; Zhou et al. 2019). By employing sparsity-inducing priors, the obtained model depends only on a subset of kernel functions for linear models Tipping (2001) and deep neural networks where the neurons can be pruned as well as all their ingoing and outgoing weights Louizos et al. (2017). Bayesian methods have also been applied to obtain a sparse and compressed network and the latter uses variational inference to learn the dropout rate (Ullrich et al. 2017; Molchanov et al. 2017). Different from existing works, our paper is the first work to employ Bayesian learning on structured pruning (filter pruning). Moreover, we also provide the first attempt to binarize and prune 1-bit CNNs in the same framework, which is an important topic in the field of deep learning.

Fig. 2
figure 2
By considering the prior distributions of the kernels and features in the Bayesian framework, we achieve three new Bayesian losses to optimize the 1-bit CNNs. The Bayesian kernel loss improves the layer-wise kernel distribution of each convolution layer, the Bayesian feature loss introduces the intra-class compactness to alleviate the disturbance induced by the quantization process, and Bayesian pruning loss centralizes channels following the same Gaussian distribution for pruning. Note that the Bayesian feature loss is only applied to the fully-connected layer

Full size image
Table 1 A brief description of the main notation used in the paper
Full size table
The Proposed Method
Bayesian learning has been applied to building and analyzing standard neural networks for computer vision tasks in Blundell et al. (2015), MacKay (1992b). In this paper, we leverage the efficacy of Bayesian learning to construct 1-bit CNNs in an end-to-end manner. In particular, the proposed Bayesian learning algorithms optimize 1-bit CNNs with improved efficiency and stability in a unified framework, which not only considers the specific kernel weight distribution in 1-bit CNNs, but also supervises the feature distribution by three new loss terms, as detailed in Fig. 2. Our Bayesian learning framework further enables the pruning of 1-bit models. For clarity, Table 1 describes the main notations used in the following sections.

Bayesian Formulation for Compact 1-bit CNNs
The state-of-the-art methods (Leng et al. 2018; Rastegari et al. 2016; Gu et al. 2019) learn 1-bit CNNs by involving the optimization in both continuous and discrete spaces. In particular, training a 1-bit CNN involves three steps: a forward pass, a backward pass, and a parameter updating through gradient calculation. The binarized weights (𝑥𝑥^) are only considered during the forward pass (inference) and gradient calculation. After updating the parameters, we have the full-precision weights (𝑥𝑥). As revealed in Leng et al. (2018), Rastegari et al. (2016), Gu et al. (2019), how to connect 𝑥𝑥^ with 𝑥𝑥 is the key to determine the performance of a quantized network. In this paper, we propose to solve it in a probabilistic framework to learn optimal 1-bit CNNs.

Bayesian Learning losses
Bayesian kernel loss Given a network weight parameter 𝑥𝑥, its quantized code should be as close to its original (full-precision) code as possible, such that the quantization error is minimized. We then define:

𝑦𝑦=𝑤−1𝑤−1∘𝑥𝑥^−𝑥𝑥,
(1)
where 𝑥𝑥,𝑥𝑥^∈𝐑𝑛 are the full-precision and quantized vectors respectively, 𝑤𝑤∈𝐑𝑛 denotes the learned vector to reconstruct 𝑥𝑥, ∘ represents the Hadamard product, and 𝑦𝑦∼𝐺(0,𝜈) is the reconstruction error that is assumed to obey a Gaussian prior with zero mean and variance 𝜈. Under the most probable 𝑦𝑦 (corresponding to 𝑦𝑦=00 and 𝑥𝑥=𝑤𝑤−1∘𝑥𝑥^, i.e., the minimum reconstruction error), we maximize 𝑝(𝑥𝑥|𝑦𝑦) to optimize 𝑥𝑥 for quantization (e.g., 1-bit CNNs) as:

max𝑝(𝑥𝑥|𝑦𝑦),
(2)
which can be solved based on Bayesian learning that uses Bayes’ theorem to determine the conditional probability of a hypotheses given limited observations. We note that the calculation of BNNs is still based on optimizing 𝑥𝑥 as shown in Fig. 1, where the binarization is done based on the sign function. Eq. (2) is complicated and difficult to solve due to the unknown 𝑤−1 as shown in Eq. (1). From a Bayesian learning perspective, we resolve this problem via maximum a posterior (MAP):

max𝑝(𝑥𝑥|𝑦𝑦)=max𝑝(𝑦𝑦|𝑥𝑥)𝑝(𝑥𝑥)=min||𝑥𝑥^− 𝑤𝑤∘𝑥𝑥||22−2𝜈log(𝑝(𝑥𝑥)),
(3)
where

𝑝(𝑦𝑦|𝑥𝑥)∝exp(−12𝜈||𝑦𝑦||22)∝exp(−12𝜈||𝑥𝑥^−𝑤𝑤∘𝑥𝑥||22).
(4)
In Eq. (4), we assume that all the components of the quantization error 𝑦𝑦 are i.i.d, thus resulting in a simplified form. As shown in Fig. 1, for 1-bit CNNs, 𝑥𝑥 is usually quantized to two numbers with the same absolute value. We neglect the overlap between the two numbers, and thus, 𝑝(𝑥𝑥) is modeled as a Gaussian mixture with two mode:

𝑝(𝑥𝑥)=≈12(2𝜋)−𝑁2det(ΨΨ)−12{exp(−(𝑥𝑥−𝜇𝜇)𝑇ΨΨ−1(𝑥𝑥−𝜇𝜇)2)+exp(−(𝑥𝑥+𝜇𝜇)𝑇ΨΨ−1(𝐱+𝜇𝜇)2)}12(2𝜋)−𝑁2det(ΨΨ)−12{exp(−(𝑥𝑥+−𝜇𝜇+)𝑇Ψ+Ψ+−1(𝑥𝑥+−𝜇𝜇+)2)+exp(−(𝑥𝑥−+𝜇𝜇−)𝑇Ψ−Ψ−−1(𝑥𝑥−+𝜇𝜇−)2)},
(5)
where 𝑥𝑥 is divided into 𝑥𝑥+ and 𝑥𝑥− according to the signs of the elements in 𝑥𝑥, and N is the dimension of 𝑥𝑥. Accordingly, Eq. (3) can be rewritten as:

min||𝑥𝑥^−𝑤𝑤∘𝑥𝑥||22+𝜈(𝑥𝑥+−𝜇𝜇+)𝑇ΨΨ−1+(𝑥𝑥+−𝜇𝜇+)+𝜈(𝑥𝑥−+𝜇𝜇−)𝑇ΨΨ−1−(𝑥𝑥−+𝜇𝜇−)+𝜈log(det(ΨΨ)),
(6)
where 𝜇𝜇− and 𝜇𝜇+ are solved independently. det(ΨΨ) is accordingly set to be the determinant of the matrix ΨΨ− or ΨΨ+. We call Eq. (6) the Bayesian kernel loss.

Bayesian Feature Loss We further design a Bayesian feature loss to alleviate the disturbance caused by the extreme quantization process in 1-bit CNNs. Considering the intra-class compactness, the features 𝑓𝑓𝑚 of the m-th class supposedly follow a Gaussian distribution with the mean 𝑐𝑐𝑚 as revealed in the center loss Wen et al. (2016). Similar to the Bayesian kernel loss, we define 𝑦𝑦𝑚𝑓=𝑓𝑓𝑚−𝑐𝑐𝑚 and 𝑦𝑦𝑚𝑓∼(00,𝜎𝜎𝑚), and have:

min||𝑓𝑓𝑚−𝑐𝑐𝑚||22+∑𝑛=1𝑁𝑓[𝜎−2𝑚,𝑛(𝑓𝑚,𝑛−𝑐𝑚,𝑛)2+log(𝜎2𝑚,𝑛)],
(7)
which is called the Bayesian feature loss. In Eq. (7), 𝜎𝑚,𝑛, 𝑓𝑚,𝑛 and 𝑐𝑚,𝑛 are the n-th elements of 𝜎𝜎𝑚, 𝑓𝑓𝑚 and 𝑐𝑐𝑚, respectively. We take the latent distributions of kernel weights and features into consideration in the same framework, and introduce Bayesian losses to improve the capacity of 1-bit CNNs.

Bayesian Pruning
After binarizing CNNs, we further prune 1-bit CNNs under the same Bayesian learning framework. We consider that different channels might follow a similar distribution, based on which similar channels are combined for pruning. From the mathematical aspect, we achieve a Bayesian formulation about the BNN pruning by directly extending our basic idea in Gu et al. (2019), which actually provides a systematic way to calculate compact 1-bit CNNs. We represent the kernel weights of the l-th layer 𝐾𝐾𝑙 as a tensor ∈𝐑𝐶𝑙𝑜×𝐶𝑙𝑖×𝐻𝑙×𝑊𝑙, where 𝐶𝑙𝑜 and 𝐶𝑙𝑖 denote the numbers of output and input channels, respectively, and 𝐻𝑙 and 𝑊𝑙 are the height and width of the kernels, respectively. For clarity, we define

𝐾𝐾𝑙=[𝐾𝐾𝑙1,𝐾𝐾𝑙2,...,𝐾𝐾𝑙𝐶𝑙𝑜],
(8)
where 𝐾𝐾𝑙𝑖,𝑖=1,2,...,𝐶𝑙𝑜, is a 3-dimensional filter ∈𝐑𝐶𝑙𝑖×𝐻𝑙×𝑊𝑙. For simplicity, l is omitted in the rest of this section. To prune 1-bit CNNs, we assimilate similar filters into the same one based on a controlling learning process. To this end, we first divide 𝐾𝐾 into different groups using the K-means algorithm and then replace the filters of each group by their average during optimization. Such a process is based on the assumption that 𝐾𝐾𝑖 in the same group follows the same Gaussian distribution during training. Then the pruning problem becomes how to find the average 𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯ to replace all 𝐾𝐾𝑖’s, which follows the same distribution. It actually leads to a similar problem as in Eq. (3). It is worth noting that the learning process with a Gaussian distribution constraint is widely considered in Hastie et al. (2005).

Accordingly, Bayesian learning is used to prune 1-bit CNNs. We denote 𝜖𝜖 as the difference between a filter and its mean, i.e., 𝜖𝜖=𝐾𝐾−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯, following a Gaussian distribution for simplicity. To calculate 𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯, we minimize 𝜖𝜖 based on MAP in our Bayesian framework, and have

𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯=argmax𝐾𝐾𝑝(𝐾𝐾|𝜖𝜖)=argmax𝐾𝐾𝑝(𝜖𝜖|𝐾𝐾)𝑝(𝐾𝐾),
(9)
𝑝(𝜖𝜖|𝐾𝐾)∝exp(−12𝜈||𝜖𝜖||22)∝exp(−12𝜈||𝐾𝐾−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯||22),
(10)
and 𝑝(𝐾𝐾) is similar to Eq. 5 but with one mode. Thus, we have

min||𝐾𝐾−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯||22+𝜈(𝐾𝐾−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯)𝑇ΨΨ−1(𝐾𝐾−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯)+𝜈log(det(ΨΨ)),
(11)
which is called the Bayesian pruning loss. In a brief summary, our Bayesian pruning solves the pruning problem in a more general way, assuming that similar kernels follow a Gaussian distribution and will be finally represented by their centers for pruning. From this viewpoint, we can obtain a more general pruning method, and it is more suitable for binary neural networks than the existing pruning methods. Moreover, we take the latent distributions of kernel weights, features, and filters into consideration in the same framework and introduce Bayesian losses and Bayesian pruning to improve the capacity of 1-bit CNNs. Comparative experimental results on model pruning also demonstrate the superiority of our BONNs over existing pruning methods.

BONNs
We employ the three Bayesian losses to optimize 1-bit CNNs, which form our Bayesian Optimized 1-bit CNNs (BONNs). To do this, we reformulate the first two Bayesian losses for 1-bit CNNs as

𝐿𝐵=𝜆2∑𝑙=1𝐿∑𝑖=1𝐶𝑙𝑜∑𝑛=1𝐶𝑙𝑖{||𝑘𝑘^𝑙,𝑖𝑛−𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛||22+𝜈(𝑘𝑘𝑙,𝑖𝑛+−𝜇𝜇𝑙𝑖+)𝑇(ΨΨ𝑙𝑖+)−1(𝑘𝑘𝑙,𝑖𝑛+−𝜇𝜇𝑙𝑖+)+𝜈(𝑘𝑘𝑙,𝑖𝑛−+𝜇𝜇𝑙𝑖−)𝑇(ΨΨ𝑙𝑖−)−1(𝑘𝑘𝑙,𝑖𝑛−+𝜇𝜇𝑙𝑖−)+𝜈log(det(ΨΨ𝑙))}+𝜃2∑𝑚=1𝑀{||𝑓𝑓𝑚−𝑐𝑐𝑚||22+∑𝑛=1𝑁𝑓[𝜎−2𝑚,𝑛(𝑓𝑚,𝑛−𝑐𝑚,𝑛)2+log(𝜎2𝑚,𝑛)]},
(12)
where 𝑘𝑘𝑙,𝑖𝑛,𝑙∈{1,...,𝐿},𝑖∈{1,...,𝐶𝑙𝑜},𝑛∈{1,...,𝐶𝑙𝑖}, is the vectorization of the i-th kernel matrix at the l-th convolutional layer, 𝑤𝑤𝑙 is a vector used to modulate 𝑘𝑘𝑙,𝑖𝑛, and 𝜇𝜇𝑙𝑖 and ΨΨ𝑙𝑖 are the mean and covariance of the i-th kernel vector at the l-th layer, respectively. And we term 𝐿𝐵 the Bayesian optimization loss. Furthermore, we assume the parameters in the same kernel are independent, and thus ΨΨ𝑙𝑖 becomes a diagonal matrix with the identical value (𝜎𝑙𝑖)2, where (𝜎𝑙𝑖)2 is the variance of the i-th kernel of the l-th layer. In this case, the calculation of the inverse of ΨΨ𝑙𝑖 is sped up, and all the elements of 𝜇𝜇𝑙𝑖 are identical and equal to 𝜇𝑙𝑖. Note that in our implementation, all elements of 𝑤𝑤𝑙 are replaced by their average during the forward process. Accordingly, only a scalar instead of a matrix is involved in the inference, and thus the computation is significantly accelerated.

After training 1-bit CNNs, the Bayesian pruning loss 𝐿𝑃 is then used for the optimization of feature channels, which can be written as:

𝐿𝑃=∑𝑙=1𝐿∑𝑗=1𝐽𝑙∑𝑖=1𝐼𝑗{||𝐾𝐾𝑙𝑖,𝑗−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗||22+𝜈(𝐾𝐾𝑙𝑖,𝑗−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗)𝑇(ΨΨ𝑙𝑗)−1(𝐾𝐾𝑙𝑖,𝑗−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗)+𝜈log(det(ΨΨ𝑙𝑗))},
(13)
where 𝐽𝑙 is the number of Gaussian clusters (groups) of the l-th layer, and 𝐾𝐾𝑙𝑖,𝑗, 𝑖=1,2,...,𝐼𝑗, are those 𝐾𝐾𝑙𝑖’s that belong to the j-th group. In our implementation, we define 𝐽𝑙=int(𝐶𝑙𝑜×𝜖), where 𝜖 is a pre-defined pruning rate. In this paper, we use one 𝜖 for all layers. Note that when the j-th Gaussian just has one sample 𝐾𝐾𝑙𝑖,𝑗,𝐾𝐾⎯⎯⎯⎯⎯𝑙𝑗=𝐾𝐾𝑙𝑖,𝑗 and ΨΨ𝑗 is a unit matrix.

In BONNs, the cross-entropy loss 𝐿𝑆, the Bayesian optimization loss 𝐿𝐵 and the Bayesian pruning loss 𝐿𝑃 are aggregated together to build the total loss as:

𝐿=𝐿𝑆+𝐿𝐵+𝜁𝐿𝑃,
(14)
where 𝜁 is 0 at binarization training and becomes 1 at pruning. The Bayesian kernel loss constrains the distribution of the convolution kernels to a symmetric Gaussian mixture with two modes, and simultaneously, minimizes the quantization error through the ||𝑘𝑘^𝑙,𝑖𝑛−𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛||22 term. Meanwhile, the Bayesian feature loss modifies the distribution of the features to reduce the intra-class variation for better classification. The Bayesian pruning loss converges similar kernels to their means and thus compresses the 1-bit CNNs further.

Bayesian Learning for BONNs
Given the proposed 1-bit CNN, we elaborate the learning procedure of the parameters, including forward propagation and backpropagation, in this section.

Forward Propagation
In the forward propagation, the binarized kernels and activations are used to accelerate the computation of convolution. The reconstruction vector is essential for 1-bit CNNs as described in Eq. (1), where 𝑤𝑤 denotes a learned vector to reconstruct the full precision vector and is shared in a layer. As mentioned in Sec. 3.2, during forward propagation, 𝑤𝑤𝑙 becomes a scalar 𝑤⎯⎯⎯⎯⎯𝑙 in each layer, where 𝑤⎯⎯⎯⎯⎯𝑙 is the mean of 𝑤𝑤𝑙 and is calculated online. The convolution process is represented as:

𝑂𝑂𝑙+1=((𝑤⎯⎯⎯⎯⎯𝑙)−1𝐾̂ 𝐾̂ 𝑙)∗𝑂̂ 𝑂̂ 𝑙=(𝑤⎯⎯⎯⎯⎯𝑙)−1(𝐾̂ 𝐾̂ 𝑙∗𝑂̂ 𝑂̂ 𝑙),
(15)
where 𝑂̂ 𝑂̂ 𝑙 denotes the binarized feature map of the l-th layer, and 𝑂𝑙+1 is the feature map of the (𝑙+1)-th layer. As Eq. (15) depicts, the actual convolution is still binary, and 𝑂𝑙+1 is obtained by just multiplying (𝑤⎯⎯⎯⎯⎯𝑙)−1 and the binarization convolution. For each layer, only one floating-point multiplication is added, which is negligible for BONNs.

In addition, we consider the Gaussian distribution in the forward process of Bayesian pruning, which updates every filter in one group based on its mean. Specifically, we replace each filter 𝐾𝐾𝑙𝑖,𝑗=(1−𝛾)𝐾𝐾𝑙𝑖,𝑗+𝛾𝐾𝐾⎯⎯⎯⎯⎯𝑙𝑗 during pruning.

Asynchronous Backward Propagation
To minimize Eq. (12), we update 𝑘𝑘𝑙,𝑖𝑛, 𝑤𝑤𝑙, 𝜇𝑙𝑖, 𝜎𝑙𝑖, 𝑐𝑐𝑚 and 𝜎𝜎𝑚 using the stochastic gradient descent (SGD) in an asynchronous manner, which updates 𝑤𝑤 instead of 𝑤⎯⎯⎯⎯⎯ as elaborated below.

Updating 𝑘𝑘𝑙,𝑖𝑛 We define 𝛿𝑘𝑘𝑙,𝑖𝑛 as the gradient of the full-precision kernel 𝑘𝑘𝑙,𝑖𝑛, and have:

𝛿𝑘𝑘𝑙,𝑖𝑛=∂𝐿∂𝑘𝑘𝑙,𝑖𝑛=∂𝐿𝑆∂𝑘𝑘𝑙,𝑖𝑛+∂𝐿𝐵∂𝑘𝑘𝑙,𝑖𝑛.
(16)
For each term in Eq. (16), we have:

∂𝐿𝑆∂𝑘𝑘𝑙,𝑖𝑛=∂𝐿𝑆∂𝑘𝑘^𝑙,𝑖𝑛∂𝑘𝑘^𝑙,𝑖𝑛∂(𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛)∂(𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛)∂𝑘𝑘𝑙,𝑖𝑛=∂𝐿𝑆∂𝑘𝑘^𝑙,𝑖𝑛∘𝟙−1≤𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛≤1∘𝑤𝑤𝑙,
(17)
∂𝐿𝐵∂𝑘𝑘𝑙,𝑖𝑛=𝜆{𝑤𝑤𝑙∘[𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛−𝑘𝑘^𝑙,𝑖𝑛]+𝜈[(𝜎𝜎𝑙𝑖)−2∘(𝑘𝑘𝑙𝑖+−𝜇𝜇𝑙𝑖+)+(𝜎𝜎𝑙𝑖)−2∘(𝑘𝑘𝑙𝑖−+𝜇𝜇𝑙𝑖−)],
(18)
where 𝟙 is the indicator function that is widely used to estimate the gradient of non-differentiable parameters Rastegari et al. (2016), and (𝜎𝜎𝑙𝑖)−2 is a vector whose elements are all equal to (𝜎𝑙𝑖)−2.

Updating 𝑤𝑤𝑙 Unlike the forward process, 𝑤𝑤 is used in the back propagation for the calculation of gradients. This process is similar to the way of calculating 𝑥̂ 𝑥̂  from 𝑥𝑥 in an asynchronous manner. Specifically, 𝛿𝑤𝑤𝑙 is composed of the following two parts:

𝛿𝑤𝑤𝑙=∂𝐿∂𝑤𝑤𝑙=∂𝐿𝑆∂𝑤𝑤𝑙+∂𝐿𝐵∂𝑤𝑤𝑙.
(19)
For each term in Eq. (19), we have:

∂𝐿𝑆∂𝑤𝑤𝑙=∑𝑖=1𝐼𝑙∑𝑛=1𝑁𝐼𝑙∂𝐿𝑆∂𝑘𝑘^𝑙,𝑖𝑛∂𝑘𝑘^𝑙,𝑖𝑛∂(𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛)∂(𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛)∂𝑤𝑤𝑙=∑𝑖=1𝐼𝑙∑𝑛=1𝑁𝐼𝐿∂𝐿𝑆∂𝑘𝑘^𝑙,𝑖𝑛∘𝟙−1≤𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛≤1∘𝑘𝑘𝑙,𝑖𝑛,
(20)
∂𝐿𝐵∂𝑤𝑤𝑙=𝜆∑𝑖=1𝐼𝑙∑𝑛=1𝑁𝐼𝑙(𝑤𝑤𝑙∘𝑘𝑘𝑙,𝑖𝑛−𝑘𝑘^𝑙,𝑖𝑛)∘𝑘𝑘𝑙,𝑖𝑛.
(21)
Updating 𝜇𝑙𝑖 and 𝜎𝑙𝑖 Note that we use the same 𝜇𝑙𝑖 and 𝜎𝑙𝑖 for each kernel (see Sec. 3.2). So the gradients here are scalars. The gradients 𝛿𝜇𝑙𝑖 and 𝛿𝜎𝑙𝑖 are computed as:

𝛿𝜇𝑙𝑖=∂𝐿∂𝜇𝑙𝑖=∂𝐿𝐵∂𝜇𝑙𝑖=𝜆𝜈𝐶𝑙𝑖×𝐻𝑙×𝑊𝑙∑𝑛=1𝐶𝑙𝑖∑𝑝=1𝐻𝑙×𝑊𝑙{(𝜎𝑙𝑖)−2(𝜇𝑙𝑖−𝑘𝑙,𝑖𝑛,𝑝),(𝜎𝑙𝑖)−2(𝜇𝑙𝑖+𝑘𝑙,𝑖𝑛,𝑝),𝑘𝑙,𝑖𝑛,𝑝≥0,𝑘𝑙,𝑖𝑛,𝑝<0,
(22)
𝛿𝜎𝑙𝑖=∂𝐿∂𝜎𝑙𝑖=∂𝐿𝐵∂𝜎𝑙𝑖=𝜆𝜈𝐶𝑙𝑖×𝐻𝑙×𝑊𝑙∑𝑛=1𝐶𝑙𝑖∑𝑝=1𝐻𝑙×𝑊𝑙{−(𝜎𝑙𝑖)−3(𝑘𝑙,𝑖𝑛,𝑝−𝜇𝑙𝑖)2+(𝜎𝑙𝑖)−1,𝑘𝑙,𝑖𝑛,𝑝≥0,−(𝜎𝑙𝑖)−3(𝑘𝑙,𝑖𝑛,𝑝+𝜇𝑙𝑖)2+(𝜎𝑙𝑖)−1,𝑘𝑙,𝑖𝑛,𝑝<0,
(23)
where 𝑘𝑙,𝑖𝑛,𝑝,𝑝∈{1,...,𝐻𝑙×𝑊𝑙}, denotes the p-th element of 𝑘𝑘𝑙,𝑖𝑛. We update 𝑐𝑐𝑚 using the same strategy as the center loss Wen et al. (2016) in the fine-tuning process. The updating 𝜎𝑚,𝑛 based on 𝐿𝐵 is straightforward and not elaborated here for brevity.

Updating 𝐾𝐾𝑙𝑖,𝑗 In pruning, our purpose is to make the filters gradually converge to their mean. So we replace each filter 𝐾𝐾𝑙𝑖,𝑗 with its corresponding mean 𝐾𝐾⎯⎯⎯⎯⎯𝑙𝑖,𝑗. The gradient of the mean is represented as follows:

∂𝐿∂𝐾𝐾𝑙𝑖,𝑗=∂𝐿𝑆∂𝐾𝐾𝑙𝑖,𝑗+∂𝐿𝐵∂𝐾𝐾𝑙𝑖,𝑗+∂𝐿𝑃∂𝐾𝐾𝑙𝑖,𝑗=∂𝐿𝑆∂𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗∂𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗∂𝐾𝐾𝑙𝑖,𝑗+∂𝐿𝐵∂𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗∂𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗∂𝐾𝐾𝑙𝑖,𝑗+∂𝐿𝑃∂𝐾𝐾𝑙𝑖,𝑗=1𝐼𝑗(∂𝐿𝑆∂𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗+∂𝐿𝐵∂𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗)+2(𝐾𝐾𝑙𝑖,𝑗−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑗)+2𝜈(ΨΨ𝑙𝑗)−1(𝐾𝐾𝑙𝑖,𝑗−𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑗),
(24)
where 𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗=1𝐼𝑗∑𝐼𝑗𝑖=1𝐾𝐾𝑙𝑖,𝑗 that is used to update the filters in one group by the mean 𝐾⎯⎯⎯⎯⎯𝐾⎯⎯⎯⎯⎯𝑙𝑗. To prune redundant filters, we leave the first filter in each group and remove the others. However, such an operation changes the distribution of the input channel of the batch-norm layer, resulting in a dimension mismatch for the next convolutional layer. To solve the problem, we maintain the size of the batch-norm layer, whose values correspond to the removed filters are set to zero. In this way, the removed information is retained to the greatest extent.

figure a
In summary, we show that the proposed method is trainable in an end-to-end manner. The learning procedure is detailed in Algorithms. 1 and 2.

figure b
Experiments
We evaluate the proposed BONNs on three widely-used tasks, including image classification, face recognition, and object detection. We further evaluate the BONN-based model pruning method on the image classification task. For image classification, our BONNs are evaluated on the CIFAR-10/100 Krizhevsky et al. (2014) and ImageNet ILSVRC12 datasets Deng et al. (2009) with three kinds of backbones, i.e., Wide-ResNets (WRNs) Zagoruyko and Komodakis (2016), ResNets He et al. (2016) and MobileNets Howard et al. (2017). For face recognition, the LFW Huang et al. (2008), Celebrities in Frontal-Profile (CFP) Sengupta et al. (2016), and AgeDB Moschoglou et al. (2017) datasets are utilized to verify the effectiveness of our method with ResNets. We further extend our method to object detection on PASCAL VOC Everingham et al. (2010) and COCO Lin et al. (2014) with both SSD Liu et al. (2016) and Faster-RCNN Ren et al. (2015) framework. Besides, model pruning based on the Bayesian algorithm is validated on the image classification task with CIFAR-10 and ImageNet ILSVRC12 datasets. Considering the favorable generalization capability of our method, BONNs can be integrated into any DCNN variant. For a fair comparison with other state-of-the-art 1-Bit CNNs, we use Wide-Resnet (WRN) Zagoruyko and Komodakis (2016), ResNet-18 He et al. (2016), and MobileNetV1 Howard et al. (2017) as the full-precision backbone network. In the following experiments, both the kernels and the activations are binarized. The leading performances reported in the following sections verify the superiority of our BONNs.

Fig. 3
figure 3
Left images are the input images chosen from ImageNet ILSVRC12 dataset. Right images are feature maps and binary feature maps from different layers of BONNs. For each group, the first and third rows are feature maps, while the second and fourth rows are corresponding binary feature maps. Although the binarization of feature map causes the information loss, BONNs could extract essential features for accurate classification

Full size image
Datasets and Implementation Details
Datasets
CIFAR-10 Krizhevsky et al. (2014) is a natural image classification dataset, which is composed of a training set and a test set, with 50,000 and 10,000 32×32 color images, respectively. These images span 10 different classes, including airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Comparatively, CIFAR-100 is a more comprehensive dataset containing 100 classes. On CIFAR-10/100, WRNs are employed as the backbone of BONNs. ImageNet ILSVRC12 object classification dataset Deng et al. (2009) is more diverse and challenging. It contains 1.2 million training images, and 50,000 validation images, across 1000 classes.

CASIA-WebFace Dong et al. (2014) is a face image dataset collected from over ten thousand different individuals, containing nearly half a million facial images. Note that compared to other private datasets used in DeepFace Taigman et al. (2014) (4M), VGGFace Parkhi et al. (2015) (2M), and FaceNet Schroff et al. (2015) (200M), our training data contains just 490K images and is more challenging.

The Labeled Faces in the Wild (LFW) dataset Huang et al. (2008) has 5,749 celebrities and collected 13,323 photos from the web. The photos are organized into 10 splits, each of which contains 6000 images. Celebrities in Frontal-Profile (CFP) Sengupta et al. (2016) consists of 7000 images of 500 subjects. The dataset contains 5000 images in frontal view and 2000 images in extreme profile to evaluate the performance on coping with the pose variation. The data is divided into 10 splits, each containing an equal number of frontal-frontal and frontal-profile comparisons. AgeDB Moschoglou et al. (2017) includes 16,488 images of various famous people. The images are categorized into 568 distinct subjects according to their identity, age, and gender attributes.

The PASCAL VOC dataset contains natural images from 20 different classes. We train our model on the VOC trainval2007 and VOC trainval2012 sets, which consist of approximately 16k images. We then evaluate our method on the VOC test2007 set, which includes 4952 images. Following Everingham et al. (2010), we use the mean average precision (mAP) as the evaluation criterion.

The COCO dataset consists of images from 80 categories. We conduct experiments on the COCO 2014 Lin et al. (2014) object detection track. Models are trained with the combination of 80k images from the COCO train2014 and 35k images sampled from COCO val2014, i.e., COCO trainval35k. On the remaining 5k images from the COCO minival, we test our method based on the average precision (AP) for IoU∈ [0.5 : 0.05: 0.95] denoted as mAP@[.5, .95]. We also report AP50, AP75, AP𝑠, AP𝑚, and AP𝑙 to further analyze our method.

Methods
WideResNet(WRN) The structure of WRN is similar to ResNet, but a depth factor k is introduced to control the feature map depth expansion through 3 stages, while the spatial dimension of the features is kept the same. For brevity, we set k to 1 in the experiments. The number of channels in the first stage is another important parameter in WRN. We set it to 16 and 64, thus resulting in two network configurations: 16-16-32-64 and 64-64-128-256. In the 64-64-128-256 network, a dropout layer with a ratio of 0.3 is added to prevent overfitting. The learning rate is initially set to 0.01, which decays by 20% per 60 epochs until the maximum epoch of 200 on CIFAR-10/100. We set 𝜈 to 1𝑒−4 for quantization error in WRN. Bayesian feature loss is only used in the fine-tuning process. Other training details are the same as those described in Zagoruyko and Komodakis (2016). WRN-22 denotes a network with 22 convolutional layers and similarly for WRN-40.

Table 2 With different 𝜆 and 𝜃, we evaluate the accuracies of BONNs based on WRN-22 and WRN-40 on CIFAR-10/100. When varying 𝜆, the Bayesian feature loss is not used (𝜃=0). However, when varying 𝜃, we choose the optimal loss weight (𝜆=1𝑒−4) for the Bayesian kernel loss
Full size table
Fig. 4
figure 4
Training and test accuracies on ImageNet when 𝜆=1𝑒−4, which shows the superiority of the proposed BONN over XNOR-Net. The backbone of the two networks is ResNet-18

Full size image
ResNet We implement two ResNets He et al. (2016) in this paper for image classification task, i.e., ResNet-18 and ResNet-50. For ResNet-18, we binarize the features and kernels in the backbone convolution layers without convolution layers in shortcuts, following the settings and network modifications in Bi-Real Net Liu et al. (2020). The SGD algorithm has a momentum of 0.9 and a weight decay of 1𝑒−4. The learning rate for 𝑤𝑤𝑙,𝜎𝑙𝑖 is set to 0.01, while for 𝑋𝑋𝑙𝑖,𝜇𝑙𝑖 and other parameters the rates are set to 0.1. 𝜈 is set to 1𝑒−3 for the quantization error in ResNet-18. The strategy of the learning rate decay is also employed, which degrades to 10% for every 30 epochs before the algorithm reaches the maximum epoch of 70. We also evaluate our BONN by fine-tuning a pre-trained ReActNet Liu et al. (2020) with our BONN and ReActNet, respectively.We denote this implementation as BONN*.

For ResNet-50, we follow the same architecture as Bi-Real Net Liu et al. (2020). And we train ResNet-50 in two steps: (1) binarizing the activations and weights in 1×1 convolutional layers along with the activations in 3×3 convolutional layers, and (2) binarizing the weights in 3×3 convolutional layers. For the 50-layer Bi-Real net, each sub-step includes 100 epochs and the batch size is set to 800. The learning rate starts from 0.1 and is multiplied by 0.1 at the 50th and the 75th epoch respectively, which is identical to Liu et al. (2020) for a fair comparison.

MobileNet For MobileNets, we select MobileNetV1-based Howard et al. (2017) modules to validate the effectiveness of our BONN further. Specifically, we follow the same MobileNetV1-based architecture as MoBiNet Phan et al. (2020) and BMES. Phan et al. (2020). To farily compare BONN with these arts, we use the same training protocol as Phan et al. (2020), Phan et al. (2020).

SSD For one stage SSD framework, the extra layers are set as real-valued following BiDet Wang et al. (2020). We select VGG-16 Simonyan and Zisserman (2015) backbone pre-trained by ImageNet ILSVRC12 for the comparison with BiDet. We used the data augmentation techniques Liu et al. (2016) when training our BONN with the SSD300 detection framework. We train the SSD model for 24 epochs with a learning rate set as 0.01 with cosine annual learning rate decay.

Faster-RCNN: For the two-stage Faster-RCNN framework, we select ResNet-18 backbone pretrained by ImageNet ILSVRC12 for the comparison with BiDet. We used the data augmentation techniques in Ren et al. (2015) when training our BONN with the Faster-RCNN detection framework. We train the aster-RCNN model for 12 epochs with a learning rate set as 0.004, which decay by multiplying 0.1 at the 9-th epoch.

Ablation Study
Hyper-parameter Selection
In this section, we evaluate the effects of the hyper-parameters on the performance of BONNs, including 𝜆 and 𝜃. The Bayesian kernel loss and the Bayesian feature loss are balanced by 𝜆 and 𝜃, respectively, for adjusting the distributions of kernels and features in a better form. WRN-22 and WRN-40 are used in the experiments. The implementation details are given below.

As shown in Table 2, we first vary 𝜆 and set 𝜃 to zero for validating the influence of the Bayesian kernel loss on the kernel distribution. The utilization of Bayesian kernel loss effectively improves the accuracy on CIFAR-10. However, the accuracy does not increase with 𝜆, indicating what we need is not a larger 𝜆, but a proper 𝜆 to reasonably balance the relationship between the cross-entropy and the Bayesian kernel loss. For example, when 𝜆 is set to 1𝑒−4, we obtain an optimal balance and the best classification accuracy.

The hyper-parameter 𝜃 dominates the intra-class variations of the features, and the effect of the Bayesian feature loss on the features is also investigated by changing 𝜃. The results illustrate that the classification accuracy varies in a way similar to 𝜆, verifying that the Bayesian feature loss can lead to a better classification accuracy when a proper 𝜃 is chosen.

We also evaluate the convergence performance of our method over comparative counterparts in terms of ResNet-18 on ImageNet ILSVRC12. As plotted in Fig. 4, the training curve of the XNOR-Net oscillates vigorously, which is suspected to be triggered by a sub-optimal learning process. In contrast, our BONN achieves better training and test accuracy.

Fig. 5
figure 5
We demonstrate the kernel weight distribution of the first binarized convolution layer of BONNs. Before training, we initialize the kernels as a single-mode Gaussian distribution. From the 2-th epoch to the 200-th epoch, with 𝜆 fixed to 1𝑒−4, the distribution of the kernel weights becomes more and more compact with two modes, which confirms that the Bayesian kernel loss can regularize the kernels into a promising distribution for binarization

Full size image
Effectiveness of Bayesian Binarization on ImageNet ILSVRC12
To better understand the Bayesian losses on the large-scale ImageNet ILSVRC12 dataset, we experiment to examine how each loss affects performance. According to the above experiments, we set 𝜆 to 1𝑒−4 and 𝜃 to 1𝑒−3, if they are used. As shown in Table 3, both the Bayesian kernel loss and the Bayesian feature loss can independently improve the accuracy on ImageNet. When applied together, the Top-1 accuracy reaches the highest value of 59.3%.

Table 3 Effect of with/without the Bayesian losses on the ImageNet dataset. The backbone is ResNet-18
Full size table
Weight Distribution
Figure 5 further illustrates the distribution of the kernel weights, with 𝜆 fixed to 1𝑒−4. During the training process, the distribution gradually approaches the two-mode GMM as assumed previously, confirming the effectiveness of the Bayesian kernel loss in a more intuitive way. We also compare the kernel weight distribution between XNOR-Net and BONN. As is shown in Fig. 6, the kernel weights learned in XNOR-Net distribute tightly around the threshold value, but those in BONN are regularized in a two-mode GMM style. Figure 7 shows the evolution of the binarized values during the training process of XNOR-Net and BONN. The two different patterns indicate the binarized values learned in BONN are more diverse.

Fig. 6
figure 6
The weight distribution of XNOR and BONN, both of which are based on WRN-22 (2nd, 8th and 14th convolutional layers) after 200 epochs. The weight distribution difference between XNOR and BONN indicates that the kernels are regularized with the proposed Bayesian kernel loss, across the convolutional layers

Full size image
Fig. 7
figure 7
The evolution of the binarized values, |x|s, during the training process of XNOR and BONN. They are both based on WRN-22 (2nd, 3rd, 8th and 14th convolutional layers) and the curves are not sharing the same y-axis. The binarized values of XNOR-Net tend to converge to small and similar values but these of BONN are learned diversely

Full size image
Effectiveness of Bayesian Feature Loss on Real-Valued Models
We further apply our Bayesian feature loss on the real-valued models, including ResNet-18 and ResNet-50 He et al. (2016). We retrain these two backbones with our Bayesian feature loss for 70 epochs. We set hyper-parameter 𝜃 to 1𝑒−3. The SGD optimizer is used with an initial learning rate set to 0.1. We use a step learning rate schedule which degrades to 10% for every 30 epochs. As shown in Table 4, our Bayesian feature loss can further boost the performance of real-valued models by a clear margin. Specifically, our method promotes the performances of ResNet-18 and ResNet-50 by 0.6% and 0.4% Top-1 accuracies, respectively.

Results on Image Classification
Results on CIFAR-10/100 datasets
We first evaluate BONNs by comparing them to XNOR-Net Rastegari et al. (2016) with WRN backbones and also report the accuracy of full-precision WRNs on CIFAR-10 and CIFAR-100. Three WRN variants were chosen for a comprehensive comparison: 22-layer WRNs with the kernel stage of 16-16-32-64 and 64-64-128-256. We also use data augmentation where each image is padded with size 4 and is randomly divided into 32×32 windows for CIFAR-10/100. Table 5 indicates that BONNs outperform XNOR-Net on both datasets by a large margin in all three cases. Compared with the full-precision WRNs, BONNs reduce the accuracy degradation to an acceptable level on the backbone, which verifies the advantage of our method in building 1-bit CNNs. Moreover, WRN-22 and WRN-40 obtain similar classification accuracies, but XNOR-Nets based on them perform with a huge difference. The 40-layer XNOR-Net fails to maintain the accuracy when it becomes deeper, while our BONNs keep similar accuracies with deeper or wider WRN backbones.

Results on ImageNet ILSVRC12
To further evaluate the performance of our method, we evaluate BONNs on the ImageNet dataset. Figure 4 shows curves of the Top-1 and Top-5 training/test accuracies. Notably, we adopt two data augmentation methods in the training set: 1) cropping the image to the size of 224×224 at random locations, and 2) flipping the image horizontally. In the test set, we simply crop the image to 224×224 from the center. ResNet-18 is the backbone, which only has slight structure adjustments as described in Liu et al. (2020).

Table 4 Effect of with/without the Bayesian feature loss on the ImageNet dataset. The backbone is real-valued ResNet-18 and ResNet-50
Full size table
Table 5 Test accuracies on the CIFAR-10/100 datasets. BONNs are based on WRNs Rastegari et al. (2016). We calculate the number of parameters for each model, and the numbers refer to the models on CIFAR-10.
Full size table
As shown in Fig. 3, we visualize the feature maps across the ResNet-18 model on the ImageNet dataset. They indicate that our method can extract essential features for accurate classification.

We compare the performance of BONN with other state-of-the-art quantized networks on the ResNet-18 backbone, including BWN Rastegari et al. (2016), DoReFa-Net Zhou et al. (2016), TBN Wan et al. (2018), XNOR-Net Rastegari et al. (2016), ABC-Net Lin et al. (2017a), BNN Courbariaux et al. (2016), Bi-Real Net Liu et al. (2020), PCNN Gu et al. (2019), IR-Net Qin et al. (2020), and ReActNet Liu et al. (2020). Table 6 indicates that BONN obtains the highest accuracy among these 1-bit CNNs, in which PCNN and IR-Net perform the best among these baselines. Yet BONN outperforms them by about 2% and 1% in Top-1 accuracy, respectively. Moreover, due to the application of the clip function Liu et al. (2020), Bi-Real Net is trained in a two-stage procedure, which requires an extra cost. It is also worth mentioning that DoReFa-Net and TBN use more than 1-bit to quantize the activations, yet the proposed model still gets the best performance in all comparisons.

Table 6 Test accuracies on ImageNet. ’W’ and ’A’ refer to the weight and activation bitwidth respectively
Full size table
Moreover, we evaluate our BONN by fine-tuning a pre-trained ReActNet with our BONN (denoted as BONN*) and ReActNet, respectively. Table 6 demonstrates our BONN improves the pre-trained ReActNet by 0.3% in Top-1 accuracy. However, ReActNet gains no improvement from pre-trained weights. Totally, BONN* obtains 66.2% Top-1 accuracy with 1-bit activations and weights.

For ResNet-50 backbone, we compare our BONN with XNOR-Net Rastegari et al. (2016) and Bi-Real Net Liu et al. (2020). As seen in the bottom lines of Table 6, BONN surpasses all prior arts again. Quantitatively, BONN outperforms XNOR-Net and Bi-Real Net by 0.7% and 1.2%, respectively. These results demonstrate that our BONN promotes the state-of-the-art result on the 1-bit ResNet-50 backbone.

Table 7 Test accuracies on ImageNet. ’W’ and ’A’ refer to the weight and activation bitwidth respectively
Full size table
With the MobileNetV1 backbone, we select two MobileNetV1-based BNNs for comparison, i.e, MoBiNet Phan et al. (2020) and BMES Phan et al. (2020). For MoBiNet-Mid, we set k =4 and achieve the architecture of 3×3 depth-wise binary convolutions replaced by 3×3 group convolution with groups =𝐶𝑙𝑖24 Phan et al. (2020). 𝐶𝑙𝑖 denotes the number of input channel. We observe that BONN surpasses baseline MoBiNet-Mid by 1.8% Top-1 accuracy as shown in Table 7 on ImageNet ILSVRC12, demonstrating the superiority of Bayesian learning. For BMES, we choose three backbones M1/M2/M3 Phan et al. (2020) with the group set as 16. As shown in Table 7, our BONN can improve the performance of BMES by 1.3%, 2.9%, and 3.5% on three employed backbones, respectively.

These results show that BONNs are not limited to small datasets and work well on the large dataset. This further verifies the generalization capability of the proposed BONNs.

Results on Face Recognition
This section compares full-precision CNNs, XNOR-Net, and PCNN with BONNs on the face recognition task. Our models are based on ResNet-18 and ResNet34 with kernel stage, 64-128-256-512 and each model has two FC layers. We use the CASIA-Webface dataset for training and LFW, CFP, AgeDB datasets for testing. The setting of hyper-parameters is similar to the strategy of CIFAR experiments, despite the difference that the learning rate decays by 10% per 8 epochs and the maximum epochs is set to 30.

Table 8 Test accuracies based on ResNet-18 and ResNet-34 on face recognition datasets
Full size table
As demonstrated in Table 8, our methods achieve the best test result among 1-bit CNNs. On LFW, BONN has only 2% accuracy degradation compared to the results of the full-precision models, which verify the potential of 1-bit networks in practice. With a deeper backbone architecture, BONNs with 34 layers further increase the face recognition accuracy by 1.11% on the AgeDB dataset.

Results on Object Detection
Results on PASCAL VOC
In this section, we compare the proposed BONN with other state-of-the-art 1-bit neural networks, including XNOR-Net Rastegari et al. (2016), Bi-Real-Net Liu et al. (2020), and BiDet Wang et al. (2020), on the same framework for the task of object detection on the PASCAL VOC datasets. We also report the detection performance of the multi-bit quantized networks DoReFa-Net Zhou et al. (2016) and TWN Li et al. (2016).

Table 9 illustrates the comparison of the mAP across different quantization methods and detection frameworks. Our BONN significantly accelerates the computation and saves the storage on various detectors by binarizing the activations and weights to 1-bit.

Table 9 Comparison of mAP (%) with state-of-the-art 1-bit CNNs in both one-stage and two-stage detection frameworks on VOC test2007
Full size table
The results for 1-bit Faster-RCNN on VOC test2007 are summarized from lines 2 to 7 in Table 9. Compared with other 1-bit methods, we observe significant performance improvements with our BONN over other state-of-the-arts. With the ResNet-18 backbone, our BONN outperforms XNOR-Net, Bi-Real-Net, and BiDet by 15.0%, 5.2%, and 3.9% mAP with the same bit-width activations and weights, i.e., the same memory usage and FLOPs.

The bottom lines in Table 9 illustrate that our BONN achieves a performance far more close to the real-valued counterpart compared with other prior arts on the SSD300 framework with VGG-16 backbone. Quantitatively speaking, our BONN surpasses XNOR-Net, Bi-Real Net and BiDet by 18.8%, 5.2%, and 3.0%, respectively. Moreover, BONN even achieves comparable performance as 4-bit DoReFa-Net (69.0% vs. 69.2%), which demonstrates the superiority of our BONN.

In short, we achieved a new state-of-the-art performance compared to other 1-bit CNNs on various detection frameworks with various backbones on PASCAL VOC. We also achieve a much closer performance to full-precision models, as demonstrated in extensive experiments, clearly validating the superiority of our BONN.

Results on COCO
The COCO dataset is much more challenging for object detection than PASCAL VOC due to its diversity and scale. We compare the proposed LWS-Det with state-of-the-art 1-bit neural networks, including XNOR-Net Rastegari et al. (2016), Bi-Real-Net Liu et al. (2020), and BiDet Wang et al. (2020), on COCO. We also report the detection performance of the 4-bit quantized DoReFa-Net Zhou et al. (2016).

Table 10 shows mAP and AP with different IoU thresholds and AP of objects with different scales. Limited by the page width, we do not show the memory usage and FLOPs in Table 10. We conduct experiments on Faster-RCNN and SSD frameworks.

Compared with the state-of-the-art 1-bit methods, our BONN outperforms other methods by significant margins. With the ResNet-18 backbone, our BONN improves the mAP@[.5, .95] by 9.4%, 5.4%, and 4.1% compared with state-of-the-art XNOR-Net, Bi-Real-Net, and BiDet, respectively. Similarly, on other APs with different IoU thresholds, our BONN outperforms other methods obviously.

On the SSD300 framework with the VGG-16 backbone, our BONN achieves 16.7% mAP@[.5, .95], which outperforms XNOR-Net, Bi-Real Net, and BiDet by 8.6%, 5.5%, and 3.5% mAP, respectively.

To conclude, compared with the baseline methods of network quantization, our method achieves the best performance in terms of the AP with different IoU thresholds and the AP for objects in different sizes on COCO, demonstrating BONN’s superiority and universality in different application settings.

Table 10 Comparison of mAP@[.5,.95](%), AP with different IoU threshold and AP for objects in various sizes with state-of-the-art binarized object detectors in Faster-RCNN and SSD detection framework on COCO minival, where the performance of real-valued and 4-bit detectors is reported for reference
Full size table
Table 11 Pruning results of ResNet-18 on CIFAR-10. GAL-0.6 is the proportion of pruning regularization is set as 0.6 to prune the BONNs. BONN-0.6 means that we keep 60% parameters
Full size table
Fig. 8
figure 8
We visualize the filters of ResNet-18 in Bayesian pruning using principal components analysis (PCA). In each figure, we demonstrate the distribution of filters in 500 epoch. The first and second principal components of filters (𝐾𝐾𝑙𝑖) are the horizontal and vertical coordinates, respectively

Full size image
Results on Pruning
We implement our Bayesian pruning method on BONNs. Specifically, we prune ResNet models, which are more general and potential for real applications. It remains a challenge to prune such a compact model. We use the same pruning rate, i.e., 𝜖, for all layers. We denote BONN-𝜖 to present pruned models with different pruning rates.

Results on CIFAR-10
ResNet-18 is used to validate the performance of our pruning methods on the CIFAR-10 dataset. We prune them based on their corresponding pre-trained BONN models, which are also the baselines.

In the implementation, we change the pruning rates to validate the effectiveness of Bayesian pruning on ResNet-18 using CIFAR-10. Bayesian pruning can prune the wider layers (with more channels) since these layers take up more storage and computation in the model. We compare our method with the state-of-the-art method Lin et al. (2019a), by keeping the same pruning rate for a fair comparison. Besides, different pruning rates are chosen from 10% (BONN-0.9), 20% (BONN-0.8), 30% (BONN-0.7). As demonstrated in Table 11, we achieve better results for all the cases than state-of-the-art GALs.

With a pruning rate of 40%, our pruning method BONN obtains better performance than GAL, 0.36% with a more lightweight pruned model (5.05×106 vs. 5.73×106 on OPs). When we set the pruning rate as 0.1, i.e., BONN-0.9, the pruned model even outperforms the baseline by 0.85% with about 12.2% OPs pruned.

Table 12 Pruning results of ResNet-18 on ImageNet ILSVRC12. X-0.75 means that we keep 75% parameters using X method
Full size table
We also visualize the evolution of filters in the training process base on principal components analysis (PCA). As shown in Fig. 8, the distribution of filters is a nearly Gaussian distribution, supporting our hypothesis for Bayesian pruning.

Results on ImageNet ILSVRC12
BONN pruning is further evaluated on ImageNet ILSVRC12 using ResNet-18. We train the pruning network for 100 epochs with an initial learning rate set to 0.1 and scaled by a factor of 0.1 at the 40-th, 60-th, and 80-th epoch. We attempt to prune about 50% of the FLOPs for models implemented on ImageNet ILSVRC12. Hence, we set the pruning rate to 0.75 for comparison. We select state-of-the-art methods for comparison, including DCP Liu et al. (2021), HRank Lin et al. (2020), and EagleEye Li et al. (2020). Uniform-0.75 in Table 12 denotes training a BONN-based 1-bit ResNet-18 with width multiplier 0.75, i.e., the kernel stage becomes 48-96-192-384.

As listed in the bottom lines of Table 12, our BONN outperforms the prior arts on ImageNet ILSVRC12, again. On 1-bit ResNet-18, BONN-0.75 achieves the Top-1 accuracy exceeding Adapt-DCP, HRank and EagleEye by 0.6%, 0.4% and 0.3% respectively. Moreover, compared with Uniform-0.75, BONN-0.75 improves the Top-1 accuracy by 1.6%.

We further report the experimental results on the real-valued ResNet-18 backbone to show the generalization of our Bayesian pruning method. As listed in the top lines of Table 12, our BONN also offers strong results on pruning real-valued networks. For instance, BONN-0.75 achieves the Top-1 accuracy exceeding HRank and Adapt-DCP by 0.7% and 0.5% respectively.

Table 13 The efficiency analysis on the ResNet-18/50, and MobileNetV1-based backbones we employed
Full size table
Memory Usage and Efficiency Analysis
Efficiency of BONN Quantization
In a real-valued network, each parameter requires 32 bits in storage. While in 1-bit CNNs, each parameter is stored with just 1 bit. The memory usage is computed as the summation of 32-bit times the number of float parameters (FLParam) and 1-bit times the number of binary parameters (BParam) in the network, i.e., Memory Usage = 32× FLParam + BParam. Further, we use operations (OPs) to measure the speed, which is calculated as 164× BOPs + FLOPs Liu et al. (2020). The results of ResNet-18 for image classification with ImageNet ILSVRC12 are given in Table 13. As shown in Table 13, the proposed BONN accelerates the real-valued ResNet-18 by 18.191.65=11.02× in theory. Furthermore, the memory saving is up to 378.4132.21=11.75×. Likewise, BONN achieves 41.182.01=20.49× and 817.9289.42=9.16× for acceleration and memory saving on the ResNet-50 backbone, respectively.

As shown in the bottom lines of Tab. 13, BONNs can further compress and accelerate the lightweight MobileNetV1 with highly efficient 1-bit convolutions. For MoBiNet-Mid backbone, BONN can achieve 0.52×108 OPs under a 1-bit framework, which accelerate MobileNetV1 by 5.840.52=11.23×. Likewise, BONN can also accelerate MobileNetV1 by 3.89×/19.47×/24.33× with BMES-M1/M2/M3 backbone. These results firmly state the efficiency of BONNs when compressing lightweight networks.

For the face recognition task, the network architectures are similar to the image classification with only different input resolutions and fully connected layers. Thus the quantization efficiency can be calculated similarly to situations on the image classification task.

Table 14 Efficiency Analysis on the three models we employed. We report FLOPs, BOPs, Computation, FLParams, BParams and Memory Usage.
Full size table
We evaluate the OPs for Faster-RCNN with ResNet-18 backbone and SSD with VGG-16 backbone for the object detection task. All results are listed in Table 14. As seen, BONN can achieve a 343.398.58=50.63× theoretical acceleration rate on full-precision Faster-RCNN with the same backbone, which is very significant for efficient object detection. The memory consumption can be saved up to 379.8420.02=18.94× times. Likewise, BONN can realize impressive 14.76× and 4.56× theoretical acceleration and model size compression for the SSD framework. To conclude, BONNs are of vital significance for real-time edge computing, due to their highly efficient XNOR and Bit-count operations.

Table 15 Comparison on the theoretical and realistic acceleration on ImageNet ILSVRC2012 test with BONN-based ResNet-18 backbone.
Full size table
Efficiency of BONN Pruning
Bayesian pruning is used to prune 1-bit CNNs, our method can further improve the memory footprint up to 50% with acceptable performance loss. To compare the theoretical and realistic acceleration, we measure the inference time of the forward propagation between the baseline and the pruned model on one NVIDIA TITAN V GPU. We test the BONN baseline and BONN-0.75. Note that the GPUs do not support the quantization acceleration. Hence, BONN has an inference speed similar to real-valued ResNet-18 on GPUs. The results are listed in Table 15. The theoretical acceleration is computed from FLOPs pruning rate, e.g., 1.650.83=1.99× for BONN-0.75. The acceleration on GPUs can be up to 0.430.30=1.43×. The theoretical and realistic model gap may come from hardware limitations, such as IO delay and buffer switch.

Conclusion and Future Work
This paper proposes Bayesian optimized 1-bit CNNs (BONNs), which consider full-precision kernel and feature distributions, resulting in a unified Bayesian framework with new Bayesian learning algorithms. We incorporate the prior distributions of full-precision kernels, features, and filters into the Bayesian framework to build 1-bit CNNs in a comprehensive end-to-end manner. The Bayesian learning algorithms are used to improve the compactness and capacity of 1-bit CNNs largely. Extensive experiments on CIFAR and ImageNet demonstrate that BONNs achieve the best classification performance for WRNs and ResNet-18, and have superior performance over other 1-bit CNNs. We also achieve a promising performance on face recognition, object detection, and model pruning, which validate the generality of our proposed method. In the future, we will combine our method with neural architecture search (NAS) to build the data-adaptive 1-bit CNNs. We will also try Bayesian optimization to find the optimal pruning rate for more compact CNNs.