Recently, records on stereo matching benchmarks are constantly broken by end-to-end disparity networks. However, the domain adaptation ability of these deep models is quite limited. Addressing such problem, we present a novel domain-adaptive approach called AdaStereo that aims to align multi-level representations for deep stereo matching networks. Compared to previous methods, our AdaStereo realizes a more standard, complete and effective domain adaptation pipeline. Firstly, we propose a non-adversarial progressive color transfer algorithm for input image-level alignment. Secondly, we design an efficient parameter-free cost normalization layer for internal feature-level alignment. Lastly, a highly related auxiliary task, self-supervised occlusion-aware reconstruction is presented to narrow the gaps in output space. We perform intensive ablation studies and break-down comparisons to validate the effectiveness of each proposed module. With no extra inference overhead and only a slight increase in training complexity, our AdaStereo models achieve state-of-the-art cross-domain performance on multiple benchmarks, including KITTI, Middlebury, ETH3D and DrivingStereo, even outperforming some state-of-the-art disparity networks finetuned with target-domain ground-truths. Moreover, based on two additional evaluation metrics, the superiority of our domain-adaptive stereo matching pipeline is further uncovered from more perspectives. Finally, we demonstrate that our method is robust to various domain adaptation settings, and can be easily integrated into quick adaptation application scenarios and real-world deployments.

Access provided by University of Auckland Library

Introduction
Stereo matching is a fundamental problem in computer vision. The task aims to find corresponding pixels in a stereo pair, and the distance between corresponding pixels is known as disparity (Hartley and Zisserman 2003). Based on the epipolar geometry, stereo matching enables stable depth perception from estimated disparity, hence it has been applied to further applications such as scene understanding (Franke and Joos 2000; Miclea and Nedevschi 2019; Zhang et al. 2010), object detection (Chen et al. 2015; Li et al. 2018, 2019; Fang et al. 2018), visual odometry (Wang et al. 2017; Zhu 2017), and SLAM (Engel et al. 2015; Gomez-Ojeda et al. 2019).

Fig. 1
figure 1
Overview examples. Top-down: Middlebury (Scharstein et al. 2014) and KITTI (Menze and Geiger 2015) examples. Leftâ€“right: left stereo image, disparity map predicted by the SceneFlow-pretrained PSMNet (Chang and Chen 2018), and disparity map predicted by our Ada-PSMNet

Full size image
Classical stereo matching methods generally use hand-craft features and pre-defined windows to compute and aggregate matching costs. Due to the weak feature representations, most classical methods have problems in the areas with no textures, perspective distortions, and illumination changes. Since deep neural networks make great progress in the image classification task (Krizhevsky et al. 2012; Simonyan and Zisserman 2014; Szegedy et al. 2015; He et al. 2016; Zbontar and LeCun 2015) first introduced the deep network to extract image features and compute matching costs, which outperforms traditional methods by a significant margin. In order to further improve the accuracy and efficiency of stereo matching, recent methods typically adopt fully convolutional networks (Long et al. (2015) to directly regress disparity maps. Utilizing the massive synthetic data (Mayer et al. 2016) rendered by graphics tools for pretraining and real-world ground-truths for finetuning, these methods have achieved state-of-the-art performance on public stereo matching benchmarks (Geiger et al. 2012; Menze and Geiger 2015; Scharstein et al. 2014). However, the performance of these methods adapted from synthetic data to real-world scenes is quite limited, which is known as the domain adaptation problem. Here, we select the PSMNet (Chang and Chen 2018) to illustrate this problem. As shown in Fig. 1, the PSMNet pretrained on the SceneFlow dataset (Mayer et al. 2016) works well on the synthetic image pairs, but fails to produce equally good results on the Middlebury (Scharstein et al. 2014) and KITTI (Menze and Geiger 2015) datasets, in which disparity predictions are incorrect on the roads, vehicle windows, and indoor walls. Therefore instead of designing powerful models for higher accuracy on specific datasets, how to obtain effective domain-adaptive stereo matching networks is more desirable now.

In this work, we aim at the important but less explored problem of domain adaptation in stereo matching. Since there is a great number of synthetic data but only a small amount of realistic data with ground-truths, hence we focus on domain gaps between synthetic and real domains. Feeding two pairs of images under distinct domains into the stereo matching model, we analyze the differences at various levels, as shown in Fig. 2. At the input image level, color and brightness are the obvious gaps. We then plot a histogram for the values in the cost volume, which is a crucial internal representation for stereo matching, and significant differences can be found in the distribution of cost values. Moreover, geometries of the output disparity map are inconsistent as well, e.g. the shape of objects, and the relative position between foreground and background. In order to bridge the domain gaps at these levels, i.e. input image, internal cost volume, and output disparity, we propose a standard and complete domain adaptation pipeline for stereo matching named AdaStereo, in which three particular modules are presented:

For input image-level alignment, an effective algorithm, non-adversarial progressive color transfer, is proposed to align input color space between source and target domains during training. It is the first attempt that adopts non-adversarial style transfer method to align input-level inconsistency in the stereo domain adaptation, avoiding side-effects of harmful geometrical distortions caused by GAN-based methods (Zhu et al. 2017). Furthermore, the proposed progressive update scheme enables capturing representative target-domain color styles during adaptation.

For internal feature-level alignment, a cost normalization layer is designed to align the distribution of cost volumes. Oriented to the stereo matching task, we propose two normalization operations: (i) channel normalization reduces the inconsistency in scaling of each feature channel; and (ii) pixel normalization further regulates the norm distribution of pixel-wise feature vector for binocular matching. Embedded these two operations in one layer, the inter-domain gaps in matching costs are effectively narrowed. It is worth noting that our cost normalization layer is parameter-free and adopted only once in the network compared to previous general normalization layers (e.g. IN (Ulyanov et al. 2016), DN (Zhang et al. 2020).

For output-space alignment, we conduct self-supervised learning through a highly related auxiliary task, self-supervised occlusion-aware reconstruction, which is the first proposed auxiliary task for stereo domain adaptation. Concretely, a self-supervised module is attached upon the main disparity network to perform image reconstructions on the target domain. To address the ill-posed occlusion problem in reconstruction, we also design a domain-collaborative learning scheme for occlusion mask predictions. Through occlusion-aware stereo reconstruction, more informative geometries from target scenes are involved in model training, so as to benefit the disparity predictions across domains.

Based on the proposed pipeline, we conduct domain adaptation from synthetic data to real-world scenes. In order to validate the effectiveness of each module, ablation studies are performed on diverse real-world datasets, including Middlebury (Scharstein et al. 2014), ETH3D (SchÃ¶ps et al. 2017), KITTI (Geiger et al. 2012; Menze and Geiger 2015) and DrivingStereo (Yang et al. 2019). Specifically, for the image-level alignment, we compare our progressive color transfer method with other GAN-based methods. The cost normalization is also compared with those general normalization techniques. Moreover, we experiment different loss combinations to demonstrate the effectiveness of our self-supervised occlusion-aware loss. Eventually, our domain-adaptive models outperform other traditional/domain generalization/domain adaptation methods and even finetuned disparity networks on multiple public benchmarks. As shown in Fig. 1, our Ada-PSMNet pretrained on the synthetic dataset performs well on both indoor and outdoor scenes. Main contributions are summarized below:

We locate the domain-adaptive problem and investigate domain gaps for deep stereo matching networks.

We propose a novel domain adaptation pipeline, including three modules to narrow the gaps at input image-level, internal feature-level and output space.

Our domain-adaptive models outperform other domain-invariant methods, and even finetuned disparity networks on multiple stereo matching benchmarks.

Differences with our conference paper (Song et al. 2021). The extended contents are mainly in the related work, method analyses and descriptions, and abundant extended experiments (more than 3 new figures, and extra 11 tables). Specifically, more insightful analyses are included to illustrate the motivations and advantages of each proposed module in our domain-adaptive stereo matching pipeline. In addition, important instructions are supplemented to clarify the structure of self-supervised occlusion-aware reconstruction module, following by corresponding training details. To further reveal the effectiveness and generalization capability of our method, we conduct more experiments, including abundant extended ablation studies, additional cross-domain/benchmark performance comparisons on the DrivingStereo test set, as well as more verification and novel applications (i.e. new evaluation metrics, verification on top of more advanced stereo networks, real-world source domain applications, semi-supervised applications, and quick adaptations).

Related Work
Stereo Matching
The classical pipeline of stereo matching usually follows the four-step pipeline (Scharstein and Szeliski 2002): matching cost computation, matching cost aggregation, disparity computation, and disparity refinement. According to the optimization mode, the methods can be divided into local matching methods and global matching methods. The local matching methods (Okutomi and Kanade 1993; Zabih and Woodfill 1994; Kang et al. 1995; Black and Rangarajan 1996; Di Stefano et al. 2005; Mattoccia et al. 2008) focus on the computation and aggregation of matching costs to find matching pixels within a certain range. The global methods transform the stereo matching into an energy-minimization problem, followed by different solutions, e.g. dynamic programming (Ohta and Kanade 1985), belief propagation (Sun et al. 2003), or graph cut (Roy and Cox 1998). Generally, local methods have high speed but low accuracy, while global methods achieve higher accuracy but suffer slower speed. Furthermore, both local methods and global methods are difficult to handle the areas with no textures, perspective distortions, and illumination changes.

With the breakthrough of deep neural networks in the image classification task (Krizhevsky et al. 2012; Simonyan and Zisserman 2014; Szegedy et al. 2015; He et al. 2016), such structure also achieved great success in the stereo matching task by representing image patches with deep features and computing patch-wise similarity scores (Zbontar and LeCun 2015; Luo et al. 2016; Shaked and Wolf 2017; Chen et al. 2015). Benefited from stronger feature representations, these methods achieve much higher accuracy than traditional methods. However, due to the limited receptive field, the performance of these methods is unsatisfactory especially in ill-posed regions, and the processing is still time-consuming because of the remained hand-engineered post-processing steps.

Inspired from the semantic segmentation task (Long et al. 2015; Chen et al. 2015), recent methods (Mayer et al. 2016; Kendall et al. 2017; Saikia et al. 2019; Cheng et al. 2020) adopt fully-convolutional networks to regress disparity maps, which further improves the accuracy and efficiency of stereo matching. Concretely, current end-to-end stereo matching networks can be roughly categorized into two types: correlation-based 2-D stereo networks and cost-volume based 3-D stereo networks. For the first category, (Mayer et al. 2016) proposed the first end-to-end disparity network DispNetC, in which warping between features of two views is conducted for matching cost calculation, and per-pixel disparity is directly regressed without any post-processing steps. Based on color or feature correlations, more advanced methods were proposed, including CRL (Pang et al. 2017), iResNet (Liang et al. 2018), HD3 (Yin et al. 2019), SegStereo (Yang et al. 2018), EdgeStereo (Song et al. 2018, 2020), etc. For the second category, 3-D convolutional neural networks show the advantages in regularizing cost volume for disparity estimation (Yang et al. 2019; Wu et al. 2019; Zhang et al. 2020; Xu and Zhang 2020; Gu et al. 2020). GC-Net (Kendall et al. 2017) first introduced the 4-D cost volume without collapsing the feature dimension. PSMNet (Chang and Chen 2018) further applied a pyramid feature extraction module and stacked 3-D hourglass blocks to improve the accuracy of disparity prediction. Recently, more advanced methods were proposed to further optimize the cost volume regularization, including GwcNet (Guo et al. 2019), EMCUA (Nie et al. 2019), CSPN (Cheng et al. 2019), GANet (Zhang et al. 2019), etc. Our proposed domain adaptation pipeline for stereo matching can be easily applied to both 2-D and 3-D stereo networks.

Domain Adaptation
Prior works on domain adaptation can be roughly divided into three categories. The first category mainly covers the conventional methods including discrepancy measures such as MMD (Long et al. 2015; Geng et al. 2011) and CMD (Zellinger et al. 2017), geodesic flow kernel (Gong et al. 2012), sub-space alignment (Fernando et al. 2013), asymmetric metric learning (Kulis et al. 2011), etc. The general idea of the second category is to align source and target domains at different representation levels, including: (1) input image-level alignment (Bousmalis et al. 2017; Hoffman et al. 2018) using image-to-image translation methods such as CycleGAN (Zhu et al. 2017), or statistics matching (Abramov et al. 2020; 2) internal feature-level alignment based on feature-level domain adversarial learning (Tzeng et al. 2017; Long et al. 2018; Zhao et al. 2019); and (3) output-space alignment (Tsai et al. 2018; Vu et al. 2019; Lopez-Rodriguez and Mikolajczyk 2020) typically by an adversarial module. For the third category, self-supervised learning based domain adaptation methods (Ghifary et al. 2016) achieve great progress, in which simple auxiliary tasks generated automatically from unlabeled data are utilized to train feature representations, such as rotation prediction (Gidaris et al. 2018), flip prediction (Xu et al. 2019), patch location prediction (Xu et al. 2019), etc. In this paper, we explicitly implement domain alignments at input level and internal feature level, while incorporating self-supervised learning into output-space alignment through a specifically designed auxiliary task.

In addition, the existing domain adaptation methods are mainly designed for high-level tasks such as classification, semantic segmentation and object detection. For semantic segmentation, the pioneering work is Hoffman et al. (2016), which combined global and local alignment methods with domain adversarial training. In Zhang et al. (2017), Zhu et al. (2018), Tsai et al. (2018), Chen et al. (2018), Gong et al. (2019), Zheng et al. (2018), authors performed output-space adaptation at feature level by an adversarial module. For object detection, strong-weak alignment (Saito et al. 2019) and local region alignment (Zhu et al. 2019) were applied to tackle the difficulty of domain adaptation. However, much less attention has been paid to domain adaptation for low-level tasks. There are several works investigating the domain-adaptive depth estimation task, including geometry-aware alignment (Zhao et al. 2019), semantic-level consistency (Lopez-Rodriguez and Mikolajczyk 2020) and image-level translation (Zheng et al. 2018; Atapour-Abarghouei and Breckon 2018).

Domain-Adaptive Stereo Matching
Although records on public benchmarks are constantly broken, few attention has been paid to the domain adaptation ability of deep stereo networks. For stereo matching domain adaptation, Pang et al. (2018) proposed a semi-supervised method utilizing the scale information. Guo et al. (2018) presented a cross-domain method using knowledge distillation. MAD-Net (Tonioni et al. 2019) was designed to adapt a compact stereo model online. Recently, StereoGAN (Liu et al. 2020) utilized CycleGAN (Zhu et al. 2017) to bridge domain gaps by joint optimizations of image style transfer and stereo matching. However, no standard and complete domain adaptation pipeline was implemented in these methods, and their adaptation performance is quite limited. Contrarily, we propose a more complete pipeline for deep stereo models following the standard domain adaptation methodology, in which alignments across domains are conducted at multiple representation levels thereby remarkable adaptation performance is achieved. In addition, we do not conduct any adversarial learning hence the training stability and semantic invariance are guaranteed.

Method
In this section, we first describe the problem of domain-adaptive stereo matching. Then we introduce the motivation and give an overview of our domain adaptation pipeline. After that, we detail the main components in the pipeline, i.e. non-adversarial progressive color transfer, cost normalization and self-supervised occlusion-aware reconstruction.

Problem Description
In this paper, we focus on the domain adaptation problem for stereo matching. Differing from domain generalization where a method needs to perform well on unseen scenes, domain adaptation enables methods to use target-domain images without target-domain ground-truths during training. Specifically for stereo matching, since there is a large amount of synthetic data (Mayer et al. 2016) but only a small number of realistic data with ground-truths (Menze and Geiger 2015; Scharstein et al. 2014; SchÃ¶ps et al. 2017), the problem can be further limited to the adaptation from virtual to real-world scenes. Given stereo image pairs (ğ¼ğ‘™ğ‘ ,ğ¼ğ‘Ÿğ‘ ) and (ğ¼ğ‘™ğ‘¡,ğ¼ğ‘Ÿğ‘¡) on source and target domains, and the ground-truth disparity map ğ‘‘ğ‘™ğ‘ ^ on the source domain, we train the model to predict the disparity map ğ‘‘ğ‘™ğ‘¡ on the target domain.

Fig. 2
figure 2
Comparisons of input images, internal representations and output disparities between synthetic and real-world datasets. Leftâ€“right: SceneFlow (Mayer et al. 2016) and Middlebury (Scharstein et al. 2014) examples. Top-down: input image, internal cost volume and output disparity map. Disparity maps of two domains are rendered by the same color map

Full size image
Motivation
As shown in Fig. 2, two images from SceneFlow (Mayer et al. 2016) and Middlebury (Scharstein et al. 2014) datasets are selected to describe inherent inconsistencies between two domains. (i) These two images own observable differences in their color and brightness. Moreover, according to the statistics on whole datasets, the mean values of RGB channels are (107, 102, 92) in SceneFlow and (148, 132, 102) in Middlebury, in which significant color variances are found between synthetic and realistic domains. (ii) For cost volumes computed from the 1D-correlation layer (Mayer et al. 2016), we analyse the distribution of matching cost values. In the second row of Fig. 2, the cost values, ranging from 0 to 30, are divided into 30 bins. We plot the histogram to depict the proportion of matching cost values in each interval and find the distributions between two domains are inconsistent. Specifically, the cost values of SceneFlow scatter between 0âˆ¼25, while the cost values of Middlebury mainly distribute between 0âˆ¼10. Since the cost values are the products of binocular features, the distributions show the differences of binocular features for cost volume construction across domains. (iii) Although the two sampled images have similar plants as foreground, the generated disparity maps still vary in the scene geometries. Both the foreground objects and background screens have quite different disparities. Therefore, we conclude that the inherent differences across domains for stereo matching lie in the image color at input level, cost volume at feature level, and disparity map at output level.

Fig. 3
figure 3
The training diagram of our, AdaStereo, with the adaptation from SceneFlow to Middlebury as an example. Color transfer and self-supervised occlusion-aware reconstruction modules are only adopted during training. (ğ¿ğ‘šğ‘ğ‘–ğ‘›ğ‘ ,ğ¿ğ‘œğ‘ğ‘ğ‘ , ğ¿ğ‘ğ‘Ÿğ‘¡, ğ¿ğ‘œğ‘ğ‘ğ‘¡, ğ¿ğ‘ ğ‘šğ‘¡) are the five training loss terms specified in Eq. (6)

Full size image
Correspondingly, to solve the domain adaptation problem of stereo matching, we propose the progressive color transfer, cost normalization, and self-supervised reconstruction to handle the domain gaps in three levels respectively. The former two strategies are presented to directly narrow the differences in color space and cost volume distribution. The latter reconstruction is appended as an auxiliary task to impose extra supervision on the estimated disparity map for target domain and finally benefits the adaptation ability.

Framework Overview
Figure 3 depicts the training pipeline of our stereo domain adaptation framework, in which three major modules are involved. For input, a randomly selected target-domain pair and a randomly selected source-domain pair which is adapted to target-domain color styles through our progressive color transfer algorithm, are simultaneously fed into a shared-weight disparity network with our cost normalization layer. The source-domain branch is under the supervision of the given ground-truth disparity map, while the target-domain branch is regulated by the proposed auxiliary task: self-supervised occlusion-aware reconstruction.

Non-adversarial Progressive Color Transfer
As mentioned in Sect. 3.2, the color difference plays a major role in the input image-level inconsistency across domains. Hence, we present an effective and stable algorithm for color transfer from source domain to target domain in a non-adversarial manner. During training, given a source-domain image ğ¼ğ‘  and a target-domain image ğ¼ğ‘¡, the algorithm outputs a transferred source-domain image ğ¼ğ‘ â†’ğ‘¡, which preserves the content of ğ¼ğ‘  and owns the target-domain color style. In Algorithm 1, the transfer is performed in the LAB color space. ğ‘‡ğ‘…ğºğµâ†’ğ¿ğ´ğµ(.) and ğ‘‡ğ¿ğ´ğµâ†’ğ‘…ğºğµ(.) denote the color space transformations. Under the LAB space, the mean value ğœ‡ and standard deviation ğœ of each channel are first computed by ğ‘†(â‹…). Then, each channel in the source-domain LAB image ğ¼ğ‘ ~ is subtracted by its mean ğœ‡ğ‘  and multiplied by the standard deviation ratio ğœ†. Finally, the transferred image ğ¼ğ‘ â†’ğ‘¡ is obtained through the addition of the progressively updated ğœ‡ğ‘¡ and color space conversion. In the training pipeline of  AdaStereo, two images in a source-domain pair are simultaneously transferred with the same ğœ‡ğ‘¡ and ğœğ‘¡.

figure a
Compared with Reinhardâ€™s color transfer method (Reinhard et al. 2001), the main contribution of our algorithm is the proposed progressive update scheme that proves to be more beneficial for domain adaptation. Considering color inconsistencies might exist across different images in the same target-domain dataset while the previous method (Reinhard et al. 2001) only enables one-to-one transformation, the transferred source-domain images can not capture the meaningful color styles that are representative for the whole target-domain dataset. The progressive update scheme is proposed to address such problem. To be specific, target-domain ğœ‡ğ‘¡ and ğœğ‘¡ are progressively re-weighted by current inputs (ğœ‡ğ‘¡ğ‘–,ğœğ‘¡ğ‘–) and historical records (ğœ‡ğ‘¡,ğœğ‘¡) with a momentum ğ›¾, simultaneously ensuring the representativeness of target-domain color styles and the diversity of training samples transferring from the source domain to the target domain during training. More evidences of its effectiveness are provided in experiments.

In a larger sense, we are the first to use a non-adversarial style transfer method to align the input-level inconsistency for stereo domain adaptation. Unlike GAN-based style transfer networks (Li et al. 2017; Zhu et al. 2017) that cause side-effects of harmful geometrical distortions for the low-level stereo matching task, our method is more training-efficient meanwhile ensuring the training stability and semantic invariance, which can be easily embedded into the training framework of domain-adaptive stereo matching. Experimental results further validate its superiority over other adversarial transfer methods.

Cost Normalization
Cost volume is one of the most important internal feature-level representation in a deep stereo network, encoding all necessary information for succeeding disparity regression. Hence for domain-adaptive stereo matching, an intuitive way is to directly narrow the deviations in cost volume distributions across different domains. Based on the analyses in Sect. 3.2, the cross-domain differences of cost volumes mainly originate from the binocular features for cost volume construction. Correspondingly, we design a cost normalization layer to regularize the left and right features, which is compatible with all cost volume building patterns (e.g. correlation or concatenation) in stereo matching, as shown in Fig. 3.

Before cost volume construction, the left lower-layer feature îˆ²îˆ¸ and right feature îˆ²îˆ¾ with the same size of ğ‘Ã—ğ¶Ã—ğ»Ã—ğ‘Š (N: batch size, C: channel, H: spatial height, W: spatial width), are both successively regularized by two proposed normalization operations: channel normalization and pixel normalization. Specifically, the channel normalization is applied across all spatial dimensions (H, W) per channel per sample individually, which is defined as:

îˆ²ğ‘›,ğ‘,â„,ğ‘¤=îˆ²ğ‘›,ğ‘,â„,ğ‘¤âˆ‘ğ»âˆ’1â„=0âˆ‘ğ‘Šâˆ’1ğ‘¤=0||îˆ²ğ‘›,ğ‘,â„,ğ‘¤||2+ğœ€â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš
(1)
where îˆ² denotes the lower-layer feature, h and w denote the spatial position, c denotes the channel, and n denotes the batch index. After the channel normalization, the pixel normalization is further applied across all channels per spatial position per sample individually, which is defined as:

îˆ²ğ‘›,ğ‘,â„,ğ‘¤=îˆ²ğ‘›,ğ‘,â„,ğ‘¤âˆ‘ğ¶âˆ’1ğ‘=0||îˆ²ğ‘›,ğ‘,â„,ğ‘¤||2+ğœ€â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš
(2)
Through channel normalization which reduces the inconsistency in norm and scaling of each feature channel, and pixel normalization which further regulates the norm distributions of pixel-wise feature vectors for binocular matching, inter-domain gaps in generated matching costs due to varied image contents and geometries are greatly reduced.

In a nutshell, our parameter-free cost normalization layer is indeed a normalization layer designed specifically for stereo domain adaptation, which is adopted only once before cost volume construction. On the contrary, previous normalization layers (e.g. BIN (Nam and Kim 2018), IN (Ulyanov et al. 2016), CN (Dai and Heckel 2019) (exactly equivalent to IN) and DN (Zhang et al. 2020)) are general normalization approaches, which contain learnable parameters and are repeatedly adopted in the networkâ€™s feature extractor. Hence regulations on cost volume from those general normalization layers are not direct and effective enough, requiring extra implicit learning process. Moreover, our cost normalization layer does not use zero-centralization to prevent extra disturbances in matching cost distributions. Experiments further validate its superiority over general normalization layers.

Self-supervised Occlusion-Aware Reconstruction
Self-supervised auxiliary tasks are demonstrated to be beneficial for aligning domains in high-level tasks (Gidaris et al. 2018; Xu et al. 2019). However, such problem has not been explored for the low-level stereo matching task. Hence in this sub-section, we propose an effective auxiliary task for stereo domain adaptation: self-supervised occlusion-aware reconstruction. As shown in Fig. 3, a self-supervised module is attached upon the main disparity network to perform image reconstructions on the target domain. To address the ill-posed occlusion problem in reconstruction, we design a domain-collaborative occlusion mask learning scheme. Through occlusion-aware stereo reconstruction, more informative geometries from target-domain scenes are involved in training.

During the self-supervised learning, stereo reconstruction is firstly measured by the difference between the input target-domain left image ğ¼ğ‘™ğ‘¡ and the corresponding warped image ğ¼ğ‘™ğ‘¡â¯â¯â¯â¯â¯ (based on the right image ğ¼ğ‘Ÿğ‘¡ and the produced disparity map ğ‘‘ğ‘™ğ‘¡). Then a small fully-convolutional occlusion prediction network takes the concatenation of ğ‘‘ğ‘™ğ‘¡, ğ¼ğ‘Ÿğ‘¡ and the pixel-wise error map ğ‘’ğ‘™ğ‘¡=|ğ¼ğ‘™ğ‘¡âˆ’ğ¼ğ‘™ğ‘¡â¯â¯â¯â¯â¯| as input and produces a pixel-wise occlusion mask ğ‘‚ğ‘™ğ‘¡ whose element denotes per-pixel occlusion probability from 0 to 1. Next, the reconstruction loss ğ¿ğ‘ğ‘Ÿğ‘¡ is re-weighted by the occlusion mask ğ‘‚ğ‘™ğ‘¡ and error map ğ‘’ğ‘™ğ‘¡ on each pixel, as shown in Fig. 4. Furthermore, we introduce the disparity smoothness loss (ğ¿ğ‘ ğ‘šğ‘¡) to avoid possible artifacts. To guide the occlusion mask learning on the target domain more effectively, as shown in Fig. 3, the weight-sharing occlusion prediction network simultaneously learns an occlusion mask ğ‘‚ğ‘™ğ‘  on the source domain under the supervision of the ground-truth occlusion mask ğ‘‚ğ‘™ğ‘ ^ generated from the ground-truth disparity map ğ‘‘ğ‘™ğ‘ ^. Compared with the widely adopted consistency check approaches (e.g. leftâ€“right consistency check (Godard et al. 2017), per-pixel minimum reprojection (Godard et al. 2019)), our domain-collaborative occlusion mask learning scheme is more training-efficient, without the need of extra views of image and disparity map.

Fig. 4
figure 4
The training details of the self-supervised occlusion-aware reconstruction module on the target domain

Full size image
As mentioned above, the occlusion prediction network simultaneously conducts occlusion mask learning on the source domain under the supervision of ground-truth occlusion mask ğ‘‚ğ‘™ğ‘ ^, which however is not provided in the source-domain dataset. Here we specify how to obtain ğ‘‚ğ‘™ğ‘ ^ given a source-domain ground-truth disparity map ğ‘‘ğ‘™ğ‘ ^. Based on the binocular geometry, per-pixel occlusion judgment is defined as:

ğ‘‚ğ‘™ğ‘ ^(ğ‘¥,ğ‘¦)={1âˆƒ ğ‘¥2>ğ‘¥, ğ‘¥2âˆ’ğ‘‘ğ‘™ğ‘ ^(ğ‘¥2,ğ‘¦)=ğ‘¥âˆ’ğ‘‘ğ‘™ğ‘ ^(ğ‘¥,ğ‘¦)0ğ‘’ğ‘™ğ‘ ğ‘’
(3)
where ğ‘‚ğ‘™ğ‘ ^(ğ‘¥,ğ‘¦)=1 or 0 denotes if the pixel (x, y) is occluded or not, and ğ‘‘ğ‘™ğ‘ ^(ğ‘¥,ğ‘¦) denotes the source-domain ground-truth disparity at (x, y).

Our self-supervised occlusion-aware reconstruction task is the first proposed auxiliary task for stereo domain adaptation. In addition, our method enables collaborative occlusion mask learning on both source and target domains, which acts like another domain adaptation sub-task on occlusion prediction that ensures the quality of target-domain occlusion masks, thereby explicitly improving the validity of reconstruction loss. Experimental results further validate the superiority of our proposed auxiliary task over other high-level auxiliary tasks.

Training Loss
On the source domain, we train the main task of disparity regression using the per-pixel smooth-L1 loss (as same in (Chang and Chen 2018)): ğ¿ğ‘šğ‘ğ‘–ğ‘›ğ‘ =ğ‘†ğ‘šğ‘œğ‘œğ‘¡â„ğ¿1(ğ‘‘ğ‘™ğ‘ âˆ’ğ‘‘ğ‘™ğ‘ ^). In addition, the per-pixel binary cross entropy loss is adopted for occlusion mask training on the source domain: ğ¿ğ‘œğ‘ğ‘ğ‘ =ğµğ¶ğ¸(ğ‘‚ğ‘™ğ‘ ,ğ‘‚ğ‘™ğ‘ ^).

On the target domain, the occlusion-aware appearance reconstruction loss is defined as:

ğ¿ğ‘ğ‘Ÿğ‘¡=ğ›¼1âˆ’ğ‘†ğ‘†ğ¼ğ‘€(ğ¼ğ‘™ğ‘¡âŠ™(1âˆ’ğ‘‚ğ‘™ğ‘¡),ğ¼ğ‘™ğ‘¡â¯â¯â¯â¯â¯âŠ™(1âˆ’ğ‘‚ğ‘™ğ‘¡))2+(1âˆ’ğ›¼)||ğ¼ğ‘™ğ‘¡âŠ™(1âˆ’ğ‘‚ğ‘™ğ‘¡)âˆ’ğ¼ğ‘™ğ‘¡â¯â¯â¯â¯â¯âŠ™(1âˆ’ğ‘‚ğ‘™ğ‘¡)||1
(4)
where âŠ™ denotes element-wise multiplication, SSIM denotes a simplified single scale SSIM (Wang et al. 2004) term with a 3Ã—3 block fiter, and ğ›¼ is set to 0.85. Besides, we apply a L1-regularization term on the produced target-domain occlusion mask: ğ¿ğ‘œğ‘ğ‘ğ‘¡=||ğ‘‚ğ‘™ğ‘¡||1. Last but not least, we adopt an edge-aware term as the target-domain disparity smoothness loss, where âˆ‚ğ¼ and âˆ‚ğ‘‘ denote image and disparity gradients:

ğ¿ğ‘ ğ‘šğ‘¡=|âˆ‚ğ‘¥ğ‘‘ğ‘™ğ‘¡|ğ‘’âˆ’|âˆ‚ğ‘¥ğ¼ğ‘™ğ‘¡|+|âˆ‚ğ‘¦ğ‘‘ğ‘™ğ‘¡|ğ‘’âˆ’|âˆ‚ğ‘¦ğ¼ğ‘™ğ‘¡|
(5)
Finally, the total training loss is a weighted sum of five loss terms above, where ğœ†ğ‘œğ‘ğ‘ğ‘ , ğœ†ğ‘ğ‘Ÿğ‘¡, ğœ†ğ‘œğ‘ğ‘ğ‘¡ and ğœ†ğ‘ ğ‘šğ‘¡ denote corresponding loss weights:

ğ¿=ğ¿ğ‘šğ‘ğ‘–ğ‘›ğ‘ +ğœ†ğ‘œğ‘ğ‘ğ‘ ğ¿ğ‘œğ‘ğ‘ğ‘ +ğœ†ğ‘ğ‘Ÿğ‘¡ğ¿ğ‘ğ‘Ÿğ‘¡+ğœ†ğ‘œğ‘ğ‘ğ‘¡ğ¿ğ‘œğ‘ğ‘ğ‘¡+ğœ†ğ‘ ğ‘šğ‘¡ğ¿ğ‘ ğ‘šğ‘¡
(6)
Table 1 Ablation studies on the KITTI, Middleburry, ETH3D and DrivingStereo training sets
Full size table
Experiment
To prove the effectiveness of our method, we extend the 2-D stereo baseline network ResNetCorr (Yang et al. 2018) as Ada-ResNetCorr and the 3-D stereo baseline network PSMNet (Chang and Chen 2018) as Ada-PSMNet. We first conduct intensive ablation studies and detailed break-down comparisons for each proposed module on multiple datasets, including KITTI (Geiger et al. 2012; Menze and Geiger 2015), Middlebury (Scharstein et al. 2014), ETH3D (SchÃ¶ps et al. 2017) and DrivingStereo (Yang et al. 2019). Next, we compare the cross-domain performance of our method with other traditional / domain generalization/domain adaptation stereo methods. Then, we show that our domain-adaptive models achieve remarkable performance on four public stereo matching benchmarks, even outperforming several end-to-end disparity networks finetuned on target domains. Finally, we present additional verification experiments and applications of our proposed domain-adaptive stereo matching pipeline.

Datasets
SceneFlow dataset (Mayer et al. 2016) is a large synthetic dataset containing 35k training pairs with dense ground-truth disparities, mainly acting as the source-domain training set. KITTI dataset includes two subsets: KITTI 2012 (Geiger et al. 2012) and KITTI 2015 (Menze and Geiger 2015), each providing about 200 stereo pairs of outdoor driving scenes with sparse ground-truth disparities for training and about 200 image pairs for testing. Middlebury dataset (Scharstein et al. 2014) is a small indoor dataset containing 15 pairs for training and 15 pairs for testing. Besides, we collect 57 unused pairs from 5 subsets (Middlebury 2001, 2003, 2005, 2006 and 2014), which are fused with all training pairs for unsupervised domain adaptation training. ETH3D dataset (SchÃ¶ps et al. 2017) includes both indoor and outdoor scenarios, containing 27 gray image pairs with dense ground-truth disparities for training and 20 image pairs for testing. DrivingStereo dataset (Yang et al. 2019) is a large-scale stereo matching dataset covering a diverse set of driving scenarios, containing 175K stereo pairs for training and 7751 pairs for testing. These four real-world datasets mainly act as different target domains for domain adaptation evaluations.

We adopt the bad pixel error (D1-error) as the evaluation metric, which calculates the percentage of pixels whose disparity errors are greater than a certain threshold (1-pixel error for ETH3D, 2-pixel error for Middlebury, 3-pixel error for KITTI and DrivingStereo).

Fig. 5
figure 5
Qualitative results of color transfer from synthetic (SceneFlow) to real-world scenes. Top-down: transfer to KITTI, Middlebury and ETH3D. As can be seen, our non-adversarial color transfer method enables highly effective image style translations, and ensures semantic invariance without any geometrical distortion, which is of vital importance for the low-level stereo matching task

Full size image
Implementation Details
Each model is trained end-to-end using Adam optimizer (ğ›½1=0.9, ğ›½2=0.999) on eight NVIDIA Tesla-V100 GPUs. The learning rate is set to 0.001 for training from scratch, and we train each model for 100 epochs with the batch size of 16 using 624Ã—304 random crops. Referring to Table 2, the momentum factor ğ›¾ in Algorithm 1 is set to 0.95. Referring to Table 5, the weights of different loss terms (ğœ†ğ‘œğ‘ğ‘ğ‘ , ğœ†ğ‘ğ‘Ÿğ‘¡, ğœ†ğ‘œğ‘ğ‘ğ‘¡, ğœ†ğ‘ ğ‘šğ‘¡) in Eq. (6) are set to (0.2, 1.0, 0.2, 0.1). Considering some target-domain training sets are small, we apply several common data augmentation operations on target-domain training pairs only for the self-supervised occlusion-aware reconstruction (not for color transfer), including color shift, saturation, contrast adjustments and style-PCA based lighting noises.

Next, we introduce the network structures of our domain-adaptive stereo models: Ada-ResNetCorr and Ada-PSMNet. Ada-ResNetCorr is extended based on the ResNetCorr (Yang et al. 2018), which is a baseline model among correlation-based 2-D disparity networks. We extend the ResNetCorr, where â€œconv1_1â€ to â€œconv1_3â€ in the ResNet-50 (He et al. 2016) are adopted as the shallow feature extractor. We also replace the single-channel output convolutional layer with a soft-argmin block in 3-D stereo networks (Kendall et al. 2017; Chang and Chen 2018) for disparity regression. The proposed cost normalization layer is directly applied on the extracted lower-layer features of two views before correlation, which are of 1/2 spatial size to the input image. The maximum displacement in the correlation layer is set to 128. Ada-PSMNet is extended based on the PSMNet (Chang and Chen 2018), which is a baseline model among cost-volume based 3-D disparity networks. We follow the structure of PSMNet except the maximum disparity range is set to 256. The cost normalization layer is directly applied on the extracted lower-layer features of two views before constructing the 4-D cost volume.

Then, we detail the structure of our occlusion prediction network (only adopted in training), which is a small fully-convolutional network. The first and second convolutional layers both use the 3Ã—3 kernel (stride=1 with 32 output channels), followed by a batch normalization layer and a ReLU layer each. The last convolutional layer uses the 1Ã—1 kernel (stride=1 with a single output channel), followed by a sigmoid layer, producing a full-size occlusion mask whose element denotes per-pixel occlusion probability from 0 to 1.

Finally, we reiterate our training and testing protocols. An individual domain-adaptive stereo model is trained for each target domain, and we do not use any images from the target-domain test set during training. For fair comparisons, all experiments are conducted rigorously following the same unsupervised domain adaptation settings: training on the source domain also with target-domain images (no target-domain labels), then directly testing on the target domain. Due to the number of submissions to online test servers is strictly limited, we conduct ablation studies and tune hyperparameters on various target-domain training sets.

Ablation Studies
Basic Ablation Studies
In Table 1, we conduct detailed ablation studies on four real-world datasets to evaluate the key components in our domain adaptation pipeline, based on Ada-PSMNet and Ada-ResNetCorr. As can be seen, applying our non-adversarial progressive color transfer method in training can significantly reduce error rates on multiple target domains, e.g. 8.3% on KITTI, 8.6% on Middlebury, 4.7% on ETH3D and 13.5% on DrivingStereo for Ada-PSMNet, benefiting from massive color-aligned training images without geometrical distortions. We also provide qualitative results of our color transfer method in Fig. 5. In addition, compared with baseline models, the error rates are reduced by 1%âˆ¼4% on target domains by integrating the proposed cost normalization layer, which also works well when implemented together with the input color transfer module. Furthermore, adopting the self-supervised occlusion-aware reconstruction can further reduce error rates by 0.5%âˆ¼1.5% on various target domains, despite the adaptation performance is already remarkable after color transfer and cost normalization. Finally, both Ada-PSMNet and Ada-ResNetCorr outperform the corresponding baseline model by notable margins on all target domains, especially an accuracy improvement of 15.8% by Ada-PSMNet on the large-scale DrivingStereo training set.

Table 2 Sensitivity tests of the momentum factor ğ›¾ in our progressive color transfer algorithm on the KITTI and DrivingStereo training sets
Full size table
Table 3 Supplementary ablation studies for cost normalization on the KITTI and DrivingStereo training sets
Full size table
For further demonstrations of effectiveness inside each proposed module, we provide several detailed ablation studies. In Table 2, we conduct the sensitivity test of the momentum factor ğ›¾ in our progressive color transfer algorithm. According to Line 6-7 in Algorithm 1, ğ›¾=1 denotes no progressive update scheme in color transfer, while the error rates on target domains begin to fall with the decrease of ğ›¾. When ğ›¾ is set to 0.95, the best performance on target domains is achieved, indicating that the progressive update scheme simultaneously ensures the diversity and representativeness of target-domain color styles during style transfer, thereby benefiting the stereo domain adaptation. Besides, our progressive color transfer algorithm shows good robustness to the momentum factor ğ›¾. In Table 3, we conduct detailed ablation studies for two proposed normalization operations in the cost normalization layer. Applying the channel normalization or pixel normalization alone, can reduce the error rate on the KITTI training set by 1.1% or 1.6% respectively, indicating that the pixel normalization is more important to some degree. When combining together, the performance gain is greater than the sum of two individual gains, demonstrating that these two operations are complementary. In Table 4, we conduct detailed ablation studies for different loss terms in self-supervised occlusion-aware reconstruction. When progressively applying the reconstruction loss (ğ¿ğ‘ğ‘Ÿğ‘¡), occlusion regularization loss (ğ¿ğ‘œğ‘ğ‘ğ‘  & ğ¿ğ‘œğ‘ğ‘ğ‘¡) and disparity smoothness loss (ğ¿ğ‘ ğ‘šğ‘¡), the error rates on the target-domain DrivingStereo training set are reduced by 0.8%, 0.4% and 0.1%. Besides, the domain-collaborative occlusion mask learning is also important, manifesting as an increase of 0.3% in D1-error if the source-domain occlusion mask training loss ğ¿ğ‘œğ‘ğ‘ğ‘  is not adopted. In Table 5, we tune different combinations of training loss weights on the KITTI training set. As can be seen, our domain-adaptive training pipeline is robust to these hyperparameters.

Table 4 Supplementary ablation studies for different loss terms in self-supervised occlusion-aware reconstruction on the KITTI and DrivingStereo training sets
Full size table
Table 5 Ablation studies for training loss weights on the KITTI training set
Full size table
Table 6 Comparisons with existing normalization layers on the KITTI and DrivingStereo training sets
Full size table
Break-Down Comparisons of Each Module
In order to further demonstrate the superiority of each module for domain-adaptive stereo matching, we perform exhaustive comparisons with other alternative methods respectively. As shown in Table 6, our specifically designed cost normalization layer which is parameter-free and adopted only once in the network, outperforms other general and learnable normalization layers (AdaBN (Li et al. 2016), BIN (Nam and Kim 2018), IN (Ulyanov et al. 2016) and DN (Zhang et al. 2020)) which are repeatedly adopted in the networkâ€™s feature extractor. In Table 7, our progressive color transfer algorithm far outperforms four popular color/style transfer networks (WCT2 (Yoo et al. 2019), WaterGAN (Li et al. 2017), CycleGAN (Zhu et al. 2017) and StereoGAN (Liu et al. 2020)), indicating that geometrical distortions from such GAN-based color/style transfer models are harmful for the low-level stereo matching task. Moreover, our method outperforms the Reinhardâ€™s color transfer method (Reinhard et al. 2001) by about 1% in D1-error, revealing the effectiveness of the proposed progressive update scheme. In Table 8, our proposed self-supervised occlusion-aware reconstruction task is demonstrated to be a highly effective auxiliary task for domain-adaptive stereo matching, while other alternatives all hurt the domain adaptation performance.

Table 7 Comparisons with other color/style transfer methods on the KITTI and DrivingStereo training sets
Full size table
Table 8 Comparisons with other auxiliary tasks for stereo domain adaptation on the KITTI and DrivingStereo training sets
Full size table
Table 9 Ablation studies for different amounts of target-domain stereo pairs used in training, on the KITTI, Middlebury (half) and DrivingStereo training sets
Full size table
Robustness to the Amount of Target-Domain Training Samples
In Table 9, we conduct detailed ablation studies for different amounts of target-domain images used in training, to validate the robustness of our domain adaptation pipeline to the amount of target-domain training samples. The experiments are conducted on three target-domain training sets with different scales: DrivingStereo (175k pairs), KITTI (200 pairs) and Middlebury (72 pairs). For the large-scale DrivingStereo dataset, even if we only use one fifth of the training samples for unsupervised domain adaptation training, a remarkable D1-error of 5.4% is achieved on the whole training set, which is only inferior to that of using all samples for training by 0.3%. The performance gap is further narrowed to 0.1% when a half of the training samples are used. For small target-domain datasets such as KITTI and Middlebury, using only half of the training samples can reduce error rates on corresponding training sets by 7.5% and 8.7% respectively. When the sampling ratio increases to 3/4, the performance gaps to using all samples for training are narrowed to 0.3% and 0.6% respectively. To sum up, our proposed domain adaptation pipeline is robust to varied capacities of target-domain datasets.

Table 10 Cross-domain comparisons with other traditional/domain generalization/domain adaptation stereo methods on the KITTI/Middlebury/ETH3D training sets, and DrivingStereo test set
Full size table
Cross-Domain Comparisons
Fig. 6
figure 6
Disparity predictions from our SceneFlow-pretrained Ada-PSMNet on the KITTI, ETH3D, Middlebury and DrivingStereo datasets. Leftâ€“right: left stereo image, colorized disparity and error maps

Full size image
In Table 10, we compare our proposed domain-adaptive stereo models with other traditional stereo methods, domain generalization, and domain adaptation stereo networks on four real-world datasets for cross-domain comparisons.

Both Ada-ResNetCorr and Ada-PSMNet show great superiority over traditional methods on all target domains.

For comparisons with domain generalization networks, unfairness may exist since our domain-adaptive models use target-domain images during training. It is caused by the problem definition of Domain Adaptation as mentioned in Sect. 3.1. However, as shown in Table 10, our Ada-PSMNet achieves tremendous gains rather than small deltas compared with all domain generalization stereo networks, including the state-of-the-art DSMNet (Zhang et al. 2020) and its baseline network GANet (Zhang et al. 2019). For more comparisons with DSMNet (Zhang et al. 2020), Zhang et al. reported an error rate of 8.5% on the KITTI 2015 training set by applying their proposed domain normalization layer and non-local filter layers on the PSMNet, while our Ada-PSMNet achieves an error rate of 3.5% using the same baseline network.

For more â€œfairerâ€ comparisons with domain generalization networks on unseen target-domain scenes, we conduct further experiments on the DrivingStereo test set. As shown in Table 10, our domain-adaptive stereo models show significant superiority over other SceneFlow-pretrained models, as well as the classical method SGM.

Among few published domain adaptation networks, only StereoGAN (Liu et al. 2020) reported such cross-domain performance, while Ada-PSMNet achieves a 3.5 times lower error rate than StereoGAN (Liu et al. 2020) on the KITTI training set.

Hence, our proposed multi-level alignment pipeline successfully address the domain adaptation problem for stereo matching. In Fig. 6, we provide qualitative results of our method on different real-world datasets, in which accurate disparity maps are predicted on outdoor and indoor scenes.

Table 11 Performance on the ETH3D online test set
Full size table
Table 12 Performance on the Middlebury online test set
Full size table
Table 13 Performance on the KITTI 2015 online test set
Full size table
Table 14 Performance on the DrivingStereo test set
Full size table
Evaluations on Public Stereo Benchmarks
To further demonstrate the effectiveness of our method, we compare our domain-adaptive model Ada-PSMNet with several unsupervised/self-supervised methods and finetuned disparity networks on public stereo matching benchmarks: KITTI, Middlebury, ETH3D and DrivingStereo. We directly upload the results from our SceneFlow-pretrained model to the online benchmark and do not finetune using target-domain ground-truths before submitting test results.

Results on the ETH3D Benchmark
As can be seen in Table 11, our proposed Ada-PSMNet outperforms the state-of-the-art patch-based network DeepPruner (Duggal et al. 2019) and end-to-end disparity networks (iResNet (Liang et al. 2018), PSMNet (Chang and Chen 2018) and StereoDRNet (Chabra et al. 2019)) finetuned with ground-truth disparities in the ETH3D training set, as well as the SOTA traditional method SGM-Forest (Schonberger et al. 2018). By the time of the paper submission, AdaStereo ranks 1ğ‘ ğ‘¡ on the ETH3D benchmark in all evaluation metrics among published stereo matching methods.

Results on the Middlebury Benchmark
As can be seen in Table 12, the synthetic-data pretrained Ada-PSMNet outperforms all other state-of-the-art end-to-end disparity networks (EdgeStereo (Song et al. 2020), CasStereo (Gu et al. 2020), iResNet (Liang et al. 2018), MCV-MFC (Liang et al. 2019) and PSMNet (Chang and Chen 2018)) which are finetuned using ground-truth disparities in the Middlebury training set by a noteworthy margin. Our Ada-PSMNet achieves a remarkable 2-pixel error rate of 13.7% on the full-resolution test set, outperforming all other finetuned end-to-end stereo matching networks on the benchmark.

Results on the KITTI Benchmark
As can be seen in Table 13, our domain-adaptive model far outperforms the online-adaptive model MADNet (Tonioni et al. 2019), the weak-supervised (Tulyakov et al. 2017) and unsupervised (Zhou et al. 2017) methods, meanwhile achieving higher accuracy than some supervised disparity networks including MC-CNN-acrt (Zbontar and LeCun 2015), L-ResMatch (Shaked and Wolf 2017), DispNetC (Mayer et al. 2016) and SGM-Net (Seki and Pollefeys 2017). Moreover, the SceneFlow-pretrained Ada-PSMNet achieves comparable performance with the KITTI-finetuned GC-Net (Kendall et al. 2017).

Table 15 â€œSemi-supervisedâ€ performance on the DrivingStereo test set
Full size table
Table 16 More verification on top of advanced 3-D stereo networks
Full size table
Results on the DrivingStereo Test Set
As can be seen in Table 14, the SceneFlow-pretrained Ada-PSMNet far outperforms the traditional stereo method SGM (Hirschmuller 2008), meanwhile achieving higher accuracy than some end-to-end disparity networks including SegStereo (Yang et al. 2018), CRL (Pang et al. 2017), ResNetCorr (Yang et al. 2018), GuideNet (Yang et al. 2019) and DispNetC (Mayer et al. 2016), which are all trained on the large-scale DrivingStereo training set with ground-truth disparity maps. Moreover, the SceneFlow-pretrained Ada-PSMNet achieves comparable performance with the DrivingStereo-trained iResNet (Liang et al. 2018).

Fig. 7
figure 7
Distance-aware ARD curve and semantic-aware radar map of the SceneFlow-pretrained PSMNet and Ada-PSMNet on the KITTI 2015 training set

Full size image
More Verification and Novel Applications
Semi-supervised Applications
We conduct additional â€œsemi-supervisedâ€ experiments, to further validate the effectiveness of our domain adaptation method, when a small faction of target-domain ground-truth disparity maps are available for finetuning. Under this circumstance, target-domain test sets are more suitable for evaluations. Considering the DrivingStereo dataset is the only public stereo matching dataset with available test set labels, we choose DrivingStereo as the target domain in this part of â€œsemi-supervisedâ€ experiments. Specifically, we randomly select 500 labeled pairs from the DrivingStereo training set (about 0.3% of the total training set) to finetune our domain-adaptive models, then report results on the DrivingStereo test set. As can be seen in Table 15, finetuning our domain-adaptive models with only 500 target-domain labeled pairs, achieves the performance on par with training from scratch on the whole target-domain training set (175K pairs) for 100 epochs, which is also significantly better than finetuning the SceneFlow-pretrained models with the same amount of target-domain labeled pairs.

Verification on More Advanced Models
In order to further demonstrate the effectiveness of our domain adaptation pipeline, we choose another two advanced 3-D stereo networks, i.e. GWCNet (Guo et al. 2019) and GANet (Zhang et al. 2019), then extend them as Ada-GWCNet and Ada-GANet respectively. As shown in Table 16, all of our proposed modules work effectively on top of these more advanced disparity networks. As can be seen, Ada-GWCNet achieves slightly worse cross-domain performance than Ada-PSMNet, while Ada-GANet performs slightly better on two target domains.

Table 17 Distance-aware ARD (Yang et al. 2019) comparisons between Ada-PSMNet and the SceneFlow-pretrained PSMNet (baseline) on the KITTI 2015 training set
Full size table
Table 18 Comparisons between different source-domain datasets (synthetic SceneFlow, real-world DrivingStereo)
Full size table
Table 19 Quick adaptation performance on the KITTI and DrivingStereo training sets
Full size table
New Evaluation Metrics
Along with the DrivingStereo dataset (Yang et al. 2019), two new metrics are introduced to evaluate the performance of a stereo matching method in driving scenarios: the distance-aware absolute relative difference (ARD) metric, and the semantic-aware matching rate (MR) metric. Specifically, the ğ´ğ‘…ğ·ğ‘˜ is defined to measure the deviation between disparity prediction ğ‘‘ğ‘ and ground-truth ğ‘‘ğ‘” in its measurement range as [ğ‘˜âˆ’ğ‘Ÿ,ğ‘˜+ğ‘Ÿ], where k denotes the depth, and r denotes the scope. We can draw the ARD curve by linking the ğ´ğ‘…ğ·ğ‘˜ at different ranges. Moreover, the global difference (GD) of ARD curve is estimated to balance samples among different ranges. The MR metric is defined to compute the matching accuracy on various categories, including vehicle, human, ground, construction, nature, and others. Here, we adopt these two metrics to compare our Ada-PSMNet with the SceneFlow-pretrained PSMNet (baseline) on the KITTI 2015 training set, in which depth and semantic labels are provided. For convenience, we draw the ARD curve and MR radar map to observe the detailed results. As shown in Fig. 7, the ARD curve of Ada-PSMNet is much lower and smoother than that of the SceneFlow-pretrained baseline, indicating better disparity predictions in all ranges from 0m to 80m after domain adaptation. Moreover, it can be found from the radar map that our Ada-PSMNet substantially outperforms the baseline on all categories of ground, especially vehicle, human, construction, and others. In addition, as shown in Table 17, the GD of the SceneFlow-pretrained baseline is 20.1%, while Ada-PSMNet achieves a GD of 6.3%. Hence, the effectiveness and superiority of our domain-adaptive stereo matching pipeline are further demonstrated.

Real-World Source Domain
To further demonstrate the effectiveness and generalization capability of our proposed domain-adaptive stereo matching pipeline, we adopt the real-world DrivingStereo dataset as the source-domain dataset which is large enough for training, then test the domain adaptation performance of Ada-ResNetCorr and Ada-PSMNet on the other three real-world datasets (KITTI, Middleburry and ETH3D). For fair comparisons with domain-adaptive models trained on the SceneFlow dataset, we only use 35k training pairs (as many as SceneFlow) uniformly sampled from the DrivingStereo training set, meanwhile other training settings remain unchanged. As can be seen in Table 18, the error rates of two domain-adaptive models on the KITTI and ETH3D training sets are all reduced, since the outdoor driving scenarios in DrivingStereo are more consistent with these two target domains than SceneFlow. However, the error rates on the Middlebury training set get slightly worse, because of the inconsistent scenarios between DrivingStereo (outdoor) and Middleburry (indoor). To sum up, our method is robust to the choice of source-domain dataset. When there are plenty of real-world stereo pairs with ground-truth disparity annotations, that serve as the source-domain training samples for real-world deployments of domain-adaptive stereo matching systems, reliable disparity predictions can be easily obtained through our domain adaptation pipeline.

Quick Adaptations
Although our method achieves remarkable domain adaptation performance, each domain-adaptive model needs training from scratch for 100 epochs for each target domain. Hence we conduct additional experiments to verify whether our domain adaptation pipeline can be used for quick adaptations: finetuning a synthetic-data pretrained model through our domain adaptation pipeline on the target domain for a few epochs. In Table 19, we compare quick adaptations in which the SceneFlow-pretrained PSMNet is finetuned for 10 or 20 epochs on the target domain without ground-truths, with training a domain-adaptive model from scratch for 100 epochs. After adaptation with only 20 epochs, the error rates on two target domains are significantly reduced (6.6% on KITTI, 13.1% on DrivingStereo), which however, are still slightly inferior to those of training from scratch. To sum up, our proposed domain-adaptive stereo matching pipeline can be easily integrated into quick adaptation application scenarios and real-world deployments, in which reasonable domain adaptation performance can be achieved with only 10â€“20% training costs.

Conclusions and Future Work
In this paper, we address the domain adaptation problem of current deep stereo matching networks. Following the standard domain adaptation methodology, we present a novel domain-adaptive approach called AdaStereo to promote the all-round domain adaptation ability in the stereo matching pipeline. Specifically, three modules are proposed to bridge the domain gaps at different levels: (i) a non-adversarial progressive color transfer algorithm for input-level alignment; (ii) a parameter-free cost normalization layer for internal feature-level alignment; (iii) a highly related self-supervised auxiliary task for output-space alignment. The intensive ablation studies and break-down comparisons on multiple target-domain datasets validate the effectiveness of each module. Benefited from the proposed methodology of multi-level alignments, our domain-adaptive stereo matching pipeline achieves state-of-the-art cross-domain performance on various target domains, including both indoor and outdoor scenarios. Moreover, our synthetic-data pretrained domain-adaptive models achieve remarkable accuracy on four public stereo matching benchmarks even without finetuning. In the future, we will further develop our AdaStereo to simultaneously adapt to multiple target domains. Besides, we will refine the proposed domain-adaptive pipeline for real-time online adaptations, in which target-domain image pairs are unavailable for training.

