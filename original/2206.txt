Person detection and Re-identification are two well-defined support tasks for practically relevant tasks such as Person Search and Multiple Person Tracking. Person Search aims to find and locate all instances with the same identity as the query person in a set of panoramic gallery images. Similarly, Multiple Person Tracking, especially when using the tracking-by-detection pipeline, requires to detect and associate all appeared persons in consecutive video frames. One major challenge shared by the two tasks comes from the contradictory goals of detection and re-identification, i.e, person detection focuses on finding the commonness of all persons while person re-ID handles the differences among multiple identities. Therefore, it is crucial to reconcile the relationship between the two support tasks in a joint model. To this end, we present a novel approach called Norm-Aware Embedding to disentangle the person embedding into norm and angle for detection and re-ID respectively, allowing for both effective and efficient multi-task training. We further extend the proposal-level person embedding to pixel-level, whose discrimination ability is less affected by misalignment. Our Norm-Aware Embedding achieves remarkable performance on both person search and multiple person tracking benchmarks, with the merit of being easy to train and resource-friendly.

Access provided by University of Auckland Library

Introduction
In visual surveillance systems, Person Search and Tracking are two practically relevant tasks, where Person Search aims to find a query person across a set of gallery images with different views, and Multiple Person Tracking requires to link all instances for each identity through consecutive video frames. Both tasks have to tackle two problems: (1) how to locate persons in images, and (2) how to determine if the candidate persons share the same identity with the target person to be searched or tracked. The above two problems are usually investigated as two independent tasks of Person Detection and Person Re-identification (re-ID). However, in practical applications, it is favorable to solve them in a joint framework, not only for convenience and high efficiency, but also for better performance. Therefore, in this work we focus on jointly solving person detection and re-ID in an efficient way, which is then applied to person search and tracking.

A conventional way to address person detection and re-ID is to use a two-step strategy, i.e, cascading a pedestrian detector and a re-identifier trained separately (e.g Zheng et al. (2017); Chen et al. (2018); Chen et al. (2020); Lan et al. (2018); Han et al. (2019)). All candidate persons are cropped from the gallery images according to the detector, and then fed into a standard person re-ID model. The major drawback of these methods is high computational costs. To improve efficiency, others propose to solve the two tasks in a coherent framework by sharing the feature extraction network between detection and re-ID  Xiao et al. (2017, 2017); Liu et al. (2017); Chang et al. (2018); Munjal et al. (2019); Yan et al. (2019); Wang et al. (2019); Lu et al. (2020). These works extend a person detector by stacking an additional fully-connected layer to produce ùêø2 normalized embeddings and the whole model is optimized jointly with standard detection losses and an identity classification loss. Nonetheless, they suffer from the contradictory objectives of detection and re-ID during training, as pointed out by Chen et al. (2018). An intuitive illustration for ùêø2 normalized embeddings is shown in Fig. 1a. The detection classification objective tends to squeeze the embedding space for all persons regardless of the identities, so as to better separate them from background. Therefore, sharing the feature space with background instances limits the angular margin between different identities. As a consequence, due to the inherent conflict between the two tasks, both detection and re-ID performances typically become lower than the separately-trained counterparts.

Fig. 1
figure 1
Illustration on how person and background representations are scattered in the embedding space. Black arrows denote background while colorful ones denote persons with different identities. Gray surfaces are the decision boundary for person and background. a For ùêø2 normalized embeddings, the inter-class angle distances for different persons are squeezed by backgrounds. b Norm-aware embeddings separate persons and background by norms and discriminate person identities by angles, thus the constrain on inter-class distances is relaxed.

Full size image
Our goal in this work is to develop a light-weight yet accurate model for joint person detection and re-ID, which perfectly fits the objective of person search task, and could be further used for multiple person tracking. We adopt the one-step strategy, i.e, jointly optimizing person detection and re-ID with a shared feature extractor, while relieving the objective contradictory problem by explicit feature decomposition. Specifically, we share the representations for detection and re-ID completely but decompose the features in the polar coordinate system, where each embedding vector is decomposed to radial norm r and angle ùúÉùúÉ. The radial norm r is used for person detection and could be interpreted as the detection confidence of a bounding box. The angle term ùúÉùúÉ measures the cosine similarity between persons, which is widely used in person re-ID. The principle idea is demonstrated in Fig. 1. During training, the embedding norms are optimized with a binary classification loss, and the angles are optimized with an OIM loss Xiao et al. (2017), which is a multi-class cross-entropy loss with normalized softmax weights. During inference, we fix the norm of the query person to 1 and calculate its similarity (dot product) to an arbitrary proposal, which is determined by both the norm and angle. Therefore, a high value of similarity indicates both high detection confidence and high identity similarity. Since the embedding norm is explicitly utilized, we call our method norm-aware embedding (NAE).

Another challenge for joint person detection and re-ID is the spatial misalignment problem. Typically, when we train a detector, one proposal is sampled as positive when it has a larger Intersection over Union (IoU) than 0.5 to any ground truth box. This relatively loose matching criterion makes sure to sample enough positives in each mini-batch, but has a negative effect as it includes many misaligned samples. Those samples with low alignment quality are harmful for re-ID performance Zheng et al. (2017) as the included background clutter usually plays a negative role on the features‚Äô discrimination ability. In order to alleviate this problem, we propose to re-weight features of each local patch according to its confidence of belonging to a person. Specifically, we perform fine-grained person/background classification for each proposal, i.e, we predict for each pixel location the confidence of belonging to a person, which is then used as the spatial attention weight for feature aggregation. After re-weighting, the features used for re-ID are expected to focus more on the person area while suppressing the background clutter, and thus become more discriminative for identity classification. Our approach using the above fine-grained classification is compatible with the norm-aware embedding, hence called NAE+.

In summary, the main contributions of this work are as follows:

We propose the norm-aware embedding method (NAE) for joint person detection and re-ID. NAE mitigates the objective contradictory objectives of two tasks by decomposing the feature embedding into norm and angle for detection and re-ID respectively.

A pixel-wise extension, denoted as NAE+, is proposed to deal with the misalignment problem for end-to-end person search.

We apply our NAE model on top of the Tracktor framework Bergmann et al. (2019) to better solve the multiple person tracking task.

Our methods are fast, explainable and achieve competitive performance on both person search (CUHK-SYSU and PRW) and multiple person tracking benchmarks (MOT17).

This manuscript originates from our conference paper Chen et al. (2020), based on which we have made significant extensions. We generalize our Norm-Aware Embedding (NAE) model from Person Search to Multiple Person Tracking (Section 3.2) based on the Tracktor paradigm Bergmann et al. (2019). Then we optimize the tracking algorithm of Tracktor according to the characteristics of our NAE model, e.g, Inter-frame Instance Anity (IIA) for data association (Section 3.2.2) and RPN-assisted bounding box regression (Section 3.2.3). Thorough ablation studies show that our method excels over the baseline method w.r.t. both accuracy and running speed (Section 5.2), which further leads to new state-of-the-art performance on the MOT17 and MOT16 benchmark (Section 5.3). We also enrich the review of the research in Sect. 2 by adding contents of Multiple Person Tracking.

Related Work
Person Search. Recently, person search has attracted intensive interests in the computer vision community. Zheng et al Zheng et al. (2017) first make a thorough evaluation on a number of combinations of different detectors and re-identifiers. They also propose a cascaded fine-tuning strategy for training and Confidence Weighted Similarity (CWS) for person matching. Lan et al Lan et al. (2018) analyze the resolution diversity problem in person search and solve the multi-scale matching problem by Cross-Level Semantic Alignment (CLSA). Chen et al Chen et al. (2018); Chen et al. (2020) raise attention on the contradictory objective problem in person search, and propose to avoid it by separating detection and re-identification. Han et al Han et al. (2019) point out that the bounding boxes produced by a vanilla detector are not optimal for re-ID. Thus they develop an RoI transform layer that enables gradient flow from the re-identifier to the detector for localization refinement.

In contrast to the above two-step methods, other works aim to solve the person search problem more efficiently using one-step methods. For example, the Online Instance Matching (OIM) loss  Xiao et al. (2017) and Center Loss Xiao et al. (2017); Wen et al. (2016) are used to address the ill-conditioned training problem and enhance the feature discrimination power. Yan et al Yan et al. (2019); Yan et al. (2020) and Munjal et al Munjal et al. (2019) propose to enrich the features with surrounding persons or the query person respectively. In Liu et al. (2017) and Chang et al. (2018), they discard the proposal generation operation and search the query person directly on the uncropped images by sequential decision making or reinforcement learning. Chen et al Chen et al. (2020) propose to model the relationship of detection and re-ID as a hierarchical structure which is then used as a prior to guide the network training. Yan et al Yan et al. (2021) first introduce anchor-free detection models Tian et al. (2019) into the domain of person search, which reduces the model running time drastically.

In this paper, we also adopt the one-step strategy. Based on the OIM model Xiao et al. (2017), we improve the feature learning with our norm-aware embedding. Additionally, the final similarity calculation of our method is similar to CWS. Different from the original form which is used in a post-processing step, CWS in our method is naturally induced from the explicit decomposition in the polar coordinate system. Therefore, it is also useful to guide the training process for better feature learning.

Multiple Person Tracking Tracking-by-detection has been a popular paradigm for multiple person tracking recently. Frame-by-frame detections Pishchulin et al. (2016); Pirsiavash et al. (2011); Milan et al. (2013); Breitenstein et al. (2009) or pre-clustered short tracks Wen et al. (2014); Tang et al. (2015); Choi (2015) are used to perform data association, which assigns the correct identity to objects and form the final tracklets. This problem is usually represented as a graph, whose nodes are detections or tracks, and edges are associative links. Researchers have been focusing on developing new algorithms to solve this graph cut problem Tang et al. (2017); Keuper et al. (2016); Tang et al. (2015).

Recently, appearance models have been proven useful to data association Tang et al. (2017); Leal-Taix√© et al. (2016); Xiang et al. (2015); Kim et al. (2015); Kuo and Nevatia (2011). Among them, person re-ID models are especially valuable. Wang et al. (2019) and Lu et al. (2020) further show that if the appearance features generated by the re-ID model are good enough, a simple linear association with Hungarian algorithm should be sufficient for a decent tracking accuracy. Meanwhile, they also show that solving detection and re-ID in a shared network is more efficient. Following the paradigm of joint detection and re-ID, Zhang et al. (2020) constructs a tracker based on CenterNetZhou et al. (2019) which treats objects as keypoints. Zhang et al. (2020) also avoids the degraded accuracy problem by solving the unfair learning of detection and re-ID with their anchor-free design. Different from Wang et al. (2019); Lu et al. (2020); Zhang et al. (2020) which solves data association with Hungarian algorithm as a post-process, Tracktor Bergmann et al. (2019) integrates data association into the model itself with the bounding box regression module of a detector, which could be faster than the re-ID with linear association paradigm since it eliminates the need for an iterative optimization. However, Tracktor Bergmann et al. (2019) still needs an additional re-ID model to deal with a small portion of hard samples, termed as Tracktor++. In this paper, we exploit the best of both approaches, i.e, solve detection and re-ID in a single model and associate detections with bounding box regression. Our Norm-Aware Embedding model serves as a handy backbone for this new solution. Moreover, we improve upon Tracktor Bergmann et al. (2019) by replacing plain detection scores with NAE similarities, which is more sensitive to ID switches.

Person re-ID. Early person re-ID models focus on designing features manually Wang et al. (2007); Farenzena et al. (2010); Zhao et al. (2013); Liao et al. (2015) and learning effective distance metrics Kostinger et al. (2012); Li et al. (2015); Zhang et al. (2016). Recently, CNNs have become the de facto standard for building a re-ID model. Such models are usually trained as a feature extractor with siamese loss Yi et al. (2014); Li et al. (2014); Ahmed et al. (2015); Varior et al. (2016); Liu et al. (2017); Xu et al. (2018), triplet loss Ding et al. (2015); Cheng et al. (2016) or cross-entropy loss Xiao et al. (2016); Zheng et al. (2016); Zheng et al. (2017); Fan et al. (2018); Xiang et al. (2018). Instead of averaging the convolutional features from all locations, latest methods extract part-level features and join them together as the final person embedding Sun et al. (2018); Wei et al. (2017); Zhao et al. (2017); Yao et al. (2019). These methods usually partition the feature maps into horizontal stripes for fine-grained feature learning. Our pixel-wise extension of the norm-aware embedding is also inspired by this approach. Instead of dividing the feature maps into blocks, we use a pixel-wise probability map to re-weight the features at every location, which is further supervised by a segmentation loss with bounding box annotations.

Pedestrian Detection. Similar to person re-ID, early pedestrian detection methods are also based on hand-crafted features Felzenszwalb et al. (2009); Dollar et al. (2014, 2009); Zhang et al. (2014, 2015). Deep neural networks, as versatile feature extractors, have dominated this task in recent years Zhang et al. (2016); Zhang et al. (2018); Ouyang and Wang (2013, 2012). Successful general object detection models are adapted for pedestrians, such as R-CNN Girshick et al. (2014); Zhang et al. (2016); Zhang et al. (2018) and Faster R-CNN Ren et al. (2017); Zhang et al. (2017, 2018). In this work, we also build our model based on the adapted Faster R-CNN, which is extensible for fine-grained feature learning and reaches a sweet spot between speed and accuracy.

Embedding Norms. It is common practice to normalize the deep embeddings with unit length in face recognition Liu et al. (2017, 2016); Deng et al. (2018), person re-ID Fan et al. (2018); Xiang et al. (2018) and person search Xiao et al. (2017). To the best of our knowledge, only two papers discuss the efficacy of embedding norms Guo and Zhang (2017); Wang et al. (2018). Guo et al Guo and Zhang (2017) find that the norm of the softmax weight vector is related to the sample number of this class. They further propose to promote the norms of underrepresented classes in order to improve the performance of one-shot face recognition. Wang et al Wang et al. (2018) also use normalized embeddings to represent face identities. Additionally, they regress the norm of the embedding to the age of the given person by reducing the mean squared error between these two during training. However, the norm information is then ignored for age-invariant face recognition when matching identities. Different from the above two works, our method makes explicit use of the embedding norms rather than employing them as a regularization term during training. By using the norm for the classification task (person vs. background), we endow the norm with a clear semantic meaning, i.e, the detection confidence, which is essential for person search.

Methodology
Fig. 2
figure 2
Overall architecture for one-step methods based on Faster R-CNN Ren et al. (2017). Black arrows denote the forward pass and colorful ones denote different supervision signals. Region Proposal Net is omitted for simplicity.

Full size image
Fig. 3
figure 3
Multi-task head architecture for Online Instance Matching (OIM), norm-aware embedding (NAE) and its pixel-wise extension (NAE+). Dashed arrows indicate that the procedure is only enabled during inference.

Full size image
A typical one-step person search method based on Faster R-CNN Ren et al. (2017) is illustrated in Fig. 2. A multi-task head for localization, detection and re-ID is added on top of the top convolutional features of Faster R-CNN.

The first and most representative one-step method is OIM Xiao et al. (2017), where an ùêø2 normalized fully connected layer is concatenated to the global average pooled convolutional features. As is shown in Fig. 3a, the box regression and region classification losses remain the same as in Faster R-CNN, with an identity classification loss supervising the person embeddings produced by the fully connected layer. In contrast, our norm-aware embedding method, illustrated in Fig. 3b, removes the original region classification branch and uses the embedding norm as the binary person/background classification confidence.

In this section, we will describe the norm-aware embedding head in detail and present the pixel-wise extension for fine-grained feature learning.

Norm-Aware Embedding
On top of the final convolutional features, we first apply global average pooling (GAP) and a fully connected (FC) layer to get the d dimensional feature vector ùê±, where d is fixed to 256 following Xiao et al. (2017). Then ùê± is decomposed explicitly in the polar coordinate system as:

ùê±=ùëü‚ãÖùúÉùúÉ,
(1)
where norm ùëü‚àà[0,+‚àû) and angle ùúÉùúÉ is a 256-dimensional vector with unit length.

To interpret the norm r as the detection confidence, we use a monotonic mapping to squeeze its magnitude to the range of [0, 1]:

ùëüÃÉ =ùúé(ùëü‚àíùîº[ùëü]Var[ùëü]+ùúñ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚àö‚ãÖùõæ+ùõΩ),
(2)
where ùúé is the sigmoid activation, within which is a batch normalization Ioffe and Szegedy (2015) layer.

The original embedding ùê± is then scaled into our norm-aware embedding ùê±ÃÉ :

ùê±ÃÉ =ùëüÃÉ ‚ãÖùúÉùúÉ
(3)
This procedure is represented by the dashed arrows in Fig. 3b.

Inference & Matching. For a query person, we first extract its embedding ùê±ÃÉ ùëû by removing the RPN module and setting the proposal coordinate with the given bounding box. Since the query bounding box definitely contains a person, we manually set the norm of ùê±ÃÉ ùëû to 1. Then, the similarity of the query person and an arbitrary detected person ùê±ùëî in the gallery is calculated as follows:

sim(ùê±ÃÉ ùëû,ùê±ÃÉ ùëî)=ùê±ÃÉ ùëáùëûùê±ÃÉ ùëî=ùëüÃÉ ùëî‚ãÖùúÉùúÉùëáùëûùúÉùúÉùëî
(4)
In the above equation, ùúÉùúÉùëáùëûùúÉùúÉùëî is the cosine similarity between the query and the gallery person. Thus, the final similarity equals the cosine similarity weighted by the detection confidence, which is especially useful to suppress false detections. Meanwhile, it also shares the same formation as Class Weighted Similarity (CWS) Zheng et al. (2017). However, instead of just using CWS as a post-processing step, we leverage it to explicitly decompose the embedding for the detection and re-ID objectives during training. We further demonstrate the efficacy of CWS in Sec. 4.3.

Training. As can be seen from Eq. 4, our norm-aware embedding is able to discriminate person identity as well as suppress false detections. Therefore, it can be supervised by re-ID and detection signals simultaneously during training. Specifically, the detection signal is cast on the scaled norm ùëüÃÉ  and formulated as a binary classification:

Óà∏det=‚àíùë¶log(ùëüÃÉ )‚àí(1‚àíùë¶)log(1‚àíùëüÃÉ )
(5)
where y is a {0,1} label indicating if this proposal is considered as background or person. Meanwhile, we use an OIM loss Xiao et al. (2017) Óà∏reid on the normalized angular vector ùúÉùúÉ, which is a multi-class cross-entropy loss that minimizes the angular margin for the same identity and maximizes that of different identities. The bounding box regression loss Óà∏box remains identical to the form defined by Faster R-CNN. The three loss functions are illustrated by the yellow, green and blue arrows respectively in Fig. 3b. Together with RPN classification and regression losses, they are jointly optimized by Stochastic Gradient Descent (SGD).

Pixel-Wise Extension
In Sec. 3.1, the convolutional features of each proposal are collapsed into a vector by global average pooling, losing spatial information. In this way, the person embeddings would suffer from distracting noise of the misaligned regions (the black region in Fig. 4). To address this problem, we propose NAE+, which is a pixel-wise extension of NAE. We carefully leverage the spatial information via highlighting the body part and suppressing the misaligned regions. Specifically, we first predict a 256√óùëò√óùëò tensor from the top feature map with a 1√ó1 convolutional layer. Then the 256-dimensional vectors at all locations can be normalized and scaled into norm-aware embeddings, while still preserving the spatial structure. An illustration is shown in Fig. 3. In this way, the mapped norm ùëüÃÉ ùëñ at each location acts as a spatial attention, calibrating the per-pixel importance before the tensor is collapsed into the final matching vector.

Fig. 4
figure 4
Pixel-wise label generation from bounding box annotation. Red box is the ground truth while blue box is the proposal. Black region is marked as 0, white being one and gray being values in between. Bilinear interpolation is used to match the label size to the feature map size.

Full size image
The training of NAE+ can be formulated in a semantic segmentation manner, i.e, supervising all the mapped norms with a per-pixel cross-entropy loss. Different from the standard semantic segmentation approach, the ground truth class map is not available in person search datasets, hence we need to generate the coarse ground truth from bounding box annotations. The generation process is shown in Fig. 4. For each RoI, we set its intersection to the ground truth bounding box as 1 and leave the rest as 0. Bilinear interpolation is used to resize the RoI box into ùëò√óùëò. The loss is then formulated as follows:

Óà∏det+=‚àí1ùëò2‚àëùëñ=1ùëò2ùë¶ùëñlog(ùëüÃÉ ùëñ)+(1‚àíùë¶ùëñ)log(1‚àíùëüÃÉ ùëñ)
(6)
where ùë¶ùëñ is the generated per-pixel label with its value lying between [0, 1]. The label generation procedure is illustrated in Fig. 4.

During inference, even though the per-pixel probability map is trained, a single probability is needed to measure the detection confidence. A direct approach is to use the averaged probability ùîºùëñ[ùëüÃÉ ùëñ] across all spatial locations. However, we find through our experiments that this approach works poorly, i.e, the confidence of a valid bounding box is too low since the mean value would be diluted by the inevitable background regions with low confidence. We address this problem by simply stretching the magnitude of ùîºùëñ[ùëüÃÉ ùëñ]. Specifically, for each image, all the detection confidences are divided by the maximum value among them, such that all of them are expanded and still range between 0 and 1.

Compared to NAE, NAE+ does not increase the number of parameters. It only adds a small overhead on computation, while improving the person search accuracy as demonstrated by experiments.

Tracktor with Norm-Aware Embedding
Based on our Norm-Aware Embedding model, we extend it from the task of Person Search to Multiple Person Tracking. We adopt Tracktor Bergmann et al. (2019) as our basic paradigm, replacing its vanilla detector with our NAE model and optimize the algorithm. In the following, we will describe our NAE Tracktor in detail by revisiting Tracktor at first, and then introducing our NAE Tracktor and RPN-assisted bounding box regression.

Revisiting Tracktor
The goal of multiple person tracking is to find the spatial temporal position of every unique person from a frame-by-frame video. These objects could be represented as a trajectory ùëáùëò, which is composed of a list of bounding boxes {ùêõùëòùë°1,ùêõùëòùë°2,‚Ä¶}. Each box is defined with a spatial coordinate ùêõùëòùë°=(ùë•,ùë¶,ùë§,‚Ñé), where t is the frame index and k represents the identity of this trajectory.

Tracktor Bergmann et al. (2019) implements this process with an object detector, e.g, Faster R-CNN. There are mainly two subsequent steps. The first one is bounding box regression for data association.

Suppose we already have the location ùêõùëòùë°‚àí1 for the k-th object in frame ùë°‚àí1, the aim is to find its updated location ùêõùëòùë° in frame t. This is achieved by cropping the spatial position of ùêõùëòùë°‚àí1 on the feature map of frame t with RoIAlign, and then predict the offset Œîùëüùëêùëõùëõ to find the updated position.

ùêõùëòùë°=ùêõùëòùë°‚àí1+Œîùëüùëêùëõùëõ
(7)
The updated box also comes with a confidence score ùë†ùëòùë°. If it is greater than a threshold ùúéùëéùëêùë°ùëñùë£ùëí, ùêõùëòùë° is then added to ùëáùëò. Otherwise ùêõùëòùë° is discarded and ùëáùëò is marked as inactive. The bounding box regression step is illustrated in Fig. 5. Once all existing tracklets are updated or terminated, we conduct step 2: new track discovery. All bounding boxes in frame t, denoted as ùêµùë°={ùêõùëò1ùë°,ùêõùëò2ùë°,‚Ä¶} are used to initialize new tracks, except for those linked to an existing track in the previous step. In the case of MOT Challenge Milan et al. (2016), ùêµùë° could come from the pre-defined public detections or private detections provided by the model itself.

Fig. 5
figure 5
The bounding box regression process of Tracktor a and our NAE Tracktor b. Differences are marked in red, which incorporates identity information. We name the red part as Inter-frame Instance Affinity (IIA).

Full size image
NAE Tracktor
Since our Norm-Aware Embedding model is also based on Faster R-CNN, it is straightforward to replace the vanilla detector in Tracktor with NAE. Moreover, we also replace the termination condition from detection confidence thresholding to feature similarity thresholding, as is illustrated by the red part of Fig. 5. Specifically, if the embedding similarity of the regressed box ùêõùëòùë° and the previous box ùêõùëòùë°‚àí1 is smaller than a threshold, i.e, sim(ùê±ÃÉ ùëòùë°,ùê±ÃÉ ùëòùë°‚àí1)<ùúéùëéùëêùë°ùëñùë£ùëí, the k-th object is considered lost and its trajectory ùëáùëò is inactive. We name this termination condition as Inter-frame Instance Affinity (IIA).

Compared to its original version, NAE Tracktor produces an identity discriminative feature for every bounding box. This characteristic brings two advantages. Firstly, the bounding box regression step is more robust at crowded scenes. Since the similarity of norm-aware embeddings considers not only the detection confidence but also the appearance similarity, the regressed box would be more sensitive to identity consistency, i.e, model is more likely to terminate the trajectory if the target person is occluded by another person. As for the original Tracktor, the trajectory tend to switch from the previous person to the occluder. Secondly, Tracktor needs an additional re-ID model to re-activate the lost tracks. In contrast, our NAE Tracktor already includes the functionality of a re-ID model. The appearance embeddings are produced at a very low cost, thus saves computational resources.

RPN-assisted Box Regression
Our NAE Tracktor and its original version in Bergmann et al. (2019) are both based on the same assumption: the position shift of a bounding box between frames is not significant. However, this assumption does not hold when 1) camera has large motion, 2) object is moving fast and 3) video frame rate is relatively low. Tracktor patches this problem by adding a motion model as pre-process, namely camera motion compensation (CMC) Evangelidis and Psarakis (2008). CMC estimates an affine transformation matrix from the input image of frame t and ùë°‚àí1 through an iterative optimization, which is highly computational expensive. We propose to solve this problem by making use of RPN, which is discarded from the trained Faster R-CNN by Tracktor.

Specifically, given a bounding box ùêõùëòùë°‚àí1 from the previous frame, we first find its closest anchor Óà≠(ùêõùëòùë°‚àí1) with largest IoU:

Óà≠(ùêõùëòùë°‚àí1)=maxùêö‚ààùêÄIoU(ùêö,ùêõùëòùë°‚àí1)
(8)
where ùêÄ is the total set of anchors of a Faster R-CNN. Then Óà≠(ùêõùëòùë°‚àí1) is adjusted by RPN and RCNN sequentially to find the regressed box

ùêõùëòùë°=Óà≠(ùêõùëòùë°‚àí1)+Œîùëüùëùùëõ+Œîùëüùëêùëõùëõ.
(9)
Compared to the one-time offset by Tracktor, our RPN-assisted regression conducts two steps of adjustments to ùêõùëòùë°‚àí1, which potentially covers a larger range of position drifts. Moreover, the re-ID embedding produced by the second stage R-CNN is more aligned to the true position of the target person, since the RoI is already adjusted for one time by RPN before RoIAlign. The whole process is simple and straightforward without any iterative optimizations, thus running significantly faster than CMC. Note that our RPN-assisted regression is also compatible with CMC. Combining the two of them could further increase the tracking accuracy, if the application environment is tolerable to running speed.

Person Search Experiments
In this section, we perform a thorough evaluation of our NAE and NAE+. We begin by introducing the datasets and evaluation protocols, after which we describe the implementation in detail. Comprehensive analysis and visual inspections are conducted to explore the efficacy of our method. We further compare our method with the state of the arts w.r.t. both search performance and running speed.

Datasets and Settings
CUHK-SYSU Xiao et al. (2017) is a hybrid dataset consisting of city scenes shot by a moving camera and screenshots of movies. A total of 18, 184 uncropped images and 96, 143 bounding boxes are collected, among which 11, 206 images and 55, 272 pedestrians are used for training. The testing set includes 2, 900 query persons and 6, 978 gallery images. For each query, different gallery sizes are defined by the benchmark to assess the scaling ability of different models. If not specified, we use the gallery size of 100 by default.

PRW Zheng et al. (2017) is extracted from video frames recorded by 6 stationary cameras that are installed at different locations in a university campus. There are 11, 816 frames with 43, 110 bounding boxes, where 34, 304 of them are annotated with 932 identities and the rest marked as unknown identities. In the training set, there are 5, 704 images with 482 identities. The testing set contains 2, 057 query persons and each of them are to be searched in a gallery with 6, 112 images. Therefore, the gallery size is significantly larger than the default setting of CUHK-SYSU.

Evaluation Protocol. Similar to person re-ID Zheng et al. (2015), Mean Average Precision (mAP) and Cumulative Matching Characteristics (CMC top-K) are standard metrics used to measure person search performance. However, a candidate in the ranking list would only be considered correct if its IoU to the ground truth bounding box is larger than 0.5, which is the main difference from the re-ID approach.

Implementation Details
Our model consists of three major partsFootnote1: a stem network for spatial feature extraction, a region proposal network (RPN) for candidate bounding box sampling and a head network for proposal classification and regression.

We adopt an ImageNet-pretrained Deng et al. (2009) ResNet-50 He et al. (2016) as our backbone network, with the foremost four residual blocks, i.e, ‚Äòconv1‚Äô to ‚Äòconv4‚Äô, used as the stem network.

A standard RPN is built on top of the stem network to generate pedestrian candidate bounding boxes. We follow the anchor settings in Lin et al. (2017) and sample the positive proposals with a lower bound IoU of 0.5 to the ground truth, and the IoU interval for negative proposals is [0.1, 0.5).

Next, the proposals are cropped and reshaped to 14√ó14 by an RoIAlign layer He et al. (2017). The head network, which is the ‚Äòconv5‚Äô residual block of ResNet-50, is used to transform the proposals into 2048-dimensional 7√ó7 feature maps. Task-specific heads for bounding box regression and norm-aware embedding generation are added on top of the feature maps. We set the spatial size k to 7 for NAE+, which is depicted in Fig. 3.

During training, we sample 5 images for each batch, which are resized to 900√ó1,500. Our model is trained on a single NVIDIA Tesla P40 GPU for 22 epochs, with an initial learning rate of 0.003 which is progressively warmed up during the first epoch and decreased by 10 at the 16-th epoch. The momentum and weight decay of SGD are set to 0.9 and 5√ó10‚àí4 respectively. As for NAE+, we initialize the weights with a trained NAE model by converting the FC layer weights into 1√ó1 convolution weights. It is then fine-tuned for 11 epochs. The learning rate is set to 0.003 for the first 8 epochs, and then decayed to 0.0003 for the remaining 3 epochs.

At test time, the number of proposals is set to 300. Non-maximum Suppression Girshick et al. (2015) with a threshold of 0.4 is used to filter out redundant boxes.

Analytical Experiments
Table 1 Detection and re-ID performance with different detected boxes. We remove the RPN of the trained person search model and set the proposals manually with the boxes to be inspected.
Full size table
Table 2 Ablation experiments on Class Weighted Similarity.
Full size table
As mentioned in the introduction section, person search accuracy is affected by both the detection quality and the identity recognition accuracy. In order to better understand how well our NAE method handles the above two sub-tasks, we disentangle the person search into detection and re-ID and evaluate their performances individually.

We implement the analysis on our norm-aware embedding and the OIM baseline model. Four variants are evaluated, namely

OIM-base: Our re-implementation of the OIM model Xiao et al. (2017) which shares the same architecture settings as our NAE model described in Sec. 4.2. Benefiting from large input image size Zhang et al. (2018), dense anchor setting Lin et al. (2017) and RoIAlign He et al. (2017), our OIM-base is significantly better than the original implementation.

OIM-base w/ CWS: Using the trained model of OIM-base and apply Class Weighted Similarity Zheng et al. (2017) when matching gallery persons to the query.

NAE: Our norm-aware embedding model as described in Sec. 3.1.

NAE w/o CWS: Identical to NAE but only using the normalized embedding ùúÉùúÉ without the scale operation (the vector marked in green in Fig. 3).

All models are trained on CUHK-SYSU and tested under a gallery size of 100.

For pedestrian detection, we use Recall and Average Precision (AP) as the performance metrics. For person re-ID, mAP and top-1 accuracy are adopted. They are the same as in person search, but the embeddings for matching are extracted differently. We remove the RPN of the trained model and set the proposals manually with the boxes to be inspected. Therefore, an end-to-end person search model serves solely as a re-ID feature extractor.

The evaluation results are collected in Table 1 and Table 2, from which we make the following conclusions.

The detection quality of NAE is better. The detection results are recorded in the second column of Table 1, from which we can see that our NAE model achieves 92.6% and 86.8% w.r.t. Recall and AP, surpassing OIM-base by 3.3 and 7.1 pp. respectively. We also train a vanilla detector without any re-ID losses on CUHK-SYSU, which yields a performance of 93.1% and 87.0% w.r.t. Recall and AP. The fact that a vanilla detector performs better than both NAE and OIM-base indicates that the re-ID loss would harm the detection performance because of the contradictory objectives. However, NAE harms less to the detection performance than OIM-base, demonstrating the effectiveness of NAE on handling the opposing relationship between detection and re-ID.

The final person search performance of NAE is also better than OIM-base, thanks to the high-quality bounding boxes.

NAE is more discriminative for re-identification. In the lower block of Table 1, we can see that NAE achieves 91.5% and 92.4% w.r.t. mAP and top-1, outperforming OIM-base with NAE detected boxes by 5.6 and 4.8 pp. The performance improvement also holds when switching the bounding boxes to the ground truth boxes or OIM-base detections, as is shown in the upper and lower block of Table 1. These results suggest that NAE has better re-ID accuracy, which indicates that the discrimination power of NAE is superior to OIM.

Class Weighted Similarity is helpful. In Table 2, we can see that adding CWS to OIM-base yields a gain of +2.7 and +2.4 pp. for mAP and top-1 respectively. Meanwhile, removing CWS from NAE makes mAP and top-1 drop from 91.5 to 89.9 and 92.4 to 91.3. These results confirm the positive efficacy of CWS. As a naturally induced form of NAE, CWS also contributes to the person search performance of our method.

In conclusion, our norm-aware embedding

provides a more reasonable solution to handle the contradictory objectives of detection and re-ID by decomposing embedding explicitly into norm and angle. The detection and re-ID sub-tasks both achieve better results than the baseline. As a result, the final person search performance of our method is remarkable, which can be attributed to the improvements on the two sub-tasks.

Table 3 Comparison with state of the arts
Full size table
Visualized Inspections
To inspect the efficacy of the NAE+ method, we visualize the output probability maps in Fig. 6. Specifically, we remove the RPN and RoIAlign modules from a trained NAE+ model and forward the input image directly through the whole network. The output probability map, composed of the mapped norms ùëüÃÉ ùëñ at each location, is upsampled with bilinear interpolation to match the input image size. We then represent the probability maps with different colors and overlay them with the corresponding input images. We observe from Fig. 6 that NAE+ successfully highlights the human body region and suppresses background clutters, which makes the embedding more robust to noises. As is shown in Table 3, NAE+ outperforms NAE consistently on CUHK-SYSU and PRW.

Fig. 6
figure 6
Pixel-wise norm predictions of NAE+ on CUHK-SYSU. Warmer color represents larger norm, which indicates a higher probability of this position being a person. The detection performance of NAE+ is 93.0% and 82.1% w.r.t. Recall and AP, remaining similar to to that of NAE.

Full size image
We also show some qualitative search results in Fig. 8. The selected cases are representative hard ones, including crowd overlapping (case a, f), confusing appearance (c, d, f, g), viewpoint change (c, d, f, g) and obstacle occlusion (e, g). Our NAE method successfully localize and match the query person in most of the hard cases, although there is still room to improve the performance on extreme instances like (f) and (g). Moreover, our NAE+ method is better than NAE as it returns the correct result for all scenarios.

Comparison to the State of the Art
In this section, we compare our NAE and NAE+ to state-of-the-art methods on person search in Table 3. All the results are gathered according to their search strategies, i.e, one-step methods in the upper block and two-step method in the lower block. ‚ÄòDPM‚Äô, ‚ÄòCNN‚Äô and ‚ÄòFPN‚Äô stand for Deformable Part Model Girshick et al. (2015), ResNet-50-based Faster R-CNN Ren et al. (2017) and Feature Pyramid Network Lin et al. (2017) respectively. They are individually trained as vanilla pedestrian detectors.

Comparison on CUHK-SYSU. As shown in Table 3, both NAE and NAE+ outperform all other one-step methods, including the strong counterparts HOIM Chen et al. (2020), QEEPS Munjal et al. (2019) and CTXGraph Yan et al. (2019). Note that their forward pass requires some computationally heavy operations, e.g, large matrix multiplication for memory buffer, siamese attention and additional graph convolutions. In contrast, our method only needs a single forward pass, consuming less computing resources and memory. Our method is also comparable to the top two-step method ‚ÄòFPN + RDLR‚Äô Han et al. (2019), which uses two backbones for detection and re-ID respectively. We believe the performance-boosting components of Han et al. (2019), i.e, feature pyramid network, RoI transform layer and proxy triplet loss, could also bring improvements to our method, which is however beyond the scope of this paper.

In Fig. 7, we further evaluate the performance under larger search scopes. As is defined in Xiao et al. (2017), each query person is matched in galleries with an increasing size. From Fig. 7 we can see that the mAP for all methods decrease monotonically as the gallery size becomes larger. This phenomenon indicates that it is more difficult to match a person in larger scopes, which is a typical challenge in real-world applications. Our method outperforms all the one-step methods by a considerable margin, while achieving similar mAP to the two-step methods at all scopes.

Comparison on PRW. In the right column of Table 3, we summarize the results of our NAE and NAE+ together with other competitive methods. Our NAE method surpasses all previous methods, including both one-step and two-step ones. In particular, our NAE outperforms the second best method by a large margin of around 9 pp. w.r.t. top-1 accuracy. Compared to CUHK-SYSU, PRW consists of less training data and larger gallery size, thus it is more challenging. Our NAE method behaves better on PRW, indicating that our method is more robust with reduced training data. Moreover, the pixel-wise extension NAE+ further improves over NAE by 0.7 and 0.2 pp. w.r.t. mAP and top-1 metrics, setting the new state of the art on PRW.

Fig. 7
figure 7
Performance comparison on CUHK-SYSU with varying gallery sizes. Dashed lines represent two-step methods while solid lines denote one-step methods.

Full size image
Table 4 Speed comparison on different GPUs. Running time is measured in milliseconds.
Full size table
Fig. 8
figure 8
Top-1 search results for several hard samples. ‚ÄòQ‚Äô stands for the query image, for each we show the top-1 match given by OIM-base, our NAE and NAE+. Green/red boxes denote the correct/wrong results respectively. (a)‚àº(e) are cases where OIM-base fails while NAE and NAE+ succeed. (f) and (g) are failure cases for both OIM-base and NAE, except for NAE+.

Full size image
Timing. We compare the speed of different methods in Table 4. Since different methods are implemented on different GPUs, we show the Tera-Floating Point Operation per-second (TFLOPs) beside each GPU for fair comparison. Our NAE and NAE+ are implemented in PyTorch Paszke et al. (2017) without bells and whistles. We test the models with an input image size as 900√ó1500, which is the same as MGTS and QEEPS Munjal et al. (2019). We can see from Table 4 that our method is around 2 times faster than the two-step method MGTS Chen et al. (2018). Our method is also 2 times faster than QEEPS, which is the current state-of-the-art one-step method. Finally, our NAE and NAE+ methods cost 83 and 98 milliseconds per-frame respectively on a V100 GPU. The fast speed of our method reveals its great potential for real-world applications.

Multiple Person Tracking Experiments
Since our NAE model achieves remarkable performance on person search, in this section we implement the multiple person tracking experiments using our NAE model. In the following, we first introduce the experiment settings including datasets, evaluation metrics and implementation details. Analytical experiments are conducted to verify the efficacy of each components of our NAE Tracktor. We further compare our method to other leading methods on the MOT17 and MOT16 benchmark.

Datasets and Settings
MOT Challenge is nowadays the de facto benchmark for multiple object tracking. It consists of four separate challenges for 2D pedestrian tracking, namely 2D MOT 2015 Leal-Taix√© et al. (2015), MOT16 Milan et al. (2016), MOT17 Milan et al. (2016) and MOT20 Dendorfer et al. (2020). We choose MOT17 and MOT16 as our evaluation set due to its wide-spread usage. The training and testing set each include 7 sequences with bounding box annotation across all frames. For the training set, each bounding box is also labeled with a person identity. Therefore it is the same as for person search datasets, as described in Sec. 4.1. For MOT17, the video sequences also come with three types of public detections, namely DPM Felzenszwalb et al. (2009), Faster R-CNN Ren et al. (2017) and SDP Yang et al. (2016), for fair comparison. For MOT16, only DPM public detections are provided. The CLEAR metrics Bernardin and Stiefelhagen (2008) are used to evaluate tracking quality. The metrics includes multiple kinds of measures, among which the most important two are Multiple Object Tracking Accuracy (MOTA) and ID F1 Score (IDF1). We also report Most Tracked (MT), Most Lost (ML), False Positive (FP), False Negative (FN) and ID Switch (ID Sw.) for comprehensive understanding of tracking quality.

We follow the settings of Tracktor Bergmann et al. (2019) which only initializes new tracks with public detections provided by the challenge. Our network configuration is also consistent with Tracktor, with the same implementation tools (PyTorch)Footnote2, and input image resolution (800√ó1333). The backbone network is a COCO Lin et al. (2014) pre-trained Faster R-CNN based on FPN Lin et al. (2017). We add a NAE head, as illustrated in Fig. 3, to the top FC layer and fine-tune the whole model on the MOT17 training set for 20 epochs, with an initial learning rate of 5√ó10‚àí4 which decays at a magnitude of 0.1 at epoch 16. The threshold ùúéùëéùëêùë°ùëñùë£ùëí is set to 0.3 throughout the paper, which is chosen to be optimal for the MOT17 training set according to Fig. 9. For the final tracking result, we use interpolation to fill in the lost period of a trajectory.

Fig. 9
figure 9
MOT performance w.r.t. MOTA under different ùúéùëéùëêùë°ùëñùë£ùëí values. We choose ùúéùëéùëêùë°ùëñùë£ùëí=0.3 throughout the paper.

Full size image
Analytical Experiments
Table 5 Analytical experiments on MOT17 training set with ‚ÄôFaster R-CNN‚Äô public detections. ‚Üë means the greater the number the better the performance, ‚Üì the otherwise.
Full size table
Table 6 Tracking performance on test set with public detections.
Full size table
Following Bergmann et al. (2019), our analytical experiments are conducted on the MOT17 training set with ‚ÄòFaster R-CNN‚Äô public detections. The results are summarized in Table. 5. We also include the performance of Tracktor Bergmann et al. (2019) in the same table for comparison. From Table 5 we make the following observations.

NAE Tracktor with Inter-frame Instance Affinity is better than plain detector-based Tracktor. This conclusion comes from the comparison between row ‚ÄòTracktor‚Äô, ‚ÄòNAE Tracktor - IIA‚Äô and ‚ÄòNAE Tracktor‚Äô. ‚ÄòNAE Tracktor - IIA‚Äô is a direct replica of ‚ÄòTracktor‚Äô which uses detection confidence score (in our case the embedding norm ùëüÃÉ ) as the termination condition of bounding box regression. We can see that it achieves comparable performance to ‚ÄòTracktor‚Äô. ‚ÄòNAE Tracktor‚Äô is identical to ‚ÄòNAE Tracktor - IIA‚Äô except that it uses inter-frame instance affinities, i.e, the similarity of norm-aware embeddings, to decide whether to de-activate the track. It is clear that ‚ÄòNAE Tracktor‚Äô excels on almost all metrics with a little sacrifice on False Positive and running speed, which verifies the effectiveness of our norm-aware embedding model and IIA.

RPN-assisted regression is a satisfactory alternative for CMC. From ‚ÄòNAE Tracktor + re-ID + RPN‚Äô and ‚ÄòNAE Tracktor + re-ID‚Äô we can see that RPN-assisted regression increases MOTA by 1.0 pp. It also achieves the lowest Most Lost number of all methods. Compared to the Camera Motion Compensation method, RPN-assisted regression is only 0.2 pp. inferior w.r.t. MOTA, but running around 4 times faster, which is favourable for real-world applications.

CMC further improves tracking performance. Since our RPN-assisted regression is also compatible with CMC, it is reasonable to combine the best of them for better performance. We can see from Table 5 that ‚ÄòNAE Tracktor + re-ID + RPN + CMC‚Äô yields the highest MOTA of all methods, surpassing ‚ÄòNAE Tracktor + re-ID + CMC‚Äô by 0.3 pp. while the speed drop is only 0.9 frames per-second.

In conclusion, our norm-aware embedding model with inter-frame instance affinity successfully improves the tracking performance of Tracktor, benefited from the better person search accuracy. RPN-assisted regression also makes substantial contribution for a good speed-accuracy trade-off. However, large inter-frame movement caused by camera motion is still a difficult problem for the Tracktor paradigm. Adding camera motion compensation as a pre-process is a feasible remedy.

Comparison to the State of the Art
In this section, we compare our NAE Tracktor with other modern multiple person tracking methods on the testing set of MOT17 and MOT16. Among all the module configurations we investigated in Sect. 5.2, we select two of them for benchmark comparison. One of them is the efficient version, namely ‚ÄòNAE Tracktor + re-ID + RPN‚Äô which runs at 16 frames per-second, achieving an MOTA of 64.0 on the MOT17 training set. We name this ‚ÄòNAE Tracktor @E‚Äô for simplicity. The other one is named ‚ÄòNAE Tracktor @A‚Äô, where ‚ÄòA‚Äô stands for accurate. It represents the ‚ÄòNAE Tracktor + re-ID + RPN + CMC‚Äô in Table 5 which achieves the best MOTA among all configurations while running at 3.7 frames per-second. For other methods, we select the leading entries from the MOT websiteFootnote3 which are officially published. The overall performance is summarized in Table 6. We can see that both of our efficient and accurate models achieve the best performance w.r.t. MOTA. In particular, our methods outperform the previous state-of-the-art method ‚ÄòTracktor++‚Äô and its upgraded version ‚ÄòTracktor++_v2‚Äô due to the effectiveness of our norm-aware embedding and RPN-assisted box regression. Moreover, our accurate model ‚ÄòNAE Tracktor @A‚Äô ranks best in terms of MOTA, MT and FN. It also achieves best MOTA, FP and FN on MOT16, setting a new state of the art on MOTChallenge.

Conclusion
In this paper, we propose an embedding decomposing method to deal with the contradictory objective problem of person detection and re-ID. Person embeddings are disintegrated into norm and angle, which are used to measure the detection confidence and identity similarity accordingly. In this way, the detection and re-ID sub-tasks both get higher performance, which in result improves the person search accuracy. We further extend our method from region-level to pixel-level in order to extract more fine-grained information. Thorough experiments on person search benchmarks confirm the advantages of our method in terms of both accuracy and speed. Based on the success of our person search method, we apply it to multiple person tracking by replacing the detector of Tracktor Bergmann et al. (2019) and optimizing the data association process with inter-frame instance affinity and RPN-assisted box regression. As a result, our method also achieves state-of-the-art performance on the MOTChallenge benchmark.