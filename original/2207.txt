Unlike the maturity of image denoising research, video denoising has remained a challenging problem. A fundamental issue at the core of the video denoising (VD) problem is how to efficiently remove noise by exploiting temporal redundancy in video frames in a principled manner. Based on the maximum a posterior (MAP) estimation framework and recent advances in deep learning, we present a novel deep MAP-based video denoising method named MAP-VDNet with adaptive temporal fusion and deep image prior. The proposed MAP-based VD algorithm allows computationally efficient untangling of motion estimation (frame alignment) and image restoration (denoising). To address the misalignment issue, we also present a robust multi-frame fusion strategy for predicting spatially varying fusion weights by a neural network. To facilitate end-to-end optimization, we unfold the proposed iterative MAP-based VD algorithm into a deep convolutional network named MAP-VDNet. Extensive experimental results on three popular video datasets have shown that the proposed MAP-VDNet significantly outperforms current state-of-the-art VD techniques such as ViDeNN and FastDVDnet. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/MAP-VDNet.htm.

Access provided by University of Auckland Library

Introduction
The field of image denoising has advanced rapidly in the past 20 years. Existing image denoising methods can be roughly classified into two categoriesâ€”i.e., model-based (Portilla et al. 2003; Dabov et al. 2007; Dong et al. 2013; Gu et al. 2014) and deep learning-based (Zhang et al. 2017a; Tai et al. 2017; Zhang et al. 2018a). Model-based methods formulate image denoising as a Bayesian estimation problem in a probabilistic setting or a variational optimization problem in a deterministic setting. Through mathematical construction of an image prior or a regularization function, we can develop principled solutions to the image denoising problem (Dabov et al. 2007; Dong et al. 2013). In the literature of image denoising, sparsity-based prior models or regularization methods are among the most popular, which lead to state-of-the-art denoising performance in model-based approaches (Dabov et al. 2007; Dong et al. 2013).

Deep learning-based approaches to image denoising have been extensively studied in recent years. Instead of mathematical construction, learning-based denoising methods directly learn a nonlinear mapping function from the space of noisy images to that of clean images (Zhang et al. 2017a; Tai et al. 2017; Zhang et al. 2018a). In view of the notorious vanishing gradient problem (Bengio et al. 1994), novel architectures such as residual and densely connected networks (Huang et al. 2017; Zhang et al. 2018b) have been developed. Other methods have exploited the domain knowledge during the construction of network architecturesâ€”e.g., unfolding iterative denoising algorithms into deep convolutional neural networks (DCNN) (Wisdom et al. 2017; Hershey et al. 2014; Dong et al. 2018b; Bertocchi et al. 2020). DCNN-based image denoising techniques (Tai et al. 2017; Zhang et al. 2017a; Dong et al. 2018b) have achieved current state-of-the-art performance due to their powerful learning capabilities.

By contrast, video denoising (VD) has been a relatively underresearched problem in the literature (Ji et al. 2010; Liu and Freeman 2010). Model-based approaches toward video denoising are often built upon well-known image denoising algorithms (Varghese and Wang 2010; Mahmoudi and Sapiro 2005; Buades et al. 2016; Maggioni et al. 2012). e.g., Gaussian scale mixture (GSM) denoising (Portilla et al. 2003) was extended in Varghese and Wang (2010), nonlocal means denoising (Buades et al. 2005) became Mahmoudi and Sapiro (2005), Buades et al. (2016) and BM3D denoising (Dabov et al. 2007) evolved into BM4D for video denoising (Maggioni et al. 2012). Besides, some model-based algorithms (Dong et al. 2018a; Pablo and Jean 2018) have obtained superior denoising performance. Most recently, a flurry of deep learning-based VD methods (Mildenhall et al. 2018; Godard et al. 2018; Xue et al. 2019; Claus and van Gemert 2019; Ehret et al. 2019; Tassano et al. 2019, 2020; Davy et al. 2019) have been developed. Most of these methods first align noisy frames and then perform denoising on the aligned noisy frames with a standard DCNN. Frame alignment can be achieved by optical flow estimation network (Ranjan and Black 2017), kernel prediction network (Mildenhall et al. 2018), and MEMC-Net (Bao et al. 2019).

While recent deep learning-based VD methods address the VD problem by directly learning a mapping function from the noisy frames to the desired clean frames, these methods are heuristic and ignore the observation model that characterizes the formation of noisy frames. Inspired by the analogy between burst and video denoising (Godard et al. 2018), we propose a maximal a posterior (MAP) estimation framework for VD with DCNN denoising prior and robust multiframe alignment/fusion. By explicitly taking the frame alignment errors (due to misregistration) into the observation model, we have derived a principled MAP estimation solution for better VD performance. As both steps of single-frame denoising and multiframe fusion can be implemented by convolutional neural networks, our MAP-based VD algorithm can be unfolded into a network implementation, allowing further optimization of network parameters through end-to-end training. In addition to the excellent performance, the resulting network (denoted as MAP-VDNet) admits a Bayesian interpretation from the MAP perspective. The key technical contributions of this work are summarized as follows.

MAP-inspired network. We first propose an iterative MAP-based VD algorithm with DCNN denoising prior, where each iteration can be efficiently computed. Then, a MAP-inspired VD network is constructed by unfolding the iterative video denoising algorithm into a multistage implementation.

Robust multiframe fusion. To address the inevitable alignment errors (due to misregistration), we propose to fuse the aligned noisy frames with spatially variant weights learned by a kernel prediction network (KPN, Mildenhall et al. (2018)). Such an analogy between burst and video denoising has not been addressed in the open literature to the best of our knowledge.

Excellent denoising performance. The proposed MAP-VDNet has dramatically advanced the state-of-the-art by over 1 dB while using a comparable number of model parameters as shown in Fig. 1. The fast implementation of MAP-VDNet strikes an improved trade-off between the cost and the performance over other competing methods.

Good generalization property. In addition to VD, this work can be readily generalized to other video restoration tasks such as superresolution and compression artifact reduction. Promising experimental results for video superresolution and compression artifact reduction have verified the good generalization property of the proposed MAP-VDNet.

Related Works
Deep Image Denoising
Image denoising has been extensively studied, not only due to its wide applications. As the simplest inverse problem, numerous image priors or modeling techniques have been used to evaluate their performance for image denoising (Portilla et al. 2003; Dabov et al. 2007; Zoran and Weiss 2011; Dong et al. 2013; Gu et al. 2014). Before the popularity of deep learning, the most popular image prior is the sparse representation over learned dictionaries (Elad and Aharon 2006; Dong et al. 2013), assuming that image patches can be well approximated by a few representative atoms learned from training images. Such methods were highly related to the maximum a posterior (MAP) techniques, where the prior distributions of the sparse coefficients play a key. In addition to the sparsity, the nonlocal self-similarity of natural images was also considered in developing an improved probability model (Dong et al. 2013), leading to significant improvements. Inspired by the great successes of DCNNs for image classification, DCNNs have also been adopted for image denoising (Zhang et al. 2017a; Tai et al. 2017; Zhang et al. 2018a). In Zhang et al. (2017b), the DCNNs were proposed to learn the residual images that are the differences between clean and noisy images. More sophisticated network architectures considering long-term dependencies have also been proposed (Tai et al. 2017). Instead of designing the network as a black box, the domain knowledge has also been incorporated into the DCNNs- i.e., unfolding the iterative processes into deep networks (Dong et al. 2018b), showing highly competitive performance. Most recently, neural architecture search (NAS) has found successful application into image denoising too (e.g., Zhang et al. (2020)).

Deep Video Denoising
Deep learning for VD has recently received increasing attention. In Xue et al. (2019), a neural network with a trainable motion estimation component and a video processing component was developed for video denoising and superresolution. Another VD network based on explicit alignment is RViDeNet (Yue et al. 2020) which takes a supervised learning approach. One of the common limitations to existing learning-based VD is the heuristic concatenation of alignment and denoising subnetworks. We conjecture that there is still much room for performance improvement by pursuing end-to-end optimization. Deep learning for video denoising without explicit motion estimation has also been studied in the literature. A 3D deformable kernel was developed for video denoising in Xu et al. (2019a) aiming at more efficiently sampling the pixels across the spatio-temporal space. A nonlocal extension of the convolutional neural network (CNN) was constructed in Davy et al. (2019) for VD and demonstrated highly competent performance to the best model-based VD VNLB (Pablo and Jean 2018). A low-complexity alternative called Kalman filtering for frame-recursive video denoising has also been considered in Arias and Morel (2019). Most recently, a VNLnet (Davy et al. 2019), a FastDVDnet (Tassano et al. 2020), and a spatio-temporal pixel aggregation network (ST-PAN, Xu et al. (2020)) has achieved the current state-of-the-art performance in VD.

Deep Burst Denoising
Deep burst denoising (Godard et al. 2018) refers to the problem of image restoration in burst imaging (Hasinoff et al. 2016), which is a special case in the situation of low light imaging (e.g., nighttime environment). Burst denoising can be viewed as the middle ground between image and video denoising. It dealt with the restoration of multiple noisy frames, but the amount of camera and object motion is usually assumed to be small. Therefore, the alignment of multiple frames is faster for burst denoisingâ€”e.g., commonly adopted global registration techniques such as Homography-based (Tekalp 2015) and local feature-based such as Lucas-Kanade tracking (Lucas and Kanade 1981) are deemed sufficient. Existing works on deep burst denoising mostly fall in the framework of kernel prediction network (KPN, Mildenhall et al. (2018)). The basic idea behind the construction of KPN is to predict spatially varying kernels that can both align and restore noisy frames. However, previous works including KPN and other methods (Godard et al. 2018; Xu et al. 2019a, 2020) have not considered the general cases of large unknown misalignments which are common in VD (as illustrated in Fig. 2). How to model structure and signal-dependent noise arising from misalignment sets up the stage for this work.

Fig. 1
figure 1
Comparisons of performance and no. of parameters. Results are evaluated on DTMC-HD test set for ğœğ‘›=25

Full size image
Fig. 2
figure 2
The illustration of structural noise introduced by the alignment module. a the reference frame, b the adjacent aligned frame (note undesirable artifacts as highlighted by red boxes). Note that these artifacts are signal-dependent and behave differently from additive white Gaussian noise (Color figure online)

Full size image
Fig. 3
figure 3
The architecture of the proposed model-guided DCNN for video denoising. a The overall architecture of the proposed network, b the architecture of the alignment module, c the architecture of the robust multi-frame fusion, and d the architecture of the encoding and decoding block of the U-net denoiser

Full size image
Proposed MAP Estimator for Video Denoising
MAP Estimator with i.i.d Gaussian Likelihood
Table 1 The average PSNR (dB) and SSIM results of the variants of the proposed method for noise level ğœğ‘›=25
Full size table
For VD, we aim at recovering each clean frame ğ‘¥ğ‘¥âˆˆâ„ğ‘ (the frame index is omitted for simplicity) from a set of adjacent noisy frames ğ‘¦ğ‘¦ğ‘—, ğ‘—=âˆ’ğ½,â€¦,ğ½. Unlike image denoising, a noisy frame can have multiple observations because it is highly correlated with adjacent frames due to temporal redundancy. A commonly used video frame observation model can be expressed as

ğ‘¦ğ‘¦ğ‘—=ğ–Ìƒ ğ‘—ğ‘¥ğ‘¥+ğ‘›ğ‘›ğ‘—,ğ‘—=âˆ’ğ½,â€¦,ğ½,
(1)
where ğ–Ìƒ ğ‘—âˆˆâ„ğ‘Ã—ğ‘ denotes the warping matrix (or an operator) that maps from the frame of interest ğ‘¥ğ‘¥ to its adjacent frame ğ‘¥ğ‘¥ğ‘— (e.g., optical flow (Ranjan and Black 2017)), and ğ‘›ğ‘›ğ‘—âˆˆâ„ğ‘ is the additive Gaussian noise. In the above formulation, ğ–Ìƒ 0 is simply an identity matrix for the central frame at ğ‘—=0. The above observation model has been adopted in Kokkinos and Lefkimmiatis (2019) for burst denoising. However, the tangle of the warping matrices ğ–Ìƒ ğ‘— with ğ‘¥ğ‘¥ making the optimization of ğ‘¥ğ‘¥ very difficult, i.e., one has to solve a sequence of large matrix inverse problems. In this paper, we propose the following noisy frame observation model, as

ğ‘¥ğ‘¥=ğ–ğ‘—ğ‘¥ğ‘¥ğ‘—=ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—+ğ‘›ğ‘›~ğ‘—, ğ‘—=âˆ’ğ½,â€¦,ğ½,
(2)
where we have used ğ‘¦ğ‘¦ğ‘—=ğ‘¥ğ‘¥ğ‘—+ğ‘›ğ‘›ğ‘—, ğ‘›ğ‘›~ğ‘—=âˆ’ğ–ğ‘—ğ‘›ğ‘›ğ‘—, and ğ–ğ‘— denotes the warping matrix from ğ‘¥ğ‘¥ğ‘— to the central frame ğ‘¥ğ‘¥. For simplicity, we assume ğ‘›ğ‘›~ğ‘— is still Gaussian with variance ğœ2ğ‘›. Based on such multi-frame observation model and the assumption that ğ‘¥ğ‘¥ and ğ–ğ‘— are independent, we propose the following Maximum a Posterior (MAP) estimation

(ğ‘¥ğ‘¥,ğ–ğ‘—)=argmaxğ‘¥ğ‘¥,ğ–ğ‘—âˆ‘ğ‘—=âˆ’ğ½ğ½logğ‘ƒ(ğ‘¦ğ‘¦ğ‘—|ğ‘¥ğ‘¥,ğ–ğ‘—)+logğ‘ƒ(ğ‘¥ğ‘¥)+logğ‘ƒ(ğ–ğ‘—),
(3)
where ğ‘ƒ(ğ‘¦ğ‘¦ğ‘—|ğ‘¥ğ‘¥) is the Gaussian likelihood, which can be expressed as

ğ‘ƒ(ğ‘¦ğ‘¦ğ‘—|ğ‘¥ğ‘¥,ğ–ğ‘—)âˆexp(âˆ’1ğœ2ğ‘›||ğ‘¥ğ‘¥âˆ’ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—||22).
(4)
For the prior terms ğ‘ƒ(ğ‘¥ğ‘¥) and ğ‘ƒ(ğ–ğ‘—), we use a general form to describe

ğ‘ƒ(ğ‘¥ğ‘¥)âˆexp(âˆ’ğ›¾Î©(ğ‘¥ğ‘¥)),
(5a)
ğ‘ƒ(ğ–ğ‘—)âˆexp(âˆ’ğ›¿Î¦1(ğ–ğ‘—)),
(5b)
where Î©(â‹…) and Î¦1(â‹…) denote energy functions related to ğ‘¥ğ‘¥ and ğ–ğ‘— respectively, and ğ›¾ and ğ›¿ are the corresponding weights. By substituting the Gaussian likelihood into Eq. (4) and prior terms of Eq. (5) in the MAP estimation of Eq. (3), we can obtain the following objective function for multiframe denoising

(ğ‘¥ğ‘¥,ğ–ğ‘—)=argminğ‘¥ğ‘¥,ğ–ğ‘—âˆ‘ğ‘—=âˆ’ğ½ğ½ğœ‚||ğ‘¥ğ‘¥âˆ’ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—||22+ğ›¾Î©(ğ‘¥ğ‘¥)+ğ›¿Î¦1(ğ–ğ‘—),
(6)
where ğœ‚=1/ğœ2ğ‘›. Regarding the prior term ğ‘ƒ(ğ‘¥ğ‘¥), instead of mathematical construction of Î©(ğ‘¥ğ‘¥), we propose to adopt the DCNN-based image prior (Dong et al. 2018b) by introducing an auxiliary variablea ğ‘£ğ‘£, the objective function of Eq. (6) could be translated into

(ğ‘¥ğ‘¥,ğ‘£ğ‘£,ğ–ğ‘—)=argminğ‘¥ğ‘¥,ğ‘£ğ‘£,ğ–ğ‘—âˆ‘ğ‘—=âˆ’ğ½ğ½ğœ‚||ğ‘¥ğ‘¥âˆ’ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—||22+ğœ†||ğ‘¥ğ‘¥âˆ’ğ‘£ğ‘£||22+ğ›¾Î©(ğ‘£ğ‘£)+ğ›¿Î¦1(ğ–ğ‘—).
(7)
It could be observed from Eq. (7) that the multiframe denoising problem could be solved by iteratively optimizing the following three subproblems

ğ–ğ‘—=argminğ–ğ‘—âˆ‘ğ‘—=âˆ’ğ½ğ½ğœ‚||ğ‘¥ğ‘¥âˆ’ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—||22+ğ›¿Î¦1(ğ–ğ‘—),
(8a)
ğ‘£ğ‘£=argminğ‘£ğ‘£ğœ†||ğ‘¥ğ‘¥âˆ’ğ‘£ğ‘£||22+ğ›¾Î©(ğ‘£ğ‘£),
(8b)
ğ‘¥ğ‘¥=argminğ‘¥ğ‘¥âˆ‘ğ‘—=âˆ’ğ½ğ½ğœ‚||ğ‘¥ğ‘¥âˆ’ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—||22+ğœ†||ğ‘¥ğ‘¥âˆ’ğ‘£ğ‘£||22.
(8c)
Regarding the ğ–ğ‘—-subproblem, instead of estimating ğ–ğ‘— with a specific prior, we propose to estimate ğ–(ğ‘¡+1)ğ‘— from ğ‘¥ğ‘¥ and ğ‘¦ğ‘¦ğ‘— using a DCNN function. Since ğ‘¥ğ‘¥ is not available, we use the current estimation ğ‘¥ğ‘¥(ğ‘¡) to compute ğ–ğ‘— as follows

ğ–ğ‘—(ğ‘¡+1)=ğ‘”1(ğ‘¥ğ‘¥(ğ‘¡),ğ‘¦ğ‘¦ğ‘—),
(9)
where ğ‘”1(â‹…) represents a DCNN function. Besides, as demonstrated in Eq. (8b), the ğ‘£ğ‘£-subproblem could be considered as a single-frame denoising problem, and ğ‘£ğ‘£ can be updated as

ğ‘£ğ‘£(ğ‘¡+1)=ğ‘“(ğ‘¥ğ‘¥(ğ‘¡)),
(10)
where ğ‘“(â‹…) denotes the DCNN denoising function. Concerning the ğ‘¥ğ‘¥-subproblem, since both the terms are ğ‘™2-norm in Eq. (8c), the above objective function of ğ‘¥ğ‘¥-subproblem admits a closed-form solution as

ğ‘¥ğ‘¥(ğ‘¡+1)=(1âˆ’ğ‘)ğ‘¦ğ‘¦â¯â¯â¯(ğ‘¡+1)+ğ‘ğ‘£ğ‘£(ğ‘¡+1),
(11)
where ğ‘=ğœ†(2ğ½+1)ğœ‚+ğœ† and ğ‘¦ğ‘¦â¯â¯â¯(ğ‘¡+1)=1(2ğ½+1)âˆ‘ğ½ğ‘—=âˆ’ğ½ğ–ğ‘—(ğ‘¡+1)ğ‘¦ğ‘¦ğ‘—. In principle, through the iterative computations of ğ–ğ‘—, ğ‘£ğ‘£ and ğ‘¥ğ‘¥, the multiframe denoising problem can be efficiently solved.

Table 2 The PSNR(dB) and SSIM performance comparison for different values of J at the noise level of ğœğ‘›=25
Full size table
Although the warping matrix ğ–ğ‘— can be estimated by any existing motion compensation method, its computational complexity is often prohibitive. For example, popular deep motion compensation methods (Ranjan and Black 2017; Wang et al. 2019) tend to use a multi-scale architecture to deal with large displacements between frames. Even with a fast optical flow estimation method such as Kroeger et al. (2016), iterative computation of the alignment matrix would involve frequent data type conversion between GPU and CPU, which renders an additional burden to limited computational resources. To tackle this problem, we opt to update ğ–ğ‘— just once in our iterations, and then iteratively optimize ğ‘¥ğ‘¥ and ğ‘£ğ‘£ with a fixed ğ–ğ‘—. In practice, we use ğ‘¦ğ‘¦0 to initialize ğ‘¥ğ‘¥(ğ‘¡) in Eq. (9) and ğ–ğ‘— can be initially estimated by ğ–ğ‘—=ğ‘”1(ğ‘¦ğ‘¦0,ğ‘¦ğ‘¦ğ‘—). Then the MAP estimator leads to a simple multiframe denoising schemeâ€”i.e., iteratively computing

ğ‘¥ğ‘¥(ğ‘¡+1)=(1âˆ’ğ‘)ğ‘¦ğ‘¦â¯â¯â¯+ğ‘ğ‘“(ğ‘¥ğ‘¥(ğ‘¡)),
(12)
where ğ‘¦ğ‘¦â¯â¯â¯=1(2ğ½+1)âˆ‘ğ½ğ‘—=âˆ’ğ½ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—. Regarding the starting point ğ‘¥ğ‘¥(0) in Eq. (12), a natural selection is the weighted average of the aligned framesâ€”i.e., ğ‘¥ğ‘¥(0)=ğ‘¦ğ‘¦â¯â¯â¯. The reason is that the averaging of the m noisy observations of a frame will reduce the noise variance from ğœ2ğ‘› to ğœ2ğ‘›ğ‘š (Chang et al. 2000). Thus, if the temporal alignment is perfect, the averaging of the aligned noisy frames will significantly reduce the noise variance by a factor equaling to the total number of aligned noisy frames. However, due to the inevitable misalignment errors from one-time alignment iteration, further refinement of MAP estimation has to be performed based on the baseline method. We will refine the noise observation model and derive a new MAP estimation for the updated observation model next.

MAP Estimator with Non-i.i.d Gaussian Likelihood
In Eq. (4), we assume that the warped noise ğ–ğ‘—ğ‘›ğ‘›ğ‘— is still Gaussian with variance ğœ2ğ‘›. However, when the motion estimation is inaccurate, some pixels in the adjacent frame after alignment might become misaligned with the reference frame causing undesirable artifacts as shown in Fig. 2. Those artifacts are signal-dependent and behave differently from additive white Gaussian noise. Therefore, the noise ğ‘›ğ‘›~ğ‘— in the aligned noisy frame is more complex than identically distributed (i.i.d) Gaussian. To address this issue, we propose a non-i.i.d Gaussian likelihood for multiframe denoising inspired by the analogy between burst denoising and video denoising (Mildenhall et al. 2018). To the best of our knowledge, such an adversary effect of motion estimation on video denoising has not been explored in the open literature.

To properly take such systematic errors into account, we introduce a new alignment error term ğ‘’ğ‘’ğ‘— to represent the potential deviation of an aligned frame from the reference frame. More specifically, we propose to rewrite the observation model in Eq. (2) as

ğ‘¥ğ‘¥=ğ–ğ‘—ğ‘¥ğ‘¥ğ‘—+ğ‘’ğ‘’ğ‘—=ğ‘¦ğ‘¦~ğ‘—+ğ‘›ğ‘›~ğ‘—+ğ‘’ğ‘’ğ‘—, ğ‘—=âˆ’ğ½,â€¦,ğ½,
(13)
where ğ‘¦ğ‘¦~ğ‘—=ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—, and ğ‘’ğ‘’ğ‘— denotes the alignment error. Generally, ğ‘¥ğ‘¥âˆ’ğ‘¦ğ‘¦~ğ‘— generally no longer satisfies the i.i.d Gaussian assumption due to the interference from alignment error term ğ‘’ğ‘’ğ‘—. Instead, it is more appropriate to pursue an adaptive estimation about the standard deviation of spatially varying noise on a pixel-by-pixel basis. It follows that we can extend the i.i.d Gaussian likelihood term in Eq. (4) into its non-i.i.d version, as

ğ‘ƒ(ğ‘¦ğ‘¦ğ‘—|ğ‘¥ğ‘¥,ğ–ğ‘—,ğœğœğ‘—)=âˆğ‘ğ‘ƒ(ğ‘¦ğ‘¦ğ‘—,ğ‘|ğ‘¥ğ‘¥ğ‘,ğ‘¦ğ‘¦~ğ‘—,ğ‘,ğœğ‘—,ğ‘)=âˆğ‘1ğœğ‘—,ğ‘2ğœ‹â€¾â€¾â€¾âˆšexp[âˆ’âˆ‘ğ‘1ğœ2ğ‘—,ğ‘(ğ‘¥ğ‘¥ğ‘âˆ’ğ‘¦ğ‘¦~ğ‘—,ğ‘)2],
(14)
where ğ‘¦ğ‘¦~ğ‘—,ğ‘ and ğ‘¥ğ‘¥ğ‘ denote the p-th elements of ğ‘¦ğ‘¦~ğ‘— and ğ‘¥ğ‘¥, respectively, and ğœğ‘—,ğ‘ is the per-pixel noise standard deviation after alignment. Obviously, ğœğ‘—,ğ‘ is relevant to the warping matrix ğ–ğ‘—, thus we propose to jointly estimate ğ–ğ‘— and ğœğœğ‘— under the MAP estimation framework as follows

(ğ‘¥ğ‘¥,ğ–ğ‘—,ğœ‚ğ‘—)=argmaxğ‘¥ğ‘¥,ğ–ğ‘—,ğœ‚ğ‘—âˆ‘ğ‘—=âˆ’ğ½ğ½logğ‘ƒ(ğ‘¦ğ‘¦ğ‘—|ğ‘¥ğ‘¥,ğ–ğ‘—,ğœ‚ğ‘—)+logğ‘ƒ(ğ‘¥ğ‘¥)+logğ‘ƒ(ğ–ğ‘—,ğœ‚ğ‘—),
(15)
where ğœ‚ğœ‚ğ‘—=1/ğœğœğ‘—. Similarly, by substituting the non-i.i.d likelihood term into the MAP estimator of Eq. (15) and introducing a denoising prior term, we can extend the objective function of Eq. (7) into the following spatially adaptive formulation

(ğ‘¥ğ‘¥,ğ‘£ğ‘£,ğ–ğ‘—,ğœ‚ğœ‚ğ‘—)=argminğ‘¥ğ‘¥,ğ‘£ğ‘£,ğ–ğ‘—,ğœ‚ğœ‚ğ‘—âˆ‘ğ‘—=âˆ’ğ½ğ½||Î£Î£ğ‘—(ğ‘¥ğ‘¥âˆ’ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—)||22+ğ›¾Î©(ğ‘£ğ‘£)+ğœ†||ğ‘¥ğ‘¥âˆ’ğ‘£ğ‘£||22+ğ›¿Î¦2(ğ–ğ‘—,ğœ‚ğœ‚ğ‘—)+log(ğœ‚ğœ‚ğ‘—2ğœ‹â€¾â€¾â€¾âˆš),
(16)
where Î¦2(â‹…) represents the energy function related to joint distribution of ğ–ğ‘— and ğœ‚ğœ‚ğ‘—, and the weighting matrix Î£Î£ğ‘—=diag(ğœ‚ğ‘—,ğ‘)âˆˆâ„ğ‘Ã—ğ‘ is a diagonal matrix whose diagonal entries (ğœ‚ğ‘—,ğ‘=1/ğœğ‘—,ğ‘) represent the reciprocal of the standard deviation for each individual noisy pixel. Similar to Eq. (7), we can solve the above optimization problem by the following formulas

(ğ–ğ‘—,ğœ‚ğœ‚ğ‘—)=argminğ–ğ‘—,ğœ‚ğœ‚ğ‘—âˆ‘ğ‘—=âˆ’ğ½ğ½||Î£Î£ğ‘—(ğ‘¥ğ‘¥âˆ’ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—)||22+ğ›¿Î¦2(ğ–ğ‘—,ğœ‚ğœ‚ğ‘—)+log(ğœ‚ğœ‚ğ‘—2ğœ‹â€¾â€¾â€¾âˆš),
(17a)
ğ‘£ğ‘£=argminğ‘£ğ‘£ğœ†||ğ‘¥ğ‘¥âˆ’ğ‘£ğ‘£||22+ğ›¾Î©(ğ‘£ğ‘£),
(17b)
ğ‘¥ğ‘¥=argminğ‘¥ğ‘¥âˆ‘ğ‘—=âˆ’ğ½ğ½||Î£Î£ğ‘—(ğ‘¥ğ‘¥âˆ’ğ–ğ‘—ğ‘¦ğ‘¦ğ‘—)||22+ğœ†||ğ‘¥ğ‘¥âˆ’ğ‘£ğ‘£||22.
(17c)
Instead of constructing a specific prior of Î¦2(ğ–ğ‘—,ğœ‚ğœ‚ğ‘—), we propose to jointly estimate ğ–ğ‘— and ğœ‚ğœ‚ğ‘— from ğ‘¥ğ‘¥ and ğ‘¦ğ‘¦ğ‘— with a DCNN function. Considering the unavailability of ğ‘¥ğ‘¥, we exploit the intermediate estimation ğ‘¥ğ‘¥(ğ‘¡). Similarly, we initialize ğ‘¥ğ‘¥(ğ‘¡) with ğ‘¦ğ‘¦0, and then conduct one iteration to save the computational overhead. Therefore, ğ–ğ‘— and ğœ‚ğœ‚ğ‘— can be estimated as

(ğ–ğ‘—,ğœ‚ğœ‚ğ‘—)=ğ‘”2(ğ‘¦ğ‘¦0,ğ‘¦ğ‘¦ğ‘—),
(18)
where ğ‘”2(â‹…) is a DCNN function to estimate ğ–ğ‘— and ğœ‚ğœ‚ğ‘—. Then ğ‘¥ğ‘¥ could be updated as the following

ğ‘¥ğ‘¥(ğ‘¡+1)==(âˆ‘ğ‘—=âˆ’ğ½ğ½Î£Î£âŠ¤ğ‘—Î£Î£ğ‘—+ğœ†ğˆ)âˆ’1(âˆ‘ğ‘—=âˆ’ğ½ğ½Î£Î£âŠ¤ğ‘—Î£Î£ğ‘—ğ‘¦ğ‘¦~ğ‘—+ğœ†ğ‘“(ğ‘¥ğ‘¥(ğ‘¡))),(ğˆâˆ’Î›Î›)ğ‘¦ğ‘¦â¯â¯â¯â€²+Î›Î›ğ‘“(ğ‘¥ğ‘¥(ğ‘¡)),
(19)
where ğ‘¦ğ‘¦â¯â¯â¯â€²=âˆ‘ğ½ğ‘—=âˆ’ğ½ğ‚ğ‘—ğ‘¦ğ‘¦~ğ‘— denotes the spatially adaptive fusion of the aligned noisy frames, ğ‚ğ‘—=diag(ğœ‚2ğ‘—,ğ‘âˆ‘ğ½ğ‘—=âˆ’ğ½ğœ‚2ğ‘—,ğ‘)âˆˆâ„ğ‘Ã—ğ‘ and Î›Î›=diag(ğœ†âˆ‘ğ½ğ‘—=âˆ’ğ½ğœ‚2ğ‘—,ğ‘+ğœ†)âˆˆâ„ğ‘Ã—ğ‘ are diagonal matrices. The fusion matrix ğ‚ğ‘— guides the spatially adaptive averaging of the aligned noisy frames, and thus Eq. (19) effectively implements the idea of robust multi-frame fusion, which is conceptually similar to per-pixel kernel prediction in burst denoising (Mildenhall et al. 2018). From Eq. (19), we can see that the MAP estimator with non-i.i.d Gaussian likelihood also leads to a simple multiframe denoising scheme. For the initial estimate ğ‘¥ğ‘¥(0) in Eq. (19), we use the adaptively averaged frame as the starting point, i.e., ğ‘¥ğ‘¥(0)=ğ‘¦ğ‘¦â¯â¯â¯â€². Comparing with Eq. (12), we can see that the updating scheme of Eq. (12) is a special case of Eq. (19) by setting ğ‚ğ‘— and Î›Î› as fixed scalar variables. Regarding the per-pixel fusion filters ğœ‚ğœ‚ğ‘—, we will use the deep network to adaptively optimize the fusion weights as described in the next section. Through the estimation of filters ğœ‚2ğ‘—,ğ‘âˆˆâ„ğ‘(ğ‘—=âˆ’ğ½,â€¦,ğ½), the diagonal matrices ğ‚ğ‘— and Î›Î› can be efficiently computed. The proposed MAP-based video denoising algorithm can be summarized in Algorithm 1.

figure a
In the conventional wisdom of model-based denoising, an implementation of Algorithm 1 requires many iterations to converge. Moreover, it will be difficult to jointly optimize the denoiser ğ‘“(â‹…), the alignment operators ğ–ğ‘—, the adaptive fusion weights ğ‚ğ‘—, and ğœ† (note that most parameters in model-based denoising are often hand-crafted). Such observation motivates us to implement the proposed MAP estimator with a deep neural network, and so all components along with their parameters can be jointly optimized through end-to-end training. In particular, regarding the per-pixel fusion weights ğ‚ğ‘— that plays an important role in robust multiframe fusion, we will use the deep network to adaptively optimize the fusion weights as described in the next section.

Deep Neural Network Implementation
Although the frame alignment, multiframe fusion, and image denoising can all be implemented by deep neural networks (DNN), an exact DNN-based implementation of the proposed MAP-based VD algorithm is not straightforward (Hershey et al. 2014). Unfolding MAP-based VD algorithm takes more effort than its image denoising counterpart (Dong et al. 2018b). The overall network architecture of the unfolded MAP-based Video Denoising Network, dubbed MAP-VDNet, is presented in Fig. 3a. Note that in our current implementation, the ğ‘”2(â‹…) function in Eq. (18) is realized by a two-step procedure, which illustrated as the alignment module to compute ğ–ğ‘—ğ‘¦ğ‘¦ğ‘— and fusion module to estimate ğœ‚ğœ‚ğ‘—, respectively. As shown in Fig. 3a, the input noisy frames ğ‘¦ğ‘¦ğ‘— are first aligned by an alignment module, which can be any existing alignment method in principle. The aligned noisy frames ğ‘¦ğ‘¦~ğ‘— are then fed into the fusion module, which performs a spatially adaptive fusion of ğ‘¦ğ‘¦~ğ‘—. The fused noisy frame ğ‘¦ğ‘¦â¯â¯â¯â€² as the initial estimate of ğ‘¥ğ‘¥ is then passed to the image denoiser ğ‘“(â‹…). The output of the denoiser ğ‘“(â‹…) is finally combined with the fused frame ğ‘¦ğ‘¦â¯â¯â¯â€² for producing an improved estimate of ğ‘¥ğ‘¥. Such a process will be iterated for several stages corresponding to the unfolding of the main loop in Algorithm 1. Thus, the proposed MAP-VDNet as shown in Fig. 3a provides an exact implementation of Algorithm 1. Next, we will provide the details of each module.

Fig. 4
figure 4
The average PSNR curves as a function of the no. of stages T of the proposed MAP-VDNet for ğœğ‘›=25

Full size image
Table 3 The PSNR(dB) and SSIM performance comparison for the effect of dense connections between the denoisers when ğœğ‘›=25
Full size table
Fig. 5
figure 5
Denoising intermediate visual and PSNR results of aligned frames ğ‘¦ğ‘¦~ğ‘—(ğ‘—=âˆ’3,â€¦,3), the average fused frame ğ‘¦ğ‘¦â¯â¯â¯, the spatially adaptive fused frame ğ‘¦ğ‘¦â¯â¯â¯â€², and outputs from all stages ğ‘¥ğ‘¥(ğ‘¡)(ğ‘¡=1,â€¦,5) for a noisy frame of Foreman video of ASU test set with noise level ğœğ‘›=25. Note that the average fused frame ğ‘¦ğ‘¦â¯â¯â¯ would generate undesirable artifacts caused by alignment errors, while the proposed spatially adaptive fused frame ğ‘¦ğ‘¦â¯â¯â¯â€² could restore shaper edges (Please zoom in to see better details)

Full size image
Alignment Module
This network implements step (2) in Algorithm 1. The adjacent 2J noisy frames need to be aligned with the frame of interest ğ‘¦ğ‘¦ first. As shown in Fig. 3b, each frame ğ‘¦ğ‘¦ğ‘— and the reference frame ğ‘¦ğ‘¦ is first used to compute the optical flow, then the estimated optical flow fields are then used to warp the noisy frame ğ‘¦ğ‘¦ğ‘— to the reference frame ğ‘¦ğ‘¦. Either deep learning-based or conventional optical flow estimation methods can be used to estimate the optical flows. In Sect. 5.2, we have used two optical flow estimation methodsâ€”i.e., the Spynet (Ranjan and Black 2017) and the fast inverse search-based method (Kroeger et al. 2016) (corresponding to the name MAP-VDNet-Fast in Fig. 1) to compare the effect of different alignment modules. The estimated optical flow fields are then used to warp the noisy frame ğ‘¦ğ‘¦ğ‘— to the reference frame ğ‘¦ğ‘¦ by the spatial transformer network of (Jaderberg et al. 2015) before fusion.

Table 4 The average PSNR/SSIM video denoising results on Vimeo, ASU, and DTMC-HD dataset at different noise levels (boldface highlights the best)
Full size table
Fusion Module
In addition to concatenation-based fusion, we have proposed two fusion strategies, as discussed in Sect. 3- i.e., the uniform average fusion ğ‘¦ğ‘¦â¯â¯â¯=1(2ğ½+1)âˆ‘ğ½ğ‘—=âˆ’ğ½ğ‘¦ğ‘¦~ğ‘— and the spatially adaptive fusion ğ‘¦ğ‘¦â¯â¯â¯â€²=âˆ‘ğ½ğ‘—=âˆ’ğ½ğ‚ğ‘—ğ‘¦ğ‘¦~ğ‘—. The former fusion method is simply the uniform average of all aligned noisy frames; the later fusion method performs per-pixel adaptive fusion according to the noise variance. For per-pixel adaptive fusion, we propose to estimate the filters ğœ‚2ğ‘—,ğ‘ by a deep neural network, which is analogous to the kernel prediction network (KPN) used in burst denoising (Mildenhall et al. 2018). Similar to KPN, the objective of fusion module is to derive spatially-adaptive fusion weights ğ‚ğ‘—. Unlike KPN (Mildenhall et al. 2018), we only need to estimate the per-pixel 1Ã—1 kernels as the frames have been aligned by the optical flow-based alignment method.

As shown in Fig. 3c, each aligned frame ğ‘¦ğ‘¦~ğ‘— and the central frame ğ‘¦ğ‘¦ are first concatenated and fed into a DCNN to predict 1Ã—1 fusion kernels. As the input frames have been aligned, we employed a compact DCNN as the backbone to predict filters ğœ‚2ğ‘—,ğ‘, which contains five residual blocks, each residual block consists of two convolutional layers with 3Ã—3 kernels and ReLU nonlinearity to generate 32-channel feature maps. Note that all 2J DCNN modules share the same parameters. Then spatially-adaptive fusion weights ğ‚ğ‘—=diag(ğœ‚2ğ‘—,ğ‘âˆ‘ğ½ğ‘—=âˆ’ğ½ğœ‚2ğ‘—,ğ‘) could be calculated using the filters ğœ‚2ğ‘—,ğ‘. Finally, the aligned frames ğ‘¦ğ‘¦~ğ‘— are averaged using fusion weights ğ‚ğ‘— to generate the fused frame ğ‘¦ğ‘¦â¯â¯â¯â€². Through the estimation of a spatially varying kernel, our multiframe fusion becomes more robust in the presence of misregistration errors and other nonuniformly distributed noise (e.g., signal-dependent compression artifacts).

Denoising Module
This module implements the denoising function ğ‘“(â‹…) in Algorithm 1 and in principle, any existing image denoising network can be used as the denoising module. Here, we opt to use the U-net of Ronneberger et al. (2015) as the backbone network for the denoiser, which consists of an encoder and a decoder. The encoder and decoder contain five encoding blocks (EB) and four decoding blocks (DB), respectively. As shown in Fig. 3d, except the last EB, each EB consists of four convolutional layers with 3Ã—3 kernels to generate 64-channel feature maps and followed by a downsampling layer to reduce the spatial resolution by a scaling factor of two, while the last EB only contains four convolutional layers to produce 64-channel feature maps and does not conduct downsampling. Each DB contains five convolutional layers. The first layer uses 1Ã—1 kernels to reduce the number of feature channels from 128 to 64, and other layers produce the 64-channel feature maps with 3Ã—3 kernels. Each DB is followed by a deconvolution layer to increase the spatial resolution of the feature maps with a scaling factor of two. To compensate for the loss of spatial information, the upsampled feature maps are concatenated with the feature maps of the same spatial dimension from the encoder. To save the total number of parameters, all denoising networks in the consecutive T stages are enforced to share the same network parameters.

Following Algorithm 1, the T denoising networks process the intermediate image ğ‘¥ğ‘¥(ğ‘¡) independently and thus cannot exploit the features from the previous denoising networks. Inspired by the success of the densenet (Huang et al. 2017), we propose to connect those feature maps from the previous denoisers to the following denoisers. As shown in Fig. 3a, the feature maps of the first encoding and last decoding blocks are connected to those of the subsequent denoisers. These long skip connections also help alleviate the notorious vanishing gradient problem (Bengio et al. 1994).

Fig. 6
figure 6
Denoising results for a noisy frame from Vimeo test set with noise level ğœğ‘›=25. a Original frame; denoised frame by b V-BM4D (Maggioni et al. 2012) (31.66 dB, 0.7169), c RTA-LSM (Dong et al. 2018a) (32.27 dB, 0.7429), d VNLB (Pablo and Jean 2018) (32.27 dB, 0.7427), e TOFlow (Xue et al. 2019) (35.13 dB, 0.8925), f TOFlow-EN (36.75 dB, 0.9484), g ViDeNN (Claus and van Gemert 2019) (37.65 dB, 0.9533), h VNLnet (Davy et al. 2019) (36.63 dB, 0.9462), i FastDVDnet (Tassano et al. 2020) (37.25 dB, 0.9546), and j MAP-VDNet (39.30 dB, 0.9627)

Full size image
Network Training and Extensions
When using the deep optical flow estimation method for frame alignment, the alignment module is first pretrained on the training images of the Vimeo dataset (Xue et al. 2019). Then we jointly train the overall network by minimizing the following loss function

Î˜Î˜=argminÎ˜Î˜âˆ‘ğ‘–=1ğ‘€âˆ‘ğ‘—=âˆ’ğ½ğ½||îˆ²(ğ‘¦ğ‘¦ğ‘–,ğ‘—;Î˜Î˜)âˆ’ğ‘¥ğ‘¥ğ‘–||1
(20)
where ğ‘¥ğ‘¥ğ‘– denotes the ith central clean frame to be recovered from its adjacent noisy frames ğ‘¦ğ‘¦ğ‘–,ğ‘—, ğ‘—=âˆ’ğ½,â€¦,ğ½ and îˆ²(â‹…;Î˜Î˜) denotes the function of the overall VD network with parameters Î˜Î˜. The ADAM optimizer (Kingma and Ba 2015) was employed to train the network with parameters setting ğ›½1=0.9, ğ›½2=0.999, ğœ–=10âˆ’8, and the learning rate 10âˆ’4. The proposed network is implemented under PyTorch platform and trained using 4 Nvidia Titan XP GPUs.

In addition to synthetic noise, we have conducted real-world raw video denoising experiments on the CRVD dataset (Yue et al. 2020). Specifically, we use raw data of 11 indoor scenes from CRVD dataset as our dataset, we randomly choose video frames of scene 8 and scene 9 as the test dataset, and video frames of other dynamic indoor scenes as the training dataset. In our current implementation, we first pack the Bayer images into 4 channels (RGBG) and then clip these packed frames into image patches with the size of 128Ã—128. Finally, we derive 72,900 sequences for training and other 16,200 sequences for testing, each sequence contains 7 temporal-sequential real noisy image patches and a clean reference image patch, and there is no overlap between the training dataset and the test dataset.

Furthermore, we have extended the proposed model to other video processing tasks such as video superresolution and compressed video artifact reduction. For the task of video superresolution, we use the widely used training datasetâ€”e.g., Vimeo dataset (Xue et al. 2019). Following the common setting (Jo et al. 2018; Yi et al. 2019; Isobe et al. 2020), the low-resolution frame was generated by first Gaussian filtering with a standard deviation of 1.6 and then Ã—4 downsampling. Our network was trained on RGB channels and tested on the Y channel and RGB channels for video superresolution. For the task of compressed video artifact reduction, Vid70 (Yang et al. 2018) is a commonly used dataset in previous studies. To make a fair comparison, we apply the same training set and test set as Yang et al. (2018): 60 sequences in Vid70 for training and the remaining 10 sequences for testing. Besides, all compressed sequences were generated by HEVC standard, using HM 16.20 LDP mode with ğ‘„ğ‘ƒ=37 and 42. Similar to other competing algorithms, our network was trained and tested on the Y channel for compressed video artifact reduction.

Fig. 7
figure 7
Denoising results for a noisy frame of Stefan video of ASU test set with noise level ğœğ‘›=50. a Original frame; denoised frame by b V-BM4D (Maggioni et al. 2012) (25.16 dB, 0.7965), c RTA-LSM (Dong et al. 2018a) (25.98 dB, 0.8731), d VNLB (Pablo and Jean 2018) (26.89 dB, 0.8885), e TOFlow (Xue et al. 2019) (26.70 dB, 0.8808), f TOFlow-EN (28.10 dB, 0.9155), g ViDeNN (Claus and van Gemert 2019) (26.74 dB, 0.8905), h VNLnet (Davy et al. 2019) (26.10 dB, 0.8751), i FastDVDnet (Tassano et al. 2020) (26.87 dB, 0.8951), and j MAP-VDNet (29.12 dB, 0.9312)

Full size image
Experimental Results
Experimental Setup
The proposed MAP-VDNet was trained using the Vimeo dataset (Xue et al. 2019), which consists of 91,701 sequences collected from 38,990 video clips. All collected sequences are divided into the training and test parts. Here, we consider two types of noiseâ€”i.e., additive white Gaussian and signal-dependent noise. In addition to the Vimeo dataset, we have also tested the proposed MAP-VDNet method on two sets of videos with different resolutions that are randomly selected from Derfâ€™s Test Media Collection,Footnote1 denoted as ASU and DTMC-HD datasets, respectively. The sequences in the ASU dataset have a spatial resolution of 352Ã—288 with a maximum of 300 frames, and the sequences in the DTMC-HD dataset have been downsampled to a resolution of 960Ã—540 with 75 frames. Additive Gaussian noises with different variances were added to the clean frames to simulate noisy frames. Our network has been trained and tested on the RGB channels for VD. In our implementation, we adopted two optical flow estimation methods, i.e., the deep learning-based Spynet method (Ranjan and Black 2017) and the fast optical flow estimation method based on dense inverse search (Kroeger et al. 2016) which has been integrated into the OpenCV library. More comparative results are available at: https://see.xidian.edu.cn/faculty/wsdong/Projects/MAP-VDNet.htm.

Ablation Study
The effect of the core components To verify the effect of the core building blocks (i.e., frame alignment, adaptive fusion, and the denoiser module) of the proposed method on the denoising performance, we have implemented several variants of the proposed method as summarized below.

MAP-VDNet: it denotes the proposed method that used the Spynet (Ranjan and Black 2017) for frame alignment, spatially adaptive fusion, and U-net denoiser.

MAP-VDNet-Fast: it denotes the proposed method that used the fast optical flow estimation method (Kroeger et al. 2016), spatially adaptive fusion, and U-net denoiser.

MAP-VDNet-NAF: it denotes the proposed method that used the Spynet (Ranjan and Black 2017) for frame alignment, non-adaptive fusion, and U-net denoiser.

MAP-VDNet-DnCNN: it denotes the proposed method that used the Spynet (Ranjan and Black 2017) for frame alignment, spatially adaptive fusion, and DnCNN denoiser (Zhang et al. 2017a).

The above variants of the proposed method contain five denoising stages, i.e., ğ‘‡=5. To verify the effectiveness of the proposed MAP-VDNet, we also implemented the following video denoising method,

VD-Unet: it denotes the video denoising method that first uses the Spynet (Ranjan and Black 2017) to align the noisy frames and then sends the aligned 2ğ½+1 frames to the U-net denoiser to output the denoised central frame.

VD-Unet-Fast: it denotes the video denoising method that first uses the fast optical flow estimation method (Kroeger et al. 2016) to align the noisy frames and then sends the aligned 2ğ½+1 frames to the U-net denoiser to output the denoised central frame.

In the above methods, we set ğ½=3. The average PSNR and SSIM results of these variants of the proposed method for noise level 25 are shown in Table 1. From Table 1 one can see that all variants of the proposed method outperform the VD-Unet and VD-Unet-Fast methods. The improvement over the VD-Unet-Fast method is much larger, verifying the effectiveness of the proposed MAP-VDNet method. When using fast optical flow estimation, the MAP-VDNet-Fast is slightly worse than MAP-VDNet. Without using the spatially adaptive fusion, the performance of MAP-VDNet-NAF is also worse than MAP-VDNet, and the performance gap is up to 0.32 dB, which demonstrates the effectiveness of the proposed spatially-adaptive fusion. When comparing MAP-VDNet-DnCNN with MAP-VDNet, we can see that performance gaps between them are small, which are in the range of (0.07â€“0.21) dB, verifying that the proposed method is non-sensitive to the choice of the deep denoisers.

The effect of the number of stages To show how the number of stages T (i.e., the number of iterations in Algorithm 1) affects the denoising performance, we have compared the proposed MAP-VDNet method with different stages. Fig. 4 shows the average PSNR performance as a function of ğ‘‡âˆˆ[1,6] for ğœğ‘›=25 on two test datasets. It can be observed from Fig. 4 that more stages do lead to improved denoising performance. However, the performance improvements quickly and become saturated when ğ‘‡â‰¥5. For the balance of performance and computational complexity, we have manually set ğ‘‡=5 in our current implementation.

The effect of the number of frames To verify the impact of the number of frames on VD performance, we have conducted comparative studies of the proposed MAP-VDNet with different numbers of input noisy frames: ğ½=1, ğ½=2, and ğ½=3. The average PSNR/SSIM results on Vimeo and DTMC-HD datasets are shown in Table 2. It can be observed that the performance monotonically increases as the number of input frames increases. Therefore, we have set ğ½=3 in our current implementation.

The effect of dense connections between the denoisers Finally, to demonstrate the effects of dense connections between the denoisers, we have conducted experiments comparing the networks with and without dense connections (denoted as MAP-VDNet-wo). The average results on Vimeo and DTMC-HD datasets are shown in Table 3. One can see that dense connections between denoisers are added, the average improvements on Vimeo and DTMC-HD datasets are 0.12 dB and 0.14 dB, respectively. These results verify the effectiveness of dense connections between denoisers in MAP-VDNet.

The inference process of the proposed iterative method In Fig. 5, we show visual and PSNR comparisons of some intermediate variables (e.g., aligned frames ğ‘¦ğ‘¦~ğ‘—(ğ‘—=âˆ’ğ½,â€¦,ğ½), the average fused frame ğ‘¦ğ‘¦â¯â¯â¯, the spatially adaptive fused frame ğ‘¦ğ‘¦â¯â¯â¯â€² and outputs from all stages ğ‘¥ğ‘¥(ğ‘¡)(ğ‘¡=1â€¦,5)) to provide a more detailed description for the inference process of the proposed method. As shown in Fig. 3b, we first feed the reference frame ğ‘¦ğ‘¦0 and each noisy frame ğ‘¦ğ‘¦ğ‘—(ğ‘—=âˆ’ğ½,â€¦,ğ½,ğ‘—â‰ 0) into the alignment module and derive aligned frames ğ‘¦ğ‘¦~ğ‘—. From the visualization in the top row of Fig. 5, we can see that there still exists some misaligned pixels in the aligned noisy frames ğ‘¦ğ‘¦~ğ‘—(ğ‘—=âˆ’ğ½,â€¦,ğ½,ğ‘—â‰ 0). Inaccurate alignment could reduce denoising performance. To avoid the adverse influence of alignment bias, we introduce the spatially adaptive fusion derived from the MAP estimation with non-i.i.d Gaussian likelihood. Through visual comparisons of the average fusion ğ‘¦ğ‘¦â¯â¯â¯=1(2ğ½+1)âˆ‘ğ½ğ‘—=âˆ’ğ½ğ‘¦ğ‘¦~ğ‘— and the spatially adaptive fusion ğ‘¦ğ‘¦â¯â¯â¯â€²=âˆ‘ğ½ğ‘—=âˆ’ğ½ğ‚ğ‘—ğ‘¦ğ‘¦~ğ‘— in Fig. 5, it can be observed that average fusion would yield undesirable artifacts caused by the alignment bias, while the output from spatially adaptive fusion has sharper edges. Therefore, spatially adaptive fusion is more effective in handling large-motion videos. Finally, we implement iterative optimizations of Eq. (19), and the visual results from all intermediate stages are demonstrated in the bottom row of Fig. 5. According to intermediate comparative results from different stages (e.g., ğ‘¥ğ‘¥(ğ‘¡)), one can see that more image details could be restored and the denoising performance (PSNR) has been improved with the increasing number of stages, the final output ğ‘¥ğ‘¥(5) achieved the best denoising performance.

Comparison with Other State-of-the-Art Methods
We have compared the performance of the proposed MAP-VDNet-Fast and MAP-VDNet methods with several other state-of-the-art methods including both model-based (i.e., V-BM4D (Maggioni et al. 2012), RTA-LSM (Dong et al. 2018a) and VNLB (Pablo and Jean 2018)) and recently developed deep learning-based methods (i.e., TOFlow (Xue et al. 2019), ViDeNN (Claus and van Gemert 2019), VNLnet (Davy et al. 2019) and FastDVDnet (Tassano et al. 2020)). For a fair comparison, we have improved the TOFlow method (Xue et al. 2019) by adding more convolutional residual blocks, such that the total number of network parameters of TOFlow is comparable with that of the proposed MAP-VDNet. The enhanced TOFlow method is denoted as TOFlow-EN. All deep-learning based competing methods (i.e., TOFlow (Xue et al. 2019), TOFlow-EN, ViDeNN (Claus and van Gemert 2019), VNLnet (Davy et al. 2019) and FastDVDnet (Tassano et al. 2020)) were retrained on the same Vimeo training dataset, and we set the same number of input frames for these competing methods (i.e., 7) for fairness. Specifically, for VNLnet, we have retrained the color denoising model and the search space of the nonlocal patches is reduced to seven frames, and for ViDeNN and FastDVDnet, we increased the number of input frames to 7 and leading to a three-step cascaded version of FastDVDnet.

Table 4 shows the average results of the test methods on the Vimeo, ASU, and DTMC-HD test datasets. From Table 4, we can see that the VNLB method (Pablo and Jean 2018) is the most competitive algorithm among model-based VD methods. By adding more convolutional layers, the TOFlow-EN method significantly outperforms the original TOFlow method and becomes competitive with other deep learning-based state-of-the-art video denoising methods such as ViDeNN and FastDVDnet methods. With a similar number of parameters, the performance gains of MAP-VDNet over TOFlow-EN demonstrate the effectiveness of our maximum a posterior estimation framework. Both the proposed MAP-VDNet-Fast and MAP-VDNet methods significantly outperform the other competing methods. With a more accurate estimation of optical flow, the MAP-VDNet method performs slightly better than its counterpart that uses a fast optical flow estimation method. The visual comparisons of the denoised frames by the test methods are shown in Figs. 6, 7 and 8. From these figures, one can see that the proposed MAP-VDNet method can reproduce sharper edges and more details than the other competing methods.

Fig. 8
figure 8
Denoising results for a noisy frame of Station video of DTMC-HD test set with noise level ğœğ‘›=50. a Original frame; denoised frame by b V-BM4D (Maggioni et al. 2012) (30.74 dB, 0.7445), c RTA-LSM (Dong et al. 2018a) (30.63 dB, 0.7682), d VNLB (Pablo and Jean 2018) (31.78 dB, 0.8020), e TOFlow (Xue et al. 2019) (31.99 dB, 0.8029), f TOFlow-EN (32.99 dB, 0.8391), g ViDeNN (Claus and van Gemert 2019) (32.72 dB, 0.8274), h VNLnet (Davy et al. 2019) (30.79 dB, 0.7759), i FastDVDnet (Tassano et al. 2020) (33.25 dB, 0.8497), and j MAP-VDNet (33.82 dB, 0.8608)

Full size image
Table 5 The average PSNR/SSIM denoising results on different test sets with signal-dependent noise (boldface highlights the best)
Full size table
Fig. 9
figure 9
Denoising results of a noisy frame from Bus sequence of ASU test set for signal-dependent noise. a The original frame; the frames denoised by b CBDNet (Guo et al. 2019) (26.42 dB, 0.7729), c TOFlow (Xue et al. 2019) (26.36 dB, 0.7839), d TOFlow-EN (26.65 dB, 0.7842), and e MAP-VDNet (28.09 dB, 0.8612)

Full size image
Video Denoising with Signal-Dependent Noise
Although the additive Gaussian noise is widely used in existing denoising studies, the distribution of the real noise of imaging sensors is much more complicated. To improve the denoising performance on real-world noisy images, more realistic signal-dependent noise models have been proposed (Foi et al. 2008). Here, we have adopted the Poisson-Gaussian noise model (Foi et al. 2008) to simulate more realistic noisy frames as follows

ğ‘¦ğ‘—,ğ‘âˆ¼îˆº(ğ‘¥ğ‘—,ğ‘,ğœ2ğ‘Ÿ+ğœğ‘ ğ‘¥ğ‘—,ğ‘),
(21)
where ğ‘¥ğ‘—,ğ‘ is the clean pixel at position p, ğœğ‘Ÿ and ğœğ‘  are related to the sensor gain (ISO) of cameras. Similar to Guo et al. (2019), we simulate the noisy frames by randomly sampling ğœğ‘  and ğœğ‘Ÿ from the ranges [0, 0.16] and [0, 0.06], respectively. Then we retrained the proposed MAP-VDNet, the TOFlow (Xue et al. 2019) and its enhanced version TOFlow-EN on the new noisy training dataset. We have also compared with the recently developed blind image denoising methodâ€”i.e., CBDNet (Guo et al. 2019) that was also trained with the same realistic noise model. Table 5 shows the denoising results by the test methods on the Vimeo, ASU and DTMC-HD datasets. From these tables, we can see that MAP-VDNet performs much better than the competing methods. The average PSNR gains over the TOFlow-EN method (the second best) are 0.78 dB, 0.83 dB and 0.68 â€˜1 on Vimeo, ASU and DTMC-HD datasets, respectively. Parts of the denoised frames by the test methods are shown in Fig. 9. It can be seen that the proposed MAP-VDNet can faithfully recover more details around the texture regions than other competing methods.

Raw Video Denoising with Real-World Noise
Recently, a raw video dataset with realistic noise is proposed, i.e., the CRVD dataset (Yue et al. 2020). Since the characteristics of real-world noise are quite different from that of simulated noise, we conduct raw video denoising comparisons on the CRVD dataset (Yue et al. 2020) to verify the real-world noise removal ability of the proposed MAP-VDNet.

The competing methods include the V-BM4D method (Maggioni et al. 2012), the TOFlow-EN method and the RViDeNet method (Yue et al. 2020). For fairness, we retrained TOFlow-EN and RViDeNet on the same CRVD training dataset and increased the input frames to 7 for RViDeNet (Yue et al. 2020). Due to the difference between sRGB video denoising and raw video denoising, some adaptions need to be conducted for these sRGB video denoising methods. For example, for 4-channel raw data, the V-BM4D method need to denoise the data of each channel separately. Regarding TOFlow-EN and MAP-VDNet methods, to exploit the information among channels, we average the 4-channel raw data to estimate optical flow in the alignment module, other processes are similar to sRGB video denoising. When training RViDeNet, since we donâ€™t consider the image signal processing (ISP) pipeline, we no longer apply the sRGB domain loss in Yue et al. (2020), and we follow the training setting in Yue et al. (2020) that first pretrain a denoiser using an additional MOT Challenge dataset (Milan et al. 2016) and then keep the predenoising network fixed when training the RViDeNet on the same CRVD training dataset.

The real raw video denoising results have been shown in Table 6, one can see that all deep-learning-based video denoising methods outperform the model-based V-BM4D method. Among the supervised learning methods, the PSNR gain of the proposed MAP-VDNet over TOFlow-EN with a comparable number of parameters is 3.26 dB for the CRVD test dataset. In addition, even though the number of parameters of RViDeNet is approximately triple of ours, the proposed method still outperforms RViDeNet by 1.46 dB. The experimental results in Table 6 have proved that the proposed MAP-VDNet could achieve excellent performance for the task of real-world raw video denoising.

Table 6 The average PSNR/SSIM denoising results on CRVD test dataset for real raw video denoising (boldface highlights the best)
Full size table
Video Superresolution
Recently, video superresolution has been widely studied in Caballero et al. (2017); Tao et al. (2017); Sajjadi et al. (2018); Jo et al. (2018); Xue et al. (2019); Haris et al. (2019); Wang et al. (2019); Yi et al. (2019); Tian et al. (2020); Isobe et al. (2020). In this paper, to verify the generalization ability of our proposed framework, we make the proposed MAP-VDNet applied to the video superresolution by adding an upscale module before the alignment module in Fig. 3a. For the architecture of the Ã—4 up-scale module, we employ a simple up-scale network without any special design and a sub-pixel convolutional layer (Shi et al. 2016) to conduct the upsampling operation.

The competing methods consist of the FRVSR method (Sajjadi et al. 2018), the DUF method (Jo et al. 2018), the TOFlow method (Xue et al. 2019), the RBPN method (Haris et al. 2019), the EDVR method (Wang et al. 2019), the PFNL method (Yi et al. 2019) and the TGA method (Isobe et al. 2020). Note that other comparative results come from their own publication or recent work (Isobe et al. 2020). Since Vid4 (Liu and Sun 2014) is a benchmark test set for video superresolution, we demonstrate the comparisons of the performance and number of parameters with other competing methods on Vid4 test set for Ã—4 video superresolution in Table 7. From the results, one can see that the proposed MAP-VDNet outperforms most competing methods except the TGA method (Isobe et al. 2020). Although the performance reflected by PSNR of our method is slightly lower than the TGA method (Isobe et al. 2020), another metric (SSIM) demonstrates the performance of our method is better and the number of parameters of the proposed MAP-VDNet is far less than the TGA method (Isobe et al. 2020). The visual comparisons are illustrated in Fig. 10, the proposed MAP-VDNet could restore more clear details than other competing methods.

Table 7 The average PSNR/SSIM super-resolution results on Vid4 test set for Ã—4 video super-resolution (boldface highlights the best)
Full size table
Fig. 10
figure 10
Video superresolution results (Ã—4 up-sampling) for a low-resolution frame of City video of Vid4 test set. a Original frame; superresolved frame by b TOFlow (Xue et al. 2019) (25.40 dB, 0.7081), c FRVSR (Sajjadi et al. 2018) (26.78 dB, 0.8136), d RBPN (Haris et al. 2019) (26.51 dB, 0.7929), e EDVR-L (Wang et al. 2019) (26.86 dB, 0.7995), f DUF-52L (Jo et al. 2018) (26.89 dB, 0.8147), g PFNL (Yi et al. 2019) (26.94 dB, 0.8298), h MAP-VDNet (27.28 dB, 0.8369)

Full size image
Table 8 The average Î”PSNR (dB) on test sequences of the Vid70 dataset (boldface highlights the best)
Full size table
Table 9 Running time on 90 frames of size 352Ã—288
Full size table
Compressed Video Artifact Reduction
In addition to video denoising and video superresolution, compressed video quality enhancement (Yang et al. 2017, 2018; Lu et al. 2018; Yang et al. 2019a; Xu et al. 2019b; Yang et al. 2019b; Deng et al. 2020; Guan et al. 2021 is an important and challenging video processing task. The quality fluctuation among compressed frames increases the difficulty of recovery. We verify the effectiveness of our MAP-VDNet model on the compressed video artifact reduction task. We compared with the DSCNN method (Yang et al. 2017), the MFQE method (Yang et al. 2018), the QG-ConvLSTM method (Yang et al. 2019a), the NL-ConvLSTM method (Xu et al. 2019b) and the MFQE 2.0 method (Guan et al. 2021). And the results of other competing algorithms are obtained from their publications. Among the competing methods, the MFQE 2.0 method (Guan et al. 2021) was trained on a more comprehensive training dataset which consists of 108 videos. Due to the sensitivity of MFQE 2.0 to the training dataset, we reported the results in their published paper. Note that these competing methods usually design their algorithms specifically for compressed video frames encoded by a fixed quality factor (QP value) -e.g., the NL-ConvLSTM method (Xu et al. 2019b) introduces a non-local strategy into the ConvLSTM module to learn spatiotemporal correlation across frames, and the MFQE methods (Yang et al. 2018; Guan et al. 2021) exploit peak quality frames that located by the detector to enhance the quality of adjacent compressed frames. Their generalization property often remains questionable; by contrast, our algorithm is not optimized for the task of compressed video quality enhancement but still achieves excellent performance. Table 8 illustrates the average PSNR gain (dB) on the test sequences of the Vid70 dataset. The results in Table 8 have shown that the proposed MAP-VDNet model is also effective for the task of compressed video artifact reduction, which justifies its excellent generalization property.

Complexity Analysis and Discussions
We have compared the proposed MAP-VDNet-Fast and MAP-VDNet method with other deep-learning-based competing methods in Fig. 1. The comparison shows the trade-off between the complexity (as measured by the number of parameters) and the denoising performance (average PSNR values). It can be observed that with a similar number of parameters, our MAP-VDNet outperforms TOFlow-EN over 1dB on DTMC-HD test set for ğœğ‘›=25. With fewer number of parameters, our MAP-VDNet-Fast could also achieve excellent denoising performance compared to other competing video denoising algorithms. Additionally, Table 9 shows the comparison of the actual running time of different denoising methods on a Nvidia Titan XP GPU. By using a fast optical flow estimation method that is implemented with OpenCV library and only runs on CPU, the running time of our MAP-VDNet-Fast is comparable to that of TOFlow (note that ours outperforms TOFlow by over 2dB as shown in Fig. 1).

Differences with other deep unfolding networks To the best of our knowledge, this is the first work that uses optimization formation to guide the design of deep neural networks for video denoising. When compared with the image denoising problem (Zhang et al. 2017b; Dong et al. 2018b), the video denoising problem is more difficult to handle because the temporal redundancy among adjacent frames needs to be modeled and exploited. In this paper, we formulate the multiframe fusion module in the MAP estimation architecture, and each iteration can be efficiently computed by the proposed MAP-VDNet. Moreover, this work explicitly takes misalignment errors into account and demonstrates a principled solution based on robust multiframe fusion.

Conclusions
In this paper, we propose a novel MAP-based video denoising algorithm MAP-VDNet. Unlike existing model-based Bayesian video denoising, we strive to optimize the parameters of a network-based denoising algorithm without involving hand-crafted procedures. Different from previous deep learning-based methods with heuristically designed network architectures, the construction of MAP-VDNet is based on an explainable optimization-inspired solution to video denoising derived from the classical MAP estimation framework. Specifically, we first propose an iterative MAP-based video denoising algorithm based on a realistic observation model of noisy frames, which allows us to solve the denoising problem in a principled manner. Combining a DCNN-based image denoiser module with an optical flow-based image alignment module, we then demonstrate how to unfold the iterative video denoising algorithm into a multistage implementation in which both image denoising and image fusion modules can be jointly trained. Moreover, we propose a robust multiframe fusion scheme by predicting the adaptive fusion coefficient on a pixel-by-pixel basis, which further improves the performance of video denoising in the presence of misalignment. Extensive experimental results on three video datasets show that the proposed method significantly outperforms the existing video denoising methods in terms of both objective and subjective quality of restored video frames. Additionally, we have achieved promising experimental results on other video restoration tasks such as real-world video denoising, video superresolution and compressed video artifact reduction, which demonstrate the good generalization property of the proposed MAP-VDNet.