Unlike the maturity of image denoising research, video denoising has remained a challenging problem. A fundamental issue at the core of the video denoising (VD) problem is how to efficiently remove noise by exploiting temporal redundancy in video frames in a principled manner. Based on the maximum a posterior (MAP) estimation framework and recent advances in deep learning, we present a novel deep MAP-based video denoising method named MAP-VDNet with adaptive temporal fusion and deep image prior. The proposed MAP-based VD algorithm allows computationally efficient untangling of motion estimation (frame alignment) and image restoration (denoising). To address the misalignment issue, we also present a robust multi-frame fusion strategy for predicting spatially varying fusion weights by a neural network. To facilitate end-to-end optimization, we unfold the proposed iterative MAP-based VD algorithm into a deep convolutional network named MAP-VDNet. Extensive experimental results on three popular video datasets have shown that the proposed MAP-VDNet significantly outperforms current state-of-the-art VD techniques such as ViDeNN and FastDVDnet. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/MAP-VDNet.htm.

Access provided by University of Auckland Library

Introduction
The field of image denoising has advanced rapidly in the past 20 years. Existing image denoising methods can be roughly classified into two categories—i.e., model-based (Portilla et al. 2003; Dabov et al. 2007; Dong et al. 2013; Gu et al. 2014) and deep learning-based (Zhang et al. 2017a; Tai et al. 2017; Zhang et al. 2018a). Model-based methods formulate image denoising as a Bayesian estimation problem in a probabilistic setting or a variational optimization problem in a deterministic setting. Through mathematical construction of an image prior or a regularization function, we can develop principled solutions to the image denoising problem (Dabov et al. 2007; Dong et al. 2013). In the literature of image denoising, sparsity-based prior models or regularization methods are among the most popular, which lead to state-of-the-art denoising performance in model-based approaches (Dabov et al. 2007; Dong et al. 2013).

Deep learning-based approaches to image denoising have been extensively studied in recent years. Instead of mathematical construction, learning-based denoising methods directly learn a nonlinear mapping function from the space of noisy images to that of clean images (Zhang et al. 2017a; Tai et al. 2017; Zhang et al. 2018a). In view of the notorious vanishing gradient problem (Bengio et al. 1994), novel architectures such as residual and densely connected networks (Huang et al. 2017; Zhang et al. 2018b) have been developed. Other methods have exploited the domain knowledge during the construction of network architectures—e.g., unfolding iterative denoising algorithms into deep convolutional neural networks (DCNN) (Wisdom et al. 2017; Hershey et al. 2014; Dong et al. 2018b; Bertocchi et al. 2020). DCNN-based image denoising techniques (Tai et al. 2017; Zhang et al. 2017a; Dong et al. 2018b) have achieved current state-of-the-art performance due to their powerful learning capabilities.

By contrast, video denoising (VD) has been a relatively underresearched problem in the literature (Ji et al. 2010; Liu and Freeman 2010). Model-based approaches toward video denoising are often built upon well-known image denoising algorithms (Varghese and Wang 2010; Mahmoudi and Sapiro 2005; Buades et al. 2016; Maggioni et al. 2012). e.g., Gaussian scale mixture (GSM) denoising (Portilla et al. 2003) was extended in Varghese and Wang (2010), nonlocal means denoising (Buades et al. 2005) became Mahmoudi and Sapiro (2005), Buades et al. (2016) and BM3D denoising (Dabov et al. 2007) evolved into BM4D for video denoising (Maggioni et al. 2012). Besides, some model-based algorithms (Dong et al. 2018a; Pablo and Jean 2018) have obtained superior denoising performance. Most recently, a flurry of deep learning-based VD methods (Mildenhall et al. 2018; Godard et al. 2018; Xue et al. 2019; Claus and van Gemert 2019; Ehret et al. 2019; Tassano et al. 2019, 2020; Davy et al. 2019) have been developed. Most of these methods first align noisy frames and then perform denoising on the aligned noisy frames with a standard DCNN. Frame alignment can be achieved by optical flow estimation network (Ranjan and Black 2017), kernel prediction network (Mildenhall et al. 2018), and MEMC-Net (Bao et al. 2019).

While recent deep learning-based VD methods address the VD problem by directly learning a mapping function from the noisy frames to the desired clean frames, these methods are heuristic and ignore the observation model that characterizes the formation of noisy frames. Inspired by the analogy between burst and video denoising (Godard et al. 2018), we propose a maximal a posterior (MAP) estimation framework for VD with DCNN denoising prior and robust multiframe alignment/fusion. By explicitly taking the frame alignment errors (due to misregistration) into the observation model, we have derived a principled MAP estimation solution for better VD performance. As both steps of single-frame denoising and multiframe fusion can be implemented by convolutional neural networks, our MAP-based VD algorithm can be unfolded into a network implementation, allowing further optimization of network parameters through end-to-end training. In addition to the excellent performance, the resulting network (denoted as MAP-VDNet) admits a Bayesian interpretation from the MAP perspective. The key technical contributions of this work are summarized as follows.

MAP-inspired network. We first propose an iterative MAP-based VD algorithm with DCNN denoising prior, where each iteration can be efficiently computed. Then, a MAP-inspired VD network is constructed by unfolding the iterative video denoising algorithm into a multistage implementation.

Robust multiframe fusion. To address the inevitable alignment errors (due to misregistration), we propose to fuse the aligned noisy frames with spatially variant weights learned by a kernel prediction network (KPN, Mildenhall et al. (2018)). Such an analogy between burst and video denoising has not been addressed in the open literature to the best of our knowledge.

Excellent denoising performance. The proposed MAP-VDNet has dramatically advanced the state-of-the-art by over 1 dB while using a comparable number of model parameters as shown in Fig. 1. The fast implementation of MAP-VDNet strikes an improved trade-off between the cost and the performance over other competing methods.

Good generalization property. In addition to VD, this work can be readily generalized to other video restoration tasks such as superresolution and compression artifact reduction. Promising experimental results for video superresolution and compression artifact reduction have verified the good generalization property of the proposed MAP-VDNet.

Related Works
Deep Image Denoising
Image denoising has been extensively studied, not only due to its wide applications. As the simplest inverse problem, numerous image priors or modeling techniques have been used to evaluate their performance for image denoising (Portilla et al. 2003; Dabov et al. 2007; Zoran and Weiss 2011; Dong et al. 2013; Gu et al. 2014). Before the popularity of deep learning, the most popular image prior is the sparse representation over learned dictionaries (Elad and Aharon 2006; Dong et al. 2013), assuming that image patches can be well approximated by a few representative atoms learned from training images. Such methods were highly related to the maximum a posterior (MAP) techniques, where the prior distributions of the sparse coefficients play a key. In addition to the sparsity, the nonlocal self-similarity of natural images was also considered in developing an improved probability model (Dong et al. 2013), leading to significant improvements. Inspired by the great successes of DCNNs for image classification, DCNNs have also been adopted for image denoising (Zhang et al. 2017a; Tai et al. 2017; Zhang et al. 2018a). In Zhang et al. (2017b), the DCNNs were proposed to learn the residual images that are the differences between clean and noisy images. More sophisticated network architectures considering long-term dependencies have also been proposed (Tai et al. 2017). Instead of designing the network as a black box, the domain knowledge has also been incorporated into the DCNNs- i.e., unfolding the iterative processes into deep networks (Dong et al. 2018b), showing highly competitive performance. Most recently, neural architecture search (NAS) has found successful application into image denoising too (e.g., Zhang et al. (2020)).

Deep Video Denoising
Deep learning for VD has recently received increasing attention. In Xue et al. (2019), a neural network with a trainable motion estimation component and a video processing component was developed for video denoising and superresolution. Another VD network based on explicit alignment is RViDeNet (Yue et al. 2020) which takes a supervised learning approach. One of the common limitations to existing learning-based VD is the heuristic concatenation of alignment and denoising subnetworks. We conjecture that there is still much room for performance improvement by pursuing end-to-end optimization. Deep learning for video denoising without explicit motion estimation has also been studied in the literature. A 3D deformable kernel was developed for video denoising in Xu et al. (2019a) aiming at more efficiently sampling the pixels across the spatio-temporal space. A nonlocal extension of the convolutional neural network (CNN) was constructed in Davy et al. (2019) for VD and demonstrated highly competent performance to the best model-based VD VNLB (Pablo and Jean 2018). A low-complexity alternative called Kalman filtering for frame-recursive video denoising has also been considered in Arias and Morel (2019). Most recently, a VNLnet (Davy et al. 2019), a FastDVDnet (Tassano et al. 2020), and a spatio-temporal pixel aggregation network (ST-PAN, Xu et al. (2020)) has achieved the current state-of-the-art performance in VD.

Deep Burst Denoising
Deep burst denoising (Godard et al. 2018) refers to the problem of image restoration in burst imaging (Hasinoff et al. 2016), which is a special case in the situation of low light imaging (e.g., nighttime environment). Burst denoising can be viewed as the middle ground between image and video denoising. It dealt with the restoration of multiple noisy frames, but the amount of camera and object motion is usually assumed to be small. Therefore, the alignment of multiple frames is faster for burst denoising—e.g., commonly adopted global registration techniques such as Homography-based (Tekalp 2015) and local feature-based such as Lucas-Kanade tracking (Lucas and Kanade 1981) are deemed sufficient. Existing works on deep burst denoising mostly fall in the framework of kernel prediction network (KPN, Mildenhall et al. (2018)). The basic idea behind the construction of KPN is to predict spatially varying kernels that can both align and restore noisy frames. However, previous works including KPN and other methods (Godard et al. 2018; Xu et al. 2019a, 2020) have not considered the general cases of large unknown misalignments which are common in VD (as illustrated in Fig. 2). How to model structure and signal-dependent noise arising from misalignment sets up the stage for this work.

Fig. 1
figure 1
Comparisons of performance and no. of parameters. Results are evaluated on DTMC-HD test set for 𝜎𝑛=25

Full size image
Fig. 2
figure 2
The illustration of structural noise introduced by the alignment module. a the reference frame, b the adjacent aligned frame (note undesirable artifacts as highlighted by red boxes). Note that these artifacts are signal-dependent and behave differently from additive white Gaussian noise (Color figure online)

Full size image
Fig. 3
figure 3
The architecture of the proposed model-guided DCNN for video denoising. a The overall architecture of the proposed network, b the architecture of the alignment module, c the architecture of the robust multi-frame fusion, and d the architecture of the encoding and decoding block of the U-net denoiser

Full size image
Proposed MAP Estimator for Video Denoising
MAP Estimator with i.i.d Gaussian Likelihood
Table 1 The average PSNR (dB) and SSIM results of the variants of the proposed method for noise level 𝜎𝑛=25
Full size table
For VD, we aim at recovering each clean frame 𝑥𝑥∈ℝ𝑁 (the frame index is omitted for simplicity) from a set of adjacent noisy frames 𝑦𝑦𝑗, 𝑗=−𝐽,…,𝐽. Unlike image denoising, a noisy frame can have multiple observations because it is highly correlated with adjacent frames due to temporal redundancy. A commonly used video frame observation model can be expressed as

𝑦𝑦𝑗=𝐖̃ 𝑗𝑥𝑥+𝑛𝑛𝑗,𝑗=−𝐽,…,𝐽,
(1)
where 𝐖̃ 𝑗∈ℝ𝑁×𝑁 denotes the warping matrix (or an operator) that maps from the frame of interest 𝑥𝑥 to its adjacent frame 𝑥𝑥𝑗 (e.g., optical flow (Ranjan and Black 2017)), and 𝑛𝑛𝑗∈ℝ𝑁 is the additive Gaussian noise. In the above formulation, 𝐖̃ 0 is simply an identity matrix for the central frame at 𝑗=0. The above observation model has been adopted in Kokkinos and Lefkimmiatis (2019) for burst denoising. However, the tangle of the warping matrices 𝐖̃ 𝑗 with 𝑥𝑥 making the optimization of 𝑥𝑥 very difficult, i.e., one has to solve a sequence of large matrix inverse problems. In this paper, we propose the following noisy frame observation model, as

𝑥𝑥=𝐖𝑗𝑥𝑥𝑗=𝐖𝑗𝑦𝑦𝑗+𝑛𝑛~𝑗, 𝑗=−𝐽,…,𝐽,
(2)
where we have used 𝑦𝑦𝑗=𝑥𝑥𝑗+𝑛𝑛𝑗, 𝑛𝑛~𝑗=−𝐖𝑗𝑛𝑛𝑗, and 𝐖𝑗 denotes the warping matrix from 𝑥𝑥𝑗 to the central frame 𝑥𝑥. For simplicity, we assume 𝑛𝑛~𝑗 is still Gaussian with variance 𝜎2𝑛. Based on such multi-frame observation model and the assumption that 𝑥𝑥 and 𝐖𝑗 are independent, we propose the following Maximum a Posterior (MAP) estimation

(𝑥𝑥,𝐖𝑗)=argmax𝑥𝑥,𝐖𝑗∑𝑗=−𝐽𝐽log𝑃(𝑦𝑦𝑗|𝑥𝑥,𝐖𝑗)+log𝑃(𝑥𝑥)+log𝑃(𝐖𝑗),
(3)
where 𝑃(𝑦𝑦𝑗|𝑥𝑥) is the Gaussian likelihood, which can be expressed as

𝑃(𝑦𝑦𝑗|𝑥𝑥,𝐖𝑗)∝exp(−1𝜎2𝑛||𝑥𝑥−𝐖𝑗𝑦𝑦𝑗||22).
(4)
For the prior terms 𝑃(𝑥𝑥) and 𝑃(𝐖𝑗), we use a general form to describe

𝑃(𝑥𝑥)∝exp(−𝛾Ω(𝑥𝑥)),
(5a)
𝑃(𝐖𝑗)∝exp(−𝛿Φ1(𝐖𝑗)),
(5b)
where Ω(⋅) and Φ1(⋅) denote energy functions related to 𝑥𝑥 and 𝐖𝑗 respectively, and 𝛾 and 𝛿 are the corresponding weights. By substituting the Gaussian likelihood into Eq. (4) and prior terms of Eq. (5) in the MAP estimation of Eq. (3), we can obtain the following objective function for multiframe denoising

(𝑥𝑥,𝐖𝑗)=argmin𝑥𝑥,𝐖𝑗∑𝑗=−𝐽𝐽𝜂||𝑥𝑥−𝐖𝑗𝑦𝑦𝑗||22+𝛾Ω(𝑥𝑥)+𝛿Φ1(𝐖𝑗),
(6)
where 𝜂=1/𝜎2𝑛. Regarding the prior term 𝑃(𝑥𝑥), instead of mathematical construction of Ω(𝑥𝑥), we propose to adopt the DCNN-based image prior (Dong et al. 2018b) by introducing an auxiliary variablea 𝑣𝑣, the objective function of Eq. (6) could be translated into

(𝑥𝑥,𝑣𝑣,𝐖𝑗)=argmin𝑥𝑥,𝑣𝑣,𝐖𝑗∑𝑗=−𝐽𝐽𝜂||𝑥𝑥−𝐖𝑗𝑦𝑦𝑗||22+𝜆||𝑥𝑥−𝑣𝑣||22+𝛾Ω(𝑣𝑣)+𝛿Φ1(𝐖𝑗).
(7)
It could be observed from Eq. (7) that the multiframe denoising problem could be solved by iteratively optimizing the following three subproblems

𝐖𝑗=argmin𝐖𝑗∑𝑗=−𝐽𝐽𝜂||𝑥𝑥−𝐖𝑗𝑦𝑦𝑗||22+𝛿Φ1(𝐖𝑗),
(8a)
𝑣𝑣=argmin𝑣𝑣𝜆||𝑥𝑥−𝑣𝑣||22+𝛾Ω(𝑣𝑣),
(8b)
𝑥𝑥=argmin𝑥𝑥∑𝑗=−𝐽𝐽𝜂||𝑥𝑥−𝐖𝑗𝑦𝑦𝑗||22+𝜆||𝑥𝑥−𝑣𝑣||22.
(8c)
Regarding the 𝐖𝑗-subproblem, instead of estimating 𝐖𝑗 with a specific prior, we propose to estimate 𝐖(𝑡+1)𝑗 from 𝑥𝑥 and 𝑦𝑦𝑗 using a DCNN function. Since 𝑥𝑥 is not available, we use the current estimation 𝑥𝑥(𝑡) to compute 𝐖𝑗 as follows

𝐖𝑗(𝑡+1)=𝑔1(𝑥𝑥(𝑡),𝑦𝑦𝑗),
(9)
where 𝑔1(⋅) represents a DCNN function. Besides, as demonstrated in Eq. (8b), the 𝑣𝑣-subproblem could be considered as a single-frame denoising problem, and 𝑣𝑣 can be updated as

𝑣𝑣(𝑡+1)=𝑓(𝑥𝑥(𝑡)),
(10)
where 𝑓(⋅) denotes the DCNN denoising function. Concerning the 𝑥𝑥-subproblem, since both the terms are 𝑙2-norm in Eq. (8c), the above objective function of 𝑥𝑥-subproblem admits a closed-form solution as

𝑥𝑥(𝑡+1)=(1−𝑎)𝑦𝑦⎯⎯⎯(𝑡+1)+𝑎𝑣𝑣(𝑡+1),
(11)
where 𝑎=𝜆(2𝐽+1)𝜂+𝜆 and 𝑦𝑦⎯⎯⎯(𝑡+1)=1(2𝐽+1)∑𝐽𝑗=−𝐽𝐖𝑗(𝑡+1)𝑦𝑦𝑗. In principle, through the iterative computations of 𝐖𝑗, 𝑣𝑣 and 𝑥𝑥, the multiframe denoising problem can be efficiently solved.

Table 2 The PSNR(dB) and SSIM performance comparison for different values of J at the noise level of 𝜎𝑛=25
Full size table
Although the warping matrix 𝐖𝑗 can be estimated by any existing motion compensation method, its computational complexity is often prohibitive. For example, popular deep motion compensation methods (Ranjan and Black 2017; Wang et al. 2019) tend to use a multi-scale architecture to deal with large displacements between frames. Even with a fast optical flow estimation method such as Kroeger et al. (2016), iterative computation of the alignment matrix would involve frequent data type conversion between GPU and CPU, which renders an additional burden to limited computational resources. To tackle this problem, we opt to update 𝐖𝑗 just once in our iterations, and then iteratively optimize 𝑥𝑥 and 𝑣𝑣 with a fixed 𝐖𝑗. In practice, we use 𝑦𝑦0 to initialize 𝑥𝑥(𝑡) in Eq. (9) and 𝐖𝑗 can be initially estimated by 𝐖𝑗=𝑔1(𝑦𝑦0,𝑦𝑦𝑗). Then the MAP estimator leads to a simple multiframe denoising scheme—i.e., iteratively computing

𝑥𝑥(𝑡+1)=(1−𝑎)𝑦𝑦⎯⎯⎯+𝑎𝑓(𝑥𝑥(𝑡)),
(12)
where 𝑦𝑦⎯⎯⎯=1(2𝐽+1)∑𝐽𝑗=−𝐽𝐖𝑗𝑦𝑦𝑗. Regarding the starting point 𝑥𝑥(0) in Eq. (12), a natural selection is the weighted average of the aligned frames—i.e., 𝑥𝑥(0)=𝑦𝑦⎯⎯⎯. The reason is that the averaging of the m noisy observations of a frame will reduce the noise variance from 𝜎2𝑛 to 𝜎2𝑛𝑚 (Chang et al. 2000). Thus, if the temporal alignment is perfect, the averaging of the aligned noisy frames will significantly reduce the noise variance by a factor equaling to the total number of aligned noisy frames. However, due to the inevitable misalignment errors from one-time alignment iteration, further refinement of MAP estimation has to be performed based on the baseline method. We will refine the noise observation model and derive a new MAP estimation for the updated observation model next.

MAP Estimator with Non-i.i.d Gaussian Likelihood
In Eq. (4), we assume that the warped noise 𝐖𝑗𝑛𝑛𝑗 is still Gaussian with variance 𝜎2𝑛. However, when the motion estimation is inaccurate, some pixels in the adjacent frame after alignment might become misaligned with the reference frame causing undesirable artifacts as shown in Fig. 2. Those artifacts are signal-dependent and behave differently from additive white Gaussian noise. Therefore, the noise 𝑛𝑛~𝑗 in the aligned noisy frame is more complex than identically distributed (i.i.d) Gaussian. To address this issue, we propose a non-i.i.d Gaussian likelihood for multiframe denoising inspired by the analogy between burst denoising and video denoising (Mildenhall et al. 2018). To the best of our knowledge, such an adversary effect of motion estimation on video denoising has not been explored in the open literature.

To properly take such systematic errors into account, we introduce a new alignment error term 𝑒𝑒𝑗 to represent the potential deviation of an aligned frame from the reference frame. More specifically, we propose to rewrite the observation model in Eq. (2) as

𝑥𝑥=𝐖𝑗𝑥𝑥𝑗+𝑒𝑒𝑗=𝑦𝑦~𝑗+𝑛𝑛~𝑗+𝑒𝑒𝑗, 𝑗=−𝐽,…,𝐽,
(13)
where 𝑦𝑦~𝑗=𝐖𝑗𝑦𝑦𝑗, and 𝑒𝑒𝑗 denotes the alignment error. Generally, 𝑥𝑥−𝑦𝑦~𝑗 generally no longer satisfies the i.i.d Gaussian assumption due to the interference from alignment error term 𝑒𝑒𝑗. Instead, it is more appropriate to pursue an adaptive estimation about the standard deviation of spatially varying noise on a pixel-by-pixel basis. It follows that we can extend the i.i.d Gaussian likelihood term in Eq. (4) into its non-i.i.d version, as

𝑃(𝑦𝑦𝑗|𝑥𝑥,𝐖𝑗,𝜎𝜎𝑗)=∏𝑝𝑃(𝑦𝑦𝑗,𝑝|𝑥𝑥𝑝,𝑦𝑦~𝑗,𝑝,𝜎𝑗,𝑝)=∏𝑝1𝜎𝑗,𝑝2𝜋‾‾‾√exp[−∑𝑝1𝜎2𝑗,𝑝(𝑥𝑥𝑝−𝑦𝑦~𝑗,𝑝)2],
(14)
where 𝑦𝑦~𝑗,𝑝 and 𝑥𝑥𝑝 denote the p-th elements of 𝑦𝑦~𝑗 and 𝑥𝑥, respectively, and 𝜎𝑗,𝑝 is the per-pixel noise standard deviation after alignment. Obviously, 𝜎𝑗,𝑝 is relevant to the warping matrix 𝐖𝑗, thus we propose to jointly estimate 𝐖𝑗 and 𝜎𝜎𝑗 under the MAP estimation framework as follows

(𝑥𝑥,𝐖𝑗,𝜂𝑗)=argmax𝑥𝑥,𝐖𝑗,𝜂𝑗∑𝑗=−𝐽𝐽log𝑃(𝑦𝑦𝑗|𝑥𝑥,𝐖𝑗,𝜂𝑗)+log𝑃(𝑥𝑥)+log𝑃(𝐖𝑗,𝜂𝑗),
(15)
where 𝜂𝜂𝑗=1/𝜎𝜎𝑗. Similarly, by substituting the non-i.i.d likelihood term into the MAP estimator of Eq. (15) and introducing a denoising prior term, we can extend the objective function of Eq. (7) into the following spatially adaptive formulation

(𝑥𝑥,𝑣𝑣,𝐖𝑗,𝜂𝜂𝑗)=argmin𝑥𝑥,𝑣𝑣,𝐖𝑗,𝜂𝜂𝑗∑𝑗=−𝐽𝐽||ΣΣ𝑗(𝑥𝑥−𝐖𝑗𝑦𝑦𝑗)||22+𝛾Ω(𝑣𝑣)+𝜆||𝑥𝑥−𝑣𝑣||22+𝛿Φ2(𝐖𝑗,𝜂𝜂𝑗)+log(𝜂𝜂𝑗2𝜋‾‾‾√),
(16)
where Φ2(⋅) represents the energy function related to joint distribution of 𝐖𝑗 and 𝜂𝜂𝑗, and the weighting matrix ΣΣ𝑗=diag(𝜂𝑗,𝑝)∈ℝ𝑁×𝑁 is a diagonal matrix whose diagonal entries (𝜂𝑗,𝑝=1/𝜎𝑗,𝑝) represent the reciprocal of the standard deviation for each individual noisy pixel. Similar to Eq. (7), we can solve the above optimization problem by the following formulas

(𝐖𝑗,𝜂𝜂𝑗)=argmin𝐖𝑗,𝜂𝜂𝑗∑𝑗=−𝐽𝐽||ΣΣ𝑗(𝑥𝑥−𝐖𝑗𝑦𝑦𝑗)||22+𝛿Φ2(𝐖𝑗,𝜂𝜂𝑗)+log(𝜂𝜂𝑗2𝜋‾‾‾√),
(17a)
𝑣𝑣=argmin𝑣𝑣𝜆||𝑥𝑥−𝑣𝑣||22+𝛾Ω(𝑣𝑣),
(17b)
𝑥𝑥=argmin𝑥𝑥∑𝑗=−𝐽𝐽||ΣΣ𝑗(𝑥𝑥−𝐖𝑗𝑦𝑦𝑗)||22+𝜆||𝑥𝑥−𝑣𝑣||22.
(17c)
Instead of constructing a specific prior of Φ2(𝐖𝑗,𝜂𝜂𝑗), we propose to jointly estimate 𝐖𝑗 and 𝜂𝜂𝑗 from 𝑥𝑥 and 𝑦𝑦𝑗 with a DCNN function. Considering the unavailability of 𝑥𝑥, we exploit the intermediate estimation 𝑥𝑥(𝑡). Similarly, we initialize 𝑥𝑥(𝑡) with 𝑦𝑦0, and then conduct one iteration to save the computational overhead. Therefore, 𝐖𝑗 and 𝜂𝜂𝑗 can be estimated as

(𝐖𝑗,𝜂𝜂𝑗)=𝑔2(𝑦𝑦0,𝑦𝑦𝑗),
(18)
where 𝑔2(⋅) is a DCNN function to estimate 𝐖𝑗 and 𝜂𝜂𝑗. Then 𝑥𝑥 could be updated as the following

𝑥𝑥(𝑡+1)==(∑𝑗=−𝐽𝐽ΣΣ⊤𝑗ΣΣ𝑗+𝜆𝐈)−1(∑𝑗=−𝐽𝐽ΣΣ⊤𝑗ΣΣ𝑗𝑦𝑦~𝑗+𝜆𝑓(𝑥𝑥(𝑡))),(𝐈−ΛΛ)𝑦𝑦⎯⎯⎯′+ΛΛ𝑓(𝑥𝑥(𝑡)),
(19)
where 𝑦𝑦⎯⎯⎯′=∑𝐽𝑗=−𝐽𝐂𝑗𝑦𝑦~𝑗 denotes the spatially adaptive fusion of the aligned noisy frames, 𝐂𝑗=diag(𝜂2𝑗,𝑝∑𝐽𝑗=−𝐽𝜂2𝑗,𝑝)∈ℝ𝑁×𝑁 and ΛΛ=diag(𝜆∑𝐽𝑗=−𝐽𝜂2𝑗,𝑝+𝜆)∈ℝ𝑁×𝑁 are diagonal matrices. The fusion matrix 𝐂𝑗 guides the spatially adaptive averaging of the aligned noisy frames, and thus Eq. (19) effectively implements the idea of robust multi-frame fusion, which is conceptually similar to per-pixel kernel prediction in burst denoising (Mildenhall et al. 2018). From Eq. (19), we can see that the MAP estimator with non-i.i.d Gaussian likelihood also leads to a simple multiframe denoising scheme. For the initial estimate 𝑥𝑥(0) in Eq. (19), we use the adaptively averaged frame as the starting point, i.e., 𝑥𝑥(0)=𝑦𝑦⎯⎯⎯′. Comparing with Eq. (12), we can see that the updating scheme of Eq. (12) is a special case of Eq. (19) by setting 𝐂𝑗 and ΛΛ as fixed scalar variables. Regarding the per-pixel fusion filters 𝜂𝜂𝑗, we will use the deep network to adaptively optimize the fusion weights as described in the next section. Through the estimation of filters 𝜂2𝑗,𝑝∈ℝ𝑁(𝑗=−𝐽,…,𝐽), the diagonal matrices 𝐂𝑗 and ΛΛ can be efficiently computed. The proposed MAP-based video denoising algorithm can be summarized in Algorithm 1.

figure a
In the conventional wisdom of model-based denoising, an implementation of Algorithm 1 requires many iterations to converge. Moreover, it will be difficult to jointly optimize the denoiser 𝑓(⋅), the alignment operators 𝐖𝑗, the adaptive fusion weights 𝐂𝑗, and 𝜆 (note that most parameters in model-based denoising are often hand-crafted). Such observation motivates us to implement the proposed MAP estimator with a deep neural network, and so all components along with their parameters can be jointly optimized through end-to-end training. In particular, regarding the per-pixel fusion weights 𝐂𝑗 that plays an important role in robust multiframe fusion, we will use the deep network to adaptively optimize the fusion weights as described in the next section.

Deep Neural Network Implementation
Although the frame alignment, multiframe fusion, and image denoising can all be implemented by deep neural networks (DNN), an exact DNN-based implementation of the proposed MAP-based VD algorithm is not straightforward (Hershey et al. 2014). Unfolding MAP-based VD algorithm takes more effort than its image denoising counterpart (Dong et al. 2018b). The overall network architecture of the unfolded MAP-based Video Denoising Network, dubbed MAP-VDNet, is presented in Fig. 3a. Note that in our current implementation, the 𝑔2(⋅) function in Eq. (18) is realized by a two-step procedure, which illustrated as the alignment module to compute 𝐖𝑗𝑦𝑦𝑗 and fusion module to estimate 𝜂𝜂𝑗, respectively. As shown in Fig. 3a, the input noisy frames 𝑦𝑦𝑗 are first aligned by an alignment module, which can be any existing alignment method in principle. The aligned noisy frames 𝑦𝑦~𝑗 are then fed into the fusion module, which performs a spatially adaptive fusion of 𝑦𝑦~𝑗. The fused noisy frame 𝑦𝑦⎯⎯⎯′ as the initial estimate of 𝑥𝑥 is then passed to the image denoiser 𝑓(⋅). The output of the denoiser 𝑓(⋅) is finally combined with the fused frame 𝑦𝑦⎯⎯⎯′ for producing an improved estimate of 𝑥𝑥. Such a process will be iterated for several stages corresponding to the unfolding of the main loop in Algorithm 1. Thus, the proposed MAP-VDNet as shown in Fig. 3a provides an exact implementation of Algorithm 1. Next, we will provide the details of each module.

Fig. 4
figure 4
The average PSNR curves as a function of the no. of stages T of the proposed MAP-VDNet for 𝜎𝑛=25

Full size image
Table 3 The PSNR(dB) and SSIM performance comparison for the effect of dense connections between the denoisers when 𝜎𝑛=25
Full size table
Fig. 5
figure 5
Denoising intermediate visual and PSNR results of aligned frames 𝑦𝑦~𝑗(𝑗=−3,…,3), the average fused frame 𝑦𝑦⎯⎯⎯, the spatially adaptive fused frame 𝑦𝑦⎯⎯⎯′, and outputs from all stages 𝑥𝑥(𝑡)(𝑡=1,…,5) for a noisy frame of Foreman video of ASU test set with noise level 𝜎𝑛=25. Note that the average fused frame 𝑦𝑦⎯⎯⎯ would generate undesirable artifacts caused by alignment errors, while the proposed spatially adaptive fused frame 𝑦𝑦⎯⎯⎯′ could restore shaper edges (Please zoom in to see better details)

Full size image
Alignment Module
This network implements step (2) in Algorithm 1. The adjacent 2J noisy frames need to be aligned with the frame of interest 𝑦𝑦 first. As shown in Fig. 3b, each frame 𝑦𝑦𝑗 and the reference frame 𝑦𝑦 is first used to compute the optical flow, then the estimated optical flow fields are then used to warp the noisy frame 𝑦𝑦𝑗 to the reference frame 𝑦𝑦. Either deep learning-based or conventional optical flow estimation methods can be used to estimate the optical flows. In Sect. 5.2, we have used two optical flow estimation methods—i.e., the Spynet (Ranjan and Black 2017) and the fast inverse search-based method (Kroeger et al. 2016) (corresponding to the name MAP-VDNet-Fast in Fig. 1) to compare the effect of different alignment modules. The estimated optical flow fields are then used to warp the noisy frame 𝑦𝑦𝑗 to the reference frame 𝑦𝑦 by the spatial transformer network of (Jaderberg et al. 2015) before fusion.

Table 4 The average PSNR/SSIM video denoising results on Vimeo, ASU, and DTMC-HD dataset at different noise levels (boldface highlights the best)
Full size table
Fusion Module
In addition to concatenation-based fusion, we have proposed two fusion strategies, as discussed in Sect. 3- i.e., the uniform average fusion 𝑦𝑦⎯⎯⎯=1(2𝐽+1)∑𝐽𝑗=−𝐽𝑦𝑦~𝑗 and the spatially adaptive fusion 𝑦𝑦⎯⎯⎯′=∑𝐽𝑗=−𝐽𝐂𝑗𝑦𝑦~𝑗. The former fusion method is simply the uniform average of all aligned noisy frames; the later fusion method performs per-pixel adaptive fusion according to the noise variance. For per-pixel adaptive fusion, we propose to estimate the filters 𝜂2𝑗,𝑝 by a deep neural network, which is analogous to the kernel prediction network (KPN) used in burst denoising (Mildenhall et al. 2018). Similar to KPN, the objective of fusion module is to derive spatially-adaptive fusion weights 𝐂𝑗. Unlike KPN (Mildenhall et al. 2018), we only need to estimate the per-pixel 1×1 kernels as the frames have been aligned by the optical flow-based alignment method.

As shown in Fig. 3c, each aligned frame 𝑦𝑦~𝑗 and the central frame 𝑦𝑦 are first concatenated and fed into a DCNN to predict 1×1 fusion kernels. As the input frames have been aligned, we employed a compact DCNN as the backbone to predict filters 𝜂2𝑗,𝑝, which contains five residual blocks, each residual block consists of two convolutional layers with 3×3 kernels and ReLU nonlinearity to generate 32-channel feature maps. Note that all 2J DCNN modules share the same parameters. Then spatially-adaptive fusion weights 𝐂𝑗=diag(𝜂2𝑗,𝑝∑𝐽𝑗=−𝐽𝜂2𝑗,𝑝) could be calculated using the filters 𝜂2𝑗,𝑝. Finally, the aligned frames 𝑦𝑦~𝑗 are averaged using fusion weights 𝐂𝑗 to generate the fused frame 𝑦𝑦⎯⎯⎯′. Through the estimation of a spatially varying kernel, our multiframe fusion becomes more robust in the presence of misregistration errors and other nonuniformly distributed noise (e.g., signal-dependent compression artifacts).

Denoising Module
This module implements the denoising function 𝑓(⋅) in Algorithm 1 and in principle, any existing image denoising network can be used as the denoising module. Here, we opt to use the U-net of Ronneberger et al. (2015) as the backbone network for the denoiser, which consists of an encoder and a decoder. The encoder and decoder contain five encoding blocks (EB) and four decoding blocks (DB), respectively. As shown in Fig. 3d, except the last EB, each EB consists of four convolutional layers with 3×3 kernels to generate 64-channel feature maps and followed by a downsampling layer to reduce the spatial resolution by a scaling factor of two, while the last EB only contains four convolutional layers to produce 64-channel feature maps and does not conduct downsampling. Each DB contains five convolutional layers. The first layer uses 1×1 kernels to reduce the number of feature channels from 128 to 64, and other layers produce the 64-channel feature maps with 3×3 kernels. Each DB is followed by a deconvolution layer to increase the spatial resolution of the feature maps with a scaling factor of two. To compensate for the loss of spatial information, the upsampled feature maps are concatenated with the feature maps of the same spatial dimension from the encoder. To save the total number of parameters, all denoising networks in the consecutive T stages are enforced to share the same network parameters.

Following Algorithm 1, the T denoising networks process the intermediate image 𝑥𝑥(𝑡) independently and thus cannot exploit the features from the previous denoising networks. Inspired by the success of the densenet (Huang et al. 2017), we propose to connect those feature maps from the previous denoisers to the following denoisers. As shown in Fig. 3a, the feature maps of the first encoding and last decoding blocks are connected to those of the subsequent denoisers. These long skip connections also help alleviate the notorious vanishing gradient problem (Bengio et al. 1994).

Fig. 6
figure 6
Denoising results for a noisy frame from Vimeo test set with noise level 𝜎𝑛=25. a Original frame; denoised frame by b V-BM4D (Maggioni et al. 2012) (31.66 dB, 0.7169), c RTA-LSM (Dong et al. 2018a) (32.27 dB, 0.7429), d VNLB (Pablo and Jean 2018) (32.27 dB, 0.7427), e TOFlow (Xue et al. 2019) (35.13 dB, 0.8925), f TOFlow-EN (36.75 dB, 0.9484), g ViDeNN (Claus and van Gemert 2019) (37.65 dB, 0.9533), h VNLnet (Davy et al. 2019) (36.63 dB, 0.9462), i FastDVDnet (Tassano et al. 2020) (37.25 dB, 0.9546), and j MAP-VDNet (39.30 dB, 0.9627)

Full size image
Network Training and Extensions
When using the deep optical flow estimation method for frame alignment, the alignment module is first pretrained on the training images of the Vimeo dataset (Xue et al. 2019). Then we jointly train the overall network by minimizing the following loss function

ΘΘ=argminΘΘ∑𝑖=1𝑀∑𝑗=−𝐽𝐽||(𝑦𝑦𝑖,𝑗;ΘΘ)−𝑥𝑥𝑖||1
(20)
where 𝑥𝑥𝑖 denotes the ith central clean frame to be recovered from its adjacent noisy frames 𝑦𝑦𝑖,𝑗, 𝑗=−𝐽,…,𝐽 and (⋅;ΘΘ) denotes the function of the overall VD network with parameters ΘΘ. The ADAM optimizer (Kingma and Ba 2015) was employed to train the network with parameters setting 𝛽1=0.9, 𝛽2=0.999, 𝜖=10−8, and the learning rate 10−4. The proposed network is implemented under PyTorch platform and trained using 4 Nvidia Titan XP GPUs.

In addition to synthetic noise, we have conducted real-world raw video denoising experiments on the CRVD dataset (Yue et al. 2020). Specifically, we use raw data of 11 indoor scenes from CRVD dataset as our dataset, we randomly choose video frames of scene 8 and scene 9 as the test dataset, and video frames of other dynamic indoor scenes as the training dataset. In our current implementation, we first pack the Bayer images into 4 channels (RGBG) and then clip these packed frames into image patches with the size of 128×128. Finally, we derive 72,900 sequences for training and other 16,200 sequences for testing, each sequence contains 7 temporal-sequential real noisy image patches and a clean reference image patch, and there is no overlap between the training dataset and the test dataset.

Furthermore, we have extended the proposed model to other video processing tasks such as video superresolution and compressed video artifact reduction. For the task of video superresolution, we use the widely used training dataset—e.g., Vimeo dataset (Xue et al. 2019). Following the common setting (Jo et al. 2018; Yi et al. 2019; Isobe et al. 2020), the low-resolution frame was generated by first Gaussian filtering with a standard deviation of 1.6 and then ×4 downsampling. Our network was trained on RGB channels and tested on the Y channel and RGB channels for video superresolution. For the task of compressed video artifact reduction, Vid70 (Yang et al. 2018) is a commonly used dataset in previous studies. To make a fair comparison, we apply the same training set and test set as Yang et al. (2018): 60 sequences in Vid70 for training and the remaining 10 sequences for testing. Besides, all compressed sequences were generated by HEVC standard, using HM 16.20 LDP mode with 𝑄𝑃=37 and 42. Similar to other competing algorithms, our network was trained and tested on the Y channel for compressed video artifact reduction.

Fig. 7
figure 7
Denoising results for a noisy frame of Stefan video of ASU test set with noise level 𝜎𝑛=50. a Original frame; denoised frame by b V-BM4D (Maggioni et al. 2012) (25.16 dB, 0.7965), c RTA-LSM (Dong et al. 2018a) (25.98 dB, 0.8731), d VNLB (Pablo and Jean 2018) (26.89 dB, 0.8885), e TOFlow (Xue et al. 2019) (26.70 dB, 0.8808), f TOFlow-EN (28.10 dB, 0.9155), g ViDeNN (Claus and van Gemert 2019) (26.74 dB, 0.8905), h VNLnet (Davy et al. 2019) (26.10 dB, 0.8751), i FastDVDnet (Tassano et al. 2020) (26.87 dB, 0.8951), and j MAP-VDNet (29.12 dB, 0.9312)

Full size image
Experimental Results
Experimental Setup
The proposed MAP-VDNet was trained using the Vimeo dataset (Xue et al. 2019), which consists of 91,701 sequences collected from 38,990 video clips. All collected sequences are divided into the training and test parts. Here, we consider two types of noise—i.e., additive white Gaussian and signal-dependent noise. In addition to the Vimeo dataset, we have also tested the proposed MAP-VDNet method on two sets of videos with different resolutions that are randomly selected from Derf’s Test Media Collection,Footnote1 denoted as ASU and DTMC-HD datasets, respectively. The sequences in the ASU dataset have a spatial resolution of 352×288 with a maximum of 300 frames, and the sequences in the DTMC-HD dataset have been downsampled to a resolution of 960×540 with 75 frames. Additive Gaussian noises with different variances were added to the clean frames to simulate noisy frames. Our network has been trained and tested on the RGB channels for VD. In our implementation, we adopted two optical flow estimation methods, i.e., the deep learning-based Spynet method (Ranjan and Black 2017) and the fast optical flow estimation method based on dense inverse search (Kroeger et al. 2016) which has been integrated into the OpenCV library. More comparative results are available at: https://see.xidian.edu.cn/faculty/wsdong/Projects/MAP-VDNet.htm.

Ablation Study
The effect of the core components To verify the effect of the core building blocks (i.e., frame alignment, adaptive fusion, and the denoiser module) of the proposed method on the denoising performance, we have implemented several variants of the proposed method as summarized below.

MAP-VDNet: it denotes the proposed method that used the Spynet (Ranjan and Black 2017) for frame alignment, spatially adaptive fusion, and U-net denoiser.

MAP-VDNet-Fast: it denotes the proposed method that used the fast optical flow estimation method (Kroeger et al. 2016), spatially adaptive fusion, and U-net denoiser.

MAP-VDNet-NAF: it denotes the proposed method that used the Spynet (Ranjan and Black 2017) for frame alignment, non-adaptive fusion, and U-net denoiser.

MAP-VDNet-DnCNN: it denotes the proposed method that used the Spynet (Ranjan and Black 2017) for frame alignment, spatially adaptive fusion, and DnCNN denoiser (Zhang et al. 2017a).

The above variants of the proposed method contain five denoising stages, i.e., 𝑇=5. To verify the effectiveness of the proposed MAP-VDNet, we also implemented the following video denoising method,

VD-Unet: it denotes the video denoising method that first uses the Spynet (Ranjan and Black 2017) to align the noisy frames and then sends the aligned 2𝐽+1 frames to the U-net denoiser to output the denoised central frame.

VD-Unet-Fast: it denotes the video denoising method that first uses the fast optical flow estimation method (Kroeger et al. 2016) to align the noisy frames and then sends the aligned 2𝐽+1 frames to the U-net denoiser to output the denoised central frame.

In the above methods, we set 𝐽=3. The average PSNR and SSIM results of these variants of the proposed method for noise level 25 are shown in Table 1. From Table 1 one can see that all variants of the proposed method outperform the VD-Unet and VD-Unet-Fast methods. The improvement over the VD-Unet-Fast method is much larger, verifying the effectiveness of the proposed MAP-VDNet method. When using fast optical flow estimation, the MAP-VDNet-Fast is slightly worse than MAP-VDNet. Without using the spatially adaptive fusion, the performance of MAP-VDNet-NAF is also worse than MAP-VDNet, and the performance gap is up to 0.32 dB, which demonstrates the effectiveness of the proposed spatially-adaptive fusion. When comparing MAP-VDNet-DnCNN with MAP-VDNet, we can see that performance gaps between them are small, which are in the range of (0.07–0.21) dB, verifying that the proposed method is non-sensitive to the choice of the deep denoisers.

The effect of the number of stages To show how the number of stages T (i.e., the number of iterations in Algorithm 1) affects the denoising performance, we have compared the proposed MAP-VDNet method with different stages. Fig. 4 shows the average PSNR performance as a function of 𝑇∈[1,6] for 𝜎𝑛=25 on two test datasets. It can be observed from Fig. 4 that more stages do lead to improved denoising performance. However, the performance improvements quickly and become saturated when 𝑇≥5. For the balance of performance and computational complexity, we have manually set 𝑇=5 in our current implementation.

The effect of the number of frames To verify the impact of the number of frames on VD performance, we have conducted comparative studies of the proposed MAP-VDNet with different numbers of input noisy frames: 𝐽=1, 𝐽=2, and 𝐽=3. The average PSNR/SSIM results on Vimeo and DTMC-HD datasets are shown in Table 2. It can be observed that the performance monotonically increases as the number of input frames increases. Therefore, we have set 𝐽=3 in our current implementation.

The effect of dense connections between the denoisers Finally, to demonstrate the effects of dense connections between the denoisers, we have conducted experiments comparing the networks with and without dense connections (denoted as MAP-VDNet-wo). The average results on Vimeo and DTMC-HD datasets are shown in Table 3. One can see that dense connections between denoisers are added, the average improvements on Vimeo and DTMC-HD datasets are 0.12 dB and 0.14 dB, respectively. These results verify the effectiveness of dense connections between denoisers in MAP-VDNet.

The inference process of the proposed iterative method In Fig. 5, we show visual and PSNR comparisons of some intermediate variables (e.g., aligned frames 𝑦𝑦~𝑗(𝑗=−𝐽,…,𝐽), the average fused frame 𝑦𝑦⎯⎯⎯, the spatially adaptive fused frame 𝑦𝑦⎯⎯⎯′ and outputs from all stages 𝑥𝑥(𝑡)(𝑡=1…,5)) to provide a more detailed description for the inference process of the proposed method. As shown in Fig. 3b, we first feed the reference frame 𝑦𝑦0 and each noisy frame 𝑦𝑦𝑗(𝑗=−𝐽,…,𝐽,𝑗≠0) into the alignment module and derive aligned frames 𝑦𝑦~𝑗. From the visualization in the top row of Fig. 5, we can see that there still exists some misaligned pixels in the aligned noisy frames 𝑦𝑦~𝑗(𝑗=−𝐽,…,𝐽,𝑗≠0). Inaccurate alignment could reduce denoising performance. To avoid the adverse influence of alignment bias, we introduce the spatially adaptive fusion derived from the MAP estimation with non-i.i.d Gaussian likelihood. Through visual comparisons of the average fusion 𝑦𝑦⎯⎯⎯=1(2𝐽+1)∑𝐽𝑗=−𝐽𝑦𝑦~𝑗 and the spatially adaptive fusion 𝑦𝑦⎯⎯⎯′=∑𝐽𝑗=−𝐽𝐂𝑗𝑦𝑦~𝑗 in Fig. 5, it can be observed that average fusion would yield undesirable artifacts caused by the alignment bias, while the output from spatially adaptive fusion has sharper edges. Therefore, spatially adaptive fusion is more effective in handling large-motion videos. Finally, we implement iterative optimizations of Eq. (19), and the visual results from all intermediate stages are demonstrated in the bottom row of Fig. 5. According to intermediate comparative results from different stages (e.g., 𝑥𝑥(𝑡)), one can see that more image details could be restored and the denoising performance (PSNR) has been improved with the increasing number of stages, the final output 𝑥𝑥(5) achieved the best denoising performance.

Comparison with Other State-of-the-Art Methods
We have compared the performance of the proposed MAP-VDNet-Fast and MAP-VDNet methods with several other state-of-the-art methods including both model-based (i.e., V-BM4D (Maggioni et al. 2012), RTA-LSM (Dong et al. 2018a) and VNLB (Pablo and Jean 2018)) and recently developed deep learning-based methods (i.e., TOFlow (Xue et al. 2019), ViDeNN (Claus and van Gemert 2019), VNLnet (Davy et al. 2019) and FastDVDnet (Tassano et al. 2020)). For a fair comparison, we have improved the TOFlow method (Xue et al. 2019) by adding more convolutional residual blocks, such that the total number of network parameters of TOFlow is comparable with that of the proposed MAP-VDNet. The enhanced TOFlow method is denoted as TOFlow-EN. All deep-learning based competing methods (i.e., TOFlow (Xue et al. 2019), TOFlow-EN, ViDeNN (Claus and van Gemert 2019), VNLnet (Davy et al. 2019) and FastDVDnet (Tassano et al. 2020)) were retrained on the same Vimeo training dataset, and we set the same number of input frames for these competing methods (i.e., 7) for fairness. Specifically, for VNLnet, we have retrained the color denoising model and the search space of the nonlocal patches is reduced to seven frames, and for ViDeNN and FastDVDnet, we increased the number of input frames to 7 and leading to a three-step cascaded version of FastDVDnet.

Table 4 shows the average results of the test methods on the Vimeo, ASU, and DTMC-HD test datasets. From Table 4, we can see that the VNLB method (Pablo and Jean 2018) is the most competitive algorithm among model-based VD methods. By adding more convolutional layers, the TOFlow-EN method significantly outperforms the original TOFlow method and becomes competitive with other deep learning-based state-of-the-art video denoising methods such as ViDeNN and FastDVDnet methods. With a similar number of parameters, the performance gains of MAP-VDNet over TOFlow-EN demonstrate the effectiveness of our maximum a posterior estimation framework. Both the proposed MAP-VDNet-Fast and MAP-VDNet methods significantly outperform the other competing methods. With a more accurate estimation of optical flow, the MAP-VDNet method performs slightly better than its counterpart that uses a fast optical flow estimation method. The visual comparisons of the denoised frames by the test methods are shown in Figs. 6, 7 and 8. From these figures, one can see that the proposed MAP-VDNet method can reproduce sharper edges and more details than the other competing methods.

Fig. 8
figure 8
Denoising results for a noisy frame of Station video of DTMC-HD test set with noise level 𝜎𝑛=50. a Original frame; denoised frame by b V-BM4D (Maggioni et al. 2012) (30.74 dB, 0.7445), c RTA-LSM (Dong et al. 2018a) (30.63 dB, 0.7682), d VNLB (Pablo and Jean 2018) (31.78 dB, 0.8020), e TOFlow (Xue et al. 2019) (31.99 dB, 0.8029), f TOFlow-EN (32.99 dB, 0.8391), g ViDeNN (Claus and van Gemert 2019) (32.72 dB, 0.8274), h VNLnet (Davy et al. 2019) (30.79 dB, 0.7759), i FastDVDnet (Tassano et al. 2020) (33.25 dB, 0.8497), and j MAP-VDNet (33.82 dB, 0.8608)

Full size image
Table 5 The average PSNR/SSIM denoising results on different test sets with signal-dependent noise (boldface highlights the best)
Full size table
Fig. 9
figure 9
Denoising results of a noisy frame from Bus sequence of ASU test set for signal-dependent noise. a The original frame; the frames denoised by b CBDNet (Guo et al. 2019) (26.42 dB, 0.7729), c TOFlow (Xue et al. 2019) (26.36 dB, 0.7839), d TOFlow-EN (26.65 dB, 0.7842), and e MAP-VDNet (28.09 dB, 0.8612)

Full size image
Video Denoising with Signal-Dependent Noise
Although the additive Gaussian noise is widely used in existing denoising studies, the distribution of the real noise of imaging sensors is much more complicated. To improve the denoising performance on real-world noisy images, more realistic signal-dependent noise models have been proposed (Foi et al. 2008). Here, we have adopted the Poisson-Gaussian noise model (Foi et al. 2008) to simulate more realistic noisy frames as follows

𝑦𝑗,𝑝∼(𝑥𝑗,𝑝,𝜎2𝑟+𝜎𝑠𝑥𝑗,𝑝),
(21)
where 𝑥𝑗,𝑝 is the clean pixel at position p, 𝜎𝑟 and 𝜎𝑠 are related to the sensor gain (ISO) of cameras. Similar to Guo et al. (2019), we simulate the noisy frames by randomly sampling 𝜎𝑠 and 𝜎𝑟 from the ranges [0, 0.16] and [0, 0.06], respectively. Then we retrained the proposed MAP-VDNet, the TOFlow (Xue et al. 2019) and its enhanced version TOFlow-EN on the new noisy training dataset. We have also compared with the recently developed blind image denoising method—i.e., CBDNet (Guo et al. 2019) that was also trained with the same realistic noise model. Table 5 shows the denoising results by the test methods on the Vimeo, ASU and DTMC-HD datasets. From these tables, we can see that MAP-VDNet performs much better than the competing methods. The average PSNR gains over the TOFlow-EN method (the second best) are 0.78 dB, 0.83 dB and 0.68 ‘1 on Vimeo, ASU and DTMC-HD datasets, respectively. Parts of the denoised frames by the test methods are shown in Fig. 9. It can be seen that the proposed MAP-VDNet can faithfully recover more details around the texture regions than other competing methods.

Raw Video Denoising with Real-World Noise
Recently, a raw video dataset with realistic noise is proposed, i.e., the CRVD dataset (Yue et al. 2020). Since the characteristics of real-world noise are quite different from that of simulated noise, we conduct raw video denoising comparisons on the CRVD dataset (Yue et al. 2020) to verify the real-world noise removal ability of the proposed MAP-VDNet.

The competing methods include the V-BM4D method (Maggioni et al. 2012), the TOFlow-EN method and the RViDeNet method (Yue et al. 2020). For fairness, we retrained TOFlow-EN and RViDeNet on the same CRVD training dataset and increased the input frames to 7 for RViDeNet (Yue et al. 2020). Due to the difference between sRGB video denoising and raw video denoising, some adaptions need to be conducted for these sRGB video denoising methods. For example, for 4-channel raw data, the V-BM4D method need to denoise the data of each channel separately. Regarding TOFlow-EN and MAP-VDNet methods, to exploit the information among channels, we average the 4-channel raw data to estimate optical flow in the alignment module, other processes are similar to sRGB video denoising. When training RViDeNet, since we don’t consider the image signal processing (ISP) pipeline, we no longer apply the sRGB domain loss in Yue et al. (2020), and we follow the training setting in Yue et al. (2020) that first pretrain a denoiser using an additional MOT Challenge dataset (Milan et al. 2016) and then keep the predenoising network fixed when training the RViDeNet on the same CRVD training dataset.

The real raw video denoising results have been shown in Table 6, one can see that all deep-learning-based video denoising methods outperform the model-based V-BM4D method. Among the supervised learning methods, the PSNR gain of the proposed MAP-VDNet over TOFlow-EN with a comparable number of parameters is 3.26 dB for the CRVD test dataset. In addition, even though the number of parameters of RViDeNet is approximately triple of ours, the proposed method still outperforms RViDeNet by 1.46 dB. The experimental results in Table 6 have proved that the proposed MAP-VDNet could achieve excellent performance for the task of real-world raw video denoising.

Table 6 The average PSNR/SSIM denoising results on CRVD test dataset for real raw video denoising (boldface highlights the best)
Full size table
Video Superresolution
Recently, video superresolution has been widely studied in Caballero et al. (2017); Tao et al. (2017); Sajjadi et al. (2018); Jo et al. (2018); Xue et al. (2019); Haris et al. (2019); Wang et al. (2019); Yi et al. (2019); Tian et al. (2020); Isobe et al. (2020). In this paper, to verify the generalization ability of our proposed framework, we make the proposed MAP-VDNet applied to the video superresolution by adding an upscale module before the alignment module in Fig. 3a. For the architecture of the ×4 up-scale module, we employ a simple up-scale network without any special design and a sub-pixel convolutional layer (Shi et al. 2016) to conduct the upsampling operation.

The competing methods consist of the FRVSR method (Sajjadi et al. 2018), the DUF method (Jo et al. 2018), the TOFlow method (Xue et al. 2019), the RBPN method (Haris et al. 2019), the EDVR method (Wang et al. 2019), the PFNL method (Yi et al. 2019) and the TGA method (Isobe et al. 2020). Note that other comparative results come from their own publication or recent work (Isobe et al. 2020). Since Vid4 (Liu and Sun 2014) is a benchmark test set for video superresolution, we demonstrate the comparisons of the performance and number of parameters with other competing methods on Vid4 test set for ×4 video superresolution in Table 7. From the results, one can see that the proposed MAP-VDNet outperforms most competing methods except the TGA method (Isobe et al. 2020). Although the performance reflected by PSNR of our method is slightly lower than the TGA method (Isobe et al. 2020), another metric (SSIM) demonstrates the performance of our method is better and the number of parameters of the proposed MAP-VDNet is far less than the TGA method (Isobe et al. 2020). The visual comparisons are illustrated in Fig. 10, the proposed MAP-VDNet could restore more clear details than other competing methods.

Table 7 The average PSNR/SSIM super-resolution results on Vid4 test set for ×4 video super-resolution (boldface highlights the best)
Full size table
Fig. 10
figure 10
Video superresolution results (×4 up-sampling) for a low-resolution frame of City video of Vid4 test set. a Original frame; superresolved frame by b TOFlow (Xue et al. 2019) (25.40 dB, 0.7081), c FRVSR (Sajjadi et al. 2018) (26.78 dB, 0.8136), d RBPN (Haris et al. 2019) (26.51 dB, 0.7929), e EDVR-L (Wang et al. 2019) (26.86 dB, 0.7995), f DUF-52L (Jo et al. 2018) (26.89 dB, 0.8147), g PFNL (Yi et al. 2019) (26.94 dB, 0.8298), h MAP-VDNet (27.28 dB, 0.8369)

Full size image
Table 8 The average ΔPSNR (dB) on test sequences of the Vid70 dataset (boldface highlights the best)
Full size table
Table 9 Running time on 90 frames of size 352×288
Full size table
Compressed Video Artifact Reduction
In addition to video denoising and video superresolution, compressed video quality enhancement (Yang et al. 2017, 2018; Lu et al. 2018; Yang et al. 2019a; Xu et al. 2019b; Yang et al. 2019b; Deng et al. 2020; Guan et al. 2021 is an important and challenging video processing task. The quality fluctuation among compressed frames increases the difficulty of recovery. We verify the effectiveness of our MAP-VDNet model on the compressed video artifact reduction task. We compared with the DSCNN method (Yang et al. 2017), the MFQE method (Yang et al. 2018), the QG-ConvLSTM method (Yang et al. 2019a), the NL-ConvLSTM method (Xu et al. 2019b) and the MFQE 2.0 method (Guan et al. 2021). And the results of other competing algorithms are obtained from their publications. Among the competing methods, the MFQE 2.0 method (Guan et al. 2021) was trained on a more comprehensive training dataset which consists of 108 videos. Due to the sensitivity of MFQE 2.0 to the training dataset, we reported the results in their published paper. Note that these competing methods usually design their algorithms specifically for compressed video frames encoded by a fixed quality factor (QP value) -e.g., the NL-ConvLSTM method (Xu et al. 2019b) introduces a non-local strategy into the ConvLSTM module to learn spatiotemporal correlation across frames, and the MFQE methods (Yang et al. 2018; Guan et al. 2021) exploit peak quality frames that located by the detector to enhance the quality of adjacent compressed frames. Their generalization property often remains questionable; by contrast, our algorithm is not optimized for the task of compressed video quality enhancement but still achieves excellent performance. Table 8 illustrates the average PSNR gain (dB) on the test sequences of the Vid70 dataset. The results in Table 8 have shown that the proposed MAP-VDNet model is also effective for the task of compressed video artifact reduction, which justifies its excellent generalization property.

Complexity Analysis and Discussions
We have compared the proposed MAP-VDNet-Fast and MAP-VDNet method with other deep-learning-based competing methods in Fig. 1. The comparison shows the trade-off between the complexity (as measured by the number of parameters) and the denoising performance (average PSNR values). It can be observed that with a similar number of parameters, our MAP-VDNet outperforms TOFlow-EN over 1dB on DTMC-HD test set for 𝜎𝑛=25. With fewer number of parameters, our MAP-VDNet-Fast could also achieve excellent denoising performance compared to other competing video denoising algorithms. Additionally, Table 9 shows the comparison of the actual running time of different denoising methods on a Nvidia Titan XP GPU. By using a fast optical flow estimation method that is implemented with OpenCV library and only runs on CPU, the running time of our MAP-VDNet-Fast is comparable to that of TOFlow (note that ours outperforms TOFlow by over 2dB as shown in Fig. 1).

Differences with other deep unfolding networks To the best of our knowledge, this is the first work that uses optimization formation to guide the design of deep neural networks for video denoising. When compared with the image denoising problem (Zhang et al. 2017b; Dong et al. 2018b), the video denoising problem is more difficult to handle because the temporal redundancy among adjacent frames needs to be modeled and exploited. In this paper, we formulate the multiframe fusion module in the MAP estimation architecture, and each iteration can be efficiently computed by the proposed MAP-VDNet. Moreover, this work explicitly takes misalignment errors into account and demonstrates a principled solution based on robust multiframe fusion.

Conclusions
In this paper, we propose a novel MAP-based video denoising algorithm MAP-VDNet. Unlike existing model-based Bayesian video denoising, we strive to optimize the parameters of a network-based denoising algorithm without involving hand-crafted procedures. Different from previous deep learning-based methods with heuristically designed network architectures, the construction of MAP-VDNet is based on an explainable optimization-inspired solution to video denoising derived from the classical MAP estimation framework. Specifically, we first propose an iterative MAP-based video denoising algorithm based on a realistic observation model of noisy frames, which allows us to solve the denoising problem in a principled manner. Combining a DCNN-based image denoiser module with an optical flow-based image alignment module, we then demonstrate how to unfold the iterative video denoising algorithm into a multistage implementation in which both image denoising and image fusion modules can be jointly trained. Moreover, we propose a robust multiframe fusion scheme by predicting the adaptive fusion coefficient on a pixel-by-pixel basis, which further improves the performance of video denoising in the presence of misalignment. Extensive experimental results on three video datasets show that the proposed method significantly outperforms the existing video denoising methods in terms of both objective and subjective quality of restored video frames. Additionally, we have achieved promising experimental results on other video restoration tasks such as real-world video denoising, video superresolution and compressed video artifact reduction, which demonstrate the good generalization property of the proposed MAP-VDNet.