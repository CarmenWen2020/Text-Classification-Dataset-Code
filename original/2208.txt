Accurate camera calibration is a precondition for many computer vision applications. Calibration errors, such as wrong model assumptions or imprecise parameter estimation, can deteriorate a systemâ€™s overall performance, making the reliable detection and quantification of these errors critical. In this work, we introduce an evaluation scheme to capture the fundamental error sources in camera calibration: systematic errors (biases) and uncertainty (variance). The proposed bias detection method uncovers smallest systematic errors and thereby reveals imperfections of the calibration setup and provides the basis for camera model selection. A novel resampling-based uncertainty estimator enables uncertainty estimation under non-ideal conditions and thereby extends the classical covariance estimator. Furthermore, we derive a simple uncertainty metric that is independent of the camera model. In combination, the proposed methods can be used to assess the accuracy of individual calibrations, but also to benchmark new calibration algorithms, camera models, or calibration setups. We evaluate the proposed methods with simulations and real cameras.

Access provided by University of Auckland Library

Introduction
Fig. 1
figure 1
Proposed evaluation scheme for camera calibration. a Target-based camera calibration where intrinsic and extrinsic camera parameters are obtained through bundle adjustment. b Method to detect systematic errors (biases). The bias ratio (BR) quantifies the fraction of systematic error in the calibration residuals. c Quantification of the uncertainty. We propose a novel resampling-based method to estimate the covariance matrix of model parameters. Propagating the uncertainty to image space, the expected mapping error (EME) then provides a model-independent uncertainty metric (Color figure online)

Full size image
Many applications in 3D computer vision rely on precisely knowing the cameraâ€™s mapping from the 3D world to the 2D image. This mapping ğ‘ğ‘:â„3â†’â„2 is obtained during camera calibration. Errors in the calibration will lead to false assumptions on the mapping, which can impact all subsequent inferences and deteriorate a systemâ€™s overall performance (Ozog and Eustice 2013; Svoboda and Svoboda 1996; Cheong and Peh 2004; Zucchelli and Kosecka 2001; Cheong and Xiang 2011; Abraham and FÃ¶rstner 1998). The detection and prevention of errors is therefore a critical aspect of the calibration.

To ensure high accuracy calibration, recent advances have focused on easily applicable calibration solutions. Starting with the seminal work by (Zhang 2000), many contributions have been made with regard to improving and simplifying the calibration process (e.g. Mei and Rives 2007; Furgale et al. 2013; Scaramuzza et al. 2006). Multiple open-source toolboxes provide directly applicable functions for camera calibration, including the detection of calibration markers, different camera models and the complete bundle adjustment (OpenCV: OpenCV Tutorial Camera Calibrations 2021; ROS: ROS Tutorial MonocularCalibration 2021). Furthermore, several current approaches aim at tackling the problem of high uncertainty by developing systems for guided calibration (Peng and Sturm 2019; Rojtberg and Kuijper 2018; Richardson et al. 2013). These systems infer most informative camera and calibration body constellations and guide users towards taking these images. Thus, both the data collection and the estimation process have been simplified substantially.

Despite this progress in the calibration process, few have focused explicitly on tools to assess and interpret calibration results. One of the most common tools to assess the quality of a calibration remains to be the inspection of reprojection errors, as they are directly accessible after every calibration (OpenCV: OpenCV Tutorial Camera Calibrations 2021; ROS: ROS Tutorial MonocularCalibration 2021). While residual errors reveal large systematic errors or unsuccessful optimizations, they cannot provide a full picture on the quality of the calibration. Such a full evaluation scheme would be critical to ensure high accuracy calibrations, but also to benchmark new calibration setups or solutions.

In general, the error sources of camera calibration can be divided into systematic errors (bias) and uncertainty (variance) (FÃ¶rstner and Wrobel 2016, p. 116; Hagemann et al. 2020). As in any data modeling problem, systematic errors occur if a chosen model is not sufficiently flexible or imposes false assumptions on the data (Fig. 1b). In camera calibration, systematic errors can be caused by a camera projection model not being able to reflect the true geometric camera characteristics, e.g. if lens distortions are neglected. But also other sources, such as an uncompensated rolling shutter (Oth et al. 2013), or non-planarity of the calibration target (Lavest et al. 1998) can result in systematic errors.

A high uncertainty, on the other hand, describes that model parameters could not be estimated reliably based on the available data. In general, uncertainties occur if the data is subject to noise and the amount of data is limited (Fig. 1c). In camera calibration, high uncertainties are commonly caused by a lack of images used for calibration, bad coverage in the image, or a non-diversity in calibration target poses (Sturm et al. 1999; Peng and Sturm 2019; Rojtberg and Kuijper 2018; Richardson et al. 2013).

In this paper, we address the challenge of quantifying both, systematic errors and uncertainty, in target-based camera calibration. Our goal is to reliably detect and quantify both types of errors and to condense the result to easily interpretable measures. Extending our recent work on evaluation metrics (Hagemann et al. 2020), we provide four main contributions (Fig. 1):

A method to detect systematic errors (biases) in camera calibration. The method is based on estimating the observational noise in marker detections and thereby disentangles random from systematic errors in the calibration residual.

A method to estimate parameter uncertainties under non-ideal conditions. We show that the standard estimator for the covariance matrix underestimates the uncertainty under non-ideal conditions and propose a resampling approach for reliable estimation.

A method to predict the expected mapping error (EME) in image space, which quantifies the uncertainty (variance) in model parameters in a model-independent way.

A detailed comparison of existing methods to quantify bias and uncertainty.

The main advantages of our approach towards quantifying systematic errors and uncertainty are that they are (a) applied as post-processing and thus build upon already captured data and (b) abstract from the underlying camera model, resulting in comparable results across different calibrations. We evaluate the proposed methods with both simulations and real cameras.

Related Work
Detecting Systematic Errors
Detecting systematic errors in camera calibration poses a challenge, because unlike the calibration of other measurement devices, there is typically no ground-truth device to compare with. Instead, camera characteristics are typically inferred indirectly through observations of well-known 3D objects.

The most common approach to detect systematic errors is by inspection of reprojection errors, i.e. the difference between predicted and observed image coordinates on the calibration dataset (OpenCV: OpenCV Tutorial Camera Calibrations 2021; ROS: ROS Tutorial MonocularCalibration 2021; Beck and Stiller 2018; Schoeps et al. 2019). As the observational noise is typically assumed to be normally distributed, the reprojection errors (i.e. the residuals) should also follow a Gaussian distribution. To detect systematic errors, the two-dimensional distribution of residuals can be visualized and deviations from the expected Gaussian distribution are an indicator for systematic errors (Beck and Stiller 2018).

To put this qualitative comparison into numbers, recent work proposed the KL-divergence between a 2D normal distribution, and the empirical distribution of reprojection error vectors as a measure of biasedness (Schoeps et al. 2019). More precisely, it was proposed to compute the median KL-divergence over a 50x50 grid across the image, where low values indicate a large similarity to the Gaussian (low bias) and high values indicate dissimilarities (higher bias).

In addition to the distribution of residuals, the magnitude of residuals is a common indicator for systematic errors. A common approach is to compare the root mean squared error (RMSE) or reconstruction result against expected values obtained from earlier calibrations or textbooks (Luhmann et al. 2013). However, the magnitude of residuals varies for different cameras, lenses, calibration targets, and marker detectors and therefore only allows capturing large errors in general.

Professional photogrammetry often makes use of precisely manufactured and highly accurate 3D calibration bodies (Rautenberg and Wiggenhagen 2002). Images captured from predefined viewpoints are then used to perform a 3D reconstruction of the calibration body, where different length ratios and their deviation from the ground truth are compared against empirical data. While these methods are both, highly accurate and repeatable, they are often not feasible or too expensive for typical research and laboratory settings and require empirical data for the camera under test.

Quantification of Uncertainty
The uncertainty of a calibration can, in general, be quantified by the (co-)variance in estimated model parameters ğœ‰Ì‚ ğœ‰Ì‚ . As calibration is typically performed as a least squares estimation, the standard estimator for the covariance matrix Î£Î£^ğœ‰Ì‚ ğœ‰Ì‚ ğœ‰Ì‚ ğœ‰Ì‚ ,std is given by an approximated backpropagation of the observational noise in the data,

Î£Î£^ğœ‰Ì‚ ğœ‰Ì‚ ğœ‰Ì‚ ğœ‰Ì‚ ,std=ğ‘ Ì‚ 2ğ‘‘(ğ½ğ½ğ‘‡calibğ½ğ½calib)âˆ’1,
(1)
where ğ½ğ½calib is the Jacobian of calibration residuals and

ğ‘ Ì‚ 2ğ‘‘=MSEcalib(1âˆ’ğ‘ğ‘ƒğ‘)
(2)
is the estimated accuracy, obtained from the mean squared error of the calibration MSEcalib, the number of parameters ğ‘ğ‘ƒ and the number of observations N (Luhmann et al. 2013, p. 92â€“96; Hartley and Zisserman 2004, p.141-142).

Since the covariance matrix is high dimensional and its interpretation is non-trivial, it is typically reduced to a scalar metric. Typical choices are the trace of the covariance matrix (Peng and Sturm 2019), or the maximum index of dispersion (Rojtberg and Kuijper 2018). However, given the variety of camera models, from a simple pinhole model with only three parameters, up to local camera models with around 105 parameters (Beck and Stiller 2018; Schoeps et al. 2019), parameter variances are difficult to interpret and not comparable across camera models.

To address this issue, the parameterâ€™s influence on the mapping can be considered. The metric maxERE (Richardson et al. 2013) propagates the parameter covariance to image space by means of a Monte Carlo simulation. More precisely, a 5x5 grid of 3D points is projected into the image with a sampled set of model parameters. This yields a distribution of image coordinates for each grid point. The value of maxERE is then defined by the standard deviation of the most uncertain grid point. The observability metric (Strauss 2015) uses an analytical approach and weights the uncertainty in estimated parameters with the parametersâ€™ influence on the mapping. This is achieved by linearly approximating the influence of parameter errors on a model cost function. Importantly, this model cost function takes into account that errors in intrinsic parameters can partially be compensated by a change of the camera coordinate system (i.e. by adjusting the extrinsics). The observability metric is then defined by an increase in calibration cost in the least observable parameter direction, where low observability values correspond to high uncertainty.

While both of these metrics provide valuable information about the uncertainty, there are some shortcomings in terms of how uncertainty is quantified. The observability metric does not consider the whole uncertainty, but only the most uncertain parameter direction. Furthermore, it quantifies uncertainty in terms of an increase in the calibration cost, which can be difficult to interpret. The metric maxERE quantifies uncertainty in image space and is thus easily interpretable. However, it relies on a Monte Carlo Simulation instead of an analytical approach and it does not incorporate potential compensations of errors in the intrinsics by a change of the coordinate system.

A final metric to assess the accuracy of a calibration are the reprojection errors on a test dataset (Sun and Cooperstock 2006; Richardson et al. 2013; Semeniuta 2016). As in machine learning, test errors significantly higher than training errors (calibration residuals) indicate an overfit which is directly related to an uncertainty in model parameters. While a test error is a valuable measure for the accuracy of a calibration, it requires capturing a full additional test dataset. In practice, this overhead can rarely be afforded which is why we focus on metrics that rely only on the calibration data.

Calibration Framework
Camera Projection Modeling
From a geometric perspective, cameras project points in the 3D world to a 2D image (Hartley and Zisserman 2004). This projection can be expressed by a function ğ‘ğ‘:â„3â†’â„2 that maps a 3D point ğ‘¥ğ‘¥=(ğ‘¥,ğ‘¦,ğ‘§)ğ‘‡ from a world coordinate system to a point ğ‘¢ğ‘¢Â¯=(ğ‘¢Â¯,ğ‘£Â¯)ğ‘‡ in the image coordinate system. The projection can be decomposed into a coordinate transformation from the world coordinate system to the camera coordinate system ğ‘¥ğ‘¥â†’ğ‘¥ğ‘ğ‘¥ğ‘ and the projection from the camera coordinate system to the image ğ‘ğ¶ğ‘ğ¶:ğ‘¥ğ‘ğ‘¥ğ‘â†’ğ‘¢ğ‘¢Â¯:

ğ‘¢ğ‘¢Â¯=ğ‘ğ‘(ğ‘¥ğ‘¥,ğœƒğœƒ,Î Î )=ğ‘ğ¶ğ‘ğ¶(ğ‘¥ğ‘ğ‘¥ğ‘,ğœƒğœƒ)=ğ‘ğ¶ğ‘ğ¶(ğ‘…ğ‘…ğ‘¥ğ‘¥+ğ‘¡ğ‘¡,ğœƒğœƒ),
(3)
where ğœƒğœƒ are the intrinsic camera parameters and Î Î  are the extrinsic parameters describing the rotation ğ‘…ğ‘… and translation ğ‘¡ğ‘¡. For a plain pinhole model, the intrinsic parameters are the focal length f and the principal point (ğ‘ğ‘¥,ğ‘ğ‘¦), i.e. ğœƒğœƒ=(ğ‘“,ğ‘ğ‘¥,ğ‘ğ‘¦). For this case, the projection ğ‘ğ¶ğ‘ğ¶(ğ‘¥ğ‘ğ‘¥ğ‘,ğœƒğœƒ) is given by

ğ‘¢Â¯=ğ‘“/ğ‘§ğ‘â‹…ğ‘¥ğ‘+ğ‘ğ‘¥,ğ‘£Â¯=ğ‘“/ğ‘§ğ‘â‹…ğ‘¦ğ‘+ğ‘ğ‘¦.
(4)
To account for lens distortions, more complex models are needed. In the following, we will consider a standard pinhole camera model with varying numbers of radial distortion parameters ğ‘˜1, ğ‘˜2, ğ‘˜3 and different focal lengths ğ‘“ğ‘¥, ğ‘“ğ‘¦

ğ‘¢Â¯=ğ‘“ğ‘¥ğ‘¥ğ‘ğ‘§ğ‘(1+ğ‘˜1ğ‘Ÿ2+ğ‘˜2ğ‘Ÿ4+ğ‘˜3ğ‘Ÿ6)+ğ‘ğ‘¥,ğ‘£Â¯=ğ‘“ğ‘¦ğ‘¦ğ‘ğ‘§ğ‘(1+ğ‘˜1ğ‘Ÿ2+ğ‘˜2ğ‘Ÿ4+ğ‘˜3ğ‘Ÿ6)+ğ‘ğ‘¦,
(5)
where ğ‘Ÿ=(ğ‘¥ğ‘ğ‘§ğ‘)2+(ğ‘¦ğ‘ğ‘§ğ‘)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš. For more wide-angled lenses, we use the OpenCV fisheye model (OpenCV: OpenCV Fisheye Camera Model 2021).

Calibration
Our methods build upon target-based camera calibration, in which planar targets are imaged in different poses relative to the camera (see Fig. 1a). Without loss of generality, we assume a single chessboard-style calibration target and a single camera. The calibration dataset is a set of images îˆ²={frameğ‘—}ğ‘îˆ²ğ‘—=1. The chessboard calibration target contains a set of corners îˆ¯={cornerğ‘–}ğ‘îˆ¯ğ‘–=1. The geometry of the target is well-defined, thus the 3D coordinates of chessboard-corner i in the world coordinate system are known as ğ‘¥ğ‘¥ğ‘–=(ğ‘¥ğ‘–,ğ‘¦ğ‘–,ğ‘§ğ‘–)ğ‘‡. The image coordinates of chessboard-corners in each image are determined by a corner detection algorithm (StrauÃŸ et al. 2014), giving the observations ğ‘¢ğ‘¢ğ‘–=(ğ‘¢ğ‘–,ğ‘£ğ‘–)ğ‘‡. Depending on the corner detector, the perspective, blur in the image, and many other factors, the observed coordinates ğ‘¢ğ‘¢ğ‘– will deviate from the true image points ğ‘¢Â¯ğ‘¢Â¯ğ‘– by a certain error ğœ–ğ‘‘ğœ–ğ‘‘, called observational noise. This error is typically assumed to be independent identically distributed (i.i.d.) ğœ–ğ‘‘ğœ–ğ‘‘âˆ¼îˆº(00,ğœ2ğ‘‘ğ¼ğ¼) with detector variance ğœ2ğ‘‘. Although more elaborate descriptions of the observational noise have been proposed (e.g. (Peng and Sturm 2019)), we for simplicity stick to the most commonly used i.i.d. description for introducing the new concepts.

Following (Zhang 2000), parameters are estimated by minimizing a calibration cost function, defined by the sum of squares of reprojection errors

ğœ–2res=âˆ‘ğ‘—âˆˆîˆ²âˆ‘ğ‘–âˆˆîˆ¯||ğ‘¢ğ‘¢ğ‘–ğ‘—âˆ’ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘–ğ‘—,ğœƒğœƒ,Î Î ğ‘—)||2.
(6)
For the sake of simplicity, we present formulas for non-robust optimization here. To reduce the impact of potential outliers, we advise robustification e.g. by using a Cauchy kernel. Optimization is performed by a non-linear least-squares algorithm, which yields parameter estimates (ğœƒÌ‚ ğœƒÌ‚ ,Î Ì‚ Î Ì‚ )=argmin(ğœ–2res).

To evaluate the calibration result, one of the most common metrics is the root mean squared error (RMSE) over all N individual corners coordinates (observations) in the calibration dataset îˆ² (Zisserman  2004, p.133):

RMSEcalib=1ğ‘âˆ‘ğ‘—âˆˆîˆ²âˆ‘ğ‘–âˆˆîˆ¯||ğ‘¢ğ‘¢ğ‘–ğ‘—âˆ’ğ‘ğ‘(ğ‘¥ğ‘¥ğ‘–ğ‘—,ğœƒÌ‚ ğœƒÌ‚ ,Î Ì‚ Î Ì‚ ğ‘—)||2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš,
(7)
or its squared version, the mean squared error MSEcalib=RMSE2calib. Note that the number of observations N is twice the number of visible corners, i.e. ğ‘=2ğ‘îˆ¯ğ‘îˆ² if the board was fully visible in all images.

The covariance matrix in estimated model parameters ğœ‰Ì‚ ğœ‰Ì‚ =(ğœƒÌ‚ ğœƒÌ‚ ,Î Ì‚ Î Ì‚ ) is theoretically given by the backpropagation of the covariance of the corner detector:

Î£Î£ğœ‰Ì‚ ğœ‰Ì‚ ğœ‰Ì‚ ğœ‰Ì‚ =(ğ½ğ½ğ‘‡calibÎ£Î£âˆ’1ğœ–ğ‘‘ğœ–ğ‘‘ğœ–ğ‘‘ğœ–ğ‘‘ğ½ğ½calib)âˆ’1=ğœ2ğ‘‘(ğ½ğ½ğ‘‡calibğ½ğ½calib)âˆ’1,
(8)
where Î£ğœ–ğ‘‘ğœ–ğ‘‘ğœ–ğ‘‘ğœ–ğ‘‘Î£ğœ–ğ‘‘ğœ–ğ‘‘ğœ–ğ‘‘ğœ–ğ‘‘=ğœ2ğ‘‘ğ¼ğ¼ is the covariance matrix of the corner detector and ğ½ğ½calib is the Jacobian of calibration residuals (Hartley and Zisserman 2004, p.141-142). As the variance of the corner detector ğœ2ğ‘‘ is not known a priori, it is typically replaced by the accuracy estimate ğ‘ Ì‚ 2ğ‘‘=MSEcalib/(1âˆ’ğ‘ğ‘ƒğ‘) (Luhmann et al. 2013, p.92-96), giving

Î£Î£^ğœ‰Ì‚ ğœ‰Ì‚ ğœ‰Ì‚ ğœ‰Ì‚ ,std=ğ‘ Ì‚ 2ğ‘‘(ğ½ğ½ğ‘‡calibğ½ğ½calib)âˆ’1.
(9)
The covariance matrix contains the (co-)variances of both, the intrinsic parameters ğœƒÌ‚ ğœƒÌ‚  and extrinsic parameters Î Ì‚ Î Ì‚ ğ‘— for each image j. The covariance matrix of intrinsic parameters Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚  can then be extracted as a submatrix of the full covariance matrix.

Fig. 2
figure 2
Detecting systematic errors. a Computation of the bias ratio: The calibration target is virtually decomposed into local groups, whose poses are re-optimized separately. Re-optimizating the poses can compensate a large fraction of potential systematic errors, so that the residuals provide an estimate for the random error contribution (detector variance ğœ2ğ‘‘). Using this estimate, the absolute bias ğœ–bias and the BR can be computed. b Exemplary results of the BR for a simulated C(6) camera, when calibrating with models of increasing complexity. The BR clearly indicates the systematic error when using insufficiently complex models (C(3), C(5)) (Color figure online)

Full size image
Detecting Systematic Errors: The Bias Ratio
The major challenge in quantifying systematic errors is the fact that the residuals are always a superposition of observational noise and potential systematic errors. As the magnitude of observational noise depends on multiple factors, including the camera, blur in the image, the calibration setup and many more, it is not known a priori. Consequently, the residuals do not provide direct information on the amount of bias.

In the following, we derive a novel method to disentangle these two contributions and thereby quantify the fraction of systematic error in the mean squared reprojection error MSEcalib of a calibration. Following the assumptions made in Sect. 3.2, MSEcalib can be formulated as a superposition of random errors, caused by the observational noise (detector variance ğœ2ğ‘‘), and a systematic error contribution. Taking into account the number of parameters and the number of observations, one finds asymptotically (by augmentation of Hartley and Zisserman 2004, p. 136)

MSEcalib=ğœ2ğ‘‘(1âˆ’ğ‘îˆ¼ğ‘)î„½î„¾î…î…‹î…‹î…‹î…‹random error+ ğœ–2bias(1âˆ’ğ‘îˆ¼ğ‘) î„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹systematic error contribution,
(10)
where ğ‘îˆ¼ is the total number of free intrinsic and extrinsic parameters and ğœ–bias denotes the bias introduced through systematic errorsFootnote1. The detector variance ğœ2ğ‘‘ is generally camera-dependent and not known a priori. Thus, to disentangle random and systematic error contributions to MSEcalib, the detector variance ğœ2ğ‘‘ must be determined independently.

The rationale behind many calibration approaches, and in particular guided calibration, is to find most informative camera-target configurations. To estimate ğœ2ğ‘‘, we propose the opposite. We explicitly use configurations which are less informative for calibration but at the same time also less likely to be impacted by systematic errors. Such an uninformative configuration is given if a calibration target only covers small local image regions, as a large fraction systematic errors can then be compensated by adjusting the target pose.

To obtain such an uninformative configuration without capturing additional data, we decompose the calibration target virtually into several smaller calibration targets î‰‚={targetğ‘£}ğ‘î‰‚ğ‘£=1, usually consisting of exclusive sets of the four corners of a checker board tile (cf. Fig. 2a). The pose of each virtual calibration target in each image of the calibration dataset is then re-estimated individually while keeping the camera intrinsic parameters fixed. Here, pose estimation is overdetermined with a redundancy of two (four tile corners and six pose parameters). From the resulting mean squared errors, MSEğ‘£ with ğ‘£âˆˆî‰‚, we compute estimates of ğœ2ğ‘‘ via (10). As systematic errors are mostly compensated after separately re-optimizing the poses, it can be assumed that ğœ–2bias is negligible here:

ğœË†2ğ‘‘ğ‘£=MSEğ‘£1âˆ’ğ‘ğ‘ƒ,ğ‘£ğ‘obs,ğ‘£=MSEğ‘£1âˆ’68=4 MSEğ‘£.
(11)
To obtain an overall estimate of ğœË†2ğ‘‘, we compute the MSE in (11) across the residuals of all virtual targets, using the median absolute deviation (MAD) as a robust estimatorFootnote2.

Given an estimate for the detector variance ğœË†2ğ‘‘, we can use the decomposition of MSEcalib (10) to determine the systematic error contribution

ğœ–Ì‚ 2bias=max(MSEcalib1âˆ’ğ‘ğ‘ƒğ‘âˆ’ğœË†2ğ‘‘,0)=max(ğ‘ Ë†2ğ‘‘âˆ’ğœË†2ğ‘‘,0),
(12)
where max(â‹…,â‹…) ensures that the expression does not get negative due to the statistical nature of MSEcalib and ğœË†2ğ‘‘. Finally, as a simple metric between zero and one, we compute the bias ratio as

BR=ğœ–Ì‚ 2bias(1âˆ’ğ‘ğ‘ƒğ‘)MSEcalib.
(13)
The bias ratio is close to zero for unbiased calibration and close to one if the results are dominated by systematic errors. In combination, the bias ratio BR and the amount of bias ğœ–Ì‚ bias provide an intuitive measure for the systematic error in a calibration.

figure a
Note, that this method is closely related to an F-test for regression model comparison (Doran and Doran 1989). By decomposing the calibration target, we are constructing a much more complex model. If there was no bias in the calibration, i.e. if the data was already explained without decomposing the target, the re-optimizations would only result in a further compensation of the observational noise. The reduction in the mean squared error (MSE) would then be fully explained by the larger number of free parameters. On the other hand, if the calibration was biased, the reduction in MSE would be larger than expected by merely adding unnecessary parameters. This additional reduction in MSE is the basis for computing the amount of bias ğœ–bias and the bias ratio.

Generally, this kind of analysis can be performed for any separableFootnote3 calibration target. In practice, one may also choose to cite ğµğ‘…â€¾â€¾â€¾â€¾âˆš, as linear quantities may be easier to interpret than the quadratic BR. The algorithm is summarized below.

Uncertainty estimation
Fig. 3
figure 3
Resampling-based uncertainty estimation. a The standard estimator for the covariance matrix of model parameters underestimates the uncertainty in non-ideal, real-world scenarios. The histograms show the distribution of the focal length ğ‘“ğ‘¥ and radial distortion parameter ğ‘Ÿ1 when running calibrations with multiple image subsets of the same camera (manta lens). The observed variance is larger than predicted by the standard covariance estimator. b Resampling-based uncertainty estimation by bootstrapping the calibration dataset. Bootstrap samples are obtained by sampling images with replacement from the original dataset. For each bootstrap sample a calibration is performed, yielding a bootstrap distribution of estimates ğœƒÌ‚ ğœƒÌ‚ . Finally, the covariance matrix is computed as the covariance of the bootstrap distribution (Color figure online)

Full size image
Fig. 4
figure 4
Validation of resampling based uncertainty estimation. For an ideal simulation, the standard method (std), the bootstrapping method (BS) and the approximated bootstrapping method (aBS) provide similar estimates for the uncertainty and the corresponding EMEs coincide with the average true error in image space (horizontal line). When simulating an underfit, the standard method significantly underestimated the uncertainty, while the estimates of both resampling methods remained close to the average error. For real cameras, the results were similar to a simulated underfit. The standard method underestimated the uncertainty, while BS and aBS remained close to the true error. For comparability, same bootstrap samples were used for the BS and aBS method (Color figure online)

Full size image
The second type of error, in addition to biases, are errors caused by a high uncertainty in estimated model parameters. The two major challenges in quantifying uncertainty are (i) the reliable estimation of the uncertainty and (ii) the formulation of parameter uncertainties as a model-independent and easily interpretable metric. In the following, we will address both challenges. First, we propose a resampling method to reliably estimate the covariance matrix of model parameters even under non-ideal conditions. Subsequently, we derive the model-independent uncertainty metric EME that predicts the expected error in image space.

Resampling-Based Uncertainty Estimation
In general, the uncertainty of a calibration can be quantified by the covariance matrix of estimated model parameters. As parameters are estimated via nonlinear least squares, the standard estimator for the covariance matrix is a backpropagation of the detector variance (Eq. 9). While this estimator is unbiased in theory and in simulations, we observed that for real data, it tends to underestimate the uncertainty of a calibration. Comparing (a) the variance in estimated model parameters across multiple calibrations with (b) the average predicted variance, the latter tends to be smaller (see Figs. 3a, 4). This effect is especially detectable in the presence of small systematic errors, including imperfections of the calibration target or unmodeled lens distortion. As such small deviations from the ideal assumptions are almost inevitable in practice, a more robust estimator for the covariance matrix is needed to reliably estimate the uncertainty of a calibration. In the following, we will derive a resampling method to estimate uncertainty and we will propose an approximation of the method that is less computationally costly.

Bootstrapping Method
We propose to apply bootstrapping (Davison and Hinkley 1997), a nonparametric statistical technique, to obtain a more robust estimate of the covariance matrix (see Fig. 3b). Instead of relying on specific assumptions on the distribution of the data and the resulting analytical expressions, bootstrapping relies only on the data itself. The main assumption is that the dataset (sample) is representative for the population. Parameter variances and confidence intervals are then obtained by repeated resampling of the data.

Let îˆ²={frameğ‘—}ğ‘îˆ²ğ‘—=1 be the set of ğ‘îˆ² calibration images. We now construct ğ‘›BS bootstrap samples {îˆ²BS,ğ‘™}ğ‘›BSğ‘™=1 by sampling with replacement from the original set of images. Each of the bootstrap samples contains a total of ğ‘îˆ² images (i.e. some images will be contained multiple times and others will be missing). Now, the calibration is conducted with each of these bootstrap samples, yielding an estimate ğœƒÌ‚ ğœƒÌ‚ BS,ğ‘™ for each sample îˆ²BS,ğ‘™. The whole set of bootstrap estimates {ğœƒÌ‚ ğœƒÌ‚ BS,ğ‘™}ğ‘™=1,â€¦,ğ‘›BS forms the so-called bootstrap distribution. Based on this distribution, the covariance matrix can directly be estimated:

Î£Ì‚ Î£Ì‚ ğœƒÌ‚ ğœƒÌ‚ ğœƒÌ‚ ğœƒÌ‚ ,BS=Cov(ğœƒÌ‚ ğœƒÌ‚ BS,ğœƒÌ‚ ğœƒÌ‚ BS)=â›ââœâœâœâœâœâœâœâœâœğœ21ğœ21â‹®ğœğ‘ğœƒğœƒ1ğœ12ğœ22â‹®ğœğ‘ğœƒğœƒ2â‹¯â‹¯â‹±â‹¯ğœ1ğ‘ğœƒğœƒğœ2ğ‘ğœƒğœƒâ‹®ğœ2ğ‘ğœƒğœƒââ âŸâŸâŸâŸâŸâŸâŸâŸâŸ.
(14)
with

ğœÌ‚ 2ğ‘š:=1ğ‘›BSâˆ’1âˆ‘ğ‘™=1ğ‘›BS(ğœƒğ‘™ğ‘šâˆ’ğœƒâ¯â¯â¯ğ‘š)2
(15)
ğœÌ‚ ğ‘›ğ‘š:=1ğ‘›BSâˆ’1âˆ‘ğ‘™=1ğ‘›BS(ğœƒğ‘™ğ‘›âˆ’ğœƒâ¯â¯â¯ğ‘›)(ğœƒğ‘™ğ‘šâˆ’ğœƒâ¯â¯â¯ğ‘š).
(16)
Approximated Bootstrapping Method
Our results indicate that the bootstrap method provides a reliable estimate of the covariance matrix (see Sect. 7.2). However, a major drawback lies in the computation time, as the entire calibration has to be conducted ğ‘›BS times.

To reduce the computation time, we propose an approximated bootstrapping method (aBS) which re-uses the already computed Jacobian of calibration residuals. Instead of conducting the entire calibration with each of the bootstrap samples, the aBS method only conducts the last iteration of the nonlinear optimization, as we will explain in detail below.

The bundle adjustment performed during camera calibration relies on non-linear least squares estimation. Optimal parameters are found iteratively by performing so-called Gaussâ€“Newton steps:

(ğ½ğ‘‡ğ½ğ½ğ‘‡ğ½)Î”ğœƒÎ”ğœƒ=ğ½ğ‘‡ğ‘Ÿğ½ğ‘‡ğ‘Ÿ
(17)
ğœƒğœƒğ‘˜+1=ğœƒğœƒğ‘˜+Î”ğœƒÎ”ğœƒ
(18)
where ğ½ğ½ is the local Jacobian at ğœƒğœƒğ‘˜ and ğ‘Ÿğ‘Ÿ are the residuals at the current iteration k. Note, that for calibration, an augmented system is typically used (e.g. Levenberg-Marquardt), but for the aBS method we omit the augmentation and use the plain Gauss-Newton formulation. At the end of the calibration, the optimal parameters ğœƒÌ‚ ğœƒÌ‚ , the local Jacobian ğ½ğ½ and the residuals ğ‘Ÿğ‘Ÿ are known, where the k-th row of the Jacobian corresponds to the local derivatives k-th residual.

This is where the approximated bootstrapping method is applied: As for the original bootstrap method, we construct ğ‘›BS bootstrap samples {îˆ²BS,ğ‘™}ğ‘™=1,â€¦,ğ‘›BS by sampling with replacement from the original set of images. For each of these bootstrap samples îˆ²BS,ğ‘™, the Jacobian and the residual vector are re-composed, such that they contain only the observations of îˆ²BS,ğ‘™. Thus, for each bootstrap sample, we get a re-composed pair (ğ½ğ½BS,ğ‘™,ğ‘Ÿğ‘ŸBS,ğ‘™). For instance, if the bootstrap sample îˆ²BS,ğ‘™ contained the first image of the original dataset twice, the pair would be given by

ğ‰ğµğ‘†,ğ‘™=â›ââœâœâœâœâœâœğ‰1ğ‰1ğ‰3â‹®ğ‰ğ‘îˆ²ââ âŸâŸâŸâŸâŸâŸ,ğ«ğµğ‘†,ğ‘™=â›ââœâœâœâœâœâœğ«1ğ«1ğ«3â‹®ğ«ğ‘îˆ²ââ âŸâŸâŸâŸâŸâŸ,
(19)
where the entry ğ‰ğ‘— describes the rows of the Jacobian that correspond to image j and ğ«ğ‘— is the residual vector corresponding to image j.

To obtain the bootstrap estimates ğœƒÌ‚ ğœƒÌ‚ BS,ğ‘™, only the last Gauss-Newton step is conducted:

(ğ½ğ½ğ‘‡BS,ğ‘™ğ½ğ½BS,ğ‘™)Î”ğœƒÎ”ğœƒBS,ğ‘™=ğ½ğ½ğ‘‡BS,ğ‘™ğ‘Ÿğ‘ŸBS,ğ‘™
(20)
ğœƒÌ‚ ğœƒÌ‚ aBS,ğ‘™=ğœƒÌ‚ ğœƒÌ‚ +Î”ğœƒÎ”ğœƒBS,ğ‘™.
(21)
Having performed this last step for each bootstrap sample, we again get a distribution of bootstrap estimates {ğœƒÌ‚ ğœƒÌ‚ aBS,ğ‘™}ğ‘™=1,â€¦,ğ‘›BS and the covariance matrix Î£Ì‚ Î£Ì‚ ğœƒÌ‚ ğœƒÌ‚ ğœƒÌ‚ ğœƒÌ‚ ,aBS can be estimated as in 5.2.

Thus, in short, the aBS simplifies the BS method by re-using the Jacobian and residual vector of the original calibration, which are typically costly to compute.

The Expected Mapping Error
Fig. 5
figure 5
Predicting the mapping error based on parameter uncertainties. a Schematic of the derived uncertainty metric EME=trace(Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 1/2ğ»ğ»Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 1/2). We define the mapping error K as the difference between two camera models in image space. Approximating K with a quadratic form in Î”ğœƒÎ”ğœƒ, we can predict its expected value by propagation of uncertainties. b Validation of the EME in simulations. The EME predicts the average true mapping error, i.e. the difference between the calibration result and the true camera model. Error bars are 95% bootstrap confidence intervals (Color figure online)

Full size image
Given a reliable estimate of the parameter covariance matrix, the second challenge lies in the formulation of the uncertainty in a model-independent and easily interpretable manner. We will now derive the expected mapping error (EME), a novel uncertainty metric which quantifies the expected difference between the mapping of a calibration result ğ‘ğ¶ğ‘ğ¶(ğ‘¥ğ‘¥,ğœƒÌ‚ ğœƒÌ‚ ) and the true (unknown) model ğ‘ğ¶ğ‘ğ¶(ğ‘¥ğ‘¥,ğœƒÂ¯ğœƒÂ¯).

Inspired by previous works (Cramariuc et al. 2020; Richardson et al. 2013), we quantify the mapping difference in image space, as pixel differences are easily interpretable. We define a set of points in image space îˆ³={ğ‘¢ğ‘¢ğ‘”}ğ‘îˆ³ğ‘”=1, from which the corresponding sight rays are computed via the inverse projection ğ‘ğ¶ğ‘ğ¶âˆ’1(ğ‘¢ğ‘¢ğ‘”,ğœƒÂ¯ğœƒÂ¯) using one set of model parameters ğœƒÂ¯ğœƒÂ¯. Then, points on the viewing rays are backprojected to the image using the other set of model parameters ğœƒÌ‚ ğœƒÌ‚  (Beck and Stiller 2018). The mapping error is then defined as the average squared distance between original image coordinates ğ‘¢ğ‘¢ğ‘” and back-projected image points ğ‘ğ¶ğ‘ğ¶(ğ‘¥ğ‘¥ğ‘”,ğœƒÌ‚ ğœƒÌ‚ ) (see Fig. 5):

ğ¾Ìƒ (ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯)=12ğ‘îˆ³âˆ‘ğ‘”âˆˆîˆ³||ğ‘¢ğ‘”ğ‘¢ğ‘”âˆ’ğ‘ğ¶ğ‘ğ¶(ğ‘ğ¶ğ‘ğ¶âˆ’1(ğ‘¢ğ‘”ğ‘¢ğ‘”,ğœƒÂ¯ğœƒÂ¯),ğœƒÌ‚ ğœƒÌ‚ )||2,
(22)
where 2ğ‘îˆ³ is the total number of image coordinates.

Formulation (22) of the mapping error assumes that an error in the intrinsics fully propagates to the image. However, the overall projection of a camera generally also includes the coordinate transformation from the world coordinate system to the camera coordinate system ğ‘¥ğ‘¥â†’ğ‘¥ğ‘ğ‘¥ğ‘. As the camera pose and therefore the extrinsics are oftentimes re-estimated in practical settings, we additionally take into account that deviations in intrinsic parameters can partially be compensated by a change in extrinsic parameters (Strauss 2015) and allow for a virtual compensating rotation ğ‘…ğ‘… of the viewing rays. Thus, we formulate the effective mapping error as follows:

ğ¾(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯)=minğ‘…ğ‘… 12ğ‘îˆ³âˆ‘ğ‘”âˆˆîˆ³||ğ‘¢ğ‘¢ğ‘”âˆ’ğ‘ğ¶ğ‘ğ¶(ğ‘…ğ‘… ğ‘ğ¶ğ‘ğ¶âˆ’1(ğ‘¢ğ‘¢ğ‘”,ğœƒÂ¯ğœƒÂ¯),ğœƒÌ‚ ğœƒÌ‚ )||2.
(23)
By compensating rotation only, we make the implicit assumption of infinitely far away points, i.e. we emulate a grid at infinity. This is reasonable, as only directions of viewing rays are part of the calibration (see also FÃ¶rstner and Wrobel 2016, p. 472), and it avoids choosing different depths of grid points explicitly.

The uncertainty metric EME is defined by the expected value of the mapping error, which can be reduced to a simple mathematical expression, as we will show below:

EME=ğ”¼[ğ¾(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯)]=trace(Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12ğ»ğ»Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12),
(24)
where Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚  is the covariance matrix in model parameters and ğ»ğ» is the so-called model matrix obtained from an approximation of the effective mapping error.

In the following, we will derive expression (24). Note, that the derivation is independent of the particular choice of K, provided that we can approximate K with a Taylor expansion around ğœƒÌ‚ ğœƒÌ‚ =ğœƒÂ¯ğœƒÂ¯ up to second order:

ğ¾(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯)â‰ˆğ¾(ğœƒÂ¯ğœƒÂ¯,ğœƒÂ¯ğœƒÂ¯)+grad(ğ¾)Î”ğœƒÎ”ğœƒ+12Î”ğœƒÎ”ğœƒğ‘‡ğ»ğ»ğ¾Î”ğœƒÎ”ğœƒâ‰ˆ12ğ‘îˆ³Î”ğœƒÎ”ğœƒğ‘‡(ğ½resğ½resğ‘‡ğ½resğ½res)Î”ğœƒÎ”ğœƒâ‰ˆÎ”ğœƒÎ”ğœƒğ‘‡ğ»ğ»Î”ğœƒÎ”ğœƒ,
(25)
where Î”ğœƒÎ”ğœƒ=ğœƒÂ¯ğœƒÂ¯âˆ’ğœƒÌ‚ ğœƒÌ‚  is the difference between true and estimated intrinsic parameters, resresğ‘”(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯) are the mapping residuals, i.e.

resresğ‘”(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯)=ğ‘¢ğ‘¢ğ‘”âˆ’ğ‘ğ¶ğ‘ğ¶(ğ‘…ğ‘… ğ‘ğ¶ğ‘ğ¶âˆ’1(ğ‘¢ğ‘¢ğ‘”,ğœƒÂ¯ğœƒÂ¯),ğœƒÌ‚ ğœƒÌ‚ ),
and ğ½resğ½res=ğ‘‘resres/ğ‘‘Î”ğœƒÎ”ğœƒ is the Jacobian of the residuals. Furthermore, we defined the model matrix ğ»ğ»:=12ğ‘îˆ³ğ½resğ½resğ‘‡ğ½resğ½res. For a more detailed derivation of the second step in 25, see Supplementary.

Estimated model parameters ğœƒÌ‚ ğœƒÌ‚  obtained from a least squares optimization are a random vector, asymptotically following a multivariate Gaussian with mean ğœ‡ğœƒğœ‡ğœƒ=ğœƒÂ¯ğœƒÂ¯ and covariance Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚  (Triggs et al. 2000, p. 8). Likewise, the parameter error Î”ğœƒÎ”ğœƒ=ğœƒÂ¯ğœƒÂ¯âˆ’ğœƒÌ‚ ğœƒÌ‚  follows a multivariate Gaussian, with mean ğœ‡Î”ğœƒğœ‡Î”ğœƒ=00 and covariance Î£Î”ğœƒÎ”ğœƒÎ£Î”ğœƒÎ”ğœƒ=Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ . We propagate the distribution of the parameter error Î”ğœƒÎ”ğœƒ to find the distribution of the mapping error ğ¾(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯). In short, we find that the mapping error ğ¾(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯) can be expressed as a linear combination of ğœ’2 random variables:

ğ¾(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯)=Î”ğœƒÎ”ğœƒğ‘‡ğ»ğ»Î”ğœƒÎ”ğœƒ=âˆ‘ğ‘›=1ğ‘ğœƒğœƒğœ†ğ‘›ğ‘„ğ‘›,withğ‘„ğ‘›âˆ¼ğœ’2(1).
(26)
The coefficients ğœ†ğ‘› are the eigenvalues of the matrix product Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12ğ»ğ»Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12 and ğ‘ğœƒğœƒ is the number of eigenvalues which equals the number of parameters ğœƒğœƒ. The full derivation of relation (26) is shown in the Supplementary. Importantly, based on expression (26), we can derive the expected value of ğ¾(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯):

ğ”¼[ğ¾(ğœƒÌ‚ ğœƒÌ‚ ,ğœƒÂ¯ğœƒÂ¯)]=ğ”¼[âˆ‘ğ‘›=1ğ‘ğœƒğœƒğœ†ğ‘›ğ‘„ğ‘›]=âˆ‘ğ‘›=1ğ‘ğœƒğœƒğœ†ğ‘›ğ”¼[ğ‘„ğ‘›]=âˆ‘ğ‘›=1ğ‘ğœƒğœƒğœ†ğ‘›=trace(Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12ğ»ğ»Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12),
(27)
where we used that the ğœ’2-distribution with one degree of freedom ğœ’2(1) has expectation value ğ”¼[ğœ’2(1)]=1. We therefore propose the expected mapping error EME=trace(Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12ğ»ğ»Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12) as a model-independent measure for the uncertainty.

figure b
Note, that although the EME is measured in the same units as the mean squared error MSEcalib and the estimated accuracy ğ‘ Ì‚ 2ğ‘‘, the three quantities conceptually differ. The value of MSEcalib describes the deviation of observations from model predictions in the calibration dataset. It can be interpreted as a training error and could be reduced by simply including less observations or more (unnecessary) model parameters in the calibration. The estimated accuracy ğ‘ Ì‚ 2ğ‘‘ corrects for the dependence on the number of parameters and observations by rescaling MSEcalib accordingly (see (2)). If there is no bias in the calibration, ğ‘ Ì‚ 2ğ‘‘ is an unbiased estimator for the observational noise in a calibration dataset and comparable across camera models. However, neither MSEcalib, nor ğ‘ Ì‚ 2ğ‘‘ provides direct information on whether model parameters could be estimated reliably based on the given calibration dataset.

The EME addresses this question by quantifying the uncertainty (covariance) estimated model parameters. Unlike MSEcalib and ğ‘ Ì‚ 2ğ‘‘, the EME will increase for calibrations with less observations or less informative (degenerate) configurations, and thereby reveal the higher uncertainty in the calibration result. As the EME is computed on a regular grid of image points, it includes the prediction into image areas not covered by observations. Thereby, it is not only comparable across camera models, but also across calibration datasets.

Experimental Evaluation
Simulations
We simulated 3D world coordinates of a single planar calibration target in different poses relative to the camera (random rotations ğœ‘ğ‘¥,ğœ‘ğ‘¦,ğœ‘ğ‘§âˆˆ[âˆ’ğœ‹4,ğœ‹4], translations ğ‘¡ğ‘§âˆˆ[0.5 m,2.5 m], ğ‘¡ğ‘¥,ğ‘¡ğ‘¦âˆˆ[âˆ’0.5 m,0.5 m]). We then computed the resulting image coordinates using different camera models. To simulate the detector noise, we added Gaussian noise with ğœğ‘‘=0.05 px to all image coordinates. For example images, see Supplementary Fig. S1.

Rendered Images
By simulating marker detection, we neglect the characteristics of the real corner, but assume i.i.d. Gaussian noise in the detection. To get a better understanding of potential deviations from this assumption, and their effect on our metrics, we additionally used rendered images. During rendering, we used a ray casting technique and simulated the cameraâ€™s point spread function using multiple weighted samples per pixel. For rendered images, both the camera model and the calibration target are known and do not introduce any bias. Thereby, the full calibration pipeline, including corner detection, can be evaluated. For example images, see Supplementary Fig. S1.

Evaluation with Real Images
We tested the methods using real images from three different lenses (see Fig. 6a for example images and Fig. S1 for more details on the lenses and the dataset). For each lens, we collected a dataset with images of a single planar calibration target. We use a coded target proposed by StrauÃŸ et al. (2014), to be able to associate corners across images. As no ground-truth model was available for the real lenses, we used a reference calibrationFootnote4 as an approximate ground-truth to verify the evaluation methods.

Fig. 6
figure 6
Experimental evaluation of the bias ratio (BR). a Exemplary rendered image and images of the same scene taken with the three different lenses used for evaluation. b Robust estimate of the RMSE (MAD, blue) and bias ratio (red) of calibrations with models of increasing complexities. For the rendered camera images, the BR drops once a model with the capability to describe the simulated mapping is employed. For all real lenses, the BR remains unexpectedly high even when using the fisheye model C(8). c Taking into account that the calibration target was not perfectly planar (Î”ğ‘§âˆ¼10âˆ’4 m) further reduced the BR significantly. Error bars are standard deviations of ten calibrations with 50 random images each (Color figure online)

Full size image
Results
Evaluating the Bias Ratio
Validating the BR in Simulations and Experiments
Fig. 7
figure 7
Comparison of different bias metrics. Each cell shows the distribution of residuals, the RMSE, the median Kullback-Leibler divergence (KLD), the bias ratio (BR) and the absolute bias (ğœ–bias) of the respective calibration. The results are based on simulated data, with varying degrees of observational noise and bias influences. Note the different limits of x- and y-axes. All metrics are of statistical nature, so that the precise values can vary for different calibrations. The same plots for real cameras are shown in the Supplementary (Color figure online)

Full size image
To test the bias metric, we ran calibrations using camera models of increasing complexity (increasing number of non-zero intrinsic parameters), including insufficiently complex models. We then computed the robust estimate of the RMSE (median absolute deviation, MAD) and the bias ratio (BR) for each calibration. For calibrations with an insufficiently complex model, the bias ratio should detect systematic errors.

Figure 6 shows the results of calibrations of four cameras (one rendered dataset and three real cameras shown in Fig. 6a). Each camera was calibrated with five different camera models of increasing complexity. In detail, the individual parameter sets are (cf. Sec. 3.1)

ğœƒğœƒğ¶(3)ğœƒğœƒğ¶(5)ğœƒğœƒğ¶(6)ğœƒğœƒğ¶(7)ğœƒğœƒğ¶(8)=(ğ‘“,ğ‘ğ‘¥,ğ‘ğ‘¦),=(ğ‘“ğ‘¥,ğ‘“ğ‘¦,ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘˜1),=(ğ‘“ğ‘¥,ğ‘“ğ‘¦,ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘˜1,ğ‘˜2),=(ğ‘“ğ‘¥,ğ‘“ğ‘¦,ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘˜1,ğ‘˜2,ğ‘˜3),=(ğ‘“ğ‘¥,ğ‘“ğ‘¦,ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘˜1,ğ‘˜2,ğ‘˜3,ğ‘˜4).
For all cameras, the MAD and BR could be reduced by using more complex camera models, as the projections were not rectilinear and thus necessitate some kind of (nonlinear) distortion modeling.

For the rendered images, the bias ratio was close to one for the first camera model C(3), indicating that the residuals were dominated by systematic errors. For the second camera model C(5), it reduced to BRâ‰ˆ0.6, indicating that this model was better, but not yet sufficient. When using the camera model with two radial distortion parameters C(6), which is consistent with the model that was used for rendering, the BR dropped to BR<0.2. Adding additional parameters (C(7), C(8)) did not lead to a further reduction. The bias ratio thus correctly detected the presence of systematic errors for models C(3) and C(5) and indicated that one of the more complex models should be chosen for the calibration.

Surprisingly, the BR of all three real lenses remained comparatively high across all tested camera models, demonstrating that some sort of systematic error remained (Fig. 6b). Further analyses revealed that the calibration board was not perfectly planar: running the calibrations again after precisely measuring the board geometry (StrauÃŸ et al. 2014) lead to a significant further reduction in systematic error (Fig. 6c). The non-planarity of the board was on the order of 10âˆ’4 m which demonstrates that even smallest imperfections in the calibration setup can be detected by the BR.

Using only the RMSE of the calibrations, some of the systematic errors could have been overlooked. For the rendered images, for instance, the RMSE already dropped to RMSE<0.1 pixels when using camera model C(5), which may have been considered sufficiently low. Likewise, even without taking into account the board-nonplanarity, the RMSE of all three real lenses was below 0.1 pixels and the calibrations may have been considered sufficiently accurate. This highlights the advantage of using the bias ratio to assess a calibration result.

For the calibrations shown in Fig. 6, the BR got close to zero, but did not reach a value of zero for any of the rendered and real cameras. This could be explained by the fact that the observational noise of the true corner detector was not perfectly i.i.d., zero-mean Gaussian distributed (Edwards et al. 2020). As the images used here were of high quality and generally showed a low observational noise (ğœğ‘‘âˆ¼0.03 px), such small deviations from the expected Gaussian can become visible in the BR. For the real lenses, it is furthermore conceivable that some small systematic error remained, despite already taking into account the deformation of the calibration target.

Comparison of Different Bias Metrics
We compare the bias ratio with other state-of-the-art bias metrics introduced in Sect. 2. We focus on the RMSE, the visualization of residuals in a smoothed 2D histogram (Beck and Stiller 2018) and the Kullback-Leibler divergence between a 2D normal distribution and the empirical distribution of residual vectors (Schoeps et al. 2019). While (Schoeps et al. 2019) uses 50x50 grid across the image and computes the KL-divergence in each cell, we use a 4x4 grid, as we are working with chessboard targets and significantly less observations. We calibrated simulated cameras with high and low observational noise, using the correct camera model (C(6), unbiased), using a model with only one radial distortion parameter (C(5), weakly biased) and using a plain pinhole model (C(3), strongly biased) (see Fig. 7).

The RMSE increased with increasing amount of bias. However, as the observational noise it not known a priori, it is hard to tell whether a value of RMSEâ‰ˆ0.4 px reflects a correspondingly high observational noise ğœğ‘‘â‰ˆ0.4 px or whether the observational noise is lower, and the RMSE reflects a bias (Fig. 7, upper middle cell).

The 2D histogram of residuals was already more informative: visible deviations from an expected 2D Gaussian indicated a bias (Fig. 7, right column, Fig. S2). However, as the simulations show, even in the presence of biases, such deviations are not always visible (Fig. 7, middle column).

Instead of relying on visual inspection, the Kullback-Leibler divergence measures deviations from the expected Gaussian distribution: Higher values indicate higher deviations and therefore systematic errors. The KLD clearly indicated the increasing bias in the presence of low observational noise (Fig. 7, upper row). However, in the presence of high observational noise, this noise partially overshadowed systematic errors (Fig. 7, lower middle cell). In this high noise / weak bias case, the KLD remained close to zero despite the bias (see also Fig. S2 for calibrations with real lenses).

A similar effect was visible for the bias ratio: in the presence of high observational noise, the BR was lower, as it describes the fraction of systematic error. To get a full picture on the amount of systematic error, we therefore recommend to take into account both, the BR and the absolute amount of bias ğœ–bias. The absolute amount of bias ğœ–bias increased consistent with the increasing amount of bias, regardless of the amount of observational noise. In combination, the BR and ğœ–bias were able to capture the systematic errors in all scenarios (see also Fig. S2 for calibrations with real lenses).

Note, that all metrics presented here are of statistical nature, and the precise values will fluctuate depending on the dataset and the optimization, even if the same model is chosen. Note also, that the KLD metric and the 2D histogram were originally designed for calibrations with highly complex camera models and significantly more observations (Schoeps et al. 2019; Beck and Stiller 2018). It is therefore conceivable that these metrics are more powerful in such settings.

Evaluating the Resampling-Based Uncertainty Estimation
To test the resampling-based uncertainty estimation, we compared the estimated (co-)variances to the average deviation of the calibration result from the ground-truth parameters. If variances were estimated correctly, the variance of a parameter should, on average, reflect the parameterâ€™s mean squared deviation from the true parameter.

We compared the covariance estimates of the standard method (std), the novel bootstrapping method (BS) and the approximated bootstrapping method (aBS) (Fig. 4). Using both, simulated and real images, we ran calibrations with 50 random datasets containing ğ‘îˆ²=25 images each. For better comparability, we visualize the EME, i.e. the propagation of the covariance matrices to image space.

For an ideal simulation with no systematic errors and i.i.d. Gaussian distributed observational noise, all three methods provided similar estimates (Fig. 4, left). For all methods, the average EME was close to the true average mapping errorFootnote5.

When simulating an underfit, i.e. when calibrating a camera with only one radial distortion parameter although the ground-truth camera contained two non-zero radial distortion parameters, the methods differed. The standard method significantly underestimated the uncertainty and thereby the EME. This means that the standard estimator lead to overly optimistic assumptions on the uncertainty. The BS method and the aBS method, on the other hand, remained close to the true average error.

Importantly, for the two real cameras, the results were similar to the underfit: the standard method underestimated the variance, leading to overly optimistic assumptions on the uncertainty. The BS method and the aBS method, on the other hand, remained close to the true errorFootnote6. Our results thus indicate that in practice, where small systematic errors are almost inevitableFootnote7, the resampling-based methods provide more reliable estimates for the uncertainty than the standard method.

Evaluating the Uncertainty Metric
Validating the Prediction of the EME
To validate the uncertainty metric EME in simulations, we simulated datasets with different numbers of images (ğ‘îˆ²âˆˆ{10,15,â€¦,50}) and ran calibrations using a sufficiently complex camera model. Figure 5b shows the uncertainty metric EME=trace(Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12ğ»ğ»Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ 12) and the average true mapping error compared to the ground-truth model. Consistent with Eq. (24), the EME predicted the average mapping error, i.e. the EME and the average mapping error were approximately equal.

Figure. 8a,b show the same plots for two real cameras. Here, we used a reference calibration as ground-truth and ran calibrations with different numbers of images (ğ‘îˆ²âˆˆ{10,15,â€¦,50}). In Fig. 8a, we used the standard covariance estimator in the computation of the EME. The EME is highly correlated with the average error, but its absolute values are significantly lower. The EME must therefore be interpreted as an upper bound for the precision that can be achieved based on a given dataset.

In Fig. 8b, we used the aBS method in the computation of the EME. Using the aBS method, the EME was consistent with the average real mapping error across most calibrations. Only for calibrations with few images (ğ‘îˆ²â‰¤15), the EME tended to overestimate the true error. This is caused by the fact that the theory of bootstrapping is based on asymptotic assumptions (infinite sample sizes). Although it typically works well for smaller sample sizes as well, predictions become less reliable. In the case of camera calibration, this results in overly pessimistic uncertainty estimates. Note, however, that for most applications an overly pessimistic error estimate is to be preferred over an overly optimistic estimate of the standard covariance estimator.

In summary, when using the standard covariance estimator, the EME provides an upper bound to precision that can be achieved based on a given dataset. When using the aBS method to estimate the covariance, the EME can be interpreted as an actual prediction of the mapping error given a sufficient number of calibration images.

Comparison of Different Uncertainty Metrics
Fig. 8
figure 8
Experimental evaluation of the expected mapping error (EME). a, b Validation of the EME with two real lenses (cinegon and manta). Using the standard covariance estimator (std), the EME and true error were highly correlated, but the absolute values of the EME were lower. Using the aBS method, the EME was consistent with the true average true error across most calibrations. c Comparison of state-of-the-art uncertainty metrics in simulated calibrations. On average, the true error K decreases with the number frames. For comparability with maxERE, we show ğ¾â€¾â€¾âˆš and ğ¸ğ‘€ğ¸â€¾â€¾â€¾â€¾â€¾â€¾âˆš in units of pixels. All metrics are correlated with the true error, but absolute values, scales, and units differ. Values are means across 50 random samples, error bars are 95% bootstrap confidence intervals (Color figure online)

Full size image
We compare the EME with the other state-of-the-art uncertainty metrics introduced in Sect. 2. We focus on trace(Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ ), maxERE (Richardson et al. 2013) and observability (Strauss 2015), as these are the metrics closest to ours. Figure 8c shows the result of all metrics for simulated calibrations with different numbers of images. To demonstrate how the uncertainty depends on both, the amount of observational noise ğœğ‘‘ and the informativeness of calibration images, we simulated three different settings (low noise / informative images, low noise / less informative images, high noise / informative images).

Figure 8c shows the true mapping error compared to the ground truth camera model ğ¾â€¾â€¾âˆš, as well as all uncertainty metrics. The uncertainty, and therefore the average true error, decreased with an increasing number of images (an increasing number of observations N). As expected from theory, the scaling is approximately given by Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ âˆğ‘âˆ’1 (see also Fig. S4). Furthermore, the uncertainty was lowest for calibrations with low noise and informative images, and significantly higher for high noise or uninformative images.

All metrics were correlated with the true error. However, the metrics quantify uncertainty in very different ways: trace(Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ ) quantifies the uncertainty in model parameters, and thus inherently differs depending on the camera model. Furthermore, as its value represents the sum of different parameters variances, it does not have a unit. By re-parametrizing the camera model, the value of trace(Î£ğœƒÌ‚ ğœƒÌ‚ Î£ğœƒÌ‚ ğœƒÌ‚ ) can change over orders of magnitude, making the comparison across different calibrations difficult.

The observability metric shows a qualitatively different scaling than the other metrics as it increases linearly with the number of images. High observabilities imply that parameters were well observable based on the data. As the EME, the observability accounts for the parameterâ€™s effect on the mapping and for compensations of errors in the intrinsics via different extrinsics. However, it does not incorporate the full uncertainty, but only the least observable direction. Furthermore, it does not account for the observational noise, but rather evaluates whether the setup was informative. It therefore serves to evaluate a calibration setup rather than the uncertainty of the specific calibration result.

Both, maxERE and the EME quantify the expected error in image space and are thus easily interpretable. While maxERE predicts a maximum error, the EME reflects the average error. In contrast to maxERE, the EME does not require a Monte Carlo simulation. Furthermore, the EME can account for a compensation via different extrinsics, which we consider a reasonable assumption in most practical scenarios.

Discussion and Conclusion
In this work, we derived an evaluation scheme for camera calibration, including the detection of systematic errors and the quantification of uncertainty.

We have shown that it is possible to reliably capture systematic errors by disentangling the systematic errors from the random errors in the calibration residuals. The proposed method can thereby uncover wrong model assumptions and smallest imperfections in the calibration setup. Compared to other state-of-the-art methods to detect biases, the proposed method was more reliable and more easily interpretable, as it does not involve visual inspections of error distributions, nor does it require experience or comparative values from past calibrations.

To quantify uncertainty, we proposed a resampling-based estimation of the covariance matrix. In contrast to the standard covariance estimator, the resampling method does not impose strict assumptions on the error distribution and can therefore be applied under practical, non-ideal conditions. While the standard estimator led to an underestimation of the uncertainty in non-ideal settings, the proposed method provided reliable estimates even in the presence of systematic errors. One disadvantage of the resampling method is a required minimum size of the calibration dataset, otherwise the estimation becomes overly pessimistic. This is because the theory behind bootstrapping is based on asymptotic assumptions (infinite sample sizes), which are not met with too few calibration images. In most applications, however, such overly pessimistic error estimates are to be preferred over the overly optimistic estimates of the standard method.

To reduce the computation time of the resampling method, we additionally derived an approximation of the method that completely avoids recomputation of Jacobians and residuals and is therefore significantly faster. Thereby, the resampling method becomes feasible in practice.

Finally, we have derived a model-independent and easily interpretable uncertainty metric called expected mapping error (EME). The EME quantifies the expected difference between the calibration result and the true camera in image space. By propagating the parameter uncertainty to image space, the metric is independent of the underlying camera model and comparable across different calibrations. The EME allows to quantify how informative a certain dataset is for the calibration and can thereby help to improve calibration setups.

Importantly, our derivation of the EME can also be used to predict other, application specific errors. As a generic choice, we have used the average error across the image, but one could also weight image regions differently, or define an application-specific error, such as a triangulation error of a stereo system. Furthermore, the EME can be applied for calibration guidance, in order to guide users to collect calibration datasets that explicitly minimize the uncertainty (Hagemann et al. 2020).

In summary, our results suggest that target-based camera calibration can be evaluated reliably by (i) computing the bias ratio to detect systematic errors, (ii) using the resampling method to estimate the uncertainty, and (iii) computing the EME to quantify the uncertainty in a model-independent manner. Together, the proposed methods can be used to assess the accuracy of individual calibrations, but also to benchmark new calibration algorithms, camera models, or calibration setups.