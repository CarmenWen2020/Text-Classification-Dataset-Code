Foreground map evaluation is crucial for gauging the progress of object segmentation algorithms, in particular in the field of salient object detection where the purpose is to accurately detect and segment the most salient object in a scene. Several measures (e.g., area-under-the-curve, F1-measure, average precision, etc.) have been used to evaluate the similarity between a foreground map and a ground-truth map. The existing measures are based on pixel-wise errors and often ignore the structural similarities. Behavioral vision studies, however, have shown that the human visual system is highly sensitive to structures in scenes. Here, we propose a novel, efficient (0.005 s per image), and easy to calculate measure known as S-measure (structural measure) to evaluate foreground maps. Our new measure simultaneously evaluates region-aware and object-aware structural similarity between a foreground map and a ground-truth map. We demonstrate superiority of our measure over existing ones using 4 meta-measures on 5 widely-used benchmark datasets. Furthermore, we conduct a behavioral judgment study over a new database. Data from 45 subjects shows that on average they preferred the saliency maps chosen by our measure over the saliency maps chosen by the state-of-the-art measures. Our experimental results offer new insights into foreground map evaluation where current measures fail to truly examine the strengths and weaknesses of models. Code: https://github.com/DengPingFan/S-measure.

Access provided by University of Auckland Library

Introduction
The evaluation of a predicted foreground map (FM) against a ground-truth (GT) annotation map is crucial in evaluating and comparing various computer vision algorithm for applications such as AR COPY & PASTE (Qin et al., 2021), object detection and recognition (Borji et al., 2013a, 2015; Kanan & Cottrell, 2010; Rutishauser et al., 2004; Islam et al., 2018), video summarization (Ghosh et al., 2012), video compression (Guo & Zhang, 2010; Itti, 2004), image segmentation (Yu et al., 2018), content-based image retrieval (Li et al., 2013a; Liu & Fan, 2013; Liu et al., 2015), visual tracking, photo synthesis (Chen et al., 2009), image-text matching (Zhuge et al., 2021b), image collection browsing (Chen et al., 2009), etc. As a specific example, here we focus on salient object detection (Borji et al., 2015; Borji & Itti, 2013; Borji, 2015; Bylinskii et al., 2015; Zhang et al., 2017, 2018a, b, c; Wang et al., 2018; Chen & Li, 2018; Gorji & Clark, 2018; Li et al., 2018; Zeng et al., 2018; Liu et al., 2018; Tiantian et al., 2018), although the proposed measure is general and can be used for other purposes. It is worth noting that the salient object does not necessarily need to be the foreground object (Feng et al., 2016; Borji et al., 2013b).Footnote1

The GT map is often the average of several manual annotations. Thus, it can be binary or non-binary. Similarly, the predicted foreground maps are either binary or non-binary. As a result, evaluation measures can be classified into two types:

1.
Binary map evaluation: Common measures here include Fğ›½-measure (Arbelaez et al., 2011; Cheng et al., 2015; Liu et al., 2011) and PASCALâ€™s VOC segmentation measure (Everingham et al., 2010).

2.
Non-binary map evaluation: Three traditional and popular measures here include area under the curve (AUC), precision-recall (PR) curve, and average precision (AP) (Everingham et al., 2010). A newly released measure known as weighted Fğ›½-measure (Fbw) (Margolin et al., 2014) has been proposed to remedy flaws of AUC, PR and AP.

It is often desired to have the foreground map contain the entire structure of an object. Thus, evaluation measures are expected to tell which model generates a more complete object. For example, in Fig. 1 (first row) the blue-border map better captures the dog than the red-border map. In the latter case, shape of the the dog is drastically degraded to a degree that it is difficult to guess the object category from its segmentation map. Surprisingly, all of the current evaluation measures fail to correctly rank these maps (in terms of preserving the structure).

We employed 10 state-of-the-art (SOTA) saliency detection algorithms to obtain 10 saliency maps (Fig. 2; 1st row) and then fed these maps to the SalCutFootnote2 (Cheng et al., 2015) algorithm to generate corresponding binary maps (2nd row). Finally, we used the proposed S-measure to rank these maps. A lower score for our measure corresponds to more destruction in the global structure of the man (columns e to j). This experiment clearly shows that our new measure emphasizes the entire structure of the object. In these ten binary maps (2nd row), there are six maps with score below 0.95, i.e. with percentage 60%. Using the same threshold (0.95), we found that the proportions of destroyed images in four popular saliency datasets [i.e., ECSSD (Xie et al., 2013), HKU-IS (Li & Yu, 2015), PASCAL-S (Li et al., 2014), and SOD (Martin et al., 2001)] are 66.80%, 67.30%, 81.82% and 83.03%, respectively. Using the ğ¹ğ›½-measure to evaluate the binary maps, these proportions are 63.76%, 65.43%, 78.32% and 82.67%, respectively. This means that our measure is more stringent than the ğ¹ğ›½-measure on object structure.

To remedy the problem of existing measures (i.e., low sensitivity to entire object structure), we present a structure-sensitive similarity measure based on two observations:

1.
Region perspectives: Although it is difficult to describe the structure of a foreground map, we notice that the entire structure of an object can be well illustrated by combining structures of constituent object parts (regions).

2.
Object perspectives: In high-quality foreground maps, the foreground regions contrast sharply with the background regions. These regions usually have approximately uniform contrast distributions.

Fig. 1
figure 1
Inaccuracy of existing evaluation measures. We compare the ranking of saliency maps generated by three state-of-the-art (SOTA) salient object detection algorithms: DISC (Chen et al., 2016), MDF (Li & Yu, 2015), and MC (Zhao et al., 2015). According to the segmentation applicationâ€™s ranking (last row; see details in Apps-Sec. 5), the blue-border map ranks first, followed by the yellow- and red-border maps. The blue-border map captures the dogâ€™s structure most accurately, with respect to the GT. The yellow-border map looks fuzzy although the overall outline of the dog is still present. The red-border map almost completely destroyed the structure of the dog. Surprisingly, all of the measures based on pixel-wise errors (first three rows) fail to rank the maps correctly. Our new measure (4th row) ranks the three maps in the right order (Color figure online)

Full size image
Fig. 2
figure 2
S-measure score (ğœ†=0.5,ğ¾=4) for the outputs of SalCut (Cheng et al., 2015) algorithm (2nd row) when fed with inputs of 10 saliency maps (1st row). The ranking results clearly indicate that our measure is good at capturing object structures and can provide a reliable ranking

Full size image
Consequently, the proposed structure measure consists of two parts, including a region-aware structural similarity measure and an object-aware structural similarity measure. The region-aware measure tries to capture the global object structure by combining the structural information of all the object parts. The structural similarity of regions has been well explored in the image quality assessment (IQA) community (Wang et al., 2004). The object-aware similarity measure tries to compare global distributions of foreground and background regions in the foreground map and the GT map.

Our measure is compared against various existing measures including AP, AUC, PASCAL, Fbw, Fğ›½-measure on several widely-used salient object detection benchmarks including ASD (Achanta et al., 2009), SOD (Martin et al., 2001), ECSSD (Xie et al., 2013), PASCAL-S (Li et al., 2014), and HKU-IS (Li & Yu, 2015). Extensive empirical investigations show that Structure-measure not only provides a reliable evaluation but also achieves significantly improved performance than current measures.

This work is an extension of our previous ICCV2017 version (Fan et al., 2017). The major differences between these two versions include: (1) We extend the preliminary version to binary foreground map evaluation and provide a unified evaluation applicable to both binary and non-binary foreground maps. Our work offers new insights into foreground maps evaluation where current measures fail to examine the strengths and weaknesses of models fully. (2) We provide a set of new experiment to validate the efficiency, robustness, and extensibility of our proposed measure. These extension focus on non-binary mapsâ€™ evaluation. Besides, we also give more details about our application-ranking framework in Appendices. (3) We build the several representative online Benchmark and model zoo of saliency detection, which integrates various publicly available saliency datasets with uniform input/output formats (i.e., JPEG for image; PNG for GT). (4) We also provide PythonFootnote3 and MatlabFootnote4 version code for existing benchmarking work which benefits many related tasks and our computer vision community. (5) We have made a lot of efforts to improve the presentations and organizations of our paper. First, several new figures are added or re-produced to better illustrate the meta-measure and key results of this work. Second, we have added several new sections to describe more details about the flaws of current measures, provide more theoretical details about our S-measure, and more theoretical details include region-aware (Sect. 4.1) and object-aware (Sect. 4.2) structure similarity evaluation. The new content will allow readers to better understand our approach.

Current Evaluation Measures
Foreground maps can be generated by various algorithms (e.g., for saliency detection or object segmentation). Saliency detection algorithms often generate non-binary maps, whereas object segmentation algorithms usually generate binary maps. As a result, the foreground maps can be divided as non-binary maps with values in the range [0, 1] or binary maps with values either 0 or 1. Each map value denotes the probability of a specific pixel belonging to the foreground (Peng et al., 2014; Margolin et al., 2014).

Evaluation of Binary Maps
To evaluate a binary map, four values are computed from the prediction confusion matrix: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). These values are then used to compute three ratios: True Positive Rate (TPR) or Recall, False Positive Rate (FPR), and Precision:

Recall=ğ‘‡ğ‘ƒğ‘…=ğ‘‡ğ‘ƒğ‘‡ğ‘ƒ+ğ¹ğ‘
(1)
FPR=ğ¹ğ‘ƒğ‘‡ğ‘+ğ¹ğ‘ƒ
(2)
Precision=ğ‘‡ğ‘ƒğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ
(3)
The Precision and Recall are combined to compute the traditional ğ¹ğ›½-measure.

ğ¹ğ›½=(1+ğ›½2)PrecisionÃ—Recallğ›½2Ã—Precision+Recall=(1+ğ›½2)ğ‘‡ğ‘ƒ(1+ğ›½2)ğ‘‡ğ‘ƒ+ğ›½2ğ¹ğ‘+ğ¹ğ‘ƒ
(4)
where ğ›½ is a parameter to balance the accuracy and the recall (typically ğ›½=1 leading to harmonic mean). Another commonly used binary map evaluation metric is the PASCAL measure:

PASCAL=ğ‘‡ğ‘ƒğ‘‡ğ‘ƒ+ğ¹ğ‘+ğ¹ğ‘ƒ
(5)
Evaluation of Non-binary Maps
AUC and AP are two universally-agreed evaluation measures. Algorithms that produce non-binary maps apply three steps to evaluate the agreement between model predictions (non-binary maps) and human annotations (GT). First, multiple thresholds are applied to the non-binary map to get multiple binary maps. Second, these binary maps are compared to the GT to get a set of TPR [see Eq. (1)] & FPR [see Eq. (2)] values. These values are plotted in a 2D plot (a.k.a ROC curve), from which the AUC distills the area under the curve.

The AP measure is computed in a similar way. One can get a Precision [see Eq. (3)] & Recall [see Eq. (1)] curve by plotting Precision p(r) as a function of Recall r. AP measure (Everingham et al., 2010) is the average value of p(r) over the evenly spaced x axis points from ğ‘Ÿ=0 to ğ‘Ÿ=1.

Recently, a measure called Fbw (Margolin et al., 2014) has offered an intuitive generalization of the ğ¹ğ›½ measure. The authors of Fbw identified three causes of inaccurate evaluation of AP and AUC measures. To alleviate these flaws, they (1) extended the four basic quantities TP, TN, FP, and FN to non-binary values and, (2) assigned different weights (w) to different errors according to different location and neighborhood information.

ğ¹ğœ”ğ›½=(1+ğ›½2)ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğœ”Ã—ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğœ”ğ›½2â‹…ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğœ”+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğœ”
(6)
While this measure improves upon other measures, sometimes it fails to correctly rank the foreground maps (see the 3rd row of the Fig. 1). In the next section, we will analyze why the current measures fail to rank these maps correctly.

Current Measures are Pixel-Wise Based
Traditional measures (AP, AUC, PASCAL, ğ¹ğ›½ and Fbw) rely on two types of basic errors: FN, FP. Since these basic errors are calculated in a pixel-wise manner (see the Fig. 3), they cannot fully capture the structural information of foreground maps. However, foreground maps with fine structural details are often desired in several applications (e.g., image retrieval, object detection and segmentation). Therefore, evaluation measures sensitive to foreground structures are favored. Unfortunately, the aforementioned measures fail to meet this expectation.

A contrived example is shown in Fig. 4 which contains two different types of foreground maps. In FM1, a black square falls inside the digit while in the FM2 it touches the boundary. In our opinion, FM1 is less favored than FM2 since it destroys the foreground map more drastically. However, the current pixel-level MAE (mean absolute error) measure favors FM1 over FM2. This seems to contradict our common sense.

Fig. 3
figure 3
The current evaluation measuresâ€™ framework. The AP, AUC and Fbw evaluation measures are computed in a similar way. They are all calculated in a pixel-wise manner and ignore the structural similarities

Full size image
Fig. 4
figure 4
A pixel-wise based evaluation example. In FM1, a black square falls inside the digit while in the FM2 it touches the boundary (1st row). They are two binary maps (2nd row) with the same TP, TN, FP and FN values. Visually, FM2 is favored over FM1 since FM1 destroys the foreground mapâ€™s structure more drastically such that the the digit is hard to recognize. Current evaluation mean absolute error (MAE) measure is calculated in a pixel-wise manner and treat pixels independently. Hence, it ignores the structure of the foreground maps, thus it favor FM1 over FM2

Full size image
A more realistic example is shown in Fig. 5. The blue-border map here better captures the pyramid than the red-border map, because the latter offers a fuzzy detection map that mostly highlights the top part of the pyramid while ignoring the rest. From an application standpoint (3rd row, the output of the SalCut algorithm fed with saliency maps; ranked by our measure, i.e., the 2nd row), the blue-border map offers a complete shape of the pyramid. In practice, this situation is very common. Thus, if the evaluation measure cannot capture the structural object information, it will not be able provide reliable information for model selection in applications.

Fig. 5
figure 5
A pixel-wise based evaluation example. Two foreground maps are generated by two saliency detection algorithms DSR (Li et al., 2013b), and ST (Liu et al., 2014). According to the applicationâ€™s ranking and our user-study (Apps-Sec. 5; last row), the blue-border map does the best, followed by the red-border map. Since Fbw measure does not account for the structural similarity, it results in the complete reverse ranking. Our measure (2nd row) correctly ranks the blue-border map as higher (Color figure online)

Full size image
Proposed Measure
In this section, we introduce our new measure to evaluate foreground maps. In the image quality assessment (IQA) field, a measure known as structural similarity measure (SSIM) (Wang et al., 2004) has been widely used to capture the structural similarity of the original image and a test image.

Let ğ‘¥={ğ‘¥ğ‘–,ğ‘–=1,2,â€¦,ğ‘} and ğ‘¦={ğ‘¦ğ‘–,ğ‘–=1,2,â€¦,ğ‘} be the FM and GT pixel values, respectively. The ğ‘¥Â¯, ğ‘¦Â¯, ğœğ‘¥, ğœğ‘¦ are the mean and standard deviations of x and y, respectively. ğœğ‘¥ğ‘¦ is the covariance between the two. The SSIM is formulated as the product of three comparison terms including luminance, contrast, and structure:

ğ‘ ğ‘ ğ‘–ğ‘š=2ğ‘¥Â¯ğ‘¦Â¯+ğ¶1(ğ‘¥Â¯)2+(ğ‘¦Â¯)2+ğ¶1â‹…2ğœğ‘¥ğœğ‘¦+ğ¶2ğœ2ğ‘¥+ğœ2ğ‘¦+ğ¶2â‹…ğœğ‘¥ğ‘¦+ğ¶3ğœğ‘¥ğœğ‘¦+ğ¶3
(7)
where the constants ğ¶1,ğ¶2, and ğ¶3 are set to very small values to avoid instability when denominator (e.g., ğ‘¥Â¯2+ğ‘¦Â¯2) is very close to zero in each component.

Fig. 6
figure 6
The framework of our S-measure

Full size image
In Eq. (7), the first two terms denote the luminance comparison and contrast comparison, respectively. The closer the two (i.e., ğ‘¥Â¯ and ğ‘¦Â¯, or ğœğ‘¥ and ğœğ‘¦), the closer the comparison (i.e., luminance or contrast) to 1. The structures of the objects in an image are independent of the luminance that is affected by illumination and the reflectance. So the design of a structure comparison formula should be independent of luminance and contrast. SSIM (Wang et al., 2004) associates two unit vectors (ğ‘¥âˆ’ğ‘¥Â¯)/ğœğ‘¥ and (ğ‘¦âˆ’ğ‘¦Â¯)/ğœğ‘¦ with the structure of the two images. Since the correlation between these two vectors is equivalent to the correlation coefficient between x and y, the formula of structure comparison is denoted by the third term in Eq. (7).

To build salient object detection or object segmentation algorithms, researchers are often more concerned about the foreground object structures. Thus, our proposed structure measure combines both region-aware and object-aware structural similarities. The region-aware structural similarity performs similar to Wang et al. (2004), which aims to capture â€œobject-partâ€ structure information without any special concern regarding how complete is the foreground. The object-aware structural similarity, on the the other hand, is designed to mainly capture the structure information of â€œobject-holisticâ€ which focus on the complete object.

Region-Aware Structural Similarity Measure
Here we explain how to measure region-aware similarity. The region-aware similarity is designed to assess the object-part structure similarity in FM against the GT map. We first divide each of the FM and GT maps into four blocks using a horizontal and a vertical cut-off lines that intersect at the centroid of the GT foreground. Then, the subimages are divided recursively as in Lazebnik et al. (2006). The total number of blocks is denoted as K. An example is shown in Fig. 6. The region-aware similarity ssim(k) of each block is computed independently using Eq. (7). We assign a different weight (ğ‘¤ğ‘˜) to each block proportional to the GT foreground region that this block covers. Thus, the region-aware structural similarity measure can be formulated as:

ğ‘†ğ‘Ÿ=âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘˜Ã—ğ‘ ğ‘ ğ‘–ğ‘š(ğ‘˜).
(8)
Our investigation shows that the proposed ğ‘†ğ‘Ÿ can well describe the object-part similarity between a FM and a GT map. We also tried to directly use SSIM to assess the similarity between FM and GT at the image level or in the sliding window fashion as mentioned in Wang et al. (2004). These approaches fail to capture region structure similarities.

Object-Aware Structural Similarity Measure
Dividing the foreground map into blocks helps evaluate the object-part structural similarity. However, the region-aware measure (ğ‘†ğ‘Ÿ) can not well account for the object similarity in a holistic way. For high-level vision tasks such as salient object detection, the evaluation of the object-level similarity is crucial. To achieve this goal, we propose a novel method to assess the foreground and background separately. Since, the GT maps usually have important characteristics including sharp foregroundâ€“background contrast and uniform distribution, the predicted FM is expected to possess these properties. This helps easily distinguish foreground from the background. We design our object-aware structural similarity measure with respect to these characteristics.

Sharp Foregroundâ€“Background Contrast
Our first observation is that the foreground region of the GT map usually contrasts sharply with the background region. We employ a formulation that is similar with the luminance component of SSIM, to measure how close the mean probability is between the foreground region of FM and the foreground region of GT. Let ğ‘¥ğ¹ğº and ğ‘¦ğ¹ğº represent the probability values of foreground region of FM and GT, respectively. ğ‘¥Â¯ğ¹ğº and ğ‘¦Â¯ğ¹ğº denote the means of ğ‘¥ğ¹ğº and ğ‘¦ğ¹ğº, respectively. The foreground comparison can be represented as:

ğ‘‚ğ¹ğº=2ğ‘¥Â¯ğ¹ğºğ‘¦Â¯ğ¹ğº(ğ‘¥Â¯ğ¹ğº)2+(ğ‘¦Â¯ğ¹ğº)2.
(9)
Equation (9) has several appealing properties:

Swapping the value of ğ‘¥Â¯ğ¹ğº and ğ‘¦Â¯ğ¹ğº, ğ‘‚ğ¹ğº will not change the result,

The range of ğ‘‚ğ¹ğº is [0, 1],

If and only if ğ‘¥Â¯ğ¹ğº=ğ‘¦Â¯ğ¹ğº, then ğ‘‚ğ¹ğº=1, and

The closer the two maps, the closer the ğ‘‚ğ¹ğº to 1 (the most important property).

These properties make Eq. (9) suitable for our purpose.

Uniform Distribution
Our second observation is that the foreground and background regions of the GT maps usually have uniform distributions. So, it is important to assign a higher score to a FM with the object being uniformly highlighted (i.e., similar values across the entire object; see the Fig. 5). If the variability of the foreground values in the FM is high, then the distribution will not be uniform.

In probability theory and statistics, the coefficient of variation defined as the ratio of the standard deviation to the mean (ğœğ‘¥/ğ‘¥Â¯) is a standard measure of dispersion of a probability distribution. Here, we use it to represent the dispersion of the FM. In other words, the coefficient of variation is used to compute the dissimilarity between FM and GT distributions. According to Eq. (9), the total dissimilarity between FM and GT at object-level can be written as:

ğ·ğ¹ğº=(ğ‘¥Â¯ğ¹ğº)2+(ğ‘¦Â¯ğ¹ğº)22ğ‘¥Â¯ğ¹ğºğ‘¦Â¯ğ¹ğº+ğœ†Ã—ğœğ‘¥ğ¹ğºğ‘¥Â¯ğ¹ğº,
(10)
where ğœ† is a constant to balance the two terms. Then, the similarity between FM and GT at object level can be formulated as:

ğ‘†â€²ğ¹ğº=1ğ·ğ¹ğº=2ğ‘¥Â¯ğ¹ğºğ‘¦Â¯ğ¹ğº(ğ‘¥Â¯ğ¹ğº)2+(ğ‘¦Â¯ğ¹ğº)2+2ğœ†Ã—ğ‘¦Â¯ğ¹ğºÃ—ğœğ‘¥ğ¹ğº.
(11)
Since in practice the mean probability of the GT foreground is exactly 1 (ğ‘¦Â¯ğ¹ğº=1), the similarity between FM and GT in object level can be rewritten as:

ğ‘†ğ¹ğº=2ğ‘¥Â¯ğ¹ğº(ğ‘¥Â¯ğ¹ğº)2+1+2ğœ†Ã—ğœğ‘¥ğ¹ğº.
(12)
To compute background comparison ğ‘†ğµğº, we regard the background as the complementary component of foreground by subtracting the FM and GT maps from 1 (change 1 to the maximum value of GT when GT is a non-binary map) as shown in Fig. 6. Then, ğ‘†ğµğº can be similarly defined as:

ğ‘†ğµğº=2ğ‘¥Â¯ğµğº(ğ‘¥Â¯ğµğº)2+1+2ğœ†Ã—ğœğ‘¥ğµğº.
(13)
Let ğœ‡ be the ratio of the foreground area in GT to the image area (ğ‘¤ğ‘–ğ‘‘ğ‘¡â„Ã—â„ğ‘’ğ‘–ğ‘”â„ğ‘¡). The final object-aware structural similarity measure can then be written as:

ğ‘†ğ‘œ=ğœ‡Ã—ğ‘†ğ¹ğº+(1âˆ’ğœ‡)Ã—ğ‘†ğµğº.
(14)
Structure Measure
Having region-aware and object-aware structural similarity evaluation definitions, we can formulate the final measure as,

ğ‘†=ğ›¼Ã—ğ‘†ğ‘œ+(1âˆ’ğ›¼)Ã—ğ‘†ğ‘Ÿ,
(15)
where ğ›¼âˆˆ[0,1]. We set ğ›¼=0.5 in our implementation to assign equal contribution to both region similarity and object similarity. Using this measure to evaluate the three foreground maps in Fig. 1, we can correctly rank the maps consistent with the application rank and human rank.

Experiments
In order to assess the quality of our new measure, we utilized 4 meta-measures proposed by Margolin et al. (2014) and 1 meta-measure (human judgments) proposed by us. These meta-measures are used to assess the quality of evaluation measures (Pont-Tuset & Marques, 2013). To conduct fair comparisons, the 4 meta-measures are computed on the ASD (a.k.a ASD1000) dataset (Achanta et al., 2009). The non-binary foreground maps (5000 maps in total) were generated using five saliency detection algorithms including CA (Goferman et al., 2012), CB (Jiang et al., 2011), RC (Cheng et al., 2015), PCA (Margolin et al., 2013), and SVO (Chang et al., 2011) [binary maps are achieved by feeding non-binary maps to the SalCut (Cheng et al., 2015)].

Setting and Runtime We assign ğœ†=0.5 and ğ¾=4 in all experiments as Fan et al. (2017). We also test our measure on the ASD1000 dataset using a single CPU machine. The average run time for a single image is 0.0053 s.

Meta-Measure 1: Application Ranking
The first meta-measure examines the ranking correlation of the evaluation measure to that of an application that uses foreground maps (Margolin et al., 2014). We assume that the GT map is the optimal input for the application (the top path in Fig. 7). Then, given a foreground map, we compare the applicationâ€™s output (the bottom path in Fig. 7) to that of the GT output. The closer the saliency map is to the GT, the closer its application output should be to the GT output. We compare the ranking result by each binary and non-binary evaluation measure: AP, AUC, Fbw, PASCAL, Fğ›½ and ours, to the ranking result by the application.

The work in Margolin et al. (2014) has examined three applications: object detection, segmentation and image retrieval. Here, we use the SalCut (Cheng et al., 2015) method (for non-binary) and image retrieval (for binary) application (see â€œAppendix Sect. 7â€) to compute this meta-measure.Footnote5

We utilize the 1-Spearmanâ€™s ğœŒ measure (Best & Roberts, 1975) to evaluate the ranking accuracy of the measures, where a lower values indicates better ranking consistency. The score of 0 indicates that the evaluation measure ranked the saliency maps identically to that of the application. The score of 2 indicates that the evaluation measure ranked the foreground maps in a complete reverse order. Comparison between different measures (AP, AUC, Fbw, Ours) is shown in Fig. 8a, which indicates that our structure measure produces the best ranking consistency among other alternative methods. According to the example shown in Fig. 1, all of the current non-binary measures fail to rank the foreground maps correctly. Our measure correctly ranks these maps. In the case of binary maps, S-measure also offers a 5.35%, 3.74%, 2.19%, improvement over the PASCAL, Fğ›½, and Fbw measure, which score 0.897, 0.882, and 0.868, respectively, compared to 0.849 by our measure.

Fig. 7
figure 7
Meta-measure 1: application ranking. To rank foreground maps according to an application, we compare the output obtained when using the GT, to the output when using the FM. The more similar a FM is to the GT map, the closer its applicationâ€™s output should be to the GT output

Full size image
Fig. 8
figure 8
Meta-measure 1â€”results The ranking correlation of an evaluation measure to that given by the SalCut segmentation (Non-binary evaluation) and image retrieval (Binary evaluation) application. We used 1-spearmanâ€™s rho as the results presentation. The lower the score, the better an evaluation measure is in term of predicting the preference of the application. Our measure achieves a better performance over other evaluation measure

Full size image
Fig. 9
figure 9
Meta-measure 2: state-of-the-art versus generic. Given the input image a and the corresponding GT in b, an evaluation measure should give the FM generated by the SOTA method (c) a higher score than the generic map (d) that does not consider the content of the image. Unfortunately, all of the current evaluation measures give the map in d a higher score than c. Only our measure correctly ranks the SOTA result higher

Full size image
Meta-Measure 2: SOTA Versus Generic
Our second meta-measure is that a measure should prefer the output achieved by a SOTA algorithm over generic baseline maps (e.g., centered Gaussian map, see Fig. 9d) that discard the image content. A good evaluation measure should rank the SM generated by a SOTA model higher than a generic map.

We count the number of times a generic map scored higher than the mean score generated by the five SOTA models [CA (Goferman et al., 2012), CB (Jiang et al., 2011), RC (Cheng et al., 2015), PCA (Margolin et al., 2013), SVO (Chang et al., 2011)]. The mean score provides an indication of model robustness. Results are shown in Fig. 10. The lower the value here, the better the measure is. Over 1000 images, our measure has only 11 errors (i.e., generic winning over the s.t.a) for non-binary maps. Meanwhile, the AP and AUC measures are very poor and make significantly more mistakes. Our measure also offers a large improvement over the PASCAL, ğ¹ğ›½, and Fbw.

Fig. 10
figure 10
Meta-Measure 2â€”results. The percentage of times that an evaluation measure ranked a generic map (non-binary circle or binary centered gaussian map) higher than the FM generated by the SOTA model. The lower the score, the better the evaluation measure is. Our measure achieves the best performance

Full size image
Fig. 11
figure 11
Meta-measure 3: ground-truth switch. The score of a FM generated from a should decrease when using a wrong Switched GT as the reference. However, both AUC and AP gave the map in b a higher score when using d instead of c as the reference GT map. Using our measure, the score of b appropriately decreased when switching to random ground-truth (d)

Full size image
Fig. 12
figure 12
Meta-measure 3â€”results. The percentage of times (tested on 1000 ASD dataset) that an evaluation measure assigned a higher score when using an incorrect GT map. The lower the score, the better the measure is. Our measure achieves significant improvement over other measures in both non-binary and binary maps

Full size image
Meta-Measure 3: Ground-Truth Switch
The third meta-measure specifies that a good SM should not obtain a higher score when switching to a wrong GT map. In Margolin et al. (2014), a SM is considered as â€œgoodâ€ when it scores at least 0.5 out of 1 (when compared to the original GT map). Using this threshold (0.5), top 41.8% of the total 5000 maps were deemed as â€œgoodâ€ ones. For a fair comparison, we follow Margolin et al. to select the same percentage of â€œgoodâ€ maps. For each of the 1000 images, 100 random GT switches were tested. We then counted the percentage of times that a measure increased a saliency mapâ€™s score when an incorrect GT map was used (see Fig. 11).

The Fig. 12 shows the results. The lower the score, the higher capability to match to the correct GT. Our measure performs the best about 10 times better compared to the second best measure. This is due to the fact that our measure captures the object structural similarity between a FM and a GT map. Our measure will assign a lower value to the â€œgoodâ€ FM when using a random selected GT since the object structure has changed in the random GT.

Meta-Measure 4: Annotation Errors
The fourth meta-measure specifies that an evaluation measure should not be sensitive to slight errors/inaccuracies in the manual annotation of the GT boundaries. To perform this meta-measure, we make a slightly modified GT map by using morphological operations. An example is shown in Fig. 13. While the two ground truth maps in (b) & (c) are slightly different, a good measure should not switch the ranking between the two foreground maps (d) & (e), when using (b) or (c) as the reference.

We use the 1-Spearmanâ€™s ğœŒ measure to examine the ranking correlation before and after annotation errors were introduced. The lower the score, the more robust an evaluation measure is to annotation errors (Margolin et al., 2014). Results are shown in Fig. 14. Our measure outperforms both the AP and the AUC but is not the best. Inspecting this reason, we realized that it is not always the case that the lower the score, the better an evaluation measure is. It is that sometimes â€œslightâ€ inaccurate manual annotations can change the structure of the GT map, which in turn can change the rank.

We examined the effect of the structure change more carefully. Major structural changes often correspond to continuous large regions in the difference map between ground truth and its morphologically changed version. We used the sum of corroded version of the difference map as a measure of major structure change and to sort all the ground truth images.

Fig. 13
figure 13
Meta-measure 4: annotation errors. An evaluation measure should not be sensitive to slight changes in the manual annotation of the GT boundaries. While GT (b) & GTâ€™ (c) are almost identical, some measures switched the ranking order of the two foreground maps (e, f), depending on the different (d) GT used. Our measure consistently ranked e higher than f. Best viewed in color (Color figure online)

Full size image
Among top 10% least changed images, our measure and Fbw have the same MM4 scores (both of them are 0). When the topology of ground truth map does not change, our measure and Fbw preserve the original ranking. This can be seen in the example in Fig. 15. While ground truth maps (GT and Morphologic GT) differ slightly, both Fbw and our measure preserve the ranking order of the two saliency maps, depending on the used ground truth map.

Fig. 14
figure 14
Meta-measure 4â€”results. The ranking correlation of an evaluation measure under small manual annotation inaccuracies. We use the 1-Spearmanâ€™s Rho measure to present the results. The lower the score, the better

Full size image
Fig. 15
figure 15
Structural unchanged case. Both of ours and the Fbw measure are not sensitive to inaccuracies (structural unchanged) in manual annotations of the GT boundaries

Full size image
For top 10% most changed images, we asked 3 users to judge whether the ground truth images have major structure changes. 95 out of 100 ground truth images were considered to have major structure changes (e.g., small bar, thins legs, slender foot and minute lines in each group; see Fig. 16), for which we believe that keeping the same ranks is not good. Figure 17 is a typical example. When we use the GT map as the reference, Fbw and our measure rank the two maps properly. However, when using Morphologic GT as the reference, ranking results are different. Clearly, the blue-border SM is visually and structurally more similar to the Morphologic GT map than the red-border SM. A good measure should rank the blue-border SM higher than red-border SM. So the ranking of these two maps should be changed. While the Fbw measure fails to meet this end, our measure gives the correct order.

Above-mentioned analysis suggests that this meta-measure is not very reliable. Therefore, we do not include it in our comparison.

Further Comparison
The results in Figs. 8, 10, and 12 show that our measure achieves the best performance using 3 meta-measures over the ASD1000 dataset. However, a good evaluation measure should perform well over almost all datasets. To demonstrate the robustness of our measure, we further use 10 SOTA algorithms for salient object detection to perform experiments on another 4 widely-used benchmark datasets.

Fig. 16
figure 16
Structural changed examples. The first row shoes the GT maps. The second row shows morphologically changed versions. We observe significant structural changes

Full size image
Fig. 17
figure 17
Structural changed case. The ranking of an evaluation measure should be sensitive to the structural changes. Surprisingly, the current best measure (Fbw) does not account for structural changes. Using our measure, we rank the maps correctly. Best viewed on screen

Full size image
Foreground Maps Collection We used 10 SOTA algorithms including 3 traditional models [i.e., ST (Liu et al., 2014), DRFI (Jiang et al., 2013), and DSR (Li et al., 2013b)] and 7 deep learning based models [DCL (Li & Yu, 2016), rfcn (Wang et al., 2016), MC (Zhao et al., 2015), MDF (Li & Yu, 2015), DISC (Chen et al., 2016), DHS (Liu & Han, 2016), and ELD (Lee et al., 2016)] to generate the binary and non-binary foreground maps. Binary maps are obtained by thresholding the non-binary maps using image dependent adaptive thresholding method in Achanta et al. (2009).

Benchmark Datasets The 4 widely-used datasets include PASCAL-S (Li et al., 2014), ECSSD (Xie et al., 2013), HKU-IS (Li & Yu, 2015), and SOD (Martin et al., 2001). PASCAL-S contains 850 challenging images, which have multiple objects in high background clutter. ECSSD contains 1000 semantically meaningful but structurally complex images. HKU-IS is another large dataset that contains 4445 large scale images. Most of the images in this dataset contain more than one salient object with low contrast. Finally, we also evaluate our measure over the SOD dataset, which is a subset of the BSDS dataset. It contains a relatively small number of images (300), but with multiple complex objects.

Table 1 Non-binary (N-binary) & Binary mapsâ€™ quantitative comparison with current measures on 3 meta-measures
Full size table
Fig. 18
figure 18
The rank distance between Fbw and our measure. The aâ€“c are the three datasets used to compute the rank distance between Fbw and our S-measure. The y axis of the plot is the number of the images. The x axis is the rank distance

Full size image
Fig. 19
figure 19
The rank distance between AP and our measure. The aâ€“c are the three datasets used to compute the rank distance between AP and our S-measure. The y axis of the plot is the number of the images. The x axis is the rank distance

Full size image
Results Non-binary and binary mapsâ€™ quantitative comparison results are shown in Table 1. Our measure performs the best according to the first meta-measure for both binary and non-binary maps evaluation. This indicates that our measure is more useful for applications than other measures.

For the evaluation results (binary and non-binary) in MM2, our measure performs better than the existing four measures (AP, AUC, F, PASCAL) with a large margin. The results on two easier datasets (ECSSD and HKU-IS) show that our measure and Fbw perform on par for both binary and non-binary maps.

According to meta-measure 3, our measure reduces the non-binary error rate by 67.62%, 44.05%, 17.81%, 69.23% on PASCAL, ECSSD, SOD and HKU-IS, respectively compared to the second ranked measure. For binary maps, our measure also reduces the error rate by 62.86%, 52.38%, 10.96%, 61.54% on PASCAL, ECSSD, SOD and HKU-IS, respectively compared to the second ranked measure. This indicates that our measure has higher capacity to capture the structural similarity between FM and GT maps.

Overall, our measure wins in the majority of cases indicating that it is more robust than other measures.

Meta-Measure 5: Human Judgments
Here, we propose a new meta-measure to evaluate foreground evaluation measures. This meta-measure specifies that the map ranking according to an evaluation measure should highly agree with the human ranking. It is argued that â€œa human being is the best judge to evaluate the output of any segmentation algorithmâ€ (Pal & Pal, 1993). However, subjective evaluation over all images of a dataset is impractical due to time and monetary costs. To the best of our knowledge, there is no such visual similarity evaluation database available in the object segmentation domain that meets these requirements. Here, we focus on the non-binary maps to collect such a database.

Fig. 20
figure 20
The rank distance between AUC and our measure. The aâ€“c are the three datasets used to compute the rank distance between AUC and our S-measure. The y axis of the plot is the number of the images. The x axis is the rank distance

Full size image
Stimuli The source foreground maps are sampled from three large scale datasets: PASCAL-S, ECSSD, and HKU-IS. As mentioned above, we use 10 SOTA saliency models to generate the maps for each dataset. Therefore, we have 10 foreground maps for each image. We use Fbw and our measure to evaluate the 10 maps and then pick the first ranked map according to each measure. If the two measures choose the same map, their rank distance is 0. If one measure ranks a map first, but the other ranks the same map in the n-th place, then their rank distance is |ğ‘›âˆ’1|. Figures 18, 19 and 20a, b, c show the histogram of rank distances between the two measures. The blue-box is the number of images for each rank distance. Some maps with rank distance greater than 0 are chosen as candidates for our user study.

User Study We randomly selected 100 pairs of maps from the three datasets. The top panel in Fig. 21b shows one example trial with the best map according to our measure on the left, and the best map according to the Fbw on the far right. The user is asked to choose the map she thinks resembles the most with the GT map. In this example, these two maps are obviously different making the user decide easily. In another example (bottom panel in Fig. 21b), the two maps are too similar making it difficult for the used to choose the one closet to the GT. Therefore, we avoid showing such cases to the subjects. Finally, we are left with a stimulus set of size 50 pairs. We developed a mobile phone app (see Fig. 21a) to conduct the user study. We collected data from 45 viewers who were naive to the purpose of the experiment. Viewers had normal or corrected vision. (age distribution is 19â€“29 years old; eduction from undergraduate to Ph.D; from 10 different majors such as history, medicine and finance; 25 males and 20 females).

Results Results (Fbw vs. our measure) are shown in Fig. 22. The percentage of trials (averaged over subjects) in which a viewer preferred the map chosen by our measure is 63.69%. We used the same procedure to conduct two additional user studies (AP vs. our measure, AUC vs. our measure). The results are 72.11% and 73.56%, respectively. This indicates that our measure correlates better with human judgments.

Fig. 21
figure 21
Our user study platform

Full size image
Fig. 22
figure 22
Results of our user study (Fbw & S-measure). The x axis is the viewer id. The y axis shows the percentage of the trials in which a viewer preferred the map chosen by our measure

Full size image
Fig. 23
figure 23
Ranking of 10 saliency models using our new measure. The y axis shows the average score on each dataset [PASCAL-S (Li et al., 2014), ECSSD (Xie et al., 2013), HKU-IS (Li & Yu, 2015), SOD (Martin et al., 2001)]

Full size image
Saliency Model Comparison
Establishing that our S-measure offers a better way to evaluate foreground maps, here we compare 10 SOTA models on 4 datasets (PASCAL-S, ECSSD, HKU-IS, and SOD). Figure 23 shows the rank of the 10 models. According to our measure, top three models in order are dhsnet, DCL, and rfcn. Moreover, we also establish many representative online benchmarks (1. http://dpfan.net/socbenchmark; 2. http://dpfan.net/d3netbenchmark; 3. http://dpfan.net/cosod3k/; 4. http://dpfan.net/camouflage/) to compared our S-measure with other measures.

Ablation Study
To investigate the contribution of each part in our S-measure, we further conduct the ablation study on a new human ranking dataset (Fan et al., 2021b). The FMDatabaseFootnote6 consists of 185 color images and 555 ranked maps. Similar to meta-measure1, we also utilize the 1-Spearmaâ€™s ğœŒ metric to evaluate the ranking performance of the measures.

Table 2 Ablation studies of human ranking (Using FMDatabase-IJCAIâ€™18) in terms of 1-Spearmanâ€™s measure
Full size table
As shown in Table 2, we observe that our S-measure outperforms other settings (i.e., object-aware, region-aware) on FMDatabase. It clearly shows that only region-level or object-level structural similarity cannot provide stable evaluation performance. Since the object-aware structure similarity mainly focusing on assessing the property of sharp foregroundâ€“background contrast. It more like a global evaluation. On the other hand, the region-aware links to the local evaluation which based on window-level statistics. Compared with existing two classical metrics (i.e., Fbw and SSIM), we also found that our metric achieve the best results.

Fig. 24
figure 24
Failure case. With the given image (a) and its corresponding GT in (b), our S-measure ranked (d) generic higher than c rfcn due to these prediction map without obvious structure

Full size image
Fig. 25
figure 25
Integration of the image with its foreground map. a, c Are the images (Imgs) with its GT and FM. b, d Are the combined images

Full size image
Discussion and Conclusion
In this paper, we analyzed the current saliency evaluation measures based on pixel-wise errors and showed that they ignore the structural similarities. We then presented a new structural similarity measure known as ğ’âˆ’ğ‘šğ‘’ğ‘ğ‘ ğ‘¢ğ‘Ÿğ‘’ which simultaneously evaluates region-aware and object-aware structural similarities between a saliency map and a ground-truth map. Our measure is based on two important characteristics: (1) sharp foregroundâ€“background contrast, and (2) uniform saliency distribution. Further, the proposed measure is efficient and easy to calculate. Experimental results on 5 datasets demonstrate that our measure performs better than the current measures including AP, AUC, and Fbw. Finally, we conducted a behavioral judgment study over a database of 100 saliency maps and 50 ground-truth maps. Data from 45 subjects shows that on average they preferred the saliency maps chosen by our measure over the saliency maps chosen by the Fwb.

All metrics are double-edged swords. Generally, itâ€™s hard to argue which measure is the best one. These measures are deeply coupled to the actual applications, e.g., some applications may favor the correctness of important regions while some may prefer the continuity in local structures. We observe a few failure cases where a prediction map without (or less) object structure will achieve a higher S-measure score. For example, as shown in Fig. 24, the proposed S-measure does not work well in this situation in which the object of the GT (b) without a clear structure. Consequently, to evaluate the foreground maps, we need to assess whether it performs well on multi metrics at the same time.

In summary, our measure offers new insights into foreground map evaluation where current measures fail to examine the strengths and weaknesses of models fully. For now, we have found that the saliency [e.g., RGB SOD (Zhuge et al., 2021a; Zhang et al., 2020a; Chen et al., 2020; Zhao et al., 2019), RGB-D SOD (Zhang et al., 2021; Zhao et al., 2020; Fan et al., 2021d; Fu et al., 2021; Zhou et al., 2021), RGB-T SOD (Zhang et al., 2019b), light field SOD (Zhang et al., 2019a; Piao et al., 2020; Jiang et al., 2020), VSOD (Ji et al., 2021), 360 SOD, SID (Li et al., 2017), Saliency Ranking (Amirul Islam et al., 2018), Co-SOD (Fan et al., 2021c, e; Zhang et al., 2020b), and HR SOD (Zeng et al., 2019)] community has begun to widely adopt this measure even in the camouflaged object detection (Fan et al., 2021a; Zhai et al., 2021; Mei et al., 2021) and medical image segmentation (Fan et al., 2020).