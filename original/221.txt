Emotion recognition from EEG signals allows the direct assessment of the “inner” state of a user, which is considered an important factor in human-machine-interaction. Many methods for feature extraction have been studied and the selection of both appropriate features and electrode locations is usually based on neuro-scientific findings. Their suitability for emotion recognition, however, has been tested using a small amount of distinct feature sets and on different, usually small data sets. A major limitation is that no systematic comparison of features exists. Therefore, we review feature extraction methods for emotion recognition from EEG based on 33 studies. An experiment is conducted comparing these features using machine learning techniques for feature selection on a self recorded data set. Results are presented with respect to performance of different feature selection methods, usage of selected feature types, and selection of electrode locations. Features selected by multivariate methods slightly outperform univariate methods. Advanced feature extraction techniques are found to have advantages over commonly used spectral power bands. Results also suggest preference to locations over parietal and centro-parietal lobes.
SECTION 1Introduction
Current efforts in human-machine-interaction (HMI) aim at finding ways to better and more appropriately interact with computers/humans. To make HMI more natural, knowledge about the emotional state of a user is considered an important factor. Emotions are important for correct interpretation of actions as well as communication. Interest in emotion recognition from different modalities (e.g. face, posture, motion, voice) has risen in the past decade and recently gained attention in the field of brain-computer-interfaces (BCIs), which has coined the term affective BCI (aBCI) [1].

1.1 Related Work
While the reliability of the ground-truth, duration and intensity of emotions remain ongoing challenges of automatic affect recognition, several attempts have been undertaken to advance the comparatively new field of emotion recognition from electroencephalography (EEG) [2]. Early work on emotion recognition from EEG goes back as far as 1997 [3], and gained more and more attention in recent years. Typically, feature extraction and electrode selection is based on neuro-scientific assumptions. In this, spectral power in various frequency bands is often associated to emotional states, as well as coherence and phase synchronization of pairs of electrodes [4]. Furthermore, frontal asymmetry in α band power as differentiator of valence levels has received a lot of attention in neurological studies [5].

Beside neuro-scientific assumptions, also advanced signal processing finds application in the field of aBCIs [6], which leads to a vast amount of possible features and makes it necessary to reduce dimensions for recognition tasks in order to avoid over-specification. Computational methods from the field of machine learning are applied to optimize the selection of features and electrodes w.r.t. achieved emotion estimation accuracy [4].

1.2 Problem Statement
In emotion recognition from EEG it is not generally agreed upon which features are most appropriate, and only a few works exist, which compared different features with each other. For example, Schaaff and Schultz compared two sets of features AI and AII (see Table 2) on a self recorded data set of five subjects and three emotions [7]. Different time-frequency techniques and frequency bands were evaluated by Hadjidimitriou and Hadjileontiadis on a data set with nine subjects and different levels of liking [8]. Petrantonakis and Hadjileontiadis [9] studied features in comparison to feature vectors used in [10] and [11] on a self recorded data set of 16 subjects and six emotions.

Other studies looked into finding the best electrode positions, e.g. Wang et al. applied mRMR feature selection (FS) to find electrode positions of TOP-30 features on a self recorded data set with five subjects and four emotions [12]. A channel selection method based on synchronization likelihood was tested on one subject by Ansari-asl et al. [13]. Finally, in our previous study we investigated electrode and feature selection of selected time and frequency-domain features on a self recorded data set of 16 subjects and five emotions [14].

However, a major limitation of these approaches is that so far only a handful of features have been compared in each study. Additionally, most studies rely on a different, yet usually small data set. In this work, we aim for a complete review of feature extraction methods used for affective EEG signal analysis and a systematic comparison on one data set that allows for a qualitative evaluation of favorable features and electrodes. In doing so, we first survey techniques for feature extraction from 33 studies and then implement and evaluate them by means of different feature selection methods on a reasonably large database of 16 subjects. Thus, the contributions of this work are threefold: 1) a comprehensive review of EEG features for emotion recognition, 2) a first systematic comparison of features on one database using multiple FS methods to arrive at more robust results when 3) comparing them to existing findings from other studies.

The remainder of this paper is organized as follows: Section 2 reviews feature extraction methods used for emotion classification from EEG. Five different feature selection techniques are introduced in Section 3. In Section 4, we present details on the recorded data set and its evaluation. Data processing and experimental setup are described in Section 5. We give results in Section 6 with an interpretation in Section 7 and end with a brief conclusion in Section 8.

1.3 Notation
Throughout this paper the following notation is used: Undefined control sequence \xibi denotes the vector of the time series of a single electrode, T is the number of time-samples in Undefined control sequence \xibi. The time derivative of Undefined control sequence \xibi is written as Undefined control sequence \xibi. The representation of Undefined control sequence \xibi in the frequency domain is given by Undefined control sequence \Xibi.

An instance of a feature of Undefined control sequence \xibi is indicated as x. The matrix of all features is denoted by X=[x1,…,xF] , where xi is the vector of all samples of one feature and F is the number of features. A feature x is considered a random variable. The set of all features is called X; a subset of features is called S.

SECTION 2Feature Extraction
In this section, we review a wide range of features relevant for emotion recognition from EEG that have been proposed in the past. A good starting point is given in a recent study by Kim et al. [4] which we extended by several important works in the field of emotion recognition from EEG (see Table 2). We generally distinguish features in time domain, frequency domain, and time-frequency domain. Features are typically calculated from the recorded signal of a single electrode, but also a few features combining signals of more than one electrode have been found in literature, which are listed at the end of this section.

2.1 Time Domain Features
Although time-domain features from EEG are not predominant, numerous approaches exist to identify characteristics of time series that vary between different emotional states.

2.1.1 Event Related Potentials (ERP)
Frantzidis et al. used amplitude and latency of ERPs (P100, N100, N200, P200, P300) as features in their study [15]. In an online application, however, it is difficult to detect ERPs related to emotions since the onset is usually unknown (asynchronous BCI).

2.1.2 Statistics of Signal
Several statistical measures have been used to characterize EEG time series [10] , [12], [16], [17]. These are:

Power: Undefined control sequence \xibi

Mean: Undefined control sequence \xibi

Standard deviation: Undefined control sequence \xibi

1st difference: Undefined control sequence \xibi

Normalized 1st difference: Undefined control sequence \xibi

2nd difference: Undefined control sequence \xibi

Normalized 2nd difference: Undefined control sequence \xibi.

The normalized first difference Undefined control sequence \xibi is also known as Normalized Length Density, and captures self-similarities of the EEG signal [18].

2.1.3 Hjorth Features
Hjorth [19] developed the following features of a time series, which were used in EEG studies, e.g. [13], [20] :

Activity: Undefined control sequence \xibi

Mobility: Undefined control sequence \xibi

Complexity: Undefined control sequence \xibi .

Since activity is just the squared standard deviation introduced above, we omit this feature in our implementation.

2.1.4 Non-Stationary Index (NSI)
Kroupi et al. employed the NSI as a measure of complexity by analyzing the variation of the local average over time [18]. The normalized signal Undefined control sequence \xibi is divided into small parts and the average of each segment is computed. The NSI is defined as the standard deviation of all means, where higher index values indicate more inconsistent local averages [21].

2.1.5 Fractal Dimension (FD)
A frequently used measure of complexity is the fractal dimension, which can be computed via several methods. For example, Sevcik’s method [13], Fractal Brownian Motion [22], Box-counting [23], or Higuchi algorithm [16] were employed. The latter is known to produce results closer to the theoretical FD values than Box-counting [16] and is implemented here. In order to compute the fractal dimension Undefined control sequence \xibi by the Higuchi algorithm [24], the time series Undefined control sequence \xibi, t=1,…,T is rewritten as:
Undefined control sequence \xibi
View Sourcewhere [⋅] denotes the Gauss notation for the floor function, m=1,…,k is the initial time and k is the time interval. Then, k sets are calculated by:
Undefined control sequence \xibi
View SourceThe average value over k sets of Lm(k), denoted ⟨L(k)⟩ , has the following relationship with the fractal dimension Undefined control sequence \xibi:
Undefined control sequence \xibi
View SourceWe obtain Undefined control sequence \xibi as the negative slope of the log-log plot of ⟨L(k)⟩ against k.

2.1.6 Higher Order Crossings (HOC)
Motivated to find an efficient and robust feature extraction method that captures the oscillatory pattern of EEG, Petrantonakis and Hadjileontiadis introduced HOC-based features [9]. Herein, a sequence of high-pass filters is applied to the zero-mean time series Z(t):
Ik{Z(t)}=∇k−1Z(t),(1)
View Sourcewhere ∇k is the iteratively applied difference operator, and the order k=1,…,10 according to [9]. The HOC sequence Dk , i.e. the resulting k features, comprises the number of zero-crossings of the filtered time series Ik{Z(t)} by counting its sign changes.

The approach has also been successfully studied in combination with hybrid adaptive filters [25]. Preprocessing for noise reduction is implemented in this work using third level decomposition of Discrete Wavelet Transform (DWT) (see Section 2.3 ).

2.2 Frequency Domain Features
2.2.1 Band Power
The most popular features in the context of emotion recognition from EEG are power features from different frequency bands. This assumes stationarity of the signal for the duration of a trial. The frequency band ranges of EEG signals are varying slightly between studies. Commonly they are defined as given in the first two columns of Table 1. Alternatively, frequency bands are computed in small equal-sized bins, e.g. Δf=1 Hz [26], or Δf=2 Hz [7], [27], [28].

TABLE 1 Frequency Band Ranges and Decomposition Levels of EEG Signals Recorded at fS=512  Hz

The mostly used algorithm to compute discrete fourier transform (DFT) is the fast fourier transform (FFT) applied by [3], [7], [12], [29], [30], [31], [32]. Commonly used alternatives are short-time fourier transform (STFT) [28], [33] or the estimation of power spectra density (PSD) using Welch’s method [15], [18], [20], [26], [27], [34], [35]. When averaging over all windows, STFT is considered more robust against noise. Hence, this method is adopted in the experiment described in Sections 4, 5, and 6, using a Hamming window of length 1,000 ms with no overlap as parameters. The features extracted from the resulting representation of the signal in frequency domain are: avrg. power (mean) of frequency bands and bins ( Δf=2 Hz), their minimum, maximum, and variance. Additionally, the ratio of mean band powers β/α is calculated for each channel.

2.2.2 Higher Order Spectra (HOS)
The group of frequency domain features. Also belonging to the group of frequency domain features are bispectra and bicoherence magnitudes used by Hosseini et al. [36], which are available in the HOSA toolbox [37]:

The bispectrum Bis represents the Fourier Transform of the third order moment of the signal, i.e.:
Undefined control sequence \Xibi
View SourceRight-click on figure for MathML and additional features.where Undefined control sequence \Xibi is the Fourier Transform of the signal Undefined control sequence \xibi, ∗ denotes its complex conjugate, and E[⋅] stands for the expectation operation.

Bicoherence Bic is simply the normalized bispectrum:
Bic(f1,f2)=Bis(f1,f2)P(f1)⋅P(f2)⋅P(f1+f2)−−−−−−−−−−−−−−−−−−−−√ ,(2)
View SourceRight-click on figure for MathML and additional features.where Undefined control sequence \Xibi is the power spectrum. Four different features are extracted for each electrode and each of 16 combinations of frequency bands, namely the sum and the sum of squares of both Bis and Bic (for details see [36]).

2.3 Time-Frequency Domain
If the signal is non-stationary, time-frequency methods can bring up additional information by considering dynamical changes.

2.3.1 Hilbert-Huang Spectrum (HHS)
Hadjidimitriou et al. compared three such methods, namely STFT based spectrogram (SPG, also used in [22]), the Zhao-Atlas-Marks (ZAM) distribution, and the Hilbert-Huang Spectrum [8]. Although results were comparable between all methods, the latter, non-linear method, showed to be more resistant against noise than SPG. We hence computed HHS for each signal, which is done via empirical mode decomposition (EMD) to arrive at intrinsic mode functions (IMFs) to represent the original signal:
Undefined control sequence \xibi
View Sourcewhere rK(t) denotes the residue which is monotonic or constant. Using the Hilbert transform of each IMFi , the analytical signal can be described by its amplitude Ai(t) and instantaneous phase θi(t). The derivative of θi(t) can be used to compute the meaningful instantaneous frequency fi(t)=12πdθidt, which yields a time-frequency representation of the amplitude Ai(t). Further, the squared amplitude is computed, and for both, the average of each frequency band is calculated as features.

2.3.2 Discrete Wavelet Transform
A more recent technique from signal processing is discrete wavelet transform, which decomposes the signal in different approximation and detail levels corresponding to different frequency ranges, while conserving the time information of the signal. The trade-off herein is made by down-sampling the signal for each level. For details, the reader is referred to [38], for example. Correspondence of frequency bands and wavelet decomposition levels depends on the sampling frequency and is given for our case of fS=512 Hz in the last column of Table 1 [39]. We implemented features from two wavelet functions as described below.

Murugappan et al. introduced feature extraction for emotion recognition from EEG via DWT using “db4” wavelet functions, extracting energy and entropy of fourth level detail coefficients (D4) corresponding to α band frequencies [11]. They extended their approach later to frequency bands β and γ, including root mean square ( RMS), recursive energy efficiency ( REE) and its log(REE) and abs(log(REE)), as given in the equations below [17], [40]:
RMS(j)=∑ji=1∑niDi(n)2∑ji=1ni−−−−−−−−−−−−−−⎷ ,(3)
View Sourcewhere Di are the detail coefficients, ni the number of Di at the ith decomposition level, and j denotes the number of levels; and
REE=EbandEtotal-3b ,(4)
View Sourcewhere Eband is the energy of a subband, and the total energy of subbands Etotal-3b=Eα+Eβ+Eγ.

A different prototype wavelet was suggested by Frantzidis et al., i.e. third order biorthogonal wavelet functions [15]. Power of δ , β, and α bands are used as features.

2.4 Features Calculated from Combinations of Electrodes
2.4.1 Multichannel Complexity D2
Multi-channel D2 is introduced by Konstantinidis et al. as a non-linear measure representing the signal’s complexity [41]. Unfortunately, description and citations in their paper are incomplete making it impossible to re-implement this feature.

2.4.2 Magnitude Squared Coherence Estimate
This feature (short MSCE) represents the correspondence of two signals Undefined control sequence \xibi and Undefined control sequence \xibi at each frequency, taking values between 0 and 1 [42]. It is defined as:
Cij(f)=|Pij(f)|2Pi(f)Pj(f) ,(5)
View Sourcewhere Pij is the cross power spectral density and Pi and Pj are the power spectral density of Undefined control sequence \xibi and Undefined control sequence \xibi, respectively. In order to reduce the large amount of features resulting from all possible combinations of electrodes, Cij is averaged over the frequency bands. MSCE features are equivalent to cross-correlation coefficients used in earlier studies [3], [7], [20].

Due to suggestions from neuro-scientific findings of hemispherical asymmetry related to emotion (e.g. [43]), some studies implemented features based on the combination of symmetrical pairs of electrodes. These features can be divided into differential asymmetry and rational asymmetry.

2.4.3 Differential Asymmetry
Most frequently used are differences in power bands of corresponding pairs of electrodes, computed as the difference of two features:
Δx=xl−xr ,(6)
View Sourcewhere (l,r) denote the symmetric pairs of electrodes on the left/right hemisphere of the scalp [26], [32], [33], [34], [35]. Additionally, Liu and Sourina studied pairwise hemispherical differences of statistical features and Fractal Dimension [16].

2.4.4 Rational Asymmetry
In few cases, ratios of features from symmetric electrodes are computed. Brown et al. used the spectrogram to compute α-power ratios xl/xr for consecutive time windows of Δt=2s [27]. From this, they extracted kurtosis, maximum, and the number of peaks. The latter was defined by a threshold of mean(xl/xr)+2σ, where σ is the standard deviation, and normalized to the length of the recording. In analogy to the differential asymmerty features, Lin et al. used the ratio of band powers in their study [33].

SECTION 3Feature Selection
The large amount of possible features makes it necessary to reduce this space in order to avoid over-specification and to make feature computation feasible online. Techniques to achieve this have been used in some of the previous studies (see last column of Table 2), but they are limited to selecting from the set of features included in each study.

TABLE 2 Studies on Emotion Recognition from EEG Listed with Used Features and Method to Extract Them, as well as the Number/Position of Electrodes and Methods for Feature Selection, If Applied

Feature Selection methods can generally be divided into filter and wrapper methods (for reviews refer to [47], [48], [49]). While wrapper methods select features based on interaction with a classifier, i.e. an underlying model, filter methods are model-independent. An advantage of filters is that they usually require less computational power than wrappers and are, hence, more suitable for big data sets. We apply several state-of-the-art filter methods introduced below, both uni- and multivariate. The latter are important to find and consider interactions of features. Generally, results obtained from applying multiple FS methods are considered more robust [50].

3.1 ReliefF
This univariate technique is an extension of the Relief algorithm, which uses a subsample of all instances to adjust weights of each feature depending on their ability to discriminate between two classes [51]. Feature selection is done by estimating a quality weight W(i) for each feature xi, i=1,…,F based on the difference diff(⋅) to the nearest hit xH and nearest miss xM of m randomly selected instances xk :
W(i)=W(i)−diff(i,xk,xH)/m+diff(i,xk,xM)/m .(7)
View SourceRight-click on figure for MathML and additional features.To overcome the high sensitivity to noise, ReliefF uses n nearest hits/misses. Multiple classes are accounted for by searching for the n nearest instances from each class weighted by their prior probabilities. A MATLAB implementation is readily provided in the statistics toolbox.

3.2 Min-Redundancy-Max-Relevance (mRMR)
The most famous method using mutual information to characterize the suitability of a feature subset is called minimal-Redundancy-Maximal-Relevance (mRMR) developed by Ding and Peng [52]. Mutual Information between two random variables x and y is defined as
I(x;y)=∫∫p(x,y)logp(x,y)p(x)p(y)dxdy,(8)
View SourceRight-click on figure for MathML and additional features.where p(x) and p(y) are the marginal probability density functions of x and y, respectively, and p(x,y) is their joint probability distribution. If I(x;y) equals zero, the two random variables x and y are statistically independent.

The multivariate mRMR method aims to optimize two criteria simultaneously: 1) Maximal-relevance criterion D, which means to maximize average mutual information I(xi;y) between each feature xi and the target vector y. 2) Minimum-redundancy criterion R, which means to minimize the average mutual information I(xi;xj) between two features. The algorithm finds near-optimal features using forward selection. Given an already chosen set Sk of k features, the next feature is selected by maximizing the combined criterion D−R:
maxxj∈X−Sk[I(xj;y)−1k∑xi∈SkI(xj;xi)].(9)
View SourceA necessary step to use the toolbox provided by Peng et al. [53] is to discretize the data beforehand (we chose 20 levels).

3.3 Effect-Size (ES)-Based Feature Selection
Several test statistics have been proposed for feature selection [49]. We introduce three effect size measures from analysis of variance (ANOVA) that draw on the difference between within-class and between-class variance of a feature (set).

3.3.1 Univariate
Cohen’s effect size f2 is a generalization to more than two classes of Cohen’s d=|μ1x−μ2xσ| used for the statistical t-test [54]. The spread of the class means μix, computed from the samples belonging to class i , in the numerator is represented as a standard deviation σm. The denominator remains the pooled standard deviation σ of the populations involved. Thus,
f=σmσ,where  σm=∑ci=1(μix−μx)2c−−−−−−−−−−−−−−√(10)
View SourceRight-click on figure for MathML and additional features.for equal sample sizes per class. Here, μx is the overall mean of a feature x. The number of classes is denoted by c.

3.3.2 Multivariate
Multivariate effect size measures from corresponding multivariate ANOVA (MANOVA) in statistics are based on the eigenvalues λi of the generalized eigenproblem
Hν=λνE ,(11)
View Sourcewhere H and E are the between and within sum of square and cross product (SSCP) matrices, respectively [55]. Since solving (16) requires the computation of the inverse of E , features have to be uncorrelated. We guarantee this by a general preprocessing step explained in Section 5.

The effect size based on Wilk’s Λ, which has been applied for feature selection in [56], is defined as
τ2=1−Λ1/r,whereΛ=∏i=1r(11+λi)(12)
View SourceRight-click on figure for MathML and additional features.and simplifies to f2 in the univariate case. The variable r=min(F,c−1). It adjusts for the degrees of freedom and the number of eigenvalues used to compute the statistic.

Roy’s statistic θ considers the largest eigenvalue only, i.e. the explained variance of the first underlying construct and is defined as
θ=λ11+λ1 .(13)
View SourceSince it does not need to be normalized, θ can be used directly as a measure of effect. For large eigenvalues, both statistics converge to one.

For all ES measures, sequential forward selection (SFS) of features is carried out by ranking these measures. In the multivariate cases, the feature that returns the best ES measure when added to the already chosen set of features S is added.

SECTION 4Database
To conduct an experiment comparing features listed in Section 2, we recorded a reasonably large database. Of special importance was to achieve a reasonably high spatial resolution of electrodes as well as to cover a broad range of emotions.

4.1 Experiment and Procedure
In total, 16 subjects (seven female, nine male) aging between 21 and 32 participated in the experiment. For each subject, the recorded data set contains eight trials of 30s EEG recording for five different emotions (happy, curious, angry, sad, quiet), which were selected to cover a good part of the valence-arousal-dominance (VAD) space. The experiment took place in a closed environment controlled for sound and light conditions. Emotions were induced using IAPS pictures [57]. We selected pictures on the basis of dimensional ratings, that are included in the IAPS database, by means of an in-house developed tool to find best matches around a user defined mean with minimal variance. Means of above mentioned emotions in VAD space were taken from Russell and Mehrabian [58]. This way, a total of 32 pictures were selected for each emotion, i.e. a total of 160 emotional pictures.

Subjects were instructed on the experiment procedure and filled out a questionnaire testing for suitability (e.g. skin allergies), well-being (WHO), and trauma screening (PTSD). No subject had to be excluded. After being equipped with recording sensors, a test-run using only neutral IAPS pictures was conducted prior to the actual experiment to accustom the subjects to the protocol and the concept of the SAM questionnaire (Self-Assessment-Manikin Test [59]), which was completed after each trial. Fig. 3 shows the induction protocol implemented. Each trial started with a black screen, followed by four randomly drawn pictures from an emotion subset, each shown for 5 seconds. Every picture was used only once per subject. To capture late effects of emotional responses, recording of each trial was prolonged by 10s showing a black screen before SAM ratings on a scale from [1;9] were taken. Each trial ended with a neutral picture (4s) to reduce the effect of biased transition probability based on the previous emotion [60]. In total, 40 trials were recorded for each subject, while the order of emotions was randomized.

A 64-channel EEG cap with g.tec gUSBamp and electrodes placed according to the extended 10-20 system was used for recording at 512  Hz. The amplifier was set to include a 50 Hz notch filter and a band-pass filter between 0.1-100 Hz. The effect of artifact removal, which is a common pre-processing step for EEG data, was found to be marginal and thus, was not carried out. This speeds up processing substantially, i.e. favors online recognition capability.

4.2 Evaluation of Induction
Since successful induction of emotion is a major challenge, we tested whether the average of dimensional rating over each combination of four pictures used for induction corresponds to the targeted emotion range. Although the IAPS pictures do not resemble the emotion values to the same extreme as literature suggests, all combinations overlapped with the target emotion region and were well separable. This particularly holds for the hand selected angry pictures.

For validation of induction, we compared the results of SAM tests with the targeted values of the presented picture sets and received an average correlation coefficient of r=.545 . The second column in Table 3 lists the correlation coefficient for each subject. Its variance is very large with the extreme values of .255 and .848, which suggests a highly individual reaction to emotion stimuli. Particularly high correlation and supposedly successful induction of emotion was reached for subjects 1, 7, 10, 11, and 13. We further discuss our results with respect to the induction success in Section 6.

TABLE 3 Pair-Wise Correlation Coefficients between IAPS and SAM Ratings, Signal Quality and Accuracy (Optimal Number of Features) for Different FS Methods Listed for Each Subject

4.3 Evaluation of EEG Data
Albeit careful preparation and signal checks previous to the experiment, its duration involved the possibility to loose proper contact of shifting electrodes. We therefore analyzed the signal quality of each subject. While for some subjects few electrode signals were permanently noisy over the whole duration of the experiment, for others signal problems occurred in some trials for almost all electrodes. Since these missing trials would reduce the already limited amount of samples per subject, we decided to exclude these subjects from further processing. As depicted in the third column of Table 3 noise corrupted electrodes were found for subject 1, 5, 7, 11, 16.

SECTION 5Classification
Pre-studies showed that it takes about 10 s for an emotion to be induced using pictures. The duration of emotions in laboratory environments is typically assumed 0.5-4 seconds [61]. Thus, time intervals between 11-15 s after emotion induction onset, i.e. the appearance of the first picture, were considered in the following analysis. Given a sampling rate of 512 Hz, this resulted in 2,048 time samples in our case.

Fig. 2 illustrates data processing for each subject. A feature matrix was generated from the EEG data of 8 trials and 5 classes, leading to 40 rows or feature vectors. Features were extracted from all 64 electrodes which results in a total of 22,881 features. The resulting number of features is indicated for each feature group. Features like MSCE, which was computed from all combinations of electrodes and frequency-ranges resulted in 12,096 features alone. Except for MSCE and asymmetry features, all features can be associated with one electrode, which allows us to draw conclusions on their importance for emotion recognition in Section 6.3. Features were z-normalized to zero mean and standard deviation equal to one. Further, we eliminated all almost identical features which yield a correlation coefficient higher than .98 in order to avoid problems with singularities, which may occur for some FS methods.

Given the relatively small number of samples for each class, leave-one-out cross validation was chosen. We evaluated each of the five listed FS methods for each subject individually. To avoid overfitting and to limit the computational effort, the maximum number of features was fixed to 200. The data were divided into eight folds with five samples each for cross-validation. Thereof, seven folds were used for feature selection and training of the classifier. In turn, the remaining fold (i.e. one sample from each class) was used for testing, until each fold was tested once. Given five classes, chance level was at 20 percent.

To evaluate and compare the proposed feature selection methods, classification was performed by means of quadratic discriminant analysis (QDA) with diagonal covariance estimates (i.e. Naive Bayes). We chose this classifier, since it is generally known as a robust baseline classifier.

SECTION 6Results
As seen in Section 4 and as discussed within the community of affective computing, reliable induction remains an ongoing challenge in the field. However, given the recorded data set and experimental results, we believe to be able to contribute some insights to the following three questions, which we discuss in the following sections: 1) How do the different feature selection techniques perform and how many features are generally required? 2) Which type of features are most successful? Can we draw conclusions w.r.t. which features are generally better across subjects? 3) Which electrodes are mostly selected? Are there commonalities/differences regarding electrode selection across feature types?

6.1 FS Methods
Table 3 lists achieved accuracies at the optimal number of features in tabular form over subjects and FS methods. The best result for each subject is highlighted in bold font. On average, multivariate methods (mRMR, ES Λ, ES θ) perform slightly better than univariate methods (ReliefF, ES f2), as can be seen in the bottom row. In particular, the average optimal number of features using ES Λ, i.e. 44.6, is small. Less than 100 features per subject are chosen on average over all FS methods (see last column in Table 3 ). However, the optimal number of features varies strongly between subjects. It should also be noted that stable results, i.e. accuracy does not vary anymore starting at around 30 features for most subjects and methods.

Classification accuracy ranges subject-dependent from 25.0 to 47.5 percent, which correlates with the evaluation of induction, i.e. the SAM test results (r=.56). In this, however, FS using ReliefF shows almost no correlation (.05), while using ES Λ yields r=.61.

Confusion Matrices (presented in Table 3 and Fig. 4) summarize class-dependent results across subjects and methods. Noticeably, FS using ES Λ reveals problems recognizing emotion class sad, while happy and curious are mostly identified correctly. Feature selection using ES θ is more balanced across classes, i.e. the diagonal values are high compared to non-diagonal values. But with this method (and also ReliefF) emotions curious and angry are sometimes confused with each other. Most stable and evenly distributed results are obtained from feature selection using mRMR. Overall, emotion happy is generally better recognized than emotion sad. Between subjects, we see a high variance of emotions well recognized. Subjects 3, 12, and 13, for example, show good separability of only one or two emotions from the rest (see darker squares in confusion matrices). A more even separability of classes is given for subjects 4, 6, 10, and 15. Emotion quiet shows smallest variance in recognition rates across subjects.

6.2 Feature Usage
To investigate which features are superior to others, i.e. have been selected more frequently by FS methods and performed more successfully in emotion classification, we compute the relative frequency of each of the 162 feature types. In this, we create a histogram of feature occurrence given the features selected at the optimal number of features for each subject and FS method. Then, we normalize each bin of the histogram by dividing the occurrence of a feature type by the number of features belonging to this feature type (e.g. 64 in the case of feature type max α band powers, i.e. one feature for every channel) to account for random assignment of features (this is important since combinations of features can not be associated with a single electrode and, thus, are each considered one type of features). We further weighted these relative frequencies to account for the following: 1) when averaging across FS methods, multiply each relative frequency with the accuracy achieved to account for different success rates of FS methods, and 2) when averaging across subjects, multiply each subject’s relative frequency with the SAM correlation coefficient to account for general reliability of the data. Given this statistic, we assume that valuable features score higher than features not important for successful emotion recognition. Fig. 1 illustrates the weighted relative occurrence of each feature type together with the average and standard deviation of subgroups of features. In general, commonalities/differences between subjects of different induction and accuracy level are not very apparent. The tendencies visible from the data are discussed in the following:


Fig. 1.
Feature type usage is measured by weighted relative occurrence scores (gray bars). Mean (solid horizontal lines) and variance (dashed vertical lines) are shown for each subgroup of feature types. Feature types computed from certain frequency bands as well as individual feature types discussed in Section 6.2 are indicated below the figure.

Show All


Fig. 2.
Schematic characterisation of data processing from trial-wise recording to feature extraction.

Show All


Fig. 3.
The emotion induction protocol for each trial consisting of the set of IAPS pictures, prolonged recording time, SAM test, and a neutral picture.

Show All

Fig. 4. - 
Confusion matrices summarizing the targeted (y-axis) and predicted (x-axis)
 emotion class are depicted for each subject individually.
Fig. 4.
Confusion matrices summarizing the targeted (y-axis) and predicted (x-axis) emotion class are depicted for each subject individually.

Show All

On the one hand, the most frequently selected feature group is HOC features, including a tendency to favor features with large values for k. Also, features measuring complexity of the time sequence, i.e. fractal dimension, NSI, and Hjorth’s Complexity, score well. The most valuable feature type among the combinations of electrodes are the rational asymmetry features. Other (sub)-groups, such as HHS and HOS bicoherence features, yield high mean values on the weighted relative occurrence, too, but show a higher variance than the HOC group. This means that particular feature types of this group are more valuable than others.

Taking a closer look at the feature types reveals that especially features computed from frequency bands β and γ are successfully selected more often than other bands (bars marked darker in Fig. 1). This holds for feature groups STFT, HOS, HHS and DWT db4 features REE and LREE.

On the other hand, results suggest that feature subgroups such as HOS squared bispectra and DWT db4 are less efficient for emotion recognition from EEG, since their weighted relative occurrence scores are low. Feature ALREE apparently has no advantage over LREE, since it has never been selected. Both the prevalent frequency band powers and equal sized bins show relatively low scores on average.

The group of statistical features affords a differentiation between feature types, since the proposed statistics are very different from each other. Except for the first difference of the time series, none reveals a very high score on the weighted relative frequencies. A special note regarding the high score of the statistical feature power of signal should be given: This does not mean to be a highly valued feature, but has to be explained by a very high occurrence when using FS method ES θ: the first feature is selected in case of equal ranking scores or no measurable improvement through any feature (saturation of selection criterion).

6.3 Electrode Usage
Based on the findings of Section 6.2, we investigated also electrode usage of feature types and groups that can be associated with a single electrode, i.e. all except MSCE and asymmetry features. In this section, we discuss the electrode usage of the six features identified as most important. These are first difference, complexity features, the group of HOC features and bicoherence features, and selected (threshold at .03) β, and γ features (i.e. from STFT, bispectrum, absolute and squared bicoherence, and REE(γ)/LREE(β)) as shown in Fig. 5. Here, the darkness of each electrode depicts the number of its occurrences within a feature group, where the scale is normalized by the total number of feature occurrences in the group, so that the scores of all 64 electrodes sum to one (since the first plot only captures electrode selections of one feature type, i.e. 64 features, we expect the results to be more pronounced here).


Fig. 5.
Electrode usage of six feature (sub-)groups is indicated by the darkness of the circle around each electrode location. Rows correspond to letters, columns to numbers of the extended 10- 20 system.

Show All

For the feature type first difference Undefined control sequence \xibi the prominent electrodes are P9 and P5 with some importance given to P7, P1, and FC2. The three complexity features Hjorth’s complexity, NSI, and fractal dimension are mostly drawn upon from electrode locations CP2, CP1, P1, and F5. For the feature group of HOC the important electrodes are CP1 and CP2, but also some right hemisphere locations like T8 and P10 are frequently used. Features for bicoherence are mostly selected from C2, FC2, and Cz. A cluster of electrodes selected for good γ features forms around CP5, CP3, CP1, P5, and P3. Additionally, location CP6 is selected often. Not quite as clear are location of good β features. Prominent electrodes are C2, Cz, P9, F7, CP5, and CP3. Generally, electrodes located over the parietal and central-parietal lobe are favored over occipital and frontal lobes.

SECTION 7Discussion
In this section, we interpret and discuss the same underlying questions posed above using results presented in Section 6.

The higher average accuracy of multivariate methods points towards interactions of features that only multivariate FS methods can actively detect and capitalize upon. High variance in measured induction success as well as emotion recognition accuracy between subjects, however, reassures the need for ongoing efforts towards a more reliable ground truth.

Spectral power in nearly all frequency bands is used in most studies, which contrasts with its low performance scores in our study. Among these features, Wang et al. found frontal and parietal locations typical under their TOP-30 features [12]. However, features related to β and γ bands seem more valuable if obtained by different (mostly more complex) extraction methods which have found only little attention so far. Moreover, our study revealed that electrode locations over parietal and centro-parietal lobes can be of advantage for these bands. Although the difference between the two STFT feature subgroups is small, the trend of equal sized bins scoring higher than frequency bands is in accordance with earlier argumentation by Reuderink et al. [26], that emotional responses can be visible in small frequency changes. The finding of β and γ bands from HHS being valuable features is in line with findings of Hadjidimitriou and Hadjileontiadis, who found these bands to discriminate best between liking and disliking judgements [8].

The usage of several complexity measures (especially FD) by many of the existing studies is endorsed by our results. The suggested electrode locations over the left anterior scalp as well as central-posterior locations are in line with those suggested in a study by Ansari-asl et al. [13].

A promising, yet little considered feature is HOC. Contrary to the frontal electrode used by Petrantonakis and Hadjileontiadis, centro-parietal locations might enhance this feature type.

To date, frontal regions are of high importance. They are used in almost every study, especially when only a few electrodes are hand-selected. On the one hand, our results opposing this trend might be biased since we did not use artifact removal in this study. On the other hand, recent studies reach consensus that frontal alpha asymmetry primarily reflects approach versus withdrawal motivation rather than the level of valence [62]. This may imply that locations other than the anterior scalp are better to differentiate multiple, discrete emotional states.

Finally, rational asymmetry seems to have advantages over differential asymmetry, although the latter is used more frequently in the surveyed studies.

When interpreting these results it should be noted that due to the high number of tested features compared to the low number of samples, we could not validate statistical significance of our results.

SECTION 8Conclusion
Different sets of features and electrodes for emotion recognition from EEG have been suggested by over 30 studies reviewed in this paper. We presented a systematic analysis and first qualitative insights comparing the wide range of available feature extraction methods using machine learning techniques for feature selection. Multivariate feature selection techniques performed slightly better than univariate methods, generally requiring less than 100 features on average. We also investigated which type of features are most promising and which electrodes are mostly selected for them. Advanced feature extraction methods such as HOC, HOS, and HHS were found to outperform commonly used spectral power bands. Results suggest preference to locations over parietal and centro-parietal lobes.

As shown by Mühl et al., different induction modalities result in modality-specific EEG responses [1]. This possibly limits our results to the induction modality of visual stimuli. We expect that repeated experiments of the above procedure on other databases will help to reach a consensus on this topic.

Another compelling question to answer in the future is whether certain types of features work better together than by themselves.

Due to the strong interpersonal variance, multi-individual studies are necessary in order to make emotion recognition from EEG feasible for applications. Therefore, more baseline data sets like the recently published DEAP [63] need to be made available to the community in order to compare approaches.