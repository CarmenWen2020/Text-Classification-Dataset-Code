The goal of this work is to present a systematic solution for RGB-D salient object detection, which addresses the following three aspects with a unified framework: modal-specific representation learning, complementary cue selection, and cross-modal complement fusion. To learn discriminative modal-specific features, we propose a hierarchical cross-modal distillation scheme, in which we use the progressive predictions from the well-learned source modality to supervise learning feature hierarchies and inference in the new modality. To better select complementary cues, we formulate a residual function to incorporate complements from the paired modality adaptively. Furthermore, a top-down fusion structure is constructed for sufficient cross-modal cross-level interactions. The experimental results demonstrate the effectiveness of the proposed cross-modal distillation scheme in learning from a new modality, the advantages of the proposed multi-modal fusion pattern in selecting and fusing cross-modal complements, and the generalization of the proposed designs in different tasks.

Access provided by University of Auckland Library

Introduction
The availability of depth sensors (e.g., in Microsoft Kinect and Intel RealSense) allows the RGB-based computer vision systems with more accurate and robust performance, therefore nurturing a wide range of applications (Han et al. 2013; Gupta et al. 2015; Camplani et al. 2015; Peng et al. 2014). Complementary to RGB data, the synchronized depth information carries additional geometry cues, which are robust to appearance changes, illumination varying, and subtle background movements. Hence, the joint inference with RGB and depth information could benefit various computer vision tasks (Gupta et al. 2015; Camplani et al. 2015). A good example is salient object detection (SOD) (Peng et al. 2014), which aims at identifying the most visually attractive objects in a scene and has been widely applied to benefit other computer vision tasks, such as image retrieval (Shao and Brady 2006) and object tracking (Mahadevan et al. 2013). Traditional RGB-based SOD methods (Harel et al. 2007; Yang and Yang 2017; Cheng et al. 2015; Borji et al. 2015) are likely to fail when the salient object and background present similar appearance. In this case, the paired depth map, which supplies auxiliary saliency cues, opens up a new opportunity to solve this challenge.

By fusing the RGB and depth data, a rich amount of methods (Fu et al. 2015, 2017; Cong et al. 2017, 2018; Chen and Li 2018; Feng et al. 2016; Lang et al. 2012; Desingh et al. 2013; Ciptadi et al. 2013; Ju et al. 2014; Cong et al. 2016; Song et al. 2017; Niu et al. 2012; Qu et al. 2017; Han et al. 2017) have been proposed for RGB-D SOD. Earliest works (Feng et al. 2016; Ju et al. 2014; Song et al. 2017; Niu et al. 2012; Peng et al. 2014; Lang et al. 2012; Desingh et al. 2013; Ciptadi et al. 2013) focus on crafting RGB-D features with prior knowledge, which cannot be well generalized to all scenarios. Recently, the success of deep learning techniques (Krizhevsky et al. 2012) motivates the prosperity of the deep learning-based RGB-D SOD community. A popular architecture is the â€œtwo-streamâ€ Convolutional Neural Network (CNN) (Qu et al. 2017; Han et al. 2017; Wang et al. 2015; Park et al. 2017; Xu et al. 2017; Zhu et al. 2016), in which, the depth stream is typically trained from scratch or initialized with the well-trained RGB CNN, and the paired RGB and depth images work independently and then aggregate in an early or late stage by direct feature concatenation or summation.

Although these networks achieve encouraging boost over handcrafted feature-based methods, three key problems limiting the cross-modal fusion sufficiency remain:

(1)
With the scarcity of labeled depth data and the cross-modal gap between RGB and depth, the traditional finetuning scheme for depth data typically ends with insufficient learning.

(2)
Without carefully selecting the real complementary cues, the direct combination strategy in previous two-stream networks is confronted with ambiguous and uninformative fusion.

(3)
With a single fusion layer as done in Qu et al. (2017), Han et al. (2017), Wang et al. (2015) and Zhu et al. (2016), it is unlikely to explore both the contextual and spatial cross-modal complementarity existed in multiple levels.

Fig. 1
figure 1
Our pipeline for RGB-D SOD

Full size image
However, for these questions in studying the complementarity in RGB-D data, a systematic solution still remains an open issue. In this work, we initiatively propose a principled framework to address these problems systematically. As illustrated in Fig. 1, we argue that an ideal RGB-D fusion system should achieve the following three goals:

(1)
Learn: In many scenarios, the specialists in one modality (e.g., geometry cues in the depth map) are missing in its counterpart (i.e., the RGB image). Accordingly, an informative RGB-D combination firstly calls for carefully extracting rich features from each modality. However, we are often confronted with an imbalanced amount of labeled data prepared for each modality. Thus, the challenge lies in how to learn rich representations from the new modality with limited labeled data.

(2)
Select: Given the heterologous representations from two modalities, an informative multi-modal fusion should be attentive to the real complementary components. This awareness enables the cross-modal fusion to select complementary representations and ignore the redundant and noisy ones.

(3)
Fuse: The last step is to fuse the selected cross-modal information sufficiently. The complements between RGB and depth data include high-level contexts and low-level spatial details simultaneously. Consequently, a sufficient RGB-D fusion process is required to combine both the low-level and high-level cross-modal complements for joint inference.

Our solution. Considering the data deficiency in the depth modality and that the RGB modality is well-learned with large-scale labeled data, we regard the RGB SOD network as the teacher and the new depth CNN as the student, and transfer the knowledge from the teacher to facilitate the learning of the student. Specifically, we use the progressive side outputs (namely, coarse-to-fine saliency maps in a top-down path) from the teacher as hierarchical supervision to train each student layer correspondingly. The student is forced to learn feature hierarchies, level-specific inference, and cross-level combinatons by approaching those progressive predictions in the teacher. These learned feature hierarchies are complementary to the representations in the source modality, and can be further improved by finetuning. We name our scheme hierarchical cross-modal distillation (HCD). Compared to using the ground-truth mask as supervision directly, our HCD method has two distinguished benefits: (1) The evolutionary supervisions from the teacher are easier to match for the student. Specifically, for the student, it is difficult for its deep layer to capture accurate object details. Also, it is powerless for its shallow layer to achieve correct salient object localization towards the ground-truth. In the HCD, the teacher assigns an easier task for the student. The teacher guides the learning of each layer in the student network more explicitly since its hierarchically coarse-to-fine supervisions define level-specific learning objective for the student, namely, the deep layer is only responsible to localization, and the shallow layer is merely in charge of detail refinement. (2) The progressive enhancement in the teacher reveals cross-level combination, which is available knowledge but is inaccessible in the ground-truth. For more analyses of the motivation and experimental verifications of the HCD, please refer to Sects. 3.1.1 and 4.3, respectively.

For the â€œSelectâ€ stage, we explicitly formulate the cross-modal complements with a residual function, and the goal of selecting cross-modal features is formulated as asymptotically approximating the residual. Different from the direct concatenation of multi-modal features, such a cross-modal residual connection is more likely to expose the desired complement.

Towards a sufficient multi-modal fusion, we design a top-down fusion pattern, in which cross-modal features are combined in each level and the integrated RGB-D representations, in turn, guide the inference of shallower layers. The resulting network demonstrates rich multi-scale RGB-D representations for joint inference and consequently, the saliency map is improved progressively from coarse to fine.

Our preliminary study (Chen and Li 2018) discussed the above-mentioned â€œSelectâ€ and â€œFuseâ€ problems. In this work, we extend (Chen and Li 2018) by additionally investigating the problem â€œLearnâ€ with designing the HCD method and forming a systematic framework.

In summary, this work has the following five contributions:

(1)
We originally propose a comprehensive analysis and a systematic solution for the key issues in fusing RGB-D data, which have great potential to benefit future works on the multi-modal fusion domain.

(2)
We design a cross-modal distillation scheme, which favors better learning of a new modality with limited labeled data and allows weakly-supervised learning for the new modality without using ground-truth.

(3)
The residual function is initiatively designed to explicitly capture the cross-modal complement, which reduces the fusion ambiguity, and contributes to informative fusion.

(4)
We propose a progressively top-down cross-modal cross-level fusion topology, which is the first multi-level RGB-D fusion network in the SOD community. The inference path comes to be aware of modal-specific and level-specific contributions, and combines cross-modal cross-level complements sufficiently.

(5)
This work achieves state-of-the-art performance on public benchmarks consistently and shows decent generalization on semantic segmentation.

Organization: In Sect. 2, we introduce related works on RGB-D fusion (Sect. 2.1), other RGB-D fusion tasks (Sect. 2.2), and cross-modal transfer learning (Sect. 2.3). In Sect. 3, we detail our systematic solution for RGB-D SOD. The designs to learn, select, and fuse are presented in Sects. 3.1â€“3.3, respectively. Section 4 reports the experiments, including datasets and evaluation metrics (Sect. 4.1), training settings (Sect. 4.2), and the verification on our two central contribution, i.e., the HCD scheme (Sect. 4.3) and the CA-Fuse block (Sect. 4.4). The comparison to SOTA is exhibited in Sect. 4.5 while Sect. 4.6 demonstrates the generalization of the proposed HCD scheme and CA-Fuse block in semantic segmentation. Section 5 makes a conclusion on this work.

Related Work
RGB-D SOD
A large body of earlier RGB-D SOD works focuses on designing RGB-D features or combining unimodal predictions, which are termed as â€œfeature fusionâ€ and â€œresult fusionâ€ solutions respectively. A common wisdom in crafting depth-induced saliency cues is that human fixations prefer closer depth ranges (Lang et al. 2012). This prior is useful but is easily confused by nearer backgrounds. On the other hand, two regions, sharing the same depth may be in different contexts and should be differentiated. Considering the scene structures, Ju et al. (2014) use relative depth instead of the absolute one for evaluation and propose the anisotropic center-surround difference for measurement. Desingh et al. (2013) adopt the global-contrast method (Cheng et al. 2015) used in the RGB-induced saliency detection with depth values as inputs. A similar framework is also used in Fan et al. (2014). Different from these global-contrast paradigms, Feng et al. (2016) propose to measure the distinction of one region in a local context by estimating the proportion of the object popping out the background. Peng et al. (2014) then propose a hybrid framework that incorporates global-contrast and local-contrast strategies. Song et al. (2017) segment the RGB-D pair into superpixels with different scales to obtain multi-scale representations.

Despite the effectiveness of these handcrafted features, they lack high-level reasoning and suffer from limited generalization ability. To address this limitation, recent works resort to deep learning techniques. Qu et al. (2017) combine the low-level features from RGB and depth as a joint input to train a CNN from scratch. Compared to the previous works based on handcrafted features, this method achieves encouraging gains. However, it may be difficult to fully leverage the power of CNNs by feeding the crafted features rather than the raw image pair as inputs. In contrast, Han et al. (2017) design a â€œtwo-streamâ€ late fusion architecture, in which the RGB and depth images are learned separately and their deep representations are combined by a joint fully connected layer for collaborative decision. Compared to Qu et al. (2017) and Han et al. (2017) achieves large improvement due to the better combination of high-level contexts. Despite this, the low-level cross-modal complements are under explored in Han et al. (2017) and the resulting saliency maps are severely blurred. In summary, both Qu et al. (2017) and Han et al. (2017) fail to combine the multi-scale cross-modal complements simultaneously. Recently, Chen et al. (2018) propose a multi-branch fusion network with fully connected layers for global reasoning and dilated convolutional layers for capturing local details. The results from two branches are combined by direct summation. However, the network is not a fully convolutional one and fails to utilize the information from all layers for joint inference.

Recent works (Chen and Li 2019; Zhao et al. 2019; Piao et al. 2019; Li et al. 2020a) in the RGB-D SOD community also adopt the multi-level fusion architecture. Concretely, Zhao et al. (2019) use contrast prior to enhance depth information and extends the progressive multi-modal fusion pattern in Chen and Li (2018) to a fluid pyramid integration manner to strengthen the cross-level connection. Piao et al. (2019) also adopt the residual function to combine the cross-modal complements at each level and the fused multi-scale multi-modal representation is weighted by depth information and followed by attention modules to select informative features for inference. In Li et al. (2020a), the authors design a depth-weighted combination block to enhance RGB features at each level. To improve fusion sufficiency, the work Li et al. (2020b) enhances cross-modal and cross-scale interactions by designing cross-modal cross-scale weighting modules. In Zhao et al. (2020a), the authors propose to insert gate units in each level to adaptively transmit informative cues from the encoder to the decoder. Different from the popular two-stream patterns, Zhao et al. (2020b) propose a single-stream network that holds considerably fewer parameters and inference time yet better performance than the previous two-stream architectures. Zhou et al. (2020) presents a comprehensive survey on RGB-D SOD methods and benchmark datasets, where the network architectures, fusion strategies, and the efficacy of attention mechanism on RGB-D SOD are compared and analyzed. Although these works achieve great improvement on the multi-modal fusion architecture to improve multi-modal fusion sufficiency, the problem of how to better learn and infer from the new modality (depth) with insufficient data is under discussed. Different from the 3D SOD domain, Zhang et al. (2019), Piao et al. (2020) and Zhang et al. (2020) focus on light field SOD, which, although share some similarities with RGB-D SOD, is a more difficult 4D SOD task that requires specific designs to integrate the supplements from the additional focal slices for SOD. In Zhang et al. (2019), a memory-oriented decoder is customized to store high-level semantics, which are then used to guide the top-down fusion process adaptively. In Piao et al. (2020), an asymmetrical two-stream architecture is proposed to transfer the focusness knowledge to a computational efficient network for the application of mobile devices. The LFNet (Zhang et al. 2020) proposes a refinement module to explore the complementarity between each focal slice and the all-focus image and uses the attention mechanism to select and integrate the refined light field features.

RGB-D Fusion Networks for Other Tasks
Deep learning techniques especially CNNs are also popular solutions for other RGB-D systems. Aiming at incorporating geometry information into CNN in an efficient manner, Wang and Neumann (2018) innovatively introduce the pairwise depth similarity to the network propagation by designing depth-aware convolution and depth-aware pooling. Considering the cross-modal data discrepancy and the wide noises in depth, more works resort to the â€œtwo-streamâ€ solution to extract features from each modality seperately. In Wang et al. (2015) and Zhu et al. (2016), a multi-modal fusion layer is designed to combine the decisions from RGB and depth by modeling their consistency and independency. More recent works (Park et al. 2017; Cheng et al. 2017; Lin et al. 2017a, b; Hazirbas et al. 2016; Li et al. 2019; Du et al. 2019; Zeng et al. 2019; Hou et al. 2019) also investigate the cross-modal complementarity in multiple levels. Speciafically, the work Hazirbas et al. (2016) presents an encoder-decoder architecture for RGB-D semantic segmentation and the RGB and depth features, extracted from two branches separately in the encoder, are level-wise combined into the RGB features as the network goes. Differently, Park et al. (2017) perform multi-modal fusion in the inference stage with level-wise cross-modal feature combination and progressive refinement. Li et al. (2018) append a recurrent attention module to the fused multi-modal features to capture a joint global context to precisely perceive the most relevant regions. Although various two-stream multi-scale RGB-D fusion networks have been proposed in previous works, they mainly focus on enriching cross-modal and cross-level fusion connections to improve RGB-D fusion sufficiency. Despite the prosperity and variation of the multi-scale RGB-D fusion topologies, they typically adopt undifferentiated element-wise summation (e.g., Park et al. 2017; Lin et al. 2017a; Li et al. 2019; Du et al. 2019) or channel concatenation (e.g., Zeng et al. 2019; Hou et al. 2019) for the problem of how to combine cross-modal features. Our work is distinct from these related works from two main views: (1) Using the residual function to explicitly model the desired cross-modal complements and formulating the problem of incorporating cross-modal complements as asymptotically approximating the residual. The ablation study in Sects. 4.4.1 and 4.6.1 shows its large improvement over undifferentiated combination manners for both salient object detection and semantic segmentation, demonstrating its advantages in boosting the cross-modal fusion adaptability. (2) We propose a cross-modal distillation method to solve the problem of how to better learn the new modality to enrich its complementary information, which is unexplored in these discussed works.

Cross-Modal Transfer
The transfer learning community mainly solves the domain adaptation problem in the same modality (Hinton et al. 2015; Romero et al. 2014; Li et al. 2017; Huang and Wang 2017). In Hinton et al. (2015), Hinton et al. use the final soft outputs of the large well-trained teacher network as targets of the small student network. Subsequent works Romero et al. (2014), Li et al. (2017) and Huang and Wang (2017) extend this idea by encouraging the student to mimic the deep representations from the teacher. Our topic lies in the cross-modal transfer problem, which is more difficult due to the severe cross-modal discrepancy. Notable works include Christoudias et al. (2010), Socher et al. (2013), Ngiam et al. (2011), Hoffman et al. (2016), Garcia et al. (2018) and Gupta et al. (2016). These works focus on the problem of either hallucinate modalities during training (Christoudias et al. 2010; Hoffman et al. 2016; Garcia et al. 2018), or learning joint embedding with a shared feature space (Socher et al. 2013; Ngiam et al. 2011). Gupta et al. (2016) generalize the idea in Romero et al. (2014), Li et al. (2017) and Huang and Wang (2017) to the cross-modal domain. This work is the most related one to our HCD scheme to date. It transfers the representations learned from the large-scale RGB data to a new modality by forcing the student features to mimic the teacher features. However, due to the cross-modal discrepancy, a considerable part of source feature maps (e.g., texture and color changes in RGB images) are inaccessible for the target modality (i.e., depth). Hence, it is too strict for the new modality to mimic the high-dimensional features from the teacher. These RGB-specific features provide uninformative and even negative supervision for the student. As a result, the student network is hard to converge especially when it is deep. In this work, the goal of the student network is relaxed to mimic the progressive side outputs in the teacher network.

Difference to Previous Works
In summary, our work differs from others in the following points: 1) In the network architecture view, our RGB-D fusion network is the earliest multi-level fusion architecture in the RGB-D SOD domain and firstly uses the residual function to model cross-modal complements. 2) In this extension version, we address a new problem of distilling knowledge from RGB data to facilitate the leaning and inference of a new modality (depth). The proposed scheme also allows decent weakly-supervised SOD from a new modality without using ground-truth. These problems are underexplored in the discussed related works. 3) Our designs also achieve large improvement on semantic segmentation, demonstrating their generalizability. Moreover, this work originally concludes a comprehensive view for the key issues in designing an RGB-D fusion system.

Fig. 2
figure 2
Visualization from each modality to show the motivation of our HCD scheme. 1st column: an input RGB-D pair; 2nd column: conv2_2 feature maps extracted from RGB and depth; 3rd column: predictions merely with conv2_2 feature maps; 4th column: the collective prediction combining conv2_2 with all deeper layers. From left to right, we notice that the cross-modal discrepancy is reduced and the learning target for the student is relaxed gradually

Full size image
Proposed Method
In this section, we sequentially detail the motivation and formulation of our designs towards how to learn representations from the new modality, select and fuse desired cross-modal complements.

Learn: Hierarchical Cross-Modal Distillation
Motivation
For cross-modal transfer learning, the core problem is how to customize suitable supervisory signals to abstract the knowledge in the source modality that can be shared for the new modality. If a excessively strict constraint is set, for example, approaching all feature maps in the teacher, it is difficult to ensure training convergence due to cross-modal difference. In contrast, an over-relaxed constraint, such as appropriating the final prediction, appears too weak to learn the shallow layers effectively. Therefore, a well-balanced knowledge distillation method should abstract sufficient modality-shared knowledge to supervise learning the student, meanwhile allows the exploration of specialists in the new modality. Intuitively, the features from two modalities, though discrepant, can make roughly consistent inference for the same task. So our primitive choice is to use the inference from each level of the teacher network to supervise learning the student. However, as observed in Lenc and Vedaldi (2015) and illustrated in the 2nd column in Fig. 2, shallow CNN features are more modal-specific. As a result, the shallow layers will hardly produce coherent inference by different modalities. Specifically illustrated in the 3rd column in Fig. 2, the shallower layers trained with RGB images are activated by texture/color changes, which are immune for the depth modality. So, using this modal-specific knowledge in the source modality as supervision is likely to obtain negative transfer.

To relieve negative transfer arising from the cross-modal discrepancy in shallow CNN layers, we further consider that with the global guidance from the deep layers, the cross-modal discrepancy between the side-outs (4th column in Fig. 2) in shallow layers, which combine the information from deeper layers, can be effectively reduced, with respect to the individual inference in each level. For example, some regions activated by local distinguished color will be suppressed with the help of global contexts from deep layers. In this way, the shallow layers in the student network are required to solely highlight the local details of the salient object, while the negative supervision are no longer encouraged to approach. To this end, we propose the HCD scheme, in which the student is forced to progressively approach the combined side-outs from the teacher. Compared to using feature maps or final predictions as supervision, this design presents several distinguished advantages:

(a)
Compared to the feature-level transfer method, the proposed HCD reduces negative transfer by relaxing the cross-modal discrepancy and allows more flexibility for the student network to explore specialists in the new modality.

(b)
Compared to using the final prediction in the teacher as supervision to train the student, which is difficult to optimize multiple levels jointly for a deep student network, in our proposed HCD scheme, the hierarchical supervision signals from the teacher decouple the joint learning of multiple layers. The progressive enhancement inferred from the teacher defines level-specific optimization objectives for each level in the student independently. The customized supervision for each level simplifies the training process for a deep student network. It also reveals cross-level collaborations, which are also pretty informative supervisory signals for the student.

These superiorities are experimentally verified in Sects. 4.3 and 4.6.

Fig. 3
figure 3
Architecture of the HCD scheme. Adaptation layers in each level are omitted for simplification. For depth data, we follow the previous approach (Gupta et al. 2014) to encode it as 3-channel HHA (horizontal, aboveground height and surface normal angle) representations

Full size image
Formulation
Given a new modality îˆ¹ğ‘‹ğ· with unlabeled training samples î‰„ğ·, our goal lies on learning rich features from î‰„ğ· by transferring knowledge from a different modality îˆ¹ğ‘‹ğ‘… with large-scale labeled images.

Denote the K layered representations Î¨={ğœ‘ğ‘–ğ‘…,ğ‘–=1,â€¦,ğ¾}, where ğœ‘ğ‘–ğ‘… denotes the representation in the ith layer for the modality îˆ¹ğ‘‹ğ‘…. Based on ğœ‘ğ‘–ğ‘…, a reliable classifier ğ›¿ğ‘–ğ‘Ÿ is learned for level-specific inference ğ‘ƒğ‘–ğ‘…=ğ›¿ğ‘–ğ‘Ÿ(ğœ‘ğ‘–ğ‘…).

Now, suppose we have a dataset îˆ°ğ‘Ÿ,ğ‘‘ which contains paired images from îˆ¹ğ‘‹ğ‘… and îˆ¹ğ‘‹ğ·. We implement our idea of relaxing the supervision for shallow student layers by densely skip-connecting the inferences from the deeper layers to all lower layers to generate collaborative side outputs. As shown in Fig. 3, the cross-modal distillation network contains two parts: (a) Unimodal cross-level connections to generate progressive predictions, which are described with dotted lines. The teacher and student share same cross-level connections; (b) Cross-modal connections, which are illustrated by solid lines. The inference of a deep layer will be combined with all shallower sideouts (e.g., in the teacher net, ğ‘ƒ6ğ‘… will be fed back to combine ğ‘ƒ5ğ‘…, ğ‘ƒ4ğ‘…, ğ‘ƒ3ğ‘…, ğ‘ƒ2ğ‘…). Formally,

ğ‘ƒÌƒ ğ‘–ğ‘…=â§â©â¨âªâªğ°ğ‘–ğ‘…ğ‘ƒğ‘–ğ‘…+âˆ‘ğ‘˜=ğ‘–+1ğ¾ğ°Ìƒ ğ‘˜ğ‘…,ğ‘–ğ‘ƒÌƒ ğ‘˜ğ‘…,ğ‘–=1,â€¦,ğ¾âˆ’1ğ‘ƒğ‘–ğ‘…,ğ‘–=ğ¾
(1)
where ğ°ğ‘–ğ‘… and ğ°Ìƒ ğ‘˜ğ‘…,ğ‘– are learnable weights for ğ‘ƒğ‘–ğ‘… and the combined side-out ğ‘ƒÌƒ ğ‘˜ğ‘… from the kth level.

Similarly for the counterpart modality îˆ¹ğ‘‹ğ·:

ğ‘ƒÌƒ ğ‘–ğ·=â§â©â¨âªâªğ°ğ‘–ğ·ğ‘ƒğ‘–ğ·+âˆ‘ğ‘˜=ğ‘–+1ğ¾ğ°Ìƒ ğ‘˜ğ·,ğ‘–ğ‘ƒÌƒ ğ‘˜ğ·,ğ‘–=1,â€¦,ğ¾âˆ’1ğ‘ƒğ‘–ğ·,ğ‘–=ğ¾
(2)
where ğ‘ƒğ‘–ğ·=ğ›¿ğ‘–ğ‘‘(ğœ™ğ‘–ğ·), ğ›·={ğœ™ğ‘–ğ·,ğ‘–=1,â€¦,ğ¾} denotes the K layered representations, ğ›¿ğ‘–ğ‘‘ is the learned classifier, ğ°ğ‘–ğ· and ğ°Ìƒ ğ‘˜ğ·,ğ‘– are the weights for ğ‘ƒğ‘–ğ· and ğ‘ƒÌƒ ğ‘˜ğ· from the deeper levels, respectively.

Our scheme for learning sufficient modal-specific representations from images in îˆ¹ğ‘‹ğ· is to train the representations ğ›· such that the combined side-out ğ‘ƒÌƒ ğ‘–ğ·(ğ¼ğ‘‘) matches the one ğ‘ƒÌƒ ğ‘–ğ‘…(ğ¼ğ‘Ÿ) inferred from its paired image in îˆ¹ğ‘‹ğ‘…. Therefore, we measure the discrepancy between the combined side-outs from two modalities with a suitable loss g:

ğ¿ğ»ğ¶ğ·=âˆ‘{ğ¼ğ‘Ÿ,ğ¼ğ‘‘}âˆˆğ·ğ‘Ÿ,ğ‘‘âˆ‘ğ‘–=1ğ¾ğ‘”(ğ‘ƒÌƒ ğ‘–ğ‘…(ğ¼ğ‘Ÿ),ğ‘ƒÌƒ ğ‘–ğ·(ğ¼ğ‘‘))
(3)
In our experiments, we adopt the L2 loss ğ‘”(ğ‘¥,ğ‘¦)=â€–ğ‘¥âˆ’ğ‘¦â€–22 for measuring the distance. By minimizing ğ¿ğ»ğ¶ğ·, the student network is encouraged to learn rich feature hierarchies and informative cross-level combinations for inference.

Table 1 Parameters of the adaptation layers
Full size table
Implementation
For a fair comparison with the previous works based on the VGG network, we also adopt the VGG-16 model as the backbone for both modalities and the detailed HCD architecture is illustrated in Fig. 3. The trunk inherits five convolutional blocks from the original VGG model. We add a new 512 13Ã—13 convolutional layer for perceiving precedent features globally to enhance the localization ability. Then the strategy similar to Hou et al. (2017) is leveraged to generate side outputs for each level. Specifically, the last layer in each convolutional block (e.g., Conv4_3 and Conv2_2) will be appended with adaptation layers to the backbone. The details of the adaptation layers are shown in Table 1. We firstly use saliency labels Y to train the teacher network with the architecture shown in the left of Fig. 3. Concretely, the adapted features are used to infer level-specific saliency ğ‘ƒğ‘–ğ‘… via a 1Ã—1 convolutional layer. Considering that it may be difficult for the first convolutional block to provide reliable cues, we do not involve it into inference for the teacher, student and the final RGB-D fusion network. Following Eq. (1), ğ‘ƒğ‘–ğ‘… will be combined with the predictions from deeper layers to generate the side-out ğ‘ƒÌƒ ğ‘–ğ‘…. Accordingly, the loss function for the teacher network consists of the distance between the ground-truth mask and each side-out as well as the joint prediction combining all the side-outs as another constraint term:

ğ¿ğ‘‡ğ‘’ğ‘ğ‘=âˆ‘ğ‘–=2ğ¾ğ‘‘(ğ‘ƒÌƒ ğ‘–ğ‘…,ğ˜)+ğ‘‘(âˆ‘ğ‘–=2ğ¾ğ°Ìƒ ğ‘–ğ‘…ğ‘ƒÌƒ ğ‘–ğ‘…,ğ˜),
(4)
where ğ°Ìƒ ğ‘–ğ‘… is the weight for ğ‘ƒÌƒ ğ‘–ğ‘… and ğ¾=6.

We adopt the cross-entropy loss for optimization when training the teacher network:

ğ‘‘(ğ‘ƒÌƒ ,ğ˜)=ğ˜logğ‘ƒÌƒ +(1âˆ’ğ˜)log(1âˆ’ğ‘ƒÌƒ ).
(5)
After we learned the teacher stream with saliency labels, we freeze the teacher stream and use RGB-D pairs to train the student stream via the HCD architecture. The student stream can be further improved by finetuning with saliency labels.

Fig. 4
figure 4
a Architecture of the RGB-D SOD network. b Traditional cross-modal fusion scheme: direct concatenation of cross-modal cross-level features without the proposed cross-modal residual connection. c Details of our complementarity-aware fusion (CA-Fuse) block

Full size image
Select: Complementarity-Aware Cross-Modal Fusion
Having learned rich representations from each modality, the following step is to select the complementary ones for informative multi-modal fusion. To this end, we propose the complementarity-aware cross-modal fusion (CA-Fuse) block to select cross-modal complements in an explicit manner. Figure 4a shows the architecture of the multi-modal fusion network and Fig. 4c exemplifies the CA-Fuse block in the mth level. Formally, the adapted deep features from the RGB and depth streams are denoted as ğ¹ğ‘šğ‘… and ğ¹ğ‘šğ·, respectively. A 1Ã—1 convolutional layer, acting as a selector, is appended after ğ¹ğ‘šğ· to select complementary information to enhance the RGB features via a cross-modal skip connection ğ¹Ìƒ ğ‘šğ‘…=ğ¹ğ‘šğ‘…+â„œ1(ğ¹ğ‘šğ·). It suggests that the target of using â„œ1(â‹…) to select complementary features from ğ¹ğ‘šğ· can be posed as approximating the residual part, i.e., ğ¹Ìƒ ğ‘šğ‘…âˆ’ğ¹ğ‘šğ‘… equivalently. Such a reformulation exposes the cross-modal complements explicitly and eases the incorporation. If ğ¹ğ‘šğ‘… is competent for inference, the solver can simply adjust the residual towards zero. Otherwise, â„œ1(â‹…) will be pushed to distill complements from ğ¹ğ‘šğ· to aid ğ¹ğ‘šğ‘… for better prediction. To further encourage the determination of the residual part, the enhanced features ğ¹Ìƒ ğ‘šğ‘… will infer saliency ğ‘ƒÌƒ ğ‘šğ‘…âˆ’ğ‘ and then compared to the ground-truth ğ˜. In minimizing the distance ğ¿ğ‘šğ‘… between ğ‘ƒÌƒ ğ‘šğ‘…âˆ’ğ‘ and ğ˜, ğ¹Ìƒ ğ‘šğ‘… as well as â„œ1(ğ¹ğ‘šğ·) will be optimized, thereby capturing the most complementary cues from the paired modality. A symmetric residual connection is also introduced from ğ¹ğ‘šğ‘… to ğ¹ğ‘šğ· to capture the complements from the RGB stream to enhance the depth features. Then these enhanced features are concatenated for joint prediction.

Table 2 Parameters of the cross-level transition layers
Full size table
Fuse: Progressively Top-Down Cross-MODAL Cross-Tevel Fusion Pattern
The last question regarding how to fuse the cross-modal complements sufficiently is solved by a progressively top-down fusion pattern, in which the cross-modal features are selected and combined by the CA-Fuse block in each level and the incorporated multi-modal features are selectively transmitted to the adjacent shallower layer for cross-level combination. Concretely, the RGB-D representations ğ¹Ìƒ ğ‘š+1ğ‘š,ğ‘…ğ·, selected from the m+1 layer ğ¹Ìƒ ğ‘š+1ğ‘…ğ· by a transition layer (detailed parameters are illustrated in Table 2), will be upsampled by a fixed de-convolutional layer and then concatenated with ğ¹Ìƒ ğ‘šğ‘… and ğ¹Ìƒ ğ‘šğ· as a cross-level cross-modal representation community ğ¹Ìƒ ğ‘šğ‘…ğ·, which will be responsible for the prediction of the mth CA-Fuse block by:

ğ‘ƒğ‘šğ‘…ğ·=ğ›¿ğ‘šğ‘Ÿğ‘‘(ğ¹Ìƒ ğ‘šğ‘…ğ·),
(6)
where ğ›¿ğ‘šğ‘Ÿğ‘‘ are the parameters for fusing cross-modal cross-level features and performing joint inference. Similar to the unimodal streams in the HCD architecture, another cross-level fusion strategy of skip-connecting the side-outs densely is also adopted in the multi-modal fusion network and implemented by the backward prediction dense-connection (BPDC) module. The combined side-out is denoted as

ğ‘ƒÌƒ ğ‘šğ‘…ğ·=â§â©â¨âªâªğ°ğ‘šğ‘…ğ·ğ‘ƒğ‘šğ‘…ğ·+âˆ‘ğ‘˜=ğ‘š+1ğ¾ğ°Ìƒ ğ‘˜ğ‘…ğ·,ğ‘šğ‘ƒÌƒ ğ‘˜ğ‘…ğ·,ğ‘š=2,â€¦,ğ¾âˆ’1ğ‘ƒğ‘šğ‘…ğ·,ğ‘š=ğ¾
(7)
where ğ°ğ‘šğ‘…ğ· and ğ°Ìƒ ğ‘˜ğ‘…ğ·,ğ‘š denotes the weights for the predictions from the current layer and all deeper layers, respectively. The joint loss function for the multi-modal fusion network consists of the side loss from each CA-Fuse block. We also add a collaborative loss to encourage an informative combination of all the side-outs:

ğ¿ğ‘“ğ‘–ğ‘›ğ‘ğ‘™=âˆ‘ğ‘š=1ğ¾(ğ‘‘(ğ‘ƒÌƒ ğ‘šğ‘…âˆ’ğ‘,ğ˜)+ğ‘‘(ğ‘ƒÌƒ ğ‘šğ·âˆ’ğ‘,ğ˜)+ğ‘‘(ğ‘ƒÌƒ ğ‘šğ‘…ğ·,ğ˜))+ğ‘‘(âˆ‘ğ‘š=1ğ¾ğ°Ìƒ ğ‘šğ‘ƒÌƒ ğ‘šğ‘…ğ·,ğ˜),
(8)
where d is the cross-entropy loss function, ğ˜ is the ground-truth mask, ğ°Ìƒ ğ‘š is the weight for ğ‘ƒÌƒ ğ‘šğ‘…ğ·. ğ‘ƒÌƒ ğ‘šğ‘…âˆ’ğ‘ and ğ‘ƒÌƒ ğ‘šğ·âˆ’ğ‘ denote the predictions by ğ¹Ìƒ ğ‘šğ‘… and ğ¹Ìƒ ğ‘šğ· in the CA-Fuse block, respectively. This joint loss enables the cross-modal and cross-level combinations to be complementary for better inference.

Experiments
In this section, we introduce the training details, experimental comparisons, and ablation studies to verify the advantages of the proposed method to learn, select, and fuse cross-modal complements, the success in weakly-supervised saliency detection from depth, and the generalization of the designs to semantic segmentation.

Datasets and Evaluation Metrics
Datasets: we evaluate our model on eight RGB-D benchmark datasets: NLPR (Peng et al. 2014) includes 1000 indoor/outdoor RGB-D pairs collected using Kinect; NJUD (Ju et al. 2014) and STEREO (Niu et al. 2012) datasets contains 2003 and 797 stereoscopic images respectively. LFSD (Li et al. 2014) , RGBD135 (Cheng et al. 2014) and SSD (Zhu and Li 2017) contain 100, 135, and 80 samples, respectively. SIP (Fan et al. 2020) is a salient person dataset which includes 929 scenes collected with a smartphone. DUT (Piao et al. 2019) is collected by Lytro camera and has 1200 samples. We only use its testing set (400 samples). We follow the previous works Chen and Li (2018), Han et al. (2017) and Chen et al. (2018) to randomly pick 650 and 1400 RGB-D pairs from the NLPR and NJUD datasets respectively and combine them as the training set. The teacher network, HCD network, and final RGB-D fusion network are trained with RGB-label pairs, RGB-depth pairs, and RGB-depth-label pairs in the same training set, respectively. Note that when we verify the efficacy of our HCD scheme for weakly-supervised SOD, the teacher network is trained with popular RGB SOD datasets, which only contain RGB images and corresponding ground-truth masks. This choice guarantees that only the RGB-depth pairs in the RGB-D training set are employed to train the HCD network and the saliency labels are not used.

Evaluation Metrics: we adopt the up-to-date generally-recognized weighted F-measure (ğ¹ğ‘¤ğ›½) (Margolin et al. 2014), mean absolute error (MAE) scores, structure-measure (S-measure) (Fan et al. 2017), and enhanced-alignment measure (E-measure) (Fan et al. 2018) as metrics to comprehensively evaluate the performance. Concretely, the weighted F-measure is defined as

ğ¹ğ‘¤ğ›½=(1+ğ›½2)â‹…ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘¤â‹…ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğ‘¤ğ›½2â‹…ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›ğ‘¤+ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ğ‘¤.
(9)
where ğ›½2=1. It is a modified version of the traditional F-measure and holds better generalization. The saliency map and binary ground-truth are normalized to [0, 1] and the MAE is to measure the pixel-wise discrepancy between the saliency map ğ‘†Â¯ and the ground-truth mask ğºÂ¯ averagely:

MAE=1ğ‘ŠÃ—ğ»âˆ‘ğ‘–=1ğ‘Šâˆ‘ğ‘—=1ğ»âˆ£âˆ£ğ‘†Â¯(ğ‘–,ğ‘—)âˆ’ğºÂ¯(ğ‘–,ğ‘—)âˆ£âˆ£,
(10)
where W and H are the width and height of the saliency map. S-measure (ğ‘†ğ›¼) (Fan et al. 2017) is a newly proposed metric that collectively measures region-level and object-level similarities between the prediction and ground-truth mask. E-measure (ğ¸ğœ‰) (Fan et al. 2018) captures image-level and pixel-level statistics jointly with an enhanced alignment term.

Training Details
We conduct our experiments using the Caffe (Jia et al. 2014) toolbox on a workstation with two GTX 1070 GPUs. The learning rate for the teacher network, the HCD network and the final RGB-D SOD network are 1Ã—10âˆ’7, 1Ã—10âˆ’6 and 2Ã—10âˆ’9, respectively.

Initialization: We train the teacher network with the ImageNet model as initialization. When we train the HCD network, both the teacher and student streams are initialized with the learned teacher model. The final RGB-D fusion network is initialized with the learned HCD network.

Fig. 5
figure 5
Individual level-specific inference from the teacher and student streams in the HCD network

Full size image
On the HCD Schema
Does the Student Network Learn Specific Cues?
The first question we want to investigate is whether the proposed HCD scheme can encourage the student network to learn specific cues to complement the source modality. Figure 5 shows the individual inference from each level without combining with the predictions from deeper layers. It is not difficult to note that the side-outs from the student network present different patterns in contrast to the ones generated by the teacher. More specifically, the shallow layers of the student network are only responsive to the depth variations while insensitive to the color/texture changes and the deeper layers, which are more responsible for locating the salient object, pay more attention to the object with distinguished depth. These differences demonstrate the success of the proposed HCD scheme to explore depth-specific saliency cues in each level, which are complementary to the ones from the RGB modality. Also, the cross-modal complementarity resides in multiple levels. These observations verify our motivations that a selector is in demand for highlighting the real complementary cues for joint inference. Besides, a sufficient fusion scheme is also necessary considering the cross-modal complements in multiple layers.

Table 3 Comparison between our HCD scheme and other pre-training/transfer learning methods for depth-induced SOD
Full size table
Advantages as a Pre-training Scheme
We add the comparison to two other transfer learning strategies acting as a pre-training scheme or a weakly-supervised learning method for SOD on depth data. We denote the feature-level supervision transfer method in Gupta et al. (2016) and the method of transferring the final prediction by â€œGupta_STâ€ and â€œFinal_pre_KDâ€, respectively. For â€œGupta_STâ€, we follow their default choices of the transferring point and training settings that are best performed in the experiments. For â€œFinal_pre_KDâ€, we adopt the same setting with our HCD scheme, including the initialization and other training parameters. In this section, we report the advantages of the proposed HCD scheme as a pre-training method for depth-induced saliency detection. After the transfer learning process, the student stream is finetuned with depth images and labels. The finetuned results for two other transfer learning methods are denoted by â€œFT-Gupta_STâ€ and â€œFT-Final_pre_KDâ€, respectively. We also compare our HCD scheme with traditional finetuning schemes, including FT-ImageNet: finetuning the original ImageNet model or FT-TeacNet: finetuning the teacher model that is trained with RGB images and ground-truth masks.

Table 4 Comparison of the HCD scheme to other training schemes for RGB-D SOD
Full size table
Table 5 Comparison of weakly-supervised saliency detection from depth images
Full size table
As shown in Table 3, compared to finetuning the teacher network (FT-TeacNet), the proposed HCD scheme, without using ground-truth to finetune the student network (noted as â€œHCDâ€), achieves a huge improvement, which demonstrates the efficacy of our cross-modal transfer strategy. This improvement indicates that approaching the ground-truth mask in each level directly appears a difficult task for the student network, especially for shallow layers and challenging scenes. In contrast, in our HCD scheme, the supervision provided by the teacher stream has been customized for different levels, which are more appropriate and easier for each layer to approach. Concretely, the evolution of the side-outs demystifies level-specific contributions and cross-level collaborations explicitly. As a result, the goal of each level in the student stream is simplified to match level-specific enhancement. For example, the goal of shallow layers is to learn low-level features for identifying object edges, which is a much easier task for them than predicting the completed saliency map. After the HCD process, the difficulty to approach the ground-truth is well reduced for the student network. As a result, finetuning the HCD network (â€œFT-HCDâ€) obtains large improvement than directly finetuning the teacher network (â€œFT-TeaNetâ€). We also notice that other two transfer methods do not make improvements over pre-training with ImageNet models, indicating that, with limited training pairs, large cross-modal discrepancy, and a noisy student modality, merely mimicking the final prediction or deepest features from the teacher stream is difficult to facilitate the learning of the student stream and is likely to introduce negative transfer. In contrast, our HCD scheme customizes appropriate supervision for each level in the student to boost the learning of all layers and is more robust to cross-modal data discrepancy and noises in the student modality.

We also report the performance of using the HCD method to pre-train the final RGB-D SOD network. We involve other two strategies for comparison. a) Two-stream Vgg: Both the RGB and depth streams are initialized by the ImageNet VGG model and the whole RGB-D fusion network is trained in an end-to-end manner directly, without separately finetuning with the RGB-D saliency datasets. This strategy is adopted in Chen and Li (2018); b) Stage-wise training: It means that we use saliency labels to finetune the RGB stream with the ImageNet VGG model as initialization firstly. Afterwards, we finetune the depth stream starting from the well-trained RGB SOD network. Lastly, the RGB-D SOD network is trained by using the finetuned RGB and depth SOD networks as initialization. This strategy is widely adopted in the previous works such as Han et al. (2017) and Chen et al. (2018); c) Our HCD: Using the trained HCD network as initialization. With the three initialization schemes, we train the RGB-D fusion network using the RGB-D pairs and ground-truth and the performance is compared in Table 4, which shows a large outperformance of the proposed HCD schema over others, suggesting its success in learning richer representations from the new modality.

For Weakly-Supervised Saliency Detection from Depth Images
The supervision transfer method in Gupta et al. (2016) only learns feature hierarchies for the new modality. Consequently, the learned student stream cannot be used for inference directly. In contrast, our HCD scheme transfers features and inference knowledge simultaneously and can be readily used for prediction. To verify the efficacy of our HCD in weakly-supervised SOD for depth data, we combine the RGB SOD datasets including MSRA10K (Cheng et al. 2015), ECSSD (Yan et al. 2013) and SED2 (Alpert et al. 2007) to train the teacher network. Then we only use the RGB-D pairs from the RGB-D SOD training set to train the HCD network. We use the result that feeding the depth (HHA) images to the teacher network (denoted by â€œHHA-Input_TeaNetâ€) as the baseline to show the transfer learning gain. We also compare our HCD scheme (denoted by â€œHCD-weak_supâ€) with the knowledge distillation method of mimicking final predictions (denoted by â€œFinal_pre_KDâ€) and the results are shown in Table 5. Benefit from the hierarchical distillation, each level in the student stream can be optimized with supervision from the teacher. The distilled student stream can learn level-specific contributions and cross-level combinations simultaneously to make the joint inference. As a result, our HCD scheme, without finetuning with labels, achieves significant improvement over the baseline and the method of merely transferring final predictions.

On the CA-Fuse Block
In this section, we analyze the components in the CA-Fuse block.

Fig. 6
figure 6
Visualization to show the benefits of the components in the CA-fuse block

Full size image
Table 6 Analyze the components in the CA-Fuse block quantitatively
Full size table
Cross-Modal Residual Fusion
We first study the importance of introducing the cross-modal residual function. Figure 6 illustrates the side outputs from each level with different designs shown in Fig. 4. The columns indexed as â€œF4(b)â€ show that the saliency maps inferred in the top-down pattern can be basically refined from coarse to fine with the help of the deep supervision in each level and the cross-level combinations. However, the salient objects are not uniformly highlighted and some background regions are failed to be identified, suggesting that directly concatenating cross-modal features as done in previous works is incapable of capturing the complementary information sufficiently.

Then we improve the â€œF4(b)â€ block by adding cross-modal residual connections and this variant is denoted as â€œF4(b)+resâ€. Benefiting from the cross-modal residual functions, the complementary cues from both modalities are exposed and incorporated more easily, resulting in more informative multi-modal fusion. The comparison between â€œF4(b)â€ and â€œF4(b)+resâ€ in Table 6 verifies the large performance gains from the cross-modal residual connections. Moreover, the comparison between â€œF4(b)+resâ€ and â€œF4(c)â€ verifies the contribution of adding supervisions on the RGB and depth branches (ğ¿ğ‘… and ğ¿ğ·), which further boost the emergence of the complementary cues from the paired modality.

Table 7 Quantitative comparison to other cross-modal connection designs
Full size table
Fig. 7
figure 7
Visual comparison of different cross-modal connection strategies

Full size image
We also compare our residual connection to other two variants: using weighted cross-stitch connections (Misra et al. 2016) or summing feature maps from depth to RGB directly (Hazirbas et al. 2016). We replace our residual modeling with their proposed cross-modal connection methods, while other strategies such as the effective cross-level fusion are retained for fair comparison. We follow the method in Misra et al. (2016) to initialize the weights in cross-stitch units. Both Hazirbas et al. (2016) and Misra et al. (2016) and our network are trained end-to-end with the ImageNet VGG-16 model as initialization. The comparison in Table 7 shows that our cross-modal connection strategy outperforms others with a large margin. The visualized results generated by different strategies are shown in Fig. 7. It can be observed that the saliency maps inferred by our proposed network hold better uniformity and contain fewer noises. This difference indicates that our residual designs enable selective combination, while other designs add all pixels of two feature maps in an undifferentiated manner, which are more likely to incorporate noisy patterns.

Cross-Level Feature Combination
Another question we want to study is whether it is beneficial to transmit cross-level features to the adjacent shallower layer. To answer this question, we remove the ğ¹Ìƒ ğ‘š+1ğ‘š,ğ‘…ğ· in Fig. 4c. Accordingly, ğ¹Ìƒ ğ‘šğ‘… and ğ¹Ìƒ ğ‘šğ· will be concatenated for joint inference. We denote this variant as â€œF4(c)-ğ¹Ìƒ ğ‘š+1ğ‘š,ğ‘…ğ·â€. The quantitative comparison between â€œF4(c)-ğ¹Ìƒ ğ‘š+1ğ‘š,ğ‘…ğ·â€ and â€œF4(c)â€ in Table 6 reports the noticeable gains by transmitting the cross-level features. We attribute this improvement to the richer RGB-D representations.

Compare to State-of-the-Art
We compare our model to 12 state-of-the-art RGB-D SOD methods: NLPR (Peng et al. 2014), EGP (Ren et al. 2015), ACSD (Ju et al. 2014), DCMC (Cong et al. 2016), LBE (Feng et al. 2016), SE (Guo et al. 2016), DF (Qu et al. 2017), CTMF (Han et al. 2017), MMCI (Chen et al. 2018), our preliminary work PCA (Chen and Li 2018), CPFP (Zhao et al. 2019) and DMRA (Piao et al. 2019) among which DF (Qu et al. 2017), CTMF (Han et al. 2017), MMCI (Chen et al. 2018), PCA (Chen and Li 2018), CPFP (Zhao et al. 2019) and DMRA (Piao et al. 2019) are CNN-based methods. We also compare our method with a state-of-the-art RGB salient object detection model DSS (Hou et al. 2017) to verify the benefits of the synchronized depth data.

Fig. 8
figure 8
Visual comparison to a state-of-the-art RGB SOD method

Full size image
Table 8 Quantitative comparison to state-of-the-art methods
Full size table
Figure 8 presents the comparison to the RGB-induced saliency visually. It can be noted that when the salient object and the background are with similar appearance, the background is seriously cluttered, or the salient object is non-uniform, it is difficult to locate the salient object correctly and highlight the salient regions uniformly by relying on RGB inputs only. In these scenes, our model effectively incorporates the complementary cues from the paired depth data to overcome these deficiencies to identify the real salient object and highlight the salient regions consistently.

The quantitative comparison in Table 8 shows that our proposed PCA method outperforms previous works significantly. Compared to other previous RGB-D SOD methods, the proposed one holds distinguished advantages in learning, selecting, and fusing cross-modal complements. The methods Peng et al. (2014), Ren et al. (2015), Guo et al. (2016), Feng et al. (2016), Ju et al. (2014), Cong et al. (2016) and Song et al. (2017) based on handcrafted RGB-D features are easy to be confused by complex background and intra-variant salient objects due to the lack of high-level global contexts.

Previous CNN-based methods Qu et al. (2017) and Han et al. (2017) that combine cross-modal features only in a single level are incapable of capturing the cross-modal complementarity residing in high-level contexts and low-level spatial cues simultaneously. The â€œearly fusionâ€ schema adopted in Qu et al. (2017) results in inconsistent highlighting of salient regions and the â€œlate fusionâ€ strategy used in Han et al. (2017) leads to severely blurred saliency maps. Although the work Chen et al. (2018) remedies this shortcoming by designing two branches for global reasoning and local capturing respectively, it only leverages the last fully connected layer and an intermediate convolutional layer for joint inference. The final results are combined by directly summing the results from two branches, which is unlikely to combine local spatial cues and global contexts robustly.

In contrast, our preliminary work Chen and Li (2018) involves the information in all layers via a top-down path, which is able to progressively select and fuse the complements from each modal/level and refine the saliency maps gradually. By further using the HCD schema proposed in this extended work, the salient object is better located. Besides, the saliency maps are more uniform and carry better details than the ones generated by Chen and Li (2018), implying the advantages of the proposed cross-modal transfer scheme in learning richer representations from the new modality. The RGB-D fusion architecture in our conference version PCA and this journal version PCA-HCD is the first multi-level fusion architecture that explores the cross-modal complements in multiple levels for RGB-D SOD. As reported in Table 8, we notice that the following two multi-level cross-modal fusion architectures CPFP and DMRA outperform previous single-level or two-level fusion models DF, CTMF, and MMCI by a large margin. This advance convincingly supports our argument that cross-modal complements should be combined in multiple levels to sufficiently explore the RGB-D complements for joint SOD. Also, we are pleased to witness the improvement of CPFP and DMRA over our conference version PCA on most datasets, which well demonstrates the efficacy and potential of the multi-level fusion architecture. The performance of CPFP is comparable to this journal version PCA-HCD and the DMRA achieves improvement on most datasets except the SIP dataset.

Fig. 9
figure 9
Visual comparison to state-of-the-art RGB-D SOD methods

Full size image
Figure 9 shows various challenging scenes, such as the background is complex (the 1stâ€“2nd rows); the salient object and background have indistinguishable appearance or depth (the 3rdâ€“4th and 5thâ€“6th rows, resp.); the appearance or depth in the salient objects is non-uniform (the 7th and 8th rows, resp.); large/small salient objects (the 9th and 10th rows, resp.); multiple separated salient objects (the 10th row). In these cases, our proposed model can learn rich representations from each modality, select the complementary cues, and fuse them informatively for successful joint inference. We also note that our method (â€œOursâ€) still holds better performance on considerable challenging scenes (e.g., the 3th, 4th, 8th, and 9th rows) over CPFP and DMRA, suggesting that merely focusing on enhancing cross-modal interactions and fusion paths cannot overcome all fusion uncertainties, whereas boosting the learning of the new modality as extended in this journal work is a new and effective perspective to solve this problem.

Application to Semantic Segmentation
Our insights focus on how to better learn from depth data and fuse RGB-D complements. Hence, the designs are agnostic to the downstream tasks and can be used for a variety of tasks. In this section, we experimentally verify the generalization of the HCD and CA-Fuse designs to semantic segmentation. For SOD, the output from each level is a single-channel map, where each value denotes the salient possibility of a pixel. In contrast, the prediction for semantic segmentation is multiple-channel maps regarding different categories, respectively. Hence, directly concatenating the predictions from different levels cannot align the prediction for each class, which means that the predictions from different layers for each class are not concatenated correspondingly. In this way, the following fusion layer is not aware that each channel in the concatenated prediction maps corresponds to which class. This ambiguity makes the cross-level fusion result to be uninformative. As an alternative, here we directly element-wise sum the predictions from different levels as done in FCN (Long et al. 2015), although this strategy is not adaptive enough.

We use ResNet50 as the backbone of each stream and perform the evaluation on the widely used SUN RGB-D (Song et al. 2015) dataset. We adopt mean accuracy (mAcc) and mean intersection over union (mIoU) as evaluation metrics.

On the Residual Cross-Modal Fusion Design
We firstly compare the traditional undifferentiated concatenation method with our residual cross-modal fusion design (i.e., the modules shown in Fig. 4b, c, respectively.). The two variants are initialized with the ImageNet model and trained in an end-to-end manner. We denote the two variants by â€œTwo-streamâ€ and â€œOur CA-Fuseâ€, respectively.

Table 9 Comparison to the baseline and concurrent SOTA RGB-D semantic segmentation models
Full size table
The comparison between the colums of â€œTwo-streamâ€ and â€œOur CA-Fuseâ€ in Table 9 exhibits a large improvement by using our residual cross-modal fusion strategy over directly concatenation, demonstrating the advantages of residual fusion in better incorporating cross-modal complements. Table 9 indicates that our fusion architecture achieves comparable performance to concurrent works. With specific adaptation on the side-outs combination method and careful tuning of the training parameters, we believe that our framework can enjoy further improvement on semantic segmentation. We defer this study to our future work.

Table 10 Comparison between our HCD scheme to other pre-training or transfer learning methods for depth-induced semantic segmentation
Full size table
On the HCD Scheme
We also study the efficacy of our HCD scheme on semantic segmentation. As same as the comparison for SOD in Sect. 4.3.2, We compare our HCD scheme with traditional finetuning schemes, including (A) FT-ImageNet: finetuning the original ImageNet model or (B) FT-TeacNet: finetuning the teacher model, and other two transfer learning methods: (C) FT-Gupta_ST finetuning the student stream that is trained using the feature-level supervision transfer strategy in Gupta et al. (2016); and (D) FT-Final_pre_KD: finetuning the student stream that is trained by mimicking the final output of the teacher network. Comparing the variants in Table 10, we find that our HCD scheme, without using labels to finetune ((E) HCD), achieves 2.5% mIoU boost than the traditional scheme of finetuning the ImageNet model ((A) FT-ImageNet), demonstrating the advantages of our hierarchical cross-modal distillation design and its potential in weakly-supervised semantic segmentation on a new modality. Finetuning the student stream after using our HCD scheme allows further improvement. The finetuned model ((F) FT-HCD)) outperforms other pre-training and transfer learning methods with a large margin.

Conclusion
In this paper, we propose a comprehensive view and a systematic solution for RGB-D salient object detection. The philosophy in designing an RGB-D system is generalized as three keys: modal-specific representations learning, complementary information selection, and cross-modal complement fusion. Accordingly, we propose a new cross-modal transfer learning scheme, an explicit cross-modal complement selector, and a sufficient cross-modal cross-level fusion pattern. The proposed solution addresses the problems of detecting salient objects from a new modality or multi-modal data jointly. It also shows good generalizability on semantic segmentation. We believe the insights provided from this work will allow us to learn better representations from other new modalities and enable sufficient fusion for other multi-modal systems.