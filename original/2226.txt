Transferring human motion and appearance between videos of human actors remains one of the key challenges in Computer Vision. Despite the advances from recent image-to-image translation approaches, there are several transferring contexts where most end-to-end learning-based retargeting methods still perform poorly. Transferring human appearance from one actor to another is only ensured when a strict setup has been complied, which is generally built considering their training regime’s specificities. In this work, we propose a shape-aware approach based on a hybrid image-based rendering technique that exhibits competitive visual retargeting quality compared to state-of-the-art neural rendering approaches. The formulation leverages the user body shape into the retargeting while considering physical constraints of the motion in 3D and the 2D image domain. We also present a new video retargeting benchmark dataset composed of different videos with annotated human motions to evaluate the task of synthesizing people’s videos, which can be used as a common base to improve tracking the progress in the field. The dataset and its evaluation protocols are designed to evaluate retargeting methods in more general and challenging conditions. Our method is validated in several experiments, comprising publicly available videos of actors with different shapes, motion types, and camera setups. The dataset and retargeting code are publicly available to the community at: https://www.verlab.dcc.ufmg.br/retargeting-motion.

Access provided by University of Auckland Library

Introduction
Human image synthesis has seen fast progress from the realistic image-to-image translation of human characters, as milestones are being passed in a wide range of applications, including face synthesis, style transferring, and motion retargeting. Despite remarkable advances in synthesizing virtual moving actors with image-to-image translation approaches, creating plausible virtual actors from images of real actors still remains a key challenge. These approaches face the unique cognitive capability of humans for perceiving attributes of objects from images. Humans start learning early in their lives to recognize human forms and make sense of what emotions and meanings are being communicated by human movement. We are, by nature, specialists in the human form and movement analysis. Even small imperfections when synthesizing virtual actors might create a false appearance, especially body motion shaking effects when dealing with moving actors.

Many works in the Computer Vision and Computer Graphics communities have made great strides in capturing human geometry and motion through model-based and learning techniques. In particular, end-to-end learning approaches such as Peng et al. (2018), Kanazawa et al. (2018), and Kolotouros et al. (2019) have achieved state of the art in capturing three-dimensional motion, shape, and appearance from videos and still images from real actors. Most recently, several methods have been proposed on body reenactment from source images (Lassner et al. 2017a; Zhao et al. 2017; Ma et al. 2017; Chan et al. 2019; Esser et al. 2018; Liu et al. 2019). The ultimate goal of these methods is to create a video where the body of a target person is reenacted according to the motion extracted from the monocular video. The motion is estimated considering the set of poses of a source person. Despite the impressive results for several input conditions, there are instances where most of these methods perform poorly. For instance, the works of Chan et al. (2019) and Wang et al. (2018), only perform good reenacting of the appearance/style from one actor to another if a strict setup has complied, e.g., static backgrounds, a large set of motion data of the target person to train, and actors in the same distance from the camera (Tewari et al. 2020). Furthermore, it is hard to gauge progress from these results in the field of retargeting, as most works only report the performance of their algorithms in their own set of images which, in general, is built considering the specificities in the training regime of their approaches.

In this paper, we tackle the problem of not only body reenactment but transferring a target actor from a video sequence to a new video. In the synthesized new video, the target must perform a similar motion to a source actor considering his/her physical motion constraints and the video’s background where the source actor appears, i.e., the target actor is placed into a new context performing the source actor’s movements. To face this problem, we propose a shape-aware human retargeting approach and a new dataset with many annotations that provide a practical benchmark to assess the progress of both retargeting and body reenactment methods.

Our dataset comprises different videos with annotated human-to-object interactions and paired motions to evaluate the task of synthesizing videos of people. This dataset can be used as a common base to improve tracking the progress of the field by showing where current approaches might fail and measuring the quality of future proposals. All data were collected in order to test methods of retargeting motion in some common real conditions, more general, challenging scenarios, and not designed according to a specific training regime. Our shape-aware human retargeting approach from monocular videos of people has several advantages. It might be applied in general scenes and conditions, it is compatible with existing rendering pipelines, and it is not actor specific, conversely to most end-to-end learning techniques. The retargeting method has four main components: motion estimation of the source actor, extraction of shape and appearance of the target actor, motion retargeting with spatio-temporal constraints, and image-based rendering and composition of the video. By imposing spatial and temporal constraints on the characters’ joints, our method preserves features of the motion, such as feet touching the floor and hands touching a particular object in the transferring.

We performed experiments using our new dataset containing several types of motions and actors with different body shapes and heights. Our results show that an image-based rendering technique can still exhibit a competitive quality compared to the recent deep learning techniques in generic transferring tests. Our approach achieved better results compared with end-to-end learning methodologies such as the works of Wang et al. (2018) and Esser et al. (2018) in most scenarios for appearance metrics as structural similarity (SSIM), learned perceptual similarity (LPIPS), mean squared error (MSE), and Fréchet Video Distance (FVD). The experimental results extend the observation of Wang et al. (2020), where they indicate that CNN-generated images are still surprisingly easy to spot. Our results suggest that retargeting strategies based on image-to-image learning still perform poorly to several motion types and transferring contexts.

Although remarkable advances have been made in neural human rendering approaches (Wang et al. 2018; Chan et al. 2019; Aberman et al. 2018; Sun et al. 2020), simultaneously transferring the appearance and motion of virtual characters is still a challenging problem. Most existing algorithms are restricted to perform self-transfer (or reenactment), suffering from poor generalization to changes in scale, camera viewpoint, and intrinsic parameters. Moreover, the lack of suitable datasets for performing and evaluating retargeting approaches on a common ground hampers to track the progress of the field. These limitations are the central motivation of our proposed approach. Therefore, the contribution of this paper is three-fold:

1.
We present a novel video retargeting technique for human motion and appearance transferring. This technique is designed to leverage invariant information along the video transferring, such as the body shape of the person, while also taking into account physical constraints of the motion in 3D and the image domain;

2.
We present a new benchmark dataset composed of 88 videos, being 32 paired videos with annotated human-to-object interactions and paired actor motions, eight videos where each actor rotates vertically while holding an A-pose, eight four-minute videos with the subject performing random moves, and eight 15-seconds videos with actors dancing in pairs. The set participating actors comprises eight human subjects, carefully chosen with varying body shapes, clothing styles, heights, and gender. The designed dataset and evaluation protocol are suitable for both self-transfer and cross-transfer scenarios;

3.
The dataset and presented results also propose to open discussions on common evaluation metrics for video retargeting schemes. In this context, our results suggest an image-based rendering technique can still exhibit a competitive quality when compared against recent end-to-end deep learning techniques in challenging and more general video-to-video human retargeting contexts.

Related Work
Human appearance transfer and image-to-image translation. The past five years have witnessed the explosion of generative adversarial networks (GANs) to new view synthesis. The synthesis is formulated as being an end-to-end learning problem (Tatarchenko et al. 2015; Dosovitskiy et al. 2015; Yang et al. 2015; Balakrishnan et al. 2018; Esser et al. 2018), where a distribution is estimated to sample new views. A representative approach is the work of Ma et al. (2017), where the authors proposed to transfer the appearance of a person to a given body pose. Similarly, Lassner et al. (2017a) proposed ClothNet to generate images of people with similar poses and shapes in different clothing styles. Esser et al. (2018) used a conditional U-Net to synthesize new images based on estimated edges and body joint locations.

Recent works such as Aberman et al. (2018) and Chan et al. (2019) start applying adversarial training to map 2D poses to the appearance of a target subject. Although these works employ a scale-and-translate step to handle the difference in the limb proportions between the source skeleton and the target, their synthesized views still have clear gaps in the test time compared with the training time. Wang et al. (2018) proposed a general video-to-video synthesis framework based on conditional GANs to generate high-resolution and temporally consistent videos of people. Shysheya et al. (2019) attempt to handle the poor generalization by training a model using different actors’ point-of-views. Their approach is also actor-specific and requires training a model for each new character, including the full acquisition setup information and camera poses. Mir et al. (2020) proposed to leverage the information from DensePose to learn a model to perform texture transfer of garments. Although their method is texture agnostic and not actor specific, it is designed to deal with garments transference (shirts and pants) and does not address the problem of full-body transference neither handle the cross-transference. It also disregards the motion constraints and human-to-object interactions (Hassan et al. 2019). In the same line, Neverova et al. (2018) investigated a combination of surface-based pose estimation and deep generative models; however, their method only considers the layout locations and ignores the personalized shape and limb (joint) rotations. Despite the impressive results for several inputs, end-to-end learning-based techniques still fail to synthesize the human body’s details, such as face and hands. Furthermore, it is worth noting that these techniques focus on transferring style, which leads to undesired distortions when the characters have different morphologies (proportions or body parts’ lengths). An alluring example is depicted in Fig. 1, where we perform the transfer between actors with differences in body shape (first row) and height (second row).

Fig. 1
figure 1
Motion and appearance transfer in different morphologies. From left to right: target person, source motion video with a human of different body shape, vid2vid (Wang et al. 2018), and our retargeting results. Note that vid2vid stretched, squeezed, and shranked the body forms whenever the transferring characters have different morphologies

Full size image
Another limitation of recent approaches such as Aberman et al. (2018), Chan et al. (2019), and Wang et al. (2018) is that they are dataset specific, i.e., they require training a different GAN for each video of the target person with different motions to perform the transferring. This training is computationally intensive and takes several days on a single GPU. In order to overcome these limitations, Liu et al. (2019) proposed a 3D body mesh recovery module to disentangle the pose and shape; however, their performance significantly decreases when the source image comes from a different domain from their dataset, indicating that they are also affected by poor generalization to camera viewing changes. Recently, Sun et al. (2020) also proposed a dataset-specific method where projections of the reconstructed 3D human model are used to condition the GAN training, in order to maintain the structural integrity of the transfer to different poses. Nevertheless, all analyses were made in a strict setup where the person is standing parallel to the image plane, and the considered motions have reduced lateral translations.

In this paper, we point out that there is still a performance gap of recent end-to-end deep learning techniques against an image-based model when this comprises carefully designed steps for human shape and pose estimation and retargeting. These results extend the observation of the works of Bau et al. (2019) and Wang et al. (2020), where the authors observed that GANs still present limited generation capacity. While Bau et al.  showed that generative network models could ignore classes that are too hard at the same time producing outputs of high average visual quality, Wang et al. demonstrated that CNN-generated images are yet surprisingly easy to spot.

3D human shape and pose estimation. Significant advances have been recently developed to estimate both the human skeleton and 3D body shape from images. Sigal et al. (2007) estimate shape by fitting a generative model, the SCAPE (Anguelov et al. 2005), to image silhouettes. Bogo et al. (2016) proposed the SMPLify method, which is a fully automated approach for estimating 3D body shape and pose from 2D joints in images. SMPLify uses a CNN to estimate 2D joint locations and then it fits an SMPL body model (Loper et al. 2015) to these joints. Lassner et al. (2017b) used the curated results from SMPLify to train 91 keypoint detectors. Similarly, Kanazawa et al. (2018) used unpaired 2D keypoint annotations and 3D scans to train an end-to-end network to infer the 3D mesh parameters and the camera pose. Kolotouros et al. (2019) combined an optimization method and a deep network to design a method less sensitive to the optimization initialization. Even though their method outperformed the works of Bogo et al., Lassner et al., and Kanazawa et al. regarding 3D joint error and runtime, their bounding box cropping strategy does not allow motion reconstruction from poses, since it frees three-dimensional pose regression from having to localize the person with scale and translation in image space. Moreover, they lack global information and temporal consistency in shape, pose, and human-to-object interactions, which are required in video retargeting with consistent motion transferring.

Retargeting motion.Gleicher (1998)’s seminal work of retargeting motion addressed the problem of transferring motion from one virtual actor to another with different morphologies. Choi and Ko (2000) pushed further Gleicher’s method by presenting a real-time motion retargeting approach based on inverse rate control. Both Gleicher’s and  Choi and Ko’s approaches require an iterative optimization with hand-designed activation constraints trajectories over time for several particular motions (like jumping or walking). Moreover, these constraints are sometimes hard to be designed if the 3D information of the environment is not explicitly available, such as in monocular videos.

Villegas et al. (2018) proposed a kinematic neural network with an adversarial cycle consistency to remove the manual step of detecting the motion constraints. In the same direction, the recent work of Peng et al. (2018) takes a step towards automatically transferring motion between humans and virtual humanoids. Similarly, Aberman et al. (2019) proposed a 2D motion retargeting using a high-level latent motion representation. Their method has the benefit of not explicitly reconstructing 3D poses and camera parameters, but it fails to transfer motions if the character walks towards the camera or with variations of the camera’s point-of-view. Differently from Gleicher, Choi and Ko, Villegas et al., and Peng et al., we propose a retargeting formulation that does not assume that all the desired constraints should be inferred only from 3D end-effectors pose, but also from the 2D locations in the image where the body-environment interactions happen (please see Fig. 5). Furthermore, our spatio-temporal modeling allows a competitive runtime during the retargeting, conversely to Gleicher’s work whose optimization can last several hours, even for a small batch of images.

Thus, inspired by the promising ideas employed to adapt motion from one character to another, 3D shape and pose estimation, our proposed methodology brings forth the advantages of jointly modeling 3D motion, shape, and appearance information. However, differently from the works mentioned above, our approach does not require training a model for each actor, keeps visual details from the target actor, does not require strict setups, and preserves the features of the transferred motion as well as the human-object interactions in the video.

Existing datasets. Datasets are one cornerstone of recent advances in Computer Vision. While there are many large datasets available for human shape and pose estimation (Andriluka et al. 2014; Lin et al. 2014; Mahmood et al. 2019), existing motion retargeting datasets are yet rare, hampering progress on this area. Villegas et al. (2018) provided human joint poses from synthetic paired motions, however, paired visual information is not available, limiting the appearance transferring. Chan et al. (2019) made available videos with random actor movements that can be used to learn the appearance of the target actor. However, the provided data is limited to their setup requirements, and it does not allow the analysis and comparison with other methods. Liu et al. (2019) presented a set of videos with random actions of target subjects, as well as videos of the subjects performing an A-pose movement. This set enables methods focusing on modeling 3D human body estimation or using few keyframes to be executed using their data. On the other hand, the lack of paired motions limits motion and appearance retargeting results in quantitative terms, where the source and target actors are different subjects. Conversely, our proposed dataset, in addition to videos with random actions of the target subjects and videos of the subjects performing an A-pose video, also provides several carefully paired reconstructed 3D motions and annotated human-to-object interactions in the image and 3D space.

Fig. 2
figure 2
Overview of our retargeting approach. Our method is composed of four main components: human motion estimation in the source video (first component); we retarget this motion into a different target character (second component), considering the motion constraints (third component), and by last, we synthesize the appearance of the target character into the source video

Full size image
Retargeting Approach
This section presents our human transferring method considering the importance of human motion, shape, and appearance in the retargeting. Unlike most techniques that transfer either appearance (Esser et al. 2018; Aberman et al. 2018; Chan et al. 2019; Wang et al. 2018) or motion independently (Villegas et al. 2018; Peng et al. 2018), we present a new method that simultaneously considers body shape, motion retargeting constraints, and human-to-object interactions over time, while retaining visual appearance quality.

Our method comprises four main components. We first estimate the motion of the character performing actions in the source video, where essential aspects of plausible movements, such as a shared coordinate system for all image frames and temporal motion smoothness are ensured. Second, we extract the body shape and texture of the target character in the second video. We also extract the texture, refine body geometry and estimate the visibility information of the body parts for transferring the appearance. Then, the retargeting component adapts the estimated movement to the body shape of the target character while considering temporal motion consistency and the physical human interactions (constraints) with the environment. Finally, the image-based rendering and composition component combines classical geometry rendering and image-based rendering to render the texture (appearance extracted from the target character) into the background of the source video. Figure 2 shows a schematic representation of the method.

Human body and motion representation. We represent the human motion by a set of translations and rotations over time of the joints that specify a human skeleton. This skeleton is attached to the actor’s body and is defined as a 24 linked joints hierarchy. The i-th joint pose 𝐏𝑖∈𝕊𝔼(3) is given by recursively rotating the joints of the skeleton tree, starting from the root joint and ending in its leaf joints, i.e., the forward kinematics denoted as FK. To represent the 3D shape of the human body, we adopted the SMPL model parametrization (Loper et al. 2015), which is composed of a learned human shape distribution , 3D joint angles (𝜃𝜃∈ℝ72 defining 3D rotations of the skeleton joint tree), and shape coefficients 𝛽𝛽∈ℝ10 that model the proportions and dimensions of the human body.

Shape-Aware Human Motion Estimation
We start estimating the actor’s motion in the source video. Our method builds upon the SPIN, a learning-based human model estimation framework of Kolotouros et al. (2019), where the human pose and body shape are predicted in the coordinate system of the person’s bounding box computed by OpenPose (Cao et al. 2017; Simon et al. 2017; Wei et al. 2016). SPIN leverages the advantages of regression and optimization-based human pose estimation frameworks to provide an efficient and accurate SMPL human model estimate. This bounding box normalizes the person in size and position. As also observed by Mehta et al. (2017), the bounding box normalization frees 3D pose estimation from the burden of computing the scale factor (between the body shape to the camera distance) and the location in the image. However, this normalization incurs in a loss of temporal pose consistency required in the motion transfer. The loss of consistency also often leads to wrong body shape estimates for each frame, which should be constant through the video.

In order to overcome these issues, we map the initial pose estimation using virtual camera coordinates, following the motion reconstruction strategy presented by Gomes et al. (2020). The loss in motion reconstruction is composed of two terms. While the first term enforces the pose projections of the joints to remain in the same locations into the common reference coordinate system, the second term favors maintaining the joints’ angles configuration reinforcing the character shape to have averaged shape coefficients (𝛽𝑠𝛽𝑠) in the entire video. The pose of the human model in each frame is then obtained with the forward kinematics (FK) from the obtained joints configuration 𝜃𝜃𝑠𝑘 of our shape-ware motion reconstruction:

[𝐏0𝑘 𝐏1𝑘 … 𝐏23𝑘]= FK (𝛽𝛽𝑠,𝜃𝜃𝑠𝑘),
(1)
where 𝐏𝑖𝑘=[ FK (𝛽𝛽𝑠,𝜃𝜃𝑠𝑘)]𝑖 is the pose of the i-th joint at frame k.

The raw actor motion is defined as 𝐌(𝛽𝛽𝑠,𝜃𝜃𝑠)=[𝐏1 𝐏2 …𝐏𝑛]∈ℝ24×4×4×𝑛, where 𝛽𝛽=[𝛽𝛽𝑠1,𝛽𝛽𝑠2,…,𝛽𝛽𝑠𝑛]∈ℝ10×𝑛 and 𝜃𝜃=[𝜃𝜃𝑠1,𝜃𝜃𝑠2,…,𝜃𝜃𝑠𝑛]∈ℝ72×𝑛 are composed of the stacked 𝛽𝛽𝑠,𝜃𝜃𝑠𝑘 over time.

Motion Regularization
Since we estimate the character poses frame-by-frame, the resulting motion might present shaking motion with high-frequency artifacts in some short sections of the video. To alleviate these effects, we perform a regularization to seek a new set of joint angles 𝜃𝜃𝑠ˆ that creates a smoother motion. After applying a cubic-spline interpolation (De Boor et al. 1978) over the joints’ motion 𝐌(𝛽𝛽𝑠,𝜃𝜃𝑠), we remove the outlier joints from the interpolated spline. The final motion estimate is obtained by minimizing the cost:

min(||𝜃𝜃𝑠ˆ−𝛩||2+𝛾|| FK (𝛽𝛽𝑠,𝜃𝜃𝑠ˆ)−𝐏𝑠𝑝||2),
(2)
where 𝛩 is the subset of inlier joints,  FK  is the forward kinematics, 𝛽𝛽𝑠 defines the proportions and dimensions of the human body in the source video, 𝐏𝑠𝑝 is the spline interpolated joint positions, and 𝛾 is the scaling factor between the original joint angles and the interpolated positions. This strategy removes high-frequency artifacts of the joints’ motion while retaining the movement features.

Target Character Processing
Most methods based on Generative Adversarial Networks, such as Wang et al. (2018), Chan et al. (2019), Aberman et al. (2018), and Sun et al. (2020), have emerged as effective approaches for human appearance synthesis. However, these methods still suffer in creating fine texture details, notably in some body parts as the face and hands. Besides, it is well known that these methods suffer from quality instability when applied in contexts slightly different from the original ones, i.e., a small difference in camera position, uncommon motions, pose translation, etc. These limitations motivate the proposal of our semantic-guided appearance retargeting method, which is designed to leverage visibility map information and semantic body parts to refine the initial target mesh model while keeping finer texture details in the transferring.

Thus, in order to create a more stable method and overcome the lack of details, we design a new semantic-guided image-based rendering approach that copies local patterns from input images to the correct position in the generated images. Our idea stems from using semantic information of the body (e.g., face, arms, torso locations, etc.) in the geometric rendering to encode patch positions and image-based rendering to copy pixels from the target images, and therefore maintaining texture details. This strategy estimates a generic target body model 𝛽𝛽𝑡, comprising body geometry, texture, and visibility information at each frame that will be transferred to the source video in the retargeting step.

Semantic-Guided Human Model Extraction
When extracting the appearance and geometry of the human body, the self-occlusion of body parts and the deformable nature of the human body (and of clothes) bring challenging conditions. In order to tackle these difficulties, we propose a semantic-guided image-based rendering of body parts that explores the global and local information of the human body into the body model estimation.

While we gathered the global geometric information from the pose and shape as discussed in Sect. 3.1, the local geometric information is extracted for each viewpoint and aligned with the contours of their semantic body parts in the image. To perform this alignment, we partitioned and computed the correspondence of the 3D body model into fourteen meaningful body part labels (face, left arm, etc.). The 2D semantic body labels are computed using the Gong et al. (2018)’s body parsing model, which we fine-tuned using the people-snapshot dataset (Alldieck et al. 2018).

After computing the semantic map of the body, for each contour point (red squares in Fig. 3) in the map, we define the vertex from the body mesh with the same semantic and the smallest Euclidean distance to the contour point as a control point (blue circles in Fig. 3). The Euclidean distance is computed between the contour point and the 2D projection of the vertex. Each control point will receive a target position given its correspondent contour point. These control points and their new locations guide the deformation of the mesh to fit the shape into the semantic map’s contour. In the deformation, we seek a new body mesh Q that is a locally rigid transformation of the source body mesh P, following the control points given by the semantic contours. The mesh deformation is solved efficiently with the local rigid deformation As-Rigid-As-Possible (ARAP) (Levi and Gotsman 2015). The correspondence between each contour point and control point is represented in Fig. 3 with colored small lines. Notice that the desired motion of the control points guides the deformation to fit the body mesh into the contours of the semantic map.

Fig. 3
figure 3
Semantic guided deformation. The contour points (red squares) in the semantic map indicate the target localization of the control points (blue circles) in the body mesh that it will guide the ARAP algorithm to fit the body mesh into the contours of the semantic map. The correspondence between each contour point and control point is illustrated with colored small lines (Color figure online)

Full size image
Human Textures and Visibility Maps Extraction
The geometric information allows rendering the human target character in a new viewpoint by applying the desired transformations and re-projecting them onto the image plane. In order to compare and merge the information from the human actor from different viewpoints, we map the views to a common UV texture map space. The mapping function is given by a parametric function  that maps a point in the mesh from frame k to a point in the texture space with coordinates (u, v), :ℝ3→ℝ2. Then, the accumulated texture map 𝑈𝑉(𝑢,𝑣) for all available images of the target character is done by the rendering with :

𝑈𝑉((𝑥,𝑦,𝑧))=(𝛱((𝑥,𝑦,𝑧),𝐊)),
(3)
where the 𝛱(.,𝐊) operator performs the rendering taking a 3D mesh point (x, y, z) and projecting it into the image plane given the camera parameters 𝐊, and  is the texture information in the image coordinates.

Finally, we assert which mesh points are visible by exploring the inverse map −1(𝑢,𝑣), as illustrated in Fig. 4. Each visibility map indicates which parts of the body model are visible per frame. Then we select the closest viewpoint to the desired new viewpoint, for each part of the body model from the visibility maps.

Fig. 4
figure 4
Rendering of the visibility maps and texture images. Top: We project each target actor viewpoint in a common UV texture space using the estimated geometry and create a binary map of visibility body parts. Bottom: Given the goal pose (retargeted pose), we estimate its visibility body parts map, and then select the better matching visibility body parts created from the viewpoints from the target actor

Full size image
Motion Retargeting using Hybrid 2D/3D Spatio-Temporal Constraints
After estimating the motion from the input video, i.e., 𝐌(𝛽𝛽𝑠,𝜃𝜃𝑠), and 3D model 𝛽𝛽𝑡 of the target human, we can proceed to the motion retargeting step. We assume that the target character has a homeomorphic skeleton structure to the source character, i.e., the geometric differences are in terms of bone lengths and body proportions. Our retargeting motion estimation loss is designed to guarantee the motion similarity and physical human-object interaction constraints over time. Similar to Gleicher (1998), our first goal is to retain the joint configuration of the target as close as possible to the source joint configurations at instant k, 𝜃𝜃𝑡𝑘≈𝜃𝜃𝑠𝑘, i.e., to keep 𝐞𝑘 small such as: 𝜃𝜃𝑡𝑘=𝜃𝜃𝑠𝑘+𝐞𝑘.

We also aim to keep similar movement style and speed in the retargeted motion. Thus, we propose a one step speed prediction in 3D space defined as 𝛥𝐌(𝛽𝛽,𝜃𝜃𝑘)= FK (𝛽𝛽,𝜃𝜃𝑘+1)− FK (𝛽𝛽,𝜃𝜃𝑘) to maintain the motion style from the original joints’ motion:

𝑃(𝐞)=∑𝑘=𝑖+1𝑖+𝑛||𝛥𝐌(𝛽𝛽𝑡,𝜃𝜃𝑠𝑘+𝐞𝑘)−𝛥𝐌(𝛽𝛽𝑠,𝜃𝜃𝑠𝑘)||1,
(4)
where 𝐞=[𝐞𝑖+1,…,𝐞𝑖+𝑛]𝑇, and n is the number of frames considered in the retargeting.

Rather than considering a loss for the total number of frames, we use only the frames belonging to a neighboring temporal window of n frames equivalent to two seconds of video. This neighboring temporal window scheme allows us to track the local temporal motion style producing a motion that tends to be natural compared with a realistic-looking of the estimated source motion. The retargeting considering a local neighboring window of frames also results in a more efficient optimization.

2D/3D human-to-object interactions. The human-to-object interactions (i.e., motion constraints) are important to identify key features of the original motion that must be preserved in the retargeted motion. The specification of these interactions typically involves only a small amount of work in comparison with the task of creating new motions. Typical interactions are, for instance, that the target character’s feet should be on the floor; holding hands while dancing or while grabbing/manipulating an object in the source video. Some examples of human-to-object motion constraints are shown in Fig. 5, where the actor is interacting with a box.

Going one step further than classic retargeting constraints defined in Gleicher (1998) and Choi and Ko (2000), where end-effectors must be at solely a desired 3D position at a given moment, we propose an extended hybrid constraint in the image domain, i.e., the joint of the character must also be projected at a specific location in the image. This type of motion constraint allows the user to exploit the visual knowledge of interactions of the actor in the scene. Some examples are shown in Fig. 5, where two types of constraints are defined: 3D interactions (blue dots) impose the feet and right hand to be in the same location after the retargeting, and 2D constraints (red dots) imposing the correct position to the left hand in the image.

Fig. 5
figure 5
Example of hybrid 2D/3D constraints from human-to-object interactions. Top row: Original video with 3D constraints (blue dots) and 2D constraints (red dots). Middle row: Motion retargeting to a new character using only 3D constraints. Bottom row: The results for the new character applying hybrid 2D/3D constraints using our retargeting approach. Please observe that the hands’ positions are more consistent when adopting our hybrid strategy (Color figure online)

Full size image
Our retargeting is capable of adapting to such situations by defining the motion retargeting constraints losses in respect to end-effectors’ (hands, feet) 3D poses 𝐏𝑅3𝐷 and 2D poses 𝐏𝑅2𝐷 as:

𝑅3𝐷(𝐞𝑘)=|| FK (𝛽𝛽𝑡,𝜃𝜃𝑠𝑘+𝐞𝑘)−𝐏𝑅3𝐷||1,
(5)
𝑅2𝐷(𝐞𝑘)=||𝛱( FK (𝛽𝛽𝑡,𝜃𝜃𝑠𝑘+𝐞𝑘),𝐊)−𝐏𝑅2𝐷||1.
(6)
Space-time loss optimization. The final motion retargeting loss  combines the source motion appearance with the different shape and constraints of the target character from Eqs. 4, 5, and 6 :

=||𝐖1𝐞||2+𝜆1𝑃(𝐞)+𝜆2𝑅3𝐷(𝐞)+𝜆3𝑅2𝐷(𝐞),
(7)
where the joint parameters to be optimized are 𝐞=[𝐞𝑖+1,…,𝐞𝑖+𝑛]𝑇, n is the number of frames considered in the retargeting window, 𝜆1, 𝜆2, and 𝜆3 are the contributions for the different error terms, and 𝐖1 is a positive diagonal matrix of weights for the motion appearance for each body joint. This weight matrix is set to penalize more errors in joints that are closer to the root joint.

One representative example of the retargeting strategy considering these hybrid 2D/3D constraints is shown in Fig. 5. In this video sequence, the target actor is bigger and taller than the one in the source video (shown in the two first rows of the figure). Notice that the retargeting of the target actor (shown in the third row) results in more bent poses to maintain the human-to-object interactions, and thus the hands’ positions are consistent when adopting our strategy.

Model Rendering and Image Compositing
In our framework’s last step, we combine the rendered target character and the source background to make a convincing final image. We first segment the source image into a background layer using, as a mask, the projection of the actor body model with a dilation, and then the background is inpainted.

Our experiments showed that different inpainting techniques, including end-to-end learning-based methods such as Yu et al. (2019), Yu et al. (2018), Wang et al. (2019), and Xu et al. (2019), displayed small differences in the quality of results, probably because most part of the filled region will be covered by the target person. Thus, we apply the method proposed by Criminisi et al. (2004) that presented the best overall results, especially when the background texture is homogeneous. To ensure temporal smoothness in the inpainting background, we compute the pixel color value as the median value between the neighboring n frames.

In the sequence, we explored the visibility map of the retargeted body model (global geometric information discussed in Sect. 3.2.2) to select the human body parts that better matches the target parts. Since the transformation between the retargeted SMPL model and the estimated SMPL model to the images is known, we apply the same transformation used in the local geometric information to move them to the correct positions. Instead of directly applying 3D warping in the selected images, we use our pre-warping step (texture map) to improve the rendering speed of 3D warping. In order to fill the remaining part holes in the warped image, we explored the median of the accumulated texture 𝑈𝑉 (see Fig. 4). Finally, the background and the target character are combined in the retargeted frame.

Human Retargeting Dataset and Evaluation
Due to the lack of suitable benchmark datasets, recent works on neural view synthesis and body reenacting (Chan et al. 2019; Esser et al. 2018; Wang et al. 2018) only analyze their results in qualitative terms or quantitative terms to self-transfer in which the source and target actors are the same subjects. The provided data is adapted to the requirements of their method setup, and it is hard to perform a comparison with other methods on this data. Cross-transfer is far more complicated than self-subject transfer. First, self-transfer is not affected by body appearance changes (from body shape, clothing). Second, the self-shape transfer does not account for human-to-object interactions or disregards the influence of existing human-environment physical interactions. Moreover, as previously discussed in Sect. 2, existing video retargeting datasets are still rare.

Fig. 6
figure 6
Human retargeting dataset. a The subjects participating in our dataset, their respective height and estimated SMPL body models. b Overview of all motions available in our proposed dataset. c Paired motions (upper and lower rows) with annotated motion constraints (3D constraints in blue and 2D constraints in red). d The reconstructed 3D motions (Color figure online)

Full size image
Human Retargeting Dataset
To evaluate the retargeting and appearance transfer with different actor motions, consistent reconstructed 3D motions, and with human-to-object interactions, we created a new dataset with paired motion sequences from different characters and annotated motion retargeting constraints. For each video sequence, we provide a refined 3D actor reconstructed motion and the actor body shape estimated with Alldieck et al. (2018). The refined reconstructed 3D motions and 2D-3D annotation of interactions were collected by manual annotation. The provided motions are not prone to typical motion artifacts such as bended knees, body shape variations, and camera-to-actor translation changes.

Our carefully designed dataset comprises 8 subjects. To keep the dataset with diversity, we choose participants (subjects S0 to S7) with different gender, sizes, clothing styles, and body shapes. Figure 6 shows the subjects, their respective body models, and labels. We also choose a set of movements that are representative of the problem of the retargeting, increasing the level of difficulty and with different motion constraints. These movements are: “pick up a box”, “spinning”, “jump”, “walk”, “shake hands”, “touch a cone”, “pull down”, and “fusion dance”.

Each actor performed all eight actions. We paired two actors to perform the same motion sequence, where the subjects were instructed to follow marks on the floor to perform the same action, resulting in four paired videos per action (a total of 32 paired videos). Then, we define the combination of actors aiming at the most challenging configuration for the task of human retargeting motion. For instance, actors S0, S2, S4, and S6 were paired respectively with S1, S3, S5, and S7. We also provide, for each subject, three videos: one video where the subject is rotating and holding an A-pose, a four-minute video where the subject is performing different poses, and a 15-second video where the subject is dancing. All videos were recorded with 1,000×1,080 of resolution at 30FPS. This information allows training most existing approaches for evaluation. Figure 6 shows some examples of frames from our dataset.

Additionally, we provide three publicly available videos acquired under uncontrolled conditions: joao-pedro, tom-cruise, and bruno-mars sequences. The bruno-mars sequence was also adopted in the experiments of Chan et al. (2019). The provided videos present more general conditions to analyze the effectiveness of reenacting and retargeting methods in the wild with more complex motions, background, and varying illumination.

Protocol
The evaluation often employed by works on character reenacting/synthesis (Aberman et al. 2018; Chan et al. 2019; Liu et al. 2019; Sun et al. 2020) consists in setting the source character equal to the target character. However, we argue that this protocol is used because of the absence of paired motion sequences, as also noted in Liu et al. (2019) and Sun et al. (2020). While this protocol might be appropriate to assess new synthesized view/pose in the same scene background, we state that it is not appropriate for evaluating the synthesis of new videos of people when taking into account motion constraints (e.g., human-to-object interactions), distinct shapes and heights, and transferring them in different backgrounds where they were initially recorded.

Therefore, we run our evaluation protocol as follows: when evaluating a new synthesized video, we place the target actor performing a similar motion to the source actor (all physical constraints and the human-to-object interactions are taken into account). Then, we move the target actor to a different scenario configuration to perform the retargeting. If the retargeting is successfully executed, the method will place the target actor moving as the source actor into the source actor’s scenario.

Evaluation Metrics
We measure the quality of the synthesized frames in terms of the following metrics: (i) the structural similarity (SSIM) (Wang et al. 2004) that compares local patterns of pixel intensities normalized for luminance and contrast. SSIM assumes that human visual perception is highly adapted for extracting structural information from objects; (ii) learned perceptual similarity (LPIPS) (Zhang et al. 2018), which provides a deep neural learned similarity distance closer to human visual perception; (iii) the mean squared error (MSE) that is computed by averaging the squared intensity differences between the pixels; and iv) Fréchet video distance (FVD) (Unterthiner et al. 2019), which was designed to capture the quality of the synthesized frames and their temporal coherence in videos. These are widely used perceptual distances to measure how similar the synthesized images are in a way that coincides with human judgment.

To properly evaluate the quality of retargeting, it is required a paired video where the target character is the same as the source video sequence. Collecting real paired motion sequences from different characters is a challenging task, even when the movement of the actors is carefully predefined and synchronized. For instance, each actor has a movement style that can result in videos with unsynchronized actions (as seen in the third column in Fig. 6c). Thus, to make possible the computation of quantitative metrics, we relax the assumption that the frame k in the synthesized video must be the frame k in the paired video by applying a small window of acceptance around k, i.e., we evaluate the quality of one synthesized frame as a better answer between the frame and a small window of frames in the paired video. The window of acceptance w is estimated for each pair of videos according to:

𝑤(𝑉1,𝑉2)=max(15,2×(|𝑙𝑒𝑛(𝑉1)−𝑙𝑒𝑛(𝑉2)|),
(8)
where len(V) is the number of frames in the video V. Equation 8 captures how much two videos are not synchronized allowing the synthesized frames to match with the paired video. The lower bound value of 15 frames was empirically selected by a visual analysis of our videos.

Aside from the image quality metrics, we also propose to evaluate the approaches with fake image detectors. Since our dataset provides paired motion sequences, we can evaluate the retargeted frames’ quality and realism with an image forgery detector. We applied the detector in the generated and real frames from the sequence where the same subject performs the retargeted motion. The retargeted frames were generated using motion extracted from source videos whose subject performing the motion is different from the target subject.

We adopted the image forgery detection algorithm presented by Marra et al. (2020) to evaluate all methods in our experiments. The Marra et al.’s method evaluates images by detecting pixel artifacts, which is a common metric for detecting CNN-generated images. We remark that we tested different image forgery detection algorithms such as the method proposed by Wang et al. (2020), but the results were inconclusive, which might be because of the high resolution of our images. Furthermore, our images are not only generated by CNN methods, and as the authors stated, their method performs at chance in visual fakes produced by image-based rendering or classical commercial rendering engines. Finally, we define the forgery performance as the difference between the probability of the paired real image and its respective synthetic frame of being classified as fake. This difference indicates how far a synthetic frame is from being recognized as fake with a similar result of a real image.

Experiments and Results
Parameter settings. We selected a fixed set of parameters for all experiments. We applied a grid search over a portion of the 4-minutes videos training examples of the dataset. The grid search 𝛾 values ranged from 5 to 100 with a step of 5, and 𝜆 from 1 to 10, with a step of 1. In the human motion estimation and retargeting steps, we used 𝛾=10, 𝜆1=5, 𝜆2=1, and 𝜆3=1. We minimize the retargeting loss function with Adam optimizer using 300 iterations, learning rate 0.01, 𝛽1=0.9, and 𝛽2=0.99. To deform the target body model with the semantic contour control points, we employed the default parameters proposed by Levi and Gotsman (2015), and we fed the method with eight images taken from different viewpoints of the actor (see Fig. 2). For a complete comparison, we also present qualitative results of the new frames from the state-of-art approaches by replacing the generated background regions from the methods by the source video background.

Table 1 Ablation study
Full size table
Table 2 Quantitative ablation analysis of the retargeting
Full size table
Fig. 7
figure 7
Qualitative ablation analysis of the retargeting. Top row: source video containing the actor S3. Middle row: transferring results to the actor S7 without the physical interactions. Bottom row: transferring results of our method with 2D–3D interactions

Full size image
Ablation Study
To verify that the space-time motion transfer optimization, motion regularization, semantic-guided deformation, and visibility maps contribute to our approach’s success in producing more realistic frames, we conducted several ablation analysis using the motion sequences from our dataset.

Table 1 shows the results of our ablation study in terms of MSE and FVD of five ablated versions of our method. We draw the following observations. First, the best result is achieved when the full method is applied. Second, removing the 2D/3D constraints reduces the performance in terms of MSE. These constraints play a key role in the compliance of the poses to motion constraints. By removing them, large fragments from the background are computed as part of the retarget character body when computing MSE using the paired video, leading to the worst MSE value. Third, without the shape-aware regularization, which hinders the temporal coherence, the model presents the worst value of FVD. We can also see that after removing the semantic guidance, which decreases the quality of the texture applied onto the 3D model, the frames have more artifacts, and the model also performs poorly in terms of FVD.

The results of a more detailed performance assessment of the effects from motion constraints in the video retargeting are shown in Table 2. The error between the computed position of the end-effectors and the target positions (motion constraints from the human-to-object interactions) is significantly smaller for all motion sequences and pairs of actors when applying our retargeting strategy. Some frames are shown in Fig. 7 to illustrate the created visual artifacts when not considering the motion constraints. In this setup, the source actor (top row) is taller than the target (bottom row), and the target actor does not touch the floor nor touch with her hands the cone without the hybrid motion constraints (middle row). Conversely, these features are kept when considering the 2D/3D human-to-object losses in the retargeting. Another representative example of the retargeted motion trajectory over time, with one shorter actor interacting with a box, is shown in Fig. 8. Please notice the smooth motion adaptation produced by the retargeting with the restrictions in frames 47 and 138 (green line) when the character’s left hand is touching the box. Additionally, these results illustrate that our method is able to impose different space-time human-to-object interactions and motion constraints in the retargeting.

Fig. 8
figure 8
Bold indicates the best valuesRetargeted trajectory with motion constraints. The curves show the left hand’s trajectory on the y-axis when transferring the motion of picking up a box between two differently sized characters: original motion (blue line), a naïve transfer without constraints at the person’s hand (red line), and with constraints (green line). Frames containing motion constraints are located between the red circles (Color figure online)

Full size image
Table 3 shows the forgery performance when synthesizing the frames after removing the visibility map extraction and the semantic-guided human model extraction. We can see that these two steps significantly enhance the quality of the results to a point where the detector returns for the “shake hands” sequence a probability of 29.24 higher when removing these two components.

Table 3 Visibility maps and semantic-guidance analysis
Full size table
Comparison with Previous Approaches
Table 4 Comparison with state of the art
Full size table
We compare our method against four recent representative methods with different assumptions, including V-Unet (Esser et al. 2018), vid2vid (Wang et al. 2018), EBDN (Chan et al. 2019), and iPER (Liu et al. 2019). V-Unet is a notorious representative of image-to-image translation methods using conditional variational autoencoders to generate images based only on a 2D skeleton and an image from the target actor. Similar to V-Unet, iPER is not dataset-specific, but it is a generative model trained in an adversarial manner. Vid2vid and EBDN methods, for their turn, are dataset-specific methods, i.e., they require training a GAN for several days over one video of the target subject in a large set of different poses.

Processing time. Although vid2vid and EBDN required a few seconds to generate a new frame, the training step of vid2vid spent approximately 10 days on an NVIDIA Titan XP GPU for each target subject, and to run the fine-tuning of the EBDN took approximately 4 days to complete all stages for each subject. On the other hand, our retargeting approach does not need to be trained. The significant parts of our method’s processing time are the retargeting optimization, the deformation, and the model rendering. On an Intel Core i7-7700 CPU and NVIDIA Titan XP GPU, the average run-time for one frame of retargeting optimization was about 1.2 seconds, including I/O. The deformation took around 12 min on 8 frames. The model rendering, i.e., selecting the best texture map considering the visibility map, warping the selected parts, and filling all the holes in the texture, took about 30 seconds per frame. Thus, the total processing time t(N) in seconds to run our method on a video with N frames with a resolution of 1,920×1,080 is approximately 𝑡(𝑁)=1.2×𝑁+720+30×𝑁.

Fig. 9
figure 9
Motion analysis in the dataset sequences. Transferring results considering the cases where the person is not standing parallel to the image plane or has the arms in front of the face. In each sequence: the first row shows the worst generated frame for each method and the second row presents the best generated frame for each method

Full size image
Quantitative Analysis
We performed the video retargeting in all sequences in our dataset, including several public dance videos also adopted in the works Chan et al. (2019) and Liu et al. (2019). Table 4 shows the comparison of our approach in the dataset considering the motion types and pair of actors. We can see that despite not being dataset-specific, V-Unet and iPER did not perform well when the reference image is not from their datasets. One can see that our method outperforms, on average, all methods in considering SSIM, LPIPS, MSE, and FVD metrics. Regarding the experiments considering the pairs of actors, our approach also achieved the best average results in all metrics. In particular, our method presented better results when the subjects have different heights (S0-S1 and S6-S7). We ascribe this performance to our method being aware of the shape and physical interactions, which allows it to correct the person’s position when the source actor is taller or smaller than the target person.

We also evaluated our results (in terms of SSIM, LIPIPS, and MSE metrics) with the Wilcoxon Signed-Rank Test to check whether the difference between our results’ central tendency and the baselines are statistically significant. For all baselines, except vid2vid, the test rejects the null hypothesis. In other words, the test indicates that the samples were drawn from a population with different distributions and the differences between the metrics indicate that our method performed better statistically. Moreover, it is noteworthy that the sequences “spinning” and “fusion dance” are challenging for all methods, including our methodology that was affected by wrong pose estimations. Our approach was slightly outperformed by vid2vid only in these two sequences.

Table 5 Forgery performance
Full size table
Table 5 shows the results for the experiments on image forgery detection. These experiments indicate that the fake detector in the frames generated by our method has the closest performances to real images. For instance, in the “shake hands” sequence, the probability of a frame synthesized by our method to be fake is only 5.67% higher than when applying the detector to the respective real frame.

Qualitative Analysis
We evaluated the capability of our method to transfer motion and appearance and retain interactions of the original motion despite the target actor having different proportions to the actor in the source video.

Fig. 10
figure 10
Qualitative evaluation to bruno-mars sequence. First row: original video and target actor S5; Second row: Result of vid2vid and their compositing with the target background; Third row: Results of EBDN and their compositing with the target background; Fourth row: Our results for both backgrounds

Full size image
Some frames used to compute the metrics in Table 4 are shown in Fig. 9. One can note the large discrepancy between the quality of the frames for the same method. As shown in Figs. 9 and 10 , the end-to-end learning techniques have impressive results when the person is standing parallel to the image plane and the arms are not in front of the face; however, these methods perform poorly when the person is out of these contexts, such as when bending. Our method, for its turn, retains the same quality for most poses.

Fig. 11
figure 11
Qualitative evaluation to tom-cruise sequence. First row: original video and target actor S2; Second row: Result of vid2vid and their compositing with the target background; Third row: Result of EBDN and their compositing with the target background; Fourth row: Our results for both backgrounds

Full size image
We also analyzed the impact of the camera pose and the actor’s scale in the quality of the resulting videos of the methods. We transferred two target persons from our dataset to two videos with different camera setups and image resolutions: “bruno-mars” and “tom-cruise” sequences. In the “bruno-mars” sequence, shown in Fig. 10, we ran vid2vid and EBDN using their respective solution to tackle with different image resolutions and actors with different proportions from the training data. Vid2vid’s strategy (scaling to keep the aspect ratio and then cropping the image) results in an actor with different proportions compared with the training data, which leads to a degradation of quality. EBDN’s strategy (pose normalization, scaling to keep the aspect ratio, and then cropping the image) keeps the similarity between the input video and training data, but body and face occlusions are still problems. The red squares highlight these issues on the right side in Fig. 10, where EBDN and vid2vid reconstructed poorly the target’s face. These strategies also incur a loss of the relative position in the original image; thus, whenever the actor presents a large translation, he/she can stay out of the crop area. Fig. 11 depicts the results of “tom-cruise” sequence. In this sequence, the source actor has a large translation in all directions; thus, we include a margin in the image to allow EBDN and vid2vid to process it. Nevertheless, the difference in the actor’s scale results in low quality for vid2vid and EBDN, which suggests their lack of generalization of these methods to different poses, and changes in camera viewpoint and intrinsic parameters. On its turn, our method did not suffer from problems caused by the camera and image resolution, and provided the best scores and visual results.

Results in the iPER Dataset
Aside from our dataset, we also evaluated our approach using the iPER dataset (Liu et al. 2019). Since the iPER dataset does not provide paired motions as our dataset, we evaluated our method according to Liu et al. (2019)’s protocol, where SSIM and LPIPS are used to measure self-imitation transferring. We also include FVD in the evaluation, which was designed to capture the temporal coherence among videos and frame quality.

Table 6 shows the transferring results in the iPER dataset. It can be seen that our method outperforms iPER in terms of SSIM and LPIPS metrics. This result also concurs with the visual results shown in Fig. 12. We can also note that iPER achieved the best FVD value, since they are tested now in the same context where it was trained. However, it still performs poorly when the person is not standing up straight, as indicated in the facial zoom (red box in Fig. 12). Our method retains the same quality for most poses.

Table 6 Comparison with iPER in their proposed dataset
Full size table
Fig. 12
figure 12
Qualitative evaluation on a sequence from the iPER dataset. First row: target actor and original video; Second row: Results of iPER network; Third row: Our results

Full size image
Applications and Limitations
In our experiments, we observed that image-based rendering and 3D reasoning still have several advantages regarding end-to-end learning human view synthesis, particularly in terms of control and generalizing to furthest views. Our method enables the replacement of the background, which is crucial for many applications. Moreover, it also allows to create a deformed model from images that can be included in virtual scenes using existing rendering frameworks, such as Blender. Figure 13 illustrates an application of this capability.

Although achieving the best results in these more generic test conditions, the proposed approach also suffers from certain limitations. Ideally, the constrained optimization problem would maintain the main features of the source motion. However, there is no single solution to a set of constraints, which can result in undesired motions, as shown in Fig. 14, where the actor positions his hand down and curve his back instead of bending his knees. The textured avatars may exhibit artifacts in the presence of part segmentation estimation errors, which can also lead to wrong deformations, and errors in the deformation result in body parts with unreal shapes. Typical failure cases such as retargeting and segmentation errors are also depicted in Fig. 14.

Regarding our method’s processing time, our current implementation is a modular Python code, but without any optimization, i.e., the code was not parallelized either and was not adapted to run fully on a GPU. However, we highlight the available room for speeding up the processing time with a parallel implementation of different parts of the approach as, for instance, in the deformation model steps, which currently require 30 seconds per frame. They could be easily adapted to be executed in parallel with multiprocessing.

Fig. 13
figure 13
Model-to-virtual. Our method is able to provide a deformed model from images that can be included in virtual scenes using existing graphic rendering tools, such as Blender. The partial occlusions between the scene and model are handled in a natural way (red squares) (Color figure online)

Full size image
Fig. 14
figure 14
Limitations. Top: Retargeting resulting in undesired motion where the actor positions his hand down and curves his back instead of bending his knees. Bottom: Typical failure cases in the avatar: artifacts in the texture and body parts with unreal shapes

Full size image
Finally, in the proposed dataset, all videos were selected and designed to stress the evaluation of the human motion and appearance transfer capabilities of the methods for several paired movements of different actors. We adopted a simpler background to avoid eventual errors that could be added during the evaluation from the foreground-background segmentation (which is done before inpainting). This choice is meant to allow a better assessment of each method’s capabilities to transfer actors’ motions and appearance with different morphologies. However, there is still a lack of paired videos with complex backgrounds to assess the quality of the human retargeting and the background synthesis together.

Conclusions
This paper proposes a new model-based retargeting methodology that incorporates different strategies to extract 3D shape, pose, and appearance to transfer motions between two real human characters using the information from monocular videos. Unlike retargeting methods that use either appearance information or motion information only, our approach simultaneously considers the critical factors to retargeting such as pose, shape, appearance, and motion features.

Furthermore, we have introduced a new dataset comprising different videos with annotated human-to-object interactions, actors with different body shapes, and paired motions to evaluate the task of retargeting humans in videos. We believe that these videos can be used as a common base to improve tracking the field’s progress by showing where current approaches might fail and measuring the quality of future proposals.

Despite the impressive advances in image-to-image translation approaches, we presented several contexts where most end-to-end image translation methods perform poorly. Yet, our experiments show that a model-based rendering technique can still exhibit a competitive quality compared to recent end-to-end learning techniques, besides having several advantages in terms of control and ability to generalize to furthest views. Our results indicate that retargeting strategies based on image-to-image learning are still challenged to retarget motions while keeping the desired movement constraints, shape, and appearance simultaneously. These findings suggest the potential of hybrid strategies by leveraging the advantages provided by model-based retargeting into recent neural rendering approaches.