Abstract
The ready accessibility of high-resolution image sensors has stimulated interest in increasing depth resolution by leveraging paired color information as guidance. Nevertheless, how to effectively exploit the depth and color features to achieve a desired depth super-resolution effect remains challenging. In this paper, we propose a novel depth super-resolution method called CODON, which orchestrates cross-domain attentive features to address this problem. Specifically, we devise two essential modules: the recursive multi-scale convolutional module (RMC) and the cross-domain attention conciliation module (CAC). RMC discovers detailed color and depth features by sequentially stacking weight-shared multi-scale convolutional layers, in order to deepen and widen the network at low-complexity. CAC calculates conciliated attention from both domains and uses it as shared guidance to enhance the edges in depth feature while suppressing textures in color feature. Then, the jointly conciliated attentive features are combined and fed into a RMC prediction branch to reconstruct the high-resolution depth image. Extensive experiments on several popular benchmark datasets including Middlebury, New Tsukuba, Sintel, and NYU-V2, demonstrate the superiority of our proposed CODON over representative state-of-the-art methods.

Access provided by University of Auckland Library

Introduction
The recent development of depth measurement technologies, such as LiDAR, time-of-flight (TOF) cameras, and structured-light 3D scanning has meant that depth images are now widely used in several fields including mobile robots, human-machine interfaces, human pose estimation, and 3D scene reconstruction (SupanÄiÄ et al., 2018; Qi et al., 2018; Wang et al., 2019; Zou et al., 2019; Zhou et al., 2019). However, depth images captured using these techniques do not always meet real-life demands, especially in terms of their (low) resolution. Therefore, how to reconstruct high-quality, high-resolution depth images from low-resolution inputs, termed depth image super-resolution, has become a major research focus.

Most previous methods handle depth image super-resolution by taking a color (or grayscale) image of a scene using additional information to guide super-resolution reconstruction of the depth image; this is known as color-guided depth image super-resolution. Conventional color-guided depth super-resolution methods (Diebel & Thrun 2006; Li et al., 2012; Kiechle et al., 2013; Ferstl et al., 2013; Kwon et al., 2015; Lu & Forsyth 2015; Ham et al., 2015) iteratively reconstruct the high-resolution depth image using different constraints obtained from the high-resolution color image. Although these conventional methods have improved results, they have a common limitation: extra texture information is mis-mapped to the depth result where the depth regions are smooth and the corresponding color areas are rich in texture.

Convolutional neural networks (CNNs) have significantly improved many computer vision tasks including color-guided depth super-resolution. Li et al. (2016) proposed a lightweight joint image filter based on CNN, which can selectively transfer salient and consistent structures in color and depth images. Zhou et al. (2017) proposed a deep fully convolutional two-branch network by auto-merging feature maps from depth and color branches. Zhu et al. (2017) proposed a deep residual two-branch network based on deep fusion and local linear regularization, which can effectively extract the correlation between depth and color images in the deep feature space. Hui et al. (2016) proposed a multi-scale guided convolutional network (MSG-Net) with multiple level fusing between depth and color branches, which can better adapt for upsampling of both fine- and large-scale structures. Recently, Lutio et al. (2019) proposed to learn a pixel-to-pixel mapping from the color image to depth domain by a multi-layer perceptron. Wen et al. (2019) devised a data-driven filter to extract guiding information from color image and then a coarse-to-fine CNN with different kernel sizes is used to reconstruct the high-resolution depth image.

However, due to diverse local structures (smooth areas, edges, textures) in the guidance color image and depth image, it is still difficult to achieve strong representation learning capability. Although the inter-layer multi-scale features from the cascaded convolutional layers shows to be useful guidance as evident in the above methods (Hui et al., 2016; Wen et al., 2019), learning the intra-layer multi-scale features to adapt to the diverse local structures is still under-explored. Indeed, deeper and wider networks do usually achieve better results, but they often lead to a surge in model parameters and computational costs. Hence, devising efficient neural modules with small amount of parameters and low computational complexity to learn discriminative features adapting to various local structures for depth super-resolution remains challenge.

Fig. 1
figure 1
Super-resolution results of Art from Middlebury Dataset, for an upsampling factor 4Ã—. a Ground truth; b result of bicubic; c result of MSG-Net (Hui et al., 2016); d result of our CODON

Full size image
Moreover, although the depth and color domains have very important complementary information, the domain discrepancy between them places a significant burden for learning effective guidance features and consequently cripples the super-resolution performance. Directly concatenating depth and color features at single or multiple levels and treating them equally in channel and spatial domains like these current CNN-based methods may transfer the high-frequency textures into the depth result and blur the depth edges, as can be found in the results by Lutio et al. (2019). How to orchestrate the depth and color features to maximize the use of complementary information in both domains is the key to reducing the domain discrepancy and improving the depth super-resolution performance, which however remains an open challenge.

To address these issues, we propose a novel CrOss-Domain Orchestration Network for depth super-resolution, which we call CODON for short in this paper. CODON has a dual-branch structure to orchestrate cross-domain attentive features from both the depth and color domains. First, we devise a recursive multi-scale convolutional module (RMC) to exploit the intra-layer and inter-layer multi-scale features to adapt to the various local structures in both domains. It is lightweight by recursively stacking weight-shared convolutional layers. Then, we devise a cross-domain attention conciliation module (CAC) to calculate conciliated attention by modeling the co-occurrence correlations between depth and color features. It is used as shared guidance to enhance the edge structures in depth feature while suppressing textures in color feature. RMC and CAC collaboratively accomplish the feature conciliation process step-by-step in a multi-level interactive manner. Eventually, the orchestrated features are fed into a RMC prediction branch to achieve high-resolution depth image reconstruction. Experimental results on several popular datasets demonstrate that CODON outperforms other state-of-the-art methods. The contributions of this study can be summarized as:

A novel cross-domain orchestration network (CODON) for depth super-resolution is proposed, which elaborately orchestrates the complementary depth and color features with multi-level interactive dual-branch structure to reconstruct high-resolution depth image. Experimental results on various datasets demonstrate its effectiveness.

A recursive multi-scale convolutional residual module (RMC) is devised to deal with the scale variance of scene structures by recursively stacking parallel convolutional layers with different receptive fields, which can enrich the feature diversity while reducing the network parameters.

A novel cross-domain attention conciliation module (CAC) is devised to guide the super-resolution networkâ€™s focus by effectively modeling co-occurrence correlations between color and depth features, which can greatly enhance depth boundaries and suppress color textures at the same time.

The remainder of the paper is organized as follows. Section 2 introduces related work. In Sect. 3, we illustrate the proposed CODON method, including the cross-domain orchestration structure, RMC module, and CAC module. We detail our experimental results in Sect. 4, which justify our approach. Finally, we conclude in Sect. 5.

Related Work
Benefiting from CNNâ€™s powerful representation capabilities, many CNN-based approaches have been proposed for color image super-resolution (Zhang et al., 2018a, c, 2019, 2020; Buades et al., 2019; Li et al., 2018; Dong et al., 2015). For depth super-resolution task, it is convenient to obtain a high-resolution color image of the same view as the low-resolution depth image using RGB-D sensors. In contrast to single branch networks for color image super-resolution, most of the CNN-based depth super-resolution methods (Hui et al., 2016; Zhou et al., 2017; Lutio et al., 2019) applied dual-branch structure to utilize the color guidance information, in order to improve the super-resolution performance. But, these methods directly concatenated color and depth features, which placed a significant burden for learning effective guidance features because of the domain discrepancy between depth and color. Different from these methods, a novel multi-level interactive dual-branch structure for depth super-resolution is designed in this paper, which orchestrates the depth and color features to maximize the use of complementary information in both domains.

Recursive learning has been exploited in color image super-resolution. Kim et al. (2016b) designed 16 convolutional recursion layers with shared weights (DRCN). However, as the network deepened, DRCN needed to supervise every recursion to avoid vanishing/exploding gradients, which hampered network training. Tai et al. (2017) proposed a recursive block consisting of 25 residual units with shared weights (DRRN). But, the residual unit of DRRN only used single-sized convolutional kernel, which limited the representation capacity of the feature maps. Recently, multi-scale convolution has also been widely studied. Inception modules (Szegedy et al., 2016) were proposed to increase the width of the network using parallel multi-scale convolutional layers. Mustafa et al. (2019) also devised a multi-scale convolution block (MSCB) to enhance the representation capacity. But, directly stacking these multi-scale modules will cause a rapid increase in the number of parameters. Wei et al. (2018) proposed a multi-scale recursive convolutional neural network, where the multi-scale module and recursive unit are separated, i.e. only the output of the multi-scale module is recursively convolved. In contrast to the above methods, the proposed RMC module integrates the advantage of recursion and multi-scale, i.e. recursively stacking multi-scale modules by sharing the kernel weights. In this way, different paths through these recursive modules have different receptive fields so can better deal with variable scales of depth structures. Therefore, the RMC module can greatly enrich feature diversity and enhance representation capacity whilst simultaneously reducing the network parameters.

Fig. 2
figure 2
The illustration of our CODON. â€˜Convâ€™, â€˜MCâ€™, â€˜RMCâ€™ and â€˜CACâ€™ denote 3Ã—3 convolutional layer, multi-scale convolutional unit, recursive multi-scale convolutional module and cross-domain attention conciliation module respectively. ReLU (Nair & Hinton 2010) is applied as the activation layer except the last 3Ã—3 convolutional layer

Full size image
Attention mechanisms are designed to distinguish and focus on the important information. Several recent works have integrated the attention mechanism into CNNs. Wang et al. (2017) introduced a residual attention network which used trunk and mask attention branches for general classification. Hu et al. (2018) proposed a â€˜squeeze-and-excitationâ€™ structure to recalibrate features and pay more attention to information-rich features during image classification. Woo et al. (2018) proposed a lightweight convolutional block attention module (CBAM) that included channel attention and spatial attention in sequence. Zhang et al. (2018b) proposed a deep residual channel attention network (RCAN) to exploit interdependencies between feature channels. In contrast to these methods, we devise the CAC module to specifically model co-occurrence correlations between color and depth features and achieve the feature conciliation in both domains. By guiding the network to focus on the structural edges, CAC effectively enhances the structural edges in depth feature while suppressing textures in color feature, which greatly reduces the domain discrepancy between depth and color features.

Orchestrating Depth and Color Features
The depth and color feature domains have very important complementary information. The high-quality structural edges in the color image are critical to enhance the depth edges. At the same time, the smooth regions in the depth image are helpful to suppress the color textures. Hence how to orchestrate the depth and color features to maximize the use of complementary information in both domains is the key to improving the depth super-resolution performance. To address this problem, a novel CrOss-Domain Orchestration Network (CODON) is proposed in this paper. The CODON architecture as shown in Fig. 2, has two main phases: a cross-domain feature conciliation phase and a depth reconstruction phase.

In the feature conciliation phase, a novel multi-level interactive dual-branch structure is constructed. Specifically, we design RMC to deal with the diverse local statistics by recursively stacking parallel convolutional layers with different receptive fields, which enriches the feature diversity whilst simultaneously reducing the network parameters. Moreover, we design CAC to guide the network to focus on modeling the co-occurrence correlations between the color and depth features at multiple levels, attending to useful co-occurring structural edges. In addition, the RMC module efficiently helps the CAC module to progressively optimize the learned attentive features in both domains.

After feature conciliation phase, our CODON not only suppresses the annoying texture of the color features but also provides more detailed structural information for depth features. In the depth reconstruction phase, the conciliated depth and color features are combined and fed into the RMC prediction branch to reconstruct the high-resolution depth images. Thanks to the guidance from the orchestrated features, the network is able to recover sharp depth boundaries and fine details.

Specifically, the CODON is constructed to learn an end-to-end mapping relationship between the low-resolution depth image ğ¼ğ·ğ¿ğ‘…, the high-resolution color image ğ¼ğ¶ğ»ğ‘…, and the high-resolution depth image ğ¼ğ·ğºğ‘‡. An initial depth image ğ¼ğ·ğ¼ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™ with the same resolution as the color image is obtained using bicubic interpolation. Since the difference between the interpolated depth image and its high-resolution ground truth is mostly in the high-frequency edge area, we adopt the residual learning idea to learn the residual and therefore ease the training process by following the common practice in previous deep super-resolution methods. We transform the RGB image to the YCbCr space and use the Y channel for training. Finally, given a dataset {(ğ¼ğ‘–ğ·ğ¼ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™,ğ¼ğ‘–ğ¶ğ»ğ‘…),ğ¼ğ‘–ğ·ğºğ‘‡}ğ‘ğ‘–=1, our final goal is to train our network to find a set of optimal parameters:

ğ›©Ì‚ =argminğ›©1ğ‘âˆ‘ğ‘–=1ğ‘ğ›¤ğ‘†ğ‘…(ğ‘“ğ›©(ğ¼ğ‘–ğ·ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™,ğ¼ğ‘–ğ¶ğ»ğ‘…)âˆ’ğ¼ğ‘–ğ·ğºğ‘‡),
(1)
where ğ‘“ğ›©(â‹…) are the function of our CODON, ğ›©Ì‚  is the network parameters of CODON, N represents the number of training samples, and ğ›¤ğ‘†ğ‘…(â‹…) denotes the loss function used to measure the difference between our super-resolution reconstructed depth image ğ¼ğ·ğ‘†ğ‘…ğ‘– and the ground-truth depth image ğ¼ğ·ğºğ‘‡ğ‘–.

Recursive Multi-scale Convolutional Module
It is urgently needed to increase the representation ability of the network to learn more detailed structural features, which is critical for depth super-resolution task. In this paper, an efficient recursive multi-scale convolutional module (RMC) is developed in our network. Each RMC module consists of several multi-scale convolutional units (MC) designed to enrich the feature diversity. The structure of our MC unit is shown in Fig. 3.

To increase the representation capacity of the network, we can increase the network depth or width. Dilated convolution can enlarge the receptive field without adding additional parameters, but this loses the information continuity critical for super-resolution. As shown in Fig. 3, to enrich feature image diversity and broaden the network, our MC unit applies two convolutional kernels of different scales arranged in parallel, which effectively deals with the variable scales of scene objects and enriches the feature diversity. As shown in Fig. 4, the feature maps of 3Ã—3 convolutional blocks can focus on small regions, and the feature maps of 5Ã—5 convolutional blocks can cover larger regions.

Fig. 3
figure 3
Framework of our MC unit. Note that B is batch size, H is image height and W is image width

Full size image
Fig. 4
figure 4
The feature map of MC units in the first stage: a ground-truth depth images; b feature map of 3Ã—3 convolutional block; c feature map of 5Ã—5 convolutional block. Note that MC can obtain different scaled features and enrich the feature diversity

Full size image
Hence, we can obtain different scaled features by concatenating the two paths and reducing the dimension by 1Ã—1 convolutional layers, such that the residual manner between input and output can be applied. Moreover, various receptive fields can be obtained in our MC units at different stages, which can greatly enrich the feature diversity and enhance the networkâ€™s representation capabilities. We call the multi-scale features learned by the convolutional layers with different kernel sizes in a MC block as intra-layer multi-scale features.

The operation of MC unit can be formulated as follows:

â§â©â¨âªâªâªâªğ‘…1=ğœ”13Ã—3âˆ—ğœ(ğ‘‹ğ‘šâˆ’1)ğ‘ƒ1=ğœ”15Ã—5âˆ—ğœ(ğ‘‹ğ‘šâˆ’1)ğ‘…2=ğœ”25Ã—5âˆ—ğœ(âŸ¨ğ‘…1,ğ‘ƒ1âŸ©)ğ‘‹ğ‘š=ğœ”31Ã—1âˆ—ğ‘…2+ğ‘‹ğ‘šâˆ’1,
(2)
where ğ‘‹ğ‘šâˆ’1 and ğ‘‹ğ‘š denote the input and output of MC unit, ğœ” denotes the weights, and the superscript and subscript indicate the layer location of the convolution layer and the size of the convolution kernel, respectively. ğœ(â‹…) is the ReLU function, and âŸ¨ğ‘…1,ğ‘ƒ1âŸ© represents the feature channel concatenation operation.

However, stacking such a unit directly would make the network computationally expensive, is extremely demanding of the hardware, and is difficult to train under normal conditions. To solve this problem of complex input parameters, the RMC block stacks the MC as a recursive unit using recursive learning, as shown in Fig. 5.

Specifically, let N represent the number of RMC and M represent the number of MC in each RMC. As shown in Fig. 5, the depth of the network d is equal to 2Ã—ğ‘€Ã—ğ‘. According to Eq. (2), we define RMC as follows:

ğ‘‹ğ‘šğ‘›=ğ›¹(ğ‘‹ğ‘šâˆ’1ğ‘›)=ğ›·(ğ‘‹ğ‘šâˆ’1ğ‘›,ğ‘Šğ‘›)+ğ‘‹0,
(3)
where the subscript ğ‘šâˆˆ{1,â€¦ğ‘€} , the superscript ğ‘›âˆˆ{1,â€¦ğ‘}, ğ›¹(â‹…) is the MC function, ğ‘‹ğ‘šn is the output of the m-th MC in the n-th RMC, ğ›·(â‹…) is a residual connection, and ğ‘‹0 is the network input. The number of parameters remains the same when adding more MC units in one RMC, as the weights ğ‘Šğ‘› of the convolutional layers between each MC unit are shared within one RMC; however, parameters vary between different RMC modules. Then, we can obtain the formula for the output of the n-th RMC:

ğ‘‹ğ‘›=ğ‘‹ğ‘€n=ğ›¹(ğ›¹(...ğ›¹(ğ‘‹0))).
(4)
Fig. 5
figure 5
Framework of our RMC module. Note that the input is connected to the output of each MC unit in the RMC by residual manner

Full size image
In this way, the MC block can efficiently learn residual features in a progressive manner, where the subsequent blocks only need to adjust a little to get a more feasible residual feature representation given the output residual from the previous block. Indeed, it divides the task of learning the residual feature maps into several sequential sub-tasks of learning the small residuals recursively and accumulating them together implicitly, which is enabled by the recursive learning and residual learning. In addition, using two convolutions of different kernel size and recursively stacking them together via residual connections can implicitly construct an ensemble of multiple paths, where each of them goes through a certain part in the RMC and bypasses the remaining parts, resulting in multiple receptive fields. We call the multi-scale features learned by the combinations of different MC blocks as inter-layer multi-scale features. In this way, the representation capacity of RMC can be enhanced. Moreover, sharing weights among different MC units can significantly reduce the number of parameters.

Cross-Domain Attention Conciliation Module
Moreover, to orchestrate the cross-domain complementary depth and color features effectively, we propose a novel CAC module to generate cross-domain conciliated attention maps between the depth and color branches at different levels. In such a conciliated attention structure, the depth edges are enhanced because of the high-quality structural edges in the color feature and the color textures are suppressed by the smooth regions in the depth feature. At the same time, the gap between the two domains is greatly reduced. Specifically, we insert the CAC module after the depth and color branch MC units to generate the cross-domain conciliated attention map. Then, the output attentive features of the CAC module are used as the input of the next MC units in cascading way.

As shown in Fig. 6, the depth branch features ğ¹ğ·âˆˆğ‘…ğµÃ—ğ»Ã—ğ‘ŠÃ—64 and color branch features ğ¹ğ¶âˆˆğ‘…ğµÃ—ğ»Ã—ğ‘ŠÃ—64 are concatenated as the input of our CAC module. The cross domain attention map ğ‘€ğ· of ğµÃ—ğ»Ã—ğ‘ŠÃ—64 shape is generated using our CAC structure. The function of CAC in the designed networks can be formulated as follows:

â§â©â¨âªâªğ¹ğ‘–ğ¶at=âŸ¨ğ¹ğ‘–ğ·,ğ¹ğ‘–ğ¶âŸ©ğ¹â€²ğ‘–ğ·=ğ‘“ğ‘–(ğ¹ğ‘–ğ¶at)âŠ—ğ¹ğ‘–ğ·ğ¹â€²ğ‘–ğ¶=ğ‘“ğ‘–(ğ¹ğ‘–ğ¶at)âŠ—ğ¹ğ‘–ğ¶,
(5)
where ğ¹ğ‘–ğ· and ğ¹ğ‘–ğ¶ are the depth and color features in both branches, ğ¹ğ‘–ğ¶at is the concatenated feature, the superscript ğ‘–={1,â€¦5} indicates different stages and the subscript denotes the color or depth branch. ğ‘“ğ‘–(â‹…) denotes the function of the i-th CAC module. âŠ— is element-wise multiplication and âŸ¨ğ¹ğ‘–ğ·,ğ¹ğ‘–ğ¶âŸ© represents the feature channel concatenation operation. ğ¹â€²ğ‘–ğ· and ğ¹â€²ğ‘–ğ¶ are the i-th CAC module-refined attentive features. The following describes the CAC module in detail.

Fig. 6
figure 6
Framework of our CAC module

Full size image
Fig. 7
figure 7
The performance of our CAC module: a the color image; b the depth image; c color feature map ğ¹ğ¶; d depth feature map ğ¹ğ·; e our cross-domain conciliated attention map ğ‘€ğ·. Note that the attention maps of our CODON mainly focus on the edge structures while suppressing texture regions

Full size image
Table 1 RMSE comparisons on the Middlebury dataset for different upsampling factors
Full size table
Table 2 SSIM comparisons on the Middlebury dataset for different upsampling factors
Full size table
As shown in Fig. 6, the cross-channel attention and cross-spatial attention are designed in parallel, with cross-channel attention responsible for which features are important and cross-spatial attention responsible for focusing on where features are important. Our CAC attention map can be regarded as a weight map for each pixel in each channel. To compute the attention map efficiently, our CAC module adopts average-pooling and max-pooling to contract the dimension of the input feature map. The detailed calculation of the CAC module operation in the designed network can be expressed as the following three parts:

Cross-Channel Attention First, concatenate feature maps ğ¹ğ¶ğ‘ğ‘¡âˆˆğ‘…ğµÃ—ğ»Ã—ğ‘ŠÃ—128 squeezed along the spatial axis to ğ¹ğ¶ğ‘€ğ‘ğ‘¥âˆˆğ‘…ğµÃ—1Ã—1Ã—128 and ğ¹ğ¶ğ´ğ‘£ğ‘”âˆˆğ‘…ğµÃ—1Ã—1Ã—128 through max-pooled and average-pooled:

ğ¹ğ¶ğ‘€ğ‘ğ‘¥=ğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™(ğ¹ğ¶ğ‘ğ‘¡),
(6)
ğ¹ğ¶ğ´ğ‘£ğ‘”=ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™(ğ¹ğ¶ğ‘ğ‘¡).
(7)
Note that we use both max pooling and average pooling to enhance the feature diversity.

Then, ğ¹ğ¶ğ‘€ğ‘ğ‘¥ and ğ¹ğ¶ğ´ğ‘£ğ‘” are send to two fully connected layers parallelly:

ğ¹â€²ğ¶ğ‘€ğ‘ğ‘¥=ğœ”1(ğœ(ğœ”0(ğ¹ğ¶ğ‘€ğ‘ğ‘¥)),
(8)
ğ¹â€²ğ¶ğ´ğ‘£ğ‘”=ğœ”1(ğœ(ğœ”0(ğ¹ğ¶ğ´ğ‘£ğ‘”)),
(9)
where ğœ”0 and ğœ”1 denote the weights of fully connected layers, weights are shared in two pooled operations, ğœ(â‹…) is the ReLU function.

After that, we fuse the fully connected outputs ğ¹â€²ğ¶ğ‘€ğ‘ğ‘¥âˆˆğ‘…ğµÃ—1Ã—1Ã—64 and ğ¹â€²ğ¶ğ´ğ‘£ğ‘”âˆˆğ‘…ğµÃ—1Ã—1Ã—64 by element-wise summation, attention maps in the cross-channel attention branch ğ‘€ğ¶âˆˆğ‘…ğµÃ—1Ã—1Ã—64 can be mapped by sigmoid function:

ğ‘€ğ¶=ğ‘“ğ¶(ğ¹ğ¶ğ‘ğ‘¡)=ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ¹â€²ğ¶ğ‘€ğ‘ğ‘¥+ğ¹â€²ğ¶ğ´ğ‘£ğ‘”).
(10)
Cross-Spatial Attention First, concatenate feature maps ğ¹ğ¶ğ‘ğ‘¡âˆˆğ‘…ğµÃ—ğ»Ã—ğ‘ŠÃ—128 squeezed along the channel axis to ğ¹ğ‘†ğ‘€ğ‘ğ‘¥âˆˆğ‘…ğµÃ—ğ»Ã—ğ‘ŠÃ—1 and ğ¹ğ‘†ğ´ğ‘£ğ‘”âˆˆğ‘…ğµÃ—ğ»Ã—ğ‘ŠÃ—1 by max-pooled and average-pooled:

ğ¹ğ‘†ğ‘€ğ‘ğ‘¥=ğ‘€ğ‘ğ‘¥ğ‘ƒğ‘œğ‘œğ‘™(ğ¹ğ¶ğ‘ğ‘¡),
(11)
ğ¹ğ‘†ğ´ğ‘£ğ‘”=ğ´ğ‘£ğ‘”ğ‘ƒğ‘œğ‘œğ‘™(ğ¹ğ¶ğ‘ğ‘¡).
(12)
Then, ğ¹ğ‘†ğ‘€ğ‘ğ‘¥ and ğ¹ğ‘†ğ´ğ‘£ğ‘” are concatenated and convolved by a 5Ã—5 convolution layer:

ğ¹ğ‘†ğ¶onv=ğœ(ğœ”2(âŸ¨ğ¹ğ‘†ğ‘€ğ‘ğ‘¥,ğ¹ğ‘†ğ´ğ‘£ğ‘”âŸ©)),
(13)
where ğœ”2 is weights of 5Ã—5 convolution, and ğœ(â‹…) is the ReLU function.

After that, the cross-spatial attention branch ğ‘€ğ‘†âˆˆğ‘…ğµÃ—ğ»Ã—ğ‘ŠÃ—1 can be mapped by sigmoid function:

ğ‘€ğ‘†=ğ‘“ğ‘†(ğ¹ğ¶ğ‘ğ‘¡)=ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘(ğ¹ğ‘†ğ¶onv).
(14)
Cross-Domain Conciliated Attention Once the two kinds of attentions are learned, they are multiplied to obtain the full-size attention map ğ‘€ğ·âˆˆğ‘…ğµÃ—ğ»Ã—ğ‘ŠÃ—64:

ğ‘€ğ·=ğ‘“(ğ¹ğ¶ğ‘ğ‘¡)=ğ‘“ğ¶(ğ¹ğ¶ğ‘ğ‘¡)âŠ—ğ‘“ğ‘†(ğ¹ğ¶ğ‘ğ‘¡)=ğ‘€ğ¶âŠ—ğ‘€ğ‘†,
(15)
which can be used to attend the depth and color features respectively. Since the attention map is learned from the input concatenated feature from depth and color branches, it is expected to learn a cross-domain attention that can strengthen the corresponding areas in both color and depth feature maps. As shown in Fig. 7, the cross-domain conciliated attention maps of our network can well focus on the edge structures while suppressing texture regions. After the feature conciliation phase, the network extracts complementary features from both domains by focusing on the structural edges while ignoring the texture areas. Then, the conciliated depth and color features are combined and fed into the subsequent depth reconstruction phase. Leveraging the orchestrated features as guidance, our CODON model is able to recover sharp depth boundaries and fine details eventually.

Experimental Results
We present the implementation details in this section and conduct sufficient experiments to demonstrate the performance of our CODON compared with other state-of-the-art methods. We also conduct ablation experiments to investigate the efficiency of the proposed dual-branch structure, RMC and CAC for depth super-resolution reconstruction.

Training Implementation Details
We performed experiments using several public datasets [Middlebury (Scharstein & Szeliski 2002, 2003; Hirschmuller & Scharstein 2007; Scharstein & Pal 2007; Scharstein et al., 2014), New Tsukuba (Martull et al., 2012), Sintel (Butler et al., 2012), NYU-V2 (Silberman et al., 2012)]. Specifically, Middlebury have several datasets (2001, 2003, 2005, 2006 and 2014) with different sizes, e.g. Thirdsize, Halfsize and Fullsize. New Tsukuba have about 20 RGBD video streams with resolution 480Ã—640, Sintel have 25 RGBD video streams with resolution 436Ã—1024. NYU-V2 is comprised of video sequences from a variety of indoor scenes using the Microsoft Kinect with resolution 480Ã—640. The bicubic interpolation is used as the down-sampling strategy to obtain initial depth images with different rates.

To train our CODON, we collected 30 Fullsize RGBD images from Middlebury (15 and 10 images are from 2006 and 2014 datasets respectively), 20 RGBD images from New Tsukuba, and 64 RGBD images from Sintel. For testing, we used 10 Thirdsize RGBD images from Middlebury (2, 2 and 6 images from 2001, 2003 and 2005 datasets respectively), 60 and 60 images without blank depth area from different video streams on Sintel and New Tsukuba respectively to evaluate the performance of different methods. Moreover, we also used about 500 images without significant fuzzy depth area from the real-world NYU-V2 labeled dataset to evaluate the performance of different methods. The training and testing RGBD data were normalized to the range [0,1] by dividing by 255. Since the ground-truths are quantized to 8-bit, we converted all recovered depth images in the same data type in order to have a fair evaluation.

Fig. 8
figure 8
Visual quality comparison on Reindeer from Middlebury dataset for upsampling factor 4Ã—. a Ground truth; b bicubic; c SDF (Ham et al., 2015); d SRCNN-Net (Dong et al., 2015); e VDSR-Net (Kim et al., 2016a); f PixelTransform (Lutio et al., 2019); g MSG-Net (Hui et al., 2016); h ours. To better demonstrate the performance, the reconstruction errors of different methods are also given at the bottom (the error threshold is set to 2). Note that our super-resolution reconstructed result has the sharpest edges and the richest details

Full size image
Fig. 9
figure 9
Visual quality comparison on Moebius from Middlebury dataset for upsampling factor 4Ã—. a Ground truth; b bicubic; c SDF; d SRCNN-Net; e VDSR-Net; f PixelTransform; g MSG-Net; h ours

Full size image
Fig. 10
figure 10
Visual quality comparison on Cones from Middlebury dataset for upsampling factor 4Ã—. a Ground truth; b bicubic; c SDF; d SRCNN-Net; e VDSR-Net; f PixelTransform; g MSG-Net; h ours

Full size image
Table 3 The average RMSE and SSIM comparisons on New Tsukuba and Sintel datasets
Full size table
We use the mean squared error (MSE) as the loss function to train our network, and to avoid network overfitting, we add the ğ¿2 regularization into the loss function to penalize the weights. The loss function is defined as follows:

ğ¿(ğ›©)=12ğ‘âˆ‘ğ‘–=1ğ‘||(ğ‘“ğ›©(ğ¼ğ‘–ğ·ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™,ğ¼ğ‘–ğ¶ğ»ğ‘…)âˆ’ğ¼ğ‘–ğ·ğºğ‘‡)||2+âˆ‘ğ‘—=1ğ‘‡ğœ†||ğœ”ğ‘—||2
(16)
where T is the number of filters and the weight decay ğœ† is set at 0.0001. In the training phase, we randomly cropped the ğ¼ğ‘–ğ·ğ‘–ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™ and corresponding ğ¼ğ‘–ğ¶ğ»ğ‘… into 100Ã—100 pixels without data augmentation. To prevent overfitting, we first removed all the CAC modules and trained the temporary network to convergence. Then we inserted the CAC to form the final network and continued training until convergence. In our network, the RMC module is used in both phases (cross-domain feature conciliation and depth reconstruction) with five and three MC units, respectively. Networks were optimized using stochastic gradient descent (SGD) (LeCun et al., 1998). The mini-batch size was 16, the momentum was 0.9, and the learning rate was decreased from 0.01 to 0.001 (a factor of 10) after every 50 epochs. Training was completed after about 100 epochs, and gradient clipping was adopted to deal with gradient explosion (chosen value 0.4). Our experiments were implemented in PyTorch with a GTX 1080Ti GPU. Root mean squared error (RMSE) and the structural similarity index (SSIM) were used to evaluate the performance of different depth super-resolution methods. The source code as well as the trained model will be released.Footnote1

Fig. 11
figure 11
Visual quality comparison on frame395 from New Tsukuba dataset for upsampling factor 4Ã—. a Ground truth; b bicubic; c SDF; d SRCNN-Net; e VDSR-Net; f PixelTransform; g MSG-Net; h ours

Full size image
Fig. 12
figure 12
Visual quality comparison on Market6-0010 from Sintel dataset for upsampling factor 4Ã—. a Ground truth; b bicubic; c SDF; d SRCNN-Net; e VDSR-Net; f PixelTransform; g MSG-Net; h ours

Full size image
Table 4 The average RMSE and SSIM comparisons on NYU-V2 dataset
Full size table
Fig. 13
figure 13
Visual quality comparisons on NYU-V2 dataset for upsampling factor 4Ã—. a Ground truth; b bicubic; c SRCNN-Net; d VDSR-Net; e MSG-Net; f ours

Full size image
Table 5 Average RMSE of different datasets for factor 4Ã— using different components
Full size table
Quantitative Evaluation
To evaluate our networkâ€™s performance and design, we first present the comparisons on 10 images from the Middlebury dataset in Tables 1 and 2. Compared with other state-of-the-art methods: TGV (Ferstl et al., 2013), Guided (He et al., 2012), Joint filter (Shen et al., 2015), SDF (Ham et al., 2015), SRCNN-Net (Dong et al., 2015), VDSR-Net (Kim et al., 2016a), PixelTransform (Lutio et al., 2019) and MSG-Net (Hui et al., 2016), CODON delivers the best RMSE and SSIM and achieves excellent performance, especially for complex pictures. For an extremely high upsampling factor 16Ã—, the resolution of the initial depth image is considerably lower than that of the color image. Hence it is really difficult for color-guided depth image reconstruction. The results of various methods in this case are all not satisfactory, and our method can achieve a slightly better or comparable performance compared with the state-of-the-art CNN-based method MSG-Net (Hui et al., 2016).

Figures 8,  9 and 10 provide visual quality comparisons, the reconstruction errors of different methods are also given at the bottom (the error threshold is set to 2), and it can be seen that CODON achieves the best performance with the clearest boundaries and the fewest blurred areas compared with other state-of-the-art CNN-based depth super-resolution methods like MSG-Net (Hui et al., 2016). Moreover, there exists serious texture mis-mapping with the newly proposed method PixelTransform (Lutio et al., 2019), and CODON effectively solves the texture mis-mapping problem while recovering the clearest boundaries.

Then, the quantitative results on New Tsukuba and Sintel dataset are given in Table 3. Here, we illustrate the average RMSE and SSIM of all the testing images to give objective comparison. It can be seen that CODON has obvious advantages for 4Ã— and 8Ã— upsampling factors. For higher upsampling factor 16Ã—, because the depth input contains very little information, the effect of our cross-domain orchestration is affected and our method can achieve a comparable performance compared with the state-of-the-art MSG-Net (Hui et al., 2016). Moreover, the superiorities of our CONDON on Sintel are more obvious than New Tsukuba. Because the scenes of Sintel are more complex, our RMC module can well deal with the structural scale diversity in the complex scenes and our CAC module can effectively focus on depth boundaries while suppressing color textures at the same time. The visual quality comparisons and reconstruction errors are also given in Figs. 11 and 12. It can be seen that CODON also achieves the best performance with the clearest boundaries and the fewest blurred areas.

Moreover, we also conducted experiments on the real-world NYU-V2 Dataset. The average RMSE and SSIM scores of different methods on all the testing images are reported in Table 4. As can be seen, CODON achieves a slightly better or comparable performance compared with the state-of-the-art CNN-based method MSG-Net (Hui et al., 2016). It is noteworthy that most real-world RGBD dataset are constructed using Kinect or Realsense sensors. However, due to the limitation of these consumer-grade sensors, there are lots of fuzzy depth areas, especially around the structural edges. Hence, the performance of a method may not be well evaluated at these areas using the RMSE and SSIM metric. To further demonstrate the performance of different methods, we present some visual results of different methods in Fig. 13. It can be seen that CODON achieves much clearer boundaries than others, demonstrating its superiority for depth super-resolution.

Table 6 Average RMSE of different datasets for factor 4Ã— using different CAC designs
Full size table
Ablation Experiments
Effects of the Dual-Branch Structure RMC and CAC To illustrate the efficiency of our dual-branch structure, except for the residual connection of the last output layer, we removed all residual connections as well as all the CACs and replaced all RMCs with 3Ã—3 convolution layers to construct a dual-branch baseline network (denoting Our Basic). Then we removed the color branch of our basic network to construct a single branch baseline network (denoting Single Branch). As shown in Table 5, our Basic network performs better than Single Branch for all datasets (case 1 and 2), demonstrating the superiority of our dual-branch structure. The results also imply that the depth and color feature domains have very important complementary information, which is beneficial for depth super-resolution.

Fig. 14
figure 14
Training losses and RMSE of different MC numbers in both phases

Full size image
Fig. 15
figure 15
Training losses and RMSE of different sets in MC unit

Full size image
The RMC module is applied in our net framework to explore more detailed structural features while controlling the network parameters. The RMC module is used in both phases (feature conciliation phase and depth reconstruction phase). As shown in Table 5, the comparisons between case 2, case 3, case 4 and case 5 show that the RMC module can well improve the performance of our network in two phases respectively, because the RMC extracts more abundant features and extends the receptive fields. Hence, our RMC module can well deal with the structural scale diversity.

The CAC module is used in the network to achieve cross-domain attentive feature conciliation. As shown in Table 5, the comparisons between Our Basic and Our Basic + CAC (case 2 and 6) and the comparisons between Our Basic + RMC and Our Basic + RMC + CAC (case 5 and 7) clearly show that the CAC plays an important role in improving depth super-resolution reconstruction performance. Combing RMC and CAC modules, our CODON can achieve more desired reconstruction performance than the basic network (as case 2 and 7 shown).

Arrangement of Our RMC Module The RMC module is used in both phases (cross-domain feature conciliation and depth reconstruction) with five and three MC units, respectively. First, to investigate the influence of the number of MC units in RMC, we conducted an experiment at different settings. The training losses and testing RMSE of different combinations are given in Fig. 14. It can be seen that using 5 MC units in the first phase and 3 MC units in the second phase can achieve the best performance. Then, we conducted experiments to demonstrate the rationality of our MC structure. Specifically, we implemented experiments at different sets including only 3Ã—3 convolutional kernel, only 5Ã—5 convolutional kernel, two 3Ã—3 convolutional kernels in parallel, two 5Ã—5 convolutional kernels in parallel, and our MC structure. As shown in Fig. 15, it can be seen that the proposed MC structure can achieve the minimal training loss and minimal testing RMSE. Using two convolutions of different kernel size and recursively stacking them together via residual connections can implicitly construct an ensemble of multiple paths, where each of them goes through a certain part in the RMC and bypasses the remaining parts, resulting in multiple receptive fields. In this way, the representation capacity of RMC can be enhanced.

Table 7 Average RMSE of different datasets for factor 4Ã— using different attention modules
Full size table
Arrangement of Our CAC Module We next conducted experimental comparisons of different attention mechanisms. In case 1, Our Basic + RMC is our baseline; in case 2, we only apply the channel cross-attention branch; in case 3, we only apply the spatial cross-attention branch; in case 4, we design two parallel CAC modules to provide attention maps for the depth and color branches, respectively; and case 5 shows the results of CODON. As shown in Table 6, we clearly find that channel cross-attention and spatial cross-attention greatly enhance the representation capacity of the network, as they can better learn the extra information provided by color pictures. Further, when two attention outputs are combined to form a cross-domain conciliated attention map, the results are much better than when the two are used separately. The results of Table 6 demonstrate the rationale for our CAC structure.

Comparisons of Different Attention Methods We next conducted experiments to demonstrate the superiority of our CAC compared with other state-of-art attention methods. In case 1, we apply the CBAM (Woo et al., 2018) in our network to replace the CAC module. In case 2, we use non-local attention (Yue et al., 2018) to replace our CAC; due to the huge calculation cost of the non-local autocorrelation matrix, the non-local attention structure was only inserted at the end of the color and depth branches. In case 3, we integrate RCAN (Zhang et al., 2018b) to replace our CAC. Case 4 represents the results of our CODON. As shown in Table 7, our CAC module outperforms other attention structures, demonstrating the effectiveness of our CAC.

Table 8 Running times (seconds) of different image sizes using different methods
Full size table
Running Time
As shown in Table 8, we summarize the depth super-resolution run time(s) of our CODON and MSG-Net using different image sizes on different datasets. Note that the computing time of MSG-Net (Hui et al., 2016) is calculated using the source code provided by the authors. The source code of PixelTransform (Lutio et al., 2019) can only deal with blocks of 256Ã—256 size, hence we have added image chunking and overlapping stitching for PixelTransform. All calculation times are obtained using a GTX 1080Ti GPU. It can be seen that the running time of PixelTransform is really huge. The running time of our CODON is much less than MSG-Net. There are only 1.87M parameters in Our CODON, which well demonstrates that CODON can improve the super-resolution performance whilst simultaneously reducing the computational costs.

Table 9 The implementation details of CODON
Full size table
Limitations and Discussion
One of the limitations of CODON is that there is no intermediate supervision on the CAC, which may potentially lead to incorrect attention in ambiguous areas of subtle textures and fine edges. Recently, adversarial learning has been proved very useful for domain adaptation (Pei et al., 2018; Tzeng et al., 2017). Leveraging adversarial training to guide the attention conciliation process is worth trying in the future work. The other limitation of both our CODON and existing representative methods is that they usually use interpolation-based upsampling algorithm such as bicubic interpolation to obtain the initial estimate. In this way, these super-resolution methods indeed focus on image refinement. However, whether or not these interpolation-based upsampling algorithms can obtain a good initial estimate remains unclear. In the future, we plan to devise a learning-based upsampling module that can be trained with the refinement module in an end-to-end manner. In addition, since bridging the gap between RGB images and depth images is challenging, it is also worth exploring to increase the resolution of depth images by transforming color and depth images into 3D point clouds, which can be refined and made dense.

Conclusion
In this paper, we propose the novel CODON to address the challenging depth super-resolution problem. Specifically, the recursive multi-scale convolutional module (RMC) is proposed to efficiently adapt to the various local structures in color and depth images; The cross-domain attention conciliation module (CAC) is devised to guide the networkâ€™s focus by effectively modeling the co-occurrence correlations between color and depth features. First, RMC and CAC are sequentially stacked in two parallel color and depth branches to collaboratively achieve cross-domain feature conciliation step-by-step. Then, the orchestrated features are fed into a RMC prediction branch to reconstruct the high-resolution depth image. Experimental results on several popular datasets confirm the values of RMC and CAC, demonstrate the superiority of CODON over representative methods.