Generalized Procrustes Analysis (GPA) is the problem of bringing multiple shapes into a common reference by estimating transformations. GPA has been extensively studied for the Euclidean and affine transformations. We introduce GPA with deformable transformations, which forms a much wider and difficult problem. We specifically study a class of transformations called the Linear Basis Warps, which contains the affine transformation and most of the usual deformation models, such as the Thin-Plate Spline (TPS). GPA with deformations is a nonconvex underconstrained problem. We resolve the fundamental ambiguities of deformable GPA using two shape constraints requiring the eigenvalues of the shape covariance. These eigenvalues can be computed independently as a prior or posterior. We give a closed-form and optimal solution to deformable GPA based on an eigenvalue decomposition. This solution handles regularization, favoring smooth deformation fields. It requires the transformation model to satisfy a fundamental property of free-translations, which asserts that the model can implement any translation. We show that this property fortunately holds true for most common transformation models, including the affine and TPS models. For the other models, we give another closed-form solution to GPA, which agrees exactly with the first solution for models with free-translation. We give pseudo-code for computing our solution, leading to the proposed DefGPA method, which is fast, globally optimal and widely applicable. We validate our method and compare it to previous work on six diverse 2D and 3D datasets, with special care taken to choose the hyperparameters from cross-validation.

Access provided by University of Auckland Library

Introduction
The problem of Generalized Procrustes Analysis (GPA) is to register a set of shapes with known correspondences into a single unknown reference shape, by estimating transformations. The existing literature focuses on GPA with Euclidean (or similarity in case of scaling) and affine transformations, which do not cope with deformations. When present in the datum shapes, the un-modeled deformations are typically considered as noise, which creates biases on the estimation thus causing misfitting to the data. This is undesirable, particularly when the shape is nonrigid and the precision is critical, for example in medical applications.

We consider GPA with deformation models. In specific, we consider a generic class of transformation models, termed Linear Basis Warps (LBWs), which generalize over affine transformation models and most commonly used warp models, like the Thin-Plate Spline (TPS) (Duchon 1976; Bookstein 1989), Radial Basis Functions (RBF) (Fornefett et al. 2001), and Free-Form Deformations (FFD) (Rueckert et al. 1999; Szeliski and Coughlan 1997). The LBW shares many similarities with the affine model, where the transformation parameters are linear as weights to a set of possibly nonlinear basis functions. This gives additional modeling flexibility to the LBW which copes with nonlinear deformations, while retaining the computational simplicity of linear models.

We propose GPA with LBWs. Our cost function is formulated in the coordinate frame of the reference shape (which is termed the reference-space cost). This cost function is linear least-squares in optimizable parameters which maximizes the benefit of using the LBW. An important point to bear in mind is that a sufficiently generic deformation model can match any arbitrary shape to another to a good degree of fitness. This makes the estimation of the reference shape highly ambiguous. We resolve these ambiguities by shape constraints, and show that these extra degree of freedoms can be determined by the eigenvalues of the covariance matrix of the reference shape, termed reference covariance prior. Importantly, the reference covariance prior is independent of the rest of the computation, thus can be enforced as a prior or posterior. We propose a method to estimate the reference covariance prior based on the similarity to the datum shapes while other options are also possible.

We propose a globally optimal solution in closed-form to the GPA formulation with LBWs. Our solution is based on a specific case of the eigenvalue problem (Theorem 1) and the characterization of LBWs (Theorem 2 and Theorem 3). We present our ideas following a specific-to-general scheme: Sect. 4 for affine GPA, Sect. 5 for GPA with LBWs, and Sect. 6 for GPA with partial datum shapes.

Section 4. We present in Sect. 4.1 our affine GPA formulation and the ideas behind the shape constraints {\varvec{S}} {\varvec{1}} = {\varvec{0}} and {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, and then in Sect. 4.2 the theoretical results on how to solve the formulation globally in closed-form. We show that a closed-form solution exists if the matrix to decompose has an all-one eigenvector. In Sect. 4.3, we present a method to estimate the reference covariance prior \varvec{\Lambda }. We show in Sect. 4.4 that the proposed formulation is invariant to coordinate transformations of the datum shapes, where the optimal reference shape will remain the same. Lastly, in Sect. 4.5, we draw the connection between our method and the classical affine GPA method by eliminating translations explicitly.

Section 5. We first introduce LBWs (Sect. 5.1) and GPA with LBWs (Sect. 5.2), and then relate the existence of an all-one eigenvector to the equivalent properties of LBWs (Sect. 5.3 and Theorem 2 in specific). We introduce the concept of free-translations (Definition 1), and show that to have the desired properties in Theorem 2 it suffices to have a LBW where its translation part is constraint-free (Proposition 4). Both the affine transformation and the TPS warp, which are the two most commonly used LBWs, fall into this category, thus the resulting GPA problems can be solved in closed-form by Theorem 1 and Proposition 1. Lastly, in Sect. 5.4, we propose a soft-constrained version (using a penalty term) when the condition is not met, which can also be solved in closed-form.

Section 6. We extend the results from full shapes to partial shapes, i.e., the estimation of the reference covariance prior via the shape completion (Sect. 6.1), the GPA formulation with LBWs for partial shapes (Sect. 6.2), and the all-one eigenvector characterization (Sect. 6.3). Section 6.4 shows that the proposed GPA formulation with LBWs is invariant to coordinate transformations of the datum shapes, where the optimal reference shape will remain the same. Section 6.5 shows how to handle reflections in the solution. Section 6.6 gives pseudo-code to benefit future research. Implementation details are summarized in Algorithm 1 and Algorithm 2.

In a nutshell, we present a closed-form GPA method, termed DefGPA (i.e., for GPA based on LBWs), which is fast, globally optimal and copes with the general problem with regularization and incomplete shapes. The rest of this article is organized as follow. Section 2 introduces our notation, the GPA problem, and the Brockett cost function on the Stiefel manifold. Section 3 reviews the related work. Sections 4–6 describe our DefGPA method. Section 7 provides experimental results. Section 8 concludes the paper.

Modeling Preliminaries
Notation and Terminology
Notation We use {\mathbb {R}} to denote the set of real numbers, {\mathbb {R}}^n the set of column vectors of dimension n, and {\mathbb {R}}^{m \times n} the set of matrices of dimension m \times n. All the vectors are column majored, denoted by lower-case characters in bold. The matrices are upper-case characters in bold. The scalars are in italics. For matrices, we reserve {\varvec{I}} for identity matrices and {\varvec{O}} for all-zero matrices. For vectors we reserve {\varvec{0}} for all-zero vectors and {\varvec{1}} for all-one vectors. All these special matrices and vectors are assumed to have proper dimensions induced by the context. {\varvec{A}}^{\intercal } is the transpose of {\varvec{A}}, {\varvec{A}}^{-1} the inverse of {\varvec{A}}, and {\varvec{A}}^{\dagger } the Moore-Penrose pseudo-inverse of {\varvec{A}}. We use the notation \mathbf {Range}\left( {\varvec{A}}\right) and \mathbf {Null}\left( {\varvec{A}}\right) to denote the range space and null space of {\varvec{A}}, respectively. {\varvec{A}} \succeq {\varvec{O}} means {\varvec{A}} is positive semidefinite. {\varvec{A}} \succeq {\varvec{B}} means {\varvec{A}} - {\varvec{B}} is positive semidefinite. The symbol \Vert \cdot \Vert _F denotes the matrix Frobenius norm, and \Vert \cdot \Vert _2 the vector \ell _2 norm. \mathbf {tr}\left( {\varvec{A}}\right) is the trace for a square matrix {\varvec{A}}. \Vert {\varvec{A}} \Vert _F^2 = \mathbf {tr}\left( {\varvec{A}} {\varvec{A}}^{\intercal }\right) = \mathbf {tr}\left( {\varvec{A}}^{\intercal } {\varvec{A}}\right) . We use \mathbf {nnz}(\cdot ) as a shorthand for “number of non-zeros”.

The d Top Eigenvectors and Bottom Eigenvectors. Let {\varvec{A}} = {\varvec{Q}} \varvec{\Lambda } {\varvec{Q}}^{\intercal } be the eigenvalue decomposition of a symmetric matrix {\varvec{A}}, where {\varvec{Q}} = \left[ {\varvec{q}}_1, {\varvec{q}}_2, \dots , {\varvec{q}}_n\right] is a square orthonormal matrix. Let \varvec{\Lambda } = \mathrm {diag}\left( \lambda _1, \lambda _2, \dots , \lambda _n\right) with \lambda _1 \ge \lambda _2 \ge \dots \ge \lambda _n. Then we call {\varvec{q}}_1, {\varvec{q}}_2, \dots , {\varvec{q}}_d in sequence the d top eigenvectors, and {\varvec{q}}_n, {\varvec{q}}_{n-1}, \dots , {\varvec{q}}_{n-d+1} in sequence the d bottom eigenvectors of {\varvec{A}}. The analogous concepts in the Singular Value Decomposition are the d leftmost and rightmost singular vectors.

Shape covariance matrix We model shapes as point-clouds. Given a d-dimensional shape of m points {\varvec{S}} \in {\mathbb {R}}^{d \times m}, the geometric center, also called centroid, is the mean of its columns, defined as \mathbf {mean}({\varvec{S}})= \frac{1}{m} {\varvec{S}} {\varvec{1}} . The concept covariance matrix of the shape {\varvec{S}} is defined by:

\begin{aligned} {\mathbb {C}}\mathrm {ov} ({\varvec{S}}) = \left( {\varvec{S}} - \mathbf {mean}({\varvec{S}}) {\varvec{1}}^{\intercal } \right) \left( {\varvec{S}} - \mathbf {mean}({\varvec{S}}) {\varvec{1}}^{\intercal } \right) ^{\intercal } . \end{aligned}
In particular, if {\varvec{S}} is at the origin of the coordinate frame, \mathbf {mean}({\varvec{S}}) = {\varvec{0}}, the shape covariance becomes {\mathbb {C}}\mathrm {ov} ({\varvec{S}}) = {\varvec{S}} {\varvec{S}}^{\intercal } .

Generalized Procrustes Analysis
A datum shape is represented as a matrix {\varvec{D}}_i \in {\mathbb {R}}^{d \times m} \left( i \in \left[ 1:n \right] \right) . Here d \in \left\{ 2,3\right\} is the dimension of the points and m the number of points in the shape. We will index the j-th point in the shape {\varvec{D}}_i as {\varvec{D}}_i [j], which is the j-th column of {\varvec{D}}_i. The shape points are arranged correspondence-wise: the j-th point in shape {\varvec{D}}_{i_1} and the j-th point in shape {\varvec{D}}_{i_2} correspond to different observations of the same physical point. Missing data are caused by unobserved points in some shapes. They are modeled by binary visibility variables \gamma _{i,j} (i \in \left[ 1:n \right] , j \in \left[ 1:m \right] ), where \gamma _{i,j} = 1 if and only if the j-th point occurs in the i-th shape, and \gamma _{i,j} = 0 otherwise. The reference shape {\varvec{S}} \in {\mathbb {R}}^{d \times m} is defined accordingly, with its j-th column {\varvec{S}}[j] corresponding to the j-th physical point. All the points occur in the reference shape {\varvec{S}}.

We denote {\mathcal {T}}_i \left( i \in \left[ 1:n \right] \right) the transformation from the datum shape {\varvec{D}}_i to the reference shape {\varvec{S}}. Here {\mathcal {T}}_i can be Euclidean, similarity, affine, or non-rigid transformations. Different choices of {\mathcal {T}}_i yield different types of GPA problems. The general GPA problem can be written as:

\begin{aligned} \begin{aligned}&\mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{\{{\mathcal {T}}_i\},\, {\varvec{S}}}\quad \sum _{i=1}^n \sum _{j=1}^m\, \gamma _{i,j} \Vert {\mathcal {T}}_i \left( {\varvec{D}}_{i}[j]\right) - {\varvec{S}}[j] \Vert _{2}^2\\&\quad \mathrm {s.t.} \quad {\mathcal {C}} \left( {\mathcal {T}}_1, {\mathcal {T}}_2, \dots , {\mathcal {T}}_n, {\varvec{S}}\right) = {\varvec{0}} , \end{aligned} \end{aligned}	(1)
where {\mathcal {C}} \left( {\mathcal {T}}_1, {\mathcal {T}}_2, \dots , {\mathcal {T}}_n, {\varvec{S}}\right) = {\varvec{0}} denotes the set of constraints used to avoid degeneracy. A typical degenerate case is {\mathcal {T}}_i \left( {\varvec{p}}\right) = {\varvec{0}} for any point {\varvec{p}}, and {\varvec{S}} = {\varvec{O}}. The detailed choices of these constraints will be provided later on.

The GPA model in formulation (1) holds for many transformation models and handles missing datum points. We give a more compact matrix form which will simplify the derivation of our methods. To this end, we define the diagonal visibility matrix \varvec{\Gamma }_i = \mathrm {diag} \left( \gamma _{i,1}, \gamma _{i,2}, \dots , \gamma _{i,m} \right) \left( i \in \left[ 1:n \right] \right) , constructed by the visibility variables corresponding to the shape {\varvec{D}}_i. By noting that:

\begin{aligned} \chi ^2_r&= \sum _{i=1}^n \sum _{j=1}^m\, \gamma _{i,j} \Vert {\mathcal {T}}_i \left( {\varvec{D}}_{i}[j]\right) - {\varvec{S}}[j] \Vert _{2}^2\\&= \sum _{i=1}^n \sum _{j=1}^m\, \Vert \gamma _{i,j} {\mathcal {T}}_i \left( {\varvec{D}}_{i}[j]\right) \\&\quad - \gamma _{i,j} {\varvec{S}}[j] \Vert _{2}^2 = \sum _{i=1}^n\, \Vert {\mathcal {T}}_i \left( {\varvec{D}}_{i}\right) \varvec{\Gamma }_i\\&\quad - {\varvec{S}} \varvec{\Gamma }_i \Vert _{F}^2 , \end{aligned}
we can write the GPA model in a compact matrix form:

\begin{aligned} \begin{aligned}&\mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{\{{\mathcal {T}}_i\},\, {\varvec{S}}}\quad \sum _{i=1}^n\, \Vert {\mathcal {T}}_i \left( {\varvec{D}}_{i}\right) \varvec{\Gamma }_i - {\varvec{S}} \varvec{\Gamma }_i\Vert _{F}^2\\&\quad \mathrm {s.t.} \quad {\mathcal {C}} \left( {\mathcal {T}}_1, {\mathcal {T}}_2, \dots , {\mathcal {T}}_n, {\varvec{S}}\right) = {\varvec{0}} . \end{aligned} \end{aligned}	(2)
In case of full shapes (shapes without missing datum points), we have \varvec{\Gamma }_i = {\varvec{I}} \left( i \in \left[ 1:n \right] \right) .

The reference-space cost and the datum-space cost The cost function defined in formulations (1) and (2) is called the reference-space cost since the residual error is evaluated in the coordinate frame of the reference shape (Bartoli et al. 2013). In contrast, it is possible to derive a cost function in the datum-space:

\begin{aligned} \chi ^2_d = \sum _{i=1}^n\, \Vert {\varvec{D}}_{i} \varvec{\Gamma }_i - {\mathcal {T}}_i^{-1} \left( {\varvec{S}}\right) \varvec{\Gamma }_i \Vert _{F}^2. \end{aligned}	(3)
The datum-space cost is a generative model since all the datum shapes {\varvec{D}}_i are generated by the single reference shape {\varvec{S}} with transformations {\mathcal {T}}_i^{-1}. In contrast, the reference-space cost is discriminative, as it seeks for the transformations {\mathcal {T}}_i that can best match the datum shapes {\varvec{D}}_i with the reference shape {\varvec{S}}. The reference-space cost and the datum-space cost are identical if {\mathcal {T}}_i represents Euclidean transformations (Bartoli et al. 2013).

Brockett Cost Function on the Stiefel Manifold
The matrix Stiefel manifold is the set of matrices satisfying:

\begin{aligned} St(d, m) = \left\{ {\varvec{X}} \in {\mathbb {R}}^{m \times d} \ \vert \ {\varvec{X}}^{\intercal } {\varvec{X}} = {\varvec{I}} \right\} . \end{aligned}
In particular, we shall use the classical result of the following Stiefel manifold optimization problem:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{ {\varvec{X}} \in St(d, m)} \ \mathbf {tr}\left( {\varvec{X}}^{\intercal } \varvec{{\mathcal {P}}} {\varvec{X}} \varvec{\Lambda } \right) . \end{aligned}	(4)
The cost in problem (4) is termed the Brockett cost function in the Stiefel manifold optimization literature (Brockett 1989; Birtea et al. 2019; Absil et al. 2009). Importantly, this problem admits a closed-form solution. The critical points of the Brockett cost function on the Stiefel manifold are the eigenvectors of \varvec{{\mathcal {P}}} (Brockett 1989; Birtea et al. 2019). The global minimum of problem (4) can thus be obtained by arranging the eigenvectors as follow. Let \left( \alpha _j, \varvec{\xi }_j\right) (j \in \left[ 1:m \right] ) be the set of eigenvalues and eigenvectors of \varvec{{\mathcal {P}}}, such that \varvec{{\mathcal {P}}} \varvec{\xi }_j = \alpha _j \varvec{\xi }_j, with 0 \le \alpha _1 \le \alpha _2 \le \dots \le \alpha _m arranged in the ascending order. Let the diagonal elements of \varvec{\Lambda } being arranged in the descending order \lambda _1 \ge \lambda _{2} \ge \dots \ge \lambda _{d} . Then {\varvec{X}}^{\star } = [\varvec{\xi }_1, \varvec{\xi }_2, \dots , \varvec{\xi }_d], comprising the d bottom eigenvectors of \varvec{{\mathcal {P}}}, is globally optimal to problem (4), with a total cost \alpha _1 \lambda _1 + \alpha _2 \lambda _{2} + \cdots + \alpha _{d} \lambda _{d} . Any other combination yields a larger cost which proves the optimality of {\varvec{X}}^{\star }, by a result of Hardy-Littlewood-Polya (Brockett 1989; Hardy et al. 1952).

Lemma 1
Let \varvec{{\mathcal {P}}} be a symmetric matrix with its d-bottom eigenvectors being {\varvec{X}} = [\varvec{\xi }_1, \varvec{\xi }_2, \dots , \varvec{\xi }_d]. Let \varvec{\Lambda } be a diagonal matrix \varvec{\Lambda } = \mathrm {diag}\left( \lambda _1, \lambda _2, \dots , \lambda _n\right) with \lambda _1 \ge \lambda _2 \ge \dots \ge \lambda _n \ge 0 . The globally optimal solution to the optimization problem:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {P}}} {\varvec{S}}^{\intercal } \right) \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda } , \end{aligned}	(5)
is {\varvec{S}}^{\star } = \sqrt{\varvec{\Lambda }} {\varvec{X}}^{\intercal }, i.e., obtained by scaling the d bottom eigenvectors of \varvec{{\mathcal {P}}} by \sqrt{\varvec{\Lambda }}.

Proof
We introduce a matrix {\varvec{X}} so that {\varvec{S}}^{\intercal } = {\varvec{X}} \sqrt{\varvec{\Lambda }}. The constraint {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda } thus becomes {\varvec{X}}^{\intercal } {\varvec{X}} = {\varvec{I}}. The cost function \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {P}}} {\varvec{S}}^{\intercal } \right) can be rewritten as \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {P}}} {\varvec{S}}^{\intercal } \right) = \mathbf {tr}\left( \sqrt{\varvec{\Lambda }} {\varvec{X}}^{\intercal } \varvec{{\mathcal {P}}} {\varvec{X}} \sqrt{\varvec{\Lambda }} \right) = \mathbf {tr}\left( {\varvec{X}}^{\intercal } \varvec{{\mathcal {P}}} {\varvec{X}} \varvec{\Lambda } \right) . The globally optimal solution to problem (5) is thus {{\varvec{S}}^{\star }}^{\intercal } = {\varvec{X}}^{\star } \sqrt{\varvec{\Lambda }} , with {\varvec{X}}^{\star } being the solution of problem (4). In other words, {\varvec{S}}^{\star } is obtained by scaling the d bottom eigenvectors of \varvec{{\mathcal {P}}} by \sqrt{\varvec{\Lambda }}. \square

Remark 1
Analogously, the globally optimal solution to the following maximization problem:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {Q}}} {\varvec{S}}^{\intercal } \right) \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }. \end{aligned}	(6)
is to scale the d top eigenvectors of the matrix \varvec{{\mathcal {Q}}} by \sqrt{\varvec{\Lambda }}.

Related Work
GPA considers shape registration with known correspondences, which abounds in the literature. We classify the literature according to the used transformation model.

Rigid Case
Two shapes The classical approach to the Procrustes analysis problem starts with linear algebraic results of two shape registration using least-squares. In particular, the closed-form solution of registering two point-clouds with orthonormal matrices (termed orthogonal Procrustes) was established half a century ago (Green 1952; Schönemann 1966). The result can be extended to tackle translations (Arun et al. 1987) and scale factors (Horn et al. 1988). The result in (Arun et al. 1987; Horn et al. 1988) uses orthonormal matrices to represent rotations. Besides, it is possible to derive the result using unit quaternions to represent rotations (Horn 1987), and furthermore, using dual quaternions to represent both rotations and translations (Walker et al. 1991). When confronting large noise, the orthonormal matrix based approaches (Arun et al. 1987; Horn et al. 1988) can give a reflection matrix (with determinant -1) as apposed to a valid rotation matrix (Umeyama 1991). A correction to the special orthogonal group (\mathrm {SO}(3), which represents valid rotations) is given in (Umeyama 1991), and a more concise derivation in (Kanatani 1994). A comparative study of these methods is provided in (Eggert et al. 1997). However from the statistical point of view, these methods assume that noise only occurs in the target point-cloud, which is isotropic, identical and independently Gaussian distributed. This assumption can deviate from real noise schemes. More sophisticated formulations based on anisotropic and inhomogeneous Gaussian noise models are discussed in (Ohta and Kanatani 1998; Matei and Meer 1999), where the renormalization technique based on the quaternion parameterization is used to solve the resulting optimization problem effectively. Overall, two shape rigid Procrustes analysis can be considered a solved problem.

Multiple shapes Multiple shape Procrustes analysis is a more difficult problem, which is termed generalized Procrustes analysis (GPA) (Kristof and Wingersky 1971; Gower 1975). The work in (Kristof and Wingersky 1971; Ten Berge 1977) examined the optimality conditions of the orthogonal GPA (Euclidean GPA without translations). The derived conditions are either necessary or sufficient, however not necessary-sufficient. Actually, due to the existence of rotations, the Euclidean GPA, or similar problems like pose graph optimization (PGO) (Bai et al. 2021) has recently been acknowledged as highly nonlinear and non-convex (Rosen et al. 2019). Therefore up to now, exact techniques for the rigid GPA are all iterative. In early days, using the pairwise rigid Procrustes result as a subroutine, rigid GPA was solved by alternating the estimation of the reference shape and that of the transformations until convergence (Gower 1975; Ten Berge 1977; Rohlf and Slice 1990; Wen et al. 2006). However, such methods are not guaranteed to attain the global optimum of the cost function. The translation part of the rigid GPA problem is free of constraints, thus can be eliminated from the cost function by a variable projection scheme (Golub and Pereyra 2003). As a result, the rigid GPA can be formulated as a rotation estimation problem on the manifold (Benjemaa and Schmitt 1998; Williams and Bennamoun 2001; Krishnan et al. 2005). Sequential techniques were proposed based on the unit quaternion (Benjemaa and Schmitt 1998) and orthonormal matrices (Williams and Bennamoun 2001). A complete approach on the \mathrm {SO(3)} Lie group has been provided in (Krishnan et al. 2005). Building upon efficient sparse linear algebra techniques (Davis 2006) and Lie group theory (Iserles et al. 2000), the Lie group approach can be considered as the current state-of-the-art. An initialization technique to the iterative solver was provided in (Bartoli et al. 2013).

Affine Case
There exists a vast body of research in shape analysis (Kendall 1984; Kendall et al. 2009; Dryden and Mardia 2016) where the GPA problem arouse in parallel, e.g., the complex arithmetic approach for 2D shapes and its relation to Procrustes analysis (Kent 1994). The shape statistics, i.e., mean and variability, are defined on aligned shapes (thus being invariant to rotation, translation and scaling). A commonly used alignment technique is the Procrustes analysis (Kendall 1984) or GPA (Goodall 1991). The affine GPA has been studied with a closed-from solution proposed (Rohlf and Slice 1990). In the present notation, following the result in (Rohlf and Slice 1990), the principal axes of the reference shape are estimated as the d top eigenvectors of the matrix \sum _{i=1}^{n} {\varvec{D}}_i^{\intercal } \left( {\varvec{D}}_i^{\intercal }\right) ^{\dagger }, after centering each {\varvec{D}}_i to the origin of the coordinate frame and eliminating the translation parameters.

We derive similar results in Sect. 4.5 in the global optimization context instead of the alternation scheme used in (Rohlf and Slice 1990), which means the scaling factor (i.e., the reference covariance prior used in the proposed shape constraint) is estimated in a different way (Sect. 4.3). We show in Sect. 4.5 that this approach gives the same result as the proposed formulation (8), while the proposed formulation without eliminating translations is more general and extensible to the LBWs.

The above affine GPA solution, in our context, is called the reference-space solution. On the other hand, the full shape affine GPA in the datum-space admits a closed-form solution using the Singular Value Decomposition (SVD). Let {\varvec{D}}_i be zero centered. Then the datum-space cost is \sum _{i=1}^n\, \Vert {\varvec{D}}_{i} - {\varvec{A}}_i {\varvec{S}} \Vert _{F}^2 . The optimal solution, [ {\varvec{A}}_1^{\intercal } , \cdots , {\varvec{A}}_n^{\intercal } ]^{\intercal } {\varvec{S}} as a whole, can be determined via the SVD of the matrix [ {\varvec{D}}_1^{\intercal } , \cdots , {\varvec{D}}_n^{\intercal } ]^{\intercal } . This idea is in the same spirit of the Tomasi-Kanade factorization (Tomasi and Kanade 1992) in computer vision, which studied the affine transformation of the orthographic camera model. We term this method as *AFF_d and use it as a benchmark algorithm in the experiments.

Deformable Case
In shape analysis, a shape is considered as an element on the so-called shape manifold (the set of all possible shapes) (Kendall 1984; Kilian et al. 2007). The shape deformation is thus studied as the shortest path on the shape manifold, i.e., the geodesic path under a chosen Riemannian metric (Kilian et al. 2007). In the rigid case, after discarding translation and scaling, the shape manifold is a quotient manifold invariant to rotations and the Riemannian metric can be chosen as the Procrustes distance (Kendall 1984). For deformable cases, the Riemannian metric can be otherwise formulated based on the rigidity or isometry (Kilian et al. 2007). The work (Kendall 1984; Kilian et al. 2007), as well as this paper, use landmarks (or meshes with fixed connections) to model shapes and define transformations. In addition, recent research has studied shape analysis based on curves (Joshi et al. 2007; Younes et al. 2008) or surfaces, e.g., level sets (Osher and Fedkiw 2003), medial surfaces (Bouix et al. 2005), Q-maps (Kurtek et al. 2010, 2011), Square Root Normal Fields (SRNF) (Jermyn et al. 2012; Laga et al. 2017), etc. See the review papers (Younes 2012; Laga 2018). There is also a line of work including skeletal structures, e.g., the medial axis representations (M-rep) (Fletcher et al. 2004), and SCAPE (mesh with an articulated skeleton) (Anguelov et al. 2005).

Beyond different shape modeling techniques, deformations based on landmarks/meshes have a rich literature. Typically, the deformation of a mesh is defined piece-wisely, with a local transformation associated to each vertex or triangle (Freifeld and Black 2012), or with additional geometric or smoothing constraints (Allen et al. 2003; Anguelov et al. 2005; Sumner et al. 2007; Song et al. 2020). It is worth mentioning that such piece-wise models are commonly used in describing template based deformations, where a reference shape is given a priori. In this paper, we use landmarks to model shapes and warps to model deformations. The landmarks can be extracted from the key-points or the samplings of the shape boundary (Cootes et al. 1995). While the proposed method applies to meshes as well, the connection of vertices (or edges of the meshes) is never required. Our analysis is based on a class of warps, called LBWs, which is a linear combination of the nonlinear basis functions (Rueckert et al. 1999; Szeliski and Coughlan 1997; Bookstein 1989; Fornefett et al. 2001; Bartoli et al. 2010). A typical example of the LBWs is the well-known TPS warp (Bookstein 1989) which we will use to demonstrate our results.

There has been work using warps to refine the shapes after solving the rigid registration (Brown and Rusinkiewicz 2007; Kim et al. 2008). Such methods are not template free in essence as when it comes to the estimation of the warp parameters, a reference shape has been known already. Little attention has been paid to estimate the reference shape and the transformation parameters all together in a unified manner. In this work, we provide a closed-form solution to the unified estimation problem.

Different from classical affine methods (Kendall 1984; Goodall 1991; Rohlf and Slice 1990), our method allows for the possibility to keep the translation parameters during the estimation (Theorem 1 and Proposition 1). This is crucial to GPA with LBWs, as in certain cases, the translation parameters cannot be explicitly identified. In specific, for LBWs, we relate the constraint-free translation (Definition 1) to the existence of an eigenvector of all-ones and the equivalent properties of LBWs (Theorem 2 and Theorem 3), which constitutes the foundation of our closed-form solution.

Generalized Procrustes Analysis with the Affine Model
Standard Form with the Shape Constraint
We start with the case without missing datum points, i.e., \varvec{\Gamma }_i = {\varvec{I}} in formulation (2). We study the affine-GPA with {\mathcal {T}}_i \left( {\varvec{D}}_i\right) = {\varvec{A}}_i {\varvec{D}}_i + {\varvec{t}}_i {\varvec{1}}^{\intercal }, with {\varvec{A}}_i \in {\mathbb {R}}^{d \times d} being linear and {\varvec{t}}_i \in {\mathbb {R}}^{d} a translation. We write the affine transformation in homogeneous form as:

\begin{aligned} {\varvec{A}}_i {\varvec{D}}_i + {\varvec{t}}_i {\varvec{1}}^{\intercal } = \begin{bmatrix} {\varvec{A}}_i&{\varvec{t}}_i \end{bmatrix} \begin{bmatrix} {\varvec{D}}_i \\ {\varvec{1}}^{\intercal } \end{bmatrix} {\mathop {=}\limits ^{\text {def}}}\tilde{\varvec{A}}_i \tilde{\varvec{D}}_i , \end{aligned}	(7)
where we term \tilde{\varvec{D}}_i the homogeneous representation of {\varvec{D}}_i by completing {\varvec{D}}_i with a row of all ones. In this representation, matrix \tilde{\varvec{A}}_i = [{\varvec{A}}_i,\, {\varvec{t}}_i] contains all parameters to be estimated.

Following the reference-space cost (2), our proposed formulation is given as:

\begin{aligned} \mathrm {I:}\quad {\left\{ \begin{array}{ll} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{\{\tilde{\varvec{A}}_i\},\, {\varvec{S}}} \quad &{} \sum _{i=1}^n\, \Vert \tilde{\varvec{A}}_i \tilde{\varvec{D}}_i - {\varvec{S}} \Vert _F^2 \\ \mathrm {s.t.} \quad &{} {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \quad {\varvec{S}} {\varvec{1}} = {\varvec{0}} . \end{array}\right. } \end{aligned}	(8)
In formulation (8), {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda } and {\varvec{S}} {\varvec{1}} = {\varvec{0}} are the shape constraints, which concretize the constraint {\mathcal {C}} \left( \cdot \right) = {\varvec{0}} in formulation (2). The matrix \varvec{\Lambda }, termed reference covariance prior, is a diagonal matrix \varvec{\Lambda } = \mathrm {diag}\left( \lambda _1, \lambda _2, \dots , \lambda _d\right) driven by d parameters \lambda _1, \lambda _2, \dots , \lambda _d. We shall shortly present in Sect. 4.3 a method to estimate \varvec{\Lambda } based on rigidity. The ideas behind these two shape constraints are motivated as follow.

The Shape Constraint {\varvec{S}} {\varvec{1}} = {\varvec{0}}
This shape constraint is used to center the reference shape to the origin of the coordinate frame. The role of this constraint is twofold: 1) it provides d constraints to remove the d gauge freedoms caused by translations; 2) it reduces the shape covariance matrix of the reference shape to the form {\mathbb {C}}\mathrm {ov} ({\varvec{S}}) = {\varvec{S}} {\varvec{S}}^{\intercal } as in this case \mathbf {mean}({\varvec{S}}) = \frac{1}{m} {\varvec{S}} {\varvec{1}} = {\varvec{0}}.

The Shape Constraint {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }
This shape constraint is used to: 1) capture the eigenvalues of the shape covariance matrix {\mathbb {C}}\mathrm {ov} ({\varvec{S}}) = {\varvec{S}} {\varvec{S}}^{\intercal } , and 2) fix the gauge freedom caused by rotations. The main insight here is that the eigenvalues of the shape covariance matrix remain unchanged if the shape undergoes rigid transformations. Moreover, by letting {\mathbb {C}}\mathrm {ov} ({\varvec{S}}) = {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda } we fix the gauge freedom caused by rotations.

Lemma 2
{\mathbb {C}}\mathrm {ov} ({\varvec{R}} {\varvec{S}} + {\varvec{t}} {\varvec{1}}^{\intercal }) = {\varvec{R}}\, {\mathbb {C}}\mathrm {ov} ({\varvec{S}}) {\varvec{R}}^{\intercal } for any arbitrary rotation {\varvec{R}} and translation {\varvec{t}}.

Proof
By the fact that \mathbf {mean}({\varvec{R}}{\varvec{S}} + {\varvec{t}} {\varvec{1}}^{\intercal }) = \frac{1}{m}\left( {\varvec{R}}{\varvec{S}} + {\varvec{t}} {\varvec{1}}^{\intercal }\right) {\varvec{1}} = {\varvec{R}}\, \mathbf {mean}({\varvec{S}}) + {\varvec{t}} , we thus have \left( {\varvec{R}}{\varvec{S}} + {\varvec{t}} {\varvec{1}}^{\intercal } \right) - \mathbf {mean}({\varvec{R}}{\varvec{S}} + {\varvec{t}} {\varvec{1}}^{\intercal }) {\varvec{1}}^{\intercal } = {\varvec{R}} \left( {\varvec{S}} - \mathbf {mean}({\varvec{S}}){\varvec{1}}^{\intercal } \right) . \square

Let {\mathbb {C}}\mathrm {ov} ({\varvec{S}}) = {\varvec{U}} \varvec{\Lambda } {\varvec{U}}^{\intercal } be the eigenvalue decomposition of {\mathbb {C}}\mathrm {ov} ({\varvec{S}}). Then {\varvec{R}}\, {\mathbb {C}}\mathrm {ov} ({\varvec{S}}) {\varvec{R}}^{\intercal } = {\varvec{R}} {\varvec{U}} \varvec{\Lambda } {\varvec{U}}^{\intercal } {\varvec{R}}^{\intercal } admits the eigenvalue decomposition of {\mathbb {C}}\mathrm {ov} ({\varvec{R}} {\varvec{S}} + {\varvec{t}} {\varvec{1}}^{\intercal }) by Lemma 2. This means matrix {\mathbb {C}}\mathrm {ov} ({\varvec{R}} {\varvec{S}} + {\varvec{t}} {\varvec{1}}^{\intercal }) has the same eigenvalues as matrix {\mathbb {C}}\mathrm {ov} ({\varvec{S}}), which are given as the diagonal elements of \varvec{\Lambda }. Moreover, {\mathbb {C}}\mathrm {ov} ({\varvec{R}} {\varvec{S}} + {\varvec{t}} {\varvec{1}}^{\intercal }) = \varvec{\Lambda } if and only if {\varvec{R}} = {\varvec{U}}^{\intercal } which fixes the rotation.

In general, we want the reference shape to look similar to the datum shapes, thus we choose the eigenvalues of {\varvec{S}} {\varvec{S}}^{\intercal } to be close to those of the datum shape covariance matrices. Based on this idea, we present a method to estimate \varvec{\Lambda } in Sect. 4.3. However, as we shall see in what follows, the matrix \varvec{\Lambda } is never used in the intermediate calculation thus can be determined separately as a prior or posterior.

Globally Optimal Solution
Problem (8) is separable. Given {\varvec{S}}, we obtain \tilde{\varvec{A}}_i = {\varvec{S}} (\tilde{\varvec{D}}_i)^{\dagger } , where (\tilde{\varvec{D}}_i)^{\dagger } = \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} is the Moore-Penrose pseudo-inverse of \tilde{\varvec{D}}_i. Substituting \tilde{\varvec{A}}_i = {\varvec{S}} (\tilde{\varvec{D}}_i)^{\dagger } into the cost function of problem (8), we obtain:

\begin{aligned} \sum _{i=1}^n\, \Vert \tilde{\varvec{A}}_i \tilde{\varvec{D}}_i - {\varvec{S}} \Vert _F^2= & {} \sum _{i=1}^n\, \Vert {\varvec{S}} \left( \tilde{\varvec{D}}_i\right) ^{\dagger } \tilde{\varvec{D}}_i - {\varvec{S}} \Vert _F^2\\= & {} \sum _{i=1}^n\, \mathbf {tr}\left( {\varvec{S}} \left( {\varvec{I}} - \left( \tilde{\varvec{D}}_i\right) ^{\dagger } \tilde{\varvec{D}}_i \right) {\varvec{S}}^{\intercal } \right) , \end{aligned}
where we have used the fact that matrix {\varvec{I}} - (\tilde{\varvec{D}}_i)^{\dagger } \tilde{\varvec{D}}_i = {\varvec{I}} - \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \tilde{\varvec{D}}_i is symmetric and idempotent (because it is the orthogonal projection matrix to the null space of \tilde{\varvec{D}}_i). Let us denote \varvec{{\mathcal {Q}}}_{\mathrm {I}} = \sum _{i=1}^{n} (\tilde{\varvec{D}}_i)^{\dagger } \tilde{\varvec{D}}_i = \sum _{i=1}^{n} \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \tilde{\varvec{D}}_i . By using {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, the cost function can be simplified as:

\begin{aligned} \Vert \tilde{\varvec{A}}_i \tilde{\varvec{D}}_i - {\varvec{S}} \Vert _F^2 = n \mathbf {tr} (\varvec{\Lambda }) - \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {Q}}}_{\mathrm {I}} {\varvec{S}}^{\intercal } \right) . \end{aligned}
Therefore after eliminating \tilde{\varvec{A}}_i from the optimization, problem (8) reduces to:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {Q}}}_{\mathrm {I}} {\varvec{S}}^{\intercal } \right) \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \quad {\varvec{S}} {\varvec{1}} = {\varvec{0}} . \end{aligned}	(9)
Lemma 3
\varvec{{\mathcal {Q}}}_{\mathrm {I}} {\varvec{1}} = n {\varvec{1}}. Moreover if we let \varvec{{\mathcal {P}}}_{\mathrm {I}} = \sum _{i=1}^{n} \left( {\varvec{I}} - (\tilde{\varvec{D}}_i)^{\dagger } \tilde{\varvec{D}}_i \right) then \varvec{{\mathcal {P}}}_{\mathrm {I}} {\varvec{1}} = {\varvec{0}}.

Proof
The matrix (\tilde{\varvec{D}}_i)^{\dagger } \tilde{\varvec{D}}_i = \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \tilde{\varvec{D}}_i is the orthogonal projection matrix to the range space of \tilde{\varvec{D}}_i^{\intercal }. Because {\varvec{1}} \in \mathbf {Range}(\tilde{\varvec{D}}_i^{\intercal }) as {\varvec{1}} is the last column of \tilde{\varvec{D}}_i^{\intercal }, we have \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \tilde{\varvec{D}}_i {\varvec{1}} = {\varvec{1}} , thus \varvec{{\mathcal {Q}}}_{\mathrm {I}} {\varvec{1}} = \sum _{i=1}^{n} \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \tilde{\varvec{D}}_i {\varvec{1}} = n {\varvec{1}} . By noticing \varvec{{\mathcal {P}}}_{\mathrm {I}} + \varvec{{\mathcal {Q}}}_{\mathrm {I}} = n {\varvec{I}}, we have \varvec{{\mathcal {P}}}_{\mathrm {I}} {\varvec{1}} = n {\varvec{1}} - \varvec{{\mathcal {Q}}}_{\mathrm {I}} {\varvec{1}} = {\varvec{0}}. \square

Therefore problem (9) admits a special property that \varvec{{\mathcal {Q}}}_{\mathrm {I}} has an eigenvector {\varvec{1}} because \varvec{{\mathcal {Q}}}_{\mathrm {I}} {\varvec{1}} = n {\varvec{1}}. As a result, this problem admits a closed-form solution by the following proposition whose detailed proof requires Theorem 1 which we will present shortly.

Proposition 1
If {\varvec{1}} is an eigenvector of \varvec{{\mathcal {Q}}}, then the globally optimal solution to problem

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {Q}}} {\varvec{S}}^{\intercal } \right) \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \quad {\varvec{S}} {\varvec{1}} = {\varvec{0}}, \end{aligned}	(10)
is to scale by \sqrt{\varvec{\Lambda }} the d top eigenvectors of \varvec{{\mathcal {Q}}} excluding the vector {\varvec{1}}.

Proof
Using {\varvec{S}}^{\intercal } = {\varvec{X}} \sqrt{\varvec{\Lambda }} , we can rewrite problem (10) into problem (11) with {\varvec{u}} = {\varvec{1}}. By Theorem 1, the optimal {\varvec{X}} is the d top eigenvectors of \varvec{{\mathcal {Q}}} excluding the vector {\varvec{1}}. \square

We summarize the globally optimal solution to the affine GPA formulation (8) as follow.

Summary 1
Problem (8) is solved in closed-form. The optimal reference shape {\varvec{S}}^{\star } is obtained by scaling by \sqrt{\varvec{\Lambda }} the d top eigenvectors of \varvec{{\mathcal {Q}}}_{\mathrm {I}} excluding the vector {\varvec{1}}. The optimal affine transformations are given by \tilde{\varvec{A}}_i^{\star } = [{\varvec{A}}_i^{\star },\, {\varvec{t}}_i^{\star }] = {\varvec{S}}^{\star } \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \left( i \in \left[ 1:n \right] \right) .

Now we present the supporting results to Proposition 1, i.e., Theorem 1 which gives the solution to problem (11). We start with the following Lemma.

Lemma 4
Let {\varvec{M}} \in {\mathbb {R}}^{m \times m} be a symmetric matrix, and (\alpha _j, \varvec{\xi }_j) (j \in \left[ 1:m \right] ) the set of eigenvalues and unit eigenvectors of {\varvec{M}} such that {\varvec{M}} \varvec{\xi }_j = \alpha _j \varvec{\xi }_j , \Vert \varvec{\xi }_j \Vert _2 = 1 . Then {\varvec{M}} can be written as: {\varvec{M}} = \sum _{j=1}^{m} \alpha _j \varvec{\xi }_j \varvec{\xi }_j^{\intercal } .

By Lemma 4, given an arbitrary eigenvector \varvec{\xi }_k of {\varvec{M}}, by adding multiples of \varvec{\xi }_k \varvec{\xi }_k^{\intercal }, saying a \varvec{\xi }_k \varvec{\xi }_k^{\intercal } (a is an arbitrary real number) to {\varvec{M}}, we have {\varvec{M}} + a \varvec{\xi }_k \varvec{\xi }_k^{\intercal } = \sum _{j=1, j\ne k}^{m} \alpha _j \varvec{\xi }_j \varvec{\xi }_j^{\intercal } + \left( \alpha _k + a \right) \varvec{\xi }_k \varvec{\xi }_k^{\intercal } . Therefore the matrix {\varvec{M}} + a \varvec{\xi }_k \varvec{\xi }_k^{\intercal } has exactly the same set of eigenvectors as {\varvec{M}}. In {\varvec{M}} + a \varvec{\xi }_k \varvec{\xi }_k^{\intercal }, the eigenvalue of \varvec{\xi }_k becomes \alpha _k + a, while the rest remains unchanged as that in {\varvec{M}}.

Theorem 1
If {\varvec{u}} is an eigenvector of \varvec{{\mathcal {Q}}}, then the solution to the optimization problem

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{X}}}\ \mathbf {tr}\left( {\varvec{X}}^{\intercal } \varvec{{\mathcal {Q}}} {\varvec{X}} \varvec{\Lambda } \right) \qquad \mathrm {s.t.} \quad {\varvec{X}}^{\intercal } {\varvec{X}} = {\varvec{I}}, \quad {\varvec{X}}^{\intercal } {\varvec{u}} = {\varvec{0}}\nonumber \\ \end{aligned}	(11)
is the d top eigenvectors of \varvec{{\mathcal {Q}}} excluding the vector {\varvec{u}}.

Proof
Both eigenvectors and the constraint {\varvec{X}}^{\intercal } {\varvec{u}} = {\varvec{0}} are invariant to scaling, so we assume {\varvec{u}} has been normalized as \Vert {\varvec{u}} \Vert _2 = 1. If this is not the case, we can scale the constraint {\varvec{X}}^{\intercal } {\varvec{u}} = {\varvec{0}} by \frac{1}{\Vert {\varvec{u}} \Vert _2} and let \varvec{\bar{u}} = \frac{{\varvec{u}}}{\Vert {\varvec{u}} \Vert _2}.

Note that {\varvec{X}}^{\intercal } {\varvec{u}} = {\varvec{0}} \iff {\varvec{u}}^{\intercal } {\varvec{X}} = {\varvec{0}}^{\intercal } \iff {\varvec{X}} is in the null-space of {\varvec{u}}^{\intercal } \iff there exists {\varvec{Y}} such that {\varvec{X}} = \left( {\varvec{I}} - {\varvec{u}} {\varvec{u}}^{\intercal } \right) {\varvec{Y}}. Let \varvec{{\mathcal {Q}}} {\varvec{u}} = \alpha {\varvec{u}}. A straightforward expansion shows \left( {\varvec{I}} - {\varvec{u}} {\varvec{u}}^{\intercal } \right) \varvec{{\mathcal {Q}}} \left( {\varvec{I}} - {\varvec{u}} {\varvec{u}}^{\intercal } \right) = \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal } . We reparameterize problem (11) in {\varvec{Y}} as:

\begin{aligned}&\mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{Y}}}\ \mathbf {tr}\left( {\varvec{Y}}^{\intercal } \left( \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal } \right) {\varvec{Y}} \varvec{\Lambda } \right) \nonumber \\&\quad \mathrm {s.t.} \quad {\varvec{Y}}^{\intercal } \left( {\varvec{I}} - {\varvec{u}} {\varvec{u}}^{\intercal } \right) ^2 {\varvec{Y}} = {\varvec{I}} . \end{aligned}	(12)
Note that \left( \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal }\right) {\varvec{u}} = {\varvec{0}} . We thus have:

\begin{aligned} \left( \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal } \right) = \left( {\varvec{I}} - {\varvec{u}} {\varvec{u}}^{\intercal } \right) \left( \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal } \right) \left( {\varvec{I}} - {\varvec{u}} {\varvec{u}}^{\intercal } \right) . \end{aligned}
By noticing that {\varvec{X}} = \left( {\varvec{I}} - {\varvec{u}} {\varvec{u}}^{\intercal } \right) {\varvec{Y}} in problem (12), we conclude that problem (11) is equivalent to:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{X}}}\ \mathbf {tr}\left( {\varvec{X}}^{\intercal } \left( \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal } \right) {\varvec{X}} \varvec{\Lambda } \right) \qquad \mathrm {s.t.} \quad {\varvec{X}}^{\intercal } {\varvec{X}} = {\varvec{I}} .\nonumber \\ \end{aligned}	(13)
Recall that \varvec{{\mathcal {Q}}} {\varvec{u}} = \alpha {\varvec{u}}. Thus \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal } has exactly the same eigenvectors as \varvec{{\mathcal {Q}}}. However the eigenvector {\varvec{u}} has eigenvalue 0 in \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal }, while the eigenvalue is \alpha in \varvec{{\mathcal {Q}}}. The optimal solution of problem (13) are the d top eigenvectors of \varvec{{\mathcal {Q}}} - \alpha {\varvec{u}} {\varvec{u}}^{\intercal }, which are the d top eigenvectors of \varvec{{\mathcal {Q}}} excluding the vector {\varvec{u}}. \square

Estimation of the Reference Covariance Prior
We estimate the reference covariance prior \varvec{\Lambda } using the eigenvalues of the datum shape covariance matrices {\mathbb {C}}\mathrm {ov}({\varvec{D}}_i {\varvec{D}}_i^{\intercal }) \left( i \in \left[ 1:n \right] \right) . By zero-centering each datum shape {\varvec{D}}_i as \varvec{\bar{D}}_i = {\varvec{D}}_i - \frac{1}{m} {\varvec{D}}_i {\varvec{1}} {\varvec{1}}^{\intercal } , we have {\mathbb {C}}\mathrm {ov}({\varvec{D}}_i {\varvec{D}}_i^{\intercal }) = \varvec{\bar{D}}_i \varvec{\bar{D}}_i^{\intercal } . Since \varvec{\Lambda } is diagonal, we denote \varvec{\Lambda } = \mathrm {diag}\left( \lambda _1, \lambda _2, \dots , \lambda _d\right) and define the vector \varvec{ \lambda } = \left[ \lambda _1, \lambda _2, \dots , \lambda _d \right] ^{\intercal }. Abusing notations, we collect the eigenvalues of each datum shape covariance {\varvec{D}}_i {\varvec{D}}_i^{\intercal } by a diagonal matrix \varvec{\Lambda }_i and define the vector \varvec{ \lambda }_i = \left[ \lambda _1^{(i)}, \lambda _2^{(i)}, \dots , \lambda _d^{(i)} \right] ^{\intercal }. Without loss of generality, we assume that the elements in \varvec{ \lambda }_i have been sorted in the descending (or non-ascending) order such that \lambda _1^{(i)} \ge \lambda _2^{(i)} \ge \dots \ge \lambda _d^{(i)} \ge 0. The task is now to estimate \varvec{ \lambda } from \varvec{ \lambda }_i \left( i \in \left[ 1:n \right] \right) .

To proceed, we consider the geometric implication of \sqrt{\varvec{ \lambda }_i} = \left[ \sqrt{\lambda _1^{(i)}}, \sqrt{\lambda _2^{(i)}}, \dots , \sqrt{\lambda _d^{(i)}} \right] ^{\intercal }, i.e., the vector comprising the d leftmost singular values of shape \varvec{\bar{D}}_i. It has been known in (Horn 1987) that the scale of the shape {\varvec{D}}_i can be represented by \frac{1}{\sqrt{m}}\Vert \varvec{\bar{D}}_i \Vert _F. Dropping the common constant \frac{1}{\sqrt{m}}, we consider:

\begin{aligned} \Vert \varvec{\bar{D}}_i \Vert _F= & {} \sqrt{ \mathbf {tr}\left( \varvec{\bar{D}}_i \varvec{\bar{D}}_i^{\intercal } \right) }\\= & {} \sqrt{ \mathbf {tr}\left( \varvec{\Lambda }_i \right) } = \sqrt{ \lambda _1^{(i)} + \lambda _2^{(i)} + \dots + \lambda _d^{(i)} } = \Vert \sqrt{\varvec{ \lambda }_i} \Vert _2 . \end{aligned}
This suggests that \sqrt{\varvec{ \lambda }_i} is a vector with each of its component representing the scale along the corresponding Euclidean axis, and its length the overall shape scale. Given \sqrt{\varvec{ \lambda }_1}, \sqrt{\varvec{ \lambda }_2}, \dots , \sqrt{\varvec{ \lambda }_n} from n datum shapes, we are interested in finding \sqrt{\varvec{\lambda }} = \left[ \sqrt{\lambda _1}, \sqrt{\lambda _2}, \dots , \sqrt{\lambda _d} \right] ^{\intercal }.

The reference shape is defined up to scale (i.e., up to a similarity transformation), thus in \sqrt{\varvec{\lambda }}, all that matters is the proportion of its components, namely the direction of the vector \sqrt{\varvec{\lambda }}. Therefore we propose to estimate the direction of \sqrt{\varvec{\lambda }} on the unit ball, denoted by \varvec{\theta }, by minimizing the angles between \varvec{\theta } and each \sqrt{\varvec{\lambda }_i} via maximizing their inner products as:

\begin{aligned} \varvec{\theta }^{\star } = \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{\varvec{\theta } \in {\mathbb {R}}^d}\, \sum _{i=1}^n \left( \varvec{\theta }^{\intercal } \frac{\sqrt{\varvec{ \lambda }_i}}{\Vert \sqrt{\varvec{ \lambda }_i} \Vert _2} \right) ^2 \qquad \mathrm {s.t.} \quad \Vert \varvec{\theta } \Vert _2 = 1 . \end{aligned}	(14)
Upon defining \varvec{\Pi } = \begin{bmatrix} \frac{\sqrt{\varvec{ \lambda }_1}}{\Vert \sqrt{\varvec{ \lambda }_1} \Vert _2}&\frac{\sqrt{\varvec{ \lambda }_2}}{\Vert \sqrt{\varvec{ \lambda }_2} \Vert _2}&\cdots&\frac{\sqrt{\varvec{ \lambda }_n}}{\Vert \sqrt{\varvec{ \lambda }_n} \Vert _2} \end{bmatrix} , problem (14) can be rewritten as the Rayleigh quotient optimization:

\begin{aligned} \varvec{\theta }^{\star } = \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{\varvec{\theta } \in {\mathbb {R}}^d}\, \varvec{\theta }^{\intercal } \varvec{\Pi } \varvec{\Pi }^{\intercal } \varvec{\theta } \qquad \mathrm {s.t.} \quad \Vert \varvec{\theta } \Vert _2 = 1 , \end{aligned}
whose solution is given in closed-form, which is the leftmost left singular vector of the matrix \varvec{\Pi }, or equivalently the top eigenvector of the matrix \varvec{\Pi } \varvec{\Pi }^{\intercal }. Since \varvec{\Pi } \varvec{\Pi }^{\intercal } has non-negative elements, by the Perron-Frobenius theorem, the elements of \varvec{\theta }^{\star } can be chosen all non-negative.

Given \varvec{\theta }^{\star }, we can choose \sqrt{\varvec{\lambda }} = s \varvec{\theta }^{\star } up to a scale factor s > 0. In practice, it is desirable to choose \sqrt{\varvec{\lambda }} to be at the similar scale of \sqrt{\varvec{\lambda }_i}. Therefore, we choose the average scale s = \frac{1}{n} \sum _{i=1}^{n} \Vert \sqrt{\varvec{ \lambda }_i} \Vert _2.

The Coordinate Transformation of Datum Shapes
We show that by applying (possibly distinct) arbitrary rigid transformations to each datum shape {\varvec{D}}_i, the optimal reference shape of problem (8) remains unchanged. Thus formulation (8) is unbiased when facing coordinate transformations.

Lemma 5
Let {\varvec{D}}' = {\varvec{R}}{\varvec{D}} + {\varvec{t}} {\varvec{1}}^{\intercal }. We further denote \tilde{\varvec{D}} = \begin{bmatrix} {\varvec{D}} \\ {\varvec{1}}^{\intercal } \end{bmatrix} and \tilde{\varvec{D}}' = \begin{bmatrix} {\varvec{D}}' \\ {\varvec{1}}^{\intercal } \end{bmatrix}. Then we have \mathbf {Range} (\tilde{\varvec{D}}'^{\intercal }) = \mathbf {Range} (\tilde{\varvec{D}}^{\intercal }) and the orthogonal projection matrices thus satisfy:

\begin{aligned} \tilde{\varvec{D}}'^{\intercal } \left( \tilde{\varvec{D}}' \tilde{\varvec{D}}'^{\intercal }\right) ^{-1} \tilde{\varvec{D}}' = \tilde{\varvec{D}}^{\intercal } \left( \tilde{\varvec{D}} \tilde{\varvec{D}}^{\intercal }\right) ^{-1} \tilde{\varvec{D}} . \end{aligned}
Proof
The matrices \tilde{\varvec{D}}'^{\intercal } and \tilde{\varvec{D}}^{\intercal } have the same range space, i.e., \mathbf {Range} (\tilde{\varvec{D}}'^{\intercal }) = \mathbf {Range} (\tilde{\varvec{D}}^{\intercal }), because:

\begin{aligned} \tilde{\varvec{D}}'^{\intercal } = \begin{bmatrix} {\varvec{D}}^{\intercal } {\varvec{R}}^{\intercal } + {\varvec{1}} {\varvec{t}}^{\intercal }&{\varvec{1}} \end{bmatrix} = \tilde{\varvec{D}}^{\intercal } \begin{bmatrix} {\varvec{R}}^{\intercal } &{} {\varvec{0}} \\ {\varvec{t}}^{\intercal } &{} 1 \end{bmatrix} . \end{aligned}
The orthogonal projection matrices (also called orthogonal projectors) to \mathbf {Range} (\tilde{\varvec{D}}'^{\intercal }) and \mathbf {Range} (\tilde{\varvec{D}}^{\intercal }) are the same by uniqueness (Meyer 2000). \square

Proposition 2
In problem (8), when we apply arbitrary rigid transformations to each datum shape {\varvec{D}}_i, the matrix \varvec{{\mathcal {Q}}}_{\mathrm {I}} remains the same. So does the optimal reference shape.

Proof
By Lemma 5, matrix \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \tilde{\varvec{D}}_i remains the same when we apply arbitrary rigid transformations to the datum shape {\varvec{D}}_i. Thus \varvec{{\mathcal {Q}}}_{\mathrm {I}} = \sum _{i=1}^{n} \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \tilde{\varvec{D}}_i remains the same as well. \square

Connection to Classical Results by Eliminating Translations
We recapitulate the key idea of affine GPA in (Rohlf and Slice 1990) as follows in formulation (15), and show that this approach attains the same result as formulation (8) while the latter is more general. In formulation (8), the optimal {\varvec{t}}_i given {\varvec{A}}_i and {\varvec{S}} is {\varvec{t}}_i = - \frac{1}{m} ({\varvec{A}}_i {\varvec{D}}_i - {\varvec{S}}) {\varvec{1}} . Moreover if {\varvec{S}} {\varvec{1}} = {\varvec{0}}, we have {\varvec{t}}_i = - \frac{1}{m} {\varvec{A}}_i {\varvec{D}}_i {\varvec{1}}. Substituting the estimate {\varvec{t}}_i = - \frac{1}{m} {\varvec{A}}_i {\varvec{D}}_i {\varvec{1}} back to formulation (15), we have:

\begin{aligned} {\left\{ \begin{array}{ll} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{\{{\varvec{A}}_i\},\, {\varvec{S}}} \quad &{} \sum _{i=1}^n\, \Vert {\varvec{A}}_i \varvec{\bar{D}}_i - {\varvec{S}} \Vert _F^2\\ \mathrm {s.t.} \quad &{} {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda } , \end{array}\right. } \end{aligned}	(15)
where \varvec{\bar{D}}_i = {\varvec{D}}_i - \frac{1}{m} {\varvec{D}}_i {\varvec{1}} {\varvec{1}}^{\intercal } \left( i \in \left[ 1:n \right] \right) are zero-centered datum shapes. Let \varvec{\mathfrak {Q}}_{\circ } = \sum _{i=1}^{n} \varvec{\bar{D}}_i^{\intercal } (\varvec{\bar{D}}_i\varvec{\bar{D}}_i^{\intercal })^{-1} \varvec{\bar{D}}_i. Following a similar derivation to Sect. 4.2, we can reduce problem (15) to formulation (6) with \varvec{{\mathcal {Q}}} = \varvec{\mathfrak {Q}}_{\circ }, thus the optimal reference shape {\varvec{S}} of problem (15) is to scale the d top eigenvectors of \varvec{\mathfrak {Q}}_{\circ } by \sqrt{\varvec{\Lambda }}. We drop the constraint {\varvec{S}} {\varvec{1}} = {\varvec{0}} in formulation (15) because this constraint is automatically satisfied for the optimal {\varvec{S}} of formulation (15), as \varvec{\mathfrak {Q}}_{\circ } {\varvec{1}} = {\varvec{0}} thus {\varvec{1}} is orthogonal to the d top eigenvectors corresponding to nonzero eigenvalues.

Proposition 3
\varvec{{\mathcal {Q}}}_{\mathrm {I}} = \frac{n}{m} {\varvec{1}} {\varvec{1}}^{\intercal } + \varvec{\mathfrak {Q}}_{\circ } . The d top eigenvectors of \varvec{{\mathcal {Q}}}_{\mathrm {I}} excluding the vector {\varvec{1}} are the d top eigenvectors of \varvec{\mathfrak {Q}}_{\circ }, thus problems (8) and (15) give the same optimal reference shape {\varvec{S}}.

Proof
Let us denote the homogeneous form of the zero-centered datum shape as:

\begin{aligned} {\tilde{{\bar{\varvec{D}}}}}_i = \begin{bmatrix} \varvec{\bar{D}}_i \\ {\varvec{1}}^{\intercal } \end{bmatrix} ,\quad \mathrm {thus}\quad {\tilde{{\bar{\varvec{D}}}}}_i {\tilde{{\bar{\varvec{D}}}}}_i^{\intercal } = \begin{bmatrix} \varvec{\bar{D}}_i \varvec{\bar{D}}_i^{\intercal } &{} {\varvec{O}} \\ {\varvec{O}} &{} m \end{bmatrix} , \end{aligned}
where we have used the fact that \varvec{\bar{D}}_i {\varvec{1}} = {\varvec{0}}. From Proposition 2, we have \varvec{{\mathcal {Q}}}_{\mathrm {I}} = \sum _{i=1}^{n} \tilde{\varvec{D}}_i^{\intercal } (\tilde{\varvec{D}}_i \tilde{\varvec{D}}_i^{\intercal })^{-1} \tilde{\varvec{D}}_i = \sum _{i=1}^{n} {\tilde{{\bar{\varvec{D}}}}}_i^{\intercal } \left( {\tilde{{\bar{\varvec{D}}}}}_i {\tilde{{\bar{\varvec{D}}}}}_i^{\intercal } \right) ^{-1} {\tilde{{\bar{\varvec{D}}}}}_i . By a straightforward calculation, it can be verified that:

\begin{aligned} {\tilde{{\bar{\varvec{D}}}}}_i^{\intercal } \left( {\tilde{{\bar{\varvec{D}}}}}_i {\tilde{{\bar{\varvec{D}}}}}_i^{\intercal } \right) ^{-1} {\tilde{{\bar{\varvec{D}}}}}_i = \frac{1}{m} {\varvec{1}}{\varvec{1}}^{\intercal } + \varvec{\bar{D}}_i^{\intercal } \left( \varvec{\bar{D}}_i\varvec{\bar{D}}_i^{\intercal }\right) ^{-1} \varvec{\bar{D}}_i , \end{aligned}
thus we obtain \varvec{{\mathcal {Q}}}_{\mathrm {I}} = \frac{n}{m} {\varvec{1}} {\varvec{1}}^{\intercal } + \varvec{\mathfrak {Q}}_{\circ } . We notice \varvec{\mathfrak {Q}}_{\circ } {\varvec{1}} = {\varvec{0}} because of \varvec{\bar{D}}_i {\varvec{1}} = {\varvec{0}}, which means {\varvec{1}} is an eigenvector of \varvec{\mathfrak {Q}}_{\circ } with eigenvalue 0. Therefore \varvec{{\mathcal {Q}}}_{\mathrm {I}} and \varvec{\mathfrak {Q}}_{\circ } have the same eigenvectors by Lemma 4, while {\varvec{1}} corresponds to the largest eigenvalue n in \varvec{{\mathcal {Q}}}_{\mathrm {I}} and in contrast to the smallest eigenvalue 0 in \varvec{\mathfrak {Q}}_{\circ }. \square

Generalized Procrustes Analysis with the Deformation Model
We consider GPA with the deformation model as nonlinear warps. In particular, we consider a class of generalized warps, termed LBWs, whose transformation parameters are linear with respect to the nonlinear basis functions that lift source points to the higher-dimensional feature space. In this section, we consider full shape registration and postpone the discussion of partial shape registration to Sect. 6.

Linear Basis Warps
A warp is a generalized transformation, that maps a source point {\varvec{p}} \in {\mathbb {R}}^d \left( d\in \left\{ 2,3\right\} \right) to its target point {\varvec{p}}' \in {\mathbb {R}}^d. The LBW is expressed as a linear combination of a set of basis functions.

Formulation
Let \varvec{\beta }({\varvec{p}}) be a vector of basis functions:

\begin{aligned} \varvec{\beta }({\varvec{p}}) = \left[ \beta _1 ({\varvec{p}}),\, \beta _2 ({\varvec{p}}), \dots , \beta _l ({\varvec{p}}) \right] ^{\intercal } , \end{aligned}
with each element \beta _k ({\varvec{p}}): {\mathbb {R}}^d \rightarrowtail {\mathbb {R}} \left( k \in \left[ 1:l \right] \right) being a scalar basis function. The vectorized basis function \varvec{\beta }({\varvec{p}}): {\mathbb {R}}^d \rightarrowtail {\mathbb {R}}^l brings a d-dimensional point {\varvec{p}} to the l-dimensional feature space, thus is also termed a feature mapping in the context of linear regression models (Bishop 2006). Given the basis function \varvec{\beta }({\varvec{p}}), we write a warp model, {\mathcal {W}} ({\varvec{p}}, {\varvec{W}}): {\mathbb {R}}^d \rightarrowtail {\mathbb {R}}^d, as:

\begin{aligned} {\mathcal {W}} \left( {\varvec{p}}, {\varvec{W}}\right) = {\varvec{W}}^{\intercal } \varvec{\beta }({\varvec{p}}) , \end{aligned}
(16)
with {\varvec{W}} \in {\mathbb {R}}^{l \times d} being the unknown weight matrix. An example is provided in Appendix A.2, showing how to write the TPS warp in this form.

Operating on Point Clouds
For a point-cloud of m points in a matrix {\varvec{D}} = \left[ {\varvec{p}}_1,\, {\varvec{p}}_2, \dots , {\varvec{p}}_m \right] \in {\mathbb {R}}^{d \times m}. We apply the warp {\mathcal {W}} ({\varvec{p}}, {\varvec{W}}) to each point in {\varvec{D}} to obtain its warped version. Abusing notations, we write the result as:

\begin{aligned} {\mathcal {W}} ({\varvec{D}}, {\varvec{W}}) = {\varvec{W}}^{\intercal } \varvec{{\mathcal {B}}}({\varvec{D}}) , \end{aligned}
(17)
with \varvec{{\mathcal {B}}}({\varvec{D}}) \in {\mathbb {R}}^{l \times m} collecting the feature of each point in {\varvec{D}} as its columns:

\begin{aligned} \varvec{{\mathcal {B}}}({\varvec{D}}) = \left[ \varvec{\beta }\left( {\varvec{p}}_1\right) ,\, \varvec{\beta }\left( {\varvec{p}}_2\right) , \dots , \varvec{\beta }\left( {\varvec{p}}_m\right) \right] . \end{aligned}
Regularization
The warp is often used with a regularization term to avoid over-fitting, for instance, if the dimension of the feature space is greater or equivalent to the number of points in the point-cloud. In the context of deformations, such a term is formed from the partial derivatives of the warp, with different physical implications. In particular, the second-order derivatives are used in the TPS warp:

\begin{aligned} {\mathcal {R}} ({\varvec{W}}) = \int _{{\mathbb {R}}^d}\, \left\| \frac{\partial ^2}{\partial {\varvec{p}}^2} {\mathcal {W}} ({\varvec{p}}, {\varvec{W}}) \right\| _F^2\, d {\varvec{p}} . \end{aligned}
This is directly proportional to the bending energy. The bending energy term is exactly zero if and only if the warp is affine (Bookstein 1989). Other possibilities of regularizations include the spring term used in elastic registration (Christensen and He 2001), and the viscosity term used in fluid registration (Bro-Nielsen and Gramkow 1996).

For the TPS warp, the integral can be solved in closed-form, as a quadratic form of the transformation parameters:

\begin{aligned} {\mathcal {R}} ({\varvec{W}}) = \Vert {\varvec{Z}} {\varvec{W}} \Vert _F^2. \end{aligned}
(18)
Here {\varvec{Z}} is given as the square root of the bending energy matrix, see Appendix A.3. For other warps, we assume the regularization term can be fairly approximated by the quadratic form as well.

The regularization term is however not compulsory. It is always possible to avoid over-fitting by limiting the dimension of the feature space, by choosing a smaller l \ll m (Rueckert et al. 1999). For completeness of the discussion, we consider the case with regularization.

Examples of Linear Basis Warps
The LBW generalizes over many deformation models, e.g., the Free-Form Deformations (FFD) (Rueckert et al. 1999; Szeliski and Coughlan 1997), and the Radial Basis Functions (RBF) (Bookstein 1989; Fornefett et al. 2001). Concretely we will use the Thin-Plate Spline (TPS) (Duchon 1976; Bookstein 1989), a theoretically principled RBF that minimizes the overall bending energy, in the practical implementation of our theory. An introduction of the TPS warp as an LBW is provided in Appendix A.

The affine transformation is a special case of the LBW without regularization. This can be shown from the homogeneous form in Eq. (7), by setting {\varvec{W}}^{\intercal } = \begin{bmatrix} {\varvec{A}}&{\varvec{t}} \end{bmatrix}, \varvec{{\mathcal {B}}} ({\varvec{D}}) = \begin{bmatrix} {\varvec{D}} \\ {\varvec{1}}^{\intercal } \end{bmatrix}.

Generalized Procrustes Analysis with Linear Basis Warps
We consider the case without missing datum points, and use the LBW in formulation (2). We constrain the reference shape to be zero centered by {\varvec{S}} {\varvec{1}} = {\varvec{0}}. The shape constraint {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda } is used to enforce the rigidity of the solution, and the reference covariance prior \varvec{\Lambda } is estimated as in Sect. 4.3. We propose the following formulation for deformable GPA:

\begin{aligned} \mathrm {II:}\quad {\left\{ \begin{array}{ll} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{\{{\varvec{W}}_i\},\, {\varvec{S}}}\quad &{} \sum _{i=1}^n\, \Vert {\varvec{W}}_i^{\intercal } \varvec{{\mathcal {B}}}_i\left( {\varvec{D}}_i\right) - {\varvec{S}} \Vert _{F}^2 + \sum _{i=1}^n\, \mu _i \Vert {\varvec{Z}}_i{\varvec{W}}_i \Vert _F^2\\ \mathrm {s.t.}\quad &{} {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \quad {\varvec{S}} {\varvec{1}} = {\varvec{0}}. \end{array}\right. } \end{aligned}
(19)
Although being termed deformable GPA, formulation (19) includes the affine-GPA formulation (8) in homogeneous form as a special case. The translation part cannot be identified directly from the LBW, thus needs to be estimated jointly inside the transformation parameters.

To proceed, we define the shorthand \varvec{{\mathcal {B}}}_i {\mathop {=}\limits ^{\text {def}}}\varvec{{\mathcal {B}}}_i({\varvec{D}}_i) . The problem is separable. Given a reference shape {\varvec{S}}, the estimate of {\varvec{W}}_i is given in closed-form by solving the linear least-squares problem whose solution is:

\begin{aligned} {\varvec{W}}_i = \left( \varvec{{\mathcal {B}}}_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i\right) ^{-1} \varvec{{\mathcal {B}}}_i {{\varvec{S}}}^{\intercal } . \end{aligned}
(20)
Substituting Eq. (20) into problem (19), we obtain an optimization problem with respect to {\varvec{S}} only:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {P}}}_{\mathrm {II}} {\varvec{S}}^{\intercal }\right) \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \quad {\varvec{S}} {\varvec{1}} = {\varvec{0}} , \end{aligned}
(21)
where matrix \varvec{{\mathcal {P}}}_{\mathrm {II}} is defined as:

\begin{aligned} \varvec{{\mathcal {P}}}_{\mathrm {II}} = \sum _{i=1}^{n} \left( {\varvec{I}} - \varvec{{\mathcal {B}}}_i^{\intercal } \left( \varvec{{\mathcal {B}}}_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i\right) ^{-1} \varvec{{\mathcal {B}}}_i \right) . \end{aligned}
We define {\varvec{Q}}_i = \varvec{{\mathcal {B}}}_i^{\intercal } \left( \varvec{{\mathcal {B}}}_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i\right) ^{-1} \varvec{{\mathcal {B}}}_i, and

\begin{aligned} \varvec{{\mathcal {Q}}}_{\mathrm {II}} = \sum _{i=1}^{n} \varvec{{\mathcal {B}}}_i^{\intercal } \left( \varvec{{\mathcal {B}}}_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i\right) ^{-1} \varvec{{\mathcal {B}}}_i = \sum _{i=1}^{n} {\varvec{Q}}_i . \end{aligned}
Then \varvec{{\mathcal {P}}}_{\mathrm {II}} = \sum _{i=1}^{n} \left( {\varvec{I}} - {\varvec{Q}}_i\right) = n {\varvec{I}} - \sum _{i=1}^{n} {\varvec{Q}}_i , and \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {P}}}_{\mathrm {II}} {\varvec{S}}^{\intercal }\right) = n \mathbf {tr}\left( \varvec{\Lambda } \right) - \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {Q}}}_{\mathrm {II}} {\varvec{S}}^{\intercal }\right) . Problem (21) can thus be equivalently written as a maximization problem:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {Q}}}_{\mathrm {II}} {\varvec{S}}^{\intercal }\right) \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \quad {\varvec{S}} {\varvec{1}} = {\varvec{0}} . \end{aligned}
(22)
If {\varvec{1}} is an eigenvector of \varvec{{\mathcal {Q}}}_{\mathrm {II}}, or equivalently if {\varvec{1}} is an eigenvector of \varvec{{\mathcal {P}}}_{\mathrm {II}}, then Problem (21) and Problem (22) can be solved globally by Proposition 1. Once we obtain the optimal reference shape {\varvec{S}}^{\star }, the optimal transformation parameters can be calculated by Eq. (20).

Eigenvector Characterization and Globally Optimal Solution
Now we characterize a class of LBWs for which the resulting \varvec{{\mathcal {P}}}_{\mathrm {II}} and \varvec{{\mathcal {Q}}}_{\mathrm {II}} have an eigenvector {\varvec{1}}. To this end, we prove that the following statements are equivalent. The proofs have been moved to Appendix B to benefit easy reading.

Theorem 2
The following statements are equivalent:

(a)
\varvec{{\mathcal {P}}}_{\mathrm {II}} {\varvec{1}} = {\varvec{0}}.

(b)
\varvec{{\mathcal {Q}}}_{\mathrm {II}} {\varvec{1}} = n {\varvec{1}}.

(c)
{\varvec{Q}}_i {\varvec{1}} = {\varvec{1}}.

(d)
The cost function \mathbf {tr}\left( {\varvec{S}} \varvec{{\mathcal {P}}}_{\mathrm {II}} {\varvec{S}}^{\intercal }\right) is invariant to translations.

(e)
There exists {\varvec{x}} such that \varvec{{\mathcal {B}}}_i({\varvec{D}}_i)^{\intercal } {\varvec{x}} = {\varvec{1}}; moreover if \mu _i > 0, {\varvec{x}} must satisfy {\varvec{Z}}_i {\varvec{x}} = {\varvec{0}}.

Theorem 2 relates various aspects of the LBW based GPA to the existence of an eigenvector {\varvec{1}} in \varvec{{\mathcal {P}}}_{\mathrm {II}} and \varvec{{\mathcal {Q}}}_{\mathrm {II}}. In particular, case (d) states that the reduced problem is invariant to translations, and case (e) stipulates the rules that the LBW as a mapping must follow. While case (e) in Theorem 2 is related to the datum shape {\varvec{D}}_i, we show in what follows that it can be satisfied if the LBW satisfies certain property, making the statement independent of the input datum shapes.

Now we show a sufficient condition to case (e) in Theorem 2 which is that the LBW must contain free-translations. We drop the subscript i and use the notation \varvec{{\mathcal {B}}}\left( \cdot \right) to indicate that this is a property of the LBW thus is independent of the warp input.

Definition 1
The LBW, given by \{{\varvec{W}}^{\intercal } \varvec{{\mathcal {B}}}\left( \cdot \right) ,\, {\varvec{Z}} {\varvec{W}},\, \mu \} is said to contain free-translations if: there exists {\varvec{x}} such that \varvec{{\mathcal {B}}}\left( \cdot \right) ^{\intercal } {\varvec{x}} = {\varvec{1}}; moreover if \mu > 0, {\varvec{x}} must satisfy {\varvec{Z}} {\varvec{x}} = {\varvec{0}}.

We now explain the idea behind Definition 1. The LBW contains free-translations, if we can recover the translation vector {\varvec{t}} explicitly by an invertible matrix {\varvec{G}} such that:


where {\varvec{W}}^{\intercal } {\varvec{G}}^{-1} contains a column vector {\varvec{t}} and \varvec{{\mathcal {B}}}\left( \cdot \right) ^{\intercal } {\varvec{G}}^{\intercal } contains a column vector {\varvec{1}}. Therefore {\varvec{1}} \in \mathbf {Range}\left( \varvec{{\mathcal {B}}}\left( \cdot \right) ^{\intercal } {\varvec{G}}^{\intercal } \right) = \mathbf {Range}\left( \varvec{{\mathcal {B}}}\left( \cdot \right) ^{\intercal } \right) . Using {\varvec{G}}, the term {\varvec{Z}} {\varvec{W}} can be decomposed as:


Without loss of generality, we assume that in {\varvec{{\mathcal {B}}}\left( \cdot \right) ^{\intercal } {\varvec{G}}^{\intercal }} the k-th column vector is {\varvec{1}}, identified by the standard basis vector {\varvec{e}}_k, such that \varvec{{\mathcal {B}}}\left( \cdot \right) ^{\intercal } {\varvec{G}}^{\intercal } {\varvec{e}}_k = {\varvec{1}}. Then it can be easily verified that the translation {\varvec{t}} is constraint-free in {\varvec{Z}} {\varvec{W}} if and only if the k-th column of {\varvec{Z}} {\varvec{G}}^{\intercal } is {\varvec{0}}, i.e., {\varvec{Z}} {\varvec{G}}^{\intercal } {\varvec{e}}_k = {\varvec{0}}. Since \varvec{{\mathcal {B}}}\left( \cdot \right) ^{\intercal } has full column rank, so {\varvec{G}}^{\intercal }{\varvec{e}}_k is the unique {\varvec{x}} such that \varvec{{\mathcal {B}}}\left( \cdot \right) ^{\intercal } {\varvec{x}} = {\varvec{1}}, {\varvec{Z}} {\varvec{x}} = {\varvec{0}}.

Proposition 4
The statements in Theorem 2 are satisfied if the LBW contains free-translations.

We state that the affine transformation and the TPS warp satisfy Definition 1. The proof for the affine case is straightforward. For the TPS warp, the algebraic proof is given in Appendix A.4. Intuitively, this can be explained because for the TPS warp, the bending energy term only affects the nonlinear part of the warp, which means that the linear part of the TPS warp is constraint-free and so is its translational component.

Proposition 5
The affine transformation and the TPS warp satisfy the statement of Theorem 2.

The above results are summarized as follow:

Summary 2
Formulation (19) can be solved globally if the the LBW contains free-translations. The optimal reference shape {\varvec{S}}^{\star } is to scale by \sqrt{\varvec{\Lambda }} the d top eigenvectors of \varvec{{\mathcal {Q}}}_{\mathrm {II}} (or equivalently the d bottom eigenvectors of \varvec{{\mathcal {P}}}_{\mathrm {II}}) excluding the eigenvector {\varvec{1}}. The optimal transformation parameters are {\varvec{W}}_i = \left( \varvec{{\mathcal {B}}}_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i\right) ^{-1} \varvec{{\mathcal {B}}}_i {{\varvec{S}}^{\star }}^{\intercal } \left( i \in \left[ 1:n \right] \right) .

Reformulation Using the Soft Constraint
In Theorem 1, we have proved the equivalence of formulations (11) and (13). Therefore, formulation (22) can be equivalently written as:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \left( \varvec{{\mathcal {Q}}}_{\mathrm {II}} - n \varvec{\bar{1}}\varvec{\bar{1}}^{\intercal } \right) {\varvec{S}}^{\intercal }\right) \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \end{aligned}
(23)
where we have used \varvec{{\mathcal {Q}}}_{\mathrm {II}} \varvec{\bar{1}} = n \varvec{\bar{1}}, with \varvec{\bar{1}} being normalized.

In problem (23), we can replace n with any n' \ge n. This is because \left( \varvec{{\mathcal {Q}}}_{\mathrm {II}} - n' \varvec{\bar{1}} \varvec{\bar{1}}^{\intercal } \right) \varvec{\bar{1}} = (n - n') \varvec{\bar{1}} , where the eigenvalue corresponding to the eigenvector \varvec{\bar{1}} becomes (n - n') \le 0, while the rest of the eigenvalue of \left( \varvec{{\mathcal {Q}}}_{\mathrm {II}} - \nu \varvec{\bar{1}}\varvec{\bar{1}}^{\intercal } \right) is nonnegative. Thus the eigenvector \varvec{\bar{1}} is excluded in the solution for any n' \ge n. The rest of the eigenvectors remain unchanged. Expanding the cost of problem (23), we obtain:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{max}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \left( \varvec{{\mathcal {Q}}}_{\mathrm {II}} \right) {\varvec{S}}^{\intercal }\right) - n' \Vert {\varvec{S}} \varvec{\bar{1}} \Vert _2^2 \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \end{aligned}
(24)
where n' \ge n. Finally we substitute \varvec{{\mathcal {P}}}_{\mathrm {II}} = n {\varvec{I}} - \varvec{{\mathcal {Q}}}_{\mathrm {II}} and \varvec{\bar{1}} = \frac{1}{m}{\varvec{1}}, then obtain a minimization problem:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \left( \varvec{{\mathcal {P}}}_{\mathrm {II}} \right) {\varvec{S}}^{\intercal }\right) + \nu \Vert {\varvec{S}} {\varvec{1}} \Vert _2^2 \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \end{aligned}
(25)
where \nu \ge n/m. Note that problem (25) is equivalent to problem (21), while the hard constraint {\varvec{S}} {\varvec{1}} = {\varvec{0}} has been reformulated as a soft constraint in the form of a penalty term.

At last, formulation (19) is equivalent to the following one with the soft constraint:

\begin{aligned}&\mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{\{{\varvec{W}}_i\},\, {\varvec{S}}}\quad \sum _{i=1}^n\, \Vert {\varvec{W}}_i^{\intercal } \varvec{{\mathcal {B}}}_i({\varvec{D}}_i) - {\varvec{S}} \Vert _{F}^2 + \sum _{i=1}^n\, \mu _i \Vert {\varvec{Z}}_i {\varvec{W}}_i \Vert _F^2 \nonumber \\&\quad + \nu \Vert {\varvec{S}} {\varvec{1}} \Vert _2^2\nonumber \\&\quad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }. \end{aligned}
(26)
The equivalence can be easily shown by noting that given {\varvec{S}}, the term \nu \Vert {\varvec{S}} {\varvec{1}} \Vert _2^2 becomes a constant, thus the relation between the estimates of {\varvec{W}}_i and {\varvec{S}} remains the same. Then with an analogous derivation to Sect. 5.2, after eliminating {\varvec{W}}_i, problem (26) is reduced to problem (25).

It is worth mentioning that formulation (26) is equivalent to formulation (19) for any LBW that satisfies Theorem 2 (for example, the affine transformation and the TPS warp). However formulation (26) is more tractable in terms of solution methods, since there is no need to take care of the hard constraint {\varvec{S}} {\varvec{1}} = {\varvec{0}}, which can be difficult if Theorem 2 is not satisfied (e.g., the translation part is constrained). In any case, formulation (26) can be reduced to a Brockett cost function, which can be easily solved globally.

Generalized Procrustes Analysis with Partial Shapes
Now we have equipped with enough insights to discuss GPA with partial shapes. We will follow the soft regularized method discussed in Sect. 5.4, which generalizes over the cases where Theorem 2 is not satisfied. As the affine transformation is a special case of the LBW, we consider GPA with the LBW only.

Estimating the Reference Covariance Prior
The reference shape {\varvec{S}} is a full shape. In order to estimate the reference covariance prior \varvec{\Lambda }, we recover a full shape representation for each {\varvec{D}}_i \left( i \in \left[ 1:n \right] \right) . Such a process is meant to be cheap, thus we use the classical pairwise similarity GPA to compute the similarity transformation between datum shapes, and then complete the missing points by averaging their occurrence in other shapes.

For each partial shape {\varvec{D}}_i, we compute the pairwise similarity transformation between {\varvec{D}}_i and {\varvec{D}}_k \left( k \in \left[ 1: n\right] \right) by:

\begin{aligned} \hat{s}_{ik},\, \varvec{\hat{R}}_{ik},\, \varvec{\hat{t}}_{ik} = \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{s_{ik},\,{\varvec{R}}_{ik},\,{\varvec{t}}_{ik}}\ \Vert \left( s_{ik} {\varvec{R}}_{ik} {\varvec{D}}_k + {\varvec{t}}_{ik} {\varvec{1}}^{\intercal } - {\varvec{D}}_i \right) \varvec{\Gamma }_i \varvec{\Gamma }_k \Vert _F^2 , \end{aligned}
where {\varvec{R}}_{ik} is an orthonormal matrix, s_{ik} a non-negative scalar, and {\varvec{t}}_{ik} a d-dimensional translation vector. This classical Procrustes problem can be solved in closed-form (see Appendix D).

Subsequently we complete the missing points in {\varvec{D}}_i using their occurrence in other shapes by:

\begin{aligned} \varvec{\mathfrak {D}}_i ={\varvec{D}}_i \varvec{\Gamma }_i + \varvec{\hat{D}}_i \varvec{\Gamma }_{+}^{-1} \left( {\varvec{I}} - \varvec{\Gamma }_i\right) , \end{aligned}
(27)
to obtain a full shape \varvec{\mathfrak {D}}_i. Here \varvec{\hat{D}}_i = \sum _{k=1}^{n} \left( \hat{s}_{ik} \varvec{\hat{R}}_{ik} {\varvec{D}}_k + \varvec{\hat{t}}_{ik} {\varvec{1}}^{\intercal } \right) \varvec{\Gamma }_k and \varvec{\Gamma }_{+} = \sum _{k=1}^{n} \varvec{\Gamma }_k. Then we estimate the reference covariance prior \varvec{ \Lambda } by the result in Sect. 4.3 based on the zero-centered full shapes \varvec{\bar{\mathfrak {D}}}_i = \varvec{\mathfrak {D}}_i - \frac{1}{m} \varvec{\mathfrak {D}}_i {\varvec{1}} {\varvec{1}}^{\intercal } , which is detailed in Algorithm 1.

figure a
Closed-Form Solution
We extend the soft-regularized formulation (26) to partial shape GPA as:

\begin{aligned} \mathrm {III:}\quad {\left\{ \begin{array}{ll} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{\{{\varvec{W}}_i\},\, {\varvec{S}}}\ &{} \sum _{i=1}^n\, \Vert {\varvec{W}}_i^{\intercal } \varvec{{\mathcal {B}}}_i({\varvec{D}}_i) \varvec{\Gamma }_{i} - {\varvec{S}} \varvec{\Gamma }_{i} \Vert _{F}^2 + \sum _{i=1}^n\, \mu _i \Vert {\varvec{Z}}_i {\varvec{W}}_i \Vert _F^2 + \nu \Vert {\varvec{S}} {\varvec{1}} \Vert _2^2\\ \mathrm {s.t.} \quad &{} {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda } . \end{array}\right. } \end{aligned}
(28)
This formulation includes formulation (26) as a special case, thus is the ultimate form we will implement.

Formulation (28) can be solved by firstly eliminating the transformation parameters, and then solving an optimization problem in {\varvec{S}} by Lemma 1. The key steps are sketched as follow. We define the shorthand \varvec{{\mathcal {B}}}_i {\mathop {=}\limits ^{\text {def}}}\varvec{{\mathcal {B}}}_i({\varvec{D}}_i) . Given {\varvec{S}}, problem (28) becomes linear least-squares in {\varvec{W}}_i whose solution is:

\begin{aligned} {\varvec{W}}_i = \left( \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i \right) ^{-1} \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i {{\varvec{S}}}^{\intercal } . \end{aligned}
(29)
By substituting Eq. (29) into problem (28), we obtain:

\begin{aligned} \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{{\varvec{S}}}\ \mathbf {tr}\left( {\varvec{S}} \left( \varvec{{\mathcal {P}}}_{\mathrm {III}} + \nu {\varvec{1}} {\varvec{1}}^{\intercal } \right) {\varvec{S}}^{\intercal }\right) \qquad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \end{aligned}
(30)
where \varvec{{\mathcal {P}}}_{\mathrm {III}} is defined as:

\begin{aligned} \varvec{{\mathcal {P}}}_{\mathrm {III}} = \sum _{i=1}^{n} \left( \varvec{\Gamma }_i - \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i^{\intercal } \left( \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i \right) ^{-1} \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i \right) .\nonumber \\ \end{aligned}
(31)
Problem (30) can be solved in closed-form by Lemma 1. We summarize the above results as:

Summary 3
Formulation (28) can be solved globally. The optimal reference shape {\varvec{S}}^{\star } is obtained as scaling the d bottom eigenvectors of \left( \varvec{{\mathcal {P}}}_{\mathrm {III}} + \nu {\varvec{1}} {\varvec{1}}^{\intercal } \right) by \sqrt{\varvec{\Lambda }}. The optimal transformation parameters are given by {\varvec{W}}_i^{\star } = \left( \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i \right) ^{-1} \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i {{\varvec{S}}^{\star }}^{\intercal } \left( i \in \left[ 1:n \right] \right) .

Eigenvector Characterization and Tuning Parameters
We rewrite \varvec{{\mathcal {P}}}_{\mathrm {III}} as \varvec{{\mathcal {P}}}_{\mathrm {III}} = \sum _{i=1}^{n} {\varvec{P}}_i , with:

\begin{aligned} {\varvec{P}}_i = \varvec{\Gamma }_i - \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i^{\intercal } \left( \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i \right) ^{-1} \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i . \end{aligned}
Then {\varvec{P}}_i satisfies: {\varvec{I}} \succeq \varvec{\Gamma }_i \succeq {\varvec{P}}_i \succeq {\varvec{O}}. This can be shown by writing {\varvec{P}}_i as {\varvec{P}}_i = \varvec{\Gamma }_i \varvec{\Phi }_i \varvec{\Gamma }_i with \varvec{\Phi }_i = {\varvec{I}} - \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i^{\intercal } \left( \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i^{\intercal } + \mu _i {\varvec{Z}}_i^{\intercal } {\varvec{Z}}_i \right) ^{-1} \varvec{{\mathcal {B}}}_i \varvec{\Gamma }_i where {\varvec{I}} \succeq \varvec{\Phi }_i \succeq {\varvec{O}}. As a summation, \varvec{{\mathcal {P}}}_{\mathrm {III}} satisfies: n {\varvec{I}} \succeq \varvec{{\mathcal {P}}}_{\mathrm {III}} \succeq {\varvec{O}} .

We extend Theorem 2 from full shapes to partial shapes as follow.

Theorem 3
The following statements are equivalent:

(a)
\varvec{{\mathcal {P}}}_{\mathrm {III}} {\varvec{1}} = {\varvec{0}}.

(b)
{\varvec{P}}_i {\varvec{1}} = {\varvec{0}}.

(c)
There exists {\varvec{x}} such that \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i({\varvec{D}}_i)^{\intercal } {\varvec{x}} = \varvec{\Gamma }_i {\varvec{1}}; moreover if \mu _i > 0, {\varvec{x}} must satisfy {\varvec{Z}}_i {\varvec{x}} = {\varvec{0}}.

Proof
The proof is analogous to that for Theorem 2. See Appendix C. \square

By Definition 1, if the LBW contains free-translations, then the LBW satisfies: “there exists {\varvec{x}} such that \varvec{{\mathcal {B}}}_i\left( \cdot \right) ^{\intercal } {\varvec{x}} = {\varvec{1}}; moreover if \mu _i > 0, {\varvec{x}} must satisfy {\varvec{Z}}_i {\varvec{x}} = {\varvec{0}}”. Thus by left-multiplying \varvec{{\mathcal {B}}}_i(\cdot )^{\intercal } with \varvec{\Gamma }_i, the following statement is also true: “there exists {\varvec{x}} such that \varvec{\Gamma }_i \varvec{{\mathcal {B}}}_i(\cdot )^{\intercal } {\varvec{x}} = \varvec{\Gamma }_i {\varvec{1}}; moreover if \mu _i > 0, {\varvec{x}} must satisfy {\varvec{Z}}_i {\varvec{x}} = {\varvec{0}}”. Therefore the property of free-translations in the LBW is sufficient for case (c) in Theorem 3.

Proposition 6
The statements in Theorem 3 are satisfied if the LBW contains free-translations. The affine transformation and the TPS satisfy the statements in Theorem 3.

If the statements in Theorem 3 are satisfied, then {\varvec{1}} is an eigenvector of \left( \varvec{{\mathcal {P}}}_{\mathrm {III}} + \nu {\varvec{1}} {\varvec{1}}^{\intercal } \right) with \left( \varvec{{\mathcal {P}}}_{\mathrm {III}} + \nu {\varvec{1}} {\varvec{1}}^{\intercal } \right) {\varvec{1}} = m \nu {\varvec{1}} . Thus if we choose \nu \ge n/m, the eigenvector {\varvec{1}} is excluded from the solution.

Proposition 7
The tuning parameter \nu can be safely set to any \nu \ge n/m.

The rows of {\varvec{S}}^{\star } are (un-normalized) eigenvectors of \left( \varvec{{\mathcal {P}}}_{\mathrm {III}} + \nu {\varvec{1}} {\varvec{1}}^{\intercal } \right) obtained by excluding the eigenvector {\varvec{1}}. By the orthogonality of eigenvectors, we know {\varvec{S}}^{\star } {\varvec{1}} = {\varvec{0}}. Thus we conclude:

Proposition 8
If the statements in Theorem 3 are satisfied and \nu \ge n/m, the optimal reference shape {\varvec{S}}^{\star } is zero-centered.

Thus if the statements in Theorem 3 are satisfied, solving formulation (28) with the soft constraint is equivalent to solving the one with the hard constraint:

\begin{aligned}&\mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{\{{\varvec{W}}_i\},\, {\varvec{S}}}\ \sum _{i=1}^n\, \Vert {\varvec{W}}_i^{\intercal } \varvec{{\mathcal {B}}}_i\left( {\varvec{D}}_i\right) \varvec{\Gamma }_{i} - {\varvec{S}} \varvec{\Gamma }_{i} \Vert _{F}^2 + \sum _{i=1}^n\, \mu _i \Vert {\varvec{Z}}_i {\varvec{W}}_i \Vert _F^2 \nonumber \\&\quad \mathrm {s.t.} \quad {\varvec{S}} {\varvec{S}}^{\intercal } = \varvec{\Lambda }, \quad {\varvec{S}} {\varvec{1}} = {\varvec{0}} . \end{aligned}
(32)
However, we would always recommend users to solve formulation (28) since it is always solvable in closed-form, and generalizes to LBWs where the statements in Theorem 3 are not satisfied.

The Coordinate Transformation of Datum Shapes
We apply (possibly distinct) rigid transformations ({\varvec{R}}_i,\, {\varvec{t}}_i) to each datum shape {\varvec{D}}_i, and denote the transformed datum shapes as {\varvec{D}}_i' = {\varvec{R}}_i {\varvec{D}}_i + {\varvec{t}}_i \left( i \in \left[ 1:n \right] \right) . We denote {\varvec{Z}}_i' the new regularization matrix in replacement of {\varvec{Z}}_i under transformed datum shapes.

Lemma 6
In formulation (28), if there exists an invertible matrix {\varvec{H}}_i such that \varvec{{\mathcal {B}}}_i\left( {\varvec{D}}_i'\right) = {\varvec{H}}_i \varvec{{\mathcal {B}}}_i({\varvec{D}}_i) and {\varvec{Z}}_i' = {\varvec{Z}}_i {\varvec{H}}_i^{\intercal } for each i \in \left[ 1:n \right] , the matrix \varvec{{\mathcal {P}}}_{\mathrm {III}} remains the same. So does the optimal reference shape {\varvec{S}}.

Proof
The proof is obvious as \varvec{{\mathcal {P}}}_{\mathrm {III}} is invariant to such transformations. \square

Proposition 9
For the TPS warp, if we apply the same rigid transformation ({\varvec{R}}_i,\, {\varvec{t}}_i) to both the datum shape {\varvec{D}}_i and the control points in {\varvec{D}}_i, then \varvec{{\mathcal {B}}}_i({\varvec{D}}_i') = \varvec{{\mathcal {B}}}_i({\varvec{D}}_i) and {\varvec{Z}}_i' = {\varvec{Z}}_i.

Proof
See Appendix A.5. \square

Together with the discussion on the affine case in Sect. 4.4, we have the following conclusion:

Proposition 10
In formulation (28), if the LBW is chosen as the affine transformation or the TSP warp, then the optimal reference shape {\varvec{S}} remains the same when we apply rigid coordinate transformations to the datum shapes ahead.

The above result indicates that we can parameterize the datum shapes in any coordinate frame, while the solution of formulation (28) will give exactly the same optimal reference shape. The optimal transformations of formulation (28) will automatically accommodate the coordinate transformations.

Reflection
The reflection in the computed reference shape {\varvec{S}}^{\star } can be easily coped with by simply flipping the sign of one row in {\varvec{S}}^{\star }. Let {\varvec{S}}^{\star } = \left[ {\varvec{s}}_1, {\varvec{s}}_2, \dots , {\varvec{s}}_d \right] ^{\intercal }. It is easy to verify that {\varvec{S}}^{\star } is still globally optimal if we flip the signs of any {\varvec{s}}_k \left( k \in \left[ 1:d \right] \right) . Assume there are no reflections between the datum shapes. The reflection in {\varvec{S}}^{\star } can be detected by computing an orthogonal Procrustes between {\varvec{S}}^{\star } and any one of {\varvec{D}}_i \left( i \in \left[ 1:n \right] \right) by:

\begin{aligned} \varvec{\hat{R}}, \varvec{\hat{t}} = \mathop {{\mathrm{arg}}\,{\mathrm{min}}}\limits _{{\varvec{R}} \in \mathrm {O}\left( d\right) ,\, {\varvec{t}}}\ \Vert \left( {\varvec{R}} {\varvec{D}}_i + {\varvec{t}} {\varvec{1}}^{\intercal } - {\varvec{S}}^{\star } \right) \varvec{\Gamma }_{i} \Vert _F^2 . \end{aligned}
The optimal {\varvec{t}} is \varvec{\hat{t}} = - \frac{1}{\mathbf {nnz}(\varvec{\Gamma }_{i})} ( \varvec{\hat{R}} {\varvec{D}}_i - {\varvec{S}}^{\star } ) \varvec{\Gamma }_{i} {\varvec{1}}. Denote {\varvec{K}}_i = \varvec{\Gamma }_{i} - \frac{1}{\mathbf {nnz}(\varvec{\Gamma }_{i})} \varvec{\Gamma }_{i}{\varvec{1}} {\varvec{1}}^{\intercal }\varvec{\Gamma }_{i}. Denote {\varvec{E}} = {\varvec{D}}_i {\varvec{K}}_i {\varvec{K}}_i^{\intercal } {{\varvec{S}}^{\star }}^{\intercal } and its SVD as {\varvec{E}} = {\varvec{U}} \varvec{\Sigma } {\varvec{V}}^{\intercal } . Then the optimal {\varvec{R}} is \varvec{\hat{R}} = {\varvec{V}} {\varvec{U}}^{\intercal }.

If \det (\varvec{\hat{R}}) = 1, there is no reflection. If \det (\varvec{\hat{R}}) = -1, we let {\varvec{S}}^{\star } = \left[ -{\varvec{s}}_1, {\varvec{s}}_2, \dots , {\varvec{s}}_d \right] ^{\intercal }. The correctness of such an approach is shown as follow. The determinants satisfy \det \left( {\varvec{E}}\right) = \det \left( {\varvec{U}}\right) \det \left( \varvec{\Sigma } \right) \det \left( {\varvec{V}}^{\intercal }\right) and \det (\varvec{\hat{R}}) = \det \left( {\varvec{U}}\right) \det ({\varvec{V}}^{\intercal }) . Because \det \left( \varvec{\Sigma } \right) > 0, we know that \det \left( {\varvec{E}}\right) and \det (\varvec{\hat{R}}) have the same sign. By flipping the sign of one row in {\varvec{S}}^{\star }, we flip the sign of one column in {\varvec{E}}, which causes the flip of the sign of \det \left( {\varvec{E}}\right) , thus \det (\varvec{\hat{R}}) as well.

Pseudo-code
Our algorithm is rather easy to implement. We term the proposed deformable GPA framework as DefGPA. The overall procedure is given as pseudo-code in Algorithm 2. We release our Matlab implementation of DefGPA to foster future research in this direction.

Code repository: https://bitbucket.org/clermontferrand/deformableprocrustes/src/master/

figure b
Experimental Results
We provide experimental results with respect to various deformable scenarios. While our method adapts to general LBWs, we use the affine transformation and the TPS warp to show the results. A brief introduction of the TPS warp is provided in Appendix A.

Experimental Setups
The datasets used for evaluation are listed in Table 1, while samples are given in Fig. 1. In Table 1, “F” standards for full shapes (without missing datum points), and “P” for partial shapes (with missing datum points). These are public datasets coming from different papers and designed for different problems. These datasets cover the case of structural deformations like facial expressions (Bartoli et al. 2013), deformable objects (Gallardo et al. 2017; Bartoli 2006), and tissue deformations (Bilic et al. 2019). Both 2D and 3D cases are considered. The Liver dataset is a mesh with 4004 corresponding vertices, while the others are 2D/3D images with corresponding landmarks.

Table 1 List of datasets used for evaluation
Full size table
Fig. 1
figure 1
Sample image of each dataset

Full size image
We use Euclidean GPA and Affine GPA with costs defined in the datum-space as benchmark algorithms. These two methods are denoted as *EUC_d and *AFF_d, where _d means the methods minimize the datum-space cost. The notion * indicates they are benchmark methods. For *EUC_d and *AFF_d, we use the MATLAB implementation by (Bartoli et al. 2013) based on closed-form initialization and Levenberg–Marquardt refinement.

Our methods are denoted by AFF_r, TPS_r(3), TPS_r(5), and TPS_r(7) respectively, where _r means the methods minimize the reference-space cost. AFF_r stands for GPA with the affine model, and TPS_r(\cdot ) stands for GPA with the TPS warp. We choose the control points of the TPS warp evenly along each principal axis of the datum shape. We examine the cases of 3, 5 and 7 points along each principal axis, which results in 9, 25, 49 overall control points for 2D datasets and 27, 125, 343 overall control points for the LiTS and Liver datasets. The ToyRug dataset is almost flat, thus we assign two layers of control points along the first two principal axes which yields 18, 50, 98 overall control points.

Our methods are implemented in MATLAB, which constitutes a fair comparison against *EUC_d and *AFF_d. The experiments are carried out by an Intel(R) Core(TM) i7-6700K CPU @ 4.00GHz \times 8 CPU, running Ubuntu 18.04.5 LTS. The MATLAB version is R2020b.

Evaluation Metrics
Landmark Residual
We use RMSE_r to denote the landmark residual defined in the reference-space, and RMSE_d the landmark residual in the datum-space. These two metrics are defined as:

\begin{aligned} \mathrm {RMSE}\_\mathrm {r}= & {} \sqrt{\frac{1}{\kappa } \sum _{i=1}^n\, \left\| \left( {\mathcal {T}}_i\left( {\varvec{D}}_i\right) - {\varvec{S}}^{\star } \right) \varvec{\Gamma }_{i}\right\| _{F}^2}\\ \mathrm {RMSE}\_\mathrm {d}= & {} \sqrt{\frac{1}{\kappa } \sum _{i=1}^n\, \left\| \left( {\varvec{D}}_i - {\mathcal {T}}_i^{-1}\left( {\varvec{S}}^{\star }\right) \right) \varvec{\Gamma }_{i} \right\| _{F}^2}, \end{aligned}
where \kappa = \sum _{i=1}^{n} \mathbf {nnz} \left( \varvec{\Gamma }_i \right) . If the transformation model is invertible, it is easy to derive RMSE_r and RMSE_d from one another. However, this is typically not the case for LBWs, e.g., the TPS warp is not invertible. We propose to use control points and their images as samples to fit an inverse TPS warp. In specific, let {\mathcal {T}}\left( \cdot \right) be a TPS warp, and {\varvec{c}}_i \left( i \in \left[ 1:l \right] \right) be its l control points. Let the images of these l control points under {\mathcal {T}}\left( \cdot \right) be {\varvec{c}}'_i = {\mathcal {T}} \left( {\varvec{c}}_i\right) \left( i \in \left[ 1:l \right] \right) . Then in essence {\mathcal {T}}\left( \cdot \right) is a regression model obtained by fitting the datum pairs \left( {\varvec{c}}_i, {\varvec{c}}'_i\right) \left( i \in \left[ 1:l \right] \right) with {\varvec{c}}_i being the input and {\varvec{c}}'_i being the output. Therefore the inverse of {\mathcal {T}}\left( \cdot \right) , denoted by {\mathcal {T}}^{-1}\left( \cdot \right) , can be defined by fitting the pairs \left( {\varvec{c}}'_i, {\varvec{c}}_i\right) \left( i \in \left[ 1:l \right] \right) , with {\varvec{c}}'_i being the input and {\varvec{c}}_i being the output. For the TPS warp, this is realized by letting {\varvec{c}}'_i be the set of control points of {\mathcal {T}}^{-1}\left( \cdot \right) , and computing the warp parameters using the relation {\mathcal {T}}^{-1}\left( {\varvec{c}}'_i\right) = {\varvec{c}}_i.

Cross-Validation Error
The warp models can overfit the data by using a small enough TPS smoothing parameter. We quantify this behavior by the Cross-Validation Error (CVE) defined as:

\begin{aligned} \mathrm {CVE} = \sqrt{\frac{1}{\kappa } \sum _{i=1}^n\, \left\| \left( \varvec{\hat{S}}_{i} - {\varvec{S}}^{\star } \right) \varvec{\Gamma }_{i} \right\| _{F}^2} , \end{aligned}
where the predicted reference shape \varvec{\hat{S}}_{i} is computed by the G-fold cross-validation as follow. We group all m points indexed as 1,2,\dots ,m as G mutually exclusive subsets as:


Each subset has N points except the subset g_G which contains the remaining left. We index the points g_k in shape {\varvec{D}}_i as {\varvec{D}}_i \left( g_k \right) , and the points in shape \varvec{\hat{S}}_{i} as \varvec{\hat{S}}_{i} \left( g_k \right) . Now for each subset g_k \left( k = \left[ 1:G \right] \right) , we solve the GPA with all the points except those in g_k. This solution is denoted as {\varvec{S}}_{g_k}^{\star } for the reference shape and {\mathcal {T}}_{g_k : i} \left( i \in \left[ 1: n\right] \right) for the transformations. Then for each datum shape, we predict the positions of points in {\varvec{D}}_i \left( g_k \right) by the transformation {\mathcal {T}}_{g_k : i} \left( {\varvec{D}}_i \left( g_k \right) \right) . We correct the gauge of {\mathcal {T}}_{g_k : i} \left( {\varvec{D}}_i \left( g_k \right) \right) with the Euclidean Procrustes between {\varvec{S}}_{g_k}^{\star } and {\varvec{S}}^{\star } (by eliminating points g_k in {\varvec{S}}^{\star }). Denote such a solution to be {\varvec{R}}_{g_k} and {\varvec{t}}_{g_k}. Finally we set \varvec{\hat{S}}_{i} \left( g_k \right) = {\varvec{R}}_{g_k} {\mathcal {T}}_{g_k : i} \left( {\varvec{D}}_i \left( g_k \right) \right) + {\varvec{t}}_{g_k} {\varvec{1}}^{\intercal } . Repeating the above procedure for all G subsets, we obtain the predicted reference shape \varvec{\hat{S}}_{i}.

The CVE resembles the RMSE, except that it handles overfitting. For the Face, Bag, Pillow, LiTS and ToyRug dataset, we set N=1. For the Liver dataset, we set N=40 to cope with the larger dataset size and dimension.

Thin-Plate Spline Smoothing Parameters
Fig. 2
figure 2
The choice of TPS smoothing parameters. The Root-Mean-Squared-Error (RMSE), Rigidity-Score (RS), and Cross-Validation-Error (CVE) are plotted with respect to different \theta values (in horizontal axes) (Color figure online)

Full size image
We set the TPS smoothing parameter \mu _i as \mu _i = \mathbf {nnz} \left( \varvec{\Gamma }_i\right) \theta , \left( i \in \left[ 1:n \right] \right) , and adjust \theta within the range \left[ 1e-5, 1e+5\right] . We report the RMSE_r, RMSE_d, and CVE with respect to different \theta in Fig. 2. The \theta used to generate our results are marked as vertical lines, annotated with the chosen value.

In Fig. 2, the RMSE_r decreases monotonically as \theta decreases, as a smaller \theta implies more flexibility. However, a sufficiently small \theta may cause overfitting. The overfitting in DefGPA is mainly twofold: the bias to the data, and the bias to the optimization criterion (i.e., the cost function). When overfitting happens, the optimization tends to fit the training data to the cost function as much as possible, thus actually creating misfitting to new data and other criteria. This suggests that we can use the fitness to new data and other criteria to detect overfitting. The idea of using new data is realized by cross-validation given as the CVE metric, and that of using other criteria by the datum-space cost (denoted by RMSE_d). In Fig. 2, the overfitting is reflected as the divergence of the RMSE_r and RMSE_d metric, as well as the slope change of the CVE curve. In general, CVE has a positive slope if RMSE_r and RMSE_d agree with one another, and a negative slope if RMSE_r and RMSE_d diverge.

Table 2 The statistics of landmark registration
Full size table
For 2D datasets, we choose \theta = 10 for the Face, and \theta = 100 for the Bag and Pillow datasets. For 3D datasets, we choose \theta = 0.1 for the LiTS, and \theta = 0.01 for the Liver and ToyRug datasets. Although the bending energy term has different constants for 2D and 3D cases, the choice of \theta is roughly stable within each category.

In terms of overfitting, we focus on the agreement of the trend of the RMSE_r and RMSE_d curves: whether they both increase or decrease. There is always a gap between the RMSE_r and RMSE_d metrics, which stands for the asymmetry between different cost functions. We will examine this asymmetry in Sect. 7.6.

Results Based on Landmarks
Statistics
We report in Table 2 the RMSE_d, RMSE_r, CVE and the computational time per GPA problem. The RMSE_d, RMSE_r and CVE are evaluated in pixels, and the time in seconds.

Visualization
For each case, we visualize the optimal reference shape {\varvec{S}}^{\star }, along with the set of predicted reference shapes \varvec{\hat{S}}_{i} computed by the leave-N-out cross-validation. The details of how to compute each \varvec{\hat{S}}_{i} have been given in Sect. 7.2. Recall that \varvec{\hat{S}}_{i} is sensitive to overfitting, thus is better to benchmark fitness than using the transformed datum shapes {\mathcal {T}}_i({\varvec{D}}_i) directly.

For the 2D datasets, we visualize \varvec{\hat{S}}_{i} and {\varvec{S}}^{\star } directly in Fig. 3. The predicted reference shapes \varvec{\hat{S}}_{i} are plotted in blue, and the optimal reference shape {\varvec{S}}^{\star } on top in green. For the 3D datasets, we visualize the points in {\varvec{S}}^{\star }, and encode the CVE with the marker size and color in Fig. 4. The bigger the marker and the redder the color, the larger the associated CVE value.

Fig. 3
figure 3
The fitness of each methods visualized by the cross-validation on 2D datasets. The reference shape {\varvec{S}}^{\star } is plotted in green and the predicted reference shapes \varvec{\hat{S}}_{i} in blue (Color figure online)

Full size image
Fig. 4
figure 4
The fitness of each methods visualized by the cross-validation on 3D datasets. We present the reference shape {\varvec{S}}^{\star } and encode the cross-validation error for each point with both the marker sizes and colors. The smaller the marker, the lower the error (Color figure online)

Full size image
The result in Fig. 3 shows that the landmark fitness is consistently improved by using the affine-GPA (*AFF_d and AFF_r) or the TPS warp based GPA compared with the Euclidean-GPA (*EUC_d). Both *AFF_d and AFF_r give similar results. There is a clear improvement of TPS_r(3) over the affine-GPA. However, the improvement of TPS_r(7) over TPS_r(5) is marginal.

The result in Fig. 4 agrees with the result in Fig. 3, which improves sequentially by using the *EUC_d, AFF_r, TPS_r(3), TPS_r(5) and TPS_r(7) methods. Both the datum-space method *AFF_d and the reference-space method AFF_r produce similar results on the LiTS and Liver datasets. The *AFF_d method gives rather poor fitness on the ToyRug dataset. This is due to the asymmetry in the cost function which be examined in detail in Sect. 7.6.

Results Based on Image Intensities
For the 2D datasets, we visualize the image intensities under the computed warps. The image pixel coordinates can be regarded as testing points (in addition to the landmarks) which are immune to the issue of overfitting. We thus evaluate the fitness using the intensities in the transformed pixel coordinates.

Let I_i be a sample image. For each pixel coordinates [u, v], we apply the transformation {\mathcal {T}}_i to obtain its target position [u',v'] = {\mathcal {T}}_i \left( [u,v] \right) . The transformed image (or warped image) I_i' is defined as I_i' (u',v') = I_i (u,v). The warped image I_i' resides in the coordinate frame of the reference shape, thus can be compared with other warped images. Having computed n warped sample images I'_i \left( i \in \left[ 1:n\right] \right) , we define the image intensity deviation as an image \Delta I'(u,v) = \mathbf {std}\left\{ I'_1(u,v), I'_2(u,v),\dots ,I'_n(u,v)\right\} . In the case of RGB images, we compute the intensity deviation for all the three channels to obtain an RGB intensity deviation image.

We visualize the image intensity deviation for the Bag and Pillow datasets in Fig. 5. The result for *AFF_d is similar to AFF_r, thus is not shown. In Fig. 5, from left to right, the methods *EUC_d, AFF_r, TPS_r(3), TPS_r(5) and TPS_r(7) consistently increase the fitness due to the increasing modeling capabilities (from rigid to affine and deformable models), shown as the shrinking of bright regions.

The image intensity deviation of the Face dataset is noisy due to perspective changes, so we visualize a typical warped sample image (with missing correspondences and perspective changes) and the corresponding transformed landmarks in Fig. 6. This result helps intuitively understand how each transformation model works: the rigid model preserves the distance; the affine model shears the image; the deformation models can deform the image nonlinearly (e.g., the area around the nose and mouth in Fig. 6). The improved fitness using deformation models is obvious without saying.

Fig. 5
figure 5
The intensity deviation of the transformed datum images. From left to right are respectively the intensity deviation for *EUC_d, AFF_r, TPS_r(3), TPS_r(5) and TPS_r(7). The bright regions correspond to large intensity deviations, and the dark regions to small intensity deviations

Full size image
Fig. 6
figure 6
The transformed images using the computed warps. From left to right are respectively the warped images by *EUC_d, *AFF_d, AFF_r, TPS_r(3), TPS_r(5) and TPS_r(7). The transformed landmarks are marked in blue, and the landmarks in the reference shape are marked in green (Color figure online)

Full size image
The Asymmetry in Cost Functions
For the ToyRug dataset, the RMSE_r and RMSE_d statistics in Table 2 have shown a strong asymmetry between the reference-space cost and datum-space cost, in particular for the *AFF_d and AFF_r methods. This asymmetry is further evidenced in Fig. 4 as the *AFF_d method yields one magnitude larger CVE error than the AFF_r method. Recall that the datum-space cost aims to find the generative model that best explains all the data; in contrast, the reference-space cost aims to find the reference shape that produces the best fitness to the data. The above results suggest that while being closely related, minimizing the datum-space cost does not necessarily result in a good fitness to the reference shape. Now we report on the other hand that minimizing the reference-space does not necessarily produce a good generative model.

Fig. 7
figure 7
The discrepancy between the generated image features and original image features on the ToyRug dataset. The generated image features are plotted in the green color, and the original image features in the red color. The blue x-markers are reprojected 2D image features from the triangulated 3D datum shapes. The RMSE statistics are measured in pixels (Color figure online)

Full size image
The 3D datum shapes of the ToyRug dataset are obtained by triangulating the image features of a stereo camera rig with known intrinsic and extrinsic parameters. To evaluate GPA methods as generative models, we use the 3D reference shape estimate and transformation estimates to generate 3D datum shapes, and reproject the generated 3D datum shapes to 2D image features. We report the discrepancies between the generated image features and the original image futures in Fig. 7, by visualizing the result of the datum shape that produces the largest error.

Contrary to the worst reference-space fitness, the *AFF_d method generates the best image features that closely match the original ones as shown in Fig. 7, because it minimizes the datum-space cost directly while being more flexible than the *EUC_d method. Due to nonrigid deformations, the datum-space method *EUC_d does not provide good result. In parallel, because of the metric asymmetry, poor performance is observed for the AFF_r method which minimizes the reference-space cost. However, although not optimizing the datum-space cost directly, the reference-space methods TPS_r(3), TPS_r(5) and TPS_r(7) are still capable to provide satisfactory results. The quality of the generated features are consistently improved over the TPS_r(3), TPS_r(5) and TPS_r(7) methods. In particular, the features in the bottom right corner generated by the method TPS_r(7) are better than those generated by the *AFF_d method.

This brings us to the final conclusion. The datum-space cost and the reference-cost are two distinct but closely related metrics, while a good result based on one cost does not necessarily guarantee a good result on the other cost due to the metric asymmetry. However it does not mean that the cost functions make a big difference for every dataset: as can be seen in Table 2, the RMSE_r and RMSE_d metrics agree most of the time. In adverse cases like the ToyRug dataset, the deformation models like the TPS warp can achieve the best result on both sides, which coins the importance of using DefGPA over classical GPA methods.

Conclusion
To summarize, we have introduced the problem of GPA with deformations. We have proposed a general problem statement applicable to LBWs, subsuming previous statements made for specific models. We have derived a closed-form solution, applicable to the general case of partial shapes and with regularization. We have extensively validated the proposed solution on various datasets with great care taken to select the regularization weights. Our future work will include studying the consistency of estimates under certain statistical models, and extending our theory to the Nonrigid Structure-from-Motion (NRSfM) problem.