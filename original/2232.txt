Person images captured by public surveillance cameras often have low resolutions (LRs), along with uncontrolled pose variations, background clutter and occlusion. These issues cause the resolution mismatch problem when matched with high-resolution (HR) gallery images (typically available during collection), harming the person reidentification (reidentification) performance. While a number of methods have been introduced based on the joint learning of super-resolution and person reidentification, they ignore specific discriminant identity information encoded in LR person images, leading to ineffective model performance. In this work, we propose a novel joint bilateral-resolution identity modeling method that concurrently performs HR-specific identity feature learning with super-resolution, LR-specific identity feature learning, and person reidentification optimization. We also introduce an adaptive ensemble algorithm for handling different low resolutions. Extensive evaluations validate the advantages of our method over related state-of-the-art reidentification and super-resolution methods on cross-resolution reidentification benchmarks. An important discovery is that leveraging LR-specific identity information enables a simple cascade of super-resolution and person reidentification learning to achieve state-of-the-art performance, without elaborate model design nor bells and whistles, which has not been investigated before.

Access provided by University of Auckland Library

Introduction
Person reidentification (reidentification) matches identity classes in person bounding box images extracted from nonoverlapping camera views in open surveillance spaces (Gong et al. 2014). Existing reidentification methods typically focus on addressing variations in illumination, occlusion, and background clutter by designing feature representations (Liao et al. 2015; Matsukawa et al. 2016; Wu et al. 2016; Qian et al. 2017; Kalayeh et al. 2018; Sun et al. 2018; Guo et al. 2019; Zhou et al. 2019) or learning matching distance metrics (Zheng et al. 2013; Wang et al. 2014; He et al. 2016b; Zhang et al. 2016; Fan et al. 2018; Zheng et al. 2018; Yu et al. 2018) or their combination (Li et al. 2014; Ahmed et al. 2015; Xiao et al. 2016; Li et al. 2017; Zheng et al. 2019b; Dai et al. 2019; Zheng et al. 2019a). The designed reidentification models of these works often do not take the impact of low resolution images into account. However, surveillance person images often have varying resolutions due to variations in the distance from the camera to the person and the camera deployment settings (Fig. 1), which gives rise to the resolution mismatch problem (Jiao et al. 2018; Cheng et al. 2020) (Fig. 2).

Fig. 1
figure 1
Illustration of person images with varying resolutions for the open-space person reidentification (reidentification) task. Three images of a person were captured by two camera views at different locations in a shopping center. The image captured by camera B has a higher resolution than the two images from camera A. This cross-resolution property makes person reidentification more challenging.

Full size image
It is challenging to reliably match low-resolution (LR) probe images against high resolution (HR) gallery images across both camera views and resolutionsFootnote1. This requires addressing the discrepancy in the amount of information in cross-resolution matching since LR images contain much less information than HR images with discriminative appearance details largely lost in the image acquisition process. It is called cross-resolution person reidentification.

Fig. 2
figure 2
Four different strategies for cross-resolution person reidentification (reidentification): (a) Super-resolving LR images to extract the HR-specific identity features, (b) downsampling HR images to extract the LR-specific identity features, (c) using multi-resolution images to extract joint bilateral resolution features for reidentification matching, and (d) aligning image features across resolution to learn resolution-invariant representations. Our approach is a multi-resolution solution exploiting both high-resolution specific and low-resolution specific features, considering the strategies of (a), (b) and (c) jointly. It differs from existing works that belong to the category of (a) (Wang et al. 2018; Cheng et al. 2020; Jiao et al. 2018) or (d) (Huang et al. 2020; Li et al. 2019a; Mao et al. 2019)

Full size image
To address the nontrivial resolution mismatch problem in cross-resolution person reidentification, a number of early methods have been proposed (Jing et al. 2015; Wang et al. 2016; Li et al. 2015). These methods, however, share a few common weaknesses: (1) Instead of recovering the missing discriminative appearance information, they perform cross-resolution representation transformation in a predefined feature space. However, these works do not solve the discrepancy in the amount of information challenge, i.e.,   as potential discriminant information from HR images may not be effectively used when matching between low-resolution and high-resolution images directly. (2) Some of them (Jing et al. 2015; Wang et al. 2016; Li et al. 2015) rely on handcrafted visual features without using deep learning for mining the complementary advantages of feature learning and matching metric joint optimization.

Another intuitive to solve the resolution mismatch problem in cross-resolution person reidentification is to super-resolve the LR images directly so that super-resolved (SR) images can serve as a bridge to realistic HR images for identification. Image super-resolution should offer an effective solution to mitigate the discrepancy in the amount of information challenge due to its capability of synthesizing high-frequency details. However, a direct combination of super-resolution and reidentification may be suboptimal in compatibility: Generic-purpose super-resolution methods are designed to improve the image visual fidelity rather than the reidentification matching performance, with visual artifacts generated in the super-resolution reconstruction process typically irrelevant and problematic to reidentification matching.

Recently, several works, including our preliminary work (Jiao et al. 2018), have shown that joint learning of the image super-resolution and person reidentification is a simple yet effective method for cross-resolution person reidentification (Cheng et al. 2020; Chen et al. 2017c; Huang et al. 2020; Li et al. 2019a). However, these methods ignore the exploitation of LR discriminant information for person reidentification and do not attempt to formulate a joint learning framework for exploring the discriminant reidentification features in both HR and LR images.

We argue that either HR-specific or LR-specific identity features alone are not sufficient for cross-resolution person reidentification. This assertion is inspired by human visual systems, which take advantage of multiscale visual information, including feature representations at both small (global contextual) and large (local saliency) scales (Navon 1977). Therefore, we develop a joint bilateral identity modeling (JBIM) framework. Specifically, JBIM combines HR-specific identity modeling (HIM) and LR-specific identity modeling (LIM): HIM aims to improve the integration compatibility between image super-resolution and person reidentification by learning identity-sensitive high-frequency appearance information, and LIM is for learning the complementary LR-specific identity information. The   framework therefore learns discriminant bilateral-resolution joint features from SR and LR images, along with person reidentification on the joint features. In the presence of different LRs, we further present a multi-resolution adaptive ensemble mechanism by aggregating a set of anchor JBIM network models (each optimized for a reference resolution) in a probe-specific manner. As shown in Fig. 2, our approach is a multi-resolution solution exploiting both high-resolution specific and low-resolution specific features, jointly considering the strategies of (a), (b) and (c) in Fig. 2. It differs from existing works that belong to the category of (a) (Wang et al. 2018; Cheng et al. 2020; Jiao et al. 2018) or (d) (Huang et al. 2020; Li et al. 2019a; Mao et al. 2019).

We have conducted extensive evaluations to verify the superiority of our JBIM approach over related state-of-the-art reidentification and image super-resolution methods on five person reidentification benchmarks: the Context Aware Vision Using Image-based Active Recognition (CAVIAR) dataset (Cheng et al. 2011), the third Chinese University of Hong Kong (CUHK03) dataset (Li et al. 2014), the Sun Yat-sen University (SYSU) dataset (Chen et al. 2017a), the Viewpoint Invariant Pedestrian Recognition (VIPeR) dataset (Gray and Tao 2008) and the Market-1501 dataset (Zheng et al. 2015). An interesting and significant finding is that with the assistance of LIM, cascading super-resolution and person reidentification directly suffices to achieve satisfactory performance without elaborate model design and parameter tuning, which reduces the burden of image super-resolution, enabling our method to effectively and flexibly integrate with different existing super-resolution models.

Fig. 3
figure 3
(a) Images with different underlying resolutions and (b) these normalized to the same spatial size without changing the underlying resolution

Full size image
Related Work
Person reidentification has attracted extensive research over the past 10 years (Gray and Tao 2008; Zheng et al. 2013; Liao et al. 2015; Ahmed et al. 2015; Zheng et al. 2015; Xiao et al. 2016; Zheng et al. 2016; Zhang et al. 2016; Ristani et al. 2016; Li et al. 2017; Chen et al. 2017b). The dominant focus is on handling the reidentification challenges arising from uncontrolled variations in illumination, background clutter and human pose. cross-resolution reidentification has also become a research hotspot (Li et al. 2015; Jing et al. 2015; Wang et al. 2016, 2018; Mao et al. 2019; Chen et al. 2017c; Li et al. 2019a; Cheng et al. 2020; Huang et al. 2020). In the literature, existing cross-resolution reidentification methods can be categorized into two groups: (1) methods for learning resolution-invariant features and (2) methods for learning joint models for both image super-resolution and person reidentification.

When learning resolution-invariant features, it is assumed in (Li et al. 2015) that images of the same person should be distributed similarly under different resolutions, and a method of simultaneously optimizing cross-resolution image alignment and distance metric modeling was designed. In (Jing et al. 2015), a semi-coupled low-rank dictionary learning approach was proposed to uncover the feature relationship between LR and HR images. In (Wang et al. 2016), the characteristics of the scale-distance function space are explored by varying the scale of LR images when matching with HR images. A common limitation of these methods is the inability to synthesize high-frequency and the loss of discriminative appearance information during image acquisition.

Table 1 Comparing state-of-the-art cross-resolution reidentification methods. â€œUsing GANâ€ means that the method applies GAN in the joint model. â€œLearning HR-specific Discriminant Informationâ€ implies that the method uses HR-specific representations for person identity classification. In contrast, â€œLearning LR-specific Discriminant Informationâ€ signifies that the method uses LR-specific representations for person identity classification
Full size table
Recently, several convolution neural network (CNN) -based methods have been proposed for learning joint models. In (Wang et al. 2018), a cascaded multiple generative adversarial network (GAN) for image super-resolution (SRGAN) was proposed to recover the details of LR images progressively. In (Mao et al. 2019), the research focused on person foreground and learning different feature extractors for HR and LR images. In (Chen et al. 2017c), a model with GAN and autoencoder was used to learn cross-resolution deep image representations for reidentification. In (Li et al. 2019a), the authors aim to learn resolution-invariant representations and meanwhile ensure recovering reidentification-oriented HR details. In (Cheng et al. 2020), an association between the features for reidentification and the features for resolution discrimination was introduced as joint learning regularization for cross-resolution person image matching. In (Huang et al. 2020), a degradation invariance learning framework was developed for extracting identity-related robust features.

In summary, most existing cross-resolution reidentification methods adopt the strategy of learning invariant cross-resolution features, regardless of whether image super-resolution is integrated. This approach nonetheless ignores the usefulness of LR image specific information, i.e., the low-resolution specific discriminant features are not explored.

As we found in experiments (see Table 3), this strategy is rather limited in learning discriminating identity features. The proposed learning method solves this limitation by introducing an LR-specific feature learning component. Consequently, the HR-specific and LR-specific identity features are jointly learned so that they can be made highly complementary to improve the cross-resolution reidentification results more effectively than with other methods.

In our preliminary work (Jiao et al. 2018), for the first time, we introduced the idea of jointly learning image super-resolution and person reidentification. Since then, our method has been frequently adopted in a number of followup works (Cheng et al. 2020; Chen et al. 2017c; Huang et al. 2020; Li et al. 2019a; Mao et al. 2019) that introduce various variations to continuously verify this idea and improve reidentification performance. In this work, we revisit the same joint learning idea for HR-specific identity feature learning on top of our early model and make a couple of novel contributions: (1) By exploiting LR-specific identity feature, which is largely ignored by all previous studies, we find that taking use of low-resolution specific identity feature could be a more effective and significant way as compared to pursuing elaborate model design (e.g., using GAN-based design); and (2) with the assistance of LIM, we empirically find that our joint learning method can be further improved significantly in terms of not only the generalizability but also the design flexibility and robustness. For instance, the choice of super-resolution models is less performance-sensitive, hence superior yet hard-to-train super-resolution methods can be bypassed, e.g., GAN models (see Table 1).

Fig. 4
figure 4
An overview of the proposed joint bilateral resolution identity modeling (JBIM) framework. The JBIM framework consists of three components: (A) a high-resolution (HR) specific identity modeling (HIM) network, (B) a low-resolution (LR) specific identity modeling (LIM) network, and (C) a feature fusion network (FFN). Specifically, the HIM network contains two modules: (d) a super-resolution module and (e and f) a person reidentification (reidentification) module. In training the HIM network, we deploy three streams taking (a) LR images, (b) synthetic LR images, and (c) HR images as input. The middle stream (b) acts as a bridge for joining (d) the image super-resolution and (e and f) person reidentification learning tasks. Besides, the LIM network takes as input (a) the LR images and (b) synthetic LR images, with (m and n) person reidentification as the learning objective. The FFN takes as input (g) and (i) HR reidentification features and (j) and (k) LR reidentification features, and outputs two feature vectors: a fusion of (g) and (k), and a fusion of (i) and (j). At the test time, we use both HIM and LIM. With HIM, we apply the super-resolution module to resolve the LR probe images and use the reidentification module to extract features of both the SR probe images and HR gallery images. With LIM, we employ the reidentification module to extract features from both the LR probe images and the downsampled gallery images. To obtain the final fused features, we separately concatenate the features from the LR probe images and those of the corresponding SR images and the features of the HR gallery images and those of the corresponding downsampled images. Finally, we utilize the ğ¿2 distance of the fused features to perform cross-resolution reidentification matching

Full size image
In addition to works involving image super-resolution in person reidentification, LR face recognition methods are also relevant and have been advanced in the literature (Wang and Tang 2005; Hennings-Yeomans et al. 2008; Huang and He 2011; Cheng et al. 2018). Their underlying idea is to synthesize HR faces by image super-resolution techniques without the need for dense feature point alignment. While feasible for structure-constrained face images, it is difficult to align person images due to greater degrees of unknown variations in body parts, e.g.,   aligning a back-view LR person image with a side-view HR person image against other clutter. These super-resolution-based LR face-matching methods are therefore not suitable for cross-resolution reidentification (Li et al. 2015). In the meantime, generic-purpose super-resolution methods have achieved remarkable success in synthesizing missing appearance fidelity from LR input images, mainly due to the powerful modeling capacity of deep learning algorithms (Dong et al. 2014, 2016b; Kim et al. 2016a, b; Lai et al. 2017; Tai et al. 2017; Ledig et al. 2017; Lim et al. 2017; Haris et al. 2018; Zhang et al. 2018; Li et al. 2019b). They may generate HR person images with higher visual quality but remain ineffective for cross-resolution reidentification, as validated by our evaluations, because they are designed for improving low-level pixel values but not high-level identity discrimination when learning to reconstruct HR images.

Approach
We need to match an LR probe person image with a set of HR gallery images. To that end, we propose a bilateral-resolution learning approach (Fig. 4). We aim to not only acquire super-resolution person images discriminative for reidentification, but also to explore identity-sensitive LR specific features for a more effective solution which is not explored before. To maximize the model performance, we propose to jointly learn HR-specific and LR-specific identity features with optimal compatibility.

Suppose ğ‘‹ğ‘™={(ğ‘¥ğ‘¥ğ‘™ğ‘–,ğ‘¦ğ‘™ğ‘–)}ğ‘ğ‘™ğ‘–=1 is an LR person image set with ğ‘ğ‘™ images from one camera view, ğ‘‹â„={(ğ‘¥ğ‘¥â„ğ‘–,ğ‘¦â„ğ‘–)}ğ‘â„ğ‘–=1 is an HR image set with ğ‘â„ images from another view, where ğ‘¥ğ‘¥ğ‘™ğ‘– and ğ‘¥ğ‘¥â„ğ‘– denote LR and HR images captured with different camera views of identity classes ğ‘¦ğ‘™ğ‘– and ğ‘¦â„ğ‘–, respectively. To extract LR-specific identity features from HR images, we generate a synthetic LR set ğ‘‹â„2ğ‘™={(ğ‘¥ğ‘¥â„2ğ‘™ğ‘–,ğ‘¦â„ğ‘–)}ğ‘â„ğ‘–=1 of ğ‘‹â„ by downsampling, where ğ‘¥ğ‘¥â„2ğ‘™ğ‘– is a synthetic LR image w.r.t.  an HR image ğ‘¥ğ‘¥â„ğ‘–.

To learn the discriminative joint bilateral-resolution features, we want to obtain the following key components: (1) an image super-resolution function îˆ²ğ‘ ğ‘Ÿ(â‹…) that can effectively compensate for reidentification information in the LR images ğ‘¥ğ‘¥ğ‘™ğ‘–; (2) an HR-specific identity discriminant feature extraction function îˆ²â„ğ‘“ğ‘’(â‹…) for both super-resolved LR images ğ‘¥ğ‘¥ğ‘ ğ‘Ÿğ‘– and realistic HR images ğ‘¥ğ‘¥â„ğ‘—, where ğ‘¥ğ‘¥ğ‘ ğ‘Ÿğ‘–=îˆ²ğ‘ ğ‘Ÿ(ğ‘¥ğ‘¥ğ‘™ğ‘–), with the objective that îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘ ğ‘Ÿğ‘–) is close to îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥â„ğ‘—) in the feature space when they share the identity label (i.e., ğ‘¦ğ‘™ğ‘–=ğ‘¦â„ğ‘—), and vice versa; (3) an LR-specific identity discriminant feature extraction function îˆ²ğ‘™ğ‘“ğ‘’(â‹…) that can extract the LR-specific identity features îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘™ğ‘–) and îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥â„2ğ‘™ğ‘—) from the realistic LR images ğ‘¥ğ‘¥ğ‘™ğ‘– and the synthetic LR images ğ‘¥ğ‘¥â„2ğ‘™ğ‘—, respectively, and when ğ‘¦ğ‘™ğ‘–=ğ‘¦â„ğ‘—, the corresponding LR-specific identity features should be similar, and vice versa; and (4) a feature fusion function îˆ²ğ‘“ğ‘¢ğ‘ (â‹…) that can fuse HR-specific and LR-specific identity features to obtain more discriminative joint bilateral resolution features. Similarly, we require the fusion features îˆ²ğ‘“ğ‘¢ğ‘ (îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘ ğ‘Ÿğ‘–),îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘™ğ‘–)) and îˆ²ğ‘“ğ‘¢ğ‘ (îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥â„ğ‘—),îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥â„2ğ‘™ğ‘—)) to be reidentification discriminant.

Formally, by learning îˆ²ğ‘ ğ‘Ÿ(â‹…), îˆ²â„ğ‘“ğ‘’(â‹…), îˆ²ğ‘™ğ‘“ğ‘’(â‹…) and îˆ²ğ‘“ğ‘¢ğ‘ (â‹…) through some joint formulation, we aim to obtain a reidentification similarity matching metric:

îˆ¿(îˆ²ğ‘“ğ‘¢ğ‘ (îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘ ğ‘Ÿğ‘–),îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘™ğ‘–)),îˆ²ğ‘“ğ‘¢ğ‘ (îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥â„ğ‘—),îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥â„2ğ‘™ğ‘—))),
(1)
subject to that after joint learning of image super-resolution, HR-specific and LR-specific identity feature extraction, and feature fusion, an LR image captured in one camera view can be associated correctly with an HR image of the same person captured in another camera view.

In the following, we first describe the HIM component for deriving discriminant HR-specific identity features by joint learning of super-resolution and identity classification. Then, we expound on the LIM component for learning discriminative LR-specific identity features and combine the HIM and LIM results in the Feature Fusion Networks (FFN) to acquire more discriminative reidentification representations for joint bilateral resolution identity learning. An overview of the JBIM framework is shown in Fig. 4.

High-Resolution Specific Identity Modeling: Super-Resolution and Identity Joint Learning
Since generic-purpose super-resolution methods are designed to improve image visual fidelity rather than the reidentification matching performance, a direct combination of independently trained image super-resolution and person reidentification for   HR-specific identity features might be suboptimal for reidentification. Our HIM framework is hence formulated as joint learning of image super-resolution and person identity classification to correlate the two learning tasks as well as maximize their compatibility and complementary advantages.

- Super-Resolving Image. We first compensate for the desired discriminative information missing in the LR images through super-resolution. To facilitate super-resolution model training, we construct the image super-resolution loss with the help of the synthetic LR version ğ‘‹â„2ğ‘™ downsampled by ğ‘‹â„. Specifically, ğ‘‹â„2ğ‘™ allows optimizing the mean squared error (MSE), which measures the image super-resolution quality:

îˆ¸ğ‘ ğ‘Ÿ({ğ‘¥ğ‘¥â„ğ‘–}ğ‘â„ğ‘–=1)=1ğ‘â„âˆ‘ğ‘–=1ğ‘â„â€–îˆ²ğ‘ ğ‘Ÿ(ğ‘¥ğ‘¥â„2ğ‘™ğ‘–)âˆ’ğ‘¥ğ‘¥â„ğ‘–â€–2ğ¹.
(2)
Minimizing loss îˆ¸ğ‘ ğ‘Ÿ enforces the super-resolved image îˆ²ğ‘ ğ‘Ÿ(ğ‘¥ğ‘¥â„2ğ‘™ğ‘–) of ğ‘¥ğ‘¥â„2ğ‘™ğ‘– to be similar to the ground-truth HR image ğ‘¥ğ‘¥â„ğ‘–. HR appearance information is critical for obtaining reliable reidentification features (Li et al. 2015). This optimization scheme (Eq. (2)) establishes the underlying relationship between the LR and HR images in the image pixel space, but without guaranteeing that the synthetic HR images are suitable for computing features discriminant for reidentification matching. The reasons are as follows.

(1) It is very challenging to train a perfect image super-resolution model given that it is a highly nonconvex and difficult-to-optimize problem with extremely complex correlations among the local and global pixels (Dong et al. 2016a).

(2) Artifacts are likely generated particularly for low-quality surveillance person images, which may negatively affect the subsequent reidentification matching results.

- Quantifying Identification. To address the above limitation, we propose enforcing an identity constraint to guide the super-resolution optimization behavior toward an image enhancement solution optimal for identity discrimination. This design differs from a typical super-resolution objective that seeks pixel-level mapping from LR input images to HR ground-truth images without a semantic top-down learning constraint.

Specifically, we concurrently optimize the classification of discriminative features w.r.t.  the same person label in the HR and synthetic LR images, along with the cross-view LR images. Formally, we formulate the reidentification classification constraint in the context of different images as:

îˆ¸ğ‘Ÿğ‘’ğ‘–ğ‘‘({(ğ‘¥ğ‘¥ğ‘™ğ‘–,ğ‘¦ğ‘™ğ‘–,ğ‘¥ğ‘¥â„ğ‘–,ğ‘¦â„ğ‘–)}ğ‘ğ‘–=1)=1ğ‘âˆ‘ğ‘–=1ğ‘(îˆ¸ğ‘ğ‘™ğ‘ (îˆ²â„ğ‘(ğ‘“ğ‘“â„â„â„ğ‘–),ğ‘¦â„ğ‘–)+îˆ¸ğ‘ğ‘™ğ‘ (îˆ²â„ğ‘(ğ‘“ğ‘“â„2ğ‘™â„â„ğ‘–),ğ‘¦â„ğ‘–)+îˆ¸ğ‘ğ‘™ğ‘ (îˆ²â„ğ‘(ğ‘“ğ‘“ğ‘™â„â„ğ‘–),ğ‘¦ğ‘™ğ‘–)),
(3)
where (ğ‘¥ğ‘¥ğ‘™ğ‘–,ğ‘¦ğ‘™ğ‘–,ğ‘¥ğ‘¥â„ğ‘–,ğ‘¦â„ğ‘–) consists of an LR image from ğ‘‹ğ‘™ and an HR image from ğ‘‹â„ as well as their corresponding identity labels. We can construct N groups from ğ‘‹ğ‘™ and ğ‘‹â„. All ğ‘“â„ğ‘“â„ notations denote the HR-specific reidentification feature vectors obtained from the following feature extraction function:

ğ‘“ğ‘“â„â„â„ğ‘–=îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥â„ğ‘–), ğ‘“ğ‘“â„2ğ‘™â„â„ğ‘–=îˆ²â„ğ‘“ğ‘’(îˆ²ğ‘ ğ‘Ÿ(ğ‘¥ğ‘¥â„2ğ‘™ğ‘–)), ğ‘“ğ‘“ğ‘™â„â„ğ‘–=îˆ²â„ğ‘“ğ‘’(îˆ²ğ‘ ğ‘Ÿ(ğ‘¥ğ‘¥ğ‘™ğ‘–)).
(4)
And îˆ²â„ğ‘(â‹…) represents an HR-specific classification function. îˆ¸ğ‘ğ‘™ğ‘ (â‹…) is the integration of the identity loss îˆ¸ğ‘–ğ‘‘(â‹…) and the triplet loss îˆ¸ğ‘¡ğ‘Ÿğ‘–(â‹…), which is defined as:

îˆ¸ğ‘ğ‘™ğ‘ =îˆ¸ğ‘–ğ‘‘+îˆ¸ğ‘¡ğ‘Ÿğ‘–.
(5)
- Simultaneous Super-Resolution Learning and reidentification.

After combining the super-resolution and reidentification formulation designs as above, we formulate the overall HR-specific reidentification loss function as:

îˆ¸ğ»ğ¼ğ‘€({(ğ‘¥ğ‘¥ğ‘™ğ‘–,ğ‘¦ğ‘™ğ‘–,ğ‘¥ğ‘¥â„ğ‘–,ğ‘¦â„ğ‘–)}ğ‘ğ‘–=1)=îˆ¸ğ‘Ÿğ‘’ğ‘–ğ‘‘({(ğ‘¥ğ‘¥ğ‘™ğ‘–,ğ‘¦ğ‘™ğ‘–,ğ‘¥ğ‘¥â„ğ‘–,ğ‘¦â„ğ‘–)}ğ‘ğ‘–=1)+ğ›¼îˆ¸ğ‘ ğ‘Ÿ({ğ‘¥ğ‘¥â„ğ‘–}ğ‘ğ‘–=1),
(6)
where the parameter ğ›¼ controls the balance between the image super-resolution loss and the reidentification loss. Optimizing the joint loss îˆ¸ğ»ğ¼ğ‘€ allows guiding the îˆ²ğ‘ ğ‘Ÿ(â‹…) to compensate the semantic appearance details of the LR images toward identity-salient fidelity synthesis and concurrently drives   îˆ²â„ğ‘“ğ‘’(â‹…) to accordingly extract identity discriminative features in a harmonious manner. Such a multitask joint learning formulation is supposed to mitigate the resolution mismatch problem in cross-resolution person reidentification.

A key characteristic of the HIM formulation (Eq. (6)) is the seamless joining of a restoration quantization super-resolution loss (Eq. (2)) and a person reidentification loss (Eq. (3)), both subject to the same synthetic LR training images ğ‘¥ğ‘¥â„2ğ‘™ğ‘– (Fig. 4b) in the context of concurrent identity discriminant supervision on all three types of training images. That is, the synthetic LR images ğ‘¥ğ‘¥â„2ğ‘™ğ‘– and its reidentification features ğ‘“ğ‘“â„2ğ‘™ğ‘– together bridge and correlate the image super-resolution (Fig. 4d) and person reidentification (Fig. 4 e and f) learning tasks. Without this connection, the two loss functions îˆ¸ğ‘ ğ‘Ÿ and îˆ¸ğ‘Ÿğ‘’ğ‘–ğ‘‘ will be optimized independently, rather than jointly and concurrently.

Learning Bilateral-Resolution Identity Features with Low-Resolution Identity Modeling
Although the information of LR images may be incomplete, a fraction of it may be absent in the HR-specific representations but may be useful for reidentification, i.e., LR-specific reidentification information. Under this consideration, we propose further learning discriminative features in the LR image space, in a similar manner as HR-specific reidentification formulation above. We ground this learning component on every pair of a synthetic LR image (i.e., ğ‘¥ğ‘¥â„2ğ‘™ğ‘– downsampled from HR image ğ‘¥ğ‘¥â„ğ‘–) and a cross-view realistic LR image (i.e., ğ‘¥ğ‘¥ğ‘™ğ‘–).

Formally, we introduce an LR reidentification loss function as:

îˆ¸ğ¿ğ¼ğ‘€({(ğ‘¥ğ‘¥ğ‘™ğ‘–,ğ‘¦ğ‘™ğ‘–,ğ‘¥ğ‘¥â„ğ‘–,ğ‘¦â„ğ‘–)}ğ‘ğ‘–=1)=1ğ‘âˆ‘ğ‘–=1ğ‘(îˆ¸ğ‘ğ‘™ğ‘ (îˆ²ğ‘™ğ‘(ğ‘“ğ‘“ğ‘™ğ‘™ğ‘™ğ‘–),ğ‘¦ğ‘™ğ‘–)+îˆ¸ğ‘ğ‘™ğ‘ (îˆ²ğ‘™ğ‘(ğ‘“ğ‘“â„2ğ‘™ğ‘™ğ‘™ğ‘–),ğ‘¦â„ğ‘–)),
(7)
where îˆ²ğ‘™ğ‘(â‹…) represents the LR-specific identity classification. Both ğ‘“ğ‘™ğ‘“ğ‘™ notations denote LR-specific reidentification feature vectors by the LR-specific feature extraction function as:

ğ‘“ğ‘“ğ‘™ğ‘™ğ‘™ğ‘–=îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘™ğ‘–),ğ‘“ğ‘“â„2ğ‘™ğ‘™ğ‘™ğ‘–=îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥â„2ğ‘™ğ‘–).
(8)
By minimizing the classification loss îˆ¸ğ¿ğ¼ğ‘€, we encourage îˆ²ğ‘™ğ‘“ğ‘’(â‹…) to learn discriminative LR-specific identity features from realistic and synthetic LR images.

- JBIM: Joint Bilateral-Resolution Identity Modeling.

Collaboratively fusing HR-specific identity learning and   LR-specific identity learning leads to the proposed JBIM framework. This framework aims to derive more discriminate feature representations from the LRs and HRs together.

Specifically, with the image super-resolution function îˆ²ğ‘ ğ‘Ÿ(â‹…) and the HR-specific feature extraction function îˆ²â„ğ‘“ğ‘’(â‹…), we can obtain the HR-specific identity feature îˆ²â„ğ‘“ğ‘’(îˆ²ğ‘ ğ‘Ÿ(ğ‘¥ğ‘¥ğ‘™ğ‘–)) of an LR image ğ‘¥ğ‘¥ğ‘™ğ‘– and the HR-specific identity feature   îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥â„ğ‘–) of an HR image ğ‘¥ğ‘¥â„ğ‘–. Moreover, the LR-specific identity feature îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘™ğ‘–) and îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥â„2ğ‘™ğ‘–) can be extracted by the discriminative LR-specific feature extraction function îˆ²ğ‘™ğ‘“ğ‘’(â‹…). We conjecture that HR representations and LR representations capture different characteristics for reidentification, i.e.,   they are complementary. To maximize this complementary effect, we optimize the fusing of the features by minimizing the following reidentification classification loss as:

îˆ¸ğ‘“ğ‘¢ğ‘ ({(ğ‘¥ğ‘¥ğ‘™ğ‘–,ğ‘¦ğ‘™ğ‘–,ğ‘¥ğ‘¥â„ğ‘–,ğ‘¦â„ğ‘–)}ğ‘ğ‘–=1)=1ğ‘âˆ‘ğ‘–=1ğ‘(îˆ¸ğ‘ğ‘™ğ‘ (îˆ²ğ‘“ğ‘(ğ‘“ğ‘“ğ‘™ğ‘“ğ‘“ğ‘–),ğ‘¦ğ‘™ğ‘–)+îˆ¸ğ‘ğ‘™ğ‘ (îˆ²ğ‘“ğ‘(ğ‘“ğ‘“â„ğ‘“ğ‘“ğ‘–),ğ‘¦â„ğ‘–)),
(9)
where îˆ²ğ‘“ğ‘(â‹…) represents the classification function of the fused feature vectors ğ‘“ğ‘“ğ‘“ğ‘“, obtained by the feature fusion function as:

ğ‘“ğ‘“ğ‘™ğ‘“ğ‘“ğ‘–ğ‘“ğ‘“â„ğ‘“ğ‘“ğ‘–=îˆ²ğ‘“ğ‘¢ğ‘ (îˆ²â„ğ‘“ğ‘’(îˆ²ğ‘ ğ‘Ÿ(ğ‘¥ğ‘¥ğ‘™ğ‘–)), îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥ğ‘™ğ‘–)),=îˆ²ğ‘“ğ‘¢ğ‘ (îˆ²â„ğ‘“ğ‘’(ğ‘¥ğ‘¥â„ğ‘–), îˆ²ğ‘™ğ‘“ğ‘’(ğ‘¥ğ‘¥â„2ğ‘™ğ‘–)).
(10)
Finally, we derive the overall objective function as:

îˆ¸ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘™=îˆ¸ğ»ğ¼ğ‘€+îˆ¸ğ¿ğ¼ğ‘€+ğ›½îˆ¸ğ‘“ğ‘¢ğ‘ ,
(11)
where the parameter ğ›½ is a balance parameter. We set an identical weight to the HR reidentification loss (îˆ¸ğ»ğ¼ğ‘€) and the LR reidentification loss (îˆ¸ğ¿ğ¼ğ‘€).

Optimizing the overall loss îˆ¸ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘™ allows: (1) guiding the super-resolution îˆ²ğ‘ ğ‘Ÿ(â‹…) to recover the appearance information lost in low-resolution identity features, which is beneficial for identity classification; (2) forcing the high-resolution feature extraction function îˆ²â„ğ‘“ğ‘’(â‹…) to extract discriminative high-resolution identity features; (3) compelling the low-resolution feature extraction function îˆ²ğ‘™ğ‘“ğ‘’(â‹…) to extract the discriminative low-resolution identity features. Besides, reducing the loss îˆ¸ğ‘“ğ‘¢ğ‘  makes the high-resolution identity mapping and low-resolution identity mapping jointly   learned for obtaining resolution-specific discriminant features, which could be complementary. For example, LIM and HIM attend to different regions of a person image, making their features complementary, as shown in Fig. 5.

Fig. 5
figure 5
A large difference in the class activation maps derived by low-resolution specific identity mapping (LIM, second row) and high-resolution specific identity mapping (HIM, third row) suggests that the learning module could extract different complementary appearance feature information from the LR and HR person images

Full size image
Model Instantiation
We consider a deep CNN for model instantiation due to its strong merits such as (1) its ability learning discriminative representations from training data with great success on both image super-resolution (Dong et al. 2016a; Wang et al. 2015) and person reidentification (Li et al. 2014; Xiao et al. 2016; 2) its strong capability of learning highly non-convex tasks and its suitability for learning complex appearance variations in lighting, occlusion and background clutter; and (3) its high flexibility of reformulating the network architecture without the need for redesigning the optimization algorithm.

In particular, we design a hybrid deep neural network to realize the nonlinear functions involved, including the super-resolution function îˆ²ğ‘ ğ‘Ÿ(â‹…), the HR-specific feature extraction function îˆ²â„ğ‘“ğ‘’(â‹…), the LR-specific feature extraction function îˆ²ğ‘™ğ‘“ğ‘’(â‹…) and the feature fusion function îˆ²ğ‘“ğ‘¢ğ‘ (â‹…). The entire framework is made up of three parts: the HIM network (Fig. 4A) to compute the discriminative HR-specific identity features, the LIM network (Fig. 4B) to obtain the discriminative LR-specific identity features, and the FFN module (Fig. 4C) to acquire the final representations fused by the HR-specific and LR-specific identity features.

Architecture for High-Resolution Identity Modeling
The network architecture for HIM is depicted in Fig. 4A. Specifically, it consists of the two following modules.

-The Super-Resolution Module aims to compensate for and recover the information lost in the LR image acquisition, i.e., realizing îˆ²ğ‘ ğ‘Ÿ(â‹…). It has two parameter-sharing streams, taking ğ‘¥ğ‘¥ğ‘™ğ‘– (an LR image) and ğ‘¥ğ‘¥â„2ğ‘™ğ‘– (a cross-view synthetic LR image) as input . Following the super-resolution CNN (SRCNN) in (Dong et al. 2014), our super-resolution module is constructed by two convolutional layers followed by a nonlinear rectified linear unit (ReLU) layer and a reconstruction convolutional layer. The MSE loss function (Eq. 2) is used for quantifying the pixel-level alignment degree between the ground-truth HR ğ‘¥ğ‘¥â„ğ‘– and the super-resolution output of ğ‘¥ğ‘¥â„2ğ‘™ğ‘– during training. The super-resolution outputs of ğ‘¥ğ‘¥ğ‘™ğ‘– are not involved in calculating the reconstruction loss.

- The HR reidentification Module aims to learn HR-specific identity discriminant features, i.e., realizing îˆ²â„ğ‘“ğ‘’(â‹…), and imposing HR-specific reidentification constraints, i.e., realizing îˆ²â„ğ‘(â‹…). It has HR parameter-sharing streams taking as input the super-resolution output of the realistic LR image ğ‘¥ğ‘¥ğ‘™ğ‘– and the super-resolution output of the synthetic LR image ğ‘¥ğ‘¥â„2ğ‘™ğ‘– as well as the HR image ğ‘¥ğ‘¥â„ğ‘–. In our implementation, we adopt a 50-layer residual neural network (ResNet-50) (He et al. 2016a), which has achieved effective results in classification and detection tasks. In each stream, the penultimate fully connected (FC) layer outputs the reidentification feature, which is then fed into the last FC layer for identity classification. The summation of all three streamsâ€™ identity losses (Eq. 3) is used as the supervision signal for jointly qualifying the identification of all inputs during model training. In the implementation, we upscale the LR images to an appropriate size (256Ã—128 in our experiments) by bicubic interpolation as (Dong et al. 2016a).

We achieve joint learning of image super-resolution and person reidentification in the proposed CNN by using multipurposed synthetic LR image ğ‘¥ğ‘¥â„2ğ‘™ğ‘– (Fig. 4b), i.e., ğ‘¥ğ‘¥â„2ğ‘™ğ‘– is used for both training super-resolution module and person reidentification module. Formally, ğ‘¥ğ‘¥â„2ğ‘™ğ‘– and its super-resolution reidentification feature vector ğ‘“ğ‘“â„2ğ‘™â„â„ğ‘– ground four loss quantities: one super-resolution loss on (ğ‘¥ğ‘¥â„2ğ‘™ğ‘–, ğ‘¥ğ‘¥â„ğ‘–) correlated with three reidentification losses on three computed features ğ‘“ğ‘“â„2ğ‘™â„â„ğ‘–, ğ‘“ğ‘“ğ‘™â„â„ğ‘– and ğ‘“ğ‘“â„â„â„ğ‘–. This loss connection design injects more reidentification discrimination awareness into a jointly optimized image super-resolution model. We will evaluate the effect of our model design in our experiments.

Architecture for Low-Resolution Identity Modeling
With LIM, we realize the learning of the LR-specific feature function, i.e., realizing îˆ²ğ‘™ğ‘“ğ‘’(â‹…) and imposing LR-specific reidentification constraints îˆ²ğ‘™ğ‘(â‹…), by a deep CNN as well (Fig. 4m and n). Two parameter-sharing streams are involved, taking a realistic LR image ğ‘¥ğ‘¥ğ‘™ğ‘– and a synthetic LR image ğ‘¥ğ‘¥â„2ğ‘™ğ‘– as input. In the same manner as the HR-specific reidentification network, we adopt ResNet-50 (He et al. 2016a). In each stream, the penultimate FC layer outputs the reidentification feature, which is then fed into the last FC layer for LR-specific identity classification. The two streamsâ€™ identity losses (Eq. 7) are summed up and used as supervision signals for jointly qualifying the identification of both the realistic and synthetic LR inputs during training, i.e., two LR reidentification losses on LR-specific identity features ğ‘“ğ‘“ğ‘™ğ‘™ğ‘™ğ‘– and ğ‘“ğ‘“â„2ğ‘™ğ‘™ğ‘™ğ‘–. The size of the LR images is fixed as 256Ã—128 to obtain the discriminative LR-specific identity features. Note that the images still have the same LRs, although the spatial size is enlarged.

High- and Low-Resolution Collaborative Learning
We design a FFN to jointly learn the HR-specific and LR-specific identity features simultaneously.It collaboratively   fuses the discriminative HR-specific identity features (Sect. 4.1) and the LR-specific identity features (Sect. 4.2). The FFN module consists of a concatenation layer, a fully-connected layer, a batch normalization layer, a ReLU layer and a dropout layer.

Specifically, taking the HR-specific feature ğ‘“ğ‘“ğ‘™â„â„ğ‘– and the LR-specific feature ğ‘“ğ‘“ğ‘™ğ‘™ğ‘™ğ‘– as input, the FFN outputs a fused feature ğ‘“ğ‘“ğ‘™ğ‘“ğ‘“ğ‘– for an LR image ğ‘¥ğ‘¥ğ‘™ğ‘–; Moreover, taking the HR-specific feature ğ‘“ğ‘“â„â„â„ğ‘– and the LR-specific feature ğ‘“ğ‘“â„2ğ‘™ğ‘™ğ‘™ğ‘– as input, the FFN concurrently outputs another fused feature ğ‘“ğ‘“â„ğ‘“ğ‘“ğ‘– for a cross-view HR image ğ‘¥ğ‘¥â„ğ‘–. Subsequently, the two fused features (i.e., ğ‘“ğ‘“ğ‘™ğ‘“ğ‘“ğ‘– and ğ‘“ğ‘“â„ğ‘“ğ‘“ğ‘–) are further fed into an FC layer for separate identity classification. Finally, the two identity losses (Eq. 9) are summed up to jointly supervise the identification of both the LR and HR inputs during training.

For feature fusion, we consider the following operation:

ğ‘“ğ‘“ğ‘“ğ‘“=îˆ²ğ‘“ğ‘¢ğ‘ (ğ‘“â„ğ‘“â„,ğ‘“ğ‘™ğ‘“ğ‘™)âˆˆâ„ğ‘›â„+ğ‘›ğ‘™
(12)
ğ‘“â„ğ‘“â„=[ğ‘“â„,1,ğ‘“â„,2,...,ğ‘“â„,ğ‘—,...,ğ‘“â„,ğ‘›â„]âˆˆâ„ğ‘›â„
(13)
ğ‘“ğ‘“ğ‘™=[ğ‘“ğ‘™,1,ğ‘“ğ‘™,2,...,ğ‘“ğ‘™,ğ‘˜,...,ğ‘“â„,ğ‘›ğ‘™]âˆˆâ„ğ‘›ğ‘™
(14)
where ğ‘“â„ğ‘“â„ is an HR feature vector (e.g., ğ‘“â„ğ‘“â„â„, ğ‘“â„ğ‘“â„ğ‘™) and ğ‘“ğ‘“ğ‘™ is an LR feature vector (e.g., ğ‘“ğ‘™ğ‘“ğ‘™â„, ğ‘“ğ‘™ğ‘“ğ‘™ğ‘™). The scalars ğ‘›â„ and ğ‘›ğ‘™ are the dimensions of ğ‘“â„ğ‘“â„ and ğ‘“ğ‘™ğ‘“ğ‘™, respectively. ğ‘“ğ‘“ğ‘“ğ‘“ denotes the final joint bilateral-resolution features generated by ğ‘“â„ğ‘“â„ and ğ‘“ğ‘™ğ‘“ğ‘™. The back propagation operation of the concatenation layer is as follows:

âˆ‚ğ‘“ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡,ğ‘–âˆ‚ğ‘“â„,ğ‘—={10 if ğ‘–=ğ‘— if ğ‘–â‰ ğ‘—
(15)
âˆ‚ğ‘“ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡,ğ‘–âˆ‚ğ‘“ğ‘™,ğ‘˜={10 if ğ‘–=ğ‘˜+ğ‘›â„ if ğ‘–â‰ ğ‘˜+ğ‘›â„
(16)
where ğ‘“ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘“ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡=[ğ‘“â„ğ‘“â„,ğ‘“ğ‘™ğ‘“ğ‘™]âˆˆâ„ğ‘›â„+ğ‘›ğ‘™. The symbol [,] means the concatenation of the two vectors. ğ‘“ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡,ğ‘–âˆˆğ‘“ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ğ‘“ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ‘–=1,2,...,ğ‘›â„+ğ‘›ğ‘™), ğ‘“â„,ğ‘—âˆˆğ‘“â„ğ‘“â„(ğ‘—=1,2,...,ğ‘›â„), ğ‘“ğ‘™,ğ‘˜âˆˆğ‘“ğ‘™ğ‘“ğ‘™(ğ‘˜=1,2,...,ğ‘›ğ‘™). Since the concatenation operation will only keep the gradient of each neuron, the partial derivative is equal to 1 at corresponding position, otherwise it is equal to 0. These two equations indicate that the gradients of the HR feature vector ğ‘“â„ğ‘“â„ and the LR feature vector ğ‘“ğ‘™ğ‘“ğ‘™ do not influence each other before they are input into the FFN module.

Model Testing
In the model test phase, we extract joint bilateral-resolution features for both LR probe and HR gallery images. The generic ğ¿2 distance metric is then used for reidentification matching.

Specifically, for the HR gallery images, we directly use the jointly learned HR reidentification subnet in the HIM network to obtain the HR-specific identity features and compute the LR-specific identity features of the corresponding downsampled LR images by LIM. For the LR probe images, we directly apply the LIM network to compute the LR-specific identity features. To compute the HR-specific identity features, we apply the image super-resolution network in HIM to super-resolve them before performing feature extraction by the reidentification subnet in the HIM network. The final joint bilateral resolution features are generated by the FFN module in which the HR-specific identity features and the corresponding LR-specific identity features are concatenated.

Multiresolution Adaptive Ensemble
The JBIM framework formulated as above assumes that all the LR images have similar underlying resolutions because the super-resolution network in the HIM network is optimized for super-resolving the LR images by a ratio m, which renders a single JBIM model suboptimal when the resolution ratio is far away from m as typically encountered in practice, where multiple different LRs are presentFootnote2.

To address this problem, we propose creating ğœ‘ anchor JBIM models {ğ‘€ğ‘€1,ğ‘€ğ‘€2,â‹¯,ğ‘€ğ‘€ğœ‘}, where each is responsible for optimizing a reference super-resolution ratio in the set {ğ‘š1,ğ‘š2,â‹¯,ğ‘šğœ‘}. These JBIM models are then used jointly to accommodate various resolutions involved in cross-resolution reidentification matching. Each model ğ‘€ğ‘€ğ‘– can be similarly learned as described above by the corresponding synthetic LR images ğ‘‹â„2ğ‘™ generated by downsampling HR images with a ratio ğ‘šğ‘–, along with realitic LR and HR training images. In our experiments, we used three models corresponding to the downsampling ratio {12,13,14}.

In the test, given an LR probe image, we first compute ğœ‘ distance vectors {ğ·ğ‘–}ğœ‘ğ‘–=1 between the probe image and all the gallery images with each anchor JBIM model, where ğ·ğ‘– denotes the distance computed by model ğ‘€ğ‘€ğ‘–, ğ‘–âˆˆ{1,2,â‹¯,ğœ‘}. Then, we compute a multiresolution fused distance vector as:

ğ·ğ‘šğ‘Ÿğ‘=âˆ‘ğ‘–=1ğœ‘ğ‘¤ğ‘–ğ·ğ‘–,
(17)
where {ğ‘¤ğ‘–}ğœ‘ğ‘–=1 represents the distance weights.

To make ğ·ğ‘šğ‘Ÿğ‘ resolution adaptive, we consider the similarity in the underlying resolution among the LR probe image, all the HR gallery images, and each JBIM model. We quantify the resolution similarity between the LR probe and HR gallery images as:

ğ‘Ÿ= ğ´ğ‘  ğ´ğ‘”~ â€¾â€¾â€¾â€¾â€¾âˆš,
(18)
where ğ´ğ‘ denotes the spatial area (i.e., the number of pixels) of the LR probe and ğ´ğ‘”~ is the mean spatial area of all the HR gallery images. They are computed on genuine resolution scales without resizing. We then take into account the super-resolving ratio ğ‘šğ‘– as:

ğ‘¤ğ‘–=exp{âˆ’ğœâˆ’2â‹…(ğ‘Ÿâˆ’ğ‘šğ‘–)2},
(19)
where ğœ is a scaling parameter estimated by cross-validation.

Experiments
Datasets
We performed evaluations on one genuine and four simulated cross-resolution person reidentification datasets (Fig. 6). Instead of assuming a single underlying resolution for all the LR images, we consider multiple LRs (MLR) as in real-world situations. Therefore, we used different downsampling rates when simulating the LR images from the HR images.

Fig. 6
figure 6
Examples of HR (1st row) and LR (2nd row) person images from five datasets

Full size image
(1) CAVIAR is an cross-resolution person reidentification dataset (Cheng et al. 2011). It contains 1220 images of 72 persons captured from two camera views in a shopping mall. Albeit of small scale, this dataset is particularly suitable for evaluating cross-resolution reidentification because the resolution of images from one camera (the distant camera) is much lower than that from the other camera (the close camera). We discarded 22 persons who appeared only in the close camera (HR images). For each of the remaining 50 persons used in our experiments, there are 10 HR and 10 LR images, i.e., a total of 1,000 images. Unlike other simulated datasets, the LR images in CAVIAR involve multiple realistic resolutions.

(2) MLR-CUHK03 was built from the CUHK03 (Li et al. 2014) dataset. The CUHK03 dataset consists of five different pairs of camera views and contains more than 14,000 images of 1,467 pedestrians. By following the settings outlined in (Xiao et al. 2016), both the manually cropped and automatically detected images were used in our evaluations. For each camera pair, we randomly selected one as the LR probe image source by performing downsampling by a ratio randomly selected from {12,13,14}. This procedure results in a simulated multiple LRs (MLRs) reidentification dataset MLR-CUHK03.

(3) MLR-SYSU is based on the SYSU dataset (Chen et al. 2017a), which has 24,446 images of a total of 502 persons captured by two cameras. We randomly selected three images per person per camera in our evaluations and created an MLR reidentification dataset MLR-SYSU as for CUHK03.

(4) MLR-VIPeR was constructed from the VIPeR (Gray and Tao 2008) dataset, which contains 632 person image pairs captured by two cameras. Each image is of HR (128Ã—48 pixels). To make this dataset suitable for cross-resolution person reidentification evaluation, we performed similar multiresolution downsampling on all the images from one camera view, while the remaining images from the other view remained the same. This procedure resulted in the MLR-VIPeR dataset.

(5) MLR-Market was constructed from the Market-1501 (Zheng et al. 2015) dataset, which contains 1501 people captured by 6 cameras with varying viewpoints and lighting conditions. Each person is captured by at least two cameras, and each camera may obtain more than 10 pictures. To make this dataset suitable for cross-resolution person reidentification evaluation, we downsampled half of the images of a person by a ratio randomly selected from {12,13,14}, while the remaining images of the other half remained the same.

Settings and Implementation
- Evaluation Protocol. We adopted the standard single-shot reidentification setting in our experiments. The CAVIAR, MLR-VIPeR and MLR-SYSU datasets were randomly divided into two halves: one-half for training and the other half for testing. That is, there are ğ‘=25, ğ‘=316 and ğ‘=251 persons in the testing sets of CAVIAR, MLR-VIPeR and MLR-SYSU, respectively. Following (Xiao et al. 2016), we utilized the benchmarking 1,367/100 training/test identity split for the MLR-CUHK03 dataset. In addition, we applied the 751/750 training/test identity split setting on the MLR-Market dataset following (Zheng et al. 2015). For the testing data, we constructed the probe set with all LR images per person and the gallery set with one randomly selected HR image per person. For performance evaluation, we used the average cumulative match characteristic (CMC) and mean average precision (mAP) to measure the cross-resolution reidentification matching performance.

- Implementation Details. We first trained the HIM and LIM networks on each target cross-resolution reidentification dataset separately and then trained the whole JBIM model with the HIM and LIM networks initialized with the independently trained parameters and the randomly initialized FFN. The backbone of our network was trained with random horizontal flipping, random cropping and batch normalization neck (BNNeck) (Luo et al. 2019). For the HIM network, we initialized the super-resolution network by the SRCNN (Dong et al. 2014) with a padding operation for all the convolution layers and the ResNet-50-based reidentification network (He et al. 2016a) with ImageNet weights. We find that using other super-resolution networks yields little performance improvement. We initialized the ResNet-50-based LIM network with ImageNet weights. The ğœ (Eq. (19)) was set to 0.5. We set the balancing coefficient ğ›¼=1 (Eq. (6)) and ğ›½=3 (Eq. (11)). The parameters ğ›¼ and ğ›½ will be discussed in the experiments.

Comparing Existing Low-Resolution reidentification Models
We compared the proposed JBIM method with eight existing state-of-the-art cross-resolution reidentification methods, including three traditional methods and five deep CNN-based methods: (1) joint multiscale discriminant component analysis (JUDEA) (Li et al. 2015): a cross-scale discriminative distance metric learning model; (2) semi-coupled low-rank discriminant dictionary learning (SLD2L) (Jing et al. 2015): a feature transformation or alignment model; (3) scale-distance function (SDF) (Wang et al. 2016): a scale-distance function learning model; (4) cascaded SR-GAN (CSR-GAN) (Wang et al. 2018): a joint learning of the person reidentification and multiple cascaded SR-GANs; (5) resolution-invariant person reidentification (RIPR) (Mao et al. 2019): a network jointly training a foreground focus super-resolution module and a reso-lution-invariant feature extractor by end-to-end CNN learning; (6) cross-resolution adversarial dual network (CAD-Net) (Li et al. 2019a): a generative dual model for cross-resolution person reidentification. (7) inter-task association critic (INTACT) (Cheng et al. 2020): a model leveraging the association between image SR and person reidentification tasks; (8) degradation-invariant reidentification (DI-REID) (Huang et al. 2020): a degradation-invariant learning framework.

Table 2 Comparison of the complexity of state-of-the-art cross-resolution reidentification methods that are based on ResNet-50 as the backbone. We calculate the complexity of these models according to the description in their papers. â€œParamsâ€ is the number of parameters of model, and â€œMACsâ€ is the number of fixed-point multiplication and accumulation operations per second
Full size table
Table 3 Comparison of the performance of the state-of-the-art reidentification methods(%). The 1st/2nd best results are indicated in bold/italic
Full size table
It is evident from Table 3 that our JBIM method outperforms all the competitors in most cases. For example, the JBIM method surpassed the best alternative traditional method JUDEA by 30.0%, 62.1%, 40.8%, and 23.7% in terms of the rank-1 matching rate on the CAVIAR, MLR-CUHK03, MLR-SYSU, and MLR-VIPeR datasets, respectively. The performance margins of the JBIM method over the SLD2L and SDF models are still larger.

Moreover, our JBIM method surpasses the four deep CNN-based cross-resolution reidentification methods on each dataset except the DI-REID model on the VIPeR dataset. Specifically, the strongest competitors are namely DI-REID (Huang et al. 2020) and INTACT (Cheng et al. 2020), particularly on the MLR-VIPeR and MLR-Market dataset. Whilst our modelâ€™s result is on par with DI-REID, the complexity of our method is largely smaller (9 times smaller), as reported in Table 2, meaning that our model is more cost-effective and more computationally scalable. In comparison with INTACT, we found that it uses OSNet (Zhou et al. 2019) as the backbone which is stronger than ResNet-50 (He et al. 2016a) as our model used for reidentification. We thus tested our model with OSNet. Table 3 shows that our method can outperform INTACT on all datasets with a clear margin using the same backbone model.

The above results indicate the advantages of the proposed JBIM model in handling both simulated and genuine cross-resolution reidentification. The performance superiority is mainly due to (1) the capability of jointly super-resolving images and learning discriminative person reidentification features, which allows us to maximize their mutual correlation. Compared to the cross-resolution alignment-based competitor, our model is able to synthesize high-frequency missing LR images by reidentification discriminative super-resolution and thus extracts richer representations, which not only directly mitigates the information amount discrepancy problem but also fills the hard-to-bridge matching gap between different resolutions with the appearance pattern divergence involved. (2) By jointly learning the bilateral-resolution identity feature, the presented multiresolution (HR and LR) features better characterize the salience of a person in cross-resolution reidentification than all the traditional and deep CNN-based methods using only single-resolution features.

Comparing Existing State-of-the-Art reidentification Methods
To validate the necessity of a specially designed framework for the cross-resolution reidentification problem, we have implemented three SOTA reidentification methods (i.e. OSNet (Zhou et al. 2019), ABD-Net (Chen et al. 2019a), and AGW (Ye et al. 2021)) in our problem setting, which are not specially designed for cross-resolution reidentification. The same training and testing settings are used as our method. As shown in Table 3, our method surpasses all these alternative reidentification methods for cross-resolution person reidentification. It demonstrates that the resolution mismatch problem need targeted solutions.

Comparing the Super-resolution + reidentification Scheme
We further evaluated the cross-resolution person reidentification performance by deploying   a straightforward   combination   of the super-resolution and person reidentification scheme. While conventional reidentification methods assume using HR images, we utilize state-of-the-art super-resolution models when LR images are given to meet their requirement. We used the same training images as the proposed JBIM method to fine-tune the super-resolution models. The proposed multiresolution adaptive ensemble algorithm was applied to all the compared methods for a fair comparison.

- Compared Methods. The conventional reidentification methods considered in our evaluations are as follows: (1) cross-view quadratic discriminant analysis (XQDA) (Liao et al. 2015): a supervised Mahalanobis metric learning method; (2) domain-guided dropout (DGD) : a widely used deep CNN reidentification model; (3) ResNet-50 (He et al. 2016a): a state-of-the-art deep CNN classification model. We utilized the contemporary local maximal occurrence (LOMO) handcrafted features (Liao et al. 2015) for XQDA.

The image super-resolution methods we selected for evaluation include two standard algorithms and five state-of-the-art algorithms: (1) Bilinear: a popular linear interpolation-based super-resolution model effective in handling generic image scaling; (2) Bicubic: another widely used image super-resolution method, which is an extension of cubic interpolation; (3) SRCNN (Dong et al. 2014): an existing state-of-the-art deep CNN-based super-resolution model; (4) fast SRCNN (FSRCNN) (Dong et al. 2016b): an accelerated deep CNN-based super-resolution model; (5) very deep super-resolution (VDSR) (Kim et al. 2016a): a super-resolution method based on a very deep CNN; (6) deeply-recursive convolutional network (DRCN) (Kim et al. 2016b):   a deeply-recursive convolutional network-based   super-resolution   method; (7) SRGAN (Ledig et al. 2016): a super-resolution method using a GAN to compensate for the image details.

- Results & Analysis. For the other methods, we show different straightforward combinations of the super-resolution methods and reidentification methods. Table 4 shows that the proposed JBIM method significantly outperformed all of the combinations of super-resolution+reidentification methods. Specifically, the rank-1 matching rate over all the competitors by the JBIM method can reach 12.9% (52.0-39.1),   14.5%(88.3-73.8),   6.3%(59.1-52.8),   17.4%(49.7-32.3) and 22.9%(88.1-65.2) on the CAVIAR, MLR-CUHK03, MLR-SYSU, MLR-VIPeR and MLR-Market datasets, respectively. These results show that the joint bilateral resolution features outperformed the single HR-specific identity features of the   images super-resolved by the generic super-resolution methods.

Table 4 Comparing combinations of image super-resolution and person reidentification schemes (%). The best and 2nd-best results are indicated in bold and italic, respectively.
Full size table
Fig. 7
figure 7
Qualitative evaluations of the super-resolved person images by different methods. The ground-truth normal-resolution images (2nd column) are indicated by red bounding boxes (Color figure online)

Full size image
- Qualitative Evaluation of Different Super-Resolution   Methods. We qualitatively compared the super-resolved person images produced by Bilinear, Bicubic, SRCNN, FSRCNN, VDSR, DRCN, SRGAN and our JBIM. Two examples are presented in Fig. 7. We make the following observations: (1) Super-resolved images by bilinear and bicubic interpolation are more blurry than those produced by the CNN-based super-resolution methods and our proposed JBIM method. (2) More edge/contour elements and better structured texture patterns are recovered by the proposed JBIM method. In addition, the color distributions of the images produced by the JBIM method are more similar to the ground-truth color distributions than those produced by the other methods. This difference visually indicates the advantages of JBIM over super-resolution + reidentification methods due to the capability of recovering missing/enhancing appearance details while ensuring better reidentification discrimination.

Further Analysis of the Proposed Method
Evaluation of the Individual Components
We provide detailed model component analysis in terms of performance contribution. The comparisons on the five cross-resolution reidentification datasets are summarized in Table 5. In particular, â€œJBIM w/o HIMâ€ means that we train the LIM branch (Fig. 4B) independently by using only the LR-specific identity features for reidentification. Similarly, â€œJBIM w/o LIMâ€ means that we train the HIM branch (Fig. 4A) independently by using only the HR-specific identity features for reidentification.

- Super-Resolution Versus Low-resolution Specific Identity Features. We evaluate the cross-resolution reidentification performance of the HR-specific identity features learned from the JBIM w/o LIM model (i.e., using the HIM network only) in comparison to the LR-specific identity features from the JBIM w/o HIM model (i.e., using the LIM network only). The results are shown on five datasets in Table 5. Although it is intuitive that using the HR features outperformed the LR features by 1.7%(48.7-47.0), 0.3%(87.1-86.8), 2.3%(58.0-55.7), 4.8%(45.6-40.8), 1.2%(87.7-86.5) in terms of the rank-1 matching rate on the CAVIAR, MLR-CUHK03, MLR-SYSU,   MLR-VIPeR and MLR-Market datasets, respectively, it is also clear that the JBIM method without HIM is not weak; i.e., discriminant identity features exist in the LR person image.

- Single-Resolution Versus Multiresolution Features. We further evaluate the cross-resolution reidentification performance advantages of our multiresolution features in comparison to independently   learned single-resolution features. Table 5 reports the matching results of the JBIM method, the JBIM method without LIM, and the JBIM method without HIM. From the experimental results, it can be observed that the joint learned multiresolution features achieve a better performance than using only one of them. Without the LIM network, the rank-1 matching rate of the JBIM method drops by 3.3%(52.0-48.7), 1.2%(88.3-87.1), 1.1%(59.1-58.0), 4.1% (49.7-45.6), 0.4% (88.1-87.7) on the CAVIAR, MLR-CUHK03, MLR-SYSU, MLR-VIPeR and MLR-Market datasets, respectively, which suggests that the LR-specific identity features are still important for constructing the joint bilateral-resolution features, although the LR-specific identity features contain less information than the HR-specific identity features. The performance margins of the JBIM method over the JBIM method without HIM (i.e., LIM) are even larger. This finding validates the effectiveness of our proposed multiresolution feature learning method in improving cross-resolution reidentification matching.

Table 5 Matching rate (%): Evaluation of the individual components
Full size table
Jointly Learning the Multiresolution Features
JBIM can be considered a joint hybrid model of the HIM network, the LIM network and the FFN to learn a multiresolution feature fused by HR-specific and LR-specific identity features. To validate the effectiveness of jointly learned multiresolution features, we conduct a comparison experiment with â€œHIM+LIMâ€ , which learns the HR-specific and LR-specific identity features independently and then concatenates the HR-specific and LR-specific identity features into the final features. As shown in Table 6, such a feature fusion method is effective: the JBIM method yields rank-1 matching rate improvement over â€œHIM+LIMâ€ by 2.8%(52.0-49.2), 0.8%(88.3-87.5), 0.4%(59.1-58.7), 2.9% (49.7-46.8) and   0.4%(88.1-87.7) on the CAVIAR, MLR-CUHK03, MLR-SYSU and MLR-Market datasets, respectively. This finding suggests that the joint learning of the HR-specific and LR-specific identity features achieves better results than concatenating the independent LR-specific and HR-specific identity features. The results validate the effectiveness of FFN of the JBIM framework.

Table 6 Matching rate (%): Comparing the independent learning methods. â€œHIM + LIMâ€ means learning HIM and LIM independently; â€œSRCNN+ResNet-50+LIMâ€ means learning super-resolution module, HR-specific reidentification module and LIM independently
Full size table
Synthetic Low-Resolution Images
We evaluate the contribution of joint super-resolving the synthetic LR images by the MSE loss (Eq. (2)), in conjunction with classifying the resolved image (Eq. (3)). To this end, we evaluate a stripped-down JBIM framework in which the HIM network without the streams of the synthetic LR images (see the green arrows in Fig. 4 (A)). As such, the MSE super-resolution loss is removed because no LR-HR training image pairs are available. Table 7 shows that an inferior cross-resolution reidentification performance will be yielded without this joint learning stream. For example, the rank-1 rate drops from 52.0% to 50.1% on the CAVIAR dataset, from 88.3% to 87.9% on the MLR-CUHK03 dataset, from 59.1% to 58.3% on the MLR-SYSU dataset, from 49.7% to 47.2% on the MLR-VIPeR dataset, and from 88.1% to 87.5% on the MLR-Market   dataset, respectively.

This performance drop validates the usefulness of the proposed joint learning approach in guiding the image super-resolution model toward generating HR images with reidentification discriminative visual information. We further directly evaluate the stripped-down HIM network (JBIM without LIM) without synthetic LR images. From Table 7, we can see that the performance also decreases without joint learning. Specifically, the rank-1 matching rate decreases by 1.8%, 0.2%, 0.8%, 1.5%, 0.8% on CAVIAR, MLR-CUHK03, MLR-SYSU, MLR-VIPeR and MLR-Market datasets, respectively. The results further indicate that super-resolution and reidentification joint learning can obtain better discriminative HR-specific identity features than other methods.

Table 7 Effect of jointly super-resolving and classifying synthetic LR images (%)
Full size table
Comparing Models Trained with All Resolution Images
Table 8 Matching rate (%): â€œAll-Resolutionâ€ means training a ResNet-50 with all resolution images (e.g. normal HR training images and LR images obtained by manually down-sampling original training images at the ratios of {12,13,14})
Full size table
We consider that there are specific features for different resolutions of an image. In order to prove the necessity of LR-specific identity information, we train a ResNet-50 (i.e., our backbone) with all resolutions of training images, and we denote this variant as â€œAll-Resolutionâ€. Compared with our JBIM that learns the collaboration of LR-specific and HR-specific information, the â€œAll-Resolutionâ€ model achieves suboptimal performance. A plausible reason is that given a large number of different resolutions with the training data, the learned network has to fit all resolutions and could discard the specific resolution information. In contrast, our approach is able to extract different specific resolution information and make them collaborate by optimisation. As shown in Table 8, our method outperforms the model trained with all resolution images. In addition, we conduct a visualization of class activation maps of these two networks, and it can be found in Fig. 8. It is found that our modelâ€™s class activation maps could capture complementary person appearance.

Super-Resolution and reidentification Loss Balancing
We evaluated the balancing effect between   image super-resolution and the person reidentification loss by varying the trade-off parameter ğ›¼ in Eq. (6) (ğ›¼=1 in all the other experiments). We conducted this analysis on all the genuine and simulated cross-resolution reidentification datasets with ğ›½=3 fixed. Table 9 shows that the experimental results are insensitive to variations in parameter ğ›¼. Specifically, the rank-1 matching rate is highest when parameter ğ›¼=1 in Table 9. Thus, we use ğ›¼=1 in our comparison experiments. Moreover, when setting ğ›¼=0, the rank-1 matching rate performance dropped 2.6%, 0.5%, 0.6%, 1.6%, 1.0% on the CAVIAR, MLR-CUHK03, MLR-SYSU, MLR-VIPeR and MLR-Market datasets, respectively, because super-resolution reconstruction is totally ignored, and thus, there is no interaction between super-resolution and reidentification. Moreover, the learning constraint on the super-resolution submodel is weak, backpropagated from person identity classification supervision, and therefore results in poor reidentification matching.

Single- and Multi- Resolution reidentification Loss Balancing
Fig. 8
figure 8
Visualizations of our JBIM and All-Resolution

Full size image
Table 9 Effect of balancing image super-resolution and the person reidentification loss (ğ›½=3)
Full size table
We further evaluated the balancing effect between the single-resolution loss (i.e., the LIM loss and the HIM loss) and the resolution fusion loss by varying the trade-off parameter ğ›½ in Eq. (11) (ğ›½=3 in all the other experiments). We conducted this analysis on all the genuine and simulated cross-resolution reidentification datasets with ğ›¼=1 fixed. Table 10 shows that the experimental results are insensitive to variations in parameter ğ›½. Specifically, the rank-1 matching rate of most datasets peaks when ğ›½=3. Therefore, we use ğ›½=3 in our comparison experiments.

Different Basic Super-Resolution or reidentification Models
Table 10 Effect of balancing the single-resolution and multiresolution reidentification losses (ğ›¼=1)
Full size table
Table 11 Effects of super-resolution CNN models (%)
Full size table
Table 12 Effects of reidentification CNN models (%)
Full size table
Table 13 Matching rate(%): Effect of scale-adaptive LR fusion of HIM and JBIM.
Full size table
We further validate the flexibility of the JBIM framework by choosing different super-resolution or reidentification CNN models to construct different variants. In particular, we replaced the default super-resolution model (i.e., SRCNN) with VDSR, DRCN and SRGAN in all the other experiments or replaced the default reidentification model (i.e., ResNet-50) with OSNet in all the other experiments for our proposed JBIM framework and JBIM framework without LIM (i.e., using HIM only) models. The results are shown in Tables 11 and  12. In the tables, â€œJBIM (VDSR)â€ and â€œHIM (VDSR)â€ mean that we use VDSR for super-resolution in the JBIM and HIM frameworks, respectively. Similarly, â€œJBIM (OSNet)â€, â€œHIM (OSNet)â€ and â€œLIM (OSNet)â€ mean that OSNet is used in our model as backbone. The results indicate that our joint learning methods perform stably across different super-resolution and reidentification models.

Scale-Adaptive Low-Resolution Fusion
We evaluated the effect of fusing discriminative feature representations from multiple LR scales for improving person matching using the proposed scale-adaptive ensemble algorithm. To this end, we evaluated the cross-resolution reidentification performance of six combination schemes from three different scale-specific JBIM models (ğ‘€12, ğ‘€13, ğ‘€14). We further evaluated the effect of the proposed scale-adaptive ensemble algorithm on the JBIM framework without LIM (i.e., using only the HIM network). They correspond to 3 downsampling ratios {12,13,14}. Table 13 shows that more scales of LR information fused by the proposed method yield better results that we can achieve. The best results over all five cross-resolution reidentification datasets are yielded by fusing all three scale-specific models. This finding validates the efficacy of the proposed multiscale fusion algorithm. Moreover, this observation is consistent in spirit with the classical pyramid matching kernel (Grauman and Darrell 2005; Lazebnik et al. 2006), except that our multiscale fusion is uniquely on multiple pixel-level resolutions rather than on multiple spatial extents of the same resolution.

Ablation Study for the Scaling Parameter ğœ
We have evaluated the effect of the scaling parameter ğœ in Eq. (19). As shown in Fig. 9, the increase of ğœ improves the performances on each dataset when ğœ is smaller than 0.2. When ğœ is larger than 0.2, the performance does not change a lot. The parameter ğœ is not an important parameter in our experiments, and we just use it to enlarge the difference between the resolution similarity r and super-resolving ratio ğ‘šğ‘–. In our experiments, ğœ is set to 0.5.

If we take ğœ as a variable s and (ğ‘Ÿâˆ’ğ‘šğ‘–)2 as a constant C, the function in Eq. (19) can be converted to

ğ‘¤ğ‘–=ğ‘’âˆ’ğ¶âˆ—ğ‘ âˆ’2,ğ‘ âˆˆ(0,+âˆ)
(20)
which is a monotone increasing bounded function whose upper bound is 1. Therefore, the distance weight ğ‘¤ğ‘– does not change a lot if we continue to increase the scaling parameter ğœ.

Table 14 Ablation study of the down-sampling ratio. â€œDown-sampling ratio for trainingâ€ refers to the down-sampling ratio of LR images using in the training process, HR images do not change. â€œDown-sampling ratio for testingâ€ means the down-sampling ratio of LR probe images using in the testing process, HR gallery images stay as normal
Full size table
Fig. 9
figure 9
Ablation study of the scaling parameter ğœ

Full size image
Study of Down-Sampling Ratios
Except the original setting with the down-sampling ratios {12,13,14} for both training and testing sets, we have now evaluated each individual ratio separately. From Table 14, we have several observations: 1) When the down-sampling ratio of test set becomes larger, the performance will drop due to more missing observations; 2) The best performance can be achieved when the same ratio is applied to the training set, suggesting that image resolution is a dimension that matters to model performance.

Conclusion
In this work, we present a joint bilateral-resolution identity modeling (JBIM) framework for solving the cross-resolution person reidentification challenge. The JBIM framework collaboratively learns both HR-specific and LR-specific identity features by introducing a synergistic interplay between super-resolution and discriminant reidentification feature learning. In particular, we have demonstrated the significance of exploiting LR-specific identity features in joint learning for overcoming cross-view and cross-resolution cross-resolution reidentification. Extensive evaluations on five benchmarks show the clear superiority of our JBIM model over existing cross-resolution reidentification methods and fusions of state-of-the-art super-resolution and reidentification models, without elaborate network architecture design. We have also conducted a full spectrum of model component analysis to validate the effectiveness of individual modules and provide insights into our model formulation.

Notes
Note that in terms of visual surveillance definitions, the quality of so-called high resolution (HR) images is poorer than that of social media photos taken by professional photographers. In this context, we define LR and HR in a relative sense for surveillance-quality image data. By default, we define the â€œresolutionâ€ as the underlying resolution (Wang et al. 2010) rather than the image spatial size (scale). A given image can be arbitrarily resized with little change to its underlying resolution (Fig. 3). Hence, the image spatial size is not an accurate indicator of the underlying resolution.

While HR images also have different resolutions, we focus on handling the LR images in this work because LR images suffer more significant information loss than HR images during data acquisition and are therefore the major cause of degraded reidentification matching performance. We assume that HR images share a similar resolution for simplicity. However, the strategy proposed here can be similarly applied to deal with HR images of different underlying resolutions.