This paper proposes a new visual tracking algorithm, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking. To this end, a regression network is learned offline to detect a set of target candidates through target template matching. To cope with target appearance variations in long-term scenarios, a target-aware feature fusion mechanism is also developed, giving rise to more effective template matching. Meanwhile, a verification network is trained online to better capture target appearance and identify the target from potential candidates. During online update, contaminated training samples can be filtered out through a monitoring module, alleviating model degeneration caused by error accumulation. The regression and verification networks operate in a cascaded manner, which allows tracking to be performed in a coarse-to-fine manner and enforces the discriminative power. To further address the target reappearance issues in long-term tracking, a learning-based switching scheme is proposed, which learns to switch the tracking mode between local and global search based on the tracking results. Extensive evaluations on long-term tracking in the wild have been conducted. We achieve state-of-the-art performance on the OxUvA long-term tracking dataset. Our submission based on the proposed method has also won the 1st place of the long-term tracking challenge in VOT-2018 competition.

Access provided by University of Auckland Library

Introduction
Single object visual tracking can be roughly divided into either short-term or long-term tracking tasks. Recent research in short-term visual tracking has witnessed a significant progress due to the application of deep learning techniques (Danelljan et al. 2016, 2017; Nam and Han 2016; Sun et al. 2018a, b; Li et al. 2018). Nevertheless, the above successes in the short-term area cannot be easily transferred to long-term tracking in the wild, which introduces many new challenging factors.

Table 1 Differences between popular short and long-term tracking benchmark datasets
Full size table
Table 1 compares the differences between popular short-term (OTB100 (Wu et al. 2015) and VOT2017 (Kristan et al. 2018)) and long-term [VOT2018 LTB35 (Lukežič et al. 2018) and OxUvA (Valmadre et al. 2018)] tracking benchmark datasets in terms of frame length and the number of frames where targets are completely out of view (indicated by absent labels). Compared to short-term ones, most long-term videos with more frames span a much longer period of time, during which both the target and the background may undergo great appearance changes, and small errors may be accumulated through time, leading to tracking drifts. In addition, the targets in long-term videos may frequently leave and re-enter the camera view with significant appearance and scale variations (a.k.a. target disappearance and reappearance issues(Kristan et al. 2018)), which draws higher demand on the tracker’s ability to re-detect the target.

Until recently, a few works (Ma et al. 2015; Zhu et al. 2018b; Kalal et al. 2012; Nebehay and Pflugfelder 2015; Lukežič et al. 2017; Hong et al. 2015) have tried to address the above long-term challenges. It has been shown that template matching based models Nebehay and Pflugfelder (2015); Zhu et al. (2018b) can well adapt to target appearance variations, but mostly fail to cope with cluttered background. In contrast, online learned classifiers Lukežič et al. (2017) have stronger capabilities to discriminate target from cluttered background, but are less robust to dramatic appearance changes. Other algorithms (Ma et al. 2015; Kalal et al. 2012) explore ensemble models to mitigate this challenge. Nonetheless, most of these methods directly integrate short-term tracking techniques which are not specifically designed for long-term tracking in the wild. Therefore, they may deliver sub-optimal performance due to the essential differences between short and long-term tracking tasks. Particularly, the target disappearance and reappearance issues cannot be well handled due to the difficulties in tracker recovery after target reappearance.

In light of the above issues, we propose to combine the advantages of matching and classification based methods and perform long-term tracking in a coarse-to-fine manner. Since matching based methods have a strong generalization ability across object categories, we train a regression network offline to measure the similarity of generic object appearances. During online tracking, it detects a set of target candidates of high confidences, effectively narrowing the target search space. To leverage the discriminative power of classification models, we further design a verification network, which is learned online to deal with cluttered background and identify the final target from the detected target candidates.

Compared to existing methods, our regression network adopts a target-aware feature fusion mechanism, which is proved to be more robust against target appearance changes after reappearances. Besides, the online training samples for verification network have been carefully selected by a monitoring module. As such, the impact of noisy training samples can hardly be accumulated through time, leading to more reliable tracking results. In addition, a learning-based switching scheme has also been explored, which triggers a global search mode after target disappearance, further improving long-term tracking accuracy (See Fig. 1 for an example).

Fig. 1
figure 1
Tracking results of our method in handling target disappearance and reappearance issues. The present and absent labels are inferred by the proposed switching scheme and serve as a signal to trigger global target search

Full size image
The main contributions of this work can be summarized as follows.

A new long-term tracking framework is proposed, which integrates matching and classification-based models in an effective cascaded manner, and enjoys the advantages of both worlds. A system switching scheme is also devised to better handle target reappearance issues in long-term tracking in the wild.

A target regression network with target-aware feature fusion is designed, which is robust to target appearance variations and improves tracking efficiency by precisely narrowing target search space.

A target verification network is developed, which is learned online to capture target appearance changes and leverages a monitoring module to alleviate noisy training samples, preventing long-term model degeneration.

Our method has set new record on the VOT2018 long-term challenge and achieve state-of-the-art performance on OxUvA long-term benchmark. Ablative studies have also verified the effectiveness of our major contributions.

Related Work
Short-Term Deep Trackers
Recent deep trackers (Nam and Han 2016; Danelljan et al. 2016; Sun et al. 2018a, b; Danelljan et al. 2017; Li et al. 2018) have achieved promising results in short-term sequences, which are usually categorized into either matching-based (Bertinetto et al. 2016b; Tao et al. 2016; Li et al. 2018) or classification-based (Wang et al. 2015, 2016; Nam and Han 2016; Danelljan et al. 2016, 2017; Sun et al. 2018b, a; Danelljan et al. 2015b) ones. The former ones attempt to train generalized deep neural networks offline, which find the best candidate being most similar to the target template in each frame. However, their outputted similarity scores cannot indicate the presence of the target, which is important in long-term tracking, but can still achieve competitive performance in the short-term task with high speed. In contrast, the classification-based trackers learn discriminative correlation filters (Danelljan et al. 2016, 2017; Sun et al. 2018a, b; Danelljan et al. 2015b) or CNN-based classifiers online (Wang et al. 2015, 2016; Nam and Han 2016) to distinguish the target from the cluttered background. The outputs of these methods are usually used for judging whether it is suitable to collect training samples in certain frames and cannot indicate target disappearances either. Generally, the offline-trained matching models are efficient but not able to well adapt to online variations. The online-updated classifiers have powerful discriminative abilities but are sensitive to drastic appearance changes. Some works (Fan and Ling 2017; He et al. 2018) have combined both two networks and achieved high performance in short-term scenarios; however, they cannot work well for long-term tracking (Lukežič et al. 2018) due to the limitation of their frameworks and re-detection schemes. Inspired by these works, we develop a novel long-term tracking framework that utilizes the advantages of both matching and classification models to tailor to long-term challenges.

Long-Term Tracking
Until now, few works have been proposed for long-term tracking (Lukežič et al. 2017; Ma et al. 2015; Fan and Ling 2017; Hong et al. 2015; Nebehay and Pflugfelder 2015; Zhu et al. 2018b; Fan and Ling 2017). Most previous works utilize hand-crafted features and fail to achieve satisfactory performance. CMT (Nebehay and Pflugfelder 2015) tracker merely conducts key points matching for long-term tracking, which is sensitive to distractions. The LCT (Ma et al. 2015) and PTAV (Fan and Ling 2017) trackers are equipped with the re-detection scheme for long-term tracking, but they merely track the targets in a local search region to expect that the lost targets will reappear around the previous location. Thus, these two methods are not able to recapture the target after it moves out of view. FCLT (Lukežič et al. 2017) learns correlation filters online and gradually increases the search range with time, but its performance is still far from state-of-the-art. MUSTer (Hong et al. 2015) exploits different models for short-term and long-term scenarios separately, so it fails to fully utilize online information and cannot achieve competitive performance. DaSiamRPN (Zhu et al. 2018b) achieves promising results on long-term benchmarks, while it needs a large amount of video data for offline training (more than 400, 000 videos). Different from previous trackers, in our work, both models benefit from offline learning and have the ability of recapturing the target by global search after target disappearances. As a result, the proposed framework can achieve competitive performance with a moderate amount of video data for offline learning (around 4,417 videos).

Training Sample Selection
SRDCF (Danelljan et al. 2015a) assigns greater weights to recent samples when learning the filter. The GMM of ECO (Danelljan et al. 2017) divides positive samples into several groups by their appearances. It aims to increase the variety of training samples and avoid over-fitting. Besides, LCT (Ma et al. 2015) just sets thresholds manually to select training samples, and TLD (Kalal et al. 2012) separates positive and negative samples merely by the trajectory of the target. Using such samples, which may include contaminated ones, can easily cause drift problems. BranchOut (Han et al. 2017) trains multiple classifiers using different samples to reduce the negative impact of inaccurate samples. However, this way still can introduce errors when there are inaccurate target localization results in consecutive frames. To alleviate the error accumulation problem, in our work, the proposed monitoring module removes noisy samples effectively based on both historic and current target appearance information.

Fig. 2
figure 2
Proposed long-term tracking approach. Our method contains four components (1) regression network, (2) verification network, (3) monitoring module and (4) tracking mode switching scheme. The regression network first detects target candidates with high confidences to narrow the target search space. Then, the verification network identifies the target from the detected candidates and is updated under the supervision of the monitoring module. Finally, the learning-based switching scheme decides whether triggering a global search mode or not, if target disappearance happens

Full size image
Proposed Tracking Approach
Overview
The proposed long-term tracking method consists of a regression network  and a verification network  guided by a switching scheme. The tracking pipeline is overviewed in Fig. 2 and explained as follows.

Candidate Regression To leverage the efficiency and generalization ability of template matching based approaches, we train the regression network  offline to match appearances of generic objects. During online tracking, the regression network  takes as input a search region from the current frame and a target template from the first frame, and densely measures the target similarity of each sub-region in the search region. Finally, bounding box regression is performed to produce a set of target candidates of high confidences. A large portion of unlikely target candidates can be eliminated, reducing unnecessary computational overhead.

Monitoring Based Verification In order to select the target from potential candidates, the verification network  is updated online to capture target appearance. It learns to assign each target candidate with a target probability, based on which the final target is chosen from candidate proposals. The online training of verification network leverages a monitoring module  to remove erroneous training samples, preventing model degeneration caused by noise accumulation.

Tracking Mode Switching Scheme To better handle target disappearance and reappearance issues, we further propose a tracking mode switching scheme based on a decision network. It infers the current status (either presence or absence) of the target according to the tracking results and switches the tracking mode between local and global search. As a result, tracking can be immediately recovered after target reappearance.

Regression Network 
The architecture design of the regression network  is inspired by recent anchor-based object detection methods (Liu et al. 2016). As shown in Fig. 2 it consists of two feature extraction branches, both of which are initialized with pre-trained MobileNet (Howard et al. 2017) parameters. The upper branch takes as input the local search region which is centered at the target location in the last frame and is four times larger than the target bounding box. It generates multi-scale feature maps (two scales of 19×19 and 10×10 in our experiments) to tackle target scale variations. Meanwhile, the lower branch takes the target template (the ground truth bounding box region given in the first frame) as input and generates a feature map with a spatial size of 4×4. The two branches have the same architecture with untied network parameters to ensure more flexibility in characterizing template and search space. We then combine the features of the template and search region using a target-aware fusion scheme. The fused features serve as the input to a matching-based region proposal network (mRPN) to detect potential target candidates. Following most object detection methods, Non-maximum-suppression (NMS) is finally performed to further refine the target candidates.

Target-Aware Feature Fusion Let us consider the feature map of the search region with a spatial size 19×19 as an example, while the fusion process for other feature scales is performed in a similar way. Denote the extracted feature map of the search region and target template as 𝐅𝑠∈ℝ19×19×𝐶 and 𝐅𝑡∈ℝ4×4×𝐶, respectively, where C indicates the number of channels. To enable an efficient dense matching between the target template and each sub-region within the search region, we process the target feature map 𝐅𝑡 in a squeeze-and-replicate manner. Specifically, we first squeeze the target feature map into a spatial size of 1×1 through a 4×4 average pooling, then replicate it 19 times both horizontally and vertically, producing a target feature map 𝐅̃ 𝑡∈ℝ19×19×𝐶. To combine the two feature maps 𝐅𝑠 and 𝐅̃ 𝑡, we firstly multiply them element-wisely to highlight target-correlated features in the search space, then concatenate the two feature maps to form the fused results 𝐅∈ℝ19×19×2𝐶. This process can be expressed as follows:

𝐅=[𝐅̃ 𝑡⊙𝐅𝑠|𝐅̃ 𝑡],
(1)
where ⊙ indicates element-wise multiplication and [⋅|⋅] denotes the concatenation of two feature maps along the channel dimension. Here, the multiplication operation, acting as an attention mechanism, is used to highlight the common and salient responses existing in both 𝐹̃ 𝑡 and 𝐅𝑠, while suppresses those that only appear in one of the feature maps and are more likely to be noises. The concatenation operation further preserves the information of the target as a reference for the following layers. As a result, the 2C dimensional feature vector in each spatial location of the fused feature map 𝐅 encodes the information of both the target and the corresponding sub-region in the search region with highlighted similar features, hence the name target-aware fusion. Then, dense template matching can be efficiently achieved using convolutional architectures.

Matching-Based Candidate Proposal In order to infer the target location from the downsampled feature maps, we decompose the template matching process into two subtasks: target similarity estimation and bounding box regression. Following anchor-based object detection methods (Liu et al. 2016), we associate each spatial location with k anchor boxes of multiple scales and aspect ratios. The target similarity estimation of each anchor box is achieved by predicting its similarity and dissimilarity scores to the target. For bounding box regression, we predict the offset between the target location and each anchor box.

The above template matching process is implemented by a matching-based region proposal network (mRPN) consisting of 3 convolutional layers. It takes the fused feature map as input, and generates an output of size 𝐻×𝑊×6𝑘, where 𝐻×𝑊 denotes the spatial feature size (either 10×10 or 19×19), and the 6k output channels correspond to the bounding box regression (4k) as well as similarity estimation results (2k) for the k anchors in each location. We build two mRPN networks for two feature scales of the search region, respectively. The parameters of mRPN are trained offline and fixed during online tracking. We employ the binary cross-entropy loss and smooth 𝐿1 loss (Liu et al. 2016) for training similarity estimation and bounding box regression, respectively.

Discussion The specific architecture design of our regression network is inspired by anchor-based object detection techniques. However, different from object detection methods that aim to locate generic objects of predefined categories, our regression network with two input branches and a target-aware feature fusion scheme enables specific target detection through efficient dense template matching. Our method also differs from existing SiameseRPN based trackers (Li et al. 2018; Zhu et al. 2018b) in terms of target searching. In these approaches, the spatial information of target appearance are retained in the target template, which may hinder their ability to accurately locate targets under non-rigid deformations. In contrast, we circumvent this issue by encoding target appearance in 1×1 feature maps which are more flexible to model appearance deformations. Besides, we further propose a verification network to generate target proposals with high confidences from potential candidates. Consequently, the integration of matching-based and classification-based approaches together with the tracking mode switching scheme ensures more superior long-term tracking performance, which has not been explored in SiameseRPN-based methods.

Verification Network
The offline trained regression network is fixed during online tracking and takes the initial target appearance as the target template. For one thing, it can narrow down target search space by eliminating most background regions. For another, not all the detected target candidates are accurate since the offline trained network sometimes may fail to discriminate the target from various backgrounds, especially when there are similar objects co-existing. As a result, the regression network is easy to drift to distractors. Based on these two observations, we propose to combine the regression network with a classification-based verification network  to determine the final target location from a set of candidate proposals. Due to its successful application to short-term tracking (Nam and Han 2016; Han et al. 2017), we adopt the pre-trained VGGM (Simonyan and Zisserman 2014) model as the backbone of our verification network. It takes each target candidate region as input and adopts a softmax classifier as the output layer to distinguish the real target from background regions.

The classification based model exhibits stronger discriminative powers but is also computationally more intensive. Different from existing methods (Nam and Han 2016), which directly apply the classifier to the entire search region in a patch-by-patch manner, our verification network operates on a manageable number of highly confident candidates, thus effectively lowering the overhead.

Another advantage of our verification network is the monitoring module for online training sample selection. Training samples may be easily contaminated due to inaccurate tracking results, target occlusion, disappearances, etc. As confirmed by Kalal et al. (2012), the contaminated training samples may be fatal for long-term tracking, causing degenerated trackers, since slight inaccuracies can be accumulated over time and lead to tracking drifts. Therefore, we introduce a monitoring module to largely alleviate this issue. The details are as follows.

Monitoring Module for Sample Selection Since the online update of tracking methods are mostly performed using sampled tracking results as pseudo ground truth, it is critical to ensure the quality of sampled results in order to prevent model degeneration. To this purpose, we propose to monitor online training sample selection by measuring the appearance consistency between current and historical tracking results. As such, inconsistent tracking results, which likely to be erroneous or contaminated, will be abandoned during online update. Given the averaged feature map of the historical tracking results as the positive sample and those of the background regions as the negative sample, we model the consistency of historical target appearance by optimizing the following objective function:

𝐸(𝐖,𝐘)=||𝐅+∗𝐖−𝐘||22−𝜆1||𝐅−∗𝐖−𝐘||22+𝜆2||𝐖||22+𝜆3||𝐘||22,
(2)
where 𝐅+ and 𝐅− denote the averaged feature maps of historical tracking results and background regions, respectively; 𝐖 represents a 2-D convolutional kernel, and 𝐘 denotes a label map. In our experiments, we use the third convolutional feature maps generated by the verification network  to characterize the target and background (i.e., 𝐅+ and 𝐅−). The feature maps, filter and label map have the same size of 3×3×512.

The first two terms of (2) can be interpreted as a contrastive loss to identify the differences between target and background features, while the last two terms are penalties imposed on the filter and label maps. An alternative optimization algorithm is developed to learn filter 𝐖 and label map 𝐘 by minimizing (2). Formally, by fixing 𝐘, the filter has the following closed form solution in the Fourier domain.

𝐖̂ =[(𝐅̂ +)∗−𝜆1(𝐅̂ −)∗]⊙𝐘̂ (𝐅̂ +)∗⊙𝐅̂ +−𝜆1(𝐅̂ −)∗⊙𝐅̂ −+𝜆21,
(3)
where 𝐅̂ + denotes the Fourier transform of 𝐅+, and (𝐅̂ +)∗ indicates its conjugate. By fixing 𝐖, we can also obtain the Fourier domain solution of the label map 𝐘 as follows:

𝐘̂ =𝐅̂ +⊙𝐖̂ −𝜆1𝐅̂ −⊙𝐖̂ 1−𝜆1+𝜆3.
(4)
Our experiments show that the optimization method can achieve a desirable solution with less than 10 iterations. More detailed derivation can be found in the supplementary material.

The learned filter 𝐖 highlights the distinction between target and background, while the learned label map 𝐘 encodes the unique but consistent feature of the target. Given the feature map 𝐅 of the current tracking result, we measure its consistency score as follows:

𝑠𝑐=||𝐅∗𝐖−𝐘||22||𝐅+∗𝐖−𝐘||22.
(5)
If the consistency score is less than a threshold 𝛿 (2.0 in our experiments), the current tracking result is deemed as unreliable and abandoned during training the verification network. Otherwise, we use this tracking result as a positive sample to update the verification network and learn 𝐖 and 𝐘 for consistency check in the subsequent frames.

Learning-Based Switching Scheme
Judging the status (presence/absence) of the target accurately is essential for long-term scenarios. For our algorithm, one simple approach is to set rigid thresholds for the output scores of the best candidate from both regression and verification networks. When the output scores are below the thresholds or do not satisfy certain constraints, the target is regarded as missing and then global search is triggered. However, we empirically observe that such solution requires burdensome hyperparameter adjustment.

To simplify the procedures for combining the scores from different networks, which are vital to handle the target disappearance and reappearance issues, we design a simple but effective decision network . It explicitly infers the status of the target by jointly considering the prediction confidence of the regression and verification networks. To be specific, the decision network consists of three fully connected layers followed by an output sigmoid layer. Given the current tracking result,  net takes as input its target similarity score and target probability estimated by the regression and verification networks, respectively, and predicts a probability P of the target being present at the camera view.

During offline training, we first run the regression and verification networks throughout a training video sequence to calculate the target similarity score and target probability for each frame. We collect these calculated results of a series of videos and use them as inputs along with the status (presence/absence) of the targets as the ground truths to learn the decision network.

During online tracking, we apply the pre-trained decision network to each frame. If the predicted probability is higher than a predefined threshold 𝜏, we perform local search (i.e., only consider the search region around the target location) in the next frame. Otherwise, the global search mode is triggered in the next frame, where we divide the whole input image into sub-regions in a sliding window manner. We perform the above target search on each sub-region. As a consequence, the target can be immediately re-detected after reappearance.

Discussion For each video, the frequency of global search depends on how often the target disappears. Since we divide the input frame into patches according to the size of the target observed before disappearance, the computation complexity of global search is related to the aspect ratio between the size of the whole image and the target, and is seven times on average than the local mode tested on LTB35 dataset (Lukežič et al. 2018).

Implementation Details
Training Regression Network Offline When training  net, sampled pairs are selected from both ILSVRC (Russakovsky et al. 2014) image and video object localization datasets with a random interval. For the former dataset, we train  for target bounding box regression in a class-agnostic manner. To be specific, we choose an object of interest from an image randomly and crop its surrounding region. For the video object localization dataset, the similarity calculation branch of  further learns a generic matching function for tracking to tolerate the general appearance variations. The regression network is trained in an end-to-end manner using stochastic gradient descent. To train  more effectively, some data augmentations are adopted including affine transformation and random erasing (Zhong et al. 2017). In long-term tracking, since the target disappears frequently and its size often changes dramatically when it re-appears, we adopt shift-invariant anchors in the proposed mRPN net for two feature scales with different aspect ratios [0.33, 0.5, 1, 2, 3]. During training, we define the anchors which have 𝐼𝑜𝑈>0.7 with ground truth as positive samples and those satisfying 𝐼𝑜𝑈<0.5 as negative ones. We also sort negative samples using the confidence loss of each default box and pick the top ones so that the ratio between the negatives and positives is at most 3 : 1. The regression network is trained offline for 500K iterations, with an initial learning rate of 10−2 and a batch size of 32 containing samples from both datasets.

Decision Network We run the  and  net on the validation set of VID dataset to collect training samples for offline training the decision network. The  net is updated continuously in each video following Nam and Han (2016). The collected training data consists of 790K training sample pairs, where the target is present in 460K samples and absent for the rest 330K samples. We train the decision network for 20, 5, and 2 epochs with learning rates 1e–2, 1e–3, and 1e–4, respectively, and a batch size of 128. During tracking, 𝜏 is set to 0.3 for the LTB35 dataset and 0.41 for the OxUvA dataset.

Online Tracking During online tracking, the parameters of  and  are fixed. The target template for matching is the groundtruth given in the first frame. We only update  in a similar way to (Nam and Han 2016) using both positive and negative samples, where image patches with >0.7 IoU overlap with the tracking results are treated as positive samples and those with <0.5 IoU are negative ones. To be specific, the parameters of the first three convolution layers are fixed and we fine-tune the last three convolution layers online. We train the forth and fifth layers with the the learning rate 10−3 and the last layer with 10−2. We crop out 500 positive samples and 5000 negative samples to train 30 iterations during initialization, 50 positive samples and 200 negative samples for the 15 iterations during each update. The regularization parameters of Eq. 2 are empirically chosen as 𝜆1=10−5, 𝜆2=1.0 and 𝜆3=10−3 for the LTB35 dataset and 𝜆1=10−5, 𝜆2=0.94 and 𝜆3=0.0012 for the OxUvA dataset.

Experiments
Our tracker is implemented using Tensorflow (Abadi et al. 2016) on a PC with an Intel i7 CPU (32G RAM) and a NVIDIA TITAN X GPU. The average tracking speed is 5fps. the verification network with RT-MDNet (Jung et al. 2018) results in a faster version of 20 fps. The source code will be released upon acceptance. We evaluate the proposed method on the VOT-2018 LTB35 dataset (Lukežič et al. 2018) and OxUvA Long-term dataset (Valmadre et al. 2018). The detailed comparisons are presented as follows.

Fig. 3
figure 3
Qualitative results of our tracker, along with DaSiam_LT and MMLT (the top-ranked trackers reported in Kristan et al. (2018)) on three challenging sequences. From top to bottom: skiing, cat1, group1. The confidence score of each tracker is shown in the top of each image. We use “absent” to denote frames in which the target is absent. Our method outputs reliable confidence score indicating the presence of the target and can successfully re-capture the target under scale variations. Best viewed in color with zoom-in for more details

Full size image
Table 2 Performance evaluation for 15 state-of-the-art algorithms on the VOT-2018 LTB35 dataset (Lukežič et al. 2018)
Full size table
Evaluation on VOT-2018 LTB35 Dataset
The VOT-2018 LTB35 dataset (Lukežič et al. 2018) is presented in Visual Object Tracking (VOT) challenge 2018 for evaluating long-term trackers, which includes 35 challenging videos of various objects (e.g., persons, car, motorcycles, bicycles and animals) with a total frame length of 146,847 frames. Each contains on average 12 long-term target disappearances, and each disappearance lasts on average 40 frames. Besides, it also includes a re-detection experiment to further compare different trackers, including the average number of frames required for re-detection (Frames) and the percentage of sequences with successful re-detection (Success). Therefore, this dataset can fully evaluate trackers’ long-term tracking capabilities, which are essential for tracking in the wild.

Following the evaluation protocol of VOT-2018 (Kristan et al. 2018), the tracking precision (Pr), recall (Re) and F-score metrics are utilized for accuracy evaluation. Based on the precision and recall, the threshold F-measure 𝐹(𝜏𝜃) is defined as

𝐹(𝜏𝜃)=2𝑃𝑟(𝜏𝜃)𝑅𝑒(𝜏𝜃)/(𝑃𝑟(𝜏𝜃)+𝑅𝑒(𝜏𝜃)),
(6)
where 𝜏𝜃 is the threshold. Then, the F-score is defined as the highest score on the F-measure plot over all thresholds 𝜏𝜃 (i.e., taken at the tracker-specific optimal threshold). This manner avoids arbitrary manual-set thresholds and then encourages fair evaluation. In VOT-2018 LTB35 (Lukežič et al. 2018), the F-score is the primary long-term tracking measure and is used for ranking different trackers. In addition, the results of the re-detection experiment are adopted to compare different trackers, including the average number of frames required for re-detection (Frames) and the percentage of sequences with successful re-detection (Success).

The detailed comparisons are reported in Table 2. We denote our tracker by Ours and the faster version by Ours*. From Table 2, we can conclude that our tracker achieves the top-ranked performance in terms of F-score, Pr and Re criteria while maintaining a high re-detection success rate. Especially, the proposed method obtains the best performance among all compared trackers in terms of the F-score measure, which is the most important metric on this dataset. SPLT (Yan et al. 2019) and DaSiam_LT are two recent state-of-the-art long-term trackers and have high speeds. However, they achieve much worse F-scores (i.e. 0.616 and 0.607) than our tracker (0.631), and the success rate of DaSiam_LT is almost zero, which means the DaSiam_LT tracker fails in the re-detection experiment. In contrast, the proposed method achieves a success rate of 100%. Notably, our tracker achieves much higher performance when using a moderate amount of video training data, while DaSiam_LT uses nearly 100 times video data than us. This is because our proposed updating scheme is effective and can greatly reduce the burden of offline training and large-scale datasets. To visualize the superiority of our tracker, we provide representative results of our tracker and other two top-ranked methods reported in Kristan et al. (2018), along with their confidence scores on challenging sequences. As shown in Fig. 3, the targets disappear frequently in these videos. Our tracker outputs a very low confidence score when the target is absent, while other trackers still give relatively higher confidence scores and drift to distractions (skiing and cat1). Thus, the confidence scores outputted by our tracker are able to indicate the presence of the tracked object accurately and verify the effectiveness of the learned decision network. Besides, as shown in “ballet”, “skiing” and “cat1”, DaSiam_LT fails to locate the target or regress a tight bounding box when the target reappears due to the scale variations, while our tracker performs stably in such situations. This proves the advantages of our target-aware fusion method ( only selects the best candidate box regressed by  and cannot handle scale and shape variations).

Evaluation on OxUvA Long-term Dataset
Table 3 Detailed comparisons of different trackers on the OxUvA dataset (Valmadre et al. 2018)
Full size table
The OxUvA long-term dataset (Valmadre et al. 2018) comprises 366 object tracks in 337 videos, which are labelled at a frequency of 1Hz, selected from YoutubeBB (Real et al. 2017) dataset. The tracklets are labelled at a frequency of 1Hz which is the same as YoutubeBB (Real et al. 2017). Each video lasts for average 2.4 minutes, seven times longer than OTB-100 (Wu et al. 2015) (the most popular short-term tracking dataset). Besides the challenging factors in short-term tracking, severe out-of-view and full occlusion introduce extra challenges which are close to practitioners’ needs. The OxUvA dataset is split into dev and test sets with 200 and 166 tracks respectively. Using these subsets, two challenges have been defined: constrained and open. For the constrained challenge, trackers can be developed using only data from OxUvA dev set (long-term videos). For the open challenge, trackers can use any public dataset for training except for the YoutubeBB validation set, from which OxUvA is constructed. Since all the trackers reported in OxUvA (Valmadre et al. 2018) are tested for the open challenge, which allows for extra training datasets, we also conduct experiment on this set (166 tracks) for fair comparison. Following the evaluation benchmark, we adopt the true positive rate (TPR), true negative rate (TNR), and maximum geometric mean (MaxGM) to compare different trackers. TPR measures the fraction of present objects that are reported as present, and TNR gives the fraction of absent objects that are reported as absent. The MaxGM is defined as,

𝐌𝐚𝐱𝐆𝐌=max0≤𝑝≤1((1−𝑝)⋅𝐓𝐏𝐑)((1−𝑝)⋅𝐓𝐍𝐑+𝑝)‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√,
(7)
which provides a more informative comparison of different trackers and is used to rank them. A larger MaxGM value means a better performance. Since the labels of its test data are not publicly available, we cannot calculate the performance using the same evaluation metric used in LTB35 dataset (Lukežič et al. 2018). Thus, for this dataset, we report its proposed MaxGM, TPR and TNR metrics.

We compare our tracker with one recent tracker SPLT (Yan et al. 2019) and ten competing algorithms reported in (Valmadre et al. 2018), including LCT (Ma et al. 2015), EBT (Zhu et al. 2016), TLD (Kalal et al. 2012), ECO-HC (Danelljan et al. 2017), BACF (Galoogahi et al. 2017), Staple (Bertinetto et al. 2016a), MDNet (Nam and Han 2016), SINT (Tao et al. 2016), SiamFC (Bertinetto et al. 2016b) and SiamFC+R (Valmadre et al. 2018). Among these trackers, SPLT (Yan et al. 2019), LCT (Ma et al. 2015), EBT (Zhu et al. 2016) and TLD (Kalal et al. 2012) have different re-detection schemes for long-term tracking. SiamFC+R (Valmadre et al. 2018) is implemented by equipping SiamFC with a sample re-detection scheme similar to Steven Supancic III and Ramanan (2017). The remaining ones are popular algorithms for short-term tracking.

The detailed comparisons are reported in Table 3. Ours* here indicates the faster version. We can see that our tracker achieves competitive performance in terms of MaxGM (the most important metric) and TPR. Although SPLT (Yan et al. 2019) achieves the highest MaxGM, they use our regression network as the candidate detector, which indicates that our contribution is effective and can be embedded into other tracking algorithms. Besides, SPLT (Yan et al. 2019) has very low TPR compared to our method, as our global search mode can be triggered in time by the proposed switching mechanism to recapture the target effectively.

Table 4 Ablation analysis of the proposed tracker on the VOT-2018 LTB35 dataset
Full size table
Ablation Study
Different Components We conduct ablation analysis to evaluate different components of our tracker. We design four variants, which are respectively named as “”, “+”, “++”, “+++”. The meanings of these notions are explained as follows. (1) “” denotes the tracker that only uses  for tracking, and the target is localized as the bounding box with the highest score predicted by , which is also used as the confidence score 𝑆𝑐, in each frame. (2) “++” and “+” indicate two variants of our tracker using  and  with and without the monitoring module. Their 𝑆𝑐 values are predicted by setting a set of pre-defined thresholds and constraints based on 𝑆𝑟 and 𝑆𝑣 manually through greedy search and burdensome adjustment. (3) “+++” denotes the tracker that has all the components we propose with the confidence score being estimated by the proposed decision network.

The results of four variants on LTB35 (Lukežič et al. 2018) are presented in Table 4, from which we can see that all components facilitate improving the long-term tracking performance. First, the performance of “+” is remarkably better than “” and “” and proves that  and  are complementary.  can propose tight bounding boxes of the target, while  randomly samples candidates introducing error. Since when the target’s appearance changes dramatically,  may not give the target the highest response among the candidates but  can give right results(like “skiing” in LTB35). Besides, it is hard for  to do global search by randomly sampling proposals, while  takes each region of the whole image as input and outputs promising candidates. Meanwhile,  conducts similarity measure and is fixed online so it easily drifts due to cluttered background, which can be partially addressed by . Second, the comparison of the “+” and “++” demonstrates the monitoring module of  boosts the robustness of our tracker in long-term tracking significantly. Third, comparing “+++” with “++”, we can conclude that our learning-based strategy works well to make the tracker switch between local search and global search while eliminates the need for burdensome adjustment of thresholds.

Table 5 Ablation analysis of the architecture of  on the VOT-2018 LTB35 dataset
Full size table
Fig. 4
figure 4
Qualitative results of using different fusion methods for . The yellow, red, blue and green boxes denote the groundtruth,  using the proposed target-aware fusion method,  using shared parameters, and  using shared parameters with correlation operation for fusion respectively. It is clear that the two variations of  cannot adapt to scale and shape changes well

Full size image
The Architecture Design of  We also analyze the architecture design of , as shown in Table 5. (1) “ w/o Concatenation” and “ w/o Multiplication” stand for different operations used in the fusion procedure of . The former one represents the tracker that only uses the result of multiplication without being concatenated with the feature of the template. The latter one denotes the fusion method that directly concatenates the feature of the search region with that of the template without multiplication. (2) “ w/ Sum” and “ w/ Subtraction” represent for replacing the multiplication operation with sum and subtraction. (3) “ (Siamese)” indicates the two branches of the feature extractor of  share the same parameters. (4) “ (SiameseRPN-Fusion)” denotes the fusion method used in  is the same as that of SiameseRPN (i.e. correlation).

Table 5 shows the performance of  under different settings and the score predicted by  is also directly used as the confidence score. Obviously, comparing “” with “ w/o Concatenation” and “ w/o Multiplication”, we can conclude that both concatenation and multiplication operations are essential in the feature fusion module. The performances of “ w/ Sum” and “ w/ Subtraction” further demonstrate that multiplication is the best choice for highlighting candidate regions, because sum operation cannot suppress features that are salient in only one branch and subtraction reduces the responses of similar features in both branches. The comparison between “” and “ (Siamese)” shows that the Siamese architecture (i.e., two branches share the same parameters) leads to inferior performance in long-term tracking. Adopting correlation operation for the fusion procedure of  as in Zhu et al. (2018a) cannot work well in long-term tracking (an F-score of only 0.404) when using only around 4, 417 video training data. In contrast, the proposed  achieves competitive performance with an F-score of 0.525. Some qualitative results are shown in Fig. 4.

Table 6 Ablation analysis of the number of iterations for learning  on the VOT-2018 LTB35 dataset
Full size table
The Number of Iterations for Learning the Monitoring Module Since Eq. (2) is not strictly convex, 𝐖 and 𝐘 are possibly to be close to 0 during learning. In the experiment, we find that getting meaningless 𝐖 and 𝐘 needs more than 100 steps of iteration. Besides, we also study the performance of different numbers of iterations by testing the F-scores of “++”. The results are shown in Table 6. In addition, we manually set 𝐖 and 𝐘 to 0 and we find that there is not any improvement over “+” (0.610). From these experimental results, we can conclude that the monitoring module can learn stably and improve the accuracy of the tracker, even though it is not convex strictly.

Conclusion
In this paper, we propose a novel framework for long-term tracking in the wild to explore the strength of both matching and classification based models. The offline trained regression network, which is equipped with an object-aware feature fusion mechanism for robust target template matching, can deal with various target appearance variations and detect target candidates with accurate bounding box predictions. The online learned verification network identifies the target from potential candidates through classification, with a novel monitoring module filtering out noisy samples for update to avoid error accumulation. Guided by the learning-based switching scheme, the proposed tracker switches between local global search dynamically to handle target disappearance and reappearance issues. Evaluations on two long-term tracking in the wild benchmarks demonstrate the effectiveness of our method. We believe that our tracker can act as a new baseline for further long-term tracking studies.

