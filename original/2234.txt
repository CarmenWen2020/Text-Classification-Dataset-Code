Dual-view snapshot compressive imaging (SCI) aims to capture videos from two field-of-views (FoVs) using a 2D sensor (detector) in a single snapshot, achieving joint FoV and temporal compressive sensing, and thus enjoying the advantages of low-bandwidth, low-power and low-cost. However, it is challenging for existing model-based decoding algorithms to reconstruct each individual scene, which usually require exhaustive parameter tuning with extremely long running time for large scale data. In this paper, we propose an optical flow-aided recurrent neural network for dual video SCI systems, which provides high-quality decoding in seconds. Firstly, we develop a diversity amplification method to enlarge the differences between scenes of two FoVs, and design a deep convolutional neural network with dual branches to separate different scenes from the single measurement. Secondly, we integrate the bidirectional optical flow extracted from adjacent frames with the recurrent neural network to jointly reconstruct each video in a sequential manner. Extensive results on both simulation and real data demonstrate the superior performance of our proposed model in short inference time. The code and data are available at https://github.com/RuiyingLu/OFaNet-for-Dual-view-SCI.

Access provided by University of Auckland Library

Introduction
As a well-developed technique, compressive sensing (CS) (Donoho 2006; Emmanuel et al. 2006) based computational imaging systems (Mait et al. 2018) have attracted much attention in recent years. These systems usually employ low-dimensional sensors to capture high-dimensional signals. Snapshot compressive imaging (SCI) (Yuan et al. 2021) is one of the promising applications of computational imaging (Llull et al. 2013; Wagadarikar et al. 2008), which utilizes a two dimensional (2D) camera to capture the 3D video or spectral data, aiming to provide an effective optical encoder for compressing the video or spectral data-cube during capture. Different from conventional cameras, such imaging systems encode the consecutive video frames (Reddy et al. 2011; Yuan et al. 2014) or spectral channels (Wagadarikar et al. 2009; Miao et al. 2019) by applying different coding patterns to each data volume along time or spectrum to obtain the final compressed measurements. With such cameras, SCI systems (Hitomi et al. 2011; Llull et al. 2013; Reddy et al. 2011; Wagadarikar et al. 2008, 2009) can encode multiple frames into a single captured image (called measurement in this work) with low-memory, low-bandwidth, low-power and potentially low-cost, which is decoded later using different reconstruction algorithms.

While previous video SCI systems only capture one scene within the field-of-view (FoV) through spatial multiplexing, extending the spatial multiplexing capability to more than one additional dimensions, i.e., multi-view imaging, is able to decrease the memory, bandwidth as well as the latency (Qiao et al. 2020a), which is necessarily required for the emerging applications such as robotics, traffic surveillance, sports photography and self-driving cars (Lu et al. 2020; Qiao et al. 2020a). Recently, the snapshot spatial-temporal compressive imaging (SSTCI) system implemented in (Qiao et al. 2020a) has achieved spatial (joint FoV) and temporal CS in a snapshot by using a single coding device, e.g., the digital micro-mirror device (DMD), plus elegant simple optical designs without compromising spatial resolution, i.e., each FoV is of full resolution. The underlying principle of SSTCI is masking various scenes with different coding patterns along time to obtain the final compressed measurements. This joint CS sheds lights on the real applications of SCI system on robotics since multi-view is an inherent requirement in machine vision. For efficient dual-view video reconstruction, a plug-and-play framework with deep denoising priors (PnP-TV-FFD) was proposed in Qiao et al. (2020a). However, one bottleneck of this method is the slow reconstruction speed and poor reconstruction quality that preclude the wide applications of SSTCI. Inspired by this novel hardware design and aiming to address the challenges of reconstruction speed and quality, we intend to develop an effective and efficient reconstruction algorithm for SSTCI. Though our long-term goal is multi-view, in this paper, we focus on the dual-view case based on the SSTCI prototype without the loss of generality.

For fast inference, one straightforward way is to train an end-to-end network based on deep learning for the inversion problem (Cheng et al. 2020; Ma et al. 2019; Qiao et al. 2020b). Yet, these networks were developed for the single-view SCI, and extending the single-view SCI inversion techniques to the muti-view reconstruction directly is non-trivial because they usually fail to clearly reconstruct each individual scene from the single compressed measurement. It is worth noting that in the SSTCI hardware design, though the coding pattern sets are different for the two FoVs, they are correlated as one set is a shifted version of the other one (Qiao et al. 2020a). Inspired by this, we propose a diversity amplifier in this paper to facilitate the distinction of two scenes by taking the different (shifted) coding patterns into consideration, which is implemented in conjunction with a dual-net separator with two branches for reconstructing each video frames respectively. By this novel design, the two FoV scenes can be well distinguished. Nevertheless, it neglects the inherent temporal correlation within each video, leading to limited reconstruction quality. To address this, we further introduce optical flow to understand the video contents for vision tasks in our network design, which is able to investigate the motion information by encoding the velocity and direction of each pixel’s movement between neighboring frames. Furthermore, the extracted optical flow are explored bidirectionally and then fed into a recurrent neural network (RNN) to exploit temporal correlations between adjacent frames, resulting in reconstructed videos with more smoothness across time and less artifacts. It is essential to know both the object status (e.g., captured by the RNN cell) and motion information (e.g., optical flow), for better understanding the video contents of vision tasks.

Contributions of This Paper
In a nutshell, we develop an Optical Flow-aided Recurrent Neural Network (OFaNet) for dual-view video SCI. Specific contributions are summarized as follows:

An end-to-end deep learning based reconstruction regime is proposed for dual-view video SCI reconstruction, where a diversity amplifier and dual-net separator are constructed to separate the two FoVs from a single measurement, and then a refine net equipped with the bidirectional optical flow is developed to improve the reconstruction quality in a sequential manner.

The bidirectional optical flow is elaborately integrated into the video SCI by exploiting explicit motion information between adjacent frames, including both the global and locally varying motions, acting as the guidance for improving video reconstruction. To the best of our knowledge, this is the first time that optical flow is introduced into video SCI problems, which is jointly optimized under the supervision of mean square error (MSE) loss of reconstruction results to better match the video SCI problem.

We apply our developed algorithms to extensive simulation and real datasets (captured by the SSTCI camera) to verify the effectiveness, efficiency and robustness of the proposed algorithm. Experimental results show superior performance and robustness with much shorter inference time (about 40,000 times speedups over DeSCI) compared with previous state-of-the-art methods.

We also extend the proposed OFaNet to the single-view SCI system on both simulation and real datasets with small modifications to enlarge the application scope. The reconstruction performance is comparable with the state-of-the-art methods for single-view video SCI.

Related Work
Single-View SCI
For temporal CS, SCI system is built as an optical encoder, by compressing the videos during capture. The modulation methods can be categorized into physical mask (Llull et al. 2013; Yuan et al. 2014) and spatial light modulator (SLM) including DMD (Qiao et al. 2020b; Reddy et al. 2011; Sun et al. 2017). Given the measurement and coding patterns, an efficient decoder is required for reconstructing the time series of video frames, which has been extensively developed before (Yuan 2016; Liu et al. 2019; Yang et al. 2014; Xu and Ren 2016; Yuan et al. 2020, 2021; Meng et al. 2020).

For the software decoder, iterative optimization based methods were proposed including GAP-TV (Yuan 2016, 2020), GMM (Yang et al. 2014), DeSCI (Liu et al. 2019), utilizing different priors such as total variation, low rank, sparsity prior for video SCI but suffering from either the slow reconstruction speed or poor reconstruction quality. Recently, several deep learning based methods have achieved performances comparable with traditional methods while being significantly faster at the inference stage. Specifically, the deep learning based approaches (Qiao et al. 2020b) have been explored such as fully connected network based method (Iliadis et al. 2018), end-to-end convolutional neural network (CNN) U-net (Qiao et al. 2020b), deep tensor based ADMM-net (Ma et al. 2019), hardware constraints based deep neural network (DNN) (Yoshida et al. 2018), RNN-based BIRNAT (Cheng et al. 2020), to reconstruct video images with higher quality and higher speed. In this paper, encouraged by those works, we develop an end-to-end framework for joint spatial and temporal compressive imaging reconstruction with an exemplar system developed in Qiao et al. (2020a).

Spatial and Temporal SCI
Instead of deploying multiple video SCI cameras by increasing the memory, bandwidth as well as the latency, the joint spatial (multi-view) and temporal compressive imaging provides a low-cost, low-bandwidth and low-memory requirement imaging system by capturing high-speed motions of multi views within a snapshot, which is in demand for the emerging applications such as traffic surveillance, sports photography and self-driving cars (Lu et al. 2020; Qiao et al. 2020a). For example, for some important vision tasks such as autonomous navigation, site modelling and surveillance for robotics, conventional cameras with a limited FoV often lose sight of objects when their bearings change suddenly due to a significant turn of the observer (e.g., a robot), the object, or both (Spacek 2005). In contrast, multi-view sensors can track objects more robustly meanwhile enjoying the advantages of low cost and feasibility in hardware. For spatial (not limited to FoV) compressive imaging, besides modulating each scene with a shifted version of the whole aperture as used in SSTCI, another intuitive method is partitioning a coded aperture to different blocks and then merging them together, but this method is usually too expensive or even infeasible to realize in hardware. Moreover, a quantization with entropy coding approach based on frame skipping was adopted in Angayarkanni et al. (2019) to achieve efficient multi-view wireless video compression based on CS. In addition, a lensless camera was proposed in Nakamura et al. (2019) with a novel design to capture the front and back sides simultaneously, where the object-side sensor works as a coding mask and the other one as a sparsified image sensor. In summary, storage and transmission of multi-view video sequences involve large volumes of redundant data, which can be efficiently compressed by the SSTCI system (hardware) and then resolved using a decoder (software). By performing the compressive sensing-based image reconstruction, the captured coded image can be decoded computationally. This paper aims to develop an efficient and effective end-to-end deep learning based decoder for multi-view video SCI reconstruction.

Optical Flow
Optical flow is a long-standing vision task aiming to estimate the per-pixel motion between video frames, which provides a plausible motion information facilitating lots of downstream tasks such as video alignment (Caballero et al. 2017), video editing (Xu et al. 2019), and video analysis (Cheng et al. 2017; Ng et al. 2015). Optical flow estimation has traditionally been treated as an energy minimization problem and recently shows a promising alternative trend by deep learning methods being significantly faster at inference stage. One of the milestone works of deep learning for optical flow estimation is FlowNet (Dosovitskiy et al. 2015), which is based on an U-net auto-encoder and achieves promising performance. Following this work, more architectures for optical flow estimation have been evolved in recent years, yielding better results with faster inference time, such as FlowNet2 (Ilg et al. 2017), PWCNet (Sun et al. 2018) and LiteFlowNet (Hui et al. 2018), RAFT (Teed and Deng 2020). These methods typically adopt an iterative updating approach training with the synthetic datasets and involve operators like cost volume, pyramidal features, and backward feature warping. Taking both the efficiency and accuracy into consideration, we employ the FlowNet2 (Ilg et al. 2017) as our optical flow estimator, while the optical flow is fed into the recurrent neural network for SCI reconstruction as the explicit motion guidance, which has not been studied by any previous video SCI approaches.

Review of the Snapshot Spatial-Temporal Compressive Imaging System in Qiao et al. (2020a)
Hardware Principle
Fig. 1
figure 1
Optical setup of SSTCI (Qiao et al. 2020a). O1, O2: objects from two views; L1, L2, L3, L4: lens; M: mirror; BS: beamsplitter; PBS: polarizing beamsplitter; DMD: digital micromirror device. M1 and M2 have slightly different orientations relative to their incident beams to introduce a lateral displacement (thus imposing different sets of masks on the scenes from two different views) between the modulated images of O1 and O2 on the camera

Full size image
Our SSTCI system was presented in Qiao et al. (2020a), which uses a single camera to simultaneously capture video streams from two scenes. Between the camera and the scenes, we deploy a single DMD to apply distinguishing spatial modulations (i.e., 2D random-binary masks) to different video frames for both scenes in a manner of element-wise product. As shown in Fig. 1, the hardware innovation in Qiao et al. (2020a) is to introduce a polarization-dependent transverse displacement of the two views on the detector plane, which is implemented by the polarized beamsplitter (to impose different polarization states onto the scenes from different views) and two mirrors, with different orientations (to shift the modulation patterns). It is worth noting that different views can be separated as long as their respective coding patterns are uncorrelated to each other. This condition is met in the SSTCI system in which the shifted versions of the random pattern are mutually uncorrelated as long as the shifting amount is greater than the pattern feature size (size of each random element). In this case, we have achieved two different sets of masks with each set for the scene of one view, via only using a single DMD and a single camera. The system thus did not scarify the spatial resolution of the DMD and camera and can capture two full views of the scene simultaneously. A single camera is used to capture both the SCI measurement and side information in Yuan et al. (2017), which can potentially be used to capture two views of the scene, but the solution used therein scarifies the spatial resolution of the camera by half.

Fig. 2
figure 2
Principle of dual-view video SCI (left) and the proposed network for reconstruction (right). Two FoV dynamic scenes, shown as two branches of images (view 1 and 2) at different timestamps, pass through two sets of dynamic aperture (produced by the same set of masks in Fig. 1), which imposes individual coding patterns. The coded frames from both FoVs are then integrated over time on a camera, forming a single-frame compressed measurement. This measurement is fed into our proposed model (right) to reconstruct the high-speed video frames of two dynamic scenes

Full size image
The modulated data streams from the two scenes are then laterally superposed (via beamsplitters) and temporally integrated within one exposure period of the camera, forming a single frame of raw measurement. In this paper, we aim to propose a reconstruction method to resolve the videos of two scenes from the single measurement. To the best of our knowledge, this is the first deep learning based reconstruction method reported in the literature for joint FoV and temporal CS system and has the potential to extend to multiple views, which will be beneficial to practical robotics and 3D applications (for example, for the left view and right view in the stereo imaging) with a fast inference speed, enjoying the advantages of low-bandwidth, low-memory cost and fast inference.

Mathematical Model of Dual-View SCI
As depicted in the left part of Fig. 2 (and also in Fig. 1), in the dual-view video SCI system, we denote object O1 with B temporal channels and 𝑛𝑥×𝑛𝑦 pixels in the transverse plane as 𝑋𝑋1∈ℝ𝑛𝑥×𝑛𝑦×𝐵 , which is modulated by the coding patterns 𝐶𝐶1∈ℝ𝑛𝑥×𝑛𝑦×𝐵, correspondingly. Let 𝑋𝑋2∈ℝ𝑛𝑥×𝑛𝑦×𝐵 denotes object O2, modulated by shifted coding patterns 𝐶𝐶2∈ℝ𝑛𝑥×𝑛𝑦×𝐵 (this shifting is introduced by the optical design in Fig. 1). The single-shot measurement 𝑌𝑌∈ℝ𝑛𝑥×𝑛𝑦 is given by

𝑌𝑌=∑𝑏=1𝐵(𝑋𝑋𝑏1⊙𝐶𝐶𝑏1+𝑋𝑋𝑏2⊙𝐶𝐶𝑏2)+𝐺𝐺,
(1)
where 𝐺𝐺∈ℝ𝑛𝑥×𝑛𝑦 represents the noise and ⊙ denotes the Hadamard (element-wise) product. The frontal slices 𝑋𝑋𝑏1, 𝐶𝐶𝑏1, 𝑋𝑋𝑏2, 𝐶𝐶𝑏2 with dimension of ℝ𝑛𝑥×𝑛𝑦 denote the b-th video frame and the corresponding coding pattern imposed on O1 and O2, respectively.

Let 𝑋𝑋=[𝑋𝑋1,𝑋𝑋2]𝑐3∈ℝ𝑛𝑥×𝑛𝑦×2𝐵 and 𝐶𝐶=[𝐶𝐶1,𝐶𝐶2]𝑐3 ∈ℝ𝑛𝑥×𝑛𝑦×2𝐵, where [⋅]𝑐3 denotes matrix concatenation operation in the third (temporal) dimension. Then (1) can be simplified as:

𝑌𝑌=∑𝑏=12𝐵𝑋𝑋𝑏⊙𝐶𝐶𝑏+𝐺𝐺.
(2)
The formulation in (2) can be expressed by the following vectorized linear equation:

𝑦𝑦=𝛷𝛷𝑥𝑥+𝑔𝑔,
(3)
where 𝑦𝑦=Vec(𝑌𝑌)∈ℝ𝑛𝑥𝑛𝑦 denotes the vectorized measurement, 𝑔𝑔=Vec(𝐺𝐺)∈ℝ𝑛𝑥𝑛𝑦 the vectorized noise and 𝑥𝑥=Vec(𝑋𝑋)∈ℝ2𝑛𝑥𝑛𝑦𝐵 the desired signal. Different from traditional compressive sensing (Donoho 2006), this sensing matrix 𝛷𝛷∈ℝ𝑛𝑥𝑛𝑦×2𝑛𝑥𝑛𝑦𝐵 in dual-view video SCI is sparse and follows a diagonal structure

𝛷𝛷=[diag(Vec(𝐶𝐶1)),…,diag(Vec(𝐶𝐶2𝐵))].
(4)
Consequently, the compressive sampling rate is equal to 12𝐵. Theoretical results have been derived recently in Jalali and Yuan (2019) considering this special sensing matrix.

Proposed Framework
To reconstruct the high-speed frames of two FoVs {𝑋𝑋𝑏1}𝐵𝑏=1 and {𝑋𝑋𝑏2}𝐵𝑏=1, we propose an overall architecture of dual video CS reconstruction framework as shown in the right part of Fig. 2. The proposed model consists of three components: (1) A diversity amplifier to distinguish two scenes. (2) A dual-net separator to reconstruct each FoV with coarse resolution. (3) An RNN in conjunction with bidirectional optical flow to exploit motion information for improving the resolution and refining the reconstructed video frames. The whole network is trained in an end-to-end manner.

Diversity Amplifier
The first step in our proposed network is to distinguish two scenes in the single snapshot measurement. Firstly, we normalize the original measurement 𝑌𝑌 to balance the integrated energy by taking all the coding patterns into consideration. Recalling the definition of 𝑌𝑌 in (2), various pixels of 𝑌𝑌 may gather different numbers of frames from {𝑋𝑏}2𝐵𝑏=1 as the integrated energy according to {𝐶𝑏}2𝐵𝑏=1, which means some pixels acquire with large energy but some others with low energy. In other words, the integrated energy of each pixel is not only related to the value of corresponding position of original scene, but also the corresponding position of the coding patterns. To alleviate the influence of imbalanced energy and encourage the network to focus on the video content with less disturbs from coding patterns, we normalize the original measurement as follows:

𝑌𝑌⎯⎯⎯⎯⎯=𝑌𝑌⊘(∑𝑏=12𝐵𝐶𝐶𝑏/(2𝐵)),
(5)
where ⊘ denotes the matrix dot (element-wise) division, and the matrix ∑2𝐵𝑏=1𝐶𝐶𝑏/(2𝐵) refers to the averaged coding patterns with the same size as 𝑌𝑌, which can be regarded as the corresponding weights of each pixel integrated into the measurement 𝑌𝑌. Figure 3 shows the illustrations of both the original measurement 𝑌𝑌 and the normalized measurement 𝑌𝑌⎯⎯⎯⎯⎯ by imposing the energy normalization. In short, 𝑌𝑌⎯⎯⎯⎯⎯ can be regarded as an approximated averaging image of all the 2B high-speed frames {𝑋𝑋𝑏1}𝐵𝑏=1 and {𝑋𝑋𝑏2}𝐵𝑏=1, hoping to normalize the imbalanced energy brought by coding patterns and facilitate the dual-view reconstruction. However, the approximated averaging image of all high-speed frames is not sufficient for dual-view SCI, because 𝑌𝑌⎯⎯⎯⎯⎯ averages two scenes without discrimination, expressing the same content of different views. Thus, to assist the separation of dual views, we develop a diversity amplification preprocessing method by taking different coding patterns of each scene into consideration, aiming to enlarge the differences of two scenes at the first step for better dual-view SCI.

Fig. 3
figure 3
Illustration of the original measurement 𝑌𝑌 (top-left), the normalized measurement 𝑌𝑌⎯⎯⎯⎯⎯ (bottom-left), and four diversity amplified images 𝐷𝐷1, 𝐷𝐷2, 𝐷𝐷3, 𝐷𝐷4 (middle) and the smoothed varieties (𝐷𝐷1),(𝐷𝐷2) (right), where we enlarge the details in red boxes for better visualization

Full size image
As depicted in Fig. 3, we first construct two precessing methods to explore the dissimilarities between two scenes as follows:

𝐷𝐷1=𝑌𝑌⊘(∑𝐵𝑏=1𝐶𝐶𝑏1/𝐵),𝐷𝐷2=𝑌𝑌⊘(∑𝐵𝑏=1𝐶𝐶𝑏2/𝐵).
(6)
For 𝐷𝐷1, each element of (∑𝐵𝑏=1𝐶𝐶𝑏1/𝐵) represents the mean coding pattern of the first scene, describing how many corresponding pixels of the first scene are integrated into the measurement 𝑌𝑌. Here, we use this mean coding pattern to normalize the measurement from the perspective of the first view, which is able to alleviate the influence of the coding patterns of the first view, leaving the non-normalized energy for the second view. As the visualization of 𝐷𝐷1 shown in Fig. 3, the first view (bear) is relatively smoother and visually clearer with less artifacts, e.g., the fur on the bear’s back, compared with the second view (swan) shown with obvious noise and artifacts. Similarly, for 𝐷𝐷2, the (∑𝐵𝑏=1𝐶𝐶𝑏2/𝐵) refers to the corresponding proportion of the second scene integrated into the measurement 𝑌𝑌, which is utilized to normalize the measurement and results in normalized energy for the second view. Specifically, as 𝐷𝐷2 shown in Fig. 3, the second scene (swan) is much smoother and has less artifacts than the other one (bear), which can be seen from those non-coincident parts such as the neck and tail of the swan. In summary, element-wise dividing the measurement by individual mean coding pattern of different scenes, the diversity of different views can be amplified by taking each mean coding pattern into consideration. Furthermore, in order to sufficiently explore the detected diversity of 𝐷𝐷1,𝐷𝐷2 and obtain the preliminary contours of each view, we develop an elaborated design to further enlarge the differences of dual views, expressed by:

𝐷𝐷3=𝐷𝐷1−(𝐷𝐷1),𝐷𝐷4=𝐷𝐷2−(𝐷𝐷2),
(7)
where  is utilized to smooth and blur image 𝐷𝐷1, e.g., a Gaussian filter, as shown in Fig. 3. The noise and artifacts of the second scene can be detected by comparing 𝐷𝐷1 to its smooth variety (𝐷𝐷1), denoted by 𝐷𝐷3. As we can see from Fig. 3, by subtracting the smooth variety (𝐷𝐷1) from 𝐷𝐷1, the position in 𝐷𝐷1 with non-normalized energy is typically detected expressed as noise and artifacts, resulting in a contour of the swan as the illustration of 𝐷𝐷3. Analogously, 𝐷𝐷4 is constructed by detecting the diversity between 𝐷𝐷2 and its smoothed version (𝐷𝐷2), resulting in a contour of the bear as the illustration of 𝐷𝐷4. It can be observed that the diversity between different scenes is amplified, meanwhile, capable of preserving the motionless information such as background and motion trails of different scenes, which is beneficial for reconstructing individual scenes for dual-view video SCI.

Fig. 4
figure 4
Architecture of the dual-net separator

Full size image
Dual-net Separator
Hereby, we construct a dual-net separator (DNS) to generate two videos through two branches, respectively. In order to make full use of the amplified diversity of different views and fuse normalize measurement 𝑌𝑌 in conjunction with various coding patterns, we construct the input of dual-net separator as:

𝐃𝐍𝐒01𝐃𝐍𝐒02=[𝑌𝑌⎯⎯⎯⎯⎯,𝐷𝐷1,𝐷𝐷2,𝐷𝐷3,𝐷𝐷4,𝑌𝑌⎯⎯⎯⎯⎯⊙𝐶𝐶11,𝐘⎯⎯⎯⎯⊙𝐶𝐶21,…,𝐘⎯⎯⎯⎯⊙𝐶𝐶𝐵1]𝑐3,=[𝑌𝑌⎯⎯⎯⎯⎯,𝐷𝐷1,𝐷𝐷2,𝐷𝐷3,𝐷𝐷4,𝑌𝑌⎯⎯⎯⎯⎯⊙𝐶𝐶12,𝐘⎯⎯⎯⎯⊙𝐶𝐶22,…,𝐘⎯⎯⎯⎯⊙𝐶𝐶𝐵2]𝑐3,
(8)
where we approximate the mask-modulated frames of different scenes by element-wise product of normalized measurement 𝑌𝑌 and the corresponding coding patterns {𝐶𝐶𝑏1/2}𝐵𝑏=1 (hereafter, the subscript 1/2 means ‘1 or 2’). As depicted in Fig. 4, the inputs 𝐃𝐍𝐒01 and 𝐃𝐍𝐒02 are then fed into two branches of convolution networks with shared architecture but different parameters, respectively. Each network branch is composed of three sub-networks, stated as:

𝑋𝑋˜1=𝑐𝑛𝑛31([𝐃𝐍𝐒21,𝐃𝐍𝐒11]𝑐3),𝐃𝐍𝐒21=𝑐𝑛𝑛21(𝐃𝐍𝐒11),𝐃𝐍𝐒11=𝑐𝑛𝑛11(𝐃𝐍𝐒01),𝑋𝑋˜2=𝑐𝑛𝑛32([𝐃𝐍𝐒22,𝐃𝐍𝐒12]𝑐3),𝐃𝐍𝐒22=𝑐𝑛𝑛22(𝐃𝐍𝐒12),𝐃𝐍𝐒12=𝑐𝑛𝑛12(𝐃𝐍𝐒02),
(9)
Fig. 5
figure 5
The architecture of bidirectional recurrent refining network, composed of a forward network (left) recurrently reconstructing each frame forwardly by integrating the corresponding optical flow, and a backward network (right) incorporating with dynamic motion in the reversed order

Full size image
where 𝑐𝑛𝑛11/2 is used to extract the fine-grained characteristics of input 𝐃𝐍𝐒01/2, consisting of four convolutional layers. When going deeper, the second subnetwork 𝑐𝑛𝑛21/2 containing three convolutional resblocks, is utilized to explore coarse and global features of the input. The third 𝑐𝑛𝑛31/2 has a mirror symmetry structure as 𝑐𝑛𝑛11/2, where fine-to-coarse spatial features of various scales are concatenated together to reconstruct dual videos. In this stage, the two scenes 𝑋𝑋˜1={𝑋𝑋˜𝑏1}𝐵𝑏=1 and 𝑋𝑋˜2={𝑋𝑋˜𝑏2}𝐵𝑏=1 are separated from each other, and then fed into the refining network as the input providing motion and visual information.

Refine Net
To further explore the spatio-temporal details of videos, the optical flow is employed here to improve the smoothness across time. Moreover, we propagate the optical dynamic information bidirectionally, implemented in conjunction with both a forward and a backward RNN to refine video reconstruction in a sequential manner.

Optical Flow Extractor

In order to maintain temporally connected dynamic motions, optical flow is utilized in our work to improve the visual quality of reconstructed results. Considering the efficiency and accuracy, we utilize the FlowNet 2.0 (Ilg et al. 2017) as the optical flow extractor, to facilitate video reconstruction by integrating explicit motion information as a guidance. Our goal is to better explore the dynamic motion between frames bidirectionally. Taking the t-th and the (𝑡+1)-th frame as examples, the optical flow extraction can be expressed as:

𝐹𝐹𝑡→𝑡+11=Flownet(𝑋𝑋˜𝑡1,𝑋𝑋˜𝑡+11),𝐹𝐹𝑡+1→𝑡1=Flownet(𝑋𝑋˜𝑡+11,𝑋𝑋˜𝑡1),𝐹𝐹𝑡→𝑡+12=Flownet(𝑋𝑋˜𝑡2,𝑋𝑋˜𝑡+12),𝐹𝐹𝑡+1→𝑡2=Flownet(𝑋𝑋˜𝑡+12,𝑋𝑋˜𝑡2),
(10)
where {𝐹𝐹𝑡→𝑡+11}𝐵𝑡=1 represents the optical flow from 𝑋𝑋˜𝑡1 to 𝑋𝑋˜𝑡+11 of the first video scene, while the 𝐹𝐹𝑡+1→𝑡1 denotes the optical flow in the reverse direction. Analogously, 𝐹𝐹𝑡→𝑡+12 and 𝐹𝐹𝑡+1→𝑡2 refer to the bidirectional motions between adjacent frames of the second video scene.

To make good use of the pre-trained model in Ilg et al. (2017), we initialize the weights of Flownet from the released pre-trained model, and jointly fine-tune the parameters under the supervision of the final reconstruction loss. This is different from previous methods (Perazzi et al. 2017) which utilize the pre-computed optical flow as an additional input, as our model aims to jointly learn useful motion representations particularly for our video SCI task. In the following, we will introduce how to integrate the bidirectional optical flow into the video refining process in a recurrent manner, which establishes reasonable connections to RNN for better video reconstruction.

Bidirectional Recurrent Refining Network After extracting the optical flow of dual videos in both the forward and backward order, we integrate the motion information into a bidirectional RNN to refine the reconstruction results of dual-net separator in a sequential manner, as depicted in Fig. 5, where the left part corresponds to the forward module and the right part refers to the backward module.

Fig. 6
figure 6
The simplified overall architecture

Full size image
Forward Refining Network: The forward refining network takes the output of DNS {𝑋𝑋˜𝑏1/2}𝐵−1𝑏=1 as the initial input, and fuses three additional visual information together including the forward motion information, the amplified diversity, and the memory information of RNN. Then each frame of forward refined video {𝑋𝑋→𝑏1/2}𝐵𝑏=2 is reconstructed recurrently. Taking the t-th frame for example, the forward refining process is expressed as:

𝑋𝑋→𝑡+11=→(𝐻𝐻−→𝑡+11),𝐻𝐻−→𝑡+11=→([𝑋−→−(𝑋𝑋˜𝑡1),𝐹−→(𝐹𝐹𝑡→𝑡+11),𝐷−→(𝐷𝐷1:4),𝐻−→−(𝐻𝐻−→𝑡1)]𝑐3),𝑋𝑋→𝑡+12=→(𝐻𝐻−→𝑡+12),𝐻𝐻−→𝑡+12=→([𝑋−→−(𝑋𝑋˜𝑡2),𝐹−→(𝐹𝐹𝑡→𝑡+12),𝐷−→(𝐷𝐷1:4),𝐻−→−(𝐻𝐻−→𝑡2)]𝑐3),
(11)
where → is a CNN block, injecting the hidden representations 𝐻𝐻−→𝑡+11 to the observation space to output a refined frame 𝑋𝑋→𝑡+11, and → refers to the forward processing. The hidden representation fuses the visual information of four components together through a CNN resblock denoted by →, including the t-th frame 𝑋𝑋˜𝑡+11 as a good initialization, the forward optical flow 𝐹𝐹𝑡→𝑡+11 providing the motion information, the amplified diversity 𝐷𝐷1:4=[𝐷𝐷1,𝐷𝐷2,𝐷𝐷3,𝐷𝐷4]𝑐3 emphasizing the difference between two scenes, and the hidden unit 𝐻𝐻𝑡1 of RNN offering the memory of the previous frames. In addition, those visual information are firstly embedded by four parallel CNN blocks, respectively, denoted by 𝑋−→−, 𝐹−→, 𝐷−→, 𝐻−→−, playing the roles of visual feature extractors. Similarly, the video frame 𝑋𝑋˜𝑡2 of the other scene is refined in the same way. While the forward refining network achieves appealing performance, it neglects the dynamic visual information in the reversed order. This encourages us to construct a backward refining network to further improve the reconstruction performance.

Backward Refining Network As illustrated in the right part of Fig. 5, the forwardly refined videos {𝑋𝑋→𝑡1}𝐵𝑡=1 and {𝑋𝑋→𝑡2}𝐵𝑡=1 are firstly reversed as {𝑋𝑋→𝑡1}1𝑡=𝐵 and {𝑋𝑋→𝑡2}1𝑡=𝐵, and then taken as the inputs of backward refining network. The structure of backward refining network is similar to the forward one, with the main difference on the reversed input and motion information. Due to the opposite order, the (𝑡+1)-th frame and the optical flow from 𝑋𝑋→𝑡+11/2 to 𝑋𝑋→𝑡1/2 are utilized to improve the t-th frame, stated as:

𝑋𝑋←𝑡1=⃖ (𝐻𝐻←−𝑡1),𝐻𝐻←−𝑡1=⃖ ([𝑋←−−(𝑋𝑋→𝑡+11),𝐹←−(𝐹𝐹𝑡+1→𝑡1),𝐷←−(𝐷𝐷1:4),𝐻←−−(𝐻𝐻←−𝑡+11)]𝑐3),𝑋𝑋←𝑡2=⃖ (𝐻𝐻←−𝑡2),𝐻𝐻←−𝑡2=⃖ ([𝑋←−−(𝑋𝑋→𝑡+12),𝐹←−(𝐹𝐹𝑡+1→𝑡2),𝐷←−(𝐷1:4),𝐻←−−(𝐻𝐻←−𝑡+12)]𝑐3),
(12)
where ← refers to the backward processing, and all the feature extractors have exactly the same architectures corresponding to the forward refining network, but without sharing parameters. As a result, the reconstructed videos of two scenes are improved by integrating the opposite dynamic information, which measures the spatio-temporal shifts of the intensity in a reversed order. Overall, the bidirectional designation has three benefits: (1) the forwardly refined results are more accurate and able to provide more detailed motion and visual information; (2) the backward optical flow is informative to provide the reversed motion for the predicting frame; (3) the bidirectional mechanism provides more gradient propagation paths when training with the gradient descent algorithm, which helps to jointly optimize the forward and backward reconstruction.

Simplified Overall Architecture
Although the architecture composed of both forward and backward refining network improves reconstruction results bidirectionally, the drawback is its large computational cost and huge memory requirement. To address this challenge, and aiming to save half of the required memory and take the advantages of the bidirectional motion information, we further condense the forward and backward refining network together, as shown in Fig. 6. Firstly, the outputs of DNS {𝑋𝑋˜𝑡1/2}𝐵𝑡=1 are fed into the flownet to extract the optical flow {𝐹𝐹𝑡→𝑡+11}𝐵−1𝑡=1 in the forward order. Similarly, the reversed {𝑋𝑋˜𝑡1/2}1𝑡=𝐵 are fed into the flownet to obtain the optical flow {𝐹𝐹𝑡+1→𝑡1}1𝑡=𝐵−1 in the backward order. Then the bidirectional motion information is taken as the input of RNN, stated as:

𝑋𝑋ˆ𝑡+11=ˆ(𝐻𝐻ˆ𝑡+11),𝑋𝑋ˆ𝑡+12=ˆ(𝐻𝐻ˆ𝑡+12),𝐻𝐻ˆ𝑡+11=ˆ([𝑋ˆ(𝑋𝑋𝑡1˜),𝐹→ˆ(𝐹𝐹𝑡→𝑡+11),𝐹⃖ ˆ(𝐹𝐹𝑡+1→𝑡1),𝐷ˆ(𝐷𝐷1:4),𝐻ˆ(𝐻𝐻ˆ𝑡1)]𝑐3),𝐻𝐻ˆ𝑡+12=ˆ([𝑋ˆ(𝑋𝑋𝑡2˜),𝐹→ˆ(𝐹𝐹𝑡→𝑡+12),𝐹⃖ ˆ(𝐹𝐹𝑡+1→𝑡2),𝐷ˆ(𝐷𝐷1:4),𝐻ˆ(𝐻𝐻ˆ𝑡2)]𝑐3),
(13)
where the main modification is that the two CNN blocks 𝐹𝐹→ˆ and 𝐹𝐹←ˆ are simultaneously utilized to inject bidirectional optical flow into the hidden space, which facilitates reconstruction of the (𝑡+1)-th frame by predicting from the dynamic information 𝐹𝐹𝑡→𝑡+11/2 and the backward reasoning from 𝐹𝐹𝑡+1→𝑡1/2. All the feature extractors {ˆ,ˆ,𝑋ˆ,𝐹→ˆ,𝐹⃖ ˆˆ,𝐷ˆ,𝐻ˆ} have the same architectures as introduced in the forward and backward refining network. We find that this framework is able to save half the memory with little performance degradation, taking the advantages of both forward and backward optical flows.

To sum up, the whole network is composed of the dual-net separator and the simplified bidirectional recurrent refining network, as shown in Fig. 6. To minimize the reconstruction error of all the frames of dual view, we employ the mean square error (MSE) as the loss function for training the end-to-end deep network, stated as:

=𝛼˜+ˆ,˜=∑𝐵𝑡=1‖‖𝐗˜𝑡1−𝐗𝑡1‖‖22+∑𝐵𝑡=1‖‖𝐗˜𝑡2−𝐗𝑡2‖‖22,ˆ=∑𝐵𝑡=1‖‖‖𝐗ˆ𝑡1−𝐗𝑡1‖‖‖22+∑𝐵𝑡=1‖‖‖𝐗ˆ𝑡2−𝐗𝑡2‖‖‖22,
(14)
where ˜ and ˆ represent the MSE loss of dual-net separator and refine net, respectively, and 𝛼 is a trade-off parameter, which is set to 1 in our experiments. During testing, we can achieve high-quality reconstructed videos in a short time with the well-learned network parameters.

Experiments
In this section, we compare the proposed optical flow-aided recurrent neural network (OFaNet) with other methods on both simulation and real datasets for dual-view SCI, and further extend it to single-view SCI for a broader scope.

Datasets, Training and Counterparts
Datasets: We first evaluate the proposed OFaNet on our collected six simulation data including Bear&Blackswan, Boy&Girl, Running Cars, Bike&Bus, Cow&Dog and Hike &Hockey, respectively, where 𝐵=10 video frames of two individual 256×256 scenes (i.e. totally 20 frames) are compressed into a single measurement for each dataset. We also demonstrate experiments on simulation data with different compressive rates (𝐵={6,14}). To evaluate the effectiveness of the proposed model in a broader scope, we also demonstrate experiments on the single-view video SCI on six widely used grayscale simulation datasets including Kobe, Runner, Drop, Traffic, Aerial and Vehicle, where 8 video frames are compressed into a single measurement. Furthermore, we also evaluate OFaNet on two real dual-view datasets with 𝐵=10, one real dual-view dataset with 𝐵=20 (Qiao et al. 2020a), captured by the real SSTIC camera, and three real single-view datasets.

Implementation Details: For simulation, We randomly crop patch cubes 256×256×𝐵 from original scenes in DAVIS2017 (Pont-Tuset et al. 2017) and randomly select two video sequences at each time to be modulated with different patterns, and obtain the training dataset containing about 32,000 data pairs. We set the maximum training epoch as 90, the batch size as 2, and start training with the initial learning rate of 3×10−4, which is decreased by 10% every 10 epochs. Our network is implemented in Pytorch and trained with the Adam optimizer 0. The experiments are conducted on a NVIDIA RTX 8000 GPU, and it takes about 3 days to complete the training of the entire network. For the real SSTIC dataset with the size of 650 × 650 × 10 (𝐵=10), we randomly crop patch cubes (650 × 650 × 10) from the same training dataset DAVIS2017, and obtain about 12000 data pairs for training with the initial learning rate of 2×10−5. Similarly, we generate 12000 data pairs with the size of 650 × 650 × 20 for training another real dataset with 𝐵=20. The detailed architecture of OFaNet is presented in Table 1.

Table 1 Network architecture of the proposed OFaNet
Full size table
Counterpart Methods and Performance Metrics: As mentioned before, the PnP-TV-FFD algorithm (Qiao et al. 2020a) has been proposed for dual-view SCI reconstruction. Further more, GAP-TV (Yuan 2016) is a widely used efficient baseline, which is able to provide decent results within short time. The previous state-of-the-art optimization based algorithm DeSCI (Liu et al. 2019) is also employed to dual-view video SCI as a strong baseline, however, which always suffers from slow reconstruction speed. For comparison with the deep learning based methods, we employ the U-net (Qiao et al. 2020b; Ronneberger et al. 2015), ADMMnet (Ma et al. 2019) and state-of-the-art network BIRNAT (Cheng et al. 2020) to the dual-view SCI tasks for comparison. In the following, we compare our proposed method against these six methods.

GAP-TV (Yuan 2016): This algorithm models the inverse problem of video SCI as a total variation minimization problem. It aims to solve

𝑥𝑥ˆ=argmin𝑥𝑥‖TV(𝑥𝑥)‖, s.t.   𝑦𝑦=𝛷𝛷𝑥𝑥,
(15)
where TV(𝑥) indicates the total variation of the signal, imposing the sparsity on the gradient of signal. GAP-TV employs 100 iterations for the dual-view SCI video reconstruction.

PnP-TV-FFD (Qiao et al. 2020a): This algorithm utilizes the deep denoising priors in the plug-and-play framework for efficient reconstruction, which solves the following problem

𝑥𝑥ˆ=argmin𝑥𝑥12‖𝑦𝑦−𝛷𝛷𝑥𝑥‖22+𝜆TV(𝑥𝑥)+𝜌𝛩𝛩(𝑥𝑥),
(16)
where 𝛩𝛩(𝑥𝑥) denotes the deep denoising prior, i.e., the fast and flexible denoising network (FFDNet) (Zhang et al. 2018). PnP-TV-FFD uses 500 iterations for the best performance.

DeSCI (Liu et al. 2019): This SCI reconstruction algorithm boosted the reconstruction quality of single-view SCI, which imposes a weighted nuclear norm on the mathematical model of SCI system. DeSCI integrates the nonlocal self-similarity of videos and the rank minimization with the SCI decoding problem. By applying the nuclear norm of nonlocal similar patch-groups in the video frames, the signals can be recovered with the minimized rank under the alternating direction method of multipliers (ADMM) framework. The iteration number Max-Iter is fixed to 60 as suggested in Liu et al. (2019).

U-net (Qiao et al. 2020b): This deep learning based network utilizes a specially designed CNN architecture to capture the local structures for reconstructing video in an end-to-end manner. For fair comparison, we use the same training datasets as utilized in our proposed model, and employ the same hyperparameters as set in the original paper. We train 80 epochs for U-net on a NVIDIA RTX 8000 GPU, and it takes about 1.5 days to complete the training of the entire network.

ADMM-net (Ma et al. 2019): A deep tensor ADMM-Net provides high-quality decoding for video SCI systems in seconds. This network unfolds inference iterations of a standard tensor ADMM algorithm into a layer-wise structure based on deep neural network, which learns the domain of low-rank tensor through computationally efficient training. We train 50 epochs until the performance converges.

BIRNAT (Cheng et al. 2020): This method firstly employs the recurrent networks to SCI and achieves state-of-the-art performance for single-view SCI reconstruction. BIRNAT employs a deep CNN with a self-attention map to reconstruct the first frame, then a forward RNN and a backward RNN to generate the following frames in a sequential manner. BIRNAT also utilizes the adversarial training for performance improvement. We train BIRNAT on an NVIDIA RTX 8000 GPU with the learning rate 1×10−4 till the performance is convergent, which takes about 12 days for training due to the doubled compression rate corresponding to dual-view SCI.

Table 2 The average results of PSNR in dB (left entry in each item) and SSIM (right entry in each item) and running time per measurement/shot in seconds by different algorithms on dual-view simulation datasets
Full size table
We evaluate the above algorithms and our proposed OFaNet on six simulation datasets and three real datasets captured by the SSTCI system (dual-view SCI). Furthermore, we also employ the proposed OFaNet and the comparison methods to the single-view SCI system, evaluating the performances on six benchmark simulation datasets and three real datasets captured by real single-view video SCI cameras (Llull et al. 2013; Qiao et al. 2020b), to validate the effectiveness of OFaNet in a broader scope.

Both peak-signal-to-noise ratio (PSNR) and structural similarity (SSIM) (Wang et al. 2004) are used as metrics for assessing the performance on simulation datasets. PSNR is the ratio between the maximum power and the power of residual errors from the “reference”, while SSIM quantifies the visual quality by considering the structural similarity, which is more consistent with the human visual perception. Besides, we measure the running time of video reconstruction at the testing stage to evaluate the applicability in real-time applications.

Simulation Data
We first test the algorithms on six dual-view simulated data: Bear&Blackswan, Boy&Girl, Running Cars, Bike&Bus, Cow&Dog and Hike&Hockey. The performance comparisons on the six benchmark datasets are summarized in Table 2, using different algorithms, i.e., GAP-TV, PnP-TV-FFD, DeSCI, U-net, ADMMnet, BIRNAT and OFaNet. The corresponding visualization results of selected reconstructed frames are shown in Fig. 7 with full reconstructed videos shown in the supplementary material (SM). It can be observed that:

(i) OFaNet leads to the best performance compared to other algorithms on all the six datasets. In specific, the average gains of OFaNet over {GAP-TV, PnP-TV-FFD, DeSCI, U-net, ADMM-net and BIRNAT} are as much as {2.63, 2.58, 0.61, 2.15, 0.98, 1.19}dB on PSNR and {0.07, 0.10, 0.03, 0.12, 0.07, 0.05} on SSIM. Intuitively, OFaNet provides superior performance owing to the motion smoothness provided by the optical flow and the elaborately designed diversity amplifier which better fits for the target task.

(ii) Selected reconstructed frames are shown in Fig. 7. It can be observed that severe structured artifacts and noise are shown in the reconstructed images of GAP-TV and PnP-TV-FFD caused by the incomplete separation of dual scenes from the single measurement. For example, the vague shape of the bus can be seen from the reconstructed bike, which leads to severe image distortion. DeSCI smooths out some details in the reconstructed video, and results in some artifacts making the reconstructed video more visually ’unrealistic’. Unet results in some white spots randomly located in the reconstructed images, and the edges and corners are blurry. Although the BIRNAT has lower PSNR than ADMM-net, the reconstructed images of BIRNAT are visually cleaner than ADMM-net, corresponding to higher SSIM and contributing to its powerful representative ability. In general, the proposed OFaNet provides finer details and clearer contours, owing to the amplified diversity and good representative ability of the DNS and refine net.

Fig. 7
figure 7
Reconstructed results of different algorithms on six simulated dual-view SCI datasets. Left: measurements, right: selected reconstructed frames from View 1 and View 2 using different algorithms

Full size image
(iii) Interestingly, compared with GAP-TV, the methods PnP-TV-FFD and U-net both obtain higher PSNR but lower SSIM, which result in the image distortions in Fig. 7 as these so-called natural scenes look ’unnatural’. Similarly, the ADMM-net also results in higher PSNR but lower SSIM than BIRNAT, corresponding to a blurred visualization. It can be seen that the proposed OFaNet gains improvements both on PSNR and SSIM and provides cleaner and sharper reconstruction corners, which might contribute to the fine-to-cross structure and the sequential dependencies constructed by RNN.

(iv) Although the deep learning based methods (U-net, ADMM-net, BIRNAT and OFaNet) require more time for training, these methods are able to reconstruct videos within 1 second at the testing stage, which are significantly faster than the iterative optimization based algorithms. It is worth noting that our proposed OFaNet achieves more than 40,000 times faster than DeSCI ( the runner-up on PSNR and SSIM) at the testing stage. Although U-net is the fastest method, it suffers from poor reconstruction quality as shown in Fig. 7. Thus OFaNet is a pretty good compromise considering the trade-off between reconstruction quality and time.

Optical Flow To further investigate the influence of optical flow, we illustrate the flow map on two simulation datasets in Fig. 8. We can see that a jointly trained optical flow extractor learns informative task-specific features to promote video reconstruction. It can be observed that the object contours and both the global and local motions are well represented in the optical flow, which gives RNN a helpful guidance for reconstructing the next frame. Both the forward and backward optical flows are very informative and capable to explicitly introduce the dynamic motion into the reconstruction of each adjacent frame, and both the global motions might caused by camera movements and the locally varying motions of various pixel-wise displacement can be detected as shown in the optical flows. This is beneficial for modeling motions and reducing the artifacts near occlusion boundaries and smoothing the change of moving object across video frames. Moreover, in the ablation study, we will quantitatively analyze the influence of integrating the information implied in optical flow.

Frame-wise Quality In order to analyze the reconstruction performance recurrently, we plot the frame-wise PSNR and SSIM curves of different algorithms in Fig. 9. It can be observed that OFaNet outperforms other counterparts at most of the frames on both PSNR and SSIM. Interestingly, the performance curves of OFaNet seem to follow an up-and-down trend on both PSNR and SSIM, which might due to the recurrent mechanism and the bidirectional optical flow. More specifically, the bidirectional optical flow is degraded into single-direction unavoidably when reconstructing the first and last frames, which results in relatively low quality at these frames and leads to an up-and-down trend. This further demonstrates the effectiveness of our integrated bidirectional optical flow.

Fig. 8
figure 8
Illustration of the fine-tuned optical flow on two simulation datasets. a b a pair of consecutive video frames with moving objects of Bear&Blackswan; c the forward optical flow between adjacent frames (a)-(b); d the backward optical flow between adjacent frames (b)-(a); e f a pair of consecutive video frames with moving objects of Hike&Hockey; g the forward optical flow between adjacent frames (e)-(f); h the backward optical flow between adjacent frames (f)-(e)

Full size image
Fig. 9
figure 9
Frame-wise reconstruction quality on PSNR (left) and SSIM (right) by different algorithms on dataset Bear&Blsckswan

Full size image
Ablation Study To demonstrate the effectiveness of each module in OFaNet, we conduct experiments on the six dual-view simulation datasets using various versions of OFaNet, as listed in Table 3, where the abbreviations of different variants of OFanet are shown in the first row, specifically corresponding to:

OFaNet without diversity amplifier (W/o DA) excludes the diversity amplified images 𝐷𝐷1,𝐷𝐷2,𝐷𝐷3,𝐷𝐷4 from the entire reconstruction framework. Thus, the input of ‘Dual-net Separator’ is changed to 𝐃𝐍𝐒01=[𝑌𝑌⎯⎯⎯⎯⎯,𝑌𝑌⎯⎯⎯⎯⎯⊙𝐶𝐶11,𝐘⎯⎯⎯⎯⊙𝐶𝐶21,…, 𝐘⎯⎯⎯⎯⊙𝐶𝐶𝐵1]𝑐3 and 𝐃𝐍𝐒02=[𝑌𝑌⎯⎯⎯⎯⎯,𝐘⎯⎯⎯⎯⊙𝐶𝐶12,𝐘⎯⎯⎯⎯⊙𝐶𝐶22,…,𝐘⎯⎯⎯⎯⊙𝐶𝐶𝐵2]𝑐3, and the input 𝐷ˆ(𝐷𝐷1:4) in (13) is removed from the bidirectional refining network. As shown in Table 3, the diversity amplifier leads to average improvements both on PSNR (0.57) and SSIM (0.04) of six datasets with neglectable computational cost. This demonstrates the effectiveness and efficiency of diversity amplifier for separating and reconstructing dual videos.

OFaNet without dual-net separator (W/o DNS) is utilized to evaluate the effectiveness of the dual-branches framework. Considering the optical flow can only be well estimated based on the coarse reconstruction results, it is not appropriate to directly remove the dual-net separator. Thus, we replace the dual-branches network with the single-branch network with the same architecture but the different output channel at the last layer. To be more specific, for the original dual-net separator, we employ two network without sharing parameters to output each video respectively, while the variety ‘OFaNet W/o Dual-net Separator’ utilizes a single network to output two videos as a single sequential data cube. Mathematically, the single-branch network can be expressed as:

𝑋𝑋˜=𝑐𝑛𝑛3([𝐃𝐍𝐒2,𝐃𝐍𝐒1]𝑐3),𝐃𝐍𝐒2=𝑐𝑛𝑛2(𝐃𝐍𝐒1),  𝐃𝐍𝐒1=𝑐𝑛𝑛1(𝐃𝐍𝐒0),𝐃𝐍𝐒0=[𝑌𝑌⎯⎯⎯⎯⎯,𝐷𝐷1,𝐷𝐷2,𝐷𝐷3,𝐷𝐷4,𝑌𝑌⎯⎯⎯⎯⎯⊙𝐶𝐶11,𝐘⎯⎯⎯⎯⊙𝐶𝐶21,…,𝐘⎯⎯⎯⎯⊙𝐶𝐶𝐵1,𝐘⎯⎯⎯⎯⊙𝐶𝐶12,𝐘⎯⎯⎯⎯⊙𝐶𝐶22,…,𝐘⎯⎯⎯⎯⊙𝐶𝐶𝐵2]𝑐3,
(17)
where the output 𝑋𝑋˜=[𝑋𝑋˜1,𝑋𝑋˜2]𝑐3 is the concatenation of two reconstructed videos. As we can see from Table 3, the single-branch network results in average descent on PSNR (0.29) and SSIM (0.02) compared to the dual-net separator. Intuitively, the dual-branches network is more appropriate to reconstruct individual video by respectively taking its corresponding coding patterns into consideration, while the computational cost is marginal and acceptable (0.0031s).

OFaNet without refine net (W/o RN) only contains the diversity amplifier and the dual-net separator. On the one hand, it can be seen that this variety provides very limited results compared with OFaNet, which implies the refining network is beneficial to the reconstruction performance, leading to an average improvement on PSNR (1.11) and SSIM (0.07) of six simulated datasets. On the other hand, the reconstruction results of this variety show superiority compared with the basic model U-net, which could verify the effectiveness of our proposed diversity amplifier and dual-net separator. It is worth noticing that this architecture without refine net only cost less than 0.01s for testing, which is applicable with the fast inference speed and a comparable reconstruction performance for the applications with strict time demand such as self-driving systems and motion control.

OFaNet without optical flow (W/o OF) excludes the optical flow from our proposed OFaNet, which can be obversed from Table 3 that the performance degrades severely (0.7 on PSNR and 0.04 on SSIM) compared with OFaNet. The significant contribution of optical flow is verified, where the optical flow helps the reconstruction of each video by feeding the motion information as a guidance. Intuitively, the dynamic motion provides the information to guide the refining network to estimate the output according to the predicted motion, leading to smooth results across time by focusing on the locally varying dynamic object and the global movement in the background.

OFaNet without bidirection (W/o Br) only includes the forward optical flow 𝐹𝑡→𝑡+1 without the backward optical flow 𝐹𝑡+1→𝑡, and we can see that adding the backward optical flow is beneficial to video reconstruction. The bidirectional dynamic information is explored among adjacent frames both forwardly and backwardly, which can further improve the quality of reconstruction by integrating into RNN.

OFaNet without joint training (W/o JT) utilizes the pre-trained model of  Ilg et al. (2017) and initializes the weights of Flownet from the released pre-trained modelFootnote1 without fine-tuning. We can see from Table 3 that joint training the parameters of the whole framework with mean square error (MSE) loss is beneficial to video reconstruction, which helps the extracted optical flow to better match the video SCI problem.

Diversity Amplifier + Dual-net Separator + Bidirectional Recurrent Refining Network (DA&DNS&BRRN) combines the “OFaNet W/o refine net” and a bidirectional recurrent network (a forward and a backward rnn) together for video reconstruction. As shown in Table 3, the reconstruction performance is comparable or even better than OFaNet, but with significantly more computational cost. In specific, both the memory and testing time in each epoch of “Bidirectional Recurrent Refining Network” is twice larger than OFaNet, and its training time is much longer than OFaNet. Bigger network parameters also make the model difficult to train and converge. The proposed OFaNet condenses the bidirectional network to save half of the required memory and meanwhile makes full use of the bidirectional motion information.

Table 3 The average results of PSNR in dB (left entry in each cell) and SSIM (right entry in each cell) and running time per measurement/shot in seconds by different variants of our proposed OFaNet on dual-view simulation datasets
Full size table
Table 4 The average results of PSNR in dB, SSIM and running time per measurement/shot in seconds by different algorithms on dual-view simulation datasets with different compression rates
Full size table
Table 5 The average results of PSNR in dB (left entry in each item) and SSIM (right entry in each item) and running time per measurement/shot in seconds by different algorithms on dual-view simulation datasets with different level Gaussian noise
Full size table
In terms of the above oblation study, we can summarize that the final version of OFaNet results in the best performance according to all these above varieties.

Results of different compression rates In order to evaluate the performance of the proposed algorithm dealing with different compression rates, we show the results on six simulation data with compression rates 𝐵={6,10,14} for each view. As shown in Table 4, the proposed OFaNet outperforms its counterparts across different compression rates. DeSCI performs well across small compression rates (𝐵={6,10}) but results in low quality with the big compression rate (𝐵=14), which may due to the parameter setting. Moreover, DeSCI takes about 16 hours to reconstruct dual videos with the size of 256×256×14 from a single measurement, which is difficult to support practical applications. In contrast, the proposed OFaNet achieves state-of-the-art performance within 1 second for fast inference. It is worth noting that the variety DA&DNS (i.e. OFaNet without refine net) of the proposed model achieves descent performance in the shortest time (≤ 0.006s), which is a computationally efficient method. For applications with strict time demand such as self-driving systems (Lu et al. 2020) and motion control, the variety network DA&DNS is a better choice to reconstruct videos with reasonably good quality within very fast time.

Table 6 The average results of PSNR in dB (left entry in each item), SSIM (right entry in each item) and running time per measurement/shot in seconds by different algorithms on six single-view grayscale benchmark simulation datasets
Full size table
Robustness to Gaussian Noise In order to evaluate the robustness of the proposed OFaNet, we investigate the effect of Gaussian noise corresponding to different algorithms. Before adding noise to simulation data, all the measurements are normalized to [0, 1]. Then the zero-mean Gaussian noise is added to the measurements with standard deviation 𝜎 of {0, 0.01, 0.05, 0.1, 0.2}, respectively. For the optimization based algorithms, we retrain the entire framework from scratch. For the deep learning based methods, we directly test on the noisy test data using the previously well-trained parameters. As shown in Table 5, four deep learning based methods are more robust and stable on the noisy data compared with the optimization based methods. In specific, the performance of the runner-up DeSCI drops very fast when the measurement is corrupted (fine tuning the parameters might be able to provide better results but time consuming). Surprisingly, the competitive deep learning method ADMM-net performs even worse than U-net, while the BIRNAT shows robustness on the corrupted data, which might due to its powerful representative capability implied in the huge network and sequential modeling. The performance of the OFaNet degrades slower than others, which indicates the proposed model is more robust to noise and can relieve the effect of Gaussian noise.

Fig. 10
figure 10
Selected reconstruction frames of six single-view grayscale benchmark simulation datasets

Full size image
Results on Single-view Simulated SCI Data

In order to evaluate the proposed method in a broader scope, we further demonstrate the experiments on the single-view system using six widely used benchmark simulated data, i.e. Kobe, Runner, Drop, Traffic, Aerial and Vehicle. The modification of OFaNet is small to extend to single-view SCI, where we remove the diversity amplifier and only keep one branch of the dual-net separator, and utilize the same training data and experiments setting as used in BIRNAT Cheng et al. (2020). The results of different comparison methods are given in Table 6. It can be seen that the proposed OFaNet achieves competitive results, 0.16dB in PSNR higher than the strong baseline DeSCI with about 44,000× shorter testing time. Compared to state-of-the-art method BIRNAT, the proposed OFaNet occupies 2 times lower memory (8934MB) during training than BIRNAT (17748MB), which is important because the big GPU memory consumption will preclude the practical large-scale SCI applications. Figure 10 plots selected reconstruction frames of different algorithms on six datasets. It can be observed that OFaNet achieves comparable visual results compared with the state-of-the-art methods; clean and sharp contours and fine details can be provided by the proposed OFaNet.

Fig. 11
figure 11
Reconstruction results by different algorithms for real SSTCI data popping water balloon punctured by a knife (top three rows) and a falling Ping-Pong ball (bottom three rows)

Full size image
Fig. 12
figure 12
Reconstruction results by different algorithms for real SSTCI data colliding water balloons (top three rows) and flying letters (bottom three rows)

Full size image
Results on Real SCI Data
Lastly, we apply the proposed OFaNet to real data captured by the SSTCI camera. Following the same setting in Qiao et al. (2020a), two FoV videos are encoded into one snapshot measurement. Three datasets are utilized here. The first one is a popping water balloon punctured by a knife and a falling Ping-Pong ball, with the size of 650×650×10 corresponding to each view. The second dataset is composed of two colliding water balloons and two flying letters (650×650×10 for each view). The third video contains 5 pendulum balls and 4 falling dominoes, where each FoV video has the size of 650×650×20. The real captured datasets capture high-speed videos in our daily life with unavoidable noise inside and thus are more challenging to reconstruct. The camera was working at 50 frames per second (fps) during capturing these measurements and thus each measurement corresponds to 20 ms in real life. Therefore, when 10 frames of each view are reconstructed from the single measurement, each frame lasts 2 ms in real life; this is 500fps high speed video. Similarly, when 20 frames of each view are reconstructed from the single measurement, each frame lasts 1 ms in real life and the reconstructed video is of 1000fps.

The measurements and the corresponding reconstructed frames of two real dual-view SCI system with 𝐵=10 are demonstrated in Figs. 11 and 12, respectively, with full videos shown in the SM. It can be observed that in both cases, our OFaNet can provide better or at least competitive results with a significant saving on the computational time compared to existing algorithms. The reconstructed videos by GAP-TV have unpleasant artifacts, and PnP-TV-FFD shows blurry boundaries and over-smoothness. PnP-TV-FFD is capable of providing better results on real data compared to the performances on simulation datsets, which might due to the FFD denoiser is more appropriate for real datasets with unavoidable noise. It can also be seen that OFaNet provides sharp boundaries and fine details with less artifacts and fuzziness from the single compressed measurement.

Fig. 13
figure 13
Reconstruction results using different algorithms for the real SSTCI data with 𝐵=20: pendulum balls (top six rows) and falling Ping-Pong balls (bottom six rows)

Full size image
Another snapshot measurement of pendulum balls and falling dominoes with 𝐵=20 and the corresponding reconstructed results are shown in Fig. 13, with full videos shown in the SM. It can be observed that GAP-TV produces significant noise and unclear boundaries, while PnP-TV-FFD tends to over smooth the moving object. OFaNet is capable of providing relatively clear and distinct pendulum balls, as well as the sharp and straight contours of falling dominoes.

In summary, the reconstruction results of our OFaNet are of higher quality compared with other algorithms, and the inference of our algorithm is significantly faster. This indicates the applicability and efficiency of our proposed OFaNet in the real SSTCI systems.

Fig. 14
figure 14
The reconstructed frames of three single-view real data Wheel, Domino, and Water Balloon

Full size image
Results on Single-view Real SCI Data To evaluate the effectiveness of OFaNet on real applications of the single-view SCI system, we conduct experiments on three real data captured by the SCI cameras (Llull et al. 2013; Qiao et al. 2020b). For the snapshot measurement Wheel, we recover a 256×256×14 high-speed video. In addition, the larger scale snapshot measurements Domino and Water Balloon are recovered as two videos of size 512×12×10. As shown in Fig. 14, it can be seen that GAP-TV introduces unpleasant noise; DeSCI over smooth the details such as the letters in domino; U-net results in blurry edges and more noise such as in the letter ‘D’ of Wheel; BIRNAT provides finer details and sharper edges compared with other methods; OFaNet provides relatively clear contours with fewer artifacts and less noise, even though it results in some blurry parts. However, the proposed OFaNet utilizes much less testing time than DeSCI and twice lower GPU memory compared with BIRNAT. These experiments indicate both the applicability and effectiveness of the proposed algorithm in real applications.

Conclusions
We have proposed an efficient deep learning network for the reconstruction of dual-view video snapshot compressive imaging, which implements joint field-of-view and temporal compressive sensing, shedding light to high throughput machine vision systems. Inspired by the hardware encoding principle, we develop a diversity amplifier to enhance the differences of the scenes from two FoVs, and design a dual-net separator to reconstruct two views from the single measurement. Following this, we integrate recurrent mechanism and optical flow into our reconstruction network to achieve competitive results in a short time. Though the network is proposed for dual-view systems, it can be extended to single-view systems and we believe it can work for multi-view systems with moderate modifications. This will pave the way of real applications of SCI systems on robots, self-driving vehicles, etc.