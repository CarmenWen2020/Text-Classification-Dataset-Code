Dual-view snapshot compressive imaging (SCI) aims to capture videos from two field-of-views (FoVs) using a 2D sensor (detector) in a single snapshot, achieving joint FoV and temporal compressive sensing, and thus enjoying the advantages of low-bandwidth, low-power and low-cost. However, it is challenging for existing model-based decoding algorithms to reconstruct each individual scene, which usually require exhaustive parameter tuning with extremely long running time for large scale data. In this paper, we propose an optical flow-aided recurrent neural network for dual video SCI systems, which provides high-quality decoding in seconds. Firstly, we develop a diversity amplification method to enlarge the differences between scenes of two FoVs, and design a deep convolutional neural network with dual branches to separate different scenes from the single measurement. Secondly, we integrate the bidirectional optical flow extracted from adjacent frames with the recurrent neural network to jointly reconstruct each video in a sequential manner. Extensive results on both simulation and real data demonstrate the superior performance of our proposed model in short inference time. The code and data are available at https://github.com/RuiyingLu/OFaNet-for-Dual-view-SCI.

Access provided by University of Auckland Library

Introduction
As a well-developed technique, compressive sensing (CS) (Donoho 2006; Emmanuel et al. 2006) based computational imaging systems (Mait et al. 2018) have attracted much attention in recent years. These systems usually employ low-dimensional sensors to capture high-dimensional signals. Snapshot compressive imaging (SCI) (Yuan et al. 2021) is one of the promising applications of computational imaging (Llull et al. 2013; Wagadarikar et al. 2008), which utilizes a two dimensional (2D) camera to capture the 3D video or spectral data, aiming to provide an effective optical encoder for compressing the video or spectral data-cube during capture. Different from conventional cameras, such imaging systems encode the consecutive video frames (Reddy et al. 2011; Yuan et al. 2014) or spectral channels (Wagadarikar et al. 2009; Miao et al. 2019) by applying different coding patterns to each data volume along time or spectrum to obtain the final compressed measurements. With such cameras, SCI systems (Hitomi et al. 2011; Llull et al. 2013; Reddy et al. 2011; Wagadarikar et al. 2008, 2009) can encode multiple frames into a single captured image (called measurement in this work) with low-memory, low-bandwidth, low-power and potentially low-cost, which is decoded later using different reconstruction algorithms.

While previous video SCI systems only capture one scene within the field-of-view (FoV) through spatial multiplexing, extending the spatial multiplexing capability to more than one additional dimensions, i.e., multi-view imaging, is able to decrease the memory, bandwidth as well as the latency (Qiao et al. 2020a), which is necessarily required for the emerging applications such as robotics, traffic surveillance, sports photography and self-driving cars (Lu et al. 2020; Qiao et al. 2020a). Recently, the snapshot spatial-temporal compressive imaging (SSTCI) system implemented in (Qiao et al. 2020a) has achieved spatial (joint FoV) and temporal CS in a snapshot by using a single coding device, e.g., the digital micro-mirror device (DMD), plus elegant simple optical designs without compromising spatial resolution, i.e., each FoV is of full resolution. The underlying principle of SSTCI is masking various scenes with different coding patterns along time to obtain the final compressed measurements. This joint CS sheds lights on the real applications of SCI system on robotics since multi-view is an inherent requirement in machine vision. For efficient dual-view video reconstruction, a plug-and-play framework with deep denoising priors (PnP-TV-FFD) was proposed in Qiao et al. (2020a). However, one bottleneck of this method is the slow reconstruction speed and poor reconstruction quality that preclude the wide applications of SSTCI. Inspired by this novel hardware design and aiming to address the challenges of reconstruction speed and quality, we intend to develop an effective and efficient reconstruction algorithm for SSTCI. Though our long-term goal is multi-view, in this paper, we focus on the dual-view case based on the SSTCI prototype without the loss of generality.

For fast inference, one straightforward way is to train an end-to-end network based on deep learning for the inversion problem (Cheng et al. 2020; Ma et al. 2019; Qiao et al. 2020b). Yet, these networks were developed for the single-view SCI, and extending the single-view SCI inversion techniques to the muti-view reconstruction directly is non-trivial because they usually fail to clearly reconstruct each individual scene from the single compressed measurement. It is worth noting that in the SSTCI hardware design, though the coding pattern sets are different for the two FoVs, they are correlated as one set is a shifted version of the other one (Qiao et al. 2020a). Inspired by this, we propose a diversity amplifier in this paper to facilitate the distinction of two scenes by taking the different (shifted) coding patterns into consideration, which is implemented in conjunction with a dual-net separator with two branches for reconstructing each video frames respectively. By this novel design, the two FoV scenes can be well distinguished. Nevertheless, it neglects the inherent temporal correlation within each video, leading to limited reconstruction quality. To address this, we further introduce optical flow to understand the video contents for vision tasks in our network design, which is able to investigate the motion information by encoding the velocity and direction of each pixelâ€™s movement between neighboring frames. Furthermore, the extracted optical flow are explored bidirectionally and then fed into a recurrent neural network (RNN) to exploit temporal correlations between adjacent frames, resulting in reconstructed videos with more smoothness across time and less artifacts. It is essential to know both the object status (e.g., captured by the RNN cell) and motion information (e.g., optical flow), for better understanding the video contents of vision tasks.

Contributions of This Paper
In a nutshell, we develop an Optical Flow-aided Recurrent Neural Network (OFaNet) for dual-view video SCI. Specific contributions are summarized as follows:

An end-to-end deep learning based reconstruction regime is proposed for dual-view video SCI reconstruction, where a diversity amplifier and dual-net separator are constructed to separate the two FoVs from a single measurement, and then a refine net equipped with the bidirectional optical flow is developed to improve the reconstruction quality in a sequential manner.

The bidirectional optical flow is elaborately integrated into the video SCI by exploiting explicit motion information between adjacent frames, including both the global and locally varying motions, acting as the guidance for improving video reconstruction. To the best of our knowledge, this is the first time that optical flow is introduced into video SCI problems, which is jointly optimized under the supervision of mean square error (MSE) loss of reconstruction results to better match the video SCI problem.

We apply our developed algorithms to extensive simulation and real datasets (captured by the SSTCI camera) to verify the effectiveness, efficiency and robustness of the proposed algorithm. Experimental results show superior performance and robustness with much shorter inference time (about 40,000 times speedups over DeSCI) compared with previous state-of-the-art methods.

We also extend the proposed OFaNet to the single-view SCI system on both simulation and real datasets with small modifications to enlarge the application scope. The reconstruction performance is comparable with the state-of-the-art methods for single-view video SCI.

Related Work
Single-View SCI
For temporal CS, SCI system is built as an optical encoder, by compressing the videos during capture. The modulation methods can be categorized into physical mask (Llull et al. 2013; Yuan et al. 2014) and spatial light modulator (SLM) including DMD (Qiao et al. 2020b; Reddy et al. 2011; Sun et al. 2017). Given the measurement and coding patterns, an efficient decoder is required for reconstructing the time series of video frames, which has been extensively developed before (Yuan 2016; Liu et al. 2019; Yang et al. 2014; Xu and Ren 2016; Yuan et al. 2020, 2021; Meng et al. 2020).

For the software decoder, iterative optimization based methods were proposed including GAP-TV (Yuan 2016, 2020), GMM (Yang et al. 2014), DeSCI (Liu et al. 2019), utilizing different priors such as total variation, low rank, sparsity prior for video SCI but suffering from either the slow reconstruction speed or poor reconstruction quality. Recently, several deep learning based methods have achieved performances comparable with traditional methods while being significantly faster at the inference stage. Specifically, the deep learning based approaches (Qiao et al. 2020b) have been explored such as fully connected network based method (Iliadis et al. 2018), end-to-end convolutional neural network (CNN) U-net (Qiao et al. 2020b), deep tensor based ADMM-net (Ma et al. 2019), hardware constraints based deep neural network (DNN) (Yoshida et al. 2018), RNN-based BIRNAT (Cheng et al. 2020), to reconstruct video images with higher quality and higher speed. In this paper, encouraged by those works, we develop an end-to-end framework for joint spatial and temporal compressive imaging reconstruction with an exemplar system developed in Qiao et al. (2020a).

Spatial and Temporal SCI
Instead of deploying multiple video SCI cameras by increasing the memory, bandwidth as well as the latency, the joint spatial (multi-view) and temporal compressive imaging provides a low-cost, low-bandwidth and low-memory requirement imaging system by capturing high-speed motions of multi views within a snapshot, which is in demand for the emerging applications such as traffic surveillance, sports photography and self-driving cars (Lu et al. 2020; Qiao et al. 2020a). For example, for some important vision tasks such as autonomous navigation, site modelling and surveillance for robotics, conventional cameras with a limited FoV often lose sight of objects when their bearings change suddenly due to a significant turn of the observer (e.g., a robot), the object, or both (Spacek 2005). In contrast, multi-view sensors can track objects more robustly meanwhile enjoying the advantages of low cost and feasibility in hardware. For spatial (not limited to FoV) compressive imaging, besides modulating each scene with a shifted version of the whole aperture as used in SSTCI, another intuitive method is partitioning a coded aperture to different blocks and then merging them together, but this method is usually too expensive or even infeasible to realize in hardware. Moreover, a quantization with entropy coding approach based on frame skipping was adopted in Angayarkanni et al. (2019) to achieve efficient multi-view wireless video compression based on CS. In addition, a lensless camera was proposed in Nakamura et al. (2019) with a novel design to capture the front and back sides simultaneously, where the object-side sensor works as a coding mask and the other one as a sparsified image sensor. In summary, storage and transmission of multi-view video sequences involve large volumes of redundant data, which can be efficiently compressed by the SSTCI system (hardware) and then resolved using a decoder (software). By performing the compressive sensing-based image reconstruction, the captured coded image can be decoded computationally. This paper aims to develop an efficient and effective end-to-end deep learning based decoder for multi-view video SCI reconstruction.

Optical Flow
Optical flow is a long-standing vision task aiming to estimate the per-pixel motion between video frames, which provides a plausible motion information facilitating lots of downstream tasks such as video alignment (Caballero et al. 2017), video editing (Xu et al. 2019), and video analysis (Cheng et al. 2017; Ng et al. 2015). Optical flow estimation has traditionally been treated as an energy minimization problem and recently shows a promising alternative trend by deep learning methods being significantly faster at inference stage. One of the milestone works of deep learning for optical flow estimation is FlowNet (Dosovitskiy et al. 2015), which is based on an U-net auto-encoder and achieves promising performance. Following this work, more architectures for optical flow estimation have been evolved in recent years, yielding better results with faster inference time, such as FlowNet2 (Ilg et al. 2017), PWCNet (Sun et al. 2018) and LiteFlowNet (Hui et al. 2018), RAFT (Teed and Deng 2020). These methods typically adopt an iterative updating approach training with the synthetic datasets and involve operators like cost volume, pyramidal features, and backward feature warping. Taking both the efficiency and accuracy into consideration, we employ the FlowNet2 (Ilg et al. 2017) as our optical flow estimator, while the optical flow is fed into the recurrent neural network for SCI reconstruction as the explicit motion guidance, which has not been studied by any previous video SCI approaches.

Review of the Snapshot Spatial-Temporal Compressive Imaging System in Qiao et al. (2020a)
Hardware Principle
Fig. 1
figure 1
Optical setup of SSTCI (Qiao et al. 2020a). O1, O2: objects from two views; L1, L2, L3, L4: lens; M: mirror; BS: beamsplitter; PBS: polarizing beamsplitter; DMD: digital micromirror device. M1 and M2 have slightly different orientations relative to their incident beams to introduce a lateral displacement (thus imposing different sets of masks on the scenes from two different views) between the modulated images of O1 and O2 on the camera

Full size image
Our SSTCI system was presented in Qiao et al. (2020a), which uses a single camera to simultaneously capture video streams from two scenes. Between the camera and the scenes, we deploy a single DMD to apply distinguishing spatial modulations (i.e., 2D random-binary masks) to different video frames for both scenes in a manner of element-wise product. As shown in Fig. 1, the hardware innovation in Qiao et al. (2020a) is to introduce a polarization-dependent transverse displacement of the two views on the detector plane, which is implemented by the polarized beamsplitter (to impose different polarization states onto the scenes from different views) and two mirrors, with different orientations (to shift the modulation patterns). It is worth noting that different views can be separated as long as their respective coding patterns are uncorrelated to each other. This condition is met in the SSTCI system in which the shifted versions of the random pattern are mutually uncorrelated as long as the shifting amount is greater than the pattern feature size (size of each random element). In this case, we have achieved two different sets of masks with each set for the scene of one view, via only using a single DMD and a single camera. The system thus did not scarify the spatial resolution of the DMD and camera and can capture two full views of the scene simultaneously. A single camera is used to capture both the SCI measurement and side information in Yuan et al. (2017), which can potentially be used to capture two views of the scene, but the solution used therein scarifies the spatial resolution of the camera by half.

Fig. 2
figure 2
Principle of dual-view video SCI (left) and the proposed network for reconstruction (right). Two FoV dynamic scenes, shown as two branches of images (view 1 and 2) at different timestamps, pass through two sets of dynamic aperture (produced by the same set of masks in Fig. 1), which imposes individual coding patterns. The coded frames from both FoVs are then integrated over time on a camera, forming a single-frame compressed measurement. This measurement is fed into our proposed model (right) to reconstruct the high-speed video frames of two dynamic scenes

Full size image
The modulated data streams from the two scenes are then laterally superposed (via beamsplitters) and temporally integrated within one exposure period of the camera, forming a single frame of raw measurement. In this paper, we aim to propose a reconstruction method to resolve the videos of two scenes from the single measurement. To the best of our knowledge, this is the first deep learning based reconstruction method reported in the literature for joint FoV and temporal CS system and has the potential to extend to multiple views, which will be beneficial to practical robotics and 3D applications (for example, for the left view and right view in the stereo imaging) with a fast inference speed, enjoying the advantages of low-bandwidth, low-memory cost and fast inference.

Mathematical Model of Dual-View SCI
As depicted in the left part of Fig. 2 (and also in Fig. 1), in the dual-view video SCI system, we denote object O1 with B temporal channels and ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦ pixels in the transverse plane as ğ‘‹ğ‘‹1âˆˆâ„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦Ã—ğµ , which is modulated by the coding patterns ğ¶ğ¶1âˆˆâ„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦Ã—ğµ, correspondingly. Let ğ‘‹ğ‘‹2âˆˆâ„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦Ã—ğµ denotes object O2, modulated by shifted coding patterns ğ¶ğ¶2âˆˆâ„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦Ã—ğµ (this shifting is introduced by the optical design in Fig. 1). The single-shot measurement ğ‘Œğ‘Œâˆˆâ„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦ is given by

ğ‘Œğ‘Œ=âˆ‘ğ‘=1ğµ(ğ‘‹ğ‘‹ğ‘1âŠ™ğ¶ğ¶ğ‘1+ğ‘‹ğ‘‹ğ‘2âŠ™ğ¶ğ¶ğ‘2)+ğºğº,
(1)
where ğºğºâˆˆâ„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦ represents the noise and âŠ™ denotes the Hadamard (element-wise) product. The frontal slices ğ‘‹ğ‘‹ğ‘1, ğ¶ğ¶ğ‘1, ğ‘‹ğ‘‹ğ‘2, ğ¶ğ¶ğ‘2 with dimension of â„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦ denote the b-th video frame and the corresponding coding pattern imposed on O1 and O2, respectively.

Let ğ‘‹ğ‘‹=[ğ‘‹ğ‘‹1,ğ‘‹ğ‘‹2]ğ‘3âˆˆâ„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦Ã—2ğµ and ğ¶ğ¶=[ğ¶ğ¶1,ğ¶ğ¶2]ğ‘3 âˆˆâ„ğ‘›ğ‘¥Ã—ğ‘›ğ‘¦Ã—2ğµ, where [â‹…]ğ‘3 denotes matrix concatenation operation in the third (temporal) dimension. Then (1) can be simplified as:

ğ‘Œğ‘Œ=âˆ‘ğ‘=12ğµğ‘‹ğ‘‹ğ‘âŠ™ğ¶ğ¶ğ‘+ğºğº.
(2)
The formulation in (2) can be expressed by the following vectorized linear equation:

ğ‘¦ğ‘¦=ğ›·ğ›·ğ‘¥ğ‘¥+ğ‘”ğ‘”,
(3)
where ğ‘¦ğ‘¦=Vec(ğ‘Œğ‘Œ)âˆˆâ„ğ‘›ğ‘¥ğ‘›ğ‘¦ denotes the vectorized measurement, ğ‘”ğ‘”=Vec(ğºğº)âˆˆâ„ğ‘›ğ‘¥ğ‘›ğ‘¦ the vectorized noise and ğ‘¥ğ‘¥=Vec(ğ‘‹ğ‘‹)âˆˆâ„2ğ‘›ğ‘¥ğ‘›ğ‘¦ğµ the desired signal. Different from traditional compressive sensing (Donoho 2006), this sensing matrix ğ›·ğ›·âˆˆâ„ğ‘›ğ‘¥ğ‘›ğ‘¦Ã—2ğ‘›ğ‘¥ğ‘›ğ‘¦ğµ in dual-view video SCI is sparse and follows a diagonal structure

ğ›·ğ›·=[diag(Vec(ğ¶ğ¶1)),â€¦,diag(Vec(ğ¶ğ¶2ğµ))].
(4)
Consequently, the compressive sampling rate is equal to 12ğµ. Theoretical results have been derived recently in Jalali and Yuan (2019) considering this special sensing matrix.

Proposed Framework
To reconstruct the high-speed frames of two FoVs {ğ‘‹ğ‘‹ğ‘1}ğµğ‘=1 and {ğ‘‹ğ‘‹ğ‘2}ğµğ‘=1, we propose an overall architecture of dual video CS reconstruction framework as shown in the right part of Fig. 2. The proposed model consists of three components: (1) A diversity amplifier to distinguish two scenes. (2) A dual-net separator to reconstruct each FoV with coarse resolution. (3) An RNN in conjunction with bidirectional optical flow to exploit motion information for improving the resolution and refining the reconstructed video frames. The whole network is trained in an end-to-end manner.

Diversity Amplifier
The first step in our proposed network is to distinguish two scenes in the single snapshot measurement. Firstly, we normalize the original measurement ğ‘Œğ‘Œ to balance the integrated energy by taking all the coding patterns into consideration. Recalling the definition of ğ‘Œğ‘Œ in (2), various pixels of ğ‘Œğ‘Œ may gather different numbers of frames from {ğ‘‹ğ‘}2ğµğ‘=1 as the integrated energy according to {ğ¶ğ‘}2ğµğ‘=1, which means some pixels acquire with large energy but some others with low energy. In other words, the integrated energy of each pixel is not only related to the value of corresponding position of original scene, but also the corresponding position of the coding patterns. To alleviate the influence of imbalanced energy and encourage the network to focus on the video content with less disturbs from coding patterns, we normalize the original measurement as follows:

ğ‘Œğ‘Œâ¯â¯â¯â¯â¯=ğ‘Œğ‘ŒâŠ˜(âˆ‘ğ‘=12ğµğ¶ğ¶ğ‘/(2ğµ)),
(5)
where âŠ˜ denotes the matrix dot (element-wise) division, and the matrix âˆ‘2ğµğ‘=1ğ¶ğ¶ğ‘/(2ğµ) refers to the averaged coding patterns with the same size as ğ‘Œğ‘Œ, which can be regarded as the corresponding weights of each pixel integrated into the measurement ğ‘Œğ‘Œ. Figure 3 shows the illustrations of both the original measurement ğ‘Œğ‘Œ and the normalized measurement ğ‘Œğ‘Œâ¯â¯â¯â¯â¯ by imposing the energy normalization. In short, ğ‘Œğ‘Œâ¯â¯â¯â¯â¯ can be regarded as an approximated averaging image of all the 2B high-speed frames {ğ‘‹ğ‘‹ğ‘1}ğµğ‘=1 and {ğ‘‹ğ‘‹ğ‘2}ğµğ‘=1, hoping to normalize the imbalanced energy brought by coding patterns and facilitate the dual-view reconstruction. However, the approximated averaging image of all high-speed frames is not sufficient for dual-view SCI, because ğ‘Œğ‘Œâ¯â¯â¯â¯â¯ averages two scenes without discrimination, expressing the same content of different views. Thus, to assist the separation of dual views, we develop a diversity amplification preprocessing method by taking different coding patterns of each scene into consideration, aiming to enlarge the differences of two scenes at the first step for better dual-view SCI.

Fig. 3
figure 3
Illustration of the original measurement ğ‘Œğ‘Œ (top-left), the normalized measurement ğ‘Œğ‘Œâ¯â¯â¯â¯â¯ (bottom-left), and four diversity amplified images ğ·ğ·1, ğ·ğ·2, ğ·ğ·3, ğ·ğ·4 (middle) and the smoothed varieties îˆ³(ğ·ğ·1),îˆ³(ğ·ğ·2) (right), where we enlarge the details in red boxes for better visualization

Full size image
As depicted in Fig. 3, we first construct two precessing methods to explore the dissimilarities between two scenes as follows:

ğ·ğ·1=ğ‘Œğ‘ŒâŠ˜(âˆ‘ğµğ‘=1ğ¶ğ¶ğ‘1/ğµ),ğ·ğ·2=ğ‘Œğ‘ŒâŠ˜(âˆ‘ğµğ‘=1ğ¶ğ¶ğ‘2/ğµ).
(6)
For ğ·ğ·1, each element of (âˆ‘ğµğ‘=1ğ¶ğ¶ğ‘1/ğµ) represents the mean coding pattern of the first scene, describing how many corresponding pixels of the first scene are integrated into the measurement ğ‘Œğ‘Œ. Here, we use this mean coding pattern to normalize the measurement from the perspective of the first view, which is able to alleviate the influence of the coding patterns of the first view, leaving the non-normalized energy for the second view. As the visualization of ğ·ğ·1 shown in Fig. 3, the first view (bear) is relatively smoother and visually clearer with less artifacts, e.g., the fur on the bearâ€™s back, compared with the second view (swan) shown with obvious noise and artifacts. Similarly, for ğ·ğ·2, the (âˆ‘ğµğ‘=1ğ¶ğ¶ğ‘2/ğµ) refers to the corresponding proportion of the second scene integrated into the measurement ğ‘Œğ‘Œ, which is utilized to normalize the measurement and results in normalized energy for the second view. Specifically, as ğ·ğ·2 shown in Fig. 3, the second scene (swan) is much smoother and has less artifacts than the other one (bear), which can be seen from those non-coincident parts such as the neck and tail of the swan. In summary, element-wise dividing the measurement by individual mean coding pattern of different scenes, the diversity of different views can be amplified by taking each mean coding pattern into consideration. Furthermore, in order to sufficiently explore the detected diversity of ğ·ğ·1,ğ·ğ·2 and obtain the preliminary contours of each view, we develop an elaborated design to further enlarge the differences of dual views, expressed by:

ğ·ğ·3=ğ·ğ·1âˆ’îˆ³(ğ·ğ·1),ğ·ğ·4=ğ·ğ·2âˆ’îˆ³(ğ·ğ·2),
(7)
where îˆ³ is utilized to smooth and blur image ğ·ğ·1, e.g., a Gaussian filter, as shown in Fig. 3. The noise and artifacts of the second scene can be detected by comparing ğ·ğ·1 to its smooth variety îˆ³(ğ·ğ·1), denoted by ğ·ğ·3. As we can see from Fig. 3, by subtracting the smooth variety îˆ³(ğ·ğ·1) from ğ·ğ·1, the position in ğ·ğ·1 with non-normalized energy is typically detected expressed as noise and artifacts, resulting in a contour of the swan as the illustration of ğ·ğ·3. Analogously, ğ·ğ·4 is constructed by detecting the diversity between ğ·ğ·2 and its smoothed version îˆ³(ğ·ğ·2), resulting in a contour of the bear as the illustration of ğ·ğ·4. It can be observed that the diversity between different scenes is amplified, meanwhile, capable of preserving the motionless information such as background and motion trails of different scenes, which is beneficial for reconstructing individual scenes for dual-view video SCI.

Fig. 4
figure 4
Architecture of the dual-net separator

Full size image
Dual-net Separator
Hereby, we construct a dual-net separator (DNS) to generate two videos through two branches, respectively. In order to make full use of the amplified diversity of different views and fuse normalize measurement ğ‘Œğ‘Œ in conjunction with various coding patterns, we construct the input of dual-net separator as:

ğƒğğ’01ğƒğğ’02=[ğ‘Œğ‘Œâ¯â¯â¯â¯â¯,ğ·ğ·1,ğ·ğ·2,ğ·ğ·3,ğ·ğ·4,ğ‘Œğ‘Œâ¯â¯â¯â¯â¯âŠ™ğ¶ğ¶11,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶21,â€¦,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶ğµ1]ğ‘3,=[ğ‘Œğ‘Œâ¯â¯â¯â¯â¯,ğ·ğ·1,ğ·ğ·2,ğ·ğ·3,ğ·ğ·4,ğ‘Œğ‘Œâ¯â¯â¯â¯â¯âŠ™ğ¶ğ¶12,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶22,â€¦,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶ğµ2]ğ‘3,
(8)
where we approximate the mask-modulated frames of different scenes by element-wise product of normalized measurement ğ‘Œğ‘Œ and the corresponding coding patterns {ğ¶ğ¶ğ‘1/2}ğµğ‘=1 (hereafter, the subscript 1/2 means â€˜1 or 2â€™). As depicted in Fig. 4, the inputs ğƒğğ’01 and ğƒğğ’02 are then fed into two branches of convolution networks with shared architecture but different parameters, respectively. Each network branch is composed of three sub-networks, stated as:

ğ‘‹ğ‘‹Ëœ1=îˆ²ğ‘ğ‘›ğ‘›31([ğƒğğ’21,ğƒğğ’11]ğ‘3),ğƒğğ’21=îˆ²ğ‘ğ‘›ğ‘›21(ğƒğğ’11),ğƒğğ’11=îˆ²ğ‘ğ‘›ğ‘›11(ğƒğğ’01),ğ‘‹ğ‘‹Ëœ2=îˆ²ğ‘ğ‘›ğ‘›32([ğƒğğ’22,ğƒğğ’12]ğ‘3),ğƒğğ’22=îˆ²ğ‘ğ‘›ğ‘›22(ğƒğğ’12),ğƒğğ’12=îˆ²ğ‘ğ‘›ğ‘›12(ğƒğğ’02),
(9)
Fig. 5
figure 5
The architecture of bidirectional recurrent refining network, composed of a forward network (left) recurrently reconstructing each frame forwardly by integrating the corresponding optical flow, and a backward network (right) incorporating with dynamic motion in the reversed order

Full size image
where îˆ²ğ‘ğ‘›ğ‘›11/2 is used to extract the fine-grained characteristics of input ğƒğğ’01/2, consisting of four convolutional layers. When going deeper, the second subnetwork îˆ²ğ‘ğ‘›ğ‘›21/2 containing three convolutional resblocks, is utilized to explore coarse and global features of the input. The third îˆ²ğ‘ğ‘›ğ‘›31/2 has a mirror symmetry structure as îˆ²ğ‘ğ‘›ğ‘›11/2, where fine-to-coarse spatial features of various scales are concatenated together to reconstruct dual videos. In this stage, the two scenes ğ‘‹ğ‘‹Ëœ1={ğ‘‹ğ‘‹Ëœğ‘1}ğµğ‘=1 and ğ‘‹ğ‘‹Ëœ2={ğ‘‹ğ‘‹Ëœğ‘2}ğµğ‘=1 are separated from each other, and then fed into the refining network as the input providing motion and visual information.

Refine Net
To further explore the spatio-temporal details of videos, the optical flow is employed here to improve the smoothness across time. Moreover, we propagate the optical dynamic information bidirectionally, implemented in conjunction with both a forward and a backward RNN to refine video reconstruction in a sequential manner.

Optical Flow Extractor

In order to maintain temporally connected dynamic motions, optical flow is utilized in our work to improve the visual quality of reconstructed results. Considering the efficiency and accuracy, we utilize the FlowNet 2.0 (Ilg et al. 2017) as the optical flow extractor, to facilitate video reconstruction by integrating explicit motion information as a guidance. Our goal is to better explore the dynamic motion between frames bidirectionally. Taking the t-th and the (ğ‘¡+1)-th frame as examples, the optical flow extraction can be expressed as:

ğ¹ğ¹ğ‘¡â†’ğ‘¡+11=Flownet(ğ‘‹ğ‘‹Ëœğ‘¡1,ğ‘‹ğ‘‹Ëœğ‘¡+11),ğ¹ğ¹ğ‘¡+1â†’ğ‘¡1=Flownet(ğ‘‹ğ‘‹Ëœğ‘¡+11,ğ‘‹ğ‘‹Ëœğ‘¡1),ğ¹ğ¹ğ‘¡â†’ğ‘¡+12=Flownet(ğ‘‹ğ‘‹Ëœğ‘¡2,ğ‘‹ğ‘‹Ëœğ‘¡+12),ğ¹ğ¹ğ‘¡+1â†’ğ‘¡2=Flownet(ğ‘‹ğ‘‹Ëœğ‘¡+12,ğ‘‹ğ‘‹Ëœğ‘¡2),
(10)
where {ğ¹ğ¹ğ‘¡â†’ğ‘¡+11}ğµğ‘¡=1 represents the optical flow from ğ‘‹ğ‘‹Ëœğ‘¡1 to ğ‘‹ğ‘‹Ëœğ‘¡+11 of the first video scene, while the ğ¹ğ¹ğ‘¡+1â†’ğ‘¡1 denotes the optical flow in the reverse direction. Analogously, ğ¹ğ¹ğ‘¡â†’ğ‘¡+12 and ğ¹ğ¹ğ‘¡+1â†’ğ‘¡2 refer to the bidirectional motions between adjacent frames of the second video scene.

To make good use of the pre-trained model in Ilg et al. (2017), we initialize the weights of Flownet from the released pre-trained model, and jointly fine-tune the parameters under the supervision of the final reconstruction loss. This is different from previous methods (Perazzi et al. 2017) which utilize the pre-computed optical flow as an additional input, as our model aims to jointly learn useful motion representations particularly for our video SCI task. In the following, we will introduce how to integrate the bidirectional optical flow into the video refining process in a recurrent manner, which establishes reasonable connections to RNN for better video reconstruction.

Bidirectional Recurrent Refining Network After extracting the optical flow of dual videos in both the forward and backward order, we integrate the motion information into a bidirectional RNN to refine the reconstruction results of dual-net separator in a sequential manner, as depicted in Fig. 5, where the left part corresponds to the forward module and the right part refers to the backward module.

Fig. 6
figure 6
The simplified overall architecture

Full size image
Forward Refining Network: The forward refining network takes the output of DNS {ğ‘‹ğ‘‹Ëœğ‘1/2}ğµâˆ’1ğ‘=1 as the initial input, and fuses three additional visual information together including the forward motion information, the amplified diversity, and the memory information of RNN. Then each frame of forward refined video {ğ‘‹ğ‘‹â†’ğ‘1/2}ğµğ‘=2 is reconstructed recurrently. Taking the t-th frame for example, the forward refining process is expressed as:

ğ‘‹ğ‘‹â†’ğ‘¡+11=îˆ¾â†’(ğ»ğ»âˆ’â†’ğ‘¡+11),ğ»ğ»âˆ’â†’ğ‘¡+11=îˆ´â†’([îˆ²ğ‘‹âˆ’â†’âˆ’(ğ‘‹ğ‘‹Ëœğ‘¡1),îˆ²ğ¹âˆ’â†’(ğ¹ğ¹ğ‘¡â†’ğ‘¡+11),îˆ²ğ·âˆ’â†’(ğ·ğ·1:4),îˆ²ğ»âˆ’â†’âˆ’(ğ»ğ»âˆ’â†’ğ‘¡1)]ğ‘3),ğ‘‹ğ‘‹â†’ğ‘¡+12=îˆ¾â†’(ğ»ğ»âˆ’â†’ğ‘¡+12),ğ»ğ»âˆ’â†’ğ‘¡+12=îˆ´â†’([îˆ²ğ‘‹âˆ’â†’âˆ’(ğ‘‹ğ‘‹Ëœğ‘¡2),îˆ²ğ¹âˆ’â†’(ğ¹ğ¹ğ‘¡â†’ğ‘¡+12),îˆ²ğ·âˆ’â†’(ğ·ğ·1:4),îˆ²ğ»âˆ’â†’âˆ’(ğ»ğ»âˆ’â†’ğ‘¡2)]ğ‘3),
(11)
where îˆ¾â†’ is a CNN block, injecting the hidden representations ğ»ğ»âˆ’â†’ğ‘¡+11 to the observation space to output a refined frame ğ‘‹ğ‘‹â†’ğ‘¡+11, and â†’ refers to the forward processing. The hidden representation fuses the visual information of four components together through a CNN resblock denoted by îˆ´â†’, including the t-th frame ğ‘‹ğ‘‹Ëœğ‘¡+11 as a good initialization, the forward optical flow ğ¹ğ¹ğ‘¡â†’ğ‘¡+11 providing the motion information, the amplified diversity ğ·ğ·1:4=[ğ·ğ·1,ğ·ğ·2,ğ·ğ·3,ğ·ğ·4]ğ‘3 emphasizing the difference between two scenes, and the hidden unit ğ»ğ»ğ‘¡1 of RNN offering the memory of the previous frames. In addition, those visual information are firstly embedded by four parallel CNN blocks, respectively, denoted by îˆ²ğ‘‹âˆ’â†’âˆ’, îˆ²ğ¹âˆ’â†’, îˆ²ğ·âˆ’â†’, îˆ²ğ»âˆ’â†’âˆ’, playing the roles of visual feature extractors. Similarly, the video frame ğ‘‹ğ‘‹Ëœğ‘¡2 of the other scene is refined in the same way. While the forward refining network achieves appealing performance, it neglects the dynamic visual information in the reversed order. This encourages us to construct a backward refining network to further improve the reconstruction performance.

Backward Refining Network As illustrated in the right part of Fig. 5, the forwardly refined videos {ğ‘‹ğ‘‹â†’ğ‘¡1}ğµğ‘¡=1 and {ğ‘‹ğ‘‹â†’ğ‘¡2}ğµğ‘¡=1 are firstly reversed as {ğ‘‹ğ‘‹â†’ğ‘¡1}1ğ‘¡=ğµ and {ğ‘‹ğ‘‹â†’ğ‘¡2}1ğ‘¡=ğµ, and then taken as the inputs of backward refining network. The structure of backward refining network is similar to the forward one, with the main difference on the reversed input and motion information. Due to the opposite order, the (ğ‘¡+1)-th frame and the optical flow from ğ‘‹ğ‘‹â†’ğ‘¡+11/2 to ğ‘‹ğ‘‹â†’ğ‘¡1/2 are utilized to improve the t-th frame, stated as:

ğ‘‹ğ‘‹â†ğ‘¡1=îˆ¾âƒ– (ğ»ğ»â†âˆ’ğ‘¡1),ğ»ğ»â†âˆ’ğ‘¡1=îˆ´âƒ– ([îˆ²ğ‘‹â†âˆ’âˆ’(ğ‘‹ğ‘‹â†’ğ‘¡+11),îˆ²ğ¹â†âˆ’(ğ¹ğ¹ğ‘¡+1â†’ğ‘¡1),îˆ²ğ·â†âˆ’(ğ·ğ·1:4),îˆ²ğ»â†âˆ’âˆ’(ğ»ğ»â†âˆ’ğ‘¡+11)]ğ‘3),ğ‘‹ğ‘‹â†ğ‘¡2=îˆ¾âƒ– (ğ»ğ»â†âˆ’ğ‘¡2),ğ»ğ»â†âˆ’ğ‘¡2=îˆ´âƒ– ([îˆ²ğ‘‹â†âˆ’âˆ’(ğ‘‹ğ‘‹â†’ğ‘¡+12),îˆ²ğ¹â†âˆ’(ğ¹ğ¹ğ‘¡+1â†’ğ‘¡2),îˆ²ğ·â†âˆ’(ğ·1:4),îˆ²ğ»â†âˆ’âˆ’(ğ»ğ»â†âˆ’ğ‘¡+12)]ğ‘3),
(12)
where â† refers to the backward processing, and all the feature extractors have exactly the same architectures corresponding to the forward refining network, but without sharing parameters. As a result, the reconstructed videos of two scenes are improved by integrating the opposite dynamic information, which measures the spatio-temporal shifts of the intensity in a reversed order. Overall, the bidirectional designation has three benefits: (1) the forwardly refined results are more accurate and able to provide more detailed motion and visual information; (2) the backward optical flow is informative to provide the reversed motion for the predicting frame; (3) the bidirectional mechanism provides more gradient propagation paths when training with the gradient descent algorithm, which helps to jointly optimize the forward and backward reconstruction.

Simplified Overall Architecture
Although the architecture composed of both forward and backward refining network improves reconstruction results bidirectionally, the drawback is its large computational cost and huge memory requirement. To address this challenge, and aiming to save half of the required memory and take the advantages of the bidirectional motion information, we further condense the forward and backward refining network together, as shown in Fig. 6. Firstly, the outputs of DNS {ğ‘‹ğ‘‹Ëœğ‘¡1/2}ğµğ‘¡=1 are fed into the flownet to extract the optical flow {ğ¹ğ¹ğ‘¡â†’ğ‘¡+11}ğµâˆ’1ğ‘¡=1 in the forward order. Similarly, the reversed {ğ‘‹ğ‘‹Ëœğ‘¡1/2}1ğ‘¡=ğµ are fed into the flownet to obtain the optical flow {ğ¹ğ¹ğ‘¡+1â†’ğ‘¡1}1ğ‘¡=ğµâˆ’1 in the backward order. Then the bidirectional motion information is taken as the input of RNN, stated as:

ğ‘‹ğ‘‹Ë†ğ‘¡+11=îˆ¾Ë†(ğ»ğ»Ë†ğ‘¡+11),ğ‘‹ğ‘‹Ë†ğ‘¡+12=îˆ¾Ë†(ğ»ğ»Ë†ğ‘¡+12),ğ»ğ»Ë†ğ‘¡+11=îˆ´Ë†([îˆ²ğ‘‹Ë†(ğ‘‹ğ‘‹ğ‘¡1Ëœ),îˆ²ğ¹â†’Ë†(ğ¹ğ¹ğ‘¡â†’ğ‘¡+11),îˆ²ğ¹âƒ– Ë†(ğ¹ğ¹ğ‘¡+1â†’ğ‘¡1),îˆ²ğ·Ë†(ğ·ğ·1:4),îˆ²ğ»Ë†(ğ»ğ»Ë†ğ‘¡1)]ğ‘3),ğ»ğ»Ë†ğ‘¡+12=îˆ´Ë†([îˆ²ğ‘‹Ë†(ğ‘‹ğ‘‹ğ‘¡2Ëœ),îˆ²ğ¹â†’Ë†(ğ¹ğ¹ğ‘¡â†’ğ‘¡+12),îˆ²ğ¹âƒ– Ë†(ğ¹ğ¹ğ‘¡+1â†’ğ‘¡2),îˆ²ğ·Ë†(ğ·ğ·1:4),îˆ²ğ»Ë†(ğ»ğ»Ë†ğ‘¡2)]ğ‘3),
(13)
where the main modification is that the two CNN blocks îˆ²ğ¹ğ¹â†’Ë† and îˆ²ğ¹ğ¹â†Ë† are simultaneously utilized to inject bidirectional optical flow into the hidden space, which facilitates reconstruction of the (ğ‘¡+1)-th frame by predicting from the dynamic information ğ¹ğ¹ğ‘¡â†’ğ‘¡+11/2 and the backward reasoning from ğ¹ğ¹ğ‘¡+1â†’ğ‘¡1/2. All the feature extractors {îˆ¾Ë†,îˆ´Ë†,îˆ²ğ‘‹Ë†,îˆ²ğ¹â†’Ë†,îˆ²ğ¹âƒ– Ë†Ë†,îˆ²ğ·Ë†,îˆ²ğ»Ë†} have the same architectures as introduced in the forward and backward refining network. We find that this framework is able to save half the memory with little performance degradation, taking the advantages of both forward and backward optical flows.

To sum up, the whole network is composed of the dual-net separator and the simplified bidirectional recurrent refining network, as shown in Fig. 6. To minimize the reconstruction error of all the frames of dual view, we employ the mean square error (MSE) as the loss function for training the end-to-end deep network, stated as:

îˆ¸=ğ›¼îˆ¸Ëœ+îˆ¸Ë†,îˆ¸Ëœ=âˆ‘ğµğ‘¡=1â€–â€–ğ—Ëœğ‘¡1âˆ’ğ—ğ‘¡1â€–â€–22+âˆ‘ğµğ‘¡=1â€–â€–ğ—Ëœğ‘¡2âˆ’ğ—ğ‘¡2â€–â€–22,îˆ¸Ë†=âˆ‘ğµğ‘¡=1â€–â€–â€–ğ—Ë†ğ‘¡1âˆ’ğ—ğ‘¡1â€–â€–â€–22+âˆ‘ğµğ‘¡=1â€–â€–â€–ğ—Ë†ğ‘¡2âˆ’ğ—ğ‘¡2â€–â€–â€–22,
(14)
where îˆ¸Ëœ and îˆ¸Ë† represent the MSE loss of dual-net separator and refine net, respectively, and ğ›¼ is a trade-off parameter, which is set to 1 in our experiments. During testing, we can achieve high-quality reconstructed videos in a short time with the well-learned network parameters.

Experiments
In this section, we compare the proposed optical flow-aided recurrent neural network (OFaNet) with other methods on both simulation and real datasets for dual-view SCI, and further extend it to single-view SCI for a broader scope.

Datasets, Training and Counterparts
Datasets: We first evaluate the proposed OFaNet on our collected six simulation data including Bear&Blackswan, Boy&Girl, Running Cars, Bike&Bus, Cow&Dog and Hike &Hockey, respectively, where ğµ=10 video frames of two individual 256Ã—256 scenes (i.e. totally 20 frames) are compressed into a single measurement for each dataset. We also demonstrate experiments on simulation data with different compressive rates (ğµ={6,14}). To evaluate the effectiveness of the proposed model in a broader scope, we also demonstrate experiments on the single-view video SCI on six widely used grayscale simulation datasets including Kobe, Runner, Drop, Traffic, Aerial and Vehicle, where 8 video frames are compressed into a single measurement. Furthermore, we also evaluate OFaNet on two real dual-view datasets with ğµ=10, one real dual-view dataset with ğµ=20 (Qiao et al. 2020a), captured by the real SSTIC camera, and three real single-view datasets.

Implementation Details: For simulation, We randomly crop patch cubes 256Ã—256Ã—ğµ from original scenes in DAVIS2017 (Pont-Tuset et al. 2017) and randomly select two video sequences at each time to be modulated with different patterns, and obtain the training dataset containing about 32,000 data pairs. We set the maximum training epoch as 90, the batch size as 2, and start training with the initial learning rate of 3Ã—10âˆ’4, which is decreased by 10% every 10 epochs. Our network is implemented in Pytorch and trained with the Adam optimizer 0. The experiments are conducted on a NVIDIA RTX 8000 GPU, and it takes about 3 days to complete the training of the entire network. For the real SSTIC dataset with the size of 650 Ã— 650 Ã— 10 (ğµ=10), we randomly crop patch cubes (650 Ã— 650 Ã— 10) from the same training dataset DAVIS2017, and obtain about 12000 data pairs for training with the initial learning rate of 2Ã—10âˆ’5. Similarly, we generate 12000 data pairs with the size of 650 Ã— 650 Ã— 20 for training another real dataset with ğµ=20. The detailed architecture of OFaNet is presented in Table 1.

Table 1 Network architecture of the proposed OFaNet
Full size table
Counterpart Methods and Performance Metrics: As mentioned before, the PnP-TV-FFD algorithm (Qiao et al. 2020a) has been proposed for dual-view SCI reconstruction. Further more, GAP-TV (Yuan 2016) is a widely used efficient baseline, which is able to provide decent results within short time. The previous state-of-the-art optimization based algorithm DeSCI (Liu et al. 2019) is also employed to dual-view video SCI as a strong baseline, however, which always suffers from slow reconstruction speed. For comparison with the deep learning based methods, we employ the U-net (Qiao et al. 2020b; Ronneberger et al. 2015), ADMMnet (Ma et al. 2019) and state-of-the-art network BIRNAT (Cheng et al. 2020) to the dual-view SCI tasks for comparison. In the following, we compare our proposed method against these six methods.

GAP-TV (Yuan 2016): This algorithm models the inverse problem of video SCI as a total variation minimization problem. It aims to solve

ğ‘¥ğ‘¥Ë†=argminğ‘¥ğ‘¥â€–TV(ğ‘¥ğ‘¥)â€–, s.t.   ğ‘¦ğ‘¦=ğ›·ğ›·ğ‘¥ğ‘¥,
(15)
where TV(ğ‘¥) indicates the total variation of the signal, imposing the sparsity on the gradient of signal. GAP-TV employs 100 iterations for the dual-view SCI video reconstruction.

PnP-TV-FFD (Qiao et al. 2020a): This algorithm utilizes the deep denoising priors in the plug-and-play framework for efficient reconstruction, which solves the following problem

ğ‘¥ğ‘¥Ë†=argminğ‘¥ğ‘¥12â€–ğ‘¦ğ‘¦âˆ’ğ›·ğ›·ğ‘¥ğ‘¥â€–22+ğœ†TV(ğ‘¥ğ‘¥)+ğœŒğ›©ğ›©(ğ‘¥ğ‘¥),
(16)
where ğ›©ğ›©(ğ‘¥ğ‘¥) denotes the deep denoising prior, i.e., the fast and flexible denoising network (FFDNet) (Zhang et al. 2018). PnP-TV-FFD uses 500 iterations for the best performance.

DeSCI (Liu et al. 2019): This SCI reconstruction algorithm boosted the reconstruction quality of single-view SCI, which imposes a weighted nuclear norm on the mathematical model of SCI system. DeSCI integrates the nonlocal self-similarity of videos and the rank minimization with the SCI decoding problem. By applying the nuclear norm of nonlocal similar patch-groups in the video frames, the signals can be recovered with the minimized rank under the alternating direction method of multipliers (ADMM) framework. The iteration number Max-Iter is fixed to 60 as suggested in Liu et al. (2019).

U-net (Qiao et al. 2020b): This deep learning based network utilizes a specially designed CNN architecture to capture the local structures for reconstructing video in an end-to-end manner. For fair comparison, we use the same training datasets as utilized in our proposed model, and employ the same hyperparameters as set in the original paper. We train 80 epochs for U-net on a NVIDIA RTX 8000 GPU, and it takes about 1.5 days to complete the training of the entire network.

ADMM-net (Ma et al. 2019): A deep tensor ADMM-Net provides high-quality decoding for video SCI systems in seconds. This network unfolds inference iterations of a standard tensor ADMM algorithm into a layer-wise structure based on deep neural network, which learns the domain of low-rank tensor through computationally efficient training. We train 50 epochs until the performance converges.

BIRNAT (Cheng et al. 2020): This method firstly employs the recurrent networks to SCI and achieves state-of-the-art performance for single-view SCI reconstruction. BIRNAT employs a deep CNN with a self-attention map to reconstruct the first frame, then a forward RNN and a backward RNN to generate the following frames in a sequential manner. BIRNAT also utilizes the adversarial training for performance improvement. We train BIRNAT on an NVIDIA RTX 8000 GPU with the learning rate 1Ã—10âˆ’4 till the performance is convergent, which takes about 12 days for training due to the doubled compression rate corresponding to dual-view SCI.

Table 2 The average results of PSNR in dB (left entry in each item) and SSIM (right entry in each item) and running time per measurement/shot in seconds by different algorithms on dual-view simulation datasets
Full size table
We evaluate the above algorithms and our proposed OFaNet on six simulation datasets and three real datasets captured by the SSTCI system (dual-view SCI). Furthermore, we also employ the proposed OFaNet and the comparison methods to the single-view SCI system, evaluating the performances on six benchmark simulation datasets and three real datasets captured by real single-view video SCI cameras (Llull et al. 2013; Qiao et al. 2020b), to validate the effectiveness of OFaNet in a broader scope.

Both peak-signal-to-noise ratio (PSNR) and structural similarity (SSIM) (Wang et al. 2004) are used as metrics for assessing the performance on simulation datasets. PSNR is the ratio between the maximum power and the power of residual errors from the â€œreferenceâ€, while SSIM quantifies the visual quality by considering the structural similarity, which is more consistent with the human visual perception. Besides, we measure the running time of video reconstruction at the testing stage to evaluate the applicability in real-time applications.

Simulation Data
We first test the algorithms on six dual-view simulated data: Bear&Blackswan, Boy&Girl, Running Cars, Bike&Bus, Cow&Dog and Hike&Hockey. The performance comparisons on the six benchmark datasets are summarized in Table 2, using different algorithms, i.e., GAP-TV, PnP-TV-FFD, DeSCI, U-net, ADMMnet, BIRNAT and OFaNet. The corresponding visualization results of selected reconstructed frames are shown in Fig. 7 with full reconstructed videos shown in the supplementary material (SM). It can be observed that:

(i) OFaNet leads to the best performance compared to other algorithms on all the six datasets. In specific, the average gains of OFaNet over {GAP-TV, PnP-TV-FFD, DeSCI, U-net, ADMM-net and BIRNAT} are as much as {2.63, 2.58, 0.61, 2.15, 0.98, 1.19}dB on PSNR and {0.07, 0.10, 0.03, 0.12, 0.07, 0.05} on SSIM. Intuitively, OFaNet provides superior performance owing to the motion smoothness provided by the optical flow and the elaborately designed diversity amplifier which better fits for the target task.

(ii) Selected reconstructed frames are shown in Fig. 7. It can be observed that severe structured artifacts and noise are shown in the reconstructed images of GAP-TV and PnP-TV-FFD caused by the incomplete separation of dual scenes from the single measurement. For example, the vague shape of the bus can be seen from the reconstructed bike, which leads to severe image distortion. DeSCI smooths out some details in the reconstructed video, and results in some artifacts making the reconstructed video more visually â€™unrealisticâ€™. Unet results in some white spots randomly located in the reconstructed images, and the edges and corners are blurry. Although the BIRNAT has lower PSNR than ADMM-net, the reconstructed images of BIRNAT are visually cleaner than ADMM-net, corresponding to higher SSIM and contributing to its powerful representative ability. In general, the proposed OFaNet provides finer details and clearer contours, owing to the amplified diversity and good representative ability of the DNS and refine net.

Fig. 7
figure 7
Reconstructed results of different algorithms on six simulated dual-view SCI datasets. Left: measurements, right: selected reconstructed frames from View 1 and View 2 using different algorithms

Full size image
(iii) Interestingly, compared with GAP-TV, the methods PnP-TV-FFD and U-net both obtain higher PSNR but lower SSIM, which result in the image distortions in Fig. 7 as these so-called natural scenes look â€™unnaturalâ€™. Similarly, the ADMM-net also results in higher PSNR but lower SSIM than BIRNAT, corresponding to a blurred visualization. It can be seen that the proposed OFaNet gains improvements both on PSNR and SSIM and provides cleaner and sharper reconstruction corners, which might contribute to the fine-to-cross structure and the sequential dependencies constructed by RNN.

(iv) Although the deep learning based methods (U-net, ADMM-net, BIRNAT and OFaNet) require more time for training, these methods are able to reconstruct videos within 1 second at the testing stage, which are significantly faster than the iterative optimization based algorithms. It is worth noting that our proposed OFaNet achieves more than 40,000 times faster than DeSCI ( the runner-up on PSNR and SSIM) at the testing stage. Although U-net is the fastest method, it suffers from poor reconstruction quality as shown in Fig. 7. Thus OFaNet is a pretty good compromise considering the trade-off between reconstruction quality and time.

Optical Flow To further investigate the influence of optical flow, we illustrate the flow map on two simulation datasets in Fig. 8. We can see that a jointly trained optical flow extractor learns informative task-specific features to promote video reconstruction. It can be observed that the object contours and both the global and local motions are well represented in the optical flow, which gives RNN a helpful guidance for reconstructing the next frame. Both the forward and backward optical flows are very informative and capable to explicitly introduce the dynamic motion into the reconstruction of each adjacent frame, and both the global motions might caused by camera movements and the locally varying motions of various pixel-wise displacement can be detected as shown in the optical flows. This is beneficial for modeling motions and reducing the artifacts near occlusion boundaries and smoothing the change of moving object across video frames. Moreover, in the ablation study, we will quantitatively analyze the influence of integrating the information implied in optical flow.

Frame-wise Quality In order to analyze the reconstruction performance recurrently, we plot the frame-wise PSNR and SSIM curves of different algorithms in Fig. 9. It can be observed that OFaNet outperforms other counterparts at most of the frames on both PSNR and SSIM. Interestingly, the performance curves of OFaNet seem to follow an up-and-down trend on both PSNR and SSIM, which might due to the recurrent mechanism and the bidirectional optical flow. More specifically, the bidirectional optical flow is degraded into single-direction unavoidably when reconstructing the first and last frames, which results in relatively low quality at these frames and leads to an up-and-down trend. This further demonstrates the effectiveness of our integrated bidirectional optical flow.

Fig. 8
figure 8
Illustration of the fine-tuned optical flow on two simulation datasets. a b a pair of consecutive video frames with moving objects of Bear&Blackswan; c the forward optical flow between adjacent frames (a)-(b); d the backward optical flow between adjacent frames (b)-(a); e f a pair of consecutive video frames with moving objects of Hike&Hockey; g the forward optical flow between adjacent frames (e)-(f); h the backward optical flow between adjacent frames (f)-(e)

Full size image
Fig. 9
figure 9
Frame-wise reconstruction quality on PSNR (left) and SSIM (right) by different algorithms on dataset Bear&Blsckswan

Full size image
Ablation Study To demonstrate the effectiveness of each module in OFaNet, we conduct experiments on the six dual-view simulation datasets using various versions of OFaNet, as listed in Table 3, where the abbreviations of different variants of OFanet are shown in the first row, specifically corresponding to:

OFaNet without diversity amplifier (W/o DA) excludes the diversity amplified images ğ·ğ·1,ğ·ğ·2,ğ·ğ·3,ğ·ğ·4 from the entire reconstruction framework. Thus, the input of â€˜Dual-net Separatorâ€™ is changed to ğƒğğ’01=[ğ‘Œğ‘Œâ¯â¯â¯â¯â¯,ğ‘Œğ‘Œâ¯â¯â¯â¯â¯âŠ™ğ¶ğ¶11,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶21,â€¦, ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶ğµ1]ğ‘3 and ğƒğğ’02=[ğ‘Œğ‘Œâ¯â¯â¯â¯â¯,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶12,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶22,â€¦,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶ğµ2]ğ‘3, and the input îˆ²ğ·Ë†(ğ·ğ·1:4) in (13) is removed from the bidirectional refining network. As shown in Table 3, the diversity amplifier leads to average improvements both on PSNR (0.57) and SSIM (0.04) of six datasets with neglectable computational cost. This demonstrates the effectiveness and efficiency of diversity amplifier for separating and reconstructing dual videos.

OFaNet without dual-net separator (W/o DNS) is utilized to evaluate the effectiveness of the dual-branches framework. Considering the optical flow can only be well estimated based on the coarse reconstruction results, it is not appropriate to directly remove the dual-net separator. Thus, we replace the dual-branches network with the single-branch network with the same architecture but the different output channel at the last layer. To be more specific, for the original dual-net separator, we employ two network without sharing parameters to output each video respectively, while the variety â€˜OFaNet W/o Dual-net Separatorâ€™ utilizes a single network to output two videos as a single sequential data cube. Mathematically, the single-branch network can be expressed as:

ğ‘‹ğ‘‹Ëœ=îˆ²ğ‘ğ‘›ğ‘›3([ğƒğğ’2,ğƒğğ’1]ğ‘3),ğƒğğ’2=îˆ²ğ‘ğ‘›ğ‘›2(ğƒğğ’1),  ğƒğğ’1=îˆ²ğ‘ğ‘›ğ‘›1(ğƒğğ’0),ğƒğğ’0=[ğ‘Œğ‘Œâ¯â¯â¯â¯â¯,ğ·ğ·1,ğ·ğ·2,ğ·ğ·3,ğ·ğ·4,ğ‘Œğ‘Œâ¯â¯â¯â¯â¯âŠ™ğ¶ğ¶11,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶21,â€¦,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶ğµ1,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶12,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶22,â€¦,ğ˜â¯â¯â¯â¯âŠ™ğ¶ğ¶ğµ2]ğ‘3,
(17)
where the output ğ‘‹ğ‘‹Ëœ=[ğ‘‹ğ‘‹Ëœ1,ğ‘‹ğ‘‹Ëœ2]ğ‘3 is the concatenation of two reconstructed videos. As we can see from Table 3, the single-branch network results in average descent on PSNR (0.29) and SSIM (0.02) compared to the dual-net separator. Intuitively, the dual-branches network is more appropriate to reconstruct individual video by respectively taking its corresponding coding patterns into consideration, while the computational cost is marginal and acceptable (0.0031s).

OFaNet without refine net (W/o RN) only contains the diversity amplifier and the dual-net separator. On the one hand, it can be seen that this variety provides very limited results compared with OFaNet, which implies the refining network is beneficial to the reconstruction performance, leading to an average improvement on PSNR (1.11) and SSIM (0.07) of six simulated datasets. On the other hand, the reconstruction results of this variety show superiority compared with the basic model U-net, which could verify the effectiveness of our proposed diversity amplifier and dual-net separator. It is worth noticing that this architecture without refine net only cost less than 0.01s for testing, which is applicable with the fast inference speed and a comparable reconstruction performance for the applications with strict time demand such as self-driving systems and motion control.

OFaNet without optical flow (W/o OF) excludes the optical flow from our proposed OFaNet, which can be obversed from Table 3 that the performance degrades severely (0.7 on PSNR and 0.04 on SSIM) compared with OFaNet. The significant contribution of optical flow is verified, where the optical flow helps the reconstruction of each video by feeding the motion information as a guidance. Intuitively, the dynamic motion provides the information to guide the refining network to estimate the output according to the predicted motion, leading to smooth results across time by focusing on the locally varying dynamic object and the global movement in the background.

OFaNet without bidirection (W/o Br) only includes the forward optical flow ğ¹ğ‘¡â†’ğ‘¡+1 without the backward optical flow ğ¹ğ‘¡+1â†’ğ‘¡, and we can see that adding the backward optical flow is beneficial to video reconstruction. The bidirectional dynamic information is explored among adjacent frames both forwardly and backwardly, which can further improve the quality of reconstruction by integrating into RNN.

OFaNet without joint training (W/o JT) utilizes the pre-trained model of  Ilg et al. (2017) and initializes the weights of Flownet from the released pre-trained modelFootnote1 without fine-tuning. We can see from Table 3 that joint training the parameters of the whole framework with mean square error (MSE) loss is beneficial to video reconstruction, which helps the extracted optical flow to better match the video SCI problem.

Diversity Amplifier + Dual-net Separator + Bidirectional Recurrent Refining Network (DA&DNS&BRRN) combines the â€œOFaNet W/o refine netâ€ and a bidirectional recurrent network (a forward and a backward rnn) together for video reconstruction. As shown in Table 3, the reconstruction performance is comparable or even better than OFaNet, but with significantly more computational cost. In specific, both the memory and testing time in each epoch of â€œBidirectional Recurrent Refining Networkâ€ is twice larger than OFaNet, and its training time is much longer than OFaNet. Bigger network parameters also make the model difficult to train and converge. The proposed OFaNet condenses the bidirectional network to save half of the required memory and meanwhile makes full use of the bidirectional motion information.

Table 3 The average results of PSNR in dB (left entry in each cell) and SSIM (right entry in each cell) and running time per measurement/shot in seconds by different variants of our proposed OFaNet on dual-view simulation datasets
Full size table
Table 4 The average results of PSNR in dB, SSIM and running time per measurement/shot in seconds by different algorithms on dual-view simulation datasets with different compression rates
Full size table
Table 5 The average results of PSNR in dB (left entry in each item) and SSIM (right entry in each item) and running time per measurement/shot in seconds by different algorithms on dual-view simulation datasets with different level Gaussian noise
Full size table
In terms of the above oblation study, we can summarize that the final version of OFaNet results in the best performance according to all these above varieties.

Results of different compression rates In order to evaluate the performance of the proposed algorithm dealing with different compression rates, we show the results on six simulation data with compression rates ğµ={6,10,14} for each view. As shown in Table 4, the proposed OFaNet outperforms its counterparts across different compression rates. DeSCI performs well across small compression rates (ğµ={6,10}) but results in low quality with the big compression rate (ğµ=14), which may due to the parameter setting. Moreover, DeSCI takes about 16 hours to reconstruct dual videos with the size of 256Ã—256Ã—14 from a single measurement, which is difficult to support practical applications. In contrast, the proposed OFaNet achieves state-of-the-art performance within 1 second for fast inference. It is worth noting that the variety DA&DNS (i.e. OFaNet without refine net) of the proposed model achieves descent performance in the shortest time (â‰¤ 0.006s), which is a computationally efficient method. For applications with strict time demand such as self-driving systems (Lu et al. 2020) and motion control, the variety network DA&DNS is a better choice to reconstruct videos with reasonably good quality within very fast time.

Table 6 The average results of PSNR in dB (left entry in each item), SSIM (right entry in each item) and running time per measurement/shot in seconds by different algorithms on six single-view grayscale benchmark simulation datasets
Full size table
Robustness to Gaussian Noise In order to evaluate the robustness of the proposed OFaNet, we investigate the effect of Gaussian noise corresponding to different algorithms. Before adding noise to simulation data, all the measurements are normalized to [0, 1]. Then the zero-mean Gaussian noise is added to the measurements with standard deviation ğœ of {0, 0.01, 0.05, 0.1, 0.2}, respectively. For the optimization based algorithms, we retrain the entire framework from scratch. For the deep learning based methods, we directly test on the noisy test data using the previously well-trained parameters. As shown in Table 5, four deep learning based methods are more robust and stable on the noisy data compared with the optimization based methods. In specific, the performance of the runner-up DeSCI drops very fast when the measurement is corrupted (fine tuning the parameters might be able to provide better results but time consuming). Surprisingly, the competitive deep learning method ADMM-net performs even worse than U-net, while the BIRNAT shows robustness on the corrupted data, which might due to its powerful representative capability implied in the huge network and sequential modeling. The performance of the OFaNet degrades slower than others, which indicates the proposed model is more robust to noise and can relieve the effect of Gaussian noise.

Fig. 10
figure 10
Selected reconstruction frames of six single-view grayscale benchmark simulation datasets

Full size image
Results on Single-view Simulated SCI Data

In order to evaluate the proposed method in a broader scope, we further demonstrate the experiments on the single-view system using six widely used benchmark simulated data, i.e. Kobe, Runner, Drop, Traffic, Aerial and Vehicle. The modification of OFaNet is small to extend to single-view SCI, where we remove the diversity amplifier and only keep one branch of the dual-net separator, and utilize the same training data and experiments setting as used in BIRNAT Cheng et al. (2020). The results of different comparison methods are given in Table 6. It can be seen that the proposed OFaNet achieves competitive results, 0.16dB in PSNR higher than the strong baseline DeSCI with about 44,000Ã— shorter testing time. Compared to state-of-the-art method BIRNAT, the proposed OFaNet occupies 2 times lower memory (8934MB) during training than BIRNAT (17748MB), which is important because the big GPU memory consumption will preclude the practical large-scale SCI applications. Figure 10 plots selected reconstruction frames of different algorithms on six datasets. It can be observed that OFaNet achieves comparable visual results compared with the state-of-the-art methods; clean and sharp contours and fine details can be provided by the proposed OFaNet.

Fig. 11
figure 11
Reconstruction results by different algorithms for real SSTCI data popping water balloon punctured by a knife (top three rows) and a falling Ping-Pong ball (bottom three rows)

Full size image
Fig. 12
figure 12
Reconstruction results by different algorithms for real SSTCI data colliding water balloons (top three rows) and flying letters (bottom three rows)

Full size image
Results on Real SCI Data
Lastly, we apply the proposed OFaNet to real data captured by the SSTCI camera. Following the same setting in Qiao et al. (2020a), two FoV videos are encoded into one snapshot measurement. Three datasets are utilized here. The first one is a popping water balloon punctured by a knife and a falling Ping-Pong ball, with the size of 650Ã—650Ã—10 corresponding to each view. The second dataset is composed of two colliding water balloons and two flying letters (650Ã—650Ã—10 for each view). The third video contains 5 pendulum balls and 4 falling dominoes, where each FoV video has the size of 650Ã—650Ã—20. The real captured datasets capture high-speed videos in our daily life with unavoidable noise inside and thus are more challenging to reconstruct. The camera was working at 50 frames per second (fps) during capturing these measurements and thus each measurement corresponds to 20 ms in real life. Therefore, when 10 frames of each view are reconstructed from the single measurement, each frame lasts 2 ms in real life; this is 500fps high speed video. Similarly, when 20 frames of each view are reconstructed from the single measurement, each frame lasts 1 ms in real life and the reconstructed video is of 1000fps.

The measurements and the corresponding reconstructed frames of two real dual-view SCI system with ğµ=10 are demonstrated in Figs. 11 and 12, respectively, with full videos shown in the SM. It can be observed that in both cases, our OFaNet can provide better or at least competitive results with a significant saving on the computational time compared to existing algorithms. The reconstructed videos by GAP-TV have unpleasant artifacts, and PnP-TV-FFD shows blurry boundaries and over-smoothness. PnP-TV-FFD is capable of providing better results on real data compared to the performances on simulation datsets, which might due to the FFD denoiser is more appropriate for real datasets with unavoidable noise. It can also be seen that OFaNet provides sharp boundaries and fine details with less artifacts and fuzziness from the single compressed measurement.

Fig. 13
figure 13
Reconstruction results using different algorithms for the real SSTCI data with ğµ=20: pendulum balls (top six rows) and falling Ping-Pong balls (bottom six rows)

Full size image
Another snapshot measurement of pendulum balls and falling dominoes with ğµ=20 and the corresponding reconstructed results are shown in Fig. 13, with full videos shown in the SM. It can be observed that GAP-TV produces significant noise and unclear boundaries, while PnP-TV-FFD tends to over smooth the moving object. OFaNet is capable of providing relatively clear and distinct pendulum balls, as well as the sharp and straight contours of falling dominoes.

In summary, the reconstruction results of our OFaNet are of higher quality compared with other algorithms, and the inference of our algorithm is significantly faster. This indicates the applicability and efficiency of our proposed OFaNet in the real SSTCI systems.

Fig. 14
figure 14
The reconstructed frames of three single-view real data Wheel, Domino, and Water Balloon

Full size image
Results on Single-view Real SCI Data To evaluate the effectiveness of OFaNet on real applications of the single-view SCI system, we conduct experiments on three real data captured by the SCI cameras (Llull et al. 2013; Qiao et al. 2020b). For the snapshot measurement Wheel, we recover a 256Ã—256Ã—14 high-speed video. In addition, the larger scale snapshot measurements Domino and Water Balloon are recovered as two videos of size 512Ã—12Ã—10. As shown in Fig. 14, it can be seen that GAP-TV introduces unpleasant noise; DeSCI over smooth the details such as the letters in domino; U-net results in blurry edges and more noise such as in the letter â€˜Dâ€™ of Wheel; BIRNAT provides finer details and sharper edges compared with other methods; OFaNet provides relatively clear contours with fewer artifacts and less noise, even though it results in some blurry parts. However, the proposed OFaNet utilizes much less testing time than DeSCI and twice lower GPU memory compared with BIRNAT. These experiments indicate both the applicability and effectiveness of the proposed algorithm in real applications.

Conclusions
We have proposed an efficient deep learning network for the reconstruction of dual-view video snapshot compressive imaging, which implements joint field-of-view and temporal compressive sensing, shedding light to high throughput machine vision systems. Inspired by the hardware encoding principle, we develop a diversity amplifier to enhance the differences of the scenes from two FoVs, and design a dual-net separator to reconstruct two views from the single measurement. Following this, we integrate recurrent mechanism and optical flow into our reconstruction network to achieve competitive results in a short time. Though the network is proposed for dual-view systems, it can be extended to single-view systems and we believe it can work for multi-view systems with moderate modifications. This will pave the way of real applications of SCI systems on robots, self-driving vehicles, etc.