Face sketch synthesis has a wide range of applications in both digital entertainment and law enforcement. State-of-the-art examplar-based methods typically exploit a Probabilistic Graphical Model (PGM) to represent the joint probability distribution over all of the patches selected from a set of training data. However, these methods suffer from two main shortcomings: (1) most of these methods capture the evidence between patches in pixel-level, which lead to inaccurate parameter estimation under bad environment conditions such as light variations and clutter backgrounds; (2) the assumption that a photo patch and its corresponding sketch patch share similar geometric manifold structure is not rigorous. It has shown that deep convolutional neural network (CNN) has outstanding performance in learning to extract high-level feature representation. Therefore, we extract uniform deep patch representations of test photo patches and training sketch patches from a specially designed CNN model to replace pixel intensity, and directly match between them, which can help select better candidate patches from training data as well as improve parameter learning process. In this way, we investigate a novel face sketch synthesis method called DPGM that combines generative PGM and discriminative deep patch representation, which can jointly model the distribution over the parameters for deep patch representation and the distribution over the parameters for sketch patch reconstruction. Then, we apply an alternating iterative optimization strategy to simultaneously optimize two kinds of parameters. Therefore, both the representation capability of deep patch representation and the reconstruction ability of sketch patches can be boosted. Eventually, high quality reconstructed sketches which is robust against light variations and clutter backgrounds can be obtained. Extensive experiments on several benchmark datasets demonstrate that our method can achieve superior performance than other state-of-the-art methods, especially under the case of bad light conditions or clutter backgrounds.

Access provided by University of Auckland Library

Introduction
Face sketch synthesis aims at solving the problem that synthesize a face sketch drawing given a face photo. It is an interesting artistic task with many useful applications. In social networks, a lot of people like to use their own facial photos as profile pictures. Using a sketch version of their facial photos as their avatars can make them more outstanding. However, drawing such sketches usually requires professional painting skills and tedious manual work. Automatically synthesize a face sketch drawing from a face photo is highly necessary. whatâ€™s more, face sketch synthesis can effectively assist law enforcement (Wang et al. 2014; Wang and Tang 2009; Tang and Wang 2004). In many criminal cases, police officers need to retrieve a sketch of a suspect drawn by an artist in a mug-short database to help identifying the suspect. However, face photos and face) sketches have different modalities, making the match between them more difficult. Face sketch synthesis can help narrow down the gap between face photos and face sketches.

Face sketch synthesis has undergone substantial development over the past ten years. Early studies mainly focused on examplar based (data-driven) method which uses sample sketches or sketch patches in training set to directly reconstruct target sketch. The exemplar based face sketch synthesis method generally contains two steps: neighbor selection and reconstruction weight computation. Firstly, each of the training and test images are divided into patches of the same size, with overlapping area between two adjacent patches. Then, some (e.g. K) nearest photo patches of each test photo patch are selected from the training photo patches. Training sketch patches corresponding to these nearest photo patches are taken as the candidates for sketch patch reconstruction. Afterwards, the combination coefficients of nearest photo patches are calculated and used as the weights for sketch patch reconstruction under the assumption that a photo patch and its corresponding sketch patch share similar geometric manifold structure, i.e. if two photo patches are similar, then their sketch patch counterparts are also similar. Finally, each target sketch patch could be reconstructed through the linear combination of the candidate sketch patches with the reconstruction weights. The most successful route of examplar based method is introducing Probabilistic Graphical Model to represent the joint probability distribution over all of the patches selected from a set of training data. However, these methods have two main defects: (1) most of these methods capture the evidence between patches in pixel-level, which lead to inaccurate parameter estimation under bad environment conditions such as light variations and clutter backgrounds; (2) the assumption that a photo patch and its corresponding sketch patch share similar geometric manifold structure is not rigorous.

Another growing branch is end-to-end regression based method, which formulate the mapping between photos and sketches as a regression problem (Meer et al. 1991), whose advantage lies in their low computation complexity. These methods develop slowly in early studies due to the lack of nonlinear regression models with sufficient representation capability and has blossomed recently benefiting from the rapid development of end-to-end deep convolutional neural networks which could learn to extract high-level image feature representation. However, such high-level image feature representation generated by convolutional operation usually lack of spatial and structural information.

In order to take full advantage of the representation ability of higher-level feature representation while introducing more spatial and structural information, we combine the generative Probabilistic Graphical Model with discriminatively trained deep convolutional neural network for face sketch synthesis. We introduce a specially designed deep collaborative networks which was proposed in (Zhu et al. 2018) and make some modifications for our own task. By using this model, we can map the test face photo and training face sketches into uniform deep image representations. These uniform deep image representations have multiple channels and the feature value of each channel is range from 0 to 255. We initialize the weight of each channel with same proportion so that the feature value after weighted summation of all channels is still range from 0 to 255. Such uniform deep image representation are lack of structure-preserving property initially. We divide all face photos, sketches and uniform deep image representation into patches with overlap between the neighboring patches so that the transformation problem of the whole image is reduced to the local patch level. Then, we no longer need to select candidate sketch patches based on their corresponding photo patches and we can directly match between test photo patches and training sketch patches with their uniform deep patch representations. Eventually, We totally have two kinds of parameters: the weights for deep patch representation and the weights for sketch patch reconstruction. We think that just like each candidate sketch has different effect, each channel of feature maps has different effect too. We not only want to model the distribution over the weights for sketch patch reconstruction but also want to model the distribution over the weights for deep patch representation. So, we design a graphical model based deep patch representation learning framework to jointly model the distribution over the weights for deep patch representation and the distribution over the weights for sketch patch reconstruction and alternately optimizing these two weights. The representation ability of deep patch representation is constantly enhancing and become more and more suitable for specific task, while the weights for sketch patch reconstruction are become progressively more reasonably.

The main contributions of this paper are threefold:

(1)
Uniform deep patch representation extracted from a specially designed deep collaborative networks, which possess stronger representation ability and robustness compared to pixel intensities, are used to represent face photo and sketch patches, thereby more accurate candidate sketch patches can be selected directly and better weight combination for sketch patch reconstruction can be obtained.

(2)
By combining deep patch representation, we design a graphical model based deep patch representation learning framework to jointly model and optimize the parameters for deep patch representation and parameters for sketch patch reconstruction. Both the representation ability of deep patch representation and the accuracy of weights for sketch patch reconstruction can be boosted iteratively.

(3)
Selecting candidate patches with extremely high dimensional deep patch representation is time-consuming. In order to speed up the algorithm while keep its performance, we design an offline candidate patch index strategy which can utilize product quantization (PQ) (Jegou et al. 2010) to establish an index field for each local patch position. We can save 4/5 time by using this strategy.

(4)
The proposed method achieves leading performance compared with state-of-the-art methods in terms of both subjective perception and objective evaluations on public benchmark datasets.

Preliminary work has been published in Zhu et al. (2017). Compared with the preliminary version, this paper has made sixth major extensions and improvements. First, a specially designed deep collaborative networks instead of pre-trained VGG-19 network (Simonyan and Zisserman 2014) has been introduced to extract uniform deep patch representation; Second, an offline candidate patch index strategy has been utilized to speed up the algorithm; Third, more systematical and detailed introduction to existing face sketch synthesis methods has been provided; Fourth, more validation experiments have been conducted to verify the contributions of the method and make it more solid; Fifth, a new database whose examples possess more lighting variation and shape exaggeration has been introduced to verify the validity of the proposed algorithm; Sixth, the results of nine other state-of-the-art face sketch synthesis methods on the database have been obtained and used to conduct the comparison experiment.

In this paper, excepted when noted, we utilize a bold lowercase letter to denote a column vector, a bold uppercase letter to denote a matrix, and regular lowercase and uppercase letters to denote a scalar. The remainder of this paper is organized as follows. Section 2 reviews the existing methods on face sketch synthesis. Section 3 detailed introduces the proposed method. Experimental results and performance evaluation are given in Sect. 4. Finally, some concluding remarks are given in the Sect. 5.

Related Work
Face Sketch Synthesis
By assuming that the mapping between a photo and its corresponding sketch is a linear transformation, Tang and Wang (2003) pioneered exemplar based approach by computing a global eigentransformation for synthesizing face sketches from face photos. However, whole face photos and face sketches cannot be simply explained by a linear transformation, especially when the hair region is considered. Liu et al. (2005) presented a patch-based approach with the idea of locally linear approximating global nonlinear. It represents each target sketch patch by a linear combination of some candidate sketch patches in the training set using locally linear embedding (LLE). The drawback of it is that each patch was independently synthesized and thus compatible relationship between the neighboring image patches was neglected. In order to tackle this problem, Wang and Tang (2009) employed multiscale Markov Random Field (MRF) to introduce probabilistic relationships between neighboring image patches. Their method synthesizes a face sketch by selecting the â€œbestâ€ candidate patches that maximize the a posteriori estimation of their MRF model. The weakness is that it can not synthesize new patches that do not exist in the training set and its optimization is NP hard. Zhou et al. (2012) presented a Markov Weight Fields (MWF) model to improve the aforementioned problem by introducing the linear combination into the MRF. Wang et al. (2017b) proposed a Bayesian framework that provided an interpretation to existing face sketch synthesis methods from a probabilistic graphical view. Sparse representation (Yang et al. 2014) was also applied to face sketch synthesis to computing the combination weight. Wang et al. (2013) proposed to adaptively determine the number of nearest neighbors by sparse representation and Gao et al. (2012) presented a sparse-representation-based enhancement strategy to enhance the quality of the synthesized photos and sketches. Most of the exemplar based methods have high computational complexity. To solve this problem, Song et al. (2014) proposed a real-time face sketch synthesis method by considering face sketch synthesis as an image denoising problem with the aid of GPU. Wang et al. (2017a) employ offline random sampling in place of online K-NN search to improve the efficiency of neighbor selection. Locality constraint is introduced to model the distinct correlations between the test patch and random sampled patches.

Regression based approaches has won more and more attention recently profiting from its real-time speed and end-to-end property. Changet al. (2011) adopted kernel ridge regression to synthesis face sketch patches from face photos. Zhang et al. (2011a) proposed to use support vector regression to express the high-frequency mappings between photo patches and sketch patches. Zhu and Wang (2016) adopted a divide and conquer strategy. Photo-sketch patch pairs are firstly divided into many different clusters each of which is equipped with a ridge regression model to learn the mapping between photo patches and sketch patches. CNN has greatly promoted the development of the nonlinear regression model (Yang et al. 2014). Zhang et al. (2015) adopted fully convolutional network (FCN) to directly model the complex nonlinear mapping between face photos and sketches. Since these methods usually use a per-pixel loss function to measure the difference between output and ground-truth images, their results often have low perceptual quality.

Image-to-Image Translation
Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) provide a perceptual loss for generative models via an adversarial process, in which two models: a generative model G that captures the data distribution and a discriminative model D that estimates the probability that a sample came from the training data rather than G are simultaneously learned. This idea has great power in image generation tasks, as it can forces the generated images to be indistinguishable from real images. Isola et al. (2016) developed a common framework-pix2pix- which uses a conditional GANs to learn a mapping from input to output images. Because of the use of adversarial loss, it can generate sharp, realistic results. This model can solve all kind of problems that translating one possible representation of a scene into another, given sufficient training data. However, when applied to face photo-sketch synthesis, this model would produce results with messy texture, which looks rather cluttered.

Neural Style Transfer
Face sketch synthesis can also be regarded as a branch of image style transfer. Gatys et al. (2016) successfully apply pre-trained VGG-19 networks (Simonyan and Zisserman 2014) to the problem of image style transfer. The method yields very impressive artistic style results, since a CNN is effective in decomposing content and style from images. Johnson et al. (2016) define a perceptual loss function to help learn a transfer network designed to produce results that match those of (Gatys et al. 2016). However, feature pyramid at different layers are directly used to capture only pixel correlations of the image style and content so that the results lack of structural similarity. To tackle this problem, Li and Wand (2016) introduce a patch based style transfer by combining a Markov Random Field (MRF) and a CNN. But it only serves as a data term to optimize pixel values of the output image. The drawbacks of these methods are that they are unable to guarantee that the transferred results are structure-preserving. In addition, they often produced stylization results with messy texture.

Fig. 1
figure 1
Framework of the proposed method. It mainly consists of three stage: (1) Uniform deep patch representation extraction (UDPRE); (2) Candidate patch selection; (3) Graphical model based deep patch representation Learning (GDPRL)

Full size image
Fig. 2
figure 2
Structure of the modified deep collaborative networks with the corresponding kernel size (k), stride (s) and number of feature maps (n) indicated for each convolutional layer

Full size image
Non-photorealistic Rendering
Non-photorealistic rendering (NPR) is an area of computer graphics that focuses on enabling a wide variety of expressive styles for digital art. Rendering pencil (or sketch) drawing from natural images is a hotspot in this field. Mao et al. (2001) propose a method that can detect local structure orientation from the input image and incorporated linear integral convolution (LIC) to produce the shading effect. Lu et al. (2012) propose a two-stage system combining both line and tonal drawing to generate pencil drawing from natural images. Other methods were reviewed in Lu et al. (2012). These methods can yield impressive pencil drawings from natural images. However, when applied to face images which have strong structural property and fine components, these methods often fail to produce satisfactory results.

Method
In this section, we describe our face sketch synthesis algorithm in detail. The overall framework of our method is shown in Fig. 1. It mainly consists of three stage: (1) Uniform deep patch representation extraction; (2) Candidate patch selection; (3) Graphical model based deep patch representation Learning. The input is an aligned test face photo and the output is a reconstructed face sketch. We firstly extract uniform deep image representations for the test photo and all training sketches. Then, we divided all the uniform deep image representations into overlapping patches as uniform deep patch representations. Candidate sketch patches for each test photo patch are selected based on these uniform deep patch representations. A graphical model based deep patch representation learning framework is used to solve the best weights for sketch patch reconstruction. Finally, we get each reconstructed sketch patch by weighted recombining the candidate sketch patches and stitch them into a whole sketch with overlapping area averaged. Detailed modeling steps are shown below.

Uniform Deep Patch Representation Extraction
Consider a training dataset including M face photo-sketch pairs and a test photo t, whose images are cropped to the size of 250Ã—200 and geometrically aligned according to three points: two eye centers and the mouth center.

We firstly train a specially designed deep collaborative networks using M face photo-sketch pairs. The deep collaborative networks was proposed in (Zhu et al. 2018). It can simultaneously learn two opposite mappings: ğº:ğ‘â„ğ‘œğ‘¡ğ‘œâ†’ğ‘ ğ‘˜ğ‘’ğ‘¡ğ‘â„ and ğ¹:ğ‘ ğ‘˜ğ‘’ğ‘¡ğ‘â„â†’ğ‘â„ğ‘œğ‘¡ğ‘œ by introducing a collaborative loss that regularize the intermediate representations of the two opposite mappings to obey the same distribution. Eventually, a middle latent domain that both two opposite mappings would going through can be obtained. Our insight to use this model is that we can extract uniform deep image representation from the learned middle latent domain. In other words, this model can help us map the test photo and training sketchs into uniform deep image representations. Different from the original model proposed in (Zhu et al. 2018), some improvements have been made to better suit our task. The modified architecture of the model is presented in Fig. 2. It consists of two opposite generators, G and F, with the same structure. G is used to generate the face sketch from the face photo, while F is used to generate the face photo from the face sketch. Each generator is constructed using stride-2 convolutions, residual blocks and fractionally strided convolutional layers. The configuration was specified as follows: k7s1n64, k3s2n128, k3s2n256, rbn256, rbn256, rbn256, rbn256, rbn256, rbn256, rbn256, rbn256, rbn256, k3s12n128, k3s12n64, k7s12n3, where k denotes kernel size, s denotes stride, n denotes kernel numbers, and rb denotes residual block. Different from the original model that only force the intermediate representations of the two opposite mappings to be subject to the same distribution, we force the features at different layer of the two opposite mappings to be subject to the same distribution. For example, the feature of G-Up1 and the feature of F-D1 are forced to obey the same distribution. Detailed objective functions and training settings can be found in (Zhu et al. 2018).

After training, we put the test photo into the trained generator G and put all training sketches into the trained generator F. Uniform deep image representation with 256 feature maps generated by the Res layer of the generator G is used to represent the test photo and uniform deep image representations with 256 feature maps generated by the D2 layer of the generator F are used to represent the training sketches. Each feature map generated by a convolution kernel can be regarded as a feature that concerned with a specific characteristic of the images. We use bicubic interpolation to upscale each feature map back to 250Ã—200 in order to consistent with the size of input images. The uniform deep image representations we get have multiple channels and the feature value of each channel is range from 0 to 255. We initialize the weight of each channel with same proportion so that the feature value after weighted summation of all channels is still range from 0 to 255. Then, the test photo, uniform deep image representations and sketches are divided into N overlapping patches. Here we treat the patch-level uniform deep image representation as deep patch representation. Let ğ­ğ‘– be the ith test photo patch, where ğ‘–=1,2,â€¦,ğ‘. The deep patch representation of the test photo patch can be represented by the linear combination of 256 feature maps weighted by the 256 dimensional vector ğ®ğ‘–:

D(ğ­ğ‘–)=âˆ‘ğ‘™=1ğ¿ğ‘¢ğ‘–,ğ‘™dğ‘™(ğ­ğ‘–)
(1)
where dğ‘™(ğ­ğ‘–) means lth feature map of patch ğ­ğ‘–, ğ‘¢ğ‘–,ğ‘™ denotes the weight of lth feature map, ğ‘™=1,2,â€¦,ğ¿, and âˆ‘ğ¿ğ‘™=1ğ‘¢ğ‘–,ğ‘™=1. Here L equals 256. Note that the weights for deep patch representation at various locations in photo patches are different, and each feature map has uniform weight in initial. Optimized weights for deep patch representation and weights for sketch patch reconstruction will be learnt later.

Candidate Patch Selection
Given a test photo patch ğ­ğ‘– and its deep patch representation D(ğ­ğ‘–), our target is to synthesize the corresponding sketch patch ğ²ğ‘–, where ğ‘–=1,2,â€¦,ğ‘. We need to find K training sketch patches {ğ²ğ‘–,1,ğ²ğ‘–,2,â€¦,ğ²ğ‘–,ğ¾} that most like ğ­ğ‘– according to the euclidean distance of their uniform deep patch representation within the search region around the location of ğ­ğ‘– as the candidate sketch patches for reconstruction. Figure 3 gives an instruction about search region. Selecting candidate patches with extremely high dimensional deep patch representation is time-consuming. In order to speed up the algorithm while keep its performance, we firstly use product quantization (PQ) (Jegou et al. 2010) to construct an index field of all the deep patch representation of training sketch patches with in the search region for each local patch position offline. Then, we can find the K nearest deep patch representation for the deep patch representation of ğ­ğ‘– quickly in online stage. The corresponding K training sketch patches {ğ²ğ‘–,1,ğ²ğ‘–,2,â€¦,ğ²ğ‘–,ğ¾} are used as candidate sketch patches for target sketch patch reconstruction.

Compared with the preliminary version, two major improvements have been made in this stage:

1)
In the preliminary version, deep patch representation of the test photo patch and deep patch representations of training sketch patches extracted from pre-trained VGG-19 networks have different distributions. Therefore, we are not able to match between them directly. We need to select candidate training photo patches based on their deep patch representations firstly and then reconstruct the target sketch patch by the linear combination of their corresponding candidate sketch patches based on the assumption that a photo patch and its corresponding sketch patch share similar geometric manifold structure. However, this assumption is not rigorous. By using the specially designed deep collaborative networks, we can map the test photo patch and training sketch patches into uniform deep patch representations. Therefore, we are able to match between them directly and no longer need the assumption. Figure 4 shows the difference between the matching strategy of DPGM and its preliminary version DGFL intuitively. More accurate candidate sketch patches can be selected directly and better weight combination for sketch patch reconstruction can be obtained.

2)
In the preliminary version, we construct a kd-tree (Beis and Lowe 1997) using all the deep patch representation of training photo patches with in the search region for each local patch position online. Because the deep patch representation has very high dimensions, matching between them will cost a lot of time. Jegou et al. (2010) provides an effective way for approximate nearest neighbor search using product quantization (PQ), which suits our task well. Product quantization can decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A high dimensional deep patch representation can be represented by a short code composed of its subspace quantization indices. The Euclidean distance between two deep patch representations can be efficiently estimated from their codes. Therefore, we utilize product quantization to construct an index field of all the deep patch representation of training sketch patches with in the search region for each local patch position offline. Then, online matching between deep patch representation of the test photo patch and deep patch representations of training sketches can save a lot of time.

The target sketch patch ğ²ğ‘– can be synthesized by the linear combination of K candidate sketch patches weighted by the K-dimentional vector ğ°ğ‘–:

ğ²ğ‘–=âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘–,ğ‘˜ğ²ğ‘–,ğ‘˜
(2)
where ğ‘¤ğ‘–,ğ‘˜ denotes the weight of the kth candidate sketch and âˆ‘ğ¾ğ‘˜=1ğ‘¤ğ‘–,ğ‘˜=1.

Fig. 3
figure 3
Illustration of search region. Blue slash area indicates the location of the probe patch while the yellow backslash area indicates the search region around the location of the probe patch

Full size image
Fig. 4
figure 4
Difference between the matching strategy of DPGM and its preliminary version DGFL

Full size image
Graphical Model Based Deep Patch representation Learning
In summary, given the observed variables ğ­ğ‘–, we need to infer two kinds of latent variables: weights for deep patch representation ğ®ğ‘– and weights for sketch patch reconstruction ğ°ğ‘–. ğ°ğ‘– directly determines the quality of the synthesized sketch patch while ğ®ğ‘– determines the representation ability of deep patch representation and indirectly influence the reconstruction weights. Note that our reconstruction method is conducted in patch level and what we ultimately needed is optimal reconstruction weights for candidate sketch patches at different spatial locations. The learned weights for sketch patch reconstruction is spatially-varying, and therefore the learned weights for deep patch representation is spatially-varying either. Inspired by (Zhou et al. 2012), we design a graphical model based deep patch representation learning framework to jointly model the distribution over the weights for deep patch representation and the distribution over the weights for sketch patch reconstruction, as shown in Fig. 5. The joint probability of ğ®ğ‘–, ğ°ğ‘–, and ğ­ğ‘– âˆ€ğ‘–âˆˆ{1,â€¦,ğ‘}, is formulated as:

ğ‘(ğ­1,â€¦,ğ­ğ‘,ğ®1,â€¦,ğ®ğ‘,ğ°1,â€¦,ğ°ğ‘)âˆâˆğ‘–=1ğ‘ğ›·(ğ­ğ‘–,ğ®ğ‘–,ğ°ğ‘–)âˆ(ğ‘–,ğ‘—)âˆˆğ›¯ğ›¹(ğ°ğ‘–,ğ°ğ‘—)âˆğ‘–=1ğ‘ğ›¶(ğ®ğ‘–),
(3)
where ğ›·(ğ­ğ‘–,ğ®ğ‘–,ğ°ğ‘–) is the local evidence function:

ğ›·(ğ­ğ‘–,ğ®ğ‘–,ğ°ğ‘–)=exp{âˆ’âˆ‘ğ‘™=1ğ¿ğ‘¢ğ‘–,ğ‘™â€–dğ‘™(ğ­ğ‘–)âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘–,ğ‘˜dğ‘™(ğ²ğ‘–,ğ‘˜)â€–2/2ğ›¿2ğ·}
(4)
Fig. 5
figure 5
Graphical model based deep patch representation learning

Full size image
and ğ›¹(ğ°ğ‘–,ğ°ğ‘—) is the neighboring compatibility function:

ğ›¹(ğ°ğ‘–,ğ°ğ‘—)=exp{âˆ’â€–âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘–,ğ‘˜ğ¨ğ‘—ğ‘–,ğ‘˜âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘—,ğ‘˜ğ¨ğ‘–ğ‘—,ğ‘˜â€–2/2ğ›¿2ğ‘†}
(5)
and ğ›¶(ğ®ğ‘–) is the regularization function:

ğ›¶(ğ®ğ‘–)=exp{âˆ’ğœ†â€–ğ®ğ‘–â€–2}.
(6)
Here dğ‘™(ğ­ğ‘–) means the lth feature map of patch ğ­ğ‘– and dğ‘™(ğ²ğ‘–,ğ‘˜) means the lth feature map of kth candidate patch. (ğ‘–,ğ‘—)âˆˆğ›¯ means the ith and jth patches are neighbors. ğ¨ğ‘—ğ‘–,ğ‘˜ represents the overlapping area of the candidate sketch patch ğ²ğ‘–,ğ‘˜ with the jth patch. ğœ† balances the regularization term with the other two terms. The posterior probability can be written as:

ğ‘(ğ®1,â€¦,ğ®ğ‘,ğ°1,â€¦,ğ°ğ‘âğ­1,â€¦,ğ­ğ‘)=1ğ‘ğ‘(ğ­1,â€¦,ğ­ğ‘,ğ®1,â€¦,ğ®ğ‘,ğ°1,â€¦,ğ°ğ‘)
(7)
where ğ‘=ğ‘(ğ­1,â€¦,ğ­ğ‘) is a normalization term. By maximizing the posterior probability, the best weights ğ®ğ‘– and ğ°ğ‘– can be achieved. This is equivalent to minimizing the following cost function:

minğ®,ğ°12ğ›¿2ğ‘†âˆ‘(ğ‘–,ğ‘—)âˆˆğ›¯â€–âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘–,ğ‘˜ğ¨ğ‘—ğ‘–,ğ‘˜âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘—,ğ‘˜ğ¨ğ‘–ğ‘—,ğ‘˜â€–2+12ğ›¿2ğ·âˆ‘ğ‘–=1ğ‘âˆ‘ğ‘™=1ğ¿ğ‘¢ğ‘–,ğ‘™â€–dğ‘™(ğ­ğ‘–)âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘–,ğ‘˜dğ‘™(ğ²ğ‘–,ğ‘˜)â€–2+âˆ‘ğ‘–=1ğ‘ğœ†â€–ğ®ğ‘–â€–2ğ‘ .ğ‘¡.âˆ‘ğ‘˜=1ğ‘˜ğ‘¤ğ‘–,ğ‘˜=1,0â‰¤ğ‘¤ğ‘–,ğ‘˜â‰¤1âˆ‘ğ‘™=1ğ¿ğ‘¢ğ‘–,ğ‘™=1,0â‰¤ğ‘¢ğ‘–,ğ‘™â‰¤1
(8)
where ğ‘–=1,2,â€¦,ğ‘, ğ‘˜=1,2,â€¦,ğ¾, ğ‘™=1,2,â€¦,ğ¿.

An alternating optimization strategy can be used to solve this problem.

(1)
Fix ğ®, the problem (8) can be simplified to:

minğ°âˆ‘ğ‘–=1ğ‘â€–D(ğ­ğ‘–)âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘–,ğ‘˜D(ğ²ğ‘–,ğ‘˜)â€–2+ğ›¼âˆ‘(ğ‘–,ğ‘—)âˆˆğ›¯â€–âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘–,ğ‘˜ğ¨ğ‘—ğ‘–,ğ‘˜âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘—,ğ‘˜ğ¨ğ‘–ğ‘—,ğ‘˜â€–2
(9)
where ğ›¼=ğ›¿2ğ·/ğ›¿2ğ‘†. This problem can be solved using the CDM proposed in  (Zhou et al. 2012).

(2)
Fix ğ°, the problem (8) can be simplified to:

minğ®ğ¢âˆ‘ğ‘™=1ğ¿ğ‘¢ğ‘–,ğ‘™ğ©ğ‘–,ğ‘™+ğœ†â€–ğ®ğ‘–â€–2â‡’minğ®ğ¢ğœ†ğ®Tğ‘–ğ®ğ‘–+ğ©Tğ‘–ğ®ğ‘–
(10)
where

ğ©ğ‘–,ğ‘™=12ğ›¿2ğ·â€–dğ‘™(ğ­ğ‘–)âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¤ğ‘–,ğ‘˜dğ‘™(ğ²ğ‘–,ğ‘˜)â€–2
(11)
The problem (10) is a standard convex QP problem which can be effectively solved.

We initially set ğ‘¢ğ‘–,ğ‘™ as 1/L and set ğ‘¤ğ‘–,ğ‘˜ as 1/K. Then (9) and (10) are alternately performed until convergence. After obtaining optimal weights ğ‘¤ğ‘–, we can reconstruct all the target sketch patches {ğ²1,ğ²2,â€¦,ğ²ğ‘} by (2). Finally, we splice all the reconstructed sketch patches into a whole sketch with overlapping area averaged. The proposed DPGM algorithm is summarized in Algorithm 1:

figure a
Experimental Results and Analysis
In this section, we report the experimental results of the proposed DPGM method quantitatively and qualitatively.

Database
The experiments are conducted on the Chinese University of Hong Kong (CUHK) face sketch (CUFS) database (Wang and Tang 2009) and the CUHK face sketch FERET (CUFSF) database (Zhang et al. 2011b). The CUFS database consists of face photo-sketch pairs from three databases: the CUHK student database (Tang and Wang 2002) (188 persons), the AR database (Martinez and Benavente 1998) (123 persons) and the XM2VTS database (Messer et al. 1999) (295 persons). The CUFSF database includes 1,194 subjects from the FERET database (Phillips et al. 2000). Each person in the database has a face photo-sketch pair. All these images are geometrically aligned relying on three points: two eye centers and the mouth center and they are cropped to 250Ã—200. Notice that face photo-sketch pairs from the CUHK student database and the AR database are more regular, with less diversity. Persons in the XM2VTS database are different in ages, skins (races) and hair styles, making the synthesis task more difficult. Face photos in the CUFSF database are with lighting variation and sketches are with shape exaggeration. Figure 6 gives some examples from these two databases.

Fig. 6
figure 6
Examples of face photo-sketch pairs from: a the CUHK student database; b the AR database; c the XM2VTS database; and d the CUHK face sketch FERET database

Full size image
Fig. 7
figure 7
Synthesized sketches using deep patch representation extracted from different layers of the deep collaborative networks

Full size image
Fig. 8
figure 8
Average SSIM score (%) of synthesized sketches on CUHK database using deep patch representation extracted from different layers of the deep collaborative networks

Full size image
Fig. 9
figure 9
Average time cost and SSIM score of MWF, DPGM and DGFL on CUHK student

Full size image
Fig. 10
figure 10
The effect of deep patch representation with initial weights and learned weights. From left to right: a using pixel intensities; b using deep patch representation with initial weights; and c using deep patch representation with learned weights

Full size image
Fig. 11
figure 11
Synthesized sketches by the PencilDrawing method, the DGFL method and the proposed DPGM method

Full size image
Fig. 12
figure 12
Synthesized sketches on the CUFS database by examplar based methods (LLE, MRF, MWF, SSD, SFS, SFS-SVR, Bayesian, RSLCR, DGFL) and the proposed DPGM method

Full size image
Settings
In all the experiments, we following the same parameter settings which is set empirically. We set the image patch size to be 10. The overlap size is set to 5. The size of search region is set to 5. The number of candidate patches K is set to 10. The ğ›¼ is set to 0.25 while the ğœ† is set to 2. For the CUHK student database, we randomly choose 88 pairs of face photo-sketch for training and the rest were used for testing. For the AR database, 80 face photo-sketch pairs are randomly chosen for training and the rest 43 pairs were used for testing. For the XM2VTS database, we randomly choose 100 pairs for training and the rest 195 pairs were used for testing. For the CUFSF database, 250 pairs of face photo-sketch are taken for training and the rest for testing. Fifteen state-of-the-art methods are compared: the PencilDrawing method (Lu et al. 2012), the LLE method (Liu et al. 2005), the MRF method (Wang and Tang 2009), the MWF method (Zhou et al. 2012), the SSD method (Song et al. 2014), the SFS method (Wang et al. 2013), the SFS-SVR method (Wang et al. 2013), the Bayesian method (Wang et al. 2017b), the RSLCR method (Wang et al. 2017a), the FCN method (Zhang et al. 2015), the pix2pix method (Isola et al. 2016), the NeuralStyle methd (Gatys et al. 2016), the FastNeuralStyle nethod (Johnson et al. 2016), the CNNMRF method (Li and Wand 2016) and our preliminary method DGFL (Zhu et al. 2017). All synthesized sketches by the PencilDrawing method, MWF method, SSD method, SFS method, SFS-SVR method, Bayesian method, RSLCR method, pix2pix method, NeuralStyle methd, FastNeuralStyle method and CNNMRF method are generated from the source codes provided by the authors. For the MRF method, we use the codes from the implementation provided by authors of (Song et al. 2014). Results of the LLE method and FCN method is based on our implementations. All experiments are conducted using Python on Ubuntu 16.04 system with i7-6700k 4.0Ghz CPU and 12G NVIDIA Titan X GPU. GPU was used to train deep collaborative networks and extract deep patch representations. We use PyTorch (Paszke et al. 2017) to realize the model.

Fig. 13
figure 13
Synthesized sketches on the CUFS database by regression based methods (FCN, pix2pix), the DGFL method and the proposed DPGM method

Full size image
Fig. 14
figure 14
Synthesized sketches of the photos with deformation or lighting variance by pix2pix and the proposed DPGM

Full size image
Fig. 15
figure 15
Synthesized sketches on the CUFS database by nerual style transfer (NeuralStyle, FastNeuralStyle, CNNMRF), the DGFL method and the proposed DPGM method

Full size image
With the help of GPU, deep patch representation extraction of a input test photo can be finished less than 1s. So, the most time-consuming part of our proposed method lies in two phases: the neighbor selection phase and the optimization phase. The time cost of neighbor selection phase depends on the number of training photos, the size of photo patches, the size of search regions and the number of feature map channels. Given a test photo patch, the most similar patch is firstly searched in every training photo and then the top K most similar photo patches are taken as the candidates. Let c denotes the number of candidates in the search region, p is the length of feature vector, M denotes the number of patches in each image and N is the number of training sketch-photo pairs. Then the time complexity of this neighbor selection process is ğ‘‚(ğ‘ğ‘2ğ‘€ğ‘). More training photos, larger search regions, and more feature maps channels would increase in time-consuming. We use product quantization (Jegou et al. 2010) to establish index field offline and conduct K-neighbor searches online in order to improve efficiency. The optimization phase mainly depends on the number of iterations. In our experiments, it always converges after five to eight iterations.

Ablation Study
We use the CUHK student database as validation set to conduct ablation study. Structural similarity index metric (SSIM) (Wang et al. 2004) are used to evaluate the quality of reconstructed sketches. Deep patch representation contributes a lot to the final performance of our algorithm. However, the representation ability of deep patch representation changes according to different layer of the deep collaborative networks. We run our method using deep patch representation extracted from different layers of the deep collaborative networks and compute the average SSIM score of the results. Figure 7 shows synthesize sketches of a sample using deep patch representation extracted from different layers of the deep collaborative networks. Figure 8 shows average SSIM score (%) of synthesized sketches on CUHK database using deep patch representation extracted from different layers of the deep collaborative networks. As we can see, the best performance is got on G-Res (F-D2) layer. Figure 9 shows the average time cost and SSIM score of MWF, DPGM and DGFL on CUHK student. As we can see, the average SSIM score of DPGM on CUHK student database improves a little compared with DGFL while the efficiency of it promotes a lot. The speed of DPGM is nearly six times faster than DGFL. Whatâ€™s more, We conduct a validation experiment to demonstrate the advantage of using uniform deep patch representation over just using pixel intensity and the advantage of joint learning of the weights for deep patch representations and weights for sketch patch reconstruction over just learning of the weights for sketch patch reconstruction only. As shown in Fig. 10, the average SSIM score on the CUHK student database using pixel intensity is 61.88%. When using uniform deep patch representation, the average SSIM score raise to 63.08%, increasing by 1.2 percentage points. After optimized the weights for deep patch representations, the average SSIM score raised to 63.41%, increasing by 0.33 percentage points. It can be seen that uniform deep patch representation is the most import component in the proposed DPGM method. Compared with the effect of using uniform deep patch representation, the effect of introducing weights for deep patch representations is not significant. Therefore, we focus on how to learn better deep patch representation and how to speed up the algorithm.

Face Sketch Synthesis
Figure 11 shows some synthesized face sketches by the PencilDrawing method, the DGFL method and the proposed DPGM method. As we can see, results of the PencilDrawing method are far from satisfactory. Note that we use the same code and the same settings as provided in original paper Lu et al. (2012) and the drawing results of natural scene images are very impressive. However, when applied to face images, its performance suffers a lot. We try to analyze the reasons and find that results of natural scene images actually possess lots of messy textures. In natural scene sketches, such messy textures can be regarded as strokes and can improve the perception quality. But when it comes to face sketches that have strong structural property and fine components, such messy textures will seriously affect the perception quality. Results of the DGFL method have no messy textures but suffer from blur effects. Results of the DPGM method have the best visual quality and match human visual perception very well.

Fig. 16
figure 16
Synthesized sketches on the CUFSF database by: a examplar based methods (LLE, MRF, MWF, SSD, SFS, SFS-SVR, Bayesian, RSLCR); b regression based methods (FCN, pix2pix); (c) neural style transfer (NeuralStyle, FastNeuralStyle, CNNMRF), the DGFL method and the proposed DPGM method

Full size image
Fig. 17
figure 17
Synthesized sketches of the photos with extreme lighting variance by different methods

Full size image
Figure 12 shows some synthesized face sketches by different examplar based methods on the CUFS database. As we can see, blurring appeared in some dominant facial regions on the results from the LLE method, the MWF method, the SSD method, the SFS method and the SFS-SVR method. Synthesized results of the MRF method has some deformations and patch mismatch around the face region. It probably because that the MRF method cannot generate sketch patches without appeared in the training dataset. In addition, results from other methods contain significant amount of noise on the nose, mouth and the hair region. Even the most recently proposed algorithms such as the Bayesian method and the RSLCR method are exist with above-mentioned defects. The proposed DPGM method performs well whether in facial details or in the hair and background area. Since data-driven (i.e. exemplar-based) methods usually reconstruct a sketch patch by weighted combine several candidate sketch patches, they always suffer from blur effects. Although the results of the DGFL method contain very little noise, they suffer a lot from blur effects. The DPGM method can generate sketches with high visual quality and sharper edges. We have marked out the areas in which our results are much better than others in Fig. 12. It can be seen that the proposed DPGM method has great advantage than other methods.

Figure 13 shows some synthesized face sketches by different regression based methods on the CUFS database. As we can see, the results of FCN is very blurry due to the poor representation ability of their network. Pix2pix model can generate images with sharper textures which can improve the visual perception quality. However, when the test photo possesses deformation or illumination, pix2pix would fail to model the reasonable texture distribution. Under this circumstance, the unreasonable sharp textures will look rather messy, thereby affect the visual quality. The proposed DPGM method can tackle this problem well. It doesnâ€™t directly map the uniform deep image representations to reconstructed sketch using deconvolution operation, but utilize a probabilistic graphical model to optimize the unary term between uniform deep image representations of the test photo patch and training sketch patches and the pairwise term between two adjacent training sketch patches. This approach can introduce more spatial and structural information. Therefore, the results of the proposed DPGM method tend to have high visual quality whether in normal or bad environment conditions. Figure 14 shows some synthesized face sketches by pix2pix and the proposed DPGM. We can find that the proposed DPGM has huge advantage than pix2pix under bad environment conditions.

Figure 15 shows some synthesized face sketches by different neural style transfer methods on the CUFS database. Because of the lack of structure information, the results generated by NeuralStyle possess extreme messy texture. By combining a Markov Random Field (MRF) and a CNN, CNNMRF is able to preserve some structure information. However, this approach performs poorly when the content image and style image have huge difference in appearance. As we can see, it can not transform the photo to sketch and the content of generated image does not like the content image. The results generated by FastNeuralStyle donâ€™t possess messy texture. But they are more like gray photos with low visual quality.

We have also investigated the robustness of the proposed methods against shape exaggeration and illumination variations on the CUFSF database. Figure 16 shows some synthesized face sketches from different methods on the CUFSF database. It can be seen that the proposed DPGM face sketch synthesis method could achieve satisfying performance compared with other state-of-the-art methods. The results of the proposed DPGM method obviously have higher clarity than the DGFL method.

Figure 17 shows some synthesized face sketches by different methods on face photos with extreme lighting variance. Other methods are obviously affected by the lighting variance, failing to capture the distribution of real data. Since deep patch representation is more robust to these noises than pixel intensity, the proposed DPGM method could reconstruct sketches with virtually no distortion. This advantage is of vital importance in real world applications.

Objective Image Quality Assessment
In order to validate that the face sketches synthesized by the proposed algorithm have good image quality, we deploy the structural similarity index metric (SSIM) (Wang et al. 2004) to objectively evaluate the visual perception quality of the synthesized sketches by different methods on CUFS database and CUFSF database. The reference image is the original sketch which is drawn by artists while the distortion image is the synthesized sketch. There are 338 (100 + 43 + 195) synthesized sketches for each method generated from the CUFS database. For the CUFSF database, 994 synthesized sketches are generated for each method. The statistics of average SSIM scores on the CUFS database and the CUFSF database are shown in Fig. 18. The horizontal axis labels represent the SSIM score from 0 to 1. The vertical axis means the percentage of synthesized sketch whose SSIM scores are not smaller than the score marked on the horizontal axis. Table 1 gives the average SSIM score on the CUFS database and the CUFSF database. It can be seen from Fig. 18 and Table 1 that the DPGM method gets the best performance. Note that speed of DPGM is nearly six times faster than DGFL.

Fig. 18
figure 18
Statistics of SSIM scores on the CUFS database and the CUFSF database

Full size image
Face Recognition
Face sketch recognition rate can also be used to evaluate the quality of the synthesized sketches. It measures the similarity of the original sketch and the synthesized sketch. We exploit null-space linear discriminant analysis (NLDA) (Chen et al. 2000) as the recognition methods to evaluate the quality of synthesized sketches by different methods on CUFS database and CUFSF database. For the CUFS database, we randomly choose 150 synthesized sketches and their corresponding sketches drawn by the artist for classifier training and the rest 188 synthesized sketches are as the gallery set. For the CUFSF database, 300 synthesized sketches and their corresponding sketches drawn by the artist are randomly chosen for classifier training and the rest 644 synthesized sketches are as the gallery set. We repeat each face recognition experiment 20 times by randomly partition the data. Table 2 gives the face recognition accuracy by NLDA on the CUFS database. It can be seen that on the CUFS database and the CUFSF database, the DGFL method has the highest recognition rate. The recognition rate of DPGM has fallen slightly compared with DGFL.

Table 1 Average SSIM score (%) on the CUFS database and the cufsf database
Full size table
Table 2 Face Recognition accuracy (%) by NLDA on the CUFS database and the CUFSF database
Full size table
User Study
Sometimes objective quality evaluation may not match human visual perception very well. To better evaluate the proposed methods, especially to see whether the synthesize sketches are well match human visual perception, we also conduct user studies. We randomly select 20 test face photos from the CUFS database, the CUFSF database and face photos in the wild. For each test photo, 4 sketches are synthesized by 4 methods: the PencilDrawing method, the pix2pix method, the NeuralStyle method and the proposed DPGM method. Users (not including any of the authors) are asked to select the most satisfactory sketch result for each test face photo. The ranking of the average percentage of the most satisfactory times is used as the final evaluation index. The questionnaire is posted online (Site: https://tp.wjx.top/m/34183434.aspx). Up to now, 22 answers have been received. The average human ranks (HR) for the 4 methods are reported in Table 3. It is clear that results of the proposed DPGM method have obtained the most satisfactory appraisal most times. Therefore, we can draw a conclusion that synthesized sketches of the proposed DPGM method have the best visual quality and best match human visual perception.

Table 3 Average human ranks (%)
Full size table
Conclusion
In this paper, we presented a graphical model based deep patch representation learning and face sketch synthesis framework. Uniform deep patch representation generated by the specially designed deep collaborative networks are used to represent face image patches, which possess stronger representation ability and robustness compared to pixel intensities. In order to introduce more structure information, we utilize probabilistic graphical model to jointly model the distribution over the weights for deep patch representation and the distribution over the weights for sketch patch reconstruction and adopt an alternating optimization strategy to solve this model. Optimal weight combination for sketch patch reconstruction can be obtained after convergence. Extensive experiments on several benchmark datasets demonstrate that our method can achieve superior performance than other state-of-the-art methods, especially under the case of bad light conditions or clutter backgrounds. In this paper, we only use the optimal weights for sketch patch reconstruction to reconstruct the target sketch patch. The optimal deep patch representation is underutilized. In the future, we would further explore the discrimination ability of the optimal deep patch representation and apply it to the recognition problem.