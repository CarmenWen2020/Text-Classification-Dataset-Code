Multi-label pedestrian attribute recognition in surveillance is inherently a challenging task due to poor imaging quality, large pose variations, and so on. In this paper, we improve its performance from the following two aspects: (1) We propose a cascaded Split-and-Aggregate Learning (SAL) to capture both the individuality and commonality for all attributes, with one at the feature map level and the other at the feature vector level. For the former, we split the features of each attribute by using a designed attribute-specific attention module (ASAM). For the later, the split features for each attribute are learned by using constrained losses. In both modules, the split features are aggregated by using several convolutional or fully connected layers. (2) We propose a Feature Recombination (FR) that conducts a random shuffle based on the split features over a batch of samples to synthesize more training samples, which spans the potential samples’ variability. To the end, we formulate a unified framework, named CAScaded Split-and-Aggregate Learning with Feature Recombination (CAS-SAL-FR), to learn the above modules jointly and concurrently. Experiments on five popular benchmarks, including RAP, PA-100K, PETA, Market-1501 and Duke attribute datasets, show the proposed CAS-SAL-FR achieves new state-of-the-art performance.

Access provided by University of Auckland Library

Introduction
Visual analysis of pedestrian attributes (Wang et al. 2017; Sarfraz et al. 2017; Lin et al. 2019; Liu et al. 2017; Zhao et al. 2018; Xiang et al. 2019; Tan et al. 2019b; Han et al. 2019; Li et al. 2019a, b; Tang et al. 2019c), e.g., gender, age and hair style, has recently received increasing attention due to its potential applications in surveillance and security applications. Although the performance has been greatly improved owing to the success of deep learning (Krizhevsky et al. 2012; Simonyan and Zisserman 2015; He et al. 2016, 2017; Huang et al. 2017; Hu et al. 2018a; Liu et al. 2020; Zhu et al. 2019; Shifeng et al. 2019) especially the Convolutional Neural Network (CNN) (LeCun et al. 1998), accurate recognition of pedestrian attributes remains a challenging task because of poor imaging quality (e.g., low resolution and motion blur), complex variations (e.g., arbitrary human poses, various camera viewing angles, and background), small training datasets and so on.

For multi-label pedestrian attribute classification, most previous works (Wang et al. 2017; Sarfraz et al. 2017; Lin et al. 2019; Liu et al. 2017; Li et al. 2019a, b) employ simple multi-task learning (MTL) framework to analyze all attributes together with a shared feature extractor. Such a shared strategy may prefer to capture the common and general features for all attribute (commonality), while the specific semantics of each attribute may be less involved (individuality). In other words, the commonality of all attributes may be overemphasized while the individuality of each attribute may be ignored. Actually, for multi-label pedestrian attribute recognition, different attributes are often related with different body regions and semantics. For example, we mainly look at the head region to recognize a pedestrian’s hairstyle while the upper body region is often used to judge the upper clothing style. Moreover, color information often determines a pedestrian’s shoe color, while the texture and shape features are essential to identify the type of shoes. Therefore, learning the individuality of each attribute is also very necessary, which ensures each attribute can learn their own semantics.

In this paper, we propose a split-and-aggregate learning (SAL) to learn both individuality and commonality among pedestrian attributes. The features of each attribute are firstly split out to capture the individualities for all attributes and then aggregated together by using several CNN layers to exploit their commonalities and relations. To fully capture the individuality and commonality, we formulate two SALs in a cascaded way, with one at the feature map level and the other at the feature vector level. For the former, an Attribute-Specific Attention Module (ASAM), which assigns each attribute with several attention maps, is designed to capture the features of an attribute from most relevant body regions. ASAM has been implemented at different feature levels to access abundant semantic information. For the later, we learn the attribute-specific features by using the constrained losses with each loss corresponding to several neurons.

Moreover, we further propose a new feature recombination operation to synthesize new representations. The key idea is to recombine the components of different attributes, which is different from the previous works of creating new samples via generative models (Goodfellow et al. 2014; Kingma and Welling 2014; Zheng et al. 2017; Fu et al. 2019). The extracted features of a sample can be regarded as a combination of the split features of all attributes, with each denoting the semantics of a specific attributes. By shuffling the split features over a batch of samples, new synthetic representations with different attribute semantics can be achieved. In recombination stage, our method keeps the integrity and semantics for each attribute-specific feature while spanning the potential samples’ variability. To our best knowledge, it is the first attempt to synthesize new samples at feature level for pedestrian attribute recognition.

The main contributions of our work are as follows: (1) We propose a novel unified framework with a cascaded split-and-aggregate features learning to capture both individuality and commonality among pedestrian attributes. (2) We propose a new feature recombination operation to synthesize new representations. (3) We propose a novel attribute-specific attention module, which can capture the features from the most important regions/pixels for each attribute. (4) We conduct extensive experiments on five popular pedestrian attribute benchmarks including RAP, PA-100K, PETA, Market-1501 and Duke attribute datasets, which shows the proposed method achieves the new state-of-the-art performance.

Related Works
Pedestrian Attribute Recognition
Earlier methods of pedestrian attribute recognition (Deng et al. 2014; Zhu et al. 2013) typically model each attribute independently based on the hand-crafted features like color and texture histograms. Recently, owing to the great successes of deep learning (Simonyan and Zisserman 2015; He et al. 2016), many approaches based on deep networks also have been developed for pedestrian attribute recognition (Wang et al. 2017; Sarfraz et al. 2017; Liu et al. 2018b; Zhao et al. 2019; Li et al. 2019a, b; Tan et al. 2019b). Previous works mainly solve the task of pedestrian attribute recognition from following aspects: (1) constructing attention mechanisms to capture discriminative features (Liu et al. 2017; Sarafianos et al. 2018; Zhao et al. 2019; Tan et al. 2019b; Tang et al. 2019c); (2) formulating a part-based model by using human poses (Liu et al. 2018b; Li et al. 2018a; Zhao et al. 2018) or Spatial Transformer Networks (STN) (Tang et al. 2019c; 3) exploiting the relations among attributes or image regions (Wang et al. 2017; Zhao et al. 2019); and (4) dealing with the imbalance data problem (Sarafianos et al. 2018; Wang et al. 2019). Most of the previous works construct their models based on the multi-task learning (MTL) framework, while the traditional MTL framework usually prefers to learn the commonality of all attributes while ignoring the individuality of each attribute. In this work, we aim to capture both the individuality and commonality of all attributes, where cascaded split-and-aggregate learning is proposed to achieve this. The proposed method is also different from the work (Tang et al. 2019c), which aims to select the most important regions for each attribute. Our work not only considers to capture the discriminative features for each attribute, but also considers how to aggregate those split features with capturing the commonalities and relations among those attributes. The split-and-aggregate learning is considered to be implemented at different levels, and then learns them jointly and concurrently.

Attention Mechanism
Attention models (Hu et al. 2018a; Fu et al. 2017; Li et al. 2018c; Woo et al. 2018; Liu et al. 2017; Tan et al. 2019b; Guo et al. 2019; Chen et al. 2019; Shuzhe et al. 2019; Xiangyu et al. 2020) have aroused great interests in recent years. Hu et al. (2018a) propose Squeeze-and-Excitation Networks with recalibrating the channel-wise responses by using a channels attention. Li et al. (2018c) jointly learn both soft pixel attention and hard regional attention for person re-identification. Woo et al. (2018) formulate an attention mechanism by sequentially extracting the discriminative features at channel and spatial dimensions. Moreover, Chen et al. (2019) propose a high-order attention to model and utilize the complex and high-order statistics information. Inspired by those works, we also propose an attribute-specific attention module to select important regions/pixels for each attribute.

Augmenting Training Samples
Previous methods of augmenting samples can be classified to following categories: (1) basic image manipulations, like flipping, translating, adding noises, random erasing (Zhong et al. 2020), mixup (Hongyi et al. 2018) and so on, (2) synthesizing new samples by using generative models (Goodfellow et al. 2014; Kingma and Welling 2014; Zheng et al. 2017; Fu et al. 2019). For example, Zheng et al. (2017) generate the synthetic samples with GAN in person re-identification, (3) borrowing the samples from relevant categories (Lim et al. 2011; Tan et al. 2018). For example, Lim et al. (2011) augment data of the classes with few samples by borrowing and transforming examples from other classes, and (4) feature space transfer (Dixit et al. 2017; Liu et al. 2018a). In the above methods, mixup (Hongyi et al. 2018) is somewhat related with our proposed FR, where both our FR and mixup augment training samples by using combinations of different samples and their labels. However, there are still some crucial differences between our proposed FR and mixup. For mixup, it employs a liner combination of a pair of images and their labels to generate new samples. For our FR, it first obtains the split features (separating the features of each attribute), and then just uses a random shuffle over a batch of samples to generate new samples, which keeps the semantic information of each attribute unchanged.

Split-and-Aggregate Learning
Some previous works (Zhang et al. 2020; Tan et al. 2019a; Szegedy et al. 2015, 2016, 2017) also adopt the idea of split-and-aggregate learning to capture more discriminative features. For example, Zhang et al. (2020) propose a Split-Attention Block, which splits the features into several groups and learns them individually. Then, the split features of all groups are aggregated together by using a concatenation. We further use a recurrent fusion to aggregate those branches together. Moreover, inception block also can be regarded as a special case of split-and-aggregate learning (Szegedy et al. 2015, 2016, 2017), where the input features are split by using several different CNN branches and each one learns different features from each other. Then, the block finally aggregates all features together to form more comprehensive features. Our work conducts the split-and-aggregate learning from a different aspect. In the split stage, we first split the attribute-specific features out and capture the individuality for each task/attribute and then aggregate those split features together to learn the commonality among all attributes.

Fig. 1
figure 1
An overview of the proposed CAS-SAL-FR. Two Split-and-Aggregate Learning (SAL) modules are sequentially applied on the feature maps level and feature vector level, where the split operation mainly learns the attribute-specific features for each attribute while the aggregate operation exploits the commonalities and relations among multiple attributes by learning how to aggregate them together. Specifically, the split operation at feature maps level is achieved by an Attribute-Specific Attention Module (ASAM). Moreover, a Feature Recombination (FR) strategy is adopted to synthesize new samples by shuffling the split features

Full size image
Proposed Method
To construct a deep model  for pedestrian attribute recognition, we assume the available training set contains n images and is denoted as ={𝐈𝑖}𝑛𝑖=1, with corresponding labels ={𝐲𝑖}𝑛𝑖=1. Each pedestrian is annotated with m attributes. For the 𝑖th image 𝐈𝑖, the corresponding image-level annotation is denoted as 𝐲𝑖=[𝑦𝑖1,𝑦𝑖2,⋯,𝑦𝑖𝑚], where 𝑦𝑖𝑗 represents the label of the 𝑗th attribute. In the following, we will introduce the proposed network, namely CAScaded Split-and-Aggregate Learning with Feature Recombination (CAS-SAL-FR).

Network Architecture Design
The network architecture of the proposed CAS-SAL-FR is illustrated in Fig. 1. It adopts ResNet-50 (He et al. 2016) as the backbone to extract the features of different levels [the backbone also can be replaced with any other CNN architecture, e.g., GoogleNet (Szegedy et al. 2015) and DenseNet (Huang et al. 2017)]. For convenience, we denote the features after ’res3d’, ’res4f’ and ’res5c’ blocks as 1(𝐈𝑖), 2(𝐈𝑖) and 3(𝐈𝑖), respectively. If we adopt a 256×128 image as the input, 1(𝐈𝑖), 2(𝐈𝑖) and 3(𝐈𝑖) would have the size of 32×16, 16×8 and 8×4, respectively. However, the resolution of 8×4 is too low, which may hardly contain enough information for all attributes. Therefore, we change the stride of the final residual block from 2 to 1. In this way, the size of 3(𝐈𝑖) would be 16×8. After extracting those features, two SALs are formulated in a cascaded way to learn both the individuality and commonality for all attributes, which will be introduced in the following.

(I) SAL at Feature Map Level After obtaining 1(𝐈𝑖), 2(𝐈𝑖) and 3(𝐈𝑖), each of them is followed by an ASAM to select some important pixels or regions for each attribute and capture the attribute-specific features (also re-called as the split features). For the employed ASAM, its detailed structure is illustrated in Fig. 2. It contains multiple sub-networks, each of which extracts the most relevant features for a specific attribute. More specifically, each sub-network has two streams, with one generating attention masks and the other extracting high-level features. For clarity, we take the sub-network for the 𝑗th attribute at the 𝜅th level as an example (𝜅∈{1,2,3}). For the upper stream, the attention masks 𝐌𝜅𝑖𝑗 are generated by using several convolutional layers and a softmax function (the softmax function is applied to the spatial dimension including height and width). Inspired by the works (Chen et al. 2017; Yu and Koltun 2016), we use the spatial pyramid convolutional layers with different receptive fields to capture abundant semantics. For the lower stream, it only contains two convolutional layers to extract high-level features 𝐇𝜅𝑖𝑗. To reduce the number of parameters, the first convolutional layer in both two streams is shared for all attributes. Finally, the attentive features 𝐗𝜅𝑖𝑗 for the 𝑗th attribute are generated by an element-by-element multiplication between the attention masks 𝐌𝜅𝑖𝑗 and high-level features 𝐇𝜅𝑖𝑗. The whole process can be denoted as:

𝐗𝜅𝑖𝑗=,𝜅𝑗(𝜅(𝐈𝑖))
(1)
where  indicates the feature map level and ,𝜅𝑗 represents the 𝑗th subnetwork in the ASAM at the 𝜅th level feature. In our implementations, we let 𝐗𝜅𝑖𝑗 has the same shape of 𝑎𝜅×ℎ𝜅×𝑤𝜅 (the number of channels, height and width, respectively), with each 𝑎𝜅 channels capturing the discriminative features for a specific attribute.

Fig. 2
figure 2
Illustration of the proposed ASAM. For each convolutional layer, {𝐾,𝐶} indicates its employed kernel size and output channels, respectively. In the spatial pyramid convolutional layers, K1, K2 and K3 represent the kernel size of 1×1, 3×3, and 3×3 with a dilatation rate of 2. We share the first convolutional layer in both two streams for all sub-networks, which aims to reduce the parameters of the network. The number of input channels 𝑐1, 𝑐2 and 𝑐3 are 512, 1024 and 2048, respectively

Full size image
Till now, we have only allocated several attentive maps for each attribute, without forcing them to capture the information only from that attribute. To achieve this, additional constrained networks should be added. For each attribute-specific feature map 𝐗𝜅𝑖𝑗, it would be followed by a small constrained network to produce the corresponding predicted score for the 𝑗th attribute, which can be mathematically denoted as:

𝑝,𝜅𝑖𝑗=𝜙,𝜅𝑗(𝐗𝜅𝑖𝑗)
(2)
where 𝜙,𝜅𝑗(⋅) indicates a constrained network with a convolutional layer, a fully connected layer and a sigmoid function as shown in Fig. 1. In this way, the predicted score 𝑝,𝜅𝑖𝑗 is generated only from the features 𝐗𝜅𝑖𝑗, which ensures its learning under the label supervision of the 𝑗th attribute. For our proposed ASAM, it is different from Squeeze-and-Excitation Networks (SE-Net) (Hu et al. 2018a) in following aspects: (1) SE-net can be considered as a channel attention which remodulates neurons’ responses by a channel-wise mask, while our ASAM is constructed based on a spatial attention mechanism; (2) Our ASAM aims to capture attribute-specific attention features with considering each attribute individually.

After the splitting stage, we aggregate those split features by using a concatenation layer, a convolutional layer and a Global Average Pooling (GAP) as shown in Fig. 1. Mathematically, we denote such aggregation as (⋅), and thus the aggregated features are obtained by:

𝐀𝑖=(𝐗1𝑖,𝐗2𝑖,𝐗3𝑖)
(3)
where 𝐗𝜅𝑖 means {𝐗𝜅𝑖𝑗}𝑚𝑗=1. In the aggregating process, the network learns how to aggregate the features from different attributes, it also learns the commonalities and relations among those attributes.

(II) SAL at Feature Vector Level At feature vector level, a SAL is further employed as shown in Fig. 1. After the GAP layer in the former aggregation module, m fully connected layers are employed to extract the split features with each layer corresponding to an attribute. We use 𝑗 to denote a fully connected layer for the 𝑗th attribute, and then the split features can be produced by:

𝐱𝑖𝑗=𝑗(𝐀𝑖).
(4)
where  indicates the feature vector level. Similarly, to ensure 𝐱𝑖𝑗 only captures the information for the 𝑗th attribute, those features are then followed by a fully connected layer to generate the predicted score 𝑝𝑖𝑗 [the generated process is similar to Eq. (2)], which is optimized by the label supervision of the 𝑗th attribute.

Then, two fully connected layers are further employed to aggregate those split features together. It helps to exploit the commonalities and relations among those attributes. Different from the previous aggregating module that outputs the aggregated features, this module directly generates the predicted scores 𝐩𝑖 for final attribute predictions, which is represented as:

𝐩𝑖=(𝐱𝑖)
(5)
where 𝐱𝑖=[𝐱𝑖1,⋯,𝐱𝑖𝑚] denotes the feature vector for all attributes, and  indicates two fully connected layers.

Remark
Our cascaded split-and-aggregated learning is different from previous works (Tan et al. 2019b; Tang et al. 2019c): (1) The work Tang et al. (2019c) extracts the attribute-specific features by using a hard regional attention while we our ASAM uses a soft scheme of assigning each attribute with several attention maps to explore more information. (2) Previous works (Tan et al. 2019b; Tang et al. 2019c) do not consider the attribute-specific features learning in feature vector level, while we achieve this by using a simple but elegant structure (constrained loss). (3) Two SALs are then formulated in a cascaded way to access more effective features.

Fig. 3
figure 3
An illustration of the feature recombination. Each row indicates a feature vector or label vector for a sample, and each column indicate the features or labels from a specific attribute over a batch of samples

Full size image
Feature Recombination (FR)
The proposed FR aims to synthesize new samples by shuffling the split features over a batch of samples. Figure 3 illustrates a simple example of FR on a batch of 3 samples, and each sample is annotated with 4 attributes. In the proposed FR, a random shuffle is conducted at the batch level, where the split features from different samples will be recombined to be new samples. The labels of those new synthetic samples are obtained by a consistent shuffle on the original labels. For example, the new corresponding label vector for the synthetic sample [𝐱31,𝐱22,𝐱13,𝐱24] is [𝑦31,𝑦22,𝑦13,𝑦24]. The attribute-specific features 𝐱𝑖𝑗 contains the semantics of indicating the absence or presence of the corresponding attribute. After feature recombination, the value of 𝐱𝑖𝑗 remains unchanged, where semantic information will be maintained. Thus, the corresponding label of the new synthetic sample for the 𝑗th attribute also will be consistent with the original label 𝑦𝑖𝑗.

Assume the batch size we used in the training stage is set as 𝑛𝑏𝑠. For the convenience in the following, we remove the subscript j in the sign of features to denote the features over a batch of samples. For example, the split features over a batch can be denoted as 𝐗𝜅=[𝐗𝜅1,⋯,𝐗𝜅𝑛𝑏𝑠] and 𝐱=[𝐱1,⋯,𝐱𝑛𝑏𝑠]. Their corresponding labels also can be denoted as 𝐲=[𝐲1,⋯,𝐲𝑛𝑏𝑠]. We denote the random shuffle operation as (⋅), and thus the recombined split features at feature map level and their corresponding labels can be generated as:

𝐗𝜅,1=(𝐗𝜅),𝐲1=(𝐲).
(6)
All of {𝐗𝜅,1}3𝜅=1 should employ a consistent shuffle to ensure the semantic consistency in the later aggregating stage, and thus we use the same superscript 1 to indicate the consistent shuffle among them, and such superscript would be used in a similar way in the following section. Later, 𝐗𝜅,1 is further used for inference to generate the predicted scores 𝐩1. The inference details can refer to Eqs. (3), (4) and (5). Similarly, the random shuffle also can be conducted on 𝐱, and thus the corresponding recombined features 𝐱2 and the corresponding new labels 𝐲2 can be obtained by:

𝐱2=(𝐱),𝐲2=(𝐲).
(7)
The predicted scores 𝐩2 of 𝐱2 can be generated by using Eq. (5).

For a specific attribute of a generated sample in the proposed FR, its values can be that attribute’s of an arbitrary sample over a sample batch. Thus, we can generate a lot of new samples as long as we do more random shuffles. To generate more synthesized samples, we conduct the random shuffle 10 times, which generates feature representations {𝐗𝜅,1} and {𝐱1} with a number of 10×𝑛𝑏𝑠 for training.

Model Training
In the training stage, the weighted binary cross-entropy loss (Li et al. 2015; Tan et al. 2019b; Tang et al. 2019c) is employed as the loss function on homogeneous binary datasets, including RAP, PA-100K, and PETA. While the multi-class softmax loss is employed on heterogeneous attribute datasets, including Matket-1501 and Duke. For clarity, we take the weighted binary cross-entropy loss as an example, and the loss form for the multi-class softmax loss can be produced similarly. All of the predicted scores are followed by loss functions. For the predicted score 𝑝,𝜅𝑖𝑗 of the 𝑗th attribute, its loss over a batch can be calculated as:

,𝜅𝑗=−1𝑛𝑏𝑠∑𝑖=1𝑛𝑏𝑠𝜌𝑖𝑗(𝑦𝑖𝑗log(𝑝,𝜅𝑖𝑗)+(1−𝑦𝑖𝑗)log(1−𝑝,𝜅𝑖𝑗))
(8)
where 𝜌𝑖𝑗 is a penalty coefficient used to alleviate the imbalanced data problem in pedestrian attribute recognition, and set the same as the work (Tan et al. 2019b). We use 𝑟𝑗 to denote the ratio of the images with the 𝑗th attribute, and then 𝜌𝑖𝑗 is calculated as follows: 𝜌𝑖𝑗=12𝑟𝑗‾‾‾√ , if 𝑦𝑖𝑗=1; otherwise 𝜌𝑖𝑗=12(1−𝑟𝑗)‾‾‾‾‾‾√ . The sum of the losses over all attributes can be denoted as: ,𝜅=∑𝑗,𝜅𝑗. The losses for the predicted scores 𝐩 and 𝐩 can be produced in a similar way, and we denote them as  and , respectively. For the generated samples, the losses 1 and 2 are calculated based on predicted scores 𝐩1 and 𝐩2 and corresponding labels 𝐲1 and 𝐲2, respectively. The calculations of 1 and 2 have a similar form to Eq. 8 but with minor changes. Here we take the 1𝑗 as an example and it can be formulated as:

1𝑗=−110×𝑛𝑏𝑠∑𝑖=110×𝑛𝑏𝑠𝜌𝑖𝑗(𝑦1𝑖𝑗log(𝑝1𝑖𝑗)+(1−𝑦𝑖𝑗1)log(1−𝑝1𝑖𝑗))
(9)
Then, the losses for all attributes are summed together: 1=∑𝑗1𝑗. The loss 2 is calculated in a similar way. The overall loss is the sum of all of those losses, and can be denoted as:

𝑜𝑣𝑒𝑟𝑎𝑙𝑙=+,1+,2+,3++1+2.
(10)
In the above equation,  denotes the loss for the whole network training. ,1, ,2, ,3 and  are the constrained losses for extracting the attribute-specific features. Besides, 1 and 2, which are used to guide the learning of synthetic samples, can be regarded as the regularization terms to span the potential samples’ variability. In the test stage, the predictions are obtained based on the predicted scores 𝐩.

Experiments
We first introduce the datasets, settings, and evaluation metrics employed in our experiments. Then, the experimental results and analysis are presented to validate the effectiveness of our method.

Datasets and Metrics
Five popular datasets including PA-100K (Liu et al. 2017), RAP (Li et al. 2018b), PETA (Deng et al. 2014), Market-1501 (Lin et al. 2019) and Duke (Lin et al. 2019) are employed for experiments. For PA-100K, RAP and PETA three datasets, all of them contain homogeneous binary attributes, while for both Market-1501 and Duke datasets, they contain heterogeneous attributes where different attributes may have a different number of categories. We adopt both homogeneous and heterogeneous attribute datasets for experiments to thoroughly verify the effectiveness of the proposed method.

PA-100K contains 100,000 pedestrian images from various outdoor scenes and is the largest dataset for pedestrian attribute recognition. Each image is annotated with 26 commonly used attributes, e.g., gender, clothing types. According to the works (Liu et al. 2017; Tan et al. 2019b) the dataset is randomly split into three subsets with 80,000, 10,000 and 10,000 images for training, validation and test, respectively. RAP is the largest pedestrian attribute dataset of indoor scenes, with containing 41,585 images. 51 attributes with the positive ratio over 1% are selected for experiments. We evaluate the proposed method over 5 random splits, where 33,268 images are used for training and 8317 images for the test in each split. We then average the results overall splits to achieve the final result. PETA is a classical dataset for pedestrian attribute recognition. Following the works (Deng et al. 2014; Tan et al. 2019b), 35 binary attributes are selected for evaluation. The whole dataset is split into three sub-sets: 9500 images for training, 1900 images for validation and 7600 images for test. Market-1501 attribute dataset contains 32,688 images of 1501 identities. This dataset is annotated in the identity level, and each image is annotated with 10 binary attributes and 2 multi-class attributes. Following to the works (Lin et al. 2019; Tan et al. 2019b), 751 identities are used for training, and 750 identities are used for the test. Duke attribute dataset is also labeled in the identity level. It contains 34,183 images from 1812 identities, and each image is annotated with 8 binary attributes, and 2 multi-class attributes. According to the works (Lin et al. 2019; Tan et al. 2019b), 16,522 images are used for training and 17,661 images are used for the test.

According to previous works (Liu et al. 2017; Li et al. 2018b; Tan et al. 2019b; Tang et al. 2019c), a label-based criterion mean accuracy (mA) and four instance-based criteria accuracy (Accu), precision (Prec), Recall, and F1 are employed for evaluation on PA-100K, RAP, and PETA datasets. When evaluating on Market-1501 and Duke datasets, we employ the accuracy on all attributes as the criterion used in Lin et al. (2019) and Tan et al. (2019b).

Experimental Settings
The RGB image with a size of 256×128 is used as the input in our experiments. The input image is first normalized by subtracting a mean and dividing a standard deviation for each color channel before being fed to the network. We also employ the data augmentation to improve the performance of pedestrian attribute recognition, including random horizontal flipping, random scaling, rotation, translation, cropping, erasing and adding random gaussian blurs. Those augmentations also facilitate the model to handle the variations of pedestrian position, human poses, camera angle, and so on. For the attribute-specific features at the feature map level, the output shapes 𝑎𝜅×ℎ𝜅×𝑤𝜅,𝜅=1,2,3 are set to 1×32×16, 3×16×8 and 6×16×8, respectively. The parameter selection of 𝑎𝜅 can be founded in Sect. 4.5. The feature dimension of 𝐱𝑖𝑗 is set as 32. Therefore, the shape of those generated features 𝐗1,1, 𝐗2,1, 𝐗3,1 and 𝐱2 are 10⋅𝑛𝑏𝑠×1⋅𝑚×32×16, 10⋅𝑛𝑏𝑠×3⋅𝑚×16×8, 10⋅𝑛𝑏𝑠×6⋅𝑚×16×8 and 10⋅𝑛𝑏𝑠×32⋅𝑚, respectively. All networks are initialized with the pretrained weights of ImageNet (Deng et al. 2009), and then finetuned on pedestrian attribute datasets. We employ the Adam optimizer (Kingma and Ba 2015) for optimization, and set 𝛽1=0.9, 𝛽2=0.999 and 𝜖=10−8. The batch size is set to 32 in the training stage. The learning rate is started with 0.0001 and reduced by a factor of 10 along with the increasing iterative times. All models are trained and tested with PyTorch on GTX 1080Ti GPU.

Ablation Studies
In this section, we conduct the ablation studies on the following modules: SAL at Feature Map level (SAL-FM), SAL at Feature Vector level (SAL-FV), Feature Recombination (FR) and cascaded learning (CAS). Following to the work (Tang et al. 2019c), two important criteria, namely mA and F1, are employed for evaluation. The experiments are conducted on RAP, PA-100K, and PETA datasets.

Table 1 The ablation studies of the SAL-FM
Full size table
Table 2 The ablation studies of the SAL-FV
Full size table
Analysis on SAL-FM Various experiments are conducted by removing attention masks, multi-level features, split losses and aggregating layers. From experimental results in Table 1 (‘w/o’ denotes ‘without’), we find attention masks, split losses, and aggregating layers are extremely important to SAL-FM. When removing the attention masks, attribute-specific attention modules degenerate into plain CNN layers and lose their strong abilities to select the important regions/pixels. When removing the splitted losses, the module fails to capture the attribute-specific features, which hardly capture the individuality for each attribute. When removing the aggregating layers, the module makes the predictions based on the averaging scores of 𝐩,1𝑖, 𝐩,2𝑖 and 𝐩,3𝑖, which are generated by only using split features. We can observe that the performance dramatically drops by over 10% at mA. The drop may be due to the following reasons: (1) In split features, the features of each attribute are denoted by the feature maps with only several channels (1, 3, 6 channels are set in the 1th, 2th and 3th feature level, respectively), which may hardly contain enough semantics for each attribute. (2) The information exchanges are not allowed among multiple attributes at high-level features, which hardly capture the relations among them. (3) Although the split features contain the individual semantics for each attribute, they may still be less discriminative than the aggregated features.

Analysis on SAL-FV To analyze this module, the network with removed the split losses and aggregating layers are employed for comparisons. The experimental results are shown in the Table 2. Although the split losses can only improve the performance slightly, those losses are indispensable components of extracting attribute-specific features, which are very necessary for later feature recombination and cascaded learning modules. When removing the aggregating layers, the performance drops significantly on all three datasets. The poor performance may due to the same reasons as mentioned above. The results also show the aggregating layers are very necessary after splitting the features.

Analysis on Feature Recombination We further add the feature recombination on both SAL-FM and SAL-FV, which are denoted as SAL-FM-FR and SAL-FV-FR, respectively. The comparisons between methods with and without feature recombination are shown in Table 3. The feature recombination improves the performance on both SAL-FM and SAL-FV networks, which clearly demonstrates its effectiveness. More specially, for SAL-FM, it improves the mA by 0.95%, 0.77% and 0.36% on RAP, PA-100K and PETA datasets, respectively.

Table 3 The ablation studies of the feature recombination
Full size table
Table 4 The ablation studies of the cascaded learning
Full size table
Analysis on Cascaded Learning We further combine SAL-FM-FR and SAL-FV-FR together, and formulate a cascaded Split-and-Aggregate Learning with Feature Recombination (CAS-SAL-FR). As shown in Table 4, the performance can be further improved on all three datasets, which shows the effectiveness of cascaded learning.

Table 5 The comparisons on RAP dataset
Full size table
Table 6 The comparisons on PA-100K dataset
Full size table
Table 7 The comparisons on PETA dataset
Full size table
Comparisons with State-of-the-arts
In this subsection, we compare the proposed CAS-SAL-FR against previous state-of-the-art methods, including HP-net (Liu et al. 2017), VeSPA (Sarfraz et al. 2017), JRL (Wang et al. 2017), Fusion (Li et al. 2018a), LG-Net (Liu et al. 2018b), VAA (Sarafianos et al. 2018), GRL (Zhao et al. 2018), RA (Zhao et al. 2019), JLPLS-PAA (Tan et al. 2019b), CoCNN (Han et al. 2019), Da-HAR (Wu et al. 2020), MT-CAS (Zeng et al. 2020), (Jia et al. 2020), DTM+AWK (Zhang et al. 2020), Gao et al. (2019), Tang et al. (2019c), PedAttriNet (Lin et al. 2019) and APR (Lin et al. 2019). On RAP, PA-100K and PETA datasets, we further add the mean accuracy of mA, Accu, Prec, Recall and F1 five criteria for evaluation (denoted as mFive). This is because some models may perform very well on a specific criterion while obtaining low performance on other criteria. Take mFive into account, and the evaluation can be more comprehensive.

The experimental results of the homogeneous binary attributes on RAP, PA-100K and PETA datasets are shown in Tables 5, 6 and 7, respectively. The experimental results of the heterogeneous attributes on Market-1501 and Duke attribute datasets are shown in Table 8. The proposed CAS-SAL-FR achieves the highest performance on all five datasets (including three homogeneous binary attribute datasets and two heterogeneous attribute datasets), showing its superiority for pedestrian attribute recognition. The mean performance of our method on RAP, PA-100K, PETA, Market, and Duke attribute datasets are 78.94%, 85.18%, 85.57%, 88.30% and 85.91%, respectively. Moreover, compared with the recent work, Tang et al. (2019c), the proposed method outperforms it by 0.66%, 1.73% and 0.29% on RAP, PA-100K, and PETA datasets, respectively. For the recent work JLPLS-PAA, our method outperforms it by 1.13%, 0.71%, 0.58%, 0.42% and 0.67% on RAP, PA-100K, PETA, Market-1501 and Duke attribute datasets, respectively. Those improvements are promising because the performance is averaging on dozens of attributes where the accuracies of some attributes are really hard to be improved due to low resolution, occlusions, unbalanced data, and so on.

Owing to the lack of a unified benchmark method in the field of pedestrian attribute recognition, different methods may adopt different backbones. The backbone of all models, e.g., AlexNet (Krizhevsky et al. 2012), CaffeNet (Jia et al. 2014), Inception_v2/v3 (Ioffe and Szegedy 2015), GoogleNet (Szegedy et al. 2015), DenseNet-201 (Huang et al. 2017), ResNet-50/101 (He et al. 2016) and SE-Net (Hu et al. 2018b), also have been clarified. Our method is constructed based on ResNet-50. Although some previous methods (Tan et al. 2019b; Sarafianos et al. 2018) construct their models based on more advanced backbones. For example, JLPLS-PAA (Tan et al. 2019b) and VAA (Sarafianos et al. 2018) build their models based on SE-Net (Hu et al. 2018a) and DenseNet-201 (Huang et al. 2017), respectively. However, our method can still achieve better performance. Moreover, in some previous methods (Tan et al. 2019b; Zhao et al. 2018; Gao et al. 2019), external information is employed to improve the performance further. For example, JLPLS-PAA (Tan et al. 2019b) captures the external semantics from human parsing, and GRL (Zhao et al. 2018) utilizes the human pose information for human body localization. Moreover, some researchers (Wang et al. 2017) employ an ensemble of multiple models to obtain higher performance. Our proposed CAS-SAL-FR still outperforms those models, which shows the effectiveness of the proposed cascaded split-and-aggregate learning and feature recombination.

Table 8 The comparisons on Market-1501 and Duke datasets
Full size table
Fig. 4
figure 4
The mA results on RAP of a all classifiers, b the network with using the attention module of different settings and c removing the splitting operation and low-level features

Full size image
Further Analysis
Performance on all Classifiers We visualize the results of all classifiers in Fig. 4a. FM(1), FM(2), FM(3), FV denote the classifiers at 1st,2nd,3th feature map levels and feature vector level, respectively. The highest performance is obtained by the final classifier (denoted by Final). The poor performance obtained by FM(1), FM(2), FM(3) and FV may due to that the split features may be less discriminative than the aggregated features’.

Analysis on 𝑎𝜅 in ASAM The number of channels for each attribute of the ASAM at 1st,2nd,3th feature map levels is denoted as 𝑎1,𝑎2,𝑎3 (denoted by 𝑎1−𝑎2−𝑎3), respectively. The experimental results by varying their values are shown in Fig. 4b. The highest performance is achieved using 1−3−6, where the small number of ASAM channels is used at low levels. 𝑎1=1, 𝑎2=3 and 𝑎3=6 are adopted in other experiments.

Removing Split Operation To investigate how much the split operation can contribute to the final performance, we conduct an additional experiment with removing the split operation on both feature map and vector levels. As shown in Fig. 4c, the mA is dropped by 1.42% when removing the split operations, which shows their effectiveness. The split operation helps the network to capture the individuality for each attribute, which ensures each attribute can learn its own semantics.

Removing Low-level Features To further verify the effectiveness of the low-level features (including both 1th and 2th levels), we conduct the experiments by removing those features in our CAS-SAL-FR. The experimental results can be found in Fig. 4c. The mA is dropped by 0.41% when removing low-level features, which shows the low-level features also contribute a lot to the final performance.

Table 9 The comparisons on parameters, FLOPs and speed
Full size table
Efficiency Analysis We compare our method with three popular methods in parameters, FLOPs and inference speed, including JRL (Wang et al. 2017), VAA (Sarafianos et al. 2018) and JLPLS-PAA (Tan et al. 2019b). As shown in Table 9, our method has certain advantages compared with three compared method. For example, JLPLS-PAA is constructed based on two large models, where one model (SE-BN-inception) is used for pedestrian attribute recognition and the other model (PSPNet, ResNet-101) is used for generate pedestrian parsing maps. Thus, JLPLS-PAA contains lots of parameters and FLOPs.

GPU memory consumption of FR Although FR generates a large number of synthetic samples for training, all of them only need to be delivered in the last few layers of the network, which takes up very little GPU memory. We conduct experiments with two settings, namely CAS-SAL and CAS-SAL-FR to verify this. Experiments show that CAS-SAL takes up 5357M GPU memory when running with a batch size of 32, and the GPU memory occupation increases to 5697M when adding the proposed FR strategy. This indicates that only about 340M GPU memory are taken up for FR, which verifies that FR only takes up very little GPU memory.

Table 10 Comparisons between FR and mixup
Full size table
FR versus mixup We conduct experiments with both FR and mixup to compare their performance, and the corresponding experimental results are listed in Table 10. Compared with using mixup, FR helps the model to obtain a higher accuracy on mA. For example, on RAP dataset, the mA accuracty of SAL-FM-FR is 3.5% higher than that of SAL-FM + mixup. In our FR, we just uses a random shuffle on the split features over a batch of samples to generate new samples, which keeps the semantic information of each attribute unchanged. For mixup, it achieves low mA accuracy may due to that the liner combination may destroy the integrity of the feature especially when the quality of images and features are not so high.

Visualizations of Attention We visualize the attention masks in ASAM after 3(𝐈𝑖). We select 6 representative attributes, i.e., Gender, Glasses, HandBag, LongSleeve, UpperPlaid and Shorts, and visualize their mean attention masks. As shown in Fig. 5, different attributes may focus on different regions to extract the discriminative features for corresponding attributes. For example, the attention module focuses on the head region for Glasses attribute. More visualizations on RAP and PETA datasets can be founded in Figs. 6 and 7. The visualizations can qualitatively show the proposed ASAM can really capture discriminative features from some important regions for each attribute. ASAM is designed to extract discriminative attribute-specific features for each attribute with attention mechanisms. Similarly, Our ASAM also can be extended to the fields of multi-task learning and multi-label classification, which helps the network to learn the discriminative features of each attribute/task separately and capture specific semantics of each task/category.

Fig. 5
figure 5
Visualizations of the attention masks of ASAM on PA-100K dataset. a Indicates the raw image, and b–g represent the attention masks of Gender, Glasses, HandBag, LongSleeve, UpperPlaid and Shorts attributes, respectively

Full size image
Fig. 6
figure 6
Visualizations of attention masks of ASAM on RAP dataset. a Indicates the raw image, and b–g represent the attention masks of Female, BlackHair, LongTrousers, LeatherShoes, Calling and CarryingbyHand attributes, respectively

Full size image
Fig. 7
figure 7
Visualizations of attention masks of ASAM on PETA dataset. a Indicates the raw image, and b–g represent the attention masks of carryingBackpack, carryingOther, footwearLeatherShoes, hairLong, personalMale and lowerBodyShorts attributes, respectively

Full size image
Qualitative Analysis Two predicted examples on the test set of the PA-100K dataset are shown in Fig. 8. The ground truth (GT) labels and the predictions of the baseline ResNet-50 and CAS-SAL-FR are denoted by red, green, and blue colors, respectively. Benefited from the cascaded split-and-aggregate learning and feature recombination, the proposed CAS-SAL-FR can achieve more reliable predictions than the baseline ResNet-50. For the first image, some attributes like Gender and Skirt&Dress are wrongly predicted by ResNet-50, while our CAS-SAL-FR can well correct them.

Fig. 8
figure 8
Two prediction examples on PA-100K dataset

Full size image
Conclusions
In this work, we have proposed a new framework for pedestrian attribute recognition, named CAScaded Split-and-Aggregate Learning with Feature Recombination (CAS-SAL-FR). At first, a cascaded Split-and-Aggregate Learning (SAL) has been proposed to capture both the individuality and commonality for all pedestrian attributes. Besides, feature recombination has been further proposed to synthesize more training representations for achieving better performance. The experiments have been conducted on five popular datasets including RAP, PA-100K, PETA, Market-1501 and Duke attribute datasets, showing the proposed method achieves the new state-of-the-art performance. Finally, we also have presented feature visualizations and a comprehensive analysis on CAS-SAL-FR to qualitatively verify its effectiveness.