Image/video compression and communication need to serve both human vision and machine vision. To address this need, we propose a scalable image compression solution. We assume that machine vision needs less information that is related to semantics, whereas human vision needs more information that is to reconstruct signal. We then propose semantics-to-signal scalable compression, where partial bitstream is decodeable for machine vision and the entire bitstream is decodeable for human vision. Our method is inspired by the scalable image coding standard, JPEG2000, and similarly adopts subband-wise representations. We first design a trainable and revertible transform based on the lifting structure, which converts an image into a pyramid of multiple subbands; the transform is trained to make the partial representations useful for multiple machine vision tasks. We then design an end-to-end optimized encoding/decoding network for compressing the multiple subbands, to jointly optimize compression ratio, semantic analysis accuracy, and signal reconstruction quality. We experiment with two datasets: CUB200-2011 and FGVC-Aircraft, taking coarse-to-fine image classification tasks as an example. Experimental results demonstrate that our proposed method achieves semantics-to-signal scalable compression, and outperforms JPEG2000 in compression efficiency. The proposed method sheds light on a generic approach for image/video coding for human and machines.

Access provided by University of Auckland Library

Introduction
Image/video contains rich semantic information, being one of the main information sources for human. With the explosive growth of image/video data, it has become impossible to fully rely on limited manpower to understand massive images. Instead, the development of machine vision algorithms, especially with the help of deep learning technologies, has greatly improved the efficiency of machine understanding of images. Nonetheless, machine vision could not completely replace human observation, so human–machine collaborative judgment will last for a while. Note that human can directly view images, but machine vision generally needs to convert pixels to compact semantic representations, namely features. In particular, various machine vision tasks generally require different features.

In order to reduce the cost of storage and transmission, images in typical scenarios are compressed with information loss. Traditional image compression algorithms do not consider feature fidelity, so the compression artifacts under low bit rates seriously affect the semantic analysis accuracy (Poyser et al. 2020; Dejean-Servières et al. 2017; Dodge and Karam 2016). A feasible solution is to obtain and compress compact feature representations extracted from the original image. Note that it is difficult to reconstruct the image only based on the features. For human–machine collaborative judgment, it appears necessary to compress and transmit both the features and the image itself simultaneously.

Scalable image compression is an promising approach for jointly compressing the image and features. Scalable compression means that the compressed bitstream can be partially decoded to obtain meaningful output. Note that the features are compact representations of the image and the information contained in the features is a subset of the image information. Accordingly, the base layer can be the feature bitstream, which contains less information of the image about its semantics. The enhancement layer may contain richer semantics-related information, and/or the information that is used to reconstruct the signal. In addition, there is redundancy between the image and its features, so the enhancement layer may refer to the base layer to improve the compression efficiency.

Some studies (Wang et al. 2019; Hu et al. 2020) propose scalable image compression methods based on feature extraction and image generation to fulfill the need of human–machine collaborative judgment. Feature extraction is a process of information contraction (Simonyan and Zisserman 2014; Zhao et al. 2019; Latif et al. 2019), during which a large amount of task-independent information related to the original image is lost, resulting in the lack of clear correspondence between high-level features and low-level pixels, namely semantic gap (Kwaśnicka and Jain 2018). The semantic gap makes it difficult to predict an image based on its sparse and compact features, and further affects the flexibility of increasing semantics-related information through partial decoding of the scalable bitstream. In addition, the extracted features may not be compact enough (Zhao et al. 2020; Ruder 2017; Baxter 1997), which incurs unnecessary bitstream cost.

In this paper, we target at the image compression for human–machine collaborative judgment from the perspective of jointly considering feature extraction, feature compression, and image compression. Firstly, to bridge the semantic gap between image and features, inspired by the scalable image coding standard, JPEG2000 (Christopoulos et al. 2000), we design a learned revertible transform, namely a hierarchical signal representation method, as illustrated in Fig. 1. The transform is trainable so that semantic tasks can be well performed by intercepting partial features. The information contained in the partial features is a subset of image information, which enhances the interpretability of the scalable structure. Secondly, we design an end-to-end encoding/decoding network to achieve layered compression of the features. Our specific contributions are summarized as follows.

First, we propose a task-driven learned revertible transform that converts an image into compact features and achieves hierarchical representations of image information. With increasing features, semantics-related information contained in the features is gradually enriched.

Second, we design a layered compression network to compresses the multiple features into a scalable bitstream. We use the end-to-end optimization strategy to achieve the joint optimization of semantic accuracy, signal fidelity, and compression ratio.

Third, we use coarse-to-fine image classification and image reconstruction as a motivating example to conduct experiments, and verify the effectiveness of the proposed semantics-to-signal scalable image compression.

Fig. 1
figure 1
Conceptual illustration of the proposed semantics-to-signal scalability with learned revertible representations. Image signal, i.e. pixels, is transformed into a set of features. The transform is revertible in the sense that the image can be perfectly reconstructed using all the features. Using partial features we can perform semantic analysis tasks; using more features, we have more semantic information. In this figure, we show a coarse-to-fine image classification task: with more features we perform finer-grained classification, from “Wren”, “House wren”, to “Northern house wren”

Full size image
The remainder of this paper is organized as follows. Related work is presented in Sect. 2. We elaborate on the hierarchical representation of the proposed semantics-to-signal scalable bitstream in Sect. 3. In Sect. 4, we introduce the proposed feature representation network. Section 5 demonstrates the proposed layered compression network for the learned representations. In Sect. 6, we show the experimental results about semantic analysis accuracy and compression efficiency. Finally, Sect. 7 concludes the paper.

Related Work
Image and features need to be compressed simultaneously for human–machine collaborative judgment. In this section, we introduce in detail the related work from two perspectives: compression and image representation.

Image and Feature Compression
Studies in Johnston et al. (2018), Dejean-Servières et al. (2017), Dodge and Karam (2016) have shown that compression artifacts have an impact on machine vision tasks. Taking classification as an example, compression has little impact on the classification accuracy at high bit rates, but at low bit rates, compression will seriously lower the classification accuracy.

To make images serve both human vision and machine vision, signal distortion and semantic analysis accuracy need to be considered in the compression. Ma et al. conduct a systematic review, analyze the joint compression of image and features (Ma et al. 2018), and explain the advantages of the joint image-feature compression for reconstruction quality and analysis accuracy. Wang et al. propose a scalable image coding framework for face recognition task (Wang et al. 2019), where base layer and enhancement layer are used to represent face features and signal residuals, respectively. The framework uses traditional coding technologies, such as quantization and entropy coding, on the face features, and uses a network-based compression scheme for the signal residuals. Its compression performance surpasses JPEG and JPEG2000 while maintaining semantic analysis accuracy. Hu et al. (2020) extend the strategy to face keypoint detection task, and further unify the entire compression scheme with a deep neural network. Yan et al. propose an image compression method based on semantic scalability in Yan et al. (2020). The multi-layer features of the deep network are compressed into a scalable bitstream, which can serve multi-grained classification tasks, verifying the advantage of joint compression for significant bits saving. Besides, some researchers (Zhang et al. 2016; Duan et al. 2020; Xia et al. 2020) extend the concept of scalability to videos and verify the effectiveness of joint video-feature compression.

In the recent years, end-to-end optimized image compression based on deep neural networks has demonstrated more flexible and efficient image compression capabilities. Toderici et al. propose the first end-to-end image coding method based on recurrent neural network (RNN) (Toderici et al. 2015), and achieve scalable coding by iteratively invoking the RNN-based encoder to compress the image or residual. Ballé et al. propose the first end-to-end image coding method based on convolutional neural network (CNN) (Ballé et al. 2016). After that, hyper-prior model (Ballé et al. 2018) and autoregressive model (Minnen et al. 2018; Lee et al. 2018) are introduced for more efficient entropy coding. By using the non-local attention module (Li et al. 2020; Zhou et al. 2019; Chen et al. 2019), the compression efficiency is further improved. Nowadays, CNN-based end-to-end coding methods outperform the state-of-the-art non-deep image coding scheme, Better Portable Graphics (BPG),Footnote1 significantly. Our encoding/decoding network is also based on CNN and adopts the hyper-prior model as well as the non-local attention module. The existing end-to-end compression methods, similar to non-deep methods, focus on improving the objective or subjective quality of reconstructed images, but ignore the fidelity of semantic information. Torfason et al. propose to use the same bitstream for machine vision and image reconstruction (Torfason et al. 2018). Intuitively, machine vision tasks (e.g. classification, detection, recognition) usually require less information quantity than image reconstruction does. Using the scheme of Torfason et al. (2018), the number of bits suitable for machine vision may be too few to reconstruct visually pleasing image, and the number of bits suitable for image reconstruction may be too redundant for machine vision. Different from their work, our designed bitstream is scalable; in our bitstream, the bits used for machine vision occupy only a small fraction (e.g. less than 10%, see Sect. 6.3.1) of the entire bitstream.

Image Representation via Transforms
Signal representation is an important approach for image analysis and processing. In image compression, discrete cosine transform (DCT) (Akansu and Liu 1991) and discrete wavelet transform (DWT) (Mallat 1989) are the most commonly used signal representation methods. DCT linearly maps the signal from the spatial domain into the frequency domain while keeping the resolution unchanged. DWT uses orthogonal basis functions to decompose the original signal into multi-resolution coefficients with a pyramid structure. Further, nonlinear wavelets (Goutsias and Heijmans 2000; Heijmans and Goutsias 2000) are proposed based on the lifting structure (Sweldens 1998), which brings the advantages of perfect reconstruction and non-redundant representation. However, DCT and DWT directly decompose low-frequency and high-frequency components in the signal from the perspective of energy distribution (Akansu et al. 2001). These relatively low-level coefficients are difficult to directly serve image understanding.

How to obtain high-level semantics-oriented features from raw image pixels remains an open problem. Feature representation based on deep learning is the current main research trend and has achieved outstanding performance in various machine vision tasks. Several popular neural networks such as VGGNet (Simonyan and Zisserman 2014), ResNet (He et al. 2016), DenseNet (Huang et al. 2017) are believed to have outstanding feature extraction capabilities. Based on the concept of information bottleneck (Tishby et al. 2000), Tishby et al. interpret the learning process of the deep neural network (DNN) as constantly forgetting information about the input while obtaining an efficient expression of the label (Tishby and Zaslavsky 2015; Shwartz-Ziv and Tishby 2017). The process of forgetting is irrevertible, which can easily lead to a semantic gap (Kwaśnicka and Jain 2018), i.e. the lack of explicit connection between features and image. Semantic gap brings more difficulties to the joint compression of image and features.

Convolutions and nonlinear activations in DNN are the main reasons for irreversibility. It is a possible direction to combine DNN with traditional transforms. In Lo et al. (2003), Ma et al. (2019, 2020), linear and nonlinear neural networks are introduced into the lifting structure, where the studies are focused on signal fidelity instead of preserving semantic information. He et al. (2019) replace partial modules in the ResNet with the lifting structure, which surpasses ResNet in remote sensing classification task, and show the effectiveness of the lifting structure for feature representation. Another wavelet-based network is designed in Rodriguez et al. (2020), where wavelet coefficients obtained by transform are directly used for classification, but the network does not have the revertible characteristic. i-RevNet is a completely revertible nonlinear convolutional neural network proposed in Jacobsen et al. (2018) on the basis of Gomez et al. (2017). In i-RevNet, the input information is always fully retained, and the features based on multi-level mapping are used for object classification and signal reconstruction. However, i-RevNet has the drawback that the feature extraction process and the classifier always use entire image information, which ignores the compactness of features.

Hierarchical Representation for Semantics-to-Signal Scalability
For human–machine collaborative judgment, an image needs to be provided to both human and machines. We now introduce an application scenario that motivates the following discussions and experiments. In the considered scenario, there are multiple semantic analysis tasks, for example coarse-grained classification and fine-grained classification. Here, coarse-grained classification has less optional classes, and fine-grained classification has more and finer classes. For example, we may want to classify each image as “dog” or “cat,” and if there is a cat, we ask which species the cat belongs to. The dog-cat problem and the dog/cat species problem are coarse-grained and fine-grained, respectively. In addition, image shall be reconstructed for human viewing.

It is intuitive that machine vision needs less, semantics-related information, whereas human vision needs more, signal-reconstructive information. For classification tasks with different granularities, the information required for coarse-grained classification is intuitively a subset of the information required for fine-grained classification. Meanwhile, features required by machine vision tasks are generally compact representations of an image, that is to say, the information required by machine vision tasks is a subset of image information. We now use symbols 𝐹1, 𝐹2, and F to denote features for coarse-grained classification, features for fine-grained classification, and image, respectively, as shown in Fig. 2. In the language of information theory, we know that

𝐻(𝐹1|𝐹2)=0
(1)
𝐻(𝐹2|𝐹)=0
(2)
then we have

𝐼(𝐹2;𝐹)=𝐻(𝐹2)−𝐻(𝐹2|𝐹)=𝐻(𝐹2)
(3)
𝐼(𝐹1;𝐹2)=𝐻(𝐹1)−𝐻(𝐹1|𝐹2)=𝐻(𝐹1)
(4)
and thus

𝐻(𝐹)=𝐻(𝐹|𝐹2)+𝐼(𝐹2;𝐹)=𝐻(𝐹|𝐹2)+𝐻(𝐹2)=𝐻(𝐹|𝐹2)+𝐻(𝐹2|𝐹1)+𝐼(𝐹1;𝐹2)=𝐻(𝐹|𝐹2)+𝐻(𝐹2|𝐹1)+𝐻(𝐹1)
(5)
which is depicted in Fig. 2. In other words, the image information can be decomposed into a series of entropies or conditional entropies of the features or the image. The hierarchical structure of entropy naturally leads to a scalable bitstream, where the base layer is corresponding to 𝐻(𝐹1), and the enhancement layers correspond to 𝐻(𝐹2|𝐹1) and 𝐻(𝐹|𝐹2), respectively. It also implies that the enhancement layers may be compressed by using prediction from the previous layer(s).

Fig. 2
figure 2
Theoretical interpretation of the semantics-to-signal scalability. Left: the entropy of image may be represented in a hierarchy. Right: the hierarchical representations can be naturally compressed into a scalable bitstream

Full size image
The hierarchical structure of entropy inspires us that features may also have a hierarchical characteristic. Motivated by JPEG2000, we design a revertible feature representation method, which distributes the image information into the feature space without any information loss and achieves the compact representations for machine vision tasks by constraining the amount of information contained in the features. By gradually adding features, the semantic information can be continuously augmented, then the hierarchical representations have the characteristic of semantics-to-signal scalability. Note that the existing feature extraction methods like Simonyan and Zisserman (2014), He et al. (2016) only consider whether the feature is useful to express semantic information, but do not consider whether the feature is able to reconstruct image; the existing neural image compression methods like Ballé et al. (2018), Minnen et al. (2018), Chen et al. (2019) only consider whether the coded representation (feature) is useful to reconstruct image, but do not consider whether the feature carries important semantic information. Different from them, we consider signal reconstruction and semantic analysis simultaneously to obtain compact features.

Proposed Learned Revertible Representation
In this section, we propose a Lifting-based Feature Representation Network (LFRNet) to convert an image into multiple features. The conversion is revertible. Using LFRNet, we achieve semantics-to-signal scalability through the hierarchical representation of the image information.

Lifting-Based Feature Representation Network
Overview
The proposed LFRNet is a fully convolutional network based on the lifting structure (Sweldens 1998). The network structure is shown in Fig. 3a. Specifically, input to the network is the image I, or rigorously speaking, the pixels. LFRNet converts I into feature representations with a hierarchical structure. The conversion can be formulated as

{𝐹𝑚𝐾,𝐹𝑑𝐾,𝐹𝑑𝐾−1,…,𝐹𝑑2,𝐹𝑑1}=𝔽→(𝐼|𝛩)
(6)
where 𝔽→ stands for the forward-transform of LFRNet, 𝛩 is the set of trainable parameters of LFRNet, and K is the order of transform. 𝐾=5 in the experiments by default. The features, {𝐹𝑚𝐾,𝐹𝑑𝐾,𝐹𝑑𝐾−1,…,𝐹𝑑2,𝐹𝑑1}, are derived from multiple Revertible Feature Representation Units (RFRUs).

Fig. 3
figure 3
The proposed lifting-based feature representation network (LFRNet) and revertible feature representation unit (RFRU)

Full size image
RFRU is the basic unit of LFRNet as shown in Fig. 3b. It is designed based on the lifting structure and CNN. RFRU includes three basic operations: Split, Predict, and Update. The input of 𝑅𝐹𝑅𝑈𝑘𝐹 (𝑘∈[1,𝐾]), the kth forward-transform unit of LFRNet, is 𝐹𝑚𝑘−1. The Split operation decomposes 𝐹𝑚𝑘−1 into the main branch subband 𝐹̃ 𝑚𝑘 and the dual branch subband 𝐹̃ 𝑑𝑘. In particular, the splitting process is revertible:

(𝐹̃ 𝑚𝑘,𝐹̃ 𝑑𝑘)= Split (𝐹𝑚𝑘−1)
(7)
Then, we use 𝐹̃ 𝑚𝑘 to predict 𝐹̃ 𝑑𝑘, and use the prediction residual 𝐹𝑑𝑘 to update 𝐹𝑚𝑘:

{𝐹𝑑𝑘𝐹𝑚𝑘=𝐹̃ 𝑑𝑘− Predict (𝐹̃ 𝑚𝑘)=𝐹̃ 𝑚𝑘+ Update (𝐹𝑑𝑘)
(8)
Equation (8) is revertible. Its inverse process is:

{𝐹̃ 𝑚𝑘𝐹̃ 𝑑𝑘=𝐹𝑚𝑘− Update (𝐹𝑑𝑘)=𝐹𝑑𝑘+ Predict (𝐹̃ 𝑚𝑘)
(9)
In other words, 𝐹𝑚𝑘−1 can be perfectly reconstructed when 𝐹𝑚𝑘 and 𝐹𝑑𝑘 are known.

The revertibility of RFRU implies that LFRNet is revertible, provided that the parameters used for the forward-transform are also used for the inverse-transform. Here, we do not consider the information loss due to numeric computations. When all the features are known, the input image I can be reconstructed by the inverse operation 𝔽⃖ :

𝐼=𝔽⃖ (𝐹𝑚𝐾,𝐹𝑑𝐾,𝐹𝑑𝐾−1,…,𝐹𝑑2,𝐹𝑑1|𝛩).
(10)
Structure of RFRU
Note that RFRU follows the general lifting structure, which was proposed initially for efficient implementation of the wavelet transform (Sweldens 1998). While the lifting structure remains the same, our RFRU is different from the traditional wavelets, because in RFRU the Predict and Update operations are implemented by trained networks.

Specifically, Predict and Update in (8) use the same network structure, but have different parameters. Besides, different RFRUs in LFRNet do not share parameters. Each Predict/Update network has three parts: redundant representation, feature extraction, and feature shrinkage. The redundant representation is using a convolutional layer with kernel size equal to 3×3 to expand in the channel dimension to 8 times. The feature extraction is using N repetitive units, each of which has a convolution, a batch normalization, and a Rectified Linear Unit (ReLU). N is the order of nonlinearity, and is set to {2,2,3,3,3} for the 𝐾=5 RFRUs. The feature shrinkage is using a convolutional layer with kernel size equal to 3×3 to shrink in the channel dimension back to that of the input.

Structure of Hierarchical Representations
LFRNet achieves the mapping from an image I to a set of features {𝐹𝑚𝐾,𝐹𝑑𝐾,𝐹𝑑𝐾−1,…,𝐹𝑑2,𝐹𝑑1} through the cascade of RFRUs. Next, these features are bound with machine vision tasks. Usually machine vision tasks need a compact set of representations for the sake of computational simplicity. Thus, we may further decompose the features into subsets and choose for example a subset of features for a task. For example, we may decompose at the subband level, or may decompose at the channel level.

In LFRNet, subband decomposition is implanted into the cascade of RFRUs. Specifically, while one RFRU outputs two subbands: main branch and dual branch, the following RFRU deals with the main branch only. Thus, the deeper RFRUs process the less data, with the hope of extracting compact features. It is also worth noting that the degree of nonlinearity also increases with deeper RFRUs.

In addition, we may also select partial channels from a subband as a subset. Because the proposed RFRU reduces spatial resolution but increases number of channels, the main-branch and dual-branch features have more and more channels. Possibly, for a machine vision task, we may need only a subset of a subband. Channel decomposition is a convenient way to achieve this.

In this paper we consider image classification task, which benefits from relatively deeper CNN that has more nonlinearity. Thus, we use the last main-branch feature 𝐹𝑚𝐾 to perform classification. For coarse-grained classification, we perform channel decomposition on 𝐹𝑚𝐾 to obtain a subset. This is illustrated in Fig. 3a, where we decompose 𝐹𝑚𝐾 into 𝐹𝑠1 and 𝐹𝑠2. Here we use a notation slightly different from that in Sect. 3: 𝐹𝑠1 is equivalent to 𝐹1 in Sect. 3, and refers to the features for coarse-grained classification. {𝐹𝑠1,𝐹𝑠2} is equivalent to 𝐹2 in Sect. 3, and refer to the features for fine-grained classification.

We would like to remark that the proposed LFRNet has the following advantages. First, the image information is redistributed in the feature space without any information loss. Second, by using a part/all of the features, compact/complete representations are obtained, respectively. Third, compared with i-RevNet that always performs feature extraction upon entire image information (Jacobsen et al. 2018), the main branches of LFRNet discard information gradually, which makes the feature extraction more interpretable and reduces number of network parameters and computational cost. Fourth, the inverse-transform directly uses the parameters of the forward-transform, avoiding additional modeling and training.

Fig. 4
figure 4
The proposed layered compression network (LCNet). 𝑅𝐴𝐹𝐶𝑙 stands for the resolution adaptive feature compression (RAFC) unit for layer l. The superscripts {𝑒} and {𝑑} stand for encoding and decoding, respectively. “PUnit” is the inter-layer prediction unit. {𝐹𝑠1,𝐹𝑠2,…,𝐹𝑠𝐿} are the features to be compressed. {𝐹𝑠1^,𝐹𝑠2^,…,𝐹𝑠𝐿^} are the reconstructed features after compression. {𝐵1,𝐵2,…,𝐵𝐿} form a scalable bitstream

Full size image
Task-Oriented Optimization
The proposed LFRNet can be optimized for specific machine vision tasks. Note that LFRNet produces a series of features, which can be input to different networks to fulfill various machine vision tasks. LFRNet and the task-specific networks may be jointly optimized to ensure the usability of the features. Here, we first give a general formulation, and then present the specific formulation for the scenario of coarse-to-fine image classification.

In general, we may have T tasks, for each task  Task𝑡 (𝑡=1,…,𝑇), we assign a subset of features 𝐹𝑡⊆{𝐹𝑚𝐾,𝐹𝑑𝐾,𝐹𝑑𝐾−1,…,𝐹𝑑2,𝐹𝑑1} to perform the task. Let 𝛩 be the set of trainable parameters of LFRNet, and 𝛩𝑡 be the set of trainable parameters of the task-specific network, the optimization problem can be defined as

min𝛩,𝛩1,…,𝛩𝑇∑𝑡=1𝑇𝜆𝑡𝑡(𝛩,𝛩𝑡)
(11)
where 𝜆𝑡 is the weight of the tth task and 𝑡 measures the task-specific loss.

Specifically, for the considered coarse-to-fine image classification, there are two tasks: coarse-grained classification and fine-grained classification. There are respectively two task-specific networks dedicated to classification. For example, we use three fully-connected layers to build a classification network. The two classification networks are denoted by ℕ𝑐 and ℕ𝑓, and their parameters are 𝛩𝑐 and 𝛩𝑓, respectively. In addition, we have mentioned that we use 𝐹𝑠1 for coarse-grained classification and we use {𝐹𝑠1,𝐹𝑠2} for fine-grained classification. Therefore, the specific optimization problem becomes:

min𝛩,𝛩𝑐,𝛩𝑓𝜆𝑐𝐻(ℕ𝑐(𝐹𝑠1|𝛩𝑐),𝐿𝑐)+𝜆𝑓𝐻(ℕ𝑓({𝐹𝑠1,𝐹𝑠2}|𝛩𝑓),𝐿𝑓)
(12)
where 𝐻(⋅,⋅) is the cross-entropy loss function, 𝐿𝑐 and 𝐿𝑓 are the ground-truth labels for coarse-grained and fine-grained classification, respectively. 𝜆𝑐=𝜆𝑓=1 in the experiments because we assign equal importance to the two tasks.

Proposed Layered Compression Network
In this section, we propose a layered compression network to compress the multi-layer features into a scalable bitstream. The compression network is optimized end-to-end to achieve the joint optimization of compression ratio, signal distortion, and semantic analysis accuracy.

Problem Formulation
Based on the hierarchical representations generated by LFRNet, we propose a Layered Compression Network (LCNet) to compress all the features. To use a unified notation, hereafter we use 𝐹𝑠𝑙 (𝑙=1,…,𝐿) to replace the previous symbols {𝐹𝑚𝐾,𝐹𝑑𝐾,𝐹𝑑𝐾−1,…,𝐹𝑑2,𝐹𝑑1}, as shown in Fig. 3. Note that we have split 𝐹𝑚𝐾 into 𝐹𝑠1 and 𝐹𝑠2, so 𝐿=𝐾+2=7 in the experiments. When optimizing LCNet, we assume the parameters of LFRNet had been trained and keep unchanged. So the optimization problem can be defined as

min𝛺 Distortion +𝜆× Rate =𝜆𝑐𝐻(ℕ𝑐(𝐹̂ 𝑠1|𝛺),𝐿𝑐)+𝜆𝑓𝐻(ℕ𝑓({𝐹̂ 𝑠1|𝛺,𝐹̂ 𝑠2|𝛺}),𝐿𝑓)+𝜆𝐷(𝔽⃖ (𝐹̂ 𝑠1,…,𝐹̂ 𝑠𝐿|𝛺),𝐼)+𝜆∑𝐿𝑙=1𝑅𝑙(𝛺)
(13)
where 𝜆 is the Lagrangian multiplier for rate-distortion tradeoff, 𝐹̂ 𝑠𝑙 is the lossily compressed and reconstructed version of 𝐹𝑠𝑙 and is dependent on the parameters 𝛺, (⋅,⋅) measures the signal distortion and 𝜆𝐷 is its weight, 𝑅𝑙 is the rate of 𝐹𝑠𝑙 and is dependent on the parameters 𝛺. Note that 𝛩,𝛩𝑐,𝛩𝑓 are omitted in the optimization problem because they all keep unchanged.

The joint optimization of bitrate, signal distortion, and semantic analysis accuracy has twofold benefits. First, it effectively retains the semantic information required by machine vision tasks, thereby ensuring the accuracy of semantics. Second, it can tradeoff between bitrate and signal distortion by more or less compressing features that are irrelevant to machine vision tasks.

Layered Compression Network
Under the guidance of the optimization problem defined in (13), we construct the layered compression network as shown in Fig. 4. The entire LCNet consists of three parts: Encoder, Decoder, and Post-Processing Unit. During the end-to-end optimization, LFRNet is involved but the parameters of LFRNet are fixed.

The Encoder in LCNet has multiple resolution-adaptive feature compression (RAFC) units as well as multiple inter-feature prediction units (PUnits). Each RAFC (encoding) unit deals with one feature 𝐹𝑠𝑙, either compressing the feature (when 𝑙=1) or compressing the residual of the feature (when 𝑙≥2) into a part of bitstream 𝐵𝑙. Each PUnit predicts the feature 𝐹𝑠𝑙 (when 𝑙≥2) from the previously reconstructed features 𝐹̂ 𝑠1,…,𝐹̂ 𝑠𝑙−1; the prediction is denoted by 𝐹𝑝𝑙 and the corresponding residual is denoted by 𝐹𝑟𝑙=𝐹𝑠𝑙−𝐹𝑝𝑙. The compressed and reconstructed residual is denoted by 𝐹̂ 𝑟𝑙. Then, the reconstructed feature is 𝐹̂ 𝑠𝑙=𝐹𝑝𝑙+𝐹̂ 𝑟𝑙. PUnit effectively reduces the inter-feature redundancy.

The Decoder in LCNet also has multiple RAFC units and multiple PUnits. Each RAFC (decoding) unit decodes the partial bitstream 𝐵𝑙, reconstructs the feature (when 𝑙=1) or the residual (when 𝑙≥2). The residual is added with the prediction to obtain the reconstructed feature when 𝑙≥2. Note that the Decoder can decode partial bitstream, which is a nature of scalable coding. For example, if one wants to perform coarse-grained classification, it is sufficient to decode 𝐵1 into 𝐹̂ 𝑠1, and then use the task-specific network. The entire bitstream is decoded only when we want to reconstruct the image.

Since the features are lossily compressed, the compression artifacts may deteriorate the quality of the reconstructed image, especially at low bit rates. Thus, we use a post-processing unit to repair the reconstructed image for human vision.

Basic Modules
Resolution-Adaptive Feature Compression Unit
Our resolution-adaptive feature compression (RAFC) unit is a simplified and adapted version of the network of Non-Local Attention optimization and Improved Context modeling-based image compression (NLAIC) (Chen et al. 2019). NLAIC is based on Ballé’s pioneering work about hyper-prior model (Ballé et al. 2018). Compared to Ballé’s work, NLAIC introduces the non-local attention optimization and the improved context model: the attention mechanism together with the nonlocal operations are used to process multi-layer features adaptively; both hyper-prior model and neighboring reconstructed features are used to improve the efficiency of context modeling. Compared to NLAIC, our RAFC has simplified the network structure and adapted some hyper-parameters for different features. Figure 5 shows the core modules of RAFC.

Fig. 5
figure 5
Left: the proposed resolution adaptive feature compression (RAFC) unit. “Conv5x5 s2” indicates a convolutional layer using a kernel of size 5×5 and a stride of 2. “𝑙𝑜𝑜𝑝𝑚𝑙” and “𝑙𝑜𝑜𝑝ℎ𝑙” respectively represent the number of stacking the specified module. Right: the proposed prediction unit (PUnit). Each ResBlock contains two convolutional layers and a rectified linear unit (ReLU) inside the two, as well as a residual connection, i.e. 𝑓(𝑥)= Conv2( ReLU ( Conv1(𝑥)))+𝑥

Full size image
The compression unit for the lth feature is denoted by 𝑅𝐴𝐹𝐶𝑙. 𝑅𝐴𝐹𝐶{𝑒}𝑙 and 𝑅𝐴𝐹𝐶{𝑑}𝑙 represent its encoding part and decoding part, respectively. 𝑅𝐴𝐹𝐶{𝑒}𝑙 is a combination of {𝔼𝑚,𝔼ℎ,𝔻ℎ,𝑄,𝐴𝐸,𝐶𝑀}. 𝑅𝐴𝐹𝐶{𝑑}𝑙 is a combination of {𝔻𝑚,𝔻ℎ,𝐴𝐷,𝐶𝑀}. In the encoder, we first use the main transformation 𝔼𝑚 to obtain representations 𝑋𝑙, then we use the secondary transformation 𝔼ℎ to obtain representations 𝑍𝑙. 𝑍𝑙 passes quantization (Q) and arithmetic-encoder (AE) to become a bitstream 𝐵ℎ𝑙. 𝐵ℎ𝑙 is decoded by arithmetic-decoder (AD) to obtain 𝑍̂ 𝑙, which is then sent to the secondary inverse-transformation 𝔻ℎ. The output of 𝔻ℎ, together with the neighboring reconstructed features that pass a masked convolution (MaskConv), is used by the context modeling (CM), which provides probability models for the AE to encode the quantized version of 𝑋𝑙 into another bitstream 𝐵𝑚𝑙. So far, we obtain the bitstream 𝐵𝑙 composed by 𝐵𝑚𝑙 and 𝐵ℎ𝑙. Note that the rate of 𝐵𝑙, in other words, 𝑅𝑙, can be estimated according to these probability models as calculated in Chen et al. (2019). Finally, we can decode 𝐵𝑚𝑙 to get 𝑋̂ 𝑙, and use the main inverse-transformation 𝔻𝑚 to obtain the reconstruction.

Compared to Chen et al. (2019), our RAFC uses a simplified non-local attention module (sNLAM). Specifically, we remove one residual-connection block (ResBlock) and reduce the number of channels from 192 to 128. This is observed efficient for computation and do not incur much compression performance loss.

In addition, all the features use RAFC units but the features have different resolutions. Accordingly, the RAFC unit for each feature slightly differs from one another, notably in the down-sampling scales of 𝔼𝑚 and 𝔼ℎ. The scales are controlled by 𝑙𝑜𝑜𝑝𝑚𝑙 and 𝑙𝑜𝑜𝑝ℎ𝑙. Denote the resolution of 𝐹𝑠𝑙 by (C, S, S) , where C is number of channels and 𝑆=2𝑗 is width/height. We set 𝑙𝑜𝑜𝑝𝑚𝑙 to 𝑓𝑙𝑜𝑜𝑟(log2(𝑆)−12), and set 𝑙𝑜𝑜𝑝ℎ𝑙 to 2. In addition, the width/height and number of channels of 𝑋𝑙 and 𝑍𝑙 are set to

𝑆𝑚𝐶𝑚𝑆ℎ𝐶ℎ=𝑆⋅2−𝑓𝑙𝑜𝑜𝑟(log2(𝑆)−12)=𝐶⋅𝑆24⋅(𝑆𝑚)2=𝑆𝑚4=𝐶𝑚1.5
(14)
𝑆𝑚 and 𝐶𝑚 are for 𝑋𝑙, and 𝑆ℎ and 𝐶ℎ are for 𝑍𝑙, respectively. Therefore, the size of 𝑋𝑙 is 1/4 of the size of the input feature, and the size of 𝑍𝑙 is 1/24 of the size of 𝑋𝑙.

Fig. 6
figure 6
Exemplar images of the used datasets. ILSVRC (often known as ImageNet) is used for network pre-training. Both CUB200-2011 and FGVC-Aircraft are used for coarse-to-fine image classification

Full size image
Inter-Feature Prediction Unit
The inter-feature prediction has two modes depending on how the features are decomposed. 𝐹𝑠1 and 𝐹𝑠2 are different channels of the same subband. The following features belong to different subbands. As shown in Fig. 5, PUnit first distinguishes the two cases. If it is channel decomposition, the feature to be predicted and the feature used to predict have the same spatial resolution, so a fully convolutional network is directly used. If it is subband decomposition, the feature used to predict shall be processed by an RFRU so that the spatial resolution is aligned, and then passes a fully convolutional network.

Each PUnit has a lightweight six-layer CNN. Its first convolutional layer uses kernel size 5×5 and 128 channels for redundant representation. The following four layers are organized into two ResBlocks. Each ResBlock has two convolutional layers with kernel size equal to 3×3 and a ReLU between the two, and a residual connection. The last convolutional layer uses kernel size 1×1 and 1 channel to output.

Post-Processing Unit
A post-processing unit is added to filter the entire reconstructed image, so as to reduce compression artifacts and enhance signal reconstruction quality. In particular, the post-processing unit uses the same network structure as the PUnit.

Experiments
Tasks and Datasets
In this paper, we consider coarse-to-fine image classification as the targeted machine vision tasks, where one image can be classified into a coarse category (the number of optional categories is small), or a fine category (the number of optional categories is large). Note that our scheme is to provide a scalable bitstream, which can be partially decoded to obtain features that then serve the classification. For the classification tasks, one may imagine a compression scheme that performs classification at the encoder side and transmits only the classification results to the decoder side. This scheme is feasible, but may have severe limitations: the transmitted data may be useless if the decoder side wants to perform another classification task with a different category set; the encoder side may not have sufficient computational resource to perform the classification. In view of these limitations, we do not experimentally compare with the imaginary scheme of “transmitting classification results.”

There are several datasets that support our designed study. For example, CUB200-2011 (Wah et al. 2011) is a bird image dataset consisting of 11,788 images, where 5994 images are for training and 5794 are for test. All the images are divided into 200 categories. According to ornithology systematics, the 200 categories can be merged into 122 coarse categories or 37 coarser categories. For CUB200-2011, we use coarse-grained to refer to 37-category classification, fine-grained to refer to 200-category classification, and intermediate-grained to refer to 122-category classification. For another example, FGVC-Aircraft (Maji et al. 2013) is an aircraft image dataset consisting of 10,000 images, where 6667 images are for training and 3333 are for test. The images can be divided by manufacturer, family, variant, into 30, 70, 100 categories, respectively. For FGVC-Aircraft, we use coarse-grained, intermediate-grained, and fine-grained to refer to the 30-category, 70-category, and 100-category classification, respectively. All these category labels are available in the dataset, so the classification accuracy can be directly calculated. Note that our proposed LFRNet and LCNet are optimized only for coarse-grained and fine-grained classification, but we will test their generalization ability for intermediate-grained classification.

Since the content of the above two datasets is relatively homogeneous, we use the ILSVRC dataset (Russakovsky et al. 2015) (often known as ImageNet) for pre-training of LFRNet. ILSVRC contains 1000 categories, and each category has about 1000 images.

All the images in the used datasets have the resolution of 256×256, or have been resized to this resolution, to ensure a fair comparison with other methods. Fig. 6 presents some images of the used datasets.

We evaluate the proposed method and compare with the others in both semantic analysis accuracy and compression efficiency. Top-1 accuracy and top-5 accuracy are used to evaluate the classification results. Compression efficiency is evaluated from three indicators: compression ratio (or bitrate in bit-per-pixel, bpp), peak signal-to-noise ratio (PSNR), and multi-scale structural similarity (MS-SSIM).

Experimental Settings
Our implementation is based on PyTorch (Paszke et al. 2017). All the training and test are conducted on a cluster of GTX1080Ti graphics processing units (GPUs). Four GPUs are used for fast training.

In the training stage, we use the stochastic gradient descent algorithm for gradient back-propagation and parameter optimization. The weights in (13) are set to 𝜆𝑐=𝜆𝑓=1, 𝜆𝐷=0.5, and 𝜆∈{0.0067,0.04,0.2,2}, respectively. Four values are used for 𝜆 to achieve different bit rates. Compared with several neural image compression networks such as Ballé et al. (2018), Chen et al. (2019) that use patches for training, our networks are trained with complete images due to the considered machine vision tasks–image classification. Note that our LFRNet and LCNet are separately trained: we train LFRNet first, then fix the parameters of LFRNet and train LCNet. Our LFRNet is pre-trained on ILSVRC and then further trained on either CUB200-2011 or FGVC-Aircraft. LCNet is not pre-trained. Table 1 summarizes the hyper-parameters for network training.

Table 1 Training hyper-parameters
Full size table
Performance of Semantics-to-Signal Scalable Compression
Four compression models are trained by setting different values for 𝜆. These models are referring to the same LFRNet model. Table 2 shows the average bit rate of compressed images with different models. On CUB200-2011, the maximal average compression ratio is 471 (0.051 bpp) and the minimal one is 34 (0.711 bpp). On FGVC-Aircraft, the maximal one is 889 (0.027 bpp) and the minimal one is 51 (0.474 bpp).

Table 2 Classification accuracy results
Full size table
Semantics-to-Signal Scalability
First, we examine the scalable coding functionality of the proposed method. Figure 7 shows the partial decoding results on CUB200-2011 using the model with 𝜆=0.0067. It also displays some reconstructed images of partial decoding, taking one picture in the CUB200-2011 test set as an example. Clearly, with more bits decoded, the classification accuracy and PSNR both increase, and the visual quality of the reconstructed images becomes better. Also, after a certain rate (e.g. 0.011 bpp for coarse-grained), the classification accuracy becomes stable. This rate is called “critical rate” for the machine vision task. Obviously, the critical rate for fine-grained classification (∼ 0.02 bpp) is higher than that for coarse-grained, but it is still far less than the rate needed for image reconstruction. As shown in Fig. 7, to reconstruct visually pleasing image, the bitrate shall be higher than 0.2 bpp. In this example, the bits required for the classification tasks occupy less than 10% of the entire bitstream that provides visually acceptable image reconstruction.

Fig. 7
figure 7
Quantitative evaluation of the semantics-to-signal scalability (𝜆=0.0067). The horizontal axis represents bitrate (bit-per-pixel, bpp), and the left and right vertical axes represent top-1 classification accuracy and reconstruction quality in PSNR, respectively. The classification accuracy is not displayed in the high bitrate range because it becomes stable after the indicated (by the green arrow) point. The reconstruction quality is not displayed in the (extremely) low bitrate range because it is too low to be useful (Color figure online)

Full size image
In Fig. 8, we randomly select two images from the CUB200-2011 test set and the FGVC-Aircraft test set, respectively, and display some reconstructed images of partial decoding. When the decoded bit rate is very low, the reconstructed images are hard to recognize, but human may still be able to identify the shape of the object. With more bits decoded, colors, edges, and textures become more and more clear. When all the bits are decoded, reconstructed images appear similar to the original images.

Fig. 8
figure 8
Reconstructed images of our method. Numbers shown below each image indicate bitrate and PSNR. The top two rows correspond to 𝜆=0.04 and the bottom two rows correspond to 𝜆=0.2, respectively

Full size image
Compression Performance
We choose three image compression methods for comparison. The first is JPEG2000, which is the widely used standard for scalable image compression. The second is BPG, which represents state-of-the-art of non-learned compression. We use the default configuration of BPG, that is to compress with YUV420 format. The third is NLAIC, a CNN-based end-to-end learned image compression method proposed in Chen et al. (2019). We choose NLAIC because our compression network also borrows some ideas from it. For JPEG2000 and BPG, we have adjusted the quantization parameter to achieve similar compression ratios as our method. For NLAIC, we use pre-trained models, so the bit rates are not well aligned to those of the other methods.

First, we show the classification results. Here for CUB200-2011 and FGVC-Aircraft, we respectively train a VGG16 model with uncompressed images, and use the same model on reconstructed images with different methods (JPEG2000, ours, BPG, NLAIC) and different bit rates. These results are summarized in Table 2. In addition, we also report the classification results of our method not using reconstructed images but using features that are decoded from partial bitstream. It can be observed that with the decrease of bit rate, the classification accuracy of reconstructed images usually drops significantly. However, our results using features are quite stable across different bit rates. For example on CUB200-2011, for coarse-grained classification, when bitrate is around 0.051 bpp, the top-1 accuracy of JPEG2000 reconstructed images is 26.6%, but our result using features is 87.6%. Note that the classifier for features is a three-layer fully-connected network and it is not stronger than VGG16. Therefore, the results show that compressing and transmitting features for machine vision tasks is a good choice at low bit rates.

Second, we compare the results of different methods on PSNR, MS-SSIM, and bitrate. Figure 9 shows the rate-PSNR/MS-SSIM curves of our method and JPEG2000, BPG, and NLAIC. Our proposed method significantly surpasses JPEG2000 in both PSNR and MS-SSIM. In addition, our method achieves comparable MS-SSIM than BPG and NLAIC at high bit rates, but does not catch up with BPG and NLAIC in PSNR. We believe the result is mainly due to the less compression efficiency of our entropy coding method. JPEG2000 compresses all the subbands simultaneously, and it has dedicated, highly efficient coding tools like zero-tree (Taubman 2000). BPG has an advanced context-adaptive binary arithmetic coding (CABAC) engine for entropy coding (Marpe et al. 2003). NLAIC compresses all the features and optimizes the uniform entropy coder in an end-to-end fashion. In our scheme, the features are compressed one by one using prediction from each to the next. The correlation among non-adjacent features is not fully exploited. As a result, our method performs less well especially in PSNR at high bit rates where entropy coding has big impact. Nonetheless, our method performs well in MS-SSIM at high bit rates; this is probably due to the effectiveness of multi-scale decomposition of our method.

Fig. 9
figure 9
Rate-distortion curves of the proposed method compared to JPEG2000, BPG, and NLAIC (Chen et al. 2019). a, b Correspond to the CUB200-2011 dataset. c, d Correspond to the FGVC-Aircraft dataset

Full size image
Complexity Analysis
We report the average running time of each module in Table 3. The time was measured on a GTX1080Ti GPU. Note that our scheme enables partial decoding, so we report the time needed for coarse-grained classification, fine-grained classification, and image reconstruction, respectively. It can be observed that the slowest module in our scheme is the decoding module, which is attributed to the autoregressive context model (Chen et al. 2019). In the future, we may simplify the context model to accelerate the decoding process. It is also noticeable that, excluding the decoding, the analysis modules at the decoder side are computationally efficient, reducing more than 90% computational time than the image-based classification.

Table 3 Average running time of each module for one image
Full size table
Performance of LFRNet
LFRNet converts an image into hierarchical feature representations in a revertible manner. Partial features may serve as a compact representation of the information needed for a machine vision task. The image can be perfectly reconstructed based on all features. In this section, we ask: how many features are sufficient for a given task? We provide empirical results to address this question. In addition, we compare with other networks, notably VGG16 (Simonyan and Zisserman 2014) and i-RevNet (Jacobsen et al. 2018).

Performance of Pre-trained Models
We have pre-trained LFRNet on the ILSVRC training set. For fair comparison, we similarly pre-train VGG16 and i-RevNet on the same training set. Then we test the three models on the ILSVRC validation set. The tested classification accuracy, as well as number of parameters, is shown in Table 4. The three models achieve comparable top-1 and top-5 accuracy, but our LFRNet has greatly reduced the number of parameters. Note that our LFRNet gradually discards information in the network, which is quite different from VGG16 and i-RevNet. It also confirms that the information required for classification may be compactly represented by a small set of features.

Table 4 Classification accuracy on ILSVRC dataset
Full size table
Performance in Coarse-to-Fine Classification
Based on the pre-trained LFRNet model and a given dataset/task (e.g. CUB200-2011, coarse-grained classification), we want to identify how many features are sufficient for the task. Here we perform a grid search using k channels of 𝐹𝑚𝐾, where 𝑘∈{8,16,24,…,96}. For each k we fine-tune LFRNet with the given dataset/task, and draw the curves of top-1 and top-5 accuracy with respect to k in Fig. 10. It can be observed that the accuracy becomes stable when k is larger than a “critical number.” According to these results, we use 24 channels for coarse-grained classification and 96 channels for fine-grained classification, respectively. In other words, 𝐹𝑠1 has 24 channels, and 𝐹𝑠2 has 72 channels.

Fig. 10
figure 10
Relation between classification accuracy and number of channels of 𝐹𝑚𝐾. In each curve, there is a solid marker showing that the top-1 accuracy becomes stable after that point (also indicated by a green arrow) (Color figure online)

Full size image
Based on the above settings, we now fine-tune LFRNet for two classification tasks simultaneously to optimize (12). For comparison, we also fine-tune VGG16 and i-RevNet, but for coarse-grained and fine-grained classification individually. Table 5 presents the classification results of different fine-tuned models on the corresponding test set. It is observed that our method achieves comparable classification accuracy, but our method uses much less features, again demonstrating the advantage of compact representation.

Table 5 Classification accuracy results of uncompressed features
Full size table
Performance in Intermediate-Grained Classification
We go one step further to ask: may LFRNet perform well on non-trained machine vision tasks? To answer this question, we introduce a third classification task, namely the intermediate-grained as mentioned before. Note that intermediate-grained classification is not considered during the LFRNet training.

Again we need to identify how many features are sufficient for this task. We now fix all the parameters of LFRNet, set different values of k, and train the classifier (also a three-layer fully-connected network) for the intermediate-grained classification task. By grid search, we found 𝑘=72 is appropriate, as shown in Fig. 10.

For comparison, we also fine-tune VGG16 and i-RevNet for the intermediate-grained classification. The results are given in Table 6. It is observed that LFRNet still achieves very competitive results, especially taken into account that VGG16 and i-RevNet are specifically trained for the task. These results demonstrate that LFRNet has extracted compact features that work well for trained tasks and meanwhile are generalizable to similar tasks.

Table 6 Classification accuracy results of uncompressed features for the intermediate-grained classification task
Full size table
Conclusion
In this paper, we have presented a semantics-to-signal scalable image compression framework with learned revertible representations. We have proposed LFRNet to learn effective and efficient features oriented to machine vision tasks. We have also proposed LCNet to compress the features into a scalable bitstream, so as to achieve a joint optimization of compression ratio, signal reconstruction quality, and semantic analysis accuracy. As a concrete example of human–machine collaborative judgment, we study coarse-to-fine image classification and image reconstruction as the targets for image compression. Our experimental results have verified the effectiveness of the proposed method, which outperforms JPEG2000 significantly.

In the future, our work can be extended in several directions. First, we may further investigate revertible networks to enhance the feature learning capabilities. Second, we may design advanced methods for more efficient compression of the features. Third, we may consider video coding in a similar way but need to address motion carefully.