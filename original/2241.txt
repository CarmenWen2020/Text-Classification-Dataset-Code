Domain adaptation aims to learn a transferable model to bridge the domain shift between one labeled source domain and another sparsely labeled or unlabeled target domain. Since the labeled data may be collected from multiple sources, multi-source domain adaptation (MDA) has attracted increasing attention. Recent MDA methods do not consider the pixel-level alignment between sources and target or the misalignment across different sources. In this paper, we propose a novel MDA framework to address these challenges. Specifically, we design a novel Multi-source Adversarial Domain Aggregation Network (MADAN). First, an adapted domain is generated for each source with dynamic semantic consistency while aligning towards the target at the pixel-level cycle-consistently. Second, sub-domain aggregation discriminator and cross-domain cycle discriminator are proposed to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and the target domain while training the task network. For the segmentation adaptation, we further enforce category-level alignment and incorporate multi-scale image generation, which constitutes MADAN+. We conduct extensive MDA experiments on digit recognition, object classification, and simulation-to-real semantic segmentation tasks. The results demonstrate that the proposed MADAN and MADAN+ models outperform state-of-the-art approaches by a large margin.

Access provided by University of Auckland Library

Introduction
Together with increased computation capacity and deep complex models, large-scale labeled data attributes to the significant success of deep learning algorithms as one key element. Consequently, promising performance has been obtained via deep neural networks in various computer vision tasks, such as image classification (Krizhevsky et al. 2012; Simonyan and Zisserman 2014; He et al. 2016; Huang et al. 2017), object detection (Girshick 2015; Ren et al. 2015; Redmon et al. 2016), and semantic segmentation (Long et al. 2015a; Badrinarayanan et al. 2017; Chen et al. 2017a). However, in many real-world applications, there are only limited or even no labeled training data, as labeling is expensive, time-consuming, and even difficult. For example, only the labels provided by experts are reliable in fine-grained recognition (Gebru et al. 2017); labeling each Cityscapes image takes about 90 minutes in semantic segmentation (Cordts et al. 2016); point-wise 3D LiDAR point clouds are difficult to label in autonomous driving (Wu et al. 2019; Yue et al. 2018). One direct way is to transfer the learned knowledge from one labeled source domain to another different but related target domain. However, because of the presence of domain shift or dataset bias (Torralba and Efros 2011), i.e. the joint probability distributions of observed data and labels are different in the two domains, direct transfer may not perform well, as shown in Fig. 1. This observation motivates the research on domain adaptation (DA) (Bousmalis et al. 2016; Tzeng et al. 2017; Zhao et al. 2020c).

Fig. 1
figure 1
An example of domain shift. Labeled source 1: GTA, Labeled source 2: SYNTHIA, Unlabeed target: Cityscapes. a Single-source domain adaptation (DA). The overall mIoU result of the FCN semantic segmentation mode (Long et al. 2015a) drops from 62.6% (trained on the target Cityscapes, unavailable in unsupervised DA and simply used for comparison here) to 21.7% and 18.5% (trained only on the source GTA and SYNTHIA). b Multi-source domain adaptation. Although CyCADA (Hoffman et al. 2018b), one state-of-the-art single-source DA method, improves the mIoU results to 38.7% and 29.2%, simply combining multiple sources and performing single-source DA (37.3%) does not outperform the best single-source DA (38.7%). We propose Multi-source Adversarial Domain Aggregation Network (MADAN), a novel adversarial model, to perform multi-source DA. Our method achieves significant performance improvements (42.8%) over source-combined DA and single-source DA

Full size image
Without requiring any labeled data from the target domain, unsupervised domain adaptation (UDA) is the most widely studied pipeline. Both theoretical analysis (Ben-David et al. 2010; Gopalan et al. 2014; Louizos et al. 2015; Tzeng et al. 2017) and algorithm design (Pan and Yang 2010; Glorot et al. 2011; Jhuo et al. 2012; Becker et al. 2013; Ghifary et al. 2015; Long et al. 2015b; Hoffman et al. 2018b; Zhao et al. 2019b) for UDA have been proposed recently. Conventional UDA methods mainly focus on the single-source scenario based on the assumption that the labeled source data is sampled from the same distribution. However, in practice, the labeled data may be collected from multiple sources with different distributions (Sun et al. 2015; Zhao et al. 2020a). Simply combining different sources into one source and directly employing single-source UDA may lead to suboptimal solutions, since the data from different sources may interfere with each other during the learning process (Riemer et al. 2019), as shown in Fig. 1b. In this case, the DA method trained on the combined sources with more labeled training samples cannot guarantee to perform better than the best model trained on one source. Therefore, effective multi-source domain adaptation (MDA) algorithms are required (Sun et al. 2015; Zhao et al. 2020a).

Early efforts on MDA mainly used shallow models (Sun et al. 2015), either learning a latent feature space for different domains (Duan et al. 2009; Sun et al. 2011; Duan et al. 2012a; Chattopadhyay et al. 2012; Duan et al. 2012b) or combining pre-learned source classifiers (Yang et al. 2007; Schweikert et al. 2009; Xu and Sun 2012; Sun and Shi 2013). Recently, some deep MDA methods that only focus on image classification have been proposed by learning a common feature space and aligning each source and target pair (Xu et al. 2018; Zhao et al. 2018; Peng et al. 2019; Zhao et al. 2020b). There are some limitations of these methods. (1) They mainly focus on global feature-level alignment, which only aligns high-level information globally. This might be sufficient for coarse-grained classification tasks, but it is obviously insufficient for fine-grained semantic segmentation, which performs pixel-wise prediction. On the one hand, these feature-level alignment methods do not consider pixel-level information, which is proved to be important for pixel-wise prediction tasks (Hoffman et al. 2018b). One may argue we can add a generator and a discriminator to conduct pixel-level alignment, such as CyCADA (Hoffman et al. 2018b). However, existing pixel-level alignment methods only work in the single-source scenario with one crop scale, which cannot well preserve the global semantics or the local details. On the other hand, different categories in segmentation tasks (e.g. car and sky) are not uniformly distributed across domains, which results in class-wise domain shift (Chen et al. 2017b). (2) They only align each source and target pair. Although different sources are matched towards the target, there may exist significant misalignment across different sources. (3) They only focus on image classification where one label is assigned to each image. Directly extending them from classification to segmentation, which assigns a semantic label (e.g. car, cyclist, pedestrian, road) to each pixel in an image, may not perform well. This is because segmentation is a structured prediction task, i.e. it has to resolve the predictions in an exponentially large label space and thus the decision function is more involved than classification (Zhang et al. 2017; Tsai et al. 2018). (4) Further, they have low interpretability, which cannot well explain why these methods work.

To address the above challenges, in this paper we propose a novel MDA framework, termed Multi-source Adversarial Domain Aggregation Network (MADAN), which consists of Dynamic Adversarial Image Generation, Adversarial Domain Aggregation, and Feature-aligned task learning. First, for each source, we generate an adapted domain using a Generative Adversarial Network (GAN) (Goodfellow et al. 2014) with cycle-consistency constraint (Zhu et al. 2017), which enforces pixel-level alignment between source images and target images. To preserve the semantics before and after image translation, we propose a novel semantic consistency loss by minimizing the Kullback-Leibler (KL) divergence between the source predictions of a pretrained task model (e.g. classification and segmentation) and the adapted predictions of a dynamic task model. Second, instead of training a classifier for each source domain (Xu et al. 2018; Peng et al. 2019; Zhao et al. 2020b), we propose sub-domain aggregation discriminator to directly make different adapted domains indistinguishable, and cross-domain cycle discriminator to discriminate between the images from each source and the images transferred from other sources. In this way, different adapted domains can be better aggregated into a more unified domain. Finally, the task model is trained on the aggregated domain, while enforcing feature-level alignment between the aggregated domain and the target domain. For segmentation adaptation, we enhance MADAN to MADAN+ with two improvements: category-level alignment to ensure class-wise domain alignment, and multi-scale image generation to enable adapted images to better preserve both global semantics and local details. Further, in the experiment, we visualize the results of both feature-level alignment and pixel-level alignment to show the interpretability on why the proposed method works.

In summary, our contributions are three-fold:

We design a novel framework termed MADAN to do multi-source domain adaptation. (i) Sub-domain aggregation discriminator and cross-domain cycle discriminator are proposed to better align different adapted domains. (ii) Besides feature-level alignment, pixel-level alignment is further considered by generating an adapted domain for each source cycle-consistently with a novel dynamic semantic consistency loss.

We propose to perform domain adaptation for semantic segmentation from multiple sources. To our best knowledge, this is the first work on multi-source structured domain adaptation. For segmentation, MADAN is enhanced to MADAN+ with category-level alignment and multi-scale image generation.

We conduct extensive experiments on several MDA bechmark datasets for digit recognition, object classification, and simulation-to-real semantic segmentation, and the results demonstrate the effectiveness of the proposed MADAN and MADAN+ models. We also demonstrate the modelsâ€™ interpretability from different aspects, such as feature transferability, style translation, and attention visualization.

One preliminary version on MADAN was previously introduced in our NeurIPS paper (Zhao et al. 2019a). As compared to the conference version, this journal paper has the following five aspects of enhancements. First, we perform a more comprehensive review to compare the proposed method with existing methods. Second, we elaborate the motivations and insights in more details on the specific designs of the proposed method. Third, we conduct MDA experiments on digit recognition and object classification, which also achieve state-of-the-art performances. Fourth, we extend the original MADAN to MADAN+ with category-level alignment and multi-scale image generation for semantic segmentation, conduct more comparative experiments, and achieve better performances. Finally, we enhance the modelsâ€™ interpretability to better understand the superiority of the proposed method.

The rest of this paper is organized as follows. Section 2 reviews related work on single-source UDA and MDA. Section 3 gives the definition of the MDA problem. Section 4 describes the proposed MADAN and extended MADAN+ models in detail. Experimental settings, results, and analysis are presented in Sect. 5. We conclude this paper in Sect. 6.

Related Work
In this section, we introduce related work on single-source unsupervised domain adaptation (UDA) and multi-source domain adaptation, and compare the proposed MADAN with these methods.

Single-source UDA
While the early single-source UDA (SUDA) methods are mainly non-deep ones (Patel et al. 2015), either re-weighting samples or transforming intermediate subspaces, the emphasis of recent SUDA methods has shifted to deep learning architectures in an end-to-end fashion. Typically, a conjoined architecture with two streams is employed in deep SUDA (Zhuo et al. 2017). One stream is used to represent the task model for the source domain, and the other is used to align the target and source domains. Correspondingly, a traditional task loss based on the labeled source data and another alignment loss to tackle the domain shift problem are jointly optimized during the training of deep SUDA. Typically, the task loss is the same among different methods, while the difference is focused on the alignment loss (Zhao et al. 2020c), such as discrepancy loss, adversarial loss, self-supervision loss, etc.

Discrepancy-based methods explicitly measure the discrepancy between the target domain and the source domain, such as the multiple kernel variant of maximum mean discrepancies (Long et al. 2015b), correlation alignment (CORAL) (Sun et al. 2016; Zhuo et al. 2017), geodesic distance (Wu et al. 2019), and contrastive domain discrepancy (Kang et al. 2019). Adversarial discriminative methods usually employ an adversarial objective with respect to a domain discriminator to encourage domain confusion (Ganin et al. 2016; Tzeng et al. 2017; Chen et al. 2017b; Shen et al. 2017; Tsai et al. 2018; Huang et al. 2018). Adversarial generative methods combine the domain discriminative model with a generative component to generate fake source or target data generally based on GAN (Goodfellow et al. 2014; Bousmalis et al. 2017) and its variants, such as CoGAN (Liu and Tuzel 2016), SimGAN (Shrivastava et al. 2017), CycleGAN (Zhu et al. 2017; Zhao et al. 2019b; Yue et al. 2019), and CyCADA (Hoffman et al. 2018b). Self-supervision based methods incorporate auxiliary self-supervised learning tasks into the original task network to bring the source and target domains closer. The commonly used self-supervision tasks include reconstruction (Ghifary et al. 2015, 2016; Chen et al. 2020), image rotation prediction (Sun et al. 2019; Xu et al. 2019), jigsaw prediction (Carlucci et al. 2019), and masking (Vu et al. 2020). While the adversarial generative methods consider the pixel-level alignment, the others mainly employ feature-level alignment. Although these methods make remarkable progress to SUDA, they suffer from large performance decay when directly applied to the MDA problem.

Table 1 Comparison of the proposed MADAN and MADAN+ models with several state-of-the-art domain adaptation methods. The full names of each property from the third to the last columns are pixel-level alignment, multi-scale image generation, feature-level alignment, category-level alignment, semantic consistency, cycle-consistency, multiple sources, domain aggregation, one task network, fine-grained prediction, and end-to-end trainable, respectively
Full size table
Multi-source Domain Adaptation
Multi-source domain adaptation (MDA) considers a more practical scenario, where the training data are collected from multiple sources (Sun et al. 2015; Zhao et al. 2019a). Some theoretical analysis (Ben-David et al. 2010; Hoffman et al. 2018a) is developed to support existing MDA algorithms. The early MDA methods mainly focus on shallow models, including two categories (Sun et al. 2015): feature representation approaches (Duan et al. 2009; Sun et al. 2011; Duan et al. 2012a; Chattopadhyay et al. 2012; Duan et al. 2012b) and combination of pre-learned classifiers (Yang et al. 2007; Schweikert et al. 2009; Xu and Sun 2012; Sun and Shi 2013). Some recent shallow MDA methods mainly aim to deal with special cases, such as incomplete MDA (Ding et al. 2018) and target shift (Redko et al. 2019).

Recently, some representative deep learning based MDA methods are proposed, such as multisource domain adversarial network (MDAN) (Zhao et al. 2018), deep cocktail network (DCTN) (Xu et al. 2018), moment matching network (MMN) (Peng et al. 2019), and multi-source distilling domain adaptation (MDDA) (Zhao et al. 2020b). All these MDA methods only consider the feature-level alignment for image classification tasks. The former three methods employ a shared feature extractor to symmetrically map the multiple sources and target into the same space. For each source-target pair in MDAN and DCTN, a discriminator is trained to distinguish the source and target features. MDAN directly concatenates all extracted source features and labels into one domain and train a single task model, while a task model is trained for each source domain in DCTN, which combines the predictions of different models for a target image using perplexity scores as weights. MMN transfers the learned knowledge from multiple sources to the target by dynamically aligning moments of their feature distributions. The final prediction of a target image is averaged uniformly based on the classifiers from different source domains. MDDA first pre-trains a feature extractor for each source and match the target feature to each source feature space asymmetrically. After distilling the pre-trained classifiers with selected representative samples in each source, the predictions of the matched target features using corresponding source classifiers are combined based on the weights obtained from the Wasserstein distance. Differently, we also consider the pixel-level alignment. Based on the aggregated intermediate domain obtained by sub-domain aggregation discriminator and cross-domain cycle discriminator, only one task model needs to be trained. Besides the image classification tasks, we also perform semantic segmentation task, which is the first work on MDA for semantic segmentation. Table 1 compares MADAN and MADAN+ with several state-of-the-art DA methods.

Problem Setup
We consider the unsupervised MDA scenario with multiple labeled source domains ğ‘†1,ğ‘†2,â‹¯,ğ‘†ğ‘€, where M is number of sources, and one unlabeled target domain T. In the ith source domain ğ‘†ğ‘–, suppose ğ‘‹ğ‘–={ğ±ğ‘—ğ‘–}ğ‘ğ‘–ğ‘—=1 and ğ‘Œğ‘–={ğ²ğ‘—ğ‘–}ğ‘ğ‘–ğ‘—=1 are the observed data and corresponding labels drawn from the source distribution ğ‘ğ‘–(ğ±,ğ²), where ğ‘ğ‘– is the number of samples in ğ‘†ğ‘–. For different tasks, the format of labels ğ²ğ‘—ğ‘– varies. For example, in classification, each image has a unique ğ²ğ‘—ğ‘–; in segmentation, ğ²ğ‘—ğ‘– is pixel-wise. In the target domain T, let ğ‘‹ğ‘‡={ğ±ğ‘—ğ‘‡}ğ‘ğ‘‡ğ‘—=1 denote the target data drawn from the target distribution ğ‘ğ‘‡(ğ±,ğ²) without label observation, where ğ‘ğ‘‡ is the number of target samples. Unless otherwise specified, we have two assumptions: (1) homogeneity, i.e. ğ±ğ‘—ğ‘–âˆˆâ„ğ‘‘,ğ±ğ‘—ğ‘‡âˆˆâ„ğ‘‘, indicating that the data from different domains are observed in the same image space but with different distributions; (2) closed set, i.e. ğ²ğ‘—ğ‘–âˆˆğ’´,ğ²ğ‘—ğ‘‡âˆˆğ’´, where ğ’´ is the label set, which means that all the domains share the same space of classes. Based on covariate shift and concept drift (Patel et al. 2015), we aim to learn an adaptation model that can correctly predict the labels of a sample from the target domain trained on {(ğ‘‹ğ‘–,ğ‘Œğ‘–)}ğ‘€ğ‘–=1 and {ğ‘‹ğ‘‡}. How to extend the unsupervised, homogeneous, and closed set MDA method to other settings, such as heterogeneous DA, open set DA, and category-shift DA remains our future work.

Fig. 2
figure 2
Overall pipeline of the proposed Multi-source Adversarial Domain Aggregation Network (MADAN). MADAN performs pixel-level alignment, feature-level alignment, and category-level alignment between different source domains and the target domain. Further, it preserves the semantic consistency dynamically between the adapted images and the source images and aggregates different adapted domains

Full size image
Fig. 3
figure 3
Detailed framework of the proposed Multi-source Adversarial Domain Aggregation Network (MADAN). The colored solid arrows represent generators, while the black and grey solid arrows indicate the task network F. The dashed arrows correspond to different losses (Color figure online)

Full size image
Multi-source Adversarial Domain Aggregation Network
In this section, we introduce the proposed Multi-source Adversarial Domain Aggregation Network (MADAN) for image classification and semantic segmentation adaptation in detail. The overall pipeline is shown in Fig. 2, and the detailed framework is illustrated in Fig. 3. MADAN consists of three components: Dynamic Adversarial Image Generation (DAIG), Adversarial Domain Aggregation (ADA), and Feature-aligned Task Learning (FTL). DAIG aims to generate adapted images from source domains to the target domain from the perspective of visual appearance while preserving the semantic information dynamically. In order to reduce the distances among the adapted domains and thus generate a more aggregated unified domain, ADA is proposed, including Cross-domain Cycle Discriminator (CCD) and Sub-domain Aggregation Discriminator (SAD). Finally, FTL learns the domain-invariant representations at the feature-level in an adversarial manner. The frameworks of different components are shown in Fig 4. For each component, we first introduce the motivations of our designs and then describe the detailed method.

Dynamic Adversarial Image Generation
Fig. 4
figure 4
The frameworks of different components in the proposed MADAN

Full size image
Motivation. The goal of DAIG is to translate images from different source domains to an intermediate domain with adapted images that are visually similar to the target images, as if they are drawn from the same target distribution. This part corresponds to pixel-level alignment, which has been demonstrated to be effective in single-source DA (Bousmalis et al. 2017; Russo et al. 2018; Hu et al. 2018; Hoffman et al. 2018b) but has not been explored in MDA. One intuitive method is to employ a GAN (Goodfellow et al. 2014) for each source to translate source images with target styles. However, such standard adversarial procedure often leads to the mode collapse problem (Zhu et al. 2017; Zhao et al. 2021), where all source images are mapped to the same output image and the optimization fails to make progress. Following CylceGAN (Zhu et al. 2017), we add a cycle-consistency loss to enforce that the adapted images can be reconstructed back to the original source images.

Besides with target styles, the adapted images are expected to preserve the semantic information of original source images so that we can train the task model based on the adapted images and corresponding source labels, but the semantic consistency cannot be guaranteed by the cycle-consistency loss. CyCADA consists of a semantic consistency loss to better preserve the semantic information by feeding both the source images and adapted images into the same task model pretrained on the source domain (Hoffman et al. 2018b). However, since the source images and adapted images are from different domains, employing the same task model to obtain the predicted results and then computing the semantic consistency loss may be detrimental to image generation. We propose to employ the pretrained task model only for the source images and a novel dynamically updated network for the adapted images so that the optimal input domain of the dynamic network can gradually change from the source domain to the target domain.

Method. For each source domain ğ‘†ğ‘–, we introduce a generator ğºğ‘†ğ‘–â†’ğ‘‡ mapping to the target T in order to generate adapted images that fool ğ·ğ‘‡, which is a pixel-level adversarial discriminator. ğ·ğ‘‡ is trained simultaneously with each ğºğ‘†ğ‘–â†’ğ‘‡ to classify real target images ğ‘‹ğ‘‡ from adapted images ğºğ‘†ğ‘–â†’ğ‘‡(ğ‘‹ğ‘–). The corresponding GAN loss is:

â„’ğ‘†ğ‘–â†’ğ‘‡ğºğ´ğ‘(ğºğ‘†ğ‘–â†’ğ‘‡,ğ·ğ‘‡,ğ‘‹ğ‘–,ğ‘‹ğ‘‡)=ğ”¼ğ±ğ‘–âˆ¼ğ‘‹ğ‘–logğ·ğ‘‡(ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–))+ğ”¼ğ±ğ‘‡âˆ¼ğ‘‹ğ‘‡log[1âˆ’ğ·ğ‘‡(ğ±ğ‘‡)].
(1)
Further, we employ an inverse mapping ğºğ‘‡â†’ğ‘†ğ‘– as well as a cycle-consistency loss (Zhu et al. 2017) to enforce ğºğ‘‡â†’ğ‘†ğ‘–(ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–))â‰ˆğ± and vice versa. Similarly, we introduce ğ·ğ‘– to classify ğ‘‹ğ‘– from ğºğ‘‡â†’ğ‘†ğ‘–(ğ‘‹ğ‘‡), with the following GAN loss:

â„’ğ‘‡â†’ğ‘†ğ‘–ğºğ´ğ‘(ğºğ‘‡â†’ğ‘†ğ‘–,ğ·ğ‘–,ğ‘‹ğ‘‡,ğ‘‹ğ‘–)=ğ”¼ğ±ğ‘–âˆ¼ğ‘‹ğ‘–log[1âˆ’ğ·ğ‘–(ğ±ğ‘–)]+ğ”¼ğ±ğ‘‡âˆ¼ğ‘‹ğ‘‡logğ·ğ‘–(ğºğ‘‡â†’ğ‘†ğ‘–(ğ±ğ‘‡)).
(2)
The cycle-consistency loss (Zhu et al. 2017) ensures that the learned mappings ğºğ‘†ğ‘–â†’ğ‘‡ and ğºğ‘‡â†’ğ‘†ğ‘– are cycle-consistent, thereby preventing them from contradicting each other, is defined as:

â„’ğ‘†ğ‘–â†”ğ‘‡ğ‘ğ‘¦ğ‘(ğºğ‘†ğ‘–â†’ğ‘‡,ğºğ‘‡â†’ğ‘†ğ‘–,ğ‘‹ğ‘–,ğ‘‹ğ‘‡)=ğ”¼ğ±ğ‘–âˆ¼ğ‘‹ğ‘–âˆ¥ğºğ‘‡â†’ğ‘†ğ‘–(ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–))âˆ’ğ±ğ‘–âˆ¥1+ğ”¼ğ±ğ‘‡âˆ¼ğ‘‹ğ‘‡âˆ¥ğºğ‘†ğ‘–â†’ğ‘‡(ğºğ‘‡â†’ğ‘†ğ‘–(ğ±ğ‘‡))âˆ’ğ±ğ‘‡âˆ¥1.
(3)
To ideally preserve the semantic information, the adapted images ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–) should be fed into a network ğ¹ğ‘‡ trained on the target domain, which is infeasible since target domain labels are not available in UDA. Instead of employing ğ¹ğ‘– on ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–), we propose to dynamically update the network ğ¹ğ´, which takes ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–) as input, so that its optimal input domain (the domain that the network performs best on) gradually changes from that of ğ¹ğ‘– to ğ¹ğ‘‡. We employ the task model F trained on the adapted domain as ğ¹ğ´, i.e. ğ¹ğ´=ğ¹, which has two advantages: (1) ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–) becomes the optimal input domain of ğ¹ğ´, and as F is trained to have better performance on the target domain, the semantic loss after ğ¹ğ´ would promote ğºğ‘†ğ‘–â†’ğ‘‡ to generate images that are closer to target domain at the pixel-level; (2) since ğ¹ğ´ and F can share the parameters, no additional training or memory space is introduced, which is quite efficient. Let ğ¾ğ¿(â‹…||â‹…) denote the KL divergence between two distributions, and then the proposed dynamic semantic consistency (DSC) loss is:

â„’ğ‘†ğ‘–ğ·ğ‘†ğ¶(ğºğ‘†ğ‘–â†’ğ‘‡,ğ‘‹ğ‘–,ğ¹ğ‘–,ğ¹ğ´)=ğ”¼ğ±ğ‘–âˆ¼ğ‘‹ğ‘–ğ¾ğ¿(ğ¹ğ´(ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–))||ğ¹ğ‘–(ğ±ğ‘–)).
(4)
Adversarial Domain Aggregation
Motivation. After DAIG, each source domain is translated to an adapted domain with target style and the semantic information is well preserved. Previous methods mainly employ two strategies to learn from different adapted domains: training different task models for each adapted domain and combining different predictions with specific weights for target images (Xu et al. 2018; Peng et al. 2019), and simply combining all adapted domains together and training one task model (Zhao et al. 2018). In the first strategy, it is challenging to determine how to select the weights for different adapted domains. Moreover, each target image needs to be fed into all task models at inference time, which is rather inefficient. For the second strategy, since the alignment space is high-dimensional, although the adapted domains are relatively aligned with the target, they may be significantly misaligned with each other, as illustrated in Fig. 5a. A detailed example of such misalignment across different adapted domains is given in Fig. 6a. As emphasized by the blue circle, the Amazon source and Caltech source are both aligned with the Webcam target, but they are obviously not aligned. In order to mitigate this issue, we propose adversarial domain aggregation to make different adapted domains more closely aggregated with two kinds of discriminators: sub-domain aggregation discriminator (SAD) and cross-domain cycle discriminator (CCD).

Fig. 5
figure 5
Illustration of the necessity of domain aggregation

Full size image
Fig. 6
figure 6
One detailed example of domain aggregation on the Office+Caltech-10 dataset (Gong et al. 2013). The employed baseline model for image translation is based on CycleGAN (Zhu et al. 2017). The learned features are visualized by t-SNE (Maaten and Hinton 2008)

Full size image
Method. SAD is designed to directly make the different adapted domains indistinguishable. For ğ‘†ğ‘–, a discriminator ğ·ğ‘–ğ´ is introduced with the following loss function:

â„’ğ‘†ğ‘–ğ‘†ğ´ğ·(ğºğ‘†1â†’ğ‘‡,â€¦ğºğ‘†ğ‘–â†’ğ‘‡,â€¦,ğºğ‘†ğ‘€â†’ğ‘‡,ğ·ğ‘–ğ´)=ğ”¼ğ±ğ‘–âˆ¼ğ‘‹ğ‘–logğ·ğ‘–ğ´(ğºğ‘†ğ‘–â†’ğ‘‡(ğ±ğ‘–))+1ğ‘€âˆ’1âˆ‘ğ‘—â‰ ğ‘–ğ”¼ğ±ğ‘—âˆ¼ğ‘‹ğ‘—log[1âˆ’ğ·ğ‘–ğ´(ğºğ‘†ğ‘—â†’ğ‘‡(ğ±ğ‘—))].
(5)
CCD is designed to discriminate between the images from each source and the images transferred from other sources. For each source domain ğ‘†ğ‘–, we transfer the images from the adapted domains ğºğ‘†ğ‘—â†’ğ‘‡(ğ‘‹ğ‘—), ğ‘—=1,â‹¯,ğ‘€,ğ‘—â‰ ğ‘– back to ğ‘†ğ‘– using ğºğ‘‡â†’ğ‘†ğ‘– and employ the discriminator ğ·ğ‘– to classify ğ‘‹ğ‘– from ğºğ‘‡â†’ğ‘†ğ‘–(ğºğ‘†ğ‘—â†’ğ‘‡(ğ‘‹ğ‘—)), which corresponds to the following loss function:

â„’ğ‘†ğ‘–ğ¶ğ¶ğ·(ğºğ‘‡â†’ğ‘†1,â€¦ğºğ‘‡â†’ğ‘†ğ‘–âˆ’1,ğºğ‘‡â†’ğ‘†ğ‘–+1,â€¦,ğºğ‘‡â†’ğ‘†ğ‘€,ğºğ‘†ğ‘–â†’ğ‘‡,ğ·ğ‘–)=ğ”¼ğ±ğ‘–âˆ¼ğ‘‹ğ‘–logğ·ğ‘–(ğ±ğ‘–)+1ğ‘€âˆ’1âˆ‘ğ‘—â‰ ğ‘–ğ”¼ğ±ğ‘—âˆ¼ğ‘‹ğ‘—log[1âˆ’ğ·ğ‘–(ğºğ‘‡â†’ğ‘†ğ‘–((ğºğ‘†ğ‘—â†’ğ‘‡(ğ±ğ‘—)))].
(6)
As shown in Figs. 5b and 6b, different sources are much better aligned after the proposed domain aggregation. Please note that using a more sophisticated combination of different discriminatorsâ€™ losses to better aggregate the domains with larger distances might improve the performance. We leave this as future work and would explore this direction by dynamic weighting of the loss terms and incorporating some prior domain knowledge of the sources.

Feature-Aligned Task Learning
Motivation. After adversarial domain aggregation, the adapted images of different domains ğ‘‹â€²ğ‘–(ğ‘–=1,â€¦,ğ‘€) are more closely aggregated and aligned. Meanwhile, the semantic consistency loss in dynamic adversarial image generation ensures that the semantic information, i.e. the labels, is preserved before and after image translation. Therefore, we can train the task model that is transferable to the target domain based on the aggregated adapted images and corresponding source labels. Similar to most MDA methods (Xu et al. 2018; Zhao et al. 2018; Peng et al. 2019; Zhao et al. 2020b), we also impose a feature-level alignment between adapted images and target images, which can improve the task performance during inference of the target images.

Method. Suppose the images of the unified aggregated domain are ğ‘‹â€²=â‹ƒğ‘–=1ğ‘€ğ‘‹â€²ğ‘– and corresponding labels are ğ‘Œ=â‹ƒğ‘–=1ğ‘€ğ‘Œğ‘–. We can then train a task learning model F based on ğ‘‹â€² and Y. For classification and segmentation, F aims to respectively minimize the following cross-entropy loss â„’ğ‘¡ğ‘ğ‘ ğ‘˜(ğ¹,ğ‘‹â€²,ğ‘Œ):

â„’ğ‘ğ‘™ğ‘(ğ¹,ğ‘‹â€²,ğ‘Œ)=âˆ’ğ”¼(ğ±â€²,ğ‘¦)âˆ¼(ğ‘‹â€²,ğ‘Œ)âˆ‘ğ¿ğ‘™=1ğŸ™[ğ‘™=ğ‘¦]log(ğœ(ğ¹(ğ±â€²))),
(7)
â„’ğ‘ ğ‘’ğ‘”(ğ¹,ğ‘‹â€²,ğ‘Œ)=âˆ’ğ”¼(ğ±â€²,ğ²)âˆ¼(ğ‘‹â€²,ğ‘Œ)âˆ‘ğ¿ğ‘™=1âˆ‘ğ»â„=1âˆ‘ğ‘Šğ‘¤=1ğŸ™[ğ‘™=ğ²â„,ğ‘¤]log(ğœ(ğ¹ğ‘™,â„,ğ‘¤(ğ±â€²))),
(8)
where L is the number of classes, H, W are the height and width of the adapted images, ğœ is the softmax function, ğŸ™ is an indicator function, and ğ¹ğ‘™,â„,ğ‘¤(ğ±â€²) is the value of ğ¹(ğ±â€²) at index (l, h, w).

We introduce a discriminator ğ·ğ¹ to conduct feature-level alignment (FLA) between ğ‘‹â€² and ğ‘‹ğ‘‡. The GAN loss of FLA is defined as:

â„’ğ¹ğ¿ğ´(ğ¹ğ‘“,ğ·ğ¹ğ‘“,ğ‘‹â€²,ğ‘‹ğ‘‡)=ğ”¼ğ±â€²âˆ¼ğ‘‹â€²logğ·ğ¹ğ‘“(ğ¹ğ‘“(ğ±â€²))+ğ”¼ğ±ğ‘‡âˆ¼ğ‘‹ğ‘‡log[1âˆ’ğ·ğ¹ğ‘“(ğ¹ğ‘“(ğ±ğ‘‡))],
(9)
where ğ¹ğ‘“(â‹…) is the output of the last convolution layer (i.e. a feature map) of the encoder in F.

MADAN Learning
The proposed MADAN learning framework utilizes adaptation techniques including pixel-level alignment, cycle-consistency, dynamic semantic consistency, domain aggregation, and feature-level alignment. Combining all these components, the overall objective loss function of MADAN is:

â„’ğ‘€ğ´ğ·ğ´ğ‘(ğºğ‘†1â†’ğ‘‡â‹¯ğºğ‘†ğ‘€â†’ğ‘‡,ğºğ‘‡â†’ğ‘†1â‹¯ğºğ‘‡â†’ğ‘†ğ‘€,ğ·1â‹¯ğ·ğ‘€,ğ·ğ‘‡,ğ·1ğ´â‹¯ğ·ğ‘€ğ´,ğ·ğ¹ğ‘“,ğ¹)=âˆ‘ğ‘–=1ğ‘€[â„’ğ‘†ğ‘–â†’ğ‘‡ğºğ´ğ‘(ğºğ‘†ğ‘–â†’ğ‘‡,ğ·ğ‘‡,ğ‘‹ğ‘–,ğ‘‹ğ‘‡)+â„’ğ‘‡â†’ğ‘†ğ‘–ğºğ´ğ‘(ğºğ‘‡â†’ğ‘†ğ‘–,ğ·ğ‘–,ğ‘‹ğ‘‡,ğ‘‹ğ‘–)+â„’ğ‘†ğ‘–â†”ğ‘‡ğ‘ğ‘¦ğ‘(ğºğ‘†ğ‘–â†’ğ‘‡,ğºğ‘‡â†’ğ‘†ğ‘–,ğ‘‹ğ‘–,ğ‘‹ğ‘‡)+â„’ğ‘†ğ‘–ğ·ğ‘†ğ¶(ğºğ‘†ğ‘–â†’ğ‘‡,ğ‘‹ğ‘–,ğ¹ğ‘–,ğ¹)+â„’ğ‘†ğ‘–ğ‘†ğ´ğ·(ğºğ‘†1â†’ğ‘‡,â€¦ğºğ‘†ğ‘–â†’ğ‘‡,â€¦,ğºğ‘†ğ‘€â†’ğ‘‡,ğ·ğ‘–ğ´)+â„’ğ‘†ğ‘–ğ¶ğ¶ğ·(ğºğ‘‡â†’ğ‘†1,â€¦ğºğ‘‡â†’ğ‘†ğ‘–âˆ’1,ğºğ‘‡â†’ğ‘†ğ‘–+1,â€¦,ğºğ‘‡â†’ğ‘†ğ‘€,ğºğ‘†ğ‘–â†’ğ‘‡,ğ·ğ‘–)]+â„’ğ‘¡ğ‘ğ‘ ğ‘˜(ğ¹,ğ‘‹â€²,ğ‘Œ)+â„’ğ¹ğ¿ğ´(ğ¹ğ‘“,ğ·ğ¹ğ‘“,ğ‘‹â€²,ğ‘‹ğ‘‡).
(10)
The training process corresponds to solving for a target model F according to the optimization:

ğ¹âˆ—=argminğ¹minğ·maxğºâ„’ğ‘€ğ´ğ·ğ´ğ‘(ğº,ğ·,ğ¹),
(11)
where G and D represent all the generators and discriminators in Eq. (10), respectively.

MADAN+ for Segmentation Adaptation
Motivation. There might be some problems when applying the aforementioned MADAN to pixel-wise segmentation adaptation. First, the feature-level alignment in Sect. 4.3 aims to align the features of the adapted images and the target images globally based on the assumption that each categoryâ€™s appearance frequency is identical in the adapted and target domains. This is obviously unreasonable since different categories (e.g., car and sky) are not uniformly distributed. Second, the image generation based on CycleGAN in Sect. 4.1 only considers one crop scale. When the scale is large, local details might be missing. When it is small, the global semantics cannot be well represented. Moreover, during CycleGANâ€™s training, a batch is composed of randomly cropped images from both the adapted and target domains at different locations. This is problematic since spatial misalignment might be caused. For example, a batch contains the upper part (e.g. sky) in an adapted image and the lower part (e.g. road) in a target image.

To address the above challenges, we propose (1) category-level alignment (CLA) to balance the appearance frequency of different classes, and (2) multi-scale image generation (MIG) with spatial alignment to generate adapted images that well preserve both global semantics and local details.

Category-level Alignment
Different from the global alignment in FLA, CLA considers the alignment of local regions in different classes between the adapted and target images. Based on FLA, we can obtain the grid-wise (pseudo) labels â„µğ‘™ğ‘›(ğ±) for class l of the nth grid in image ğ±. Here ğ‘™=1,â‹¯,ğ¿,ğ‘›=1,â‹¯,ğ‘. Following (Chen et al. 2017b), we employ one discriminator ğ·ğ‘™ğ¶ to differentiate class l between the adapted and target domainsFootnote1. Let ğ‘Œ(ğ±ğ‘‘) denote the labeling function for image ğ±ğ‘‘ in domain d, and we have:

ğ‘Œ(ğ±ğ‘‘)={ğ²ğ‘‘,        if  ğ‘‘âˆˆ{1,â‹¯,ğ‘€},ğ¹(ğ±ğ‘‘),  if  ğ‘‘=ğ‘‡.
(12)
Suppose â„›(ğ‘›) is the group of pixels in grid n, and then we can obtain the grid-wise (pseudo) labels â„µğ‘™ğ‘›(ğ±ğ‘‘) as:

â„µğ‘™ğ‘›(ğ±ğ‘‘)=âˆ‘ğ‘Ÿâˆˆâ„›(ğ‘›)|ğ‘Œ(ğ±ğ‘Ÿğ‘‘)==ğ‘™||â„›(ğ‘›)|.
(13)
In order to balance the appearance frequency of the adapted and target (pseudo) labels, we normalize â„µğ‘™ğ‘›(ğ±ğ‘‘) as:

â„µËœğ‘™ğ‘›(ğ±ğ‘‘)=â„µğ‘™ğ‘›(ğ±ğ‘‘)âˆ‘ğ‘ğ‘›=1â„µğ‘™ğ‘›(ğ±ğ‘‘).
(14)
And then the GAN loss of CLA can be obtained as:

â„’ğ¶ğ¿ğ´(ğ¹ğ‘“,ğ·1ğ¶,â‹¯,ğ·ğ¿ğ¶,ğ‘‹â€²,ğ‘‹ğ‘‡)=ğ”¼ğ±â€²âˆ¼ğ‘‹â€²âˆ‘ğ‘™=1ğ¿âˆ‘ğ‘›=1ğ‘â„µËœğ‘™ğ‘›(ğ±â€²)logğ·ğ‘™ğ¶(ğ¹ğ‘“(ğ±â€²)ğ‘›)+ğ”¼ğ±ğ‘‡âˆ¼ğ‘‹ğ‘‡âˆ‘ğ‘™=1ğ¿âˆ‘ğ‘›=1ğ‘â„µËœğ‘™ğ‘›(ğ±ğ‘‡)log[1âˆ’ğ·ğ‘™ğ¶(ğ¹ğ‘“(ğ±ğ‘‡)ğ‘›)].
(15)
Multi-scale Image Generation
Besides global semantics, the local details of the intermediate adapted domain are more important for segmentation adaptation as compared to classification adaptation. For example, a clear boundary between the foreground and the background can contribute to the segmentation. Therefore, it is crucial to generate high-quality images during image generation process. To address this issue, we propose multi-scale image generation (MIG) with spatial alignment.

First, we resize the images from both the adapted and target domains to make the resolution aligned. Second, we randomly select a point as the center to uniformly crop both the adapted and target images into multiple sizes {ğ¶1,â€¦,ğ¶ğ¾}. We observe that the spatial distributions of the classes between the adapted and target domains are roughly the same (e.g. class sky is basically on the top of an image in both domains). Therefore, uniform cropping is crucial to ensure spatial alignment. Finally, we resize the pyramid samples into a fixed resolution. In this way, the adapted images by multi-scale image generation can well preserve both global semantics and local details. During inference, the full-size target image can be directly fed into the image generator to generate high-quality intermediate images.

Following previous steps, we can form a mini-batch ğ‘‹Ëœğ‘˜ğ‘– and ğ‘‹Ëœğ‘˜ğ‘‡,ğ‘˜=1,â‹¯,ğ¾ for each scale k during the training of CycleGAN. The MIG loss is defined as:

â„’ğ‘€ğ¼ğº(ğºğ‘†1â†’ğ‘‡â‹¯ğºğ‘†ğ‘€â†’ğ‘‡,ğºğ‘‡â†’ğ‘†1â‹¯ğºğ‘‡â†’ğ‘†ğ‘€,ğ·1â‹¯ğ·ğ‘€,ğ·ğ‘‡)=âˆ‘ğ‘–=1ğ‘€âˆ‘ğ‘˜=1ğ¾[â„’ğ‘†ğ‘–â†’ğ‘‡ğºğ´ğ‘(ğºğ‘†ğ‘–â†’ğ‘‡,ğ·ğ‘‡,ğ‘‹Ëœğ‘˜ğ‘–,ğ‘‹Ëœğ‘˜ğ‘‡)+â„’ğ‘‡â†’ğ‘†ğ‘–ğºğ´ğ‘(ğºğ‘‡â†’ğ‘†ğ‘–,ğ·ğ‘–,ğ‘‹Ëœğ‘˜ğ‘‡,ğ‘‹Ëœğ‘˜ğ‘–)+â„’ğ‘†ğ‘–â†”ğ‘‡ğ‘ğ‘¦ğ‘(ğºğ‘†ğ‘–â†’ğ‘‡,ğºğ‘‡â†’ğ‘†ğ‘–,ğ‘‹Ëœğ‘˜ğ‘–,ğ‘‹Ëœğ‘˜ğ‘‡)+â„’ğ‘†ğ‘–ğ·ğ‘†ğ¶(ğºğ‘†ğ‘–â†’ğ‘‡,ğ‘‹Ëœğ‘˜ğ‘–,ğ¹ğ‘–,ğ¹)].
(16)
MADAN+ Learning
Combining MADAN with CLA and MIG, we can obtain the overall objective loss function of MADAN+ as:

â„’ğ‘€ğ´ğ·ğ´ğ‘+(ğºğ‘†1â†’ğ‘‡â‹¯ğºğ‘†ğ‘€â†’ğ‘‡,ğºğ‘‡â†’ğ‘†1â‹¯ğºğ‘‡â†’ğ‘†ğ‘€,ğ·1â‹¯ğ·ğ‘€,ğ·ğ‘‡,ğ·1ğ´â‹¯ğ·ğ‘€ğ´,ğ·ğ¹ğ‘“,ğ¹,ğ·1ğ¶,â‹¯,ğ·ğ¿ğ¶)=â„’ğ‘€ğ¼ğº(ğºğ‘†1â†’ğ‘‡â‹¯ğºğ‘†ğ‘€â†’ğ‘‡,ğºğ‘‡â†’ğ‘†1â‹¯ğºğ‘‡â†’ğ‘†ğ‘€,ğ·1â‹¯ğ·ğ‘€,ğ·ğ‘‡)+âˆ‘ğ‘–=1ğ‘€[â„’ğ‘†ğ‘–ğ‘†ğ´ğ·(ğºğ‘†1â†’ğ‘‡,â€¦ğºğ‘†ğ‘–â†’ğ‘‡,â€¦,ğºğ‘†ğ‘€â†’ğ‘‡,ğ·ğ‘–ğ´)+â„’ğ‘†ğ‘–ğ¶ğ¶ğ·(ğºğ‘‡â†’ğ‘†1,â€¦ğºğ‘‡â†’ğ‘†ğ‘–âˆ’1,ğºğ‘‡â†’ğ‘†ğ‘–+1,â€¦,ğºğ‘‡â†’ğ‘†ğ‘€,ğºğ‘†ğ‘–â†’ğ‘‡,ğ·ğ‘–)]+â„’ğ‘¡ğ‘ğ‘ ğ‘˜(ğ¹,ğ‘‹â€²,ğ‘Œ)+â„’ğ¹ğ¿ğ´(ğ¹ğ‘“,ğ·ğ¹ğ‘“,ğ‘‹â€²,ğ‘‹ğ‘‡)+â„’ğ¶ğ¿ğ´(ğ¹ğ‘“,ğ·1ğ¶,â‹¯,ğ·ğ¿ğ¶,ğ‘‹â€²,ğ‘‹ğ‘‡).
(17)
The training process of MADAN+ is similar to MADAN.

Experiments
In this section, we first introduce the experimental settings and then compare the DA results of the proposed MADAN with several state-of-the-art approaches both quantitatively and qualitatively, followed by some empirical analysis on ablation study, feature visualization, and model interpretability. Our source code is released at: https://github.com/Luodian/MADAN.

Experimental Settings
In this section, the datasets, baselines, evaluation metrics, and implementation details are described.

Datasets
Digit Recognition. Digits-five includes 5 digit image datasets sampled from different domains, including handwritten mt (MNIST) (LeCun et al. 1998), combined mm (MNIST-M) (Ganin and Lempitsky 2015), street image sv (SVHN) (Netzer et al. 2011), synthetic sy (Synthetic Digits) (Ganin and Lempitsky 2015), and handwritten up (USPS) (Hull 1994). Following (Xu et al. 2018; Peng et al. 2019), we sample 25,000 images for training and 9,000 for testing in mt, mm, sv, sy, and select the entire 9,298 images in up as a domain.

Object Classification. Office-31 (Saenko et al. 2010) contains 4,110 images within 31 categories, which are collected from office environment in three image domains: A (Amazon) downloaded from amazon.com, W (Webcam) and D (DSLR) taken by web camera and digital SLR camera, respectively.

Office+Caltech-10 (Gong et al. 2013) consists of the 10 overlapping categories shared by Office-31 (Saenko et al. 2010) and C (Caltech-256) (Griffin et al. 2007). Totally there are 2,533 images.

Office-Home (Venkateswara et al. 2017) is a larger object dataset with 30,475 images within 65 categories. There are 4 different domains: Artistic images (Ar), Clip-Art images (Cl), Product images (Pr) and Real-World images (Rw).

Semantic Segmentation. Cityscapes (Cordts et al. 2016 contains vehicle-centric urban street images collected from a moving vehicle in 50 cities from Germany and neighboring countries. There are 5,000 images with pixel-wise annotations. The images have resolution of 2048Ã—1024 and are labeled into 19 classes.

BDDS (Yu et al. 2018 contains 10,000 real-world dash cam video frames with accurate pixel-wise annotations. It has a compatible label space with Cityscapes and the image resolution is 1280Ã—720..

GTA (Richter et al. 2016 is a vehicle-egocentric image dataset collected in the high-fidelity rendered computer game GTA-V. It contains 24,966 images (video frames) with the resolution 1914Ã—1052. There are 19 classes compatible with Cityscapes.

SYNTHIA (Ros et al. 2016 is a large synthetic dataset. A subset, named SYNTHIA-RANDCITYSCAPES, is designed to pair with Cityscapes with 9,400 images with resolution 960Ã—720 which are automatically annotated with 16 object classes, one void class, and some unnamed classes.

Baselines
We compare MADAN with the following methods. (1) Source-only, i.e. train on the source domains and directly test on the target domain. We can view this as a lower bound of DA. (2) Single-source DA, perform multi-source DA via single-source DA. (3) Multi-source DA, extend some single-source DA method to multi-source settings.

For digit recognition and object classification, we employ two strategies to implement the source-only and single-source DA standards: (1) single-best, i.e. performing adaptation on each single source and selecting the best adaptation result in the target test set; (2) source-combined, i.e. all source domains are combined into a traditional single source. The compared single-source DA includes TCA (Pan et al. 2010), GFK (Gong et al. 2012), DDC (Tzeng et al. 2015), DRCN (Ghifary et al. 2016), RevGrad (Ganin and Lempitsky 2015), DAN (Long et al. 2015b), RTN (Long et al. 2016), CORAL (Sun et al. 2016), DANN (Ganin et al. 2016), ADDA (Tzeng et al. 2017), JAN (Long et al. 2017), and CyCADA (Hoffman et al. 2018b). The compared multi-source DA includes DCTN (Xu et al. 2018), MDAN (Zhao et al. 2018), MMN (Peng et al. 2019), and MDDA (Zhao et al. 2020b). Please note that we only compare the methods that report the results on corresponding tasks.

For semantic segmentation, besides source combined, we also implement the source-only and single-source DA standards on each source, i.e. performing adaptation on each single source. The compared single-source DA includes FCNs Wld (Hoffman et al. 2016), CDA (Zhang et al. 2017), ROAD (Chen et al. 2018), AdaptSeg (Tsai et al. 2018), CyCADA (Hoffman et al. 2018b), and DCAN (Wu et al. 2018). Since MADAN is the first work on MDA for segmentation, we extend the original classification network in MDAN (Zhao et al. 2018) to our segmentation task for comparison. We also report the results of an oracle setting, where the segmentation model is both trained and tested on the target domain.

Evaluation Metrics
For digit recognition and object classification adaptation, we employ the average classification accuracy of all categories to evaluate the results following (Ganin et al. 2016; Tzeng et al. 2017; Hoffman et al. 2018b). The larger the classification accuracy is, the better the result is.

For pixel-wise segmentation adaptation, we employ class-wise intersection-over-union (cwIoU) and mean IoU (mIoU) to evaluate the results of each class and all classes as in (Hoffman et al. 2016; Zhang et al. 2017; Hoffman et al. 2018b). Let ğ’«ğ‘™ and ğ’¢ğ‘™ respectively denote the predicted and ground-truth pixels that belong to class l, and then ğ‘ğ‘¤ğ¼ğ‘œğ‘ˆğ‘™=|ğ’«ğ‘™âˆ©ğ’¢ğ‘™||ğ’«ğ‘™âˆªğ’¢ğ‘™|, ğ‘šğ¼ğ‘œğ‘ˆ=1ğ¿âˆ‘ğ¿ğ‘™=1ğ‘ğ‘¤ğ¼ğ‘œğ‘ˆğ‘™, where |â‹…| denotes the cardinality of a set. Larger cwIoU and mIoU values represent better performances.

Implementation Details
Although MADAN can be trained in an end-to-end manner, due to constrained hardware resources, we train it in three stages. First, we train several CycleGANs (9 residual blocks for generator and 4 convolution layers for discriminator) (Zhu et al. 2017) without semantic consistency loss for each source and target pair, and then train a task model F on the adapted images with corresponding labels from the source domains. Second, after updating ğ¹ğ´ with F trained above, we generate adapted images using CycleGAN with the proposed DSC loss in Eq. (4) and aggregate different adapted domains using SAD and CCD. Finally, we train the task model F on the newly adapted images in the aggregated domain with feature-level alignment. The above stages are trained iteratively. We leave the end-to-end training as future work by deploying model parallelism or experimenting with larger GPU memory.

Table 2 Comparison with the state-of-the-art DA methods for digit recognition on Digits-five dataset measured by classification accuracy (%). The best method is emphasized in bold
Full size table
Table 3 Comparison with the state-of-the-art DA methods for object classification on Office31 dataset measured by classification accuracy (%). The best method is emphasized in bold
Full size table
Table 4 Comparison with the state-of-the-art DA methods for object classification on Office+Caltech-10 dataset measured by classification accuracy (%). The best method is emphasized in bold
Full size table
Table 5 Comparison with the state-of-the-art DA methods for object classification on Office-Home dataset measured by classification accuracy (%). The best method is emphasized in bold
Full size table
Table 6 Comparison with the state-of-the-art DA methods for semantic segmentation from GTA and SYNTHIA to Cityscapes using FCN-VGG16 backbone. The best class-wise IoU and mIoU trained on the source domains are emphasized in bold (similar below)
Full size table
In Digits-five, Office-31 and Office+Caltech-10 experiments, we use AlexNet Krizhevsky et al. (2012) as our backbone. In Office-Home experiments, we adopt ResNet-50 He et al. (2016) as our backbone. In the training stage, we use an Adam optimizer with a batch size of 32 and a learning rate of 1e-3 and 1e-4 respectively for the classification model and feature-level alignment.

In segmentation adaptation experiments, we choose to use FCN Long et al. (2015a) as our semantic segmentation network, and, as the VGG family of networks is commonly used in reporting DA results, we use VGG-16 Simonyan and Zisserman (2015) as the FCN backbone. The weights of the feature extraction layers in the networks are initialized from models trained on ImageNet Deng et al. (2009). The network is implemented in PyTorch and trained with Adam optimizer Kingma and Ba (2015) using a batch size of 8 with initial learning rate 1e-4. We keep the image size the same before and after image translation, and crop the adapted images to 400Ã—400 during the segmentation model training with 40 epochs. We take the 16 intersection classes of GTA and SYNTHIA, compatible with Cityscapes and BDDS, for all mIoU evaluations. To better illustrate the effectiveness of our proposed model, we also employ DeepLabV2 Chen et al. (2017a) with ResNet-101 He et al. (2016) pretrained on ImageNet as the semantic segmentation model.

For digit recognition and object classification, one domain is selected as the target domain and the rest are considered as source domains. For semantic segmentation, we choose synthetic GTA and SYNTHIA as source domains and real Cityscapes and BDDS as target domains.

Comparison with State-of-the-art
Table 2, Table 3, Table 4, and Table 5 show the performance comparisons between the proposed MADAN model and the other baselines, including source-only, single-source DA, source-combined DA, and multi-source DA, on Digits-five, Office-31, Office+Caltech-10, and Office-Home datasets, respectively. The simulation-to-real semantic segmentation adaptation from synthetic GTA and SYNTHIA to real Cityscapes and BDDS are shown in Table 6 and Table 7 for FCN-VGG16 backbone, and Table 8 and Table 9 for DeepLabV2-ResNet101 backbone, respectively. From the results, we have the following similar observations among different adaptation tasks:

Table 7 Comparison with the state-of-the-art DA methods for semantic segmentation from GTA and SYNTHIA to BDDS using FCN-VGG16 backbone
Full size table
Table 8 Comparison with the state-of-the-art DA methods for semantic segmentation from GTA and SYNTHIA to Cityscapes using DeepLabV2-ResNet101 backbone. The best class-wise IoU and mIoU trained on the source domains are emphasized in bold (similar below)
Full size table
Table 9 Comparison with the state-of-the-art DA methods for semantic segmentation from GTA and SYNTHIA to BDDS using DeepLabV2-ResNet101 backbone
Full size table
Table 10 Comparison between the proposed dynamic semantic consistency (DSC) loss in MADAN and the original SC loss in Hoffman et al. (2018b) on Cityscapes using FCN-VGG16 backbone. The better mIoU for each pair is emphasized in bold
Full size table
Table 11 Comparison between the proposed dynamic semantic consistency (DSC) loss in MADAN and the original SC loss in (Hoffman et al. 2018b) on BDDS using FCN-VGG16 backbone. The better mIoU for each pair is emphasized in bold
Full size table
(1) The source-only method that directly transfers the task models trained on the source domains to the target domain obtains the worst performance in most adaptation settings. This is obvious, because the joint probability distributions of observed images and labels are significantly different among the sources and the target, due to the presence of domain shift. Without domain adaptation, the direct transfer cannot well handle this domain gap.

(2) Comparing source-only with corresponding single-best DA and source-combined DA for digit recognition and object classification, and comparing source-only with single-source DA for semantic segmentation, it is clear that almost all adaptation methods perform better than source-only, which demonstrates the effectiveness of domain adaptation. For example, in Table 3, the average accuracy of source-only combined method is 80.2%, while the accuracy of source-combined ADDA is 83.7%.

Table 12 Ablation study on different components in MADAN+ on Cityscapes using FCN-VGG16 backbone. Baseline denotes using pixel-level alignment with cycle-consistency, +SAD denotes using the sub-domain aggregation discriminator, +CCD denotes using the cross-domain cycle discriminator, +DSC denotes using the dynamic semantic consistency loss, +FLA denotes using feature-level alignment, +MIG denotes using multi-scale image generation
Full size table
Table 13 Ablation study on different components in MADAN+ on BDDS using FCN-VGG16 backbone
Full size table
(3) Generally, multi-source DA outperforms other adaptation standards by exploring the complementarity of different sources. This is more obvious when comparing the DA methods that employ similar architectures, such as our MADAN vs. CyCADA (Hoffman et al. 2018b), MDDA (Zhao et al. 2020b) vs. ADDA (Tzeng et al. 2017), and MDAN (Zhao et al. 2018) vs. DANN (Ganin et al. 2016). Besides the domain gap between the sources and the target, multi-source DA also tries to bridge the domain gap across different sources. This demonstrates the necessity and superiority of multi-source DA over single-source DA.

(4) MADAN achieves the best average results among all adaptation methods, benefiting from the joint consideration of pixel-level and feature-level alignments, cycle-consistency, dynamic semantic consistency, domain aggregation, and multiple sources. MADAN also significantly outperforms source-combined DA, in which domain shift also exists among different sources. By bridging this gap, multi-source DA can boost the adaptation performance. On the one hand, compared to single-source DA like CyCADA (Hoffman et al. 2018b), MADAN utilizes more useful information from multiple sources. On the other hand, other multi-source DA methods (Xu et al. 2018; Zhao et al. 2018; Peng et al. 2019; Zhao et al. 2020b) only consider feature-level alignment, which is obviously insufficient especially for fine-grained tasks, e.g. semantic segmentation, a pixel-wise prediction task. In addition, we consider pixel-level alignment with a dynamic semantic consistency loss and further aggregate different adapted domains.

(5) Take segmentation segmentation for example, the oracle method that is trained on the target domain performs significantly better than the others. However, to train this model, the ground truth labels from the target domain are required, which are actually unavailable in UDA settings. We can deem this performance as an upper bound of UDA. Obviously, there is still a large performance gap between all adaptation algorithms and the oracle method, requiring further efforts on DA.

There are also some task-specific observations:

(1) Simply combining different source domains into one source and performing source-only or single-source DA does not guarantee better performance than corresponding single-best method. For example, for the source-only standard, the single-best method outperforms the combined method on Digits-five, Office-31, Office+Caltech-10 datasets, while the combined method performs better on Office-Home, Cityscapes, and BDDS datasets. For the single-source DA, we usually have opposite observations. For example, in Table 6, the mIoUs of CyCADA from GTA to Cityscapes and from SYNTHIA to Cityscapes are 38.7% and 29.2%, while the mIoU of source-combined DA is 37.3%. Currently, there is no accurate explanation on this observation. On the one hand, combining multiple sources into one source results in more training data, which can intuitively boost the performance. On the other hand, the data from different sources are collected from different distributions, which may interfere with each other. Therefore, the comparison between the single-best method and the combined method depends on which aspect is stronger.

(2) For semantic segmentation adaptation, MADAN+ outperforms MADAN with a remarkable margin. For example, the average performance gains of MADAN+ over MADAN using DeepLabV2 backbone are 3.1% and 2.3% on Cityscapges and BDDS, respectively. Further, MADAN+ achieves the best cwIoU scores of 6 to 9 out of 16 categories. These results demonstrate the superiority of MDAN+ over MADAN for pixel-wise segmentation adaptation with the help of category-level alignment and multi-scale image generation.

Segmentation Visualization. The qualitative semantic segmentation results are shown in Fig. 7. We can clearly see that after adaptation by the proposed method, the visual segmentation results are improved notably, which look more similar to the ground truth (b). Take the second row for example, the contours of pedestrians and cyclists by MADAN+ (i) are more clear than those by the methods of source only (c) and CycleGAN (d).

Ablation Study
To demonstrate the effectiveness of different components in the proposed MADAN and MADAN+ models, we conduct ablation studies on the segmentation adaptation tasks.

First, we compare the proposed dynamic semantic consistency (DSC) loss with the original semantic consistency (SC) loss (Hoffman et al. 2018b) using the DA methods of CycleGAN (Zhu et al. 2017) and CyCADA (Hoffman et al. 2018b). The results on Cityscapes and BDDS are shown in Tables 10 and  11, respectively. We can see that for all adaptation settings, DSC achieves better mIoU results than SC. For example, the mIoU improvements of DSC over SC in CycleGAN and CyCADA from GTA to Cityscapes are 5.4% and 1.3%, respectively, while the corresponding improvements are 3.2% and 2.6% from SYNTHIA to Cityscapes. These results demonstrate the effectiveness of our proposed DSC loss.

Second, we incrementally evaluate the influence of different components in MADAN+. The results on Cityscapes and BDDS using FCN-VGG16 backbone are shown in Tables 12 and 13, respectively. We have several observations. (1) Both domain aggregation methods, i.e. SAD and CCD, obtain larger mIoU scores than baseline with SAD performing better. The performance gains are obtained by making different adapted domains more closely aggregated. (2) Adding the DSC loss could further improve the segmentation performance, again demonstrating the effectiveness of DSC. (3) Feature-level alignment is also helpful with 1.2% and 1.0% improvements on Cityscapes and BDDS, respectively, obviously contributing to the adaptation task. (4) Category-level alignment (CLA) is complementary to the feature-level alignment (FLA). While FLA aims to align the target and source features globally, CLA makes the features in local regions indistinguishable. (5) Multi-scale image generation (MIG) significantly contributes to the adaptation task. (6) The modules are orthogonal to each other to some extent, since adding each one of them does not introduce performance degradation. (7) As compared to MADAN, MADAN+ achieves better results with 1.4% and 5.3% performance gains on Cityscapes and BDDS, respectively. Moreover, by adding CLA and MIG, the cwIoU of most categories are increased. These results demonstrate the superiority of MADAN+ over MADAN for pixel-wise adaptation.

Fig. 7
figure 7
Qualitative semantic segmentation result from GTA and SYNTHIA to Cityscapes. From left to right are: a original image, b ground truth annotation, c source only from GTA, d CycleGANs on GTA and SYNTHIA, e +CCD+DSC, f +SAD+DSC, g +CCD+SAD+DSC, h +CCD+SAD+DSC+FLA (MADAN), and i +CCD+SAD+DSC+FLA+CLA+MIG (MADAN+)

Full size image
Fig. 8
figure 8
The t-SNE (Maaten and Hinton 2008) visualization of the learned features for task a Digits-five: mm, up, sv, syâ†’mt and b Office-31: D, Wâ†’A. In each pair, the features are extracted using the last layer of source domain encoder from the samples of source and target domain in the first image, and the target domain features are extracted using the the last layer of adapted encoder in the second one. Red: source, blue: target (Color figure online)

Full size image
Fig. 9
figure 9
Visualization of image translation for classification adaptation. From left to right are: a Digits-five: mt, mm, sv, sy â†’ up, b Office-31: W, D â†’ A, c, Office+Caltech-10: D, C, A â†’ W d Office-Home: Ar, Rw, Pr â†’ Cl. Red: source, blue: target (Color figure online)

Full size image
Fig. 10
figure 10
Visualization of image translation for segmentation adaptation from GTA and SYNTHIA to Cityscapes. From left to right are: a original source image, b CycleGAN, c CycleGAN+DSC, d CycleGAN+CCD+DSC, e CycleGAN+SAD+DSC, f CycleGAN+CCD+SAD+DSC, g CycleGAN+CCD+SAD+MIG, and h target Cityscapes image. The top two rows and bottom rows are GTA â†’ Cityscapes and SYNTHIA â†’ Cityscapes, respectively

Full size image
Fig. 11
figure 11
Comparison of the attention maps before and after adaptation on a Office-31 and b Office-Home datasets. For each group, the five columns from left to right are: the original target image, attention map before adaptation, image with attention map before adaptation, attention map after adaptation, and image with attention map after adaptation. Red regions indicate more attention

Full size image
Model Interpretability
In this section, we show the modelsâ€™ interpretability by feature transferability, style translation, and attention visualization.

Feature Transferability. First, we visualize the features before and after adaptation with t-SNE embedding (Maaten and Hinton 2008) in two tasks: (a) Digits-five: mm, up, sv, syâ†’mt and (b) Office-31: D, Wâ†’A. As illustrated in Fig. 8, we can observe that after adaptation, the target domain is more indistinguishable from the source domains, which demonstrates that the proposed MADAN model can align the distributions between the source and target domains. Based on the more transferable features after adaptation, the task classifier learned on the source domains can work well on the target domain, leading high task performance on the target domain.

Style Translation. Second, we visualize the results of pixel-level alignment (PLA) before and after adaptation. Specifically, we show the comparison among source images, adapted images, and target images for classification and segmentation adaptation in Figs. 9 and  10, respectively. We can see that the styles of the adapted images by our PLA method are closer to the target than the source to the target. Meanwhile, the semantic information is well preserved. For classification in Fig. 9: (a) although styles of the source images are different, the corresponding adapted images are uniformly changed to the handwritten brush style of the target images; (b) the background is removed in the adapted images; (c) a desktop background is added to the adapted images; (d) the adapted images are cartooned to have similar styles to the target images. For segmentation in Fig. 10, comparing the columns from (a) to (g) with the column (h) especially (a) vs. (h) and (g) vs. (h), we can observe that with our final FLA method (g), the styles (e.g. overall hue and brightness) of the adapted images are much more similar to the target Cityscapes.

Attention Visualization. Finally, we visualize the attention before and after the proposed domain adaptation method using the heat map generated by the Grad-Cam algorithm (Selvaraju et al. 2017). The comparison before and after adaptation on Office-31 and Office-Home datasets are illustrated in Fig. 11. It is clear that different regions in the images have different attentions but the attentions generated by our domain adaptation method can focus more on the desirable and discriminative regions. For example, on the Office-31 dataset, for the image in the top right group, the calculator is highlighted with more attention after adaptation, while more attention is focused on a region in the background before adaptation; for the image in the bottom right group, after adaptation more attention is paid to the helmet and the attention diminishes for the complex background with messy objects. On the Office-Home dataset, for the image in the top left group, the attention before adaptation focuses on the background and the edge of the speaker, while the more discriminative and transferable trumpets are emphasized after adaptation; for the image in the bottom right group, only the lens of the Webcam is highlighted after adaptation since it is more transferable than the base of the camera. These observations intuitively demonstrate that the attended regions by our adaptation model are invariant across different domains and discriminative for the learning task.

Discussions
Fig. 12
figure 12
The distribution of different categories in the four domains for semantic segmentation

Full size image
Computation Cost. Since the proposed framework deals with a harder problem, i.e. multi-source domain adaptation, more modules are used to align different sources, which results in a larger model. In our segmentation adaptation experiments, MADAN is trained on 4 NVIDIA Tesla P40 GPUs for 40 hours using two source domains which is about twice the training time as on a single source. However, MADAN does not introduce any additional computation during inference, which is the biggest concern in real industrial applications, e.g. autonomous driving.

Application and Generalization. The proposed MADAN and MADAN+ models work under multi-source, unsupervised, homogeneous, and closed set settings. There exists obvious domain gap between different domains in the employed datasets. For example, in Digits-five, there are handwritten digits, street digits, and synthetic digits; in Office-Home, the objects range from artistic images, clip-art images to product images and real-world images. We give detailed comparisons of different domains for the simulation-to-real segmentation adaptation. The distribution of different categories in the four domains are shown in Fig. 12. We can see clear distribution difference across domains. Specifically, GTA (Richter et al. 2016) is collected from a simulation environment. The driving conditions are pretty diverse, including both city and countryside. The images in GTA are of very high fidelity graphics and are all collected from front dash cameras. SYNTHIA (Ros et al. 2016) is collected from a simulation environment. The driving conditions are mostly in cities. The images in SYNTHIA do not have very high fidelity and are taken from cameras of various angles and heights. Cityscapes (Cordts et al. 2016) is collected in real-world environments. All images are collected from the front cameras of vehicles driving in European cities. BDDS (Yu et al. 2018) is collected in populous areas in the US with front cameras in driving vehicles. The Driving environments are more diverse than Cityscapes, e.g. more diverse weather and times of day. The experiments on MDA for digit recognition, object classification, and semantic segmentation demonstrate the effectiveness and superiority of the proposed models in various practical applications.

We admit that to achieve good performances, we employ different alignment strategies, which result in complicated models with multiple losses to optimize. The training is also computationally expensive. These are the weaknesses of the proposed method. We leave improving the computational efficiency as our future work. To generalize the proposed models to new real-world applications, we release the source codes with step-by-step instructions.

On Different Implementations of DSC. The effectiveness of the proposed DSC has been demonstrated in Sect. 5.3. The motivation of the DSC design, i.e. minimizing the KL divergence between the outputs of ğ¹ğ´ and ğ¹ğ‘–, is described in Sect. 4.1. Another intuitive implementation of DSC is to minimize the mismatch between the ground truth ğ‘Œğ‘– of domain i and ğ¹ğ´ (e.g., with cross-entropy loss). To compare these two implementations, we take the adaptation from GTA and SYNTHIA to Cityscapes using FCN-VGG16 backbone as an example. The class-wise IoU and mIoU of MADAN+ in Table 6 using KL divergence-based DSC are 87.9, 41.0, 76.4, 21.4, 1.3, 28.4, 20.3, 22.3, 77.3, 80.0, 54.9, 21.5, 80.1, 29.7, 15.1, 26.5, 42.8 (mIoU). The results of MADAN+ using cross-entropy loss-based DSC are 88.9, 39.0, 75.9, 19.7, 0.7, 24.4, 22.5, 25.7, 70.5, 69.4, 52.7, 20.6, 78.9, 30.2, 17.4, 28.9, 41.6 (mIoU). It is clear that our KL divergence-based DSC outperforms cross-entropy loss-based DSC. We have the following observations. In the beginning, the effect of image generation is not excellent, and the generated images will be biased towards the source domain. Therefore, if the learning target of ğ¹ğ´ is ğ‘Œğ‘–, the gradient will make the model more difficult to translate from the source domain to the target domain. Using the hard-coding label ğ‘Œğ‘– makes it harder to learn well, while using a soft-coding label ğ¹ğ‘–(ğ‘¥ğ‘–) makes the training easier to converge since it tries to mimic the behavior of ğ¹ğ‘–. So we prefer to use ğ¹ğ‘–(ğ‘¥ğ‘–) to generate a reference tag rather than relying entirely on ğ‘Œğ‘–.

On the End-to-end Training. Similar to CyCADA (Hoffman et al. 2018b), the proposed MADAN and MADAN+ are end-to-end trainable based on Eqs. (10) and  (17). Due to constrained hardware resources in practice, such as GPU memory, we train the models in three stages as described in Sect. 5.1.4. We need to mention that end-to-end training can obtain similar results as multi-stage training. For example, on Office-Home dataset (Venkateswara et al. 2017), the classification accuracy on Rw, Pr, Cl, Ar, and the average accuracy are 81.5, 78.2, 54.9, 66.8, 70.4 for multi-stage training and 79.4, 80.6, 53.1, 67.2, 70.1 for end-to-end training. Such slight fluctuation is normal and acceptable in deep learning-based model training. Besides the hardware constraints, there are some other concerns that motivate us to employ multi-stage training. First, parameter tuning is difficult for end-to-end training which has to optimize more parameters simultaneously. Second, at the beginning of end-to-end training, the generated images are of low quality, leading to a poor task model, which in turn affects the image generation. The final convergence depends heavily on the modelâ€™s initialization.

On the Poorly Performing Classes in Segmentation. There are two main reasons for the poor performance on certain classes (e.g. fence and pole): 1) lack of images containing these classes and 2) structural differences of objects between simulation images and real images (e.g. the trees in simulation images are much taller than those in real images). Generating more images for different classes and improving the diversity of objects in the simulation environment are two promising directions for us to explore in future work that may help with these problems.

Conclusion
In this paper, we proposed a novel framework, termed Multi-source Adversarial Domain Aggregation Network (MADAN), for multi-source domain adaptation (MDA). For each source domain, based on cycle-consistent GAN at pixel-level alignment, we first generated adapted images with a novel dynamic semantic consistency loss. Further, we proposed a sub-domain aggregation discriminator and cross-domain cycle discriminator to better aggregate different adapted domains. Finally, we trained the task model using the adapted images in the aggregated domain and corresponding labels in the source domains. The experiments showed that MADAN achieves 2.8%, 3.0%, 2.2%, and 4.6% classification accuracy improvements compared with the existing best MDA methods, respectively on Digits-five, Office-31, Office+Caltech-10, and Office-Home datasets. We also studied MDA for semantic segmentation, which is the first work on adapting pixel-wise prediction task with multiple sources. To better deal with the pixel-wise adaptation, we extended MDAN to MADAN+ with category-level alignment and multi-scale image generation. For the FCN-VGG16 backbone, MADAN+ achieves 17.0%, 3.0%, 5.5%, and 13.4% mIoU improvements compared with best source-only, best single-source DA, source-combined DA, and other multi-source DA, respectively on Cityscapes from GTA and SYNTHIA, and 17.0%, 5.9%, 7.9%, 16.6% on BDDS.

For future studies, we plan to investigate multi-modal DA, such as using both image and LiDAR data, to further boost the adaptation performance. Improving the computational efficiency of MADAN, with techniques such as neural architecture search, is another direction worth investigating. In addition, we will study how to automatically weigh the relative importance of different sources and the samples in each source to further improve the performance of MADAN.