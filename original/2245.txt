Recent image-to-image (Image-to-Image) translation algorithms focus on learning the mapping from a source to a target domain. However, the continuous translation problem that synthesizes intermediate results between two domains has not been well-studied in the literature. Generating a smooth sequence of intermediate results bridges the gap of two different domains, facilitating the morphing effect across domains. Existing Image-to-Image approaches are limited to either intra-domain or deterministic inter-domain continuous translation. In this work, we present an effectively signed attribute vector, which enables continuous translation on diverse mapping paths across various domains. In particular, we introduce a unified attribute space shared by all domains that utilize the sign operation to encode the domain information, thereby allowing the interpolation on attribute vectors of different domains. To enhance the visual quality of continuous translation results, we generate a trajectory between two sign-symmetrical attribute vectors and leverage the domain information of the interpolated results along the trajectory for adversarial training. We evaluate the proposed method on a wide range of Image-to-Image translation tasks. Both qualitative and quantitative results demonstrate that the proposed framework generates more high-quality continuous translation results against the state-of-the-art methods.
Introduction
Image-to-Image (Image-to-Image) translation (Isola et al. 2017) aims to learn the mapping function between different visual domains. It can be applied to a wide range of tasks such as semantic image synthesis (Wang et al. 2018; Park et al. 2019), photo enhancement (Zhu et al. 2017), style transfer (Lee et al. 2020; Gong et al. 2019), season transfer (Huang et al. 2018; Lee et al. 2018; Yu et al. 2019), domain adaptation (Hoffman et al. 2018; Chen et al. 2019), and object transfiguration (Wu et al. 2019; Choi et al. 2018; Liu et al. 2019; Choi et al. 2020). Given an image of the source domain, we can render it into an image with the target domain’s style. However, it remains challenging to generate smooth and continuous translated images for existing Image-to-Image methods. As shown in Fig. 1, continuous translation enables applications such as image morphing. Furthermore, modeling the intermediate results facilitates a better understanding of the translation process between the two domains.

Fig. 1
figure 1
Continuous and diverse translation across domains. Our approach performs continuous and diverse Image-to-Image translation results. On each row, we show continuous interpolation from the source image to the style of the target image, including summer → winter translation, photo → Monet translation, male → female translation, and cat → dog translation. Green and blue bounding boxes denote generated images of the source and target domains, respectively (Color figure online)

Full size image
Existing Image-to-Image translation approaches mainly address multi-modal (Zhu et al. 2017; Huang et al. 2018; Lee et al. 2018), multi-domain (Choi et al. 2018; Anoosheh et al. 2018; Liu et al. 2019) translation, or both (Yu et al. 2019; Lee et al. 2020; Choi et al. 2020). However, these methods are not effective in rendering continuous translated images across domains. While recent translation frameworks (Zhu et al. 2017; Huang et al. 2018; Lee et al. 2018, 2020; Choi et al. 2020) based on domain-specific attribute (style) space can generate continuous translation by performing linear interpolation between different attribute vectors, such schemes are limited to intra-domain due to separate attribute spaces of different domains. To handle continuous Image-to-Image translation, the DLOW model (Gong et al. 2019) generates intermediate results by taking an additional interpolated domain label as input. However, this method can only produce a deterministic translation path given an image and an interpolated domain label.

In this paper, we focus on continuous image-to-image translation and generate diverse translation paths (multi-modal) across various visual domains (multi-domain) in a single model. There exist two challenges. First, previous approaches such as MUNIT (Huang et al. 2018) and DRIT (Lee et al. 2018) adopt separate attribute spaces for different domains, thus cannot model the continuous variation across domains. Second, there are no ground-truth intermediate samples for learning continuous translation results in-between two domains.

To address the issues mentioned above, we propose a novel Image-to-Image translation framework based on signed attribute vectors (SAVs). We disentangle images into the content and the attribute representations extracted by a content encoder and an attribute encoder. To enable inter-domain continuous translation, we introduce a unified attribute space containing domain-specific attributes of all domains. We consider each attribute dimension from the prior Gaussian distribution as independent and identically distributed random variables and draw samples. Then, the sign operation is applied to make the values of the attributes in a particular domain positive and those in other domains negative. The proposed SAVs and the content representations are fed into the generator to synthesize corresponding domain images. Furthermore, we adopt the maximum mean discrepancy (MMD) constraint to align the distribution of SAVs with the attribute encoder embedding distribution. Thus, the attribute representation can either be sampled from the signed attribute space or extracted from the given reference image.

There are two advantages of the proposed method. First, it facilitates continuous translation across various domains within a unified attribute space. Second, owing to the sign information, we propose a translation trajectory between the SAV of the source domain and its sign-symmetrical attribute vector of the target domain. We leverage the domain information on interpolated results along the trajectory during the training and apply the adversarial loss to enhance the quality. Figure 1 shows the effectiveness of the proposed method for generating continuous and diverse translation results across domains.

The main contributions of this work are summarized as follows:

We propose a simple yet effective SAV-based Image-to-Image framework to construct a unified attribute space for all domains, enabling continuous and diverse translation paths across various visual domains.

We design a sign-symmetrical operation to create a translation trajectory between two domains during training. By leveraging the domain information of intermediate results along the trajectory, we apply the adversarial loss to enhance the quality.

Extensive experiments validate that the proposed method can synthesize continuous and diverse translation results on a wide range of Image-to-Image translation tasks. Qualitative and quantitative results demonstrate that the proposed framework generates more high-quality continuous translation results against the state-of-the-art approaches.

Related Work
Image-to-Image Translation
Image-to-Image (Image-to-Image) translation (Isola et al. 2017) aims to learn the mapping of images between different domains. Isola et al. (2017) propose Pix2Pix to address the problem with paired data. Numerous Image-to-Image methods (Zhu et al. 2017; Liu et al. 2017; Kim et al. 2017) exploit the idea of cycle-consistency to train the model with unpaired data. However, these approaches can only perform one-to-one (i.e., uni-modal) mapping translation.

Recent methods achieve one-to-many mapping from different perspectives: multi-modal (Zhu et al. 2017; Huang et al. 2018; Lee et al. 2018), multi-domain (Choi et al. 2018; Anoosheh et al. 2018; Liu et al. 2019) translation, or both (Yu et al. 2019; Lee et al. 2020; Choi et al. 2020). Nevertheless, these schemes focus on generating images of the target distribution, ignoring the continuous translation process that produces intermediate results across domains.

Several recent approaches aim to model the intermediate state between two domains for continuous translation. The DLOW method (Gong et al. 2019) introduces an additional interpolated domain variable, which facilitates mapping the source image to the intermediate domain. Lira et al. (2020) design multi-hops in one generator to gradually transform the source image to the target domain. However, both approaches are built upon the uni-modal translation model Cycle-GAN (Zhu et al. 2017). The generator only synthesizes one deterministic continuous translation path to the target domain for a source image.

To achieve multi-modality, MUNIT (Huang et al. 2018) and DRIT (Lee et al. 2018) disentangle images into domain-invariant content representations and domain-specific attribute representations. However, due to the separate construction of domain-specific attribute space of different domains, they can only perform continuous interpolation within the intra-domain. Several schemes, such as DRIT++ (Lee et al. 2020), DMIT (Yu et al. 2019), and StarGAN-v2 (Choi et al. 2020), integrate the attribute (style) representations and domain labels in a single framework for multi-modal and multi-domain translations. Nevertheless, they still do not perform well in continuous translation across multiple domains. The attribute encoder and the generator of DRIT++ (Lee et al. 2020) require an additional domain label as input, which prevents interpolation on attribute vectors from different domains. DMIT (Yu et al. 2019) disentangles images into the domain-invariant content space, the domain-shared style space, and the domain-specific attribute space represented by domain labels. Directly interpolating vectors of domain-shared style space does not lead to continuous translations from the source domain to the target one. Despite that StarGAN-v2 (Choi et al. 2020) introduces the unified style encoder to encode style information of different domains, the design of multiple embedding branches still separates attribute space of different domains. Applying interpolation straightforwardly to style vectors of different domains does not generate smooth translated images, as shown in our experimental results. In contrast, we present a unified attribute representation that contains domain-specific attributes from all domains. The SAVs are then proposed utilizing the sign operation to embed the domain information, facilitating continuous translation on diverse mapping paths across various domains.

Table 1 Comparisons of Image-to-Image translation networks
Full size table
Numerous few-shot Image-to-Image translation methods (Liu et al. 2019; Saito et al. 2020), e.g., FUNIT (Liu et al. 2019), construct a unified class encoder to learn a class-specific style code for different classes. These approaches can achieve continuous translation by interpolating two style codes of different classes. However, without explicit constraint on the latent space, we note that the interpolated results depend on the number of translation classes and the batch size during training, as shown in our experiments. In contrast, owing to the SAVs, we create a translation trajectory between two domains using two sign-symmetrical attribute vectors. By leveraging the domain information of intermediate results along the trajectory, the adversarial loss can further enhance interpolated images. Table 1 summarizes the differences between recent unsupervised Image-to-Image translation frameworks.

Image Synthesis by Varying Attributes
Numerous image synthesis methods have been developed by varying the underlying attributes. For style transfer, Kotovenko et al. (2019) vary the art style of input photo from one artist to another, e.g., from Cezanne to Van Gogh, for continuous image synthesis. For head reenactment, Burkov et al. (2020) learn the latent pose representation and synthesize images with varying views by interpolating two pose vectors.

Several facial attributes continuous manipulation approaches can continuously manipulate one specific attribute, such as the smile attribute on face images, by varying the value of the annotated attribute label. The Fader Networks (Lample et al. 2017) method uses binary attribute labels for training. The model can then be generalized to use continuous float values to manipulate the specific facial attribute strength during inference. The RelGAN (Wu et al. 2019) scheme proposes a relative-attribute-based approach for facial attribute editing. Guided by an interpolation discriminator, the framework learns to interpolate a specific attribute during training, leading to a smoother continuous facial attribute manipulation. Instead of linearly interpolating the annotated attribute labels, the recent approach HomoInterpGAN (Chen et al. 2019) embeds images into latent representations by an encoder and learns an interpolator network to interpolate the latent features. The interpolator network is then trained with a homomorphic loss to manipulate latent representations consistent with annotated attributes. However, since these approaches require fine-grained binary annotations during the training stage, they are not directly applicable to the continuous Image-to-Image task, which only provides few domain labels. Recently, several approaches explore interpretable attributes in the latent space of pre-trained generative adversarial network (GAN) models (Shen et al. 2020; Voynov and Babenko 2020) (i.e., GAN inversion) for image synthesis.

In contrast to existing methods, we aim to continuously manipulate the domain-specific attributes from the source to the target domains. The proposed framework learns the domain-specific attributes directly from the data for Image-to-Image translation.

Image Morphing
Image morphing (Wolberg 1998) aims to change from an image to another through a seamless transition. Existing approaches (Wolberg 1998) usually accomplish the task via multiple steps: determining the corresponding mapping between the images on specific feature space (Liao et al. 2014), applying a 2D geometric transformation to warp the image for retaining geometric alignment on the feature space, interpolating the color space to blend the texture.

With the advances of GANs, numerous methods (Abdal et al. 2019, 2020) manipulate the latent space of state-of-the-arts GAN models such as StyleGAN (Abdal et al. 2019), StyleGAN-v2 (Abdal et al. 2020), and BigGAN (Brock et al. 2018), which have been demonstrated to be effective for image morphing. In particular, they project images into these pre-trained GAN models’ latent space and then perform the morphing effect by interpolating latent vectors of different images.

Unlike interpolating latent features embedded by pre-trained GANs, we learn to disentangle images into the content and attribute representations. By interpolating attribute vectors of different domains, we generate morphing effects to the target domain while preserving the source image’s content.

Fig. 2
figure 2
Overview. a An image is disentangled into the content and attribute representations by a unified content encoder 𝐸𝑐 and a unified attribute encoder 𝐸𝑎. b We present the male → female translation and design a unified attribute space containing attributes of all domains. The sign information is incorporated into the attribute vector to describe domain membership. Given a target domain 𝑦̂ , we enforce the domain-specific attributes 𝐳𝑗(𝑗=𝑦̂ ) positive and others 𝐳𝑗(𝑗≠𝑦̂ ) negative, which forms the SAVs (Sect. 3.1). On the other hand, we extract the attribute vector from the reference image of the domain 𝑦̂ , whose distribution is aligned with the SAVs of the domain 𝑦̂  using the MMD constraint. Finally, G synthesizes the translated images using the content representation extracted from the source image and the target attribute vector. c In the testing phase, we extract the target attribute vector of a reference image (case 1: reference-guided) or randomly generate an SAV of the target domain (case 2: latent-guided). The continuous translation is realized by interpolating the extracted attribute vector of the source image and the target attribute vector

Full size image
Proposed Method
Our goal is to learn continuous and diverse Image-to-Image translation across visual domains while preserving the domain-invariant content. As illustrated in Fig. 2a, given an image 𝐱, we use a content encoder 𝐸𝑐 to obtain the domain-invariant content representation 𝐜, and a unified attribute encoder 𝐸𝑎 to extract the attribute vector 𝐳. We can then generate continuous translation by interpolating two different attribute vectors. However, existing approaches (Huang et al. 2018; Lee et al. 2018, 2020; Choi et al. 2020) are more effective for intra-domain interpolation due to separate attribute spaces for different domains. To enable inter-domain continuous translation, we propose a unified attribute space shared by all domains, denoted as

𝐳=[𝑧11,𝑧12,…,𝑧1𝑑,𝑧21,𝑧22,…,𝑧2𝑑,…,𝑧𝑁1,𝑧𝑁2,…,𝑧𝑁𝑑],𝐳∈ℝ𝑑⋅𝑁,
(1)
where d is the attribute vector’s dimension for each domain, and N represents the number of visual domains. In the following, we present a sign operation to encode the domain information into the unified attribute vector and the framework’s training strategies.

Signed Attribute Vectors
In this work, we encode the domain information into the unified attribute space. The prior of the translation between two domains lies in one domain that has some more prominent attributes than the other domain. Given a source image 𝐱 of the domain y, we assume that the attribute values corresponding to the domain y should be large, while those from other domains should be relatively small. For instance, the beard is usually longer on male faces compared to female faces. Therefore, we propose to use the sign operation to formulate this assumption.

Fig. 3
figure 3
Illustration of interpolation between two sign-symmetrical points. Using a 2D-plane, we show that the signed attribute vector contains attributes of two domains, and each domain has one attribute dimension. a The sign-symmetrical attribute vector is obtained by reversing the sign of the attributes in the source and target domains. b When 𝛽<0.5, the interpolated point lies in the source domain. c When 𝛽=0.5, the interpolated point is an intermediate state. d When 𝛽>0.5, the interpolated point belongs to the target domain

Full size image
First, we sample a vector 𝐳𝑝∈ℝ𝑑⋅𝑁 from a prior distribution. Each attribute dimension of the vector is i.i.d. sampled from the prior Gaussian distribution (0,1). For a domain label y, we use the sign operation to compute the SAVs:

𝐳𝑠=𝑦(𝐳𝑝)𝐳𝑝∽(0,𝐈),𝑦∈{1…𝑁}.
(2)
Specifically, the sign operation 𝑦 makes the attribute values {𝑧𝑗=𝑦𝑖}𝑑𝑖=1 of domain y positive, and those for other domains {𝑧𝑗≠𝑦𝑖}𝑑𝑖=1 negative by,

𝑦(𝐳𝑝)=[−|𝑧11|,−|𝑧12|,…,−|𝑧1𝑑|,…,+|𝑧𝑦1|,+|𝑧𝑦2|,…,+|𝑧𝑦𝑑|,…,−|𝑧𝑁1|,−|𝑧𝑁2|,…,−|𝑧𝑁𝑑|].
(3)
We show an example of the proposed sign operation on an attribute vector of target domain 𝑦̂  in the gray block of Fig. 2b. Then, the SAV 𝐳𝑠 can be applied to align the distribution of the unified attribute vector 𝐳 extracted by the attribute encoder 𝐸𝑎 using the MMD (Zhao et al. 2017) constraint:

MMD=𝔼𝑝(𝐳𝑠),𝑝(𝐳)[𝑘(𝐳𝑠,𝐳)]+𝔼𝑞(𝐳𝑠),𝑞(𝐳)[𝑘(𝐳𝑠,𝐳)]−2𝔼𝑝(𝐳𝑠),𝑞(𝐳)[𝑘(𝐳𝑠,𝐳)],
(4)
where k is the Gaussian kernel 𝑘(𝐳𝑠,𝐳)=𝑒−‖𝐳𝑠−𝐳‖22𝜎2.

As presented in Fig. 2b, the content representation extracted from the source image along with the attribute representation, which is either sampled by the signed attribute space or extracted from the reference target image, are fed into the generator to synthesize the translated images of target domain 𝑦̂ . We use a multi-task discriminator D (Mescheder et al. 2018; Liu et al. 2019; Choi et al. 2020) with multi-branch outputs for all domains during the training stage to ensure that the translated images belong to the corresponding domain. For target domain 𝑦̂ , the corresponding branch 𝐷𝑦̂  is learned by the domain adversarial loss,

domainadvor=𝔼𝐱,𝐱𝑦̂ [log𝐷𝑦̂ (𝐱𝑦̂ )+log(1−𝐷𝑦̂ (𝐺(𝐜,𝐳))],=𝔼𝐱,𝐱𝑦̂ ,𝐳𝑠[log𝐷𝑦̂ (𝐱𝑦̂ )+log(1−𝐷𝑦̂ (𝐺(𝐜,𝐳𝑠))],
(5)
where 𝐱𝑦̂  is the real image of target domain 𝑦̂ , 𝐜=𝐸𝑐(𝐱) is the content representation of the source image, and the attribute vector can be derived from 𝐳=𝐸𝑎(𝐱𝑦̂ ) or 𝐳𝑠=𝑦̂ (𝐳𝑝).

We embed only the sign information in the attribute vectors sampled from a prior distribution in this work. For the unified attribute vectors extracted by the attribute encoder 𝐸𝑎, the sign information is learned via the domain adversarial loss domainadv and the MMD constraint MMD. Furthermore, we adopt the style reconstruction loss (Johnson et al. 2016) to ensure the style consistency of translated images with reference images:

style=𝔼𝐱,𝐱𝑦̂ [‖Gram(𝜙(𝐺(𝐸𝑐(𝐱),𝐸𝑎(𝐱𝑦̂ )))−Gram(𝜙(𝐱𝑦̂ ))‖1],
(6)
where Gram is gram matrix, and 𝜙 is embedding feature space. In particular, we use ReLU3_1 VGG features.

With such a simple sign operation on SAVs, we construct a unified attribute space incorporating the domain information, thus allowing continuous translation across domains. Images are translated by interpolating the extracted attribute vector of a source image and a target attribute vector of the target domain. As illustrated in Fig. 2c, the target attribute vector can be obtained from a reference image (case 1: reference-guided) or a randomly sampled SAV of the target domain (case 2: latent-guided).

Improving Quality by Sign-Symmetrical Attribute Vectors
Thanks to the unified attribute space, we can interpolate the attribute vectors of two different domains. However, improving the quality of intermediate results remains a challenge. Due to the lack of real interpolation samples, we cannot directly apply the domain adversarial loss in Eq. 5 to ensure the interpolated images’ realism. Therefore, we propose two sign-symmetrical attribute vectors to create a continuous translation trajectory for conducting the domain adversarial training.

Given an SAV 𝐳𝑠 from source domain y, we reverse the attribute sign of source domain y and that of target domain 𝑦̂ . As an example shown in Fig. 3a, it produces a sign-symmetrical attribute vector 𝐳sym for the target domain. We formulate the process as

𝐳sym=𝑟(𝐳𝑠)=[−|𝑧11|,−|𝑧12|,…,−|𝑧1𝑑|,…,+|𝑧𝑦̂ 1|,+|𝑧𝑦̂ 2|,…,+|𝑧𝑦̂ 𝑑|,…,−|𝑧𝑦1|,−|𝑧𝑦2|,…,−|𝑧𝑦𝑑|,…,−|𝑧𝑁1|,−|𝑧𝑁2|,…,−|𝑧𝑁𝑑|].
(7)
For the translated image 𝐱̂  using the sign-symmetrical attribute vector 𝐳sym, we first apply the domain adversarial loss to assure its domain membership belongs to target domain 𝑦̂  and name it as the reverse sign domain adversarial loss rvsadv:

rvsadv=𝔼𝐱,𝐱𝑦̂ ,𝐳𝑠[log𝐷𝑦̂ (𝐱𝑦̂ )+log(1−𝐷𝑦̂ (𝐱̂ ))],
(8)
where 𝐱̂ =𝐺(𝐜,𝐳sym), and 𝐜=𝐸𝑐(𝐱).

Since traversing these two vectors 𝐳𝑠 and 𝐳sym forms a continuous translation trajectory across domains, we leverage this path during the training stage. Specifically, we sample the interpolation coefficient from a uniform distribution, i.e., 𝛽∈(0,1), and conduct linear interpolation on the two sign-symmetrical vectors 𝐳𝑠 and 𝐳sym in domain y and 𝑦̂ , respectively. The interpolated attribute vector can be formulated as 𝐳i=(1−𝛽)⋅𝐳𝑠+𝛽⋅𝐳sym. Combining a content representation 𝐜, we generate the interpolated translated result 𝐱i=𝐺(𝐜,𝐳i). Based on the sign information of the attribute vector, there exist three cases of 𝐱i:

1) When 𝛽<0.5, the interpolated attribute vector still locates in domain y, as illustrated in Fig. 3b. Therefore, the generated interpolated image belongs to domain y.

2) When 𝛽>0.5, the interpolate attribute vector lies in target domain 𝑦̂ , as demonstrated in Fig. 3d. At this time, the interpolated image belongs to domain 𝑦̂ .

3) When 𝛽=0.5, this is the intermediate state, as shown in Fig. 3c. In this case, we regard the interpolated result indistinguishable from neither domain y nor domain 𝑦̂ . Compared to 𝛽∈(0,0.5) and 𝛽∈(0.5,1), this point is much less sampled.

As a result, we can apply the domain adversarial loss to images generated along this trajectory to ensure the quality of continuous translation results, which is defined as the interpolated domain adversarial loss interpadv and summarized as follows

interpadv=⎧⎩⎨⎪⎪⎪⎪𝔼𝐱,𝐳𝑠,𝛽[log𝐷𝑦(𝐱)+log(1−𝐷𝑦(𝐱i))]𝔼𝐱,𝐱𝑦̂ ,𝐳𝑠,𝛽[12log𝐷𝑦(𝐱)+12log(1−𝐷𝑦(𝐱i))+12log𝐷𝑦̂ (𝐱𝑦̂ )+12log(1−𝐷𝑦̂ (𝐱i))]𝔼𝐱,𝐱𝑦̂ ,𝐳𝑠,𝛽[log𝐷𝑦̂ (𝐱𝑦̂ )+log(1−𝐷𝑦̂ (𝐱i))]𝛽<0.5,𝛽=0.5,𝛽>0.5.
(9)
Although we only exploit this specific interpolation path during training, the model is generalized to ensure interpolation results between any two attribute vectors at the inference stage.

Other Loss Objectives
In addition to the above loss functions, we also apply several loss objectives commonly used in Image-to-Image translation approaches to train the proposed model.

Content Adversarial Loss To further disentangle the content and attribute representations, we adopt the content discriminator 𝐷𝑐 (Lee et al. 2018, 2020; Yu et al. 2019) to distinguish the content representations belong to different domains. On the other hand, the content encoder 𝐸𝑐 aims to generate the content representations fool the content discriminator 𝐷𝑐. Then, the content adversarial loss is defined by

contentadv=𝔼𝐱,𝐱𝑦̂ [12log𝐷𝑐(𝐸𝑐(𝐱)+12log(1−𝐷𝑐(𝐸𝑐(𝐱))+12log𝐷𝑐(𝐸𝑐(𝐱𝑦̂ )+12log(1−𝐷𝑐(𝐸𝑐(𝐱𝑦̂ ))].
(10)
Cycle-Consistency Loss To preserve the consistency of domain-invariant characteristics of generated images, we impose the cycle-consistency loss (Choi et al. 2020; Zhu et al. 2017),

cc1or=𝔼𝐱,𝐱𝑦̂ [‖𝐱−𝐺(𝐸𝑐(𝐱̂ ),𝐸𝑎(𝐱))]‖1],=𝔼𝐱,𝐳𝑠[‖𝐱−𝐺(𝐸𝑐(𝐱̂ ),𝐸𝑎(𝐱))]‖1],
(11)
where 𝐱̂ =𝐺(𝐸𝑐(𝐱),𝐸𝑎(𝐱𝑦̂ )) or 𝐱̂ =𝐺(𝐸𝑐(𝐱),𝑦̂ (𝐳𝑝)).

Self-Reconstruction Loss We reconstruct the original image (Lee et al. 2018, 2020) using the encoded content representation and attribute representation as

recon1=𝔼𝐱[‖𝐺(𝐸𝑐(𝐱),𝐸𝑎(𝐱))−𝐱‖1].
(12)
Latent Regression LossZhu et al. (2017), Lee et al. (2018, 2020), Yu et al. (2019) and Choi et al. (2020) is adopted to further encourage the invertible mapping between generated images and the signed attribute space. We reconstruct the signed attribute vector 𝐳𝑠 as

latent1=𝔼𝐱,𝐳𝑠[|𝐸𝑎(𝐺(𝐸𝑐(𝐱),𝐳𝑠))−𝐳𝑠‖1].
(13)
Mode Seeking Loss To alleviate the mode collapse problem and improve the diversity of generated images. We introduce another SAV 𝐳𝑠2 to calculate the mode seeking loss (Mao et al. 2019) as

max ms=𝔼𝐱,𝐳𝑠1,𝐳𝑠2[‖𝐺(𝐸𝑐(𝐱),𝐳𝑠1)−𝐺(𝐸𝑐(𝐱),𝐳𝑠2)‖1‖𝐳𝑠1−𝐳𝑠2‖1].
(14)
The objective function of our framework is

min𝐺,𝐸𝑐,𝐸𝑎max𝐷,𝐷𝑐min𝐺,𝐸𝑐,𝐸𝑎𝜆contentadvcontentadv+𝜆domainadvdomainadv+𝜆rvsadvrvsadv+𝜆interpadvinterpadv,𝜆MMDMMD+𝜆stylestyle+𝜆cc1cc1+𝜆recon1recon1+𝜆latent1latent1+𝜆ms1ms,
(15)
where the term 𝜆∗ controls the importance of each loss function.

Implementation Details
The proposed model is implemented in Pytorch (Paszke et al. 2017), and the source code and pre-trained models are available at https://github.com/HelenMao/SAVImage-to-Image.

Datasets We evaluate the proposed method on four representative datasets, including the style translation and shape-variation translation tasks. For style translation, the Yosemite (Zhu et al. 2017) dataset includes the summer and the winter two domains. The Photo2Artwork dataset (Zhu et al. 2017) contains the photo, Monet, Van Gogh, and Ukiyo-e domains. For shape-variation translation, the CelebA-HQ (Karras et al. 2018) dataset in which we split the male and female domains for translation. The AFHQ (Choi et al. 2020) dataset consists of animal faces with the cat, dog, and wildlife domains.

Network Architecture The proposed model consists of a content encoder 𝐸𝑐, an attribute encoder 𝐸𝑎, a generator G, a discriminator D, and a content discriminator 𝐷𝑐. We set the size of an attribute vector to 𝐳∈ℝ8⋅𝑁, where N represents the number of visual domains in the dataset. To better fuse the attributes, we feed the attribute vectors into a fusing network F with three-layer MLP before feeding them into the generator. Since the style translation tasks require more content preservation than shape-variation translation tasks, we adopt different network architecture choices for these two tasks. More details on the network architectures can be found in Appendix B.

Training Process The resolution of each image is 256×256 pixels for all the experiments. We adopt the following hyper-parameters for the training in all the experiments: 𝜆contentadv=1, 𝜆domainadv=1, 𝜆rvsadv=1, 𝜆interpadv=1, 𝜆style=1, 𝜆cc1=10, 𝜆recon1=10, 𝜆latent1=10, 𝜆ms=1. For AFHQ, we employ 𝜆MMD=10, and 𝜆MMD=1 for other datasets. We use the batch size of 1 as well as the Adam (Kingma and Ba 2015) optimizer with a learning rate of 10−4 and exponential decay rates (𝛽1,𝛽2)=(0,0.99). We adopt the non-saturating adversarial loss (Goodfellow et al. 2014) with 𝑅1 regularization (Mescheder et al. 2018) using 𝛾=10 for style translation tasks, and 𝛾=1 for shape-variation translation tasks. All of the models are trained on two NVIDIA Tesla-P100 GPUs with 16GB memory.

Experiments
Continuous and Diverse Image-to-Image Translation
We present diverse continuous translation paths from the source domain to the target one. An input image 𝐼𝑠 in the source domain can be continuously translated to various images 𝐼𝑡1,𝐼𝑡2,…,𝐼𝑡𝑁 in the target domain. As illustrated in Fig. 2c, we first embed an image of the source domain to obtain the content representation and the source attribute vector. Then, we compute a target attribute vector by either extracting a reference image sampled from the target domain (reference-guided) or randomly generate an SAV of the target domain (latent-guided). We apply linear interpolation of the source and target attribute vectors to generate continuous translation results. Fig. 4 shows examples of reference-guided continuous and diverse translation results for two different scenarios: one with holistic views and the other one with dominant objects in the scenes. For the first scenario, the translation task is similar to style transfer, where the holistic views are considered (i.e., look and feel). For the second scenario, the translation task focuses on varying the dominant object’s shape and texture in each scene. Our model learns to continuously vary the source’s attributes to the target domains in both translation tasks. More results can be found in Appendix A.1.

Fig. 4
figure 4
Reference-guided continuous and diverse translation results. a Translation results on natural scenes where there are no dominant objects in the scenes. b Translations results on animals where one dominant object is in the scene. On each row, we show continuous translation results from the source to the target domains using the attribute vector extracted from a reference image. Green and blue bounding boxes denote generated images of the source and target domains, respectively (Color figure online)

Full size image
Comparisons with the State-of-the-arts
We present qualitative and quantitative evaluations with the state-of-the-art approaches on the CelebA-HQ and AFHQ datasets.

State-of-the-art Methods.
We evaluate the proposed method against the state-of-the-art models, including:

Image-to-Image translation approaches: StarGAN-v2 (Choi et al. 2020), FUNIT (Liu et al. 2019), and DLOW (Gong et al. 2019).

Attribute variation schemes: HomoInterpGAN (Chen et al. 2019), RelGAN (Wu et al. 2019), and Fader Networks (Lample et al. 2017).

For the Image-to-Image translation approaches, we adopt the pre-trained StarGAN-v2 model provided by the authors. Although the FUNIT model is originally designed to address the few-shot setting of Image-to-Image translation, we use the same training protocols to train the FUNIT model and adopt two settings for performance evaluation: training the model with the batch size of 1 (the same as ours) and the model with the batch size of 8. For multi-domain translation, the DLOW method mixes styles of multiple target domains and does not perform well. As such, we train three models on the AFHQ dataset for translation between any two domains. Instead of using fine-grained attribute annotations, we only use domain labels to train Image-to-Image models. For fair companions, we use the models and codes provided by their authors.

Evaluation Metrics
For quantitative evaluation, we use four widely-used metrics to assess interpolated images under different 𝛽 values:

Target domain translation accuracy (ACC) (Chen et al. 2019; Liu et al. 2019). We measure the percentage that interpolated images belong to the target domain, i.e., 

𝐴𝐶𝐶=∑𝑁𝑖=1𝛿[(𝐱𝑖)==𝑦𝑡𝑎𝑟𝑔𝑒𝑡]𝑁,
(16)
where N is the total number of interpolated images, 𝐱𝑖 is the i-th example of interpolated images, 𝑦𝑡𝑎𝑟𝑔𝑒𝑡 is the target domain label, (⋅) is a domain classifier that predicts domain label of 𝐱𝑖, and 𝛿(⋅) is a function that outputs 1 if predicted label equals to the target domain label, and 0 otherwise. We use the ResNet-50 binary classifier in our experiments.

Target domain Fréchet inception distance (FID) (Heusel et al. 2017).

We compute the FID score between interpolated images and real images of the target domain.

Domain-invariant perceptual distance (DIPD) (Liu et al. 2019). We calculate the 𝐿2 distance between two normalized VGG Conv5 (Simonyan and Zisserman 2015) features extracted from the interpolated image and the source image.

Learned perceptual image patch similarity (LPIPS) score (Zhang et al. 2018) between two adjacent interpolated images.

The target domain translation ACC and FID scores measure whether translated results successfully change into the target domain and the degree of image realism against the target domain. The DIPD score evaluates whether interpolated images preserve the domain-invariant features of source images. The variation of the LPIPS score between two adjacent interpolated images can be considered an indicator of translation smoothness.

Evaluation Protocol
We randomly select 500 examples from the test set of each domain for evaluation. The interpolated coefficient 𝛽 continuously takes the value at an interval of 0.1 from 0 to 1. When generating continuous results, the FUNIT (Liu et al. 2019) and HomoInterpGAN (Chen et al. 2019) methods use reference images in the target domain as guidance. The DLOW (Gong et al. 2019), RelGAN (Wu et al. 2019), and Fader Networks (Lample et al. 2017) interpolate discrete domain labels to generate intermediate results. The StarGAN-v2 and proposed schemes use target attribute vectors either extracted from reference images or randomly sampled from the target domain’s latent space for interpolation.

We feed the source and target image into the class encoder of the FUNIT method (Liu et al. 2019) and interpolate the class codes according to the interpolated coefficients to generate interpolated results. The HomoInterpGAN scheme (Chen et al. 2019) embeds the source and target image into the unified latent feature space by the encoder. We adjust the value of the control vector 𝐯∈[0,1]𝑐×1 of the domain branch to control the interpolation results. For the StarGAN-v2 model, we feed the source and target image into the style encoder (Choi et al. 2020) and obtain style vectors from the source and target domain branches. Then, we apply linear interpolation on these two style vectors. In addition, we can randomly sample a latent vector from the Gaussian distribution and feed it into the mapping network to acquire the target domain’s style vector from the corresponding domain branch (Choi et al. 2020). We then apply interpolation between source and target style vectors. The DLOW method (Gong et al. 2019) uses interpolated domain labels as additional inputs. For the RelGAN approach (Wu et al. 2019), we construct relative-attribute-vectors regarding domain labels and multiply it with interpolated coefficients. The Fader Networks scheme generates attributes based on domain labels (Lample et al. 2017) and applies linear interpolation between the source and target domain attributes.

Fig. 5
figure 5
Translation from male → female images. a Translation based on reference images (top); Translation based on interpolated domain labels or randomly sampled latent vectors (bottom). Green and blue bounding boxes denote generated images of the source and target domain, respectively. b From left to right, the y-axis of each sub-figure are target domain translation ACC (the larger, the better), target domain FID (the smaller, the better), DIPD (the smaller, the better), and LPIPS between two adjacent interpolated images (the smaller, the better). Each curve is plotted under different 𝛽 values. Solid lines indicate methods using reference images. Dash lines denote approaches using interpolated domain labels or randomly sampled latent vectors of the target domain (Color figure online)

Full size image
Fig. 6
figure 6
Translation from wildlife → cat images. a Translation based on reference images (top); Translation based on interpolated domain labels or randomly sampled latent vectors (bottom). Green and blue bounding boxes denote generated images of the source and target domain, respectively. b From left to right, the y-axis of each sub-figure are target domain translation ACC (the larger, the better), target domain FID (the smaller, the better), DIPD (the smaller, the better), and LPIPS between two adjacent interpolated images (the smaller, the better). Each curve is plotted under different 𝛽 values. Solid lines indicate methods using reference images. Dash lines denote approaches using interpolated domain labels or randomly sampled latent vectors of the target domain (Color figure online)

Full size image
Table 2 User preference scores
Full size table
Experimental Results Analysis
Figures 5a and 6a show translation results by the evaluated methods. In Figs. 5b and 6b, we compute the target domain translation ACC and FID values using all interpolated images and calculate the average DIPD and LPIPS scores of all interpolated images at each 𝛽 value to plot the curves “ACC vs. 𝛽”, “FID vs. 𝛽”, “DIPD vs. 𝛽”, and “LPIPS vs. 𝛽”. In these plots, “Ours-ref” and “StarGAN-v2-ref” indicate continuous translation using target attribute vectors extracted from reference exemplars; “Ours-rdm” and “StarGAN-v2-rdm” represent continuous translation using latent vectors randomly sampled from the target domain; “FUNIT-bs1” as well as “FUNIT-bs8” denote the FUNIT model trained with the batch size of 1 and 8.

Image-to-Image Translation Approaches The StarGAN-v2 method does not generate intermediate results well due to the separate attribute spaces for different domains. The curves of “DIPD vs. 𝛽” and “LPIPS vs. 𝛽” thus have an abrupt mutation and form a peak when 𝛽 is 0.6, as described in Figs. 5b and 6b. In contrast, our method can generate smooth intermediate results across domains with the proposed SAV.

For Image-to-Image translation of face images, the FUNIT model cannot capture target images’ style well with the class code when trained with only two classes, as illustrated in Fig. 5a. Thus, Fig. 5b demonstrates that the target domain translation ACC and FID values do not perform as well as the proposed model. The “FID vs. 𝛽” curve in Fig. 5b illustrates that the interpolated images of the FUNIT-bs8 method with a large batch size have higher realism scores. However, without any explicit constraint in the latent space, the FUNIT-bs8 model does not generate a smooth transition, as shown in Fig. 5a. The FUNIT model achieves better continuous interpolation in animal faces translation when more training classes are used. Nevertheless, Fig. 6b shows that the proposed model performs better against it in terms of ACC and FID scores.

The DLOW method can translate images to the target domains (the ACC score is larger than 50% when 𝛽 is larger than 0.5). However, the translated results contain only local variations (e.g., makeup) and do not exhibit the hairstyle in Fig. 5a. The ACC and FID scores achieved by the DLOW method are lower than those by the proposed model, as shown in Figs. 5b and 6b.

Attribute Variation Schemes The synthesized images by the RelGAN approach do not undergo smooth transition. For example, the leopard (𝛽=0.3) changes into a cat (𝛽=0.4) in Fig. 6a. The spike (𝛽=0.4) of the curve in Fig. 6b “LPIPS vs. 𝛽” also shows the RelGAN approach is not able to translate images smoothly. On the other hand, although the HomoInterpGAN and Fader Networks nearly have no variation in the curve of “LPIPS vs. 𝛽”, they cannot continuously translate the source image into the target domain (i.e., the ACC score is less than 50% when 𝛽 is larger than 0.5), as shown in Figs. 5b and 6b.

Overall, our approach synthesizes images with desired attributes such as hairstyle, makeup, and skin-color in the male → female translation. The proposed method achieves the best ACC and FID scores when 𝛽 is larger than 0.5 among all the evaluated schemes, as shown in Figs. 5b and 6b. In terms of smooth Image-to-Image translation, the LPIPS scores increase and decrease steadily. Unlike the StarGAN-v2 and RelGAN methods, the curves of LIPIS scores by the proposed method do not contain any spikes. Furthermore, the trends of curves in “ACC vs. 𝛽”, “FID vs. 𝛽”, and “DIPD vs. 𝛽” are consistent without any abrupt changes when increasing the 𝛽 value. These results demonstrate that the proposed model can accomplish both high-quality interpolation and smooth transition.

Fig. 7
figure 7
Ablation study on the male → female translation. a Green and blue bounding boxes denote generated images of the source and target domain, respectively. b The x-axis is the 𝛽. From left to the right, the y-axis in each sub-figure are target domain translation ACC (the larger, the better), target domain FID (the smaller, the better), DIPD (the smaller, the better), and LPIPS between two adjacent interpolated images (the smaller, the better), respectively (Color figure online)

Full size image
User Study
We evaluate the user preference among the proposed method and the state-of-the-art approaches through pairwise comparisons on the CelebA-HQ and AFHQ datasets. For each test, we present a source image and two videos with translation images by the proposed and other methods and some examples of the target domain. To better present intermediate translated results, we generate videos by repeating each interpolated image five times. We then ask three questions for each test: (1) Which model translates images better (both in the transition and the end) in terms of realism? (2) Which method generates smoother translated images? (3) Overall, which approach performs better?

For quality control, the survey randomly selects two source images for two directions (i.e., Male → Female and Female → Male) of the CelebA-HQ dataset, and two source images for three translation directions (i.e., Cat → Dog, Dog → Wildlife, and Wildlife → Cat) of the AFHQ dataset for each evaluated approach. Therefore, with eight methods for evaluation, there are 32 and 48 tests of each dataset shown to each participant in total. We collect the answers from 20 participants.

Table 2 shows that most participants prefer the results generated by the proposed method (from 83.75𝑡𝑜97.50% on the CelebA-HQ dataset and 64.17% to 98.33% on the AFHQ dataset) than those by all the other evaluated approaches. For image realism, we analyze that users prefer more considerable variation, e.g., exhibiting hairstyle variation in the male ⇌female translation. Although we ask users to pay attention to the intermediate results, we find that participants are still more concerned with the final translated effects than the intermediate results. Thus, the gap between StarGAN-v2 (or FUNIT) and the proposed method is smaller than that using the interpolated domain label for interpolation. When judging the translated images in terms of smoothness, it is difficult for participants to ignore the influence of other factors such as translation efficiency. For example, Figs. 5 and 6 show that the HomoInterpGAN and Fader Networks methods can translate images with a smooth transition but not to the target domain. However, most subjects still prefer the proposed method in terms of smooth translation, as shown in Table 2.

Ablation Studies
To better understand each component’s effectiveness in the proposed method, we present the ablation studies on the male → female translation in Fig. 7. We first analyze three proposed components: SAV, reverse sign domain adversarial loss rvsadv, and interpolated domain adversarial loss interpadv. Then, we demonstrate the efficiency of the style reconstruction loss style, the fusing network F, and the MMD constraint mmd. More ablation studies on other loss objectives can be found in Appendix A.5.

Proposed Components Both quantitative and qualitative results demonstrate that the proposed SAV plays an essential role in the continuous translation across domains. Without the sign operation, all interpolation results belong to the female domain, as illustrated in Fig. 7a. Figure 7b shows that, compared to the final model, most translated images are classified to the female domain when 𝛽<0.5 (ACC >50%). Since the dataset contains more female images than male images in the training set (17K vs. 9K), the model tends to learn more female domain attributes without the sign information embedding. The rvsadv ensures the domain-membership of the translated images using the sign-symmetrical attribute vector. As shown in Fig. 7a, without rvsadv, the translated target image cannot preserve the pose of the source image well. In particular, the pose of interpolated faces images varies from right to left. Thus, it achieves the highest DIPD scores when 𝛽>0.5 and most variation in the LPIPS score. When the model is trained without interpadv, it achieves the third-highest FID score when 𝛽>0.5 and second-largest variation in the LPIPS score. Furthermore, Fig. 7 shows that it does not generate the intermediate results between two domains well. Therefore, applying interpadv of interpolated results on the trajectory between sign-symmetrical attribute vectors is essential to improve the quality and smoothness.

Style Reconstruction Loss and Fusing Network We observe that training without style cannot capture the style of the reference image for the translated image, as presented in Fig. 7a. Therefore, it achieves the highest FID values when 𝛽>0.5. Feeding the attribute vector into a fusing network before the generator enhances the quality of interpolated results, as shown in the “FID vs. 𝛽” curve of Fig. 7b.

Fig. 8
figure 8
Ablation study on MMD with different values of 𝜆MMD on the Cat → wildlife translation. Green and blue bounding boxes denote generated images of the source and target domain, respectively (Color figure online)

Full size image
MMD Constraint The variation of the “LPIPS vs. 𝛽” curve from the model trained without MMD in the male → female translation is smoother than that by the final model. The translation ACC, FID as well as DIPD scores also have comparable performance against the final model. However, the translated images do not capture the style of lions in the AFHQ dataset when the weighting parameter of 𝜆MMD is small, as shown in Fig. 8. Since the wildlife domain contains numerous species such as the lion, tiger, fox, and wolf, the attribute vector extracted by 𝐸𝑎 can better embed the sign information and further represent the style information of target images by aligning the distribution under the constraint of MMD. Thus, we use this constraint in the final model.

Fig. 9
figure 9
Visualization of the latent representations of two domains on CelebA-HQ dataset using t-SNE. a Each data point is a content representation encoded from an image of that domain. b Each data point is an attribute representation encoded from an image of that domain. We also show interpolated attribute representations at 𝛽=0.5

Full size image
Discussion
Visualization of latent space Figure 9 visualizes the distributions of the latent representations from two domains on the CelebA-HQ dataset using t-SNE. As shown in Fig. 9a, the content representations of the two domains are aligned well. In comparison, Fig. 9b demonstrates that the attribute representations of two domains are separate from each other. Meanwhile, the interpolated attribute representations at 𝛽=0.5 can smoothly bridge the attribute representations of the two domains.

Applicability Analysis We conduct continuous translation on facial expression to demonstrate the proposed method’s applicability on “shape-like” attributes manipulation. Similar to Chen et al. (2019), we split the CelebA-HQ dataset into two domains using attributes related to the expression, i.e., “Smile” and “Mouth-slightly open”. Figure 10 shows that our model can generate diverse and continuous expression translation results. Furthermore, we can split the CelebA-HQ dataset into four domains considering attributes related to both the gender and facial expression, i.e., male with expression, male without expression, female with expression, and female without expression. Figure 11 demonstrates that our method is also applicable to manipulating multiple attributes in one single model. These results show that the proposed model is task-independent and can learn domain-specific attributes based on the datasets and tasks.

Fig. 10
figure 10
Continuous expression translation. Source domain: No-expression; target domain: with-expression. We present more results in Appendix A.3

Full size image
Fig. 11
figure 11
Continuous gender and expression translation.

Full size image
Fig. 12
figure 12
A limitation case. Our method manipulates domain-independent attributes such as age and eyeglasses in the female → male translation

Full size image
Limitation There are several limitations of the proposed model. Without fine-grained annotations, some domain-independent attributes such as age and eye-glasses would leak into the attribute latent space in the male⇌female translation task, as illustrated in Fig. 12. Although the proposed framework can continuously translate a source image into the target domain using the proposed signed attribute vectors, it is of great interest to derive such fine-grained visual information directly from images via disentangled representations. Our future work will focus on developing effective representation learning schemes to facilitate smooth image translation.

Conclusions
In this paper, we present a signed attribute vector to enable continuous and diverse Image-to-Image translation across domains. To enhance the continuous translation quality, we propose to use the sign-symmetrical attribute vectors to form a translation trajectory between different domains. Then, we leverage the domain information of intermediate results for adversarial training. We evaluate our methods on a wide range of Image-to-Image translation tasks. Both qualitative and quantitative results demonstrate that the proposed method achieves high-quality and diverse continuous translation across domains.