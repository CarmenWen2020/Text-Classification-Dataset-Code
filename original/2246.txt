The development of a real-time and robust RGB-T tracker is an extremely challenging task because the tracked object may suffer from shared and specific challenges in RGB and thermal (T) modalities. In this work, we observe that the implicit attribute information can boost the model discriminability, and propose a novel attribute-driven representation network to improve the RGB-T tracking performance. First, according to appearance change in RGB-T tracking scenarios, we divide the major and special challenges into four typical attributes: extreme illumination, occlusion, motion blur, and thermal crossover. Second, we design an attribute-driven residual branch for each heterogeneous attribute to mine the attribute-specific property and therefore build a powerful residual representation for object modeling. Furthermore, we aggregate these representations in channel and pixel levels by using the proposed attribute ensemble network (AENet) to adaptively fit the attribute-agnostic tracking process. The AENet can effectively make aware of appearance change while suppressing the distractors. Finally, we conduct numerous experiments on three RGB-T tracking benchmarks to compare the proposed trackers with other state-of-the-art methods. Experimental results show that our tracker achieves very competitive results with a real-time tracking speed. Code will be available at https://github.com/zhang-pengyu/ADRNet.

Access provided by University of Auckland Library

Introduction
Given the target position in the initial frame, visual object tracking, which captures the target in the whole sequence, is a fundamental task that has achieved substantial promotion in both accuracy and robustness Lu and Wang 2019. Several frameworks, including correlation filter Bolme et al. 2010; Danelljan et al. 2017, sparse learning Zhang et al, 2012; Wang et al. 2015; Lan et al. 2019, support vector machine Ning et al. (2016); Seunghoon Hong Tackgeun You and Han (2015) and Siamese network Bertinetto et al. 2016; Li et al. Li et al., have been designed. However, trackers are fragile to drift in challenging scenes and acute weathers, such as night, rainy, and fog. With the emerging of the easy-accessible and low-cost binocular camera, data from other modalities, such as laser Song et al. 2013, audio Megherbi et al. 2005, radar Kim and Jeon 2014, thermal Li et al. 2016, 2017, 2018; Lan et al. 2018, depth Kart et al. 2019; Camplani et al. Camplani et al.; Kart et al. 2018; Ding and Song 2015; Kart et al. 2018 and natural language Li et al. 2017; Feng et al. 2020, 2019; Yang et al. 2020 etc., are employed to model the target collaboratively. In specific, thermal (T) image, which measures target temperature, can be a powerful supplement to visible image, and RGB-T tracking has been paid increasing attention to addressing the aforementioned challenges.

In recent years, more and more researchers have attached attention to constructing RGB-T trackers with high accuracy and robustness and several RGB-T tracking methods have been published Lan et al. 2018; Zhu et al. 2018; Li et al. 2016; Gao et al. 2019 to fuse the multimodal information in developing a more accurate tracker. In previous years, sparse-learning-based methods are popular in handling how to fuse the multimodal information for tracking task. Lan et al. Lan et al. 2018 propose an optimal framework based on sparse learning to learn discriminative-consist features and modality reliability collaboratively. Li et al. Li et al. 2016 jointly learn the representations from different modalities using their similarity and incorporate the modality reliability into the sparse representation to fuse the data from different sources. Li et al. Li et al. 2018 embed soft cross-modality consistency to the sparse learning framework to remove the heterogeneity of both modalities, thereby achieving a robust performance. Recently, methods with deep learning frameworks, including Siamese network Zhang and Peng 2019, multi-domain tracking framework Wang et al. 2020; Li et al. 2019, etc., show surprising strengths in handling RGB-T task with substantial performance and low computation cost. Zhu et al. Zhu et al. 2018 fuse the RGB-T features in modality and layer levels, thereby achieving reasonable tracking performance. Gao et al. Gao et al. 2019 propose a recursive fusion method to integrate features from all layers in an end-to-end manner adaptively. Zhang et al. Zhang and Peng 2019 develop a modified SiamDW using two parallel trackers for both modalities and the features are fused using cross-attention module, thereby achieving satisfying performance in VOT-RGBT challenge. All these methods focus on studying the multimodal information fusion from heterogeneous sources. However, there exist two imperfections in existing methods. First, trackers are still fragile to drift in challenging cases, thereby influencing the tracking performance significantly. The early attempt has been conducted on RGB tracking Qi et al. 2019, which learns an attribute-based CNN with 5 branches to classify the target under specific attributes. Then, the features of all the attributes are fused with a convolution layer for attribute-agnostic tracking. Recently, Li et al. Li et al. 2020 propose a challenge-aware RGB-T tracking method to handle modality-shared and modality-specific challenges. The features in weak modality are guided by another modality to enhance the discriminability.

Fig. 1
figure 1
Tracking results of our ADRNet and other algorithms. Our real-time ADRNet tracker can handle the major challenges, including extreme illumination, motion blur, occlusion and thermal crossover and achieves competitive result among the state-of-the-art trackers

Full size image
Furthermore, existing methods can hardly meet the real-time requirement, because multiple data are introduced by the additional input. Although some speed-up techniques are applied, such as feature pruning Zhu et al. 2019, the speed promotion is not obvious to satisfy efficient tracking with real-time speed. To address these imperfections, we propose a real-time tracker, called attribute-driven representation network

(ADRNet), to learn effective residual representations to enhance the target appearance under various challenging circumstances individually and adaptively aggregate them for the attribute-agnostic tracking situation from spatial and channel aspects. Our ADRNet fully exploits target appearance guided by the attribute information and can predict the target state online effectively. Different with the similar works Qi et al. 2019; Li et al. 2020, there exist two main differences. First, prior works focus on building a challenge-specific model using limited attributes, which cannot cover all tracking scenes. To build a comprehensive representation, we set a general branch to handle the attributes, which are not mentioned and more suitable for attribute-agnostic tracking situation. Second, they simply aggregate features using a convolution layer and cannot achieve an attribute-aware tracker. To fully exploit the potential of heterogeneous attributes, which also brings noise for tracking, we propose an attribute ensemble network, which further fuses the residual features for specific challenges in channel and spatial levels. Our attribute ensemble network can predict the targetâ€™s attribute online. Figure 1 shows the effectiveness of ADRNet in handling challenging cases, such as extreme illumination, motion blur, occlusion and thermal crossover. The contributions of this work can be summarized as follows.

We decouple tracking challenges into four typical attributes according to target appearance, namely, extreme illumination (EI), occlusion (OCC), motion blur (MB), and thermal crossover (TC), and fine-tune the target representation via the attribute-driven residual branch (ADRB) to handle each heterogeneous challenge individually.

We propose an attribute ensemble network (AENet) for aggregating those representations for different attributes in channel-wise and pixel-wise to fit the attribute-agnostic tracking scenes. The AENet can predict current target attribute and suppress distractors effectively.

We conduct numerous experiments on three existing RGB-T tracking datasets to validate the effectiveness of our method. Experimental results show that our real-time tracker achieves very competitive results against other state-of-the-art trackers.

Related Work
Visual Object Tracking
Visual object tracking, aiming to localize the target during the whole sequence by the initial position in the first frame, have been paid much attention. Numerous frameworks, including correlation filter Bolme et al. 2010; Danelljan et al. 2017; Li and Zhu 2014; Danelljan et al. 2017, sparse coding Wang et al. 2015; Wanga and Yeung 2013 and deep learning based paradigms Bertinetto et al. 2016; Nam and Han 2016; Bhat et al. 2019; Voigtlaender et al. 2020 are proposed to build a tracker with high precision and robustness. Recently, CNN-based trackers dominate in this field with high accuracy and tracking speed, which can be roughly categorized into two types, that is, trackers with Siamese networks and with multi-domain learning networks. Siamese-based trackers Bertinetto et al. 2016; Li et al. Li et al.; Yu et al. 2020; Chen et al. 2020; Xu et al. 2020 estimate the similarity between the target and the current candidates and select the box with the highest score as the final result. Li et al. Li et al. Li et al. propose a SiamRPN tracker, which obtains a more precise scale estimation via a region proposal network. Yu et al. Yu et al. 2020 propose an attention module to adaptively tune the target branch that only embeds offline information, thereby achieving satisfying performance. ATOM-variant trackers Danelljan et al. 2019; Bhat et al. 2019; Danelljan et al. 2020, utilizing the strength of CNN, replace the correlation filter with learnable convolution layers, thereby achieving high accuracy and low computation cost. Furthermore, trackers with multi-domain learning Nam and Han 2016; Jung et al. 2018; Qi et al. 2019 consider tracking as a classification task that distinguishes the target against surroundings. Nam et al. Nam and Han 2016 firstly propose an MDNet tracker, with the online and offline learning mechanisms and powerful deep features, which wins the VOT2015 championship and validates its effectiveness. Jung et al. Jung et al. 2018 present a real-time variant of MDNet by removing the exhausting sampling operation with ROI pooling and utilizing a multi-task loss to distinguish the target from distractors with similar semantics. Qi et al. Qi et al. 2019 utilize the attribute information to overcome the lack of data and propose a two-stage training process for the network. In this work, our method is also designed based on multi-domain learning methods Jung et al. 2018 with widely applied in RGB-T tracking.

RGB-T Tracking
RGB-T tracking aims to combine the complementary advantages of visible and thermal data to enable the tracker work day and night. With the easy-accessible RGB-T binocular camera, many RGB-T tracking algorithms and benchmarks are developed Lan et al. 2019; Luo et al. 2019; Zhai et al. 2019; Wang et al. 2020; Li et al. 2018, 2019. Several frameworks, including sparse learning Li et al. 2018; Liu and Sun 2012; Wu et al. 2011, correlation filter Zhang et al. (Zhang et al.) and multi-domain network Wang et al. 2020; Li et al. 2020, etc., are utilized to build an accurate tracker. Lan et al. Lan et al. 2018 propose a sparse-learning-based framework to optimize modality-consistent feature and modality reliability jointly. Wang et al. Wang et al. 2020 exploit the cross-modal correlation in spatial and temporal aspects. Li et al. Li et al. 2018 learn the cross-modal features via their proposed manifold ranking methods. Zhu et al. Zhu et al. 2019 propose a feature aggregation network to fuse multimodal information followed by a feature pruning operation to filter out redundant representation. With the success of multi-domain framework in RGB tracking, Zhang et al. Zhang et al. 2018 construct a MDNet variant using RGB-T data. JMMAC Zhang et al. Zhang et al. propose an RGB-T tracking framework with jointly modeling motion and appearance information. The appearance cue is fused using the proposed multimodal fusion network and the tracker can adaptively select which cue is used for tracking via a tracker switcher.

Fig. 2
figure 2
Overall framework of our method. Two components, namely, attribute-driven residual branch (ADRB) and attribute ensemble network (AENet), are mainly proposed. (1) ADRB, which consists of five specific residual branches, aims to model the target appearance under specific circumstance, individually; (2) AENet, which consists of channel ensemble network (CENet) and spatial ensemble network (SENet), aims to aggregate those residual representations in both channel and spatial aspects. CENet makes a soft selection among different attributes to fit various challenges. SENet produces a pixel-wise weight to suppress distractors in the spatial level. The overall framework, called ADRNet, enhances the target representation and makes aware of attribute variation, thereby leading to an effective and real-time tracker

Full size image
Above all, these methods are mainly focus on how to fuse multimodal data, which aims to narrow the gap between two heterogeneous modalities. Besides, these methods usually achieve low tracking speed and incompatible with practical circumstances. The computation cost stems from the multiplied sources, which leads to an obvious speed decrease. Though DAPNet Zhu et al. 2019 aims to remove the redundant feature channels using weighted random selection and pooling operation, it cannot meet the requirement of real-time performance, which greatly limits the range of application. Different from previous works, in this work, we focus on modeling targets in specific attributes and handling each challenge individually, and improve the RGB-T tracking performance in both accuracy and speed.

Attribute-Driven Representation for Vision Tasks
To obtain a comprehensive representation according to various characteristics of data, researchers aim to exploit specific property in data with various attributes. ANT Qi et al. 2019 contains a network with a shared backbone and multiple branches for learning corresponding attributes. The representation is learned from different attributes and then concatenated for tracking the target. CAT Li et al. 2020 follows the similar architecture with ANT while the representation is learned guided by its related attributes. Then, all the learned representation is aggregated using a convolution layer. Ak et al. Ak et al. 2018 propose the FashionSearchNet, which extracts attribute-specific representations using the generated attribute activation maps (AAM). AAM is capable of identifying the region belonging to related attributes, thereby improving the model discriminability. Wang et al. Wang et al. 2016 construct attribute-specific features by considering the structural information in feature space and propose a label constrained dictionary to suppress the intra-class noise. All the aforementioned methods demonstrate the potential strength of attribute information. Inspired by ANT, we build attribute-specific representations via the proposed attribute-driven residual branch and adaptively fuse them at various levels to obtain a more robust appearance model.

Methodology
We detail the description of the proposed ADRNet method, which consists of two main components, namely, attribute-driven residual branch (ADRB) and attribute ensemble network (AENet). First, ADRB exploits the attribute-specific information guided by the property of given attributes. Then, AENet aims to ensemble these information in an adaptive manner from both channel and spatial aspects. The overall architecture of our method is shown in Fig. 2.

Attribute-Driven Residual Branch (ADRB)
Network architecture. The proposed ADRB aims to build a robust and discriminative appearance model when RGB or T modality is not reliable for tracking. To this end, according to the target state, we label the typical cases into four attributes, namely, extreme illumination (EI), occlusion (OCC), motion blur (MB), and thermal crossover (TC), with different properties. For instance, information extracted from visible image is less reliable for the target location in EI, whereas thermal information seems meaningless when suffering in TC. As for occlusion, only partial target is visible with appearance interfered by surroundings. When the target is at fast speed, the context information is missing, and artifact occurs. Observed by the heterogeneity of those attributes, we aim to model the target appearance in different attributes individually via the proposed ADRB, which consists of a 3Ã—3 convolution (Conv) layer followed by ReLU. Furthermore, we set an additional GENeral ADRB (GEN) to learn the representation for attribute-agnostic objects, except for those attributes. The residual feature for several attributes can be computed as,

ğ‘ğ‘=ğ›¿(ğ‘“ğ‘(ğ…3)),
(1)
where ğ‘“ğ‘(â‹…) is used to express the Conv layer in ADRB, whose kernel size is 3 Ã— 3. ğ‘ğ‘âˆˆâ„ğ¶Ã—ğ»Ã—ğ‘Š denotes the residual feature of specific attribute, and subscript ğ‘âˆˆîˆ­={GEN,EI,OCC,MB,TC} denotes the attributes. C denotes the number of feature channel. ğ…3âˆˆâ„ğ¶Ã—ğ»Ã—ğ‘Š is the feature from the backbone network, and ğ›¿ refers to activation function.

Note that, different from the similar works Qi et al. 2019; Li et al. 2020, we stress on the tracking performance in challenging cases where any modality is not available. Although our method can adapt other challenging attributes by simply setting individual branches, in this paper, we employ a GEN branch to learn the attribute-agnostic feature and construct a general appearance model involving the unmentioned attributes.

Fig. 3
figure 3
Overall architecture of our CENet. Two steps, namely, summation and selection, are operated to output the channel-wise weight to highlight corresponding channel according to the current target attribute

Full size image
Attribute Ensemble Network (AENet)
In the previous subsection, we learn the residual representations with the guidance of attribute annotation. However, the attribute information is unavailable in practical scene, which requires a feature aggregation mechanism to fuse those representations adaptively. To this end, we propose the AENet module to aggregate the residual features from different attributes in channel and spatial levels, which consists of two subnetworks: channel ensemble network (CENet) and spatial ensemble network (SENet).

Channel ensemble network (CENet). Inspired by the significant performance of Hu et al. 2018 in image classification, we design a novel CENet module to obtain the channel weights for all the residual representations, which contains summation and selection operations. The architecture of CENet is shown in Fig. 3. In the summation step, the residual features are first squeezed into a vector via the element-wise summation and global average pooling (GAP) processes. Then, the mixture feature is fine-tuned by a fully-connected (FC) layer. The summation step is expressed as,

ğ«ğ‘ ğ‘¢ğ‘š=ğ‘”ğ‘’ğ‘›(GAP(ğ‘ğ´)),
(2)
ğ‘ğ´=âˆ‘ğ‘âˆˆîˆ­ğ‘ğ‘,
(3)
where ğ‘ğ´ is the summation of residual features, and ğ«ğ‘ ğ‘¢ğ‘šâˆˆâ„ğ¶â€²Ã—1Ã—1 is obtained by conducting GAP. ğ‘”ğ‘’ğ‘›(â‹…) denotes the FC layer used for information embedding.

During the selection process, five FC layers followed by inter-channel softmax are conducted to produce the channel weight, each of which outputs the weight vector the corresponding attribute. The selection process can be shown as,

ğ°ğ‘=ğ‘”ğ‘ ğ‘’ğ‘™ğ‘(ğ«ğ‘ ğ‘¢ğ‘š),
(4)
where ğ°ğ‘âˆˆâ„ğ¶Ã—1Ã—1 denotes the channel weight for attribute ğ‘âˆˆîˆ­, and ğ‘”ğ‘ ğ‘’ğ‘™ğ‘(â‹…) stands for the FC layer for channel selection.

Fig. 4
figure 4
Illustration of our SENet. SENet aims to obtain a spatial weight map ğ–ğ‘  via the two-stream flow. The proposed network, embedding both offline and online information by the pixel-wise similarity and U-Net structure, can suppress the distractors with similar semantic information

Full size image
Spatial ensemble network (SENet). We design a two-stream SENet produces a spatial weight ğ–ğ‘ âˆˆâ„1Ã—ğ»Ã—ğ‘Š to highlight the candidates and suppress distractors in the spatial level. SENet consists of two streams that are responsible for learning the spatial map with offline and online information. In the offline stream, the pixel-wise similarity Wang et al. 2019 is calculated to measure the similarity between the initial template and the search region. Given the backbone feature of the template and search region, ğ“3âˆˆâ„ğ¶Ã—3Ã—3 and ğ…3âˆˆâ„ğ¶Ã—ğ»Ã—ğ‘Š, the pixel-wise similarity ğ…sâˆˆâ„(3Ã—3)Ã—ğ»Ã—ğ‘Š can be calculated as,

ğ…ğ‘†(ğ‘¢Ã—ğ‘£,ğ‘¥,ğ‘¦)=ğ…3(ğ‘¥,ğ‘¦)ğ‘‡ğ“3(ğ‘¢,ğ‘£),
(5)
where ğ…3(ğ‘¥,ğ‘¦) and ğ“3(ğ‘¢,ğ‘£)âˆˆâ„ğ¶Ã—1Ã—1 are the channel-wise vectors located in coordinates (x, y) and (u, v). We resize the target to a fixed size ğ‘†ğ‘¡ to guarantee the fixed-size ğ“3 (ğ‘†ğ‘¡ is set to 95 in our experiments). ğ…ğ‘  indicates a semantic correlation between the initial and current frames. Then, the similarity is aggregated by the channel-wise max pooling (CMP) operation. However, two problems need to be addressed. First, as shown in Fig. 4, the pixel-wise similarity decouples target into several parts, and fails to utilize the global context, which highlights the surroundings with similar semantic. Second, the pixel-wise similarity only embeds the offline information with the initial frame, which cannot adapt the target variation. Hence, we apply an online steam to handle the aforementioned issues via a simplified U-Net Ronneberger et al. 2015. Taken ğ‘A as input, the online stream consists of two Conv and deconvolution layers, whose kernel is 3Ã—3 and outputs a spatial weight. Then, the concatenation of two weights output by the two streams is sent to an ensemble layer followed by a Sigmoid function. Finally, the final weight map ğ–ğ‘  is obtained. The two-stream SENet, which considers online and offline cues, can achieve feature enhancement and avoid drifting to similar surroundings.

Combination of CENet and SENet. After obtaining the channel weight ğ°ğ‘ and spatial weight ğ–ğ‘  from CENet and SENet, we combine them to produce the final 3D weight map ğŒğ‘âˆˆâ„ğ¶Ã—ğ»Ã—ğ‘Š for each attribute a by element-wise production â¨‚, which can be expressed as

ğŒğ‘=îˆ¯ğ‘ (ğ°ğ‘)â¨‚îˆ¯ğ‘(ğ–ğ‘ ).
(6)
Before aggregating them, we expand the channel and spatial weights along the spatial and channel dimensions, using the function îˆ¯ğ‘  and îˆ¯ğ‘, respectively. After that, those weights are in the same size (i.e. ğ¶Ã—ğ»Ã—ğ‘Š). Then, the 3D weight map is element-wisely multiplied with the residual features ğ‘ğ‘. Finally, the refined feature ğ…ğ‘Ÿ is obtained by element-wise summation with the backbone feature ğ…3

ğ…ğ‘Ÿ=ğ…3+âˆ‘ğ‘âˆˆîˆ­ğŒğ‘â¨‚ğ‘ğ‘.
(7)
Tracking with ADRNet
Implementation details. Followed by RT-MDNet Jung et al. 2018, we adopt the truncated VGG-M Simonyan and Zisserman 2015 network as the backbone, which consists of three Conv layers. We utilize precise RoI pooling Jiang et al. 2018 to crop the RoI feature. Our method is implemented on Pytorch platform with Intel-i9 CPU with 64G RAM and RTX-2080 Ti GPU with 11G memory, which runs at 25 frames per second (fps) approximately. We will make our source codes to be public.

Data augmentation for ADRB training. In the aforementioned section, the learning of ADRB requires the data with attribute annotation. However, the existing datasets provide a coarse annotation in video level, where the videos can be labeled by multiple attributes. Thus, the attributes provided by datasets cannot indicate the precise attribute information in frame level, which is unsuitable for model learning. Furthermore, the number of data belonging to each attributes is imbalanced, thereby leading to bias learning. To address the above issues, we adopt the data augmentation to generate the data with four motioned attributes.

1.
EI data: The image with extreme illumination is defined as that the image is overexposed or underexposed caused by illumination factor. Thus, we utilize the gamma correction to adjust the image brightness. We randomly select the gamma from 0.1 to 0.7 and 1.5 to 4 for the low and high illumination, respectively. Since the infrared sensor is insensitive to illumination change, we only apply the gamma correction to RGB images.

2.
MB data: The motion blur from both camera and object moving will cause the target blurred, which degrades the clarity of target appearance. We apply a motion kernel to simulate fast moving. The length of motion kernel is set from 40 and 100 with random direction and the motion kernel is applied to both RGB and thermal images.

3.
OCC data: To generate synthetic data where the target is occluded by the surroundings, we adopt a rectangle-shaped distractor with arbitrary size to cover the part of the target and the color of distractor is depended by the mean of image patch centered by the target. Both RGB-T images are processed individually.

4.
TC data: Thermal crossover occcurs when the target has the similar temperature with the surroundings, thereby leading a confusing thermal map. To achieve this goal, we apply an average filter, whose size is randomly selected from 1 to 10, to the thermal image, which can degrade the discriminability of the thermal image.

The proposed method is equally conducted to all the original data to generate the same amount of images with specific attributes, thereby avoiding the bias learning. The example of the augmented data are shown in Fig. 5.

Multi-step training. In the training phase, we aim to embed attribute information into ADRB and learn an attribute-aware tracker in attribute-agnostic condition, which needs to train separately. To this end, we adopt a multi-step training strategy, which consists of three steps. First, we train ADRB and backbone with augmented and raw data, respectively. After obtaining attribute-specific representation, we train AENet to enhance the feature. Finally, we fine-tune the FC layers to adapt the enhanced representation. To evaluate the tracker on RGBT234 and VOT-RGBT datasets, we use GTOT as the training set. To test the tracker on GTOT, we train our model using the RGBT234 dataset.

Step I: Train ADRB and backbone. We first train the backbone, i.e. the pretrained VGG-M, the GEN branch and FC layers, which utilizes the raw data without attribute annotation. Then four specific residual branches are trained separately using corresponding augmented data with other layers frozen. Followed by the previous work Jung et al. 2018, a multi-task loss is adopted for our model learning, including a binary classification loss îˆ¸ğ‘ğ‘™ğ‘  and an instance embedding loss îˆ¸ğ‘–ğ‘›ğ‘ ğ‘¡. The binary classification loss is a softmax cross-entropy loss, which is defined as,

îˆ¸ğ‘ğ‘™ğ‘ =âˆ’1ğ‘âˆ‘ğ‘–=1ğ‘(ğ‘¦ğ‘–ğ‘‘â‹…ğ‘™ğ‘œğ‘”ğ‘’ğ‘¥ğ‘(ğ‘ +ğ‘–ğ‘‘)ğ‘’ğ‘¥ğ‘(ğ‘ +ğ‘–ğ‘‘+ğ‘ âˆ’ğ‘–ğ‘‘)+(1âˆ’ğ‘¦ğ‘–ğ‘‘)â‹…ğ‘™ğ‘œğ‘”ğ‘’ğ‘¥ğ‘(ğ‘ âˆ’ğ‘–ğ‘‘)ğ‘’ğ‘¥ğ‘(ğ‘ +ğ‘–ğ‘‘+ğ‘ âˆ’ğ‘–ğ‘‘))
(8)
where ğ‘¦ğ‘–ğ‘‘âˆˆ{0,1} is the ground-truth label indicating whether the candidate i belongs to the domain d or not, and ğ‘ +ğ‘–ğ‘‘ and ğ‘ âˆ’ğ‘–ğ‘‘ denote the score of the network, which represent the confidence where the candidate i is in domain d or not, respectively. N is the number of candidates. The instance embedding loss aims to enlarge the distance of targets which belongs to different domains, thereby achieving a distinctive feature embedding. The îˆ¸ğ‘–ğ‘›ğ‘ ğ‘¡ is expressed as follow,

îˆ¸ğ‘–ğ‘›ğ‘ ğ‘¡=âˆ’1ğ‘âˆ‘ğ‘–=1ğ‘âˆ‘ğ‘‘=1ğ·(ğ‘¦ğ‘–ğ‘‘â‹…ğ‘™ğ‘œğ‘”ğ‘’ğ‘¥ğ‘(ğ‘ +ğ‘–ğ‘‘)ğ‘’ğ‘¥ğ‘(âˆ‘ğ·ğ‘˜=1ğ‘ +ğ‘–ğ‘˜))
(9)
We note that the instance embedding loss only calculates the positive samples to make the positive score of targets in current domain become larger while suppressing the scores in other domains. Overall, those two losses are weighted-combined, which is given by,

îˆ¸=îˆ¸ğ‘ğ‘™ğ‘ +ğ›¼Ã—îˆ¸ğ‘–ğ‘›ğ‘ ğ‘¡
(10)
where ğ›¼ is a trade-off parameter, which is set to 0.1 in our experiment. As for training each residual branch, We adopt the same settings except for using corresponding synthetic data. The network is optimized via the stochastic gradient descent (SGD) method with the learning rate, momentum and weight decay setting to 0.0001, 0.9 and 0.0005, respectively. The epoch is set to 3000.

Step II: Train AENet. After obtaining the learned ADRB, we train the AENet using the raw data without attribute annotation and the backbone, ADRB and FC layers are frozen. we minimize Eq. (6) via SGD and the hyper-parameters are the same as step I.

Step III: Fine-tune FC layers. In previous steps, a comprehensive representation are obtained by ADRB and AENet. We fine-tune the FC layers to fit the aggregated feature with a smaller learning rate of 1ğ‘’âˆ’5, with unchanged training settings. The network is trained in 3000 epochs and achieves convergence.

Fig. 5
figure 5
Examples of the augmented data

Full size image
Online tracking. Once tracking, the last FC layer is randomly initialized for each test sequence. Furthermore, with the guidance of the initial frame, we fine-tune all FC layers by 50 epochs by setting the learning rate of the last FC layer to 0.003 and other layers to 0.0003. We crop 500 positive and 5000 negative samples, whose IoU between the target box are larger than 0.7 and less than 0.3, respectively. With the t-th frame comes, 256 candidates are collected around the target with the Gaussian distribution and their confidences are output by ADRNet. The final bounding box is obtained by averaging the top-5 candidates with the highest scores. Following the setting of RT-MDNet, ADRNet adopts two updating mechanisms, which consist of short-term update and long-term update. In short-term update, the model will be updated within the latest 20 frames by 15 epochs when tracking score is lower than a threshold (the threshold is set to 0 in our experiment). As for long-term update, the model will be updated with an interval of 100 frames. Please refer to Jung et al. (2018) for more details.

Table 1 Comparisons of different trackers (with various challenging factors) using the RGBT234 dataset. Maximum success rate (MSR%) and maximum precision rate (MPR%) are used for evaluation
Full size table
Experiments
Datasets and Metrics
Datasets. We conduct comprehensive experiments on three popular datasets (i.e., RGBT234, GTOT and VOT19-RGBT). As the largest dataset for RGB-T tracking, RGBT234 Li et al. 2019 contains 234 videos with over 116K image pairs, annotated by 12 challenging factors, including no occlusion (NO), partial occlusion (PO), heavy occlusion (HO), low illumination (LI), low resolution (LR), thermal crossover (TC), deformation (DEF), fast motion (FM), scale variation (SV), motion blur (MB), camera moving (CM), and background clutter (BC). GTOT Li et al. 2016 is constructed by 50 grayscale-thermal sequences with a still camera, and most of the targets are in low resolution with small spatial size. Since 2019, the VOT committee has held the subchallenge of RGB-T tracking, whose dataset is a subset of RGBT234, consisting of 60 video clips Kristan et al. 2019.

Evaluation metrics. As for RGBT234 and GTOT, the results are measured with maximum success rate (MSR) and maximum precision rate (MPR) via one pass evaluation rule. MSR indicates the Area Under Curve of Intersection over Union plot with different thresholds and adopts the maximum value between two modalities as the final result. MPR represents the maximum frame ratio, whose center location error between the result and ground truth is smaller than the threshold th. th is set to 20 and 5 in RGBT234 and GTOT, respectively. VOT19-RGBT utilizes Expected Average Overlap (EAO) to evaluate trackers in terms of accuracy (A) and robustness (R).

Compared algorithms. For a thorough comparison on GTOT and RGBT234, we select eight latest RGB-T trackers with high performance, including CAT Li et al. 2020, MaCNet Zhang et al. 2020, TODA Yang et al. 2019, DAFNet Gao et al. 2019, MANet Li et al. 2019, DAPNet Zhu et al. 2019, CMR Li et al. 2018, and SGT Li et al. 2017. In addition, we compare our method with nine recent trackers on VOT19-RGBT, which are reported on the VOT-RGBT challenge Kristan et al. 2019.

Comparison with State-of-the-art Trackers
RGBT234. First, we compare our tracker with competitors on the RGBT234 dataset and report the tracking performance in Table 1. ADRNet performs significantly better than all other trackers on all metrics with 57.1% and 80.9% in MSR and MPR, which validates the effectiveness of our method. Compared with the most recent tracker (also the second-best tracker), our method shows superior performance to CAT with 1.0% and 0.6% promotion on MSR and MPR, respectively. Table 1 also evaluates different trackers in handling various challenging factors. Experimental results show that our method achieves significant performance especially for partial occlusion, low resolution, fast motion, motion blur and camera moving.

Table 2 Attribute-based comparison with eight competitors on the GTOT dataset
Full size table
GTOT. Second, we conduct a comparison using the GTOT dataset. As shown in Table 2, our method shows substantial result among all the competitors with 73.9% and 90.4% in MSR and MPR, respectively. We also conduct attribute-based analysis, which is shown in Table 2. ADRNet achieves top-three performance in handling all the attributes in GTOT. Specifically, our method shows strong strength in low illumination, thermal crossover challenges, which benefits from the comprehensive representation constructed by our ADRB module. Since SENet enhances the aggregated features at the spatial level, the more precise representations are obtained to handle target with small size, where the target needs to highlight while suppressing the distractors.

VOT19-RGBT. Finally, we test our tracker on the VOT19-RGBT benchmark and compare ADRNet with all participants in all three metrics, including EAO, A, and R. Our tracker achieves the second rank according to the EAO metric. Though our tracker is inferior to the top-rank tracker (JMMAC), we claim that the JMMAC is limited by its low speed (4fps), and our tracker with real-time speed achieves more than 6Ã— speed promotion with a larger range of application scenes.

Table 3 Results on VOT19-RGBT
Full size table
Table 4 Comparison of each component in ADRNet on GTOT and RGBT234 datasets
Full size table
Ablation Analysis
We conduct an in-depth comparison for different variants of our ADRNet using the RGBT234 and GTOT datasets:

B(RGB): the baseline method only with the visible modality.

B(T): the baseline method only with the thermal modality.

B(RGBT): the baseline method with both RGB and thermal modalities.

B(RGBT)+ADRB: the features of ADRB are aggregated with the average summation.

B(RGBT)+ADRB+CENet: the features of ADRB are aggregated with the CENet only.

B(RGBT)+ADRB+SENet: the features of ADRB are aggregated with the SENet only.

B(RGBT)+ADRB+CENet+SENet: the features of ADRB are aggregated with both CENet and SENet, which results in our final ADRNet tracker.

The comparisons of different variants on RGBT234 and GTOT datasets are shown in Table 4. As for RGBT234, ADRB learns a more reasonable representation for tracking by fine-tuning the backbone feature, which achieves 3.3% and 3.4% promotion in MSR and MPR. Furthermore, our AENet aggregates the target information in heterogeneous scenes adaptively, and improves the modelâ€™s discriminability in a large margin. Specifically, CENet and SENet achieve 3.0% and 2.6% promotion in MSR with comparable MPR. Tracker equipped the overall AENet achieves the top performance in both MSR and MPR. We also conduct an in-depth analysis to validate the effectiveness of components in ADRNet on GTOT. Each module has a reasonable contribution to the final results. Compared with the baseline method using two modalities (B(RGBT)), the tracker with ADRB(

B(RGBT)+ADRB) achieves 2.6% and 0.7% promotion in MSR and MPR, respectively. This indicates that our ADRB, aiming to construct comprehensive representations to fit appearance variation, shows satisfying improvement in tracking accuracy. Moreover, the proposed AENet(B(RGBT)+ADRB+CENet+SENet) adjusts the target representations in an adaptive manner from channel and spatial aspects, which obtains a further improvement with 3.4% and 2.6% in MSR and MPR. Note that both sub-networks in AENet, i.e., CENet and SENet, work well, which validate their strengths.

Table 5 Analysis of ADRB on handling corresponding attributes on RGBT234
Full size table
Table 6 Analysis of ADRB on handling corresponding attributes in GTOT
Full size table
Fig. 6
figure 6
Visualization of features obtained by the single ADRB. Compared with the original backbone feature (Backbone Feat.), each ADRB can produce semantic-meaningful representations (ADRB Feat.) for target under corresponding attribute, thereby improving the model for target location

Full size image
Fig. 7
figure 7
Qualitative analysis of CENet. CENet achieves an accurate prediction on target attributes, and constructs an adaptive appearance model according to the target state. The attribute (Attr) shown on the top-right of the thermal image represents the attribute type with the highest overall weight among all channels

Full size image
Attribute-based Analysis for ADRB. We further validate the contribution of each ADRB in dealing with the corresponding attribute. The attribute-based comparisons on RGBT234 and GTOT are depicted in Tables 5 and 6, respectively. Each attribute-driven branch shows the top performance on corresponding attributes, which shows our ADRB can learn robust residual representations for specific challenges guided by the attribute information. Furthermore, the GEN branch learns the general representation without attribute annotation which achieves the top performance in the whole dataset. Moreover, we implement the tracker, which removes the GEN branches, namely B(RGBT)+ADRB-woGEN. Compared with the tracker equipped with GEN branch (B(RGBT)+ADRB), B(RGBT)+ADRB-woGEN obtains a decreasing result and is superior to the baseline methods in overall performance with respect to both MSR and MPR, which validates the necessity of GEN branch and the effectiveness of all other ADRB.

Besides, we validate the effectiveness of ADRB qualitatively. The representations learned by ADRB are semantically meaningful. As shown in Fig. 6, since attribute information is exploited to build an attribute-aware tracker, the targets in various scenes are highlighted by ADRB in the feature level, resulting in better performance. Overall, our ADRB can yield comprehensive and robust feature representations to handle the challenging cases by building both attribute-specific and general appearance model.

Analysis for CENet. As shown in Tables 5 and 6, the performance decreases when the attribute-driven representations are combined simply in average summation manner (namely, B(RGBT)+ADRB), which indicates the necessity of designing a reasonable module to predict the attribute variation online. We argue that our CENet estimates the target attributes effectively and makes a good switch among various ADRB blocks to obtain better representation. To validate this, we depict the attribute (Attr) with the highest overall channel weight in Fig. 7. The target attributes are predicted correctly in the aforementioned cases, which shows the strength of CENet.

To show the strength of CENet in attribute switching, we compare several methods with/without CENet in RGBT234. We show the success plot in Fig. 8. CENet shows great potential in making aware of attributes, thereby achieving substantial performance. First, we conduct feature aggregation without CENet in average summation manner (B(RGBT)+ADRB-summation). Furthermore, we apply two fusion types using CENet, i.e., soft and one-hot (OH) aggregation. Compared with B(RGBT)+ADRB-summation, both two aggregation methods using CENet achieve more than 1.6% promotion in MSR. Note that soft aggregation (B(RGBT)+ADRB+CENet-soft), as the final aggregation method, indicates that the feature is ensembled by weighted summation with the channel weights obtained from CENet and one-hot aggregation (B(RGBT)+ADRB+CENet-OH) means that the weight is firstly transformed to a one-hot vector, utilizing the attribute with the highest weight in the channel level.

Fig. 8
figure 8
CENet analysis on RGBT234. The comparison with CENet shows obvious superiority in feature aggregation and attribute switching

Full size image
Analysis for SENet. We validate the effectiveness of each stream in SENet on RGBT234, which utilizes online and offline information to construct an accurate spatial map. As shown in Fig. 9, the trackers with single online and offline stream are denoted as B(RGBT)+ADRB+SENet-online and B(RGBT)+ADRB+SENet-offline, respectively. Both streams make positive contributions to performance improvement. Moreover, the final model, B(RGBT)+ADRB+SENet combining the advantage of those two branches, makes a further improvement on MSR. We also compare a related attention based work (CBAM Woo et al. 2018), which also provides a spatial weight for object detection. The comparison result on RGBT234 is shown in Table 7. Tracker with SENet shows superior performance to that with CBAM, which validates that SENet can output a more reasonable spatial weights for RGB-T tracking.

Table 7 Comparison between proposed SENet and spatial attention in CBAM Woo et al. 2018
Full size table
Fig. 9
figure 9
SENet analysis of ADRNet on RGBT234. Both branches can boost the tracking performance

Full size image
Fig. 10
figure 10
Accuracy-speed plot on RGBT234. We utilize MSR to measure the trackersâ€™ accuracy. Our ADRNet shows significant advantage in both tracking accuracy and speed

Full size image
Fig. 11
figure 11
Qualitative analysis for SENet. The SENet consists of two streams, namely offline and online streams. We show the spatial weight obtained by those two streams and the overall SENet. Our SENet, embedding both online and offline information, has great potential in suppressing distractors

Full size image
Fig. 12
figure 12
Qualitative result of ADRNet with other competitors. Our method shows strong superiority to other trackers in handling target state variation, such as extreme illumination, occlusion, motion blur, and thermal crossover

Full size image
As shown in Fig. 11, we depict the tracking result and spatial weights ğ–offline, ğ–online and ğ–S output by online stream, offline stream and the overall SENet, respectively. The online stream is capable of suppressing the distractors when background clutter occurs, while the offline stream measuring the pixel-wise similarity between the template and the search region can give satisfying guidance when the target is partially occluded. Utilizing both online and offline cues, the overall SENet can provide an accurate weight thereby obtaining a tight bounding box.

Table 8 Analysis of the proposed data augmentation method on the RGBT234 dataset. The model can learn the attribute information with the guidance of data enhancement method
Full size table
Effectiveness of data augmentation. To test the contribution of our data augmentation method, we compared the trackers with (B(RGBT)+ADRB) and without data augmentation (B(RGBT)+ADRB-woDA). As for tracker without data augmentation, we train the residual branches for four specific attributes using the sequence belonging to corresponding attributes, which is annotated by the dataset. The GEN attribute is trained in the same manner, described in Sect. 3.3. The results are shown in Table 8.

Table 9 Execution time of each proposed module. All our modules are efficient for tracking with negligible time cost
Full size table
Qualitative Analysis
Figure 12 depicts that our ADRNet has great potential in handling several challenging cases. For instance, in sequence â€˜CarLightâ€™, the target appearance suffers the strong illumination, whereas other trackers cannot handle the scale change during the illumination effect. ADRNet, which aims to enhance the target representation under EI, can capture the car with a tight bounding box. In â€˜Baketballwalikingâ€™, the man is occluded by trees, thereby leading to most trackers fail. In this case, our ADRNet can locate the object accurately when the target reappears. Motion blur is also a critical challenge. Trackers are fragile to drift because appearance changes dramatically. Our ADRNet achieves a precise tracking result in â€˜Cycle4â€™. In sequence â€˜BlackSwan1â€™, the target has similar appearance in thermal images. Our method also works well in handling this thermal crossover challenge.

Speed Analysis
We argue that the proposed method has great potential in both accuracy and speed among all the competitors. To validate this, we depict the accuracy-speed plot in Fig. 10. We measure the trackerâ€™s accuracy by the MSR in RGBT234 and utilize frames per second (fps) to report the speed. Only the proposed ADRNet can meet the requirement of real-time tasks and work well in a large range of applications. Since most of the trackers have not released their source codes, to achieve a fair comparison, we list the experimental setting and summarize their speed in the original paperFootnote1.

Furthermore, we report the execution time of each module, which is listed in Table 9. We state that our proposed modules, including ADRB, SENet and CENet are efficient for real-time tracking, with negligible cost.

Conclusion
In this work, we propose a novel ADRNet method for real-time RGB-T tracking. First, we summarize tracking challenges as four typical attributes according to the target appearance variation. Then, we develop an ADRB to construct a robust appearance model under different circumstances, individually. Moreover, we design an AENet for effective feature aggregation. AENet ensembles the representations obtained by ADRB in spatial and channel levels, thereby obtaining a good switch on various attributes and achieving a satisfying performance. Extensive results on three popular RGB-T tracking datasets (i.e., RGBT234, GTOT and VOT19-RGBT) validate the effectiveness of our ADRNet, with fast speed.