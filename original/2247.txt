Multi-orientation scene text detection has recently gained significant research attention. Previous methods directly predict words or text lines, typically by using quadrilateral shapes. However, many of these methods neglect the significance of consistent labeling, which is important for maintaining a stable training process, especially when it comprises a large amount of data. Here we solve this problem by proposing a new method, Orderless Box Discretization (OBD), which first discretizes the quadrilateral box into several key edges containing all potential horizontal and vertical positions. To decode accurate vertex positions, a simple yet effective matching procedure is proposed for reconstructing the quadrilateral bounding boxes. Our method solves the ambiguity issue, which has a significant impact on the learning process. Extensive ablation studies are conducted to validate the effectiveness of our proposed method quantitatively. More importantly, based on OBD, we provide a detailed analysis of the impact of a collection of refinements, which may inspire others to build state-of-the-art text detectors. Combining both OBD and these useful refinements, we achieve state-of-the-art performance on various benchmarks, including ICDAR 2015 and MLT. Our method also won the first place in the text detection task at the recent ICDAR2019 Robust Reading Challenge for Reading Chinese Text on Signboards, further demonstrating its superior performance. The code is available at https://git.io/TextDet.

Access provided by University of Auckland Library

Introduction
Scene text detection in arbitrary orientations has garnered significant attention in computer vision because of its numerous potential applications, including augmented reality and robot navigation. Scene text detection is also the foundation and prerequisite for text recognition, which provides a reliable and straightforward approach to scene understanding. However, this challenge remains largely unsolved because text instances in natural images are often of multi-orientation, low-quality representations, having perspective distortions of various sizes and scales.

In the literature, several methods (Jaderberg et al. 2016; Neumann and Matas 2012, 2015a, 2015; Tian et al. 2015, 2016) have been developed for solving horizontal scene text detection. However, scene text in the wild is typically presented in a multi-orientation form, attracting a few recent studies (Zhong et al. 2016; Liu and Jin 2017; Shi et al. 2017a; Xue et al. 2018; Xie et al. 2019a; Liu et al. 2019a; Liao et al. 2017, 2018a, b; Liu et al. 2018; He et al. 2017a, b) that can be roughly categorized into two groups: segmentation and regression-based methods. Segmentation-based methods often employ networks, such as fully convolution networks (FCNs) (Long et al. 2015) and Mask R-CNN (He et al. 2017c). Segmentation-based methods have become the mainstream approach, because they are sufficiently robust in many complicated scenarios. One limitation is that segmented text instances often require additional post-processing steps. For example, the segmentation results obtained by Mask R-CNN must be fitted into rotated quadrilateral bounding boxes, which necessitates a number of heuristic settings and geometric assumptions.

On the other hand, Regression-based methods (Zhu and Du 2018; Liu and Jin 2017; Xue et al. 2019; Liao et al. 2018b; Ma et al. 2018; Liao et al. 2018a; He et al. 2018a; Zhou et al. 2017; He et al. 2017b) are comparatively simple. For multi-orientation text, explicitly predicting the vertices obtains the four boundaries of the text instances. Thus, no additional grouping procedure is required. Although these methods can directly predict vertex positions, the significance of regression without facing inconsistent labeling has rarely been discussed. Consider the efficient and accurate scene text (EAST) detector (Zhou et al. 2017) method as an example. In EAST, each feature within a text instance is responsible for regressing the corresponding quadrilateral bounding box by predicting four distances to the boundaries and a rotation angle from the viewpoint. A pre-processing step to assign regression targets is required. As shown in Fig. 1, the regression targets can be altered drastically, even with a minor rotation. Such ambiguities lead to an unstable training process, which considerably degrades the performance. Our experiments indicate that the accuracy of EAST (Zhou et al. 2017) deteriorates sharply (by more than 10%) when equipped with a random rotation technique for data augmentation, which is supposed to boost the performance.

Fig. 1
figure 1
Comparison of a previous methods and b our proposed OBD. Previous methods directly regress the vertices, which can often be adversely affected by inconsistent labeling of training data, resulting in unstable training and unsatisfactory performances. Our method tackles this problem and removes the ambiguity by discretizing a quadrilateral bounding box that is orderless

Full size image
To address this problem, we propose a novel method, (i.e., Orderless Box Discretization (OBD)), which consists of two modules: Key Edges Detection and Matching-Type Learning. The fundamental idea is to employ invariant representations (e.g., minimum x, minimum y, maximum x, maximum y, mean center point, and intersecting point of the diagonals) that are irrelevant to the label sequence to deduce the bounding box coordinates inversely. To simplify the parameterization, the OBD method first locates all discretized horizontal and vertical edges that contain a vertex. Then, a sequence labeling matching type is learned to determine the best-fit quadrilateral. By avoiding the ambiguity of the training targets, our approach successfully improves performance when a large amount of rotated data is involved.

We complement our method with a few critical technical innovations that further enhance performance. We conduct extensive experiments and ablation studies based on our method to explore the influence of six relevant issues: (namely, data arrangement, pre-processing, backbone, proposal generation, prediction head, and post-processing) to determine the significance of the various components. We thus provide useful tips for designing state-of-the-art text detectors. Leveraging OBD and these useful refinements, we won first place in the task of Text Line Detection at the ICDAR2019 Robust Reading Challenge on Reading Chinese Text on Signboards.

Our main contributions are summarized as follows.

1.
Our method addresses the inconsistent labeling issue of regression-based methods, which is of great importance for achieving good detection accuracy.

2.
The flexibility of our proposed method allows us to make use of several key refinements that are critical to further boosting accuracy. Our method achieves state-of-the-art performance on various scene text detection benchmarks, including ICDAR2015 (Karatzas et al. 2015) and MLT (Nayef et al. 2017). Additionally, our method won the first place in the Text Detection task of the recent ICDAR2019 Robust Reading Challenge on Reading Chinese Text on Signboard. Based on the detection results, we integrate advanced recognition models to achieve state-of-the-art results.

3.
Our method can be generalized to ship detection in aerial images without minimum modification. The significant improvement in terms of the TIoU-Hmean metric further demonstrates the robustness of our approach.

Related Work
Fig. 2
figure 2
Previous solutions can be negatively affected by the inconsistent labeling issue

Full size image
Recently, the emergence of new datasets (Ch’ng et al. 2019; Liu et al. 2019b; Sun et al. 2019; Chng et al. 2019) has propelled arbitrarily shaped scene text detection to mainstream research. Multi-orientation scene text detection is one of its most important representations, because multi-orientation scene text comprises most of the text found in real-world visual scenes. The computer-driven detection task remains complex, and there is much room for improvement with regards to decoding multi-orientation text from pictures. Hence, detection benchmarks, such as the MLT (Nayef et al. 2017, 2019) dataset, are leveraged to refine the process. However, using quadrilateral bounding boxes can result in some problems for both current segmentation and non-segmentation-based methods.

Segmentation-based Segmentation-based methods (Zhang et al. 2016; Long et al. 2015; He et al. 2017c; Deng et al. 2018; Lyu et al. 2018a; Wu and Natarajan 2017; Wang et al. 2019a; He et al. 2016) usually require additional steps to group pixels into polygons.

Non-segmentation-based Non-segmentation based methods (Zhu and Du 2018; Xue et al. 2019; Liao et al. 2018b; Ma et al. 2018; Liao et al. 2018a; He et al. 2018a; Liu and Jin 2017; Zhou et al. 2017; He et al. 2017b) can directly learn the exact bounding box for localizing the text instances, but they are easily affected by the label sequence. Usually, such methods use a typical sorting method of the coordinate sequence to alleviate this issue. However, the solutions are not robust because the entire sequence may change even with a small amount of interference. To clarify this, we discuss some of the previous solutions as follows:

Given an annotation having coordinates of four points, a common sorting method of the coordinate sequence to alleviate this issue is to choose the point having the minimum x as the first point, then deciding the rest of the points in a clockwise manner. However, this protocol is not robust. Considering the horizontal rectangle as an example, using this protocol, let us decide that the first point is the top-left point. Thus, the fourth point is the bottom-left point. Suppose that the bottom-left point moves leftward one pixel (which is possible because of the inconsistent labeling). In that case, the original fourth point becomes the first point, and the whole sequence changes, resulting in very unstable learning.

As shown in Fig. 2a, DMPNet (Liu and Jin 2017) proposed a protocol that uses the slope to determine the sequence. However, if the diagonal is vertical, leftward, or rightward, change of a pixel can result in a completely different sequence.

As shown in Fig. 2b, given four points, Textboxes++ (Liao et al. 2018a) uses the distances between the annotation points and the vertices of the circumscribed horizontal rectangle to determine the sequence. However, if 𝑞1 and 𝑞4 have the same distance to 𝑝1, and one pixel rotation can completely change the whole sequence.

As shown in Fig. 2c, QRN (He et al. 2018a) first finds the mean center point of the four given points then constructs a Cartesian coordinate system. Using the positive x axis, QRN ranks the intersection angles of the four points and chooses the point having the minimum angle as the first. However, if the first point is in the positive x axis, one pixel change upward or downward will result in an entirely different sequence.

Although these methods (Liu and Jin 2017; Liao et al. 2018a; He et al. 2018a) can alleviate confusion to some extent, the results can be significantly undermined when using pseudo samples having large degrees of rotation.

Unlike these methods, our method is the first to directly produce a compact quadrilateral bounding box without complex post-processing. Moreover, it can completely avoid inconsistent labeling issues.

Fig. 3
figure 3
Overview of the proposed detection framework

Full size image
Our Method
Our proposed scene text detection system consists of three core components: an Orderless Box Discretization (OBD) block, a matching-type learning (MTL) block, and re-scoring and post-processing (RPP) block. Figure 3 illustrates the overall pipeline of the proposed framework, and more details are presented in the following sections.

Orderless Box Discretization
The purpose of multi-orientation scene text detection is to accurately localize the textual content by generating outputs in the form of rectangular or quadrilateral bounding boxes. Compared with rectangular annotations, quadrilateral labels demonstrate an increased capability to cover effective text regions, especially for rotated texts. However, as discussed in Sect. 2, simply replacing rectangular bounding boxes with quadrilateral annotations can introduce inconsistency because of the sensitivity of the non-segmentation-based methods to label sequences. As shown in Fig. 1, the detection model might fail to obtain accurate features for the corresponding points when facing small disturbances. One possible reason behind this is that the neural-network-based regressor for bounding box prediction is essentially a nonlinear continuous function, which means that each input is only mapped to one output. Thus a non-function or a function with a steep gradient cannot be effectively fitted. In our case, a small disturbance may completely change the whole sequence of the vertex and thus a similar input may result in completely different output as well as a steep gradient. Therefore, instead of predicting sequence-sensitive distances or coordinates, an OBD block is proposed to discretize the quadrilateral box into eight Key Edges (KE) comprising order-irrelevant points; i.e., minimum 𝑥(𝑥𝑚𝑖𝑛) and 𝑦(𝑦𝑚𝑖𝑛)), the second-smallest 𝑥(𝑥2) and 𝑦(𝑦2), the second-largest 𝑥(𝑥3) and 𝑦(𝑦3), and the maximum 𝑥(𝑥𝑚𝑎𝑥) and 𝑦(𝑦𝑚𝑎𝑥) (see Fig. 1). We use x-KEs and y-KEs in the following sections to represent [𝑥𝑚𝑖𝑛, 𝑥2, 𝑥3, 𝑥𝑚𝑎𝑥] and [𝑦𝑚𝑖𝑛, 𝑦2, 𝑦3, 𝑦𝑚𝑎𝑥], respectively.

Specifically, the proposed approach is based on the widely used generic object detection framework, Mask R-CNN (He et al. 2017c). As shown in Fig. 4, the proposals processed by RoIAlign are fed into the OBD block with the pooling size of 14×14, where the feature maps are forwarded through four convolutional layers with 256 output channels. The output features are then upsampled by a 2× deconvolutional layer and a 2× bilinear upscaling layers. Thus, the output size of the feature maps 𝐹𝑜𝑢𝑡 is 𝑀×𝑀, where M is 56 in our implementation. Furthermore, two convolution kernels shaped as 1×𝑀 and 𝑀×1 with six channels are employed to shrink the horizontal and vertical features for the x-KEs and y-KEs, respectively. Finally, the OBD model is trained by minimizing the cross-entropy loss 𝐿𝑘𝑒 over an M-way softmax output, where the corresponding positions of the ground-truth KEs are assigned to each output channel.

Fig. 4
figure 4
Illustration of the OBD and MTL blocks

Full size image
In practice, OBD does not directly learn the x-KEs and y-KEs because of the restriction of the region of interest (RoI). Specifically, the original Mask R-CNN framework limits the prediction inside the RoI areas. Thus, if the regression bounding box is not accurate, the missing pixels outside of the bounding box will not to be restored. To solve this problem, the x-KEs and y-KEs are encoded in the form of “half lines” during training. Suppose we have x-KEs, 𝑥𝑖∈[𝑥𝑚𝑖𝑛,𝑥2,𝑥3,𝑥𝑚𝑎𝑥], and y-KEs, 𝑦𝑖∈[𝑦𝑚𝑖𝑛,𝑦2,𝑦3,𝑦𝑚𝑎𝑥]. Then, the “half lines” are defined as follows:

x𝑖ℎ𝑎𝑙𝑓=𝑦𝑖ℎ𝑎𝑙𝑓=𝑥𝑖+𝑥𝑚𝑒𝑎𝑛2,𝑦𝑖+𝑦𝑚𝑒𝑎𝑛2,
(1)
where 𝑥𝑚𝑒𝑎𝑛 and 𝑦𝑚𝑒𝑎𝑛 represent the value of the mean central point of the ground-truth bounding box for the x and y axes, respectively. By employing such a training strategy, the proposed OBD block can break the RoI restriction (see Fig. 5). Thus, it is more likely to produce accurate bounding box because 𝑥ℎ𝑎𝑙𝑓 and 𝑦ℎ𝑎𝑙𝑓 fall into the area of the RoIs in most cases, even if the border of the text instance is located outside the RoIs.

Similar to Mask R-CNN, the overall detector is trained in a multi-task manner. Thus, the loss function comprises four terms:

𝐿=𝐿𝑐𝑙𝑠+𝐿𝑏𝑜𝑥+𝐿𝑚𝑎𝑠𝑘+𝐿𝑘𝑒,
(2)
where the first three terms, 𝐿𝑐𝑙𝑠, 𝐿𝑏𝑜𝑥 and 𝐿𝑚𝑎𝑠𝑘, follow the same settings as presented in He et al. (2017c). 𝐿𝑘𝑒 is the cross-entropy loss, which is used for learning the Key Edges prediction task. The authors made an interesting observation in which the additional keypoint branch can harm the bounding box detection performance (He et al. 2017c). However, based on our experiments (see Tables 1 and 2), the proposed OBD block is the key component that significantly boosts the detection accuracy. There may be two reasons for this. First, ours is different from the keypoint detection task, which has to learn 𝑀2 classes against each other. Thus, the numbers of competitive pixels in the OBD block is only M. Second, for the keypoint detection task, neither one-hot point nor a small circled area can be used to describe the target keypoint accurately, while the KEs produced by OBD are well defined. Thus, our method may provide more accurate supervision for training the network.

Fig. 5
figure 5
The proposed framework can break the restrictions of the RoIs. The green solid quadrilateral and red dashed rectangular boxes represent the predictions and proposals, respectively. (Color figure online)

Full size image
Table 1 Ablation studies demonstrating the effectiveness of the proposed method. The 𝛾 of RPP is set to 1.4 (best practice). The results on this table also adopt MLT training data and data augmentation strategies to help improve the final performance
Full size table
Table 2 Ablation studies for comparing the mask branch and KE branch. The 𝛾 of RPP is set to 0.8 (best practice). Compared to Table 1, the results here are all tested in different branches of the same model without any data augmentation
Full size table
Fig. 6
figure 6
Illustration of different matching types

Full size image
Matching-Type Learning
It is noteworthy that the OBD block only learns to predict the numerical values of eight KEs but is unable to predict the connection between the x-KEs and y-KEs. Therefore, we need to design a proper matching procedure to reconstruct the quadrilateral bounding box from the KEs. Otherwise, the incorrect matching type may lead to completely unreasonable results (see Fig. 6).

As described in Sect. 3.1, there are four x-KEs and four y-KEs outputted by the OBD block. Each x-KE should match one of the y-KEs to construct a corner point, such as (𝑥𝑚𝑖𝑛,𝑦𝑚𝑖𝑛), (𝑥2,𝑦𝑚𝑎𝑥), and (𝑥𝑚𝑎𝑥,𝑦2). Then, all four constructed corner points are assembled for the final prediction, giving us the quadrilateral bounding box. It is important to note that different orders of the corners would produce different results. Hence, the total number of matching-types between the x-KEs and y-KEs can be simply calculated by 𝐴44=24. For example, the predicted matching-type in Fig. 6a is [(𝑥𝑚𝑖𝑛,𝑦2),(𝑥2,𝑦𝑚𝑎𝑥),(𝑥3,𝑦𝑚𝑖𝑛),(𝑥𝑚𝑎𝑥,𝑦3)]. Based on this, a simple yet effective MTL module is proposed to learn the connections between x-KEs and y-KEs. Specifically, as shown in Fig. 4, the feature maps that are used for predicting the x-KEs and y-KEs are used for classifying the matching-types. Specifically, the output feature of the deconvolution layer is connected to a convolutional layer having an 𝑀/2×𝑀/2 kernel size with 24 output channels. Thus, the matching procedure is formed as a 24-category classification task. In our method, the MTL head is trained by minimizing the cross-entropy loss, and the experiments demonstrate that the convergence speed is very fast.

Fig. 7
figure 7
Different patterns of 𝑆𝑂𝐵𝐷 outputted by OBD block. a Is the normal case while b–d are abnormal cases

Full size image
Re-scoring and Post-processing
The fact that the detectors can sometimes output high confidence scores for false positive samples is a long-standing issue in the detection community for both generic objects and text. One possible reason for this may be that the scoring head used in most of the current literature is supervised by the softmax loss, which is designed for classification but not for explicit localization. Moreover, the classification score only considers whether the instance is foreground or background, and it shows less sensitivity to the compactness of the bounding box.

Therefore, a confidence RPP block, is proposed to suppress unreasonable false positives. Specifically, RPP adopts a policy similar to multiple expert systems to reduce the risk of outputting high scores for negative samples. In RPP, an OBD score 𝑆𝑂𝐵𝐷 is first calculated based on eight KEs (four x-KEs and four y-KEs):

𝑆𝑂𝐵𝐷=1𝐾∑𝑘=1𝐾max𝑣𝑘𝑓(𝑣𝑘),
(3)
where 𝐾=8 is the number of KEs, 𝑣𝑘 is the output score vector of the 𝑘𝑡ℎ KE shown in (4), and 𝑓(𝑣𝑘) is defined to sum up the peak value, 𝑣𝑖, and its neighbors. As shown in Fig. 7a, the distribution of 𝑆𝑂𝐵𝐷 demonstrates a one-peak pattern in most cases. Nonetheless, the peak value is still significantly lower than 1. Hence, we sum up four adjacent scores that are near the peak value for each KE score to avoid a confidence score that is too low.

𝑣𝑘=[𝑣1,𝑣2,…,𝑣𝑖−2,𝑣𝑖−1,𝑣𝑖,𝑣𝑖+1,𝑣𝑖+2𝑓(𝑣𝑘)=∑𝑃=𝑚𝑖𝑛(𝑛,𝑖+2)𝑝=𝑚𝑎𝑥(𝑖−2,1)(𝑣𝑝),…𝑣𝑛].
(4)
It is important to note that the number of adjacent values will be less than four if the peak value is located at the head or tail of the vector. Thus, only the existing neighbors should be counted. Finally, the refined confidence can be obtained by:

𝑠𝑐𝑜𝑟𝑒=(2−𝛾)𝑆𝑏𝑜𝑥+𝛾𝑆𝑂𝐵𝐷2,
(5)
where 0≤𝛾≤2 is the weighting coefficient and 𝑆𝑏𝑜𝑥 is the original softmax confidence for the bounding box. Because both 𝑆𝑏𝑜𝑥 and 𝑆𝑂𝐵𝐷 are both between [0,1], the value of 𝑠𝑐𝑜𝑟𝑒(ℜ) is also between [0, 1] . Counting the 𝑆𝑂𝐵𝐷 into the final score enables the proposed detector to draw lessons from multiple agents (eight KE scores) while enjoying the benefits of a tightness-aware confidence supervised by the KE prediction task.

Fig. 8
figure 8
Compared with the segmentation head, the proposed KE head predicts more compact bounding boxes and shows a higher recall rate for instances that were missed by segmentation. Colored quadrangles are the final detection results, whereas white transparent areas are the mask predictions grouped by the minimum area rectangle. (Color figure online)

Full size image
Discussion
It has been proven that the detection performance can be often boosted with the multi-task learning framework. For example, as shown in He et al. (2017c), simultaneously training a detection head with an instance segmentation head can significantly improve the detection accuracy. Similarly, a segmentation head is also employed in the proposed OBD network to predict the area inside the bounding box, which forces the model to regularize pixel-level features to enhance both performance and robustness. However, some issues associated with the segmentation head are highlighted in Fig. 8. In (a), the segmentation mask can sometimes produce false positive pixels while the OBD prediction remains correct. In (b), the segmentation head fails to maintain some positive samples that have been successfully detected by the OBD block. Therefore, compared with some segmentation-based approaches that directly reconstruct the bounding box by exploiting the segmentation mask, the MTL block can learn geometric constraints to avoid false positives caused by an inaccurate segmentation output. This also reduces the heavy reliance on the segmentation task. Specifically, as shown in Fig. 6b, the blue dashed line matches an invalid shape that violates the definition of a quadrilateral, because the sides should only have two intersections, at the head and tail. By simply removing these abnormal results, the MTL block can further eliminate some false positives that might cheat the segmentation branch.

Another interesting observation is that the RPP block exhibits a strong capability to suppress false positives, making predictions more reliable. To provide an analysis, we visualize the term 𝑆𝑂𝐵𝐷, which is used in the RPP block (see Equation (5)). Doing so, we find that there are two typical patterns for the KE scores output by the OBD block, as shown in Fig. 7. Sub-figure (a) shows a one-peak pattern, and sub-figure (b) shows a multi-peak pattern. In normal cases, the KE scores show a regular pattern, in which there is only one peak value in the output vector (see Fig. 7a). However, with hard negative samples, two or more peak values appear (see Fig. 7b–d). These multiple peaks share confidence, and the total score is normalized to one. Therefore, based on Equations (3) and (5), the final score will be decreased such that the proposed model is less likely to output high confidence for those false-positive instances.

Based on our observation, we find that the matching-type prediction could be wrong even if KE is accurate. An example is shown in the bottom instance of the lower-right corner image of Fig. 13b, where 𝑥𝑚𝑖𝑛 is mistakenly matched to 𝑦𝑚𝑖𝑛. If 𝑥𝑚𝑖𝑛 and the second smallest x change their matching y key edge, the detection result can be tighter. Although such a case does not obviously affect both the detection and recognition performance, it is an underlying weakness of the MTL. It is worth mentioning that sometimes the matching type may form an irregular bounding box, i.e., the sides have self-intersection. We find that such cases are very rare and mostly occur with false negatives. For such irregular results, we simply remove them.

Ablation Studies
Implementation Details
Our model is implemented using PyTorch. We first evaluate the proposed components of our methods. The initial learning rate is set to 0.01, which is decreased by 10 at 10,000 iterations and 15,000 iterations. The maximum iterations is 20,000 and the image batch size is set to 4. The shorter size of the input image is randomly scaled from 680 to 1000 with the interval of 40, while the maximum size is set to 1480. The weights of KE and matching type learning are set to 0.1 and 0.01, respectively. Flip, random crop, and random rotation are used to improve the generalization ability. Unless specified otherwise, the re-scoring ratio is kept to be 1.4.

For ablation studies of refinements, each experiment uses a single network that is a variation of our baseline model (first row of Table 5). Each network is trained on the official ReCTS training set unless specified otherwise. Additionally, because the test scale may significantly influence the final detection result, the testing max size is fixed at 2,000 pixels, and the scale is fixed to 1,400 pixels for strictly fair ablation experiments. The ratio of the flip is also fixed at 0.5, which is the flipping probability for deciding whether to horizontally flip the images for data augmentation. Results are reported on the validation set of ReCTS based on the widely used main performance metric, Hmean. We also report the best confidence threshold that leads to the best performance, which can also reveal some important information.

The number of iterations for training one network is set to 80,000 iterations, with a batch size of four images per GPU on four 1080ti GPUs. The final cumulative model is trained for 160 epochs on four V100 GPUs, which takes approximately 6 days. The baseline model employs ResNet-101-FPN as the backbone, which is initialized by a model pretrained on the MLT (Nayef et al. 2017) data. We only use fixed batch normalization for the stem and bottleneck, i.e., the batch statistics and the affine parameters are fixed. For all prediction heads, we do not use batch normalization.

Fig. 9
figure 9
Ablation study on the ICDAR 2015 benchmark. X-axis represents confidence threshold and Y-axis represents Hmean result. Baseline represents Mask R-CNN. By integrating with proposed OBD, the detection results can be substantially better than the results of the Mask R-CNN baseline

Full size image
Ablation Studies of the Proposed Method
In this section, we report ablation studies on the ICDAR 2015 (Karatzas et al. 2015) dataset, to validate the effectiveness of each component of our method. First, we evaluate the influence of the proposed modules on performance. The results are presented in Table 1 and Fig. 9. From Table 1, we can see that OBD and RPP can lead to improvements of 2.4 and 0.6%, respectively, in terms of Hmean. Additionally, Fig. 9 shows that our method can substantially outperform the baseline Mask R-CNN under different confidence thresholds, further demonstrating its effectiveness.

Furthermore, we conduct experiments by comparing the mask and KE branches (including OBD and RPP) on the same network. Thus, we test only on one of the branches. We simply use the provided training samples of IC15 without any data augmentation. The results are presented in Table 2, verifying that the proposed modules can effectively improve the scene text detection performance.

More importantly, we also conduct experiments to verify that introducing ambiguity in the training is harmful to achieving good results. Specifically, by using the same configuration, we first train Textboxes++ (Liao et al. 2018a), EAST (Zhou et al. 2017), CTD (Liu et al. 2019b), APE (Zhu et al. 2020) (the champion method of DOAI2019 competition task1), and the proposed method with the original 1,000 training images of the ICDAR 2015 dataset. Then, we randomly rotate the training images [0∘,15∘,30∘,…,360∘] and randomly select additional 2,000 images from the rotated dataset to fine-tune these models. We also randomly select additional 2,000 images that are between [−30∘,30∘] to evaluate the difference under lower rotation degree. The results are presented in Table 3. Our method can effectively address the inconsistent labeling issue without drastically degrading the accuracy. Furthermore, as shown in Table 4, our proposed method exhibit higher robustness under various degrees of rotation.

Note for the resnet-50 version and the following final competition version of our method, the inference time is 4.5 FPS and 0.83 FPS, respectively. The speed is tested using a single NVIDIA GTX 2080 Ti and the short size of the input image is scaled to 1000.

Table 3 Comparison on ICDAR 2015 dataset showing different methods’ ability of resistant to the inconsistent labeling issue (by adding rotated pseudo samples). TB: Textboxes++. LD: using lower rotation degrees
Full size table
Table 4 Hmean results under different rotation degrees on ICDAR 2015 dataset. The rotation angle represents the value used for the data augmentation during the training phase
Full size table
Ablation Studies of Refinements Based on Our Method
In this section, we provide a detailed analysis of the impact of refinements based on the proposed methods, to evaluate the limits of our method and whether it can be mutually promoted by existing modules. By combining effective refinements, our method achieves first place in the detection task of the ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboards.

In the following sections, we present an extensive set of experiments that rate our baseline model. Thus, we present results of OBD having alternative architectures and different strategies with respect to six relevant components for training, including data arrangement, pre-processing, backbone, proposal generation, prediction head, and post-processing.

The objective is to show that the proposed model corresponds to a local optimum in the space of architectures and parameters and to evaluate the sensitivity of the final performance to each design choice. The following discussions follow the structure of Table 5. Note that the significant breadth and exhaustibility of the following experiments represent more than 3,000 GPU hours of training time.

Competition Dataset
The competition dataset, Reading Chinese Text on Signboards (ReCTS), is a practical and challenging multi-orientation natural scene text dataset containing 25,000 signboard images. A total of 20,000 images are used for the training set, with a total of 166,952 text instances. The remaining 5,000 images are used for the test set. Examples of this dataset are shown in Fig. 10. The layout and arrangement of Chinese characters in this dataset are clearly different from those in other benchmarks. Because the function of a signboard is to attract a customer base, it is very common to notice their aesthetic appearance. Thus, the Chinese characters can be arranged in any kind of layout with various fonts. Additionally, characters from one word can be in diverse orientations, diverse fonts, or diverse shapes, which complicates the challenge. This dataset provides both text lines and character annotations to inspire new algorithms that can take advantage of the arrangement of characters. To evaluate the function of each component, we split the original training set into 18,000 training images and 2,000 validation images.

Fig. 10
figure 10
Example images of the ReCTS. Small, stacked multi-orientation, illumination, and annotation ambiguity are the main challenges for this dataset

Full size image
Table 5 Ablation studies of different refinements based on our method. Each variation is evaluated on the ReCTS validation set. It is worth mentioning that we regard difficult samples as true negatives in the validation because they cannot be recognized and only loosely annotated in the competition dataset. However, in the final ranking, detection box in the difficult region are set to “do not care”, which can result in a leap improvement. We evaluate variations of our baseline model (second row). Every row corresponds to one variation in different part. We train each variation with ResNet-101-FPN and fixed random seeds and equal 80,000 iterations (unless specifying) and report Hmean in the best confident threshold (grid search)
Full size table
Ablation Study of Data Arrangement
Considering the image diversity and the consistency and quality of annotation, we collected a 60,000-item dataset for pretraining, which consisted of 30,000 images from the LSVT (Sun et al. 2019) training set, 10,000 images from the MLT 2019 (Nayef et al. 2019) training set, and 5,603 images from ArT (Chng et al. 2019), which contained all the images from SCUT-CTW1500 (Liu et al. 2019b) and Total-text (Chng and Chan 2017; Ch’ng et al. 2019). The remaining 14,859 images were selected from RCTW-17 (Shi et al. 2017b), ICDAR 2015 (Karatzas et al. 2015), ICDAR 2013 (Karatzas et al. 2013), MSRA-TD500 (Yao et al. 2012), COCO-Text (Veit et al. 2016), and USTB-SV1K (Yin et al. 2015). Note that we transferred polygonal annotations to the minimum area rectangle for training.

The ablation results are presented in Table 5. If we only were to use the pretrained data without the split training data from the ReCTS, the result in the ReCTS validation set would be significantly worse than that of the baseline, even if the pretrained model were trained with more iterations. This is because the diversity and annotation granularity of the selected pretrained dataset is still very different from that of the ReCTS dataset. However, using the model trained with pretrained data is better than using the ImageNet model. For example, when directly using the ImageNet ResNet-101 model instead of the MLT pretrained model from the baseline method, the Hmean is reduced by 0.5%. Using the model having 60,000 pretrained data, followed by finetuning the model on the split ReCTS training data improved the result by 1.2% in terms of Hmean. To evaluate the importance of the data quality, we mimicked the manual annotation error by removing 5% of the training annotation instances and did not correct some samples with annotation ambiguity from the original ReCTS training data. The results indicate that using defective training data significantly degrades the performance.

Ablation Study of Pre-processing
Our baseline model used a pretrained model having only a flip strategy for data augmentation. We compared the baseline with various other data augmentation methods.

Cropping and rotation Without introducing extra parameters or training/testing times, the results presented in Table 5 verify that both rotation and data cropping augmentation strategies improved the detection results. We further conducted a sensitivity analysis of how the ratios of using these two strategies influence the performance, as shown in Fig. 11. Some useful findings can be derived from Fig. 11a, as summarized below.

With appropriate ratios, three rotated degrees (30∘, 15∘, and 5∘) outperformed the baseline method in most ratios, with 0.5, 0.6, and 0.4%, respectively.

Under a 0.1 rotated ratio, the performances with the three rotated degrees were all worse than the baseline. This may be because the pseudo samples changed the distribution of the original dataset, whereas very few pseudo samples were insufficient to improve the generalization ability. Conversely, the ratios to achieve the best results for various rotated degrees always lie between 0.3 and 0.8, which empirically suggests that using a medium ratio for the rotated data augmentation strategy might be a suitable choice.

We can also see that the performance using a rotated angle of 15∘ was consistently better than that with 30∘ and 5∘.

Compared with the rotated data augmentation strategy, the random cropping strategy significantly improved detection performance. The best performance, as shown in Table 5, achieved a 1.9% improvement in terms of Hmean, compared with the baseline method. Sensitivity analysis, as shown in Fig. 11b, was also conducted, revealing that, as the crop ratio improved, the performance also tended to improve. The result suggests that always using the crop strategy was conducive to improving the detection results. Note that a crop ratio of 0.1 only improved the Hmean by 0.5%, whereas other ratios improved it by more than 1%, which is similar to the phenomenon when using a rotated ratio 0.1.

Fig. 11
figure 11
Ablation studies of data augmentation strategies

Full size image
Color jittering We also conduct a simple ablation study to evaluate the performance of color jittering. Based on the same settings as of the baseline method, we empirically set the ratios of brightness, contrast, saturation, and hue to 0.5, 0.5, 0.5, and 0.1, respectively. The ratio represents the degree of disturbance of each specific transformation. The results in Table 5 indicate that using color jittering data augmentation slightly improved the result by 0.2% in terms of Hmean.

Training image scale The training image scale/size is specifically important for a scene text detection. To evaluate how the training scale influences the results of our method, we used two parameters (i.e., scale and MaxSize) to control the training scale. The first item resized the minimum side of the image to a specific parameter.

In our implementation, there are a set of values for random scaling. The second item restricts the maximum size of the image sides. The value of scale must be less than MaxSize, and the entire scaling process strictly retains the original aspect ratio. We primarily compare three different settings: the default training scale (scale: 560 to 920 with intervals of 40, MaxSize was 1,300); medium training scale (scale: 680 to 1,120 with intervals of 40, MaxSize was 1,800); and large training scale (scale 800 to 1,400 with intervals of 40, MaxSize was 2,560).

The results are presented in Table 6, which verify the following: 1) a larger training scale requires a larger testing scale for the best performance. 2) As the larger training scale increases, so does the performance. Note that, although a larger training scale can improve performance, it is costly and may require significantly more GPU memory.

Table 6 Ablation experiments for large scale training. Hmean1, Hmean2, and Hmean3 represent default training scale, medium training scale, and large training scale, respectively. The first row compares the performance based on the baseline setting. The other three rows are the best setting (using grid search to find the best scale and MaxSize) for each training scale
Full size table
Ablation Study of the Backbone
A well-known hypothesis is that a deeper and wider network architecture delivers better performance than does a shallower and thinner one. However, increasing the network depth naively will significantly increase the computational cost with only limited improvement. Therefore, we investigate different types of backbone architectures. The results are shown in Table 5 and are summarized as follows:

By changing the backbone, ResNet-101-FPN of the baseline model into a ResNeXt-152-32x8d-FPN-IN5k, Hmean can be increased by 2.5%. Note that the pretrained model of ResNeXt-152-32x8d-FPN-IN5k was pretrained on ImageNet using the Facebook Detectron framework.

Atrous spatial pyramid pooling (ASPP) (Chen et al. 2017) is effective in semantic segmentation, which is known for its function in increasing the receptive field. However, in this scene text detection task, using ASPP in the KE head or backbone reduced performance by 1.1 and 2.1%, respectively. One possible reason is that the change in network architecture usually requires more iterations. However, the best confidence thresholds for the best performance using ASPP were 0.91 and 0.89, which are similar to the best threshold of the baseline model, suggesting that the network had already converged.

Deformable convolution (Dai et al. 2017) is an effective module used for many tasks. It adds 2D offsets to the regular sampling grid of the standard convolution, allowing free form deformation of the convolutional operation. This is suitable for scene text detection, owing to the mutable characteristics of the text. We experimented with three methods of deformable convolution by adding deformable convolutions from the C4-1, C4-2, and C3 of the backbone, and the results show that the performance could be significantly improved by 2.6, 2.5, and 2.5%, respectively, in terms of Hmean.

Motivated by the panoptic feature pyramid networks (Kirillov et al. 2019), we also tested whether a panoptic segmentation loss was useful for scene text detection. To this end, we used a dice loss in the output of the FPN for panoptic segmentation, which had two classes: background and text. The result in Table 5 indicates that Hmean was reduced by 0.1%. However, the best threshold was 0.67, which indicates that the background noise may have somehow reduced the confidence of the training procedure.

The pyramid attention network (PAN) (Huang et al. 2019a) is a novel structure that combines an attention mechanism and a spatial pyramid to extract precise dense features for semantic segmentation tasks. Because it can effectively suppress false alarms caused by text-like backgrounds, we integrated it into the backbone and tested its function. The results show that using PAN led to a 1.2% improvement in terms of Hmean, but it also increased the computational cost with an increase of 2.4 GB video memory.

The multi-scale network (MSN) (Xue et al. 2019) is robust for scene text detection because it employs multiple network channels to extract and fuse features at different scales concurrently. In our experiment, integrating MSN into the backbone also increased the performance by 1.2% in terms of Hmean. Note that, compared with PAN, the recall of the MSN was much better under a higher best threshold, which suggests that different architectures may have had different functions related to the performance of the detector.

Ablation Study on Proposal Generation
The proposed model is based on a two-stage framework, and the region proposal network (RPN) (Ren et al. 2015) is used as the default proposal generation mechanism.

Previous studies have modified the anchor generation mechanism, including DMPNet (Liu and Jin 2017), DeRPN (Xie et al. 2019b), Kmeans anchor (Redmon and Farhadi 2017), scale-adaptive anchor (Li et al. 2019a), and guided anchor (Wang et al. 2019b), to improve the results. For simplicity, we retrain the default RPN structure with the statistical setting of the anchor box based on the training set.

The other important part in this proposal generation stage is the sampling process, (e.g., RoI pooling Ren et al. 2015, RoI align He et al. 2017c (our default setting), and PSRoI pooling Dai et al. 2016). We choose to evaluate Deformable PSRoI Pooling (Dai et al. 2017) for our method, because it has been effective for scene text detection (Yang et al. 2018), and the flexible process may be beneficial to the proposed OBD. The result is shown in Table 5: using deformable PSRoI Pooling improved the baseline method by 0.9% in terms of Hmean.

Table 7 Ablation results of using cascade r-cnn. cf: best threshold. R: recall. P: precision. H: Hmean
Full size table
Table 8 Ablation experiments for using character head. H: Hmean
Full size table
Fig. 12
figure 12
Ablation study of the testing scale. Note that the training scale is the default setting mentioned in Sect. 4.5

Full size image
Ablation Study on the Prediction Head
The final part of the two-stage detection framework is the prediction head. To clearly evaluate the effectiveness of the components, ablation experiments are separately conducted on different heads.

Box head Empirically, online hard negative examples mining (OHEM) (Shrivastava et al. 2016) is not always effective with respect to different benchmarks. For example, using the same framework minus the training data can significantly improve the results with the ICDAR 2015 benchmark (Karatzas et al. 2015) while reducing the results on the MLT benchmark (Nayef et al. 2017). This finding may be related to the data distribution, which is difficult to trace.

Thus, we test three versions of the OHEM in the validation set. The first version, OHEMv1, is the same as the original implementation; the second version, OHEMv2, simply ignores the top 5 hard examples to avoid outliers. These two versions have the same ratio, which is set to 0.25. The third version, OHEMv3, simply uses a higher ratio (0.5) to guarantee more hard samples and less easy samples. The results in Table 5 show that three versions all reduce Hmean, by 0.7, 0.8, and 0.5, respectively. Note that using OHEM will also result in the reduction of the best confidence, which means that the forced learning of hard examples can reduce the confidence of normal examples. Conversely, we also evaluated the performance of the cascade R-CNN, and the results are shown in Table 7. However, the results show that using a cascade does not result in further improvements.

Mask head To improve the mask head, we evaluate two methods (i.e., mask scoring Huang et al. 2019a), as shown in Table 5. The results show that modification of the mask head does not contribute to the detection performance. However, the mask prediction results are visually more compact and accurate compared with the baseline.

Character head It is well known that stronger supervision can result in better performance. Because the competition also provides a character ground truth, we build and evaluate the performance of an auxiliary character head. The implementation of the character head is exactly the same as that for the box head, except for the ground truth. Unlike the box, mask, and KE head, the proposed character head is built on a different RPN. Thus, the character head does not share the same proposal with the other heads. The KE head directly produces a quadrilateral bounding box (word box) directly used for the final detection, and we test whether the auxiliary head could indirectly (shared backbone) improve the word-box detection performance.

The ablation results in Table 8 demonstrate this idea, which shows that using a character head improved the Hmean by 0.7%. Additionally, if we add a mask prediction head to the character head (i.e., the mask character in Table 8), the result would remain the same. Moreover, we employ a triplet loss to learn the connection between the characters. The ground truth includes whether the characters belong to the same text instances. However, the improvement is decreased to 0.5%. This may be because the instance connection introduced an inconsistent labeling issue. We further test the performance using only the character head with an instance connection and without the KE head. Hmean is reduced by 3.9% compared with the baseline method, suggesting that using character as an auxiliary head instead of the final prediction head is a good choice.

Table 9 Ablation experiments for different approaches of model ensemble. ‘def’: deformable convolution
Full size table
Ablation Study of Post-processing
The last step is to apply post-processing methods for final improvement. To this end, we compare the baseline with a series of standard and more effective post-processing methods.

Polygonal non-maximize suppression (PNMS) Traditional non-maximum suppression (NMS) methods between horizontal rectangular bounding boxes can cause unnecessary suppression. Thus, we conduct ablation experiments to evaluate the performance of the PNMS. We use grid search to find the best threshold to find both NMS and PNMS for fair combination, which is 0.3 and 0.15, respectively. The result in Table 5 shows that using PNMS performs better than NMS by 0.8% in terms of Hmean. Additionally, PNMS is much more effective when using a test ensemble in practice.

Table 10 Experimental results for the ICDAR 2015 dataset. R: recall. P: precision
Full size table
Key edge RPP The proposed key edge RPP proved effective on the ICDAR 2015 benchmark. Thus, we also test whether it applies to the competition dataset. The ablation result in Table 5 shows that it slightly improves the Hmean by 0.1% compared with the baseline. It is worth noticing that, although the best confidence threshold is 0.91, which is the same as that of the baseline, the recall is increased by 0.4% while only reducing the precision by 0.2%.

Large-scale testing We also conduct experiments to evaluate how the testing scale influenced performance. The results are shown in Fig. 12, which demonstrates that a proper setting of scale and MaxSize significantly improves the detection performance. Additionally, the results reveal that there is a limitation of the MaxSize. That is, if the value of MaxSize is higher than a certain value, the performance would be gradually reduced.

Test ensemble To evaluate the performance of the test ensemble, we conduct ablation experiments with four different aspects: different backbone ensemble; multiple intermediate model ensemble; a multi-scale ensemble; and an independent model ensemble. Note that, to achieve the best performance, implementing ensemble or multi-scale testing requires some tricks. Otherwise, the results may be worse. We summarize the results as follows:

Using a high confidence threshold. One weakness of multi-scale ensembling is that if a true-negative detection exists in one of the testing scales, it cannot be avoided unless we set a high confidence threshold to exclude it during the ensemble phase. Therefore, for each scale, we first test its best confidence threshold (cf) on the validation set. Then, we use a higher confidence for the model ensemble.

Variant scale of multi-scale testing. The performance of small scale (600 (scale), 1200 (MaxSize)) is rated. For example, in the ReCTs competition, it is much worse than that of large-scale (1,600, 1,600). However, small scales are better for detecting large instances compared with large scales, and they can always be mutually promoted in practice.

Using a strict PNMS threshold. A normal case for the ensemble result is that the recall can be significantly improved, whereas the prediction is dramatically reduced. When observing the final integrated detection boxes, it is easy to find that the reduction was caused by boxes-in-boxes and many stacked redundant boxes. Using a strict PNMS can effectively solve this issue.

Based on these principles, we conclude the results of the four ensemble aspects as follows.

Different backbone ensembles. We train three models using the baseline setting with three types of deformable convolution, starting from C4-1, C4-2, and C3 of ResNet-101, respectively. The ensemble results of the three methods are shown in Table 9. From the table, we can see that integrating the models with a series of simple backbone modifications improved the detection performance, even based on a relatively high baseline. Additionally, the results show that integrating more components resulted in better performance.

Multiple intermediate model ensembles. We also evaluate the performance of integrating intermediate models. We use the trained model with the ResNext-152 backbone as a strong baseline and selected the last three intermediate iterating models with 10,000 iterations as intervals for the ensemble. The results shown in Table 9 also demonstrate that when using the model ensemble, the intermediate models could be mutually promoted.

Multi-scale ensemble. To evaluate the performance of the multi-scale ensemble, we use grid searching to find the best PNMS threshold for three specified settings (scale, MaxSize), representing large, medium, and small text instances, respectively. Each detection result was then integrated with a PNMS threshold 0.02 higher than the original best threshold, which resulted in approximate optimum integrating results with 0.6% improvement in terms of Hmean, as shown in Table 9.

Independent model ensembles. Finally, we test the performance of integrating the two final models. The first model contains the baseline setting plus deformable convolution, and the second model contains the baseline setting with the ResNext-152 backbone. We independently integrate each model using an intermediate model ensemble and a multi-scale ensemble. Then, we assemble the final results of the two models. As shown in Table 9, the detection result can still be improved.

Table 11 Experimental results for the MLT dataset. SS represents a single scale. R: recall. P: precision. Note that we only used a single scale for all experiments
Full size table
Table 12 Competition results on the ReCTS dataset. The results are from the competition website https://tinyurl.com/ReCTS2019. For the detection task, the ranking is based on Hmean. For End-to-End detection and recognition task, the ranking is based on 1-NED. NED: normalized edit distance
Full size table
Comparison with State-of-the-Art Methods
To further evaluate the effectiveness of the proposed method, we carry out experiments and compare our final model with other state-of-the-art methods on three scene text datasets: ICDAR 2015 (Karatzas et al. 2015), MLT (Nayef et al. 2017), and ReCTS (See Sect. 4.3.1). We also conduct an experiment on one aerial dataset, HRSC2016 (Liu et al. 2017), to further demonstrate the generalization ability of our method.

Table 13 Experimental results for HRSC_2016 dataset. cf: confidence threshold, which is set to 0.01 in the last line
Full size table
Fig. 13
figure 13
Visualization of the qualitative results outputted by the proposed approach

Full size image
Fig. 14
figure 14
Qualitative detection results on the HRSC2016 dataset

Full size image
Final model. The final model is designed by combine the effective modules evaluated in Table 5. Specifically, based on the baseline setting, we refine our model in all six aspects. During the data arrangement stage, we use 60,000 pretrained data items to train a pretrained model for 200,000 iterations, and we then use the original training data of each dataset for finetuning. In the pre-processing part, apart from the baseline setting, we also apply color jittering, random cropping, and random rotation with their best ratios as evaluated on the validation dataset for data augmentation. Additionally, the images are trained with a medium setting of the random scale training for maximizing the utilization of the video memory. For the backbone setting, we integrate the ResNext-152-32x8d-FPN-IN5k model, deformable convolution (C4-2), PAN, and MSN modules together to construct a powerful feature extractor. During the proposal generation stage, we adopt deformable PSROI pooling for feature alignment, whereas in the prediction head, we only add an auxiliary character head for mutual promotion using only the ReCTS dataset. Finally, in the post-processing stage, we utilize all effective settings, including polygonal non-maximum suppression, key edge RPP, intermediate model ensemble, and multi-scale ensemble. Note that we also find the sync BN (Zhang et al. 2018) can improve the text detection accuracy (by 1.1% compared to the baseline in Table 5) lately, but it was not integrated in the final model.

The ICDAR 2015 Incidental Scene Text (Karatzas et al. 2015) is one of the most popular benchmarks for oriented scene text detection. The images are incidentally captured mostly from streets and shopping malls. Thus, the challenges of this dataset rely on oriented, small, and low-resolution text. This dataset contains 1,000 training samples and 500 testing samples with approximately 2,000 content-recognizable quadrilateral word-level bounding boxes. The results of ICDAR 2015 are given in Table 10. From this table, it is clear that our method outperformed all previous methods.

The ICDAR 2017 MLT (Nayef et al. 2017) is the largest multi-lingual (nine languages) oriented scene text dataset, including 7,200 training samples, 1,800 validation samples, and 9,000 testing samples. The challenges associated with this dataset are manifold. Different languages have different annotating styles. For example, most Chinese annotations are long, and there is no specific word interval for sentences. However, most English annotations are short. The annotations of Bangla or Arabic may be frequently entwined with each other, and there is more multi-orientation, perspective distorted text on various complex backgrounds. Furthermore, many images have more than 50 text instances. All instances are well annotated with compact quadrangles. As shown in Table 11, the proposed approach achieved the best performance on the MLT dataset.

ReCTS is the recent ICDAR 2019 Robust Reading ChallengeFootnote1 described in Sect. 4.3.1. Competitors were restricted to submitting at most five results, and all results were evaluated after the deadline. The competition attracted numerous competitors from well-known universities and high-tech companies. The results of the ReCTS are shown in Table 12. Our method won first place in the ReCTS detection competition. To clearly evaluate the performance of the final model, we also provide the results of our method on the ReCTS validation set without using a model ensemble. As shown in Table5, the final model significantly outperformed the baseline by 7.1% in terms of Hmean.

ReCTS End-to-End. One of the main goals of scene text detection is to recognize a text instance (Xie et al. 2019c) that is highly related to the performance of the detection system. To validate the effectiveness and robustness of our detection method, we build a recognition system that incorporate several state-of-the-art methods. Typically, the recognition performance is highly relevant to the quality of the detected boxes. To reveal the precision of our detection, we construct an end-to-end recognition system to demonstrate how our method benefits recognition models. We first crop the images using detected boxes and fed them into four popular recognition models, including decouple attention network (Tianwei et al. 2020), convolutional recurrent neural network (Shi et al. 2017c), network of show, attend, read (Li et al. 2019b), and transformer-based networks (Wang et al. 2019e). The four models are trained on real samples and 600,000 extra synthetic samples following their default settings for training. The real samples are provided by the official training set, whereas the synthetic samples are synthesized using a render engine (Jaderberg et al. 2016) and the corpus of the official training set. All images are resized to a specific required height for each recognition model while maintaining the aspect ratio of the original image. In a data batch, all images are padded with white to the maximum width of the images. During the inference stage, we choose the prediction having the highest confidence as the final ensemble result. Both quantitative and qualitative results are presented in Table 12 and Fig. 13b, respectively.

HRSC2016. To demonstrate the generalization ability of our method, we further evaluate its performance on the Level-1 task of the HRSC2016 dataset (Liu et al. 2017) to demonstrate multi-directional object detection. The ship instances in this dataset are presented in various orientations, and the annotating bounding boxes are based on rotated rectangles. There were 436, 181, and 444 images for training, validating, and testing, respectively. Only the training and validation sets are used for training. The evaluation metric is the same as in Liu et al. (2019a), Karatzas et al. (2015). The result is shown in Table 13, showing a significant improvement over the TIoU-Hmean (Liu et al. 2019d). It also demonstrates the robustness of our method. Qualitative examples of the detection results are shown in Fig. 14.

Conclusion
In this paper, we have addressed multi-orientation scene text detection using an effective OBD method. Using discretization methodology, OBD, can solve the inconsistent labeling issue by discretizing the point-wise prediction into orderless key edges. To decode accurate vertex positions, we have proposed a simple but effective MTL method to reconstruct the quadrilateral bounding box. Benefiting from OBD, we improve the reliability of the confidence of the bounding box and adopted more effective post-processing methods to improve performance.

Additionally, based on our method, we have conducted thorough ablation studies on six training components, including data arrangement, pre-processing, backbone, proposal generation, prediction head, and post-processing, to explore the potential upper limit of our method. By combining effective modules, we have achieved state-of-the-art results on various benchmarks and won the first place in the recent ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboards. Moreover, using a recognition model, we perform the best in the end-to-end detection and recognition task, verifying that our method is conducive to current recognition methods. To test the generalization ability, we have conducted an experiment on an oriented general object dataset HRSC2016; the results verify that our method can significantly outperform recent state-of-the-art methods.