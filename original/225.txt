Student engagement is a key concept in contemporary education, where it is valued as a goal in its own right. In this paper we explore approaches for automatic recognition of engagement from students' facial expressions. We studied whether human observers can reliably judge engagement from the face; analyzed the signals observers use to make these judgments; and automated the process using machine learning. We found that human observers reliably agree when discriminating low versus high degrees of engagement (Cohen's κ = 0.96). When fine discrimination is required (four distinct levels) the reliability decreases, but is still quite high ( κ = 0.56). Furthermore, we found that engagement labels of 10-second video clips can be reliably predicted from the average labels of their constituent frames (Pearson r=0.85), suggesting that static expressions contain the bulk of the information used by observers. We used machine learning to develop automatic engagement detectors and found that for binary classification (e.g., high engagement versus low engagement), automated engagement detectors perform with comparable accuracy to humans. Finally, we show that both human and automatic engagement judgments correlate with task performance. In our experiment, student post-test performance was predicted with comparable accuracy from engagement labels ( r=0.47) as from pre-test scores ( r=0.44).

SECTION 1.Introduction
The test of successful education is not the amount of knowledge that pupils take away from school, but their appetite to know and their capacity to learn. Sir Richard Livingstone, 1941 [36].

Student engagement has been a key topic in the education literature since the 1980s. Early interest in engagement was driven in part by concerns about large drop-out rates and by statistics indicating that many students, estimated between 25 and 60 percent, reported being chronically bored and disengaged in the classroom [32] [51]. Statistics such as these led educational institutions to treat student engagement not just as a tool for improving grades but as an independent goal unto itself [16]. Nowadays, fostering student engagement is relevant not just in traditional classrooms but also in other learning settings such as educational games, intelligent tutoring systems (ITS) [4]–[30][43][52], and massively open online courses (MOOCs).

The education research community has developed various taxonomies for describing student engagement. Fredricks et al. [20] analyzed 44 studies and proposed that there are three different forms of engagement: behavioral, emotional, and cognitive. Anderson et al. [3] organized engagement into behavioral, academic, cognitive, and psychological dimensions. The term behavioral engagement is typically used to describe the student's willingness to participate in the learning process, e.g., attend class, stay on task, submit required work, and follow the teacher's direction. Emotional engagement describes a student's emotional attitude towards learning—it is possible, for example, for students to perform their assigned work well, but still dislike or be bored by it. Such students would have high behavioral engagement but low emotional engagement. Cognitive engagement refers to learning in a way that maximizes a person's cognitive abilities, including focused attention, memory, and creative thinking [3].

The goal of increasing student engagement has motivated the interest in methods to measure it [25]. Currently the more popular tools for measuring engagement include: (1) Self-reports, (2) Observational checklists and ratings scales, and (3) Automated measurements.

Self-reports. Self-reports are questionnaires in which students report their own level of attention, distraction, excitement, or boredom [14]–[24][45]. These surveys need not directly ask the students explicitly how “engaged” they feel but instead can infer engagement as an explanatory latent variable from the survey responses, e.g., using factor analysis [41]. Self-reports are undoubtedly useful. For example, it is of interest to know that between 25 and 60 percent of middle school students report to be bored and disengaged [32] [51]. Yet self-reports also have well-known limitations. For example, some students may think it is “cool” to say they are non-engaged; other students may think it is embarrassing to say so. Self-reports may be biased by primacy and recency memory effects. Students may also differ dramatically in their own sense of what it means to be engaged.

Observational checklists and rating scales. Another popular way to measure engagement relies on questionnaires completed by external observers such as teachers. These questionnaires may ask the teacher's subjective opinion of how engaged their students are. They may also contain checklists for objective measures that are supposed to indicate engagement. For example, do the students sit quietly? Do they do their homework? Are they on time? Do they ask questions? [48]. In some cases, external observers may rate engagement based on live or pre-recorded videos of educational activities [29] [46]. Observers may also consider samples of the student's work such as essays, projects, and class notes [48].

While both self-reports and observational checklists and ratings are useful, they are still very primitive: they lack temporal resolution, they require a great deal of time and effort from students and observers, and they are not always clearly related to engagement. For example, engagement metrics such as “sitting quietly”, “good behavior”, and “no tardy cards” appear to measure compliance and willingness to adhere to rules and regulations rather than engagement per se.

Automated measurements. The intelligent tutoring systems community has pioneered the use of automated, real-time measures of engagement. A popular technique for estimating engagement in ITS is based on the timing and accuracy of students’ responses to practice problems and test questions. This technique has been dubbed “engagement tracing” [8] in analogy to the standard “knowledge tracing” technique used in many ITS [30]. For instance, chance performance on easy questions or very short response times might be used as an indication that the student is not engaged and is simply giving random answers to questions without any effort. Probabilistic inference can be used to assess whether the observed patterns of time/accuracy are more consistent with an engaged or a disengaged student [8] [26].

Another class of automated engagement measurement is based on physiological and neurological sensor readings. In the neuroscience literature, engagement is typically equated with level of arousal or alertness. Physiological measures such as EEG, blood pressure, heart rate, or galvanic skin response have been used to measure engagement and alertness [9]–[18][23][39][49]. However, these measures require specialized sensors and are difficult to use in large-scale studies.

A third kind of automatic engagement recognition—which is the subject of this paper—is based on computer vision. Computer vision offers the prospect of unobtrusively estimating a student's engagement by analyzing cues from the face [10]–[11][29][42], body posture and hand gestures [24] [29]. While vision-based methods for engagement measurement have been pursued previously by the ITS community, much work remains to be done before automatic systems are practical in a wide variety of settings.

If successful, a real-time student engagement recognition system could have a wide range of applications: (1) Automatic tutoring systems could use real-time engagement signals to adjust their teaching strategy the way good teachers do. So-called affect-sensitive ITS are a hot topic in the ITS research community [5]–[12][13][19][26][59], and some of the first fully-automated closed-loop ITS that use affective sensors for feedback are starting to emerge [12] [59]. (2) Human teachers in distance-learning environments could get real-time feedback about the level of engagement of their audience. (3) Audience responses to educational videos could be used automatically to identify the parts of the video when the audience becomes disengaged and to change them appropriately. (4) Educational researchers could acquire large amounts of data to data-mine the causes and variables that affect student engagement. These data would have very high temporal resolution when compared to self-report and questionnaires. (5) Educational institutions could monitor student engagement and intervene before it is too late.

Contributions. In this paper we document one of the most thorough studies to-date of computer vision techniques for automatic student engagement recognition. In particular, we study techniques for data annotation, including the timescale of labeling; we compare state-of-the-art computer vision algorithms for automatic engagement detection; and we investigate correlations of engagement with task performance.

Conceptualization of engagement. Our goal is to estimate perceived engagement, i.e., student engagement as judged by an external observer. The underlying logic is that since teachers rely on perceived engagement to adapt their teaching behavior, then automating perceived engagement is likely to be useful for a wide range of educational applications. We hypothesize that a good deal of the information used by humans to make engagement judgements is based on the student's face.

Our paper is organized as follows: First we study whether human observers reliably agree with each other when estimating student engagement from facial expressions. Next we use machine learning methods to develop automatic engagement detectors. We investigate which signals are used by the automatic detectors and by humans when making engagement judgments. Finally, we investigate whether human and automated engagement judgments correlate with task performance.

SECTION 2.Data Set Collection and Annotation for an Automatic Engagement Classifier
The data for this study were collected from 34 undergraduate students who participated in a “Cognitive Skills Training” experiment that we conducted in 2010-2011 [58]. The purpose of this experiment was to measure the importance to teaching of seeing the student's face. In the experiment, video and synchronized task performance data were collected from subjects interacting with cognitive skills training software. Cognitive skills training has generated substantial interest in recent years; the goal is to boost students’ academic performance by first improving basic skills such as memory, processing speed, and logic and reasoning. A few prominent systems include Brainskills (by Learning RX [1]) and FastForWord (by Scientific Learning [2]). The Cognitive Skills Training experiment utilized custom-built cognitive skills training software (reminiscent of BrainSkills) that we developed at our laboratory and installed on an Apple iPad. A webcam was used to videorecord the students; it was placed immediately behind the iPad and aimed directly at the student's face.

The game software in the experiment consisted of three games—Set, Remember, and Sum—that trained logical, reasoning, perceptual, and memory skills. The games were designed to be mentally taxing. Hard time limits were imposed on each round of the games, and the human trainers who controlled the game software (in either the Wizard-of-Oz or 1-on-1 conditions, as described below) were instructed to “push” students to perform the task more quickly. In this sense, the cognitive skills training domain of our experiment might resemble a setting in which a student is taking a stressful exam. In terms of physical environment, typical ITS and the cognitive skills setting in our study are very similar—a student sits directly in front of a computer or iPad, and a web camera retrieves frontal video of the student. It is possible that the appearance of affective states such as engagement might differ between cognitive skills training and ITS interactions. Nevertheless, it is likely that the methodology of labeling and the computer vision techniques for training automated classifiers could still generalize to more traditional ITS use cases.

The dependent variables during the 2010-2011 experiment were pre- and post-test performance on the Set game. The “Set” game in our study (see Fig. 1 right) was very similar to the classic card game: the student is shown a board of nine cards, each of which can vary along three dimensions: size, shape, and color. The objective is to form as many valid sets of three cards in the time allotted as possible. A set is valid if and only if the three cards in the set are either all the same or all different for each dimension. After forming a valid set, the three cards in that set are removed from the board, and three new cards are dealt. This process then continues until the time elapses.


Fig. 1. Left: Experimental setup in which the subject plays cognitive skills training software on an iPad. Behind the iPad is a web camera that records the session. Right: The “Set” game in the cognitive skills training experiment that elicited various levels of engagement.
Show All

Experimental data for the engagement study in this paper were taken from 34 subjects from two pools: (a) the 26 subjects who participated in the Spring 2011 version of the Cognitive Skills Training study at a Historically Black College/University (HBCU) in the southern United States. All of these subjects were African-American, and 20 were female. Additional data were collected from (b) the eight subjects who participated in the Summer 2011 version of the Cognitive Skills Training study at a University in California (UC), all of whom were either Asian-American or Caucasian-American, and five of whom were female. The game software used in the UC data set was identical to the software used in the HBCU except for minor differences in game parameters (e.g., how fast cards are dealt). For the present study, the HBCU data served as the primary data source for training and testing the engagement recognizer. The UC data set allowed us to assess how well the trained system would generalize to subjects of a different race—a known issue in modern computer vision systems.

In the experimental setup, each subject sat in a private room and played the cognitive skills training software either alone or together with the experimenter. The iPad was placed on a stand and horizontally situated approximately 30 centimeters in front of the subject's face and vertically so that the iPad was slightly below eye level. Behind the iPad pointing towards the subject was a Logitech web camera that recorded the entire session. As described in [58], each subject was assigned either to a Wizard-of-Oz condition or a 1-on-1 condition. In the Wizard-of-Oz condition, the subject sat alone in the room while interacting with the game software, which was controlled remotely by a human wizard who could watch the student in real time. In the 1-on-1 condition, the subject played the games alongside a human trainer who would control the software overtly. In the HBCU data set, 20 subjects were in the Wizard-of-Oz condition and 6 subjects were in the 1-on-1 condition. In the UC data set, all subjects were in the Wizard-of-Oz condition.

During each session, the subject gave informed consent and then watched a 3 minute video on the iPad explaining the objectives of the three games and how to play them. The subject then took a 3 minute pre-test on the Set game to measure baseline performance. Test performance was measured as the number of valid “sets” of three cards (according to the game rules) that the student could form within 3 minutes. The particular cards dealt during testing were the same for all subjects. After the pre-test, the subject then underwent 35 minutes of cognitive skills training using the training software. The trainer's goal (in both the Wizard-of-Oz and 1-on-1 conditions) was to help the student maximize his/her test performance on Set. During the training session, the trainer could change the task difficulty, switch tasks, and provide motivational prompts. After the training period, the subject took a post-test on Set and then was done.

2.1. Data Annotation
Given the recorded videos of the cognitive training sessions, the next step was to label them for engagement. We organized a team of labelers consisting of undergraduate and graduate students from computer science, cognitive science, and psychology from the two universities where data were collected. These labelers viewed and rated the videos for the appearance of engagement. Note that not all labelers labeled the exact same sets of images/videos. Instead, we chose to balance the goals of obtaining many labels per image/video, and annotating a large amount of data for developing an automated detector. When labeling videos, the audio was turned off, and labelers were instructed to label engagement based only on appearance.

In contrast to the more thoroughly studied domains of automatic basic emotion recognition (happy, sad, angry, disgusted, fearful, surprised, or neutral) [6]–[33][61] or facial action unit classification [7]–[27][37][47] (from the Facial Action Coding System [17]), affective states that are relevant to learning such as frustration or engagement may be difficult to define clearly [50]. Hence, arriving at a sufficiently clear definition and devising an appropriate labeling procedure, including the timescale at which labeling takes place, is important for ensuring both the reliability and validity of the training labels [50]. In pilot experimentation we tried three different approaches to labeling:

Watching video clips (at normal viewing speed) and giving continuous engagement labels by pressing the the Up/Down arrow keys.

Watching video clips and giving a single number to rate the entire video.

Viewing static images and giving a single number to rate each image.

We found approach (1) very difficult to execute in practice. One problem was the tendency to habituate to each subject's recent level of engagement, and to adjust the current rating relative to that subject's average engagement level of the recent past. This could yield labels that are not directly comparable between subjects or even within subjects. Another problem was how to rate short events, e.g., brief eye closure or looks to the side: should these brief moments be labeled as “non-engagement”, or should they be overlooked as normal behavior if the subject otherwise appears highly engaged? Finally, it was difficult to provide continuous labels that were synchronized in time with the video; proper synchronization would require first scanning the video for interesting events, and then re-watching it and carefully adjusting the engagement up or down at each moment in time. We found the labeling task was easier using approaches (2) and (3), provided that clear instructions were given as to what constitutes “engagement”.

2.2. Engagement Categories and Instructions
Given the approach of giving a single engagement number to an entire video clip or image, we decided on the following approximate scale to rate engagement:

Not engaged at all—e.g., looking away from computer and obviously not thinking about task, eyes completely closed.

Nominally engaged—e.g., eyes barely open, clearly not “into” the task.

Engaged in task—student requires no admonition to “stay on task”.

Very engaged—student could be “commended” for his/her level of engagement in task.

The clip/frame was very unclear, or contains no person at all.

Example images for each engagement level are shown in Fig. 2. Note that these guidelines pertain certainly to “behavioral engagement” [20] but they also contain elements of cognitive and emotional engagement. For example, whether or not a student is “into” the task is related to her attitude towards the learning task. Also, in our definitions above, the distinction between engagement levels 3 and 4 is related to the student's motivational state.


Fig. 2. Sample faces for each engagement level from the HBCU subjects. All subjects gave written consent to publication of their face images.
Show All

Labelers were instructed to label clips/images for “How engaged does the subject appear to be”. The key here is the word appear—we purposely did not want labelers to try to infer what was “really” going on inside the students’ brains because this left the labeling problem too open-ended. This has the consequence that, if a subject blinked, then he/she was labeled as very non-engaged (Engagement =1) because, at that instant, he/she appeared to be non-engaged. In practice, we found that this made the labeling task clearer to the labelers and still yielded informative engagement labels. If the engagement scores of multiple frames are averaged over the course of a video clip (see Section 2.4), momentary blinks will not greatly affect the average score anyway. In addition, labelers were told to judge engagement based on the knowledge that subjects were interacting with training software on an iPad directly in front of them. Any gaze around the room or to another person (i.e., the experimenter) should be considered non-engagement (rating of 1) because it implied the subject was not engaging with the iPad. (Such moments occurred at the very beginning or very end of each session when the experimenter was setting up or tearing down the experiment.) The goal here was to help the system generalize to a variety of settings where students should be looking directly in front of them.

2.3. Timescale
An important variable in annotating video is the timescale at which labeling takes place. For approach (2) (described in Section 2.1), we experimented with two different time scales: clips of 60 sec and clips of 10 sec. Approach (3) (single images) can be seen as the lower limit of the length of a video clip. In a pilot experiment we compared these three timescales for inter-coder reliability. As performance metric we used Cohen's κ (see Appendix for more details). Since the engagement labels belong to an ordinal scale ({1,2,3,4}) and are not simply categories, we used a weighted κ with quadratic weights to penalize label disagreement.

For the 60 sec labeling task, all the video sessions (∼45 minutes/subject) from the HBCU subjects were watched from start to end in 60 sec clips, and two labelers entered a single engagement score after viewing each clip. For the 10 sec labeling task, 505 video clips of 10 sec each were extracted at random timepoints from the session videos and shown to seven labelers in random order (in terms of both time and subject). Between the 60 sec clips and the 10 sec labeling tasks, we found the 10 sec labeling task more intuitive. When viewing the longer clips, it was difficult to know what label to give if the subject appeared non-engaged early on but appeared highly engaged at the end. The inter-coder reliability of the 60 sec clip labeling task was κ=0.39 (across two labelers); for the 10 sec clip labeling task κ=0.68 (across seven labelers).

For approach (3), we created custom labeling software in which seven labelers annotated batches of 100 images each. The images for each batch were video frames extracted at random timepoints from the session videos. Each batch contained a random set of images spanning multiple timepoints from multiple subjects. Labelers rated each image individually but could view many images and their assigned labels simultaneously on the screen. The labeling software also provided a Sort button to sort the images in ascending order by their engagement label. In practice, we found this to be an intuitive and efficient method of labeling images for the appearance of engagement. The inter-coder reliability for image-based labeling was κ=0.56. This reliability can also be increased by averaging frame-based labels across multiple frames that are consecutive in time (see Section 2.4).

2.4. Static versus Motion Information
One interesting question is how much information about students’ engagement is captured in the static pixels of the individual video frames compared to the dynamics of the motion. We conducted a pilot study to examine this question. In particular, we randomly selected 120 video clips (10 sec each) from the set of all HBCU videos. The random sample contained clips from 24 subjects. Each clip was then split into 40 frames spaced 0.25 sec apart. These frames were then shuffled both in time and across subjects. A human labeler labeled these image frames for the appearance of engagement, as described in “approach (3)” of Section 2.1. Finally, the engagement values assigned to all the frames for a particular clip were reassembled and averaged; this average served as an estimate of the “true” engagement score given by that same labeler when viewing that video clip as described in “approach (2)” above. We found that, with respect to the true engagement scores, the estimated scores gave a κ=0.78 and a Pearson correlation r=0.85. This accuracy is quite high and suggests that most of the information about the appearance of engagement is contained in the static pixels, not the motion per se.

We also examined the video clips in which the reconstructed engagement scores differed the most from the true scores. In particular, we ranked the 120 labeled video clips in decreasing order of absolute deviation of the estimated label (by averaging the frame-based labels) from the “true” label given to the video clip viewed as a whole. We then examined these clips and attempted to explain the discrepancy: In the first clip (greatest absolute deviation), the subject was swaying her head from side to side as if listening to music (although she was not). It is likely that the coder treated this as non-engaged behavior. This behavior may be difficult to capture from static frame judgments. However, it was also an anomalous case.

In the second clip, the subject turned his head to the side to look at the experimenter, who was talking to him for several seconds. In the frame-level judgments, this was perceived as off-task, and hence non-engaged behavior; this corresponds to the instructions given to the coders that they rate engagement under the assumption that the subject should always be looking towards the iPad. For the video clip label, however, the coder judged the student to be highly engaged because he was intently listening to the experimenter. This is an example of inconsistency on the part of the coder as to what constitutes engagement and does not necessarily indicate a problem with splitting the clips into frames.

Finally, in several clips the subjects sometimes shifted their eye gaze downward to look at the bottom of the iPad screen. At a frame level, it was difficult to distinguish the subject looking at the bottom of the iPad from the subject looking to his/her own lap or even closing his/her eyes, both of which would be considered non-engagement. From video, it was easier to distinguish these behaviors from the context. However, these downward gaze events were rare and can be effectively filtered out by simple averaging.

In spite of these problems, the relatively high accuracy of estimating video-based labels from frame-based labels suggests an approach for how to construct an automatic classifier of engagement: Instead of analyzing video clips as video, break them up into their video frames, and then combine engagement estimates for each frame. We used this approach to label both the HBCU and the UC data for engagement. In the next section, we describe our proposed architecture for automatic engagement recognition based on this frame-by-frame design.

SECTION 3.Automatic Recognition Architectures
Based on the finding from Section 2.4 that video clip-based labels can be estimated with high fidelity simply by averaging frame-based labels, we focus our study on frame-by-frame recognition of student engagement. This means that many techniques developed for emotion and facial action unit classification can be applied to the engagement recognition problem. In this paper we proposed a three-stage pipeline.

Face registration: The face and facial landmark (eyes, nose, and mouth) positions are localized automatically in the image; the face box coordinates are computed; and the face patch is cropped from the image [35]. We experimented with 36×36 and 48×48 pixel face resolution.

The cropped face patch is classified by four binary classifiers, one for each engagement category l∈{1,2,3,4}.

The outputs of the binary classifiers are fed to a regressor to estimate the image's engagement level.

Stage (1) is standard for automatic face analysis, and our particular approach is described in [35]. Stage (2) is discussed in the next subsection, and stage (3) is discussed in Section 3.1.1. This architecture is reminiscent of an automated head pose estimation system we developed previously [57], which combines the outputs of multiple binary classifiers to form a real-valued judgment.

3.1. Binary Classification
We trained four binary classifiers of engagement—one for each of the four levels described in Section 2.1. The task of each of these classifiers is to discriminate an image (or video frame) that belongs to engagement level l from an image that belongs to some other engagement level l′≠l. We call these detectors 1-v-other, 2-v-other, etc. We compared three commonly used and demonstrably effective feature type + classifier combinations from the automatic facial expression recognition literature:

GentleBoost with Box Filter features (Boost(BF)): this is the approach popularized Viola and Jones in [53] for face detection.

Support vector machines with Gabor features (SVM(Gabor)): this approach has achieved some of the highest accuracies in the literature for facial action and basic emotion classification [35].

Multinomial logistic regression with expression outputs from the Computer Expression Recognition Toolbox [35] (MLR(CERT)): here, we attempt to harness an existing automated system for facial expression analysis to train engagement classifiers.

Our goal is not to judge the effectiveness of each feature type (or each learning method) in isolation, but rather to assess the effectiveness of these state-of-the-art computer vision architectures for a novel vision task. As relatively little research has yet examined how to recognize the emotional states specific to students in real learning environments, it is an open question how well these methods would perform for engagement recognition. We describe each approach in more detail below.

3.1.1. Boost(BF)
Box Filter features measure differences in average pixel intensity between neighboring rectangular regions of an image. They have been shown to be highly effective for automatic face detection [53] as well as smile detection [56]. For example, for detecting faces, a 2-rectangle Box Filter can capture the fact that the eye region of the face is typically darker than the upper cheeks. At run-time, BF features are fast to extract using the “integral image” technique [53]. At training time, however, the number of BF features relative to the image resolution is very high compared to other image representations (e.g., a Gabor decomposition), which can lead to overfitting. BF features are typically combined with a boosted classifier such as Adaboost [21] or GentleBoost (Boost) [22], which performs both feature selection during training and actual classification at run-time. In our GentleBoost implementation, each weak learner consists of a non-parametric regressor smoothed with a Gaussian kernel of bandwidth σ, to estimate the log-likelihood ratio of the class label given the feature value. Each GentleBoost classifier was trained for 100 boosting rounds. For the features, we included six types of Box Filters in total, comprising two-, three-, and four-rectangle features similar to those used in [53], and an additional two-rectangle “center-surround” feature (see Fig. 3). At a face image resolution of 48×48 pixels, there were 5,397,601 BF features; at a face resolution of 36×36 pixels, there were 1,683,109 features.


Fig. 3. Box Filter features, sometimes known as Haar-like wavelet filters, that were used in the study.
Show All

3.1.2. SVM(Gabor)
Gabor Energy Filters [44] are bandpass filters with a tunable spatial orientation and frequency. They model the complex cells of the primate's visual cortex. When applied to images, they respond to edges at particular orientations, e.g., horizontal edges due to wrinkling of the forehead, or diagonal edges due to “crow's feet” around the eyes. Gabor Energy Filters have a proven record in a wide variety of face processing applications, including face recognition [31] and facial expression recognition [35]. In machine learning applications Gabor features are often classified by a soft-margin linear support vector machine with parameter C specifying how much misclassified training examples should penalize the objective function. In our implementation, we applied a “bank” of 40 Gabor Energy Filters consisting of eight orientations (spaced at 22.5deg intervals) and five spatial frequencies ranging from 2 to 32 cycles per face. The total number of Gabor features is N×N×8×5, where N is the face image width in pixels.

3.1.3. MLR(CERT)
The Facial Action Coding System [17] is a comprehensive framework for objectively describing facial expression in terms of Action Units, which measure the intensity of over 40 distinct facial muscles. Manual FACS coding has previously been used to study student engagement and other emotions relevant to automated teaching [28] [42]. In our study, since we are interested in automatic engagement recognition, we employ the Computer Expression Recognition Toolbox, which is a software tool developed by our laboratory to estimate facial action intensities automatically [35]. Although the accuracies of the individual facial action classifiers vary, we have found CERT to be useful for a variety of facial analysis tasks, including the discrimination of real from faked pain [34], driver fatigue detection [54], and estimation of students’ perception of curriculum difficulty [55]. CERT outputs intensity estimates of 20 facial actions as well as the 3-D pose of the head (yaw, pitch, and roll). For engagement recognition we classify the CERT outputs using multinomial logistic regression, trained with an L2 regularizer on the weight vector of strength α. We use the absolute value of the yaw, pitch, and roll to provide invariance to the direction of the pose change. Since we are interested in real-time systems that can operate without baselining the detector to a particular subject, we use the raw CERT outputs (i.e., we do not z-score the outputs) in our experiments.

Internally, CERT uses the SVM(Gabor) approach described above. Since CERT was trained on hundreds to thousands of subjects (depending on the particular output channel), which is substantially higher than the number of subjects collected for this study, it is possible that CERT's outputs will provide an identity-independent representation of the students’ faces, which may boost generalization performance.

3.2. Data Selection
We started with a pool of 13,584 frames from the HBCU data set. We then applied the following procedure to select training and testing data for each binary classifier to distinguish l-v-other:

If the minimum and maximum label given to an image differed by more than 1 (e.g., one labeler assigns a label of 1 and another assigns a label of 3), then the image was discarded. This reduced the pool from 13,584 to 9,796 images.

If the automatic face detector (from CERT [35]) failed to detect a face, or if the largest detected face was less than 36 pixels wide (usually indicative of a erroneous face detection), the image was discarded. This reduced the pool from 9,796 to 7,785 images.

For each of the labeled images, we considered the set of all labels given to that image by all the labelers. If any labeler marked the frame as X (no face, or very unclear), then the image was discarded. This reduced the pool from 7,785 to 7,574 images.

Otherwise, the “ground truth” label for each image was computed by rounding the average label for that image to the nearest integer (e.g., 2.4 rounds to 2; 2.5 rounds to 3). If the rounded label equalled l, then that image was considered a positive example for the l-v-other classifier's training set; otherwise, it was considered a negative example.

In total there were 7574 frames from the HBCU data set and 16711 from the UC data set selected using this approach. The distributions of engagement in HBCU were 6.03, 9.72, 46.28, and 37.97 percent for engagement levels 1 through 4, respectively. For UC, they were 5.37, 8.46, 42.31, and 43.85 percent, respectively.

3.3. Cross-Validation
We used 4-fold subject-independent cross-validation to measure the accuracy of each trained binary classifier. The set of all labeled frames was partitioned into four folds such that no subject appeared in more than one fold; hence, the cross-validation estimate of performance gives a sense of how well the classifier would perform on a novel subject on which the classifier was not trained.

3.4. Accuracy Metrics
We use the 2-alternative forced choice (2AFC) [40] [60] metric to measure accuracy, which expresses the probability of correctly discriminating a positive example from a negative example in a 2-alternative forced choice classification task. The 2AFC is an unbiased estimate of the area under the Receiver Operating Characteristics curve, which is commonly used in the facial expression recognition literature (e.g., [37]). A 2AFC value of 1 indicates perfect discrimination, whereas 0.5 indicates that the classifier is “at chance”. In addition, we also compute Cohen's κ with quadratic weighting. To compare the machine's accuracy to inter-human accuracy, we computed the 2AFC and κ for human labelers as well, using the same image selection criteria as described in Section 3.2. When computing κ for the automatic binary classifiers, we optimized κ over all possible thresholds of the detector's outputs.

3.5. Hyperparameter Selection
Each of the classifiers listed above has a hyperparameter associated with it (either σ, C, or α). The choice of hyperparameter can impact the test accuracy substantially, and it is a common pitfall to give an overly optimistic estimate of a classifier's accuracy by manually tuning the hyperparameter based on the test set performance. To avoid this pitfall, we instead optimize the hyperparameters using only the training set by further dividing each training set into four subject-independent inner cross-validation folds in a double cross-validation paradigm. We selected hyperparameters from the following sets of values: σ∈{10−2,10−1.5,…,100}, C∈{0.1,0.5,2.5,12.5,62.5,312.5}, and α∈{10−5,10−4,…,10+5}.

3.6. Results: Binary Classification
Classification results are shown in Table 1 for cropped face resolution of 48×48 pixels. Each cell reports the accuracy (2AFC) averaged over four cross-validation folds, along with standard deviation in parentheses. Accuracies at 36×36 pixel resolution were very slightly lower. All results are for subject-independent classification.

TABLE 1 Top: Subject-Independent, within-Data Set (HBCU) Engagement Recognition Accuracy (2AFC Metric) for Each Engagement Level l∈{1,2,3,4} Using Each of the Three Classification Architectures, along with Inter-Human Classification Accuracy Bottom: Engagement recognition accuracy on a different data set (UC) not used for training.

From the upper part of Table 1, we see that the binary classification accuracy given by the machine classifiers is very similar to inter-human accuracy. All of the three architectures tested delivered similar performance averaged across the four tasks (1-v-other, 2-v-other, etc.). However, MLR(CERT) performed worse on 1-v-other than the other classifiers. As we discuss in Section 4, many images labeled as Engagement =1 exhibit eye closure. It is possible that CERT's eye closure detector is relatively inaccurate, and in comparison the Boost(BF) and SVM(Gabor) approaches are able to learn an accurate eye closure detector from the training data themselves. On the other hand, CERT performs better than the other approaches for 4-v-other. As described in Section 4, Engagement =4 can be discriminated using pose information. Here, CERT may have an advantage because CERT's pose detector was trained on tens of thousands of subjects.

Since inter-coder reliability is commonly reported using Cohen's κ, we report those values as well, both for HBCU and UC data sets, in Table 2. The “Avg” κ is the mean of the κ values for the four binary classifiers. As in Table 1, both the SVM(Gabor) and BF(Boost) classifiers demonstrate performance that is close to inter-human accuracy.

TABLE 2 Similar to Table 1, but Shows Cohen's κ Instead of 2AFC Top: HBCU Test Set Bottom: UC generalization set.
Table 2- Similar to Table 1, but Shows Cohen's $\kappa$ Instead of 2AFC Top: HBCU Test Set Bottom: UC generalization set.
Overall we find the results encouraging that machine classification of engagement can reach inter-human levels of accuracy.

3.7. Generalization to a Different Data Set
A well-known issue for contemporary face classifiers is to generalize to people of a different race from the people in the training set; in particular, modern face detectors often have difficulty detecting people with dark skin [56]. For our study, we collected data both at HBCU, where all the subjects were African-American, as well as UC, where all the subjects were either Asian-American or Caucasian-American. This gives us the opportunity to assess how well a classifier trained on one data set generalizes to the other. Here, we measure performance of the binary classifiers described above that were trained on HBCU when classifying subjects from UC.

Results are shown in Tables 1 and 2 (bottom) for each classification method. The most robust classifier was SVM(Gabor): average 2AFC fell only slightly from 0.729 to 0.691, and average κ fell from 0.306 to 0.231. Interestingly, the MLR(CERT) architecture was not particularly robust to the change in population, despite being trained on a much larger number of subjects. It is possible that the head pose features that are measured by CERT and are useful for the HBCU data set do not generalize to the UC data set. Between the Boost(BF) and SVM(Gabor) approaches, it is possible that the larger number of BF features compared to Gabor features led to overfitting—the Boost(BF) classifiers generalized well to subjects within the same population, but not to subjects of a different population.

3.8. Discrimination of Extreme Emotion States
In addition to the l-v-rest classification results described above, we also assess the accuracy of the SVM(Gabor) classifier on the task of discriminating between a student who is very engaged (i.e., Engagement =4) from a student who is very non-engaged (i.e., Engagement =1). On this binary task, the accuracy (2AFC) on the HBCU test set was 0.9280. On the UC generalization set, it was 0.7979.

3.9. Effect of Data Selection Procedures
As described in Section 3.2, we excluded images on which there is large label disagreement (step 1). It is conceivable that this could bias the results to be too optimistic because the “harder” images might be ones on which labelers tend to disagree. In a supplementary analysis using just the first training/testing fold for evaluation, we compared SVM(Gabor) classifiers on image sets created without excluding images with high disagreement (which resulted in 10,409 images in which the face was detected, instead of 7,574), to SVM(Gabor) classifiers trained with excluding those images. Results were very similar: the average accuracy (2AFC) over all four binary engagement classifiers was 0.7632 after excluding the images with high disagreement, and just slightly lower at 0.7570 without those images. This suggests that the larger number of images available for training can compensate for the noisier labels.

3.10. Confusion Matrices
An important question when developing automated classifiers is what kinds of mistakes the system makes. For example, does the 1-v-rest classifier ever believe that an image, whose true engagement label is 4, is actually a 1? To answer this question, we must first select a threshold on the real-valued output of each classifier so that it can make a binary decision. Here, we choose the threshold τ to maximize the balanced error rate on each test fold, which we define as the average of the false positive rate and the false negative rate. If the l-v-rest classifier's real-valued output on an image is greater than τ, then the classifier decides the image has engagement level l; otherwise, it decides the image has some engagement level not l. Using this threshold selection procedure, and averaging results across folds, we computed the confusion matrices on both the HBCU and UC data sets of the SVM(Gabor) engagement classifiers; the matrices are shown in Table 3. Each cell gives the probability that the binary classifier l-v-other will classify an image, whose “true” engagement (the rounded average label over all labels given to a particular image) is given by E=l′, as engagement l. Note that neither the rows nor the columns sum to 1—this is natural because the classifiers are binary, not 4-way.

TABLE 3 Confusion Matrices for the Binary SVM(Gabor) Classifiers Each cell is the probability that the classifier l-v-other will classify an image, whose “true” engagement is given by E=l′, as engagement l Top: results for HBCU test set. Bottom: results for UC generalization set.
Table 3- Confusion Matrices for the Binary SVM(Gabor) Classifiers Each cell is the probability that the classifier $l$-v-other will classify an image, whose “true” engagement is given by $E=l^{\prime }$, as engagement $l$ Top: results for HBCU test set. Bottom: results for UC generalization set.
As expected, the matrix diagonals dominate over all other values in the rows, which means that each l-v-rest classifier is most likely to respond to an image whose true engagement level is l. However, we also observe that the binary classifiers sometimes make “egregious mistakes”. For example, the 3-v-other classifier on the HBCU data set responded positively on 38.49 percent of images whose true engagement level was 1.

3.11. Regression
After performing binary classification of the input image for each engagement level l∈{1,2,3,4}, the final stage of the pipeline is to combine the binary classifiers’ outputs into a final engagement estimate. For the binary classifiers, we chose the SVM(Gabor) architecture and used two alternative strategies: (1) linear regression for real-valued engagement regression, and (2) multinomial logistic regression for four-way discrete engagement level classification.

3.12. Results
3.12.1. Linear Regression
Subject-independent 4-fold cross-validation accuracy, measured using Pearson's correlation r, was 0.50 on the HBCU test set. For comparison, inter-human accuracy on the same task was 0.71. On the UC generalization set, the mean Pearson correlation (over four folds) of the regressor was 0.36.

3.12.2. Multinomial Logistic Regression
As an alternative to linear regression, we used multinomial logistic regression to obtain discrete-valued engagement outputs in {1,2,3,4}. The average Cohen's κ (over all four folds) of the MLR, when compared with human labels on the HBCU data set, was 0.42 (std. dev. =0.13); on the UC generalization set, it was 0.23 (std. dev. =0.13).

Table 4 shows confusion matrices both on the HBCU test set and the UC generalization set using MLR as the regressor. The table shows the conditional probability (averaged over four folds) that the detector's output D equals l, given that the “true” engagement E (defined as the rounded average engagement label over all human labelers who labeled that image) equals l′. For example, on the HBCU data set, the probability that the engagement regressor outputs D=1, given that the true engagement was E=1, is 0.5961. The bottom of the table shows the analogous confusion matrix for human labelers. Overall, the confusion matrix of the automated MLR regressor and the matrix for humans are similar. Note, however, that the automated regressor sometimes makes “egregious” mistakes, e.g., mis-classifying images whose true engagement is 1 as belonging to engagement category 4 (P(D=4 | E=1)=0.1047 for HBCU).

TABLE 4 Confusion Matrices Specifying the Conditional Probability P(D=l | E=l′) of the Automatic MLR-Based Engagement Regressor's Output D Given the “true” Engagement Label E Top: results on HBCU test set. Middle: results on UC generalization set. Bottom: results for human labelers on HBCU test set.
Table 4- Confusion Matrices Specifying the Conditional Probability $P(D=l\ \vert \ E=l^{\prime })$ of the Automatic MLR-Based Engagement Regressor's Output $D$ Given the “true” Engagement Label $E$ Top: results on HBCU test set. Middle: results on UC generalization set. Bottom: results for human labelers on HBCU test set.
TABLE 5 Engagement Statistics that Correlated with Either Pre-Testor Post-Test Performance P(Engagement =l) denotes the fraction of video frames in which a subject's engagement level was estimated to be l Correlations with a * are statistically significant (p<0.05, 2-tailed).
Table 5- Engagement Statistics that Correlated with Either Pre-Testor Post-Test Performance P(Engagement $=l)$ denotes the fraction of video frames in which a subject's engagement level was estimated to be $l$ Correlations with a * are statistically significant ($p<0.05$, 2-tailed).
Finally, on the task of discriminating E=1 from E=4 (similar to Section 3.8), the accuracy of the MLR was κ=0.72; for human labelers on this task, κ=0.96.

SECTION 4.Reverse-Engineering the Labelers
Given that our goal in this project is to recognize student engagement as perceived by an external observer, it is instructive to analyze how the human labelers formed their judgments. We can use the weights assigned to the CERT features that were learned by the MLR(CERT) classifiers to assess quantitatively how the human labelers judged engagement—if the MLR weight assigned to AU 45 (eye closure) had a large magnitude, for example, then that would suggest that eye closure was an important factor in how humans labeled the data set on which that MLR classifier was trained. In particular, we examined the MLR weights of the 4-v-other MLR(CERT) classifier. Prior to training, the training data set was first normalized to have unit variance for each feature so that all features had the same scale. After training the MLR, we selected the five MLR weights with the highest magnitude; results are shown in Fig. 4.

Fig. 4. - Weights associated with different Action Units and head pose coordinates to discriminate Engagement $=4$ from Engagement $\ne 4$, along with examples of AUs 1 and 10. Pictures courtesy of Carnegie Mellon University's Automatic Face Analysis group, http://www.cs.cmu.edu/~face/facs.htm.
Fig. 4. Weights associated with different Action Units and head pose coordinates to discriminate Engagement =4 from Engagement ≠4, along with examples of AUs 1 and 10. Pictures courtesy of Carnegie Mellon University's Automatic Face Analysis group, http://www.cs.cmu.edu/~face/facs.htm.
Show All

The most discriminating feature was the absolute value of roll (in-plane rotation of the face), with which Engagement =4 was negatively associated (weight of −0.5659). It is possible that the hand-resting-on-hand that is prominent for Engagement =2 also induces roll in the head, and that the MLR(CERT) classifier learned this trend. The second most discriminating facial action was Action Unit 10 (upper lip raiser), which was positively correlated with Engagement =4. However, this correlation could potentially be spurious, as there were many moments when the learners exhibited hand-on-mouth gestures that may have corrupted the AU 10 estimates. Such gestures have been recognized as an important occlusion in automated teaching settings [38].

AU 1 (inner brow raiser), AU 45 (eye closure), and the absolute value of pitch (tilting of the head up and down) were also negatively correlated with Engagement =4. AU 1 has previously been reported to correlate with students’ self-report of frustration [10], but not engagement. The negative correlations with AU 45 and pitch are intuitive—they are suggestive that the student has tuned out (or even fallen asleep), or is looking down away from the screen.

SECTION 5.Correlation with Test Scores
In this section we investigate the correlation between human and automatic perceptions of engagement with student test performance and learning. We show results for Pearson correlation. Results for Spearman rank correlation were generally lower and are not reported.

5.1. Test Performance
5.1.1. Human Labels
We first compared human judgments of engagement with test performance by computing the mean engagement label over all labeled frames for each subject in the HBCU data set, and then correlating these mean engagement labels with pre-test and post-test scores (see Table 5). The Pearson correlation between engagement and pre-test was r=0.52 (p=0.0167, 2-tailed) and between engagement and post-test was r=0.37 (p=0.1027, dof =19, 2-tailed).

We also examined which of the four engagement levels was most predictive of task performance by correlating the fraction of frames labeled as Engagement =1, Engagement =2, etc., with student test performance. Only Engagement =4 was positively correlated with pre-test (r=0.57,p=0.0066, dof =19, 2-tailed) and post-test (r=0.47,p=0.0324, dof =19, 2-tailed) performance. In fact, the fraction of frames for which a student appeared to be in engagement level 4 (which we denote as P(Engagement =4)) was a better predictor than the mean engagement predictor described above. All the other engagement levels l<4 were negatively (though non-significantly) correlated with test performance, suggesting that Engagement =4 is the only “positive” engagement state.

For comparison, the correlation between students’ pre-test and post-test scores was r=0.44 (p=0.0471, dof =19, 2-tailed), which is slightly (though not statistically significantly) less than the correlation between P(Engagement =4) and post-test. In other words, human perceptions of student engagement were just as good of a predictor of post-test performance as the student's pre-test score. A partial correlation between P(Engagement =4) and post-test, given pre-test score, gave r=0.29 (p=0.2073, dof =19, 2-tailed).

Finally, it is worth noting that another interpretation of the correlation between engagement and pre-test is that a student's pre-test score is predictive of his/her engagement level during the subsequent learning session.

5.1.2. Automatic Estimates
We also computed the correlation between automatic judgments of engagement and student pre- and post-test performance. Since the best predictor of test performance from human judgments was from the fraction of frames labeled as Engagement =4, we focused on the output of the 4-v-other classifier. In particular, we correlated the fraction of frames over each subject's entire video session that the 4-v-other detector predicted to be a “positive” frame by thresholding with τ, where τ is the median detector output over all subjects’ frames. In other words, frames on which the detector's output exceeded τ was considered to be a “positive” frame for engagement level 4. The correlation with this automatic P(Engagement =4) predictor and pre-test performance was 0.64 (p=0.0023, dof =19,2-tailed); for post-test performance, it was r=0.27 (p=0.2436, dof =19, 2-tailed). This is the same pattern of correlations as in Section 5.1.1—engagement was more predictive of pre-test than of post-test.

5.2. Learning
In addition to raw test performance, we also examined correlations between engagement and learning. The average difference between the post-test and pre-test scores (across 21 subjects) was 2.81 sets, which was statistically significant (t(20)=4.3746, p=0.0002, 2-tailed), and which suggests that students were learning. However, we did not find significant correlations between engagement and learning, either using human labels or automatically estimated engagement labels.

5.3. Discussion
The correlation between engagement and pre-and post-test scores is of interest. Particularly telling is that post-test performance can be predicted just as accurately by looking at students’ faces during learning (r=0.47) as by looking at their pre-test scores (0.44). These results are consistent with [15], who found a positive correlation between “student energy” (valence) and math pre-test score as well as a positive correlation between a student being “on-task” and math post-test scores.

The lack of correlation between engagement and learning was somewhat disappointing but we believe it is an important clue for planning future research. The more engaged students have higher pre-test scores, which suggests there may be ceiling effects. It is possible, for example, that improving a test score from 10 to 11 is more difficult than improving from 1 to 2. We explored this hypothesis by optimizing the correlation between engagement and learning gains over different monotonic transformations both of engagement and of test scores. In particular, by searching over all monotonic mappings from {1,…,4} into {0,…,4} for engagement, and from {0,…,12} (the range of test scores observed in our experiment) into {0,…,20} for test scores, we identified a transformation that gave moderate (r=0.44, p=0.0458, dof =19, 2-tailed) but statistically significant correlations between learning and engagement. This non-linear monotonic transformation was effectively “undoing” the ceiling effect, weighting learning gains more heavily that started from larger pre-test baselines. However, we tried a large number of monotonic transformations, and thus the statistical significance of this analysis should be taken with a grain of salt. We also note that the correlation between engagement and learning might become significant if the number of subjects were increased.

Finally, and most importantly, in short term laboratory studies such as ours, most students are quite motivated and engaged. Indeed, while examining the videos, we rarely found periods of prolonged non-engagement. This is obviously different from classroom situations in which some students are consistently engaged and some students consistently disengaged across days, months, and years. Future work would benefit from focusing on long-term learning situations where variance in engagement is more likely to be observed and the effect of engagement on learning is more likely to become apparent.

SECTION 6.Conclusions
Increasing student engagement has emerged as a key challenge for teachers, researchers, and educational institutions. Many of the current tools used to measure engagement—such as self-reports, teacher introspective evaluations, and checklists—are cumbersome, lack the temporal resolution needed to understand the interplay between engagement and learning, and in some cases capture student compliance rather than engagement.

In this paper we explored the development of real-time automated recognition of engagement from students’ facial expressions. The motivating intuition was that teachers constantly evaluate the level of their students’ engagement, and facial expressions play a key role in such evaluations. Thus, understanding and automating the process of how people judge student engagement from the face could have important applications.

Our work extends prior research on engagement recognition using computer vision [10]–[11][29][42] and is arguably the most thorough study on this topic to date: We collected a data set of student facial expressions while performing a cognitive training task. We experimented with multiple approaches for human observers to assess student engagement. We found that interobserver reliability is maximized when the length of the observed clips is approximately 10 seconds. Shorter clips do not provide enough context and reliability suffers. Longer clips tend to be harder to evaluate because they often mix different levels of engagement. When discriminating low versus high levels of engagement, inter-observer reliability was high (Cohen's κ=0.96). We also found that the engagement judgments of 10-second clips could be reliably approximated (Pearson r=0.85) by averaging single frame judgments over the 10 seconds. This indicates that static expressions contain the bulk of the information observers use to assess student engagement. We found that observers rely on head pose, and elementary facial actions like brow raise, eye closure, and upper lip raise to make their judgments.

Our results suggest that machine learning methods could be used to develop a real-time automatic engagement detector with comparable accuracy to that of human observers. We showed that both human and automatic engagement judgments correlate with task performance. In particular, student post-test performance was predicted just as accurately (and statistically significantly) by observing the face of the student during learning (r=0.47) as from the pre-test scores (r=0.44). We failed to find significant correlations between perceived engagement and learning. However, a-posteriori statistical analysis suggests this may be due to ceiling effects and a fundamental limitation of short-term laboratory studies such as ours. In such studies, most students tend usually to be quite engaged, which is quite different from the long-term engagement or disengagement found in classrooms. This points to the importance of long-term studies that approximate the classroom ecology in which some students are engaged and others are chronically disengaged for days, months, and years.

While the progress made here is modest, it reinforces the idea that automatic recognition of student engagement is possible and could potentially revolutionize education as we know it. For example, using computer vision systems, a set of low-cost, high-resolution cameras could monitor engagement levels of entire classrooms, without the need for self-report or questionnaires. The temporal resolution of the technology could help understand when and why students get disengaged, and perhaps to take action before it is too late. Web-based teachers could obtain real-time statistics of the level of engagement of their students across the globe. Educational videos could be improved based on the aggregate engagement signals provided by the viewers. Such signals would indicate not only whether a video induces high or low engagement, but most importantly, which parts of the videos do so. Our work underlines the importance of focusing on long-term field studies in real-life classroom environments. Collecting data in such environments is critical to train more reliable and ecologically valid engagement recognition systems. More importantly, sustained, long-term studies in actual classrooms are needed to gain a better understanding of the interplay between engagement and learning in real life.