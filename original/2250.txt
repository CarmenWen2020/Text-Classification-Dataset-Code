Semantic edge detection (SED), which aims at jointly extracting edges as well as their category information, has far-reaching applications in domains such as semantic segmentation, object proposal generation, and object recognition. SED naturally requires achieving two distinct supervision targets: locating fine detailed edges and identifying high-level semantics. Our motivation comes from the hypothesis that such distinct targets prevent state-of-the-art SED methods from effectively using deep supervision to improve results. To this end, we propose a novel fully convolutional neural network using diverse deep supervision within a multi-task framework where bottom layers aim at generating category-agnostic edges, while top layers are responsible for the detection of category-aware semantic edges. To overcome the hypothesized supervision challenge, a novel information converter unit is introduced, whose effectiveness has been extensively evaluated on SBD and Cityscapes datasets.

The aim of classical edge detection is to detect edges and object boundaries in natural images. It is category-agnostic, in that object categories need not be recognized. Classical edge detection can be viewed as a pixel-wise binary classification problem, whose objective is to classify each pixel as belonging to either the class indicating an edge, or the class indicating a non-edge. In this paper, we consider more practical scenarios of semantic edge detection (SED), which jointly achieves edge detection and edge category recognition within an image. SED (Hariharan et al., 2011; Yu et al., 2017; Maninis et al., 2017; Bertasius et al., 2015b) is an active computer vision research topic due to its wide-ranging vision applications, including object proposal generation (Bertasius et al., 2015b), occlusion and depth reasoning (Amer et al., 2015; Bian et al., 2021), 3D reconstruction (Shan et al., 2014), object detection (Ferrari et al., 2008, 2010), and image-based localization (Ramalingam et al., 2010).

In the past several years, deep convolutional neural networks (DCNNs) reign undisputed as the new de-facto method for category-agnostic edge detection (Xie and Tu 2015, 2017; Liu et al., 2017, 2019; Hu et al., 2018), where near human-level performance has been achieved. However, deep learning for category-aware SED, which jointly detects visually salient edges as well as recognizing their categories, has not yet witnessed such vast popularity. Hariharan et al. (2011) first combined generic object detectors with bottom-up edges to recognize semantic edges. Yang et al. (2016) proposed a fully convolutional encoder-decoder network to detect object contours but without recognizing specific categories. More recently, CASENet (Yu et al., 2017) introduces a skip-layer structure to enrich the top-layer category-aware edge activation with bottom-layer features, improving previous state-of-the-art methods with a significant margin. However, CASENet imposes supervision only at the Side-5 and final fused classification and uses feature maps from Side-1âˆ¼Side-3 without deep supervision. After unsuccessfully trying various ways of adding deep supervision, CASENet claims that imposing deep supervision at bottom network sides (Side-1âˆ¼Side-4) is unnecessary. This conclusion has also been widely accepted by recent SED works (Yu et al., 2018; Acuna et al., 2019; Hu et al., 2019).

Fig. 1
figure 1
An example of our DDS algorithm. a The original image from the SBD dataset (Hariharan et al., 2011). b, c Its semantic edge map and corresponding color codes. dâ€“g Display category-agnostic edges from Side-1âˆ¼Side-4. h, i Semantic edges of Side-5 and DDS (DDS-R) output, respectively

Full size image
SED naturally requires achieving two distinct supervision targets: (i) locating fine detailed edges by capturing discontinuity among image regions, mainly using low-level features; and (ii) identifying abstracted high-level semantics by summarizing different appearance variations of the target categories. While it may be intuitive and straightforward to impose category-agnostic edge supervision at bottom network sides for low-level edge details and impose category-aware edge supervision at top sides for semantic classification, directly doing this as in CASENet (Yu et al., 2017) even degrades the performance compared with directly learning semantic edges without deep supervision or category-agnostic edge guidance. We hypothesize that distinct supervision targets prevent state-of-the-art SED methods (Yu et al., 2017, 2018; Acuna et al., 2019; Hu et al., 2019) from successfully applying deep supervision (Lee et al., 2015). Specifically, we observe that the success stories of deep supervision, including image categorization (Szegedy et al., 2015), object detection (Lin et al., 2020), visual tracking (Wang et al., 2015), and category-agnostic edge detection (Xie and Tu 2017; Liu et al., 2017), usually adopt the same type of supervision for all network sides. In contrast, CASENet directly imposes distinct supervision targets to bottom and top network sides. Therefore, we consider achieving such distinct supervision using some buffers, i.e., in an indirect manner, to prevent the backbone network from being directly influenced by distinct targets.

In this paper, we propose a diverse deep supervision (DDS) method, which employs deep supervision with different loss functions for high-level and low-level feature learning, as shown in Fig. 2b. To this end, we propose an information converter unit to change the backbone DCNN features into different representations, for training category-agnostic or semantic edges, respectively. Hence, information converters act as buffers, making distinct supervision targets indirectly affect top and bottom convolution (i.e., conv layers. The existence of information converters separates the information content in conv layers by assigning unique sets of parameters and imposing separate losses to each network side. This makes a single backbone network be effectively trained end-to-end towards different targets. An example of DDS is shown in Fig. 1. The bottom sides of the neural network help Side-5 to find fine details, thus making the final fused semantic edges (Fig. 1i) smoother than those coming from Side-5 (Fig. 1h).

In summary, our main contributions include:

We analyze the reason why state-of-the-art SED methods cannot apply deep supervision to improve results, i.e., due to the distinct supervision targets in SED (Sect. 3).

We propose a new SED method, called diverse deep supervision (DDS), which uses information converters to separate the information content in backbone conv layers and thus achieve distinct supervision in an indirect manner (Sect. 4).

We provide detailed ablation studies to further understand the proposed method (Sect. 5.2).

We extensively evaluate DDS on SBD (Hariharan et al., 2011) and Cityscapes (Cordts et al., 2016) datasets. DDS achieves state-of-the-art performance, demonstrating the reasonability of our analyses and thus opening up a new path for future SED research.

Related Work
An exhaustive review of the abundant literature on this topic is out of the scope of this paper. Instead, we first summarize the most important threads of research to solve the problem of classical category-agnostic edge detection, followed by the discussions of deep learning-based approaches, semantic edge detection (SED), and the technique of deep supervision.

Classical Category-Agnostic Edge Detection
Edge detection is conventionally solved by designing various filters [e.g., Sobel Sobel (1970) and Canny (1986)] or complex models (Mafi et al., 2018; Shui and Wang 2017) to detect pixels with highest gradients in their local neighborhoods (Trahanias and Venetsanopoulos 1993; Hardie and Boncelet 1995; Henstock and Chelberg 1996). To the best of our knowledge, Konishi et al. (2003) proposed the first data-driven edge detector in which, unlike previous model based approaches, edge detection was posed as statistical inferences. Pb features consisting of brightness, color and texture are used in Martin et al. (2004) to obtain the posterior probability of each boundary point. Pb is further extended to gPb ArbelÃ¡ez et al. (2011) by computing local cues from multi-scale and globalizing them through spectral clustering. Sketch tokens are learned from hand-drawn sketches for contour detection (Lim et al., 2013), while random decision forests are employed in DollÃ¡r and Zitnick (2015) to learn the local structure of edge patches, delivering competitive results among non-deep-learning approaches.

Deep Category-Agnostic Edge Detection
The number of success stories of machine learning has seen an all-time rise across many computer vision tasks recently. The unifying idea is deep learning which utilizes neural networks with many hidden layers aimed at learning complex feature representations from raw data (Chan et al., 2015; Tang et al., 2017; Liu et al., 2018). Motivated by this, deep learning based methods have made vast inroads into edge detection as well (Wang et al., 2019; Deng et al., 2018; Yang et al., 2017). Ganin et al. (2014) applied deep neural network for edge detection using a dictionary learning and nearest neighbor algorithm. DeepEdge (Bertasius et al., 2015a) first extracts candidate contour points and then classifies these candidates. HFL (Bertasius et al., 2015b) uses SE (DollÃ¡r and Zitnick 2015) to generate candidate edge points in contrast to Canny (1986) used in DeepEdge. Compared with DeepEdge which has to process input patches for every candidate point, HFL turns out to be more computationally feasible as the input image is only fed into the network once. DeepContour (Shen et al., 2015) partitions edge data into subclasses and fits each subclass using different model parameters. Xie et al.  (2015, 2017) leveraged deeply-supervised nets to build a fully convolutional network for image-to-image prediction. Their deep model, known as HED, fuses the information from the bottom and top conv layers. Kokkinos (2016) proposed some training strategies to retrain HED. Liu et al. (2017, 2019) introduced the first real-time edge detector, which achieves higher F-measure scores than average human annotators on the popular BSDS500 dataset (ArbelÃ¡ez et al., 2011).

Semantic Edge Detection
By virtue of their strong capacity for semantic representation learning, DCNNs based edge detectors tend to generate high responses at object boundary locations, e.g., Fig. 1dâ€“g. This has inspired research on simultaneously detecting edge pixels and classifying them based on associations with one or more object categories. This so-called â€œcategory-awareâ€ edge detection is highly beneficial to a wide range of vision tasks including object recognition, stereo vision, semantic segmentation, and object proposal generation.

Hariharan et al. (2011) proposed the first principled way of combining generic object detectors with bottom-up contours to detect semantic edges. Yang et al. (2016) proposed a fully convolutional encoder-decoder network for object contour detection. HFL (Bertasius et al., 2015b) produces category-agnostic binary edges and assigns class labels to all boundary points using deep semantic segmentation networks. Maninis et al. (2017) coupled their convolutional oriented boundaries (COB) with semantic segmentation generated by dilated convolutions (Yu and Koltun 2016) to obtain semantic edges. A weakly supervised learning strategy is introduced in Khoreva et al. (2016), where bounding box annotations alone are sufficient to produce high-quality object boundaries without any object-specific annotations. Gated-SCNN (Takikawa et al., 2019) converts the semantic edge representation from different ResNet layers to a representation suitable for segmentation, improving semantic segmentation substantially.

Yu et al. (2017) proposed a novel network, CASENet, which has pushed SED performance to a new state-of-the-art. In their architecture, low-level features are only used to augment top classifications. After several failed experiments, they reported that imposing deep supervision at bottom sides is unnecessary for SED. More recently, Yu et al.  (2018) introduced a new training approach, SEAL, to train CASENet (Yu et al., 2017). This approach can simultaneously align ground-truth edges and learn semantic edge detectors. However, the training of SEAL is very time-consuming due to the heavy CPU computation load. For example, it needs over 16 days to train CASENet on the SBD dataset (Hariharan et al., 2011), despite that we have used a powerful CPU (Intel Xeon(R) CPU E5-2683 v3 @ 2.00 GHz Ã— 56). Hu et al. (2019) proposed a novel dynamic feature fusion (DFF) strategy to assign different fusion weights for different input images and locations adptively in the fusion of multi-scale DCNN features. Acuna et al. (2019) focused on semantic thinning edge alignment learning (STEAL). They presented a simple new layer and loss to train CASENet (Yu et al., 2017), so that they can learn sharp and precise semantic boundaries. However, all above methods give up applying deep supervision to bottom layers due to the distinct supervision targets in SED. In this work, we aim to solve this problem, so our method is compatible with previous methods, including SEAL (Yu et al., 2018), DFF (Hu et al., 2019), and STEAL (Acuna et al., 2019).

Deep Supervision
Deep supervision has been demonstrated to be effective in many vision and learning tasks such as image classification (Lee et al., 2015; Szegedy et al., 2015), object detection (Lin et al., 2017, 2020; Liu et al., 2016), visual tracking (Wang et al., 2015), category-agnostic edge detection (Xie and Tu 2017; Liu et al., 2017), salient object detection (Hou et al., 2019), and so on. Theoretically, the bottom layers of deep networks can learn discriminative features so that classification/regression at top layers is easier. In practice, one can explicitly influence the hidden layer weight/filter update process to favor highly discriminative feature maps using deep supervision. However, traditional deep supervision usually adopts the same type of supervision at all layers, so it may be suboptimal for SED to directly apply distinct supervision of category-agnostic and category-aware edges to bottom and top network sides, respectively. In the following sections, we will first analyze the problem of distinct supervision targets of SED and then introduce a new semantic edge detector with successful diverse deep supervision.

Fig. 2
figure 2
A comparison between two SED models: CASENet (Yu et al., 2017) and our DDS. CASENet only adds top supervision on the Side-5 activation, and the authors claimed that deep supervision was not necessary in their architecture. However, our proposed DDS network adds deep supervision at all network sides. Note that information converters are crucial for resolving the distinct supervision targets of category-agnostic and category-aware edges

Full size image
Distinct Supervision Targets in SED
Before expounding the proposed method, we first analyze the problem caused by the distinct supervision targets of SED.

A Typical Deep Model for SED
To introduce previous attempts for using deep supervision in SED, without loss of generality, we take a typical deep model as an example, i.e., CASENet (Yu et al., 2017). As shown in Fig. 2a, this typical model is built on the well-known backbone network of ResNet (He et al., 2016). It connects a 1Ã—1 conv layer after each of Side-1âˆ¼Side-3 to produce a single-channel feature map ğ¹(ğ‘š). The top Side-5 is connected to a 1Ã—1 conv layer to output K-channel class activation map ğ´(5)={ğ´(5)1,ğ´(5)2,â€¦,ğ´(5)ğ¾}, where K is the number of categories. Then, the shared concatenation replicates bottom features ğ¹(ğ‘š) to separately concatenate each channel of the class activation map:

ğ¹ğ‘“={ğ¹(1),ğ¹(2),ğ¹(3),ğ´(5)1,â€¦,ğ¹(1),ğ¹(2),ğ¹(3),ğ´(5)ğ¾}.
(1)
Next, a K-grouped 1Ã—1 conv is performed on ğ¹ğ‘“ to generate a semantic edge map with K channels, in which the k-th channel represents the edge map for the k-th category. Other SED models (Yu et al., 2018; Hu et al., 2019) have similar network designs.

Discussion
Previous SED models (Yu et al., 2017, 2018; Bertasius et al., 2015b; Hu et al., 2019) only impose supervision on Side-5 and the final fused activation. In CASENet, the authors have tried several deeply supervised architectures. They first separately used all of Side-1âˆ¼Side-5 for SED, with each side connected with a semantic classification loss. The evaluation results are even worse than the basic architecture that directly applies 1Ã—1 convolution at Side-5 to obtain semantic edges. It is widely accepted that the bottom layers of DCNNs contain low-level, less-semantic features such as local edges, which are less effective for semantic classification because semantic category recognition needs abstracted high-level features that mainly appear in the top layers of neural networks. Thus, they would obtain poor classification results at bottom sides. Unsurprisingly, simply connecting each low-level feature layer and high-level feature layer with a classification loss function to achieve deep supervision for SED would result in a clear performance drop.

Yu et al. (2017) also attempted to impose deep supervision of binary edges at Side-1âˆ¼Side-3 in CASENet but observed divergence in the semantic classification at Side-5. Here, we provide an intuitive and reasonable explanation for this phenomenon. With the top supervision of semantic edges, the top layers of the network will be supervised to learn abstracted high-level semantics that can summarize different appearance variations of object categories. Since bottom layers are the bases of top layers for the representation power of DCNNs, bottom layers will be supervised to serve top layers for obtaining high-level semantics through back propagation. Conversely, with bottom supervision of category-agnostic edges, bottom layers are taught to focus on distinction between edges and non-edges, rather than visual representations for semantic classification. Hence, bottom layers have two conflict supervision targets. Compared to traditional deep supervision applications that usually adopt the same type of supervision, we believe that such distinct supervision targets of SED lead to the failure of previous attempts to apply deep supervision for SED. Our motivation of this work comes from this hypothesis by trying to resolve such distinct supervision targets.

Note that Side-4 is not used in CASENet. We think that it is a naive way to alleviate the supervision conflicts by regarding the whole res4 block as a buffer unit between bottom and top sides. Indeed, when adding Side-4 to CASENet (see Sect. 5.2), the new model (CASENet+S4) achieves a 70.9% mean F-measure, compared to 71.4% of original CASENet. This suggests that our hypothesis about the buffer function of res4 block may be reasonable. Moreover, the classical 1Ã—1 conv layer after each side (Xie and Tu 2017; Yu et al., 2017) is too weak to buffer the conflicts. We therefore propose an information converter unit to try to separate the information content in the backbone layers by assigning unique sets of parameters and imposing separate losses to each network side. In this way, we tackle the distinct supervision targets of SED in an indirect manner, rather than the previous direct manner.

Methodology
Intuitively, by employing different but â€œappropriateâ€ ground truths for bottom and top sides, the learned intermediate representations of the different levels may contain complementary information. However, directly imposing deep supervision does not seem to be beneficial. In this section, we propose a new network architecture for the complementary learning of bottom and top sides for SED.

Diverse Deep Supervision
Based on the above discussion, we hypothesize that the bottom sides of neural networks may not be directly beneficial to SED. However, we still believe that bottom sides encode fine details complementary to the top side (Side-5). With appropriate architecture re-design, maybe they can be used for category-agnostic edge detection to improve the localization accuracy of semantic edges generated by the top side. To this end, we design a novel information converter to assist low-level feature learning, making it consistent with high-level feature learning. This is essential as this enables bottom layers to learn find-grained details and serve top layers to favor highly discriminative features simultaneously.

Our proposed network architecture is presented in Fig. 2b. We follow CASENet to use ResNet (He et al., 2016) as our backbone network. After each information converter (Sect. 4.2) in Side-1âˆ¼Side-4, we connect a 1Ã—1 conv layer with a single output channel to produce an edge response map. These predicted maps are then upsampled to the original image size using bilinear interpolation. These side-outputs are supervised by binary category-agnostic edges. We perform K-channel 1Ã—1 convolution on Side-5 to obtain semantic edges, where each channel represents the binary edge map of one category. We adopt the same upsampling operation as for Side-1âˆ¼Side-4. Semantic edges are used to supervise the training of Side-5.

We denote the produced binary edge maps from Side-1âˆ¼Side-4 as ğ¸={ğ¸(1),ğ¸(2),ğ¸(3),ğ¸(4)}. The semantic edge map from Side-5 is still represented by ğ´(5). A shared concatenation is then performed to obtain the stacked edge activation map:

ğ¸ğ‘“={ğ¸,ğ´(5)1,ğ¸,ğ´(5)2,ğ¸,ğ´(5)3,â€¦,ğ¸,ğ´(5)ğ¾}.
(2)
Note that ğ¸ğ‘“ is a stacked edge activation map, while ğ¹ğ‘“ in CASENet is a stacked feature map. Finally, we apply K-grouped 1Ã—1 convolution on ğ¸ğ‘“ to generate the fused semantic edges. The fused edges are supervised by the ground truth of the semantic edges. As shown in HED (Xie and Tu 2017), the 1Ã—1 convolution can fuse the edges from bottom and top sides well.

Information Converter
From the above analyses, the core for improving SED is the existence of the information converter. In this paper, we try a simple design for information converter to validate our hypothesis. Recently, residual networks have been proved to be easier to optimize than plain networks (He et al., 2016). The residual learning operation is embodied by a shortcut connection and element-wise addition. We describe a residual conv block in Fig. 3, which consists of two alternatively connected ReLU and conv layers, and the output of the first ReLU layer is added to the output of the last conv layer. Our proposed information converter combines two residual modules and is connected to each side of the DDS network to transform the learned representation into the proper form. This operation is expected to avoid the conflicts caused by the discrepancy in different losses.

Fig. 3
figure 3
Schematic of our information converter unit (illustrated in the orange box in Fig. 2)

Full size image
The top supervision of semantic edges will guide top layers in learning semantic features, while the bottom supervision of category-agnostic edges will guide bottom layers in learning category-agnostic features. Hence, bottom layers would have two distinct supervision through back propagation if the distinct supervision is directly imposed as discussed in Sect. 3. Our information converters can separate the information content in the backbone layers by assigning unique sets of parameters and imposing separate losses to each network side, playing a buffering role. In this way, the distinct supervision targets are imposed to the backbone network in an indirect manner, rather than the previous direct manner (Yu et al., 2017, 2018; Bertasius et al., 2015b; Hu et al., 2019). Note that this paper mainly claims the importance of the existence of the information converter, not its specific format, so we only adopt a simple design. In the experimental part, we will demonstrate different designs for the information converter achieve similar performance.

Our proposed network can successfully combine the fine details from bottom sides and the semantic information from top sides. Our experimental results demonstrate that this method solves the problem of conflicts caused by diverse deep supervision. Unlike CASENet, our semantic classification at Side-5 can be well optimized without any divergence. The produced binary edges from bottom sides help Side-5 make up fine details. Thus, the final fused semantic edges can achieve better localization quality.

We use binary edges of single pixel width to supervise Side-1âˆ¼Side-4 and thick semantic boundaries to supervise Side-5 and the final fused edges. One pixel is viewed as a binary edge if it belongs to the semantic boundaries of any category. We obtain thick semantic boundaries by seeking the difference between a pixel and its neighbors in ground-truth semantic segmentation, as in CASENet (Yu et al., 2017). A pixel with label k is regarded as a boundary of class k if at least one neighbor with a label ğ‘˜â€² (ğ‘˜â€²â‰ ğ‘˜) exists.

Multi-task Loss
Two different loss functions, which represent category-agnostic and semantic edge detection losses, respectively, are employed in our multi-task learning framework. We denote all layer parameters in the network as W. Suppose an image I has a corresponding binary edge map ğ‘Œ={ğ‘¦ğ‘–:ğ‘–=1,2,â€¦,|ğ¼|}. The reweighted sigmoid cross-entropy loss function for Side-1âˆ¼Side-4 can be formulated as

ğ¿(ğ‘š)ğ‘ ğ‘–ğ‘‘ğ‘’(ğ‘Š)=âˆ’âˆ‘ğ‘–âˆˆğ¼[ğ›½â‹…(1âˆ’ğ‘¦ğ‘–)â‹…ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ƒ(ğ¸(ğ‘š)ğ‘–;ğ‘Š))+(1âˆ’ğ›½)â‹…ğ‘¦ğ‘–â‹…ğ‘™ğ‘œğ‘”(ğ‘ƒ(ğ¸(ğ‘š)ğ‘–;ğ‘Š))],(ğ‘š=1,â€¦,4),
(3)
where we have ğ›½=|ğ‘Œ+|/|ğ‘Œ| and 1âˆ’ğ›½=|ğ‘Œâˆ’|/|ğ‘Œ|. ğ‘Œ+ and ğ‘Œâˆ’ represent edge and non-edge ground-truth label sets, respectively. ğ¸(ğ‘š)ğ‘– is the produced activation value at pixel i for the m-th side. ğ‘ƒ(â‹…) is the standard sigmoid function.

Table 1 ODS F-measure (%) of DDS-R/DDS-U and ablation methods on the SBD dataset (Hariharan et al., 2011) using the original benchmark protocol in Hariharan et al. (2011)
Full size table
For an image I, suppose the semantic ground-truth label is {ğ‘ŒÂ¯1,ğ‘ŒÂ¯2,â€¦,ğ‘ŒÂ¯ğ¾}, in which ğ‘ŒÂ¯ğ‘˜={ğ‘¦Â¯ğ‘˜ğ‘–:ğ‘–=1,2,â€¦,|ğ¼|} is the binary edge map for the kth category. Note that each pixel may belong to the boundaries of multiple categories. We define the reweighted multi-label loss for Side-5 as

ğ¿(5)ğ‘ ğ‘–ğ‘‘ğ‘’(ğ‘Š)=âˆ’âˆ‘ğ‘˜âˆ‘ğ‘–âˆˆğ¼[ğ›½â‹…(1âˆ’ğ‘¦Â¯ğ‘˜ğ‘–)â‹…ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ƒ(ğ´(5)ğ‘˜,ğ‘–;ğ‘Š))+(1âˆ’ğ›½)â‹…ğ‘¦Â¯ğ‘˜ğ‘–â‹…ğ‘™ğ‘œğ‘”(ğ‘ƒ(ğ´(5)ğ‘˜,ğ‘–;ğ‘Š))],
(4)
in which ğ´(5)ğ‘˜,ğ‘– is the Side-5â€™s activation value for the kth category at pixel i. The loss of the fused semantic activation map is denoted as ğ¿ğ‘“ğ‘¢ğ‘ ğ‘’(ğ‘Š), which can be similarly defined as

ğ¿ğ‘“ğ‘¢ğ‘ ğ‘’(ğ‘Š)=âˆ’âˆ‘ğ‘˜âˆ‘ğ‘–âˆˆğ¼[ğ›½â‹…(1âˆ’ğ‘¦Â¯ğ‘˜ğ‘–)â‹…ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ƒ(ğ´ğ‘“ğ‘˜,ğ‘–;ğ‘Š))+(1âˆ’ğ›½)â‹…ğ‘¦Â¯ğ‘˜ğ‘–â‹…ğ‘™ğ‘œğ‘”(ğ‘ƒ(ğ´ğ‘“ğ‘˜,ğ‘–;ğ‘Š))],
(5)
where ğ´ğ‘“ is the final fused semantic edge map. The total loss is formulated as

ğ¿(ğ‘Š)=âˆ‘ğ‘š=1,â€¦,5ğ¿(ğ‘š)ğ‘ ğ‘–ğ‘‘ğ‘’(ğ‘Š)+ğ¿ğ‘“ğ‘¢ğ‘ ğ‘’(ğ‘Š).
(6)
Using this total loss function, we can optimize all parameters in an end-to-end way. We denote DDS trained using the reweighted loss L(W) as DDS-R.

Recently, Yu et al. (2018) proposed to simultaneously align and learn semantic edges. They found that the unweighted (regular) sigmoid cross-entropy loss performed better than reweighted loss with their alignment training strategy. Due to the heavy computational load on the CPU, their approach was very time-consuming (over 16 days for SBD dataset (Hariharan et al., 2011) with 28 CPU kernels and an NVIDIA TITAN Xp GPU) to train a network. We use their method (SEAL) to align ground-truth edges only once prior to training and apply unweighted sigmoid cross-entropy loss to train the aligned edges. The loss function for Side-1âˆ¼Side-4 can thus be formulated as

ğ¿â€²(ğ‘š)ğ‘ ğ‘–ğ‘‘ğ‘’(ğ‘Š)=âˆ’âˆ‘ğ‘–âˆˆğ¼[(1âˆ’ğ‘¦ğ‘–)â‹…ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ƒ(ğ¸(ğ‘š)ğ‘–;ğ‘Š))+ğ‘¦ğ‘–â‹…ğ‘™ğ‘œğ‘”(ğ‘ƒ(ğ¸(ğ‘š)ğ‘–;ğ‘Š))],(ğ‘š=1,â€¦,4).
(7)
The unweighted multi-label loss for Side-5 is

ğ¿â€²(5)ğ‘ ğ‘–ğ‘‘ğ‘’(ğ‘Š)=âˆ’âˆ‘ğ‘˜âˆ‘ğ‘–âˆˆğ¼[(1âˆ’ğ‘¦Â¯ğ‘˜ğ‘–)â‹…ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ƒ(ğ´(5)ğ‘˜,ğ‘–;ğ‘Š))+ğ‘¦Â¯ğ‘˜ğ‘–â‹…ğ‘™ğ‘œğ‘”(ğ‘ƒ(ğ´(5)ğ‘˜,ğ‘–;ğ‘Š))].
(8)
ğ¿â€²ğ‘“ğ‘¢ğ‘ ğ‘’(ğ‘Š) can be similarly defined as

ğ¿â€²ğ‘“ğ‘¢ğ‘ ğ‘’(ğ‘Š)=âˆ’âˆ‘ğ‘˜âˆ‘ğ‘–âˆˆğ¼[(1âˆ’ğ‘¦Â¯ğ‘˜ğ‘–)â‹…ğ‘™ğ‘œğ‘”(1âˆ’ğ‘ƒ(ğ´ğ‘“ğ‘˜,ğ‘–;ğ‘Š))+ğ‘¦Â¯ğ‘˜ğ‘–â‹…ğ‘™ğ‘œğ‘”(ğ‘ƒ(ğ´ğ‘“ğ‘˜,ğ‘–;ğ‘Š))].
(9)
The total loss is the sum across all sides:

ğ¿â€²(ğ‘Š)=âˆ‘ğ‘š=1,â€¦,5ğ¿â€²(ğ‘š)ğ‘ ğ‘–ğ‘‘ğ‘’(ğ‘Š)+ğ¿â€²ğ‘“ğ‘¢ğ‘ ğ‘’(ğ‘Š).
(10)
We denote DDS trained using the unweighted loss ğ¿â€²(ğ‘Š) as DDS-U.

Table 2 Ablation studies for the design of the information converter on the SBD dataset (Hariharan et al., 2011)
Full size table
Implementation Details
We implement our method using the well-known deep learning framework of Caffe (Jia et al., 2014). The proposed network is built on ResNet (He et al., 2016). We follow CASENet (Yu et al., 2017) to change the strides of the first and fifth convolution blocks from 2 to 1, so that the output scales of five convolution blocks are 1, 1/2, 1/4, 1/8, and 1/8 compared to the input image, respectively. The atrous algorithm is used to keep the receptive field sizes the same as original ResNet. Specifically, from the second convolution block to the fourth, we use dilated convolutions with a dilation rate of 2; for the fifth block, we use a dilation rate of 4. We also follow CASENet to pre-train the convolution blocks on the COCO dataset (Lin et al., 2014).

The network is optimized with stochastic gradient descent (SGD). Each SGD iteration chooses 10 images at uniformly random and crops a 352Ã—352 patch from each of them. The weight decay and momentum are set to 0.0005 and 0.9, respectively. We use the learning rate policy of â€œpolyâ€, where the current learning rate equals the base one multiplying (1âˆ’ğ‘ğ‘¢ğ‘Ÿğ‘Ÿ_ğ‘–ğ‘¡ğ‘’ğ‘Ÿ/ğ‘šğ‘ğ‘¥_ğ‘–ğ‘¡ğ‘’ğ‘Ÿ)ğ‘ğ‘œğ‘¤ğ‘’ğ‘Ÿ. The parameter of power is set to 0.9. We run 25k/80k iterations (ğ‘šğ‘ğ‘¥_ğ‘–ğ‘¡ğ‘’ğ‘Ÿ) of SGD for SBD (Hariharan et al., 2011) and Cityscapes (Cordts et al., 2016), respectively. For DDS-R training, the base learning rate is set to 5eâˆ’7/2.5eâˆ’7 for SBD and Cityscapes, respectively. For DDS-U training, the loss at the beginning of training is very large. Therefore, for both SBD and Cityscapes, we first pre-train the network with a fixed learning rate of 1eâˆ’8 for 3k iterations and then use the base learning rate of 1eâˆ’7 to continue training with the same settings as described above. The upsampling operation is implemented with deconvolution layers by fixing the parameters to perform bilinear interpolation. All experiments are performed using an NVIDIA TITAN Xp GPU.

Experiments
Experimental Settings
Datasets
We evaluate our method on the SBD (Hariharan et al., 2011) and Cityscapes (Cordts et al., 2016) datasets. SBD (Hariharan et al., 2011) comprises 11,355 images and corresponding labeled semantic edge maps for 20 object classes. It is divided into 8498 training and 2857 testing images. We follow (Yu et al., 2017) to use the training set to train our network and the test set for evaluation. The Cityscapes dataset (Cordts et al., 2016) is a large-scale semantic segmentation dataset with stereo video sequences recorded in street scenarios from 50 different cities. It consists of 5000 images divided into 2975 training, 500 validation, and 1525 testing images. The ground truth of the test set has not been published because it is an online competition for semantic segmentation labeling and scene understanding. Hence, we use the training set for training and the validation set for testing.

Evaluation Metrics
For performance evaluation, we adopt several standard metrics with the recommended parameter settings in the original papers. The first metric is the benchmark protocol in Hariharan et al. (2011). It calculates the class-wise F-measure score that is the harmonic mean of the precision and recall. We follow the default settings with the matching distance tolerance of 0.02 for all datasets. The maximum F-measure at the optimal dataset scale (ODS) for each class and mean maximum F-measure across all classes are reported.

We also follow Yu et al. (2018) to evaluate semantic edges with stricter rules than the benchmark in Hariharan et al. (2011). The ground-truth maps are instance-sensitive edges for Yu et al. (2018). This differs from Hariharan et al. (2011) which uses instance-insensitive edges. Besides, Hariharan et al. (2011) thins the prediction before matching by default. Yu et al. (2018) further proposes to match the raw predictions with unthinned ground truths. This mode and the above conventional mode are referred as â€œRawâ€ and â€œThinâ€, respectively. In this paper, we report both the â€œThinâ€ and â€œRawâ€ scores for the benchmark protocol in Yu et al. (2018). We follow Yu et al. (2018) to set the matching distance tolerance of 0.02 for the original SBD dataset (Hariharan et al., 2011), 0.0075 for the re-annotated SBD dataset (Yu et al., 2018), and 0.0035 for the Cityscapes dataset (Cordts et al., 2016). The image borders of 5-pixels width are ignored for the SBD dataset, while not for the Cityscapes dataset.

We follow Yu et al. (2018) to generate both â€œThinâ€ and â€œRawâ€ ground truths for both instance-sensitive and instance-insensitive edges. The produced edges can be viewed as the boundaries of semantic objects or stuff in semantic segmentation. We downsample the ground truths and predicted edge maps of Cityscapes dataset to half the original dimensions to speed up evaluation as in previous works (Yu et al., 2017, 2018; Hu et al., 2019; Acuna et al., 2019). For the performance comparison with baseline methods, we use the default code and pre-trained models released by the original authors to produce edge predictions.

Table 3 Class-agnostic evaluation results on the SBD dataset (Hariharan et al., 2011)
Full size table
Ablation Studies
We first perform ablation studies on the SBD dataset (Yu et al., 2018) to investigate various aspects of the proposed DDS before comparing it with existing state-of-the-art methods. To this end, we propose seven DDS variants:

Softmax which only adopts the top side (Side-5) with a 21-class softmax loss function, such that the ground-truth edges of each category do not overlap and thus each pixel has one specific class label.

Basic which employs the top side (Side-5) for multi-label classification, meaning that we directly connect the loss function of ğ¿(5)ğ‘ ğ‘–ğ‘‘ğ‘’(ğ‘Š) on res5c to train the detector.

DSN which directly applies the deeply supervised network architecture, in which each side of the backbone network is connected to a 1Ã—1 conv layer with K output channels for SED, and the resulting activation maps from all sides are fused to generate the final semantic edges.

CASENet+S4 which is similar to CASENet but takes into consideration Side-4 by connecting it to a 1Ã—1 conv layer to produce a single-channel feature map, while CASENet only uses Side-1âˆ¼Side-3 and Side-5.

DDSâˆ– Convt which removes the information converters in DDS, such that deep supervision is directly imposed after each side.

DDSâˆ– Convtâ€  which not only removes the information converters in DDS but also applies a progressive training strategy, i.e., each block of the ResNet (He et al., 2016) along with their corresponding side-outputs is trained separately and then frozen, to simulate the effect of information converters.

DDSâˆ– DeSup which removes the deep supervision from Side-1âˆ¼Side-4 of DDS but retains the information converters.

All these variants are trained using the reweighted loss function Eq. 6 (except Softmax) and the original SBD dataset for a fair comparison.

We evaluate these variants and the original DDS and CASENet (Yu et al., 2017) on the SBD dataset using the original benchmark protocol in Hariharan et al. (2011). The evaluation results are shown in Table 1. We can see that Softmax suffers from significant performance degradation. Because the predicted semantic edges of neural networks are usually thick and overlap with other classes, it is improper to assign a single label to each pixel. Hence, we apply multi-label loss in this paper. The Basic variant achieves an ODS F-measure of 70.6%, which is 0.3% higher than DSN. This further verifies our hypothesis presented in Sect. 3 that features from bottom layers are not sufficiently discriminative for semantic classification. Furthermore, CASENet+S4 performs better than DSN, demonstrating that bottom convolutional features are more suitable for binary edge detection. Moreover, the F-measure of CASENet+S4 is lower than original CASENet.

Why does DDS Work Well?
The improvement from DDSâˆ–DeSup to DDS-R shows that the success of DDS does not arise due to more parameters (conv layers) but instead from the coordination between deep supervision and information converters. On the contrary, adding more conv layers but without deep supervision may make network convergence more difficult. Our conclusion is consistent with Yu et al. (2017), when comparing DDSâˆ–Convt with the results of CASENet, namely that there is no value in directly adding binary edge supervision to bottom sides.

Table 4 ODS F-measure (%) of DDS-R/DDS-U and other competitors on the SBD dataset (Hariharan et al., 2011)
Full size table
Discussion About the Proposed DDS
Intuitively, employing different but â€œappropriateâ€ ground truths to bottom and top sides may enhance the feature learning in different layers. Upon this, the learned intermediate representations of different levels will tend to contain complementary information. However, in our case, it may be useless to directly add deep supervision of category-agnostic edges to bottom sides, because bottom layers would receive two distinct supervision in the loss function of Eq. 6, as discussed above. Instead, we show that with proper architecture re-design, we can employ deep supervision to significantly boost performance. The information converters adopted in the proposed method play a central role in guiding bottom layers for category-agnostic edge detection. In this way, low-level edges from bottom layers encode more details, which then assist top layers to better localize semantic edges. They serve as buffers to separate the information content in the backbone layers. This is essential, as they enable bottom layers to serve as the basis of top layers to favor highly discriminative feature maps for correct semantic classification.

The significant performance improvement provided by the proposed DDS-R/DDS-U over CASENet+S4 and DDSâˆ–Convt demonstrates the importance of our design, in which different sides use different supervision after the information format conversion. We also note that DDS-U achieves better performance than DDS-R by applying the unweighted loss function and aligned edges (Yu et al., 2018).

Progressive Training
DDSâˆ–Convtâ€  adopts progressive training to simulate the effect of information converters, as suggested by a paper reviewer. However, from Table 1, we can see that DDSâˆ–Convtâ€  performs significantly worse than other variants. This is because progressive training cannot optimize DCNNs well. As widely acknowledged, it is necessary for DCNNs to adopt end-to-end training for deriving optimal parameters. In fact, progressive training is an old way for DCNN training (Hinton et al., 2006). After the invention of ReLU (Nair and Hinton 2010), batch normalization (Ioffe and Szegedy 2015), and dropout (Srivastava et al., 2014), progressive training has not been used in the deep learning community due to its poor performance. Hence, DDSâˆ–Convtâ€  cannot simulate information converters.

Discussion About the Design of the Information Converter
This paper mainly discusses and resolves the distinct supervision targets in SED, and the core is the existence, not the specific format, of the information converter. Hence we design a simple information converter that consists of two sequential residual conv units. Here, we conduct ablation studies for this design. Results are shown in Table 2. We experiment with three different converter designs: i) with only one conv unit; ii) with three conv units; iii) without residual connections in the conv units (plain conv units). It is easy to observe that the information converter with three residual conv units achieves the best performance, but it is only slightly better than that with two residual conv units. To make a trade-off between model complexity and performance, we use two residual conv units as the default setting.

We also evaluate the effect of the number of parameters of the information converter. Here, we change its size by simply multiplying a constant to the number of channels of the default information converter. In this way, the resulting variants have various model sizes but keep the same structure. Specifically, we try three constants of 1/4, 1/2, and 2, leading to 1/16, 1/4, and 4 times of the default model size, respectively. The results are depicted in Table 2. It is interesting to find that the proposed method is quite robust to different model sizes, demonstrating that the improvement mainly comes from the existence of the information converter, not its specific format.

Improvement of the Edge Localization
To demonstrate if the introduced information converter actually improves the localization of the semantic edges, we ignore the semantic labels and perform class-agnostic evaluation for the proposed DDS and previous baselines. Given an input image, SED methods generate an edge probability map for each class. To generate a class-agnostic edge map for an image, at each pixel, we view the maximum edge probability across all classes as the class-agnostic edge probability at this pixel. For ground truth, at each pixel, if any class has an edge on this pixel, this pixel is viewed as a class-agnostic edge pixel. Then, we use the standard benchmark in Hariharan et al. (2011) for evaluation. From Table 3, we find DDS can significantly improve the edge localization accuracy, which demonstrates that imposing class-agnostic edge supervision at bottom network sides can well benefit edge localization. After exploring DDS with several variants and establishing the effectiveness of the approach, we summarize the results obtained by our method and compare it with previous state-of-the-art methods.

Fig. 4
figure 4
A qualitative comparison of DSN, CASENet and DDS-R. First row: the original image, ground truth, and category color codes. This image is taken from the SBD dataset (Hariharan et al., 2011). Second row: the semantic edges predicted by different methods. Third row: an enlarged area of predicted edges. Fourth row: the predicted horse boundaries only. Last row: the predicted person boundaries only. Green, red, white, and blue pixels represent true positive, false positive, true negative, and false negative points, respectively, at the threshold of 0.5. Best viewed in color (Color figure online)

Full size image
Table 5 ODS F-measure (%) of DDS-R/DDS-U and other competitors on the re-annotated SBD dataset (Yu et al., 2018)
Full size table
Evaluation on SBD
In this part, we compare DDS-R/DDS-U on the SBD dataset (Hariharan et al., 2011) with previous state-of-the-art methods, including InvDet (Hariharan et al., 2011), HFL-FC8 (Bertasius et al., 2015b), HFL-CRF (Bertasius et al., 2015b), BNF (Bertasius et al., 2016), WS (Khoreva et al., 2016), DilConv (Yu and Koltun 2016), DSN (Yu et al., 2017), COB (Maninis et al., 2017), CASENet (Yu et al., 2017), SEAL (Yu et al., 2018), STEAL (Acuna et al., 2019), DFF (Hu et al., 2019), and Gated-SCNN (Takikawa et al., 2019). Among them, DFF (Hu et al., 2019) shares the same distinct supervision problem as CASENet, so we also integrate DDS-R into DFF to demonstrate the generalizability of DDS-R. We adopt the same code implementation and training strategies for DFF-based DDS-R as the original DFF. Gated-SCNN (Takikawa et al., 2019) learns semantic edges for improving the training of semantic segmentation. Hence, we retrain it for semantic edge detection by removing its segmentation loss and dual task loss, and the other settings are kept by default.

Results are summarized in Table 4. DDS-U achieves the state-of-the-art performance across all competitors. The ODS F-measure of the proposed DDS-U is 1.7% higher than SEAL and 3.4% higher than CASENet in terms of the metric in Hariharan et al. (2011), so delivering a new state-of-the-art. We can observe that DDS-R can also improve the performance of DFF (Hu et al., 2019). Therefore, the proposed DDS can be viewed as a general idea to improve SED. The improvement from CASENet to DDS is also larger than the improvement of STEAL. Moreover, InvDet (Hariharan et al., 2011) is a non-deep learning based approach which shows competitive results among other conventional approaches. COB (Maninis et al., 2017) is a state-of-the-art category-agnostic edge detection method, and combining it with semantic segmentation of DilConv (Yu and Koltun 2016) produces a competitive semantic edge detector. COBâ€™s superiority over DilConv reflects the effectiveness of its fusion algorithm. The fact that both CASENet and DDS-R/DDS-U outperform COB illustrates the importance of directly learning semantic edges, because the combination of binary edges and semantic segmentation is insufficient for SED. The average runtime of DSN, CASENet, and DDS is shown in Table 6. DDS can generate state-of-the-art semantic edges with only a slight reduction in speed.

Table 6 Average runtime per image on the SBD dataset (Hariharan et al., 2011)
Full size table
Table 7 ODS F-measure (%) of DDS-R/DDS-U and other competitors on the Cityscapes dataset (Cordts et al., 2016)
Full size table
Fig. 5
figure 5
Side activation maps on the input image of Fig. 4. The first two columns display DSNâ€™s side class classification activation for the classes of horse and person, respectively. The last two columns show the side features of Side-1âˆ¼Side-3 and class classification activation of Side-5 for CASENet and our DDS-R, respectively. These images are obtained by normalizing the activation to [0, 255]. Note that all activations are directly outputted without any non-linearization, e.g., sigmoid function

Full size image
Fig. 6
figure 6
Some examples from SBD dataset (Hariharan et al., 2011). From top to bottom: color codes, original images, ground truth, DSN, CASENet (Yu et al., 2017), SEAL (Yu et al., 2018), STEAL (Acuna et al., 2019), DFF (Hu et al., 2019), our DDS-R and DDS-U. We follow the color coding protocol in Yu et al., (2018)

Full size image
Fig. 7
figure 7
Some examples from Cityscapes dataset (Cordts et al., 2016). From top to bottom: color codes, original images, ground truth, CASENet (Yu et al., 2017), SEAL (Yu et al., 2018), STEAL (Acuna et al., 2019), DFF (Hu et al., 2019), our DDS-R and DDS-U. We follow the color coding protocol in Yu et al., (2018). The produced edges of DDS are smoother and clearer

Full size image
Yu et al. (2018) discovered that some of the original SBD labels are a little noisy, so they re-annotated 1059 images from the test set to form a new test set. We compare our method with CASENet (Yu et al., 2017), SEAL (Yu et al., 2018), STEAL (Acuna et al., 2019), Gated-SCNN (Takikawa et al., 2019), and DFF (Hu et al., 2019) on this new dataset. The results are shown in Table 5. DDS can improve the performance for both CASENet and DFF in terms of all evaluation metrics. Specifically, the ODS F-measures of DDS-U is 3.0% and 2.7% higher than recent SEAL (Yu et al., 2018) in terms of the â€œThinâ€ and â€œRawâ€ metrics in Yu et al. (2018), respectively. Note that SEAL retrains CASENet with a new training strategy: i.e., simultaneous alignment and learning. With the same training strategy, DDS-R obtains a 4.3% and 8.0% higher ODS F-measure than CASENet in terms of the â€œThinâ€ and â€œRawâ€ metrics in Yu et al. (2018), respectively.

To better visualize the edge prediction results, an example is shown in Fig. 4. We also show the normalized images of side activation in Fig. 5. All activations are obtained before sigmoid non-linearization. For a simple arrangement of figures, we do not display Side-4 activation of DDS-R. From Side-1 to Side-3, one can see that the feature maps of DDS-R are significantly clearer than those of DSN and CASENet. Clear category-agnostic edges can be found with DDS-R, while DSN and CASENet suffer from noisy activation. For example, in CASENet, without imposing deep supervision on Side-1âˆ¼Side-3, edge activation can barely be found. For category classification activation, DDS-R can separate horse and person clearly, while DSN and CASENet can not. Therefore, the information converters also help to better optimize Side-5 for category-specific classification. This further verifies the feasibility of the proposed DDS architecture.

More qualitative examples are displayed in Fig. 6. DDS-R/DDS-U can produce clearer and smother edges than other detectors. In the second column, it is interesting to note that most detectors can recognize the boundaries of the objects with missing annotations, i.e., the obscured dining table and human arm. In the third column, DDS-R/DDS-U can generate strong responses at the boundaries of the small cat, while all other detectors only have weak or noisy responses. This demonstrates that DDS is more robust for detecting small objects. We also find that DDS-U and SEAL can generate thinner edges, suggesting that training with regular unweighted sigmoid cross entropy loss and refined ground-truth edges is helpful for accurately locating thin boundaries.

Evaluation on Cityscapes
The Cityscapes dataset (Cordts et al., 2016) is more challenging than SBD (Hariharan et al., 2011). The images in Cityscapes are captured in more complicated scenes, usually in urban street scenes in different cities. There are more objects, especially overlapping objects, in each image. Hence, we also adopt Cityscapes for evaluating semantic edge detectors using the â€œThinâ€ and â€œRawâ€ metrics in Yu et al. (2018). We compare DDS with CASENet (Yu et al., 2017), SEAL (Yu et al., 2018), STEAL (Acuna et al., 2019), DFF (Hu et al., 2019), and Gated-SCNN (Takikawa et al., 2019).

The evaluation results are reported in Table 7. Both DDS-R and DDS-U significantly outperform other methods in terms of both â€œThinâ€ and â€œRawâ€ metrics. With the same loss function, the ODS F-measure of DDS-R is 2.8% higher than CASENet in terms of the â€œThinâ€ metric in Yu et al. (2018), and DDS-U is 4.7% higher than SEAL correspondingly. STEAL and Gated-SCNN achieve similar performance, and both are much worse than DDS-R and DDS-U. Note that Gated-SCNN is the state-of-the-art semantic segmentation model, suggesting that it is necessary to study semantic edge detection rather than directly applying existing related techniques. Some qualitative comparisons are shown in Fig. 7. We can see that DDS-R/DDS-U produces smoother and clearer edges in various complicated scenarios, which is brought by the low-level binary edge supervision of DDS.

Conclusion
In this paper, we study the SED problem. Previous methods suggest that deep supervision is not necessary (Yu et al., 2017, 2018; Hu et al., 2019) for SED. Here, we show that this is false and, with proper architecture re-design, that the network can be deeply supervised to improve detection results. The core of our approach is the introduction of the novel information converter, which plays a central role in resolving the distinct supervision targets by successfully applying category-aware edges at the top side and the category-agnostic edges at bottom sides. The proposed DDS achieves state-of-the-art performance on the popular SBD (Hariharan et al., 2011) and Cityscapes (Cordts et al., 2016) datasets. Our idea to leverage deep supervision for training a deep network opens up a new path towards putting more emphasis utilizing rich feature hierarchies from deep networks for SED as well as other high-level tasks such as semantic segmentation (Maninis et al., 2017; Chen et al., 2016), object detection (Ferrari et al., 2008; Maninis et al., 2017), and instance segmentation (Kirillov et al., 2017; Hayder et al., 2017).

Future Work
Besides category-agnostic edge detection and SED, relevant tasks commonly exist in computer vision (Zamir et al., 2018), such as segmentation and saliency detection, object detection and keypoint detection, edge detection and skeleton extraction. Building multi-task networks to solve relevant tasks is a good way to save computational resources in practical applications (Hou et al., 2018). However, distinct supervision targets usually prevent this goal, as shown in this paper. From this point of view, the proposed DDS provides a new perspective to multi-task learning. In the future, we plan to leverage the idea of information converter for more relevant tasks.

