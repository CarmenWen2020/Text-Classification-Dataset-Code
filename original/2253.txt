We propose a monocular depth estimation method SC-Depth, which requires only unlabelled videos for training and enables the scale-consistent prediction at inference time. Our contributions include: (i) we propose a geometry consistency loss, which penalizes the inconsistency of predicted depths between adjacent views; (ii) we propose a self-discovered mask to automatically localize moving objects that violate the underlying static scene assumption and cause noisy signals during training; (iii) we demonstrate the efficacy of each component with a detailed ablation study and show high-quality depth estimation results in both KITTI and NYUv2 datasets. Moreover, thanks to the capability of scale-consistent prediction, we show that our monocular-trained deep networks are readily integrated into ORB-SLAM2 system for more robust and accurate tracking. The proposed hybrid Pseudo-RGBD SLAM shows compelling results in KITTI, and it generalizes well to the KAIST dataset without additional training. Finally, we provide several demos for qualitative evaluation. The source code is released on GitHub.

Access provided by University of Auckland Library

Introduction
The CNN-based monocular depth estimation (Eigen et al. 2014) has shown significant promise for many Computer Vision tasks. The supervised methods (Fu et al. 2018; Yin et al. 2019) achieve high performance, while they require expensive range sensors to capture the ground-truth data for training. To this end, recent work explores unsupervised learning for monocular depth estimation, which either uses the calibrated stereo pairs (Garg et al. 2016; Godard et al. 2017) or unlabelled videos (Yin and Shi 2018; Zhou et al. 2017) for training. In these frameworks, the color consistency between multiple views serves as the main supervision signal. Since they do not require ground truth labels, and particularly the recent method (Gordon et al. 2019) showing unsupervised depth estimation can work with the unknown camera intrinsics, these methods attract a lot of interest in the Computer Vision community. In this paper, we are interested in the video-based unsupervised learning framework because it has a minimum requirement for training data.

Compared with stereo-based learning, video-based learning (Zhou et al. 2017) is often more challenging due to the unknown camera motion. More importantly, due to scale ambiguity, a natural issue in monocular vision, the predicted depth by the latter has an unknown scaling to the real world. This is the so-called relative depth, as opposed to the metric depth in the previous setting. The relative depth is also widely used, e.g., ORB-SLAM (Mur-Artal et al. 2015) and COLMAP (Schonberger and Frahm 2016) both generate results up to an unknown scale. However, one critical issue that we identify in video-based learning is that methods may generate scale-inconsistent predictions over different frames since they suffer from a per-frame scale ambiguity. This does not impact the single-image based tasks, while it is critical for video-based applications, e.g., inconsistent depths cannot be used for camera tracking in the Visual SLAM system‚ÄîSee Fig. 9.

In this paper, we propose an improved unsupervised learning framework for higher depth accuracy and consistency. First, we propose a geometry consistency loss (ùêøùê∫) to encourage networks to predict scale-consistent depths. It explicitly penalizes the pixel-wise inconsistency of predicted depths between adjacent frames during training. It enables more effective learning and allows for more consistent predictions at inference time‚ÄîSee Table 6. Second, we propose a self-discovered mask (ùëÄùë†) for handling moving objects during training, which violates the underlying static scene assumption. It improves the performance significantly (See Table 2) and does not require additional overhead since the proposed mask is simply derived from ùêøùê∫.

Fig. 1
figure 1
Generalization on our self-captured video. The data is collected in Adelaide, an Australia city. Our depth and pose networks are trained on KITTI without additional finetuning. The scene is so challenging that ORB-SLAM2 (Mur-Artal and Tard√≥s 2017) failed to initialize or quickly lost tracking after initialization, while our Pseudo-RGBD SLAM system can provide an accurate trajectory, which is consistent with the Google Map. See more details in Sect. 5.4

Full size image
To show the benefits from scale-consistent depth prediction and demonstrate our contribution for downstream tasks, we integrate our trained networks into the ORB-SLAM2 (Mur-Artal and Tard√≥s 2017) system for more accurate and robust tracking. The proposed hybrid Pseudo-RGBD SLAM system has distinct advantages over traditional monocular systems, including (a) it starts tracking at any frame without latency; (b) it enables more robust and accurate tracking with the help of predicted depths; and (c) it allows for dense 3D reconstruction‚ÄîSee Fig. 12. We report comprehensive quantitative results and provide several demos in Sect. 5.4 for qualitative evaluation. An example is shown in Fig. 1, where we visualize the depth, point cloud, and camera trajectory generated by our method on a real-world driving video.

Our preliminary version was presented in NeurIPS 2019 (Bian et al. 2019a), where we propose an unsupervised learning framework for scale-consistent depth and pose estimation. In this paper, we (i) add more technical details of our proposed method; (ii) make a more clear explanation of our contribution and distinguish our method from existing methods; (iii) improve our learning framework by changing network architectures and integrating effective components from related work; (iv) conduct a more comprehensive evaluation and show the potential of our method to Visual SLAM.

Related Work
Single-view depth estimation The depth estimation problem was mainly solved by using traditional geometry based methods (Geiger et al. 2011; Sch√∂nberger et al. 2016) before deep learning based methods emerged. They rely on correspondences search (Bian et al. 2020a; Lowe 2004), model fitting (Bian et al. 2019b; Zhang 1998), and multi-view triangulation (Hartley and Zisserman 2003). Therefore, at least two different views of the scene are required as input for computing the depth. In contrast, recent deep learning based methods leverage the expressive power of convolutional neural networks, and they are able to regress the depth from a single image only. According to the training data, we can categorize learning-based methods into four classes: First, (Eigen et al. 2014) use the sensor captured depths (e.g., LiDAR or RGB-D devices) as the ground truth for training. The following work (Fu et al. 2018; Garg et al. 2019; Huynh et al. 2020; Liu et al. 2016; Yin et al. 2019) proposes more advanced network architectures or learning objectives to improve the performance. These methods achieve high performance, while it is expensive to capture ground-truth data in many real-world scenes. Second, (Chen et al. 2019a; Li et al. 2019b; Li and Snavely 2018; Wang et al. 2019; Xian et al. 2018; Yin et al. 2020) collect stereo images or videos from the web and use off-the-shelf tools (e.g., stereo matching (Hirschmuller 2005) or multi-view stereo (Sch√∂nberger et al. 2016)) to compute dense ground-truth depths. Besides, (Ranftl et al. 2020) export perfect depths from the synthetic 3D movies (Butler et al. 2012). Although these methods can obtain cheap ground-truth data, there often exists a domain gap between the collected data and the desired scenes. More importantly, the learned scale information is hard to generalize across different scenes so that they often predict the relative depth. This prevents them from predicting consistent depths on a video. Third, (Garg et al. 2016) use the calibrated stereo images for training models, where they warp images using the predicted depth with the known camera baseline and use the photometric loss to penalize the warping error. Then (Godard et al. 2017) exploit the left-right consistency in image pairs, and (Zhan et al. 2018) exploit the temporary consistency in videos. Pilzer et al. (2018) leverage adversarial learning, and Poggi et al. (2020) study the uncertainty of predicted depths. These methods can predict the metric depth, while it requires well-calibrated stereo cameras to collect training data. Fourth, (Zhou et al. 2017) train models from unlabelled videos, where they jointly train the depth and pose networks using adjacent frames with photometric loss and differentiable warping (Jaderberg et al. 2015). Due to the simplicity and generality,, it attracts a lot of researchers‚Äô interests and inspires a series of works, including (Chen et al. 2019b; Godard et al. 2019; Gordon et al. 2019; Guizilini et al. 2020a, b; Klingner et al. 2020; Mahjourian et al. 2018; Ranjan et al. 2019; Wang et al. 2018; Yin and Shi 2018; Zhao et al. 2020; Zhou et al. 2019; Zou et al. 2018). Our method falls into this category, and we target improving the depth accuracy and consistency for advancing downstream video-based tasks.

Scale consistency It is an important problem in Visual SLAM (Mur-Artal et al. 2015), but to the best of our knowledge, we are the first ones to discuss the scale inconsistency behind unsupervised video-based depth learning. Nevertheless, we find that our proposed geometry consistency loss is technically similar to two previous methods. First, (Mahjourian et al. 2018) propose a 3D ICP loss to penalize the misalignment of predicted depths, where they approximate gradients for depth and pose networks independently because the ICP is not differentiable. This ignores second-order effects between depth and pose networks, and hence it limits the performance. By contrast, our geometry consistency loss is naturally differentiable and results in better performance. Second, (Zou et al. 2018) propose a depth consistency loss, which enforces corresponding points in two images to have identical depth predictions. This is physically incorrect because the scene depth is view-dependent, i.e., it should be different in different views. We instead synthesize the depth for the second view using the predicted depth in the first view via rigid transformation, and we penalize the difference between predicted depths and synthesized depths in the second view. Not only does our approach improve the depth accuracy, but also it enables scale-consistent depth prediction for advancing video-based applications such Visual SLAM (Mur-Artal and Tard√≥s 2017). After the publication of our conference paper, we notice that more recent works pay attention to consistent depth prediction, including (Luo et al. 2020; Tiwari et al. 2020; Zhao et al. 2020; Zou et al. 2020).

Moving objects As the moving objects violate the underlying static world assumption for learning depths, related work often detects dynamic regions and masks them out when computing the photometric loss. Zhou et al. (2017) predict a mask from a pair of images by using the neural network. However, due to lacking effective supervision signals, the performance is limited. Vijayanarasimhan et al. (2017) learn a moving object mask from synthetic data (Menze and Geiger 2015), which is often hard to generalize to real-world scenes. Chen et al. (2019b); Ranjan et al. (2019); Yin and Shi (2018); Zou et al. (2018) additionally train an optical flow network and compare the optical flow with depth-based mapping for detecting moving objects. This is effective, but training an optical flow network is time-consuming due to the complex correlation computation. Casser et al. (2019a, 2019b); Gordon et al. (2019); Guizilini et al. (2020b); Huynh et al. (2020); leverage the semantic information for localizing dynamic objects. They either require the pretrained semantic segmentation network or need the manually labelled class labels for multi-task training. Godard et al. (2019) mask out the moving objects that have the same velocity as the camera, while it cannot handle other object motions. Compared with previous methods, our method does not require semantic inputs and does not require training additional networks. Our proposed mask is analytically derived from the geometry consistency loss, and it is able to handle arbitrary object motions and occlusions. After ours, (Li et al. 2020) propose to learn the dense 3D translation field of objects relative to the scene by using the neural network, which is also efficient and effective.

Depth estimation for Visual SLAM Traditional methods use either feature matching (Geiger et al. 2011; Klein and Murray 2007) or direct image alignment (Engel et al. 2017; Forster et al. 2014) for camera tracking and mapping. Recently, (Yin et al. 2017) use a supervised depth estimation model to help recover the absolute scale for monocular methods. CNN-SLAM (Tateno et al. 2017) uses the depth estimation network within a monocular SLAM system for dense reconstruction. CodeSLAM (Bloesch et al. 2018) jointly optimizes the depth and pose via a learned latent code. Although promising results are reported, these methods rely on supervised training, which is not always available in real-world scenarios. UndeepVO (Li et al. 2018) and (Zhan et al. 2018) train depth and pose networks on the calibrated stereo videos using the photometric loss, and they show that the learned pose network can inference on monocular videos like a visual odometer. CNN-SVO (Loo et al. 2019) combines the stereo-learned depth network and SVO (Forster et al. 2014) for more accurate trajectory estimation. DVSO (Yang et al. 2018a) and D3VO (Yang et al. 2020) also train depth models on stereo videos, and they further conduct geometric optimization. Note that all the aforementioned methods do not suffer from the scale ambiguity issue, as opposed to ours, because they can recover the metric depth. In this paper, we show that the monocular-trained model can predict the scale-consistent results, and it can be used for visual odometry. After ours, (Zou et al. 2020) propose to model the long-term dependency by using a two-layer convolutional LSTM module, which improves the pose prediction accuracy significantly. However, the pure learning-based methods are easy to overfit, and we believe that combing deep learning and geometry-based methods is a more promising direction. As a result, our hybrid system generalizes well to the previously unseen dataset and to our self-captured videos.

SC-Depth
Framework Overview
Our goal is to train depth and pose CNNs from unlabeled videos. Given two adjacent frames (ùêºùëé, ùêºùëè) randomly sampled from a video, their depth maps (ùê∑ùëé, ùê∑ùëè) and relative 6-DoF camera pose ùëÉùëéùëè are first estimated by the depth and pose CNNs, respectively. With the predicted depth and pose, we can synthesize the reference image ùêºùëé using the source image ùêºùëè by differentiable warping (Jaderberg et al. 2015), which generates ùêº‚Ä≤ùëé. Then the network is supervised by the photometric loss between the real ùêºùëé and the synthesized ùêº‚Ä≤ùëé. To explicitly constrain the depth CNN to predict scale-consistent results on adjacent frames, we propose a geometry consistency loss ùêøùê∫. To handle invalid cases such as static frames and dynamic objects, we introduce two masks. First, a self-discovered mask ùëÄùë† (Eq. 7) is introduced to reason the dynamics and occlusions by checking the depth consistency. Fig. 2 illustrates the proposed loss and mask. Second, we use the auto-mask (ùëÄùëé) (Godard et al. 2019) to remove stationary points on image pairs where the camera is not moving.

Fig. 2
figure 2
Illustration of the proposed geometry consistency loss and self-discover mask. Given two consecutive frames (ùêºùëé, ùêºùëè), we first estimate their depth maps (ùê∑ùëé, ùê∑ùëè) and relative pose (ùëÉùëéùëè) using the network. Then we compute the ùê∑diff (Eq. 5), i.e., pixel-wise depth inconsistency between ùê∑ùëé and ùê∑ùëè. Finally, we derive our geometric consistency loss ùêøùê∫ (Eq. 6) and self-discovered mask ùëÄùë† (Eq. 7) from ùê∑diff to regularize the network training and hanlding dynamics and occlusions (Fig. 4). For clarity, the photometric loss and smoothness loss are not shown in this figure

Full size image
Our objective function is formulated as follows:

ùêø=ùõºùêøùëÄùëÉ+ùõΩùêøùëÜ+ùõæùêøùê∫,
(1)
where ùêøùëÄùëÉ stands for the photometric loss ùêøùëÉ weighted by the proposed ùëÄùë†. ùêøùëÜ stands for the smoothness loss, and ùêøùê∫ is the geometric consistency loss. [ùõº,ùõΩ,ùõæ] are the loss weighting terms. The loss is averaged over valid points, which are determined by ùëÄùëé. In the following sections, we first introduce the photometric loss and smoothness loss in Sect. 3.2, then we describe the proposed geometric consistency loss ùêøùê∫ in Sect. 3.3 and the self-discovered mask ùëÄùë† in Sect. 3.4.1, and finally, we elaborate the auto-mask ùëÄùëé in Sect. 3.4.2.

Photometric and Smoothness Loss
Leveraging brightness constancy and spatial smoothness priors is ubiquitous in classical dense correspondence algorithms (Baker and Matthews 2004). Previous works (Ranjan et al. 2019; Yin and Shi 2018; Zhou et al. 2017) have used the photometric loss between the warped frame and the reference frame as an unsupervised loss function for network training. With the predicted depth ùê∑ùëé and pose ùëÉùëéùëè, we synthesize ùêº‚Ä≤ùëé by warping ùêºùëè, where differentiable warping (Jaderberg et al. 2015) is used. With the synthesized ùêº‚Ä≤ùëé and the reference image ùêºùëé, we formulate the objective function as

ùêøùëÉ=1|ÓâÇ|‚àëùëù‚ààÓâÇ(ùúÜ‚Äñùêºùëé(ùëù)‚àíùêº‚Ä≤ùëé(ùëù)‚Äñ1+(1‚àíùúÜ)1‚àíSSIMùëéùëé‚Ä≤(ùëù)2),
(2)
where ÓâÇ stands for the set of valid points that are successfully projected from ùêºùëé to the image plane of ùêºùëè, and p stands for a generic point in ÓâÇ. We choose ùêø1 loss due to its robustness to outliers. Besides, SSIMùëéùëé‚Ä≤ stands for the element-wise similarity between ùêºùëé and ùêº‚Ä≤ùëé by the SSIM function (Wang et al. 2004). This is used to better handle complex illumination changes since it normalizes the pixel illumination. More specifically,

SSIM (x,y)=(2ùúáùë•ùúáùë¶+ùê∂1)(2ùúéùë•ùë¶+ùê∂2)(ùúá2ùë•+ùúá2ùë¶+ùê∂1)(ùúé2ùë•+ùúé2ùë¶+ùê∂2),
(3)
where x, y stands for two 3 by 3 patches around the central pixel. ùê∂1 and ùê∂2 are constants. ùúá and ùúé are local statistics of the image color, i.e., mean and variance, respectively. Following (Godard et al. 2017; Ranjan et al. 2019; Yin and Shi 2018), we use ùê∂1=0.0001, ùê∂2=0.0009, and ùúÜ=0.15.

As the photometric loss is not informative in low-texture regions of the scene, existing work also incorporates a smoothness prior to regularize the estimated depth map. We adopt the edge-aware smoothness loss used in Ranjan et al. (2019). Formally,

ùêøùëÜ=‚àëùëù(ùëí‚àí‚àáùêºùëé(ùëù)‚ãÖ‚àáùê∑ùëé(ùëù))2,
(4)
where ‚àá is the first derivative along spatial directions. It ensures smoothness to be guided by image edges.

Fig. 3
figure 3
Differentiable depth inconsistency computation. This operation takes two depth maps (ùê∑ùëé, ùê∑ùëè) and their relative pose (ùëÉùëéùëè) as input and outputs the pixel-wise inconsistency. Firstly, we project ùê∑ùëé to 3D space and then to the image plane of ùê∑ùëè using ùëÉùëéùëè, obtaining the ùê∑ùëéùëè that stands for the synthesized ùê∑ùëè. Then, we hope to compute the difference between ùê∑ùëéùëè and ùê∑ùëè. However, it is not practical because the projection does not religiously lie in the grid of ùê∑ùëè. Therefore, we obtain the ùê∑‚Ä≤ùëè by using the differentiable bilinear interpolation (Jaderberg et al. 2015). Finally, we compare ùê∑ùëéùëè with ùê∑‚Ä≤ùëè to obtain the depth inconsistency (ùê∑diff). Here, we use the relative loss (Eq. 5), although other loss functions such as L1 and L2 are also applicable

Full size image
Geometry Consistency Loss
To explicitly enforce geometry consistency, we constrain that the predicted ùê∑ùëé and ùê∑ùëè (related by ùëÉùëéùëè) conform the same 3D structure by penalizing their inconsistency. Specifically, we propose a differentiable depth inconsistency operation to compute the pixel-wise inconsistency between two depth maps, as shown in Fig. 3. Here, ùê∑ùëéùëè is the synthesized depth for ùêºùëè, which is generated by ùê∑ùëé and pose ùëÉùëéùëè with the underlying rigid transformation. ùê∑‚Ä≤ùëè is an interpolation of ùê∑ùëè for aligning and comparing with ùê∑ùëéùëè. Given them, we compute the depth inconsistency map ùê∑diff for each ùëù‚ààÓâÇ as:

ùê∑diff(ùëù)=|ùê∑ùëéùëè(ùëù)‚àíùê∑‚Ä≤ùëè(ùëù)|ùê∑ùëéùëè(ùëù)+ùê∑‚Ä≤ùëè(ùëù),
(5)
where we normalize depth differences by their summation. This works better than the absolute distance in practice as it treats points at different absolute depths equally in optimization. Besides, the function is symmetric, and the outputs are naturally ranging from 0 to 1, which makes the training more stable.

With the inconsistency map, we define the proposed geometry consistency loss as:

ùêøùê∫=1|ÓâÇ|‚àëùëù‚ààÓâÇùê∑diff(ùëù),
(6)
which minimizes the geometric inconsistency of predicted depths over two views. By minimizing the depth inconsistency between samples in a batch, we naturally propagate such consistency to the entire sequence: the depth of ùêº1 agrees with the depth of ùêº2 in a batch; the depth of ùêº2 agrees with the depth of ùêº3 in another training batch. Eventually, depths of ùêºùëñ of a sequence should all agree with each other, leading to scale-consistent results over the entire sequence.

Fig. 4
figure 4
Visual results of depth and masking. Top to bottom: sample image, estimated depth, self-discovered mask ùëÄùë†, and auto-mask ùëÄùëé (Godard et al. 2019). The proposed ùëÄùë† detects dynamics and occlusions (dark regions), and the binary mask ùëÄùëé finds invalid stationary points (black pixels)

Full size image
Masking Scheme
The assumption of a moving camera and a static scene is underlying in the unsupervised depth learning framework, where the moving objects in the scene and image pairs with identity camera pose provide invalid signals. To be specific, the moving objects create the non-rigid flow that cannot be represented by the depth-based mapping, and the static camera consistently creates the identical flow that is independent to the depth prediction. Therefore, we propose to mask out these regions by introducing a self-discovered mask (ùëÄùë†) and adopting the auto-mask (ùëÄùëé) by Monodepth2 (Godard et al. 2019). The proposed ùëÄùë† computes weights (ranging from 0 to 1) for points in ÓâÇ by checking their depth consistency, and the ùëÄùëé simply removes invalid points from ÓâÇ. The proposed two masks are readily integrated into the proposed learning framework.

Self Discovered Mask
As moving objects and occlusions naturally violate the geometry consistency assumption, they will cause large depth inconsistency in our pre-computed ùê∑diff (Eq. 5). This encourages us to define the ùëÄùë† as:

ùëÄùë†=1‚àíùê∑diff,
(7)
where the ùëÄùë† is in [0, 1] and it attentively assign low weights for geometrically inconsistent pixels and high weights for consistent pixels.

Auto-Mask
To remove the invalid points in static pairs, e.g., two images are captured at the same position, we use the auto-mask ùëÄùëé that is proposed in Godard et al. (2019). It compares the photometric losses between the mapping by depth and pose and the identity mapping, and it removes the points where the identity mapping leads to a lower loss. Formally, for each ùëù‚ààÓâÇ, we have

ùëÄùëé(ùëù)={10if ‚Äñùêºùëé(ùëù)‚àíùêº‚Ä≤ùëé(ùëù)‚Äñ1<‚Äñùêºùëé(ùëù)‚àíùêºùëè(ùëù)‚Äñ1,otherwise.
(8)
Here ùëÄùëé is a binary mask for each point in ÓâÇ (valid points), and ùêº‚Ä≤ùëé is the warped image from the source image ùêºùëè using the estimated depth and pose. It makes the network to ignore objects that move at the same velocity as the camera, and it even ignores whole frames when the relative pose is identity.

How to use Masks
First, to use ùëÄùëé in our loss function, we remove invalid points in ÓâÇ that have ùëÄùëé(ùëù)=0. When training the network, we only compute losses on the remaining valid points. Second, we use the proposed ùëÄùë† to re-weight the photometric loss in Eq. 2 by:

ùêøùëÄùëÉ=1|ÓâÇ|‚àëùëù‚ààÓâÇ(ùëÄùë†(ùëù)‚ãÖùêøùëÉ(ùëù)).
(9)
This mitigates the noisy signals caused by moving objects and occlusions. Fig. 4 shows visual results for the two types of masks, which coincides with our anticipation. The dark regions in ùëÄùë† correspond to moving objects that violate the static scene assumption, e.g., the car region and human ride region. In the binary ùëÄùëé, black regions correspond to pixels that have similar speed with the camera, e.g., the moving vehicle in the left example and the static scene in the right example. Table 2 shows the ablation study results, which shows that the proposed masks results in a significant performance improvement.

Fig. 5
figure 5
Pipeline of Pseudo-RGBD SLAM. For the current frame ùêºùë°, we first estimate its depth ùê∑ùë° using our trained depth CNN. Then, we estimate its relative pose to previous frame ùêºùë°‚àí1 (its pose ùëÉùë°‚àí1 has been known in previous tracking) to recover the current pose. Next, we feed the color images, predicted depths, and estimated poses into ORB-SLAM2 (Mur-Artal and Tard√≥s 2017), which outputs the accurate camera trajectory and a sparse 3D map. Finally, given the consistent depth and camera trajectory, we construct the dense voxel representation using InfiniTAMv3 (Prisacariu et al. 2017). Note that the dense reconstruction here is only used for qualitative demonstration

Full size image
Pseudo-RGBD SLAM
In this section, we present a Pseudo-RGBD SLAM system, which is based on our trained models and existing SLAM systems. We overview the system pipeline in Sect. 4.1, followed by elaborating each component in Sect. 4.2, and finally, we discuss the advantages and limitations of the proposed system in Sect. 4.3.

System Overview
Fig. 5 shows an overview of the proposed method, which is composed of our SC-Depth, ORB-SLAM2 (Mur-Artal and Tard√≥s 2017), and InfiniTAMv3 (Prisacariu et al. 2017). The whole system takes a monocular RGB video as input and outputs a globally consistent 6-DoF camera trajectory and sparse/dense 3D maps. First, we initialize the tracking and mapping by using the predicted depth on the starting frame ùêº0, which creates an initial 3D map. Second, for a new frame ùêºùë°, we estimate its depth and relative pose to the previous view ùêºùë°‚àí1 using our trained networks. As the camera pose of ùêºùë°‚àí1 has been known from previous tracking or initialization, we can obtain the pose estimate for the current view by accumulation. Third, we feed the color image, depth map, and the estimated pose for ùêºùë° as input into ORB-SLAM2 (Mur-Artal and Tard√≥s 2017), which performs matching and optimization, resulting in an optimized camera pose as well as an increased map. In such an incremental way, we eventually obtain a globally consistent camera trajectory and a sparse 3D map from the video. Finally, we feed the color images, depth maps, and camera trajectories into InfiniTAMv3 (Prisacariu et al. 2017), which fuses depth maps to construct the dense and textured voxel volumes (Fig. 6).

System Details
ORB-SLAM2 The original RGB-D system takes the sensor captured depth as input, while we use the estimated depth. It relies on ORB features (Rublee et al. 2011) to generate correspondences, and it minimizes the reprojection error for pose optimization. Poor correspondences (beyond the error threshold) are detected and removed as outliers, and the remaining correspondences are used for all sub-tasks, including tracking, mapping, loop closing, and re-localization. We will elaborate on how our predicted depth and pose influence the correspondence and optimization in the system (Fig. 7).

Depth The predicted depths are used to initialize a 3D map at the beginning, and they are used in the objective function during optimization. Specifically, beyond 2d reprojection error, the system also minimizes the difference between the projected depth (from the 3D map to the image) and the predicted depth. Formally,

{ùê∏2ùê∑=(ùëùùë•‚àíùëù‚Ä≤ùë•)2+(ùëùùë¶‚àíùëù‚Ä≤ùë¶)2‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚àöùê∏3ùê∑=(ùëùùë•‚àíùëù‚Ä≤ùë•)2+(ùëùùë¶‚àíùëù‚Ä≤ùë¶)2+(ùëùùëë‚àíùëù‚Ä≤ùëë)2‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚àö,
(10)
where p stands for points in current image plane, and ùëù‚Ä≤ stands for points projected from 3D map. ùëùùëë and ùëù‚Ä≤ùëë are their disparities, i.e., inverse depths. Note that ùëùùëë is computed from our predicted depth map, so it is unavailable in the monocular system. This extends the reprojection error from 2D into 3D, which greatly improves the performance. Consequently, the consistency of estimated depths is vital in tracking. For example, inconsistent depths would increase the reprojection error, and correct matches would be wrongly removed as outliers, which causes the system to fail‚ÄîSee Fig. 9.

Pose The predicted pose is used as the initial pose during tracking, in which the system first projects the sparse keypoints in a 3D map to the live view using the estimated pose and then searches for correspondences in the neighboring regions. The camera pose is optimized through the Bundle Adjustment (Mur-Artal and Tard√≥s 2017). After tracking, we enrich the 3D map by unprojecting the keypoints detected in the live view to the map using the optimized camera pose. The original ORB-SLAM2 uses the constant velocity motion model for initial pose, which simply assumes that the camera motion is the same as the previous frame. Formally,

ùëáùë°‚Üíùë°+1={ùëáùë°‚àí1‚Üíùë°PoseNet(ùêºùë°,ùêºùë°+1)ORB-SLAM2,Ours,
(11)
where T stands for relative pose. However, this assumption is often violated in real scenarios, such as abrupt motion in driving scenes. Though these frames are few in the sequence, they usually contribute the most of the drift in the final evaluation. Our trained pose CNN has the potential to cope with these cases.

InfiniTAMv3 It takes RGB-D videos and can densely reconstruct the scene structure. We disable the internal tracking module and use our optimized camera poses and predicted depths for reconstruction. This is only used for visualization purposes, and it is also a demonstration of our consistent results. Note that the dense reconstruction is very sensitive to geometry consistency, i.e., it will crash when depths are not sufficiently consistent. Figure 11 shows the screenshot of our demo, which can be found in the supplementary material.

Discussion
Our proposed SLAM system leverages the advantage of deep learning, and it optimizes the predicted poses in the multi-view geometry-based framework. This has distinctive advantages over existing solutions.

Advantages Compared with classical monocular SLAM systems such as ORB-SLAM2 (Mur-Artal and Tard√≥s 2017), our advantages include:

1.
ORB-SLAM2 is often hard to initialize because it requires the multi-view triangulation, while our method can initialize at any time without latency by using the estimated dense depth‚ÄîSee Fig. 9.

2.
ORB-SLAM2 often loses tracking when the 3D map is over-sparse, while our method is more robust because we can enrich the map by using the predicted dense depth‚ÄîSee Fig. 9.

3.
ORB-SLAM2 can only provide a sparse map, while our method enables dense 3D reconstruction by using the predicted dense depth‚ÄîSee Fig. 12.

Compared with learning-based methods (Li et al. 2018), our advantage is the post geometric optimization e.g., Loop Closing (Mur-Artal and Tard√≥s 2014), which can effectively correct drifts and improve the performance, as shown in Fig. 8.

Limitations Our method cannot recover the absolute scale because only monocular videos are used. However, in real-world applications, the metric scale can be recovered by using other sensors and cues, like IMU and road landmarks.

Table 1 Single-view depth estimation results on KITTI (Geiger et al. 2013). Legends: D‚Äîdepth supervision; S‚Äîstereo pairs; M‚Äîmonocular snippets; L‚Äîsemantic labels or networks; F‚Äîjoint learning with optical flow
Full size table
Experiments
Implementation details
Network architecture Our depth network takes a single RGB image as input and outputs an inverse depth map. It is a U-Net structure (Ronneberger et al. 2015), and we use the ResNet50 (He et al. 2016) encoder to extract features. The decoder is the DispNet as used in Zhou et al. (2017). The activations are sigmoids at the output layer and ELU nonlinearities (Clevert et al. 2015) elsewhere. We convert the sigmoid output x to depth with ùê∑=1/(ùëéùë•+ùëè), where a and b are chosen to constrain D between 0.1 and 100 units. It is a widely assumed depth range for outdoor driving scenes, which is the same with all related works (Ranjan et al. 2019; Yin and Shi 2018; Zhou et al. 2017). Besides, our pose network accepts two RGB frames as input and outputs their 6D relative camera pose. We use the ResNet18 (He et al. 2016) encoder to extract features. In order to accept two frames, we modify the first layer to have six channels. Then features are decoded to 6-DoF parameters via four convolutional layers.

Single scale supervision Previous methods compute the losses on an image pyramid, i.e., usually four layers. They either work on the decoder‚Äôs side outputs (Yin and Shi 2018; Zhou et al. 2017; Zou et al. 2018) or upsample them to the original image resolution (Godard et al. 2019). However, it introduces great computational overhead in training. By contrast, we only compute the loss on the original image resolution. This has a less computational cost and achieves on par performance with the multi-scale solution in MonoDepth2 (Godard et al. 2019), as shown in Table 2. The motivation is that we empirically find that the supervision on low-resolution images is inaccurate, and the camera movement between training image pairs is small so that the multi-scale solution is unnecessary.

Training details We implement the proposed method using the PyTorch (Paszke et al. 2017). Following (Ranjan et al. 2019; Wang et al. 2018; Zhou et al. 2017), we use a snippet of three sequential video frames as a training sample. We compute the projection and losses from the second frame to others and reverse them again for maximizing the data usage. The images are augmented with random scaling, cropping, and horizontal flips during training. We use ADAM (Kingma and Ba 2014) optimizer and set the learning rate to be 10‚àí4. During training, we set ùõº=1.0, ùõΩ=0.1, and ùõæ=0.5 in Eq. 1. For fast convergence, we initialize the encoder of our networks by using the pre-trained model on ImageNet (Deng et al. 2009).

Datasets For depth estimation evaluation, we use both the KITTI (Geiger et al. 2013) and NYUv2 (Silberman et al. 2012) datasets. In KITTI, we use the same training/testing split as in Zhou et al. (2017). The 697 images are used for testing. We train the network for 200K iterations, where we set the batch size to be 4 and resize images to 832√ó256 resolution for training. In NYUv2, we use the officially provided 654 densely labeled images for testing, and use the rest sequences (no overlapping with the testing scenes) for training. We extract one frame from every 10 frame in the original video to remove redundant frames, and we resize images to 320√ó256 resolution as input of the network. We train models for 50 epochs, and the batch size is 8. For visual odometry evaluation, we use the KITTI odometry dataset (Seq. 00-08) for training, and we test our method on the Seq. 09-10. Moreover, we use the KAIST urban dataset (Jeong et al. 2019) to validate the zero-shot generalization ability of our method. We use one of the hardest scenes (urban39-pankyo), which contains more than 18000 street-view images, and we split it into 9 sequences with each sequence containing 2000 images for testing.

Table 2 Ablation study results on KITTI. We use the ResNet18 model, and the image resolution is 416√ó128
Full size table
Table 3 Trade-offs between image resolution, network, and speed. We train models on KITTI using a TESLA V100 GPU and test the inference speed in an RTX 2080 GPU
Full size table
Fig. 6
figure 6
Qualitative comparison with the Monodepth2 (Godard et al. 2019) on KITTI

Full size image
Evaluation metrics For depth evaluation, following previous methods (Yin et al. 2019; Zhou et al. 2017), we use the mean absolute relative error (AbsRel), mean log10 error (Log10), root mean squared error (RMS), root mean squared log error (RMSlog), and the accuracy under threshold (ùõøùëñ < 1.25ùëñ, ùëñ=1,2,3). As unsupervised methods cannot recover the absolute scale, we multiply the predicted depth maps by a scalar that matches the median with that of the ground truth, as in Zhou et al. (2017). The predicted depths are capped at 80m/10m in KITTI and NYUv2 datasets, respectively. For visual odometry evaluation, we follow the standard evaluation metrics, including the translational (ùë°ùëíùëüùëü) and rotational errors (ùëüùëíùëüùëü) averaged over the entire sequence (Geiger et al. 2013), and the absolute trajectory error (ATE) (Sturm et al. 2012).

Depth Estimation
Results on KITTI Table 1 shows the results, which shows that the supervised methods (Fu et al. 2018; Yin et al. 2019) are best-performing, followed by the stereo trained models (Yang et al. 2020). Besides, it shows that learning with semantic labels (Guizilini et al. 2020b) or optical flow (Zhao et al. 2020) can effectively improve the performance of monocular methods. We are here more interested in the monocular methods that do not use additional information. In this category, our method outperforms previous methods (before 2020), and it shows on par performance with the MonoDepth2 (Godard et al. 2019). However, we argue that our advantage against Monodepth2 is the depth consistency (Table 6), which has important implications on downstream video-based tasks. For example, contributed to the consistent depth prediction, our method can be readily plugged into the Visual SLAM systems, while the Monodepth2 is unable‚ÄîSee Fig. 9 for detailed analysis.

Table 4 Single-view depth estimation results on NYUv2 (Silberman et al. 2012). Legends: D‚Äîdepth supervision; M‚Äîunsupervised training using monocular snippets; F‚Äîjoint learning with the optical flow; WR‚Äîweak rectification (Bian et al. 2020b) which pre-processes the hand-held camera captured videos for better training. More specifically, (Bian et al. 2020b) remove the relative rotation between training pairs since they find that it is hard for the pose network to learn image rotation
Full size table
Efficacy of the proposed methods Table 2 summarizes the results. It shows that the proposed ùêøùê∫ makes training more stable by enforcing depth consistency, and the proposed ùëÄùë† can boost performance significantly by handling scene dynamics. Besides, it shows that using ùëÄùëé can contribute to extra marginal performance improvement by removing the stationary points. Consequently, the final solution (with all terms) can achieve the best performance. Moreover, Table 3 shows the relation between depth accuracy, training time, inference speed, network architecture, and image resolution.

Multi-scale supervision Table 2 shows the results of our method with the modified multi-scale solution proposed in Godard et al. (2019). It upsamples the predicted four depth maps to original image resolution and then computes losses instead of downsampling the original color image (Zhou et al. 2017). The result demonstrates that our method could hardly benefit from that, and it requires two times longer time for training. Therefore, we use single-scale supervision in our framework.

SSIM vs NCC Table 2 shows the results of our method with the normalized cross-correlation (NCC) loss, in which we replace the SSIM. Both losses compute the local image similarity on a 3 by 3 patch. The results show that SSIM leads to better performance than NCC in our unsupervised learning framework.

Results on NYUv2 Table 4 shows the results, which shows that our method outperforms previous unsupervised methods by a large margin. Besides, following (Bian et al. 2020b), we remove the relative rotation between training image pairs since they find that it is hard for the pose network to learn image rotation. This leads to a significant improvement because rotation is the dominate ego-motion in hand-held camera captured videos. Zhao et al. (2020) solves the problem by replacing the Pose CNN with a traditional geometry-based pose solver. The qualitative results are shown in Fig. 7. We find that MonoDepth2 (Godard et al. 2019) often collapses in training, so we report the best result. Compared with the supervised methods, our method is inferior to the state-of-the-art (Yin et al. 2019) but outperforms many previous methods (Chakrabarti et al. 2016; Eigen and Fergus 2015; Li et al. 2017; Liu et al. 2016; Saxena et al. 2006; Wang et al. 2015).

Fig. 7
figure 7
Qualitative comparison with the state-of-the art unsupervised methods on NYUv2

Full size image
Table 5 Visual odometry results on KITTI (Geiger et al. 2013). S/M stands for training on stereo/monocular videos, and G stands for geometric optimization. ‚úó stands for failure in initialization or tracking
Full size table
Fig. 8
figure 8
Estimated trajectory on Seq. 09 (left) and 05 (right). The results optimized by the proposed Pseudo-RGBD SLAM are more accurate than our SC-Depth and other learning-based methods, and the improvement is especially large when loops are detected and closed. For example, the ùë°ùëíùëüùëü is reduced from 5.91 to 1.67 on Seq. 05

Full size image
Visual Odometry
Comparing with deep learning based methods Table 5 shows the visual odometry results on KITTI. For methods that train on monocular videos, we align the scale of their predicted results with the ground truth by using the 7-DoF optimization. The results show that the proposed SC-Depth outperforms the previous monocular alternatives, and it even shows on par performance with the stereo trained method (Li et al. 2018). However, it is not as good as the very recent approach (Zou et al. 2020) that models the long-term geometry by using the LSTM module. Besides, the results show that the proposed Pseudo-RGBD SLAM system improves the accuracy significantly over our SC-Depth, which is contributed to the geometric optimization. The success of D3VO (Yang et al. 2020) also confirms the importance of geometric optimization for odometry accuracy. However, note that stereo-trained methods can estimate depths at the metric scale, which are readily optimized in existing SLAM frameworks. By contrast, the monocular trained methods suffer from the scale inconsistency issue, which makes the post-optimization non-trivial‚ÄîSee Fig. 9. Our contribution here is enabling the monocular trained methods to predict the scale consistent results so that it allows for optimizing the predicted depths and poses by using the classical geometric frameworks. A qualitative comparison is provided in Fig. 8, which shows that the trajectory optimized by our Pseudo-RGBD SLAM is more well-aligned with the ground truth than our SC-Depth and other learning-based methods.

Table 6 Depth consistency results on Seq. 09. Fitness measures the overlapping area of two point clouds (# of inlier correspondences / # of points in target). RMSE is averaged over all inlier correspondences (#Corr)
Full size table
Fig. 9
figure 9
Number of tracked keypoints on Seq. 09. We extract 2000 feature points for all methods, and the values in the figure are smoothed for visualization

Full size image
Table 7 Visual odometry results on KITTI. We evaluate the results on all frames and on keyframes that are selected by the ORB-SLAM2 since the latter cannot provide results for the full sequence due to unsuccessful initialization or tracking failure. The ATE (m) metric is used. We use 2K keypoints as default, and we analyze the effect of keypoint numbers on system performance by increasing it to 8K
Full size table
Fig. 10
figure 10
Estimated trajectory on Seq. 08. ORB-SLAM2 is hard to maintain consistent scales over a long video (e.g., left is small, and right is big), while our method is able by leveraging the scale-consistent depth prediction

Full size image
Table 8 Zero-short generalization on KAIST dataset (Jeong et al. 2019). We compare our method with ORB-SLAM2 using the ATE (m) metric
Full size table
Depth consistency evaluation We evaluate the geometry consistency of predicted depths by using the point cloud registration metric that is implemented in the Open3D library (Zhou et al. 2018). To be specific, we use the ‚Äúopen3d.registration.evaluate_registration‚Äù function. It computes the RMSE of two aligned point clouds and recognizes the inlier correspondences by a constant threshold. Then it measures the overlapping area of point clouds by counting the ratio of inlier correspondences in all the target points. More details can be founded in the Open3D library. For a given testing sequence, we predict the depth and relative pose for every adjacent image pair, and we convert the depth into point clouds for evaluation, where all the depth maps are resized to 832√ó256 resolution for a fair comparison. Table 6 shows the results, where we compare our method with Monodepth2 (Godard et al. 2019). It shows that our predicted depths are significantly more consistent than the latter, and we hypothesize this is the reason why our method can be readily plugged into the ORB-SLAM2 system while the Monodepth2 fails. We conduct a more detailed comparison by reporting the number of tracked keypoints in each frame. The results are shown in Fig. 9.

Fig. 11
figure 11
Dense multi-view reconstruction on Seq. 09. The left column shows the reconstructed 3D voxels. The right column shows the input RGB image, estimated depth map. We use the depth CNN trained on Seq. 00-08, and the predicted depth is cropped and masked by using our proposed ùëÄùë†

Full size image
Pose network or motion model Table 5 shows the results, where using the built-in motion model in ORB-SLAM or using our pose CNN for pose initialization leads to similar performance. We conjecture the reason is that the motion model is satisfied in most driving scenarios, where forward motion is dominant. However, we believe that using the pose network is a more general solution because the constant velocity model is violated when abrupt motion occurs.

Fig. 12
figure 12
Point cloud visualization on Seq. 09. For each incoming image (right 1st row), we predict the depth map (right 2nd row) using our trained network and convert it to a 3D point cloud, which is rendered using the color image and visualized in an eye-bird view (left)

Full size image
Comparing with ORB-SLAM2 Table 7 shows the odometry results on KITTI (Geiger et al. 2013). We evaluate results on all frames and on keyframes that are selected by ORB-SLAM2 (Mur-Artal and Tard√≥s 2017), since the latter cannot provide results for the full sequence due to unsuccessful initialization or tracking failure. The results on eleven sequences show that our method either achieves on par accuracy with the ORB-SLAM2 or significantly outperforms the latter. Besides tracking accuracy, our system is more robust than the ORB-SLAM2. A detailed comparison is provided in Fig. 9, where our method always tracks more points than the latter (e.g., about 800 vs 100). Moreover, we find that ORB-SLAM2 sometimes suffers from heavy scale drifts in long sequences‚ÄîSee Fig. 10 where ORB-SLAM2 provides inconsistent scales between left and right parts. In this scenario, our method can maintain a consistent scale over the entire sequence by leveraging the scale-consistent depth prediction.

Using more or less keypoints Table 7 shows the ablation study results, where our system with 2K keypoints is more accurate than that with 8K keypoints. We conjecture the reason is that using more keypoints would also introduce more outliers, while the geometric optimization requires only a few accurate sparse points. We hence recommend users choosing keypoint numbers by considering the trade-off between the system accuracy and robustness (Fig. 9).

Zero-short generalization We validate the generalization ability of our proposed Pseudo-RGBD SLAM on KAIST urban dataset (Jeong et al. 2019), where our models are trained on KITTI. The results are reported in Table 8. It shows that our method consistently outperforms ORB-SLAM2, which demonstrates the robustness of our proposed system. Moreover, we demonstrate the generalization ability of our method by presenting a real-world demo‚ÄîSee Fig. 1.

Qualitative Evaluation
We provide several demos in the supplementary material, which are briefly described below.

Per-frame 3D visualization Fig. 12 shows the visualization for predicted depths and textured point clouds on Seq. 09. We use the model trained on Seq. 00-08. This demo is to show that the predicted scene or object structure by our trained depth CNN is visually reasonable, and their scales are consistent over time. Note that inconsistent prediction would cause flickering videos, while it is doesn‚Äôt occur in our demo.

Dense multiple view reconstruction Fig. 11 shows our dense reconstruction demo. As the depth range is wild in outdoor scenes, we have to reduce the voxel size of TSDF (Curless and Levoy 1996) for affording the memory requirement, which degrades the reconstruction quality. Although the reconstruction is inferior to the state-of-the-art methods, this demo clearly demonstrates the high consistency of our estimated depths.

Generalization on real-world videos Fig. 1 shows the depth and camera trajectory generated by our method on a self-captured driving video. The video is captured in Adelaide, Australia. We use a single camera, which is mounted on a driving car. Due to the lack of an accurate ground truth trajectory, we use the Google map for qualitative evaluation. The scene is so challenging that ORB-SLAM2 (Mur-Artal and Tard√≥s 2017) is unable to generate a complete trajectory, while the proposed Pseudo-RGBD SLAM performs well(Fig. 12).

Conclusion
This paper proposes a video-based unsupervised depth learning method. Thanks to the proposed geometry consistency loss and masking scheme, our trained network can predict scale-consistent and accurate depths over a video. The depth accuracy is comprehensively evaluated in both indoor and outdoor scenes, and the quantitative results are attached. Besides, we demonstrate better consistency against the related work which shows on par depth accuracy to our method, and we show that such consistency enables our method to be readily plugged into the existing Visual SLAM system. This shows the possibility of leveraging the depth network that is unsupervised trained from monocular videos for camera tracking and dense reconstruction.