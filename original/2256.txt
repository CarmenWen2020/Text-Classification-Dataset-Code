We aim to tackle the interesting yet challenging problem of generating videos of diverse and natural human motions from prescribed action categories. The key issue lies in the ability to synthesize multiple distinct motion sequences that are realistic in their visual appearances. It is achieved in this paper by a two-step process that maintains internal 3D pose and shape representations, action2motion and motion2video. Action2motion stochastically generates plausible 3D pose sequences of a prescribed action category, which are processed and rendered by motion2video to form 2D videos. Specifically, the Lie algebraic theory is engaged in representing natural human motions following the physical law of human kinematics; a temporal variational auto-encoder is developed that encourages diversity of output motions. Moreover, given an additional input image of a clothed human character, an entire pipeline is proposed to extract his/her 3D detailed shape, and to render in videos the plausible motions from different views. This is realized by improving existing methods to extract 3D human shapes and textures from single 2D images, rigging, animating, and rendering to form 2D videos of human motions. It also necessitates the curation and reannotation of 3D human motion datasets for training purpose. Thorough empirical experiments including ablation study, qualitative and quantitative evaluations manifest the applicability of our approach, and demonstrate its competitiveness in addressing related tasks, where components of our approach are compared favorably to the state-of-the-arts.

Access provided by University of Auckland Library

Introduction
Human-centric activities always play a key role in our daily life. In recent years, noticeable progresses have been made in video forecasting (Wu et al. 2020; Gao et al. 2019) and synthesis (Zhu et al. 2020; Tulyakov et al. 2018; Vondrick and Torralba 2017; Denton and Fergus 2018). Meanwhile, it remains a substantial challenge in generating realistic videos of diverse and plausible human motions. This is evidenced in many recent video generation efforts (Yang et al. 2018; Cai et al. 2018; Kim et al. 2019), where the appearances of synthesized human characters are unfortunately either blurring or surreal, and are still far from being photo-realistic; their motions are often distorted and unnatural.

Fig. 1
figure 1
Our action2video pipeline generates human full-body motion videos of prescribed actions in two steps: action2motion first generates diverse and natural 3D motions of predefined actions; motion2video proceeds to extract 3D surface shape and texture from an additional 2D input image, and to render 2D videos of the generated motions

Full size image
These observations stress the importance of properly modeling human body postures and temporal articulations, as well as the surface shapes and textures of the local body parts. It also motivates us to examine the problem of generating videos of human motions based on action categories, the basic ingredient of human behaviors.

Due to the complexity of human articulations and pose dynamics, generating human videos is far from being trivial. Existing efforts usually represent human motions in 2D space, which are then rendered pixel-wise to form 2D videos. Moreover, extra information such as an initial 2D pose or a partial/entire motion sequence is usually required, which is practically undesirable. For instance, Yang et al. (2018) produces deterministic sequence of 2D motions, which is followed by synthesizing the appearances frame-by-frame through adversarial training. Action-conditioned 2D human behavior modeling is also studied in Cai et al. (2018), where 2D pose generator and motion generator are trained progressively. Very recently, the efforts of (Weng et al. 2019; Huang et al. 2020) consider the related task of extracting 3D characters from single images, which is then animated to form 3D motions; de Souza et al. (2020) addresses another related task of generating human action videos by composing the human motions and scenes with probabilistic graphical models in 3D game engine;. However, the motions used in both methods are real-life motions that have been made available in prior, instead of being synthesized on the spot.

Overall, the existing methods fall short in the following aspects: (1) direct modeling of 2D motions is inherently insufficient to capture the underlying 3D human pose articulations and shape deformations. The absence of 3D geometric information often leads to visual distortions and ambiguities; (2) coordinate locations of body joints are commonly used as the human pose representation, which undesirably entangle the human skeletons and their motion trajectories. Moreover, this creates extra barriers in modeling human kinematics; (3) initial poses often impede the diversity of generated human dynamics. For example, in actions such as warm up and boxing, initial poses crucially influence the formation of the rest sequences; and (4) the popular choice of pixel-to-pixel synthesis among existing efforts on action conditioned video generation has been evidenced incapable of generating detailed and high-resolution views. The aforementioned observations inspire us to consider a two-step pipeline: action2motion generates diverse and natural 3D human motions from prescribed action categories, and motion2video proceeds to extract human character out of an additional input image, to rig, animate, and render to form 2D videos, as illustrated in Fig. 1.

In action2motion, we aim at generating diverse motions to traverse the motion space, and to cover various styles of individuals performing the same type of actions; meanwhile, each motion is expected to be visually plausible. This leads to our temporal variational auto-encoder (VAE) approach using Lie algebra pose representation. Inspired by the work of Denton and Fergus (2018) in generic video generation, here we leverage the posterior distribution learned from previous poses as a learned prior to gauge the generation of present pose; by tapping into the recurrent neural net (RNN) implementation, this learned prior also encapsulates temporal dependencies across consecutive poses. For pose representation, human pose could be characterized as a kinematic tree based on human body kinematics. There are multiple advantages of using Lie algebraic representation over the popular joint-coordinate representation: (i) lie representation disentangles the skeleton anatomy, temporal dynamics, and scale information; (ii) it faithfully encodes the anatomical constraints of skeletons by following the forward kinematics (Murray et al. 1994); (iii) the dimension of Lie algebraic space corresponds exactly to the degree of freedom (DoF), which is more compact compared to joint-coordinate representation. In practice, the adoption of Lie representation notably mitigates the change-of-length and trembling phenomenons prevailing in joint coordinates representations; it also facilitates the generation of natural, lifelike motions, and simplifies the training process. Furthermore, a global and local movement integration module is used to infer the global pose trajectory from temporal articulations of body parts. This promotes consistence between local shape deformations and global motion trajectory (i.e. direction and velocity), especially when synthesizing locomotion actions such as walking and jumping.

It is followed in our pipeline by motion2video, where a 3D character is extracted, rigged, animated according with stochastically generated motions, and rendered to form 2D videos. In fact, animating 3D characters remains an open problem. A common strategy is to extract their 3D shapes and textures from a single input image. Prior efforts such as Weng et al. (2019) align the silhouette and texture of single image to a 3D human shape (e.g. SMPL (Loper et al. 2015)). Due to single input view, nonetheless, they fail to synthesize body textures of unseen views. Recent deep learning methods (Lazova et al. 2019; Saito et al. 2019, 2020; Huang et al. 2020; Zheng et al. 2021) shed lights on reliable recovery of 3D surfaces and textures from single images. Meanwhile their results suffer from either low-fidelity, with input image resolution limited to at most 512×512 (Saito et al. 2019; Huang et al. 2020; Zheng et al. 2021), or ill-posed texturing on occluded areas and novel view (Saito et al. 2020). A simple strategy is developed in our work, leading to improved texture mapping in these cases.

In summary, our main contributions are three-fold: first, a novel two-step pipeline of action2motion and motion2video is proposed to address the challenging problem of 3D human motion and video generation from action type and single image; second, a dedicated Lie Algebra based VAE framework is developed, capable of producing diverse life-like human motions from prescribed action categories; third, as part of our pipeline, an improved strategy is used in extracting 3D shapes and textures from single images, that is capable of synthesizing visually-appealing texture of unseen views. Moreover, an in-house 3D human motion dataset, HumanAct12, has been curated.

This paper differs from our preceding effort (Guo et al. 2020) in a number of aspects:

A more general problem of 3D human video generation is considered here, where the task of action2motion examined in Guo et al. (2020) becomes the first step of our solution pipeline. The motion2video step is entirely new from Guo et al. (2020).

A new local-global movement integration module is proposed, which significantly improves the synthesized 3D locomotion results when comparing to Guo et al. (2020).

A much broader and more thorough discussion is provided comparing to our short version (Guo et al. 2020). It also includes applications to latent interpolation, action transition, outpainting, as well as evaluation of the synthesized motions from coarse- versus fine-grained action categories.

Related Work
Our focus is to review literature related to generating video of human full-body motions, instead of the more generic theme of video generation (Tulyakov et al. 2018; Denton et al. 2017; Vondrick et al. 2016). Our tally includes the discussion of action video generation (Sect. 2.1), the generation of human motions (Sect. 2.2), motion transfer and rigid body animation (Sect. 2.3). We also review related activities of VAE sequence modeling (Sect. 2.4), skeletal human pose representation (Sect. 2.5), and 3D human motion datasets (Sect. 2.6).

Action Video Generation
The task of generating human action videos has drawn research attentions very recently. In the work of Cai et al. (2018), 2D human motions are generated from known actions, they are then synthesized into 2D videos frame-by-frame with U-Net (Ronneberger et al. 2015) and a dedicated image discriminator. In Yang et al. (2018), based on an initial 2D pose extracted from a given image, a deterministic sequence of future 2D poses is produced for given action category; this pose sequence are subsequently used to guide video generation via adversarial training. A similar method is considered in Kim et al. (2019), where future 2D poses are instead generated stochastically with variational auto-encoder. These efforts focus on tiny pixel-wise video generation, and human poses are manipulated in 2D image space. A recent work (de Souza et al. 2020) propose to generate 3D human videos directly from 3D game engine using scene composition rules and procedural animation techniques. Our work differs from this work in two folds: (1) de Souza et al. (2020) generate 3D motions by extracting atomic motions from existing motion capture (MoCap) datasets, then stitches these atomic motions into action sequences through predefined rules. For example, a walking animation involves repetitions of swinging a left leg, then swinging a right leg, as well as corresponding pendular arm movements. However, this process is fairly labor-intensive. In our work, diverse 3D actions are automatically produced from a learned generative model end-to-end; (2) de Souza et al. (2020) animate artist-designed 3D avatars (rigid and clothed), while our method generates videos by rigging and animating characters with their 3D shapes and textures extracted from single 2D images.

Human Motion Generation
In addition to video generation, there are also research efforts focusing on synthesizing human motions, usually in the form of 2D or 3D skeletons, where the input could be of various forms, including but not limited to audio and text. One trendy research direction aims to generate deterministic motion sequences, which is typically realized by RNN models. For example, Tang et al. (2018) and Shlizerman et al. (2018) adopt LSTM models to translate music beats to body motion dynamics. In the efforts of Lin et al. (2018), Ahn et al. (2018), Plappert et al. (2018), and Yamada et al. (2018), human motions are generated from textual descriptions through a encoder-decoder RNN model. Ahuja and Morency (2019) considers a closely related task of constructing a joint embedding space between sentences and human pose sequences. The work of Stoll et al. (2020) engages neural machine translation model with attention mechanism for text-to-sign-pose prediction. Similarly, a recurrent architecture is used in Pavllo et al. (2020) to unfold an input global trajectory to locomotive humanoid movements.

To enable the stochasticity of human dynamics, deep generative models are also considered. Habibie et al. (2017) propose a recurrent variational autoencoder model for global trajectory based locomotion generation. Lee et al. (2019) use GANs model to generate diverse movements from music signals. Huang et al. (2021) explore a curriculum training strategy to allow variable sequence lengths. In Cai et al. (2018), a two-stage GAN framework is proposed to generate 2D human motion progressively. To synthesize human motions from scratch, Zhao et al. (2020) and Zhao and Ji (2018) make use of Bayesian inference; the work of Yan et al. (2019) instead considers a combined strategy of graph convolutional networks and GANs. The recent work of Xu et al. (2020) synthesizes novel motions by free combination of style and content codes extracted from existing MoCap library.

Motion Transfer and Rigid Body Animation
Motion transfer is a traditional topic, aiming to transfer human motions from a source object to target. Recent deep learning based efforts typically consider 2D pixel-wise approaches, where mappings from source and target are based on local pixels or 2D patches. Wang et al. (2018) and Chan et al. (2019), for example, directly learn to map between human poses and appearances of one specific source subject. The aim of (Siarohin et al. 2019; Wang et al. 2019a; Lee et al. 2020; Liu et al. 2019a) is to work toward a more general problem of driving an arbitrary target image with a source 2D pose sequence or videos. This is often realized by establishing connections between the source pose sequence and the target textured shape extracted from an given image, followed by warping the reference image to form the target video frame-by-frame. Although assembling promising results, the mainstream pixel-wise approaches nonetheless possess a number of limitations, including its innate difficulties in dealing with changing views or lifting to 3D motion spaces, as well as the level of complications in producing high-resolution and sharp images. The works of (Villegas et al. 2018; Aberman et al. 2020) also consider a similar task, where motions from the source 3D character are re-targeted to 3D characters with different skeletons (e.g. joint number, bone lengths). Meanwhile, the 3D shapes of these target characters have been artistically designed and well-rigged ahead of time.

Meanwhile, it has also been a continuous line of research on rigid body animation of 2D/3D human characters that is especially empowered by advances in computer graphics techniques. Early work such as Zhou et al. (2012) uses a simple pose-retrieval framework, where a segmented garment database indexed by 2D skeleton poses is built for online searching during human image animation. Rigged human models are exploited in later endeavors for articulated object animation. In Hornung et al. (2007), characters extracted from 2D pictures are driven as-rigid-as-possible by external 3D MoCap sequences. At intermediate steps, a 2D mesh with 2D skeleton is constructed for the shape extracted from input image. Weng et al. (2019) further lifts this animation process into 3D space. Specifically, a semi-naked SMPL template is drawn out of 2D images, and deformed to a rigged 3D mesh model with boundary that closely matches to the human silhouette in input image. The recent work of Huang et al. (2020) learns to directly predict a 3D animatable clothed human shape from a single image.

VAE in Sequence Modeling
Variational autoencoder (Kingma and Welling 2014) are the encoder-decoder neural nets trained by maximizing the marginal data likelihood with variational methods. It has been widely used in the so-called deep generative models as a powerful learning technique in addressing various learning scenarios, including conditional generation (Sohn et al. 2015), semi-supervised learning (Kingma et al. 2014; Siddharth et al. 2017), controllable generation (Cheng et al. 2020), few-shot learning (Schonfeld et al. 2019), disentangle representation learning (Ding et al. 2020; Zhu et al. 2020; Higgins et al. 2016) and VAE-GAN architecture (Larsen et al. 2016).

To work with sequential data, VAEs are typically plugged in a recurrent network model, e.g. GRU and LSTM. Variational RNN (Chung et al. 2015), a pioneer work, uses vanilla RNN to model temporal dependencies in intermediate time-frames. The RNN output of previous frame is used in generating posterior and prior distributions, as well as the follow-up decoding process. Variational RNN has been particularly favored in speech generation and handwriting character generation. Bowman et al. (2016) and Yang et al. (2017) investigate the LSTM-based VAE for NLP modelling based on a sequence-to-sequence architecture, where the sequence encoder predicts a posterior distribution, from which the sequence decoder samples a latent vector and reconstruct the sequence. More specifically, temporal VAE models has been considered in motion and video generation. Marwah et al. (2017) consider generating videos from textual caption, which is incorporated as semantic attentive vectors and fed to their temporal VAE. In VideoVAE (He et al. 2018), on the other hand, a structured latent unit is devised to model conditional factors including motion category and an initial frame to complete the rest frames. To predict future frames under uncertainty, Denton and Fergus (2018) inspect the use of two separate RNNs to capture temporal dependencies of conditional posterior and prior spaces. Similar network structure is also scrutinized in Wang et al. (2019b), where it is extended to synthesize videos with pre-specified start and end frames. In terms of 3D motion prediction, given a start human pose, Habibie et al. (2017) complete the rest 3D human motion with a LSTM-based VAE model. In Yan et al. (2018), similar model is engaged to learn the transition from observed sequence to future sequence for stochastic motion forecasting. A very recent work by Aliakbarian et al. (2020) adopts VAE and a mix-and-perturbation strategy to statistically predict future motions.

Skeletal Human Pose Representation
A number of human pose representations have been considered over the years. The most-often used option is the joint-coordinate representation (Han et al. 2017; Hussein et al. 2013) that directly characterizes the human pose by an ordered sequence of 2D/3D joint coordinates. It has a few variants: Wang et al. (2012) consider incorporating the pair-wise relative positions of neighboring joints; meanwhile, only those informative joints are utilized in Chaaraoui et al. (2014). Part-based method is another line of pose representation. Specifically, a human pose is modeled as a ordered list of body parts. For example, in Yacoob and Black (1999), human body is divided into five main parts (i.e. torso and four limbs); pose sequences are then formulated by the displacement and rotations of body parts over time. Alternatively, the work of Müller (2007) models the temporal information using dynamic time warping. Finally, Lie group or axis-angle based representation (Gavrila et al. 1995; Vemulapalli et al. 2014; Huang et al. 2017; Xu et al. 2017; Liu et al. 2019b; Pavllo et al. 2020) characterizes the skeleton as a kinematic tree, with its articulations realized by forward kinematics.

3D Human Motion Datasets
CMU MoCap (CMU 2003) and HDM05 (Müller et al. 2007) have more than 100,000 3D poses and 2000 3D motion sequences that are associated with succinct textual descriptions. Unfortunately, the motions are markedly uneven-distributed over action categories. UTKinect-Action (Xia et al. 2012) and MSR-Action3D (Li et al. 2010), on the other hand, have much smaller tally of motion sequences. NTU-RGBD (Liu et al. 2020) is by far the largest human motion dataset, consisting of over 100,000 motions belonging to 120 classes. Nevertheless, the joint positions acquired from Microsoft Kinect-I cameras are notably inaccurate. These observations motivate us instead curating our in-house 3D human action dataset, HumanAct12, as well as revamping the pose annotations of NTU-RGBD.

Preliminary Backgrounds
Variational Auto-Encoder
Variational auto-encoder(VAE) (Kingma and Welling 2014) consists of an encoder and a decoder, which are normally two separate neural networks. Its goal is to learn a 𝜃-parameterized generative model, 𝑝𝜃(𝐱,𝐳), over data 𝐱 and latent variables 𝐳. Technically, the learning objective is to maximize the likelihood function of 𝐱, which could be further formulated as a marginal likelihood with regard to the latent variable 𝐳, 𝑝𝜃(𝑥)=∫𝐳𝑝𝜃(𝐱|𝐳)𝑝𝜃(𝐳). Following the variational principle, a 𝜙-parameterized neural network(i.e. encoder), 𝑞𝜙(𝐳|𝐱), is engaged to approximate the unknown posterior distribution 𝑝𝜃(𝐳|𝐱). We thus obtain the the following evidence lower bound (ELBO) to our data likelihood function:

log𝑝𝜃(𝐱)=log∫𝐳𝑝𝜃(𝐱|𝐳)𝑝(𝐳)≥𝔼𝑞𝜙(𝐳|𝐱)log𝑝𝜃(𝐱|𝐳)−𝐷KL(𝑞𝜙(𝐳|𝐱)∥𝑝(𝐳)).
(1)
The first ELBO term encourages the generated samples to be sufficiently close to the real samples; the second term penalizes KL-divergence between the prior and the approximated posterior distribution. Subsequently, the original objective of maximizing the data likelihood over data 𝐱 becomes that of maximizing over the 𝜃- and 𝜙-parameterized ELBO function. In Sohn et al. (2015), a follow-up conditional variational auto-encoder (CVAE) framework is conceived by introducing a conditional variable, 𝐲, as

log𝑝𝜃(𝐱|𝐲)=log∫𝐳𝑝𝜃(𝐱|𝐳,𝐲)𝑝(𝐳|𝐲)≥𝔼𝑞𝜙(𝐳|𝐱,𝐲)log𝑝𝜃(𝐱|𝐳,𝐲)−𝐷KL(𝑞𝜙(𝐳|𝐱,𝐲)∥𝑝(𝐳)).
(2)
Lie Groups and Lie Algebras
In what follows, we provide a succinct introduction of Lie groups and Lie algebra basics. Interested readers may refer to (Murray et al. 1994) for more details.

Lie Groups
Mathematically, a Lie group is a group as well as a smooth manifold. 3D rotation transformations, also known as the Special Orthogonal group, SO3={𝑅∈ℝ3×3|𝑅⊤𝑅=𝐼,det(𝑅)=+1}, is a classical example of Lie group. Moreover, the product of multiple SO3 groups (i.e. a kinematic chain) is still a Lie group. In other words, for a tree-structured human skeleton model, each of the kinematic chains corresponds to a point in Lie group SO(3)×SO(3)×⋯×SO(3). As a consequence, it is usually far from being trivial in terms of optimization in such a curved space. We instead work in its tangent space, also known as Lie algebra 𝔰𝔬(3)—being a flat space, our familiar linear algebra techniques could work again.

Lie Algebras
The tangent space of Lie group SO(3) at identity I3 is referred to as its Lie algebra 𝔰𝔬(3). Each element of 𝔰𝔬(3) is in the form of a 3×3 skew-symmetric matrix 𝑊̂ , as

𝑊̂ =⎛⎝⎜⎜0𝑤3−𝑤2−𝑤30𝑤1𝑤2−𝑤10⎞⎠⎟⎟,
(3)
which essentially spans a 3-dimensional vector space, 𝐰=(𝑤1,𝑤2, 𝑤3)⊤∈ℝ3.

Exponential Map
To map from a Lie algebra element 𝑊̂ ∈𝔰𝔬(3) to a point in the manifold (i.e. Lie group), 𝑅∈SO(3), an exponential map exp:𝔰𝔬(3)→SO(3) is formulated as

𝑅=exp(𝑊̂ )=I+sin(‖𝐰‖)‖𝐰‖𝑊̂ +1−cos(‖𝐰‖)‖𝐰‖2𝑊̂ 2.
(4)
Here ‖⋅‖ is a vector norm. Since 𝐰 is periodically mapped to R, in practice we normally limit 𝐰 by its norm within the range of [−𝜋,𝜋]. Its inverse map, the logarithm map log(SO(3)): SO(3)→𝔰𝔬(3) map be similarly constructed.

Our Approach
The pipeline of our approach, action2video, consists of two steps: step one (action2motion) synthesizes human pose sequences from a prescribed action category (Sect. 4.1); step two (motion2video) extracts a specific 3D human shape and texture from a reference image to render the generated motions into 2D videos (Sect. 4.2).

Step One: Action2Motion
Our action2motion framework comprises a temporal VAE (Sect. 4.1.3) with a Lie algebra based representation (Sect. 4.1.1). We also investigate four strategies to decode neural hidden unit to obtain global 3D positions of motions (Sects. 4.1.4 and 4.1.5).

Disentangled Representation with Lie Algebra
As shown in Fig. 2, a human pose could be characterized in the form of a kinematic tree that consists of five kinematic chains: main spine and four limbs. Meanwhile, this skeleton model is formed by N oriented edges (i.e. bones) 𝐸={𝑒1,…,𝑒𝑁} that interconnect 𝑁+1 joints. By incorporating Lie algebraic apparatus, motion of 3D joints could be decomposed into three parts: skeleton anatomical information, motion trajectories, and bone lengths.

Fig. 2
figure 2
An example of human skeleton which consists of 21 joints and 20 body parts

Full size image
For each skeletal bone, 𝑒𝑛, a local coordinate is attached, with the bone itself being aligned with the x-axis and its starting joint being stuck to the coordinate origin. The relative 3D locations between two consecutive bones could be modeled as a series of 3D rigid transformations. Specifically, given two connected bones 𝑒𝑛 and 𝑒𝑛+1 along a kinematic chain, a joint 𝐜=(𝑥,𝑦,𝑧)⊤ in the local coordinate of 𝑒𝑛 amounts to a transformed location 𝐜′=(𝑥′,𝑦′,𝑧′)⊤ in the local coordinate of 𝑒𝑛+1, by exercising the following transformation

(𝐜′1)=(𝐑𝑛0𝐝𝑛1)(𝐜1).
(5)
Here, 𝐑𝑛∈ℝ3×3 is a rotation matrix, 𝐝𝑛=(𝑏𝑛,0,0)⊤ ∈ℝ3 a translation vector along x-axis, and 𝑏𝑛 the length of bone 𝑒𝑛.

For a 3D rotation matrix 𝑅∈SO(3), the associated Lie algebraic vector 𝐰∈𝔰𝔬(3) is an axis-angle vector. For a human skeleton, the exact degree of freedom (DoF) of a axis-angle vector is determined by the rotation orientations of two successive bones, and is up to 3. For example, if two bones are oriented in the same or reverse direction, 𝐰 is a zero vector with 0 DoF; if one bone only rotates along one axis, then the DoF reduces to 1.

Mapping Lie Algebra Parameters to 3D Positions
Now we focus on an articulate object with K kinematic chains; assume the k-th chain have 𝑚𝑘 joints, with each joint parameterized by a 3-dimensional 𝔰𝔬(3) vector, 𝐰𝑘𝑖,𝑖∈{1,2,…,𝑚𝑘}. A human pose is thus represented by composition of Lie algebra vectors of joints/bones on kinematics chains, 𝐩Lie=(𝑤11⊤,…,𝑤1𝑚1⊤, …,𝑤𝐾1⊤,…, 𝑤𝐾𝑚𝐾⊤). Now, the 3D position of a joint i in a chain k, 𝐉𝑘𝑖, is obtained following a exponential map of the Lie algebraic values, also known as forward kinematics, as

𝐉𝑘𝑖=[∏𝑗=0𝑖−1exp(𝑊̂ 𝑘𝑗)]𝐝𝑘𝑖+𝐉𝑘𝑖−1.
(6)
Here 𝐝𝑘𝑖=(𝑏𝑘𝑖,0,0), with 𝑏𝑘𝑖 representing the bone length of 𝑒𝑘𝑖. In addition, forward kinematics typically starts from a root joint whose position 𝐉0∈ℝ3, and Lie algebraic values 𝑊̂ 0 stand for the global location and orientation of the entire human body. In our representation, the global location 𝐉0 is independent from the pose. Therefore, given a motion with T successive poses, the sequence (𝐉0,1,…,𝐉0,𝑇)∈ℝ3×𝑇 makes up the body motion trajectory, with 𝐉0,𝑡 denoting its global location at frame t.

Fig. 3
figure 3
Visual diagram of action2motion, the first step in our pipeline. Top row shows the training phase: at time t, the posterior and prior networks take as input a concatenation of three parts—action category a, time counter 𝑐𝑡 and immediate pose vector (𝑝𝑡 or 𝑝𝑡−1). The generator receives an addition latent vector 𝐳𝑡 that is sampled from the learned posterior distribution. Afterwards, the 3D joints of current pose is obtained from the decoder of generator through pose decoding module. Bottom row depicts the testing phase: a latent vector is alternatively sampled from the prior distribution, which triggers the aforementioned process in generating 3D pose sequences

Full size image
Accordingly, the 3D coordinates vector of a body pose, formally denoted as 𝐩=(𝐉11⊤,…, 𝐉𝐦11⊤,… ,𝐉1𝐾⊤,…,𝐉𝐦𝐊𝐾⊤) could be obtained by the joint-wise forward kinematics of a composition of bone lengths, root position, and Lie algebraic vector. For simplicity, we denote this mapping as ΓΓ(𝐩Lie):𝐩Lie→𝐩. Overall, a human motion is represented by three parts:

Lie algebra parameters 𝐌Lie=(𝐩1Lie,…,𝐩𝑇Lie).

Root trajectory (𝐉0,1,…,𝐉0,𝑇): root trajectory could be represented by either absolute root locations or relative translations between consecutive root locations. The latter works better in our setting.

Bone lengths (𝑏0,…,𝑏𝑁): due to the invariant nature of bone lengths of human skeleton, the skeleton bone lengths are acquired from typical real-life human bodies, and are fixed over time. This also reciprocally enables us to generate motions with controllable body scales by manipulating the bone lengths.

Conditioned Temporal VAE
Consider a real motion or pose sequence 𝐌=(𝐩1,…,𝐩𝑇). Our temporal VAE aims to maximize the likelihood of the pose sequence 𝐌. At time t, a posterior network 𝑞𝜙(𝐳𝑡|𝐩1:𝑡) approximates the true posterior distribution conditioned on 𝐩1:𝑡−1. Then, with sampled latent variables 𝐳1:𝑡 and previous states 𝐩1:𝑡−1, our RNN generator 𝑝𝜃(𝐩𝑡|𝐩1:𝑡−1,𝐳1:𝑡) reconstructs the current pose 𝐩𝑡. This leads to the following variation lower bound:

log𝑝𝜃(𝐌)≥∑𝑡[𝔼𝑞𝜙(𝐳𝑡|𝐩1:𝑡)log𝑝𝜃(𝐩𝑡|𝐩1:𝑡−1,𝐳1:𝑡)−𝐷KL(𝑞𝜙(𝐳𝑡|𝐩1:𝑡)∥𝑝(𝐳𝑡))].
(7)
Note at time t, our RNN module takes as input the immediate past frame 𝐩𝑡−1 and 𝐳𝑡. The influence from previous time slices 𝐩1:𝑡−2 and 𝐳1:𝑡−1 lies in the ability of RNN module capturing long-term temporal dependencies.

Fig. 4
figure 4
Four variants of the pose decoding module conceived in our work: a direct generation of 3D joint positions; b generation with Lie algebraic representation; and c, d global and local movement integration (GLMI)-based generation with Lie algebraic representation, implemented by multi-layer perceptron (GLMI-M) or GRU (GLMI-R)

Full size image
In terms of the prior 𝑝(𝐳𝑡), one option is to consider an identity Normal distribution, (0,𝐈). This is unsuitable though for the motion generation problem, as the pose variation varies over time. Take running motions as example, the temporal pose variances are typically relatively small, which however could become significantly larger when e.g. the runner makes a U-turn.

Inspired by the observation that the variation of present pose is highly correlated to its past time-steps (Denton and Fergus 2018), we model its prior by a neural network that conditions on its previous steps 𝐩1:𝑡−1, 𝑝𝜓(𝐳𝑡|𝐩1:𝑡−1). This leads to a re-formulation of the ELBO objective function

log𝑝𝜃(𝐌)≥∑𝑡[𝔼𝑞𝜙(𝐳𝑡|𝐩1:𝑡)log𝑝𝜃(𝐩𝑡|𝐩1:𝑡−1,𝐳1:𝑡)−𝐷KL(𝑞𝜙(𝐳𝑡|𝐩1:𝑡)∥𝑝𝜓(𝐳𝑡|𝐩1:𝑡−1))],
(8)
where the distance penalty between prior and posterior distributions further encourages temporal consistency.

Architecture of Action2Motion
Our action2motion step consists of three main components: posterior network, prior network, and generator, which are shown in Fig. 3. The input vector contains the following parts: the pose vector 𝐩𝑡 or 𝐩𝑡−1, an one-hot vector 𝐚 to encode action category, and 𝑐𝑡∈[0,1], a time-counter to keep record of where we are in the sequence generation progress. As depicted in Fig. 3, during training, a noise vector is sampled from the posterior distribution 𝑞𝜙(𝐳𝑡|⋅), and fed into the generator, which then produces the final 3D pose prediction by running through the pipeline of encoder En, GRU unit GRU𝜃, decoder Dn, and pose decoding module. In testing, as the real data 𝐩𝑡 is not available, 𝐳𝑡 is instead sampled from the learned prior distribution, 𝑝𝜓(𝐳𝑡|⋅).

Specifically, our encoder En and decoder Dn are composed of linear fully connected layers with different weights, and updated with the whole network. Moreover, our posterior network (𝑞𝜙) and prior network (𝑝𝜓) utilize the same architecture, but with different parameters. They are respectively described as:

𝐡𝑡(𝜇𝜙(𝑡),𝜎𝜙(𝑡))=En(𝐩𝑡,𝐚,𝑐𝑡),𝑐𝑡=𝑡𝑇=GRU𝜙(𝐡𝑡)
(9)
and

𝐡′𝑡−1(𝜇𝜓(𝑡),𝜎𝜓(𝑡))=En(𝐩𝑡−1,𝐚,𝑐𝑡),𝑐𝑡=𝑡𝑇=GRU𝜓(𝐡′𝑡−1).
(10)
Further investigation of the pose decoding module is provided in the following section.

Pose Decoding
Figure 4 illustrates the four pose decoding variants investigated in our work. The most straightforward and commonly-used approach is Fig. 4a, where the 3D joint locations are directly and simultaneously regressed from the decoder. It however contains redundant parameters, and does not follow the kinematics law that dictates the 3D articulations of the body skeleton. Alternatively, the Fig. 4b variant incorporates Lie algebraic representation, which is the one adopted in our previous work (Guo et al. 2020). The decoder here contains two vectors, skeletal Lie algebraic values 𝐩̂ 𝑡Lie, and global root position 𝐉̂ 0,𝑡. The final 3D joints are produced by forward kinematics (see Sect. 4.1.3). Though working well for many motion scenarios, it encounters issues when local body movements and global motions are highly correlated. Take action walk for example, the instantaneous velocity of walking is significantly affected by the movement of legs; independently generating global and local body motions is observed to lead to e.g. sliding-feet phenomenon, as depicted in Fig. 12.

Global and local movement integration Existing efforts in motion forecasting or generation usually predict only relative body joint positions, this is, relative to the root joint, at the cost of neglecting the global motion all together (Wang et al. 2020; Yan et al. 2019; Liu et al. 2019b; Xu et al. 2017). In other words, the root joint of human full-body is fixed to coordinate origin during the entire motion sequence. Recently, Adeli et al. (2020) consider global motion by directly enforcing MSE or ℓ2 loss between predicted and ground-truth root joint locations, which is similar to the Fig. 4a variant.

Intuitively, the transition between two consecutive poses, measured by the displacement of the root joint in the two frames, is highly correlated to the body gesture of these two poses. Consider a person who is walking on a flat ground, his walking pace depends upon how wide his legs span. This inspires us to propose a global and local movement integration unit (GLMI) which, rather than predicting global transition and local joints concurrently, will first generate relative poses, then infer global motion from consecutive local poses, as illustrated in Fig. 4c. Here 𝐩̂ 𝑡Lie is the Lie parameter vector produced by the generator, which is then transformed to 3D joint locations 𝐩̂ 𝑜𝑡 through forward kinematics; 𝐩𝑜𝑡−1 is the offset value of 3D coordinates of previous pose; 𝐡𝑜𝑡 is a hidden vector containing upstream information. The three vectors are fed into a fully connected layer, MLP, which then produces the velocity (i.e. relative translation) 𝐕̂ 0,𝑡 at time t. Finally, the 3D global position 𝐩̂ 𝑡 could be obtained by summation of the three components: root position of previous pose 𝐉0,𝑡−1, estimated velocity 𝐕̂ 0,𝑡, and the current local pose 𝐩̂ 𝑜𝑡. Mathematically, this process is expressed as

(𝐩̂ 𝑡Lie,𝐡𝑜𝑡)𝐩̂ 𝑜𝑡𝐕̂ 0,𝑡𝐩̂ 𝑡=De(𝐡𝜃𝑡)=ΓΓ(𝐩̂ 𝑡Lie)=MLP(𝐩̂ 𝑜𝑡,𝐩𝑜𝑡−1,𝐡𝑜𝑡)=𝐩̂ 𝑜𝑡+𝐉0,𝑡−1+𝐕̂ 0,𝑡.
(11)
To further capture the temporal dependency of a global trajectory, another version of GLMI is also proposed, with the backbone of MLP replaced by recurrent units, GRU, as presented in Fig. 4d. Besides, a trajectory alignment loss between the predicted velocities 𝐕̂ 0,𝑡 and real velocities 𝐕0,𝑡 is also introduced, to encourage accurate velocity estimation. Among these variants, the GLMI-M variant is found to produce the overall best results, and is utilized in our approach by default.

Fig. 5
figure 5
Illustration of the motion2video process. Shapes and textures of 3D human characters are extracted from single 2D images, that are rigged, animated with motions generated from the action2motion step, and rendered to produce final videos

Full size image
Final Objective
To summarize, our final objective function becomes

𝜃,𝜙,𝜓=−∑𝑡=1𝑇[𝔼𝑞𝜙(𝐳𝑡|𝐩1:𝑡,𝐚,𝑐𝑡)log𝑝𝜃(𝐩𝑡|𝐩1:𝑡−1,𝐳1:𝑡,𝐚,𝑐𝑡)−𝜆𝑘𝑙𝐷KL(𝑞𝜙(𝐳𝑡|𝐩1:𝑡,𝐚,𝑐𝑡)∥𝑝𝜓(𝐳𝑡|𝐳1:𝑡−1,𝐚,𝑐𝑡))−𝜆𝑎𝑙𝑖𝑔𝑛‖𝐕0,𝑡−𝐕̂ 0,𝑡‖2],
(12)
where 𝜆𝑘𝑙 and 𝜆𝑎𝑙𝑖𝑔𝑛 are two tuning parameters to trade-off among reconstruction error 𝑟𝑒𝑐, KL-divergence, and trajectory alignment loss. Empirically, a larger 𝜆𝑘𝑙 is observed to enhance the quality of generated motions but may decrease their diversity; and vice versa for a smaller 𝜆𝑘𝑙.

Fig. 6
figure 6
A comparison of reconstructing 3D characters from single images by the original methods of PIFu, PIFuHD, and our improved variant. Each 3D reconstruction result is shown in front, side, and back views. Salient errors are pointed by the red arrows. See text for details

Full size image
For the reconstruction error (the first term in Eq. (12)), the per-joint loss suggested in Aksan et al. (2019) is considered, as

𝑟𝑒𝑐(𝐩𝑡,𝑝̂ 𝑡)=∑𝑘=1𝑁+1‖𝐉𝑘,𝑡−𝐉̂ 𝑘,𝑡‖2.
(13)
Here 𝑁+1 denotes the number of skeletal joints.

In our work, the trajectory alignment loss is only used in the methods of Fig. 4c and d, where the models are trained with the re-parameterization trick of Kingma and Welling (2014).

Training Strategy
One common issue in sequence modeling is the discrepancy of information exposure during training versus testing phases. For example, in a RNN model, a ground-truth pose is taken as input to generate next pose in training; while in testing phase, a generated pose is used instead to produce next pose. To mitigate the issue, a mixed training strategy is adopted here, that chooses whether to use (or not to use) teacher forcing (Bengio et al. 2015) by randomly draws from a Bernoulli distribution, 𝑉∼Bernoulli(𝑝tf). In particular, teacher forcing is chosen for the entire sequence 𝐩1:𝑇 if V is 1, and not if otherwise. As a boundary condition in generating the initial pose 𝐩̂ 1, its previous pose input 𝐩0 for the prior network (𝑞𝜓) is a zero vector. In addition, curriculum learning (Bengio et al. 2009) is used in the training phase that is to progressively increase the value of 𝜆𝑘𝑙.

Step Two: Motion2Video
Recall in step one of our approach, action2motion, diverse motions are generated from prescribed action categories. At this point, a motion is shown as a sequence of 3D skeletal articulations. To produce videos, it remains to settle the full-body shapes and textures of the involved human characters. This is addressed in step two, motion2video, where a specific setup is conceived: a reference person image is presented as input, from which 3D shape and texture of the person are extracted; this is followed by rigging and animating the characters with synthesized motions from the action2motion step, and rendering to generate final 2D videos. Unlike existing motion transfer methods (Chan et al. 2019; Liu et al. 2019a; Wang et al. 2019a) that emphasize in 2D space, our work advocates a fully 3D approach, and we claim our 3D-enabled modelling choice helps to preserve the geometric and appearance aspects in the final video production. Figure 5 illustrates the components in our motion2video process that is to be detailed in the following subsections.

Human Shape Reconstruction from a Single 2D Image
From a single 2D image, a 3D human character is extracted to preserve sufficient geometric and textural details consistent with the input. PIFu (Saito et al. 2019) and PIFuHD (Saito et al. 2020) are the two state-of-the-art methods on single-image based human shape recovery that have their unique pros and cons. The 3D shapes and textures extracted by both methods are reasonably adhere to their 2D image inputs. Meanwhile, the texture map extracted by PIFu (Saito et al. 2019) has relatively low resolution and accuracy, see e.g. the protruded knee pointed by the red arrow in Fig. 6b. Although PIFuHD produces high-resolution 3D human geometry construction, notable errors are introduced at the unseen side by the symmetric assumption. As e.g. shown by the red arrows in Fig. 6c, the frontal human face is also erroneously synthesized at the back side of the 3D character head.

Aiming at refining the reconstruction results, our improved variant takes advantage of PIFuHD in better estimating 3D geometry and camera-view appearance, as well as PIFu in better inpainting of texture for the unseen views. Moreover, we also adopt a heuristic in producing smooth transition near the boundary of visible and occluded surface regions, as follows: to detect the stitching boundary, we project the character (facing 𝑍+ direction) onto XY plane and match the edge of 2D silhouette with the 3D character; for a point x in the transition region or inside the occluded region O with color 𝑐𝑥, its color 𝑐𝑥 is expected to be close to the color 𝑐p𝑥 of the corresponding point on PIFu surface; at the same time, 𝑐𝑥 should also be close to those of its neighbors, 𝑥. This is formulated as the following convex objective function,

min∑𝑥∈𝑂[‖𝑐𝑥−𝑐p𝑥‖2+𝜆𝑛𝑛1|𝑥|∑𝑥′∈𝑥‖𝑐𝑥−𝑐𝑥′‖2].
(14)
In practice, the vertex colors 𝑐𝑥 in O are iteratively updated until a consistent convergence. For transition near the boundaries, only the second term of Eq. (14) is considered. As shown in Fig. 6d, our result is able to leverage the benefits of of both PIFu and PIFuHD methods, and produces a more natural transition near the boundary regions.

Rigging, Animation, and Rendering
Fitting SMPL for Extracted 3D Shape The SMPL human shape, a generative 3D human representation controlled by pose and shape parameters, is used to facilitate the follow-up rigging and animation process. This requires to fit SMPL as close as possible to the reconstructed 3D human shape that amounts to estimating the pose (𝜃𝜃) and shape (𝛽𝛽) parameters by minimizing the following composite objective,

(𝛽𝛽,𝜃𝜃)=surface(𝛽𝛽,𝜃𝜃)+𝜆𝑗joints(𝛽𝛽,𝜃𝜃)+𝜆𝑟reg(𝜃𝜃).
(15)
The joints fitting term 𝑗𝑜𝑖𝑛𝑡𝑠 enforces the joints location of the SMPL shape to match with the predicted 3D joints from 2D image. Here, the initial 3D joints prediction 𝐽̂ 𝑐 is obtained by regressing 2D joints from input image with OpenPose (Cao et al. 2021), and by inverse projection into the reconstructed 3D human shape. Denote 𝑓(⋅) a transformation function of specific joint from initial position to current position following skeleton kinematics chain. Denote 𝜌(⋅) a differentiable Geman-McClure penalty function (Geman and McClure 1987), and w the confidence of 2D joint prediction. We have,

joints(𝛽𝛽,𝜃𝜃)=∑𝑖∈|𝐽|𝜔𝑖𝜌(𝑓(𝐽(𝛽𝛽)𝑖,𝜃𝜃)−𝐽̂ 𝑐,𝑖).
(16)
Then the surface fitting term surface is applied to minimize distance between vertex 𝑆𝑖 of the reconstructed human shape S and its nearest vertex v of the SMPL shape (𝛽𝛽,𝜃𝜃),

surface(𝛽𝛽,𝜃𝜃)=∑𝑖∈|𝑆|min𝑣∈(𝛽𝛽,𝜃𝜃)‖‖𝑆𝑖−𝑣‖‖2.
(17)
Finally, the pose regularization term reg(𝜃𝜃) penalizes unusual poses through the learned Gaussian mixture model from CMU dataset (CMU 2003). Following (Bogo et al. 2016), it is of the form

reg(𝜃𝜃)=−log∑𝑖(𝑔𝑖𝑁(𝜃𝜃;𝜇𝜃𝜃,𝑖,Σ𝜃𝜃,𝑖)),
(18)
where 𝑁(𝜃𝜃;𝜇𝜃𝜃,𝑖,Σ𝜃𝜃,𝑖) is a Gaussian distribution with its mean 𝜇𝜃𝜃,𝑖 and variance Σ𝜃𝜃,𝑖, and 𝑔𝑖 are weights of mixture Gaussian model.

In practice, to minimize the above objective function, during the first two iterations we only consider the joints and the pose regularization constraints for quick convergence; the surface constraint is then incorporated during the rest iterations.

3D Model Deformation and Animation After obtaining the above optimized SMPL model that closely fits to the reconstructed 3D human mesh model, the SMPL model is used as an anchor to deform the 3D models to new poses. To start with, the vertex-level correspondences between the SMPL surface and the 3D human model are established by nearest neighbor search. In addition, body part information is used to eliminate possible mismatched pairs, especially these around the inter-joint of arms and torso. Specifically, the body parts information of reference image could be obtained using DensePose (Alp Güler et al. 2018), which then are back-projected to the surface of the 3D shape. As SMPL shape has pre-defined body segmentation, this could be utilized to filter out vertex pairs coming from different body parts. Next, we compute a displacement map from the optimized SMPL mesh to their correspondences on the 3D human model,

𝑆𝑗=𝑖(𝛽𝛽∗,𝜃𝜃∗)+𝑑𝑖→𝑗.
(19)
where 𝛽𝛽∗ and 𝜃𝜃∗ are the optimized shape and pose parameters of the SMPL model. 𝑆𝑗 and 𝑖(𝛽𝛽∗,𝜃𝜃∗) are the correspondences and 𝑑𝑖→𝑗 is the displacement from optimized SMPL model to reconstructed 3D human model.

Intuitively, to repose the human shape, we could acquire the target positions 𝑆∗ of shape vertices by applying the displacement map to the reposed SMPL as in Eq.(19). However, this will lead to imperfections due to free-form deformation. Following Zuo et al. (2020), we instead utilize the vertices of 𝑆∗ as control points to deform the 3D human model as rigid as possible, by enforcing a local rigidity constraint. The locally rigid deformation 𝑅𝑅 and the deformed human model 𝑆̂  are obtained by minimizing the following objective,

𝑑𝑒𝑓(𝑅𝑅,𝑆̂ )=∑𝑖∈|𝑆|∑𝑗∈𝑖𝑘𝑖𝑗‖‖(𝑆̂ 𝑖−𝑆̂ 𝑗)−𝑅𝑅𝑖(𝑆𝑖−𝑆𝑗)‖‖2+∑𝑙∈|𝑆|‖‖‖𝑆̂ 𝑙−𝑆∗,𝑙‖‖‖2.
(20)
Here 𝑖 is the set of the neighboring vertices of 𝑆𝑖; 𝑘𝑖𝑗 is the corresponding weights of neighboring vertices. 𝑅𝑅𝑖 is a rotation matrix. The above objective function is optimized by iteratively solving the rotation matrix R and the deformed mesh 𝑆̂  (Sorkine and Alexa 2007).

Rendering The target 3D shape are deformed and driven by the generated pose sequences frame-by-frame, which are subsequently fed into 3D game engine (Unity3D) to integrate physical conditions such as illuminations and shadows and produce the final videos. Specifically, spot light and directional light are used to illuminate the character from top. Four cameras, fixed at half height of the 3D character, are aimed at the subject to record the front, back, left side and right side views, respectively.

Empirical Evaluations
A comprehensive set of experiments are conducted to systematically evaluate the performance of our action2video approach, which consists of the two-step pipeline of action2motion and motion2video. We start by introducing the related datasets, and our implementation details. This is followed by a detailed examination of our action2motion process at Sect. 5.3, and comparisons for our motion2video with related efforts at Sect. 5.4. Finally, Sect. 5.5 provides a holistic evaluation of our full pipeline, action2video.

Datasets
Ideally, we expect to work with motion datasets that contain considerable amount of distinct motion clips of various action categories, and with proper 3D pose annotations. In practice, we achieve this by postprocessing existing popular datasets, including re-annotating 3D positions of NTU-RGBD (Shahroudy et al. 2016) and action categories of CMU MoCap (CMU 2003). We also curate an in-house dataset, HumanAct12. In these three datasets, all human poses are uniformly annotated into 3D joints connected into 5 kinematics chains, with pelvis being the root joint.

NTU-RGBD is a large-scale 3D human motion dataset containing nearly one million motion sequences of 120 action types. Its pose annotation (i.e. 3D joint positions) is from MS Kinect readout, which is known unreliable and temporally unstable. In our experiments, the state-of-art video 3D shape estimation method (Kocabas et al. 2020) is employed to re-estimate the 3D poses from video feeds. Note in our scenario, it’s sufficient for these poses to appear realistic, and they are not necessarily matched perfectly with the true poses. A subset of 13 distinct actions are further selected in our empirical evaluation, such as cheer up, pick up, salute, consisting of 3900 motion clips. Each pose is represented by 18 joints (i.e. 17 bones).

CMU MoCap is dataset accurately annotated by motion capture markers, with 2605 pose sequences. However, the dataset is not originally organized by action types. We identify 8 distinct actions based on their motion captions, including running, walking, climbing, jumping. In the end, 1088 motions are re-organized by action type, with each skeleton constituting 22 3D joints (i.e. 21 bones). In implementation, these pose sequences are down-sampled from 100 HZ to a frequency of 12 HZ.

HumanAct12 is our in-house dataset that comes with proper annotations. It consists of 1191 motion clips and 90,099 frames in total, which are categorized into 12 coarse-grained action categories, including e.g. warm up, lift dumbbell, and 34 fine-grained action types such as warm up (Leg pressing), lift dumbbell (with right hand). The fine-grained annotations give more specific and dedicated information of the motions. We test our model on both coarse- and fine-grained annotations. Our dataset, HumanAct12, contains more accurate and stable 3D position annotations compared to NTU-RGBD; and has more well-organized action annotations than CMU MoCap. Note each body pose contains 24 joints (i.e. 23 bones).

Fig. 7
figure 7
Input images used in our experiments are from different sources, including a BUFF dataset (Zhang et al. 2017), b people Snapshot dataset (Alldieck et al. 2018), c internet images, d CG image, and e our in-house captured images. See text for details

Full size image
To showcase that our pipeline could work with wide range of applications, input images from myriad sources are considered in our experiments, as displayed in Fig. 7. They include images from the BUFF dataset (Zhang et al. 2017), People Snapshot dataset (Alldieck et al. 2018), as well internet images, computer-generated (CG) images,Footnote1 and our in-house captured images. BUFF dataset provides 26 4D human sequences with different cloth styles and performing different actions. We then render 2D images from these human shapes. People Snapshot dataset contains 12 subjects and 24 video sequences with different backgrounds. More examples are provided in the supplementary file.

Implementation Details
Our action2video pipeline is mostly implemented by PyTorch. For all encoder layers, the output size is set to 128. One-layer GRU is used for prior network, posterior network and pose decoding module, while generator uses two-layer GRU. The hidden unit size of GRU is 128. And the noise vector 𝐳 and 𝐡𝑜𝑡 has the dimension of 30 and 20 respectively. The Adam optimizer is applied for training throughout all experiments, with learning rate of 0.0002, weight decaying of 0.00001, and default parameter values including 𝛽1=0.9, 𝛽2=0.999. Our model is trained with mini-batch size of 128. To stabilize the training process, teacher forcing rate 𝑝tf is set to 0.6. The values of aforementioned hyper-parameters are fixed throughout our empirical experiments across all datasets.

Afterwards, we generate motions with length of 60, 100 and 60 on NTU-RGBD, CMU MoCap and HumanAct12, respectively. The hyper-parameter 𝜆𝑘𝑙 is a trade-off between reconstruction constraints and KL-divergence penalty. During training, the value of 𝜆𝑘𝑙 for all datasets are initialized with 0.001 and linearly increased to 0.1, 0.1 and 0.01 at the end for above datasets respectively. During training, the value of 𝜆𝑎𝑙𝑖𝑔𝑛 is set to 10 throughout these experiments.

In motion2video step, to extract 3D shape from single image, 𝜆𝑛𝑛 and 10 neighbors are used in Eq. (14) for occluded region. The values of 𝜆𝑗 and 𝜆𝑟 in Eq. (15) are set to 2.0 and 0.2, respectively.

Step 1: Action2motion
Thorough evaluations of the action2motion step are carried out in this section. They include both quantitative and qualitative reports of motion generation results, and fine-grained analysis of the locomotion generation module; We also provide demonstrations of specific action2motion applications such as motion interpolation in the latent space, motion transition, and 3D motion outpainting. By default, the action2motion GLMI-M variant is utilized in our approach.

Evaluations
We start by introducing a tally of evaluation metrics and baseline methods used throughout this section, which is followed by a series of qualitative and quantitative evaluations.

Evaluation Metrics
We aim to evaluate the generated motions from the aspects of being natural and diverse. To achieve this, the three metrics in Lee et al. (2019) are adopted in our evaluations: Frechet Inception Distance(FID) to characterize the visually realistic aspect, Diversity and Multimodality to quantify the diverse levels. The action recognition accuracy is additionally used to gauge the similarity between generated motions and real-life motions, as well as the degree of generated motions belonging to the prescribed action.

FID is perhaps the most important indicator in our scenario. A lower FID suggests a better result. For multimodality and diversity, a result is claimed better only if its diversity and multimodality scores are closer to their respective values obtained from real motions. To calculate these metrics, we rely on a feature extractor to obtain the high-level features of motions. Since there is no standard implementation of such motion feature extraction, a vanilla RNN action recognition classifier is trained for each dataset; and the final layer of classifier is used as the motion feature extractor.

We elaborate these four metrics as below:

Frechet Inception Distance (FID) FID is an effective metric to evaluate the overall quality in motion generation. A large amount (in our case, 3000) of generated motions and real motions are sampled and then are transformed to two sets of features. For real motion, we sample from test set with replacement. Then, FID is measured by computing the distance between the feature distribution of generated motions and that of the real motions.

Recognition Accuracy Recognition accuracy is calculated as the accuracy of applying a pre-trained RNN action recognition classifier to the motion of interest.

Diversity Diversity indicates the variance of the motions across all action types. Specifically, a large set of motions are sampled from all varieties of action types, from which two subsets are randomly sampled with the same size 𝑆𝑑. The corresponding sets of motion feature vectors {𝐯1,…,𝐯𝑆𝑑} and {𝐯′1, …,𝐯′𝑆𝑑} are extracted respectively. Then, the diversity of this set of motions is evaluated by

Diversity=1𝑆𝑑∑𝑖=1𝑆𝑑∥𝐯𝑖−𝐯′𝑖∥2,
(21)
where 𝑆𝑑=200 is used throughout our experiments.

Multimoldality Different from diversity, multimodality indicates how much the sampled motions vary within each action category. Suppose there are C action types in the set of motion sequences. For the c-th action, two subsets with same size 𝑆𝑚 are randomly sampled, which are then transformed to two subset of feature vectors {𝐯𝑐,1,… ,𝐯𝑐,𝑆𝑚} and {𝐯′𝑐,1,…,𝐯′𝑐,𝑆𝑚}. The multimodality is defined as

Multimodality=1𝐶×𝑆𝑚∑𝑐=1𝐶∑𝑖=1𝑆𝑚‖‖𝐯𝑐,𝑖−𝐯′𝑐,𝑖‖‖2,
(22)
where 𝑆𝑚=20 is used in our experiments.

Fig. 8
figure 8
Visual comparison of motions generated by the baseline methods and our four action2motion variants. Two warm up motion sequences are sampled for each of the comparison methods. Every 6th frame is shown. See text for details. Best viewed in Adobe Acrobat Reader to activate the animations by clicking the boxed items in the top row. Note each item in the top row is with specific tag and color corresponding to its row of motion sequence displayed below

Full size image
Baseline Methods
Since the problem of action2motion, aka action-conditioned 3D human motion generation, is relatively new, there are few existing methods to compare with. We thus adapt the state-of-art methods from related areas to our context, as follows:

CondGRU Condition GRU is used as a deterministic baseline in our setting, which is also the principal model for audio-to-motion translation in Shlizerman et al. (2018) and text-to-motion generation in (Ahn et al. 2018; Stoll et al. 2020). Here, a small modification of the model is made that the input is the concatenation of condition vector and pose vector at present step and the output is the pose vector for next step.

Two-stage GAN Cai et al. (2018) propose a two-stage GAN method for 2D human motion generation based on action types. In particular, a Wasserstein GAN (Arjovsky et al. 2017) is first trained as the pose generator. After that, the motion generator is learned to produce input latent vector for pose generator to synthesize pose at each time. By using adversarial training, the entire generated pose sequences are judged by a motion discriminator. We adapt this method for 3D human motion generation through necessary modifications.

Act-MoCoGAN MoCoGAN (Tulyakov et al. 2018) is a widely used method for both conditional and unconditional video generation. While generating a video, the input noise vector are composed of two parts: one is a shared vector over time, another is a instinct noise vector sampled at each time. These two inputs are expected to map to the stationary content and dynamic motions in videos. In our experiment, to generate 3D human dynamics, we keep the original architecture and replace the video and image discriminators to motion and pose discriminators, respectively.

Dancing2Music Dancing2Music (Lee et al. 2019) generates 2D dancing motion sequences from audio signals, which consists of two main stages, decomposition and composition. During decomposition, a motion sequence is segmented into short motion snippets, with dance unit VAE (DU-VAE) model being trained to generate these motion snippets given the latent vectors of motion content and an initial frame; during composition, a music-to-movement GAN (MM-GAN) is trained to generate latent vectors of motion snippet contents conditioned on the given music signals. To make a meaningful comparison, the official implementation is adapted by replacing the music signals with action categories.

LatentTransition Wang et al. (2020) consider a two-stage GAN (Cai et al. 2018), with a Bi-LSTM being employed to produce input latent vectors for pose generation. An additional auxiliary action classifier further ensures the action-awareness of the generative model.

Action2Motion (plain). Oue action2motion variant by adopting the pose decoding module of Fig. 4a, where the 3D position of joints are directly produced from generator.

Action2Motion (w/Lie). Our action2motion variant with the pose decoding module of Fig. 4b, where the Lie algebra parameters and root joint locations are generated independently.

Action2Motion (GLMI-M). Our action2motion variant with the pose decoding module of Fig. 4c, where both the Lie algebra and GLMI are used, and GLMI is implemented by MLP.

Action2Motion (GLMI-R). Our action2motion variant with the pose decoding module of Fig. 4d, where both the Lie algebra and GLMI are used, and GLMI is implemented by GRU network instead.

Fig. 9
figure 9
Motion examples of fine-grained action categories generated by our action2motion (GLMI-M). Every 6th frame is shown. a Lift dumbbell with (from top to bottom) right hand, left hand, both hand, both hand over head, and both hand over head and squat. b Warm up with (from top to bottom) alt chest expansion, chest expansion, wrist circles, left side reach and right side reach. Best viewed in Adobe Acrobat Reader to activate the animations by clicking the boxed items in the top row. Note each item in the top row is with specific tag corresponding to its row of motion sequence displayed below

Full size image
Visual Comparisons
Figure 8 provides qualitative comparisons of skeletal motions generated from different methods: given an action category of warm up, two motions of length 60 are sampled, with every 6th frame being displayed.

Conditional GRU (Shlizerman et al. 2018) requires as input an initial ground-truth pose to kick-start its generation process. Unfortunately the generated poses often collapse into a cloud of 3D points near the root joint. Two-stage GAN (Cai et al. 2018) produces better results, which however are still perceptually not satisfactory. The skeletal sequence result of Act-MoCoGAN by Tulyakov et al. (2018) is visually the best among these three methods. The generated poses nonetheless often froze to a fixed posture quickly. Dancing2Music (Lee et al. 2019) shows capability of yielding natural poses and motions. Meanwhile, a single such motion usually contains multiple actions, with the motion context deviating from the prescribed action type. For instance, in the left column of Fig. 8, the stick man first performs lift dumbbell (from 𝑡=1 to 𝑡=18), then a short-time warm up (from 𝑡=24 to 𝑡=36), and finally drifting into drinking. On the other hand, LatentTransition (Wang et al. 2020) always starts with natural poses, then struggles with proper modeling of long-term motion dependencies, which typically deteriorates to unrecognizable movements. These results are in sharp contrast to that of our four action2motion variants, whose results are in general visually more appealing. Here, the action2motion (plain) variant sometimes generate visual defects noticeable to human eyes. For example, in the left column of Fig. 8, the arm bone lengths of the same individual abnormally vary from 𝑡=1 to 𝑡=24. This is due to the intrinsic 3D-coordinate skeletal representation adopted by the plain variant that does not obey the underlying skeletal kinematics. Skeletal motions generated by the other action2motion variants are typically more faithfully resemble to real-life motions, which we attribute to their adherence to kinematics by their use of Lie group/algebraic skeletal representations.

Diversity is another important evaluation criteria. In Fig. 8, motions generated from conditional GRU tends to be visually least appealing; this is followed by those of two-stage GAN and LatentTransition; the results of Act-MoCoGAN often suffers from the mode collapsing issue, with similar results popping up after multiple separate runs; In comparison, Dancing2Music is capable of producing diverse motions by transiting between different short motion snippets. However, the generated motions could not be faithfully aligned to the prescribed action type; On the contrary, our action2motion variants are shown to be capable of generating both diverse and consistent motions.

Moreover, our action2motion framework is also capable of producing motions from fine-grained action categories, as showcased in Fig. 9. The motions generated by our action2motion (GLMI-M) variant faithfully assemble the subtle characteristics of local motions (e.g. leg pressing and chest expansion), and body parts (e.g. left hand and right hand) from a range of fine-grained action types.

Quantitative Comparisons
Quantitative evaluations are conducted on a range of datasets. Specifically, Table 1 displays results on our in-house HumanAct12 dataset, where coarse-grained and fine-grained action annotations are both considered; Table 2 presents comparison results on the popular benchmarks of CMU MoCap and NTU-RGBD. Considering the stochastic nature of motion generation, each experiment is repeated 20 times, a statistical confidence interval of 95% is reported in both tables. Note action2motion (GLMI) is however not applicable to the post-processed NTU-RGBD dataset, since the re-estimated pose sequences from videos does not contain global trajectory information.

Table 1 Performance evaluation on HumanAct12 benchmark on coarse-grained and fine-grained action categories, respectively
Full size table
Table 2 Performance evaluation on CMU MoCap and NTU-RGBD Dataset. ± indicates 95% confidence interval
Full size table
Among the four evaluation metrics in both tables, FID is perhaps the most important indicator, as it evaluates the overall quality of the generated motions. Recognition accuracy quantifies how well a generated motion fits into an action category. Diversity and multimodality (i.e. MModality) are metrics quantifying the diversity aspects of the generated motions. Note the values of FID (or accuracy) is lower (or higher) the better; for Diversity and MModality though the values are as close to the real motion scores the better. From Tables 1 and 2, we have the following observations. As a deterministic method, conditional GRU fails to generate diverse motions that is essentially an one-to-many mapping problem. GAN models such as two-stage GAN, Act-MoCoGAN and LatentTransition have improved upon conditional GRU in both metrics of FID and recognition accuracy. The considerably high accuracy obtained by Act-MoCoGAN may be attributed to its use of action classifier during training. A sharp drop of FID is observed in Dancing2Music, which however comes at the price of much lower accuracy. Meanwhile, our action2motion clearly outperforms the rest on FID, and the GLMI-M variant consistently excels among the four action2motion variants. The success could be partly attributed to the incorporation of Lie algebraic pose representation.

Given substantial performance on FID and perhaps also accuracy scores, the scores of diversity and multimodality are also important indicators for the model capacity of producing diverse motions. Note for diversity and multimodality, the higher values do not necessarily reflect better performance; instead the values are best to be close to those from the real motions, denoted as → in Tables 1 and 2. Act-MoCoGAN generates motions with severely limited diversity. Overall, our action2motion variants, while performing best on FID and accuracy, also maintain a considerable extent of diversity and multimodality.

Crowd-Sourced Subjective Evaluation
In addition to the aforementioned objective experiments, two user studies are conducted on Amazon Mechanical Turk. The principal criteria used in these two user surveys are the visual perceptual quality of the motion, and the magnitude it is adhere to the intended action categories. Users who possess hit approval rate higher than 97% and 1000 completed hits are considered.

The first user study is illustrated in Fig. 10, which compares the first two action2motion variants, ours (plain) and ours (w/ Lie), with baseline methods. Here, same amount (i.e. 36) of motions are generated by different methods. The users are then asked to rank their preferences of these motions evenly sampled over all action categories. Our action2motion variants receive the highest user ratings. Contrarily, conditional RNN, two-stage GAN and LatentTransition are the three least performed methods. Dancing2Music and Act-MoCoGAN rank somewhere in-between. More positive feedback is observed in our action2motion plain variant, with 10% motions being graded the first by users. By adopting the Lie algebraic representation, our ours w/ Lie variant further narrows the gap to real motions, with 54% generated motions being secured at the top-2 spots by user ratings.

Fig. 10
figure 10
Crowd-sourced subjective assessment results of motions generated by comparison methods. For each method, there is a bar of different colors (from red to blue) indicating the percentage of corresponding preference levels (least to most preferred). See text for details (Color figure online)

Full size image
The second user study compares bewteen our two action2motion variants: ours (GLMI) and ours (w/ Lie). As GLMI-M outperforms GLMI-R in most cases, we focus on the evaluation of GLMI-M in this survey. Here the motions are generated following the same protocol conceived in the first study. As shown in table 3, ours with GLMI earns more appreciation from users when compared with ours (w/ Lie), with over a half motion sequences (i.e. 54.4 %) being preferred by users. When comparing to real motions, samples generated by ours (w/ Lie) are slightly inferior to real-life human motions, with 46.2% being preferred. Meanwhile ours (GLMI-M) is almost indistinguishable to the real motions. The results suggest the potentials of applying our algorithm to more interesting VR/AR applications.

Table 3 Crowd-sourced subjective assessment to compare motions sampled from Ours (GLMI-M), Ours (w/ Lie), and real motions
Full size table
Fig. 11
figure 11
Crowd-sourced subjective assessment to compare generated motions together with their global displacements from Ours (GLMI-M) and Ours (w/ Lie)

Full size image
We further investigate the global displacement aspect of the generated motions. As demonstrated in Fig. 11, motions generated from ours (GLMI) are always more preferred by users than those from ours (w/ Lie) over all these four action categories.

In summary, our GLMI-M variant, i.e. ours (GLMI), delivers overall best results among our four action2motion variants, which are often indistinguishable from real-life human motions.

Locomotion Generation Analysis
Locomotions (e.g. walking) are the most common activities in our daily life, which typically involve full-body displacements. Fig. 12 visually compares walking motions produced with versus without our global local movement integration (GLMI) module. When without, the walking motions appear surreal like ghost haunting on the ground, with arm and leg local movements not tuned to its global motion trajectory. By contrast, our proposed GLMI module significantly mitigates these issues. For example, the waving patterns of left (or right) arm is now synchronized with the right (or left) leg; the local-part moments are also well in agreement with the full-body motion trajectories.

Table 4 quantitatively evaluates the effects of incorporating GLMI module for locomotion generation on CMU MoCap dataset. The same evaluation metrics of Sect. 5.3.2 are considered here. The number of motion sampling is set to 500. Overall, ours with GLMI variants perform best over all the three metrics. In contrast, ours (plain) attains worst results, which we attribute to the missing modules of Lie algebraic representation and GLMI. Moreover, GLMI-M , i.e. GLMI with MLP implementation, works best in generating Walking motions, while GLMI-R takes the lead in Jump Forward.

Fig. 12
figure 12
Examples of locomotion generated without GLMI (top) versus with GLMI (bottom). Note the ghosting manoeuvre patterns when without GLMI. Best viewed in Adobe Acrobat Reader to see the animations upon clicking

Full size image
Table 4 Performance evaluation over CMU MoCap dataset on two locomotion action types
Full size table
Interpolation in Latent Space
Generative models could be regarded as a function mapping between points in a latent space and those in the real data space. Meanwhile, similar to the concept of well-posed problems, a well-learned generative model is expected to behave smoothly from a small perturbation in the latent space. In other words, when we perform interpolations between two distinct latent codes, their generated motions are supposed to transit smoothly. It is thus of interest to examine how interpolations in the latent space would change the motion generation behaviors of our action2motion. It also demonstrates the model capability in producing non-existent samples.

The task is a bit complicated in our situation, as our model generates motion sequences instead of single images. Alternatively, we use the first poses as anchors to perform interpolation between two motions. Specifically, the first poses of two pose sequences are selected. Then, a series of points can be created on the linear path between the latent vectors (i.e. noise vectors) of these two poses. After that, these points are input as initial latent vectors into our model to kick-start the generation of rest poses.

Fig. 13
figure 13
Examples of motion interpolation in lift dumbbell. Every 6th frame is shown. See text for details. Best viewed in Adobe Acrobat Reader to activate the animations by clicking the boxed items in the top row. Note each item in the top row is with specific tag corresponding to its row of motion sequence displayed below

Full size image
Figure 13 considers lift dumbbell action. Here two pose sequences are deliberately selected from motions generated by action2motion (GLMI-M), where the first poses of the two sequences are a person lifting with the left (and the right) hand, respectively. We have the following observations. (1) As demonstrated in the first column, transition from the left hand pose to the right hand pose is realistic at the first poses, by gradually putting one hand down and lifting another hand up. (2) From each of these initial interpolated poses, a visually natural motion sequence is generated. (3) Interestingly the interpolation leads to the generation of a novel motion, lift dumbbell with both hands.

Fig. 14
figure 14
Action transition examples. Every 5th frame is shown. The top three rows show transition between two actions. from top to bottom, they are sit-drink, jump up-lift dumbbell, lift dumbbell-jump up, respectively. The bottom two rows display transition of three actions, which are (from top to bottom) sit-jump up-sit and sit-jump up-lift dumbbell, respectively. Best viewed in Adobe Acrobat Reader to activate the animations by clicking items in the top row. Note each item in the top row is with specific tag corresponding to its row of motion sequence displayed below

Full size image
Action Transition
To showcase the flexibility of our motion synthesis process, action transition is explored by switching the action categories during sequence generation. Exemplar results are presented in Fig. 14. To our surprise, our action2motion model is able to produce unseen motions through action transition. In the first row of Fig. 14, after switching from sit down to drink, the character starts to open the bottle and drink with a sitting pose. However, all drinking motions in our training set are performed in standing poses. As shown in these examples, the resulting motion sequences are rather realistic and with natural transitions which is well maintained in transitions of not only two actions, but also three actions. This experiment clearly demonstrates the capacity of our approach in synthesizing unseen motions that goes beyond merely memorizing training examples.

Motion Outpainting
Our method could also serve as a motion outpainting tool: provided the initial few poses, apply our method to complete the rest of the motion sequence. This is realized by simply fixing the beginning poses, and generating the rest. Executing multiple independent runs usually creates distinct yet plausible outcomes. Fig. 15 illustrates such an example. Here black poses denote the fixed initial poses of Walk. This is completed by our model with visually plausible walking motions of distinct velocities and directions. This also suggests the necessity of modeling motion forecasting and generation in a non-deterministic manner.

Step 2: Motion2video
Side-by-side evaluations are performed in terms of reconstructing 3D human shapes and textures from single images in Sect. 5.4.1, and animation in Sect. 5.4.2.

3D Shape and Texture Reconstruction
Here we focus on the evaluation of reconstructing 3D human shape and texture from single images, where the respective part of our approach is compared side-by-side with the state-of-the-arts, namely PaMIR (Zheng et al. 2021), PIFu (Saito et al. 2019) and PIFuHD (Saito et al. 2020). PaMIR (Zheng et al. 2021) combines parametric SMPL body model with deep implicit function for robust 3D shape reconstruction. In our comparison, 30 images are obtained from a wide variety of sources, including the BUFF dataset (Zhang et al. 2017), the People Snapshot dataset (Alldieck et al. 2018), internet images, CG image, and our in-house captured images. Following the network architectures, the input resolution of PaMIR and PIFU is 512×512, whereas the input image resolution is 1024×1024 for PIFuHD and our approach.

Fig. 15
figure 15
Examples of motion outpainting of Walking. Provided several initial poses (in black), our method completes the rest motion sequence with multiple plausible outcomes. Best viewed in Adobe Acrobat Reader to see the animations upon clicking

Full size image
Fig. 16
figure 16
A qualitatively comparison of reconstructing 3D human shape and texture from single image. The input single images are show on the left, where the top image is from People Snapshot dataset (Alldieck et al. 2018) and the bottom one from BUFF dataset (Zhang et al. 2017). Comparing with the state-of-the-art methods of PaMIR (Zheng et al. 2021), PIFu (Saito et al. 2019) and PIFuHD (Saito et al. 2020), our approach improves upon PIFuHD and PIFu by integrating their otherwise segregated strengths of high-resolution geometry and high-quality texture at novel views

Full size image
Exemplar results of reconstructed textured shapes from single input images are shown in Fig. 16. The shapes and textures extracted by PaMIR and PIFu commonly lack details, and are oftentimes inaccurate. For example, the 3D shape of lady produced by PaMIR is overly slim, together with an smooth face that lack geometric details which is noticeable especially from side-views. PIFuHD is capable of recovering 3D shapes with better facial geometry and in high-resolution, yet the texture is often visually unpleasantly wrong, especially when viewing from the back. In contrast, our method maintains a delicate balance of shape and texture, thus stands at a better position in facilitating the follow-up animation and realistic rendering processes in our pipeline.

For quantitative evaluation, user study is further conducted to measure the perceptual quality of the comparison methods. For each input image, 20 Amazon mechanical turk Workers are enrolled to rank their preferences over the shapes reconstructed by their corresponding comparison methods. Table 5 displays the average rank of each method, with more detailed rank distributions presented in Fig. 18. Our method clearly stands out with the most appreciations from users, where almost half (i.e. 51%) results are ranked the first. By contract, PIFuHD is the least preferred one, of which 78% results are placed as least favorable. In-between are PIFu with the second lowest average rank, and PaMIR that receives considerable more positive feedback compared to PIFuHD.

Motion2Video Animation
In Fig. 17, We present two single image animation showcases using our method. 3D shape and texture are predicted from input images, which are driven by two challenging motions, cartwheel, from Adobe Mixamo.Footnote2 As shown, our method could obtain accurate shape and texture predictions from all views, as well as plausible animations with provided motions.

Table 5 Quantitative comparison of reconstructing 3D human shape and texture from single images
Full size table
Fig. 17
figure 17
Two animation results of our method. Given single images of frontal view of individuals shown on the left, their 3D shapes are reconstructed, 2D videos are obtained, using prescribed off-the shelf motion sequences. The videos produced by our method are visually plausible

Full size image
In what follows, we elaborate the comparisons between our method and other three state-of-the-art image animation methods (Liu et al. 2019a; Huang et al. 2020; Weng et al. 2019). For quantitative evaluation, we conduct user study on Mechanical Turk which pairs the videos animated with the same image and motion from our and comparison method, and request the workers to determine which one that is “more realistic”. For each animation, 50 workers with Hit approval rate higher than 97% are enrolled for perceptual assessment.

Comparison with Liquid Warping GAN (Liu et al. 2019a). Liquid Warping GAN (Liu et al. 2019a) is a learning based motion transfer method in pseudo-3D space, where 3D SMPL model estimated from reference video frames are used to re-pose the person in source image. Figure 19 presents the animated videos by our method (bottom) and Liquid Warping GAN (top), when feeding with the same input image and motion. While successfully modeling the motion dynamics, the individual images obtained by Liquid Warping GAN are very blurring such that the characteristic personal landmarks of face or T-shirt logo are nearly unrecognizable. In contrast, the animation results of our method are of high-resolution and high quality. A user study is performed for quantitative evaluation, based on 22 animations from Liquid Warping GAN and our method covering a variety of input images and motion sequences, including composed of 9 Mixamo motions and 13 motions generated by our action2motion step. As shown in Table 6, 84.3% of our animations are preferred by users.

Fig. 18
figure 18
User preference distributions of reconstructing 3D human shape and texture from single images

Full size image
Fig. 19
figure 19
Comparing our method (bottom) with Liquid Warping GAN (Liu et al. 2019a) (top) and ARCH (Huang et al. 2020) (middle), animated using the same input image and motion sequence. Results are displayed by pairing the corresponding video frames

Full size image
Comparison with ARCH (Huang et al. 2020). ARCH Huang et al. (2020) uses a semantic deformation field to produce 3D rigged full-body human avatars from a single image, which is already animatable. However, the implementation and pre-trained model of ARCH has not been released yet. We managed to obtain 3 animated 3d model sequences from the authors with our provided images and Mixamo motions. We render video frames of these 3d model sequences in Unity3D with the same environment setting (e.g. light, camera) as ours. Figure 19 presents a visual comparison between ARCH’s result (middle) and our result (bottom). Though ARCH shows capability of generating reasonable rendering, the person appearance is yet to be realistic. For example, the pants comes with several blue debris; the two feet of the man are in wrong color (black); and the texture of T-shirt is overly bright. A user study is again conducted regarding the 3 animations from ARCH and our method. As given in Table 6, our method earns more preference (i.e. 59.3%) from users. Please refer to the supplementary video for more visual comparisons.

Comparison with Weng et al. (2019). the work of Weng et al. (2019) is also closely related to part of our motion2video step, where a 3D character is extracted out of a single image and is further animated to form videos. Their implementation is unfortunately not publicly available, instead we obtain from the authors of Weng et al. (2019) two animated action sequences (i.e. sit and walk) from the two input images provided by us. Note that the motions involved in Weng et al. (2019) are real MoCap motion sequences, while our motions are generated by ourselves. For an easy side-by-side visual comparison, we hand pick two of our generated motions that resemble the animations used (Weng et al. 2019). The walk and sit visual results are displayed and compared in Figs. 20 and 21, respectively.

Table 6 Crowd-sourced subjective assessment to compare the videos animated with the same image and motion, produced by Ours, Liquid Warping GAN (Liu et al. 2019a), ARCH (Huang et al. 2020) and Weng et al. (2019)
Full size table
When viewing from frontal view, the results of Weng et al. (2019) possess incomplete and distorted errors including the incomplete feet (Fig. 20b), over-slim arms, and torn pants (Fig. 21b), as highlighted by red arrows. These artifacts come from the fact that the textures are directly copied and pasted from the 2D input image, which is inadequate to maintain intact appearance in 3D geometry. In comparison, our results are noticeably better at preserving detailed structure and appearance, e.g. around the feet.

When inspecting from the side and back views of the extracted 3D characters that are not directly visible from the input image view, the textured results of Weng et al. (2019) are simply mirrored from the frontal region, as shown in the back side of head and torso - the visual results are thus significantly deteriorated to being funny. In contrast, our results preserve reasonable 3D shape and consistent appearance across multiple views including the frontal view. Moreover, a similar user study is conducted among the two set of generated videos. As in Table 6, our method is 70.3% more preferred over Weng et al. (2019).

Fig. 20
figure 20
Comparing our action2video with Weng et al. (2019) by animating walking motions. For each given image on the left, we show the results of Weng et al. (2019) (middle column) and ours (right column) from different views. Weng et al. (2019) fail to build an intact 3D texture model (e.g. incomplete feet), and the appearance of unseen part is distorted. Our method could generate plausible animation from all angles

Full size image
The Full Action2Video Pipeline
This section is devoted to the examination of our full action2video pipeline. We start by comparing with state-of-the-art 2D-based human video generation results. Further experiments also demonstrate the capacity of our action2video approach in accommodating input images from different sources.

Fig. 21
figure 21
Comparing our method with Weng et al. (2019) by animating sitting motions

Full size image
Fig. 22
figure 22
Visual comparison of three methods: (top) a state-of-the-art 2D-based method Kim et al. (2019), (middle) Liquid Warping GAN (Liu et al. 2019a), and (bottom) ours

Full size image
Fig. 23
figure 23
Exemplar videos produced by our action2video pipeline. Given a reference image and a specific action category, our action2video could extract 3D human shapes and cloth textures, and animate and render into diverse motion videos. For boxing and throw actions, one video are shown, each animated by a different 3D character extracted from a single image; similarly, two distinct videos and four distinct videos are presented for the warm-up action and walking action respectively

Full size image
Fig. 24
figure 24
An generated walking video from the following views: a front, b right-side, c back, and d left-side

Full size image
Comparison with Existing Methods
The work of Kim et al. (2019) is state-of-art in generating human motion videos, which is 2D-based and relies on large-scale training set of videos. Figure 22 presents a comparison of their results and ours that share in common similar poses and views. Compared with our results, the frames of Kim et al. (2019) is of low resolution (128×128). Moreover, there are visible lack of details of face, hands and clothes, and unrealistic shape deformations, which we attribute to their innate 2D based limitations. For example, lengths of legs and arms in Kim et al. (2019) of the same lady character vary over time. Moreover, as presented in the middle row of Fig. 22, the exemplar video result generated by engaging Liquid Warping GAN based on the same motion generated by our action2motion step, where edges and facial details are very foggy and fuzzy, when comparing to our results shown at the bottom row.

Diverse Input Image Sources
This experiment is to evaluate the flexibility of our action2video pipeline in accommodating input images from varied sources. Figure 23 presents our action2video results based on BUFF images (e.g. 1st row), People Snapshot images (e.g. 2nd row), Internet images (e.g. 4th row), these captured by our mobile-phone (e.g. 3rd row) as the input images. Overall our approach is able to adapt to these different applications, and to produce videos of visually pleasing quality. More visual results are shown in the supplementary video.

Multiple Camera Views
Figure 24 displays an exemplar video sequence generated by our approach, that is inspected from four different views. It demonstrates (1) our extracted 3D shape and clothing texture are reasonably realistic when examined in different rendered views, and (2) compared to the popular 2D-based methods, our generated videos are consistent among distinct views.

Conclusion and Discussion
Conclusion
We propose an action2video approach to tackle the exciting and challenging problem of generating natural and diverse 3D motions and videos of human actions. This is accomplished in this paper by a 2-step pipeline: action2motion focuses on generating 3D human motions, which are then turned into videos by motion2video. Empirical studies demonstrate the effectiveness of our approach.

Limitation and Future Work
Our approach performs reasonably well in practice; empirically it outperforms the state-of-the-art methods in many aspects along the full pipeline. On the other hand, we recognize that our training set, primarily the in-house HumanAct12 dataset is relatively small, which contains 1191 motions. For future work, we plan to acquire a larger dataset with broader set of actions, to generate motions and videos from a wider range of human activities including interactions with multiple people, with surroundings and objects, and to improve the reconstructed shape details such as fingers. Furthermore, we would investigate its possible applications such as augmenting data for human-centric tasks (action recognition, pose estimation), and VR/AR.