Human keypoint detection from a single image is very challenging due to occlusion, blur, illumination, and scale variance. In this paper, we address this problem from three aspects by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, we find that context information plays an important role in reasoning human body configuration and invisible keypoints. Inspired by this, we propose a cascaded context mixer (CCM), which efficiently integrates spatial and channel context information and progressively refines them. Then, to maximize CCM’s representation capability, we develop a hard-negative person detection mining strategy and a joint-training strategy by exploiting abundant unlabeled data. It enables CCM to learn discriminative features from massive diverse poses. Third, we present several sub-pixel refinement techniques for postprocessing keypoint predictions to improve detection accuracy. Extensive experiments on the MS COCO keypoint detection benchmark demonstrate the superiority of the proposed method over representative state-of-the-art (SOTA) methods. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark. The source code will be released at https://github.com/chaimi2013/CCM.

Access provided by University of Auckland Library

Introduction
Human keypoint detection is also known as human pose estimation (HPE) refers to detecting human body keypoint location and recognizing their categories for each person instance from a given image. It is very useful in many downstream applications such as activity recognition (Ni et al. 2017; Baradel et al. 2018; Liu et al. 2018), human–robot interaction (Mazhar et al. 2018; Zhang and Tao 2020), and video surveillance (Hattori et al. 2018; Varadarajan et al. 2018). However, HPE is very challenging even for human annotators. For example, almost 50% of instancesFootnote1 in the COCO training dataset (Lin et al. 2014) have at least six unannotated keypoints (Fig. 1a) and 35% keypoints are unannotated (Fig. 1b) due to various factors including occlusion, truncation, under-exposed imaging, blurry appearance and low-resolution of person instances. Some examples from the MS COCO dataset are shown in Fig. 1.

Fig. 1
figure 1
a, b Statistics of the annotated keypoints in the MS COCO training dataset (Lin et al. 2014). c Some examples from the MS COCO dataset, where occluded, under-exposed, and blurry person instances are very common. Blue and red dots denote the annotated visible and invisible keypoints, respectively

Full size image
Prior methods have made significant progress in this area with the success of deep convolutional neural networks (DCNN) (Toshev and Szegedy 2014), which can be categorized into two groups: top-down methods and bottom-up methods. Top-down methods are composed of two phases, i.e., person detection and keypoint detection, while bottom-up methods directly detect all keypoints from the image and associate them with corresponding person instances. Although bottom-up methods (Cao et al. 2017) are usually fast, top-down methods still dominate the leaderboard of public benchmark datasets like MS COCO due to their high accuracy. Using heatmaps to represent keypoint locations and fully CNNs with an encoder–decoder structure to learn features has gained prominence in recent studies (Newell et al. 2016; Huang et al. 2017), since spatial correspondence can be preserved between feature maps and heatmaps. Recently, to learn strong feature representations at multiple resolutions, pyramid-based networks have been proposed (Yang et al. 2017; Chen et al. 2018b). Although these methods improve detection accuracy by learning multi-scale features, there are still some issues to be addressed.

First, detecting invisible keypoints is more difficult than visible ones due to ambiguous appearance and inconsistent context bodies. How to effectively model multi-source context information to infer the hard keypoints is still under-explored. Second, external datasets such as the AI Challenger dataset,Footnote2 have been used to learn more discriminative feature representations (Xiao et al. 2018; Sun et al. 2019). However, they may have different annotation formats with the target set, for example, 17 keypoints for the MS COCO dataset and 14 keypoints for the AI Challenger dataset, or even have no annotations like the MS COCO unlabeled dataset. How to effectively leverage these external datasets to learn human pose configuration and discriminative feature representations for recognizing diverse poses remains challenging. Third, the ground truth keypoint location is annotated in pixel (or sub-pixel) in the high-resolution image plane, while the regression target is usually in the low-resolution heatmap, e.g., 1/4 size of the input image. This scale mismatch of representation will degrade the keypoint detection performance. To address this issue, sub-pixel representation or post-processing techniques (Chen et al. 2018b; Zhang et al. 2020) have been proposed. Nonetheless, a systematic study of post-processing techniques is still absent and worth further discovering.

In this paper, we address these issues to improve human keypoint detection by devising an efficient network structure, proposing three effective training strategies, and exploiting four useful postprocessing techniques. First, inspired by the keypoint detection process carried out by humans, where the “context” contributes to the perception and inference process, we advance the research by studying the role of context information for human keypoint detection. Specifically, we adopt an encoder–decoder network structure and propose a novel cascaded context mixer (CCM) in the decoder. It can efficiently integrate both spatial and channel context information and progressively refine them. Then, we propose a joint training strategy and a knowledge-distilling approach to exploit abundant unlabeled data. Besides, we also propose a hard-negative person detection mining strategy to migrate the inconsistency of person instances between training and testing. These strategies endow the detection network with the capability of learning discriminative features. Third, we present and comprehensively evaluate four sub-pixel refinement techniques for postprocessing keypoint predictions. Extensive experiments on the MS COCO keypoint detection benchmark validate the effectiveness of the proposed CCM model, the training strategies, and the sub-pixel techniques. Our single model achieves comparable performance with the winner of the 2018 COCO Keypoint Detection Challenge. The final ensemble model sets a new SOTA on this benchmark.Footnote3

The contributions of this work can be summarized as follows:

∙ We devise an effective cascaded context mixer in the decoder which can learn both spatial and channel context information to infer the human body and hard keypoints.

∙ We propose several efficient training strategies to guide our model to deal with false-positive person detections and learn discriminative features from diverse poses.

∙ We present some sub-pixel refinement techniques to enhance location accuracy and comprehensively evaluate their performance and complementarity.

Related Work
HPE methods can be grouped into 2D pose estimation (Rogez et al. 2012; Toshev and Szegedy 2014; Newell et al. 2016; Fang et al. 2017; Huang et al. 2017; Cao et al. 2017; Yang et al. 2017; Xiao et al. 2018; Sun et al. 2018; Chen et al. 2018b; Sun et al. 2019; Zhang et al. 2019a; Li et al. 2019; Girdhar et al. 2018) and 3D pose estimation (Rogez et al. 2012; Pavlakos et al. 2018b, a; Rhodin et al. 2018; Hossain and Little 2018; Yang et al. 2018) according to the dimension of the coordinates of the keypoint locations. In this paper, we focus on 2D pose estimation, specifically, the multi-person pose estimation (MPPE) problem, which is more challenging than the single-person pose estimation (SPPE) problem. MPPE approaches can be further divided into bottom-up and top-down approaches. A summary of these approaches is presented in Table 1, where we outline their features according to the network structure, whether using extra data or not, and the postprocessing techniques used to improve detection accuracy. The details are presented in the following part.

Table 1 A summary of the human keypoint detection methods based on DCNN
Full size table
Bottom-up approaches first detect all human keypoints and then associate them with each detected person instance (Cao et al. 2017; Newell et al. 2017; Pishchulin et al. 2016). This approach is usually faster than top-down approaches, but the assembly step can become intractable when person instances are ambiguous due to occlusions, blur, etc., degrading accuracy compared to top-down approaches. Top-down approaches first detect all person instances in the image and then apply SPPE on each detected person. Benefiting from recent progress in DCNN-based object detection (Ren et al. 2015; Ouyang et al. 2016; He et al. 2017; Lin et al. 2017; Liu et al. 2020; Chen et al. 2020), person detectors have achieved promising detection accuracy. Further, different neural networks have been proposed to learn strong feature representations based on multi-scale feature fusion and multi-level supervisions, e.g., the Pyramid Residual Module in Yang et al. (2017), the Cascaded Pyramid Network in Chen et al. (2018b), the simple baseline model in Xiao et al. (2018), and the High-resolution Net in Sun et al. (2019), which detect keypoint locations with high accuracy. Our proposed method follows the top-down scheme and has an encoder–decoder structure. However, in contrast to the above methods, we study the role of context information for human keypoint detection by devising a cascaded context mixer module in the decoder to sequentially capture both spatial and channel context information.

Deep neural models benefit from a large scale of training data and efficient training strategies. Trained on more examples with diverse poses, the keypoint detection model can learn to infer the occluded or blurry keypoints from similar poses (Xiao et al. 2018; Li et al. 2019). For example, all the top entries of the COCO keypoint detection leaderboardFootnote4 leverage external data such as the AI Challenger human keypoint detection dataset. In this paper, we also validate the benefit of using external data. However, instead of using transfer learning, we propose a more effective joint training strategy to harvest external data with heterogeneous labels. Further, we make use of unlabeled data, e.g., the unlabeled MS COCO dataset, referring to the knowledge distilling idea, where the pseudo-labels are generated by a teacher model. Besides, few of the above approaches deal with the mismatch problem of person detections during the training phase and testing phase, i.e., training with ground truth person instances while testing with detected ones, which may be false positives. In this paper, we propose an effective hard-negative person detection mining strategy in the training phase, adapting the model to predict no keypoints for those false person instances.

Dominant methods adopt a heatmap to represent the keypoint location where a Gaussian density map is placed on the corresponding pixel. However, the heatmap is usually in low-resolution compared with the input, which has a side effect on the location accuracy. Increasing the heatmap resolution means to decode high-resolution features, which may incur extra computational cost and model complexity. Instead, prior methods adopt computationally efficient postprocessing techniques to refine the predictions, for example, shifting the detected location by 0.25 pixels according to the local gradient directions (Chen et al. 2018b; Xiao et al. 2018; Sun et al. 2019). In contrast to them, we present a sub-pixel refinement technique using the second-order approximation, it turns out to be more effective than the above one and boost the detection accuracy by a large margin. Besides, we comprehensively study the sub-pixel refinement techniques for postprocessing including the second-order approximation (SOA), the Soft Non-Maximum Suppression (Soft-NMS), the sub-pixel shift of flipped heatmaps (SSP), and the Gaussian filtering on heatmaps (GF). Experiment results validate that these techniques can significantly boost the keypoint detection performance. Moreover, they are complementary to each other, and so the combination of them will boost the performance further.

Cascaded Context Mixer for Human Keypoint Detection
In this part, we propose a novel human keypoint detection model based on a cascaded context mixer module to explicitly and simultaneously model spatial and channel context information. To further exploit the representation capacity of the model, we propose three efficient training strategies including a hard-negative person detection mining strategy to migrate the mismatch between training and testing, a joint-training strategy to use abundant unlabeled samples by knowledge distilling, and a joint-training strategy to exploit external data with heterogeneous labels. To improve the detection accuracy, we also present four postprocessing techniques to refine predictions at the sub-pixel level.

Motivation
Since occlusions are ubiquitous in and between human bodies, we take it as an example to show how humans carry out the detection process when dealing with the aforementioned hard cases. As shown in Fig. 2a, occlusions include self-occlusion (A, C, D, E, F), occlusion by others (C, G), and truncation (B, H). Note that all the faces are self-occluded in Fig. 2a, i.e., half of each face is invisible. Humans can easily recognize a complete object and its boundaries, even if it is occluded. According to Gestalt psychology in visual perception, we can group fragmented contours under the law of closure (Wagemans et al. 2012), e.g., D and E. We have also seen numerous human body positions and acquired the common sense that a body consists of symmetric legs, hands, and face and that different body parts move and form different poses. Therefore, we can easily infer the occluded parts like A, B, C, G, and H. For the more difficult case F, we may infer the invisible arms by judging the boy’s intention and rehearsing the same action psychologically.

Fig. 2
figure 2
a Ubiquitous occlusions in an image from the MS COCO dataset. b Illustration of the keypoint detection process carried out by humans when faced with occlusions

Full size image
The Recognition-by-Components (RBC) theory tells us that humans quickly recognize objects even under occlusions by characterizing the object’s components (Biederman 1987). One possible path of the keypoint detection process carried out by humans could be divided into four main stages as shown in Fig. 2b). First, we recognize and locate a human body (The top-down approaches follow this paradigm Chen et al. 2018b; Xiao et al. 2018; Sun et al. 2019). Then, we recognize each body part to further locate the keypoints belonging to it (Some graph-based models complete this step explicitly Felzenszwalb et al. 2008; Holt et al. 2011; Wang and Li 2013; Yang and Ramanan 2013). Here, we can easily identify some distinct and visible keypoints. Finally, we infer the remaining keypoints which may be invisible or ambiguous. How do we accomplish this? By reviewing the inference process and the occlusion cases in Fig. 2a, we think that the “context” plays an important role when we associate separate body parts into a whole or infer an invisible keypoint (Chen et al. 2020; Ma et al. 2020). Another key factor may be that we have a priori knowledge of human body configurations in all possible poses. The context tells us about the surrounding visible body parts, and the a priori knowledge helps us to determine what the category and location of the invisible part should be. This motivates us to design a context-aware model that can efficiently learn useful feature representation from diverse poses.

Fig. 3
figure 3
Visualization of the feature maps learned by the ResNet-50 encoder on two test images as shown in (a, b) and (c, d), respectively

Full size image
Fig. 4
figure 4
The diagram of the proposed human keypoint detection model based on cascaded context mixer (CCM)

Full size image
Cascaded Context Mixer-Based Decoder
In this paper, we tackle the multi-person pose estimation problem by following a topdown scheme. First, a human detector is used to detect the bounding box for each person instance. Then, the proposed CCM model detects keypoints for each person instance. After aggregating the detections using Object Keypoint Similarity (OKS)-based Non-Maximum Suppression (NMS), we obtain the final pose estimation. As shown in Fig. 4, the CCM model has an encoder–decoder structure where we use ResNet (He et al. 2016) or HRNet (Sun et al. 2019) as the backbone encoder network. The decoder consists of three Context Mixer (CM) modules in a cascaded manner. The details of CM are presented as follows.

Context Mixer
One the one hand, as we know that DCNN is able to learn distributed semantics at each feature channel, which leads to powerful representation ability to recognize a vast number of categories. In Fig. 3, we visualized some feature channels from the ResNet-50 encoder. As can be seen, the encoder learned to activate one or multiple explicit body parts. On the other hand, as discussed in Sect. 3.1, context information could be useful to infer invisible keypoints and help to learn human body configuration. Thereby, we exploit two types of context information in this paper, i.e., global context and local context.

First, since different feature channels correspond to certain body parts (Fig. 3a, c), the relationship between them can be modeled to further enhance the feature representation. To this end, we use a channel attention module to encode the relationship between different channels, which inherits the idea of squeeze-and-excitation (SE) network (Hu et al. 2018). The attention vector is used to reweigh features in a channel-wise manner, which is a global operation from the perspective of spatial dimension. In this way, we can extract the global context to enhance the feature representation. Second, since some of the feature responses at different body parts have strong correlations (Fig. 3b, d), we can encode the spatial relationship by using convolution layers with large receptive fields such that they can cover different body parts. To this end, we leverage hybrid-dilated convolutions to capture multi-scale spatial context information within different receptive fields, which inherits the idea of atrous spatial pyramid pooling (ASPP) (Chen et al. 2018a). These two types of context information collaborate with each other to learn discriminative feature representations. Besides, we also use a residual branch to reuse the learned features from the previous stage. Thereby, there are three branches in CM as shown in the middle part of Fig. 4. We present the details of each branch as follows.

In the residual branch of the kth CM, feature maps 𝑓𝐶𝑀𝑘−1 from the previous stage are first up-sampled two times before being fed into a 1×1 convolutional layer to output feature maps 𝑓𝑅𝐸𝑆𝑘 of size 𝐻𝑘×𝑊𝑘×𝐶𝑘, i.e.,

𝑓𝑅𝐸𝑆𝑘=𝜙𝑅𝐸𝑆𝑘(𝑓𝐶𝑀𝑘−1↑2),
(1)
where ↑2 denotes the up-sampling operation, 𝜙𝑅𝐸𝑆𝑘(⋅) denotes the function learned by the convolutional layer.

In the SE branch of the kth CM, the feature maps 𝑓𝐶𝑀𝑘−1 first go through a global pooling layer. Then, the obtained feature vector is fed into a bottle-neck layer with 1×1 convolutions. The feature dimension is reduced to 1×1×𝐶𝑘/4. Then, it is fed into a subsequent 1×1 convolutional layer to increase the feature dimension to 1×1×𝐶𝑘. A sigmoid function is used to squeeze the feature vector 𝑓𝑆𝐸𝑘 into the range [0,1], i.e.,

𝛼𝑆𝐸𝑘=𝜎(𝜙𝑆𝐸𝑘(𝐺𝑃(𝑓𝐶𝑀𝑘−1))),
(2)
where 𝐺𝑃(⋅) denotes the global pooling operation, 𝜙𝑆𝐸𝑘(⋅) denotes the function learned by those two 1×1 convolutional layers, and 𝜎(⋅) denotes the sigmoid activation function.

In the HDC branch of the kth CM, the feature maps 𝑓𝐶𝑀𝑘−1 go through four 3×3 convolutional layers with different dilated rates, i.e., 1, 2, 3, and 4. Each convolutional layer has 𝐶𝑘/4 kernels. These feature maps are then concatenated and fed into a deconvolutional layer of stride 2. The output feature maps 𝑓𝐻𝐷𝐶𝑘 are of size 𝐻𝑘×𝑊𝑘×𝐶𝑘, i.e.,

𝑓𝐻𝐷𝐶𝑘=𝜙𝐻𝐷𝐶𝑘([𝜙𝑑1𝑘(𝑓𝐶𝑀𝑘−1);…;𝜙𝑑4𝑘(𝑓𝐶𝑀𝑘−1)]),
(3)
where 𝜙𝑑1𝑘(⋅)∼𝜙𝑑4𝑘(⋅) denote functions learned by the dilated convolutional layers, 𝜙𝐻𝐷𝐶𝑘(⋅) denotes the function learned by the deconvolutional layer, and [;] denotes the concatenation operation.

Then, the output of the kth CM is calculated as:

𝑓𝐶𝑀𝑘=𝑓𝐻𝐷𝐶𝑘⊙𝛼𝑆𝐸𝑘+𝑓𝑅𝐸𝑆𝑘≜𝜙𝐶𝑀𝑘(𝑓𝐶𝑀𝑘−1),
(4)
where ⊙ denotes the channel-wise multiplication and 𝜙𝐶𝑀𝑘(⋅) denotes the mapping function learned by the kth CM. Batch normalization (Ioffe and Szegedy 2015) is used after each convolutional layer and deconvolutional layer. ReLU is used after the first convolutional layer in the SE branch, all the convolutional layers in the HDC branch, and the output of each CM module (Krizhevsky et al. 2012).

Cascaded Context Mixer
To capture the context information at multiple resolutions and learn multi-scale feature representation, we stack K CMs sequentially to decode the features from the encoder step by step and increase their resolutions accordingly. Mathematically, it can be written as:

𝑓𝐸𝑁𝐶=𝜙𝐸𝑁𝐶(𝑖𝑚𝑔),
(5)
𝑓𝐷𝐸𝐶=𝜙𝐶𝑀𝐾(…(𝜙𝐶𝑀1(𝑓𝐸𝑁𝐶)))≜𝜙𝐷𝐸𝐶(𝑓𝐸𝑁𝐶)
(6)
where img represents the input image, 𝜙𝐸𝑁𝐶(⋅) denotes the function learned by the encoder, 𝑓𝐸𝑁𝐶 is the encoded feature, 𝜙𝐷𝐸𝐶(⋅) denotes the function learned by the decoder, 𝑓𝐷𝐸𝐶 is the decoded feature. Note that we denote 𝑓𝐶𝑀0≜𝑓𝐸𝑁𝐶 for consistency.

The decoded feature 𝑓𝐷𝐸𝐶 is fed into a final 1×1 convolutional layer to predict the target heatmaps:

ℎ=𝜙𝑃𝑅𝐸(𝑓𝐷𝐸𝐶),
(7)
where 𝜙𝑃𝑅𝐸(⋅) denotes the function learned by the prediction layer and h represents the predicted heatmaps.

Auxiliary Decoder and Intermediate Supervision
Deep supervision (Lee et al. 2015) refers to the technique that adds auxiliary supervision on some intermediate layers within a deep neural network. It facilitates multi-scale and multi-level feature learning by allowing error information back-propagation from multiple paths and alleviating the problem of vanishing gradients in deep neural networks. Leveraging the deep supervision idea, we also add an auxiliary decoder 𝜙𝐷𝐸𝐶𝑎𝑢𝑥(⋅) after the penultimate stage of the encoder. Its structure is identical to 𝜙𝐷𝐸𝐶(⋅) described above. These two decoders do not share weights.

Training Objective
The ground truth heatmap is constructed by placing a Gaussian peak at each keypoint’s location in the image plane. The number of heatmaps is identical to the number of keypoints predefined in the dataset, for example, 17 for the MS COCO dataset (Lin et al. 2014) and 14 for the AI Challenger dataset. We use an MSE loss to supervise the network during training. Mathematically, it is defined as:

𝐿𝑚𝑎𝑖𝑛=1𝐻𝐾𝑊𝐾𝐶∑𝑖,𝑗,𝑐‖‖ℎ(𝑖,𝑗,𝑐)−ℎ𝐺𝑇(𝑖,𝑗,𝑐)‖‖2,
(8)
where C is the number of heatmaps, i, j, and c are the spatial and channel index, h and ℎ𝐺𝑇 are the predicted and ground truth heatmaps, respectively. Similar to Eq. (8), an extra MSE loss 𝐿𝑎𝑢𝑥 is also added to the auxiliary decoder as an intermediate supervision. The final training objective is defined as the weighted sum of both losses:

𝐿=𝐿𝑚𝑎𝑖𝑛+𝜆𝐿𝑎𝑢𝑥,
(9)
where 𝜆 is the weight for the auxiliary loss.

Learning from Diverse Poses with Efficient Strategies
CCM’s capacity to model context information and learn discriminative feature representation can be exploited by learning from massive training samples with diverse poses. In this paper, we propose a hard-negative person detection mining strategy to migrate the mismatch problem of person detections during the training phase and testing phase, a joint-training strategy on unlabeled samples by knowledge distilling, and a joint-training strategy on external data with heterogeneous labels.

Hard-Negative Person Detection Mining (HNDM)
Top-down approaches detect person instances before detecting keypoints on them. On the one hand, although modern detection models have achieved a good detection performance, they may still produce some false positive detections due to occlusion, similar appearances, etc. On the other hand, the keypoint detection model is usually trained with ground truth bounding box annotations enclosing exact person instances. It has never seen any false positive detections during the training phase. Therefore, there is a mismatch between training and testing, which may lead to incorrect keypoint predictions for those false positive person detections. To address this issue, we propose a hard-negative person detection mining strategy.

Fig. 5
figure 5
Illustration of the hard-negative detections. Green: ground truth person instances. Red: hard-negative detections. Yellow: low-score false positive detections

Full size image
First, we trained a Mask R-CNN on the MS COCO training set containing only the category of person. ResNeXt152 was used as the backbone network. It achieved a mean average precision (AP) of 60.4 on the COCO minival dataset. Then, we evaluated the training set using the detection model and screened out those detections with sufficiently high scores, e.g., ≥0.5, but no intersections with ground truth person instances. These were treated as hard-negative detections in this paper. Some examples are shown in Fig. 5. During training CCM, these hard-negative detections could be added to the training set. Their keypoint heatmaps are set to all-zero maps. In this way, CCM learns to predict no keypoints on those false “person instances”.

Joint-Training on Unlabeled Samples by Knowledge Distilling
To increase pose diversity in the training samples, we leverage the MS COCO unlabeled dataset, which contains over 120k images. Since there are no person and keypoint annotations, we generate pseudo labels by referring to the knowledge distilling idea. First, we used the above-trained person detector to detect all possible person instances within the unlabeled images. Then, we screened out those detections with scores above a predefined threshold, which is determined by guaranteeing the number of average person instances per image to be identical to the one calculated from the ground truth annotations of the MS COCO training set. In our case, this threshold was 0.9924. Next, we trained several keypoint detection models using ResNet152 and HRNet-w48 as the backbone encoder network and used them to detect keypoints on the person detections obtained from the previous stage. We fused the predictions by different models, kept all keypoints with scores above 0.9 as the pseudo labels, and treated the rest as unlabeled. In this way, we distill the learned “knowledge” in the keypoint detection model to the pseudo labels of unlabeled training samples and use them to supervise the network.

Joint-Training on External Data with Heterogeneous Labels
Different datasets may not share the same annotation norms, even if they are used for the same purpose. For instance, 17 keypoints are used to define a human skeleton in the MS COCO dataset (Lin et al. 2014), but only 14 in AI Challenger (AIC). Five keypoints correspond to the eyes, ears, and nose in MS COCO, while only the “top of head” is annotated in AIC. AIC has a keypoint annotation for the neck, which is absent in MS COCO. The other 12 keypoints corresponding to limbs are the same in both datasets. To use AIC with MS COCO, a common practice is to train a network on AIC and then change the number of channels in the final prediction layer and fine-tune this network on MS COCO. In this paper, we propose a simple but effective joint-training strategy that mixes the training samples in both datasets to leverage the diverse poses simultaneously. To make the training tractable, we align the labels in AIC with the ones in MS COCO by keeping the 12 common annotations and discarding the others.

To further adapt the trained model to the MS coco dataset, we can also add a finetune stage. In conclusion, the training strategies described above can be summarized as:

𝑇𝑟𝑎𝑖𝑛|𝛷→𝐹𝑖𝑛𝑒𝑡𝑢𝑛𝑒|𝛩,
(10)
where 𝛷∈{𝐴,𝐴𝐶,𝐴𝐶𝐻,𝐴𝐶𝐻𝑈,𝐶𝐻𝑈}, 𝛩∈{𝐶,𝐶𝐻}, A denotes the AIC training dataset, C denotes the COCO training set, H denotes the hard-negative training samples, and U denotes the reprocessed unlabeled training set.

Sub-pixel Refinement Techniques for Postprocessing
To migrate the scale mismatch of representation between the high-resolution ground-truth keypoint location and the corresponding position on the low-resolution heatmap, different sub-pixel refinement techniques have been introduced, e.g., the 0.25-pixel shift of the maximum response pixel (Chen et al. 2018b; Xiao et al. 2018; Sun et al. 2019; Li et al. 2019), the one-pixel shift of flipped heatmap (Xiao et al. 2018; Sun et al. 2019), and Gaussian filtering of predicted heatmap (Chen et al. 2018b; Li et al. 2019). With this regard, we devise four sub-pixel refinement techniques by (1) exploring the second-order approximation to locate the maximum response at the sub-pixel level accuracy, (2) introducing the Soft Non-Maximum Suppression (Soft-NMS) to refine the maximum response’s location instead of the original NMS, (3) extending the one-pixel shift of flipped heatmap to a general sub-pixel form, and (4) applying the Gaussian filter on predicted heatmaps. They are complementary (or orthogonal) to each other and can be conducted sequentially at different stages of the inference phase.

Sub-pixel Refinement by the Second-Order Approximation
Sub-pixel refinement by the second-order approximation has ever been used in the depth super-resolution and disparity calculation literature (Yang et al. 2007). They use the cost volume at different disparities to find the optimal disparity that minimizes the inconsistency between the left and right views. Therefore, they face the issue to estimate the sub-pixel disparity given the costs at integer disparities. Likewise, we can adopt such a technique to estimate the sub-pixel keypoint location given the predicted heatmaps. In contrast to the one-dimensional cost volume, the heatmaps are in the two-dimensional space. Therefore, we present two kinds of second-order approximation by either using a parabola approximation for each dimension or using a paraboloid approximation simultaneously for the two dimensions.

Fig. 6
figure 6
The sub-pixel refinement by the second order approximation, i.e., the case of parabola

Full size image
The Parabola Approximation
As we know that the heatmap is a Gaussian function w.r.t. the pixel dimension, nevertheless, it is reasonable to approximate it with a parabola function in a local neighborhood of the maximum pixel as shown in Fig. 6:

𝑧(𝑥)=𝑎𝑥2+𝑏𝑥+𝑐,
(11)
where a, b, and c are the coefficients of the parabola. The maximum z is subject to the first order condition:

∂𝑧∂𝑥=2𝑎𝑥+𝑏=0.
(12)
Therefore, the maximum z is reached at 𝑥∗=−𝑏2𝑎. Given 𝑧1 is the maximum pixel at 𝑥0, 𝑧0 and the 𝑧2 is the heatmap response at 𝑥0−1 and 𝑥0+1, we can calculate 𝑥∗ as:

𝑥∗=𝑥0+𝑧0−𝑧22(𝑧0+𝑧2−2𝑧1).
(13)
The second term in the right-hand side (RHS) of Eq. (13) is the sub-pixel shift from the detected maximum response pixel 𝑥0 to the underlying maximum one 𝑥∗. Similarly, we can calculate the optimal 𝑦∗ along the vertical dimension.

Fig. 7
figure 7
The sub-pixel refinement by the second order approximation, i.e., the case of paraboloid

Full size image
The Paraboloid Approximation
Since the heatmap is represented as a two-dimension Gaussian function, we can approximate it with a paraboloid function in a local neighborhood of the maximum pixel as shown in Fig. 7:

𝑧(𝑥,𝑦)=𝑎𝑥2+𝑏𝑦2+𝑐𝑥𝑦+𝑑𝑥+𝑒𝑦+𝑓,
(14)
where a, b, c, d, e, and f are the coefficients of the paraboloid. The maximum z is subject to the first order condition:

{∂𝑧∂𝑥=2𝑎𝑥+𝑐𝑦+𝑑=0∂𝑧∂𝑦=2𝑏𝑦+𝑐𝑥+𝑒=0.
(15)
Therefore, the maximum z is reached at:

⎧⎩⎨⎪⎪𝑥∗=2𝑏𝑑−𝑐𝑒𝑐2−4𝑎𝑏𝑦∗=2𝑎𝑒−𝑐𝑑𝑐2−4𝑎𝑏.
(16)
As shown in Fig. 7, 𝑧11 is the maximum in the heatmap at (𝑥0,𝑦0), 𝑧00∼𝑧22 are its 8-neighbor heatmap responses. Assuming that the coordinate original is at (𝑥0,𝑦0), we can calculate the coefficients 𝑎∼𝑒 as:

𝑎=18[2(𝑧12+𝑧10−2𝑧11)+(𝑧02+𝑧00−2𝑧01)  +(𝑧22+𝑧20−2𝑧21)],
(17)
𝑏=18[2(𝑧01+𝑧21−2𝑧11)+(𝑧00+𝑧20−2𝑧10)  +(𝑧02+𝑧22−2𝑧12)],
(18)
𝑐=14(𝑧00+𝑧22−𝑧02−𝑧20),
(19)
𝑑=18[(𝑧02−𝑧00)+(𝑧22−𝑧20)+2(𝑧12−𝑧10)],
(20)
𝑒=18[(𝑧20−𝑧00)+(𝑧22−𝑧02)+2(𝑧21−𝑧01)],
(21)
Therefore, the maximum z is reached at:

⎧⎩⎨⎪⎪𝑥∗=𝑥0+2𝑏𝑑−𝑐𝑒𝑐2−4𝑎𝑏𝑦∗=𝑦0+2𝑎𝑒−𝑐𝑑𝑐2−4𝑎𝑏.
(22)
The second terms in the RHS of Eq. (22) is the sub-pixel shift from the detected maximum response pixel 𝑥0 (resp. 𝑦0) to the underlying maximum one 𝑥∗ (resp. 𝑦∗). Given the heatmap, after locating the maximum pixel, we calculate the optimal location according to Eq. (13) or Eq. (22).

Sub-pixel Refinement by Soft-NMS
During the inference phase of top-down approaches, there may be several bounding boxes detected around a person instance. Consequently, we may estimate several poses on them. Similar to the Non-Maximum Suppression (NMS) postprocessing step used in object detection, NMS is also applied to the detected poses (Chen et al. 2018b; Xiao et al. 2018; Sun et al. 2019). Different from the Intersection over Union (IoU) used to compare the overlap between two bounding boxes, the Object Keypoint Similarity (OKS)-based IOU (OKS-IOU) is used to compare two poses. The original NMS is to filter out all the poses that have sufficient large OKS-IOUs with top-ranked detection. We argue that those poses can also be treated as reasonable estimates, which can be used to get a more stable result. Therefore, we present Soft-NMS for sub-pixel refinement as follows.

Fig. 8
figure 8
The sub-pixel refinement by Soft NMS

Full size image
As shown in Fig. 8, the poses marked in red, blue, and green dots are three estimates and the red pose has the highest score. We can calculate the fusion results by leveraging the OKS-IOU as the fusion weight, i.e.,

𝑝∗𝑖=∑𝑗∈𝛬𝑖𝐼𝑂𝑈𝑂𝐾𝑆𝑖𝑗𝑝𝑗∑𝑗∈𝛬𝑖𝐼𝑂𝑈𝑂𝐾𝑆𝑖𝑗,
(23)
where 𝛬𝑖 is the index set of poses to be filtered given the top-ranked detection 𝑝𝑖, 𝐼𝑂𝑈𝑂𝐾𝑆𝑖𝑗 is the OKS-IOU between 𝑝𝑖 and 𝑝𝑗. Note that we treat 𝑖∈𝛬𝑖 and set 𝐼𝑂𝑈𝑂𝐾𝑆𝑖𝑖=1. We can re-write Eq. (23) as:

𝑝∗𝑖=𝑝𝑖+∑𝑗∈𝛬𝑖𝐼𝑂𝑈𝑂𝐾𝑆𝑖𝑗(𝑝𝑗−𝑝𝑖)∑𝑗∈𝛬𝑖𝐼𝑂𝑈𝑂𝐾𝑆𝑖𝑗,
(24)
where the second term in the RHS is the sub-pixel shift from the top-ranked detection 𝑝𝑖 to the underlying best one 𝑝∗𝑖.

Gaussian Filtering on Heatmaps
The regressed heatmap may not be as smooth as the ground truth Gaussian density map. A Gaussian filter-based postprocessing technique is proposed to smooth it and minimize the variance of the prediction (Chen et al. 2018b; Li et al. 2019). We evaluate this technique and compare it with other sub-pixel refinement techniques.

Sub-pixel Shift of Flipped Heatmaps
During the inference phase, some methods predict the pose from the flipped image and average the flipped heatmap with the original one to get the final prediction (Chen et al. 2018b; Xiao et al. 2018). However, since the flipped heatmap is not aligned with the original one, one common practice is to shift the flipped heatmap by one pixel.

Fig. 9
figure 9
The sub-pixel shift of flipped heatmaps

Full size image
In this paper, we extend the one-pixel shift to the sub-pixel shift as shown in Fig. 9. First, we resize the flipped heatmap by a scale ratio r (𝑟≥1) along the horizontal axis. Then, we shift it by one pixel to the right. Then, we resize it back to the original size. As can be seen, the effective shift becomes 1/r pixel. We name it as the sub-pixel shift (SSP) in this paper. Note the above process is equivalent to a linear interpolation of the original heatmap and its one-pixel shifted version, i.e.,

ℎ∗𝑓(𝑖,𝑗)=(1−1𝑟)ℎ𝑓(𝑖,𝑗)+1𝑟ℎ𝑓(𝑖,𝑗−𝑗0),
(25)
where ℎ𝑓 is the flipped heatmap, 𝑗0 is the maximum shifted pixels, i.e., 𝑗0=1 in this paper. ℎ∗𝑓 is the sub-pixel estimate. It becomes the one-pixel shift technique when 𝑟=1.

Experiments
We conducted extensive experiments to demonstrate the effectiveness of the proposed model. First, comprehensive ablation studies on the components of CM were presented, followed by the comparative studies of the proposed training strategies and sub-pixel refinement techniques. Next, we compared the proposed model with representative state-of-the-art methods in terms of detection accuracy, model complexity, and computational cost. Then, we presented some visual examples of the detection results by our model and explained the detection process by inspecting the learned features at each stage. Finally, we empirically studied the impact of visible and invisible annotations in our model and obtained useful some insights.

Experimental Settings
Datasets
The COCO Keypoint Challenge addresses multi-person pose estimation in challenging uncontrolled conditions (Lin et al. 2014). The dataset is split into training, minival, test-dev, and test-challenge sets. The training set includes 118k images and 150k person instances, the minival dataset includes 5000 images, and the test-dev set includes 20k images. It also provides an unlabeled dataset containing 123k images. 110k person instances and corresponding keypoints were detected using the method described in Sect. 4.2. The external dataset from AIC contains a training set with 237k images and 440k person instances and a validation set with 3000 images. We also evaluated CCM and the baseline model on the recently proposed OCHuman benchmark (Zhang et al. 2019b) comprising heavily-occluded human instances to compare their performance on handling occluded cases. This dataset contains 8110 human instances with detailed keypoint annotations like COCO. It is divided into two subsets: OCHuman-Moderate and OCHuman-Hard. The first subset contains instances with MaxIoU in the range of 0.5 and 0.75, while the second contains instances with MaxIoU larger than 0.75. MaxIoU denotes the max IoU of a person with others in an image.

Evaluation Metrics
We report the main results based on the object keypoint similarity (OKS)-based mean average precision (AP) over 10 OKS thresholds, where OKS defines the object keypoint similarity between different human poses. They are calculated as follows (Lin et al. 2014):

𝐴𝑃=𝑚𝑒𝑎𝑛{𝐴𝑃@(0.50:0.05:0.95)},
(26)
𝐴𝑃@𝑠=∑𝑝𝛿(𝑂𝐾𝑆𝑝>𝑠)∑𝑝1,
(27)
𝑂𝐾𝑆𝑝=∑𝑖exp(−𝑑2𝑝𝑖/(2𝑎2𝑝𝜎2𝑖))𝛿(𝑣𝑝𝑖>0)∑𝑖𝛿(𝑣𝑝𝑖>0),
(28)
where p is the person instance index, i is the keypoint index, 𝛿(⋅) is the Kronecker function. 𝛿(⋅)=1 if the condition holds, otherwise 0. s is a threshold, 𝑑𝑝𝑖 is the Euclidean distance between the predicted ith keypoint of the person instance p and its ground truth, 𝑎𝑝 is the area of the person instance p, 𝜎𝑖 is the normalization factor predefined for each keypoint type, and 𝑣𝑝𝑖 is the visible status.

Implementation Details
The feature dimension 𝐶𝑖 of each CM was set to 256 for ResNet-50 and 128, 96, 64 for ResNet-152, 32 for HRNet-w32, and 48 for HRNet-w48. All backbone networks were pre-trained on the ImageNet dataset (Deng et al. 2009). Gaussian initialization was used for convolutional and deconvolutional layers in the decoder. The weights and bias in BatchNorm layers were initialized as 1 and 0, respectively. CCM was implemented in Pytorch (Paszke et al. 2017) and trained on four NVIDIA Tesla V100 GPUs using the Adam optimizer. Hyper-parameters were set by following (Xiao et al. 2018; Sun et al. 2019). We used the detection results on the minival set and test-dev set released in Xiao et al. (2018) for a fair comparison, which were obtained by a faster-RCNN detector (Ren et al. 2015) with detection AP 56.4 for the person category on COCO val2017. We obtained the final predictions by averaging the heatmaps of the original and flipped image as in Chen et al. (2018b), Newell et al. (2016), Xiao et al. (2018).

Ablation Studies
Ablation Study of the Components of CM
First, we conducted an ablation study on the components of CM by training different variants on the COCO training set and calculating the AP and AR on the minival set. The backbone network was ResNet-50 (R50) and HRNet-w32, and the input size was 256×192. The results are shown in Table 2. For the ResNet-50 backbone, we used three CM modules in the decoder to keep consistent with the baseline model which used three deconvolutional layers. In this way, the feature map size increased from 8×6 at the end of the encoder to 64×48 at the end of the decoder. As can be seen, each component achieved gains over the baseline model (Xiao et al. 2018), for example, adding the SE branch or HDC branch improved the detection accuracy by a margin of 1.3 AP or 1.4 AP over the baseline model. The auxiliary decoder and intermediate supervision also benefited the detection model and achieved a gain of 1.7 AP. Using dilated convolutions in the encoder could produce feature maps with 2× higher resolution (i.e., 128×96), thereby benefiting the localization accuracy. As can be seen, it achieved a gain of 2.3 AP over the baseline model. These components are complementary to each other that the combination of them improved the detection accuracy further, i.e., a gain of 0.8–1.8 over the individual component.

Table 2 Ablation study on the components of CCM
Full size table
For the HRNet-w32 backbone, since it could produce high-resolution features (i.e., 64×48), we only attached one CM module after HRNet-w32 and did not use any dilated convolutions, thereby increasing the feature map size to 128×96. Nonetheless, we also investigated whether extra CM modules were useful or not. To this end, we attached one or two extra CM modules (i.e., K=2 or 3) accordingly. To avoid huge computations for processing larger feature maps, we replaced the deconvolutional layers in the extra CM modules with convolutional layers to keep the feature map size. As can be seen from Table 2, CCM (K=1) outperformed the vanilla HRNet-w32 by a gain of 1.1 AP. Besides, adding extra CM modules only led to marginally better results while the parameters and computations increased from 25.56M and 7.92 GFLOPs (K=1) to 28.58M and 8.16 GFLOPs (K=2), and 28.6M and 8.4 GFLOPs (K=3), respectively. To make a trade-off between accuracy and computational efficiency, we chose K=1 as the default setting for the HRNet-w32 backbone.

Table 3 Comparison of CCM and the Baseline model on the OCHuman dataset (Zhang et al. 2019b)
Full size table
Table 4 Comparison of CCM and the Baseline model on the PoseTrack validation set (Andriluka et al. 2018)
Full size table
Table 5 Comparisons of CCM trained with the different strategies described in Sect. 4
Full size table
Besides, to compare the generalization ability of the proposed CCM and the baseline models when dealing with unseen heavily-occluded cases from other benchmarks, e.g., the OCHuman benchmark (Zhang et al. 2019b) and the PoseTrack benchmark (Andriluka et al. 2018), we evaluated them in a zero-shot manner, where all models were trained on the COCO training set without further fine-tuning. It is noteworthy that we only include the results of the baseline models and our model for the following several reasons. Firstly, the goal is of this paper is to investigate the key factors that have strong impacts on the performance of human keypoint detection. Since it has already been evidenced by prior excellent work (Xiao et al. 2018; Sun et al. 2019) that a better keypoint detector benefits pose tracking more effectively, we only focus on human keypoint detection in this paper. Here, we use the OCHuman dataset and PoseTrack dataset to validate the effectiveness of the proposed method for handling occluded cases. Secondly, the detection and tracking performance heavily depends on the person detector, but the detector as well as the implementation of joint propagation in Xiao et al. (2018), Sun et al. (2019) are not available. As a result, it will be unfair to compare with their results in a different setting. We plan to investigate the role of context for pose tracking in our future work, where spatial-, temporal- and channel-wise context could be exploited together to enhance feature representation. Thirdly, the Simple baseline method (Xiao et al. 2018) and the High-resolution Network method (Sun et al. 2019) are already two strong baseline models. It is noteworthy that many recent methods use HRNet as the backbone and have achieved SOTA performance on public datasets or challenges. Thereby, it is representative to compare our model with them on these two datasets. Besides, we use the zero-shot transfer setting to evaluate how the model trained on a representative dataset (e.g., MS COCO) generalizes to unseen samples from different data distributions, especially those samples containing large occlusions.

The results on the OCHuman benchmark are listed in Table 3. As can be seen, CCM outperformed the baseline model for handling occlusions by large margins, e.g., 3.3 points of AP gain on the validation set and 2.8 points of AP gain on the test set. The gains mainly arise from the occlusion cases 𝑣𝑎𝑙[0.5−0.75] and 𝑡𝑒𝑠𝑡[0.5−0.75], which contain occluded instances with MaxIoU in the range of 0.5 and 0.75. The results on the PoseTrack validation set are listed in Table 4. Our model outperforms the the baseline model with a ResNet-50 backbone and the HRNet-w32 model by 1.7 AP and 0.8 AP, confirming the superiority of CCM when dealing with heavily-occluded human instances. For a person instance with occluded body parts, it is challenging to detect both visible and invisible keypoints due to the self-occlusion, incomplete body, and perplexity with adjacent overlapped bodies. The proposed CM enables the detection model to learn discriminative feature representation for diverse poses and help it to recognize occluded keypoints. After the network sees diverse poses, it “memorizes” different poses with/without occlusions in the form of feature mapping. Inferring an occluded keypoint thus becomes easier by associating it with similar poses. More discussions will be presented in Sects. 6.4 and 6.5.

Remark
1) CM has better representative capacity than the plain deconvolution layer in the simple baseline method (Xiao et al. 2018) because it leverages spatial and channel context information explicitly; 2) CM is also complementary to the high-resolution module in Sun et al. (2019) and improves the performance of the stronger HRNet-w32; and 3) CM effectively handles occlusions by learning context features to infer the occluded keypoints.

Comparison of Training Strategies
Next, we present the results of using different training strategies described in Sect.  4 in Table 5, where A→C denotes the transfer learning strategy, AC→C denotes the joint-training strategy, i.e., training CCM on both AIC and COCO datasets then fine-tuning it on the COCO dataset. Other symbols have a similar meaning. As can be seen, leveraging the external AIC dataset increased the AP by 1.3 compared with the model trained on the COCO dataset in Table 2. The improvement became 1.5 AP when using the proposed joint-training strategy. The proposed HNDM method increased the AP further by an extra gain of 0.3, while the AR remained the same. This is reasonable since HNDM aims to suppress the keypoints of the false-positive person detections, meaning that it can increase the precision but has little influence on the recall. After exploiting the unlabeled dataset, CCM obtained a final AP of 75.6, a gain of 2.1 over the same model trained on the COCO dataset.

We also evaluated the impact of the detection score threshold in HNDM. Specifically, we set its value to 0.5, 0.7, and 0.9 in the training strategy “ACH→CH”. The results are summarized in Table 6. As can be seen, the performance decreased with the increase of the threshold. Note that fewer hard negative detections were included in the training set when a larger threshold was used. For example, there were 11,762 detections at the threshold of 0.5 while only 4,359 and 952 detections were left at the threshold of 0.7 and 0.9, respectively. On the one hand, since there were many person detection proposals with low scores from the person detectors to increase the recall, thereby leveraging hard negative person detections with low scores (e.g., 0.5∼0.7) and predicting all-zero heatmaps could reduce false positive keypoints on those person detection proposals. On the other hand, due to the annotation policy, human-like objects such as model, sculpture, and doll (please see Fig. 5) were not annotated as the person category, which however could be detected with high scores by the person detector. Since they shared similar appearance with real human bodies, only using these hard negative person detections with high scores (e.g., ≥0.9) increased the difficulty for learning discriminative feature representation for keypoint detection.

Table 6 Study of the threshold setting in HNDM for the training strategy “ACH→CH” described in Sect. 4.1
Full size table
Remark
1) The proposed joint-training strategy is more effective than transfer learning by reducing the domain gap during the pretraining phase; 2) HNDM can deal with the false-positive person detections, thereby improving the detection precision; and 3) exploiting extra unlabeled data using knowledge distilling enables the network to learn more discriminative features from abundant and diverse samples.

Comparison of Sub-pixel Refinement Techniques
We conducted the contrastive experiments by using different sub-pixel refinement techniques in both the simple baseline method (Xiao et al. 2018) and the High-Resolution Network. The results are summarized in Table 7 and Table 8.

Table 7 Experiments on different sub-pixel refinement techniques using the simple baseline method (Xiao et al. 2018)
Full size table
Table 8 Experiments on different settings of sub-pixel refinement tricks using the High-Resolution Network (Sun et al. 2019) (HRNet-w32)
Full size table
First, the sub-pixel refinement techniques by second-order approximation described in Sect. 5.1 consistently improved the performance of both methods. Note that shifting towards the gradient directions by 0.25 pixels was effective and improved the performance of ResNet-50 and HRNet-w32 by 1.9 AP and 2.1 AP, respectively. Nevertheless, it only used the first-order derivative information and the shift was a fixed value, limiting its performance. In contrast, the proposed SOA refinement further increased the AP from 68.5 to 71.0, and 71.7 to 74.3, for ResNet-50 and HRNet-w32, respectively. With the second-order approximation, SOA could adaptively calculate the shift vector for each heatmap. The paraboloid-based SOA and parabola-based SOA performed similarly. It is reasonable because the target heatmap is a 2D Gaussian density map where the density along the x-axis is independent of the density along the y-axis. The predicted heatmap had a similar pattern. Therefore, the paraboloid-based SOA had no obvious advantage over the parabola-based SOA. Nevertheless, the paraboloid-based SOA may be useful for those scenarios where the joint densities along different axes are not independent of each other, i.e., there is an elliptical response with a bias direction in the heatmap.

Second, Soft-NMS was effective, which improved the performance by 0.8 AP and 0.9 AP for ResNet-50 and HRNet-w32, respectively. The weighted fusion defined by Eq. (23) shifted the keypoint locations in sub-pixels by considering the reasonable estimations rather than filtering them out as done in standard NMS. Besides, the SOA technique was complementary to Soft-NMS. For example, the parabola-based SOA technique combined with Soft-NMS improved the performance further by 0.4 AP compared with using the parabola-based SOA technique individually and improved the performance further by 2.1 AP compared with using Soft-NMS individually for both baselines.

Third, the flip test together with the one-pixel shift of flipped heatmaps improved the performance consistently, i.e., by a margin of 1.2 AP and 1.0 AP ResNet-50 and HRNet-w32, respectively. The SOA technique was complementary to the flip test. We leave the analysis on the sub-pixel shift later.

Fourth, Gaussian filtering was beneficial for improving detection performance. A gain of 0.3 AP and 0.6 AP was achieved for ResNet-50 and HRNet-w32, respectively. It was complementary to the SOA technique. Using them together outperformed using each of them individually. To show the complementarity among all the techniques, we used them together in both methods. They boosted the vanilla baseline’s performance by a large margin, e.g., 4.1 AP for ResNet-50 and 4.0 AP for HRNet-w32.

We evaluated the influence of the shifted pixel for the flipped heatmap described in Sect. 5.4. As can be seen from the bottom rows in Table 7 and Table 8, the performance dropped significantly without shifting the flipped heatmap, i.e., from 72.6 AP to 70.7 AP for ResNet-50, and from 75.7 AP to 73.8 AP for HRNet-w32, since the flipped heatmap was not aligned with the original one. Generally, increasing the shifted pixel from zero to 0.8 consistently improved the detection accuracy. It saturated at a shift of 0.8 pixels and then dropped at a shift of one pixel. We used the sub-pixel refinement techniques in our submission to the 2019 COCO Keypoint Detection Challenge.

Remark
1) Each sub-pixel refinement technique has a positive but slightly different influence on the performance, i.e., SOA ≥ SSP ≥ Soft-NMS ≥ GF; 2) the proposed SOA and SSP are much better than their vanilla counterparts due to the closed-form and sub-pixel level approximation; and 3) these techniques are complementary since they are carried out in subsequent steps for unique and explicit purposes.

Inference Time Analysis
We also compared the inference time of different sub-pixel refinement techniques and the proposed CM module. Specifically, we chose HRNet-w32 as the backbone network. We divided the inference process into three parts, i.e., 1) the network forward process (denoting “Network Forward”); 2) the process of getting final keypoint coordinates from predicted heatmaps (denoting “Map → Coord”); and 3) the process of calculating the evaluation metrics of the keypoint detection results (denoting “Evaluation”), and recorded their inference time separately. For each setting, we ran the model three times and calculated the average inference time for single person instance. The results are summarized in Table 9.

Table 9 Inference time (millisecond, i.e., ms) of different sub-pixel refinement techniques and the proposed CM module
Full size table
As can be seen, the inference time of different post-processing techniques mainly differs in the process of “Map → Coord”. For example, replacing the NMS in the default setting of the baseline model (Xiao et al. 2018) with the proposed soft-NMS only increased by 0.04 millisecond (ms). SOA and SSP increased the inference time slightly, i.e., from 3.85 ms to 4.13 ms and from 4.73 ms to 4.99 ms. However, Gaussian filtering required much more computations and increased the inference time from 4.13 ms to 4.73 ms. It is noteworthy that the process of “Map → Coord” cost more inference time than “Network Forward” since it was mainly carried out on CPU while network forward computation was carried on GPU. Adding a CM module on HRNet-w32 only increased the network forward time slightly, i.e., about 0.05 ms. However, since the heatmaps generated by CCM were two times larger than those by the baseline model, the process of “Map → Coord” became much slower, i.e., about two times. It is worth further study to find a fast GPU implementation for this process.

Comparison with State-of-the-Art Methods
The results of CCM and SOTA methods on the COCO minival are summarized in Table 10. CCM was trained on the COCO dataset without using the external AI Challenger dataset. We evaluated the proposed approach on three groups of backbone networks, i.e., the small ones including ShuffleNet-v2 and MobileNet-v2, the medium ones including ResNet-50 and HRNet-w32, and the large ones including ResNet-152 and HRNet-w48, respectively. As can be seen, our small model based on ShuffleNet-v2 and MobileNet-v2 significantly improved the detection accuracy compared with the baseline model (Xiao et al. 2018), i.e., a gain of 4.0 AP and 4.1 AP, respectively. The improvement is at the cost of 5–10% more parameters and about 15% more GFLOPs, which are affordable. CCM also outperformed the Hourglass model (Newell et al. 2016) and was comparable with the CPN (Chen et al. 2018b) based on ResNet-50.

Table 10 Comparisons of CAPE-Net and SOTA methods on the COCO minival set
Full size table
As for the medium backbone networks, simple baseline method (Xiao et al. 2018) achieved similar performance using ResNet-50 and ResNeXt-50, but they were inferior to the High-Resolution Network (Sun et al. 2019) based on HRNet-w32. The proposed CCM based on ResNet-50 achieved a 74.3 AP and outperformed other models with the same input size and backbone network, for example, a gain of 3.9 AP over the simple baseline method (Xiao et al. 2018). Replacing ResNet-50 to ResNeXt-50 leads to a slightly better result, i.e., from 74.3 AP to 74.5 AP. When using the HRNet-w32 as the backbone network, our CCM model increased the AP from 74.4 to 76.7 and achieved the best performance among all the models. Note that CCM based on ResNet-50 or ResNeXt-50 has more parameters and GFLOPs than the baseline model since it uses dilated convolutions to increase the feature map size and three CMs in the decoder. As for the one based on HRNet-w32, it has roughly the same amount of parameters and GFLOPs as its counterpart since only one CM was used.

Our large model based on ResNet-152 with input size 384×288 achieved a gain of 2.4 AP over the simple baseline model (Xiao et al. 2018) and a gain of 0.4 AP over the recent HRNet-w48 model (Sun et al. 2019). For example, CCM outperformed HRNet-w48 by a margin of 0.4 AP and 0.5 AP at the threshold 0.5 and 0.75. However, CCM was inferior to HRNet-w48 for large person instances, i.e., a drop of 0.2 AP. One possible explanation is that HRNet-w48 learned a high-resolution and discriminative feature representation by integrating the features from different scales. We attached a CM to HRNet-w48 and used the sub-pixel refinement techniques for postprocessing. It further improved the detection accuracy of HRNet-w48 from 76.3 AP to 77.5 AP. Besides, the result of large person instances was improved by 0.6 AP. Our CCM model achieved the best performance among all other models based on comparable backbone networks. Besides, both models have nearly the same parameters and GFLOPs as the baseline models since we decreased the number of filters in the CM for ResNet-152 and only used one CM for HRNet-w48. These results demonstrate the effectiveness of the proposed CCM, training strategies, and sub-pixel refinement techniques.

The results of CCM and SOTA methods on the COCO test-dev set are summarized in Table 11. The CCM using ResNet-152 as the backbone network trained on the COCO dataset outperformed the baseline model (Xiao et al. 2018) by 2.1 AP. It also outperformed the recent HRNet-w48 model (Sun et al. 2019) by 0.3 AP. Using HRNet-w48 as the backbone network, CCM outperformed the vanilla HRNet-w48 model, MSPN (Li et al. 2019), and DARK (Zhang et al. 2020) by a margin of 1.1 AP, 0.5 AP, and 0.4 AP, respectively. It was even better than the ensemble simple baseline models trained with the external AI Challenger dataset. After joint-training with this external dataset, CCM based on ResNet-152 outperformed both the simple baseline method and HRNet-w48. Besides, replacing the backbone network from ResNet-152 to HRNet-w48, the performance was further improved by 0.7 AP. It was better than the recently proposed method DARK* (Zhang et al. 2020) by a margin of 0.6 AP using the same person detection results and comparable with the ensemble models MSPN+* (Li et al. 2019) (the champion of the 2018 COCO Keypoint Challenge), i.e., 78.0 v.s. 78.1. Generally, the external dataset brought about 1.5 AP improvement, which mainly arose from the AP at the larger threshold, demonstrating that training on more diverse poses helped the model to learn discriminative features and improve the location accuracy. Our final ensemble models brought another 0.9 AP and set a new state-of-the-art on this benchmark, i.e., 78.9 AP. It was comparable with RSN+ (Cai et al. 2020) (the champion of the 2019 COCO Keypoint Challenge), which used a better person detector and had 59.8 AP for the person category on COCO val2017.

Table 11 Comparisons of CCM and SOTA methods on the COCO test-dev set
Full size table
Fig. 10
figure 10
Some visual examples of the keypoint detection results on the COCO minival set (Lin et al. 2014)

Full size image
Fig. 11
figure 11
Some visual examples of the keypoint detection results on the PoseTrack validation set (Andriluka et al. 2018)

Full size image
Fig. 12
figure 12
Visualization of the feature maps from the CMs in CCM based on ResNet-50. “HDC k” stands for the feature maps from the kth dilated convolutional layer in the hybrid-dilated convolutional branches. “HDC x SE” stands for the first term in Eq. (4). “Deconv” stands for the outputted feature maps from the CMs, i.e., the left side of Eq. (4)

Full size image
Remark
1) Our CCM model consistently outperforms the baseline models using small, medium, and large backbone networks, but the gain becomes smaller with the increasing of the model capacity, i.e., MobileNet ≈ ShuffleNet ≈ ResNet-50 ≈ ResNeXt-50 ≥ HRNet-w32 ≈ ResNet-152 ≥ HRNet-w48. It is reasonable because “bigger” backbone networks themselves have stronger representation capacity, thereby the impact of CM decreases accordingly; and 2) our CCM model benefits from the effective CM modules to model the context information, the efficient training strategies to learn discriminative features, and the sub-pixel refinement techniques to locate keypoints accurately, in a collaborative and complementary manner.

Subjective Visual Inspection and Discussion
We presented some visual examples of the keypoint detection results by using the CCM model based on HRNet-w48 on the COCO minival set and PoseTrack validation set in Figs. 10 and 11, respectively. As can be seen from, our model could handle various poses. Besides, it also successfully inferred the occluded keypoints (self-occluded or occluded by other objects). In the bottom two rows, we presented the detection results on multiple person instances within each image, which were also promising. Our model could handle small instances, blurry ones, low-light images as well as various occlusions. To see how CCM achieved the performance, we conducted an experiment to visually inspect the learned features by the network.

We overlaid the output feature maps from the CMs in CCM on the input images to inspect what has been learned by the network. Figure 12 shows the feature maps learned by the CMs at different levels. “HDC k” stands for the feature maps from the kth dilated convolutional layer in the hybrid-dilated convolutional branches. “HDC x SE” stands for the first term in Eq. (4). ’Deconv’ stands for the outputted feature maps from the CMs, i.e., the left side of Eq. (4). “Level k” stands for the index of CM in CCM. The keypoints belonging to the left (right) body were connected by red (blue) lines. The left-right symmetric keypoints were connected by yellow lines. The predicted invisible keypoints of occluded body parts were indicated by red arrows.

As can be seen, HDC paid more attention to the context with the increase of the dilation rate. CCM learned to identify easy keypoints and inferred hard keypoints progressively through the cascaded CMs. Please check the arrows and ellipses. Moreover, 1) CCM probably learned the body configuration by inferring the invisible keypoints and potential poses conditioned on the visible keypoints as shown in the first row; 2) CCM also learned the body symmetry, e.g., there were two legs and arms in the human body, as shown in the left part of the second row; 3) CCM could also handle blurry images and infer the head and right arms as shown in the right part of the second row.

Fig. 13
figure 13
Visualization of the feature maps learned by CCM based on ResNet-50 at different stages. “Deconv” stands for the outputted feature maps from the CMs, i.e., the left side of Eq. (4). One or several of keypoints of each person is manually occluded by a mask

Full size image
To illustrate the effectiveness of CCM for handling occlusions, we manually added masks on some body parts, e.g., the head, hip, and ankle as supplements to the existing occlusions, as shown in Fig. 13. CCM first recognized and located the human bodies using the encoder (Res5), then detected different body parts (Deconv1) to help identify some distinct keypoints (Deconv2). In the final stage (Deconv3), CCM predicted the difficult occluded keypoints using the context information of identified body parts and keypoints. To further investigate the effectiveness of CCM for handling severe occlusions, we manually masked out either the upper or lower body of each person as shown in the first column of Fig. 13. As can be seen, although the predicted human poses were different from the ground truth annotations at the masked regions, CCM showed the ability to learn human body configuration and infer reasonable invisible keypoints. Moreover, the results also confirm that CCM detected those distinct keypoints in the earlier stage while predicting the difficult occluded ones in the later stage.

Remark
(1) Empirically, CCM’s detection process follows a “Localization → Componentization → Identification → Prediction” routine, similar to the procedure that humans detect human keypoints in occluded settings (Sect. 3.1); and (2) CCM probably has learned the body configuration such as symmetric body parts and reasonable distances between adjacent keypoints, evidenced by visual examples.

Empirical Studies on Annotations
As we know that occluded keypoints are more difficult to be detected than visible ones, we are wondering whether the invisible keypoint annotations have the same impact on the model or not, compared with the same amount of visible keypoint annotations? To this end, we conducted an experiment to gain some insight into the keypoint annotations.

We constructed two training sets based on the COCO training set. The first one was COCO-I which was obtained by removing all the annotations of invisible keypoints in the original training set. The second one was COCO-V which was obtained by removing the same amount of visible keypoint annotations randomly. The resulting keypoints without annotations were treated as unlabeled. Then, we trained the proposed CCM using the ResNet-50 as the backbone encoder on COCO-I and COCO-V. They were denoted as “CCM-I” and “CCM-V”, respectively. Their results on the COCO minival set are summarized in the first two rows of Table 12. As a reference, we also listed the model trained on the original COCO training set in the bottom row.

Table 12 Comparison between CCM-I and CCM-V on the COCO minival set
Full size table
Table 13 Comparison between CCM-I and CCM-V on the visible and invisible keypoints in the COCO minival set
Full size table
Table 14 Comparison between CCM-I and CCM-V on different types of instances w.r.t. the numbers of annotated keypoints
Full size table
As can be seen, the scores of CCM-V dropped marginally compared with CCM. However, the scores of CCM-I dropped significantly by a large margin compared with CCM-V and CCM. These results confirm that annotating invisible keypoints matters, i.e.. The invisible keypoint annotations contributed more to the model than the same amount of visible ones. Since it is more difficult to detect invisible keypoints, the invisible keypoint annotations provide stronger supervisory signals to the model than the visible ones did. To infer an invisible keypoint with such supervision, it probably learned useful features from the context since the keypoint itself was invisible. In this way, the model could learn the knowledge of body configuration implicitly, e.g., as a form of discriminative feature for each category of keypoints.

To further analyze the impact of invisible keypoint annotations, we calculated the indexes on visible and invisible keypoints, respectively. We removed all the invisible keypoint annotations from the COCO minival set. The resulting minival set was used to evaluate the model’s performance on the visible keypoints. Similarly, we also removed all the visible keypoint annotations from the COCO minival set to evaluate the model’s performance on the invisible keypoints. We used the ground truth bounding boxes as the person detection results. The results are listed in Table 13. ±𝛥 denoted the gain of CCM-V over CCM-I.

Unsurprisingly, CCM-V achieved better results on invisible keypoints compared with CCM-I, demonstrating that the gains of CCM-V over CCM-I in Table 12 mainly arose from the invisible ones. Note that although CCM-I was trained without using the annotations of invisible keypoints, it still obtained 40.2 AP on them, demonstrating that it had a bit of generalization on predicting the invisible keypoints. Since there were distinct appearance differences between visible keypoints and invisible ones of the same category and the model did not get any supervisory signal from the invisible keypoints, it implies that CCM probably learned useful features from contextual keypoints to infer the invisible ones. Using the invisible annotations, CCM achieved better performance by exploiting its representation capacity.

We also reported APs of the two models on instances with different numbers of annotated keypoints from the COCO minival set. The results are shown in Table 14. As can be seen, the gains mainly arose from the occluded instances, for example, instances with less than 10 annotated keypoints. It implies that the invisible keypoint annotations helped the model to learn the body configuration for inferring the invisible keypoints and handling occlusions. Some visual results were presented in Figs. 13 and 14.

Although annotating invisible keypoints is more difficult than visible ones, the above results confirm that the invisible keypoint annotations are more valuable. Consequently, we could improve our model by (1) annotating more occluded keypoints to train a better model; (2) exploiting the active learning strategy to identify hard keypoints that should be annotated to continuously improve the model; (3) developing effective multi-view learning algorithms to utilize the complementary information between different views of data.

Fig. 14
figure 14
Visualization of the feature maps learned by CCM based on ResNet-50 at different stages. Either the upper or lower body of each person is manually masked out

Full size image
Remark
(1) The invisible keypoint annotations have a larger impact on the model than the same amount of visible ones; and (2) even without the invisible annotations, the proposed CCM model is still able to generalize to the invisible keypoints.

Conclusion
In this paper, we address the human keypoint detection problem by devising a new cascaded context mixer-based neural network (CCM), which has a strong representative capacity to simultaneously model the spatial and channel context information. We also propose three efficient training strategies including a hard-negative person detection mining strategy to migrate the mismatch between training and testing, a joint-training strategy to use abundant unlabeled samples by knowledge distilling, and a joint-training strategy to exploit external data with heterogeneous labels. They collaboratively enable CCM to learn discriminative features from abundant and diverse poses. Besides, we present four postprocessing techniques to refine predictions at the sub-pixel level accuracy. These complementary techniques are carried out sequentially during the inference phase for unique and explicit purposes, which further improve the detection accuracy. Our CCM model consistently outperforms public state-of-the-art models with various backbone networks by a large margin. We empirically show that CCM’s detection process is similar to humans in occluded settings and probably learns human body configuration. Moreover, we also identify that invisible keypoint annotations have a larger impact on the model than the same amount of visible ones and present some promising research topics.