Ghosting artifacts caused by moving objects and misalignments are a key challenge in constructing high dynamic range (HDR) images. Current methods first register the input low dynamic range (LDR) images using optical flow before merging them. This process is error-prone, and often causes ghosting in the resulting merged image. We propose a novel dual-attention-guided end-to-end deep neural network, called DAHDRNet, which produces high-quality ghost-free HDR images. Unlike previous methods that directly stack the LDR images or features for merging, we use dual-attention modules to guide the merging according to the reference image. DAHDRNet thus exploits both spatial attention and feature channel attention to achieve ghost-free merging. The spatial attention modules automatically suppress undesired components caused by misalignments and saturation, and enhance the fine details in the non-reference images. The channel attention modules adaptively rescale channel-wise features by considering the inter-dependencies between channels. The dual-attention approach is applied recurrently to further improve feature representation, and thus alignment. A dilated residual dense block is devised to make full use of the hierarchical features and increase the receptive field when hallucinating missing details. We employ a hybrid loss function, which consists of a perceptual loss, a total variation loss, and a content loss to recover photo-realistic images. Although DAHDRNet is not flow-based, it can be applied to flow-based registration to reduce artifacts caused by optical-flow estimation errors. Experiments on different datasets show that the proposed DAHDRNet achieves state-of-the-art quantitative and qualitative results.

Access provided by University of Auckland Library

Introduction
The dynamic range of natural luminance values of natural scenes often varies over several orders of magnitude. The inherent limitations of camera sensors mean that they measure a limited fraction of this range. The resulting low dynamic range (LDR) images thus often exhibit over or under-exposed regions, and fail to capture significant details therein. High dynamic range (HDR) imaging has been developed to compensate for these limitations, and aims to generate a single image that captures all of the details of the scene irrespective of its luminance.

Fig. 1
figure 1
LDR images with different exposures are shown in a, and our result after tonemapping is shown in b. The areas of the images that exhibit both large-scale movement and saturation are displayed in c. DAHDRNet generates an HDR image with fewer ghosting artifacts and more detail in the otherwise saturated regions (see zoomed-in patches in d)

Full size image
Specialized hardware devices (Nayar and Mitsunaga 2002; Tumblin et al. 2005) have been proposed to produce HDR images directly. For instance, Nayar and Mitsunaga (2002) produced HDR images using the process of spatially varying pixel exposures. Unfortunately, despite the tremendous potential, these devices are usually too expensive to be widely adopted. As a result, computational HDR imaging methods have drawn more attention. The most common strategy is to take a series of LDR images at different exposures and then merge them into an HDR image (Debevec and Malik 1997; Mann et al. 1995; Miguel et al. 2010; Reinhard et al. 2005; Yan et al. 2017). In multiple exposure methods, one of the LDR images is usually considered as the reference image (shown with the green border in Fig. 1a). Although these methods often generate high-quality HDR results when the scene and camera are entirely static, they often suffer from significant ghosting and blurring artifacts when there is motion between the input images.

Global image misalignments can be compensated for using homographies (Szpak et al. 2014, 2015; Wu et al. 2018). Homography-based approaches cannot accommodate general scene, or camera motion, however. Two major categories of methods have arisen as a result. The first category of methods performs a fine-grained alignment of the LDR images before merging (Tursun et al. 2015; Zimmer et al. 2011; Kalantari and Ramamoorthi 2017). Many of these methods align the LDR images through optical flow (Kalantari and Ramamoorthi 2017; Kang et al. 2003; Zimmer et al. 2011). Pixel-accurate optical flow estimation is an error-prone process, however, and often leads to alignment errors. The second category of methods assumes that only a small portion of each image is affected by motion or misalignment (Jacobs et al. 2008; Grosch 2006; Srikantha and Sidibe 2012). These algorithms aim to reject the corresponding image segments as outliers, and merge the inliers using traditional approaches. Pixel-accurate identification of moving objects and misalignment is difficult to achieve robustly, however, particularly when relying on simple pixel-level characteristics (e.g., pixel color Raman and Chaudhuri 2011).

Inspired by the successes of the deep neural networks (DNNs) in many image restoration tasks (Ledig et al. 2017; Gong et al. 2016; Yang et al. 2018; Gong et al. 2018), a variety of deep learning-based approaches (Kalantari and Ramamoorthi 2017; Wu et al. 2018; Yan et al. 2019) have been proposed recently to improve the HDR image composition process. In Kalantari and Ramamoorthi (2017), a DNN is proposed to merge the LDR images after an optical-flow-based alignment process. However, the DNN cannot handle the distortions caused by the inevitable optical flow estimation error (see Kalantari et al.’s method in Fig. 1d). In Wu et al. (2018), the HDR imaging task is treated as an image translation problem. Although the model produces satisfactory results in some examples, it produces ghosting artifacts when there are large-scale movements between the images. The DNN-based methods can reconstruct missing detail in saturated or under-exposed regions, but cannot do so for large areas, particularly when there is also occlusion.

The structure of DAHDRNet is shown in Fig. 2. The neural network learns the relationships between input LDR images and HDR output. Previous methods (Kalantari and Ramamoorthi 2017; Wu et al. 2018) take stacked LDR images, or LDR image feature maps, as the input to the merging process. This mixes the misaligned image components at an early stage of the network, making it difficult to obtain ghost-free HDR results. DAHDRNet, in contrast, trains dual-attention (i.e., spatial and channel) modules to guide the merging process on the basis of the LDR image contents. The spatial attention module acts as a control gate performing an element-wise product of convolutional feature maps to remove potential ghosting artifacts. The resulting soft attention maps reflect the importance of each image region for obtaining the final HDR image. The expectation is that they will highlight the features complementary to the reference image and exclude regions with motion and severe saturation.

Most existing DNN-based methods treat all channel-wise features equally (Kalantari and Ramamoorthi 2017; Wu et al. 2018). This approach lacks the flexibility to adaptively select features (Zhang et al. 2018). To make the network focus on more informative features for ghosting removal, we exploit the inter-dependencies between feature channels, resulting in a channel attention mechanism. Information in the non-reference LDR images has both static and dynamic components. The static parts are consistent with the reference image. The dynamic components would usually be regions corresponding to motion, saturation, and other artifacts. The channel attention modules minimise the impact of these artifacts by adaptively rescaling each channel’s relevant features by modeling the inter-dependencies across feature channels with global information. Finally, these two attention modules are fused to extract the non-motion features from inputs. To further improve the representation capability of the dual attention module, we adopt a recurrent dual-attention module to progressively highlight the useful features from LDR images for HDR imaging.

The LDR image features with dual-attention guidance are fed to the merging network to generate the HDR image. Such merging networks typically achieve higher performance with deeper structure. This consumes more computing resources, however, and does not make full use of all the layers’ information. In addition, to obtain more content information, the network demand to quickly increase the receptive field of CNNs. To address these shortcomings, we construct a merging network using dilated residual dense blocks (DRDBs). This is achieved by employing the dilated convolution layers in the residual dense block (RDB) proposed in Zhang et al. (2018). The DRDBs help to make full use of the information from different convolutional layers and thus preserve more details from the input LDR images. On the other hand, the DRDBs enlarge the receptive fields, helping to recover the details in areas contaminated by saturation and moving objects. Each dilated convolutional layer in DRDB has access to all the subsequent layers and passes on information that needs to be preserved (Huang et al. 2017; Zhang et al. 2018). Thus, the merging procedure can reconstruct satisfying details that are not captured by LDR images due to occlusion or saturation.

The results of extensive quantitative and qualitative evaluation are provided below, but demonstrate that the proposed approach is robust and gets better performance than existing state-of-the-art HDR image reconstruction approaches for challenging natural images (See Fig. 1). The main contributions of the paper can be summarized as:

We propose a new dual-attention-guided network that learns a recurrent dual-attention module for ghost-free HDR imaging. It has all of the benefits of a neural network model and overcomes one of the primary problems in HDR imaging is that it is robust to large misalignments of image pixels and saturation.

We propose a network based on dilated residual dense blocks to merge the attention guided feature maps from LDR images. The dilated residual dense blocks can simultaneously preserve the image details and enlarge the receptive fields, allowing the network to hallucinate the contents in saturated regions.

Extensive experiments on different datasets validate the superiority of the proposed DAHDRNet. We also conduct ablation studies to quantify the roles of different components in our model.

This proposed algorithm is extended from our preliminary work [AHDR (Yan et al. 2019)] with the following differences. First, we introduce a more effective network (with the dual attention module and recurrent attention) for ghost-free HDR imaging. Second, we conduct more analyses and discussions on the mechanism of the attention module for suppressing the ghosts in HDR imaging (See the section of experiments). Third, we carry out a rationality analysis of the parameters in the network and loss function. Moreover, we provide more details and discussions about the model, implementation, and experiments.

The rest of this paper is organized as follows. A brief review of related works from literature is given in Sect. 2. The architecture of the proposed DAHDRNet is presented in Sect. 3. Experimental results on both quantitative and qualitative are shown in Sect. 4. Finally, conclude the paper with a brief summary in Sect. 5.

Related Work
In the past few decades, there are many HDR imaging methods have been proposed. A complete review of HDR imaging (HDR image generation and tone mapping) can be learned in Reinhard et al. (2005). Since our goal is HDR imaging, we will focus on the algorithms related to HDR generation. The primary relevant works are as follows.

Methods Relying on Pixel Rejection  These approaches label each pixel as belonging to a static region or a moving object based on the assumption that the images are globally registered. Grosch (2006) defined an error map that uses the color difference of inputs to get the ghost-free HDR image. Jacobs et al. (2008) detected ghost regions based on a weighted variance measure. Pece and Kautz (2010) computed the median threshold bitmap for input images to detect motion regions. Heo et al. (2011) roughly detected motion regions by joint probability densities, and these regions are refined using energy minimization based on graph-cuts methods. Zhang and Cham (2012) proposed quality measures based on image gradients to generate a weighting map over the inputs. Rank minimization (Lee et al. 2014; Oh et al. 2015) has also been used to detect motion regions and reconstruct HDR images. Even it is achieved to the required pixel accuracy, rejecting pixels reduces the information available to reconstruct the HDR image, which often leads to missing details [See Oh’s method (Oh et al. 2015) in Fig. 1].

Methods Relying on Registration  These approaches reconstruct each HDR region by searching for the best matching region in LDR images. This is achieved using pixel (optical flow methods) or patch (patch-based methods) based dense correspondences. Bogoni (2000) estimated motion vectors using optical flow and used parameters to warp pixels in the exposures. Kang et al. (2003) transformed intensities of LDR images to the luminance domain using exposure time information and computed the optical flow to find corresponding pixels among the LDR images. Sen et al. (2012) proposed a patch-based energy minimization approach that integrates alignment and HDR reconstruction in a joint optimization. Hu et al. (2013) optimized image alignment based on brightness and gradient consistencies on the transformed domain. Hafner et al. (2014) proposed an energy-minimization approach that simultaneously calculates HDR irradiance and displacement fields. This approach improves robustness, but fails for large motions, doesn’t learn by examples, and makes no attempt to compensate for saturation.

Deep Learning Based Methods  Many deep learning approaches (Eilertsen et al. 2017; Kalantari and Ramamoorthi 2017; Wu et al. 2018) have been developed. Eilertsen et al. (2017) proposed a deep autoencoder network to predict HDR values from one image. Endo et al. (2017) synthesized multiple LDR images from one LDR image with the deep-learning-based approach, then reconstructed an HDR image by merging them. Santos et al. (2020) proposed a feature masking mechanism that reduces the contribution of the features from the saturated areas and adopts the VGG-based perceptual loss function to be able to synthesize visually pleasing textures. Gharbi et al. (2017) introduced a new neural network architecture inspired by bilateral grid processing and local affine color transforms. Yang et al. (2018) formulated the image correction task as an HDR transformation process consisting of two DNNs for HDR image reconstruction and tone mapping. Cai et al. (2018) proposed to use the DNN to train a SICE enhancer for a single image. These works are devoted to generating HDR images for a single LDR image. They are not suffered from the ghosting artifacts but often fail to reconstruct the information in saturation regions. For multi-frames HDR imaging, Kalantari and Ramamoorthi (2017) used optical flow to align the input images to the reference image, then employed a convolutional neural network to obtain the HDR image. Wu et al. (2018) proposed a network that can learn to translate multiple LDR images into a ghost-free HDR image. Yan et al. (2020) removed ghosting artifacts by exploiting the non-local correlation in inputs. Prabhakar et al. (2020) proposed an HDR deghosting approach to process high-resolution images with bilateral guided upsampler. These methods have the advantage that they can exploit information extracted from training data to identify and compensate for image regions that do not meet the assumptions underlying the HDR process.

In computational photography, there is extensive work investigating how to optimally co-design hardware encoding (optics/sensor) and image processing algorithms. Several different strategies have been developed to overcome the limited dynamic range of available sensors. Metzler et al. (2020) preserved information about the saturated pixel values by encoding information about the brightest pixel values into nearby pixels via an optical filter with an optimized point spread function (PSF). Kalantari and Ramamoorthi (2019) proposed sequential convolutional neural networks, which consist of aligning and combination to produce the HDR image. Martel et al. (2020) introduced neural sensors as a methodology to optimize per-pixel shutter functions jointly with a differentiable image processing method in an end-to-end fashion. Sun et al. (2020) proposed a method for snapshot HDR imaging by learning an optical HDR encoding in a single image which maps saturated highlights into neighboring unsaturated areas using a diffractive optical element (DOE). These methods address an important issue, but none has the flexibility and robustness that the proposed dual-attention-based approach enables (See Fig. 1).

Attention Mechanisms in Deep Learning Methods  Attention has shown to be a pivotal development in deep learning and has been used in many computer vision applications. Lu et al. (2017) proposed a novel adaptive attention model with a visual sentinel for image captioning. Fan and Zhou (2018) stacked latent attention for multiple multimodal reasoning tasks. Zhao et al. (2017a) proposed a diversified visual attention network to address the problem of fine-grained object classification. In the image restoration task, Zhang et al. (2018) employed the very deep residual channel attention networks to choose information across channels unequally. Suin et al. (2020) proposed an efficient pixel adaptive and feature attentive design for handling large blur variations across different spatial locations and process each test image adaptively. Zhang et al. (2020) introduced attention strategy to suppress undesired chromatic aberration and noise. Saeed and Nick (2019) applied feature attention to exploiting the channel dependencies in denoising. Each has achieved the hitherto impossible performance and robustness by allowing models to focus on only the relevant information.

Fig. 2
figure 2
The architecture of the proposed DAHDRNet. The network consists of a dual-attention network for feature extraction and a merging network for predicting the HDR image. The dual-attention module is used to progressively exclude the harmful components caused by misalignment and saturation or highlight the useful details. The merging network is constructed based on a series of dilated residual dense blocks (DRDBs). The global residual skip connection is used to boost the training. The final HDR result is obtained by tonemapping. All the feature maps have 64 channels, and the kernel size is 3

Full size image
Fig. 3
figure 3
Structure of the recurrent dual-attention module. We use the recurrent dual-attention module to fully exploit the correlation between the features from reference and non-reference branches and exclude the harmful components progressively. Since each dual-attention module has the same purpose of adaptively refining the non-reference features (𝑍1, 𝑍3), the parameters are shared among blocks. The visualized map is a presentation of the averaged attention features

Full size image
The Proposed Network for HDR Imaging
Given a series of LDR images of a dynamic scene (𝐼1,𝐼2,…,𝐼𝑘) with different exposures, the target of HDR imaging is to recover an HDR image H aligned to a prescribed reference image 𝐼𝑟 (selected from the input LDR images). All of the images 𝐼𝑖 and H are RGB images with three channels. Following the settings in Kalantari and Ramamoorthi (2017), Wu et al. (2018), we use three LDR images (𝐼1,𝐼2,𝐼3) (sorted by their exposure lengths), i.e., 𝑘=3, and let the middle exposure image 𝐼2 be the reference image.

Before feeding the LDR images to the network, we first map the input LDR images {𝐼𝑖} to the HDR domain relying on gamma correction (Kalantari and Ramamoorthi 2017; Wu et al. 2018) to generate a corresponding set of {𝐻𝑖}:

𝐻𝑖=𝐼𝛾𝑖/𝑡𝑖,∀𝑖=1,2,3,
(1)
where 𝛾>1 denotes the gamma correction parameter and 𝑡𝑖 denotes the exposure time of the image 𝐼𝑖. We set 𝛾=2.2 in this work. As suggested in Kalantari and Ramamoorthi (2017), we concatenate images 𝐼𝑖 and 𝐻𝑖 along the channel dimension to obtain the 6-channel tensors 𝑋𝑖=[𝐼𝑖,𝐻𝑖],𝑖=1,2,3 as the input of the network. Intuitively, the LDR images 𝐿𝑖 help to identify the noisy and saturated regions, while the 𝐻𝑖 facilitate the detection of the alignments (Kalantari and Ramamoorthi 2017). Given (𝑋1,𝑋2,𝑋3) as input, the proposed DAHDRNet obtains the HDR image by

𝐻=𝑓(𝑋1,𝑋2,𝑋3;𝜃),
(2)
where 𝑓(⋅) denotes the proposed HDR network, and 𝜃 is the network parameters. The attention mechanism works as part of the end-to-end DAHDRNet network 𝑓(⋅). Note that the input images of the proposed model can be the original images without any alignment preprocessing.

Overview of the DAHDRNet Architecture
Unlike the previous methods (Kalantari and Ramamoorthi 2017; Wu et al. 2018) that stack the input images 𝑋𝑖 or the extracted feature maps in the early stage of the network for merging, the proposed DAHDRNet obtains the attention maps by comparing the encoded image features and then merges features with the guidance of the attention maps. Our preliminary AHDR method (Yan et al. 2019) only considers the spatial attention in the network. As shown in Fig. 2, the DAHDRNet consists of two major subnetworks, i.e., the dual-attention network (for feature extraction) and the merging network (for HDR image estimation).

The dual-attention network first separately extracts features from each LDR image relying on the corresponding convolutional encoders. Then, we apply specific attention maps on the non-reference images to identify the beneficial features. The attention maps are obtained via the attention modules according to the feature maps from the reference image and each non-reference image. Considering that the target of the model is to generate the HDR image with the scene consistent with the reference image, the motivation of applying attention on the non-reference images is to identify the misaligned components before merging the features for alleviating the ghosting artifacts. Moreover, attention can also help to highlight the useful features and identify the saturated regions from the under and over-exposed non-reference images.

The merging network takes the features extracted with the attention guidance as input and estimates the HDR image relying on a series of dilated residual dense blocks (DRDBs) and the global residual learning (GRL) strategy. The DRDBs and GRL help to effectively utilize the image features and obtain the HDR image with plausible details. The merging network fuses the features from the LDR images and hallucinates the details in the regions contaminated by the saturation and misaligned moving objects.

Dual-Attention Network for Feature Extraction
Given three 6-channel input images 𝑋𝑖,𝑖=1,2,3 corresponding to the three LDR images, the attention network first uses a shared encoding layer to extract feature maps 𝑍𝑖,𝑖=1,2,3 with 64 channels from three inputs. For clarity, we define notations 𝑋𝑟 and 𝑍𝑟 to indicate 𝑋2 and 𝑍2 corresponding to the reference LDR image in some special context. As shown in Fig. 3, to obtain the attention maps for the non-reference images, we feed the features 𝑍𝑖,𝑖=1,3 of the non-reference images to the first recurrent dual-attention module 𝑎1𝑖(⋅),𝑖=1,3 along with the reference image feature map 𝑍𝑟, and then obtain the attention maps 𝐴1𝑖 for the non-reference images:

𝐴1𝑖=𝑎1𝑖(𝑍𝑖,𝑍𝑟),𝑖=1,3,
(3)
𝐴1𝑖 has the same size as 𝑍𝑖. The values in 𝐴1𝑖 are in the range [0, 1]. Details of the dual-attention modules are provided below. The predicted attention maps are used to attend the features of the non-reference images via:

𝑍1𝑖=𝐴1𝑖∘𝑍𝑖,𝑖=1,3,
(4)
where ∘ denotes the point-wise multiplication and 𝑍1𝑖 denotes the feature maps with the first attention guidance.

We propose a recurrent mechanism to fully exploit the correlation between the features from reference and non-reference branches and exclude the harmful components (e.g., misaligned or saturated regions) progressively. We stack n dual-attention modules, where each module serves as the same purpose of adaptively refining the non-reference features (𝑍1, 𝑍3). The parameters are shared among modules, and the output of the previous layers 𝑍𝑗𝑖,𝑖=1,2,3; 𝑗=2,…,𝑛 is used as the inputs of the next layer.

𝐴𝑗𝑖=𝑎𝑗𝑖(𝑍𝑗𝑖,𝑍𝑗𝑟).
(5)
Then, the refined 𝑍𝑗𝑖 is calculated by:

𝑍𝑗𝑖=𝐴𝑗𝑖∘𝑍𝑗𝑖.
(6)
Figure 3 displays the structure of the recurrent dual-attention module. Since the HDR imaging process centers on the reference image, the attention maps are predicted and applied according to the reference. When some regions in the reference are saturated or noisy, the attention maps can also highlight useful features in the non-reference images. Furthermore, the first dual-attention module captures most of the information in the non-reference image (easy areas). The attention maps after the second and third dual-attention module are prone to progressively highlight tough areas (i.e., saturated regions), which is useful for HDR image restoration. More studies in Sect. 4.2.1 further prove the effectiveness of the proposed attention mechanism in HDR imaging.

Instead of stacking the original feature maps 𝑍𝑖’s for HDR merging, we stack the reference feature map 𝑍𝑛𝑟 and the features of the non-reference images 𝑍𝑛1 and 𝑍𝑛3 for merging. The attention network obtains a stack of features with the guidance of the reference as 𝑍𝑠:

𝑍𝑠=Concat(𝑍𝑛1,𝑍𝑛2,𝑍𝑛3),
(7)
where Concat(⋅) denotes the concatenation operation. 𝑍𝑠 will be used as the input of the merging network.

Dual-Attention Module  The dual-attention modules 𝑎𝑗𝑖(⋅),𝑖=1,3 in Eq. (5) are two parallel small CNNs. The structure of the attention modules is shown in Fig. 4, which consists of spatial and channel attentions. The spatial attention unit (red dashed line in Fig. 4) first concatenates the input feature maps 𝑍𝑖 and 𝑍𝑟 and obtains the attention map after two convolution (Conv) layers. Each Conv layer applies 64-channel with kernel size 3×3. The two Conv layers are followed by a ReLU activation and a sigmoid activation, respectively. The channel attention unit (blue dashed line in Fig. 4) takes the channel-wise global spatial information into a channel descriptor by using global average pooling (GAP). Then, two Conv layers with kernel size 1×1 and ReLU activation to learn non-linear interactions between channels. Here we choose sigmoid activation as the gating strategy. As a result, the dual-attention module can obtain the 64-channel attention map with values in the range [0, 1].

Fig. 4
figure 4
The dual-attention module consists of spatial and channel attentions. The spatial attention first concatenates the two inputs and then obtains attention maps via two Conv layers, which restricts the output in [0,1] using a sigmoid activation. The channel attention fully captures channel-wise dependencies by global average pooling and introduces a simple gating mechanism with a sigmoid function

Full size image
Merging Network for HDR Image Estimation
The merging network takes the stacked feature map 𝑍𝑠 and the reference image feature map 𝑍𝑟 as input. In the design of the merging network, we take account of the characteristics of the HDR imaging problem and use the basic structure of the residual dense network in Zhang et al. (2018) as the reference. As shown in Fig. 2, the network consists of several convolution layers, dilated residual dense blocks, and several skip connections. The generated feature maps at different layers are noted as 𝐹𝑗,𝑗=0,1,…,7.

Given the stacked feature 𝑍𝑠, the merging network first obtains a 64-channel feature map after a Conv layer and then feeds it into three DRDBs, which results in three corresponding feature maps 𝐹1, 𝐹2 and 𝐹3. Instead of using the RDB proposed in Zhang et al. (2018), we proposed to use the RDBs with dilated convolution (DRDB) for HDR imaging. The details of DRDB can be found in the following. By applying 3×3 Conv on the concatenated feature map 𝐹4, we generate the merged and transferred feature map 𝐹5.

Global Residual Learning with the Reference Features  Before reconstructing the HDR image from 𝐹5, inspired by the super-resolution methods (Ledig et al. 2017; Zhang et al. 2018), we apply a global residual learning strategy to obtain feature maps by

𝐹6=𝐹5+𝑍𝑟,
(8)
where 𝑍𝑟 is the shallow feature maps of the reference image. The merging network thus tends to learn the residual features. In the proposed DAHDRNet, we have the shallow feature map 𝑍𝑟 containing the pure information from the reference image. We thus apply the global residual learning with the reference feature maps. We consider that the feature map 𝐹6 contains enough information to reconstruct the HDR image.

Fig. 5
figure 5
Illustration of dilated residual dense block structure with three convolution layers. We adopt a residual dense block (Zhang et al. 2018) as its backbone and each convolution layer can be substituted by dilated convolution. By using dilated residual dense blocks, the receptive field at each block is expanded

Full size image
After two convolution layers (followed by activations), we estimate the HDR image 𝐻ˆ in the HDR domain. The final HDR image is displayed via the tonemapping operation (See Sect. 3.4).

Dilated Residual Dense Block  Since the reconstruction of some local areas of the HDR images cannot get enough information from the LDR images due to the occlusion of moving objects and saturation, the merging network requires a larger receptive field for hallucinating details. We thus apply the 2-dilated convolutions (Yu and Koltun 2015) in the residual dense block (RDB) (Zhang et al. 2018). As shown in Fig. 5, the proposed dilated residual dense block (DRDB) consists of a series of Conv layers followed by ReLU activations and dense concatenation based skip-connections. Each Conv layer takes the concatenation of all the feature maps from previous layers as input. In contrast to the dense block proposed in Huang et al. (2017), the RDB and DRDB apply a local residual skip-connection between the input and output of a block. More details of the RDB can be found in Zhang et al. (2018). In our implementation, we use 6 Conv layers in each DRDB. The empirical ablations studies in Sect. 4.2.1 show the effectiveness of the DRDBs.

Training Loss
Our training loss is defined as the weighted sum of the pixel-wise loss, perceptual loss, and TV loss.

Pixel-Wise Loss  As described in Sect. 3.3, the proposed DAHDRNet predicts the HDR image H in the HDR domain. Since the HDR images are usually displayed after tonemapping, training the network on the tonemapped images is more effective than training directly in the HDR domain (Kalantari and Ramamoorthi 2017). Given an HDR image H in HDR domain, we compress the range of the image using 𝜇-law:

𝒯(𝐻)=log(1+𝜇𝐻)log(1+𝜇),
(9)
where 𝜇 is a parameter defining the amount of compression and 𝒯(𝐻) denotes the tonemapped image. In this work, we always keep H in the range [0, 1] and set 𝜇=5000. The tonemapper in Eq. 9 is differentiable, which is very suitable for training the network.

In our method, we train the network by minimizing ℓ1-norm based distance between the tonemapped estimated and the ground truth HDR images. Our loss function is defined as:

ℒ𝑝𝑖𝑥𝑒𝑙=‖𝒯(𝐻ˆ)−𝒯(𝐻)‖1,
(10)
where 𝐻ˆ represents the ground truth. We also tested the ℓ2 loss used in previous work (Kalantari and Ramamoorthi 2017; Wu et al. 2018) and noticed that ℓ1 loss is more powerful for preserving details (See Sect. 4.2.2), which is consistent with the observation in Zhao et al. (2017b).

Perceptual Loss  Instead of relying on pixel-wise losses, we also employ a loss function similar to perceptual similarity (Ledig et al. 2017). We define the perceptual loss based on the ReLU activation layers of the pre-trained 19 layer VGG network. Let 𝜙𝑖,𝑗 denote the features obtained by the j-th convolution before the i-th max-pooling layer within the VGG19 network. The perceptual loss is defined as the euclidean distance between the feature representations of the predicted HDR image H and the ground truth 𝐻ˆ.

ℒ𝑝𝑒𝑟𝑐𝑒𝑝=‖𝜙𝑖,𝑗(𝐻ˆ)−𝜙𝑖,𝑗(𝐻)‖2,
(11)
TV Loss  Total variation (TV) loss has been used for learning the natural image characteristics via modeling the relationship between neighbor image pixels. Minimizing the TV loss encourages the image to be piece-wise smooth. The estimated HDR image with ghosting often violates this rule. This observation motivates us to use total variation of HDR image as a penalty to help improve reconstruction results. We define the TV loss function as:

ℒ𝑇𝑉=‖∇𝑥𝐻‖2+‖∇𝑦𝐻‖2,
(12)
where ∇𝑥 and ∇𝑦 represent the spatial derivative along the x-axis and y-axis respectively.

The final loss function:

ℒ=ℒ𝑝𝑖𝑥𝑒𝑙+𝜆ℒ𝑝𝑒𝑟𝑐𝑒𝑝+𝜇ℒ𝑇𝑉,
(13)
where 𝜆 and 𝜇 are weighting coefficients.

Implementation Details
In our implementation, we apply 64 3×3 features in the Conv layers, which are followed by ReLU activations, if not specified otherwise. We set the stride size for all Conv layers as 1 and keep the feature map size using zero padding. We define the output layer to produce 3-channel images. The growth rate of all DRDBs is 32. We do not use any batch normalization layers in order to retain the range flexibility of features. To facilitate the training process, we use global residual learning in the merging network and attention network (refer to the red lines in Fig. 2). The last Conv layer in each DRDB applies 1×1 convolution to compress the feature maps. From quantitative evaluations (See Table 1), we find that the dual-attention module with 3 recurrent dual-attention modules (𝑛=3) achieves the best performance. Thus we set n to 3 as the default parameter of the proposed DAHDRNet.

Table 1 Quantitative comparisons of different models
Full size table
For training, we use Adam optimizer (Kingma and Adam 2014) and set the batch size and learning rate as 8 and 1×10−5, receptively. Given training images, we randomly crop the 256×256 patches for training. The 𝜆 and 𝜇 are set as 10−4 and 10−2. All weights of the network are initialized using Xavier method (Glorot and Bengio 2010). We implement our model using PyTorch (Paszke et al. 2017), which takes takes 0.35s to process a 1500×1000 image with an NVIDIA GeForce 1080 Ti GPU.

Experiments
Experimental Settings
Training Data  We train the DAHDRNet on the HDR dataset (Kalantari and Ramamoorthi 2017) which includes 74 samples for training and 15 samples for testing. For each sample, three different LDR images are captured with exposure biases of {−2,0,+2} or {−3,0,+3}. Transformations on the cropped patches are applied as data augmentation to alleviate overfitting.

Testing Data  We test the proposed DAHDRNet on the Kalantari’s dataset (Kalantari and Ramamoorthi 2017), and the datasets without ground truth, such as Sen’s (2012), and Tursun’s (2016) datasets.

Fig. 6
figure 6
Visual results of DAHDRNet and its baseline variants

Full size image
Evaluation Metrics We conduct evaluations with four metrics as the following. We compute the PSNR values for images after tonemapping using 𝜇-law (PSNR-𝜇), Matlab function tonemap (PSNR-M), and linear (PSNR-L) domains. We also conduct a quantitative evaluation by computing the HDR-VDP-2 (Mantiuk et al. 2011). HDR-VDP-2 value measures the quality of the reconstructed image with respect to the ground truth HDR image, expressed as a mean-opinion-score.

Ablation Studies
Study on the Model Architecture
We investigate the architecture of DAHDRNet and validate the importance of different individual components in the whole DAHDRNet. We achieve this ablation study by comparing the proposed DAHDRNet and the following variants of DAHDRNet:

Baseline (i.e., DAHDRNet w/o dual attention, dilation and dense connection). This baseline is a merging network based on the residual block (RB).

Model1 More RBs are used to approach the model compressibility of the proposed network.

Model2 (i.e., DAHDRNet w/o dual attention and dilation). This variant of DAHDRNet does not contain the dual-attention operation and dilated convolution layers.

Model3 (i.e., DAHDRNet w/o dual attention). We remove the attention module in this variant, in which the feature maps 𝑍𝑖’s are directly stacked and fed to the merging network.

Model4 (i.e., DAHDRNet w/o channel attention). We do not use channel attention in this variant of DAHDRNet, which is our preliminary work AHDR with only spatial attention.

Model5 (i.e., DAHDRNet w/o spatial attention). Spatial attention strategy is removed from this variant of DAHDRNet. There is only channel attention in Model5.

DAHDRNet The full model of the DAHDRNet.

Fig. 7
figure 7
Example image patches and the corresponding attention maps. From left to right: the reference image, one non-reference image, and the attention map applied on the non-reference image

Full size image
Effect of Attention Module The attention module is a very effective mechanism for HDR image de-ghosting tasks. We will analyze spatial, channel, and dual-attention, respectively.

Spatial attention. As shown in Fig. 6, compared with Model3, Model4 can alleviate the ghosting artifacts due to the spatial attention module. A similar result can be observed with Model5 and the proposed method. Although Model3 can remove ghosting artifacts partially, it tends to generate artifacts in saturated regions (the top patch of Fig. 6). The proposed method with a spatial attention module can eliminate ghosting artifacts while retaining the background information (See Fig. 7). As shown by the quantitative results in Table 1, 𝐷𝐴𝐵1 and Model4 acquire a better improvement than the Model5 and Model3.

Since the HDR imaging process centers on the reference image, the attention maps are predicted and applied according to the reference. As shown in Fig. 7, the attention maps can suppress the misaligned (See the middle row of each sample) and over/under-saturated regions (See the bottom row of each sample) in the non-reference images, which avoids the harmful features getting into the merging process and thus alleviates the ghosts from the source. When some regions in the reference are saturated or noisy, the attention maps can also highlight useful features in the non-reference images.

Channel attention. We further display the effect of channel attention based on the observations and discussions above. Figure 6 shows the visual comparison. When we compare the results of Model3 and Model5, we find that networks with channel attention obtain better visual effects than those without channel attention. The results of Model3 without channel attention have the ghosting artifacts (such as arms, hands, and shoulder in Fig. 6d). The results are improved when we add the channel attention units, as shown in Fig. 6f, which proves that the channel attention can discard the motion features and alleviate the ghosting artifacts. A similar result can be observed with Model4 and the proposed method. In Table 1, when the channel attention unit is employed, the PSNR values are increased. For example, when the channel attention unit is applied in Model5, the PSNR-M increases to 32.04dB from 31.42dB comparing to Model3 without the unit. This indicates that the channel attention strategy is applicable to remove ghosting for HDR imaging.

Fig. 8
figure 8
Attention maps of recurrent dual-attention module. The first attention map obtains the majority information of input, while the succeeding attention maps have strong response on the difficult areas. The 3-th attention map only captures trivial information, which means useful information is extracted progressively by selecting features in a recurrent way

Full size image
Dual-attention module. As shown in Fig. 6, the model with the dual-attention module performs better than the models with only spatial or channel attention units, i.e., Model4 and Model5, respectively. As shown in Table 1 and Fig. 6, since the method with spatial attention (Model4) has achieved high-quality results and a significant improvement compared to other methods, the improvements from the dual-attention (with channel attention) seem not very visually significant. But we still can see obvious improvement after using the channel attention in the dual-attention method in Table 1. This comparison demonstrates that spatial and channel attentions are complementary and facilitate to generate better results.

Effect of Recurrent Dual-Attention Module The proposed recurrent dual-attention module progressively highlights the useful features and discards the harmful features from inputs for HDR imaging. We show the attention map of each dual-attention module in Fig. 8, from which we find that the first attention map obtains the majority information of input, while the succeeding attention maps have strong responses on the problematic areas. The third attention map only captures trivial information, which means useful information is extracted progressively by selecting features in a recurrent way. The quantitative results are shown in Table 1, compared with using only one dual-attention module in 𝐷𝐴𝐵1, a recurrent network with dual-attention in the proposed method achieves better performance. Here, we evaluate the dual-attention module in terms of the number of recurrent blocks n. As shown in Table 1, the dual-attention module with 3 recurrent blocks performs best, with improvements of 0.18 dB over the 𝐷𝐴𝐵1 on PSNR-𝜇. In addition, we also try to use more than 3 recurrent blocks, but the performance is decreased.

Effect of Dilated Residual Dense Blocks Compared with Model3, the Baseline’s results contain visible ghosts (See Fig. 6a, d). Even the results of Model1 that has more RBs cannot remove ghosting artifacts. Hence, increasing the depth of the network is not a practical approach to enhance HDR image quality. On the other hand, the Model3 with the same network depth can capture more contents and alleviate ghosts. The performance of Model3 in Table 1 is better than Baseline and Model2.

Effect of Dilated Convolution To demonstrate the capability of dilated convolution, we compare Model2 and Model3. As displayed in Fig. 6c and d, the results of Model3 suffer from less ghosting artifacts compared with Model2. The results show that a larger receptive field is helpful to suppress the ghosting artifacts and hallucinate the missing details. The quantitative comparisons in Table 1 show that the models with dilated convolution can obtain higher values on PSNR metrics.

Fig. 9
figure 9
Visual comparison for different loss function and parameters

Full size image
Table 2 Quantitative comparisons of different loss functions and parameters setting
Full size table
Table 3 Performance of loss functions with different perceptual losses
Full size table
Fig. 10
figure 10
The reconstruction results of different perceptual losses

Full size image
Fig. 11
figure 11
Visual comparisons on the testing data from Kalantari and Ramamoorthi (2017). The top half part shows the input LDR images, LDR image patches, and the HDR image produced by the proposed method. We compare the zoomed-in local areas of the HDR images estimated by our methods and the compared methods. The propose network can produce a high-quality HDR image, especially saturated and object motions regions (Color figure online)

Full size image
Fig. 12
figure 12
Results for Kalantari et al.’s (2017) dataset using different methods (Color figure online)

Full size image
Table 4 Quantitative comparison of proposed network with several state-of-the-art methods
Full size table
Study on Training Loss Function
In this experiment, we compare the performances of our method with different loss functions. Specifically, we investigate the pixel-wise loss, different parameters of TV loss, and perceptual loss.

Pixel-Wise Loss It is well known that the ℓ1 loss is more powerful for preserving details, but easier affected by noise (Zhao et al. 2017b), ℓ2 loss may cause the smooth result. Comparison results of our model trained with pixel-wise loss are shown in Fig. 9 (first two rows), from which we observe that the ℓ1 loss generates more shape details than ℓ2 loss, especially edge areas. Quantitative comparisons of the results are reported in Table 3. Compared with ℓ2 loss, ℓ1 loss yields the higher PSNR as it products more details, the PSNR-𝜇 promotes about 1dB, and PSNR-M promotes large than 1db. We thus train our model using ℓ1 loss.

TV Loss The TV loss encourages spatial smoothness in the generated images. TV loss is effective for simultaneously preserving edges whilst smoothing away noise in flat regions, even at low signal-to-noise ratios. Visual comparisons are in Fig. 9. We found that the results without TV loss have obvious blurring artifacts. And TV loss can help to generate better visual results. The last four rows in Table 3 show quantitative comparisons of TV Loss with different parameters. When compared with loss function without TV loss, our method (𝜆=10−2) performs the best on all the testing data.

Perceptual Loss We investigate the effect of different content loss choices in the perceptual loss for the loss function. Quantitative results are summarized in Tables 2 and 3 and visual examples are provided in Fig. 10. We evaluate the performance of the perceptual loss function with different feature maps obtained by the VGG19 network (i.e., 𝜙𝑖,𝑗). In Fig. 10, we observe a trend that using the middle-level VGG feature map 𝜙3,2 yields better details compared to low/high-level feature maps. We consider the pixel-wise loss has sufficient capacity to recover the semantic information but neglects the details, especially on saturation regions. Combined with the perception loss 𝜙3,2, the total loss function solutions with the highest metrics values obtain convincing visual effects.

Comparison with State-of-the-Art Methods
We evaluate the proposed method and compare it with previous state-of-the-art methods on a variety of datasets. Specifically, we compare the proposed method with two patch-based methods (Sen et al. 2012; Hu et al. 2013), the method based on motion detection (Oh et al. 2015), the flow-based approach with DNN merger (Kalantari and Ramamoorthi 2017; Prabhakar et al. 2020) and the DNN method without optical flow (Wu et al. 2018; Yan et al. 2020). In addition, we compare with single-frame HDR imaging method (Eilertsen et al. 2017). The same training dataset and setting are used for deep learning methods. To evaluate the robustness of the proposed method, we choose several cases consisting of indoor, outdoor, saturation, and large foreground motions. In each case, we select the middle image of the sequence as the reference. Note that all the displayed HDR images are tonemapped using Matlab function tonemap with the same parameters.

Evaluation on Kalantari and Ramamoorthi (2017) Dataset
We compare our method with several state-of-the-art methods on the testing data of Kalantari and Ramamoorthi (2017) (Figs. 11, 12), which contains some challenging samples with saturated background and foreground motions. The middle LDR image highlighted with blue color is the reference image. All the approaches can generate perfect results in the areas without foreground motion or saturation. Specifically, the learning-based methods generally recover the details better. For this reason, most approaches often show the results of static scenes with small motion, which are unlike that of Fig. 11 with saturation and occlusion. As shown the result in Fig. 11, the patch-based methods (Sen et al. 2012; Hu et al. 2013) cannot find corresponding patches and produce artifacts. The method of Oh et al. (2015) cannot recover the details in the saturated areas. Since the single image method HDRCNN (Eilertsen et al. 2017) only uses the single reference image, they can avoid the ghosting artifacts but cannot reconstruct the sharp results and produce color distortion. The method of Kalantari and Ramamoorthi (2017) products artifacts (See the red block in Fig. 11) with two main reasons, i.e., misalignment of optical flow and the limitation of their merging process. Wu et al.’s method (Wu et al. 2018) generates over smooth results, and cannot completely remove the ghosting artifacts (See the red block in Figs. 11, 12). We deem the reason is that it simply stacks the feature maps to fusion. Although Yan et al.’s method (Yan et al. 2020) alleviates ghosting artifacts, it suffers from the over-saturated regions. Prabhakar et al. (2020) combine the detection-based method and optical-flow method. But there are still flaws in the results. Our method uses the attention maps (Fig. 7) to select the useful regions and remove harmful components. It suppresses the ghosting artifacts and recovers the occluded or saturated details (See the blue block in Fig. 12).

We conduct quantitative evaluations and comparisons based on ground truth. Results are reported in Table 4. All the values are averaged over 15 testing images, and larger values mean higher quality. Note that the testing images are not overlapped with the training set and captured from different scenes. The proposed DAHDRNet method produces better numerical performance than other methods on all metrics. The result has a significant improvement in terms of PSNR-𝜇 and PSNR-M, showing the effectiveness of our model. Even compared with previous methods which use optical flow (Kalantari and Ramamoorthi 2017) or global alignment (Wu et al. 2018), the proposed method can produce better numerical results. Compared with the combination method (Prabhakar et al. 2020), our method can produce better results.

Evaluation on the Datasets w/o Ground Truth
We compare the proposed DAHDRNet with other methods on Sen et al.’s (2012), and Tursun et al.’s (2016) datasets without ground truth. We select three images from each sample as inputs, the results are shown in Figs. 13 and 14. For better visual comparison, we also show zoomed-in images of the two specific regions in the HDR imaging results. The patch-based Sen et al.’s (2012) and Hu et al.’s (2013) methods produce artifacts in complex motion regions (zoomed-in patches in Fig. 14). These methods cannot find corresponding patches in the non-reference LDR images. As shown in Figs. 13 and 14, the single frame method HDRCNN (Eilertsen et al. 2017) is prone to generate serious noise and color distortion in the under-exposed regions. The method of Kalantari and Ramamoorthi (2017) introduces artifacts (Fig. 14) due to the alignment error. The results of Wu et al.’s method (Wu et al. 2018) and Yan et al.’s method (Yan et al. 2020) lack details and suffer from obvious over-smoothness (Figs. 13, 14). The results of Prabhakar et al. (2020) have indistinct texture or color distortion. In comparison, our proposed DAHDRNet produces appealing results where the geometry distortion, color artifacts, and noise are significantly reduced compared with existing methods.

Validation on Night-Scene Image
Figure 15 shows the result of the proposed method on the night scene, from which the produced image displays approving visual effects with moderate luminance. As shown in the figure, our method can effectively recover the high dynamic range details in the scene while containing a relatively higher level of noise from the low-exposure input image. Although the proposed method is not specifically trained on the night-scene images, many sub-regions in the training samples from Kalantari and Ramamoorthi (2017) have included the under-exposed regions suffering from different levels of noise. Thus the proposed method trained on this dataset can handle the night scene well. We also think the higher level of noise causes a slight impact on the features of the network not trained on the specific night data, leading to a slight performance decreasing.

Fig. 13
figure 13
Visualization results for two random samples of Sen et al.’s dataset (Sen et al. 2012) without ground truth. The DAHDRNet obtains results with sharper details and less artifacts

Full size image
Fig. 14
figure 14
Visual comparisons on the Tursun et al.’s (2016) dataset without ground truth. The DAHDRNet focuses on learning the edges and details

Full size image
Table 5 Average running time for different methods
Full size table
Running Time
Table 5 shows the comparisons of running time, in which the results are the average of 15 sequences with resolution size 1500×1000. In Table 5, since the non-deep learning methods (Sen et al. 2012; Hu et al. 2013; Oh et al. 2015) need matching process, which takes much time to process a sequence with three LDR images. Among the deep learning-based methods, Kalantari et al.’s network (Kalantari and Ramamoorthi 2017) includes the LDR images alignment stage and image fusion stage. Thus the total processing time takes 29.14s, where image alignment stage (optical flow) consumes most of the time. The method of Wu et al. (2018) and Yan et al. (2020) use encoder–decoder architecture, but Yan et al. (2020) employ non-local module which will consume time. The method of Prabhakar et al. (2020) is a sequential structure, which spends 0.37s to process three LDR images. The running time of our method is on-par with Wu et al.’s method. However, we should claim that the reported running times of Wu et al. (2018), and Prabhakar et al. (2020) exclude the time of aligning images. Considering the pre-processing time, the running times of Kalantari and Ramamoorthi (2017), Wu et al. (2018), and Prabhakar et al. (2020) will become longer. In general, since our method does not need pre-processing, it generates better results with less time.

Fig. 15
figure 15
Validation of the proposed method on the night-scene images

Full size image
Limitations
To remove ghosting artifacts, we use a recurrent dual-attention module to extract attention maps. The recurrent operations spend more computation time than a single forward module in processing. We consider exploring more efficient model structures with similar or better performances. The other limitation of our approach is that it is trained to a fixed number (i.e., three) of images as input, which is the common limitation of similar works. To tackle this limitation, we will explore dynamic fusion strategies to handle more diverse input formats.

Conclusion
The HDR imaging methods try to generate an HDR image from multiple exposures but are limited in its application due to ghosting and saturating artifacts. In this paper, we propose an effective method for removing ghosting based on the dual-attention technique. Built upon recurrent dual-attention modules, which contain both spatial attention and channel attention, the carefully designed DAHDRNet can suppress undesired components caused by misalignments and saturation and enhance desirable fine details in the non-reference images. Benefiting from the DRDB, the proposed network makes full use of the hierarchical features and increases the receptive field for hallucinating the missing details. Most notably, the proposed method can generate high-quality HDR images even in the presence of large image motion and saturation. It thus offers the prospect of more extensive applications of HDR imaging. We will explore more efficient and effective architectures for future work to better exploit global and local visual information for HDR imaging.