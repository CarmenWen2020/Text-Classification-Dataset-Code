Optical marker-based motion capture (MoCap) remains the predominant way to acquire high-fidelity articulated body motions. We introduce DeMoCap, the first data-driven approach for end-to-end marker-based MoCap, using only a sparse setup of spatio-temporally aligned, consumer-grade infrared-depth cameras. Trading off some of their typical features, our approach is the sole robust option for far lower-cost marker-based MoCap than high-end solutions. We introduce an end-to-end differentiable markers-to-pose model to solve a set of challenges such as under-constrained position estimates, noisy input data and spatial configuration invariance. We simultaneously handle depth and marker detection noise, label and localize the markers, and estimate the 3D pose by introducing a novel spatial 3D coordinate regression technique under a multi-view rendering and supervision concept. DeMoCap is driven by a special dataset captured with 4 spatio-temporally aligned low-cost Intel RealSense D415 sensors and a 24 MXT40S camera professional MoCap system, used as input and ground truth, respectively.

Access provided by University of Auckland Library

Introduction
Extensive research efforts have been devoted to the development of motion capture (MoCap), one of the de facto standards for human-centric interactive media capture and production. Nowadays, optical MoCap solutions, especially the marker-based ones, are considered essential to several applications and industry sectors such as film making and VFX, sports, health, gaming and immersive realities (XR). MoCap enables online and offline high-fidelity capturing and digitization of body, hand and facial movements derived from the performances of real people and beyond. This enables its use in various applications such as 3D character animation, computer-human interaction, robotic units control, physical exercise monitoring, and more.

Despite the appearance of several commercial and academic motion capture solutions, marker-based MoCap still remains the gold-standard in the field. That is due to its extremely high accuracy and frequency, as well as the production-ready maturity that ensures high quality outcomes in a short amount of time.

Nevertheless, the marker-based MoCap production process is not flawless, it suffers from well-known drawbacks. Raw optical motion capture data are often erroneous, due to marker occlusions or mislabeling from marker swapping during tracking, with high frequency noise or jitter and requiring time-consuming post-processing by hand. Beyond data cleaning, articulated body part fitting to marker subsets and skeleton retargeting for local joint transformation solving are further required, undoubtedly making it a laborious and time-consuming process. On top of that, the complexity and costs of marker-based MoCap systems with numerous infrared specialized cameras are high, making them inaccessible to the wider interested audience.

These complications attract the interest of the research community to investigate and propose novel alternatives. On the one hand, computer graphics researchers intensify their efforts on models that resolve or soften these issues (Holden (2018); Han et al. (2018); Bascones (2019); Perepichka et al. (2019)), while computer vision labs concentrate their efforts on ground-breaking markerless methods, posing MoCap mostly as a 3D pose estimation task (Iskakov et al. (2019); Qiu et al. (2019); Tu et al. (2020)).

Nevertheless, existing marker-based solutions do not satisfy the need for flexible and low-cost options, while recent markerless models following the deep learning paradigm, though effective and more flexible, cannot reach equal robustness and precision levels in the absence of strong and deterministic priors such as body-worn markers. Focusing on this gap, our research is driven by three main factors: (i) the accuracy and precision of marker-based solutions that obtain the articulated body movements with the aid of body-attached markers in a highly accurate and deterministic way, (ii) the remarkable ability of deep models to solve vision-based problems and (iii) the recent developments on consumer-grade and low-cost depth-sensing cameras. We inherit the best of their characteristics and blend them in an efficient way to present a “hybrid” lightweight motion capture approach.

In this paper, we propose DeMoCap, targeting low-cost marker-based motion capture by combining traditional marker-based MoCap and deep neural networks applied on visual data captured with low-cost depth-sensors. To the best of our knowledge, DeMoCap, though not equal to professional high-end MoCap systems on several aspects such as the high frequency or the size of the capturing space, is the first deep model that enables the use of far lower-cost equipment and professional retro-reflective marker configuration for robust motion capture (Fig. 1).

We employ a sparse, spatio-temporally aligned multi-view setup of consumer-grade and low-cost depth-sensing cameras to track a dense configuration of spherical retro-reflective markers. To address the marker-based MoCap challenges, i.e. marker denoising, labeling, tracking and joint transformation solving and retargeting, and the limitations of low-cost depth sensor setup, i.e. viewpoint sparsity, depth noise, under constraint data and infrared image blurriness for accurate blob detection, we propose an end-to-end, fully differentiable data-driven model to directly regress the 3D pose. This way, we pose the problem of MoCap as a markers-to-pose regression task.

Even though our input is three dimensional, we avoid the use of 3D convolutions, as we target lightweight, real-time and close to real-time applications. We introduce a novel spatial 3D regression module on top of latent heatmaps predicted by a 2D fully convolutional neural network (FCN) to regress the markers and joints 3D coordinates, in a fully differentiable manner. We experiment with various multi-stage FCN architectures by building super-stages, i.e. grouping the former and the latter stages to regress the marker and joint 3D coordinates, respectively, staging a smooth representation transition from markers to 3D pose (markers-to-pose). Following this approach, we drive the network to learn the spatial and hierarchical relation between the markers and the underlying pose .

Fig. 1
figure 1
DeMoCap stands as the first marker-based MoCap solution enabling the use of only a sparse set of consumer-grade depth sensors for far lower costs, and higher portability and flexibility, than commercial high-end solutions, trading off, however, some of their typical features

Full size image
For feeding our network, we apply a volumetric scale normalization to uniformly distribute the marker cloud in a cuboid 3D space for higher data spatial invariance and sparsity. Then, we adopt a multi-view rendering technique to render the markers from opposing orthographic cameras whose principal axis passes through the center of the sparse marker cloud. Notably, we preserve the 3D information of the markers by splatting their relative depth on multiple views, sticking to two opposing views for DeMoCap, creating “sparse” depth maps of the markers to feed our model. From that point, we approach our task as a 3D keypoint regression problem from dual-view 2D depth maps. The model is driven to assimilate the articulated relation between markers and joints, sequentially regressing their 3D coordinates in a forward pass per infrared-depth multi-view frame, without any body structure prior or explicit association between body parts and marker subsets. Summarizing, our contributions are:

To the best of our knowledge, DeMoCap constitutes the first data-driven approach that employs efficient fully convolutional neural networks to simultaneously regress optical markers and 3D pose from sparse 3D point sets, captured with the use of a low-cost multi-view depth-sensor setup .

A scale and translation invariant representation in a normalized 3D space is introduced to train our network. Learning upon it, the model overcomes bias on our relatively limited training data and generalizes well.

We stage a smooth representation transition from markers to 3D pose. Our model is driven to learn the underlying structural relation between human body and marker configuration placement, decoding marker and bone associations from sparse and noisy 3D marker point clouds, resulting in accurate pose estimation.

We pose 3D keypoint estimation as a joint 2D localization and regression objective within our normalized 3D space to embed the z-dimension indirectly with the introduction of a new fully differentiable module for 3D regression.

We make our special dataset publicly available. Our dataset contains inter- and intra-system spatio-temporally aligned infrared-depth and motion capture data. On top of that, the former have been captured with hardware synchronization for precise temporal consistency between the multiple views.

The rest of the paper is structured as follows. Section 2 gives a brief overview of related data-driven works on (a) marker-based MoCap automation and refinement, (b) multi-view markerless vision-based 3D pose estimation and (c) keypoint localization techniques applicable on pose estimation. Section 3 presents the dataset creation and processing to familiarize the reader with the nature of our challenge and the way we approach it. In Sect. 4, our methodical approach is discussed, presenting and explaining in depth the rationale behind its contributions. In Sect. 5, we present quantitative and qualitative experimental results along with ablation outcomes to justify our contributions. In Sect. 6, we discuss the pros and cons of DeMoCap in comparison with existing marker-based motion capture solutions and recent markerless pose estimation approaches. Sect. 7 concludes and presents potential avenues for future work.

Related work
Our approach requires domain knowledge from marker-based practices and recent data-driven 3D pose estimation techniques. To this end, in Sect. 2.1, we first review approaches recently appeared in the literature that focus on the processing of marker-based optical motion capture data (Bascones (2019); Loper et al. (2014); Holden (2018); Han et al. (2018); Perepichka et al. (2019)). These works highlight traditional MoCap challenges such as marker labeling and denoising, or direct joint transformation solving from optical data in an efficient and automatic fashion by applying machine learning techniques. Another challenge addressed by recent works, such as the ones proposed by Loper et al. (2014); Mahmood et al. (2019), is related to body shape deformation issues. Similar to most prior works, we do not explicitly account for this, but instead, focus on simultaneous overcoming of the aforementioned MoCap challenges.

On the other hand, in spite of the use of reflective markers in the present work, we pose our solution mostly as a 3D pose estimation task from noisy and sparse 3D data. Thus, in Sect. 2.2, we discuss recent data-driven approaches that estimate 3D pose using FCNs for keypoint 3D coordinate regression, highlighting the pros and cons of each solution. Finally, in Sect. 2.3, we give a short overview of keypoint localization approaches to correlate them with the 3D coordinate regression technique we introduce.

Marker-Based Optical Motion Capture
The classic marker-based optical solutions have been undoubtedly the gold-standard in motion capture for decades. Nevertheless, the existence of drawbacks such as the need for post-processing for data cleaning as well as the expensive hardware and complexity of their setups, are considered a challenge and attract the interest of the research community. We discuss recent and novel works that apply machine learning techniques on marker-based optical data for marker denoising and joint transformation solving.

Bascones (2019) tackles automatic marker labeling as a machine learning classification problem, to train a set of weak classifiers in an ensemble of partial solvers. The result is used to feed an online algorithm providing efficient and lightweight marker labeling. Alexanderson et al. (2017) present a robust online method for identification and tracking of passive motion capture markers attached to non-rigid structures. The method is especially suited for large capture volumes and sparse marker sets. By using multiple assignment hypotheses and soft decisions, it can robustly recover from challenging poses with several simultaneous occlusions and false observations (ghost markers). Holden (2018) proposes a fast method for robust joint transformations solving of optical motion capture data by using machine learning denoising techniques. This data-driven approach, being robust to erroneous marker 3D positions, replaces the solving part of the motion capture pipeline, removing the need for manual data cleaning. However, the method is limited to be used with motion capture data from commercial solutions, while not applicable in real-time use cases. Recently, Perepichka et al. (2019) introduced a method that robustly detects and repairs marker trajectories by replacing erroneous segments with synthetically generated ones producing kinematically correct paths. Using the joint transformation solver proposed by Holden (2018), an initial kinematic motion is constructed, and using it as reference, erroneous trajectories are detected and filled by transferring the paths from the kinematic solver in a shape preserving way.

Han et al. (2018) introduce an online optical marker labeling model for hand tracking, framing the labeling problem as a direct keypoint regression demonstrating it to sequences with occluded and ghost markers. The model is accurate and fast, since trained in a great amount of synthetic data, albeit not evaluated on highly noisy and challenging marker sets. The model regresses the marker 3D coordinates with the use of fully connected layers which are prone to overfitting when the pool of data is limited, hampering the generalization ability of the overall network.

Chatzitofis et al. (2019) firstly introduced the joint use of deep neural networks and multi-view depth-sensing for marker-based motion capture. Nevertheless, the method is not based on an end-to-end data-driven model. The model detects and labels the markers, however the body pose is estimated by applying forward kinematics to an articulated human body prior. Moreover, instead of spherical commercial markers that allow for high-fidelity tracking, a sparse and coarse set of custom retro-reflective straps and patches was attached on the subjects’ bodies. Despite its valid concept, the method has limited degrees-of-freedom due to the low number of markers and its custom marker placement, while the marker labeling takes place separately for each camera, most likely making the model biased to the camera poses and their intrinsic parameters.

Even though classic marker-based MoCap constitutes a specialized solution for professional motion capture, the research works discussed in this section try to overcome well-known issues that MoCap suffers from, i.e. marker labeling, ghost marker denoising, occluded marker recovery, marker swapping and joint transformation solving. In the present work, we overcome these issues in an end-to-end manner by directly regressing the marker and pose coordinates, staging in our network a smooth representation transition between them.

Markerless 3D Pose Estimation
On top of these challenges, the use of markers increases the complexity of the motion capture setup, while the body joint solving from markers is a non-trivial task. During the last decade, numerous research labs intensively work on simple, markerless and more flexible approaches using low-cost resources (Sigal et al. (2012)). Most recent methods focus on monocular vision, mostly using color (Mehta et al. (2017); Pavllo et al. (2018); Mehta et al. (2019); Cheng et al. (2019); Guler and Kokkinos (2019); Rüegg et al. (2020)), and some using depth (Haque et al. (2016); Park et al. (2017); Martínez-González et al. (2018b, 2018a)). Fewer but not limited methods approach 3D pose estimation from multi-view color streams (Burenius et al. (2013); Elhayek et al. (2015); Rhodin et al. (2018); Qiu et al. (2019); Iskakov et al. (2019); Tu et al. (2020)), while pose estimation from multi-view depth maps is relatively unexplored (Bekhtaoui et al. (2020)). More relevant to our approach are recent 3D pose estimation works on spatio-temporally aligned multi-view visual streams.

Iskakov et al. (2019) present two variations of a learnable triangulation-based technique, an algebraic and a volumetric one, to estimate 3D pose jointly from multiple 2D color views. The former is based on soft triangulation with learnable camera-joint confidence weights, while the latter is based on dense geometric aggregation of 2D heatmap predictions from multiple viewpoints. The aggregated volume is then refined via 3D convolutions to produce 3D heatmaps that allow modelling a human pose prior. The method showcases satisfying results, as presented in the original work and shown in our experiments in Sect. 5.3, however it is slow and requires multi-view color input, a domain sensitive input (Buhrmester et al. (2019)).

Qiu et al. (2019) propose another approach under a similar concept, i.e. estimating 2D heatmaps in multi-view images and recovering 3D poses from multi-view 2D predictions. At first, a convolutional neural network (CNN) jointly estimates 2D poses through a cross-view fusion scheme which allows for refined 2D pose estimation. Then, applying a recursive pictorial structure model (RPSM), the 3D pose is recovered from the multi-view 2D poses and gradually improves, since RPSM recursively discretizes the volume around each joint previously predicted 3D location into a finer-grained grid. The inference performance limits the online operation of the method.

Tu et al. (2020) recently presented VoxelPose, a multi-view and multi-person data-driven 3D pose estimation approach. Contrary to the aforementioned multi-view methods whose cross-view correspondence is based on weak 2D pose estimates, VoxelPose directly operates in the 3D space. Features from the camera views are aggregated in the 3D space and fed into a cuboid proposal network to localize multiple subjects in the capturing space by predicting a number of 3D cuboid proposals from the 3D feature volume. Then, a separate finer-grained feature volume, centered at each proposal, is created and fed into a 3D pose regression network. Despite the frequent occlusions between multiple people at the same scene, the approach is accurate and robust.

Other approaches are based on parametric body models with skeleton hierarchy that allow the expression of the body pose and motion. Joo et al. (2018) build upon generative body deformation models to fit to data from multiple viewpoints. Leveraging face, body and hand landmarks with the use of 2D detectors from multiple views, 3D keypoints are obtained and used to train the parametric models, also allowing for capturing of additional variations of hair and clothing. To that end, full body (face, hands, body) motion capture is achieved with the use of 3D deformable models. Potential errors of single-view 2D keypoint detection can add bias to the models.

Zhang et al. (2020b) propose a new multi-view and multi-person motion capture approach. Based on confidence map (heatmaps) and part-affinity fields (PAFs) predicted with the use of OpenPose by Cao et al. (2017), the proposed method unifies per-view parsing, cross-view matching and temporal tracking with the introduction of a 4D association graph. The 4D association graph is efficiently solved with the introduction of 4D limb bundle parsing based on heuristic searching and a bundle Kruskal’s algorithm.

Contrary to our approach, most of the aforementioned methods, though effective, are not lightweight, cannot perform even close to real-time, require multi-view color images (a domain sensitive input), while most of them are biased to the errors of the initially required 2D detections.

Fig. 2
figure 2
The capturing setup with 24 VICON (1984) MXT40S cameras and a low-cost multi-view depth sensor system equipped with 4 Intel RealSense D415 stereo-based depth sensing devices. Intense marker reflections are provoked to the infrared streams by emitting infrared light to the retro-reflective markers attached on the subjects’ body. To limit the image blurriness, we reduced the exposure time of the sensors which, consequently, reduced the lightness of the image leading to more distinguishable marker reflections in comparison to the default settings. Infrared-depth image pairs are shown on the bottom of the figure, while at the left side, the ground truth markers and pose are projected on one of the infrared views to depict the spatio-temporal alignment between the motion capture and the infrared-depth systems

Full size image
Keypoint Localization
We approach MoCap as a cooperative marker and joint 3D coordinate regression task from sparse depth images and, thus, we offer an overview of the state-of-the-art approaches for keypoint localization. Nowadays, many vision-based problems are posed as 2D/3D keypoint localization tasks, where deep 2D CNNs and FCNs have been proven effective. In the recent literature, the state-of-the-art methods for keypoint localization fall into three main categories, i.e. direct regression, dense prediction, and spatial regression.

Direct regression methods, used in several tasks, such as pose estimation in DeepPose by Toshev and Szegedy (2014) and optical marker labeling (Han et al. (2018)), bypass the spatial nature of images due to its fixed-size representation. Instead, these methods implicitly learn to directly regress the 2D/3D positions based on the expressive power of the models. Nevertheless, direct regression can be supervised with distance-based losses, which is the direct objective of keypoint localization.

Dense, heatmap-based prediction methods employ FCNs to predict confidence scores for each input pixel and are supervised during training with heatmaps reconstructed in most cases via 2D Gaussian distributions with fixed-variance. Such techniques have been employed in well-known pose estimation works (Cao et al. (2017); Wei et al. (2016)), presenting higher image translation invariance than direct regression due to their spatial aspect. The keypoints in these methods are localized by calculating the ArgMax or alternative heuristic approaches (Tompson et al. (2014)) on the predicted dense heatmap. The main drawback of these methods is the use of intermediate, structural loss functions that train the network to predict pixel confidence scores, thus, the supervision during training is not aligned to the direct objective of the method, as in direct regression.

Spatial regression methods have proved the most effective in various vision-based tasks, combining the strengths of direct regression and dense heatmap prediction methods. In particular, as in dense heatmap prediction, these models are translation invariant due to the use of FCNs, also allowing for distance-based loss supervision. While dense heatmap prediction methods train the network to predict heatmaps matching to pre-defined arbitrary heatmaps, spatial regression networks learn the optimal latent heatmap that yields the most accurate point localization. Spatial regression, the Center of Mass (CoM) of a probability map as discussed in the work proposed by Tensmeyer and Martinez (2019), was almost simultaneously introduced by Sun et al. (2018) as integral regression, by Nibali et al. (2018) as differentiable spatial-to-numerical transform (DSNT), and by Luvizon et al. (2019) as Soft-argmax.

Given its proved effectiveness, we also adopt spatial regression for point localization. However, aiming efficient 3D localization, we go beyond standard techniques by jointly encoding x-, y- the z-dimension in latent heatmaps with the introduction of a fully differentiable module for 3D coordinate regression. We describe this new 3D coordinate regression module in Sect. 4.2.1.

Depth-Based Optical Marker Data
We created a special and unique dataset of spatio-temporally aligned motion capture and multi-view infrared-depth data to serve our scope (see Sect. 3.2). Our dataset constitutes the first visual data collection that contains spatio-temporally aligned multi-view colored infrared and depth images with 3D pose and marker annotations. We captured various activities performed by actors with retro-reflective markers attached on their bodies, being visible and distinguishable to the infrared images. To achieve that, we used the depth sensor infrared emitters to emit infrared light in the scene causing reflections on the retro-reflective markers (see Fig. 2). The infrared and depth images, which are aligned and defined on the same image domain, enable single-view marker 3D localization, which is discussed in Sect. 3.3. We achieve marker and pose 3D keypoint annotations by addressing the challenge of synchronization and spatial calibration between a professional motion capture and a multi-view depth sensor system, described in detail in Sect. 3.4.

We captured data of 4 actors, 2 males and 2 females, performing 11 different activities of approximately 20 s each, starting their performances in a T-Pose ensuring appropriate tracking initialization of the commercial MoCap system.Footnote1

In total, more than 20,000 samples are included in our dataset, however details on how we split it for training and evaluation based on the subject and activity criteria are given in Sect. 5.1.

Capturing Setup
A professional VICON (1984) motion capture system with 24 Vicon MXT40S cameras and a low-cost volumetric capturing systemFootnote2 with 4 Intel RealSense D415 stereo-based depth sensing devices (Keselman et al. (2017)) were used, recording the data at 120 and 30 frames/second, respectively.

Marker configurations and body structure
For the marker set, we used 𝑀=53 adhesive spherical retro-reflective markers of 14 mm diameter, which were attached on the motion capture suits of the actors.

We consider the post-processed, clean marker data 𝐌𝑔𝑡∈ℝ𝑀×3 as ground truth. With respect to the body structure, the original sequences provide poses of 33 different joints, however we simplify the structure and use 𝐽=19. The clean pose data, 𝐉𝑔𝑡∈ℝ𝐽×3, are considered as ground truth. Samples of the annotated depth-infrared image pairs along with the capturing setup in the MoCap studio where the dataset creation took place, are depicted in Fig. 2.

Optical Marker Data from Multiple Depth Sensors
Most of the recent low-cost consumer-grade depth sensing devices are equipped with infrared cameras and emitters (Keselman et al. (2017); Zhang (2012)). The Intel RealSense D415 depth camera is based on active stereo vision to calculate depth, consisting of a two infrared camera eyes and an infrared projector to improve depth accuracy in scenes with low texture features. The infrared projector casts static infrared pattern to the scene where the markers reflect back to the receiver enabling straightforward amplitude-based detection. We consider C spatio-temporally aligned depth cameras 𝑐∈{1,…,𝐶}, perimetrically placed around a capturing space of approximately 4m diameter, as shown in Fig. 2. Each camera acquires a pair of one colored infrared image 𝐈𝑐(𝐩)∈ℝ3 and one depth map 𝐷𝑐(𝐩)∈ℝ, with 𝐩:=(𝑥,𝑦)∈𝛺 being the coordinates of the pixels in the image domain 𝛺 defined in a 𝑤×ℎ grid, with w and h being its width and height, respectively. The sensor poses 𝐓𝑐:=[𝐑𝑐0𝐭𝑐1] are known in a common coordinate system, where 𝐑𝑐 and 𝐭𝑐 denote rotation and translation respectively. Hence, we can transform the depth image domain coordinates of each view to a global coordinate system by:

𝑐(𝐩)=𝐓𝑐𝜋−1(𝐷𝑐(𝐩),𝐊𝑐,𝐩),
(1)
with 𝐓𝑐 being the relative pose from the local coordinate system of sensor c to the global one and 𝜋−1 denoting the deprojection function that transforms the depth pixel to 3D coordinates, using sensor’s intrinsic parameters matrix 𝐊𝑐. Given that the infrared 𝐈𝑐 and depth 𝐷𝑐 images of camera c are aligned and defined on the same image domain 𝛺, we apply a linear thresholding as proposed by Gaschler (2011) for efficient marker segmentation. Following, a fast contour detection algorithm is applied to yield the blob centers per view, which we then map in the common, global 3D space using Eq. 1. During this process, the points not contained in a 3D bounding box set to limit the capturing 3D space, are considered as outliers and are discarded. To this end, a sparse marker cloud, 𝐌𝑟∈ℝ𝑀𝑓×3 of 𝑀𝑓 3D points is extracted per frame, containing the raw marker 3D coordinates as obtained from the multiple sensors. Given the noise of the sensors as well as the separate detection from each view, 𝑀𝑓 is varying around the real number of markers, i.e. 𝑀=53.

The quality of the raw marker tracking is analogous to the number of views, as in the majority of multi-view systems, mostly due to the elimination of occlusions and, consequently, of missing markers. We selected a 4-sensor setup in a cross placement for the creation of our dataset, considering it the trade-off between avoiding occlusions and low cost. Nevertheless, the proposed model is trained on highly noisy data resulted from weak optical marker tracking (one valid observation is enough at least in one of the views) in the form of a sparse and spatially invariant 3D data representation, eliminating the bias of the camera poses, intrinsic parameters or systematic depth noise, as we discuss in our ablation study, in Sect. 5.4.

Spatio-Temporal Alignment
High synchronization precision between our low-frequency depth sensors (30 frames/second in our dataset, see Sect. 3.1 for further details) is a prerequisite. The selected Intel RealSense D415 offers intra- and inter-sensor hardware synchronization, allowing for high precision temporal alignment. With respect to the inter-system (D415, low-cost - VICON, high-cost) synchronization, the global temporal offset between the systems was detected with a clapperboard equipped with 2 markers at the beginning of each sequence. The varying frame rate inter-system sequences were then aligned considering their local time steps after removing the global offset.

The spatial alignment between the VICON and depth sensor system is achieved by a two-step process. We perform an initial alignment by using the 3D positions of the markers, as estimated by each modality, i.e. triangulation-based for VICON and depth-based for D415. We exploit the starting, static T-Pose phase of each sequence where the markers are easily detected from the low-cost system, as the infrared images are crisp and sharp. We apply Iterative Closest Point (ICP) to coarsely transform 𝐌𝑟 to the coordinate system of the ground-truth markers 𝐌𝑔𝑡, resulting in 𝐌′𝑟. We use ICP since, as mentioned in Sect. 3.3, the number of the detected markers is varying per frame and there are no direct correspondences.

For an accurate spatial registration, we follow up with a sensor pose refinement step. At first, we find the correspondences between 𝐌𝑔𝑡 and 𝐌′𝑟,𝑐, where 𝐌′𝑟,𝑐⊂𝐌′𝑟 is the subset of the markers belonging to each sensor c. To solve this, we construct bipartite graphs between 𝐌𝑔𝑡 and 𝐌′𝑟,𝑐, where the edge weights represent euclidean distances between 3D points. Finally, we apply minimum weight bipartite matching by:

𝑀,𝑐(𝐌′𝑟,𝑐,𝐌𝑔𝑡)=min𝑀𝑔𝑡∑𝑖||𝑥𝑖−𝑦𝑖||2
(2)
where 𝑥𝑖∈𝐌′𝑟,𝑐 and 𝑦𝑖∈𝐌𝑔𝑡. From 𝑀,𝑐, we only use the correspondences under a strict threshold, ensuring a high quality correspondence group between 𝐌𝑔𝑡 and the detected blobs on 𝐈𝑐. Then, we apply Bundle Adjustment (Hartley and Zisserman (2003)) to refine the sensor poses using 𝐌𝑔𝑡 as reference. In detail, considering 𝐌𝑔𝑡 and intrinsic camera parameters 𝐊𝑐 constant, we jointly and iteratively refine the camera poses to minimize the reprojection errors. This provides a refined spatial alignment based on ground-truth optical marker data 𝐌𝑔𝑡, resulting in 𝐌″𝑟,𝑐 for the markers of each view and, consequently, to 𝐌″𝑟, also by applying a strict spatial clustering for marker points distances lower than 10 mm to merge only the ones that have been detected extremely close to each other.

It is worth noting that this alignment process is considered for the creation of the dataset and is not required during the model inference where the VICON data are absent. The results of the spatial and temporal alignment between the VICON and the multi-sensor system are presented in Fig. 2 where the pose and markers from VICON are overlayed on an infrared image sample.

Normalized Orthographic Depth Rendering
Working on a sparse 3D cloud instead of raw infrared or depth images allows us to overcome well-known limitations of such under constraint data. Several data-driven models suffer from these limitations such as the bias on specific camera poses and lightning conditions, the overfitting on the domain specific training set or the distance and systematic depth sensor noise.

We simplify our task by posing it as a spatial 3D regression problem on orthographically rendered depth maps under a multi-view context, i.e. with the use of two (or more) opposing renderings to overcome single-view ambiguities. We apply a volumetric scale and translation normalization transform  in order for 𝐌″𝑟 to occupy 80% of each dimension of a normalized cuboid 3D space , i.e. ranging between [0.1, 0.9] along the axes, resulting in 𝐌̂ 𝑟. Although obvious, it is worth noting that the same normalization transform  is applied on the ground-truth data 𝐌𝑔𝑡 and 𝐉𝑔𝑡, resulting in 𝐌̂ 𝑔𝑡 and 𝐉̂ 𝑔𝑡 respectively for supervision. To that end, the 3D bounding box containing every sample occupies the same volume in 3D, while the margin of 10% from the boundaries ensures the appropriate behaviour of the 2D convolutions across the network layers. We then render 𝐌̂ 𝑟 from two opposing views resulting in two sparse depth images through two orthographic cameras with the principal point centered on the center of 𝐌̂ 𝑟. We render the depth images in high pixel resolution (i.e. 800×800) and we next linearly interpolate them to 160×160 input’s resolution, aiming to eliminate the encoding of quantization error and information loss due to quantized rendering (Zhang et al. (2020a)). On rendering, the range of [0.1, 0.9] across “z” axis makes the marker points distinguishable from zero-values to both rendered depth maps. To that end, the depth values preserve marker 3D positions by representing their normalized depth as small areas of splatted depth pixels, creating two “sparse” depth images, 𝑓𝑟𝑜𝑛𝑡 and 𝑏𝑎𝑐𝑘, to feed our network. Samples from these rendered normalized depth maps are illustrated in Fig. 3.

Before the application of the normalization transform , we apply a random rotational augmentation around the X, Y and Z axes by [−10∘,10∘], [−180∘,180∘] and [−10∘,10∘] ranges respectively, increasing the variance of the human body part lengths depending on their orientation across the axes of the cuboid volume. It is worth noting that the 3D rotational augmentation in our case, with the input being a sparse cloud of 3D points, has the physical meaning of changing the rendering viewpoint of the camera. This enables the creation of completely new depth map inputs for the network during training, contrary to the limited effect of pseudo-rotational augmentation applied on dense input representations such as color images, depth maps or any other 2D-grid input.

To this end, we introduce a lightweight model for efficient inference on sparse 3D data. We avoid the use of 3D convolutional architectures since, despite their effectiveness (Riegler et al. (2017); Qi et al. (2017); Tu et al. (2020)), 3D convolutions are still computationally expensive and inefficient.

Fig. 3
figure 3
Input data visualization. 𝑓𝑟𝑜𝑛𝑡 and 𝑏𝑎𝑐𝑘 with colorization for the sake of clarity

Full size image
Fig. 4
figure 4
Using a multi-view setting with an arbitrary number of spatio-temporally aligned depth-infrared cameras perimetrically placed around a subject with reflective markers attached on the body, we capture the body movements. We detect the markers exploiting markers’ intense reflections provoked on the infrared images and sensors’ depth perception. Rendering the 3D markers after normalization on two opposing depth images, we train a FCN to jointly and sequentially predict marker and joint heatmaps, decoding then with a novel fully differentiable module the 3D markers and joint positions. At run-time, we conduct a two-step forward pass, where the first stages localize the markers and the latter stages estimate the body pose (we illustrate the two-view example of our multi-view input/supervision concept for the sake of brevity)

Full size image
Deep Depth-Based Motion Capture
The key factor of DeMoCap’s low cost is its reliance on cheap commodity stereo-based infrared-depth sensors, which, despite their noisy sensing, can satisfactorily observe the 3D locations of the retro reflective markers in a monocular fashion (i.e without the need for triangulation between multi-view observations). The low cost of stereo-based infrared-depth sensing still has the price of observation inaccuracy which, along with the aforementioned challenges of marker-based motion capture, we overcome by utilizing a deep neural network which enables simultaneous and effective marker and joint 3D coordinate regression.

With DeMoCap, we introduce a data-driven approach for marker-based motion capture from multiple infrared-depth streams, modeled as a staged markers-to-pose 3D regression from noisy marker data, orthographically rendered to multiple viewpoints as depth maps. Our end-to-end data-driven model for marker-based MoCap introduces:

Marker observation clustering by grouping the raw 3D points as captured by the different viewpoints.

Ghost marker denoising by ignoring ghost markers caused by erroneous marker detection.

Missed marker recovery of either occluded or undetected markers.

Recovering from marker swaps, due to the discrete, one-shot inference.

Labeled marker localization by spatially regressing the 3D coordinates from latent marker heatmaps.

Instantaneous 3D pose regression from labeled 3D markers without prior knowledge of body structure. This is backed up by all the above marker-robustness traits in combination with marker labelling and exploitation of 3D information.

The overall pipeline of the proposed method is illustrated in Fig. 4. The acquired multi-view infrared-depth frames captured from the sensors are processed for the extraction of raw marker 3D positions 𝐌″𝑟, which are then normalized yielding 𝐌̂ 𝑟. 𝐌̂ 𝑟 are then orthographically rendered on the two opposing depth images, 𝑓𝑟𝑜𝑛𝑡 and 𝑏𝑎𝑐𝑘. Given that 𝑓𝑟𝑜𝑛𝑡 and 𝑏𝑎𝑐𝑘 represent the spatial distribution of the markers attached on the human body, we feed them to a fully convolutional markers-to-pose staged network. This model sequentially predicts marker and joint latent heatmaps on which we apply a novel dual-view and fully-differentiable spatial 3D regression to precisely regress the normalized 3D coordinates both of 𝑀=53 markers, 𝐗̂ 𝑀∈ℝ𝑀×3, and 𝐽=19 joints, 𝐗̂ 𝐽∈ℝ𝐽×3. Finally, we apply the inverse scale and translation transformation −1 (Sect. 3.5) to recover the marker and pose 3D coordinates to their original physical dimensions.

Staged Markers-to-Pose Networks
Network Architecture
Our approach is based on a particular concept with respect to the network design. We propose multi-stage FCN architectures with smooth staging from markers-to-pose heatmap predictions that lead to better performing, efficiently designed models, as proved and discussed in Sect. 5.3.1. Given that the prior of the final pose is the optical marker spatial distribution, we design our networks to predict/refine the marker coordinates at an early stage, letting the joint coordinates to be localized from the latter stages. That way, the prior is refined before localizing the joint coordinates, resulting in robust and reliable predictions.

We build upon highly effective heatmap prediction networks such as Convolutional Pose Machines (CPM) by Wei et al. (2016), Stacked Hourglass (SH) by Newell et al. (2016) and a more recent one, HRNET by Wang et al. (2020). We predict dual-view heatmaps by feeding 𝑓𝑟𝑜𝑛𝑡 and 𝑏𝑎𝑐𝑘 to the networks in two separate forward passes. Hence, our models process each of the views, while both inferences are later fused to produce a final prediction supervised by a shared objective.

We follow the same architecture design for all networks. At first, we feed each 𝑣,𝑣={𝑓𝑟𝑜𝑛𝑡,𝑏𝑎𝑐𝑘} depth map to an initial pre-processing module to extract a feature map 𝐅𝑣 (𝐅 for the sake of brevity). We design a 2K-stage network, 𝐾∈ℕ, and we split it in two super-stages consisting of K stages each. The former super-stage predicts the marker heatmaps 𝐇¯𝑀 and the latter the joint heatmaps 𝐇¯𝐽, resulted as aggregations of the intermediate heatmaps 𝐇𝑆,𝑠𝑡 predicted by each stage 𝑠𝑡∈{1,2,…,2𝐾} of each super-stage 𝑆∈{𝑀,𝐽}. The feature map 𝐅 is concatenated with 𝐇𝑆,𝑠𝑡,𝑠𝑡<2𝐾 at every stage to feed the next one.

Rather than supervising heatmaps as originally proposed by the authors of the networks (per stage intermediate supervision in CPM and SH and last stage heatmap supervision in HRNET), we supervise only the aggregated heatmaps 𝐇¯𝑀 and 𝐇¯𝐽 with the coordinates and structural heatmaps of the respective ground truth joints. As further discussed in Sect. 5.3, this aggregation scheme converges faster and orchestrates slightly better the staged transition from dense observations to sparse marker and joint coordinate regression, as also used and validated by Zanfir et al. (2018).

Network details along with a high-level overview sketch (Fig. 12) of the proposed architectures are presented in “Appendix A”.

Heatmap Prediction
Let 𝐇𝑘𝑠𝑡,𝑠𝑡∈{1,2,…,2𝐾} denote the k-th latent heatmap before Softmax at stage 𝑠𝑡. Then, heatmap aggregation across stages is performed via summation:

𝐇¯𝑘=∑𝑠𝑡=𝑓𝑙𝐇𝑘𝑠𝑡
(3)
In total, two aggregations are being performed, one for marker position regression with 𝑓=1 and 𝑙=𝐾 and one for joint position regression with 𝑓=𝐾+1 and 𝑙=2𝐾. By applying Softmax on each of the aggregated heatmaps 𝐇¯𝑘 results in 𝐇̃ 𝑘:

𝐇̃ 𝑘(𝐩)=𝑒𝐇¯𝑘(𝐩)∑𝐩∈𝛺𝑘𝑒𝐇¯𝑘(𝐩)
(4)
where 𝐩 denotes a heatmap layer pixel and 𝛺𝑘 the spatial domain of the heatmaps. Visualizations of 𝐇¯ and 𝐇̃  heatmaps are illustrated in Fig. 5.

Fig. 5
figure 5
Surface plotting of the predicted heatmaps before (𝐇¯𝑘) and after Softmax (𝐇̃ 𝑘). We let the network predict latent heatmaps satisfying both tasks, i.e. the average of 𝐇¯𝑘 values to be equal to the z-coordinate of the 3D keypoint, while after Softmax, the heatmap 𝐇̃ 𝑘 to approach a gaussian distribution for xy-coordinate estimation

Full size image
Multi-view Spatial 3D Regression
We regress 𝑀=53 normalized marker position coordinates 𝐗̂ 𝑀∈ℝ𝑀×3 and 𝐽=19 normalized joint position coordinates 𝐗̂ 𝐽∈ℝ𝐽×3 by decoding heatmaps 𝐇¯ and 𝐇̃  predicted by each corresponding super-stage.

Center of Mass 3D with zMean Layer
We contribute to 3D coordinate regression by proposing Center of Mass in 3-dimensional space (CoM3D) with the insertion of a fully differentiable zMean layer to the well-known CoM coordinate decoding from heatmaps technique. Our statement is that fully convolutional networks can learn to predict heatmap distributions in varying value ranges, underlying an extra spatial information layer that allows the encoding of the third dimension. Thus, we regress 3D coordinates with the introduction of CoM3D combining two fully differentiable layers, zMean and CoM, (𝑧𝑀𝑒𝑎𝑛+𝐶𝑜𝑀=𝐶𝑜𝑀3𝐷), with zMean being our proposed contribution. While for CoM we follow the standard procedure introduced by Tensmeyer and Martinez (2019) for x and y coordinate regression, the motivation behind zMean is to exploit one extra degree of freedom which Softmax allows under direct heatmap supervision, in order to additionally constrain the average of 𝐇¯𝑘 to approach a ground-truth z coordinate, leading to a compact 3D coordinate encoding. In detail, let (𝑥𝑘,𝑦𝑘,𝑧𝑘) denote the predicted normalized 3D coordinates for marker or joint k, with k being either in {1,…,𝑀} or in {1,…,𝐽}, respectively. Then, we regress 𝑧𝑘 by:

𝑧𝑘=1𝑁𝑥𝑁𝑦∑𝐩∈𝛺𝑘𝐇¯𝑘(𝐩)
(5)
and (𝑥,𝑦)𝑘 by:

(𝑥,𝑦)𝑘=(1𝑁𝑥,1𝑁𝑦)∘∑𝐩∈𝛺𝐇̃ 𝑘(𝐩)⋅𝐩
(6)
with 𝑁𝑥=40, 𝑁𝑦=40 the cardinality of each 2D heatmap pixel coordinate domain, as designed to our network architectures, and ∘ denoting element-wise multiplication.

Multi-view Supervision
Finally, for two opposing rendering views, we conduct joint dual-view supervision by estimating one single 3D point per dual input rotating the normalized coordinate prediction (𝑥𝑘,𝑏𝑎𝑐𝑘,𝑦𝑘,𝑏𝑎𝑐𝑘,𝑧𝑘,𝑏𝑎𝑐𝑘) for 𝑏𝑎𝑐𝑘 by 180∘ around the Y-axis and averaging it with the normalized coordinate prediction (𝑥𝑘,𝑓𝑟𝑜𝑛𝑡,𝑦𝑘,𝑓𝑟𝑜𝑛𝑡,𝑧𝑘,𝑓𝑟𝑜𝑛𝑡) for 𝑓𝑟𝑜𝑛𝑡 by:

(𝑥𝑘^,𝑦𝑘^,𝑧𝑘^)=⎛⎝⎜⎜⎜12(𝑥𝑘,𝑓𝑟𝑜𝑛𝑡+(1−𝑥𝑘,𝑏𝑎𝑐𝑘)),12(𝑦𝑘,𝑓𝑟𝑜𝑛𝑡+𝑦𝑘,𝑏𝑎𝑐𝑘),12(𝑧𝑘,𝑓𝑟𝑜𝑛𝑡+(1−𝑧𝑘,𝑏𝑎𝑐𝑘))⎞⎠⎟⎟⎟.
(7)
That way, we approach every single 3D coordinate from two opposing sides covering the 3D volume where the human body is contained, regressing 𝐗̂ 𝑀 and 𝐗̂ 𝐽 normalized marker and joint 3D coordinates, correspondingly. In other words, our model learns to predict heatmaps whose average value approaches the normalized ground-truth “z” coordinate while their normalized 2D center of mass, after Softmax, approaches the normalized ground-truth “x” and “y” coordinates.

Fig. 6
figure 6
A high-level diagram of DeMoCap that depicts the various steps of our concept. The rendered depth multi-view input is fed to the former marker super-stage that, sequentially, feeds the latter pose one, both predicting the 𝐇¯ marker and joint heatmaps, correspondingly, resulting to 𝐇̃  after Softmax. Applying CoM3D, we decode z coordinate from 𝐇¯ with zMean and x,y coordinates from 𝐇̃  with CoM. Regressing that way the 3D coordinates from each view, we fuse them in the final stage. We supervise both 𝐇̃  and x, y, z marker and joint predictions with 𝐷 and 𝑤𝑖𝑛𝑔 respectively, to train the network in an end-to-end manner

Full size image
Losses
During training, we jointly supervise 𝐗̂ 𝑀 and 𝐗̂ 𝐽 with the ground truth coordinates, 𝐌̂ 𝑔𝑡 and 𝐉̂ 𝑔𝑡, respectively. On the one hand, contrary to DSNT proposed by Nibali et al. (2018), instead of using Euclidean Distance loss extended in 3D, we use Wing loss, 𝑤𝑖𝑛𝑔, proposed by Feng et al. (2018) which leads to a better-learnt data representation. Wing constitutes a loss function which behaves as a log function with an offset for small errors, while for larger errors as L1. 𝑤𝑖𝑛𝑔 loss function is defined by:

𝑤𝑖𝑛𝑔(𝑥)={𝑤ln(1+|𝑥|/𝜖)|𝑥|−𝐶𝑖𝑓|𝑥|<𝑤𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒
(8)
where the non-negative w sets the range of the nonlinear part to (−𝑤,𝑤), 𝜖 limits the curvature of the non-linear region and 𝐶=𝑤−𝑤ln(1+𝑤/𝜖) is a constant value that links in a smooth way the piecewise-defined linear and nonlinear parts of the function. The input x to 𝑤𝑖𝑛𝑔 is the 3D euclidean distance between predicted and ground-truth points of interest.

On the other hand, similarly to DSNT, we directly supervise also the spread of the heatmap since the strongly supervised pixel-wise gradients enhance the training of the model, improving its performance. We impose strict regularization on latent heatmap to directly drive it towards a certain shape and distribution. More specifically, we force the heatmaps to resemble spherical Gaussians by minimizing the divergence between generated heatmaps and targeted gaussian distributions centered at the 2D orthographic projections 𝐩𝑔𝑡 of the normalized ground truth 3D positions from the respective viewpoint. The distribution regularization term is defined by:

𝐷(𝐇̃ ,𝐩𝑔𝑡)=𝐷(𝐻̃ ||(𝐩𝑔𝑡,𝜎2𝐈2))
(9)
where 𝐷(⋅||⋅) is the Jensen–Shannon divergence (Fuglede and Topsoe (2004)), 𝜎 denotes the target variance and () the target normal distribution.

Finally, the total loss used to compute the network gradients is defined as:

𝑡𝑜𝑡𝑎𝑙=𝜆1(𝑤𝑖𝑛𝑔,𝑀+𝑤𝑖𝑛𝑔,𝐽)+𝜆2(𝐷,𝑓𝑟𝑜𝑛𝑡+𝐷,𝑏𝑎𝑐𝑘),
(10)
where 𝜆1,𝜆2 are hyper-parameters that weight the coordinates and heatmap distribution losses, respectively. A high-level diagram that depicts the various steps of our concept is illustrated in Fig. 6.

Experimental Evaluation
In this section, we present the experiments we conducted to assess our method. In Sect. 5.1, we present the dataset created according to the pre-processing steps we described in Sect. 3 and used for training, validating and testing our model. Then, we discuss the evaluation methodology we followed by presenting the metrics we used (Sect. 5.2.1), the state-of-the-art methods we compared against ours (Sect. 5.2.2) and the implementation details for the execution of these experiments (Sect. 5.2.3). In Sect. 5.3, we present and discuss quantitative and qualitative experimental results, giving insights with respect to the performance of our model. Finally, an ablation study gives evidence regarding the necessity and impact of our contributions in Sect. 5.4.

Dataset
Considering the data capturing and pre-processing steps described in Sect. 3, we create a set of 12,197 samples from 11 single-person activities. We divide the subjects S1, S2, S3, S4 into two male-female couples using the data of the first couple (S3 and S4) performing 7 of the 11 activities for training (running, basketball_dribbling, sitting_down, object_dropping_n_picking, stretching_n_talking, watching_scary_movie and in-flight_safety_announcement) and the data from the second one (S1 and S2) performing the remaining ones for validation (jumping_jack and bending) and testing (punching_n_kicking and sitting_on_a_stool). We split our dataset that way to assess the models on unseen subjects with different body structures and unseen activities, providing reliable and fair conclusions with respect to their performance.

The training, validation and testing data sets consist of 8165, 1990 and 2042 samples, as presented in Table 1.

Table 1 Training, validation and testing datasets number of samples, activities and subjects involved
Full size table
Methodology
Metrics
We measure the errors of the methods in physical dimensions by applying the inverse scale and translation transformation −1, as described in Sec. 3, with the use of the metrics below:

We assess the 3D pose regression accuracy by using the commonly used mean per joint position error (MPJPE) (Li et al. (2015)) and mean per marker position error (MPMPE) when applicable, i.e. when markers are predicted by the models.

In a similar fashion, we also measure the Root Mean Squared Per Joint and Marker Position errors (RMSPJPE and RMSPMPE) which constitute variations of MPJPE and MPMPE, respectively, based on Root Mean Square Error (RMSE) instead of Mean Absolute Error (MAE), as used by Chatzitofis et al. (2020). Both metrics are affected by large outliers but RMSPJPE and RMSPMPE incorporate better the variance of the predictions and their bias.

We use mean Average Precision (mAP) using Percentage of Correct Keypoints 3D (PCK3D) metric proposed by Yang and Ramanan (2011) in a range of 𝛼3𝐷 error thresholds.

Beyond the position-based metrics, we present results calculated by fusing the pose data (forward direction of the bones) and the orientation driven by the various marker groups mapped to joints, providing an extra metric with respect to the stability of the predictions and the capabilities provided by the simultaneous regression of markers and joints. We consider the mean and root mean per joint angular errors, MPJAE and RMSPJAE, by measuring the angle 𝜃 in degrees, between the ground truth and predicted joint orientations by:

𝜃=𝑐𝑜𝑠−1(2⟨ 𝑞̂ 𝑗,𝑔𝑡,𝑞𝑗^⟩2−1)
(11)
where ⟨ 𝑞̂ 𝑗,𝑔𝑡,𝑞̂ ⟩ denotes the inner product between 𝑞̂ 𝑗,𝑔𝑡 and 𝑞𝑗^ of joint j.

It is worth noting that for the sake of comparability against other methods, we use 17 out of the 19 total joints of the regressed pose for the assessment, excluding the toes.

Comparison against State-of-the-art Methods
Due to the lack of public state-of-the-art methods targeting this specific task, i.e. markers and pose regression from noisy optical marker data, relevant methods were identified and re-trained to adapt in our dataset, offering valid comparisons. We identify marker-based and markerless methods; for the former, the input is 𝐌̂ 𝑟, i.e. the normalized sparse cloud of the detected markers, while for the latter, we use the spatio-temporally aligned multi-view colored infrared images along with the camera intrinsic and extrinsic parameters.

In detail, we compare our model against two marker-based methods, an adaptation of a graph-based model designed for image-based hand-object pose estimation proposed by Doosti et al. (2020) (HOPE) adapted to our task and a direct 3D regression method used for online marker labeling (OML) adapted for simultaneous marker-joint 3D coordinate regression from markers by feeding a single and dual marker depth map (Han et al. (2018)). On top of that, we further assess three markerless methods, two top-down pose estimation methods from spatio-temporally aligned multi-view color images relying on the concept of learnable triangulation (LT), proposed by Iskakov et al. (2019) and one bottom-up multi-view and graph-based pose estimation approach (4DA) from spatio-temporally aligned multi-view color images proposed by Zhang et al. (2020b).

Marker-based methods: HOPE is a lightweight model designed to jointly estimate hand and object pose in 2D and 3D space based on a cascade of two adaptive graph convolutional neural networks. We adapt the first one to estimate 2D coordinates of the joints on the orthographic depth maps, followed by the second, Adaptive Graph-U-Net (Gao and Ji (2019)), to convert 2D to 3D coordinates. We also modify the input to our depth map resolution (1×160×160) instead of the color images of the original work and we train the model from scratch with Xavier (Glorot and Bengio (2010)) weight initialization.

As in HOPE, we adapt OML to the new input depth map resolution (160×160 instead of 52×52 of the original work), initializing the model weights with Xavier initialization. Furthermore, we adapt the output of the network to predict a vector with 𝑀=53 and 𝐽=19 3D positions, i.e. the target of our task, while we present an extra variation of the approach that consumes multi-view depth data similarly to our concept.

Markerless methods: With respect to the LT methods, LT(alg.) is based on algebraic triangulation with learnable camera-joint confidence weights, while LT(vol) constitutes a volumetric triangulation approach based on dense geometric aggregation of 2D heatmap predictions from multiple viewpoints. The input used to train these models is a batch of N spatio-temporally aligned color images along with the corresponding camera poses and intrinsic parameters. Aiming at a fair comparison between LT methods and DeMoCap, we re-train the LT models on our dataset initializing with the pre-trained weights due to the domain differences between the datasets.

For the initial bounding box detection, LT methods use Mask R-CNN 2D detector (He et al. (2017)) with ResNet-152 (He et al. (2016)) backbone to predict the human bounding boxes, however we use the ground truth bounding boxes to avoid the need for re-training of the detector.

We use the weights of the pre-trained modelsFootnote3 trained on Human3.6 (Ionescu et al. (2013)) with the 2D backbone pre-trained on COCO dataset (Lin et al. (2014)) and we further train them on our colored infrared dataset for 10 epochs adopting the training configuration proposed by the authors. We use the colored infrared data to re-train the models instead of the sparse data renderings which would require training of the models from scratch. Note that the predictions obtained from the algebraic are used for the volumetric triangulation approach. For training, we use the official repository and guidelines provided by the authorsFootnote4.

4DA considers the temporal aspect of consecutive frames. With the use of OpenPose by Cao et al. (2017), human body part candidates (heatmaps) and connection confidence (part affinity fields) scores between body parts are retrieved from each single view. Fusing the obtained human body part features between two sequential frames, a 4D graph is constructed with per-view parsing edges connecting adjacent body parts, cross-view matching edges connecting the same body part across the various views, and temporal tracking edges for mapping detected 3D nodes on a previous frame with new 2D detections on the next one. With the same practice we followed for LT, we re-train the model for 10 epochs, initializating it with the weigths of the pre-trained model of the official repository of OpenPoseFootnote5.

Implementation details
We train our network for 200 epochs using Adam (Kingma and Ba (2014)) optimizer with an initial learning rate equal to 1𝑒−4, while we apply a frequent linear rolling drop of 0.95 every 4 epochs. The batch size is 16, and the heatmap standard deviation during supervision is 𝜎=1.0. 𝜆 weights of 𝑤𝑖𝑛𝑔 and 𝐷 losses, as defined in Eq. 10, are set to 𝜆1=2 and 𝜆2=1, and the parameters for the wing loss 𝑤𝑖𝑛𝑔 are set to 𝑤=10 and 𝜖=2.

The model is implemented with PyTorch (Paszke et al. (2019)) and moai (2021) and the experiments ran on a conventional computer with 1 single NVIDIA GTX 1080 Ti graphics card of 12 GB RAM, and an Intel i7(R) processor, using the same manual seed for all epxeriments for fair comparison and reproducibility. The code and the dataset are publicly available onlineFootnote6.

Experimental Results
The validation set was used for training hyper-parameter tuning, while the evaluation on the test set took place after the selection of the best models. We present results on both sets, showing the variance of the inference accuracy between them for the various models, indicating their generalization potential.

Assessment with various FCN networks
At first, we assess DeMoCap by building our staged concept upon various FCN architectures, as discussed in Sec. 4.1. The reason is twofold; i) to validate our position that staged architectures for markers-to-pose predictions perform better independently of which FCN architecture used to predict the heatmaps and ii) to select the best performing model for the comparison against state-of-the-art. We train DeMoCap building upon CPM, SH and HRNET in three variations each:

Fig. 7
figure 7
We design all FCN architectures in 3 variations to assess our staged markers-to-pose concept. We feed 𝑓𝑟𝑜𝑛𝑡 and 𝑏𝑎𝑐𝑘 to all variations with pose, markers+pose and markers-to-pose yielding joints only (𝐗̂ 𝐽∈ℝ𝐽×3), simultaneously markers and joints (𝐗̂ 𝑀+𝐽∈ℝ(𝑀+𝐽)×3), and sequentially markers (𝐗̂ 𝑀∈ℝ𝑀×3) and joints (𝐗̂ 𝐽∈ℝ𝐽×3), correspondingly

Full size image
pose: We train the models in an end-to-end design, regressing and supervising only the pose heatmaps with markers absent, though focusing on one single task, the prediction of joint 3D positions 𝐗̂ 𝐽∈ℝ𝐽×3.

markers+pose: Similarly to pose variation, we train the models in an end-to-end concept, however regressing and supervising both the joint and the marker heatmaps in every forward pass, resulting in 𝐗̂ 𝑀+𝐽∈ℝ(𝑀+𝐽)×3.

markers-to-pose: We train DeMoCap on its original concept where the first super-stage predicts the marker, and the second one the pose only heatmaps, resulting sequentially in 𝐗̂ 𝑀∈ℝ𝑀×3 and 𝐗̂ 𝐽∈ℝ𝐽×3, correspondingly.

The outcomes of these experiments are illustrated in Table 2. The markers-to-pose staged approach achieves higher performance for all models on the main task of the method, i.e. the estimation of the pose, however, in some experiments, the markers are more precisely localized by the markers+pose variation. For cooperative marker and pose regression, which also enables joint 3D rotation estimation, the computation load per stage and the network weights are the optimum for the markers-to-pose approach, since, for markers+pose, the marker and joint heatmaps are predicted across all the stages of the network.

With respect to the models, although SH performs remarkably in the testing set outperforming HRNET, we consider the latter for the rest of the experiments since our indications were based on the experimental results retrieved in the validation set.

Table 2 MPJPE, RMSPJPE, MPMPE, RMSPMPE, mAP50mm, MPJAE and RMSPJAE results between DeMoCap with CPM, SH and HRNET in three variations each, pose, marker+pose and markers-to-pose
Full size table
Quantitative Analysis
We compare DeMoCapHRNET-8-{markers-to-pose}, referring to it as DeMoCap for the sake of brevity, against other methods (Sect. 5.2.2) presenting and discussing total, per joint and per action quantitative results.

Total Results. In Table 3, we depict the results of the methods with the use of MPJPE, RMSPJPE, MPMPE, RMSPMPE, mAP50mm, MPJAE and RMSPJAE metrics.

The markerless multi-view color-based pose estimation models 4DA and LT are effective showing their ability to estimate 3D poses from multiple spatio-temporally aligned color views, being relatively robust to body part occlusions and partial views. As presented in the original work by Iskakov et al. (2019), the volumetric approach performs better in our dataset, showing greater accuracy than the algebraic one.

Our method yields more reliable and accurate predictions than 4DA and LT across all metrics with respect to the total results presenting lower MPJPE, greater mAP50mm and lower RMSPJPE, despite the fact that these models have been trained with far larger datasets than ours. That is due to the major differences of DeMoCap against multi-view methods built to operate with dense visual data. DeMoCap is trained to predict markers and pose from sparse marker data where the input is exclusively related to the human body pose in 3D space. There is no context redundancy including various backgrounds, cloth types, fabric or colors, lighting conditions or any other aspect as happening for deep models trained on dense visual streams. In other words, the input for DeMoCap is not domain sensitive as color, depth or any other dense visual stream is. On top of that, DeMoCap is trained on purely 3D data, while LT and 4DA are based on the fusion of multiple partial 2D detections, weakening the localization of the final 3D keypoints. Despite the trial to reduce the weights of erroneous 2D predictions with learnable weighting or graph association techniques before the final 3D fusion, the errors cannot be totally eliminated, leading to erroneous estimates when body parts are exceeding the field of view of at least one of the cameras or being occluded resulting in malicious predictions. For DeMoCap, the disappearance of a marker from one of the views is not considered enough to drive the model in failure. At least one marker detection from one single depth camera, a highly possible case when multiple depth sensors are capturing the performance, can be enough to lead the model to accurate prediction given the low variance of the 3D input.

Fig. 8
figure 8
A plot comparison between Han et al. (2018)OML, Iskakov et al. (2019)LT and DeMoCap showing mAP using PCK3D metric against 𝛼3𝐷 threshold in millimeters in the test set. Our results reach high scores at low 𝛼3𝐷 thresholds showcasing the effectiveness of our method

Full size image
Table 3 MPJPE, RMSPJPE, MPMPE, RMSPMPE, mAP50mm, MPJAE and RMSPJAE results between HOPE by Doosti et al. (2020), OML by Han et al. (2018), LT by Iskakov et al. (2019), 4DA by Zhang et al. (2020b) and our method are presented. For the sake of clarity, C for Color and M for Marker data input with cell colorization indicate the markerless and marker-based methods, respectively
Full size table
Despite our efforts to finetune the HOPE and OML models, their performance is relatively low, struggling to accurately regress the 3D coordinates in samples from unseen subjects and activities.

Their low performance could be potentially explained by the use of direct regression with the use of fully connected layers (fully connected and graph layers for HOPE) in the network and our relatively small training dataset, contrary to the dataset size of the original works and the use of pre-trained models which were not applicable on depth map input. Moreover, our 3D coordinate regression task is more challenging since we regress 𝑀+𝐽=72 3D coordinates in comparison with HOPE and OML designed to regress less than 30 3D keypoints. Nevertheless, it is worth noting that the dual-view trained OML presents relatively better results than the single-view model, showcasing the potential of the multi-view supervision concept.

A more comprehensive analysis with respect to the performance of the methods in the testing set is illustrated in Fig. 8, where we compare the methods with mAP metric using PCK 3D against 𝛼3𝐷 threshold in mm. DeMoCap outperforms the rest of the methods with higher mAP against the whole range of 𝛼3𝐷 thresholds, while for 𝛼3𝐷=35 mm, the mAP is already  70%. OML methods are incapable of being comparable to 4DA, LT and DeMoCap for 𝛼3𝐷∈(0,60) in mm. Finally, both LT models showcase higher precision than 4DA.

Per Joint Results. Beyond the presentation of the total results, we evaluate the performance of the methods on a per human body joint analysis in Table 4, where DeMoCap outperforms the compared methods to most of the body joints.

This analysis allows us to assess the consistency of the models in the estimation of the various joints individually. The rationale behind this analysis is that, traditionally in human pose estimation, the difficulty level for the localization of joints gradually increases while moving from the torso joints of the body, i.e. hips, spines, shoulders, neck, to the head and the end joints of the limbs, i.e. the ankles and the wrists. The latter, being end nodes of an articulated structure, i.e. the human body, move more freely than the rest of the body showing large variance with respect to their global and local body positioning. Nevertheless, we consider it significant for a method that targets human motion capture to present robustness and consistency across all joints estimates. In our experiments, the same challenge applies to all methods, however the errors between end and torso joints for HOPE, OML, 4DA and LT show higher variance than ours, meaning that DeMoCap regresses the pose with more equally balanced accuracy across the joints of the body than the other approaches. Rapid body part movements when fast actions are performed cause image blurriness leading the vision-based models to erroneous estimates. To overcome this challenge, DeMoCap has been explicitly designed to perform the pose regression on two phases. The initial noisy and incomplete marker input is refined/recovered on a first phase, driving the estimation of the pose in a later stage based on refined marker data. The refinement of the markers allow the model to more accurately perform the last stage estimates of the joints, which are only related to the marker positions, without any other contextual binding to the initial blurry color input. We obtain the remarkably lower errors for Wrists and Ankles joints in Table 4, especially in the testing set where during the totally freely performed punching_n_kicking action, many body parts are out of the cameras’ field of view for several frames.

Table 4 3D Euclidean Distance Error per joint in millimeters
Full size table
Table 5 MPJPE per action results for the validation and testing sets, presenting the performance of the models across different actions. For the sake of clarity, C for Color and M for Marker data input with cell colorization indicate the markerless and marker-based methods, respectively
Full size table
Per Action Results. In Table 5, we present and discuss the model outcomes on a per action analysis to assess the model performance across sequences of varying poses. Including the actions both of the validation and testing sets, we present MPJPE for 4 actions: jumping_jack, bending, punching_n_kicking and sitting_on_a_stool. Despite the fine-tuning of the models on the validation set, both sets share the same characteristics considering that both include unseen subjects and actions in relation to the training set. Hence, the difficulty level for capturing these motions is determined mostly by the objective challenges of each specific performance such as their complexity and speed, resulting in several occlusions or missing data, or body part movements out of the field of views of the cameras. That is proved from the lower errors in sitting_on_a_stool action which demonstrates lower errors than the validation set actions.

For the punching_n_kicking action, the subjects were asked to punch and kick in front of the cameras without guidance or other constraints. This resulted in extremely challenging data due to the rapid and free-style movements with noisy samples due to blurriness, partial occlusions or body parts out of the field of view of the cameras. This is observed in the per action analysis where the results of all methods on this specific action present the highest errors.

Fig. 9
figure 9
In the first three columns, we visualize noisy raw (yellow), ground-truth (blue) and predicted (green) markers projected on one single infrared view per group frame (different group frame per row). In the fourth column, we illustrate the predicted poses (yellow) and ground-truth (blue). Red, magenta and yellow circles indicate ghost marker cleaning, recovery of missing markers, and depth sensor errors, correspondingly. Green circles highlight the blurriness issues that our model overcomes

Full size image
Qualitative Analysis
In this section, we present and discuss qualitative outcomes, as illustrated in Fig. 9. We project the marker and pose 3D coordinates on the infrared views in order to correlate them with the actors’ actual performances. For the sake of visualization clarity, we depict the raw noisy input, the ground-truth and the predicted marker data separately. The predicted and ground-truth poses are visualized together to facilitate the visual comparison between them.

Given the infrared images in the background, we highlight the targeted and addressed challenges by our model. In particular, we indicate the marker corrections on the noisy input and the robust pose regression behaviour of the predictions. One can easily observe that the raw marker input captured with the low-cost D415 system is highly noisy. That is due to the marker 3D localization from each sensor separately, which, in combination with the depth sensor errors, result in considerably distant 3D points that represent the 3D position of the same marker. Comparing the regressed marker positions against the initial input, we point the solutions given to this marker-based MoCap problem. Marker observation clustering is achieved by automatically grouping the noisy raw 3D points captured by different sensors (Fig. 9, yellow circles). In this rationale, ghost markers from erroneous detection are ignored (Fig. 9, red circles). Missed markers, either occluded or non-detected, are recovered (Fig. 9, magenta). Image blurriness are effectively handled to eliminate the joint coordinate regression errors (Fig. 9, green circles). In other words, we accurately localize and label optical markers captured with low-cost sensors that provide highly noisy data. Due to the discrete inference on every single frame, marker swapping present in traditional MoCap is resolved. Our model directly provides instantaneous 3D pose regression from labeled 3D markers without the use of any humanoid prior or knowledge for body structure, meaning that there is no prior information or side inference with respect to bone lengths and joint relative placement, instead, the model predicts the pose in one single-shot. Nevertheless, it is significant to highlight that DeMoCap models are trained based on a specific marker configuration prior, meaning that different marker placement can lead our MoCap model in erroneous predictions.

Fig. 10
figure 10
Qualitative results of the ground-truth (blue) and predicted (yellow) poses in 3D. Our model regresses 3D poses comparable to ground-truth. In the last row, failure cases are illustrated, when the poses of the subjects are extremely challenging and fast

Full size image
In Fig. 10, we present more qualitative results with the use of 3D visualizations. We illustrate a batch of 20 samples from the testing set, illustrating the ground-truth data in blue and our predictions in yellow, including failure cases in the last row of the grid.

Table 6 Ablation results. We ablate the contributions of our model one by one to showcase their effectiveness and necessity
Full size table
Ablation
We conducted and discuss an extensive ablation study to justify the design of the proposed approach. We replace, remove or tune differently one single contribution of our approach per experiment, showcasing its weight to the reader separately. In detail, we ablate:

1.
Our newly introduced fully differentiable CoM3D module (Sect. 4.2.1) against integral 3D regression module by Sun et al. (2018),

2.
The 1- versus dual-view input/supervision (Sect. 4.2.2),

3.
The 4- versus dual-view input/supervision (Sect. 4.2.2),

4.
The quantization bias between high- (Sect. 3.5) and low-resolution depth rendering of the input,

5.
The use of data augmentation (Sect. 3.5),

6.
The use of data normalization (Sect. 3.5),

7.
The use of intermediate heatmap aggregation (Sect. 4.1) against last stage only heatmap prediction,

8.
The inference of our model on marker data captured by 3 cameras only,

9.
The inference of our model on marker data captured by 2 opposing cameras only.

The summary of this ablation following the same enumeration is shown in Table 6.

#1. Integral 3D regression versus CoM3D. One of the main contributions of our method is the introduction of the CoM3D fully differentiable module for 3D regression comprising a zMean layer followed by Softmax and a CoM layer. The main difference against other spatial regression approaches is the use of zMean for z-coordinate regression. In order to assess its value, we train our model substituting CoM3D module with integral pose regression proposed by Sun et al. (2018). As shown in Table 6, the use of integral pose regression module is not effective enough in our task resulting in greater errors than the original model (83.68 mm and 89.32 mm against 33.83 mm and 40.04 mm MPJPE in validation and testing sets, respectively), while the inference time increases noticeably.

Number of rendering views. The input of our model is a pair of depth maps resulted by rendering the 3D positions of the reflective markers from two opposing viewpoints. Conceptually, we take advantage of the three dimensional information as well as its sparsity that allows us to “generate” numerous 2D inputs from one single 3D sample, claiming that multi-view input and supervision yields more robust inference. To assess this claim, we conduct two experiments tweaking the number of rendering views.

#2. 1- versus 2-view depth input. At first, we render only one single depth map and train the network with single-view input. As depicted in Table 6 (exp #2), this model showcases lower performance in comparison with the proposed dual-view approach across all metrics, validating that the multi-view concept drives the model to more accurate and robust predictions.

#3. 4- versus 2-view depth input. On top of that, another experiment with increased number of rendering views is conducted (Table 6 (exp #9)). We train and assess a model with a 4-view input showing favorable comparison against the proposed dual-view original model. In detail, the model performs similarly in the validation set, showing clear outperformance nevertheless in the testing set across all metrics, i.e. approximately 3 mm absolute improvement across all euclidean distance-based error metrics, 1.72% mAP50mm and 3.67∘ and 5.41∘ MPJAE and RMSPJAE, respectively, given the higher reliability of the results when trained on multiple inputs and based on higher number of multi-view estimates.

The conclusion for these experiments is threefold; firstly, despite the sparsity of the marker point cloud, single-view ambiguities still exist in challenging and complex body poses, resulting in locally dense marker subsets and potential marker occlusions, which multi-view rendering can overcome. Secondly, increasing the number of rendering views, the reliability and accuracy of the predictions is improved, validating our claims with respect to the contribution of multi-view supervision. Finally, increasing the number of rendered depth inputs linearly increases the computational complexity and performance costs of the model. We find the use of two opposing depth renderings ideal as a trade-off between effectiveness and efficiency for deployment, given their comparable results.

#4. High- versus low-resolution rendering. We build DeMoCap posing a purely 3D problem as a 3D regression from multiple 2.5D inputs mission, allowing the use of 2D fully convolutional architectures with proved and remarkable effectiveness in keypoint localization tasks. That way though, we pay the cost of encoding 3D non-quantized data to quantized 2D grids, leading to some loss of information. Specifically, the rendered depth maps based on quantized coordinates are not totally accurate leading to sub-optimal supervision and degraded model performance. We limit the quantization error and information loss by rendering the depth images in high pixel resolution, i.e. 800×800, and linearly interpolate them to our input size, i.e. 160×160.

In Table 6 (exp #4), we train and assess the model with data directly rendered to low-resolution, thereby encoding higher quantization errors. This models shows lower performance, validating the consideration that low-resolution rendering leads to biased coordinate regression. It is worth mentioning that the errors are not extremely higher than ours, assuming that the multi-view supervision can better handle this bias as well as the sub-pixel coordinate regression characteristics of CoM3D heatmap coordinate decoding module.

Table 7 Results on DeMoCap clean MoCap data. We train DeMoCap with clean MoCap data to assess the performance of the models in various combinations between training and validation/testing sets
Full size table
#5. W/o versus w/ data augmentation. To demonstrate the contribution of 3D rotational augmentation of our data (Sect. 3.5), we train our model by excluding it (Table 6 (exp #5)). Our sparse point cloud input provides us the privilege to actually rotate it before rendering, tremendously increasing the amount of new depth map inputs during training with significant effect as figured in the results, an important difference in comparison with the limitations of pseudo-rotational augmentation applied on 2D visual data.

#6. W/o versus w/ data normalization. In Table 6 (exp #6), we evaluate the contribution of the volumetric scale and translation normalization transform  we perform to the marker point cloud to occupy equal volume in the normalized voxel-grid for all samples. We observe that this normalization boosts the performance of our model across all metrics. Our consideration is that this transform leads to high variance with respect to the human body structures, allowing the model to learn how to directly reconstruct the scale normalized absolute 3D poses.

Fig. 11
figure 11
Qualitative results of various frames illustrated on the same scene (locomotion) from DanceTurns002 sequence of SFU Dataset (Ying (2011)), on totally unseen body structures and activities. The yellow poses represent the predictions of DeMoCap, while the blue ones the ground-truth data of the dataset

Full size image
#7. W/o versus w/ heatmap aggregation. In our approach, instead of supervising heatmaps as originally proposed for the architectures we build upon, we supervise only the aggregated heatmaps. This aggregation scheme drives our model to faster convergence and slightly better results, especially for the marker estimation, as shown in Table 6 (exp #7), in comparison with last stage only supervision (HRNetV1), as proposed for HRNET in the respective work (Wang et al. (2020)).

Number of sensors. Finally, we conduct experiments of our model on the validation and testing sets taking into account the marker observations only from 3 and 2 sensors (Table 6 (exp #8 and #9)), respectively, instead of the full 4-sensor setup with which we trained DeMoCap. We present this experiment to assess the bias of our model on the training set and the generalization capabilities, as well as its sensitivity to sensor decrease.

#8. 3 versus 4 capturing depth sensors. As expected, the accuracy is lower in relation to the 4-sensor captured data, nevertheless, the results are still better than the compared methods, showing lower MPJPE and RMSPJPE errors and higher mAP50mm accuracy (46.81 mm, 53.50 mm and 77.17% in the validation and 47.58 mm, 60.02 mm and 81.88 mm in the testing set, respectively).

#9. 2 versus 4 capturing depth sensors. On the 2-sensor assessment, the performance is further decreased, however, the results can be considered fair enough given the lack of information. Our conclusion from this experiment is that DeMoCap follows a reasonable dependency on the number of sensors that capture the markers, as the high-end MoCap systems do with their specialized cameras.

Study on Clean MoCap data
On DeMoCap Dataset
We further benchmark our model by training and assessing it using as input the post-processed, clean marker data from VICON used as ground-truth, under the same learning configurations. These experiments showcase the behaviour of the model in ideal conditions where the marker data are totally clean and highly precise, without the noise present in unclean optical marker data either captured with low-cost depth sensors or high-end MoCap systems before post-processing. In Table 7, we present results of DeMoCap and DeMoCap trained with VICON data (DeMoCapvicon) assessed both on clean VICON and noisy data from consumer-grade depth sensors (RS).

As expected, DeMoCapvicon achieves significantly high accuracy both on the validation and testing VICON sets, achieving MPJPE and MPMPE lower than 3 cm and mAP50mm 99.81% and 94.07% in each set, respectively. On the other hand, DeMoCapvicon showcases low performance on noisy RS data exceeding 6 cm for absolute distance errors and mAP50mm performance lower than 56%, even lower than DeMoCap assessed on 2-viewpoint data only, given the dissimilarity of the evaluation sets in comparison to the training set.

Results of particular interest are presented by our model when assessed on the clean validation and testing set from VICON. DeMoCap demonstrates significantly better performance in VICON than RS data, though trained on the latter, letting us consider that the model generalizes well without bias on the systemic camera noise, poses or pinhole parameters. The model is trained to handle noisy and clean data, showing that increasing the accuracy of the marker capturing leads to more reliable inference.

On SFU Dataset
We also evaluate the performance of our models, DeMoCap and DeMoCapvicon on a public MoCap dataset with a relatively similar marker configuration and pose structure with 53 markers and 30 joints, SFU Motion Capture Database by Ying (2011). Indicatively, in our experiments, we include two challenging activities, DanceTurns002Footnote7 and HopOverObstacle001.Footnote8 The quantitative results for 575 samples in total are shown in Table 8. Visually, the models showcase comparable results, as illustrated in Fig. 11, numerically though, only DeMoCapvicon reaches high scores, while for DeMoCap, the task is proved more challenging. It is worth highlighting the spatial offsets existing between different body structures for the various datasets that insert a constant error in the measurements, as discussed in Sect. 6.

Table 8 Results on SFU (Ying (2011)) clean MoCap data. We train DeMoCap with clean MoCap data to assess the performance of the models in various combinations between training and validation/testing sets
Full size table
Discussion
In this section, we present a summary of our observations, discussing the pros and cons of the various motion capture solutions, in relation to our approach and beyond.

Strengths
For decades, marker-based motion capture has been the gold-standard for high-fidelity motion capturing and tracking. Nevertheless, despite its sub-millimeter accuracy on marker tracking, the use of marker-based systems is globally limited given the high costs of the setups, the software licenses, the maintenance and more. To the best of our knowledge, DeMoCap is the first computer vision method that enables the use of low-cost equipment for marker-based motion capture with comparable results to high-end MoCap systems, to the extent the hardware and data limitations allow for.

DeMoCap is one of the pioneering methods in deep marker-based motion capture that allows one-shot regression of pose from a sparse set of 3D points. The method performs better than recent state-of-the-art color-based methods (i.e. LT and 4DA) despite the use of highly erroneous depth estimates from low-cost sensors (depth error higher than 3 cm in 1.5 m distance from the camera), while the mean average per joint error drops under 2.5 cm when trained and assessed on clean data. DeMoCap generalizes well even with the use of low number of cameras (2 or 3 sensors, Sect. 5.4), showcasing increased stability in comparison with methods based on potentially erroneous partial view detections (e.g. 2D pose detectors). The model is driven to reject outliers and detect missing markers at the first stage (marker inference), allowing for pose estimation from refined prior marker information.

DeMoCap focuses exclusively on the information that solves the pose, i.e. the markers attached on the body, without interference from background context as color or dense depth data do. Finally, DeMoCap performs better when noise is reduced (as assessed in Sect. 5.5), despite the existence of systematic noise of the depth sensors in the training set, showcasing that our model, when the marker configuration is the same, is affected mostly by the quality of the marker tracking, as all marker-based motion capture system do.

Weaknesses
Nonetheless, DeMoCap still presents weaknesses in comparison with traditional high-end marker-based systems and markerless methods based on dense visual data. The current consumer-grade sensors used to trade the high costs of the specialized MoCap cameras are limited with regards to the capturing frequency (30 Hz vs 120/240 Hz or higher) and depth-sensing range (up to 4 m keeping acceptable accuracy). To this end, contrary to professional marker-based solutions or methods applied on dense visual data applicable in large scale capturing areas, e.g. arenas, sport fields or stadiums, DeMoCap is particularly limited with regards to the capturing space volume, at least based on the existing consumer-grade depth and infrared sensing technologies.

Furthermore, similarly to all data-driven statistical models, DeMoCap is trained on a special dataset captured with a specific 53-marker configuration placement for human motion capture. That fact will lead DeMoCap in erroneous predictions in the appearance of different marker configurations or skeleton structures, requiring re-training on data captured with the settings. Contrarily, traditional MoCap can be applied on a variety of moving entities, from humans to animals and objects, where data-driven models lag behind with regards to this flexibility of the gold-standard marker-based MoCap solutions. Traditional MoCap also requires new configuration for marker labeling and skeleton tracking, however, it is “cheaper” due to the shorter time and less effort needed for its completion, without the need for dataset creation. In other words, for all marker-based solutions, the placement of markers is a strong prior for their operation, however, this prior is even stronger for DeMoCap due to its data-driven modeling.

DeMoCap provides one-shot inference for markers and pose 3D regression avoiding the possibility for marker swapping, a common case in MoCap tracking when markers are getting very close to each other. The temporal aspect though can extremely eliminate potential errors, being a strong driver for correct predictions. DeMoCap’s inference is instantaneous, without considering the temporal aspect of the marker trajectories. This constitutes a limitation for DeMoCap in comparison with marker-based solutions designed for out-of-the-box marker tracking of high stability and precision.

Conclusions
In this paper, we introduced DeMoCap, a low-cost lightweight data-driven model for marker-based motion capture with the use of spatio-temporally aligned infrared- and depth-sensing streams acquired with consumer-grade devices. We train our model on noisy optical marker data captured with a low-cost multi-view system to accurately regress marker and joint 3D coordinates by staging a smooth representation transition from markers to 3D pose to learn the underlying structural relation between human body and marker configuration placement in an end-to-end, scale- and translation-invariant manner. Learning upon it, the model overcomes bias on our relatively limited training data and generalizes well. Technically, our method is the first that introduces the use of fully convolutional networks to be applied on extremely sparse depth maps efficiently regressing marker and joint 3D coordinates by posing their estimation as a joint 2D localization and regression objective within a normalized 3D space to embed the z-dimension indirectly with the introduction of a new fully differentiable module for 3D regression. The special dataset we created to drive our model is publicly available, containing inter- and intra-system spatio-temporally aligned infrared-depth and motion capture data.

Fig. 12
figure 12
Markers-to-pose multi-stage FCN Architectures. We illustate in high level the architectures we used to train DeMoCap. In all of them, we follow the same concept where the first stages predict 𝐇𝑀,1…𝐾, aggregated to 𝐇¯𝑀, while the last stages predict 𝐇𝐽,𝐾+1,…,2𝐾 aggregated to 𝐇¯𝐽. The predictions of each stage and the feature maps F are concatenated for each subsequent stage

Full size image
We focus our future work to overcome the aforementioned limitations of our method. Our approach is limited to regress markers and pose of one single person in the capturing space. That is due to use of spatial regression that limit us to regress one single coordinate per latent heatmap layer. We aim to conduct research on that challenging task to enable multi-person motion capture. Even though we apply our method on temporally continuous 3D data, DeMoCap is not designed to maintain internal memory for sequential data processing, instead, the inference is single-shot without considering the previous predictions. On the one hand, the discrete per frame inference allows us to skip issues that tracking techniques can cause such as marker swaping, however, considering temporal information can lead to higher motion capture accuracy and robustness. Hence, we will explore potential techniques that will allow us to introduce temporal features in our work for the development of more efficient and effective deep learning models for motion capture. Finally, given the regressed labeled marker data, soft inverse kinematics solvers can be explored to result in joint transformation solving similar to professional motion capture solutions that will allow the regression of bone orientation in a data-driven end-to-end manner.