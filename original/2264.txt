The Jaccard index, also known as Intersection-over-Union (IoU), is one of the most critical evaluation metrics in image semantic segmentation. However, direct optimization of IoU score is very difficult because the learning objective is neither differentiable nor decomposable. Although some algorithms have been proposed to optimize its surrogates, there is no guarantee provided for the generalization ability. In this paper, we propose a margin calibration method, which can be directly used as a learning objective, for an improved generalization of IoU over the data-distribution, underpinned by a rigid lower bound. This scheme theoretically ensures a better segmentation performance in terms of IoU score. We evaluated the effectiveness of the proposed margin calibration method on seven image datasets, showing substantial improvements in IoU score over other learning objectives using deep segmentation models.

Access provided by University of Auckland Library

Introduction
Semantic segmentation in images is a fundamental yet challenging problem in computer vision. The task is to build a computational model to accurately assign a class label to every pixel. Semantic segmentation has drawn a broad research interest for many applications such as robotic sensing Cadena and KoÅ¡eckÃ¡ (2014) and auto-navigation Xiao and Quan (2009). Recently, the development of deep convolutional neural networks has led to remarkable progress in semantic segmentation due to their powerful feature representation ability to describe the local visual properties. Deep parsing networks are often fine-tuned based on the pre-trained classification networks, e.g., deep residual networks He et al. (2016).

To train a reliable deep learning model for semantic segmentation, the learning objective is one of the most critical ingredients. The most straightforward way is to treat the semantic segmentation as a dense classification task, which examines each pixel in images individually, comparing the class-predictions to the one-hot encoded ground truth. As a surrogate relaxation of the mis-classification rate, cross-entropy becomes the most intuitive loss function in training deep semantic segmentation models. The minimization of cross-entropy is directly related to the maximization of pixel accuracy. In the training process, cross-entropy loss averages over all pixels in images, which is essentially asserting equal learning to each pixel in an image batch. This is problematic in semantic segmentation if the actual classes are imbalanced in the image corpus, as training can be dominated by the most prevalent class, e.g., the small foreground interest regions are submerged by large background areas. Although applying a cost-sensitive re-weighting scheme Wong et al. (2018) to alleviate the data imbalance and emphasize the â€œimportantâ€ pixels, it is unclear how to determine the weights for the best IoU scores Ma et al. (2021), because the pixels of minority class do not necessarily mean they are difficult to be classified (see the ablation study in Sect. 4.3). Furthermore, the measure of cross-entropy on the validation set is a poor indicator of the model quality Berman et al. (2018), as minimizing the pixel-wise loss cannot guarantee a higher IoU score, which is more commonly used in semantic segmentation and can better sketch the contours of interest regions. To address these issues, some recently proposed loss functions have been proposed, e.g., Focal loss Lin et al. (2017) and LovÃ¡sz-softmax Berman et al. (2018). The focal loss is an improved cross-entropy loss that tries to handle the class imbalance problem by assigning more weights to hard or easily misclassified examples and down-weight easy examples. The LovÃ¡sz-softmax is a LovÃ¡sz surrogate that mimics the IoU, making it consistent with the evaluation metric in semantic segmentation.

A â€œbetterâ€ machine learning model should feature a better-generalized performance, i.e., the performance measured on the underlying data distribution, where the unknown instances are sampled from. Clearly, there is a gap between the empirical performance on the training dataset and the generalized performance regarding IoU, i.e., there always exists the IoU differences between training and validation datasets. This gap is commonly called the generalization error. Even though some regularization schemes have been applied to the training of neural networks, their influence on the generalization error of IoU still remains unclear. In this work, we explicitly show how this generalization error is related to label distribution and can be controlled by adding some class-dependent bias terms in the output of the deep semantic segmentation network. These bias terms are connected to the margins among multiple classes, where we are inspired by the idea of margins from the well-known Support Vector Machines (SVMs) Boser et al. (1992). In Blaschko and Lampert (2008), the authors proposed to apply the structured regression to predict the bounding box for object localization. For data-imbalanced learning problems, uneven margins can be applied to well calibrate the importance of specific classes Li et al. (2002); Khan et al. (2019); Cao et al. (2019). In semantic segmentation, class imbalance widely exists in most image datasets, which hinders the generalization ability of the model, because the IoU score for each class is jointly optimized with others. The power of the â€œunevenâ€ margins inspires us to develop a proper margin calibration scheme for a better generalization ability of semantic segmentation models.

In this paper, we propose a novel distribution-aware margin calibration method, to optimize the IoU in semantic segmentation. The margins across multiple classes are pre-computed based on the label distribution, which can well calibrate the distance between foreground and background classes. Our method has the following three compelling advantages over other learning objectives: (1) it provides a lower bound for data-distribution IoU, which means the model has a guaranteed generalization ability; (2) the margin-offsets can be efficiently computed, which is readily pluggable into deep segmentation models; (3) the proposed learning objective is directly related to IoU scores, i.e., it is consistent with the evaluation metric. Due to the high discriminative power and stability, it is worth using the proposed margin calibration method as a learning objective in the challenging semantic segmentation tasks. We conduct extensive experiments on seven public image datasets, which indicates our method can achieve a considerable improvement compared to other learning objectives.

The rest of the paper is organized as follows. Section 2 introduces related work. Section 3 elaborates the proposed margin calibration method. Experimental results and analysis are presented in Sect. 4. Finally, Sect. 5 concludes the paper.

Related Work
Deep Learning-Based Semantic Segmentation Models
Deep learning-based image segmentation models have achieved significant progress on large-scale benchmark datasets Zhou et al. (2017); Cordts et al. (2016) in recent years. The deep segmentation methods can be generally divided into two streams: the fully-convolutional networks (FCNs) and the encoder-decoder structures. The FCNs Long et al. (2015) are mainly designed for general segmentation tasks, such as scene parsing and instance segmentation. Most FCNs are based on a stem-network (e.g., deep residual networks He et al. (2016)) pre-trained on a large-scale dataset. These classification networks usually stack convolution and down-sampling layers to obtain visual feature maps with rich semantics. The deeper layer features with rich semantics are crucial for accurate classification, but lead to the reduced resolution and in turn spatial information loss. To address this issue, the encoder-decoder structures such as U-Net Ronneberger et al. (2015) have been proposed. The encoder maps the original images into low-resolution feature representations, while the decoder mainly restores the spatial information with skip-connections. Another popular method that has been widely used in semantic segmentation is the dilated (atrous) convolution Yu and Koltun (2015), which can enlarge the receptive field in the feature maps without adding more computation overhead, thus more visual details are preserved. Some methods, such as DeepLab v3+ Chen et al. (2018), just combine the encoder-decoder structure and dilated convolution, to effectively boost the pixel-wise prediction accuracy.

Learning Objectives for Semantic Segmentation
As a dense prediction task, the commonly used cross-entropy is a natural learning objective in training a semantic segmentation model. However, the classification accuracy is inconsistent with the evaluation metric IoU. In recent years, various learning objectives have been proposed specifically for semantic segmentation, and most of them can be used in a plug-and-play way. For example, the distribution-based loss functions (e.g., weighted cross-entropy loss Ronneberger et al. (2015) and focal loss Lin et al. (2017)), the region-based loss functions (e.g., IoU loss Rahman and Wang (2016), Dice loss Eelbode et al. (2020) and Tversky loss Salehi et al. (2017)) and boundary-based loss functions (e.g., Hausdorff distance loss Karimi and Salcudean (2019) and Boundary loss Kervadec et al. (2019)). In medical image segmentation, Ma et al. presented a comprehensive review of 20 general loss functions Ma et al. (2021). These loss functions can also be jointly used in model optimization Abraham and Khan (2019). In deep learning based segmentation methods, the model outputs continuous class probabilities of all pixels, which are indirectly related to IoU scores. To deal with this problem, Maxim et al. proposed to use submodular measures to readily optimize the segmentation model in the continuous setting Berman et al. (2018). In many real application scenarios, especially scene parsing, the pixel-labels are highly imbalanced, so we prefer to balance the label weights among different classes. By adding down-weights to the well-classified records and assign large weights to misclassified records, focal loss can effectively boost the performance of dense prediction.

On the other hand, design proper surrogates as learning objectives is also applicable to mimic the IoU in semantic segmentation. For example, Nowozin proposed a statistical approximation based on parametric linear programming as a tractable decision making process Nowozin (2014). Ahmed et al. combine the expected-intersection over expected-union (EIoEU) with optimizing the expected-IoU (EIoU) for a set of candidate solutions Ahmed et al. (2015). For the deep learning based semantic segmentation, Nagendar et al. proposed to plug a surrogate network into the deep segmentation model for the approximation of IoU Nagendar et al. (2018), while such a scheme can be also extended to other non-decomposable evaluation metrics, e.g., miss-classification rate (MCR) and Average Precision (AP), in universal machine learning tasks Grabocka et al. (2019).

However, the above methods are mainly to minimize the empirical risk in the model training procedure, without the consideration of the generalization of IoU. In deep learning based segmentation tasks, one may directly use some general methods such as ğ‘™2-norm, weight-decay, drop-out or extensive data augmentation to improve the generalization ability, but it is unclear how or whether these methods are correlated to the generalization of IoU. As one of the critical objective in design machine learning models, optimizing the generalized performance can be achieved through (1) optimizing the empirical performance approximated by a surrogate loss associated with the performance metric Berman et al. (2018); Grabocka et al. (2019), e.g. IoU; and (2) controlling the generalized error. In our work, we design a margin calibration scheme with a proper loss function to overcome this difficulty, which provides a better learning objective for semantic segmentation compared to other learning metrics, both theoretically and practically.

Method
Problem Setup and Notations
Semantic segmentation in images is essentially a dense classification problem, where a model predicts the one-hot labels to distinguish each foreground class from the background class, using the definition of true positive (TP), false positive (FP) and false negative (FN). The Jaccard index (IoU) is defined as the size of intersection divided by the size of the union of the sample sets:

ğ¼ğ‘œğ‘ˆ=ğ‘‡ğ‘ƒğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ+ğ¹ğ‘.
(1)
In semantic segmentation, IoU is measured from the pixel-wise classifications, which differs from object localization, where IoU is calculated by the regression of bounding boxes Blaschko and Lampert (2008). In this paper, we do not consider the regression case.

A similar metric to IoU is Dice Similarity Coefficient (DSC), which is equivalent to F1-score:

ğ·ğ‘†ğ¶=2Ã—ğ‘‡ğ‘ƒ2Ã—ğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ+ğ¹ğ‘.
(2)
However, the above two metrics are count-based measures, whereas the outputs of deep segmentation models are probability values representing the likelihood of the pixels belonging different classes. Therefore, neither IoU score nor DSC can be directly and accurately measured from the output of the network.

For a multi-class semantic segmentation problem, we formally define an input space î‰„âˆˆâ„ğ‘¤Ã—â„Ã—ğ‘ and the target space î‰…={1,â€¦,ğ¾}ğ‘¤Ã—â„, where w, h, c are the width, height and numbe of channels of an input image, and K is the total number of classes to be segmented. For simplicity, we use ğ‘€=ğ‘¤Ã—â„ to represent the total number of pixels in an image. The function ğœƒâˆˆÎ˜:î‰„â†¦î‰… is a complex non-linear projection from images to masks (pixel labels). In deep learning-based semantic segmentation methods, Î˜ can be a learning framework with trainable parameters.

Given an image ğ±âˆˆî‰„ with a corresponding mask ğ²âˆˆî‰…, we denote the discrete predicted label for i-th pixel is ğ‘¦Ì‚ ğ‘–. Then, given a ground truth ğ² and a prediction ğ²Ì‚ , the empirical IoU regarding the k-th foreground class is:

ğ¼ğ‘œğ‘ˆğ‘˜=ğ‘ƒğ‘˜âˆ’ğ‘ƒğ‘˜0ğ‘ƒğ‘˜+ğ‘ƒ0ğ‘˜.
(3)
where ğ‘ƒğ‘˜0 denotes the empirical probability that a foreground class k pixel is observed but is predicted as the background class by ğœƒ, i.e.,

ğ‘ƒğ‘˜0=1ğ‘€âˆ‘ğ‘–=1ğ‘€ğ•€(ğ‘¦ğ‘–=ğ‘˜âˆ§ğ‘¦Ì‚ ğ‘–â‰ ğ‘˜),
(4)
with ğ•€(â‹…) an indicator function. Similarly, ğ‘ƒ0ğ‘˜ denotes the empirical probability that a pixel of the background class is observed but is predicted as the foreground class k, i.e.,

ğ‘ƒ0ğ‘˜=1ğ‘€âˆ‘ğ‘–=1ğ‘€ğ•€(ğ‘¦ğ‘–â‰ ğ‘˜âˆ§ğ‘¦Ì‚ ğ‘–=ğ‘˜).
(5)
We use ğ‘ƒğ‘˜ to denote the empirical probability that a class k foreground pixel is observed, i.e.,

ğ‘ƒğ‘˜=1ğ‘€âˆ‘ğ‘–=1ğ‘€ğ•€(ğ‘¦ğ‘–=ğ‘˜).
(6)
In the evaluation of the segmentation performance, IoU is computed globally over an image dataset, in which the total number of pixels is N, where ğ‘â‰«ğ‘€. From the statistical perspective, we assume that the image samples in the whole dataset are independently and identically distributed (i.i.d) according to some unknown distribution îˆ° over î‰„Ã—î‰…, and let îˆ°î‰… denote the projection of îˆ° over î‰…. Note that we do not assume the pixels in an image are i.i.d. The IoU of k-th class over the whole data distribution is:

	(7)
Note that we use P and IoU of the normal font to represent the empirical probability and IoU on the finite image dataset, while use  and  of the calligraphic font to represent the probability and IoU over the whole data distribution.

When there are K classes presented, the empirical mean IoU (mIoU) is defined as ğ‘šğ¼ğ‘œğ‘ˆ=1ğ¾âˆ‘ğ‘˜=1ğ¾ğ¼ğ‘œğ‘ˆğ‘˜, and similarly, the mIoU over the data distribution îˆ° is defined as .

Ideally, a function ğœƒ should produce a high  over the data distribution to ensure the stable performance of ğœƒ on any data sampled from îˆ°. Unfortunately, the data distribution îˆ° is usually fixed but unknown. Thus, we can only optimize the empirical mIoU, so that with a high probability it can lead to a high  over îˆ°. The problem here is that how mIoU and  are close to each other. Next, we present our method to minimize the error bound between mIoU and , which can theoretically support the proposed margin calibration method.

Method Overview
In semantic segmentation tasks, the label imbalance is an inherent issue for dense prediction, so equally treating all pixel labels in the model training may lead to the biased IoU scores towards the majority classes. Since in deep semantic segmentation models, the one-hot pixel labels of multiple classes are simultaneously optimized, the minority class may be still under-fitted when the majority class is already over-fitted. An intuitive approach is to set different weights to the loss function based on the number of pixels of each objective class, e.g., weighted cross-entropy. However, it is unclear if the weights of the loss functions based on the number of total pixels of the objective classes are optimal, because the â€œhardnessâ€ of segmenting the minority classes is not directly related to the number of training pixels. In our own experience, we found that weighted cross-entropy barely improves the model performance in terms of IoU.

In our approach, we instead set different margins for the pixel classes, which differs from the weight loss functions. Specifically, we would derive an optimal margin setting for a small error bound between mIoU and . Denote the output score of i-th pixel in the image dataset regarding the k-th foreground class by ğ‘ ğ‘–ğ‘˜. Here we define the margin for the i-th pixel with regard to class k in the whole image set as:

ğœ†ğ‘–ğ‘˜=ğ‘ ğ‘–ğ‘˜âˆ’maxğ‘—â‰ ğ‘˜ğ‘ ğ‘–ğ‘—.
(8)
If the i-th pixel belongs to k-th foreground class, it is preferable to have a large positive value of ğœ†ğ‘–ğ‘˜. Otherwise, we expect it to be a negative value. We then combine the margin ğœ†ğ‘–ğ‘˜ with a ğœŒ-margin loss function ğœ™ğœŒ(â‹…) defined in Mohri et al. (2018)[Definition 5.5], to build the relationship between IoU score and the margin ğœ†ğ‘–ğ‘˜. The ğœŒ-margin loss is defined as:

ğœ™ğœŒ(ğœ†)=min(1,max(0,1âˆ’ğœ†ğœŒ)),
(9)
which encourages the margin ğœ† to be larger than ğœŒ and provides an upper bound for 0-1 loss, as is illustrated in Fig. 1. We call the parameter ğœŒ margin-offset. We can then bound the empirical probabilities ğ‘ƒğ‘˜0 and ğ‘ƒ0ğ‘˜ in Eq.(3) as:

ğ‘ƒğ‘˜0(ğœƒ)ğ‘ƒ0ğ‘˜(ğœƒ)<1ğ‘âˆ‘ğ‘–âˆˆğ‘Œğ‘˜ğœ™ğœŒğ‘˜0(ğœ†ğ‘–ğ‘˜)=â„“ğ‘˜0(ğœƒ,ğœŒğ‘˜0),<1ğ‘âˆ‘ğ‘–âˆˆğ‘Œâˆ–ğ‘Œğ‘˜ğœ™ğœŒ0ğ‘˜(âˆ’ğœ†ğ‘–ğ‘˜)=â„“0ğ‘˜(ğœƒ,ğœŒ0ğ‘˜),
(10)
where we use ğ‘Œğ‘˜ and ğ‘Œâˆ–ğ‘Œğ‘˜ to denote the index set of the foreground pixels of k-th class and background pixels, respectively. ğœŒ0ğ‘˜ and ğœŒğ‘˜0 are pre-defined margin-offsets. Then, we can give a lower bound for Eq.(3) as:

ğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ğ‘˜=ğ‘ƒğ‘˜âˆ’â„“ğ‘˜0(ğœƒ,ğœŒğ‘˜0)ğ‘ƒğ‘˜+â„“0ğ‘˜(ğœƒ,ğœŒ0ğ‘˜),
(11)
and the corresponding lower bound for mIoU is:

ğ‘šğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯=1ğ¾âˆ‘ğ‘˜=1ğ¾ğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ğ‘˜.
(12)
Fig. 1
figure 1
The ğœŒ-calibrated log-loss (blue dotted line) and ğœŒ-margin loss (orange solid line) functions. The ğœŒ-margin loss is a upper bound for 0-1 loss. For the ğœŒ-calibrated log-loss, ğœ‘ğœŒ(ğœŒ)=1 and it upper bounds the ğœŒ-margin loss

Full size image
Theoretical motivation
We can derive a generalization error bound regarding IoU with the margin-offsets ğœŒğ‘˜0 and ğœŒ0ğ‘˜, based on the following theorem:

Theorem 1
For any function ğœƒâˆˆÎ˜, define ğœ‡ğ‘˜=ğœŒğ‘˜0ğœŒ0ğ‘˜ and îˆ²=ğ¶(Î˜)+ğœ(1ğœ‚). ğ¶(Î˜) is some proper complexity measure of the hypothesis class Î˜ and ğœ(1ğœ‚)â‰œğœŒmax4ğ¾2ğ‘€log2ğ¾ğœ‚â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš is typically a low-order term in 1ğœ‚ with ğœŒmax=max{ğœŒğ‘˜0,ğœŒ0ğ‘˜}ğ¾ğ‘˜=1. Given a training dataset of N image pixels including ğ‘ğ‘˜ pixels of class k, with each image consisting of M pixels, then for any ğœ‚>0, with probability at least 1âˆ’ğœ‚,

	(13)
where

ğœ–=1ğ¾âˆ‘ğ‘˜=1ğ¾ğ‘âˆ’ğ‘ğ‘˜â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš+ğ‘ğ‘˜âˆšğœ‡ğ‘˜ğ‘ğ‘˜4ğ¾îˆ²ğœŒ0ğ‘˜âˆ’ğ‘âˆ’ğ‘ğ‘˜â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš.
(14)
Note that this theorem involves a complexity measure îˆ²=ğ¶(Î˜)+ğœ(1ğœ‚), where ğ¶(Î˜) is derived from the Rademacher complexity. The Rademacher complexity typically scales in ğ¶(Î˜)ğ‘ğ‘˜â€¾â€¾â€¾â€¾âˆšMohri et al. (2018). Such a scale has been used in related works (see Cao et al. (2019); Neyshabur et al. (2018) and the references therein) to imply a connection between Rademacher complexity and number of pixels ğ‘ğ‘˜. See the proof of the theorem in the appendix. This theorem enables us to maximize the  on the data distribution by maximizing a lower bound ğ‘šğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ for the empirical IoU on a training dataset with a high probability. Meanwhile, we would prefer a small error bound ğœ– so that the lower bound ğ‘šğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ on the empirical IoU could be a reliable estimation for . This scheme guarantees the performance of associated function ğœƒ on the unseen image data.

Remark
At first glance, the relationship between N, ğ‘ğ‘˜ and ğœ– seems complicated. However, ğœ– decreases if we increase N and ğ‘ğ‘˜ proportionally, as can be inferred from Eq. (14), so decreasing ğœ– would need more these pixels accordingly. Theorem 1 indicates that a smaller ğœ– requires more foreground class pixels, and a simple fit function for a smaller ğ¶(Î˜). Another important factor is that we can adjust the margin-offset ğœŒ0ğ‘˜ to minimize the error bound ğœ–. Note that increasing ğœŒ0ğ‘˜ also increases the ğ¶(Î˜) implicitly, because a larger margin-offset may require more complex hypothesis class Î˜. Otherwise, ğ‘šğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ may decrease due to the under-fitting. Besides, the direct calculation of the optimal margin-offsets in Theorem 1 is difficult because it is related the complexity measure ğ¶(Î˜), which is measured by the structure of deep neural networks. Nevertheless, we can give the optimal ğœŒ0ğ‘˜ that is irrelevant to ğ¶(Î˜), by the following corollary:

Corollary 1
Assume âˆ‘ğ‘˜=1ğ¾ğœŒ0ğ‘˜=some constant. Let ğœ‡ğ‘˜=ğ‘ƒğ‘˜ğ‘ğ‘˜âˆšğœ(ğ‘âˆ’ğ‘ğ‘˜)âˆ’ğ‘ƒğ‘˜ğ‘âˆ’ğ‘ğ‘˜âˆš with ğœ (ğœ>0) being a hyper-parameter. Then the minimum of the error bound ğœ– in Theorem 1 is attained given the following condition:

ğœŒ0ğ‘–ğœŒ0ğ‘—=ğ‘ğ‘—ğ‘ğ‘–ğ‘âˆ’ğ‘ğ‘–â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšğ‘âˆ’ğ‘ğ‘—â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš,ğœŒğ‘˜0ğœŒ0ğ‘˜=ğœ‡ğ‘˜,âˆ€ğ‘–,ğ‘—,ğ‘˜âˆˆ[1,ğ¾].
(15)
See the proof of the corollary in the appendix. Corollary 1 provides a theoretical guarantee for setting the margin-offsets towards a smaller error bound ğœ–. The margin-offset ğœŒ0ğ‘˜ is proportional to ğ‘âˆ’ğ‘ğ‘˜âˆšğ‘ğ‘˜, which indicates a larger margin is required for k-th class, with comparably fewer pixels. We introduce a hyper-parameter ğœ (ğœ>0) to scale the margin-offsets, which can be tuned on the validation dataset. Note that another margin ğœŒğ‘˜0=ğœ‡ğ‘˜ğœŒ0ğ‘˜ is usually small compared to ğœŒ0ğ‘˜ in practice with a well-tuned ğœ, so we can mainly focus on ğœŒ0ğ‘˜ here. A proper setting of ğœ and ğœ can provide a balance between ğœ– and ğ‘šğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ for the maximization of . Empirically, just setting ğœ=10 and ğœ=1 can obtain a satisfactory result.

A Practical Implementation with ğœŒ-Calibrated Log-Loss Function
Based on the above statistical analysis of label distribution, the learning objective needs to compute the margin-offsets from the whole pixel label set before optimizing the network. Given a pixel label set with K classes, the computation of margin-offsets is summarized in Algorithm 1.

figure a
The learning objective of semantic segmentation is to maximize  for the best performance. Ideally, we should maximize its lower bound ğ‘šğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ with a small error bound ğœ–, because the margin-offset can provide a guarantee for its generalization. However, in the training of deep neural networks, the direct optimization of ğ‘šğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ is impractical because the network is trained in a mini-batch manner. Unlike other decomposable evaluation metrics, such as classification accuracy, where the expectation of the metric on a mini-batch sample is equivalent to the metric on the whole dataset, the expectation of the mini-batch mIoU is an estimation to the overall mIoU on the whole dataset, i.e., the empirical IoU on the training dataset may be sub-optimal.

For a practical implementation, we instead minimize the sum of ğœŒ-margin losses ğ‘™ğ‘˜0 and ğ‘™0ğ‘˜ involved ğ‘šğ¼ğ‘œğ‘ˆâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯, with the optimal margin-offset. So for a mini-batch images, the loss ğ¿(ğœƒ) is calculated by:

ğ¿(ğœƒ)=âˆ‘ğ‘˜=1ğ¾(â„“ğ‘˜0(ğœƒ,ğœŒğ‘˜0)+â„“0ğ‘˜(ğœƒ,ğœŒ0ğ‘˜))=1ğ‘ğ‘ âˆ‘ğ‘˜=1ğ¾(âˆ‘ğ‘–âˆˆğ‘Œğ‘˜ğœ™ğœŒğ‘˜0(ğœ†ğ‘–ğ‘˜)+âˆ‘ğ‘–âˆˆğ‘Œâˆ–ğ‘Œğ‘˜ğœ™ğœŒ0ğ‘˜(âˆ’ğœ†ğ‘–ğ‘˜)),
(16)
with ğœ†ğ‘–ğ‘˜ defined in Eq.(8), and ğ‘ğ‘  is the number of pixels in a mini-batch. In the forward pass of training, the network ğœƒ outputs a batch of pixel-wise scores, in which the i-th pixel with regard to class k is ğ‘ ğ‘–ğ‘˜. Then ğ‘ ğ‘–ğ‘˜ is used to calculate ğœ†ğ‘–ğ‘˜ by Eq.(8).

In practice, the non-smoothness of ğœŒ-margin loss function may bring instability in the optimization. As is shown in Fig. 1, the gradient regarding the ğœŒ-margin loss can be prohibitively large when ğœŒ is very small, while the gradients outside the interval (0,ğœŒ) is zero. Thus, we substitute the ğœŒ-margin loss ğœ™ğœŒ(ğœ†) used in Eq.(16) with ğœŒ-calibrated log-loss ğœ‘ğœŒ(ğœ†)=log2(1+2âˆ’ğœ†+ğœŒ). The relationship between the ğœŒ-margin loss ğœ™ğœŒ(ğœ†) and the ğœŒ-calibrated log-loss ğœ‘ğœŒ(ğœ†) is illustrated in Fig. 1. Now we apply the margin-offsets to get a biased score ğ‘ Â¯ğ‘–ğ‘˜ for ğœŒ-margin loss. The computation of ğ‘ Â¯ğ‘–ğ‘˜ is described in Algorithm 2.

figure b
For the use of the ğœŒ-calibrated log-loss ğœ‘ğœŒ(ğœ†), we first calibrate the output {ğ‘ ğ‘–ğ‘˜} via Algorithm 2, then the ğœŒ-calibrated log-loss bounds the ğœŒ-margin loss from above and leads to:

â„“ğ‘˜0(ğœƒ,ğœŒğ‘˜0)<1ğ‘ğ‘ âˆ‘ğ‘–âˆˆğ‘Œğ‘˜log2(1+2âˆ’ğ‘ Â¯ğ‘–ğ‘˜)=â„“ğ‘˜0â¯â¯â¯â¯â¯â¯(ğœƒ,ğœŒğ‘˜0),
(17)
and

â„“0ğ‘˜(ğœƒ,ğœŒ0ğ‘˜)<1ğ‘ğ‘ âˆ‘ğ‘–âˆˆğ‘Œâˆ–ğ‘Œğ‘˜log2(1+2ğ‘ Â¯ğ‘–ğ‘˜)=â„“0ğ‘˜â¯â¯â¯â¯â¯â¯(ğœƒ,ğœŒ0ğ‘˜).
(18)
Based on the above two inequalities, we simply use â„“ğ‘˜0â¯â¯â¯â¯â¯â¯(ğœƒ,ğœŒğ‘˜0) and â„“0ğ‘˜â¯â¯â¯â¯â¯â¯(ğœƒ,ğœŒ0ğ‘˜) to replace â„“ğ‘˜0(ğœƒ,ğœŒğ‘˜0) and â„“0ğ‘˜(ğœƒ,ğœŒ0ğ‘˜) in Eq.(16) as the final learning objective.

Complexity Analysis
Given the output scores {ğ‘ ğ‘–ğ‘˜}ğ‘ğ‘ Ã—ğ¾ of ğ‘ğ‘  pixels in an image batch, with the parallel computation provided by GPUs, calculating the margin ğœ†ğ‘–ğ‘˜ needs ğ‘‚(ğ‘ğ‘ ) time and ğ‘‚(ğ‘ğ‘ ğ¾) space, and the subsequent calibrated log-loss incurs ğ‘‚(ğ‘ğ‘ ğ¾) time complexity. So compared to the cross-entropy loss, the calibration method requires extra ğ‘‚(ğ‘ğ‘ +ğ‘ğ‘ ğ¾) time and ğ‘‚(ğ‘ğ‘ ğ¾) space complexities overhead in computing the calibrated log-loss.

Discussions
How to optimize non-decomposable loss like IoU is an open problem. This problem becomes far more challenging in the mini-batch training setting because in this case, we optimize the mini-batch IoU, which is an estimation of the overall IoU on the whole dataset. In our method, we mainly deal with a ratio distribution (both denominator and numerator of IoU are random variables regarding data distribution), where the central limit theorem can not be applied. As such, currently, no method can deal with this mini-batch setting accurately including lovÃ¡sz-softmax, which claims to be a surrogate for optimizing IoU. We also compromise on the learning objectives to optimize a related ğœŒ-margin calibrated log-loss, which is independent of the margin calibration process. This makes our IoU on the training set a more reliable indicator for the IoU over the underlying distribution than other methods.

In the deep learning based semantic segmentation settings, directly applying margin calibration incurs additional space and time complexities. Consequently, the computation may be slower and more memory-consuming. To alleviate this, the network parameters can be initialized via training with cross-entropy, the most efficient but not the â€œperfectâ€ learning objective, then fine-tuned with margin-calibration.

Experiment
In this section, we use the deep segmentation models to conduct the experiments on five publicly available datasets. Unlike the proposal of deep learning architectures that aims to achieve the best segmentation performance in some recent works Wang et al. (2020); Shen et al. (2020); Ding et al. (2020), our contribution is mainly on the design of a novel learning method for better IoU scores when the learning framework is fixed. Given a deep segmentation model, we mainly compare the final performance when applying commonly used learning objectives and our margin calibration method.

Datasets
We conducted the experiment of semantic segmentation on seven datasets: Robotic Instrument Allan et al. (2019), COCO-Stuff 10K Caesar et al. (2018), PASCAL VOC2012 Everingham et al. (2015), MIT SceneParse150 Zhou et al. (2019), Cityscapes Cordts et al. (2016), BDD100K Yu et al. (2020) and Mapillary Vistas Neuhold et al. (2017).

The Robotic Instrument dataset provides 8 robotic surgical videos, in which 225 frames are sampled from each video. In the frames, each part is manually annotated by a trained team. Here we conduct the instrument part segmentation as an ablation study, in which we aim to correctly segment each articulating part of the instrument.

The COCO-Stuff 10K contains 9,000 images for training and 1,000 images for validation (testing). Following Ding et al. (2018), we evaluate the IoU performance on 171 categories (80 objects and 91 stuff) to each pixel.

The PASCAL VOC 2012 semantic benchmark contains 20 foreground object classes and one background class. The original dataset has 1,464 and 1,449 images for training and validation, respectively. To augment the training dataset, we also use extra annotations provided by Hariharan et al. (2011). The model is not pre-trained with MS COCO dataset.

The MIT SceneParse150 dataset is built based on ADE20K Zhou et al. (2017) as a scene parsing benchmark. It contains more than 20K scene images, annotated by 150 classes of dense labels. Here we use 2,000 validation images for qualitative evaluation.

The Cityscapes, BDD100K and Mapillary Vistas are three different street-view datasets. The data in Cityscapes were taken from 50 European cities, which provides fine-grained pixel-level annotations of 19 classes including buildings, pedestrians, bicycles, cars, etc. The training/validation/testing splits are with 2,975, 500 and 1,525 images, respectively. It also has 20,000 coarsely-labelled images, which can be used to pre-train the segmentation model.

Different from the Cityscapes dataset comes from Germany, the images of BDD100K are mainly from the US cities, and there is a dramatic domain shift between the two datasets for semantic segmentation models, although their labels are the same.

The Mapillary Vistas dataset contains 25,000 high- resolution images annotated into 66 fine-grained object categories, featuring locations from all around the world, and taken from a diverse source of image capturing devices.

Settings
We implemented the segmentation model based on PyTorchFootnote1. For the experiment on the Robotic Instrument dataset, we applied the recently proposed COPLE-Net Wang et al. (2020), a variant of U-Net Ronneberger et al. (2015) for medical image segmentation. COPLE-Net has much fewer trainable parameters compared to FCN, thus it is quite suitable for the segmentation tasks in a simple yet fixed application scenario. On the rest four datasets for general semantic segmentation tasks, we used DeepLab v3+ Chen et al. (2017) backend on SEResNeXt-50 He et al. (2018), with the output stride 8. We employed the AdamW optimizer Loshchilov and Hutter (2019) with the initial learning rate 10âˆ’4 in the training process. We used the mixed precision and gradient checkpoint, which allow to set a larger mini-batch size and can effectively save the GPU memory usage without hurting the batch normalization layers. Our experiments were conducted on a server equipped with a single NVIDIA Tesla V100 GPU card.

Ablation Study on Robotic Instrument Dataset
We sequentially split the Robotic Instrument dataset into 1,200, 200 and 400 images according to the frame index for training, validation and testing, respectively. For this task, we aim to accurately label four instrument parts, including shaft, wrist, claspers and probe. The pixel ratios in the 4 parts are 4.9%, 1.4%, 1.6% and 0.8%, respectively, while the background class occupies 91.2% of the total pixels, making the label distribution extremely imbalanced.

We used categorical cross-entropy as the baseline, as semantic segmentation can be treated as a dense prediction for each image pixel. In addition, we tested several recently proposed methods, including focal loss Lin et al. (2017) (with the scale factor 0.4), lovÃ¡sz-softmax Berman et al. (2018), generalized dice loss Sudre et al. (2017) and Tversky loss Salehi et al. (2017). All these methods were used as independent learning objectives in the medical segmentation tasks, and the segmentation networks were all trained from the random state.

We recorded all the intermediate results during the training process, as is shown in Fig. 2. By observing these figures, we can see directly using the two count-based loss functions, Dice loss and Tversky loss, the segmentation network fails to converge, as they are not differentiable to pixel-wise categories and can only work in conjunction with distribution-based loss functions. The rest four methods show similar mIoU and loss curves. However, although lovÃ¡sz-softmax can utilize the sub-modular property to minimize the Jaccard loss, it is more likely to over-fit. The proposed margin calibration method can generally act as a plug-and-play learning objective like cross-entropy and focal loss in training a semantic segmentation network.

Fig. 2
figure 2
Training mIoU and loss curves on Robotic Instrument dataset

Full size image
In the model inference, we did not use horizontal flipping, multi-scale prediction or CRF post-processing to augment the segmentation performance. The quantitative results are shown in Tables 1 and 2. The two evaluation metrics, pixel accuracy and IoU score, although have a very high correlation in terms of the absolute values, the best one single metric cannot guarantee the other. For example, simply using cross-entropy achieve a better pixel accuracy compared to the focal loss and lovÃ¡sz-softmax, but its IoU score is the worst. In semantic segmentation, the IoU score is usually a better evaluation to quantify the percent overlap between the pixel-label output and target mask. Using Dice loss as the learning objective, the model fails to segment shaft and claspers. Similarly, using Tversky loss cannot segment probe. Compared with cross-entropy, focal loss and LovÃ¡sz-softmax, the proposed margin calibration method obtains the best pixel accuracy and IoU scores on this dataset. Specifically, as a single learning objective, margin calibration outperforms the second-best, with 3.0% performance gain in terms of the mIoU score. Although margin calibration is not specifically designed to optimize the pixel accuracy, it can also benefit from the generalization ability.

We illustrate the segmentation examples in Fig. 3. By observing the results, we can see that applying the proposed margin calibration method can effectively reduce the false positives, forming more smooth contours and obtaining more accurate results.

Results on COCO-Stuff 10K and PASCAL VOC2012 Datasets
In the training of the segmentation model, we re-scaled the shorter image size to 400 then randomly cropped it to 384Ã—384. In the inference, we used the single-scale inference without flipping or any other augmentations. The comparisons on the two validation sets with the three baseline methods are reported in Tables 3 and 4, respectively. Results show that when fixing the deep neural network, both focal loss and LovÃ¡sz-softmax and improve the mIoU scores. By pre-computing the margin-offsets and applying the proposed margin calibration method, a single segmentation model can achieve a further 0.4% of the mIoU score on the COCO-Stuff 10K, while on the PASCAL VOC2012 dataset, the mIoU score with margin calibration is only 0.1% higher than the second-best.

By observing the two datasets, we found that they have different properties regarding pixel-label annotations. The dense prediction on the COCO-Stuff 10K dataset aims to auto-label all pixel classes, while on the PASCAL VOC 2012 the task is to distinguish one or two foreground objects from the unique background class. Also, the label imbalance in COCO-Stuff 10K is much more imminent than PASCAL VOC 2012, leading to the significant difference in margin calibrations. On the PASCAL VOC 2012 dataset, the background class void occupies 80% of the total pixels in the image corpus, while each foreground class has a very similar number of pixels (around 1%). So applying the ğœŒ-margin calibration in Algorithm 2, the learning objective is more like a scaled log-loss.

Table 1 The overall segmentation performance on the robotic instrument test set
Full size table
Table 2 Per-class IoU on the robotic instrument test set
Full size table
Fig. 3
figure 3
Segmentation examples on the robotic instrument test set

Full size image
Results on MIT SceneParse150 Dataset
We experimented on the large-scale MIT SceneParse150 dataset to verify the effectiveness of the margin calibration method. Unlike the multi-task learning framework such as Xiao et al. (2018) to use multiple supervised information for the best segmentation performance, we only use the 150 scene labels and compare different single learning objectives in training the semantic segmentation model. The images were rescaled to 384Ã—384. On this dataset, we first used cross-entropy as the default learning objective to train the DeepLab v3+ model, then fine-tuned the network with focal loss, LovÃ¡sz-softmax and the proposed margin calibration independently, to see the performance improvement of mIoU scores. The ground truth of the testing set has not been released, so we use the 2,000 validation images for qualitative evaluation. In the model inference, we adopted horizontal flipping and multi-scale prediction to augment the segmentation performance.

The comparisons on the validation set with the three baseline methods are reported in Table 5. Results show that when fixing the deep neural network, just using the proposed margin calibration method as a single learning objective, can boost the mIoU in very complex scene parsing tasks. When applying the flipping and multi-scale prediction, fine-tuning with focal loss and LovÃ¡sz-softmax can improve the mIoU by 0.7% and 0.5%, respectively, while using the proposed margin calibration can achieve a further 0.9% of performance gain compared to categorical cross-entropy. Some example segmentation results in both indoor and outdoor environments are illustrated in Fig. 4. We can see compared to other learning objectives, applying margin calibration can better annotate the pillow, building exterior and sidewalk in these examples.

Table 3 The segmentation performance on the COCO-Stuff 10K validation set
Full size table
Table 4 The segmentation performance on the PASCAL VOC2012 validation set
Full size table
Table 5 The segmentation performance on the MIT SceneParse150 validation set
Full size table
Fig. 4
figure 4
Scene parsing examples on MIT SceneParse150 validation set

Full size image
Results on Cityscapes, BDD100K, and Mapillary Vistas Datasets
We experimented the DeepLab v3+ model with different learning objectives on three street-view datasets. Similar to the training on the MIT SceneParse150 dataset, we fine-tuned the network parameters based on the pre-trained model obtained from the best checkpoint using categorical cross-entropy. The models on these datasets were independently trained. On BDD100K and Mapillary Vistas datasets, the images were re-scaled to 1280Ã—720 with the crop-size 720Ã—720. On the Cityscapes dataset, the images were not re-scaled but with the crop-size 800Ã—800. The ground-truth of test images of the three datasets are withheld by the organizers. However, the model performance of Cityscapes can be tested by submitting the segmentation results to their evaluation server.

Table 6 The segmentation performance on the Cityscapes validation set (fine labels only)
Full size table
Table 7 The segmentation performance on the BDD100K validation set
Full size table
Tables 6, 7 and 8 show that using focal loss, LovÃ¡sz-softmax and margin calibration can all lead to higher mIoU scores on the three validation sets. Specifically, using margin calibration to fine-tune the pretraind segmentation model can beat the second-best by 0.5%, 0.3%, 0.5% on Cityscapes, BDD100K and Mapillary Vistas datasets, respectively. Focal loss is essentially a kind of dynamically scaled cross-entropy, where the scaling factor decays to zero as confidence in the correct class increases. This scaling factor can automatically down-weight the contribution of easy pixels during training and rapidly focus the model on hard pixels. So focal loss can generally replace cross-entropy in dense classification tasks. In our case, focal loss has a similar performance with LovÃ¡sz-softmax, which is specifically designed for IoU optimization. However, thanks to the theoretical guarantee of the error bound and the explicit consideration of label imbalance, our method can achieve even higher mIoU compared to focal loss and LovÃ¡sz-softmax.

Table 8 The segmentation performance on the mapillary vistas validation set
Full size table
Table 9 Online evaluation of Cityscapes testing set
Full size table
We submitted the prediction with defferent settings to the evaluation server of CityscapesFootnote2. The overall comparisons of our model with some recently proposed methods are summarized in Table 9. Note that our method mainly aims to optimize the mIoU measure named Class IoU in Cityscapes. From the table, we can see that training with margin calibration, a single deep segmentation model achieves very promising results. Without the pre-training using the 20,000 coarsely labelled images, simply replacing the cross-entropy with the proposed margin calibration, the mIoU can be improved by 1%. If the model is pre-traind with the coarsely labelled data then finetuned, the final mIoU can be further boosted by 0.6%. Compared to the original implementation of DeepLab v3+ in Chen et al. (2018), our segmentation model backend on SEResNeXt-50, which is a shallower network pre-traind on ImageNet-1K Russakovsky et al. (2015) but not the much larger JFT-300M Sun et al. (2017). Even so, with the margin calibration as a better learning objective, the final performance of our implementation is slightly better than the Deeplab v3+ backend on Aligned Xception. Some examplar segmentation results for the scene parsing visualization are illustrated in Fig. 5. We can see that fine-tuning with margin calibration can generally reduce false positives and lead to finer details.

Fig. 5
figure 5
Semantic segmentation examples on the Cityscapes validation set

Full size image
Conclusion
We have presented a versatile distribution-aware margin calibration method as a better learning method, to optimize the Jaccard index in image semantic segmentation. With the consideration of both empirical performance and the error bound, the scheme can increase the discriminative power with a better generalization ability. We gave both theoretical and experimental analysis to demonstrate its effectiveness, substantially improving the IoU scores by inserting it into a deep semantic segmentation network