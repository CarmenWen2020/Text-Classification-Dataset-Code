In this paper, we introduce a method to automatically reconstruct the 3D motion of a person interacting with an object from a single RGB video. Our method estimates the 3D poses of the person together with the object pose, the contact positions and the contact forces exerted on the human body. The main contributions of this work are three-fold. First, we introduce an approach to jointly estimate the motion and the actuation forces of the person on the manipulated object by modeling contacts and the dynamics of the interactions. This is cast as a large-scale trajectory optimization problem. Second, we develop a method to automatically recognize from the input video the 2D position and timing of contacts between the person and the object or the ground, thereby significantly simplifying the complexity of the optimization. Third, we validate our approach on a recent video + MoCap dataset capturing typical parkour actions, and demonstrate its performance on a new dataset of Internet videos showing people manipulating a variety of tools in unconstrained environments.

Introduction
People can easily learn how to break concrete with a sledgehammer or cut hay using a scythe by observing other people performing such tasks in instructional videos, for example. They can also easily perform the same task in a different context. This involves advanced visual intelligence capabilities such as recognizing and interpreting complex person–object interactions that achieve a specific goal. Understanding such complex interactions is a key to building autonomous machines that learn how to interact with the physical world by observing people.

This work makes a step in this direction and describes a method to estimate both the 3D motion and the actuation forces of a person manipulating an object given a single unconstrained video as input, as shown in Fig. 1. This is an extremely challenging task. First, there are inherent ambiguities in the 2D-to-3D mapping from a single view: multiple 3D human poses correspond to the same 2D input. Second, human–object interactions often involve contacts, resulting in discontinuities in the motion of the object and the human body part in contact. For example, one must place a hand on the hammer handle before picking the hammer up. The contact motion strongly depends on the physical quantities such as the mass of the object and the contact forces exerted by the hand, which renders modeling of contacts a very difficult task. Finally, the tools we consider in this work, such as hammer, scythe, or spade, are particularly difficult to recognize due to their thin structure, lack of texture, and frequent occlusions by hands and other parts of human body.

Fig. 1
figure 1
Our method automatically estimates the 3D motion and forces of object manipulation action from a single video. Top row: sample frames from an input video. Bottom row: the estimated person–object 3D motion and 6D contact forces (yellow arrows for linear forces, white arrows for torques)

Full size image
To address these challenges, we propose a method to jointly estimate the 3D trajectory of both the person and the object by visually recognizing contacts in the video and modeling the dynamics of the interactions. We focus on rigid stick-like hand tools (e.g. hammer, barbell, spade, scythe) with no articulation and approximate them as 3D line segments. Our key idea is that, when a human joint is in contact with an object, the object can be integrated as a constraint on the movement of the human limb. For example, the hammer in Fig. 1 provides a constraint on the relative depth between the person’s two hands. Conversely, 3D positions of the hands in contact with the hammer provide a constraint on the hammer’s depth and 3D rotation. To deal with contact forces, we integrate physics in the estimation by modeling dynamics of the person and the object. Inspired by recent progress in humanoid locomotion research (Carpentier and Mansard 2018b), we formulate person–object trajectory estimation as an optimal control problem given the contact state of each human joint. We show that contact states can be automatically recognized from the input video using a deep neural network. Our code and data is available at https://www.di.ens.fr/willow/research/motionforcesfromvideo/.

Related Work
Here we review the key areas of related work in both computer vision and robotics literature.

Single-view 3D pose estimation aims to recover the 3D joint configuration of the person from the input image. Recent human 3D pose estimators either attempt to build a direct mapping from image pixels to the 3D joints of the human body or break down the task into two stages: estimating pixel coordinates of the joints in the input image and then lifting the 2D skeleton to 3D. Existing direct approaches either rely on generative models to search the state space for a plausible 3D skeleton that aligns with the image evidence (Sidenbladh et al. 2000; Gammeter et al. 2008; Gall et al. 2010) or, more recently, extract deep features from images and learn a regressor from the 2D image to the 3D pose (Kanazawa et al. 2018; Moreno-Noguer 2017; Pavlakos et al. 2017; Tekin et al. 2016). The models can be further extended to learn 3D human dynamics from 2D in-the-wild video data (Kanazawa et al. 2019).

Building on the recent progress in 2D human pose estimation (Newell et al. 2016, 2017; Insafutdinov et al. 2016; Cao et al. 2017), two-stage methods have been shown to be effective (Akhter and Black 2015; Zhou et al. 2016; Bogo et al. 2016; Chen and Ramanan 2017) achieving competitive results (Martinez et al. 2017; Xiang et al. 2019) on 3D human pose benchmarks (Ionescu et al. 2014). The output can have an impressive level of detail including face deformations and position of individual fingers (Xiang et al. 2019). To deal with depth ambiguities, these estimators rely on good pose priors, which are either hand-crafted or learnt from large-scale MoCap data (Zhou et al. 2016; Bogo et al. 2016; Kanazawa et al. 2018; Kocabas et al. 2020). Others have looked at incorporating physical constraints. Examples include incorporating geometric constraints representing the proximity to the ground plane or collisions between different people (Zanfir et al. 2018), or, closer to our work, modelling the dynamics of the human motion and the contacts with the ground (Rempe et al. 2020; Shimada et al. 2020). However, unlike our work, these methods do not consider explicit physical models for 3D interactions between the person and the handled object.

Understanding human–object interactions involves both recognition of actions and modeling of interactions. In action recognition, most existing approaches that model human–object interactions do not consider 3D, instead model interactions and contacts in the 2D image space (Gupta et al. 2009; Delaitre et al. 2011; Yao and Fei-Fei 2012; Prest et al. 2013). Recent works in scene understanding Jiang et al. (2013), Fouhey et al. (2014) consider interactions in 3D but have focused on static scene elements rather than manipulated objects as we do in this work. Tracking 3D poses of people interacting with the environment has been demonstrated for bipedal walking (Brubaker et al. 2007, 2009) or in sports scenarios (Wei and Chai 2010). However, these works do not consider interactions with objects. Furthermore, Wei and Chai (2010) requires manual annotation of the input video.

There is also related work on modeling person–object interactions in robotics (Tassa et al. 2012) and computer animation (Boulic et al. 1990). Similarly to people, humanoid robots interact with the environment by creating and breaking contacts (Herdt et al. 2010), for example, during walking. Typically, generating artificial motion is formulated as an optimal control problem, transcribed into a high-dimensional numerical optimization problem, seeking to minimize an objective function under contact and feasibility constraints (Diehl et al. 2006; Schultz and Mombaur 2010). A known difficulty is handling the non-smoothness of the resulting optimization problem introduced by the creation and breaking of contacts (Westervelt et al. 2003). Due to this difficulty, the sequence of contacts is often computed separately and not treated as a decision variable in the optimizer (Kuffner et al. 2005; Tonneau et al. 2018a). Recent work has shown that it may be possible to decide both the continuous movement and the contact sequence together, either by implicitly formulating the contact constraints (Posa et al. 2014) or by using invariances to smooth the resulting optimization problem (Mordatch et al. 2012; Winkler et al. 2018).

In this paper, we take advantage of rigid-body models introduced in robotics and formulate the problem of estimating 3D person–object interactions from monocular video as an optimal control problem under contact constraints. We overcome the difficulty of contact irregularity by first identifying the contact states from the visual input, and then localizing the contact points in 3D via our trajectory estimator. This allows us to treat multi-contact sequences (like walking) without manually annotating the contact phases.

Object 3D pose estimation methods often require depth or RGB-D data as input (Tejani et al. 2014; Doumanoglou et al. 2016; Hinterstoisser et al. 2016), which is restrictive since depth information is not always available (e.g. for outdoor scenes or specular objects), as is the case of our instructional videos. Recent work has also attempted to recover object pose from RGB input only (Brachmann et al. 2016; Rad and Lepetit 2017; Xiang et al. 2017; Li et al. 2018; Oberweger et al. 2018; Grabner et al. 2018; Rad et al. 2018). However, we found that the performance of these methods is limited for the stick-like objects we consider in this work. Instead, we recover the 3D pose of the object via localizing and segmenting the object in 2D, and then jointly recovering the 3D trajectory of both the human limbs and the object. As a result, both the object and the human pose help each other to improve their joint 3D trajectory by leveraging the contact constraints.

Instructional videos Our work is also related to recent efforts in learning from Internet instructional videos (Malmaud et al. 2015; Alayrac et al. 2016) that aim to segment input videos into clips containing consistent actions. In contrast, we focus on extracting a detailed representation of the object manipulation in the form of a 3D person–object trajectory with contacts and underlying interaction forces.

Fig. 2
figure 2
Overview of the proposed method. In recognition stage (orange box, b–d), the system estimates from the input video (a) the locations of person’s 2D joints (b), the locations of 2D endpoints of the tool (d), and contact states of the individual joints (c). The human joints and the object endpoints are visualized as colored dots in the image. Human joints recognized as in contact are shown in green in (c), joints not in contact in red. In estimation stage (blue box, e), these image measurements are fused in a trajectory estimator to recover the human and object 3D motion together with the contact positions and forces (shown as yellow arrows)

Full size image
Approach Overview
We are given a video clip of a person manipulating an object or in another way interacting with the scene. Our approach, illustrated in Fig. 2, receives as input a sequence of frames and automatically outputs the 3D trajectories of the human body, the manipulated object, and the ground plane. At the same time, it localizes the contact points and recovers the contact forces that actuate the motion of the person and the object. Our approach proceeds along two stages. In the first stage, the recognition stage, we extract 2D measurements from the input video. These consist of 2D locations of human joints, 2D locations of a small number of predefined object endpoints, and contact states of selected joints over the course of the video. In the second stage, the estimation stage, these image measurements are then fused in order to estimate the 3D motion, 3D contacts, and the controlling forces of both the person and the object. The person and object trajectories, contact positions, and contact forces are jointly constrained by our carefully designed contact motion model, force model, and dynamics equations.

The proposed problem is difficult, yet feasible to solve under a number of reasonable assumptions on the physical properties of the person, the manipulated object and the scene. First of all, we assume that there is at most one person that appears in the input video. We adopt the mass properties of the full-body anatomical human model described in (Maldonado 2018). This model captures the body weight statistics of an average human adult. Our approach applies the same body mass distribution to any input video. If there is an object manipulated by the person, we assume that the object is rigid, non-articulated and has a stick-like shape. We apply a single object mass distribution to any input video with the same type of object. For example, we assume that all sledgehammers share the same head weight. Our method can also handle input videos without the manipulated object. In this case, we only model contacts between the person and the ground. We further assume the camera is static with canonical (or known) intrinsic parameters. Most body joints, especially the ones that may interact with the environment (e.g. hands, feet, knees, etc) should be visible at least in a short period of time in the input video. We assume that the gravity is perpendicular to the ground plane, but the model can be tuned to fit other cases such as a sloping ground. In the subsequent sections, we will include an object model in our formulation, but as discussed above the object is not necessary for the model to be applied.

In the following, we start in Sect. 4 by describing the estimation stage giving details of the formulation as an optimal control problem. Then, in Sect. 5 we give details of the recognition stage including 2D human pose estimation, contact recognition, and object 2D endpoint estimation. Finally, we describe results including the failure modes in Sect. 6.

Estimating Person–Object Trajectory Under Contact and Dynamics Constraints
We assume that we are provided with a video clip of duration T depicting a human subject manipulating an object. We encode the 3D poses of the human and the object, including joint translations and rotations, in the configuration vectors 𝑞h and 𝑞o, for the human and the object respectively. We define a constant set of K contact points between the human body and the object (or the ground plane). Each contact point corresponds to a human segment, and is activated whenever that human segment is recognized as in contact. At each contact point, we define a contact force 𝑓𝑘, whose value is non-zero whenever the contact point k is active. The state of the complete dynamical system is then obtained by concatenating the human and the object joint configurations q and velocities 𝑞˙ as 𝑥:=(𝑞h,𝑞o,𝑞˙h,𝑞˙o). Let 𝜏hm be the joint torque vector describing the actuation by human muscles. This is a 𝑛𝑞−6 dimensional vector where 𝑛𝑞 is the dimension of the human body configuration vector. We define the control variable u as the combination of the joint torque vector together with the contact forces at the K contact point, 𝑢:=(𝜏hm,𝑓𝑘,𝑘=1,…,𝐾). To deal with sliding contacts, we further define a contact state c that consists of the relative positions of all the contact points with respect to the object (or ground) in the 3D space.

Our goal is two-fold. We wish to (i) estimate smooth and consistent human–object and contact trajectories 𝑥⎯⎯ and 𝑐⎯⎯, while (ii) recovering the control 𝑢⎯⎯ which gives rise to the observed motion.Footnote1 This is achieved by jointly optimizing the 3D trajectory 𝑥⎯⎯, contacts 𝑐⎯⎯, and control 𝑢⎯⎯ given the measurements (2D positions of human joints and object endpoints together with contact states of human joints) obtained from the input video. The intuition is that the human and the object’s 3D poses should match their respective projections in the image while their 3D motion is linked together by the recognized contact points and the corresponding contact forces. In detail, we formulate person–object interaction estimation as an optimal estimation problem with contact and dynamics constraints:

minimize𝑥⎯⎯,𝑢⎯⎯,𝑐⎯⎯∑𝑒∈{h,o}∫𝑇0𝑙𝑒(𝑥,𝑢,𝑐)d𝑡,
(1)
subject to𝜅(𝑥,𝑐)=0(contact motion model),
(2)
𝑥˙=𝑓(𝑥,𝑐,𝑢)(full-body dynamics),
(3)
𝑢∈(force model),
(4)
where e denotes either ‘h’ (human) or ‘o’ (object), and the constraints (2)–(4) must hold for all 𝑡∈[0,𝑇]. The loss function 𝑙𝑒 is a weighted sum of multiple costs capturing (i) the data term measuring simultaneously the consistency between the observed and re-projected 2D joint and object endpoint positions and the discrepancy of the estimated 3D joint positions with respect to some reference positions, (ii) the prior on the human 3D poses, (iii) the physical plausibility of the motion and (iv) the temporal smoothness of the estimated trajectory. Next, we describe these cost terms as well as the insights leading to their design choices. For simplicity, we ignore the superscript e when introducing a cost term that exists for both the human 𝑙h and the object 𝑙o component of the loss. We describe the individual terms using continuous time notation as used in the overall problem formulation (1). A discrete version of the problem as well as the optimization and implementation details are relegated to Sect. 4.5.

Data Term: Enforcing 2D and 3D Consistency
Given the 2D locations of human joints and object endpoints predicted from image, we wish to optimize a 3D pose trajectory that consolidates these 2D measurements. This is done by minimizing the re-projection error of the estimated 3D human joints and 3D object endpoints with respect to the 2D measurements obtained in each video frame. In detail, let 𝑗=1,…,𝑁 be human joints or object endpoints and 𝑝2D𝑗 their 2D position observed in the image. We minimize the 2D consistency loss 𝑙2D:

𝑙2D=∑𝑗𝜌(𝑝2D𝑗−𝑃cam(𝑝𝑗(𝑞))),
(5)
where 𝑃cam is the camera projection matrix and 𝑝𝑗 the 3D position of joint or object endpoint j induced by the person–object configuration vector q. To deal with outliers, we use the robust Huber loss, denoted by 𝜌.

In addition, we employ a direct 3D consistency loss if a reference 3D pose trajectory is available:

𝑙3D=∑𝑗𝜌(𝑝3D𝑗−𝑝𝑗(𝑞)),
(6)
where 𝑝3D𝑗 denotes the reference 3D position of joint j. In our case, the reference human 3D poses are computed using the HMR estimator (Kanazawa et al. 2018). But it is possible to use other pose estimators instead.

In practice, we find that minimizing a weighted sum of the 2D and 3D consistency losses achieves good performance. The data term is finally expressed as:

𝑙data=𝑤2D𝑙2D+𝑤3D𝑙3D,
(7)
where 𝑤2D and 𝑤2D are non-negative scalars.

Prior on 3D Human Poses
A single 2D skeleton can be a projection of multiple 3D poses, many of which are unnatural or impossible exceeding the human joint limits. To resolve this, we incorporate into the human loss function 𝑙h a pose prior similar to Bogo et al. (2016). The pose prior is obtained by fitting the SMPL human model (Loper et al. 2015) to the CMU MoCap dataset using MoSh (Loper et al. 2014) and fitting a Gaussian Mixture Model (GMM) to the resulting SMPL 3D poses. We map our human configuration vector 𝑞h to a SMPL pose vector 𝜃 and compute the likelihood under the pre-trained GMM

𝑙hpose=−log(𝑝(𝑞h;GMM)).
(8)
During optimization, 𝑙hpose is minimized in order to favor more plausible human poses against rare or impossible ones.

Physical Plausibility of the Motion
Human–object interactions involve contacts coupled with interaction forces, which are not included in the data-driven cost terms (7) and (8). Modeling contacts and physics is thus important to reconstruct object manipulation actions from the input video. Next, we outline models for describing the motion of the contacts and the forces at the contact points. Finally, the contact motions and forces, together with the system state 𝑥⎯⎯, are linked by the laws of mechanics via the dynamics equations, which constrain the estimated person–object interaction. This full body dynamics constraint is detailed at the end of this subsection.

Contact motions In the recognition stage, our contact recognizer predicts, given a human joint (for example, left hand, denoted by j), a sequence of contact states 𝛿𝑗:𝑡⟶{1,0}. Similarly to Carpentier and Mansard (2018b), we call a contact phase any time segment in which j is in contact, i.e., 𝛿𝑗=1. Our key idea is that the 3D distance between human joint j and the active contact point on the object (denoted by k) should remain zero during a contact phase:

‖‖𝑝h𝑗(𝑞h)−𝑝c𝑘(𝑥,𝑐)‖‖=0(point contact),
(9)
where 𝑝h𝑗 and 𝑝c𝑘 are the 3D positions of joint j and object contact point k, respectively. Note that position of the object contact point 𝑝c𝑘(𝑥,𝑐) depends on the state vector x describing the human–object configuration and the relative position c of the contact along the object. The position of contact 𝑝c𝑘 is subject to a feasible range denoted by . For stick-like objects such as hammer,  is approximately the 3D line segment representing the handle. For the ground, the feasible range  is a 3D plane. In practice, we implement 𝑝c𝑘∈ by putting a constraint on the trajectory of relative contact positions 𝑐⎯⎯.

Equation (9) applies to most common cases where the contact area can be modeled as a point. Examples include the hand-handle contact and the knee-ground contact. To model the planar contact between the human sole and ground, we approximate each sole surface as a planar polygon with four vertices, and apply the point contact model at each vertex. In our human model, each sole is attached to its parent ankle joint, and therefore the four vertex contact points of the sole are active when 𝛿ankle=1.

The resulting overall contact motion function 𝜅 in problem (1) is obtained by unifying the point and the planar contact models:

𝜅(𝑥,𝑐)=∑𝑗∑𝑘∈𝜙(𝑗)𝛿𝑗‖‖𝑇(𝑘𝑗)(𝑝h𝑗(𝑞h))−𝑝c𝑘(𝑥,𝑐)‖‖,
(10)
where the external sum is over all human joints. The internal sum is over the set of active object contact points mapped to their corresponding human joint j by mapping 𝜙(𝑗). The mapping 𝑇(𝑘𝑗) translates the position of an ankle joint j to its corresponding k-th sole vertex; it is an identity mapping for non-ankle joints.

Contact forces During a contact phase of the human joint j, the environment exerts a contact force 𝑓𝑘 on each of the active contact points in 𝜙(𝑗). 𝑓𝑘 is always expressed in contact point k’s local coordinate frame. We distinguish two types of contact forces: (i) 6D spatial forces exerted by objects and (ii) 3D linear forces due to ground friction. In the case of object contact, 𝑓𝑘 is an unconstrained 6D spatial force with 3D linear force and 3D moment. In the case of ground friction, 𝑓𝑘 is constrained to lie inside a 3D friction cone 3 [also known as the quadratic Lorentz “ice-cream” cone (Carpentier and Mansard 2018b)] characterized by a positive friction coefficient 𝜇. In practice, we approximate 3 by a 3D pyramid spanned by a basis of 𝑁=4 generators, which allows us to represent 𝑓𝑘 as the convex combination 𝑓𝑘=∑𝑁𝑛=1𝜆𝑘𝑛𝑔(3)𝑛, where 𝜆𝑘𝑛≥0 and 𝑔(3)𝑛 with 𝑛=1,2,3,4 are the 3D generators of the contact force. We sum the contact forces induced by the four sole-ground contact points and express a unified contact force in the ankle’s frame:

𝑓𝑗=∑𝑘=14(𝑓𝑘𝑝𝑘×𝑓𝑘)=∑𝑘=14∑𝑛=1𝑁𝜆𝑗𝑘𝑛𝑔(6)𝑘𝑛,
(11)
where 𝑝𝑘 is the position of contact point k expressed in joint j’s (left/right ankle) frame, × is the cross product operator, 𝜆𝑗𝑘𝑛≥0, and 𝑔(6)𝑘𝑛 are the 6D generators of 𝑓𝑗. Please see Appendix B for additional details including the expressions of 𝑔(3)𝑛 and 𝑔(6)𝑘𝑛.

Full body dynamics The full-body movement of the person and the manipulated object is described by the Lagrange dynamics equation:

𝑀(𝑞)𝑞¨+𝑏(𝑞,𝑞˙)=𝑔(𝑞)+𝜏,
(12)
where M is the generalized mass matrix, b covers the centrifugal and Coriolis effects, g is the generalized gravity vector and 𝜏 represents the joint torque contributions. 𝑞˙ and 𝑞¨ are the joint velocities and joint accelerations, respectively. Note that (12) is a unified equation which applies to both human and object dynamics, hence we drop the superscript e here. Only the expression of the joint torque 𝜏 differs between the human and the object and we give the two expressions next.

For human, it is the sum of two contributions: the first one corresponds to the internal joint torques (exerted by the muscles for instance) and the second one comes from the contact forces:

𝜏h=(06𝜏hm)+∑𝑘=1𝐾(𝐽h𝑘)𝑇𝑓𝑘,
(13)
where 𝜏hm is the human joint torque exerted by muscles, 𝑓𝑘 is the contact force at contact point k and 𝐽h𝑘 is the Jacobian mapping human joint velocities 𝑞˙h to the Cartesian velocity of contact point k expressed in k’s local frame. Let 𝑛h𝑞 denote the dimension of 𝑞h, 𝑞˙h and 𝑞¨h, then 𝜏hm and 𝐽h𝑘 are of dimension 𝑛ℎ𝑞−6 and 3×𝑛ℎ𝑞, respectively. We model the human body and the object as two free-floating base systems. In the case of human body, the six first entries in the configuration vector q correspond to the 6D pose of the free-floating base (translation + orientation), which is not actuated by any internal actuators such as human muscles. This constraint is taken into consideration by adding the zeros in Eq. (13).

In the case of the manipulated object, there is no actuation other than the contact forces exerted by the human. Therefore, the object torque is expressed as

𝜏o=−∑object contact 𝑘(𝐽o𝑘)𝑇𝑓𝑘,
(14)
where the sum is over the object contact points, 𝑓𝑘 is the contact force, and 𝐽o𝑘 denotes the object Jacobian, which maps from the object joint velocities 𝑞˙o to the Cartesian velocity of the object contact point k expressed in k’s local frame. 𝐽o𝑘 is a 3×𝑛o𝑞 matrix where 𝑛o𝑞 is the dimension of object configuration vectors 𝑞o, 𝑞˙o and 𝑞¨o.

We concatenate the dynamics equations of both human and object to form the overall dynamics in Eq. (3) in problem (1), and include a muscle torque term 𝑙htorque=‖𝜏hm‖2 in the overall cost. Minimizing the muscle torque acts as a regularization over the energy consumption of the human body.

Enforcing the Trajectory Smoothness
Regularizing human and object motion Taking advantage of the temporal continuity of video, we minimize the sum of squared 3D joint velocities and accelerations to improve the smoothness of the person and object motion and to remove incorrect 2D poses. We include the following motion smoothing term to the human and object loss in (1):

𝑙smooth=∑𝑗(‖‖𝜈𝑗(𝑞,𝑞˙)‖‖2+‖‖𝛼𝑗(𝑞,𝑞˙,𝑞¨)‖‖2),
(15)
where 𝜈𝑗 and 𝛼𝑗 are the spatial velocity and the spatial accelerationFootnote2 of joint j, respectively. In the case of object, j represents an endpoint on the object. By minimizing 𝑙smooth, both the linear and angular movements of each joint/endpoint are smoothed simultaneously.

Regularizing contact motion and forces In addition to regularizing the motion of the joints, we also regularize the contact states and control by minimizing the velocity of the contact points, the temporal variation of the contact forces and the magnitude of the contact forces. The is implemented by including the following contact smoothing term in the cost function in problem (1):

𝑙csmooth=∑𝑗∑𝑘∈𝜙(𝑗)(𝜔𝑘‖𝑐˙𝑘‖2+𝛾𝑘‖𝑓˙𝑘‖2+𝜁𝑘‖𝑓𝑘‖2),
(16)
where 𝑐˙𝑘 and 𝑓˙𝑘 represent, respectively, the temporal variation of the position and the contact force at contact point k. 𝑓𝑘 is the contact force at contact point k. 𝜔𝑘, 𝛾𝑘 and 𝜁𝑘 are scalar weights of the regularization terms. Note that some contact points, for example the four contact points of the human sole during the sole-ground contact, should remain fixed with respect to the object or the ground during the contact phase. To tackle this, we use a higher 𝜔𝑘 for sole contact points to prevent the foot sole form sliding. We also found important to use higher 𝜁𝑘 for hand contact forces and smaller 𝜁𝑘 for ground contact forces to favor larger ground contact forces when both hand and ground contacts are recognized.

Optimization
Conversion to a numerical optimization problem We convert the continuous problem (1) into a discrete nonlinear optimization problem using the collocation approach (Biegler 2010). All trajectories are discretized and the constraints (2), (3), (4) are only enforced on the “collocation” nodes of a time grid matching the discrete sequence of video frames. The optimization variables are the sequence of human and object poses [𝑥0…𝑥𝑇], torque and force controls [𝑢1…𝑢𝑇], contact locations [𝑐0…𝑐𝑇], and the ground plane. We replace the integral in the objective function by a sum over video frames, and rewrite the cost and constraint terms which include derivatives of the state (e.g. joint accelerations) by approximating the derivatives with the backward finite difference scheme (e.g. 𝑎𝑡:=(𝑣𝑡−𝑣𝑡−1)/𝛥𝑡, with 𝛥𝑡 the duration between two video frames). The resulting problem is nonlinear, constrained and sparse (due to the sequential structure of trajectory optimization).

Problem sparsity The problem after discretization becomes a large, sparse and non-linear optimization problem. This is because the discretized objective function becomes a sum of terms that each depend on one time sample (denoted by i) and a subset of the variables [𝑥𝑖,𝑢𝑖,𝑐𝑖] corresponding to i. Only a few regularization terms, e.g. the motion smoothing term (15) and the contact smoothing term (16), may depend on two or three successive frames. The problem sparsity is important to take into account, as it significantly reduces the complexity of computation from (𝑇3) (without sparsity) to (𝑇) (using the problem sparsity).

Solving the problem We solve the problem using the Levenberg-Marquardt algorithm. We rely on the Ceres solver (Agarwal et al. 2012), which is dedicated to solving sparse estimation problems (e.g. bundle adjustment (Triggs et al. 1999)), and on the Pinocchio software (Carpentier et al. 2019, 2015–2019) for the efficient computation of kinematic and dynamic quantities and their derivatives (Carpentier and Mansard 2018a). As Ceres solver only allows to define bound constraints, hence we implement our nonlinear constraints as penalties in the cost function.

Multi-stage optimization In practice, we find that solving the optimization problem all at once usually leads to poor local minima. Instead we design a multi-stage optimization strategy taking inspiration in multi-stage optimization used for planning motion of humanoid robots (Tonneau et al. 2018b; Carpentier et al. 2017). In detail, we solve a cascade of sub-problems composed of four stages.

In stage 1, we solve the discretized version of problem (1) only for the person’s kinematic variables (𝑞h, 𝑞˙h, 𝑞¨h) by “freezing” all variables and constraints related to the object, the ground plane, and the dynamics in Eqs. (3) and (4). This gives us a rough estimate of the person’s 3D trajectory.

In stage 2, we recover the 3D position of the ground plane given the estimated 3D trajectory of the person and the contact states recognized from the video sequence. In detail, we “unfreeze” the 3D position 𝑞g ground plane and jointly solve for the trajectory of the person 𝑞h and the position of the ground plane 𝑞g.

Stage 3 is dedicated to initializing the object’s 3D trajectory. This is achieved by solving for the object’s kinematic variables (𝑞o, 𝑞˙o, 𝑞¨o) under the contact constraints, while keeping the other variables fixed. Note that the location of the manipulated object varies significantly across the Handtool dataset. To address this, we sample four initialization options with different pre-defined 3D object orientations. We run stage 3 of the optimization for each initialization and pick among the four resulting solutions the one with the lowest cost.

Finally, in stage 4, we solve for the complete set of kinematic and control variables all at once, starting from the values provided by the previous stages. It is possible to continue improving the solution by pursing the aforementioned alternative descent scheme, but we found that a single pass was already sufficient to obtain good results.

Setting hyper-parameters Hyper-parameters of our trajectory estimator, including the weights used for the cost terms, the camera model, the number of iterations, etc., are determined by following a combination of manual adjustment and a grid search: given a parameter of interest and a search grid, we run the optimization on a set of validation videos with known ground-truth 3D motion, evaluate the joint errors at every grid point, and update the hyper-parameter with the value leading to the lowest error. The same process is repeated in an iterative manner for the different hyper-parameters until the model outputs reasonable results on all the validation videos.

Run time We report run time of trajectory optimization on a MacBook Pro 2016 (with 2.9GHz Intel Core i5 and 8GB memory). The optimization takes on average 3.23 seconds per frame. In detail, the four stages of the optimization, from stage 1 to stage 4, take on average 0.40, 0.02, 0.31 and 2.50 seconds per frame, respectively. When the pose of the object is not modeled, which is the case of one of our datasets introduced in the experimental section, the optimization is faster as stage 3 is skipped. By default, the optimization is run on the whole input video (around 100 frames in our datasets). We also provide an interface for running the optimization in a sliding window manner, which allows applying our method on longer videos.

Extracting 2D Measurements from Video
In this section, we describe how 2D measurements are extracted from the input video frames during the first, recognition stage of our system. In particular, we extract the 2D human joint positions, the 2D object endpoint positions and the contact states of human joints.

Estimating 2D positions of human joints We use the state-of-the-art Openpose (Cao et al. 2017) human 2D pose estimator, which achieved excellent performance on the MPII Multi-Person benchmark (Andriluka et al. 2014). Taking a pre-trained Openpose model, we do a forward pass on the input video in a frame-by-frame manner to obtain an estimate of the 2D trajectory of human joints, 𝑝h,2D𝑗.

Fig. 3
figure 3
The main contact recognition steps. Given estimated 2D human joints, we crop image patches around a set of joints of interest, which includes neck, hands, knees, foot soles and toes. Based on the type of human joint, we feed each image patch to the corresponding CNN to predict whether the joint appearing in the patch is in contact (shown in green on the right) or not (shown in red) with the environment

Full size image
Recognizing contacts We wish to recognize and localize contact points between the person and the manipulated object or the ground. This is a challenging task due to the large appearance variation of the contact events in the video. However, we demonstrate here that a good performance can be achieved by training a contact recognition CNN module from manually annotated contact data that combine both still images and videos harvested from the Internet. In detail, the contact recognizer operates on the 2D human joints predicted by Openpose. As shown in Fig. 3, given 2D joints at video frame i, we crop fixed-size image patches around a set of joints of interest, which may be in contact with an object or ground. Based on the type of human joint, we feed each image patch to the corresponding CNN to predict whether the joint appearing in the patch is in contact or not. The output of the contact recognizer is a sequence 𝛿𝑗𝑖 encoding the contact states of human joint j at video frame i, i.e. 𝛿𝑗𝑖=1 if joint j is in contact at frame i and zero otherwise. Note that 𝛿𝑗𝑖 is the discretized version of the contact state trajectory 𝛿𝑗 presented in Sec. 4.

Our contact recognition CNNs are built by replacing the last layer of an ImageNet pre-trained Resnet model (He et al. 2016) with a fully connected layer that has a binary output. We have trained separate models for five types of joints: hands, knees, foot soles, toes, and neck. To construct the training data, we collect still images of people manipulating tools using Google image search. We also collect short video clips of people manipulating tools from Youtube in order to also have non-contact examples. We run Openpose pose estimator on this data, crop patches around the 2D joints, and annotate the resulting dataset with contact states.

Estimating 2D object pose The objective is to estimate the 2D position of the manipulated object in each video frame. To achieve this, we build on instance segmentation, computed by Mask R-CNN (He et al. 2017). We train Mask R-CNN separately for each object class (i.e., barbell, hammer, scythe and spade) and apply it to the corresponding Handtool dataset videos. Using the inferred segmentation masks and bounding boxes, we estimate the 2D location of the object endpoints (i.e. its two extremities) in each frame. The resulting 2D endpoint coordinates are used as an input to the trajectory optimizer. Details are given next.

In order to generate training data for the instance segmentation, we used two different approaches. In the case of barbell, hammer and scythe, we created a 3D model for each object class (i.e. one model for all barbell instances, for example), roughly approximating the shape of the corresponding object instances in the Handtool dataset videos, and computed the mask of the model shape in 2D from multiple viewpoints using a perspective camera. For spade, we collected a small number (13) of still images capturing different instances of person-spade manipulation similar to those in the Handtool dataset, and annotated 2D masks of the spade in them. Then we augmented the resulting 2D shape masks to train a separate Mask R-CNN model for each object class. In order to handle the variation of object poses in the videos, we augmented the training set by random 2D geometric transformations (translation, rotation, scale, flip). In addition, to handle the intra-class variation of instance surface appearance as well as changes caused by illumination, we applied domain randomization (Loing et al. 2018; Tobin et al. 2017): the geometrically transformed 2D mask was filled with a random (foreground) image and pasted on another random (background) image; the random images were taken from the MS COCO dataset (Lin et al. 2014). Starting with a Mask R-CNN (Abdulla 2017) model pre-trained on the MS COCO dataset, we train a separate model for each object class by fine-tuning the head layers using the corresponding augmented training set.

At test time, we use the segmentation masks and bounding boxes from the trained Mask R-CNN to estimate the 2D coordinates of the object endpoints. In our set-up, the Mask R-CNN is constrained to output no more than one segmented instance per image frame. The endpoints are calculated as the intersection of a line fitted through the segmentation mask (estimate of object’s main axis) and the bounding box (estimate of object’s extremities). However, we discard the endpoints if the distance of either wrist joint from the line segment between the endpoints is larger than a threshold (incorrect segmentation of the manipulated object). The relative orientation of the object (i.e. which endpoint corresponds to the “head” of the tool and which to its “handle”, for example) is determined by the relative proximity of each endpoint to the wrist joints (hammer) or by the relative spatial location of the endpoints in the video frames (barbell, scythe, spade). Figure 4 illustrates the output of our object localization and endpoint detection.

Fig. 4
figure 4
Detecting and localizing objects in video frames. Example qualitative results on the Handtool dataset. Left: Input video frame (top to bottom: barbell, hammer, scythe, spade). Right: Output object mask (magenta) and object endpoints (yellow and cyan circles, corresponding to the “head” and the “handle” of the tool, respectively, where applicable)

Full size image
Table 1 Mean per joint position error (in mm) of the recovered 3D motion for each action on the Parkour dataset
Full size table
Table 2 Estimation errors of the contact forces exerted on soles and hands on the Parkour dataset
Full size table
Experiments
In this section we present quantitative and qualitative evaluation of the reconstructed 3D person–object interactions. Since we recover not only human poses but also object poses and contact forces, evaluating our results is difficult due to the lack of ground truth forces and 3D object poses in standard 3D pose benchmarks such as Ionescu et al. (2014). Consequently, we evaluate our motion and force estimation quantitatively on a recent Biomechanics video/MoCap dataset capturing challenging dynamic parkour motions (Maldonado et al. 2017). In addition, we report joint errors on our newly collected dataset of videos depicting handtool manipulation actions. Furthermore, we show qualitative results on both datasets to demonstrate the quality of our motion/force estimation. Finally, we discuss the main failure modes of our method at the end of the section.

Parkour Dataset
This dataset contains RGB videos capturing human subjects performing four typical parkour actions: kong-vault, moving-up, pull-up and safety-vault. These are highly dynamic motions with rich contact interactions with the environment. Half of the videos in the dataset are provided with ground truth 3D motion and contact forces captured with a Vicon motion capture system and force sensors. Due to the blur of fast motion in the parkour actions, this dataset is challenging for computer vision algorithms.

Evaluation set-up We evaluate our method on the 28 parkour sequences with ground truth 3D motion and contact forces, while the remaining videos are used for training the contact recognizer. We evaluate the accuracy of the recovered 3D human poses using the common approach of computing the mean per joint position error (MPJPE) of the estimated 3D pose with respect to the ground truth after rigid alignment (Gower 1975). For evaluating contact forces we express the estimated and the ground truth 6D forces at the position of the contact aligned with the world coordinate frame provided in the dataset. We split the 6D force into linear and moment components and report the average Euclidean distance of the linear force and the moment with respect to the ground truth.

Table 3 Mean per joint position error (in mm) of the recovered 3D human poses for each tool type on the Handtool dataset
Full size table
Table 4 Ablation of the 3D data term (6)
Full size table
Table 5 Ablation of force regularization terms (Eq. (16))
Full size table
Table 6 The percentage of endpoints for which the estimated 2D location lies within 25/50/100 pixels (in 600×400 pixel image) from the manually annotated ground truth location
Full size table
Fig. 5
figure 5
Example qualitative results on the Handtool (rows 1-4) and Parkour (rows 5–6) datasets. Top-to-bottom: hammer, barbell, scythe, spade, muscle-up and pull-up. Each example shows the input frame (left) and two different views of the output 3D pose of the person and the object (middle, right). The yellow and the white arrows in the output show the contact forces and moments, respectively. The length of the arrow represents the magnitude of the force normalized by gravity

Full size image
Fig. 6
figure 6
Example qualitative results on image sequences. Columns 1–3: muscle-up (Parkour dataset), Columns 4–6: hammer (Handtool dataset). Please see Project webpage (2021) for additional video results

Full size image
Results We report joint errors for different actions in Table 1 and compare results with the HMR (Kanazawa et al. 2018) method, which is used to warm-start our method. To make it a fair comparison, we use the same Openpose 2D joints as input. In addition, we evaluate the SMPLify (Bogo et al. 2016) 3D pose estimation method. We also compare results with the previous version of this work (Li et al. 2019), which uses slightly different regularization of the estimated trajectory and forces. We report results for two variants of our approach. The first variant (“generic model”) uses the same hyperparameters of the cost-function for all actions. The second variant (“action-specific models”) uses action-specific hyperparameters adapted for each action (e.g. to regularize more strongly the motion of the legs in actions where legs are not used). Starting from the hyper-parameters of the generic model, the action-specific hyper-parameters are obtained by performing grid search, as described in Sect. 4.5 but here using validation videos of only one action class. The results show that our generic model outperforms all the baseline methods by more than 10mm on average on this challenging data, and that our action-specific models always achieve better performance on the corresponding actions compared to the baselines.

The force estimation results are summarized in Table 2 where we also report results of the previous version of this work (Li et al. 2019), which produces similar results. We observe higher errors of the estimated moments at hands (compared to soles), which we believe is due to the challenging nature of the Parkour sequences where the entire person’s body is often supported by hands. In this case, the hand may exert significant force and torque to support the body, and a minor shift in the force direction may lead to significant errors. In Fig. 7, we also show an example of temporal evolution of the magnitude of the estimated linear force and torque compared with the ground truth coming from the force sensors. The estimates correspond fairly well to the ground truth. We believe the spurious peak in the estimate around frame 40 is due to the error in contact recognition, which produces a spurious linear force and a small torque.

Handtool Dataset
In addition to the Parkour data captured in a controlled set-up, we would like to demonstrate generalization of our approach to the “in the wild” Internet instructional videos. For this purpose, we have collected a dataset of object manipulation videos, which we refer to as the Handtool dataset. The dataset contains videos of people manipulating four types of tools: barbell, hammer, scythe, and spade. For each type of tool, we chose among the top videos returned by YouTube five videos covering a range of actions. We then cropped short clips from each video showing the whole human body and the tool.

Evaluation of 3D human poses For each video in the Handtool dataset, we have manually annotated the 3D positions of the person’s left and right shoulders, elbows, wrist, hips, knees, and ankles, for the first, the middle, and the last frame. The 3D annotation is done using the Berkeley Human Annotation Tool (Bourdev and Malik 2011), by following these three steps: (i) annotate the 2D joint locations in the image, (ii) specify the relative depth ordering for linked joints, and (iii) run the optimization approach described in Taylor (2000) to obtain a 3D stick figure. This annotation process is repeated until the 3D figure is visually correct according to the annotator. We evaluate the accuracy of the recovered 3D human poses by computing their MPJPE after rigid alignment. Quantitative evaluation of the recovered 3D poses is shown table 3. On average, our generic model (the same as for the Parkour dataset) outperforms all the baselines on this dataset. Our action-specific models achieve on average even better performance. Our approach achieves the best results on all individual actions except on scythe. After manual inspection of the results, we believe that this is due to the inaccuracy of the 3D model of the scythe, which is represented as a 3D line segment without explicitly modelling the handle of the scythe, which in turn affects the accuracy of the estimated 3D human poses (via the person–object contact model). However, the differences between the methods are reaching the limits of the accuracy of the manually provided 3D human pose annotations on this dataset. For example, Marinoiu et al. (2013) point out that manual 3D annotation errors can range up to 100 mm per joint (Ionescu et al. 2014).

Evaluation of 2D object poses To evaluate the quality of estimated object poses, we manually annotated 2D object endpoints in every 5th frame of each video in the Handtool dataset and calculated the 2D Euclidean distance (in pixels) between each manually annotated endpoint and its estimated 2D location provided by our method. The 2D location is obtained by projecting the estimated 3D tool position back to the image plane. We compare our results to the output of the Mask R-CNN instance segmentation baseline (He et al. 2017) (which provides initialization for our person–object interaction model). In Table 6 we report for both methods the percentage of endpoints for which the estimated endpoint location lies within 25, 50, and 100 pixels from the annotated ground truth endpoint location. The results demonstrate that our approach provides in most cases more accurate and stable object endpoint locations compared to the Mask R-CNN baseline thanks to modeling the interaction between the object and the person. Lower results of our approach for scythe for the strict 25 pixel threshold can be again attributed to the inaccuracy of the 3D scythe model approximated only as a 3D line segment.

Ablation Study
To gain further insight into the improvements over the conference version of this work (Li et al. 2019), we perform an ablation study of (i) the newly introduced person 3D consistency loss (6) (also referred to as the 3D data term) and (ii) the new force regularization term (16), which smooths not only the temporal variation but also the magnitude of the estimated contact forces. These experiments are done using the Parkour dataset which has precise and dense ground truth for the 3D motion and contact forces captured by MoCap and force sensors. Unless otherwise mentioned, the experiments are based on the generic model described previously.

Ablation of the 3D data term In this ablation, we remove the 3D data term (6) from the generic model while keeping the rest of the cost terms and the related parameters. The results are reported in Table 4, where we compare the mean per joint position error (MPJPE) of the ablated model with the original generic model. The results show that on average the new 3D data term improves the 3D pose estimates, though the improvement is relatively minor. Qualitatively, we have observed that the 3D data term plays the role of a pose prior that encodes, for example, the relative depth of the different joints (e.g. between the person’s left and right hand), which is captured in the strong 3D prior of the HMR approach (Kanazawa et al. 2018).

Fig. 7
figure 7
Example of temporal evolution of the magnitude of the estimated linear force (top) and torque (middle) at the person’s left hand compared with the ground truth coming from the force sensors on an example sequence from the Parkour dataset. The x-axis represents time (here frame numbers). Sample frames from the sequence with their corresponding frame numbers are shown at the bottom

Full size image
Ablation of force regularization The new force regularization term (16) smooths not only the temporal variation of the estimated contact forces (Li et al. 2019) but also the magnitude of the estimated contact forces. Therefore, we evaluate and compare two ablated models against our generic model. In the first ablated model (Ours (no force regularization)), we remove both terms regularizing the temporal variation and the magnitude of the estimated contact forces (i.e. the second and the third term in Eq. (16)). In the second ablated model (Ours (no ‖𝑓𝑘‖2 in (16))), we only remove the third term regularizing the magnitude of the estimated contact forces, i.e. this model regularizes only the temporal variation of the estimated contact forces. Note that this form of force regularization was used in the conference version of this work (Li et al. 2019). Quantitative results are reported in Table 5 and clearly show the benefit of regularizing both the temporal variation and the magnitude of the estimated contact forces (Ours (generic model)), which results in the lowest errors. While we cannot compute force estimation errors on the Handtool dataset due to the lack of ground truth data, we can still perform a simple ablation analysis by plotting the temporal variation of the estimated linear forces and torques with and without the force regularization terms. This is shown on an example video sequence for the left-hand contact force in Fig. 11. Please note how the regularization of both the temporal variation and magnitude of the estimated forces (16) effectively smoothes the estimated forces reducing their abrupt temporal changes and unrealistic magnitudes. Figure 11d also compares the output of our model with and without force regularization at two example frames. In particular, frame #10 corresponds to the case where the model without force regularization outputs a linear force with an unrealistic orientation and magnitude (highlighted with a bold yellow line in the image) whereas the regularized model outputs a more realistic force estimate in terms of both the orientation and magnitude. Similarly, for frame #51 the model with force regularization outputs a torque with a smaller and hence more realistic magnitude.

Fig. 8
figure 8
Precision–recall curves of our trained models for recognizing the hand-, sole-, toes-, neck-, and knee-contact states

Full size image
Fig. 9
figure 9
Qualitative comparison with the baseline HMR estimator (Kanazawa et al. 2018). In each example, the top row shows the input frame (left) and the output of our method from two different viewpoints (middle, right). The bottom row shows the estimated 2D joints (left) and the output of the HMR baseline shown from two different viewpoints (middle, right). In the hammering example (the left panel) the person’s hands holding the hammer are restricted to be on the handle by our contact model, thus reducing the depth ambiguity compared to 3D human poses provided by the baseline HMR (Kanazawa et al. 2018) estimator, which often outputs open arms. The second example (the right panel) shows an “outlier” frame of a Parkour video where HMR fails to estimate correct human body orientation w.r.t the camera due to heavy occlusion and motion blur

Full size image
Evaluation of Contact Recognition
In this section we evaluate the quality of our contact recognizers. The training data, the recognition architecture and the training process are described in Sect. 5. To form the test set we annotate contact states in the entire Handtool dataset and a subset of the Parkour dataset obtained by sampling every 5-th frame. Following the same annotation process as done for training, we have cropped image patches around individual human joints in the test set. This results in a separate test set for each of the five joint types: hand, sole, toes, neck and knee. The neck and the knee test sets include only patches from the Handtool dataset as the Parkour dataset does not consider these types of contacts. We evaluate each contact recognizer using a precision-recall curve on its corresponding test set. The positive class means the joint is “in contact”. The evaluation results are shown in Fig. 8. Each precision-recall curve is also summarized using average precision (AP). The results demonstrate good quality of our contact recognition models despite the appearance variation present in both the Handtool and Parkour datasets.

Qualitative Results
Here we show qualitative examples. Additional video results are available on our Project webpage (2021).

Figure 5 shows a collection of qualitative results at sampled video frames in the Handtool (top four rows) and the Parkour (bottom four rows) datasets. For each sample, we first show the original frame (left image), followed by the estimated 3D motion and forces from the original viewpoint (middle image), and the same 3D scene from a different viewpoint (right image). Note that for the Parkour dataset we recognize the contact states of human joints but do not recognize and model the pose of the object (the metal construction) the person is interacting with. In addition to results for individual frames from different videos, we provide in Fig. 6 results for two sequences of frames to demonstrate the continuity of the reconstructed actions. The sequences demonstrate that the outputs of our method are temporally consistent and smooth.

Figure 9 shows a comparison of our model with the baseline HMR approach (Kanazawa et al. 2018). In the first example (hammering action), the person’s hands holding the hammer are restricted to be on the handle by our contact model, thus reducing the depth ambiguity compared to 3D human poses provided by the baseline HMR (Kanazawa et al. 2018) estimator, which often outputs open arms. The second example shows an “outlier” frame of a Parkour video where HMR fails to estimate correct human body orientation due to heavy occlusion and motion blur. In this case, our method relies on the model of dynamics and the pose prior to synthesize the person’s motion in between good predictions. Due to these reasons, we observe that our method often predicts better poses than the baseline methods that are applied to individual frames and do not model the temporal interaction between the person and the tool.

The qualitative results also demonstrate that our model predicts reasonable contact forces. The directions of the contact forces exerted on the person’s hands are consistent with the object’s motion trajectory and gravity, and the ground reaction forces generally point towards the direction opposite to gravity. Specifically, in the video with the person practicing back squat with barbell (see the left example in the second row of Fig. 5), the reconstructed object contact forces and ground reaction forces are distributed evenly on the person’s hands, and knees, respectively. Another example is scythe (third row of Fig. 5, right), where the distribution of ground reaction forces at the person’s feet follows the swings of the body while cutting the grass. In the shown frame the person’s center of mass is above their right leg, leading to larger contact force at the right leg.

Fig. 10
figure 10
Main failure modes of our method: (i) missing object 2D endpoint detections (top row): the handle of the barbell is not detected, which affects the 3D output of our model; (ii) contact recognition errors (middle row): the person’s right knee is incorrectly recognized as not in contact (red), leading to incorrect force estimates shown on the right; (iii) incorrect 2D human joints (bottom row): the missing 2D detection of the person’s left foot has lead to errors in estimating the 3D location of the left leg

Full size image
Failure Modes
Figure 10 shows three typical failure modes described below.

Missing object 2D endpoint detections The estimated object 2D endpoints are often noisy due to heavy occlusions between human limbs and the manipulated object. To solve this problem, we filter out endpoints with low confidence at the end of the recognition stage. However, this produces missing observations in the estimated 2D endpoint sequences as shown in the first example in Fig. 10, where there is no predicted endpoint as the barbell handle is completely occluded (imaged from the side). In this case, our contact motion model can infer the position of the barbell handle from the position of hands, but the results are often not very accurate.

Contact recognition errors The second row of Fig. 10 shows an example with incorrectly estimated contact state. In this case, the person’s right knee is incorrectly recognized as not in contact, leading to incorrect force estimation.

Incorrectly localized human joints in the image Our method struggles to estimate correct 3D poses if the quality of 2D detection is low. An example is shown in the bottom row of Fig. 10, where the missing 2D detection of the person’s left foot has lead to errors in estimating the 3D location of the left leg.

Fig. 11
figure 11
Plots of the estimated linear contact force (a) and torque (b) at the right hand for an example video from the Handtool dataset. In all plots the x-axis represents time (in frame numbers). In both a, b, the top plot is without force regularization and the bottom plot is with force regularization (i.e. the generic model). c shows example frames with their corresponding frame numbers. d shows the estimated 3D scene at two sample frames with the highlighted linear contact force (bold yellow) and torque (bold white) at the right hand. Please note how force regularization effectively smoothes the estimated forces and torques reducing their unrealistic abrupt temporal changes and large magnitudes (note the different scales of the y-axis in the different plots)

Full size image
Limitations
Our approach has several limitations, which we discuss next. First, our object model is currently limited to rigid, stick-like tools. Modeling other types of rigid objects, e.g. boxes, would require recognizing and modelling other object shapes, which is technically possible with our model but we leave it for future work. Recognizing and modelling interactions with non-rigid objects such as cloth is still an open challenge. Second, the proposed method models the hand-object contact at a relatively coarse level by taking into account only a single joint location (the wrist). While this is reasonable for the type of objects considered in this work, it is too coarse for a more fine-grained manipulation of smaller objects such as pencils or cups. Third, we initialize our model with (Kanazawa et al. 2018) to provide the size and shape of the depicted person, but then use only the body skeletal rig in the estimation stage. A mesh-based representation could be more descriptive. Finally, our method does not consider object-object and object-ground interactions. For example, in the case of breaking concrete with a hammer, our method does not currently model the contact force exerted on the hammer by the concrete. Modeling the interactions between the object and the environment is an exciting direction of future work.

Conclusion
We have developed a visual recognition system that takes as input video frames together with a simple object model, and outputs a 3D motion of the person and the object including contact forces and torques actuated by the human limbs.

We have validated our approach on a recent video MoCap dataset with ground truth contact forces. Finally, we have collected a new dataset of unconstrained instructional videos depicting people manipulating different objects and have demonstrated benefits of our approach on this data. Our work opens up the possibility of large-scale learning of human–object interactions from Internet instructional videos (Alayrac et al. 2016).

