Weakly supervised temporal action localization is a practical yet challenging task. Although great efforts have been made in recent years, the existing methods still have limited capacity in dealing with the challenges of over-localization, joint-localization, and under-localization. Based on our investigation, the first two challenges arise from insufficient ability to suppress background response, while the third challenge is due to the lack of discovering action frames. To better address these challenges, we first propose the astute background response strategy. By enforcing the classification target of the background category to be zero, such a strategy can endow the conductive effect between video-level classification and frame-level classification, thus guiding the action category to suppress responses at background frames astutely and helping address the over-localization and joint-localization challenges. For alleviating the under-localization challenge, we introduce the self-distillation learning strategy. It simultaneously learns one master network and multiple auxiliary networks, where the auxiliary networks enhance the master network to discover complete action frames. Experimental results on three benchmarks demonstrate the favorable performance of the proposed method against previous counterparts, and its efficacy to tackle the existing three challenges.

Access provided by University of Auckland Library

Introduction
Given long untrimmed videos, temporal action localization aims at discovering action instances within these videos, i.e. precisely determining the action start time and end time, as well as accurately predicting its category label. Temporal action localization can automatically mine informative video segments from long video sequences and serves as the foundation for the intelligent video processing system, e.g. highlight extraction (Gao et al. 2018b), anomaly detection (Lu et al. 2019), smart surveillance (Hattori et al. 2018). Recently, weakly supervised temporal action localization attracts arising research interests, where the algorithm directly learns from video-level tags without requiring the instance-level annotations (i.e. start time and end time).

Fig. 1
figure 1
Illustration of three challenges for weakly supervised temporal action localization task and our solution. The over-localization (a) and joint-localization (b) challenges arise from insufficiency to suppress background, while the under-localization challenge arises from insufficiency to discovering action. To alleviate these challenges, we propose the astute background response strategy and the self-distillation learning strategy

Full size image
One of the fundamental challenges for weakly supervised temporal action localization task is the ambiguity mapping problem, which is caused by the ambiguity between one certain video-level classification label and the multiple potential instance-level actions in the weakly supervised learning paradigm. Specifically, the ambiguity mapping problem usually leads to three dilemmas: over-localiza-tion (Shi et al. 2020; Choe et al. 2020), joint-localization (Xu et al. 2019; Narayan et al. 2019), and under-localization (Liu et al. 2019a; Min and Corso 2020). As shown in Fig. 1a, over-localization indicates the predicted action segment contains not only the ground truth frames but also the neighboring background frames. This is caused by the disturbing context frames, which are closely related to the video-level classification target. In Fig. 1a, take the action baseball pitch as an example, because the preparation frames and the finishing frames are closely related with the real action baseball pitch, the algorithm regards these neighboring frames as action and encounters the over-localization problem. Besides, Fig 1b shows the joint-localization challenge. In this case, there are multiple action instances densely occurred in this video, but the algorithm fails to separate each instance and regard them as a long and continuous action instance. This case is because background frames lying in adjacent action instances usually show similar appearance characteristics or motion patterns with these action frames, which confuses the action localization algorithm. While in the under-localization scenario, the algorithm only captures the most discriminative action part but ignores other action parts, which is usually caused by the large variances among action instances from the same category. As shown in Fig. 1c, the high jump action exhibits large variances, e.g. the specific high jump action can be â€œFosbury flopâ€ or â€œcut-off scissors jumpâ€, but all these instances exhibit one common ground at â€œjump and rotationâ€ moments. Consequently, the algorithm captures this common ground, shows a high response for the specific part of actions but ignores other necessary action parts.

Fig. 2
figure 2
Illustration of single-action-frame annotation and single-action-background-frame annotation. SF-Net (Ma et al. 2020) annotation is shown in (a), only annotating single-action-frame. Our SODA annotates both single-action-frame and single-background-frame, but we only annotate half of the whole video for a fair comparison with similar annotation cost

Full size image
In this paper, we propose to tackle the ambiguity mapping problem via the astute background response and self-distillation learning strategies, namely SODA. We first make a detailed analysis of the score aggregation process and the video-level classification process. This drives us to make a counterintuitive judgment, that, we should assign the background label as zero when calculating the video-level classification loss. Specifically, for video-level classification, label one means the current video contains action instances from the specific category, while label zero means not any frames of the current video are from the corresponding category. Although all videos contain background frames, our counterintuitive judgment is that the background classification label should be zero. This strategy guides the action category to exhibit low response at background frames via integrating the video-level classification and the frame-level classification. Specifically, given label zero, the video-level classification mechanism requires all frames to show low responses for the background category. Meanwhile, on annotated background frames, the supervised frame-level classification requires the response of any action category to be lower than the background categoryâ€™s response. Consequently, the conduction effect of the video-level classification and the frame-level classification implicitly constrains the action categoryâ€™s response to be low at the background frame. Because the classifier learns to sensitively and accurately discover the background frames, we name it as the astute background response strategy. When the background frames can be astutely perceived, the algorithm can distinguish the disturbing background frames from the action instances, which alleviate the over-localization and joint-localization challenges.

For addressing the under-localization challenge, i.e. the insufficient recall of the learning algorithm, previous works usually learn parallel but diverse branches or try to erase the representative action parts. In this paper, we propose a self-distillation learning strategy by learning multiple networks to capture comprehensive action frames. Specifically, we simultaneously learn one master network and multiple auxiliary networks and make auxiliary networks correspond with the master network by comparing their responses on each frame. Because these networks are initialized with distinct random values, their optimization processes would exhibit certain differences and could potentially capture complementary action parts. Consequently, the correspondences between auxiliary networks and the master network would guide the master network to discover inapparent action frames correctly, and alleviate the under-localization challenge.

Fig. 3
figure 3
Framework of the proposed SODA method. The baseline network of SODA is the Class Activation Sequence Network (CASNet). SODA tackles the temporal action localization task via performing video-level classification (îˆ¸ğ‘ğ‘™ğ‘ ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘œ), frame-level classification (îˆ¸ğ‘ğ‘™ğ‘ ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’) and self-distillation learning (îˆ¸ğ‘ ğ‘‘). The background classification label zero endows the astute background response strategy (Sect. 4), while the self-distillation learning strategy is built on one master network and multiple auxiliary networks (Sect. 5). In inference, only the master network is employed to discover action instances

Full size image
The whole framework follows SF-Net (Ma et al. 2020) to facilitate the weakly supervised learning process under the single-frame annotation. However, Ma et al. (2020) study the single-action-frame supervision, but we focus on the single-action-background frame supervision, i.e. annotating both single-action-frame from action segment and single-back-ground-frame from background segment (see Fig. 2 and Sec. 6.2 for more details). Under the same annotation cost, we empirically verified that the single-action-background frame supervision tends to be more effective.

The framework of the proposed SODA method is shown in Fig. 3. Specifically, SODA first employs the I3D network (Carreira and Zisserman 2017) to extract video features. Then, features are fed through the convolutional layers to predict the class activation sequence and estimate the video-level classification score. The astute background response strategy sets the background label zero for video-level classification score, while the self-distillation strategy makes correspondence between one master network and multiple auxiliary networks. To summarize, our contributions are three-fold:

We reveal the fundamental ambiguity mapping problem for weakly supervised temporal action localization task and propose a unified framework SODA to tackle it.

We propose the astute background response strategy to alleviate the over-localization and joint-localization challenges. It assigns the background classification label as zero, utilizes the conduction effect between the video-level classification and the frame-level classification, and guides the action category to show low response at background frames astutely.

We propose the self-distillation learning strategy to tackle the under-localization challenge. It makes correspondence between the master network and auxiliary networks, which introduces the complementary action patterns to the master network.

We carry comprehensive experiments on three benchmarks that demonstrate the new high performance built by SODA and the efficacy of the astute background response strategy and the self-distillation learning strategy.

The rest of this paper is organized as follows: Sect. 2 reviews related works for the proposed SODA method. Section 3 depicts the holistic methodology of SODA, where the detailed mechanism for astute background response and self-distillation learning is elaborated in Sects. 4 and 5, respectively. After that, ablation studies and comparison experiments are carried in Sect. 6. Finally, Sect. 7 draws a conclusion.

Related Work
Action Recognition
Action recognition is one of the most important tasks in video analysis, which aims to classify actions that appear in the video. Early works (Wang et al. (2013); Wang and Schmid (2013)) adopt hand-crafted features. In recent years, with the development of deep learning (Gong et al. (2017); Han et al. (2018a); Zhang et al. (2018a); Yang et al. (2018); Gong et al. (2018); Lu et al. (2020)) in image or video analysis and understanding, most action recognition works also utilize convolutional neural networks. Simonyan and Zisserman (2014) propose two-stream ConvNet architecture which includes spatial stream and temporal stream. Spatial stream uses single frame RGB image as input to capture appearance information. Temporal stream employs multi-frame optical flow to capture motion information. Whereafter, TSN (Wang et al. 2016) proposes temporal segment networks to model long-range temporal structure. Some works use 3D ConvNets, e.g. C3D (Tran et al. 2015) shows that 3D ConvNets are more suitable to learn spatiotemporal features. I3D (Carreira and Zisserman 2017) inflates 2D ConvNet to obtain two-stream inflated 3D ConvNet. P3D (Qiu et al. 2017) designs pseudo-3D residual net by recycling off-the-shelf 2D ConvNet. Lately, SlowFast (Feichtenhofer et al. 2019) incorporates a slow pathway and a fast pathway and achieves state-of-the-art performance just need RGB image. Meanwhile, Zhu and Yang (2020) leverage freely available unlabeled video data to facilitate few-shot video classification, which has also been found beneficial in the image field (Ramanathan et al. 2020). However, these action recognition works only process short trimmed videos which are inapplicable for real-world videos with much background.

Fully Supervised Temporal Action Localization
Fully supervised temporal action localization aims at predicting the start time, end time, and category label of action instances in a given untrimmed video. Which needs frame-level annotations in the training phase. Typically, S-CNN (Shou et al. 2016) designs a multi-stage segment 3D CNN framework to localize action temporal boundaries. Inspired by Faster R-CNN (Ren et al. 2015), R-C3D (Xu et al. 2017) proposes an end-to-end region convolutional 3D network, which can detect arbitrary length actions, and TAL (Chao et al. 2018) introduces TAL-Net, which improves receptive field alignment, exploits the temporal context, and designs late feature fusion. To deal with complex variable actions, GTAN (Long et al. 2019) exploits Gaussian kernels to optimize the temporal scale of each action proposal dynamically, which is further developed by A2Net (Yang et al. 2020a). To efficiently generate reliable confidence scores for proposals, BMN (Lin et al. 2019) proposes a boundary matching mechanism to evaluate confidence scores of densely distributed proposals. Based on GCN (Kipf and Welling 2016), P-GCN (Zeng et al. 2019) utilizes graph convolutional networks to explore relations among action proposals explicitly, and G-TAD (Xu et al. 2020) considers temporal action localization as a sub-graph localization issue and adaptively combines multi-level semantic context into video features. Recently, BUMR (Zhao et al. 2020) proposes the intra-phase and inter-phase consistency regularization to constrain the starting, continuing, and ending of action. There is a big performance improvement, but accurate boundary annotation is time-consuming and cumbersome.

In addition, we notice that one recent work (Wu et al. 2019b) studies the audio-visual event localization task. They utilize the global feature representation from audio modality to guide the visual features to discover the relevant event segments, and vice versa. However, this paper focuses on the weakly supervised temporal action localization task. As a different task, the audio-visual event localization task is beyond the scope of this paper.

Weakly Supervised Temporal Action Localization
In the research field of image understanding, remarkable processes have been made for building learning frameworks under the weak supervision (Gong et al. 2020a; Sun et al. 2020). E.g. Sun et al. (2020) aim at learning to accomplish the semantic segmentation task under image-level annotation. Specially, weakly supervised temporal action localization only needs video-level annotations in the training phase. UntrimmedNet (Wang et al. 2017) is the first to present weakly-supervised temporal action localization, which couples classification module and selection module to learn an action localization model. Then, STPN (Nguyen et al. 2018) designs attention weights modules to do adaptive temporal pooling. However, because of the ambiguity mapping problem with video-level supervision, many works are devoted to alleviating it. W-TAL (Paul et al. 2018) proposes co-activity similarity loss to mining similar features of common category video pairs. CMCS (Liu et al. 2019a) introduces diversity loss to discover distinctive action parts on multiple branches, meanwhile, they use static clips as negative training data. WSBM (Nguyen et al. 2019) models background to obtain a richer concept of actions and their temporal extension. TSM (Yu et al. 2019) introduces the temporal structure mining method to model the temporal relations among segments. More recently, BaSNet (Lee et al. 2020) designs a background suppression network to suppress background activation. TSCN (Zhai et al. 2020) introduces a two-stream consensus network, which employs an iterative refinement training strategy to obtain a precise prediction. DGAM (Shi et al. 2020) uses conditional variational auto-encoder to solve the action context confusion problem. ActionBytes (Jain et al. 2020) brings in short trimmed videos to assist in learning to localize untrimmed videos. ACL (Gong et al. 2020b) utilizes class-specific and class-agnostic attention simultaneously. A2CL-PT (Min and Corso 2020) adopts an adversarial approach to localize more complete actions. EM-MIL (Luo et al. 2020) proposes an expectation-maximization multiple instance learning framework to optimize the likelihood of lower bound. However, existing works tackle the ambiguity mapping problem from specific aspect, we study over-localization, joint-localization and under-localization challenges comprehensively.

Frame Supervision
Fully supervised methods can achieve great performance but need time-consuming and tedious annotations. And pure weakly supervised methods only need simple and efficient annotations, but performance is under restrictions. How to find a balance between necessary supervision information and efficient labeling methods has attracted some attention. Bearman et al. (2016) are the first to come up with point supervision for the task of semantic segmentation. Meanwhile, Mettes et al. (2016) propose to annotate actions with points rather than boxes in spatio-temporal action localization task. Similarly, although recent weakly supervised temporal action localization works have achieved an impressive performance. Lacking precise supervision annotations still causes a large gap to fully supervised temporal action localization methods.

Lately, SF-Net (Ma et al. 2020) made the earliest effort to utilize the frame supervision information to train temporal action localization networks. Differences between SF-Net and the proposed SODA method lie in two aspects. Firstly, SF-Net annotates a single frame within each action instance. On the contrary, SODA annotates single frames for both action instances and background segments, but it only annotates on the half of the whole video. Based on our finding, given the same annotation cost, annotating both action and background can achieve superior performance. Secondly, SF-Net focuses on supervised classification on annotated frames. Its cores is the classification module and the actionness module. The proposed SODA aims at improving the quality of class activation sequence and our core designs are the astute background response strategy and the self-distillation learning strategy.

Knowledge Distillation
Knowledge distillation (BuciluÇ et al. 2006; Hinton et al. 2015) is proposed to distillate the knowledge from a teacher model into a student model. The teacher model is usually an ensemble or complex model with high performance, and the student model is usually single or simple. As a result, a compressed model with satisfactory performance is obtained. Recently, mutual learning strategy (Zhang et al. 2018c) is designed, which is different from knowledge distillation with the teacher-student pattern. By mutual learning, a group of students can learn from each other. Then, Wu et al. (2019a) leverage mutual learning on top of each block of the salient object detection backbone. Meanwhile, Zhang et al. (2019c) propose self-distillation by distilling knowledge within the network itself. Crowley et al. (2018) propose to produce a student network by transforming the teacher network. Hou et al. (2019) present self-attention distillation to deal with lane detection task. Meanwhile, Zhang et al. (2019c) propose self-distillation by distilling knowledge within the network itself. Yun et al. (2020) distill the prediction distribution between different samples with the same label. Wei et al. (2020) introduces mutual learning to image classification with noisy labels, which is designed to decrease the diversity of two networks in the training phase. Pang et al. (2020) implement aggregate interaction module on multi-level features to suppress the interference caused by large resolution differences in feature fusion. Yuan et al. (2020) come up with a framework of teacher-free Knowledge distillation with improved label smoothing regularization. Lately, Yang et al. (2020b) put forward mutual learning from network width and resolution simultaneously. Inspired by mutual learning (Zhang et al. 2018c), we proposed self-distillation learning to deal with weakly supervised temporal action localization. Itâ€™s worth noting that there are some differences between our method and the approach proposed by Zhang et al. (2018c). First, they focus on the task of multi-class classification, but our task is weakly supervised temporal action localization. Then, they impose the loss function on networkâ€™s classification predictions. On the contrary, we propose to apply loss on class activation sequence. Finally, we expand the self-distillation learning from two networks to multiple networks. Details are discussed in Sect. 5.

Fig. 4
figure 4
Detailed architecture of the CASNet. It first employs three temporal convolutional layers to generate the class activation sequence, then utilize top-k mean aggregation to estimate the video-level classification score

Full size image
Methodology
Problem Formulation
We learn temporal action localization models from untrimmed videos. The dataset consists of C action categories in total, and each video contains a classification label ğ²=(ğ‘¦0,ğ‘¦1,ğ‘¦2,...,ğ‘¦ğ¶), where ğ‘¦ğ‘âˆˆ{0,1}, indicating whether this video contains any action instance from ğ‘ğ‘¡â„ categories and ğ‘¦0 is for background. In addition to video-level classification label, we make single-frame annotations for the selected video segments (details will be elaborated in Sect. 6.2). If the ğ‘¡ğ‘¡â„ frame is annotated, we can obtain the frame-level classification label ğšğ‘¡=[ğ‘0,ğ‘¡,ğ‘1,ğ‘¡,...,ğ‘ğ‘,ğ‘¡], where ğ‘ğ‘,ğ‘¡âˆˆ{0,1} to indicate whether this frame belonging to ğ‘ğ‘¡â„ categories, and ğ‘0 is for the background. Given a video, we first extract frames from video, then uniformly divided the frame sequence into T clips, and finally extract a feature representation for each clip. Consequently, the feature sequence ğ… can be represented as ğ…=[ğŸ1,ğŸ2,...,ğŸğ‘‡], where ğŸğ‘¡ is the extracted feature for the ğ‘¡ğ‘¡â„ clip. The temporal action localization algorithm tackles the feature sequence for each video, learns from video-level tags and single-frame annotations, and aims to discover potential action instances via determining the starting times, ending times and action categories.

Class Activation Sequence Network
Given video features, weakly supervised temporal action localization methods perform classification at each clip feature, and obtain the class activation sequence ğâˆˆâ„(ğ¶+1)Ã—ğ‘‡, where ğ¶+1 is the category number (including all action categories and the background) and T indicates temporal length of the feature sequence. In Table 1, we summarize different operations used by existing methods to predict the class activation sequence. In general, the temporal convolution operations or fully connected operations are widely used to first learn representative features then perform classification. Although the concrete operations vary among different methods, they essentially pursue the same purpose, i.e. predicting the classification score as accurately as possible.

Table 1 Existing methods employ the temporal convolution (Conv) or fully connected (FC) operations, along with the ReLU activation, to predict class activation sequences
Full size table
We summarize a commonly used network from existing method, namely CASNet (the Class Activation Sequence Network), to serve as the baseline for the proposed SODA method. The framework of CASNet is depicted in Fig. 4. Faced with extracted video features, CASNet first adopts two temporal convolutional layers with kernel size 3 to convert the raw features, extracted by the I3D model (Carreira and Zisserman 2017), into features suitable for temporal action localization task. Then, CASNet uses one temporal convolutional layer with kernel size 1 to perform frame-level classification, generating the class activation sequence ğâˆˆâ„(ğ¶+1)Ã—ğ‘‡. After that, CASNet adopts the top-k mean aggregation strategy to estimate the video-level classification score ğ¬=[ğ‘ 0,ğ‘ 1,...,ğ‘ ğ¶], which can be supervised by the video-level classification label ğ². Specifically, we calculate the cross entropy loss as follows:

îˆ¸ğ‘ğ‘™ğ‘ ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘œ=âˆ’âˆ‘ğ‘=0ğ¶ğ‘¦ğ‘^logğ‘ ğ‘^,
(1)
where ğ²Ì‚ =[ğ‘¦0^,ğ‘¦1^,...,ğ‘¦ğ¶^] is obtained via the L1 normalization on raw ground truth ğ², and ğ¬Ì‚ =[ğ‘ 0^,ğ‘ 1^,...,ğ‘ ğ¶^] is obtained via the softmax normalization on raw video-level classification score ğ¬.

In weakly supervised learning, the mapping from a video-level classification label to instance-level actions is not unique. For example, the representative action instances can provide confident evidence to reveal the presence of an action category within a video. In contrast, the confident video segments determined by the video-level classification score may not be a complete action instance, e.g. being only the representative action parts. We summarize this challenge as the ambiguity mapping challenge, which is partially explored by previous works, e.g. background suppression network (Lee et al. 2020), background modeling (Nguyen et al. 2019), discriminative and generative attention (Shi et al. 2020), single-frame supervision (Ma et al. 2020). Among these works, SF-Net makes an obvious step forward, while it requires negligible extra annotation cost. However, SF-Net only annotates action frames, and jointly performs the class-specific classification and the class agnostic classification on each annotated frame. In this work, given the same annotation costs, we find it is more effective to simultaneously annotate single action frames and single background frames (details in Sect. 6.2 and 6.3 ). Additionally, we find the class-specific classification is capable to thoroughly explore the frame-level annotations when we properly design the video-level classification label (see Sect. 4.5). Consequently, we discard the class-agnostic classification and only perform the supervised classification on annotated frames as follows:

îˆ¸ğ‘ğ‘™ğ‘ ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’=âˆ’1ğ‘ğ‘âˆ‘ğ‘=0ğ¶ğš[ğ‘,ğ‘¡]â‹…logğÌ‚ [ğ‘,ğ‘¡],
(2)
where ğ‘ğ‘ is the number of annotated frames within this video, ğš is the single-frame annotations, and ğÌ‚  is obtained via performing the softmax normalization along the category dimension on ğ.

Astute Background Response
This paper proposes the single-action-background-frame supervision (see Sect. 6.2 for detailed annotation processes). On annotated frames, the quality of class activation sequence can be improved by the supervised classification process. As for the video-level classification, the label for background category can be set as zero or one. This section discusses what is a proper classification label for the background category. The top-k mean aggregation strategy selects k largest response values from T values and treats the mean response as the video-level confidence score for this category. In the raw video-level classification label ğ²=[ğ‘¦0,ğ‘¦1,...,ğ‘¦ğ¶], ğ‘¦ğ‘âˆˆ{0,1} indicates whether this video contains action instances from ğ‘ğ‘¡â„ category. Considering each video contains background frames, it is intuitive to set the training target for the background category as one. However, the background label one would cause the learning bias problem and suppress the response for real action categories (details in Sect. 4.5). Moreover, after performing detailed analysis of the score aggregation process, we draw a counterintuitive judgment that we should set the video-level classification label for background as zero. Specifically, under target zero, the video-level classification tends to suppress the background response at all frames. Meanwhile, on annotated background frames, the supervised classification constrains that any action categoryâ€™s response should be lower than the background categoryâ€™s response. These two criteria formulate the conduct effectively, which guides the action category to astutely show low response at background frames. Equipped with the astute background response strategy, the proposed SODA method can sensitively and accurately figure out background frames, which contributes to alleviate the over-localization challenge and joint-localization challenge.

Fig. 5
figure 5
Illustration that the astute background response can improve the quality of the class activation sequence via enlarging the responses for action frames (shown in ) and suppressing the responses for background frames (shown in ). where â€œâ€ and â€œâ€ indicate actions and backgrounds, respectively. Results are from video_test_0000673 in the THUMOS14 dataset (Color figure online)

Full size image
As shown in Fig. 5, when only using the baseline method, the generated class activation sequence cannot clearly distinguish background frames from action frames. After introducing the astute background response strategy, the responses of action frames are enlarged while the response of background frames are suppressed. Consequently, the quality of class activation sequence is improved, which is beneficial to accurately localize action instances.

Self-Distillation Learning
Based on CASNet, the astute background response strategy is effective in tackling the over-localization and joint-localization challenges. Besides, we found that the under localization challenge can be significantly alleviated if we carry on knowledge distillation among multiple CASNets. Essentially, the diversity among multiple class activation sequences plays a key role in alleviating the under-localization issue. In weakly supervised temporal action localization, a common hypothesizes is that a frame contributing more to the video-level classification is more likely to be an action frame. As a result, the algorithm is prone to focus on the most discriminative part of an action instance but fails to consistently show high responses for all action frames. In the proposed SODA method, besides the master network, we simultaneously learn multiple auxiliary networks. Because all these networks are randomly initialized, when they are converged, the model weights exhibit certain differences. Consequently, multiple class activation sequences show adequate diversity, i.e., different models exhibit high values at different action frames, which can alleviate the under-localization challenge.

Different from traditional paradigm, we do not rely on the teacher network, which usually requires complex designs and heavy computations. In contrast, we simultaneously learn multiple variants of the vanilla CASNet, which are initialized with different randomness and converge to different weights. We regard the first CASNet as the master network and other ğ‘€âˆ’1 CASNets as auxiliary network, where the master CASNet corresponds with auxiliary CASNets via comparing the class activation sequences. Because this knowledge distillation does not rely on extra teacher network, we name it as the self-distillation learning strategy. Specifically, we calculate the self-distillation loss as follows:

îˆ¸ğ‘ ğ‘‘=1ğ‘‡â‹…(ğ‘€âˆ’1)âˆ‘ğ‘âˆˆğ¶âˆ—âˆ‘ğ‘¡=1ğ‘‡âˆ‘ğ‘š=2ğ‘€|ğ1[ğ‘,ğ‘¡]âˆ’ğğ‘š[ğ‘,ğ‘¡]|,
(3)
where |â‹…| indicates calculating the absolute value, ğ¶âˆ— indicates collection of categories that appear in the given video, and [â‹…] means select ğ‘¡ğ‘¡â„ value from ğ‘ğ‘¡â„ category.

Training and Inference
As shown in Fig. 3, the proposed SODA method jointly learns from the video-level classification loss îˆ¸ğ‘ğ‘™ğ‘ ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘œ, the frame-level classification loss îˆ¸ğ‘ğ‘™ğ‘ ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’, and the the self-distillation loss îˆ¸ğ‘ ğ‘‘. The total loss to drive the training process can be calculated as follows:

îˆ¸=îˆ¸ğ‘ğ‘™ğ‘ ğ‘£ğ‘–ğ‘‘ğ‘’ğ‘œ+îˆ¸ğ‘ğ‘™ğ‘ ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’+ğœ†îˆ¸ğ‘ ğ‘‘,
(4)
where ğœ† is the trade-off coefficient.

In inference, only the master network is employed to localize action instances. We forward the feature sequence through the master network, and subsequently obtain the class activation sequence ğ as well as the video-level classification score ğ¬. According to threshold ğœğ‘ğ‘™ğ‘ , we select potential category-specific class activation sequences and discover action instances via setting thresholds. The confidence scores for the discovered action instances are determined by the inner-outer-contrastive strategy (Shou et al. 2018).

Fig. 6
figure 6
Illustration of different aggregation operations. There are four kinds of operations to convert the class activation sequence ğ into video-level classification score ğ¬, among which the top-k mean aggregation is a practical and effective choice. ğ´âˆ— indicates the corresponding aggregation strategy

Full size image
Table 2 Illustration of four different aggregation strategies. In the characteristics column, the limitations are shown in Italic and the advantages are shown in Bold
Full size table
Mechanism Analysis for the Astute Background Response Strategy
In this section, we will elaborately analyze the mechanism behind astute background response strategy. Actually, its efficacy arises from the top-k mean aggregation operations, which is a necessary step to convert raw class activation sequence ğ into predicted video-level classification score ğ¬. Consequently, we start our discussions by analyzing four potential aggregation strategies: (1) global average pooling (Sect. 4.1), (2) temporal normalization and aggregation (Sect. 4.2), (3) category normalization and aggregation (Sect. 4.3), and (4) top-k mean aggregation (Sect. 4.4), as depicted in Fig. 6. As shown in Table 2, among four potential aggregation strategies, â€œTop-k Mean Aggregationâ€ is proper for WTAL due to its dynamical selection property and separability of class activation sequences. Finally, we reveal the reason for the superiority of astute background response strategy in Sect. 4.5.

Global Average Pooling & Its Limitation
The global average pooling strategy originated from researches about the interpretability of neural networks (Zhou et al. 2016), and is widely developed by researches about weakly supervised object detection (Han et al. 2018b; Liu et al. 2020), object localization (Zhang et al. 2018b; Choe and Shim 2019) and semantic segmentation (Chan et al. 2020). The global average pooling strategy has the aptitude to discover the most discriminative object parts, providing the foundation for subsequent processes. Being introduced into the studied weakly supervised temporal action localization task, the global average pooling operation can be formulated as:

ğ‘ ğ‘”ğ‘ğ‘ğ‘=1ğ‘‡âˆ‘ğ‘¡=1ğ‘‡ğ[ğ‘,ğ‘¡],
(5)
where ğ‘ ğ‘”ğ‘ğ‘ğ‘ is the predicted score for ğ‘ğ‘¡â„ category by the global average pooling mechanism. However, this is improper for the studied untrimmed videos. As pointed out by Chao et al. (2018), on THUMOS14 dataset, the untrimmed videos contain a large percentage of background frames, while the action frames only take up about 29%. Considering the noisy background frames may disturb the estimation of video-level classification, the global average pooling operation cannot be directly applied to the weakly supervised temporal action localization task.

Fig. 7
figure 7
The part domination problem caused by the softmax normalization process. In comparison between figure (a) and figure (b), extremely close raw scores can lead to close value after the softmax normalization (a), while a certain difference in raw scores would generate obvious value gap after the softmax normalization (b)

Full size image
Fig. 8
figure 8
Influence of the temporal normalization operation. We show the evolution of clipâ€™s response. It can be found that the class activation sequence gradually concentrates on limited discriminative frames with the increase of training process. Results are from video_test_0001040 in the THUMOS14 dataset

Full size image
Temporal Normalization and Aggregation & Its Limitation
The temporal normalization and aggregation strategy can differentiate the importance of each temporal frame via performing temporal normalization. As shown in Fig. 6b, the normalization process can be formulated as:

ğÌ‚ ğ‘¡[ğ‘,ğ‘¡]=exp(ğ[ğ‘,ğ‘¡])âˆ‘ğ‘‡ğ‘—=1exp(ğ[ğ‘,ğ‘—]).
(6)
After that, the normalization results ğÌ‚ ğ‘¡[ğ‘,ğ‘¡] are regarded as the attention weight and are employed to aggregate raw classification score at each frame. Then the video-level classification score can be calculated as:

ğ‘ ğ‘¡ğ‘›ğ‘ğ‘=âˆ‘ğ‘¡=1ğ‘‡ğ[ğ‘,ğ‘¡]â‹…ğÌ‚ [ğ‘,ğ‘¡], ğ‘=[0,1,...,ğ¶],
(7)
where ğ‘ ğ‘¡ğ‘›ğ‘ğ‘ is the predicted score for ğ‘ğ‘¡â„ category by the temporal normalization and aggregation mechanism.

However, the exponential operation is nonlinear, which means a certain difference in raw scores would generate obvious value gaps in final weights, as illustrated in Fig. 7. At the start of the aggregation process, the network discovers some confident frames and assigns them high response scores. Then, these frames would receive large attention weights under the temporal softmax normalization mechanism. After that, the high response scores along with the large attention weights would generate a high video-level classification score. The response scores and the attention weights enhance each other on the discovered representative frames, which prevents the class activation sequence from revealing complete action instances.

Numerically, we replace the top-k mean operation with the â€œtemporal normalization and aggregationâ€ operation, and perform an experiment to verify the influence of temporal softmax normalization. As depicted in Fig. 8, under softmax operation, the aggregation weights gradually focus on limited video frames. In summary, the â€œtemporal normalization and aggregationâ€ may not be a proper choice for converting class activation sequence ğ to video-level classification score ğ¬.

Category Normalization and Aggregation & Its Limitation
The category normalization and aggregation strategy first performs softmax normalization along the category dimension, and converts the value of class activation sequence ğ into standard confidence score. Then, it aggregates scores from each temporal frame and estimate video-level classification score. In the early years, the frame-level classification process only considers action categories, but ignores the background category, i.e. the class activation sequence is ğâ€²âˆˆâ„ğ¶Ã—ğ‘‡. Under this circumstance, Lee et al. Lee et al. (2020) point out the risk that incorrectly classify a background frame into one of the given action categories. In this work, the frame-level classification considers both all action categories and the background, generating the class activation sequence ğâˆˆâ„(ğ¶+1)Ã—ğ‘‡.

Fig. 9
figure 9
Influence of applying category-dimension normalization on the class activation sequence. Without normalization, the network can assign high (or low) responses with large magnitude to present (or absent) action frames, making the target actions clearly separated from others. However, under category-dimension normalization, all responses are restricted in range (0, 1). Thus, the limited score range would make different categories show close or even mixed responses, which goes against the subsequent temporal action localization process. Results are from video_test_0000444 in the THUMOS14 dataset

Full size image
As shown in Fig. 6c, it is intuitive to first perform category-dimension normalization and obtain aggregation weights ğÌ‚ ğ‘[ğ‘,ğ‘¡], then aggregate the classification socre via weighted sum. This process can be formulated as:

ğÌ‚ ğ‘[ğ‘,ğ‘¡]=exp(ğ[ğ‘,ğ‘¡])âˆ‘ğ¶ğ‘—=0exp(ğ[ğ‘—,ğ‘¡]),
(8)
ğ‘ ğ‘ğ‘›ğ‘ğ‘=1ğ‘‡âˆ‘ğ‘¡=1ğ‘‡ğ[ğ‘,ğ‘¡]â‹…ğÌ‚ ğ‘[ğ‘,ğ‘¡], ğ‘=[0,1,...,ğ¶],
(9)
where ğ‘ ğ‘ğ‘›ğ‘ğ‘ is the predicted score for ğ‘ğ‘¡â„ category by the category normalization and aggregation mechanism .

However, performing category-dimension normalization would damage the separability between the responses of action frames and background frames. Specifically, Fig. 9 depicts two kinds of class activation sequences, where the first one is learned without category-dimension normalization while the second one is learned under the normalization. Essentially, category-dimension normalization compulsively limits the responses for all categories that must lie in the range (0, 1), where the responses of action frames and background frames may be close to each other. In contrast, without category-dimension normalization, for confident background frames, the responses can be negative values with large magnitude; while for confident action frames, the response can be positive values larger than one. Consequently, the class activation sequence of true action categories can be adequately separated from the one of other action categories and the background category.

Top-k Mean Aggregation & Its Advantage
In the above, we discuss three possible strategies to aggregate the class activation sequence into video-level classification score. Specifically, the â€œglobal average poolingâ€ strategy is prone to be influenced by extensively existed background noises, and the â€œtemporal normalization and aggregationâ€ strategy suffers from several dominant large values, while the â€œcategory normalization and aggregationâ€ strategy damages the separability among class activation sequences. Compared with these strategies, the â€œtop-k mean aggregatioâ€ strategy can capture a majority of action frames, and does not destroy the raw magnitude of the class activation sequence, making it a solid choice for the baseline network CASNet. As shown in Fig. 6d, Specifically, given class activation sequence ğ, the top-k mean aggregation strategy predicts the video-level classification score ğ¬=[ğ‘ 0,ğ‘ 1,...,ğ‘ ğ¶] as follows:

ğ‘ ğ‘¡ğ‘šğ‘ğ‘=1ğ‘˜ maxâˆ‘ğ‘¡=1ğ‘‡ğ‘¤ğ‘¡â‹…ğ[ğ‘,ğ‘¡]ğ‘ .ğ‘¡.  ğ‘˜=âŒŠğ‘‡â‹…ğœ‚âŒ‹,ğ‘¤ğ‘¡âˆˆ{0,1}, âˆ‘ğ‘¡=1ğ‘‡ğ‘¤ğ‘¡=ğ‘˜,
(10)
where ğ‘ ğ‘¡ğ‘šğ‘ğ‘ is the predicted score for ğ‘ğ‘¡â„ category by the top-k mean aggregation mechanism, k is the number of selected largest values, which is jointly determined by the hyper-parameter ğœ‚ and the temporal length T. Whether selecting the ğ‘¡ğ‘¡â„ value of ğ[ğ‘,:] is controlled by the indicator ğ‘¤ğ‘¡, which selects k largest values in total, i.e. âˆ‘ğ‘‡ğ‘¡=1ğ‘¤ğ‘¡=ğ‘˜.

In training, the top-k aggregation strategy dynamically selects the most confident k values for each category, and effectively aggregates the class activation sequence ğ into the video-level classification score ğ¬. It provides a practical solution to associate the classification score of T clips with the one and only video-level classification score ğ‘ ğ‘–.

Zero Constraint for Background Category
Fig. 10
figure 10
Effectiveness of video-level classification with background label zero. We show class activation sequence for present action category, absent action category and background category, respectively, where the selected values to perform top-k aggregation are shown in circle â€œâˆ˜â€. For clarity, we show raw aggregated score without softmax normalization. The effectiveness of background label zero is highlighted in pink mask â€œâ€ (Color figure online)

Full size image
In the calculation of video-level classification loss (Equ. 3.2), We find that assigning background label as one inevitably brings the following two limitations, i.e. the learning bias limitation and the response suppression limitation. Firstly, the background label one would set learning bias to the CASNet. Because it is certain that all videos must contain background and the corresponding video-level classification label satisfies ğ‘¦0=1. Under this circumstance, the CASNet would keep â€œlazyâ€ in the training phase and is prone to regard uncertain video frames as background. This prevents CASNet from fully mining training videos and causes limited action localization performance. Secondly, assigning background label as one would influence the video-level classification of other categories. Specifically, consider a video containing ğ‘ğ‘ different action categories, i.e. ğ‘ğ‘=âˆ‘ğ¶ğ‘=1ğ‘¦ğ‘, the training target for ground truth action categories would be 1ğ‘ğ‘+1 when we set the background label ğ‘¦0=1 and perform L1-normalization. In contrast, if we set the background label ğ‘¦0=0, the training target for ground truth categories would be 1ğ‘ğ‘. When ğ‘ğ‘ is small, e.g. ğ‘ğ‘=1, the above two cases pursue distinct training targets. In other words, setting background label as one would decrease the training target for ground truth action categories, which prevents learning high responses for action frames. The learning bias limitation and response suppression limitation inspire us to consider label zero for background category in the video-level classification process.

As shown in Fig. 10, for action categories, we set video-level classification target as one (or zero) for present (or absent) category, which lifts (or suppresses) the corresponding class activation sequence. As for the background category, it should be noticed that the number of annotated background frames is far less than the total frames in the video as well as the number of selected values by the top-k mean strategy. Consequently, from the video-level classification view, the background response for most frames are dominated by the video-level background classification label. Setting background label as zero tends to suppress the background response. Moreover, as for the frame-level classification, we perform supervised classification on annotated background frames, which requires response for any action category should be lower than the backgroundâ€™s response. These two rules conduct an informative guidance to the response of the action category, that action category should exhibit quite low responses at background frames. This is a valuable cue for generating high-quality class activation sequences. Although the studies weakly supervised temporal action localization task does not provide precise annotations for each frame, the astute background response strategy can sensitively and accurately perceive background frames.

Table 3 Parameters for the ğ‘–ğ‘¡â„ Gaussian functions in numerical analysis (ğ‘–=1,2,...7)
Full size table
Mechanism Analysis for Self-Distillation Learning
Theoretical Analysis about the Efficacy of Self-Distillation Learning
Essentially, the temporal action localization network performs classification on each frame feature ğŸğ‘¡ and strives to correctly distinguish action frames from backgrounds, i.e. serving as a frame-level classifier. Suppose this frame-level classifier îˆ´ takes the risk ğœ– to make incorrect classification prediction:

îˆ¼(îˆ´(ğŸğ‘¡)â‰ îˆµ(ğŸğ‘¡))=ğœ–,
(11)
where îˆµ(ğŸğ‘¡) is a function to indicate the ground truth frame-level classification label. Given a video, we can simultaneously use M networks and make M classification predictions, which can be fused together to determine the classification label of the disposed video frame. If these M networks make independent predictions, the error risk of the fused classification prediction can be calculated as:

îˆ¼(îˆ´ğ‘–(ğŸğ‘¡)â‰ îˆµ(ğŸğ‘¡))=âˆ‘ğ‘–=0âŒŠğ‘€/2âŒ‹(ğ‘€ğ‘–)(1âˆ’ğœ–)ğ‘–ğœ–ğ‘€âˆ’ğ‘–,
(12)
According to Hoeffding inequality (Hoeffding 1994), the supremum of the fused classification prediction satisfies the following inequation:

âˆ‘ğ‘–=0âŒŠğ‘€/2âŒ‹(ğ‘€ğ‘–)(1âˆ’ğœ–)ğ‘–ğœ–ğ‘€âˆ’ğ‘–â‰¤exp(âˆ’12ğ‘€(1âˆ’2ğœ–)2),
(13)
This indicates that the risk of the classification error will exponentially decrease with the increase of independent classifiers. Consequently, in order to improve the frame-level classification accuracy, we can employ additional temporal action localization networks and guide them to make high-quality but independent predictions.

Fig. 11
figure 11
Illustration different gradient descent optimization processes and the achieved minimum values. We show some representative optimization processes obtained by the simulated analysis. The two-dimension loss surface is determined by Equ. 14 and the oprimization process is driven by the gradient descent algorithm as shown in Equ. 16. Best viewed in color and zoom in (Color figure online)

Full size image
Simulated Analysis about the efficacy of Self-Distillation Learning
In this section, we make a deeper explanation about the self-distillation learning mechanism via simulated analysis. We first construct a function with multiple local minimum values, then we make qualitative experiments to exhibit the processes that search for the global minimum value.

We regard the two-dimension gaussian function as the basic unit and construct a non-convex loss surface l via accumulating n (e.g. 7) different basic units as follows:

ğ‘™=âˆ‘ğ‘–=1ğ‘›ğ‘ğ‘–2ğœ‹ğœ2ğ‘–ğ‘’âˆ’((ğ‘¥1âˆ’ğœ‡ğ‘–)2+(ğ‘¥2âˆ’ğœˆğ‘–)2)2ğœ2ğ‘–,
(14)
where the parameters for each two-dimension gaussian function are reported in Table 3.

The two-dimension searching space is visualized in Fig. 11. In order to make simulative analysis about the effect of the proposed self-distillation learning, we perform gradient descent optimization on the constructed non-convex function. Specifically, within the considered two-dimension space, the gradient of loss value l relatively to variable ğ‘¥1 can be calculated as follows:

âˆ‚ğ‘™âˆ‚ğ‘¥1=âˆ‘ğ‘–=17âˆ’ğ‘ğ‘–(ğ‘¥1âˆ’ğœ‡ğ‘–)ğœ‹ğœ2ğ‘–â‹…ğ‘’âˆ’((ğ‘¥1âˆ’ğœ‡ğ‘–)2+(ğ‘¥2âˆ’ğœˆğ‘–)2)2ğœ2ğ‘–.
(15)
Similarly, we can calculate the gradient of loss value l relatively to another variable ğ‘¥2 and consequently obtain the gradient [âˆ‚ğ‘™âˆ‚ğ‘¥1,âˆ‚ğ‘™âˆ‚ğ‘¥2] for each point. Based on these gradients, we can make gradient optimization as follows:

[ğ‘¥Ì‚ 1ğ‘¥Ì‚ 2]=[ğ‘¥1ğ‘¥2]âˆ’ğ›¼â‹…â¡â£â¢â¢âˆ‚ğ‘¦âˆ‚ğ‘¥1âˆ‚ğ‘¦âˆ‚ğ‘¥2â¤â¦â¥â¥,
(16)
where ğ›¼ is the learning rate, controlling the learning stride.

In the implementation, we start from a random initial point (ğ‘¥1,ğ‘¥2),ğ‘¥1âˆˆ[0,50],ğ‘¥2âˆˆ[20,30] and repeatedly perform the gradient descent algorithm to gradually reduce the loss value l. Some representative optimization processes are visualized in Fig. 11. It can be found that, the obtained minimum location (ğ‘¥1,ğ‘¥2) exhibits distinct variances when starting from different initial values. Although the gradient descends optimization process has a certain chance to achieve global minimum, it faces massive risks to be stuck into local minimums, which usually limits the performance of the learned model. In contrast, in the proposed self-distillation learning, we simultaneously start from M initial points, {(ğ‘¥ğ‘–1,ğ‘¥ğ‘–2),ğ‘–=1,2,...,ğ‘€,ğ‘¥ğ‘–1âˆˆ[0,50],ğ‘¥ğ‘–2âˆˆ[20,30]}. These ğ‘€âˆ’1 optimization processes of the auxiliary network correspond with the one of the master network, and make the master network be aware of the lower loss value that has been achieved by one initialization process. Consequently, with the correspondence among different optimization processes, the proposed self-distillation strategy is more likely to jump local minimum and achieve high-quality optimization results.

Experiments and Results
In this section, we first introduce three benchmark datasets, evaluation criteria and implementation details.Then, detailed ablation studies and analyses are conducted. Nest, a performance comparison of our method and related state-of-the-art methods is reported. At last, we show some qualitative examples on all three datasets.

Datasets and Evaluation Criteria
We evaluate the usefulness of our method on three datasets: THUMOS14 (Jiang et al. 2014), ActivityNet1.2 and ActivityNet1.3 (Fabian Caba Heilbron and Niebles 2015). Thereinto, ActivityNet1.3 is an enhanced version of ActivityNet1.2. However, there are still some works that show the performance of ActivityNet1.2, for a more complete comparison, we present both results.

For THUMOS14 dataset, it consists of 413 untrimmed videos from 20 classes for the temporal action localization task. Which is divided into 200 and 213 videos for training and evaluation, respectively. Following previous works (Gong et al. 2020b; Jain et al. 2020; Liu et al. 2019b; Yu et al. 2019), we presents mAP (mean average precision) under different IoU (Intersection-over-Union) thresholds: [0.3, 0.4, 0.5, 0.6, 0.7]. It needs to be emphasized that the mAP under IoU of 0.5 is kept a watchful eye on by different methods.

For ActivityNet1.2 dataset, it contains 9682 untrimmed videos from 100 classes. As an updated version, AcitivityNet1.3 consists of 19994 untrimmed videos from 200 classes. Each dataset is split up into three subsets: training, validation, and testing by the ratio of 2:1:1. We report mAP under per IoU threshold from 0.5 to 0.95 with a step of 0.05. According to the previous works, the average mAP of IoU thresholds [0.5:0.05:0.95] is a key evaluation criterion for comparing performance among related works.

Table 4 Ablation studies about the performance of each component on THUMOS14 dataset, measured by mAP at IoU thresholds 0.5
Full size table
Implementation Details
Features We utilize I3D (Carreira and Zisserman 2017) to extract both spatial and temporal features, which is pre-trained on the Kinetics-400 (Carreira and Zisserman 2017) action recognition dataset. The inputs for extracting spatial features are raw RGB video frames to represent appearance information. And the inputs for extracting temporal features are optical flow frames computed through the TV-L1 (Zach et al. 2007) to represent motion information. It is worth noting that the I3D model is not finetuned on THUMOS14 or ActivityNet. Every 16 frames as a snippet input to obtain 1024-dimension features. Then, we concatenate RGB features and optical flow features to get 2048-dimension. Follow BaSNet (Lee et al. 2020; Lin et al. 2018), we scale temporal dimension to unified T during both the training phase and inference phase. The T is 750, 100, and 100 for THUMOS14, ActivivtyNet1.2, and ActivityNet1.3, respectively.

Settings We use PyTorch1.5.1 (Paszke et al. 2019), CUDA Toolkit 10.1.243 (Toolkit 2019) platform to implement our method. Adam (Kingma and Ba 2015) is utilized as an optimizer with a learning rate of 0.0001 and weight decay of 0.0005. The batch size is 16 and the training epoch is 150 for THUMOS14, ActivityNet1.2 and ActivityNet1.3 are set 128 for batch size, 25 for training epoch. Following BaSNet (Lee et al. 2020), hyper-parameter ğœ‚ is set as 1/8. According to the experiment (Details in 6.3), the best performance is achieved when trade-off coefficient ğœ† = 0.6 . In the inference phase, the classification score threshold ğœğ‘ğ‘™ğ‘ is set as 0.25 as previous work (Lee et al. 2020).

Single-frame Annotation SF-Net (Ma et al. 2020) shows that the distribution of the relative position of humans single-action-frame annotation to the corresponding action segment for THUMOS14 dataset, is approximately a uniform distribution. Thus, they use the simulated single frame annotation to implement the experiments. In this work, we follow SF-Net (Ma et al. 2020) to obtain single-frame annotations by uniform sampling. Notice that the SF-Net only annotates single-action-frame but we find it is important to annotate both the single-action-frame and the single-background-frame (For detailed analysis, see Sect. 6.3 ). To make fair comparisons, we only annotate half of the video duration, as shown in Fig. 2. Specifically, we first get videos segment with equivalent interval sampling(e.g. sample 10s, skip 10s etc. ), then we annotate single-action-frame from action segment and single-background-frame from background segment. So our method has the same annotation time cost as SF-Net on the whole.

Table 5 Performance of different frame supervision on THUMOS14 dataset, measured by mAP at different IoU thresholds [0.3, 0.4, 0.5, 0.6, 0.7]
Full size table
Ablation Studies
Effectiveness of each module The Table 4 shows performance on different settings under the mean average precision (mAP) by using different IoU thresholds. If only using weakly supervised video-level action categories information, it can obtain 19.5 at mAP@0.5. When single-action-background supervision information is utilized, it just gets 25.8 by setting the video-level background label as 1. However, with the proposed astute background response, the performance reaches 33.5. Although it is a simple change, it can bring a sharp increase with 7.7 mAP. In order to further improve the performance, we proposed self-distillation learning on two different initialization simple networks. As shown in Table 4, the final result is 35.6, it achieves the new state-of-the-art performance. Actually, the self-distillation learning itself shows disadvantages to the over-localization problem but advantages to the under-localization problem. While the well-performed CASNets can increase its advantages to the under-localization problem and make up its disadvantage to the over-localization problem.

As shown in Table 4, the self-distillation learning strategy improves the performance from 19.5 to 25.1, and brings 6.4 point improvements under mAP@0.5. In addition, the combination of baseline and the astute background response achieves 33.5. Upon this strategy, the self-distillation learning strategy still brings 2.1 improvements. This verifies that the self-distillation learning strategy can brings more advantages than disadvantages to improve the overall performance.

Benefits of annotating both action and background frame As shown in Table 5, comparing to video-level category supervision, it obtains 1.2 gains at mmAP@0.5, when only using single-action-frame supervision (That is similar to SF-Net (Ma et al. 2020), which does not increase instantly (20.4 vs. 20.4Â±0.55)). It may be because Top-k mean strategy is equivalent to single-action-frame supervision in some ways. If training with single-background-frame, it achieves 29.2. Nevertheless, it may lack a clear balance from the action. If both single-action-frame and single-background-frame are used, it reaches 35.6.

Fig. 12
figure 12
Ablation studies about the influence of trade-off coefficient ğœ†, measured by mAP at IoU thresholds 0.5

Full size image
Table 6 Evaluation of the effectiveness when increasing number of auxiliary CASNets, measured by mAP at different IoU thresholds [0.3, 0.4, 0.5, 0.6, 0.7]
Full size table
Table 7 Performance of different ensemble strategies on THUMOS14 dataset, measured by mAP@0.5
Full size table
Trade-off coefficient Evaluation of the effectiveness under different trade-off coefficient ğœ†. Figure 12 shows the performances under mAP@0.5 from ğœ† = 0.2 to ğœ† = 1.0 with step 0.2 on THUMOS14. When ğœ† = 0.6, it can obtain the best performance 35.6. So we utilize this value on THUMOS14, ActivityNet1.2 and ActivityNet1.3.

Number of CASNets In Table 6, we study the influence of using different number of CASNets. Performance of master CASNet without auxiliary CASNets is 33.5 under mAP@0.5. When CASNet with one auxiliary CASNets, it achieves 35.6. If more auxiliary CASNets is utilized, there will still be gains, but it will slow down. In experiments, we use one master network and one auxiliary network, unless otherwise stated.

Comparison with the ensemble Since our framework includes multiple branches, one naive and possible way to improve performance is the ensemble. We design a series of ensemble methods, the performances are shown in the Table 7. First of all, we train and evaluate two different initialized networks separately. Under mAp@0.5, 33.2, and 33.7 are obtained, respectively. Then we try different fuse method, for example, â€™Score mean, CAS meanâ€™ means that calculate mean of video-level classification score from two separate networks as ensemble classification score and maximum of class activation sequences (CAS) from two separate networks as ensemble classification class activation sequences. â€™Score mean, CAS maxâ€™, â€™Score max, CAS maxâ€™, â€™Score max, CAS meanâ€™ are similar operations. It can be seen that there is no obvious improvement. â€™First CAS mean, then calculate scoreâ€™ means that first calculate mean of class activation sequences from two separate networks as ensemble classification class activation sequences, then get ensemble classification score from ensemble classification class activation sequences directly. However, it still did not get better performance. It proves that in weakly supervised temporal action localization task, the simple ensemble method can not bring performance improvement, it also highlights the significance of our proposed method.

Table 8 Performance of different loss functions in the self-distillation learning process on THUMOS14 dataset, measured by mAP at different IoU thresholds [0.3, 0.4, 0.5, 0.6, 0.7]
Full size table
Table 9 Efficacy of employing the self-distillation learning for the video-level classification scores (baseline + VidScore), measured by mAP at different IoU thresholds [0.3, 0.4, 0.5, 0.6, 0.7]
Full size table
Analysis of different loss functions During the implementation of the self-distillation learning strategy, we have considered L1 loss, L2 loss and KL divergence loss. When compared with L2 loss, L1 loss is more suitable to optimize the proposed self-distillation process.The reason is that the classification score s of each frame usually satisfies ğ‘ âˆˆ(0,1). Under this circumstance, the absolute difference between two class activation sequences is usually small, i.e. in the range of (0, 1). Thus, the gradient of L2 loss will become nearly flat along with the decrease of the difference, which will slow down the optimization process. In contrast, the gradient of L1 loss keeps constant and continues to guide the optimization process. Besides, in the self-distillation process, we hope the response values (rather than the distributions) of two class activation sequences to be similar so that we can use a common threshold on these class activation sequences. However, the KL divergence loss is used to measure the difference between two distributions.

To verify the effectiveness of these losses in the proposed learning framework, we separately try â€œL1 lossâ€, â€œL2 lossâ€ and â€œKL divergence lossâ€ to measure differences among the master CASNetâ€™s class activation sequence and the auxiliary CASNetâ€™s class activation sequences. As shown in Table 8, the â€œL1 lossâ€ performs better than the other two.

In addition, we verify the efficacy of employing the self-distillation learning for the video-level classification scores. As shown in Table 9, doing so would lead to performance drop. The reason is that although this strategy could improve the video-level classification performance, it would make the class activation sequences focus on most representative frames and miss complete action instances.

Table 10 Comparison experiments on THUMOS14 dataset, measured by mAP at different IoU thresholds [0.3, 0.4, 0.5, 0.6, 0.7]
Full size table
Table 11 Comparison experiments on ActivityNet1.2 dataset, measured by mAP at different IoU thresholds and average mAP
Full size table
Table 12 Comparison experiments on ActivityNet1.3 dataset, measured by mAP at different IoU thresholds and average mAP
Full size table
Comparison with the State-of-the-art
THUMOS14 As shown in Table 10, we present the performance of not only weakly supervised temporal action localization methods but also some fully supervised ones. There are both early classical and recent approaches. Specifically, some recent weakly supervised method, not only use video-level action category labels but also resort to other forms of supervision information. For example, CMCS (Liu et al. 2019a) searches for static video clips from training subset as hard negative data in training phase. 3C-Net (Narayan et al. 2019) utilizes action count information in each video, WSBM (Nguyen et al. 2019) collects short, untrimmed video clips from social media platforms as training supplements, ActionBytes (Jain et al. 2020) leverage trimmed action videos from Kinetics-400 (Carreira and Zisserman 2017) for training. Lately, SF-Net (Ma et al. 2020) proposed to use single-action-frame supervision. But we come up with single-action-background-frame supervision. For a fair comparison, we only annotate half of the video to maintain a similar annotation cost to SF-Net (Ma et al. 2020). Our method outperforms the state-of-the-art performance at all IoU thresholds. Notably, at IoU 0.5, our method achieves 5.1 improvements. It is worth mentioning that, in the training phase, the proposed SODA method simultaneously trains one master network and multiple auxiliary networks, which may require a bit more computational costs. But in the testing phase, SODA only employs the master network to tackle the input video, predicts the class activation sequence and localizes action instances. This process requires similar computational cost as most existing works (Narayan et al. 2019; Lee et al. 2020; Min and Corso 2020). Moreover, our method is superior to some recent fully supervised temporal action localization woks, e.g. , S-CNN (Shou et al. 2016; Xu et al. 2017; Gao et al. 2018a). And our method achieves performance mAP@0.5=35.6 , which is close to GTAN (Long et al. 2019) with performance mAP@0.5=38.8. Of course, the start-of-the-art fully supervised temporal action localization method PGCN (Zeng et al. 2019) reaches 49.1 under the metric mAP@0.5. It reminds us that weakly supervised methods should continue to carry out researches.

Fig. 13
figure 13
The challenge of over-localization. Qualitative visualization comparison of predicted class activation sequences (CAS) between baseline (single-fame under background label one ) and SODA (ours). GT means ground truth, where â€œâ€ and â€œâ€ indicate actions and backgrounds, respectively. Results are from video_test_0000964 in the THUMOS14 dataset with category BaseballPitch (Color figure online)

Full size image
Fig. 14
figure 14
The challenge of joint-localization. Qualitative visualization comparison of predicted class activation sequences (CAS) between baseline (single-fame under background label one ) and SODA (ours). GT means ground truth, where â€œâ€ and â€œâ€ indicate actions and backgrounds, respectively. Results are from video_test_0000026 in the THUMOS14 dataset with category TennisSwing (Color figure online)

Full size image
Fig. 15
figure 15
The challenge of under-localization. Qualitative visualization comparison of predicted class activation sequences (CAS) between baseline (single-fame under background label one ) and SODA (ours). GT means ground truth, where â€œâ€ and â€œâ€ indicate actions and backgrounds, respectively. Results are from video_test_0000220 in the THUMOS14 dataset with category HighJump (Color figure online)

Full size image
ActivityNet1.2 Table 11 shows the performance comparison of ActivityNet1.2 dataset. AutoLoc (Shou et al. 2018) is one of the earliest methods to report the performance on ActivityNet1.2, and achieves 16.0 with average mAP under thresholds [0:50:0:05:0:95]. At the same time, W-TALC (Paul et al. 2018) reaches 18.0. There are five works that sprang up in 2019, what followed is significant performance improvement, such as CMCS (Liu et al. 2019a) with the performance of 22.4. Although more methods are proposed since 2020, the improvement of performance tends to be stable. In particular, SF-Net (Ma et al. 2020) shows the performance of 22.8 when additional supervised information is used, that is single-action-frame annotations. And, our method utilizes a similar annotation cost to SF-Net, but achieves the performance of 27.4. Compared to some other carefully designed methods, e.g. , DGAM (Shi et al. 2020), ACL (Gong et al. 2020b), performance of our method still is the highest.

Fig. 16
figure 16
Qualitative visualization of class activation sequences (CAS) and ground truth (GT) on THUMOS14 dataset, where â€œâ€ and â€œâ€ indicate actions and backgrounds, respectively. Red boxes mean failure cases. The model predicts some background as actions, because these background exhibits similar motion pattern to ground truth action (Color figure online)

Full size image
Fig. 17
figure 17
Qualitative visualization of class activation sequences (CAS) and ground truth (GT) on ActivityNet1.2 dataset, where â€œâ€ and â€œâ€ indicate actions and backgrounds, respectively (Color figure online)

Full size image
Fig. 18
figure 18
Qualitative visualization of class activation sequences (CAS) and ground truth (GT) on ActivityNet1.3 dataset, where â€œâ€ and â€œâ€ indicate actions and backgrounds, respectively

Full size image
ActivityNet1.3 Some works only report the performance on ActivityNet1.3 dataset, so we also compare with them in Table 12. STPN (Nguyen et al. 2018) makes the first attempt to evaluate weakly supervised temporal action localization performance on this dataset. But, because ActivityNet1.3 is more variable and complex than ActivityNet1.2, the improvement of performance is limited. For example, CMCS (Liu et al. 2019a) achieves an average mAP of 21.2 in 2019, by the year 2020, TSCN (Zhai et al. 2020) just reaches 21.7. Most methods only report the mAP at thresholds 0.5, 0.75, 0.95, we follow A2CL-PT (Min and Corso 2020) to present the performance details at different thresholds. And our method outperforms the state-of-the-art performance at all IoU thresholds. Itâ€™s worth noting that our method achieves a performance of 25.6 under average mAP.

Visualization and Analysis of Localization Results
Over-localization As shown in Fig. 13, we present the comparison between baseline (single-fame under background label one ) and our SODA. The CAS (class activation sequences) from baseline predicts background around the action BaseballPitch as action. But our SODA obtains accurate boundary.

Joint-localization As shown in Fig. 14, we present the comparison between baseline (single-fame under background label one ) and our SODA. There are three action instances adjoin with the category BaseballPitch. From CAS, we can see that the segment between three action instances presents a very high response from baseline. In this case, When a threshold is utilized to separate action and background, the prediction results are made as a whole. In contrast, the CAS from our SODA, the segment between two action instances is suppressed effectively. As a result, the joint-localization problem is well solved.

Under-localization As shown in Fig. 15, we present the comparison between baseline (single-fame under background label one ) and our SODA. The CAS from baseline only predicts the part of action BaseballPitch. In contrast, our SODA achieves complete prediction.

More examples We also present some examples of our complete method with the class activation sequences (CAS) and corresponding ground truth (GT) actions and background on three benchmark datasets. Figure 16 shows qualitative visualization on THUMOS14 dataset. The action instances are dense in THUMOS14, but our method can obtain good predictions. Specifically, some action instances are very close to each other, and our method can still effectively separate them. Qualitative visualizations on ActivityNet1.2 and ActivityNet1.3 are shown in Figs. 17 and 18, respectively. The duration of the action takes up a large proportion of the whole video in both ActivityNet1.2 and ActivityNet1.3. But, the region with high response in the predicted class activation sequences can continuously cover the ground truth action segment.

Conclusion
In this paper, we have proposed a unified framework SODA to tackle the temporal action localization task. It employs astute background response strategy and self-distillation learning strategy, to effectively tackle the over-localization, joint-localization, and under-localization challenges. The astute background response utilizes the conductive effect between video-level classification and frame-level classification to guide the action category to show low response at background frames. The self-distillation learning enhances the ability of the master network via corresponding with multiple auxiliary networks. Extensive experiments on three benchmarks demonstrate the effectiveness of the proposed SODA method. In the future, we will verify the efficacy of the proposed astute background response strategy on similar weakly supervised learning tasks, such as weakly supervised object detection (Zhang et al. 2019b; Song et al. 2020; Zhang et al. 2020a), weakly supervised semantic segmentation (Chan et al. 2020; Yan et al. 2020).