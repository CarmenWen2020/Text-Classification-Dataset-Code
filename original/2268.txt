Abstract
Winograd’s minimal filtering algorithm has been widely used in 2-D Convolutional Neural Networks (CNNs) to reduce the number of multiplications for faster processing. However, it is only effective on convolutions with kernel size as 3 and stride as 1, because it suffers from significantly increased FLOPs and numerical accuracy problems for kernel size larger than 3 and fails on convolution with stride larger than 1. Worse, the extension to ND convolution will intensify the numerical accuracy problem. These problems severely obstruct Winograd’s minimal filtering algorithm’s application to video analysis. In this paper, we propose a novel Decomposable Winograd Method (DWM) for the ND convolution acceleration, which breaks through the limitation of original Winograd’s minimal filtering algorithm to more general convolutions. DWM decomposes kernels with large size or stride>1 to several small kernels with stride as 1 for further applying Winograd algorithm, so that DWM can reduce the number of multiplications while keeping the numerical accuracy. It enables the fast exploration of larger kernel size, larger stride value, and higher dimensions in CNNs for high performance and accuracy and even the potential for new CNNs. Comparing against the original Winograd algorithm, the proposed DWM is able to support all kinds of ND convolutions with a speedup of 1.44×–3.38×, without affecting the numerical accuracy.

Access provided by University of Auckland Library

Introduction
Deep Convolutional Neural Networks (CNNs) have emerged as the premier algorithmic technique for many video related tasks, but their developments are severely impeded by the notorious problem of the massive number of computations. For example, for recognizing a single video frame using CNNs, C3D (Tran et al. 2015) requires 38.5 GFLOPs, I3D (Carreira and Zisserman. 2017) requires 107.9 GFLOPs, and R (2+1) D-34 (Tran et al. 2018) requires 152.4 GFLOPs. Since both 2-D convolutions and 3-D convolutions are important components in CNNs related video analysis, reducing the FLOPs in ND convolutions is essential and meaningful.

The Winograd algorithm has been widely used to reduce the number of computations for a long time. Lavin and Gray (2016) applied Winograd’s minimal filtering algorithm to half reduce the number of multiplications in 2-D convolutions with the kernel of 3×3. As the extension of the 2-D Winograd algorithm, Lan et al. (2017) and Wang et al. (2017) proposed the 3-D version of the Winograd algorithm to accelerate the 3-D convolutions with the kernel of 3×3×3. Unfortunately, these algorithms have several limitations and thus cannot be applied to general situations. First, the size of the convolution kernel should be small such as 3×3 or 3×3×3, otherwise these Winograd algorithms will introduce much more decimal multiplications, which will lead to serious accuracy and efficiency decline. Second, the stride of convolution must be 1 in principle, otherwise these Winograd algorithms will be invalid. Third, when the dimension of convolution increases from 2-D to ND, the first problem will become even worse. Since ND convolutions of kernel size larger than 3 and stride > 1 are frequently used in CNNs, the above three restrictions severely limited the application of Winograd algorithm.

To tackle these problems, we propose the Decomposable Winograd Method (DWM) for ND convolutions, which can speed up all kinds of ND convolutions without any accuracy loss. DWM is inspired by two key observations of the Winograd algorithm. One observation is the scalability of Winograd algorithm: The Winograd algorithm not only can be used on convolutions with kernel size 3, but also can be used on any convolutions with kernel size smaller than 3, including irregular sizes such as 1×2, 2×3, 2×2×3, 2×3×3. The other observation is the decomposability of convolutions: Any convolution with a large kernel size is equivalent to a group of convolutions with small kernel sizes, e.g., a 6×6 convolution can be decomposed to 4 3×3 convolutions. The decomposability of convolutions makes it possible to apply an acceleration algorithm friendly to convolutions of small kernel size to convolutions of large kernel size.

DWM is proposed based on the above two observations. The core idea of DWM is to use small “lego kernels” to assemble any large kernels, based on the decomposability of convolutions. Then we apply the Winograd acceleration algorithms to small “lego kernels” to calculate the results of large kernels, based on the scalability of Winograd algorithm. DWM can speed up all kinds of ND convolutions without any accuracy degradation, especially convolutions of large kernels and stride > 1. Specifically, for kernels larger than 3, DWM decomposes the original convolution kernel into several kernels equal or smaller than 3, on which we can apply the ND Winograd algorithms successfully. Thus, for large kernels situations, DWM can still reduce the number of multiplications by 50–70% and keep the numerical accuracy the same as the original convolution. Moreover, for stride larger than 1 situations, DWM splits the kernels into several parts with stride 1. e.g., the 3D convolutions with stride 2 is split into 8 parts, and each part is a convolution with smaller kernel sizes and stride 1. This also corresponds to split the polynomials of Winograd’s minimal filtering algorithm into odd ones and even ones and compute them respectively, which guarantees the equivalence theoretically. Finally, we derive the Winograd algorithm with NDimension. When applying Winograd algorithm to ND convolutions with kernel sizes smaller than 3, for convolutions with different dimensions, the only difference is the power of coefficient 2 in the transformation matrices, which does not harm the accuracy and efficiency. From the above, DWM breaks through the kernel size and stride restrictions of the traditional Winograd algorithm, and it can be extended to NDimension convolutions.

DWM shows the advantages of both efficiency and numerical accuracy. DWM can efficiently reduce the multiplications of regular convolutions with any kernel size, while the numerical accuracy dropping is negligible. These advantages of DWM enables the fast exploring of ND convolutions with larger kernel size and larger stride value in CNNs for high performance and accuracy, especially for video related tasks which are limited by high computational complexity. The application of DWM may also inspire the potential for new ND CNNs for video analysis, such as networks from Neural Architecture Search (NAS) which has many convolutions with large kernels (Cai et al. 2018; Piergiovanni et al. 2019).

We verify the effectiveness and efficiency of DWM on many 2-D and 3-D convolutions with different kernel sizes, as well as many networks of multiple tasks related to videos. Our experiments show that DWM achieves ∼2× acceleration in 2-D convolutions and ∼3× acceleration in 3-D convolutions while keeping the numerical error under E-07. The numerical accuracy of DWM is close to or even higher than the numerical accuracy of FP32 convolutions, while the traditional Winograd algorithm has poor accuracy when the kernel size of the convolution is large. Furthermore, we estimate the influence of this accuracy problem in some video architectures, such as video super-resolution, video denoising, and optical flow estimation. We find that using DWM always keeps the inference result the same as the original convolutions, while using the traditional Winograd algorithm may harm the result seriously. Additionally, another experiment on the comparison among 1-D to 6-D convolutions reveals that the increase of dimension will intensify the numerical problem of the traditional Winograd algorithm, but has no influence on DWM.

This paper extends our previous work (Huang et al. 2020) with the following additional contributions: (1) Theoretically, we extend the original DWM to ND convolutions, present the theoretical analysis of acceleration, and reveal the relationship between the decomposition methods and the FLOPs. (2) Practically, we implement the 3-D DWM, measure the numerical accuracy and the wall clock time of different scales of convolution operations, and show the accuracy performance on several networks in video analysis.

The rest of the paper is organized as follows. In Sect. 2, we discuss the history of the Winograd algorithm and different kinds of CNNs on video analysis. Sections 3 and 4 introduce the background information of the Winograd algorithm and the principal method of DWM with a theoretical analysis. Section 5 illustrates the experimental results of ND DWM, and Sect. 6 gives the conclusion.

Related Work
The Winograd Algorithm
The Winograd algorithm is a popular algorithm for convolution acceleration of small kernels. At the very beginning, Lavin and Gray (2016) applied Winograd’s minimal filtering algorithm to half reduce the number of multiplications in 3×3 convolutions. For 3-D cases, Lan et al. (2017) and Wang et al. (2017) proposed a novel Winograd algorithm on 3×3× convolutions, which gains a 1.05−2× speedup compared to cuDNN (Chetlur et al. 2014).

Despite the dimension extension, some researchers made their efforts to overcome the defects of the Winograd algorithm. Some researchers attempted to solve the numerical accuracy and kernel size problem. Barabasz and Gregg (2019) investigated a wider range of Winograd algorithms for DNNs, which significantly improve floating-point accuracy in larger feature maps cases. Although this approach gives a 6.5 times better accuracy in FP16, it still suffers from accuracy problems and has not been tested on large scale applications. Vincent et al. (2017) decreased the numerical error of large tile Winograd algorithm by selecting the polynomial points. However, this method has its limitation and cannot solve the accuracy problem thoroughly. Meng and Brothers. (2019) extended the Winograd algorithm to larger tiles by introducing complex numbers during the Winograd transformation, but the complexity of the calculation of complex numbers impede its scalability and implementation.

Other researchers focused on the hardware implementation of the Winograd algorithm. Due to the low memory ceiling of GPU hardware, the Winograd algorithm can be used to speed up CPU convolution operations and achieves 5 to 25-fold improvement in throughput compared to previous state-of-art implementations (Budden et al. 2017). Besides, for the mobile CPU acceleration, the Winograd algorithm achieves up to 60% performance improvements in the full network compared to im2col/im2row based optimization techniques (Maji et al. 2019).

Different from the researches mentioned above, our methods focus on the scalability of the Winograd algorithm. By extending the Winograd algorithm to much wider situations, including larger kernel sizes, larger strides, and higher dimensions to ND, we efficiently reduce the number of multiplications while keeping the calculation’s numerical accuracy stably high.

CNNs for Video Analysis
CNNs have been widely used in video related tasks. It is very important to effectively capture the temporal information in videos. Two-stream CNNs with 2-D convolutions are widely adopted to capture the spatio and temporal information respectively. Simonyan and Zisserman (2014) proposed a two-stream convolutional network on video classification. For the two-stream networks, how to fuse the two streams of spatio and temporal remains a question. Feichtenhofer et al. (2016) presented several strategies for spatio-temporal fusion.

Considering the temporal dimension in video as a new dimension for convolution, many researchers have applied 3-D convolutions to video tasks successfully. 3-D convolutions are capable of exploiting spatio-temporal information simultaneously. C3D Tran et al. (2015) tested for several different sizes of 3-D convolutions, and chose 3×3×3 for action recognition and video object detection. Hara et al. (2018); Tran et al. (2017) used the 3-D version of ResNets (He et al. 2016) to obtain deeper and better CNNs and achieve superior performance of action recognition. I3D (Carreira and Zisserman. 2017) extended Inception networks to 3-D versions and succeeded in action recognition. However, the huge amount of computations limits the application of 3-D convolutions in practice.

Furthermore, ignoring the efficiency consideration, convolutions with large kernel sizes are of great importance to capture long time interactions. Recently, SlowFast (Feichtenhofer et al. 2019) network combined large 3-D convolutions with ResNets, and achieved high performance in action recognition. Piergiovanni et al. (2019) adopted many convolutions with large kernel sizes using neural architecture search (NAS). In addition to action recognition and video object detection, V2V (Tran et al 2016) did well in video semantic segmentation, optical flow estimation, and video coloring by equipping C3D with several 3-D convolutions and deconvolutions with large kernel sizes. Wang et al. (2019a) and Chang et al. (2019) applied 3-D convolutions with large kernel sizes to video inpainting.

Besides, ND convolutions with higher dimensions also have their potential for video related tasks. Choy et al. (2019) proposed one kind of 4-D CNNs called Minkowski Convolutional Neural Networks for 3-D video analysis. V4D (Zhang et al. 2020a) employed 4-D residual blocks to model the evolution of long-range spatio-temporal representation. Shi et al. (2020) used a sparse 4D convolution to process the 4D grid generated by high-level perceptions.

In conclusion, ND convolutions and convolutions with large kernel sizes show potential advantages for video analysis, but the high computation complexity restricts their application in real-world. Thus, accelerating ND convolutions with large kernel sizes is a significant question that will benefit the development of researches for video related tasks.

Preliminary on the Winograd Algorithm
The Winograd Algorithm
As an equivalent problem of multi-dimensional FIR filters problem, convolution can be implemented more efficiently using Winograd minimal filtering algorithm (Lavin and Gray 2016). Denoting the result of computing m outputs with an r-tap FIR filter as F(m, r), which can be considered as a 1-D convolution with stride 1, output size m, and kernel size r, the corresponding convolution algorithm for it requires 𝑚+𝑟−1 multiplications.

The traditional Winograd algorithm is derived from the relationship between polynomial multiplication and 1-D convolutions using the Chinese Remainder Theorem(CRT) (Winograd 1980). For the fixed m and r, the whole algorithm contains three fixed transformation matrices: A, B, and G.

Considering the original 1-D situation, the r element filter g(x) and 𝑙=𝑚+𝑟−1 element input signal (i.e.feature map) d(x) can be represented as polynomials (0<𝑟<𝑙):

𝑔(𝑥)𝑑(𝑥)=𝑔𝑟−1𝑥𝑟−1+𝑔𝑟−2𝑥𝑟−2+⋯+𝑔1𝑥+𝑔0,=𝑑𝑙−1𝑥𝑙−1+𝑑𝑙−2𝑥𝑙−2+⋯+𝑑1𝑥+𝑑0,
(1)
then the result of convolution 𝑔(𝑥)∗𝑑(𝑥) can be obtained by calculating the coefficients of polynomial multiplication

𝑦(𝑥)=𝑔(𝑥)𝑑(𝑥).
(2)
Applying CRT, we can get three transformation matrices A, B and G, and the process of doing convolution can be formulated as the following:

𝑌=𝐴𝑇[(𝐺𝑔)⊙(𝐵𝑇𝑑)],
(3)
where Y denotes the convolution output and ⊙ denotes element-wise multiplication. From Eq. (3), we can derive the gradient of neuron (denoted as ∇𝑑) and the gradient of weight (denoted as ∇𝑔) of Winograd algorithm using the chain rule:

∇𝑑∇𝑔=[(∇𝑌𝐴𝑇)⊙(𝑔𝐺𝑇)]𝐵𝑇,=[(∇𝑌𝐴𝑇)⊙(𝑑𝐵)]𝐺,
(4)
where ∇𝑌 is the gradient passed from the next layer. For ND convolutions, we can nest the F(m, r) with itself for N times. A 2-D case is

𝑌=𝐴𝑇[(𝐺𝑔𝐺𝑇)⊙(𝐵𝑇𝑑𝐵)]𝐴,
(5)
and the corresponding backpropogation is:

∇𝑑∇𝑔=𝐵[(𝐴∇𝑌𝐴𝑇)⊙(𝐺𝑔𝐺𝑇)]𝐵𝑇,=𝐺𝑇[(𝐴∇𝑌𝐴𝑇)⊙(𝐵𝑇𝑑𝐵)]𝐺.
(6)
Drawbacks
The Winograd algorithm will become less accurate, inefficient, or even invalid in two situations: large kernel size and stride >1. Details are illustrated as follows.

Large Kernel Size
The benefit of the Winograd algorithm comes from the simplicity of transformation matrices. For example, applying the Winograd algorithm, the transformation matrices of F(2, 3) (i.e.the convolution with output size 2 and kernel size 3) are shown as follows:

𝐵𝑇𝐺𝐴𝑇=⎡⎣⎢⎢⎢⎢100001−11−1110000−1⎤⎦⎥⎥⎥⎥,=⎡⎣⎢⎢⎢⎢11/21/2001/2−1/2001/21/21⎤⎦⎥⎥⎥⎥,=[10111−10−1].
(7)
However, when the kernel size grows beyond 3, the transformation matrices will become complicated, for they will be filled with decimals. The larger the kernel size is, the more complicated the transformation matrices will be. Considering F(2, 5) (i.e.the convolution with output size 2 and kernel size 5) as an example, the transformation matrices becomes something like

𝐵𝑇𝐺𝐴𝑇=⎡⎣⎢⎢⎢⎢⎢⎢⎢4000000−44−224−5−4−4−1−1001−12−2−5111110000001⎤⎦⎥⎥⎥⎥⎥⎥⎥,=⎡⎣⎢⎢⎢⎢⎢⎢⎢1/4−1/6−1/61/241/2400−1/61/61/12−1/1200−1/6−1/61/61/600−1/61/61/3−1/300−1/6−1/62/32/31⎤⎦⎥⎥⎥⎥⎥⎥⎥,=[10111−1121−201].
(8)
The huge number of decimals in transformation matrices makes the Winograd transformation not only high consumption, but also less accurate.

Stride > 1
Another problem of the Winograd algorithm is that it cannot be applied to stride > 1 convolutions. When the stride is larger than 1, the convolution cannot be equivalent to the polynomial multiplication, which will make the derivation in Sect. 3.1 invalid. Thus, the Winograd algorithm cannot be applied to convolutions with stride > 1 directly. Therefore, although the Winograd algorithm can implement convolutions much more efficiently, it is always used on 3×3/3×3×3 and stride 1 convolutions only.

The ND Decomposable Winograd Method
In this section, we propose a series of techniques called the Decomposable Winograd Method (DWM) to apply the Winograd algorithm mentioned above to more general cases, including NDimension, larger kernels and stride > 1.

The ND Winograd Algorithm
figure a
We firstly introduce the ND version of Winograd algorithm. Considering that both the convolution operation and the Winograd transformations are linear, Winograd algorithm of the NDimension situation can be decomposed into the combination of that of 1-dimensions. Without loss of generality, suppose the length of feature maps in the Nth dimension is m, then the ND feature map d can be split into {𝑑1,…,𝑑𝑚}, where each 𝑑𝑖(1≤𝑖≤𝑚) is (𝑁−1)-dimensional. Similarly, suppose the length of the convolution kernels in the Nth dimension is 𝑛(𝑛<𝑚), then the ND convolution kernel g can be split into {𝑔1,…,𝑔𝑛}, where each 𝑔𝑖(1≤𝑖≤𝑛) is (𝑁−1)-dimensional. The ND convolution of d and g can be decomposed into two processes: the corresponding (𝑁−1)-D convolution of 𝑑𝑖 and 𝑔𝑖, and the sliding of 𝑔𝑖 along the Nth dimension. By this analogy, the ND convolutions can be decomposed into the combination of 1-D ones. Thus, the transformation step of the ND Winograd algorithm consists of a straight forward iteration: Each dimension of data D will be multiplied by the corresponding transformation matrix M, as described in Algorithm 1. Although the ND Winograd transformation introduced here is a serial version, the calculation of each dimension are independent, and the coefficients in the transformation matrices are fixed. Therefore, in the implementation, we usually use a parallel one to accelerate the transformation process on graphics processing units (GPUs) or other accelerating hardwares, which can significantly reduce the cost of transformation.

Since the ND Winograd algorithm is the combination of 1-D Winograd algorithm, it is also less accurate for large kernel situations and invalid for stride > 1 situation. As mentioned in Sect. 3, when applying Winograd algorithm to convolutions with kernel size larger than 3, the transformation matrices are full of decimals, which is easy to cause the loss of accuracy. More seriously, the N times matrix multiplication in the ND Winograd algorithm will intensify the numerical accuracy problem because the loss of accuracy will be enlarged in an exponential way according to N. For stride > 1 situation, the derivation of the ND Winograd algorithm is also invalid, which makes the ND Winograd algorithm be failed in convolutions with stride > 1.

Large Kernel Size
As we denote before, g(x) represents the 1-D convolution filter and thus r represents the size of convolution filter. Observing the derivation process above, we find that when r becomes larger, g(x) in Eq. (1) can be split into several small polynomials, to which we can apply the traditional Winograd algorithm:

⎧⎩⎨⎪⎪⎪⎪⎪⎪𝑔(0)(𝑥)𝑔(1)(𝑥)⋮𝑔(⌊𝑟/3⌋)(𝑥)=𝑔2𝑥2+𝑔1𝑥+𝑔0=(𝑔5𝑥2+𝑔4𝑥+𝑔3)𝑥3=∑𝑖=0𝑟−1𝑚𝑜𝑑3𝑔𝑟−𝑖−1𝑥(𝑟−1𝑚𝑜𝑑3)−𝑖𝑥3⌊𝑟/3⌋.
(9)
Then from

𝑔(𝑥)=∑𝑖=0⌊𝑟/3⌋𝑔(𝑖)(𝑥)
(10)
we can get

𝑦(𝑥)=𝑔(𝑥)𝑑(𝑥)=∑𝑖=0⌊𝑟/3⌋[𝑔(𝑖)(𝑥)𝑑(𝑥)]=∑𝑖=0⌊𝑟/3⌋𝑦𝑖(𝑥),
(11)
We can apply F(2, 2) or F(2, 3) to each 𝑦𝑖(𝑥) separately, with half multiplications reduced. As for ND convolution, we can split the large kernel into (𝑟/3+1)𝑁 small parts, and then apply Winograd algorithm to each part separately. The whole process is illustrated by Fig. 3 (take 2-D version as an example). We can process a common large kernel convolution in five steps:

Splitting Split the convolution kernel into several parts whose kernel size is smaller than 3, and then process the input signal (i.e.feature map) by slicing the redundant edges. This step is shown in Fig. 1a for 2-D convolutions and Fig. 2a for 3-D convolutions.

Transformation Apply corresponding Winograd transformation 𝐵𝑇 and G using Algorithm 1 (in F(2, 2) or F(2, 3)) on each part.

Calculation Do element-wise multiplication and channel-wise summation. It is usually be implemented as a transposed batched matrix multiplication in practise.

Detransformation Do the A inverse transformation using Algorithm 1 (in F(2, 2) or F(2, 3)) to change the intermediate results to spatial domain.

Aggregation Sum the calculation results of each part, which gives the final result that equivalent to the original convolution.

Fig. 1
figure 1
a Splitting a 5×5 and stride 1 convolution into four smaller convolutions; b splitting a 5×5 and stride 2 convolution into four stride 1 convolutions. “*” denotes doing convolution with corresponding parts.

Full size image
Fig. 2
figure 2
a Splitting a 5×5×5 and stride 1 convolution into eight smaller convolutions; b splitting a 5×5×5 and stride 2 convolution into eight stride 1 convolutions. ”*” denotes doing convolution with corresponding parts.

Full size image
Fig. 3
figure 3
The process of doing 2-D convolution using Winograd algorithm. The four dotted frames denote convolution procedure of the four split parts, and each procedure can be divided into five steps: splitting, transformation, calculation, detransformation and aggregation.

Full size image
For example, when 𝑟=5, we can split the 1-D situation into two parts

{𝑔(0)(𝑥)𝑔(1)(𝑥)=𝑔2𝑥2+𝑔1𝑥+𝑔0=(𝑔4𝑥+𝑔3)𝑥3,
(12)
and

𝑔(𝑥)=𝑔(0)(𝑥)+𝑔(1)(𝑥).
(13)
Then we get:

𝑦(𝑥)=𝑔(𝑥)𝑑(𝑥)=𝑔(0)(𝑥)𝑑(𝑥)+𝑔(1)(𝑥)𝑑(𝑥).
(14)
For 𝑔(0)(𝑥)𝑑(𝑥), we can apply F(2, 3) to it, and for 𝑔(1)(𝑥)𝑑(𝑥), we can apply F(2, 2). For a 2-D 5×5 convolution case, we can split the kernel into 4 parts: 3×3, 3×2, 2×3 and 2×2, just as the method shown in Fig. 1a. When it comes to a 3-D convolution case with kernel size 5×5×5 as illustrated in Fig. 2a, we can split the kernel into 8 parts: 3×3×3, 3×3×2, 3×2×3, 3×2×2, 2×3×3, 2×3×2, 2×2×3, and 2×2×2.

This method’s advantage is that using F(2, 2) and F(2, 3) instead of larger ones reduces the amounts of decimal multiplications in transformation matrices. Therefore, DWM can not only implement the multiplications of Winograd transformation efficiently but also keep the Winograd algorithm’s accuracy. Furthermore, these two advantages still hold for the dimension extension, i.e.ND convolutions can be accelerated by this algorithm without accuracy loss.

Stride > 1
Normal polynomial multiplication indicates stride 1 convolution, but we can surmount this barrier by grouping the polynomial into several parts. Denoting convolution stride as s, 1-D convolution kernel g(x) with r elements can be split into

⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪𝑔(0)(𝑥)𝑔(1)(𝑥)⋮𝑔(𝑠−1)(𝑥)=∑𝑖=0⌊(𝑟−1)/𝑠⌋𝑔𝑠∗𝑖𝑥𝑠∗𝑖=∑𝑖=0⌊(𝑟−2)/𝑠⌋𝑔𝑠∗𝑖+1𝑥𝑠∗𝑖+1=∑𝑖=0⌊(𝑟−𝑠−1)/𝑠⌋𝑔𝑠∗𝑖+𝑠−1𝑥𝑠∗𝑖+𝑠−1.
(15)
The input signal (i.e.feature map) d(x) with l elements can be split into 𝑑(0)(𝑥),𝑑(1)(𝑥),…,𝑑(𝑠−1)(𝑥) similarly. Then we get several stride 1 convolutions which can be represented by polynomials multiplications 𝑦(𝑖)(𝑥)=𝑔(𝑖)(𝑥)𝑑(𝑖)(𝑥) and 𝑦(𝑥)=∑𝑦(𝑖)(𝑥). We can apply Winograd algorithm to each 𝑦(𝑖)(𝑥) which is valid. Thus, the DWM reduces the multiplications of 1-D convolutions with stride > 1 successfully.

For ND convolutions, we nest the 1-D convolution methods for N times, decomposing the kernel into 𝑁𝑠 part. This process also contains five parts: splitting, transformation, calculation, detransformation, and aggregation, which is similar to Fig. 3.

For instance, when we are dealing with stride 2 convolutions, we can group the convolution kernel g(x) and input signal (i.e.feature map) d(x) by their degree’s parity, and then get

⎧⎩⎨⎪⎪⎪⎪𝑔(0)(𝑥)𝑔(1)(𝑥)=∑𝑖=0⌊(𝑟−1)/2⌋𝑔2𝑖𝑥2𝑖=∑𝑖=0⌊(𝑟−2)/2⌋𝑔2𝑖+1𝑥2𝑖+1
(16)
and

⎧⎩⎨⎪⎪⎪⎪𝑑(0)(𝑥)𝑑(1)(𝑥)=∑𝑖=0⌊(𝑙−1)/2⌋𝑑2𝑖𝑥2𝑖=∑𝑖=0⌊(𝑙−2)/2⌋𝑑2𝑖+1𝑥2𝑖+1.
(17)
In a 2-D case, a 5×5 stride 2 convolution on 7×7 activation can be split into doing four stride 1 convolutions: 3×3, 3×2, 2×3 and 2×2. The splitting method is shown in Fig. 1b. As for a 3-D case, a 5×5×5 stride 2 convolution on 7×7×7 activation can be split into doing 8 stride 1 convolutions: 3×3×3, 3×3×2, 3×2×3, 3×2×2, 2×3×3, 2×3×2, 2×2×3, and 2×2×2, shown in Fig. 2b.

Occasionally, we may need to do a stride > 2 convolution with a large kernel size, then we can combine the two techniques mentioned above. By applying these two techniques, we can efficiently reduce the number of multiplications of all kinds of ND convolutions.

Backpropagation
Using Eq. (4), we can apply the Winograd algorithm to the process of calculating gradients, and this will also reduce the number of multiplications by the same scale. Employing the chain rule, we can derive that for 1-D case,

𝑔𝑤𝑖𝑛𝑜(𝑡+1)=𝐺(𝑔𝑛𝑜𝑟𝑚(𝑡)+∇𝑔𝑛𝑜𝑟𝑚(𝑡))=𝐺𝑔𝑤𝑖𝑛𝑜(𝑡)+𝐺(𝐺𝑇[(𝐴∇𝑌(𝑡))⊙(𝐵𝑇𝑑(𝑡))]),
(18)
where 𝑔𝑛𝑜𝑟𝑚 denotes the weight before the Winograd transformation, 𝑔𝑤𝑖𝑛𝑜 denotes the weight after the Winograd transformation and t denotes the update iteration. Noticing that 𝐺𝐺𝑇≠𝐸, we should keep weight W in the normal form (the data before transformation) instead of the Winograd form (the data after transformation) during the training process in order to make the Winograd algorithm being an equivalent substitute of the normal convolution.

For 1-D DWM, considering the two techniques mentioned above, we can derive the corresponding back propagation rules. Denoting the DWM output as Y, the two techniques have the same form in the aggregation step:

𝑌=∑𝑖𝑌(𝑖).
(19)
From the derivative rules we know that for part j:

∇𝑌(𝑗)=∇(𝑗)𝑌(𝑗)+∇(𝑗)∑𝑖≠𝑗𝑌(𝑖)=∇(𝑗)∑𝑖𝑌(𝑖)=∇(𝑗)𝑌.
(20)
This means that we don’t need to store each output 𝑌(𝑖) for the backpropagation. Furthermore, we can derive that in the splitting step, the backpropagation acts as follows:

∇𝑑=∑∇𝑑(𝑖),∇𝑔=∑∇𝑔(𝑖),
(21)
where i denotes different parts produced by DWM. The ND case of the backpropagation can be derived easily from Algorithm 1, which is also an N times nesting of 1-D case.

The Decomposition Strategy
Fig. 4
figure 4
One of the decomposition strategies of the 3-D convolution kernel

Full size image
In this part, we will give the relationship between the theoretical FLOPs and decomposition strategy of large convolution kernels. To calculate the theoretical FLOPs, a simple assumption is that the cost of the Winograd transformation of convolutions with small kernels can be ignored. There are four reasons. First, the simplicity of transformation. Almost all the transformation coefficients are the power of 2, and thus there are only add and manipulation of the exponent during the transformation process, costing little computation resources. Second, the reuse of feature maps and kernels. For the convolution operation, each input transformation can be reused for the number of the filters times, and each weight transformation can be reused for the batch size times, which largely reduces the average cost of transformations. Third, the optimization of implementation. The time cost by the transformations can be largely optimized by the implementation, using techniques like soft pipeline and so on, and the theoretical analysis gives the upper bound of the real speed. Finally, if we use DWM for inference purposes only instead of training, then the transformation of weight can be done offline separately, which will further reduce the transformation cost.

Adopting this assumption, for example, given a 3-dimension convolution kernel with (𝐷≤3,𝐻≤3,𝑊≤3), fixing the size of output to 2×2×2 and ignoring the batch size and the channels, we can calculate the theoretical FLOPs of this convolution is (𝐷+1)(𝐻+1)(𝑊+1).

When the kernel size is larger than 3, DWM decomposes it into several kernels with kernel size smaller than 3. The relationship between the theoretical FLOPs and decomposition strategy is as follows:

claim Given a NDimension convolution kernel with size (𝑆1,𝑆2,…,𝑆𝑁), and decomposing each dimension into (𝑚1,𝑚2,…,𝑚𝑁) pieces , the theoretical total FLOPs will always be ∏𝑖=1𝑁(𝑆𝑖+𝑚𝑖), which has nothing to do with the pieces’ details.

Proof
Without loss of generality, we can assume the size of each dimension piece is

(𝑥11,𝑥12,…,𝑥1𝑚1,𝑥21,𝑥22,…,𝑥2𝑚2,…,𝑥𝑁1,𝑥𝑁2,…,𝑥𝑁𝑚𝑁),
(22)
see Fig. 4 for a 3-D case. The total FLOPs can be calculated as follows:

	(23)
This claim shows that the fewer pieces we decompose the large kernel into, the less computation is required. Thus, ignoring the implementation details, the best decomposition strategy is to make the small kernels as large as possible, under the constraint that the size of them should always equal or smaller than 3.

Memory Consuming
The main drawback of DWM is its memory consuming. Taking stride 1 convolutions as an example, we will give a theoretical analysis in this part.

The convolution operation’s memory is strongly correlated to the number of parameters and the size of feature maps. Thus, the memory expansion of DWM comes from two parts: the transformation of convolution kernels and the split of feature maps. For the former reason, as shown in Fig. 3, the Winograd transformation will expand the convolution kernels. For example, a 3x3 convolution kernel will be expanded to a 4×4 one after the Winograd transformation. Furthermore, DWM intensifies this problem. Similar to Equation 23, we can derive the size of the transformed ND convolution kernels using the same notations as Eq. 23 and P for the number of parameters:

	(24)
where 𝐶𝑖𝑛 denotes the number of input channels and 𝐶𝑜𝑢𝑡 denotes the number of filters. The expansion ratio is

	(25)
The numerator is obviously larger than the denominator by an addend 𝑚𝑗. While 𝑚𝑗 denotes the pieces split in the jth dimension, the more pieces are split into, the larger the memory cost of parameters. In practice, the largest number of pieces in each dimension is 4 for kernel size 11, making the expansion of memory acceptable.

For the latter reason, also shown in Fig. 3, the Winograd algorithm is always applied to small feature maps, so the original feature map is needed to be split into several small pieces. For example, when we are doing a 3×3 convolution with stride 1, the original feature map must be split into 4×4 pieces with overlap 2. The overlap leads to the expansion of memories, and DWM intensifies this by splitting the feature map more times. Similar to Eqs. 23 and 24 above and denoting A as the number of activation elements, we can derive that:

	(26)
where B denotes the batch size and 𝑂𝑖 denotes the output size of the ith dimension. Considering the overlap when doing the traditional convolution, the expansion ratio can be written as:

	(27)
This equation is not always greater than 1. However, we notice that the implementation has a substantial impact on the memory cost. Practically, the overlap of the convolution can be reused easier than DWM. Moreover, when we use“non-fused”implementation which means that implementing the algorithm in separated different CUDA kernels, more device memories are used; and when we use“fused” implementation, which implements the algorithm in only one kernel at the cost of shared memory, fewer device memories are used..

Comparison and Discussion
There are also many existing model acceleration methods including channel pruning, fine-grained pruning, quantization, tensor factorization, and compact network design. We will discuss the relationship of DWM and these methods in this part.

Structured Pruning Structured pruning aims to prune the less important structures using methods such as loss minimization, layer-by-layer approaches (Guo et al. 2020a), regularization (xin et al. 2019) and a combination of several approaches (Guo et al. 2020b). These methods are orthogonal to DWM, which means that DWM still works after applying channel pruning methods to the networks. The reason is that DWM is a pure convolution acceleration algorithm which has nothing to do with anything except for the computation stage of convolution. Thus, once we use channel pruning methods to compress and accelerate the computation of our models, we can continue to use DWM to accelerate the computing stage of convolution.

Fine-grained Pruning Aiming to reduce both the number of parameters and the amount of calculation, fine-grained pruning (Han et al. 2016) is driven by the idea that parameters with small values are not important. Comparing with this, DWM utilizes the decomposable property of convolution calculation, which is a different perspective. Different from channel pruning, fine-grained pruning destroys the structure inside kernels and changes the computation stage of convolution. Thus, it cannot be efficiently combined with DWM. Some researchers attempt to solve this problem (Liu et al. 2018).

Quantization Quantization methods aim to accelerate the computing of networks by converting floating-point arithmetics to integer ones (Courbariaux et al. 2014; Gupta et al. 2015; Zhou et al. 2016; Wu et al. 2016; Zhou et al. 2017; Wang et al. 2019b; Jacob et al. 2018; Zhang et al. 2020b; Zhu et al. 2020). The transformation process of DWM may produce 12, which conflicts with quantization. However, skipping4 the transformation process, the computation process of DWM can be quantized, which achieves a greater speedup ratio. Therefore, quantization methods can also be combined with DWM, with some special techniques.

Tensor Factorization Tensor factorization attempts to decompose convolution kernels into smaller ones using inequivalent methods such as singular value decomposition (SVD) (Xue et al. 2013) and low-rank expansion (Jaderberg et al. 2014), aiming to save the amount of parameters and gain a speed acceleration. Driven by the similar idea of decomposing, DWM is an equivalent accelerating method that does not have any risk of accuracy loss. This property makes DWM a fast equivalent substitute for the convolution.

Compact Network Design The goal of compact network design is to find fast, small, and high-performance networks using hand-crafted structures (Wang et al. 2016; Zolfaghari et al. 2018; Chen et al. 2019) and Neural Architecture Search (NAS) (Zoph and Le. 2017; Cai et al. 2018; Liu et al. 2019; Real et al. 2019; Piergiovanni et al. 2019; Liu et al. 2021). DWM is compatible with all these architectures, since DWM is an equivalent substitute for convolution. Moreover, the rise of NAS shows the potential of large kernel size convolutions in video analysis. For example, EvaNet (Piergiovanni et al. 2019) shows that given a suitable training strategy, the 3-D convolutions with large kernels like 5, 7, 9, and even 11, play an important role in video-related tasks such as action recognition. The success of DWM in accelerating ND convolutions with large kernel sizes will benefit the development of researches for video-related tasks.

Other Winograd Methods We are not the first one who tries to solve the large kernel size problem that appeared while applying the Winograd algorithm. Lu et al. (2017) tried to implement the Winograd algorithm on FPGAs, and they solved large kernel problems by padding the kernel. However, the padded elements will be filled with non-zero values after the Winograd transformation, which means that paddings will bring lots of extra calculations. For DWM, we find a way that precisely separates the convolution operations without padding. This method avoids any redundant floating-point multiplications during Winograd transformation, and thus achieves the best acceleration without any numerical accuracy loss. Additionally, this paper extends DWM to NDimension, which shows the strong scalability of DWM.

Table 1 The mean squared errors (MSE) of 2-D convolutions which are measured between different acceleration algorithms and the FP64 convolution results
Full size table
Table 2 The mean squared errors (MSE) of 3-D convolutions, which are measured between different acceleration algorithms and the FP64 convolution results
Full size table
Experiments
Overall Settings
All the results are tested on the graphics processing unit (GPU) NVIDIA Tesla V100. We implement DWM on TensorFlow (Abadi et al. 2016) and PyTorch (Paszke et al. 2017). On both platforms, the implemented DWM performs like a plug-and-play operator, which makes it convenient to use. All the data used in single layer experiments are generated randomly by the standard normal distribution.

For convenience, the experiments on the whole network, including FLOPs analysis and accuracy test, are measured on PyTorch v1.0.0 or PyTorch v1.7.0, depending on the requirements of open-source on GitHub. The model architectures are also gained from the GitHub implementation, which will be mentioned later.

Numerical Accuracy of Single Layer
We estimate the MSE between several methods and the FP64 results by running a convolution operation.

Settings We assume two application situations: larger feature maps (28×28/28×28×28) with fewer channels (128) and smaller feature maps (14×14/14×14×14) with more channels (256). The feature maps and convolution weights are random numbers generated by standard normal distribution using NumPy (Harris et al. 2020). Both the convolution shapes and data distributions are consistent with the convolution layers in real networks.

Results Tables 1 and 2 show the numerical accuracy of different implementations and data formats for 2-D and 3-D convolutions. (1) Both DWM FP32 and DWM FP16 have better numerical accuracy than the Winograd FP32 and Winograd FP16 almost in all situations. It is evident that the Winograd algorithm is faced with a serious numerical accuracy problem as the kernel size increases. In 2-D case shown in Table 1, when the kernel size is 7×7 or larger, the error of the FP32 Winograd algorithm approaches FP16’s, which may cause accuracy problems. By contrast, DWM’s numerical error will not be amplified as kernel size increases, and DWM FP32 is close to the result of FP32, meaning that DWM FP32 can be applied to all kinds of convolution operations without any accuracy problem. (2) Winograd FP16 incurs an overflow, while DWM FP16 does not. In 3-D case shown in Table 2, the FP16 Winograd algorithm is all NaN. This may be caused by the intermediate results of the Winograd transformation. When kernel size becomes larger, the transformation matrices will be filled with large numbers, and these numbers may make the intermediate results of the transformation become too large to be represented in FP16. This shows the advantage of using DWM instead of the Winograd algorithm. A further study of how does this accuracy problem influences the network will be provided later in Analysis on Networks. (3) In some situations, DWM FP32 is more accurate than FP32 ones, especially in 3-D convolution. This is reasonable because DWM consumes fewer multiplications, which may make it has better numerical accuracy. The larger the convolution dimension is, the fewer multiplications DWM costs comparing with the original convolution algorithm, and that makes most convolutions of 3-D FP32 DWM has better numerical accuracy than the original FP32 ones.

FLOPs Estimation on Single Layer
We calculate the FLOPs of convolutions with different kernel sizes and two kinds of stride.

Settings We ignore the influence of 0,±2𝑛 and ±12𝑛 which can be easily implemented by shifting.

Results Tables 3 and 4 shows the FLOPs and the speedup of the Winograd algorithm and DWM, and we can get the following results: (1) DWM costs less computation than the traditional Winograd algorithm in all situations. These two algorithms cost the same FLOPs when the kernel size is 3×3. However, as the kernel size becomes larger, the FLOPs of traditional Winograd algorithm increases heavily, and most FLOPs concentrates on the Winograd transformation because the transformation process becomes a non-sparsity matrix multiplication. On the contrary, the speedup of DWM keeps steady because of its simple transformation matrices. (2) DWM speeds up stride 2 convolutions by 2×∼3×, while the traditional Winograd algorithm cannot deal with the stride 2 convolutions. Due to the decomposition method of DWM, the speedup of stride 2 convolution still holds stably. These advantages lead to stably speedup on almost all kinds of convolutions.

Table 3 The speedup of several accelerating algorithms on different kinds of 2-D convolutions
Full size table
Table 4 The speedup of several accelerating algorithms on different kinds of 3-D convolutions
Full size table
Wall Clock Time on Single Layer
We measure the wall clock time of convolution operations based on a naive implementation of DWM.

Settings The results are measured by nvprof,Footnote1 a profiling tool of NVIDIA. The time of cuDNN is measured using cuDNN v7.4.0 with the most recommended algorithm. The time of Fast Fourier Transform (FFT) is measured by setting the convolution algorithm of cuDNN to ”FFT Tiling”. Our naive implementation of 3D DWM does not use optimization techniques such as kernel fusion and soft pipeline.

Results According to Tables 5 and 6, we can draw five main conclusion: (1) DWM always outperforms the Winograd algorithm when the kernel size is larger than 3. The low efficiency of the Winograd algorithm is caused by the huge amount of computation during the transformation process. In almost every case, DWM is better than FFT. The high computation cost of FFT may come from its complex transformation compared to DWM. (3) The wall clock time of FFT fluctuates with the size of activation and the kernel. This is reasonable because according to the FFT algorithm, the activation and kernel needed to be padded to the same size, which wastes extra time. The closer the size of activation and the size of kernel, the less padding is needed. In contrast, DWM does not need redundant padding except for the case that the size of the output cannot be divided by 2. (4) DWM is even faster than cuDNN in some situations. We have measured around 2000 kinds of different scales of convolutions totally, and DWM performs the best on ∼25% of them. In both 2-D and 3-D cases, when the kernel size becomes large, cuDNN/DWM becomes high, which means DWM can suit large kernel cases better. When the kernel size is larger than 7×7 in 2-D and 5×7×7 in 3-D, DWM performs better than cuDNN. (5) Different scales of convolutions give different acceleration results, which may mismatch the theoretical analyses above. There two reasons. First, the implementation of DWM is not optimal, which cannot reach the algorithm’s full potential in all cases. Second, there are many different fast convolution algorithms integrated into cuDNN, especially in 2-D cases. These algorithms make cuDNN behave differently under different circumstances, which is related to several factors such as the kernel size, the feature map size, the number of channels, and filters.

Table 5 The wall clock time of several accelerating algorithms running on different kinds of 2-D convolutions tested by nvprof
Full size table
Table 6 The wall clock time of several accelerating algorithms running on different kinds of 3-D convolutions tested by nvprof
Full size table
The Bottleneck from 1-D to ND
To further illustrate the weakness of the Winograd algorithm on large ND convolutions, we conduct an experiment on the numerical accuracy of doing the ND convolutions.

Table 7 The mean squared error (MSE) of ND convolutions with kernel size 7 and 9
Full size table
Table 8 The FLOPs and theoretical speedup of several accelerating algorithms on different networks
Full size table
Settings The kernel sizes are fixed to be 7 or 9. The output size of convolutions is fixed to 2, e.g.2×2×2 in 3-D convolutions and 2×2×2×2 in 4-D convolutions. The batch size and the number of filters are set to 1, because they are not involved in the calculation producing a single output, meaning that they have nothing to do with MSE. Moreover, the number of channels is set to 1. Although increasing it will enlarge the MSE of imprecise algorithms, setting to 1 is enough to present the numerical accuracy problems of the ND Winograd algorithm. The random seed is set to 11.

Results As summarized in Table 7, (1) the Winograd algorithm always performs worse than DWM, especially when the dimension of convolutions is high. In the 6-D convolution case, the accuracy of DWM is 106 higher than the Winograd algorithm. (2) When the dimension N increases from 1 to 6, the numerical accuracy of the Winograd algorithm decreases, while the performance of DWM keeps stable. This phenomenon is in line with our expectations in Sect. 4, that the increase of dimension N will enlarge the drawback of the Winograd algorithm on doing convolutions with large sizes.

Analysis on Networks
Then, we analyse several representative networks. This analysis includes the comparison of total FLOPs and the networks’ accuracy performance.

Settings For the FLOPs analysis, four convolution algorithms are measured: (1) Conv:the original convolution algorithm. (2) Winograd: replace every stride 1 convolutions with the Winograd algorithm. (3) Conv + Winograd: only replace 3×3/3×3×3 and stride 1 convolutions with the Winograd algorithm to get the fastest acceleration. (4) DWM: replace every convolutions with DWM. For the networks’ accuracy performance, three convolution algorithms are tested: (1) Baseline: the original convolution. (2) Winograd: replace every stride 1 convolutions with the Winograd algorithm. (3) DWM: replace every convolutions with DWM.

Table 9 The results of TOFlow. Baseline indicates the given baseline on github while Baseline* is the baseline reproduced by ourselves
Full size table
FLOPs Table 8 shows the results of the theoretical FLOPs analysis.Footnote2 It can be observed that, ”Winograd” has a poor speedup on networks with large kernel sizes, while DWM performs stable.

Specifically, When there are some large kernels in the network, such as 7×7 in SPyNet and 4×4×4 in V2V, DWM performs much better than both ”Winograd” and ”Conv + Winograd”, while when the network consists of 3×3, ”Conv + Winograd” is efficient enough.

TOFlow Table 9 shows the peak signal to noise ratio (PSNR) and structural similarity (SSIM) of TOFlowFootnote3 using different data formats and acceleration algorithms. The dataset used is Vimeo-90K (Xue et al. 2018), including three tasks: interpolation, denoising, and super-resolution. It can be observed that the results are close, while DWM is slightly better than the Winograd algorithm. However, we will show the potential risk of the Winograd algorithm that close when it comes to SPyNet.

SPyNet Table 10 shows the end point errors (EPE) of SPyNetFootnote4 using different data formats and acceleration algorithms. The dataset used is the complete set of MPI-Sintel Final (Butler et al. 2012), including training and validation.

As we can see, Winograd FP16 get NaN while DWM keeps EPE similar to Baseline. The reason is as follows: SPyNet is mostly composed of large convolutions like 7×7 convolutions, whose Winograd transformation matrices (F(2, 7)) consist of small decimals and large numbers, which are complicated to compute. Worse, the data distribution of SPyNet is wide enough to cause an FP16 overflow during the transformation process. Thus, once an intermediate result overflows, the corresponding 2×2 convolution output of the Winograd algorithm will become NaN.

Table 10 The result of SPyNet
Full size table
Fig. 5
figure 5
The visualization of optical flow estimates of SPyNet on the MPI Sintel dataset, using different acceleration algorithms in FP16 data format

Full size image
Table 11 The accuracy and mean Average Precision (mAP) results of different methods on SlowFast-R50 (Standard)
Full size table
Table 12 The results of GDConvNet
Full size table
This is also illustrated by Fig. 5, where the black blocks indicate the regions of NaN. Although parts of the Winograd FP16 outputs keep similar to the baseline, some of them cause an overflow, while DWM FP16 produces outputs almost the same as the baseline. Note that there is nearly no difference between the results of Winograd FP32 and DWM FP32. The reason is that, compared to FP16, FP32 is much more robust to the representation of large and small numbers, and the precision problem of Winograd is not too bad to make these examples fail (precision details can be found in Table 1 and Table 2). However, the risk of using Winograd still exists. When we test some other images or use a low precision calculation like FP16 to speed up the calculation, Winograd may fail.

Table 13 The wall clock time of representative networks
Full size table
SlowFast Table 11 presents the performance of SlowFastFootnote5 on the dataset Something-Something V2 and Charades using different methods. Note that the results of“Winograd”are not shown because the only stride 1 convolution in SlowFast is 3×3 convolution, and 3×3 Winograd has already been integrated into cuDNN and PyTorch. Thus, there is totally no difference between “Baseline”and“Winograd”on SlowFast. It can be observed that DWM always keeps its accuracy similar to the original convolution. This indicates that DWM does not have any accuracy loss on SlowFast. Note that on Something-Something V2, we change the default test configuration from ”NUM_SPATIAL_CROPS=1” to ”NUM_SPATIAL_CROPS=3” .Footnote6

Table 14 The memory cost comparison between the original convolution and DWM
Full size table
GDConvNet Table 12 shows the PSNR and SSIM of GDConvNetFootnote7 on video frame interpolation task using different data formats and acceleration algorithms. The dataset is the test set of Vimeo-90K Septuplet. Similar to SPyNet, Winograd FP16 get NaN in all situations while DWM FP16 keeps the same result as BaselineFP16. However, there are two differences. First, all three methods get NaN on Polynomial interpolation, including Baseline. Second, the figures of NaN are totally broken, unlike Fig. 5, which only breaks in small parts. The reason is that GDConvNet is a very deep network with much calculation. Thus, once a NaN caused by overflow appears, it can be broadcast to other places as the calculation of layers goes on.

Other discussions There are some other typical tasks for video analysis that we haven’t test results on, such as video inpainting. This is because that the stride 1 convolutions in these networks are mainly 3×3 convolutions, on which the Winograd algorithm does not have accuracy problem, and the Winograd algorithm cannot be applied to large stride convolutions, which makes the accuracy comparison between Winograd and DWM meaningless. Overall, the results of single layer in Tables 1 and 2 have illustrated the potential risk of using Winograd on large kernel convolutions, which DWM does not have. This section aims to select several typical networks and give some further demonstrations.

Wall Clock Time on Networks
In this part, we show the wall clock times of several representative networks, compared between the industrial state-of-the-art cuDNN and DWM.

Settings The results are measured using GPU NVIDIA Tesla V100, PyTorch v1.0.0, CUDA v10.1, cuDNN v7.4.0. Warm up strategy for GPU is used for both cuDNN and DWM. The transformation and main computation of Winograd for large kernels is implemented using matrix multiplication. The main computation of DWM is implemented using Compute Unified Device Architecture (CUDA), a C-like programming language for NVIDIA GPUs, with PyTorch packaging outside.

Results The comparison between Winograd and DWM can show our practical improvement of the Winograd algorithm on large kernel size convolutions. The comparison between cuDNN and DWM can show DWM’s potential to exceed the best convolution library on NVIDIA GPUs. According to Table 13, we can conclude that: (1) DWM outperforms Winograd, especially on 3D ShapeNet. The main reason is that 3D ShapeNet consists of several large kernel 3D convolutions without much other operation like pooling or interpolation. Thus, it reflects a much pure convolution time than other networks. (2) DWM always performs better than cuDNN, but the speedup ratio is not large enough to match the theoretical analysis. This gap mainly comes from the difference of tools when implementing. For cuDNN, NVIDIA implements it using Parallel Thread eXecution (PTX), a low-level parallel thread execution virtual machine and instruction set architecture (ISA), which is assembly-like, and the implementation of convolution is highly tuned.Footnote8 For DWM, we implement it using Compute Unified Device Architecture (CUDA), which is C-like and is hard to control the implementation details compared with PTX. Thus, there is still a large optimization space for DWM on GPUs. The full optimization of DWM can be left as our future work like other researchers (Wei et al. 2020; Yan et al. 2020; Jia et al. 2020).

Memory Consuming
Finally, we further illustrate the memory consuming problem of DWM by an experiment.

Settings The results are measured using GPU NVIDIA Tesla V100, PyTorch v1.0.0. We implement DWM in a“non-fused”manner, which means that the whole process is separated into several different kernels.

Results As shown in Table 14, we can conclude that: (1) DWM always requires more memory than simple convolution. This is in line with our expectations in the theoretical analysis. (2) To some degree, the larger the kernel size, the higher the memory cost ratio of DWM/Conv. This also stays the same with our analysis that more split pieces will cause more significant memory usage. (3) When it comes to 7×7×7 convolution kernels, the ratio becomes lower. This may be caused by the decrease of the output size and the increase of the convolution memory cost. Overall, DWM sacrifices some memory efficiency to gain a convolution acceleration.

Conclusion
In this paper, we propose a novel DWM to extend Winograd’s minimal filtering algorithm to more general ND convolutions. Winograd’s minimal filtering algorithm has been widely used to reduce the number of multiplications for faster processing both in 2-D CNNs and 3-D CNNs. However, both the 2-D and 3-D Winograd algorithm has the drawbacks of suffering from significantly increased FLOPs and numerical accuracy problem for kernel size larger than 3 and failing on convolution with stride larger than 1, so it is only effective on convolutions with kernel size as 3 and stride as 1. Worse, the numerical accuracy problem will be intensified when the dimension grows from 2-D to ND. To solve these problems, we propose DWM to break through the limitation of the traditional Winograd algorithm on convolutions of large kernels, large strides, and larger dimensions. DWM decomposes kernels with large size or large stride to several small kernels with stride as 1 for further applying Winograd method, so that DWM can reduce the number of multiplications while keeping the numerical accuracy. Moreover, the simple coefficients in the transformation matrices guarantee the extension of DWM from 2-D to ND. Experimental results show that the proposed DWM is able to support all kinds of ND convolutions with a speedup of 1.44×∼3.38×, without any accuracy loss. These good properties of DWM enable the fast exploring of larger kernel size, larger stride value, and higher dimension in CNNs for high performance and high accuracy, and thus improves the potential for discovering new CNNs. However, we have also discovered that one of the drawbacks of DWM is the huge memory cost during decomposition and transformation, which may be left for future work in another different area of implementation optimization.