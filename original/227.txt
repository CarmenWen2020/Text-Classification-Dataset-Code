In this paper, we investigate stable patterns of electroencephalogram (EEG) over time for emotion recognition using a machine learning approach. Up to now, various findings of activated patterns associated with different emotions have been reported. However, their stability over time has not been fully investigated yet. In this paper, we focus on identifying EEG stability in emotion recognition. We systematically evaluate the performance of various popular feature extraction, feature selection, feature smoothing and pattern classification methods with the DEAP dataset and a newly developed dataset called SEED for this study. Discriminative Graph regularized Extreme Learning Machine with differential entropy features achieves the best average accuracies of 69.67 and 91.07 percent on the DEAP and SEED datasets, respectively. The experimental results indicate that stable patterns exhibit consistency across sessions; the lateral temporal areas activate more for positive emotions than negative emotions in beta and gamma bands; the neural patterns of neutral emotions have higher alpha responses at parietal and occipital sites; and for negative emotions, the neural patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The performance of our emotion recognition models shows that the neural patterns are relatively stable within and between sessions.
SECTION 1Introduction
Emotions play an important role in human communication and decision making. Although in our daily life emotions seem natural to us, we have little knowledge of the mechanisms behind the emotional function of the brain for modeling human emotion [1]. In recent years, research on emotion recognition based on EEG has attracted great interest from a vast number of interdisciplinary fields, from psychology to engineering, including basic studies on emotion theories and applications to affective Brain-Computer Interactions (aBCIs) [2], [3], which enhance BCI systems with the ability to detect, process, and respond to users’ affective states using physiological signals.

Although much progress has been made in the theories, methods and experiments that support affective computing over the past several years [4], the problem of detecting and modeling human emotions in aBCIs remains largely unexplored [3]. Emotion recognition is the primary and important phase for aBCIs. However, emotion recognition based on EEG is very challenging due to the fuzzy boundaries and differences in individual variations of emotion. In addition, we cannot obtain the ‘ground truth’ behind human emotions in theory, that is, the true label for an EEG corresponding to different emotional states, because emotion is considered as a function of time, context, space, language, culture, and races [5].

Many previous studies have focused on participant-dependent and participant-independent patterns and evaluations of emotion recognition. However, the stability of patterns and performance of models over time has not been fully exploited, and they are very important for real-world applications. Stable EEG patterns are considered as neural activities related to critical brain areas and critical frequency bands that share commonality across individuals and sessions under different emotional states. Although task-related EEG is sensitive to change due to differences in cognitive states and environmental variables [6], we intuitively consider that the stable patterns for specific tasks should exhibit consistency among repeated sessions of the same participants. In this paper, we focus on the following issues of EEG-based emotion recognition: What is the capability of EEG signals for discriminating between different emotions? Are there any stable EEG patterns of neural oscillations or brain regions for representing emotions? What is the day-to-day performance of the models based on machine learning approaches?

The main contributions of this paper to emotion recognition from EEG can be summarized as follows: 1) We have developed a novel emotion EEG dataset as a subset of SJTU Emotion EEG Dataset (SEED), which is publicly available for evaluating stable patterns across participants and sessions. To the best of our knowledge, there is no available public EEG dataset for the analysis of the stability of neural patterns regarding emotion recognition. 2) We carry out a systematic comparison and a qualitative evaluation of different feature extraction, feature selection, feature smoothing and pattern classification methods on a publicly available EEG dataset, DEAP, and our own dataset, SEED. 3) We adopt a discriminative Graph regularized Extreme Learning Machine (GELM) to identify stable patterns over time and evaluate the stability of our emotion recognition model with cross-session schemes. 4) Our experimental results reveal that neural signatures for three emotions (positive, neutral and negative) do exist and that EEG patterns at critical frequency bands and brain regions are relatively stable within and between sessions.

The layout of the paper is as follows. In Section 2, we provide a brief overview of related work on emotion recognition based on EEG as well as the findings on stable patterns for different emotions. A systematic description of brain signal analysis methods and classification procedures for feature extraction, dimensionality reduction and classifiers is given in Section 3. Section 4 presents the motivation for and rationale behind our experimental setting. A detailed explanation of all the materials and protocols that we have used is also described. A systematic evaluation of different methods is conducted using the DEAP dataset and our SEED dataset. We use time-frequency analysis to find the neural signatures and stable patterns of different emotions, and we evaluate the stability of our emotion recognition models over time. In Section 5, we present conclusions about our work.

SECTION 2Related Work
In the field of affective computing, a vast number of studies has been conducted on emotion recognition based on different signals. A detailed review of emotion recognition methods can be found in [19]. With the fast development of micro-nano technologies and embedded systems, it is now conceivable to port aBCI systems from the laboratory to real-world environments. Many advanced dry electrodes and embedded systems are developed to handle the wearability, portability, and practical use of these systems in real-world applications [20], [21]. Various studies conducted by the affective computing community attempt to build computational models to estimate emotional states based on EEG features. Kim et al. presented a review on the computational methods for EEG-based emotion estimation [22]. In short, a brief summary of emotion recognition using EEG is presented in Table 1. These studies show the efficiency and feasibility of building computational models of emotion recognition using EEG. In these studies, the stimuli used in emotion recognition experiments include still images, music and videos, and the emotions evaluated in most of the studies are discrete.

TABLE 1 Various Studies on Emotion Classification Using EEG and the Best Performance Reported in Each Study

One of the goals of affective neuroscience is to examine whether patterns of brain activity for specific emotions exist and whether these patterns are to some extent common across individuals. Various studies have examined the neural correlations of emotions. It seems that processing modules for specific emotion do not exist. However, neural signatures of specific emotions, as a distributed pattern of brain activity [23], may exist. Mauss and Robinson [24] proposed that the emotional state is likely to involve circuits rather than any brain region considered in isolation. To AC researchers, identifying neural patterns that are both common across participants and stable across sessions can provide valuable information for emotion recognition based on EEG.

Cortical activity in response to emotional cues is related to the lateralization effect. Schmidt and Trainor [25] found that the pattern of asymmetrical frontal EEG activity distinguished valence, while for intensity, the overall frontal EEG activity did. Muller et al. [26] reported increased gamma (30-50 Hz) power for a negative valence over the left temporal region. Davidson et al. [27], [28] showed that frontal EEG asymmetry is hypothesized to be related to approach and withdrawal emotions, with heightened approach tendencies reflected in the left frontal activity and heightened withdrawal tendencies reflected in the right frontal activity. Nie et al. [29] reported that the participant-independent features associated with positive and negative emotions are mainly in the right occipital lobe and parietal lobe for the alpha band, the central site for the beta band, and the left frontal lobe and right temporal lobe for the gamma band. Balconi et al. [30] found that frequency band modulations are affected by valence and arousal rating, with an increased response for high arousal and negative or positive stimuli compared to that for low arousal and neutral stimuli.

For EEG-based emotion recognition, participant-dependent and participant-independent schemes are always used for evaluating the performance of emotion recognition systems. As shown in Table 1, some findings related to activated patterns such as critical channels and oscillations associated with different emotions have been proposed. However, a major limitation is that they extract activated patterns only across participants but do not consider the time factor.

Studies on the internal consistency and test-retest stability of EEG date back to many years ago [6], [31], [32], especially for clinical applications [33]. McEvoy et al. [6] proposed that under appropriate conditions, task-related EEG has sufficient retest reliability for use in assessing clinical changes. However, these previous studies investigated the stability of EEG features under different conditions, for example, a working memory task [6]. Moreover, in these studies, stability and reliability are often quantified using statistical parameters such as intraclass correlation coefficients [32], instead of the performance of pattern classifiers.

So far, a few preliminary studies on the stability and reliability of neural patterns for emotion recognition have been conducted. Lan et al. [34] presented a pilot study on the stability of features in emotion recognition algorithms. However, in their stability assessment, the same features derived from the same channel from the same emotion class of the same participant were grouped together to compute the correlation coefficients. Furthermore, their experiments were conducted on a small group of participants with 14-channel EEG signals. They investigated the stability of each feature instead of the neural patterns that we focus on in this paper. Till now, no systematic evaluation has been conducted on the stability of activated patterns over time. The performance of emotion recognition systems over time is still an unsolved problem in developing real-world application systems. Therefore, our major aim in this paper is to investigate stable EEG patterns over time using time frequency analysis and machine learning approaches. We should emphasize that we do not study neural patterns under emotion regulation [35], but rather to study specific emotional states during different times.

To investigate various critical problems related to emotion recognition based on EEG, we face a serious lack of publicly available emotional EEG datasets. To the best of our knowledge, the only publicly available emotional EEG datasets are MAHNOB HCI [15] and DEAP [13]. The first one includes EEG, physiological signals, eye gazes, audio, and facial expressions of 30 people while watching 20 emotional videos. The DEAP dataset includes the EEG and peripheral physiological signals of 32 participants when watching 40 one-minute music videos. It also contains the participants’ rate for each video in terms of the levels of arousal, valence, like/dislike, dominance, and familiarity. However, these datasets do not contain EEG data from different sessions for the same participant, which cannot be used for investigating the stable patterns over time. Because there are no available published EEG datasets for the analysis of the stability of neural patterns for emotion recognition, we develop a new emotional EEG dataset for this study as a subset of SEED.1

SECTION 3Emotion Experiment Design
In order to investigate the neural signatures of different emotions and stable patterns over time, we design new emotion experiments to collect EEG data, which are different from those other existing publicly available datasets. In our experiments, the same participant performs the emotion experiments three times, at an interval of one week or longer. We chose film clips as emotion elicitation materials. These emotional films contain both scene and audio, which can expose participants to more real-life scenarios and elicit strong subjective and physiological changes.

In our emotion experiments, Chinese film clips are used, as we consider that native culture factors may affect elicitation in emotion experiments [36], [37]. In the preliminary study, we manually selected a pool of emotional film clips from famous Chinese films. Twenty participants were asked to assess their emotions when watching the selected film clips using scores (1-5) and keywords (positive, neutral and negative). The criteria for selecting the film clips were as follows: (a) the length of the whole experiment should not be too long in case it will give the participants visual fatigue, (b) the videos should be understood without explanation, and (c) the videos should elicit a single desired target emotion. Finally, 15 Chinese film clips for positive, neutral and negative emotions were chosen from the pool of materials, which received a score of 3 or higher on the mean ratings from the twenty participants. Each emotion has five film clips in one experiment. The duration of each film clip is about 4 minutes. Each film clip is edited well to create coherent emotion eliciting. The details of the film clips used in the experiments are listed in Table 2.

TABLE 2 Details of the Film Clips Used in Our Emotion Experiment

Fifteen participants (seven males and eight females; age range: 19-28 years old, mean: 23.27, std: 2.37), different from those in film clips selection, participated in the experiments. In order to investigate the neural signatures and stable patterns across sessions and individuals, each participant was required to perform the experiments for three sessions. The time interval between two sessions was one week or longer. All the participants are native Chinese students from Shanghai Jiao Tong University with self-reported normal or corrected-to-normal vision and normal hearing. Before the experiments, the participants were informed about the experiment and instructed to sit comfortably, watch the forthcoming movie clips attentively without diverting their attention from the screen, and refrain as much as possible from overt movements.

Facial videos and EEG data were recorded simultaneously. EEG was recorded using an ESI NeuroScan System2 at a sampling rate of 1000 Hz from a 62-channel active AgCl electrode cap according to the international 10-20 system. The layout of EEG electrodes on the cap is shown in Fig. 1. The impedance of each electrode had to be less than 5 kΩ. The frontal face videos were recorded from the camera mounted in front of the participants. Facial videos were encoded in AVI format with a frame rate of 30 frames per second and a resolution of 160×120.


Fig. 1.
The EEG cap layout for 62 channels.

Show All

In total, there were 15 trials for each experiment. There was a 15 s hint of start before each clip and 10 s of feedback after each clip. For the feedback, participants were told to report their emotional reactions to each film clip by completing the questionnaire immediately after watching each clip. The questions are the following [38]: (1) what they had actually felt in response to viewing the film clip, (2) how they felt at the specific time they were watching the film clips, (3) whether they had watched the movie before, and (4) whether they had understood the film clips. They also rated the intensity of subjective emotional arousal using a five-point scale according to what they actually felt during the task [39]. Fig. 2 presents the detailed protocol. For EEG signal processing, the raw EEG data were first down-sampled to a 200 Hz sampling rate. In order to filter the noise and remove the artifacts, the EEG data were then processed with a bandpass filter between 0.5 to 70 Hz. The EEG data were visually checked and the recordings seriously contaminated by EMG and EOG were removed manually from the dataset. EOG was simultaneously recorded in the experiments to help identify blink artifacts from the recordings.


Fig. 2.
The protocol used in our emotion experiment.

Show All

SECTION 4Methodology
4.1 Feature Extraction
From our previous work[40], [41], [42], we have found that the following six different features and electrode combinations are efficient for EEG-based emotion recognition: power spectral density (PSD), differential entropy (DE), differential asymmetry (DASM), rational asymmetry (RASM), asymmetry (ASM) and differential caudality (DCAU) features from the EEG. As a result, we use these six different features in this study. According to the five frequency bands, delta (1-3Hz), theta (4-7 Hz), alpha (8-13 Hz), beta (14-30 Hz) and gamma (31-50 Hz), we computed the traditional PSD features using Short Time Fourier Transform (STFT) with a 1-s-long window and no overlapping Hanning window. The differential entropy feature for Gaussian distribution is defined as follows [40],
h(X)=−∫∞−∞12πσ2−−−−√exp(x−μ)22σ2log12πσ2−−−−√exp(x−μ)22σ2dx=12log2πeσ2,(1)
View Sourcewhere X denotes the Gaussian distribution N(μ,σ2), x is a variable, and π and e are constants. According to [40], in a certain band, DE is equivalent to the logarithmic spectral energy for a fixed-length EEG sequence.

Because there is evidence that the lateralization between the left and right hemisphere is associated with emotions [28], we investigate asymmetry features. We computed the differential asymmetry and rational asymmetry features as the differences and ratios between the DE features of 27 pairs of hemispheric asymmetry electrodes (Fp1-Fp2, F7-F8, F3-F4, FT7-FT8, FC3-FC4, T7-T8, P7-P8, C3-C4, TP7-TP8, CP3-CP4, P3-P4, O1-O2, AF3-AF4, F5-F6, F7-F8, FC5-FC6, FC1-FC2, C5-C6, C1-C2, CP5-CP6, CP1-CP2, P5-P6, P1-P2, PO7-PO8, PO5-PO6, PO3-PO4, and CB1-CB2). DASM and RASM can be expressed, respectively, as
DASM=DE(Xleft)−DE(Xright),(2)
View Sourceand
RASM=DE(Xleft)/DE(Xright).(3)
View SourceRight-click on figure for MathML and additional features.ASM features are the direct concatenation of DASM and RASM features for comparison. In the literature, the patterns of spectral differences along the frontal and posterior brain regions have also been explored [43]. To characterize the spectral band asymmetry with respect to caudality (in the frontal-posterior direction), we define DCAU features as the differences between DE features of 23 pairs of frontal-posterior electrodes (FT7-TP7, FC5-CP5, FC3-CP3, FC1-CP1, FCZ-CPZ, FC2-CP2, FC4-CP4, FC6-CP6, FT8-TP8, F7-P7, F5-P5, F3-P3, F1-P1, FZ-PZ, F2-P2, F4-P4, F6-P6, F8-P8, FP1-O1, FP2-O2, FPZ-OZ, AF3-CB1, and AF4-CB2). DCAU is defined as
DCAU=DE(Xfrontal)−DE(Xposterior).(4)
View SourceRight-click on figure for MathML and additional features.The dimensions of PSD, DE, DASM, RASM, ASM and DCAU are 310 (62 electrodes × 5 bands), 310 (62 electrodes × 5 bands), 135 (27 electrode pairs × 5 bands), 135 (27 electrode pairs × 5 bands), 270 (54 electrode pairs × 5 bands), and 115 (23 electrode pairs × 5 bands), respectively.

4.2 Feature Smoothing
Most of the existing approaches for emotion recognition from EEG may be suboptimal because they map EEG signals to static discrete emotional states and do not take the temporal dynamics of the emotional state into account. However, in general, emotion should not be considered as a discrete psychophysiological state [44]. Here, we assume that the emotional state is defined in a continuous space and that emotional states change gradually. Our approach focuses on tracking the change of the emotional state over time from the EEG. In our approach, we introduce the dynamic characteristics of emotional changes into emotion recognition and investigate how the observed EEG is generated from a hidden emotional state. We apply the linear dynamic system (LDS) approach to filter out components that are not associated with emotional states [45], [46]. For comparison, we also evaluate the performance of the conventional moving average method.

To make use of the time dependency of emotion changes and further reduce the influence of emotion-unrelated EEG, we introduce the LDS approach to smooth features. A linear dynamic system can be expressed as follows,
xt=zt+wt,(5)
View Sourceand
zt=Azt−1+vt,(6)
View Sourcewhere xt denotes the observed variables, zt denotes the hidden emotion variables, A is a transition matrix, wt is Gaussian noise with mean w¯ and variance Q, and vt is Gaussian noise with mean v¯ and variance R. These equations can also be expressed in equivalent form in terms of Gaussian conditional distributions,
p(xt|zt)=N(xt|zt+w¯,Q),(7)
View SourceRight-click on figure for MathML and additional features.and
p(zt|zt−1)=N(zt|Azt−1+v¯,R).(8)
View SourceRight-click on figure for MathML and additional features.The initial state is assumed to be
p(z1)=N(z1|π0,S0).(9)
View SourceThe above model is parameterized by θ={A,Q,R,w¯,v¯,π0,S0}. θ can be determined using maximum likelihood through the EM algorithm [47] based on the observation sequence xt. To infer the latent states zt from the observation sequence xt, the marginal distribution, p(zt|X), must be calculated. The latent state can be expressed as
zt=E(zt|X),(10)
View Sourcewhere E denotes the expectation. This marginal distribution can be achieved by using the message propagation method [47]. We use cross-validation to estimate the prior parameters.

4.3 Dimensionality Reduction
We computed the initial EEG features using signal analysis. However, the features extracted may be uncorrelated with emotion states and lead to the performance degradation of classifiers. Additionally, the high dimensionality of the features may result in the classifiers suffering from the ‘curse of dimensionality’ [48]. In addition, for real-world applications, dimensionality reduction could help to increase the speed and stability of the classifier. Hence, in this study, we compare two popular approaches: principal component analysis (PCA) and minimal redundancy maximal relevance (MRMR) algorithm [49].

Although PCA can reduce the feature dimensions, it cannot preserve the original domain information such as channel and frequency after the transformation. Hence, we choose the MRMR algorithm to select a feature subset from an initial feature set. The MRMR algorithm uses mutual information as the relevance measure with the max-dependency criterion and minimal redundancy criterion. Max-Relevance searches for features satisfying (11) with the mean value of all the mutual information values between the individual feature xd and class c as follows,
maxD(S,c),D=1|S|∑xd∈SI(xd;c),(11)
View Sourcewhere S represents the feature subset to select. When two features highly depend on each other, the respective class-discriminative power would not change much if one of them is removed. Therefore, the following minimal redundancy condition can be added to select for mutually exclusive features,
minR(S),R=1|S|2∑xdi,xdj∈SI(xdi,xdj).(12)
View SourceThe criterion, combined with the above two constraints, is the minimal-redundancy-maximal-relevance, which can be expressed as
maxφ(D,R),(13)
View Sourcewhere φ=D−R. In practice, an incremental search method is used to find the near-optimal K features.

4.4 Classification
The extracted features are further fed to three conventional pattern classifiers, i.e., k nearest neighbors (KNN), logistic regression (LR), and support vector machine (SVM), and a newly developed pattern classifier, discriminative Graph regularized Extreme Learning Machine (GELM) [50], to build emotion recognition systems. For the KNN classifier, the euclidean distance is selected as the distance metric, and the number of nearest neighbors is set to 5 using cross-validation. For LR, the parameters are computed using maximal likelihood estimation. We use the LIBLINEAR software [51] to build the SVM classifier with a linear kernel. The soft margin parameter is selected using cross-validation.

Extreme Learning Machine (ELM) is a single hidden layer feed-forward neural network (SLFN) [52], and learning with local consistency of the data has drawn much attention to improve the performance of the existing machine learning models in recent years. Peng et al. [50] proposed a discriminative Graph regularized Extreme Learning Machine based on the idea that similar samples should share similar properties. GELM yields a much better performance in comparison with other models for face recognition [50] and emotion classification [41].

Given a training data set,
L={(xi,ti)|xi∈Rd,ti∈Rm},(14)
View Sourcewhere xi=(xi1,xi2,…,xid)T and ti=(ti1,ti2,…,xim)T. In GELM, the adjacent W is defined as follows,
xi={1/Nt,0,if hi and hj belong to the tth classotherwise,(15)
View Sourcewhere hi=(g1(xi),…,gK(xi))T and hj=(g1(xj),…,gK(xj))T are hidden layer outputs for two input samples xi and xj. We can then compute the graph Laplacian L=D−W, where D is a diagonal matrix and each of the entries in D contains the column sums of W. Therefore, GELM can incorporate two regularization terms into the conventional ELM model. The objective function of GELM is defined as follows,
minβ||Hβ−T||22+λ1Tr(HβLβTHT)+λ2||β||22,(16)
View SourceRight-click on figure for MathML and additional features.where Tr(HβLβTHT) is the graph regularization term, ||β||2 is the l2-norm regularization term, and λ1 and λ2 are regularization parameters to balance the two terms.

By setting the derivative of the objective function (16) with respect to β as zero, we have
β=(HHT+λ1HLHT+λ2I)−1HT.(17)
View SourceRight-click on figure for MathML and additional features.

In GELM, the constraint imposed on the output weights enforces the outputs of samples from the same class to be similar. The constraint can be formulated as a regularization term in the objective function of a basic ELM, which also enables direct calculation of the output weight matrix.

SECTION 5Experiment Results
5.1 Experiment Results on DEAP Data
In this section, to validate the efficiency of the machine learning algorithms used in this study, we first evaluate these algorithms with the publicly available emotion dataset, the DEAP dataset3 [13], and we compare the performance of our models with those of other methods used in the existing studies on the same emotion EEG dataset.

The DEAP dataset consists of EEG and peripheral physiological signals of 32 participants who watched 40 excerpts of one-minute duration music videos. The EEG signals were recorded from 32 active electrodes (channels) according to the international 10-20 system, whereas peripheral physiological signals (8 channels) include the galvanic skin response, skin temperature, blood volume pressure, respiration rate, electromyogram and electrooculogram (horizontal and vertical). More details on the DEAP dataset are given in [13].

In this experiment, we used an emotion representation model based on the valence-arousal model. Each dimension has values ranging from 1 to 9. We further segmented the four quadrants of the valence-arousal (VA) space according to the ratings. LALV, HALV, LAHV, and HAHV denote low arousal/low valence, high arousal/low valence, low arousal/high valence, and high arousal/high valence, respectively. Considering the fuzzy boundary of emotions and the variations of participants’ ratings possibly associated with individual differences on the rating scale, we added a gap to segment the quadrants of VA space to ensure the correct ratings of participants’ true self-elicitation emotion and discard the EEG data with ratings of arousal and valence between 4.8 and 5.2. The numbers of instances for LALV, HALV, LAHV, and HAHV are 12,474, 16,128, 10,962 and 21,420, respectively. The rating distribution of DEAP on the arousal-valence plane (VA plane) for the four conditions is shown in Fig. 3. We can see that the ratings are distributed approximately uniformly [13]. We label the EEG data according to the participants’ ratings of valence and arousal.

Fig. 3. - 
The rating distribution of DEAP on the arousal-valence plane (VA plane) for the four conditions (LALV, HALV, LAHV, and HAHV).
Fig. 3.
The rating distribution of DEAP on the arousal-valence plane (VA plane) for the four conditions (LALV, HALV, LAHV, and HAHV).

Show All

We first extracted the PSD, DE, DASM, RASM, ASM and DCAU features of the 32-channel EEG data. The original EEG data from the DEAP dataset were preprocessed with down-sampling to 128 Hz and a bandpass frequency filter from 4.0-45.0 Hz, and EOG artifacts were removed. Therefore, we extracted the features in the four frequency bands: theta: 4-7 Hz, alpha: 8-13 Hz, beta: 14-30 Hz, and gamma: 31-45 Hz. The features were further smoothed using the linear dynamic system approach. We then selected SVM and GELM as the classifiers. In this study, we used the SVM classifier with a linear kernel, and the number of hidden layer neurons for GELM was fixed as 10 times the dimensions of the input. To use the entire data set for training and testing the classifiers, a five-fold cross-validation scheme was adopted. All the experiments were performed with five-fold cross-validation, and the classification performance was evaluated using the classification accuracy rate.

Table 3 shows the mean accuracy rates of SVM and GELM classifiers for different features obtained from various frequency bands (theta, alpha, beta and gamma) and the total frequency bands. It should be noted that ‘Total’ in Table 3 represents the direct concatenation of all features from four frequency bands. Because the EEG data of DEAP are preprocessed with a bandpass frequency filter from 4.0-45.0 Hz, the results of the delta frequency bands are not included. The average accuracies (%) are 61.46, 69.67, 52.54, 52.70, 51.82 and 55.26 for the PSD, DE, DASM, RASM, ASM and DCAU features from the total frequency bands, respectively. The best accuracy of the GELM classifier is 69.67 percent using the DE features of total frequency bands, and the best accuracy of the SVM classifier is 54.34 percent. We also evaluate the performance of KNN, logistic regression and SVM with the RBF kernel on DEAP, which achieve the respective accuracies (%) and standard deviations (%) of 35.50/14.50, 40.86/16.87, and 39.21/15.87, respectively, using the DE features of the total frequency bands. We perform one-way analysis of variance (ANOVA) to study the statistical significance. The DE features outperform the PSD features significantly (p<0.01), and for classifiers, the performance of GELM is better than that of SVM (p<0.01). As we can see from Table 3, the diversity of classification accuracy for different frequency bands is not significant for the DEAP dataset (p>0.95). The results here do not show specific frequency bands for the quadrants of the VA space. We can also see that the DCAU features achieve comparable accuracies. These results indicate that there exists some kind of asymmetry that provides discriminative information for the four affect elicitation conditions (LALV, HALV, LAHV, and HAHV), as discussed in Section 2.2.

TABLE 3 The Mean Accuracy Rates (%) of SVM and GELM Classifiers for Different Features Obtained from Separate and Total Frequency Bands

A comparison of the recognition accuracy of various systems using EEG signals in the DEAP dataset is presented in Table 4. The single modality signal (EEG) is used without a combined modality fusion manner. Chung et al. [53] defined a weighted-log-posterior function for the Bayes classifier and evaluated the method with the DEAP dataset. The accuracies for valence and arousal classification are 66.6 and 66.4 percent for two classes and 53.4 and 51.0 percent for three classes, respectively. Koelstra et al. [13] developed the DEAP dataset and obtained an average accuracy of 62.0 and 57.6 percent for the valence and arousal (2 classes), respectively. Liu et al. [54] proposed a real-time fractal dimension (FD)-based valence level recognition algorithm from EEG signals and obtained a mean accuracy of 63.04 percent for arousal-dominance recognition (4 classes) with the selected 10 participants. Zhang et al. [55] described an ontological model for representation and integration of the EEG data, and their model yielded an average recognition ratio of 75.19 percent for valence and 81.74 percent for arousal for the eight participants. Although their accuracies were relatively high, there are only two categories for each dimension, and these results were achieved using a subset of the original dataset. In contrast, from Table 3, we can see that our method GELM achieves an average accuracy of 69.67 percent on the same data set for quadrants of the VA space (LALV, HALV, LAHV, and HAHV) with DE features of total frequency bands for all 32 participants.

TABLE 4 Comparison of the Various Studies Using EEG in the DEAP Dataset

5.2 Experiment Results on SEED data
In this section, we present the results of our approaches on the SEED dataset. A very important difference between SEED and DEAP is that SEED contains three sessions at the time interval of one week or longer for the same participant.

5.2.1 Performance of Emotion Recognition Models
We first compare six different features, namely PSD, DE, DASM, RASM, ASM and DCAU, from the total frequency bands. We use GELM as the classifier, and the number of hidden layer neurons is fixed as 10 times the dimensions of the input. We adopt a five-fold cross-validation scheme. From Table 5, we can see that DE features have a higher accuracy and lower standard deviation than the traditional PSD features, implying that DE features are more suitable for EEG-based emotion recognition than the five other different features. For the asymmetry features, although they have a fewer number of dimensions than the PSD features, they can achieve significantly better performance than PSD, which means that brain processing related to positive, neutral and negative emotions has asymmetrical characteristics.

TABLE 5 The Means and Standard Deviations of Accuracies (%) for the PSD, DE, DASM, RASM, ASM and DCAU Features from the Total Frequency Bands

We also evaluate the performance of two different feature smoothing algorithms. Here, we compare the linear dynamic system approach with the conventional moving average algorithm. The size of moving windows is five in this study. The means and standard deviations of the accuracies in percentages (%) for without smoothing, moving average, and the LDS approach are 70.82/9.17, 76.07/8.86 and 91.07/7.54, respectively. We can see that the LDS approach significantly outperforms the moving average method (p<0.01), which achieves 14.41 percent higher accuracy. The results also demonstrate that feature smoothing plays a significant role in EEG-based emotion recognition.

We compare the performance of four different classifiers, KNN, Logistic Regression, SVM and GELM. In this evaluation, DE features of 310 dimensions were used as the inputs of classifiers. The parameter K of KNN was fixed to be the constant value of five. For LR and linear SVM, grid search with cross-validation was used to tune the parameters. The mean accuracies and standard deviations in percentage (%) of KNN, LR, SVM with RBF kernel, SVM with linear kernel and GELM are 70.43/12.73, 84.08/8.77, 78.21/9.72, 83.26/9.08 and 91.07/7.54, respectively. From the above results, we can see that GELM outperforms other classifiers with higher accuracies and lower standard deviations, which imply that GELM is more suited for EEG-based emotion recognition.

5.2.2 Neural Signatures and Stable Patterns
Fig. 4 presents the average accuracies of the GELM classifier for the six different features extracted from five frequency bands (delta, theta, alpha, beta and gamma) and the direct concatenation of these five frequency bands. The results in Fig. 4 indicate that the features obtained from the gamma and beta frequency bands perform better than those from other frequency bands, which imply that beta and gamma oscillations of brain activity are more related to the processing of these three emotional states than other frequency oscillations, as described in [56], [57], [58].


Fig. 4.
The average accuracies of GELM using different features obtained from five frequency bands and using a fusion method.

Show All

Fig. 5 shows the spectrogram of the electrode position T7 in an experiment, and Fig. 6 shows the average spectrogram over participants for each session at some electrodes (FPZ, FT7, F7, FT8, T7, C3, CZ, C4, T8, P7, PZ, P8 and OZ). As we can see from Figs. 5 and 6, the spectrograms have different patterns for different elicited emotions. The dynamics of higher-frequency oscillations are more related to positive/negative emotions, especially for the temporal lobes. Moreover, the neural patterns over time are relatively stable for each session. To obtain the neural patterns associated with emotion processing, we project the DE features to the scalp to determine the temporal dynamics of frequency oscillations and stable patterns across participants.


Fig. 5.
The spectrogram of the electrode position T7 in one experiment. As different emotions elicited, we can see that the spectrogram has different patterns.

Show All

Fig. 6. - 
The average spectrogram of the participants for each session at some electrodes, which shows the stable neural patterns over time in the temporal lobes and high-frequency bands (a red color indicates a high amplitude).
Fig. 6.
The average spectrogram of the participants for each session at some electrodes, which shows the stable neural patterns over time in the temporal lobes and high-frequency bands (a red color indicates a high amplitude).

Show All

Fig. 7 depicts the average neural patterns for positive, neutral and negative emotions. The results demonstrate that neural signatures associated with positive, neutral and negative emotions do exist. The lateral temporal areas activate more for positive emotions than negative emotions in beta and gamma bands, and the energy of the prefrontal area is increased for negative emotions over positive emotions in beta and gamma bands. While the neural patterns of neutral emotions are similar to those of negative emotions, which both show less activation in the temporal areas, the neural patterns of neutral emotions have higher alpha responses at parietal and occipital sites. For negative emotions, the neural patterns have significant higher delta responses at parietal and occipital sites and significantly higher gamma responses at prefrontal sites. The existing studies [59], [60] have shown that EEG alpha activity reflects attentional processing and that beta activity reflects emotional and cognitive processes. When participants watched neutral stimuli, they tended to be more relaxed and less attentional, which evoked alpha responses. For positive emotion processing, the energy of the beta and gamma responses was increased. The findings of these neural patterns are consistent with previous emotion studies [14], [17], [42], [59], [61].


Fig. 7.
The average neural patterns for all participants and sessions for different emotions, which shows that neural signatures associated with positive, neutral and negative emotions do exist. The lateral temporal areas are activated more for positive emotions than negative emotions in the beta and gamma bands. While the neural patterns of neutral emotions are similar to those of negative emotions, which both show less activation in temporal areas, the neural patterns of neutral emotion have higher alpha responses at parietal and occipital sites. The negative emotion patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites.

Show All

5.2.3 Dimensionality Reduction
As discussed above, the brain activities of emotion processing have critical frequency bands and brain areas, which imply that there must be a low-dimension manifold structure for emotion-related EEG signals. Therefore, we investigate how the dimension of features will affect the performance of emotion recognition. Here, we compare two dimensionality reduction algorithms, the principle component analysis algorithm and the minimal redundancy maximal relevance algorithm, with DE features of 310 dimensions as inputs and GELM as a classifier.

We find that dimensionality reduction does not affect the performance of our model greatly. For the PCA algorithm, when the dimension is reduced to 210, the accuracy drops from 91.07 to 88.46 percent and then reaches a local maximum value of 89.57 percent at the dimension of 160. For the MRMR algorithm, the accuracies vary slightly with lower dimension features. Comparing PCA and MRMR, it is better to apply the MRMR algorithm for EEG-based emotion recognition. Because the MRMR algorithm yields the best emotion-relevant and minimal redundancy features, it also preserves original domain information such as channel and frequency bands, which have the most discriminative information for emotion recognition after transformation. This discovery helps us reduce the computations required for the features and the complexity of the computational models.

Fig. 8 presents the distribution of the 20 top participant-independent features selected using the correlation coefficient. These 20 top features were selected from the alpha frequency bands at the electrode location FT8, the beta frequency bands at electrode locations AF4, F6, F8, FT7, FC5, FC6, FT8, T7, and TP7 and the gamma frequency band at the electrode locations FP2, AF4, F4, F6, F8, FT7, FC5, FC6, T7, and C5. These selected features are mostly from the beta and gamma frequency bands and at the lateral temporal and frontal brain areas, which is consistent with the above findings for the time frequency analysis.


Fig. 8.
Distribution of the 20 top participant-independent features selected using the correlation coefficients.

Show All

5.2.4 Stability of the Emotion Recognition Model over Time
It should be noted that SEED consists of 15 participants, and each participant performed the experiments three times. The interval between two sessions is one week or longer. By using SEED, we evaluated whether the performance of our emotion recognition model is stable with time. We split the data from different sessions for one participant into training data and testing data and trained the model using GELM. The features employed here are the DE features extracted from the total frequency bands after LDS smoothing.

The results are presented in Tables 6 and 7. From the mean values of the accuracy and standard deviation, we find that the accuracies obtained with the training set and test set from the same sessions are much higher than those obtained from different sessions. The performance of the emotion recognition model is better with training data and test data obtained from sessions performed for a short time. In Table 6, a comparative mean classification accuracy of 79.28 percent is achieved using our emotion recognition model with training and test datasets from different sessions. This result implies that the relation between the variation of the emotional states and the EEG signal is stable for one person over a period of time. With the passage of time, the performance of the model may become worse. Therefore, the adaption of the computational model should be further studied in the future.

TABLE 6 The Average Accuracies (%) of Our Emotion Model Across Sessions
Table 6- 
The Average Accuracies (%) of Our Emotion Model Across Sessions
TABLE 7 The Classification Accuracies (%) of the Training and Test Data from Different Sessions Using GELM

We now consider the situation of cross-participants and examine the participant-independent emotion recognition model. We employ a leave-one-out cross-validation to investigate the classification performance in a participant-independent approach and use linear SVM classifier with DE features from five frequency bands as inputs. The average accuracy and standard deviation with participant-independent features are 60.93 and 13.95 percent, respectively. These results indicate that the participant-independent features are relatively stable and that it is possible to build a common emotion recognition model. However, on the other hand, the factors of individual differences should be considered to build a more robust affective computing model.

We have investigated how stable our emotion recognition model is across both participants and sessions, and we find that the performance of the model across participants and sessions is worse than that for a single experiment. In general, we want to train the model on the EEG data from a set of participants or sessions and perform inference on the new data from other unseen participants or sessions. However, this is technically difficult due to individual differences across participants with the inherent variability of the EEG measurements such as environmental variables [62]. Although different emotions share some commonalities of neural patterns as we have reported above, they still contain some individual differences for different participants and different sessions, which may lead to changes in the underlying probability distribution from participant to participant or from session to session. This is why the average accuracy of the classifiers trained and tested on each individual participant or session is much higher than that of a classifier trained on a set of participants or sessions and tested on other participants or sessions.

SECTION 6Conclusions and Future Work
In this paper, we have systematically evaluated the performance of different popular methods for feature extraction, feature selection, feature smoothing and pattern classification for emotion recognition on our SEED dataset and the public DEAP dataset. From the experimental results, we have found that GELM with the differential entropy features outperforms other methods. We have achieved the best average classification accuracies of 69.67 and 91.07 percent on the DEAP and SEED datasets, respectively. The comparative classification accuracies achieved show the reliability and superior performance of our machine learning methods in comparison with the existing approaches. We have utilized these methods to investigate the stability of neural patterns over time.

On our SEED dataset, an average classification accuracy of 79.28 percent is achieved with training and testing datasets from different sessions. The experimental results indicate that neural signatures and stable EEG patterns associated with positive, neutral and negative emotions do exist. We have found that the lateral temporal areas activate more for positive emotions than negative emotions in the beta and gamma bands, the neural patterns of neutral emotions have higher alpha responses at parietal and occipital sites, and the negative emotion patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The experiment results also indicate that the stable EEG patterns across sessions exhibit consistency among repeated EEG measurements of the same participant.

In this study, we investigated the stable neural patterns of three emotions: positive, neutral and negative. For future work, more categories of emotions will be studied, and we will evaluate extending the generalization of our proposed approach to more categories of emotions. The order of presentation is the same for different sessions in this study. We are developing a larger stimuli database for emotion experiments and make the stimuli different for different sessions. Moreover, several important factors such as gender, age, and race should be considered. To render the automatic emotion recognition models to be adaptable, factors such as individual differences and temporal evolution should be considered. One possible way of dealing with these problems is to adopt transfer learning techniques [63], [64], [65], [66].