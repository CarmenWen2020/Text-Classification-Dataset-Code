Nonnegativity based matrix factorization is usually powerful for learning the parts-based “shallow” representation, however it fails to discover deep hidden information within both the basis concept and representation spaces. In this paper, we therefore propose a new dual-constrained deep semi-supervised coupled factorization network (DS2CF-Net) for learning hierarchical representations. DS2CF-Net is formulated as the joint partial-label and structure-constrained deep factorization network using multi-layers of linear transformations, which coupled updates the basic concepts and new representations in each layer. An error correction mechanism with feature fusion strategy is also integrated between consecutive layers to improve the representation ability of features. To improve the discriminating abilities of both representation and coefficients in feature space, we clearly consider how to enrich the prior knowledge by the coefficients-based label prediction, and incorporate the enriched prior knowledge as the additional label and structure constraints. To be specific, the label constraint enables the intra-class samples to have the same coordinate in the feature space, while the structure constraint forces the coefficients in each layer to be block-diagonal so that the enriched prior knowledge are more accurate. Besides, we integrate the adaptive dual-graph learning to retain the locality structures of both the data manifold and feature manifold in each layer. Finally, a fine-tuning process is performed to refine the structure-constrained matrix and data weight matrix in each layer using the predicted labels for more accurate representations. Extensive simulations on public databases show that our method can obtain state-of-the-art performance.
Introduction
For high-dimensional data analysis in emerging computer vision applications, one core issue is how to obtain the compact expression with strong representation ability from complex high-dimensional data (Pan & Gillis, 2021; Rahiche & Cheriet, 2021; Shen et al., 2019; Zhang et al., 2020c). To compute strong and effective representations, different algorithms can be used, among which matrix factorization (MF) is one of the widely-used techniques for representation (Golub & Reinsch, 1970; Gray, 1984; Jolliffe, 1986; Lee & Seung, 1999; Lin et al., 2020; Ma et al., 2019; Peng et al., 2019; Wei & Gong, 2004; Xiao et al., 2019; Zhang et al., 2021b). Classical MF methods include singular value decomposition (SVD) (Golub & Reinsch, 1970), vector quantization (VQ) (Gray, 1984), nonnegative matrix factorization (NMF) (Lee & Seung, 1999) and concept factorization (CF) (Wei & Gong, 2004), etc. It is noteworthy that NMF and CF use the nonnegative constraints on the factorization matrices, which enables them to obtain parts-based representations that correspond to the useful distinguishing features for subsequent clustering and classification (Lee & Seung, 1999; Wei & Gong, 2004). Specifically, NMF and CF aim at decomposing a given data matrix X into two or three nonnegative matrix factors whose product is the approximation to X (Lee & Seung, 1999; Wei & Gong, 2004), where one factor contains the basis vectors capturing the high-level features and each sample is reconstructed by a linear combination of the basis vectors. The other nonnegative factor corresponds to the learnt new representation.

CF offers an obvious advantage over NMF, i.e., it can be performed in kernel space and any other representation space, however they both cannot encode the local geometry of features and also fail to apply the label information even if available. To handle the locality preserving issue, some graph regularized methods have been proposed, e.g., graph regularized NMF (GNMF) (Cai et al., 2011a), graph-regularized LCF (GRLCF) (Ye & Jin, 2017), locally consistent CF (LCCF) (Cai et al., 2011b), graph-regularized CF with local coordinate (LGCF) (Li et al., 2017a), dual regularization NMF (DNMF) (Shang et al., 2012) and dual-graph regularized CF (GCF) (Ye & Jin, 2014). These algorithms usually use the graph Laplacian to smooth the representation and encode the geometry information of the data space. Different from GNMF and LCCF, both DNMF and GCF can not only retain the geometrical structures of the data manifold but also the feature manifold using the dual-graph regularization strategy (Dhillon, 2001; Gu & Zhou, 2009; Shang et al., 2012; Ye & Jin, 2014). Although the above algorithms have obtained encouraging clustering results by considering the locality properties, they still suffer from some shortcomings: (1) high sensitivity and tricky optimal determination of the number k of nearest neighbors (Roweis & Saul, 2000; Tenenbaum et al., 2000); (2) separating the graph construction from the matrix factorization by two independent steps cannot ensure the pre-encoded weights to be optimal for subsequent representation; (3) they cannot take advantage of the label information to improve the representation and clustering due to the unsupervised nature, similarly as NMF and CF. To improve the discriminating ability of the MF, some semi-supervised algorithms have been proposed, such as constrained nonnegative matrix factorization (CNMF) (Liu et al., 2012), constrained concept factorization (CCF) (Liu et al., 2014) and semi-supervised GNMF (SemiGNMF) (Cai et al., 2011a). SemiGNMF incorporates partial label information into the graph construction, while CNMF and CCF obtain the representations consistent with the known label information by defining an explicit label constraint matrix, so that the original labeled samples sharing the same label can be mapped into the same class in the feature space. Although CNMF, SemiGNMF and CCF can use the label information of labeled samples clearly, they still fail to fully utilize the unlabeled samples. Since they did not consider predicting the labels of the unlabeled samples and mapping them into their respective subspaces in the feature space as well by learning an explicit label indicator matrix for the unlabeled data. In addition, CNMF, SemiGNMF and CCF also cannot self-express the input data in a recovered feature space. Although preserving the local geometrical structures and incorporating the supervised prior information can improve the representation abilities of NMF and CF effectively, however all aforementioned MF-based models still suffer from a common drawback, i.e., they use a single-layer mode so that they can only discover the “shallow” features while cannot discover deep hidden features and hierarchical information that have been proved to be important and useful for representation learning.

In this paper, we propose a novel deep semi-supervised self-expressive coupled MF strategy that can represent the input data more appropriately using partially labeled data and a deep structure. The main contributions of this work are summarized as follows:

(1)
Technically, a novel supervised prior enrichment guided dual-constrained deep semi-supervised coupled factorization network (shortly, DS2CF-Net) is proposed. To learn the hierarchical coupled representation and extract hidden deep features, we seamlessly integrate the deep coupled semi-supervised concept factorization, prior knowledge enrichment, self-expressive discriminating representation, and the joint label/structure constraints into a unified framework. To encode the deep features accurately, we design a novel updating strategy for the deep MF, i.e., it coupled optimizes the basis vectors and representation matrix in each layer by learning with partial labeled data. Figure 1 illustrates the flowchart of our DS2CF-Net, where the semi-supervised MF, adaptive graph learning, error correction and finetuning processes are described.

(2)
For discriminating representation, the innovations of DS2CF-Net are twofold; (1) it considers enriching the supervised prior knowledge by the joint coefficients-based label prediction; (2) it incorporates the enriched label information as the additional dual label and structure constraints. To enrich the prior knowledge, DS2CF-Net tries to make full use of the unlabeled data by propagating and predicting the labels of unlabeled data using a robust label predictor learned from the labeled data. The dual-constraints are included to improve the discriminating power of the learned representations. Specifically, the enriched prior knowledge based label constraint can enable the originally labeled samples of one class and the unlabeled data with the predicted same label to have the same coordinate in feature space. The enriched prior knowledge based structure constraint forces the self-expressive coefficients matrix to be block-diagonal in each layer so that the manifold is more smooth and accurate for label prediction. Besides, to make full use of the predicted labels, we also consider refining the structure constraint matrix and the data weight matrix to further make the learned representations better using the obtained soft labels in each layer.

(3)
To obtain neighborhood-preserving higher-level representations, DS2CF-Net presents a self-weighted dual-graph learning strategy in each layer, i.e., optimizing the graph weights jointly with the MF. Specifically, in each layer, DS2CF-Net performs an adaptive weight learning over the deep basis vector graph and deep feature graph through minimizing the reconstruction errors based on the deep basis vectors and deep features at the same time. The self-weighted dual-graph learning can also avoid the tricky issue of selecting the optimal number of the nearest neighbors, which is suffered in most existing locality preserving MF models. More importantly, such operation can enable the model to obtain adaptive neighborhood preserving deep basis vectors and deep features for enhancing the representations.

(4)
When the number of layers increases, to obtain more stable and reliable representation and clustering results, we incorporate an error analysis mechanism and a feature fusion strategy into our framework. Specifically, when the clustering result on the features obtained in the current layer is lower than that of the previous layer, a feature fusion will be operated. Note that such operation can effectively prevent the issue that the performance decreases fast with the increase of layers.

Fig. 1
figure 1
The flowchart and learning principle of our proposed DS2CF-Net framework

Full size image
We outline the paper as follows. Section 2 briefly reviews the related work. We present DS2CF-Net in Sect. 3. In Sect. 4, we show the optimization procedures of our DS2CF-Net. Section 5 describes the simulation settings and results. Finally, the paper is concluded in Sect. 6.

Related Work
In this section, we introduce the related single-layer and multilayer frameworks to our proposed DS2CF-Net.

Related Single-Layer CF Based Frameworks
Concept Factorization (CF) (Wei & Gong, 2004)
Given a nonnegative data matrix 𝑋=[𝑥1,𝑥2,…,𝑥𝑁]∈ℝ𝐷×𝑁, where 𝑥𝑖 is a sample vector, N is the number of samples and D denotes the original dimension of the input space. Denote by 𝑈∈ℝ𝐷×𝑟 and 𝑉∈ℝ𝑁×𝑟 two nonnegative matrices whose product 𝑈𝑉𝑇∈ℝ𝐷×𝑁 denotes the approximation to 𝑋, where r is the rank. By representing each basis by a linear combination of 𝑥𝑖, i.e., ∑𝑁𝑖=1𝑤𝑖𝑗𝑥𝑖, where 𝑤𝑖𝑗≥0, then CF proposes to solve the following minimization problem:

𝑂=‖‖𝑋−𝑋𝑊𝑉𝑇‖‖2𝐹, s.t.𝑊,𝑉≥0,
(1)
where 𝑊=[𝑤𝑖𝑗]∈ℝ𝑁×𝑟, XW approximates the bases, VT is the learned representation of X, which can be applied for clustering, and VT is the transpose of matrix V.

Self-Representative Manifold CF (SRMCF) (Ma et al., 2018)
SRMCF integrates the adaptive neighbor structure and manifold regularizer into the CF framework. Specifically, it considers WVT in CF as the coefficient matrix based on the dictionary of the raw data matrix X. Then, it incorporates the self-representation with the adaptive neighbor structure to assign neighbors for all samples. The objective function of SRMCF is defined as

𝑂=‖‖𝑋−𝑋𝑊𝑉𝑇‖‖22+𝜆1∑𝑁𝑖,𝑗=1{‖‖(𝑊𝑉𝑇)𝑖−(𝑊𝑉𝑇)𝑗‖‖22Θ𝑖𝑗+𝜉Θ2𝑖𝑗}+𝜆2𝑡𝑟(𝑉𝑇𝐿𝑉𝑉),𝑠.𝑡.𝑊≥0,𝑉≥0,∀𝑖Θ𝑇𝑖1=1,0≤Θ𝑖≤1,
(2)
where 1 is an all-ones column vector, 𝜉 is a positive trade-off parameter, and Θ𝑖𝑗 denotes the probability of 𝑥𝑗∈[𝑥1,𝑥2,…,𝑥𝑁](excluding itself) being connected to 𝑥𝑖 as a neighbor. Θ𝑖∈ℝ𝑁×1 is a vector with the jth element as Θ𝑖𝑗. Note that the constraints Θ𝑇𝑖1=1 and 0≤Θ𝑖≤1 are used to ensure the probability property of Θ𝑖. 𝐿Θ is the Laplacian matrix of Θ and 𝐿𝑉 is a predefined Laplacian matrix by 0–1 weight based on the Euclidean distances between each sample 𝑥𝑗∈[𝑥1,𝑥2,…,𝑥𝑁] as (Belkin & Niyogi, 2003). 𝜆1 and 𝜆2 are two parameters. Note that SRMCF still suffers from the tough choice of the number of nearest neighbors.

Dual-Graph Regularized CF (GCF) (Ye & Jin, 2014)
GCF introduces the graph regularizers of both the data manifold and feature manifold into CF simultaneously by constructing a k nearest neighbor data graph GV and a k nearest feature graph GU. Then, GCF uses the 0–1 weighting scheme for GV and GU and defines the corresponding weight matrices SV and SU as follows:

(𝑆𝑉)𝑗𝑠(𝑆𝑈)𝑗𝑠={10𝑖𝑓𝑥𝑠∈𝑁𝑘(𝑥𝑗)𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒;𝑗,𝑠=1,2…,𝑁={10𝑖𝑓𝑥𝑇𝑠∈𝑁𝑘(𝑥𝑇𝑗)𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒;𝑗,𝑠=1,2…,𝑀,
(3)
where Nk(xj) denotes the set of the k nearest neighbors of xj. The graph Laplacian over GV and GU are defined as LV = DV − SV and LU = DU − SU, where DV and DU are diagonal matrices with entries being (𝐷𝑉)𝑗𝑗=∑𝑠(𝑆𝑉)𝑗𝑠 and (𝐷𝑈)𝑗𝑗=∑𝑠(𝑆𝑈)𝑗𝑠. Finally, the objective function of GCF is formulated as

𝑂=‖‖𝑋−𝑋𝑊𝑉𝑇‖‖2𝐹+𝛼𝑡𝑟(𝑉𝑇𝐿𝑉𝑉)+𝛽𝑡𝑟(𝑊𝑇𝐿𝑊𝑊),
(4)
where 𝐿𝑊=𝑋𝑇𝐿𝑈𝑋,𝛼 and 𝛽 are parameters. Clearly, GCF has the difficulty issue to choose the optimal k on various datasets.

Constrained CF (CCF) (Liu et al., 2014)
To improve the discriminating power, CCF extends CF to the semi-supervised scenario by using label information of the labeled data as an additional constraint. Suppose that the data matrix X contains a labeled sample set 𝑋𝐿∈ℝ𝐷×𝑙 and an unlabeled set 𝑋𝑈∈ℝ𝐷×𝑢, that is, 𝑙+𝑢=𝑁 and 𝑋=[𝑋𝐿,𝑋𝑈]∈ℝ𝐷×(𝑙+𝑢), where l and u are the numbers of labeled and unlabeled samples respectively, then CCF guides the constrained CF by defining a label constraint matrix A. Denote by 𝐴𝐿∈ℝ𝑙×𝑐 the class indicator matrix defined on labeled data, where c is the number of classes. The element (𝐴𝐿)𝑖𝑗 is defined as 1 if 𝑥𝑖 is labeled as the jth class, and 0 otherwise. Since CCF did not define an explicit class indicator for 𝑋𝑈 and simply used an identity matrix 𝐼𝑢×𝑢 of dimension 𝑢×𝑢 for unlabeled data. Thus, the overall label constraint matrix A is defined as

𝐴=[(𝐴𝐿)𝑙×𝑐00𝐼𝑢×𝑢]∈ℝ(𝑙+𝑢)×(𝑐+𝑢).
(5)
To ensure the data points sharing the same label to be mapped into the same class in low-dimensional space (i.e., same vi), CCF imposes the label constraints by an auxiliary matrix Z:

𝑉=𝐴𝑍.
(6)
By substituting V = AZ into CF, CCF finds a non-negative matrix 𝑊∈ℝ𝑁×𝑟 and a non-negative auxiliary matrix 𝑍∈ℝ(𝑐+𝑢)×𝑟 from the following objective function:

𝑂=‖‖𝑋−𝑋𝑊𝑍𝑇𝐴𝑇‖‖2𝐹,𝑠.𝑡.𝑊,𝑍≥0.
(7)
Related Deep/Multilayer MF Frameworks
We then introduce the architectures of several related deep/multilayer matrix factorization algorithms.

Traditional Multilayer MF
The multilayer MF methods of this category usually use the output of the previous layer (i.e., intermediate representation V) as the input of subsequent layer directly, without properly considering to optimize the representation and basis vectors in each layer. Classical methods include MNMF, MCF and GMCF, etc. These methods aim to minimize the objective function in each layer independently and simply use 𝑉m−1(𝑚≥2) obtained in the (m − 1)th layer as the input of the Mth layer. That is, they cannot ensure the intermediate representation to be a good representation for subsequent layers, which may cause the degraded performance. We show the multilayer structure of this category methods in Fig. 2a.

Fig. 2
figure 2
Architecture comparison of existing multilayer MF framework, including a traditional multilayer CF model (e.g., MNMF, MCF and GMCF); b DSCF-Net model and c our proposed DS2CF-Net

Full size image
Optimized Deep MF Models
The recent fast developments of deep learning have led to a renewed interest in designing the deep or multi-layer MF (Cichocki & Zdunek, 2006; Li & Tang, 2017; Li et al., 2015, 2017b; Lin et al., 2020; Ma et al., 2019; Rajabi & Ghassemian, 2015; Trigeorgis et al., 2015; Xiao et al., 2019)) for deep representation learning and clustering. One of the most widely-used approach to extend the single-layer model to the deep model of M-layers is to iteratively take the outputted representation of the last layer as the inputs of the next layer directly for further MF (Cichocki & Zdunek, 2006; Li et al., 2015, 2017b; Rajabi & Ghassemian, 2015), where M denotes the number of layers, such as multilayer NMF (MNMF) (Cichocki & Zdunek, 2006), Multilayer CF (MCF) (Li et al., 2015), Spectral unmixing using multilayer NMF (MLNMF) (Rajabi & Ghassemian, 2015) and graph regularized multilayer CF (GMCF) (Li et al., 2017b). However, such a strategy may be invalid and even unreasonable in practice, because in this case the learned representation of the first layer determines the learning abilities of the whole framework, while most existing models cannot ensure this issue. In other words, one cannot ensure that the output of the last layer is already a good representation, so directly feeding it to the next layer may mislead and degrade the learning power of subsequent layers. To address these issues, the other popular and optimized way is to discover hidden deep feature information by adopting multiple layers of linear transformations and updating the basis vectors or feature representations in each layer (Li & Tang, 2017; Trigeorgis et al., 2015), such as weakly-supervised deep MF (WDMF) (Li & Tang, 2017), deep semi-nMF (DSNMF) (Trigeorgis et al., 2015) and deep self-representative concept factorization network (DSCF-Net) (Zhang et al., 2019). In general, WDMF aims at fixing the basis vectors and optimizes the representations in each layer, while DSCF-Net argues that learning a set of optimal basis vectors will be more important and accurate for reconstructing given data by a linear combination of the bases, which aims at optimizing the basis vectors to update the representation matrix in each layer. It is noted that WDMF mainly focuses on the social image understanding tasks, i.e., tag refinement, tag assignment and image retrieval, and the initial input of WDMF is the tagging matrix F rather than the data matrix X as other MF models. In addition, DSCF-Net also incorporates the subspace recovery process and adaptive locality-preserving power into a united framework for enhancing the feature representations. Different from WDMF and DSCF-Net, DSNMF is just a two-stage approach, where the strategy in the first stage is the same as traditional MNMF, MCF, MLNMF and GMCF, i.e., directly feeding the learned representation matrix of the last layer into the next layer for further MF, and the second stage refines the representation matrices and basis vectors directly based on the outputs of each layer in the first stage using an independent step. It is clear that the refining step can obtain deep features, but the learned deep features are directly based on the first stage. As such, DSNMF will suffer from the same performance-degrading issue as traditional MNMF, MCF, MLNMF and GMCF. In addition, DSNMF also cannot preserve the manifold structures of samples, especially in an adaptive manner, and it also cannot use supervised prior information for the discriminant data representations. Note that we illustrate the multilayer structure of DSCF-Net for uncovering hidden features in Fig. 2b. For comparison, we also illustrate the deep coupled factorization network of our DS2CF-Net in Fig. 2c, from which we can see that DS2CF-Net coupled optimizes the basis vectors and representation matrix in each layer.

Deep Semi-supervised Coupled Factorization Network (DS2CF-Net)
We introduce the formulation of DS2CF-Net. Given a partially labeled data matrix 𝑋=[𝑋𝐿,𝑋𝑈]∈ℝ𝐷×(𝑙+𝑢), DS2CF-Net performs the semi-supervised representation-based clustering over the whole dataset. As a classical semi-supervised learning setting, the labeled set contains a small proportion of samples, while the unlabeled set contains a large proportion of samples. The base model of DS2CF-Net is built based on the semi-supervised formulation of CCF, i.e., incorporating a label constraint matrix A and approximating the representation matrix V with AZ. However, to enhance the data representation and clustering abilities, DS2CF-Net designs a hierarchical and coupled factorization framework that has M layers. Technically, DS2CF-Net is modeled as the formulation of learning M updated pairs of representation matrices and basis vectors 𝑋𝑊1…𝑊𝑀, and M updated label constraint matrices A. That is, the label constraint matrix A in our DS2CF-Net is alternately updated and enriched over unlabeled data, instead of fixing it as CCF does.

Factorization Model
Before presenting the factorization model, we first describe the initial optimization problem of DS2CF-Net as follows:

𝑂𝐷𝑆2𝐶𝐹−𝑁𝑒𝑡=‖‖𝑋−𝑋𝑊0…𝑊𝑀(𝑍0…𝑍𝑀)𝑇𝐴𝑇‖‖2𝐹+𝛼𝐽2+𝛽𝐽3+𝛾𝐽1𝑠.𝑡.∀𝑖∈{1,2,…,𝑀}𝑊𝑖≥0,𝑍𝑖≥0,
(8)
where XW0…WM corresponds to the set of deep basis vectors, (Z0…ZM)TAT denotes the learned deep low-dimensional representation, ‖‖𝑋−𝑋𝑊0…𝑊𝑀(𝑍0…𝑍𝑀)𝑇𝐴𝑇‖‖2𝐹 denotes the deep reconstruction error, J1, J2 and J3 will be shown shortly. W0 and Z0 are included to facilitate the description and optimization, and both are fixed to be the identity matrices. The overall label constraint matrix A in our network is defined as follows:

𝐴=[𝐴𝐿00𝐴𝑈]∈ℝ(𝑙×𝑢)×(𝑐+𝑐),𝐴𝐿∈ℝ𝑙×𝑐,𝐴𝑈∈ℝ𝑢×𝑐,
(9)
where 𝐴𝐿 is a class indicator matrix for labeled data, which can be easily defined as (Liu et al., 2014), i.e., (AL)i,j = 1 if sample xi is labeled as the class j, and else 0. However, DS2CF-Net also computes an explicit class indicator AU for the unlabeled data to enrich the supervised prior rather than fixing it to be an identity matrix as CCF, which can well group the representations of both the labeled and unlabeled samples based on the enriched supervised prior guided dual label and structure constraints.

According to the self-expressive properties on the coefficient matrix (Ma et al., 2018), the reconstruction error can be rewritten as

‖𝑋−𝑋𝑅𝑀‖2𝐹,𝑤ℎ𝑒𝑟𝑒𝑅𝑀=𝑊0…𝑊𝑀(𝑍0…𝑍𝑀)𝑇𝐴𝑇,
(10)
where RM can be regarded as the meaningful coefficient matrix self-expressing X. Then, the proposed multi-layer factorization model can be presented as follows:

𝑋←𝑈𝑀𝑉𝑇𝑀𝑈𝑀=𝑈𝑀−1𝑊𝑀⋮𝑈2=𝑈1𝑊2𝑈1=𝑋𝑊1and𝑉𝑀=𝑉𝑀−1𝑍𝑀⋮𝑉2=𝑉1𝑍2𝑉1=𝐴𝑍1,
(11)
where Um (m = 1, 2,…, M) is the set of basis vectors of the mth layer, 𝑉𝑇𝑚(m = 1, 2,…, M) is the low-dimensional representation, Wm (m = 1, 2,…, M) is the intermediate matrix for updating basis vectors and Zm (m = 1, 2,…, M) is the intermediate auxiliary matrix for updating the representations. It is noteworthy that the factorization model of DS2CF-Net does not need to initialize the network using the traditional multi-layer model as DSNMF that initializes the network by directly feeding the learnt representation matrix of the last layer into the next layer for MF. And the deep factorization process of DSNMF completely depends on the intermediate outputs of traditional multi-layer model.

Enriched Prior Based Dual-Constraints
We first describe how to enrich the supervised prior information. DS2CF-Net learns a robust label predictor 𝑃∈ℝ𝐷×𝑐 over labeled data by minimizing a label fitness error ‖‖𝐴𝐿−𝑋𝑇𝐿𝑃‖‖2𝐹, where c is the number of classes, which can map each sample xi into a label space in terms of PTxi. In addition, DS2CF-Net also considers preserving the neighborhood information of the embedded soft labels PTXi in the projective label space by self-expressing it with the coefficient matrix RM. The formulation of learning the label predictor P can then be defined as follows:

𝐽1=‖‖𝐴𝐿−𝑋𝑇𝐿𝑃‖‖2𝐹+‖‖𝑃𝑇𝑋−𝑃𝑇𝑋𝑅𝑀‖‖2𝐹+‖𝑃‖2,1=‖‖𝐴𝐿−𝑋𝑇𝐿𝑃‖‖2𝐹+‖‖𝑃𝑇𝑋−𝑃𝑇𝑋(𝑊0…𝑊𝑀(𝑍0…𝑍𝑀)𝑇𝐴𝑇)‖‖2𝐹+‖𝑃‖2,1,
(12)
where L2,1-norm based regularization can enable the label predictor to be robust against the outliers and error in data. In addition, L2,1-norm can enable the discriminative labels to be predicted and estimated in a latent sparse feature subspace.

Enriched Prior Based Label Constraint
After the label predictor P is obtained, we can easily predict the soft label of each unlabeled sample 𝑥𝑖∈𝑋𝑈 as 𝑥𝑇𝑖𝑃. Then, we obtain AU by using the normalized soft labels that are described as follows:

(𝐴𝑈)𝑖𝑗=(𝑋𝑇𝑈𝑃)𝑖𝑗/∑𝑐𝑗=1(𝑋𝑇𝑈𝑃)𝑖𝑗.
(13)
That is, the normalized soft labels meet the column-sum-to-one constraint AU1 = 1, where 1∈ℝ𝑐×1 is a column vector of ones. Note that one recent related work is called robust semi-supervised adaptive concept factorization (RS2ACF) (Zhang et al., 2020b) has also discussed the partially labeled CF model and considered learning a class indicator AU for unlabeled data, but our DS2CF-Net is different from it in three aspects. First, DS2CF-Net is a deep MF model, while RS2ACF is a single-layer model. Second, the manifold smoothness for label prediction in DS2CF-Net is defined based on the self-expressive deep coefficient matrix in each layer, while RS2ACF encodes the manifold smoothness by learning an extra weight matrix and is performed in a single-layer mode. Third, DS2CF-Net defines the class indicator AU based on the normalized soft labels of unlabeled data rather than directly embedding XU into P. Since the predicted soft label value (AU)ij indicates the probability of each xi belonging to the class j, forcing AU1 = 1 may be more accurate and reasonable.

Enriched Prior Based Structure Constraint
Since the coefficients W0…WM(Z0…ZM)TAT can characterize the locality of the features, it should have a good block-diagonal structure, where each block corresponds to a subspace or a class. As such, each sample can be reconstructed more accurately by the samples of the same class as much as possible. Thus, we introduce a block-diagonal structure constraint matrix Q to constrain the coefficient matrix by minimizing the approximation error between Q and W0…WM(Z0…ZM)TAT in each layer:

𝐽2=‖‖𝑄−𝑊0…𝑊𝑀(𝑍0…𝑍𝑀)𝑇𝐴𝑇‖‖2𝐹+‖‖𝑊0…𝑊𝑀(𝑍0…𝑍𝑀)𝑇𝐴𝑇‖‖2𝐹,
(14)
where the structure constraint matrix Q is defined as follows:

𝑄𝑄𝐿=[𝑄𝐿00𝑄𝑈]∈ℝ(𝑙+𝑢)×(𝑙+𝑢),=⎡⎣⎢⎢⎢⎢𝑄10000𝑄20000...0000𝑄𝑐⎤⎦⎥⎥⎥⎥∈ℝ𝑙×𝑙,
(15)
where QL and QU are the structure constraint matrices defined based on the labeled data XL and unlabeled data XU. Since the samples of XL are originally labeled, 𝑄𝐿 is a strict block-diagonal matrix, where each block Qi (I = 1, 2, …, c) is an 𝑙𝑖×𝑙𝑖 matrix of all ones, defined according to the labeled samples, and li is the number of samples in class i in XL. For example, if we have 9 labeled samples, where x1 and x2 are from the class 1, x3, x4, x5 and x6 are from class 2 and the remaining ones are from class 3, the sub-matrices Q1, Q2 and Q3 can be defined as

𝑄1=[1111],𝑄2=⎡⎣⎢⎢⎢⎢1111111111111111⎤⎦⎥⎥⎥⎥,𝑄3=⎡⎣⎢⎢111111111⎤⎦⎥⎥,
It should be noted that we initiate QU by the cosine similarities over the samples in XU and update QU in Mth (M > 1) layer using the cosine similarity matrix defined on the new representation of XU, i.e., (Vm)i, 𝑖∈{𝑙+1,…,𝑁}. In this way, we can ensure the overall coefficient matrix W0…WM(Z0…ZM)TAT to have a good structure for the representation learning.

Self-Weighted Dual-graph Learning
To obtain the locality preserving representation, we further add a self-weighted dual-graph learning into DS2CF-Net, which can preserve the neighborhood information of both the deep basis vectors XW0…WM and representations (Z0…ZM)TAT in an adaptive manner at the same time. Specifically, we compute the data weight matrix 𝑆𝑉∈ℝ𝑁×𝑁 over the deep representations and the feature weight matrix 𝑆𝑈∈ℝ𝐷×𝐷 over the deep basis vectors adaptively by solving the following reconstructive loss:

𝐽3=‖‖(𝑋𝑊0…𝑊𝑀)𝑇−(𝑋𝑊0…𝑊𝑀)𝑇𝑆𝑈‖‖2𝐹+‖‖((𝑍0…𝑍𝑀)𝑇𝐴𝑇)−((𝑍0…𝑍𝑀)𝑇𝐴𝑇)𝑆𝑉‖‖2𝐹,𝑠.𝑡.𝑆𝑈≥0,𝑆𝑉≥0.
(16)
Clearly, the nonnegative dual-graph weights of DS2CF-Net are different from those of GCF (Ye & Jin, 2014) in two aspects. First, GCF is a single-layer model that defines the weights over the “shallow” basis vectors and features, while our DS2CF-Net encodes the locality based on the deep basis vectors and features. Second, our DS2CF-Net does not need to specify the number of nearest neighbors, suffered in GCF, since the neighbors of each sample are determined automatically in DS2CF-Net by directly minimizing the reconstruction error. In addition, the dual-graph weights are adaptive, and are also updated with the factorization process, which can enable DS2CF-Net to be adaptive to different datasets and produce accurate feature representations.

Fine-Tuning of the Structure-Constrained Matrix Q U and Data Weight Matrix S V Using A U
In this process, we consider how to refine QU and SV after obtaining the soft labels AU for XU, so that the learned representations are better. Specifically, in each iteration, we first obtain the hard labels 𝐴∗𝑈 by setting the maximum value in each column of AU to 1 and otherwise setting it to 0. The fine-tuning process can then be performed as follows: if (𝐴∗𝑈)𝑖𝑗=0, then (QU)ij = 0 and (SV)ij = 0. This is easy to understand, since (𝐴∗𝑈)𝑖𝑗=0 means that the unlabeled samples xi and xj are not in the same class. Hence, the structure constraint information and the data weight between them shall be ideally zeros for the consideration of discrimination.

Error Correction Mechanism and Feature Fusion
For a multi-layer model, it is important to ensure that the performance will not decrease fast with the increasing number of layers. In order to avoid this potential risk for our DS2CF-Net, we clearly incorporate an error correction mechanism and a feature fusion strategy into our framework. Specifically, between two consecutive layers, we include a clustering evaluation module to evaluate the clustering performance over the features in each layer. Specifically, for the new representation Vi in the ith (i > 1) layer, we input it into the clustering evaluation module, i.e., we perform the clustering evaluations by K-means algorithm based on Vi, and then we can obtain the clustering accuracy ACi. To avoid fast degradation as the number of layers increases, the error correction and feature fusion are performed as follows. If ACi is larger than ACi-1, then directly moving to the (i + 1)th layer, otherwise performing the feature fusion over Vi, i.e., adding Vi and the features of the first i − 1 layers together to update Vi = V1 +  ⋯ + Vi. In this way, DS2CF-Net can deliver more stable and reliable results in the multi-layer case. Note that these strategies will be verified by extensive simulations.

Objective Function
Based on the above analysis, the final objective function of our DS2CF-Net method can be formulated as

𝑂𝐷𝑆2𝐶𝐹−𝑁𝑒𝑡=min𝑊1,...,𝑊𝑀,𝑍1,...,𝑍𝑀,𝑆𝑈,𝑆𝑉,𝑃‖‖𝑋−𝑋𝑊0...𝑊𝑀(𝑍1...𝑍𝑀)𝑇𝐴𝑇‖‖2𝐹+𝛼[‖𝑄−𝑅𝑀‖2𝐹+‖𝑅𝑀‖2𝐹]+𝛽[‖‖𝑈𝑇𝑀−𝑈𝑇𝑀𝑆𝑈‖‖2𝐹+‖‖𝑉𝑇𝑀−𝑉𝑇𝑀𝑆𝑉‖‖2𝐹],+𝛾[‖‖𝐴𝐿−𝑋𝑇𝐿𝑃‖‖2𝐹+‖‖𝑃𝑇𝑋−𝑃𝑇𝑋𝑅𝑀‖‖2𝐹+‖𝑃‖2,1]𝑠.𝑡.∀𝑚∈{1,2,...,𝑀}𝑊𝑚≥0,𝑍𝑚≥0,𝑆𝑈≥0,𝑆𝑉≥0
(17)
where 𝑈𝑀=𝑋𝑊0⋅⋅⋅𝑊𝑀, 𝑉𝑀=𝐴(𝑍0...𝑍𝑀) and 𝑅𝑀=𝑊0...𝑊𝑀𝑉𝑇𝑀.

Optimization and Computational Complexity
Optimization
From the objective function of DS2CF-Net, we can easily find that the involved variables Wm, Zm (𝑚∈{1,2,...,𝑀}), SU, SV and P depend on each other, so they cannot be solved directly. Following the common procedures, we present an iterative optimization strategy using the Multiplicative Update Rules (MUR) method (Li et al., 2017c; Zhao & Tan, 2018) for obtaining local optimal solutions. Specifically, we solve the problem by updating the variables alternately and optimize one of them each time by fixing the others. The detailed optimization procedures are showed as follows:

Fix Others, Update the Factors W m and Z m
We first show how to optimize Wm and Zm. For the mth layer, W1, …, Wm−1, Z1,…, Zm−1 and P are all known as the constants. By defining Π𝑚−1=𝑊0...𝑊𝑚−1 and Λ𝑚−1=𝑍0...𝑍𝑚−1, the reduced sub-problem associated with Wm and Zm can be defined as

min𝑊𝑚,𝑍𝑚‖‖𝑋−𝑋Π𝑚−1𝑊𝑚(Λ𝑚−1𝑍𝑚)𝑇𝐴𝑇‖‖𝐹+𝛼(‖𝑄−𝑅𝑀‖2𝐹+‖𝑅𝑀‖2𝐹)+𝛽(‖‖𝑈𝑇𝑀−𝑈𝑇𝑀𝑆𝑈‖‖2𝐹+‖‖𝑉𝑀−𝑉𝑀𝑆𝑉‖‖2𝐹),+𝛾‖‖𝑃𝑇𝑋−𝑃𝑇𝑋𝑅𝑀‖‖,𝑠.𝑡.𝑊𝑚,𝑍𝑚≥0
(18)
where 𝑈𝑀=𝑋Π𝑚−1𝑊𝑚, 𝑉𝑀=𝐴Λ𝑚−1𝑍𝑚 and 𝑅𝑀=Π𝑚−1𝑊𝑚(Λ𝑚−1𝑍𝑚)𝑇𝐴𝑇. Let 𝜓𝑤𝑖𝑘 and 𝜓𝑧𝑖𝑘 be the Lagrange multipliers for the constraints (𝑊𝑚)𝑖𝑘≥0 and (𝑍𝑚)𝑖𝑘≥0, Ψ𝑤=[𝜓𝑤𝑖𝑘] and Ψ𝑧=[𝜓𝑧𝑖𝑘], then the Lagrange function can be constructed as

℘=‖‖𝑋−𝑋Π𝑚−1𝑊𝑚(Λ𝑚−1𝑍𝑚)𝑇𝐴𝑇‖‖𝐹+𝛼[‖‖𝑄−Π𝑚−1𝑊𝑚(Λ𝑚−1𝑍𝑚)𝑇𝐴𝑇‖‖2𝐹+‖‖Π𝑚−1𝑊𝑚(Λ𝑚−1𝑍𝑚)𝑇𝐴𝑇‖‖2𝐹]+𝛽𝑡𝑟[(𝑋Π𝑚−1𝑊𝑚)𝑇𝐻𝑢(𝑋Π𝑚−1𝑊𝑚)+(𝐴Λ𝑚−1𝑍𝑚)𝑇𝐻𝑣(𝐴Λ𝑚−1𝑍𝑚)]+𝛾‖‖𝑃𝑇𝑋−𝑃𝑇𝑋Π𝑚−1𝑊𝑚(Λ𝑚−1𝑍𝑚)𝑇𝐴𝑇‖‖+𝑡𝑟(Ψ𝑤𝑊𝑇𝑚)+𝑡𝑟(Ψ𝑧𝑍𝑇𝑚),
(19)
where 𝐻𝑢=(𝐼−𝑆𝑈)(𝐼−𝑆𝑈)𝑇, 𝐻𝑣=(𝐼−𝑆𝑉)(𝐼−𝑆𝑉)𝑇 and I denotes an identity matrix. Then, Wm and Zm can be alternately updated by fixing others. Let 𝐾𝑋=𝑋𝑇𝑋, 𝐾𝐴=𝐴𝑇𝐴 and 𝐾𝑃=𝑋𝑇𝑃𝑃𝑇𝑋, the derivatives w.r.t. Wm and Zm can be obtained as

∂℘/∂𝑊𝑚=2(Π𝑇𝑚−1𝐾𝑋Π𝑚−1𝑊𝑚𝑉𝑇𝑀𝑉𝑀−Π𝑇𝑚−1𝐾𝑋𝑉𝑀)+2𝛼(−Π𝑇𝑚−1𝑄𝑉𝑀+2𝑄𝑇Π𝑚−1𝑊𝑚𝑉𝑇𝑀𝑉𝑀)+𝛽(Π𝑇𝑚−1𝑋𝑇(𝐻𝑢+𝐻𝑇𝑢)𝑋Π𝑚−1𝑊𝑚)+2𝛾(Π𝑇𝑚−1𝐾𝑃Π𝑚−1𝑊𝑚𝑉𝑇𝑀𝑉𝑀−Π𝑇𝑚−1𝐾𝑃𝑉𝑀)+Ψ𝑤.
(20)
∂℘/∂𝑍𝑚=2(Λ𝑇𝑚−1𝐾𝐴Λ𝑚−1𝑍𝑚𝑈𝑇𝑀𝑈𝑀−Λ𝑇𝑚−1𝐴𝑇𝐾𝑋Π𝑚)+2𝛼(2Λ𝑇𝑚−1𝐾𝐴Λ𝑚−1𝑍𝑚𝑊𝑇𝑚𝑄𝑇Π𝑚−Λ𝑇𝑚−1𝐴𝑇𝑄𝑇Π𝑚)+𝛽(Λ𝑇𝑚−1(𝐻𝑣+𝐻𝑇𝑣)Λ𝑚−1𝑍𝑚𝐾𝑇𝐴)+2𝛾(Λ𝑇𝑚−1𝐾𝐴Λ𝑚−1𝑍𝑚𝑈𝑇𝑀𝑃𝑃𝑇𝑈𝑇𝑀−Λ𝑇𝑚−1𝐴𝑇𝐾𝑃Π𝑚)+Ψ𝑧,
(21)
where Π𝑚=Π𝑚−1𝑊𝑚, and Π𝑚 is known when updating Zm. By using the KKT conditions 𝜓𝑤𝑖𝑘(𝑊𝑚)𝑖𝑘=0 and 𝜓𝑧𝑖𝑘(𝑍𝑚)𝑖𝑘=0, we can obtain the updating rules for Wm and Zm:

(𝑊𝑚)𝑖𝑘←(𝑊𝑚)𝑖𝑘(2Π𝑇𝑚−1𝐾𝑋𝑉𝑀+2𝛼Π𝑇𝑚−1𝑄𝑉𝑀+Ω𝑊)𝑖𝑘(2Π𝑇𝑚−1𝐾𝑋Π𝑚−1𝑊𝑚𝑉𝑇𝑀𝑉𝑀+Φ𝑊)𝑖𝑘,
(22)
(𝑍𝑚)𝑖𝑘←(𝑍𝑚)𝑖𝑘(2Λ𝑇𝑚−1𝐴𝑇𝐾𝑋Π𝑚+2𝛼Λ𝑇𝑚−1𝐴𝑇𝑄𝑇Π𝑚+Ω𝑍)𝑖𝑘(2Λ𝑇𝑚−1𝐾𝐴Λ𝑚−1𝑍𝑚𝑈𝑇𝑀𝑈𝑀+Φ𝑍)𝑖𝑘,
(23)
where Φ𝑊=4𝛼𝑄𝑇Π𝑚−1𝑊𝑚𝑉𝑇𝑀𝑉𝑀+𝛽Π𝑇𝑚−1𝑋𝑇(𝐻𝑢+𝐻𝑇𝑢)𝑋Π𝑚−1𝑊+ 2𝛾Π𝑇𝑚−1𝐾𝑃Π𝑚−1𝑊𝑚𝑉𝑇𝑀𝑉𝑀, Φ𝑍=4𝛼Λ𝑇𝑚−1𝐾𝐴Λ𝑚−1𝑍𝑚𝑊𝑇𝑚𝑄𝑇Π𝑚+𝛽Λ𝑇𝑚−1 (𝐻𝑣+𝐻𝑇𝑣)Λ𝑚−1𝑍𝑚𝐾𝑇𝐴+2𝛾Λ𝑇𝑚−1𝐾𝐴Λ𝑚−1𝑍𝑚𝑈𝑇𝑀𝑃𝑃𝑇𝑈𝑀, Ω𝑊=2𝛾Π𝑇𝑚−1𝐾𝑃𝑉𝑀 and Ω𝑍=2𝛾Λ𝑇𝑚−1𝐴𝑇𝐾𝑃Π𝑚 are auxiliary matrices.

Fix Others, Update the Weight Matrices S U and S V
When other variables are computed, we can use them to update the dual-graph weights SU and SV by removing the irrelevant terms to SU and SV from the objective function. Let 𝜗𝑢𝑖𝑘 and 𝜗𝑣𝑖𝑘 denote the Lagrange multipliers for the constraints 𝑆𝑈𝑖𝑘≥0 and 𝑆𝑉𝑖𝑘≥0, Ω𝑢=[𝜗𝑢𝑖𝑘] and Ω𝑣=[𝜗𝑣𝑖𝑘], then the Lagrange function of the reduced problem can be similarly defined as

℘ˆ=𝛽(‖‖𝑈𝑇𝑀−𝑈𝑇𝑀𝑆𝑈‖‖2𝐹+‖‖𝑉𝑇𝑀−𝑉𝑇𝑀𝑆𝑉‖‖2𝐹)+𝑡𝑟(Ω𝑢𝑆𝑈𝑇)+𝑡𝑟(Ω𝑣𝑆𝑉𝑇),
(24)
where 𝑈𝑀=𝑋Π𝑚−1𝑊𝑚 and 𝑉𝑀=𝐴Λ𝑚−1𝑍𝑚 are known variables in this step. Based on the KKT conditions 𝜗𝑢𝑖𝑘𝑆𝑈𝑖𝑘=0 and 𝜗𝑣𝑖𝑘𝑆𝑉𝑖𝑘=0, we can obtain the updating rules for SU and SV:

(𝑆𝑈)𝑖𝑘←(𝑆𝑈)𝑖𝑘((𝑋Π𝑚−1𝑊𝑚)(𝑋Π𝑚−1𝑊𝑚)𝑇)𝑖𝑘((𝑋Π𝑚−1𝑊𝑚)(𝑋Π𝑚−1𝑊𝑚)𝑇𝑆𝑈)𝑖𝑘,
(25)
(𝑆𝑉)𝑖𝑘←(𝑆𝑉)𝑖𝑘((𝐴Λ𝑚−1𝑍𝑚)(𝐴Λ𝑚−1𝑍𝑚)𝑇)𝑖𝑘((𝐴Λ𝑚−1𝑍𝑚)(𝐴Λ𝑚−1𝑍𝑚)𝑇𝑆𝑉)𝑖𝑘.
(26)
Fix Others, Update the Robust Label Predictor P
Finally, we solve the projection P from Eq. (17), with Wm, Zm, SU and SV known. By the properties of L2,1-norm (Hou et al., 2014; Yang et al., 2011; Zhang et al., 2020a, 2021a), we have ‖𝑃‖2,1=2𝑡𝑟(𝑃𝑇𝐵𝑃), where B is a 𝐷×𝐷 diagonal matrix with entries 𝑏𝑖𝑖=1/(2‖‖𝑝𝑖‖‖2), where pi is the ith row of P. Thus, we can infer the label predictor P from the following problem:

min𝑃,𝐵‖‖𝐴𝐿−𝑋𝑇𝐿𝑃‖‖2𝐹+‖‖𝑃𝑇𝑋(𝐼−𝑅𝑀)‖‖2𝐹+𝑡𝑟(𝑃𝑇𝐵𝑃),
(27)
where each 𝑝𝑖≠0. By seeking the derivative of the above problem w.r.t. P, we can infer P in each layer as follows:

𝑃=(𝑋𝐿𝑋𝑇𝐿+𝑋𝐻𝑀𝑋𝑇+𝐵)−1𝑋𝐿𝐴𝐿,
(28)
where HM = (I − RM)(I − RM)T. After P is obtained, we can use it to update the diagonal matrix B and predict the labels of unlabeled samples. After that, we can use the normalized soft labels to optimize the label constraint matrix A for representation.

Algorithm 1: Optimization procedures of DS2CF-Net
Inputs: Partially labeled data matrix X = [XL, XU], constant r and tunable parameters 𝛼,𝛽,𝛾
Initializations:
Initialize W and Z to be the random matrices;
Initialize the diagonal matrix B as the identity matrix;
Initialize the linear label predictor as 𝑃=(𝑋𝐿𝑋𝑇𝐿+𝐼)−1𝑋𝐿𝐴𝐿, use P to predict the soft labels of unlabeled data as 𝑋𝑇𝑈𝑃, and then apply the normalized soft labels by Eq. (13) to initialize the label constraint matrix A by using Eq. (9);
Initialize QU by the cosine similarities over XU;
Initialize SU using the cosine similarities over X and initialize SV using the semi-supervised weights, i.e., supervised ones for XL and the cosine similarities for XU;
Initial clustering accuracy AC0 = 0; t = 0; Initialize m = 1
For each fixed number m of layers:
While not converged do
1. Update the matrix factors 𝑊𝑡+1𝑚 and 𝑍𝑡+1𝑚 by Eqs. (22–23), and then we can obtain 𝑉𝑡+1𝑚=𝐴𝑍0...𝑍𝑡+1𝑚;
2. Update the weights (𝑆𝑈)𝑡+1 and (𝑆𝑉)𝑡+1 by Eqs. (25–26);
3. Update the linear label predictor 𝑃𝑡+1 by Eq. (28), update the soft labels of XU as 𝑋𝑇𝑈𝑃𝑡+1, and then update AU by Eq. (13);
4. Update the full label-constraint matrix A by Eq. (9);
5. Update QU by the cosine similarities defined based on (𝑉𝑡+1𝑚)𝑖, 𝑖∈{𝑙+1,...,𝑁}, and update the structure-constraint matrix Q;
6. Fine-tuning of SV and Q based on the soft labels AU;
7. Check for convergence: if ‖‖𝑊𝑡+1𝑚−𝑊𝑡𝑚‖‖2𝐹≤𝜀 and ‖‖𝑉𝑡+1𝑚−𝑉𝑡𝑚‖‖2𝐹≤𝜀, stop; else t = t + 1
End while
8. Clustering evaluation on Vm by K-means and obtain ACm.;
9. Error correction and feature fusion: If ACm < ACm−1, then obtaining the fused features as 𝑉𝑚=𝑉𝑚+...+𝑉1; else go to step 1
End for
Output: Deep representation 𝑉∗𝑚 and clustering result ACm	 
For complete presentation, we summarize the optimization procedures of DS2CF-Net in Algorithm 1, where the diagonal matrix B is initialized as an identity matrix. We initialize the linear label predictor 𝑃=(𝑋𝐿𝑋𝑇𝐿+𝐼)−1𝑋𝐿𝐴𝐿 as (Zhang et al., 2020b) and predict the soft labels of unlabeled data as 𝑋𝑇𝑈𝑃, and then we normalize the soft labels by Eq. (13). Based on the normalized soft labels of unlabeled data, we can initialize the label constraint matrix A. Since DS2CF-Net jointly optimizes the basis vectors and representation matrices that are two major variables, to ensure the proposed algorithm to converge, the stopping condition can be simply set to ‖‖𝑊𝑡+1𝑚−𝑊𝑡𝑚‖‖2𝐹≤𝜀 and ‖‖𝑉𝑡+1𝑚−𝑉𝑡𝑚‖‖2𝐹≤𝜀 (𝜀=10−3) in the mth layer, where 𝑉𝑡+1𝑚=𝐴𝑍0...𝑍𝑡+1𝑚 is the computed representation matrix in the mth layer and the approximation errors measure the difference between two sequential sets of basis vectors and representation matrices, which can make sure that the representation learning result will not change drastically. Note that an early version was presented in Cohen et al. (2017). This paper has also incorporated an error analysis mechanism and a feature fusion strategy, so that more stable and reliable representation and clustering results can be obtained. In addition, a new fine-tuning process is performed to refine the structure-constrained matrix and data weight matrix in each layer for obtaining more accurate representations. Besides, this paper also provided the time complexity analysis and conducted a thorough evaluation on the tasks of representation learning and clustering by including visual image analysis and adding more real-world databases.

Computational Complexity Analysis
We discuss the time complexity of DS2CF-Net. We use the big O notation to show the complexity of our algorithm as (Cormen, 2009). For each layer, according to the updating rules of our DS2CF-Net, we need to perform the extra computation of AU, SU, SV, Q, and P over the CCF algorithm. We can find that the big O of each updating operation for each variable is not more than O(N3) in the optimization procedure if N is larger than the dimensionality D. Since the number of layers and the number of iteration times are all constants, they are negligible when calculating the computational complexity. Overall, the time complexity of our DS2CF-Net is O(N3). Note that we also report the actual runtime performance comparison to other methods in Sect. 5.

Experimental Results and Analysis
In this section, we mainly conduct simulations to examine the data representation and clustering performance of our DS2CF-Net. The experimental results of our DS2CF-Net are compared with those of 5 deep ML models [i.e., MNMF (Cichocki & Zdunek, 2006), MCF (Li et al., 2015), GMCF (Li et al., 2017b), DSNMF (Trigeorgis et al., 2015) and DSCF-Net (Zhang et al., 2019)), 3 single-layer MF models (i.e., DNMF (Shang et al., 2012), GCF (Ye & Jin, 2014) and SRMCF (Ma et al., 2018)], and four semi-supervised MF models [i.e., SemiGNMF (Cai et al., 2011a), CNMF (Liu et al., 2012), CCF (Liu et al., 2014) and RS2ACF (Zhang et al., 2020b)]. Note that SemiGNMF adds class information of labeled data into the graph structures by modifying the graph weight matrix (Cai et al., 2011a; Liu et al., 2014). In this study, 6 public image databases are involved, including two face image databases [i.e., AR (Bergstra et al., 2013) and MIT CBCL (Weyrauch et al., 2004)], two object image databases [i.e., COIL100 (Nayar et al., 1996) and ETH80 (Leibe & Schiele, 2003)], three handwritten image datasets [i.e., USPS (Hull, 1994), EMNIST Letters (Cormen, 2009) and EMNIST Digits (Cormen, 2009)], and one fashion products database [i.e., Fashion MNIST (Xiao et al., 2017)]. Some sample images of the evaluated datasets are shown in Fig. 3, and the detailed information about the used databases are described in Table 1, where we show the total number of samples, dimension and number of classes. For each face or object image, we follow the common procedures (He et al., 2017; Yang & Yang, 2002) to resize it into 32 × 32 pixels, forming a 1024-dimensional sample vector. Finally, we can obtain a data matrix with the vectorized representations of all the images as columns. The vectorized process for USPS, EMNIST Letters, EMNIST Digits and Fashion MNIST databases are similar. In this work, we normalize each column of the input data matrix to have unit norm for each database. We perform all experiments on a PC with Intel Core i5-4590 CPU @ 3.30 GHz 3.30 GHz 8G.

Fig. 3
figure 3
Sample images of the evaluated real image databases

Full size image
Table 1 List of evaluated databases and database information
Full size table
Visual Image Analysis by Visualization
Visualization of the Adaptive Weight Matrix SV
The obtained representation 𝑉𝑀=𝐴(𝑍0...𝑍𝑀) is the final output of our model, we first evaluate the representation ability of 𝑉𝑀 by visualizing the adaptive weights SV on 𝑉𝑀. AR face database and COIL100 object database are used in this study. For the AR database, we randomly choose 2 categories to construct the adjacency graph SV for clear observation, with 10 labeled images per class (that is, 20 labeled samples and 32 unlabeled samples in total). For COIL100 database, we randomly choose 4 categories to construct SV, with 28 labeled samples per category (i.e., 28 labeled and 44 unlabeled). The weight matrices SV are shown in Fig. 4, where we show the adaptive weights obtained by DS2CF-Net in the first four layers on each database. Note that the green box contains the weights on labeled data and the yellow box contains those on unlabeled data. We see that the weight matrices have approximate block-diagonal structures in each layer. Specifically, the structures of the adaptive weights get clearer with less noise and inter-class connections as the number of layers increases, which means that the learned new representation 𝑉𝑀 has a strong representation ability and moreover our deep model can potentially improve the similarity measure.

Fig. 4
figure 4
Visualization of the data weight matrix SV obtained by our DS2CF-Net in the first four layers over AR and COIL100 databases

Full size image
Visualization of the Structure Constraint Matrix Q
Since the structure constraint matrix Q determines the structures of the self-expressive coefficient matrix to encode the smoothness of manifolds in the process of label propagation, we also visualize its structures for observation. AR face database and COIL100 database are used. For AR, we randomly choose 2 categories for the test with 10 labeled images per class; For COIL100, we randomly choose 4 categories for the test, with 28 labeled samples per category. The structure constraint matrices Q are shown in Fig. 5, where we show the results obtained in the first four layers over each database, the green box and the yellow box contains the parts QL and QU on the labeled and unlabeled data, respectively. QL is defined according to the known labels, while QU is the cosine similarity defined based on the new representation of unlabeled samples. We find that the constraint matrix Q has a clear block-diagonal structure, and moreover the structures become better with the increasing number of layers, which implies that the structure constraint matrix Q in each layer has a strong discriminative representation power.

Fig. 5
figure 5
Visualization of the structure constraint matrix Q obtained by our DS2CF-Net in the first four layers over AR and COIL100 databases

Full size image
Convergence Analysis and Runtime Comparison
Convergence Analysis
The involved variables of our algorithm are optimized alternately in each layer, while the actual runtime performance of each iterative algorithm is closely related to the number of iterations in reality, so we would like to present some convergence analysis results. The MIT CBCL and Fashion MNIST databases, with 40% samples labeled, are used to train our method in this study. We show the convergence results of our DS2CF-Net in the first three layers in Fig. 6, where the X-axis shows the number of iterations and Y-axis denotes the difference between two consecutive basis vectors (i.e., 𝑊𝑡+1 and 𝑊𝑡) and two consecutive representations (i.e.,𝑉𝑡+1 and 𝑉𝑡) respectively, i.e.,‖‖𝑊𝑡+1−𝑊𝑡‖‖2𝐹 and ‖‖𝑉𝑡+1−𝑉𝑡‖‖2𝐹. We see that: (1) DS2CF-Net converges rapidly in each layer; (2) with the increasing number of layers, our DS2CF-Net converges more rapidly due to the effects of deep representation, which usually converges within 5 iterations in the 3rd layer. Note that as a multilayer model, this is beneficial for the efficiency.

Fig. 6
figure 6
Convergence analysis of our proposed DS2CF-Net algorithm on the MIT CBCL face database and Fashion MNIST database

Full size image
Runtime Comparison
In addition to presenting the computational time complexity analysis of our DS2CF-Net, we also would like to show the actual running time of each method (in second and averaged based on 10 runs) for the fair comparison. It is worth noting that we have compared 6 multi-layer methods, so to facilitate the comparison, we report the averaged runtime of different layers for these multi-layer models. In this study, we employ six databases, including AR, MIT CBCL, COIL100, USPS, EMNIST Letters, and ETH80 for evaluations. For each database, we randomly select 2, 5 and 8 categories to train each model. The runtime performance comparison results are given in Fig. 7. We see that: (1) the needed running time is increased with the increasing number of samples, i.e., from small-scale to large-scale; (2) DSNMF needs more time for AR, ETH80 and COIL100 databases, while needing less time for large-scale databases relatively, since it spends most of the running time in initialization. SRMCF needs the most time for several databases. CNMF, CCF and GCF also needs more time than other methods in most cases, especially on the large-scale databases. The main reason may be because they need more time for the convergence of the algorithm; (3) our proposed DS2CF-Net needs comparable time to RS2ACF based on each database, which spend slightly more time than other methods. Overall, in most cases the actual running time of our DS2CF-Net is acceptable due to fast convergence, although it is a multi-layer model.

Fig. 7
figure 7
Averaged runtime performance comparison of each method based on the six databases

Full size image
Quantitative Clustering Evaluation
Clustering Evaluation Process
For the quantitative clustering evaluations, we perform the K-means algorithm with cosine distance on the representation obtained by each model. Following the procedures in Liu et al. (2014) (Sugiyama, 2007), for each number K of clusters, we choose K categories from each database randomly and use the samples of K categories to form the data matrix X. The value of K is tuned from 2 to 10 in our study. The rank of the representation is set to K + 1 for clustering as (Liu et al., 2014). The clustering results are averaged based on 10 random selections of the K categories. For fair comparison, we choose 40% labeled samples per class for each semi-supervised algorithm (i.e., SemiGNMF, CNMF, CCF, RS2ACF and our DS2CF-Net). For fair comparison to the existing multi-layer matrix factorization methods (i.e., MNMF, MCF, GMCF, DSNMF and DSCF-Net), we report the highest clustering scores in their first 10 layers of each method, rather than fixing the number of layers.

Clustering Evaluation Metric
We employ two widely-used evaluation methods, i.e., Accuracy (AC) and F-measure (Cai et al., 2005; He et al., 2006). AC is the percentage of the cluster labels to the true labels provided by the original data corpus, defined as follows:

𝐴𝐶=[∑𝑁𝑖=1𝛿(𝑟𝑖,𝑚𝑎𝑝(𝑝𝑖))]/𝑁,
(29)
where N is the number of samples, and the function 𝑚𝑎𝑝(𝑝𝑖) is the permutation mapping function that maps the cluster label 𝑝𝑖 obtained by the clustering method to the true label 𝑟𝑖 provided by the data corpus, and the best mapping can be obtained by the Kuhn-Munkres algorithm (Lovasz & Plummer, 1986) according to Cai et al. (2005). The clustering F-measure is defined as follows:

𝐹𝜇=(𝜇2+1)𝑃𝑅𝐸𝐶𝐼𝑆𝐼𝑂𝑁×𝑅𝐸𝐶𝐴𝐿𝐿𝜇2𝑃𝑅𝐸𝐶𝐼𝑆𝐼𝑂𝑁+𝑅𝐸𝐶𝐴𝐿𝐿,
(30)
where we set the parameter 𝜇=1. Note that both values of the AC and F-measure range from 0 to 1, i.e., the higher the value is, the better the clustering result will be.

Clustering Evaluation Results
Face Clustering
We first use face images to evaluate the clustering ability of learned representation by each method. AR and MIT CBCL face database are evaluated. The clustering performance in terms of AC and F-measure over varied K numbers is tested. The clustering curves on the AR and MIT CBCL databases are shown in Fig. 8a, b, respectively. The averaged AC and F-scores according to the curves in Fig. 8a, b are summarized in Tables 2 and 3, respectively. We can see that: (1) the obtained AC and F-measure of each method go down as the number of categories is increased, which is easy to understand, since clustering data of less categories is relatively easier than clustering more categories; (2) our DS2CF-Net delivers higher values of AC and F-measure than other compared methods in the investigated cases. Both RS2ACF and DSCF-Net performs better than other remaining methods in most cases. CNMF also delivers promising results over the MIT CBCL database.

Fig. 8
figure 8
Clustering performance over varied K values based on the eight evaluated databases

Full size image
Table 2 Averaged clustering accuracies (AC) of the algorithms based on the eight evaluated image databases
Full size table
Table 3 Averaged F-score values of the algorithms based on the eight evaluated real databases
Full size table
Object Clustering
We then evaluate each method for representing and clustering the object image data. In this experiment, COIL100 and ETH80 object databases are evaluated. The clustering curves on ETH80 and COIL100 databases are shown in Fig. 8c, d, respectively. The averaged AC and F-scores according to the curves in Fig. 8c, d are summarized in Tables 2 and 3, respectively. We can see that the increasing number of selected categories clearly decreases the performance of each method due to the fact that clustering data of less categories is relatively easier. It can also be found that DS2CF-Net delivers higher values of AC and F-measure than other evaluated methods in most cases. In addition, the semi-supervised SemiGNMF, CNMF, CCF and RS2ACF and methods can perform better than other algorithms using partially labelled data, where RS2ACF is the best method in this case. DSCF-Net also delivers relatively better clustering performance than other multilayer methods.

Handwritten Digit Clustering
We also examine the performance of each method for clustering the handwritten digits of the USPS, EMNIST Letters and EMNIST Digits databases. For USPS, we train each model based on the first 3000 samples. For EMNIST Digits database, we randomly choose 10,000 samples per class to form training set due to memory limit. The clustering curves under different numbers of the selected categories on USPS database are shown in Fig. 8e–g, and Tables 2 and 3 describe the averaged AC and F-scores according to the curves in Fig. 8e–g. Similar observations can be found from the results. That is, the AC and F-measure of each algorithm go down as the number of categories increases. It can also be found that SemiGNMF, CNMF and RS2ACF deliver promising clustering results by using partial labeled data. Among the multilayer MF models, DSCF-Net obtains relatively better performance than other methods. By further enriching the supervised prior by predicting the labels of unlabeled data, and designing a more reasonable dual-constrained deep structures, our DS2CF-Net outperforms all its competitors by delivering better results.

Fashion Products Clustering
Finally, we test each method for representing the fashion product images of Fashion MNIST database. We train each model by a subset of Fashion MNIST, i.e., totally 10,000 samples from 10 classes. The clustering results in terms of AC and F-measure are evaluated and shown in Fig. 8h. Tables 2 and 3 describe the statistics in terms of averaged AC and F-scores according to Fig. 8h. From the results, we can similarly see that: (1) our DS2CF-Net delivers enhanced performance than other competitors in most cases, especially when the number of K is relatively small. We also find that semi-supervised methods can generally deliver enhanced performance than unsupervised ones. But note that SRMCF also obtains the promising results, which implies that the self-expression property is also important to improve the representation ability. It should be noted that our DS2CF-Net also employs the self-expression scheme in the proposed multilayer structures. In addition, one can also find that the results of the multilayer MNMF, MCF and GMCF models are worse than those of the single-layer models, which verifies that their multilayer structures of directly feeding the learnt representation from the last layer to the next layer is indeed not reasonable.

Ablation Study
Clustering with Different Proportions of Labeled Samples
We first evaluate each semi-supervised factorization model, i.e., CNMF, CCF, SemiGNMF, RS2ACF and our DS2CF-Net, by using different numbers of labeled data in each class. In this study, for each database the proportion of labeled samples varies from 10 to 90%, and we randomly choose three categories for this test. We average the results over 10 random selections of categories and 30 initializations for the K-means clustering for each MF approach to avoid the randomness. For comparison, we also report the clustering results of four representative unsupervised methods (i.e., MCF, DSNMF, GMCF, and DSCF-Net) as baselines using the flat dashed lines. The clustering results based on the evaluated databases are reported in Fig. 9. We see that: (1) the increasing number of labeled samples can greatly improve the clustering performance of each method. It can also be found that the improvement by our DS2CF-Net over other compared methods is more obvious, especially when the proportion of label data is relatively small; (2) our DS2CF-Net delivers better results across different labeled proportions by fully mining the intrinsic relations between the labeled and unlabeled data, and predicting the labels of unlabeled samples to enrich the supervised prior knowledge. RS2ACF also performs well by delivering better results than other remaining methods.

Fig. 9
figure 9
Clustering accuracies versus varied proportions of labeled samples based on the evaluated image databases

Full size image
Further Discussion Between Semi-supervised and Unsupervised CF-Based Clustering
As shown in Fig. 9, unsupervised methods (such as DSCF-Net) obtain better results than DS2CF-Net when the proportion of labeled samples is low (such as 10%), especially on the AR, ETH80, COIL100 and USPS datasets. The major reasons are twofold. First, although DSCF-Net is an unsupervised method and cannot preserve the locality in feature space compared with DS2CF-Net, it clearly incorporates the noise removal process in its model. That is, DSCF-Net performs the factorization in the noise-removed clean data space, while DS2CF-Net performs in the original space; Second, even though the formulation of our DS2CF-Net looks like that of DSCF-Net when DS2CF-Net does not exploit labeled data, note that there are several obvious differences between them: (1) although they both are deep matrix factorization methods, their factorization mechanisms are totally different. Specifically, DSCF-Net uses a single-channel mode, which optimizes the basis vectors to update the representation matrix indirectly in each layer. While DS2CF-Net designs a two-channel factorization model, which can jointly update the basis vectors and representation matrix in each layer; (2) DSCF-Net can only retain the local information of the data space, while our DS2CF-Net can preserve the local manifold structures of both the data space and feature space. Therefore, DSCF-Net and DS2CF-Net are totally different two methods. In other words, DS2CF-Net is not a simple extension of DSCF-Net. As a result, DSCF-Net has a potential to outperform DS2CF-Net in some cases, for example when the proportion of labeled samples is relatively low.

Clustering with Different Numbers of Layers
We investigate the effects of the number of layers on the representation learning and clustering abilities of each multilayer model, including MNMF, MCF, GMCF, DSNMF, DSCF-Net and our DS2CF-Net. In this simulation, we vary the number of layers from 1 to 10 with step 1. For each database, we randomly choose 3 categories for the clustering evaluations. The averaged clustering ACs are illustrated in Fig. 10, from which we see that: (1) our DS2CF-Net delivers the highest accuracies than other methods in most cases; (2) the increase of the number of layers can generally improve the clustering results, which implies that discovering hidden deep features can indeed improve the performance. However, the clustering results of MNMF, DSNMF, MCF and GMCF go down apparently when the number of layers passes 4 in most cases, which maybe because MNMF, MCF and GMCF cannot ensure the intermediate representation from the previous layer to be a good representation for subsequent layers. Note that the first stage of DSNMF is performed similarly as MNMF, MCF and GMCF, thus it also suffers from the degrading issue as the number of layers is increased to a high level. This observation can once again show that the multilayer structures of directly feeding the learnt representation from the last layer to the next layer is not reasonable. By updating the basis vectors to optimize the low-dimensional representation in each layer, DSCF-Net also perform well by delivering higher and more stable results than the other remaining methods, i.e., MNMF, DSNMF, MCF and GMCF. By this analysis, we can choose a proper number of layers for each multilayer MF model for the representation and clustering tasks in the experiments.

Fig. 10
figure 10
Clustering accuracies versus varied number of layers based on the evaluated image databases

Full size image
Fig. 11
figure 11
Clustering accuracies of GMCF and SemiGNMF under various parameters over AR database and COIL100 database

Full size image
Hyperparameter Sensitivity Analysis
We investigate the effects of the hyper-parameters of each compared method on the representation and clustering abilities. For the compared methods, five methods (MNMF, MCF, DSNMF, CNMF and CCF) have no hyper-parameter in their models; two methods (GMCF and SemiGNMF) have one hyper-parameter 𝛼 in their models; three methods (DNMF, GCF and SRMCF) contain two hyper-parameters 𝛼 and 𝛽 in their models; last two algorithms (DSCF-Net and RS2ACF), have three hyper-parameters 𝛼, 𝛽 and 𝛾 in their models. Note that we uniformly use 𝛼, 𝛽 and 𝛾 to represent the first, the second and the third hyper-parameters of each method if have, and all hyper-parameters will be selected from the same candidate set {10–5, 10–4, …, 105} for fair comparison. Specifically, for the method with only one parameter 𝛼, we directly tune it from the candidate set and evaluate the performance. For the methods with two parameters 𝛼 and 𝛽, we adopt the commonly-used grid search strategy (Ren et al., 2019; Zhang et al., 2016) to tune them from the candidate set. For the methods with three hyper-parameters, we first fix 𝛾=1 and tune 𝛼 and 𝛽 using the grid search strategy, and then fix the selected 𝛼 and 𝛽 to tune 𝛾. For RS2ACF, a fixed parameter setting, i.e., 𝛼= 104,𝛽= 10–4 and 𝛾= 104, was provided in Zhang et al., 2020b, so we directly use this setting in our experiments. For each database, we choose the samples of three categories to train each method, set the number of layers is set to 3 for each multi-layer method and the proportion of labeled data is still set to 40% for each semi-supervised method. The results are averaged over 30 random initializations of the cluster centers for the K-means clustering algorithm. The hyper-parameters sensitivity analysis results of each method over AR and COIL100 databases are displayed in Figs. 11, 12 and 13 as examples. Finally, we report the best choice of the hyperparameters of each method in Table 4, which have also been used in the clustering evaluations of this paper.

Fig. 12
figure 12
Clustering accuracies of DNMF, GCF and SRMCF under various parameters over AR database and COIL100 database

Full size image
Fig. 13
figure 13
Clustering accuracies of DSCF-Net and DS2CF-Net under various parameters over AR database and COIL100 database

Full size image
Table 4 Settings of parameters for each algorihtm based on the evaluated image databases
Full size table
Concluding Remarks
We proposed a new enriched prior knowledge guided dual-constrained deep semi-supervised coupled factorization model for discovering hierarchical information. To capture hidden deep information, our DS2CF-Net designs a joint label and structure-constrained factorization network using multiple layers of linear transformations of basis vectors and representations. An error correction mechanism with a feature fusion strategy is also integrated between consecutive layers to improve the representation.

To improve the discrimination of deep representation and coefficients, DS2CF-Net clearly considers enriching the supervised prior knowledge by the joint deep coefficients-regularized label prediction, and incorporates enriched prior information as additional label and structure constraints. Moreover, DS2CF-Net also proposes to keep the locality structures in both the data and feature spaces by adopting an adaptive dual-graph weighting strategy. A fine-tuning process is finally included to refine the structure-constrained matrix and the data weight matrix in each layer using the predicted labels for more accurate representations.

We have evaluated our DS2CF-Net for image representation and clustering, and the results are compared with several related single-layer and multilayer frameworks. Both the visual image analysis and quantitative clustering evaluation demonstrate the effectiveness of our framework. In future, we will evaluate our method for the other related application areas, such as document retrieval and recommended system. More efficient coupled factorization strategy will also be investigated for the consideration of scalability. In addition, we will explore how to integrate the factorization model with the deep convolutional neural network for handling the large-scale vision tasks.

