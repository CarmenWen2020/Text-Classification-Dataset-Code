In this paper, a squeeze-and-decomposition network (SDNet) is proposed to realize multi-modal and digital photography image fusion in real time. Firstly, we generally transform multiple fusion problems into the extraction and reconstruction of gradient and intensity information, and design a universal form of loss function accordingly, which is composed of intensity term and gradient term. For the gradient term, we introduce an adaptive decision block to decide the optimization target of the gradient distribution according to the texture richness at the pixel scale, so as to guide the fused image to contain richer texture details. For the intensity term, we adjust the weight of each intensity loss term to change the proportion of intensity information from different images, so that it can be adapted to multiple image fusion tasks. Secondly, we introduce the idea of squeeze and decomposition into image fusion. Specifically, we consider not only the squeeze process from source images to the fused result, but also the decomposition process from the fused result to source images. Because the quality of decomposed images directly depends on the fused result, it can force the fused result to contain more scene details. Experimental results demonstrate the superiority of our method over the state-of-the-arts in terms of subjective visual effect and quantitative metrics in a variety of fusion tasks. Moreover, our method is much faster than the state-of-the-arts, which can deal with real-time fusion tasks.

Access provided by University of Auckland Library

Introduction
Due to the limitation of hardware devices and optical imaging, an image obtained by a single sensor or under a single shooting setting can often capture only part of the details in the scene. For example, the image generated by capturing visible light usually only withstands a limited illumination variation and has a predefined depth-of-field. In addition, it is also susceptible to external factors such as the weather when shooting. Naturally, image fusion can extract the most meaningful information from images acquired by different sensors or under different shooting settings, and combine the information to generate a single image, which contains more texture content (Piella 2009). Because of the excellent performance of the fused image, image fusion as an enhancement method is widely used in many fields such as military detection, medical diagnosis, and remote sensing (Ma et al. 2017; Xing et al. 2018; Ma et al. 2021; Kong et al. 2007; Shen et al. 2015).

Fig. 1
figure 1
Schematic illustration of multi-modal image fusion and digital photography image fusion. First row: source image pairs to be fused; second row: fused results of our proposed method (SDNet)

Full size image
Typically, image fusion scenarios can be divided into two categories according to the differences in imaging of source images. The first category is multi-modal image fusion. Due to factors such as imaging environment or device performance, sometimes a single sensor cannot effectively describe the entire scene. Combining multiple sensors for observation is a better choice. For example, the positron emission tomography (PET) can produce images that reflect the metabolic state of the body, while the magnetic resonance imaging (MRI) can provide excellent structure textures of organs and tissues (Liu et al. 2017). The infrared image can distinguish the target from background, while the visible image contains more texture details (Ma et al. 2016). The second category is digital photography image fusion. Due to the limit of technology, the sensor is often unable to characterize all content in the scenario under a single setting. Concretely, it is difficult to have all objects of different depth-of-field to be all-in-focus within one image (Ma et al. 2020). Besides, the image is sometimes exposed to inappropriate exposures such as underexposure and overexposure (Hayat and Imran 2019; Goshtasby 2005). Under these circumstances, the scene can be described more comprehensively by combining with images under different shooting settings. A few examples are provided in Fig. 1 to illustrate these two types of image fusion scenarios more intuitively.

In recent years, researchers have proposed a number of methods to solve the image fusion problem, which can be broadly divided into two categories. The first category is traditional image fusion method, which usually uses the relevant mathematical transformation to realize the fusion by designing the activity level measurement and fusion rules in the spatial domain or the transform domain (Li et al. 2012; Zhao et al. 2019; Shen et al. 2014; Paul et al. 2016; Ballester et al. 2006; Szeliski et al. 2011). The second category is deep learning-based methods. Methods of this type usually constrain the fused image by constructing an objective function to make it have the desired distribution characteristics. Because of the strong nonlinear fitting ability of neural networks, this kind of methods can usually achieve better fused results (Ma et al. 2019; Prabhakar et al. 2017; Liu et al. 2017; Lai and Fang 1998).

Although the existing methods have achieved promising results in most cases, there are still several aspects to be improved. First, the existing traditional methods usually need to manually design the activity level measurement and fusion rules, which become complex because of the diversity of source images. This also limits fusion results because it is impossible to consider all the factors in one manually designed way. Second, the most prominent obstacle in applying deep learning to image fusion is the lack of the ground-truth fused image for supervised learning. A few methods solve this difficulty by manually constructing the ground truth, which is usually inaccurate and will set an upper limit for learning. Third, as mentioned earlier, there are large differences between image fusion tasks. In multi-modal image fusion, source images are captured by different sensors. Conversely, source images in digital photography image fusion are taken by the same sensor under different shooting settings. As a result, the existing methods cannot solve different image fusion problems according to the same idea. Finally, the existing methods are usually less competitive in operating efficiency due to the large number of parameters or the high complexity of fusion rules.

To address the above mentioned challenges, we design a squeeze-and-decomposition network, called SDNet, to implement multi-modal image fusion and digital photography image fusion end-to-end in real time. Our design is mainly developed from the following two aspects.

On the one hand, we model the multi-modal image fusion and the digital photography image fusion as the extraction and reconstruction of intensity and gradient information. Our opinion is that the information contained in the image can be divided into gradient and intensity information, wherein the gradient information represents the texture structure, while the intensity information indicates the overall brightness distribution of the image. Based on this idea, we design a loss function in a universal form for the above two types of image fusion scenarios, which can force the network to extract the gradient and intensity information and fuse them by two different rules. Specifically, for the gradient information, we believe that in addition to noise, other areas with strong gradients are clear or have a large amount of texture content. Based on this observation, we propose an adaptive decision block, which firstly uses the Gaussian low-pass filter to reduce the effects of noise, and then scores each pixel based on the level of the gradient, thereby directing the gradient distribution of the fused image to approximate the source pixel with larger gradient strength. For the intensity information, because different fusion tasks have different preferences for intensity information preservation, we select more effective and interesting intensity information to be preserved in the fused result by adjusting the weight proportion of each intensity loss item. By using these two strategies to extract and reconstruct gradient and intensity information, the proposed loss function can be well adopted in multi-modal image fusion and digital photography image fusion.

On the other hand, we propose a fast SDNet to implement more effective image fusion. The previous methods only consider the squeeze process from the source image to the fusion result, then whether the fused result can be decomposed to regenerate source images? Although part of the information will inevitably be discarded in the fusion process, requiring the decomposition result to be consistent with the source images will reduce the information loss as much as possible. In other words, this decomposition consistency will force the fused result to contain more scene details, because the quality of the decomposition result directly depends on the fused result. Based on this motivation, we design a squeeze-and-decomposition network, which contains two parts: squeeze and decomposition. In the squeeze stage, the source images are fused into a single image. While in the decomposition stage, the fused result is re-decomposed into source images. Similarly, this squeeze-and-decomposition network is also suitable for both the multi-modal and digital photography image fusion.

Our method has the following advantages. First of all, our method does not need to design the activity level measurement and fusion rules, which can implement end-to-end fusion. Second, our network does not require ground truth for supervised learning, but unsupervised learning with weak constraints. Third, our method is not only applicable to fusion of images obtained by multi-modality imaging, but also to fusion of images obtained by digital photography. It is worth noting that due to the use of 1Ã—1 convolution kernels and the control of the number of feature channels, the quantity of parameters in our network is limited within a certain range. As a result, our method can achieve fusion at a high speed.

Our contributions include the following five aspects:

We propose a new end-to-end image fusion model, which can realize the multi-modal image fusion and the digital photography image fusion well.

We design a specific form of loss function, which can force the network to generate expected fused results.

We propose an adaptive decision block for the gradient loss terms, which can reduce the effect of noise and effectively guide the fused result to contain richer texture details.

We design a squeeze-and-decomposition network, which can focus on the two stages of fusion and decomposition at the same time, so as to make the fused result contain more scene details.

Our method can perform image fusion in real time for multiple fusion tasks. The code is publicly available at: https://github.com/HaoZhang1018/SDNet.

A preliminary version of this manuscript appeared in Zhang et al. (2020). The primary new contributions include the following two aspects. First, we design an adaptive decision block to constrain the gradient information instead of the previous manually proportioned setting strategy. On the one hand, it reduces the number of super-parameters that need to be manually adjusted. On the other hand, it makes our method perform better, especially in the multi-focus image fusion. Second, we have further improved the network, which considers not only the fusion process but also the decomposition process. This decomposition consistency can make the fused image contain more scene details and thus have a better visual effect.

The remainder of this paper is organized as follows. Section 2 describes some related work, including an overview of existing traditional and deep learning-based fusion methods. Section 3 provides the overall framework, loss functions and network architecture design. In Sect. 4, we give the detailed experimental settings and compare our method with several state-of-the-art methods on publicly available datasets by qualitative and quantitative comparisons. In addition, we also carry out comparative experiments of efficiency, ablation experiments, visualization of decomposition, infrared and RGB visible image fusion, sequence image fusion, comparison with preliminary version, and application verification (Zhang et al. 2020) in this section. Conclusions are given in Sect. 5.

Related Work
With various methods proposed, the field of image fusion has made great progress. Existing methods can be broadly divided into traditional and deep learning-based methods.

The traditional methods usually use the related mathematical transformation and manual design of the fusion rules to realize the image fusion. Piella (2003) presented an overview on image fusion techniques using multi-resolution decompositions, which make a multi-resolution segmentation based on all different input images, and this segmentation is subsequently used to guide the infrared and visible image fusion process. A ghost-free multi-exposure image fusion technique using the dense SIFT descriptor and guided filter was proposed by Hayat and Imran (2019), which can produce high-quality images without the artifacts using ordinary cameras. Paul et al. (2016) proposed a general algorithm for multi-focus and multi-exposure image fusion, which is based on blending the gradients of the luminance components of the input images using the maximum gradient magnitude at each pixel location and then obtaining the fused luminance using a Haar wavelet-based image reconstruction technique. Fu et al. (2019) introduced a more accurate spatial preservation based on local gradient constraints into remote sensing image fusion, which can fully utilize spatial information contained in the PAN image while maintaining spectral information. As a result, they can obtain very promising fused results.

Compared with traditional methods, deep learning-based methods can learn fusion models with good generalization ability from a large amount of data. In the field of infrared and visible image fusion, Ma et al. (2019) proposed an end-to-end model called FusionGAN, which generates a fused image with a dominant infrared intensity and an additional visible gradient on the basis of GAN. Subsequently, they introduced a dual-discriminator (Ma et al. 2020), a detail loss and a target edge-enhancement loss (Ma et al. 2020) based on FusionGAN to further enhance the texture details in the fused results. In the field of multi-exposure image fusion, Prabhakar et al. (2017) proposed an unsupervised deep learning framework that utilizes a no-reference quality metric as a loss function and can produce satisfactory fusion results. Xu et al. (2020) introduced an end-to-end architecture based on GAN with self-attention mechanism and has achieved the state-of-the-art performance. In medical image fusion, Liu et al. (2017) used a neural network to generate the weight map that integrates pixel activity levels of two source images, while the fusion process is conducted in a multi-scale manner via image pyramids. With the application of deep learning, great progress also has been made in the field of multi-focus image fusion. In particular, Ma et al. (2020) proposed an unsupervised network to generate the decision map for fusion, which can indicate whether the pixel is focused. Deep network has also promoted the progress of remote sensing image fusion. Zhou et al. (2019) designed a deep model composed of encoder network and pyramid fusion network to fuse the low-resolution hyperspectral and high-resolution multi-spectral images, which improves the preservation of spatial information by this progressive refinement. Ma et al. (2020) proposed an unsupervised deep model for pansharpening to make full use of the texture structure in panchromatic images. They transformed pansharpening into multi-task learning by using two independent discriminators, which preserve spectral and spatial information well. Our preliminary version PMGI (Zhang et al. 2020) proposed a new image fusion network based on proportional maintenance of gradient and intensity information, which can realize a variety of image fusion tasks. However, changing the maintenance proportion of the gradient information by adjusting the weight will cause a certain degree of the texture structure loss or blur, thereby reducing the quality of the fused result. Therefore, in this paper we improve PMGI to realize better fusion performance.

Method
In this section, we give a detailed introduction of our SDNet. We first introduce the overall framework, and then give the definition of the loss function. Finally, we provide the detailed structure of the network. Note that the source images are assumed to be pre-registered in our method (Ma et al. 2021).

Overall Framework
The idea of image fusion is to extract and combine the most meaningful information from the source images. On the one hand, for different image fusion tasks, the most meaningful information contained in source images is different. Because there is no same standard for such meaningful information, existing methods are usually difficult to be migrated to other fusion tasks. Thus, it is desirable to develop a versatile model to fulfill multiple types of image fusion tasks. On the other hand, it is very important to preserve as much information in source images as possible in the fused image. Our method is designed based on the above two observations, which is an end-to-end model.

Firstly, we define the meaningful information into two categories: gradient and intensity information. For any image, the most essential element of it is the pixel. The intensity of pixels can represent the overall brightness distribution, which can reflect contrast characteristics of the image. The difference between the pixels constitutes the gradient, which can represent the texture details in the image. Therefore, the multi-modal image fusion and digital photography image fusion can be model as the extraction and reconstruction of these two kinds of information, as shown in Fig. 2. The extraction and reconstruction of gradient and intensity information is dependent on the design of the loss function. In our model, we propose a universal loss function for different image fusion tasks, which consists of the gradient loss term and the intensity loss term constructed between the fused image and both two source images. Although using intensity loss and gradient loss (Ma et al. 2019, 2020) is a common practice in a specific image fusion task (Szeliski et al. 2011), it is non-trivial to extend them to other image fusion tasks. To this end, the reconstruction rules we design for gradient information and intensity information are greatly different. For gradient information reconstruction, we introduce an adaptive decision block that acts on the gradient loss term. The adaptive decision block first uses the Gaussian low-pass filtering to reduce the influence of noise on the decision-making process, and then evaluates the importance of corresponding pixels based on the gradient richness, so as to generate the pixel-scale decision map that guides textures in the fused image to approximate that in the source pixels with richer textures. As a result, the texture details contained in the fused image are consistent with the texture details that are strongest in the corresponding regions of source images. For the intensity information reconstruction, we adopt the proportioned setting strategy. Specifically, we adjust the weight ratio of the intensity loss items between the fused image and two source images, so as to satisfy the requirements of different tasks on intensity distribution. For example, in the infrared and visible image fusion, the intensity distribution of the fused image should be more biased towards the infrared image to maintain significant contrast. In the MRI and PET image fusion, the intensity distribution of the fused image should be more inclined to the PET image, so as to preserve the functional and metabolic information of biological tissues. The adaptive decision block and the proportioned setting strategy will be introduced in detail in Sect. 3.2.

Secondly, we propose the idea of squeezing and decomposing to preserve as much information in source images as possible in the fused result. Concretely, the proposed SDNet is composed of two parts: the squeeze network and the decomposition network, as shown in Fig. 2. The squeeze network is the target network for realizing image fusion, which is dedicated to squeezing source images into a single fused image. In contrast, the decomposition network is dedicated to decomposing this fused result to obtain images consistent with source images. On the whole, SDNet is very similar to the auto-encoder network. The difference is that the intermediate result of SDNet is the fused image, while the intermediate result of the auto-encoder network is the encoding vector. But the same is that they both require the intermediate result to contain more scene content, which is conducive to reconstructing source images. Therefore, the squeeze-and-decomposition network we design can force the fused result to contain richer scene details, and thus have a better fusion effect.

Fig. 2
figure 2
Overall fusion framework of our SDNet

Full size image
Loss Functions
Our SDNet is divided into two parts, where the squeeze network generates a single fused image through the extraction and reconstruction of intensity and gradient information, and the decomposition network is dedicated to decomposing results that approximates source images from the fused result. Correspondingly, the loss function also consists of two parts: squeeze fusion loss îˆ¸sf and decomposition consistency loss îˆ¸dc, which is defined as:

îˆ¸=îˆ¸sf+îˆ¸dc.
(1)
Squeeze Fusion Loss
The squeeze fusion loss îˆ¸sf determines the type of information extracted and the primary and secondary relationships between various types of information in reconstruction. Because our method is based on the extraction and reconstruction of gradient and intensity information, so our loss function consists of two types of loss terms, the gradient loss îˆ¸grad and the intensity loss îˆ¸int. We formalize it as:

îˆ¸sf=ğ›½îˆ¸grad+îˆ¸int,
(2)
where ğ›½ is used to balance the intensity and gradient terms.

The gradient loss îˆ¸grad forces the fused image to contain rich texture detail. We introduce an adaptive decision block into the gradient loss term to guide the texture of fused image to be consistent with the strongest texture in the corresponding position of source images, which is defined as:

îˆ¸grad=1ğ»ğ‘Šâˆ‘ğ‘–âˆ‘ğ‘—ğ‘†1ğ‘–,ğ‘—â‹…(âˆ‡ğ¼fusedğ‘–,ğ‘—âˆ’âˆ‡ğ¼1ğ‘–,ğ‘—)2+ğ‘†2ğ‘–,ğ‘—â‹…(âˆ‡ğ¼fusedğ‘–,ğ‘—âˆ’âˆ‡ğ¼2ğ‘–,ğ‘—)2,
(3)
where i and j represent the pixel in the i-th row and the j-th column in the decision maps or gradient maps, H and W represent the height and width of the image, ğ¼1 and ğ¼2 are the source images, ğ¼fused is the fused image, and âˆ‡(â‹…) represents the operation of finding the gradient map using the Laplacian operator. In addition, ğ‘†(â‹…) is the decision map generated by the decision block based on the gradient level of source images. The schematic diagram of the adaptive decision block is shown in Fig. 3. In order to reduce the influence of noise on the gradient judgment, the decision block first performs a Gaussian low-pass filter on source images. Then, we find the gradient maps using the Laplacian, and the decision maps are generated at the pixel scale according to the magnitude of the gradient. The entire generation process of the decision diagram can be formalized as:

ğ‘†1ğ‘–,ğ‘—=sign(âˆ£âˆ£âˆ‡(ğ¿(ğ¼1ğ‘–,ğ‘—))âˆ£âˆ£âˆ’min(âˆ£âˆ£âˆ‡(ğ¿(ğ¼1ğ‘–,ğ‘—))âˆ£âˆ£,âˆ£âˆ£âˆ‡(ğ¿(ğ¼2ğ‘–,ğ‘—))âˆ£âˆ£)),
(4)
ğ‘†2ğ‘–,ğ‘—=1âˆ’ğ‘†1ğ‘–,ğ‘—,
(5)
where |â‹…| indicates the absolute value function, âˆ‡(â‹…) is the Laplacian operator, ğ¿(â‹…) denotes the Gaussian low-pass filter function, min(â‹…) denotes a minimum function, and sign(â‹…) is the sign function. It is worth noting that the size of ğ‘†(â‹…) is also ğ»Ã—ğ‘Š. Since the two source images are both filtered with the low pass function and the pixels with large gradient values are selected, so the normal texture is hardly misjudged. The similar idea of choice decision is mentioned in Goshtasby (2005). However, our adaptive decision block is obviously more advanced for the following reasons. First, the weight function used in Goshtasby (2005) is on the patch scale, while the proposed adaptive decision block is on the pixel scale. From this perspective, our adaptive decision block is more refined. Second, the weight function in Goshtasby (2005) is based on the information entropy, which is not robust to noise. Specifically, when a patch contains more noise, it will be unreasonably given a greater weight instead. In contrast, the proposed adaptive decision block not only considers the influence of noise, but also makes decisions based on the scene texture richness, which is more reasonable. Third, the weight function in Goshtasby (2005) is directly applied to the source images, which is essentially a linear mapping from the source images to the fused image. The difference is that our adaptive decision block acts on the gradient loss function to guide the fused image to preserve rich textures on the pixel scale in an optimized manner, which is actually a non-linear mapping with better performance.

Fig. 3
figure 3
Schematic diagram of the adaptive decision block

Full size image
The intensity loss îˆ¸int guides the fused image to preserve useful information that is represented by the pixel intensity, such as the contrast. Meanwhile, it can make the overall scene style of the fused image more natural and not divorced from reality. The intensity loss can be formalized:

îˆ¸int=1ğ»ğ‘Šâˆ‘ğ‘–âˆ‘ğ‘—(ğ¼fusedğ‘–,ğ‘—âˆ’ğ¼1ğ‘–,ğ‘—)2+ğ›¼(ğ¼fusedğ‘–,ğ‘—âˆ’ğ¼2ğ‘–,ğ‘—)2.
(6)
Because different image fusion tasks have different propensity requirements for the intensity distribution of the fused image. Thus, we adopt the proportioned setting strategy to adjust ğ›¼, so as to satisfy the intensity distribution requirements of different types of fusion tasks. The proportioned setting strategy of ğ›¼ are related to the type of image fusion, which are summarized below.

For multi-modal image fusion, the intensity distribution of the fused result is often biased to a certain source image. For instance, in the infrared and visible image fusion, the main intensity information should be obtained from the infrared image, so as to retain significant contrast. Similarly, for the MRI and PET image fusion, the main intensity information should be obtained from the PET image, thus preserving the functional activity information of the organism. Therefore, the parameter ğ›¼ should meet the following setting rules:

ğ¼1=ğ¼ir, ğ¼2=ğ¼vis, ğ›¼<1.
(7)
ğ¼1=ğ¼PET, ğ¼2=ğ¼MRI, ğ›¼<1.
(8)
Fig. 4
figure 4
Network architecture of the proposed SDNet

Full size image
For digital photography image fusion, the scene content of source images captured under different shooting settings is often highly complementary, and the intensity information of the fused image should come from all source images uniformly. For example, in multi-exposure image fusion task, both overexposed and underexposed images contain texture details, but their intensity is too strong or too weak. Therefore, the same weights should be set to balance them to get the right intensity. Similarly, for multi-focus image fusion, source images contain complementary textures, and their intensity is equally important. So the parameter ğ›¼ should meet the following setting rules:

ğ¼1=ğ¼over, ğ¼2=ğ¼under, ğ›¼=1.
(9)
ğ¼1=ğ¼focus1, ğ¼2=ğ¼focus2, ğ›¼=1.
(10)
Decomposition Consistency Loss
The decomposition consistency loss îˆ¸dc requires the decomposition results from the fused image to be as similar as possible to source images, which is defined as:

îˆ¸dc=1ğ»ğ‘Šâˆ‘ğ‘–âˆ‘ğ‘—(ğ¼1_değ‘–,ğ‘—âˆ’ğ¼1ğ‘–,ğ‘—)2+(ğ¼2_değ‘–,ğ‘—âˆ’ğ¼2ğ‘–,ğ‘—)2,
(11)
in which ğ¼1_de and ğ¼2_de are results of decomposition from the fused image, and ğ¼1 and ğ¼2 are the source images. Because the degree of similarity between the decomposition results and the source images directly depends on the quality of the fused image, the decomposition consistency loss can force the fused result to contain more scene content, so as to achieve a better fusion performance.

Network Architecture
The proposed SDNet is composed of two sub-networks, i.e., squeeze network and decomposition network, as shown in Fig. 4.

The purpose of the squeeze network is to fuse source images into a single image that contains richer texture content. The nature of the source image pairs tends to be quite different, some of which are captured by different sensors, and some are shot with the same sensor at different shooting settings. As a result, it is a better choice to handle them separately. Because the pseudo-siamese network is very good at processing data with large differences, we design the squeeze network with reference to the pseudo-siamese network, to realize a variety of image fusion tasks. At the same time, as there is information loss caused by padding in the convolution process, we use dense connections like DenseNet (Huang et al. 2017) to reduce information loss and maximize the use of information. In each path, we use four convolutional layers for feature extraction. The first layer uses a 5Ã—5 convolution kernel and the latter three layers use a 3Ã—3 convolution kernel, all with the leaky ReLU activation function. Then, we fuse the features extracted from the two paths, and use the strategies of concatenating and convolution to achieve this purpose. We concatenate the two feature maps along the channel. The kernel size of the last convolutional layer is 1Ã—1, and the activation function is tanh.

The decomposition network is dedicated to decomposing the fused image to obtain results similar to source images. We first use one common convolutional layer to extract features from the fused image. Then, we implement decomposition from two branches to generate results, where each branch contains three convolutional layers. The first common convolutional layer uses the convolution kernel with a size of 1Ã—1, and the remaining convolutional layers all use the convolution kernel with a size of 3Ã—3. Except for the last convolutional layer, all other convolutional layers use the leaky ReLU as the activation function, and the last uses the tanh as the activation function.

In all convolution layers, the padding is set to SAME and stride is set to 1. As a result, none of these convolutional layers changes the size of feature map.

Experiments
In this section, we verify the superiority of our SDNet on multi-modal image fusion and digital photography image fusion. First, we give the specific experimental settings. Then, we provide the qualitative and quantitative experimental results of two types of fusion scenarios, and analyze the results. Besides, we also conduct the comparative experiment of efficiency, ablation experiments, visualization of decomposition, infrared and RGB visible image fusion, sequence image fusion, and comparison with our preliminary version (Zhang et al. 2020). Finally, the application verification is provided.

Experimental Settings
Data
We verify our SDNet in multi-modal image fusion and digital photography image fusion. Two of the representative multi-modal fusion tasks are medical image fusion and infrared and visible image fusion. While the typical digital photography image fusion tasks are multi-exposure image fusion, and multi-focus image fusion. The training and test sets for all fusion tasks are from publicly available datasets: MRI and PET images from Harvard medical school websiteFootnote1 for medical image fusion task; TNOFootnote2 dataset for infrared and visible image fusion task; the dataset provided by Cai et al. (2018) for multi-exposure image fusion task; the dataset provided by Nejati et al. (2015) for multi-focus image fusion.

In these four image fusion tasks, the number of image pairs used for testing is all 10. For training, in order to obtain more training data, we adopt the expansion strategy of tailoring and decomposition. Specifically, for the medical image fusion task, we crop the rest of images to 13, 328 image patch pairs of size 120Ã—120 for training; for the infrared and visible image fusion, we crop the rest of images to 80, 881 image patch pairs of size 120Ã—120 for training; for the multi-exposure image fusion task, we crop the rest of images to 91, 840 image patch pairs of size 120Ã—120 for training; for the multi-focus image fusion task, we crop the rest of images to 184, 790 image patch pairs of size 60Ã—60 for training. Since the proposed SDNet is a fully convolutional network, the source images do not need to be cropped into small patches with the same size as the training data during the test phase. In other words, the test is performed on the original size of source images.

Fig. 5
figure 5
Qualitative results of the medical image fusion. From left to right: PET image, MRI image, fused results of ASR (Liu and Wang 2014), PCA (Naidu and Raol 2008), NSCT (Zhu et al. 2019), CNN (Liu et al. 2017), U2Fusion (Xu et al. 2020) and our SDNet

Full size image
Fig. 6
figure 6
Qualitative results of infrared and visible fusion. From left to right: visible image, infrared image, results of GTF (Ma et al. 2016), MDLatLRR (Li et al. 2020), DenseFuse (Li and Wu 2018), FusionGAN (Ma et al. 2019), U2Fusion (Xu et al. 2020) and our SDNet

Full size image
Training Details
For fusion tasks where the source images are all grayscale images, the proposed model can be directly used to fuse source images to generate the fused result, such as infrared and visible image fusion. For grayscale image and color image fusion tasks, such as medical image fusion, we first transform the color image from RGB to YCbCr color space. Because the Y channel (luminance channel) can represent structural details and the brightness variation, we devote to fusing the Y channel of color source image and the grayscale source image. Then, we directly concatenate the Cb and Cr channels of the color source image with the fused Y channel, and transferred these components to RGB space to obtain the final result. For the fusion task where the source images are all color images, such as multi-exposure and multi-focus image fusion, we transform all color source images from RGB to YCbCr color space. Then, we fuse the Y channels of source images using the proposed model, and follow the Eq. (12) to fuse the Cb or Cr channels of source images:

ğ¶=ğ¶1|ğ¶1âˆ’ğœ|+ğ¶2|ğ¶2âˆ’ğœ||ğ¶1âˆ’ğœ|+|ğ¶2âˆ’ğœ|,
(12)
where |â‹…| indicates the absolute value function, C is the fused Cb or Cr, ğ¶1 and ğ¶2 represent the Cb or Cr of two source images, respectively. In addition, ğœ is the median value of the dynamic range, which is set to 128. Finally, the fused components are transferred to RGB space to obtain the final result.

The batch size is set to b, and it takes m steps to train one epoch. The total number of training epochs is M. In our experiment, we set ğ‘=32, ğ‘€=30, and m is set as the ratio between the whole number of patches and b. The parameters in our SDNet are updated by AdamOptimizer. In addition, ğ›¼ of Eq. (5) in four tasks are set according to the rules in Eqs. (7)â€“(10): 0.5, 0.5, 1 and 1. For ğ›½ in Eq. (1), we aim to obtain the best results by setting them in four tasks as: 10, 80, 50 and 3. All deep learning-based methods run on the same GPU RTX 2080Ti, while other methods run on the same CPU Intel i7-8750H. It is worth noting that during the testing phase, only the squeeze network is used to generate the fused result.

Results on Multi-modal Image Fusion
The two representative tasks of multi-modal image fusion we selected are medical image fusion and infrared and visible image fusion. For the former, we use five state-of-the-art medical image fusion methods for comparison, including ASR (Liu and Wang 2014), PCA (Naidu and Raol 2008), NSCT (Zhu et al. 2019), CNN (Liu et al. 2017) and U2Fusion (Xu et al. 2020). For the latter, we select five state-of-the-art infrared and visible image fusion methods for comparison, such as GTF (Ma et al. 2016), MDLatLRR (Li et al. 2020), DenseFuse (Li and Wu 2018), FusionGAN (Ma et al. 2019) and U2Fusion (Xu et al. 2020).

Qualitative Comparisons
In each fusion task, three typical image pairs are selected to qualitatively demonstrate the performance of each method, which are shown in Figs. 5 and  6. The qualitative analysis is as follows.

For medical image fusion, the three typical results we select are on different transaxial sections of the brain-hemispheric. From these results, we can see that our SDNet has two advantages over other methods. First, the results of our method contain a wealth of brain structural textures. Only NSCT, CNN, U2Fusion and our SDNet can well preserve the texture details in the MRI image, while ASR and PCA cannot, such as the structural textures in the first and second set of results in Fig. 5. However, these textures are much finer and sharper in the results of our method. Second, our SDNet can maintain the better functional information, in other words, the color distortion rarely occurs in our SDNet over other methods. For example, in the highlighted part of the third set of results of Fig. 5, the results of NSCT, CNN and U2Fusion appear whitening, which is inconsistent with the distribution of functional information in the PET image.

In infrared and visible image fusion, according to the characteristics of fused results, these comparative methods can be divided into two categories. The results of the first category are biased towards the visible image, such as MDLatLRR, DenseFuse and U2Fusion. Specifically, although their fused results contain richer texture details, they cannot maintain the significant contrast of the infrared image. The results of the second category are more similar to the infrared image. They maintain the significant contrast well, but the texture details in them are not rich enough, which look more like sharpened infrared images, such as GTF and FusionGAN. In comparison, our method is more like a combination of these two categories. First, our method can maintain the significant contrast and effectively highlight the target from the background, like methods of the second category. For example, only the fused results of GTF, FusionGAN and our method have significant contrast and can highlight the targets, such as the human in the first and third group of results in Fig. 6. Second, the fused results of our method also contain rich texture structures, just like methods of the first category. For example, in the second row of Fig. 6, our SDNet maintains the texture details of the shrub and clothes well, while GTF and FusionGAN cannot.

In general, the qualitative results of our SDNet have certain superiority compared with other comparative methods in multi-modal image fusion.

Quantitative Comparisons
In order to assess our method in multi-modal image fusion more comprehensively, we conduct a quantitative comparison on ten pairs of images in each fusion task. Considering the characteristics of multi-modal image fusion, four objective metrics are selected to evaluate the quality of the fused images, namely entropy (EN) (Roberts et al. 2008), mutual information of discrete cosine features (ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡) (Haghighat and Razian 2014), the peak signal-to-noise ratio (PSNR), and mean gradient (MG). The reasons for selecting them are as follows. In multi-modal image fusion, part of the information will inevitably be discarded, so we adopt EN to evaluate the amount of information remaining in the fused image. The larger the value of EN, the more information the fused image contains. We use ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡ to assess the amount of features that are transferred from source images to the fused image. It can also reflect the degree of correlation between the features in the fused image and the source image, which has a characterizing significance for data fidelity. A large ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡ metric generally indicates that considerable feature information is transferred from source images to the fused image. Affected by the imaging environment, visible images often contain a lot of noise. Similarly, the particularity of medical images also requires the noise level to be as low as possible. Therefore, we introduce the PSNR to evaluate the noise level in the fused results. A larger PSNR value indicates less noise relative to the useful information. In addition, we adopt the MG to assess the richness of texture structure. A large MG metric indicates that the fused image contains rich texture details. The quantitative results of multi-modal image fusion are shown in Table 1.

Table 1 Quantitative results of multi-modal image fusion
Full size table
For medical image fusion, it can be seen that our method achieves the largest average values on three metrics EN, ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡ and MG. In metric PSNR, our method are second only to U2Fusion. From these results, we can conclude that the results of our SDNet contain the most information, and can obtain the most features from source images. Moreover, the result of our method contains the richest texture details, which proves that our SDNet can preserve sufficient structural information of MRI images. It is worth noting that the level of noise in our results is low, which means the proposed SDNet is rigorous in fusing information.

Fig. 7
figure 7
Qualitative results of multi-exposure image fusion. From left to right: underexposed image, overexposed image, fused results of AWPIGG (Lee et al. 2018), DSIFT (Hayat and Imran 2019), GFF (Li et al. 2013), DeepFuse (Prabhakar et al. 2017), U2Fusion (Xu et al. 2020) and our SDNet

Full size image
Fig. 8
figure 8
Qualitative results of multi-focus fusion. From left to right: the near focus image, the far focus image, fused results of CNN (Liu et al. 2017), DSIFT (Liu et al. 2015), GD (Paul et al. 2016), SESF (Ma et al. 2020), U2Fusion (Xu et al. 2020) and our SDNet

Full size image
In the infrared and visible image fusion, our SDNet achieves the largest average values on ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡, PSNR and MG. Therefore, the results of our SDNet have a amount of feature information that is transferred from source images, have the largest signal-to-noise ratio, and contain the richest texture details. Interestingly, our method does not achieve a good EN value like in medical image fusion, and this is caused by the characteristics of infrared and visible image fusion. Specifically, visible images usually contain a large amount of noise, and the noise suppression of our method reduces the entropy of fused results to a certain extent.

Overall, the proposed SDNet performs better in quantitative comparisons over other comparative methods in multi-modal image fusion, which is consistent with the qualitative comparison.

Results on Digital Photography Image Fusion
Our SDNet also performs well on the digital photography image fusion. To verify this point, we conducted comparative experiments on two typical digital photography image fusion tasks: multi-exposure image fusion and multi-focus image fusion. For multi-exposure image fusion, we select five state-of-the-art methods to compare with our SDNet, which are AWPIGG (Lee et al. 2018), DSIFT (Hayat and Imran 2019), GFF (Li et al. 2013), DeepFuse (Prabhakar et al. 2017) and U2Fusion (Xu et al. 2020). For the multi-focus image fusion, five state-of-the-art methods are also selected to compare with our method. These methods are CNN (Liu et al. 2017), DSIFT (Liu et al. 2015), GD (Paul et al. 2016), SESF (Ma et al. 2020) and U2Fusion (Xu et al. 2020).

Qualitative Comparisons
In each digital photography image fusion task, we give three typical intuitive results to qualitatively compare our SDNet with other methods, as shown in Figs. 7 and 8. The detailed qualitative analysis is as follows.

In the multi-exposure image fusion task, we can find that the proposed SDNet has two major advantages. Firstly, our method can avoid strange black shadows and unnatural illumination transitions. Concretely, the fused results of AWPIGG, DSIFT and GFF show these strange shadows, such as the sky in the second set of results, while DeepFuse, U2Fusion and the proposed SDNet do not. Secondly, in areas of extreme overexposure and extreme underexposure (where texture details are only visible in a single source image), our SDNet can better preserve these details and their shapes. For example, in the third group of results in Fig. 7, the lamps are blurred and their edges are even invisible in the results of other three methods except DeepFuse, U2Fusion and our SDNet. However, compared to DeepFuse and U2Fusion, our SDNet can preserve these texture details more finely, such as the roof in the first set of results and the tree branches in the second group of results. Note that our results have insufficient local contrast in some scenes. Even so, the visual effect of our method is still better than that of other methods.

In the multi-focus image fusion task, the five comparative methods selected can be divided into two categories according to the principle. Methods of the first category is to generate the decision map based on focus detection to fuse multi-focus images, such as CNN, SESF and DSIFT. Such methods often lose detail due to misjudgment near the junction of focused and non-focused regions. The other category is to fuse the multi-focus image from the global perspective instead of focused area detection at pixel scale. The disadvantages of this kind of methods are that the intensity distortion and blur effect will appear in fused results, such as GD and U2Fusion. From the results, we see that our SDNet has clear advantages over these two categories of methods. First of all, compared with the decision map-based methods, our method can accurately retain details near the junction of focused and non-focused regions. For example, in the first set of results in Fig. 8, these methods lose the golf ball. Secondly, our method can also maintain the same intensity distribution as the source image. It can be clearly seen that the fused results of GD have intensity distortion compared to the source images, and the results of U2Fusion have a certain degree of detail blur, while our method does not. Note that there are some visible halos in the results of our and other methods at the boundary between the focused and non-focused regions. This is caused by the outline of the foreground target in the source image spreading to the background area due to defocus.

Overall, in the digital photography image fusion scenario, our SDNet has better performance in terms of intuitive effect.

Quantitative Comparisons
We also perform quantitative comparisons to further demonstrate the performance of our SDNet in the digital photography image fusion scenario. Considering the characteristics of digital photography image fusion, four objective metrics are selected to evaluate the fused results, which are ğ‘„ğ´ğµ/ğ¹, mutual information of discrete cosine features (ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡) (Haghighat and Razian 2014), the peak signal-to-noise ratio (PSNR) and ğ‘ğ´ğµ/ğ¹ (Kumar 2013). PSNR and ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡ have been described previously, and the rest are explained below. Different from multi-modal image fusion, the scene information reflected by source images in digital photographic image fusion is strictly complementary according to regions, so the preservation of the scene information is as important as the fidelity of the scene information. Therefore, we adopt ğ‘„ğ´ğµ/ğ¹ to assess the amount of edge structures that are transferred from source images to the fused image rather than MG. A large ğ‘„ğ´ğµ/ğ¹ means that the scene content contained in the fused image is richer and more comprehensive. In addition, in multi-exposure image fusion, the fused result often contains unnatural artifacts, while in multi-focus image fusion, the fused result often suffers from overall contrast distortion. We introduce ğ‘ğ´ğµ/ğ¹ to evaluate artifacts and contrast distortion in the fused image. A smaller ğ‘ğ´ğµ/ğ¹ value indicates less artifacts and contrast distortion. The quantitative results of digital photography image fusion are shown in Table 2.

In multi-exposure image fusion, our SDNet achieves the largest average values on the first three metrics ğ‘„ğ´ğµ/ğ¹, ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡ and PSNR. For the ğ‘ğ´ğµ/ğ¹, our method is second only to DeepFuse by a slight difference. These can explain that the results of our method contain the richest and the most comprehensive scene content, contain a mount of feature information that is transferred from source images, and involve the lowest level of noise. In addition, like Deepfuse, our method can also avoid unnatural artifacts, which can also be seen from the qualitative results in Fig. 7.

In multi-focus image fusion, as can be seen from the statistical results, our SDNet can also achieve the largest average values on ğ‘„ğ´ğµ/ğ¹, ğ¹ğ‘€ğ¼ğ‘‘ğ‘ğ‘¡ and PSNR. These results can indicate that the fused results of our method contain the most scene content, can obtain the most feature information from source images, and have the largest signal-to-noise ratio. Moreover, our SDNet ranks second on ğ‘ğ´ğµ/ğ¹, next to DSIFT.

Generally, in the digital photography image fusion, our method performs better than the comparative methods in quantitative comparison.

Comparisons of Efficiency
In our method, the number of parameters used for testing is 0.287 M, which is very lightweight. In order to evaluate it more comprehensively, we carry out comparative experiments on the efficiency of multi-modal image fusion and digital photography image fusion. The number of image pairs for testing in four representative tasks is all 10.

Table 2 Quantitative results of digital photography fusion
Full size table
Table 3 Average running time of different methods in multi-modal image fusion and digital photography image fusion (unit: second)
Full size table
The results are shown in Table 3. It can be seen that our method achieves the highest running efficiency in all four image fusion tasks: medical image fusion, infrared and visible image fusion, multi-exposure image fusion, and multi-focus image fusion. In general, our SDNet has the significant superiority in running time, almost an order of magnitude faster than the comparative methods, which can deal with real-time fusion tasks.

Ablation Experiments
To verify the effectiveness of the specific designs in this paper, we perform relevant ablation experiments. First, we reveal the role of two proposed key modules, including the adaptive decision block and the decomposition network. Then, we separately evaluate the fusion performance when the intensity loss, the gradient loss, and the squeeze fusion loss are removed.

Adaptive Decision Block Analysis
In this work, the adaptive decision block acts on the gradient loss term, which can adaptively guide the gradient distribution of the fused image to approximate to the pixel with stronger gradient. To our knowledge, it is the first time that the pixel-scale guidance strategy is adopted for addressing the image fusion task. More importantly, it fits well with both multi-modal image fusion and digital photography image fusion. We analyze the feasibility of the adaptive decision block in four image fusion tasks as follows. In medical image fusion, because the difference in the intensity of PET and MRI images is very large, the phenomenon of imbalance of structural textures and functional information often occurs. More specifically, texture textures from MRI images is often submerged in functional information from PET images, causing texture details to be weakened or lost. The adaptive decision block can decide the target of gradient optimization at the pixel scale, which can make the network retain the functional information while preserving the structural texture as significantly as possible. In infrared and visible image fusion, visible images often contain a lot of noise due to environmental factors such as weather. Reducing the influence of noise on the fusion process is the key to improving the quality of the infrared and visible image fusion. The adaptive decision block can enhance the robustness of our method against noise, because there is the Gaussian low-pass filtering operation in the decision block, which can reduce the noise interference to a certain extent. In multi-exposure image fusion, the exposure degree of the source image is uncertain. In other words, it is difficult to determine the exposure level of the source image and the distance of the ideal illumination. In this case, the adaptive decision block can measure the appropriateness of exposure based on the gradient magnitude at the pixel scale, because both overexposure and underexposure can cause texture detail loss. Therefore, the adaptive decision block can force the fused results to have more appropriate lighting while preserving more comprehensive texture details. In multi-focus image fusion, the adaptive decision block can also make the final fused result more promising. The adaptive decision block can select the maximum gradient at the corresponding pixel position as the optimization target to force the fused result to contain richer texture details, which avoids the fusion result being the intermediate result between clear and fuzzy. In order to fully verify the role of it, we train our network without it. Specifically, we adopt the proportioned setting strategy for the gradient loss like in the intensity loss, weighting from the global perspective of image patches.

Fig. 9
figure 9
Qualitative results of ablation on the adaptive decision block. From left to right: source image pairs, results without the adaptive decision block and ours. ADB refers to the adaptive decision block

Full size image
The qualitative results are shown in Fig. 9. In medical image fusion, it can be seen that when using the adaptive decision block, the structural textures in the fused result are maintained very well. At the same time, the functional information is not worse than the fused result without the adaptive decision block. In infrared and visible image fusion, it is obvious that the visible image contains a lot of noise. When there is no adaptive decision block, the gradient constraint is severely affected by noise and the detail in the window of the car is weak, as highlighted. In contrast, this detail is well preserved when using the adaptive decision block. For multi-exposure image fusion, the fused result with the adaptive decision block has more suitable lighting, and can retain more scene details, which are poorly done without it. In multi-focus image fusion, when there is no adaptive decision block, the texture detail of the fused result is an intermediate result between sharpness and blurring, which is not as clear as the result using the adaptive decision block. Therefore, the adaptive decision block has the rationality in all four fusion tasks, which can help our method produce more promising visualization results.

The quantitative results are reported in Table 4. It can be seen that the results with the adaptive decision block are better than those without the adaptive decision blocks on 11/16 metrics, which demonstrate the effectiveness of the adaptive decision block.

Decomposition Network Analysis
The role of the decomposition network is to decompose the fused image to generate results that approximate source images. Because the quality of the decomposed results directly depends on the fused image, the decomposition consistency constraint can force the fused image to contain more scene details. To validate the effectiveness of the decomposition network, we train our model without the decomposition network. The difference of the results is shown in Fig. 10. In medical image fusion, the result without decomposition network loses some structural textures of the brains, while that with decomposition network can retain it well. In infrared and visible image fusion, the decomposition network makes the smoke thinner, thereby more clearly highlighting the soldier. In the multi-exposure image fusion, the result with decomposition network can more clearly preserve the texture structures in the underexposed region, while in the result without DN these texture structures are weak. Similarly, in multi-focus image fusion, decomposition network can make the fused result contain richer texture details.

The quantitative results are reported in Table 4. The results with the gradient loss are better than those without the gradient loss on 12/16 metrics. These results show that the decomposition network can indeed improve the fusion performance.

Table 4 Quantitative results of ablation experiments. ADB indicates the adaptive decision block, and DN is the decomposition network. Bold indicates the best result
Full size table
Fig. 10
figure 10
Qualitative results of ablation on the decomposition network. From left to right: source image pairs, fused results without the decomposition network and ours

Full size image
Fig. 11
figure 11
Results of ablation on the intensity loss. From left to right: source image pairs, fused results without the intensity loss and ours

Full size image
Intensity Loss Analysis
The intensity loss guides the fused image to preserve useful information that is characterized by the pixel intensity, such as the contrast. Meanwhile, it can make the overall scene style of the fused image more natural and not divorced from reality. In order to evaluate the effectiveness of the intensity loss, we conduct the ablation experiment on it, and the results are shown in Fig. 11. It can be seen that in most image fusion tasks, the results without the intensity loss have problems such as information loss and unrealistic style. For example, the result of infrared and visible image fusion loses the saliency of the thermal radiation target, and even has an intensity reversal. And the results of several other image fusion tasks have unrealistic styles. These results show that the intensity loss is very important. As the results without the intensity loss are far from expected, we do not perform quantitative experiments.

Fig. 12
figure 12
Results of ablation on the gradient loss. From left to right: source image pairs, fused results without the gradient loss and ours

Full size image
Fig. 13
figure 13
Results of ablation on the squeeze fusion loss. From left to right: source image pairs, fused results without the squeeze fusion loss and ours

Full size image
Gradient Loss Analysis
In our model, the gradient loss forces the fused image to contain rich texture detail. To verify the effect of the gradient loss, we provide the qualitative results of ablation experiment on the gradient loss, as shown in Fig. 12. It can be seen that in all four image fusion tasks, the results without the gradient loss suffer from texture loss and sharpness degradation. On the contrary, the results with the gradient loss can maintain the original sharpness well and contain rich texture details.

Further, we report the quantitative results of the ablation experiment on the gradient loss in Table 4. The results with the gradient loss are better than those without the gradient loss on 15/16 metrics, which strongly prove that the gradient loss is very important to the fusion performance.

Fig. 14
figure 14
Fusion and decomposition of images with the same scene. Left: decomposition of multi-modal image fusion; right: decomposition of digital photographic image fusion

Full size image
Fig. 15
figure 15
Fusion and decomposition of different scenes. Left: decomposition of multi-modal image fusion; right: decomposition of digital photographic image fusion

Full size image
Squeeze Fusion Loss Analysis
Further, we completely remove the squeeze fusion loss. In other words, we not only remove the intensity loss but also the gradient loss, so that both the squeeze network and the decomposition network are optimized only under the guidance of the decomposition consistency loss. The results are shown in Fig. 13. After completely removing the squeeze fusion loss, although the fused results can contain a certain degree of scene content, the overall style is far from that of source images, which deviates from the reality. Because of this, there is no need to implement quantitative verification. In general, these results demonstrate that the squeeze fusion loss is very important to the fusion rationality.

Visualization of Decomposition
Fusion and Decomposition of the Same Scene
When source images describe the same scene, the decomposition network is dedicated to decomposing the fused image produced by the squeeze network into results similar to source images, so as to force the fused image to contain more scene details of source images. In order to show this process intuitively, for the multi-modal image fusion and digital photographic image fusion, we select one representative task to perform visualization respectively, saying medical image fusion and multi-exposure image fusion. The visualization results are in Fig. 14. It can be seen that no matter in multi-modal image fusion or digital photography image fusion, good decomposed results can be obtained, which are very similar to source images. Simultaneously, the fused results can provide a good visual perception, which verifies the feasibility of our design.

Fusion and Decomposition of Different Scenes
An interesting idea is that if the scenes represented by source images are different, what will the results of the squeeze network and decomposition network look like? In order to observe this, we implement the fusion and decomposition of source images with different scenes, and the results are shown in Fig. 15. It can be seen that although the decomposition network does not completely separate the different scene content, the decomposed results are still dominated by the scene content represented by the corresponding source image. In addition, the fused results are able to integrate the content of different scenes well.

Decomposition of the Real Image
In the proposed method, the squeeze network and the decomposition network are collaboratively optimized. In other words, what the decomposition network disintegrate is actually a synthesized fused image, which is not a real image. To verify the performance of the decomposition network, we adopt the trained decomposition network to decompose the real images in COCO dataset (Lin et al. 2014), and the results are shown in Fig. 16. It can be seen that the decomposition network can well decompose the well-exposed real images into underexposed and overexposed images. And the decomposed results are natural, in line with human visual experience.

Fig. 16
figure 16
Decomposition of the real image. The decomposition network is able to decompose the well-exposed real image into overexposed and underexposed images

Full size image
Different Exposure Levels Fusion
The proposed model has good adaptability to test data with different distributions. In order to verify this point, we test the trained model in source images with different exposure levels, and the results are shown in Fig. 17. It can be seen that the exposure levels of these three pairs of source images are significantly different, which means that their distribution is different. However, our SDNet can generate good fused results in all three tests, which contain rich scene details in source images. This demonstrates that our SDNet has good adaptability.

Fig. 17
figure 17
Fusion of different exposure levels. The proposed method can generate good fused results in all three tests

Full size image
Infrared and RGB Visible Image Fusion
The proposed SDNet is also applicable to the infrared and RGB image fusion. We provide two typical results to demonstrate this, as shown in Fig. 18. It can be seen that the fused results not only accurately maintain significant contrast, but also contain rich texture details. Moreover, the colors in RGB visible images are also well transferred to the fused images, which have a good visual effect.

Fig. 18
figure 18
Results of the infrared and RGB visible image fusion

Full size image
Sequence Image Fusion
In digital photographic image fusion scenario, the number of source images may exceed two. In this case, our method is also applicable. To confirm this point, we implement our method on a sequence with three multi-focus source images, and a sequence with three multi-exposure source images, respectively. Like in Nejati et al. (2015), we first fuse two of the source images as before, and then fuse this intermediate result with the last source image to obtain the final fused image. The results are shown in Fig. 19. In multi-focus image fusion, it can be seen that the fused result of our method contains all the clear regions in the three source images, which is a full-clear image with good visual effects. Similarly, in multi-exposure image fusion, our result has proper lighting and can retain almost all scene content. All these prove that our SDNet is suitable for the fusion of image sequences.

Composite Fusion Scene
Because the proposed method has the same idea for different tasks, it can handle some composite image fusion scenes well. For example, some visible images are overexposed, and some details are blurred or even invisible, which is a new challenge for infrared and visible image fusion. This is more like a mixture of two fusion tasks, namely multi-exposure image fusion and infrared and visible image fusion. We provide a typical example to illustrate this intuitively, as shown in Fig. 20. Among them, GTF (Ma et al. 2016) and FusionGAN (Ma et al. 2019) are two algorithms designed specifically for the infrared and visible image fusion. It can be seen that GTF and FusionGAN lose the vehicles in the overexposed region, while our method can preserve them well. Therefore, we believe it is desirable to design a universal model for different image fusion tasks, which may bring new inspirations to the image fusion community.

Fig. 19
figure 19
Sequence image fusion. From left to right: source image 1, source image 2, source image 3 and the result of our SDNet

Full size image
Fig. 20
figure 20
Results of the composite fusion scene

Full size image
Fig. 21
figure 21
Comparative results of our SDNet with the previous PMGI

Full size image
SDNet vs. PMGI
The previous version of the proposed SDNet is PMGI (Zhang et al. 2020), and the improvements have two main aspects. Firstly, we design a new adaptive decision block and introduce it into the construction of gradient loss. In PMGI, the weight of the gradient loss term is proportionally set according to the global texture richness of source images. For example, visible images can provide more texture details, so a large weight is set for the gradient loss term of the visible image and a small weight is set for the gradient loss term of the infrared image. And once these global weights are set, they are fixed throughout the training process. The direct negative effect brought by this is the texture structure loss and the sharpness reduction. Instead, we use the adaptive decision block to guide the optimization of gradient loss in SDNet, which can adaptively select the gradient of sharp source pixels as the optimization target at the pixel scale, making the fused result contain richer texture structures and higher sharpness. Secondly, unlike PMGI that only considers the squeezing process of image fusion, the SDNet proposed in this paper considers both squeeze and decomposition. Concretely, the proposed SDNet is composed of two parts: the squeeze network and the decomposition network. The squeeze network is dedicated to squeezing source images into a single fused image, while the decomposition network is devoted to decomposing this fused result to obtain images consistent with source images. The decomposition consistency can force the fused result to contain richer scene details, and thus have a better fusion effect.

Compared to PMGI, SDNet performs better on both multi-modal image fusion and digital photography image fusion. To visually show the difference, we compare their results, as shown in Fig. 21. Firstly, in multi-modal image fusion, our SDNet can better preserve the texture structure, and has the expected contrast distribution. For instance, in the infrared and visible image fusion, SDNet retains the clear roof texture in the first column, while PMGI loses it. In the second column of results, the contour of the target in the result of PMGI has artifacts, while the one in SDNet is clean and sharp. Similarly, in the medical image fusion task, SDNet can better preserve the distribution of brain structures in MRI images. In addition, the function information is more similar to that in PET. Secondly, in digital photographic image fusion, our method can retain almost all scene content, so as to has better visual performance. For example, in the multi-exposure image fusion task, the results of SDNet have more reasonable illumination, which contain more comprehensive texture details in the scenes. On the contrary, they are lost in the results of PMGI. The most obvious difference is in multi-focus image fusion. PMGI produces results between clarity and blur. In contrast, SDNet can produce quite promising full-clear results. Overall, SDNet performs better than PMGI on both multi-modal and digital photography image fusion.

Table 5 Efficiency comparisons of our SDNet with the previous version PMGI (Zhang et al. 2020). Bold indicates the best result
Full size table
In addition, we optimize the network structure so that it has higher operating efficiency. We provide the average running time of PMGI and SDNet in Table 5 for comparison. It can be seen that in all image fusion tasks, SDNet is faster than PMGI. Therefore, we can conclude that the newly designed network structure in SDNet has higher efficiency without affecting performance.

Application Verification
Taking infrared and visible image fusion as an example, we demonstrate the advantages of the proposed method in preserving scene textures and salient targets through downstream applications, including the feature matching between multiple views and the pedestrian semantic segmentation.

Feature Matching between Multiple Views
Since feature matching performance is closely related to scene textures, it can verify the retention ability of the fusion method to scene textures. Specifically, we select infrared and visible image pairs from two views, and test the matching performance on the visible modality, the infrared modality and different fused results. The open source VLFeat toolbox (Vedaldi and Fulkerson 2010) is employed to determine the putative correspondence with SIFT (Lowe 2004). Note that the images used are captured from the FLIR video sequence, and the scenes usually involve moving objects such as pedestrians and cars in our matching pairs. Thus, it is extremely challenging to identify the inlier correspondences by recovering the 3D scene geometry in the dataset, and we establish ground truth by manually labeling each correspondence. The results are shown in Fig. 22 and Table 6. It can be seen that the matching performance on our fused result is the best, which demonstrates that the proposed method can best maintain the texture structure in the scene.

Fig. 22
figure 22
Feature matching visualization on different modalities. Blue indicates the true matches, and red represents the false matches

Full size image
Table 6 Feature matching accuracy on different modalities. Bold indicates the best result
Full size table
Table 7 Pedestrian semantic segmentation accuracy on different fused results
Full size table
Pedestrian Semantic Segmentation
In the infrared image, the pedestrians as the thermal targets are salient, and it is suitable to implement semantic segmentation on this modality. Therefore, we first train a model for pedestrian semantic segmentation on the infrared image. Then, the segmentation model is directly tested on the results of different fusion methods, and the segmentation accuracy can indicate the quality of each fusion method to preserve the salient target. The results are shown in Fig. 23 and Table 7. It can be seen that the segmentation performance of our results is the best, which can accurately detect pedestrians, even small-scale ones. These results prove that the proposed method can best preserve the salient thermal targets in the infrared image. It is worth noting that although the segmentation performance of our results is comparable to that of the infrared image, our results that integrate the visible scene information will be probably more conducive to higher-level decision-making tasks, such as the pedestrian identification.

Conclusion and Discussion
In this paper, a squeeze-and-decomposition network, called SDNet, is proposed to generally fulfill multi-modal image fusion and digital photography image fusion. On the one hand, we model multiple image fusion tasks as the extraction and reconstruction of gradient and intensity information, and design a universal form of loss function accordingly. For gradient information, an adaptive decision block is designed to decide the optimization target of the gradient distribution according to the texture richness on the pixel scale, which can adaptively force the fused result to contain richer texture details. For intensity information, we adopt the proportioned setting strategy to adjust the loss weight ratio, so as to satisfy the requirement of intensity distribution tendency in different image fusion tasks. On the other hand, we design a squeeze-and-decomposition network, which not only considers the squeeze process from source images to the fused result, but also strives to decompose results that approximate source images from the fused image. The decomposition consistency will force the fused image to contain more scene details. Extensive qualitative and quantitative experiments demonstrate the superiority of our SDNet over state-of-the-art methods in terms of both subjective visual effect and quantitative metrics in multiple image fusion tasks. Moreover, our method is about one order of magnitude faster compared with the state-of-the-art, which is suitable for addressing real-time fusion tasks.

Fig. 23
figure 23
Visualization results of the pedestrian semantic segmentation

Full size image
In a real scene, the images captured by the sensor are all unregistered. Unfortunately, the existing methods cannot handle these real unregistered data, as shown in Fig. 24. Inevitably, these fusion methods rely on pre-processing of registration algorithms. As a result, they may have certain limitations in real scenes, such as low efficiency and dependence on registration accuracy. In the future, we will focus on the research of unregistered fusion algorithms, so as to fulfill the image registration and fusion in an implicit manner. We believe this will greatly improve the suitability of image fusion for real-world scenarios.

Fig. 24
figure 24
The proposed method cannot handle non-registered data from Ha et al. (2017), and artifacts appear in the fused results

Full size image