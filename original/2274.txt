Markerless motion capture and understanding of professional non-daily human movements is an important yet unsolved task, which suffers from complex motion patterns and severe self-occlusion, especially for the monocular setting. In this paper, we propose SportsCapâ€”the first approach for simultaneously capturing 3D human motions and understanding fine-grained actions from monocular challenging sports video input. Our approach utilizes the semantic and temporally structured sub-motion prior in the embedding space for motion capture and understanding in a data-driven multi-task manner. To enable robust capture under complex motion patterns, we propose an effective motion embedding module to recover both the implicit motion embedding and explicit 3D motion details via a corresponding mapping function as well as a sub-motion classifier. Based on such hybrid motion information, we introduce a multi-stream spatial-temporal graph convolutional network to predict the fine-grained semantic action attributes, and adopt a semantic attribute mapping block to assemble various correlated action attributes into a high-level action label for the overall detailed understanding of the whole sequence, so as to enable various applications like action assessment or motion scoring. Comprehensive experiments on both public and our proposed datasets show that with a challenging monocular sports video input, our novel approach not only significantly improves the accuracy of 3D human motion capture, but also recovers accurate fine-grained semantic action attribute.

Access provided by University of Auckland Library

Introduction
The past ten years have witnessed a rapid development of markerless motion capture and understanding for human daily activities, which benefits various real-world applications such as immersive VR/AR experience, action quality assessment (Pan et al., 2019) and vision-based robotics (Ran et al., 2017). How to further capture professional non-daily human motions and provide fine-grained analysis has recently received substantive attention.

Fig. 1
figure 1
SportsCap: a multi-task approach for 3D motion capture and action understanding of challenging sports videos. We collect a sport-related motion capture dataset to build the Sports Pose Embedding Spaces on specific sports, like balance beam (Blue), boxing (Orange), high jump (Yellow), and other sports. This Sports Pose Embedding Spaces achieve significant superiority on challenging motion capture and encode semantic meanings (see Fig. 5) for action parsing tasks

Full size image
In this paper, we focus on markerless motion capture and fine-grained understanding for challenging professional human movements which are essential for many applications such as training and evaluation for gymnastics, sports, and dancing. However, these professional movements like diving and balance-beam suffer from complex motion patterns and severe self-occlusion, especially under the monocular setting, leading to inferior results and impractical usage of existing 3D motion capture (Xiao et al., 2018; Kocabas et al., 2020) and 2D pose detection approaches (Cao et al., 2019). When motion capture is unreliable, further motion analysis is even more challenging, which aims to provide both mid-level sub-motion categories and detailed semantic descriptions for each sub-motion or the whole motion sequence at the finest granularity. On the other hand, even though itâ€™s natural to split such challenging sports movements into sub-motions due to their repeatability and self-similarity, the literatures on utilizing such sub-motion prior to strengthening the motion capture and understanding are sparse. Moreover, most existing action understanding solutions (Parmar and Morris, 2019b; Shao et al., 2020) are limited to the pure high-level action assessment, where the abundant 3D motion capture information of sub-motions has been ignored.

To tackle these challenges, we propose SportsCapâ€”the first joint 3D motion capture and fine-grained understanding approach for various challenging sports movements from only a single RGB video input (see Fig. 1 for an overview). With the aid of mid-level sub-motion embedding analysis of plausible motion manifold, SportsCap explores and validates the mutual gain between 3D motion capture and fine-grained motion understanding. Our novel pipeline not only achieves significant superiority to previous capture methods for challenging motions, but also provides accurate fine-grained semantic assessment simultaneously for motion understanding, whilst still maintaining a monocular setup.

More specifically, we formulate this joint human motion capture and understanding problem in a multi-task learning framework. To this end, we first introduce a motion embedding space to model the manifold of plausible human poses for each sub-motion via the principal component analysis (PCA) technique. Then, for the motion capture task, an effective motion embedding network is proposed to estimate the per-frame implicit embedding parameters so as to recover the 3D motion details via a corresponding mapping function as well as a sub-motion classifier. Our motion capture scheme leverages the rich semantic and temporally structural prior of sub-motions in the motion embedding space to tackle the severe occlusion and depth ambiguities inherent to the monocular sports video input. For further motion understanding task, we predict the fine-grained semantic action attributes using a spatial-temporal Graph Convolutional Network(ST-GCN), based on both the original motion embedding stream and the recovered 3D detailed motion stream of the whole video clip. Our novel multi-stream ST-GCN module encodes both the implicit and explicit motion information from the previous capture stage for more accurate action attribute parsing. Finally, a semantic attribute mapping block is adopted to assemble various correlated action attributes into a high-level action label for the whole sequence (i.e. the diving number for the diving motion), which provides an extra overall detailed understanding of the whole video to enable various applications like action assessment or motion scoring. To summarize, the main contributions of SportsCap include:

We propose a novel join human 3D motion capture and motion understanding scheme in a data-driven multi-task manner under the monocular setting, achieving significant superiority to existing state-of-the-arts.

By utilizing the semantic and temporally structured motion prior in the embedding space, we propose a novel motion embedding module, as well as an effective multi-stream ST-GCN module to reconstruct both detailed 3D motions and accurate fine-grained actions, attributes simultaneously.

We make available our Sports Motion and Recognition Tasks (SMART) dataset, consisting of various challenging sports video clips with manually annotated poses and fine-grain action labels as well as the relevant ground truth 3D poses for motion embedding analysis.

Related Work
Pose and Shape Estimation aims to recover the underlying kinematic structure of a person. The results of these work can be 2D/3D poses or 3D human models that match the image/video observations. Earlier methods adopted geometric constraints (Yang and Ramanan, 2011) to construct poses. Recently, with the success of deep neural networks in many computer vision tasks, many deep learning-based pose estimation approaches (Cao et al., 2019; Pishchulin et al., 2016; Raaj et al., 2019; Sun et al., 2019, 2018; Tang et al., 2018) have achieved remarkable performance. OpenPose (Cao et al., 2019) employs Part Affinity Fields (PAF) to support bottom-up estimation. Sun et al. (2019) exploits multi-scale high-resolution networks to improve feature representation. Li et al., (2019) jointly optimize human pose and segmentation. However, such methods focus on regular movements and actions and have limitations to handling professional sports, which consist of more complex poses and occlusions in monocular videos. A few recent approaches aim to tackle special actions. Luvizon et al.  (2018) propose a semantic-based, multi-task learning framework, and Bertasius et al. (2018b) tailors a predictor specific to certain actions. Xiaohan Nie et al. (2015) uses a hierarchical structure to decompose an action into sub-poses and further divides each pose into many parts. He et al., (2021) captures challenging 3D human motions with the multi-modal references. These approaches do not consider rich semantic information embedded in sports, and the structure constraints within sub-motions. In contrast, our approach explicitly uses the underlying semantic and ordering rules in sport to reduce the complexity of the problem. And we utilize PCA to capture the similarities of poses in each sub-motion and constrain estimated poses in reasonable forms to further improve the accuracy.

Traditional 3D human estimation methods either use multi-camera dome systems (Kanade et al., 1997; Collet et al., 2015; Suo et al., 2021) or exploit the RGB-D sensors (Dou et al., 2016; Newcombe et al., 2015; Xu et al., 2019), and recover the human geometry via multi-view stereo and point cloud fusion. With the advance in parametric 3D human body models and deep neural networks, especially with the emergence of the SCAPE (Anguelov et al., 2005), SMPL (Loper et al., 2015), SMPL-X (Pavlakos et al., 2019), recovering the human shape from a single viewpoint image/video becomes more and more popular. SMPLify-X (Pavlakos et al., 2019) fits the face, hand, and body parts of the SMPL-X model to images with pre-estimated 2D poses. TightCap Chen et al., (2019) captures both the body shape and dressed garments with a single 3D human scan. HMR (Kanazawa et al., 2018) proposes an end-to-end framework to regress the pose/shape parameters of human model directly from a single image supervised by an adversarial prior. Similarly, the VIBE (Kocabas et al., 2020) leverages the human pose data from a motion capture dataset, AMASS (Mahmood et al., 2019), and develops an adversarial framework to discriminate between real human motions and the produced temporal pose and shape. Recovering the human shape from competitive sports images/videos is even more challenging. The aforementioned methods rely on 2D poses to regress the human pose and shape parameters, while athletes in competitive sports exhibit highly complex poses and fast motions that wonâ€™t appear in daily activities. We tackle the problem through embedding these highly complex but standard human poses (typical poses in many sports guidelines, like twisting in diving, turning in balance beam) to parametric space.

Action Parsing can be categorized into short vs. long dynamics, depending on the length of the motion patterns. For short term dynamics, Karpathy et al. (2014) uses 2D CNNs to learn deep appearance features and conduct frame-level classification. IDT (Feichtenhofer et al., 2016) extends the technique with shallow motion features and Hussein et al. (2019) uses 3D CNNs such as C3D (Tran et al., 2015) to capture spatial-temporal patterns of consecutive frames within the sequence. For long term dynamics, TRN (Zhou et al., 2018) exploits temporal dependencies across video frames over multiple hierarchies. TRN (Zhou et al., 2018) proposes a multi-stream architecture to extract even richer temporal features. LTC (Varol et al., 2017) treats the temporal resolutions as a substitute to temporal windows whereas Hussein et al. (2019) conducts long-range action recognition. We observe that competitive sports, like diving and gym, are always a mixture of long and short dynamics: actions such as twisting or somersaults map to short dynamics whereas the complete dive, with a corresponding dive number, map to long dynamics. We hence combine the advantages of short and long dynamics techniques.

To specifically tackle sports videos, Kanojia et al. (2019) proposes an attentive guided LSTM-based neural network for fine-grained motion recognition. Pishchulin et al. (2014) combines the dense motion trajectories and pose estimations to improve recognition accuracy. Choutas et al. (2018) represents the movement of semantic keypoints as a color encoded trajectory map, called PoTion, and subsequently conducts classification on the PoTion. In a similar vein, Fani et al. (2017) stacks the poses features generated by an hourglass network into a reference frame and then performs the fine-grained action recognition from hockey sports videos. Pan et al. (2019) builds a joint relation graph to model both the joint relations within a time step and across two immediate time steps. Nevertheless, though these approaches rely on the joint motions for action recognition, they ignore the patterns of the human body motion in certain activities. In contrast, we observe that the joint motion within a fine-grained action tends to be regular in competitive sports. Hence, we adopt a fine-grained manner to model the pose in each fine-grained action and finally resorts to the recent Graph Convolutional Network (GCN) (Defferrard et al., 2016; Henaff et al., 2015; Li et al., 2018a, b; Shi et al., 2019) for spatial-temporal representations.

Sports images/videos provide more challenging motions and environment for learning tasks. These tasks are numerous, ranging from correcting athletesâ€™ movements (Pan et al. 2019; Shao et al., 2020) for improving their performance to digitally producing 3D avatars (Rematas et al., 2018; Zhu et al., 2020) for video games and feature films. Rematas et al. (2018) built a CNN-based system to transform a soccer video into a moving 3D reconstruction, while Zhu et al. (2020) reconstruct skinned models of basketball players with a single input photo of a clothed player. For accurate tracking during big sports, such as soccer and basketball, Chen and and Little (2019) and Sha et al. (2020) propose automatic approaches of camera calibration with semantic segmentation and detected edge of sports environment, like field marking. Bertasius et al. (2018a), Su et al. (2017), Bertasius et al. (2017) propose the learning-based approaches to estimate motion, behaviors, and performance assessment of basketball players. Moreover, many official sports organizing committees provide the detailed rules of standard poses and assessment approaches, like Federation Internationale de Natation (FINA) for diving and Fdration Internationale de Gymnastique (FIG) for gymnastics.

Dataset is the basis for deep learning-based motion estimation and action parsing methods. There are some large-scale human image/video datasets, such as COCO (Lin et al., 2014) and MPII (Andriluka et al., 2014). They mainly focus on motions in daily motions. Competitive sports video understanding relies heavily on available sports datasets. Zhang et al. (2013) proposes a simple motion dataset of 15 actions with annotated body joints but no action labels. Parmar and Morris (2019a; b) presents the MTL-AQA dataset that exploits multi-task networks along with a caption generation model to simultaneously assess the move and produce a caption. Li et al. (2018c) proposes the Diving48 dataset for competitive diving video understanding. The UCF101 dataset (Soomro et al., 2012) contains 101 classes of in-the-wild actions and the ActivityNet. Heilbron et al. (2015) covers a wide range of complex human activities in daily living. More recently, Shao et al. (2020) proposes the FineGym dataset which contains 10 event categories, 303 competition records and provides coarse-to-fine annotations both temporally and semantically. Competitive sports videos contain both rich semantic action information and strict human body motions. Similar to the FineGym and Diving48, our SMART dataset contains per-frame annotated action labels. In addition to the fine-grained semantic labels, we further add manually annotated human pose, MoCap pose space of each fine-grained action, and action assessment from professional referees. To our knowledge, the SMART dataset is the only one that provides the fine-grained semantic labels, 2D and 3D annotated poses, and assessment information.

Fig. 2
figure 2
Our SportsCap is composed of two main components: the Motion Embedding Module and the Action Parsing Module. Motion Embedding Module estimates motion embedding information, 3D joints, and 3D body meshes, while Action Parsing Module predicts the fine-grained semantic attributes and final action labels of sports

Full size image
Overview
This paper aims to reconstruct both the 3D human motion and the corresponding fine-grained action attributes from monocular professional sports video input. To handle this challenging problem, our SportsCap splits each professional motion into a sequence of elementary sub-motions, and utilizes the motion manifold prior of these sub-motions in a multi-task learning framework, as illustrated in Fig. 1. Our approach not only captures the fine 3D motion details for each sub-motion, but also provides detailed motion understanding attributes, such as the action type and rotation angle in Fig. 1. To model this motion capture and understanding problem in a data-driven manner, we collect a new Sports Motion and Recognition Tasks (SMART) dataset. It contains various challenging sports video clips, manually annotated ground truth poses and fine-grain action labels, and the corresponding relevant 3D poses captured via a motion capture system. A brief introduction of our pipelineâ€™s two main components is provided as follows, which explores and proves the mutual gain between 3D motion capture and fine-grained motion understanding.

Motion Embedding Module To handle a challenging sports video, we first propose a motion embedding space to model the manifold of plausible human poses for each sub-motion via the PCA technique. Based on such embedding prior, we further introduce a novel network to estimate the per-frame implicit motion embedding parameters so as to recover the 3D motion details, including the pose, shape parameters of the human statistical model SMPL Loper et al. (2015) and camera parameters. Our embedding module consists of a sub-motion classifier, a CNN encoder to regress the embedding, and the corresponding mapping function from the embedding space to the 3D motion output (see Sect. 4.1).

Action Parsing Module For further motion understanding tasks, we predict the fine-grained action attributes for the whole motion sequence using a novel multi-stream ST-GCN module, which makes full use of both the implicit pose embedding and the explicit 3D joints from the previous capture stage. We further propose a semantic attribute mapping block to map the predicted attributes to the final action label, which enables various applications such as action number prediction (like diving or gymnastics number) for motion scoring and action assessment.

Technical Details of SportsCap
Figure 2 shows the architecture of our SportsCap, which takes a sports video as input and reconstructs both 3D motion details and accurate action attributes in an end-to-end multi-task manner. We assume the input video clip corresponds to the complete sports motion and split it into several sub-motions (such as Fig. 7a), which are segments that correspond to its sports stages similar to previous work (Hu and Qi, 2019). Our SportsCap consists of two modules for both per-frame 3D pose/shape reconstruction and action understanding, such as fine-grained labeling and action assessment. The recovered pose and shape parameters can be further applied to drive a 3D parametric human model to conduct the same sports movement in 3D. For motion capturing, we construct their corresponding motion embedding functions for respective segments where each frame is fed to its respective encoder to obtain its motion embedding information, joints, and bones. For action labeling, we construct a multi-stream ST-GCN for multi-task action attribution prediction, which takes the coefficients, joints, and bones as input. Our ST-GCN contains an attributes mapping block that assembles action attributes into the final label, which indicates an action number or score.

Fig. 3
figure 3
Visualization on motion embedding pose spaces of 2D poses, 3D poses, and pose parameters of SMPL by changing the pose coefficients: a 2D Pose embedding spaces for somersault and entry (two sub-motions of competitive diving). b Motion embedding spaces of 3D poses and pose parameters of SMPL for boxing. For each sub-motion, from left to right, we show the input frame and the first three principal components(PC) within 0.5 or 1 deviations of pose coefficients from the mean. The lines connecting the corresponding elements within the component indicate the linear change according to the basis

Full size image
Motion Embedding and Capturing
Professional poses in sports have complex structure information and always bring occlusions in monocular videos, which impose significant challenges to existing pose/shape estimators such as OpenPose (Cao et al., 2019), SimpleBaseline (Xiao et al., 2018), HMR (Kanazawa et al., 2018) and VIBE (Kocabas et al., 2020). Figure 10 shows some typical results. This is partially due to the pose variants in sports. More importantly, those approaches do not explore the specific semantic and structural constraints in sports. We thus present a novel motion embedding space of each specific sport, to model the manifold of plausible human poses for each sub-motion via the PCA technique, and use the motion embedding network to estimate the per-frame implicit embedding parameters so as to recover the 3D motion details.

We first recognize that the complete sport move in profession always follows several stages, called sub-motions. A sub-motion is a segmentation of the video sequence in the temporal domain, according to movement regularity and semantically meaningful. For example, boxing action can be segmented into three sub-motions: punching, kicking, and dodging, and the diving action has four sub-motions, as shown in Fig. 7a). In each sub-motion, the poses exhibit high resemblance across athletes, e.g., divers straighten their bodies in twisting while curling up in somersault. To achieve this, we use an accurate and effective classifier, WS-DAN (Hu and Qi, 2019), to segment sub-moves. Considering these sports characteristics, we then construct a motion embedding function for each sub-motion to capture the structural similarities.

To build the motion embedding space, we follow the successful parametric model, Skinned Multi-Person Linear model (SMPL) (Loper et al., 2015), which represents the pose parameters (rotation vectors) as ğœƒğœƒ. However, different from SMPL or other parametric pose/shape models (Romero et al., 2017; Pavlakos et al., 2019), we propose the pose coefficient ğ›¼ğ›¼ (see Fig. 3) to leverage the rich semantic and temporally structural prior of sub-motions in the motion embedding space. Specifically, for a sub-motion m, the motion embedding function is formulated as follows:

ğœƒğœƒ=îˆ¹ğ‘š(ğ›¼ğ›¼),
(1)
ğœƒğœƒ=âˆ‘ğ‘˜=1ğ¾ğ›¼ğ‘˜ğ›ğ‘šğ‘˜+ğšğ‘š=ğ›¼ğ›¼âŠºğğ‘š+ğšğ‘š,
(2)
where îˆ¹ğ‘š(ğ›¼ğ›¼):â„ğ¾â†¦â„3ğ‘, N denotes the joint number of SMPL, and K denotes the dimension of pose coefficients ğ›¼ğ›¼. ğšğ‘š is the mean of pose parameters and ğ›¼ğ›¼=[ğ›¼1,â‹¯,ğ›¼ğ¾]âŠº are the pose coefficients. Although the poses of the sports are challenging, the similarity of the poses in a sub-motion shows a desirable feature on lower-dimension. Thus, we adopt the Principal Component Analysis (PCA) to model the pose space of each sub-motion. We collect a MoCap dataset with more than 50 thousand frames to provide the set of pose parameters {ğœƒğœƒğ‘–} and conduct PCA on {ğœƒğœƒğ‘–} to generate a set of pose bases ğğ‘š={ğ›ğ‘šğ‘˜}ğ¾ğ‘˜=1, so that ğœƒğœƒ under sub-motion m can be represented as a linear combination of the bases. With our MoCap data, we calculate all pose bases {ğğ‘š,ğšğ‘š} for each sub-motion m before our training parts.

We not only formulate the motion embedding function îˆ¹ğ‘š(ğ›¼ğ›¼) on pose parameters ğœƒğœƒ in Eq. 2, but also formulate it on 2D or 3D joints as ğ‰=îˆ¹â€²ğ‘š(ğ›¼ğ›¼). We use Fig. 3 to visualize the motion embedding spaces on 2D joints, 3D joints, and pose parameters, namely (ğ‰2ğ·,ğ‰3ğ·,ğœƒğœƒ). It describes the variation on first three pose bases {ğ›ğ‘šğ‘˜}3ğ‘˜=1 in two sub-motions of diving and boxing, where mean denotes ğšğ‘š of these pose variables, for example, +0.5 std on PC1 indicates ğ›¼ğ›¼={0.5,0,...,0} in Eq. 2. The approach reduces the dimension of poses, benefiting for training, and regression. It can also robustly handle all sub-motions even for traditionally challenging poses because of the extracted structure of pose spaces. The resulting pose coefficients representation also encodes semantic meanings (see Fig. 5), like rotation angle, important for subsequent action parsing Sect. 4.2.

From the pose bases for all sub-motions, we construct the Motion Embedding Module that estimates the pose coefficients and 3D joints. This motion embedding representation can be suitable for many kinds of backbones. In our case, Motion Embedding Module consists of a ResNet-152 convolutional encoder followed by two fully connected layers to regress the pose coefficients ğ›¼ğ›¼ used to reconstruct the joint positions:

ğ›¼ğ›¼(ğ±)=îˆ²ğ‘šğ‘ğ‘œğ‘›ğ‘£(ğ±;ğ–),
(3)
ğ‰(ğ±)=ğ›¼ğ›¼(ğ±)âŠºğğ‘š+ğšğ‘š,
(4)
where ğ± denotes input image/frame and îˆ²ğ‘šğ‘ğ‘œğ‘›ğ‘£ is the motion embedding network for the sub-motion m. of shape capturing. We utilize this network to estimate pose coefficients, shape parameters and camera parameters (see Eq. 6) from images. Then, we recover 3D human body meshes from estimated pose and shape parameters of SMPL, the parametric human model.

Unlike prior approaches that target at general poses by implicitly encoding pose regularity into a complex network and hence cannot easily enforce semantic constraints, Motion Embedding Module manages to exploit the structural constraints in sport poses with action semantics. Even though we formulate the motion embedding function (Eq. 2) with the pose parameters of SMPL, Motion Embedding Module can also be applied to other joint representations, like 2D/3D joint location/rotation, which is used in our experiment of 2D pose estimation (Fig. 10).

Action Parsing
We then provide the estimated pose coefficients and 3D joint positions of all frames from Motion Embedding Module to the Action Parsing Module for analyzing the complete action. It includes inferring semantic meaningful labels and the action number(code) from the image sequence of sports, and later assessing the performance.

Specifically, we introduce Semantic Attributes (SAs) to represent the semantic meaningful label assumed by the sport guidelines/rules. The action number represents a valid combination of SAs, an overall description of a sports action. For example, as shown in Fig. 7, the five SAs for a diving action are the take-off type, twisting number, somersault number, arm-stand, and dive position, while the specific action number consists of are a set of these SAs.

The brute-force approach would be to build a black-box network to map the pose sequence to the action number. Competitive sports have detailed defined elements and fixed semantic attribute types. So, we adopt a different approach that treats the action number as attributes of the action. Recall the action number encodes critical semantic meaning of the sport move in Fig. 7. We call them semantic attributes (SAs) and aim to learn how each frame contributes to respective attributes. Our Action Parsing Module explicitly recovers SAs via a two-stage architecture: in the first, we use a multi-Stream ST-GCN for SA predictions, and in the second, a attributes mapping block infers the action number or score from the SAs.

Fig. 4
figure 4
We show our spatial-temporal graph of the joints and bones. Right shows the spatial configuration partitioning strategy: we divide the nodeâ€™s one neighbor into three subsets: the root node (green dot), the centripetal subset (red dots), and the centrifugation subset (yellow dots), details in Li et al. (2018a)

Full size image
Spatial-Temporal Feature Extraction A number of previous approaches such as (Wen et al., 2019; Si et al., 2018; Zhang et al., 2019) exploit skeletons alone as inputs to the GCN. pang: Our pose coefficients encode meaningful semantic sub-motion using proposed motion embedding analysis. In addition to skeletons (bones and joints), pose coefficients obtained from the Motion Embedding Module provide useful information on action parsing, as shown in Table 6. We thus construct a multi-Stream convolutional module that takes joints (J-stream), bones (B-stream), and pose coefficients (P-stream). For J- and B-Stream, we adopt the 2s-AGCN structure that can adaptively learn graph edge connections. Details on graph construction and partitioning are shown in Fig. 4. Specifically, we adopt the human joints and bones setting in OpenPose (Cao et al., 2019). In the J-stream, the joints are mapped to graph nodes, and the bones map to edges. In the B-Stream, the mappings are reversed. Note that we feed 90 consecutive frames of skeletons into a 10-layer ST-GCN to extract two feature vectors. For the P-stream, we represent pose coefficients as a 1D vector and use layers of 1D convolution with residual blocks to generate features of 256 dimensions. We demonstrate the effectiveness of the proposed P-Stream. In Fig. 5, we visualize one of the feature maps of a specific sequence obtained from the P-Stream, at a resolution of 90Ã—25 (90 frames and 25 dimensions in feature). We also plot the first two dimensions vs. frame index. We observe that they can be readily used to infer the somersault number of competitive diving. We finally concatenate all feature vectors generated by the J-, B- and P-Stream as inputs to the following attributes mapping block.

Fig. 5
figure 5
Extracted feature maps of pose coefficients from the P-stream encode the semantic meaning of sport. Each feature map is of 90Ã—25 (90 frames and 25-dimensional features) pang: In each figure, the upper half is the principal components feature map of the entire sequence, and the lower half is the specific numerical curve of the first two principal components.. The first two principal components already reveal the number of rounds of half-somersault (left: 4Ã—2âˆ’1=7 rounds; right: 3Ã—2âˆ’1=5 rounds). Red arrows correspond to the start position of a somersault (toe pointing to the ground), whereas the final half-somersault corresponds to entry to the water

Full size image
Semantic Attributes Mapping Block The Semantic Attributes Mapping Block aims to learn the mapping between the extracted spatial-temporal features and the final action label, i.e., to tell which dive number or action scores the video corresponds to. Instead of directly learning the mapping via a black-box solution, we sought to use Semantic Attributes (SAs) explicitly. Specifically, our goal is to partition the whole action sequence in terms of the SAs, or more precisely, how individual frames contribute to specific SAs. We use two fully connected layers to predict their contributions where the categories of all SAs are represented as vectors. Finally, we stack the resulting SAs and feed the results to another two fully connected layers to infer the action number. Compared with black-box end-to-end approaches, our results show the use of intermediate SAs better supervise the training process, provide heuristic cues analogous to human labeling, and accelerate the training process. Moreover, decomposing the whole action into the SAs resembles human perception, which helps analyze sports videosâ€™ fine-grained actions.

Multi-task Training
To train our end-to-end multi-task network, we adopt a deeply-supervised strategy to design five losses for the Motion Embedding Module and Action Parsing Module.

Loss for Motion Embedding Module In our network, we use three types of representations, pose coefficients, pose parameters and 3D joints, to model poses. The pose coefficients define the motion embedding space of the pose, whereas the joint positions better describe the visibility between joints within an image. We therefore design the prior loss (the pose coefficients loss) îˆ¸ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ as:

îˆ¸ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ=â€–â€–ğ–(ğ›¼ğ›¼â¯â¯â¯â¯âˆ’ğ›¼ğ›¼^)â€–â€–2,
(5)
where ğ›¼ğ›¼â¯â¯â¯â¯ is the mean of pose coefficients in training set. ğ›¼ğ›¼^ is the predicted pose coefficients. Then, ğ– is the weights, which calculates from the eigenvalues of a covariance matrix in the calculation of pose embedding bases. We use the smaller weight for the larger eigenvalue to enable more tolerance on principal components. We design the data loss (the 2D joint position loss) îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘ as:

ğ‰Ì‚ =ğ‘ Ì‚ ğ›±(îˆ¶(îˆ¹ğ‘š(ğ›¼ğ›¼^),ğ›½ğ›½^))+ğ‘¡Ì‚ ,
(6)
îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘=â€–â€–ğ•(ğ‰âˆ’ğ‰Ì‚ )â€–â€–2,
(7)
where ğ‰ is the ground truth 2D joints and ğ• indicates the visibility of the ground truth joint. Pose parameters ğœƒğœƒ^=îˆ¹ğ‘š(ğ›¼ğ›¼^) is recovered from our embedded pose coefficients ğ›¼ğ›¼^, using this motion embedding function of the sub-motion m. 3D joints îˆ¶(ğœƒğœƒ^,ğ›½ğ›½^) are obtained by linear regression from the final mesh vertices of SMPL. We then follow Kanazawa et al. (2018) using a weak-perspective project system with only scale s, translation parameters ğ‘¡,ğ‘¡âˆˆâ„2, and the orthographic projection function ğ›±(â‹…). We design the SMPL loss as:

îˆ¸ğ‘ ğ‘šğ‘ğ‘™=â€–â€–â€–ğœƒğœƒâˆ’ğœƒğœƒ^â€–â€–â€–2+â€–â€–â€–ğ›½ğ›½âˆ’ğ›½ğ›½^â€–â€–â€–2,
(8)
where ğœƒğœƒ,ğ›½ğ›½ are the supervision of pose/shape parameters, which are obtained through MoSh (Loper et al., 2014) and provided mocap data.

We combine these three loss terms, the prior loss of our motion embedding, the data loss, and the pose/shape parameter loss of SMPL with the corresponding weights ğœ†ğ‘‘ğ‘ğ‘¡ğ‘, ğœ†ğ‘ ğ‘šğ‘ğ‘™ (10 and 2 in our case) as the final loss of the Motion Embedding Module:

îˆ¸ğ‘šğ‘’ğ‘š=îˆ¸ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ+ğœ†ğ‘‘ğ‘ğ‘¡ğ‘îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘+ğœ†ğ‘ ğ‘šğ‘ğ‘™îˆ¸ğ‘ ğ‘šğ‘ğ‘™.
(9)
Loss for Action Parsing Module For the Action Parsing Module, we use the cross-entropy loss between the predicted and the ground truth attributes, which can be written as follows,

îˆ¸ğ‘ğ‘¡ğ‘¡ğ‘Ÿ=âˆ‘ğ‘=1ğ‘ğ‘ âˆ‘ğ‘–=1ğ‘ğ‘ğ‘¦ğ‘ğ‘–log(ğ‘¥ğ‘ğ‘–),
(10)
where c indicates the attribute type, ğ‘ğ‘  denotes the number of attributes, and ğ‘ğ‘ is the categories of each attribute c.For example, the number of somersault in diving has 10 categories, indicating the rotation angle for 0 to 3240 (0âˆ—360 to 9âˆ—360) degrees. Here ğ‘¥ğ‘ğ‘– denotes the prediction for the i-th label of attribute c whereas ğ‘¦ğ‘ğ‘– is the ground-truth.

For the action labeling task, we also add the cross-entropy loss between the prediction and the ground truth action label as below. We note that such a task loss depends on the target application and can be easily adjusted according to the final task.

îˆ¸ğ‘¡ğ‘ğ‘ ğ‘˜=âˆ‘ğ‘—=1ğ‘ğ‘“ğ‘¦ğ‘—log(ğ‘¥ğ‘—),
(11)
where ğ‘ğ‘“ denotes the total number of all possible action labels (action number or score), and ğ‘¥ğ‘—,ğ‘¦ğ‘— are the prediction and ground truth of the j-th final label.

The overall loss for the Action Parsing Module is a combination of the attribute loss and the task loss as following:

îˆ¸ğ‘ğ‘ğ‘š=îˆ¸ğ‘ğ‘¡ğ‘¡ğ‘Ÿ+ğœ†ğ´îˆ¸ğ‘¡ğ‘ğ‘ ğ‘˜,
(12)
where ğœ†ğ´ (2 in our case) is a weight to balance two loss terms.

Training Strategy While our entire network can be trained in an end-to-end fashion, we exploit its modular architecture and develop a stage-wise strategy, which is more efficient in practice. Specifically, our training procedure is composed of three stages: 1) We train a Motion Embedding Module for each sub-motion independently and fix its parameters, 2) We then train the action attribute prediction and label classification modules in the Action Parsing Module jointly, and 3) Finally we fine-tune the entire network using the combined losses of Motion Embedding Module and Action Parsing Module (i.e., îˆ¸ğ‘šğ‘’ğ‘š+îˆ¸ğ‘ğ‘ğ‘š).

Fig. 6
figure 6
Our SMART dataset contains both pose and action annotation. The upper part shows specific categories of sport. The lower part depicts the sub-motions of each sport and its typical poses

Full size image
Experimental Results
In this section, we evaluate our SportsCap in a variety of challenging scenarios. We first report a new proposed dataset and the training details of our approach on the utilized datasets, followed by evaluating our main technical components, including both qualitative and quantitative comparisons with previous state-of-the-art methods on both motion capture and action parsing tasks. Finally, limitations and discussions regarding our approach are provided.

Dataset
Many datasets exist for human action analysis, such as the MPII (Andriluka et al., 2014) and COCO (Lin et al., 2014) for human pose in daily activities, the PennAction (Zhang et al., 2013) for coarse sport recognition, and the AQA (Parmar and Morris, 2019b) and FineGym (Shao et al., 2020) dataset for fine-grained action recognition. We propose a challenging sports dataset called Sports Motion and Recognition Tasks (SMART) dataset, which contains per-frame action labels, manually annotated pose and action assessment of various challenging sports video clips from professional referees. We also collect the human pose data in sports activities using a marker-based motion capture system (see Fig. 8) to provide pose prior to our Motion Embedding Module. To our best knowledge, our SMART dataset is the most complete dataset for human motion capture and action analysis for sport video (see Table 1).

Our SMART dataset consists of both competitive sports and daily exercise videos (see Fig. 6), including balance beam, competitive diving, uneven bars, vault-women, hurdling, pole vault, and high jump in competitive sports, and boxing, keep-fit, and badminton in the daily exercise category. The SMART has 640 videos (110K frames) in total, with per-frame skeleton annotation and sub-motion labels, semantic attribute labels, and action assessment scores for competitive sports. There are about 450,000 annotated skeletons, 25 joints like OpenPose(Cao et al., 2019), with corresponding bounding boxes. In addition to joint locations, we also annotate the visibility of each joint as three types: visible, labeled but not visible, and not labeled, same as COCO (Lin et al., 2014). To fulfill our goal of 3D pose estimation and fine-grained action recognition, we collect two types of annotations, i.e. the sub-motions (SMs) and semantic attributes (SAs), as we described in Sects. 4.1 and 4.2 and Fig. 7a, b. We also include the difficulty scores, the number of valid referees, the execution scores, and the final assessment scores for competitive sports in the SMART dataset. The action labels (not include joints) of gymnastics sports in SMART dataset are from FineGym (Shao et al., 2020). All the other annotations, including joints and action labels, are manually generated with professional cross-validation between more than two individuals to guarantee the annotation accuracy. We will share our SMART dataset with the community.

Table 1 Comparison between the SMART dataset and existing datasets, including MPII-Pose (Andriluka et al., 2014), Penn Action (Zhang et al., 2013), COCO (Lin et al., 2014), AQA (Parmar and Morris, 2019b) and FineGym (Shao et al., 2020), regarding size, per video action labels, pose annotation, action assessment, and pose type
Full size table
Fig. 7
figure 7
Two important types of label annotations in the SMART dataset, sub-motions (SMs) and semantic attributes (SAs). We introduce SAs and SMs of competitive diving as example. a SMs indicate the different pose spaces, usually different stages in a sport, like somersault and twisting in this case. b SAs indicate the specific number of a motion, like the rotation angle, take-off type and so on

Full size image
In addition to the annotated video data, we also collect the 3D human pose data using a motion capture system to provide pose motion prior for our Motion Embedding Module. Specifically, we adopt the Vicon system (a 12 views marker-based motion capture system) to capture the rich human pose sequences and hire 30 athletes and two professional fitness instructors as our performers. The performers move according to the corresponding sport guidelines to make sure their body movements cover as many sub-motions of the sports as possible (except the motions that are impossible to execute in the capture environment). For the generalization purpose, each performer repeats the movements two to five times, and the same motion is captured from more than two different subjects. Since only the relative motion matters, we convert the skeleton results from Vicon to the SMPL pose parameters to avoid the variations imposed by the absolute lengths of bones. In total, we collect more than 500,000 motion frames of 30 performers covering nine activities, about 1000 frames for each performance.

With the annotated 2D poses and MoCap 3D pose data, we collect the Sports Motion Embedding Spaces according to our motion embedding function (Eq. 2) and use it as the pose priors for sports videos. Currently, Sports Motion Embedding Spaces provides the priors on 2D joints, 3D joints location, and pose parameters of SMPL for nine sports, as shown in Table 1, and we are planning to add more sports in the near future. Because of the regularity of the human body motion in sports/exercises, the Sports Pose Embedding Spaces provides strong prior and regularization to ensure that the generated pose result lies in the corresponding action space. The Sports Pose Embedding Spaces greatly improves the accuracy and robustness of the 2D/3D pose estimation, human body capturing, action recognition/parsing, and action assessment tasks as described in Sect. 4.1. The Sports Pose Embedding Spaces data will be included in the SMART dataset.

Training Details
Our Motion Embedding Module relies on both the fine-grained action labels and pose information. Therefore, we first train and test the Motion Embedding Module on the SMART dataset as few other datasets provide both information. Then we fix the Motion Embedding Module and train the complete SportsCap on SMART, AQA (Parmar and Morris, 2019b) and FineGym dataset (Shao et al., 2020) for 3D sports motion estimation and action understanding.

We resize image patches that contain the human body at a resolution of 256Ã—256 (using the ground truth bounding box in our SMART dataset and detect the bounding box in AQA Parmar and Morris, 2019b using Liu et al., 2016). We re-sample the video to 90 frames each. For Motion Embedding Module training, we conduct data augmentation via random rotations (âˆ’45âˆ˜ to +45âˆ˜), random scaling (0.7 to 1.3), and flipping horizontally. For Action Parsing Module training, we also augment the skeleton and pose parameters data for J-, B- and P-Streams, respectively. For J-Stream and B-Stream, we scale the joint positions via interpolation to simulate the far and near camera views. For the P-Stream, we also scale the coefficients vector.

We use the Adam optimizer (Kingma and Ba, 2015), train the Motion Embedding Module in the first 100 epochs and AMP in the following 50 epochs. We train the complete SportsCap in the last ten epochs. The learning rates of the 0th, 70th, and 150th epoch are 10âˆ’3, 10âˆ’4, and 10âˆ’5, respectively. We train our SportsCap on 4 NVidia 2080Ti GPUs, and the process takes 10 hours for the Motion Embedding Module, 5 hours for Action Parsing Module, and 2.5 hours for the whole SportsCap. Once trained, the network processes the 90Ã—256Ã—256 video data at 0.5s for Motion Embedding Module, 0.05s for Action Parsing Module, and 1.0s for the data fetching.

For fair comparisons, we re-train HRNet (Sun et al., 2019) and SimpleBaseline (Xiao et al., 2018) using our SMART dataset. For SCADC (Parmar and Morris, 2019b), C3D-LSTM (Parmar and Tran Morris, 2017), C3D-AVG (Parmar and Morris, 2019b), and R2+1D (Tran et al., 2018), we first pre-train the corresponding networks using the UCF101 dataset (Soomro et al., 2012) and I3D (Sun et al., 2019) on the Kinetics dataset (Sun et al., 2019), replace their output or the regression layers with our proposed Semantic Attributes Mapping Block module, and fine-tune Semantic Attributes Mapping Block with our SMART dataset.

Evaluation on 3D Motion Capture
In this sub-section, we evaluate our SportsCap approach with three motion-relevant tasks, including the motion embedding module evaluation, comparison of motion capture, and the sub-motion classification (Fig. 8).

Fig. 8
figure 8
The motion capture system. The structure of dome is shown in the left and the motion capture camera is shown in the bottom right. Our system includes 12 multi-view motion capture cameras. The top right figure shows the motion cap suit with 63 marks. With professional performers, we utilize this system to capture 3D challenging motion of sports

Full size image
Fig. 9
figure 9
Cumulative relative variance of our sports dataset explained as a function of the number of pose coefficients. a and b are the sub-motions of diving with different metrics. â€œAllâ€ refers to use all training data rather than restricted to each semantic pose space. c and d are general motion embedding of gymnastics and daily sport. PCKs indicates the percentage of correct keypoints (see details in Andriluka et al., 2014), and the ratio follows (Loper et al., 2015)

Full size image
Fig. 10
figure 10
The gallery and comparison of our experiments on 2D pose estimation. With 2D pose embedding spaces, we show the comparison with fine-tuned HRNet (Sun et al., 2019) (The first column) on SMART dataset. Our results (start from the second column) are more robust and reliable under challenging poses and motion blur

Full size image
Fig. 11
figure 11
3D human shape recovery results on challenging sport videos. For each type of sport, the top row shows the input images, the middle row shows the recovered body mesh, and the bottom row shows the rendering result of the recovered body from an alternative view

Full size image
Fig. 12
figure 12
Qualitative comparison with the state-of-the-art human shape recovery methods. a The input images. bâ€“d are the results of HMR (Kanazawa et al., 2018), VIBE (Kocabas et al., 2020), and SMPLify-X (Pavlakos et al., 2019) respectively. We fine-tune HMR and VIBE on our training set, and provide the 2D poses of our method for SMPLify-X. e Our results of SportsCap On SMART dataset

Full size image
Table 2 Ablation study of our Motion Embedding Module: PCA w/o sub-motion labeling, training w/o multi-task learning (w/o action parsing module), 50/101/152 ResNet as Backbone, and îˆ¸ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ/îˆ¸ğ‘‘ğ‘ğ‘¡ğ‘ as loss for training
Full size table
Motion Embedding Module evaluation There are two ablation results, Fig. 9 and Table 2. The first experiment verifies that it is more effective to use motion embedding analysis for different pose spaces by comparing PCA analysis in different pose spaces. Another experiment validates our Motion Embedding Moduleâ€™s specific designs and the mutual gain between 3D motion capture and fine-grained motion understanding. It illustrates that mutual gain will make the 2D keypoint projection results more accurate.

In Fig. 9, PCA analysis demonstrates that the poses of each sub-motion lay in a low-dimensional parametric space, which is similar to the low-dimensional shape space in SMPL (Loper et al., 2015). In Fig. 9a, b, we use two metrics, the relative cumulative variance ratio and PCK-0.3, to evaluate semantic/general pose spaces from the training and testing sets, respectively. Hence, under the same dimension of pose coefficients, each semantic pose spaceâ€™s pose spaces show better accuracy than the general pose space. This proves that motion embedding analysis on each sub-motion is necessary and effective. Thus, compared with conducting PCA on the complete action data, we further reduce the space dimension by cooperating with the different sub-motion labels.

In Table 2, we evaluate our Motion Embedding Module quantitatively from the ablation study on the SMART dataset. We project the 3D joints with camera parameters and consider the predicted results with distance errors less than 0.3 and torso length errors less than 0.5 as the correct predictions (see details in Andriluka et al., 2014) and report the percentage of correct keypoints (PCK-0.3 and PCK-0.5) as our metrics. We train Motion Embedding Module without the prior loss provided by Sports Pose Embedding Spaces (with îˆ¸ğ‘ğ‘Ÿğ‘–ğ‘œğ‘Ÿ), which is an approximately 5% drop in accuracy. We also train this module w/o Action Parsing Module (Multi or Single), which is above 1.2% improvement on PCK-0.5 from this multi-task learning. To follow the PCA analysis on sub-motion poses in Fig. 9, we also use General or Semantic pose spaces in different trainings. We further test ResNet-50, -101, -152 as our backbones for the encoder and find that about every 50 more layers lead to an above 2% increase for PCK-0.3. This experiment demonstrates the effectiveness of the Sports Motion Embedding Spaces and the improvement of the fine-grained pose embedding spaces.

Comparison We first evaluate our approach with the qualitative results on pose estimation and 3D motion capture in Figs. 10 and 11, to show our generalization on various sports and environments. For qualitative comparison, in Fig. 12, we compare our method with the state-of-the-art 3D shape recovery methods. We fine-tune HMR (Kanazawa et al., 2018) and VIBE (Kocabas et al., 2020) with our dataset, and provide our results on 2D poses for SMPLify-X (Pavlakos et al., 2019). Furthermore, we show both the recovered human shape rendered at the original and alternative viewpoints. As shown in Fig. 12, for challenging sports movements, i.e., the handspring and somersault, both HMR and VIBE perform poorly while ours achieve much more accurate estimation. In contrast, our method has the ability to recover the limbs with higher fidelity than HMR and VIBE, as denoted by the colored box. The SMPLify-X method provides reasonable 2D keypoint estimations but does not produce as good 3D shape results. Our approach outperforms all methods by generating both accurate 2D keypoints and well-matched 3D human shapes.

For quantitative comparison, we still use PCK-0.3 and PCK-0.5 as our metrics. Table 3 shows the performance of the Motion Embedding Module compared with the state-of-the-art pose estimation work, including the HRNet (Sun et al., 2019) and the SimpleBaseline (Xiao et al., 2018). We also compare with state-of-the-art 3D human shape recovery methods, including the HMR (Kanazawa et al., 2018) and VIBE (Kocabas et al., 2020). For HMR and VIBE, we project the joints of the recovered 3D human models into images and then use the projected joints as their predicted poses. For a fair comparison, we re-train the HRNet, SimpleBaseline, and et.al. on the SMART dataset. Note that we also provide the results of similar sub-motions (SM-1 to SM-4) from different sports to evaluate on various poses, especially these complex and challenging poses. Please refer to Table 4 for these specific sub-motions.

Table 3 The comparisons with different methods trained on the same SMART dataset: HRNet (Sun et al., 2019), SimpleBaseline (Xiao et al., 2018), HMR (Kanazawa et al., 2018), and VIBE (Kocabas et al., 2020) of our Motion Embedding Module
Full size table
As shown in Table 3, our approach outperforms the other baselines, especially on these challenging sport poses. HRNet (Sun et al., 2019) and SimpleBaseline(Xiao et al., 2018) perform better on the performance of SM-1 (Take, Run, Mount in Table 4), these more general poses, but can hardly handle other complex sub-motions. Similarly, VIBE and HMR perform better on SM-1, which is the preliminary motion of sports, while performing worse on others, which is the twist/ turning/ handspring motion. Twisting and other professional motions involve fast rotation and flipping of the body, while our proposed motion embedding from PCA has structure constraints on each sub-motion pose. With the real human poses as templates, our method achieves higher accuracy, leading by 4.3âˆ¼4.9% on PCK-0.3, and 7.1âˆ¼8.5% on PCK-0.5. It indicates our approach is more accurate in generating joint locations and the joints are more natural and stable.

Table 4 Performance of the fine-grained sub-motion classification using the WS-DAN (Hu and Qi, 2019) on competitive diving video
Full size table
Sub-motion classification We also provide more evaluations of our sub-motion classifier on various sports. Notes that our Motion Embedding Module aims to embed the pose motion within a certain sub-motion of sports actions into parametric space. It relies on both the sub-motion labels and per-frame annotated pose for evaluation, we thus evaluate the classifier (Hu and Qi, 2019) on predicting the sub-motion label. We observe this technique can achieve high accuracy, and the predicted sub-motion label helps the Motion Embedding Module for pose and shape recovery. Table 4 shows our sub-motion classification produces an average accuracy around 96% on various sports.

Evaluation on Fine-grained Action Understanding
We evaluate the Action Parsing Module of the SportsCap and compare it with other state-of-the-arts on the SMART, AQA (Parmar and Morris, 2019b), and FineGym (Shao et al., 2020) datasets. These tasks include the fine-grained action parsing and the action assessment.

Table 5 Action parsing evaluation using state-of-the-art approaches vs. our method on the FineGym dataset (Shao et al., 2020)
Full size table
Table 6 Action parsing evaluation using state-of-the-art approaches vs. our Action Parsing Module method with joint/ joint+bone/ joint+bone+pose(coefficients) streams, on the SMART dataset and the AQA dataset (Parmar and Morris, 2019b)
Full size table
Fine-Grained Action Parsing In Table 5, we compare the SportsCap with the tested approaches in FineGym (Shao et al., 2020). Following their metrics and annotations, we test on the same fine-grained action labels with the mean accuracy. For annotations, e.g., Salto backward stretched with 2 twist is decomposed to Salto backward, stretched, and 2 twist (three specific SAs), please refer to FineGym (Shao et al., 2020) for the more details of â€œannotating element labelsâ€. We also use the motion embedding analysis under our 3D motion capture data. and please refer to FineGym Shao et al. (2020) for these compared approaches, Random in Shao et al. (2020), ST-GCN (Yan et al., 2018), and TRN-2stream (Wang et al., 2018). It can be seen from the experimental results that our method performs 3-4% improvement on the FineGym dataset than ST-GCN that also uses pose information. This is mainly because our 3D motion capture data and motion embedding analysis can better parse this type of sports motion under specific pose spaces.

In Table 6, we first show the ablation study on the multi-stream structure and Semantic Attributes Mapping Block (SAMB) of Action Parsing Module. We evaluate all results with the Top-1 accuracy. Specifically, our multi-stream structure enables faster convergence. The 2s-AGCN structure (J- and B-Stream only) takes 70 epochs to converge, whereas the multi-stream structure converges after only 50 epochs. With SAMB, we further accelerate convergence to 10 epochs. For the effect of each stream, the result (Table 6 row 4-6) shows the use of P-Stream significantly improves the accuracy vs. baseline (Table 6 row 4, 5). Although the accuracy on the somersault attribute drops (Table 6 row 5), this is expected as the network easily focuses only on one attribute without structure like SAMB. We further keep the multi-stream backbone but replace SAMB with Black-Box without using attribute loss îˆ¸ğ‘ğ‘¡ğ‘¡ğ‘Ÿ. The one with Black-Box converges after over 30 epochs with 78.0% accuracy. In contrast, the network with SAMB converges after ten epochs with 82.2% accuracy.

We also compare the overall action parsing performance of diving with methods including MSCADC (Parmar and Morris, 2019b), C3D-LSTM (Parmar and Tran Morris, 2017), C3D-AVG (Parmar and Morris, 2019b), I3D (Carreira and Zisserman, 2017) and R2+1D (Tran et al., 2018) in Table 6. We still use the Top-1 accuracy as the metric. For the processing on the SMART dataset, we use the same strategy for all methods. Specifically, we sample 90 frames for a video clip. For multiple predictions, we also use multi-task blocks like ours. For AQA and FineGym dataset, we also sample each clip to 90 frames. Also, like them, we regard the fine-grained action recognition as a regression problem, to decompose all SAs to several to dozens of labels. Thus, we can compare with these approaches by regressing these labels. For the SMART dataset, SportsCap achieves the highest accuracy with 82.2% Top-1 accuracy. It proves that using the pose coefficient from motion embedding benefits action parsing with P stream. For AQA dataset, we also compare SportsCap with Nibaili (Nibali et al., 2017). SMART achieves slightly better performance as C3D-AVG and outperforms MSCADC and Nabaili. It shows that our motion embedding method is effective not only on our dataset but also on other datasets with the same pose space.

Table 7 Action assessment comparisons on SMART and AQA dataset (Parmar and Morris, 2019b), and compared with C3D-LSTM (Parmar and Tran Morris, 2017) and R2+1D (Tran et al., 2018)
Full size table
Action Assessment We further evaluate our approach for overall detailed action assessment using the diving motion, which relies on the dive number for final motion scoring. We use spearmanâ€™s rank correlation (Sp. Cor.) (Pirsiavash et al., 2014) as the metric, and this action score can be regarded as a specific semantic attribute. Specifically, instead of treating it as a regression problem, we discretize the score range 0â€“100 to 49 labels evenly. We use the cross-entropy loss Eq. 10 and conduct the same training strategy with the SA learning. We train our Motion Embedding Module module on our dataset for the poses, while training and testing Action Parsing Module for execution score and final score on our dataset and AQA dataset, respectively. In Table 7, we show the result of the testing results, 61.7% on SMART and 86.2% on AQA.

Discussion
As the first novel trial to explore the problem of joint 3D human motion capture and fine-grained motion understanding from monocular challenging non-daily video input, the proposed SportsCap still owns limitations. We list these discussions as follows.

Failure Cases By tailoring our network for a specific subset of moves, our approach may generate erroneous estimations on degraded images. Figure 13 provides several representative failure cases, where the input image does not fall into any of the pre-defined pose categories, some body parts are severely occluded and invisible (e.g., head entry into the water), or the input image is incomplete due to clipping. We plan to improve the motion embedding function with visibility and similarity parameters to handle invisible parts and unusual poses.

Fig. 13
figure 13
Failure cases. From left to right: a The input image does not fall into any of the pre-defined pose categories. b Some body parts are severely occluded and invisible. c The input image is incomplete due to clipping

Full size image
Limitations Our approach, in essence, exploits the semantic action analysis, for human pose estimation. This is different from HRNet (Sun et al., 2018) or SimpleBaseline (Xiao et al., 2018) that separately predict individual joints. Consequently, our network, once trained, only tackles specific sports rather than general movements as in prior art. In addition, our approach may generate erroneous estimations for large body parts outside the viewport. Moreover, our method can tolerate common deviations from the standard movement as we purposely add such cases into the dataset. However, when an athlete makes rare mistakes, it is difficult for our method to detect and analyze the situation accurately. Lastly, like many other works, SportsCap only estimates the motion of a single person for each inference. Although sub-motion is not a strict definition of motion semantics, it is not well suitable for team sports. Hence, the motion labels and spaces for multi-player sports might be necessary, to handle semantic interaction and multiple occlusions.

Future Work Our current setup assumes a single video stream as input. In sports, it is common practice to show two or more video streams. In the future, we plan to combine multiple streams for a 3D pose/shape task. In addition, general human activities can always split into small sub-motions. Hence we plan to extend our work to general-purpose pose estimation through human action decomposition, including expanding our SMART dataset as a more general dataset. Besides, itâ€™s also an interesting direction to combine the NLP techniques to provide more natural and detailed illustrations and understanding for action assessment.

Conclusions and Discussion
We have presented the first approach for monocular markers-less 3D motion capture and understanding for professional non-daily motions and a new dataset consisting of various challenging sports video clips with rich manually annotated 2D/3D poses and fine-grain action labels. The key insight of our approach is to utilize the semantic and temporally structured sub-motion prior in the motion embedding space and formulate the joint motion capture and understanding task in a data-driven multi-task manner. Our motion embedding module achieves robust 3D motion details reconstruction from implicit motion embedding parameters, while our novel multi-stream ST-GCN, as well as the semantic attribute mapping block, enable accurate fine-grained semantic action attributes prediction for various understanding applications like action assessment or motion scoring. Our experimental results demonstrate the effectiveness of SportsCap for both compelling 3D motion capture and fine-grained semantic action attribute reconstruction in various challenging sports scenarios, which compares favorably to the state-of-the-arts. We believe that it is a significant step to enable robust 3D motion capture and fine-grained understanding, with many potential applications in VR/AR, gaming, action recognition, and performance evaluation for gymnastics, sports, and dancing.