Human motion prediction aims to forecast future human poses given a historical motion. Whether based on recurrent or feed-forward neural networks, existing learning based methods fail to model the observation that human motion tends to repeat itself, even for complex sports actions and cooking activities. Here, we introduce an attention based feed-forward network that explicitly leverages this observation. In particular, instead of modeling frame-wise attention via pose similarity, we propose to extract motion attention to capture the similarity between the current motion context and the historical motion sub-sequences. In this context, we study the use of different types of attention, computed at joint, body part, and full pose levels. Aggregating the relevant past motions and processing the result with a graph convolutional network allows us to effectively exploit motion patterns from the long-term history to predict the future poses. Our experiments on Human3.6M, AMASS and 3DPW validate the benefits of our approach for both periodical and non-periodical actions. Thanks to our attention model, it yields state-of-the-art results on all three datasets. Our code is available at https://github.com/wei-mao-2019/HisRepItself.

Access provided by University of Auckland Library

Introduction
Human motion prediction consists of forecasting the future poses of a person given a history of their previous motion. Predicting human motion can be highly beneficial for tasks such as human tracking (Gong et al. 2011), human-robot interaction (Koppula and Saxena 2013), and human motion generation for computer graphics (Levine et al. 2012; Kovar et al. 2008; Sidenbladh et al. 2002).

Fig. 1: Human motion prediction
figure 1
aims to forecast future human poses (>0ğ‘ ) given past ones. From top to bottom, we show the ground-truth pose sequence, the predictions of LTD (Mao et al. 2019) and those of our approach. The frames where LTD (Mao et al. 2019) yields larger errors on the arms and legs are highlighted with a blue and red box, respectively. Note that our results better match the ground truth than those of LTD (Mao et al. 2019)

Full size image
Traditional methods, such as hidden Markov models (Brand and Hertzmann 2000) and Gaussian Process Dynamical Models (Wang et al. 2008), have proven effective for simple motions, such as walking and golf swings. However, they are typically outperformed by deep learning ones on more complex motions. The most common trend in modeling the sequential data that constitutes human motion consists of using Recurrent Neural Networks (RNNs)(Martinez et al. 2017; Fragkiadaki et al. 2015; Jain et al. 2016). However, as discussed in (Li et al. 2018a), in the mid- to long-term horizon, RNNs tend to generate static poses because they struggle to keep track of long-term history. To tackle this problem, existing works (Li et al. 2018a; Gui et al. 2018) either rely on Generative Adversarial Networks (GANs), which are notoriously hard to train (Arjovsky and Bottou 2017), or introduce an additional long-term encoder to represent information from the further past (Li et al. 2018a). Unfortunately, such an encoder treats the entire motion history equally, thus not allowing the model to put more emphasis on the parts of the past motion that better reflect the context of the current motion.

In this paper, by contrast, we introduce an attention-based motion prediction approach that effectively exploits historical information by dynamically adapting its focus on the previous motions to the current context. Our method is motivated by the observation that humans tend to repeat their motion, not only in short periodical activities, such as walking, but also in more complex actions occurring across longer time periods, such as sports and cooking activities (Runia et al. 2018; Li et al. 2018b). Therefore, we aim to find the relevant historical information to predict future motion.

While Tang et al. (2018) have attempted to leverage attention for motion prediction, they achieved this in a frame-wise manner, by comparing the human pose from the last observable frame with each one in the historical sequence. As such, this approach fails to reflect the motion direction and is affected by the fact that similar poses may appear in completely different motions. For instance, in most Human3.6M activities, the actor will at some point be standing with their arm resting along their body. To overcome this, we therefore propose to model motion attention, and thus compare the last visible sub-sequence with a history of motion sub-sequences.

For periodical motions, such as walking and jogging, humans tend to repeat their full-body motion across long time horizons. However, for non-periodical motions, such as discussion and cooking, motion repetitiveness rather happens at the level of body parts. To model this, we explore the use of motion attention at three different levels: full pose, body parts, and individual joints.

We observe that, as they capture different kinds of motion repetitiveness, the effectiveness of each of these different levels of motion attention varies across different activities and sequences. To handle this, we therefore introduce a fusion model that combines different attention levels and focuses on the attention level best-suited for the current motion context.

When dealing with a time-related problem such as motion prediction, the question of how to encode the temporal information naturally arises. The most common trend consists of using Recurrent Neural Networks (RNNs) (Fragkiadaki et al. 2015; Jain et al. 2016; Martinez et al. 2017; Gui et al. 2018). However, as argued in (Gui et al. 2018; Li et al. 2018a), RNNs for motion prediction suffer from error accumulation and discontinuities between the last observed frame and the first predicted one. As an alternative, convolutions across time on the observed poses are used in (Butepage et al. 2017; Li et al. 2018a). The temporal dependencies that such an approach can encode, however, strongly depend on the size of the convolutional filters. To remove such a dependency, here, we introduce a drastically different approach to modeling temporal information for motion prediction. Inspired by ideas from the nonrigid structure-from-motion literature (Akhter et al. 2009), we propose to represent human motion in trajectory space instead of pose space, and thus adopt the Discrete Cosine Transform (DCT) to encode temporal information.

Another question that arises when working with human poses is how to encode the spatial dependencies among the joints. In Butepage et al. (2017), this was achieved by exploiting the human skeleton, and in Li et al. (2018a) by defining a relatively large spatial filter size. While the former does not allow one to model dependencies across different limbs, such as left-right symmetries, the dependencies encoded by the latter again depend on the size of the filters. In this paper, we propose to overcome these two issues by exploiting graph convolutions (Kipf and Welling 2017). However, instead of using a pre-defined, sparse graph as in (Kipf and Welling 2017), we introduce an approach to learning the graph connectivity. This strategy allows the network to capture joint dependencies that are neither restricted to the kinematic tree, nor arbitrarily defined by a convolutional kernel size.

Altogether, our overall framework represents each sub-sequence in trajectory space using the discrete cosine transform (DCT). We then exploit our motion attention at different levels as weights to aggregate the entire DCT-encoded motion history into a future motion estimate. This estimate is combined with the latest observed motion, and the result acts as input to a graph convolutional network (GCN), which lets us better encode spatial dependencies between the different joints. As evidenced by our experiments on Human3.6M (Ionescu et al. 2014), AMASS (Mahmood et al. 2019), and 3DPW (von Marcard et al. 2018), and illustrated in Fig. 1, our motion attention-based approach consistently outperforms the state of the art on short-term and long-term motion prediction by training a single unified model for both settings. This contrasts with our previous, state-of-the-art LTD model (Mao et al. 2019), which requires training different models for different settings to achieve its best performance. Furthermore, we demonstrate that our approach can effectively leverage motion repetitiveness in even longer sequences.

Our contributions can be summarized as follows. (i) We introduce an attention-based model that exploits motions instead of static frames to better leverage historical information for motion prediction; (ii) Our motion attention allows us to train a unified model for both short-term and long-term prediction; (iii) Our approach can effectively make use of motion repetitiveness in long-term history; (iv) It yields state-of-the-art results and generalizes better than existing methods across datasets and actions.

This article extends our previous works (Mao et al. 2019, 2020) in the following ways:

Instead of modeling attention on the full body only, as in (Mao et al. 2020), we study the use of attention at three different levels: full body, body parts, and individual joints. Our experiments evidence that different activities or sequences benefit from different levels of attention.

We introduce a fusion module that combines our multi-level attention mechanisms to achieve better performance than the full body pose-level attention model we proposed in (Mao et al. 2020).

Related Work
RNN-Based Human Motion Prediction
 RNNs have proven highly successful in sequence-to-sequence prediction tasks (Sutskever et al. 2011; Kiros et al. 2015). As such, they have been widely employed for human motion prediction (Fragkiadaki et al. 2015; Jain et al. 2016; Martinez et al. 2017; Gopalakrishnan et al. 2019). For instance, Fragkiadaki et al. (2015) proposed an Encoder Recurrent Decoder (ERD) model that incorporates a non-linear multi-layer feed-forward network to encode and decode motion before and after recurrent layers. To avoid error accumulation, curriculum learning was adopted during training. Jain et al. (2016) introduced a Structural-RNN model relying on a manually-designed spatio-temporal graph to encode motion history. The fixed structure of this graph, however, restricts the flexibility of this approach at modeling long-range spatial relationships between different limbs. To improve motion estimation, Martinez et al. (2017) proposed a residual-based model that predicts velocities instead of poses. Furthermore, it was shown in this work that a simple zero-velocity baseline, i.e., constantly predicting the last observed pose, led to better performance than (Fragkiadaki et al. 2015; Jain et al. 2016). While this led to better performance than the previous pose-based methods, the predictions produced by the RNN still suffer from discontinuities between the observed poses and predicted ones. To overcome this, Gui et al. (2018) proposed to adopt adversarial training to generate smooth sequences. Hernandez et al. (2019) treat human motion prediction as a tensor inpainting problem and exploit a generative adversarial network for long-term prediction. While this approach further improves performance, the use of an adversarial classifier notoriously complicates training (Arjovsky and Bottou 2017), making it challenging to deploy on new datasets.

Feed-Forward Methods and Long Motion History Encoding
In view of the drawbacks of RNNs, several works considered feed-forward networks as an alternative solution (Butepage et al. 2017; Li et al. 2018a; Mao et al. 2019). In particular, Butepage et al. (2017) introduced a fully-connected network to process the recent pose history, investigating different strategies to encode temporal historical information via convolutions and exploiting the kinematic tree to encode spatial information. However, similarly to (Jain et al. 2016), and as discussed in (Li et al. 2018a), the use of a fixed tree structure does not reflect the motion synchronization across different, potentially distant, human body parts. To capture such dependencies, Li et al. (2018a) built a convolutional sequence-to-sequence model processing a two-dimensional pose matrix whose columns represent the pose at every time step. This model was then used to extract a prior from long-term motion history, which, in conjunction with the more recent motion history, was used as input to an autoregressive network for future pose prediction. While more effective than the RNN-based frameworks, the manually-selected size of the convolutional window highly influences the temporal encoding.

Our work builds on our previous work (Mao et al. 2019), which showed that encoding the short-term history in frequency space using the DCT, followed by a GCN to encode spatial and temporal connections led to state-of-the-art performance for human motion prediction up to 1s. However, encoding long-term history in DCT yields an overly-general motion representation, leading to worse performance than using short-term history. In this paper, we overcome this drawback by introducing a motion attention based approach to human motion prediction. This allows us to capture the motion recurrence in the long-term history. Furthermore, in contrast to (Li et al. 2018a), whose encoding of past motions depends on the manually-defined size of the temporal convolution filters, our model dynamically adapts its history-based representation to the context of the current prediction.

Attention Models for Human Motion Prediction
While attention-based neural networks are commonly employed for machine translation (Vaswani et al. 2017; Bahdanau et al. 2015), their use for human motion prediction remains largely unexplored. The work of Tang et al. (2018) constitutes an exception, incorporating an attention module to summarize the recent pose history, followed by an RNN-based prediction network. This work, however, uses frame-wise pose-based attention, which may lead to ambiguous motion, because static poses do not provide information about the motion direction and similar poses occur in significantly different motions. To overcome this, we propose to leverage motion attention. As evidenced by our experiments, this, combined with a feed-forward prediction network, allows us to outperform the state-of-the-art motion prediction frameworks.

In a similar spirit to our approach, the concurrent work of Cai et al. (2020) leverages an attention-based transformer for human motion prediction. Nevertheless, their attention module mainly serves to model the global spatial dependencies among the joint trajectories. By contrast, our motion attention aims to capture the motion repetitiveness in history, thus modeling temporal motion dependencies. In addition to the attention-based module, Cai et al. (2020) proposed to progressively predict the joint trajectories with a dictionary which stores the global motion patterns of training data. These two components, however, are orthogonal to the attention-based module. Our experiments demonstrate that our method outperforms that of Cai et al. (2020) with only the attention-based module and is comparable to the full-model of Cai et al. (2020).

Our Approach
Let us now introduce our approach to human motion prediction. Let ğ—1:ğ‘=[ğ±1,ğ±2,ğ±3,â‹¯,ğ±ğ‘] encode the motion history, consisting of N consecutive human poses, where ğ±ğ‘–âˆˆğ‘ğ¾, with K the number of parameters describing each pose, in our case 3D coordinates or angles of human joints. Our goal is to predict the poses ğ—ğ‘+1:ğ‘+ğ‘‡ for the future T time steps. To this end, we introduce a motion attention model that allows us to form a future motion estimate by aggregating the long-term temporal information from the history. We then combine this estimate with the latest observed motion and input this combination to a GCN-based feed-forward network that lets us learn the spatial and temporal dependencies in the data. Below, we discuss these two steps in detail.

Motion Attention Model
As humans tend to repeat their motion across long time periods, our goal is to discover sub-sequences in the motion history that are similar to the current sub-sequence. We propose to achieve this via an attention model. To capture motion repetitiveness at different levels, we introduce a general framework that models attention on body parts. Specifically, a part can be the entire body, a human limb, e.g., the right arm, or an individual joint. This framework allows us to study different levels of attention, such as pose motion attention, part motion attention and joint motion attention.

To this end, we first divide each human pose ğ±ğ‘–âˆˆğ‘ğ¾ into P parts as

ğ±ğ‘–=â¡â£â¢â¢â¢â¢â¢â¢ğ±1ğ‘–ğ±2ğ‘–ğ±3ğ‘–â‹®ğ±ğ‘ƒğ‘–â¤â¦â¥â¥â¥â¥â¥â¥
where ğ±ğ‘ğ‘–âˆˆğ‘ğ¾ğ‘ concatenates the 3D coordinates (or rotation angles) of one body part and âˆ‘ğ‘ƒğ‘=1ğ¾ğ‘=ğ¾. In particular, ğ‘ƒ=1 corresponds to treating the entire human pose as a single part, ğ‘ƒ=ğ‘ğ½, with ğ‘ğ½ is the number of skeleton joints, means that each joint acts as a part, whereas ğ‘ƒâˆˆ(1,ğ‘ğ½) ranges between these two extreme cases, grouping multiple joints into a part.

Fig. 2: Overview of our motion attention pipeline.
figure 2
The past poses are shown as blue and red skeletons and the predicted ones in green and purple. The last observed M poses are initially used as query. For every M consecutive poses in the history (key), we compute an attention score to weigh the DCT coefficients (values) of the corresponding sub-sequence. The weighted sum of such values is then concatenated with the DCT coefficients of the last observed sub-sequence to predict the future. At test time, to predict poses in the further future, we use the output of the predictor as input and predict future motion recursively (as illustrated by the dashed line)

Full size image
Following the machine translation formalism of Vaswani et al. (2017), we describe our attention model as a mapping from a query and a set of key-value pairs to an output. The output is a weighted sum of values, where the weight, or attention, assigned to each value is a function of its corresponding key and of the query. Mapping to our motion attention model, the query corresponds to a learned representation of the last observed sub-sequence, and the key-value pairs are treated as a dictionary within which keys are learned representations for historical sub-sequences and values are the corresponding learned future motion representations. Our motion attention model output is defined as the aggregation of these future motion representations based on partial motion similarity between the latest motion sub-sequence and historical sub-sequences.

In our context, we aim to compute attention from short sequences. To this end, we first divide the motion history of each body part ğ—ğ‘1:ğ‘=[ğ±ğ‘1,ğ±ğ‘2,ğ±ğ‘3,â‹¯,ğ±ğ‘ğ‘], with ğ‘âˆˆ{1,2,â‹¯,ğ‘ƒ}, into ğ‘âˆ’ğ‘€âˆ’ğ‘‡+1 sub-sequences {ğ—ğ‘ğ‘–:ğ‘–+ğ‘€+ğ‘‡âˆ’1}ğ‘âˆ’ğ‘€âˆ’ğ‘‡+1ğ‘–=1, each of which consists of ğ‘€+ğ‘‡ consecutive body part poses. By using sub-sequences of length ğ‘€+ğ‘‡, we assume that the motion predictor, which we will introduce in Sect. 3.2, exploits the past M frames to predict the future T frames. We then take the first M poses of each sub-sequence ğ—ğ‘ğ‘–:ğ‘–+ğ‘€âˆ’1 to be a key, and the whole sub-sequence ğ—ğ‘ğ‘–:ğ‘–+ğ‘€+ğ‘‡âˆ’1 is the corresponding value. Furthermore, we define the query as the latest sub-sequence ğ—ğ‘ğ‘âˆ’ğ‘€+1:ğ‘ with length M.

To make the output of our attention model consistent with that of the final predictor, we map the resulting values to trajectory space using the DCT on the temporal dimension. That is, we take our final values to be the DCT coefficients {ğ•ğ‘ğ‘–}ğ‘âˆ’ğ‘€âˆ’ğ‘‡+1ğ‘–=1, where ğ•ğ‘ğ‘–âˆˆğ‘ğ¾ğ‘Ã—(ğ‘€+ğ‘‡). Each row of ğ•ğ‘ğ‘– contains the DCT coefficients of one joint coordinate sequence. In practice, we can truncate some high frequencies to avoid predicting jittery motion.

Fig. 3: Predictor.
figure 3
We first apply the DCT to encode temporal pose information in trajectory space. The DCT coefficients concatenated with the output of motion attention model are treated as features input to graph convolutional layers. In each layer, we depict how our framework aggregates information from multiple nodes via learned adjacency matrices

Full size image
As depicted by Fig. 2, the query and keys are used to compute attention scores, which then act as weights to combine the corresponding values. To this end, we first map the query and keys to vectors of the same dimension d by two functions ğ‘“ğ‘ğ‘:ğ‘ğ¾ğ‘Ã—ğ‘€â†’ğ‘ğ‘‘ and ğ‘“ğ‘ğ‘˜:ğ‘ğ¾ğ‘Ã—ğ‘€â†’ğ‘ğ‘‘ modeled with neural networks. This can be expressed as

ğªğ‘=ğ‘“ğ‘ğ‘(ğ—ğ‘ğ‘âˆ’ğ‘€+1:ğ‘),
(1)
ğ¤ğ‘ğ‘–=ğ‘“ğ‘ğ‘˜(ğ—ğ‘ğ‘–:ğ‘–+ğ‘€âˆ’1),
(2)
where ğªğ‘,ğ¤ğ‘ğ‘–âˆˆğ‘ğ‘‘, ğ‘–âˆˆ{1,2,â‹¯,ğ‘âˆ’ğ‘€âˆ’ğ‘‡+1}, and ğ‘âˆˆ{1,2,â‹¯,ğ‘ƒ}. For each key, we then compute an attention score as

ğ‘ğ‘ğ‘–=ğªğ‘ğ¤ğ‘ğ‘–ğ‘‡âˆ‘ğ‘âˆ’ğ‘€âˆ’ğ‘‡+1ğ‘–=1ğªğ‘ğ¤ğ‘ğ‘–ğ‘‡.
(3)
Note that, instead of the softmax function which is commonly used in attention mechanisms, we simply normalize the attention scores by their sum, which we found to avoid the gradient vanishing problem that may occur when using a softmax. While this division only enforces the sum of the attention scores to be 1, we further restrict the outputs of ğ‘“ğ‘ğ‘ and ğ‘“ğ‘ğ‘˜ to be non-negative with ReLU to avoid obtaining negative attention scores.

We then compute the output of the attention model for each body part as the weighed sum of values, i.e.,

ğ”ğ‘=âˆ‘ğ‘–=1ğ‘âˆ’ğ‘€âˆ’ğ‘‡+1ğ‘ğ‘ğ‘–ğ•ğ‘ğ‘–,
(4)
where ğ”ğ‘âˆˆğ‘ğ¾ğ‘Ã—(ğ‘€+ğ‘‡). The final output for the whole body is the concatenation of those for all body parts ğ”=[ğ”ğ‘‡1,ğ”ğ‘‡2,â‹¯,ğ”ğ‘‡ğ‘ƒ]ğ‘‡ with ğ”âˆˆğ‘ğ¾Ã—(ğ‘€+ğ‘‡). This initial estimate is then combined with the latest sub-sequence and processed by the prediction model described below to generate future poses ğ—Ì‚ ğ‘+1:ğ‘+ğ‘‡.

At test time, to generate longer future motion, we augment the motion history with the last predictions and update the query with the latest sub-sequence in the augmented motion history, and the key-value pairs accordingly. These updated entities are then used for the next prediction step.

Prediction Model
To predict the future motion, we reuse the motion prediction model we introduced in (Mao et al. 2019) as shown in Fig. 3. Specifically, as mentioned above, we use a DCT-based representation to encode the temporal information for each joint coordinate or angle and GCNs with learnable adjacency matrices to capture the spatial dependencies among the coordinates or angles.

Temporal Encoding
Given a motion sequence ğ—1:ğ¿, whose ğ‘˜ğ‘¡â„ row can be expressed as [ğ‘¥ğ‘˜,1,ğ‘¥ğ‘˜,2,â‹¯,ğ‘¥ğ‘˜,ğ¿], the corresponding ğ‘™ğ‘¡â„ DCT coefficient of this row is computed as

ğ¶ğ‘˜,ğ‘™=2ğ¿â€¾â€¾â€¾âˆšâˆ‘ğ‘›=1ğ¿ğ‘¥ğ‘˜,ğ‘›11+ğ›¿ğ‘™1â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšcos(ğœ‹2ğ¿(2ğ‘›âˆ’1)(ğ‘™âˆ’1)),
(5)
where ğ‘™âˆˆ{1,2,â‹¯,ğ¿} and ğ›¿ğ‘–ğ‘— denotes the Kronecker delta function, i.e.,

ğ›¿ğ‘–ğ‘—={10if ğ‘–=ğ‘—if ğ‘–â‰ ğ‘—.
(6)
Given such coefficients, the original pose representation (coordinates or angles) can be obtained via the Inverse Discrete Cosine Transform (IDCT) as

ğ‘¥ğ‘˜,ğ‘›=2ğ¿â€¾â€¾â€¾âˆšâˆ‘ğ‘™=1ğ¿ğ¶ğ‘˜,ğ‘™11+ğ›¿ğ‘™1â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšcos(ğœ‹2ğ¿(2ğ‘›âˆ’1)(ğ‘™âˆ’1)),
(7)
where ğ‘›âˆˆ{1,2,â‹¯,ğ¿}.

To predict future poses ğ—ğ‘+1:ğ‘+ğ‘‡, we make use of the latest sub-sequence ğ—ğ‘âˆ’ğ‘€+1:ğ‘, which is also the query in the attention model. Adopting the same padding strategy as in (Mao et al. 2019), we replicate the last observed pose ğ—ğ‘ T times to generate a sequence of length ğ‘€+ğ‘‡ and the DCT coefficients of this sequence are denoted as ğƒâˆˆğ‘ğ¾Ã—(ğ‘€+ğ‘‡). We then aim to predict DCT coefficients of the future sequence ğ—ğ‘âˆ’ğ‘€+1:ğ‘+ğ‘‡ given ğƒ and the attention modelâ€™s output ğ”.

Spatial Encoding
To capture spatial dependencies between different joint coordinates or angles, we regard the human body as a fully-connected graph with K nodes. The input to a graph convolutional layer m is a matrix ğ‡(ğ‘š)âˆˆğ‘ğ¾Ã—ğ¹, where each row is the F dimensional feature vector of one node. For example, for the first layer, the network takes as input the ğ¾Ã—2(ğ‘€+ğ‘‡) matrix that concatenates ğƒ and ğ”. A graph convolutional layer then outputs a ğ¾Ã—ğ¹Ì‚  matrix of the form

ğ‡(ğ‘š+1)=ğœ(ğ€(ğ‘š)ğ‡(ğ‘š)ğ–(ğ‘š)),
(8)
where ğ€(ğ‘š)âˆˆğ‘ğ¾Ã—ğ¾ is the trainable adjacency matrix of layer m, representing the strength of the connectivity between nodes, ğ–(ğ‘š)âˆˆğ‘ğ¹Ã—ğ¹Ì‚  also encodes trainable weights but used to extract features, and ğœ(â‹…) is an activation function, such as ğ‘¡ğ‘ğ‘›â„(â‹…). We stack several such layers to form our GCN-based predictor.

Given ğƒ and ğ”, the predictor learns a residual between the DCT coefficients ğƒ of the padded sequence and those of the true sequence. By applying IDCT to the predicted DCT coefficients, we obtain the coordinates or angles ğ—Ì‚ ğ‘âˆ’ğ‘€+1:ğ‘+ğ‘‡, whose last T poses ğ—Ì‚ ğ‘+1:ğ‘+ğ‘‡ are predictions in the future.

Fig. 4: Different fusion model.
figure 4
a Simply concatenate the outputs of all three motion models with the DCT coefficient ğƒ. b Pre-fusion: The outputs of three motion models are first combined by a fusion model and fed into the predictor. (c) Post-fusion: The fusion process occurs after the predictions are made

Full size image
Fusion Model
As mentioned before, different activities/sequences may benefit from using attention at different levels, i.e., full body, parts, or individual joints. To model this , we introduce a fusion model that automatically combines different attention model and obtains the best-suited attention level for the current context. Specifically, partitioning the human skeleton into full pose, body parts and individual joints, corresponding to different choices of P in Sect. 3.1, we compute the motion attentions ğ”ğ‘ğ‘œğ‘ ğ‘’, ğ”ğ‘ğ‘ğ‘Ÿğ‘¡, and ğ”ğ‘—ğ‘œğ‘–ğ‘›ğ‘¡, respectively, and treat them as motion priors. We then study the three different ways to exploit these motion priors depicted by Fig. 4. The first one (Fig. 4a) consists of simply concatenating them with the DCT coefficients ğƒ of the padded sequence before being fed to the predictor. The other two ways both involve training a fusion model which outputs 3 normalized weights, one for each type of motion prior. The difference is where it is applied. For pre-fusion shown in Fig. 4b, the fusion model is used to fuse the outputs of the motion models before fed into the predictor, while in post-fusion, the fusion model is trained to combine the predictions from three different predictors given different level of motion attention outputs. As verified by our experiments, the post-fusion model of Fig. 4c yields the best performance, and we therefore adopt it for our approach.

Training
Let us now introduce the loss functions we use to train our model on either 3D coordinates or joint angles. For 3D joint coordinates prediction, we make use of the Mean Per Joint Position Error (MPJPE) proposed in (Ionescu et al. 2014). In particular, for one training sample, this yields the loss

â„“=1ğ½(ğ‘€+ğ‘‡)âˆ‘ğ‘¡=1ğ‘€+ğ‘‡âˆ‘ğ‘—=1ğ½â€–ğ©Ì‚ ğ‘¡,ğ‘—âˆ’ğ©ğ‘¡,ğ‘—â€–2,
(9)
where ğ©Ì‚ ğ‘¡,ğ‘—âˆˆğ‘3 represents the 3D coordinates of the ğ‘—ğ‘¡â„ joint of the ğ‘¡ğ‘¡â„ human pose in ğ—Ì‚ ğ‘âˆ’ğ‘€+1:ğ‘+ğ‘‡, and ğ©ğ‘¡,ğ‘—âˆˆğ‘3 is the corresponding ground truth.

For the angle-based representation, we use the average â„“1 distance between the predicted joint angles and the ground truth as loss. For one sample, this can be expressed as

â„“=1ğ¾(ğ‘€+ğ‘‡)âˆ‘ğ‘¡=1ğ‘€+ğ‘‡âˆ‘ğ‘˜=1ğ¾|ğ‘¥Ì‚ ğ‘¡,ğ‘˜âˆ’ğ‘¥ğ‘¡,ğ‘˜|,
(10)
where ğ‘¥Ì‚ ğ‘¡,ğ‘˜ is the predicted ğ‘˜ğ‘¡â„ angle of the ğ‘¡ğ‘¡â„ pose in ğ—Ì‚ ğ‘âˆ’ğ‘€+1:ğ‘+ğ‘‡ and ğ‘¥ğ‘¡,ğ‘˜ is the corresponding ground truth.

Network Structure
As shown in Fig. 2, our motion prediction framework consists of two modules: a motion attention model and a predictor. For the attention model, we use the same architecture for ğ‘“ğ‘ğ‘ and ğ‘“ğ‘ğ‘˜. Specifically, we use a network consisting of two 1D convolutional layers, each of which is followed by a ReLU activation function. In our experiments, the kernel size of these two layers is 6 and 5, respectively, to obtain a receptive field of 10 frames. The dimension of the hidden features, the query vector ğªğ‘ and the key vectors {ğ¤ğ‘ğ‘–}ğ‘âˆ’ğ‘€âˆ’ğ‘‡+1ğ‘–=1 is set to 256.

For the predictor, we use the same GCN with residual structure as in our previous work (Mao et al. 2019). It is made of 12 residual blocks, each of which contains two graph convolutional layers, with an additional initial layer to map the DCT coefficients to features and a final layer to decode the features to DCT residuals. Details of the predictor network structure are shown in Fig. 3. The learnable weight matrix ğ– of each layer is of size 256Ã—256, and the size of the learnable adjacency matrix ğ€ depends on the dimension of one human pose. For example, for 3D coordinates, ğ€ is of size 66Ã—66. Thanks to the simple structure of our attention model, the overall network remains still compact. Specifically, in our experiments, it has around 3.4 million parameters for both 3D coordinates and angles. The implementation details are included in supplementary material. For the fusion model, we use a similar GCN-based network structure as the predictor but without the overall residual connection.

Experiments
Following previous works (Gopalakrishnan et al. 2019; Li et al. 2018a; Mao et al. 2019; Martinez et al. 2017; Pavllo et al. 2019), we evaluate our method on Human3.6m (H3.6M) (Ionescu et al. 2014) and AMASS (Mahmood et al. 2019). We further evaluate our method on 3DPW (von Marcard et al. 2018) using our model trained on AMASS to demonstrate the generalizability of our approach. Below, we discuss these datasets, the evaluation metric and the baseline methods, and present our results using joint angles and 3D coordinates.

Datasets
Human3.6M
 (Ionescu et al. 2014) is the most widely used benchmark dataset for motion prediction. It depicts seven actors performing 15 actions. Each human pose is represented as a 32-joint skeleton. We compute the 3D coordinates of the joints by applying forward kinematics on a standard skeleton as in (Mao et al. 2019). Following (Li et al. 2018a; Mao et al. 2019; Martinez et al. 2017), we remove the global rotation, translation and constant angles or 3D coordinates of each human pose, and down-sample the motion sequences to 25 frames per second. As previous work (Li et al. 2018a; Mao et al. 2019; Martinez et al. 2017), we test our method on subject 5 (S5). However, instead of testing on only 8 random sub-sequences per action, which was shown in (Pavllo et al. 2019) to lead to high variance, we report our results on 256 sub-sequences per action. Nevertheless, for the baselines (Tang et al. 2018; Cai et al. 2020) whose code is not publicly available, we compare our results to theirs on the same 8 sub-sequences of each action.

AMASS
The Archive of Motion Capture as Surface Shapes (AMASS) dataset (Mahmood et al. 2019) is a recently published human motion dataset, which unifies many mocap datasets, such as CMU, KIT and BMLrub, using a SMPL (Loper et al. 2015; Romero et al. 2017) parameterization to obtain a human mesh. SMPL represents a human by a shape vector and joint rotation angles. The shape vector, which encompasses coefficients of different human shape bases, defines the human skeleton. We obtain human poses in 3D by applying forward kinematics to one human skeleton. In AMASS, a human pose is represented by 52 joints, including 22 body joints and 30 hand joints. Since we focus on predicting human body motion, we discard the hand joints and the 4 static joints, leading to an 18-joint human pose. As for H3.6M, we down-sample the frame-rate to 25Hz. Since most sequences of the official testing splitFootnote1 of AMASS consist of transition between two irrelevant actions, such as dancing to kicking, kicking to pushing, they are not suitable to evaluate our prediction algorithms, which assume that the history is relevant to forecast the future. Therefore, instead of using this official split, we treat BMLrubFootnote2 (522 min. video sequence), as our test set as each sequence consists of one actor performing one type of action. We then split the remaining parts of AMASS into training and validation data.

3DPW. The 3D Pose in the Wild dataset (3DPW) (von Marcard et al. 2018) consists of challenging indoor and outdoor actions. We only evaluate our model trained on AMASS on the test set of 3DPW to show the generalization of our approach.

As mentioned in Sect. 3.1, we model motion attention at 3 different levels. The full-pose and individual-joints ones are self-explanatory. For part-based motion attention, we divide the human body into 5 parts following the human kinematic tree: torso (including neck and head), left arm, right arm, left leg and right leg. Each part consists of several human joints.

Table 1 Short-term prediction of 3D joint positions on H3.6M. The error is measured in millimeter. For â€œLTDâ€, we use the number of observed frames and that of future frames to predict during training to distinguish different models. For instance, â€œLTD-50-25â€ means the model is trained to observe past 50 frames and predict future 25 frames. Following QuaterNet (Pavllo et al. 2019), we report the average error on 256 sub-sequences except for those with â€œ(8 Sub-seq)â€ after their method names is averaging over 8 sub-sequences per action. Our approach achieves state of the art performance across all 15 actions at almost all time horizons, especially for actions with a clear repeated history, such as â€œWalkingâ€. The proposed extension â€œPost-fusionâ€ further improves the results compared to the base model â€œPose Motion Att.â€
Full size table
Evaluation Metrics and Baselines

Metrics
For the models that output 3D positions, we report the Mean Per Joint Position Error (MPJPE) (Ionescu et al. 2014) in millimeter, which is commonly used in human pose estimation. For those that predict angles, we follow the standard evaluation protocol (Martinez et al. 2017; Li et al. 2018a; Mao et al. 2019) and report the Euclidean distance in Euler angle representation.

Baselines
We compare our approach with two RNN-based methods, Res. sup. (Martinez et al. 2017) and MHU (Tang et al. 2018), two feed-forward models, convSeq2Seq (Li et al. 2018a) and LTD (Mao et al. 2019), which constitutes the state of the art. We further compare it with the concurrent work LPJ (Cai et al. 2020), which exploits an attention-based transformer. The angular results of Res. sup. (Martinez et al. 2017), convSeq2Seq (Li et al. 2018a) on H3.6M are obtained by running the official training code and report on 256 sub-sequences. For the other results of Res. sup. (Martinez et al. 2017) and convSeq2Seq (Li et al. 2018a), we adapt the code provided by the authors for H3.6M to 3D and AMASS. The results of MHU (Tang et al. 2018) and LPJ (Cai et al. 2020) on H3.6M are directly taken from the respective paper. For our LTD (Mao et al. 2019), we rely on the pre-trained models released for H3.6M, and train the model on AMASS using the released code.

While Res. sup. (Martinez et al. 2017), convSeq2Seq (Li et al. 2018a) and MHU (Tang et al. 2018) are all trainedto generate 25 future frames, LTD (Mao et al. 2019) has 3 different models, which we refer to as LTD-50-25 (Mao et al. 2019), LTD-10-25 (Mao et al. 2019), and LTD-10-10 (Mao et al. 2019). The two numbers after the method name indicate the number of observed past frames and that of future frames to predict, respectively, during training. For example, LTD-10-25 (Mao et al. 2019) means that the model is trained to take the past 10 frames as input to predict the future 25 frames.

Table 2 Short-term prediction of 3D joint positions on 4 actions of H3.6. Since LPJ (Cai et al. 2020) only provide their results on 4 action of H3.6M, we compare our results with that of LPJ (Cai et al. 2020) on these actions. The â€œatt.â€, â€œprog.â€ and â€œdict.â€ refer to attention-based prediction, progressive prediction and motion dictionary which are the 3 components proposed in LPJ (Cai et al. 2020). With only the attention module, our method outperforms that of LPJ (Cai et al. 2020) by a large margin on all cases
Full size table
Table 3 Long-term prediction of 3D joint positions on H3.6M. On average, our approach performs the best. Note that, on â€œWalkingâ€ at 1000ms, the 3D error of our method is 17% lower than that of LTD-10-10 (Mao et al. 2019), which uses the same predictor but no attention model
Full size table
Table 4 Short-term prediction of joint angles on H3.6M. Following QuaterNet (Pavllo et al. 2019), we report the average error on 256 sub-sequences, except when indicating â€œ(8 Sub-seq)â€ after a methodâ€™s name, in which case the error is averaged over 8 sub-sequences per action, as reported in the corresponding paper
Full size table
Table 5 Long-term prediction of joint angles on H3.6M
Full size table
Table 6 Short-term and long-term prediction of 3D joint positions (upper) and joint angles (bottom) on BMLrub (left) and 3DPW (right)
Full size table
Results
Following the setting of our baselines (Martinez et al. 2017; Li et al. 2018a; Tang et al. 2018; Mao et al. 2019), we report results for short-term (<500ğ‘šğ‘ ) and long-term (>500ğ‘šğ‘ ) prediction. On H3.6M, our model is trained using the past 50 frames to predict the future 10 frames, and we produce poses further in the future by recursively applying the predictions as input to the model. On AMASS, our model is trained using the past 50 frames to predict the future 25 frames.

Human3.6M
In Tables 1 and 3, we provide the H3.6M results for short-term and long-term prediction in 3D space, respectively. Note that we outperform all the baselines on average for both short-term and long-term prediction. In particular, our method yields larger improvements on activities with a clear repeated history, such as â€œWalkingâ€ and â€œWalking Togetherâ€. Nevertheless, our approach remains competitive on the other actions. Note that we consistently outperform LTD-50-25, which is trained on the same number of past frames as our approach. This, we believe, evidences the benefits of exploiting attention on the motion history.

Moreover, the performance of our motion attention model (denoted as â€œPose Motion Att.â€ in the tables) is consistently improved with the use of our fusion model, for both short-term and long-term prediction. Our approach performs comparable to the concurrent LPJ (Cai et al. 2020). Note that our motion attention strategy and fusion model are orthogonal to the progressive joint prediction and motion dictionary of LPJ (Cai et al. 2020), and thus one could expect further improvement by combining these two strategies. We further show that our methods with only the attention module outperforms that of LPJ (Cai et al. 2020) by a large margin in Table 2.

Fig. 5
figure 5
Qualitative comparison of short-term (â€œDiscussionâ€ and â€œWalking Dogâ€) and long-term (â€œWalkingâ€) predictions on H3.6M. From top to bottom, we show the ground truth, and the results of LTD-10-25, LTD-10-10 and our approach on 3D positions. The ground truth is shown as blue-red skeletons, and the predictions as green-purple ones

Full size image
Fig. 6
figure 6
Visualization of attention maps and joint trajectories. The x-axis denotes the frame index, with prediction starting at frame 0. The y-axis of the attention map (top) is the prediction step. Specifically, since the model is trained to predict 10 future frames, we recursively perform prediction for 3 steps to generate 30 frames. Each row of an attention map is then the attention vector when predicting the corresponding 10 future frames. For illustration purpose, we show per-frame attention, which represents the attention for its motion subsequence consisting of M-1 frames forward and T frames afterwards. (a) Predicted attention map and trajectory of the left footâ€™s x coordinate for â€™Walkingâ€™, where the future motion closely resembles that between frames âˆ’45 and âˆ’10. Our model correctly attends to that very similar motion in the history. (b) Predicted attention map and trajectory of the right wristâ€™s x coordinate for â€™Discussionâ€™. In this case, the attention model searches for the most similar motion in the history. For example, in the 1ğ‘ ğ‘¡ prediction step, to predict frames 0 to 10 where a peak occurs, the model focuses on frames âˆ’30 to âˆ’20, where a similar peak pattern occurs

Full size image
Fig. 7
figure 7
Visualization of attention maps and joint coordinate trajectories for â€œSmokingâ€ on H3.6M. a Results of our model observing 50 past frames. b Results of our model observing 100 frames. c Results obtained when replacing the motion of the past 40 frames with a constant pose

Full size image
Let us now focus on the LTD (Mao et al. 2019) baseline, which constitutes the state of the art. Although LTD-10-10 is very competitive for short-term prediction, when it comes to generate poses in the further future, it yields higher average error, i.e., 114.0mm at 1000ms. By contrast, LTD-10-25 and LTD-50-25 achieve good performance at 880ms and above, but perform worse than LTD-10-10 at other time horizons. Our approach with a unified model, however, yields state-of-the-art performance for both short-term and long-term predictions. To summarize, our motion attention model improves the performance of the predictor for short-term prediction and further enables it to generate better long-term predictions. This is further confirmed by Tables 4 and 5, where we report the short-term and long-term prediction results in angle space on H3.6M, and by the qualitative comparison in Fig. 5.

Table 7 Short-term and long-term prediction of 3D positions on selected sequences where similar patterns occur in the longer history. The number after â€œOursâ€ indicates the observed frames during testing. Both methods observed 50 frames during training
Full size table
AMASS & 3DPW
The results of short-term and long-term prediction in 3D on AMASS and 3DPW are shown in Table 6. Our method consistently outperforms baseline approaches, which further shows the benefits of our motion attention model. Since none of the methods were trained on 3DPW, these results further demonstrate that our approach generalizes better to new datasets than the baselines.

Visualisation of Attention
In Fig. 6, we visualize the attention maps computed by our motion attention model on a few sampled joints for their corresponding coordinate trajectories. In particular, we show attention maps for joints in a periodical motion (â€œWalkingâ€) and a non-periodical one (â€œDiscussionâ€). In both cases, the attention model can find the most relevant sub-sequences in the history, which encode either a nearly identical motion (periodical action), or a similar pattern (non-periodical action).

Motion Repeats Itself in Longer-Term History
Our model, which is trained with fixed-length observations, can nonetheless exploit longer history at test time if it is available. To evaluate this and our modelâ€™s ability to capture long-range motion dependencies, we manually sampled 100 sequences from the test set of H3.6M, in which similar motion occurs in the further past than that used to train our model.

In Table 7, we compare the results of a model trained with 50 past frames and using either 50 frames (Ours-50) or 100 frames (Ours-100) at test time. Although the performance is close in the very short term (<160ğ‘šğ‘ ), the benefits of our model using longer history become obvious when it comes to further future, leading to a performance boost of 4.2mm at 1s. In Fig. 7, we compare the attention maps and predicted joint trajectories of Ours-50 (a) and Ours-100 (b). The highlighted regions (in red box) in the attention map demonstrate that our model can capture the repeated motions in the further history if it is available during test and improve the motion prediction results.

To show the influence of further historical frames, we replace the past 40 frames with a static pose, thus removing the motion in that period, and then perform prediction with this sequence. As shown in Fig. 7c, attending to the similar motion between frames âˆ’80 and âˆ’60, yields a trajectory much closer to the ground truth than only attending to the past 50 frames.

Importance of Different Levels of Attention
Our different levels of motion attention complement each other in the two ways discussed below. Note that, in this discussion, we categorize our 3 different levels of motion attention into 2 relative levels: global and local. For example, parts motion attention is referred to as local-level attention when compared to full pose motion attention, but as global-level attention when compared to joint motion attention.

Fig. 8
figure 8
Visualization of attention maps and joint coordinate trajectories of different motion attentions for â€œWalkingâ€ on H3.6M. Pose motion attention captures the repeated motion for right foot (first row of (b)) while miss the the motion pattern for right hand (first row of (a)). Joint motion attention however, attends to the most relevant historical motion for both joints with two different attention maps and leads to a better prediction

Full size image
Fig. 9
figure 9
Visualization of attention maps and joint coordinate trajectories of different motion attentions for â€œTaking Photoâ€ on H3.6M. At the first prediction step, as highlighted in red box, part motion attention, which generates one attention map for both joints by treating them as one body part, attends to historical motions that better reflect the current context

Full size image
Table 8 Per-action 3D error at 1s on H3.6M
Full size table
Table 9 Per-joint 3D error at 1s on H3.6M. The numbers after the joint names are the joint index defined in H3.6M dataset. The index starts from 0. Note that, we eliminate the joints that are fixed such as the â€œHip (0)â€. For joints that share same 3D location, we only keep one of them in the table. For example, the 13th, 16th and 24th joints share the same 3D location, we thus only show the results on the 13th one
Full size table
On one hand, modeling attention at a global level is not effective for motions whose local movements are not synchronized. Specifically, when the motion patterns of different body parts/joints are different, computing attention for them separately is more effective than using one shared attention. Such out of sync motions are common in non-periodical actions and sometimes even occur in periodical ones, such as the one shown in Fig. 8. Specifically, in Fig. 8, we compare the attention maps generated by pose motion attention (first row), joint motion attention (second row) and the predicted trajectories (third row) of two different joints (right hand and right foot) in a â€œWalkingâ€ sequence. As the motion patterns of the foot joint and hand joint are not synchronized, pose motion attention correctly captures the repeated pattern of the foot while attending to the wrong area for the hand. By contrast, joint motion attention, which generates two attention maps for these two joints, attends to the most relevant historical motions for both joints and leads to a better prediction.

On the other hand, relying purely on local-level attention is not always optimal. Since local-level attention is computed from only the history of local movements, it may attend to sub-optimal areas in history, where different local body parts/joints have no or multiple similar motion patterns. Global-level attention helps to disambiguate the motion in such situations. We provide one example of this in Fig. 9, where we show the attention maps and the trajectories predicted with part motion attention and joint motion attention for a â€œTaking Photoâ€ sequence. Given the historical motion of individual joints only, joint motion attention wrongly attends to the area where a sharp motion in the negative direction occurs. By contrast, by leveraging information about complete body parts, part motion attention finds the historical motion that best reflects the current context.

As to quantitative results, we will provide an ablation study on fusing different motion attention in Sect. 4.3. Here, we would like to emphasize that our multi-level motion attention fusion improves the motion prediction performance over pose motion attention only consistently for all actions. As shown in Table 8, these improvements vary for actions of different natures. For instance, motion attention at the full pose level is sufficient to capture the motion patterns of periodical actions, such as â€œWalkingâ€; in such cases, the improvement obtained by our multi-level motion attention fusion model is indeed relatively small. By contrast, for other actions, such as â€œPosingâ€ and â€œWalking Dogâ€, fusing multi-level motion information yields significant improvements, of up to 5 mm, as evidenced by results in Table 8.This is due to the fact that, in such actions, the repetitive motion patterns do not involve the full body but only body parts/joints.

To better understand the error distribution for each joint, we further show the 3D error for each joint separately after 1 second of prediction on H3.6M in Table 9. Our â€œMotion Att. + Post-fusionâ€ consistently improves the performance on all joints. For some joints, such as â€œLeft Footâ€, the improvements go up to 4 mm.

Table 10 Short & long-term prediction of 3D joint positions on H3.6M with different levels of observation noise. The first column indicates the standard deviation (in millimeter) of the Gaussian noise added to the historical sequences. ğœ=0 means that no noise was added
Full size table
Table 11 Comparison of different fusion strategies. â€œConcat.â€ corresponds to concatenating the outputs of all motion attention models, as shown in Fig. 4a
Full size table
Influence of Noisy History
We further study our modelâ€™s ability of handling noisy history. In Table 10, we provide the results of our model obtained using observations corrupted by different levels of noise. Specifically, given the pretrained model, we added Gaussian noise ({îˆº(0,ğœ2)}ğœ={0,2,4,6,8,10}) to all joint coordinates of each frame in the history. As further shown in Fig. 10, the 3D error grows linearly with the noise level (ğœ).

We further analyze the influence of jitter in Fig. 11. Jitter was created by corrupting each historical pose with Gaussian noise îˆº(0,10). Our model is robust to such jitter and produces smooth future motions that are close to the ones predicted with the ground-truth history. This is because, instead of performing frame by frame prediction as in (Martinez et al. 2017), our model generates a temporal encoding (DCT) of the sequence, which encourages global smoothness.

Ablation Study
To further evaluate our fusion model, below, we first compare the performance of the different fusion strategies introduced in Sect. 3.3. We then investigate the performance of fusing among different motion attention models.

Fusion Strategies
In Table 11, we compare the performance of three different fusion strategies. Post-fusion provides the best performance for both 3D joint positions and joint angles prediction.

Ablation on Post-Fusion
In Table 12, we evaluate the influence of fusing among the outputs of different motion attention models. For 3D joint position representation, the best results are obtained by fusing from all three motion attention models. By contrast, for joint angle representation, fusing among pose motion attention and part motion attention performs best.

This is mainly due to the bias on training set. In particular, as shown in Table 13, for joint angle representation, joint motion attention performs better than others on 59.3% of the motion sequences in training set. After training on the biased training set, the fusion model tends to focus on predictions from joint motion attention model at all cases which leads to a inferior performance on the unbiased test set.

Table 12 Ablation on post-fusion strategy. We compare the average 3D joint position error (upper) and joint angle error (bottom) on H3.6M. For 3d joint position, best performance is obtained by fusing among all 3 motion attention models. For joint angle, fusing pose motion attention and part motion attention performs the best
Full size table
Table 13 Performance bias on training set comparing to test set of 3 different attention models. We show the average angle error on test set (top) and training set (bottom) of H3.6M. Besides, the last column demonstrates the percentage of each type of motion attention model outperforms the others among all sequences. For example, the joint attention model outperforms others on 59.3% training samples, leading to a consistent better performance across all time horizons. However, all 3 attention models perform comparable to each other on test set
Full size table
Fig. 10
figure 10
3D error vs noise levels

Full size image
Fig. 11
figure 11
Influence of jitter in the motion history. We created jitter by corrupting each historical pose with Gaussian noise îˆº(0,10). Our model still produces smooth future motions that are close to those predicted when using GT history

Full size image
GCNs vs. Fully-Connected Networks
Finally, we evaluate the importance of using GCNs vs fully-connected networks and of learning the connectivity in the GCN instead of using a pre-defined adjacency matrix based on the kinematic tree. The results of these experiments, provided in Table 14, demonstrate the benefits of both using GCNs and learning the corresponding graph structure. Altogether, this ablation study indicates the importance of both aspects of our contribution: Using the DCT to model temporal information and learning the connectivity in GCNs to model spatial structure.

Table 14 Influence of GCNs and of learning the graph connectivity. Top: angle error on 8 sequences per action; Bottom: 3D error on 8 sequences per action. Note that GCNs with a pre-defined connectivity yield much higher errors than learning this connectivity as we do. Here, we reused the results from (Mao et al. 2019)
Full size table
Conclusion
In this paper, we have introduced an attention-based motion prediction approach that selectively exploits historical information according to the similarity between the current motion context and the sub-sequences in the past. This has led to a predictor equipped with a motion attention model that can effectively make use of historical motions, even when they are far in the past. Furthermore, we have studied the use of motion attention at different levels, full body, body parts, joints, and shown that combining these different attention levels led to better performance. Our approach achieves state-of-the-art performance on the commonly-used motion prediction benchmarks and on recently-published datasets. Moreover, our experiments have demonstrated that our network generalizes to previously-unseen datasets without re-training or fine-tuning, and can handle longer history than that it was trained with to further boost performance on non-periodical motions with repeated history. In the future, we will investigate the combination of our approach with the progressive prediction strategy of Cai et al. (2020).

