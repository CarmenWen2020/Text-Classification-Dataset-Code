With the recent availability and affordability of commercial depth sensors and 3D scanners, an increasing number of 3D (i.e., RGBD, point cloud) datasets have been publicized to facilitate research in 3D computer vision. However, existing datasets either cover relatively small areas or have limited semantic annotations. Fine-grained understanding of urban-scale 3D scenes is still in its infancy. In this paper, we introduce SensatUrban, an urban-scale UAV photogrammetry point cloud dataset consisting of nearly three billion points collected from three UK cities, covering 7.6 km2. Each point in the dataset has been labelled with fine-grained semantic annotations, resulting in a dataset that is three times the size of the previous existing largest photogrammetric point cloud dataset. In addition to the more commonly encountered categories such as road and vegetation, urban-level categories including rail, bridge, and river are also included in our dataset. Based on this dataset, we further build a benchmark to evaluate the performance of state-of-the-art segmentation algorithms. In particular, we provide a comprehensive analysis and identify several key challenges limiting urban-scale point cloud understanding. The dataset is available at http://point-cloud-analysis.cs.ox.ac.uk/.

Access provided by University of Auckland Library

Introduction
Giving machines the ability to semantically interpret 3D scenes is highly important for accurate 3D perception and scene understanding. This is also the prerequisite for numerous real-world applications such as object-level robotic grasping (Rao et al. , 2010), scene-level robot navigation (Valada et al. , 2017) and autonomous driving (Geiger et al. , 2013), or even large-scale urban 3D modeling, where autonomous machines are required to interact competently within our physical world. Although increasing research attention has been applied to this field, it remains challenging due to the high geometrical complexity of urban scenes, and limited high quality labelled data resources.

Recently, an increasingly number of sophisticated neural pipelines have been proposed based on different representations of 3D scenes, including: (1) 3D voxel-based methods such as SegCloud Tchapmi et al. (2017), SparseConvNet Graham et al. (2018), MinkowskiNet Choy et al. (2019), PVCNN Liu et al. (2019), Cylinder3D Zhu et al. (2021) and (2) 2D projection-based approaches such as RangeNet++ Milioto et al. (2019), SalsaNext Cortinhal et al. (2020) and SqueezeSeg Wu et al. (2018a), PolarNet Zhang et al. (2020) and (3) recent point-based architectures e.g. PointNet/PointNet++ Qi et al. (2017a, 2017b), PointCNN Li et al. (2018), DGCNN Wang et al. (2019), KPConv Thomas et al. (2019), RandLA-Net Hu et al. (2020) and PointTransformer Zhao et al. (2020).

The core of these techniques, however, relies heavily on the wide availability of large-scale and high-quality open datasets. The datasets provide realistic and diverse data resources and act as benchmarks to fairly evaluate and compare the performance of different algorithms. Existing representative 3D data repositories can be generally classified as: (1) object-level 3D models such as ModelNet Wu et al. (2015), ShapeNet Chang et al. (2015) and ScanObjectNN Uy et al. (2019), (2) indoor scene-level 3D scans, e.g., S3DIS Armeni et al. (2017), ScanNet Dai et al. (2017), Matterport3D Chang et al. (2018) and SceneNN Zhou et al. (2017), and (3) outdoor roadway-level 3D point clouds including Semantic3D Hackel et al. (2017), SemanticKITTI Behley et al. (2019), NPM3D Roynard et al. (2018), and Toronto3D Tan et al. (2020).

However, there is no large-scale photorealistic 3D point cloud dataset available for fine-grained semantic understanding of urban scenarios. Moreover, it remains an open question as to whether existing techniques can be scaled to these urban-scale point clouds. Firstly, in contrast to existing datasets for objects, rooms, or streets which are usually less than 200m in scale, the urban-scale datasets collected by aerial platforms typically span extremely wide areas, e.g. kilometres. How to efficiently and effectively preprocess massive point sets (e.g., over 108) to feed into neural networks is a particular question of interest. Secondly, existing photogrammetric mapping techniques allow reconstructing photorealistic colorized point clouds. Along with the 3D spatial coordinates, is the inclusion of appearance beneficial to semantic understanding and what is the impact if any? Thirdly, real-world urban scenarios usually exhibit extreme class imbalance. The majority of points are dominated by categories such as ground and vegetation, while the critical categories such as rail and water only occupy a small proportion of the total number of points. Fourthly, and potentially most importantly, what is the generalization performance of existing deep neural networks? Can a trained model be well-generalized to unseen data, particularly from a different region? or even generalized to different dataset? Lastly, is it possible to learn semantics with sparser labels? How can we unleash the potential of self-supervised pre-training and semi-supervised learning on 3D point clouds?

In this paper, we take a step towards resolving the above issues. In particular, we first build a UAV photogrammetric point cloud dataset called SensatUrban for urban-scale 3D semantic understanding. This dataset covers 7.6 km2 of urban areas in three UK cities i.e., Birmingham, Cambridge, and York (Fig. 1), along with nearly 3 billion richly annotated 3D points. Each point in the Birmingham and Cambridge set is enriched with one of 13 predefined semantic categories such as ground, vegetation, car, etc., while the points in York remain unlabeled for potential semi-supervised researches. The 3D point clouds are reconstructed from highly overlapped sequential aerial images captured by a professional-grade UAV mapping system. For more detailed data acquisition pipelines, please refer to Sect. 3. Compared with existing 3D datasets, the uniqueness of our SensatUrban lies in two aspects:

Urban-scale spatial coverage In contrast to existing datasets which mainly focus on objects (Wu et al. , 2015; Chang et al. , 2015), rooms (Zhou et al. , 2017; Armeni et al. , 2017; Dai et al. , 2017) and roadways (Hackel et al. , 2017; Behley et al. , 2019; Roynard et al. , 2018; Tan et al. , 2020), the point clouds in our SensatUrban dataset continuously cover several square kilometers of real-world urban areas, opening up new opportunities towards urban-scale applications such as smart cities, and national infrastructure planning and management.

Photorealistic and dense point clouds Our dataset is reconstructed from high-resolution aerial images captured by professional calibrated cameras. Unique aerial images from nadir (top-down) and oblique perspectives for the entire landscape of cities are also provided for optimized and high-quality point clouds. Naturally, the geometric patterns, textures, natural colors, point density, and distributions are distinct from existing LiDAR-based datasets.

Fig. 1
figure 1
This shows an example of urban-scale point clouds in our SensatUrban dataset. It is acquired from the city center of York through UAV photogrammetry. It has a spatial coverage of more than 3 square kilometers and represents a typical urban suburb

Full size image
Based on the proposed SensatUrban dataset, we further highlight several new challenges faced by generalizing existing segmentation algorithms to urban-scale point clouds in Sect. 5. In particular, these challenges include urban-scale data preparation, the usage of color information, learning from extremely imbalanced class distribution, cross-city generalization, and weakly and self-supervised learning from urban-scale point clouds. Note that, this paper does not aim to fully tackle these challenges, but to unveil them and provide insights to the community for future exploration.

To summarize, the main contributions of this paper are as follows:

We propose a new urban-scale photogrammetric point cloud dataset for 3D semantic understanding, with an unprecedented spatial coverage at fine scale and rich semantic annotations.

We provide a comprehensive benchmark for semantic segmentation of urban-scale point clouds. Extensive experimental results of different state-of-the-art approaches are provided, with detailed discussions and analysis.

We highlight several unique challenges faced by generalizing existing neural pipelines to extremely large-scale point clouds, and provide an in-depth outlook of the future directions of 3D semantic learning.

A preliminary version of this work has been published in Hu et al. (2021), this journal extension particularly provides more details with regards to the data collection and point cloud reconstruction, additional experimental results and analysis in cross-dataset generalization, and weakly supervised semantic segmentation of urban-scale point clouds. In addition, the first challenge on large-scale point cloud analysis for urban scene understanding held in ICCV 2021 is based on this dataset. For more details, please refer to https://urban3dchallenge.github.io/.

Table 1 Comparison with existing representative 3D point cloud datasets
Full size table
Related Work
Datasets for 3D Scene Understanding
We first give a brief introduction to the dataset used for 3D scene understanding. For a comprehensive survey, please refer to Guo et al. (2020) for more details.

In general, existing representative datasets can be roughly categorized into the following four subgroups based on the spatial coverage: (1) Object-level 3D models. Early datasets are mainly focused on the recognition of individual objects, thereby usually composed of a collection of synthetic 3D CAD models. Representative datasets include the synthetic ModelNet Wu et al. (2015), ShapeNet Chang et al. (2015), ShapePartNet Yi et al. (2016), PartNet Mo et al. (2019) and the real-world ScanObjectNN Uy et al. (2019). (2) Indoor scene-level 3D scans. These datasets are usually acquired and further reconstructed by using commodity short-range depth scanners in indoor environments, including NYU3D Silberman et al. (2012), SUN RGB-D Song et al. (2015), S3DIS Armeni et al. (2017), SceneNN Zhou et al. (2017), Matterport3D Chang et al. (2018) and ScanNet Dai et al. (2017). Additionally, the SceneNet Handa et al. (2016) and SceneNet RGB-D McCormac et al. (2016) dataset also provide large-scale photorealistic rendering of indoor synthetic layouts. (3) Outdoor roadway-level 3D point clouds. Most of these datasets are driven by the increasing demand of autonomous driving application, and usually collected by using modern laser scanner systems, including static Terrestrial Laser Scanners (TLS) and Mobile Laser Scanners (MLS). Representative datasets include the early Oakland Munoz et al. (2009), KITTI Geiger et al. (2012), Sydney Urban Objects De Deuge et al. (2013) and the recent Semantic3D Hackel et al. (2017), Paris-Lille-3D Roynard et al. (2018), Argoverse Chang et al. (2019), SemanticKITTI Behley et al. (2019), SemanticPOSS Pan et al. (2020), Toronto-3D Tan et al. (2020), nuScenes Caesar et al. (2020), A2D2 Geyer et al. (2020), CSPC-Dataset Tong et al. (2020), Lyft datasetFootnote1 and Waymo dataset Sun et al. (2020). Additionally, synthetic datasets (Ros et al. , 2016; Gaidon et al. , 2016) composed of realistic simulation of LiDAR point clouds are also included. (4) Urban-level aerial 3D point clouds. These datasets are usually acquired by professional-grade airborne LiDAR systems, including the recent DublinCity (Zolanvari et al. , 2019), DALES Varney et al. (2020) and LASDU Ye et al. (2020). Lacking the color information is the main limitation of these datasets, especially for the fine-grained semantic understanding of 3D scenarios. Interestingly, the very recent OpenGF Qin et al. (2021) dataset has started to investigate ultra-large-scale ground filtering datasets. However, this dataset mainly focuses on the task of ground extraction, instead of the fine-grained semantic understanding.

The recent Campus3D (Li et al. , 2020) 3DOM (√ñzdemir et al. , 2019), and H3D (K√∂lle et al. , 2021) are the most similar datasets to our SensatUrban datasets. They are also composed of large-scale photogrammetric 3D point clouds generated from high-resolution aerial images. However, our SensatUrban provides larger-scale 3D urban scenes with several times the number of points, as well as richer semantic annotations.

Semantic Learning of 3D Scenes
Thanks to the wide availability of various different 3D datasets, a large number of insightful research works have been presented and facilitated. The tremendous progress in semantic learning in turn greatly improved the best performance in several competitive leaderboards. Fundamentally, the semantic learning of 3D point clouds can be attributed to a representation learning problem, and the existing neural architectures can be roughly divided into the following three paradigms:

(1) Voxel-based approaches Early works (Le and Duan , 2018; Meng et al. , 2019; Tchapmi et al. , 2017) usually voxelize point clouds into dense cubic grids, and then leverage the mature 3D CNN architectures to learn the semantics of each point. Although promising results have been achieved on several benchmarks, these techniques usually require cubically growing computation and memory with the input resolution. This severely limits the application of these methods on large-scale point clouds. To reduce the computational and memory cost, the sparse volumetric representation (Graham et al. , 2018; Choy et al. , 2019; Cheng et al. , 2021) and point-voxel joint representation (Liu et al. , 2019; Tang et al. , 2020) are further introduced. Additionally, various different volumetric representations such as spherical voxels (Lei et al. , 2020), cylindrical voxels (Zhu et al. , 2021) are also proposed to adapt to the data distribution of specific point clouds (e.g., LiDAR).

(2) 2D projection-based methods Similarly, these pipelines (Milioto et al. , 2019; Lyu et al. , 2020; Cortinhal et al. , 2020; Wu et al. , 2018a; Wu et al. , 2019; Xu et al. , 2020) leverage the well-developed 2D CNN frameworks to learn 3D semantics after projecting the point clouds onto 2D images. However, critical geometric information is very likely to be dropped in the 3D-2D projection (e.g., the commonly-used birds-eye-view images), and therefore they are not suitable to learn the relatively small object categories within urban-scale scenarios.

(3) Point-based architectures (Qi et al. , 2017a; Qi et al. , 2017b; Li et al. , 2018; Wang et al. , 2019; Thomas et al. , 2019; Hu et al. , 2020; Wu et al. , 2018b; Yan et al. , 2020; Ye et al. , 2018; Boulch , 2019; Wang et al. , 2019a). These methods directly operate on the unstructured point clouds, without relying on any explicit intermediate regular representation. This is achieved by using the simple shared MLPs to learn individual per-point features, and symmetrical aggregation functions to ensure permutation invariance (Qi et al. , 2017a). In particular, PointNet++ (Qi et al. , 2017b) is proposed to hierarchically learn the local features, DGCNN (Wang et al. , 2019) is introduced to model the topological structure through a graph architecture with Edge-Conv operation. A kernel point convolution (Thomas et al. , 2019) is proposed to learn spatially correlation in unstructured point clouds. Hu et al. (2020) explore the efficient semantic learning of large-scale point clouds based on the point-based framework. Due to the simple implementation and straightforward architecture, this class of techniques has been widely investigated in several relevant tasks including 3D object detection (Zhou and Tuzel , 2018; Lang et al. , 2019) and instance segmentation (Yang et al. , 2019; Jiang et al. , 2020). However, it remains unclear whether the existing point pipelines can be well generalized to urban-scale point clouds. To this end, we build our SensatUrban dataset and investigate the unique challenges arising from the semantic understanding of urban-scale scenarios.

Dataset Acquisition and Annotation
In this section, we first describe how we collect (Sect. 3.1) and reconstruct (Sect. 3.2) the urban-scale 3D point clouds using UAV photogrammetry techniques, followed by the detailed procedures to label the dataset over several large urban areas in the UK (Sect. 3.3).

Fig. 2
figure 2
The drones and cameras we used in the urban survey

Full size image
Sequential Aerial Imagery Acquisition
Considering the clear advantages of UAV photogrammetry over similar mapping techniques (such as LiDAR) in terms of cost, data quality, and practicality, we adopt a cost-effective fixed-wing mapping drone, Ebee X,Footnote2 equipped with a cutting-edge Sensefly S.O.D.A. camera, to stably capture high-resolution aerial image sequences, as shown in Fig. 2. Note that, the camera has the ability to take both oblique and nadir photographs, ensuring that vertical surfaces are captured appropriately. The detailed specification of the camera can be found in Table 2.

In order to fully and evenly cover the survey area, all flight paths are pre-planned in a grid fashion and automated by the flight control system (e-Motion). Several factors have been taken into consideration during the data collection workflow: the area covered, the flying permissions, the level of detail required, and the resolution needed, etc. In light of the limited battery capacity, multiple individual flights are applied in sequence to capture the whole site (each flight lasts between 40-50 minutes). For illustration, Fig. 3 shows the paths of the pre-planed multiple flights to cover the selected area in Cambridge city.

These multiple aerial image sequences can then be geo-referenced by Ground Control Points (GCPs) which can be measured by independent professional surveyors with high precision GNSS equipment. Alternatively, the Cambridge data is directly geo-referenced using a highly precise onboard Realtime Kinematic (RTK) GNSS and the final horizontal and vertical RMSEs are ¬±50 mm and ¬±75 mm, respectively (note they can be improved by introducing GCPs). As a comparison, the expected positioning accuracy of point clouds acquired by airborne LiDAR is around 5 to 10 cm, depending on the equipment quality, flying configuration, post-processing, etc. Zhang et al. (2018). The resolution (point density) of our data depends on the number of input images and 3D reconstruction settings. Normally, photogrammetric point clouds are very dense from the process of dense image matching and so need to be subsampled. In our case, all points are subsampled to 2.5 cm, which is denser than most LiDAR data such as DALES Varney et al. (2020).

Fig. 3
figure 3
An illustration of the survey in a region of Cambridge. A total of 9 flights were carried out together to cover the whole site. Different flight paths of UAVs are represented in lines with different colors. Note that, the drones fly in a grid fashion (i.e., perpendicular flight) to capture more details of the facades of the urban environment. The circular path is the takeoff and landing pattern

Full size image
Table 2 Detailed specifications of the camera (i.e., Sensefly SODA 3D camera) used in our survey
Full size table
Urban-Scale 3D Point Clouds Reconstruction
Our SensatUrban dataset is reconstructed by using the well-established Structure-from-Motion with Multi-View Stereo (SfM-MVS) techniques (Westoby et al. , 2012) on the highly overlapped 2D aerial image sequences. The camera positions and orientation, and the scene geometry are first recovered simultaneously using a highly redundant iterative bundle adjustment, based on matched features extracted from overlapping offset images. The multi-view stereo image matching technique is then applied to reconstruct dense and coloured 3D point clouds.

Table 3 Statistics of the reconstructed 3D point clouds in different cities
Full size table
In this paper, we use the off-the-shelf software Pix4DFootnote3 to generate the 3D point clouds and orthomosasics. The final outputs of the survey include reconstructed 3D point clouds, 2D orthomosaic images, and 2.5D Digital Surface Model (DSM). In this work, we focus on the 3D point clouds, while the byproduct orthomosaics are only used for visualization purposes. Specifically, we feed all the captured sequential images to Pix4D to generate the 3D point clouds of each region, including the urban area on the periphery of Birmingham, the urban region adjacent to the city centre of Cambridge, and the central area of York. The statistics of the final output point clouds are summarized in Table 3.

Point-Wise Semantic Annotations
To provide fine-grained information of our dataset for subsequent tasks, we further enrich the reconstructed 3D point clouds with point-wise semantic annotations. However, it is non-trivial and particularly important to decide the categories of interest before manual annotation. In this paper, we specify the semantic categories based on the following three principles: (1) Each annotated category should be of interest to social or commercial purposes, such as asset management (Hou et al. , 2014), automated structural damage assessment (Gerke and Kerle , 2011), and urban planning (Hu et al. , 2003), etc. (2) Each category should have a clear and unambiguous semantic meaning. (3) Different categories should have significant variance in terms of geometric structure or appearance.

Table 4 Class definitions and ordering of ourSensatUrban dataset
Full size table
Based on these three criteria, we first labeled the point cloud as highly detailed 31 categories via off-the-shelf point cloud labeling tools (i.e., CloudCompare), including fine-grained urban elements such as benches, bollards, road signs, traffic lights, etc. Considering the scarcity of data points in certain categories, we merged some similar categories together and finally identified the below 13 semantic classes for all the 3D points in Birmingham and Cambridge. The detailed definition of the semantic categories are shown in Table 4. The points in York remain unlabeled, but made available for possible pre-training in semi-supervised schemes. To ensure the annotation quality, all points are annotated independently by two professional operators in the first round. This is followed by cross-validation in the second round. We also give timely and regular feedback to annotators to address potential issues. All discrepancies in the annotation are carefully addressed, greatly reducing the biases from operators and keeping the annotations consistent and high quality. It takes around 600 working hours to label the entire dataset, and there are no unassigned points discarded in the process. Figure 4 shows examples of our annotations. Table 1 compares the statistics of our SensatUrban with a number of existing 3D datasets.

Note that, our SensatUrban dataset not only incorporates common categories such as ground, building, and vegetation, but also involves several new categories that were not included in the previous urban-scale point cloud datasets (Varney et al. , 2020; Zolanvari et al. , 2019), such as rail, bridge, and water. In particular, these categories are derived by discussing with the industry professionals, and are particularly important for urban planning and infrastructure mapping.

Fig. 4
figure 4
Visualization of example point cloud tiles in our SensatUrban dataset. Top: the raw point clouds. Bottom: the semantic annotations of corresponding point clouds. Points belonging to different semantic categories are displayed in different colors

Full size image
The SensatUrban dataset has been made publicly available,Footnote4 all point clouds and the point-wise ground-truth label of the training set are provided for network training, and an online hidden test setFootnote5 is used to evaluate the final segmentation performance. To prevent overfitting on the test set, the maximum number of submissions is also limited.

Fig. 5
figure 5
Additional examples of our SensatUrban dataset. Different semantic classes are labeled by different colors. The top two rows are point clouds collected from Birmingham, and the bottom two rows are point clouds acquired from Cambridge

Full size image
Fig. 6
figure 6
Statistics of our SensatUrban dataset. The number of points in different semantic categories is reported. Please note that the vertical axis is on the logarithmic scale. Additionally, there are no points annotated as rail in Cambridge

Full size image
Benchmarks
Statistics of Train/Val/Test Split
Based on the proposed SensatUrban dataset, we further set up a benchmark to evaluate the performance of existing state-of-the-art segmentation methods. Notably, we first follow DALES Varney et al. (2020) to divide the urban-scale point clouds into similarly sized small tiles (without overlap), so that existing methods can be trained and tested on modern GPUs. Specifically, the urban point clouds collected in Birmingham have been split into 14 tiles, and the Cambridge point clouds are similarly divided into 29 tiles in total. Note that, each tile is approximately 400√ó400 square meters. We also report the detailed statistics of the training/validation/testing subsets in both Birmingham and Cambridge in Fig. 6. It can be seen that the number of points belongs to different semantic categories varies greatly. For example, the dominant three semantic categories, i.e., ground / building / vegetation, together account for more than 50% of the total points. However, the minor yet important two categories (e.g., bike / rail) only account for 0.025% of the total points. This clearly shows that the class distribution of our SensatUrban dataset is extremely imbalanced, which is in correspondence with the long-tailed distribution of real data. As described in Sect. 4, the imbalanced distribution nature of our dataset also poses great challenges in generalizing the existing segmentation approaches.

Representative Baselines
To comprehensively evaluate the performance of existing point cloud segmentation pipelines on urban-scale point clouds, we carefully select 7 representative approaches, which cover the three mainstream paradigms as discussed in Sect. 2.1, as solid baselines in our SensatUrban benchmark. A short summary of these baselines are as follows:

SparseConvNet (Graham et al. , 2018). A strong baseline that introduces the submanifold sparse convolutional networks for efficient semantic segmentation of 3D point clouds. This method and its follow-up works (Choy et al. , 2019; Han et al. , 2020) lead the ScanNet benchmark.

TangentConv (Tatarchenko et al. , 2018). This method introduces tangent convolution, which operates directly on surface geometry, for large-scale point clouds processing. In particular, the 3D point clouds are first projected as tangent images, followed by 2D convolutional networks.

PointNet (Qi et al. , 2017a). This is the pioneering work for directly operating on orderless point clouds by using shared MLPs and symmetrical max-pooling aggregation.

PointNet++ (Qi et al. , 2017b). This is the follow-up work of PointNet. It introduced multi-scale/resolution grouping to extract local geometrical patterns, and farthest point sampling to reduce memory and computational cost.

KPConv (Thomas et al. , 2019). This approach presents a powerful kernel point convolution to learn spatially correlation from unstructured 3D point clouds. A set of rigid or deformable kernel points are placed to learn varying local geometries. It has achieved state-of-the-art performance on the aerial DALES dataset (Varney et al. , 2020).

SPGraph (Landrieu and Simonovsky , 2018). This is one of the first learning-based frameworks that capable of processing large-scale point clouds with millions of points. The pipeline composed of geometrically homogeneous partition, followed by superpoint graph construction and contextual segmentation. It is one of the top-performing approaches on the Semantic3D dataset.

RandLA-Net (Hu et al. , 2020). It is one of the latest works for efficient semantic understanding of large-scale point clouds. The computational and memory-efficient random sampling, and the hierarchical local feature aggregation are the key to the great performance of this method. It also achieves leading performance on the Semantic3D leaderboard (Hackel et al. , 2017).

Evaluation Metrics
Similar to most of the existing 3D point cloud benchmarks (Hackel et al. , 2017; Behley et al. , 2019; Armeni et al. , 2017), Overall Accuracy (OA), mean class Accuracy (mAcc), and mean Intersection-over-Union (mIoU) are adopted as the primary evaluation criteria of our SensatUrban benchmark. The detailed score of each metric is calculated as follows:

OA=TPTP+FP+FN+TN
(1)
mAcc=1ùê∂‚àëùëê=1ùê∂TPùëêTPùëê+FNùëê
(2)
mIoU=1ùê∂‚àëùëê=1ùê∂TPùëêTPùëê+FPùëê+FNùëê
(3)
where TP, TN, FP, FN denote the true-positive, true-negative, false positive, and false negative, separately. C is the total number of semantic categories. Note that, these scores can also be calculated based on the confusion matrix.

Benchmark Results
We then evaluate the performance of the aforementioned baselines on our urban-scale SensatUrban dataset. The quantitative results including the per-class IoU scores are reported in Table 5. Note that, we faithfully follow the experimental settings and the publicly available implementation provided by each baseline in their original manuscript. All methods are trained on the same training split for a fair comparison.

Table 5 Evaluation of selected baselines on our SensatUrban benchmark
Full size table
Not surprisingly, the performance of all baselines shows varying degrees of degradation, compared with that achieved in other similar aerial point cloud datasets (Varney et al. , 2020). Specifically, the recent KPConv (Thomas et al. , 2019) shows the best mIoU performance, but only with a mIoU score of 57.58%, which is still far from satisfactory in practice. In particular, several infrastructure-oriented semantic categories such as rail, footpath, and bridge are poorly segmented. Additionally, we also noticed that the category bike is completely misclassified by all baselines. In general, all baselines are more likely to achieve better segmentation performance in categories with simple geometrical structure and dominant proportion, such as ground, vegetation, building and car, while achieving relatively limited performance on categories such as wall, bridge, and water. Additionally, different baselines have vastly different performances in individual semantic categories, without a clear leader. Overall, there remain several particular challenges for selected baselines to achieve satisfactory segmentation performance in the proposed city-scale SensatUrban dataset. Motivated by this, we then dive deep into the challenges that arise from our new urban-scale dataset.

Challenges
In this section, we further analyze the key challenges to generalize existing deep segmentation models to urban-scale photogrammetry point clouds. In particular, we first identify several unique challenges from the perspective of dataset characteristics. Next, we further explore the potential solutions and perform specific experiments to verify the effectiveness. Note that, this paper is not aiming to introduce specific new algorithms to solve all these challenges, but hopes to point out unresolved issues and provide in-depth analysis and insights, eventually stimulate the development of fine-grained urban-scale point cloud understanding.

Data Preparation
Considering the limited memory of modern GPUs, it is infeasible and unrealistic to directly accommodate and process urban-scale point clouds with billions of points in practice. As a result, the original point cloud data are usually partitioned into small pieces or downsampled before feed into existing neural architectures, so as to find a trade-off between the computational efficiency and segmentation accuracy.

In particular, the early works including PointNet (Qi et al. , 2017a), PointNet++ (Qi et al. , 2017b), and their variants usually first divide the large point clouds into equally-sized small blocks with partial overlap (e.g., 1m√ó1 m blocks in the S3DIS dataset (Armeni et al. , 2017). However, the final segmentation performance is highly-sensitive to the input block size. Large blocks with massive points lead to an unaffordable GPU memory cost, while small blocks inevitably break the objects‚Äô geometrical structure. Recent works such as KPConv (Thomas et al. , 2019) and RandLA-Net (Hu et al. , 2020) resort to grid or random down-sampling at the beginning to reduce the total amount of points. Additionally, several other works (Ye et al. , 2018) applied different partitioning or downsampling steps to preprocess the raw point clouds. Overall, various data preparation steps are intensively-involved in existing neural pipelines, but there are still no standard and principled preparation steps in literature, not to mention comprehensive evaluation and analysis.

To further investigate the impact of different data preparations on the final segmentation performance, we standardized a unified two-step data preprocessing framework. The detailed descriptions are as follows:

Step 1 Reducing the redundant points in the original point clouds through downsampling. This can be achieved by using (1) random downsampling (Hu et al. , 2020) or (2) grid-downsampling (Thomas et al. , 2019). In particular, random downsampling has superior computational and memory efficiency, while grid downsampling is robust to varying point densities. Both methods can significantly reduce the total amount of points.

Step 2 Iteratively feeding mini-batches of point subsets into the network. This can be achieved by first constructing efficient space partitioning data structures such as a KDTree, and then either query (1) constant-number point subsets or (2) constant-volume point subsets from specific regions. In particular, constant-number input sets are usually obtained by querying a fixed number of neighboring points with regard to a specific point (Hu et al. , 2020), while the constant-volume input sets are achieved by cropping fixed-size point cloud chunks (e.g., cubes, spheres) (Qi et al. , 2017a; Qi et al. , 2017b) centered on a specific point. Note that, the query points are random initialized and dynamically updated as in Thomas et al. (2019).

To evaluate the impact of 4 different combinations of both Step 1 and Step 2 on the segmentation performance, we select two representative approaches PointNet (Qi et al. , 2017a) and RandLA-Net (Hu et al. , 2020) as the baselines. For a fair comparison, we set the grid size for grid downsampling as 0.2m, while the downsampling ratio is set to 1/10, so as to keep similar number of points after downsampling operation. For constant-number inputs, we implement this by using a prebuilt KDTree to query a fixed number of neighboring points of the center point as inputs. For constant volume inputs, we first crop a fixed-size volume (e.g., 8m√ó8 m block) around the center point, followed by random down(up)-sampling to align the number of different input sets.

Analysis Table 6 reports the quantitative semantic segmentation scores achieved by baseline approaches with different input preparations. Table 7 shows the number of points left after downsampling, and the detailed time used for downsampling. It can be seen from the results that:

Both two baseline approaches consistently show better performance when adopting constant number input sets, compared with corresponding variants which using constant volume input sets.

Both PointNet and RandLA-Net show slightly better segmentation performance when using grid downsampling at the very beginning, compared with the counterpart using random downsampling. However, the total time consumption for grid downsampling is significantly large than using random downsampling (129s vs. 1107s) when evaluated on the same hardware configuration with an Intel CoreTM i9-10900X CPU and an NVIDIA RTX 3090 GPU.

Table 6 Semantic segmentation results achieved by selected baselines (Hu et al. , 2020; Qi et al. , 2017a) with different input preparation steps
Full size table
Table 7 Comparison of the grid downsampling and random downsampling at the beginning of data preparation
Full size table
To summarize, our experiments demonstrate the importance of data preparation for the semantic segmentation performance. Although this issue has been overlooked by the community for a long time, we show that the same network architecture can bring up to 10% performance gap, when equipped with different data preparation steps. Therefore, it is desirable and encouraged to further investigate the effective data preparation schemes, especially for our urban-scale point cloud datasets.

Geometry Versus Appearance
Different from the point clouds acquired by the airbone LiDAR sensors (Varney et al. , 2020; Roynard et al. , 2018; Behley et al. , 2019; Zolanvari et al. , 2019) , the point clouds in our SensatUrban are colorized with fine-grained point-wise RGB information. Intuitively, the additional color features can provide informative appearance, further enable existing neural architectures to distinguish between heterogeneous semantic categories with similar geometrical structure (e.g., grass on the ground). However, the additional color information may also introduce distractors, which in turn deteriorate the final performance.

Existing techniques usually integrated the RGB color as additional channels of the input feature map feed into the network. However, the recent ShellNet (Zhang et al. , 2019) learn the semantics from the pure spatial coordinates, but also achieves surprisingly good results. Overall, it remains an open question whether, and how, the color information impacts the final segmentation performance. To this end, we further conduct comparative experiments to verify the impact of the color information to the final segmentation performance. In particular, five baselines including PointNet/PointNet++ (Qi et al. , 2017a; Qi et al. , 2017b), SPGraph (Landrieu and Simonovsky , 2018), KPConv (Thomas et al. , 2019), and RandLA-Net (Hu et al. , 2020) are selected for 10 groups of experiments. Each baseline is trained with the pure geometrical information (i.e., 3D coordinates) or both 3D coordinates and RGB appearance, separately.

Analysis. We report the quantitative results achieved by the selected five baselines with/without the usage of color in the input point clouds. We can see that:

All of PointNet/PointNet++, KPConv, and RandLA-Net achieve significant performance improvement when the color features are utilized, compared with the use geometrical coordinates alone. It is noted that categories with significant performance improvements include bridge, footpath, and water, since these categories are geometrically indistinguishable.

We also noticed that the performance improvement of SPGraph is relatively marginal (only 2%) compared with other baseline approaches. This is likely due to the homogenous geometrical partition used in its framework, which purely relies on the geometrical structure but ignores the informative color.

Apart from the quantitative results, we also explicitly visualize the qualitative results achieved by these baselines in Fig. 7. To summarize, our experiments highlight the importance of color information to the fine-grained understanding of urban-scale point clouds, reflecting the advantages of our SensatUrban dataset over other existing aerial point clouds datasets collected by LiDAR, such as DALES (Varney et al. , 2020), NPM3D (Roynard et al. , 2018), and DublinCity (Zolanvari et al. , 2019). In particular, the color information is particularly important for distinguishing heterogeneous categories with consistent geometric structures (e.g., grass on the road), enabling a higher level of semantic understanding. This also provides insights for future aerial mapping campaigns, where color information and even other spectral bands may be useful for the semantic understanding.

The Impact of Skewed Class Distribution
Although data preparations and the usage of color information have been considered, it is still noted that the performance of different semantic categories varies greatly. For example, all baseline methods can achieve excellent segmentation performance on vegetation, with IoU scores up to 99%, while completely failed in detecting rare patterns such as bikes. Fundamentally, this is because of the extremely imbalanced distribution of our dataset. As illustrated in Fig. 6, the SensatUrban dataset is dominated by categories such as ground/vegetation/building, which are commonly appeared in urban areas of modern cities. However, categories such as rail/bike, despite being highly important for infrastructure-oriented applications, occurring much less frequently than the prevalent categories. As consequence, the selected baselines show a biased tendency towards the prevalent categories during inference, due to the scarce occurrence of the under-represented categories.

Fig. 7
figure 7
Qualitative results of PointNet (Qi et al. , 2017a), PointNet++ (Qi et al. , 2017b), RandLA-Net (Hu et al. , 2020) and KPConv (Thomas et al. , 2019) on the test set of SensatUrban dataset. The black dashed box highlights the inconsistency predictions with the ground-truth label

Full size image
Table 8 Evaluation of semantic segmentation performance of five selected baselines on our SensatUrban dataset with/without the usage of color information
Full size table
Table 9 Evaluation of semantic segmentation performance of PointNet (Qi et al. , 2017a) and RandLA-Net (Hu et al. , 2020) with different loss functions
Full size table
Table 10 Cross-city generalization performance of selected baselines on our SensatUrban dataset
Full size table
Table 11 Cross-city generalization performance of selected baselines on our dataset
Full size table
It remains an open question to learn from training data with skewed distributions. In this paper, we attempt to alleviate this problem from the perspective of the loss function. In particular, advanced loss functions are utilized to adaptively re-weight the contributions of each point that belongs to different categories, eventually guiding the network to achieve a more balanced performance across different categories. Specifically, by taking PointNet and RandLA-Net as baselines, we replace the original vanilla cross-entropy loss with four off-the-shelf loss functions, including weighted cross-entropy with inverse frequency (Cortinhal et al. , 2020), or with inverse square root (sqrt) frequency (Rosu et al. , 2019), Lov√°sz-softmax loss (Berman et al. , 2018), and focal loss (Lin et al. , 2017).

Analysis We report the detailed segmentation performance of two baselines achieved with five different loss functions. We can see the performance of all baselines has been improved when advanced loss functions are adopted. This clearly demonstrated that the sophisticated loss functions are indeed effective to alleviate the problem of imbalanced class distribution. Notably, we also noticed that the mIoU score of RandLA-Net has been improved by 5% when using the weighted cross-entropy loss. In particular, the score in the most under-represented category bike has been improved by more than 20%. Although the performance on minority categories is still far from satisfactory, the improvement is considerably encouraged and we suggest that more research could be conducted on this challenge, so as to fully tackle this research problem.

Table 12 The class mapping from DALES and SensatUrban dataset to the final unified semantic categories
Full size table
Table 13 Statistics of the DALES dataset and SensatUrban dataset after class mapping
Full size table
Cross-City Generalization
One of the main challenges of existing deep neural architectures is how to enhance the generalization capability to unseen scenarios, especially out-of-distribution data, since neural networks are usually data hungry and tend to overfit the training data. Motivated by this, we further explore the generalization performance of existing representative baselines on our SensatUrban dataset, since our dataset is composed of data collected from different cities, hence naturally suitable for evaluation the generalization abilities. In particular, five baseline approaches are included in our generalization experiments, that is: PointNet/PointNet++ (Qi et al. , 2017a; Qi et al. , 2017b), SPGraph (Landrieu and Simonovsky , 2018), KPConv (Thomas et al. , 2019), and RandLA-Net (Hu et al. , 2020). The detailed four groups of experimental schemes are described as follows:

Group 1: Train Birmingham/Test Birmingham. All of the five baseline approaches are only trained and tested on the training split and test split of Birmingham, respectively.

Group 2: Generalize from Birmingham to Cambridge: All of the five baselines are trained on the training split of Birmingham, and tested on the testing split of Cambridge.

Group 3: Train Cambridge/Test Cambridge: Analogous to group 1, all of the 5 baselines are only trained on the training split of Cambridge, and then tested on the testing split of the same region.

Group 4: Generalize from Cambridge to Birmingham: the above well-trained 5 baseline models in group 3 are directly tested on the testing split of Birmingham.

Analysis We report the detailed results achieved in the first two groups of experiments in Table 10, and the last two groups of experiments in Table 11. It can be seen that the performance of all baselines shows a significant decrease (approximate 20% drop on average in mIoU scores) when generalizing the trained model to unseen urban areas in other cities, despite the data collected from different cities are actually in the same domain (i.e., captured using the same sensor). This demonstrates the limited generalization capacity of selected baseline approaches. Interestingly, we also noticed that the performance of dominant semantic categories such as vegetation and building are not severely affected, while the under-presented categories including rail and water show visible performance degradation. This shows the baseline approaches are actually overfitted to the prevalent categories, while they failed to learn generalized and meaningful representation for minority categories. Overall, generalizing the trained deep segmentation model to unseen data, especially point clouds with different distributions, remains an open question in this area. Therefore, we hope our SensatUrban dataset could highlight the limited generalization capacity of existing deep neural architectures, and inspire more research to be conducted on this challenging problem.

Cross-Dataset Generalization
Another interesting question is whether the deep model trained on our SensatUrban dataset can be well generalized to other similar airborne point cloud datasets (Varney et al. , 2020; Zolanvari et al. , 2019), or vice versa. Intuitively, this task seems even more challenging than cross-city generalization, since the point clouds acquired from different cities in our dataset are inherently homogeneous. That is, the point cloud is reconstructed from sequential aerial images captured by the same camera, with the identical data process pipeline. However, point clouds in different datasets are likely to be collected by distinct acquisition sensors (i.e., airborne LiDAR vs. photogrammetry camera) and generated by using different mapping techniques. Moreover, the data distribution, point density, geographic regions, scene contents and annotation practices may vary greatly. Albeit interesting, there are few relevant studies in the field of 3D point cloud semantic understanding.

In this paper, we move a step forward to explore how the domain shift in different datasets affects the semantic learning of deep neural networks. Specifically, we select the recent aerial LiDAR point clouds dataset DALES (Varney et al. , 2020), along with the proposed photogrammetry SensatUrban dataset, to evaluate the cross dataset generalization capacity of existing segmentation algorithms. Note that, due to the inconsistent taxonomies and annotation practice, the semantic categories (i.e., 8 valid categories in DALES vs. 13 valid categories in SensatUrban) and definitions are different in these two datasets. Therefore, we first reconcile the taxonomies and map the semantic categories into the newly defined 6 consistent semantic categories, so as to properly evaluate the generalization performance across datasets. The detailed class mapping from each dataset to the unified taxonomy is shown in Table 12. The statistics (i.e., the number of points in the training and test subset) after class mapping is reported in Table 13. It can be seen that the class distribution of the two datasets exhibits visible differences. Here, we select the representative PointNet and RandLA-Net as the baselines, for the evaluation of intra-dataset generalization, and cross-dataset generalization performance. Note that, the baseline approaches are trained with the usage of 3D spatial coordinates only, since the color information is not available in LiDAR point clouds provided by the DALES dataset.

Table 14 Quantitative cross-dataset generalization results were achieved by the selected baseline approaches on the proposed SensatUrban dataset and the DALES dataset
Full size table
Analysis We can see that: (1) RandLA-Net has achieved superior performance in the intra-dataset evaluation, since the overall difficulty is reduced after the class mapping. (2) Although we considered the point density and the number of input points during data preprocessing, the cross-dataset generalization performance of both two baseline approaches is still significantly lower than the intra-dataset evaluation (30%+), demonstrating that domain shifts in different datasets play a key role in preventing model generalization. Future studies are encouraged to further reduce the domain gap between different point cloud datasets, especially in light of the different configurations of existing LiDAR point clouds.

Semantic Learning with Fewer Labels
Deep learning-based methods are hungry for massive training data (Wei et al. , 2020). For fully-supervised segmentation pipelines such as (Hu et al. , 2020; Thomas et al. , 2019; Qi et al. , 2017a; Qi et al. , 2017b), a large amount of fine-grained per-point annotations are usually required. However, it is extremely time-consuming and labor-intensive to manually annotate an urban-scale point cloud dataset with thousands of millions of points in practice. To this end, we further investigate the possibility of semantic learning with limited annotations on our SensatUrban dataset.

Table 15 Quantitative results achieved by PointNet (Qi et al. , 2017a), PointNet++ (Qi et al. , 2017b), and RandLA-Net (Hu et al. , 2020) with different settings (varying number of semantic annotations)
Full size table
Table 16 Quantitative results achieved by using OcCo (Wang et al. , 2020), Jigsaw (Sauder and Sievers , 2019) and Random (Rand) initialization on the SensatUrban dataset, based on PointNet (Qi et al. , 2017a), PCN (Yuan et al. , 2018) and DGCNN (Wang et al. , 2019) encoders
Full size table
Inspired by the weak supervision setting proposed in Xu and Lee (2020), we have conducted six groups of experiments by training all baselines with different forms of semantic annotations (i.e., weak supervisions) in our dataset. For simplicity, we only adopt PointNet (Qi et al. , 2017a), PointNet++ (Qi et al. , 2017b) and RandLA-Net (Hu et al. , 2020) as baseline networks in the following groups of experiment:

Only 1 point annotated per category in each point cloud.

Only 1% points annotated per category in each point cloud.

Only 1% points annotated in each point cloud (randomly).

10% points annotated per category in each point cloud.

10% points annotated in each point cloud (randomly).

100% (all) points annotated in each point cloud.

Analysis Table 15 shows the detailed quantitative results achieved by three baselines under different settings. It can be seen that:

Surprisingly, both PointNet, PointNet++, and RandLA-Net can achieve comparable performance with their fully-supervised counterpart, even when training with a small fraction of labeled points (e.g., 1% or 10%). This implies that the existing per-point annotations may exist large information redundancy, it is possible to learn semantics with limited annotations.

The performance of all baselines in group 2 and group 4 are better than group 3 and group 5, demonstrating that randomly annotating a tiny fraction (e.g., 1%, 10%) of all points is inferior to randomly annotating a tiny fraction of points in each semantic category. Although randomly annotating a tiny fraction of all points is more practical and feasible.

The segmentation performance of all baselines in group 1 (i.e., with 1 point annotation) are far from satisfactory. Basically, the networks cannot converge, primarily because the supervision information is extremely insufficient. However, this is also one of the simplest and cheapest ways of annotation in practice. More studies should be conducted in this direction to further improve the performance.

Thanks to the availability of several large-scale point cloud datasets, the community can always assume that the amount of labeled training data is sufficient. However, we have demonstrated that comparable performance can also be achieved using the same architecture with limited semantic annotations, highlighting the great potential of weakly-supervised semantic segmentation frameworks. This motivates us to further investigate how to achieve better performance under limited annotation, and how to choose the best annotation strategy under fixed budgets.

Self-Supervised Pre-Training on 3D Point Clouds
Pre-training a network on a rich source set in a self-supervised or unsupervised way has been demonstrated to be highly effective for high-level downstream tasks (e.g., segmentation, detection) in several 2D vision tasks (Chen et al. , 2020; He et al. , 2020). However, self-supervised pre-training on 3D point clouds is still in its infancy, only a handful of recent works (Sauder and Sievers , 2019; Wang et al. , 2020; Xie et al. , 2020; Zhang et al. , 2021; Hou et al. , 2020; Poursaeed et al. , 2020) have started to explore self-supervised learning on unstructured 3D point clouds. In particular, all existing methods are still pre-trained on the object-level datasets (e.g., ModelNet40 (Wu et al. , 2015)) or indoor scene-level datasets (e.g., ScanNet (Dai et al. , 2017)). Considering the urban-scale property of SensatUrban, it is particularly suitable for verifying the effectiveness of the existing pretraining strategy on our dataset.

To this end, we conducted three groups of experiments on our SensatUrban dataset to compare the performance of:

Pre-training with occlusion completion (Wang et al. , 2020).

Pre-training with context prediction (jigsaw) (Sauder and Sievers , 2019).

Training from scratch.

For simplicity, we faithfully follow the three baseline networks used in their original paper (Wang et al. , 2020; Sauder and Sievers , 2019), including PointNet (Qi et al. , 2017a), PCN (Yuan et al. , 2018), and DGCNN (Wang et al. , 2019). The detailed experimental results are shown in Table 16.

Analysis From the results in Table 16 we can see that, although both baseline networks are purely pre-trained on the object-level point clouds in ModelNet40, the fine-tuning models can still achieve a certain performance improvement on our dataset. In particular, the performance of several minority categories, such as rail and bridge, has a significant performance improvement (up to nearly 10%), primary because the pre-trained models are less prone to overfitting to the majority categories, compared to directly training from scratch.

This further demonstrates the feasibility and potential of the self-supervised pre-training paradigm. However, the existing pre-training framework (Wang et al. , 2020; Sauder and Sievers , 2019) are still limited to object-level point clouds, and it is non-trivial to be extended to large-scale point clouds. On the other hand, most of the existing pre-training schemes are based on auxiliary (pre-text) tasks. It is worth investigating how to leverage contrastive learning to achieve better performance on 3D point clouds. Finally, to further facilitate the research in this research area, we also release the unlabeled York point clouds, encouraging further research exploration on this part of the data.

Discussion and Limitations
Although the proposed SensatUrban dataset is currently the largest publicly available photogrammetric point cloud dataset, it is not without limitation. In general, instance annotation would be a meaningful addition to our dataset. However, due to the tremendous labeling effort of point-wise instance labels, we leave the integration of instance labels for future exploration. On the other hand, our dataset is reconstructed from the sequential aerial images captured by a single sensor (i.e., camera), it would be interesting to further investigate the same-source data acquired by different sensors. For example, the data acquired from both a camera and a LiDAR system integrated on the same UAV platform (K√∂lle et al. , 2021).

Summary and Outlook
This paper introduces SensatUrban: an urban-scale photogrammetric point cloud dataset composed of 7.6 km2 of urban areas in three UK cities, and nearly 3 billion richly annotated points (each with one of the 13 semantic categories). A comprehensive benchmark is also built based on this dataset and a number of selected representative baselines. In particular, extensive comparative experiments have revealed several challenges in generalizing existing semantic segmentation methods to urban-scale point clouds, including how to conduct data preparation, whether and how to utilize the color information, how to tackle with the extremely imbalanced class distribution, generalizing to unseen scenarios, and the potential of weakly/self-supervised learning techniques. Besides, extensive benchmarking results are conducted and in-depth analysis are also provided. In the future, we will further increase the scale and richness (i.e., instance annotation, corresponding 2D images) of our SensatUrban dataset. We hope that our SensatUrban dataset could be an immensely useful resource and a canonical benchmark to related research communities including 3D computer vision, earth vision and remote sensing, inspiring and supporting future advancing research in related areas.