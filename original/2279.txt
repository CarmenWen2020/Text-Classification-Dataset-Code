Pedestrian detection and re-identification have progressed significantly in the last few years. However, occluded people are notoriously hard to detect and recognize, as their appearance varies substantially depending on a wide range of occlusion patterns. In this paper, we aim to propose a simple and compact method based on CNNs for occlusion handling. We start with interpreting CNN channel features of a pedestrian detector, and we find that different channels activate responses for different body parts respectively. These findings motivate us to employ an attention mechanism across channels to represent various occlusion patterns in one single model, as each occlusion pattern can be formulated as some specific combination of body parts. Therefore, an attention network with self or external guidances is proposed as an add-on to the baseline CNN method. Also, we propose an attention guided self-paced learning method to balance the optimization across different occlusion levels. Our proposed method shows significant improvements over the baseline methods for both pedestrian detection and re-identification tasks. For pedestrian detection, we achieve a considerable improvement of 8pp to the baseline FasterRCNN detector on the heavy occlusion subset of CityPersons and on Caltech we outperform the state-of-the-art method by 5pp. For pedestrian re-identification, our method surpasses the baseline and achieves state-of-the-art performance on multiple re-identification benchmarks.

Access provided by University of Auckland Library

Introduction
Pedestrian detection and re-identification are two essential tasks of analyzing pedestrians in visual data. They have been attracting intensive interests in both academia and industry. During the last decade, great progress has been achieved (Benenson et al. 2014; Zhang et al. 2018a), especially since CNNs were successfully adopted for both tasks (Hosang et al. 2015; Zhang et al. 2016b, 2017; Yi et al. 2014; Li et al. 2014). Although the state-of-the-art performance is plausible across different datasets, we observe that the performance of both tasks drops significantly as occlusion grows. In real world applications, occlusion happens very often but is challenging to handle.

In the area of pedestrian detection, some efforts have been made to handle occlusion, but most of them train ensemble models for most frequent occlusion patterns (Enzweiler et al. 2010; Mathias et al. 2013; Ouyang and Wang 2012; Tian et al. 2015a). The major drawback of those methods is that it is very time-consuming at both training and testing times. Some other works propose to model different occlusion patterns in a joint framework (Zhou and Yuan 2017; Ouyang and Wang 2013), but they still rely on an integration of a set of occlusion/part detection scores. The above methods are not able to cover all occlusion patterns, and the independent integration procedure inhibits error propagation to the occlusion/part detection module. To encourage more work for occlusion handling, the CityPersons dataset has been proposed (Zhang et al. 2017) for pedestrian detection, which consists of a large number of occlusion cases with various patterns.

In the area of pedestrian re-identification, there are fewer works focusing on occlusion handling. Zheng et al. (2015b) proposed a fused matching strategy composed of a local patch-level model called Ambiguity-sensitive Matching Classifier (AMC) and a global part-based model called Sliding Window Matching (SWM). However, the computational demand of AMC+SWM is rather high. He et al. (2018) proposed to reconstruct the feature maps of a holistic pedestrian from the visible parts by lasso regression. Other works (Zhong et al. 2017b; Huang et al. 2018) manually generate occluded samples for data augmentation. However, they are not able to handle the challenge of arbitrary natural occlusions.

In order to deal with a wide range of frequent and less frequent occlusion patterns in one coherent model, we propose different attention mechanisms, which guide the network to pay more attention to the visible body parts. These attention mechanisms are motivated by the fact that different channels of CNN-based detectors, in our case a FasterRCNN pedestrian detector, are selective and show strong responses for different body parts. These explicit representations of body parts motivate us to propose channel-wise attention mechanisms to learn proper attention parameters for different channels so as to handle different occlusion patterns effectively.

In summary, our contributions are as follows:

(1)
We provide an analysis to understand the relation between body regions and different CNN channel features of a pedestrian detector, and find many of them are localizable and interpretable.

(2)
Motivated by the above findings, we propose a channel-wise attention mechanism to obtain more effective representations for occluded pedestrians. Different occlusion patterns are dynamically handled via an additional attention net added to the feature extraction network. We explore different attention guidances, including CNN features, visible-boxes and body parts. This mechanism has been successfully used for both pedestrian detection and re-identification tasks, where significant improvements have been achieved for occluded pedestrians. Also, our method only makes minor changes to the vanilla backbone network, and thus is easy to implement and to train.

(3)
In order to achieve balanced optimization across different occlusion levels, we propose an attention guided self-paced learning method, i.e.at each iteration only those samples well represented by the attention net are taken into account for loss computation. This self-paced learning method guarantees consistent improvements across different occlusion levels.

To the best of our knowledge, this paper is the first attempt to employ channel-wise attention for pedestrian detection and re-identification, focusing on occludion handling.

This paper is organized as follows: Sect. 2 presents related works in the fields of pedestrian detection and re-identification; Sect. 3 provides some analysis on the correlation between body parts and channel features of a CNN-based pedestrian detection; Sect. 4 introduces our approach using channel-wise attention in CNNs to obtain more effective representations for pedestrians; Sects. 5 and 6 provide experiments for pedestrian detection and re-identification and some discussions regarding how the attention handles occlusion; in the end, Sect. 7 makes a conclusion and points out possible extensions for future work.

Related Work
Since we use CNNs as our basic structure, and an attention network as an add-on to handle occlusion, we review recent work on CNN based pedestrian detectors and re-identification methods, occlusion handling for pedestrians and attention mechanisms respectively.

Pedestrian Detection with CNNs
Convolutional neural networks (convnets) have achieved great success for the generic object detection task on the ImageNet (Krizhevsky et al. 2012), Pascal, and MS COCO datasets (He et al. 2016). Early works (Hosang et al. 2015; Zhang et al. 2016b) applying convnets for pedestrian detection are based on the RCNN structure (Girshick et al. 2014), which relies on high-quality external proposals to achieve good performance. More recently, FasterRCNN (Ren et al. 2015) has become the de-facto standard architecture, which allows end-to-end learning. Most recent detectors are built based on the two-stage FasterRCNN architecture and achieve better performance (Cai et al. 2016; Zhang et al. 2017; Brazil and Liu 2019; Li et al. 2016; Lin et al. 2018). Some other works employ the single-stage detection architecture aiming for high computational speed (Liu et al. 2018b; Noh et al. 2018; Song et al. 2018; Wei Liu 2019). In this paper, we use the adapted FasterRCNN (Zhang et al. 2017) as our baseline, which makes proper modifications to the vanilla FasterRCNN (Zhang et al. 2017) and reaches state-of-the-art results with simple implementation.

Pedestrian Re-Identification with CNNs
CNN-based models have attracted extensive attention in the field of pedestrian re-identification since the success of two pioneer works (Yi et al. 2014; Li et al. 2014). Most of those CNN models can be categorized into two groups. The first group uses the siamese model with image pairs (Yi et al. 2014; Li et al. 2014; Ahmed et al. 2015; Varior et al. 2016; Liu et al. 2017) or triplets (Ding et al. 2015; Cheng et al. 2016) as inputs. The main idea of these works is to minimize the feature distance between the same person and maximize the distance between different persons. The second group of works formulate the re-identification task as a classification problem (Xiao et al. 2016; Zheng et al. 2016a, 2017a). Recently, some works utilize human keypoint heatmaps as an axillary information for pedestrian re-identification. For instance, (Su et al. 2017) propose to align the pedestrian images by cropping and re-locating each body part according to the detected joints. Saquib Sarfraz et al. (2018) concatenate keypoint heatmaps along the RGB channels of the input image to assist viewpoint prediction. Xu et al. (2018) simultaneously solve pose estimation and pedestrian re-identification in a joint framework, where spatial attention maps and visibility scores are generated from the pose estimation results. Our work shares similar motivation, but employs channel-wise attention instead of spatial attention. In our method, the pose estimation problem is solved independent to the re-identification task, resulting in an easier optimization procedure.

Occlusion Handling for Pedestrian Detection
The most intuitive occlusion handling strategy is to learn a set of detectors, each corresponding to one specific manually designed occlusion pattern. Different features are employed, including hand-crafted features (Enzweiler et al. 2010; Mathias et al. 2013) and deep convolutional features (Ouyang and Wang 2012; Tian et al. 2015a). The final decision is made by integrating the output of these ensemble models. The drawback of those methods is that each part/occlusion pattern detector is learned independently, and it is time consuming to apply the set of models at test time. On the other hand, some other works proposed to learn multiple occlusion patterns in a joint way (Zhou and Yuan 2017; Ouyang and Wang 2013), which saves a lot of training and testing time. However, the final decision is still made by integrating multiple part scores, which makes the whole procedure more complex and hard to train.

More recently, many efforts have been made to achieve more effective representations. Some recent works find that it is beneficial to enhance the feature representations via aggregating features from nearby proposals, sub-regions inside each proposal or neighbouring frames (Li et al. 2020; Zhang et al. 2018b; Wu et al. 2020); it is also helpful to introduce auxiliary tasks, e.g.segmentation (Brazil et al. 2017; Noh et al. 2018; Pang et al. 2019), counting (Xie et al. 2020) or visible box estimation (Zhou and Yuan 2018). On the other hand, NMS serves as post-processing for each detector. Some efforts have also been made to improve it with the help of predicted density or visible boxes (Liu et al. 2019a; Huang et al. 2020a), since traditional greedy NMS can hardly handle crowded regions.

But these methods are constrained to limited occlusion patterns represented by the visible box and are not able to be extended to diverse occlusions. In contrast, we propose an attention net, which is more flexible to various occlusion patterns and is easier to adapt to different detection architectures and guidances.

Attention Mechanisms in CNNs
The attention mechanism has been widely used in CNNs for different computer vision tasks, for instance, object detection (Bell et al. 2016), digits recognition (Jaderberg et al. 2015), and pose estimation (Newell et al. 2016). The above works all investigate to model spatial correlations. In contrast, (Hu et al. 2017) proposes squeeze-and-excitation networks to model the interdependencies between channels of convolutional features. However, the channel-wise attention is self guided, i.e.no external signal is employed. In contrast, in this paper we will show that external guidance can be used and is verified to be more helpful to improve the performance of channel-wise attention mechanisms.

Body Parts and Channel Features
Convnets have shown to be capable of learning representative features for object detection and recognition, and some recent works analyze the interpretability of the hidden neurons by visualizing their activations. For instance, (Bau et al. 2017) performs network dissection and finds that many individual units respond to specific high-level concepts and (Zeiler and Fergus 2014; Gonzalez-Garcia et al. 2017) also find some filter responses can be linked to semantic parts.

Similarly, in this paper, we investigate whether channels can be related to human body parts in a pedestrian detector. We first train a FasterRCNN (VGG16) detector on the CityPersons training set. After training, we pick one arbitrary image from the CityPersons validation set, which contains multiple people, and let it pass the network for feature extraction. As default, on the top convolutional layer, we have 512 channels in total.

Fig. 1
figure 1
Relation to body parts of different channel features from a FasterRCNN pedestrian detector. Highlighted regions trigger strong activation inside each channel

Full size image
In the following, we examine the activations of each channel respectively. As shown in Fig. 1 for three representative channels, the original image is overlaid with the activation map. From the visualizations, we make the following observation: Many channels show some highly localizable activation pattern, relating them to specific body regions or body parts; the three channels show strong activations at people’s head, upper body and feet respectively. Similar findings are shown in (Simon et al. 2014), that in a bird classification network some channels are associated with parts.

To better understand the relation between body parts and all channels in a statistical way, we implement pixel-wise XOR operation between each binarized channel feature map and part detection heatmap (Insafutdinov et al. 2016). The correlation value for each pair is measured by the percentage of one values in the XOR map. The correlation value of t th channel can be computed as follows:

𝑐𝑜𝑟𝑟(𝑡)=max0≤𝑝≤𝑃−1𝑓𝑐ℎ𝑛(𝑡)⊕𝑝𝑎𝑟𝑡(𝑝),0≤𝑡≤𝐻−1
(1)
where 𝑓𝑐ℎ𝑛(𝑡) denotes the features from t th channel, and part(p) is the part detection heatmap of p th body part. The correlation value of t th channel is represented by the maximum value across P body parts. In the part detector we use (Insafutdinov et al. 2016), 𝑃=14.

We find on average, more than 35% channels show strong correlation (𝑐𝑜𝑟𝑟(𝑡)≥60%) with one of 14 part detection heatmaps. In Fig. 2, we show the correlation values histogram across 512 channels for all images on the CityPersons training set.

For each image, we consider those channels showing a correlation value larger than 60% as interpretable channels, among which, we further analyze which body parts are correlated by more channels. In Fig. 3, we make a comparison w.r.t. number of channels among different body parts for all images on the CityPersons validation set. We can see that the head-top and thorax keypoints are represented by the largest number of channels (over 20 channels each); each shoulder is represented by close to 20 channels; other body parts have less correlated channels. This observation is consistent with our observation that the head-shoulder area of pedestrians are most visible across all samples and are thus highly represented by channel features. On the other hand, we also find that the gap among different body parts is small, i.e.all body parts are well represented by the channel features.

Fig. 2
figure 2
Distribution of correlation values for 512 channels of all images on the CityPersons training set

Full size image
Fig. 3
figure 3
Distribution of interpretable channels correlated with different body parts. Numbers are counted by averaging on all images on the CityPersons validation set

Full size image
The above observations encourage us to explore the possibility of channel-wise attention for occluded pedestrian detection as such an attention mechanism can guide the final feature representations focus more on the visible body regions and focus less on the occluded regions.

Fig. 4
figure 4
Different occlusion patterns lead to variation of human appearance

Full size image
Fig. 5
figure 5
Flowchart of attention guided network for pedestrian detection or re-identification. An attention network is added to the basic architecture to generate the weighting parameters 𝛺 for the top conv features

Full size image
Guided Attention in CNNs for Occlusion Handling
The major challenge of handling occlusion comes from the large variety of occlusion patterns, which leads to rather diverse appearances of human bodies, as shown in Fig. 4. In this paper, we propose to employ channel-wise attention in convnets allowing the network to learn more representative features for different occlusion patterns in one coherent model.

Overview
We show the flowchart of our attention guided network in Fig. 5, where both pedestrian detection and re-identification tasks are formulated as a classification problem. In our proposed method, an additional attention net is proposed to regress the channel-wise attention vector, namely 𝛺, which is used to apply a re-weighting operation on the multi-channel convolutional features. After the re-weighting procedure, the features are passed to the classification network.

Pedestrian Detection
The FasterRCNN detector obtains state-of-the-art results in pedestrian detection (Zhang et al. 2017). In this paper, we use it as the base detector in our experiments, while adding an attention network as a separate component to generate a channel-wise attention vector. The flowchart of our FasterRCNN detector with an attention network is shown in Fig. 6. The upper flow is a typical feature extraction procedure of a FasterRCNN detector: first, the input images go through the base net (e.g.VGG16); and then a region proposal network (RPN) is used to generate proposals; after that, the features for each proposal are generated by cropping from the top convolutional feature maps and a following RoiPooling layer produces the same length of features for each proposal. These features will go through the classification network for category prediction and bounding box regression. The FasterRCNN network can be trained end-to-end by optimizing the following loss function:

𝐿0=𝐿𝑟𝑝𝑛_𝑐𝑙𝑠+𝐿𝑟𝑝𝑛_𝑟𝑒𝑔+𝐿𝑐𝑙𝑠+𝐿𝑟𝑒𝑔,
(2)
where: 𝐿𝑟𝑝𝑛_𝑐𝑙𝑠 and 𝐿𝑐𝑙𝑠 are the cross-entropy loss for classification in the RPN and region classification network; 𝐿𝑟𝑝𝑛_𝑟𝑒𝑔 and 𝐿𝑟𝑒𝑔 are the 𝐿1 loss for bounding box regression.

Fig. 6
figure 6
Flowchart of attention guided FasterRCNN pedestrian detector. An attention network is added to the FasterRCNN architecture to generate the weighting parameters 𝛺 for the top conv features

Full size image
Pedestrian Re-identification
Among the state-of-the-art methods in pedestrian re-identification, ResNet50 (He et al. 2016) is widely used as the backbone network (Zheng et al. 2017a; Saquib Sarfraz et al. 2018; Zheng et al. 2016b; Huang et al. 2018; Xiao et al. 2017). In this paper, we truncate a ResNet50 from the input layer to ‘conv_4’ as the backbone net. The attention vector 𝛺 is multiplied upon ‘conv_4’ along channels. The re-weighted feature maps are then passed through the remaining layers of ResNet50 until the Global Average Pooling (GAP) layer, followed by a fully-connected layer with softmax activation.

The whole model is supervised by an identification loss. We choose cross-entropy loss for re-ID datasets and Online Instance Matching (OIM) (Xiao et al. 2017) for person search datasets, which is an improved cross entropy loss scalable to datasets with unlabeled persons.

At test time, we use L2 distance on top of the final person embedding for person matching.

Channel-Wise Attention
As discussed in section 3, many channels in a pedestrian CNN are localizable and can be related to different body parts. This observation strongly motivates us to perform re-weighting of channel features to handle various occlusion patterns. Let occlusion pattern n be defined as the following vector:

𝑜𝑐𝑐𝑙(𝑛)=[𝑣0𝑝0,𝑣1𝑝1,...,𝑣𝑘𝑝𝑘],𝑣𝑖∈{0,1},𝑖∈[0,𝑘],
(3)
where 𝑝𝑖 represents each body part and 𝑣𝑖 is a binary variable, indicating the visibility of the i-th part.

In typical CNNs, channels’ weights are fixed and thus do not vary across different samples. This mechanism limits the network’s adaptivity to various appearances. For example, when the person’s body is occluded as in Fig. 4a, the feet channel contributes to the final score irrespective of the occlusion. This, however, will typically result in a lower overall score as the occlusion patterns are too variable to allow to generate an equally high score as for un-occluded pedestrians.

Our intuition is to allow the network to decide for each sample, how much each channel should contribute in the final feature pool. Intuitively, the network should let those channels representing the visible parts contribute more, while the invisible parts contribute less.

The re-weighting of channels can be presented as follows:

𝑓𝑜𝑐𝑐𝑙(𝑛)=𝛺𝑇𝑛𝑓𝑐ℎ𝑛,
(4)
where 𝑓𝑐ℎ𝑛 indicates the top channel features, and 𝛺𝑛 is the weighting parameter vector for the nth occlusion pattern.

In this way, the importance of the channel features varies for each sample as its occlusion pattern changes. For example, when the left body is occluded, 𝛺 should be adjusted so that the corresponding channels representing the left body region have lower weights, which means they have lower impact on the final score.

Attention Networks
Fig. 7
figure 7
Three different attention nets use different attention guidances. The pedestrian detectors using these attention nets are denoted as FasterRCNN+ATT-self, FasterRCNN+ATT-vbb and FasterRCNN+ATT-part in the experiments respectively. The pedestrian re-identifier using part attention net is denoted as OIM+ATT-part in the experiments

Full size image
The attention network is an important component in our method to generate the attention vector 𝛺. As shown in the lower part of Fig. 5, the attention network takes an input of attention guidance G, and then learns a mapping function F used to regress 𝛺 as output:

𝛺=𝐹(𝐺𝑇).
(5)
While we have motivated the attention vector 𝛺 being related to specific occlusion patterns, it is important to note, that our attention vector 𝛺 in all our attention networks is continuous and thus not restricted to any particular discrete set of occlusion patterns as some previous work (Tian et al. 2015a; Zhou and Yuan 2017; Ouyang and Wang 2013). Instead, the attention vector 𝛺 is trained end-to-end for all our attention networks either through self-attention or guided by some additional external information.

We consider three different types of guidance G: (1) top convolutional features; (2) visible bounding boxes; (3) part detection heatmaps. Depending on which information we use as guidance, we define our attention nets as: CNNFtr attention, visible-box attention and part attention nets, respectively. We start with CNNFtr attention, and then further exploit to use external information as stronger guidance. We show an illustration for the above three attention nets in Fig. 7.

CNNFtr Attention Net
SENet is the first attempt to exploit channel-wise attention in CNNs (Hu et al. 2017). The goal is to enhance the representational ability for various samples by explicitly modelling the interdependencies between the convolutional channels. To this end, a “Squeeze-and-Excitation” (SE) block is proposed to perform sample-dependent feature re-weighting, through which the more informative features are selected while less useful ones are suppressed. The SE block is composed of one global average pooling layer and two consecutive fully connected layers. SENet is easy to implement, obtaining remarkable improvements while adding little additional computational costs.

Inspired by SENet, we design our CNNFtr attention net to learn the channel-wise attention parameters 𝛺. It is a re-implementation of SENet with an identical block structure. Since only convolutional features are used as input, we call it CNNFtr attention. We show the CNNFtr attention net in Fig. 7a, where we use the top convolutional features as guidance G to regress 𝛺.

Visible-box Attention Net
The CNNFtr attention net models the channel-wise attention using the channels themselves, while we believe the attention network’s capacity can be improved with external information as additional input or supervision. Intuitively, one useful guidance to regress 𝛺 should be the occlusion patterns themselves, as they contain information about visibility of body parts. Ideally, occlusion patterns should be defined as in equation 3, by indicating the visibility of each body part. However, in practice it is too expensive to obtain body part annotations. Alternatively, we define it coarsely by the combination of one full body bounding box along with one visible box, which are provided in some popular pedestrian datasets. Since we use the visible box as external guidance, we refer to this net as visible-box attention net.

However, the visible box is not available at test time, thus the occlusion pattern can not be simply used as input to the attention net. To overcome this problem, we propose to learn the occlusion pattern in a supervised manner inside the attention net. By analyzing the training data on the CityPersons dataset, we find the most frequent occlusions are as follows: (1) fully visible; (2) upper body visible; (3) left body visible; (4) right body visible. The other patterns are ignored as too little training data is available. In this way, the occlusion pattern estimation is formulated as a four-class classification task.

The visible-box attention network architecture is shown in Fig. 7b, where the occlusion pattern estimation subnet consists of one convolutional and two fully connected layers. Once the occlusion pattern is estimated, one convolutional layer is used for feature extraction followed by two fully connected layers to regress 𝛺. In this way, we add one more task of occlusion estimation to the pipeline, and the loss function of the whole system can be written as follows:

𝐿𝐴𝑇𝑇−𝑣𝑏𝑏=𝐿0+𝛼𝐿𝑜𝑐𝑐𝑙,
(6)
where 𝐿0 is the loss function used in vanilla pedestrian detection and re-identification methods (e.g. Eq. 2), and 𝐿𝑜𝑐𝑐𝑙 is defined as a cross-entropy loss for occlusion pattern classification. All the parameters in the network are optimized in an end-to-end fashion. We set 𝛼=1 by default.

It is worth noting that, as a side-effect, an estimate of the occlusion pattern is obtained at test time, not provided by previous methods.

Part Attention Net
Making use of visible bounding boxes allows us to train an occlusion pattern estimation subnet, which serves as a guidance to regress the continuous attention vector 𝛺. However, there are two problems with visible bounding boxes: (1) It is expensive to obtain visible boxes as additional training annotations; (2) Sometimes occlusion happens irregularly, resulting in that the visible part can hardly be covered by one single rectangular box, see two examples in Fig. 8.

To overcome the above two problems, we investigate to estimate the occlusion pattern by using body part detection results, which are supposed to predict the visibility of each body part, e.g.head, shoulder, arm, etc..

In principle, we can implement our part attention net in the same way as the visible-box attention net, inside which the occlusion patterns can be estimated and immediately used as guidance to regress 𝛺. However, on most pedestrian datasets, we do not have body part annotations for supervision, so we decide to use a pre-trained part detection network trained on the MPII Pose Dataset (Insafutdinov et al. 2016). This detector is a fully convolutional network, providing precise predictions for 14 human body keypoints. We apply this part detector without any changes or finetuning on the CityPersons dataset and achieve surprisingly good results. We show two examples in Fig. 8.

From Fig. 8, we can see that the two persons occluded by a pole and a car still trigger rather strong response at the location of the visible parts on the heatmaps. These results inform us that when a full body detector fails for an occluded person, the part detector is still able to make precise predictions for visible parts. Therefore, the part detection heatmaps can be used as an effective hint of occlusion patterns to guide the attention network.

The attention network using part detections is shown in Fig. 7c, where 14 keypoint heatmaps are used as input. As we assume that the spatial information plays an important role for guidance we apply one convolutional layer for feature extraction and two fully connected layers to regress the continuous attention vector 𝛺. This is in contrast to the CNNFtr attention net that uses global pooling instead.

Fig. 8
figure 8
Occluded persons show strong response on heatmaps for visible parts. We use pretrained part detectors from the DeeperCut paper (Insafutdinov et al. 2016). For some cases, the visible part cannot be covered by one single rectangle, while part heatmaps represent the occlusion patterns more precisely

Full size image
Attention Guided Self-Paced Training
We notice that for pedestrian detection, the optimization across different occlusion levels is extremely difficult. When we simply include all samples at different occlusion levels for training, the performance on the overall subset increases but it drops on some subsets, e.g.the fully visible subset. Aiming for a balanced training procedure, we propose to add occluded samples to the training pool in a progressive way. This can be achieved by an attention guided self-paced training strategy, which only considers those well represented samples for updating gradients at the current iteration, leaving those hard ones for latter training iterations.

The whole training procedure is composed of two rounds. First, we train a basic model by switching off the attention net only using fully visible samples, so as to allow the convolutional features to represent different body parts. At the second round, we include occluded samples at different occlusion levels in the training sample pool, but we carefully examine for each sample whether it should be taken into account for the loss computation at each iteration. Specifically, we introduce a weighting variable for each positive sample, and the loss function in Eq. 2 is modified as follows:

𝐿𝑆𝑃=𝐿𝑟𝑝𝑛_𝑐𝑙𝑠+𝐿𝑟𝑝𝑛_𝑟𝑒𝑔+1𝑀∑𝑗=1𝑀(𝑙𝑗𝑐𝑙𝑠+𝑙𝑗𝑟𝑒𝑔)+1𝑁∑𝑖=1𝑁𝑣𝑖(𝑙𝑖𝑐𝑙𝑠+𝑙𝑖𝑟𝑒𝑔),
(7)
where M and N are the numbers of negative and positive samples in the current batch; 𝑣𝑖 is the weight for a positive sample i. Please note the RPN losses are kept the same as in Eq. 2 as we found the RPN generally provides high recall across different occlusion levels, and thus we only modify the RCNN losses for better classification and regression.

Next, we determine 𝑣𝑖 for sample i based on the consistency between the generated attention vector 𝛺𝑖 and its occlusion pattern. Since this procedure requires priors on occlusion patterns, we only apply it to visible-box and part attention nets.

We determine 𝑣𝑖 for positive sample i (matched with ground truth t) via the following three steps:

Step 1: to group channels. For an arbitrary channel c, we observe its feature responses inside the full body bounding box 𝑓𝑏𝑏𝑡 for the matched ground truth t. We compute the average response both inside and outside the visible box 𝑣𝑏𝑏𝑡 as follows:

𝑟𝑡𝑖𝑛=1𝐴𝑣𝑏𝑏𝑡∑𝑟𝑝𝑐,𝑝∈𝑣𝑏𝑏𝑡,
(8)
𝑟𝑡𝑜𝑢𝑡=1𝐴𝑓𝑏𝑏𝑡−𝐴𝑣𝑏𝑏𝑡∑𝑟𝑝𝑐,𝑝∈∁(𝑓𝑏𝑏𝑡)(𝑣𝑏𝑏𝑡),
(9)
where 𝐴𝑓𝑏𝑏𝑡 and 𝐴𝑣𝑏𝑏𝑡 denote the number of pixels inside 𝑓𝑏𝑏𝑡 and 𝑣𝑏𝑏𝑡, respectively; 𝑟𝑝𝑐 represents the feature value at location p along channel c.

Intuitively, if the average response inside the visible box 𝑣𝑏𝑏𝑡 is larger than that outside it, then this channel is taken as a visible-part-correlated one; and vice versa. For each channel, we assign a tag based on the following criteria:

𝑇𝑐={𝑣𝑖𝑠,𝑟𝑡𝑖𝑛≥𝑟𝑡𝑜𝑢𝑡;𝑖𝑛𝑣𝑖𝑠,𝑟𝑡𝑖𝑛<𝑟𝑡𝑜𝑢𝑡.
(10)
For all C channels, we group them into two sets: visible-part-correlated channels and invisible-part-correlated channels, with different tags. The number of channels in the above two sets are denoted as 𝐾𝑣𝑖𝑠 and 𝐾𝑖𝑛𝑣𝑖𝑠.

Step 2: to compute attention weights statistics. In principle, visible-part-correlated channels are expected to be assigned larger weights by the attention net; while invisible-part-correlated channels are expected to be assigned smaller weights. Given the attention weight vector 𝛺𝑖 output from the attention net for sample i, we compute its statistics, i.e.the average weights for visible-part-correlated channels and invisible-part-correlated channels, respectively, as follows:

𝑤𝑖𝑣𝑖𝑠=1𝐾𝑣𝑖𝑠∑𝑐∈[1,𝐶],𝑇𝑐=𝑣𝑖𝑠𝛺𝑖𝑐,
(11)
𝑤𝑖𝑖𝑛𝑣𝑖𝑠=1𝐾𝑖𝑛𝑣𝑖𝑠∑𝑐∈[1,𝐶],𝑇𝑐=𝑖𝑛𝑣𝑖𝑠𝛺𝑖𝑐.
(12)
Step 3: to determine loss weight 𝑣𝑖. For an arbitrary sample i, if its weights for visible-part-correlated channels are generally larger than that of invisible-part-correlated channels, then this sample is considered as well represented by the attention net and is taken into account for gradient computation at the current iteration; otherwise, this sample is discarded in the loss function. According to this criteria, the binary loss weight 𝑣𝑖 is determined by the following equation:

𝑣𝑖={1,𝑤𝑖𝑣𝑖𝑠≥𝑤𝑖𝑖𝑛𝑣𝑖𝑠;0,𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒.
(13)
In this way, occluded samples are considered for network optimization step-by-step, avoiding too difficult optimization at the very beginning. To observe this progress, we also plot a curve to show the difficulty of training samples w.r.t. highest occlusion ratio along the number of training iterations. As shown in Fig. 9, the highest occlusion ratio increases as the number of training iterations grows, i.e.more difficult occluded samples are included progressively.

Fig. 9
figure 9
In our proposed self-paced training, the highest occlusion ratio for considered training samples increases as training progresses, i.e.more difficult occluded samples are included progressively. This curve is for the FasterRCNN+ATT-vbb+SP detector on the CityPersons dataset; similar trends are observed for other models

Full size image
Experiments for Pedestrian Detection
In this section, we will first introduce the evaluation metrics we use, followed by a brief description to the datasets used for experiments, and some implementation details. After that, we will show experimental results for the different attention networks, and make a comparison to the state of the art. In the end, we will visualize how attention works in our detectors.

Evaluation Metrics
We use the standard average-log miss rate (MR) in all of our experiments, which is computed in the FPPI range of [10−2,100] (Dollár et al. 2012). Since we care more about occluded pedestrians in this paper, we will show our results across different occlusion levels:

(1)
Reasonable (R): visibility ∈[0.65,𝑖𝑛𝑓];

(2)
Heavy occlusion (HO): visibility ∈[0.20,0.65];

(3)
R+HO: visibility ∈[0.20,𝑖𝑛𝑓].

The performance on the R+HO subset is used to measure the overall performance as it includes a wide range of occlusions. Note that we only consider pedestrians with height ∈[50,𝑖𝑛𝑓] for all experiments.

Datasets
CityPersons. We use the CityPersons dataset (Zhang et al. 2017) for most of our experiments. The CityPersons dataset was built upon the Cityscapes dataset (Cordts et al. 2016), which was recorded in multiple cities and countries across Europe and thus shows high diversity. Importantly, it includes a large number of occlusion cases. We use the original training and validation split, which are composed of 2,975 and 500 images respectively.

Caltech. The Caltech (Dollár et al. 2012) dataset is one of the most popular ones for pedestrian detection. It consists of approximately 10 hours of 640×480 30Hz video taken from a vehicle driving through Los Angeles. We use set00-set05 for training and sample with 10Hz to get a large amount of training data (42,782 images in total). The test set consists of 4,024 images sampled with 1Hz from set06-set10.

Table 1 Comparison of the number of pedestrian boxes in each evaluation subset on the CityPersons validation set, Caltech test set, CrowdHuman validation set and ETH dataset.
Full size table
Table 2 Results of detectors using different attention networks on the CityPersons validation set.
Full size table
CrowdHuman The CrowdHuman dataset (Shao et al. 2018) is collected by crawling images with the Google image search engine with approximately 150 query keywords. The whole dataset consists of around 25,000 images, from more than 40 different cities around the world, and covers various activities and numerous viewpoints. As the name suggests, the CrowdHuman dataset contains a large number of persons at crowds.

ETH The ETH dataset (Ess et al. 2008) “Setup 1 (chariot Mk I)” consists of three sequences (1804 images in total) for testing. As the images were captured in the city center, it contains intensive crowds, thus a suitable test base for occluded pedestrian detection.

In Table 1, we show statistics on the evaluation subsets for different datasets. Although Caltech provides more images for testing, the number of occlusion cases is smaller than that on the CityPersons dataset.

Implementation Details
On the CityPersons dataset, we finetune from the ImageNet model with the Adam solver (Kingma and Ba 2015). We train with an initial learning rate of 10−3 for 20, 000 iterations and train for another 5, 000 iterations with a decreased learning rate of 10−4; for the FasterRCNN baseline, we use VGG-16 as the backbone and the original image scale; for the FasterRCNN*+ATT-part method, we use ResNet-50 as the backbone and upsample the input images by a factor of 1.3.

On Caltech, we finetune from the CityPersons model. We start with a small learning rate of 10−4, and then decrease the learning rate after 20, 000 iterations. The model converges at 30, 000 iterations; we upsample the images to 900×1200.

Our experiments are implemented with PyTorch on a Tesla V100 GPU. On the CityPersons dataset, the baseline detector FasterRCNN* takes 215ms per image for inference; and our method FasterRCNN*+ATT-part(+self-paced training) takes 298ms, including 83ms for part detection.

Comparison of Three Attention Nets
Table 3 Impact of parameter 𝛼 in Eq. 6 on performance.
Full size table
Table 4 Comparison to state-of-the-art occlusion handling methods on the CityPersons validation set.
Full size table
Fig. 10
figure 10
Visualization of detection results (at FPPI=0.1) from our FasterRCNN+ATT-part detector (upper row) and the baseline FasterRCNN detector (lower row). Our attention based detector achieves higher recall for occluded pedestrians. The sample images are selected from the CityPersons (Zhang et al. 2017) validation set; we show ground truth annotations in green and detection results in red. All figures are best viewed in color (Color figure online)

Full size image
We compare our detectors to the baseline FasterRCNN detector on the CityPersons validation set in Table 2, and we can make the following observations:

Attention helps overall
While looking at the overall performance measure of MR on R+HO, all three methods with attention mechanism show some improvement to the FasterRCNN baseline, ranging from 1pp to 4pp. We also compare to the FasterRCNN+part detector, which directly uses the part detection heatmaps as additional features for classification. The gap between FasterRCNN+ATT-part and FasterRCNN+part demonstrates that our attention net is a more effective way of exploiting occlusion patterns from part detections.

Attention helps more for heavy occlusion cases
The gap given by attention networks becomes larger for the heavy occlusion cases, which are more challenging to detect. Especially, we notice the FasterRCNN+ATT-part(+SP) detector achieves more than 8pp improvement.

External attention helps more than CNNFtr attention
With CNNFtr attention, FasterRCNN+ATT-self obtains a 1.35 pp gain on R+HO, which is smaller than the other two (2.27 pp and 3.01 pp) using external attention guidance. The gap on HO is even larger. FasterRCNN+ATT-self obtains a 3.53 pp gain, while FasterRCNN +ATT-vbb and FasterRCNN+ATT-part obtains 6.67 pp and 8.18 pp gains, respectively. Please note we empirically choose the value for 𝛼 in Eq. 6 based on the results in Table 3, where we find 𝛼=1 gives the best detection performance and an reasonable accuracy (92%) for occlusion pattern classification.

Self-paced training brings consistent improvement across different occlusion levels
We observe for FasterRCNN+ATT-X methods, although they obtain significant improvements on the HO and R+HO, they deteriorate the performance on R. However, our proposed attention guided self-paced training compensate this effect and we can see FasterRCNN+ATT-X+SP methods improve over the baseline across different occlusion levels.

We also show some detection results of FasterRCNN+ATT-part and baseline FasterRCNN in Fig. 10, where we can see our detector obtains a higher recall than the baseline FasterRCNN.

Comparison to State of the Art
Table 5 Comparison to state-of-the-art detectors on the Caltech test set.
Full size table
To further verify the effect of our proposed attention nets, we make comparisons to the state of the art on CityPersons, Caltech and ETH datasets.

In Table 4, we compare our methods with previous top performing occlusion handling methods on the CityPersons validation set. We find our FasterRCNN+ATT-part+SP method outperforms previous methods on the R+HO subset; by using ResNet-50 and a larger image scale, our FasterRCNN*+ATT-part+SP method outperforms previous methods on both HO and R+HO subsets, and achieves comparable performance to the state of the art on R subset.

Fig. 11
figure 11
Qualitative results from our detector and other competitive methods on the Caltech test set (at FPPI=0.1). The green solid and green dotted boxes indicate ground truth and ignored ground truth annotations; the red boxes denote detection results

Full size image
The results on the Caltech test set are shown in Table 5, where we make a comparison to state-of-the-art methods on both original and new annotations (Zhang et al. 2016b). First, we can see the rankings of different methods evaluated on two sets of annotations are roughly consistent. Among all methods, MS-CNN (Cai et al. 2016), RPN+BF (Zhang et al. 2016a) and FasterRCNN achieve top results on the reasonable subset, but fail miserably on heavy occlusion cases due to the lack of occlusion handling. Our FasterRCNN*+ATT-vbb+SP detector outperforms the previous state-of-the-art detector JL-TopS (Zhou and Yuan 2017) by about 5pp on the heavy occlusion subset, and establishes a new state of the art on the R+HO subset, which consists of a wide range of occlusion levels. We also show some qualitative results in Fig. 11, where we can see our detector produces robust detections for different occlusion patterns. For instance, in the first example containing crowds, people are occluded with each other, the other two detectors either miss some of them or produce many false positives, while our detector generates well-aligned detections for all of them.

Table 6 Results on the CrowdHuman validation set.
Full size table
We also implement experiments on the CrowdHuman dataset, which is of rather high density. From Table 6, we can see that our FasterRCNN*+ATT-vbb+SP detector achieves comparable results to state-of-the-art methods.

We apply our CityPersons models on the ETH dataset. Since no visible boxes are available, we can only evaluate on all occlusion levels. We show our results in Table 7, where we can see that our FasterRCNN+ATT-part model outperforms the FasterRCNN baseline by 1.80 pp, and our FasterRCNN*+ATT-part+SP detector further surpasses all previous methods.

Table 7 Comparison to state-of-the art detectors on the ETH dataset.
Full size table
The above results demonstrate that our attention models achieve state-of-the-art performance across different datasets, which are recorded in different cities, under different weather and illumination conditions, and also involve various occlusion patterns.

Discussion
Fig. 12
figure 12
Visualization of how 𝛺 behaves for occluded people. Each row shows the channel features on the left, and proposals (for six people detected) with the decreasing ranking of 𝛺 for this channel on the right. The two channels represent feet and upper body respectively. For those people whose feet are occluded, 𝛺 for the feet channel is lower ranked; while 𝛺 for the upper body channel is highly ranked

Full size image
In order to understand how attention handles occlusion in our models we analyze how 𝛺 varies for pedestrian proposals with different occlusion patterns and different channels.

Assume we have H channels at the top convolutional layer, then 𝛺 for proposal l is a vector of length H:

𝛺𝑙=[𝜔0𝑙,𝜔1𝑙,...,𝜔𝐻−1𝑙],
(14)
where 𝜔𝑡𝑙 will be applied on t th channel for re-weighting operation. In our detectors, 𝐻=512.

The elements in 𝛺𝑙 are then sorted in an increasing order, so as to get the ranking vector:

𝑅𝑙=[𝑟0𝑙,𝑟1𝑙,...,𝑟𝐻−1𝑙],
(15)
where 𝑟0𝑙 indicates the index of channel with the lowest impact in the final feature pool, and vice versa.

We denote the rank of channel t for proposal l as 𝐶𝑡𝑙, and it can be defined as:

𝐶𝑡𝑙=𝑚,if𝑟𝑚𝑙==𝑡.
(16)
For channel t, if 𝐶𝑡𝑙>𝐶𝑡𝑣, i.e. 𝜔𝑡𝑙 ranks higher than 𝜔𝑡𝑣, then this channel plays a more important role for proposal l than proposal v.

In Fig. 12, we show two channels, representing the feet and the upper body respectively. And for each channel, we show the proposals for six people detected in the image, with decreasing C value side by side. Among all channels, the given channel has a higher impact on the proposals on the left than on the right. We can see that for those people whose feet are occluded, the feet channel has a relatively lower impact than those fully visible people; on the other hand, the upper body is visible for all six proposals, but it ranks higher for occluded ones, this is because other channels for invisible parts are ranked lower. In this way, 𝛺 re-weights the channels and allows occluded people to generate a high confidence in the final feature pool by up-weighing visible channels.

In the following, we will analyze the distribution of 𝛺 for different proposals in a quantitative way. In principle, for those fully visible proposals, each channel plays an equal role for the final feature representation, resulting in a equally distributed histogram, e.g. higher entropy values; in contrast, for those occluded ones, different channels are of different importance, resulting in a more concentrated histogram, e.g. lower entropy values.

Again, 𝛺 for proposal l is denoted as 𝛺𝑙, whose histogram can be computed and denoted as follows:

𝑇𝑙=[ℎ0𝑙,ℎ1𝑙,...ℎ𝑖𝑙,...ℎ𝐵−1𝑙],
(17)
where B is the number of histogram bins. We then measure the concentration of 𝑇𝑙 by computing the entropy:

𝑆𝑙=−∑𝑖=0𝐵−1ℎ𝑖𝑙logℎ𝑖𝑙.
(18)
In our experiments, we choose 𝐵=10. The average entropy values for fully visible and heavily occluded proposals are 1.2 and 0.3. The results are consistent with our assumption.

Experiments for Pedestrian Re-identification
Since the part attention net achieves the best performance for pedestrian detection, in this section we implement the pedestrian re-identification experiments using the part attention net. In the following, we will provide a brief introduction to the datasets and evaluation metrics we use for experiments, followed by implementation details, comparisons to the baseline method as well as state-of-the-art methods.

Datasets
Occlusion Oriented Datasets
The Partial REID and Partial iLIDS datasets (Zheng et al. 2015b) are used as test bases to verify the occlusion handling ability of different re-ID methods. Partial REID includes 600 images from 60 identities. Among the 10 images of each identity, 5 of them are partial body images and are used as queries; another 5 full-body images are used as gallery. Partial iLIDS is distilled from the iLIDS dataset (Zheng et al. 2009) by selecting occluded images of each identity and manually cropping the non-occluded images to generate partial observations. It consists of 476 images of 119 individuals, among which half of them are served as queries and the other half captured by different cameras are served as gallery images. Since the two datasets are small, they are only used as test sets.

Conventional Datasets
The three most commonly used datasets are Market1501 (Zheng et al. 2015a), CUHK03 (Li et al. 2014) and DukeMTMC-reID (Zheng et al. 2017b). Market1501 is collected at a university campus with 6 position-fixed cameras. It contains 751 identities with 12,936 images for training and another 750 identities with 19,732 images for testing. The pedestrian bounding boxes are obtained by applying the Deformable Part Model (DPM) (Felzenszwalb et al. 2009) instead of manual labelling. Therefore, this dataset prohibits challenges such as viewpoint change, occlusion, misalignment, etc.from both the environment and noisy detections. CUHK03 contains 13,164 images of 1,467 identities captured by two cameras. It offers both manually-labelled and DPM-detected bounding boxes and the latter is used in this paper. We follow the train/test split proposed by Zhong et al. (2017a), which assigns 767 identities for training and 700 identities for testing. DukeMTMC-reID is a cropped subset of DukeMTMC (Ristani et al. 2016). It provides 702 identities with 16,522 images for training. For testing, there are in total 702 identities with 2,228 images as the query set; and the gallery set is composed of 17,661 images, including 702 query IDs and 408 IDs that do not appear in the query set. Different from Market1501 and CUHK03, all the bounding boxes are manually labelled and are thus of high quality.

Person Search Datasets
We further verify our method on person search datasets, i.e.CUHK-SYSU (Xiao et al. 2017) and PRW (Zheng et al. 2017a). By cropping out pedestrian patches based on ground truth bounding boxes, these datasets can be also used for the pedestrian re-ID task, providing more complex conditions like semi-supervised setting, domain gap, various viewpoints and a larger gallery size. CUHK-SYSU (Xiao et al. 2017) is a hybrid dataset consisting of movie snapshots and street/urban scene image shots captured by a hand-held camera. The training set includes 55, 272 bounding boxes, where 11, 206 of them are labelled with 5, 532 identities and the rest are with unknown IDs. The testing set contains 2, 900 probe persons and 40, 871 gallery boxes. For each probe person, a gallery size of 100 defined by the dataset protocol is used in our experiments. PRW (Zheng et al. 2017a) shares the same source as Market1501, but provides 43, 110 manually labelled bounding boxes, among which 34, 304 pedestrians are labelled with 932 identities. We follow the standard train/test split where 18, 270 boxes are used for training, and 24, 840 gallery proposals are used to find 2, 057 probe persons. Different from CUHK-SYSU, the whole gallery set serves as the search space for each probe person. Therefore, its gallery size is much larger than all the other datasets.

Evaluation Metrics
We adopt the mean Average Precision (mAP) and Cumulative Matching Characteristics (CMC top-K) as performance metrics for pedestrian re-identification. The mAP metric reflects the accuracy and matching rate of searching a probe person from gallery images. CMC top-K is widely used for person re-identification task, where a matching is counted if there is at least one of the top-K predicted bounding boxes shares the same identity label with the probe image.

Implementation Details
We choose the successful IDE model (Zheng et al. 2018) as our baseline. We propose to add the part based attention net on top of it, and our proposed method is named IDE+ATT-part in the following experiments.

Starting from an ImageNet pre-trained model, we fine-tune our model with Stochastic Gradient Descent (SGD). The batch size is set to 32. The learning rate is gradually warmed up from 0.005 to 0.1 in the first 10 epochs, and then decayed by a factor of 10 at epoch 30 and 55 respectively. The model is then trained for another 25 epochs until convergence. Label smoothing (Szegedy et al. 2016) is also used to prevent overfitting.

Table 8 Performance comparison on occlusion oriented pedestrian re-identification datasets
Full size table
Fig. 13
figure 13
Qualitative re-ID results of IDE baseline (upper row in each sub-figure) and our proposed IDE+ATT-part (bottom row) from the Partial REID dataset. The top-10 results are sorted according to their similarities to the probe image in a descending order. The correct matches are marked with red boxes

Full size image
The input images are resized to 384×128, followed by standard augmentation techniques, viz. horizontal flipping and random erasing (Zhong et al. 2017b).

All the training settings are kept identical across different datasets for both our baseline IDE and our proposed method IDE+ATT-part.

Experimental Results
In Table 8, we first report the evaluation results on the Partial REID and Partial iLIDS datasets. Previous state-of-the-art partial re-ID methods are used for comparison, including Ambiguity-sensitive Matching Classifier (AMC), Sliding Window Matching (SWM) and their combination SWM + AWC (Zheng et al. 2015b). An FCN based method, namely Deep Spatial Reconstruction (DSR) (He et al. 2018) is also considered for comparison. Our proposed IDE+ATT-part method achieves significant improvements compared to our IDE baseline, improving the top-1 accuracy by 2.0 pp and 6.7 pp respectively. A qualitative comparison between our IDE baseline and IDE+ATT-part is also shown in Fig. 13. We can see that our IDE+ATT-part method is able to rank the correct match higher than the IDE baseline under partial conditions. Furthermore, our method outperforms the state-of-the-art DSR method by 16.4 and 1.8 pp. w.r.t.top-1 accuracy on the two datasets respectively. The performance gain demonstrates that our proposed IDE+ATT-part method better handles occlusion on the person re-identification task, similar to the findings on the pedestrian detection task.

Table 9 Comparison to pose-based and attention-based methods on conventional pedestrian re-identification datasets
Full size table
We also report our pedestrian re-identification results on three conventional datasets in Table 9. Since our proposed method employs additional information of pose and attention mechanism, we provide a comparison to pose-based methods, i.e.PDC (Su et al. 2017), PABR (Suh et al. 2018), Pose-transfer (Liu et al. 2018a), PSE (Saquib Sarfraz et al. 2018), and attention-based methods, i.e.DuATM (Si et al. 2018), HA-CNN (Li et al. 2018), AACN (Xu et al. 2018). Our proposed method excels previous state-of-the-art methods, performing the best on Market1501 and CUHK03, while reaching comparable performance on DukeMTMC-reID. The results also indicate that using pose information for attention is effective for pedestrian re-identification.

Table 10 Results on person search datasets
Full size table
We further compare the performance of IDE+ATT-part and its IDE baseline on two person search datasets in Table 10. Since other works do not report their performance under the re-identification setting, they are not considered for comparison. Similar to the results on the occlusion oriented datasets, IDE+ATT-part improves over the IDE baseline on both datasets w.r.t.both metrics. We observe a boost of over 5 pp. w.r.t.mAP on both datasets. The top-1 accuracies increase by 2.3 pp and 10.3 pp on CUHK-SYSU and PRW respectively. These improvements demonstrates that the proposed IDE+ATT-part method is robust to dynamic scenarios, moving camera and larger gallery size.

Overall, our method IDE+ATT-part not only outperforms its baseline IDE, but also achieves state-of-the-art results for pedestrian re-ID on multiple benchmarks. In particular, the large improvements on occlusion oriented datasets demonstrate the effectiveness of our method under occlusion conditions.

Conclusion
In this paper, we propose to employ channel-wise attention to handle occlusion for pedestrian detection and re-identification. From the visualization, we find that many channel features are localizable and often correspond to different body parts. Motivated by these findings, we design an attention net to generate attention vectors for re-weighting the top convolutional channels. This attention net can be added as an additional component to any CNN based method. We explore different attention guidances, and find that all improve performance for occluded cases while the most effective one is the one based on part detections. We also propose an attention guided self-paced learning method to balance the optimization across different occlusion levels for pedestrian detection.

We report experimental results for pedestrian detection on the CityPersons, Caltech and ETH datasets, and show significant improvements over the baseline FasterRCNN detector. In particular, on CityPersons, we achieve a significant improvement of 8pp on the heavy occlusion subset and on Caltech, we outperform the previous state of the art by 5pp for heavily occluded people.

We also report experimental results for pedestrian re-identification on diverse datasets, where using part attention net greatly improves the baseline method and obtains on par results with state-of-the-art methods.

Encouraged by the success in pedestrian detection and re-identification tasks, we believe that the proposed attention nets can be also applied to the generic object detection and recognition tasks, where occlusion is also a major challenge.

