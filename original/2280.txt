The existing RGB-D scene recognition approaches typically employ two separate and modality-specific networks to learn effective RGB and Depth representations respectively. This independent training scheme fails to capture the correlation of two modalities, and thus may be suboptimal for RGB-D scene recognition. To address this issue, this paper proposes a general and flexible framework to enhance RGB-D representation learning with a customized cross-modal pyramid translation branch, coined as TRecgNet. This framework unifies the tasks of cross-modal translation and modality-specific recognition with a shared feature encoder, and aims at leveraging the correspondence between two modalities to regularize the representation learning of each modality. Specifically, we present a cross-modal pyramid translation strategy to perform multi-scale image generation with a carefully designed layer-wise perceptual supervision. To improve the complementarity of cross-modal translation to modality specific scene recognition, we devise a feature selection module to adaptively enhance the discriminative information during the translation procedure. In addition, we train multiple auxiliary classifiers to further regularize the behavior of generated data to be consistent with its paired data on label prediction. Meanwhile, our translation branch enables us to generate cross-modal data for training data augmentation and further improve single modality scene recognition. Extensive experiments on benchmarks of SUN RGB-D and NYU Depth V2 demonstrate the superiority of the proposed method to the state-of-the-art RGB-D scene recognition methods. We also generalize the TRecgNet to the single modality scene recognition benchmark of MIT Indoor, and automatically synthesize a depth view to boost the final recognition accuracy.

Access provided by University of Auckland Library

Introduction
In recent years, computer vision community has witnessed the success of Convolutional Neural Networks (CNN) (Krizhevsky et al. 2012) on various vision tasks (He et al. 2016, 2017; Ren et al. 2015; Wang et al. 2017a, 2018a, b). Meanwhile, the rapid development of cost-affordable depth sensors (e.g., Microsoft Kinect and Intel Realsense) have triggered more attractions to revisit computer vision problems using RGB-D data, such as object detection (Gupta et al. 2016; Xu et al. 2017b), image segmentation (McCormac et al. 2017), activity recognition (Zhang et al. 2016a; Wang et al. 2017b).

We focus on the RGB-D scene recognition task, the goal of which is to classify the scene images accurately with aligned color and depth information. As shown in Fig. 1, RGB scene images are good at presenting texture and color details, while depth ones are less insensitive to illumination changes and exhibit meaningful contour information, which could complement corresponding RGB scenes. Previous methods typically train two separate networks, i.e., RGB and depth streams, independently to learn modality-specific features for the RGB-D scene recognition. However, this would fail to capture the correlation of RGB-D data for learning discriminative modality-specific representation, which may result in a suboptimal RGB-D scene recognition solution. On the other hand, RGB-D scene recognition tends to suffer a lot from data scarcity problem due to the complex data collection procedure, e.g., even the largest RGB-D scene dataset (Song et al. 2015) is still order-of-magnitude smaller than many existing RGB datasets, which makes it difficult to learn effective modality-specific RGB-D representation.

Fig. 1
figure 1
It has been found that RGB data is good at presenting texture and color details while depth images are less insensitive to illumination changes and texture variation. We are interested in training a more discriminative modality-specific model, by exploring the correlation between the paired RGB-D data

Full size image
In this work, we propose Translate to Recognize Networks (TRecgNet), in which we introduce a cross-modal pyramid Translation framework, to learn effective RGB-D modality-specific scene Recognition representation. Our TRecgNet improves the existing methods from the following aspects: 1) We couple the modality-specific classification with a cross-modal translation task. These two branches are trained in a multitask learning manner with a shared feature representation, hoping to improve its generalization power on modality-specific recognition by explicitly inferring structure information towards its complementary modality; 2) We carefully devise a customized image generation loss to guide the cross-modal translation, which could not only improve the classification performance of TRecgNet but also contribute to generate translation images of high quality; 3) We effectively augment training data and help mitigate the aforementioned data scarcity problem to some extent.

Specifically, Fig. 2 shows the framework of the RGB TRecgNet, i.e., scene recognition on RGB scenes and translation from RGB to depth data. The core of our TRecgNet is the cross-modal pyramid translation branch with several unique designs. First, we perform pyramid image generation (i.e., images of different sizes are generated in the translation process) instead of only translating in a single scale, with the intuition that supervision on small images would benefit the training of larger ones to further improve the translation procedure. Then, instead of using the popular image generation losses, e.g., pixel-wise L1 or L2 loss (Laina et al. 2016) or coupled with the GAN techniques (Isola et al. 2017), we exploit a new layer-wise perceptual loss to measure the image similarity in the image generation process. To further improve the complementarity of cross-modal translation to modality-specific recognition, we devise a feature selection module to adaptively enhance the discriminative information during the translation procedure. In addition, we propose the auxiliary classifiers to guide the training of the translation branch, which explicitly incorporates the label information into the translation procedure, thus bridging the optimization gap between the translation and the recognition branches.

We test the TRecgNet on two benchmarks of RGB-D Indoor scene recognition task, SUN RGB-D dataset and NYU Depth dataset V2. Our TRecgNet could achieve state-of-the-art performance on both datasets. We also adapt TRecgNet to the scene recognition task where only RGB data is available. We test this condition on MIT67 dataset by simultaneously training a cross-modal scene recognition network using generated depth data, in which we achieve better performance than the original single modality network does. To sum up, the main contributions of this paper are as follows:

We propose TRecgNet, a general and flexible multi-modal learning framework, to unify the tasks of modality specific recognition and cross-modal translation with a shared representation, thus improving its generalization power.

We introduce a new layer-wise perceptual loss for the cross-modal translation, which has better performance than traditional image generation losses.

We devise a cross-modal activation module to selectively enhance discriminative features to improve the complementarity of cross-modal translation to modality-specific scene recognition.

We optimize the cross-modal translation with label information, which in turn stabilizes the training of the target classification task and further boost its performance.

Our method could be adapted to the scene recognition scenario where only single modality data is available, which further improves its performance using transferred cross-modal information.

A preliminary version of this work was presented in the conference paper (Du et al. 2019). We make important extensions from different aspects as follows:

A. Technical extension

We extend the modality translation to a multi-scale manner and adapt the generation loss and behavior constraints to this pyramid translation scheme.

We propose two improvements over the Similarity Net design, to better explore the complementary and discriminative information of RGB-D data during the translation procedure, thus achieving better modality-specific scene recognition performance.

The cross-modal translation is optimized with a new auxiliary classification loss on the perceptual features, in which the label information is incorporated in to further bridge the gap between cross-modal translation and modality-specific recognition.

B. Experimental extension

We conduct comprehensive ablation studies for the pyramid scheme, auxiliary classification loss and different variants of the Similarity Net, to show the effectiveness of each new proposed module.

We conduct sufficient experiments on image quality measurement, to evaluate the quality of the generated images and show comparison with other image generation losses.

We implement the proposed method with several backbone networks to demonstrate its generalization ability.

Visualizations for the learned ConvNets are provided to present the characteristics of the proposed method.

C. Task extension

We adapt our method to the scene recognition scenario where only RGB data is available.

Related Work
RGB-D Scene Recognition
Earlier works relied on handcrafted features to capture the characteristic properties of RGB-D scenes. Banica and Sminchisescu (2015) used second-order pooling to quantize local features for segmentation and scene classification. Gupta et al. (2015) proposed to quantize segmentation outputs by detecting contours from depth images as local features for scene classification. Recently, multi-layered networks such as CNNs are used to learn discriminative representations from large amounts of data. However, as the scale of RGB-D dataset is still not comparable to RGB datasets, works have focused on learning effective depth features for the data scarcity problem of depth scenes. A few methods (Wang et al. 2016; Zhu et al. 2016) directly used the pre-trained RGB CNN weights to fine tune a depth CNN, however, they give only limited improvements because of the considerable bias between RGB and depth data. In Song et al. (2017), argued that learning depth features from pre-trained RGB CNN models are biased and seek to learn depth features from scratch using weakly supervised depth patches. More recently, researchers have focused on exploring RGB-D correlation. Xiong et al. (2020) extracted the modal-consistent and modal-specific features simultaneously. Gupta et al. (2016) transferred semantic supervision from labeled RGB images to unlabeled depth images, which has limits on transform direction and has foreseeable upper bound performance in this distillation manner. In Song et al. (2019) and Yuan et al. (2019)), object relations are modeled to obtain a more comprehensive understanding of the scene. In contrast, our work focuses on enhancing modality specific RGB-D classification task leveraging complementary modality data, and the correlation modeled in the translation could be generalized to scenarios where only single modality data is available.

Paired Image-to-Image Translation
Paired image-to-image translation problems are usually formulated as pixel level mapping functions (Zhang et al. 2016b). In recent years, GANs (Goodfellow et al. 2014; Radford et al. 2015) have achieved impressive results in image generation. Wang and Gupta (2016) factored the generation into two steps: surface normal generation and content generation. They used pixel-wise surface normal constraints as additional supervision. In Isola et al. (2017), authors proposed to use GANs to learn the mapping functions between paired images. These GAN constraints help generate more variegated images from data distribution learning, however, they still relied on pixel-level distance similarity measurement to keep the content correspondence. Generated images using GAN loss lack the semantic consistency with source images, which makes them suboptimal for the extra training data. In addition, in many style transfer works (Gatys et al. 2016; Johnson et al. 2016; Chen et al. 2018), authors use the perceptual loss from a specific layer of pre-trained RGB models, to maintain the structural content during the translation. In this paper, we leverage perceptual loss from multiple layers to constrain the modality translation process of RGB-D data, which extends the perceptual features to the image generation area. Unlike traditional image generation task, we focus more on how this translation process benefits the classification task and care about the effectiveness of generated images for data augmentation.

Cross-Modal Representation Learning
The cross-modal transfer learning or domain adaptation aims at overcoming the cross-domain discrepancies and mainly focuses on aligning the domains by minimizing the domain divergence with different similarity measures to transfer helpful cross-modal features. There has been an enormous body of work on transferring knowledge between different modalities (Christoudias et al. 2010; Ngiam et al. 2011; Srivastava and Salakhutdinov 2012; Socher et al. 2013; Liu et al. 2016; Li and Wang 2020). They focused on learning representations from multiple modalities into a shared feature space jointly. Typically for image based cross-modal representation learning, Gupta et al. (2016) proposed to use ‘paired’ images from the two modalities and utilized the mid-level representations from the labeled modality to supervise learning representations on the paired unlabeled modality. In Aytar et al. (2017), learned a shared representation that is agnostic of the modality to connect objects cross modalities. Xu et al. (2017a) proposed to learn a non-linear mapping to model the relations between RGB and thermal data, and transfered the learned representation for detection results. Ren and Jae Lee (2018) used synthetic data to help learn generalizable high-level visual representation from multiple translation tasks, and leverage GAN models to overcome domain adaption problem. However, it also adopts pixel-level image generation supervision. Different from those works, our work transfers cross-modal cues by training a modality translation model. This enables us to learn representative modality-specific embeddings from multiple modalities. We propose a new modified perceptual loss for modality translation task, which achieves better performance. We further make behavior constraint on the generated data to in turn improve the generation process, which has not been explored in the related previous works.

Multitask Learning
Multitask has been studied for numerous vision problems such as action recognition (Kapidis et al. 2019; Luvizon et al. 2018), semantic segmentation (Misra et al. 2016; Kendall et al. 2018; Takikawa et al. 2019), face detection (Zhang et al. 2014), joint object and action detection (Kalogeiton et al. 2017), surface normal, boundary and saliency estimation together with object segmentation and detection (Kokkinos 2017), etc. It allows training a model for multiple objectives in parallel and aims at improving the performance by capturing a shared representation that could complement at least one of those tasks. We employ this idea by simultaneously training a modality translation network for the classification network. We use paired data from two modalities and we focus on learning the intrinsic similarity characteristics. The modality translation task could regularize the training of modality-specific representation learning that it forces the recognition network to focus on cues explicitly from complementary data. Different from other multitask based methods, our framework has extra useful by-product. Apart from the shared representation learning, the translation network enables to produce images of high quality to effectively augment the training data, which further improve the target task.

Fig. 2
figure 2
The framework of TRecgNet. The pipeline consists of two parallel streams: (1) Classification Branch is for recognizing scene images, in which Nets 𝐸→𝐶 are updated using the supervised classification loss. (2) Modality Translation Branch aims at constructing the paired data of the input which is trained by a pyramid modality translation supervision from the S Net and the A Net. The dash bordered part on the top details the settings of three variants for the S Net, and the weights of S net are frozen in the training. CNN module details are omitted for clarity and more information could be found in Sect. 3.1. TRecgNet is trained simultaneously with three tasks and we only use the classification branch in the test phase

Full size image
Method
This section details our proposed TRecgNet. We first give an overview on our method. Next we present the modality translation scheme for improving the modality-specific representation. Then we elaborate the layer-wise perceptual supervision with three variants introduced for the similarity net, and how we involve the label information in the modality translation. At last we show the training strategies for both RGB-D modality-specific and fusion scheme for the final prediction.

Overview of Translation-to-Recognize Networks
Our TRecgNet contains two branches, as showed in Fig. 2. The first one is the scene classification branch for the arbitrary modality (source modality). It contains a regular classification CNN model that is composed of the encoder network (E Net) for feature extraction and a classifier (C Net). The other is the modality translation branch, which shares the same encoder network with the classification branch. In the translation branch, we use a pre-trained RGB model (Similarity Net (S Net)) to supervise the modality translation task. Here are some considerations for the architecture design.

Encoder Net (E). The encoder network could be any popular CNN backbones that suit the recognition task.

Decoder Net (D). We add a decoder on top of the encoder that gradually recovers the spatial information of complementary data. We use skip layers to integrate low-level context information from the encoder features which is similar to UNet (Ronneberger et al. 2015). We use multiscale modality translation, i.e., we generate a 3-channel image after each bilinear interpolation based upsample stage, e.g., given an image of size 224×224, we output the generated images of size 56×56, 112×112 and 224×224. The output number is a hyper-parameter in our model. To enhance the effect of the translation, we introduce an effective residual upsample layer in the D network. Typically, a residual upsample layer upsamples the feature maps with a bilinear interpolation operation, which is accompanied with one residual block that mimics the bottleneck residual block of the ResNet (He et al. 2016), as shown in Fig. 3.

Similarity Net (S). S net is a pre-trained CNN network that helps extract perceptual features for the modality translation task. To be fair, S uses the same backbone and pre-trained weights with the E net, which means we do not introduce additional information in terms of data or network structure in the translation. There are three variants for the similarity net which we will detail in Sect. 3.2.2.

Aux Classification Net (A). We use multiple auxiliary classifiers taking as input a series of perceptual features of the generated data, which are extracted from corresponding layers in the multiscale modality translation. These classifiers are all composed of 1-fc layers.

Classification Net (C). The classification net is composed of linear operations integrating the CNN features to produce a class label. As TRecgNet could use myriad CNN backbones as the encoder network, we simply leverage their corresponding classifiers as the C network, e.g., for ResNet based TRecgNet we employ the original 1-fc layer which performs on the results of global average pooling (GAP) features (Lin et al. 2013).

Fig. 3
figure 3
The architecture of the proposed residual upsample layer. We upsample the feature maps to its double size followed by learnable convolutional blocks and perform a residual add operation similar to the ResNet

Full size image
Cross-Modal Pyramid Translation
In this section, we give a detailed description of the proposed TRecgNet. Before Du et al. (2019), few people worked on enhancing modality-specific RGB-D representation by exploring the correlation between modalities, except that authors in Gupta et al. (2016) transferred RGB semantics to unlabeled depth or optical flow data to tackle the data scarcity problem. Their assumption is that for the paired data of two modalities, features extracted from the same level of each network should act similarly. However, it has a limit on the supervision direction, i.e., the RGB network fails to benefit from other modalities. Instead, we are interested in modeling a more flexible modality-specific scene recognition model, in which we leverage complementary cues through a modality translation by building the following mapping function F:

𝐹=𝐌𝐀→ℝ𝑑→𝐌𝐁,
(1)
where (𝐌𝐀,𝐌𝐁) denotes the paired RGB-D images. This translation process aims at regularizing the training of the modality-specific classification network, to learn better modality-specific representation by exploring the correlation between modalities.

Layer-Wise Perceptual Translation
The image similarity measurement plays an important role for TRecgNet; an improper generation function could bring about biased or insufficient exploration of cross-modal cues, which may result in suboptimal recognition performance. Another consideration in the translation is that generated images should be of high quality to help mitigate the data scarcity problem. To achieve these ends, we do not use the extensively employed pixel-level euclidean constraints (Isola et al. 2017; Laina et al. 2016) in the modality translation. We argue that this would fail to capture the helpful structure information of the targeted images, and such losses are prone to generate blurred images which would give limited assistance to the training data enhancement. Instead, we supervise the modality translation in a more comprehensive way. It has been shown that CNNs trained with sufficient labeled data on specific vision tasks, such as object classification, has already learned to extract semantics and this generalization ability is not limited to specific datasets or tasks. This insight is commonly leveraged in style transfer related works (Gatys et al. 2016; Johnson et al. 2016; Chen et al. 2018), in which authors used the perceptual loss from one specific layer, usually conv4_x of VGG (Simonyan and Zisserman 2014) model, to keep the rough content of the image. In this work, we go deeper into the perceptual loss and extend it to supervise the modality translation, coined as layer-wise perceptual loss.

Specifically, we measure the similarity of perceptual features from from lower layers to higher ones, for a simple yet effective intuition that higher layers tend to preserve the semantic content while lower ones do better in capturing the detailed information. Compared with pixel-level constraint, this could provide richer and comprehensive cues to measure the similarity between the generated and the complementary data. It’s worth noting that the perceptual loss has never been explored in previous image generation tasks in this way.

Suppose we are training the classification task for 𝐌𝐀. Generated images 𝐌𝐁′ and ground-truth 𝐌𝐁 are fed into S net, and we can get their corresponding layer-wise perceptual presentations, respectively, and we constrain them be close using L1 distance which can be formulated as:

𝑐𝑜𝑛𝑡𝑒𝑛𝑡=∑𝑑∈𝐷∑𝑙∈𝐿𝑑‖𝑆𝑙(𝑀𝐵𝑑)−𝑆𝑙(𝑀𝐵′𝑑)‖1
(2)
where l means the specific layer we extract the perceptual features from. Typically we use a multiscale image generation schedule, in which we downsample the ground-truth 𝑀𝐵 to different sizes in D according with the scale of intermediate feature maps in the decoding phase. For example, as mentioned in Sect.3.1 we generate images of 224×224, 112×112, 56×56 given an input image of 224×224 when downsample number is set as 2. Accordingly, we respectively make 𝐿224∈[0,1,2,3,4], 𝐿112∈[0,1,2,3] and 𝐿56∈[0,1,2] so that perceptual feature maps in the last layer are all 7×7 in size. We use the L1 loss for the layer-wise similarity of each generation scale.

Similarity Net Design
The settings of three similarity net variants are shown on the top of Fig. 2. The vanilla similarity net (variant-A) is an encoder network from a CNN model that is pre-trained on a large-scale RGB classification dataset, e.g., ImageNet or Places (Zhou et al. 2014). Relying on the well learned universal RGB representation, TRecgNet is allowed to capture comprehensive information for the modality translation. However, since RGB and depth data could show very different appearance (ref. Fig. 1), when we translate RGB images to the depth ones, relying on networks trained for RGB data to extract depth perceptual features seems to be a suboptimal solution. In addition, as there usually exists much noise in the depth data, due to the inherent capture process, the translation might get badly tuned to imitate such nuisance.

To this end, we propose another variant for the similarity net (variant-B) by adding a classifier to the vanilla one. Instead of directly leveraging the pre-trained RGB weights, we fine tune the network with complementary data to make the network skilled in this translated modality. Since this variant is pre-trained modality specifically, it could alleviate the interference of the noise information which has no help for the classification task, thus behaves more robustly in this condition.

In variant-C, we come up with a feature selection module to guide the perceptual features, and enforce the S network to focus on the complementary parts in the modality translation procedure. We additionally import a complementary classification network (CPL Net) which is trained using the images of source modality (𝐌𝐀). Here the insight is that we treat the complementary characteristics of RGB-D as the modality-specific information, which could not be well captured in the unmatched modality-specific model. In other words, if we feed arbitrate modality data to a classification network, which is not tuned for this modality, the activation guided from the cross-modal gradients could indicate the complements and the commonness of two modalities for the feature selection.

Specifically, to locate the complementary features, we perform a cross-modal forward by inputing 𝐌𝐁 to the CPL net (trained with 𝐌𝐀) and calculate the gradients of features from the layer_0 (our layer-wise perceptual features start from the layer_0). We average the cross-modal gradients to the weight vector W, by integrating each gradient map to a scalar, the length of which equals the channel numbers of the features. We generate the complementary mask by applying the weight to the features, preserving the unactivated position (activated one means the commonness) after a ReLU operation and performing a binarization operation:

¬𝑅𝑒𝐿𝑈(𝐀𝑙0⊙𝑊)→𝑚𝑎𝑠𝑘,
(3)
where 𝐀𝑙0 denotes the activation of layer_0 (𝑙0) and ⊙ refers to the dot production. The original features of generated data and the ground-truth data are both filtered by this mask to continue the subsequent perceptual feature extraction.

Note that whatever the variant we use, the parameters of the S net are not updated in the training. The layer-wise perceptual loss allows to measure multidimensional similarity in the modality translation task by calculating feature map distance at different levels. We find this helpful to transfer rich complementary cues for the classification branch, and it also enables us to get more genuine data of high quality and semantics for the training data sampling enhancement.

Translation with Label Consistency
Apart from the perceptual supervision which directly leverages the complementary images, we further constrain the translation using the label information. This accounts for the behavior of generation, i.e., label consistency of RGB-D pairs. Note that our goal is not to train a new classification network that is skilled in discriminating the generated data; our goal is to optimize the image generation with high-level semantics that could in turn benefit the modality-specific recognition task. To be specific, we newly build several auxiliary classifiers (A networks, for multiscale use) and supervise generated data as follows:

𝑎𝑢𝑥_𝑐𝑙𝑠=∑𝑑∈𝐷𝑐𝑟𝑜𝑠𝑠𝑒𝑛𝑡𝑟𝑜𝑝𝑦(𝑆𝑙𝑙𝑎𝑠𝑡(𝑀𝐵′𝑑)),
(4)
where auxiliary classifiers directly take the last perceptual features of generated data [obtained in Eq. (2)] as inputs. In this way, perceptual features are further leveraged to enhance the modality-specific recognition task. Since the S net uses the frozen weights, the classification on perceptual features establishes an optimization connection between the encoder network and the generated images on labels, in which we could effectively regularize the training of the modality-specific scene recognition.

Training Strategy
In this part we introduce our optimization procedure in detail. To jointly learn the embeddings for both classification task and modality translation task, we optimize two branches in a multi-task manner. Specifically, we simultaneously update the parameters of (1) 𝐸+𝐷 to minimize the distance of layer-wise features for similarity measurement, (2) 𝐸+𝐷+𝐴 and (3) 𝐸+𝐶 for the scene classification tasks of generated data and source modality data, respectively. The total loss is updated with a linear combination as follows:

𝑡𝑜𝑡𝑎𝑙=𝛼𝑐𝑜𝑛𝑡𝑒𝑛𝑡+𝛽𝑎𝑢𝑥_𝑐𝑙𝑠+𝛾𝑐𝑙𝑠,
(5)
where 𝑐𝑜𝑛𝑡𝑒𝑛𝑡 is calculated by Eq. (2). 𝑎𝑢𝑥𝑐𝑙𝑠 and 𝑐𝑙𝑠 use the cross-entropy loss for corresponding classification task. The coefficients 𝛼,𝛽,𝛾 are set as 10, 1, 1 for the best trial. We only use the classification branch in the inference phase for the modality-specific recognition, as illustrated in Fig. 2.

Fusion
After training two discriminative modality-specific recognition networks, we fuse them for the final RGB-D scene recognition prediction. The optimization goal is composed of two parts: (1) two modality translation tasks, and (2) the final fusion prediction. We find that keeping the translation branch in fusion stage could help maintain the learned cross-modal cues that may not be learned in the complementary classification branch. On the other hand, it is convenient for us to leverage the generated data for the data sampling enhancement in the training. The concatenation embedding is operated on global average pooling (GAP) to reduce the number of parameters followed by three fully connected layers. We illustrate the fusion architecture in Fig. 4. As before, we only use the classification branch in the test phase.

Fig. 4
figure 4
Fusion Network. Two pre-trained TRecgNets are jointly trained for the final recognition result with two modality translation tasks simultaneously optimized. Two translation branches randomly produce generated data for the training data sampling enhancement. We omit the multiscale schedule in this figure for clarity

Full size image
Experiments
In this section, we first introduce the datasets we use in experiments and the implementation details of our proposed approach. Then we discuss the ablation study of TRecgNet including the effectiveness of proposed modality-specific representation enhancement scheme, the layer contribution of S net, cross-modal weights transfer for the single-modal scenario, etc. We also quantitatively and qualitatively compare the quality of generated images with other approaches. Finally, we evaluate the performance of our approach with state-of-the-art RGB-D scene recognition methods. We report the average accuracy over all the scene categories following the conventional evaluation scheme. To achieve some reliable results, we play three runs for all the ablation and validation experiments, to give mean performance for each report number, in which the last five epochs’ results are averaged for the final evaluation. To compare with the state-of-the-art methods, we report the best performance of the model.

Datasets
SUN RGB-D Dataset is currently the largest RGB-D scene recognition dataset. It contains RGB-D images from NYU depth v2, Berkeley B3DO (Janoch et al. 2013), and SUN3D (Xiao et al. 2013) and is compromised of 3784 Microsoft Kinect v2 images, 3389 Asus Xtion images, 2003 Microsoft Kinect v1 images and 1159 Intel RealSense images. Following the standard experimental settings stated in Song et al. (2015), we only work with 19 major scene categories, each of which contains more than 80 images. As per standard splits, there are in total 4845 images for training and 4659 for testing.

NYU Depth Dataset V2 (NYUD2) is a relatively small dataset. Following the standard split in (Silberman et al. 2012), all the 27 well presented categories are grouped into ten including nine most common categories and an other category representing the rest. 795/654 images are used for training/testing following the standard split, respectively.

MIT67 (Quattoni and Torralba 2009) is an earlier RGB indoor scene recognition dataset. It contains 67 Indoor categories, and a total of 15,620 images, provided with standard train/test splits.

Implementation Details
The proposed approach is implemented in popular deep learning framework, PyTorch (Paszke et al. 2019), on an NVIDIA TITAN Xp GPU. We train the model using Adam stochastic optimization (Kingma and Ba 2014), with the batch size set to 40. The 224×224 RGB-D images are randomly cropped from an image which is resized to 256×256, horizontally flipped with a probability of 0.5. We train the TRecgNet in 80 epochs (10,000 iterations), and the learning rate is initialized as 2×10−4 at the first 20 epochs and linearly decades in the rest of 60. In the test phase, we use a center crop operation on test images. We employ geocentric HHA (Horizontal disparity, Height above ground and Angle with gravity) (Gupta et al. 2014) to encode depth images, which has been shown better performance to capture structural and geometrical properties of depth scenes for kinds of vision tasks.

Study on the Effectiveness of TRecgNet
Baseline. We use a ResNet18 network as the baseline, and all the compared methods use the same backbone if not specifically stated.

Initialization. We use two settings of weights initialization for the backbone network as follows:

1. no pre-train (random): randomly initialize the weights of backbone network;

2. using RGB weights (Places/pre-trained): copy weights from a RGB network trained on the Places dataset. Models pre-trained on this dataset usually give better results for scene related tasks than that from the ImageNet.

In this section, we make a comprehensive investigation of the proposed TRecgNet by exploring the following questions with corresponding experiments. We use variant-A for the S network in the evaluation, if not specially stated for convenience.

(A) Does the cross-modal translation scheme work for modality-specific scene recognition? The first question we investigate is if we are able to improve the modality-specific recognition task through the modality translation scheme. We look into the effect of enabling and disabling different core components of our TRecgNet. From Table 1, we can see that when only using layer-wise perceptual loss for modality translation (+Trans in the Table 1), TRecgNet boosts the performance from 48.2 to 51.8%, and 44.1–47.5% respectively, for RGB and depth data. This indicates that by simultaneously training a modality translation task, the training of classification gets regularized and benefits from the transferred cross-modal cues.

Table 1 Ablation study of the proposed TRecgNet
Full size table
Fig. 5
figure 5
Class accuracy comparison with baseline. Promotions could be found for most classes when using our method, for both RGB and depth scene recognition networks. The backbone is pre-trained on the Places dataset

Full size image
Table 2 Ablation study on effectiveness of multi-scale modality translation and the auxiliary classifiers
Full size table
On the other hand, when constraining the perceptual features with label information (+Trans+Aux in the Table 1), we observe further promotions for both modalities by a big margin, i.e., 1.6% and 1.3% for two modalities. We also train a new classification network (using pre-trained weights as the S net) that takes the generated images as input (+Trans+Aux new in the Table 1) and the weights are not fixed. We find that this new trained auxiliary classification network fails to benefit the main classification task, and the performance even drops when compared with the +Trans condition. This shows the effectiveness of our auxiliary classifiers on perceptual features that takes advantages of label information in a right way. We also visualize the comparisons on each class between the baseline and our full TRecgNet in Fig. 5. Promotions could be found for most classes when using our method, especially for classes whose results are relatively worse, e.g.,  discussion_area, home_office, lecture_lab, etc.

Furthermore, we illustrate that multiscale translation is able to enhance the performance of TRecgNet. As we also adapt auxiliary classifiers for this pyramid translation, we jointly evaluate them in terms of image sizes and auxiliary classifier switch in Table 2. From the numbers we find better results when applying multiscale translation strategy, and this also works well with multiple auxiliary classifiers. For example, when involving label information in the translation, pyramid translation boost the single scale condition from 50.7 to 53.2% for RGB and 46.9–48.5% for depth, respectively. This proves the effectiveness of the proposed pyramid modality translation.

Fig. 6
figure 6
We show effectiveness of layer-wise perceptual loss and layer_x means using separate perceptual constraint. Tested on SUN RGB-D dataset. All the translation based models outperform the baseline and the layer-wise perceptual loss achieves the best numbers

Full size image
Observations could also be found in single scale settings (and w/o aux) that when using smaller target images as generation supervision, TRecgNet-RGB makes a further boosting of nearly 1% but Depth-TRecgNet’s performance drops. We argue that this is due to the characteristics of RGB-D modalities as explained before. Content supervision on larger images provides more complementary details but also results in more interference from noise, which may give some promising insights for the size of generated data.

Fig. 7
figure 7
Examples of modality translation supervision from different layers. Similarity constraints from lower layers tend to keep textures and higher layers focus more on abstract content of the images. The combination of layer-wise content supervision (column (f)) gives the best photo-realist translation comprehensively. The input images are all from the test set of SUN RGB-D dataset

Full size image
(B) Is layer-wise perceptual constraint really better than the original one from specific layer? The effectiveness of transferring complementary cues relies much on the similarity model S. Therefore, we take an interest in how the S net affects the translation and the recognition tasks when using different layer as supervision. We separately use layer_0 ∼ layer_4 as well as their combination as the translation constraint. All the experiments are conducted on the SUN RGB-D dataset and use the single-scale translation setting. We test their scene recognition performance compared with directly fine tuning pre-trained ResNet18 model, and the mean accuracy is plotted against the number of training iterations in Fig. 6.

The following observations could be made: (1) the proposed TRecgNet achieves remarkable improvement using the perceptual supervision from the combined layers. (2) With the layer-wise perceptual constraint, the training procedure becomes more steady and we achieve the best performance. We also give an example of generated images guided from conditions above, as shown in Fig. 7. We can find that the similarity constraints from lower layers tend to keep textures and the very lower layer_0 even results in some blurring effects like the pixel-level euclidean distance loss does. Constrains from higher layers focus more on the abstract content where main contours are well preserved ignoring the luminance or the textures. As a compromise, layer-wise perceptual supervision combines characteristics from multiple layers, which could not only get better performance on the scene classification task but generate more photo-realistic images.

(C) What is the performance of other image generation losses? In this study, we investigate the conditions when using other popular image generation losses for the modality translation, including (1) berHu loss (Laina et al. 2016), and (2) pixel to pixel GAN loss (Isola et al. 2017) for the cross-modal distribution learning. We do not use the auxiliary classification loss for clearer comparison.

Table 3 Comparison of different image generation losses on the effectiveness for the classification task
Full size table
Fig. 8
figure 8
Generated examples by different methods, including berHu loss, pixel-to-pixel GAN, and our layer-wise perceptual loss. We can find that our layer-wise perceptual loss is good at capturing structural information as well as low-level details

Full size image
We first evaluate these losses for TRecgNet on the modality-specific recognition task, and Table 3 shows the comparison result. We can observe that all the generation losses help achieve better results than the baseline method, which further validates the effectiveness of the proposed Translate-to-Recognize scheme. Among all the compared image generation losses as the modality translation supervision, the proposed layer-wise perceptual loss helps the classification branch achieve the best performance for both RGB and depth data. We owe the superiority of layer-wise perceptual loss to its representative ability of images, as it could capture more comprehensive features in the modality translation process that complements most for the classification branch.

Table 4 Image quality evaluation using PSNR (the larger the better), SSIM (the larger the better), FID (Heusel et al. 2017) (the smaller the better), GAN-train & GAN-test (Shmelkov et al. 2018) (the larger the better) and data augmentation in practice (Data Aug, the larger the better)
Full size table
Next, we evaluate the quality of generated data using different losses mentioned above. First, we qualitatively compare some generated examples in Fig. 8. Images generated by pixel intensity supervision tend to be blurred stiffly imitating the training data while our layer-wise content supervision produces more natural images, especially for generating depth data, even for cases that the ground-truth is of significant errors. RGB images generated based on GAN shows an impressive effect on color diversity. On the other hand, layer-wise perceptual loss enables us to generate images of explicit structure, which comes from the comprehensive understanding of the images. More generated images using the proposed layer-wise perceptual loss are shown in Fig. 9.

We also quantitatively evaluate the quality of generated images using several popular measurements including: Peak Signal-to-Noise Ratio (PSNR), which is commonly used to measure the quality of reconstruction with absolute pixel errors; Structural Similarity Index Measure (SSIM), which considers inter-dependencies especially when they are spatially close; Fréchet Inception Distance (FID) is calculated by computing the Fréchet distance between two Gaussians fitted to feature representations of the Inception network (Heusel et al. 2017); GAN-test (train on real, test on synthetic) and GAN-train (train on synthetic, test on real) measure the quality of generated images through downstream classification tasks (Shmelkov et al. 2018), and Data Augmentation (Data Aug) evaluates the performance boost when leveraging the generated images as the training data augmentation in practice. We show the comparison in Table 4.

Observations could be found that for absolute error based evaluations like PSNR and SSIM, TRecgNet does not exhibit obvious advantages. We think this can be explained from the characteristics of RGB-D data, especially the depth data in which the ground-truth inherently exists very much noise. Pixel based generation losses manage to reproduce such incorrect information that turned out to obtain competitive scores. As for other quality indicators which are built on the downstream task, the proposed layer-wise perceptual loss outperforms other image generation losses with a big margin. We also test the image quality using layer-wise perceptual loss with auxiliary classification constraint. Better results indicate that involving the label information could not only enhance the scene recognition but also improve the quality of generated data. It’s worth noting that among all the indicators, data augmentation evaluation is the most helpful one in practice to alleviate the data scarcity problem. Pixel-based generation losses fail to provide satisfactory help, while our layer-wise perceptual loss shows impressive results in RGB-D conditions (+0.8% for RGB and +2.2% for depth).

From the results above, we can find that the proposed layer-wise perceptual loss does not aim at simply imitating the appearance of the ground-truth, instead, it takes both low-level details and semantics into consideration, thus the generated images give the most useful perturbation for the data augmentation. Also it can be concluded that though TRecgNet succeeds in transferring complementary information from the translation procedure, the recognition gain does not directly correlate to the quality of generated images, especially on some absolute error based indicators in the RGB-D settings.

Table 5 Scene recognition evaluation with different variants of the similarity network
Full size table
(D) What is the effect of each variant of the Similarity net? We evaluate different variants of the Similarity network on the modality-specific scene recognition. For variant-B, we use two pre-trained ResNet18 networks, which are separately trained with RGB and depth scenes. As variant-C introduces an extra modality-specific classification network to perform the feature selection, we involve the two networks mentioned above in one TRecgNet. To further validate the effectiveness of the feature selection module, we also conduct experiments with (1) the complementary mask of that from Eq. 3 which indicates the commonness of RGB-D data, and (2) a channel based dropout on the perceptual features with the drop rate set as 0.5, in the middle of perceptual features extraction. For better comparison we do not include the auxiliary classification loss here and Table 5 shows the results.

For RGB-TRecgNet (translate RGB to Depth), variant-B exceeds the variant-A with a margin of 0.5%. We argue that the improvement comes from the well learned depth-specific representation, which helps the Similarity network extract better depth perceptual features for the translation. Variant-C locates the complementary features in the translation which further boosts the performance of the classification branch (+0.6%). In contrast, when focusing on the commonness of RGB-D data in the translation, only small improvements could be observed (0.2%), and we owe it to the less helpful activation when lacking the complementary information in the image similarity measurement. In addition, a random dropout on perceptual features fails to improve the classification branch. Experiments on variant-C shows the importance of proper selection for the perceptual features in the modality translation, and verifies the effectiveness of the proposed complementary feature selection module. Similar results are obtained for the Depth-TRecgNet (translate Depth to RGB) except that we do not find obvious advantages in the variant-B, i.e., we pre-train the Similarity net with the RGB scene images for RGB image generation. We think this is because variant-A is pre-trained on the Places dataset, and has already learned good representation to perceive representative features of RGB scenes, thus further fine tuning on similar RGB scene data does not enable an obvious better result. Another consideration is the computational cost. Although the modality-specific pre-training could be performed in advance, it is inevitable for variant-C to take an extra full forward and backward consumption in the training, which is an efficiency/performance trade-off in practice.

Fig. 9
figure 9
Examples of generated data by our TRecgNet from test set of SUN RGB-D dataset. (a) are depth images translated from original RGB data,and (b) are generated RGB images using original depth ones

Full size image
To visualize how each variant affects the translation procedure, we show the heatmap of the perceptual features in Fig. 10, which are extracted of the ground truth images from layer_4 (most easily to observe). We can find different characteristics: variant-A (column (a)) is interested in universal RGB cues, while variant-B (column (b)) focuses more on scene-related parts. The cross-modal forward process (column (c)) shows very different activation from the previous two, leading to the complementary information discovery for variant-C (column (d)). Note that the heatmap is a normalized result which could not reflect a one-to-one correspondence. Though, some intuitive results are still observable. For example, for RGB images generation in the second row, the trees (of little relation with the indoor scene understanding) outside the window are activated by variant-A, and in contrast variant-B is less affected to such condition. Removal of common activation enables variant-C to capture more information from windows and the ceiling lamp (column (d)). For depth generation, a depth-specific pre-training enables the Similarity net to pay less attention to the noise information (remote wall in the third row, column (b)), and also to give extra attention for the scene-related desk (the last row, column (b)). Variant-C shows similar results to the RGB generation for the complementary information exploration. These visualizations show that the newly proposed two variants exhibit better adaptation in the translation procedure for the scene classification task.

Table 6 Evaluation using unlabeled RGB-D data for initialization
Full size table
(E) Does unlabeled RGB-D data help in the modality translation process? Using pre-trained weights is one of the key factors for CNN models to achieve kinds of state-of-the-art results. As mentioned in Sect.1, the sizes of existing labeled RGB-D datasets are in much smaller order-of-magnitude compared with RGB datasets. Although we can directly fine tune RGB models, it lacks feasibility for backbone network design and often suffers from modality biased training problem (Song et al. 2017). Fortunately, there is plenty of unlabeled RGB-D data we can capture from RGB-D video sequences.

In the part, we test the performance of TRecgNet that only uses unlabeled RGB-D data for pre-training instead using popular RGB weights. Specifically, we use 5k unlabeled RGB-D video sequences from the NYUD2 dataset to pre-train the TRecgNets and then fine tune them on SUN RGB-D dataset. Since there is no label information for these images, we do not employ the auxiliary classifier in the pre-training phase. We show the results in Table 6. We could observe the strong power of using pre-trained RGB weights compared with baseline which is trained from scratch on small datasets (several thousands). Surprisingly, the performance of depth TRecgNet turned out to outperform the baseline model by 2.3% which is pre-trained on the Places dataset. This reflects the problem that directly fine tuning RGB models with depth data may be improper given the weight bias. Our method, however, helps mitigate the scarcity problem (40.2% for depth data) of labeled RGB-D data by levering a label-free modality translation preprocessing with unlabeled RGB-D data. For RGB modality, we have a gap of 1.3% to the baseline with much less data (5k vs millions), which is still very promising, since we can flexibly design a model pre-training it with much less unlabeled RGB-D data instead of relying on large-scale datasets like ImageNet or Places.

Table 7 RGB scene recognition methods aided with depth data from TRecgNet, tested on MIT67 dataset
Full size table
(F) Could the learned cross-modal cues be transferred to new single-modal dataset? We owe the effectiveness of TRecgNet mainly to the transferred cross-modal cues from the modality translation. Here, we evaluate if we could transfer the trained TRecgNet to the new dataset where only single modality is available. We conduct the experiment on MIT67 dataset, i.e.,  we only have RGB data of the scenes. We apply our TRecgNet to this condition as follows. We first train or leverage an existing RGB scene classification model on MIT67. Then we construct a depth scene recognition network, the input of which is the generated depth data from a RGB-TRecgNet pre-trained on the SUN RGB-D dataset. This network is initialized using the encoder network of a Depth-TRecgNet from the SUN RGB-D dataset. To train the depth network on MIT67 dataset, we use the same label of its corresponding RGB data as the supervision. In the test phase, we fuse the results of RGB and depth networks with a ratio of 3 : 1 for the scores of all categories in a late fusion manner.

We apply TRecgNet to several state-of-the-art methods which have released models or codes in public. The quantitative results are shown in Table 7. It turned out that all the methods (using variant of backbones) get better results when coupled with a complementary depth network provided by TRecgNet. The vanilla Places-ResNet50 (Zhou et al. 2017) makes a boosting of 1.5% while other methods also get obvious gains up to nearly 1%. This shows that the constructed depth network effectively learns discriminative depth-specific representations, which is difficult to learn from RGB data without exploring the correlation between modalities. Our TRecgNet could transfer the learned RGB-D cues from RGB-D datasets and generate helpful cross-modal data for single-modal scene datasets. This generalization ability allows TRcegNet to aid the scene recognition task where only RGB data is available.

Table 8 Performance of TRecgNet with different backbone networks on the test set of SUN RGB-D
Full size table
(G) Does TRecgNet work when applying to other backbones? At last, we conduct experiments on the SUN RGB-D dataset using different popular backbone networks (we do not use too deep networks, because they fail to obtain further improvement on the RGB-D recognition task, due to the limit size of the dataset), which includes VGG networks (VGG11, VGG19) with batch normalization, ResNet101, and the relatively slim MobileNet v2 (Howard et al. 2017). We report the results in Table 8. TRecgNet improves the performance of both modality-specific networks for all the tested backbone networks with obvious margins, which shows the compromising generalization ability of the proposed method.

Table 9 Comparison with state-of-the-art methods on the test set of SUN RGB-D dataset
Full size table
Comparison with the State-of-the-Art Methods
Most state-of-the-art RGB-D scene recognition methods reported numbers using AlexNet pre-trained on the Places dataset (Zhou et al. 2014) as the backbone. However, TRecgNet works upon an image generation process and few image generation works adopted AlexNet as the backbone network since it downsamples feature maps irregularly and bounds in many pooling operations which we do not prefer. To fairly compare with these methods, we retrain an 8-layer network similar to AlexNet on the Places dataset with features maps downsampled regularly using convolution operation with stride set as 2. The retrained AlexNet achieves the comparable performance compared to the original one. As our model could produce helpful generated data to enhance the data sampler, we also report a TRecgNet Aug model, in which we randomly use the generated data in the training, the number of which is controlled as 30% in each batch. We use variant-C for the similarity network to produce the best results.

SUN RGB-D. We report qualitative results on the SUN RGB-D test set compared with state-of-the-art methods in Table 9. Our TRecgNet achieves state-of-the-art results on two modality-specific classification tasks (if any) with significant advantages and obtains the best fusion performance. Song et al. (2017) focused on learning effective depth representations by using patches to weakly supervised the classification task via SSP (He et al. 2014). However, they only focused on depth modality, and no correlation between modalities is explored. Wang et al. (2016) and Song et al. (2019) rely on exploring objects correlations in the scene to extract local semantic cues, and combine global information with pooling or attention mechanism. Xiong et al. (2020) extract the modal-consistent and modal-specific features simultaneously. Our method, instead, explores the correlation between modalities in an end-to-end way, in which we encode the discriminative representation in the translation. We think this may help learn subtle features between aligned patterns without such soft encoding manner. What’s more, the modality translation allows us to enhance the data sampling phase, and we could see further boosting for TRecgNet Aug model. It’s worth noting that only our model enables to transfer cross-modal cues to single modality dataset.

Fig. 10
figure 10
Heatmap visualization for different variants of the similarity net. Cross-modal refers to the activation from the CPL net that is leveraged in the variant-C

Full size image
Table 10 Comparison with state-of-the-art methods on the test set of the NYUD2 dataset
Full size table
NYUD2. We also evaluate TRecgNet on the NYUD2 test set and compare with other representative works. We report the results in Table 10. Our TRecgNet only yields a comparable results when only using the data from the NYUD2 dataset. We argue that it is mainly because the size of the NYUD2 dataset is too small (several hundreds); negative effects from noise of depth maps badly affect the modality translation. In particular, we pre-train the TRecgNet using 5k unlabeled data as mentioned in Sect.4.3 and fine tune the models on the NYUD2 dataset. We could find that this encourages the TRecgNet to overcome this problem, enhance the modality-specific representation power and improve its final recognition performance. When using generated data as training data, we observe further promotions for both modalities and fusion. Experiments on the NYUD2 dataset reveal that the training of TRecgNet requires much on the scale of training data to some degree. Translation on small dataset is difficult for benefiting the classification task, especially for modalities which exist measuring errors that can’t be ignored.

Fig. 11
figure 11
T-SNE visualization of layer4 features of different models. Spots of the same colors refer to the images of the same category. 𝑤/Aux and w/o Aux denotes whether or not using the auxiliary classification loss on generated perceptual features in the modality translation

Full size image
Fig. 12
figure 12
Visualization of learned ConvNets using CAM. (a)→(d) refer to four different CAM techniques, i.e., a Grad-CAM (Selvaraju et al. 2017), b Grad-CAM++ (Chattopadhay et al. 2018), c Smooth Grad- CAM++ (Omeiza et al. 2019), and d Score-CAM (Wang et al. 2020). +Trans means only using layer-wise perceptual loss in the modality translation, while +Aux denotes involving the label information in the TRecgNet

Full size image
Visualization
Besides recognition accuracies and generation qualities, we would like to go deeper into the learned ConvNet models. We first use T-SNE (Maaten and Hinton 2008) technique for dimensionality reduction that is particularly well studied for the visualization of high-dimensional features. We use the layer4 features of the encoder for visualization and compare the baseline with our TRecgNet w/ and w/o auxiliary classification loss on the SUN RGB-D dataset (train and val sets), and show the result in Fig. 11. We can easily find that for both RGB and depth data, when coupled with a modality translation process, we get a better clustering result using learned features which gets further optimized by adding auxiliary classification loss on perceptual features. We can draw a conclusion that the translation module indeed helps the modality-specific representation learning.

In addition, we also use the CAM technique to visualize the learned model. Specifically, we use four types of CAMs to make a comprehensive comparison, in which (a) Grad-CAM (Selvaraju et al. 2017) generalizes CAM to models without global average pooling, (b) Grad-CAM++ (Chattopadhay et al. 2018) improves GradCAM for more accurate pixel-level contribution to the activation, (c) Smooth Grad-CAM++ (Omeiza et al. 2019) proposed SmoothGrad mechanism coupled with the Grad-CAM, and (d) Score-CAM (Wang et al. 2020) used score weighting of class activation for better interpretability. We give some examples in Fig. 12 showing the effectiveness of our method. The main advantage for our TRecgNet is that we explore the complementary information in the training, which enables to give a more comprehensive understanding of the scene focusing on more types of objects. For example, in the first RGB scenes (the first three rows on the left), when coupled with the pyramid perceptual translation, the learned ConvNets recognize the living room by paying more attention to more furnitures instead of to the bed only in baseline model. Involving label information in the translation helps the model focus on a wider range to reach the effective desk areas (column (d)). Another interesting observation is that our TRecgNet is capable of exploring subtle parts of the objects, rather than directly extract the less discriminative whole object, e.g., in the last example, our ConvNets provides divided attention for objects on the cooking bench, showing a more discriminative power from the modality translation process (Fig. 9).

Conclusion and Future Work
In this paper, we have presented an effective Translate-to-Recognize Network (TRecgNet) to learn modality-specific RGB-D representations for RGB-D scene recognition task. TRecgNet enables a CNN classification network to learn more discriminative features by a pyramid modality translation, in which we proposed three variants of the similarity net with layer-wise perceptual supervision and the label information in the translation process. Training TRecgNet also allows using unlabeled RGB-D data for pre-training which makes up for the data scarcity problem. Also, we could produce images of high quality for enhancing training data sampling which is proved better than several popular image generation methods. As experiments demonstrated on the SUN RGB-D and NYUD2 datasets, we both achieve the state-of-the-art results validating the effectiveness of the proposed method. Experiments on MIT67 show that our TRecgNet could transfer learned depth cues to new RGB dataset, which effectively aids the scene recognition task where only single modality data is available.

In the future, we would like to extend this work to more vision tasks like RGB-D semantic segmentation, in which the classification of pixel is explored instead of the whole image. As the segmentation mask could also be regarded as a type of modalities, we would also explore the translation from RGB to segmentation masks which could adapt the method directly to RGB semantic segmentation task.