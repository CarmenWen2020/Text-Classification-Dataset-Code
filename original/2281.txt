The 3D CAD shapes in current 3D benchmarks are mostly collected from online model repositories. Thus, they typically have insufficient geometric details and less informative textures, making them less attractive for comprehensive and subtle research in areas such as high-quality 3D mesh and texture recovery. This paper presents 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated and large-scale repository of 3D furniture shapes in the household scenario. At the time of this technical report, 3D-FUTURE contains 9992 modern 3D furniture shapes with high-resolution textures and detailed attributes. To support the studies of 3D modeling from images, we couple the CAD models with 20,240 scene images. The room scenes are designed by professional designers or generated by an industrial scene creating system. Given the well-organized 3D-FUTURE and its characteristics, we provide a package of baseline experiments, such as joint 2D instance segmentation and 3D object pose estimation, image-based 3D shape retrieval, 3D object reconstruction from a single image, texture recovery for 3D shapes, and furniture composition, to facilitate related future researches on our database.

Access provided by University of Auckland Library

Table 1 Statistics of some representative 3D benchmarks
Full size table
Introduction
The rapid progress of modern machine learning methods, such as deep neural models, has led to various impressive breakthroughs towards 2D computer vision (CV) and natural language processing (NLP). One key to facilitating the advancement of these approaches is the availability of large-scale labeled benchmarks. Mirroring this pattern, the computer graphics and 3D vision communities have put tremendous efforts in establishing 3D datasets over the past years, expecting to enable and innovate the avenues of future research (Chang et al. 2015; Xiao et al. 2013, 2016; Song et al. 2015; Sun et al. 2018a; Xiang et al. 2014, 2016; Silberman et al. 2012; Dai et al. 2017a; Hua et al. 2016). For example, the largest 3D repositories, like ShapeNet (Chang et al. 2015) and ModelNet (Wu et al. 2015), collected massive 3D shapes from online repositories and organized them under the WordNet taxonomy. Relying on the repositories, several works, such as Pascal 3D+ (Xiang et al. 2014), ObjectNet3D (Xiang et al. 2016), Pix3D (Sun et al. 2018a), and Stanford Cars (Krause et al. 2013), further provided images and shapes associations or alignments with fine-grained pose annotations. Other works like NYU Depth Dataset (Silberman et al. 2012), SUN RGB-D (Song et al. 2015), ScanNet (Dai et al. 2017a), SceneNN (Hua et al. 2016), and Matterport3D (Chang et al. 2017) introduced RGB-D scans of real-world indoor environments with many estimated and manually verified annotations. Considering there are rich 3D benchmarks, why do we need one more? (Table 1)

In contrast to the 2D counterparts (Krizhevsky et al. 2012; Lin et al. 2014; Geiger et al. 2012), we realize that there is still a big gap between 3D academic research and industrial productions. For instance, the 3D CAD models in existing datasets mainly come from public online repositories like Trimble 3D WarehouseFootnote1 and Yobi3D.Footnote2 These 3D shapes typically have fewer geometry details and uninformative textures or even no textures. Specific to shapes in the household scenario, most of them are outdated and dull furniture deprecated by modern professional designers. Therefore, these 3D shapes may inadequate for comprehensive and subtle research in areas such as industry closely related fine-grained 3D shape understanding and texture recovery. Besides, existing benchmarks only provide pseudo image-shape alignments and the estimated camera pose annotations. Namely, the benchmark designers manually choose a roughly matched 3D CAD model from available 3D shape benchmarks according to the object in the image. Thus, annotators may largely ignore some local shape details, which prevents the progress of fundamental data-driven studies such as high-quality 3D reconstruction from real-world images and high-accuracy image-based 3D shape retrieval. Last but not least, there is no well-organized benchmark that offers realistic synthetic indoor images with both instance-level semantic annotations and the involved 3D shapes with textures (2D–3D Alignments).

Motivated by the observations, we present 3D Furniture shape with TextURE (3D-FUTURE): a richly-annotated, large-scale repository of 3D furniture shapes specific to the household scenario as shown in Fig. 1. At this time, 3D-FUTURE provides 9992 3D furniture models which are with rich geometry details and informative textures. The 3D furniture shapes are used for modern industrial productions and have fine-grained attributes, such as category, style, theme, and material. These attributes are related to their geometries and textures. We clarify a qualified 3D model repository should also well support the studies of 3D modeling from images. We thus utilize these 3D CAD models for interior designing and render 20,240 scene images via an industry-leading 3D rendering software V-Ray.Footnote3 The room scenes are developed by experts or produced by our scene design algorithm. We release as most as the rendering information, including six degrees of freedom (6DoF) poses, camera field of view (FoV), and instance segmentation annotations. Beyond these features, we believe 3D-FUTURE would enable many fundamental studies and some new research opportunities such as furniture composition and high-quality texture recovery.

Fig. 1
figure 1
3D-FUTURE is a new, large-scale, and richly-annotated 3D repository. It provides 9992 high-quality 3D CAD furniture models with high-resolution textures and detailed attributes. These furniture shapes are contained in 5K designed rooms. We use the computer-generated imagery rendering software V-Ray to render 20K photo-realistic images to support the studies of 3D modeling from images and interior designing understanding. The statistics and highlights of 3D-FUTURE are presented in Sect. 5

Full size image
It is, however, nontrivial to collect thousands of CAD interior designs. It would take a designer several days to complete a house’s design. Thus, we considered two main research questions when establishing 3D-FUTURE: (1) can we develop a framework that allows creators to design delicate rooms efficiently? (2) can we automatically create some designs based on the professional layout information? To investigate the former question, we build a furnishing suite composition (FSC) system which has been integrated into Alibaba Topping HomestylerFootnote4 Design Platform.Footnote5 The system recurrently recommends visually matched furniture by considering style and color compatibility during the design progress. FSC allows a designer to create a qualified room design in two or three hours. For the latter question, we reuse the expert layouts, generate multiple furnishing suite candidates with some rules and the furnishing suite composition system, render the scene, and manually select visually appealing designs. These AI-created designs have be reviewed by experts to ensure good quality. One can try an improved FSC, other AI designing features, and a custom render interface on this platform.

The remainder of this paper is organized as follows. First, we briefly review the public 3D benchmarks and discuss their imperfections in Sect. 2. Second, we present the data acquisition process in Sect. 3. Third, we introduce the properties and statistics of 3D-FUTURE in Sect. 5. Finally, we conduct various experiments leveraging on the properties in Sect. 6. These experiments cannot well supported by existing 3D model datasets, and can serve as baselines for subsequent research on 3D-FUTURE.

Related Work
Lots of 3D benchmarks have been established and made publicly available over the past decades (Chang et al. 2015; Wu et al. 2015; Xiao et al. 2013, 2016; Song et al. 2015; Sun et al. 2018a; Xiang et al. 2014, 2016; Zhang et al. 2017; Silberman et al. 2012; Dai et al. 2017a; Hua et al. 2016; Choi et al. 2016; Shilane et al. 2004). These datasets can be mainly divided into two groups, including 3D models and RGB-D scenes. We will briefly review some representative 3D benchmarks in the following (Fig. 2).

Fig. 2
figure 2
Left: An illustration of the deep furnishing suite model (DFSM). The development of the framework borrows the concepts from Bert (Devlin et al. 2018), a modern language model. We construct two tasks here, including mask prediction and compatibility scoring. There is only one visual embedding network (VEN) which is shared in both the two tasks. The deep visual embedding (“orange”) for a specific item is captured by the trained VEN. Right: An example of how we create an interior design based on a template expert design

Full size image
3D Models One of the large and exhaustively studied 3D shape repositories is ShapeNet (Chang et al. 2015). It collects millions of raw 3D CAD models from public online repositories such as Warehouse3D and Yobi3D. By re-organizing the datasets, the subsets ShapeNetCore and ShapeNetSem have been made publicly available, including 51,300 and 12,000 models. ShapeNet assigns rich semantic annotations to part of the shapes, such as synsets in the WordNet taxonomy, functional patterns, parts, keypoints, and categories. 3D shape repositories like ModelNet (Wu et al. 2015) and Princeton Shape Benchmark (Shilane et al. 2004) also share similar content as ShapeNet. Several other works like (Choi et al. 2016) and ScanObjectNN (Uy et al. 2019) create the datasets of 3D scans of real objects based on state of the art (SOTA) RGB-D reconstruction approaches (Dai et al. 2017b; Schönberger and Frahm 2016; Schönberger et al. 2016). These benchmarks have largely driven the fundamental 3D studies, including 3D representation, 3D shape recognition, 3D object reconstruction, and part segmentation. However, since the 3D shapes are collected online, many may lack geometry details and have dreamlike or no textures. IKEA (Lim et al. 2013) shares 219 high-fidelity furniture shapes. Nevertheless, they have not provided the textures and attributes of these CAD models. And its small dataset order cannot well support data-driven 3D modeling studies.

Fig. 3
figure 3
We provide the 2D–3D alignments. That means for an image, we share the contained CAD models and the precise 6DoF pose annotations. Previous benchmarks like ObjectNet3D (Xiang et al. 2016), PASCAL3D+ (Xiang et al. 2014), and Pix3D (Sun et al. 2018a) show limitations in two aspects: (1) They only give pseudo 2D–3D alignments since the annotators manually choose roughly matched 3D CAD models from a 3D pool based on the given images; and (2) They offer one image to one shape pairs, while 3D-FUTURE provides multiple involved 3D CAD models of each image. Zoom in for better view

Full size image
Relying on these large-scale 3D shape databases, the community also builds benchmarks with image and shape associations to facilitate the research of 3D object understanding from images. For example, PASCAL3D+ (Xiang et al. 2014) and ObjectNet3D (Xiang et al. 2016) aligned objects in the 2D images with the 3D shapes and provided raw 3D pose annotation. Further, Pix3D (Sun et al. 2018a) contributed more accurate 2D–3D alignment for 395 3D shapes of nine object categories. Unluckily, these pseudo alignments may largely ignore some local shape details since the annotators manually choose roughly matched 3D models from a given 3D pool. Moreover, the expensive labor efforts make it difficult to build a large-scale benchmark with precise 2D–3D alignment.

RGB-D Scenes In recent years, the community has put significant efforts into building RGB-D datasets to expand researches on 3D scene understanding. For example, NYU Depth V2 (Silberman et al. 2012) captured 464 short Kinect RGB-D sequences from 464 different indoor scenes, where 1449 images are with dense per-pixel labeling, including depth, surface normal, and semantic labels. SUN RGB-D (Song et al. 2015) followed the pattern by annotating 10,335 RGB-D frames, and offered 3D bounding boxes. To capture the full 3D extent of indoor environments, SUN3D (Xiao et al. 2013) obtained 415 long sequences in 254 unique spaces with comprehensive views. Further, Dai et al. established ScanNet (Dai et al. 2017a), an RGB-D video dataset containing 2.5M views in 1513 scenes annotated with estimated 3D camera poses, surface reconstructions, semantic segmentation, and a broad set of CAD model alignments. Later, a more extensive dataset Matterport3D (Chang et al. 2017) was made publicly available, contributing to panoramic HDR color images with 3D scene annotations. Different from these scanned real-world RGB-D databases, we focus on professional exquisite interior designs used in industrial productions (Fig. 3).

The works most closely related to ours are InteriorNet (Li et al. 2018) and Structured3D (Zheng et al. 2019), which also offer photorealistic images by rendering professional house designs. However, there are two significant differences. First of all, we provide furniture shapes with textures in the scenes. The 6DoF pose and camera FoV are shared in 3D-FUTURE. Second, 3D-FUTURE additionally expects to foster studies of exquisite interior design understanding. Thus, for each room, the camera viewpoint is suggested by its designer, so that the captured image contains the whole design ideas. A previous 3D repository SUNCG (Song et al. 2017) shared the holistic CAD house packages, but became unavailable now. Our 3D-FUTURE focuses more on the modern furniture shapes and 2D-to-3D associations. The images are rendered using the expert designs or created ones.

Data Acquisition Process
In this section, we introduce the pipeline of the project to acquire the 3D-FUTURE dataset. We are lucky that we have a large-scale 3D database with many modern 3D furniture models, as introduced in Sect. 3.1. However, as aforementioned, it’s better for a 3D repository to provide model and image correspondences (or 2D–3D alignments) to support studies of 3D modeling from images (Xiang et al. 2016; Sun et al. 2018a; Krause et al. 2013). Thus, developing many interior designs using these CAD models in an acceptable project cycle and rendering photo-realistic images are significant. Towards designing efficiency and pretty design creation, we state our solution mainly in Sects. 3.2 and 3.3 (Fig. 4).

Fig. 4
figure 4
Samples of the 3D models and their textures in 3D-FUTURE. These modern furniture shapes have rich geometry and texture details. Zoom in for better view

Full size image
Large-Scale Interior Database
The 3D-FUTURE project is built on a large-scale computer-aided design (CAD) interior database offered by Alibaba Topping Homestyler. It’s the official home furnishing eCommerce platform of Alibaba. Briefly, the database contains a large amount of industrial 3D CAD furnishing models (about 1M). Each model is richly annotated with diverse attributes, including theme color, style, material, brand, real-world size, and category in the WordNet taxonomy. There are 500 fine-grained categories in five levels of the taxonomy. Based on these objects, hundreds of experienced designers have created ∼ 60K decorative houses for different scenarios in several years. Given a house design, several senior designers of Topping Homestyler would evaluate its quality into five levels, including other, normal, good, excellent, and brilliant based on their experience. The final design level is the average score. Here, “Other” means unfinished or invalid designs. It is worth to mention that a normal design does not mean it not good for non-professionals, it also considers furnishing compatibility. We cannot publish all these content due to copyright restrictions. But we can set up a project based on the large database to build 3D-FUTURE.

Furnishing Suite Composition (FSC)
We develop a high-performing furnishing suite composition framework so that experienced designers could perform interior designing more efficiently. This allows us collecting many pretty room designs in an acceptable project cycle, and create scene-level 2D–3D alignment annotations.. It learns a deep furnishing suite model (DFSM) to extract representative visual feature, and optimizes a decision tree to perform furnishing suite re-scoring (Fig. 5).

Fig. 5
figure 5
Top: samples of photo-realistic synthetic images and their corresponding instance-level annotations from 3D-FUTURE. Bottom: natural images from the widely studied large-scale scene parsing benchmark ADE20K (Zhou et al. 2017)

Full size image
The training set for the algorithm below has the form as {1,2,...,𝑁}, and 𝑖={𝑥𝑖1,𝑥𝑖2,...,𝑥𝑖𝑚𝑖}. Here, N is the total number of experienced house designs. 𝑚𝑖 is the number of items in house 𝑖, and 𝑥𝑖𝑗 is a specific object contained in house 𝑖. Note that the elements in 𝑖 are in order, which means 𝑥𝑖𝑗 is a former item selected by designers followed by 𝑥𝑖𝑗+1.

Deep Furnishing Suite Model Given a furnishing suite 𝑖, we randomly capture a subsequence 𝑋𝑖𝑗∼𝑘={𝑥𝑖𝑗,𝑥𝑖𝑗+1,...,𝑥𝑖𝑘}, where 1≤𝑗≤𝑘<𝑚𝑖. Our goal is to predict 𝑥𝑖𝑘+1 given 𝑋𝑖𝑗∼𝑘. Based on the category label of 𝑋𝑖𝑗∼𝑘, we randomly choose three negative examples from the 3D pool to construct a candidate set 𝐶={𝑥𝑖𝑘+1,𝑧0,𝑧1,𝑧2} in an online manner. We ensure that the negative examples 𝑧0/𝑧1 / 𝑧2 have the same style/color/material as 𝑥𝑖𝑘+1. We feed both 𝑋𝑖𝑗∼𝑘 and the candidate images into VEN to extract visual features. In our paper, we take the CNN part of MobileNetV2 followed by a projection layer as VEN, and pre-train it via the unsupervised learning strategy stated in Wu et al. (2018).

After obtaining the image features, we construct two tasks, i.e., mask prediction and compatibility scoring, based on the impressive transformers in natural language processing works (Devlin et al. 2018; Vaswani et al. 2017). See Fig. 2 for an illustration. For the former one, we have a sequence of feature vectors 𝑖={VEN(𝑋𝑖𝑗∼𝑘), [Mask]} with dimension d, where [Mask] denotes a particular mask embedding. The task is to predict the masked item given the previous ones. We thus feed  into TransEnc1 to capture the enhanced feature ˜𝑖, and optimize the model via the following loss:

𝑚𝑝=−1𝑁∑𝑖=1𝑁log((𝑥𝑖𝑘+1|˜𝑖;Θ,Φ)),(𝑥𝑖𝑘+1|˜𝑖;Θ,Φ))=𝑒𝑥𝑝(𝑓̃ 𝑖𝑚𝑎𝑠𝑘𝑓𝑇𝑥𝑖𝑘+1)∑𝑐𝑒𝑥𝑝(𝑓̃ 𝑖𝑚𝑎𝑠𝑘𝑓𝑇𝑐),
(1)
where Θ and Φ are the learnable parameters of VEN and TransEnc1, respectively; 𝑐∈𝐶 is a candidate; 𝑓𝑇𝑥 is the transpose of 𝑓𝑥; 𝑓𝑥 denotes the visual embedding of item x, i.e., 𝑓𝑥= VEN(x); and 𝑓̃ 𝑖𝑚𝑎𝑠𝑘∈𝐹˜𝑖 represents the feature vector of the [Mask] token from TransEnc1 (Figs. 6, 7, 8).

Fig. 6
figure 6
The statistics of the attribute annotations of the 9992 shapes in 3D-FUTURE. Furniture shapes with the attributes such as “Modern”, “Japanese”, “Smooth Net”, “Texture Mark”, “Rough Cloth”, and “Wood” may be more welcomed by designers when designing the rooms. Besides, except for some special cases, each attribute category has at least 90 shapes

Full size image
Fig. 7
figure 7
The shape number of the 34 categories in 3D-FUTURE. These categories are verified and used by experienced designers in their daily works. The figure also implies the frequency of furniture selected by designers to design the room scenes. There are only 7 dressing chairs because designers commonly choose other chairs as the replacements of dressing chairs when designing a room. For example, Classic Chinese Chair and Chaise Longue Sofa only appear in some special designs

Full size image
Fig. 8
figure 8
The percentile plot of the number of vertices and faces over ShapeNetCore (Chang et al. 2015), ModelNet40 (Wu et al. 2015) and 3D-FUTURE. While other datasets have some extremely low-resolution shapes, 3D shapes in 3D-FUTURE show uniformed distributions on both vertices and faces

Full size image
For the second task, we take the candidate suites as inputs and directly learn their compatibility scores. Let 𝐹(𝑋𝑖𝑗∼𝑘,𝑐)= {[Start], VEN(𝑋𝑖𝑗∼𝑘), 𝑓𝑐} be the visual feature vectors of a candidate suite 𝑂(𝑋𝑖𝑗∼𝑘,𝑐), where [Start] is a particular start token embedding. To estimate the compatibility score of a suite, we need to first capture an embedding that can represent it. We thus employ TransEnc2 to acquire 𝐹˜(𝑋𝑖𝑗∼𝑘,𝑐), and use the feature vector of the [Start] token as the representation of the suite 𝑂(𝑋𝑖𝑗∼𝑘,𝑐) (denoted as 𝑟(𝑋𝑖𝑗∼𝑘,𝑐)). Further, we utilize two fully connected layers and a sigmoid function to secure a score (𝑠(𝑋𝑖𝑗∼𝑘,𝑐)), which is the measure of the quality of the suite. For conventional presentation, 𝑠(𝑋𝑖𝑗∼𝑘,𝑐) is abbreviated as 𝑠(𝑥𝑖𝑘+1) hereafter. Since the ground truth compatibility scores are not available, we minimize a margin ranking loss with a simple hard sample mining policy. The objective is expressed as:

𝑐𝑠𝑠(𝑧)=−1𝑁∑𝑖=1𝑁𝑚𝑎𝑥(0,−𝑠(𝑥𝑖𝑘+1)+𝑠(𝑧)+𝛼),=𝑚𝑎𝑥(𝑠(𝑧0),𝑠(𝑧1),𝑠(𝑧2)),
(2)
where 𝛼 is set to 0.1 in our experiments. The trained visual embedding network (VEN) is used to extract the visual feature for each furniture item.

Decision Tree Based Re-scoring The main goal here is to infer attribute-based matching patterns, i.e., attribute crosses. Considering both interpretability and scalability, we utilize GBDT (Friedman 2001) to automatically construct attribute crosses. We will not introduce the details of GBDT here, but only present some facts in training the decision trees. Particularly, we employ six attributes to represent a specific item, including theme color, style, material, real-world size, the second-level category, and visual information (the learned visual embedding). Here, we denote the learned visual embedding as an attribute. For the discrete attributes (style, material, and the second-level category), we directly convert them to one-hot vectors. For theme color and real-world size, we first adopt k-Means Clustering (Kanungo et al. 2002) to discretize real values and then transform them into one-hot vectors. By further considering the visual embedding, we can represent each item as a feature vector. We assign a label (positive or negative) to each specific furnishing suite to train the decision trees (Table 2).

Table 2 User studies on data quality: 3D-FUTURE vs. ShapeNet
Full size table
Re-training via Hard Sample Mining We may easily obtain some naive negative samples during the training process due to the large-scale 3D pool, causing some inaccuracies in both the deep embedding networks and the decision trees. We fine-tune the visual embedding network and re-train the decision tree model via a hard sample mining policy to address the issue. Specifically, given 𝑋𝑖𝑗∼𝑘, we can have the TopK recommendations using the trained DFSM. We then randomly select negative samples from the TopK pool. We set K to 40 in our experiments. After the re-training stage, we fix VEN’s parameters and build an automatically re-training system to update the decision tree model daily using continuously enlarged online designs.

Interior Design Creation
We have collected 5K interior room designs in the 3D-FUTURE project. All contributed by designers of Topping Homestyler. The artist would set a camera position and adjust the light sources and strengths for his/her designed room, and render a porcelain image to show his/her art. These rendering parameters have been recorded. We do not plan to provide several synthetic images in different viewpoints for each room. Instead, we expect to deliver more designs to bring more research possibilities. Thus, we take these professional designs as templates and create several pretty interior designs based on each template. See Fig. 2 (right) for an overview of the creation process. For example, given a template room with professional design ideas, we first replace the interior finishing according to the materials, room style, and other descriptions. The design ideas for a room consist of the category labels of objects that are suggested to put in, and their positions, orientations, sizes, and styles. Second, we choose a furniture seed (e.g., dining table) based on the interior finishing information. Third, we iteratively perform recommendations based on FSC and other rules to generate a furnishing list. Finally, we put these items into their corresponding positions. In the third step, we also learn one-to-one visual compatibility models (e.g., bed-nightstand, dining table-dining chair, sofa-coffee table) as additional rules to improve the recommendation ability.

With the pipeline, we can automatically create many interior designs as shown in Fig. 9. Since the expert design ideas have been preserved during the creation process, the produced designs are usually not bad. To ensure the quality, we render an image for each created design based on its template rendering parameters, and send it to the designer team of Topping Homestyler. Then, one of their experts will accept “Good” or better designs based on his/her experience. Other designs are deprecated. See Sect. 3.1 for design levels. As a result, we obtain 15K visually appealing images from the created designs.

Fig. 9
figure 9
Left: professional design templates. Right: created interior designs. An example of how to create a design is illustrated in Fig. 2(right). These generated designs are reviewed by Alibaba Topping Homestyler’s designers

Full size image
Validation and Assessment
Here, we validate and assess the quality and utility of 3D-FUTURE from several aspects. The highlights of the dataset are discussed in Sect. 5.

Evaluating the Recommender System
We conduct the fill-in-the-blank (FITB) and furniture suite compatibility experiments and report FITB accuracy and Area Under The Curve (AUC) to evaluate the recommender system. We collect 8K room designs and their design logs from the online Topping Homestyler design platform to construct the validation set. Given a sub-suite (A, B, C) from a room design, where a designer select A, B, and C in order, we randomly sample one example based on the third-level category label of C to generate a negative suite (A, B, C’) for the furniture suite compatibility task. For the FITB tasks, given a room suite that contains at least four objects, we sample four alternatives for each object. See the fashion compatibility works (Cucurull et al. 2019; Chen et al. 2019a) for details about the two tasks and metrics.

We make comparison with POG (Chen et al. 2019a) and CAVCP (Cucurull et al. 2019). The models are trained on our large-scale database. The scores are reported in Table 3. FSC produces 0.766 and 69.7% on AUC and FITB Acc., respectively, outperforms other methods by a remarkable margin. It’s worth to mention that FSC has been integrated into the online Topping Homestyler design platform to speed up the design efficiency. FSC allows a designer to create a qualified room design in two or three hours. Besides, FSC was used to create teaser images for furniture sales agents in Alibaba Taobao and Topping Homestyler. See Fig. 2 (right) for an example. The rate of our customer preferred images (or designs) is 88%, while it is only 71% for designs from junior designers.

Table 3 Comparisons with our methods for room suite composition
Full size table
Quality Assessment
We conduct several user studies, on Amazon Mechanical Turk (AMT), to assess the quality of the data provided by 3D-FUTURE. The quality criteria considered include those related to individual objects (in terms of texture quality and geometry preferability) and scene images (in terms of design quality). As for the user study setting, we randomly sample 50 pairs of 3D models from 3D-FUTURE and ShapeNet based on model category, and 50 rendered scene images (of created designs) from 3D-FUTURE. For design quality evaluation, we require the annotators to answer if the shown interior design is pretty or not? We give the Turkers a hint that they should focus more on the furniture compatibility w.r.t color, shape style, and size. Each pair (or question) was labeled by 100 master-level annotators in AMT. Thus, the model and scene scores are all calculated using 5K feedback. From the scores reported in Table 2, the majority of Turkers (between 74% and 76%), preferred 3D models presented by 3D-FUTURE. And 92% of Turkers think our created designs are pretty.

Properties of 3D-FUTURE
In this section, we summarize the properties of our 3D-FUTURE database. Compared to previous 3D benchmarks, 3D-FUTURE has some prominent properties that can bring more possibilities for future 3D research.

High-Quality Shapes with Informative Textures
The 3D shapes contained in previous large-scale shape repositories (Chang et al. 2015; Shilane et al. 2004; Wu et al. 2015) are mainly collected from online repositories. These 3D CAD models usually contain few geometry details and low-resolution textures. Luckily, 3D-FUTURE provides modern high-quality 3D furniture shapes with rich details in various styles, including European furniture, which often contains intricate carvings. All the shapes come with informative textures and have been used for modern industrial productions. We show some samples in Fig. 4. We believe these features can potentially facilitate innovative research on high-quality 3D shape understanding and generation. In Fig. 8, we compare the proportion of different number of vertices and faces over ShapeNetCore (Chang et al. 2015), ModelNet40 (Wu et al. 2015) and our dataset. While other datasets have some extremely low-resolution shapes, 3D shapes in 3D-FUTURE show uniform distributions in both vertices and faces.

Fine-Grained Attributes
ShapeNet (Chang et al. 2015) provides functional attribute annotations in WordNet taxonomy for 3D shapes. However, these attributes are not well organized and do not consider their textures and materials. In contrast, for each textured shape in 3D-FUTURE, we provide four types of attributes verified by professional designers. We have 34 shape categories, 8 super-categories, 19 styles, 15 materials, and 16 themes. These attributes have been demonstrated valuable for interior designs and content understanding by industrial productions. We present the statistics of these attributes in Figs. 7 and 6. These figures imply the preferences of experienced modern designers when designing the rooms.

Synthetic Images with Suggested Viewpoints
3D-FUTURE offers 20,240 synthetic images corresponding to 20,240 interior designs. As aforementioned, we have 5K professional room designs and 15,240 automatically created designs. We render one image for each room scene via a render cluster driven by V-Ray, an industry-leading 3D rendering software powered by adaptive ray tracing technology. For Global Illumination (GI), we use its brute force settings, and turn on GI caustics, ambient occlusion, and photon mapped caustics features. All the contents in the scenes are with V-ray materials. The rendering ability is also offered by Alibaba Topping Homestyler. We refer to its design platform for a customer render interface. Previous datasets, such as Structured3D (Zheng et al. 2019) and InteriorNet (Li et al. 2018), provide realistic indoor images and scene parsing annotations. However, they put cameras in random positions and capture redundant images for each house. These images were not manually verified, thus suffer from unexpected viewpoints. In 3D-FUTURE, for the professional designs (or template designs), the camera positions are suggested by professional designers to obtain a good viewpoint for each room. The light sources and strengths for each room are also given by expert designers. For created designs, these rendering information sources from the corresponding expert design templates. See Fig. 2(right) for an example. Besides, 3D-FUTURE provides instance semantic labels of 34 categories and ten supper-categories for these images.

2D–3D Alignments
Previous benchmarks only provide pseudo 2D–3D alignment annotations (Xiang et al. 2016, 2014; Sun et al. 2018a; Dai et al. 2017a; Krause et al. 2013). Namely, they manually choose a roughly matched 3D CAD model from public 3D shape benchmarks according to the object in an image. Annotators thus may largely ignore some local shape details. As a result, these benchmarks offer a small number of matched 3D shape and 2D image pairs. Besides, previous benchmarks with alignment annotations do not come with scene images. In contrast, 3D-FUTURE provides precious 2D–3D alignments and 3D pose annotations. It contains 9992 unique 3D shapes and 20,240 scene images. By cropping instances from the scene images, we can further secure 37,441 image and shape pairs with slight occlusions, as reported in Table 4. Some samples are presented in Fig. 3.

Table 4 The train and test sets for the subject of cross-domain image-based 3D shape retrieval
Full size table
Baseline Experiments
In the section, we conduct several baseline experiments by leveraging the properties of 3D-FUTURE, including shape recognition, joint 2D instance segmentation and 3D pose estimation, image-based shape retrieval, 3D object reconstruction, and texture synthesis. We consider both the scenes and the involved shapes to split the train and test set. As a result, we split the 3D shapes into a training set with 6699 models, and a test set with 3293 models. The scene images are divided into 14,761 and 5479 for training and test, respectively. A training shape only appears in some training images. We will briefly present the experimental details for each task and report the scores.

Fine-Grained 3D Object Recognition
Over the past years, most 3D object recognition methods extend deep convolutional neural networks (DCNNs) to modeling 3D data. Because 3D CNNs are too memory intensive (Ji et al. 2012), some researchers prefer to either develop special deep learning operations on point clouds and mesh surfaces (Qi et al. 2017a, b; Hanocka et al. 2019; Feng et al. 2019), or project 3D shapes to several 2D images and then apply 2D convolutional networks (Su et al. 2015). However, it is nontrivial to extend the projection-based methods to high-resolution 3D scene understanding. Moreover, point and mesh-based approaches suffer from computation bottlenecks and are thus limited to sparse point clouds and a small number of surfaces.

In contrast to ShapeNet (Chang et al. 2015) and ModelNet (Wu et al. 2015), 3D-FUTURE enables the study of fine-grained 3D furniture recognition, which requires the networks to capture more local and global geometric details. Here we consider the well-known MVCNN (Su et al. 2015) and PointNet++ (Qi et al. 2017b) as the baselines. In specific, we train a 12-view MVCNN with ResNet50 as the backbone. For PointNet++, we sample 1024 points for each shape instance and adopt the multi-scale grouping (MSG) strategy (Qi et al. 2017b) and normal vectors to secure the best performance. We train the networks using 6699 shapes and evaluate the trained models via the remaining 3293 shapes. The classification accuracy for each category is presented in Table 5. While these methods can reach 90% accuracy on ModelNet40 and ShapenetCore (Su et al. 2015; Qi et al. 2017b), they do not perform well (69.2–69.9%) on 3D-FUTURE, due to the presence of fine-grained furniture categories. This observation would motivate researchers to exploit more efficient 3D representation learning approaches for deeper 3D shape analysis. Furthermore, we evaluate the trained PointNet++ here on ModelNet40 with a post fine-tuning process. It yields an accuracy of 93.7%, while PointNet++ trained on ModelNet40 obtains an accuracy of 91.8%.

Table 5 Classification accuracy on 3D-FUTURE
Full size table
Image-Based 3D Shape Retrieval
Cross-domain image-based 3D shape retrieval (IBSR) is to identify the CAD models of the objects contained in query images. The primary issue in IBSR is the large appearance gaps between 3D shapes and 2D images. To tackle this challenge, early works made efforts to map cross-domain representations into a unified constrained embedding space via adaptation techniques such as weight-sharing constraints, metric learning, and distance matching (Li et al. 2015; Aubry et al. 2014; Lee et al. 2018; Massa et al. 2016; Tasse and Dodgson 2016; Girdhar et al. 2016). Recent works (Sun et al. 2018a; Huang et al. 2018; Wu et al. 2017; Bansal et al. 2016; Bachman 1978; Grabner et al. 2018, 2019; Fu et al. 2020) predict 2.5D sketches from images, such as surface normal, depth, and location field, to bridge the gaps between 3D and 2D domains. However, the performance of state-of-the-art IBSR methods show a large gap than its 2D counterpart, i.e., content-based image retrieval. This is because there are no large-scale benchmarks that offer large amounts of precious 2D–3D alignment annotations.

In this experiment, we train the baseline using 31,444 image-shape pairs and evaluate the retrieval algorithm via the other 5994 image-shape pairs. Then we crop the furniture instances with occlusion levels of “NO”, “Slight” and “Standard” from the scene images to produce the image-shape pairs. The statistics of the train and test sets are presented in Table 4. We develop a DCNN based metric learning network to study the cross-domain shape similarities, as shown in Fig. 13. Specifically, we first project the selected 3D shapes into 2D planes using the toolboxFootnote6 to bridge the 3D and 2D gaps. Given a query image and its corresponding 3D shape, we randomly sample a negative 3D shape from the 3D pool to construct a triplet. We then feed the triples (2D images) into a ResNet-34 feature extractor and adopt a margin ranking loss to push the query image close to its corresponding 3D shape.We utilize a category classification loss and an instance classification loss (Wu et al. 2018) such that the network can capture shape similarity among furniture instances.

We take TopK Recall (TopK@R) and Top5 average F-score (mean F-score) as our metrics. The latter is used to measure the retrieval sequences. The retrieval results for each category are reports in Table 6. We also show some qualitative retrieval sequences in Fig. 12. We can see that while the captured Top1@R for a large portion of categories is less than 30.0%, the retrieval sequences seem to be visually acceptable. Besides, there is a remarkable gap between Top1@R (23.4%) and Top3@R (40.6%). Moreover, we evaluate the baseline model on Pix3D with or without further fine-tuning. The Top1@R for the two settings are 63.5% and 58.2%, respectively. We also directly train the baseline model on Pix3D, it yields a Top1@R of 51.7%. The observations demonstrate that 3D-FUTURE contains many furniture shapes with similar geometric characteristics, coupling with lots of precious 2D–3D alignments, it would provide potential opportunities for fine-grained shape retrieval studies.

Table 6 Numerical retrieval results on 3D-FUTURE for category level
Full size table
Instance Segmentation and 3D Pose Estimation
Image-based 6DoF pose estimation is a fundamental 3D vision task that can benefit many intelligent applications such as autonomous driving, augmented reality, and robotic manipulation. Typical methods first build point-wise correspondences between 3D models and 2D images, then utilize the Perspective-n-Point (PnP) algorithm to compute pose parameters (Collet et al. 2011; Rothganger et al. 2006). These approaches perform well for objects with rich textures but are not robust to featureless or occluded cases. Recent works thus employ RGB-D sensors and deep learning to improve keypoints detection or directly predict 6DoF pose from images (Kehl et al. 2016; Brachmann et al. 2014; Bo et al. 2014; Hinterstoisser et al. 2012; Xiang et al. 2017; Peng et al. 2019; Song et al. 2020; Tekin et al. 2018; Rad and Lepetit 2017; Park et al. 2020). Nevertheless, the main issues such as occlusion and clutter, scalability to multiple objects, and symmetries have not been well addressed.

Instance segmentation is the task of detecting and delineating each distinct object of interest appearing in an image. Current instance segmentation methods can be roughly categorized into two paradigms: segmentation-based methods and detection-based methods. The former category of approaches group the predicted category labels via techniques such as clustering (Dhanachandra et al. 2015), metric learning (Fathi et al. 2017), and watershed algorithms (Najman and Schmitt 1994), to form instance segmentation results. The latter predicts the mask for region instances detected by SOTA object detectors. Methods such as Mask R-CNN series (He et al. 2017; Huang et al. 2019; Cai and Vasconcelos 2019) have achieved impressive performance for daily objects.

Fig. 10
figure 10
An illustration of the network for joint instance segmentation and pose estimation. B: region proposals. C: object recognition. H: network head. M: mask prediction. R&T: pose estimation. Seg: the network in the instance segmentation branch. Pose: the network in the pose estimation branch

Full size image
Fig. 11
figure 11
Histograms of the instance segmentation AP and rotation estimation AOS of the 34 categories on the test set. The closer AOS is to AP, the better the rotation estimation

Full size image
In this experiment, we learn to predict instance segmentation in 2D images and estimate their 6DoF poses in a unified framework. In contrast to the well-studied benchmarks such as ObjectNet3D (Xiang et al. 2016), PASCAL3D+ (Xiang et al. 2014), and Pix3D (Sun et al. 2018a), 3D-FUTURE encourages estimating pose parameters for multiple objects with occlusions in diverse indoor scenes. We provide 3D pose annotations for 100K+ objects in the scene images. The objects are further divided into five occlusion levels, including “NO”, “Slight”, “Standard”, “Heavy”, and “N/A”. Here, an object labeled as “N/A” means that its corresponding 3D shape is not available, or a part of the object is out of the camera view. We train our model on the 14,761 training images and test it on the remaining 5479 test images.

We modify Cascade Mask-RCNN (Cai and Vasconcelos 2019; He et al. 2017) as our baseline. The network architecture is shown in Fig. 10. Specifically, we take ResNeXt-101 (Xie et al. 2017) with the setting of 64-4d (group number: 64, width of group: 4) as the backbone, and adopt FPN (Lin et al. 2017) to extract the dense features. Then, we utilize a three-stage cascade architecture to perform bounding box regression and object classification. Finally, we add two branches that consist of several fully connected layers to predict the instance masks and their 6DoF poses simultaneously. We cast rotation estimation as a viewpoint classification problem. In detail, we convert the rotation matrices to Euler angles and divide the 360-degree azimuth, 180-degree elevation, and 360-degree in-plane rotation into 18 bins, 9 bins, and 18 bins, respectively. For translation estimation, we use L1 smooth loss to regress the translation parameters (Figs. 11, 12, 13).

Fig. 12
figure 12
The retrieval sequences for several query images. 3D-FUTURE contains fine-grained shapes for each furniture category

Full size image
Fig. 13
figure 13
An illustration of the baseline method of cross-domain image-based 3D shape retrieval. We use instance-level non-parametric softmax loss (Wu et al. 2018) so that the network can capture shape similarity among furniture instances

Full size image
For 2D instance segmentation, we report Average Precision (AP) and Average Recall (AR) over different IoU thresholds following (He et al. 2017). For 3D pose estimation, we take both Average Viewpoint Precision (AVP) in PASCAL3D+ (Xiang et al. 2014) and Average Orientation Similarity (AOS) in KITTI (Geiger et al. 2012) to measure the rotation predictions as (Xiang et al. 2016), and employ Root Mean Square Error (RMSE) to evaluate the translation predictions. In specific, we define the difference between an estimated rotation matrix R and its ground truth 𝑅𝑔𝑡 as ∇(𝑅,𝑅𝑔𝑡)=12√∥log(𝑅𝑇𝑅𝑔𝑡)∥𝐹. In AVP, a correct estimation should satisfy ∇(𝑅,𝑅𝑔𝑡)<𝜋6. The cosine similarity between rotations in AOS is computed as 𝑐𝑜𝑠(∇(𝑅,𝑅𝑔𝑡)).

We present the instance segmentation and pose estimation results in Table 7. Here, the metrics for camera poses are with respect to AP and AR, where the IoU thresholds range from 0.5 to 0.95. For instance segmentation, our baseline captures a mean AP of 0.55 on 3D-FUTURE. The score is at a similar level to those reported on the MSCOCO leaderboard achieved by recent SOTA methods. For 3D pose estimation, our baseline yields a mean AVP of 43%. Besides, as analyzed in Xiang et al. (2016), AP is an upper bound of AOS. This means the closer AOS is to AP, the more accurate the rotation estimation is. By showing the gaps between AOS and AP in Fig. 11, we can see that the estimated rotation (0.43) can be further improved. From the observations, we conclude that most objects’ 3D poses are not well modeled in our challenging setting. This suggests that researchers may need to carefully study 3D pose estimation with different levels of occlusions based on 3D-FUTURE. Some qualitative results are shown in Figs. 14 and 15 to further justify our conclusions.

Fig. 14
figure 14
Instance segmentation results. The images are captured under suggested viewpoint (by designer) for design exhibition. Zoom in for better view

Full size image
Fig. 15
figure 15
The pose estimation results. 3D-FUTURE encourages estimating pose parameters for multiple objects with occlusions in diverse indoor scenes that cannot be supported by previous 3D model repositories like ObjectNet3D (Xiang et al. 2016), PASCAL3D+ (Xiang et al. 2014), and Pix3D (Sun et al. 2018a). Zoom in for better view

Full size image
Table 7 Quantitative results of the Cascade-Mask R-CNN baseline for joint instance segmentation and 3D pose estimation
Full size table
Single-View 3D Object Reconstruction
Inferring 3D structure from a single image has been an active research area for a long time. In the supervised setting, traditional methods investigated shape from shading (Durou et al. 2008; Zhang et al. 1999) and defoce (Favaro and Soatto 2005) to reason the visible parts of objects. Leveraging on large-scale shape repositories, various works examined deep architectures to produce shapes in 3D volume (Choy et al. 2016), point cloud (Fan et al. 2017), and mesh surface (Groueix et al. 2018) directly. Recently, several SOTA methods recovered 3D meshes from initializations using shape deformation based on deep networks (Wang et al. 2018). In the unsupervised setting, 3D recovery has been recast as a 2D image reconstruction progress of unobserved views with differentiable rendering (Liu et al. 2019; Chen et al. 2019b).

In this paper, we examine several SOTA reconstruction algorithms as the baselines, including ONet (Mescheder et al. 2019), Pixel2Mesh (Wang et al. 2018), and DISN (Xu et al. 2019). We report the widely studied Intersection over Union (IoU), Chamfer Distance (CD), and F-score to evaluate these approaches on 3D-FUTURE. We refer (Xu et al. 2019) for the definitions of these metrics. We randomly render 24 different view images each model for training and a random view image for testing. The resolution of each image is 256×256. As shown in Table 8 and Fig. 16, Pixel2Mesh is more robust in general 3D object reconstruction and results a CD (×0.001) of 32.35 on 3D-FUTURE. However, these methods can obtain CD around 10.0 on ShapeNetCore (Chang et al. 2015). These results show that 3D object reconstruction on 3D-FUTURE is more challenging since its 3D models contain many geometric details. We also fine-tune the trained Pixel2Mesh here on the ShapeNetCore dataset. The resulted metrics are 62.17, 77.05, and 0.528 on IoU, CD, and F-score, respectively. The performance is about 11%, 7.5% and 18.8% better than the baseline model that is only trained on ShapeNetCore.

Fig. 16
figure 16
Sample reconstruction results on the 3D-FUTURE benchmark. The unsatisfied CD (>30) means the reconstructed meshes only roughly approximate the true surfaces of input images

Full size image
Table 8 Numerical comparison of our several baselines for single image 3D reconstruction on our 3D-FUTURE dataset
Full size table
Texture Synthesis For 3D Shapes
Unlike geometry reconstruction, texture reconstruction of 3D objects has received less attention from the community. Previous works studied the subject by learning colored 3D reconstruction on voxels or point clouds (Sun et al. 2018b; Tulsiani et al. 2017) based on view synthesis and multi-view geometry. While voxel representations are limited to the low resolutions, point representations are sparse and thus ignore geometric details. Recent approaches alternatively learned a 2D texture atlas (UV mapping) for 3D meshes to map a point on the shape manifold to a pixel in the texture atlas. These methods mainly take advantage of differentiable rendering to recast the problem as an unobserved view synthesis problem (Raj et al. 2019; Oechsle et al. 2019).

Existing 3D repositories contain less low-quality textures and cannot support high-quality texture recovery studies. In contrast, 3D-FUTURE provides furniture shapes with informative textures, which are used in modern industrial productions. We examine two baselines for texture synthesis, i.e., Texture Fileds (Oechsle et al. 2019) and a novel BicycleGAN++ method. Here, BicycleGAN++ extends BicycleGAN (Zhu et al. 2017) for texture synthesis. An illustration of the network is shown in Fig. 17. In specific, we incorporate a texture encoder such that the learned model can perform controllable texture synthesis. Importantly, by enlarging the weights of the reconstruction losses and introducing a texture consistency loss, we find that the produced multi-view textured images will show preferable consistency in overlap regions.

Fig. 17
figure 17
An illustration of our BicycleGAN++ baseline. It’s an extension of BicycleGAN (Zhu et al. 2017) for conditional image synthesis. The input are rendered images from 3D shapes. E: Texture Encoder. G: Generator

Full size image
We conduct experiments on four super-categories, including Sofa, Bed, Chair, and Table. The details of our train and test splits are reported in Table 9. We randomly render 32 views of images for each shape to enlarge the training set. For each baseline, we first train them on the whole train set and then perform category-specific fine-tuning. Following (Oechsle et al. 2019), we use structure similarity image metric (SSIM) (Wang et al. 2004), L1, Frechet Inception Distance (FID) (Heusel et al. 2017), and Feat1 as our metrics to evaluate the quality of the synthetic texture. Here, L1 is the L1 distance between the ground-truth view rendering and the produced textured image under the same viewpoint. Feat1 is a global perceptual measure operated on the Inception-net (Szegedy et al. 2015) feature space using the L1 distance. As shown in Table 10, while BicycleGAN++ earns higher scores on FID and Feat1, Texture Fields performs better in terms of SSIM and L1, indicating that BicycleGAN++ produces more realistic images with higher quality and Texture Fields focuses more on structured texture details. We also give some qualitative results in Fig. 18. We can see that BicylcGAN++ can only learn the main color information while largely ignores the semantic parts of objects. Texture Fields can partially preserve the structured texture details but produces dreamlike textures. These observations demonstrate that achieving visually appealing texture recovery for 3D meshes is still very challenging, especially for the modern 3D shapes with lots of texture details (Fig. 19).

Fig. 18
figure 18
A quantitative comparison between Texture Fields and BicycleGAN++ for conditional texture synthesis. Achieving visually appealing texture recovery for 3D meshes is still very challenging, especially for the modern 3D shapes with informative textures

Full size image
Table 9 The statistics of the training and test sets for the subject of texture synthesis for 3D shapes
Full size table
Table 10 Quantitative evaluation for texture synthesis
Full size table
Fig. 19
figure 19
Here we show two fill-in-the-blank examples in the 3D-FUTURE test set. The numbers are the compatibility scores obtained by FSC

Full size image
In addtion to the user studies presented in Sect. 4.2, here we conduct a quantitative test to further assess the texture quality of 3D-FUTURE. We train TM-Net (Gao et al. 2020) on both 3D-FUTURE and ShapeNet for the generation of chair textures. We use the test chairs in both datasets for evaluation. For each chair, we synthesize 10 textures based on the trained 3D-FUTURE TM-Net with 10 pre-generated noises. Then, we compute the LPIPS score for the chair over the texture crosses. For TM-Net trained on ShapeNet, we calculate the LPIPS score as the same manner with the same noises. The average LPIPS metrics are 0.215 (ShapeNet) and 0.289 (3D-FUTURE), respectively. Since LPIPS measures the diversity of the generated textures, the higher LPIPS on 3D-FUTURE shows that its shared textures hold richer information. The user studies also state that the Turkers preferred to select 3D-FUTURE (76.2%) when asking them which shape contains richer texture.

Fig. 20
figure 20
Top: Three rooms of a CAD house provided by Alibaba Topping Homestyler. Bottom: Putting the created furniture suites by FSC trained on 3D-FUTURE to the rooms

Full size image
Furnishing Composition
In Home Furnishings Industry, the agents have a great demand to put their furniture and decorations into different virtual rooms. They thus would desire that someone is always ready to efficiently create many room suites with their items. 3D-FUTURE enables the study of furnishing composition. It shares 20K furniture suits that are constructed by 10K modern furniture shapes. Before 3D-FUTURE, SC3DF (Liu et al. 2015) annotated 11,865 triplets using 359 furniture shapes to study pair-wise shape style compatibility. 3D-FUTURE surpasses SC3DF in four aspects: furniture suites vs. pairs, textures vs. no textures, detailed furniture attributes vs. no attributes, and model numbers (9993 vs. 359). Although the furniture suites are created by FSC, they has been reviewed by professional designers as aforementioned. The user study in Table 2 and Sect. 4.2 also promise their quality.

We study three baselines, including POG (Chen et al. 2019a), CAVCP (Cucurull et al. 2019), and FSC. For both POG and CAVCP, we use the instance contrastive learning (ICL) method (Wu et al. 2018) to learn the initial visual feature for each item. ICL has been pre-trained on ImageNet. For POG, the graphic embeddings are directly source from CAVCP, the TextCNN is removed. We adopt the 14,761/5479 suits for training and test, respectively. For evaluation, we examine the file-in-the-blank (FITB) and furniture suite compatibility tasks as Sect. 4.1. Given a test suite (A, B, C), we construct negative suits for A, B, and C, respectively. From Table 11. FSC yields the best scores on the two experiments. Some qualitative results are shown in Fig. 20.

Table 11 Furnishing composition on 3D-FUTURE
Full size table
Conclusion
In this paper, we have built the large-scale 3D-FUTURE benchmark specific to the household scenario with rich 3D and 2D annotations. 3D-FUTURE contains 9992 high-quality 3D CAD furniture shapes and 20,240 realistic synthetic images. The features include but are not limited to the photo-realistic renderings, 2D–3D alignments, and most significantly the industrial 3D furniture shapes with informative textures and different attributes. We conduct several experiments to show the special properties of 3D-FUTURE. The experiments can serve as baselines for future research using our database. We hope that 3D-FUTURE can facilitate innovative research on high-quality 3D shape understanding and generation, bring new research opportunities for 3D vision, and build a bridge between academic study and 3D industrial applications.