Face recognition has been significantly advanced by deep learning based methods. In all face recognition methods based on convolutional neural network (CNN), the convolutional kernels for feature extraction are fixed regardless of the input face once the training stage is finished. By contrast, we humans are usually impressed by some unique characteristics of different persons, such as oneâ€™s blue eyes while another oneâ€™s crooked nose, or even someoneâ€™s naevus at specific location. Inspired by this observation, we propose a personalized convolution method which aims to extract special distinguishing characteristics of each person for more accurate face recognition. Specifically, given a face, we adaptively generate a set of kernels for him/her, named by us ordinary kernel, which is further analytically decomposed into two orthogonal components, i.e., the commonality component and the specialty component. The former characterizes the commonality among subjects which is optimized on a reference set. The latter is the residual part by filtering out the commonality component from the ordinary kernel, so as to capture those special characteristics, named by us personalized kernel. The CNNs with personalized kernels for convolution can highlight those specialty of a personâ€™s distinguishing characteristics while suppress his/her commonality with others, leading to better distinguishing of different faces. Additionally, as a by-product, the reference set also facilitates the adaptation of our method to different scenarios by simply selecting faces of a particular population. Extensive experiments on the challenging LFW, IJB-A and IJB-C datasets validate that our proposed personalized convolution achieves significant improvement over the conventional CNN, and also other existing methods for face recognition.

Access provided by University of Auckland Library

Introduction
Face recognition aims to identify or verify the identity of a person by using his/her face. As an effective method of biometric authentication, it has been deployed in various venues, ranging from public and finance security, to Face ID on mobile phone and conference registration. Generally, there are two types of face recognition tasks: face identification and face verification. The former classifies a given face to a specific identity in the gallery, while the latter aims at determining whether a pair of faces are of the same identity. In either cases, discriminative representation of face images is the key to high-accuracy face recognition.

In recent years, the most successful face recognition technology employs the powerful convolutional neural network (CNN). This is because a deep CNN model can extract informative face features with stable invariance to complex appearance variations of oneâ€™s face, and high separability even between millions of faces, benefited from its excellent capability of modeling non-linearity. A few impressive studies include DeepFace (Taigman et al., 2014), Deep ID series (Chen et al., 2014; Sun et al., 2014, 2015a, b), FaceNet (Schroff et al., 2015), VGGFace (Parkhi et al., 2015), SphereFace (Liu et al., 2017), CosFace (Wang et al., 2018), ArcFace (Deng et al., 2018), UniformFace (Duan et al., 2019), RegularFace (Zhao et al., 2019) etc. In these CNN-based approaches, firstly one needs to train a CNN model on the training set, and then extract the deep features of testing faces with this model for face recognition. The parameters in CNN are fixed once training is finished, so all testing face images are processed with identical kernels.

Fig. 1
figure 1
Illustration that distinguishing characteristics differ from one person to another. For instance, one can easily notice Chris Hemsworthâ€™s blue eyes, while notice Stephen Fryâ€™s crooked nose relative to the reference set R

Full size image
By contrast, we humans are usually impressed by distinct characteristics of different persons, such as oneâ€™s blue eyes while another oneâ€™s crooked nose, as shown in Fig. 1. Inspired by the above observation about humanâ€™s face perception, this work proposes a personalized convolution method to extract the unique features of each person for better face recognition, in which personalized kernel is adaptively generated for each input face image to highlight his/her special distinguishing characteristics.

To achieve the above goal, for each individual, we first leverage a kernel generator to create a set of kernels for him/her, called by us ordinary kernel, which contains his/her compound information. To further filter out the common information and preserve the special information, the ordinary kernel is then decomposed into two orthogonal components, i.e. the commonality component and specialty component. The former (denoted as commonality kernel) depicts the commonality shared by all persons and is analytically computed with the aid of a reference set containing multiple face images as a proxy of all the persons. Then, the personalized kernel of the individual is obtained as the residual of orthogonal projection of his/her ordinary kernel on the commonality kernel. As a by-product, the proposed method can be used as a population-adaptive method by simply constructing the reference set with face images of a target population.

When there is only one face image in the reference set, the personalized kernel degenerates to a special case, i.e. the contrastive kernel proposed in our previous work (Han et al., 2018). Only in this sense, this work can be regarded as an extension of our conference version (Han et al., 2018). However, they are significantly different in three aspects. Firstly, the motivation is different. The conference version emphasizes the difference between two faces under comparison mainly for verification, while this paper emphasizes the distinctions of one face from the public faces. Secondly, the framework and formulation are different. This paper proposes a more general framework by introducing a reference set, while the conference version can be seen as a special variant of this framework. Moreover, the personalized kernel is mathematically optimized by decomposing the ordinary kernel into two orthogonal components, i.e. commonality component and specialty component, instead of the intuitive subtraction of the ordinary kernels of two faces in the conference version. Thirdly, as a by-product, the proposed method can be exploited as a population-adaptive method by simply constructing the reference set with face images of a target population. This paper achieves significant performance improvement over the conference version up to 5% on IJB-C with a 16-layer network, and also includes more experimental comparisons and analyses.

Briefly, the contributions of this work are in three folds.

Firstly different from existing methods, our personalized convolution can adaptively extract the special distinguishing features of an input face for adaptive and better face recognition.

Secondly a general framework decomposing oneâ€™s ordinary kernel into commonality component and specialty component is proposed to generate the personalized kernel for extracting oneâ€™s special distinguishing features. A reference set is introduced and serves as a strainer to filter out the commonality component from the ordinary kernel while leaving the specialty component as the personalized kernel. Different constitutions of the reference set derive distinct variants of this framework with potential for varying application scenarios.

Thirdly our method achieves quite impressive face recognition accuracy on the challenging LFW, IJB-A and IJB-C datasets, demonstrating the effectiveness of personalized feature extraction for face recognition.

Fig. 2
figure 2
The pipeline of our personalized CNN. Given an input face image x and a reference face set ğ‘…={ğ‘¥1,...,ğ‘¥ğ‘›}, a basic feature extractor E consisting of several cascaded convolutional layers is firstly used to obtain base feature representations ğ‘§ğ‘¥ and {ğ‘§1,...,ğ‘§ğ‘›} of them respectively. Then, the kernel generator G creates the ordinary kernels for the input face image and the reference face set respectively, based on which the commonality kernel ğ¾ğ‘Ÿ is optimized as the maximum commonality of ordinary kernels {ğ¾1,...,ğ¾ğ‘›} in the reference set, and the personalized kernel ğ¾ğ‘ğ‘¥ is achieved as the residual of orthogonal projection of oneâ€™s ordinary kernel ğ¾ğ‘¥ on the commonality kernel ğ¾ğ‘Ÿ. Finally, personalized features ğ‘“ğ‘¥ of input x are extracted by convolving ğ‘§ğ‘¥ with its personalized kernel ğ¾ğ‘ğ‘¥, with which similarities between different face images are calculated. Note that the CE Loss in the figure means cross entropy loss

Full size image
Related Work
In this section, we first briefly review the existing methods for face recognition, of which all adopt the same feature extraction for testing face images. Besides, we revisit some related studies in other areas which are different but also follow the adaptive modeling thought.

Face Recognition
Face recognition is an important and long-standing problem in computer vision, whose accuracy heavily depends on expressive facial feature representations. In the early years, work on face recognition mainly obtains feature representations from the raw image pixels through linear or local linear projection (Wang & Deng, 2018), such as linear subspace (Belhumeur et al., 1997; Moghaddam et al., 1998; Turk & Pentland, 1991), manifold learning (He et al., 2005; Yan et al., 2005), and sparse representation (Deng et al., 2012; Wright et al., 2009; Zhang et al., 2011). However, those methods struggle to handle face recognition with complex appearance variations. At the same time, hand-crafted local features instead of raw pixels, such as Gabor (Liu & Wechsler, 2002), Local Binary Pattern (LBP) (Ahonen et al., 2006), as well as their extensions (Tan & Triggs, 2010; Xie et al., 2010; Wenchao et al., 2005; Zhang et al., 2007), are proposed and achieve favorable results in controlled environment. Joint Bayesian (Chen et al., 2012) has further improved the accuracy by formulating face embedding as the addition of the identity and the intra-class variation, which shares similar thoughts to our ordinary kernel decomposition in high level. However, the goal and basis of decomposition are different. Joint Bayesian decomposes the face information as the addition of identity and intra-class variation aiming for better estimating the covariance matrix of intra-personal pairs and extra-personal pairs, while our personalized CNN further decomposes the identity information of a face into the common identity-component and personal identity-component in order for more distinguishing identity information. The discrimination ability of hand-crafted features heavily depends on the design principles which may limit the accuracy of face recognition. Going a step further, a few learning-based approaches are proposed to learn more informative but mostly shallow representations (Cao et al., 2010; Duan et al., 2018; Lei et al., 2014). These learned shallow representations are favorable for controlled scenarios, but still hardly model those complex facial appearance variations in uncontrolled scenarios.

In recent years, deep convolutional neural network (CNN) has greatly improved the accuracy of face recognition owing to its excellent modeling capability of non-linear variations. DeepFace (Taigman et al., 2014) is the first deep method for face recognition. In DeepFace, all faces are aligned to be frontal through a general 3D shape model, with which CNN is trained for feature extraction. DeepFace outperforms many conventional non-deep face recognition methods, and inspires numerous follow-ups. Afterwards, Deep ID series (Chen et al., 2014; Sun et al., 2014, 2015a, b) use both identification and verification supervision to guide the learning of both intermediate and final feature extraction layers to obtain more discriminative feature representations. They even surpass the humanâ€™s performance on the Labeled Face in the Wild (LFW) dataset (Huang & Learned-Miller, 2014). In FaceNet (Sankaranarayanan et al., 2016), triplet loss is employed to reduce/enlarge the distance between the positive/negative pair in a triplet on a large-scale face image set, achieving state-of-the-art results on multiple challenging benchmarks including LFW (Huang & Learned-Miller, 2014) and YouTubeFaces (Wolf et al., 2011) datasets.

Recent deep learning-based face recognition studies have made great progress from different aspects. Some studies focus on modifying the softmax loss function by explicitly adding a margin to the loss function. Among them, Large-Margin softmax (L-Softmax) loss (Liu et al., 2016) and SphereFace (Liu et al., 2017) propose multiplicative angular margin penalty to enforce extra intra-class compactness and inter-class discrepancy simultaneously. CosFace (Wang et al., 2018) and ArcFace (Deng et al., 2018) further add cosine margin and arc-cosine margin penalty to the target logit in Softmax loss respectively. UniformFace (Duan et al., 2019) further imposes an equidistributed constraint by uniformly spreading the class centers, so that the minimum distance between class centers can be maximized. Some other studies focus on improving the learning scheme. CurricularFace (Huang et al., 2020b) employs a curriculum learning loss to emphasize the easy samples in the early stage while the hard ones in the latter stage by adaptively adjusting the relative importance of easy and hard samples during different training stages. BroadFace (Kim et al., 2020b) proposes a learning process to comprehensively consider a massive set of identities in each iteration to increase the optimality of the classifier in the entire datasets, which can further globally optimize the encoder. The above studies have made great advances on face recognition accuracy.

Although the accuracy of face recognition on LFW and YTF datasets has reached human level, there still exists a gap between human performance and automatic face recognition with extreme pose, illumination, expression, age, or resolution variations in unconstrained environment, as reflected by the challenging IJB-A/B/C dataset. Therefore, some latest works focus on addressing face recognition robust to large face variations due to age, pose, expression and heavy occlusions, as exemplified in the following. Decorrelated Adversarial Learning (DAL) algorithm (Wang et al., 2019) proposes to factorize the mixed face feature into identity-dependent component and age-dependent component by adversarially minimizing the correlation between the two components. Age-Invariant Model (AIM) (Jian et al., 2019) unifies cross-age face synthesis and recognition in a mutual boosting way. To address the large change of the facial pose, expression, and illumination, Attentional Feature-pair Relation Network (AFRN) (Kang et al., 2019) represents a face by the relevant pairs of local appearance block features. To eliminate the influence of facial occlusion on face recognition, the study in Song et al. (2019) proposes a mask learning strategy to find and discard corrupted feature elements from recognition by exploiting the differences between the convolutional features of occluded and occlusion-free face pairs. GroupFace (Kim et al., 2020a) integrates group-aware representations into the embedding feature to narrow down the search space of the target identity. Although great progress has been made for face recognition, more accurate algorithms to address those complex scenarios are still in great demand.

Adaptive Convolution
Our work is also related to adaptive convolution, which has been employed for several problems in computer vision. In Chen et al. (2017), adaptive kernels are applied to image style transfer, where a given image is transformed from the original style to a target style through convolving the given image with kernels of the target style. In Zhang et al. (2017), scale-adaptive convolution is proposed to acquire flexible-size receptive fields for scene parsing to tackle the issue of inconsistent predictions of large objects and invisibility of small objects in conventional CNN. In Liao and Shao (2019), the authors employ adaptive convolution to achieve local matching for person re-identification, where the matching process and results become interpretable and generalizable. In Klein et al. (2015) and Jia et al. (2016), dynamically generated filters are used to predict the movement of pixels between frames for highway driving or weather image prediction. In Kang et al. (2017), the authors propose an adaptive convolutional neural network (ACNN), whose convolutional kernel is adapted to the current scene context via the side information (e.g. camera perspective, noise level, blur kernel parameters). The ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context, to address the supervised learning problem such as crowd counting, corrupted digit recognition, and image deblurring. The method in Bertinetto et al. (2016) proposes a second network, named Learnet, to predict parameters of a pupil network from a single exemplar, to handle the one-shot image classification task. CondInst (Tian et al., 2020) proposes to dynamically generate the filter parameters conditioned on the target instance in each mask head for effective instance segmentation.

All these methods explore the adaptive modeling schema. However, they are essentially different from ours. The main difference is the basis of creating kernels. The above mentioned adaptive kernels are generated only according to one input, in order for capturing the specific features of the input. Differently, our personalized kernel is created according to both the input and a reference set, where the reference set is used to filter out the commonality between distinct persons. Benefited from this design, those common features can be elaborately weakened and thus highlight oneâ€™s characteristics more precisely. Another difference is that they have distinct purposes, i.e. solving different problems. As is described above, adaptive convolution in those previous studies mainly focuses on image generation or image prediction, while our personalized kernel aims at distinguishing between subjects, thus leading to different designs of the convolution operation.

Personalized Convolution
Given a face image, our personalized convolution extracts its special features with personalized kernel aiming at better distinguishing him/her from others. The overall structure is presented in Fig. 2, and the proposed CNN is referenced to as personalized CNN in the following for convenience.

Our personalized CNN mainly consists of three parts: a convolutional backbone E to extract basic features for all subjects, a kernel generator G to generate ordinary kernel for each subject, a module of personalized kernel optimization to decompose the ordinary kernel into commonality component and specialty component. Finally personalized kernel is used to convolve with the basic features of the input face image to obtain personalized feature representation. The personalized kernel generation is formulated as an analytical optimization process and thus the whole personalized CNN is optimized with two simple cross entropy losses imposed on personalized features and the ordinary kernels respectively in end-to-end manner. After the training stage is finished, the similarity of two test face images is calculated as the cosine similarity of their personalized features. The three parts are detailed in the following.

Basic Feature Extracting
As is shown in Fig. 2, initially a CNN backbone E shared by all persons is used to extract basic features for all input images. This feature extractor E can be implemented as any kind of network architecture, such as several stacked convolutional layers or the popular ResNet-50 backbone. For any input image x, its basic features are simply extracted as:

ğ‘§ğ‘¥=ğ¸(ğ‘¥)âˆˆâ„ğ‘ğ‘§Ã—â„ğ‘§Ã—ğ‘¤ğ‘§,
(1)
where ğ‘ğ‘§, â„ğ‘§, and ğ‘¤ğ‘§ are the number of channel, height, and width of the output feature maps respectively.

Ordinary Kernel Generating
With the basic feature extractor, features of all persons are computed in an identical manner, i.e. using the same kernel for feature extraction. By contrast, we humans naturally focus on distinct characteristics of different persons so as to better distinguish them. Following the same thought, we design personalized kernel that is adaptive to each subject to highlight his/her special features to distinguish him/her from others more accurately.

A direct way is to introduce a kernel generator G to adaptively generate the convolutional kernel for each input subject. The kernel generator takes basic feature maps ğ‘§ğ‘¥ as input, and outputs a set of kernels as follows:

ğ¾ğ‘¥=ğº(ğ‘§ğ‘¥).
(2)
Although the kernel generated from Eq. (2) is adaptive to each person, it usually captures compound features, including not only those special distinguishing characteristics of the input face but also some common distinguishing characteristics among different faces. In this way, although ğ¾ğ‘¥ is adaptive, while those special information may be not notable especially compared to the following proposed personalized kernel, so in this work ğ¾ğ‘¥ is named as ordinary kernel. To further obtain the personalized kernel of each person, the common characteristics should be removed from the ordinary kernel, as presented in Sect. 3.3.

Generally, the kernel generator G can be designed as any kind of network architectures, such as convolutional layers, fully connected layers, or a mixture of them. However, the number of parameters of kernel generator in the form of those architectures will multiplicatively increase with the dimension of the input feature map ğ‘§ğ‘¥ and the output kernel ğ¾ğ‘¥. So, to control the complexity, the kernel generator in this work is designed as a two-layer architecture, in which the first layer is a standard convolutional layer and the second layer is a channel-wise fully-connected layer to avoid parameters explosion. Supposing ğ¾ğ‘¥âˆˆâ„ğ‘›ğ‘˜Ã—ğ‘ğ‘˜Ã—â„ğ‘˜Ã—ğ‘¤ğ‘˜ with ğ‘›ğ‘˜ kernels, ğ‘ğ‘˜ channels and filter size â„ğ‘˜Ã—ğ‘¤ğ‘˜, the kernel generator can be specifically formulated in the following:

ğ¾ğ‘¥=ğœ™(ğ‘Š2ğ‘”âŠ™ğœ™(ğ‘Š1ğ‘”âˆ—ğ‘§ğ‘¥+ğ‘1ğ‘”)+ğ‘2ğ‘”),
(3)
where ğ‘Š1ğ‘”, ğ‘1ğ‘”, and ğ‘Š2ğ‘”, ğ‘2ğ‘” are the parameters of kernel generator G in the first layer and second layer respectively. ğœ™ is the non-linear activation function, âˆ— means conventional convolution, and âŠ™ means channel-wise connection. In the channel-wise fully connected layer, the j-th ordinary kernel ğ¾ğ‘¥(ğ‘—)âˆˆâ„ğ‘ğ‘˜Ã—â„ğ‘˜Ã—ğ‘¤ğ‘˜ is generated from the j-th channel of ğœ™(ğ‘Š1ğ‘”âˆ—ğ‘§ğ‘¥+ğ‘1ğ‘”) with the fully connection. Such an architecture of the kernel generator better trades off the accuracy and the efficiency.

Personalized Kernel Generating
As mentioned in the introduction part, to make the kernel of a given person focus on his/her special distinguishing characteristics, those common characteristics should be filtered out from his/her ordinary kernel. Therefore, we consider decomposing the ordinary kernel ğ¾ğ‘¥ into two orthogonal components, i.e. the commonality component and the specialty component, in order to isolate those most special characteristics. This decomposition is formulated as follows.

ğ¾ğ‘¥=ğ‘ğ‘¥ğ¾ğ‘Ÿ+ğ¾ğ‘ğ‘¥,with ||ğ¾ğ‘Ÿ||=1,ğ¾ğ‘ŸâŠ¥ğ¾ğ‘ğ‘¥.
(4)
Here, ğ¾ğ‘Ÿ denotes the commonality kernel that captures the common distinguishing characteristics among a group of subjects. ğ¾ğ‘ğ‘¥ is orthogonal to the commonality kernel ğ¾ğ‘Ÿ, so it captures those uncommon i.e. the special characteristics of x. ğ‘ğ‘¥âˆˆâ„=(ğ¾ğ‘¥)ğ‘‡ğ¾ğ‘Ÿ is the magnitude of orthogonal projection of ğ¾ğ‘¥ on ğ¾ğ‘Ÿ, i.e. the magnitude of commonality.

Given the ordinary kernel ğ¾ğ‘¥ and commonality kernel ğ¾ğ‘Ÿ, the personalized kernel can be obtained by removing the projection on ğ¾ğ‘Ÿ from ğ¾ğ‘¥, i.e.:

ğ¾ğ‘ğ‘¥=ğ¾ğ‘¥âˆ’ğ‘ğ‘¥ğ¾ğ‘Ÿ=ğ¾ğ‘¥âˆ’ğ¾ğ‘Ÿ(ğ¾ğ‘¥)ğ‘‡ğ¾ğ‘Ÿ
(5)
Therefore, the key becomes how to model the commonality kernel ğ¾ğ‘Ÿ. By its name, ğ¾ğ‘Ÿ captures the common distinguishing characteristics shared by a group of people. In order to optimize ğ¾ğ‘Ÿ, a reference set containing multiple images is introduced as the proxy of persons in a specific scenario, such as persons appearing in a movie or living in a community or just persons in the same dataset.

Optimization of Commonality Kernel
Formally, we denote the reference set with n face images as ğ‘…={ğ‘¥1,â‹¯,ğ‘¥ğ‘›}, where ğ‘¥ğ‘– means the i-th face image in the reference face set. The basic features and ordinary kernels of face images in the reference set are computed as below:

ğ‘§ğ‘–=ğ¸(ğ‘¥ğ‘–)
(6)
ğ¾ğ‘–=ğº(ğ‘§ğ‘–).
(7)
With the ordinary kernels {ğ¾ğ‘–âˆˆâ„ğ‘›ğ‘˜Ã—ğ‘ğ‘˜Ã—â„ğ‘˜Ã—ğ‘¤ğ‘˜|ğ‘›ğ‘–=1} of the reference set, we explicitly model the commonality kernel ğ¾ğ‘Ÿ as the maximum commonality between them. For the convenience of notation, we take a 3D kernel in â„ğ‘ğ‘˜Ã—â„ğ‘˜Ã—ğ‘¤ğ‘˜ from ğ¾ğ‘– as example for presentation, which is first reshaped into a vector in â„ğ¿Ã—1 and still denoted as ğ¾ğ‘– in the following. Here, ğ¿=ğ‘ğ‘˜Ã—â„ğ‘˜Ã—ğ‘¤ğ‘˜. Similar as Eq. (4), each ğ¾ğ‘– is decomposed to two orthogonal components, the commonality component ğ‘ğ‘–ğ¾ğ‘Ÿ and the specialty component ğ‘…ğ‘– as below:

ğ¾ğ‘–=ğ‘ğ‘–ğ¾ğ‘Ÿ+ğ‘…ğ‘–with ||ğ¾ğ‘Ÿ||=1,ğ¾ğ‘ŸâŠ¥ğ‘…ğ‘–,
(8)
where ğ¾ğ‘Ÿâˆˆâ„ğ¿Ã—1 represents the commonality kernel of kernels {ğ¾ğ‘–|ğ‘›ğ‘–=1}, and ğ‘ğ‘–âˆˆâ„ is the projection magnitude of ğ¾ğ‘– on ğ¾ğ‘Ÿ with ğ‘ğ‘–=(ğ¾ğ‘–)ğ‘‡ğ¾ğ‘Ÿ.

As the commonality kernel ğ¾ğ‘Ÿ represents the commonality between kernels {ğ¾ğ‘–|ğ‘›ğ‘–=1}, the projection magnitude of each kernel ğ¾ğ‘– on ğ¾ğ‘Ÿ is expected to be identical, i.e. the variance of {ğ‘ğ‘–|ğ‘›ğ‘–=1} is expected to be zero. Only with the variance constraint, the commonality kernel ğ¾ğ‘Ÿ may degenerate to trivial solution, e.g. ğ‘ğ‘–=0,ğ‘–âˆˆ{1,...,ğ‘›} in extreme case. Thus, the projection magnitude ğ‘ğ‘– is expected to be as large as possible to make ğ¾ğ‘Ÿ contain the maximum commonality. Therefore, an objective is formulated to fulfill these expectations:

minğ¾ğ‘Ÿ&ğ›¼â‹…1ğ‘›âˆ‘ğ‘–(ğ‘ğ‘–âˆ’ğ‘â¯â¯â¯)2âˆ’(1âˆ’ğ›¼)â‹…1ğ‘›âˆ‘ğ‘–(ğ‘ğ‘–)2==ğ›¼ğ‘›âˆ‘ğ‘–((ğ¾ğ‘–)ğ‘‡ğ¾ğ‘Ÿâˆ’ğ¾â¯â¯â¯â¯â¯ğ‘‡ğ¾ğ‘Ÿ)2âˆ’1âˆ’ğ›¼ğ‘›âˆ‘ğ‘–((ğ¾ğ‘–)ğ‘‡ğ¾ğ‘Ÿ)2ğ›¼ğ‘›||ğ¾ğ‘‡ğ¾ğ‘Ÿâˆ’ğŸ™ğ¾â¯â¯â¯â¯â¯ğ‘‡ğ¾ğ‘Ÿ||22âˆ’1âˆ’ğ›¼ğ‘›||ğ¾ğ‘‡ğ¾ğ‘Ÿ||22
(9)
where (ğ¾ğ‘Ÿ)ğ‘‡ğ¾ğ‘Ÿ=1, ğ‘â¯â¯â¯ and ğ¾â¯â¯â¯â¯â¯ are the mean of {ğ‘1,...,ğ‘ğ‘›} and {ğ¾1,...,ğ¾ğ‘›} with ğ‘â¯â¯â¯=1ğ‘›âˆ‘ğ‘–ğ‘ğ‘– and ğ¾â¯â¯â¯â¯â¯=1ğ‘›âˆ‘ğ‘–ğ¾ğ‘– respectively, K is the matrix concatenating {ğ¾ğ‘–|ğ‘›ğ‘–=1} by column, and ğŸ™ is a all-one matrix with shape ğ‘›Ã—1. In the Eq. (9), the first term enforces the projection of the ordinary kernel ğ¾ğ‘– on the commonality kernel ğ¾ğ‘Ÿ to be identical by minimizing their variance, and the second term aims to maximize the magnitude, i.e. make ğ¾ğ‘Ÿ contain commonality as much as possible. The parameter ğ›¼âˆˆ[0,1] is to balance the commonality and the magnitude terms as they may contradict each other in complex applications.

This objective can be analytically optimized with the Lagrange multiplier method. First, considering the constraint condition (ğ¾ğ‘Ÿ)ğ‘‡ğ¾ğ‘Ÿ=1, in Lagrange multiplier method, the objective in Eq. (9) can be written as:

=ğ½=  2ğ›¼âˆ’1ğ‘›(ğ¾ğ‘Ÿ)âŠ¤ğ¾ğ¾âŠ¤ğ¾ğ‘Ÿâˆ’ğ›¼ğ‘›2ğ¾âŠ¤ğ‘Ÿğ¾ğŸ™ğŸ™âŠ¤ğ¾âŠ¤ğ¾ğ‘Ÿ  ğ›¼ğ‘›||ğ¾âŠ¤ğ¾ğ‘Ÿâˆ’ğŸ™ğ¾Â¯âŠ¤ğ¾ğ‘Ÿ||22âˆ’1âˆ’ğ›¼ğ‘›||ğ¾âŠ¤ğ¾ğ‘Ÿ||22  âˆ’ğœ‡((ğ¾ğ‘Ÿ)âŠ¤ğ¾ğ‘Ÿâˆ’1)  âˆ’ğœ‡((ğ¾ğ‘Ÿ)âŠ¤ğ¾ğ‘Ÿâˆ’1),
where ğœ‡ is the multiplier. The derivative of J on ğ¾ğ‘Ÿ is:

âˆ‚ğ½âˆ‚ğ¾ğ‘Ÿ=2(2ğ›¼âˆ’1)ğ‘›ğ¾ğ¾ğ‘‡ğ¾ğ‘Ÿâˆ’2ğ›¼ğ‘›2ğ¾ğŸ™ğŸ™ğ‘‡ğ¾ğ‘‡ğ¾ğ‘Ÿâˆ’2ğœ‡ğ¾ğ‘Ÿ.
Let the derivative be equal to 0. The equation can be written as:

(2ğ›¼âˆ’1ğ‘›ğ¾ğ¾ğ‘‡âˆ’ğ›¼ğ‘›2ğ¾ğŸ™ğŸ™ğ‘‡ğ¾ğ‘‡)ğ¾ğ‘Ÿ=ğœ‡ğ¾ğ‘Ÿ.
(10)
Thus, the objective of Eq. (9) achieves its minimal value as the least eigenvalue of matrix 2ğ›¼âˆ’1ğ‘›ğ¾ğ¾ğ‘‡âˆ’ğ›¼ğ‘›2ğ¾ğŸ™ğŸ™ğ‘‡ğ¾ğ‘‡ when ğ¾ğ‘Ÿ is taken as the corresponding eigenvector, i.e.:

ğ¾ğ‘Ÿ=LeastEigVec(2ğ›¼âˆ’1ğ‘›ğ¾ğ¾ğ‘‡âˆ’ğ›¼ğ‘›2ğ¾ğŸ™ğŸ™ğ‘‡ğ¾ğ‘‡).
(11)
In special, when ğ›¼=0.5, i.e. taking the two terms equally important, ğ¾ğ‘Ÿ degenerates into the normalized mean kernel of {ğ¾ğ‘–|ğ‘›ğ‘–=1}. When ğ›¼=1.0, i.e. only keeping the commonality term, ğ¾ğ‘Ÿ is the kernel lying in the null-space of the scatter matrix of {ğ¾ğ‘–} if 1ğ‘›ğ¾ğ¾ğ‘‡âˆ’1ğ‘›2ğ¾ğŸ™ğŸ™ğ‘‡ğ¾ğ‘‡ is singular, i.e.

ğ¾ğ‘Ÿ=â§â©â¨âªâªğ¾â¯â¯â¯â¯â¯||ğ¾â¯â¯â¯â¯â¯||,  if  ğ›¼=0.5,Null(1ğ‘›ğ¾ğ¾ğ‘‡âˆ’1ğ‘›2ğ¾ğŸ™ğŸ™ğ‘‡ğ¾ğ‘‡), if ğ›¼=1.0,
(12)
where Null(â‹…) means the null-space of a matrix.

After obtaining the commonality kernel ğ¾ğ‘Ÿ, the personalized kernel of the given face x is achieved as the residual of his/her ordinary kernel ğ¾ğ‘¥ on ğ¾ğ‘Ÿ, formulated in Eq. (5). The personalized kernel is then used to convolve with the basic features of the given face image x to finally obtain the corresponding personalized feature representation as follow:

ğ‘“ğ‘¥=ğ¾ğ‘ğ‘¥âˆ—ğ‘§ğ‘¥,
(13)
where âˆ— means the standard convolution operation.

The whole architecture to calculate the personalized features ğ‘“ğ‘¥ is optimized with two types of cross entropy loss detailed in Sect. 3.4.

After the training is finished, given any two face images ğ‘¥1 and ğ‘¥2 at testing stage, their features are firstly extracted according to Eq. (13), and then the similarity of them is calculated as the cosine similarity as follows:

ğ‘ (ğ‘¥1,ğ‘¥2)=(ğ‘“ğ‘¥1â€–ğ‘“ğ‘¥1â€–)ğ‘‡â‹…(ğ‘“ğ‘¥2â€–ğ‘“ğ‘¥2â€–).
(14)
Fig. 3
figure 3
Variants of the personalized kernel with different reference sets. a Reference set with images of abundant persons to represent the public. b Reference set with a special group of persons (children in the figure). c Reference set only with only one face image that the input image is compared with

Full size image
Objective Function
By using Eq. (1), Eq. (5), and Eq. (13), the personalized feature of an input face image is calculated as ğ‘“ğ‘¥. For face recognition, the feature ğ‘“ğ‘¥ is expected to distinguish x from others. So a cross entropy loss for identity classification is necessary to enforce ğ‘“ğ‘¥ to be discriminative. Besides, another cross entropy loss is introduced to enforce the ordinary kernels generated from different images of the same subject to be identical. This can make the ordinary kernel and personalized kernel well capture the distinguishing characteristics (i.e. identity information) of a person.

Given a face image x with label l, its personalized features ğ‘“ğ‘¥ are expected to be classified into the l-th class like the conventional CNN does. Supposing ğ‘Šğ‘“ is the parameter of classifier, and each column of ğ‘Šğ‘“ is the classifying vector for a class, the probability ğ‘ğ‘™ğ‘¥ that personalized feature ğ‘“ğ‘¥ is classified into class l can be calculated by the softmax function:

ğ‘ğ‘™ğ‘¥=ğ‘’ğ‘¢ğ‘™âˆ‘ğ¶ğ‘—=1ğ‘’ğ‘¢ğ‘—,
(15)
ğ‘¢=(ğ‘¢1,ğ‘¢2,...,ğ‘¢ğ¶)=ğ›½(ğ‘“ğ‘¥)ğ‘‡ğ‘Šğ‘“,
(16)
where C is the number of subjects in the training set. Here, the length of ğ‘“ğ‘¥ and each column vector in ğ‘Šğ‘“ is normalized. To make the training converge, we multiply a scale value ğ›½ on the cosine similarity (ğ‘“ğ‘¥)ğ‘‡ğ‘Šğ‘“ like (Wang et al., 2018) does. The classification loss in our method is the typical cross entropy loss:

ğ¿ğ‘“=1ğ‘âˆ‘ğ‘–=1ğ‘âˆ’ğ‘™ğ‘œğ‘”(ğ‘ğ‘™ğ‘–ğ‘¥ğ‘–),
(17)
where N is the number of images in each training batch, ğ‘¥ğ‘– and ğ‘™ğ‘– are the i-th image and its label in a batch respectively.

Moreover, those ordinary kernels ğ¾ğ‘¥ created by kernel generator G are expected to capture the intrinsic characteristics of a person, which means that ordinary kernels of face images from the same person should be the same even with various poses, illuminations or expressions, forming another cross entropy loss over the ordinary kernels as follows:

ğ¿ğ¾=1ğ‘âˆ‘ğ‘–=1ğ‘âˆ’ğ‘™ğ‘œğ‘”(ğ‘ğ‘™ğ‘–ğ¾ğ‘¥ğ‘–)
(18)
ğ‘£=1ğ‘âˆ‘ğ‘–=1ğ‘âˆ’ğ‘™ğ‘œğ‘”ğ‘’ğ‘£ğ‘™ğ‘–âˆ‘ğ¶ğ‘—=1ğ‘’ğ‘£ğ‘—,=(ğ‘£1,ğ‘£2,...,ğ‘£ğ¶)=(ğ¾ğ‘¥ğ‘–)ğ‘‡ğ‘Šğ‘˜.
(19)
Here, ğ¾ğ‘¥ğ‘– is the ordinary kernel of ğ‘¥ğ‘– generated by using Eq. (2), and ğ‘ğ‘™ğ‘–ğ¾ğ‘¥ğ‘– denotes the probability of kernel ğ¾ğ‘¥ğ‘– being classified into class ğ‘™ğ‘–. ğ‘Šğ‘˜ is the classification weight for those generated kernels. Generally, one more layer can be added to decrease the dimension of kernels ğ¾ğ‘¥ğ‘– before calculating the probability ğ‘ğ‘™ğ‘–ğ¾ğ‘¥ğ‘–.

Overall, the objective function of our personalized CNN is formulated as simply combining the above two losses as below:

minğ¸,ğºğ¿ğ‘“+ğœ†ğ¿ğ¾.
(20)
Here, ğœ† is a balance parameter. The first term of the objective function aims to make those personalized features be discriminative, while the second term endeavors to make ordinary kernels be intra-class invariant. This objective can be easily optimized in an end-to-end manner by using stochastic gradient decent (SGD) like most CNN-based methods do.

Variants
In our proposed personalized convolution, the reference face set is introduced as a proxy of persons in a specific scenario, which can filter out the commonality while leaving the specialty. Therefore, to adapt to different scenarios, the reference set can be constructed by including different faces, deriving different variants for better capturing oneâ€™s distinguishing features in different scenarios.

As is shown in Fig. 3 (a), generally, the reference set can consist of images of many general persons so that it can represent the commonality of general public. However, in some scenarios we may only want to recognize persons from a specific group, such as recognizing children in primary school, or recognizing girls in girlâ€™s apartment. In these cases, the commonality of the specific group to recognize is usually different from that of general public. To stress a child or a girlâ€™s discriminative features, the reference set should be set as a collection of children or girlsâ€™ images. Thus, the reference set can be specially constructed as a particular group of face images for better distinguishing different subjects for various scenarios, as shown in Fig. 3 (b).

More specially, the reference set can contain only one face image. This usually happens in the case of face verification. For example, when we compare Audrey Hepburn with RenÃ©e Zellweger, we focus more on Audrey Hepburnâ€™s eyes, while when we compare Audrey Hepburn with Chrissy Metz, we may rather pay attention to the contour of Audrey Hepburnâ€™s face. This means that, the compared person, RenÃ©e Zellweger or Chrissy Metz, is used as the â€˜reference systemâ€™ instead of the general public or a specific group. In this case, the proposed personalized kernel becomes a special variant, i.e. the contrastive kernel proposed in our conference work (Han et al., 2018) to highlight those distinct characteristics between a pair of face images being compared.

Overall, when aiming for different scenarios, the reference set can be constructed as faces of different subjects, deriving distinct variants of the personalized kernel.

Table 1 Architectures of the CNN used in our method with 4, 10, 16 layers respectively. There are 4 stages in each CNN, and the shape of feature map is the same in each stage. Each stage contains multiple convolution units. conv[c, k, s, g] denotes convolution with c filters of size ğ‘˜Ã—ğ‘˜, stride s, and group g. The max[3, 2] denotes the max pooling within pooling window size 3 Ã— 3 and stride 2. In CNNs with 10 and 16 layers, the residual network structure is used for better performance and the residual units are shown in the double-column brackets. In the last layer of personalized convolution, the convolution is the same as conventional convolution except that its kernels are dynamically generated during testing
Full size table
Experiments
In this section, we evaluate the proposed personalized CNN for both face identification and verification tasks. We first give the experimental settings, and also the implementation details. Then, we develop ablation study to analyze the proposed personalized kernel via comparing our method with baselines, and evaluate different variants of reference set. Next, we study the influence of hyper-parameter ğ›¼ and ğœ† respectively. Last, we compare our method with some existing methods on LFW, IJB-A and IJB-C, and visualize feature response of vanilla CNN and our personalized CNN.

Experimental Settings
Dataset
In all experiments, five datasets are used for training or testing. The CASIA-WebFace (Yi et al., 2014), and VGGFace2 (Cao et al., 2018) datasets are used for training, and the LFW (Huang et al., 2008), IJB-A (Klare et al., 2015), and IJB-C (Maze et al., 2018) datasets are used for testing.

The CASIA-WebFace dataset (Yi et al., 2014) is a large scale face dataset containing about 500,000 face images of about 10,000 different individuals collected from Internet by Institute of Automation, Chinese Academy of Sciences (CASIA). It is the default training dataset if unspecified. To fairly compare with existing methods, we also develop experiments on VGGFace2 dataset. The VGGFace2 dataset (Cao et al., 2018) consists of 3.3 million faces from 9,131 subjects, which covers a large range of poses, ages, ethnicities and professions. The two datasets are commonly used datasets to train a deep network for unconstrained face recognition, such as in Yi et al. (2014), Ding and Tao (2015), Liu et al. (2016), Liu et al. (2017), Weidi et al. (2018), Huang et al. (2020a).

During training on CASIA-WebFace, the general Reference Set R in our method is constructed with 46K images of 1000 individuals randomly selected from CASIA-WebFace. When training on VGGFace2, 169K images of 500 individuals are randomly selected from this dataset as the reference set R. In the stage of network optimizing, 20 images are randomly chosen from the general reference set as the batch reference images in each iteration to compute the commonality kernel ğ¾ğ‘Ÿ in Eq. (9). When the network optimizing is finished, the commonality kernel ğ¾ğ‘Ÿ can be simply calculated over the entire reference set before seeing any testing image, and then used to extract features of testing images at test time. This strategy offers us a good way to calculate the commonality kernel ğ¾ğ‘Ÿ when a new or more elaborated reference set is available. However, it will cause additional computational overhead. Thus, the moving average strategy is further proposed to avoid the computational overhead, where the statistics including scatter matrix ğ¾ğ¾ğ‘‡ and mean kernel ğ¾ğŸ™ in Eq. (11) of the reference set are updated on each mini-batch in the moving average way during network optimizing. Therefore, the commonality kernel is well calculated when the network optimizing is finished and can be directly used in the testing phase. Note that class labels of faces in the reference set is not needed, which means the reference set can be generally developed by using an additional unlabeled set.

The LFW dataset (Huang et al., 2008) is a standard benchmark for the research of unconstrained face recognition. It includes 13,233 face images from 5,749 different identities with large variations in pose, expression and illumination. On this dataset, we follow the standard evaluation protocol of â€˜unrestricted with labeled outside dataâ€™, and report the mean verification accuracy (mAcc) of 10-folder verification sets.

The IJB-A dataset (Klare et al., 2015) is an open challenging benchmark containing 5,712 images and 2,085 videos from 500 subjects captured from the wild environment. Because of the extreme variation in head pose, illumination, expression and resolution, so far IJB-A is regarded as one of the most challenging datasets for both verification and identification. The standard protocol on this dataset performs evaluations in the template-based manner, for both identification(1:N face search) and verification (1:1 comparison). A template may include images and/or videos of a subject.

The IJB-C face dataset (Maze et al., 2018) advances the goal of robust unconstrained face recognition by increasing dataset size and variability, which is developed upon the previous public IJB-A and IJB-B (Whitelam et al., 2017) datasets. It includes 31,334 images and 11,779 videos from 3,531 subjects with variations in head orientation, facial expression, illumination, and also occlusion and reduced resolution. The protocol of this dataset also operates in template-based manner.

Implementation Details
Preprocessing For all five datasets, MTCNN (Zhang et al., 2016) is first used to detect faces and landmarks. Then all detected faces are aligned to a canonical one according to the five landmarks (2 eyes centers, 1 nose tip, and 2 mouth corners). Finally, all aligned images are resized into 112Ã—112 for training and testing. Note that, for those undetectable faces on IJB-A and IJB-C, bounding boxes offered by corresponding dataset are used to crop them, and the mean of landmarks of all detected faces in corresponding dataset is used to align them. This preprocessing is the same as most existing work such as Yin et al. (2017), Liu et al. (2017), Wang et al. (2018), Deng et al. (2018).

Architecture All the experiments are implemented with pytorch. For extensive investigation, the proposed personalized CNN with three backbone architectures of 4-layer, 10-layer, and 16-layer are evaluated respectively. Those backbone structures are similar with those shallower networks used in SphereFace (Liu et al., 2017). The detailed architectures of the three backbones are listed in Table 1. The structure of kernel generator is the same for all the three backbone architectures, which is simply designed as a small 2-layer subnetwork, and the detailed design is introduced in Sect. 3.2. Note that, in testing stage, kernels of the last convolutional layer of our personalized CNNs are dynamically generated by the kernel generator learned in the training stage.

Training Parameters When training on CASIA-WebFace dataset (Yi et al., 2014), the batch size (doesnâ€™t include reference images) is set as 128. All models are trained with 100K iterations. The learning rate is set as 0.2 at the beginning, and decays as 0.02 at the 50k-th iteration, and as 0.002 at the 80k-th iteration. For experiments on VGGFace2 dataset (Cao et al., 2018), the batch size is set as 512, the learning rate is divided by 10 at 80K-th, 120K-th, and 150K-th iteration respectively starting with 0.2, and the training process is finished at 160K iterations. Random flapping and cropping face images are employed in the training stage for data augmentation as most existing methods do. In our implementation, the size of generated ordinary kernel is 512Ã—1Ã—3Ã—3. Thus, the personalized convolution is conducted as the group convolution with 1 channel in each group.

Evaluation Details: For both identification and verification, the evaluation involves similarity computation and evaluation protocols. Thus, we introduce the evaluation details from the two aspects in the following.

Table 2 Comparison with L-vanilla CNN, self-attention CNN, and contrastive CNN on LFW with three different backbones. The architecture is detailed in Table 1. â€ denotes directly calculating the commonality kernel over the reference set, and â€¡denotes calculating it with the moving average strategy during network optimizing.
Full size table
The similarity computation includes two levels, i.e., image-level similarity on LFW, and template-level similarity on IJB-A and IJB-C introduced as follows.

The image-level similarity means cosine similarity of features extracted by a CNN of two face images like Eq. (14) in the paper.

Following (Masi et al., 2016b), the SoftMax operator is used to calculate the similarity between two templates. Specifically, for two templates îˆ¼, and îˆ½, the SoftMax operator for calculating their similarity can be seen as a weighted average of similarities of all pairs (p, q) with ğ‘âˆˆîˆ¼,ğ‘âˆˆîˆ½. The weight for the weighted average is calculated as ğ‘’ğ›½ğ‘ (ğ‘,ğ‘)âˆ‘ğ‘âˆˆîˆ¼,ğ‘âˆˆîˆ½ğ‘’ğ›½ğ‘ (ğ‘,ğ‘) for a pair (p, q), where s(p, q) means the image-level similarity. The SoftMax hyper-parameter ğ›½ controls the trade-off between averaging the similarity or taking the maximum (or minimum). That is:

ğ‘ ğ›½(â‹…,â‹…)=â§â©â¨âªâªmax(â‹…),    ifğ›½â†’âˆmean(â‹…),   ifğ›½=0min(â‹…),    ifğ›½â†’âˆ’âˆ,
(21)
and

ğ‘ ğ›½(îˆ¼,îˆ½)=âˆ‘ğ‘âˆˆîˆ¼,ğ‘âˆˆîˆ½ğ‘ (ğ‘,ğ‘)ğ‘’ğ›½ğ‘ (ğ‘,ğ‘)âˆ‘ğ‘âˆˆîˆ¼,ğ‘âˆˆîˆ½ğ‘’ğ›½ğ‘ (ğ‘,ğ‘),
(22)
where p and q are images in îˆ¼ and îˆ½ respectively. The final similarity between two templates is the average of SoftMax response over multiple values of ğ›½âˆˆ{0,1,...,20} as follow:

ğ‘†(îˆ¼,îˆ½)=121âˆ‘20ğ›½=0ğ‘ ğ›½(îˆ¼,îˆ½).
(23)
After the similarity of two face images or two templates is computed, mean accuracy (mAcc) or receiver operating characteristic (ROC) is used for face verification, and Rank-1/Rank- 5 recognition rate is used for face identification for performance evaluation following the standard evaluation protocols. Detailed introductions are as follows.

The mean accuracy (mAcc) is used for the evaluation of face verification on LFW. Specifically, all verification pairs on this dataset are split into 10 folds, and the mAcc is calculated as the average percentage of accurate verification on 10 separate folds in a leave-one-out cross verification scheme, i.e. nine of the folds are used to search an optimal threshold and the left one is used for testing.

The receiver operating characteristic (ROC) is used for the evaluation of face verification on IJB-A and IJB-C. At a given threshold (the independent variable) ROC analysis measures the true accept rate (TAR), which is the fraction of genuine comparisons that correctly exceed the threshold, and the false accept rate (FAR), which is the fraction of impostor comparisons that incorrectly exceed the threshold.

Rank-k accuracy is used for the evaluation of face identification on IJB-A. Specifically, rank-k accuracy means the percentage of correctly recognized probe templates where a probe template is considered to be recognized correctly if the gallery template with the same identity appears in the top k gallery templates ranked according to their similarity.

Ablation Study
Fig. 4
figure 4
Evaluation on the same testing set but with different reference sets on IJB-A, in terms of verification rate at FAR=1e-2. a Testing set with age in [50, max], while the reference sets are with ages in [0, max], [0, 35], [35, 50], [50, max] respectively. b Testing set with skin tone in {5, 6}, while the reference sets are with skin tone in {1, 2, 3, 4, 5, 6}, {1, 2},{3, 4},{5, 6} respectively

Full size image
Table 3 Comparison with L-vanilla CNN, self-attention CNN, and contrastive CNN on IJB-A with three different backbones. The architecture is detailed in Table 1
Full size table
Table 4 Comparison with L-vanilla CNN, self-attention CNN and contrastive CNN on IJB-C with three different backbones. The architecture is detailed in Table 1
Full size table
Effectiveness of personalized kernel To investigate the effectiveness of our proposed personalized kernel, we compare our personalized CNN with the vanilla CNN, the self-attention CNN, and the contrastive CNN with the same backbone architecture trained on CASIA-WebFace. As our personalized kernel is generated from an additional kernel generator with two layers, two additional layers are added to the vanilla CNN (referred to as L-Vanilla CNN), so that they have the same number of parameters for fair comparison. The self-attention CNN is constructed by removing the reference set and using an attention map generator with the same structure with our kernel generator to generate the attention map to multiply the original feature ğ‘§ğ‘¥ for an input face x. The self-attention CNN is designed to evaluate the influence of removing reference set. Moreover, We also retrain our contrastive CNN proposed in the conference version (Han et al., 2018) with the same setting for fair comparison. For comprehensive investigation, the comparison is performed with three types of backbone shown in Table 1 on three testing sets.

The results on LFW, IJB-A and IJB-C are shown on Table 2, 3, and 4. As can be seen, contrastive CNN, self-attention CNN, and our personalized CNN consistently outperform the vanilla CNN. Especially, on the verification task, our personalized CNN with 16-layer backbone improves the vanilla CNN over 9% at FAR=1e-3 on IJB-A, and over 20% at FAR=1e-5 on IJB-C, which safely shows the effectiveness of personalized feature extraction. Moreover, compared to the self-attention CNN, and the Contrastive CNN which has no reference set or uses single image as the reference, our personalized CNN with a general reference set achieves improvement up to 4% at FAR=1e-3 on IJB-A, and 6% at FAR=1e-5 on IJB-C. These improvements demonstrate that removing the commonality can better highlight oneâ€™s special characteristics for higher recognition accuracy, verifying the effectiveness of the personalized convolution. Table 2, 3, and 4 also compare the two ways of calculating the commonality kernel, where the accuracy of using moving average strategy for calculating the commonality kernel during network optimizing is comparable to that of directly calculating over the reference set. This indicates that we can calculate the commonality kernel with the moving-average strategy when the general reference set is used, or directly calculate it over the reference set when more elaborated reference set is available.

Table 5 Influence of the balance parameter ğ›¼ in our personalized CNN on LFW, IJB-A, and IJB-C
Full size table
To better understand and analyze the sources of the improvement of our method, we also evaluate the same testing set but with different reference sets organized according to the attributes of age and skin tone on IJB-A dataset. Specifically, in the first evaluation, the testing set only contains images with age in [50, max], while four reference sets are organized with ages respectively in [0,max], [0, 35], [35, 50], [50, max]. The results are shown in Fig. 4 (a). As can be seen, when reference set is similar with the testing set, i.e. both with age in [50, max], best accuracy is achieved with significant improvement over the baseline. On the contrary, when the reference set is not so similar with the testing set, such as reference set with age in [0, 35], the performance improvement is limited. Even in the worst case that the reference set (e.g. the pupils) is completely irrelevant to the testing scenario (e.g. the elders), personalized kernel just degenerates to a kind of self-attention CNN (better than vanilla-CNN), because the commonality kernels would capture nothing about the persons in testing scenario under our orthogonal projection formulation. Similarly, in the second evaluation, the testing set only contains images with skin tone in {5, 6}, while four reference sets are organized with skin tone in {1, 2, 3, 4, 5, 6}, {1, 2}, {3, 4} and {5, 6} respectively. The same observation can be obtained, i.e. the higher the similarity between reference set and testing set, the better the recognition accuracy. This verifies that the main source of our improvement comes from the elegant design of personalized kernel.

Variants of reference set As discussed in Sect. 3.5, generally the reference set is used as a proxy of persons in a specific scenario. In special, the reference set can contain a particular group of faces, or even only one face image for different scenarios. To evaluate the variants of personalized CNN, besides the general reference set, three additional kinds of reference set are developed. The first is designed with face images grouped with age, the second is constructed with face images grouped with their skin tone, and the third is our conference work (Han et al., 2018), i.e. containing only one face image in the reference set according to who one face is compared with. Specifically, in the experiments with grouped reference set, all face images in testing set and reference set are divided into three groups respectively according to their age/skin tone. For age as criterion, three groups are with age in interval [0, 35], [35, 50], and [50, max] respectively, while for skin tone as the criterion, they are with skin tone {1, 2}, {3, 4}, and {5, 6} respectively, according to the label on IJB-A and IJB-C datasets. For face images in a specific face group, oneâ€™s personalized features are extracted with corresponding reference set to highlight the special characteristics.

The personalized CNN with the general reference set and three variants are evaluated on IJB-A and IJB-C datasets as shown in Fig. 5. As can be seen, the personalized CNN significantly outperforms the vanilla CNN benefiting from focusing more on those personalized characteristics. Besides, personalized CNN with more delicate reference set, such as different age groups, different skin tone groups, performs better than that with general reference set. Taking the recognition of children face as an example, if the reference set more accurately depicts the commonality of all children, the common component of children faces will be removed from the input child face. Therefore, the most distinguishing facial characteristics that can distinguish the child from other children are highlighted. However, as can be seen, in the conference version where the reference set contains only one face from the pair faces being compared, the accuracy is slightly declined. This is because with one single face in the reference, the commonality calculated only between two faces may be inaccurate due to the divers inter-person variations.

Table 6 Influence of the balance parameter ğœ† in our personalized CNN on LFW, IJB-A, and IJB-C
Full size table
Fig. 5
figure 5
Performance of personalized CNN w.r.t different reference sets on the IJB-A and IJB-C verification. Best viewed in color

Full size image
Fig. 6
figure 6
Visualization of feature map from the personalized kernel and the commonality kernel w.r.t. different ğ›¼. The first row is the feature maps from the personalized kernel, and the second row is the feature maps from the commonality kernel. In each of the three groups, the first column is the input image, and the rest three ones are the feature maps from the model trained with different ğ›¼

Full size image
Influence of Hyper-Parameters
Influence of parameter ğ›¼. In Eq. (9), ğ›¼ is used to control the balance between the variance and magnitude of commonality component in the process of optimizing the commonality kernel. Here, we investigate the influence of ğ›¼ by fixing ğœ† as 0.5 since ğ›¼ and ğœ† are independent to each other . The experiments are conducted on our personalized CNN with 4-layer backbone. Specifically, we set the ğ›¼ as 0.3, 0.4, 0.5, 0.6, 0.8, 1.0 respectively. As shown in Table 5, when increasing ğ›¼, the performance first rises and then descends, with highest accuracy achieved at ğ›¼=0.6. This shows that both constraints of the small variance and large magnitude for optimizing the commonality kernel are necessary. Only small variance constraint would make the commonality kernel only contain few characteristics, while excessive large magnitude would make the commonality kernel capture many unshared characteristics, both leading to decreased accuracy.

Furthermore, we also visualize the feature maps from the personalized kernel w.r.t. the commonality kernel obtained from different ğ›¼. As is shown in Fig. 6, with bigger ğ›¼, the high response region of feature maps from the personalized kernel expands, while that from commonality kernel shrinks as expected, and vice versa.

Influence of parameter ğœ†. In Eq. (20), parameter ğœ† is used to balance the two types of cross entropy loss for feature classification and kernelâ€™s intra-person invariance respectively. We investigate the influence of ğœ† by fixing the ğ›¼ as 0.6, and setting ğœ† as 0, 0.1, 0.3, 0.5, 0.7, and 0.9 respectively. The experiments are conducted on our personalized CNN with 4-layer backbone. As shown in Table 6, the performance also first rises and then descends. The highest accuracy is achieved at ğœ†=0.5, which means the cross entropy loss over generated kernels is beneficial but can not be weighted too large. Thus, in all experiments, we set ğ›¼=0.6 and ğœ†=0.5 unless otherwise specified.

Table 7 Comparison with existing methods on LFW in terms of mean accuracy (mAcc)
Full size table
Table 8 Comparison with existing methods on IJB-A, and top-1, top-5 accuracy for identification
Full size table
Table 9 Comparison with state-of-the-arts on IJB-C
Full size table
Comparison with Existing Methods
The above ablation studies investigate the factors that are involved in our personalized convolution and analyzes the advantage of our method.

Furthermore, we also compare our proposed method with a few state-of-the-art methods on LFW, IJB-A, and IJB-C. In this experiment, personalized CNN with ResNet-50 backbone trained on VGGFace2 is developed for fair comparison as most existing methods are equipped with large architectures and training datasets. Among these compared methods, FaceNet (Schroff et al., 2015), LargeMargin (Liu et al., 2016), SphereFace (Liu et al., 2017), CosFace (Wang et al., 2018), and ArcFace (Deng et al., 2018), UniformFace (Duan et al., 2019) propose new loss functions, PAM (Masi et al., 2016a), 3DMM (Tuan Tran et al., 2017), DR-GAN (Luan et al., 2017), Deep Multi-pose (AbdAlmageed et al., 2016), and FFGAN (Yin et al., 2017) design novel architectures to especially address the large pose variations. Our method can be also regarded as a novel architecture, which is orthogonal to those methods with new loss function (Deng et al., 2018; Liu et al., 2016, 2017; Schroff et al., 2015; Wang et al., 2018), and can be combined for further improvement. Here, the ArcFace loss is employed to investigate that our personalized CNN can be further improved with more advanced loss function.

The comparison on LFW is shown in Table 7. As can be seen, this dataset is almost saturated, and most recent proposed methods achieve performance higher than 99% in terms of mAcc, including Facenet (Schroff et al., 2015), SphereFace (Liu et al., 2017), CosFace (Wang et al., 2018), ArcFace (Deng et al., 2018), RegularFace (Zhao et al., 2019), and UniformFace (Duan et al., 2019). When training on the Webface dataset, our method achieves comparable accuracy to them even with a much shallower backbone. When training on bigger dataset with deeper backbone and more advanced loss, the accuracy of our personalized CNN is further boosted, and even outperforms the recently proposed DemoID (Gong et al., 2020). Those clearly demonstrate the effectiveness of our personalized convolution.

Furthermore, the comparison is performed on the more challenging IJB-A and IJB-C dataset, and the results are shown in Table 8 and 9 respectively. On IJB-A, unlike PAM (Masi et al., 2016a), 3DMM (Tuan Tran et al., 2017), DR-GAN (Luan et al., 2017), Deep Multi-pose (AbdAlmageed et al., 2016), and FFGAN (Yin et al., 2017), which are proposed pose-aware face recognition methods, our method isnâ€™t especially designed for dealing with extreme pose variations of faces. But surprisingly, our method still obtains higher accuracy than those specially designed methods when trained on the same dataset, i.e. CASIA-WebFace (Yi et al., 2014). Whatâ€™s more, our Personalized CNN trained on VGGFace2 dataset even achieves comparable results with UniformFace (Duan et al., 2019) which are trained on a much bigger dataset. On the IJB-C, our personalized CNN is mainly compared to methods trained on VGGFace2 (Cao et al., 2018). As can be seen in Table 9, our method outperforms the recent proposed DemoID (Gong et al., 2020). Those comparisons further show that the extracted personalized features extracted with our personalized CNN can better distinguish different persons even in challenging scenario.

Fig. 7
figure 7
Comparison of feature maps between our personalized CNN and vanilla CNN. (1) In our personalized CNN, distinct features are focused for different face images. The first row mainly focuses on the regions around eyes, the second row highlights features of nose, and the third row focuses not only eyes but also mouth. (2) In the vanilla CNN, features of all face images are focused almost equally

Full size image
Fig. 8
figure 8
Illustration of feature map of different images of the same person from personalized kernel. The rows a, c, e, g show the images of one person with variance on pose, expression respectively, and the rows b, d, f, h show corresponding feature maps from personalized kernel

Full size image
Fig. 9
figure 9
Success and failure cases of our personalized CNN. a faces that our personalized CNN can correctly recognize, while the vanilla CNN can not. b failure cases of our personalized CNN. The notation #images means the number of images in the probe template or Rank-1 template

Full size image
Visualization and Failure Cases
To intuitively verify whether personalized kernel captures oneâ€™s personalized characteristics, we visualize the averaged feature maps across channel in the last convolutional layer of our personalized CNN and vanilla CNN as it is easy to see which regions are with higher responses in Fig. 7. However, this does not imply other regions with lower responses are not exploited in the feature representation, though loosely speaking smaller average responses generally mean less important features. As can be seen, our personalized CNN focuses on different features of distinct persons such as oneâ€™s eyes and another oneâ€™s mouth, while the vanilla CNN focuses almost the same center features leading to inferior discrimination.

To show the robustness of extracted personalized features of one person to the variance of poses and expressions, we visualize the feature maps of different images for the same person output from the personalized convolutional layer. As shown in Fig. 8, for the same person in each row, the regions with high response of images from the same person are similar even with variance on pose, and expression, e.g, eye regions for persons in the rows (a) (c), nose regions for the person in the row (e), and eye and nose regions for the person in the row (g).

We also show several success and failure cases of our personalized CNN on IJB-A dataset for face identification in Fig. 9. As can be seen from Fig. 9 (a), the personalized CNN can successfully recognize some face images with extreme poses, occlusion, or blur variations while the vanilla CNN can not. However, there are also some failure cases that our method can hardly handle, while the L-vanilla CNN can still recognize correctly as shown in the Fig. 9 (b). The failure cases are substantially less than the success cases. On IJB-A for face verification, at FAR=0.001, the success cases that our personalized CNN can correctly verify while the vanilla CNN canâ€™t are up to 12.94%; and the failure cases that are wrongly verified by our personalized CNN but still correctly verified by the vanilla CNN only take up 2.38%. Those failure cases are mainly because our personalized kernel highlights oneâ€™s specialty that is different from others for better recognition, while at the same time it may also enlarges some within-class variations between the probe and gallery if it is extreme enough to be more like between-class variations (like ğ‘2>ğ‘ if ğ‘>1). This leaves interesting room for further study.

Conclusion and Future Work
Inspired by the observations about humansâ€™ face recognition perception, this work proposes a personalized convolution method with personalized kernel to extract oneâ€™s special features for more accurate face recognition. To achieve this goal, ordinary kernel of each person created by a kernel generator is decomposed into two orthogonal components, i.e. the commonality component and the specialty component. The commonality component is filtered out by a reference set, and the personalized component is retained as the personalized kernel. Especially, the reference set can be constructed as different types for various scenarios. Extensive experiments demonstrate that our method achieves quite promising performance of face recognition, implying that personalized convolution can extract more discriminative features than a fixed convolution.

In the future, we would like to explore more generic structure to deal with those extreme variations of pose, expression, and etc.

