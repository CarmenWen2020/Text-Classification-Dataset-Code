In this paper, we propose a multi-vehicle localization approach relying exclusively on cameras installed on connected cars (e.g. vehicles with Internet access). The proposed method is designed to perform in real-time while requiring a low bandwidth connection as a result of an efficient distributed architecture. Hence, our approach is compatible with both LTE Internet connection and local Wi-Fi networks. To reach this goal, the vehicles share small portions of their respective 3D maps to estimate their relative positions. The global consistency between multiple vehicles is enforced via a novel graph-based strategy. The efficiency of our system is highlighted through a series of real experiments involving multiple vehicles. Moreover, the usefulness of our technique is emphasized by an innovative and unique multi-car see-through system resolving the inherent limitations of the previous approaches. A video demonstration is available via: https://youtu.be/GD7Z95bWP6k.

Access provided by University of Auckland Library

Introduction
The constant increase in computing power, the improvement in artificial intelligence, and the fast development of communication technologies are shaping the cars of tomorrow, leading to safer, more comfortable, and more assisted driving. As a result, autonomous driving is to become one of the most disruptive transportation technologies since the beginning of the automobile era (Wiseman 2018). On the technical level, autonomous cars rely on measurements from various sensors such as LiDARs, cameras, wheel odometry, and more (Levinson et al. 2011). The fusion between these sources of information is utilized to sense the environment and to control the vehicle accordingly to ensure the safety of the passengers, pedestrians, and surrounding cars. However, these decisions are limited by the perception of the sensors mounted on the vehicle which can only provide partial and biased measurements of the environment—due to occlusions in a cluttered scene or range limitation. This may lead to erroneous decisions which can have dramatic consequences in a real traffic scenario such as collisions with cars or pedestrians.

To cope with these limitations, we propose to take advantage of recent wireless network technologies to aid the pose estimation between connected vehicles. Knowing the poses and the motion of the surrounding vehicles is the key information to drastically increase the area covered by the sensors via measurements sharing between cars. Moreover, multi-car localization also allows taking upstream decisions which can be crucial for the safety of the passengers. Overall, it is an essential component to perform a wide spectrum of tasks for autonomous or assisted driving such as collision avoidance, traffic control, or see-through effect (Rameau et al. 2016b). This paper is one of the pioneering works taking advantage of communication systems technology for vehicles navigation using exclusively a vision system. Specifically, we solve the problem of real-time localization of connected vehicles using only onboard stereo cameras and a wireless network (i.e. Internet or a local Wi-Fi network). All the agents (i.e. connected vehicles) are connected to a central server, which redistributes the tasks and sends orders to the cars. Using this setup, we propose a scalable and efficient approach to know the poses between a group of cars in real-time. To perform this task, we propose a unique architecture to limit the bandwidth requirement while maintaining a high accuracy localization of multiple vehicles.

To summarize our system, every car runs a Simultaneous Localization And Mapping (SLAM) algorithm and sends Bag-Of-Words (BoW) (Gálvez-López and Tardos 2012) descriptors to a centralized server to find similar visited places between vehicles. On this basis, the poses between cars are estimated and regularly updated by a 2D-3D matching (Zheng et al. 2013). The central server keeps track of the interactions between cars in the form of a graph to improve the pose estimation and to efficiently distribute the tasks and the information between vehicles. Additionally, our architecture permits to receive the poses between cars with a low latency thanks to an a-posteriori update strategy.

The main contribution of this work lies in the design of a collaborative vehicles localization which, unlike existing CO-SLAM techniques (Zou et al. 2019), is neither fully centralized nor decentralized. Instead, we propose a novel architecture where only light information is centralized (place recognition) while heavier data is regularly shared locally between connected agents. This trade-off allows to drastically reduce the bandwidth and computation load. Additionally, we propose an efficient multi-car see-through system inspired by Rameau et al. (2016b). The original system has been modified and extended to work with multiple connected vehicles thanks to our multi-car localization technique (see Fig. 1). These modifications resolve numerous limitations of previous works (Rameau et al. 2016b) and allow to deal with complex overtaking scenarios involving more than two cars. Finally, we have conducted a large set of experiments and analyses highlighting the relevance and effectiveness of the proposed approach. Presented results are screen-captures of live demonstrations (Rameau et al. 2018) performed under different environments, geographical locations (two countries), and camera setups. Overall, our multi-car localization is a central element for a wider range of applications such as collision prediction or collaborative autonomous driving.

Fig. 1
figure 1
Multi-car localization and see-through system. (top-left) Original image; (bottom-left) See-through effect applied on two preceding connected vehicles where the color bars denote a distance to the cars; (right) 3D map and multi-car localization where image acquiring vehicle is circled in pink (Color figure online)

Full size image
Related Work
In the field of intelligent transportation systems, Global Navigation Satellite System (GNSS), such as the GPS, remains a privileged approach. GPS localization system has many advantages as the localization is performed in a single and absolute referential with a refreshment rate up to 10 Hz. However, due to many factors (e.g. atmospheric effects (Steiner et al. 2013), clock errors, and urban canyon), GPS accuracy remains limited (i.e. more than 1-m uncertainty (Wing et al. 2005)). Traditionally, the accuracy of this technology is improved via software. Specifically, filtering approaches are utilized to incorporate a motion model in the position estimation (Rezaei and Sengupta 2007). Alternatively, image information can be fused with GPS information to significantly improve the accuracy (Tao et al. 2013). For multi-car localization, the vehicles can exchange their GPS positions leading to a more constrained system. For instance, in (Shen et al. 2018; Liu et al. 2017), the authors use this extra information to improve the localization of the vehicles in a team of connected cars. Contrarily, in this work, we do not rely on any GNSS, and the entire localization of the vehicles is achieved via cameras installed on the cars.

While visual odometry for a single vehicle has been widely explored (Pereira et al. 2018; Mur-Artal and Tardós 2017; Pire et al. 2017; Persson et al. 2015), the navigation of multiple cars using vision systems attracted less attention. The existing multi-agent localization and mapping techniques can be divided into two main categories: centralized (all the information and computation is centralized on a single server) and decentralized systems (the computational cost is shared between agents, a central platform is not required).

Centralized Approaches We can find multiple attempts of multi-agents localization in the robotics community. One of the pioneering approaches is Co-SLAM (Collaborative SLAM) (Zou and Tan 2012) which has the particularity to deal with dynamic scenes. Co-SLAM has been extended for real-time robotics applications (Perron et al. 2015), however, a constant overlapping between the cameras have to be preserved. Moreover, this method does not consider the limitation of the network and requires synchronized video streams between the agents of the system, which is technically complex, to accurately achieve through a network. To alleviate this assumption, more straightforward techniques have been developed considering a rigid environment. For instance, in (Forster et al. 2013) and (Schmuck and Chli 2017, 2019), the authors propose to localize multiple UAVs navigating in the same scene. These techniques can be improved by fusing information from multiple sensors, for instance (Karrer et al. 2018; Morrison et al. 2016) propose a collaborative visual inertial SLAM. Similar techniques for RGB-D cameras have also been successfully developed for indoor environment (Riazuelo et al. 2014; Mohanarajah et al. 2015; Golodetz et al. 2018).

Decentralized Approaches All the previously mentioned approaches share a common feature: they are based on existing SLAM techniques where the mapping process is performed on a central server. Therefore, we refer to these techniques as “centralized approaches”. They usually ensure a good consistency of the reconstruction as a result of a global Bundle-Adjustment (BA) optimization. However, following this pipeline, the computational complexity of the problem grows quickly with the increasing number of agents, limiting the scalability of the centralized systems to a few connected devices. Despite attempts to reduce the bandwidth consumption (Van Opdenbosch et al. 2018), these approaches often require the transfer of large data to the server which also restricts their applicability. To overcome these limitations, a few attempts have been proposed. Nettleton et al. (2000) were among the first to propose a fully decentralized approach to localize multiple robotic platforms by fusing information from a large number of sensors, including IMU, GPS, camera, and radar. All this information is shared between agents and combined to solve the problem of decentralized localization and mapping. This approach has been exclusively confirmed through simulation and requires a large number of sensors to perform effectively. Using cameras only, one of the most promising work is (Cieslewski et al. 2017), where the authors have successfully distributed the computation over all the connected devices. However, the solution is purely theoretical and has not been tested in real-time. A more practical approach (Chen et al. 2018) shows similarities with our technique. Particularly, the authors propose a collaborative and decentralized SLAM technique taking advantage of a place recognition algorithm to initiate the inter-car pose estimation before streaming keyframes between cars. While this approach is efficient for two connected cars, it remains hardly scalable to a larger number of agents due to the bandwidth required to constantly send keyframes between each connected agents. More recently, Dubois et al. (2019) propose a decentralized visual SLAM where each connected agent shares a summarized 3D map for collaborative localization.

Centralized and decentralized techniques have their benefits and drawbacks. Centralized systems are hardly scalable to a large number of connected agents due to the high computational complexity on a single machine. On the other hand, using decentralized approaches, the transfer of redundant data remains difficult to avoid. In this work, we propose a technique taking advantage of both architectures. Specifically, we have designed our system to centralize only light data to a central server, while large data are shared locally between cars.

Alternatively, several approaches aim to improve the vehicle localization in an urban environment by utilizing road information and pre-built maps. Specifically, Bailo et al. (2017) use road markings to precisely localize a vehicle in a prebuild 3D map based on SLAM. Similarly, Wei et al. (2014) propose to link road signs detections to correct the vehicle pose. However, these approaches strongly rely on the extraction of contextual and semantic information which may vary between countries and situations. Moreover, the maintenance and update of these 3D maps are costly and complex.

In this paper, for the sake of scalability, we do not target to reconstruct a global consistent 3D map. Instead, we propose to localize the vehicles between each other using short segments of their 3D maps (computed online by each vehicle) whenever required. The number of connected cars is neither predefined nor limited. The vehicles can be located at completely different places, forming multiple groups of connected cars that can be merged automatically if they visit a similar place. Moreover, no prior planar road assumption is needed. Finally, our strategy has the advantage to drastically limit the computational load and data to transfer to the server (a slow Internet connection is sufficient) while solving the inter-car pose estimation via a direct exchange between cars. Our a-posteriori inter-car pose adjustment leads to virtually no delay.

Most of the existing works are tested exclusively on synthetic data (Nettleton et al. 2000). Other techniques (Dubois et al. 2019) are assessed offline on real data (e.g. KITTI (Geiger et al. 2012) or EUROC (Burri et al. 2016)). However, these datasets have not been created for multi-agent localization and the images have been acquired by a single robot. Moreover, performing offline experiments does not fully reflect the complexity of real-time systems where all connected devices have to share data through a wireless network. Real-time demonstration has also been shown (Forster et al. 2013; Moratuwage et al. 2010) but rarely with more than two connected agents looking at the same scene (mostly using UAVs). In this paper, we propose live experiments with up to 4 connected cars.

Notation
In this paper, we consider a fleet of V connected cars ={𝑐𝑖}𝑉𝑖=1 having a stereo camera and the ability to communicate between each other through the Internet or local Wi-Fi connection. Prior to the details, we first clarify the notation related to our problem.

Rigid Transformation Between Cars The notation for a rigid transformation between cars 𝑐𝑗 and 𝑐𝑘 is defined as 𝑐𝑗𝑐𝑘𝐌=[𝑐𝑗𝑐𝑘𝐑 𝑐𝑗𝑐𝑘𝐭; 01×3 1],Footnote1 where 𝑐𝑗𝑐𝑘𝐑 and 𝑐𝑗𝑐𝑘𝐭 are respectively the 3×3 rotation matrix and the 3×1 translation vector between the two cars’s referentials.

Map Point Every single vehicle 𝑐𝑖 is creating its own 3D map 𝑐𝑖={𝑐𝑖𝐏𝑛}𝑁𝑛=1 containing N map points encapsulating together the 3D location of the point 𝑐𝑖𝐗𝑛=[𝑐𝑖𝑋𝑛,𝑐𝑖𝑌𝑛,𝑐𝑖𝑍𝑛]⊤ and its descriptor 𝑐𝑖𝐃𝑛.

Hence, 𝑐𝑖𝐏𝑛=[𝑐𝑖𝐗⊤𝑛,𝑐𝑖𝐃⊤𝑛]⊤.

Stereo Keyframes 3D map points are computed from M KeyFrames (KF) noted 𝑐𝑖={𝑐𝑖𝐾𝑗}𝑀𝑗=1. An arbitrary keyframe 𝑐𝑖𝐾𝑗 stores various types of information such as its 4×4 pose matrix 𝑐𝑖𝐌𝑗, the BoW representation of the keyframe 𝑐𝑖𝐁𝑗, and the set of stereo points 𝑐𝑖𝑗. The k th stereo point in the set contains the left and right keypoints location noted 𝑐𝑖𝐱𝑘𝐿𝑗=(𝑐𝑖𝑢𝑘𝐿𝑗𝑐𝑖𝑣𝑘𝐿𝑗) and 𝑐𝑖𝐱𝑘𝑅𝑗, respectively. A stereo point also contains the image descriptor extracted on the left image 𝑐𝑖𝐃𝑘𝑗.

Local Map Our system relies on local maps transfer between vehicles to estimate the inter-car pose. The local map 𝑐𝑖𝑗 consists of all the map points visible in the keyframe 𝑐𝑖𝐾𝑗 (more details in Sect. 5.4).

Intrinsic and Extrinsic Parameters In this work, we consider a calibrated stereo rig, where the 3×3 intrinsic matrices of the left and right cameras of the vehicle 𝑐𝑖 are noted as 𝑐𝑖𝐊𝐿 and 𝑐𝑖𝐊𝑅, respectively. The baseline between the cameras is noted as 𝑐𝑖𝑏, where we assume that the rotation between the calibrated stereo cameras is an identity matrix. Therefore, the translation between cameras can be noted 𝑐𝑖𝐭𝐿𝑅=[−𝑏,0,0]⊤.

System Architecture
This work aims to localize cars  connected through the Internet (and/or wireless local network) in real-time. For this purpose, every single car is equipped with a laptop computer, a stereo-vision rig, and a network connection. This is a particularly challenging task due to the limited bandwidth available and the high computational requirements. To overcome the restricted bandwidth constraint, we have designed a distributed architecture where compact information sent to the server is used to harmoniously distribute the data transferred between vehicles. On the other hand, the computational complexity is alleviated by sharing the computational cost between the different agents, while a central server distributes the tasks appropriately. Moreover, instead of trying to impose a global consistency of the map (Schmuck and Chli 2017)—which requires sharing a substantial amount of data to perform expensive global BAs—we have adopted the strategy to perform local and frequent updates of the inter-car poses.

Fig. 2
figure 2
Architecture of the multi-car localization and see-through system with two connected vehicles. The messages are transferred constantly while services are transferred on demand. The centralized messages and services are displayed in red. The only heavy centralized data is the BoW transfer, while the car pose graph is compact and updated once per second. The heavier local inter-car messages and services, such as the local 3D map and the see-through image patch, are depicted in violet. In this figure, the car 2 sends its 3D map to the car 1 to perform the inter-car localization. It means that the car 2 is the preceding car generating the see-through effect. In our system, the role of the car 1 and car 2 are automatically interchangeable if the car 1 overtakes the car 2 (Color figure online)

Full size image
The whole system can be decomposed into four main processes: SLAM, place recognition, inter-car pose estimation, and group management of the connected vehicles. The architecture is depicted in Fig. 2. For clarity, only two cars are shown, although the architecture can handle an arbitrary number of agents.

Every car estimates its motion and 3D map individually through our SLAM approach (Sect. 5). For each car and for every new keyframe added to their map, a BoW descriptor is computed from ORB (Rublee et al. 2011). This compact image descriptor is transferred to a central server to be compared against places already visited by other connected vehicles (Sect. 6.1).

When a similar place is detected between two agents, the central server sends an action message requesting the two vehicles to exchange a local 3D map to check the photometric and geometric consistency of the match (Sect. 6.2). If the match is valid, the two cars can share their poses expressed in a similar referential (Sect. 6.3). A graph-based strategy allows an efficient managing of the group of vehicles that have visited the same places and allows to update the oldest inter-car poses with new estimations (Sect. 6.5). To reduce the drifting effect inherent to any visual SLAM technique, the inter-car poses are updated regularly (Sect. 6.4).

The scalability of our system is ensured via a distributed architecture. As depicted in red in Fig. 2, only light BoW features are transferred to the server in order to appropriately send commands to the connected vehicles. Thus, “bandwidth-hungry” data (e.g. local 3D maps) is directly shared locally between cars. This trade-off has the advantage of avoiding the limitations of both fully decentralized and centralized methods. Indeed, centralized approaches are hardly scalable to a large number of agents since all the computations are performed on a single computer (Schmuck and Chli 2017), while decentralized approaches can suffer from redundant information transfer (Chen et al. 2018).

It should be noticed that we never directly send images through wireless network—for multi-car localization—since it would not be data effective. Instead, only sparse 2D or 3D information is shared between connected agents. This strategy also allows us to better distribute the computations over all the connected cars instead of centralizing them on the server.

One of the important points of our strategy to reduce bandwidth consumption is to share local 3D maps punctually. The stereo-SLAM implemented in each vehicle is used to interpolate the inter-car poses between two updates (assuming unbiased motion estimation), thus, a drift accumulates over time. To cope with this problem, we propose a graph pose update (Sect. 6.5) and a “pose tracking” technique (Sect. 6.4) ensuring a regular update of the poses. Despite these contributions, the drift accumulation phenomenon can still be observed in the experiments Sects. 9.2 and 9.6. However, we can notice that for short distances between the vehicles, our system remains accurate and allows performing a seamless see-through effect (see Sect. 9.8) underlying the high accuracy of the proposed methodology.

In Fig. 2, we also include the see-through car modules. Whenever two cars are paired, the inter-car pose is made available to be used by the see-through system. The preceding car is generating a synthetic see-through patch from the viewpoint of the following vehicle using the inter-car pose and a dense stereo disparity map. This image patch is sent to the following car to display the occluded area of the image. The details of this process are available in Sect. 7

Simultaneous Localization and Mapping
Our multi-car localization solely relies on the matching between images and maps acquired from different vehicles (note that no other sensors than cameras are ever used). The computation of these maps is performed by our stereo-SLAM algorithm designed specifically for this problem. In the context of connected vehicle localization, two major concerns have to be addressed by the visual SLAM technique: its speed and the size of the generated 3D map—a lighter map is desirable to reduce the quantity of data transferred between cars.

The main pipeline of our algorithm remains conceptually close to existing parallel graph-based SLAM (Mur-Artal and Tardós 2017; Pire et al. 2017; Klein and Murray 2009). These approaches have demonstrated their robustness and effectiveness as a result of the parallelization of the tracking and mapping processes. The tracking essentially consists in the estimation of the camera pose inside the current local map, while the mapping refines the keyframe poses and the structure of the scene locally. Although many SLAM strategies include another thread dedicated to loop closure, this aspect is omitted here to simplify the multi-car system.

Keypoint Detection and Description
The keypoint detection is a fundamental stage in SLAM, as the first step in the visual odometry chain, its performance affects the quality of the resulting localization and mapping. Our SLAM relies on FAST (Rosten et al. 2010). In addition, we propose to exploit a recent efficient Adaptive Non-Maximal Suppression (ANMS) technique tailored to ensure a homogeneous distribution of the keypoints on the image (Bailo et al. 2018).

Concerning the keypoint descriptors, we are using Oriented FAST and Rotated BRIEF (ORB) (Rublee et al. 2011). This technical choice can be justified by the low computational time required for their extraction (Miksik and Mikolajczyk 2012). Additionally, as a binary descriptor, its size is relatively compact (only 32 bytes per keypoint).

Stereo-Matching
Our stereo-matching technique consists of two steps: first, the FAST points from the left and right images are matched using ORB descriptor and epipolar constraints. To speed-up this stage, we employ a tree-based structure for a fast query of the points sharing the same vertical coordinates. While this strategy is computationally efficient, the resulting matches can be corrupted by outliers. For a more robust matching and to reach a subpixelic accuracy, we propose to refine the stereo matches with a bi-directional affine SSD patch registration (Lucas et al. 1981), the points drifting too far from their original locations are classified as outliers and removed from further consideration.

Map Initialization
Map initialization is a crucial and relatively complex step for monocular SLAM (Mur-Artal and Tardós 2017). For a stereo-vision system, this stage is straightforward as the calibrated baseline between the cameras ensures an accurate and real-scale triangulation of 3D points.

Therefore, the initialization consists in a standard stereo triangulation (Hartley and Zisserman 2003) between the matched stereo-points. As part of the initialization process, the first stereo-frame is directly considered as the first keyframe 𝐾1 and the triangulated points are included in the map. The second stereo-image in the sequence is matched with the first triangulated keyframe via a KD-Tree descriptor matching strategy (Silpa-Anan and Hartley 2008). This type of matching is undeniably slower than the tracking by re-projection approach (see Sect. 5.5) but does not require any prior on the motion of the cameras and is utilized for the first two stereo frames only.

Keyframe Covisibility and Local Map
Keyframe covisibility is a cornerstone in many recent graph-based SLAM approaches (Mur-Artal and Tardós 2017). In our algorithm, we also take advantage of this concept to reduce the size of the local map utilized to track the motion of the camera. The local map 𝑖 associated with the keyframe 𝐾𝑖 is composed of the map points visible from the keyframes sharing common 3D points with the reference keyframe 𝐾𝑖. In (Mur-Artal and Tardós 2017), for example, all the keyframes sharing at least 15 points with 𝐾𝑖 are taken into consideration. In our case, we only select the l best keyframes (usually 𝑙=10) sharing the highest number of 3D points with the keyframe of interest. This strategy allows ensuring a fast and constant computational time.

Points Tracking by Reprojection
A robust prediction of the camera pose is a strong a priori which allows to speed up the tracking process in the SLAM algorithm. For this purpose, we utilize a 6-DOF Extended Kalman Filter (EKF) (Ha et al. 2015) to predict the next pose of the stereo-rig. In this paper, the current pose prediction at a time t is denoted 𝐌̂ 𝑡. Using the predicted pose 𝐌̂ 𝑡, the local 3D points are reprojected in the left image of the stereo-rig at the 2D locations 𝐱̂ 𝐿 such that their respective correspondences in the current frame can be searched inside a window of radius r. These potential matches obtained by reprojection are further filtered with their ORB matching score and ratio test (Persson et al. 2015). To filter out the remaining outliers and to estimate the current pose 𝐌𝑡 jointly, we propose to utilize a P3P (Kneip et al. 2011) warped in a PROSAC (Chum and Matas 2005) algorithm. Finally, a robust non-linear refinement of the pose is applied to incorporate left and right points into the pose estimation. This optimization is a “pose-only bundle adjustment” refining only the current pose 𝐌𝑡 given the one to one correspondence between the 2D and 3D points—including mono and stereo points. The loss function to be minimized can be formulated as:

arg min𝐫𝑡,𝐭𝑡∑𝑖=1𝑆(‖𝐱𝐿𝑖−𝜋𝐿(𝐊𝐿,𝐌𝑡,𝐗𝑖)‖𝐻+‖𝐱𝑅𝑖−𝜋𝑅(𝐊𝑅,𝐌𝑡,𝐗𝑖,𝐭𝐿𝑅)‖𝐻)+∑𝑗=1𝑈‖𝐱𝐿𝑗−𝜋𝐿(𝐊𝐿,𝐌𝑡,𝐗𝑗)‖𝐻,
(1)
where U and S are the number of monocular and stereo points respectively. While, 𝐫𝑡 and 𝐭𝑡 are the rotation (Rodrigues representation of 𝐑𝑡) and translation vectors of the current pose 𝐌𝑡.

𝜋𝐿(𝐊𝐿,𝐌𝑡,𝐗) and 𝜋𝑅(𝐊𝑅,𝐌𝑡,𝐗,𝐭𝐿𝑅) are the reprojection functions of the left and right camera respectively. For robustness, a Huber loss function, ‖⋅‖𝐻 is employed.

New Keyframe Selection
Including new keyframe has to be performed very carefully to ensure a proper balance between the accuracy and size of the map. In our algorithm, we define a simple policy (based on heuristics) to insert a new keyframe in the map. A new keyframe is added if:

less than 100 tracked points remain

more than 3 m have been traveled since the last KF

the rotation from the last keyframe is more than 35∘

Local Bundle Adjustment
The local bundle adjustment is the last step of our mapping strategy. It refines together the location of the 3D points and the poses of the keyframes in the local map:

arg min𝐫𝑘,𝐭𝑘,𝐗𝑖∈∑𝑘=1𝐿∑𝑖=1𝑆(‖𝐱𝑘𝐿𝑖−𝜋𝐿(𝐊𝐿,𝐌𝑘,𝐗𝑖)‖𝐻+‖𝐱𝑘𝑅𝑖−𝜋𝑅(𝐊𝑅,𝐌𝑘,𝐗𝑖,𝐭𝐿𝑅)‖𝐻)+∑𝑘=1𝐿∑𝑗=1𝑈‖𝐱𝑘𝐿𝑗−𝜋𝐿(𝐊𝐿,𝐌𝑘,𝐗𝑗)‖𝐻,
(2)
where L is the number of keyframes in the local map.

Multi-Car Localization
This section focuses on the details of our multi-car localization strategy. The overall architecture is depicted in Fig. 2.

Place Recognition
For each connected car, and for every new keyframe included in their map, a compact image representation is sent to the server, where these image descriptors are compared to the ones from other connected agents to find similar visited places.

While recent advances in deep learning have significantly outperformed handcrafted techniques, deep learning-based place recognition methods (Piasco et al. 2018) remain computationally costly to be integrated into a vehicular system. Moreover, these methods hardly reach real-time performance. Thus, we propose to utilize a BoW representation of an image which tends to be more suitable for our scenario (Gálvez-López and Tardos 2012). BoW is satisfying in terms of computational efficiency and it scales better for many cars, and a large number of accumulated keyframes.

When no apriori on the cars’ positions is given, an exhaustive search between connected vehicles is performed to find mutually visited places. This search is speeded up by an inverse index mapping strategy (Gálvez-López and Tardos 2012). It means that only the most relevant (having enough “words” in common) BoW vectors are considered as potential candidates and are compared by their L2 norm. One of the major disadvantages of BoW place recognition is a large number of false positives. Therefore, to enforce a spatiotemporal constraint, we only consider a match when multiple BoWs are consecutively matched in the same area. Conceptually, our strategy is close to the “island” technique described in (Gálvez-López and Tardos 2012).

When each vehicle is compared pair-wise with each other, it leads to a quadratic complexity. To improve it, we have implemented a multi-threaded solution where one thread per car is created dynamically on the server. Therefore, theoretically, with a single server, tens of cars could be localized in real-time. However, practically, a large number of connected cars could result in resource over-utilization creating potential bottlenecks during the localization. Fortunately, this problem can be overcome by using apriori information such as rough GPS location or by decentralizing these computations locally (vehicle itself). This extension is not included in this work since we only employ visual information.

When the server detects a mutually visited place between two cars, a request is sent to the preceding vehicle (i.e. the vehicle that visited the place earlier) to perform the inter-car pose estimation (see Sect. 6.2). This message contains the identification number of both cars along with their keyframe indices potentially corresponding to the same visited place.

To avoid using too “old” position, we only consider the last 1000 keyframes (usually covering more than 1 km) of each car. This process allows to have a relatively fixed BoW place recognition time and to avoid a biased estimation that can result from the SLAM drift.

Inter-Car Pose Estimation
Whenever a potential match between two cars 𝑐𝑝 (car which visited the place first, also refers as “preceding” car) and 𝑐𝑓 (the “following” car) is detected between the keyframes 𝑐𝑝𝐾𝑖 and 𝑐𝑓𝐾𝑗, the server sends a request to the preceding vehicle 𝑐𝑝 to share the local map 𝑐𝑝𝑖 with the following car 𝑐𝑓. Notice that the vehicle which visited the place first is assumed to have a better map quality since it had time to accumulate a larger number of keyframes in its covisibility graph. To reduce the quantity of data, a frustum culling (Pire et al. 2017) is applied on the local map before the transfer.

To quickly reject the wrong place recognition responses, a 2D-3D ORB descriptor matching between the local map 𝑐𝑝𝑖 and the descriptor 𝑐𝑓𝐃𝑗 is performed in the keyframe 𝑐𝑓𝐾𝑗. If at least 50 correspondences are found, the match is kept for further consideration. Otherwise, the pairing is rejected. Else, if enough potential 2D–3D matches remain, a geometric verification is performed. Specifically, a P3P RANSAC algorithm is applied to remove the outliers from the correspondence set and to estimate the initial pose 𝑐𝑓𝑐𝑝𝐌̂  between the two vehicles. If less than 30 matches remain after the RANSAC filtering, the match is discarded.

To improve the pose estimation between the cars, the keyframes in the covisibility map of 𝑐𝑓𝐾𝑗 are integrated into the pose estimation process. To find the corresponding points between the local map 𝑐𝑝𝑖 and the keyframes surrounding 𝑐𝑓𝐾𝑗, a points matching by reprojection (described in Sect. 5.5) is applied using the initial pose estimation 𝑐𝑓𝑐𝑝𝐌̂ .

Using the resulting correspondences between the local map and the keyframes, a robust non-linear optimization is used to improve the overall accuracy of the inter-car pose estimation. Only the inter-car pose 𝑐𝑓𝑐𝑝𝐌 is refined in the cost function:

arg min𝑐𝑓𝑐𝑝𝐫,𝑐𝑓𝑐𝑝𝐭∑𝑘=1𝐿∑𝑗=1𝑈‖𝐱𝑘𝐿𝑗−𝜋(𝑐𝑓𝐊𝐿,𝑐𝑓𝐌𝑘,𝑐𝑓𝑐𝑝𝐌,𝐗𝑗)‖𝐻,
(3)
with 𝜋(⋅) the reprojection function taking into consideration both the inter-car transformation 𝑐𝑓𝑐𝑝𝐌 and the poses of the keyframes 𝑐𝑓𝐌𝑘. After this final refinement, a confirmation is transferred to the car 𝑐𝑝 and the resulting inter-car pose 𝑐𝑓𝑐𝑝𝐌 is provided to the server to be shared between cars.

Pose Transfer
When two cars 𝑐𝑓 (following) and 𝑐𝑝 (preceding) are paired, they cross-share their respective poses (estimated via their SLAM) 𝑓𝐌𝑡 and 𝑝𝐌𝑡 at a time t. To be utilized, these poses have to be expressed in the referential of the receiving car. For instance, the car 𝑐𝑓 will transform the pose 𝑝𝐌𝑡 in its own coordinates via the following matrix composition:

𝑐𝑓𝑐𝑝𝐌𝑡=𝑐𝑝𝐌𝑡 𝑐𝑓𝑐𝑝𝐌,
(4)
where 𝑐𝑓𝑐𝑝𝐌𝑡 is the current position of the car 𝑐𝑝 expressed in the referential of the car 𝑐𝑓 at the current time t and 𝑐𝑓𝑐𝑝𝐌 the inter-car pose computed previously (Sect. 6.2). This pose can be directly utilized for various purposes including distance estimation, automatic convoying, cruise control, see-through, etc. This pose transfer is achieved for every single frame ensuring a 15Hz update (frame-rate of our cameras) which is significantly faster than GPS-based approaches. Notice that this technique allows transferring a constant flow of poses with very low delay (only the transfer of a pose vector of 6 floats and the computation time required by the SLAM to process an image). We call this strategy a-posteriori pose estimation.

Tracking-Based Car Matching
To speed up the place localization even further, we have implemented a tracking process that allows updating the place localization between the cars more frequently. Specifically, whenever two cars are paired (their inter-car pose is computed), the tracking process is initiated. Every 200 m the preceding car sends a local map to the following car to be compared to its theoretical closest keyframes (this keyframe is chosen assuming the drift between the two vehicles to be negligible). If one keyframe satisfies the matching criterion (see Sect. 6.2), the pose between the cars is updated accordingly. If the tracking fails twice in a row, the tracking stops and the BoW place recognition strategy is reinitialized.

Car Group Management and Update
The cars are matched pair-wise while the goal of this work is to simultaneously know the pose of all the surrounding vehicles. To achieve this goal, the server keeps track of the connected vehicles’ interactions. These connections are summarized in a directed cyclic graph shared among all cars in the network (this graph size is negligible and has a 1 Hz refreshment rate).

In Fig. 3, the nodes contain the ID of the agents while the edges contain both: 𝑐𝑝𝑐𝑓𝑡 (the time elapsed since the last update) and the inter-car transformation 𝑐𝑝𝑐𝑓𝐌 between cars 𝑐𝑝 and 𝑐𝑓. From this information, every car can determine which vehicles directly or indirectly surround it by graph traversal. Thus, even if the pose between two cars 𝑐1 and 𝑐5 has not been determined yet, assuming the inter-car poses 𝑐1𝑐3𝐌 and 𝑐3𝑐5𝐌 to be known, the transformation between the car 𝑐1 and 𝑐5 can be determined as a simple composition of the poses: 𝑐1𝑐5𝐌=𝑐1𝑐3𝐌𝑐3𝑐5𝐌. The same logic can be applied to a larger number of agents.

This strategy has another advantage. Due to the drift inherent to any visual SLAM algorithm, the accuracy of the poses between the cars tends to decline over time. It implies that to keep a sufficient level of accuracy, frequent updates have to be performed. Our graph-based representation is an efficient and elegant way to overcome this problem by computing the most recent update in the graph. The most recent updates can be a composition of multiple poses between cars. To take a concrete example, if the interpose 𝑐1𝑐2𝐌 has been updated 𝑐1𝑐2𝑡 seconds ago, it can be more efficient, instead, to use the composition 𝑐3𝑐2𝐌 𝑐1𝑐3𝐌−1 if 𝑐1𝑐3𝑡+𝑐3𝑐2𝑡<𝑐1𝑐2𝑡. More complex compositions involving a larger number of vehicles are possible. In practice, we utilize the Dijkstra’s algorithm to find the shortest path (minimum time elapsed) between each pair of cars to always ensure the best (i.e. the most recent) inter-car poses.

Fig. 3
figure 3
Example of the graph-based multi-car management with 5 vehicles 𝑐1…𝑐5. Here, the vehicle 𝑐4 is not connected to any vehicles yet. The vehicles 𝑐1, 𝑐2, 𝑐3 have strong inter connections while the car 𝑐5 is exclusively paired with 𝑐3

Full size image
Multi-Car See-Through System
Worldwide, it is estimated that traffic accidents are responsible for about 1.3 million casualties per year (WHO 2015). According to (Naja et al. 2013), 1% of car accidents happen during overtaking maneuvers. While it represents a relatively small portion of accidents, this type of collision is among the deadliest due to the high-speed and frontal collision. Overtaking accidents are especially common in rural areas and developing countries where the road infrastructures (road markings and signs) are limited. This type of accident is mainly due to the limited visibility of the driver. In this context, we propose to revise, adapt and improve the “see-through car” system (Rameau et al. 2016b). This technology has been initially developed to avoid a frontal collision during overtaking scenarios by letting the driver of the rear-vehicle “see-through” the occluding car blocking the view (see Fig. 4).

Multiple attempts have been proposed to develop robust and efficient see-through systems. For instance, several works (Olaverri-Monreal et al. 2010; Rameau et al. 2016a) propose to use markers to estimate the pose between the vehicles in order to synthesize the occluded view. Alternatively, Chen et al. (2015) suggest a more elegant strategy that does not require any markers but this technique is unsuitable for a real-time application and does not provide a realistic reconstruction of the environment. More recently, Rameau et al. (2016b) take advantage of a stereo vision system to synthesize the blind spot and to estimate the poses between cars. A similar concept using LIDARs has also been proposed by Ikeda et al. (2018).

One common limitation of all these existing systems is their master-slave configuration. Thus, none of these techniques is ready for a real deployment due to their lack of versatility. Specifically, the preceding car and the following car cannot switch their roles and the system is fully fixed in this unique configuration. Moreover, these approaches are also limited to only two cars and cannot solve the problem of multiple vehicles following each other. In this work, we propose a new implementation that takes full advantage of our multi-car real-time localization technique to see-through multiple cars simultaneously regardless of the cars’ order, distances, alignments, or configurations. The integration of the see-through module is depicted in Fig. 2.

It can be noted that two critical components are required to achieve the see-through effect: (1) the inter-car pose between the preceding and the following vehicles, and (2) the depth maps from the preceding cars. The poses are directly provided by the SLAM technique described in Sect. 6 and depth maps are calculated from a real-time dense stereo matching technique (Geiger et al. 2010). In this context, our collaborative localization strategy is an essential tool to provide fast and accurate estimates of the relative pose between each pair of connected vehicles. For more details, an exhaustive listing of the benefits provided by our localization strategy is proposed in Sect. 9.7.

Fig. 4
figure 4
Conceptual representation of the see-through system: (left) overtaking configuration, (right) see-through effect

Full size image
Overview
In this section, we introduce the overall concept of our see-through system. For the sake of clarity, we will restrict this explanation to a scenario of three vehicles 𝑐1, 𝑐2 and 𝑐3 at a time t, as depicted in Fig. 5. In this configuration, we would like to virtually make “invisible” the occluding cars 𝑐1 and 𝑐2 from the viewpoint of 𝑐3.

The very first stage of our strategy is to determine whether or not connected vehicles are visible in the field of view of each car. To address this problem, we use the poses obtained from our collaborative localization to determine if the activation of the see-through effect is required (see Sect. 7.2). In our example scenario (Fig. 5), two connected vehicles are located in the field of view of the following car, thus, 𝑐3 send a request to the car 𝑐1 and 𝑐2 to generate the see-through effect. As a result, both front cars are independently computing a dense 3D reconstruction of the environment from their stereo vision systems (Geiger et al. 2010). Meanwhile, each preceding vehicle initiates the image synthesis process consisting in warping the left images of 𝑐1 and 𝑐2 to the following car 𝑐3 viewpoint. This warping lead to the generation of a synthetic non-occluded image from the viewpoint of the following car (Sect. 7.3).

To reduce the bandwidth consumption, we send the relevant information only, i.e. the occluded part in the synthetic image. To determine the ROI containing the occlusion, we once again take advantage of the inter-car poses from our localization strategy to compute an approximate localization of the occluding cars in the left image of 𝑐3. Thus, the relevant patch can be cropped, post-processed and transferred to 𝑐3 (Sect. 7.4). Finally, these patches are stitched to the current left image of 𝑐3 to remove the occluded areas (Sect. 7.5). A representative example of all the elements involved in the see-through process between two cars is available in Fig. 6.

Fig. 5
figure 5
See-through process for 3 connected vehicles. In this example, each preceding car (𝑐1 and 𝑐2) is generating a synthetic patch to be transferred to the following car 𝑐3 to overcome the occlusions

Full size image
Fig. 6
figure 6
Depiction of each element involved in the see-through process between two vehicles. a 3D car position and 3D point cloud obtained by dense stereo matching, the left camera of the preceding and following cars are displayed in blue and red respectively. b Left image of the preceding car. c Stereo disparity map from the front car, this disparity map is used to compute the unoccluded synthetic image visible in d. e Occluded patch in the following car viewpoint, this patch has to be replaced by the see-through patch. f, g See-through patch cropped from d, before and after post-processing (hole filling and distance bar). h Left image of the following car where the see-through effect is to be applied, the red bounding box depicts the detection of the front vehicle. i Final result of the see-through system

Full size image
Automatic Occlusion Detection
In our system, the cars do not necessarily follow each other and are assumed to be unordered. Therefore, the see-through system is not constantly needed and has to be activated automatically. To determine if the see-through is necessary, each car checks if any “paired” vehicle (car in the same car group) lies in its frustum of view. If it is the case, a request is sent to the set of occluding vehicles to initiate the see-through. This request also encapsulates the left camera intrinsic parameters of the following car needed to generate the see-through. The car exchanges the see-through messages constantly until the front car leaves the frustum of the rear car, this estimation is updated once every second. Therefore, if a car 𝑐1 overtakes a car 𝑐2, the vehicle 𝑐2 automatically has the ability to see through 𝑐1. To our knowledge, it is the only system able to perform such a complex task (including academic and industrial projects). Technically speaking, checking if a car 𝑐2 is visible in the field of view of a car 𝑐1 is straightforward. Typically, the planes’ equations of the frustum of view of 𝑐1 can be determined from the intrinsic parameter 𝑐1𝐊𝐿, thus, we verify if the camera center 𝑐1𝑐2𝐭 exists inside this frustum. If it is the case, 𝑐2 is occluding the view of 𝑐1 and the see-through between these two cars has to be initiated.

Table 1 Hardware configurations used to test the system
Full size table
Image Synthesis
Image synthesis is the process employed to generate an unoccluded virtual view from the rear car’s viewpoint with images acquired from the preceding car. Similarly to (Rameau et al. 2016b), our image synthesis relies on a multi-threaded stereo disparity computation (Geiger et al. 2010). To reach real-time performance, the stereo images are downscaled by a factor of two. The dense stereo reconstruction is used to warp the image to the following vehicles thanks to the inter-car pose computed from our multi-car localization technique. The re-projection process for a preceding car 𝑐𝑝 and a following car 𝑐𝑓 can be formalized as follows:

𝑐𝑓𝐱𝑘𝐿=𝜋(𝑐𝑓𝐊𝐿,𝑐𝑝𝑐𝑓𝐌𝑡,𝑐𝑝,𝑐𝑝𝐊−1𝐿𝐱𝑘𝐿𝑑𝑘)  ∀𝑘,
(5)
where 𝑑𝑘 is the depth of the pixel k th expressed in left image of the preceding car 𝑐𝑝. A representative example of synthetic view obtained from this process is available in Fig. 6d.

See-Through Patch Detection and Post-Processing
The synthetic image generation allows computing an unoccluded image having the same size as the left image of the following car, as illustrated in Fig. 6d. As highlighted in this figure, only a relatively small portion of the image can be synthesized due to the distance between the vehicles and the limited field of view of the cameras. Moreover, to apply the see-through effect, only the patch containing the occlusion (see Fig. 6e) is relevant. Therefore, to reduce the bandwidth consumption, we propose to send exclusively this synthetic patch (Fig. 6f) such that it can be stitched appropriately. To determine the occluded area, we follow the same process developed in (Rameau et al. 2016b). Assuming a rough estimate of the car’s dimensions (front vehicle) and the position of the camera relative to the vehicle, we can project the 3D position of the 8 corners of the car’s 3D bounding box in the rear car image using the inter-car pose provided by our multi-car localization. The resulting 2D points are used to determine the ROI in the following car viewpoint (see red bounding box in Fig. 6h). Then, this area is cropped, post-processed, and transferred. It is different with the previous work (Rameau et al. 2016b) where the generated patch is directly sent (Fig. 6f) to the rear vehicle.

In this manuscript, multiple modifications have been proposed to improve the quality and relevance of the see-through effect. For instance, the “holes” due to stereo shadow or mismatches are filled by a fast inpainting approach (Liu et al. 2012). Moreover, a color bar indicating the distance between the vehicles is included to provide more information to the driver while covering the part located under the vehicle not visible by the cameras (see Fig. 6f, g).

See-Through Multiple Cars and Stitching
Compared to previous system (Rameau et al. 2016b), we would like to see through n vehicles simultaneously. Thus, the synthetic patches have to be stitched in the proper order to generate the multi-car see-through effect. For this purpose, the distance between the vehicles is computed (using the inter-car poses) and the patch stitching process is sorted from the closest to the furthest car. This technique allows performing the see-through effect such as in Fig. 15. Another practical issue is the synchronization between the synthetic patches received. It is technically complex to received images from multiple car simultaneously. Thus, the patches are sent and stitched as soon as they are received.

Hardware Configuration
All the results presented in this paper have been computed in real-time with our connected vehicles. We have performed multiple demonstrations (see Table 1) under various meteorological conditions, vehicles, hardware setups, countries (Korea and Germany), and environments (rural and urban) (Fig. 7). Moreover, two network configurations have been tested. In the first network configuration, each car is equipped with a standard LTE Internet connection shared from smartphones providing a theoretical bandwidth of 50 Mb/s in download and 5 Mb/s in upload speed (in practice the measured speed is significantly lower). The data is centralized on a central server located in a remote building and connected to a high-speed Internet connection. Practically, a static IP is provided to each car through a private and secured VPN server (Feilner 2006). This network configuration offers the possibility to share information between the vehicles regardless of the distance between them. While this bandwidth satisfies our multi-car localization, it is insufficient to offer a good quality see-through effect (a strong jpeg compression is required).

To cope with this limitation, we have proposed a second configuration, where every car is equipped with a router “Asus AC1900 Dual-Band” connected together via the “AiMesh” technology proposed by the manufacturer. Notice that the see-through results presented in this paper have been acquired from this setup. For this network configuration, the server is located in one of the connected vehicles. Finally, in both configurations, we use a multi-ROS platform to make the system more scalable and flexible. As an alternative solution, we can consider the Dedicated Short Range Communications (DSRC) protocol, which is often preferred for V2V applications (Xu et al. 2017). DSRC provides a secured, robust and long-range communication between cars (even at high speed). Our system has been developed mostly with consumer level communication devices, however, according to our bandwidth evaluation (see Sect. 9.3), the proposed approach remains fully compatible with DSRC solutions.

The stereo-rig cameras have been carefully calibrated using thousands of image pairs. The calibration approach is a modified version of (Ha et al. 2017), which provides a maximum accuracy for the corners’ localization and ensures a mean re-projection error under 0.1 px. As an important detail, the cameras are synchronized with an Arduino Nano guaranteeing a constant frame rate of 15 fps. The quality of the synchronization has been carefully checked following (Wimmer 2005). To avoid illumination issues, the auto-exposure has been adapted to be computed exclusively on the lower part of the image (road surface).

For each car, the onboard computation is performed with a laptop (processor i7-6700HQ 2.6 GHz and 8 GB DDR4 RAM). Notice that no GPU processing is ever exploited and that our system has been optimized for multithreading CPU processing.

Fig. 7
figure 7
(top) Our cars equipped with cameras (Setup 1, Korea), (bottom) Close-up of a stereo camera mounted on one vehicle

Full size image
Fig. 8
figure 8
Mean (left) translational and (right) rotational errors measured from the KITTI dataset comparing Viso2 (Geiger et al. 2011), ORB-SLAM (Mur-Artal and Tardós 2017), VINS-stereo (Qin et al. 2019) and ours

Full size image
Results and Analysis
In this section, we present a series of experiments assessing the accuracy of our SLAM, an analysis of the necessary bandwidth utilized by our system, an accuracy comparison against GPS, and a large number of qualitative results.

SLAM Evaluation
For this project, we have designed a SLAM algorithm that allows estimating both the 3D reconstruction of the scene and the motion of the camera. Our algorithm is tailored to reduce the number of keyframes in the map to lower the bandwidth requirement.

In this section, we quantitatively and qualitatively compare our algorithm against three approaches namely ORB-SLAM (Mur-Artal and Tardós 2017) (without loop-closure), Viso2 (Geiger et al. 2011) and VINS-Stereo (Qin et al. 2019) which are applicable for a stereo vision system. To perform this comparison, we utilize the KITTI dataset (Geiger et al. 2012) which allows comparing vehicular visual odometry techniques against an accurate ground truth. The obtained results are visible in Fig. 8. We can clearly notice that our technique strongly outperforms Viso2 and VINS while it remains very competitive against ORB-SLAM, both in terms of rotation and translation errors. Moreover, our technique only requires 750 points to be detected per image while ORB-SLAM utilizes 2000 points. Also, our SLAM system requires only 40 ms per frame while ORB-SLAM takes more than 60 ms.

A representative set of the computed trajectories are available in Fig. 9. Qualitatively, we can notice that the proposed SLAM is very competitive since our trajectory and the one computed from ORB-SLAM mostly coincide with each other.

Quantitative Multi-Car Localization Evaluation
Estimating the accuracy of our multi-car localization system is a complex task due to the lack of available datasets with ground-truth. Thus, a common practice in evaluating collaborative localization systems is to use a single car/robot dataset and to run multiple agents on overlapping areas (Cieslewski et al. 2017; Rameau et al. 2016b). For this purpose, we propose to simulate multiple cars in the KITTI dataset (Geiger et al. 2012). Inspired by (Rameau et al. 2016b), we simulate two cars following each other by running two agents with a delay between them. In order to estimate the rotational and translational inter-car pose errors (same metric as described in Sect. 9.1) for various distances between vehicles, we ran two experiments on the first 1500 frames of the KITTI seq00. The first experiment consists in simulating two cars following each other closely (up to 20 m), the second one simulates larger distances between vehicles (between 30 and 120 m). The results of these tests are available in Fig. 10. For a large distance between the cars, we can clearly notice that the translational error tends to increase with respect to the distance between the agents. This can be understood by the drift accumulation over time, nonetheless, the error remains under 1.5 m with more than 100 m between the vehicles. For close-by vehicles, such correlation is not distinct with a translational error under 0.5 m for most of the frames. The rotational error remains under 2 degrees for both scenarios.

Fig. 9
figure 9
Top-view trajectories with Viso2 (black), ORB-SLAM (green), ours (blue), and VINS (pink) versus ground-truth (red). KITTI a Seq00, b Seq01 (Color figure online)

Full size image
Fig. 10
figure 10
Translations and rotation errors for two simulated cars on the KITTI dataset, the first row a and b respectively show the translational and rotational errors for short distance between vehicles. The second row depicts the errors for large distances between vehicles

Full size image
Bandwidth Requirement
The multi-car localization system has been designed to reduce the quantity of information to be transferred between the connected vehicles. For this purpose, certain information is exclusively transferred when required. Therefore, it is difficult to provide a clear evaluation of the overall network usage. In this section, though, we individually analyze the messages sent continuously and the services transferred on demand.

BoW Transfer
The only message that is constantly sent by every vehicle to the server is the BoW descriptor computed for every new keyframe. The size of this message depends upon the number of points detected in the keyframe (750 in our case). Every single word requires an integer (4 bytes) for the vocabulary index and a score stored as a float (4 bytes). Alongside this information, a covisibility update is also provided to the server to enforce the spatio-temporal constraint. This covisibility summarizes the relationships between recent keyframes and does not contain more than 100 integers. Therefore, the maximum data to be transferred per keyframe is 6.4 kB. Considering a frame-rate of 15 fps, the maximum bandwidth required to transfer the BoW vectors of one car is 96 kB/s. However, only one or two KFs per second is usually included in the map under normal conditions, leading to a bandwidth usage of about 12.8 kB/s for the BoW transfer.

Services Between Connected Vehicles
Another type of data to be transferred is the local map utilized to estimate the pose between two cars. This local map is transferred to update the pose (reduce the drift between the vehicles) or to find an initial correspondence between two cars. The size of this map can significantly vary depending upon the environment and the number of 2D points detected per frame. In practice, we noticed that this map rarely excessed 3000 points, but in this evaluation, we consider a local 3D map of 5000 points. This 3D map is composed of different elements such as the 3D points coordinate (3 floats per point) and their respective ORB descriptors (32 bytes per point). Thus, the maximum data needed for one transfer is 175 kB.

See-Through Images
Transferring the see-through images is the most demanding part of our system in terms of bandwidth. Therefore, this part is compatible with the LTE connection only if a strong compression of the synthetic patches is applied. Under these conditions, the image is excessively deteriorated to be useful to the driver. As specified in (Rameau et al. 2016b), synthetic patch size can drastically vary with the distance between the vehicles. Assuming a distance of 2 m, a patch of 300×300 px is generated (with our hardware configuration). After a jpeg compression, this patch reaches a size of around 50 kB, thus, for 15 fps, a bandwidth of 750 kB/s is needed. In our case, multiple cars can be made transparent simultaneously, thus, if the see-through effect has to be applied to three cars, a bandwidth of 2.25 MB/s is required. It should be noticed that it remains largely compatible with most V2V network system (3.4 MB/s with DSRC).

Delay Evaluation
In this section, we propose a realistic evaluation of the transfer delay between multiple cars. This evaluation covers different types of data (i.e. poses transfer and see-through patches transfer) and two different network configurations (i.e. LTE Internet connection and Wi-Fi). The transfer has been tested between two vehicles driving at close distance and following each other on the same lane. The delays reported in Fig. 11 have been measured using ROS time-stamp after an NTP clock synchronization between the computers.

For the pose transfer, we take into consideration the entire processing time plus the transfer time. The mean delay over a sequence of 2000 frames is around 45 ms through Wi-Fi (mostly due to SLAM tracking processing time) while the LTE connection suffers from about 140 ms delay in average. Concerning the see-through, only the transfer of the images is considered (with a jpeg compression quality factor set to 0.9), through Wi-Fi the image transfer takes about 7 ms while the delay through a portable Internet connection is about 100 ms.

Additionally, we would like to state the effect of network delay on the overall system performance. For the multicar localization system, both Wi-Fi and LTE lead to very similar results. In practice, LTE should perform relatively worse since the slower transfer of local 3D maps leaves more time for drift accumulation. However, this phenomenon does not seem predominant nor perceptible from a human standpoint. For the see-through, this delay is significantly more problematic because images require high bandwidth to be transferred to other vehicles. Also, the warped see-through pictures have to be transferred to multiple vehicles simultaneously. Thus, this delay can cause a large mis-synchronization between the actual position of the car and the augmented see-through patch stitched in the image.

Fig. 11
figure 11
Transmission delay a Inter-car poses, b See-through image

Full size image
See-Through Quality Versus Localization Accuracy
In this experiment, we propose to study the influence of the pose accuracy on the quality of the see-through effect. For this purpose, we simulate two cars following each other in the first sequence of the KITTI dataset (with 15 frames delay between cars). Using the ground-truth pose, a “perfect” see-through effect between the two vehicles can be computed. By adding noise to the ground-truth pose we can simulate inaccurate inter-car pose estimation. In order to analyse the effect of the pose accuracy on the see-through effect, we propose to calculate the image alignment quality using the Peak Signal to Noise Ratio (PSNR) score between the “noisy” see-through patch and the real patch (ground-truth). For completeness, we provide the Intersection Over Union (IOU) between the noisy front car bounding box and the ground-truth one. The results are represented as heatmaps in Fig. 12, in which, each cell contains the mean PSNR and IOU value (computed on 4541 images) for different noise magnitude on the translation (in a range from 0 to 1 m) and rotation (in a range from 0∘ to 5.5∘) component of the pose. We can notice a quick degradation of the image alignment as the pose quality decreases. For instance, a pose inaccuracy of 3∘ leads to an IOU of 0.6. Under this condition, the generated see-through patch is significantly shifted and could impair the performance of the system. Moreover, we can notice that the rotational component has a stronger impact on the quality of the see-through effect since it leads to a stronger misalignment of the generated patch. In the results obtained from real scenarios (see Sect. 9.8) the alignment is well preserved which denotes an accurate pose estimation between vehicles.

Fig. 12
figure 12
Evaluation of the see-through quality against pose accuracy using two simulated car in the KITTI seq00. Each cell corresponds to the mean value computed on all the frames in the sequence. a PSNR score for different rotation and translation errors, b IOU score for different rotation and translation errors

Full size image
Accuracy Comparison Against GPS
To assess the actual accuracy of our multi-car localization, we propose a quantitative comparison against GPS+IMU measurements fused through an auto-adaptive Kalman filter. For this experiment, four vehicles have been utilized (Setup 2 in Table 1 with one additional vehicle), each car is equipped with a GPS (Lord 3DM-GX5-45) and with our system (see Sect. 8). The cars were driving at various distances between them (from a few meters to more than 200 m) and taking different paths, the GPS trajectories are provided in Fig. 13a. The GPS measurements aligned with our results are visible in Fig. 13b. Qualitatively, good preservation of the scale and trajectories is observed.

To provide a more comprehensive evaluation, we compare the distance between the vehicles obtained via GPS+IMU and through our method in Fig. 13c. These results have been arbitrarily captured from the “Car1”, we can notice that at the beginning of the sequence, only the “Car2” has been connected since the remaining vehicles have not visited the same path yet. In this experiment, despite the large distances between the connected cars, our algorithm successfully estimates the inter-car distances with a relative error under 5%.

For completeness, we introduce another test where the cars are driving closer (less than 50 m) to each other (see Fig. 13d). Here, the relative error against GPS is under 3 m which is the accuracy limitation of the device.

Fig. 13
figure 13
Comparison against GPS with 4 cars, a GPS trajectories, b Our poses aligned with GPS measurements, c Distances between cars for long distance estimation, d Distances between cars for short distance estimation

Full size image
Comparison with the Previous See-Through System
While (Rameau et al. 2016b) presents a robust and effective see-through car system, it relies on numerous assumptions which makes it hardly expandable to a swarm of vehicles. In this section, we would like to highlight the major differences between our system and the previous work in order to provide a better understanding of the necessity of our multi-car localization approach in the context of multi-car see-through.

Homogeneous Design (Rameau et al. 2016b) assumes two vehicles following each other in a given and fixed master-slave configuration. This limitation is inherent to its heterogeneous hardware and software design. On the contrary, we propose a fully homogeneous configuration for every connected vehicle which brings significantly higher flexibility and allows resolving the problem of car overtaking. Our system is distributed and each car can simultaneously generate and receive see-through images allowing to see-through more than one vehicle in real-time.

Bandwidth Consumption If Rameau et al. (2016b) was extended to multiple cars, the bandwidth consumption would be 2.8 Mbits/s per pair of cars, exclusively for transferring local 3D maps (no see-through). Therefore, for 3 connected vehicles, a minimum bandwidth of 5.6 Mbits/s per vehicle is required. With our newly developed system, the local 3D maps are sent episodically, limiting the bandwidth consumption from approximately 0,48 Mbits/s to 1.50 Mbits/s per vehicle (for localization). This reduced bandwidth consumption makes our system compatible with internet connection and most V2V communication systems.

Computational Load Thanks to its distributed nature, our multi-car localization strategy allows us to drastically reduce the computational load. In particular, Rameau et al. (2016b) initial localization technique relies on a brute-force matching between a 3D point cloud and 2D keypoints extracted on the current following car image. This matching technique is known to be particularly ineffective and would be incompatible with a large number of vehicles located very far apart. In contrast to this technique, our novel method relies on a hierarchical design where the coarse localization is achieved on a deported server via global descriptors (BoW), and the fine localization is performed locally between vehicles, as described in Sect. 6. Moreover, our a posteriori correction strategy allows having virtually no delay related to the inter-car localization.

Visual Odometry Drift In light of the previously mentioned points, it appears that our novel multi-car localization strategy brings a very high versatility to the system. Here, we would like to compare the drift of the proposed SLAM against the RANSAC-Less visual odometry proposed in the previous see-through car system (Rameau et al. 2016b). A low drift is of fundamental importance in the context of see-through as large drift can lead to poor see-through results (Sect. 9.5). To analyze this drift, we ran both algorithms on every training sequence of KITTI and computed the mean drift in terms of rotation and translation for various predefined distances: 10, 20, and 30 m. These distances are selected because they are representative use-cases we can face while using the see-through system.

The obtained results are visible in Fig. 14. In this experiment, we can notice that the proposed solution provides a significantly more accurate rotation estimate for all the sequences. This is an important point since the quality of the rotation is of utter importance for the see-through effect (Sect. 9.5). Regarding the translational accuracy, we notice that the previous approach remains competitive (and even better for certain sequences) for the selected distances. We conjecture that it is because the keyframe-based SLAM (used in this work) does not optimize the pose of intermediate frames unlike visual odometry techniques dealing with consecutive images.

Overall, our SLAM offers similar or better accuracy (especially regarding the rotation) than the previous system. Additionally, the proposed SLAM is significantly more flexible since it can be extended to a large number of cars, unlike the previous approach.

Fig. 14
figure 14
Drift estimation for the rotation and translation computed from our method and the RANSACLess approach presented in Rameau et al. (2016b). a Mean rotational drift expressed in deg/m, b mean translational drift in meters. The drift has been computed on every training sequence of the KITTI dataset, for three representative distances: 10, 20, and 30 m

Full size image
Our Real-Time See-Through Results
In this section, we present the see-through results obtained in real-time in two different countries, with different vehicles, and set of cameras. Figure 15 contains a set of representative results obtained in South Korea (KAIST Campus) from three connected vehicles. These results have been captured by the same vehicle following the two other cars. Multiple scenarios have been covered in Fig. 15a, c, f where the cars follow each other in the same lane allowing to see-through two other cars recursively. For instance, in Fig. 15a a car is clearly distinguishable through the two connected vehicles present in the field of view. Furthermore, in the same image, we can also notice that the driver of the very front vehicle is opening a car door. In Fig. 15c, the see-through effect remains geometrically consistent across the three images even for fast moving objects—such as the car turning in front.

Fig. 15
figure 15
Representative set of see-through car results: (left column) see-through results; (right column) top view of the 3D map and 3D position of the connected vehicles

Full size image
Fig. 16
figure 16
Representative set of see-through car results: (left column) original image for each three vehicles; (middle column) see-through results; (right column) top view of the map including the three cars

Full size image
Fig. 17
figure 17
Representative set of see-through car results. (left column) Original image for each 3 vehicles, (middle column) See-through results, (right column) Top view of the map including the 3 cars

Full size image
We also present another interesting configuration in Fig. 15b, d, e), where the two front cars are located on different lanes and are about to perform an overtaking. Here, our system can display a see-through for the two cars simultaneously, providing an unoccluded perception across the width of the road. Even under complex scenarios, our system performs adequately. For instance, in Fig. 15e, two front cars are located on separate lanes at different distances. Since our approach has been developed to be compatible with any scenario, even sharp turns and slopes are managed well (see Fig. 15d). On the right side of Fig. 15, where the top view of the 3D map including the positions of the connected vehicle is provided, we can notice that this display is systematically consistent with the position of the vehicles in the world. Note that the 3D boxes are significantly bigger than the cars’ real size to ensure a better visualization. The color bar at the bottom of each synthetic see-through view represents the distance between the vehicle (red: close cars and purple: far away vehicles). This area is actually impossible to synthesize since it corresponds to the area located under the cars. Certain extreme cases may contain artifacts due to the field of view limitation of the cameras and the disparity discrepancy on the border of the images.

Figure 16 presents sequences obtained in a rural area of Germany. For this experiment, we recorded the results from every single vehicle. The left column of the figure contains the input left image of the stereo rig for the car 1, 2, and 3 respectively. The middle column shows the see-through result computed online, and the third column contains the 3D position of the vehicles on the map. For the sake of readability, we circle the position of each vehicle in the 3D map (𝑐1, 𝑐2 and 𝑐3 are respectively in purple, red and blue). In this set of results, we can notice that the cars are not always in the same order since we have performed multiple overtaking maneuvers during the sequence (for instance, an overtaking is visible in Figs. 16a, 17).

Future Work and Conclusion
In this paper, we have presented an approach to localize multiple connected cars in real-time. For this purpose, we have developed a distributed system where only a small amount of data is centralized. Our method proved its versatility under various conditions (different vehicles, countries, and networks) and accuracy against GPS. Thanks to a posteriori pose estimation, our system works with a very low delay—expanding the applicability of the approach. Additionally, we have included a novel multi-car see-through system by taking advantage of our multi-car localization strategy. The accuracy of this see-through system underlines the efficiency and accuracy of the proposed car localization and brings a novel application for assisted driving. This work paves the way toward various applications, such as collaborative collision detection or collaborative object detection and localization.