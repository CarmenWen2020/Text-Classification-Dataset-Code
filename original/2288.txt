In this paper, we address the semantic segmentation task with a new context aggregation scheme named object context, which focuses on enhancing the role of object information. Motivated by the fact that the category of each pixel is inherited from the object it belongs to, we define the object context for each pixel as the set of pixels that belong to the same category as the given pixel in the image. We use a binary relation matrix to represent the relationship between all pixels, where the value one indicates the two selected pixels belong to the same category and zero otherwise. We propose to use a dense relation matrix to serve as a surrogate for the binary relation matrix. The dense relation matrix is capable to emphasize the contribution of object information as the relation scores tend to be larger on the object pixels than the other pixels. Considering that the dense relation matrix estimation requires quadratic computation overhead and memory consumption w.r.t. the input size, we propose an efficient interlaced sparse self-attention scheme to model the dense relations between any two of all pixels via the combination of two sparse relation matrices. To capture richer context information, we further combine our interlaced sparse self-attention scheme with the conventional multi-scale context schemes including pyramid pooling (Zhao et al. 2017) and atrous spatial pyramid pooling (Chen et al. 2018). We empirically show the advantages of our approach with competitive performances on five challenging benchmarks including: Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff.

Access provided by University of Auckland Library

Introduction
Semantic segmentation is a fundamental topic in computer vision and is critical for various scene understanding problems. It is typically formulated as a task of predicting the category of each pixel, i.e., the category of the object that the pixel belongs to. We are mainly interested in improving the pixel classification accuracy through explicitly identifying the object region that the pixel belongs to.

Extensive efforts based on deep convolutional neural networks have been made to address the semantic segmentation since the pioneering approach of the fully convolutional network (FCN) (Shelhamer et al. 2017). The original FCN approach suffers from two main drawbacks including the reduced feature resolution that loses the detailed spatial information and the small effective receptive field that fails to capture long-range dependencies. There exist two main paths to tackle the above drawbacks: (i) raising the resolution of feature maps for improving the spatial precision or maintaining a high-resolution response map through all stages, e.g., through dilated convolutions (Chen et al. 2018; Yu and Koltun 2016), decoder network (Ronneberger et al. 2015; Badrinarayanan et al. 2017) or high-resolution networks (Sun et al. 2019a, b). (ii) exploiting the global context to capture long-range dependencies, e.g., ParseNet (Liu et al. 2015), DeepLabv3 (Chen et al. 2018), and PSPNet (Zhao et al. 2017). In this work, we focus on the second path and propose a more efficient context scheme. We define the context of a pixel as a set of selected pixels and its context representation as an aggregation of all selected pixelsâ€™ representations if not specified.

Most previous representative studies mainly exploit the multi-scale context formed from spatially nearby or sampled pixels. For instance, the pyramid pooling module (PPM) in PSPNet (Zhao et al. 2017) divides all pixels into multiple regions and selects all pixels lying in the same region with a pixel as its context. The atrous spatial pyramid pooling module (ASPP) in DeepLabv3 (Chen et al. 2017) selects the surrounding pixels of a pixel with different dilation rates as its context. Therefore, the selected pixels of both PPM context and ASPP context tend to be the mixture of object pixels, relevant background pixels and irrelevant background pixels. Motivated by the fact that category of each pixel is essentially the category of the object that it belongs to, we should enhance the object pixels that constitute the object.

To explicitly emphasize the contribution of the object pixels, we present an object context that aims at only gathering the pixels that belong to the same category as a given pixel as its context. Compared to the conventional multi-scale context schemes, our object context pays more attention to the necessary object information. Although estimating the accurate object context is not an easy task, we empirically find that a coarse estimation of the object context already outperforms both PPM and ASPP schemes on various benchmarks.

For a given pixel, we can use a binary vector to record pixels that belong to the same category as it with 1 and 0 otherwise. Thus, a binary relation matrix of ğ‘Ã—ğ‘ can be used to record the pair-wise relations between any two of N pixels. Since computing the binary relation matrix is intractable, we use a dense relation matrix to serve as a surrogate of it, in which each relation value is computed based on the high-level featuresâ€™ inner-product similarities. Therefore, the relation value of the semantically similar pixels tend to be larger. In our implementation, we use the conventional self-attention scheme (Vaswani et al. 2017) to predict the dense relation matrix, which requires îˆ»(ğ‘2) computation complexity. To address the efficiency problem, we propose a new interlaced sparse self-attention scheme that significantly improves the efficiency while maintaining the performance via two sparse relation matrices to approximate the dense relation matrix. To illustrate that our approach is capable of enhancing the object pixels, we show some examples of the predicted dense relation matrices in Fig. 1, where the relation values on the object pixels are larger than the relation values on the background pixels.

Fig. 1
figure 1
Illustrating the predicted dense relation matrices. The first column illustrates example images sampled from the Cityscapes val, and we mark three pixels on object car, person and road with âœ™ respectively. The second column illustrates ground truth segmentation maps. The third column illustrates the dense relation matrices (or approximated object context maps) of the three pixels. We can see that the relation values corresponding to the pixels belonging to the same category as the selected pixel tend to be larger

Full size image
We further illustrate two extensions that capture richer context information: (i) pyramid object context, which estimates the object context within each sub-region generated by the spatial pyramid partitions following the PPM (Zhao et al. 2017). (ii) atrous spatial pyramid object context, which combines ASPP (Chen et al. 2017) with the object context. We summarize our main contributions as following:

We present a new object context scheme that explicitly enhances the object information.

We propose to instantiate the object context scheme with an efficient interlaced sparse self-attention that significantly decreases the complexity compared to the conventional self-attention scheme.

We construct the OCNet based on three kinds of object context modules and achieve competitive performance on five challenging semantic segmentation benchmarks including Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff.

Related Work
Resolution
Earlier studies based on conventional FCN (Shelhamer et al. 2017) apply the consecutive convolution striding and pooling operations to extract low-resolution feature map with high-level semantic information. For example, the output feature map size of ResNet-101 is 132 of the input image, and such significant loss of the spatial information is one of the main challenges towards accurate semantic segmentation. To generate high-resolution feature map without much loss of the semantic information, many efforts (Chen et al. 2017; Ronneberger et al. 2015; Sun et al. 2019a; Yu and Koltun 2016; Badrinarayanan et al. 2017) have proposed various efficient mechanisms. In this paper, we adopt the dilated convolution (Yu and Koltun 2016; Chen et al. 2017) on ResNet-101 to increase the output stride from 32 to 8 by following the same settings of PSPNet (Zhao et al. 2017). Besides, we also conduct experiments based on the recent HRNet (Sun et al. 2019a) with output stride 4. We empirically verify that our approach is more efficient than the conventional multi-scale context mechanism, PPM and ASPP, with high resolution output feature map. More detailed comparisons are summarized in Table 4.

Context
Context plays an important role in various computer vision tasks and it is of various forms such as global scene context, geometric context, relative location, 3D layout and so on. Context has been investigated for both object detection (Divvala et al. 2009) and part detection (Gonzalez-Garcia et al. 2018).

The importance of context for semantic segmentation is also verified in the recent works (Liu et al. 2015; Zhao et al. 2017; Chen et al. 2017; Shetty et al. 2019). It is common to define the context as a set of pixels in the literature of semantic segmentation. Especially, we can divide most of the existing context mechanisms into two kinds: (i) nearby spatial context: ParseNet (Liu et al. 2015) treats all pixels over the whole image as the context, and PSPNet (Zhao et al. 2017) performs pyramid pooling over sub-regions of four pyramid scales and all pixels within the same sub-region are treated as the context for the pixels belonging to the sub-region. (ii) sampled spatial context: DeepLabv3 (Chen et al. 2017) applies multiple atrous convolutions with different atrous rates to capture spatial pyramid context information and regards these spatially regularly sampled pixels as the context.

These two kinds of context are defined over regular rectangle regions and might carry pixels belonging to the background categories. Different from them, our object context is defined as the set of pixels belonging to the same object category, emphasizing the object pixels that are essential for labeling the pixel. There also exist some con-current efforts (Fu et al. 2019a; Zhang et al. 2019b; Huang et al. 2019; Li et al. 2019; Zhao et al. 2018) that exploit the semantic relations between pixels to construct the context, and our approach is different from most of them as we propose a simple yet effective interlaced sparse mechanism to model the relational context with smaller computation cost.

Attention
Self-attention (Vaswani et al. 2017) and non-local neural network (Wang et al. 2018b) have achieved great success on various tasks with its efficiency on modeling long-range contextual information. The self-attention scheme (Vaswani et al. 2017) calculates the context at one position as a aggregation of all positions in a sentence (at the encoder stage). Wang et al.  further proposed the non-local neural network (Wang et al. 2018b) for vision tasks such as video classification, object detection and instance segmentation based on self-attention scheme.

Our implementation is inspired by the self-attention scheme. We first apply the self-attention scheme to predict the dense relation matrix and verify its capability to approximate the object context, and there also exist some concurrent studies (Fu et al. 2019a; Zhang et al. 2019b) that apply the self-attention scheme for semantic segmentation. Some recent efforts (Huang et al. 2019; Yue et al. 2018; Zhu et al. 2019) propose different mechanisms to decrease the computation complexity and memory consumption of self-attention scheme. For example, CGNL (Yue et al. 2018) (Compact Generalized Non-local) applies the Taylor series of the RBF kernel function to approximate the pair-wise similarities, RCCA (Huang et al. 2019) (Recurrent Criss-Cross Attention) applies two consecutive criss-cross attention to approximate the original self-attention scheme.

Our interlaced sparse self-attention scheme is different from both CGNL and RCCA through factorizing the dense relation matrix to two sparse relation matrices, and we find that similar mechanisms have been applied in the previous studies on network architecture design including ShuffleNet (Ma et al. 2018) and Interleaved Group Convolution (Xie et al. 2018; Zhang et al. 2017b). The concurrent sparse transformer (Child et al. 2019) also apply the similar mechanism on one dimensional text/audio related tasks that require sequential masked inputs.

Approach
We introduce our approach with four subsections. First, we introduce the general mathematical formulation of the context representation and the definition of object context (Sect. 3.1). Second, we instantiate the object context with the conventional self-attention (SA) (Vaswani et al. 2017) and our interlaced sparse self-attention (ISA) (Sect. 3.3). Third, we present the pyramid extensions of object context (Sect. 3.4). Last, we illustrate the overall pipeline and the implementation details of OCNet (Sect. 3.5).

Formulation
Preliminary
We define the general mathematical formulation of the context representation as:

ğ³ğ‘–=ğœŒ(1|îˆµğ‘–|âˆ‘ğ‘—âˆˆîˆµğ‘–ğ›¿(ğ±ğ‘—)).
(1)
We use ğ— and ğ™ to represent the input representation and the context representation respectively. ğ±ğ‘— is the j-th element of ğ— and ğ³ğ‘– is the i-th element of ğ™. ğ›¿(â‹…) and ğœŒ(â‹…) are two different transform functions. îˆµ={1,â‹¯,ğ‘} represents a set of N pixels. We use îˆµğ‘– to represent a subset of îˆµ, in other words, îˆµğ‘– is the set of context pixels for pixel i. We show how îˆµğ‘– selects pixels in the following discussions. Intuitively, the above formula of context representation is to describe a pixel with the weighted average representations of a set of relevant pixels.

The above mathematical formulations are based on the one-dimensional case for convenience and can be generalized to the higher dimensional cases easily. One of the main differences between the existing representative context methods is the formulation of the context îˆµğ‘–.

Multi-Scale Context
Zhao et al. (2017) proposes the pyramid pooling (PPM) context that constructs the îˆµğ‘– as a set of spatially-close pixels around pixel i within the regular regions of different scales. Chen et al. (2017) introduces the atrous spatial pyramid pooling (ASPP) context that estimates îˆµğ‘– as a set of sparsely sampled pixels with different dilation rates around pixel i.

We take the representative multi-scale context scheme ASPP as an example to illustrate the formulation of îˆµğ‘–:

îˆµğ‘–=â‹ƒğ‘Ÿâˆˆ{12,24,36}{ğ‘—âˆˆîˆµ | |ğ‘–âˆ’ğ‘—|=ğ‘Ÿ},
(2)
where ğ‘Ÿâˆˆ{12,24,36} is the dilation rate, |ğ‘–âˆ’ğ‘—| represents the spatial distances between pixel i and j, and îˆµğ‘– is defined over one-dimensional input. Therefore, the ASPP context of pixel i is a set of sampled pixels that have the predefined spatial distances with i. Besides, we illustrate the formulation of îˆµğ‘– based on PPM scheme in Appendix A.

Both kinds of context tend to be a mixture of object pixels and background pixels. Therefore, they have not explicitly enhanced the contribution from the object pixels. Motivated by the fact that the category of each pixel is essentially inherited from the category of the object that it lies in, we propose a new scheme named object context to explicitly enhance the information of object pixels.

Object Context
We define the object context for pixel i as:

îˆµğ‘–={ğ‘—âˆˆîˆµ|ğ‘™ğ‘—=ğ‘™ğ‘–},
(3)
where ğ‘™ğ‘– and ğ‘™ğ‘— are the label of pixel i and j respectively. We can see the object context for pixel i is essentially a set of pixels that belong to the same object category as i.

We can represent the pairwise relations between any two of N pixels (encoded in the ground-truth object context) with a binary relation matrix of ğ‘Ã—ğ‘, where the i-th row records all pixels belonging to the same category with pixel i with 1 and 0 otherwise. Especially, the binary relation matrix only encodes partial information of the ground-truth object context, i.e., the (same category) label co-occurring relations. In other words, all classes could be permuted and the binary relation matrix would be unchanged.

Considering it is intractable to estimate the binary relation matrix, we propose to use a dense relation matrix to serve as a surrogate of the binary relation matrix. We expect that the relation values between the pixels belonging to the same object category are larger than the ones belonging to different categories, thus, the contributions of the object pixels are enhanced.

In the following discussions, we first illustrate the formulation of the dense relation scheme that directly estimates the dense relation matrix ğ– of size ğ‘Ã—ğ‘. Second, to improve efficiency, we propose a sparse relation scheme that factorizes the dense relation matrix as the combination of two sparse relation matrices including ğ–ğ‘™ and ğ–ğ‘”, where both sparse relation matrices are of size ğ‘Ã—ğ‘. More details are illustrated as follows.

Dense Relation
The dense relation scheme estimates the relations between each pixel i and all pixels in îˆµ. We illustrate the context representation based on dense relation:

ğ³ğ‘–=ğœŒ(âˆ‘ğ‘—âˆˆîˆµğ‘¤ğ‘–ğ‘—ğ›¿(ğ±ğ‘—)),
(4)
where ğ‘¤ğ‘–ğ‘— is the relation value between pixel i and j, i.e., the element of ğ– at coordinates (i, j). As we need to estimate the relations between i and all pixels in îˆµ directly, the computational complexity of estimating ğ– is quadratic to the input size: îˆ»(ğ‘2).

Sparse Relation
The sparse relation scheme only estimates the relations between the pixel i and two subsets of selected pixels following the â€œinterlacing (a.k.a. interleaving) methodâ€ (Greenspun 1999; Roelofs and Koman 1999). We illustrate the context representation based on sparse relation:

ğ³ğ‘”ğ‘–=ğœŒâ›ââœâœâˆ‘ğ‘—âˆˆîˆµğ‘”ğ‘–ğ‘¤ğ‘”ğ‘–ğ‘—ğ›¿(ğ±ğ‘—)ââ âŸâŸ,
(5)
ğ³ğ‘™ğ‘–=ğœŒâ›ââœâœâˆ‘ğ‘—âˆˆîˆµğ‘™ğ‘–ğ‘¤ğ‘™ğ‘–ğ‘—ğ›¿(ğ³ğ‘”ğ‘—)ââ âŸâŸ.
(6)
We use the superscript g / l to mark the operators and operations associated with the global / local relation stage respectively. For example, ğ³ğ‘”ğ‘– / ğ³ğ‘™ğ‘– represents the context representation after the global / local relation stage of the i-th pixel. Refer to Sect. 3.3 for more details. ğ‘¤ğ‘”ğ‘–ğ‘— / ğ‘¤ğ‘™ğ‘–ğ‘— is the relation between pixel i and pixel j that belongs to îˆµğ‘”ğ‘– / îˆµğ‘™ğ‘– respectively. îˆµğ‘”ğ‘– and îˆµğ‘™ğ‘– are the selected context pixels:

îˆµğ‘”ğ‘–={ğ‘—âˆˆîˆµ:ğ‘—â‰¡ğ‘–(modğ‘ƒ)},
(7)
îˆµğ‘™ğ‘–={ğ‘—âˆˆîˆµ:âŒŠğ‘—âˆ’1ğ‘ƒâŒ‹=âŒŠğ‘–âˆ’1ğ‘ƒâŒ‹},
(8)
where îˆµğ‘”ğ‘– / îˆµğ‘™ğ‘– is a subset of pixels with the same remainder / quotient as the pixel i when divided by P respectively. Both i and j in the above illustrations represent the spatial positions of pixel i and j in the one-dimensional case. P represents the group number in the global relation stage (Sect. 3.3) and it determines the selection of context pixels. The main advantage of the sparse relation scheme lies at we only need to estimate the relation values between pixel i and îˆµğ‘”ğ‘–âˆªîˆµğ‘™ğ‘– instead of îˆµ, thus, saves a lot of computation cost.

Fig. 2
figure 2
Illustrating the Interlaced Sparse Self-Attention. Our approach is consisted of a global relation module and a local relation module. The feature map in the left-most/right-most is the input/output. First, we color the input feature map ğ— with four different colors. We can see that there are 4 local groups and each group is consisted of four different colors. For the global relation module, we permute and group (divide) all positions with the same color having long spatial interval distances together in ğ—, which outputs ğ—ğ‘”. Then, we divide the ğ—ğ‘” into 4 groups and apply the self-attention on each group independently. We merge all the updated feature map of each group together as the output ğ™ğ‘”. For the local relation module, we permute the ğ™ğ‘” to group the originally nearby positions together and get ğ—ğ‘™. Then we divide and apply self-attention following the same manner as global relation, obtain the final feature map ğ™ğ‘™. We can propagate the information from all input positions to each output position with the combination of the global relation module and the local relation module. The positions with the same saturation of color mark the value of the feature maps are kept unchanged. We only increase the saturation the color when we update the feature maps with self-attention operation

Full size image
Considering the pixels with equal remainder / quotient share the same îˆµğ‘”âˆ— / îˆµğ‘™âˆ—, thus, we ignore the subscript and use îˆµğ‘” / îˆµğ‘™ to represent all groups of pixels that share equal remainder / quotient respectively. We compute (part of) the sparse relation matrices ğ–ğ‘” / ğ–ğ‘™ within each group of pixels in îˆµğ‘” / îˆµğ‘™ respectively. Especially, both ğ–ğ‘” and ğ–ğ‘™ are sparse block matrices and we can approximate the original dense relation matrix as the product of these two sparse relation matrices:

ğ–=ğ–ğ‘™ğâŠ¤ğ–ğ‘”ğ,
(9)
where we use ğ to represent a permutation matrix of size ğ‘Ã—ğ‘ that ensures the pixel orderings of the two sparse relation matrices are matched and ğâŠ¤ is the transpose of ğ. We illustrate the definition of each value ğ‘ğ‘–,ğ‘— in ğ in Appendix B and why the sparse relation scheme is more efficient than the dense relation scheme in Appendix C.

Instantiations
We explain the specific instantiations of dense relation and sparse relation based on the self-attention and the interlaced sparse self-attention respectively.

Self-Attention
The implementation of dense relation scheme based on self-attention is illustrated as following,

ğ–=Softmax(ğœƒ(ğ—)ğœ™(ğ—)âŠ¤ğ‘‘â€¾â€¾âˆš),
(10)
ğ™=ğœŒ(ğ–ğ›¿(ğ—)),
(11)
ğ—âˆˆâ„ğ‘Ã—ğ¶in is the input representation, ğ–âˆˆâ„ğ‘Ã—ğ‘ is the dense relation matrix, and ğ™âˆˆâ„ğ‘Ã—ğ¶out is the output representation. We assume ğ¶in=ğ¶out=ğ¶ in the following discussion for convenience. ğœƒ and ğœ™ are two different functions that transform the input to lower dimensional space and ğœƒ(ğ—),ğœ™(ğ—)âˆˆâ„ğ‘Ã—ğ¶2. The inner product in the lower dimensional space is used to compute the dense relation matrix ğ–. The scaling factor d is used to to solve the small gradient problem of softmax function according to Vaswani et al. (2017) and we set ğ‘‘=ğ¶2. Self-attention uses the function ğœŒ and ğ›¿ to learn a better embedding and we have ğœŒ(â‹…)âˆˆâ„ğ‘Ã—ğ¶ and ğ›¿(â‹…)âˆˆâ„ğ‘Ã—ğ¶2. According to the original description of self-attention in  Vaswani et al. (2017), we can also call ğœƒ, ğœ™ and ğ›¿ as query-, key-, and value-transform function respectively.

We implement both ğœƒ(â‹…) and  ğœ™(â‹…) with two consecutive groups of 1Ã—1 convâ†’BNâ†’ReLU. BN is the abbreviation for batch normalization (Ioffe and Szegedy 2015) that synchronize the statistics. We implement ğ›¿(â‹…) and  ğœŒ(â‹…) with 1Ã—1 conv. Specifically, ğœƒ(â‹…),  ğœ™(â‹…) and ğ›¿(â‹…) halve the input channels while  ğœŒ(â‹…) doubles the input channels.

Interlaced Sparse Self-Attention
The implementation of the sparse relation scheme, i.e., the interlaced sparse self-attention, first divides all pixels into multiple subgroups and then applies the self-attention on each subgroup to compute the sparse relation matrices, i.e., ğ–ğ‘” and ğ–ğ‘™, and the context representations.

We illustrate the overall pipeline of the interlaced sparse self-attention scheme with a two dimensional example in Fig. 2, where we estimate ğ–ğ‘” with the global relation module and ğ–ğ‘™ with the local relation module. With the combination of these two sparse relation matrices, we can approximate the dense relations between any two of all pixels, which is explained with an example in Appendix D.

Global relation. We divide all positions into multiple groups with each group consists of a subset of sampled positions according to the definition of îˆµğ‘”. Considering that the pixels within each group are sampled based on the remainder divided by the number of groups P and they are distributed across the global image range, thus, we call it global relation.

We first permute the input feature map ğ—:

ğ—ğ‘”=Permute(ğ—)=ğğ—,
(12)
Second, we divide ğ—ğ‘” into P groups with each group containing Q neighboring positions (ğ‘=ğ‘ƒÃ—ğ‘„):

ğ—ğ‘”âˆ’â†’âˆ’âˆ’âˆ’Divide{ğ—ğ‘”1,ğ—ğ‘”2,â‹¯,ğ—ğ‘”ğ‘ƒ},
(13)
where each ğ—ğ‘”ğ‘âˆˆâ„ğ‘„Ã—ğ¶ is a subset of ğ—ğ‘” and we have ğ—ğ‘”=[ğ—ğ‘”1âŠ¤,ğ—ğ‘”2âŠ¤,â‹¯,ğ—ğ‘”ğ‘ƒâŠ¤]âŠ¤. Third, we apply the self-attention on each ğ—ğ‘”ğ‘ independently:

ğ–ğ‘”ğ‘=Softmax(ğœƒ(ğ—ğ‘”ğ‘)ğœ™(ğ—ğ‘”ğ‘)âŠ¤ğ‘‘â€¾â€¾âˆš),
(14)
ğ™ğ‘”ğ‘=ğœŒ(ğ–ğ‘”ğ‘ğ›¿(ğ—ğ‘”ğ‘)),
(15)
where ğ–ğ‘”ğ‘âˆˆâ„ğ‘„Ã—ğ‘„ is a small dense relation matrix based on all positions from ğ—ğ‘”ğ‘, ğ™ğ‘”ğ‘âˆˆâ„ğ‘„Ã—ğ¶ is the updated representation based on ğ—ğ‘”ğ‘, and d takes the same value as in the previous Equation 10. We apply the same implementation for all transform functions including ğœƒ(â‹…),  ğœ™(â‹…), ğ›¿(â‹…) and  ğœŒ(â‹…) following the implementation of self-attention. We illustrate the overall sparse relation matrix ğ–ğ‘” in the global relation stage:

ğ–ğ‘”=â¡â£â¢â¢â¢â¢â¢ğ–ğ‘”10â‹®00ğ–ğ‘”2â‹®0â‹¯â‹¯â‹±â‹¯00â‹®ğ–ğ‘”ğ‘ƒâ¤â¦â¥â¥â¥â¥â¥,
(16)
where only the relation values in the diagonal blocks are non-zero. Therefore, we only need to estimate the relation values between pixel pairs belonging to the same group and ignore the relations between pixel pairs from different groups.

We concatenate all ğ™ğ‘”ğ‘ from different groups and get the output representation ğ™ğ‘”=[ğ™ğ‘”1âŠ¤,ğ™ğ‘”2âŠ¤,â‹¯,ğ™ğ‘”ğ‘ƒâŠ¤]âŠ¤ after the global relation stage:

{ğ™ğ‘”1,ğ™ğ‘”2,â‹¯,ğ™ğ‘”ğ‘ƒ}âˆ’â†’âˆ’âˆ’âˆ’Mergeğ™ğ‘”,
(17)
Local relation. In the local relation stage, we divide the positions into multiple groups according to the definition of îˆµğ‘™, where each group of pixels are sampled based on the quotient and they are distributed within the local neighboring range, thus, we call it local relation.

We apply another permutation on the output feature map from the global relation module following:

ğ—ğ‘™=Permute(ğ™ğ‘”)=ğâŠ¤ğ™ğ‘”,
(18)
Then, we divide ğ—ğ‘™ into Q groups with each group containing P neighboring positions:

ğ—ğ‘™âˆ’â†’âˆ’âˆ’âˆ’Divide[ğ—ğ‘™1,ğ—ğ‘™2,â‹¯,ğ—ğ‘™ğ‘„],
(19)
where each ğ—ğ‘™ğ‘âˆˆâ„ğ‘ƒÃ—ğ¶ and we have ğ—ğ‘™=[ğ—ğ‘™1âŠ¤,ğ—ğ‘™2âŠ¤,â‹¯,ğ—ğ‘™ğ‘„âŠ¤]âŠ¤.

We apply the self-attention on each ğ—ğ‘™ğ‘ independently, which is similar with the Equations 14 and  15 in the global relation module. Accordingly, we can get ğ–ğ‘™ğ‘ and ğ™ğ‘™ğ‘, where ğ–ğ‘™ğ‘âˆˆâ„ğ‘ƒÃ—ğ‘ƒ is a small dense relation matrix based on ğ—ğ‘™ğ‘, ğ™ğ‘™ğ‘âˆˆâ„ğ‘ƒÃ—ğ¶ is the updated representation based on ğ—ğ‘™ğ‘. We illustrate the sparse relation matrix computed based on the local relation:

ğ–ğ‘™=â¡â£â¢â¢â¢â¢â¢ğ–ğ‘™10â‹®00ğ–ğ‘™2â‹®0â‹¯â‹¯â‹±â‹¯00â‹®ğ–ğ‘™ğ‘„â¤â¦â¥â¥â¥â¥â¥,
(20)
where the above relation matrix ğ–ğ‘™ based on the local relation is also very sparse and most of the relation values are zero.

We concatenate all ğ™ğ‘™ğ‘ from different groups and get output representation ğ™ğ‘™=[ğ™ğ‘™1âŠ¤,ğ™ğ‘™2âŠ¤,â‹¯,ğ™ğ‘™ğ‘„âŠ¤]âŠ¤ after the local relation stage:

{ğ™ğ‘™1,ğ™ğ‘™2,â‹¯,ğ™ğ‘™ğ‘„}âˆ’â†’âˆ’âˆ’âˆ’Mergeğ™ğ‘™,
(21)
where ğ™ğ‘™ is also the final output representation of interlaced sparse self-attention scheme.

Complexity
Given an input feature map of size ğ»Ã—ğ‘ŠÃ—ğ¶, we analyze the computation/memory cost of both the self-attention mechanism and our interlaced sparse self-attention scheme as follows.

The computation complexity of self-attention mechanism is îˆ»(ğ»ğ‘Šğ¶2+(ğ»ğ‘Š)2ğ¶), and the complexity of our interlaced sparse self-attention mechanism is îˆ»(ğ»ğ‘Šğ¶2+(ğ»ğ‘Š)2ğ¶(1ğ‘ƒâ„ğ‘ƒğ‘¤+1ğ‘„â„ğ‘„ğ‘¤)), where we divide the height dimension into ğ‘ƒâ„ groups and the width dimension to ğ‘ƒğ‘¤ groups in the global relation stage and ğ‘„â„ and ğ‘„ğ‘¤ groups during the local relation stage. We have ğ»=ğ‘ƒâ„ğ‘„â„, ğ‘Š=ğ‘ƒğ‘¤ğ‘„ğ‘¤. The complexity of our approach can be reduced to îˆ»(ğ»ğ‘Šğ¶2+(ğ»ğ‘Š)32ğ¶) when ğ‘ƒâ„ğ‘ƒğ‘¤=ğ»ğ‘Šâ€¾â€¾â€¾â€¾â€¾âˆš. Detailed formulations and proof of the computation complexity are provided in Appendix E. We compare the theoretical GFLOPs of the interlaced sparse self-attention scheme and the conventional self-attention scheme in Fig. 3, where we can see that our interlaced sparse self-attention is much more efficient than the conventional self-attention when processing inputs of higher resolution. We further report the actual GPU memory cost (measured by MB), computation cost (measured by GFLOPs), and inference time (measured by ms) of both mechanisms in Fig. 4 to illustrate the advantage of our method.

Fig. 3
figure 3
FLOPs vs. input size. The x-axis represents the height or width of the input feature map (we assume the height is equal to the width for convenience) and the y-axis represents the computation cost measured with GFLOPs. We can see that the GFLOPs of self-attention (SA) mechanism increases much faster than our interlaced sparse self-attention (ISA) mechanism with inputs of higher resolution

Full size image
Fig. 4
figure 4
GPU memory/FLOPs/Running time comparison between SA and ISA. All numbers are tested on a single Titan XP GPU with CUDA8.0 and an input feature map of 1Ã—512Ã—128Ã—128 during inference stage. The lower, the better for all metrics. We can see that the proposed interlaced sparse self-attention (ISA) only uses 10.2% GPU memory and 24.6% FLOPs while being nearly 2Ã— faster when compared with the self-attention (SA)

Full size image
We present the PyTorch code of the proposed interlaced sparse self-attention in Algorithm 1. We explain the rough correspondence between Fig. 2 and Algorithm 1. For example, in global relation stage, the combination of reshape in line-9 and permute in line-10 of Algorithm 1 corresponds to the Permute in Fig. 2, and the reshape in line-11 of Algorithm 1 corresponds to the Divide in Fig. 2. Our implementation is optimized for the efficiency as we are applying the interlacing operations on the tensors of high dimension. Especially, the permute function (in both line-10 and line-16 of Algorithm 1) does not correspond to the Permute (for one-dimensional situation) illustrated in Fig 2.

figure a
Fig. 5
figure 5
Illustrating the overall framework of OCNet. (a) The overall network pipeline of OCNet: given an input image, we use a backbone to extract the feature map, then we apply an object context module on the feature map and output the contextual feature map. Based on the contextual feature map, we apply a classifier to predict the final segmentation map. (b) Base-OC: we perform an object context pooling (OCP) on the input feature map, then we concatenate the output of OCP and the input feature map as the final output. (c) Pyramid-OC: we apply four parallel OCPs independently. Each branch divides the input to different pyramid scales, and the object context pooling is shared within each branch, then we concatenate the four output feature maps with a new feature map that is generated by increasing the channels of the input feature map by 4Ã— (512â†’2048). (d) ASP-OC: we apply an OCP and four dilated convolutions (these four branches are the same with the original ASPP and the rate represents the dilation rate), then we concatenate the five output feature maps as the output. All the convolutions are followed by a group of BNâ†’ReLU operation

Full size image
Object Context Pooling
We use self-attention or interlaced sparse self-attention to implement the object context pooling module (OCP). The object context pooling estimates the context representation of each pixel i by aggregating the representations of the selected subset of pixels based on the estimated dense relation matrix or the two sparse relation matrices. We first apply the object context pooling module based on either self-attention scheme or the proposed interlaced sparse self-attention scheme to compute the context representation, and then we concatenate the input representation with the context representation as the output representation, resulting in a baseline method named as Base-OC, and we illustrate the details in Fig. 5b.

Pyramid Extensions
To handle objects of multiple scalesFootnote1, we further combine our approach with the conventional multi-scale context schemes including PPM and ASPP.

Combination with PPM
Inspired by the previous pyramid pooling module (Zhao et al. 2017), we divide the input image into regions of four pyramid scales: 1Ã—1 region, 2Ã—2 regions, 3Ã—3 regions and 6Ã—6 regions, and we update the feature maps for each scale by feeding the feature map of each region into the object context pooling module respectively, then we combine the four updated feature maps together. Finally, we concatenate the multiple pyramid object context representations with the input feature map. We call the resulting method as Pyramid-OC. More details are illustrated in Fig. 5c.

Combination with ASPP
The conventional atrous spatial pyramid pooling (Chen et al. 2017) consists of 5 branches including: an image-level pooling branch, a 1Ã—1 convolution branch and three 3Ã—3 dilated convolution branches with dilation rates being 12, 24 and 36, respectively. We replace the image-level pooling branch with the object context pooling to exploit the relation-based object context information, resulting in a method which we name as ASP-OC. More details are illustrated in Fig. 5 (d).

Network Architecture
We illustrate the overall pipeline of our OCNet in Fig. 5 (a). More details are illustrated as follows.

Backbone
We use the ResNet-101 (He et al. 2016) or HRNetV2-W48 (Sun et al. 2019b) pretrained over the ImageNet dataset as the backbone. For the ResNet-101, we make some modifications by following PSPNet (Zhao et al. 2017): replace the convolutions within the last two blocks by dilated convolutions with dilation rates being 2 and 4, respectively, so that the output stride becomes 8. For the HRNetV2-W48, we directly apply our approach on the final concatenated feature map with output stride 4.

Base-OC
Before feeding the feature map into the OCP, we apply a dimension reduction module (a 3Ã—3 convolution) to reduce the channels of the feature maps output from the backbone to 512 for both ResNet-101 and HRNetV2-W48. Then we feed the updated feature map into the OCP and concatenate the output feature map of the OCP with the input feature map to the OCP. We further perform a 1Ã—1 convolution to decrease the channels of the concatenated feature map from 1024 to 512, which is not included in Fig. 5b.

Pyramid-OC
We first apply a 3Ã—3 convolution to reduce the channels to 512 in advance, then we feed the dimension reduced feature map to the Pyramid-OC and perform four different pyramid partitions (1Ã—1 region, 2Ã—2 regions, 3Ã—3 regions, and 6Ã—6 regions) on the input feature map, and we concatenate the four different output object context feature maps output by the four parallel OCPs. Each one of the four object context feature maps has 512 channels. We apply a 1Ã—1 convolution to increase the channel of the input feature map from 512 to 2048 and concatenate it with all four object context feature maps. Lastly, we use a 1Ã—1 convolution on the concatenated feature map with 4096 channels and produce the final feature map with 512 channels , which is not included in Fig. 5c.

ASP-OC
We only perform the dimension reduction within the object context pooling branch, where we use a 3Ã—3 convolution to reduce the channel to 256. The output feature map from object context pooling module has 256 channels. For the other four branches, we exactly follow the original ASPP module and apply a 1Ã—1 convolution within the second above branch and 3Ã—3 dilated convolution with different dilation rates (12, 24, 36) in the three remaining parallel branches. We set the output channel as 256 in all these four branches following the original settings (Chen et al. 2017). Lastly, we concatenate these five parallel output feature maps and use a 1Ã—1 convolution to decrease the channel of the concatenated feature map from 1280 to 256, which is not included in Fig. 5d.

Discussion
The concept of object context is also discussed in our another work: object contextual representations (OCR) (Yuan et al. 2020). The main difference is that this work is focused on efficiently modeling the dense relations between pixel and pixel while OCR mainly exploits the coarse segmentation maps to construct a set of object region representations and models the dense relations between pixel and object regions.

Experimental Results
We evaluate our approach on five challenging semantic segmentation benchmarks. First, we study various components within our approach and compare our approach to some closely related mechanisms (Sect. 4.3). Second, we compare our approach to the recent state-of-the-art methods to verify that we achieve competitive performance (Sect. 4.4). Last, we apply our approach on the conventional Mask-RCNN to verify that our method generalizes well (Sect. 4.5). Besides, we also illustrate the quantitative improvements along the boundary (Table 6) and the qualitative improvements on various benchmarks (Fig. 9) based on our approach.

Datasets
Cityscapes
Cityscapes (Cordts et al. 2016) contains 5, 000 finely annotated images with 19 semantic classes. The images are in 2048Ã—1024 resolution and captured from 50 different cities. The training, validation, and test sets consist of 2, 975,  500,  1, 525 images, respectivelyFootnote2.

ADE20K
ADE20K (Zhou et al. 2017) is very challenging and it contains 22K densely annotated images with 150 fine-grained semantic concepts. The training and validation sets consist of 20K, 2K images, respectivelyFootnote3.

LIP
LIP (Gong et al. 2017) is a large-scale dataset that focuses on semantic understanding of human bodies. It contains 50K images with 19 semantic human part labels and 1 background label for human parsing. The training, validation, and test sets consist of 30K, 10K, 10K images, respectivelyFootnote4.

PASCAL-Context
PASCAL-Context (Mottaghi et al. 2014) is a challenging scene parsing dataset that contains 59 semantic classes and 1 background class. The training set and test set consist of 4, 998 and 5, 105 images, respectivelyFootnote5.

COCO-Stuff
COCO-Stuff (Caesar et al. 2018) is a challenging scene parsing dataset that contains 171 semantic classes. The training set and test set consist of 9K and 1K images, respectivelyFootnote6.

Implementation Details
Training setting. We initialize the parameters within the object context pooling module and the classification head randomly. We perform the polynomial learning rate policy with factor (1âˆ’(ğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘–ğ‘¡ğ‘’ğ‘Ÿğ‘šğ‘ğ‘¥)0.9). We set the weight on the final loss as 1 and the weight on the auxiliary loss as 0.4 following PSPNet (Zhao et al. 2017). The auxiliary loss is applied on the representation output from stage-3 of ResNet-101 or the final representation of HRNetV2-W48. We all use the  (BulÃ² et al. 2018) to synchronize the mean and standard-deviation of batch normalization across multiple GPUs. For the data augmentation, we perform random flipping horizontally, random scaling in the range of [0.5, 2] and random brightness jittering within the range of [âˆ’10,10]. More details are illustrated as following.

For the experiments on Cityscapes: we set the initial learning rate as 0.01, weight decay as 0.0005, crop size as 512Ã—1024 and batch size as 8. For the experiments evaluated on val/test, we set training iterations as 60K/100K on train/train+val respectively.

For the experiments on ADE20K: we set the initial learning rate as 0.02, weight decay as 0.0001, crop size as 520Ã—520, batch size as 16 and training iterations as 150K if not specified.

For the experiments on LIP: we set the initial learning rate as 0.007, weight decay as 0.0005, crop size as 473Ã—473, batch size as 32 and training iterations as 100K if not specified.

For the experiments on PASCAL-Context: we set the initial learning rate as 0.001, weight decay as 0.0001, crop size as 520Ã—520, batch size as 16 and training iterations as 30K if not specified.

For the experiments on COCO-Stuff: we set the initial learning rate as 0.001, weight decay as 0.0001, crop size as 520Ã—520, batch size as 16 and training iterations as 60K if not specified.

Ablation Study
We choose the dilated ResNet-101 as our backbone to conduct all ablation experiments, and we also use ResNet-101 alternatively for convenience. We choose the OCNet with Base-OC (ISA) as our default setting if not specified.

Table 1 Influence of ğ‘ƒâ„ and ğ‘ƒğ‘¤, the order of global relation and local relation within the interlaced sparse self-attention on Cityscapes val
Full size table
Group numbers
In order to study the influence of group numbers within the Base-OC (ISA) scheme, we train the Base-OC (ISA) method by varying ğ‘ƒâ„ and ğ‘ƒğ‘¤, where we can determine the value of ğ‘„â„ and ğ‘„ğ‘¤ according to ğ»=ğ‘ƒâ„Ã—ğ‘„â„ and ğ‘Š=ğ‘ƒğ‘¤Ã—ğ‘„ğ‘¤. In Table 1, we illustrate the results on Cityscapes val. We can see that our approach with different group numbers consistently improves over the baseline and we get the best result with ğ‘ƒâ„=ğ‘ƒğ‘¤=8, therefore, we set ğ‘ƒâ„=ğ‘ƒğ‘¤=8 in all experiments by default setting if not specified.

Global+Local vs. Local+Global
We study the influence of the order of global relation and local relation within Base-OC (ISA) module. We report the results in the 6th row and 10th row of Table 1. We can see that both mechanisms improve over the baseline by a large margin. Applying the global relation first seems to be favorable. We apply the global relation first unless otherwise specified for all our experiments. Besides, we also compare the results with only sparse global attention or only local relation. We report the results (measured by mIoU): only global relation: 78.9% and only local relation: 77.2%, which verifies that the global relation is more important and the local relation is complementary with the global relation.

Comparison to Multi-Scale Context
We compare the proposed relational context scheme to two conventional multi-scale context schemes including: PPM (Zhao et al. 2017) and ASPP (Chen et al. 2017).

Table 2 Comparison with multi-scale context scheme including PPM (Zhao et al. 2017) and ASPP (Chen et al. 2017) on Cityscapes val and ADE20K val
Full size table
We conduct the comparison experiments under the same training/testing settings, e.g., the same training iterations and batch size. We report the related results in Table 2. Our reproduced PPM outperforms the original reported performance (Ours: 78.5% vs. Paper (Zhao et al. 2017): 77.6%). Our approach consistently outperforms both PPM and ASPP on the evaluated benchmarks including Cityscapes and ADE20K. For example, Base-OC (ISA) outperforms the PPM by 0.99%/0.61% on Cityscapes/ADE20K, respectively measured by mIoU. Compared to the ASPP, our approach is more efficient according to the complexity comparison reported in Table 4.

Table 3 Comparison with SA with 2Ã— downsampling, RCCA and CGNL on Cityscapes val
Full size table
Table 4 Efficiency comparison given input feature map of size [2048Ã—128Ã—128] during inference stage. All results are based on Pytorch 0.4.1 with a single Tesla V100 with CUDA 9.0
Full size table
Besides, we also compare the performance based on the conventional self-attention (SA) and the proposed interlaced sparse self-attention (ISA) in Table 2, and we can see that our ISA achieves comparable performance while being much more efficient.

Comparison to RCCA/CGNL/Efficient-Attention
We compare our approach with several existing mechanisms that focus on addressing the efficiency problem of self-attention/non-local, such as SA-2Ã— (Wang et al. 2018b), RCCA (Huang et al. 2019), CGNL (Yue et al. 2018) and Efficient Attention (Shen et al. 2018). For SA-2Ã—, we directly down-sample the feature map for 2Ã— before computing the dense relation matrix. We evaluate all these mechanisms on the Cityscapes val and report the results in Table 3. We can see that our approach consistently outperforms all these three mechanisms, which verifies that our approach is more reliable. We all report the average performance for fairness considering the mIoU variance of RCCA is large.

Complexity. We compare the complexity of our approach with PPM (Zhao et al. 2017), DANet (Fu et al. 2019a), RCCA (Huang et al. 2019), CGNL (Yue et al. 2018) and Efficient Attention (Shen et al. 2018) in this section. We report the GPU memory, GFLOPs and inference time when processing input feature map of size 2048Ã—128Ã—128 under the same setting in Table 4. We can see that our approach based on ISA is much more efficient than most of the other approaches except the Efficient Attention scheme. For example, the proposed Base-OC (ISA) is nearly 3Ã— faster and saves more than 88% GPU memory when compared with the DANet. Besides, our approach also requires less GPU memory/inference time than both RCCA and CGNL.

Pyramid extensions. We study the performance with the two pyramid extensions including the Pyramid-OC and ASP-OC. We choose the dilated ResNet-101 as our baseline and summarize all related results in Table 5. We can find that the ASP-OC consistently improves the performance for both the SA scheme and ISA scheme while the Pyramid-OC slightly degrades the performance compared to the Base-OC mechanism. Accordingly, we only report the performance with Base-OC (ISA) module and ASP-OC (ISA) module for the following experiments if not specified.

Comparison to State-of-the-Arts
We choose the object context pooling module based on ISA, e.g., Base-OC (ISA) and ASP-OC (ISA), by default. We evaluate the performance of OCNet (w/ Base-OC) and OCNet (w/ ASP-OC) on 5 benchmarks and illustrate the related results as follows.

Cityscapes
We report the comparison to the state-of-the-art methods on Cityscapes test in Table 7, and we apply OHEM, the multi-scale testing and flip testing following the previous work. We apply our approach on both ResNet-101 and HRNetV2-W48, and our approach achieves competitive performance with both backbones. For example, we improve the performance of HRNetV2-W48 from 81.6% to 82.5% with the ASP-OC module, which also outperforms the recent ACNet (Fu et al. 2019b).

Table 5 Comparison to the pyramid extensions of object context pooling on Cityscapes val
Full size table
Table 6 Category-wise improvements over the baseline based on Base-OC (ISA) in terms of boundary F-score on Cityscapes val
Full size table
ADE20K
In Table 8, we compare our approach to the state-of-the-arts on the ADE20K val. Our approach also achieves competitive performance, e.g., OCNet (w/ ASP-OC) based on ResNet-101 and HRNetV2-W48 achieve 45.40% and 45.50% respectively, both are slightly worse than the recent ACNet (Fu et al. 2019b) that exploits rich global context and local context.

PASCAL-Context
As illustrated in Table 9, we compare our approach with the previous state-of-the-arts on the PASCAL-Context test. We can find that our approach significantly improves the performance of HRNet (Sun et al. 2019a) and achieves 56.2% with OCNet (w/ Base-OC), which also outperforms most of the other previous approaches.

LIP
We compare our approach to the previous state-of-the-arts on LIP val and illustrate the results in Table 10. The OCNet (w/ ASP-OC) based on HRNetV2-48 achieves competitive performance 56.35%, which is slightly worse than the recent CNIF (Wang et al. 2019).

COCO-Stuff
From Table 11, we can see that our method also achieves competitive performance 40.0% on COCO-Stuff test, which is comparable with the very recent state-of-the-art method.

Visualization
We visualize some examples of the global relation, local relation and dense relation predicted with our approach, e.g., OCNet based on dilated ResNet-101 + Base-OC (ISA), on different benchmarks in Fig. 6 and Fig. 7.

For all examples, we down-sample the the ground-truth label map to match the size of the dense relation map, which is 18 of the input size. We choose the same group numbers ğ‘ƒâ„=ğ‘ƒğ‘¤=8 for all datasets, thus, the global relation matrix and the local relation matrix are of various shapes. For example, the dense relation matrix / global relation matrix / local relation matrix is of size 256Ã—128 / 32Ã—16 / 8Ã—8 respectively for Cityscapes images. We generate the dense relation matrix by multiplying the local relation with the global relation, and we can see that the estimated dense relation matrix puts its most relation weights on the pixels belonging to the same category as the chosen pixel, which well approximates the ground-truth object context.

We compare the segmentation maps predicted with our approach and the baseline (dilated ResNet-101) to illustrate the qualitative improvements, and we visualize the results in Fig. 9. We can find that our method produces better segmentation maps compared with the baseline. We mark all of the improved regions with white dashed boxes.

Boundary Analysis
We report the boundary improvements within 3, 5, 9 and 12 pixels width based on our approach on Cityscapes val in Table 6, and we can find that our approach significantly improves the boundary quality for several object categories including wall, truck, bus, train and so on.

Table 7 Comparison with state-of-the-arts on Cityscapes test
Full size table
Application to Mask-RCNN
Dataset
We use COCO (Lin et al. 2014) dataset to evaluate our approach. The dataset is one of the most challenging datasets for object detection and instance segmentation, which contains 140K images annotated with object bounding boxes and masks of 80 categories. We follow the COCO2017 split as in (He et al. 2017), where the training, validation and test sets contains 115K, 5K, 20K images, respectively. We report the standard COCO metrics including Average Precision (AP), AP50 and AP75 for both bounding boxes and masks.

Training Settings
We use Mask-RCNN (He et al. 2017) as baseline to conduct our experiments. Similar to (Wang et al. 2018b), we insert 1 non-local block or object context pooling module based on interlaced sparse self-attention before the last block of res-4 stage of the ResNet-50 FPN (Lin et al. 2017b) backbone. All models are initialized with ImageNet pretrained weights and built upon open source toolbox (Massa and Girshick 2018). We train the models using SGD with batch size of 16 and weight decay of 0.0001. We conduct experiments using training schedules including â€œ1Ã— scheduleâ€ and â€œ2Ã— scheduleâ€ (Massa and Girshick 2018). The 1Ã— schedule starts at a learning rate of 0.02 and is decreased by a factor of 10 after 60K and 80K iterations and finally terminates at 90K iterations. We train for 180K iterations for 2Ã— schedule and decreases the learning rate proportionally. The other training and inference strategies keep the same with the default settings in the (Massa and Girshick 2018).

Table 8 Comparison with state-of-the-arts on ADE20K val
Full size table
Table 9 Comparison with state-of-the-arts on PASCAL-Context test
Full size table
Table 10 Comparison with state-of-the-arts on LIP val
Full size table
Results
We report the results on COCO dataset in Table 12. We can see that adding one non-local block (Wang et al. 2018b) or interlaced sparse self-attention module consistently improves the Mask-RCNN baseline by âˆ¼1% on all metrics involving both object detection and instance segmentation. Similar gains are observed for both 1Ã— schedule and 2Ã— schedule. For example, our approach improves the box AP/mask AP of Mask-RCNN from 38.7/34.9 to 39.7/35.7 with 2Ã— schedule. Especially, the performance of our approach is comparable with the non-local block on all metrics while decreasing the computation complexity significantly.

Table 11 Comparison with state-of-the-arts on COCO-Stuff test
Full size table
Last, we visualize the object detection and instance segmentation results of our approach and the Mask-RCNN on the validation set of COCO in Fig. 8. We can find that our approach improves the Mask-RCNN consistently on all the examples. For example, the Mask-RCNN fails to detect multiple cars in the last example while our approach achieves better detection performance.

Conclusion
In this paper, we present the object context that is capable of enhancing the object information via exploiting the semantic relations between pixels. Our object context is more in line with the definition of the semantic segmentation that defines the category of each pixel as the category of the object that it belongs to. We propose two different kinds of implementations including: (i) dense relation based on the conventional self-attention scheme and (ii) sparse relation based on the proposed interlaced sparse self-attention scheme. We demonstrate that the effectiveness of our method on five challenging semantic segmentation benchmarks, e.g., Cityscapes, ADE20K, LIP, PASCAL-Context and COCO-Stuff. We also extend our approach on Mask-RCNN to verify the advantage and we believe our approach might benefit various vision tasks through replacing the original self-attention or non-local scheme with our interlaced sparse self-attention mechanism.

Fig. 6
figure 6
Visualization of the predicted dense relation matrices by OCNet on Cityscapes val, ADE20K val and PASCAL-Context test. We apply the dilated ResNet-101 + Base-OC (ISA) to generate these relation matrices. We visualize both the global relation and the local relation for each selected pixel and we compute the dense relation as the product of the global relation and the local relation

Full size image
Fig. 7
figure 7
Visualization of the predicted dense relation matrices by OCNet on LIP val and COCO-Stuff test. We apply the dilated ResNet-101 + Base-OC (ISA) to generate these relation matrices

Full size image
Fig. 8
figure 8
Visualization of the object detection and instance segmentation results of Mask-RCNN (He et al. 2017) and our approach on the validation set of COCO (Best viewed in color)

Full size image
Fig. 9
figure 9
Qualitative comparison on Cityscapes val, ADE20K val, PASCAL-Context test, COCO-Stuff test and LIP val. We choose dilated ResNet-101 as the baseline and further apply the Base-OC (ISA) on dilated ResNet-101 as our approach

Full size image
Table 12 Comparison with non-local (Wang et al. 2018b) (NL) on the validation set of COCO. We use Mask-RCNN (He et al. 2017) as baseline and choose ResNet-50 FPN backbone for all models
Full size table
Future Work
Although our object context scheme achieves competitive results on various benchmarks, there still exist many other important paths to construct richer context information. We illustrate three potential candidates:

use the co-occurring relations between different object categories to refine the coarse segmentation map, e.g., the â€œriderâ€ tends to co-occur with the â€œbicycleâ€, thus, we can refine the â€œriderâ€ pixels being misclassified as â€œpersonâ€.

use the shape structure information to regularize the segmentation, e.g., the shape of â€œbusâ€ tends to be quadrilateral, pentagon, or hexagon under various views, thus, we might use a set of prior shape masks to refine the predictions like the recent ShapeMask (Kuo et al. 2019).

the spatial location relation information, e.g., the â€œkeyboardâ€ is typically lying under the â€œmonitorâ€, thus, we can use the prior knowledge on the spatial relations between different objects to refine the predictions. Besides, there also exist some efforts (Krishna et al. 2017) that focused on predicting the â€œrelationshipâ€ between different objects from the input image directly.