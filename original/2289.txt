Video Question Answering (Video QA) challenges modelers in multiple fronts. Modeling video necessitates building not only spatio-temporal models for the dynamic visual channel but also multimodal structures for associated information channels such as subtitles or audio. Video QA adds at least two more layers of complexity – selecting relevant content for each channel in the context of the linguistic query, and composing spatio-temporal concepts and relations hidden in the data in response to the query. To address these requirements, we start with two insights: (a) content selection and relation construction can be jointly encapsulated into a conditional computational structure, and (b) video-length structures can be composed hierarchically. For (a) this paper introduces a general-reusable reusable neural unit dubbed Conditional Relation Network (CRN) taking as input a set of tensorial objects and translating into a new set of objects that encode relations of the inputs. The generic design of CRN helps ease the common complex model building process of Video QA by simple block stacking and rearrangements with flexibility in accommodating diverse input modalities and conditioning features across both visual and linguistic domains. As a result, we realize insight (b) by introducing Hierarchical Conditional Relation Networks (HCRN) for Video QA. The HCRN primarily aims at exploiting intrinsic properties of the visual content of a video as well as its accompanying channels in terms of compositionality, hierarchy, and near-term and far-term relation. HCRN is then applied for Video QA in two forms, short-form where answers are reasoned solely from the visual content of a video, and long-form where an additional associated information channel, such as movie subtitles, presented. Our rigorous evaluations show consistent improvements over state-of-the-art methods on well-studied benchmarks including large-scale real-world datasets such as TGIF-QA and TVQA, demonstrating the strong capabilities of our CRN unit and the HCRN for complex domains such as Video QA. To the best of our knowledge, the HCRN is the very first method attempting to handle long and short-form multimodal Video QA at the same time.

Access provided by University of Auckland Library

Introduction
Answering natural questions about a video is a powerful demonstration of cognitive capability. The task involves the acquisition and manipulation of spatio-temporal visual, acoustic and linguistic representations from the video guided by the compositional semantics of linguistic cues Gao et al. (2018); Lei et al. (2018); Li et al. (2019); Song et al. (2018); Tapaswi et al. (2016); Wang et al. (2018). As questions are potentially unconstrained, Video QA requires deep modeling capacity to encode and represent crucial multimodal video properties such as linguistic content, object permanence, motion profiles, prolonged actions, and varying-length temporal relations in a hierarchical manner. For Video QA, the visual and textual representations should ideally be question-specific and answer-ready.

Fig. 1
figure 1
Examples of short-form Video QA for which frame relations are key toward correct answers. (1) Near-term frame relations are required for counting of fast actions. (2) Far-term frame relations connect the actions in long transition. HCRN with the ability to model hierarchical conditional relations handles successfully, while baseline struggles.

Full size image
Fig. 2
figure 2
Example of long-form Video QA. This is a typical question where a model needs to collect sufficient relevant cues from both visual content of a given video and textual subtitles to give the correct answer. In this particular example, our baseline is likely to suffer from the linguistic bias (“stage and played guitar”) while our model successfully manages to arrive at the correct answer by connecting the linguistic information from the first shot and visual content in the second one.

Full size image
The common approach toward modeling videos for QA is to build neural architectures specially designed for a particular data format and modality. In this perspective, the two common variants of Video QA emerge: short-form Video QA where relevant information is contained in the visual content of short video snippets of a single event (see Fig. 1 for examples), and long-form Video QA (also called Video Story QA) where the keys to answer are carried in the mixed visual-textual data of longer multi-shot video sequences (see Fig. 2 for example). Because of this specificity, such hand crafted architectures tend to be non-optimal for changes in data modality Lei et al. (2018), varying video length Na et al. (2017) or question types (such as frame QA Li et al. (2019) versus action count Fan et al. (2019)). This has resulted in proliferation of heterogeneous networks.

We wish to build a family of effective models for both long-form and short-form Video QA from reusable units that are more homogeneous and easier to construct, maintain and comprehend. We start by realizing that Video QA involves two sub-tasks: (a) selecting relevant content for each channel in the context of the linguistic query, and (b) composing spatio-temporal concepts and relations hidden in the data in response to the query. Much of sub-task (a) can be abstracted into a conditional computational structure that computes multi-way interaction between the query and the several objects. With this ability, solving sub-task (b) can be approached by composing the hierarchical structure of such abstraction from the ground up.

Toward this goal, we propose a general-purpose reusable neural unit called Conditional Relation Network (CRN) that encapsulates and transforms an array of objects into a new array of relations conditioned on a contextual feature. The unit computes sparse high-order relations between the input objects, and then modulates the encoding through a specified context (See Fig. 4). The flexibility of CRN and its encapsulating design allow it to be replicated and layered to form deep hierarchical conditional relation networks (HCRN) in a straightforward manner (See Fig. 5 and Fig. 6). Within the scope of this work, the HCRN is a two-stream network of visual content and textual subtitles, in which the two sub-networks share a similar design philosophy but are customized to suit the properties of each input modality. Whilst the visual stream is built up by stacked CRN units at different granularities, the textual stream is composed of a single CRN unit taking linguistic segments as inputs. The stacked units in the visual stream thus provide contextualized refinement of relational knowledge from visual objects – in a stage-wise manner it combines appearance features with clip activity flow and linguistic context, and afterwards incorporates the context information from the whole video motion and linguistic features. On the textual side, the CRN unit functions in the same manner but on textual objects. The resulting HCRN is homogeneous, agreeing with the design philosophy of networks such as InceptionNet Szegedy et al. (2015), ResNet He et al. (2016) and FiLM Perez et al. (2018).

The hierarchy of the CRNs for each input modality is shown as follows. At the lowest level of the visual stream, the CRNs encode the relations between frame appearance in a clip and integrate the clip motion as context; this output is processed at the next stage by CRNs that now integrate in the linguistic context. In the following stage, the CRNs capture the relation between the clip encodings, and integrate in video motion as context. In the final stage the CRN integrates the video encoding with the linguistic feature as context (See Fig. 5). As for the textual stream, due to its high-level abstraction as well as the diversity of nuances of expression comparing to its visual counterpart, we only use the CRN to encode relations betweenlinguistic segments in a given dialogue extracted from textual subtitles and leverage well-known techniques in sequential modeling, such as LSTM Hochreiter and Schmidhuber (1997) or BERT Devlin et al. (2019), to understand sequential relations at the word level. By allowing the CRNs to be stacked hierarchically, the model naturally supports modeling hierarchical structures in video and relational reasoning. Likewise, by allowing appropriate context to be introduced in stages, the model handles multimodal fusion and multi-step reasoning. For long videos, further levels of hierarchy can be added enabling encoding of relations between distant frames.

We demonstrate the capability of HCRN in answering questions in major Video QA datasets, including both short-form and long-form videos. The hierarchical architecture with four-layers of CRN units achieves favorable answer accuracy across all Video QA tasks. Notably, it performs consistently well on questions involving either appearance, motion, state transition, temporal relations, or action repetition demonstrating that the model can analyze and combine information in all of these channels. Furthermore the HCRN scales well on longer length videos simply with the addition of an extra layer. Fig. 1 and Fig. 2 demonstrate several representative cases those were difficult for the baseline of flat visual-question interaction but can be handled by our model. Our model and results demonstrate the impact of building general-purpose neural reasoning units that support native multimodality interaction in improving robustness and generalization capacities of Video QA models.

This paper advances the preliminary work Le et al. (2020) in three main aspects: (1) devising a new network architecture that leverages the design philosophy of HCRN in handling long-form Video QA, demonstrating the flexibility and generic applicability of the proposed CRN unit in various input modalities; (2) providing a comprehensive theoretical analysis on the complexity of CRN computational unit as well as the resulting HCRN architectures; (3) conducting more rigorous experiments and ablation study to fully examining the capability of the proposed network architectures, especially for the long-form Video QA. These experiments thoroughly assure the consistency in behavior of the CRN unit upon different settings and input modalities. We also simplify the sampling procedure in the main algorithm, reducing the run time significantly.

The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 details our main contributions – the CRN, the HCRNs for Video QA on both short-from videos and long-form videos that include subtitles, as well as complexity analysis. The next section describes the results of the experimental suite. Section 5 provides further discussion and concludes the paper.

Related Work
Our proposed HCRN model advances the development of Video QA by addressing three key challenges: (1) Efficiently representing videos as amalgam of complementing factors including appearance, motion and relations, (2) Effectively allows the interaction of such visual features with the linguistic query and (3) Allows integration of different input modalities in Video QA with one unique building block.

Long/short-form Video QA is a natural extension of image QA and it has been gathering a rising attention in recent years, with the release of a number of large-scale short-form Video QA datasets, such as TGIF-QA Jang et al. (2017); Xu et al. (2016), as well as long-form MovieQA datasets with accompanying textual modality, such as TVQA Lei et al. (2018), MovieQA Tapaswi et al. (2016) and PororoQA Kim et al. (2016). All studies on Video QA treats short-form and long-form Video QA as two separate problems in which proposed methods Fan et al. (2019); Gao et al. (2018); Jang et al. (2017); Kim et al. (2016, 2019); Lei et al. (2018) are deviated to handle either one of the two problems. Different from those works, this paper takes the challenge of designing a generic method that covers both long-form and short-form Video QA with simple exercises of block stacking and rearrangements to switch one unique model between the two problem depending upon the availability of input modalities for each of them.

Spatio-temporal video representation is traditionally done by variations of recurrent networks (RNNs) among which many were used for Video QA such as recurrent encoder-decoder Zhu et al. (2017); Zhao et al. (2019), bidirectional LSTM Kim et al. (2016) and two-staged LSTM Zeng et al. (2017). To increase the memorizing ability, external memory can be added to these networks Gao et al. (2018); Zeng et al. (2017). This technique is more useful for videos that are longer Xu et al. (2016) and with more complex structures such as movies Tapaswi et al. (2016) and TV programs Lei et al. (2018) with extra accompanying channels such as speech or subtitles. On these cases, memory networks Kim et al. (2016); Na et al. (2017); Wang et al. (2019) were used to store multimodal features Wang et al. (2018) for later retrieval. Memory augmented RNNs can also compress video into heterogeneous sets Fan et al. (2019) of dual appearance/motion features. While in RNNs, appearance and motion are modeled separately, 3D and 2D/3D hybrid convolutional operators Tran et al. (2018); Qiu et al. (2017) intrinsically integrates spatio-temporal visual information and are also used for Video QAJang et al. (2017); Li et al. (2019). Multiscale temporal structure can be modeled by either mixing short and long term convolutional filters Wu et al. (2019) or combining pre-extracted frame features non-local operators Tang et al. (2018); Li et al. (2017). Within the second approach, the TRN network Zhou et al. (2018) demonstrates the role of temporal frame relations as an another important visual feature for video reasoning and Video QA Le et al. (2020). Relations of predetected objects were also considered in a separate processing stream Jin et al. (2019) and combined with other modalities in late-fusion Singh et al. (2019). Our HCRN model emerges on top of these trends by allowing all three channels of video information namely appearance, motion and relations to iteratively interact and complement each other in every step of a hierarchical multi-scale framework.

Earlier attempts for generic multimodal fusion for visual reasoning include bilinear operators, either applied directly Kim et al. (2018) or through attention Kim et al. (2018); Yu et al. (2017). While these approaches treat the input tensors equally in a costly joint multiplicative operation, HCRN separates conditioning factors from refined information, hence it is more efficient and also more flexible on adapting operators to conditioning types.

Temporal hierarchy has been explored for video analysis Lienhart (1999), most recently with recurrent networks Pan et al. (2016); Baraldi et al. (2017) and graph networks Mao et al. (2018). However, we believe we are the first to consider hierarchical interaction of multi-modalities including linguistic cues for Video QA.

Linguistic query–visual feature interaction in Video QA has traditionally been formed as a visual information retrieval task in a common representation space of independently transformed question and referred video Zeng et al. (2017). The retrieval is more convenient with heterogeneous memory slots Fan et al. (2019). On top of information retrieval, co-attention between the two modalities provides a more interactive combination Jang et al. (2017). Developments along this direction include attribute-based attention Ye et al. (2017), hierarchical attention Liang et al. (2018); Zhao et al. (2018, 2017), multi-head attention Kim et al. (2018); Li et al. (2019), multi-step progressive attention memory Kim et al. (2019) or combining self-attention with co-attention Li et al. (2019). For higher order reasoning, question can interact iteratively with video features via episodic memory or through switching mechanism Yang et al. (2019). Multi-step reasoning for Video QA is also approached by Xu et al. (2017) and Song et al. (2018) with refined attention.

Unlike these techniques, our HCRN model supports conditioning video features with linguistic clues as a context factor in every stage of the multi-level refinement process. This allows the representation of linguistic cues to be involved earlier and deeper into video presentation construction than any available methods.

Neural building blocks - Beyond the Video QA domain, CRN unit shares the idealism of uniformity in neural architecture with other general purpose neural building blocks such as the block in InceptionNet Szegedy et al. (2015), Residual Block in ResNet He et al. (2016), Recurrent Block in RNN, conditional linear layer in FiLM Perez et al. (2018), and matrix-matrix-block in neural matrix net Do et al. (2018). Our CRN departs significantly from these designs by assuming an array-to-array block that supports conditional relational reasoning and can be reused to build networks of other purposes in vision and language processing. As a result, our HCRN is a perfect fit not only for short-form video where questions are all about visual content of a video snippet but also for long-form video (Movie QA) where a model has to look at both visual cue and textual cue (subtitles) to arrive at correct answers. Due to the great challenges posed by long-form Video QA and the diversity in terms of model building, current approaches in Video QA mainly spend efforts on handling the visual part while leaving the textual part for common techniques such as LSTM Lei et al. (2018) or latest advance in natural language processing BERT Yang et al. (2020). To the best of our knowledge, HCRN is the first model that could solve both short-form and long-form Video QA with models building up from a generic neural block.

Method
Fig. 3
figure 3
Overall multimodal Video QA architecture. The two streams handle visual and textual modalities in parallel, followed by an answer decoder for feature joining and prediction.

Full size image
The goal of Video QA is to deduce an answer 𝑎̃  from a video  and optionally from additional information channels such as subtitles  in response to a natural question q. The answer 𝑎̃  can be found in an answer space  which is a pre-defined set of possible answers for open-ended questions or a list of answer candidates in the case of multi-choice questions. Formally, Video QA can be formulated as follows:

𝑎̃ =argmax𝑎∈𝜃(𝑎∣𝑞,,),
(1)
where 𝜃 is the model parameters of scoring function .

Within the scope of this work, we address two common settings of Video QA: (a) short-form Video QA where the visual content of a given single video shot singularly suffice to answer the questions and (b) long-form Video QA where the essential information disperses among visual content in the multi-shot video sequence and conversational content in accompanying textual subtitles.

HCRN was designed in the endeavor for a homogeneous neural architecture that can adapt to solve both problems. Its overall workflow is depicted in Fig. 3. In long-form videos, when both visual and textual streams are present, HCRN distills relevant information from the visual stream and the textual stream, both are conditioned on the question. Eventually, it combines them into an answer decoder for final prediction in late-fusion multimodal integration. In short-form cases, where only video frames are available, the visual stream is solely active, working with the single-input answer decoder. One of the ambitions of the design is to build each processing stream as a hierarchical network simply by stacking common core processing units of the same family. Similar to all previous deep-learning-based approaches in the literature Jang et al. (2017); Li et al. (2019); Fan et al. (2019); Gao et al. (2018); Lei et al. (2018); Le et al. (2020), our HCRN operates on top of the feature embeddings of multiple input modalities, making use of the powerful feature representations extracted by either common visual recognition models pre-trained on large-scale datasets such as ResNet He et al. (2016), ResNeXt Xie et al. (2017); Hara et al. (2018) or pre-trained word embeddings such as GloVe Pennington et al. (2014) and BERT Devlin et al. (2019).

In the following subsections, we present the design of the core unit in Sec. 3.1, the hierarchical designs tailored to each modality in Sec. 3.2, the answer decoder in Sec. 3.3. Theoretical analysis of the computational complexity of the models follows in Sec. 3.4.

Table 1 Notations of CRN unit operations.
Full size table
Conditional Relation Network Unit
We introduce a general computation unit, termed Conditional Relation Network (CRN), which takes as input an array of n objects ={𝑥𝑖}𝑛𝑖=1 and a conditioning feature c serving as global context. Objects are assumed to live either in the same vector space ℝ𝑑 or tensor space, for example, ℝ𝑊×𝐻×𝑑 in case of images (or video frames). CRN generates an output array of objects of the same dimensions containing high-order object relations of input features given the global context. The global context is problem-specific, serving as a modulator to the formation of the relations. When in use for Video QA, CRN’s input array is composed of features at either frame level, short-clip levels, or textual features. Examples of global context include the question and motion profiles at a given hierarchical level.

figure a
Fig. 4
figure 4
Conditional Relation Network. a) Input array  of n objects are first processed to model k-tuple relations from t sub-sampled size-k subsets by sub-network 𝑔𝑘(.). The outputs are further conditioned with the context c via sub-network ℎ𝑘(.,.) and finally aggregated by 𝑝𝑘(.) to obtain a result vector 𝑟𝑘 which represents k-tuple conditional relations. Tuple sizes can range from 2 to (𝑛−1), which outputs an (𝑛−2)-dimensional output array.

Full size image
Given input set of object , CRN first considers the set of subsets ={𝑄𝑘}𝑘max𝑘=2 where each set 𝑄𝑘 contains t size-k subsets randomly sampled from , where t is the sampling frequency, 𝑡<𝐶(𝑛,𝑘). On each collection 𝑄𝑘, CRN then uses member relational sub-networks to infer the joint representation of k-tuple object relations. In videos, due to temporal coherence, the objects {𝑥𝑖}𝑛𝑖=1 share a great amount of mutual information, therefore, it is reasonable to use a subsampled set of t combinations instead of considering all possible combinations for 𝑄𝑘. This is inspired by Zhou et al. (2018), however, we sample t size-k subsets directly from the original  rather than randomly take subsets in a pool of all possible size-k subsets as in their method. By doing this, we can reduce the computational complexity of the CRN as it is much cheaper to sample elements out of a set than building all combinations of subsets which cost (2𝑛) time. We provide more analysis on the complexity of the CRN unit later in Sec. 3.4.

Regarding relational modeling, each member subset of 𝑄𝑘 then goes through a set-based function 𝑔𝑘(.) for relational modeling. The relations across objects are then further refined with a conditioning function ℎ𝑘(.,𝑐) in the light of conditioning feature c. Finally, the k-tuple relations are summarized by the aggregating function 𝑝𝑘(.). The similar operations are done across the range of subset size from 2 to 𝑘max. Regarding the choice of 𝑘max, we use 𝑘max=𝑛−1 in later experiments, resulting in the output array of size 𝑛−2 if 𝑛>2 and an array of size 1 if 𝑛=2.

The detailed operation of the CRN unit is presented formally as pseudo-code in Alg. 1 and visually in Fig. 4. Table 1 summarizes the notations used across these presentations.

Networks Implementation
Set aggregation:

The set functions 𝑔𝑘(.) and 𝑝𝑘(.) can be implemented as any aggregation sub-networks that join a random set into a single representation. As a choice in implementation, the function 𝑔𝑘(.) is either average pooling or a simple concatenation operator while 𝑝𝑘(.) is average pooling.

Conditioning function:

The design of the conditioning sub-network that implements ℎ𝑘(.,𝑐) depends on the relationship between the input set  and the conditioning feature c as well as the properties of the channels themselves. Here we present four forms of neural operation implementing this function.

Additive form:

A simple form of ℎ𝑘(.,𝑐) is feature concatenation followed by a MLP that models the non-linear relationships between multiple input modalities:

ℎ𝑘(.,𝑐)=ELU(𝑊ℎ1[.;𝑐]),
(2)
where [;] to denote the tensor concatenation operation and ELU is the non-linear activation function introduced in Clevert et al. (2016). Eq. 2 is sufficient when x and c are additively complementary.

Multiplicative form:

To support more complex the relationship between the input x and the conditioning feature c, a more sophisticated joining operation is warranted. For example, when c implies a selection criterion to modulate the relationship between elements in x, the multiplicative relation between them can be represented in conditioning function by:

ℎ𝑘(𝑥,𝑐)=ELU(𝑊ℎ1[𝑥;𝑥⊙𝑐;𝑐]),
(3)
where ⊙ denotes Hadamard product.

Sequential form:

As aforementioned, how to properly choose the sub-network ℎ𝑘(.,𝑐) is also driven by the properties of the input set . Given the context of Video QA of later use of the CRN unit where elements in  may contain strong temporal relationships, we additionally integrate sequential modeling capability, which is a BiLSTM network in this paper, along with the conditioning sub-network as presented in Eq. 2 and Eq. 3. Formally, ℎ𝑘(.,𝑐) is defined as:

𝑠=[𝑥;𝑥⊙𝑐;𝑐],
(4)
𝑠′=BiLSTM(𝑠),
(5)
ℎ𝑘(.,𝑐)=maxpool(𝑠′).
(6)
Dual conditioning form:

In the later use of CRN in Video QA where it can happen that two concurrent signals 𝑐1,𝑐2 are used as conditioning features, we simply extend Eq. 2 and Eq. 3 and Eq. 4 as:

ℎ𝑘(𝑥,𝑐)=\ ELU(𝑊ℎ1[𝑥;𝑐1;𝑐2]),
(7)
ℎ𝑘(𝑥,𝑐)=\ ELU(𝑊ℎ1[𝑥;𝑥⊙𝑐1;𝑥⊙𝑐2;𝑐1;𝑐2]),
(8)
𝑠=[𝑥;𝑥⊙𝑐1;𝑥⊙𝑐2;𝑐1;𝑐2],
(9)
respectively.

Hierarchical Conditional Relation Network for Multimodal Video QA
We use CRN blocks to build a deep network architecture that supports a wide variety of Video QA settings. In particular, two variations are specifically designed to work on short-form and long-form Video QA. For each of the settings, the network design adapts to exploit inherent characteristics of a video sequence namely temporal relations, motion, linguistic conversation and the hierarchy of video structure, and to support reasoning guided by linguistic questions. We term the proposed network architecture Hierarchical Conditional Relation Networks (HCRN). The design of the HCRN by stacking reusable core units is partly inspired by modern CNN network architectures, of which InceptionNet Szegedy et al. (2015) and ResNet He et al. (2016) are the most well-known examples. In the general form of Video QA, HCRN is a multi-stream end-to-end differentiable neural network in which one stream is to handle visual content and the other one handling textual dialogues in subtitles. The network is modular and each network stream plays a plug-and-play role adaptively to the presence of input modalities.

Fig. 5
figure 5
Visual stream. The CRNs are stacked in a hierarchy, embedding the video input at different granularities including frame, short clip and entire video levels. At each level of granularity, the video feature embedding is conditioned on the respective level-wise motion feature and universal linguistic cue.

Full size image
Preprocessing
With the Video QA as defined in Eq. 1, HCRN takes input and question represented as sets of visual or textual objects and computes the answer. In this section, we describe the preprocessing of raw videos into appropriate input sets for HCRN.

Visual representation:

We begin by dividing the video  of L frames into N equal length clips 𝐶={𝐶1,...,𝐶𝑁}. For short-form videos, each clip 𝐶𝑖 of length 𝑇=⌊𝐿/𝑁⌋ is represented by two sources of information: frame-wise appearance feature vectors 𝑉𝑖={𝑣𝑖,𝑗∣𝑣𝑖,𝑗∈ℝ𝟚𝟘𝟜𝟠}𝑇𝑗=1 , and a motion feature vector at clip level 𝑓𝑖∈ℝ𝟚𝟘𝟜𝟠. Appearance features are vital for video understanding as the visual saliency of objects/entities in the video is usually of interest to human questions. In short clips, moving objects and events are primary video artifacts that capture our attention. Hence, it is common to see the motion features coupled with the appearance features to represent videos in the video understanding literature. On the contrary, in long-form video such as those in movies and TV programs, the concerns can be less about specific motions but more into movie plot or film grammar. As a result, we use the frame-wise appearance as the only feature for the long-form Video QA. In our experiments, 𝑣𝑖,𝑗 are the pool5 output of ResNet He et al. (2016) features and 𝑓𝑖 are derived by ResNeXt-101 Xie et al. (2017); Hara et al. (2018).

Subsequently, linear feature transformations are applied to project {𝑣𝑖𝑗} and 𝑓𝑖 into a standard d-dimensions feature space to obtain {𝑣̂ 𝑖𝑗∣𝑣̂ 𝑖𝑗∈ℝ𝕕} and 𝑓̂ 𝑖∈ℝ𝕕, respectively.

Linguistic representation:

Linguistic objects are built from question, answer choices and long-form videos’ subtitles. We explore two options of representation learning for them using BiLSTM and BERT.

Sequential embedding with BiLSTM:

All words in linguistic cues including those in questions, answer choices and subtitles are first embedded into vectors of 300 dimensions by pre-trained GloVe word embeddings Pennington et al. (2014).

For question and answer choices, we further pass these context-independent embedding vectors through a BiLSTM. Output hidden states of the forward and backward LSTM passes are finally concatenated to form the overall query representation 𝑞∈ℝ𝑑 for the questions and 𝑎∈ℝ𝑑 for the answer choices if available (multi-choice question-answer pairs).

For the accompanying subtitles provided in long-form Video QA, instead of treating them as one big passage as in prior works Lei et al. (2018); Kim et al. (2019), we dissect the subtitle passage  into a fixed number of M overlapping segments ={1,..,𝑀}. The number of words in sibling segments is identical 𝑇=length()/𝑀 but varies from one video to another depending on the overall length of the given subtitle passage .

Similarly to the case of processing questions, we process each segment with pre-trained word embeddings followed by a BiLSTM. The hidden states of the BiLSTM which is also of size T are then used as textual objects:

𝑈𝑖=BiLSTM(𝑖).
(10)
{𝑈𝑖}𝑀𝑖=1 are the final representations ready to be used by the textual stream which will be described in Sec. 3.2.3.

Contextual embedding using BERT:

As an alternative option for linguistic representation, we utilize pre-trained BERT network Devlin et al. (2019) to extract contextual word embeddings. Instead of encoding words independently as in GloVe, BERT embeds each word in the context of its surrounding words using a self attention mechanism.

For short-form Video QA, we tokenize the given question and answer choices in multi-choice questions and subsequently feed the tokens into the pre-trained BERT network. Averaged embeddings of words in a sentence are used as a unified representation for that sentence. This applies to generate both the question representation q and answer choices {𝑎𝑖}𝑖=1,...,𝐴.

For long-form Video QA, with each answer choice, we form a long string 𝑙𝑖 by stacking it with subtitles  and the question sentence. We then tokenize and embed each string 𝑙𝑖 with BERT into contextual hidden matrix H of size 𝑚×𝑑 where m is the maximum number of tokens in the input and d is hidden dimensions of BERT. This tensor H is then split up into corresponding embedding vectors of the subtitles 𝐻𝑠, of the question 𝐻𝑞 and of the answer choice 𝐻𝑎𝑖:

(𝐻𝑠,𝐻𝑞,𝐻𝑎𝑖)=BERT(𝑙𝑖).
(11)
Eventually, we suppress the contextual tokens question representation and answer choices into their respective single representation by mean pooling:

𝑞=mean(𝐻𝑞);𝑎𝑖=mean(𝐻𝑎𝑖),
(12)
while keeping those of subtitles 𝐻𝑠 as a set of textual objects. {𝑈𝑖}𝑀𝑖=1 are obtained by sliding overlapping windows of the same length over 𝐻𝑠.

Visual Stream
An effective model for Video QA needs to distill the visual content in the context of the question and filter out the usually large portion of the data that is not relevant to the question. Drawing inspiration from the hierarchy of video structure, we boil down the problem of Video QA into a process of video representation in which a given video is encoded progressively at different granularities, including short clip (a sequence of video frames) and entire video levels (a sequence of clips). It is crucial that the whole process conditions on linguistic cues.

With that in mind, we design a two-level structure to represent a video, one at clip level and the other one at video level, as illustrated in Fig. 5. At each hierarchy level, we use two stacked CRN units, one conditioned on motion features followed by one conditioned on linguistic cues. Intuitively, the motion feature serves as a dynamic context shaping the temporal relations found among frames (at the clip level) or clips (at the video level). It also provides a saliency indicator of which relations are worth the attention Mahapatra et al. (2008).

As the shaping effect is applied to all relations in a complementary way, selective (multiplicative) relation between the relations and the conditioning feature is not needed, and thus a simple concatenation operator followed by a MLP suffices. On the other hand, the linguistic cues are by nature selective, that is, not all relations are equally relevant to the question. Thus we utilize the multiplicative form for feature fusion as in Eq. 3 for the CRN units which condition on question representation.

Fig. 6
figure 6
Textual stream. Both segment-level and passage-level textual objects are modulated with the question by a pre-selection module. They then serves as input and conditioning features for an CRN modeling long-term relationships between segments.

Full size image
With this particular design of network architecture, the input array at clip level consists of frame-wise appearance feature vectors {𝑣̂ 𝑖𝑗}, while that at a video level is the output at the clip level. The motion conditioning feature at clip level CRNs is corresponding clip motion feature vector 𝑓̂ 𝑖. They are further passed to an LSTM, whose final state is used as video-level motion features. Note that this particular implementation is not the only option. We believe we are the first to progressively incorporate multiple modalities of input in such a hierarchical manner in contrast to the typical approach of treating appearance features and motion features as a two-stream network.

Deeper hierarchy:

To handle a video of longer size, up to thousands of frames which is equivalent to dozens of short-term clips, there are two options to reduce the computational cost of CRN in handling large sets of subsets {𝑄𝑘∣𝑘=2,...,𝑘max} given an input array : (i) limit the maximum subset size 𝑘max, or (ii) extend the visual stream networks to deeper hierarchy. For the former option, this choice of sparse sampling may have the potential to lose critical relation information of specific subsets. The latter, on the other hand, is able to densely sample subsets for relation modeling. Specifically, we can group N short-term clips into 𝑁1×𝑁2 hyper-clips, of which 𝑁1 is the number of the hyper-clips and 𝑁2 is the number of short-term clips in one hyper-clip. By doing this, the visual stream now becomes a 3-level of hierarchical network architecture. See Sec. 3.4 for the effect of going deeper on running time and Sec. 4.4.3 on accuracy.

Computing the output:

At the end of the visual stream, we compute the average visual feature which is driven by the question representation q. Assuming that the outputs of the last CRN unit at video level are an array 𝑂={𝑜𝑖∣𝑜𝑖∈ℝ𝐻×𝑑}𝑁−4𝑖=1, we first stack them together, resulting in an output tensor 𝑜∈ℝ(𝑁−4)×𝐻×𝑑. We further vectorize this output tensor to obtain the final output 𝑜′∈ℝ𝐻′×𝑑,𝐻′=(𝑁−4)×𝐻. The weighted average information is given by:

𝐼=[𝑊𝑜′𝑜′;𝑊𝑜′𝑜′⊙𝑊𝑞𝑞],
(13)
𝐼′=ELU(𝑊𝐼𝐼+𝑏),
(14)
𝛾=softmax(𝑊𝐼′𝐼′+𝑏),
(15)
𝑜̃ =∑ℎ=1𝐻′𝛾ℎ𝑜′ℎ;𝑜̃ ∈ℝ𝑑,
(16)
where, [. ,.] denotes concatenation operation, and ⊙ is the Hadamard product. In case of multi-choice questions which are coupled with answer choices {𝑎𝑖}, Eq. 13 becomes 𝐼=[𝑊𝑜′𝑜′;𝑊𝑜′𝑜′⊙𝑊𝑞𝑞;𝑊𝑜′𝑜′⊙𝑊𝑎𝑎𝑖].

Textual Stream
HCRN architecture is readily applicable to the accompanying textual subtitles in a similar bottom-up fashion as in visual stream. The HCRN textual stream also consists of two-level hierarchy structure that process textual objects of each segment and joining segments into passage level (See Fig. 6).

The input of the stream is preprocessed subtitles, question, and answer choices (Sec. 3.2.1). The subtitles  is represented as its overall representation 𝐻𝑠 and also a sequence of equal-length segments, each of which has been encoded into a set of textual objects 𝑈𝑖={𝑢𝑡𝑖}𝑡=1,..,𝑇∈ℝ𝑇×𝑑. Meanwhile, the question is encoded into a single vector 𝑞∈ℝ𝑑.

Question-relevant pre-selection:

Unlike video frames, subtitles  contains irregularly timed conversations between movie characters. Furthermore, while relevant visual features are abundant throughout the video, only a small portion of the subtitles is relevant to the query and reflective of the answers a. To assure such relevance, antecedent to CRN unit, we modulate the representation of passage 𝐻𝑠 and those of M segments {𝑈𝑖}𝑀𝑖=1 with both use the question and the answer choice. It is done with the pre-selection operator described below.

Fig. 7
figure 7
The adapted architecture of visual stream (Fig.5) for long-form video. At each level, only question-conditioned CRNs are employed. The motion-conditioned CRNs are unnecessary as the low-level motion features are less relevant in long-form media.

Full size image
At the segment level, the modulated representation 𝑈̂ 𝑖∈ℝ𝑇×𝑑 of each segment i of T objects are produced by

𝑈̂ 𝑖=𝑊𝑢[𝑈𝑖;𝑈𝑖⊙𝑞].
(17)
Similarly, at video level, the subtitle modulated representation 𝐻𝑠^∈ℝ𝑆×𝑑 is built as

𝐻𝑠^=𝑊ℎ[𝐻𝑠;𝐻𝑠⊙𝑞].
(18)
CRN unit:

A single CRN unit of the stream operates at passage level which models the relationships between segments. The modulated {𝑈̂ 𝑖}𝑀𝑖=1 are passed as input objects to a CRN unit. The conditioning feature of the CRN is a max-pooled vector of the modulated representation of the whole subtitle passage 𝐻̂ 𝑠. At the end of the textual stream, a temporal max-pooling operator is applied over the outputs of the CRN to obtain a single vector.

Adaptation & implementation
Short-form Video QA:

Recall that short-form Video QA in this paper refers to QA about single-shot videos of a few seconds without accompanying textual data. For these cases, we employ the standard visual stream as described in Sec. 3.2.2 to distill video/question joint representation. This representation is ready to be used by the answer decoder (See Sec.3.3) for generating the final output.

Long-form Video QA:

Different from the short-form Video QA, long-form Video QA involves reasoning on both visual information from video frames and textual content from subtitles. Compared to short snippets where local low-level motion saliency plays a critical role, discovering high-level semantic concepts associated with movie characters is more important Sang and Xu (2010). Such semantics interleave in the data of both modalities. Further difference comes from the fact that long-form videos are of greater duration hence require appropriate treatment.

Although long-form videos share with short-forms in having a hierarchical structure, they are distinctive in terms of semantic compositionality and length. We employ visual and textual streams as described in Sec. 3.2.2 and Sec. 3.2.3 with some adaptation for better suitability with the data format and structure and simply use the joint representation of the two streams for answer prediction. Ideally, long-form Video QA requires modeling interactions between a question, visual content and textual content in subtitles. Whist both the visual stream and textual stream described above involve early integration of the question representation into a visual representation and textual representation of subtitles, we do not opt for early interaction between visual content and textual content in subtitles in this work. Pairwise joint semantic representation between visual and language has proven to be useful in Video QA and closely related tasks Yu et al. (2018), however, it assumes the existence of pairs of multimodal sequence data in which they are highly semantically compatible. Those pairs are either a video sequence and a textual description of the video or a video sequence and positive/negative answer choices to a question about the visual content in the video. This is not always the case for the visual content and textual content in subtitles in long-form videos such as those taken from movies. Although the visual content and textual content may complement each other to some extent, in many cases, they may be greatly distant from each other. Let’s take the following scenario as an example, in a movie scene, two characters are standing and chatting with each other. While the visual content may provide information about where the conversation takes place, it hardly contains any information about the topic of their conversation. As a result, combining the visual information and textual information at an early stage, in this case, has the potential to cause information distortion and make it difficult for information retrieval. In addition, treating visual stream and textual stream in separation makes it easier to justify the benefits of using CRN units in modeling the relational information in each modality, hence, easier to assess the generic use of the CRN unit.

Note that in the visual stream in the HCRN architecture for long-form Video QA, we use CRN units to handle a subset of sub-sampled frames in each clip at the clip level. At the video level, another CRN gathers long-range dependencies between this clip-level information sent up from lower level CRN outputs.

All CRN units at both levels take question representation as conditioning features (See Fig. 7). Compared to the standard architecture introduced in Sec. 3.2.2 and Fig. 5, we drop all CRN units that condition on motion features. This adaptation is to accommodate the fact that low-level motion is less relevant than overall semantic flow in both clip- and video-level.

Answer Decoders
Following the previous works Fan et al. (2019); Jang et al. (2017); Song et al. (2018) we adopt different answer decoders depending on the task, as follows:

QA pairs with open-ended answers are treated as multi-class classification problems. For these, we employ a classifier which takes as input the combination of the retrieved information from visual stream 𝑜̃ 𝑣, the retrieved information from textual stream 𝑜̃ 𝑡 and the question representation q, and computes answer probabilities 𝑝∈ℝ||:

𝑦=ELU(𝑊𝑜[𝑜̃ 𝑣;𝑜̃ 𝑡;𝑊𝑞𝑞+𝑏]+𝑏),
(19)
𝑦′=ELU(𝑊𝑦𝑦+𝑏),
(20)
𝑝=softmax(𝑊𝑦′𝑦′+𝑏).
(21)
In the case of multi-choice questions where answer choices are available, we iteratively treat each answer choice {𝑎𝑖}𝑖=1,...,𝐴 as a query in exactly the manner that we handle the question. Eq. 17 and Eq. 18, therefore, take each of the answer choices’ representation {𝑎𝑖} as a conditioning feature along with the question representation. Regarding Eq. 20, it is replaced by:

𝑦=ELU(𝑊𝑜[𝑜̃ 𝑣qa;𝑜̃ 𝑡qa;𝑊𝑞𝑞+𝑏;𝑊𝑎𝑎+𝑏]+𝑏),
(22)
where 𝑜̃ 𝑣qa is output of visual stream respect to queries as question and answer choices, whereas 𝑜̃ 𝑡qa is output of the textual stream counterpart.

For short-form Video QA, we simply drop the retrieved information from textual stream 𝑜̃ 𝑡 in Eq. 20 and Eq. 22. We also use the popular hinge loss as what presents in Jang et al. (2017) for pairwise comparisons, max(0,1+𝑠𝑛−𝑠𝑝), between scores for incorrect 𝑠𝑛 and correct answers 𝑠𝑝 to train the network:

𝑠=𝑊𝑦′𝑦′+𝑏.
(23)
Regarding long-form Video QA, we use cross entropy as training loss for fair comparisons with prior works.

For repetition count task, we use a linear regression function taking 𝑦′ in Eq. 20 as input, followed by a rounding function for integer count results. The loss for this task is Mean Squared Error (MSE).

Complexity Analysis
We now provide a theoretical analysis of running time for CRN units and HCRN. We will show that adding layers saves computation time, just providing a justification for deep hierarchy.

CRN Units
For clarity, let us recall the notations introduced in our CRN units: 𝑘max is maximum subset (also tuple) size considered from a given input array of n objects, subject to 𝑘max<𝑛; t is number of size-k subsets, (𝑘=2,3,...,𝑘max), randomly sampled from the input set; 𝑔𝑘(.),ℎ𝑘(.,.) and 𝑝𝑘(.) are sub-networks for relation modeling, conditioning and aggregating, respectively. In our implementation, 𝑔𝑘(.) and 𝑝𝑘(.) are chosen to be set functions and ℎ𝑘(.,.) is a nonlinear transformation that fuses modalities.

Each input object in CRN is arranged into a matrix of size 𝐾×𝐹, where K is the number of object elements and F is the embedding size for each element. All the operations involving input objects are element-wise, that is, they are linear in time w.r.t. K. Assume that the set function of order k in the CRN’s operation in Alg. 1 has linear time complexity in k. This holds true for most aggregation functions such as mean, max, sum or product. With the relation orders ranging from 𝑘=2,3,...,𝑘max and sampling frequency t, inference cost in time for a CRN is:

costCRN(𝑡,𝑘max,𝐾,𝐹)=cost(𝑔)+cost(ℎ),
(24)
where,

cost(𝑔)cost(ℎ)=(𝑡2𝑘max(𝑘max−1)𝐾𝐹),=((4𝑡+2)(𝑘𝑚𝑎𝑥−1)𝐾𝐹2).
Here the running time of cost(𝑔) is quadratic in length because each g(.) that takes k objects as input will cost k time, for 𝑘=2,3,...,𝑘max. The running time of cost(𝑔) is quadratic in F due to the feature transformation operation that costs 𝐹2 time. When 𝑘max≪𝐹, cost(ℎ) will dominate the time complexity. However, since the function h(.) involves matrix operations only, it is usually fast.

The unit produces an output array of length 𝑘𝑚𝑎𝑥−1, where each output object is of the same size as the input objects.

HCRN Models
We adhere to the complexity analysis of visual stream only which increases linearly in complexity with a video length. The overall complexity of HCRN depends on the design choice for each CRN unit and the specific arrangement of CRN units. For clarity, let 𝑡=2 and 𝑘max=𝑛−1, which are found to work well in experiments. Let L be the video length, organized into N clips of length T each, i.e., 𝐿=𝑁𝑇.

2-level HCRN:

Consider, for example, the 2-level architecture HCRN, representing clips and video. Each level is a stack of two CRN layers, one for motion conditioning followed by the other for linguistic conditioning. The clip-level CRNs cost 𝑁×costCRN(2,𝑇−1,1,𝐹) time for motion conditioning and 𝑁×costCRN(2,𝑇−3,1,𝐹) time for question conditioning, where costCRN is the cost estimator in Eq. 24. This adds to roughly (2𝑇𝐿𝐹)+(10𝐿𝐹2) time.

Now the output array of size (𝑇−4)×𝐹 for the question-conditioned clip-level CRN becomes one in N input objects the video-level CRNs. The CRNs at the video level, therefore, take a cost of costCRN(2,𝑁−1,𝑇−4,𝐹) time for the motion-conditioned one and costCRN(2,𝑁−3,𝑇−4,𝐹) time for the question-conditioned one, respectively, totaling (2𝑁𝐿𝐹)+(10𝐿𝐹2) in order. Here we have made use of the identity 𝐿=𝑁𝑇. The total cost is therefore in the order of (2(𝑇+𝑁)𝐿𝐹)+(20𝐿𝐹2).

3-level HCRN:

Let us now analyze a 3-level architecture HCRN that generalizes the 2-level HCRN. The N clips are organized into P sub-videos, each has Q clips, i.e., 𝑁=𝑃𝑄. Since the CRNs at clip level remain the same, the first level costs 2TLF time to compute as before. Moving to the next level, each sub-video CRN takes as input an array of length Q, whose elements have size (𝑇−4)×𝐹. Applying the same logic as before, the set of sub-video-level CRNs cost roughly 𝑃×costCRN(2,𝑄−1,𝑇−4,𝐹) time or approximately (2𝑁𝑃𝐿𝐹)+(10𝐿𝐹2). Here we have used the identities 𝑁=𝑃𝑄 and 𝐿=𝑁𝑇.

A stack of two sub-video CRNs now produces an output array of size (𝑄−4)(𝑇−4)×𝐹, serving as an input object in an array of length P for the video-level CRNs. Thus the video-level CRNs cost roughly costCRN(2,𝑃−1,(𝑄−4)(𝑇−4),𝐹) time or approximately (2𝑃𝐿𝐹)+(10𝐿𝐹2) . Here we again have used the identities 𝐿=𝑁𝑇 and 𝑁=𝑃𝑄. Thus the total cost is in the order of (2(𝑇+𝑁𝑃+𝑃)𝐿𝐹)+(30𝐿𝐹2).

Deeper Models Might Save Time for Long Videos:
Recall that the 2-level HCRN has time cost of

(2(𝑇+𝑁)𝐿𝐹)+(20𝐿𝐹2),
and the 3-level HCRN the cost of

(2(𝑇+𝑁𝑃+𝑃)𝐿𝐹)+(30𝐿𝐹2).
Here the cost that is linear in F is due to the g functions, and the quadratic cost in F is due to the h functions.

When going from 2-level to 3-level architectures, the cost for the g functions drops by (2(𝑁−𝑁𝑃−𝑃)𝐿𝐹), and the cost for the h functions increases by (10𝐿𝐹2). Now assuming 𝑁≫max{𝑃,𝑁𝑃}, for example 𝑃≈𝑁‾‾√ and the number of clips 𝑁>20. Then the linear drop can be approximated further as (2𝑁𝐿𝐹). As 𝑁=𝐿𝑇, this can be written as (2𝐿2𝑇𝐹). In practice the clip size T is often fixed, thus the drop in the g functions scales quadratically with video length L, whereas the increase in the h functions scales linearly with L. This suggests that going deeper in hierarchy could actually save the running time for long videos.

See Sec. 4.4.3 for empirical validation of the saving.

Experiments
Datasets
We evaluate the effectiveness of the proposed CRN unit and the HCRN architecture on a series of short-form and long-form Video QA datasets. In particular, we use three different datasets as benchmarks for the short-form Video QA, namely TGIF-QA Jang et al. (2017), MSVD-QA Xu et al. (2017) and MSRVTT-QA Xu et al. (2016). All those three datasets are collected from real-world videos. We also evaluate HCRN on long-form Video QA using one of the largest datasets publicly available, TVQA Lei et al. (2018). Details of each benchmark are as below.

TGIF-QA:

This is currently the most prominent dataset for Video QA, containing 165K QA pairs and 72K animated GIFs. The dataset covers four tasks addressing unique properties of video. Of which, the first three require strong spatio-temporal reasoning abilities: Repetition Count - to retrieve the number of occurrences of an action, Repeating Action- multi-choice task to identify the action that is repeated for a given number of times, State Transition - multi-choice tasks regarding temporal order of events. The last task - Frame QA is akin to image QA where the answer to a given question can be found from one particular video frame.

MSVD-QA: This is a small dataset of 50,505 question answer pairs annotated from 1,970 short clips. Questions are of five types, including what, who, how, when and where, of which 61% of questions are used for training whilst 13% and 26% are used as the validation set and test set, respectively.

MSRVTT-QA:

The dataset contains 10K videos and 243K question answer pairs. Similar to MSVD-QA, questions are of five types. Splits for train, validation and test are with the proportions are 65%, 5%, and 30%, respectively. Compared to the other two datasets, videos in MSRVTT-QA contain more complex scenes. They are also much longer, ranging from 10 to 30 seconds long, equivalent to 300 to 900 frames per video.

TVQA:

This is one of the largest long-form Video QA datasets annotated from 6 different TV shows: The Big Bang Theory, How I Met Your Mother, Friends, Grey’s Anatomy, House, Castle. There are total 152.5K question-answer pairs associated with 5 answer choices each from 21,8K long clips of 60/90 secs which comes down to 122K, 15,25K and 15,25K for train, validation and test set, respectively. The dataset also provides start and end timestamps for each question to limit the video portion where one can find corresponding answers.

Regarding the evaluation metric, we mainly use accuracy in all experiments, excluding those for repetition count on TGIF-QA dataset where Mean Square Error (MSE) is applied.

Implementation Details
Feature Extraction
For short-form Video QA datasets, each video is preprocessed into N short clips of fixed lengths, 16 frames each. In detail, we first locate N equally spaced anchor frames. Each clip is then defined as a sequence of 16 consecutive video frames taking a pre-computed anchor as the middle frame. For the first and the last clip where frame indices may exceed the boundaries, we repeat the first frame or the last frame of the video multiple times until it fills up the clip’s size.

Given segmented clips, we extract motion features for each clip using a pre-trained model of the ResNeXt-101Footnote1 Xie et al. (2017); Hara et al. (2018). Regarding the appearance feature used in the experiments, we take the pool5 output of ResNet He et al. (2016) features as a feature representation of each frame. This means we completely ignore the 2D structure of spatial information of video frames which is likely to be beneficial for answering questions particularly interested in object’s appearance, such as those in the Frame QA task in the TGIF-QA dataset. We are aware of this but deliberately opt for light-weighted extracted features, and drive the main focus of our work on the significance of temporal relation, motion, and the hierarchy of video data by nature. Note that most of the videos in the datasets in the short-form Video QA category, except those in the MSRVTT-QA dataset, are short. Hence, we intentionally divide each video into 8 clips (8×16 frames) to produce partially overlapping frames between clips to avoid temporal discontinuity. Longer videos in MSRVTT-QA are additionally segmented into 24 clips of 16 frames each, primarily aiming at evaluating the model’s ability to handle very long sequences.

For the TVQA long-form dataset, we did a similar strategy as for short-video divide each video into N clips. However, as TVQA videos are longer and only recorded at 3fps, we adapt by choosing 𝑁=6, each clip contains 8 frames based on empirical experiences. The pool5 output of ResNet features is also used as a feature representation of each frame.

For the subtitles, the maximum subtitle’s length is set at 256. We simply cut off those who are longer than that and do zero paddings for those who are shorter. Subtitles are further divided into 6 segments overlapping at half a segment size.

Table 2 Comparison with the state-of-the-art methods on TGIF-QA dataset. For count, the lower the better (MSE) and the higher the better for the others (accuracy). *Means with standard deviations over 10 runs.
Full size table
Network Training
HCRN and its variations are implemented in Python 3.6 with Pytorch 1.2.0. Common settings include 𝑑=512, 𝑡=2 for both visual and textual streams. For all experiments, we train the model using Adam optimizer with a batch size of 32 , initially at a learning rate of 10−4 and decay by half after every 5 epochs for counting task in the TGIF-QA dataset and after every 10 epochs for the others. All experiments are terminated after 25 epochs and reported results are at the epoch giving the best validation accuracy. Depending on the amount of training data and hierarchy depth, it may take around 4-30 hours of training on one single NVIDIA Tesla V100 GPU. Pytorch implementation of the model is publicly available.Footnote2

As for experiments with the large-scale language representation model BERT, we use the latest pre-trained model provide by Hugging Face.Footnote3 We fine-tune the BERT model during training with a learning rate of 2×10−5 in all experiments.

Fig. 8
figure 8
Performance comparison on MSVD-QA and MSRVTT-QA dataset with state-of-the-art methods: Co-mem Gao et al. (2018), HME Fan et al. (2019), HRA Chowdhury et al. (2018), and AMU Xu et al. (2017).

Full size image
Quantitative Results
We compare our proposed model with state-of-the-art methods (SOTAs) on the aforementioned datasets. By default, we use pre-trained GloVe embedding Pennington et al. (2014) for word embedding following by a BiLSTM for sequential modeling. Experiments using contextual embeddings by a pre-trained BERT network Devlin et al. (2019) is explicitly specified with (BERT)”. Detailed experiments for short-form Video QA and long-form Video QA are as the following.

Short-form Video QA
For TGIF-QA, we compare with most recent SOTAs, including Fan et al. (2019); Gao et al. (2018); Jang et al. (2017); Li et al. (2019), over four tasks. These works, except for Li et al. (2019), make use of motion features extracted from either optical flow or 3D CNNs and its variants.

The results are summarized in Table 2 for TGIF-QA, and in Fig. 8 for MSVD-QA and MSRVTT-QA. Results of the previous works are taken from either the original papers or what reported by Fan et al. (2019). It is clear that our model consistently outperforms or is competitive with SOTA models on all the tasks across all datasets. The improvements are particularly noticeable when strong temporal reasoning is required, i.e., for the questions involving actions and transitions in TGIF-QA. These results confirm the significance of considering both near-term and far-term temporal relations toward finding correct answers. In addition, results on the TGIF-QA dataset over 10 runs of different random seeds confirm the robustness of the CRN units against the randomness in input subsets selection. More analysis on how relations affect the model’s performance is provided in later ablation studies.

Regarding results with the recent advance in language representation model BERT, it does not have much effect on the results across tasks that require strong temporal reasoning, such as the Action and Transition tasks in the TGIF-QA dataset. It specifically performs poorly on the Action task. This is because the Action task has the smallest number of training samples (20475 QA pairs) among all the tasks in this dataset (Transition - 52704 QA pairs, Frame QA - 39393 QA pairs, Count - 26843 QA pairs). In addition, all the tasks except the Frame QA rely on some fixed predefined templates with small sets of vocabulary. The Frame QA contains more diverse questions covering a wider range of question types thanks to its free-form nature. In particular, while there are 5754 unique tokens used in the Frame QA task, while only 2586 tokens, 4128 tokens and 2446 tokens are used in the Action, Transition, and Count tasks, respectively. Larger data volume and a richer vocabulary explain why the Frame QA task benefits the most from fine-tuning BERT. On the contrary, the Count task does not affect much regardless of the use of embedding, even though it also has a small training set and small vocabulary. Our later ablation studies in Table 5, Sec. 4.4 will show that HCRN’s performance on the Count task does not rely much on the question representation. Our analysis is also well aligned with existing works Ezen-Can (2020) where local models such as BiLSTM work better than fine-tuning BERT on small corpora across different NLP tasks.

We also empirically find out that fine-tune only the last two layers of the BERT network (HCRN (BERT embs - fine-tune last 2 layers)) produces more favorable performance comparing to either fine-tuning all layers (HCRN (BERT embs - fine-tune all layers)) or keeping BERT embeddings fixed during HCRN training (HCRN (BERT embs - no fine-tune)).

Table 3 Model design choices and input modalities in comparison. See Table 2 for corresponding performance on TGIF-QA dataset.
Full size table
The MSVD-QA and MSRVTT-QA datasets represent highly challenging benchmarks for machine compared to the TGIF-QA, thanks to their open-ended nature. Our model HCRN outperforms existing methods on both datasets, achieving 36.8% and 35.4% accuracy which are 2.4 points and 0.4 points improvement on MSVD-QA and MSRVTT-QA, respectively. This suggests that the model can handle both small and large datasets better than existing methods. We also fine-tune the contextual embeddings by BERT on these two datasets. Results show that the contextual embeddings bring great benefits on both MSVD-QA and MSRVTT-QA, achieving 39.3% and 38.3% accuracy, respectively. These figures are consistent with the result on the Frame QA task of the TGIF-QA dataset. Please note that we also fine-tune only last two layers of the BERT network in these experiments simply due to empirical results.

Table 4 Comparison with baselines and state-of-the-art methods on TVQA dataset. w/o ts: without using timestamp annotation to limit the search space of where to find answers; w/ ts: making use of the timestamp annotation. BERT indicates experiments that fine-tune the contextual embeddings given by a pre-trained BERT network Devlin et al. (2019) for language representation. * STAGE uses object proposal features by pretrained Faster R-CNN while other models rely on frame-wise CNN features.
Full size table
Finally, we provide a justification for the competitive performance of our HCRN against existing rivals by comparing model features in Table 3. Whilst it is not straightforward to compare head-to-head on internal model designs, it is evident that effective video modeling necessitates handling of motion, temporal relation and hierarchy at the same time. We will back this hypothesis by further detailed studies in Sec. 4.4 (for motion, temporal relations, shallow hierarchy) and Sec. 4.4.3 (deep hierarchy).

Long-form Video QA
For TVQA, we compare our method to several baselines as well as state-of-the-art methods (See Table 4). The dataset is relatively new and due to the challenges with long-form Video QA, there are only several attempts in benchmarking it. Comparisons of the proposed method with the baselines are made on the validation set while results of compared methods are taken as reported in original publications. Experiments using timestamp information are indicated by w/ ts and those with full-length subtitles are with w/o ts. If not indicated explicitly, V.”, S.”, “Q. short for visual features, subtitle features and question features, respectively. The evaluated settings are as follows:

(B) Q. : This is the simplest baseline where we only use question features for predicting the correct answer. Results in this setting will reveal how much our model relying on the linguistic bias to arrive at correct answers.

(B) S. + Q. (w/o pre-selection): In this baseline, we use question and subtitle features for prediction. Both question and subtitle features are simply obtained by concatenating the final output hidden states of forward and backward LSTM passes and further pass to a classifier as in Sec. 3.3.

(B) S. + Q. (w/ pre-selection): This baseline is to show the significance of pre-selection in textual stream as in Eq. 18. Having said that, question features and selective output between the question and the subtitles as explained in Eq. 18 are fed into a classifier for prediction.

(B) V. + Q.: In this baseline, we simply apply average pooling to smash visual features of entire videos into a vector and further combine it with question features for prediction.

(B) S. + V. + Q. (w/ pre-selection): We combine two baselines “(B) S. + Q. (w/ pre-selection)” and “(B) V. + Q.” right above. Given that, subtitle features are extracted with pre-selection as in Eq. 18 and video features are smashed over space-time before going through a classifier.

(HCRN) S. + Q.: This is to evaluate the effect of the textual stream alone in our proposed network architecture as described in Fig. 6.

(HCRN) V. + Q.: This is to evaluate the effect of the visual stream counterpart alone in Fig. 7.

(HCRN) S. + V. + Q.: Our full proposed architecture with the presence of both two modalities.

As shown in Table 4, using BERT for contextual word embeddings significantly improves performance comparing to GloVe embeddings and BiLSTM counterpart. The results are consistent with what was reported by Yang et al. (2020). Although our model only achieves competitive performance with Yang et al. (2020) in the setting of using timestamps (71.3 vs. 72.1), we significantly outperform them by 3.0 absolute points on the more challenging setting when we do not have access to timestamp indicators of where answers located. This clearly shows that our bottom-up approach is promising to solve long-form Video QA in its general setting. Although HCRN only achieves competitive performance (66.1%) against STAGE (68.6%, without temporal supervision) Lei et al. (2019), the direct comparison between HCRN (and other frame-based methods) and STAGE (and other object-based methods) are hard to make because the latter uses more external information from object proposal features. Recent studies Anderson et al. (2018) have shown that object-based features generally bring improvements to VQA models compared to frame-based ones with the trade off of using more external annotated data and models.

Even though BERT is designed to do contextual embedding which pre-computes some word relations, HCRN still shows additional benefit in modeling relations between segments in the passage. However, this benefit is not as clearly demonstrated as in the case of using GloVe embedding coupled with BiLSTM (Table 4 - Exp. 10 vs. Exp. 8 and Exp. 14 vs. Exp. 15). To deeper understanding the behavior of HCRN, we will concentrate our further analysis using comparison with GloVe + LSTM.

Table 5 Ablation studies on TGIF-QA dataset. For count, the lower the better (MSE) and the higher the better for the others (accuracy). When not explicitly specified, we use 𝑘𝑚𝑎𝑥=𝑛−1,𝑡=2 for relation order and sampling resolution.
Full size table
It clearly shows that using hidden states of BiLSTM to represent subtitle features directly for classification has a very limited effect (See Table 4 - Exp. 4 vs Exp. 5). The results could be explained as LSTM fails to handle such long subtitles of hundreds of words. In the meantime, pre-selection plays a critical role in finding relevant information in the subtitles to a question which boosts performance from 42.3% to 57.9% when using full subtitles and from 41.8% to 62.1% when using timestamps (See Exp. 6). Our HCRN further improves approximately 1.0 points (without timestamp annotation) and 1.5 points (with timestamp annotation) comparing to the baseline of GloVe embeddings and BiLSTM with pre-selection for the textual stream only setting. As for the effects on visual stream, our HCRN gains around 1.0 points over the baseline of averaging visual features. This leads to an improvement over 1.0 points when leveraging both visual and textual streams together (See Exp. 13 vs. Exp. 16) on both settings with timestamp annotation and without timestamp annotation. Even though our results are behind Lei et al. (2018), we wish to point out that their context matching model with the bi-directional attention flow (BiDAF) Seo et al. (2016) significantly contributes to the performance. Whereas our model only makes use of vanilla BiLSTM to sequence modeling. Therefore, a direct comparison between the two methods is unfair. We instead mainly compare our method with a simple variant of Lei et al. (2018) by dropping off the BiDAF to show the contribution of our CRN unit as well as the HCRN network. This is equivalent to the baseline (B) S. + V. + Q. (w/ pre-selection) in this paper.

Ablation Studies
To provide more insights about the roles of HCRN’s components, we conduct extensive ablation studies on the TGIF-QA and TVQA dataset with a wide range of configurations. The results are reported in Table 5 for the short-form Video QA and in Table 6 for the long-form Video QA.

Short-form Video QA
Overall we find that our sampling method does not hurt the HCRN performance that much while it is much more effective than the one used by Zhou et al. (2018) in terms of complexity. We also find that ablating any of the design components or CRN units would degrade the performance for temporal reasoning tasks (Action, Transition and Action Counting). The effects are detailed as follows.

Effect of relation order 𝑘𝑚𝑎𝑥 and resolution t:

Without relations (𝑘𝑚𝑎𝑥=1,𝑡=1) the performance suffers, specifically on actions and events reasoning, whereas counting tends to be better. This is expected since those questions often require putting actions and events in relation to a larger context (e.g., what happens before something else), while motion flow is critical for counting but for far-term relations. In this case, most of the tasks benefit from increasing sampling resolution t (𝑡>1) because of a better chance to find a relevant frame as well as the benefits of the far-term temporal relation learned by the aggregating sub-network 𝑝𝑘(.) of the CRN unit. However, when taking relations into account (𝑘𝑚𝑎𝑥>1), we find that HCRN is robust against sampling resolution t but depends critically on the maximum relation order 𝑘𝑚𝑎𝑥. The relative independence w.r.t. t can be due to visual redundancy between frames, so that resampling may capture almost the same information. On the other hand, when considering only low-order object relations, the performance is significantly dropped in Action and Transition tasks, while it is slightly better for counting and Frame QA. These results confirm that high-order relations are required for temporal reasoning. As the Frame QA task requires only reasoning on a single frame, incorporating temporal information might confuse the model. Similarly, when the model only makes use of the high-order relations (Fix 𝑘=𝑘𝑚𝑎𝑥,𝑘𝑚𝑎𝑥=𝑛−1,𝑡=2), the performance suffers, suggesting combining both low-order object relations and high-order object relations is a lot more efficient.

Effect of hierarchy:

We design two simpler models with only one CRN layer:

1-level, 1 CRN video on key frames only: Using only one CRN at the video-level whose input array consists of key frames of the clips. Note that video-level motion features are still maintained.

1.5-level, clip CRNs → pooling: Only the clip-level CRNs are used, and their outputs are mean-pooled to represent a given video. The pooling operation represents a simplistic relational operation across clips. The results confirm that a hierarchy is needed for high performance on temporal reasoning tasks.

Effect of motion conditioning:

We evaluate the following settings:

w/o short-term motions: Remove all CRN units that condition on the short-term motion features (clip level) in the HCRN.

w/o long-term motions: Remove the CRN unit that conditions on the long-term motion features (video level) in the HCRN.

w/o motions: Remove motion feature from being used by HCRN. We find that motion, in agreeing with prior arts, is critical to detect actions, hence computing action count. Long-term motion is particularly significant for the counting task, as this task requires maintaining a global temporal context during the entire process. For other tasks, short-term motion is usually sufficient. E.g. in the Action task, wherein one action is repeatedly performed during the entire video, long-term context contributes little. Not surprisingly, motion does not play a positive role in answering questions on single frames as only appearance information needed.

Table 6 Ablation studies on TVQA dataset. S., Q., V. stand for subtitle, question and visual features, respectively. MUL denotes the use of multiplicative relation in the conditioning sub-network ℎ𝑘(.,.) in Sec. 3.1.
Full size table
Effect of linguistic conditioning and multiplicative relation:

Linguistic cues represent a crucial context for selecting relevant visual artifacts. For that we test the following ablations:

w/o question @clip level: Remove the CRN unit that conditions on question representation at clip level.

w/o question @video level: Remove the CRN unit that conditions on question representation at video level.

w/o linguistic condition: Exclude all CRN units conditioning on linguistic cue while the linguistic cue is still in the answer decoder. Likewise, the multiplicative relation form offers a selection mechanism. Thus we study its effect as follows:

w/o MUL. in all CRNs: Exclude the use of multiplicative relations in all CRN units.

w/ MUL. relation for question & motion: Leverage multiplicative relations in all CRN units.

We find that the conditioning question provides an important context for encoding video. Conditioning features (motion and language), through the multiplicative relation as in Eq. 3, offers further performance gain in all tasks rather than Frame QA, possibly by selectively passing question-relevant information up the inference chain.

Effect of subset sampling:

We conduct experiments with the full model of Fig. 5 (a) with 𝑘𝑚𝑎𝑥=𝑛−1,𝑡=2. The experiment “Sampled from pre-computed superset” refers to our CRN units of using the same sampling trick as what is in Zhou et al. (2018), where the set 𝑄𝑘 is sampled from a pre-computed collection of all possible size-k subsets. “Directly sampled”, in contrast, refers to our sampling method as described in Alg. 1. The empirical results show that directly sampling size-k subsets from an input set does not degrade much of the performance of the HCRN for short-form Video QA, suggesting it a better choice when dealing with large input sets in size to reduce the complexity of the CRN units.

Long-form Video QA
We focus on studying the effect of different options for the conditioning sub-network ℎ𝑘(.,.) in Sec. 3.1 which reflects the flexibility of our model when dealing with different forms of input modalities. The options are:

S. + Q. (w/ MUL. + w/ LSTM): Only textual stream is used. The conditioning sub-network ℎ𝑘(.,.) is with multiplicative relation between tuples of segments and conditioning feature, and coupled with BiLSTM as formulated in Eqs. (4, 5 and 6).

S. + Q. (w/ MUL. + w/o LSTM): Remove the BiLSTM network in the experiment S. + Q. (w/ MUL. + w/ LSTM) to evaluate the effect of sequential modeling in textual stream.

S. + Q. (w/o MUL. + w/ LSTM): The multiplicative relation in the experiment S. + Q. (w/ MUL. + w/ LSTM) is now replaced with a simple concatenation of conditioning feature and tuples of segments.

S. + Q. (w/o MUL. + w/o LSTM): Remove the BiLSTM network in the right above experiment S. + Q. (w/o MUL. + w/ LSTM) to evaluate the effect of when both selective relation and sequential modeling are missing.

V. + Q. (w/ MUL. + w/ LSTM): Only visual stream is under consideration. We use the multiplicative form as in Eq. 3 for the conditioning sub-network instead of a simple concatenation operation in Eq. 2. We additionally use a BiLSTM network for sequential modeling as same as the way we have done with the textual stream. This is because both visual content and textual content are temporal sequences by nature.

V. + Q. (w/ MUL. + w/o LSTM): Similar to the above experiment V. + Q. (w/ MUL. + w/ LSTM) but without the use of the BiLSTM network.

V. + Q. (w/o MUL. + w/ LSTM): The conditioning sub-network is simply a tensor concatenation operation. This is to compare against the one using multiplicative form V. + Q. (w/ MUL. + w/ LSTM).

V. + Q. (w/o MUL. + w/o LSTM): Similar to V. + Q. (w/o MUL. + w/ LSTM) but without the use of the BiLSTM network for sequential modeling.

S. + V. + Q. (S. w/ MUL. + LSTM, V. w/ MUL.): Both two streams are present for prediction. We combine the best option of each network stream, S. + Q. (w/ MUL. + w/ LSTM) for the textual stream and V. + Q. (w/ MUL. + w/o LSTM) for the visual stream for comparison.

It is empirically shown that the simple concatenation as in Eq. 2 is insufficient to combine conditioning features and output of relation network in this dataset. In the meantime, the multiplicative relation between the conditioning features and those relations is a better fit as it greatly improves the model’s performance, especially in the visual stream. This shows the consistency in empirical results between long-form Video QA and shot-form Video QA where the relation between visual content and the query is more about multiplicative (selection) rather than simple additive.

On the other side, sequential modeling with BiLSTM is more favorable for textual stream than visual stream even though both streams are sequential by nature. This well aligns with our analysis in Sec. 3.1.

Table 7 Results for going deeper hierarchy on MSRVTT-QA dataset. Run time is reduced by factor of 4 for going from 2-level to 3-level hierarchy.
Full size table
Deepening Model Hierarchy Saves Time
Fig. 9
figure 9
Extended examples on the TGIF-QA dataset. HCRN, with advantages of hierarchical relation modelings, can handle hard questions that require combinations of near-term frame relations and far-term frame relations (1-4). However, HCRN often struggles to handle questions requiring fine-grain object detection and recognition (5-6).

Full size image
We test the scalability of the HCRN on long videos in the MSRVTT-QA dataset, which are organized into 24 clips (3 times longer than the other two datasets). We consider two settings:

2-level hierarchy, 24 clips→1 vid: The model is as illustrated in Fig. 5, where 24 clip-level CRNs are followed by a video-level CRN.

3-level hierarchy, 24 clips→4 sub-vids→1 vid: Starting from the 24 clips as in the 2-level hierarchy, we group 24 clips into 4 sub-videos, each is a group of 6 consecutive clips, resulting in a 3-level hierarchy. These two models are designed to have a similar number of parameters, approx. 44M.

The results are reported in Table 7. Unlike existing methods which usually struggle with handling long videos, our method is scalable for them by offering deeper hierarchy, as analyzed theoretically in Sec. 3.4. The theory suggests that using a deeper hierarchy can reduce the training time and inference time for HCRN when the video is long. This is validated in our experiments, where we achieve 4 times reduction in training and inference time by going from 2-level HCRN to 3-level counterpart whilst maintaining competitive performance.

Discussion
HCRN presents a new class of neural architectures for multimodal Video QA, pursuing the ease of model construction through reusable uniform building blocks. Different from temporal attention based approaches which put effort into selecting objects, HCRN concentrates on modeling relations and hierarchy in different input modalities in Video QA. In the scope of this work, we study how to deal with visual content and subtitles where applicable. The visual and text streams share similar structure in terms of near-term, far-term relations and information hierarchy. The difference in methodology and design choices between ours and the existing works leads to distinctive benefits in different scenarios as empirically proven.

Within the scope of this paper, we did not consider the audio channel and early fusion between modalities, leaving these issues open for future investigation. Since the design of the CRN unit is generic, we envision an HCRN for the audio stream to be similar to the textual stream in Sec. 3.2.3. Regarding the early fusion, as inputs to a CRN unit are generic objects of the same tensor size, it can be used to combine a mixture of objects coming from different input modalities at any hierarchical level. Alternatively, we can use separate CRN units to handle object sets coming from different input modalities and then combine them all together at the end of each hierarchical level. As the audio, subtitle and visual content are often partly synchronized, modeling the cross-modalities interactions at an early stage can bring benefits to the reasoning process.

Although HCRN has shown its strong capability in modeling near-term frame relations and far-term frame relations, our analysis of HCRN’s behaviors on the TGIF-QA dataset reveals that it often struggles to answer questions that require fine-grain object detection and recognition (See Fig. 9). This suggests a promising direction for future works to include object-oriented representation of videos as these are native to the CRN unit, thanks to its generality. As CRN is a relational model, both cross-object relations and cross-time relations can be modeled. Research towards this direction will improve the interpretability of the model and reflect better the way humans reason about the visual world. CRN units can be further augmented with attention mechanisms to cover better object selection ability, so that related tasks such as frame QA in the TGIF-QA dataset can be further improved.

Conclusion
We introduced a general-purpose neural unit called Conditional Relational Networks (CRNs) and a method to construct hierarchical networks for multimodal Video QA using CRNs as building blocks. A CRN is a relational transformer that encapsulates and maps an array of tensorial objects into a new array of relations, all conditioned on a contextual feature. In the process, high-order relations among input objects are encoded and modulated by the conditioning feature. This design allows flexible construction of sophisticated structure such as stack and hierarchy, and supports iterative reasoning, making it suitable for QA over multimodal and structured domains like video. The HCRN was evaluated on multiple Video QA datasets covering both short-form Video QA where questions are mainly about activities/events happening in a short video (TGIF-QA, MSVD-QA, MSRVTT-QA), and long-form Video QA where answers are located at either visual cues or other information channels such as as movie subtitles (TVQA dataset). HCRN demonstrated its competitive reasoning capability on a wide range of different settings against state-of-the-art methods. The examination of CRN in Video QA highlights the significance of a generic neural reasoning unit that supports native multimodal interactions and compositional reasoning in complex visual reasoning.