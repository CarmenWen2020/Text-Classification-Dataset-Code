Transformer architectures have brought about fundamental changes to computational linguistic field, which had been dominated by recurrent neural networks for many years. Its success also implies drastic changes in cross-modal tasks with language and vision, and many researchers have already tackled the issue. In this paper, we review some of the most critical milestones in the field, as well as overall trends on how transformer architecture has been incorporated into visuolinguistic cross-modal tasks. Furthermore, we discuss its current limitations and speculate upon some of the prospects that we find imminent.

Access provided by University of Auckland Library

Introduction
Ever since the advent of deep learning revolution, the de-facto standard for cross-modal tasks with language and vision has been to use convolutional neural networks (CNN) (LeCun et al. 1998; Krizhevsky et al. 2012) to extract features from visual domain, with VGG (Simonyan and Zisserman 2015) or ResNet (He et al. 2016) being the most frequently used CNN architectures, while employing recurrent neural networks (RNN) (Elman 1990), such as long short-term memory (LSTM) (Hochreiter and Schmidhuber 1997) or gated recurrent unit (GRU) (Cho et al. 2014), to learn the language representation. While a plethora of variations exist as to specific ways to extract features and how to blend them into common embedding space, the fundamental pipeline has almost invariably been restricted to the combination of CNN and RNN.

Fig. 1
figure 1
A brief timeline showing important models in vision (upper) and language (lower) domains respectively, and how they have affected approaches in cross-modal domains (middle)

Full size image
This steadfast landscape started to change with the introduction of transformer architecture (Vaswani et al. 2017). Transformer has first demonstrated its capacity in natural language processing (NLP), achieving state-of-the-art performances in countless number of NLP tasks (Peters et al. 2018; Dai et al. 2019; Yang et al. 2019b), and has rapidly replaced recurrent neural networks. Its application has also been expanded to speech recognition domain (Dong et al. 2018; Wang et al. 2020c). While a variety of transformer-based language models exist, BERT (Devlin et al. 2019) has particularly gained wide attention not only with its performance, but also with its unique approach to pre-training and adaptability to downstream tasks. A GPT-line of works (Radford and Sutskever 2018; Radford et al. 2019; Brown et al. 2020) have also demonstrated that pre-training with a very large corpus and fine-tuning the model to a target task can outperform conventional models by a large margin. In particular, GPT-3 shows that pre-training with extremely large amount of corpus and parameters easily extends to high performance in few-shot learning tasks without performing any fine-tuning.

Success of transformer architecture in language domain has naturally led to its further application in cross-modal tasks involving language and vision. ViLBERT (Lu et al. 2019) was one of the first models to demonstrate that pre-training objectives of BERT can be extended to cross-modal learning, and that it obtains state-of-the-art or comparable performances to models based on conventional CNN-RNN approach. Many other models followed with similar approach, and now the assumption that pre-training with a large amount of data leads to superior performance holds true for cross-modal domain as well.

Yet, many important arguments can be raised with regards to the limitations and prospects of transformer-based cross-modal models. For example, while most models require that images or videos be tokenized and serialized in some way, they still fundamentally rely on CNN-based models to extract features for each visual token. It also remains arguable whether transformers learn fundamentally superior embeddings or whether its performance is simply due to a large amount of computations and data, bringing up the issue of computational efficiency. Furthermore, blending the transformer architecture with generative models is an open challenge, which is just beginning to be explored.

Figure 1 shows a brief timeline over the past few years with important models in vision and language domains respectively, and how they have influenced the approaches for cross-modal tasks. Looking back at the important milestones and current trends, we project that transformer architecture may take over the vision representation part of cross-modal tasks, as its performance in vision domain has been shown to be comparable to CNNs (Dosovitskiy et al. 2020). More about this will be discussed in Sects. 5 and 6.

This paper attempts to review some of the representative works on transformer-based cross-modal models, with emphasis on pre-training schemes, discussing the common characteristics in various models along with their differences. We also investigate models of novel and promising direction, such as transformer-based vision representation, or image generation from text. By doing so, our goal is to acquire up-to-date insights as to various aspects of cross-modal learning, as well as prospects on how they may continue to influence deep learning field.

The rest of the paper is organized as following: we first review the conventional visuolinguistic cross-modal tasks and recent benchmark tasks, along with frequently used evaluation metrics in Sect. 2. We also review the elementary transformer architecture and its variations, mostly restricted to language domain, centered around BERT in Sect. 3. We then introduce and inspect the recent models on cross-modal tasks that employ transformer architecture in Sect. 4, focusing on their architectural modifications and pre-training schemes. While the works introduced in Sect. 4 mostly employ transformer architecture to acquire linguistic representation, Sect. 5 introduces works that examine representing vision with transformer architecture, suggesting the possibility of replacing convolutional neural networks. Section 6 discusses some of the prospects on transformer architecture for cross-modal tasks. Finally, we conclude the paper in Sect. 7, by summarizing the important points raised in the paper, while also discussing the current limitations and future works.

Preliminaries I: Visuolinguistic Tasks
In this section and Sect. 3, we briefly go over the preliminaries necessary to understand the topics and implications put forward by this paper. We first review some of the representative tasks in visuolinguistic domain, namely image captioning and visual question answering, with commonly used approaches. We also visit other important tasks, some of which have become benchmark tasks for transformer-based cross-modal tasks. We then go over evaluation metrics for these tasks.

Classical Tasks
Prior to deep learning era, early models frequently tackled visuolinguistic cross-modal tasks with template-based model (Barbu et al. 2012; Elliott and Keller 2013; Ushiku et al. 2012) or ranking and retrieval models (Farhadi et al. 2010; Ordonez et al. 2011; Hodosh et al. 2013). With the advent of deep learning, however, the mainstream paradigm for tackling cross-modal tasks has rapidly shifted towards the approach incorporating convolutional and recurrent neural networks.

Image captioning and visual question answering (VQA) (Antol et al. 2015; Zhang et al. 2016; Goyal et al. 2017) have conventionally been considered two of the most representative examples of cross-modal tasks involving language and vision. In image captioning, the model is trained with pairs of image and the captions describing that image, and learns to generate descriptive captions for unseen images. While early works relied on straightforward combination of CNN and LSTM, (Vinyals et al. 2015; Karpathy and Li 2014), advanced models appeared with dense localization (Johnson et al. 2016), semantic attention (Xu et al. 2015; You et al. 2016; Zhou et al. 2016), to name a few. Some works (Dai et al. 2017) went steps further to incorporate generative adversarial networks (GANs) (Goodfellow et al. 2014) for image captioning, in which GANs are used to predict whether the captioning is natural.

VQA is a task in which a question in natural language is asked about an image, and the model is asked to provide an answer to the question. While many variations of VQA task exist, VQA as a benchmark task usually refer to the one based on MS COCO (Lin et al. 2014) with roughly 0.6M questions, each of which comes with 10 crowd-sourced answers. For VQA, most approaches can be categorized by 4 primary components; image representation, text representation, common embedding scheme, and attention mechanism. Image representation mostly relied on CNNs, often employing region detection models, while text representations may rely on RNN-line of models, Skipthoughts (Kiros et al. 2015), or more classical models, such as word2vec (Mikolov et al. 2013) or Glove (Pennington et al. 2014). While cross-modal embedding scheme may be simple concatenation or element-wise addition and multiplications, more sophisticated schemes have also been proposed, such as compact bilinear pooling (Fukui et al. 2016), low-rank bilinear pooling (Kim et al. 2017), or cross-modal tucker fusion (Ben-younes et al. 2017). Various attention mechanisms (Yang et al. 2015; Lu et al. 2016) have also been proposed, and have demonstrated its effectiveness.Footnote1

Image captioning and VQA have also been extended to cross-modal domain involving language and video, as video captioning (Das et al. 2013; Gella et al. 2018) and video QA (Tapaswi et al. 2016) respectively. As in image captioning and VQA, the models in video domain (Donahue et al. 2014; Venugopalan et al. 2015b, a) still heavily relied on CNNs and RNNs for representing video and language.

Benchmark Tasks
While image captioning and visual question answering have been exemplary visuolinguistic cross-modal tasks, other important and intriguing variations have emerged. In particular, some of these novel variations, along with VQA task, have been consolidated as benchmark tasks for pre-trained cross-modal tasks. Interestingly, image captioning task, despite being a representative visuolinguistic task, has only rarely been tackled with recent transformer-based models. We conjecture that it may be attributable to the fact that image captioning task employs a different array of evaluation metrics, and makes it less intuitive to compare the performance along with other simple accuracy-based tasks, as we shall see in Sect. 2.3. However, it is still being actively examined in video and language field, and is likely to remain an important indicator of cross-modal models’ performances.

One of the frequently visited benchmark tasks is visual commonsense reasoning (VCR) (Zellers et al. 2019), which extends VQA by not only asking a question about a referred agent’s action, but also asking why the model chose that answer. This notably attempts to accomplish the logical inference made by humans. Three subtasks with varying difficulties exist within the task, where 1) the question is given and the model predicts the answer (Q→A), or 2) the question and the answer are given and the model predicts the reason or intention (QA→R), or 3) the question is given and the model predicts both the answer and the reason for that answer (Q→AR). VCR dataset contains roughly 290k multiple choice questions accompanied by bounding boxes and semantic masks to indicate which agent the question or the answer candidate refers to.

Natural language for visual reasoning (NLVR) (Suhr et al. 2019) also requires an advanced understanding of vision and language, by judging whether a statement is true with regards to a juxtaposition of two images, based on the dataset consisting of roughly 100k human statements made about the pairs of images. The unique setting of the task in which there are two images to consider brings new challenges as to how to embed the images, e.g., whether they should be embedded separately or together, and how the decision should affect common embedding with text. RefCOCO (Yu et al. 2016) is also a frequently visited benchmark task, where the goal is to identify which object in the image is referred to by the linguistic statement, based on the dataset consisting of 140k referring expressions from ReferitGame (Kazemzadeh et al. 2014). Grounding referring expressions (Kazemzadeh et al. 2014), caption-based image retrieval (Young et al. 2014), and embodied visual recognition (Yang et al. 2019a) are also noteworthy tasks involving vision and language. Finally, image generation from text is also an important yet largely unexplored axis in visuolinguistic task. So far, it has mostly been limited to certain target domains, such as birds or flowers (Reed et al. 2016; Zhang et al. 2018b), but promising works have started to appear, as we will see in Sect. 6.

As we shall see in Sect. 4.1.2, benchmark tasks appear to be less consolidated in video and language domain, although video captioning tasks with YouCook2 (Zhou et al. 2017) and ActivityNet (Heilbron et al. 2015) are frequently visited.

Evaluation Metrics
Evaluation metrics for visuolinguistic tasks can be categorized as those for classification task and those for generation tasks, such as image captioning. While the classification tasks mostly rely on straightforward accuracy, generation tasks usually employ multiple metrics for evaluation. BLEU (Papineni et al. 2002), originally proposed for machine translation, is one of the most frequently used evaluation metrics and computes the portion of n-grams in candidate caption that also appear in ground truth. While BLEU places emphasis on precision, ROUGE (Lin 2004) takes more recall-oriented approach by counting the occurrences of exact match of n-grams. While BLEU and ROUGE look for exact match of words, METEOR (Banerjee and Lavie 2005) evaluates n-grams considering synonyms and stems, based on WordNet (Miller 1995). CIDEr (Vedantam et al. 2015), designed specifically for image captioning task, looks at the consensus between generated image caption and the reference caption, with assigning more weights on n-grams that appear specifically for the image and less weights on n-grams that appear frequently for all images. SPICE (Anderson et al. 2016) is also a popular metric for image captioning task, where a set of tuples are extracted from the semantic parse graph of the sentences. All of these metrics are also widely used for video captioning task as well.

Most of the benchmark tasks apart from captioning evaluate the performance with straightforward accuracy, although accuracy might be defined slightly differently depending on the task. For example, VQA defines accuracy with respect to the number of humans that provided the same answer out of all ground truth answers available for the question. Also, GQA (Hudson and Manning 2019) proposes to supplement accuracy by proposing additional metrics such as consistency, validity and plausibility. While the straightforward nature of the accuracy metric may risk loss of accountability for more subtle aspects of performance, such as the model’s ability to yield the second best answer when not giving the correct answer, it is usually reported over a set of frequently visited tasks, and such aggregated evaluations usually appear to retain mutual agreement to a fair extent. Also, when necessary, accuracy is measured at different recall levels, as in image retrieval task shown in Table 1.

Preliminaries II: Transformer-Based Models
In this section, we first describe transformer architecture by looking at multi-heads self-attention mechanism. We then introduce BERT, which is a transformer-based model and has become a crucial component in recent surge of transformer-based cross-modal models, with emphasis on its unique pre-training objectives. We also introduce pre-training objectives from models other than BERT for reference.

Transformer Architecture
Vaswani et al. (2017) proposed transformer architecture, and demonstrated that it outperforms the then-dominant RNN or CNN-based approaches on several sequential transduction tasks, such as machine translation. Transformer comprises an encoder part and a decoder part, and both of them consist of a series of self-attention based modules. Differently from CNN and RNN, it adopts self-attention as a basic operation in the model instead of convolution or memory gating, which leads to obtaining suitable properties for handling sequential data as will be shown later.

In a self-attention process, each input vector is first transformed into three vectors called query, key, and value. The output is computed as a weighted sum of the values, where the weight of each value is assigned according to the similarity between the query and the corresponding key. Let Q, K, and V be matrices that contain all queries, keys, and values extracted from the given input vectors, respectively. The self-attention process can then be formulated as

Attention(𝑄,𝐾,𝑉)=softmax(𝑄𝐾𝑇𝑑𝑘‾‾‾√)𝑉,
(1)
where 𝑑𝑘 is a dimensionality of the key.

To boost the flexibility of the self-attention process, the transformer adopted multi-head attention mechanism instead of the simple self-attention, as shown in Eq. (2). In multi-head attention, several self-attention processes are conducted in parallel, and each output is integrated by concatenation followed by a linear projection to obtain the final output vector as shown in the following equations:

MultiHead(𝑄,𝐾,𝑉)=Concat(head1,...,headℎ)𝑊𝑂,
(2)
head𝑖=Attention(𝑄𝑊𝑄𝑖,𝐾𝑊𝐾𝑖,𝑉𝑊𝑉𝑖),
(3)
where 𝑊𝑂 is a projection matrix to integrate the outputs of all attention processes. The queries, keys, and values for each attention process are computed by a linear projection of the original ones, and their projection matrices 𝑊𝑄𝑖, 𝑊𝐾𝑖, and 𝑊𝑉𝑖 are jointly optimized with 𝑊𝑂 and other trainable parameters via training.

In the self-attention process, the order of the input vectors does not affect the resulting output vectors. For example, a model’s prediction on a certain sentence does not depend on the order of words in the sentence, which should be inappropriate in many NLP tasks. To avoid this problem, position information of each input is encoded and added to the input embedded token before being fed to the transformer-based model. Specifically, the position information is encoded as following:

PE(𝑝,2𝑖)=sin(𝑝/100002𝑖/𝑑model),
(4)
PE(𝑝,2𝑖+1)=cos(𝑝/100002𝑖/𝑑model),
(5)
where p is the position of the target token in the input sequence, 𝑑model and i are the dimensionality of the embedded token and its index, respectively. As for positional embedding, other embedding methods have been proposed. For example, Wang et al. (2021a) have shown that relative sinusoidal positional embedding outperforms absolute positional embedding in longer distances. Modified attention matrix (MAM) (Dufter et al. 2021) has also been proposed, where positional embedding is added as bias of attention map.

Some of the distinctive merits of transformer over previous models are modeling long-term dependency and its flexibility for parallel computing. For instance, RNNs generally find it difficult to preserve context as the distance between tokens becomes longer. Since they are executed in a sequential manner, it also becomes non-trivial to run them in parallel. Long-distance dependency is also challenging for CNNs, as it would require a proportionally large number of layers. On the other hand, attention mechanism can model dependency for words of any distance within the pre-defined sequence length, and as it does not require sequential order of the input, it is highly suitable for parallel, distributed computation.

BERT
While a large number of variations on transformer exist, BERT (bidirectional encoder representations from transformers) (Devlin et al. 2019) is particularly important in our topic, as its architecture has been employed by many cross-modal models, as well as its pre-training tasks that have been extended to account for cross-modal setting. Following transformer, BERT applies layer normalization (Ba et al. 2016) on top of multi-head attention with residual connection, and applies feed-forward propagation with its unique choice of non-linear activation function, namely Gaussian error linear units (GELU) (Hendrycks and Gimpel 2016).

BERT is also known for its unique choice of pre-training tasks, namely masked language modeling and next-sentence prediction. In masked language modeling, tokens are randomly masked with 15% probability, and the model is trained to predict those masked tokens. For next sentence prediction, two sentences are provided, where the second sentence may be actual succeeding sentence or a random sentence with 50% probability, and the model is given a task of binary classification of whether the second sentence is an actual succeeding sentence. While the first objective of masked language modeling aims to learn token-level dependency, the motivation of the second task is to learn the inter-sentence relationship. Although Liu et al. (2019b) has shown that the next sentence prediction turns out not to be crucial for the model’s performance, these two objectives, along with the bidirectional architecture, characterize and differentiate BERT from other transformer-based language models.

For reference, we would like to highlight a few more notable pre-training objectives proposed by other models. XLNet (Yang et al. 2019b) attempts to overcome some of the drawbacks of masked language modeling of BERT, e.g., the absence of masked tokens in real data, or its inability to model the joint probability with product rule used in autoregressive language modeling. Specifically, they propose permutation language modeling, where the model maximizes likelihood of the input sequence over all possible permutations of the factorization order, thereby learning bidirectional context while retaining the merits of autoregressive models. MT-DNN (Liu et al. 2019a) performs multi-task learning, which leverages supervised data from multiple related tasks, such as single-sentence or pairwise text classification and relevance ranking, on top of language model pre-training. The paper claims that it prevents the model from overfitting to a specific task, with complementary effect to general language model pre-training. Chang et al. (2020) proposes three pre-training objectives with emphasis on capturing granularity of semantics between the query and document, namely inverse cloze task to capture local semantic context, body first selection to capture global context within the document, and wiki link prediction to capture distant inter-article context. Their experiments suggest that the proposed pre-training objectives can perform significantly better than masked language modeling.

Analysis of Cross-Modal Embeddings
We now look into cross-modal models that have employed transformer/BERT architectures. We first review their pre-training objectives, many of which have been directly inspired by BERT, albeit with notable variations, and speculate on how they relate to the models’ performances. We also look at the models’ attempts to construct network architectures that learn inter-modal dependency, along with how they deal with distinct modalities for mutual compatibility.

Pre-training Tasks
One of the primary arguments laid by the transformer-based language models, including BERT, is that large-scale pre-training is the key to their successful performance. It follows evidently that the choice of pre-training tasks is essential in obtaining a high-quality language model. Cross-modal models employing transformer architecture have generally followed the same presumption, placing a strong emphasis on the design of pre-training objectives, and the key challenge is consequently how to replicate large-scale pre-training with cross-modal setting. For example, as shown in Sect. 3.2, BERT is known for its unique pre-training tasks of masked language modeling and next sentence classification, but since it is designed solely for language, it inevitably requires adjustments to be extended to cross-modal setting. While most models adopt those two pre-training tasks in modified ways, some models propose additional pre-training tasks designed specifically for cross-modal setting.

Masked language modeling, one of the two pre-training tasks of BERT, is used as is for linguistic input tokens in cross-modal models, almost without exception. One notable variation is InterBERT (Lin et al. 2021), which performs masked language modeling by replacing the masked word with continuous words, rather than random words as with most other models. B2T2 (Alberti et al. 2019) also extends the task by training language model while seeing the image. Another pre-raining task of BERT, next sentence prediction, is mostly converted to binary classification of whether input image and sentence are semantically matched. This also is performed by most cross-modal models, with few exceptions such as VL-BERT (Su et al. 2020) or UnifiedVLP (Zhou et al. 2019) that explicitly opt not to perform this pre-training task. InterBERT again makes a unique variation by explicitly involving matching with hard negative examples.

One of the key challenges for pre-training tasks among cross-modal embedding models is with the way they implement masked language modeling task for visual inputs, as it cannot be naively extended to vision domain in a straightforward manner, due to the non-sequential nature of vision. In fact, some models, such as B2T2 and VisualBERT (Li et al. 2019), simply opt not to perform any extended masked modeling task for visual inputs. On the other hand, many models propose novel ways to apply masked language modeling for visual tokens. For example, ViLBERT handles this challenge by proposing to mask the image regions extracted by Faster R-CNN (Ren et al. 2015), and trains the model to predict the class distribution of the region, with the class distribution output from Faster R-CNN as the ground truth. VL-BERT (Su et al. 2020) and Unicoder-VL (Li et al. 2020a) are also notable examples that perform masked visual modeling with class prediction. In particular, VL-BERT proposes masked RoI classification with linguistic clues, where a region of interest (for example, a cat) is randomly masked out, and the model is trained to predict the category of the masked out RoI by solely relying on linguistic clues, such as “kitten drinking from bottle.” Here, the RoIs are obtained via Fast R-CNN (Girshick 2015). Such setting is derived from the concern that there may be cases where it is nearly impossible to identify a region when the region is masked. While VL-BERT opts not to perform image-sentence matching task, this unique setup intends to make up for the learning of cross-modal dependency.

While the models above relied on the prediction of class distribution, it has also been found beneficial to incorporate feature regression into masked visual modeling task. LXMERT (Tan and Bansal 2019) is one such example. On top of masked object classification, where the label for the masked RoI should be classified based on other visual inputs and linguistic inputs, they also perform RoI-feature regression with L2 loss. UnifiedVLP also reports that combining class prediction and feature prediction improves performance. UNITER (Chen et al. 2020c) also performs both class prediction and feature regression, but they propose to add a third task for masked visual modeling, where they perform class prediction with KL divergence.

Some models further propose novel pre-training tasks that do not fall into any of the 3 categories above, which frequently depends on the target downstream task. For example, LXMERT, SemVLP (Li et al. 2021a) and (Kervadec et al. 2019) propose image question answering task for pre-training, whereas PixelBERT (Huang et al. 2020b) employs pixel random sampling. Luo et al. (2020a) proposes contrastive pre-training, where they encourage the model to learn similar representations for sequences that share the same semantics by matching the original sequence and the corresponding corrupted sequence.

While most models so far have relied on Fast/Faster R-CNN for region extraction, MiniVLM (Wang et al. 2020b) employs a region extraction module inspired by EfficientDet (Tan et al. 2020), and as such, they first attempt to enhance their visual features by performing large-scale image classification and object detection with Objects365 dataset (Shao et al. 2019). Cross-modal representation is subsequently fine-tuned with captioning and tagging, where captions and tags are obtained by existing models.

Table 1 Comparison of performances of cross-modal models on various tasks
Full size table
Table 2 Pre-training tasks performed by various cross-modal models
Full size table
Table 3 We report the setting for the largest model reported in respective papers
Full size table
Table 4 Comparison of frequently used datasets for pre-training visuolinguistic models
Full size table
Fig. 2
figure 2
Visualization of representative cross-modal models in terms of dataset size used for pre-training, performance as measured by VQA task, the type of pre-training objectives employed, and the model size. The size of block for respective model corresponds to the size of the model, as approximated by the implementation details reported in respective paper. VM1 and VM2 refer to two distinct tasks of masked visual modeling with image regions

Full size image
Analysis of Performance
Table 1 compares the performance of various models on representative cross-modal tasks. Note that the models are trained with different size of datasets with varying size of model sizes, so direct comparison of performance does not necessarily imply superiority of certain models over others. Table 2 compares pre-training tasks performed by each model, with 4 rough categories for pre-training tasks. Table 3 compares settings of the models, ranging from datasets used for pre-training to their implementation details. Figure 2 visualizes representative models in terms of the dataset size used for pre-training, their performance in VQA task, the types of pre-training objectives performed, and the respective model size. While it remains arguable whether we can attribute a certain aspect to the respective model’s performance, the models with higher performance have one or more of the following characteristics in common; pre-trained on a relatively large amount of data, larger model size, and more pre-training objectives. While the observations are quite generic, note that they are consistent with recent trend of pre-training a model with a very large number of parameters with large-scale dataset, as affirmed by GPT-3.

Table  4 shows some of the frequently used datasets for both images and videos, along with the description of their contents, annotation types, and sizes. Although it would be difficult to generalize from a small group of models described so far, as it is easy to be overfitting, further observations can be made with respect to dataset and pre-training objectives that may likely be of reference. It must first be noted that all datasets with images are at the order of millions, once again affirming the indispensable role played by pre-training with large-scale dataset. In fact, even models of relatively small sizes can attain fair performance when pre-trained with large-scale dataset, as is the case with mini-VLM. Pre-training with large-scale dataset also appears to overcome the noisiness of labels obtained via weak, web supervision. For example, ERNIE-ViL is pre-trained with Conceptual Captions and SBU Captions, both of which are ”webly” supervised, yet demonstrates fairly high performance throughout the tasks. To be fair, however, under similar settings, pre-training with manually annotated datasets appears to provide an edge over pre-training with datasets with web supervision, particularly in terms of the size of the dataset necessary to obtain the desirable level of performance, which makes sense considering the inevitable presence of noise in web supervision. Higher efficiency of human annotation in terms of the required dataset size is well-illustrated by VisualBERT and a variation of LAMP, both of which are trained only with MS COCO, displaying more reliable performance than ViLBERT trained with Conceptual Captions, although Conceptual Captions is more than 5 times larger than MS COCO. As noted above, however, it should be highlighted again that this is equivalent to saying, alternatively, that pre-training with web supervision can demonstrate comparable performance to human annotations when given a sufficient amount of data, as is the case with models like ERNIE-ViL. As we shall see in Sect. 4.1.2, a similar tendency can also be observed with video and language models. For example, VideoBERT (Sun et al. 2019), which is pre-trained with large-scale YouTube videos with annotations obtained by YouTube video annotation system and uses human-annotated YouCook2 (Zhou et al. 2017) for evaluation only, falls below the models directly pre-trained with YouCook2, such as UniVL (Luo et al. 2020b).

It may also be inferred that pre-training with dataset designed for the same or similar task as the target task helps boost performance in that target task. LXMERT, OSCAR, and SemVLP, which are trained with at least one dataset designed for image QA task, all demonstrate higher performance on VQA task, as shown in Fig. 2. Similarly, in terms of correlation between pre-training objectives and performance on specific tasks, it appears beneficial to perform similar type of pre-training task for a certain target downstream task. LXMERT and SemVLP are again notable examples, as they propose unique pre-training objective of image QA and indeed achieve better performances on VQA task. MiniVLM also performs additional pre-training objective of image captioning task to demonstrate reliable performance on image captioning task. Furthermore, video and language models pre-trained with YouCook2 annotations can be considered a similar case.

Table 5 Performances of transformer-based cross-modal models on video captioning for YouCook2 (Zhou et al. 2017 and ActivityNet (Heilbron et al. 2015)
Full size table
Video and Language
Extending vision domain of cross-modal tasks from images to videos naturally brings new challenges. For example, a truly reliable supervision would require annotations at frame-level, which necessitates prohibitive amount of manual labour. In fact, existing large-scale video datasets, such as YouTube-8M (Abu-El-Haija et al. 2016) or Sports-1M (Karpathy et al. 2014), often rely on user-attached tags, resulting in noisy labels. As such, self-supervised learning has become a frequently used approach for learning video representations (Vondrick et al. 2018; Agrawal et al. 2015; Wang and Gupta 2015). Transformer-based cross-modal models tackling video and language are relatively newly emerging research topic, and have displayed varying approaches with more task-oriented principles. While many models adopted approaches employed in the models for images, for example the BERT-inspired pre-training objectives, there are also models that opt to diverge, as we shall see below.

When aligning video and language, where language supervision is extracted from the corresponding audio of the unannotated raw videos, a frequent problem is that the semantic of the language and video may not align. For example, the speaker may be talking about cars, where the video shows the speaker himself. Sun et al. (2019) notes that cooking videos have high probability of visual and linguistic semantics temporally well-aligned, and exploit such properties to examine video caption generation and next frame prediction with their VideoBERT. In order to extend BERT’s pre-training objectives to video domain, they obtain visual tokens using hierarchical vector quantization to video features. On top of the masked language modeling and masked visual token modeling, next-sentence classification is extended as alignment classification for visual and linguistic sentences. Even with cooking videos, however, the alignment may still be noisy, and they deal with this problem by concatenating neighboring sentences into a long single sentence, and varying the subsampling rate of video tokens, making the model robust to variations in video speed.

Similarly to VideoBERT, CBT (Sun et al. 2020) employs sliding window approach to extract visual tokens using S3D network (Xie et al. 2018). Visual and linguistic tokens are concatenated and passed to a shallow 1-layer cross-modal transformer, where the mutual information between the two is computed. For pre-training objectives, masked language modeling and masked visual token modeling are employed, where HowTo100M dataset (Miech et al. 2019) is used, rather than crawled videos from YouTube as is the case with VideoBERT. Also, unlike VideoBERT where the features are clustered with vector quantization and the task is to estimate to which cluster the masked feature belongs to, CBT attempts to directly regress the masked features. While the models above rely on densely sampled visual tokens, ClipBERT (Lei et al. 2021) notably demonstrates that sparse visual tokens can be sufficient for video representation.

Novel pre-training objectives that are specific to video and language domain have also appeared. For example, ActBERT (Zhu and Yang 2020) proposes masked action classification, where the objective is to predict the action label given the linguistic and object features, whereas HERO (Li et al. 2020b) proposes frame order modeling. VideoAsMT (Korbar et al. 2020) and UniVL (Luo et al. 2020b) also incorporate generative modeling approach, by proposing to regard each modality as a translation of each other.

Models that do not explicitly employ BERT-inspired approaches have also appeared. Gabeur et al. (2020) proposes multi-modal transformer calibrated for video retrieval task. Rather than directly extending pre-training objectives of BERT, they rely on bidirectional max-margin ranking, enforcing high similarity between video and captions. Notably, they extract video features by the combination of pre-trained models from 7 different domains, which differentiates them from other models that generally employ one pre-trained model for each modality. Similarly, Cooperative hierarchical transformer (COOT) (Ging et al. 2020) proposes cross-modal cycle-consistency loss to enforce semantic alignment, on top of alignment losses from Zhang et al. (2018a), rather than simply extending pre-training objectives of BERT. Zhou et al. (2018) also proposes to use masked transformer dense video captioning task, but simply uses it as a captioning decoder based on inputs from visual encoder and proposal decoder, without explicit pre-training objective designed for cross-modal learning.

As we have seen, while there are only a limited number of transformer-based models dealing with video and language, they tend to be specific to task and domain, taking varying approaches. Many models have employed approaches from cross-modal models for image and language, but it may be still early to capture a definite common ground for these models, since much more remains to be explored yet. Table 5 summarizes the models’ performances on video captioning task, one of the frequently visited tasks in video and language cross-modal domain.

Network Architecture
Fig. 3
figure 3
Comparison of single-stream and two-stream cross-modal transformer blocks. 𝐻(𝑖)𝑣 and 𝐻(𝑖)𝑤 refer to embedding of visual and word tokens respectively, output by i-th layer

Full size image
Network architecture for cross-modal embedding using transformer can be roughly divided into two categories; single-stream models, where the transformer block is modality-specific, and two-stream models, where the inputs to each transformer block are inter-modal (Fig. 3).

ViLBERT (Lu et al. 2019) is a representative two-stream model, in which they proposed co-attention transformer mechanism, with input keys and values of one modality being passed as inputs to the transformer block of another modality. In other words, keys and values for the language are input to transformer block for the vision part, and vice versa. Since the queries for each modality still go into the corresponding modalities, the transformer block ends up learning to embed features for each modality conditioned on the other modality. In order to tokenize the vision, they extract image regions using Faster R-CNN with ResNet (He et al. 2016) backbone, while also employing 5-dimensional spatial location vector. LXMERT (Tan and Bansal 2019) takes a similar approach, where, after embedding each modality and encoding them with transformer separately, cross-modality encoder is applied, for which query vector ℎ𝑘𝑖 from one modality and context vector 𝑣𝑘𝑗 from another modality are the inputs, with k being the number of respective single-modality encoders. DeVLBERT (Zhang et al. 2020) and (Kervadec et al. 2019) also follow ViLBERT by exchanging queries, or equivalently keys and values, for each modality. SemVLP (Li et al. 2021a) notably performs two-stream cross-modal learning only at the upper parts of the blocks, in order to learn high-level semantic alignment. ERNIE-ViL (Yu et al. 2020), while mostly following ViLBERT, incorporates scene graph approach, where the input text is parsed into the nodes of objects, relations, and attributes that the text mentions. Scene graph representation is shown to improve the semantic alignment between vision and language, and as the authors note, hints at the possibility of extending cross-modal tasks to graph neural networks.

Single-stream cross-modal learning is fairly intuitive and simple, as the conventional transformer blocks can be extended in a straightforward manner, without architectural modifications, for concatenated inputs. Unicoder-VL (Li et al. 2020a), UNITER (Chen et al. 2020c), and VisualBERT (Li et al. 2019) are some of the notable examples of single-stream cross-modal models.

Notably, nearly all models perform early fusion of language and vision embeddings, in which they are concatenated prior to being fed to cross-modal transformer blocks. In fact, B2T2 (Alberti et al. 2019), designed specifically for VQA and VCR tasks, modifies dual encoder (Wu et al. 2017; Gillick et al. 2018) and compares performing early fusion with text and bounding boxes to late fusion with the feature from the entire image, and reports that early fusion outperforms the late fusion.

MiniVLM (Wang et al. 2020b) aims at building a light-weight cross-modal model, and unlike the models that employed Fast R-CNN or Faster R-CNN to extract visual features, they propose two-stage efficient feature extractor (TEE), consisting of EfficientNet (Tan and Le 2019) and compact BERT fusion model. In particular, the visual feature extraction consists of EfficientNet with bidirectional feature pyramid network as in EfficinetDet (Tan et al. 2020), followed by non-maximum suppression. MiniVLM is also unique in that they propose a triplet input to cross-modal learning block, consisting of visual features, tokenized sentences, and tokenized object names. The motivation is that inputting object labels explicitly may help further enforce the learning of dependency between objects and corresponding text. Similar approach is also employed by OSCAR (Li et al. 2020c), which inputs object tags along with word tokens and region features.

Apart from the cross-modal learning mechanism and input format, positional embedding is another source of variation among the models. Unicoder-VL (Li et al. 2020a) extracts image regions with Faster R-CNN along with 5-d vector, but they notably use the same position embedding for all image regions, rather than random permutations. In VisualBERT (Li et al. 2019) position embedding for each visual token is matched to the corresponding input token, whenever the alignments between image regions and the input tokens are available.

VL-BERT (Su et al. 2020) proposes a format where each input element consists of 4 different types of embeddings, namely token embedding, visual feature embedding, segment embedding, and sequence position embedding. Most notable is visual feature embedding, which is a combination of visual appearance feature, corresponding to the output of fully-concatenated features from Fast R-CNN for the image region, and visual geometry embedding, a 4-d vector for coordinates of each corner of the region. Note that, for linguistic tokens, the entire image is used for visual feature embedding. Sequence position embedding for image tokens can be randomized.

Both single-stream and two-stream have appeared in video and language domain as well. For example, ActBERT proposes tangled transformer, where each of query, key, and value come from different modalities, while UniVL employs cross encoder, where language and video encodings are combined along the dimension of sequence. On the other hand, E2vid (Huang et al. 2020a) relies on separate-modality architecture, and only partially employs cross-modal co-attention transformer.

In summary, we can make a few observations that are common in most models. While some models explicitly come up with two-stream cross-attention scheme by exchanging inputs to attention heads, many models do not explicitly design cross-attention scheme and simply rely on pre-training objectives for learning cross-modal dependency. Whether single-stream or two-stream, images are tokenized as regions without exception, although the entire image may replace regions in some models for position embedding. Also, regions are mostly accompanied by low-dimensional spatial vectors containing their coordinate information. Features are extracted with pre-trained object detection models rather than classification models, except models like MiniVLM, which employs EfficientNet backbone and further trains the detection model. Attaching position embedding to image regions is one source of variations among the models. Many models employ coordinate-based ordering, while some models simply use random permutation, or the same position embedding for all visual tokens.

Visual Representation with Transformer
So far, most works tackling cross-modal tasks with transformer architecture have primarily applied it to linguistic representation, with certain schemes to embed it together with visual representation, mostly by tokenizing the visual representation in some ways. Such tokenization of visual representations has for the most part relied on pre-trained convolutional neural networks. For example, ViLBERT (Lu et al. 2019) relies on Faster R-CNN to extract image regions as tokens. However, some of the recent researches have suggested that even convolution may be safely replaced by transformer architecture, which may arguably imply that more fundamentally different changes may be possible with obtaining visual representations. While these works do not explicitly deal with cross-modal tasks, they suggest critical implications as to the prospects of cross-modal tasks, as will be discussed in Sect. 6. We thus briefly introduce recent important works on vision representation with transformers in this section.

Dosovitskiy et al. (2020) proposes Vision Transformer (ViT) and suggests that pure transformers can achieve comparable performance on image classification tasks. While closely following the original transformer architecture, they split images into patches, feed the sequence of linear embeddings of these patches as input to transformer, so that 2D images are represented as a sequence of flattened patches. In a similar spirit as masked language modeling in BERT, they perform masked patch prediction, in which the model is trained to predict the mean 3bit color for each patch. In particular, their ablation study with variations in dataset size shows that pre-training with the largest dataset, JFT-300M (Sun et al. 2017), resulted in better performance, implicating that, as in language domain, training a transformer-based model with a massively large amount of image data can lead to models with outstanding performances. Pyramid vision transformer (Wang et al. 2021b) expands upon vision transformer and demonstrates that convolution-free models can be extended to a wider range of computer vision tasks, including object detection, semantic segmentation, and instance segmentation. VidTr (Li et al. 2021b) further shows that videos can also be handled without using convolutions.

iGPT (Chen et al. (2020b)) proposes another direction for applicability of transformer in vision. It proposes to train a sequence transformer for pixel prediction, first by reshaping the pixels into 1D sequence, then by performing pre-training objectives of auto-regressive next pixel prediction and masked pixel prediction. The model that is trained at the scale and architecture of GPT-2 turns out to outperform Wide ResNet on CIFAR-10 pixel prediction task. This result implies a promising research direction for image generation and image enhancement with transformer. In fact, Image Transformer (Parmar et al. 2018) and image processing transformer (IPT) (Chen et al. 2020a) have demonstrated that transformer-based models can outperform conventional models in various tasks, such as image super-resolution and de-noising.

On top of the works introduced above, transformer architecture has already started to tackle other major computer vision tasks, mostly in conjunction with CNNs. For example, DETR (Carion et al. 2020) employs transformer encoder and decoder on top of CNN backbone for object detection, whereas Max-DeepLab (Wang et al. 2020a) performs panoptic segmentation by dual architecture of CNN and mask transformer, just to name a few. For further details on the application of transformer for general computer vision tasks, we refer the readers to two survey papers (Han et al. 2021; Khan et al. 2021).

Prospects
One of the biggest concerns raised about the transformer-based language models is its massively large scale of training a model which is inevitably accompanied by prohibitive financial costs to afford such training procedure that have skyrocketed. Sharir et al. (2020) has estimated that it would cost $80k to $1.6m to train a a 1.5 billion parameter model, and now we have models with over 100 billion parameters (Brown et al. 2020), further aggravating such concerns regarding costs. Moreover, such tendency has also proven true for vision and cross-modal tasks; as noted in Sect. 5, training a model with larger image data led to noticeable performance boosts, and as Table 1 shows, nearly all cross-modal models show in their ablation study that they have benefited immensely from training with larger models. This rapid inflation in training costs has raised concerns, as the affordability is limited to only a handful of large corporations.

While potential alternatives may require a community-wise discussion, one of the most unambiguous directions, as (Sharir et al. 2020) points out, would be to develop more efficient network architectures; in fact, research efforts focusing on lessening the computational burden of transformer-based models have subsequently appeared in each modality. For example, in language domain, DistillBERT (Sanh et al. 2020) reduces the number of parameters by 40% from BERT, while retaining nearly identical performance, with its knowledge distillation integrated to the loss function. TinyBERT (Jiao et al. 2020) similarly proposes to leverage knowledge distillation, and reports comparable performances with up to 7.5 times smaller model. In vision domain, data-efficient image transformer (DeiT) (Touvron et al. 2020), built upon ViT, has been introduced, with an introduction of distillation token that utilizes student-teacher strategy through attention, and demonstrates a performance comparable to CNNs, while trained on ImageNet only. Reformer (Kitaev et al. 2020) is also an attempt aimed at improving the efficiency of transformer, and approaches the goal with locality-sensitive hashing and reversible residual layers, reducing its complexity from exponential to logarithmic. Sparse transformers (Child et al. 2019) and switch transformers (Fedus et al. 2021) also strive to diminish computational complexity with sparse attention mechanism. These results in respective domain and general transformer architecture bring high anticipation towards subsequent light-weighted transformer-based models in cross-modal tasks.

As shown in Sect. 5, transformer architecture has been shown to be effective not only in language, but also in vision, even when no convolutional neural networks are involved. From such results, we speculate that a cross-modal model solely based on transformer architecture, in which both language and vision representations are acquired solely through transformer without convolution and recurrent neural networks, is imminent. Such possibility is important because the resulting architectural integrity is likely to lead to a development of hardware optimized for transformer architecture, as recent works claim transformers’ superiority over CNNs in terms of computational efficiency. For example, (Dosovitskiy et al. 2020) shows that ViT is up to 4 times more memory-efficient than ResNets. If such memory efficiency is coupled with distillation models, the cost-performance issues described above will be further alleviated to a substantial extent, and architectural transitions in much wider parts of deep learning are likely to follow. In fact, ViLT (Kim et al. 2021) and UniT (Hu and Singh 2021) have been proposed to demonstrate that convolution-free model based on transformer for both modalities can accomplish comparable or better performances, while being up to 60 times faster. VATT (Akbari et al. 2021) goes a step further to include raw video and audio on top of text, with convolution-free transformer model. It will also be of our interest to see how the pre-training objectives evolve with the emergence of transformer-exclusive models. We expect more cross-modal models based on transformer architecture for both modalities to follow in near future, and predict that they are likely to play an important role in the transition of deep learning as described above.

As shown in Sect. 5, iGPT (Chen et al. 2020b) demonstrated that transformer can be employed to generate images. Extending this accomplishment to cross-modal domain, we encounter the task of image synthesis from text description. While image generation from text has been a long-time aspiration in machine learning community, its success has been limited; for example, to specific domains, such as birds or flowers (Reed et al. 2016; Zhang et al. 2018b). However, Dall-e (Ramesh et al. 2021) used GPT-3-based language model to demonstrate image synthesis from text with a wide variety, ranging from realistic images to illustrations, encompassing geographic and temporal knowledge. Along with text tokens, they tokenized images using discrete latent codes learned via discrete VAE (Kingma and Welling 2014; Rezende et al. 2014). StyleCLIP (Patashnik et al. 2021) also demonstrates that image style can be manipulated with text input by leveraging contrastive language-image pre-training (Radford et al. 2021) on StyleGAN-based image generation pipeline (Karras et al. 2019, 2020). This line of work is particularly intriguing since most cross-modal works employing transformer architecture have tackled the tasks in which images or videos are the inputs along with text, and the text or a label is the output. In that sense, image synthesis from text is a task in its reversed direction, and along with the combination with existing generative models, it is expected to be a highly promising future research direction.

Conclusion
In this paper, we reviewed the recent trends of transformer-based models for cross-modal tasks with language and vision, with emphasis on pre-training scheme and network architecture. As in language domain, the performance for cross-modal models turns out to strongly depend on the aspects such as model size, dataset size, and the pre-training objectives, reinforcing the recent trend of bigger models and more data for better performance witnessed by the likes of GPT-3. We have also shown that, with other things being equal, pre-training with datasets annotated manually by humans is more efficient in terms of the necessary dataset size than pre-training with datasets whose annotations are obtained under weak supervision via crawling, although the discrepancy can be overcome with a sufficient amount of data. We also observed that pre-training directly with datasets used for target task can help achieve better performance on that particular target task.

We also introduced the works that applied transformer architecture for vision representation, and discussed potential prospects with regards to transformer-based cross-modal models, such as the potential of transformer-exclusive models, data-efficient models, and the models for generative tasks. As the topics discussed in this paper are still relatively in their early stages, we hope this paper serves as a useful reference that summarizes the first phase of the topic that will potentially grow increasingly important in deep learning field.