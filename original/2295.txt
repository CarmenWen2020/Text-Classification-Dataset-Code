We propose a novel adaptive approximation approach for test-time resourceconstrained prediction motivated by Mobile, IoT, health, security and other applications, where constraints in the form of computation, communication, latency
and feature acquisition costs arise. We learn an adaptive low-cost system by training a gating and prediction model that limits utilization of a high-cost model to
hard input instances and gates easy-to-handle input instances to a low-cost model.
Our method is based on adaptively approximating the high-cost model in regions
where low-cost models suffice for making highly accurate predictions. We pose an
empirical loss minimization problem with cost constraints to jointly train gating
and prediction models. On a number of benchmark datasets our method outperforms state-of-the-art achieving higher accuracy for the same cost.
1 Introduction
Resource costs arise during test-time prediction in a number of machine learning applications. Feature costs in Internet, Healthcare, and Surveillance applications arise due to to feature extraction
time [23], and feature/sensor acquisition [19]. In addition to feature acquisition costs, communication and latency costs pose a key challenge in the design of mobile computing, or the Internet-ofThings(IoT) applications, where a large number of sensors/camera/watches/phones (known as edge
devices) are connected to a cloud.
Adaptive System: Rather than having the edge devices constantly transmit measurements/images
to the cloud where a centralized model makes prediction, a more efficient approach is to allow
the edge devices make predictions locally [12], whenever possible, saving the high communication
cost and reducing latency. Due to the memory, computing and battery constraints, the prediction
models on the edge devices are limited to low complexity. Consequently, to maintain high-accuracy,
adaptive systems are desirable. Such systems identify easy-to-handle input instances where local
edge models suffice, thus limiting the utilization cloud services for only hard instances. We propose
to learn an adaptive system by training on fully annotated training data. Our objective is to maintain
high accuracy while meeting average resource constraints during prediction-time.
There have been a number of promising approaches that focus on methods for reducing costs while
improving overall accuracy [9, 24, 19, 20, 13, 15]. These methods are adaptive in that, at testtime, resources (features, computation etc) are allocated adaptively depending on the difficulty of
the input. Many of these methods train models in a top-down manner, namely, attempt to build out
the model by selectively adding the most cost-effective features to improve accuracy.
In contrast we propose a novel bottom-up approach. We train adaptive models on annotated training
data by selectively identifying parts of the input space for which high accuracy can be maintained at
a lower cost. The principle advantage of our method is twofold. First, our approach can be readily
applied to cases where it is desirable to reduce costs of an existing high-cost legacy system. Second,
training top-down models in case of feature costs leads to fundamental combinatorial issues in multi-
stage search over all feature subsets (see Sec. 2). In contrast, we bypass many of these issues by
posing a natural adaptive approximation objective to partition the input space into easy and hard
cases.
Figure 1: Upper: single stage schematic
of our approach. We learn low-cost gating
g and a LPC model to adaptively approximate a HPC model. Lower: Key insight
for adaptive approximation. x-axis represents feature space; y-axis represents conditional probability of correct prediction; LPC
can match HPC’s prediction in the input region corresponding to the right of the gating threshold but performs poorly otherwise.
Our goal is to learn a low-cost gating function that attempts to send examples on the
right to LPC and the left to HPC.
In particular, when no legacy system is available, our
method consists of first learning a high-accuracy model
that minimizes the empirical loss regardless of costs. The
resulting high prediction-cost model (HPC) can be readily trained using any of the existing methods. For example, this could be a large neural network in the cloud
that achieves the state-of-the-art accuracy. Next, we
jointly learn a low-cost gating function as well as a low
prediction-cost (LPC) model so as to adaptively approximate the high-accuracy model by identifying regions of
input space where a low-cost gating and LPC model are
adequate to achieve high-accuracy. In IoT applications,
such low-complexity models can be deployed on the edge
devices to perform gating and prediction. At test-time, for
each input instance, the gating function decides whether
or not the LPC model is adequate for accurate classification. Intuitively, “easy” examples can be correctly classified using only an LPC model while “hard” examples
require HPC model. By identifying which of the input
instances can be classified accurately with LPCs we bypass the utilization of HPC model, thus reducing average
prediction cost. The upper part of Figure 1 is a schematic
of our approach, where x is feature vector and y is the
predicted label; we aim to learn g and an LPC model to
adaptively approximate the HPC. The key observation as
depicted in the lower figure is that the probability of correct classification given x for a HPC model is in general
a highly complex function with higher values than that of
a LPC model. Yet there exists regions of the input space
where the LPC has competitive accuracy (as shown to the
right of the gating threshold). Sending examples in such
regions (according to the gating function) to the LPC results in no loss of prediction accuracy while reducing prediction costs.
The problem would be simpler if our task were to primarily partition the input space into regions where LPC
models would suffice. The difficulty is that we must also
learn a low-cost gating function capable of identifying input instances for which LPC suffices. Since both prediction and gating account for cost, we favor
design strategies that lead to shared features and decision architectures between the gating function
and the LPC model. We pose the problem as a discriminative empirical risk minimization problem
that jointly optimizes for gating and prediction models in terms of a joint margin-based objective
function. The resulting objective is separately convex in gating and prediction functions. We propose
an alternating minimization scheme that is guaranteed to converge since with appropriate choice of
loss-functions (for instance, logistic loss), each optimization step amounts to a probabilistic approximation/projection (I-projection/M-projection) onto a probability space. While our method can be
recursively applied in multiple stages to successively approximate the adaptive system obtained in
the previous stage, thereby refining accuracy-cost trade-off, we observe that on benchmark datasets
even a single stage of our method outperforms state-of-art in accuracy-cost performance.
2 Related Work
Learning decision rules to minimize error subject to a budget constraint during prediction-time is an
area of active interest[9, 17, 24, 19, 22, 20, 21, 13, 16]. Pre-trained Models: In one instantiation
2
of these methods it is assumed that there exists a collection of prediction models with amortized
costs [22, 19, 1] so that a natural ordering of prediction models can be imposed. In other instances,
the feature dimension is assumed to be sufficiently low so as to admit an exhaustive enumeration of
all the combinatorial possibilities [20, 21]. These methods then learn a policy to choose amongst
the ordered prediction models. In contrast we do not impose any of these restrictions. Top-Down
Methods: For high-dimensional spaces, many existing approaches focus on learning complex adaptive decision functions top-down [9, 24, 13, 21]. Conceptually, during training, top-down methods
acquire new features based on their utility value. This requires exploration of partitions of the input
space together with different combinatorial low-cost feature subsets that would result in higher accuracy. These methods are based on multi-stage exploration leading to combinatorially hard problems.
Different novel relaxations and greedy heuristics have been developed in this context. Bottom-up
Methods: Our work is somewhat related to [16], who propose to prune a fully trained random forests
(RF) to reduce costs. Nevertheless, in contrast to our adaptive system, their perspective is to compress the original model and utilize the pruned forest as a stand-alone model for test-time prediction.
Furthermore, their method is specifically tailored to random forests.
Another set of related work includes classifier cascade [5] and decision DAG [3], both of which aim
to re-weight/re-order a set of pre-trained base learners to reduce prediction budget. Our method,
on the other hand, only requires to pre-train a high-accuracy model and jointly learns the low-cost
models to approximate it; therefore ours can be viewed as complementary to the existing work.
The teacher-student framework [14] is also related to our bottom-up approach; a low-cost student
model learns to approximate the teacher model so as to meet test-time budget. However, the goal
there is to learn a better stand-alone student model. In contrast, we make use of both the lowcost (student) and high-accuracy (teacher) model during prediction via a gating function, which
learns the limitation of the low-cost (student) model and consult the high-accuracy (teacher) model if
necessary, thereby avoiding accuracy loss. Our composite system is also related to HME [10], which
learns the composite system based on max-likelihood estimation of models. A major difference
is that HME does not address budget constraints. A fundamental aspect of budget constraints is
the resulting asymmetry, whereby, we start with an HPC model and sequentially approximate with
LPCs. This asymmetry leads us to propose a bottom-up strategy where the high-accuracy predictor
can be separately estimated and is critical to posing a direct empirical loss minimization problem.
3 Problem Setup
We consider the standard learning scenario of resource constrained prediction with feature costs. A
training sample S = {(x
(i)
, y(i)
) : i = 1, . . . , N} is generated i.i.d. from an unknown distribution,
where x
(i) ∈ <K is the feature vector with an acquisition cost cα ≥ 0 assigned to each of the features
α = 1, . . . , K and y
(i)
is the label for the i
th example. In the case of multi-class classification y ∈
{1, . . . , M}, where M is the number of classes. Let us consider a single stage of our training method
in order to formalize our setup. The model, f0, is a high prediction-cost (HPC) model, which is either
a priori known, or which we train to high-accuracy regardless of cost considerations. We would like
to learn an alternative low prediction-cost (LPC) model f1. Given an example x, at test-time, we
have the option of selecting which model, f0 or f1, to utilize to make a prediction. The accuracy of
a prediction model fz is modeled by a loss function `(fz(x), y), z ∈ {0, 1}. We exclusively employ
the logistic loss function in binary classification: `(fz(x), y) = log(1 + exp(−yfz(x)), although
our framework allows other loss models. For a given x, we assume that once it pays the cost to
acquire a feature, its value can be efficiently cached; its subsequent use does not incur additional
cost. Thus, the cost of utilizing a particular prediction model, denoted by c(fz, x), is computed as
the sum of the acquisition cost of unique features required by fz.
Oracle Gating: Consider a general gating likelihood function q(z|x) with z ∈ {0, 1}, that outputs
the likelihood of sending the input x to a prediction model, fz. The overall empirical loss is:
ESn Eq(z|x)
[`(fz(x), y)] = ESn
[`(f0(x), y)] + ESn

q(1|x) (`(f1(x), y) − `(f0(x), y))
| {z }
Excess Loss
3  
The first term only depends on f0, and from our perspective a constant. Similar to average loss we
can write the average cost as (assuming gating cost is negligible for now):
ESn Eq(z|x)
[c(fz, x)] = ESn
[c(f0, x)] − ESn
[q(1|x) (c(f0, x) − c(f1, x))
| {z }
Cost Reduction
],
where the first term is again constant. We can characterize the optimal gating function (see [19])
that minimizes the overall average loss subject to average cost constraint:
Excess loss
z }| {
`(f1, x) − `(f0, x)
q(1|x)=0
><
q(1|x)=1
η
Cost reduction
z }| {
(c(f0, x) − c(f1, x))
for a suitable choice η ∈ R. This characterization encodes the important principle that if the marginal
cost reduction is smaller than the excess loss, we opt for the HPC model. Nevertheless, this characterization is generally infeasible. Note that the LHS depends on knowing how well HPC performs
on the input instance. Since this information is unavailable, this target can be unreachable with
low-cost gating.
Gating Approximation: Rather than directly enforcing a low-cost structure on q, we decouple
the constraint and introduce a parameterized family of gating functions g ∈ G that attempts to
mimic (or approximate) q. To ensure such approximation, we can minimize some distance measure D(q(·|x), g(x)). A natural choice for an approximation metric is the Kullback-Leibler (KL)
divergence although other choices are possible. The KL divergence between q and g is given by
DKL(q(·|x)kg(x)) = P
z
q(z|x) log(q(z|x)/σ(sgn(0.5 − z)g(x))), where σ(s) = 1/(1 + e
−s
) is
the sigmoid function. Besides KL divergence, we have also proposed another symmetrized metric
fitting g directly to the log odds ratio of q. See Suppl. Material for details.
Budget Constraint: With the gating function g, the cost of predicting x depends on whether the
example is sent to f0 or f1. Let c(f0, g, x) denote the feature cost of passing x to f0 through g.
As discussed, this is equal to the sum of the acquisition cost of unique features required by f0 and
g for x. Similarly c(f1, g, x) denotes the cost if x is sent to f1 through g. In many cases the cost
c(fz, g, x) is independent of the example x and depends primarily on the model being used. This
is true for linear models where each x must be processed through the same collection of features.
For these cases c(fz, g, x) , c(fz, g). The total budget simplifies to: ESn
[q(0|x)]c(f0, g) + (1 −
ESn
[q(0|x)])c(f1, g) = c(f1, g) + ESn
[q(0|x)](c(f0, g) − c(f1, g)). The budget thus depends on 3
quantities: ESn
[q(0|x)], c(f1, g) and c(f0, g). Often f0 is a high-cost model that requires most, if
not all, of features so c(f0, g) can be considered a large constant.
Thus, to meet the budget constraint, we would like to have (a) low-cost g and f1 (small c(f1, g));
and (b) small fraction of examples being sent to the high-accuracy model (small ESn
[q(0|x)]). We
can therefore split the budget constraint into two separate objectives: (a) ensure low-cost through
penalty Ω(f1, g) = γ
P
α
cαkVα + Wαk0, where γ is a tradeoff parameter and the indicator variables Vα, Wα ∈ {0, 1} denote whether or not the feature α is required by f1 and g, respectively.
Depending on the model parameterization, we can approximate Ω(f1, g) using a group-sparse norm
or in a stage-wise manner as we will see in Algorithms 1 and 2. (b) Ensure only Pfull fraction of
examples are sent to f0 via the constraint ESn
[q(0|x)] ≤ Pfull.
Putting Together: We are now ready to pose our general optimization problem:
min
f1∈F,g∈G,q
ESn
Losses
zX }| {
z
[q(z|x)`(fz(x), y)] +
Gating Approx
z }| {
D(q(·|x), g(x)) +
Costs
z }| {
Ω(f1, g) (OPT)
subject to: ESn
[q(0|x)] ≤ Pfull. (F raction to f0)
The objective function penalizes excess loss and ensures through the second term that this excess
loss can be enforced through admissible gating functions. The third term penalizes the feature cost
usage of f1 and g. The budget constraint limits the fraction of examples sent to the costly model f0.
Remark 1: Directly parameterizing q leads to non-convexity. Average loss is q-weighted sum
of losses from HPC and LPC; while the space of probability distributions is convex, a finitedimensional parameterization is generally non-convex (e.g. sigmoid). What we have done is to
keep q in non-parametric form to avoid non-convexity and only parameterize g, connecting both via
4
a KL term. Thus, (OPT) is now convex with respect to the f1 and g for a fixed q. It is again convex
in q for a fixed f1 and g. Otherwise it would introduce non-convexity as in prior work. For instance,
in [5] a non-convex problem is solved in each inner loop iteration (line 7 of their Algorithm 1).
Remark 2: We presented the case for a single stage approximation system. However, it is straightforward to recursively continue this process. We can then view the composite system f0 , (g, f1, f0)
as a black-box predictor and train a new pair of gating and prediction models to approximate the
composite system.
Remark 3: To limit the scope of our paper, we focus on reducing feature acquisition cost during
prediction as it is a more challenging (combinatorial) problem. However, other prediction-time costs
such as computation cost can be encoded in the choice of functional classes F and G in (OPT).
Surrogate Upper Bound of Composite System: We can get better insight for the first two terms
of the objective in (OPT) if we view z ∈ {0, 1} as a latent variable and consider the composite
system Pr(y|x) = P
z Pr(z|x; g) Pr(y|x, fz). A standard application of Jensen’s inequality reveals
that, − log(Pr(y|x)) ≤ Eq(z|x)`(fz(x), y) + DKL(q(z|x)kPr(z|x; g)). Therefore, the conditionalentropy of the composite system is bounded by the expected value of our loss function (we overload
notation and represent random-variables in lower-case format):
H(y | x) , E[− log(Pr(y|x))] ≤ Ex×y[Eq(z|x)`(fz(x), y) + DKL(q(z|x)kPr(z|x; g))].
This implies that the first two terms of our objective attempt to bound the loss of the composite
system; the third term in the objective together with the constraint serve to enforce budget limits on
the composite system.
Group Sparsity: Since the cost for feature re-use is zero we encourage feature re-use among gating and prediction models. So the fundamental question here is: How to choose a common, sparse
(low-cost) subset of features on which both g and f1 operate, such that g can effective gate examples between f1 and f0 for accurate prediction? This is a hard combinatorial problem. The main
contribution of our paper is to address it using the general optimization framework of (OPT).
4 Algorithms
To be concrete, we instantiate our general framework (OPT) into two algorithms via different parameterizations of g, f1: ADAPT-LIN for the linear class and ADAPT-GBRT for the non-parametric class.
Algorithm 1 ADAPT-LIN
Input: (x
(i)
, y(i)
), Pfull, γ
Train f0. Initialize g, f1.
repeat
Solve (OPT1) for q given g, f1.
Solve (OPT2) for g, f1 given q.
until convergence
Algorithm 2 ADAPT-GBRT
Input: (x
(i)
, y(i)
), Pfull, γ
Train f0. Initialize g, f1.
repeat
Solve (OPT1) for q given g, f1.
for t = 1 to T do
Find f
t
1 using CART to minimize (1).
f1 = f1 + f
t
1
.
For each feature α used, set uα = 0.
Find g
t using CART to minimize (2).
g = g + g
t
.
For each feature α used, set uα = 0.
end for
until convergence
Both of them use the KL-divergence as distance
measure. We also provide a third algorithm
ADAPT-LSTSQ that uses the symmetrized distance in the Suppl. Material. All of the algorithms perform alternating minimization of
(OPT) over q, g, f1. Note that convergence
of alternating minimization follows as in [8].
Common to all of our algorithms, we use two
parameters to control cost: Pfull and γ. In practice they are swept to generate various costaccuracy tradeoffs and we choose the best one
satisfying the budget B using validation data.
ADAPT-LIN: Let g(x) = g
T x and f1(x) =
f
T
1 x be linear classifiers. A feature is used if the
corresponding component is non-zero: Vα = 1
if f1,α 6= 0, and Wα = 1 if gα 6= 0. The minimization for q solves the following problem:
min
q
1
N
PN
i=1 [(1 − qi)Ai + qiBi − H(qi)]
s.t. 1
N
PN
i=1 qi ≤ Pfull,
(OPT1)
where we have used shorthand notations qi =
q(z = 0|x
(i)
), H(qi) = −qi
log(qi) − (1 −
qi) log(1 − qi), Ai = log(1 + e
−y
(i)
f
T
1 x
(i)
) +
5
log(1 + e
g
T x
(i)
) and Bi = − log p(y
(i)
|z
(i) = 0; f0) + log(1 + e
−g
T x
(i)
). This optimization has
a closed form solution: qi = 1/(1 + e
Bi−Ai+β
) for some non-negative constant β such that the
constraint is satisfied. This optimization is also known as I-Projection in information geometry
because of the entropy term [8]. Having optimized q, we hold it constant and minimize with respect
to g, f1 by solving the problem (OPT2), where we have relaxed the non-convex cost P
α
cαkVα +
Wαk0 into a L2,1 norm for group sparsity and a tradeoff parameter γ to make sure the feature budget
is satisfied. Once we solve for g, f1, we can hold them constant and minimize with respect to q again.
ADAPT-LIN is summarized in Algorithm 1.
min
g,f1
1
N
XN
i=1
h
(1 − qi)

log(1 + e
−y
(i)f
T
1 x
(i)
) + log(1 + e
g
T x
(i)
)

+ qi
log(1 + e
−g
T x
(i)
)
i
+ γ
X
α
q
g
2
α + f
2
1,α.
(OPT2)
ADAPT-GBRT: We can also consider the non-parametric family of classifiers such as gradient
boosted trees [7]: g(x) = PT
t=1 g
t
(x) and f1(x) = PT
t=1 f
t
1
(x), where g
t
and f
t
1
are limiteddepth regression trees. Since the trees are limited to low depth, we assume that the feature utility
of each tree is example-independent: Vα,t(x) u Vα,t, Wα,t(x) u Wα,t, ∀x. Vα,t = 1 if feature α appears in f
t
1
, otherwise Vα,t = 0, similarly for Wα,t. The optimization over q still solves
(OPT1). We modify Ai = log(1 + e
−y
(i)
f1(x
(i)
)
) + log(1 + e
g(x
(i)
)
) and Bi = − log p(y
(i)
|z
(i) =
0; f0) + log(1 + e
−g(x
(i)
)
). Next, to minimize over g, f1, denote loss:
`(f1, g) = 1
N
X
N
i=1 "
(1 − qi) ·

log(1 + e
−y
(i)
f1(x
(i)
)
) + log(1 + e
g(x
(i)
)
)

+ qi
log(1 + e
−g(x
(i)
)
)
#
,
which is essentially the same as the first part of the objective in (OPT2). Thus, we need to minimize
`(f1, g) + Ω(f1, g) with respect to f1 and g. Since both f1 and g are gradient boosted trees, we
naturally adopt a stage-wise approximation for the objective. In particular, we define an impurity
function which on the one hand approximates the negative gradient of `(f1, g) with the squared
loss, and on the other hand penalizes the initial acquisition of features by their cost cα. To capture
the initial acquisition penalty, we let uα ∈ {0, 1} indicates if feature α has already been used in
previous trees (uα = 0), or not (uα = 1). uα is updated after adding each tree. Thus we arrive at
the following impurity for f1 and g, respectively:
1
2
X
N
i=1
(−
∂`(f1, g)
∂f1(x
(i))
− f
t
1
(x
(i)
))2 + γ
X
α
uαcαVα,t, (1)
1
2
X
N
i=1
(−
∂`(f1, g)
∂g(x
(i))
− g
t
(x
(i)
))2 + γ
X
α
uαcαWα,t. (2)
Minimizing such impurity functions balances the need to minimize loss and re-using the already
acquired features. Classification and Regression Tree (CART) [2] can be used to construct decision trees with such an impurity function. ADAPT-GBRT is summarized in Algorithm 2. Note
that a similar impurity is used in GREEDYMISER [24]. Interestingly, if Pfull is set to 0, all the examples are forced to f1, then ADAPT-GBRT exactly recovers the GREEDYMISER. In this sense,
GREEDYMISER is a special case of our algorithm. As we will see in the next section, thanks to the
bottom-up approach, ADAPT-GBRT benefits from high-accuracy initialization and is able to perform
accuracy-cost tradeoff in accuracy levels beyond what is possible for GREEDYMISER.
5 Experiments
BASELINE ALGORITHMS: We consider the following simple L1 baseline approach for learning
f1 and g: first perform a L1-regularized logistic regression on all data to identify a relevant, sparse
subset of features; then learn f1 using training data restricted to the identified feature(s); finally,
learn g based on the correctness of f1 predictions as pseudo labels (i.e. assign pseudo label 1 to
example x if f1(x) agrees with the true label y and 0 otherwise). We also compare with two stateof-the-art feature-budgeted algorithms: GREEDYMISER[24] - a top-down method that builds out an
6
ensemble of gradient boosted trees with feature cost budget; and BUDGETPRUNE[16] - a bottom-up
method that prunes a random forest with feature cost budget. A number of other methods such as
ASTC [13] and CSTC [23] are omitted as they have been shown to under-perform GREEDYMISER
on the same set of datasets [15]. Detailed experiment setups can be found in the Suppl. Material.
We first visualize/verify the adaptive approximation ability of ADAPT-LIN and ADAPT-GBRT on the
Synthetic-1 dataset without feature costs. Next, we illustrate the key difference between ADAPT-LIN
and the L1 baseline approach on the Synthetic-2 as well as the Letters datasets. Finally, we compare
ADAPT-GBRT with state-of-the-art methods on several resource constraint benchmark datasets.
(a) Input Data (b) Lin Initialization (c) Lin after 10 iterations
(d) RBF Contour (e) Gbrt Initialization (f) Gbrt after 10 iterations
Figure 2: Synthetic-1 experiment without feature cost. (a): input data. (d): decision contour of
RBF-SVM as f0. (b) and (c): decision boundaries of linear g and f1 at initialization and after 10
iterations of ADAPT-LIN. (e) and (f): decision boundaries of boosted tree g and f1 at initialization
and after 10 iterations of ADAPT-GBRT. Examples in the beige areas are sent to f0 by the g.
POWER OF ADAPTATION: We construct a 2D binary classification dataset (Synthetic-1)
as shown in (a) of Figure 2. We learn an RBF-SVM as the high-accuracy classifier f0
as in (d). To better visualize the adaptive approximation process in 2D, we turn off the
feature costs (i.e. set Ω(f1, g) to 0 in (OPT)) and run ADAPT-LIN and ADAPT-GBRT.
Figure 3: A 2-D synthetic example for
adaptive feature acquisition. On the left:
data distributed in four clusters. The
two features correspond to x and y coordinates, respectively. On the right:
accuracy-cost tradeoff curves. Our algorithm can recover the optimal adaptive system whereas a L1-based approach cannot.
The initializations of g and f1 in (b) results in wrong predictions for many red points in the blue region. After 10
iterations of ADAPT-LIN, f1 adapts much better to the local region assigned by g while g sends about 60% (Pfull)
of examples to f0. Similarly, the initialization in (e) results in wrong predictions in the blue region. ADAPTGBRT is able to identify the ambiguous region in the center and send those examples to f0 via g. Both of our algorithms maintain the same level of prediction accuracy as
f0 yet are able to classify large fractions of examples via
much simpler models.
POWER OF JOINT OPTIMIZATION: We return to the
problem of prediction under feature budget constrains.
We illustrate why a simple L1 baseline approach for
learning f1 and g would not work using a 2D dataset
(Synthetic-2) as shown in Figure 3 (left). The data points
are distributed in four clusters, with black triangles and
red circles representing two class labels. Let both feature
1 and 2 carry unit acquisition cost. A complex classifier
f0 that acquires both features can achieve full accuracy
at the cost of 2. In our synthetic example, clusters 1 and 2 are given more data points so that the
L1-regularized logistic regression would produce the vertical red dashed line, separating cluster 1
from the others. So feature 1 is acquired for both g and f1. The best such an adaptive system can
7
do is to send cluster 1 to f1 and the other three clusters to the complex classifier f0, incurring an
average cost of 1.75, which is sub-optimal. ADAPT-LIN, on the other hand, optimizing between
q, g, f1 in an alternating manner, is able to recover the horizontal lines in Figure 3 (left) for g and
f1. g sends the first two clusters to the full classifier and the last two clusters to f1. f1 correctly
classifies clusters 3 and 4. So all of the examples are correctly classified by the adaptive system; yet
only feature 2 needs to be acquired for cluster 3 and 4 so the overall average feature cost is 1.5, as
shown by the solid curve in the accuracy-cost tradeoff plot on the right of Figure 3. This example
shows that the L1 baseline approach is sub-optimal as it doesnot optimize the selection of feature
subsets jointly for g and f1.
15 20 25 30 35 40 45 50
Average Feature Cost
0.920
0.925
0.930
0.935
Test Accuracy
Adapt_Gbrt
GreedyMiser(Xu et al. 2012)
BudgetPrune (Nan et al. 2016)
(a) MiniBooNE
10 15 20 25 30
Average Feature Cost
0.84
0.86
0.88
0.90
0.92
Test Accuracy
(b) Forest Covertype
40 60 80 100 120 140 160 180
Average Feature Cost
0.128
0.130
0.132
0.134
0.136
0.138
Average Precision@5
(c) Yahoo! Rank
0 50 100 150 200 250 300 350 400
Average Feature Cost
0.65
0.70
0.75
0.80
Test Accuracy
(d) CIFAR10
Figure 4: Comparison of ADAPT-GBRT against GREEDYMISER and BUDGETPRUNE on four
benchmark datasets. RF is used as f0 for ADAPT-GBRT in (a-c) while an RBF-SVM is used as
f0 in (d). ADAPT-GBRT achieves better accuracy-cost tradeoff than other methods. The gap is significant in (b) (c) and (d). Note the accuracy of GREEDYMISER in (b) never exceeds 0.86 and its
precision in (c) slowly rises to 0.138 at cost of 658. We limit the cost range for a clearer comparison.
Table 1: Dataset Statistics
Dataset #Train #Validation #Test #Features Feature Costs
Letters 12000 4000 4000 16 Uniform
MiniBooNE 45523 19510 65031 50 Uniform
Forest 36603 15688 58101 54 Uniform
CIFAR10 19761 8468 10000 400 Uniform
Yahoo! 141397 146769 184968 519 CPU units
REAL DATASETS: We test various aspects
of our algorithms and compare with stateof-the-art feature-budgeted algorithms on five
real world benchmark datasets: Letters, MiniBooNE Particle Identification, Forest Covertype datasets from the UCI repository [6],
CIFAR-10 [11] and Yahoo! Learning to
Rank[4]. Yahoo! is a ranking dataset where each example is associated with features of a querydocument pair together with the relevance rank of the document to the query. There are 519 such
features in total; each is associated with an acquisition cost in the set {1,5,20,50,100,150,200},
which represents the units of CPU time required to extract the feature and is provided by a Yahoo!
employee. The labels are binarized into relevant or not relevant. The task is to learn a model that
takes a new query and its associated documents and produce a relevance ranking so that the relevant
documents come on top, and to do this using as little feature cost as possible. The performance
metric is Average Precision @ 5 following [16]. The other datasets have unknown feature costs so
we assign costs to be 1 for all features; the aim is to show ADAPT-GBRT successfully selects sparse
subset of “usefull” features for f1 and g. We summarize the statistics of these datasets in Table 1.
Next, we highlight the key insights from the real dataset experiments.
Generality of Approximation: Our framework allows approximation of powerful classifiers such
as RBF-SVM and Random Forests as shown in Figure 5 as red and black curves, respectively.
In particular, ADAPT-GBRT can well maintain high accuracy while reducing cost. This is a key
advantage for our algorithms because we can choose to approximate the f0 that achieves the best
accuracy. ADAPT-LIN Vs L1: Figure 5 shows that ADAPT-LIN outperforms L1 baseline method
on real dataset as well. Again, this confirms the intuition we have in the Synthetic-2 example as
ADAPT-LIN is able to iteratively select the common subset of features jointly for g and f1. ADAPTGBRT Vs ADAPT-LIN: ADAPT-GBRT leads to significantly better performance than ADAPT-LIN in
approximating both RBF-SVM and RF as shown in Figure 5. This is expected as the non-parametric
non-linear classifiers are much more powerful than linear ones.
ADAPT-GBRT Vs BUDGETPRUNE: Both are bottom-up approaches that benefit from good initializations. In (a), (b) and (c) of Figure 4 we let f0 in ADAPT-GBRT be the same RF that BUDGETPRUNE starts with. ADAPT-GBRT is able to maintain high accuracy longer as the budget decreases.
8
Thus, ADAPT-GBRT improves state-of-the-art bottom-up method. Notice in (c) of Figure 4 around
the cost of 100, BUDGETPRUNE has a spike in precision. We believe this is because the initial
pruning improved the generalization performance of RF.
But in the cost region of 40-80, ADAPT-GBRT maintains much better accuracy than BUDGETPRUNE. Furthermore, ADAPT-GBRT has the freedom to approximate the best f0 given the problem.
So in (d) of Figure 4 we see that with f0 being RBF-SVM, ADAPT-GBRT can achieve much higher
accuracy than BUDGETPRUNE.
11 12 13 14 15 16
Average Feature Cost
0.88
0.90
0.92
0.94
0.96
0.98
Test Accuracy
Adapt_Gbrt RF
Adapt_Lin RF
L1 RF
Adapt_Gbrt RBF
Adapt_Lin RBF
L1 RBF
GreedyMiser(Xu et al 2012)
Figure 5: Compare the L1 baseline approach, ADAPT-LIN and ADAPT-GBRT
based on RBF-SVM and RF as f0’s on
the Letters dataset.
ADAPT-GBRT Vs GREEDYMISER: ADAPT-GBRT outperforms GREEDYMISER on all the datasets. The gaps in
Figure 5, (b) (c) and (d) of Figure 4 are especially significant.
Significant Cost Reduction: Without sacrificing top accuracies (within 1%), ADAPT-GBRT reduces average feature costs during test-time by around 63%, 32%, 58%,
12% and 31% on MiniBooNE, Forest, Yahoo, Cifar10
and Letters datasets, respectively.
6 Conclusions
We presented an adaptive approximation approach to account for prediction costs that arise in various applications. At test-time our method uses a gating function to
identify a prediction model among a collection of models
that is adapted to the input. The overall goal is to reduce
costs without sacrificing accuracy. We learn gating and
prediction models by means of a bottom-up strategy that trains low prediction-cost models to approximate high prediction-cost models in regions where low-cost models suffice. On a number of
benchmark datasets our method leads to an average of 40% cost reduction without sacrificing test
accuracy (within 1%). It outperforms state-of-the-art top-down and bottom-up budgeted learning
algorithms, with a significant margin in several cases.