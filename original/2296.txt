We present a new affine-invariant optimization algorithm called Online Lazy Newton.
The regret of Online Lazy Newton is independent of conditioning: the algorithm‚Äôs
performance depends on the best possible preconditioning of the problem in
retrospect and on its intrinsic dimensionality. As an application, we show how
Online Lazy Newton can be used to achieve an optimal regret of order ‚àö
rT for the
low-rank experts problem, improving by a ‚àö
r factor over the previously best known
bound and resolving an open problem posed by Hazan et al. [15].
1 Introduction
In the online convex optimization setting, a learner is faced with a stream of T convex functions
over a bounded convex domain X ‚äÜ Rd
. At each round t the learner gets to observe a single convex
function ft and has to choose a point xt ‚àà X. The aim of the learner is to minimize the cumulative
T-round regret, defined as
X
T
t=1
ft(xt) ‚àí min
x‚ààX
X
T
t=1
ft(x).
In this very general setting, Online Gradient Descent [25] achieves an optimal regret rate of Œò(GD‚àö
T),
where G is a bound on the Lipschitz constants of the ft and D is a bound on the diameter of the
domain, both with respect to the Euclidean norm. For simplicity, let us restrict the exposition to linear
losses ft(xt) = g
T
t
xt
, in which case G bounds the maximal Euclidean norm kgt k; it is well known that
the general convex case can be easily reduced to this case.
One often useful way to obtain faster convergence in optimization is to employ preconditioning,
namely to apply a linear transformation P to the gradients before using them to make update steps.
In an online optimization setting, if we have had access to the best preconditioner in hindsight we
could have achieved a regret rate of the form Œò(infP GPDP
‚àö
T), where DP is the diameter of the set
P ¬∑ Xand GP is a bound on the norm of the conditioned gradients kP
‚àí1gt k. We shall thus refer to the
quantity GPDP as the conditioning of the problem when a preconditioner P is used.
In many cases, however, it is more natural to directly assume a bound |g
T
t
xt
| ‚â§ C on the magnitude of
the losses rather than assuming the bounds D and G. In this case, the condition number need not
be bounded and typical guarantees of gradient-based methods such as online gradient descent do
not directly apply. In principle, it is possible to find a preconditioner P such that GPDP = O(C
‚àö
d),
and if one further assumes that the intrinsic dimensionality of the problem (i.e., the rank of the
loss vectors g1, . . ., gT ) is r  d, the conditioning of the optimization problem can be improved
to O(C
‚àö
r). However, this approach requires one to have access to the transformation P, which is
typically data-dependent and known only in retrospect.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
In this paper we address the following natural question: can one achieve a regret rate of O(C
‚àö
rT)
without the explicit prior knowledge of a good preconditioner P? We answer to this question in the
affirmative and present a new algorithm that achieves this rate, called Online Lazy Newton. Our
algorithm is a variant of the Online Newton Step algorithm due to Hazan et al. [14], that employs a lazy
projection step. While the Online Newton Step algorithm was designed to exploit curvature in the loss
functions (specifically, a property called exp-concavity), our adaptation is aimed at general‚Äîpossibly
even linear‚Äîonline convex optimization and exploits latent low-dimensional structure. It turns out
that this adaptation of the algorithm is able to achieve O(C
‚àö
rT) regret up to a small logarithmic factor,
without any prior knowledge of the optimal preconditioner. A crucial property of our algorithm is its
affine-invariance: Online Lazy Newton is invariant to any affine transformation of the gradients gt
, in
the sense that running the algorithm on gradients g
0
t = P
‚àí1gt and applying the inverse transformation
P on the produced decisions results with the same decisions that would have been obtained by applying
the algorithm directly to the original vectors gt
.
As our main application, we establish a new regret rate for the low rank experts problem, introduced
by Hazan et al. [15]. The low rank experts setting is a variant of the classical prediction with expert
advice problem, where one further assumes that the experts are linearly dependent and their losses
span a low dimensional space of rank r. The challenge in this setting is to achieve a regret rate which
is independent of number of experts, and only depends on their rank r. In this setting, Hazan et al.
[15] proved a lower bound of ‚Ñ¶(
‚àö
rT) on the regret, but fell short of providing a matching upper
bound and only gave an algorithm achieving a suboptimal O(r
‚àö
T) regret bound. Applying the Online
Lazy Newton algorithm to this problem, we are able to improve upon the latter bound and achieve
a O(
p
rT logT) regret bound, which is optimal up to a p
logT factor and improves upon the prior
bound unless T is exponential in the rank r.
1.1 Related work
Adaptive regularization is an important topic in online optimization that has received considerable
attention in recent years. The AdaGrad algorithm presented in [9] (as well as the closely related
algorithm was analyzed in [22]) dynamically adapts to the geometry of the data. In a sense, AdaGrad
learns the best preconditioner from a trace-bounded family of Mahalanobis norms. (See Section 2.2
for a detailed discussion and comparison of guarantees). The MegaGrad algorithm of van Erven and
Koolen [23] uses a similar dynamic regularization technique in order to obliviously adapt to possible
curvature in the loss functions. Lower bounds for preconditioning when the domain is unbounded
has been presented in [7]. These lower bounds are inapplicable, however, once losses are bounded
(as assumed in this paper). More generally, going beyond worst case analysis and exploiting latent
structure in the data is a very active line of research within online learning. Work in this direction
include adaptation to stochastic i.i.d data (e.g., [11, 12, 20, 8]), as well as the exploration of various
structural assumptions that can be leveraged for better guarantees [4, 12, 13, 5, 19].
Our Online Lazy Newton algorithm is a part of a wide family of algorithms named Follow the
Regularized Leader (FTRL). FTRL methods choose at each iteration the minimizer of past observed
losses with an additional regularization term [16, 12, 21]. Our algorithm is closely related to the
Follow The Approximate Leader (FTAL) algorithm presented in [14]. The FTAL algorithm is designed
to achieve logarithmic regret rate for exp-concave problems, exploiting the curvature information
of such functions. In contrast, our algorithm is aimed for optimizing general convex functions
with possibly no curvature; while FTAL performs FTL over the second-order approximation of the
functions, Online Lazy Newton instead utilizes a first-order approximation with an additional rank-one
quadratic regularizer. Another algorithm closely related to ours is the Second-order Perceptron
algorithm of Cesa-Bianchi et al. [3] (which in turn is closely related to the Vovk-Azoury-Warmuth
forecaster [24, 1]), which is a variant of the classical Perceptron algorithm adapted to the case where
the data is ‚Äúskewed‚Äù, or ill-conditioned in the sense used above. Similarly to our algorithm, the
Second-order Perceptron employs adaptive whitening of the data to address its skewness. Finally,
the SON algorithm, proposed in [18] is an enhanced version of Online Newton Step which utilizes
sketching to improve over previous second order online learning algorithms. Similar to our paper,
they propose a version that is completely invariant to linear transformations. Their regret bound (for
our setting) depends linearly on the dimension of the ambient space, and quadratic in the rank of the
loss matrix. In contrast, our regret bounds do not depend on the dimension of the ambient space and
are linear in the rank of the loss matrix ‚Äì two properties that are necessary in order to achieve optimal
regret bound for the low rank expert problem.
2
This work is highly inspired and motivated by the problem of low rank experts to which we give an
optimal algorithm. The problem was first introduced in [15] where the authors established a regret
rate of Oe(r
‚àö
T), where r is the rank of the experts‚Äô losses, which was the first regret bound in this
setting that did not depend on the total number of experts. The problem has been further studied and
investigated by Cohen and Mannor [6], Barman et al. [2] and Foster et al. [10]. Here we establish the
first tight upper bound (up to logarithmic factor) that is independent of the total number of experts N.
2 Setup and Main Results
We begin by recalling the standard framework of Online Convex Optimization. At each round
t = 1, . . ., T a learner chooses a decision xt from a bounded convex subset X ‚äÜ Rd
in d-dimensional
space. An adversary then chooses a convex cost function ft
, and the learner suffers a loss of ft(xt).
We measure the performance of the learner in terms of the regret, which is defined as the difference
between accumulated loss incurred by the learner and the loss of the best decision in hindsight.
Namely, the T-round regret of the learner is given by
RegretT =
X
T
t=1
ft(xt) ‚àí min
x‚ààX
X
T
t=1
ft(x).
One typically assumes that the diameter of the set X is bounded, and that the convex functions
f1, . . ., fT are all Lipschitz continuous, both with respect to certain norms on Rd
(typically, the norms
are taken as dual to each other). However, a main point of this paper is to refrain from making explicit
assumptions on the geometry of the optimization problem, and design algorithms that are, in a sense,
oblivious to it.
Notation. Given a positive definite matrix A  0 we will denote by k ¬∑ kA the norm induced by A,
namely, kxkA =
‚àö
x
T Ax. The dual norm to k ¬∑ kA is defined by kgk
‚àó
A
= supkxkA ‚â§1
|x
Tg| and can be
shown to be equal to kgk
‚àó
A
= kgkA‚àí1 . Finally, for a non‚Äìinvertible matrix A, we denote by A
‚Ä†
its
Moore‚ÄìPenrose psuedo inverse.
2.1 Main Results
Our main results are affine invariant regret bounds for the Online Lazy Newton algorithm, which we
present below in Section 3. We begin with a bound for linear losses that controls the regret in terms
of the intrinsic dimensionality of the problem and a bound on the losses.
Theorem 1. Consider the online convex optimization setting with linear losses ft(x) = g
T
t
x, and
assume that |g
T
t
x| ‚â§ C for all t and x ‚àà X. If Algorithm 1 is run with Œ∑ < 1/C, then for every H  0
the regret is bounded as
RegretT ‚â§
4r
Œ∑
log
1 +
(DH GHT)
2
r

+ 3Œ∑

1 +
X
T
t=1
|g
T
t x
?
|
2

, (1)
where r = rank(
PT
t=1
gtg
T
t
) ‚â§ d and
DH = max
x,y‚ààX
kx ‚àí ykH , GH = max
1‚â§t ‚â§T
kgt k
‚àó
H .
By a standard reduction, the analogue statement for convex losses holds, as long as we assume that
the dot-products between gradients and decision vectors are bounded.
Corollary 2. Let f1, . . ., fT be an arbitrary sequence of convex functions over X. Suppose Algorithm 1
is run with 1/Œ∑ > maxt maxx‚ààX |‚àáT
t
xt
|. Then, for every H  0 the regret is bounded as
RegretT ‚â§
4r
Œ∑
log
1 +
(DH GHT)
2
r

+ 3Œ∑

1 +
X
T
t=1
|‚àáT
t x
?
|
2

, (2)
where r = rank(
PT
t=1
‚àát‚àá
T
t
) ‚â§ d and
DH = max
x,y‚ààX
kx ‚àí ykH , GH = max
1‚â§t ‚â§T
k‚àát k
‚àó
H .
3
In particular, we can use the theorem to show that as long as we bound |‚àáf (xt)
Txt
| by a constant‚Äîa
significantly weaker requirement than assuming bounds on the diameter of Xand on the norms of the
gradients‚Äîone can find a norm k ¬∑ kH for which the quantities DH and GH are properly bounded.
We stress again that, importantly, Algorithm 1 need not know the matrix H in order to achieve the
corresponding bound.
Theorem 3. Assume that
max
1‚â§t ‚â§T
max
x‚ààX
|‚àáT
t x| ‚â§ C.
Let r = rank(
PT
t=1
‚àát‚àá
T
t
) ‚â§ d, and run Algorithm 1 with Œ∑ = Œò
p
r log(T)/T

. The regret of the
algorithm is then at most O

C
p
rT logT

.
2.2 Discussion
It is worth comparing our result to previously studied adaptive regularization algorithms techniques.
Perhaps the most popular gradient method that employs adaptive regularization is the AdaGrad
algorithm introduced in [9]. The AdaGrad algorithm enjoys the regret bound depicted in Eq. (3). It is
competitive with any fixed regularization matrix S  0 such that Tr(S) ‚â§ d:
RegretT
(AdaGrad) = O

‚àö
d inf
S0,
Tr(S)‚â§d
qXT
t=1
kx
?k
2
2
k‚àát k
2
S
‚àó
!
, (3)
RegretT
(OLN) = Oe

‚àö
r inf
S0
qXT
t=1
kx
?k
2
S
k‚àát k
2
S
‚àó

. (4)
On the other hand, for every matrix S  0 by the generalized Cauchy-Schwartz inequality we have
k‚àáT
t
x
?k ‚â§ k‚àát k
‚àó
S
kx
?kS. Plugging this into Eq. (2) and a proper tuning of Œ∑ gives a bound which is
competitive with any fixed regularization matrix S  0, depicted in Eq. (4).
Our bound improves on AdaGrad‚Äôs regret bound in two ways. First, the bound in Eq. (4) scales with
the intrinsic dimension of the problem: when the true underlying dimensionality of the problem is
smaller than the dimension of the ambient space, Online Lazy Newton enjoys a superior regret bound.
Furthermore, as demonstrated in [15], the dependence of AdaGrad‚Äôs regret on the ambient dimension
is not an artifact of the analysis, and there are cases where the actual regret grows polynomially with
d rather than with the true rank r  d.
The second case where the Online Lazy Newton bound can be superior over AdaGrad‚Äôs is when there
exists a conditioning matrix that improves not only the norm of the gradients with respect to the
Euclidean norm, but also that the norm of x
? is smaller with respect to the optimal norm induced by S.
More generally, whenever PT
t=1
(‚àáT
t
x
?)
2 <
PT
t=1
k‚àát k
2
S
kx
?k
2
2
, and in particular when kx
?kS < kx
?k2,
Eq. (4) will produce a tighter bound than the one in Eq. (3).
3 The Online Lazy Newton Algorithm
We next present the main focus of this paper: the affine-invariant algorithm Online Lazy Newton (OLN),
given in Algorithm 1. The algorithm maintains two vectors, xt and yt
. The vector yt
is updated at
each iteration using the gradient of ft at xt
, via yt = yt‚àí1 ‚àí ‚àát where ‚àát = ‚àáft(xt). The vector yt
is
not guaranteed to be at X, hence the actual prediction of OLN is determined via a projection onto the
set X, resulting with the vector xt+1 ‚àà X. However, similarly to ONS, the algorithm first transforms
yt via the (pseudo-)inverse of the matrix At given by the sum of the outer products Pt
s=1
‚àás‚àá
T
s
, and
projections are taken with respect to At
. In this context, we use the notation
Œ†
A
X(y) = arg min
x‚ààX
(x ‚àí y)
T A(x ‚àí y).
to denote the projection onto a set X with respect to the (semi-)norm k ¬∑ kA induced by a positive
semidefinite matrix A  0.  
Algorithm 1 OLN: Online Lazy Newton
Parameters: initial point x1 ‚àà X, step size Œ∑ > 0
Initialize y0 = 0 and A0 = 0
for t = 1, 2 . . . T do
Play xt
, incur cost ft(xt), observe gradient ‚àát = ‚àáft(xt)
Rank 1 update At = At‚àí1 + Œ∑‚àát‚àá
T
t
Online Newton step and projection:
yt = yt‚àí1 ‚àí ‚àát
xt+1 = Œ†
At
X
(A
‚Ä†
t
yt)
end for
return
The main motivation behind working with the At as preconditioners is that‚Äîas demonstrated in
our analysis‚Äîthe algorithm becomes invariant to linear transformations of the gradient vectors ‚àát
.
Indeed, if P is some linear transformation, one can observe that if we run the algorithm on P‚àát
instead
of ‚àát
, this will transform the solution at step t from xt
to P
‚àí1xt
. In turn, the cumulative regret is
invariant to such transformations. As seen in Theorem 1, this invariance indeed leads to an algorithm
with an improved regret bound when the input representation of the data is poorly conditioned.
While the algorithm is very similar to ONS, it is different in several important aspects. First, unlike
ONS, our lazy version maintains at each step a vector yt which is updated without any projections.
Projection is then applied only when we need to calculate xt+1. In that sense, it can be thought as a
gradient descent method with lazy projections (analogous to dual-averaging methods) while ONS is
similar to gradient descent methods with a greedy projection step (reminiscent of mirror-descent type
algorithms). The effect of this, is a decoupling between past and future conditioning and projections:
and if the transformation matrix At changes between rounds, the lazy approach allows us to condition
and project the problem at each iteration independently.
Second, ONS uses an initialization of A0 = Id (while OLN uses A0 = 0) and, as a result, it is not
invariant to affine transformations. While this difference might seem negligible as  is typically chosen
to be very small, recall that the matrices At are used as preconditioners and their small eigenvalues
can be very meaningful, especially when the problem at hand is poorly conditioned.
4 Application: Low Rank Experts
In this section we consider the Low-rank Experts problem and show how the Online Lazy Newton
algorithm can be used to obtain a nearly optimal regret in that setting. In the Low-rank Experts
problem, which is a variant of the classic problem of prediction with expert advice, a learner has to
choose at each round t = 1, . . ., T between following the advice of one of N experts. On round t,
the learner chooses a distribution over the experts in the form of a probability vector xt ‚àà ‚àÜn (here
‚àÜn denotes the n-dimensional simplex); thereafter, an adversary chooses a cost vector gt ‚àà [‚àí1, 1]
N
assigning losses to experts, and the player suffers a loss of x
T
t
gt ‚àà [‚àí1, 1]. In contrast with the standard
experts setting, here we assume that in hindsight the expert share a common low rank structure and
we have that rank(g1, . . ., gT ) ‚â§ r, for some r < N.
It is known that in the stochastic setting (i.e., gt are drawn i.i.d. from some fixed distribution) a
follow-the-leader algorithm will enjoy a regret bound of min{
‚àö
rT,
p
T log N}. In [15] the authors
asked whether one can achieve the same regret bound in the online setting. Here we answer this
question on the affirmative.
Theorem 4 (Low Rank Experts). Consider the low rank expert setting, where rank(g1, . . ., gT ) ‚â§ r.
Set Œ∑ =
p
r log(T)/T, and run Algorithm 1 with X = ‚àÜn and ft(x) = g
T
t
x. Then, the obtained regret
satisfies
RegretT = O(
p
rT logT).
This bound matches the ‚Ñ¶(
‚àö
rT) lower bound of [15] up to a logT factor, and improves upon their
O(r
‚àö
T) upper bound, so long as T is not exponential in r. Put differently, if one aims at ensuring an
5
average regret of at most , the OLN algorithm would need O((r/
2
) log(1/)) iterations as opposed
to the O(r
2
/
2
) iterations required by the algorithm of [15]. We also remark that, since the Hedge
algorithm can be used to obtain regret rate of O(
p
T log N), we can obtain an algorithm with regret
bound of the form O

minp
rT logT,
p
T log N
	
by treating Hedge and OLN as meta-experts and
applying Hedge over them.
5 Analysis
For the proofs of our main theorems we will rely on the following two technical lemmas.
Lemma 5 ([17], Lemma 5). Let Œ¶1, Œ¶2 : X 7‚Üí R be two convex functions defined over a closed and
convex domain X ‚äÜ Rd
, and let x1 ‚àà arg minx‚ààX Œ¶1(x) and x2 ‚àà arg minx‚ààX Œ¶2(x). Assume that Œ¶2
is œÉ-strongly-convex with respect to a norm k ¬∑ k. Then, for œÜ = Œ¶2 ‚àí Œ¶1 we have
kx2 ‚àí x1 k ‚â§ 1
œÉ
k‚àáœÜ(x1)k‚àó
.
Furthermore, if œÜ is convex then
0 ‚â§ œÜ(x1) ‚àí œÜ(x2) ‚â§ 1
œÉ

k‚àáœÜ(x1)k‚àó
2
.
The following lemma is a slight strengthening of a result given in [14].
Lemma 6. Let g1, . . ., gT ‚àà Rd be a sequence of vectors, and define Gt = H +
Pt
s=1
gsg
T
s
for all t,
where H is a positive definite matrix such that kgt k
‚àó
H
‚â§ Œ≥ for all t. Then
X
T
t=1
g
T
t G
‚àí1
t gt ‚â§ r log
1 +
Œ≥
2T
r

,
where r is the rank of the matrix Pt
s=1
gsg
T
s
.
Proof. Following [14], we first prove that
X
T
t=1
g
T
t G
‚àí1
t gt ‚â§ log det GT
det H
= log det
H
‚àí1/2GT H
‚àí1/2

. (5)
To this end, let G0 = H, so that we have Gt = Gt‚àí1 + gtg
T
t
for all t ‚â• 1. The well-known matrix
determinant lemma, which states that det(A ‚àí uuT
) = (1 ‚àí u
T A
‚àí1u) det(A), gives
g
T
t G
‚àí1
t gt = 1 ‚àí
det(Gt ‚àí gtg
T
t
)
det Gt
= 1 ‚àí
det(Gt‚àí1)
det Gt
.
Using the inequality 1 ‚àí x ‚â§ log(1/x) and summing over t = 1, . . ., T, we obtain
X
T
t=1
g
T
t G
‚àí1
t gt ‚â§
X
T
t=1
log det Gt
det Gt‚àí1
= log det GT
det H
,
which yields Eq. (5).
Next, observe that H
‚àí1/2GT H
‚àí1/2 = I +
PT
s=1 H
‚àí1/2gsg
T
s H
‚àí1/2
and
Tr X
T
s=1
H
‚àí1/2
gsg
T
s H
‚àí1/2
!
=
X
T
s=1
Tr
g
T
s H
‚àí1
gs

=
X
T
s=1
(kgs k
‚àó
H )
2 ‚â§ Œ≥
2T.
Also, the rank of the matrix PT
s=1 H
‚àí1/2gsg
T
s H
‚àí1/2 = H
‚àí1/2
(
PT
s=1
gsg
T
s
)H
‚àí1/2
is at most r. Hence,
all the eigenvalues of the matrix H
‚àí1/2GT H
‚àí1/2
are equal to 1, except for r of them whose sum is at
most r + Œ≥
2T. Denote the latter by Œª1, . . ., Œªr ; using the concavity of log(¬∑) and Jensen‚Äôs inequality,
we conclude that
log det
H
‚àí1/2GT H
‚àí1/2

=
Xr
i=1
log Œªi ‚â§ r log
1
r
Xr
i=1
Œªi

‚â§ r log
1 +
Œ≥
2T
r

,
which together with Eq. (5) gives the lemma      
We can now prove our main results. We begin by proving Theorem 1.
Proof of Theorem 1. For all t, let
Àúft(x) = g
T
t x +
Œ∑
2
(g
T
t x)
2
and set
Fet(x) =
Xt
s=1
Àúfs(x) = ‚àíy
T
t x +
1
2
x
T Atx.
Observe that xt+1, which is the choice of Algorithm 1 at iteration t + 1, is the minimizer of Fet
; indeed,
since yt
is in the column span of At
, we can write up to a constant:
Fet(x) =
1
2

x ‚àí A
‚Ä†
t
yt
T
At

x ‚àí A
‚Ä†
t
yt

+ const.
In other words, Algorithm 1 is equivalent to a follow-the-leader algorithm on the functions Àúft
.
Next, fix some positive definite matrix H  0 and let DH = maxx,y‚ààX kx ‚àí ykH and GH =
max1‚â§t ‚â§T kgt k
‚àó
H
. Next we have
Fet(x) +
Œ∑
2
kx ‚àí xt+1 k
2
H =
=
1
2
x
T Atx ‚àí y
T
t x +
Œ∑
2
kx ‚àí xt+1 k
2
H
=
1
2
kxk
2
At
+
Œ∑
2
kxk
2
H ‚àí y
T
t x ‚àí 2x
T
t+1
x + kxt+1 k
2
H
=
Œ∑
2
kxk
2
Gt
‚àí y
T
t x ‚àí 2x
T
t+1
x + kxt+1 k
2
H,
where Gt =
Pt
s=1
gtg
T
t + H.
In turn, we have that the function is Œ∑-strongly convex with respect to the norm k ¬∑ kGt
, where
Gt = H +
P
gtg
T
t
, and is minimized at x = xt+1. Then by Lemma 5 with Œ¶1(x) = Fet‚àí1(x) and
Œ¶2(x) = Fet(x) +
Œ∑
2
kx ‚àí xt+1 k
2
H
, thus œÜ(x) = Àúft(x) +
Œ∑
2
kx ‚àí xt+1 k
2
H
, we have
Àúft(xt) ‚àí Àúft(xt+1) +
Œ∑
2
kxt ‚àí xt+1 k
2
H
‚â§
1
Œ∑
(kgt + Œ∑gtg
T
t xt + Œ∑H(xt ‚àí xt+1)k‚àó
Gt
)
2
‚â§
2
Œ∑
(1 + Œ∑g
T
t xt)
2
(kgt k
‚àó
Gt
)
2 + 2Œ∑(kH(xt ‚àí xt+1)k‚àó
Gt
)
2 ‚àµ kv + uk
2 ‚â§ 2kvk
2 + 2kuk
2
‚â§
8
Œ∑
(kgt k
‚àó
Gt
)
2 + 2Œ∑(kH(xt ‚àí xt+1)k‚àó
Gt
)
2 ‚àµ
1
Œ∑
‚â• max
x‚ààX
|g
T
t x|
‚â§
8
Œ∑
(kgt k
‚àó
Gt
)
2 + 2Œ∑(kH(xt ‚àí xt+1)k‚àó
H )
2 ‚àµ H ‚â∫ Gt ‚áí H
‚àí1  G
‚àí1
t
=
8
Œ∑
(kgt k
‚àó
Gt
)
2 + 2Œ∑kxt ‚àí xt+1 k
2
H .
Overall, we obtain
X
T
t=1
Àúft(xt) ‚àí Àúft(xt+1) ‚â§ 8
Œ∑
X
T
t=1
g
T
t G
‚àí1
t gt +
3Œ∑
2
X
T
t=1
kxt ‚àí xt+1 k
2
H .
By the FTL-BTL Lemma (e.g., [16]), we have that PT
t=1
Àúft(xt) ‚àí Àúft(x
?) ‚â§ PT
t=1
Àúft(xt) ‚àí Àúft(xt+1).
Hence, we obtain that:
X
T
t=1
Àúft(xt) ‚àí Àúft(x
?
) ‚â§ 8
Œ∑
X
T
t=1
g
T
t G
‚àí1
t gt +
3Œ∑
2
X
T
t=1
kxt ‚àí xt+1 k
2
H .  
Plugging in ft(x) = g
T
t
x +
Œ∑
2
(g
T
t
x)
2
and rearranging, we obtain
X
T
t=1
g
T
t

xt ‚àí x
?

‚â§
8
Œ∑
X
T
t=1
g
T
t G
‚àí1
t gt +
3Œ∑
2
X
T
t=1
kxt ‚àí xt+1 k
2
H +
Œ∑
2
X
T
t=1
(g
T
t x
?
)
2
‚â§
8
Œ∑
X
T
t=1
g
T
t G
‚àí1
t gt +
Œ∑
2
X
T
t=1
(g
T
t x
?
)
2 +
3Œ∑
2
T D2
H
‚â§
8r
Œ∑
log
1 +
G
2
H
T
r

+
3Œ∑
2
T D2
H +
Œ∑
2
X
T
t=1
(g
T
t x
?
)
2
,
Finally, note that we have obtained the last inequality for every matrix H  0. By rescaling a matrix H
and re-parametrizing H ‚Üí H/(‚àö
T DH ) we obtain a matrix whose diameter is DH ‚Üí 1/
‚àö
T and
GH ‚Üí
‚àö
T DH GH . Plugging these into the last inequality yield the result.
Proof of Theorem 3. To simplify notations, let us assume that |‚àáT
t
x
‚àó
| ‚â§ 1. We get from Corollary 2
that for every Œ∑:
RegretT ‚â§
2r
Œ∑
log
1 +
D
2G
2T
2
r

+ 3Œ∑(1 + T).
For each GH and DH we can set Œ∑ =
q
(2r/T) log(1 + G2
H D2
H
/r) and obtain the regret bound
RegretT ‚â§
s
rT log
1 +
D2
H G2
H
T
r

.
Hence, we only need to show that there exists a matrix H  0 such that D
2
H G
2
H
= O(r). Indeed, set
S = span(‚àá1, . . ., ‚àáT ), and denote XS to be the projection of Xonto S (i.e., XS = PXwhere P is the
projection over S). Define
B = {‚àá ‚àà S : |‚àáT
x| ‚â§ 1, ‚àÄ x ‚àà XS }.
Note that by definition we have that {‚àát }
T
t=1
‚äÜ B. Further, B is a symmetric convex set, hence by an
ellipsoid approximation we obtain a positive semidefinite matrix B  0, with positive eigenvalues
restricted to S, such that
B ‚äÜ {‚àá ‚àà S : ‚àá
TB‚àá ‚â§ 1} ‚äÜ rB.
By duality we have that
1
r XS ‚äÜ
1
r B‚àó ‚äÜ {v ‚àà S : v
TB
‚Ä†
v ‚â§ 1}.
Thus if PS is the projection over S we have for every x ‚àà Xthat x
TPSB
‚Ä†PSx ‚â§ r. On the other hand
for every ‚àát we have ‚àá
T
t B‚àát ‚â§ 1. We can now choose H = B
‚Ä† + Id where  is arbitrary small and
have
‚àá
T
t H
‚àí1‚àát = ‚àá
T
t
(B
‚Ä† + Id)
‚àí1‚àát ‚â§ 2
and
x
THx = x
TP
T
SB
‚Ä†PSx +  kxk
2 ‚â§ 2r  