We provide new results for noise-tolerant and sample-efficient learning algorithms
under s-concave distributions. The new class of s-concave distributions is a broad
and natural generalization of log-concavity, and includes many important additional
distributions, e.g., the Pareto distribution and t-distribution. This class has been
studied in the context of efficient sampling, integration, and optimization, but
much remains unknown about the geometry of this class of distributions and their
applications in the context of learning. The challenge is that unlike the commonly
used distributions in learning (uniform or more generally log-concave distributions),
this broader class is not closed under the marginalization operator and many such
distributions are fat-tailed. In this work, we introduce new convex geometry
tools to study the properties of s-concave distributions and use these properties
to provide bounds on quantities of interest to learning including the probability
of disagreement between two halfspaces, disagreement outside a band, and the
disagreement coefficient. We use these results to significantly generalize prior
results for margin-based active learning, disagreement-based active learning, and
passive learning of intersections of halfspaces. Our analysis of geometric properties
of s-concave distributions might be of independent interest to optimization more
broadly.
1 Introduction
Developing provable learning algorithms is one of the central challenges in learning theory. The study
of such algorithms has led to significant advances in both the theory and practice of passive and active
learning. In the passive learning model, the learning algorithm has access to a set of labeled examples
sampled i.i.d. from some unknown distribution over the instance space and labeled according to
some underlying target function. In the active learning model, however, the algorithm can access
unlabeled examples and request labels of its own choice, and the goal is to learn the target function
with significantly fewer labels. In this work, we study both learning models in the case where the
underlying distribution belongs to the class of s-concave distributions.
Prior work on noise-tolerant and sample-efficient algorithms mostly relies on the assumption that
the distribution over the instance space is log-concave [1, 12, 7, 30]. A distribution is log-concave
if the logarithm of its density is a concave function. The assumption of log-concavity has been
made for a few purposes: for computational efficiency reasons and for sample efficiency reasons.
For computational efficiency reasons, it was made to obtain a noise-tolerant algorithm even for
seemingly simple decision surfaces like linear separators. These simple algorithms exist for noiseless
scenarios, e.g., via linear programming [28], but they are notoriously hard once we have noise [15,
25, 19]; This is why progress on noise-tolerant algorithms has focused on uniform [22, 26] and
âˆ—Corresponding author.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
log-concave distributions [4]. Other concept spaces, like intersections of halfspaces, even have no
computationally efficient algorithm in the noise-free settings that works under general distributions,
but there has been nice progress under uniform and log-concave distributions [27]. For sample
efficiency reasons, in the context of active learning, we need distributional assumptions in order to
obtain label complexity improvements [16]. The most concrete and general class for which prior
work obtains such improvements is when the marginal distribution over instance space satisfies
log-concavity [32, 7]. In this work, we provide a broad generalization of all above results, showing
how they extend to s-concave distributions (s < 0). A distribution with density f(x) is s-concave
if f(x)
s
is a concave function. We identify key properties of these distributions that allow us to
simultaneously extend all above results.
How general and important is the class of s-concave distributions? The class of s-concave
distributions is very broad and contains many well-known (classes of) distributions as special cases.
For example, when s â†’ 0, s-concave distributions reduce to log-concave distributions. Furthermore,
the s-concave class contains infinitely many fat-tailed distributions that do not belong to the class of
log-concave distributions, e.g., Cauchy, Pareto, and t distributions, which have been widely applied in
the context of theoretical physics and economics, but much remains unknown about how the provable
learning algorithms, such as active learning of halfspaces, perform under these realistic distributions.
We also compare s-concave distributions with nearly-log-concave distributions, a slightly broader
class of distributions than log-concavity. A distribution with density f(x) is nearly-log-concave if
for any Î» âˆˆ [0, 1], x1, x2 âˆˆ R
n, we have f(Î»x1 + (1 âˆ’ Î»)x2) â‰¥ e
âˆ’0.0154f(x1)
Î»f(x2)
1âˆ’Î»
[7]. The
class of s-concave distributions includes many important extra distributions which do not belong to
the nearly-log-concave distributions: a nearly-log-concave distribution must have sub-exponential
tails (see Theorem 11, [7]), while the tail probability of an s-concave distribution might decay much
slower (see Theorem 1 (6)). We also note that efficient sampling, integration and optimization
algorithms for s-concave distributions have been well understood [13, 23]. Our analysis of s-concave
distributions bridges these algorithms to the strong guarantees of noise-tolerant and sample-efficient
learning algorithms.
1.1 Our Contributions
Structural Results. We study various geometric properties of s-concave distributions. These properties serve as the structural results for many provable learning algorithms, e.g., margin-based active
learning [7], disagreement-based active learning [29, 21], learning intersections of halfspaces [27],
etc. When s â†’ 0, our results exactly reduce to those for log-concave distributions [7, 2, 4]. Below,
we state our structural results informally:
Theorem 1 (Informal). Let D be an isotropic s-concave distribution in R
n. Then there exist closedform functions Î³(s, m), f1(s, n), f2(s, n), f3(s, n), f4(s, n), and f5(s, n) such that
1. (Weakly Closed under Marginal) The marginal of D over m arguments (or cumulative distribution
function, CDF) is isotropic Î³(s, m)-concave. (Theorems 3, 4)
2. (Lower Bound on Hyperplane Disagreement) For any two unit vectors u and v in R
n,
f1(s, n)Î¸(u, v) â‰¤ Prxâˆ¼D[sign(u Â· x) 6= sign(v Â· x)], where Î¸(u, v) is the angle between u
and v. (Theorem 12)
3. (Probability of Band) There is a function d(s, n) such that for any unit vector w âˆˆ R
n and any
0 < t â‰¤ d(s, n), we have f2(s, n)t < Prxâˆ¼D[|w Â· x| â‰¤ t] â‰¤ f3(s, n)t. (Theorem 11)
4. (Disagreement outside Margin) For any absolute constant c1 > 0 and any function f(s, n),
there exists a function f4(s, n) > 0 such that Prxâˆ¼D[sign(u Â· x) 6= sign(v Â· x) and |v Â· x| â‰¥
f4(s, n)Î¸(u, v)] â‰¤ c1f(s, n)Î¸(u, v). (Theorem 13)
5. (Variance in 1-D Direction) There is a function d(s, n) such that for any unit vectors u and a in R
n
such that kuâˆ’ak â‰¤ r and for any 0 < t â‰¤ d(s, n), we have Exâˆ¼Du,t [(aÂ·x)
2
] â‰¤ f5(s, n)(r
2+t
2
),
where Du,t is the conditional distribution of D over the set {x : |u Â· x| â‰¤ t}. (Theorem 14)
6. (Tail Probability) We have Pr[kxk >
âˆš
nt] â‰¤
h
1 âˆ’
cst
1+ns i(1+ns)/s
. (Theorem 5)
If s â†’ 0 (i.e., the distribution is log-concave), then Î³(s, m) â†’ 0 and the functions f(s, n), f1(s, n),
f2(s, n), f3(s, n), f4(s, n), f5(s, n), and d(s, n) are all absolute constants.
2
Table 1: Comparisons with prior distributions for margin-based active learning, disagreement-based
active learning, and Baumâ€™s algorithm.
Prior Work Ours
Margin (Efficient, Noise) uniform [3] log-concave [4] s-concave
Disagreement uniform [20] nearly-log-concave [7] s-concave
Baumâ€™s symmetric [9] log-concave [27] s-concave
To prove Theorem 1, we introduce multiple new techniques, e.g., extension of Prekopa-Leindler
theorem and reduction to baseline function (see the supplementary material for our techniques),
which might be of independent interest to optimization more broadly.
Margin Based Active Learning: We apply our structural results to margin-based active learning of
a halfspace w
âˆ— under any isotropic s-concave distribution for both realizable and adversarial noise
models. In the realizable case, the instance X is drawn from an isotropic s-concave distribution and
the label Y = sign(w
âˆ—
Â· X). In the adversarial noise model, an adversary can corrupt any Î· (â‰¤ O())
fraction of labels. For both cases, we show that there exists a computationally efficient algorithm that
outputs a linear separator wT such that Prxâˆ¼D[sign(wT Â· x) 6= sign(w
âˆ—
Â· x)] â‰¤  (see Theorems 15
and 16). The label complexity w.r.t. 1/ improves exponentially over the passive learning scenario
under s-concave distributions, though the underlying distribution might be fat-tailed. To the best
of our knowledge, this is the first result concerning the computationally-efficient, noise-tolerant
margin-based active learning under the broader class of s-concave distributions. Our work solves
an open problem proposed by Awasthi et al. [4] about exploring wider classes of distributions for
provable active learning algorithms.
Disagreement Based Active Learning: We apply our results to agnostic disagreement-based active
learning under s-concave distributions. The key to the analysis is estimating the disagreement
coefficient, a distribution-dependent measure of complexity that is used to analyze certain types of
active learning algorithms, e.g., the CAL [14] and A2
algorithm [5]. We work out the disagreement
coefficient under isotropic s-concave distribution (see Theorem 17). By composing it with the
existing work on active learning [17], we obtain a bound on label complexity under the class of
s-concave distributions. As far as we are aware, this is the first result concerning disagreementbased active learning that goes beyond log-concave distributions. Our bounds on the disagreement
coefficient match the best known results for the much less general case of log-concave distributions [7];
Furthermore, they apply to the s-concave case where we allow an arbitrary number of discontinuities,
a case not captured by [18].
Learning Intersections of Halfspaces: Baumâ€™s algorithm is one of the most famous algorithms
for learning the intersections of halfspaces. The algorithm was first proposed by Baum [9] under
symmetric distribution, and later extended to log-concave distribution by Klivans et al. [27] as these
distributions are almost symmetric. In this paper, we show that approximate symmetry also holds for
the case of s-concave distributions. With this, we work out the label complexity of Baumâ€™s algorithm
under the broader class of s-concave distributions (see Theorem 18), and advance the state-of-the-art
results (see Table 1).
We provide lower bounds to partially show the tightness of our analysis. Our results can be potentially
applied to other provable learning algorithms as well [24, 31, 10, 30, 8], which might be of independent interest. We discuss our techniques and other related papers in the supplementary material.
2 Preliminary
Before proceeding, we define some notations and clarify our problem setup in this section.
Notations: We will use capital or lower-case letters to represent random variables, D to represent
an s-concave distribution, and Du,t to represent the conditional distribution of D over the set
{x : |u Â· x| â‰¤ t}. We define the sign function as sign(x) = +1 if x â‰¥ 0 and âˆ’1 otherwise. We
denote by B(Î±, Î²) = R 1
0
t
Î±âˆ’1
(1âˆ’t)
Î²âˆ’1dt the beta function, and Î“(Î±) = R âˆž
0
t
Î±âˆ’1
e
âˆ’tdt the gamma
function. We will consider a single norm for the vectors in R
n, namely, the 2-norm denoted by
kxk. We will frequently use Âµ (or Âµf , ÂµD) to represent the measure of the probability distribution
D with density function f. The notation ball(w
âˆ—
, t) represents the set {w âˆˆ R
n : kw âˆ’ w
âˆ—k â‰¤ t}.
For convenience, the symbol âŠ• slightly differs from the ordinary addition +: For f = 0 or g = 0,
{f
s âŠ• g
s}
1/s = 0; Otherwise, âŠ• and + are the same. For u, v âˆˆ R
n, we define the angle between
them as Î¸(u, v).
3
2.1 From Log-Concavity to S-Concavity
We begin with the definition of s-concavity. There are slight differences among the definitions of
s-concave density, s-concave distribution, and s-concave measure.
Definition 1 (S-Concave (Density) Function, Distribution, Measure). A function f: R
n â†’ R+ is
s-concave, for âˆ’âˆž â‰¤ s â‰¤ 1, if f(Î»x + (1 âˆ’ Î»)y) â‰¥ (Î»f(x)
s + (1 âˆ’ Î»)f(y)
s
)
1/s for all Î» âˆˆ [0, 1],
âˆ€x, y âˆˆ R
n.
2 A probability distribution D is s-concave, if its density function is s-concave. A
probability measure Âµ is s-concave if Âµ(Î»A + (1 âˆ’ Î»)B) â‰¥ [Î»Âµ(A)
s + (1 âˆ’ Î»)Âµ(B)
s
]
1/s for any
sets A, B âŠ† R
n and Î» âˆˆ [0, 1].
Special classes of s-concave functions include concavity (s = 1), harmonic-concavity (s = âˆ’1),
quasi-concavity (s = âˆ’âˆž), etc. The conditions in Definition 1 are progressively weaker as s becomes
smaller: s1-concave densities (distributions, measures) are s2-concave if s1 â‰¥ s2. Thus one can
verify [13]: concave (s = 1) ( log-concave (s = 0) ( s-concave (s < 0) ( quasi-concave (s =
âˆ’âˆž).
3 Structural Results of S-Concave Distributions: A Toolkit
In this section, we develop geometric properties of s-concave distribution. The challenge is that unlike
the commonly used distributions in learning (uniform or more generally log-concave distributions),
this broader class is not closed under the marginalization operator and many such distributions are fattailed. To address this issue, we introduce several new techniques. We first introduce the extension of
the Prekopa-Leindler inequality so as to reduce the high-dimensional problem to the one-dimensional
case. We then reduce the resulting one-dimensional s-concave function to a well-defined baseline
function, and explore the geometric properties of that baseline function. We summarize our high-level
proof ideas briefly by the following figure.
n-D	s-concave																			 					1-D	!-concave																									1-D â„Ž # = %(1 + )#)+/-
Extension	of
Prekopa-Leindler
Baseline	Function
3.1 Marginal Distribution and Cumulative Distribution Function
We begin with the analysis of the marginal distribution, which forms the basis of other geometric
properties of s-concave distributions (s â‰¤ 0). Unlike the (nearly) log-concave distribution where the
marginal remains (nearly) log-concave, the class of s-concave distributions is not closed under the
marginalization operator. To study the marginal, our primary tool is the theory of convex geometry.
Specifically, we will use an extension of the PrÃ©kopa-Leindler inequality developed by Brascamp and
Lieb [11], which allows for a characterization of the integral of s-concave functions.
Theorem 2 ([11], Thm 3.3). Let 0 < Î» < 1, and Hs, G1, and G2 be non-negative integrable
functions on R
m such that Hs(Î»x+ (1âˆ’Î»)y) â‰¥ [Î»G1(x)
sâŠ•(1âˆ’Î»)G2(y)
s
]
1/s for every x, y âˆˆ R
m.
Then R
Rm Hs(x)dx â‰¥

Î»
R
Rm G1(x)dxÎ³
+ (1 âˆ’ Î»)
R
Rm G2(x)dxÎ³1/Î³ for s â‰¥ âˆ’1/m, with
Î³ = s/(1 + ms).
Building on this, the following theorem plays a key role in our analysis of the marginal distribution.
Theorem 3 (Marginal). Let f(x, y) be an s-concave density on a convex set K âŠ† R
n+m with
s â‰¥ âˆ’ 1
m . Denote by K|Rn = {x âˆˆ R
n : âˆƒy âˆˆ R
m s.t. (x, y) âˆˆ K}. For every x in K|Rn , consider
the section K(x) , {y âˆˆ R
m : (x, y) âˆˆ K}. Then the marginal density g(x) ,
R
K(x)
f(x, y)dy is
Î³-concave on K|Rn , where Î³ =
s
1+ms
. Moreover, if f(x, y) is isotropic, then g(x) is isotropic.
Similar to the marginal, the CDF of an s-concave distribution might not remain in the same class.
This is in sharp contrast to log-concave distributions. The following theorem studies the CDF of an
s-concave distribution.
Theorem 4. The CDF of s-concave distribution in R
n is Î³-concave, where Î³ =
s
1+ns
and s â‰¥ âˆ’ 1
n
.
2When s â†’ 0, we note that limsâ†’0(Î»f(x)
s + (1 âˆ’ Î»)f(y)
s
)
1/s = exp(Î» log f(x) + (1 âˆ’ Î») log f(y)).
In this case, f(x) is known to be log-concave.    
Theorem 3 and 4 serve as the bridge that connects high-dimensional s-concave distributions to
one-dimensional Î³-concave distributions. With them, we are able to reduce the high-dimensional
problem to the one-dimensional one.
3.2 Fat-Tailed Density
Tail probability is one of the most distinct characteristics of s-concave distributions compared to
(nearly) log-concave distributions. While it can be shown that the (nearly) log-concave distribution
has an exponentially small tail (Theorem 11, [7]), the tail of an s-concave distribution is fat, as proved
by the following theorem.
Theorem 5 (Tail Probability). Let x come from an isotropic distribution over R
n with an s-concave
density. Then for every t â‰¥ 16, we have Pr[kxk >
âˆš
nt] â‰¤
h
1 âˆ’
cst
1+ns i(1+ns)/s
, where c is an
absolute constant.
Theorem 5 is almost tight for s < 0. To see this, consider X that is drawn from a one-dimensional
Pareto distribution with density f(x) = (âˆ’1 âˆ’
1
s
)
âˆ’ 1
s x
1
s (x â‰¥
s+1
âˆ’s
). It can be easily seen that
Pr[X > t] = h
âˆ’s
s+1 t
i s+1
s
for t â‰¥
s+1
âˆ’s
, which matches Theorem 5 up to an absolute constant factor.
3.3 Geometry of S-Concave Distributions
We now investigate the geometry of s-concave distributions. We first consider one-dimensional sconcave distributions: We provide bounds on the density of centroid-centered halfspaces (Lemma 6)
and range of the density function (Lemma 7). Building upon these, we develop geometric properties
of high-dimensional s-concave distributions by reducing the distributions to the one-dimensional
case based on marginalization (Theorem 3).
3.3.1 One-Dimensional Case
We begin with the analysis of one-dimensional halfspaces. To bound the probability, a normal
technique is to bound the centroid region and the tail region separately. However, the challenge is that
the s-concave distribution is fat-tailed (Theorem 5). So while the probability of a one-dimensional
halfspace is bounded below by an absolute constant for log-concave distributions, such a probability
for s-concave distributions decays as s (â‰¤ 0) becomes smaller. The following lemma captures such
an intuition.
Lemma 6 (Density of Centroid-Centered Halfspaces). Let X be drawn from a one-dimensional
distribution with s-concave density for âˆ’1/2 â‰¤ s â‰¤ 0. Then Pr(X â‰¥ EX) â‰¥ (1 + Î³)
âˆ’1/Î³ for
Î³ = s/(1 + s).
We also study the image of a one-dimensional s-concave density. The following condition for
s > âˆ’1/3 is for the existence of second-order moment.
Lemma 7. Let g : R â†’ R+ be an isotropic s-concave density function and s > âˆ’1/3. (a) For all x,
g(x) â‰¤
1+s
1+3s
; (b) We have g(0) â‰¥
q 1
3(1+Î³)
3/Î³ , where Î³ =
s
s+1 .
3.3.2 High-Dimensional Case
We now move on to the high-dimensional case (n â‰¥ 2). In the following, we will assume âˆ’
1
2n+3 â‰¤
s â‰¤ 0. Though this working range of s vanishes as n becomes larger, it is almost the broadest range
of s that we can hopefully achieve: Chandrasekaran et al. [13] showed a lower bound of s â‰¥ âˆ’ 1
nâˆ’1
if one require the s-concave distribution to have good geometric properties. In addition, we can
see from Theorem 3 that if s < âˆ’
1
nâˆ’1
, the marginal of an s-concave distribution might even not
exist; Such a case does happen for certain s-concave distributions with s < âˆ’
1
nâˆ’1
, e.g., the Cauchy
distribution. So our range of s is almost tight up to a 1/2 factor.
We start our analysis with the density of centroid-centered halfspaces in high-dimensional spaces.
Lemma 8 (Density of Centroid-Centered Halfspaces). Let f : R
n â†’ R+ be an s-concave density
function, and let H be any halfspace containing its centroid. Then R
H
f(x)dx â‰¥ (1 + Î³)
âˆ’1/Î³ for
Î³ = s/(1 + ns).
Proof. W.L.O.G., we assume H is orthogonal to the first axis. By Theorem 3, the first marginal of f is
s/(1+(nâˆ’1)s)-concave. Then by Lemma 6, R
H
f(x)dx â‰¥ (1+Î³)
âˆ’1/Î³, where Î³ = s/(1+ns).
5
The following theorem is an extension of Lemma 7 to high-dimensional spaces. The proofs basically
reduce the n-dimensional density to its first marginal by Theorem 3, and apply Lemma 7 to bound
the image.
Theorem 9 (Bounds on Density). Let f : R
n â†’ R+ be an isotropic s-concave density. Then
(a) Let d(s, n) = (1 + Î³)
âˆ’1/Î³ 1+3Î²
3+3Î²
, where Î² =
s
1+(nâˆ’1)s
and Î³ =
Î²
1+Î²
. For any u âˆˆ R
n such that
kuk â‰¤ d(s, n), we have f(u) â‰¥

kuk
d
((2 âˆ’ 2
âˆ’(n+1)s
)
âˆ’1 âˆ’ 1) + 11/s
f(0).
(b) f(x) â‰¤ f(0) h 1+Î²
1+3Î²
p
3(1 + Î³)
3/Î³2
nâˆ’1+1/ss
âˆ’ 1
i1/s
for every x.
(c) There exists an x âˆˆ R
n such that f(x) > (4eÏ€)
âˆ’n/2
.
(d) (4eÏ€)
âˆ’n/2
h 1+Î²
1+3Î²
p
3(1 + Î³)
3/Î³2
nâˆ’1+ 1
s
s
âˆ’ 1
iâˆ’ 1
s
< f(0) â‰¤ (2 âˆ’ 2
âˆ’(n+1)s
)
1/s nÎ“(n/2)
2Ï€n/2dn .
(e) f(x) â‰¤ (2 âˆ’ 2
âˆ’(n+1)s
)
1/s nÎ“(n/2)
2Ï€n/2dn
h 1+Î²
1+3Î²
p
3(1 + Î³)
3/Î³2
nâˆ’1+1/ss
âˆ’ 1
i1/s
for every x.
(f) For any line ` through the origin, R
`
f â‰¤ (2 âˆ’ 2
âˆ’ns)
1/s (nâˆ’1)Î“((nâˆ’1)/2)
2Ï€(nâˆ’1)/2dnâˆ’1
.
Theorem 9 provides uniform bounds on the density function. To obtain more refined upper bound on
the image of s-concave densities, we have the following lemma. The proof is built upon Theorem 9.
Lemma 10 (More Refined Upper Bound on Densities). Let f : R
n â†’ R+ be an isotropic s-concave
density. Then f(x) â‰¤ Î²1(n, s)(1 âˆ’ sÎ²2(n, s)kxk)
1/s for every x âˆˆ R
n, where
Î²1(n, s) = (2 âˆ’ 2
âˆ’(n+1)s
)
1
s
2Ï€
n/2d
n
(1 âˆ’ s)
âˆ’1/snÎ“(n/2)  1 + Î²
1 + 3Î²
q
3(1 + Î³)
3/Î³2
nâˆ’1+1/ss
âˆ’ 1
1/s
,
Î²2(n, s) = 2Ï€
(nâˆ’1)/2d
nâˆ’1
(n âˆ’ 1)Î“((n âˆ’ 1)/2)(2 âˆ’ 2
âˆ’ns)
âˆ’ 1
s
[(a(n, s) + (1 âˆ’ s)Î²1(n, s)
s
)
1+ 1
s âˆ’ a(n, s)
1+ 1
s ]s
Î²1(n, s)
s(1 + s)(1 âˆ’ s)
,
a(n, s) = (4eÏ€)
âˆ’ns/2
h 1+Î²
1+3Î²
p
3(1 + Î³)
3/Î³2
nâˆ’1+1/ss
âˆ’ 1
iâˆ’1
, Î³ =
Î²
1+Î²
, Î² =
s
1+(nâˆ’1)s
, and
d = (1 + Î³)
âˆ’ 1
Î³
1+3Î²
3+3Î²
.
We also give an absolute bound on the measure of band.
Theorem 11 (Probability inside Band). Let D be an isotropic s-concave distribution in R
n. Denote
by f3(s, n) = 2(1+ns)/(1+ (n+ 2)s). Then for any unit vector w, Prxâˆ¼D[|w Â·x| â‰¤ t] â‰¤ f3(s, n)t.
Moreover, if t â‰¤ d(s, n) ,

1+2Î³
1+Î³
âˆ’
1+Î³
Î³ 1+3Î³
3+3Î³
where Î³ =
s
1+(nâˆ’1)s
, then Prxâˆ¼D[|w Â· x| â‰¤ t] >
f2(s, n)t, where f2(s, n) = 2(2 âˆ’ 2
âˆ’2Î³
)
âˆ’1/Î³(4eÏ€)
âˆ’1/2

2

1+Î³
1+3Î³
âˆš
3

1+2Î³
1+Î³
 3+3Î³
2Î³
Î³
âˆ’ 1
!âˆ’1/Î³
.
To analyze the problem of learning linear separators, we are interested in studying the disagreement
between the hypothesis of the output and the hypothesis of the target. The following theorem captures
such a characteristic under s-concave distributions.
Theorem 12 (Probability of Disagreement). Assume D is an isotropic s-concave distribution in R
n.
Then for any two unit vectors u and v in R
n, we have dD(u, v) = Prxâˆ¼D[sign(uÂ·x) 6= sign(v Â·x)] â‰¥
f1(s, n)Î¸(u, v), where f1(s, n) = c(2 âˆ’ 2
âˆ’3Î±)
âˆ’ 1
Î±
h 1+Î²
1+3Î²
p
3(1 + Î³)
3/Î³2
1+1/Î±Î±
âˆ’ 1
iâˆ’ 1
Î±
(1 +
Î³)
âˆ’2/Î³ 
1+3Î²
3+3Î²
2
, c is an absolute constant, Î± =
s
1+(nâˆ’2)s
, Î² =
s
1+(nâˆ’1)s
, Î³ =
s
1+ns
.
Due to space constraints, all missing proofs are deferred to the supplementary material.
4 Applications: Provable Algorithms under S-Concave Distributions
In this section, we show that many algorithms that work under log-concave distributions behave well
under s-concave distributions by applying the above-mentioned geometric properties. For simplicity,
we will frequently use the notations in Theorem 1.
6
4.1 Margin Based Active Learning
We first investigate margin-based active learning under isotropic s-concave distributions in both
realizable and adversarial noise models. The algorithm (see Algorithm 1) follows a localization
technique: It proceeds in rounds, aiming to cut the error down by half in each round in the margin [6].
Algorithm 1 Margin Based Active Learning under S-Concave Distributions
Input: Parameters bk, Ï„k, rk, mk, Îº, and T as in Theorem 16.
1: Draw m1 examples from D, label them and put them into W.
2: For k = 1, 2, ..., T
3: Find vk âˆˆ ball(wkâˆ’1, rk) to approximately minimize the hinge loss over W s.t. kvkk â‰¤ 1:
`Ï„k â‰¤ minwâˆˆball(wkâˆ’1,rk)âˆ©ball(0,1) `Ï„k
(w, W) + Îº/8.
4: Normalize vk, yielding wk =
vk
kvkk
; Clear the working set W.
5: While mk+1 additional data points are not labeled
6: Draw sample x from D.
7: If |wk Â· x| â‰¥ bk, reject x; else ask for label of x and put into W.
Output: Hypothesis wT .
4.1.1 Relevant Properties of S-Concave Distributions
The analysis requires more refined geometric properties as below. Theorem 13 basically claims that
the error mostly concentrates in a band, and Theorem 14 guarantees that the variance in any 1-D
direction cannot be too large. We defer the detailed proofs to the supplementary material.
Theorem 13 (Disagreement outside Band). Let u and v be two vectors in R
n and assume that
Î¸(u, v) = Î¸ < Ï€/2. Let D be an isotropic s-concave distribution. Then for any absolute constant
c1 > 0 and any function f1(s, n) > 0, there exists a function f4(s, n) > 0 such that Prxâˆ¼D[sign(u Â·
x) 6= sign(v Â· x) and |v Â· x| â‰¥ f4(s, n)Î¸] â‰¤ c1f1(s, n)Î¸, where f4(s, n) = 4Î²1(2,Î±)B(âˆ’1/Î±âˆ’3,3)
âˆ’c1f1(s,n)Î±3Î²2(2,Î±)
3 ,
B(Â·, Â·) is the beta function, Î± = s/(1 + (n âˆ’ 2)s), Î²1(2, Î±) and Î²2(2, Î±) are given by Lemma 10.
Theorem 14 (1-D Variance). Assume that D is isotropic s-concave. For d given by Theorem 9
(a), there is an absolute C0 such that for all 0 < t â‰¤ d and for all a such that ku âˆ’ ak â‰¤ r and
kak â‰¤ 1, Exâˆ¼Du,t [(a Â· x)
2
] â‰¤ f5(s, n)(r
2 + t
2
), where f5(s, n) = 16 + C0
8Î²1(2,Î·)B(âˆ’1/Î·âˆ’3,2)
f2(s,n)Î²2(2,Î·)
3(Î·+1)Î·2 ,
(Î²1(2, Î·), Î²2(2, Î·)) and f2(s, n) are given by Lemma 10 and Theorem 11, and Î· =
s
1+(nâˆ’2)s
.
4.1.2 Realizable Case
We show that margin-based active learning works under s-concave distributions in the realizable case.
Theorem 15. In the realizable case, let D be an isotropic s-concave distribution in R
n.
Then for 0 <  < 1/4, Î´ > 0, and absolute constants c, there is an algorithm (see the supplementary material) that runs in T = dlog 1
c
e iterations, requires mk =
O

f3 min{2
âˆ’k
f4f
âˆ’1
1
,d}
2âˆ’k

n log f3 min{2
âˆ’k
f4f
âˆ’1
1
,d}
2âˆ’k +log 1+sâˆ’k
Î´
 labels in the k-th round, and outputs
a linear separator of error at most  with probability at least 1 âˆ’ Î´. In particular, when s â†’ 0 (a.k.a.
log-concave), we have mk = O

n + log( 1+sâˆ’k
Î´
)

.
By Theorem 15, we see that the algorithm of margin-based active learning under s-concave distributions works almost as well as the log-concave distributions in the resizable case, improving
exponentially w.r.t. the variable 1/ over passive learning algorithms.
4.1.3 Efficient Learning with Adversarial Noise
In the adversarial noise model, an adversary can choose any distribution Pe over R
n Ã— {+1, âˆ’1} such
that the marginal D over R
n is s-concave but an Î· fraction of labels can be flipped adversarially. The
analysis builds upon an induction technique where in each round we do hinge loss minimization in
the band and cut down the 0/1 loss by half. The algorithm was previously analyzed in [3, 4] for the
special class of log-concave distributions. In this paper, we analyze it for the much more general
class of s-concave distributions.
Theorem 16. Let D be an isotropic s-concave distribution in R
n over x and the label y
obey the adversarial noise model. If the rate Î· of adversarial noise satisfies Î· < c0 for
some absolute constant c0, then for 0 <  < 1/4, Î´ > 0, and an absolute constant c, Algorithm 1 runs in T = dlog 1
c
e iterations, outputs a linear separator wT such that
Prxâˆ¼D[sign(wT Â· x) 6= sign(w
âˆ—
Â· x)] â‰¤  with probability at least 1 âˆ’ Î´. The label complexity
 
in the k-th round is mk = O

[bkâˆ’1s+Ï„k(1+ns)[1âˆ’(Î´/(
âˆš
n(k+k
2
)))s/(1+ns)
]+Ï„ks]
2
Îº2Ï„
2
k
s
2 n

n + log k+k
2
Î´
,
where Îº = max n
f3Ï„k
f2 min{bkâˆ’1,d}
,
bkâˆ’1
âˆš
f5
Ï„k
âˆš
f2
o
, Ï„k = Î˜ 
f
âˆ’2
1
f
âˆ’1/2
2
f3f
2
4
f
1/2
5
2
âˆ’(kâˆ’1)
, and bk =
min{Î˜(2âˆ’kf4f
âˆ’1
1
), d}. In particular, if s â†’ 0, mk = O

n log( n
Î´ )(n + log( k
Î´
))
.
By Theorem 16, the label complexity of margin-based active learning improves exponentially over
that of passive learning w.r.t. 1/ even under fat-tailed s-concave distributions and challenging
adversarial noise model.
4.2 Disagreement Based Active Learning
We apply our results to the analysis of disagreement-based active learning under s-concave distributions. The key is estimating the disagreement coefficient, a measure of complexity of active
learning problems that can be used to bound the label complexity [20]. Recall the definition of the
disagreement coefficient w.r.t. classifier w
âˆ—
, precision , and distribution D as follows. For any r > 0,
define ballD(w, r) = {u âˆˆ H : dD(u, w) â‰¤ r} where dD(u, w) = Prxâˆ¼D[(u Â· x)(w Â· x) < 0].
Define the disagreement region as DIS(H) = {x : âˆƒu, v âˆˆ H s.t. (u Â· x)(v Â· x) < 0}. Let the
Alexander capacity capwâˆ—,D =
PrD(DIS(ballD(w
âˆ—
,r)))
r
. The disagreement coefficient is defined as
Î˜wâˆ—,D() = suprâ‰¥
[capwâˆ—,D(r)]. Below, we state our results on the disagreement coefficient under
isotropic s-concave distributions.
Theorem 17 (Disagreement Coefficient). Let D be an isotropic s-concave distribution over R
n. For
any w
âˆ— and r > 0, the disagreement coefficient is Î˜wâˆ—,D() = O
 âˆš
n(1+ns)
2
s(1+(n+2)s)f1(s,n)
(1 âˆ’ 
s
1+ns )

.
In particular, when s â†’ 0 (a.k.a. log-concave), Î˜wâˆ—,D() = O(
âˆš
n log(1/)).
Our bounds on the disagreement coefficient match the best known results for the much less general
case of log-concave distributions [7]; Furthermore, they apply to the s-concave case where we allow
arbitrary number of discontinuities, a case not captured by [18]. The result immediately implies
concrete bounds on the label complexity of disagreement-based active learning algorithms, e.g.,
CAL [14] and A2
[5]. For instance, by composing it with the result from [17], we obtain a bound
of Oe

n
3/2 (1+ns)
2
s(1+(n+2)s)f(s)
(1 âˆ’ 
s/(1+ns)
)

log2 1
 +
OP T 2

2
 for agnostic active learning under an
isotropic s-concave distribution D. Namely, it suffices to output a halfspace with error at most
OP T + , where OP T = minw errD(w).
4.3 Learning Intersections of Halfspaces
Baum [9] provided a polynomial-time algorithm for learning the intersections of halfspaces w.r.t.
symmetric distributions. Later, Klivans [27] extended the result by showing that the algorithm works
under any distribution D as long as ÂµD(E) â‰ˆ ÂµD(âˆ’E) for any set E. In this section, we show that it
is possible to learn intersections of halfspaces under the broader class of s-concave distributions.
Theorem 18. In the PAC realizable case, there is an algorithm (see the supplementary material) that
outputs a hypothesis h of error at most  with probability at least 1 âˆ’ Î´ under isotropic s-concave
distributions. The label complexity is M(/2, Î´/4, n2
) + max{2m2/,(2/2
) log(4/Î´)},
where M(, Î´, m) is defined by M(, Î´, n) = O

n

log 1
 +
1

log 1
Î´

, m2 =
M(max{Î´/(4eKm1), /2}, Î´/4, n), K = Î²1(3, Îº)
B(âˆ’1/Îºâˆ’3,3)
(âˆ’ÎºÎ²2(3,Îº))3
3+1/Îº
h(Îº)d3+1/Îº , d = (1 + Î³)
âˆ’1/Î³ 1+3Î²
3+3Î²
,
h(Îº) =
1
d
((2 âˆ’ 2
âˆ’4Îº
)
âˆ’1 âˆ’ 1) + 1 1
Îº
(4eÏ€)
âˆ’ 3
2
h 1+Î²
1+3Î²
p
3(1+Î³)
3/Î³2
2+ 1
Îº
Îº
âˆ’1
iâˆ’1/Îº
, Î² =
Îº
1+2Îº
,
Î³ =
Îº
1+Îº
, and Îº =
s
1+(nâˆ’3)s
. In particular, if s â†’ 0 (a.k.a. log-concave), K is an absolute constant.
5 Lower Bounds
In this section, we give information-theoretic lower bounds on the label complexity of passive and
active learning of homogeneous halfspaces under s-concave distributions.
Theorem 19. For a fixed value âˆ’
1
2n+3 â‰¤ s â‰¤ 0 we have: (a) For any s-concave distribution D in
R
n whose covariance matrix is of full rank, the sample complexity of learning origin-centered linear
separators under D in the passive learning scenario is â„¦ (nf1(s, n)/); (b) The label complexity of
active learning of linear separators under s-concave distributions is â„¦ (n log (f1(s, n)/)).
If the covariance matrix of D is not of full rank, then the intrinsic dimension is less than d. So
our lower bounds essentially apply to all s-concave distributions. According to Theorem 19, it is
possible to have an exponential improvement of label complexity w.r.t. 1/ over passive learning by
active sampling, even though the underlying distribution is a fat-tailed s-concave distribution. This
observation is captured by Theorems 15 and 16.   
6 Conclusions
In this paper, we study the geometric properties of s-concave distributions. Our work advances the
state-of-the-art results on the margin-based active learning, disagreement-based active learning, and
learning intersections of halfspaces w.r.t. the distributions over the instance space. When s â†’ 0, our
results reduce to the best-known results for log-concave distributions. The geometric properties of
s-concave distributions can be potentially applied to other learning algorithms, which might be of
independent interest more broadly