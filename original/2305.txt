In this paper we study the well-known greedy coordinate descent (GCD) algorithm
to solve `1-regularized problems and improve GCD by the two popular strategies:
Nesterovâ€™s acceleration and stochastic optimization. Firstly, based on an `1-norm
square approximation, we propose a new rule for greedy selection which is nontrivial to solve but convex; then an efficient algorithm called â€œSOft ThreshOlding
PrOjection (SOTOPO)â€ is proposed to exactly solve an `1-regularized `1-norm
square approximation problem, which is induced by the new rule. Based on the
new rule and the SOTOPO algorithm, the Nesterovâ€™s acceleration and stochastic
optimization strategies are then successfully applied to the GCD algorithm. The resulted algorithm called accelerated stochastic greedy coordinate descent (ASGCD)
has the optimal convergence rate O(
p
1/); meanwhile, it reduces the iteration
complexity of greedy selection up to a factor of sample size. Both theoretically and
empirically, we show that ASGCD has better performance for high-dimensional
and dense problems with sparse solutions.
1 Introduction
In large-scale convex optimization, first-order methods are widely used due to their cheap iteration
cost. In order to improve the convergence rate and reduce the iteration cost further, two important
strategies are used in first-order methods: Nesterovâ€™s acceleration and stochastic optimization.
Nesterovâ€™s acceleration is referred to the technique that uses some algebra trick to accelerate firstorder algorithms; while stochastic optimization is referred to the method that samples one training
example or one dual coordinate at random from the training data in each iteration. Assume the
objective function F(x) is convex and smooth. Let F
âˆ— = minxâˆˆRd F(x) be the optimal value. In
order to find an approximate solution x that satisfies F(x) âˆ’ F
âˆ— â‰¤ , the vanilla gradient descent
method needs O(1/) iterations. While after applying the Nesterovâ€™s acceleration scheme [16],
the resulted accelerated full gradient method (AFG) [16] only needs O(
p
1/) iterations, which is
optimal for first-order algorithms [16]. Meanwhile, assume F(x) is also a finite sum of n sample
convex functions. By sampling one training example, the resulted stochastic gradient descent (SGD)
and its variants [13, 23, 1] can reduce the iteration complexity by a factor of the sample size. As an
alternative of SGD, randomized coordinate descent (RCD) can also reduce the iteration complexity
by a factor of the sample size [15] and obtain the optimal convergence rate O(
p
1/) by Nesterovâ€™s
acceleration [14, 12]. The development of gradient descent and RCD raises an interesting problem:
can the Nesterovâ€™s acceleration and stochastic optimization strategies be used to improve other
existing first-order algorithms?
âˆ—This work is supported by the National Natural Science Foundation of China under grant Nos. 61771273,
61371078.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
In this paper, we answer this question partly by studying coordinate descent with Gauss-Southwell
selection, i.e., greedy coordinate descent (GCD). GCD is widely used for solving sparse optimization
problems in machine learning [22, 9, 17]. If an optimization problem has a sparse solution, it is
more suitable than its counterpart RCD. However, the theoretical convergence rate is still O(1/).
Meanwhile if the iteration complexity is comparable, GCD will be preferable than RCD [17]. However
in the general case, in order to do exact Gauss-Southwell selection, computing the full gradient
beforehand is necessary, which causes GCD has much higher iteration complexity than RCD. To be
concrete, in this paper we consider the well-known nonsmooth `1-regularized problem:
min
xâˆˆRd
n
F(x)
def = f(x) + Î»kxk1
def =
1
n
Xn
j=1
fj (x) + Î»kxk1
o
, (1)
where Î» â‰¥ 0 is a regularization parameter, f(x) = 1
n
Pn
j=1 fj (x) is a smooth convex function that is
a finite average of n smooth convex function fj (x). Given samples {(a1, b1),(a2, b2), . . . ,(an, bn)}
with aj âˆˆ R
d
(j âˆˆ [n]
def = {1, 2, . . . , n}), if each fj (x) = fj (a
T
j x, bj ), then (1) is an `1-regularized
empirical risk minimization (`1-ERM) problem. For example, if bj âˆˆ R and fj (x) = 1
2
(bj âˆ’ a
T
j x)
2
,
(1) is Lasso; if bj âˆˆ {âˆ’1, 1} and fj (x) = log(1 + exp(âˆ’bja
T
j x)), `1-regularized logistic regression
is obtained.
In the above nonsmooth case, the Gauss-Southwell rule has 3 different variants [17, 22]: GS-s, GS-r
and GS-q. The GCD algorithm with all the 3 rules can be viewed as the following procedure: in
each iteration based on a quadratic approximation of f(x) in (1), one minimizes a surrogate objective
function under the constraint that the direction vector used for update has at most 1 nonzero entry.
The resulted problems under the 3 rules are easy to solve but are nonconvex due to the cardinality
constraint of direction vector. While when using Nesterovâ€™s acceleration scheme, convexity is needed
for the derivation of the optimal convergence rate O(
p
1/) [16]. Therefore, it is impossible to
accelerate GCD by the Nesterovâ€™s acceleration scheme under the 3 existing rules.
In this paper, we propose a novel variant of Gauss-Southwell rule by using an `1-norm square
approximation of f(x) rather than quadratic approximation. The new rule involves an `1-regularized
`1-norm square approximation problem, which is nontrivial to solve but is convex. To exactly
solve the challenging problem, we propose an efficient SOft ThreshOlding PrOjection (SOTOPO)
algorithm. The SOTOPO algorithm has O(d + |Q| log |Q|) cost, where it is often the case |Q|  d.
The complexity result O(d + |Q| log |Q|) is better than O(d log d) of its counterpart SOPOPO [18],
which is an Euclidean projection method.
Then based on the new rule and SOTOPO, we accelerate GCD to attain the optimal convergence rate
O(
p
1/) by combing a delicately selected mirror descent step. Meanwhile, we show that it is not
necessary to compute full gradient beforehand: sampling one training example and computing a noisy
gradient rather than full gradient is enough to perform greedy selection. This stochastic optimization
technique reduces the iteration complexity of greedy selection by a factor of the sample size. The
final result is an accelerated stochastic greedy coordinate descent (ASGCD) algorithm.
Assume x
âˆ—
is an optimal solution of (1). Assume that each fj (x)(for all j âˆˆ [n]) is Lp-smooth w.r.t.
k Â· kp (p = 1, 2), i.e., for all x, y âˆˆ R
d
,
kâˆ‡fj (x) âˆ’ âˆ‡fj (y)kq â‰¤ Lpkx âˆ’ ykp, (2)
where if p = 1, then q = âˆ; if p = 2, then q = 2.
In order to find an x that satisfies F(x) âˆ’ F(x
âˆ—
) â‰¤ , ASGCD needs O
 âˆš
CL1kx
âˆ—
âˆš
k1


iterations (see
(16)), where C is a function of d that varies slowly over d and is upper bounded by log2
(d). For
high-dimensional and dense problems with sparse solutions, ASGCD has better performance than the
state of the art. Experiments demonstrate the theoretical result.
Notations: Let [d] denote the set {1, 2, . . . , d}. Let R+ denote the set of nonnegative real number. For
x âˆˆ R
d
, let kxkp = (Pd
i=1 |xi
|
p
)
1
p (1 â‰¤ p < âˆ) denote the `p-norm and kxkâˆ = maxiâˆˆ[d]
|xi
|
denote the `âˆ-norm of x. For a vector x, let dim(x) denote the dimension of x; let xi denote the i-th
element of x. For a gradient vector âˆ‡f(x), let âˆ‡if(x) denote the i-th element of âˆ‡f(x). For a set
S, let |S| denote the cardinality of S. Denote the simplex 4d = {Î¸ âˆˆ R
d
+ :
Pd
i=1 Î¸i = 1}.
2
2 The SOTOPO algorithm
The proposed SOTOPO algorithm aims to solve the proposed new rule, i.e., minimize the following
`1-regularized `1-norm square approximation problem,
hËœ
def = arg min
gâˆˆRd

hâˆ‡f(x), gi +
1
2Î·
kgk
2
1 + Î»kx + gk1

, (3)
xËœ
def = x + h, Ëœ (4)
where x denotes the current iteration, Î· a step size, g the variable to optimize, hËœ the director vector for
update and xËœ the next iteration. The number of nonzero entries of hËœ denotes how many coordinates
will be updated in this iteration. Unlike the quadratic approximation used in GS-s, GS-r and GS-q
rules, in the new rule the coordinate(s) to update is implicitly selected by the sparsity-inducing
property of the `1-norm square kgk
2
1
rather than using the cardinality constraint kgk0 â‰¤ 1 (i.e., g
has at most 1 nonzero element) [17, 22]. By [6, Â§9.4.2], when the nonsmooth term Î»kx + gk1 in (1)
does not exist, the minimizer of the `1-norm square approximation (i.e., `1-norm steepest descent)
is equivalent to GCD. When Î»kx + gk1 exists, generally, there may be one or more coordinates to
update in this new rule. Because of the sparsity-inducing property of kgk
2
1
and kx + gk1, both the
direction vector hËœ and the iterative solution xËœ are sparse. In addition, (3) is an unconstrained problem
and thus is feasible.
2.1 A variational reformulation and its properties
(3) involves the nonseparable, nonsmooth term kgk
2
1
and the nonsmooth term kx + gk1. Because
there are two nonsmooth terms, it seems difficult to solve (3) directly. While by the variational
identity kgk
2
1 = infÎ¸âˆˆ4d
Pd
i=1
g
2
i
Î¸i
in [4]
2
, in Lemma 1, it is shown that we can transform the
original nonseparable and nonsmooth problem into a separable and smooth optimization problem on
a simplex.
Lemma 1. By defining
J(g, Î¸)
def
= hâˆ‡f(x), gi +
1
2Î·
X
d
i=1
g
2
i
Î¸i
+ Î»kx + gk1, (5)
gËœ(Î¸)
def = arg mingâˆˆRd J(g, Î¸), J(Î¸)
def
= J(Ëœg(Î¸), Î¸), (6)
ËœÎ¸
def = arg infÎ¸âˆˆ4d J(Î¸), (7)
where gËœ(Î¸) is a vector function. Then the minimization problem to find hËœ in (3) is equivalent to the
problem (7) to find ËœÎ¸ with the relation hËœ = Ëœg(
ËœÎ¸). Meanwhile, gËœ(Î¸) and J(Î¸) in (6) are both coordinate
separable with the expressions
âˆ€i âˆˆ [d], gËœi(Î¸) = Ëœgi(Î¸i)
def
= sign(xi âˆ’ Î¸iÎ·âˆ‡if(x)) Â· max{0, |xi âˆ’ Î¸iÎ·âˆ‡if(x)| âˆ’ Î¸iÎ·Î»} âˆ’ xi
, (8)
J(Î¸) = X
d
i=1
Ji(Î¸i), where Ji(Î¸i)
def
= âˆ‡if(x) Â· gËœi(Î¸i) + 1
2Î·
X
d
i=1
gËœ
2
i
(Î¸i)
Î¸i
+ Î»|xi + Ëœgi(Î¸i)|. (9)
In Lemma 1, (8) is obtained by the iterative soft thresholding operator [5]. By Lemma 1, we can
reformulate (3) into the problem (5), which is about two parameters g and Î¸. Then by the joint
convexity, we swap the optimization order of g and Î¸. Fixing Î¸ and optimizing with respect to (w.r.t.)
g, we can get a closed form of gËœ(Î¸), which is a vector function about Î¸. Substituting gËœ(Î¸) into J(g, Î¸),
we get the problem (7) about Î¸. Finally, the optimal solution hËœ in (3) can be obtained by hËœ = Ëœg(
ËœÎ¸).
The explicit expression of each Ji(Î¸i) can be given by substituting (8) into (9). Because Î¸ âˆˆ 4d, we
have for all i âˆˆ [d], 0 â‰¤ Î¸i â‰¤ 1. In the following Lemma 2, it is observed that the derivate J
0
i
(Î¸i) can
be a constant or have a piecewise structure, which is the key to deduce the SOTOPO algorithm.
2The infima can be replaced by minimization if the convention â€œ0/0 = 0â€ is used.
3
Lemma 2. Assume that for all i âˆˆ [d], J
0
i
(0) and J
0
i
(1) have been computed. Denote ri1
def
=
âˆš
|xi|
âˆ’2Î·J0
i
(0)
and ri2
def
= âˆš
|xi|
âˆ’2Î·J0
i
(1)
, then J
0
i
(Î¸i) belongs to one of the 4 cases,
(case a) : J
0
i
(Î¸i) = 0, 0 â‰¤ Î¸i â‰¤ 1, (case b) : J
0
i
(Î¸i) = J
0
i
(0) < 0, 0 â‰¤ Î¸i â‰¤ 1,
(case c) : J
0
i
(Î¸i) = (
J
0
i
(0), 0 â‰¤ Î¸i â‰¤ ri1
âˆ’
x
2
i
2Î·Î¸2
i
, ri1 < Î¸i â‰¤ 1
, (case d) : J
0
i
(Î¸i) =
ï£±
ï£´ï£²
ï£´ï£³
J
0
i
(0), 0 â‰¤ Î¸i â‰¤ ri1
âˆ’
x
2
i
2Î·Î¸2
i
, ri1 < Î¸i < ri2
J
0
i
(1), ri2 â‰¤ Î¸i â‰¤ 1
.
Although the formulation of J
0
i
(Î¸i) is complicated, by summarizing the property of the 4 cases in
Lemma 2, we have Corollary 1.
Corollary 1. For all i âˆˆ [d] and 0 â‰¤ Î¸i â‰¤ 1, if the derivate J
0
i
(Î¸i) is not always 0, then J
0
i
(Î¸i) is a
non-decreasing, continuous function with value always less than 0.
Corollary 1 shows that except the trivial (case a), for all i âˆˆ [d], whichever J
0
i
(Î¸i) belong to (case b),
(case c) or case (d), they all share the same group of properties, which makes a consistent iterative
procedure possible for all the cases. The different formulations in the four cases mainly have impact
about the stopping criteria of SOTOPO.
2.2 The property of the optimal solution
The Lagrangian of the problem (7) is
L(Î¸, Î³, Î¶)
def = J(Î¸) + Î³
X
d
i=1
Î¸i âˆ’ 1

âˆ’ hÎ¶, Î¸i, (10)
where Î³ âˆˆ R is a Lagrange multiplier and Î¶ âˆˆ R
d
+ is a vector of non-negative Lagrange multipliers.
Due to the coordinate separable property of J(Î¸) in (9), it follows that âˆ‚J(Î¸)
âˆ‚Î¸i
= J
0
i
(Î¸i). Then the
KKT condition of (10) can be written as
âˆ€i âˆˆ [d], J0
i
(Î¸i) + Î³ âˆ’ Î¶i = 0, Î¶iÎ¸i = 0, and X
d
i=1
Î¸i = 1. (11)
By reformulating the KKT condition (11), we have Lemma 3.
Lemma 3. If (ËœÎ³, ËœÎ¸, ËœÎ¶) is a stationary point of (10), then ËœÎ¸ is an optimal solution of (7). Meanwhile,
denote S
def
= {i :
ËœÎ¸i > 0} and T
def
= {j :
ËœÎ¸j = 0}, then the KKT condition can be formulated as
ï£±
ï£²
ï£³
P
iâˆˆS
ËœÎ¸i = 1;
for all j âˆˆ T, ËœÎ¸j = 0;
for all i âˆˆ S, Î³Ëœ = âˆ’J
0
i
(
ËœÎ¸i) â‰¥ maxjâˆˆT âˆ’J
0
j
(0).
(12)
By Lemma 3, if the set S in Lemma 3 is known beforehand, then we can compute ËœÎ¸ by simply
applying the equations in (12). Therefore finding the optimal solution ËœÎ¸ is equivalent to finding the
set of the nonzero elements of ËœÎ¸.
2.3 The soft thresholding projection algorithm
In Lemma 3, for each i âˆˆ [d] with ËœÎ¸i > 0, it is shown that the negative derivate âˆ’J
0
i
(
ËœÎ¸i) is equal to a
single variable Î³Ëœ. Therefore, a much simpler problem can be obtained if we know the coordinates of
these positive elements. At first glance, it seems difficult to identify these coordinates, because the
number of potential subsets of coordinates is clearly exponential on the dimension d. However, the
property clarified by Lemma 2 enables an efficient procedure for identifying the nonzero elements of
ËœÎ¸. Lemma 4 is a key tool in deriving the procedure for identifying the non-zero elements of ËœÎ¸.
Lemma 4 (Nonzero element identification). Let ËœÎ¸ be an optimal solution of (7). Let s and t be two
coordinates such that J
0
s
(0) < J0
t
(0). If ËœÎ¸s = 0, then ËœÎ¸t must be 0 as well; equivalently, if ËœÎ¸t > 0,
then ËœÎ¸s must be greater than 0 as well.
4
Lemma 4 shows that if we sort u
def = âˆ’âˆ‡J(0) such that ui1 â‰¥ ui2 â‰¥ Â· Â· Â· â‰¥ uid
, where {i1, i2, . . . , id}
is a permutation of [d], then the set S in Lemma 3 is of the form {i1, i2, . . . , i%}, where 1 â‰¤ % â‰¤ d.
If % is obtained, then we can use the fact that for all j âˆˆ [%],
âˆ’J
0
ij
(
ËœÎ¸ij
) = ËœÎ³ and X%
j=1
ËœÎ¸ij = 1 (13)
to compute Î³Ëœ. Therefore, by Lemma 4, we can efficiently identify the nonzero elements of the optimal
solution ËœÎ¸ after a sort operation, which costs O(d log d). However based on Lemmas 2 and 3, the sort
cost O(d log d) can be further reduced by the following Lemma 5.
Lemma 5 (Efficient identification). Assume ËœÎ¸ and S are given in Lemma 3. Then for all i âˆˆ S,
âˆ’J
0
i
(0) â‰¥ max
jâˆˆ[d]
{âˆ’J
0
j
(1)}. (14)
By Lemma 5, before ordering u, we can filter out all the coordinates iâ€™s that satisfy âˆ’J
0
i
(0) <
maxjâˆˆ[d] âˆ’J
0
j
(1). Based on Lemmas 4 and 5, we propose the SOft ThreshOlding PrOjection
(SOTOPO) algorithm in Alg. 1 to efficiently obtain an optimal solution ËœÎ¸. In the step 1, by Lemma 5,
we find the quantity vm, im and Q. In the step 2, by Lemma 4, we sort the elements {âˆ’J
0
i
(0)| i âˆˆ Q}.
In the step 3, because S in Lemma 3 is of the form {i1, i2, . . . , i%}, we search the quantity Ï from
1 to |Q| + 1 until a stopping criteria is met. In Alg. 1, the number of nonzero elements of ËœÎ¸ is Ï or
Ï âˆ’ 1. In the step 4, we compute the Î³Ëœ in Lemma 3 according to the conditions. In the step 5, the
optimal ËœÎ¸ and the corresponding h, Ëœ xËœ are given.
Algorithm 1 xËœ =SOTOPO(âˆ‡f(x), x, Î», Î·)
1. Find
(vm, im)
def = (maxiâˆˆ[d]{âˆ’J
0
i
(1)}, arg maxiâˆˆ[d]{âˆ’J
0
i
(1)}), Q def = {i âˆˆ [d]| âˆ’ J
0
i
(0) > vm}.
2. Sort {âˆ’J
0
i
(0)| i âˆˆ Q} such that âˆ’J
0
i1
(0) â‰¥ âˆ’J
0
i2
(0) â‰¥ Â· Â· Â· â‰¥ âˆ’J
0
i|Q|
(0), where
{i1, i2, . . . , i|Q|} is a permutation of the elements in Q. Denote
v
def = (âˆ’J
0
i1
(0), âˆ’J
0
i2
(0), . . . , âˆ’J
0
i|Q|
(0), vm), and i|Q|+1
def = im, v|Q|+1
def = vm.
3. For j âˆˆ [|Q| + 1], denote Rj = {ik|k âˆˆ [j]}. Search from 1 to |Q| + 1 to find the quantity
Ï
def = min 
j âˆˆ [|Q| + 1]| J
0
ij
(0) = J
0
ij
(1) or X
lâˆˆRj
|xl
| â‰¥ p
2Î·vj or j = |Q| + 1	
.
4. The Î³Ëœ in Lemma 3 is given by
Î³Ëœ =
(P
lâˆˆRÏâˆ’1
|xl
|
2
/(2Î·), if P
lâˆˆRÏâˆ’1
|xl
| â‰¥ p
2Î·vÏ;
vÏ, otherwise.
5. Then the ËœÎ¸ in Lemma 3 and its corresponding h, Ëœ xËœ in (3) and (4) are obtained by
(
ËœÎ¸l
, hËœ
l
, xËœl) =
ï£±
ï£´ï£²
ï£´ï£³
 |xl|
âˆš
2Î·Î³Ëœ
, âˆ’xl
, 0

, if l âˆˆ RÏ\{iÏ};

1 âˆ’
P
kâˆˆ RÏ\{iÏ}
ËœÎ¸k, gËœl(
ËœÎ¸l), xl + Ëœgl(
ËœÎ¸l)

, if l = iÏ;
(0, 0, xl), if l âˆˆ [d]\RÏ.
In Theorem 1, we give the main result about the SOTOPO algorithm.
Theorem 1. The SOTOPO algorihtm in Alg. 1 can get the exact minimizer h, Ëœ xËœ of the `1-regularized
`1-norm square approximation problem in (3) and (4).
The SOTOPO algorithm seems complicated but is indeed efficient. The dominant operations in Alg.
1 are steps 1 and 2 with the total cost O(d + |Q| log |Q|). To show the effect of the complexity
reduction by Lemma 5, we give the following fact.  
Proposition 1. For the optimization problem defined in (5)-(7), where Î» is the regularization parameter of the original problem (1), we have that
0 â‰¤ max
iâˆˆ[d]
(s
âˆ’2J
0
i
(0)
Î·
)
âˆ’ max
jâˆˆ[d]
ï£±
ï£²
ï£³
s
âˆ’2J
0
j
(1)
Î·
ï£¼
ï£½
ï£¾
â‰¤ 2Î». (15)
Assume vm is defined in the step 1 of Alg. 1. By Proposition 1, for all i âˆˆ Q,
s
âˆ’2J
0
i
(0)
Î·
â‰¤ max
kâˆˆ[d]
(s
âˆ’2J
0
k
(0)
Î·
)
â‰¤ max
jâˆˆ[d]
ï£±
ï£²
ï£³
s
âˆ’2J
0
j
(1)
Î·
ï£¼
ï£½
ï£¾
+ 2Î» =
r
2vm
Î·
+ 2Î»,
Therefore at least the coordinates jâ€™s that satisfy qâˆ’2J
0
j
(0)
Î· >
q2vm
Î· + 2Î» will be not contained in
Q. In practice, it can considerably reduce the sort complexity.
Remark 1. SOTOPO can be viewed as an extension of the SOPOPO algorithm [18] by changing the
objective function from Euclidean distance to a more general function J(Î¸) in (9). It should be noted
that Lemma 5 does not have a counterpart in the case that the objective function is Euclidean distance
[18]. In addition, some extension of randomized median finding algorithm [10] with linear time in
our setting is also deserved to research. Due to the limited space, it is left for further discussion.
3 The ASGCD algorithm
Now we can come back to our motivation, i.e., accelerating GCD to obtain the optimal convergence
rate O(1/
âˆš
) by Nesterovâ€™s acceleration and reducing the complexity of greedy selection by stochastic optimization. The main idea is that although like any (block) coordinate descent algorithm, the
proposed new rule, i.e., minimizing the problem in (3), performs update on one or several coordinates,
it is a generalized proximal gradient descent problem based on `1-norm. Therefore this rule can be
applied into the existing Nesterovâ€™s acceleration and stochastic optimization framework â€œKatyushaâ€
[1] if it can be solved efficiently. The final result is the accelerated stochastic greedy coordinate
descent (ASGCD) algorithm, which is described in Alg. 2.
Algorithm 2 ASGCD
Î´ = log(d) âˆ’ 1 âˆ’
p
(log(d) âˆ’ 1)2 âˆ’ 1;
p = 1 + Î´, q =
p
pâˆ’1
, C =
d
2Î´
1+Î´
Î´
;
z0 = y0 = Ëœx0 = Ï‘0 = 0;
Ï„2 =
1
2
, m = d
n
b
e, Î· =
1
(1+2 nâˆ’b
b(nâˆ’1) )L1
;
for s = 0, 1, 2, . . . , S âˆ’ 1, do
1. Ï„1,s =
2
s+4 , Î±s =
Î·
Ï„1,sC
;
2. Âµs = âˆ‡f(Ëœxs);
3. for l = 0, 1, . . . , m âˆ’ 1, do
(a) k = (sm) + l;
(b) randomly sample a mini batch B of size b from {1, 2, . . . , n} with equal probability;
(c) xk+1 = Ï„1,szk + Ï„2xËœs + (1 âˆ’ Ï„1,s âˆ’ Ï„2)yk;
(d) âˆ‡Ëœ
k+1 = Âµs +
1
b
P
jâˆˆB(âˆ‡fj (xk+1) âˆ’ âˆ‡fj (Ëœxs));
(e) yk+1 =SOTOPO(âˆ‡Ëœ
k+1, xk+1, Î», Î·);
(f) (zk+1, Ï‘k+1) = pCOMID(âˆ‡Ëœ
k+1, Ï‘k, q, Î», Î±s);
end for
4. xËœs+1 =
1
m
Pm
l=1 ysm+l
;
end for
Output: xËœS
6
Algorithm 3 (Ëœx, Ï‘Ëœ) = pCOMID(g, Ï‘, q, Î», Î±)
1. âˆ€i âˆˆ [d], Ï‘Ëœ
i = sign(Ï‘i âˆ’ Î±gi) Â· max{0, |Ï‘i âˆ’ Î±gi
| âˆ’ Î±Î»};
2. âˆ€i âˆˆ [d], xËœi =
sign(Ï‘Ëœi)|Î¸Ëœi|
qâˆ’1
kÏ‘Ëœk
qâˆ’2
q
;
3. Output: x, Ëœ Ï‘Ëœ.
In Alg. 2, the gradient descent step 3(e) is solved by the proposed SOTOPO algorithm, while the
mirror descent step 3(f) is solved by the COMID algorithm with p-norm divergence [11, Sec. 7.2].
We denote the mirror descent step as pCOMID in Alg. 3. All other parts are standard steps in the
Katyusha framework except some parameter settings. For example, instead of the custom setting
p = 1 + 1/log(d) [19, 11], a particular choice p = 1 + Î´ (Î´ is defined in Alg. 2) is used to minimize
the C =
d
2Î´
1+Î´
Î´
. C varies slowly over d and is upper bounded by log2
(d). Meanwhile, Î±k+1 depends
on the extra constant C. Furthermore, the step size Î· =
1
(1+2 nâˆ’b
b(nâˆ’1) )L1
is used, where L1 is defined
in (2). Finally, unlike [1, Alg. 2], we let the batch size b as an algorithm parameter to cover both the
stochastic case b < n and the deterministic case b = n. To the best of our knowledge, the existing
GCD algorithms are deterministic, therefore by setting b = n, we can compare with the existing
GCD algorithms better.
Based on the efficient SOTOPO algorithm, ASGCD has nearly the same iteration complexity with
the standard form [1, Alg. 2] of Katyusha. Meanwhile we have the following convergence rate.
Theorem 2. If each fj (x)(j âˆˆ [n]) is convex, L1-smooth in (2) and x
âˆ—
is an optimum of the
`1-regularized problem (1), then ASGCD satisfies
E[F(Ëœx
S
)] âˆ’ F(x
âˆ—
) â‰¤
4
(S + 3)2

1 +
1 + 2Î²(b)
2m
C

L1kx
âˆ—
k
2
1 = O

CL1kx
âˆ—k
2
1
S2

, (16)
where Î²(b) = nâˆ’b
b(nâˆ’1) , S, b, m and C are given in Alg. 2. In other words, ASGCD achieves an
-additive error (i.e., E[F(Ëœx
S)] âˆ’ F(x
âˆ—
) â‰¤  ) using at most O
 âˆš
CL1kx
âˆ—
âˆš
k1


iterations.
In Table 1, we give the convergence rate of the existing algorithms and ASGCD to solve the `1-
regularized problem (1). In the first column, â€œAccâ€ and â€œNon-Accâ€ denote the corresponding
algorithms are Nesterovâ€™s accelerated or not respectively, â€œPrimalâ€ and â€œDualâ€ denote the corresponding algorithms solves the primal problem (1) and its regularized dual problem [20] respectively,
`2-norm and `1-norm denote the theoretical guarantee is based on `2-norm and `1-norm respectively.
In terms of `2-norm based guarantee, Katyusha and APPROX give the state of the art convergence rate
O
 âˆš
L2kx
âˆ—
âˆš
k2


. In terms of `1-norm based guarantee, GCD gives the state of the art convergence rate
O(
L1kxk
2
1

), which is only applicable for the smooth case Î» = 0 in (1). When Î» > 0, the generalized
GS-r, GS-s and GS-q rules generally have worse theoretical guarantee than GCD [17]. While the
bound of ASGCD in this paper is O(
âˆš
L1kxk1 log d
âˆš

), which can be viewed as an accelerated version
of the `1-norm based guarantee O(
L1kxk
2
1

). Meanwhile, because the bound depends on kx
âˆ—k1 rather
than kx
âˆ—k2 and on L1 rather than L2 (L1 and L2 are defined in (2)), for the `1-ERM problem, if the
samples are high-dimensional, dense and the regularization parameter Î» is relatively large, then it is
possible that L1  L2 (in the extreme case, L2 = dL1 [9]) and kx
âˆ—k1 â‰ˆ kx
âˆ—k2. In this case, the
`1-norm based guarantee O(
âˆš
L1kxk1 log d
âˆš

) of ASGCD is better than the `2-norm based guarantee
O
 âˆš
L2kx
âˆ—
âˆš
k2


of Katyusha and APPROX. Finally, whether the log d factor in the bound of ASGCD
(which also appears in the COMID [11] analysis) is necessary deserves further research.
Remark 2. When the batch size b = n, ASGCD is a deterministic algorithm. In this case, we can use
a better smooth constant T1 that satisfies kâˆ‡f(x) âˆ’ âˆ‡f(y)kâˆ â‰¤ T1kx âˆ’ yk1 rather than L1 [1].
Remark 3. The necessity of computing the full gradient beforehand is the main bottleneck of GCD in
applications [17]. There exists some work [9] to avoid the computation of full gradient by performing
some approximate greedy selection. While the method in [9] needs preprocessing, incoherence
7
Table 1: Convergence rate on `1-regularized empirical risk minimization problems. (For GCD, the
convergence rate is applied for Î» = 0. )
ALGORITHM TYPE PAPER CONVERGENCE RATE
NON-ACC, PRIMAL, `2-NORM SAGA [8] O

L2kx
âˆ—k
2
2


ACC, PRIMAL, `2-NORM KATYUSHA [1] O
 âˆšL2kx
âˆ—
âˆš
k2


ACC, ACC-SDCA [21]
O
 âˆšL2kx
âˆ—
âˆš
k2

log( 1

)
 DUAL, SPDC [24]
`2-NORM APCG [14]
APPROX [12]
NON-ACC, PRIMAL, `1-NORM GCD [2] O

L1kx
âˆ—k
2
1


ACC, PRIMAL, `1-NORM ASGCD (THIS PAPER) O
 âˆšL1kx
âˆ—k1 log d âˆš

condition for dataset and is somewhat complicated. Contrary to [9], the proposed ASGCD algorithm
reduces the complexity of greedy selection by a factor up to n in terms of the amortized cost by simply
applying the existing stochastic variance reduction framework.
4 Experiments
In this section, we use numerical experiments to demonstrate the theoretical results in Section 3
and show the empirical performance of ASGCD with batch size b = 1 and its deterministic version
with b = n (In Fig. 1 they are denoted as ASGCD (b = 1) and ASGCD (b = n) respectively). In
addition, following the claim to using data access rather than CPU time [19] and the recent SGD
and RCD literature [13, 14, 1], we use the data access, i.e., the number of times the algorithm
accesses the data matrix, to measure the algorithm performance. To show the effect of Nesterovâ€™s
acceleration, we compare ASGCD (b = n) with the non-accelerated greedy coordinate descent
with GS-q rule, i.e., coordinate gradient descent (CGD) [22]. To show the effect of both Nesterovâ€™s
acceleration and stochastic optimization strategies, we compare ASGCD (b = 1) with Katyusha
[1, Alg. 2]. To show the effect of the proposed new rule in Section 2, which is based on `1-norm
square approximation, we compare ASGCD (b = n) with the `2-norm based proximal accelerated
full gradient (AFG) implemented by the linear coupling framework [3]. Meanwhile, as a benchmark
of stochastic optimization for the problems with finite-sum structure, we also show the performance
of proximal stochastic variance reduced gradient (SVRG) [23]. In addition, based on [1] and our
experiments, we find that â€œKatyushaâ€ [1, Alg. 2] has the best empirical performance in general for
the `1-regularized problem (1). Therefore other well-known state-of-art algorithms, such as APCG
[14] and accelerated SDCA [21], are not included in the experiments.
The datasets are obtained from LIBSVM data [7] and summarized in Table 2. All the algorithms are
used to solve the following lasso problem
min
xâˆˆRd
{f(x) + Î»kxk1 =
1
2n
kb âˆ’ Axk
2
2 + Î»kxk1} (17)
on the 3 datasets, where A = (a1, a2, . . . , an)
T = (h1, h2, . . . , hd) âˆˆ R
nÃ—d with each aj âˆˆ R
d
representing a sample vector and hi âˆˆ R
n representing a feature vector, b âˆˆ R
n is the prediction
vector.
Table 2: Characteristics of three real datasets.
DATASET NAME # SAMPLES n # FEATURES d
LEUKEMIA 38 7129
GISETTE 6000 5000
MNIST 60000 780
For ASGCD (b = 1) and Katyusha [1, Alg. 2], we can use the tight smooth constant L1 =
maxjâˆˆ[n],iâˆˆ[d]
|a
2
j,i| and L2 = maxjâˆˆ[n] kajk
2
2
respectively in their implementation. While for AS8
Î» Leu Gisette Mnist
10âˆ’2
0 1 2 3 4 5
Number of Passes Ã—10 4
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log loss
CGD
AFG
ASGCD (b=n)
SVRG
Katyusha
ASGCD (b=1)
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of Passes
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of Passes
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
10âˆ’6
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Number of Passes
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
0 1 2 3 4 5 6 7 8 9 10
Number of Passes Ã—104
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
Number of Passes Ã—104
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
Figure 1: Comparing AGCD (b = 1) and ASGCD (b = n) with CGD, SVRG, AFG and Katyusha on
Lasso.
GCD (b = n) and AFG, the better smooth constant T1 =
maxiâˆˆ[d] khik
2
2
n
and T2 =
kAk
2
n
are used respectively. The learning rate of CGD and SVRG are tuned in {10âˆ’6
, 10âˆ’5
, 10âˆ’4
, 10âˆ’3
, 10âˆ’2
, 10âˆ’1}.
Table 3: Factor rates of for the 6 cases
Î» LEU GISETTE MNIST
10âˆ’2
(0.85, 1.33) (0.88, 0.74) (5.85, 3.02)
10âˆ’6
(1.45, 2.27) (3.51, 2.94) (5.84, 3.02)
We use Î» = 10âˆ’6
and Î» = 10âˆ’2
in the experiments. In addition, for each case (Dataset, Î»), AFG is
used to find an optimum x
âˆ— with enough accuracy.
The performance of the 6 algorithms is plotted in Fig. 1. We use Log loss log(F(xk) âˆ’ F(x
âˆ—
)) in the
y-axis. x-axis denotes the number that the algorithm access the data matrix A. For example, ASGCD
(b = n) accesses A once in each iteration, while ASGCD (b = 1) accesses A twice in an entire outer
iteration. For each case (Dataset, Î»), we compute the rate (r1, r2) =  âˆš
CL1kx
âˆ—
âˆš
k1
L2kxâˆ—k2
,
âˆš
CT1kx
âˆ—
âˆš
k1
T2kxâˆ—k2

in Table 3. First, because of the acceleration effect, ASGCD (b = n) are always better than the
non-accelerated CGD algorithm; second, by comparing ASGCD(b = 1) with Katyusha and ASGCD
(b = n) with AFG, we find that for the cases (Leu, 10âˆ’2
), (Leu, 10âˆ’6
) and (Gisette, 10âˆ’2
), ASGCD
(b = 1) dominates Katyusha [1, Alg.2] and ASGCD (b = n) dominates AFG. While the theoretical
analysis in Section 3 shows that if r1 is relatively small such as around 1, then ASGCD (b = 1)
will be better than [1, Alg.2]. For the other 3 cases, [1, Alg.2] and AFG are better. The consistency
between Table 3 and Fig. 1 demonstrates the theoretical analysis.