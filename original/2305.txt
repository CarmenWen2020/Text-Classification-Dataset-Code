In this paper we study the well-known greedy coordinate descent (GCD) algorithm
to solve `1-regularized problems and improve GCD by the two popular strategies:
Nesterov‚Äôs acceleration and stochastic optimization. Firstly, based on an `1-norm
square approximation, we propose a new rule for greedy selection which is nontrivial to solve but convex; then an efficient algorithm called ‚ÄúSOft ThreshOlding
PrOjection (SOTOPO)‚Äù is proposed to exactly solve an `1-regularized `1-norm
square approximation problem, which is induced by the new rule. Based on the
new rule and the SOTOPO algorithm, the Nesterov‚Äôs acceleration and stochastic
optimization strategies are then successfully applied to the GCD algorithm. The resulted algorithm called accelerated stochastic greedy coordinate descent (ASGCD)
has the optimal convergence rate O(
p
1/); meanwhile, it reduces the iteration
complexity of greedy selection up to a factor of sample size. Both theoretically and
empirically, we show that ASGCD has better performance for high-dimensional
and dense problems with sparse solutions.
1 Introduction
In large-scale convex optimization, first-order methods are widely used due to their cheap iteration
cost. In order to improve the convergence rate and reduce the iteration cost further, two important
strategies are used in first-order methods: Nesterov‚Äôs acceleration and stochastic optimization.
Nesterov‚Äôs acceleration is referred to the technique that uses some algebra trick to accelerate firstorder algorithms; while stochastic optimization is referred to the method that samples one training
example or one dual coordinate at random from the training data in each iteration. Assume the
objective function F(x) is convex and smooth. Let F
‚àó = minx‚ààRd F(x) be the optimal value. In
order to find an approximate solution x that satisfies F(x) ‚àí F
‚àó ‚â§ , the vanilla gradient descent
method needs O(1/) iterations. While after applying the Nesterov‚Äôs acceleration scheme [16],
the resulted accelerated full gradient method (AFG) [16] only needs O(
p
1/) iterations, which is
optimal for first-order algorithms [16]. Meanwhile, assume F(x) is also a finite sum of n sample
convex functions. By sampling one training example, the resulted stochastic gradient descent (SGD)
and its variants [13, 23, 1] can reduce the iteration complexity by a factor of the sample size. As an
alternative of SGD, randomized coordinate descent (RCD) can also reduce the iteration complexity
by a factor of the sample size [15] and obtain the optimal convergence rate O(
p
1/) by Nesterov‚Äôs
acceleration [14, 12]. The development of gradient descent and RCD raises an interesting problem:
can the Nesterov‚Äôs acceleration and stochastic optimization strategies be used to improve other
existing first-order algorithms?
‚àóThis work is supported by the National Natural Science Foundation of China under grant Nos. 61771273,
61371078.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
In this paper, we answer this question partly by studying coordinate descent with Gauss-Southwell
selection, i.e., greedy coordinate descent (GCD). GCD is widely used for solving sparse optimization
problems in machine learning [22, 9, 17]. If an optimization problem has a sparse solution, it is
more suitable than its counterpart RCD. However, the theoretical convergence rate is still O(1/).
Meanwhile if the iteration complexity is comparable, GCD will be preferable than RCD [17]. However
in the general case, in order to do exact Gauss-Southwell selection, computing the full gradient
beforehand is necessary, which causes GCD has much higher iteration complexity than RCD. To be
concrete, in this paper we consider the well-known nonsmooth `1-regularized problem:
min
x‚ààRd
n
F(x)
def = f(x) + Œªkxk1
def =
1
n
Xn
j=1
fj (x) + Œªkxk1
o
, (1)
where Œª ‚â• 0 is a regularization parameter, f(x) = 1
n
Pn
j=1 fj (x) is a smooth convex function that is
a finite average of n smooth convex function fj (x). Given samples {(a1, b1),(a2, b2), . . . ,(an, bn)}
with aj ‚àà R
d
(j ‚àà [n]
def = {1, 2, . . . , n}), if each fj (x) = fj (a
T
j x, bj ), then (1) is an `1-regularized
empirical risk minimization (`1-ERM) problem. For example, if bj ‚àà R and fj (x) = 1
2
(bj ‚àí a
T
j x)
2
,
(1) is Lasso; if bj ‚àà {‚àí1, 1} and fj (x) = log(1 + exp(‚àíbja
T
j x)), `1-regularized logistic regression
is obtained.
In the above nonsmooth case, the Gauss-Southwell rule has 3 different variants [17, 22]: GS-s, GS-r
and GS-q. The GCD algorithm with all the 3 rules can be viewed as the following procedure: in
each iteration based on a quadratic approximation of f(x) in (1), one minimizes a surrogate objective
function under the constraint that the direction vector used for update has at most 1 nonzero entry.
The resulted problems under the 3 rules are easy to solve but are nonconvex due to the cardinality
constraint of direction vector. While when using Nesterov‚Äôs acceleration scheme, convexity is needed
for the derivation of the optimal convergence rate O(
p
1/) [16]. Therefore, it is impossible to
accelerate GCD by the Nesterov‚Äôs acceleration scheme under the 3 existing rules.
In this paper, we propose a novel variant of Gauss-Southwell rule by using an `1-norm square
approximation of f(x) rather than quadratic approximation. The new rule involves an `1-regularized
`1-norm square approximation problem, which is nontrivial to solve but is convex. To exactly
solve the challenging problem, we propose an efficient SOft ThreshOlding PrOjection (SOTOPO)
algorithm. The SOTOPO algorithm has O(d + |Q| log |Q|) cost, where it is often the case |Q|  d.
The complexity result O(d + |Q| log |Q|) is better than O(d log d) of its counterpart SOPOPO [18],
which is an Euclidean projection method.
Then based on the new rule and SOTOPO, we accelerate GCD to attain the optimal convergence rate
O(
p
1/) by combing a delicately selected mirror descent step. Meanwhile, we show that it is not
necessary to compute full gradient beforehand: sampling one training example and computing a noisy
gradient rather than full gradient is enough to perform greedy selection. This stochastic optimization
technique reduces the iteration complexity of greedy selection by a factor of the sample size. The
final result is an accelerated stochastic greedy coordinate descent (ASGCD) algorithm.
Assume x
‚àó
is an optimal solution of (1). Assume that each fj (x)(for all j ‚àà [n]) is Lp-smooth w.r.t.
k ¬∑ kp (p = 1, 2), i.e., for all x, y ‚àà R
d
,
k‚àáfj (x) ‚àí ‚àáfj (y)kq ‚â§ Lpkx ‚àí ykp, (2)
where if p = 1, then q = ‚àû; if p = 2, then q = 2.
In order to find an x that satisfies F(x) ‚àí F(x
‚àó
) ‚â§ , ASGCD needs O
 ‚àö
CL1kx
‚àó
‚àö
k1


iterations (see
(16)), where C is a function of d that varies slowly over d and is upper bounded by log2
(d). For
high-dimensional and dense problems with sparse solutions, ASGCD has better performance than the
state of the art. Experiments demonstrate the theoretical result.
Notations: Let [d] denote the set {1, 2, . . . , d}. Let R+ denote the set of nonnegative real number. For
x ‚àà R
d
, let kxkp = (Pd
i=1 |xi
|
p
)
1
p (1 ‚â§ p < ‚àû) denote the `p-norm and kxk‚àû = maxi‚àà[d]
|xi
|
denote the `‚àû-norm of x. For a vector x, let dim(x) denote the dimension of x; let xi denote the i-th
element of x. For a gradient vector ‚àáf(x), let ‚àáif(x) denote the i-th element of ‚àáf(x). For a set
S, let |S| denote the cardinality of S. Denote the simplex 4d = {Œ∏ ‚àà R
d
+ :
Pd
i=1 Œ∏i = 1}.
2
2 The SOTOPO algorithm
The proposed SOTOPO algorithm aims to solve the proposed new rule, i.e., minimize the following
`1-regularized `1-norm square approximation problem,
hÀú
def = arg min
g‚ààRd

h‚àáf(x), gi +
1
2Œ∑
kgk
2
1 + Œªkx + gk1

, (3)
xÀú
def = x + h, Àú (4)
where x denotes the current iteration, Œ∑ a step size, g the variable to optimize, hÀú the director vector for
update and xÀú the next iteration. The number of nonzero entries of hÀú denotes how many coordinates
will be updated in this iteration. Unlike the quadratic approximation used in GS-s, GS-r and GS-q
rules, in the new rule the coordinate(s) to update is implicitly selected by the sparsity-inducing
property of the `1-norm square kgk
2
1
rather than using the cardinality constraint kgk0 ‚â§ 1 (i.e., g
has at most 1 nonzero element) [17, 22]. By [6, ¬ß9.4.2], when the nonsmooth term Œªkx + gk1 in (1)
does not exist, the minimizer of the `1-norm square approximation (i.e., `1-norm steepest descent)
is equivalent to GCD. When Œªkx + gk1 exists, generally, there may be one or more coordinates to
update in this new rule. Because of the sparsity-inducing property of kgk
2
1
and kx + gk1, both the
direction vector hÀú and the iterative solution xÀú are sparse. In addition, (3) is an unconstrained problem
and thus is feasible.
2.1 A variational reformulation and its properties
(3) involves the nonseparable, nonsmooth term kgk
2
1
and the nonsmooth term kx + gk1. Because
there are two nonsmooth terms, it seems difficult to solve (3) directly. While by the variational
identity kgk
2
1 = infŒ∏‚àà4d
Pd
i=1
g
2
i
Œ∏i
in [4]
2
, in Lemma 1, it is shown that we can transform the
original nonseparable and nonsmooth problem into a separable and smooth optimization problem on
a simplex.
Lemma 1. By defining
J(g, Œ∏)
def
= h‚àáf(x), gi +
1
2Œ∑
X
d
i=1
g
2
i
Œ∏i
+ Œªkx + gk1, (5)
gÀú(Œ∏)
def = arg ming‚ààRd J(g, Œ∏), J(Œ∏)
def
= J(Àúg(Œ∏), Œ∏), (6)
ÀúŒ∏
def = arg infŒ∏‚àà4d J(Œ∏), (7)
where gÀú(Œ∏) is a vector function. Then the minimization problem to find hÀú in (3) is equivalent to the
problem (7) to find ÀúŒ∏ with the relation hÀú = Àúg(
ÀúŒ∏). Meanwhile, gÀú(Œ∏) and J(Œ∏) in (6) are both coordinate
separable with the expressions
‚àÄi ‚àà [d], gÀúi(Œ∏) = Àúgi(Œ∏i)
def
= sign(xi ‚àí Œ∏iŒ∑‚àáif(x)) ¬∑ max{0, |xi ‚àí Œ∏iŒ∑‚àáif(x)| ‚àí Œ∏iŒ∑Œª} ‚àí xi
, (8)
J(Œ∏) = X
d
i=1
Ji(Œ∏i), where Ji(Œ∏i)
def
= ‚àáif(x) ¬∑ gÀúi(Œ∏i) + 1
2Œ∑
X
d
i=1
gÀú
2
i
(Œ∏i)
Œ∏i
+ Œª|xi + Àúgi(Œ∏i)|. (9)
In Lemma 1, (8) is obtained by the iterative soft thresholding operator [5]. By Lemma 1, we can
reformulate (3) into the problem (5), which is about two parameters g and Œ∏. Then by the joint
convexity, we swap the optimization order of g and Œ∏. Fixing Œ∏ and optimizing with respect to (w.r.t.)
g, we can get a closed form of gÀú(Œ∏), which is a vector function about Œ∏. Substituting gÀú(Œ∏) into J(g, Œ∏),
we get the problem (7) about Œ∏. Finally, the optimal solution hÀú in (3) can be obtained by hÀú = Àúg(
ÀúŒ∏).
The explicit expression of each Ji(Œ∏i) can be given by substituting (8) into (9). Because Œ∏ ‚àà 4d, we
have for all i ‚àà [d], 0 ‚â§ Œ∏i ‚â§ 1. In the following Lemma 2, it is observed that the derivate J
0
i
(Œ∏i) can
be a constant or have a piecewise structure, which is the key to deduce the SOTOPO algorithm.
2The infima can be replaced by minimization if the convention ‚Äú0/0 = 0‚Äù is used.
3
Lemma 2. Assume that for all i ‚àà [d], J
0
i
(0) and J
0
i
(1) have been computed. Denote ri1
def
=
‚àö
|xi|
‚àí2Œ∑J0
i
(0)
and ri2
def
= ‚àö
|xi|
‚àí2Œ∑J0
i
(1)
, then J
0
i
(Œ∏i) belongs to one of the 4 cases,
(case a) : J
0
i
(Œ∏i) = 0, 0 ‚â§ Œ∏i ‚â§ 1, (case b) : J
0
i
(Œ∏i) = J
0
i
(0) < 0, 0 ‚â§ Œ∏i ‚â§ 1,
(case c) : J
0
i
(Œ∏i) = (
J
0
i
(0), 0 ‚â§ Œ∏i ‚â§ ri1
‚àí
x
2
i
2Œ∑Œ∏2
i
, ri1 < Œ∏i ‚â§ 1
, (case d) : J
0
i
(Œ∏i) =
Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥
J
0
i
(0), 0 ‚â§ Œ∏i ‚â§ ri1
‚àí
x
2
i
2Œ∑Œ∏2
i
, ri1 < Œ∏i < ri2
J
0
i
(1), ri2 ‚â§ Œ∏i ‚â§ 1
.
Although the formulation of J
0
i
(Œ∏i) is complicated, by summarizing the property of the 4 cases in
Lemma 2, we have Corollary 1.
Corollary 1. For all i ‚àà [d] and 0 ‚â§ Œ∏i ‚â§ 1, if the derivate J
0
i
(Œ∏i) is not always 0, then J
0
i
(Œ∏i) is a
non-decreasing, continuous function with value always less than 0.
Corollary 1 shows that except the trivial (case a), for all i ‚àà [d], whichever J
0
i
(Œ∏i) belong to (case b),
(case c) or case (d), they all share the same group of properties, which makes a consistent iterative
procedure possible for all the cases. The different formulations in the four cases mainly have impact
about the stopping criteria of SOTOPO.
2.2 The property of the optimal solution
The Lagrangian of the problem (7) is
L(Œ∏, Œ≥, Œ∂)
def = J(Œ∏) + Œ≥
X
d
i=1
Œ∏i ‚àí 1

‚àí hŒ∂, Œ∏i, (10)
where Œ≥ ‚àà R is a Lagrange multiplier and Œ∂ ‚àà R
d
+ is a vector of non-negative Lagrange multipliers.
Due to the coordinate separable property of J(Œ∏) in (9), it follows that ‚àÇJ(Œ∏)
‚àÇŒ∏i
= J
0
i
(Œ∏i). Then the
KKT condition of (10) can be written as
‚àÄi ‚àà [d], J0
i
(Œ∏i) + Œ≥ ‚àí Œ∂i = 0, Œ∂iŒ∏i = 0, and X
d
i=1
Œ∏i = 1. (11)
By reformulating the KKT condition (11), we have Lemma 3.
Lemma 3. If (ÀúŒ≥, ÀúŒ∏, ÀúŒ∂) is a stationary point of (10), then ÀúŒ∏ is an optimal solution of (7). Meanwhile,
denote S
def
= {i :
ÀúŒ∏i > 0} and T
def
= {j :
ÀúŒ∏j = 0}, then the KKT condition can be formulated as
Ô£±
Ô£≤
Ô£≥
P
i‚ààS
ÀúŒ∏i = 1;
for all j ‚àà T, ÀúŒ∏j = 0;
for all i ‚àà S, Œ≥Àú = ‚àíJ
0
i
(
ÀúŒ∏i) ‚â• maxj‚ààT ‚àíJ
0
j
(0).
(12)
By Lemma 3, if the set S in Lemma 3 is known beforehand, then we can compute ÀúŒ∏ by simply
applying the equations in (12). Therefore finding the optimal solution ÀúŒ∏ is equivalent to finding the
set of the nonzero elements of ÀúŒ∏.
2.3 The soft thresholding projection algorithm
In Lemma 3, for each i ‚àà [d] with ÀúŒ∏i > 0, it is shown that the negative derivate ‚àíJ
0
i
(
ÀúŒ∏i) is equal to a
single variable Œ≥Àú. Therefore, a much simpler problem can be obtained if we know the coordinates of
these positive elements. At first glance, it seems difficult to identify these coordinates, because the
number of potential subsets of coordinates is clearly exponential on the dimension d. However, the
property clarified by Lemma 2 enables an efficient procedure for identifying the nonzero elements of
ÀúŒ∏. Lemma 4 is a key tool in deriving the procedure for identifying the non-zero elements of ÀúŒ∏.
Lemma 4 (Nonzero element identification). Let ÀúŒ∏ be an optimal solution of (7). Let s and t be two
coordinates such that J
0
s
(0) < J0
t
(0). If ÀúŒ∏s = 0, then ÀúŒ∏t must be 0 as well; equivalently, if ÀúŒ∏t > 0,
then ÀúŒ∏s must be greater than 0 as well.
4
Lemma 4 shows that if we sort u
def = ‚àí‚àáJ(0) such that ui1 ‚â• ui2 ‚â• ¬∑ ¬∑ ¬∑ ‚â• uid
, where {i1, i2, . . . , id}
is a permutation of [d], then the set S in Lemma 3 is of the form {i1, i2, . . . , i%}, where 1 ‚â§ % ‚â§ d.
If % is obtained, then we can use the fact that for all j ‚àà [%],
‚àíJ
0
ij
(
ÀúŒ∏ij
) = ÀúŒ≥ and X%
j=1
ÀúŒ∏ij = 1 (13)
to compute Œ≥Àú. Therefore, by Lemma 4, we can efficiently identify the nonzero elements of the optimal
solution ÀúŒ∏ after a sort operation, which costs O(d log d). However based on Lemmas 2 and 3, the sort
cost O(d log d) can be further reduced by the following Lemma 5.
Lemma 5 (Efficient identification). Assume ÀúŒ∏ and S are given in Lemma 3. Then for all i ‚àà S,
‚àíJ
0
i
(0) ‚â• max
j‚àà[d]
{‚àíJ
0
j
(1)}. (14)
By Lemma 5, before ordering u, we can filter out all the coordinates i‚Äôs that satisfy ‚àíJ
0
i
(0) <
maxj‚àà[d] ‚àíJ
0
j
(1). Based on Lemmas 4 and 5, we propose the SOft ThreshOlding PrOjection
(SOTOPO) algorithm in Alg. 1 to efficiently obtain an optimal solution ÀúŒ∏. In the step 1, by Lemma 5,
we find the quantity vm, im and Q. In the step 2, by Lemma 4, we sort the elements {‚àíJ
0
i
(0)| i ‚àà Q}.
In the step 3, because S in Lemma 3 is of the form {i1, i2, . . . , i%}, we search the quantity œÅ from
1 to |Q| + 1 until a stopping criteria is met. In Alg. 1, the number of nonzero elements of ÀúŒ∏ is œÅ or
œÅ ‚àí 1. In the step 4, we compute the Œ≥Àú in Lemma 3 according to the conditions. In the step 5, the
optimal ÀúŒ∏ and the corresponding h, Àú xÀú are given.
Algorithm 1 xÀú =SOTOPO(‚àáf(x), x, Œª, Œ∑)
1. Find
(vm, im)
def = (maxi‚àà[d]{‚àíJ
0
i
(1)}, arg maxi‚àà[d]{‚àíJ
0
i
(1)}), Q def = {i ‚àà [d]| ‚àí J
0
i
(0) > vm}.
2. Sort {‚àíJ
0
i
(0)| i ‚àà Q} such that ‚àíJ
0
i1
(0) ‚â• ‚àíJ
0
i2
(0) ‚â• ¬∑ ¬∑ ¬∑ ‚â• ‚àíJ
0
i|Q|
(0), where
{i1, i2, . . . , i|Q|} is a permutation of the elements in Q. Denote
v
def = (‚àíJ
0
i1
(0), ‚àíJ
0
i2
(0), . . . , ‚àíJ
0
i|Q|
(0), vm), and i|Q|+1
def = im, v|Q|+1
def = vm.
3. For j ‚àà [|Q| + 1], denote Rj = {ik|k ‚àà [j]}. Search from 1 to |Q| + 1 to find the quantity
œÅ
def = min 
j ‚àà [|Q| + 1]| J
0
ij
(0) = J
0
ij
(1) or X
l‚ààRj
|xl
| ‚â• p
2Œ∑vj or j = |Q| + 1	
.
4. The Œ≥Àú in Lemma 3 is given by
Œ≥Àú =
(P
l‚ààRœÅ‚àí1
|xl
|
2
/(2Œ∑), if P
l‚ààRœÅ‚àí1
|xl
| ‚â• p
2Œ∑vœÅ;
vœÅ, otherwise.
5. Then the ÀúŒ∏ in Lemma 3 and its corresponding h, Àú xÀú in (3) and (4) are obtained by
(
ÀúŒ∏l
, hÀú
l
, xÀúl) =
Ô£±
Ô£¥Ô£≤
Ô£¥Ô£≥
 |xl|
‚àö
2Œ∑Œ≥Àú
, ‚àíxl
, 0

, if l ‚àà RœÅ\{iœÅ};

1 ‚àí
P
k‚àà RœÅ\{iœÅ}
ÀúŒ∏k, gÀúl(
ÀúŒ∏l), xl + Àúgl(
ÀúŒ∏l)

, if l = iœÅ;
(0, 0, xl), if l ‚àà [d]\RœÅ.
In Theorem 1, we give the main result about the SOTOPO algorithm.
Theorem 1. The SOTOPO algorihtm in Alg. 1 can get the exact minimizer h, Àú xÀú of the `1-regularized
`1-norm square approximation problem in (3) and (4).
The SOTOPO algorithm seems complicated but is indeed efficient. The dominant operations in Alg.
1 are steps 1 and 2 with the total cost O(d + |Q| log |Q|). To show the effect of the complexity
reduction by Lemma 5, we give the following fact.  
Proposition 1. For the optimization problem defined in (5)-(7), where Œª is the regularization parameter of the original problem (1), we have that
0 ‚â§ max
i‚àà[d]
(s
‚àí2J
0
i
(0)
Œ∑
)
‚àí max
j‚àà[d]
Ô£±
Ô£≤
Ô£≥
s
‚àí2J
0
j
(1)
Œ∑
Ô£º
Ô£Ω
Ô£æ
‚â§ 2Œª. (15)
Assume vm is defined in the step 1 of Alg. 1. By Proposition 1, for all i ‚àà Q,
s
‚àí2J
0
i
(0)
Œ∑
‚â§ max
k‚àà[d]
(s
‚àí2J
0
k
(0)
Œ∑
)
‚â§ max
j‚àà[d]
Ô£±
Ô£≤
Ô£≥
s
‚àí2J
0
j
(1)
Œ∑
Ô£º
Ô£Ω
Ô£æ
+ 2Œª =
r
2vm
Œ∑
+ 2Œª,
Therefore at least the coordinates j‚Äôs that satisfy q‚àí2J
0
j
(0)
Œ∑ >
q2vm
Œ∑ + 2Œª will be not contained in
Q. In practice, it can considerably reduce the sort complexity.
Remark 1. SOTOPO can be viewed as an extension of the SOPOPO algorithm [18] by changing the
objective function from Euclidean distance to a more general function J(Œ∏) in (9). It should be noted
that Lemma 5 does not have a counterpart in the case that the objective function is Euclidean distance
[18]. In addition, some extension of randomized median finding algorithm [10] with linear time in
our setting is also deserved to research. Due to the limited space, it is left for further discussion.
3 The ASGCD algorithm
Now we can come back to our motivation, i.e., accelerating GCD to obtain the optimal convergence
rate O(1/
‚àö
) by Nesterov‚Äôs acceleration and reducing the complexity of greedy selection by stochastic optimization. The main idea is that although like any (block) coordinate descent algorithm, the
proposed new rule, i.e., minimizing the problem in (3), performs update on one or several coordinates,
it is a generalized proximal gradient descent problem based on `1-norm. Therefore this rule can be
applied into the existing Nesterov‚Äôs acceleration and stochastic optimization framework ‚ÄúKatyusha‚Äù
[1] if it can be solved efficiently. The final result is the accelerated stochastic greedy coordinate
descent (ASGCD) algorithm, which is described in Alg. 2.
Algorithm 2 ASGCD
Œ¥ = log(d) ‚àí 1 ‚àí
p
(log(d) ‚àí 1)2 ‚àí 1;
p = 1 + Œ¥, q =
p
p‚àí1
, C =
d
2Œ¥
1+Œ¥
Œ¥
;
z0 = y0 = Àúx0 = œë0 = 0;
œÑ2 =
1
2
, m = d
n
b
e, Œ∑ =
1
(1+2 n‚àíb
b(n‚àí1) )L1
;
for s = 0, 1, 2, . . . , S ‚àí 1, do
1. œÑ1,s =
2
s+4 , Œ±s =
Œ∑
œÑ1,sC
;
2. ¬µs = ‚àáf(Àúxs);
3. for l = 0, 1, . . . , m ‚àí 1, do
(a) k = (sm) + l;
(b) randomly sample a mini batch B of size b from {1, 2, . . . , n} with equal probability;
(c) xk+1 = œÑ1,szk + œÑ2xÀús + (1 ‚àí œÑ1,s ‚àí œÑ2)yk;
(d) ‚àáÀú
k+1 = ¬µs +
1
b
P
j‚ààB(‚àáfj (xk+1) ‚àí ‚àáfj (Àúxs));
(e) yk+1 =SOTOPO(‚àáÀú
k+1, xk+1, Œª, Œ∑);
(f) (zk+1, œëk+1) = pCOMID(‚àáÀú
k+1, œëk, q, Œª, Œ±s);
end for
4. xÀús+1 =
1
m
Pm
l=1 ysm+l
;
end for
Output: xÀúS
6
Algorithm 3 (Àúx, œëÀú) = pCOMID(g, œë, q, Œª, Œ±)
1. ‚àÄi ‚àà [d], œëÀú
i = sign(œëi ‚àí Œ±gi) ¬∑ max{0, |œëi ‚àí Œ±gi
| ‚àí Œ±Œª};
2. ‚àÄi ‚àà [d], xÀúi =
sign(œëÀúi)|Œ∏Àúi|
q‚àí1
kœëÀúk
q‚àí2
q
;
3. Output: x, Àú œëÀú.
In Alg. 2, the gradient descent step 3(e) is solved by the proposed SOTOPO algorithm, while the
mirror descent step 3(f) is solved by the COMID algorithm with p-norm divergence [11, Sec. 7.2].
We denote the mirror descent step as pCOMID in Alg. 3. All other parts are standard steps in the
Katyusha framework except some parameter settings. For example, instead of the custom setting
p = 1 + 1/log(d) [19, 11], a particular choice p = 1 + Œ¥ (Œ¥ is defined in Alg. 2) is used to minimize
the C =
d
2Œ¥
1+Œ¥
Œ¥
. C varies slowly over d and is upper bounded by log2
(d). Meanwhile, Œ±k+1 depends
on the extra constant C. Furthermore, the step size Œ∑ =
1
(1+2 n‚àíb
b(n‚àí1) )L1
is used, where L1 is defined
in (2). Finally, unlike [1, Alg. 2], we let the batch size b as an algorithm parameter to cover both the
stochastic case b < n and the deterministic case b = n. To the best of our knowledge, the existing
GCD algorithms are deterministic, therefore by setting b = n, we can compare with the existing
GCD algorithms better.
Based on the efficient SOTOPO algorithm, ASGCD has nearly the same iteration complexity with
the standard form [1, Alg. 2] of Katyusha. Meanwhile we have the following convergence rate.
Theorem 2. If each fj (x)(j ‚àà [n]) is convex, L1-smooth in (2) and x
‚àó
is an optimum of the
`1-regularized problem (1), then ASGCD satisfies
E[F(Àúx
S
)] ‚àí F(x
‚àó
) ‚â§
4
(S + 3)2

1 +
1 + 2Œ≤(b)
2m
C

L1kx
‚àó
k
2
1 = O

CL1kx
‚àók
2
1
S2

, (16)
where Œ≤(b) = n‚àíb
b(n‚àí1) , S, b, m and C are given in Alg. 2. In other words, ASGCD achieves an
-additive error (i.e., E[F(Àúx
S)] ‚àí F(x
‚àó
) ‚â§  ) using at most O
 ‚àö
CL1kx
‚àó
‚àö
k1


iterations.
In Table 1, we give the convergence rate of the existing algorithms and ASGCD to solve the `1-
regularized problem (1). In the first column, ‚ÄúAcc‚Äù and ‚ÄúNon-Acc‚Äù denote the corresponding
algorithms are Nesterov‚Äôs accelerated or not respectively, ‚ÄúPrimal‚Äù and ‚ÄúDual‚Äù denote the corresponding algorithms solves the primal problem (1) and its regularized dual problem [20] respectively,
`2-norm and `1-norm denote the theoretical guarantee is based on `2-norm and `1-norm respectively.
In terms of `2-norm based guarantee, Katyusha and APPROX give the state of the art convergence rate
O
 ‚àö
L2kx
‚àó
‚àö
k2


. In terms of `1-norm based guarantee, GCD gives the state of the art convergence rate
O(
L1kxk
2
1

), which is only applicable for the smooth case Œª = 0 in (1). When Œª > 0, the generalized
GS-r, GS-s and GS-q rules generally have worse theoretical guarantee than GCD [17]. While the
bound of ASGCD in this paper is O(
‚àö
L1kxk1 log d
‚àö

), which can be viewed as an accelerated version
of the `1-norm based guarantee O(
L1kxk
2
1

). Meanwhile, because the bound depends on kx
‚àók1 rather
than kx
‚àók2 and on L1 rather than L2 (L1 and L2 are defined in (2)), for the `1-ERM problem, if the
samples are high-dimensional, dense and the regularization parameter Œª is relatively large, then it is
possible that L1  L2 (in the extreme case, L2 = dL1 [9]) and kx
‚àók1 ‚âà kx
‚àók2. In this case, the
`1-norm based guarantee O(
‚àö
L1kxk1 log d
‚àö

) of ASGCD is better than the `2-norm based guarantee
O
 ‚àö
L2kx
‚àó
‚àö
k2


of Katyusha and APPROX. Finally, whether the log d factor in the bound of ASGCD
(which also appears in the COMID [11] analysis) is necessary deserves further research.
Remark 2. When the batch size b = n, ASGCD is a deterministic algorithm. In this case, we can use
a better smooth constant T1 that satisfies k‚àáf(x) ‚àí ‚àáf(y)k‚àû ‚â§ T1kx ‚àí yk1 rather than L1 [1].
Remark 3. The necessity of computing the full gradient beforehand is the main bottleneck of GCD in
applications [17]. There exists some work [9] to avoid the computation of full gradient by performing
some approximate greedy selection. While the method in [9] needs preprocessing, incoherence
7
Table 1: Convergence rate on `1-regularized empirical risk minimization problems. (For GCD, the
convergence rate is applied for Œª = 0. )
ALGORITHM TYPE PAPER CONVERGENCE RATE
NON-ACC, PRIMAL, `2-NORM SAGA [8] O

L2kx
‚àók
2
2


ACC, PRIMAL, `2-NORM KATYUSHA [1] O
 ‚àöL2kx
‚àó
‚àö
k2


ACC, ACC-SDCA [21]
O
 ‚àöL2kx
‚àó
‚àö
k2

log( 1

)
 DUAL, SPDC [24]
`2-NORM APCG [14]
APPROX [12]
NON-ACC, PRIMAL, `1-NORM GCD [2] O

L1kx
‚àók
2
1


ACC, PRIMAL, `1-NORM ASGCD (THIS PAPER) O
 ‚àöL1kx
‚àók1 log d ‚àö

condition for dataset and is somewhat complicated. Contrary to [9], the proposed ASGCD algorithm
reduces the complexity of greedy selection by a factor up to n in terms of the amortized cost by simply
applying the existing stochastic variance reduction framework.
4 Experiments
In this section, we use numerical experiments to demonstrate the theoretical results in Section 3
and show the empirical performance of ASGCD with batch size b = 1 and its deterministic version
with b = n (In Fig. 1 they are denoted as ASGCD (b = 1) and ASGCD (b = n) respectively). In
addition, following the claim to using data access rather than CPU time [19] and the recent SGD
and RCD literature [13, 14, 1], we use the data access, i.e., the number of times the algorithm
accesses the data matrix, to measure the algorithm performance. To show the effect of Nesterov‚Äôs
acceleration, we compare ASGCD (b = n) with the non-accelerated greedy coordinate descent
with GS-q rule, i.e., coordinate gradient descent (CGD) [22]. To show the effect of both Nesterov‚Äôs
acceleration and stochastic optimization strategies, we compare ASGCD (b = 1) with Katyusha
[1, Alg. 2]. To show the effect of the proposed new rule in Section 2, which is based on `1-norm
square approximation, we compare ASGCD (b = n) with the `2-norm based proximal accelerated
full gradient (AFG) implemented by the linear coupling framework [3]. Meanwhile, as a benchmark
of stochastic optimization for the problems with finite-sum structure, we also show the performance
of proximal stochastic variance reduced gradient (SVRG) [23]. In addition, based on [1] and our
experiments, we find that ‚ÄúKatyusha‚Äù [1, Alg. 2] has the best empirical performance in general for
the `1-regularized problem (1). Therefore other well-known state-of-art algorithms, such as APCG
[14] and accelerated SDCA [21], are not included in the experiments.
The datasets are obtained from LIBSVM data [7] and summarized in Table 2. All the algorithms are
used to solve the following lasso problem
min
x‚ààRd
{f(x) + Œªkxk1 =
1
2n
kb ‚àí Axk
2
2 + Œªkxk1} (17)
on the 3 datasets, where A = (a1, a2, . . . , an)
T = (h1, h2, . . . , hd) ‚àà R
n√ód with each aj ‚àà R
d
representing a sample vector and hi ‚àà R
n representing a feature vector, b ‚àà R
n is the prediction
vector.
Table 2: Characteristics of three real datasets.
DATASET NAME # SAMPLES n # FEATURES d
LEUKEMIA 38 7129
GISETTE 6000 5000
MNIST 60000 780
For ASGCD (b = 1) and Katyusha [1, Alg. 2], we can use the tight smooth constant L1 =
maxj‚àà[n],i‚àà[d]
|a
2
j,i| and L2 = maxj‚àà[n] kajk
2
2
respectively in their implementation. While for AS8
Œª Leu Gisette Mnist
10‚àí2
0 1 2 3 4 5
Number of Passes √ó10 4
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log loss
CGD
AFG
ASGCD (b=n)
SVRG
Katyusha
ASGCD (b=1)
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of Passes
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
0 200 400 600 800 1000 1200 1400 1600 1800 2000
Number of Passes
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
10‚àí6
0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Number of Passes
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
0 1 2 3 4 5 6 7 8 9 10
Number of Passes √ó104
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 5
Number of Passes √ó104
-20
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
Log Loss
Figure 1: Comparing AGCD (b = 1) and ASGCD (b = n) with CGD, SVRG, AFG and Katyusha on
Lasso.
GCD (b = n) and AFG, the better smooth constant T1 =
maxi‚àà[d] khik
2
2
n
and T2 =
kAk
2
n
are used respectively. The learning rate of CGD and SVRG are tuned in {10‚àí6
, 10‚àí5
, 10‚àí4
, 10‚àí3
, 10‚àí2
, 10‚àí1}.
Table 3: Factor rates of for the 6 cases
Œª LEU GISETTE MNIST
10‚àí2
(0.85, 1.33) (0.88, 0.74) (5.85, 3.02)
10‚àí6
(1.45, 2.27) (3.51, 2.94) (5.84, 3.02)
We use Œª = 10‚àí6
and Œª = 10‚àí2
in the experiments. In addition, for each case (Dataset, Œª), AFG is
used to find an optimum x
‚àó with enough accuracy.
The performance of the 6 algorithms is plotted in Fig. 1. We use Log loss log(F(xk) ‚àí F(x
‚àó
)) in the
y-axis. x-axis denotes the number that the algorithm access the data matrix A. For example, ASGCD
(b = n) accesses A once in each iteration, while ASGCD (b = 1) accesses A twice in an entire outer
iteration. For each case (Dataset, Œª), we compute the rate (r1, r2) =  ‚àö
CL1kx
‚àó
‚àö
k1
L2kx‚àók2
,
‚àö
CT1kx
‚àó
‚àö
k1
T2kx‚àók2

in Table 3. First, because of the acceleration effect, ASGCD (b = n) are always better than the
non-accelerated CGD algorithm; second, by comparing ASGCD(b = 1) with Katyusha and ASGCD
(b = n) with AFG, we find that for the cases (Leu, 10‚àí2
), (Leu, 10‚àí6
) and (Gisette, 10‚àí2
), ASGCD
(b = 1) dominates Katyusha [1, Alg.2] and ASGCD (b = n) dominates AFG. While the theoretical
analysis in Section 3 shows that if r1 is relatively small such as around 1, then ASGCD (b = 1)
will be better than [1, Alg.2]. For the other 3 cases, [1, Alg.2] and AFG are better. The consistency
between Table 3 and Fig. 1 demonstrates the theoretical analysis.