Recognizing human emotions based on electroencephalogram (EEG) signals has received a great deal of attentions. Most of the existing studies focused on offline analysis, and real-time emotion recognition using a brain computer interface (BCI) approach remains to be further investigated. In this paper, we proposed an EEG-based BCI system for emotion recognition. Specifically, two classes of video clips that represented positive and negative emotions were presented to the subjects one by one, while the EEG data were collected and processed simultaneously, and instant feedback was provided after each clip. Ten healthy subjects participated in the experiment and achieved a high average online accuracy of 91.5 ± 6.34 percent. The experimental results demonstrated that the subjects emotions had been sufficiently evoked and efficiently recognized by our system. Clinically, patients with disorder of consciousness (DOC), such as coma, vegetative state, minimally conscious state and emergence minimally conscious state, suffer from motor impairment and generally cannot provide adequate emotion expressions. Consequently, doctors have difficulty in detecting the emotional states of these patients. Therefore, we applied our emotion recognition BCI system to patients with DOC. Eight DOC patients participated in our experiment, and three of them achieved significant online accuracy. The experimental results show that the proposed BCI system could be a promising tool to detect the emotional states of patients with DOC.
SECTION 1Introduction
Brain injuries or cerebral hemorrhages may cause the disorder of consciousness (DOC) including coma, vegetative state (VS), minimal conscious state (MCS), and emergence from MCS (EMCS). Clinically, behavioral scales, such as the Coma Recovery Scale-Revised (CRS-R) scale, are widely used in the diagnosis of patients with DOC. These behavioral scales rely on the patients’ behavioral responses to external stimuli [1]. However, the behavioral response based methods may result in high misdiagnosis rates since these patients suffer from sever motor impairments and generally cannot provide adequate behavioral expressions [2], [3]. Recently, researchers have employed the technology of brain computer interface (BCI) to detect residual consciousness including command following, communication ability and number recognition in patients with DOC [4], [5], [6].

Emotion plays an important role in the daily lives of human beings [7]. As an important brain function, emotion is related to many cognitive functions, such as language comprehension, decision making and selective attention [8], [9], [10]. In recent years, the study of emotion recognition/detection has received increasing attentions [7]. Many methods have been proposed to recognize human emotions. One class of methods are to detect emotional responses using audio and visual features, such as speeches, facial expressions, and body gestures [7], [11]. Another class of methods are to recognize emotional states from physiological signals, for example, via electroencephalogram (EEG), electromyogram (EMG), and heart rate measurement [12], [13]. Among various available signals for recognizing emotional states, EEG has been widely used because it provides a noninvasive and intuitive way to measure emotions [14], [15]. Until now, no study has considered emotion of patients with DOC using a BCI approach. Whether the patients with DOC own the emotion is unclear. Since there are strong connections between emotion and consciousness, the research of emotion recognition in patients with DOC may help us assess their residual consciousness and the impaired brain functions [16], [17]. However, there is not any effective method for doctors to detect the emotional states of patients with DOC since these patients suffer from severe motor impairments and generally cannot provide adequate emotion expressions.

In this study, we proposed a real-time EEG-based BCI system to recognize the emotions of patients with DOC, which were evoked by video clips. Two classes of video clips that represent positive and negative emotions were played in a random order on a screen, and the subjects were guided to focus on them. First, a calibration run was conducted to train an SVM model. During the process of real-time emotion recognition, the EEG data were collected and processed simultaneously. Specifically, the feature vectors were extracted and fed to the SVM classifier. The recognition results were displayed on the screen as feedback for each video clip. We first validated our BCI system in healthy subjects. Ten healthy subjects participated in our experiment and achieved an average online accuracy of 91.5 ± 6.34 percent, which indicated that the subjects’ emotions had been sufficiently evoked and efficiently recognized, and thus verified the efficiency of our BCI system. Next, we applied our EEG-based emotion BCI system to patients with DOC. Eight patients with DOC (1 VS, 6 MCS, and 1 EMCS) participated in our online experiments and three of them achieved significant online accuracies. The results showed that the emotions of these three patients had been evoked and recognized successfully. Therefore, our BCI system provides a potential way to detect the emotional states of patients with DOC.

The remaining sections of this paper are organized as follows. A brief overview of related works on EEG-based emotion recognition and applications of BCIs in patients with DOC are presented in Section 2. Section 3 describes the methodologies, including subjects, data acquisition, stimulus materials, graphical user interface (GUI) and experimental procedure, online signal processing, and offline data analysis. The experiments and results are presented in Section 4. The discussion and conclusion are available in Sections 5 and 6, respectively.

SECTION 2Related Work
2.1 EEG-Based Emotion Recognition
Although no study has focused on the emotion recognition in patients with DOC, there are lots of researches based on healthy people [18], [19], [20], [21], [22]. By considering its real-time capability, an EEG-based recognition method can be discriminated as an offline or an online one. Furthermore, the stimulus materials that can induce specific emotions include pictures, music, and videos [23], [24], [25]. Hidalgo et al. established an offline EEG-based algorithm to recognize users emotional states while viewing pictures. Using a spectral turbulence measure and a support vector machine-recursive feature elimination (SVM-RFE) algorithm, they achieved an accuracy of 80.77 percent for two emotional classes (positive valence versus negative valence) [26]. Lin et al. categorized users status into four emotional states (joy, anger, sadness, and pleasure) while listening to music. In their offline analysis, an average accuracy of 82.29 ± 3.06 percent was obtained by using asymmetric spectral features [21]. Soleymani et al. proposed an offline subject-independent emotion recognition method using multimodal signals, including EEG, pupillary responses, and gaze distances, which were collected when videos were presented to the subjects [27]. They compared different fusion strategies and found that the method of decision level fusion achieved the best recognition accuracy of 76.4 percent for three arousal levels. To identify the stability of neural patterns over time, Zheng and colleagues proposed an offline approach to recognize subjects emotional responses to videos by collecting EEG data from different days. Using the differential entropy (DE) features and a discriminative Graph regularized Extreme Learning Machine (GELM), a mean recognition accuracy of 79.28 percent was achieved for three emotional states (positive, neutral, and negative) [22]. It is thus clear that the accuracy of the emotion recognition needs to be improved.

Several online EEG-based emotion recognition systems have been reported. Liu et al. proposed a real-time fractal dimension based algorithm to classify subjects emotional states in 2 valence levels and 3 arousal levels [28]. The authors further applied the algorithm to control a real-time music playing and a three dimensional (3D) emotional avatar in a virtual environment. Sourina et al. developed a real-time EEG-based emotion recognition system for music therapy [29]. Specifically, by using the fractal dimension values extracted from EEG signals and the music segments selected from International Affective Digitized Sounds (IADS) database, their system could recognize subjects emotions and perform music therapy. However, these two studies showed only the frameworks of the online systems, and the experimental results and system performances were not presented. More recently, Daly et al. reported an affective BCI system to detect the current affective states of the subjects while listening to music. Using band-power features and support vector machine (SVM) classifier, they achieved an online average accuracy of 53.96 percent for three arousal levels of eight subjects [30]. Above all, there need a great deal of efforts to develop real-time EEG-based emotion recognition BCI systems with high accuracies.

2.2 Applications of BCI on Patients with DOC
BCIs offer a potential way to probe the residual brain functions of patients with DOC [31], [32]. For instance, several BCI studies have been reported to assess the patients’ awareness including command following, communication ability and number recognition [4], [5], [6]. Goldfine et al. proposed an EEG-based motor imagery method to detect the awareness of VS patients [4]. Three of 16 patients showed repeated and reliable EEG responses to two commands. Lul and colleagues proposed a 4-choice auditory oddball BCI system to detect command following and communication abilities in patients with DOC [33]. Eighteen patients participated in the experiment, and one locked-in state patient was able to communicate using the BCI system. Coyle et al. designed a sensorimotor modulation BCI system to assess the awareness of MCS patients [34]. All 4 of the 4 participants demonstrated significant and appropriate brain responses, and the results showed that MCS patients might have the capacity to operate a simple BCI system. In our previous studies, we have designed several BCI paradigms to assess the residual awareness, number recognition, and communication abilities of patients with DOC [5], [6], [35]. However, none of the existing studies has developed an EEG-based emotion recognition system for patients with DOC.

SECTION 3Methods
3.1 Subjects
Ten students (H1 to H10, aged 24-35, mean age 26 years, 8 men) from South China University of Technology were first recruited to participant the experiment as a control group. All healthy subjects had normal or corrected-to-normal vision and normal hearing. Written informed consent was obtained from all subjects prior to the experiment. Next, eight DOC patients from a local hospital took part in our experiment (P1 to P8, aged 12-60, mean age 35 years, 6 men; see Table 4). The patients met the following criteria: the patient was a) in a stable condition, b) with normal visual and auditory ability before impairment, and c) no psychiatric medications were used within the prior two days. Written informed consent for the study was obtained from all of the participants or their legal surrogates. Table 1 shows the clinical data of these DOC patients. The experimental protocol in this study was approved by the Ethics Committee of the General Hospital of Guangzhou Military Command of Peoples Liberation Army, which complies with the Code of Ethics of the World Medical Association (Declaration of Helsinki).

TABLE 1 Summary of Clinical Status of Patients
Table 1- 
Summary of Clinical Status of Patients
3.2 Data Acquisition
In our system, EEG signals are recorded with the SynAmps2 amplifier (Compumedics, Neuroscan, Inc., Australia). A 32-channel EEG cap with electrodes placed according to the international 10-20 system is used. The right mastoid is set as the reference, and the ground electrode is positioned on the forehead. The EEG signals are amplified and sampled at 250 Hz. The impedances of all electrodes are maintained below 5 kΩ during data collection.

3.3 Stimulus Materials
In our experiment, native Chinese emotional clips were selected to quickly and effectively elicit specific emotional states (positive and negative) of subjects. The process for selecting video clips is described as follows: first, we collected 140 clips that contained positive or negative scenes from famous Chinese movies or crosstalk shows. Next, each clip was edited to approximately 30 seconds and the audio power levels of all clips were matched by adjusting the total power value. Then, ten volunteers (not the participants in the BCI experiment) were asked to assess their emotions with a level (i.e., not at all, slightly, or extremely) and a keywords (i.e., positive or negative) while watching the clips. Finally, 40 Chinese video clips that all volunteers scored as extremely positive or negative were chosen. Each emotion category (positive and negative) contained 20 clips.

Notably, to unify the definition of emotion, researchers have focused on two main methods: one is to classify emotions into discrete categories (i.e., anger, fear, disgust, happiness, sadness, or surprise) [36] and more complex emotions based on combinations of these basic categories; the other method is to define emotion in several continuous dimensions (i.e., valence, arousal, and dominance) [37]. In the present study, we chose two typical emotional states and used a uniform label for each emotional state for the following reasons: 1) the ratings of valence and arousal using Self-Assessment Manikin (SAM) have large variations between subjects, and the meaning of the scales is subjective [38]; 2) the process of scoring the emotions of patients with DOC, who lack the ability to perform normal physical movements and have limited consciousness in the experiment, is difficult [39], [40]; and 3) considering too many emotional states may increase the burden on the patients.

3.4 Graphical User Interface (GUI) and Experimental Paradigm
Before the experiment, each participant was informed about the procedure of the experiment and instructed to sit in a comfortable chair approximately 0.5m from a 22-in LED monitor. During the emotion recognition experiment, participants were asked to gaze at the screen and stay still as much as possible. Before the test run, the participants performed a calibration run of 20 trials to collect training data, and a SVM model was built based on the training data. The online test run also consisted of 20 trials. During the training or test run, each category of emotion included 10 trials and each trial contained one clip. Furthermore, all clips appeared in a random order.

The experimental paradigm for each trial is show in Fig. 1. Each trial begins with a 5 seconds cue to remind the participant whether the following clip is a positive or negative video clip. Participants are told to watch the video clip attentively and experience the emotions contained in it. The instruction is the following video is a positive/negative one, please be prepared. Then, the video clip that represents a positive/negative emotion, respectively, is played and the EEG data are collected and processed simultaneously. Then, the online recognition result is displayed on the screen as feedback. In this study, a smiling/crying cartoon face is presented as feedback, which represents the detection of a positive/negative emotion, respectively. The feedback lasts for 5 seconds, and is followed by 5 seconds of rest time.


Fig. 1.
Experimental protocol of our real-time emotion recognition system.

Show All

Notably, there are several differences between the experimental procedures for DOC patients compared with healthy subjects. Specifically, there are totally five sessions performed on separate days, each of which contained a calibration run of 10 trials and a test run of 10 trials. The procedure of each trial was similar to that for the healthy subjects. In addition, there was a break of at least 10 seconds between two adjacent trials and that could be adjusted depending on the patients state. We used these settings mainly due to the fact that the DOC patients easily fatigued.

3.5 Online Signal Processing
The online signal processing is designed to monitor a subjects emotional state in real-time and output the feedback. This process includes the following steps.

Preprocessing. In this study, we adopted a universal preprocessing method to reduce any disturbances of noises. The data baseline was first corrected by subtracting the mean value of the 1s signal before the stimulus start. The notch filter was first applied to remove the 50 Hz power-line noise. Then, the raw EEG data from each scalp electrode was filtered with a tenth order minimum-phase FIR bandpass filter between 0.1 to 70 Hz.

Feature Extraction. Based on the filtered trial, we extract the spectral feature vector corresponding to positive and negative emotions. First, we compute the spectral power changes of each channel using a 512-point Short Time Fourier Transform (STFT) with a non-overlapped Hanning window of 1 second. Next, the band-power values are calculated by averaging the power values in each of the following frequency bands: delta (1-3 Hz), theta (4-7 Hz), alpha (8-13 Hz), beta (14-30 Hz), and gamma (31-50 Hz). Previous studies have shown that the EEG spectral amplitude is more linearly in the logarithmic scale than in the linear scale [41], [42]. Therefore, to linearize the multiplicative effects, the averaged power spectrum of each epoch is converted to the logarithmic scale. Finally, the logarithmic power spectrums in each subband of all 30 channels are concatenated to obtain a 150 dimension feature vector.

In addition, the logarithmic power spectral density (PSD) is equal to the DE for a fixed length [43]. Previous studies have demonstrated the efficiency of the DE feature [24], [43]. Therefore, in this study, the logarithmic PSD feature is equal to the DE feature, and can be calculated using the following formula:
DE=log(|x(m,fk)|2),(1)
View Sourcewhere |x(m,fk)| denotes the absolute amplitude of the original signal after STFT. Note that all features were normalized to the range from 0 to 1 in each dimension before feeding to a classifier.

Model Training and Classification. The LIBSVM toolbox is used to establish the SVM classifier [44]. The linear kernel is selected as a kernel function and other parameters are set to default values. Using the training data from the calibration run, an SVM classifier is trained, in which the feature vectors corresponding to the positive and negative clips are labeled +1 and -1, respectively. The online feature vectors are fed into the trained SVM classifier, and the SVM scores are obtained. If the score of a feature vector is positive, then the system predicts a positive emotion and outputs a smiling cartoon face; otherwise, the system predicts a negative emotion and outputs a crying cartoon face.

3.6 Offline Data Analysis
Statistical Test. We first test the significance of the online accuracy. In our study, the accuracy is calculated as the ratio between the correct recognition trials and the total number of trials. To evaluate the significance of the accuracy, we used a chi-squared test as shown below [45]:
χ2=∑i=12(foi−fei)2fei,(2)
View SourceRight-click on figure for MathML and additional features.where foi is the observed number and fei is the expected number of the ith class (i=1,2). There were only two possible output states in our study and the degree of freedom was 1. For instance, each patient performed 50 online test trials, and 25 is the expected number for both emotional classes. Given a significance level of p =0.05, the value of χ2 is 3.84. Therefore, a significant accuracy for 50 trials is 64 percent.

Offline Classification. To effectively use the data, we combine the data from calibration run and test run. Therefore, the session of each healthy subject includes 40 trials, whereas each of 5 sessions for each DOC patient includes 20 trials. Furthermore, by randomly partitioning the data, we repeat a 5-fold cross-validation scheme for 10 times to improve the reliability of the results. Based on the 40 trials for each healthy subject, we can obtain a classification accuracy for each fold of a 5-fold cross-validation. For each 5-fold cross-validation, an average accuracy across 5 folds is calculated and the accuracies of the 10 cross-validations are further averaged. Similarly, for each session of a DOC patient, we can obtain an average accuracy through 10 times of the 5-fold cross-validation scheme. These accuracies are further averaged for 5 sessions. Note that the preprocessing, feature extraction and classification procedures are the same as those in the previous online method. To identify the importance of single subband and feature, the classification accuracies are also calculated for each of the five subbands (delta, theta, alpha, beta, and gamma) and each of 150 features, similarly as above.

SECTION 4Experiments and Results
Two experiments were designed in this study. First, ten healthy students from South China University of Technology were recruited to evaluate the emotion recognition performance of the proposed system. Next, eight patients with DOC from a local hospital participated in our experiment to illustrate the potential application of our system. Especially, for patients with DOC, the BCI experiment was conduct only if they were awake and free of centrally acting sedative drugs. During the whole experiment, doctors accompany the patients to prevent a sudden seizure. Before the online experiment, each patient performed a calibration run of 10 trials. To achieve more reliable results, the online experiment for each patient included five sessions, each of which contained 10 trials (five with positive emotion and five with negative emotion, which were presented in a random order). Because the patients were easily fatigued, the five online sessions were conducted on separate days, and the experiment times were arranged by the clinical doctors. The entire experimental period lasted approximately three months for healthy subjects and DOC patients.

4.1 Results for Healthy Subjects
Table 2 presents the online accuracies for all 10 healthy participants. Using our emotion recognition system, all of the healthy participants achieved online accuracies ranging from 80 to 100 percent. The average online accuracy of 91.5 ± 6.34 percent (p<0.001, χ2 test) was significantly higher than the random level of 50 percent. These results validated the fact that the emotions of healthy subjects were well evoked and recognized by the emotion recognition system.

TABLE 2 Accuracy Rates of Online Emotion Recognition for Healthy Subjects

Table 3 shows the offline classification results based on the features of different frequency bands. EEG features that concatenated all five subbands achieved the best classification performance, except for subject H8. Based on Table 3, even a single frequency band could achieve a satisfactory classification performance (the mean accuracy rate ranged from 74.38 to 87.0 percent). Furthermore, the theta band achieved the highest average accuracy in all five subbands. We performed a non-parametric Friedman test on the accuracies from different frequency bands. The Friedman test revealed significant effects on the classification accuracies of different frequency bands (χ2(5),p<0.05). The feature that concatenated all five subbands achieved significantly higher classification accuracies than the other bands (p<0.05, FDR corrected). The theta band significantly outperformed the alpha and beta bands (p<0.05, FDR corrected), but the diversity of classification accuracies for the other different bands was not significant.

TABLE 3 Offline Accuracy Rates (%) Based on the Features of Different Bands

To identify the most relevant frequency bands and electrode locations for emotional processing, we explored the classification performance of each single feature using a 5-fold cross-validation across the 10 healthy subjects. Table 4 lists the classification accuracies and electrodes of the top-20 features. In general, most of the top-20 features that achieved the highest classification performance belonged to the theta band, which is consistent with the data shown in Table 3 and further illustrates the importance of the theta band in emotion recognition.

TABLE 4 Top-20 Significant Features for Healthy Subjects Using Single Feature Classification

Furthermore, as shown in Fig. 2a, the classification weight of each electrode is the average of the weights of all five subbands. The weight values were extracted from the SVM training model and averaged using a 5-fold cross-validation scheme. We can clearly see that the left frontal areas correlated to positive emotion, while negative emotion was mainly processed in the right hemisphere. In addition, the reported frontal midline areas were associated with the process of positive emotion [46], and this response could be observed in our weight map.


Fig. 2.
Topographical map of (a) the classification weight and (b) the averaged DE features. Note that these maps were averaged across all subjects and trials.

Show All

To illustrate the neural patterns corresponding to positive and negative emotions, topographies were plotted based on different frequency bands. Fig. 2b depicts the average power changes for negative and positive emotions in the five bands (i.e., delta, theta, alpha, beta, and gamma). Specifically, the features were averaged across all ten healthy subjects and all trials. The neural patterns between negative and positive emotions were distinctly different. In the delta band, the right anterior areas were activated more for positive emotion than for negative emotion. Meanwhile, the left anterior region exhibits the opposite characteristics. In the theta band, the prefrontal regions and occipital lobe show higher power during positive emotional state than during negative emotional state. In the alpha band, the power decreased in the right frontal areas during negative emotion, whereas the power of the frontal areas increased during positive emotion. For the neural patterns in the beta and gamma bands, the power in the lateral temporal areas for positive emotion was significantly higher than that for negative emotion.

4.2 Results for Patients with DOC
Table 5 shows the online accuracies for patients with DOC, and three of the eight patients, i.e., P1, P4, and P6, achieved significant recognition accuracies (p<0.05, χ2 test). Among these patients, patient P4 achieved the highest online recognition accuracy of 72 percent. These results show that the proposed system could induce and recognize the positive and negative emotional states of patients with DOC.

TABLE 5 Accuracy Rates of Online Emotion Recognition for Patients with DOC
Table 5- 
Accuracy Rates of Online Emotion Recognition for Patients with DOC
As was done for healthy subjects, the classification performance of each single feature was explored, where the classification accuracy rates for top-20 significant features are shown in Table 6. To further illustrate the neural patterns of positive and negative emotional states for the patients, the topographic maps are presented. Because of the large individual differences between patients, we analyze the data of patients who achieved significant online accuracy rates (i.e., P1, P4, and P6).

TABLE 6 Top-20 Significant Features for Patients (P1, P4, and P6) Using Single Feature Classification

For patient P1, Table 6 lists the offline classification accuracies and locations of the top-20 features. Most of the top-20 features drop into the delta band, which differs from the healthy subjects. Unlike healthy subjects, the electrodes that contain the top-20 features were distributed in a more dispersed way. As shown in the topological graph Fig. 3a, the prefrontal, left temporal and parietal areas had greater activation in delta and theta bands during negative emotional state. In particular, an asymmetric phenomenon was observed in the alpha band in the prefrontal regions. In the alpha band, the left prefrontal areas were more active during negative emotional state, while the right frontal regions exhibited greater activation during positive emotional state.


Fig. 3.
Topographical maps of the average DE features for P1, P4, and P6 over five runs and trials for different emotions.

Show All

Patient P4 achieved the highest online accuracy among all patients, and the top-20 features were spread across the delta, theta and alpha bands, which is a similar result as the healthy subjects (see Table 6). The electrodes that correlated with the top-20 features were mainly located in the temporal lobe, central area and occipital lobe. For positive emotion, the frontal midline had a significant higher theta response. Meanwhile, as shown in Fig. 3b, the parietal and frontal areas were activated more in the alpha band in response to positive emotion. In the beta and gamma frequency bands, the occipital lobe presented greater activation for positive emotional state than negative emotional state.

For patient P6, most of the top-20 features were located in the delta and theta frequency bands (see Table 6). The distribution of electrodes that correlated with the top-20 features was similar to that of patient P1. From the topography of patient P6 (Fig. 3c), we can observe that the bilateral regions were more activated in the delta and theta band in response to negative emotion. In the alpha band, the prefrontal areas and parietal lobe regions exhibited greater activation for negative emotion than for positive emotion. Additionally, the occipital lobe, which is related to visual signal processing, was activated in beta and gamma band for both emotional states.

SECTION 5Discussions
In this study, a real-time EEG-based emotion recognition BCI system was developed, to recognize subjects positive and negative emotional states while they were watching video clips. We further applied this system to patients with DOC. Unlike the conventional offline method, our proposed system can evoke and recognize the emotional states of users in real-time, and the online output can be presented as feedback to enhance the systems efficacy. Our experimental results demonstrated the recognition efficiency of our BCI system and its potential application.

5.1 Effectiveness of the Emotion BCI System
An online average recognition accuracy of 91.5 ± 6.34 percent was achieved when applying our emotion BCI system to ten healthy subjects. The following factors might contribute to the high recognition performance of our BCI system. First, it is essential to sufficiently evoke the emotions of subjects. Therefore, selection of the stimulus materials plays an important role. The well-chosen native Chinese video clips used in this study might partially contribute to the high emotion recognition accuracy. Second, the online feedback might decrease the sense of boredom and encourage participants to focus more on the emotion recognition task. Furthermore, the online feedback could encourage subjects to adopt appropriate strategies to regulate their emotions.

The proper emotional patterns were validated in our study. First, as shown in Fig. 2a, positive emotion was mainly processed in the left frontal brain regions, while negative emotion was mainly processed in the right hemisphere; this asymmetric phenomenon is consistent with the findings of previous studies [46], [47]. The frontal midline areas, which are closely associated with positive emotion [48], can also be observed in Fig. 2a. Second, as listed in Table 3, the theta band plays an important role in the classification task, and it had the highest classification accuracy among all five subbands. Further exploration of single-feature classification performance showed that the top-20 features resulted mainly from the theta band (see Table 4). Finally, the EEG topographies presented in Fig. 2b depict the neural patterns in detail. Specifically, in the theta and alpha bands, the prefrontal and lateral temporal areas were more active for positive emotion than for negative emotion, and the lateral temporal areas exhibited greater activation in beta and gamma bands for positive emotion than for negative emotion. These patterns are consistent with those reported in previous studies [22], [48], [49]. These findings also proved that our high recognition accuracies were not result of EMG activities, which mainly converged in the high-frequency band [50]. Most importantly, the subjects’ emotions were successfully evoked and recognized.

5.2 Significance to Patients with DOC
In the offline analysis, we further explored the emotional patterns of patients who achieved significant online recognition accuracies (i.e., P1, P4 and P6). Through analysis of the classification performance of a single feature and the topographical maps of the DE features, emotional-related patterns were observed. For patients P1 and P6, during negative emotional state, the prefrontal and parietal areas exhibited significantly higher responses in the delta band, whereas the temporal areas exhibited higher theta responses. Furthermore, the top-20 features were mainly located in the delta and theta bands. Patient P4 achieved the highest online accuracy among the DOC patients, and the distribution of the top-20 features and topographical maps were more similar to those of healthy subjects; the prefrontal and parietal regions exhibited stronger theta and alpha responses for positive emotion; and the lateral temporal sites exhibited higher beta and gamma responses for positive emotion. These findings are partially consistent with the results of previous studies [22], [49], [51]. For instance, the power of the delta band increased over the parietal regions more for negative emotion than for positive emotion [22]. Increases in the beta and gamma responses in temporal regions for positive emotion have been reported [22], [51]. Taken together, reliable emotional responses were evoked and observed for the three patients.

For patients with DOC, the assessment of emotion is very important. First, emotion and consciousness are the products of neuronal activity in the brain, and there are close relations between them [16], [17]. Several studies have suggested that emotion is a facet of consciousness [52], [53]. So, the reliable emotional activities in patients can provide potential evidence of residual consciousness. Second, emotion is an advanced brain function of human beings, which is related to other cognitive functions including language comprehension, decision making and selective attention [8], [9], [10]. Hence, evaluating emotion may help us understand other cognitive functions of patients with DOC. Third, we may conjecture that it might be useful for the rehabilitation of patients with DOC to regulate their emotions.

Notably, patient P4 was in the state of EMCS (CRS-R score: 13), which means that this patient had the highest consciousness level among all the patients involved in this study. The highest consciousness level might be the key factor that contributed to his highest online accuracy among the patients. Patients P3 and P8 achieved BCI accuracies of 52 and 44 percent, respectively, which were not significantly higher than the chance level. The main reason might be that they were in VS state, based on the clinical diagnosis (CRS-R scores, P3: 5, P8: 7). The rest of the patients (i.e., P1, P2, P5, P6, and P7) were in the state of MCS, and two of them (P1 and P6) achieved significant BCI accuracies (68 and 66 percent, respectively). We summarized the possible reasons for the nonsignificant BCI accuracies of MCS patients P2, P5, and P7 in the following. First, the specific emotions might not have been evoked sufficiently, which were related to the patients attention levels and interests. Second, the emotional states might not be successfully recognized by our BCI algorithm, even if their emotions had been evoked properly.

5.3 Potential Applications
In our daily life, people often fail to recognize their emotions and to regulate their emotions through conscious steps [54]. However, learning to manage ones emotions is important for maintaining mental health and productivity. Therefore, the establishment of a real-time emotion recognition system is beneficial for people to learn to better manage and regulate their emotions. The proposed system can promptly and accurately identify a subject's emotional state, and thus provide an efficient way to monitor and provide feedback about the subject's emotions.

In clinical practice, several behavioral scales have been designed to assess the consciousness levels of DOC patients, which generally rely on patients eye responses, motor responses, verbal responses, brain reflexes, and respirations. Among these scales, the commonly used scales are the Coma Recovery Scale-Revised (CRS-R) scale, the Glasgow Coma Scale (GCS), and the Full Outline of UnResponsiveness (FOUR). However, all of these behavior assessment scales do not consider emotion and thus cannot assess the ability of emotion processing of patients with DOC. In this study, reliable emotional responses were evoked and observed in several patients (P1, P4 and P6), which demonstrated that a part of patients with DOC have the ability of emotion processing. Furthermore, our emotion BCI system could evoke and recognize the emotions of patients with DOC, and may help them to regulate their emotions. According to the theory of mirror neuron system (MNS), the subjects will exhibit the same or similar emotional responses while watching emotional clips [55]. This might support the designs of many existing EEG-based emotion recognition methods/systems including ours. Finally, our emotion BCI system may help us to assess the residual consciousness of patients with DOC. As there are closely relationships between emotion and consciousness [16], [17], our emotion BCI system may be a potential tool for evaluating the consciousness levels of patients with DOC.

5.4 Limitations and Future Work
Although the proposed system achieved high classification performance in healthy subjects and was successfully applied to patients with DOC, there are still some limitations. First, the number of subjects needs to be increased, especially the number of patients with DOC. Second, the recognition performance of patients with DOC needs to be further improved. One possible way to improve the recognition performance is to individually design the BCI paradigm, including the stimulus materials for each DOC patient. Third, more types of emotion should be considered, e.g., fear, surprise, disgust, etc. Finally, daily life scenarios and the self-induced emotion signals generally with low-amplitude property have not been considered in our emotion BCI system. In the future, we will consider these limitations to improve our emotion BCI system and extend it in more general daily life scenarios, and develop emotional regulation BCI system.

SECTION 6Conclusion
In this study, we proposed an EEG-based BCI system to distinguish video-induced positive and negative emotions. We first conducted an experiment with ten healthy subjects. The excellent recognition results demonstrated the efficacy of our BCI system. That is, the subjects’ emotions were well evoked and recognized by our BCI system. Next, we applied our BCI system to patients with DOC. Eight DOC patients participated in our experiment, and three of them achieved significant online accuracy rates, which showed that their emotions could be evoked and detected. Clinically, it is difficult to assess the emotional states of patients with DOC. Therefore, our system provides an potential approach to detect the emotions in patients with DOC.