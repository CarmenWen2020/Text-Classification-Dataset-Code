We study online reinforcement learning in average-reward stochastic games (SGs).
An SG models a two-player zero-sum game in a Markov environment, where state
transitions and one-step payoffs are determined simultaneously by a learner and
an adversary. We propose the UCSG algorithm that achieves a sublinear regret
compared to the game value when competing with an arbitrary opponent. This
result improves previous ones under the same setting. The regret bound has a
dependency on the diameter, which is an intrinsic value related to the mixing
property of SGs. If we let the opponent play an optimistic best response to the
learner, UCSG finds an Îµ-maximin stationary policy with a sample complexity of
OËœ (poly(1/Îµ)), where Îµ is the gap to the best policy.
1 Introduction
Many real-world scenarios (e.g., markets, computer networks, board games) can be cast as multi-agent
systems. The framework of Multi-Agent Reinforcement Learning (MARL) targets at learning to act in
such systems. While in traditional reinforcement learning (RL) problems, Markov decision processes
(MDPs) are widely used to model a single agentâ€™s interaction with the environment, stochastic games
(SGs, [32]), as an extension of MDPs, are able to describe multiple agentsâ€™ simultaneous interaction
with the environment. In this view, SGs are most well-suited to model MARL problems [24].
In this paper, two-player zero-sum SGs are considered. These games proceed like MDPs, with the
exception that in each state, both players select their own actions simultaneously 1
, which jointly
determine the transition probabilities and their rewards . The zero-sum property restricts that the
two playersâ€™ payoffs sum to zero. Thus, while one player (Player 1) wants to maximize his/her total
reward, the other (Player 2) would like to minimize that amount. Similar to the case of MDPs, the
reward can be discounted or undiscounted, and the game can be episodic or non-episodic.
In the literature, SGs are typically learned under two different settings, and we will call them online
and offline settings, respectively. In the offline setting, the learner controls both players in a centralized
manner, and the goal is to find the equilibrium of the game [33, 21, 30]. This is also known as finding
the worst-case optimality for each player (a.k.a. maximin or minimax policy). In this case, we
care about the sample complexity, i.e., how many samples are required to estimate the worst-case
optimality such that the error is below some threshold. In the online setting, the learner controls only
one of the players, and plays against an arbitrary opponent [24, 4, 5, 8, 31]. In this case, we care
1Turn-based SGs, like Go, are special cases: in each state, one playerâ€™s action set contains only a null action.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
about the learnerâ€™s regret, i.e., the difference between some benchmark measure and the learnerâ€™s total
reward earned in the learning process. This benchmark can be defined as the total reward when both
players play optimal policies [5], or when Player 1 plays the best stationary response to Player 2 [4].
Some of the above online-setting algorithms can find the equilibrium simply through self-playing.
Most previous results on offline sample complexity consider discounted SGs. Their bounds depend
heavily on the chosen discount factor [33, 21, 30, 31]. However, as noted in [5, 19], the discounted
setting might not be suitable for SGs that require long-term planning, because only finite steps are
relevant in the reward function it defines. This paper, to the best of our knowledge, is the first to give
an offline sample complexity bound of order OËœ (poly(1/Îµ)) in the average-reward (undiscounted
and non-episodic) setting, where Îµ is the error parameter. A major difference between our algorithm
and previous ones is that the two players play asymmetric roles in our algorithm: by focusing on
finding only one playerâ€™s worst-case optimal policy at a time, the sampling can be rather efficient.
This resembles but strictly extends [13]â€™s methods in finding the maximin action in a two-stage game.
In the online setting, we are only aware of [5]â€™s R-MAX algorithm that deals with average-reward SGs
and provides a regret bound. Considering a similar scenario and adopting the same regret definition,
we significantly improve their bounds (see Appendix A for details). Another difference between our
algorithm and theirs is that ours is able to output a currently best stationary policy at any stage in the
learning process, while theirs only produces a TÎµ-step fixed-horizon policy for some input parameter
TÎµ. The former could be more natural since the worst-case optimal policy is itself a stationary policy.
The techniques used in this paper are most related to RL for MDPs based on the optimism principle
[2, 19, 9] (see Appendix A). The optimism principle built on concentration inequalities automatically
strikes a balance between exploitation and exploration, eliminating the need to manually adjust the
learning rate or the exploration ratio. However, when importing analysis from MDPs to SGs, we
face the challenge caused by the opponentâ€™s uncontrollability and non-stationarity. This prevents the
learner from freely exploring the state space and makes previous analysis that relies on stationary
distributionâ€™s perturbation analysis [2] useless. In this paper, we develop a novel way to replace the
opponentâ€™s non-stationary policy with a stationary one in the analysis (introduced in Section 5.1),
which facilitates the use of techniques based on perturbation analysis. We hope that this technique
can benefit future analysis concerning non-stationary agents in MARL.
One related topic is the robust MDP problem [29, 17, 23]. It is an MDP where some state-action
pairs have adversarial rewards and transitions. It is often assumed in robust MDP that the adversarial
choices by the environment are not directly observable by the Player, but in our SG setting, we
assume that the actions of Player 2 can be observed. However, there are still difficulties in SG that
are not addressed by previous works on robust MDP.
Here we compare our work to [23], a recent work on learning robust MDP. In their setting, there are
adversarial and stochastic state-action pairs, and their proposed OLRM2 algorithm tries to distinguish
them. Under the scenario where the environment is fully adversarial, which is the counterpart to our
setting, the worst-case transitions and rewards are all revealed to the learner, and what the learner
needs to do is to perform a maximin planning. In our case, however, the worst-case transitions and
rewards are still to be learned, and the opponentâ€™s arbitrary actions may hinder the learner to learn
this information. We would say that the contribution of [23] is orthogonal to ours.
Other lines of research that are related to SGs are on MDPs with adversarially changing reward
functions [11, 27, 28, 10] and with adversarially changing transition probabilities [35, 1]. The
assumptions in these works have several differences with ours, and therefore their results are not
comparable to our results. However, they indeed provide other viewpoints about learning in stochastic
games.
2 Preliminaries
Game Models and Policies. A SG is a 4-tuple M = (S, A, r, p). S denotes the state space and
A = A1 Ã— A2
the playersâ€™ joint action space. We denote S = |S| and A = |A|. The game starts
from an initial state s1. Suppose at time t the players are at state st. After the players play the joint
actions (a
1
t
, a2
t
), Player 1 receives the reward rt = r(st, a1
t
, a2
t
) âˆˆ [0, 1] from Player 2, and both
players visit state st+1 following the transition probability p(Â·|st, a1
t
, a2
t
). For simplicity, we consider
2
deterministic rewards as in [3]. The extension to stochastic case is straightforward. We shorten our
notation by a := (a
1
, a2
) or at
:= (a
1
t
, a2
t
), and use abbreviations such as r(st, at) and p(Â·|st, at).
Without loss of generality, players are assumed to determine their actions based on the history. A
policy Ï€ at time t maps the history up to time t, Ht = (s1, a1, r1, ..., st) âˆˆ Ht, to a probability
distribution over actions. Such policies are called history-dependent policies, whose class is denoted
by Î HR. On the other hand, a stationary policy, whose class is denoted by Î SR, selects actions as a
function of the current state. For either class, joint policies (Ï€
1
, Ï€2
) are often written as Ï€.
Average Return and the Game Value. Let the players play joint policy Ï€. Define the T-step total
reward as RT (M, Ï€, s) :=
PT
t=1 r(st, at), where s1 = s, and the average reward as Ï(M, Ï€, s) :=
limTâ†’âˆž
1
T E [RT (M, Ï€, s)], whenever the limit exists. In fact, the game value exists2
[26]:
Ï
âˆ—
(M, s) := sup
Ï€1
inf
Ï€2
lim
Tâ†’âˆž
1
T
E

RT (M, Ï€1
, Ï€2
, s)

.
If Ï(M, Ï€, s) or Ï
âˆ—
(M, s) does not depend on the initial state s, we simply write Ï(M, Ï€) or Ï
âˆ—
(M).
The Bias Vector. For a stationary policy Ï€, the bias vector h(M, Ï€, Â·) is defined, for each coordinate
s, as
h(M, Ï€, s) := E
"Xâˆž
t=1
r(st, at) âˆ’ Ï(M, Ï€, s)



s1 = s, at âˆ¼ Ï€(Â·|st)
#
. (1)
The bias vector satisfies the Bellman equation: âˆ€s âˆˆ S,
Ï(M, Ï€, s) + h(M, Ï€, s) = r(s, Ï€) +X
s
0
p(s
0
|s, Ï€)h(M, Ï€, s0
),
where r(s, Ï€) := Eaâˆ¼Ï€(Â·|s)
[r(s, a)] and p(s
0
|s, Ï€) :=Eaâˆ¼Ï€(Â·|s)
[p(s
0
|s, a)].
The vector h(M, Ï€, Â·) describes the relative advantage among states under model M and (joint) policy
Ï€. The advantage (or disadvantage) of state s compared to state s
0 under policy Ï€ is defined as the
difference between the accumulated rewards with initial states s and s
0
, which, from (1), converges
to the difference h(M, Ï€, s) âˆ’ h(M, Ï€, s0
) asymptotically. For the ease of notation, the span of a
vector v is defined as sp(v) := maxi vi âˆ’ mini vi
. Therefore if a model, together with any policy,
induces large sp (h), then this model will be difficult to learn because visiting a bad state costs a lot
in the learning process. As shown in [3] for the MDP case, the regret has an inevitable dependency
on sp(h(M, Ï€âˆ—
, Â·)), where Ï€
âˆ—
is the optimal policy.
On the other hand, sp(h(M, Ï€, Â·)) is closely related to the mean first passage time under the Markov
chain induced by M and Ï€. Actually we have sp(h(M, Ï€, Â·)) â‰¤ T
Ï€
(M) := maxs,s0 T
Ï€
sâ†’s
0 (M),
where T
Ï€
sâ†’s
0 (M) denotes the expected time to reach state s
0
starting from s when the model is M and
the player(s) follow the (joint) policy Ï€. This fact is intuitive, and the proof can be seen at Remark
M.1.
Notations. In order to save space, we often write equations in vector or matrix form. We use
vectors inequalities: if u, v âˆˆ R
n, then u â‰¤ v â‡” ui â‰¤ vi âˆ€i = 1, ..., n. For a general matrix game
with matrix G of size n Ã— m, we denote the value of the game as val G := max
pâˆˆâˆ†n
min
qâˆˆâˆ†m
p
>Gq =
min
qâˆˆâˆ†m
max
pâˆˆâˆ†n
p
>Gq, where âˆ†k is the probability simplex of dimension k. In SGs, given the estimated
value function u(s
0
) âˆ€s
0
, we often need to solve the following matrix game equation:
v(s) = max
a1âˆ¼Ï€1(Â·|s)
min
a2âˆ¼Ï€2(Â·|s)
{r(s, a1
, a2
) +X
s
0
p(s
0
|s, a1
, a2
)u(s
0
)},
and this is abbreviated with the vector form v = val{r + P u}. We also use solve1 G and solve2 G to
denote the optimal solutions of p and q. In addition, the indicator function is denoted by 1{Â·} or 1{Â·}.
2Unlike in one-player MDPs, the sup and inf in the definition of Ï
âˆ—
(M, s) are not necessarily attainable.
Moreover, players may not have stationary optimal policies.
3  
3 Problem Settings and Results Overview
We assume that the game proceeds for T steps. In order to have meaningful regret bounds (i.e., sublinear to T), we must make some assumptions to the SG model itself. Our two different assumptions
are
Assumption 1. max
s,s0
max
Ï€1âˆˆÎ SR
max
Ï€2âˆˆÎ SR
T
Ï€
1
,Ï€
2
sâ†’s
0 (M) â‰¤ D.
Assumption 2. max
s,s0
max
Ï€2âˆˆÎ SR
min
Ï€1âˆˆÎ SR
T
Ï€
1
,Ï€
2
sâ†’s
0 (M) â‰¤ D.
Why we make these assumptions is as follows. Consider an SG model where the opponent (Player 2)
has some way to lock the learner (Player 1) to some bad state. The best strategy for the learner might
be to totally avoid, if possible, entering that state. However, in the early stage of the learning process,
the learner wonâ€™t know this, and he/she will have a certain probability to visit that state and get locked.
This will cause linear regret to the learner. Therefore, we assume the following: whatever policy the
opponent executes, the learner always has some way to reach any state within some bounded time.
This is essentially our Assumption 2.
Assumption 1 is the stronger one that actually implies that under any policies executed by the players
(not necessarily stationary, see Remark M.2), every state is visited within an average of D steps. We
find that under this assumption, the asymptotic regret can be improved. This assumption also has a
sense similar to those required for Q-learning-type algorithmsâ€™ convergence: they require that every
state be visited infinitely often. See [18] for example.
These assumptions define some notion of diameters that are specific to the SG model. It is known
that under Assumption 1 or Assumption 2, both players have optimal stationary policies, and the
game value is independent of the initial state. Thus we can simply write Ï
âˆ—
(M, s) as Ï
âˆ—
(M). For a
proof of these facts, please refer to Theorem E.1 in the appendix.
3.1 Two Settings and Results Overview
We focus on training Player 1 and discuss two settings. In the online setting, Player 1 competes with
an arbitrary Player 2. The regret is defined as
Reg(on)
T =
X
T
t=1
Ï
âˆ—
(M) âˆ’ r(st, at).
In the offline setting, we control both Player 1 and Player 2â€™s actions, and find Player 1â€™s maximin
policy. The sample complexity is defined as
LÎµ =
X
T
t=1
1{Ï
âˆ—
(M) âˆ’ min
Ï€2
Ï(M, Ï€1
t
, Ï€2
) > Îµ},
where Ï€
1
t
is a stationary policy being executed by Player 1 at time t. This definition is similar to those
in [20, 19] for one-player MDPs. By the definition of LÎµ, if we have an upper bound for LÎµ and run
the algorithm for T > LÎµ steps, there is some t such that Ï€
1
t
is Îµ-optimal. We will explain how to
pick this t in Section 7 and Appendix L.
It turns out that we can use almost the same algorithm to handle these two settings. Since learning in
the online setting is more challenging, from now on we will mainly focus on the online setting, and
leave the discussion about the offline setting at the end of the paper. Our results can be summarized
by the following two theorems.
Theorem 3.1. Under Assumption 1, UCSG achieves Reg(on)
T = OËœ(D3S
5A + DSâˆš
AT) w.h.p. 3
Theorem 3.2. Under Assumption 2, UCSG achieves Reg(on)
T = OËœ(
âˆš3 DS2AT2) w.h.p.
4 Upper Confidence Stochastic Game Algorithm (UCSG)
3We write, â€œwith high probability, g = OËœ(f)â€ or â€œw.h.p., g = OËœ(f)â€ to indicate â€œwith probability
â‰¥ 1 âˆ’ Î´, g = f1O(f) + f2â€, where f1, f2 are some polynomials of log D, log S, log A, log T, log(1/Î´).
4
Algorithm 1 UCSG
Input: S, A = A1 Ã— A2
, T.
Initialization: t = 1.
for phase k = 1, 2, ... do
tk = t.
1. Initialize phase k: vk(s, a) = 0, nk(s, a) = max n
1,
Ptkâˆ’1
Ï„=1 1(sÏ„ ,aÏ„ )=(s,a)
o
,
nk(s, a, s0
) = Ptkâˆ’1
Ï„=1 1(sÏ„ ,aÏ„ ,sÏ„+1)=(s,a,s0)
, pË†k(s
0
|s, a) = nk(s,a,s0
)
nk(s,a)
, âˆ€s, a, s0
.
2. Update the confidence set: Mk = {MËœ : âˆ€s, a, pËœ(Â·|s, a) âˆˆ Pk(s, a)}, where
Pk(s, a) := CONF1(Ë†pk(Â·|s, a), nk(s, a)) âˆ© CONF2(Ë†pk(Â·|s, a), nk(s, a)).
3. Optimistic planning:
M1
k
, Ï€1
k

= MAXIMIN-EVI(Mk, Î³k), where Î³k := 1/
âˆš
tk.
4. Execute policies:
repeat
Draw a
1
t âˆ¼ Ï€
1
k
(Â·|st); observe the reward rt and the next state st+1.
Set vk(st, at) = vk(st, at) + 1 and t = t + 1.
until âˆƒ(s, a) such that vk(s, a) = nk(s, a)
end for
Definitions of confidence regions:
CONF1(Ë†p, n) :=

pËœ âˆˆ [0, 1]S
: kpËœâˆ’ pË†k1 â‰¤
q
2S ln(1/Î´1)
n

, Î´1 =
Î´
2S2A log2 T
.
CONF2(Ë†p, n) :=

pËœ âˆˆ [0, 1]S : âˆ€i,


p
pËœi(1 âˆ’ pËœi) âˆ’
p
pË†i(1 âˆ’ pË†i)

 â‰¤
q2 ln(6/Î´1)
nâˆ’1
,
|pËœi âˆ’ pË†i
| â‰¤ min q
ln(6/Î´1)
2n
,
q2Ë†pi(1âˆ’pË†i)
n
ln 6
Î´1
+
7
3(nâˆ’1) ln 6
Î´1

.
The Upper Confidence Stochastic Game algorithm (UCSG) (Algorithm 1) extends UCRL2 [19],
using the optimism principle to balance exploitation and exploration. It proceeds in phases (indexed
by k), and only changes the learnerâ€™s policy Ï€
1
k
at the beginning of each phase. The length of each
phase is not fixed a priori, but depends on the statistics of past observations.
In the beginning of each phase k, the algorithm estimates the transition probabilities using empirical
frequencies pË†k(Â·|s, a) observed in previous phases (Step 1). With these empirical frequencies, it can
then create a confidence region Pk(s, a) for each transition probability. The transition probabilities
lying in the confidence regions constitute a set of plausible stochastic game models Mk, where the
true model M belongs to with high probability (Step 2). Then, Player 1 optimistically picks one
model M1
k
from Mk, and finds the optimal (stationary) policy Ï€
1
k
under this model (Step 3). Finally,
Player 1 executes the policy Ï€
1
k
for a while until some (s, a)-pairâ€™s number of occurrences is doubled
during this phase (Step 4). The count vk(s, a) records the number of steps the (s, a)-pair is observed
in phase k; it is reset to zero in the beginning of every phase.
In Step 3, to pick an optimistic model and a policy is to pick M1
k âˆˆ Mk and Ï€
1
k âˆˆ Î SR such that âˆ€s,
min
Ï€2
Ï(M1
k
, Ï€1
k
, Ï€2
, s) â‰¥ max
MËœ âˆˆMk
Ï
âˆ—
(M, s Ëœ ) âˆ’ Î³k. (2)
where Î³k denotes the error parameter for MAXIMIN-EVI. The LHS of (2) is well-defined because
Player 2 has stationary optimal policy under the MDP induced by M1
k
and Ï€
1
k
. Roughly speaking,
(2) says that min
Ï€2
Ï(M1
k
, Ï€1
k
, Ï€2
, s) should approximate max
MËœ âˆˆMk,Ï€1
min
Ï€2
Ï(M, Ï€ Ëœ 1
, Ï€2
, s) by an error
no more than Î³k. That is, (M1
k
, Ï€1
k
) are picked optimistically in Mk Ã— Î SR considering the most
adversarial opponent.
4.1 Extended SG and Maximin-EVI
The calculation of M1
k
and Ï€
1
k
involves the technique of Extended Value Iteration (EVI), which also
appears in [19] as a one-player version.
Consider the following SG, named M+. Let the state space S and Player 2â€™s action space A2
remain
the same as in M. Let A1+, p
+(Â·|Â·, Â·, Â·), r
+(Â·, Â·, Â·) be Player 1â€™s action set, the transition kernel, and
 
the reward function of M+, such that for any a
1 âˆˆ A1
and a
2 âˆˆ A2
and an admissible transition
probability pËœ(Â·|s, a1
, a2
) âˆˆ Pk(s, a1
, a2
), there is an action a
1+ âˆˆ A1+ such that p
+(Â·|s, a1+, a2
) =
pËœ(Â·|s, a1
, a2
) and r
+(s, a1+, a2
) = r(s, a1
, a2
). In other words, Player 1 selecting an action in A1+
is equivalent to selecting an action in A1
and simultaneously selecting an admissible transition
probability in the confidence region Pk(Â·, Â·).
Suppose that M âˆˆ Mk, then the extended SG M+ satisfies Assumption 2 because the true model
M is embedded in M+. By Theorem E.1 in Appendix E, it has a constant game value Ï
âˆ—
(M+)
independent of the initial state, and satisfies Bellman equation of the form val{r + P f} = Ï Â· e + f,
for some bounded function f(Â·), where e stands for the all-one constant vector. With the above
conditions, we can use value iteration with Schweitzer transform (a.k.a. aperiodic transform)[34]
to solve the optimal policy in the extended EG M+. We call it MAXIMIN-EVI. For the details
of MAXIMIN-EVI, please refer to Appendix F. We only summarize the result with the following
Lemma.
Lemma 4.1. Suppose the true model M âˆˆ Mk, then the estimated model M1
k
and stationary policy
Ï€
1
k
output by MAXIMIN-EVI in Step 3 satisfy
âˆ€s, min
Ï€2
Ï(M1
k
, Ï€1
k
, Ï€2
, s) â‰¥ max
Ï€1
min
Ï€2
Ï(M, Ï€1
, Ï€2
, s) âˆ’ Î³k.
Before diving into the analysis under the two assumptions, we first establish the following fact.
Lemma 4.2. With high probability, the true model M âˆˆ Mk for all phases k.
It is proved in Appendix D. With Lemma 4.2, we can fairly assume M âˆˆ Mk in most of our analysis.
5 Analysis under Assumption 1
In this section, we import analysis techniques from one-player MDPs [2, 19, 22, 9]. We also develop
some techniques that deal with non-stationary opponents.
We model Player 2â€™s behavior in the most general way, i.e., assuming it using a history-dependent
randomized policy. Let Ht = (s1, a1, r1, ..., stâˆ’1, atâˆ’1, rtâˆ’1, st) âˆˆ Ht be the history up to st, then
we assume Ï€
2
t
to be a mapping from Ht to a distribution over A2
. We will simply write Ï€
2
t
(Â·) and
hide its dependency on Ht inside the subscript t. A similar definition applies to Ï€
1
t
(Â·). With abuse
of notations, we denote by k(t) the phase where step t lies in, and thus our algorithm uses policy
Ï€
1
t
(Â·) = Ï€
1
k(t)
(Â·|st). The notations Ï€
1
t
and Ï€
1
k
are used interchangeably. Let Tk := tk+1 âˆ’ tk be the
length of phase k. We decompose the regret in phase k in the following way:
Î›k := TkÏ
âˆ—
(M) âˆ’
tkX+1âˆ’1
t=tk
r(st, at) = X
4
n=1
Î›
(n)
k
, (3)
in which we define
Î›
(1)
k = Tk

Ï
âˆ—
(M) âˆ’ min
Ï€2
Ï(M1
k
, Ï€1
k
, Ï€2
, stk
)

,
Î›
(2)
k = Tk

min
Ï€2
Ï(M1
k
, Ï€1
k
, Ï€2
, stk
) âˆ’ Ï(M1
k
, Ï€1
k
, Ï€Â¯
2
k
, stk
)

,
Î›
(3)
k = Tk

Ï(M1
k
, Ï€1
k
, Ï€Â¯
2
k
, stk
) âˆ’ Ï(M, Ï€1
k
, Ï€Â¯
2
k
)

,
Î›
(4)
k = TkÏ(M, Ï€1
k
, Ï€Â¯
2
k
) âˆ’
tkX+1âˆ’1
t=tk
r(st, at),
where Ï€Â¯
2
k
is some stationary policy of Player 2 which will be defined later. Since the actions of
Player 2 are arbitrary, Ï€Â¯
2
k
is imaginary and only exists in analysis. Note that under Assumption 1, any
stationary policy pair over M induces an irreducible Markov chain, so we do not need to specify the
initial states for Ï(M, Ï€1
k
, Ï€Â¯
2
k
) in (3). Among the four terms, Î›
(2)
k
is clearly non-positive, and Î›
(1)
k
,
by optimism, can be bounded using Lemma 4.1. Now remains to bound Î›
(3)
k
and Î›
(4)
k
.
 
5.1 Bounding P
k Î›
(3)
k
and P
k Î›
(4)
k
The Introduction of Ï€Â¯
2
k
. Î›
(3)
k
and Î›
(4)
k
involve the artificial policy Ï€Â¯
2
k
, which is a stationary policy
that replaces Player 2â€™s non-stationary policy in the analysis. This replacement costs some constant
regret but facilitates the use of perturbation analysis in regret bounding. The selection of Ï€Â¯
2
k
is based
on the principle that the behavior (e.g., total number of visits to some (s, a)) of the Markov chain
induced by M, Ï€1
k
, Ï€Â¯
2
k
should be close to the empirical statistics. Intuitively, Ï€Â¯
2
k
can be defined as
Ï€Â¯
2
k
(a
2
|s) :=
Ptk+1âˆ’1
t=tk
1st=sÏ€
2
t
(a
2
)
Ptk+1âˆ’1
t=tk
1st=s
. (4)
Note two things, however. First, since we need the actual trajectory in defining this policy, it can only
be defined after phase k has ended. Second, Ï€Â¯
2
k
can be undefined because the denominator of (4) can
be zero. However, this will not happen in too many steps. Actually, we have
Lemma 5.1. P
k
Tk1{Ï€Â¯
2
k
not well-defined}â‰¤ OËœ(DS2A) with high probability.
Before describing how we bound the regret with the help of Ï€Â¯
2
k
and the perturbation analysis, we
establish the following lemma:
Lemma 5.2. We say the transition probability at time step t is Îµ-accurate if |p
1
k
(s
0
|st, Ï€t) âˆ’
p(s
0
|st, Ï€t)| â‰¤ Îµ âˆ€s
0 where p
1
k
denotes the transition kernel of M1
k
. We let Bt(Îµ) = 1 if the
transition probability at time t is Îµ-accurate; otherwise Bt(Îµ) = 0. Then for any state s, with high
probability, PT
t=1 1st=s1Bt(Îµ)=0 â‰¤ OËœ

A/Îµ2

.
Now we are able to sketch the logic behind our proofs. Letâ€™s assume that Ï€Â¯
2
k models Ï€
2
k
quite well,
i.e., the expected frequency of every state-action pair induced by M, Ï€1
k
, Ï€Â¯
2
k
is close to the empirical
frequency induced by M, Ï€1
k
, Ï€2
k
. Then clearly, Î›
(4)
k
is close to zero in expectation. The term Î›
(3)
k
now becomes the difference of average reward between two Markov reward processes with slightly
different transition probabilities. This term has a counterpart in [19] as a single-player version. Using
similar analysis, we can prove that the dominant term of Î›
(3)
k
is proportional to sp(h(M1
k
, Ï€1
k
, Ï€Â¯
2
k
, Â·)).
In the single-player case, [19] can directly claim that sp(h(M1
k
, Ï€1
k
, Â·)) â‰¤ D (see their Remark 8),
but unfortunately, this is not the case in the two-player version. 4
To continue, we resort to the perturbation analysis for the mean first passage times (developed
in Appendix C). Lemma 5.2 shows that M1
k will not be far from M for too many steps. Then
Theorem C.9 in Appendix C tells that if M1
k
are close enough to M, T
Ï€
1
k
,Ï€Â¯
2
k (M1
k
) can be bounded by
2T
Ï€
1
k
,Ï€Â¯
2
k (M). As Remark M.1 implies that sp(h(M1
k
, Ï€1
k
, Ï€Â¯
2
k
, Â·)) â‰¤ T
Ï€
1
k
,Ï€Â¯
2
k (M1
k
) and Assumption 1
guarantees that T
Ï€
1
k
,Ï€Â¯
2
k (M) â‰¤ D, we have sp(h(M1
k
, Ï€1
k
, Ï€Â¯
2
k
, Â·)) â‰¤ T
Ï€
1
k
,Ï€Â¯
2
k (M1
k
) â‰¤ 2T
Ï€
1
k
,Ï€Â¯
2
k (M) â‰¤
2D.
The above approach leads to Lemma 5.3, which is a key in our analysis. We first define some
notations. Under Assumption 1, any pair of stationary policies induces an irreducible Markov chain,
which has a unique stationary distribution. If the policy pair Ï€ = (Ï€
1
, Ï€2
) is executed, we denote its
stationary distribution by Âµ(M, Ï€1
, Ï€2
, Â·) = Âµ(M, Ï€, Â·). Besides, denote vk(s) :=
Ptk+1âˆ’1
t=tk
1st=s.
We say a phase k is benign if the following hold true: the true model M lies in Mk, Ï€Â¯
2
k
is well-defined,
sp(h(M1
k
, Ï€1
k
, Ï€Â¯
2
k
, Â·)) â‰¤ 2D, and Âµ(M, Ï€1
k
, Ï€Â¯
2
k
, s) â‰¤
2vk(s)
Tk
âˆ€s. We can show the following:
Lemma 5.3. P
k
Tk1{phase k is not benign} â‰¤OËœ(D3S
5A) with high probability.
Finally, for benign phases, we can have the following two lemmas.
Lemma 5.4. P
k Î›
(4)
k
1{Ï€Â¯
2
k
is well-defined }â‰¤ OËœ(D
âˆš
ST + DSA) with high probability.
4The argument in [19] is simple: suppose that h(M1
k , Ï€1
k, s) âˆ’ h(M1
k , Ï€1
k, s0
) > D, by the communicating
assumption, there is a path from s
0
to s with expected time no more than D. Thus a policy that first goes from s
0
to s within D steps and then executes Ï€
1
k will outperform Ï€
1
k at s
0
. This leads to a contradiction. In two-player
SGs, with a similar argument, we can also show that sp(h(M1
k , Ï€1
k, Ï€2âˆ—
k , Â·)) â‰¤ D, where Ï€
2âˆ—
k is the best response
to Ï€
1
k under M1
k . However, since Player 2 is uncontrollable, his/her policy Ï€
2
k (or Ï€Â¯
2
k) can be quite different from
Ï€
2âˆ—
k , and thus sp(h(M1
k , Ï€1
k, Ï€Â¯
2
k, Â·)) â‰¤ D does not necessarily hold true.
 
Lemma 5.5. P
k Î›
(3)
k
1{phase k is benign} â‰¤OËœ(DSâˆš
AT + DS2A) with high probability,
Proof of Theorem 3.1. The regret proof starts from the decomposition of (3). Î›
(1)
k
is bounded with
the help of Lemma 4.1: P
k Î›
(1)
k â‰¤
P
k
Tk/
âˆš
tk = O(
âˆš
T).
P
k Î›
(2)
k â‰¤ 0 by definition. Then with
Lemma 5.1, 5.3, 5.4, and 5.5, we can bound Î›
(3)
k
and Î›
(4)
k
by OËœ(D3S
5A + DSâˆš
AT).
6 Analysis under Assumption 2
In Section 5, the main ingredient of regret analysis lies in bounding the span of the bias vector,
sp(h(M1
k
, Ï€1
k
, Ï€Â¯
2
k
, Â·)). However, the same approach does not work because under the weaker Assumption 2, we do not have a bound on the mean first passage time under arbitrary policy pairs. Hence we
adopt the approach of approximating the average reward SG problem by a sequence of finite-horizon
SGs: on a high level, first, with the help of Assumption 2, we approximate the T multiple of the
original average-reward SG game value (i.e. the total reward in hindsight) with the sum of those of
H-step episodic SGs; second, we resort to [9]â€™s results to bound the H-step SGsâ€™ sample complexity
and translates it to regret.
Approximation by repeated episodic SGs. For the approximation, the quantity H does not appear
in UCSG but only in the analysis. The horizon T is divided into episodes each with length H. Index
episodes with i = 1, ..., T /H, and denote episode iâ€™s first time step by Ï„i
. We say i âˆˆ ph(k) if all H
steps of episode i lie in phase k. Define the H-step expected reward under joint policy Ï€ with initial
state s as VH(M, Ï€, s) := E
hPH
t=1 rt|at âˆ¼ Ï€, s1 = s
i
. Now we decompose the regret in phase k as
âˆ†k := TkÏ
âˆ— âˆ’
tkX+1âˆ’1
t=tk
r(st, at) â‰¤
X
6
n=1
âˆ†
(n)
k
, (5)
where
âˆ†
(1)
k =
P
iâˆˆph(k) H

Ï
âˆ— âˆ’ minÏ€2 Ï(M1
k
, Ï€1
k
, Ï€2
, sÏ„i
)

,
âˆ†
(2)
k =
P
iâˆˆph(k)

H minÏ€2 Ï(M1
k
, Ï€1
k
, Ï€2
, sÏ„i
) âˆ’ minÏ€2 VH(M1
k
, Ï€1
k
, Ï€2
, sÏ„i
)

,
âˆ†
(3)
k =
P
iâˆˆph(k)

minÏ€2 VH(M1
k
, Ï€1
k
, Ï€2
, sÏ„i
) âˆ’ VH(M1
k
, Ï€1
k
, Ï€2
i
, sÏ„i
)

,
âˆ†
(4)
k =
P
iâˆˆph(k)

VH(M1
k
, Ï€1
k
, Ï€2
i
, sÏ„i
) âˆ’ VH(M, Ï€1
k
, Ï€2
i
, sÏ„i
)

,
âˆ†
(5)
k =
P
iâˆˆph(k)

VH(M, Ï€1
k
, Ï€2
i
, sÏ„i
) âˆ’
PÏ„i+1âˆ’1
t=Ï„i
r(st, at)

, âˆ†
(6)
k = 2H.
Here, Ï€
2
i
denotes Player 2â€™s policy in episode i, which may be non-stationary. âˆ†
(6)
k
comes from the
possible two incomplete episodes in phase k. âˆ†
(1)
k
is related to the tolerance level we set for the
MAXIMIN-EVI algorithm: âˆ†
(1)
k â‰¤ TkÎ³k = Tk/
âˆš
tk. âˆ†
(2)
k
is an error caused by approximating an
infinite-horizon SG by a repeated episodic H-step SG (with possibly different initial states). âˆ†
(3)
k
is
clearly non-positive. It remains to bound âˆ†
(2)
k
, âˆ†
(4)
k
and âˆ†
(5)
k
.
Lemma 6.1. By Azuma-Hoeffdingâ€™s inequality, P
k âˆ†
(5)
k â‰¤ OËœ(
âˆš
HT) with high probability.
Lemma 6.2. Under Assumption 2, P
k âˆ†
(2)
k â‰¤ T D/H +
P
k
TkÎ³k.
From sample complexity to regret bound. As the main contributor of regret, âˆ†
(4)
k
corresponds
to the inaccuracy in the transition probability estimation. Here we largely reuse [9]â€™s results where
they consider one-player episodic MDP with a fixed initial state distribution. Their main lemma
states that the number of episodes in phases such that |VH(M1
k
, Ï€k, s0) âˆ’ VH(M, Ï€k, s0)| > Îµ
will not exceed OËœ

H2S
2A/Îµ2

, where s0 is their initial state in each episode. In other words,
P
k
Tk
H
1{|VH(M1
k
, Ï€k, s0) âˆ’ VH(M, Ï€k, s0)| > Îµ} = OËœ(H2S
2A/Îµ2
). Note that their proof allows
Ï€k to be an arbitrarily selected non-stationary policy for phase k.
 
We can directly utilize their analysis and we summarize it as Theorem K.1 in the appendix. While
their algorithm has an input Îµ, this input can be removed without affecting bounds. This means that
the PAC bounds holds for arbitrarily selected Îµ. With the help of Theorem K.1, we have
Lemma 6.3. P
k âˆ†
(4)
k â‰¤ OËœ(S
âˆš
HAT + HS2A) with high probability.
Proof of Theorem 3.2. With the decomposition (5) and the help of Lemma 6.1, 6.2, and 6.3,
the regret is bounded by OËœ(
TD
H + S
âˆš
HAT + S
2AH) = OËœ(
âˆš3 DS2AT2) by selecting H =
max{D, p3 D2T /(S2A)}.
7 Sample Complexity of Offline Training
In Section 3.1, we defined LÎµ to be the sample complexity of Player 1â€™s maximin policy. In our
offline version of UCSG, in each phase k we let both players each select their own optimistic policy.
After Player 1 has optimistically selected Ï€
1
k
, Player 2 then optimistically selects his policy Ï€
2
k
based
on the known Ï€
1
k
. Specifically, the model-policy pair
M2
k
, Ï€2
k

is obtained by another extended value
iteration on the extended MDP under fixed Ï€
1
k
, where Player 2â€™s action set is extended. By setting the
stopping threshold also as Î³k, we have
Ï(M2
k
, Ï€1
k
, Ï€2
k
, s) â‰¤ min
MËœ âˆˆMk
min
Ï€2
Ï(M, Ï€ Ëœ 1
k
, Ï€2
, s) + Î³k (6)
when value iteration halts. With this selection rule, we are able to obtain the following theorems.
Theorem 7.1. Under Assumption 1, UCSG achieves LÎµ = OËœ(D3S
5A + D2S
2A/Îµ2
) w.h.p.
Theorem 7.2. Let Assumption 2 hold, and further assume that max
s,s0
max
Ï€1âˆˆÎ SR
min
Ï€2âˆˆÎ SR
T
Ï€
1
,Ï€
2
sâ†’s
0 (M) â‰¤ D.
Then UCSG achieves LÎµ = OËœ(DS2A/Îµ3
) w.h.p.
The algorithm can output a single stationary policy for Player 1 with the following guarantee: if we
run the offline version of UCSG for T > LÎµ steps, the algorithm can output a single stationary policy
that is Îµ-optimal. We show how to output this policy in the proofs of Theorem 7.1 and 7.2.
8 Open Problems
In this work, we obtain the regret of OËœ(D3S
5A + DSâˆš
AT) and OËœ(
âˆš3 DS2AT) under different
mixing assumptions. A natural open problem is how to improve these bounds on both asymptotic and
constant terms. A lower bound of them can be inherited from the one-player MDP setting, which is
â„¦(âˆš
DSAT) [19].
Another open problem is that if we further weaken the assumptions to maxs,s0 minÏ€1 minÏ€2 T
Ï€
1
,Ï€
2
sâ†’s
0 â‰¤
D, can we still learn the SG? We have argued that if we only have this assumption, in general we
cannot get sublinear regret in the online setting. However, it is still possible to obtain polynomial-time
offline sample complexity if the two players cooperate to explore the state-action space 