This paper presents a unified framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to
be equivalent to a matrix nearness problem with respect to Bregman divergences. Our
framework thus naturally generalizes a previously proposed regularization based on the
Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with
the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot
moverâ€™s distance in reference to the classical earth moverâ€™s distance. By exploiting alternate
Bregman projections, we develop the alternate scaling algorithm and non-negative alternate scaling algorithm, to compute efficiently the regularized optimal plans depending on
whether the domain of the regularizer lies within the non-negative orthant or not. We further enhance the separable case with a sparse extension to deal with high data dimensions.
We also instantiate our framework and discuss the inherent specificities for well-known
regularizers and statistical divergences in the machine learning and information geometry
communities. Finally, we demonstrate the merits of our methods with experiments using
synthetic data to illustrate the effect of different regularizers, penalties and dimensions, as
well as real-world data for a pattern recognition application to audio scene classification.
Keywords: alternate projections, convex analysis, regularized optimal transport, rot
moverâ€™s distance, statistical divergences
1. Introduction
A recurrent problem in statistical machine learning is the choice of a relevant distance
measure to compare probability distributions. Various information divergences are famous,
c 2018 Arnaud Dessein, Nicolas Papadakis and Jean-Luc Rouas.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v19/17-361.html.
Dessein, Papadakis and Rouas
among which Euclidean, Mahalanobis, Kullback-Leibler, Itakura-Saito, Hellinger, Ï‡
2
, `p
(quasi-)norm, total variation, logistic loss function, or more general CsiszÂ´ar and Bregman
divergences and parametric families of such divergences such as Î±- and Î²-divergences.
An alternative family of distances between probability distributions can be introduced in
the framework of optimal transport (OT). Rather than performing a pointwise comparison
of the distributions, the idea is to quantify the minimal effort for moving the probability
mass of one distribution to the other, where the transport plan to move the mass is optimized
according to a given ground cost. This makes OT distances suitable and robust in certain
applications, notably in the field of computer vision where the discrete OT distance, also
known as earth moverâ€™s distance (EMD), has been popularized to compare histograms of
features for pattern recognition tasks (Rubner et al., 2000).
Despite its appealing theoretical properties, intuitive formulation, and excellent performance in various problems of information retrieval, the computation of the EMD involves
solving a linear program whose cost quickly becomes prohibitive with the data dimension.
In practice, the best algorithms currently proposed, such as the network simplex (Ahuja
et al., 1993), scale at least with a super-cubic complexity. Embeddings of the distributions
can be used to approximate the EMD with linear complexity (Indyk and Thaper, 2003;
Grauman and Darrell, 2004; Shirdhonkar and Jacobs, 2008), and the network simplex can
be modified to run in quadratic time (Gudmundsson et al., 2007; Ling and Okada, 2007; Pele
and Werman, 2009). Nevertheless, the distortions inherent to such embeddings (Naor and
Schechtman, 2007), and the exponential increase of costs incurred by such modifications,
make these approaches inapplicable for dimensions higher than four. Instead, multi-scale
strategies (Oberman and Ruan, 2015) and shortcut paths (Schmitzer, 2016a) can speed up
the estimation of the exact optimal plan. These approaches are yet limited to particular
convex costs such as `2, while other costs such as `1 and truncated or compressed versions
are often preferred in practice for an increased robustness to data outliers (Pele and Werman, 2008, 2009; Rabin et al., 2009). For general applications, a gain in performance can
also be obtained with a cost directly learned from labeled data (Cuturi and Avis, 2014).
The aforementioned accelerated methods that are dedicated to `2 or convex costs are thus
not adapted in this context.
On another line of research, the regularization of the transport plan, for example via
graph modeling (Ferradans et al., 2014), has been considered to deal with noisy data,
though this latter approach does not address the computational issue of efficiency for high
dimensions. In this continuity, an entropic regularization was shown to admit an efficient
algorithm with quadratic complexity that speeds up the computation of solutions by several orders of magnitude, and to improve performance on applications such as handwritten
digit recognition (Cuturi, 2013). In addition, a tailored computation can be obtained via
convolution for specific ground costs (Solomon et al., 2015). Since the introduction of the
entropic regularization, OT has benefited from extensive developments in the machine learning community, with applications such as label propagation (Solomon et al., 2014), domain
adaptation (Courty et al., 2015), matrix factorization (Zen et al., 2014), dictionary learning (Rolet et al., 2016; Schmitz et al., 2018), barycenter computation (Cuturi and PeyrÂ´e,
2016), geodesic principal component analysis (Bigot et al., 2013; Seguy and Cuturi, 2015;
Cazelles et al., 2017), data fitting (Frogner et al., 2015), statistical inference (Bernton et al.,
2
Rot Moverâ€™s Distance
2017), training of Boltzmann machines (Montavon et al., 2016) and generative adversarial
networks (Arjovsky et al., 2017; Bousquet et al., 2017; Genevay et al., 2017).
With the entropic regularization, the gain in computational time is only important for
high dimensions or large levels of regularization. For low regularization, advanced optimization strategies can still be used to obtain a significant speed-up (Thibault et al., 2017;
Schmitz et al., 2018). It is also a well-known effect that the entropic regularization overspreads the transported mass, which may be undesirable for certain applications as in the
case of interpolation purposes. An interesting perspective of these works, however, is that
many more regularizers are worth investigating to solve OT problems both efficiently and
robustly (Galichon and SalaniÂ´e, 2015; Muzellec et al., 2018; Blondel et al., 2017). This is
the idea we address in the present work, focusing on smooth convex regularization.
1.1 Notations
For the sake of simplicity, we consider distributions with same dimension d, and thus work
with the Euclidean space R
dÃ—d of square matrices. It is straightforward, however, to extend
all results for a different number of bins m, n by using rectangular matrices in R
mÃ—n
instead.
We denote the null matrix of R
dÃ—d by 0, and the matrix full of ones by 1. The Frobenius
inner product between two matrices Ï€, Î¾ âˆˆ R
dÃ—d
is defined by:
hÏ€, Î¾i =
X
d
i=1
X
d
j=1
Ï€ijÎ¾ij . (1)
When the intended meaning is clear from the context, we also write 0 for the null vector of
R
d
, and 1 for the vector full of ones. The notation Â·
> represents the transposition operator
for matrices or vectors. The probability simplex of R
d
is defined as follows:
Î£d = {p âˆˆ R
d
+ : p
>1 = 1} . (2)
The operator diag(v) transforms a vector v âˆˆ R
d
into a diagonal matrix Ï€ âˆˆ R
dÃ—d
such
that Ï€ii = vi
, for all 1 â‰¤ i â‰¤ d. The operator vec(Ï€) transforms a matrix Ï€ âˆˆ R
dÃ—d
into a
vector x âˆˆ R
d
2
such that xi+(jâˆ’1)d = Ï€ij , for all 1 â‰¤ i, j â‰¤ d. The operator sgn(x) for x âˆˆ R
returns âˆ’1, 0, +1, if x is negative, null, positive, respectively. Functions of a real variable,
such as the absolute value, sign, exponential or power functions, are considered elementwise when applied to matrices. The max operator and inequalities between matrices should
also be interpreted element-wise. Matrix divisions are similarly considered element-wise,
whereas element-wise matrix multiplications, also known as Hadamard or Schur products,
are denoted by  to remove any ambiguity with standard matrix multiplications. Lastly,
addition or subtraction of a scalar and a matrix should be understood element-wise by
replicating the scalar.
1.2 Background and Related Work
Given two probability vectors p, q âˆˆ Î£d, and a cost matrix Î³ âˆˆ R
dÃ—d
+ whose coefficients Î³ij
represent the cost of moving the mass from bin pi to qj , the total cost of a given transport
plan, or coupling, Ï€ âˆˆ Î (p, q) can be quantified as hÏ€, Î³i. An optimal cost is then obtained
3
Dessein, Papadakis and Rouas
by solving a linear program:
dÎ³(p, q) = min
Ï€âˆˆÎ (p,q)
hÏ€, Î³i , (3)
with the transport polytope of p and q, also known as the polytope of couplings between
p and q, defined as the following polyhedron:
Î (p, q) = {Ï€ âˆˆ R
dÃ—d
+ : Ï€1 = p,Ï€
>1 = q} . (4)
The EMD associated to the cost matrix Î³ is given by dÎ³ and is a true distance metric on
the probability simplex Î£d whenever Î³ is itself a distance matrix. In general, the optimal
plans, or earth moverâ€™s plans, have at most 2d âˆ’ 1 nonzero entries, and consist either of a
single vertex or of a whole facet of the transport polytope. One of the earth moverâ€™s plans
can be obtained with the network simplex (Ahuja et al., 1993) among other approaches.
For a general cost matrix Î³, the complexity of solving an OT problem scales at least in
O(d
3
log d) for the best algorithms currently proposed, including the network simplex, and
turns out to be super-cubic in practice as well.
Cuturi (2013) proposed a new family of OT distances, called Sinkhorn distances, from
the perspective of maximum entropy. The idea is to smooth the original problem with a
strictly convex regularization via the Boltzmann-Shannon entropy. The primal problem
involves the entropic regularization as an additional constraint:
d
0
Î³,Î±(p, q) = min
Ï€âˆˆÎ Î±(p,q)
hÏ€, Î³i , (5)
with the regularized transport polytope defined as follows:
Î Î±(p, q) = {Ï€ âˆˆ Î (p, q): E(Ï€) â‰¤ E(pq>) + Î±} , (6)
where Î± â‰¥ 0 is a regularization term and E is minus the Boltzmann-Shannon entropy as
defined in (28). It is also straightforward to prove that we have:
Î Î±(p, q) = {Ï€ âˆˆ Î (p, q): K(Ï€k1) â‰¤ K(pq>k1) + Î±} , (7)
Î Î±(p, q) = {Ï€ âˆˆ Î (p, q): K(Ï€kpq>) â‰¤ Î±} . (8)
where K is the Kullback-Leibler divergence as defined in (27). This enforces the solution to
have sufficient entropy, or equivalently small enough mutual information, by constraining it
to the Kullback-Leibler ball of radius K(pq>k1) + Î±, respectively Î±, and center the matrix
1 âˆˆ R
dÃ—d
++ , respectively the transport plan pq> âˆˆ R
dÃ—d
++ , which have maximum entropy.
The dual problem exploits a Lagrange multiplier to relax the entropic regularization as a
penalty:
dÎ³,Î»(p, q) = hÏ€
?
Î»
, Î³i , (9)
with the regularized optimal plan Ï€
?
Î»
defined as follows:
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
hÏ€, Î³i + Î»E(Ï€) , (10)
where Î» > 0 is a regularization term. The problem can then be solved empirically in
quadratic complexity with linear convergence using the Sinkhorn-Knopp algorithm (Sinkhorn
4
Rot Moverâ€™s Distance
and Knopp, 1967) based on iterative matrix scaling, where rows and columns are rescaled
in turn so that they respectively sum up to p and q until convergence. Finally, it is easy
to prove that we have:
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
hÏ€, Î³i + Î»K(Ï€k1) , (11)
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
hÏ€, Î³i + Î»K(Ï€kpq>) . (12)
This again shows that the regularization enforces the solution to have sufficient entropy, or
equivalently small enough mutual information, by shrinking it toward the matrix 1 and the
joint distribution pq> which have maximum entropy.
Benamou et al. (2015) revisited the entropic regularization in a geometrical framework
with iterative information projections. They showed that computing a Sinkhorn distance
in dual form actually amounts to the minimization of a Kullback-Leibler divergence:
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
K(Ï€k exp(âˆ’Î³/Î»)) . (13)
Precisely, this amounts to computing the Kullback-Leibler projection of exp(âˆ’Î³/Î») âˆˆ R
dÃ—d
++
onto the transport polytope Î (p, q). In this context, the Sinkhorn-Knopp algorithm turns
out to be a special instance of Bregman projection onto the intersection of convex sets via
alternate projections. Specifically, we see Î (p, q) as the intersection of the non-negative
orthant with two affine subspaces containing all matrices with rows and columns summing
to p and q respectively, and we alternate projection on these two subspaces according to
the Kullback-Leibler divergence until convergence.
Kurras (2015) further studied this equivalence in the wider context of iterative proportional fitting. He notably showed that the Sinkhorn-Knopp and alternate Bregman
projections can be extended to account for infinite entries in the cost matrix Î³, and thus
null entries in the regularized optimal plan. Hence, it is possible to develop a sparse version of the entropic regularization to OT problems. This becomes interesting to store the
d Ã— d matrix variables and perform the required computations when the data dimension
gets large.
Dhillon and Tropp (2007) had already enlightened such an equivalence in the field of
matrix analysis. They actually considered the estimation of contingency tables with fixed
marginals as a matrix nearness problem based on the Kullback-Leibler divergence. In more
detail, they use a rough estimate Î¾ âˆˆ R
dÃ—d
++ to produce a contingency table Ï€
?
that has fixed
marginals p, q by Kullback-Leibler projection of Î¾ onto Î (p, q):
Ï€
? = argmin
Ï€âˆˆÎ (p,q)
K(Ï€kÎ¾) . (14)
They showed that alternate Bregman projections specialize to the Sinkhorn-Knopp algorithm in this context. However, no relationship to OT problems was highlighted.
1.3 Contributions and Organization
Our main contribution is to formulate a unified framework for discrete regularized optimal
transport (ROT) by considering a large class of smooth convex regularizers. We call the
5
Dessein, Papadakis and Rouas
underlying distance the rot moverâ€™s distance (RMD) and show that a given ROT problem
actually amounts to the minimization of an associated Bregman divergence. This allows the
derivation of two schemes that we call the alternate scaling algorithm (ASA) and the nonnegative alternate scaling algorithm (NASA), to compute efficiently the regularized optimal
plans depending on whether the domain of the regularizer lies within the non-negative
orthant or not. These schemes are based on the general form of alternate projections for
Bregman divergences. They also exploit the Newton-Raphson method to approximate the
projections for separable divergences. The separable case is further enhanced with a sparse
extension to deal with high data dimensions. We also instantiate our two generic schemes
with widely-used regularizers and statistical divergences.
The proposed framework naturally extends the Sinkhorn-Knopp algorithm for the regularization based on the Boltzmann-Shannon entropy (Cuturi, 2013), or equivalently the
minimization of a Kullback-Leibler divergence (Benamou et al., 2015), and their sparse version (Kurras, 2015), which turn out to be special instances of ROT problems. It also relates
to matrix nearness problems via minimization of Bregman divergences, and it is straightforward to construct more general estimators for contingency tables with fixed marginals
than the classical estimator based on the Kullback-Leibler divergence (Dhillon and Tropp,
2007). Lastly, it brings some new insights between transportation theory (Villani, 2009) and
information geometry (Amari and Nagaoka, 2000), where Bregman divergences are known
to possess a dually flat structure with a generalized Pythagorean theorem in relation to
information projections.
The remainder of this paper is organized as follows. In Section 2, we introduce some
necessary preliminaries. In Section 3, we present our theoretical results for a unified framework of ROT problems. We then derive the algorithmic methods for solving ROT problems
in Section 4. We also discuss the inherent specificities of ROT problems for classical regularizers and associated divergences in Section 5. In Section 6, we provide experiments
to illustrate our methods on synthetic data and real-world audio data in a classification
problem. Finally, in Section 7, we draw some conclusions and perspectives for future work.
2. Theoretical Preliminaries
In this section, we introduce the required preliminaries to our framework. We begin with
elements of convex analysis (Section 2.1) and of Bregman geometry (Section 2.2). We proceed with theoretical results for convergence of alternate Bregman projections (Section 2.3)
and of the Newton-Raphson method (Section 2.4).
2.1 Convex Analysis
Let E be a Euclidean space with inner product hÂ·, Â·i and induced norm k Â· k. The boundary,
interior and relative interior of a subset X âŠ† E are respectively denoted by bd(X ), int(X ),
and ri(X ), where we recall that for a convex set C, we have:
ri(C) = {x âˆˆ C : âˆ€y âˆˆ C, âˆƒÎ» > 1, Î»x + (1 âˆ’ Î»)y âˆˆ C} . (15)
In convex analysis, scalar functions are defined over the whole space E and take values
in the extended real number line R âˆª {âˆ’âˆž, +âˆž}. The effective domain, or simply domain,
6
Rot Moverâ€™s Distance
of a function f is then defined as the set:
dom f = {x âˆˆ E : f(x) < +âˆž} . (16)
A convex function f is proper if f(x) < +âˆž for at least one x âˆˆ E and f(x) > âˆ’âˆž for
all x âˆˆ E, and it is closed if its lower level sets {x âˆˆ E : f(x) â‰¤ Î±} are closed for all Î± âˆˆ R.
If dom f is closed, then f is closed, and a proper convex function is closed if and only if it is
lower semi-continuous. Moreover, a closed function f is continuous relative to any simplex,
polytope or polyhedral subset in dom f. It is also well-known that a convex function f is
always continuous in the relative interior ri(dom f) of its domain.
A function f is essentially smooth if it is differentiable on int(dom f) 6= âˆ… and verifies
limkâ†’+âˆž kâˆ‡f(xk)k = +âˆž for any sequence (xk)kâˆˆN
from int(dom f) that converges to
a point x âˆˆ bd(dom f). A function f is of Legendre type if it is a closed proper convex
function that is also essentially smooth and strictly convex on int(dom f).
The Fenchel conjugate f
? of a function f is defined for all y âˆˆ E as follows:
f
?
(y) = sup
xâˆˆint(dom f)
hx, yi âˆ’ f(x) . (17)
The Fenchel conjugate f
?
is always a closed convex function. Moreover, if f is a closed
convex function, then (f
?
)
? = f, and f is of Legendre type if and only if f
?
is of Legendre
type. In this latter case, the gradient mapping âˆ‡f is a homeomorphism between int(dom f)
and int(dom f
?
), with inverse mapping (âˆ‡f)
âˆ’1 = âˆ‡f
?
, which guarantees the existence of
dual coordinate systems x(y) = âˆ‡f
?
(y) and y(x) = âˆ‡f(x) on int(dom f) and int(dom f
?
).
Finally, we say that a function f is cofinite if it verifies:
lim
Î»â†’+âˆž
f(Î»x)/Î» = +âˆž , (18)
for all nonzero x âˆˆ E. Intuitively, it means that f grows super-linearly in every direction.
In particular, a closed proper convex function is cofinite if and only if dom f
? = E.
2.2 Bregman Geometry
Let Ï† be a convex function on E that is differentiable on int(dom Ï†) 6= âˆ…. The Bregman
divergence generated by Ï† is defined as follows:
BÏ†(xky) = Ï†(x) âˆ’ Ï†(y) âˆ’ hx âˆ’ y, âˆ‡Ï†(y)i , (19)
for all x âˆˆ dom Ï† and y âˆˆ int(dom Ï†). We have BÏ†(xky) â‰¥ 0 for any x âˆˆ dom Ï† and
y âˆˆ int(dom Ï†). If in addition Ï† is strictly convex on int(dom Ï†), then BÏ†(xky) = 0 if and
only if x = y. Bregman divergences are also always convex in the first argument, and are
invariant under adding an arbitrary affine term to their generator.
Bregman divergences are not symmetric and do not verify the triangle inequality in
general, and thus are not necessarily distances in the strict sense. However, they still
enjoy some nice geometrical properties that somehow generalize the Euclidean geometry.
In particular, they verify a four-point identity similar to a parallelogram law:
BÏ†(xky) + BÏ†(x
0
ky
0
) = BÏ†(x
0
ky) + BÏ†(xky
0
) âˆ’ hx âˆ’ x
0
, âˆ‡Ï†(y) âˆ’ âˆ‡Ï†(y
0
)i , (20)
7
Dessein, Papadakis and Rouas
for all x, x
0 âˆˆ dom Ï† and y, y
0 âˆˆ int(dom Ï†). A special instance of this relation gives rise to
a three-point property similar to a triangle law of cosines:
BÏ†(xky) = BÏ†(xky
0
) + BÏ†(y
0
ky) âˆ’ hx âˆ’ y
0
, âˆ‡Ï†(y) âˆ’ âˆ‡Ï†(y
0
)i , (21)
for all x âˆˆ dom Ï† and y, y
0 âˆˆ int(dom Ï†).
Suppose now that Ï† is of Legendre type, and let C âŠ† E be a closed convex set such that
C âˆ© int(dom Ï†) 6= âˆ…. Then, for any point y âˆˆ int(dom Ï†), the following problem:
PC(y) = argmin
xâˆˆC
BÏ†(xky) , (22)
has a unique solution, then called the Bregman projection of y onto C. This solution actually
belongs to C âˆ© int(dom Ï†), and is also characterized as the unique point y
0 âˆˆ C âˆ© int(dom Ï†)
that verifies the variational relation:
hx âˆ’ y
0
, âˆ‡Ï†(y) âˆ’ âˆ‡Ï†(y
0
)i â‰¤ 0 , (23)
for all x âˆˆ C âˆ© dom Ï†. This characterization is equivalent to a well-known generalized
Pythagorean theorem for Bregman divergences, which states that the Bregman projection
of y onto C is the unique point y
0 âˆˆ C âˆ© int(dom Ï†) that verifies the following inequality:
BÏ†(xky) â‰¥ BÏ†(xky
0
) + BÏ†(y
0
ky) , (24)
for all x âˆˆ C âˆ© dom Ï†. When C is further an affine subspace, or more generally when the
Bregman projection further belongs to ri(C), the scalar product actually vanishes:
hx âˆ’ y
0
, âˆ‡Ï†(y) âˆ’ âˆ‡Ï†(y
0
)i = 0 , (25)
leading to an equality in the generalized Pythagorean theorem:
BÏ†(xky) = BÏ†(xky
0
) + BÏ†(y
0
ky) . (26)
A famous example of Bregman divergence is the Kullback-Leibler divergence, defined
for matrices Ï€ âˆˆ R
dÃ—d
+ and Î¾ âˆˆ R
dÃ—d
++ as follows:
K(Ï€kÎ¾) = X
d
i=1
X
d
j=1

Ï€ij log 
Ï€ij
Î¾ij 
âˆ’ Ï€ij + Î¾ij
. (27)
This divergence is generated by a function of Legendre type for Ï€ âˆˆ R
dÃ—d
+ given by minus
the Boltzmann-Shannon entropy:
E(Ï€) = K(Ï€k1) = X
d
i=1
X
d
j=1
(Ï€ij log(Ï€ij ) âˆ’ Ï€ij + 1) , (28)
with the convention 0 log(0) = 0. Another well-known example is the Itakura-Saito divergence, defined for matrices Ï€, Î¾ âˆˆ R
dÃ—d
++ as follows:
I(Ï€kÎ¾) = X
d
i=1
X
d
j=1

Ï€ij
Î¾ij
âˆ’ log 
Ï€ij
Î¾ij 
âˆ’ 1

. (29)
8
Rot Moverâ€™s Distance
This divergence is generated by a function of Legendre type for Ï€ âˆˆ R
dÃ—d
++ given by minus
the Burg entropy:
F(Ï€) = X
d
i=1
X
d
j=1
(Ï€ij âˆ’ log Ï€ij âˆ’ 1) . (30)
On the one hand, these examples belong to a particular type of so-called separable
Bregman divergences between matrices on R
dÃ—d
, that can be seen as the aggregation of
element-wise Bregman divergences between scalars on R:
BÏ†(Ï€kÎ¾) = X
d
i=1
X
d
j=1
BÏ†ij (Ï€ijkÎ¾ij ) , (31)
Ï†(Ï€) = X
d
i=1
X
d
j=1
Ï†ij (Ï€ij ) . (32)
Often, all element-wise generators Ï†ij are chosen equal, and are thus simply written as Ï† with
a slight abuse of notation. Other examples of such divergences are discussed in Section 5,
and include the logistic loss function generated by minus the Fermi-Dirac entropy, or the
squared Euclidean distance generated by the Euclidean norm.
On the other hand, a classical example of non-separable Bregman divergence is half the
squared Mahalanobis distance, defined for matrices Ï€, Î¾ âˆˆ R
dÃ—d as follows:
M(Ï€kÎ¾) = 1
2
vec(Ï€ âˆ’ Î¾)
>P vec(Ï€ âˆ’ Î¾) , (33)
for a positive-definite matrix P âˆˆ R
d
2Ã—d
2
. This divergence is generated by a function of
Legendre type for Ï€ âˆˆ R
dÃ—d given by a quadratic form:
Q(Ï€) = 1
2
vec(Ï€)
>P vec(Ï€) . (34)
This example is also discussed in Section 5.
2.3 Alternate Bregman Projections
Let Ï† be a function of Legendre type with Fenchel conjugate Ï†
? = Ïˆ. In general, computing
Bregman projections onto an arbitrary closed convex set C âŠ† E such that C âˆ©int(dom Ï†) 6= âˆ…
is nontrivial. Sometimes, it is possible to decompose C into the intersection of finitely many
closed convex sets:
C =
\s
l=1
Cl
, (35)
where the individual Bregman projections onto the respective sets C1, . . . , Cs are easier
to compute. It is then possible to obtain the Bregman projection onto C by alternate
projections onto C1, . . . , Cs according to Dykstraâ€™s algorithm.
In more detail, let Ïƒ : N â†’ {1, . . . , s} be a control mapping that determines the sequence
of subsets onto which we project. For a given point x0 âˆˆ C âˆ© int(dom Ï†), the Bregman
9
Dessein, Papadakis and Rouas
projection PC(x0) of x0 onto C can be approximated with Dykstraâ€™s algorithm by iterating
the following updates:
xk+1 â† PCÏƒ(k)
(âˆ‡Ïˆ(âˆ‡Ï†(xk) + y
Ïƒ(k)
)) , (36)
where the correction terms y
1
, . . . , y
s
for the respective subsets are initialized with the null
element of E, and are updated after projection as follows:
y
Ïƒ(k) â† y
Ïƒ(k) + âˆ‡Ï†(xk) âˆ’ âˆ‡Ï†(xk+1) . (37)
Under some technical conditions, the sequence of updates (xk)kâˆˆN
then converges in norm
to PC(x0) with a linear rate. Several sets of such conditions have been studied, notably by
Tseng (1993), Bauschke and Lewis (2000), Dhillon and Tropp (2007).
We here use the conditions proposed by Dhillon and Tropp (2007), which reveal to
be the less restrictive ones in our framework. Specifically, the convergence of Dykstraâ€™s
algorithm is guaranteed as soon as the function Ï† is cofinite, the constraint qualification
ri(C1) âˆ© Â· Â· Â· âˆ© ri(Cs) âˆ© int(dom Ï†) 6= âˆ… holds, and the control mapping Ïƒ is essentially cyclic,
that is, there exists a number t âˆˆ N such that Ïƒ takes each output value at least once during
any t consecutive input values. If a given Cl
is a polyhedral set, then the relative interior
can be dropped from the constraint qualification. Hence, when all subsets Cl are polyhedral,
the constraint qualification simply reduces to C âˆ© int(dom Ï†) 6= âˆ…, which is already enforced
for the definition of Bregman projections.
Finally, if all subsets Cl are further affine, then we can relax other assumptions. Notably,
we do not require Ï† to be cofinite (18), or equivalently dom Ïˆ = E, but only dom Ïˆ to be
open. The control mapping need not be essentially cyclic anymore, as long as it takes each
output value an infinite number of times. More importantly, we can completely drop the
correction terms from the updates, leading to a simpler technique known as projections
onto convex sets (POCS):
xk+1 â† PCÏƒ(k)
(xk) . (38)
2.4 Newton-Raphson Method
Let f be a continuously differentiable scalar function on an open interval I âŠ† R. Assume
f is increasing on a non-empty closed interval [x
âˆ’, x+] âŠ‚ I, and write y
âˆ’ = f(x
âˆ’) and
y
+ = f(x
+). Then, for any y âˆˆ [y
âˆ’, y+], the equation f(x) = y has at least one solution
x
? âˆˆ [x
âˆ’, x+]. Such a solution can be approximated by iterative updates according to the
Newton-Raphson method:
x â† max 
x
âˆ’, min 
x
+, x âˆ’
f(x) âˆ’ y
f
0(x)
 , (39)
where the fraction takes infinite values when f
0
(x) = 0 and f(x) 6= y, and a null value
by convention when f
0
(x) = 0 and f(x) = y. It is well-known that the Newton-Raphson
method converges to a solution x
? as soon as x is initialized sufficiently close to x
?
. Convergence is then quadratic provided that f
0
(x
?
) 6= 0. However, this local convergence has
little importance in practice because it is hard to quantify the required proximity to the
solution.
10
Rot Moverâ€™s Distance
(A) Affine constraints (B) Polyhedral constraints
(A1) Ï† is of Legendre type. (B1) Ï† is of Legendre type.
(A2) (0, 1)dÃ—d âŠ† dom Ï†. (B2) (0, 1)dÃ—d âŠ† dom Ï†.
(A3) dom Ï† âŠ† R
dÃ—d
+ . (B3) dom Ï† * RdÃ—d
+ .
(A4) dom Ïˆ is open. (B4) dom Ïˆ = R
dÃ—d
.
(A5) R
dÃ—d
âˆ’ âŠ‚ dom Ïˆ.
Table 1: Set of assumptions for the considered regularizers Ï†.
Thorlund-Petersen (2004) elucidated results on global convergence of the Newton-Raphson
method. He proved a necessary and sufficient condition of convergence for an arbitrary
value y âˆˆ [y
âˆ’, y+] and from any starting point x âˆˆ [x
âˆ’, x+]. This condition is that for any
a, b âˆˆ [x
âˆ’, x+], f(b) > f(a) implies:
f
0
(a) + f
0
(b) >
f(b) âˆ’ f(a)
b âˆ’ a
. (40)
In particular, a sufficient condition is that the underlying function f is an increasing convex or increasing concave function on [x
âˆ’, x+], or can be decomposed as the sum of such
functions. In addition, if f satisfies the necessary and sufficient condition and is strictly increasing with f
0
(x) > 0 for all x âˆˆ [x
âˆ’, x+], then initializing with a boundary point x
âˆ’ 6= x
?
or x
+ 6= x
?
ensures that the entire sequence of updates is interior to (x
âˆ’, x+), so that we
can actually drop the min and max truncation operators in the updates:
x â† x âˆ’
f(x) âˆ’ y
f
0(x)
. (41)
3. Mathematical Formulation
In this section, we develop a unified framework to define ROT problems. We start by
drawing some technical assumptions for our generalized framework to hold (Section 3.1).
We then formulate primal ROT problems and study their properties (Section 3.2). We
also formulate dual ROT problems and discuss their properties in relation to primal ones
(Section 3.3). Finally, we provide some geometrical insights to summarize our developments
in the light of information geometry (Section 3.4).
3.1 Technical Assumptions
Some mild technical assumptions are required on the convex regularizer Ï† and its Fenchel
conjugate Ïˆ = Ï†
âˆ—
for the proposed framework to hold. Some assumptions relate to required
conditions for the definition of Bregman projections and convergence of the algorithms, while
others are more specific to ROT problems. In our framework, we also need to distinguish
between two situations where the underlying closed convex set can be described as the
intersection of either affine subspaces or polyhedral subsets. The two sets of assumptions
(A) and (B) are summarized in Table 1.
For the first assumptions (A1) and (B1), we recall that a closed proper convex function
is of Legendre type if and only if it is essentially smooth and strictly convex on the interior
11
Dessein, Papadakis and Rouas
of its domain (Section 2.1). This is required for the definition of Bregman projections (Section 2.2). In addition, it guarantees the existence of dual coordinate systems on int(dom Ï†)
and int(dom Ïˆ) via the homeomorphism âˆ‡Ï† = âˆ‡Ïˆ
âˆ’1
:
Ï€(Î¸) = âˆ‡Ïˆ(Î¸) , (42)
Î¸(Ï€) = âˆ‡Ï†(Ï€) . (43)
With a slight abuse of notation, we omit the reparameterization to simply denote corresponding primal and dual parameters by Ï€ and Î¸.
The second assumptions (A2) and (B2) imply that ri(Î (p, q)) âŠ‚ dom Ï† and ensure the
constraint qualification Î (p, q)âˆ©int(dom Ï†) 6= âˆ… for Bregman projection onto the transport
polytope, independently of the input distributions p, q as long as they do not have null or
unit entries. We assume hereafter that this implicitly holds, and discuss in the practical
considerations (Section 4.6) how our methods actually generalize to deal explicitly with null
or unit entries in the input distributions.
The third assumptions (A3) and (B3) separate between two cases depending on whether
dom Ï† lies within the non-negative orthant or not for the alternate Bregman projections
(Section 2.3). In the former case, non-negativity is already ensured by the domain of the
regularizer, so that the underlying closed convex set is made of two affine subspaces for the
row and column sum constraints, and the POCS method can be considered. The fourth
assumption (A4) thus requires that dom Ïˆ be open for convergence of this algorithm. In the
latter case, there is one additional polyhedral subset for the non-negative constraints and
Dykstraâ€™s algorithm should be used. The fourth assumption (B4) hence further requires
that dom Ïˆ = R
dÃ—d
, or equivalently that Ï† be cofinite (18), for convergence. In both cases,
we remark that we necessarily have dom Ïˆ = dom âˆ‡Ïˆ.
The fifth assumption (A5) in the affine constraints ensures that âˆ’Î³/Î» belongs to dom âˆ‡Ïˆ
for definition of ROT problems, independently of the non-negative cost matrix Î³ and positive regularization term Î». Notice that this is already guaranteed by the fourth assumption
in the polyhedral constraints. We also show in the sparse extension (Section 4.5) how to
deal with infinite entries in the cost matrix Î³ for separable regularizers, so as to enforce
null entries in the regularized optimal plan.
On the one hand, some common regularizers under assumptions (A) are the BoltzmannShannon entropy associated to the Kullback-Leibler divergence, the Burg entropy associated
to the Itakura-Saito divergence, and the Fermi-Dirac entropy associated to the logistic loss
function. To solve the underlying ROT problems, we employ our method called ASA
based on the POCS technique, where alternate Bregman projections onto the two affine
subspaces for the row and column sum constraints are considered (Section 4.3). On the
other hand, examples under assumptions (B) include the Euclidean norm associated to the
Euclidean distance, and the quadratic form associated to the Mahalanobis distance. For
these ROT problems, we use our second method called NASA based on Dykstraâ€™s algorithm,
where correction terms and a further Bregman projection onto the polyhedral non-negative
orthant are needed (Section 4.4).
3.2 Primal Problem
We start our primal formulation with the following lemmas and definition for the RMD.
12
Rot Moverâ€™s Distance
Lemma 1 The regularizer Ï† attains its global minimum uniquely at Î¾
0 = âˆ‡Ïˆ(0).
Proof Using the assumptions (A4) and (A5), respectively (B4), we have that 0 âˆˆ dom Ïˆ =
int(dom Ïˆ). Thus, there exists a unique Î¾
0 âˆˆ int(dom Ï†) such that âˆ‡Ï†(Î¾
0
) = 0, or equivalently Î¾
0 = âˆ‡Ïˆ(0), via the homeomorphism âˆ‡Ï† = âˆ‡Ïˆ
âˆ’1
ensured by assumption (A1),
respectively (B1). Hence, Ï† attains its global minimum uniquely at Î¾
0 by strict convexity
on int(dom Ï†).
Lemma 2 The restriction of the regularizer Ï† to the transport polytope Î (p, q) attains its
global minimum uniquely at the Bregman projection Ï€
0 of Î¾
0
onto Î (p, q).
Proof Using the assumption (A2), respectively (B2), we have that Î (p, q)âˆ©int(dom Ï†) 6= âˆ….
Since Î¾
0 âˆˆ int(dom Ï†) and Î (p, q) is a closed convex set, the Bregman projection Ï€
0 of Î¾
0
onto Î (p, q) according to the function Ï† of Legendre type is well-defined. Moreover, it is
characterized by the variational relation (23) as follows:
hÏ€ âˆ’ Ï€
0
, âˆ‡Ï†(Ï€
0
)i â‰¥ 0 , (44)
for all Ï€ âˆˆ Î (p, q) âˆ© dom Ï†. We also have BÏ†(Ï€kÏ€
0
) > 0 when Ï€ 6= Ï€
0 by strict convexity
of Ï† on int(dom Ï†). As a result, we have:
Ï†(Ï€) âˆ’ Ï†(Ï€
0
) > hÏ€ âˆ’ Ï€
0
, âˆ‡Ï†(Ï€
0
)i . (45)
Combining the two inequalities, we obtain Ï†(Ï€) > Ï†(Ï€
0
) and the restriction of Ï† to Î (p, q)
attains its global minimum uniquely at Ï€
0
.
Lemma 3 The restriction of the cost hÂ·, Î³i to the regularized transport polytope:
Î Î±,Ï†(p, q) = {Ï€ âˆˆ Î (p, q): Ï†(Ï€) â‰¤ Ï†(Ï€
0
) + Î±} , (46)
where Î± â‰¥ 0, attains its global minimum.
Proof The regularized transport polytope is the intersection of the compact set Î (p, q)
with a lower level set of Ï† which is also closed since Ï† is closed. Hence, Î Î±,Ï†(p, q) is compact and the restriction of hÂ·, Î³i to Î Î±,Ï†(p, q) attains its global minimum by continuity on
a compact set.
Definition 4 The primal rot moverâ€™s distance is the quantity defined as:
d
0
Î³,Î±,Ï†(p, q) = min
Ï€âˆˆÎ Î±,Ï†(p,q)
hÏ€, Î³i . (47)
A minimizer Ï€
0?
Î± is then called a primal rot moverâ€™s plan.
13
Dessein, Papadakis and Rouas
Remark 1 For the sake of notation, we omit the dependence on p, q, Î³, Ï† in the index of
primal rot moverâ€™s plans Ï€
0?
Î±.
The regularization enforces the associated minimizers to have small enough Bregman
information Ï†(Ï€
0?
Î±) â‰¤ Ï†(Ï€
0
) + Î± compared to the minimal one Ï†(Ï€
0
) for transport plans.
We also have a geometrical interpretation where the solutions are constrained to a Bregman
ball whose center Î¾
0
is the matrix with minimal Bregman information.
Proposition 5 The regularized transport polytope is the intersection of the transport polytope with the Bregman ball of radius BÏ†(Ï€
0kÎ¾
0
) + Î± and center Î¾
0
:
Î Î±,Ï†(p, q) = {Ï€ âˆˆ Î (p, q): BÏ†(Ï€kÎ¾
0
) â‰¤ BÏ†(Ï€
0
kÎ¾
0
) + Î±} . (48)
Proof Expanding the Bregman divergences from their definition (19), we obtain:
BÏ†(Ï€kÎ¾
0
) = Ï†(Ï€) âˆ’ Ï†(Î¾
0
) âˆ’ hÏ€ âˆ’ Î¾
0
, âˆ‡Ï†(Î¾
0
)i , (49)
BÏ†(Ï€
0
kÎ¾
0
) = Ï†(Ï€
0
) âˆ’ Ï†(Î¾
0
) âˆ’ hÏ€
0 âˆ’ Î¾
0
, âˆ‡Ï†(Î¾
0
)i . (50)
Since âˆ‡Ï†(Î¾
0
) = 0, the last terms with scalar products vanish, leading to:
Ï†(Ï€) âˆ’ Ï†(Ï€
0
) = BÏ†(Ï€kÎ¾
0
) âˆ’ BÏ†(Ï€
0
kÎ¾
0
) . (51)
Therefore, in the definition (46) of Î Î±,Ï†(p, q), we have Ï†(Ï€) â‰¤ Ï†(Ï€
0
) + Î± if and only if Ï€ is
in the Bregman ball of radius BÏ†(Ï€
0kÎ¾
0
) + Î± and center Î¾
0
.
Under some additional conditions, this geometrical interpretation still holds with a Bregman ball whose center Ï€
0 has minimal Bregman information for transport plans.
Proposition 6 If Ï€
0 âˆˆ ri(Î (p, q)), then the regularized transport polytope is the intersection of the transport polytope with the Bregman ball of radius Î± and center Ï€
0
:
Î Î±,Ï†(p, q) = {Ï€ âˆˆ Î (p, q): BÏ†(Ï€kÏ€
0
) â‰¤ Î±} . (52)
Proof Since Ï€
0 âˆˆ ri(Î (p, q)), there is equality in the generalized Pythagorean theorem (26):
BÏ†(Ï€kÎ¾
0
) = BÏ†(Ï€kÏ€
0
) + BÏ†(Ï€
0
kÎ¾
0
) . (53)
The regularized transport polytope as seen from (48) is then the intersection of the transport polytope Î (p, q) with the Bregman ball of radius Î± and center Ï€
0
.
Remark 2 The proposition also holds trivially when the global minimum is attained on the
transport polytope, that is, when Î¾
0 = Ï€
0
.
Corollary 7 Under assumptions (A), the regularized transport polytope is the intersection
of the transport polytope with the Bregman ball of radius Î± and center Ï€
0
:
Î Î±,Ï†(p, q) = {Ï€ âˆˆ Î (p, q): BÏ†(Ï€kÏ€
0
) â‰¤ Î±} . (54)
14
Rot Moverâ€™s Distance
Proof This is a result of Ï€
0 âˆˆ Î (p, q) âˆ© int(dom Ï†) = ri(Î (p, q)) when dom Ï† âŠ† R
dÃ—d
+ . Indeed, we then have ri(Î (p, q)) âŠ‚ Î (p, q) and ri(Î (p, q)) âŠ‚ int(dom Ï†), so that ri(Î (p, q)) âŠ†
Î (p, q) âˆ© int(dom Ï†). Conversely, let Ï€ âˆˆ Î (p, q) âˆ© int(dom Ï†) so that Ï€ âˆˆ R
dÃ—d
++ . Then,
for a given Ï€ âˆˆ Î (p, q), let us pose Ï€Î» = Î»Ï€ + (1 âˆ’ Î»)Ï€ for Î» > 1. We easily have
Ï€Î»1 = p and Ï€
>
Î»
1 = q. Moreover, since all entries of Ï€ are positive and that of Ï€ are
non-negative, we can always choose a given Î» sufficiently close to 1 such that Ï€Î» âˆˆ R
dÃ—d
+ .
We then have Ï€Î» âˆˆ Î (p, q) so that Ï€ âˆˆ ri(Î (p, q)) as characterized by (15), and thus
Î (p, q) âˆ© int(dom Ï†) âŠ† ri(Î (p, q)).
Remark 3 Under assumptions (B), the Bregman projection Ï€
0 does not necessarily lie
within ri(Î (p, q)). Hence, the geometrical interpretation in terms of a Bregman ball might
break down, although the solutions are still constrained to have a small enough Bregman
information above that of Ï€
0
.
Although Sinkhorn distances verify the triangular inequality when Î³ is a distance matrix, thanks to specific chain rules and information inequalities for the Bolzmann-Shannon
entropy and Kullback-Leibler divergence, it is not necessarily the case for the RMD with
other regularizations, even for separable regularizers. Hence, the RMD does not provide
a true distance metric on Î£d in general even if Î³ is a distance matrix. Nonetheless, the
RMD is symmetric as soon as Ï† is invariant by transposition, which holds for separable
regularizers Ï†ij = Ï†, and Î³ is symmetric. We now study some properties of the RMD that
hold for general regularizers.
Property 1 The primal rot moverâ€™s distance d
0
Î³,Î±,Ï†(p, q) is a decreasing convex and continuous function of Î±.
Proof The fact that it is decreasing is a direct consequence of the regularized transport
polytope Î Î±,Ï†(p, q) growing with Î±. The convexity can be proved as follows. Let Î±0, Î±1 â‰¥ 0,
and 0 < Î» < 1. We pose Î±Î» = (1 âˆ’ Î»)Î±0 + Î»Î±1 â‰¥ 0. We also choose arbitrary rot moverâ€™s
plans Ï€
0?
Î±0
,Ï€
0?
Î±1
,Ï€
0?
Î±Î»
. We finally pose Ï€Î» = (1 âˆ’ Î»)Ï€
0?
Î±0 + Î»Ï€
0?
Î±1
. By convexity of Ï†, we
have:
Ï†(Ï€Î») â‰¤ (1 âˆ’ Î»)Ï†(Ï€
0?
Î±0
) + Î»Ï†(Ï€
0?
Î±1
) (55)
â‰¤ (1 âˆ’ Î»)(Î±0 + Ï†(Ï€
0
)) + Î»(Î±1 + Ï†(Ï€
0
)) (56)
= Î±Î» + Ï†(Ï€
0
) . (57)
Hence, Ï€Î» âˆˆ Î Î±Î»,Ï†(p, q), and by construction we have hÏ€
0?
Î±Î»
, Î³i â‰¤ hÏ€Î», Î³i, or equivalently:
hÏ€
0?
Î±Î»
, Î³i â‰¤ (1 âˆ’ Î»)hÏ€
0?
Î±0
, Î³i + Î»hÏ€
0?
Î±1
, Î³i . (58)
The continuity for Î± > 0 is a direct consequence of convexity for Î± > 0, since a convex
function is always continuous on the relative interior of its domain. Lastly, the continuity at
Î± = 0 can be seen as follows. Let (Î±k)kâˆˆN
be a sequence of positive numbers that converges
to 0. We choose arbitrary rot moverâ€™s plans (Ï€
0?
Î±k
)
kâˆˆN
. By compactness of Î (p, q), we can
extract a subsequence of rot moverâ€™s plans that converges in norm to a point Ï€
0? âˆˆ Î (p, q).
15
Dessein, Papadakis and Rouas
For the sake of simplicity, we do not relabel this subsequence. By construction, we have
Ï†(Ï€
0
) â‰¤ Ï†(Ï€
0?
Î±k
) â‰¤ Ï†(Ï€
0
) + Î±k, and Ï†(Ï€
0?
Î±k
) converges to Ï†(Ï€
0
). By lower semi-continuity
of Ï†, we thus have Ï†(Ï€
0?
) â‰¤ Ï†(Ï€
0
). Since the global minimum of Ï† on Î (p, q) is attained
uniquely at Ï€
0
, we must have Ï€
0? = Ï€
0
, and the original sequence also converges in norm
to Ï€
0
. By continuity of the total cost hÂ·, Î³i on R
dÃ—d
, hÏ€
0?
Î±k
, Î³i converges to hÏ€
0
, Î³i. Hence,
the limit of the RMD when Î± tends to 0 from above is hÏ€
0
, Î³i, which equals the RMD for
Î± = 0 as shown in the next property.
Property 2 When Î± = 0, the primal rot moverâ€™s distance reduces to:
d
0
Î³,0,Ï†(p, q) = hÏ€
0
, Î³i , (59)
and the unique primal rot moverâ€™s plan is the transport plan with minimal Bregman information:
Ï€
0?
0 = Ï€
0
. (60)
Proof Since Ï€
0
is the unique global minimizer of Ï† on Î (p, q), the regularized transport
polytope reduces to the singleton Î 0,Ï†(p, q) = {Ï€ âˆˆ Î (p, q): Ï†(Ï€) â‰¤ Ï†(Ï€
0
)} = {Ï€
0}. The
property follows immediately.
Property 3 When Î± tends to +âˆž, the primal rot moverâ€™s distance converges to the earth
moverâ€™s distance:
lim Î±â†’+âˆž
d
0
Î³,Î±,Ï†(p, q) = dÎ³(p, q) . (61)
Proof Let Ï€
? âˆˆ Î (p, q) be an earth moverâ€™s plan so that dÎ³(p, q) = hÏ€
?
, Î³i. By continuity
of the total cost hÂ·, Î³i on R
dÃ—d
, we have that for all  > 0, there exists an open neighborhood
of Ï€
?
such that hÏ€, Î³i â‰¤ hÏ€
?
, Î³i+ for any transport plan Ï€ within this neighborhood. We
can always choose a transport plan such that Ï€ âˆˆ ri(Î (p, q)). Since ri(Î (p, q)) âŠ‚ dom Ï†,
Ï†(Ï€) is finite and we can fix Î± = Ï†(Ï€) âˆ’ Ï†(Ï€
0
) â‰¥ 0. Hence, Ï€ âˆˆ Î Î±,Ï†(p, q) for any Î± â‰¥ Î±,
and we have dÎ³(p, q) â‰¤ d
0
Î³,Î±,Ï†(p, q) â‰¤ hÏ€, Î³i â‰¤ dÎ³(p, q) + .
Property 4 If [0, 1)dÃ—d âŠ† dom Ï†, then there exists a minimal Î±
0 â‰¥ 0 such that for all
Î± â‰¥ Î±
0
, the primal rot moverâ€™s distance reduces to the earth moverâ€™s distance:
d
0
Î³,Î±,Ï†(p, q) = dÎ³(p, q) . (62)
Proof The extra condition guarantees that Î (p, q) âŠ‚ dom Ï†, and thus that Ï† is bounded
on the closed set Î (p, q). The property is then a direct consequence of Î Î±,Ï†(p, q) = Î (p, q)
for Î± large enough.
16
Rot Moverâ€™s Distance
Property 5 If [0, 1)dÃ—d âŠ† dom Ï† and Ï† is strictly convex on [0, 1)dÃ—d
, then the unique
primal rot moverâ€™s plan for Î± = Î±
0
is the earth moverâ€™s plan Ï€
?
0 with minimal Bregman
information:
Ï€
0?
Î±0 = Ï€
?
0
. (63)
Proof First, we recall that the set of earth moverâ€™s plans Ï€
?
is either a single vertex or
a whole facet of Î (p, q). Hence, it forms a closed convex subset in Î (p, q), and there is
a unique earth moverâ€™s plan Ï€
?
0 with minimal Bregman information by strict convexity of
Ï† on this subset. Second, it is trivial that all primal rot moverâ€™s plan Ï€
0?
Î±0 must be earth
moverâ€™s plans. If there is a single vertex as earth moverâ€™s plan, then the property follows
immediately. Otherwise, we can see the property geometrically as follows. The whole facet
of earth moverâ€™s plans is orthogonal to Î³. Nevertheless, by strict convexity of Ï† on [0, 1)dÃ—d
,
the facet must be tangent to Î Î±0
,Ï†(p, q) at the unique earth moverâ€™s plan Ï€
?
0 with minimal
Bregman information Ï†(Ï€
?
0
) = Ï†(Ï€
0
)+Î±
0
, and Ï€
?
0
is also the rot moverâ€™s plan Ï€
0?
Î±0. Another
way to prove the property more formally is as follows. Suppose that a primal rot moverâ€™s
plan Ï€
?
is not the earth moverâ€™s plan with minimal Bregman information. We thus have
Ï†(Ï€
?
0
) < Ï†(Ï€
?
) â‰¤ Ï†(Ï€
0
) + Î±
0
. We can then choose a smaller Î±
0
such that Ï†(Ï€
?
0
) â‰¤ Ï†(Ï€
0
) + Î±
0
and the RMD still equals the EMD for this smaller value, and actually all values in between
by monotonicity. This leads to a contradiction and Ï€
?
0 must be the earth moverâ€™s plan with
minimal Bregman information.
Remark 4 When Î± > Î±0
, the regularized transport polytope might grow to include several
earth moverâ€™s plans with different Bregman information, which are then all minimizers
for the RMD. When we do not have strict convexity outside (0, 1)dÃ—d
, there might also be
multiple earth moverâ€™s plans with minimal Bregman information.
If [0, 1)dÃ—d âŠ† dom Ï†, then it is easy to check that the strict convexity of Ï† on [0, 1)dÃ—d
is always verified when Ï† is separable under assumptions (A) or (B), or when [0, 1)dÃ—d âŠ‚
int(dom Ï†) under assumptions (B). This holds for almost all typical regularizers, notably
for all regularizers considered in this paper except from minus the Burg entropy as defined
in (30) and associated to the Itakura-Saito divergence in (29). For this latter regularizer, the
solutions for an increasing Î± all lie within ri(Î (p, q)), and the RMD never reaches the EMD.
In such cases where the minimal Î±
0 does not exist, we can use the convention Î±
0 = +âˆž
since the RMD always converges to the EMD in the limit when Î± tends to +âˆž. We can
then prove that there is a unique rot moverâ€™s plan Ï€
0?
Î± as long as 0 < Î± < Î±0
, which can be
seen informally as follows. The solutions geometrically lie at the intersection of Î Î±,Ï†(p, q)
and of a supporting hyperplane with normal Î³. By strict convexity of Ï† on ri(Î (p, q)),
this intersection is a singleton inside the polytope. When the intersection reaches a facet,
the only facet that can coincide locally with the hyperplane is the one that contains the
earth moverâ€™s plans. Hence, we also have a singleton on the boundary of the polytope before
reaching an earth moverâ€™s plan. We formally prove this uniqueness result next by exploiting
duality.
3.3 Dual Problem
We now present the following two lemmas before defining our dual formulation for the RMD.
17
Dessein, Papadakis and Rouas
Lemma 8 The regularized cost hÂ·, Î³i + Î»Ï†(Â·), where Î» > 0, attains its global minimum
uniquely at Î¾ = âˆ‡Ïˆ(âˆ’Î³/Î»).
Proof The regularized cost is convex with same domain as Ï†, and is strictly convex on
int(dom Ï†). Thus, it attains its global minimum at a unique point Î¾ âˆˆ int(dom Ï†) if and
only if Î³ + Î»âˆ‡Ï†(Î¾) = 0, or equivalently âˆ‡Ï†(Î¾) = âˆ’Î³/Î». By assumptions (A4) and (A5),
respectively (B4), âˆ’Î³/Î» âˆˆ dom âˆ‡Ïˆ, so that the global minimum is attained uniquely at
Î¾ = âˆ‡Ïˆ(âˆ’Î³/Î») in virtue of the homeomorphism in (42) and (43).
Lemma 9 The restriction of the regularized cost hÂ·, Î³i + Î»Ï†(Â·) to the transport polytope
Î (p, q) attains its global minimum uniquely.
Proof We notice that the regularized cost is equal to a Bregman divergence up to a positive
factor and additive constant:
hÏ€, Î³i + Î»Ï†(Ï€) âˆ’ Î»Ï†(Î¾) = Î»BÏ†(Ï€kÎ¾) . (64)
Hence, its minimization over the closed convex set Î (p, q) is equivalent to the Bregman
projection of Î¾ âˆˆ int(dom Ï†) onto Î (p, q) according to the function Ï† of Legendre type.
Since Î (p, q) âˆ© int(dom Ï†) 6= âˆ…, this projection exists and is unique.
Definition 10 The dual rot moverâ€™s distance is the quantity defined as:
dÎ³,Î»,Ï†(p, q) = hÏ€
?
Î»
, Î³i , (65)
where the dual rot moverâ€™s plan Ï€
?
Î»
is given by:
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
hÏ€, Î³i + Î»Ï†(Ï€) . (66)
Remark 5 For the sake of notation, we omit the dependence on p, q, Î³, Ï† in the index of
dual rot moverâ€™s plans Ï€
?
Î»
.
We proceed with the following proposition that enlightens the relation between the RMD
and associated Bregman divergence.
Proposition 11 The dual rot moverâ€™s plan is the Bregman projection of Î¾ onto the transport polytope:
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
BÏ†(Ï€kÎ¾) . (67)
Proof This is a consequence of the proof for Lemma 9. Indeed, from the definition in (66),
we see that the rot moverâ€™s plan also minimizes (64). Therefore, it is the unique Bregman
projection of Î¾ onto the transport polytope.
We have a geometrical interpretation where the regularization shrinks the solution toward the matrix Î¾
0
that has minimal Bregman information.
18
Rot Moverâ€™s Distance
Proposition 12 The dual rot moverâ€™s plan Ï€
?
Î»
can be obtained as:
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
hÏ€, Î³i + Î»BÏ†(Ï€kÎ¾
0
) . (68)
Proof Developing the Bregman divergence based on its definition (19), we have:
BÏ†(Ï€kÎ¾
0
) = Ï†(Ï€) âˆ’ Ï†(Î¾
0
) âˆ’ hÏ€ âˆ’ Î¾
0
, âˆ‡Ï†(Î¾
0
)i . (69)
Since âˆ‡Ï†(Î¾
0
) = 0, the last term with scalar product vanishes and we are left out with Ï†(Ï€)
plus a constant term with respect to Ï€. Hence, we can replace Ï†(Ï€) by BÏ†(Ï€kÎ¾
0
) in the
minimization (66) that defines Ï€
?
Î»
.
Under some additional conditions, this interpretation can also be seen as shrinking
toward the transport plan Ï€
0 with minimal Bregman information.
Proposition 13 If Ï€
0 âˆˆ ri(Î (p, q)), then the dual rot moverâ€™s plan Ï€
?
Î»
can be obtained as:
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
hÏ€, Î³i + Î»BÏ†(Ï€kÏ€
0
) . (70)
Proof If Ï€
0 âˆˆ ri(Î (p, q)), then we have equality in the generalized Pythagorean theorem (26), leading to:
BÏ†(Ï€kÎ¾
0
) = BÏ†(Ï€kÏ€
0
) + BÏ†(Ï€
0
kÎ¾
0
) . (71)
Since the last term is constant with respect to Ï€, we can replace BÏ†(Ï€kÎ¾
0
) by BÏ†(Ï€kÏ€
0
) in
the minimization (68) that characterizes Ï€
?
Î»
.
Remark 6 The proposition also holds trivially when the global minimum is attained on the
transport polytope, that is, when Î¾
0 = Ï€
0
.
Corollary 14 Under assumptions (A), the dual rot moverâ€™s plan Ï€
?
Î»
can be obtained as:
Ï€
?
Î» = argmin
Ï€âˆˆÎ (p,q)
hÏ€, Î³i + Î»BÏ†(Ï€kÏ€
0
) . (72)
Proof This is a result of Ï€
0 âˆˆ Î (p, q) âˆ© int(dom Ï†) = ri(Î (p, q)) when dom Ï† âŠ† R
dÃ—d
+ , as
shown in the proof of Corollary 7.
In the sequel, we also extend naturally the definition of the dual RMD for Î» = 0 as the
EMD. We then do not necessarily have uniqueness of dual rot moverâ€™s plans for Î» = 0, and
the geometrical interpretation in terms of a Bregman projection does not hold anymore for
Î» = 0. However, we have the following theorem based on duality theory that shows the
equivalence between primal and dual ROT problems.
19
Dessein, Papadakis and Rouas
Theorem 15 For all Î± > 0, there exists Î» â‰¥ 0 such that the primal and dual rot moverâ€™s
distances are equal:
d
0
Î³,Î±,Ï†(p, q) = dÎ³,Î»,Ï†(p, q) . (73)
Moreover, if Î± < Î±0
, then a corresponding value is such that Î» > 0, and the primal and dual
rot moverâ€™s plans are unique and equal:
Ï€
0?
Î± = Ï€
?
Î»
. (74)
Proof The primal problem can be seen as the minimization p
? of the cost hÏ€, Î³i on
Î (p, q) subject to Ï†(Ï€) âˆ’ Ï†(Ï€
0
) âˆ’ Î± â‰¤ 0. The domain of this constrained convex problem
is D = Î (p, q) âˆ© dom Ï† 6= âˆ…. The Lagrangian on D Ã— R is given by L(Ï€, Î») = hÏ€, Î³i +
Î»(Ï†(Ï€) âˆ’ Ï†(Ï€
0
) âˆ’ Î±), and its minimization over D for a fixed Î» â‰¥ 0 has the same solutions
Ï€
? as the dual problem. In addition, Slaterâ€™s condition for convex problems, stating that
there is a strictly feasible point in the relative interior of the domain, is verified as long
as Î± > 0. Indeed, we have ri(D) = ri(Î (p, q)). The existence of a strictly feasible point
Ï†(Ï€) < Ï†(Ï€
0
) + Î± then holds by continuity of Ï† at Ï€
0 âˆˆ int(dom Ï†). As a result, we have
strong duality with a zero duality gap p
? = d
?
, where d
?
is the maximization of g(Î») subject
to Î» â‰¥ 0. Moreover, if d
?
is finite, then it is attained at least once at a point Î»
?
. This is
the case since we already know that p
?
is finite. Since p
?
is also attained at least once at a
point Ï€
?
solution of the primal problem, we have the following chain:
p
? = d
?
(75)
= min
Ï€âˆˆD
L(Ï€, Î»?
) (76)
â‰¤ L(Ï€
?
, Î»?
) (77)
= hÏ€
?
, Î³i + Î»
?
(Ï†(Ï€
?
) âˆ’ Ï†(Ï€
0
) âˆ’ Î±) (78)
â‰¤ hÏ€
?
, Î³i (79)
= p
?
. (80)
Therefore, all inequalities are in fact equalities, Ï€
? also minimizes the Lagrangian over D
and thus is a solution of the dual problem. In other words, the primal and dual RMD
for Î± and Î»
? are equal, and the primal solutions must be dual solutions too. For Î± < Î±0
,
the RMD has not reached the EMD yet, and thus we must have Î»
? > 0. Hence, the dual
solution is unique, so that the primal solution is unique too and equal to the dual one.
Remark 7 Corresponding values of Î± and Î» depend on p, q, Î³, Ï†. In addition, there might
be multiple values of Î» that correspond to a given Î±.
Again, the RMD does not verify the triangular inequality in general, and hence does not
provide a true distance metric on Î£d even if Î³ is a distance matrix. Nevertheless, we still
have the result that the RMD is symmetric as soon as Ï† is invariant by transposition, which
holds for separable regularizers Ï†ij = Ï†, and Î³ is symmetric. We also obtain properties for
the dual RMD that are similar to the ones for the primal RMD.
20
Rot Moverâ€™s Distance
Property 6 The dual rot moverâ€™s distance dÎ³,Î»,Ï†(p, q) is an increasing and continuous
function of Î».
Proof The fact that it is increasing can be seen as follows. Let 0 â‰¤ Î»1 < Î»2. By
construction, we have the following inequalities:
hÏ€
?
Î»1
, Î³i + Î»1Ï†(Ï€
?
Î»1
) â‰¤ hÏ€
?
Î»2
, Î³i + Î»1Ï†(Ï€
?
Î»2
) , (81)
hÏ€
?
Î»2
, Î³i + Î»2Ï†(Ï€
?
Î»2
) â‰¤ hÏ€
?
Î»1
, Î³i + Î»2Ï†(Ï€
?
Î»1
) . (82)
Subtracting these two inequalities, we obtain that Ï†(Ï€
?
Î»1
) â‰¥ Ï†(Ï€
?
Î»2
). Reinserting this result in the first inequality, we finally get hÏ€
?
Î»1
, Î³i â‰¤ hÏ€
?
Î»2
, Î³i. The continuity of the dual
RMD results from that of the primal RMD. Let Î» â‰¥ 0, and choose an arbitrary dual rot
moverâ€™s plan Ï€
?
Î»
and earth moverâ€™s plan Ï€
?
. On the one hand, we have hÏ€
?
, Î³i â‰¤ hÏ€
?
Î»
, Î³i.
On the other hand, we have hÏ€
?
Î»
, Î³i + Î»Ï†(Ï€
?
Î»
) â‰¤ hÏ€
0
, Î³i + Î»Ï†(Ï€
0
), and thus hÏ€
?
Î»
, Î³i â‰¤
hÏ€
0
, Î³i + Î»(Ï†(Ï€
0
) âˆ’ Ï†(Ï€
?
Î»
)) â‰¤ hÏ€
0
, Î³i. Suppose we have a discontinuity of the dual RMD at
Î». Then by monotonicity, there is a value hÏ€
?
, Î³i < d < hÏ€
0
, Î³i that is not in the image of
the dual RMD. But d is in the image of the primal RMD for a given Î± > 0 by continuity.
It means that @Î» > 0 such that hÏ€
?
Î»
, Ci = d, whereas, by continuity of the primal problem,
we know that there exist Î± > 0 such that hÏ€
?
Î±, Ci â‰¥ d. This is in contradiction with the
duality result in Theorem 15, which implies that the image of the primal RMD for Î± > 0
must be included in that of the dual RMD for Î» â‰¥ 0.
Property 7 When Î» tends to +âˆž, the dual rot moverâ€™s distance converges to:
lim
Î»â†’+âˆž
dÎ³,Î»,Ï†(p, q) = hÏ€
0
, Î³i , (83)
and the dual rot moverâ€™s plan converges in norm to the transport plan with minimal Bregman
information:
lim
Î»â†’+âˆž
Ï€
?
Î» = Ï€
0
. (84)
Proof Let (Î»k)kâˆˆN
be a sequence of positive numbers that tends to +âˆž, and (Ï€
?
Î»k
)
kâˆˆN
the associated rot moverâ€™s plans. By compactness of Î (p, q), we can extract a subsequence of rot moverâ€™s plans that converges in norm to a point Ï€
? âˆˆ Î (p, q). For
the sake of simplicity, we do not relabel this subsequence. By construction, we have
hÏ€
?
Î»k
, Î³i + Î»kÏ†(Ï€
0
) â‰¤ hÏ€
?
Î»k
, Î³i + Î»kÏ†(Ï€
?
Î»k
) â‰¤ hÏ€
0
, Î³i + Î»kÏ†(Ï€
0
). The scalar products are
bounded, so dividing the inequalities by Î»k and taking the limit, we obtain that Ï†(Ï€
?
Î»k
)
converges to Ï†(Ï€
0
). By lower semi-continuity of Ï†, we thus have Ï†(Ï€
?
) â‰¤ Ï†(Ï€
0
). Since the
global minimum of Ï† on Î (p, q) is attained uniquely at Ï€
0
, we must have Ï€
? = Ï€
0
, and
the original sequence also converges in norm to Ï€
0
. Hence, the dual rot moverâ€™s plan Ï€Î»
converges in norm to Ï€
0 when Î» tends to +âˆž. By continuity of the total cost hÂ·, Î³i on R
dÃ—d
,
hÏ€
?
Î»k
, Î³i converges to hÏ€
0
, Î³i. Hence, the limit of the RMD when Î» tends to +âˆž is hÏ€
0
, Î³i.
Property 8 When Î» tends to 0, the dual rot moverâ€™s distance converges to the earth
moverâ€™s distance:
lim
Î»â†’0
dÎ³,Î»,Ï†(p, q) = dÎ³(p, q) . (85)
21
Dessein, Papadakis and Rouas
Proof This is a direct consequence of the dual RMD being continuous at Î» = 0.
Property 9 If [0, 1)dÃ—d âŠ† dom Ï† and Ï† is strictly convex on [0, 1)dÃ—d
, then the dual rot
moverâ€™s plan converges in norm when Î» tends to 0 to the earth moverâ€™s plan Ï€
?
0 with minimal
Bregman information:
lim
Î»â†’0
Ï€
?
Î» = Ï€
?
0
. (86)
Proof Let (Î»k)kâˆˆN
be a sequence of positive numbers that converges to 0, and (Ï€
?
Î»k
)
kâˆˆN
the associated rot moverâ€™s plans. By compactness of Î (p, q), we can extract a subsequence of rot moverâ€™s plans that converges in norm to a point Ï€
? âˆˆ Î (p, q). For
the sake of simplicity, we do not relabel this subsequence. By construction, we have
hÏ€
?
0
, Î³i + Î»kÏ†(Ï€
?
Î»k
) â‰¤ hÏ€
?
Î»k
, Î³i + Î»kÏ†(Ï€
?
Î»k
) â‰¤ hÏ€
?
0
, Î³i + Î»kÏ†(Ï€
?
0
). The regularizer Ï† is
continuous on the polytope Î (p, q) âŠ† dom Ï†, so taking the limit, we obtain that hÏ€
?
Î»k
, Î³i
converges to hÏ€
?
0
, Î³i. Therefore, Ï€
? must be an earth moverâ€™s plan. Now dividing by Î»k
and taking the limit, we obtain that Ï†(Ï€
?
) â‰¤ Ï†(Ï€
?
0
). Since Ï€
?
0
is the unique earth moverâ€™s
plan with minimal Bregman information, we must have Ï€
? = Ï€
?
0
.
3.4 Geometrical Insights
Our primal and dual formulations enlighten some intricate relations between optimal transportation theory (Villani, 2009) and information geometry (Amari and Nagaoka, 2000),
where Bregman divergences are known to possess a dually flat structure with a generalized Pythagorean theorem for information projections. A schematic view of the underlying
geometry for ROT problems is represented in Figure 1, and can be discussed as follows.
Our constructions start from the global minimizer Î¾
0
of the regularizer Ï† (Lemma 1).
The Bregman projection Ï€
0 of Î¾
0
onto the transport polytope Î (p, q) has minimal Bregman
information on Î (p, q) (Lemma 2). The linear cost restricted to the regularized transport
polytope Î Î±,Ï†(p, q) also attains its global minimum (Lemma 3). Such a minimizer Ï€
0?
Î± is
a primal rot moverâ€™s plan (Definition 4). We can interpret Î Î±,Ï†(p, q) as the intersection
of Î (p, q) with the Bregman ball of radius BÏ†(Ï€
0kÎ¾
0
) + Î± and center Î¾
0
(Proposition 5).
In certain cases, Î Î±,Ï†(p, q) is also the intersection of Î (p, q) with the Bregman ball of
radius Î± and center Ï€
0
, as a result of the generalized Pythagorean theorem BÏ†(Ï€kÎ¾
0
) =
BÏ†(Ï€kÏ€
0
) + BÏ†(Ï€
0kÎ¾
0
) (Proposition 6, Corollary 7). All in all, this enforces the solutions to
have small enough Bregman information, by constraining them to lie close to the matrix Î¾
0
or transport plan Ï€
0 with minimal Bregman information.
In our developments, we next introduce the global minimizer Î¾ of the regularized cost
(Lemma 8). The regularized cost restricted to Î (p, q) also attains its global minimum
uniquely (Lemma 9). This minimizer defines the dual rot moverâ€™s plan Ï€
?
Î»
(Definition 10).
Actually, Ï€
?
Î»
can be seen as the Bregman projection of Î¾ onto Î (p, q) (Proposition 11). The
regularization by the Bregman information is also equivalent to regularizing the solution
toward Î¾
0
(Proposition 12). In some cases, this can also be seen as regularizing toward
Ï€
0
, as a result of the generalized Pythagorean theorem BÏ†(Ï€kÎ¾
0
) = BÏ†(Ï€kÏ€
0
) + BÏ†(Ï€
0kÎ¾
0
)
(Proposition 13, Corollary 14). Again, this enforces the solutions to have small enough
22
Rot Moverâ€™s Distance
Figure 1: Geometry of regularized optimal transport.
Bregman information, by shrinking them toward the matrix Î¾
0
or transport plan Ï€
0 with
minimal Bregman information.
We have duality between the primal and dual formulations, so that primal and dual rot
moverâ€™s plans follow the same path on Î (p, q) from no regularization (Î± = +âˆž, Î» = 0) to
full regularization (Î± = 0, Î» = +âˆž) (Theorem 15). In the limit of no regularization, we
obviously retrieve an earth moverâ€™s plan Ï€
?
for the cost matrix Î³. By duality, it is also
intuitive that the additional constraint for the primal formulation, seen in the equivalent
forms of Ï†(Ï€), BÏ†(Ï€kÎ¾
0
) or BÏ†(Ï€kÏ€
0
), leads to an analog penalty for the dual formulation
in the same respective form.
Since Î¾
0 = 1, Ï€
0 = pq>, Î¾ = exp(âˆ’Î³/Î»), for minus the Boltzmann-Shannon entropy and
Kullback-Leibler divergence, we retrieve the existing results discussed in Section 1.2 as a
specific case (Cuturi, 2013; Benamou et al., 2015). In addition, we can readily generalize the
estimation of contingency tables with fixed marginals to a matrix nearness problem based on
other divergences than the Kullback-Leibler divergence (Dhillon and Tropp, 2007). Given
a rough estimate Î¾ âˆˆ int(dom Ï†), a contingency table with fixed marginals p, q can be
estimated by Bregman projection of Î¾ onto Î (p, q):
Ï€
? = argmin
Ï€âˆˆÎ (p,q)
BÏ†(Ï€kÎ¾) . (87)
This simply amounts to solving a dual ROT problem with an arbitrary penalty Î» > 0 and
a cost matrix Î³ = âˆ’Î»âˆ‡Ï†(Î¾).
23
Dessein, Papadakis and Rouas
Finally, since Bregman divergences are invariant under adding an affine term to their
generator, it is straightforward to generalize ROT problems by shrinking toward an arbitrary
prior matrix Î¾, Î¾
0 âˆˆ int(dom Ï†), or transport plan Ï€
0 âˆˆ Î (p, q). This is indeed equivalent
to translating the regularizer by the appropriate amount Ï†(Ï€) + hÏ€, Î´i, so that the global
minimizer is now attained at the desired point. Equivalently, this amounts to translating
the cost matrix as Î³ + Î»Î´ instead.
4. Algorithmic Derivations
In this section, we introduce algorithmic methods to solve ROT problems. We focus without
lack of generality on the dual problem, which can be solved efficiently via alternate Bregman
projections. The primal problem can then easily be solved for 0 < Î± < Î±0 by a bisection
search on Î» > 0. For Î± = 0, we could simply use alternate Bregman projections to project
âˆ‡Ïˆ(0) instead of âˆ‡Ïˆ(âˆ’Î³/Î») in virtue of Lemmas 1 and 2, which actually corresponds to
the special case Î³ = 0 in our algorithms, though this is not really relevant in practice
since this completely removes the linear influence of the total cost from the ROT problem.
In the limit Î± â‰¥ Î±
0
, a classical OT solver such as the network simplex can directly be
used. We first study the underlying Bregman projections in their generic form (Section 4.1)
and specifically develop the case of separable divergences (Section 4.2). We then derive
the two generic schemes of ASA (Section 4.3) and NASA (Section 4.4) to solve dual ROT
problems, depending on whether the domain of the smooth convex regularizer lies within
the non-negative orthant or not. We also enhance both algorithms in the separable case
with a sparse extension (Section 4.5), and finally discuss some practical considerations of
our methods (Section 4.6). To simplify notations, we omit the penalty value Î» in the index
and simply write Ï€
?
for the rot moverâ€™s plan.
4.1 Generic Projections
The closed convex transport polytope Î (p, q) is the intersection of the non-negative orthant:
C0 = R
dÃ—d
+ , (88)
which is a polyhedral subset, with two affine subspaces:
C1 = {Ï€ âˆˆ R
dÃ—d
: Ï€1 = p} , (89)
C2 = {Ï€ âˆˆ R
dÃ—d
: Ï€
>1 = q} . (90)
The Bregman projection Ï€
? onto Î (p, q) can then be obtained by alternate Bregman projections onto C0, C1, C2, where we expect that these latter projections are easier to compute.
On the one hand, the Karush-Kuhn-Tucker conditions for Bregman projection Ï€
?
0
of a
given matrix Ï€ âˆˆ int(dom Ï†) onto C0 are necessary and sufficient, and write as follows:
Ï€
?
0 â‰¥ 0 , (91)
âˆ‡Ï†(Ï€
?
0
) âˆ’ âˆ‡Ï†(Ï€) â‰¥ 0 , (92)
(âˆ‡Ï†(Ï€
?
0
) âˆ’ âˆ‡Ï†(Ï€))  Ï€
?
0 = 0 . (93)
24
Rot Moverâ€™s Distance
While these conditions are nontrivial to solve in general, we shall see that they admit an
elegant solver specific to the non-separable squared Mahalanobis distances defined in (33)
and generated by the quadratic form in (34). In addition, they also greatly simplify for
separable divergences, which encompass all other divergences used in this paper.
On the other hand, the Lagrangians with Lagrange multipliers Âµ, Î½ âˆˆ R
d
for the Bregman projections Ï€
?
1
and Ï€
?
2
of a given matrix Ï€ âˆˆ int(dom Ï†) onto C1 and C2 respectively
write as follows:
L1(Ï€, Âµ) = Ï†(Ï€) âˆ’ hÏ€, âˆ‡Ï†(Ï€)i + Âµ
>(Ï€1 âˆ’ p) , (94)
L2(Ï€, Î½) = Ï†(Ï€) âˆ’ hÏ€, âˆ‡Ï†(Ï€)i + Î½
>(Ï€
>1 âˆ’ q) . (95)
Their gradients are given on int(dom Ï†) by:
âˆ‡L1(Ï€, Âµ) = âˆ‡Ï†(Ï€) âˆ’ âˆ‡Ï†(Ï€) + Âµ1
> , (96)
âˆ‡L2(Ï€, Î½) = âˆ‡Ï†(Ï€) âˆ’ âˆ‡Ï†(Ï€) + 1Î½
> , (97)
and vanish at Ï€
?
1
,Ï€
?
2 âˆˆ int(dom Ï†) if and only if:
Ï€
?
1 = âˆ‡Ïˆ(âˆ‡Ï†(Ï€) âˆ’ Âµ1
>) , (98)
Ï€
?
2 = âˆ‡Ïˆ(âˆ‡Ï†(Ï€) âˆ’ 1Î½
>) . (99)
By duality, the Bregman projections onto C1, C2 are thus equivalent to finding the unique
vectors Âµ, Î½, such that the rows of Ï€
?
1
sum up to p, respectively the columns of Ï€
?
2
sum up
to q:
âˆ‡Ïˆ(âˆ‡Ï†(Ï€) âˆ’ Âµ1
>)1 = p , (100)
âˆ‡Ïˆ(âˆ‡Ï†(Ï€) âˆ’ 1Î½
>)
>
1 = q . (101)
Similarly, solving for the Lagrange multipliers is an expensive problem in general, since
the search space is of dimension d and we evaluate matrix functions of size d Ã— d. This is
because a given entry Âµi
, Î½j can actually modify any entry of the d Ã— d matrix functions
being evaluated. Again, we shall see that they can nevertheless be computed efficiently for
separable divergences as well as the non-separable Mahalanobis distances.
4.2 Separable Case
Assuming that the regularizer Ï† is separable, the underlying Bregman projections can be
computed more efficiently. To keep notations simple, we focus on separable divergences with
same element-wise regularizer, and thus chiefly omit the indices Ï†ij = Ï†. We emphasize,
however, that it is straightforward to apply all our methods for separable divergences with
different element-wise regularizers, which notably enables weighting a given element-wise
regularizer.
In case of separability, the Karush-Kuhn-Tucker conditions for projection onto C0 simplify to provide a closed-form solution on primal parameters:
Ï€
?
0,ij = max{0, Ï€ij} . (102)
25
Dessein, Papadakis and Rouas
Since Ï†
0
is increasing, this is equivalent on dual parameters to:
Î¸
?
0,ij = max{Ï†
0
(0), Î¸ij} . (103)
Now turning to projections onto C1, C2 for primal parameters Ï€
?
1,ij , Ï€?
2,ij , we can divide
the initial problems into d parallel subproblems in search space of dimension 1 each. This
is much more efficient to solve than in the non-separable case. This can be summarized as
looking for d separate Lagrange multipliers Âµi
, respectively Î½j , such that:
X
d
j=1
Ïˆ
0
(Î¸ij âˆ’ Âµi) = pi
, (104)
X
d
i=1
Ïˆ
0
(Î¸ij âˆ’ Î½j ) = qj . (105)
Finding the optimal values Âµi
, Î½j âˆˆ R through Ïˆ
0 and the sums over rows or columns,
however, is still nontrivial in general.
An analytical solution can be obtained in specific cases. Intuitively, we need to factor
Âµi
, Î½j out of Ïˆ
0 as additive or multiplicative terms. This is related to Pexiderâ€™s functional
equations, which hold only for functions with a linear form Ïˆ
0
(Î¸) = aÎ¸ + b, or exponential
form Ïˆ
0
(Î¸) = a exp(bÎ¸), with a, b âˆˆ R. This leads to regularizers with a quadratic form
Ï†(Ï€) = aÏ€2+bÏ€+c, or entropic form Ï†(Ï€) = aÏ€ log Ï€+bÏ€+c, with a, b, c âˆˆ R. The constants
a, b actually only scale and translate the cost matrix, whereas the constant c has no effect.
Referring to Table 1, the quadratic case holds under assumptions (B), and thus requires
Dykstraâ€™s algorithm for alternate Bregman projections with correction terms to ensure nonnegativity by projection onto the polyhedral non-negative orthant. The entropic case holds
under assumptions (A), using the POCS technique for alternate Bregman projection with no
correction terms since the non-negativity is already ensured by the domain of the regularizer.
The latter case reduces to the regularization of Cuturi (2013) and Benamou et al. (2015),
so that we actually end up with the Sinkhorn-Knopp algorithm. Hence, the Euclidean
norm associated to the squared Euclidean distance, and the entropic case associated to the
Kullback-Leibler divergence, are reasonably the only two existing analytical schemes to find
the sum constraint projections. For other ROT problems, available solvers for line search
can be employed instead.
For simplicity, we assume hereafter that Ïˆ is twice continuously differentiable with Ïˆ
00
positive and Ïˆ
0 verifying the necessary and sufficient condition (40) on its whole domain.
Therefore, we can use the Newton-Raphson method with guarantees of global convergence.
This encompasses most of the common regularizers, and notably all regularizers used in this
paper except from the Fermi-Dirac entropy, `p norms and Hellinger distance. When the
condition (40) for global convergence is not met on the whole domain, it is still possible to
apply the Newton-Raphson method after careful initialization, so as to restrict to a smaller
interval where the condition holds. This is discussed in more detail with practical examples
for the Fermi-Dirac entropy, `p norms and Hellinger distance in Section 5, where the firstorder derivatives are increasing convex on half of the domain and increasing concave on the
other half. When the second-order derivatives do not exist, are not continuous or vanish at
some points, a similar strategy can be applied. This is again discussed for the `p norms in
26
Rot Moverâ€™s Distance
Section 5, where the second-order derivative is undefined or vanishes at 0 depending on the
value of the parameter. If such an initialization is not possible, then a bisection search can
always be applied instead of the Newton-Raphson method.
To apply the Newton-Raphson method, we exploit the following functions:
f(Âµi) = âˆ’
X
d
j=1
Ïˆ
0
(Î¸ij âˆ’ Âµi) , (106)
g(Î½j ) = âˆ’
X
d
i=1
Ïˆ
0
(Î¸ij âˆ’ Î½j ) , (107)
defined respectively on the open intervals (Ë†Î¸iâˆ’Î¸, +âˆž) and (Ë‡Î¸j âˆ’Î¸, +âˆž), where 0 < Î¸ â‰¤ +âˆž
is such that dom Ïˆ = (âˆ’âˆž, Î¸), and Ë†Î¸i = max{Î¸ij}1â‰¤jâ‰¤d,
Ë‡Î¸j = max{Î¸ij}1â‰¤iâ‰¤d. Their
continuous derivatives are given by:
f
0
(Âµi) = X
d
j=1
Ïˆ
00(Î¸ij âˆ’ Âµi) , (108)
g
0
(Î½j ) = X
d
i=1
Ïˆ
00(Î¸ij âˆ’ Î½j ) , (109)
and are positive, so that f, g are strictly increasing on their whole domain, and thus on any
closed interval with endpoints consisting of a feasible point and a solution. By construction,
f, g also verify the necessary and sufficient condition (40) for global convergence, and we
know that there are unique solutions to f(Âµi) = âˆ’pi and g(Î½j ) = âˆ’qj . Hence, the NewtonRaphson updates:
Âµi â† Âµi +
Pd
j=1 Ïˆ
0
(Î¸ij âˆ’ Âµi) âˆ’ pi
Pd
j=1 Ïˆ00(Î¸ij âˆ’ Âµi)
, (110)
Î½j â† Î½j +
Pd
i=1 Ïˆ
0
(Î¸ij âˆ’ Î½j ) âˆ’ qj
Pd
i=1 Ïˆ00(Î¸ij âˆ’ Î½j )
, (111)
converge to the optimal solutions with a quadratic rate for any feasible starting points. By
construction, we also know that initialization can be done with Âµi â† 0, Î½j â† 0. To avoid
storing the intermediate Lagrange multipliers, the updates can then directly be written on
dual parameters:
Î¸
?
1,ij â† Î¸
?
1,ij âˆ’
Pd
j=1 Ïˆ
0
(Î¸
?
1,ij ) âˆ’ pi
Pd
j=1 Ïˆ00(Î¸
?
1,ij )
, (112)
Î¸
?
2,ij â† Î¸
?
2,ij âˆ’
Pd
i=1 Ïˆ
0
(Î¸
?
2,ij ) âˆ’ qj
Pd
i=1 Ïˆ00(Î¸
?
2,ij )
, (113)
after initialization by Î¸
?
1,ij â† Î¸ij , Î¸
?
2,ij â† Î¸ij .
27
Dessein, Papadakis and Rouas
Algorithm 1 Alternate scaling algorithm.
Î¸
? â† âˆ’Î³/Î»
repeat
Î¸
? â† Î¸
? âˆ’ Âµ1
>, where Âµ uniquely solves âˆ‡Ïˆ(Î¸
? âˆ’ Âµ1
>)1 = p
Î¸
? â† Î¸
? âˆ’ 1Î½
>, where Î½ uniquely solves âˆ‡Ïˆ(Î¸
? âˆ’ 1Î½
>)
>
1 = q
until convergence
Ï€
? â† âˆ‡Ïˆ(Î¸
?
)
4.3 Alternate Scaling Algorithm
Under assumptions (A), we can drop the non-negative constraint since it is already ensured
by dom Ï† âŠ† R
dÃ—d
+ (Table 1). The POCS technique in its basic form (38) then states that
the projection of Î¾ onto Î (p, q) can be obtained by alternate Bregman projections onto
the affine subspaces C1 and C2 with linear convergence. Clearly, the underlying control
mapping takes each output value an infinite number of times. Since we have just two sets,
the only possible alternative in the control mapping is to swap the order of projections
starting from C2 instead of C1, which actually amounts to swapping the input distributions
p, q and transposing the cost matrix Î³, to obtain the transposed of the rot moverâ€™s plan.
We thus focus on the first choice without lack of generality.
Starting from Î¾ and writing the successive vectors Âµ
(k)
, Î½
(k) along iterations, we have
the following sequence:
âˆ‡Ïˆ(âˆ’Î³/Î») â†’ âˆ‡Ïˆ

âˆ’Î³/Î» âˆ’ Âµ
(1)1
>

(114)
â†’ âˆ‡Ïˆ

âˆ’Î³/Î» âˆ’ Âµ
(1)1
> âˆ’ 1Î½
(1)>

(115)
â†’ . . . (116)
â†’ âˆ‡Ïˆ

âˆ’Î³/Î» âˆ’ Âµ
(1)1
> âˆ’ 1Î½
(1)> âˆ’ Â· Â· Â· âˆ’ Âµ
(k)1
>

(117)
â†’ âˆ‡Ïˆ

âˆ’Î³/Î» âˆ’ Âµ
(1)1
> âˆ’ 1Î½
(1)> âˆ’ Â· Â· Â· âˆ’ Âµ
(k)1
> âˆ’ 1Î½
(k)>

(118)
â†’ . . . (119)
â†’ Ï€
?
. (120)
In other terms, we obtain the rot moverâ€™s plan Ï€
? by scaling iteratively the rows and columns
of the successive estimates through âˆ‡Ïˆ. An efficient algorithm, called ASA, is to store a
unique d Ã— d matrix in dual parameter space and update it by alternating the projections
in primal parameter space (Algorithm 1). The updates have a complexity in O(d
2
) once
the vectors Âµ, Î½ are obtained.
In the separable case, the projections can be obtained by iterating the respective NewtonRaphson update steps, which can be written compactly with matrix and vector operations
(Algorithm 2). The complexity for the updates are now clearly in O(d
2
). In more detail,
each update step features one vector row or column replication, one vector element-wise
division, one vector subtraction, one matrix subtraction, two matrix row or column sums,
and two element-wise matrix function evaluations. Because of separability, we can expect
28
Rot Moverâ€™s Distance
Algorithm 2 Alternate scaling algorithm in the separable case.
Î¸
? â† âˆ’Î³/Î»
repeat
repeat
Î¸
? â† Î¸
? âˆ’
Ïˆ
0
(Î¸
?
)1âˆ’p
Ïˆ00(Î¸
?
)1
1
>
until convergence
repeat
Î¸
? â† Î¸
? âˆ’ 1
1>Ïˆ
0
(Î¸
?
)âˆ’q>
1>Ïˆ00(Î¸
?
)
until convergence
until convergence
Ï€
? â† Ïˆ
0
(Î¸
?
)
the required number of iterations for convergence in the different loops to be independent
of the data dimension, and thus expect a quadratic empirical complexity as well.
4.4 Non-negative Alternate Scaling Algorithm
Under assumptions (B), we must now include the non-negative constraint since dom Ï† *
R
dÃ—d
+ (Table 1). We suggest to ensure non-negativity of each update, and thus follow a cycle
of projections onto C0, C1, C0, C2. The underlying control mapping is a fortiori essentially
cyclic. For practical reasons, we also ensure non-negativity of the output solution with a
final projection onto C0. Again, swapping the order of projections onto C1, C2 is equivalent
to swapping the input distributions p, q and transposing the cost matrix Î³ to obtain the
transposed of the rot moverâ€™s plan. Other control mappings could also be exploited, for
example by ensuring non-negativity every two or more sum constraint projections. We do
not discuss such variants here and focus on the above-mentioned sequence. The non-negative
orthant being polyhedral but not affine, we also need to incorporate correction terms Ï‘, %, Ï‚
for all three projections. In more detail, the projections are computed after correction
so that we do not directly project the obtained updates Î¸
? but the corrected updates
Î¸ = Î¸
? + Ï‘, Î¸ = Î¸
? + %, and Î¸ = Î¸
? + Ï‚ for the respective subsets. The correction terms
are also updated as the difference Î¸ âˆ’ Î¸
? between the projected point and its projection.
Dykstraâ€™s algorithm (36) for Bregman divergences with corrections (37) then guarantees
that the projection of Î¾ onto Î (p, q) is obtained with linear convergence.
A general algorithm, called NASA, is to store d Ã— d matrices for projected points,
projections and correction terms in dual parameter space, update them accordingly and
finally go back to primal parameter space (Algorithm 3). The updates have a complexity
in O(d
2
) once the Karush-Kuhn-Tucker conditions are solved or Lagrange multipliers Âµ, Î½
are obtained.
In the separable case, the non-negativity constraint can be obtained analytically and the
sequence of updates greatly simplifies. Starting from Î¾ and writing the successive vectors
Âµ
(k)
, Î½
(k) along iterations, we have:
Ïˆ
0
(âˆ’Î³/Î») â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î»}

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î»} âˆ’ Âµ
(1)1
>

2 
Dessein, Papadakis and Rouas
Algorithm 3 Non-negative alternate scaling algorithm.
Î¸
? â† âˆ’Î³/Î»
Ï‘ â† 0
% â† 0
Ï‚ â† 0
Î¸ â† Î¸
? + Ï‘
Î¸
? â† Î¸, where Î¸ uniquely solves âˆ‡Ïˆ(Î¸) â‰¥ 0, Î¸ â‰¥ Î¸, (Î¸ âˆ’ Î¸)  âˆ‡Ïˆ(Î¸) = 0
Ï‘ â† Î¸ âˆ’ Î¸
?
repeat
Î¸ â† Î¸
? + %
Î¸
? â† Î¸ âˆ’ Âµ1
>, where Âµ uniquely solves âˆ‡Ïˆ(Î¸ âˆ’ Âµ1
>)1 = p
% â† Î¸ âˆ’ Î¸
?
Î¸ â† Î¸
? + Ï‘
Î¸
? â† Î¸, where Î¸ uniquely solves âˆ‡Ïˆ(Î¸) â‰¥ 0, Î¸ â‰¥ Î¸, (Î¸ âˆ’ Î¸)  âˆ‡Ïˆ(Î¸) = 0
Ï‘ â† Î¸ âˆ’ Î¸
?
Î¸ â† Î¸
? + Ï‚
Î¸
? â† Î¸ âˆ’ 1Î½
>, where Î½ uniquely solves âˆ‡Ïˆ(Î¸ âˆ’ 1Î½
>)
>
1 = q
Ï‚ â† Î¸ âˆ’ Î¸
?
Î¸ â† Î¸
? + Ï‘
Î¸
? â† Î¸, where Î¸ uniquely solves âˆ‡Ïˆ(Î¸) â‰¥ 0, Î¸ â‰¥ Î¸, (Î¸ âˆ’ Î¸)  âˆ‡Ïˆ(Î¸) = 0
Ï‘ â† Î¸ âˆ’ Î¸
?
until convergence
Ï€
? â† âˆ‡Ïˆ(Î¸
?
)
â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(1)1
>}

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(1)1
>} âˆ’ 1Î½
(1)>

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(1)1
> âˆ’ 1Î½
(1)>}

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(1)1
> âˆ’ 1Î½
(1)>} + Âµ
(1)1
> âˆ’ Âµ
(2)1
>

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(2)1
> âˆ’ 1Î½
(1)>}

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(2)1
> âˆ’ 1Î½
(1)>} + 1Î½
(1)> âˆ’ 1Î½
(2)>

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(2)1
> âˆ’ 1Î½
(2)>}

â†’ . . .
â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(k)1
> âˆ’ 1Î½
(k)>}

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(k)1
> âˆ’ 1Î½
(k)>} + Âµ
(k)1
> âˆ’ Âµ
(k+1)1
>

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(k+1)1
> âˆ’ 1Î½
(k)>}

â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(k+1)1
> âˆ’ 1Î½
(k)>} + 1Î½
(k)> âˆ’ 1Î½
(k+1)>

30
Rot Moverâ€™s Distance
Algorithm 4 Non-negative alternate scaling algorithm in the separable case.
Î¸e â† âˆ’Î³/Î»
Î¸
? â† max{Ï†
0
(0), Î¸e}
repeat
Ï„ â† 0
repeat
Ï„ â† Ï„ +
Ïˆ
0
(Î¸
?âˆ’Ï„1>)1âˆ’p
Ïˆ00(Î¸
?âˆ’Ï„1>)1
until convergence
Î¸e â† Î¸e âˆ’ Ï„1
>
Î¸
? â† max{Ï†
0
(0), Î¸e}
Ïƒ â† 0
repeat
Ïƒ â† Ïƒ +
1>Ïˆ
0
(Î¸
?âˆ’1Ïƒ>)âˆ’q>
1>Ïˆ00(Î¸
?âˆ’1Ïƒ>)
until convergence
Î¸e â† Î¸e âˆ’ 1Ïƒ
>
Î¸
? â† max{Ï†
0
(0), Î¸e}
until convergence
Ï€
? â† Ïˆ
0
(Î¸
?
)
â†’ Ïˆ
0

max{Ï†
0
(0), âˆ’Î³/Î» âˆ’ Âµ
(k+1)1
> âˆ’ 1Î½
(k+1)>}

â†’ . . .
â†’ Ï€
?
.
An efficient algorithm then exploits the differences Ï„
(k) = Âµ
(k) âˆ’ Âµ
(kâˆ’1) and Ïƒ
(k) = Î½
(k) âˆ’
Î½
(kâˆ’1) to scale the rows and columns (Algorithm 4). We store d Ã— d matrices as well as
difference vectors instead of correction matrices. The algorithm can then be interpreted as
producing interleaved updates between the projections according to the max operator and
according to the respective scalings. The updates in NASA now clearly have a complexity
in O(d
2
) when using the Newton-Raphson method for scaling, with similar matrix and
vector operations to ASA in the separable case, and an expected empirical complexity that
is quadratic.
4.5 Sparse Extension
In the separable case, it is possible to develop a sparse extension of both our methods
ASA and NASA. Storing and updating full dÃ—d matrices becomes expensive with the data
dimension. Instead, we allow for infinite entries in the cost matrix Î³, meaning that the
transport of mass between certain bins is proscribed. As a result, the corresponding entries
of Ï€
? must be null. Eventually, we can drop all these entries so that we just need to store
and update the remaining ones. The RMD via the Frobenius inner product hÏ€
?
, Î³i is then
computed without accounting for discarded entries, or equivalently by setting indefinite
element-wise products 0 Ã— âˆž = 0 by convention, so it naturally costs nothing to move no
mass on a path that is forbidden. This leads to an expected complexity in O(r), where r is
31
Dessein, Papadakis and Rouas
the number of finite entries in Î³. Typically, r can be chosen in the order of magnitude of
d, so as to obtain a linear instead of quadratic empirical complexity.
In practice, both ASA and NASA are compatible with this strategy. We always have
limÎ¸â†’âˆ’âˆž Ïˆ
0
(Î¸) = 0 under assumptions (A) for ASA. Under assumptions (B) for NASA,
this limit might be finite or infinite but is necessarily negative, so also leads to 0 after
enforcing non-negativity by projection onto the non-negative orthant. As a result, the
obtained sequence of projections preserves the desired zeros in both algorithms, and an
infinite element-wise cost does lead to no mass transport at all between the corresponding
bins. In theory, we can understand this extension in light of the dual formulation seen
as a Bregman projection in (67). Under assumptions (B), we always have 0 âˆˆ int(dom Ï†)
and thus BÏ†(0k0) = 0. Hence, Dykstraâ€™s algorithm is readily applicable in the sparse
version. Under assumptions (A), however, we have 0 âˆˆ/ int(dom Ï†), and even sometimes
0 âˆˆ/ dom Ï† as for the Itakura-Saito divergence. We can nonetheless extend the domain of
the element-wise divergence at the origin by continuity on the diagonal, that is, by setting
it null as BÏ†(0k0) = 0. This is akin to considering absolutely continuous measures, also
known as dominated measures, and Radon-Nikodym derivatives to generalize the definition
of Bregman divergences. Kurras (2015) then showed that the POCS method still holds with
this convention by introducing a notion of locally affine spaces.
With such a sparse extension, however, we must take care that a sparse solution does
exist, meaning that there is a transport plan in the transport polytope that has the desired
zeros. For example, if all entries of Î³ are infinite, then there are obviously no possible
sparse solutions since we enforce all entries of the plan to be null. A necessary condition
for the existence of a sparse solution is that for any entry qj , all entries pk from which
we are allowed to transport mass must provide enough total mass to fill qj completely.
Similarly, for any entry pi
, all entries qk to which we are allowed to transport mass must
require enough total mass to empty pi completely. Unfortunately, sufficient conditions are
not so intuitive. Idel (2016, Theorem 4.1) studied such problems thoroughly and elucidated
several necessary and sufficient conditions for sparse solutions to exist, but these conditions
are nontrivial to use from in practice. Kurras (2015) advocates trying first to compute a
solution with the desired sparsity, and if no solution can be found, then gradually reduce
sparsity until a solution is found. This might still speed up computation drastically because
of the linear instead of quadratic complexity. Lastly, we remark that it is not evident to
propose a sparse extension for the non-separable case in general, since a given entry of Î³
might influence all entries of Ï€
?
.
4.6 Practical Considerations
As noticed by Cuturi (2013) and Benamou et al. (2015), the Sinkhorn-Knopp algorithm
might fail to converge because of numerical instability when the penalty Î» gets small. In
particular, unless taking special care of numerical stabilization (Schmitzer, 2016b), a direct
limitation is the machine precision under which some entries of exp(âˆ’Î³/Î») are represented
as zeros in memory. Such issues occur similarly for other regularizations, notably via the
representation âˆ‡Ïˆ(âˆ’Î³/Î») of the unconstrained solution to project. Therefore, the proposed
methods are actually competitive in a range where the penalty Î» is not too small, and for
which the rot moverâ€™s plan Ï€
?
exhibits a significant amount of smoothing. Hence, we do
32
Rot Moverâ€™s Distance
not target the same problems as traditional schemes such as interior point methods or the
network simplex.
In addition, the different Bregman projections in our algorithms are most of the time
approximate up to a given tolerance depending on the termination criterion used for convergence. Exceptions occur for the sum constraints with the Euclidean distance or KullbackLeibler divergence, as well as the non-negativity constraints in the separable case, which
are obtained analytically. A natural question to raise is then whether our algorithms still
converge when the projections are approximate only. However, this is relatively hard to
answer in theory. We did not observe in practice any problem of convergence when using
sufficiently good approximations. Furthermore, first approximations can be quite rough
without affecting convergence as long as final approximations are good enough. Sometimes,
even alternating a single or two steps of the Newton-Raphson method throughout the main
iterations the algorithm still works, though this is not systematic. Thus, we advocate for
safety to use a tight tolerance for the auxiliary projections.
We also observed numerical instability of the Newton-Raphson updates for separable
divergences under assumptions (A). This is due to the denominator being based on Ïˆ
00 with
limit limÎ¸â†’âˆ’âˆž Ïˆ
00(Î¸) = 0, that is, for entries Ï€ close to zero. It is possible, however, to make
the updates of Âµi
, Î½j much more stable in practice by using the max truncation operator,
despite theoretical guarantees of convergence without it. Specifically, we know that the
entries Ï€
?
1,ij must lie between 0 and pi
, and Ï€
?
2,ij between 0 and qj . Hence, we can lower
bound Âµi and Î½j by Ë†Î¸i âˆ’ Ï†
0
(pi) and Ë†Î¸j âˆ’ Ï†
0
(qj ), respectively. Interestingly, this also speeds
up the convergence of the updates significantly when the initialization by 0 is far from the
actual solution.
A possible termination criterion for the main and auxiliary iterations is to compute the
marginal difference between the updated matrix and p, q. In the auxiliary iterations for
the two scaling projections, we compare the sums of rows or columns to p or q respectively,
and in the main iterations of the algorithm, we compare both marginals simultaneously.
Typically, we can use the `p (quasi-)norm with 0 < p â‰¤ +âˆž to assess the marginal difference,
and the auxiliary tolerance should be at least the main one for sufficient precision of the
approximations. Two alternative quantities in absolute or relative scales can also be used
for termination, either the variation with `p (quasi-)norm in the updated matrix or in the
updated distance. Here the auxiliary tolerance should be at least the square of the main
one. This seems reasonable for Ï€
? given the quadratic rate of convergence for the NewtonRaphson method versus the linear one for alternate Bregman projections, as well as for
hÏ€
?
, Î³i under the Cauchy-Schwarz inequality. In all cases, convergence can be checked
either after each iteration or after a given number of iterations to reduce the underlying
cost of computing the termination criterion. We can also fix a maximum number of main
and auxiliary iterations to limit the overall running time.
Regarding implementation, the matrix and vector operations used for ASA and NASA
in the separable case are well-suited for fast calculation on a GPU and for processing of
multiple input distributions in parallel. By working directly in the primal parameter space,
the Sinkhorn-Knopp algorithm is also readily suited for dealing with sparse plans, based
on existing libraries. In more general ROT problems, however, a specific library should be
written for the sparse extension because null entries in the transport plan are not represented
by null entries in the dual parameter space, so that tailored data structures and operations
33
Dessein, Papadakis and Rouas
for such matrices need to be coded. Therefore, we only implemented and will focus in our
experiments on the non-sparse version of our methods.
Finally, although we implicitly assumed throughout that the entries of p and q are
strictly comprised between 0 and 1 for theoretical issues, it is often possible in practice
to deal explicitly with null or unit entries in the input distributions. Intuitively, no mass
can be moved from or to a null entry, so the transport plans have null rows and columns
for the corresponding null entries of p and q, respectively. In the separable case, we can
thus simply remove these entries, solve the reduced ROT problem, and reinsert the corresponding null entries in the rot moverâ€™s plan Ï€
?
. The same reasoning as for the sparse
extension can be made to show that our two algorithms still hold with this strategy from
a theoretical standpoint. In the non-separable case, however, this is not as straightforward
again because the influences of the different entries of Ï€
? are interleaved through the regularizer Ï†. Nonetheless, as long as we have [0, 1)dÃ—d âŠ‚ int(dom Ï†) under assumptions (B),
then we have Î (p, q) âŠ‚ int(dom Ï†) and we can apply NASA without modification. This is
notably the case for the Mahalanobis distances whose domain is R
dÃ—d
. For a non-separable
regularizer under assumptions (A), it is not easy to account for null entries because the constraint qualification Î (p, q) âˆ© int(dom Ï†) 6= âˆ… never holds due to mandatory null entries in
the transport plans. Nevertheless, common regularizers under assumptions (A), including
the ones used in this paper, are separable in general. Lastly, it is direct to cope with unit
entries in p or q in all cases, since the transport polytope then reduces to a singleton, so
that there is a unique transport plan pq> which is the rot moverâ€™s plan.
5. Classical Regularizers and Divergences
In this section, we discuss the specificities of the ASA (Algorithm 1) and NASA (Algorithm 3) methods to solve ROT problems for classical regularizers and associated divergences. We start with several separable regularizers under assumptions (A), based on the
Boltzmann-Shannon entropy related to the Kullback-Leibler divergence (BSKL, Section 5.1),
the Burg entropy related to the Itakura-Saito divergence (BIS, Section 5.2), and the FermiDirac entropy related to a logistic loss function (FDLOG, Section 5.3), as well as the parametric families of Î²-potentials related to the Î²-divergences (BETA, Section 5.4). We then
discuss the separable `p quasi-norms (LPQN, Section 5.5), which require a slight adaptation of assumptions (A). We also consider separable regularizers under assumptions (B)
related to `p norms (LPN, Section 5.6), as well as the Euclidean norm related to the Euclidean distance (EUC, Section 5.7) and the Hellinger distance (HELL, Section 5.8). Finally,
we study a non-separable regularizer under assumptions (B) via quadratic forms in relation
to Mahalanobis distances (Section 5.9). We plot all separable regularizers in Figure 2. All
regularizers and their corresponding divergences are also summed up in Table 2. Lastly, we
provide in Table 3 the related terms based on derivatives that are needed to instantiate the
separable versions of ASA (Algorithm 2) or NASA (Algorithm 4) accordingly.
5.1 Boltzmann-Shannon Entropy and Kullback-Leibler Divergence
Assumptions (A) hold for minus the Boltzmann-Shannon entropy Ï€ log Ï€ âˆ’Ï€ + 1 associated
to the Kullback-Leibler divergence. Hence, the ROT problem can be solved with the ASA
scheme. In addition, the updates in the POCS technique can be written analytically, leading
34
Rot Moverâ€™s Distance
Ï†(Ï€) / Ï†(Ï€) BÏ†(Ï€kÎ¾) / BÏ†(Ï€kÎ¾) dom Ï† dom Ïˆ
Boltzmann-Shannon entropy Kullback-Leibler divergence
Ï€ log Ï€ âˆ’ Ï€ + 1 Ï€ log Ï€
Î¾ âˆ’ Ï€ + Î¾ R+ R
Burg entropy Itakura-Saito divergence
Ï€ âˆ’ log Ï€ âˆ’ 1
Ï€
Î¾ âˆ’ log Ï€
Î¾ âˆ’ 1 R++ (âˆ’âˆž, 1)
Fermi-Dirac entropy Logistic loss function
Ï€ log Ï€ + (1 âˆ’ Ï€) log(1 âˆ’ Ï€) Ï€ log Ï€
Î¾ + (1 âˆ’ Ï€) log 1âˆ’Ï€
1âˆ’Î¾
[0, 1] R
Î²-potentials (0 < Î² < 1) Î²-divergences
1
Î²(Î²âˆ’1) (Ï€
Î² âˆ’ Î²Ï€ + Î² âˆ’ 1) 1
Î²(Î²âˆ’1) (Ï€
Î² + (Î² âˆ’ 1)Î¾
Î² âˆ’ Î²Ï€Î¾Î²âˆ’1
) R+ (âˆ’âˆž,
1
1âˆ’Î²
)
`p quasi-norms (0 < p < 1)
âˆ’Ï€
p âˆ’Ï€
p + pÏ€Î¾pâˆ’1 âˆ’ (p âˆ’ 1)Î¾
p R+ Râˆ’âˆ’
`p norms (1 < p < +âˆž)
|Ï€|
p
|Ï€|
p âˆ’ pÏ€ sgn(Î¾)|Î¾|
pâˆ’1 + (p âˆ’ 1)|Î¾|
p R R
Euclidean norm Euclidean distance
1
2
Ï€
2 1
2
(Ï€ âˆ’ Î¾)
2 R R
Hellinger distance
âˆ’(1 âˆ’ Ï€
2
)
1
2 (1 âˆ’ Ï€Î¾)(1 âˆ’ Î¾
2
)
âˆ’ 1
2 âˆ’ (1 âˆ’ Ï€
2
)
1
2
[âˆ’1, 1] R
Quadratic forms (P  0) Mahalanobis distances
1
2
vec(Ï€)
>P vec(Ï€)
1
2
vec(Ï€ âˆ’ Î¾)
>P vec(Ï€ âˆ’ Î¾) R
dÃ—d R
dÃ—d
Table 2: Convex regularizers and associated Bregman divergences.
â€” Î² = 0 Î² = 0.5 Î² = 1 p = 0.1 p = 0.5 p = 0.9 p = 1.1 p = 1.5 p = 2 â€”
FDLOG BETA BETA BETA LPQN LPQN LPQN LPN LPN LPN HELL
â€” BIS â€” BSKL â€” â€” â€” â€” â€” EUC â€”
Figure 2: Separable regularizers on (0, 1).
to the Sinkhorn-Knopp algorithm. Specifically, the two projections amount to normalizing
in turn the rows and columns of Ï€
?
so that they sum up to p and q respectively:
Ï€
? â† diag  p
Ï€?1

Ï€
?
, (121)
Ï€
? â† Ï€
? diag  q
Ï€?>1

. (122)
35
Dessein, Papadakis and Rouas
Ï†(Ï€) Ï†
0
(Ï€) Ïˆ
0
(Î¸) Ïˆ
00(Î¸)
Boltzmann-Shannon entropy
Ï€ log Ï€ âˆ’ Ï€ + 1 log Ï€ exp Î¸ exp Î¸
Burg entropy
Ï€ âˆ’ log Ï€ âˆ’ 1 1 âˆ’ Ï€
âˆ’1
(1 âˆ’ Î¸)
âˆ’1
(1 âˆ’ Î¸)
âˆ’2
Fermi-Dirac entropy
Ï€ log Ï€ + (1 âˆ’ Ï€) log(1 âˆ’ Ï€) log Ï€
1âˆ’Ï€
exp Î¸
(1+exp Î¸)
exp Î¸
(1+exp Î¸)
2
Î²-potentials (0 < Î² < 1)
1
Î²(Î²âˆ’1) (Ï€
Î² âˆ’ Î²Ï€ + Î² âˆ’ 1) 1
Î²âˆ’1
(Ï€
Î²âˆ’1 âˆ’ 1) ((Î² âˆ’ 1)Î¸ + 1)
1
Î²âˆ’1 ((Î² âˆ’ 1)Î¸ + 1)
1
Î²âˆ’1 âˆ’1
`p quasi-norms (0 < p < 1)
âˆ’Ï€
p âˆ’pÏ€pâˆ’1 p
âˆ’ 1
pâˆ’1 (âˆ’Î¸)
1
pâˆ’1 âˆ’
p
âˆ’ 1
pâˆ’1
pâˆ’1
(âˆ’Î¸)
1
pâˆ’1 âˆ’1
`p norms (1 < p < +âˆž)
|Ï€|
p
p sgn(Ï€)|Ï€|
pâˆ’1
p
âˆ’ 1
pâˆ’1 sgn(Î¸)|Î¸|
1
pâˆ’1
p
âˆ’ 1
pâˆ’1
pâˆ’1
|Î¸|
1
pâˆ’1 âˆ’1
Euclidean norm
1
2
Ï€
2 Ï€ Î¸ 1
Hellinger distance
âˆ’(1 âˆ’ Ï€
2
)
1
2 Ï€(1 âˆ’ Ï€
2
)
âˆ’ 1
2 Î¸(1 + Î¸
2
)
âˆ’ 1
2 (1 + Î¸
2
)
âˆ’ 3
2
Table 3: Separable regularizers and related terms based on derivatives.
This can be optimized by remarking that the iterates Ï€
?(k)
after each couple of projections verify:
Ï€
?(k) = diag(u
(k)
)Î¾ diag(v
(k)
) , (123)
where Î¾ = exp(âˆ’Î³/Î»), and vectors u
(k)
, v
(k)
satisfy the following recursion:
u
(k) =
p
Î¾v(kâˆ’1) , (124)
v
(k) =
q
Î¾
>u(k)
, (125)
with convention v
(0) = 1. This allows a fast implementation by performing only matrixvector multiplications using a fixed matrix Î¾ = exp(âˆ’Î³/Î»). We can further save one
element-wise vector multiplication per update:
u â†
1
diag 
1
p

Î¾ v
, (126)
v â†
1
diag 
1
q

Î¾
> u
, (127)
36
Rot Moverâ€™s Distance
where the matrices diag 
1
p

Î¾ and diag 
1
q

Î¾
> are precomputed and stored.
5.2 Burg Entropy and Itakura-Saito Divergence
Assumptions (A) also hold for minus the Burg entropy Ï€âˆ’log Ï€âˆ’1 associated to the ItakuraSaito divergence, so the ROT problem can be solved with the ASA scheme. Eventually,
the Newton-Raphson steps to update the alternate projections in POCS technique can be
written as follows:
Î¸
? â† Î¸
? âˆ’
(1 âˆ’ Î¸
?
)
âˆ’1
1 âˆ’ p
(1 âˆ’ Î¸
?
)
âˆ’2
1
1
> , (128)
Î¸
? â† Î¸
? âˆ’ 1
1
> (1 âˆ’ Î¸
?
)
âˆ’1 âˆ’ q
>
1> (1 âˆ’ Î¸
?
)
âˆ’2
. (129)
Each step can be optimized by computing first an element-wise matrix inverse (1 âˆ’ Î¸
?
)
âˆ’1
for the numerator, and then performing an element-wise matrix multiplication of this matrix
by itself to obtain a matrix for the denominator instead of applying an additional elementwise matrix power. Since Ïˆ
0
is convex and strictly increasing with Ïˆ
00 positive everywhere,
the convergence of the updates is guaranteed.
5.3 Fermi-Dirac Entropy and Logistic Loss Function
Assumptions (A) again hold for minus the Fermi-Dirac entropy Ï€ log Ï€ + (1 âˆ’ Ï€) log(1 âˆ’ Ï€),
also known as bit entropy, associated to a logistic loss function. The ROT problem can thus
be solved with the ASA scheme, and the Newton-Raphson steps to update the alternate
projections in the POCS technique can be written as follows:
Î¸
? â† Î¸
? âˆ’
exp Î¸
?
1+exp Î¸
? 1 âˆ’ p
exp Î¸
?
(1+exp Î¸
?
)
2 1
1
> , (130)
Î¸
? â† Î¸
? âˆ’ 1
exp Î¸
?
1+exp Î¸
? âˆ’ q
>
1> exp Î¸
?
(1+exp Î¸
?
)
2
. (131)
Each step can be optimized by storing first the element-wise matrix exponential exp Î¸
?
,
then applying an element-wise matrix division by the temporary matrix 1+exp Î¸
?
to obtain
a matrix for the numerator, and lastly performing an element-wise matrix division of these
two matrices to obtain a matrix for the denominator and thus save an additional elementwise matrix power as well as several element-wise matrix exponentials. However, even if
Ïˆ
0
is strictly increasing with Ïˆ
00 positive everywhere, Ïˆ
0
is neither convex nor concave and
does not verify the necessary and sufficient condition (40) for global convergence of the
Newton-Raphson method.
Nevertheless, Ïˆ
0
is convex on Râˆ’ and concave on R+. It thus divides for a given 1 â‰¤
i â‰¤ d, respectively 1 â‰¤ j â‰¤ d, the real line into at most d + 1 intervals âˆ’âˆž < Ë†Î¸
(1)
i â‰¤ Ë†Î¸
(2)
i â‰¤
Â· Â· Â· â‰¤ Ë†Î¸
(dâˆ’1)
i â‰¤ Ë†Î¸
(d)
i < +âˆž, respectively âˆ’âˆž < Ë‡Î¸
(1)
j â‰¤ Ë‡Î¸
(2)
j â‰¤ Â· Â· Â· â‰¤ Ë‡Î¸
(dâˆ’1)
j â‰¤ Ë‡Î¸
(d)
j < +âˆž, with
the values (Ë†Î¸
(k)
i
)
1â‰¤kâ‰¤d
from row i of Î¸, respectively (Ë‡Î¸
(k)
j
)
1â‰¤kâ‰¤d
from column j of Î¸, sorted
37
Dessein, Papadakis and Rouas
in increasing order. On each of these intervals, the necessary and sufficient condition (40) is
verified since we can decompose f(Âµi), respectively g(Î½j ), as the sum of an increasing convex
and an increasing concave function. Hence, we have global convergence on the interval that
contains the solution. It is further possible to restrict the search to the two last intervals only.
Indeed, we have Pd
j=1 Ïˆ
0
(Î¸ij âˆ’ Ë†Î¸
(dâˆ’1)
i
) â‰¥ Ïˆ
0
(
Ë†Î¸
(dâˆ’1)
i âˆ’ Ë†Î¸
(dâˆ’1)
i
) +Ïˆ
0
(
Ë†Î¸
(d)
i âˆ’ Ë†Î¸
(dâˆ’1)
i
) â‰¥ 2Ïˆ
0
(0) = 1,
so that Ë†Î¸
(dâˆ’1)
i < Âµi < +âˆž. Similarly, we have Pd
i=1 Ïˆ
0
(Î¸ij âˆ’ Ë‡Î¸
(dâˆ’1)
j
) â‰¥ Ïˆ
0
(
Ë‡Î¸
(dâˆ’1)
j âˆ’ Ë‡Î¸
(dâˆ’1)
j
) +
Ïˆ
0
(
Ë‡Î¸
(d)
j âˆ’ Ë‡Î¸
(dâˆ’1)
j
) â‰¥ 2Ïˆ
0
(0) = 1, so that Ë‡Î¸
(dâˆ’1)
j < Î½j < +âˆž. As a result, it suffices to initialize
Âµi with Ë†Î¸i = Ë†Î¸
(d)
i = max {Î¸ij}1â‰¤jâ‰¤d
, respectively Î½j with Ë‡Î¸j = Ë‡Î¸
(d)
j = max {Î¸ij}1â‰¤iâ‰¤d
, to
guarantee convergence of the updates.
5.4 Î²-potentials and Î²-divergences
Assumptions (A) hold for the Î²-potentials (Ï€
Î² âˆ’ Î²Ï€ + Î² âˆ’ 1)/(Î²(Î² âˆ’ 1)) with 0 < Î² < 1,
associated to the so-called Î²-divergences. Hence, the ROT problem can be solved with the
ASA scheme, and the Newton-Raphson steps to update the alternate projections in the
POCS technique can be written as follows:
Î¸
? â† Î¸
? âˆ’
((Î² âˆ’ 1)Î¸
? + 1)
1
Î²âˆ’1 1 âˆ’ p
((Î² âˆ’ 1)Î¸
? + 1)
1
Î²âˆ’1 âˆ’1
1
1
> , (132)
Î¸
? â† Î¸
? âˆ’ 1
1
> ((Î² âˆ’ 1)Î¸
? + 1)
1
Î²âˆ’1 âˆ’ q
>
1> ((Î² âˆ’ 1)Î¸
? + 1)
1
Î²âˆ’1 âˆ’1
. (133)
Each step can be optimized by computing first the temporary matrix (Î² âˆ’1)Î¸
? + 1, then
applying an element-wise matrix power of 1/(Î² âˆ’ 1) âˆ’ 1 to this temporary matrix to obtain
a matrix for the denominator, and lastly performing an element-wise matrix multiplication
of these two matrices to obtain a matrix for the numerator and thus save one element-wise
matrix power. Since Ïˆ
0
is convex and strictly increasing with Ïˆ
00 positive, the convergence
of the updates is guaranteed.
Interestingly, the regularizer tends to minus the Burg and Boltzmann-Shannon entropies
in the limit Î² = 0 and Î² = 1, respectively. Therefore, the Î²-divergences interpolate between
the Itakura-Saito and Kullback-Leibler divergences. We finally remark that the regularizer
can also be defined for other values of the parameter Î² using the same formula, but do not
verify assumptions (A) for these values.
5.5 `p quasi-norms
Considering regularizers âˆ’Ï€
p with 0 < p < 1, all assumptions (A) are verified except from
(A5) since R
dÃ—d
âˆ’ 6âŠ‚ dom Ïˆ = R
dÃ—d
âˆ’âˆ’ . Hence, our primal formulation does not hold here
because 0 âˆˆ/ dom âˆ‡Ïˆ. However, it is straightforward to check that our dual formulation for
ROT problems with the ASA scheme can still be applied as long as the cost matrix Î³ does
not have null entries so that âˆ’Î³/Î» âˆˆ dom âˆ‡Ïˆ. Eventually, the Newton-Raphson steps to
update the alternate projections in the POCS technique can be written as follows:
Î¸
? â† Î¸
? +
(âˆ’Î¸
?
)
1
pâˆ’1 1 âˆ’ p
1
pâˆ’1 p
1
pâˆ’1
(âˆ’Î¸
?
)
1
pâˆ’1 âˆ’1
1
1
> , (134)
38
Rot Moverâ€™s Distance
Î¸
? â† Î¸
? + 1
1
> (âˆ’Î¸
?
)
1
pâˆ’1 âˆ’ p
1
pâˆ’1 q
>
1
pâˆ’1
1> (âˆ’Î¸
?
)
1
pâˆ’1 âˆ’1
. (135)
Each step can be optimized by computing first the temporary matrix âˆ’Î¸
?
, then applying
an element-wise matrix power of 1/(p âˆ’ 1) âˆ’ 1 to obtain a matrix for the denominator, and
lastly performing an element-wise matrix multiplication of these two matrices to obtain
a matrix for the numerator and thus save one element-wise matrix power. Since Ïˆ
0
is
convex and strictly increasing with Ïˆ
00 positive everywhere, the convergence of the updates
is guaranteed.
5.6 `p norms
Assumptions (B) hold for the `p norms |Ï€|
p with 1 < p < +âˆž, so the ROT problem can
be solved with the NASA scheme. For p 6= 2, the Newton-Raphson steps to update the
alternate projections in Dykstraâ€™s algorithm can be written as follows:
Ï„ â† Ï„ +
n
sgn(Î¸
? âˆ’ Ï„1
>)  |Î¸
? âˆ’ Ï„1
>|
1
pâˆ’1
o
1 âˆ’ p
1
pâˆ’1 p
1
pâˆ’1
|Î¸
? âˆ’ Ï„1>|
1
pâˆ’1 âˆ’1
1
, (136)
Ïƒ â† Ïƒ +
1
>
n
sgn(Î¸
? âˆ’ Ï„1
>)  |Î¸
? âˆ’ Ï„1
>|
1
pâˆ’1
o
âˆ’ p
1
pâˆ’1 q
>
1
pâˆ’1
1> |Î¸
? âˆ’ Ï„1>|
1
pâˆ’1 âˆ’1
. (137)
Denoting Î¸ = Î¸
? âˆ’ Ï„1
> or Î¸ = Î¸
? âˆ’ 1Ïƒ
> in the respective updates, each step can
be optimized by computing first the temporary matrix |Î¸|, then applying an element-wise
matrix power of 1/(pâˆ’1)âˆ’1 to obtain a matrix for the denominator, and lastly performing
an element-wise matrix multiplication of these two matrices and of sgn Î¸ to obtain a matrix
for the numerator and thus save one element-wise matrix power as well as several vector
replications and matrix subtractions. However, even if Ïˆ
0
is strictly increasing with Ïˆ
00 > 0
on R
âˆ—
, Ïˆ
0
is neither convex nor concave and does not verify the necessary and sufficient condition (40) for global convergence of the Newton-Raphson method. Moreover, Ïˆ
00 vanishes
at 0 for p < 2, and Ïˆ
0
is not differentiable at 0 for p > 2.
Nevertheless, Ïˆ
0
is concave on Râˆ’ and convex on R+ for p < 2, as well as convex on Râˆ’
and concave on R+ for p > 2. It thus divides for a given 1 â‰¤ i â‰¤ d, respectively 1 â‰¤ j â‰¤ d,
the real line into at most d + 1 intervals âˆ’âˆž < Ë†Î¸
(1)
i â‰¤ Ë†Î¸
(2)
i â‰¤ Â· Â· Â· â‰¤ Ë†Î¸
(dâˆ’1)
i â‰¤ Ë†Î¸
(d)
i < +âˆž,
respectively âˆ’âˆž < Ë‡Î¸
(1)
j â‰¤ Ë‡Î¸
(2)
j â‰¤ Â· Â· Â· â‰¤ Ë‡Î¸
(dâˆ’1)
j â‰¤ Ë‡Î¸
(d)
j < +âˆž, with the values (Ë†Î¸
(k)
i
)
1â‰¤kâ‰¤d
from row i of Î¸, respectively (Ë‡Î¸
(k)
j
)
1â‰¤kâ‰¤d
from column j of Î¸, sorted in increasing order. The
necessary and sufficient condition (40) is verified on the interior of each of these intervals
since we can decompose f(Âµi), respectively g(Î½j ), as the sum of an increasing convex and
an increasing concave function. Hence, we have global convergence on the interior of the
interval that contains the solution. In both cases, we must remove the finite endpoints
to ensure differentiability of Ïˆ
0 and positivity of Ïˆ
00. It is also further possible to prune
the last interval from the search. Indeed, we have Pd
j=1 Ïˆ
0
(Î¸ij âˆ’ Ë†Î¸
(d)
i
) â‰¤
Pd
j=1 Ïˆ
0
(0) =
0, so that Âµi < Ë†Î¸i = Ë†Î¸
(d)
i = max {Î¸ij}1â‰¤jâ‰¤d
. Similarly, we have Pd
i=1 Ïˆ
0
(Î¸ij âˆ’ Ë‡Î¸
(d)
j
) â‰¤
39
Dessein, Papadakis and Rouas
Pd
i=1 Ïˆ
0
(0) = 0, so that Î½j < Ë‡Î¸j = Ë‡Î¸
(d)
j = max {Î¸ij}1â‰¤iâ‰¤d
. Lastly, we can restrict the first
interval with a finite lower bound instead. Indeed, we have Pd
j=1 Ïˆ
0
(Î¸ij âˆ’ Ë†Î¸
(1)
i + Ï†
0
(pi/d)) â‰¥
Pd
j=1 Ïˆ
0
(Ï†
0
(pi/d)) = pi
, so that Âµi â‰¥ Ë†Î¸
(1)
i âˆ’ Ï†
0
(pi/d). Similarly, we have Pd
i=1 Ïˆ
0
(Î¸ij âˆ’
Ë‡Î¸
(1)
j + Ï†
0
(qj/d)) â‰¥
Pd
i=1 Ïˆ
0
(Ï†
0
(qj/d)) = qj , so that Î½j â‰¥ Ë‡Î¸
(1)
j âˆ’ Ï†
0
(qj/d). As a result, we can
perform at most d binary searches in parallel to determine within which of the remaining
bounded intervals the solutions Âµi
, respectively Î½j , lie. Initialization is then done with the
midpoint to guarantee convergence of the updates. A given search thus requires a worstcase logarithmic number of tests, each of which requires a linear number of operations, for
a total complexity in O(d
2
log d) instead of O(d
2
) if no such binary search were needed.
Now for p = 2, the regularizer specializes to the Euclidean norm, leading to the squared
Euclidean distance as the associated divergence. In addition, the formula for Ïˆ
00 still holds
with the convention 00 = 1, and Ïˆ
00 is actually constant equal to 1/2. Eventually, the projections can be written in closed form, and we can resort to the analytical algorithm derived
in the next example specifically for the Euclidean distance, after doubling the penalty Î» to
account for the regularizer being halved.
5.7 Euclidean Norm and Euclidean Distance
Assumptions (B) hold for half the Euclidean norm Ï€
2/2 associated to half the squared
Euclidean distance. Therefore, the ROT problem can be solved with the NASA scheme,
where Dykstraâ€™s algorithm can actually be written in closed form. Specifically, the nonnegative projection reduces to:
Ï€
? â† max{0,Ï€e} , (138)
and is interleaved with the scaling projections which amount to offsetting the rows and
columns of Ï€e by an amount such that the rows and columns of Ï€
?
sum up to p and q
respectively:
Ï€e â† Ï€e âˆ’
1
d
(Ï€
?1 âˆ’ p) 1
> , (139)
Ï€e â† Ï€e âˆ’
1
d
1 (1
>Ï€
? âˆ’ q
>) . (140)
As a remark, we notice that half the squared Euclidean distance can be seen as a
Î²-divergence using the provided formula for Î² = 2. However, the Î²-divergence generated is
not of Legendre type because the domain is restricted to R+, whereas it could actually be
extended to R so that the regularizer would then be of Legendre type. This is why we fall
under assumptions (B) rather than assumptions (A) in this case.
5.8 Hellinger Distance
Assumptions (B) hold for the regularizer âˆ’(1 âˆ’ Ï€
2
)
1
2 akin to a Hellinger distance. Hence,
the ROT problem can be solved with the NASA scheme, and the Newton-Raphson steps to
40
Rot Moverâ€™s Distance
update the alternate projections in Dykstraâ€™s algorithm can be written as follows:
Ï„ â† Ï„ +

(Î¸
? âˆ’ Ï„1
>) 

1 + (Î¸
? âˆ’ Ï„1
>)
2
âˆ’ 1
2

1 âˆ’ p

1 + (Î¸
? âˆ’ Ï„1>)
2
âˆ’ 3
2
1
, (141)
Ïƒ â† Ïƒ +
1
>

(Î¸
? âˆ’ 1Ïƒ
>) 

1 + (Î¸
? âˆ’ 1Ïƒ
>)
2
âˆ’ 1
2

âˆ’ q
>
1>

1 + (Î¸
? âˆ’ 1Ïƒ>)
2
âˆ’ 3
2
. (142)
Denoting Î¸ = Î¸
? âˆ’Ï„1
> or Î¸ = Î¸
? âˆ’1Ïƒ
> in the respective updates, each step can be optimized by computing first the temporary matrix 1/(1 +Î¸
2
), then applying an element-wise
matrix square root to this temporary matrix, performing an element-wise matrix multiplication of these two matrices to obtain a matrix for the denominator, and lastly an element-wise
matrix multiplication of the temporary matrix with Î¸ to obtain a matrix for the numerator
and thus save one element-wise matrix power as well as several vector replications and matrix subtractions. However, even if Ïˆ
0
is strictly increasing with Ïˆ
00 positive everywhere, Ïˆ
0
is neither convex nor concave and does not verify the necessary and sufficient condition (40)
for global convergence of the Newton-Raphson method.
Nevertheless, Ïˆ
0
is convex on Râˆ’ and concave on R+. It thus divides for a given 1 â‰¤
i â‰¤ d, respectively 1 â‰¤ j â‰¤ d, the real line into at most d + 1 intervals âˆ’âˆž < Ë†Î¸
(1)
i â‰¤ Ë†Î¸
(2)
i â‰¤
Â· Â· Â· â‰¤ Ë†Î¸
(dâˆ’1)
i â‰¤ Ë†Î¸
(d)
i < +âˆž, respectively âˆ’âˆž < Ë‡Î¸
(1)
j â‰¤ Ë‡Î¸
(2)
j â‰¤ Â· Â· Â· â‰¤ Ë‡Î¸
(dâˆ’1)
j â‰¤ Ë‡Î¸
(d)
j < +âˆž, with
the values (Ë†Î¸
(k)
i
)
1â‰¤kâ‰¤d
from row i of Î¸, respectively (Ë‡Î¸
(k)
j
)
1â‰¤kâ‰¤d
from column j of Î¸, sorted
in increasing order. On each of these intervals, the necessary and sufficient condition (40)
is verified since we can decompose f(Âµi), respectively g(Î½j ), as the sum of an increasing
convex and an increasing concave function. Hence, we have global convergence on the
interval that contains the solution. It is further possible to prune the last interval from the
search. Indeed, we have Pd
j=1 Ïˆ
0
(Î¸ij âˆ’ Ë†Î¸
(d)
i
) â‰¤
Pd
j=1 Ïˆ
0
(0) = 0, so that Âµi < Ë†Î¸i = Ë†Î¸
(d)
i =
max {Î¸ij}1â‰¤jâ‰¤d
. Similarly, we have Pd
i=1 Ïˆ
0
(Î¸ij âˆ’ Ë‡Î¸
(d)
j
) â‰¤
Pd
i=1 Ïˆ
0
(0) = 0, so that Î½j < Ë‡Î¸j =
Ë‡Î¸
(d)
j = max {Î¸ij}1â‰¤iâ‰¤d
. Lastly, we can restrict the first interval with a finite lower bound
instead. Indeed, we have Pd
j=1 Ïˆ
0
(Î¸ij âˆ’ Ë†Î¸
(1)
i + Ï†
0
(pi/d)) â‰¥
Pd
j=1 Ïˆ
0
(Ï†
0
(pi/d)) = pi
, so that
Âµi â‰¥ Ë†Î¸
(1)
i âˆ’Ï†
0
(pi/d). Similarly, we have Pd
i=1 Ïˆ
0
(Î¸ij âˆ’ Ë‡Î¸
(1)
j +Ï†
0
(qj/d)) â‰¥
Pd
i=1 Ïˆ
0
(Ï†
0
(qj/d)) =
qj , so that Î½j â‰¥ Ë‡Î¸
(1)
j âˆ’ Ï†
0
(qj/d). As a result, we can perform d binary searches in parallel
to determine within which of the remaining intervals the solutions Âµi
, respectively Î½j , lie.
Initialization is then done with the midpoint to guarantee convergence of the updates. A
given search requires a worst-case logarithmic number of tests, each of which requires a
linear number of operations, for a total complexity in O(d
2
log d) instead of O(d
2
) if no such
binary search were needed.
5.9 Quadratic Forms and Mahalanobis Distances
Assumptions (B) hold for the quadratic forms (1/2) vec(Ï€)
>P vec(Ï€) with positive-definite
matrix P âˆˆ R
d
2Ã—d
2
, associated to the Mahalanobis distances, so the ROT problem can be
41
Dessein, Papadakis and Rouas
Figure 3: Earth moverâ€™s plan Ï€
?
for the cost matrix Î³ and input distributions p, q.
solved with the NASA scheme. For a diagonal matrix P, the regularizer is separable and
the Newton-Raphson steps to update the alternate projections in Dykstraâ€™s algorithm are
similar to that for the Euclidean distance with appropriate weights. For a non-diagonal
matrix P, however, the regularizer is not separable anymore and we must resort to the
generic NASA scheme.
In this general case, the scaling projections amount to convex quadratic programs with
linear equality constraints. They can be solved using classical techniques such as the rangespace and null-space approaches, Krylov subspace methods or active set strategies. The
non-negative projection reduces to a convex quadratic program with a linear inequality
constraint. It can be solved elegantly with an iterative algorithm for non-negative quadratic
programming proposed by Sha et al. (2007) using multiplicative updates with a complexity
in O(d
4
). All in all, we recommend using a sparse matrix P with a block-diagonal structure
and an order of magnitude of d
2 non-null entries, so as to obtain a quadratic instead of
quartic empirical complexity.
6. Experimental Results
In this section, we present the results of our methods on different experiments. We first
design an synthetic test to showcase the behavior of different regularizers and penalties
on the output solutions or computational times (Section 6.1). We then consider a pattern
recognition application to audio scene classification on a real-world dataset (Section 6.2).
6.1 Synthetic Data
We start by visualizing the effects of different regularizers Ï† and varying penalties Î» on
synthetic data. For the input distributions, we discretize and normalize continuous densities
on a uniform grid (xi)
1â‰¤iâ‰¤d
of [0, 1] with dimension d = 256. We use for p a univariate
normal with mean 0.5 and variance 0.2, and for q a mixture of two normals with equal
weights, respective means 0.25 and 0.75, and same variance 0.1. We set the cost matrix
Î³ as the squared Euclidean distance Î³ij = (xi âˆ’ xj )
2
on the grid. The input distributions
(bottom left and top right), cost matrix (top left) and unique earth moverâ€™s plan (bottom
right) computed for classical OT using the solver of Rubner et al. (2000) with standard
settings, are shown in Figure 3.
42
Rot Moverâ€™s Distance
We test all separable regularizers Ï† introduced in Section 5. Because these regularizers
have different ranges in the sensible values of the rot moverâ€™s plans Ï€
?
, we manually tune the
penalties Î» so that they feature similar amounts of regularization. For ease of comparison,
we set Î» = Î» Î»0
, with Î» constant for each Ï†, and Î»
0 varying similarly for all Ï†. The limit case
when Î» tends to infinity is simply obtained by setting Î³/Î» = 0 in the algorithms, except
from `p quasi-norms for which we use Î» = 1010. The null values of Î³ are also fixed to 10âˆ’12
for `p quasi-norms. We do not limit the number of iterations in the different algorithms, and
use a small tolerance of 10âˆ’8
for convergence with the `âˆž norm on the marginal difference
checked after each iteration as a termination criterion.
The rot moverâ€™s plans obtained for ROT for d = 256 with the different regularizers and
penalties are visualized in Figure 4. We first observe that all rot moverâ€™s plans converge to
the earth moverâ€™s plan for low values of the penalty as shown theoretically in Property 9.
Nevertheless, the rot moverâ€™s plans exhibit different shapes depending on the regularizers for
intermediary and large values of the penalty. In the limit when the penalty grows to infinity,
we obtain the transport plan with minimal Bregman information as shown theoretically in
Property 7. In particular, this leads to pq> with an ellipsoidal shape for BSKL (BoltzmannShannon entropy and Kullback-Leibler divergence), meaning that the mass is relatively
spread among neighbor bins. The same pattern is observed for FDLOG (Fermi-Dirac entropy
and logistic loss function), which can be explained in this synthetic example by the rot
moverâ€™s plans having low values and the two regularizers being equivalent up to a constant
in the neighborhood of zero. The profile gets more rectangular for BIS (Burg entropy and
Itakura-Saito divergence), implying that the mass is even more spread across the different
bins. Using an intermediary value Î² = 0.5 in BETA (Î²-potentials and Î²-divergences) allows
the interpolation between these two limits of a rectangle for Î² = 1 and an ellipsoid for Î² = 0,
so that the parameter Î² actually helps to control the spread of mass in the regularization.
We observe similar results for LPQN (`p quasi-norms) with an ellipsoid for p = 0.9, a rectangle
for p = 0.1, and a shape in between for p = 0.5. When the power parameter further increases
in LPN (`p norms), we obtain new shapes that feature less spread of mass. These shapes for
p = 1.1 and p = 1.5 now interpolate up to a lozenge for p = 2 in EUC (Euclidean norm and
Euclidean distance), so that the parameter p also provides control on the spread of mass.
A similar diamond profile is obtained for HELL (Hellinger distance), which is due again to
the rot moverâ€™s plans having low values and the two regularizers being equivalent up to a
constant in the neighborhood of zero. Lastly, we remark that varying the penalty between
the two extremes allows a smooth interpolation of the earth moverâ€™s plan and optimal plan
with minimal Bregman information, while keeping similar shapes and effects in terms of
spreading of mass.
We next report in Table 4 the computational times required to reach convergence for the
different regularizers and penalties. As a stopping criterion, we use the relative variation
with tolerance 10âˆ’2
in `2 norm for the main loop of alternate Bregman projections, and the
absolute variation with tolerance 10âˆ’5
in `2 norm for the auxiliary loops of the NewtonRaphson method. We use the same synthetic data as above but also vary the dimension d to
assess its influence on speed. As already observed specifically for Sinkhorn distances (Cuturi,
2013), computing ROT distances is faster for important regularization with larger values
of Î». The regularizers under assumptions (A) do not require the extra projections onto the
non-negative orthant, and thus intuitively require less computational effort than the ones
43
Dessein, Papadakis and Rouas
Ï†
Î»
/Î»
0 10âˆ’2 10âˆ’1 10+0 10+1 +âˆž
â€”FDLOG
â€”10
âˆ’2
BSKL
BETA
Î² = 1
10
âˆ’2
â€”BETA
Î² = 0.5
10
âˆ’4
BIS
BETA
Î² = 0
10
âˆ’6
â€”LPQN
p = 0.1
10
âˆ’4
â€”LPQN
p = 0.5
10
âˆ’3
â€”LPQN
p = 0.9
10
âˆ’1
â€”LPN
p = 1.1
10+0
â€”LPN
p = 1.5
10+1
EUC
LPN
p = 2
10+2
â€”HELL
â€”10+2
Figure 4: Rot moverâ€™s plans Ï€
?
for different regularizers Ï† and penalties Î» = Î» Î»0
.
44
Rot Moverâ€™s Distance
that verify assumptions (B). In addition, we notice that when the projections have closedform expressions, the algorithms are also faster. The results further illustrate the influence
of the data dimension d and the difference between ROT and classical OT performances. For
a low dimension d, the RMD is competitive with EMD in his historical implementation EMD
OLD (Rubner et al., 2000). The super-cubic complexity of the EMD with EMD OLD becomes
prohibitive as the data dimension increases in contrast to the RMD which scales better. It
should nevertheless be underlined that for reasonable dimensions, fast computation of the
EMD can be obtained with a more recent, optimized implementation of the network simplex
solver EMD NEW (Bonneel et al., 2011). For higher dimensions, the super-cubic complexity
makes EMD NEW less attractive, though it stays competitive with the RMD under a dimension
d = 4096.
As a consequence, a numerical alternative to our algorithms for solving ROT problems
with reasonable dimensions is to rely on conditional gradient methods similar to (Ferradans
et al., 2014). Indeed, such methods imply the iterative resolution of linearized ROT problems, that can be reformulated as EMD problems and therefore be solved with the fast
network simplex approach (Bonneel et al., 2011). Lastly, for a fair interpretation of the
above timing results, we must mention that the two EMD schemes tested were run under
MATLAB from native C/C++ implementations1,2 via compiled MEX files3,4. Hence, these
EMD codes are quite optimized in comparison to our pure MATLAB prototype codes5
for
the RMD. It is thus plausible that optimized C/C++ implementations of our algorithms
would be even more competitive in this context.
6.2 Audio Classification
We now assess our methods in the context of audio classification, and specifically address
the task of acoustic scene classification where the goal is to assign a test recording to one of
predefined classes that characterizes the environment in which it was captured. We consider
the framework of the DCASE 2016 IEEE AASP challenge with the TUT Acoustic Scenes
2016 database (Mesaros et al., 2016). The data set consists of audio recordings at 44.1 kHz
sampling rate and 24-bit resolution. The metadata contains ground-truth annotations on
the type of acoustic scene for all files, with a total of 15 classes: home, office, library,
cafÂ´e/restaurant, grocery store, city center, residential area, park, forest path, beach, car,
train, bus, tram, metro station. The audio material is cut into 30-second segments, and
is split into two subsets of 75%â€“25% containing respectively 78â€“26 segments per class for
development and evaluation, resulting in a total of 1170â€“390 files for training and testing.
A 4-fold cross-validation setup is given with the training set. The classification accuracy,
that is, the number of correctly classified segments among the total number of segments, is
used as a score to evaluate systems.
A baseline system is also provided with the database for comparison. This system is
based on Mel-frequency cepstral coefficient (MFCC) timbral features with Gaussian mixture
model (GMM) classification. One GMM with diagonal covariance matrix is learned per class
1
http://robotics.stanford.edu/~rubner/emd/default.htm
2
http://liris.cnrs.fr/~nbonneel/FastTransport/
3
https://github.com/francopestilli/life/tree/master/external/emd
4
https://arolet.github.io/code/
5
https://www.math.u-bordeaux.fr/~npapadak/GOTMI/codes.php
45
Dessein, Papadakis and Rouas
d 128 256 512
Algorithm Ï† Î²/p Î»/Î»
0 10âˆ’2 10âˆ’1 10+0 10âˆ’2 10âˆ’1 10+0 10âˆ’2 10âˆ’1 10+0
RMD â€” FDLOG â€” 10âˆ’2 0.366 0.079 0.044 1.091 0.311 0.116 1.865 0.571 0.273
RMD BSKL BETA 1.00 10âˆ’2 0.105 0.013 0.008 0.259 0.055 0.017 0.680 0.100 0.038
RMD â€” BETA 0.50 10âˆ’4 0.971 0.102 0.044 1.922 0.251 0.147 3.526 1.339 0.281
RMD BIS BETA 0.00 10âˆ’6 0.916 0.106 0.019 1.466 0.108 0.053 2.598 0.398 0.096
RMD â€” LPQN 0.10 10âˆ’4 0.968 0.068 0.055 0.732 0.173 0.152 0.416 0.309 0.305
RMD â€” LPQN 0.50 10âˆ’3 0.404 0.057 0.042 0.778 0.163 0.160 0.780 0.305 0.304
RMD â€” LPQN 0.90 10âˆ’1 0.226 0.047 0.040 0.751 0.178 0.131 1.110 2.492 0.214
RMD â€” LPN 1.10 10+0 1.570 0.349 0.148 5.941 1.557 0.492 6.357 0.293 0.926
RMD â€” LPN 1.50 10+1 0.399 0.099 0.053 1.170 0.474 0.166 6.688 2.163 0.532
RMD EUC LPN 2.00 10+2 0.074 0.043 0.043 0.253 0.240 0.237 7.308 3.190 0.966
RMD â€” HELL â€” 10+2 0.197 0.097 0.087 0.429 0.316 0.299 5.570 1.826 0.823
EMD OLD â€” â€” â€” â€” 0.231 1.912 10.95
EMD NEW â€” â€” â€” â€” 0.003 0.011 0.076
d 1024 2048 4096
Algorithm Ï† Î²/p Î»/Î»
0 10âˆ’2 10âˆ’1 10+0 10âˆ’2 10âˆ’1 10+0 10âˆ’2 10âˆ’1 10+0
RMD â€” FDLOG â€” 10âˆ’2 4.156 3.517 1.410 15.85 9.663 5.109 54.19 33.87 17.24
RMD BSKL BETA 1.00 10âˆ’2 2.992 0.705 0.192 13.42 1.923 0.630 49.95 7.074 2.548
RMD â€” BETA 0.50 10âˆ’4 8.015 2.769 0.888 42.95 7.538 3.557 101.3 21.13 10.86
RMD BIS BETA 0.00 10âˆ’6 4.439 0.777 0.550 6.590 3.262 2.218 41.80 12.96 6.742
RMD â€” LPQN 0.10 10âˆ’4 4.068 2.174 1.291 6.962 4.890 4.334 51.15 15.86 14.02
RMD â€” LPQN 0.50 10âˆ’3 7.819 4.198 1.314 26.34 6.129 4.301 53.74 15.65 11.98
RMD â€” LPQN 0.90 10âˆ’1 3.584 2.264 1.054 13.80 4.571 3.285 43.83 14.22 11.51
RMD â€” LPN 1.10 10+0 9.110 4.924 1.956 38.98 16.47 8.400 145.6 65.95 32.82
RMD â€” LPN 1.50 10+1 18.97 9.509 2.539 61.92 20.41 9.314 236.6 77.87 45.94
RMD EUC LPN 2.00 10+2 11.90 5.805 2.161 31.43 14.22 4.906 117.1 50.88 27.67
RMD â€” HELL â€” 10+2 18.22 6.629 3.456 35.45 20.03 7.199 205.0 48.42 31.75
EMD OLD â€” â€” â€” â€” 85.56 482.2 +âˆž
EMD NEW â€” â€” â€” â€” 0.482 2.760 13.23
Table 4: Computational times in seconds required to reach convergence for different regularizers Ï† and penalties Î» = Î» Î»0
, with varying dimensions d.
by expectation-maximization (EM), after concatenating and normalizing in mean and variance the extracted MFCCs from the training segments in that class. A test file is assigned
to the class whose trained GMM leads to maximum likelihood for the extracted MFCCs
for that file, where the MFCCs are considered as independent samples and normalized with
the learned mean and variance for the respective classes. The baseline system is ran with
its default parameters: 40 ms frame size, 20 ms hop size, 60-dimensional MFCCs comprising
20 static (including energy) plus 20 delta and 20 acceleration coefficients extracted with
46
Rot Moverâ€™s Distance
standard settings in RASTAMAT, 16 GMM components learned with standard settings in
VOICEBOX.
Since MFCCs potentially take negative values, OT tools cannot be applied directly to
this kind of features. Therefore, the common approach is to compute OT appropriately on
GMMs estimated from MFCCs instead. Our proposed system follows this principle, and
is implemented in the very same pipeline as the baseline for a fair comparison, with the
following differences. One GMM is learned by EM for each training segment instead of
class. Any normalization on the MFCCs per class is thus removed. Since less components
are typically required to model one segment compared to one class, the spurious GMM
components are further discarded as post-processing by keeping only those with weight and
variances all greater than 10âˆ’2
. Instead of applying a GMM classifier, all individual models
are exploited to train a support vector machine (SVM) classifier. An exponential kernel for
the SVM is designed by introducing a distance between two mixtures P, Q based on the
RMD as follows:
Îº(P, Q) = exp(âˆ’dÎ³,Î»,Ï†(Ï‰, Ï…)/Ï„ ) , (143)
where the exponential decay rate Ï„ > 0 is a kernel parameter, and Ï‰, Ï… âˆˆ Î£d are the
respective weights of the d = 16 (or less) components for the two GMMs P, Q. The cost
matrix Î³ âˆˆ R
dÃ—d
+ depends on P, Q and is the square root of a symmetrized Kullback-Leibler
divergence, called the Jeffrey divergence, between the pairwise Gaussian components:
Î³ij =
vuut
1
4
X
l
k=1
(Ïƒ
2
ik âˆ’ Ï‚
2
jk)
2 + (Ïƒ
2
ik + Ï‚
2
jk)(Âµik âˆ’ Î½jk)
2
Ïƒ
2
ikÏ‚
2
jk
, (144)
where Âµi and Ïƒ
2
i
, respectively Î½j and Ï‚
2
j
, are the means and variances of the l = 60 MFCC
features for component i in the first mixture P, respectively component j in the second
mixture Q. The SVM classifier is implemented with standard settings in LIBSVM, and
requires an additional soft-margin parameter C > 0 to be tuned. Notice that, even if
the kernel is not positive-definite, LIBSVM is still able to provide a relevant classification
by guaranteeing convergence to a stationary point (Lin and Lin, 2003; Haasdonk, 2005;
Alabdulmohsin et al., 2014). All separable regularizers Ï† from Section 5 with different
penalties Î» > 0 are tested for the RMD in comparison to the EMD. The two distances
between p, q and q, p with cost matrix Î³ transposed are computed and averaged, so as
to remove any asymmetry due to practical issues. The number of iterations is limited to
100 for the main loop of the algorithm and to 10 for the auxiliary loops of the NewtonRaphson method, and the tolerance is set to 10âˆ’6
in all loops for convergence with the
`âˆž norm on the marginal difference checked after each iteration as a termination criterion.
The parameters Ï„, C âˆˆ 10{âˆ’1,+0,+1,+2} and penalty Î» âˆˆ Î›, where Î› is a manually chosen
set of four successive powers of ten depending on the range of the regularizer Ï†, are tuned
automatically by cross-validation.
The obtained results on this experiment in terms of accuracy per system are reported
in Table 5. The optimal penalties Î» âˆˆ Î› selected by cross-validation for each regularizer Ï†
are also included, while the optimal parameters Ï„, C are not displayed since they actually
all equal 10+1 independently of the kernel used. We first notice that the proposed system
SVM (support vector machine classifier) consistently outperforms the baseline system GMM
47
Dessein, Papadakis and Rouas
(Gaussian mixture model classifier). This proves the benefits of incorporating individual
information per sound via an SVM rather than exploiting global information per class with
a GMM. This further demonstrates the relevance of OT and more general ROT problems
for the design of kernels between GMMs in the SVM pipeline. We also notice that RMD (rot
moverâ€™s distance kernel) is at least competitive with EMD (earth moverâ€™s distance kernel) for
all proposed regularizers, except from EUC which does not perform as well. This might be a
consequence of the regularization profile for EUC, or equivalently LPN with p = 2, which does
not spread enough mass across similar bins, implying a lack of robustness to slight variations
in the means and variances of the GMM components. Reducing the power parameter in
LPN brings back to a competitive system with EMD for p = 1.1, and even a better trade-off
with improved accuracy for p = 1.5. We obtain similar results for LPQN with p = 0.9 and
p = 0.5, with now the best compromise for the lowest power value p = 0.1 which clearly
outperforms EMD. As a remark, the accuracy for LPN and LPQN is not unimodal with respect
to p which controls the spread of mass in the regularization. We suspect this is because
the performance is a function of both the spread of mass and the amount of regularization,
whose coupling allows for similar compromises in terms of results within different regimes
of use. Concerning BETA now, we observe that the existing Sinkhorn-Knopp algorithm BSKL
for Î² = 1 does not improve the accuracy compared to EMD. Increasing the spread of mass
with Î² = 0 in BIS is even worse. The best performance is obtained with a range in between
for Î² = 0.5, which slightly improves results over EMD. Using LOG here slightly degrades the
performance compared to EMD and BSKL. Interestingly, the overall best accuracy on this
application is obtained for HELL which beats all other systems, including EUC, by a safe
margin. In contrast to the experiment on synthetic data with dimension 256 presented in
Section 6.1, where both BSKL and LOG, respectively EUC and HELL, behave similarly due
to equivalence up to a constant for low values in the transport plans, the range of the
transport plans here is much higher since the dimension of the input distributions is at
most 16 (typically less than 10). This raises the importance of choosing a good regularizer
depending on the actual task and its inherent design criteria such as the data dimension.
7. Conclusion
In this paper, we formulated a unified framework for smooth convex regularization of discrete
OT problems. We also derived some algorithmic methods to solve such ROT problems,
and detailed their specificities for classical regularizers and associated divergences from the
literature. We finally designed a synthetic experiment to illustrate our proposed methods,
and proved the relevance of ROT problems and the RMD on a real-world application to
audio scene classification. The obtained results are encouraging for further development of
the present work, and we now discuss some interesting perspectives for future investigation.
Firstly, we want to assess the effect of other regularizers on the solutions, notably when
adding an affine term. From a geometrical viewpoint, such a transformation is equivalent to
simply translating the cost matrix, with no effect on the Bregman divergence itself. For a
given regularizer, we could therefore parametrize a whole family of interpolating regularizers,
and tune the translation parameter according to the application. In particular, a recent work
developed independently of ours makes use of Tsallis entropies to regularize OT problems
with ad hoc solvers (Muzellec et al., 2018). These regularizers could be integrated readily to
48
Rot Moverâ€™s Distance
Classifier Ï† Î²/p Î› Î» Accuracy
GMM â€” â€” â€” â€” â€” â€” 77.2%
SVM
EMD â€” â€” â€” â€” â€” 81.3%
RMD
â€” FDLOG â€” 10{âˆ’2,âˆ’1,+0,+1} 10âˆ’1 81.0%
BSKL BETA 1.00 10{âˆ’2,âˆ’1,+0,+1} 10+0 81.3%
â€” BETA 0.50 10{âˆ’3,âˆ’2,âˆ’1,+0} 10âˆ’2 81.5%
BIS BETA 0.00 10{âˆ’4,âˆ’3,âˆ’2,âˆ’1} 10âˆ’2 81.3%
â€” LPQN 0.10 10{âˆ’2,âˆ’1,+0,+1} 10âˆ’1 82.1%
â€” LPQN 0.50 10{âˆ’2,âˆ’1,+0,+1} 10âˆ’1 81.3%
â€” LPQN 0.90 10{âˆ’2,âˆ’1,+0,+1} 10+0 81.0%
â€” LPN 1.10 10{âˆ’1,+0,+1,+2} 10+0 81.0%
â€” LPN 1.50 10{+0,+1,+2,+3} 10+1 81.8%
EUC LPN 2.00 10{+1,+2,+3,+4} 10+3 77.4%
â€” HELL â€” 10{+1,+2,+3,+4} 10+2 82.8%
Table 5: Results of the experiment on audio classification.
our more general framework based on alternate Bregman projections, since Tsallis entropies
are equivalent to Î²-potentials and `p (quasi)-norms up to an affine term.
In another direction, we would like to extend some theoretical results that hold for the
Boltzmann-Shannon entropy and associated Kullback-Leibler divergence. Specifically, it
is known that the related rot moverâ€™s plan converges in norm to the earth moverâ€™s plan
with an exponential rate as the penalty decreases (Cominetti and San MartÂ´Ä±n, 1994). It
is not straightforward, however, to generalize this to other regularizers and divergences.
In addition, it would be worth elucidating some technical restrictions under which metric
properties such as the triangular inequality can be proved similarly to Sinkhorn distances.
We also plan to study other pattern recognition tasks in text, image and audio signal
processing. Intuitive possibilities include retrieval and classification for various kinds of
data modeled via histograms of features or GMMs. Among potential approaches, this can
be addressed by exploiting the RMD either directly in a nearest-neighbor search, or in the
design of kernels for an SVM as done here for acoustic scenes. For such tasks, it would be
relevant to provide insight into the choice of a good regularizer for the actual problem, or
develop methods for automatic tuning of regularization parameters, and for learning the
cost matrix from the data as can be done for the EMD (Cuturi and Avis, 2014). Even if we
mostly focused on separable regularizers, it would be relevant to further use the quadratic
forms associated to Mahalanobis distances in certain applications, and maybe propose a
parametric learning scheme for the quadratic regularizer from the data.
Lastly, a more prospective idea is to use the RMD instead of Sinkhorn distances in
the recent works built on the entropic regularization mentioned in Section 1. We also
think that variational ROT problems could be formulated for statistical inference, notably
parameter estimation in finite mixture models by minimizing loss functions based on the
RMD (Dessein et al., 2017). This would leverage new applications of our ROT framework for
more general machine learning problems. Such developments are yet involved and require
some theoretical effort before reaching enough maturity to address practical setups.