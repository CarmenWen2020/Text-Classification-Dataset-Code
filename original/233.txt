Recognition of a human's continuous emotional states in real time plays an important role in machine emotional intelligence and human-machine interaction. Existing real-time emotion recognition systems use stimuli with low ecological validity (e.g., picture, sound) to elicit emotions and to recognise only valence and arousal. To overcome these limitations, in this paper, we construct a standardised database of 16 emotional film clips that were selected from over one thousand film excerpts. Based on emotional categories that are induced by these film clips, we propose a real-time movie-induced emotion recognition system for identifying an individual's emotional states through the analysis of brain waves. Thirty participants took part in this study and watched 16 standardised film clips that characterise real-life emotional experiences and target seven discrete emotions and neutrality. Our system uses a 2-s window and a 50 percent overlap between two consecutive windows to segment the EEG signals. Emotional states, including not only the valence and arousal dimensions but also similar discrete emotions in the valence-arousal coordinate space, are predicted in each window. Our real-time system achieves an overall accuracy of 92.26 percent in recognising high-arousal and valenced emotions from neutrality and 86.63 percent in recognising positive from negative emotions. Moreover, our system classifies three positive emotions (joy, amusement, tenderness) with an average of 86.43 percent accuracy and four negative emotions (anger, disgust, fear, sadness) with an average of 65.09 percent accuracy. These results demonstrate the advantage over the existing state-of-the-art real-time emotion recognition systems from EEG signals in terms of classification accuracy and the ability to recognise similar discrete emotions that are close in the valence-arousal coordinate space.
SECTION 1Introduction
Affective computing has recently emerged as a converging technology for recognising, interpreting, and processing that relates to, arises from, or deliberately influences emotion or other affective phenomena. One of the key elements in affective computing is emotion recognition, which identifies human emotional states from their behavioral and physiological signals [1]. This information can be used to decide upon proper actions to express emotion and design new ways for machine emotional intelligence and human-machine interaction, e.g., in chatting robot and emotion-aware intelligent recommendation. With the rapid expansion of multimedia content, characterising affective multimedia content with discriminating tags is vital for multimedia information retrieval [2]. Currently, a large portion of user-defined tags are motivated by increasing his/her popularity and reputation in social networks. Moreover, users evaluate affective media content based on subjective judgments without consistent criteria. Some might tag multimedia content with words to express their emotions while others might use tags to describe the content. Therefore, implicit affective tagging (i.e., the effortless generation of subjective and/or emotional tags) can help recommendation and retrieval systems to improve their performance.

Some measures have been employed for emotion recognition, such as self-report, startle, behavioral responses, and physiological measures [3], [4]. Amongst them, electroencephalograph (EEG) provides a direct and comprehensive means for emotion recognition by detecting immediate responses to emotional stimuli with an excellent temporal resolution [5], [6], [7]. Compared to behavioral responses (e.g., facial expression, vocal intonation, and body posture), it is difficult to disguise an individual's real feelings by the recognition of emotions from EEG signals [8]. Compared to other physiological signals (e.g., electrocardiograph, skin temperature, and galvanic skin response), emotion recognition from EEG signals can achieve a higher classification accuracy [9].

Recently, new wireless headsets (e.g., Emotiv) have become more cost-effective, easier to use, and mobile, with increased practicability and less physical restriction [8]. EEG carries distinct advantages in practical usage and therefore has been a primary option for the development of online emotion recognition systems. In fact, there has been a growing amount of effort to recognise a person's emotion in real time from EEG signals. For example, EmoRate is a commercial product/application that detects and reacts to human emotions and will change drastically the way we interact with them in our daily lives. It is built on a 14-electrode EEG headset, created by Emotiv systems, Inc. The EmoRate system was trained by comedic shows, talk shows and videos on YouTube to recognise happiness, sadness, anger and fear. However, the recognition accuracy of the EmoRate system has not been systematically studied and reported.

There are two existing models of emotion that guide researchers to understand and construct emotional space: the discrete model and the dimension model. The discrete model considers that the emotional space consists of a limited number of basic discrete emotions. Although some viewpoints are not consistent, most studies tend to agree with at least six basic emotions: joy, sadness, surprise, fear, anger, and disgust [10] . On the other hand, the dimensional model divides the emotional space into two (i.e., valence-arousal, VA) or three (i.e., pleasure-arousal-dominance, PAD) dimensions [11]. The pleasure refers to the degree of pleasure that an individual feels, namely, the positive and negative characteristics (i.e., valence). Arousal reflects the mental vigilance level of emotion and the intensity of physiological activation that an individual feels. Dominance refers to an individual's status: in control or being controlled.

From the dimensional perspective to establish the emotional model, even if it is difficult to name a particular feeling, the dimensional model can use numerical values to mark different emotions in the valence-arousal coordinate space. However, although the dimensional model is easier to operate than the discrete model, when two or more discrete emotions are close in the valence-arousal coordinate space, the recognition ability of the dimensional model will be degraded dramatically [12]. One explanation is that similar emotions (such as anxiety and anger in the valence-arousal dimension) could have their own unique representation and induce completely different behavioral responses [13].

The existing studies of real-time emotion recognition, in essence, identify different valence and arousal classes instead of discrete emotions. Although researchers combined different valence and arousal classes and attempted to map them into basic emotions, it is different from the discrete emotions that are directly elicited from target materials. In addition, the intensities of the stimuli in the same category of emotion are different. We feel happy when we see a baby smiling. We also feel happy when we see a beautiful sunset, but the feeling might not be so strong [14]. Therefore, emotions in the same category (e.g., joy) could have different levels of valence and arousal, but not vice versa.

To overcome aforementioned limitations, in this paper, we propose a real-time system to recognise an individual's emotional states (including not only the valence and arousal dimensions but also similar discrete emotions in the valence-arousal coordinate space) based on EEG signals and make the following contributions:

To evoke target-specific emotion reliably and reproducibly, we construct a standardised database of 16 emotional film clips selected from over one thousand film excerpts, and build 8 emotional categories. The standardised database including participants’ self-ratings will be made publicly available1 to provide a useful benchmark for elicited emotion evaluation.

Our system outperforms state-of-the-art real-time emotion recognition systems in terms of classification accuracy and the ability to recognise similar discrete emotions that are close in the valence-arousal coordinate space. In particular, our system achieved an averaged accuracy of 92.26 percent (50 percent accuracy at random) in recognising high-arousal and valenced emotions from neutrality. Based on basic discrete emotions that were elicited by standardised film clips, our system classified three positive emotions (joy, amusement, tenderness) and four negative emotions (anger, disgust, fear, sadness) with an overall accuracy of 86.43 percent (33.33 percent accuracy at random) and 65.09 percent (25 percent accuracy at random), respectively.

SECTION 2Literature Review
A few existing studies have implemented real-time EEG-based emotion detection systems (see Table 1).

TABLE 1 Existing Real-Time EEG-Based Emotion Recognition System
Table 1- 
Existing Real-Time EEG-Based Emotion Recognition System
Jatupaiboon, Pan-ngum and Israsena [15] used EEG signals to classify happy and unhappy emotions (i.e., 2 classes of valence) that were elicited by pictures and classical music in real time. They decomposed EEG signals into 5 frequency bands (i.e., delta, theta, alpha, beta, and gamma) and computed the power spectral density (PSD) of each frequency band at 2 electrode sites (T7 and T8), which led to a total of 10 features. The support vector machine (SVM) was adopted for classification. The offline analyses indicated that the average accuracies of the subject-dependent model and subject-independent model were 75.62 percent and 65.12 percent, respectively. However, the real-time classification accuracy of the two valence classes was not provided.

Liu, Sourina and Minh Khoa [8] proposed a real-time fractal-dimension-based algorithm to identify 2 valence classes and 3 arousal classes. These 6 combinations were mapped into 6 basic emotions: sad, frustrated, fearful, satisfied, pleasant, and happy. The authors used music and sound stimuli from the International Affective Digitised Sounds (IADS) database to elicit the target emotions. They calculated the fractal dimension values at 3 electrode sites (AF3, F4, and FC6) as features. Real-time applications were proposed and implemented in 3D virtual environments (emotional avatar and music therapy).

Viet Hoang, Manh Ngo, Bang Ban and Thang Huynh [16] developed a real-time emotion recognition system to identify 2 valence classes and 2 arousal classes, which resulted in a combination of 4 basic emotions (happy, relaxed, sad, and angry) and the neutral state. The authors adopted the Higuchi fractal dimension algorithm and SVM as a classifier. EEG signals from FC6, AF3, and F4 were the input of the emotion recognition system. The average accuracy for the emotion recognition across all subjects was 70.5 percent (ranging from 66 to 76 percent).

All of these emotion recognition systems presented the standard visual or auditory stimuli to evoke emotions, such as the international affective picture system [18] and the international affective digitised sound system [19]. Standardised emotional stimuli were necessary to elicit target emotions when developing a real-time emotion recognition system. Typically, standardised stimuli are selected based on a large sample of participants’ subjective feelings and emotional experiences and have been proven to better elicit a single and pure target emotion than those stimuli created relying on a smaller sample size in a single experiment. Therefore, the advantage of standardised stimuli is to label EEG signals as the ground truth regardless of a user's subjective feelings after watching the emotional content. In contrast, the majority of research on EEG-based emotion recognition uses supervised learning algorithms (e.g., SVM, Naïve Bayes, K-Nearest Neighbours) to predict emotional states. Researchers collect a user's self-reported scores on the levels of arousal and valance after presenting emotional stimuli. Then, the user's ratings are separated into two (e.g., low and high) or more classes by selecting the threshold of a 9-point self-assessment rating scale [2]. Obviously, this approach does not apply to the real-time recognition of emotion. The introduction of standardised emotional stimuli is the prime condition of success.

The recent trend has been to extend traditional materials (e.g., picture and sound) to combine visual and auditory stimuli, usually film clip and music video [20]. This trend is consistent with real-life emotional experiences that rely on the presence of combined stimuli coming from different modalities. Movies or music videos are more complex and have a more realistic context for involving emotion states compared with static photos and slides [21]. In addition, the dynamic visual-and-auditory-combined stimuli could induce emotions without deception [22] , [23].

Castermans, Duvinage and Riche [17] attempted to detect the valence of emotions elicited by different video excerpts amongst several spectators seated in a realistic movie theatre, on the basis of their EEG signals. A total of 12 PSD features (3 frequency bands × 4 electrode sites) were computed and sent to a Linear Discriminant Analysis (LDA) classifier. Preliminary results obtained with consumer-grade EEG headsets indicated that positive and negative video excerpts might be discriminated in the training data set but not on an independent data set, which indicates an over-fitting problem. In fact, this work was not a truly real-time emotion system because the authors used the participants’ self-assessment to characterise their global emotional states during a movie of several minutes duration. To overcome this limitation, in this paper, we develop a standardised database of movie-induced emotional stimuli with each user's self-assessment of arousal and valance levels as well as their discrete emotional experiences. Based on this database, we propose a real-time emotion recognition system.

In addition to the lack of a standardised database of visual-and-auditory-combined stimuli, another limitation of the previous real-time emotion recognition systems was the lack of discrete emotions built beyond the traditional valence-arousal dimensional model as introduced above. In essence, all of the existing systems recognised different levels of arousal and valence, which are further mapped into discrete values/emotions. However, such mapping of the combination of arousal and valence into corresponding emotions works only when two or more discrete emotions are far from each other in the valence-arousal coordinate space.

Last but not least, real-time emotion recognition takes not only the classification accuracy but also the execution time into consideration, while emotion recognition from offline EEG signals considers only the classification accuracy. As a result, to achieve better real-time classification performance within a time limit, researchers must remove artifacts from EEG signals based on the statistical characteristics of a single trial or a few trials, extract features that are representative and sensitive to the subtle changes of brain activities due to emotional fluctuations, and implement efficient machine learning techniques to recognise emotional states. This requirement might be another reason that only a few existing studies have implemented real-time EEG-based emotion recognition systems.

SECTION 3Construction of a Standardised Database of Chinese Emotional Film Clips
To elicit target emotions, we construct a standardised database of film clips. Nine research assistants (4 males and 5 females) who majored in psychology watched more than one thousand Chinese film excerpts for preliminary screening. They were asked to select film excerpts that lasted 1 to 3 minutes and contained independent and integrated content to elicit a single target emotion, including four negative emotional states (sadness, anger, fear, and disgust), eight positive emotional states (joy, romance, warmth, well-being, love, mutual affection, amusement, and contentment), and a neutral state. Three specialists (2 males and 1 female) in the area of emotion elicitation evaluated each film segment that was selected by the research assistants for its potential to successfully elicit a target emotional state. The film excerpts were decided by a consensus amongst three specialists. Finally, 111 film excerpts were selected and further examined in our experimental session.

A total of 462 Chinese-speaking undergraduate and graduate students (195 males and 267 females) whose average age was 22.2 years (range = 18-30, SD = 2.2) took part in our group film-viewing experiment. At each time, 7-10 participants constituted a group to view and rate film excerpts. They were separated by partitions and asked to use their own computers and headsets to avoid any interference from other group members (e.g., voice, behavioral response). During the experiment, the participants were advised that they felt free to withdraw consent and drop out of the experiment whenever they wanted, and all of their responses were totally anonymous.

To control the presentation of the film excerpts and record the participants’ self-reported ratings, the whole experiment was implemented using the E-prime. A total of 111 film excerpts were assigned into 10 groups. In each group, the number of film clips in the same emotional category did not exceed 2. All of the film clips were viewed by at least 30 participants. The presentation order of film excerpts in each group was random. After viewing each film clip, the participants completed three questionnaires: self-assessment manikin (SAM), the positive and negative affect schedule (PANAS), and differential emotion scale (DES). The SAM was a 9-point (1 = “not at all”, 9 = “extremely”) self-assessment scale for arousal, valence, liking, familiarity, and dominance, which was modified from [24], [25]. The PANAS [26] consisted of two subscales: positive and negative affect. Each subscale contained 10-item words, which assessed the degree of the subject's affect while viewing the film clips based on a 5-point scale (1 = “not at all”, 5 = “extremely”). The DES was on a 9-point scale (1 = “not at all”, 9 = “extremely”) to assess the intensity of each self-reported emotional dimension, which was modified from [23]. It consisted of the following 12 items: joy, amusement, contentment, well-being, love, mutual affection, romance, sadness, warmth, anger, fear, and disgust. The participants were encouraged to answer all of the questions based on their true emotional feelings when watching each film excerpt, instead of their expected feelings or general mood.

We conducted R-type cluster analysis with an agglomeration approach and between-group linkage, to identify the clusters of emotions. The sum of each variable was measured based on the squared euclidean distance. We standardised the variables by computing the Z scores. The cluster analysis found that amusement was separated from the other positive emotions. Contentment and joy belonged to one cluster. Warmth, well-being, romance, mutual affection, and love were all related to tenderness. Each negative emotion was one cluster. Then a total of eight emotional categories were built: three positive emotional categories (joy, amusement, and tenderness), four negative emotional categories (anger, sadness, fear, and disgust), and the neutral state.

We selected film clips for each emotional category based on two objective scores: 1) Hit rate is defined as the proportion of users who rate the emotional target score at least one point higher than the non-target emotion; and 2) Success index represents the sum of two z-scores derived from normalising the hit rate and the target emotional score [23], [27]. For neutral films, the mean ratings on each emotional dimension were less than 3. For the remaining seven emotional categories, there were seven hit rate scores and seven success indices for each film clip. The selected film clip met two criteria at the same time: the hit rate was more than 20 percent, and the mean rating on the target score was at least 1 larger than that on the non-target scores. Eventually, we selected 16 film clips according to their success indices and constructed a standardised database of Chinese emotional film clips, in which all of the subtitles and advertisements were removed. Each emotional category consisted of two film clips (see Table 2 ). Note that two disgusting film clips came from the same film.

TABLE 2 Brief Description of Chinese Emotional Film Clips in Our Standardised Database

All of the selected film clips were expected to elicit high-arousal emotions except for the neutral clips. The standardised database including 30 participants’ self-ratings will be made publicly available.

In the following section, we propose a real-time discrete emotion recognition system that uses these standardised film excerpts to induce emotions.

SECTION 4Real-Time Movie-Induced Discrete Emotion Recognition
We propose a real-time EEG-based emotion recognition framework (see Fig. 1). This framework consists of six modules: (1) emotion elicitation, (2) data acquisition, (3) data preprocessing, (4) feature extraction, (5) emotion learning and pattern classification, and (6) a human machine interface (HMI).


Fig. 1.
The process of real-time emotion recognition system.

Show All

The extracted EEG features in module 4 and the feature-based classification techniques used in module 5 are deliberately selected from the literature. The contribution of our framework is to develop a working system with high performance because such a good model did not exist for discrete emotion recognition using realistic movie clips with high ecological validity. The performance of our recognition system can be regarded as a benchmark, such that more advanced techniques for feature extraction, feature selection, and classification deserve further study in their own right and will be reported in future work. Emotion elicitation is presented in Section 3. Modules 2-6 are presented in the following sections.

4.1 EEG Data Acquisition
In our work, motivated by the success of Emotiv EPOC system (see Fig. 2) [28] in the application of emotion recognition [8], [15], [16], [17], [29], [30], [31], we chose this system to collect EEG signals.


Fig. 2.
Electrode placement for the Emotiv EPOC system.

Show All

Emotiv EPOC is a 14-electrode wireless EEG headset with a 128 Hz sampling rate. We use it to record EEG signals when users watch film clips. It monitors the user's brain activities and enables a real-time display of the Emotiv headset data stream, including the EEG, signal quality, Fast Fourier Transform (FFT), gyro (if applicable), wireless packet acquisition/loss display, marker events, and headset battery level.

Emotiv EPOC provides only some basic EEG data processing functions (e.g., filters and FFT with adjustable sampling window sizes and window functions). To incorporate more powerful signal processing, feature extraction and machine learning algorithms, we build a buffer to store the EEG data stream from the Emotiv EPOC and then load the EEG data section by section in EEGLAB, which is an open source Matlab toolbox for physiological signal processing [32]. Specifically, when a reading command is triggered, all of the samples in the buffer are taken out, and the buffer is cleared. Then, we use a queue to buffer the EEG data from Emotiv to EEGLAB. The queue is refreshed by the current number of samples in the Emotiv's buffer every time that the reading command is triggered.

EEGLAB provides alternative and powerful algorithms to process the EEG data, extract the features, and recognise the emotional states. Moreover, the Matlab environment allows us to further combine Emotiv EPOC with other wearable devices (e.g., eye tracking glasses, smart bracelet) to provide real-time multi-channel physiological feedback from the entire set of user experiences for emotional applications.

4.2 EEG Data Preprocessing
EEG signals in the acquisition process can easily be contaminated with noise signals. We preprocess the data by the removal of artifacts in the process of EEG recording to maintain signal stability and retain the effective data segments. Continuous EEG data are filtered with a band-pass (1-45 Hz) filter to remove linear trends and minimise the introduction of artifacts.

To remove electrooculography (EOG) artifacts, we perform independent component analysis (ICA) to decompose the whole training dataset of the EEG signals into 14 independent components (ICs). The 14 computed ICs are characterised by their topographies and PSDs which are visually inspected and rated by two experts as either an EOG artifact or EEG signal component. The SVM with a RBF kernel function is trained and used for identification of EOG artifacts in the testing dataset of the EEG signals. We implement the CORRMAP in EEGLAB to remove EOG artifacts. After automated classification, the components classified as artifacts are discarded from the subsequent process. The remaining ones, classified as signal components, are back projected to reconstruct artifact-free EEG signals [33], [34].

4.3 Feature Extraction and Normalization
To recognise a discrete emotion in real time, we use the short-time Fourier transform (STFT) with a sliding time-window approach for feature extraction and normalisation based on time-frequency (TF) analysis. TF analysis represents the EEG signals in a two-dimensional spectral domain and provides insights into both the frequency and temporal evolution of the energy- and power-related features that can be associated with brain activities [35], [36]. The TF features of the EEG signals are estimated based on the concept of event-related desynchronisation (ERD) and synchronisation (ERS) [37], [38], [39], [40]. The ERD/ERS theory assumes that the event-related activity represents changes of the power in specific frequency bands. The ERD/ERS phenomena could be considered to be changes in one or more parameters that control oscillations in neuronal networks.

STFT is a linear decomposition of the EEG signals into elementary components. The STFT involves a sliding window of the EEG signal x(m) around a time instant t from an experimental trial and the subsequent calculation of the Fourier transform for each t
X(t,f) =∫∞−∞w(m−t) x(m)e−j2πfmdm(1)
View SourceRight-click on figure for MathML and additional features.where w(m−t)  represents the short-time analysis window, and X(t,ω) represents the power distribution of the signal in the TF plane. In essence, STFT extracts several frames of the signal to be analysed by using a window that moves with time. If the time window is sufficiently narrow, then each frame that is extracted can be viewed as stationary, which enables the Fourier transform to be used. By moving the window along the time axis, the relation between the variance of the frequency and time can be identified.

Fig. 3 shows the schematic representation of the sliding time-window approach for the TF feature extraction. The power spectral features of the EEG signals are computed in a time window wn and in the frequency band fb of interest. The power spectral features that are recorded during the process of watching a film clip Xwnfb  are averaged over both time and frequency, i.e.,
Xwnfb=1Nwn ∑t1Nfb∑fX(t,f)(2)
View SourceRight-click on figure for MathML and additional features.where Nwn and Nfb denote the number of samples in each time window wn and the number of samples in each frequency band fb, respectively [35]. Similarly, the power spectral features recorded during a resting period Xfb  are averaged over the same frequency band and time. Here, a resting period is used to correct the emotional baseline to exclude the confounding effects of other factors.


Fig. 3.
Schematic representation of our sliding time-window approach for time-frequency feature extraction.

Show All

In this work, EEG signals are segmented into a number of time windows with a 50 percent overlap between two successive time windows (see Fig. 4). The power spectral features of the EEG signals on 14 channels are extracted in 5 frequency bands: delta (1-4 Hz), theta (4-8 Hz), alpha (8-12 Hz), beta (13-30 Hz), and gamma (31-45 Hz). In addition to the power spectral features, the difference between the spectral power of all of the 7 symmetrical pairs of electrodes on the right and the left hemispheres in 5 frequency bands is extracted to measure the possible asymmetry in the brain activity due to the emotional stimuli. These 7 pairs of electrodes are: AF3-AF4, F3-F4, F7-F8, FC5-FC6, T7-T8, P7-P8, and O1-O2. As a result, a total number of 105 (70 PSD and 35 ASM) EEG features are used.

Fig. 4. - 
EEG data are loaded every 1 second. Then, a 2-s window with a 50% overlap between two consecutive windows is
 used to segment the filtered and artifact-free EEG data. 105 EEG features are computed in each 2-s window to predict
 emotional states. We make a decision upon the three consecutive 2-s windows according to the majority rule. In other
 words, we make the first decision after EEG data are continuously imported for 4 seconds. After that, emotional states
 are predicted every 1 second.
Fig. 4.
EEG data are loaded every 1 second. Then, a 2-s window with a 50% overlap between two consecutive windows is used to segment the filtered and artifact-free EEG data. 105 EEG features are computed in each 2-s window to predict emotional states. We make a decision upon the three consecutive 2-s windows according to the majority rule. In other words, we make the first decision after EEG data are continuously imported for 4 seconds. After that, emotional states are predicted every 1 second.

Show All

To reduce the individual variability and the lasting effects of the previous elicited emotions on the current target emotion, in our experimental protocol, each trial begins with a 60-s Go/NoGo task such that the participant is kept in a neural state (see Section 5.2 for more details). Furthermore, after feature extraction, we normalise the segmented data when watching each film clip with a corresponding resting period with eyes open. Then, we subtract the baseline power of each resting period from all of the corresponding segments of the extracted features, in such a way that the change in the power is based only on the current film clip and is not affected by the previous clips [41].

4.4 Emotion Classification with Feature Selection
105 EEG features are extracted from EEG signals on 14 electrodes. We observe that not all of these features are necessary to be used for emotion classification; that is, some features may be irrelevant to emotion classification, and some features may be highly correlated and thus redundant in emotion classification. In our work, we apply a classifier that performs feature selection; that is, a classifier that intelligently uses a subset of all 105 EEG features.

We choose Linear Discriminant Analysis (LDA) [42] for feature selection, which projects high-dimensional data with labels into a low-dimensional space with good class-separability by maximizing Fisher separation criterion. Feature selection is achieved by finding a subset of features that have the largest Fisher separation value. However, when the number of selected features is still larger than the number of observations, the within-class covariance matrix of the features becomes singular. To solve this problem, we apply the Sparse LDA (SLDA) [43], which overcomes this limitation by performing LDA with a sparseness criterion imposed such that classification and feature selection are performed simultaneously. Accordingly, the selected features enter as input features to a Support Vector Machine (SVM) for classification. In our implementation using Matlab, we apply the SpaSM toolbox for SLDA and LIBSVM toolbox for SVM. We optimise the parameter STOP in SpaSM toolbox by searching the range [-105, -1] with a step size 15. In LIBSVM toolbox, we choose the RBF kernel function and optimise the cost parameter c (in the range [-8, 8]) and the gamma parameter g (in the range [-8, 8]) using SVMcgForClass.

We use SVM to classify eight discrete emotions; that is, three positive emotions (joy, amusement and tenderness), four negative emotions (anger, sadness, fear and disgust) and neutrality. Note that the original SVM only solves a two-class problem. To achieve a multiclass classification, LIBSVM constructs k(k−1)/2 classifiers with each classifier trained for two classes, where k is the number of classes [44]. In addition to directly apply eight-class classification in LIBSVM, we further improve the classification accuracy by incorporating a priori knowledge: in eight emotions, three positive emotions including joy, amusement and tenderness are similar and four negative emotions including anger, sadness, fear and disgust are similar. Based on this priori knowledge, we propose a three-level classification scheme. At the first level, neural and non-neural emotions are classified. At the second level, non-neural emotions are classified into positive and negative emotions. At the third level, positive emotions are classified into joy, amusement and tenderness, and negative emotions are classified into anger, sadness, fear and disgust. The experimental results summarised in Section 5 demonstrate that our three-level classification scheme significantly improves the accuracy of the default eight-class classification in LIBSVM.

4.5 Human Machine Interface and Prototype System
We develop a prototype system to implement the framework illustrated in Fig. 1 . The human machine interface of this proposed system provides five pieces of information and is illustrated in Fig. 5. First, a film clip that is selected to elicit a user's target emotion is presented in the middle of the HMI. When a user wearing the Emotiv EPOC headset watches this film clip (upper right), this suite continuously records his/her EEG signals (lower right). This system recognises not only the arousal and valence levels (lower left) but also discrete emotional states in terms of 3 positive emotional categories, 4 negative emotional categories, and neutrality (upper left). The number of colour bars reflects the classification posterior probability or the degree of confidence when classifying an emotional state into a specific class.


Fig. 5.
The human machine interface of the proposed real-time movie-induced emotion recognition system from EEG signals. In this screenshot, our system successfully recognises joy (target emotion) from amusement and tenderness (both are high-arousal and positive emotions).

Show All

In the screenshot shown in Fig. 5, a film clip aims to elicit a positive (i.e., more yellow bars in the right side) and high-arousal (i.e., the number of red bars is more than half) emotion, namely, joy. The proposed system successfully recognises joy from amusement and tenderness (i.e., the number of green bars is more than the number of orange and blue bars), which are two similar emotions in the valence-arousal coordinate space. Due to their rich content and dynamic characteristics, real-world emotions might not be singular and unique but instead could be a mix of components. In this case, our system can predict a distribution of different emotions depending on the degree of physiological activation of each emotion instead of classifying into a single emotion.

In addition, after watching a film clip, the user can view the summary of his/her personal emotional experiences. As illustrated in Fig. 6, the HMI can present the levels of arousal and valence that change over time (upper panel). The redder area indicates a higher level of mental vigilance of the target emotion that an individual feels. Meanwhile, the HMI can also present the portion of each discrete emotion within its valence category, which changes over time (lower panel). The larger portion indicates that the corresponding emotion is elicited more than the other emotions that are similar in the valence-arousal coordinate space. The deeper the colour is, the higher the intensity of physiological activation that an individual feels. By viewing this summary, the user can realise when and where a specific emotion (e.g., joy) is evoked and how long it lasts.

Fig. 6. - 
The proposed system provides real-time recognition of discrete emotions and the levels of arousal and valence.
Fig. 6.
The proposed system provides real-time recognition of discrete emotions and the levels of arousal and valence.

Show All

SECTION 5Experiment
5.1 Participants
30 healthy, male, right-handed undergraduate or graduate students (age range: 19-26, mean = 23, SD = 1.73 years) without neurological illness or psychiatric disorder took part in the experiment. All of the participants were required to not take any tobacco or caffeine for 24 hours before the experiment.

5.2 Experimental Protocol and Procedure
All of the film clips were adjusted to the same resolution (720 × 576). They were presented randomly on a 15-inch LCD screen. The film clips that targeted the same valence were presented no more than twice consecutively [23]. Two speakers were used, and the volume was adjusted to an appropriate level to keep the participants comfortable. The participants were seated in a soundproofed room and were required to turn off all wireless and Bluetooth devices to isolate possible interferences. Their eyes were approximately 0.6 meter from the screen centre.

Each trial began with a 60-s Go/NoGo task to keep all of the participants in a neutral state. In general, Go/NoGo testing refers to a pass/fail principle that uses two boundary conditions. The test is passed only when the Go condition is met and also the NoGo condition fails. The test was used to “distract” the participants from the influence of the last film clip. Then, the participants were instructed to rest with their eyes open for 40-s and their eyes closed for another 40-s. After two resting baselines, the participants watched a film clip and then completed two questionnaires: SAM and DES, as introduced in Section 3. All of the participants were instructed to rate all of the scales according to their real feelings while watching the film clips instead of meeting the expected standards or their daily mood.

In the training session, we played 8 film clips to elicit 7 discrete emotions and one neutral state. The play order was randomised: the film clips with the same valence or those that targeted the same emotional category were not presented consecutively more than twice. After each trial of the training session, the EEG signals were analysed immediately, to train the system for each discrete emotion. The training session was followed by a 5-min break. Afterward, the testing session had another 8 film clips played for real-time recognition of emotion. The play order was randomised following the same rules.

Before recording the EEG, we put EMOTIV on the participant's head for a while to prevent undesired emotions that can arise from unfamiliar or uncomfortable feelings. Then, we described the process of recording and advised the participant to stay as still as possible to prevent artifacts from movements of the body. The 14-channel wireless EMOTIV EPOC consists of AF3, AF4, F3, F4, F7, F8, FC5, FC6, P7, P8, T7, T8, O1, and O2.

We collected the participants’ true feelings and responses after watching each film clip. Because our system used the standardised database of movie-induced emotions as the ground truth to label the EEG signals, we compared every participant's self-assessments with the standardised database and ignored the cases where he or she did not rate the target emotion at least one point higher than the other non-target emotions (i.e., inconsistent with the target emotions [8]).

5.3 Experimental Results
We used the standardised database of Chinese film clips to elicit seven high-arousal and valenced discrete emotions and neutrality (Sections 5.3.1, 5.3.2 , and 5.3.3). Furthermore, our system applied the three-level classification scheme proposed in Section 4.4 to recognise eight discrete emotions at the same time, which outperformed the default eight-class classification in LIBSVM ( Section 5.3.4).

5.3.1 Assessment of Valence and Arousal
The participants’ assessments for valence and arousal were consistent with our expectations (see Fig. 7). Self-reported scores on the valence dimension were the highest for positive emotions (M = 6.12, SD = 1.57), which were higher than the scores for neutrality (M = 3.03, SD = 1.86) and negative emotions (M = 1.66, SD = 0.79). The higher score on the valence dimension is, the more positive the characteristics of an emotion. On the other hand, both positive (M = 5.58, SD = 1.66) and negative (M = 6.26, SD = 1.62) film clips successfully elicited higher arousal states than neutrality (M = 2.74, SD = 1.61).


Fig. 7.
Self-reported valence and arousal of positive, negative, and neutral emotions. Error bars indciate ±1 standard error.

Show All

5.3.2 Assessment of Positive Emotions
The participants’ self-reported scores on three positive emotions: joy, amusement, and tenderness, were shown in Fig. 8. On average, their ratings on differential emotion scales met the criterion of the standardised database of emotional film clips. After watching a joyful film clip, the participants rated the joy (M = 6.93, SD = 1.46) at least one point higher than amusement (M = 5.31, SD = 2.36) and tenderness (M = 5.72, SD = 1.41). Similarly, an amusing film clip elicited higher subjective feelings of amusement (M = 7.76, SD = 1.27) than joy (M = 6.19, SD = 1.86) and tenderness (M = 2.34, SD = 1.82). Tenderness was successfully elicited with a higher score on the target dimension (M = 7.03, SD = 1.76) compared to joy (M = 4.78, SD = 1.95) and amusement (M = 1.59, SD = 1.15).

Fig. 8. - 
Self-reported scores on positive emotions (joy, amusement, and tenderness) elicited by three positive film clips
 (joyful, amusing, and tender). Error bars indicate ±1 standard error.
Fig. 8.
Self-reported scores on positive emotions (joy, amusement, and tenderness) elicited by three positive film clips (joyful, amusing, and tender). Error bars indicate ±1 standard error.

Show All

The participants’ self-reported scores on the arousal and valence dimensions of three positive emotions were similar. Individual subjective ratings on the arousal: joy (M = 5.83, SD = 1.61), amusement (M = 6, SD = 1.69), and tenderness (M = 5.38, SD = 1.59). On the other hand, the participants rated the valence: joy (M = 6.52, SD = 1.24), amusement (M = 6.76, SD = 1.55), and tenderness (M = 5.72, SD = 1.69). The difference in the self-reported scores on the arousal and valence between three positive emotions was less than 1.

5.3.3 Assessment of Negative Emotions
The participants’ self-reported scores on four negative emotions: anger, disgust, fear, and sadness were illustrated in Fig. 9. Their averaged ratings on the target emotion scale were at least one point greater than the other negative emotion scales. Specifically, the participants rated the anger (M = 6.55, SD = 1.64) higher than disgust (M = 5.15, SD = 2.15), fear (M = 2, SD = 1.49), and sadness (M = 3.69, SD = 2.48) after watching an angry film clip. A disgusting film clip elicited higher subjective feelings of disgust (M = 5.96, SD = 2.1) than anger (M = 3.93, SD = 2.05), fear (M = 2.28, SD = 1.69), and sadness (M = 3.41, SD = 2.01). Fear was successfully elicited with a higher score on the target dimension (M = 6.19, SD = 1.64) compared to anger (M = 2.34, SD = 1.88), disgust (M = 4.1, SD = 2.38), and sadness (M = 2, SD = 1.6). The selected sad film clip acquired an optimal result: self-reported scores on the sadness dimension (M = 6.28, SD = 2.19) were evidently greater than those on the anger (M = 1.31, SD = 0.81), disgust (M = 1.66, SD = 1.54), and fear dimension (M = 1.31, SD = 0.71).

Fig. 9. - 
Self-reported scores on negative emotions (anger, disgust, fear, and sadness) elicited by four negative film clips
 (angry, disgusting, fearful, and sad). Error bars indicate ±1 standard error.
Fig. 9.
Self-reported scores on negative emotions (anger, disgust, fear, and sadness) elicited by four negative film clips (angry, disgusting, fearful, and sad). Error bars indicate ±1 standard error.

Show All

The participants’ subjective ratings on the arousal between four negative emotions were close to each other: anger (M = 6.38, SD = 1.72), fear (M = 6.69, SD = 1.44), and sadness (M = 6.41, SD = 1.62) except disgust (M = 5.55, SD = 1.53). On the other hand, the participants rated the valence: anger (M = 1.55, SD = 0.74), disgust (M = 1.55, SD = 0.74), fear (M = 1.66, SD = 0.77), and sadness (M = 1.86, SD = 0.92). The difference in the self-reported scores on the valence between four negative emotions was less than 0.5.

5.3.4 Recognition of Discrete Emotions
In Section 4.4, we proposed a three-level classification scheme based on prior knowledge to differentiate eight discrete emotions at the same time. At the first level, we separated high-arousal and valenced emotions from neutrality with an average accuracy of 92.26 percent (SD=0.17%) (see Table 3). At the second level, high-arousal and valenced emotions were further classified into positive or negative categories with an accuracy of 86.63 percent (SD=0.27%). The decrement in the classification accuracy could be attributed to a lower explanatory ability of the valence dimension compared to the valence-arousal coordinate space. From a traditional valence-arousal perspective, the differences between high-arousal and valenced emotions and neutrality exist in both valence and arousal spaces, while discrimination between positive and negative emotions relies on valenced characteristics exclusively.

TABLE 3 Comparison of Classification Accuracy Between Default Eight-Class Classification in LIBSVM and Three-Level Classification Scheme with Prior Knowledge (Average and Standard Deviation Across Participants)
Table 3- 
Comparison of Classification Accuracy Between Default Eight-Class Classification in LIBSVM and Three-Level
 Classification Scheme with Prior Knowledge (Average and Standard Deviation Across Participants)
At the third level, positive emotions were classified into joy, amusement and tenderness with an accuracy of 86.43 percent (SD=1.03%) (33.33 percent accuracy at random). Negative emotions were classified into anger, sadness, fear and disgust with an accuracy of 65.09 percent ( SD=0.66%) (25 percent accuracy at random). Applying this three-level classification scheme based on prior knowledge, our proposed system recognised eight discrete emotions at the same time with an overall accuracy of 60.55 percent, which was computed by 0.9226×0.8663×(0.8643+0.6509)/2. In contrast, a baseline approach was to directly use the default eight-class classification in LIBSVM, which constructed 28 two-class classifiers (computed by 8×(8−1)/2). This eight-class classification resulted in an average accuracy of 32.31 percent (SD=0.46%) (12.5 percent accuracy at random). Obviously, our proposed three-level classification scheme with prior knowledge outperformed an eight-class classification to differentiate eight discrete emotions at the same time.

To validate the integrity of the trained classification models, we repeat the classification task but with the shuffled emotion labels (i.e., randomly assigned labels) of testing dataset. In this scenario, our system achieved an overall accuracy of 47.67 percent in recognising high-arousal and valenced emotions from neutrality and 47.35 percent in recognising positive from negative emotions. Moreover, our system classified three positive emotions (joy, amusement, tenderness) with an average of 31.39 percent accuracy and four negative emotions (anger, disgust, fear, sadness) with an average of 20.77 percent accuracy. These accuracy ratios were much lower than those with unshuffled emotion labels, demonstrating that there was no systematic bias in the classification procedure.

In addition, our system can be further used to recognise discrete emotions in a long-duration trial. Fig. 10 and Supplemental Material, available online, present the assessment of system performance as a function of time during movie watching. As illustrated in Fig. 10, our system predicted continuous classification accuracies of valence and arousal states and continuous recognition of four negative emotions similar in the valence-arousal coordinate space when participants watched a sad film clip. The highest proportion (average = 77.78 percent, range = 60-93.33 percent) of negative emotions was sadness and this proportion of sadness reached the maximum at the time from 63 to 65 seconds (film content: the father found this son was died).


Fig. 10.
Continuous recognition of valence and arousal states (upper panel) and four negative emotions similar in the valence-arousal coordinate space elicited by a sad film clip (lower panel). More results are presented in Supplemental Material, available online.

Show All

5.3.5 SLDA-Based Selected Features
We found that frontal alpha asymmetry (FAA) features played an important role to recognise different emotional states. As shown in Table 4, F3-F4 distinguished between positive and negative emotions, while AF3-AF4 differentiated a positive emotion from similar emotions in both valence and arousal dimensions. Human brain reflect the affective and motivational processing of emotion states asymmetrically [45]. FAA refers to the patterns of asymmetrical anterior EEG activity in the alpha band, which was proved to be a typical indicator of asymmetric brain activity in the frontal cortex [46].

TABLE 4 SLDA-Based Top 5 PSD and Asymmetry Features

Past research have indicated that positive emotions correlates with approach motivation and negative emotions always correlates with withdrawal motivation [47]. The valence model of FAA states that greater left hemisphere activity (lower alpha power) is associated with positive emotion, whereas greater right hemisphere activity is associated with negative emotion [48]. Both trait positive (or negative) emotions and state positive (or negative) emotions are associated with greater left than right frontal cortical activity, which are manifested in greater right than left power values of alpha band [49]. However, anger is a typical negatively valenced emotion which evokes behavioral tendencies of approach [50], [51], [52]. The fact that higher scores on a trait anger scale associated with higher resting levels of left frontal activity and lower right frontal activity has been reported by several studies [53], [54]. Similar results are also found in the state-induced anger [47], [55].

We also found that delta and beta frequency bands in the right frontal and parietal lobe, theta frequency band in the temporal and parietal lobe, and gamma frequency band in the temporal lobe were sensitive to recognise different emotional states. Previous studies observed a positive correlation of valence with beta and gamma bands, emanating from anterior temporal cerebral sources [56]. A highly significant increase of temporal gamma power was found in [2], which might result from the impact of muscle activity. Theta frequency band in the frontal middle line [57], [58] and parietotemporal region [59] was another source of dominant features to recognise positive and negative emotions.

SECTION 6Discussion
In this experiment, our system achieved an average accuracy of 92.26 percent in recognising high-arousal and valenced emotions from neutrality and 86.63 percent in recognising two valence classes. Due to having a limited number of existing real-time emotion recognition systems, we did not find any result in the literature that was comparable to our findings. Viet Hoang, Manh Ngo, Bang Ban and Thang Huynh [16] developed a real-time emotion recognition system to identify 2 valence classes and 2 arousal classes, which resulted in 4 basic emotions (happy, relaxed, sad, and angry) and the neutral state. The average accuracy in recognising each emotion was 70.5 percent across all subjects. However, the existing work used standardised pictures or sounds to elicit the target emotions. Predictions of emotional states were made upon a number of independent pictures or sounds. In contrast, our system used standardised film clips to elicit target emotions. The EEG signals recorded from a film clip were segmented into a number of consecutive windows, and predictions were made on each segment. Moreover, the definition of the classification accuracy was different. The existing work (e.g., [16]) computed the proportion of correct classification for each emotion and the average accuracy over 5 emotional states. In contrast, we defined the accuracy as the number of correct predictions from all of the predictions made.

Interestingly, the average accuracy for recognising high-arousal and valenced emotions from neutrality (i.e., 92.26 percent) was higher than that of classifying two valence classes (i.e., 86.63 percent). One possible reason was that the trained classifier captured more information to enlarge its ability to perform pattern recognition in the valence-arousal coordinate space than the model that had a single valence dimension. Compared to the neutral state (i.e., low-arousal and non-valenced), a discrete emotion (i.e., high-arousal and valenced) in this work was different in both the arousal and valence dimensions. As a result, it was reasonable for our system to achieve a higher classification accuracy to recognise a discrete emotion from neutrality. Jatupaiboon, Pan-ngum and Israsena [15] used real-time EEG signals to classify happy and unhappy emotions (i.e., 2 classes of valence) that were elicited by pictures and classical music. The average accuracy of the subject-dependent model was 75.62 percent, which was lower than the performance of our system in recognising two valence classes.

In addition, our system successfully classified three positive emotions and four negative emotions with an overall accuracy of 86.43 and 65.09 percent, respectively. To the best of our knowledge, this work was the first attempt to classify similar discrete emotions that were close in the valence-arousal coordinate space. For example, in our experiment, the participants’ subjective ratings on arousal were 5.83 for joy and 6.0 for amusement. Their self-reported scores on valence were 6.52 for joy and 6.76 for amusement. From the dimensional perspective to establish the emotional model, it was almost impossible to classify two or more similar discrete emotions that are close in the valence-arousal coordinate space. Even if we set up a cut-off point to divide joy and amusement into two classes, the recognition ability of the dimensional model would decline dramatically [12]. Similar emotions (such as joy and amusement in the valence-arousal dimension) could have their own unique representation and induce completely different behavioral responses [13]. In this case, our system demonstrated an advantage over the existing state-of-the-art real-time emotion recognition systems from EEG signals in terms of accuracy and the ability to recognise similar emotional states in both valence and arousal dimensions.

SECTION 7Conclusion
In this paper, we established a standardised database of 16 Chinese film clips, and proposed a real-time EEG-based emotion recognition system for identifying an individual's emotional states through the analysis of brain waves. The system consisted of six modules: emotion elicitation, EEG data acquisition, data preprocessing, feature extraction, emotion classification, and human machine interface. We conducted an experiment to validate the efficiency and effectiveness of the system. Thirty participants took part in this experiment and watched a few emotional film clips that targeted seven basic discrete emotions (joy, amusement, tenderness, anger, disgust, fear, sadness) and neutrality. Our system achieved an average accuracy of 92.26 percent in recognising high-arousal and valenced emotions from neutrality and 86.63 percent in recognising two valence classes. In addition, the system successfully classified three positive emotions (joy, amusement, tenderness) and four negative emotions (anger, disgust, fear, sadness) with an overall accuracy of 86.43 and 65.09 percent, respectively. These results demonstrated an advantage over the existing state-of-the-art real-time emotion recognition systems from EEG signals in terms of the accuracy and the ability to recognise several similar discrete emotions that are close in the valence-arousal coordinate space.