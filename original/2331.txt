We consider the performance of the bootstrap in high-dimensions for the setting of linear
regression, where p < n but p/n is not close to zero. We consider ordinary least-squares as
well as robust regression methods and adopt a minimalist performance requirement: can
the bootstrap give us good confidence intervals for a single coordinate of β (where β is the
true regression vector)?
We show through a mix of numerical and theoretical work that the bootstrap is fraught
with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of
power) as the ratio p/n grows. We also show that the jackknife resampling technique for
estimating the variance of βˆ severely overestimates the variance in high dimensions.
We contribute alternative procedures based on our theoretical results that result in
dimensionality adaptive and robust bootstrap methods.
Keywords: Bootstrap, high-dimensional inference, random matrices, resampling
1. Introduction
The bootstrap (Efron, 1979) is a ubiquitous tool in applied statistics, allowing for inference when very little is known about the properties of the data-generating distribution.
The bootstrap is a powerful tool in applied settings because it does not make the strong
assumptions common to classical statistical theory regarding this data-generating distribution. Instead, the bootstrap resamples the observed data to create an estimate, Fˆ, of
the unknown data-generating distribution, F. The distribution Fˆ then forms the basis of
further inference.
Since its introduction, a large amount of research has explored the theoretical properties
of the bootstrap, improvements for estimating F under different scenarios, and how to most
effectively estimate different quantities from Fˆ (see the pioneering Bickel and Freedman,
1981 for instance and many many more references in the book-length review of Davison and
Hinkley, 1997, as well as van der Vaart, 1998 for a short summary of the modern point of
view on these questions). Other resampling techniques exist of course, such as subsampling,
m-out-of-n bootstrap, and jackknifing, all of which have been studied and much discussed
(see Efron, 1982; Hall, 1992; Politis et al., 1999; Bickel et al., 1997; and Efron and Tibshirani,
1993 for a practical introduction).
An important limitation for the bootstrap is the quality of Fˆ. The standard bootstrap
estimate of F based on the empirical distribution of the data may be a poor estimate when
the data has a non-trivial dependency structure, when the quantity being estimated, such
as a quantile, is sensitive to the discreteness of Fˆ, or when the functionals of interest are
not smooth (see e.g., Bickel and Freedman, 1981 for a classic reference, as well as Beran
and Srivastava, 1985 or Eaton and Tyler, 1991 in the context of multivariate statistics).
An area that has received less attention is the performance of the bootstrap in high
dimensions and this is the focus of our work. In particular, we consider the setting of
standard linear models where data yi are drawn from the linear model
∀i, yi = β
0Xi + i
, 1 ≤ i ≤ n , where Xi ∈ R
p
.
We are interested in the bootstrap or resampling properties of the estimator defined as
βbρ = argminb∈Rp
Xn
i=1
ρ(yi − X0
i
b) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The first is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs (yi
, Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y
∗
i = βb0
ρXi + ˆ
∗
i
, where ˆ
∗
i
is drawn from
the empirical distribution of the estimated residuals, ei
. We also consider the jackknife,
a resampling method focused specifically on estimating the variance of functionals of βbρ.
These three methods are extremely flexible for linear models regardless of the method of
fitting β or the error distribution of the i
.
The high dimensional setting: p/n → κ ∈ (0, 1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the
number of observations, n, formalized mathematically by assuming that p/n → κ ∈ (0, 1).
Several reasons motivate our theoretical study in this regime. The asymptotic behavior of
the estimate βbρ is known to depend heavily on whether one makes the classical theoretical
assumption that p/n → 0 or instead assumes p/n → κ ∈ (0, 1) (see Section 1.2 and AppendixA and references therein). But from the standpoint of practical usage on moderatesized data sets (i.e., n and p both moderately sized with p < n), it is not always obvious
which assumption is justified. Working in the high-dimensional regime of p/n → κ ∈ (0, 1)
captures better the complexity encountered even in reasonably low-dimensional practice
than using the classical assumption p/n → 0. In fact, asymptotic predictions based on the
2
Can We Trust the Bootstrap in High-dimensions?
high-dimensional assumption can work surprisingly well in very low-dimension (see Johnstone, 2001). Furthermore, in these high-dimensional settings, where much is still unknown
theoretically, the bootstrap is a natural and compelling alternative to asymptotic analysis.
Another motivation for our investigation is that of very large scale applications (Chapelle
et al., 2014; Criteo, 2017; Langford et al., 2007), where one might resort to subsampling
methods or recent variants like the bag-of-little-bootstraps (Kleiner et al., 2014) for uncertainty assessment. Subsampling is also very commonly used in this setting for simple
computational speed-up. In such cases, even if one had started with a data set where
p  n, after subsampling one often ends up with p comparable to n on the subsamples
where bootstrap-like computations are performed. It is therefore important to know if the
bootstrap and other resampling plans perform well when p is comparable to n.
Defining success: accurate inference on β1 The common theoretical definition of whether
the bootstrap “works” is that the bootstrap distribution of the entire bootstrap estimate
βb∗
ρ
converges conditionally almost surely to the sampling distribution of the estimator βbρ
(see e.g., van der Vaart, 1998). The work of Bickel and Freedman (1983) on the residual
bootstrap for least squares regression, which we discuss in the background section 1.2, shows
that this theoretical requirement is not fulfilled even for the simple problem of least squares
regression.
In this paper, we choose to focus only on accurate inference for the projection of our
parameter on a pre-specified direction υ. More specifically, we concentrate only on whether
the bootstrap gives accurate confidence intervals for υ
0β. We think that this is the absolute
minimal requirement we can ask of a bootstrap inferential method, as well as one that is
meaningful from the standpoint of applied statistics. This is of course a much less stringent
requirement than performing well on complicated functionals of the whole parameter vector,
which is the implicit demand of standard definitions of bootstrap success. For this reason,
we focus throughout the exposition on inference for β1 (the first element of β) as an example
of a pre-defined direction of interest (where β1 corresponds to choosing υ = e1, the first
canonical basis vector).
We note that considering the asymptotic behavior of υ
0β as p/n → κ ∈ (0, 1) implies that
υ = υ(p) changes with p. By “pre-defined” we will mean simply a deterministic sequence
of directions υ(p). We will continue to suppress the dependence on p in writing υ in what
follows for the sake of clarity.
1.1 Organization and Main Results of the Paper
In Section 2 we demonstrate that in high dimensions residual-bootstrap resampling results
in extremely poor inference on the coordinates of βρ with error rates much higher than the
reported Type I error. We show that the error in inference based on residual bootstrap
resampling is due to the fact that the distribution of the residuals ei are a poor estimate
of the distribution of i
; we further illustrate that common methods of standardizing the ei
do not solve the problem for general ρ. We propose two new dimension-adaptive methods
of residual resampling that appear promising for use in bootstrapping linear models. We
also provide some theoretical results for the behavior of this method as p/n → 1.
In Section 3 we examine pairs-bootstrap resampling and show that confidence intervals
based on bootstrapping the pairs also perform very poorly. Unlike in the residual-bootstrap
3
El Karoui and Purdom
case discussed in Section 2, the confidence intervals obtained from the pairs-bootstrap are
instead conservative to the point of being non-informative. This results in a dramatic loss of
power. We prove in the case of L2 loss, i.e., ρ(x) = x
2
, that the variance of the bootstrapped
v
0βb∗
is greater than that of v
0βb, leading to the overly conservative performance we see in
simulations. We demonstrate that a different resampling scheme we propose can provide
accurate confidence intervals in moderately high dimensions.
In Section 4, we discuss another resampling scheme, the jackknife. We focus on the
jackknife estimate of variance and show that it has similarly poor behavior in high dimensions. In the case of L2 loss with Gaussian design matrices, we further prove that the
jackknife estimator over estimates the variance of our estimator by a factor of 1/(1 − p/n);
we also provide corrections for other losses that improve the jackknife estimate of variance
in moderately high dimensions.
We rely on simulation results to demonstrate the practical impact of the failure of the
bootstrap. The settings for our simulations and corresponding theoretical analyses are idealized, without many of the common settings of heteroskedasticity, dependency, outliers and
so forth that are known to be a problem for bootstrapping. This is intentional, since even
these idealized settings are sufficient to demonstrate that the standard bootstrap methods
have poor performance. For brevity, we give only brief descriptions of the simulations in
what follows; detailed descriptions can be found in AppendixD.1.
Similarly, we focus on the basic implementations of the bootstrap for linear models.
While there are many proposed alternatives (often for specific loss functions or types of
data), the standard methods we study are most commonly used and recommended in practice. Furthermore, to our knowledge none of the alternative bootstrap methods we have
seen specifically address the underlying theoretical problems that appear in high dimensions
without making low-dimensional assumptions about either the design matrix or the sparsity
of β, and therefore are likely to suffer from the same fate as standard methods. We note that
in truly large scale applications, sparsity assumptions are not always made by practitioners
(Chapelle et al., 2014; Langford et al., 2007; Criteo, 2017) and it is hence natural to study
the performance of estimators outside of sparse settings. We have also experimented with
more complicated ways to build confidence intervals (e.g., bias correction methods), but
have found their performance to be erratic in high-dimension and offer no improvement.
We first give some background regarding the bootstrap and estimation of linear models
in high dimensions before presenting our new results.
1.2 Background: Inference Using the Bootstrap
We consider the setting yi = β
0Xi + i
, where E(i) = 0 and var (i) = σ
2

. The vector β is
estimated as minimizing the average loss,
βbρ = argminb∈Rp
Xn
i=1
ρ(yi − X0
i
b), (1)
where ρ defines the loss function for a single observation. The function ρ is assumed to
be convex in all the paper. Common choices are ρ(x) = x
2
, i.e., least-squares, ρ(x) = |x|,
which defines L1 regression, or Huberk loss where ρ(x) = (x
2/2)1|x|<k + (k|x| − k
2/2)1|x|≥k
.
4
Can We Trust the Bootstrap in High-dimensions?
Bootstrap methods are used in order to estimate the distribution of the estimate βbρ
under the true data-generating distribution, F. The bootstrap estimates this distribution
with the distribution obtained when the data is drawn from an estimate Fˆ of F. Following
standard convention, we designate this bootstrapped estimator βb∗
ρ
to note that this is an
estimate of β using loss function ρ when the data-generating distribution is known to be
exactly equal to Fˆ. Since Fˆ is completely specified, we can in principle exactly calculate
the distribution of βb∗
ρ and use it as an approximation of the distribution of βbρ under F. In
practice, we simulate B independent draws of size n from the distribution Fˆ and perform
inference based on the empirical distribution of βb∗b
ρ
, b = 1, . . . , B.
In bootstrap inference for the linear model, there are two common methods for resampling, which results in different estimates Fˆ. In the first method, called the residual
bootstrap, Fˆ is an estimate of the conditional distribution of yi given β and Xi
. In this
case, the corresponding resampling method consists of resampling 
∗
i
from an estimate of
the distribution of  and forming data y
∗
i = X0
iβbρ + 
∗
i
, from which βb∗
ρ
is computed. This
method of bootstrapping assumes that the linear model is correct for the mean of y (i.e.,
that E (yi) = X0
iβ); it also assumes fixed Xi design vectors because the sampling is conditional on the Xi
. In the second method, called pairs bootstrap, Fˆ is an estimate of the
joint distribution of the vector (yi
, Xi) ∈ Rp+1 given by the empirical joint distribution
of {(yi
, Xi)}
n
i=1; the corresponding resampling method resamples the pairs (yi
, Xi). This
method makes no assumption about the mean structure of y and, by resampling the Xi
, also
does not condition on the values of Xi
. For this reason, pairs resampling is often considered
to be more generally applicable than residuals resampling (see e.g., Davison and Hinkley,
1997).
1.3 Background: High-dimensional Inference of Linear Models
Recent research shows that βbρ has very different asymptotic properties when p/n has a limit
κ that is bounded away from zero than it does in the classical setting where p/n → 0 (see
e.g., Huber, 1973; Huber and Ronchetti, 2009; Portnoy, 1984, 1985, 1986, 1987; Mammen,
1989 for κ = 0; El Karoui et al., 2013 for κ ∈ (0, 1)). A simple example is that the vector βbρ
is no longer consistent in Euclidean norm when κ > 0. We should be clear, however, that
projections on fixed non-random directions such as we consider, i.e., υ
0βbρ, are √
n consistent
for υ
0β, even when κ > 0. In particular, the coordinates of βbρ are √
n−consistent for the
coordinates of β (El Karoui et al., 2013, Lemma 1). Hence, in practice the estimator βbρ is
still a reasonable quantity to consider.
Bootstrap in high-dimensional linear models Very interesting work exists already in the
literature about bootstrapping regression estimators when p is allowed to grow with n
(Shorack, 1982; Wu, 1986; Mammen, 1989, 1992, 1993; Parzen et al., 1994; Koenker, 2005,
Section 3.9). With a few exceptions, this work has been in the classical, low-dimensional
setting where either p is held fixed or p grows slowly relative to n (i.e., κ = 0 in our
notation). For instance, in Mammen (1993), it is shown that under mild technical conditions
and assuming that p
1+δ/n → 0, δ > 0, the pairs bootstrap distribution of linear contrasts
v
0
(βb∗
ρ−βbρ) is in fact very close to the sampling distribution of v
0
(βbρ−β) with high-probability,
when using least-squares. Other results, such as Shorack (1982) and Mammen (1989),
5
El Karoui and Purdom
also allow for increasing dimensions, for example in the case of linear contrasts in robust
regression, by making assumptions on the diagonal entries of the hat matrix. In our context,
these assumptions would be satisfied only if p/n → 0. Hence those interesting results do
not apply to the present study. We also note that Hall (1992, p. 167) contains cautionary
notes about using the bootstrap in high-dimension.
While there has not been much theoretical work on the bootstrap in the setting where
p/n → κ ∈ (0, 1), one early work of Bickel and Freedman (1983) considered bootstrapping scaled residuals for least-squares regression when κ > 0. They show that when
p/n → κ ∈ (0, 1), there exists a data-dependent direction c, such that c
0βb∗
LS does not
have the correct asymptotic distribution (Bickel and Freedman, 1983, Theorem 3.1, p.39),
i.e., its distribution is not conditionally in probability close to the sampling distribution of
c
0βb
LS. Furthermore, they show that when the errors in the model are Gaussian, under the
assumption that the diagonal entries of the hat matrix are not all close to a constant, the
empirical distribution of the residuals is a scaled-mixture of Gaussian, which is not close to
the original error distribution.
As we previously explained, in this work we instead only consider inference for predefined
contrasts υ
0β. The important and interesting problems pointed out in Bickel and Freedman
(1983) disappear if we focus on fixed, non-data-dependent projection directions. Hence, our
work complements the work of Bickel and Freedman (1983) and is not redundant with it.
There has been some recent interest in residual bootstrap methods for penalized likelihood methods in high-dimensions (often proposed for the case when p >> n), for example
lasso estimates (Chatterjee and Lahiri, 2010, 2011), adaptive lasso estimates (Chatterjee
and Lahiri, 2013), de-biased lasso estimates (Belloni et al., 2015; Dezeure et al., 2017), and
ridge regression (Lopes, 2014). These bootstrap results make the assumption of sparsity
of some form, generally in terms of the number of non-zero components of β, but in the
case of Lopes (2014) by the assumption that the design matrix X is nearly low-rank. As
explained previously, our work is focused on a very different line of inquiry: the case of
a comparatively diffuse signal in β, where there is no reduction of the high-dimensional
problem to a low-dimensional approximation.
The role of the distribution of X An important consideration in interpreting theoretical
work on linear models in high dimensions is the role of the design matrix X. In classical
asymptotic theory, the results can be stated conditionally on X so that the assumptions
can be stated in terms of conditions that can be evaluated on any observed design matrix
X. In the high dimensional setting, the available theoretical tools do not yet allow for
an asymptotic analysis conditional on X; instead the results make assumptions about the
distribution of the entries of X. Theoretical work in the nascent literature for the high
dimensional setting usually allows for a fairly general class of distributions for the individual
elements of Xi and can handle covariance between the predictor variables. However, the
Xi
’s are generally considered i.i.d., which limits the ability of any Xi to be too influential
in the fit of the model (see AppendixA for more detail). For discussion of limitations of
the corresponding models for statistical purposes, see Diaconis and Freedman (1984); Hall
et al. (2005); El Karoui (2009).
6
Can We Trust the Bootstrap in High-dimensions?
1.4 Notations and Default Conventions
When referring to the Huber loss in a numerical context, we refer (unless otherwise noted)
to the default implementation in the rlm package in R, where the transition from quadratic
to linear behavior is at k = 1.345. We call X the design matrix and {Xi}
n
i=1 its rows. We
have Xi ∈ R
p
. β denotes the true regression vector, i.e., the population parameter. βbρ refers
to the estimate of β using loss ρ; from this point on, however, we will often drop the ρ and
refer to simply βb. The i-th residual is denoted as ei
, i.e., ei = yi − X0
iβb. Throughout the
paper, we assume that the linear model holds, i.e., yi = X0
iβ + i for some fixed β ∈ R
p and
that i
’s are i.i.d. with mean 0 and var (i) = σ
2

. We call G the distribution of . When we
need to stress the impact of the error distribution on the distribution of βbρ, we will write
βbρ(G) or βbρ() to denote our estimate of β obtained assuming that i
’s are i.i.d. G.
We denote generically by κ = limn→∞ p/n. We restrict ourselves to κ ∈ (0, 1). The
standard notation βb
(i)
refers to the leave-one-out estimate of βb where the i-th pair (yi
, Xi)
is excluded from the regression, and ˜ei(i) , yi − X0
iβb
(i)
is the i-th predicted error (based
on the leave-one-out estimate of βb). We also use the notation ˜ej(i) , yj − X0
jβb
(i)
. The hat
matrix is of course H = X(X0X)
−1X0
. oP denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iidv N (0, Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R 7→ R
+. We use the standard notation ψ = ρ
0
. We finally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We first focus on the method of bootstrap resampling where Fˆ is the conditional distribution
y|β, X. b In this case the distribution of βb∗ under Fˆ is formed by independent resampling
of 
∗
i
from an estimate Gˆ of the distribution G that generated i
. Then new data y
∗
i
are
formed as y
∗
i = X0
iβb + 
∗
i
and the model is fitted to this new data to get βb∗
. Generally the
estimate of the error distribution, Gˆ, is taken to be empirical distribution of the observed
residuals, so that the 
∗
i
are found by sampling with replacement from the ei
.
Yet, even a cursory evaluation of ei
in the simple case of least-squares regression (ρ(x) =
x
2
) reveals that the empirical distribution of the ei may be a poor approximation to the
error distribution of i
. In particular, it is well known that ei has variance equal to σ
2

(1−hi)
where hi
is the ith diagonal element of the hat matrix. This problem becomes particularly
pronounced in high dimensions. For instance, if Xi
iidv N (0, Σ), hi = p/n + oP (1) so that ei
has variance approximately σ
2

(1 − p/n), i.e., generally much smaller than the true variance
of  for lim p/n > 0. This fact is also true when assuming more general distributions for the
design matrix X (see e.g.,Wachter, 1978; Haff, 1979; Silverstein, 1995; Pajor and Pastur,
2009; El Karoui and Koesters, 2011, where the main results of some of these papers require
minor adjustments to get the approximation of hi we just mentioned).
In Figure 1, we plot the error rate of 95% bootstrap confidence intervals based on resampling from the residuals for different loss functions, based on a simulation when the entries
of X are i.i.d. N (0, 1) and  ∼ N(0, 1). Even in this idealized situation, as the ratio of
7
El Karoui and Purdom
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
(a) L1 loss
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
●
●
●
●
(b) Huber loss
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
●
●
●
●
●
●
Residual
Jackknife
Pairs
Std. Residuals
(c) L2 loss
Figure 1: Performance of 95% confidence intervals of β1 : Here we show the coverage error rates for 95% confidence intervals for n = 500 based on applying common resampling-based methods to simulated data: pairs bootstrap (red), residual
bootstrap (blue), and jackknife estimates of variance (yellow). These bootstrap
methods are applied with three different loss functions shown in the three plots
above: (a) L1, (b) Huber, and (c) L2. For L2 and Huber loss, we also show
the performance of methods for standardizing the residuals before bootstrapping
described in the text (blue, dashed line). If accurate, all of these methods should
have an error rate of 0.05 (shown as a horizontal black line). Error rates above
5% correspond to anti-conservative methods. Error rates below 5% correspond
to conservative methods. The error rates are based on 1,000 simulations, with
N(0, 1) error, and entries of the design matrix i.i.d N(0, 1); see the description in
Appendix D.1 for more details. The exact values plotted here are given in Table
A-1 in Appendix I.
p/n increases the error rate of the confidence intervals in least squares regression increases
well beyond the expected 5%: we observe error rates of 10-15% for p/n = 0.3 and approximately 20% for p/n = 0.5. We see similar error rates for other robust-regression methods,
such as L1 and Huber loss, and also for different error distributions and distributions of X
(Supplementary Figures A-1 and A-2). We explain some of the reasons for these problems
in Subsection 2.2 below.
2.1 Bootstrapping from Corrected Residuals
While resampling directly from the uncorrected residuals is widespread and often given
as a standard bootstrap procedure (e.g., Koenker, 2005; Chernick, 1999), the discrepancy
between the distribution of i and ei has spurred more refined recommendations in the
8
Can We Trust the Bootstrap in High-dimensions?
case of least-squares: form corrected residuals ri = ei/
√
1 − hi and sample the 
∗
i
from the
empirical distribution of the ri − r¯ (see e.g., Davison and Hinkley, 1997).
This correction is known to exactly align the variance of ri with that of i regardless
of the design vectors Xi or the true error distribution, using simply the fact that the hat
matrix is a rank min(n, p) orthogonal projection matrix. We see that for L2 loss it corrects
the error in bootstrap inference in our simulations (Figure 1). This is not so surprising,
given that with L2 loss, the error distribution G impacts the inference on β only through
σ
2

, in the case of homoskedastic errors (see Section 2.4 for much more detail).
However, this adjustment of the residuals is a correction specific to the least-squares
problem. Similar corrections for robust estimation procedures using a loss function ρ are
given by McKean et al. (1993) with standardized residuals ri given by,
ri =
ei
√
1 − dhi
, where d =
2
Pe
0
jψ(e
0
j
)
Pψ(e
0
j
)
−
Pψ(e
0
j
)
2
(
Pψ(e
0
j
))2
, (2)
where hi
is the i-th diagonal entry of the hat matrix, e
0
j = ej/s, s is a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
confidence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in highdimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in highdimension, the residuals tend to have a very different distribution from that of the true
errors. Their distributions differ not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of βbρ as p/n → κ ∈ (0, 1); in the course of
doing so, they characterize the asymptotic relationship of ei to i
in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of Gˆ for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and i
in high-dimensions given in the above cited work (see AppendixA for a more detailed and technical summary). Let βb
(i) be the estimate of β based on
fitting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj − X0
jβb
(i)
9
El Karoui and Purdom
For simplicity of exposition, Xi
is assumed to have an elliptical distribution, i.e., Xi = λiΓi
,
where Γi ∼ N(0, Σ), and λi
is a scalar random variable independent of Γi with E

λ
2
i

= 1.
For simplicity in restating their results, we will assume Σ = Idp, but equivalent statements
can be made for arbitrary Σ; similar results also apply when Γi = Σ1/2
ξi
, with ξi having
i.i.d. non-Gaussian entries, satisfying a few technical requirements (see AppendixA).
With this assumption on Xi
, for any sufficiently smooth loss function ρ and any size
dimension where p/n → κ < 1, the relationship between the i-th residual ei and the true
error i can be summarized as,
e˜i(i) = i + |λi
|kβb
ρ(i) − βk2Zi + oP (un) (3)
ei + ciλ
2
i ψ(ei) = ˜ei(i) + oP (un) (4)
where Zi
is a random variable distributed N(0, 1) and independent of i
. The variable un
refers to a sequence of numbers tending to 0. The quantities ci
, λi and kβb
ρ(i) − βk2 are
all of order 1, i.e., they are not close to 0 in general in the high-dimensional setting. The
scalar ci
is given as 1
n
trace
S
−1
i

, where Si =
1
n
P
j6=i ψ
0
(˜ej(i)
)XjX0
j
. For p, n large the ci
’s
are approximately equal and kβb
ρ(i) − βk2 ' kβbρ − βk2 ' E

kβbρ − βk2

; furthermore ciλ
2
i
can be approximated by X0
iS
−1
i Xi/n. Note that when ρ is either non-differentiable at all
points (L1) or not twice differentiable (Huber), arguments can be made that make these
expressions valid, using for instance the notion of sub-differential for ψ (Hiriart-Urruty and
Lemar´echal, 2001).
Interpretation of Equations (3) and (4) Equation (3) means that the marginal distribution
of the leave-i-th-out predicted error, ˜ei(i)
, is asymptotically a convolution of the true error,
i
, and an independent scale mixture of Normals. Furthermore, Equation (4) means that
the i-th residual ei can be understood as a non-linear transformation of ˜ei(i)
. As we discuss
below, these relationships are qualitatively very different from the classical case p/n → 0.
2.2.1 Consequence for the Residual Bootstrap
We apply these results to the question of the residual bootstrap to give an understanding
of why bootstrap resampling of the residuals can perform so badly in high-dimension. The
distribution of the ei
is far removed from that of the i
, and hence bootstrapping from the
residuals effectively amounts to sampling errors from a distribution that is very different
from the original error distribution, .
The impact of these discrepancies for bootstrapping is not equivalent for all dimensions,
error distributions, or loss functions. It depends on the constant ci and the risk, kβb
ρ(i)−βk2,
both of which are highly dependent on the dimensions of the problem, the distribution of
the errors and the choice of loss function. We now discuss some of these issues.
Least Squares regression In the case of least squares regression, the relationships given in
Equation (3) are exact, i.e., un = 0. Further, ψ(x) = x, and ci = hi/(1 − hi), giving the
well known linear relationship ei = (1 − hi)˜ei(i)
(see, e.g., the standard reference Weisberg,
2014). This linear relationship is exact regardless of dimension, though the dimensionality
aspects are captured by hi
. This expression can be used to show that asymptotically
E
Pn
i=1 e
2
i

= σ
2

(n − p), when i
’s have the same variance. Hence, sampling at random   
Can We Trust the Bootstrap in High-dimensions?
from the residuals results in a distribution that underestimates the variance of the errors
by a factor 1 − p/n. The corresponding bootstrap confidence intervals are then naturally
too small, and hence the error rate increases far from the nominal 5% - as we observed in
Figure 1c.
More general robust regression The situation is much more complicated for general robust
regression estimators. One clear implication of Equations (3) and (4) is that simply rescaling
the residuals ei should not in general result in an estimated error distribution Gˆ that will
have similar properties to those of G. The relationship between the residuals and the errors
is very non-linear in high-dimensions. This is why in what follows we will propose to work
with leave-one-out predicted errors ˜ei(i)
instead of the residuals ei
.
The classical case of p/n → 0: In this setting, ci → 0 and therefore Equation (3) shows
that the residuals ei are approximately equal in distribution to the predicted errors, ˜ei(i)
.
Similarly, βbρ is L2 consistent when p/n → 0, so kβb
ρ(i) − βk
2
2 → 0 and Equation (4) gives
e˜i(i) ' i
. Hence, the residuals should be fairly close to the true errors in the model when
p/n is small. This dimensionality assumption is key to many theoretical analyses of robust
regression, and underlies the derivation of corrected residuals ri of McKean et al. (1993)
given in Equation (2) above for losses other than L2.
2.3 Alternative Residual Bootstrap Procedures
We propose two methods for improving the performance of confidence intervals obtained
through the residual bootstrap. Both do so by providing alternative estimates of Gˆ from
which bootstrap errors 
∗
i
can be drawn. They estimate a Gˆ appropriate for the setting of
high-dimensional data by accounting for relationship of the distribution of  and ˜ei(i)
.
Method 1: Deconvolution The relationship in Equation (3) says that the distribution of
e˜i(i)
is a convolution of the correct G distribution and a normal distribution. This suggests
applying techniques for deconvolving a signal from Gaussian noise. Specifically, we propose
the following bootstrap procedure: 1) calculate the predicted errors, ˜ei(i)
; 2) estimate the
variance of the normal (i.e., |λi
|kβb
ρ(i)−βk
2
2
); 3) deconvolve in ˜ei(i)
the error term i from the
normal term; 4) Use the resulting estimate Gˆ to draw errors 
∗
i
for residual bootstrapping.
Deconvolution problems are known to be very difficult (see Fan, 1991, Theorem 1 p.
1260, that gives 1/ log(n)
α rates of convergence when convolving with a Gaussian distribution). The resulting deconvolved errors are likely to be quite noisy estimates of i
. However,
it is possible that while individual estimates are poor, the distribution of the deconvolved
errors is estimated well-enough to form a reasonable Gˆ for the bootstrap procedure.
We used the deconvolution algorithm in the decon package in R (Wang and Wang, 2011)
to estimate the distribution of i
. The deconvolution algorithm requires knowledge of the
variance of the Gaussian that is convolved with the i
, i.e., estimation of |λi
|kβb
ρ(i) − βk2
term. In what follows, we assume a Gaussian design, i.e., λi = 1, so that we need to
estimate only the term kβb
ρ(i) − βk
2
2
. An estimation strategy for the more general setting of
|λi
| 6= 1 is presented in AppendixB.5. We use the fact that kβb
ρ(i) − βk
2
2 ' kβbρ − βk
2
2
for
all i and estimate kβb
ρ(i) − βk2 as var d(˜ei(i)
) − σˆ
2

, where var d(˜ei(i)
) is the empirical variance
of the ˜ei(i) and ˆσ
2

is an estimate of the variance of G, which we discuss below. We note
11
El Karoui and Purdom
that the deconvolution strategy we employ makes assumptions of homoskedastic errors i
’s,
which is true in our simulations but may not be true in practice. See AppendixB for details
regarding the implementation of Method 1.
Method 2: Bootstrapping from standardized e˜i(i) A simpler alternative is bootstrapping
from the predicted error terms, ˜ei(i)
, without deconvolution. Specifically, we propose to
bootstrap from a scaled version of ˜ei(i)
,
r˜i(i) = q
σˆ
var d(˜ei(i)
)
e˜i(i)
,
where var d(˜ei(i)
) is the standard estimate of the variance of ˜ei(i) and ˆσ is an estimate of
σ. This scaling aligns the first two moments of ˜ei(i) with those of i
. On the face of it,
resampling from ˜ri(i)
seems problematic, since Equation (3) demonstrates that ˜ei(i) does not
have the same distribution as i
, even if the first two moments are the same. However, as we
demonstrate in simulations, this distributional mismatch appears to have limited practical
effect on our bootstrap confidence intervals.
Estimation of σ
2
 Both methods described above require an estimator of σ that is consistent
regardless of dimension and error distribution. As we have explained earlier, for general ρ
we cannot rely on the observed residuals ei nor on ˜ei(i)
for estimating σ (see Equations (3)
and (4)). The exception is the standard estimate of σ
2

from least-squares regression, i.e.,
ρ(x) = x
2
,
σb
2
,LS =
1
n − p
X
i
e
2
i,L2
.
σb
2
,LS is a consistent estimator of σ
2
for any error distribution G, assuming i.i.d. errors
and mild moment requirements. In implementing the two alternative residual-bootstrap
methods described above, we use σb,LS as our estimate of σ, including for bootstrapping
robust regression where ρ(x) 6= x
2
.
Performance in bootstrap inference In Figure 2 we show the error rate of confidence intervals
based on the two residual-bootstrap methods we proposed above. We see that both methods
control the Type I error, unlike bootstrapping directly from the residuals, and that both
methods are conservative. There is little difference between the two methods with this
sample size (n = 500), though with n = 100, we observe the deconvolution performance to
be worse in L1 (data not shown).
The deconvolution strategy, however, depends on the distribution of the design matrix,
which in these simulations we assumed was Gaussian (so we did not have to estimate λi
’s).
For elliptical designs (λi 6= 1), the error rate of the deconvolution method described above,
with no adaptation for the design, was similar to that of uncorrected residuals in high
dimensions (i.e., > 0.25 for p/n = 0.5). Individual estimates of λi might improve the
deconvolution strategy, but this problem points to the general reliance of the deconvolution
method on precise knowledge about the design matrix. The bootstrap using standardized
predicted errors, on the other hand, had a Type I error for an elliptical design only slightly
higher than the target 0.05 (around 0.07, data not shown), suggesting that it might be less
sensitive to the properties of the design matrix.
12
Can We Trust the Bootstrap in High-dimensions?
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
* * *
*
(a) L1 loss
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
* *
* *
● Residual
Std. Pred Error
Deconv
N(0, σ
^
ε, LS
2
)
(b) Huber loss
Figure 2: Bootstrap based on predicted errors: We plotted the error rate of 95% confidence intervals for the alternative bootstrap methods described in Section 2.3:
bootstrapping from standardized predicted errors (green) and from deconvolution
of predicted error (magenta). We demonstrate its improvement over the standard
residual bootstrap (blue) for (a) L1 loss and (b) Huber loss. The error distribution is double exponential, but otherwise the simulations parameters are as in
Figure 1. The error rates on confidence intervals based on bootstrapping from a
N(0, σb
2
,LS) (dashed curve) are as a lower bound on the problem. For the precise
error rates see Appendix, Table A-3.
Given our previous discussion of the behavior of ˜ei(i)
, it is somewhat surprising that
resampling from the distribution of ˜ri(i) performed well in our simulations. Clearly a few
cases exist where ˜ri(i)
should work well as an approximation of i
. We have already noted
that as p/n → 0, the effect of the convolution with the Gaussian disappears since kβbρ−βk →
0; in this case both ei and ˜ri(i)
should be good estimates of i
. Similarly, in the case
i ∼ N(0, σ2
), Equation (3) tells us that ˜ei(i) are also asymptotically marginally normally
distributed, so that correcting the variance should result in ˜ri(i) having the same distribution
as i
, at least when Xi,j are i.i.d.
Surprisingly, for larger p/n we do not see a deterioration of the performance of bootstrapping from ˜ri(i)
. This is unexpected, since as p/n → 1 the risk kβbρ − βk
2
2
grows to
be much larger than σ
2

(a claim we will make more precise in the next section); together
with Equation (3), this implies that ˜ri(i)
is essentially distributed N(0, σb
2
,LS) as p/n → 1
regardless of the original distribution of i
. This is confirmed in Figure 2 where we superimpose the results of bootstrap confidence intervals from when we simply estimate Gˆ with
N(0, σˆ
2
,LS); we see the Type I error rate of the confidence intervals based on bootstrapping
from ˜ri(i) do indeed approach that of N(0, σˆ
2
,LS). Putting these two pieces of information
13
El Karoui and Purdom
together leads to the conclusion that as p/n → 1 we can estimate Gˆ simply as N(0, σˆ,LS)
regardless of the actual distribution of .
In the next section we give some theoretical results that seek to understand this phenomenon.
2.4 Behavior of the Risk of βb When κ → 1
In the previous section we saw even if the distribution of the bootstrap errors 
∗
i
, given by Gˆ,
is not close to that of G, we can sometime get accurate bootstrap confidence intervals. For
example, in least squares Equation (3) makes clear that even the standardized residuals, ri
,
do not have the same marginal distribution as i
, yet they still provide accurate bootstrap
confidence intervals in our simulations. We would like to understand for what choice of
distributions Gˆ will we see the same performance in our bootstrap confidence intervals of
βb1?
When working conditional on X as in residual resampling, the statistical properties of
(βb∗ − βb) differ from that of (βb − β) only because the errors are drawn from a different
distribution: Gˆ rather than G. Then to understand whether the distribution of βb∗
1 matches
that of βb1 we can ask, what are the distributions of errors, G, that yield the same distribution
for the resulting βb1(G)? In this section, we narrow our focus on understanding not the entire
distribution of βb1, but only its variance. We do so because under assumptions on the design
matrix X, it is known that βb1 is asymptotically normally distributed. This is true for both
the classical setting of κ = 0 and the high-dimensional setting of κ ∈ (0, 1) (see AppendixA
for a review of these results and a more technical discussion). Our previous question is then
reduced to understanding which distributions G give the same var 
βb1(G)

.
In the setting of least squares, it is clear that the only property of i
iidv G that matters
for the variance of βb
1,L2
is σ
2

, since var 
βb
1,L2

= (X0X)
−1
(1, 1)σ
2

. For general ρ, if
we assume p/n → 0, then var 
βb1,ρ
will depend on features of G beyond the first two
moments (specifically through E

ψ
2
()

/[E (ψ
0
())]2
, (Huber, 1973)). If we assume instead
p/n → κ ∈ (0, 1), then it has been shown (El Karoui et al., 2013) that var 
βb1,ρ(G)

depends on G only by the effect of G on the squared risk of the vector βbρ(G), i.e., through
E

kβbρ(G) − βk
2
2

(for the convenience of the reader we give a review of these results, which
are a bit scattered in the literature, in AppendixA).
For this reason, in the setting of p/n → κ ∈ (0, 1), we need to characterize the risk of βbρ
to understand when different distributions of  result in the same variance of βb1,ρ. In what
follows, we denote by r
2
ρ
(κ; G) the asymptotic squared risk of βbρ(G) as p and n tend to ∞,
r
2
ρ
(κ; G) = lim
n,p→∞,
p
n→κ
E||βbρ(G) − β||2
.
The dependence of r
2
ρ
(κ; G) on  is characterized, under appropriate technical conditions
on X, ρ and i
’s, by a system of two non-linear equations (El Karoui et al., 2013). Specifically, if we define ˆz =  + rρ(κ; G)Z, where Z ∼ N (0, 1) is independent of , and  has the
same distribution G as the i
’s, then there exists a constant c such that the pair of positive,
1 
Can We Trust the Bootstrap in High-dimensions?
finite, and deterministic scalars (c, rρ(κ; G)) satisfy the following system of equations:
E ((prox(cρ))0
(ˆz)) = 1 − κ ,
κr2
ρ
(κ; G) = E

[ˆz − prox(cρ)(ˆz)]2

.
(5)
In this system, prox(cρ) refers to Moreau’s proximal mapping of the convex function cρ (see
Moreau, 1965; Hiriart-Urruty and Lemar´echal, 2001).
It is therefore not entirely trivial to characterize those distributions Γ for which r
2
ρ
(κ; G) =
r
2
ρ
(κ; Γ). In the following theorem, however, we show that as κ → 1, r
2
ρ
(κ; G) converges to a
constant that depends only on σ
2

. This implies that when κ → 1, two different error distributions that have the same variances will result in estimators βb1,ρ with the same variance.
Before stating our theorem formally, however, we will review the necessary assumptions for
the system of equations in (5) to hold. For a precise statement of the assumptions, see El
Karoui (2017)
Assumptions for Equation 5: The proof of (5) provided in El Karoui (2013) assumes
that the Xi
’s have mean 0, cov (Xi) = Idp, and they satisfy sub-Gaussian concentration
assumptions (with constants dependent on n). El Karoui (2013) further assumes that the
i have a unimodal density, are independent from the Xij , sup1≤i≤n
|i
| = OP (polyLog(n)),
and that similiar bounds also hold for a few moments of i (the number of such moments
depends on the loss function ρ). Log-concave densities such as those corresponding to
double exponential or Gaussian errors used in the current paper fall within the scope of
this theorem. The reader interested in generalizations and truly heavy-tailed situation is
referred to El Karoui (2017) and references therein.
The loss function ρ is assumed by El Karoui (2013) (in the unpenalized case) to be
non-negative, twice differentiable, strongly convex, non-linear, taking value 0 at 0, and with
a derivative that grows at most polynomially at infinity and a second derivative that is
locally Lipschitz, with local Lipschitz constant that grow at most polynomially at infinity.
It should be noted that distributions with sufficiently many moments, the condition of
strong convexity of ρ can be obtained by adding δx2/2 to the initial ρ, with δ “small”,
e.g., δ = 10−10, and that modification will change very little or anything to the estimator.
Furthermore, the requirement of strong convexity of ρ, though superficially limiting, is likely
an artifact of the proof, where the main motivation was log-concave distributions with an
eye towards optimality (Bean et al., 2013). In fact, the theoretical predictions of (5) were
verified numerically in El Karoui et al. (2011) outside of the assumptions stated above,
and the predictions of Equation (5) were found to be very accurate in simulations even for
non-smoothed `1 and Huber losses with certain error distributions.
We now state the theorem formally; see AppendixE for the proof of this statement.
Theorem 1 Suppose we are working with robust regression estimators with loss ρ, and
p/n → κ. Suppose that r
2
ρ
(κ; G) is characterized by the system of equations in (5). Then,
lim
κ→1
1 − κ
σ
2

r
2
ρ
(κ; G) = 1 ,
provided ρ is additionally differentiable near 0 and ρ
0
(x) ∼ x near 0.
1 
El Karoui and Purdom
Implications for the Bootstrap For the purposes of the residual-bootstrap, Theorem 1 implies that different methods of estimating the residual distribution Gˆ will result in similar
residual-bootstrap confidence intervals as p/n → 1, if Gˆ has the same variance. This agrees
with our simulations, where both of our proposed bootstrap strategies set the variance of Gˆ
equal to σb
2
,LS and both had similar performance in our simulations for large p/n. Furthermore, as we noted, for p/n closer to 1, they both had similar performance to a bootstrap
procedure that simply sets Gˆ = N (0, σb
2
,LS) (Figure 2) (see also AppendixA.3 for further
discussion of residual bootstrap methods which draw from the “wrong” distribution, i.e.,
forms of wild bootstraps (Wu, 1986)).
We return specifically to the bootstrap based on ˜ri(i)
, the standardized predicted errors.
Equation (3) tells us that the marginal distribution of ˜ei(i)
is a convolution of the distribution
of i and a normal, with the variance of the normal governed by the term kβbρ−βk2. Theorem
1 makes rigorous our previous assertion that as p/n → 1, the normal term will dominate
and the marginal distribution of ˜ei(i) will approach normality, regardless of the distribution
of . However, Theorem 1 also implies that as p/n → 1, inference for the coordinates of β
will be increasingly less reliant on features of the error distribution beyond the variance,
implying that our standardized predited errors, ˜ri(i)
, will still result in an estimate Gˆ that
will give accurate confidence intervals. Conversely, as p/n → 0 classical theory tells us that
the inference of β relies heavily on the distribution G beyond the first two moments, but in
that case the distribution of ˜ri(i) approaches the correct distribution as we explained earlier.
So bootstrapping from the marginal distribution of ˜ri(i) also makes sense when p/n is small.
For κ between these two extremes it is difficult to theoretically predict the risk of
βbρ(Gˆ) when the distribution Gˆ is given by resampling from the ˜ri(i)
. We turn to numerical
simulations to evaluate this risk. Specifically, for i ∼ G, we simulated data that is a
convolution of G and a normal with variance equal to r
2
ρ
(κ; G); we then scale this simulated
data to have variance σ
2

. The scaled data are the 
∗
i
and we refer to the distribution of 
∗
i
as the convolution distribution, denoted Gconv. Then, Gconv is the asymptotic version of
the marginal distribution of the standardized predicted errors, ˜ri(i)
, used in our bootstrap
method proposed above.
In Figure 3 we plot for both Huber loss and L1 loss the average risk rρ(κ; Gconv) (i.e.,
errors given by Gconv) relative to the average risk rρ(κ; G) (i.e., errors distributed according
to G), where G has a double exponential distribution. We also plot the relative average
risk rρ(κ; Gnorm), where Gnorm = N(0, σ2

). As predicted by Theorem 1, for κ close to
1, rρ(κ; Gconv)/rρ(κ; G) and rρ(κ; Gnorm)/rρ(κ; G) converge to 1. Conversely, as κ → 0,
rρ(κ; Gnorm)/rρ(κ; G) diverges dramatically from 1, while rρ(κ; Gconv)/rρ(κ; G) approaches
1, as expected. For Huber, the divergence of rρ(κ; Gconv)/rρ(κ; G) from 1 is at most 8%,
but the difference is larger for L1 (12%), probably due to the fact that the convolution with
a normal error has a larger effect on the risk for L1.
3. Pairs Bootstrap
As described above, estimating the distribution Fˆ from the empirical distribution of (yi
, Xi)
(pairs bootstrapping) is generally considered the most general and widely applicable method
of bootstrapping, allowing for the linear model to be incorrectly specified (i.e., E (yi) is not
a linear function of Xi). It is also considered to be slightly more conservative compared
16
Can We Trust the Bootstrap in High-dimensions?
0.0 0.2 0.4 0.6 0.8
1.00 1.02 1.04 1.06 1.08 1.10 1.12
p/n
Relative Risk
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
(a) Relative risk of Gconv to G
0.0 0.2 0.4 0.6 0.8
1.0 1.1 1.2 1.3 1.4 1.5 1.6
p/n
Relative Risk
●
● ● ●
●
● ●
●
● ● ●
●
●●
●
● ●
● ●
●
●
●
●
● ●
●
●
●
●
●●
●
●
●
● Huber
L1
Convolved
Normal
(b) Relative risk of Gconv and Gnorm to G
Figure 3: Relative Risk of βb for scaled predicted errors vs original errors - population version: (a) Plotted with a solid lines are the ratios of the average risk of
βb(Gconv) to the average risk of βb(G) for Huber and L1 loss. (b) shows the same
plot, but added to the plot (dotted lines) is the relative risk of βb(G) when the
errors are distributed Gnorm = N (0, σ2

) . For both figures, the y-axis gives the
relative risk, and the x-axis is the ratio p/n, with n fixed at 500. Blue/triangle
plotting symbols indicate L1 loss; red/circle plotting symbols indicate Huber loss.
The average risk is calculated over 500 simulations, where the design matrix X
has Gaussian entries. The “true” error distribution G is the standard Laplacian
distribution with σ
2
 = 2. Each simulation uses the standard estimate of σ
2

from
the generated i
’s. rρ(κ; G) was computed using a first run of simulations with
i
iidv G. The Huber loss in this plot is Huber1 and not the default Huber1.345 of
the rlm function.
to bootstrapping from the residuals. In the case of random design, it makes also a lot
of intuitive sense to use the pairs bootstrap, since resampling the predictors might be
interpreted as mimicking the data generating process.
However, as in residual bootstrap, it is clear that the pairs bootstrap will have problems,
at least in quite high dimensions. In fact, when resampling the Xi
’s from Fˆ, the number
of times a certain vector Xi0
is picked has asymptotically Poisson(1) distribution. So the
expected number of different vectors appearing in the bootstrapped design matrix X∗
is
n(1 − 1/e). When p/n is large, with increasingly high probability the bootstrapped design
matrix X∗ will no longer be of full rank. For example, if p/n > (1 − 1/e) ≈ 0.63 then
with probability tending to one as n → ∞, the bootstrapped design matrix X∗
is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that
situation makes little statistical sense (El Karoui, 2010, Subsection 4.4; Zheng et al., 2014).
For smaller ratios of p/n, we evaluate the performance of pairs bootstrapping on simulated data. We see that the performance of the bootstrap for inference also declines
17
El Karoui and Purdom
dramatically as the dimension increases, becoming increasingly conservative (Figure 1). In
pairs bootstrapping, the error rates of 95%-confidence-intervals drop far below the nominal
5%, and are essentially zero for the ratio of p/n = 0.5. Like residual bootstrap, this overall
trend is seen for all the settings we simulated under (Supplemental Figures A-1, A-2). For
L1 loss, even ratios as small as 0.1 yield incredibly conservative bootstrap confidence intervals for βb1, with the error rate dropping to less than 0.01. For Huber and L2 losses, the
severe loss of power in our simulations starts for ratios of 0.3.
A minimal requirement for the distribution of the bootstrapped data to give reasonable
inferences is that the variance of the bootstrap estimator βb∗
1 needs to be a good estimate
of the variance of βb1. This is not the case in high-dimensions. In Figure 5 we plot the
ratio of the variance of βb∗
1
to the variance of βb1 evaluated over simulations. We see that
for p/n = 0.3 and design matrices X with i.i.d. N (0, 1) entries, the average variance of βb∗
1
roughly overestimates the true variance of βb1 by a factor 1.3 in the case of least-squares; for
Huber and L1 the bootstrap estimate of variance is roughly twice as large as it should be.
In the case of least-squares, we can further quantify this loss in power by comparing
the size of the bootstrap confidence intervals to the size of the correct confidence interval
based on theoretical results (Figure 4). We see that even for ratios κ as small as 0.1, the
confidence intervals for some design matrices X were 15% larger for pairs bootstrap than
the correct size (e.g., the case of elliptical distributions where λi
is exponential). For much
higher dimensions of κ = 0.5, the simple case of i.i.d. normal entries for the design matrix
gives intervals that are 80% larger than needed; for the elliptical distributions we simulated,
the width of the bootstrap confidence interval was as much as 3.5 times larger than that
of the correct confidence interval. Furthermore, as we can see in Figure 1, least-squares
regression represents the best case scenario; L1 and Huber will have even worse loss of
power and at smaller values of κ.
3.1 Theoretical Analysis for Least-Squares
In the setting of least-squares, we can for some distributions of the design matrix X theoretically determine the asymptotic expectation of the variance of v
0βb∗ and show that it is a
severe over-estimate of the true variance of v
0βb.
We first setup some notation for the theorem that follows. Define βbw as the result of
regressing y on X with random weight wi for each observation (yi
, Xi). In other words,
βbw = argminu∈Rp
Xn
i=1
wi(yi − X0
iu)
2
.
We assume that the weights are independent of {yi
, Xi}
n
i=1 and define βb∗
w to be the random
variable with distribution equal to that of βbw conditional on the data {yi
, Xi}
n
i=1, i.e., βb∗
w
L=
βbw|{yi
, Xi}
n
i=1. For the standard pairs bootstrap, the distribution of βb∗
from resampling
from the pairs (yi
, Xi) is equivalent to the distribution of βb∗
w, where w is drawn from a
multinomial distribution with expectation 1/n for each entry. In which case, the variance
of v
0βb∗
w refers to the standard bootstrap estimate of variance given by the distribution of
v
0βb∗ over repeated resampling from the pairs (yi
, Xi).
18
Can We Trust the Bootstrap in High-dimensions?
1 1
1
1
Ratio (κ)
% Increase in Average CI Width
2
2
2
2
3
3
3
3
0.01 0.10 0.30 0.50
1
10
50
100
350 1
2
3
Normal
Ellip. Normal
Ellip. Exp
Figure 4: Comparison of width of 95% confidence intervals of β1 for L2 loss:
Here we demonstrate the increase in the width of the confidence interval due to
pairs bootstrapping. Shown on the y-axis is the percent increase of the average
confidence interval width based on simulation (n = 500), as compared to the
average for the standard confidence interval based on normal theory in L2; the
percent increase is plotted against the ratio κ = p/n (x-axis). Shown are three
different choices in simulating the entries of the design matrix X: (1) Xij ∼
N(0, 1) (2) elliptical Xij with λi ∼ N(0, 1) and (3) elliptical Xij with λi ∼
Exp(
√
2). The methods of simulation are the same as described in Figure 1;
exact values are given in Table A-2 in AppendixI.
19
El Karoui and Purdom
We have the following result for the expected value of the bootstrap variance of any
contrast v
0βb∗
w where v is deterministic, assuming independent weights with a Gaussian
design matrix X and some mild conditions on the distribution of the w’s.
Theorem 2 Let the weights (wi)
n
i=1 be i.i.d. and without loss of generality that E (wi) = 1;
we suppose that the wi’s have 8 moments and for all i, wi > η > 0. Suppose Xi’s are i.i.d.
N (0, Σ), Σ is positive definite and the vector v is deterministic with kvk2 = 1.
Suppose βb is obtained by solving a least-squares problem and yi = X0
iβ + i, i’s being
i.i.d. mean 0, with var (i) = σ
2

.
If lim p/n = κ < 1 then the expected variance of the bootstrap estimator, asymptotically
as n → ∞, is given by
p
E

var 
v
0βb∗
w

v
0Σ−1v
= p
E

var 
v
0βbw|{yi
, Xi}
n
i=1
v
0Σ−1v
→ σ
2


κ
1 − κ − f(κ)
−
1
1 − κ

,
where f(κ) = E

1
(1+cwi)
2

and c is the unique solution of E

1
1+cwi

= 1 − κ.
We note that E

1
(1+cwi)
2

≥
h
E

1
1+cwi
i2
= (1−κ)
2
- where the first inequality comes
from Jensen’s inequality, and therefore the expression we give for the asymptotic limit of the
expected bootstrap variance is non-negative. For a proof of this theorem and a consistent
estimator of this limit, see AppendixF.
In light of previous work on model robustness issues in high-dimensional statistics (see
e.g., (Diaconis and Freedman, 1984; Hall et al., 2005; El Karoui, 2009, 2010)), it is natural
to ask whether the central results of Theorem 2 still apply when Xi
is not Gaussian but
has an elliptical distribution. The formula in Theorem 2 does not apply directly to this
latter case. However, the proof given in AppendixF extends to that setting, and we refer
the interested reader to the AppendixF.1 where we give the necessary details of how to
change the formulas and proof to encompass the elliptical case (we do not provide them in
rigorous mathematical detail in this work as they are substantially more cumbersome than
those in Theorem 2 and do not give enough additional insights to justify inclusion). On the
other hand, a number of the quantities appearing in the proof of Theorem 2 will converge
to the same limit as that given in Theorem 2 when i.i.d. Gaussian predictors are replaced
by i.i.d. predictors with mean 0 and variance 1 and sufficiently many moments (an example
being bounded random variables). Thus the results we present here should be fairly robust
to changing i.i.d. normality assumptions for the entries of the design matrix X, but again
the technical work necessary for making this rigorous is beyond the scope of this paper.
Implications for Pairs Bootstrap In the standard pairs bootstrap, the weights are chosen
according to a Multinomial(n, 1/n) distribution. This violates two conditions in the previous theorem: independence of wi
’s and the condition wi > 0. In AppendixF.2, we give the
technical details for how to extend the proof of Theorem 2 to multinomial weights. In what
follows, however, we use i.i.d. Poisson(1) weights, which asymptotically and marginally
correspond to the Multinomial(n, 1/n) weights, to develop intuition about the bootstrap.
In this case, we can apply the formula in Theorem 2 to explain why pairs bootstrap confidence intervals perform poorly in high-dimensions, at least for least squares regression with
Gaussian design matrix.
20
Can We Trust the Bootstrap in High-dimensions?
When Xi
iidv N (0, Σ), it is well known in the least-squares case that the quantity
p var 
v
0βb

/v0Σ
−1v converges asymptotically to κ/(1 − κ)σ
2

(this can be shown through
simple Wishart computations Haff, 1979; Mardia et al., 1979). If the variance of v
0βb∗
w converged to the variance of v
0βb, we should be able to equate this latter quantity to the limit
given in Theorem 2, i.e.,

κ
1 − κ − f(κ)
−
1
1 − κ

=
κ
1 − κ
,
and hence should have
f(κ) = E

1
(1 + cwi)
2

=
1 − κ
1 + κ
.
However, this relationship does not hold for most weight distributions, and in particular does not hold for weights following a Poisson(1) distribution (which asymptotically
corresponds to the standard pairs bootstrap, as explained above). Thus the pairs bootstrap
does not correctly estimate the variance of v
0βb. In Figure 5a we calculate the theoretical
predictions of E

var 
βb∗
w
 given by Theorem 2 (using Poisson(1) weights and Σ = Idp),
and we compare them to the asymptotic variance of βb1 given by κ/(1−κ)σ
2
 /p. We see that
Theorem 2 predicts that the pairs bootstrap overestimates the variance of the estimator
by a factor that ranges from 1.2 to 3 as κ varies between 0.3 and 0.5. These theoretical
predictions correspond to the level of overestimation of the variance seen in our bootstrap
simulations (Figure 5b).
3.2 Alternative Weight Distributions for Resampling
The formula given in Theorem 2 suggests that resampling from a distribution Fˆ defined
using weights other than i.i.d. Poisson(1) (or, equivalently for our asymptotics,
Multinominal(n,1/n)) should give us better bootstrap estimators than using the standard
pairs bootstrap. In fact, we should require, at least, that the bootstrap expected variance
of these estimators match the correct variance var 
v
0βb

= κ/(1−κ)σ
2
 /p (for the Gaussian
design, when Σ = Idp). We focus our discussion on the case Σ = Idp; see AppendixC for
the case Σ 6= Idp.
We note that if we use wi = 1, ∀i, the bootstrap variance will be 0, since with such
a resampling scheme the resampled data set is always the original data set. On the other
hand, we have seen that with wi ∼ Poisson(1), the expected bootstrap variance was too
large compared to κ/(1−κ)σ
2
 /p. Hence, we tried to find alternative weights via calculating
a parameter α such that if
wi
iidv 1 − α + αPoisson(1) , (6)
the expected bootstrap variance would match the theoretical value of κ/(1 − κ)σ
2
 /p.
We numerically solved this problem to find α(κ) (for details of computation see AppendixC). We then used these values and performed bootstrap resampling using the weights
defined in Equation (6). We evaluated bootstrap estimate of var 
βb1

as well as the confidence interval coverage of the true β1. We find that this adjustment of the weights in
21
El Karoui and Purdom
0 0.1 0.2 0.3 0.4 0.5
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
Ratio p/n
Overestimation Factor
(a) L2 (Theoretical)
●
●●
●●
●●●
●●
●
●●
●
●
●●●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●●●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
1 2
3
4
5
6
7
Ratio (κ)
var(
^
β*) / var(
^
β)
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
L2 Huber L1
(b) All (Simulated)
Figure 5: Factor by which standard pairs bootstrap over-estimates the variance:
(a) plotted is the ratio of the value of the expected bootstrap variance computed
from Theorem 2 using Poisson(1) weights to the asymptotic variance κ/(1−κ)σ
2

.
(b) boxplots of the ratio of the bootstrap variance of βb∗
1
to the variance βb1,
as calculated over 1000 simulations (i.e., var 
βb

is estimated across simulated
design matrices X, and not conditional on X). The theoretical prediction for the
mean of the distribution from Theorem 2 is marked with a ‘X’ for L2 regression.
Simulations were performed with normal design matrix X and normal error i
with values of n = 500. For the median values of each boxplot, see Table A-6 in
AppendixI.
estimating Fˆ results in accurate bootstrap estimates of variance and appropriate levels of
confidence interval coverage (Table 1).
However, small changes in the choice of α can result in fairly large changes in
E

var 
v
0βbw|X, . For instance, for κ = 0.5, using the value of α = 0.95 which is close
to the correct value of α(0.5) = 0.92 results in an expected bootstrap variance roughly 30%
larger than it should be.
Moreover, this strategy for finding a good weight distribution requires knowing a great
deal about the distribution of the design matrix. Hence the work we just presented on
finding new weight distributions for bootstrapping is a proof of principle that alternative
weighting schemes could be used for pairs bootstrapping in high-dimension, but important
practical details would depend strongly on the statistical model that is assumed. This is in
sharp contrast with the low-dimensional situation, where a unique and model-free bootstrap
resampling technique works in a broad variety of situations.
22
Can We Trust the Bootstrap in High-dimensions?
κ
.1 .2 .3 .5
α .9875 .9688 .9426 .9203
Error Rate of 95% CIs 0.051 0.06 0.061 0.057
Ratio of Variances 1.0119 1.0236 0.9931 0.9992
Table 1: Summary of weight-adjusted bootstrap simulations for L2 : Given are the
results of performing bootstrap resampling for n = 500 according to the estimate
of Fˆ given by the weights in Equation (6). “Error Rate of 95% CIs” denotes
the percent of bootstrap confidence intervals that did not contain the correct
value of the parameter β1. “Ratio of Variances” gives the ratio of the empirical
expected bootstrap variance over our simulations divided by the theoretical value
σ
2
 κ/(1 − κ). Results are based on 1000 simulations, with a Gaussian random
design and errors distributed as double exponential.
4. The Jackknife
In the context we are investigating, where we know that the distribution of βb1 is asymptotically normal, it is natural to ask whether we could simply use the jackknife to estimate
the variance of βb1. The jackknife relies on leave-one-out procedures to estimate var 
βb1

.
More specifically, for a fixed vector v, the jackknife estimate of var 
v
0βb

is given by:
var dJACK(v
0βb) = n − 1
n
Xn
i=1
(v
0
[βb
(i) − β˜])2
(7)
where β˜ =
1
n
Pn
i=1 βb
(i)
. The case of βb1 corresponds to picking v = e1, i.e., the first canonical
basis vector. The Efron-Stein inequality guarantees in general that the expectation of the
jackknife estimate of variance gives an upper-bound on the variance of the statistic under
consideration (Efron and Stein, 1981).
Given the problems we just documented with the pairs bootstrap, it is natural to ask
whether confidence intervals based on the jackknife estimate of variance perform better
than pairs bootstrap intervals in high-dimensions. The jackknife is known to have problems
(Efron, 1982 or Koenker, 2005, p.105), but the reliance of the jackknife on leave-one-out
estimates βb
(i) might suggest it could be more robust to dimensionality issues than other
methods.
Empirical findings As in the pairs bootstrap case, simulations show that confidence intervals
based on the jackknife estimate of variance lead to extremely poor inference for β1 (Figure
1) and that the jackknife dramatically overestimates the variance of βb1 (Figure 6). For L2
and Huber loss, the jackknife estimate of variance is 10-15% too large for p/n = 0.1, and for
p/n = 0.5 the jackknife estimate of variance is 2-2.5 times larger than it should be. In the
case of L1 loss, the jackknife variance is completely erratic, even in low dimensions; this is not
completely surprising given the known problems with the jackknife for the median (Koenker,
23
El Karoui and Purdom
●
●●
●
●
●● ●
●
●●
●● ●
●
●
●●
● ●
●
●●●
●
●
●●
●
●
●
●●
●
●●●
●●
●
●
●
●
●
●
●●
●
●
●
●
●●●
●
●
●●●
Ratio (κ)
v
a
rJa
c
k(
^
β) / var(
^
β)
1
1.5
3
5
10
20
50
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
L2 Huber L1
Figure 6: Factor by which jackknife over-estimates the variance: boxplots of the
ratio of the jackknife estimate of the variance βb1 to the variance of βb1 as calculated
over 1000 simulations. Simulations were with normal design matrix X and normal
error i with values of n = 500. Note that because the L1 jackknife estimates
so wildly overestimate the variance, in order to put all the methods on the same
plot the boxplot of ratio is on log-scale; y-axis labels give the corresponding ratio
to which the log values correspond. For the median values of each boxplot, see
Table A-6 in AppendixI.
2005). Even for p/n = 0.01, the estimate is not unbiased for L1, with median estimates
twice as large as they should be and enormous variance in the estimates of variance. Higher
dimensions only worsen the behavior with jackknife estimates being 15 times larger than
they should.
4.1 Theoretical Results
Again, in the case of least-squares regression with a Gaussian design matrix, we can theoretically evaluate the behavior of the jackknife. The proof of the following theorem is given
in AppendixG (when the observations have covariance Id) and in AppendixH (to show how
to extend the results to general covariance).
Theorem 3 Let us call varJACK the jackknife estimate of variance of v
0βb given in (7),
where v is any deterministic vector with kvk2 = 1. Suppose the design matrix X is such
that Xi
iidv N (0, Σ), βb is computed using least-squares, and the errors  have a variance.
Then we have, as n, p → ∞ and p/n → κ < 1,
E (varJACK)
var 
v
0βb
 →
1
1 − κ
.
24
Can We Trust the Bootstrap in High-dimensions?
As in Theorem 2, the proof of Theorem 3 is based on random matrix techniques where
further technical work should allow an extension for the entries of Xi,j to be i.i.d. from a
distribution other than Gaussian, provided Xi,j ’s have sufficiently many moments. This is
also beyond the scope of our work, but interested readers can see AppendixG.3 for more
details.
Correcting the Jackknife in Least Squares Theorem 3 implies that scaling the jackknife
estimate of variance by multiplying it by 1 − p/n will result in an estimate of var 
βb1

with
the correct expectation; simulations shown in Figure 7 confirm that confidence intervals
based on this corrected estimate of variance yield correct confidence intervals for leastsquares estimates of βb when the design matrix X is Gaussian. However this scaling factor
is not robust to violations of these assumptions. In particular when the X matrix follows
an elliptical distribution the correction of 1 − p/n from Theorem 3 gives little improvement
even when the loss is still L2 (Figure 7).
Corrections for more general settings For the more general setting of an elliptical design
matrix X and loss function ρ, preliminary computations suggest an alternative result. Let
S be the random matrix defined by
S =
1
n
Xn
i=1
ψ
0
(ei)XiX0
i
.
Then in our asymptotic regime, and when Σ = Idp, preliminary heuristic calculations
suggest that we can estimate the amount by which E (varJACK) overestimates the variance
of βb1 by E (ˆγ), where
γˆ ,
trace
S
−2

/p
[trace (S−1) /p]
2
. (8)
Note that when applied to least-squares regression with X ∼ N (0, Idp) this conforms
to our result in Theorem 3. Theoretical considerations suggest that in our asymptotics, for
smooth ρ, ˆγ ' E (ˆγ), which suggests a data-driven correction to the jackknife estimate of
variance; however that correction depends having information about the distribution of the
design matrix.
Equation (8) assumes that the loss function can be twice differentiated, which is not
the case for either Huber or L1 loss. In the case of non-differentiable ρ and ψ, we can use
appropriate regularizations to make sense of those functions. For ρ = Huberk, i.e., a Huber
function that transitions from quadratic to linear at |x| = k, ψ
0
should be understood as
ψ
0
(x) = 1|x|≤k
. For L1 loss, ψ
0
should be understood as ψ
0
(x) = 1x=0.
In Figure 7 we show simulation results for confidence intervals created based on rescaling
the jackknife estimate of variance by E (ˆγ) defined in Equation (8). In the case of leastsquares with an elliptical design matrix, this correction—which directly uses the distribution
of the observed X matrix—leads to a definite improvement in our jackknife confidence
intervals. Similarly, for the Huber loss we see a definite improvement as compared to the
standard jackknife estimate, as well as an improvement over the simpler correction of 1−p/n
that would be appropriate for squared error loss.
It should be noted that the quality of this proposed correction seems to depend on
how smooth is the function ψ. In particular, even using the previous interpretations, the
2 
El Karoui and Purdom
0.00 0.02 0.04 0.06 0.08 0.10
95% CI Error Rate
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
Normal X
Normal err
Normal X
Db Exp err
Normal X
Normal err
Normal X
Db Exp err
Ellip X
Normal err
Huber L2
Standard Jackknife
1−p/n Correction
Specific Correction
Figure 7: Rescaling jackknife estimate of variance: Shown are the error rates for
confidence intervals for different re-scalings of the jackknife estimate of variance:
the standard jackknife estimate (green); re-scaling using 1 − p/n as given in
Theorem 3 for the L2 case with normal design matrix X (blue); and re-scaling
based on the heuristic in Equation (8) for those settings not covered by the
assumptions of Theorem 3 (magenta). The Huber loss in this plot is Huber1
rather than the default Huber1.345; Huber1 is further from L2 than Huber1.345
and therefore better shows the improvement gained by using the heuristic in
Equation (8).
correction does not perform well for L1 (at least for n = 1000 and κ = 0.1, 0.3, 0.5, data
not shown) - though as we mentioned Figure 6 shows that jackknifing in L1-regression is
probably not a good idea; see also Koenker (2005, Section 3.9).
We also note that the assumption of cov (Xi) = Idp is essential to the jackknife correction
proposed in Equation (8). Let βbρ(Σ) denotes our estimator of β when cov (Xi) = Σ. ˆγ in
equation (8) is an estimate of
R(βbρ) =
E

varJACK(v
0βbρ(Idp))
var 
v
0βbρ(Idp)
 .
Invariance arguments applied to cov (Xi) can be used to show that when Xi has an elliptical
distribution, then R(βbρ(Idp)) = R(βbρ(Σ)) for all convex loss functions ρ (see AppendixH).
26
Can We Trust the Bootstrap in High-dimensions?
However, even though this population quantity is unchanged when cov (Xi) = Σ changes,
the estimate ˆγ we give above depends crucially on knowing that cov (Xi) = Idp and cannot
be used as-is when Σ 6= Idp.
5. Conclusion
In this paper, we have studied various resampling plans in the high-dimensional setting
where p/n is not close to zero. One of our main findings is that the two most widely-used
and advocated bootstraps will yield either highly conservative or highly anti-conservative
confidence intervals. This is in sharp contrast to the low-dimensional setting where p is
fixed and n → ∞ or p/n → 0. Under various assumptions underlying our simulations, we
explained theoretically the phenomena we were observing in our numerical work.
Beside our theoretical contributions, we propose improved and dimension-adaptive bootstrap methods for both pairs and residual bootstraps, as well as jackknife corrections, that
give confidence intervals with approximately correct coverage probability. These methods are novel dimensionality-robust resampling techniques for linear models. The resulting
modified pairs bootstrap gives a principled method for pairs bootstrapping regardless of the
value of p/n < 1, and avoids the problem of non-invertible bootstrapped design matrices
X∗
that commonly result from the standard pairs bootstrap. The most promising of our
proposed resampling schemes is our proposed residual bootstrapping method that resamples
from appropriately scaled predicted errors. This bootstrap routine performed well without
distribution-specific corrections that some of our other methods require. It has the greatest
potential to be a general-purpose bootstrap method for linear models in high dimensions.
This work has focused on estimation of the linear model, where there are theoretical
benchmarks. The real practical power of the bootstrap lays in giving the ability to perform
inference in complicated settings involving sophisticated statistical procedures for which
we do not even begin to have theoretical results for the behavior of our estimators. Yet
our work shows that even for the simple case of inference in the linear model and for the
simplest inferential question, the two most common and natural resampling techniques perform very poorly in only moderately high-dimensions. More importantly, these two equally
intuitive methods have completely divergent statistical behavior with one being extremely
conservative and the other anti-conservative. This casts serious doubts about the reliability, interpretability and accuracy of inferential statements made through generic resampling
methods in moderate and high dimensions. This is troubling for more complicated problems
where resampling techniques are the only inference tools currently available. Our findings
also suggest that appropriate resampling methods for high-dimensional problems may not
be able to rely on generic resampling procedures but rather need to be tailored to the statistical problem of interest. This raises many interesting new theoretical and methodological
questions for the future.

Appendix A. Technical Background on Existing Literature on Robust
Regression
Recall that we consider
βbρ = argminu∈Rp
Xn
i=1
ρ(yi − X0
iu) , where yi = i + X0
iβ .
The i
’s are assumed to be i.i.d with mean 0 here.
A.1 Classical Results and Asymptotic Normality
Least-squares In this case ρ(x) = x
2/2 and we have of course
βb
LS − β = (X0X)
−1X0
 .
Hence,
cov 
βb
LS
= (X0X)
−1
var () .
Robust regression We recall the classic result of Huber (Huber, 1973) and (Huber and
Ronchetti, 2009), Chapter 7: when p is fixed and n → ∞, the limiting covariance of βbρ is,
with a slight abuse of notation,
cov 
βbρ

=
1
n

X0X
n
−1 E

ψ
2
()

[E (ψ0())]2
.
See also the papers Portnoy (1984, 1985, 1987); Mammen (1989) for the situation where
p → ∞ and p/n → 0 at various rates.
Asymptotic normality questions and impact on confidence intervals: p/n → 0 In the case
of least-squares, the Lindeberg-Feller theorem (Stroock, 1993) guarantees that under mild
conditions on the p × n matrix X, the coordinates of βb
LS are asymptotically Normal.
Similarly if the 1 × n vector v
0
(X0X)
−1X0
satisfy the conditions of the the Lindeberg-Feller
theorem, then v
0
(βb
LS − β) is asymptotically normal. Similarly, under mild conditions on
X, the classic papers mentioned above guarantee asymptotic normality of the coordinates
of βbρ when p/n → 0. In these cases, the width of confidence intervals for the coordinates of
β are hence only dependent asymptotically on the variance of the coordinates of βbρ.
A.2 Summary of recent results on high-dimensional robust regression
We summarize in this section the key results we use from the recent papers El Karoui et al.
(2011); El Karoui et al. (2013); El Karoui (2013, 2017). The third paper is a mathematically
rigorous version of the heuristic arguments of the first two; the first paper is the long-form
version of the second one. Those papers are concerned with the asymptotic properties of
βbρ when p/n → κ ∈ (0, 1). The predictor vectors Xi
’s are assumed to be random and
independent, with Xi = λiΣ
1/2X˜
i
, where X˜
i has i.i.d (not necessarily Gaussian) entries
with mean 0 and variance 1. λi
’s are independent random variables with E

λ
2
i

= 1. (The
  
Can We Trust the Bootstrap in High-dimensions?
n×p design matrix X is full rank with probability 1. Σ has only positive eigenvalues.) Xi
’s
are independent of i
’s.
Role of cov (Xi) = Σ It is shown in these papers that, if βb(β; Σ) is the regression vector
corresponding to the situation where yi = X0
iβ + i and cov (Xi) = Σ for all i,
βbρ(β; Σ) = β + Σ−1/2βb(0; Idp) .
This follows from a simple change of variable. It also means that to understand the properties of βbρ(β; Σ), it is enough to understand the “null case” β = 0 and Σ = Idp.
Consequence for leave-one-out-predicted errors The result we just mentioned has an important consequence for our leave-one-out predicted error, i.e ˜ei(i) = yi − X0
iβb
(i)
: ˜ei(i)
(β; Σ) =
e˜i(i)
(0; Idp). In other words, we can assume without loss of generality that β = 0 and
Σ = Idp when working with leave-one-out-predicted errors.
A non-asymptotic and exact stochastic representation in the elliptical case When Xi
iidv λiυi
,
where υi ∼ N (0, Σ) and λi
is a random variable independent of υi
, it is shown that
βbρ(β; Σ) L= β + kβbρ(0; Idp)k2Σ
−1/2u ,
where u is uniformly distributed on the unit sphere in R
p and kβbρ(0; Idp)k2 is independent
of u. kβbρ(0; Idp)k2 is simply the norm of βbρ when β = 0 and cov (Xi) = Idp. Note that u
has the stochastic representation u
L= Zp/kZpk2, where Zp ∼ N (0,Idp).
Consequence of the previous representation for large p Since kZpk2 has χp distribution, it
is clear that as p → ∞, if v is a deterministic vector,
√
p
v
0
(βbρ(β; Σ) − β)
kβbρ(0; Idp)k2
=⇒ N (0, v0Σ
−1
v) ,
where =⇒ denotes weak convergence of distributions. Hence, provided kβbρ(0; Idp)k2 and
v
0Σ
−1v remain bounded, v
0βbρ(β; Σ) is √p-consistent for v
0β.
Properties of kβbρ(0; Idp)k2 It is shown, under various technical assumptions, that as p and n
tend to infinity with p/n → κ, the variance of the random variable kβbρ(0; Idp)k2 goes to zero.
Hence, for practical matters, kβbρ(0; Idp)k2 can be considered non-random. In particular,
that implies that
√
pv0
(βbρ(β; Σ) − β) is approximately Normal as p/n → κ .
Of great importance is the characterization of kβbρ(0; Idp)k2, since it will affect the width of
confidence intervals. It can be characterized, in the case where λi = 1 (see the papers for the
case λi 6= 1) in the following way: kβbρ(0; Idp)k2 → rρ(κ). The non-random scalar rρ(κ) can
be characterized through a system of two non-linear equations, involving another constant,
c. The pair of positive and deterministic scalars (c, rρ(κ)) satisfy: if ˆz =  + rρ(κ)Z, where
Z ∼ N (0, 1) is independent of , and  has the same distribution as i
’s:

E ((prox(cρ))0
(ˆz)) = 1 − κ ,
κr2
ρ
(κ) = E

[ˆz − prox(cρ)(ˆz)]2

.
(9)
3 
El Karoui and Purdom
In this system, prox(cρ) refers to Moreau’s proximal mapping of the convex function cρ
- see Moreau (1965) or Hiriart-Urruty and Lemar´echal (2001). (The system is rigorously
shown in El Karoui (2013) under the assumption that the Xi
’s have i.i.d entries with mean
0 and variance 1, as well as a few other minor requirements; these assumptions are satisfied
when Xi,j have a Gaussian distribution, or are bounded, or do not have heavy tails, the
latter requiring appeal to various truncation arguments. Another proof of the validity of
this system, which first appeared in El Karoui et al. (2011), can be found in Donoho and
Montanari (2013). That proof is limited to the case of Xi
’s having i.i.d Gaussian entries.)
The assumptions on i
’s and ρ are relatively mild. See El Karoui (2017) for the latest,
handling the situation where i
’s have for instance a Cauchy distribution. We note that
some of the results in El Karoui (2013) are stated with ρ strongly convex (and i
’s having
many moments). While the proof in that paper suggests several ways of removing this
assumption, it is also possible to change ρ in to ρ+ηx2/2 with η very small (e.g η = 10−100)
to satisfy this technical assumption and change essentially nothing to the statistical problem
at hand.
Consequences for the distribution of βb1 or other contrasts of interest In our simulation setup,
the previous results imply that the distribution of βb1 (or any other coordinates or contrasts
v
0βb for v deterministic) is asymptotically normal. In the case where Σ = Idp, the variance of
√p(βb1 − β1) is roughly N (0, r2
ρ
(κ)). See Bean et al. (2013) and its supplementary material
for a longer discussion and questions related to building confidence intervals.
Asymptotic normality questions and impact on confidence intervals: p/n → κ ∈ (0, 1)
Because we know that, in the Gaussian design case, the coordinates of βbρ are asymptotically normal, the width of these intervals is completely determined by the variance of the
coordinates of βbρ. We explain above how these variances depend on the distribution of  and
the loss function ρ: basically through kβb(ρ; Id)k2 and hence rρ(κ). Therefore, as was the
case in the low-dimensional situation, the variance of the coordinates of βbρ can be used as a
proxy for the width of the confidence interval in the high-dimensional case where p/n → κ,
0 < κ < 1.
In (Bean et al., 2013), these asymptotic normality results are used to create confidence
intervals for v
0β in the Gaussian design case: if z1−α/2
is the (1 − α/2)-quantile of the
Gaussian distribution a 100(1 − α)% confidence interval for v
0β is
v
0βb ±
z1−α/2
√p
rˆ
q
(1 − p/n)v
0Σb−1v ,
where ˆr is a consistent estimator of kβbρ(0; Idp)k2. In (Bean et al., 2013), it is said without
more precision that leave-one-techniques can be used to come up with ˆr; we propose in the
current paper estimates ˆr based on leave-one-out predicted errors that can therefore be used
for the purpose of building those confidence intervals. (See Section 2.3 in the main paper)
Leave-one-out approximations for βb It is shown in the aforementioned papers that
βb ' βb
(i) +
1
n
S
−1
i Xiψ(ei) ,
where ' means that we are neglecting a quantity that is negligible for all our mathematical
and statistical purposes (see the papers for very precise bounds on the quantity we are
32
Can We Trust the Bootstrap in High-dimensions?
neglecting). This approximation is the key to the approximations in Equations (3) and (4)
which we use in the main paper. Recall that Si =
1
n
P
j6=i ψ
0
(˜ej(i)
)XjX0
j
.
A.3 Consequences for the residual bootstrap
We call {
∗
i
}
n
i=1 the estimated errors used in the residual bootstrap. When doing a residual
bootstrap, we are effectively sampling from a model with fixed design X, “true β” taken to
be equal to βbρ and i.i.d errors sampled according to the empirical distribution of the {
∗
i
}
n
i=1.
As a shortcut, we call this distribution 
∗
in what follows. We call βb∗
ρ
the bootstrapped
version of βb.
Case p/n → 0 Naturally, the classic results mentioned above imply that the distribution
of v
0
(βb∗
ρ − βbρ) is going to be asymptotically normal (under mild conditions on X that are
satisfied in our simulations); the variance of the coordinates of βb∗
ρ
, on the other hand depends
on
E(ψ
2
(
∗))
[E(ψ0(
∗))]2 . Hence, even if the distribution of the estimated errors 
∗
is very different
from that of the “true” errors, , the residual bootstrap may work very well: indeed, if 
and 
∗ have two very different distribution but
E

ψ
2
(
∗
)

[E (ψ0(
∗))]2
=
E

ψ
2
()

[E (ψ0())]2
,
using a residual bootstrap with “the wrong error distribution”, 
∗
, will give us bootstrap
confidence intervals of the right width. An important question then becomes, when p/n is
small: what class of distributions 
∗
is such that E(ψ
2
(
∗))
[E(ψ0(
∗))]2 =
E(ψ
2
())
[E(ψ0())]2 , as this class defines
all acceptable error distributions from the point of view of our residual bootstrap.
Case p/n → κ ∈ (0, 1) We note that at this point in the case p/n → κ ∈ (0, 1) we are
not aware of central limit theorems for the coordinates of βb that are valid conditional on
the design matrix X. However, it is expected that such theorems will hold if the design
matrix results from a draw of a random design matrix similar to the ones we consider (with
very high-probability with respect to the sampling of the design matrix). The discussions
above make then clear that the key quantity to describe the width of the residual bootstrap
confidence intervals becomes the risk kβbρ(0; Idp; 
∗
)k2, i.e the risk kβbρ(0; Idp)k2 when the
error distribution is 
∗
. A “good” error distribution is therefore one for which rρ(κ; 
∗
) '
rρ(κ; ). (We used the notation rρ(κ; ) = limn→∞kβbρ(0; Idp; )k, when p/n → κ.)
The case of least squares Let us call Gˆ
n,p the distribution of the errors we use in our residual
bootstrap. We assume that Gˆ
n,p has mean 0. Let us call w
0 = v
0
(X0X)
−1X0
- where we
choose to not index v and w by p for the sake of clarity. v is a deterministic sequence of pdimensional vectors. Assume that w and Gˆ
n,p satisfy the conditions of the Linderberg-Feller
theorem for triangular arrays, and that limn→∞ var 
Gˆ
n,p
= σ
2

. Then the Lindeberg-Feller
theorem guarantees that
v
0
(βb∗ − βb)
kwk
=⇒ N (0, σ2

) .
  
El Karoui and Purdom
Note that it also guarantees, under the same assumptions on w that
v
0
(βb − β)
kwk
=⇒ N (0, σ2

) .
These results do not depend on the size of κ, the limit of the ratio p/n.
Informally, what this means is that provided that the entries of w are all relatively small,
that Gˆ
n,p has mean 0 and var 
Gˆ
n,p
is close to σ
2

, then bootstrapping from the residuals
in least-squares works for approximating the distribution v
0
(βb − β).
Conclusion for the purposes of the main paper In our discussions we use kβbρ(0; Idp; 
∗
)k
and its closeness to its value under the correct error distribution, kβbρ(0; Idp; )k, as a proxy
to understand a priori the quality of residual bootstrap confidence intervals when using 
∗
to sample the errors instead of . The previous discussion explains why we do so. Our
numerical work in Section 2.3 of the main text shows numerically that this yields valuable
insights. This is why our discussion in Section 2.4 is focused on understanding kβb(0; Idp; )k2
for various error distributions. In particular, Theorem 1 shows that when p/n is close to 1,
if 
∗ has approximately the same two first moments as , kβb(0; Idp; 
∗
)k/kβb(0; Idp; )k ' 1.
This explains why the scaled ˜ri(i)
is probably a good error distribution 
∗
to use in the
residual bootstrap when κ is close to 0 or 1. We note that when κ is close to 1, ˜ri(i) gives an
error distribution that is in general very different from the distribution of . Our numerical
work of Section 2.3 shows that it is nonetheless a good error distribution from the point of
view of the residual bootstraps we consider.
Appendix B. Deconvolution Bootstrap
In the main text, we considered situations where our predictors Xi are i.i.d with an elliptical
distribution and assume for instance that Xi = λiξi
, where ξi ∼ N (0, Σ) and λi are i.i.d
scalar random variables with E

λ
2
i

= 1. As described in the main text, if X is elliptical,
e˜i(i)
is a convolution of the correct G distribution and a Normal distribution,
e˜i(i) ' i + Z˜
i
,
where
Z˜
i
iidv N (0, λ2
i kβb
ρ(i) − βk
2
2
)
and are independent of i
.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Specifically, we proposed the
following bootstrap procedure:
1. Calculate the predicted errors, ˜ei(i)
2. Estimate |λi
|kβb
ρ(i) − βk2 (the standard deviation of the Z˜
i)
3. Deconvolve in ˜ei(i)
the error term i from the Z˜
i term ;
4. Use the resulting estimates of G as the estimate of Gˆ in residual bootstrapping.
3 
Can We Trust the Bootstrap in High-dimensions?
B.1 Estimating kβbρ − βk and the Variance of the Zi
Deconvolution methods that deconvolve  from the Z˜
i require an estimate of the variance
of the Z˜
i
. Equation (3) gives the variance as λ
2
i
kβb
ρ(i) − βk
2
2
, and we need to estimate this
quantity from the data. We use the approximation
kβb
ρ(i) − βk2 ' kβbρ − βk2.
See AppendixA and references therein for justification of this approximation.
Furthermore, as we note in the main text, in our implementation of this deconvolution in
simulations we assume X ∼ N (0, Idp) so that λi = 1 (see Section B.5 below for estimating
λi
in the elliptical case). This means we are estimating the variance of Z˜
i as kβb − βk
2
2
for
all i. We estimate this as
var d(Z˜
i) = var d(˜ei(i)
) − σˆ
2

,
where var d(˜ei(i)
) is the standard estimate of variance and ˆσ
2

is the estimate of variance from
the least squares fit, σb
2
,LS, defined in the main text.
In the case where var d(˜ei(i)
) ≤ σˆ
2

, we do not do a deconvolution, but simply bootstrap
from the ˜ei(i)
. This is generally only the case when p/n is quite small.
B.2 Estimating Gˆ
We used the deconvolution algorithm in the decon package in R (Wang and Wang, 2011) to
estimate the distribution of i
. Deconvolution algorithms require selection of a bandwidth in
the kernels that make up the functional basis of the estimate. The appropriate bandwidth
parameter in deconvolution problems is tied intrinsically to the use of the estimate, with
optimal bandwidths depending on what functional of the distribution is wanted (e.g. the
pdf versus the cdf). Moreover, the optimal bandwidth depends on the distribution of Z˜
i
with which the signal is being convolved. Ultimately, our procedure resamples from the
distribution Gˆ, requiring estimates of G−1
(y), and the distribution of Z˜
i
is Gaussian. There
is no specific theory for the optimal bandwidth in this setting (though see the work of Hall
and Lahiri (2008) for optimal bandwidth selection for estimations of the quantiles of Gˆ
if the Z˜
i are distributed according to a distribution whose characteristic function decays
polynomially at infinity - see Assumption (A.11) on p.2133; this is clearly violated in our
case where Z˜
i are normally distributed.)
We used the bandwidth estimation procedure bw.dboot2 provided in the package decon.
Delaigle (2014) outlines problems in the estimation of bandwidth parameter in decon; specifically that the implementation in decon of existing bandwidth estimation procedures does
not match their published descriptions. bw.dboot2 was not one of the bandwidth procedures with these discrepancies. However, we also compared our results with a bandwidth
selected via the bandwidth selection method of Delaigle and Gijbels (2002, 2004) and used
the R code implementation provided by the authors on http://www.ms.unimelb.edu.au/
~aurored/links.html#Code. The two different choices in bandwidth, however, had little
effect on the coverage of the bootstrap confidence intervals (Supplemental Figure 8). The
results in Figure 2 in the main text make use of the bandwidth parameter of Delaigle and
Gijbels (2002, 2004).
35
El Karoui and Purdom
0.00 0.02 0.04 0.06 0.08
Ratio (κ)
95% CI Error Rate
●
●
●
●
●
●
●
●
0.01 0.30 0.50
●
Delaigle
Decon
(a) L1 loss
0.00 0.02 0.04 0.06 0.08
Ratio (κ)
95% CI Error Rate
●
●
●
●
●
● ●
●
0.01 0.30 0.50
(b) Huber loss
Figure 8: Different bandwidths for Method 1: We plotted the error rate of 95% confidence intervals for the deconvolution bootstrap (Method 1) using two different
choices of bandwith: the bw.dboot2 in decon (light blue) or that of Delaigle and
Gijbels (2002, 2004) (maroon). The solid lines refer to bootstrapping by drawing
{
∗
i
}
n
i=1 as i.i.d draws from Gˆ; the dashed lines refer to {
∗
i
}
n
i=1 drawn from repeated resampling of a single draw ({ˆi}
n
i=1) from Gˆ. See section B.4 below. Note
that the y-axis for these plots is different than that shown in the main text.
For both bandwidth selections, we estimated the cdf using the function DeconCdf provided in the decon package and provided the bandwidth parameters described above. We
specified the error distribution as ‘Normal’ and set the variance of Z˜
i as described above in
Section B.1. The number of grid points for evaluating the cdf (the ‘ngrid’ argument) was
set to be the number needed to get a space of 0.01 across the range of observed ˜ei(i)
, with
a lower bound of 512 grid points (the default of ‘ngrid’ given by the DeconCdf function).
Other options were set to the default of DeconCdf.
B.3 Random Draws from Gˆ
The end result of the DeconCdf function was values of the Gˆ evaluated at specific grid points
x. The resulting Gˆ(x) was not always guaranteed to be ≤ 1 nor monotonically decreasing;
this is likely due to the fact that use of higher-order kernels estimates (which is standard
practice in deconvolution literature) does not constrain the estimate be a proper density.
Furthermore, the tail ends of the cdf are based on little data and unlikely to be reliable,
as well as having problems either non-monotonicity or extending beyond the boundaries of
(0, 1). We truncated the left tail of Gˆ(x) to be within 0.001 by finding the largest such x0
such that Gˆ(x0) ≤ 0.001 and setting Gˆ(x) = 0.001 for x ≤ x0; and we similarly trimmed
the right tail based on 1 − 0.001. We then calculated the differences di = Gˆ(xi) − Gˆ(xi−1)
36
Can We Trust the Bootstrap in High-dimensions?
and for di < 0 set di = 0. We then defined a monotone cdf based on the cumulative sum of
the di
,
C(xj ) = X
j
i=1
di
.
We then renormalized the values C(xj ) so that they extend from 0 to 1, giving the final
monotone estimate of Gˆ(xj ) as
Gˆ(xj ) = C(xj ) − mini C(xi)
maxi C(xi) − mini C(xi)
To randomly sample from Gˆ, we needed to be able to evaluate Gˆ for all x. We did this
by linearly interpolating between the Gˆ(xj ) values. In what follows, we consider the values
Gˆ(x) based on this smoothed and monotone version of the original output of the DeconCdf
function.
We create random draws from Gˆ by drawing random variables Ui from a Unif(0, 1) and
calculating Ei = Gˆ(Ui). We further centered and standardized the draws Ej from Gˆ to get

∗
j = (Ej − meani(Ej )) σb,LS
p
var (Ei)
so that the resulting 
∗
j
have mean zero and variance σb
2
,LS. This was done because the
variance of Gˆ was not guaranteed to have the correct variance, despite the fact we had to
pre-specify the variance in the deconvolution call. Ensuring the correct moments of 
∗
j was
a critical component for reasonable coverage of the bootstrap confidence intervals. When
we did not standardize the results and just took the draws from Ej , the resulting bootstrap
confidence intervals became more and more conservative as p/n grew. This again highlights
the results of Theorem 1 – the variance of Gˆ is the most important feature of the distribution
in order to have accurate confidence intervals.
B.4 Bootstrap Estimates βb∗
from Gˆ
We used Gˆ to create bootstrap errors, {
∗
i
}
n
i=1 in two ways. For the first method we estimated
{
∗
i
}
n
i=1 as i.i.d draws from Gˆ, and repeatedly drew such samples from Gˆ, B times. In the
second method, we drew one single estimate {ˆi}
n
i=1 as i.i.d draws from Gˆ and then created
{
∗
i
}
n
i=1 from resampling from the empirical distribution of the {ˆi}
n
i=1, and repeated this
resampling from the empirical distribution of {ˆi}
n
i=1 B times. For both methods, we then
calculated βb∗
from the data (Xi
, y∗
i
) where y
∗
i = X0
iβb + 
∗
i
, as in the standard residual
bootstrap. The first method seems to do slightly better in simulations, see Figure 8.
B.5 Estimation of λ
2
i
To extend the deconvolution bootstrapping method to the elliptical case when p/n →
κ ∈ (0, 1), one needs to be able to estimate λi
, at least up to sign. In which case, one
could estimate individually the variance of Z˜
i and feed these individual estimates into the
deconvolution method described above.
37
El Karoui and Purdom
We recall a simple proposal from the paper (El Karoui, 2010) to solve this problem.
Specifically, the author proposes to use
λbi
2
=
kX2
i
k2/p
1
p
trace 
Σb
 =
kXik
2
2
1
n
Pn
i=1kXik
2
2
,
where Σ = b 1
n
Pn
i=1 XiX0
i
. Under mild conditions on Σ and λi
, it can be shown that when
n → ∞ and p/n → κ ∈ (0, ∞)
sup
1≤i≤n
|λ
2
i − λc2
i
| → 0 in probability.
The intuition and proof are as follows. Concentration of measure arguments (Ledoux, 2001)
show that if ξi ∼ N (0, Σ), kξik
2/p ' trace (Σ) /p and hence kXik
2/p ' λ
2
i
trace (Σ) /p. The
law of large numbers and a little bit of further technical work then imply that 1
n
Pn
i=1kXik
2/p '
E

λ
2
i

trace (Σ) /p = trace (Σ) /p.
Appendix C. Alternative Weight Distributions for Pairs Bootstrap
The formula given in Theorem 2 suggests resampling from a distribution Fˆ defined using
weights other than i.i.d. Poisson(1). An acceptable weight distribution is such that the
variance of the resampled estimator is equal to the variance of the sampling distribution
of the original estimator. In Section 3.2 we consider the least-squares estimator where the
variance, in the case where Σ = Idp is asymptotically κ/(1 − κ)σ
2
 /p.
In the main text, we proposed to use
wi
iidv 1 − α + αPoisson(1)
We determined α numerically so that

κ
1
1 − κ − Ewi
h
1
(1+cwi)
2
i −
1
1 − κ

 =
κ
1 − κ
. (10)
This was done via a simple dichotomous search for α over the interval [0, 1]. Our initial
α was .95. We specified a tolerance of 10−2
for the results reported in the paper in Table
1. This means that we stopped the algorithm when the ratio of the two terms in Equation
(10) was within 1% of 1. We used a sample size of 106
to estimate all the expectations.
Table 2 gives the values of α(κ) found.
C.1 Case Σ 6= Idp
In the case where Σ 6= Id, both E

var 
v
0βb∗
 and var 
v
0βb

depend on v
0Σ
−1v. It is
therefore natural to ask how we could estimate this quantity. If we are able to do so, it is
clear that we could follow the same strategy as above to find α from the data. Standard
Wishart results (Mardia et al. (1979), Theorem 3.4.7) give that
v
0Σ
−1v
v
0Σb−1v
∼
χ
2
n−p
n − 1
→ (1 − κ)in probability.
3 
Can We Trust the Bootstrap in High-dimensions?
κ 0.05 0.10 0.15 0.20 0.25
α(κ) 0.9938 0.9875 0.9812 0.9688 0.9562
κ 0.30 0.35 0.40 0.45 0.50
α(κ) 0.9426 0.9352 0.9277 0.9222 0.9203
Table 2: Values of α(κ) to use to fix the variance estimation issue in high-dimensional
pairs-bootstrap
This of course suggests using (1 − p/n)v
0Σb−1v as an estimator of v
0Σ
−1v and solves the
question we were discussing above.
However, we note that since
E

var 
v
0βb∗

var 
v
0βb

does not depend on Σ when the design is Gaussian or Elliptical, the same α should work
regardless of Σ, provided it is positive definite. In particular, an acceptable weight distribution for resampling as defined above could be computed by assuming Σ = Idp and would
work for any positive definite Σ.
Appendix D. Description of Numerics
Here we describe the implementation of various computational numerics used in the paper.
D.1 Simulation Description
In the simulations described in the paper, we explored variations in the distribution of the
design matrix X, the error distribution, the loss function, the sample size (n), and the ratio
of κ = p/n, detailed below.
All results in the paper were based upon 1, 000 replications of our simulation routine for
each combination of these values. Each simulation consisted of
1. Simulation of data matrix X, {i}
n
i=1 and construction of data yi = X0β+i
. However,
for our simulations, β = 0 (without loss of generality for the results, which are shift
equivariant), so yi = i
.
2. Estimate βˆ using the corresponding loss function. For L2 this was via the lm command
in R, for Huber via the rlm command in the MASS package with default settings
(k = 1.345) (Venables and Ripley, 2002), and for L1 via an internal program making
use of MOSEK optimization package and accessed in R using the Rmosek package
(MOSEK, 2014). The internal L1 program was checked to give the same results as
the rq function that is part of the R package quantreg (Koenker, 2013), but was
much faster for simulations.
3. Bootstrapping according to the relevant bootstrap procedure (using the boot package)
and estimating βˆ∗
for each bootstrap sample. Each bootstrap resampling consisted of
39
El Karoui and Purdom
R = 1, 000 bootstrap samples, the minimum generally suggested for 95% confidence
intervals (Davison and Hinkley, 1997). For jackknife resampling and for calculating
leave-one-out prediction errors ˜ei(i)
, we wrote an internal function that left out each
observation in turn and recalculated βˆ
(i)
.
4. Construction of confidence intervals for βˆ
1. For bootstrap resampling, we used the
function boot.ci in the boot package to calculate confidence intervals. We calculated
“basic”, “percentile”, “normal”, and “BCA” confidence intervals (see help of boot.ci
and Davison and Hinkley (1997) for details about each of these), but all results shown
in the manuscript rely on only the percentile method. The percentile method calculates the boundaries of the confidence intervals as the estimates of 2.5% and 97.5%
percentiles of βˆ∗
1
(note that the estimate is not exactly the observed 2.5% and 97.5%
of βˆ∗
1
, since there is a correction term for estimating the percentile, again see Davison
and Hinkley (1997)). For the jackknife confidence intervals, the confidence interval
calculated was a standard normal confidence interval (±1.96q
var dJack(βˆ
1))
D.2 Values of Simulation Parameters
Design Matrix For the design matrix X, we considered the following designs for the distribution of an element Xij of the matrix X, ensuring that the vectors Xi had covariance Idp
in all cases :
• Normal: Xij are i.i.d N(0, 1)
• Double Exp: Xij are i.i.d. double exponential with variance 1.
• Elliptical: Xij ∼ λiZij where the Zij are i.i.d N(0, 1) and the λi are i.i.d according to
– λi ∼ Exp(
√
2) (i.e. mean 1/
√
2), so E

λ
2
i

= 1
– λi ∼ N(0, 1)
– λi ∼ Unif(
q
12
13 0.5,
q
12
13 1.5) so that E

λ
2
i

= 1
Error Distribution We used two different distributions for the i.i.d errors i
: N(0, 1) and
standard double exponential (with variance 2).
Dimensions We simulated from n = 100, 500, and 1, 000 though we showed only n = 500
in our results for simplicity. Except where noted, no significant difference in the results was
seen for varying sample size. The ratio κ was simulated at 0.01, 0.1, 0.3, 0.5.
D.3 Details of Additional Numerics
In Tables A-1 to A-5 in AppendixI we give the precise numerical results from our simulations
that are plotted in both the main text and supplementary figures.
Calculating Correction Factors for Jackknife We computed these quantities using the formula we mentioned in the text and Matlab. We solve the associated regression problems
with cvx (Grant and Boyd, 2014, 2008), running Mosek (ApS, 2015) as our optimization
  
Can We Trust the Bootstrap in High-dimensions?
engine. We used n = 500 and 1, 000 simulations to compute the mean of the quantities we
were interested in.
Relative Risk (Figure 3) In Figure 3 we plot for both Huber loss and L1 loss the average
risk rρ(κ; Gconv) (i.e. errors given by Gconv) relative to the average risk rρ(κ; G) (i.e. errors
distributed according to G), where G has a double exponential distribution. We also plot
the relative average risk rρ(κ; Gnorm), where Gnorm = N(0, σ2

). The values for Figure 3
were generated with Matlab, using cvx and Mosek, as described above. We picked n = 500
and did 500 simulations. p was taken in (5, 10, 30, 50, 75, 100, 125, 150, 175, 200, 225,
250, 275, 300, 350, 400, 450). We used our simulations for the case of the original errors
to estimate E

kβb − βk2

. We used this estimate in our simulation under the convolved
error distribution. The Gaussian error simulations were made with N (0, 2) to match the
variance of the double exponential distribution.
Calculation of amount of variance overestimated in pairs bootstrap (Figure 5a)
In Figure 5a, we plot the theoretical factor by which the pairs bootstrap overestimates
the actual variance of β1ρ. This figure was generated by assuming Poisson(1) weights
and computing deterministically the expectations of interest. This was easy since if W ∼
Poisson(1), P(W = k) = exp(−1)
k!
.
We truncated the expansion of the expectation at K = 100, so we neglected terms
of order 1/100! or lower only. The constant c was found by dichotomous search, with
tolerance 10−6
for matching the equation E (1/(1 + W c)) = 1 − p/n. Once c was found, we
approximated the expectation in Theorem 2 in the same fashion as we just described.
Once we had computed the quantity appearing in Theorem 2, we divided it by κ/(1−κ).
We repeated these computations for κ = .05 to κ = .5 by increments of 10−3
to produce
our figure.
Appendix E. Proof of Theorem 1 (Residual bootstrap, p/n close to 1)
Proof
Recall the system describing the asymptotic limit of kβbρ − βk when p/n → κ and the
design matrix has i.i.d mean 0, variance 1 entries, is, under some conditions on i
’s and
some mild further conditions on the design (see Section A above): kβbρ − βk → rρ(κ) and
the pair of positive and deterministic scalars (c, rρ(κ)) satisfy: if ˆz =  + rρ(κ)Z, where
Z ∼ N (0, 1) is independent of , and  has the same distribution as i
’s:

E ((prox(cρ))0
(ˆz)) = 1 − κ ,
κr2
ρ
(κ) = E

[ˆz − prox(cρ)(ˆz)]2

.
In this system, prox(cρ) refers to Moreau’s proximal mapping of the convex function cρ -
see Moreau (1965) or Hiriart-Urruty and Lemar´echal (2001).
We first give an informal argument to “guess” the correct values of various quantities of
interest, namely c and of course, rρ(κ).
Note that when |x|  c, and when ψ(x) ∼ x at 0, prox(cρ)(x) '
x
1+c
. Hence, x −
prox(cρ)(x) ' xc/(1 + c). (Note that as long as ψ(x) is linear near 0, we can assume
that ψ(x) ∼ x, since the scaling of ρ by a constant does not affect the performance of the
estimators.)
4 
El Karoui and Purdom
We see that 1 − κ ' 1/(1 + c), so that c ' κ/(1 − κ) - assuming for a moment that we
can apply the previous approximations in the system. Hence, we have
κrρ(κ)
2 ' (c/(1 + c))2
[rρ(κ)
2 + σ
2

] ' κ
2
[rρ(κ)
2 + σ
2

] .
We can therefore conclude (informally at this point) that
rρ(κ)
2 ∼
σ
2
 κ
1 − κ
∼
σ
2

1 − κ
.
Once these values are guessed, it is easy to verify that rρ(κ)  c and hence all the
manipulations above are valid if we plug these two expressions in the system driving the
performance of robust regression estimators described above. We note that our argument is
not circular: we just described a way to guess the correct result. Once this has been done,
we have to make a verification argument to show that our guess was correct.
In this particular case, the verification is done as follows: we can rewrite the expectations
as integrals and split the domain of integration into (−∞, −sκ), (−sκ, sκ), (sκ, ∞), with
sκ = (1 − κ)
−3/4
. Using our candidate values for c and rρ(κ), we see that the corresponding zb has extremely low probability of falling outside the interval (−sκ, sκ) - recall that
1−κ → 0. Coarse bounding of the integrands outside this interval shows the corresponding
contributions to the expectations are negligible at the scales we consider. On the interval
(−sκ, sκ), we can on the other hand make the approximations for prox(cρ)(x) we discussed
above and integrate them. That gives us the verification argument we need, after somewhat
tedious but simple technical arguments. (Note that the method of propagation of errors
in analysis described in (Miller, 2006) works essentially in a similar a-posteriori-verification
fashion. Also, sκ could be picked as (1 − κ)
−(1/2+δ)
for any δ ∈ (0, 1/2) and the arguments
would still go through.)
Appendix F. Proof of Theorem 2 (Expected Variance of the Pairs
Bootstrap Estimator)
In this section, we compute the expected variance of the bootstrap estimator.
We recall that for random variables T, Γ, we have
var (T) = var (E (T|Γ)) + E (var (T|Γ)) .
In our case, T = v
0βbw, the projection of the regression estimator βbw obtained using the
random weights w on the contrast vector v. Γ represents both the design matrix and the
errors. We assume without loss of generality that kvk2 = 1.
Hence,
var 
v
0βbw

= var 
v
0E

βbw|Γ
 + E

var 
v
0βbw|Γ
 .
In plain English, the variance of v
0βbw is equal to the variance of the bagged estimator
plus the expectation of the variance of the bootstrap estimator (where we randomly weight
observation (yi
, Xi) with weight wi).
42
Can We Trust the Bootstrap in High-dimensions?
As explained in Section H, we can study without loss of generality the case where Σ = Idp
and β = 0. This is what we do in this proof. Further the rotational invariance arguments
we give in Section H mean that we can focus on the case v = ep,the p-th canonical basis
vector, without loss of generality.
We consider the case where Xi
iidv N (0,Idp). This allows us to work with results in El
Karoui et al. (2011); El Karoui et al. (2013), El Karoui (2013).
Notational simplification To make the notation lighter, in what follows in this proof we use
the notation βb for βbw. There are no ambiguities as we are always using a weighted version
of the estimator and hence this simplification should not create any confusion.
In particular, we have, using the derivation of Equation (9) in El Karoui et al. (2013)
and noting that in the least-squares case all approximations in that paper are actually exact
equalities,
βbp = ˆc
Pn
i=1 wiXi(p)ei,[p]
p
.
ei,[p] here are the residuals based on the first p − 1 predictors, when β = 0. We note
that, under our assumptions on Xi
’s and wi
’s, ˆc =
1
n
trace
S
−1
w

+ oL2
(1), where Sw =
1
n
Pn
i=1 wiXiX0
i
. It is known from work in random matrix theory (see e.g El Karoui (2009))
that 1
n
trace
S
−1
w

is asymptotically deterministic in the situation under investigation with
our assumptions on w and X, i.e 1
n
trace
S
−1
w

= c + oL2
(1), where c = E

1
n
trace
S
−1
w
.
We also recall the residuals representation from El Karoui et al. (2013), which are exact
in the case of least-squares : namely here,
βb − βb
(i) =
wi
n
S
−1
i Xiψ(ei) ,
which implies that, with Si =
1
n
P
j6=i wjXjX0
j
,
e˜i(i) = ei + wi
X0
iS
−1
i Xi
n
ψ(ei) .
In the case of least-squares, ψ(x) = x, so that
ei =
e˜i(i)
1 + wici
,
where
ci =
X0
iS
−1
i Xi
n
.
These equalities also follow from simple linear algebra since we are in the least-squares
case. We note that ci = c + oP (1), where c is deterministic, as explained in e.g El Karoui
(2010), El Karoui (2013). Furthermore, here the approximation holds in L2 because of our
assumptions on w’s and existence of moments for the inverse Wishart distribution - see e.g
Haff (1979). As explained in El Karoui (2013), the same is true for ci,[p] which is the same
quantity computed using the first (p − 1) coordinates of Xi
, vectors we denote generically
by Vi
. We can rewrite
βbp = ˆc
Pn
i=1 wiXi(p)
e˜i(i),[p]
1+wici,[p]
p
     
El Karoui and Purdom
Let us call bb the bagged estimate. We note that ˜ei(i),[p]
is independent of wi and so is ci,[p]
.
We have already seen that ˆc is close to a constant, c. So taking expectation with respect to
the weights, we have, if w(i) denotes {wj}j6=i
, and using independence of the weights,
bbp =
1
p
Xn
i=1
Ewi

cwi
1 + cwi

Xi(p)Ew(i)

e˜i(i),[p]

[1 + oL2
(1)] .
Now the last term is of course the prediction error for the bagged problem, i.e
Ew(i)

e˜i(i),[p]

= i − V
0
i
(gb(i) − γ)
where gb(i)
is the bagged estimate of ˆγ and ˆγ is the regression vector obtained by regressing
yi on the first p − 1 coordinates of Xi
. (Recall that in these theoretical considerations we
are assuming that β = 0, without loss of generality.)
So we have, since we can work in the null case where γ = 0 (without loss of generality),
bbp =
1
p
Xn
i=1
Ewi

cwi
1 + cwi

Xi(p)

i − V
0
i
gb(i)

(1 + oL2
(1)) .
Hence,
E

pbb
2
p

=
1
p
Xn
i=1

Ewi

cwi
1 + cwi
2
(σ
2
 + E

kgb(i)k
2
2

)(1 + o(1)) .
Now, in expectation, using e.g El Karoui (2013), E

kgb(i)k
2
2

(1 + o(1)) = E

kbbk
2
2

=
pE

bb
2
p

. The last equality comes from the fact that all coordinates play a symmetric role
in this problem, so they are all equal in law.
Now, recall that according to e.g El Karoui et al. (2013), top-right equation on p. 14562,
or El Karoui (2010)
1
n
Xn
i=1
1
1 + cwi
= 1 −
p
n
+ oL2
(1) ,
since the previous expression effectively relates trace
DwX(X0DwX)
−1X0

to n − p, the
rank of the corresponding “hat matrix”.
Since cwi
1+cwi
= 1 −
1
1+cwi
, we see that
Ewi

cwi
1 + cwi

=
p
n
+ o(1) .
Hence, for the bagged estimate, we have the equation
E

kbbk
2
2

=
p
n

σ
2 + E

kbbk
2
2
 (1 + o(1)) .
We conclude that
E

kbbk
2
2

= (1 + o(1)) κ
1 − κ
σ
2
.
Note that κ
1−κ
σ
2 = E

kβb
sLSk
2
2

, where the latter is the standard (i.e non-weighted) least
squares estimator       
Can We Trust the Bootstrap in High-dimensions?
We note that the rotational invariance argument given in El Karoui et al. (2011);
El Karoui et al. (2013) still apply here, so that we have the
bb − β
L= kbb − βku ,
where u is uniform on the sphere and independent of kbb − βk (recall that this simply comes
from the fact that if Xi
is changed into OXi
, where O is orthogonal, bb is changed into
Obb - and we then apply invariance arguments coming from rotational invariance of the
distribution of Xi). Therefore,
var 
v
0
(bb − β)

=
kvk
2
p
E

kbb − βk
2
2

.
So we conclude that
pE

var 
v
0βbw|Γ
 = pvar 
v
0βbw

−
κ
1 − κ
σ
2
kvk
2
2 + o(1) .
Now, the quantity var 
v
0βbw

is well understood. The rotational invariance arguments
we mentioned before give that
var 
v
0βbw

=
kvk
2
2
p
E

kβbw − βk
2
2

.
In fact, using the notation Dw for the diagonal matrix with Dw(i, i) = wi
, since
βbw − β = (X0DwX)
−1X0Dw ,
we see that
E

kβbw − βk
2
2

= σ
2
 E

trace
(X0DwX)
−2X0Dw2X
 .
(Note that under mild conditions on , X and w, we also have kβbw−βk
2
2 = E

kβbw − βk
2
2

+
oL2
(1) - owing to concentration results for quadratic forms of vectors with independent
entries; see Ledoux (2001).)
We now need to simplify this quantity.
Analytical simplification of trace
(X0DwX)
−2X0Dw2X

Of course,
trace
(X0DwX)
−2X0Dw2X

= trace
DwX(X0DwX)
−2X0Dw

=
Xn
i=1
w
2
i X0
i
(X0DwX)
−2Xi
.
Hence, if Σbw =
1
n
Pn
i=1 wiXiX0
i ,
wi
n XiX0
i + Σb
(i)
, we have
trace
(X0DwX)
−2X0Dw2X

=
1
n
Xn
i=1
w
2
i
X0
iΣb−2Xi
n
.
Call Σ( b z) = Σb − zIdp. Using the identity
(Σb − zIdp)(Σb − zIdp)
−1 = Idp      
El Karoui and Purdom
we see, after taking traces, that (Silverstein (1995))
1
n
Xn
i=1
wiX0
i
(Σb − zIdp)
−1Xi − ztrace 
(Σb − zIdp)
−1

= p .
We call, for z ∈ C, c(z) = 1
n
trace 
(Σb − zIdp)
−1

and ci(z) = X0
i
(Σb
(i)−zIdp)
−1Xi
, provided
z is not an eigenvalue of Σ. b
Differentiating with respect to z and taking z = 0 (we know here that Σ is non-singular b
with probability 1, so this does not create a problem), we have
1
n
Xn
i=1
wiX0
iΣb−2Xi − trace 
Σb−1

= 0 .
Also, since, by the Sherman-Morrison-Woodbury formula (Horn and Johnson (1990)),
X0
iΣ( b z)
−1Xi =
X0
iΣb
(i)
(z)
−1Xi
1 + wi
1
nX0
iΣb
(i)
(z)−1Xi
,
we have, after differentiating,
1
n
X0
iΣb−2Xi =
c
0
i
(0)
[1 + wici(0)]2
,
where of course c
0
i
(0) = X0
iΣb−2
(i) Xi
. Hence,
1
n
Xn
i=1
w
2
i
1
n
X0
iΣb−2Xi =
1
n
Xn
i=1
w
2
i
c
0
i
(0)
[1 + wici(0)]2
= c
0
(0) 1
n
Xn
i=1
w
2
i
[1 + wic(0)]2
.
(Note that the arguments given in e.g El Karoui (2010) or El Karoui and Koesters (2011)
for why ci(z) = c(z)(1 + oP (1)) extend easily to c
0
i
and c
0 given our assumptions on w’s
and the fact that these functions have simple interpretations in terms of traces of powers
of inverses of certain well-behaved - under our assumptions - matrices.)
Going back to
1
n
Xn
i=1
wiX0
i
(Σb − zIdp)
−1Xi − ztrace 
(Σb − zIdp)
−1

= p ,
and using the previously discussed identity
wi
n
X0
i
(Σb − zIdp)
−1Xi = 1 −
1
1 + wici(z)
,
we have
n −
Xn
i=1
1
1 + wici(z)
− znc(z) = p .
46
Can We Trust the Bootstrap in High-dimensions?
In other words,
1 − κ =
1
n
Xn
i=1
1
1 + wici(z)
+ zc(z) .
Now,
c(z)
1
n
Xn
i=1
wi
1 + wic(z)
=
1
n
Xn
i=1
(1 −
1
1 + wic(z)
)
=
1
n
Xn
i=1
(1 −
1
1 + wici(z)
) + η(z)
= κ + zc(z) + η(z) ,
where η(z) is such that η(z) = oP (1) and η
0
(z) = oP (1) (η has an explicit expression which
allows us to verify these claims). Therefore, by differentiation, and after simplifications,
1
n
X
wi
1 + wic(0)2
c
0
(0) = κ
c
0
(0)
[c(0)]2
− 1 + oP (1) .
Hence,
trace
(X0DwX)
−2X0Dw2X

=

κ
trace 
Σb−2
w

/n
[trace 
Σb−1
w

/n]
2
− 1

 + oP (1) .
The fact that we can take expectations on both sides of this equation and that oP (1) is
in fact oL2
(1) come from our assumptions about wi
’s - especially the fact that they are
independent and bounded away from 0 - and properties of the inverse Wishart distribution.
Conclusion We can now conclude that a consistent estimator of the expected variance
of the bootstrap estimator is
kvk
2
2
p
σ
2


κ
trace 
Σb−2
w

/n
[trace 
Σb−1
w

/n]
2
−
1
1 − κ

 .
Using the fact that
1 − κ =
1
n
Xn
i=1
1
1 + wic(z)
+ zc(z) ,
we see that, since 1
n
trace 
Σb−2
w

= c
0
(0),
1
n
trace 
Σb−2
w

=
c(0)
1
n
Pn
i=1 wi/(1 + wic(0))2
.
We further note that asymptotically, when wi are i.i.d and satisfy our assumptions,
c(0) → c, which solves:
Ewi

1
1 + wic

= 1 − κ .
4 
El Karoui and Purdom
Hence, asymptotically, when wi
’s are i.i.d and satisfy our assumptions, we have
trace 
Σb−2
w

/n
[trace 
Σb−1
w

/n]
2
→
1
cEwi
[wi/(1 + wic)
2]
.
Since cwi/(1 + cw2
i
) = 1/(1 + cwi) − 1/(1 + cwi)
2
, we finally see that
cEwi

wi
(1 + wic)
2

= Ewi

1
1 + cwi

− Ewi

1
(1 + cwi)
2

,
= 1 − κ − Ewi

1
(1 + cwi)
2

.
So asymptotically, the expected bootstrap variance is equivalent to, when kvk2 = 1,
σ
2

p

κ
1
1 − κ − E

1
(1+cwi)
2
 −
1
1 − κ

 ,
where E

1
1+cwi

= 1 − κ.
In particular, when wi = 1, we see, unsurprisingly, that the above quantity is 0, as it
should, given that the bootstrapped estimate does not change when resampling.
We finally make note of a technical point, that is addressed in papers such as El Karoui
(2010, 2013) and on which we rely here by using those papers. Essentially, theoretical
considerations regarding quantities such as 1
p
trace 
Σb−k
w

are easier to handle by working
rather with 1
p
trace 
(Σbw + τ Idp)
−k

, for some τ > 0. In the present context, it is easy to
show (and done in those papers) that this approximation allows us to take the limit - even
in expectation - for τ → 0 in all the expressions we get for τ > 0 and that that limit is
indeed E

1
p
trace 
Σb−k
w
. Technical details rely on using the first resolvent identity (Kato,
1995), using moment properties of inverse Wishart distributions and using the fact that
wi
’s are bounded below.
F.1 Extension: Elliptical Design
In this case, we have X˜
i = λiXi
, where Xi ∼ N (0,Idp) and yi = i + X˜0
iβ. We assume
λi 6= 0 for all i, E

λ
2
i

= 1, λi
’s are i.i.d and bounded away from 0.
We can go through the proof of Theorem 2 and make necessary adjustments.
Of course, we have
E

kβbw − βk
2
2

= σ
2
 E

trace 
(X˜0DwX˜)
−2X˜0Dw2X˜
 .
If we reformulate this expression in terms of X we get
E

kβbw − βk
2
2

= σ
2
 E

trace
(X0Dλ2wX)
−2X0Dλ2w2X
 .   
Can We Trust the Bootstrap in High-dimensions?
So this quantity is affected by the distribution of λi
’s; hence the risk of βbw is different in
the Gaussian and elliptical design case.
The other important part of the proof is the computation of the risk of the bagged
estimator. In this case, earlier work in random matrix theory (e.g El Karoui (2009); El
Karoui and Koesters (2011)) shows that we can use the approximations
ci ' λ
2
i
c ,
where c = limn,p→∞ E

1
n
trace
S
−1
, where S =
1
n
Pn
i=1 λ
2
i wiXiX0
i
, i.e S =
1
nX0Dλ2wX.
If we call
g(λ
2
i
) = Ewi

cλ2
i
1 + cλ2
i wi

,
we see by keeping track of changes in the earlier proof that we have asymptotically
E

k
ˆbk
2
2

=
Eλi
(λ
2
i
g
2
(λ
2
i
))
κ − Eλi
(λ
4
i
g
2(λ
2
i
))σ
2

.
The same arguments we used before give that
E

g(λ
2
i
)

= κ .
Based on this information, we can compute E

var 
v
0βb∗
w
 as we had in the proof of
Theorem 2 and compare it to var 
v
0βb

. The expressions do not seem to simplify much
further however in this case, by contrast to the Gaussian design case where λi = 1 for all i.
(For instance, when λi=1 for all i’s, g(λi) = g(1) = κ and we recover the results of Theorem
2.)
Importantly, the characteristics of the distribution of λi that affect E

var 
v
0βb∗
w
 go
beyond E

λ
2
i

. And hence the expression we gave in Theorem 2 won’t apply directly to
the elliptical case.
F.2 Extension: Multinomial(n, 1/n) weights
A natural question is whether the computations we have made can be extended to wi
’s that
are i.i.d P oisson(1) and/or Multinomial(n, 1/n), as in the standard bootstrap.
In both cases, technical issues arise because with asymptotically negligible but non-zero
probability, the matrix X0DwX may be of rank less than p. This can handled in several
ways. A simple one is to replace the weights wi by wi(τ ) = τ + (1 − τ )wi and study the
problem when τ → 0.
Beyond that technicality, an important question is whether one can handle the fact that
the weights are dependent in the multinomial case. For quantities of the type 1
n
trace
(X0DwX)
−1

,
it was argued in El Karoui (2010) that one could ignore the dependency issue and treat
the problem as if the weights where i.i.d Po(1). This type of arguments would be easy to
extend where we need them here, for instance in quantities that arise in the computation
of E

kβbw − βk
2
2

or to show that we can write ci = c + oP (1), where c is deterministic     
El Karoui and Purdom
The remaining question is therefore the characterization of the risk of the bagged estimator. We have, with a slight modification with respect to the case of independent weights,
ˆbp =
1
p
Xn
i=1
Xi(p)

Ewi

cwi
1 + cwi

i − V
0
i Ew

γˆ(i)
cwi
1 + cwi
 (1 + oP (1)) .
As before, Ewi

cwi
1+cwi

= p/n + o(1). The problem is the dependence between ˆγ(i) and
wi
. The rotational invariance arguments we invoked before still hold, so that ˆγi = kγˆ(i)k2u,
where u is uniform on the unit sphere and independent of kγˆik2. It is also independent of
Vi
, since ˆγ(i)
is the leave-one-out estimate of γ. The same rotational invariance arguments
hold for the bagged estimate Ewγˆ(i)
cwi
1+cwi
. Hence, after a little bit of work we see that
E

[V
0
i Ew

γˆ(i)
cwi
1 + cwi

]
2

= E

kEw

γˆ(i)
cwi
1 + cwi

k
2
2

.
Using the fact that w(i)
|wi ∼ Multinomial(n−wi
, 1/(n−1)), the only real technical hurdle is
to show that Ewi
γˆ(i)
is asymptotically deterministic and independent of wi
. A strategy for
this is to create a coupling: one can compare ˆγ(i)
to gˆ(i)
, where gˆ(i)
is computed using a n−1
dimensional vector of weights with distribution Multinomial(n − 1, 1/(n − 1)) - i.e running
wi − 1 multinomial trials after having obtained w(i)
(the case wi = 0 is easy to handle
separately). Clearly, the distribution of gˆ(i)
is independent of wi
, by construction. On the
other hand, a bit of work on top of the leave-one-observation-out expansions shows that
kgˆ(i) −γˆ(i)k
2
2
is roughly of size at most w
2
i
/n → 0. Furthermore, kEw(gˆ(i)
)−Ew(ˆγ(i)
)k2 → 0
for the same reason. This suggests that further technical work along those lines will give
that
Ew

γˆ(i)
cwi
1 + cwi

' Ew

γˆ(i)

Ew

cwi
1 + cwi

,
where ' means that the approximation is valid in Euclidean norm. The same coupling
arguments will give that
kEw

γˆ(i)

k ' kˆbk ,
where ˆb is the bagged estimator. This will yield the same results as in the i.i.d Po(1) case.
Numerical results We verified that our theoretical results (i.e Theorem 2) hold for Poisson(1)
weights in limited simulations (note that in this case wi = 0 is possible). For Gaussian design
matrix, double exponential errors, and ratios κ = .1, .3, .5 we found that the ratio of the
observed bootstrap expected variance of βb∗
1
to our theoretical prediction using Poisson(1)
weights was 1.0027, 1.0148, and 1.0252, respectively (here n = 500, and there were R = 1000
bootstrap resamples for each of 1000 simulations).
Appendix G. Proof of Theorem 3 (Jackknife Variance)
As explained in Section H, we can study without loss of generality the case where Σ = Idp
and β = 0. This is what we do in this proof.
We study it in details in the least-squares case, and postpone a detailed analysis of the
robust regression case to future studies.
  
Can We Trust the Bootstrap in High-dimensions?
According to the approximations in El Karoui et al. (2013), which are exact for least
squares, or classic results Weisberg (2014) we have:
βb − βb
(i) =
1
n
Σb−1
(i) Xiei
.
Recall also that
ei =
e˜i(i)
1 + 1
nX0
iΣb−1
(i) Xi
.
Hence,
v
0
(βb − βb
(i)
) = 1
n
v
0Σb−1
(i) Xi
e˜i(i)
1 + 1
nX0
iΣb−1
(i) Xi
.
Hence,
n
Xn
i=1
[v
0
(βb − βb
(i)
)]2 =
1
n
Xn
i=1
[v
0Σb−1
(i) Xie˜i(i)
]
2
[1 + 1
nX0
iΣb−1
(i) Xi
]
2
.
Note that at the denominator, we have
1 +
1
n
X0
iΣb−1
(i) Xi = 1 +
1
n
trace 
Σb−1

+ oP (1) ,
= 1 +
p
n
1
1 − p/n + oP (1) = 1
1 − p/n + oP (1) .
by appealing to standard results about concentration of high-dimensional Gaussian random
variables, and standard results in random matrix theory and classical multivariate statistics
(see Mardia et al. (1979); Haff (1979)). By the same arguments, this approximation works
not only for each i but for all 1 ≤ i ≤ n at once. The approximation is also valid in
expectation, using results concerning Wishart matrices found for instance in Mardia et al.
(1979).
For the numerator, we see that
Ti = v
0Σb−1
(i) Xie˜i(i) = v
0Σb−1
(i) Xi(i − X0
i
(βb
(i) − β)) .
Since i
is independent of Xi and Σb
(i)
, we see that
E

T
2
i

= E


2
i

E

(v
0Σb−1
(i) Xi)
2

+ E

[X0
i
(βb
(i) − β)]2
[v
0Σb−1
(i) Xi
]
2

.
If α and β are fixed vectors, α
0Xi and β
0Xi are Gaussian random variables with covariance
α
0β, since we are working under the assumption that Xi ∼ N (0,Idp). It is easy to check
that if Z1 and Z2 are two Gaussian random variables with covariance γ and respective
variances σ
2
1
and σ
2
2
, we have
E

(Z1Z2)
2

= σ
2
1σ
2
2 + 2γ
2
.
We conclude that
E

(a
0Xi)
2
(b
0Xi)
2

= kak
2
2kbk
2
2 + 2(a
0
b)
2
.    
El Karoui and Purdom
We note that
E

[v
0Σb−1
(i) Xi
]
2

= E

v
0Σb−2
(i)
v

.
Classic Wishart computations give (Haff (1979), p.536 (iii)) that as n, p → ∞,
E

Σb−2
(i)

=

1
(1 − p/n)
3
+ o(1)
Idp .
Hence, in our asymptotics,
E

(v
0Σb−1
(i) Xi)
2

→
1
(1 − p/n)
3
kvk
2
2
.
We also note that
E
h
(v
0Σb−1
(i)
βb
(i)
)
2
i
=
1
n
v
0Σb−3
(i)
v .
Hence,
E

(v
0Σb−1
(i)
βb
(i)
)
2

= o(1)in our asymptotics .
Therefore,
E

T
2
1

=
1
(1 − p/n)
3
kvk
2
2σ
2

(1 + p/n
1 − p/n) + o(1)
since E

kβb
(i) − βk
2
2

= σ
2

p/n
1−p/n + o(1).
When v = e1, we therefore have
E

T
2
1

= σ
2

1
(1 − p/n)
4
+ o(1) .
Therefore, in that situation,
E

n
Xn
i=1
(v
0
(βb
(i) − βb)
2
)
!
= σ
2

1
(1 − p/n)
2
+ o(1) .
In other words,
E
 Xn
i=1
(v
0
(βb
(i) − βb)
2
)
!
=

1
1 − p/n + o(1)
var 
βb1

G.1 Dealing with Centering
Let us call βb
(·) =
1
n
Pn
i=1 βb
(i)
. We have previously studied the properties of Pn
i=1([v
0
(βb −
βb
(i)
)]2
) and now need to show that the same results apply to Pn
i=1([v
0
(βb
(·) − βb
(i)
)]2
).
To show that replacing βb by βb
(·) does not affect the result, we consider the quantity
n
2
[v
0
(βb − βb
(·)
)]2
.
Since βb − βb
(i) =
1
nΣb−1
(i) Xiei
, we have
βb − βb
(·) =
1
n2
Xn
i=1
Σb−1
(i) Xiei
.
  
Can We Trust the Bootstrap in High-dimensions?
Hence,
n
2
[v
0
(βb − βb
(·)
)]2 =
"
1
n
Xn
i=1
v
0Σb−1
(i) Xi(i − X0
i
(βb − β))#2
.
A simple variance computation gives that 1
n
Pn
i=1 v
0Σb−1
(i) Xii → 0 in L
2
, since each term has
mean 0 and the variance of the sum goes to 0.
Recall now that
Σb−1Xi =
Σb−1
(i) Xi
1 + ci
,
where all ci
’s are equal to p/n/(1 − p/n) + oP (1). Let us call c = p/n/(1 − p/n).
We conclude that
1
n
Xn
i=1
v
0Σb−1
(i) XiX0
i
(βb − β) = v
0
(βb − β)(1 + c + o(1)) .
When v is given, we clearly have v
0
(βb − β) = oP (p
−1/2
), given the distribution of βb − β
under our assumptions on Xi
’s and i
’s. So we conclude that
n
2
[v
0
(βb − βb
(·)
)]2 → 0 in probability .
Because we have enough moments, the previous result is also true in expectation.
G.2 Putting Everything Together
The jackknife estimate of variance of v
0βb is up to a factor going to 1
n
n − 1
JACK(var 
v
0βb

) = Xn
i=1
[(v
0βb
(i) − βb
(·)
)]2
=
Xn
i=1
[(v
0βb
(i) − βb)]2 + n[v
0
(βb − βb
(·)
)]2
.
Our previous analyses therefore imply (using v = e1) that
n
n − 1
E

JACK(var 
βb1

)

=

1
1 − p/n + o(1)
var 
βb1

.
This completes the proof of Theorem 3
G.3 Extension: More Involved Designs and Different Loss Functions
Our approach could be used to analyze similar problems in the case of elliptical designs.
However, in that case, it seems that the factor that will appear in quantifying the amount
by which the variance is mis-estimated will depend in general on the ellipticity parameters.
We refer to El Karoui (2013) for computations of quantities such as v
0Σb−2v in that case,
which are of course essential to measuring mis-estimation.
We obtained the possible correction we mentioned in the paper for these more general
settings following the ideas used in the rigorous proof we just gave, as well as approximation
53
El Karoui and Purdom
arguments given in El Karoui et al. (2013) and justified rigorously in El Karoui (2013).
Checking fully rigorously all the approximations we made in this Jackknife computation
would require a very large amount of technical work, and since this is tangential to our
main interests in this paper, we postpone that to a future work of a more technical nature.
It is also clear, since all these results and the proof we just gave rely on random matrix
techniques, that a similar analysis could be carried out in the case where Xi,j are i.i.d
with a non-Gaussian distribution, provided that distribution has enough moments (see e.g
Pajor and Pastur (2009) or El Karoui and Koesters (2011) for examples of such techniques,
actually going beyond the case of i.i.d entries for the design matrix). The main issues in
carrying out this program seem to be technical and not conceptual at this point, so we leave
this problem to possible future work.
Appendix H. Going from Σ = Idp to Σ 6= Idp
As discussed in Section A, we have
βbρ(yi
; Xi
; i) − β = Σ−1/2βbρ(i
; Σ−1/2Xi
; i) ,
In other words, βbρ(˜yi
; Σ−1/2Xi
; i) is the robust regression estimator in the null case where
β = 0 and Xi
is replaced by X˜
i = Σ−1/2Xi
. Of course, if cov (Xi) = Σ, cov 
X˜
i

= Idp.
H.1 Consequences for the Jackknife
Naturally the same equality applies to leave-one-out estimators. So, with the notations of
Equation (7) in the main text, we have, when span({Xi}
n
i=1) = R
p and Σ is positive definite,
(v
0
[βb
(i) − β˜])2 = (v
0Σ
−1/2
[βb
(i)
(i
; Σ−1/2Xi
; i) − β˜(i
; Σ−1/2Xi
; i)])2
.
Let us call βbρ(Σ; β) our robust regression estimator when cov (Xi) = Σ and E (yi
|Xi) = X0
iβ.
It is clear from the previous display that the properties of varJACK(v
0βbρ(Σ; β)) are the
same as those of varJACK(v
0Σ
−1/2βbρ(Idp; 0)). So understanding the null case is enough to
understand the general case, which is why we focus on the null case in our computations.
Furthermore, by the same arguments, we have
var 
v
0βbρ(yi
; Xi
; i)

= var 
v
0Σ
−1/2βbρ(Idp; 0)
.
So we have
varJACK(v
0βbρ(Σ; β))
var 
v
0βbρ(Σ; β)
 =
varJACK(v
0Σ
−1/2βbρ(Idp; 0))
var 
v
0Σ−1/2βbρ(Idp; 0) .
Calling u1 = Σ−1/2v/kΣ
−1/2vk, we see that u1 is a unit vector. And we finally see that
varJACK(v
0βbρ(Σ; β))
var 
v
0βbρ(Σ; β)
 =
varJACK(u
0
1βbρ(Idp; 0))
var 
u
0
1
βbρ(Idp; 0) .
54
Can We Trust the Bootstrap in High-dimensions?
Hence, characterizing varJACK(v
0βbρ(Idp;0))
var(v
0βbρ(Idp;0))
for all fixed unit vectors v characterizes
varJACK(v
0βbρ(Σ; β))
var 
v
0βbρ(Σ; β)

for all β and invertible Σ. This is why our proof is focused on the null case Σ = Idp and
β = 0.
H.2 Consequences for the Pairs Bootstrap
Let us call Dw the diagonal matrix with (i, i)-entry D(i, i) = wi
. We consider only the case
where wi > 0, so we do not have to consider the case where fewer than p Xi
’s are assigned
positive weights - which would result in βbρ being ill-defined (since infinitely many solutions
would then be feasible).
In particular, for least squares, we have in our setting
βbw = (X0DwX)
−1X0DwY = β + (X0DwX)
−1X0Dw .
More generally, by a simple change of variables, since wi > 0 and span({Xi}
n
i=1) = R
p
,
when Σ is invertible,
βbw,ρ(yi
; {Xi}
n
i=1; i) − β = Σ−1/2βbw,ρ(i
; Σ−1/2Xi
; i) .
If bρ is the corresponding bagged estimate, obtained by averaging βbw,ρ over w’s, we also
have
bρ(yi
; {Xi}
n
i=1; i) − β = Σ−1/2
bρ(i
; Σ−1/2Xi
; i) .
Hence, we also have
βbw,ρ(yi
; {Xi}
n
i=1; i) − bρ(yi
; {Xi}
n
i=1; i) = Σ−1/2
h
βbw,ρ(i
; Σ−1/2Xi
; i) − bρ(i
; Σ−1/2Xi
; i)
i
We further note that since yi = i + X0
iβ, yi = i + (Σ−1/2Xi)
0Σ
1/2β and hence
βbw,ρ(yi
; Σ−1/2Xi
; i) = Σ1/2β + βbw,ρ(i
; Σ−1/2Xi
; i) .
The previous equation clearly implies that, if v is a fixed vector and u1 = Σ−1/2v
v
0
(βb∗
ρ
(yi
; {Xi}
n
i=1; β) − bρ(yi
; {Xi}
n
i=1; β))
= u
0
1
h
βb∗
ρ
(yi
; Σ−1/2Xi
; i) − bρ(yi
; Σ−1/2Xi
; i)
i
,
= u
0
1
h
βb∗
ρ
(i
; Σ−1/2Xi
; i) − bρ(i
; Σ−1/2Xi
; i)
i
.
We note that if cov (Xi) = Σ, the last line in the previous display corresponds to the
bootstrap distribution of our estimator in the null case where β = 0 and Σ = Idp, but v has
been replaced by u1 = Σ−1/2v. This shows that understanding the bootstrap properties of
v
0
(βb∗
ρ − bρ) in the null case cov (Xi) = Σ and β = 0 gives the result we seek in the general
case of Σ 6= Idp and β 6= 0. (Here we centered our estimator around the bagged estimator,
55
El Karoui and Purdom
because it is natural when computing bootstrap variances. The arguments above show that
many other centering choices are possible, however.)
The last small issue that one needs to handle is the fact that our computations are done
for v with unit norm and u1 may not have unit norm. This is easily handled by simply
scaling by the deterministic ku1k. In particular, it is easy to see through simple scaling
arguments that
E

var 
v
0βb∗
ρ
(Σ; β)

var 
v
0βbρ(Σ; β)
 =
E

var 
u˜1
0βb∗
ρ
(Idp; 0)
var 
u˜
0
1
βbρ(Idp; 0) ,
where ˜u1 = u1/ku1k has unit norm.
H.3 Rotational Invariance Arguments and Consequences
Motivated by the arguments in the previous two subsections, we now consider the null case
where β = 0 and cov (Xi) = Idp. Note that then yi = i
. Also, if Xi
is replaced by OXi
,
where O is an orthogonal matrix, and βb is replaced by Oβb. In other words,
βbρ(i
; {OXi}
n
i=1; i) = Oβbρ(i
; {Xi}
n
i=1; i) , .
Note that when the design matrix is such that OXi
L= Xi for all i (i.e the distribution of
Xi
’s is invariant by rotation),
βbρ(i
; {OXi}
n
i=1; i)
L= βbρ(i
; {Xi}
n
i=1; i) .
When wi > 0 for all i, we see that exactly the same arguments apply to βbw,ρ(i
; {Xi}
n
i=1; i)
and hence βb∗
ρ
(i
; {Xi}
n
i=1; i). In particular, for any orthogonal matrix O, since Xi
L= OXi
,
E

var 
v
0βb∗
ρ
(i
; {Xi}
n
i=1; i)
 = E

var 
v
0βb∗
ρ
(i
; {OXi}
n
i=1; i)

= E

var 
v
0Oβb∗
ρ
(i
; {Xi}
n
i=1; i)
 .
This implies that for any unit vector v, we have, if e1 is the first canonical basis vector,
E

var 
v
0βb∗
ρ
(i
; Xi
; i)
 = E

var 
e
0
1βb∗
ρ
(i
; Xi
; i)
 .
Indeed, we just need to take O to be such that O0v = e1 to prove the above result.
In the case where Xi
’s are i.i.d N (0,Idp), we do have Xi
L= OXi
, so the arguments above
apply. Therefore, to understand E

var 
v
0βb∗
ρ
 in this case it is sufficient to understand
E

var 
e
0
1βb∗
ρ
. This latter case is the case tackled in the proof of Theorem 2. (These
rotational invariance arguments are closely related to those in El Karoui et al. (2013).)
56
Can We Trust the Bootstrap in High-dimensions?
Appendix I. Supplementary Tables & Figures
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
(a) L1 loss
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
●
●
●
●
(b) Huber loss
● ●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
●
●
●
●
●
●
Residual
Jackknife
Pairs
Std. Residuals
(c) L2 loss
Figure A-1: Performance of 95% confidence intervals of β1 (double exponential
error): Here we show the coverage error rates for 95% confidence intervals
for n = 500 with the error distribution being double exponential (with σ
2 = 2)
and i.i.d. normal entries of X. See the caption of Figure 1 for more details.
57
El Karoui and Purdom
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
●
●
●
●
●
●
Residual
Jackknife
Pairs
Std. Residuals
(a) Normal X
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
● ● ●
●
(b) Ellip. X, Unif
●
●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
●
●
●
●
(c) Ellip. X, N(0, 1)
● ●
●
●
0.00 0.05 0.10 0.15 0.20
Ratio (κ)
95% CI Error Rate
0.01 0.30 0.50
● ●
●
●
(d) Ellip. X, Exp
Figure A-2: Performance of 95% confidence intervals of β1 for L2 loss (elliptical design X): Here we show the coverage error rates for 95% confidence intervals for n = 500 with different distributions of the design matrix X using ordinary least squares regression: (a) N(0, 1), (b) elliptical with
λi ∼ U(
q
12
13 0.5,
q
12
13 1.5), (c) elliptical with λi ∼ N(0, 1), and (d) elliptical
with λi ∼ Exp(
√
2). In all of these plots, the error is distributed N(0, 1) and
the loss is L2. See the caption of Figure 1 for additional details.
58
Can We Trust the Bootstrap in High-dimensions?
Residual Jackknife Pairs
κ =0.01 0.063 0.089 0.035
κ =0.1 0.113 0.005 0.013
κ =0.3 0.137 0.000 0.003
κ =0.5 0.210 0.000 0.000
(a) L1 loss
Residual Jackknife Pairs
κ =0.01 0.057 0.054 0.054
κ =0.1 0.068 0.037 0.041
κ =0.3 0.090 0.015 0.004
κ =0.5 0.198 0.002 0.000
(b) Huber loss
Residual Jackknife Pairs
κ =0.01 0.040 0.061 0.040
κ =0.1 0.060 0.034 0.052
κ =0.3 0.098 0.021 0.033
κ =0.5 0.188 0.005 0.000
(c) L2 loss
Table A-1: Error rate of 95% confidence intervals of β1 for n = 500 This table gives
the exact error rates plotted in Figure 1. κ = p/n indicates the ratio of p/n
used in the simultation for this and future tables. See Figure 1’s caption for
more details.
Normal Ellip. Normal Ellip. Exp
κ =0.01 1.001 1.001 1.017
κ =0.1 1.016 1.090 1.156
κ =0.3 1.153 1.502 1.655
κ =0.5 1.737 3.123 3.635
Table A-2: Ratio of CI Width of Pairs compared to Standard. This table gives the
ratio of the average width of the confidence intervals from pairs bootstrapping
to the average for the standard interval given by theoretical results, i.e. using
var(βˆ) = σ
2
(X0X)
−1 and creating standard confidence interval. These values
were used for Figure 4 in the text.
59
El Karoui and Purdom
Residual Std. Pred Error Deconv
κ =0.01 0.064 0.042 0.031
κ =0.1 0.091 0.028 0.018
κ =0.3 0.135 0.026 0.022
κ =0.5 0.182 0.030 0.035
(a) L1 loss
Residual Std. Pred Error Deconv
κ =0.01 0.065 0.048 0.036
κ =0.1 0.051 0.054 0.039
κ =0.3 0.098 0.035 0.037
κ =0.5 0.174 0.034 0.036
(b) Huber loss
Table A-3: Error rate of 95% confidence intervals using predicted errors. This
table gives the exact error rates plotted in Figure 2. See figure caption for more
details.
Residual Jackknife Pairs
κ =0.01 0.064 0.073 0.032
κ =0.1 0.091 0.002 0.005
κ =0.3 0.135 0.001 0.001
κ =0.5 0.182 0.000
(a) L1 loss
Residual Jackknife Pairs
κ =0.01 0.065 0.061 0.059
κ =0.1 0.051 0.042 0.027
κ =0.3 0.098 0.009 0.009
κ =0.5 0.174 0.001 0.000
(b) Huber loss
Residual Jackknife Pairs
κ =0.01 0.052 0.052 0.052
κ =0.1 0.056 0.036 0.045
κ =0.3 0.114 0.018 0.022
κ =0.5 0.155 0.008 0.002
(c) L2 loss
Table A-4: Error rate of 95% confidence intervals of β1 for double exponential
error This table gives the exact error rates plotted in Figure A-1. See figure
caption for more details.
60
Can We Trust the Bootstrap in High-dimensions?
Residual Jackknife Pairs
κ =0.01 0.057 0.052 0.053
κ =0.1 0.071 0.047 0.056
κ =0.3 0.118 0.017 0.018
κ =0.5 0.171 0.007 0.001
(a) Ellipical, Unif
Residual Jackknife Pairs
κ =0.01 0.041 0.046 0.047
κ =0.1 0.061 0.034 0.036
κ =0.3 0.098 0.005 0.006
κ =0.5 0.177 0.002 0.000
(b) Elliptical, Normal
Residual Jackknife Pairs
κ =0.01 0.059 0.041 0.060
κ =0.1 0.063 0.011 0.025
κ =0.3 0.115 0.005 0.002
κ =0.5 0.157 0.000 0.000
(c) Elliptical, Exp
Table A-5: Error rate of 95% confidence intervals of β1 for elliptical design X This
table gives the exact error rates plotted in Figure A-2. See figure caption for
more details.
L2 Huber L1
κ =0.01 0.964 0.991 2.060
κ =0.1 1.115 1.173 5.432
κ =0.3 1.411 1.613 10.862
κ =0.5 1.986 2.671 14.045
(a) Jackknife
L2 Huber L1
κ =0.01 1.078 0.923 1.081
κ =0.1 1.041 1.098 1.351
κ =0.3 1.333 1.954 2.001
κ =0.5 2.808 4.507 3.156
(b) Pairs Bootstrap
Table A-6: Over estimation of variance for Pairs bootstrap and Jackknife This
table gives the median values of the boxplots plotted in Figures 5 and 6. See
relevant figure captions for more details.