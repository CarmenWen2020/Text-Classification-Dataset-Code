We consider the performance of the bootstrap in high-dimensions for the setting of linear
regression, where p < n but p/n is not close to zero. We consider ordinary least-squares as
well as robust regression methods and adopt a minimalist performance requirement: can
the bootstrap give us good confidence intervals for a single coordinate of Î² (where Î² is the
true regression vector)?
We show through a mix of numerical and theoretical work that the bootstrap is fraught
with problems. Both of the most commonly used methods of bootstrapping for regressionâ€”
residual bootstrap and pairs bootstrapâ€”give very poor inference on Î² as the ratio p/n
grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of
power) as the ratio p/n grows. We also show that the jackknife resampling technique for
estimating the variance of Î²Ë† severely overestimates the variance in high dimensions.
We contribute alternative procedures based on our theoretical results that result in
dimensionality adaptive and robust bootstrap methods.
Keywords: Bootstrap, high-dimensional inference, random matrices, resampling
1. Introduction
The bootstrap (Efron, 1979) is a ubiquitous tool in applied statistics, allowing for inference when very little is known about the properties of the data-generating distribution.
The bootstrap is a powerful tool in applied settings because it does not make the strong
assumptions common to classical statistical theory regarding this data-generating distribution. Instead, the bootstrap resamples the observed data to create an estimate, FË†, of
the unknown data-generating distribution, F. The distribution FË† then forms the basis of
further inference.
Since its introduction, a large amount of research has explored the theoretical properties
of the bootstrap, improvements for estimating F under different scenarios, and how to most
effectively estimate different quantities from FË† (see the pioneering Bickel and Freedman,
1981 for instance and many many more references in the book-length review of Davison and
Hinkley, 1997, as well as van der Vaart, 1998 for a short summary of the modern point of
view on these questions). Other resampling techniques exist of course, such as subsampling,
m-out-of-n bootstrap, and jackknifing, all of which have been studied and much discussed
(see Efron, 1982; Hall, 1992; Politis et al., 1999; Bickel et al., 1997; and Efron and Tibshirani,
1993 for a practical introduction).
An important limitation for the bootstrap is the quality of FË†. The standard bootstrap
estimate of F based on the empirical distribution of the data may be a poor estimate when
the data has a non-trivial dependency structure, when the quantity being estimated, such
as a quantile, is sensitive to the discreteness of FË†, or when the functionals of interest are
not smooth (see e.g., Bickel and Freedman, 1981 for a classic reference, as well as Beran
and Srivastava, 1985 or Eaton and Tyler, 1991 in the context of multivariate statistics).
An area that has received less attention is the performance of the bootstrap in high
dimensions and this is the focus of our work. In particular, we consider the setting of
standard linear models where data yi are drawn from the linear model
âˆ€i, yi = Î²
0Xi + i
, 1 â‰¤ i â‰¤ n , where Xi âˆˆ R
p
.
We are interested in the bootstrap or resampling properties of the estimator defined as
Î²bÏ = argminbâˆˆRp
Xn
i=1
Ï(yi âˆ’ X0
i
b) , where Ï is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The first is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs (yi
, Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y
âˆ—
i = Î²b0
ÏXi + Ë†
âˆ—
i
, where Ë†
âˆ—
i
is drawn from
the empirical distribution of the estimated residuals, ei
. We also consider the jackknife,
a resampling method focused specifically on estimating the variance of functionals of Î²bÏ.
These three methods are extremely flexible for linear models regardless of the method of
fitting Î² or the error distribution of the i
.
The high dimensional setting: p/n â†’ Îº âˆˆ (0, 1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the
number of observations, n, formalized mathematically by assuming that p/n â†’ Îº âˆˆ (0, 1).
Several reasons motivate our theoretical study in this regime. The asymptotic behavior of
the estimate Î²bÏ is known to depend heavily on whether one makes the classical theoretical
assumption that p/n â†’ 0 or instead assumes p/n â†’ Îº âˆˆ (0, 1) (see Section 1.2 and AppendixA and references therein). But from the standpoint of practical usage on moderatesized data sets (i.e., n and p both moderately sized with p < n), it is not always obvious
which assumption is justified. Working in the high-dimensional regime of p/n â†’ Îº âˆˆ (0, 1)
captures better the complexity encountered even in reasonably low-dimensional practice
than using the classical assumption p/n â†’ 0. In fact, asymptotic predictions based on the
2
Can We Trust the Bootstrap in High-dimensions?
high-dimensional assumption can work surprisingly well in very low-dimension (see Johnstone, 2001). Furthermore, in these high-dimensional settings, where much is still unknown
theoretically, the bootstrap is a natural and compelling alternative to asymptotic analysis.
Another motivation for our investigation is that of very large scale applications (Chapelle
et al., 2014; Criteo, 2017; Langford et al., 2007), where one might resort to subsampling
methods or recent variants like the bag-of-little-bootstraps (Kleiner et al., 2014) for uncertainty assessment. Subsampling is also very commonly used in this setting for simple
computational speed-up. In such cases, even if one had started with a data set where
p  n, after subsampling one often ends up with p comparable to n on the subsamples
where bootstrap-like computations are performed. It is therefore important to know if the
bootstrap and other resampling plans perform well when p is comparable to n.
Defining success: accurate inference on Î²1 The common theoretical definition of whether
the bootstrap â€œworksâ€ is that the bootstrap distribution of the entire bootstrap estimate
Î²bâˆ—
Ï
converges conditionally almost surely to the sampling distribution of the estimator Î²bÏ
(see e.g., van der Vaart, 1998). The work of Bickel and Freedman (1983) on the residual
bootstrap for least squares regression, which we discuss in the background section 1.2, shows
that this theoretical requirement is not fulfilled even for the simple problem of least squares
regression.
In this paper, we choose to focus only on accurate inference for the projection of our
parameter on a pre-specified direction Ï…. More specifically, we concentrate only on whether
the bootstrap gives accurate confidence intervals for Ï…
0Î². We think that this is the absolute
minimal requirement we can ask of a bootstrap inferential method, as well as one that is
meaningful from the standpoint of applied statistics. This is of course a much less stringent
requirement than performing well on complicated functionals of the whole parameter vector,
which is the implicit demand of standard definitions of bootstrap success. For this reason,
we focus throughout the exposition on inference for Î²1 (the first element of Î²) as an example
of a pre-defined direction of interest (where Î²1 corresponds to choosing Ï… = e1, the first
canonical basis vector).
We note that considering the asymptotic behavior of Ï…
0Î² as p/n â†’ Îº âˆˆ (0, 1) implies that
Ï… = Ï…(p) changes with p. By â€œpre-definedâ€ we will mean simply a deterministic sequence
of directions Ï…(p). We will continue to suppress the dependence on p in writing Ï… in what
follows for the sake of clarity.
1.1 Organization and Main Results of the Paper
In Section 2 we demonstrate that in high dimensions residual-bootstrap resampling results
in extremely poor inference on the coordinates of Î²Ï with error rates much higher than the
reported Type I error. We show that the error in inference based on residual bootstrap
resampling is due to the fact that the distribution of the residuals ei are a poor estimate
of the distribution of i
; we further illustrate that common methods of standardizing the ei
do not solve the problem for general Ï. We propose two new dimension-adaptive methods
of residual resampling that appear promising for use in bootstrapping linear models. We
also provide some theoretical results for the behavior of this method as p/n â†’ 1.
In Section 3 we examine pairs-bootstrap resampling and show that confidence intervals
based on bootstrapping the pairs also perform very poorly. Unlike in the residual-bootstrap
3
El Karoui and Purdom
case discussed in Section 2, the confidence intervals obtained from the pairs-bootstrap are
instead conservative to the point of being non-informative. This results in a dramatic loss of
power. We prove in the case of L2 loss, i.e., Ï(x) = x
2
, that the variance of the bootstrapped
v
0Î²bâˆ—
is greater than that of v
0Î²b, leading to the overly conservative performance we see in
simulations. We demonstrate that a different resampling scheme we propose can provide
accurate confidence intervals in moderately high dimensions.
In Section 4, we discuss another resampling scheme, the jackknife. We focus on the
jackknife estimate of variance and show that it has similarly poor behavior in high dimensions. In the case of L2 loss with Gaussian design matrices, we further prove that the
jackknife estimator over estimates the variance of our estimator by a factor of 1/(1 âˆ’ p/n);
we also provide corrections for other losses that improve the jackknife estimate of variance
in moderately high dimensions.
We rely on simulation results to demonstrate the practical impact of the failure of the
bootstrap. The settings for our simulations and corresponding theoretical analyses are idealized, without many of the common settings of heteroskedasticity, dependency, outliers and
so forth that are known to be a problem for bootstrapping. This is intentional, since even
these idealized settings are sufficient to demonstrate that the standard bootstrap methods
have poor performance. For brevity, we give only brief descriptions of the simulations in
what follows; detailed descriptions can be found in AppendixD.1.
Similarly, we focus on the basic implementations of the bootstrap for linear models.
While there are many proposed alternatives (often for specific loss functions or types of
data), the standard methods we study are most commonly used and recommended in practice. Furthermore, to our knowledge none of the alternative bootstrap methods we have
seen specifically address the underlying theoretical problems that appear in high dimensions
without making low-dimensional assumptions about either the design matrix or the sparsity
of Î², and therefore are likely to suffer from the same fate as standard methods. We note that
in truly large scale applications, sparsity assumptions are not always made by practitioners
(Chapelle et al., 2014; Langford et al., 2007; Criteo, 2017) and it is hence natural to study
the performance of estimators outside of sparse settings. We have also experimented with
more complicated ways to build confidence intervals (e.g., bias correction methods), but
have found their performance to be erratic in high-dimension and offer no improvement.
We first give some background regarding the bootstrap and estimation of linear models
in high dimensions before presenting our new results.
1.2 Background: Inference Using the Bootstrap
We consider the setting yi = Î²
0Xi + i
, where E(i) = 0 and var (i) = Ïƒ
2

. The vector Î² is
estimated as minimizing the average loss,
Î²bÏ = argminbâˆˆRp
Xn
i=1
Ï(yi âˆ’ X0
i
b), (1)
where Ï defines the loss function for a single observation. The function Ï is assumed to
be convex in all the paper. Common choices are Ï(x) = x
2
, i.e., least-squares, Ï(x) = |x|,
which defines L1 regression, or Huberk loss where Ï(x) = (x
2/2)1|x|<k + (k|x| âˆ’ k
2/2)1|x|â‰¥k
.
4
Can We Trust the Bootstrap in High-dimensions?
Bootstrap methods are used in order to estimate the distribution of the estimate Î²bÏ
under the true data-generating distribution, F. The bootstrap estimates this distribution
with the distribution obtained when the data is drawn from an estimate FË† of F. Following
standard convention, we designate this bootstrapped estimator Î²bâˆ—
Ï
to note that this is an
estimate of Î² using loss function Ï when the data-generating distribution is known to be
exactly equal to FË†. Since FË† is completely specified, we can in principle exactly calculate
the distribution of Î²bâˆ—
Ï and use it as an approximation of the distribution of Î²bÏ under F. In
practice, we simulate B independent draws of size n from the distribution FË† and perform
inference based on the empirical distribution of Î²bâˆ—b
Ï
, b = 1, . . . , B.
In bootstrap inference for the linear model, there are two common methods for resampling, which results in different estimates FË†. In the first method, called the residual
bootstrap, FË† is an estimate of the conditional distribution of yi given Î² and Xi
. In this
case, the corresponding resampling method consists of resampling 
âˆ—
i
from an estimate of
the distribution of  and forming data y
âˆ—
i = X0
iÎ²bÏ + 
âˆ—
i
, from which Î²bâˆ—
Ï
is computed. This
method of bootstrapping assumes that the linear model is correct for the mean of y (i.e.,
that E (yi) = X0
iÎ²); it also assumes fixed Xi design vectors because the sampling is conditional on the Xi
. In the second method, called pairs bootstrap, FË† is an estimate of the
joint distribution of the vector (yi
, Xi) âˆˆ Rp+1 given by the empirical joint distribution
of {(yi
, Xi)}
n
i=1; the corresponding resampling method resamples the pairs (yi
, Xi). This
method makes no assumption about the mean structure of y and, by resampling the Xi
, also
does not condition on the values of Xi
. For this reason, pairs resampling is often considered
to be more generally applicable than residuals resampling (see e.g., Davison and Hinkley,
1997).
1.3 Background: High-dimensional Inference of Linear Models
Recent research shows that Î²bÏ has very different asymptotic properties when p/n has a limit
Îº that is bounded away from zero than it does in the classical setting where p/n â†’ 0 (see
e.g., Huber, 1973; Huber and Ronchetti, 2009; Portnoy, 1984, 1985, 1986, 1987; Mammen,
1989 for Îº = 0; El Karoui et al., 2013 for Îº âˆˆ (0, 1)). A simple example is that the vector Î²bÏ
is no longer consistent in Euclidean norm when Îº > 0. We should be clear, however, that
projections on fixed non-random directions such as we consider, i.e., Ï…
0Î²bÏ, are âˆš
n consistent
for Ï…
0Î², even when Îº > 0. In particular, the coordinates of Î²bÏ are âˆš
nâˆ’consistent for the
coordinates of Î² (El Karoui et al., 2013, Lemma 1). Hence, in practice the estimator Î²bÏ is
still a reasonable quantity to consider.
Bootstrap in high-dimensional linear models Very interesting work exists already in the
literature about bootstrapping regression estimators when p is allowed to grow with n
(Shorack, 1982; Wu, 1986; Mammen, 1989, 1992, 1993; Parzen et al., 1994; Koenker, 2005,
Section 3.9). With a few exceptions, this work has been in the classical, low-dimensional
setting where either p is held fixed or p grows slowly relative to n (i.e., Îº = 0 in our
notation). For instance, in Mammen (1993), it is shown that under mild technical conditions
and assuming that p
1+Î´/n â†’ 0, Î´ > 0, the pairs bootstrap distribution of linear contrasts
v
0
(Î²bâˆ—
Ïâˆ’Î²bÏ) is in fact very close to the sampling distribution of v
0
(Î²bÏâˆ’Î²) with high-probability,
when using least-squares. Other results, such as Shorack (1982) and Mammen (1989),
5
El Karoui and Purdom
also allow for increasing dimensions, for example in the case of linear contrasts in robust
regression, by making assumptions on the diagonal entries of the hat matrix. In our context,
these assumptions would be satisfied only if p/n â†’ 0. Hence those interesting results do
not apply to the present study. We also note that Hall (1992, p. 167) contains cautionary
notes about using the bootstrap in high-dimension.
While there has not been much theoretical work on the bootstrap in the setting where
p/n â†’ Îº âˆˆ (0, 1), one early work of Bickel and Freedman (1983) considered bootstrapping scaled residuals for least-squares regression when Îº > 0. They show that when
p/n â†’ Îº âˆˆ (0, 1), there exists a data-dependent direction c, such that c
0Î²bâˆ—
LS does not
have the correct asymptotic distribution (Bickel and Freedman, 1983, Theorem 3.1, p.39),
i.e., its distribution is not conditionally in probability close to the sampling distribution of
c
0Î²b
LS. Furthermore, they show that when the errors in the model are Gaussian, under the
assumption that the diagonal entries of the hat matrix are not all close to a constant, the
empirical distribution of the residuals is a scaled-mixture of Gaussian, which is not close to
the original error distribution.
As we previously explained, in this work we instead only consider inference for predefined
contrasts Ï…
0Î². The important and interesting problems pointed out in Bickel and Freedman
(1983) disappear if we focus on fixed, non-data-dependent projection directions. Hence, our
work complements the work of Bickel and Freedman (1983) and is not redundant with it.
There has been some recent interest in residual bootstrap methods for penalized likelihood methods in high-dimensions (often proposed for the case when p >> n), for example
lasso estimates (Chatterjee and Lahiri, 2010, 2011), adaptive lasso estimates (Chatterjee
and Lahiri, 2013), de-biased lasso estimates (Belloni et al., 2015; Dezeure et al., 2017), and
ridge regression (Lopes, 2014). These bootstrap results make the assumption of sparsity
of some form, generally in terms of the number of non-zero components of Î², but in the
case of Lopes (2014) by the assumption that the design matrix X is nearly low-rank. As
explained previously, our work is focused on a very different line of inquiry: the case of
a comparatively diffuse signal in Î², where there is no reduction of the high-dimensional
problem to a low-dimensional approximation.
The role of the distribution of X An important consideration in interpreting theoretical
work on linear models in high dimensions is the role of the design matrix X. In classical
asymptotic theory, the results can be stated conditionally on X so that the assumptions
can be stated in terms of conditions that can be evaluated on any observed design matrix
X. In the high dimensional setting, the available theoretical tools do not yet allow for
an asymptotic analysis conditional on X; instead the results make assumptions about the
distribution of the entries of X. Theoretical work in the nascent literature for the high
dimensional setting usually allows for a fairly general class of distributions for the individual
elements of Xi and can handle covariance between the predictor variables. However, the
Xi
â€™s are generally considered i.i.d., which limits the ability of any Xi to be too influential
in the fit of the model (see AppendixA for more detail). For discussion of limitations of
the corresponding models for statistical purposes, see Diaconis and Freedman (1984); Hall
et al. (2005); El Karoui (2009).
6
Can We Trust the Bootstrap in High-dimensions?
1.4 Notations and Default Conventions
When referring to the Huber loss in a numerical context, we refer (unless otherwise noted)
to the default implementation in the rlm package in R, where the transition from quadratic
to linear behavior is at k = 1.345. We call X the design matrix and {Xi}
n
i=1 its rows. We
have Xi âˆˆ R
p
. Î² denotes the true regression vector, i.e., the population parameter. Î²bÏ refers
to the estimate of Î² using loss Ï; from this point on, however, we will often drop the Ï and
refer to simply Î²b. The i-th residual is denoted as ei
, i.e., ei = yi âˆ’ X0
iÎ²b. Throughout the
paper, we assume that the linear model holds, i.e., yi = X0
iÎ² + i for some fixed Î² âˆˆ R
p and
that i
â€™s are i.i.d. with mean 0 and var (i) = Ïƒ
2

. We call G the distribution of . When we
need to stress the impact of the error distribution on the distribution of Î²bÏ, we will write
Î²bÏ(G) or Î²bÏ() to denote our estimate of Î² obtained assuming that i
â€™s are i.i.d. G.
We denote generically by Îº = limnâ†’âˆ p/n. We restrict ourselves to Îº âˆˆ (0, 1). The
standard notation Î²b
(i)
refers to the leave-one-out estimate of Î²b where the i-th pair (yi
, Xi)
is excluded from the regression, and Ëœei(i) , yi âˆ’ X0
iÎ²b
(i)
is the i-th predicted error (based
on the leave-one-out estimate of Î²b). We also use the notation Ëœej(i) , yj âˆ’ X0
jÎ²b
(i)
. The hat
matrix is of course H = X(X0X)
âˆ’1X0
. oP denotes a â€œlittle-ohâ€ in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Î£, we mean that Xi
iidv N (0, Î£). Throughout the paper, the loss function
Ï is assumed to be convex, R 7â†’ R
+. We use the standard notation Ïˆ = Ï
0
. We finally
assume that Ï is such that there is a unique solution to the robust regression problemâ€”an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We first focus on the method of bootstrap resampling where FË† is the conditional distribution
y|Î², X. b In this case the distribution of Î²bâˆ— under FË† is formed by independent resampling
of 
âˆ—
i
from an estimate GË† of the distribution G that generated i
. Then new data y
âˆ—
i
are
formed as y
âˆ—
i = X0
iÎ²b + 
âˆ—
i
and the model is fitted to this new data to get Î²bâˆ—
. Generally the
estimate of the error distribution, GË†, is taken to be empirical distribution of the observed
residuals, so that the 
âˆ—
i
are found by sampling with replacement from the ei
.
Yet, even a cursory evaluation of ei
in the simple case of least-squares regression (Ï(x) =
x
2
) reveals that the empirical distribution of the ei may be a poor approximation to the
error distribution of i
. In particular, it is well known that ei has variance equal to Ïƒ
2

(1âˆ’hi)
where hi
is the ith diagonal element of the hat matrix. This problem becomes particularly
pronounced in high dimensions. For instance, if Xi
iidv N (0, Î£), hi = p/n + oP (1) so that ei
has variance approximately Ïƒ
2

(1 âˆ’ p/n), i.e., generally much smaller than the true variance
of  for lim p/n > 0. This fact is also true when assuming more general distributions for the
design matrix X (see e.g.,Wachter, 1978; Haff, 1979; Silverstein, 1995; Pajor and Pastur,
2009; El Karoui and Koesters, 2011, where the main results of some of these papers require
minor adjustments to get the approximation of hi we just mentioned).
In Figure 1, we plot the error rate of 95% bootstrap confidence intervals based on resampling from the residuals for different loss functions, based on a simulation when the entries
of X are i.i.d. N (0, 1) and  âˆ¼ N(0, 1). Even in this idealized situation, as the ratio of
7
El Karoui and Purdom
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
(a) L1 loss
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
â—
â—
â—
â—
(b) Huber loss
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
â—
â—
â—
â—
â—
â—
Residual
Jackknife
Pairs
Std. Residuals
(c) L2 loss
Figure 1: Performance of 95% confidence intervals of Î²1 : Here we show the coverage error rates for 95% confidence intervals for n = 500 based on applying common resampling-based methods to simulated data: pairs bootstrap (red), residual
bootstrap (blue), and jackknife estimates of variance (yellow). These bootstrap
methods are applied with three different loss functions shown in the three plots
above: (a) L1, (b) Huber, and (c) L2. For L2 and Huber loss, we also show
the performance of methods for standardizing the residuals before bootstrapping
described in the text (blue, dashed line). If accurate, all of these methods should
have an error rate of 0.05 (shown as a horizontal black line). Error rates above
5% correspond to anti-conservative methods. Error rates below 5% correspond
to conservative methods. The error rates are based on 1,000 simulations, with
N(0, 1) error, and entries of the design matrix i.i.d N(0, 1); see the description in
Appendix D.1 for more details. The exact values plotted here are given in Table
A-1 in Appendix I.
p/n increases the error rate of the confidence intervals in least squares regression increases
well beyond the expected 5%: we observe error rates of 10-15% for p/n = 0.3 and approximately 20% for p/n = 0.5. We see similar error rates for other robust-regression methods,
such as L1 and Huber loss, and also for different error distributions and distributions of X
(Supplementary Figures A-1 and A-2). We explain some of the reasons for these problems
in Subsection 2.2 below.
2.1 Bootstrapping from Corrected Residuals
While resampling directly from the uncorrected residuals is widespread and often given
as a standard bootstrap procedure (e.g., Koenker, 2005; Chernick, 1999), the discrepancy
between the distribution of i and ei has spurred more refined recommendations in the
8
Can We Trust the Bootstrap in High-dimensions?
case of least-squares: form corrected residuals ri = ei/
âˆš
1 âˆ’ hi and sample the 
âˆ—
i
from the
empirical distribution of the ri âˆ’ rÂ¯ (see e.g., Davison and Hinkley, 1997).
This correction is known to exactly align the variance of ri with that of i regardless
of the design vectors Xi or the true error distribution, using simply the fact that the hat
matrix is a rank min(n, p) orthogonal projection matrix. We see that for L2 loss it corrects
the error in bootstrap inference in our simulations (Figure 1). This is not so surprising,
given that with L2 loss, the error distribution G impacts the inference on Î² only through
Ïƒ
2

, in the case of homoskedastic errors (see Section 2.4 for much more detail).
However, this adjustment of the residuals is a correction specific to the least-squares
problem. Similar corrections for robust estimation procedures using a loss function Ï are
given by McKean et al. (1993) with standardized residuals ri given by,
ri =
ei
âˆš
1 âˆ’ dhi
, where d =
2
Pe
0
jÏˆ(e
0
j
)
PÏˆ(e
0
j
)
âˆ’
PÏˆ(e
0
j
)
2
(
PÏˆ(e
0
j
))2
, (2)
where hi
is the i-th diagonal entry of the hat matrix, e
0
j = ej/s, s is a estimate of Ïƒ, and Ïˆ
is the derivative of Ï, assuming Ïˆ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
confidence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in highdimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in highdimension, the residuals tend to have a very different distribution from that of the true
errors. Their distributions differ not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of Î²bÏ as p/n â†’ Îº âˆˆ (0, 1); in the course of
doing so, they characterize the asymptotic relationship of ei to i
in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of GË† for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and i
in high-dimensions given in the above cited work (see AppendixA for a more detailed and technical summary). Let Î²b
(i) be the estimate of Î² based on
fitting the linear model of Equation (1) without using observation i, and Ëœej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., Ëœej(i) = yj âˆ’ X0
jÎ²b
(i)
9
El Karoui and Purdom
For simplicity of exposition, Xi
is assumed to have an elliptical distribution, i.e., Xi = Î»iÎ“i
,
where Î“i âˆ¼ N(0, Î£), and Î»i
is a scalar random variable independent of Î“i with E

Î»
2
i

= 1.
For simplicity in restating their results, we will assume Î£ = Idp, but equivalent statements
can be made for arbitrary Î£; similar results also apply when Î“i = Î£1/2
Î¾i
, with Î¾i having
i.i.d. non-Gaussian entries, satisfying a few technical requirements (see AppendixA).
With this assumption on Xi
, for any sufficiently smooth loss function Ï and any size
dimension where p/n â†’ Îº < 1, the relationship between the i-th residual ei and the true
error i can be summarized as,
eËœi(i) = i + |Î»i
|kÎ²b
Ï(i) âˆ’ Î²k2Zi + oP (un) (3)
ei + ciÎ»
2
i Ïˆ(ei) = Ëœei(i) + oP (un) (4)
where Zi
is a random variable distributed N(0, 1) and independent of i
. The variable un
refers to a sequence of numbers tending to 0. The quantities ci
, Î»i and kÎ²b
Ï(i) âˆ’ Î²k2 are
all of order 1, i.e., they are not close to 0 in general in the high-dimensional setting. The
scalar ci
is given as 1
n
trace
S
âˆ’1
i

, where Si =
1
n
P
j6=i Ïˆ
0
(Ëœej(i)
)XjX0
j
. For p, n large the ci
â€™s
are approximately equal and kÎ²b
Ï(i) âˆ’ Î²k2 ' kÎ²bÏ âˆ’ Î²k2 ' E

kÎ²bÏ âˆ’ Î²k2

; furthermore ciÎ»
2
i
can be approximated by X0
iS
âˆ’1
i Xi/n. Note that when Ï is either non-differentiable at all
points (L1) or not twice differentiable (Huber), arguments can be made that make these
expressions valid, using for instance the notion of sub-differential for Ïˆ (Hiriart-Urruty and
LemarÂ´echal, 2001).
Interpretation of Equations (3) and (4) Equation (3) means that the marginal distribution
of the leave-i-th-out predicted error, Ëœei(i)
, is asymptotically a convolution of the true error,
i
, and an independent scale mixture of Normals. Furthermore, Equation (4) means that
the i-th residual ei can be understood as a non-linear transformation of Ëœei(i)
. As we discuss
below, these relationships are qualitatively very different from the classical case p/n â†’ 0.
2.2.1 Consequence for the Residual Bootstrap
We apply these results to the question of the residual bootstrap to give an understanding
of why bootstrap resampling of the residuals can perform so badly in high-dimension. The
distribution of the ei
is far removed from that of the i
, and hence bootstrapping from the
residuals effectively amounts to sampling errors from a distribution that is very different
from the original error distribution, .
The impact of these discrepancies for bootstrapping is not equivalent for all dimensions,
error distributions, or loss functions. It depends on the constant ci and the risk, kÎ²b
Ï(i)âˆ’Î²k2,
both of which are highly dependent on the dimensions of the problem, the distribution of
the errors and the choice of loss function. We now discuss some of these issues.
Least Squares regression In the case of least squares regression, the relationships given in
Equation (3) are exact, i.e., un = 0. Further, Ïˆ(x) = x, and ci = hi/(1 âˆ’ hi), giving the
well known linear relationship ei = (1 âˆ’ hi)Ëœei(i)
(see, e.g., the standard reference Weisberg,
2014). This linear relationship is exact regardless of dimension, though the dimensionality
aspects are captured by hi
. This expression can be used to show that asymptotically
E
Pn
i=1 e
2
i

= Ïƒ
2

(n âˆ’ p), when i
â€™s have the same variance. Hence, sampling at random   
Can We Trust the Bootstrap in High-dimensions?
from the residuals results in a distribution that underestimates the variance of the errors
by a factor 1 âˆ’ p/n. The corresponding bootstrap confidence intervals are then naturally
too small, and hence the error rate increases far from the nominal 5% - as we observed in
Figure 1c.
More general robust regression The situation is much more complicated for general robust
regression estimators. One clear implication of Equations (3) and (4) is that simply rescaling
the residuals ei should not in general result in an estimated error distribution GË† that will
have similar properties to those of G. The relationship between the residuals and the errors
is very non-linear in high-dimensions. This is why in what follows we will propose to work
with leave-one-out predicted errors Ëœei(i)
instead of the residuals ei
.
The classical case of p/n â†’ 0: In this setting, ci â†’ 0 and therefore Equation (3) shows
that the residuals ei are approximately equal in distribution to the predicted errors, Ëœei(i)
.
Similarly, Î²bÏ is L2 consistent when p/n â†’ 0, so kÎ²b
Ï(i) âˆ’ Î²k
2
2 â†’ 0 and Equation (4) gives
eËœi(i) ' i
. Hence, the residuals should be fairly close to the true errors in the model when
p/n is small. This dimensionality assumption is key to many theoretical analyses of robust
regression, and underlies the derivation of corrected residuals ri of McKean et al. (1993)
given in Equation (2) above for losses other than L2.
2.3 Alternative Residual Bootstrap Procedures
We propose two methods for improving the performance of confidence intervals obtained
through the residual bootstrap. Both do so by providing alternative estimates of GË† from
which bootstrap errors 
âˆ—
i
can be drawn. They estimate a GË† appropriate for the setting of
high-dimensional data by accounting for relationship of the distribution of  and Ëœei(i)
.
Method 1: Deconvolution The relationship in Equation (3) says that the distribution of
eËœi(i)
is a convolution of the correct G distribution and a normal distribution. This suggests
applying techniques for deconvolving a signal from Gaussian noise. Specifically, we propose
the following bootstrap procedure: 1) calculate the predicted errors, Ëœei(i)
; 2) estimate the
variance of the normal (i.e., |Î»i
|kÎ²b
Ï(i)âˆ’Î²k
2
2
); 3) deconvolve in Ëœei(i)
the error term i from the
normal term; 4) Use the resulting estimate GË† to draw errors 
âˆ—
i
for residual bootstrapping.
Deconvolution problems are known to be very difficult (see Fan, 1991, Theorem 1 p.
1260, that gives 1/ log(n)
Î± rates of convergence when convolving with a Gaussian distribution). The resulting deconvolved errors are likely to be quite noisy estimates of i
. However,
it is possible that while individual estimates are poor, the distribution of the deconvolved
errors is estimated well-enough to form a reasonable GË† for the bootstrap procedure.
We used the deconvolution algorithm in the decon package in R (Wang and Wang, 2011)
to estimate the distribution of i
. The deconvolution algorithm requires knowledge of the
variance of the Gaussian that is convolved with the i
, i.e., estimation of |Î»i
|kÎ²b
Ï(i) âˆ’ Î²k2
term. In what follows, we assume a Gaussian design, i.e., Î»i = 1, so that we need to
estimate only the term kÎ²b
Ï(i) âˆ’ Î²k
2
2
. An estimation strategy for the more general setting of
|Î»i
| 6= 1 is presented in AppendixB.5. We use the fact that kÎ²b
Ï(i) âˆ’ Î²k
2
2 ' kÎ²bÏ âˆ’ Î²k
2
2
for
all i and estimate kÎ²b
Ï(i) âˆ’ Î²k2 as var d(Ëœei(i)
) âˆ’ ÏƒË†
2

, where var d(Ëœei(i)
) is the empirical variance
of the Ëœei(i) and Ë†Ïƒ
2

is an estimate of the variance of G, which we discuss below. We note
11
El Karoui and Purdom
that the deconvolution strategy we employ makes assumptions of homoskedastic errors i
â€™s,
which is true in our simulations but may not be true in practice. See AppendixB for details
regarding the implementation of Method 1.
Method 2: Bootstrapping from standardized eËœi(i) A simpler alternative is bootstrapping
from the predicted error terms, Ëœei(i)
, without deconvolution. Specifically, we propose to
bootstrap from a scaled version of Ëœei(i)
,
rËœi(i) = q
ÏƒË†
var d(Ëœei(i)
)
eËœi(i)
,
where var d(Ëœei(i)
) is the standard estimate of the variance of Ëœei(i) and Ë†Ïƒ is an estimate of
Ïƒ. This scaling aligns the first two moments of Ëœei(i) with those of i
. On the face of it,
resampling from Ëœri(i)
seems problematic, since Equation (3) demonstrates that Ëœei(i) does not
have the same distribution as i
, even if the first two moments are the same. However, as we
demonstrate in simulations, this distributional mismatch appears to have limited practical
effect on our bootstrap confidence intervals.
Estimation of Ïƒ
2
 Both methods described above require an estimator of Ïƒ that is consistent
regardless of dimension and error distribution. As we have explained earlier, for general Ï
we cannot rely on the observed residuals ei nor on Ëœei(i)
for estimating Ïƒ (see Equations (3)
and (4)). The exception is the standard estimate of Ïƒ
2

from least-squares regression, i.e.,
Ï(x) = x
2
,
Ïƒb
2
,LS =
1
n âˆ’ p
X
i
e
2
i,L2
.
Ïƒb
2
,LS is a consistent estimator of Ïƒ
2
for any error distribution G, assuming i.i.d. errors
and mild moment requirements. In implementing the two alternative residual-bootstrap
methods described above, we use Ïƒb,LS as our estimate of Ïƒ, including for bootstrapping
robust regression where Ï(x) 6= x
2
.
Performance in bootstrap inference In Figure 2 we show the error rate of confidence intervals
based on the two residual-bootstrap methods we proposed above. We see that both methods
control the Type I error, unlike bootstrapping directly from the residuals, and that both
methods are conservative. There is little difference between the two methods with this
sample size (n = 500), though with n = 100, we observe the deconvolution performance to
be worse in L1 (data not shown).
The deconvolution strategy, however, depends on the distribution of the design matrix,
which in these simulations we assumed was Gaussian (so we did not have to estimate Î»i
â€™s).
For elliptical designs (Î»i 6= 1), the error rate of the deconvolution method described above,
with no adaptation for the design, was similar to that of uncorrected residuals in high
dimensions (i.e., > 0.25 for p/n = 0.5). Individual estimates of Î»i might improve the
deconvolution strategy, but this problem points to the general reliance of the deconvolution
method on precise knowledge about the design matrix. The bootstrap using standardized
predicted errors, on the other hand, had a Type I error for an elliptical design only slightly
higher than the target 0.05 (around 0.07, data not shown), suggesting that it might be less
sensitive to the properties of the design matrix.
12
Can We Trust the Bootstrap in High-dimensions?
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
* * *
*
(a) L1 loss
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
* *
* *
â— Residual
Std. Pred Error
Deconv
N(0, Ïƒ
^
Îµ, LS
2
)
(b) Huber loss
Figure 2: Bootstrap based on predicted errors: We plotted the error rate of 95% confidence intervals for the alternative bootstrap methods described in Section 2.3:
bootstrapping from standardized predicted errors (green) and from deconvolution
of predicted error (magenta). We demonstrate its improvement over the standard
residual bootstrap (blue) for (a) L1 loss and (b) Huber loss. The error distribution is double exponential, but otherwise the simulations parameters are as in
Figure 1. The error rates on confidence intervals based on bootstrapping from a
N(0, Ïƒb
2
,LS) (dashed curve) are as a lower bound on the problem. For the precise
error rates see Appendix, Table A-3.
Given our previous discussion of the behavior of Ëœei(i)
, it is somewhat surprising that
resampling from the distribution of Ëœri(i) performed well in our simulations. Clearly a few
cases exist where Ëœri(i)
should work well as an approximation of i
. We have already noted
that as p/n â†’ 0, the effect of the convolution with the Gaussian disappears since kÎ²bÏâˆ’Î²k â†’
0; in this case both ei and Ëœri(i)
should be good estimates of i
. Similarly, in the case
i âˆ¼ N(0, Ïƒ2
), Equation (3) tells us that Ëœei(i) are also asymptotically marginally normally
distributed, so that correcting the variance should result in Ëœri(i) having the same distribution
as i
, at least when Xi,j are i.i.d.
Surprisingly, for larger p/n we do not see a deterioration of the performance of bootstrapping from Ëœri(i)
. This is unexpected, since as p/n â†’ 1 the risk kÎ²bÏ âˆ’ Î²k
2
2
grows to
be much larger than Ïƒ
2

(a claim we will make more precise in the next section); together
with Equation (3), this implies that Ëœri(i)
is essentially distributed N(0, Ïƒb
2
,LS) as p/n â†’ 1
regardless of the original distribution of i
. This is confirmed in Figure 2 where we superimpose the results of bootstrap confidence intervals from when we simply estimate GË† with
N(0, ÏƒË†
2
,LS); we see the Type I error rate of the confidence intervals based on bootstrapping
from Ëœri(i) do indeed approach that of N(0, ÏƒË†
2
,LS). Putting these two pieces of information
13
El Karoui and Purdom
together leads to the conclusion that as p/n â†’ 1 we can estimate GË† simply as N(0, ÏƒË†,LS)
regardless of the actual distribution of .
In the next section we give some theoretical results that seek to understand this phenomenon.
2.4 Behavior of the Risk of Î²b When Îº â†’ 1
In the previous section we saw even if the distribution of the bootstrap errors 
âˆ—
i
, given by GË†,
is not close to that of G, we can sometime get accurate bootstrap confidence intervals. For
example, in least squares Equation (3) makes clear that even the standardized residuals, ri
,
do not have the same marginal distribution as i
, yet they still provide accurate bootstrap
confidence intervals in our simulations. We would like to understand for what choice of
distributions GË† will we see the same performance in our bootstrap confidence intervals of
Î²b1?
When working conditional on X as in residual resampling, the statistical properties of
(Î²bâˆ— âˆ’ Î²b) differ from that of (Î²b âˆ’ Î²) only because the errors are drawn from a different
distribution: GË† rather than G. Then to understand whether the distribution of Î²bâˆ—
1 matches
that of Î²b1 we can ask, what are the distributions of errors, G, that yield the same distribution
for the resulting Î²b1(G)? In this section, we narrow our focus on understanding not the entire
distribution of Î²b1, but only its variance. We do so because under assumptions on the design
matrix X, it is known that Î²b1 is asymptotically normally distributed. This is true for both
the classical setting of Îº = 0 and the high-dimensional setting of Îº âˆˆ (0, 1) (see AppendixA
for a review of these results and a more technical discussion). Our previous question is then
reduced to understanding which distributions G give the same var 
Î²b1(G)

.
In the setting of least squares, it is clear that the only property of i
iidv G that matters
for the variance of Î²b
1,L2
is Ïƒ
2

, since var 
Î²b
1,L2

= (X0X)
âˆ’1
(1, 1)Ïƒ
2

. For general Ï, if
we assume p/n â†’ 0, then var 
Î²b1,Ï
will depend on features of G beyond the first two
moments (specifically through E

Ïˆ
2
()

/[E (Ïˆ
0
())]2
, (Huber, 1973)). If we assume instead
p/n â†’ Îº âˆˆ (0, 1), then it has been shown (El Karoui et al., 2013) that var 
Î²b1,Ï(G)

depends on G only by the effect of G on the squared risk of the vector Î²bÏ(G), i.e., through
E

kÎ²bÏ(G) âˆ’ Î²k
2
2

(for the convenience of the reader we give a review of these results, which
are a bit scattered in the literature, in AppendixA).
For this reason, in the setting of p/n â†’ Îº âˆˆ (0, 1), we need to characterize the risk of Î²bÏ
to understand when different distributions of  result in the same variance of Î²b1,Ï. In what
follows, we denote by r
2
Ï
(Îº; G) the asymptotic squared risk of Î²bÏ(G) as p and n tend to âˆ,
r
2
Ï
(Îº; G) = lim
n,pâ†’âˆ,
p
nâ†’Îº
E||Î²bÏ(G) âˆ’ Î²||2
.
The dependence of r
2
Ï
(Îº; G) on  is characterized, under appropriate technical conditions
on X, Ï and i
â€™s, by a system of two non-linear equations (El Karoui et al., 2013). Specifically, if we define Ë†z =  + rÏ(Îº; G)Z, where Z âˆ¼ N (0, 1) is independent of , and  has the
same distribution G as the i
â€™s, then there exists a constant c such that the pair of positive,
1 
Can We Trust the Bootstrap in High-dimensions?
finite, and deterministic scalars (c, rÏ(Îº; G)) satisfy the following system of equations:
E ((prox(cÏ))0
(Ë†z)) = 1 âˆ’ Îº ,
Îºr2
Ï
(Îº; G) = E

[Ë†z âˆ’ prox(cÏ)(Ë†z)]2

.
(5)
In this system, prox(cÏ) refers to Moreauâ€™s proximal mapping of the convex function cÏ (see
Moreau, 1965; Hiriart-Urruty and LemarÂ´echal, 2001).
It is therefore not entirely trivial to characterize those distributions Î“ for which r
2
Ï
(Îº; G) =
r
2
Ï
(Îº; Î“). In the following theorem, however, we show that as Îº â†’ 1, r
2
Ï
(Îº; G) converges to a
constant that depends only on Ïƒ
2

. This implies that when Îº â†’ 1, two different error distributions that have the same variances will result in estimators Î²b1,Ï with the same variance.
Before stating our theorem formally, however, we will review the necessary assumptions for
the system of equations in (5) to hold. For a precise statement of the assumptions, see El
Karoui (2017)
Assumptions for Equation 5: The proof of (5) provided in El Karoui (2013) assumes
that the Xi
â€™s have mean 0, cov (Xi) = Idp, and they satisfy sub-Gaussian concentration
assumptions (with constants dependent on n). El Karoui (2013) further assumes that the
i have a unimodal density, are independent from the Xij , sup1â‰¤iâ‰¤n
|i
| = OP (polyLog(n)),
and that similiar bounds also hold for a few moments of i (the number of such moments
depends on the loss function Ï). Log-concave densities such as those corresponding to
double exponential or Gaussian errors used in the current paper fall within the scope of
this theorem. The reader interested in generalizations and truly heavy-tailed situation is
referred to El Karoui (2017) and references therein.
The loss function Ï is assumed by El Karoui (2013) (in the unpenalized case) to be
non-negative, twice differentiable, strongly convex, non-linear, taking value 0 at 0, and with
a derivative that grows at most polynomially at infinity and a second derivative that is
locally Lipschitz, with local Lipschitz constant that grow at most polynomially at infinity.
It should be noted that distributions with sufficiently many moments, the condition of
strong convexity of Ï can be obtained by adding Î´x2/2 to the initial Ï, with Î´ â€œsmallâ€,
e.g., Î´ = 10âˆ’10, and that modification will change very little or anything to the estimator.
Furthermore, the requirement of strong convexity of Ï, though superficially limiting, is likely
an artifact of the proof, where the main motivation was log-concave distributions with an
eye towards optimality (Bean et al., 2013). In fact, the theoretical predictions of (5) were
verified numerically in El Karoui et al. (2011) outside of the assumptions stated above,
and the predictions of Equation (5) were found to be very accurate in simulations even for
non-smoothed `1 and Huber losses with certain error distributions.
We now state the theorem formally; see AppendixE for the proof of this statement.
Theorem 1 Suppose we are working with robust regression estimators with loss Ï, and
p/n â†’ Îº. Suppose that r
2
Ï
(Îº; G) is characterized by the system of equations in (5). Then,
lim
Îºâ†’1
1 âˆ’ Îº
Ïƒ
2

r
2
Ï
(Îº; G) = 1 ,
provided Ï is additionally differentiable near 0 and Ï
0
(x) âˆ¼ x near 0.
1 
El Karoui and Purdom
Implications for the Bootstrap For the purposes of the residual-bootstrap, Theorem 1 implies that different methods of estimating the residual distribution GË† will result in similar
residual-bootstrap confidence intervals as p/n â†’ 1, if GË† has the same variance. This agrees
with our simulations, where both of our proposed bootstrap strategies set the variance of GË†
equal to Ïƒb
2
,LS and both had similar performance in our simulations for large p/n. Furthermore, as we noted, for p/n closer to 1, they both had similar performance to a bootstrap
procedure that simply sets GË† = N (0, Ïƒb
2
,LS) (Figure 2) (see also AppendixA.3 for further
discussion of residual bootstrap methods which draw from the â€œwrongâ€ distribution, i.e.,
forms of wild bootstraps (Wu, 1986)).
We return specifically to the bootstrap based on Ëœri(i)
, the standardized predicted errors.
Equation (3) tells us that the marginal distribution of Ëœei(i)
is a convolution of the distribution
of i and a normal, with the variance of the normal governed by the term kÎ²bÏâˆ’Î²k2. Theorem
1 makes rigorous our previous assertion that as p/n â†’ 1, the normal term will dominate
and the marginal distribution of Ëœei(i) will approach normality, regardless of the distribution
of . However, Theorem 1 also implies that as p/n â†’ 1, inference for the coordinates of Î²
will be increasingly less reliant on features of the error distribution beyond the variance,
implying that our standardized predited errors, Ëœri(i)
, will still result in an estimate GË† that
will give accurate confidence intervals. Conversely, as p/n â†’ 0 classical theory tells us that
the inference of Î² relies heavily on the distribution G beyond the first two moments, but in
that case the distribution of Ëœri(i) approaches the correct distribution as we explained earlier.
So bootstrapping from the marginal distribution of Ëœri(i) also makes sense when p/n is small.
For Îº between these two extremes it is difficult to theoretically predict the risk of
Î²bÏ(GË†) when the distribution GË† is given by resampling from the Ëœri(i)
. We turn to numerical
simulations to evaluate this risk. Specifically, for i âˆ¼ G, we simulated data that is a
convolution of G and a normal with variance equal to r
2
Ï
(Îº; G); we then scale this simulated
data to have variance Ïƒ
2

. The scaled data are the 
âˆ—
i
and we refer to the distribution of 
âˆ—
i
as the convolution distribution, denoted Gconv. Then, Gconv is the asymptotic version of
the marginal distribution of the standardized predicted errors, Ëœri(i)
, used in our bootstrap
method proposed above.
In Figure 3 we plot for both Huber loss and L1 loss the average risk rÏ(Îº; Gconv) (i.e.,
errors given by Gconv) relative to the average risk rÏ(Îº; G) (i.e., errors distributed according
to G), where G has a double exponential distribution. We also plot the relative average
risk rÏ(Îº; Gnorm), where Gnorm = N(0, Ïƒ2

). As predicted by Theorem 1, for Îº close to
1, rÏ(Îº; Gconv)/rÏ(Îº; G) and rÏ(Îº; Gnorm)/rÏ(Îº; G) converge to 1. Conversely, as Îº â†’ 0,
rÏ(Îº; Gnorm)/rÏ(Îº; G) diverges dramatically from 1, while rÏ(Îº; Gconv)/rÏ(Îº; G) approaches
1, as expected. For Huber, the divergence of rÏ(Îº; Gconv)/rÏ(Îº; G) from 1 is at most 8%,
but the difference is larger for L1 (12%), probably due to the fact that the convolution with
a normal error has a larger effect on the risk for L1.
3. Pairs Bootstrap
As described above, estimating the distribution FË† from the empirical distribution of (yi
, Xi)
(pairs bootstrapping) is generally considered the most general and widely applicable method
of bootstrapping, allowing for the linear model to be incorrectly specified (i.e., E (yi) is not
a linear function of Xi). It is also considered to be slightly more conservative compared
16
Can We Trust the Bootstrap in High-dimensions?
0.0 0.2 0.4 0.6 0.8
1.00 1.02 1.04 1.06 1.08 1.10 1.12
p/n
Relative Risk
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
(a) Relative risk of Gconv to G
0.0 0.2 0.4 0.6 0.8
1.0 1.1 1.2 1.3 1.4 1.5 1.6
p/n
Relative Risk
â—
â— â— â—
â—
â— â—
â—
â— â— â—
â—
â—â—
â—
â— â—
â— â—
â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â— Huber
L1
Convolved
Normal
(b) Relative risk of Gconv and Gnorm to G
Figure 3: Relative Risk of Î²b for scaled predicted errors vs original errors - population version: (a) Plotted with a solid lines are the ratios of the average risk of
Î²b(Gconv) to the average risk of Î²b(G) for Huber and L1 loss. (b) shows the same
plot, but added to the plot (dotted lines) is the relative risk of Î²b(G) when the
errors are distributed Gnorm = N (0, Ïƒ2

) . For both figures, the y-axis gives the
relative risk, and the x-axis is the ratio p/n, with n fixed at 500. Blue/triangle
plotting symbols indicate L1 loss; red/circle plotting symbols indicate Huber loss.
The average risk is calculated over 500 simulations, where the design matrix X
has Gaussian entries. The â€œtrueâ€ error distribution G is the standard Laplacian
distribution with Ïƒ
2
 = 2. Each simulation uses the standard estimate of Ïƒ
2

from
the generated i
â€™s. rÏ(Îº; G) was computed using a first run of simulations with
i
iidv G. The Huber loss in this plot is Huber1 and not the default Huber1.345 of
the rlm function.
to bootstrapping from the residuals. In the case of random design, it makes also a lot
of intuitive sense to use the pairs bootstrap, since resampling the predictors might be
interpreted as mimicking the data generating process.
However, as in residual bootstrap, it is clear that the pairs bootstrap will have problems,
at least in quite high dimensions. In fact, when resampling the Xi
â€™s from FË†, the number
of times a certain vector Xi0
is picked has asymptotically Poisson(1) distribution. So the
expected number of different vectors appearing in the bootstrapped design matrix Xâˆ—
is
n(1 âˆ’ 1/e). When p/n is large, with increasingly high probability the bootstrapped design
matrix Xâˆ— will no longer be of full rank. For example, if p/n > (1 âˆ’ 1/e) â‰ˆ 0.63 then
with probability tending to one as n â†’ âˆ, the bootstrapped design matrix Xâˆ—
is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that
situation makes little statistical sense (El Karoui, 2010, Subsection 4.4; Zheng et al., 2014).
For smaller ratios of p/n, we evaluate the performance of pairs bootstrapping on simulated data. We see that the performance of the bootstrap for inference also declines
17
El Karoui and Purdom
dramatically as the dimension increases, becoming increasingly conservative (Figure 1). In
pairs bootstrapping, the error rates of 95%-confidence-intervals drop far below the nominal
5%, and are essentially zero for the ratio of p/n = 0.5. Like residual bootstrap, this overall
trend is seen for all the settings we simulated under (Supplemental Figures A-1, A-2). For
L1 loss, even ratios as small as 0.1 yield incredibly conservative bootstrap confidence intervals for Î²b1, with the error rate dropping to less than 0.01. For Huber and L2 losses, the
severe loss of power in our simulations starts for ratios of 0.3.
A minimal requirement for the distribution of the bootstrapped data to give reasonable
inferences is that the variance of the bootstrap estimator Î²bâˆ—
1 needs to be a good estimate
of the variance of Î²b1. This is not the case in high-dimensions. In Figure 5 we plot the
ratio of the variance of Î²bâˆ—
1
to the variance of Î²b1 evaluated over simulations. We see that
for p/n = 0.3 and design matrices X with i.i.d. N (0, 1) entries, the average variance of Î²bâˆ—
1
roughly overestimates the true variance of Î²b1 by a factor 1.3 in the case of least-squares; for
Huber and L1 the bootstrap estimate of variance is roughly twice as large as it should be.
In the case of least-squares, we can further quantify this loss in power by comparing
the size of the bootstrap confidence intervals to the size of the correct confidence interval
based on theoretical results (Figure 4). We see that even for ratios Îº as small as 0.1, the
confidence intervals for some design matrices X were 15% larger for pairs bootstrap than
the correct size (e.g., the case of elliptical distributions where Î»i
is exponential). For much
higher dimensions of Îº = 0.5, the simple case of i.i.d. normal entries for the design matrix
gives intervals that are 80% larger than needed; for the elliptical distributions we simulated,
the width of the bootstrap confidence interval was as much as 3.5 times larger than that
of the correct confidence interval. Furthermore, as we can see in Figure 1, least-squares
regression represents the best case scenario; L1 and Huber will have even worse loss of
power and at smaller values of Îº.
3.1 Theoretical Analysis for Least-Squares
In the setting of least-squares, we can for some distributions of the design matrix X theoretically determine the asymptotic expectation of the variance of v
0Î²bâˆ— and show that it is a
severe over-estimate of the true variance of v
0Î²b.
We first setup some notation for the theorem that follows. Define Î²bw as the result of
regressing y on X with random weight wi for each observation (yi
, Xi). In other words,
Î²bw = argminuâˆˆRp
Xn
i=1
wi(yi âˆ’ X0
iu)
2
.
We assume that the weights are independent of {yi
, Xi}
n
i=1 and define Î²bâˆ—
w to be the random
variable with distribution equal to that of Î²bw conditional on the data {yi
, Xi}
n
i=1, i.e., Î²bâˆ—
w
L=
Î²bw|{yi
, Xi}
n
i=1. For the standard pairs bootstrap, the distribution of Î²bâˆ—
from resampling
from the pairs (yi
, Xi) is equivalent to the distribution of Î²bâˆ—
w, where w is drawn from a
multinomial distribution with expectation 1/n for each entry. In which case, the variance
of v
0Î²bâˆ—
w refers to the standard bootstrap estimate of variance given by the distribution of
v
0Î²bâˆ— over repeated resampling from the pairs (yi
, Xi).
18
Can We Trust the Bootstrap in High-dimensions?
1 1
1
1
Ratio (Îº)
% Increase in Average CI Width
2
2
2
2
3
3
3
3
0.01 0.10 0.30 0.50
1
10
50
100
350 1
2
3
Normal
Ellip. Normal
Ellip. Exp
Figure 4: Comparison of width of 95% confidence intervals of Î²1 for L2 loss:
Here we demonstrate the increase in the width of the confidence interval due to
pairs bootstrapping. Shown on the y-axis is the percent increase of the average
confidence interval width based on simulation (n = 500), as compared to the
average for the standard confidence interval based on normal theory in L2; the
percent increase is plotted against the ratio Îº = p/n (x-axis). Shown are three
different choices in simulating the entries of the design matrix X: (1) Xij âˆ¼
N(0, 1) (2) elliptical Xij with Î»i âˆ¼ N(0, 1) and (3) elliptical Xij with Î»i âˆ¼
Exp(
âˆš
2). The methods of simulation are the same as described in Figure 1;
exact values are given in Table A-2 in AppendixI.
19
El Karoui and Purdom
We have the following result for the expected value of the bootstrap variance of any
contrast v
0Î²bâˆ—
w where v is deterministic, assuming independent weights with a Gaussian
design matrix X and some mild conditions on the distribution of the wâ€™s.
Theorem 2 Let the weights (wi)
n
i=1 be i.i.d. and without loss of generality that E (wi) = 1;
we suppose that the wiâ€™s have 8 moments and for all i, wi > Î· > 0. Suppose Xiâ€™s are i.i.d.
N (0, Î£), Î£ is positive definite and the vector v is deterministic with kvk2 = 1.
Suppose Î²b is obtained by solving a least-squares problem and yi = X0
iÎ² + i, iâ€™s being
i.i.d. mean 0, with var (i) = Ïƒ
2

.
If lim p/n = Îº < 1 then the expected variance of the bootstrap estimator, asymptotically
as n â†’ âˆ, is given by
p
E

var 
v
0Î²bâˆ—
w

v
0Î£âˆ’1v
= p
E

var 
v
0Î²bw|{yi
, Xi}
n
i=1
v
0Î£âˆ’1v
â†’ Ïƒ
2


Îº
1 âˆ’ Îº âˆ’ f(Îº)
âˆ’
1
1 âˆ’ Îº

,
where f(Îº) = E

1
(1+cwi)
2

and c is the unique solution of E

1
1+cwi

= 1 âˆ’ Îº.
We note that E

1
(1+cwi)
2

â‰¥
h
E

1
1+cwi
i2
= (1âˆ’Îº)
2
- where the first inequality comes
from Jensenâ€™s inequality, and therefore the expression we give for the asymptotic limit of the
expected bootstrap variance is non-negative. For a proof of this theorem and a consistent
estimator of this limit, see AppendixF.
In light of previous work on model robustness issues in high-dimensional statistics (see
e.g., (Diaconis and Freedman, 1984; Hall et al., 2005; El Karoui, 2009, 2010)), it is natural
to ask whether the central results of Theorem 2 still apply when Xi
is not Gaussian but
has an elliptical distribution. The formula in Theorem 2 does not apply directly to this
latter case. However, the proof given in AppendixF extends to that setting, and we refer
the interested reader to the AppendixF.1 where we give the necessary details of how to
change the formulas and proof to encompass the elliptical case (we do not provide them in
rigorous mathematical detail in this work as they are substantially more cumbersome than
those in Theorem 2 and do not give enough additional insights to justify inclusion). On the
other hand, a number of the quantities appearing in the proof of Theorem 2 will converge
to the same limit as that given in Theorem 2 when i.i.d. Gaussian predictors are replaced
by i.i.d. predictors with mean 0 and variance 1 and sufficiently many moments (an example
being bounded random variables). Thus the results we present here should be fairly robust
to changing i.i.d. normality assumptions for the entries of the design matrix X, but again
the technical work necessary for making this rigorous is beyond the scope of this paper.
Implications for Pairs Bootstrap In the standard pairs bootstrap, the weights are chosen
according to a Multinomial(n, 1/n) distribution. This violates two conditions in the previous theorem: independence of wi
â€™s and the condition wi > 0. In AppendixF.2, we give the
technical details for how to extend the proof of Theorem 2 to multinomial weights. In what
follows, however, we use i.i.d. Poisson(1) weights, which asymptotically and marginally
correspond to the Multinomial(n, 1/n) weights, to develop intuition about the bootstrap.
In this case, we can apply the formula in Theorem 2 to explain why pairs bootstrap confidence intervals perform poorly in high-dimensions, at least for least squares regression with
Gaussian design matrix.
20
Can We Trust the Bootstrap in High-dimensions?
When Xi
iidv N (0, Î£), it is well known in the least-squares case that the quantity
p var 
v
0Î²b

/v0Î£
âˆ’1v converges asymptotically to Îº/(1 âˆ’ Îº)Ïƒ
2

(this can be shown through
simple Wishart computations Haff, 1979; Mardia et al., 1979). If the variance of v
0Î²bâˆ—
w converged to the variance of v
0Î²b, we should be able to equate this latter quantity to the limit
given in Theorem 2, i.e.,

Îº
1 âˆ’ Îº âˆ’ f(Îº)
âˆ’
1
1 âˆ’ Îº

=
Îº
1 âˆ’ Îº
,
and hence should have
f(Îº) = E

1
(1 + cwi)
2

=
1 âˆ’ Îº
1 + Îº
.
However, this relationship does not hold for most weight distributions, and in particular does not hold for weights following a Poisson(1) distribution (which asymptotically
corresponds to the standard pairs bootstrap, as explained above). Thus the pairs bootstrap
does not correctly estimate the variance of v
0Î²b. In Figure 5a we calculate the theoretical
predictions of E

var 
Î²bâˆ—
w
 given by Theorem 2 (using Poisson(1) weights and Î£ = Idp),
and we compare them to the asymptotic variance of Î²b1 given by Îº/(1âˆ’Îº)Ïƒ
2
 /p. We see that
Theorem 2 predicts that the pairs bootstrap overestimates the variance of the estimator
by a factor that ranges from 1.2 to 3 as Îº varies between 0.3 and 0.5. These theoretical
predictions correspond to the level of overestimation of the variance seen in our bootstrap
simulations (Figure 5b).
3.2 Alternative Weight Distributions for Resampling
The formula given in Theorem 2 suggests that resampling from a distribution FË† defined
using weights other than i.i.d. Poisson(1) (or, equivalently for our asymptotics,
Multinominal(n,1/n)) should give us better bootstrap estimators than using the standard
pairs bootstrap. In fact, we should require, at least, that the bootstrap expected variance
of these estimators match the correct variance var 
v
0Î²b

= Îº/(1âˆ’Îº)Ïƒ
2
 /p (for the Gaussian
design, when Î£ = Idp). We focus our discussion on the case Î£ = Idp; see AppendixC for
the case Î£ 6= Idp.
We note that if we use wi = 1, âˆ€i, the bootstrap variance will be 0, since with such
a resampling scheme the resampled data set is always the original data set. On the other
hand, we have seen that with wi âˆ¼ Poisson(1), the expected bootstrap variance was too
large compared to Îº/(1âˆ’Îº)Ïƒ
2
 /p. Hence, we tried to find alternative weights via calculating
a parameter Î± such that if
wi
iidv 1 âˆ’ Î± + Î±Poisson(1) , (6)
the expected bootstrap variance would match the theoretical value of Îº/(1 âˆ’ Îº)Ïƒ
2
 /p.
We numerically solved this problem to find Î±(Îº) (for details of computation see AppendixC). We then used these values and performed bootstrap resampling using the weights
defined in Equation (6). We evaluated bootstrap estimate of var 
Î²b1

as well as the confidence interval coverage of the true Î²1. We find that this adjustment of the weights in
21
El Karoui and Purdom
0 0.1 0.2 0.3 0.4 0.5
1
1.2
1.4
1.6
1.8
2
2.2
2.4
2.6
2.8
3
Ratio p/n
Overestimation Factor
(a) L2 (Theoretical)
â—
â—â—
â—â—
â—â—â—
â—â—
â—
â—â—
â—
â—
â—â—â—
â—â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—
â—â—
â—
â—
â—â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—â—â—â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
1 2
3
4
5
6
7
Ratio (Îº)
var(
^
Î²*) / var(
^
Î²)
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
L2 Huber L1
(b) All (Simulated)
Figure 5: Factor by which standard pairs bootstrap over-estimates the variance:
(a) plotted is the ratio of the value of the expected bootstrap variance computed
from Theorem 2 using Poisson(1) weights to the asymptotic variance Îº/(1âˆ’Îº)Ïƒ
2

.
(b) boxplots of the ratio of the bootstrap variance of Î²bâˆ—
1
to the variance Î²b1,
as calculated over 1000 simulations (i.e., var 
Î²b

is estimated across simulated
design matrices X, and not conditional on X). The theoretical prediction for the
mean of the distribution from Theorem 2 is marked with a â€˜Xâ€™ for L2 regression.
Simulations were performed with normal design matrix X and normal error i
with values of n = 500. For the median values of each boxplot, see Table A-6 in
AppendixI.
estimating FË† results in accurate bootstrap estimates of variance and appropriate levels of
confidence interval coverage (Table 1).
However, small changes in the choice of Î± can result in fairly large changes in
E

var 
v
0Î²bw|X, . For instance, for Îº = 0.5, using the value of Î± = 0.95 which is close
to the correct value of Î±(0.5) = 0.92 results in an expected bootstrap variance roughly 30%
larger than it should be.
Moreover, this strategy for finding a good weight distribution requires knowing a great
deal about the distribution of the design matrix. Hence the work we just presented on
finding new weight distributions for bootstrapping is a proof of principle that alternative
weighting schemes could be used for pairs bootstrapping in high-dimension, but important
practical details would depend strongly on the statistical model that is assumed. This is in
sharp contrast with the low-dimensional situation, where a unique and model-free bootstrap
resampling technique works in a broad variety of situations.
22
Can We Trust the Bootstrap in High-dimensions?
Îº
.1 .2 .3 .5
Î± .9875 .9688 .9426 .9203
Error Rate of 95% CIs 0.051 0.06 0.061 0.057
Ratio of Variances 1.0119 1.0236 0.9931 0.9992
Table 1: Summary of weight-adjusted bootstrap simulations for L2 : Given are the
results of performing bootstrap resampling for n = 500 according to the estimate
of FË† given by the weights in Equation (6). â€œError Rate of 95% CIsâ€ denotes
the percent of bootstrap confidence intervals that did not contain the correct
value of the parameter Î²1. â€œRatio of Variancesâ€ gives the ratio of the empirical
expected bootstrap variance over our simulations divided by the theoretical value
Ïƒ
2
 Îº/(1 âˆ’ Îº). Results are based on 1000 simulations, with a Gaussian random
design and errors distributed as double exponential.
4. The Jackknife
In the context we are investigating, where we know that the distribution of Î²b1 is asymptotically normal, it is natural to ask whether we could simply use the jackknife to estimate
the variance of Î²b1. The jackknife relies on leave-one-out procedures to estimate var 
Î²b1

.
More specifically, for a fixed vector v, the jackknife estimate of var 
v
0Î²b

is given by:
var dJACK(v
0Î²b) = n âˆ’ 1
n
Xn
i=1
(v
0
[Î²b
(i) âˆ’ Î²Ëœ])2
(7)
where Î²Ëœ =
1
n
Pn
i=1 Î²b
(i)
. The case of Î²b1 corresponds to picking v = e1, i.e., the first canonical
basis vector. The Efron-Stein inequality guarantees in general that the expectation of the
jackknife estimate of variance gives an upper-bound on the variance of the statistic under
consideration (Efron and Stein, 1981).
Given the problems we just documented with the pairs bootstrap, it is natural to ask
whether confidence intervals based on the jackknife estimate of variance perform better
than pairs bootstrap intervals in high-dimensions. The jackknife is known to have problems
(Efron, 1982 or Koenker, 2005, p.105), but the reliance of the jackknife on leave-one-out
estimates Î²b
(i) might suggest it could be more robust to dimensionality issues than other
methods.
Empirical findings As in the pairs bootstrap case, simulations show that confidence intervals
based on the jackknife estimate of variance lead to extremely poor inference for Î²1 (Figure
1) and that the jackknife dramatically overestimates the variance of Î²b1 (Figure 6). For L2
and Huber loss, the jackknife estimate of variance is 10-15% too large for p/n = 0.1, and for
p/n = 0.5 the jackknife estimate of variance is 2-2.5 times larger than it should be. In the
case of L1 loss, the jackknife variance is completely erratic, even in low dimensions; this is not
completely surprising given the known problems with the jackknife for the median (Koenker,
23
El Karoui and Purdom
â—
â—â—
â—
â—
â—â— â—
â—
â—â—
â—â— â—
â—
â—
â—â—
â— â—
â—
â—â—â—
â—
â—
â—â—
â—
â—
â—
â—â—
â—
â—â—â—
â—â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—â—â—
Ratio (Îº)
v
a
rJa
c
k(
^
Î²) / var(
^
Î²)
1
1.5
3
5
10
20
50
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
L2 Huber L1
Figure 6: Factor by which jackknife over-estimates the variance: boxplots of the
ratio of the jackknife estimate of the variance Î²b1 to the variance of Î²b1 as calculated
over 1000 simulations. Simulations were with normal design matrix X and normal
error i with values of n = 500. Note that because the L1 jackknife estimates
so wildly overestimate the variance, in order to put all the methods on the same
plot the boxplot of ratio is on log-scale; y-axis labels give the corresponding ratio
to which the log values correspond. For the median values of each boxplot, see
Table A-6 in AppendixI.
2005). Even for p/n = 0.01, the estimate is not unbiased for L1, with median estimates
twice as large as they should be and enormous variance in the estimates of variance. Higher
dimensions only worsen the behavior with jackknife estimates being 15 times larger than
they should.
4.1 Theoretical Results
Again, in the case of least-squares regression with a Gaussian design matrix, we can theoretically evaluate the behavior of the jackknife. The proof of the following theorem is given
in AppendixG (when the observations have covariance Id) and in AppendixH (to show how
to extend the results to general covariance).
Theorem 3 Let us call varJACK the jackknife estimate of variance of v
0Î²b given in (7),
where v is any deterministic vector with kvk2 = 1. Suppose the design matrix X is such
that Xi
iidv N (0, Î£), Î²b is computed using least-squares, and the errors  have a variance.
Then we have, as n, p â†’ âˆ and p/n â†’ Îº < 1,
E (varJACK)
var 
v
0Î²b
 â†’
1
1 âˆ’ Îº
.
24
Can We Trust the Bootstrap in High-dimensions?
As in Theorem 2, the proof of Theorem 3 is based on random matrix techniques where
further technical work should allow an extension for the entries of Xi,j to be i.i.d. from a
distribution other than Gaussian, provided Xi,j â€™s have sufficiently many moments. This is
also beyond the scope of our work, but interested readers can see AppendixG.3 for more
details.
Correcting the Jackknife in Least Squares Theorem 3 implies that scaling the jackknife
estimate of variance by multiplying it by 1 âˆ’ p/n will result in an estimate of var 
Î²b1

with
the correct expectation; simulations shown in Figure 7 confirm that confidence intervals
based on this corrected estimate of variance yield correct confidence intervals for leastsquares estimates of Î²b when the design matrix X is Gaussian. However this scaling factor
is not robust to violations of these assumptions. In particular when the X matrix follows
an elliptical distribution the correction of 1 âˆ’ p/n from Theorem 3 gives little improvement
even when the loss is still L2 (Figure 7).
Corrections for more general settings For the more general setting of an elliptical design
matrix X and loss function Ï, preliminary computations suggest an alternative result. Let
S be the random matrix defined by
S =
1
n
Xn
i=1
Ïˆ
0
(ei)XiX0
i
.
Then in our asymptotic regime, and when Î£ = Idp, preliminary heuristic calculations
suggest that we can estimate the amount by which E (varJACK) overestimates the variance
of Î²b1 by E (Ë†Î³), where
Î³Ë† ,
trace
S
âˆ’2

/p
[trace (Sâˆ’1) /p]
2
. (8)
Note that when applied to least-squares regression with X âˆ¼ N (0, Idp) this conforms
to our result in Theorem 3. Theoretical considerations suggest that in our asymptotics, for
smooth Ï, Ë†Î³ ' E (Ë†Î³), which suggests a data-driven correction to the jackknife estimate of
variance; however that correction depends having information about the distribution of the
design matrix.
Equation (8) assumes that the loss function can be twice differentiated, which is not
the case for either Huber or L1 loss. In the case of non-differentiable Ï and Ïˆ, we can use
appropriate regularizations to make sense of those functions. For Ï = Huberk, i.e., a Huber
function that transitions from quadratic to linear at |x| = k, Ïˆ
0
should be understood as
Ïˆ
0
(x) = 1|x|â‰¤k
. For L1 loss, Ïˆ
0
should be understood as Ïˆ
0
(x) = 1x=0.
In Figure 7 we show simulation results for confidence intervals created based on rescaling
the jackknife estimate of variance by E (Ë†Î³) defined in Equation (8). In the case of leastsquares with an elliptical design matrix, this correctionâ€”which directly uses the distribution
of the observed X matrixâ€”leads to a definite improvement in our jackknife confidence
intervals. Similarly, for the Huber loss we see a definite improvement as compared to the
standard jackknife estimate, as well as an improvement over the simpler correction of 1âˆ’p/n
that would be appropriate for squared error loss.
It should be noted that the quality of this proposed correction seems to depend on
how smooth is the function Ïˆ. In particular, even using the previous interpretations, the
2 
El Karoui and Purdom
0.00 0.02 0.04 0.06 0.08 0.10
95% CI Error Rate
â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
0.01
0.1
0.3
0.5
Normal X
Normal err
Normal X
Db Exp err
Normal X
Normal err
Normal X
Db Exp err
Ellip X
Normal err
Huber L2
Standard Jackknife
1âˆ’p/n Correction
Specific Correction
Figure 7: Rescaling jackknife estimate of variance: Shown are the error rates for
confidence intervals for different re-scalings of the jackknife estimate of variance:
the standard jackknife estimate (green); re-scaling using 1 âˆ’ p/n as given in
Theorem 3 for the L2 case with normal design matrix X (blue); and re-scaling
based on the heuristic in Equation (8) for those settings not covered by the
assumptions of Theorem 3 (magenta). The Huber loss in this plot is Huber1
rather than the default Huber1.345; Huber1 is further from L2 than Huber1.345
and therefore better shows the improvement gained by using the heuristic in
Equation (8).
correction does not perform well for L1 (at least for n = 1000 and Îº = 0.1, 0.3, 0.5, data
not shown) - though as we mentioned Figure 6 shows that jackknifing in L1-regression is
probably not a good idea; see also Koenker (2005, Section 3.9).
We also note that the assumption of cov (Xi) = Idp is essential to the jackknife correction
proposed in Equation (8). Let Î²bÏ(Î£) denotes our estimator of Î² when cov (Xi) = Î£. Ë†Î³ in
equation (8) is an estimate of
R(Î²bÏ) =
E

varJACK(v
0Î²bÏ(Idp))
var 
v
0Î²bÏ(Idp)
 .
Invariance arguments applied to cov (Xi) can be used to show that when Xi has an elliptical
distribution, then R(Î²bÏ(Idp)) = R(Î²bÏ(Î£)) for all convex loss functions Ï (see AppendixH).
26
Can We Trust the Bootstrap in High-dimensions?
However, even though this population quantity is unchanged when cov (Xi) = Î£ changes,
the estimate Ë†Î³ we give above depends crucially on knowing that cov (Xi) = Idp and cannot
be used as-is when Î£ 6= Idp.
5. Conclusion
In this paper, we have studied various resampling plans in the high-dimensional setting
where p/n is not close to zero. One of our main findings is that the two most widely-used
and advocated bootstraps will yield either highly conservative or highly anti-conservative
confidence intervals. This is in sharp contrast to the low-dimensional setting where p is
fixed and n â†’ âˆ or p/n â†’ 0. Under various assumptions underlying our simulations, we
explained theoretically the phenomena we were observing in our numerical work.
Beside our theoretical contributions, we propose improved and dimension-adaptive bootstrap methods for both pairs and residual bootstraps, as well as jackknife corrections, that
give confidence intervals with approximately correct coverage probability. These methods are novel dimensionality-robust resampling techniques for linear models. The resulting
modified pairs bootstrap gives a principled method for pairs bootstrapping regardless of the
value of p/n < 1, and avoids the problem of non-invertible bootstrapped design matrices
Xâˆ—
that commonly result from the standard pairs bootstrap. The most promising of our
proposed resampling schemes is our proposed residual bootstrapping method that resamples
from appropriately scaled predicted errors. This bootstrap routine performed well without
distribution-specific corrections that some of our other methods require. It has the greatest
potential to be a general-purpose bootstrap method for linear models in high dimensions.
This work has focused on estimation of the linear model, where there are theoretical
benchmarks. The real practical power of the bootstrap lays in giving the ability to perform
inference in complicated settings involving sophisticated statistical procedures for which
we do not even begin to have theoretical results for the behavior of our estimators. Yet
our work shows that even for the simple case of inference in the linear model and for the
simplest inferential question, the two most common and natural resampling techniques perform very poorly in only moderately high-dimensions. More importantly, these two equally
intuitive methods have completely divergent statistical behavior with one being extremely
conservative and the other anti-conservative. This casts serious doubts about the reliability, interpretability and accuracy of inferential statements made through generic resampling
methods in moderate and high dimensions. This is troubling for more complicated problems
where resampling techniques are the only inference tools currently available. Our findings
also suggest that appropriate resampling methods for high-dimensional problems may not
be able to rely on generic resampling procedures but rather need to be tailored to the statistical problem of interest. This raises many interesting new theoretical and methodological
questions for the future.

Appendix A. Technical Background on Existing Literature on Robust
Regression
Recall that we consider
Î²bÏ = argminuâˆˆRp
Xn
i=1
Ï(yi âˆ’ X0
iu) , where yi = i + X0
iÎ² .
The i
â€™s are assumed to be i.i.d with mean 0 here.
A.1 Classical Results and Asymptotic Normality
Least-squares In this case Ï(x) = x
2/2 and we have of course
Î²b
LS âˆ’ Î² = (X0X)
âˆ’1X0
 .
Hence,
cov 
Î²b
LS
= (X0X)
âˆ’1
var () .
Robust regression We recall the classic result of Huber (Huber, 1973) and (Huber and
Ronchetti, 2009), Chapter 7: when p is fixed and n â†’ âˆ, the limiting covariance of Î²bÏ is,
with a slight abuse of notation,
cov 
Î²bÏ

=
1
n

X0X
n
âˆ’1 E

Ïˆ
2
()

[E (Ïˆ0())]2
.
See also the papers Portnoy (1984, 1985, 1987); Mammen (1989) for the situation where
p â†’ âˆ and p/n â†’ 0 at various rates.
Asymptotic normality questions and impact on confidence intervals: p/n â†’ 0 In the case
of least-squares, the Lindeberg-Feller theorem (Stroock, 1993) guarantees that under mild
conditions on the p Ã— n matrix X, the coordinates of Î²b
LS are asymptotically Normal.
Similarly if the 1 Ã— n vector v
0
(X0X)
âˆ’1X0
satisfy the conditions of the the Lindeberg-Feller
theorem, then v
0
(Î²b
LS âˆ’ Î²) is asymptotically normal. Similarly, under mild conditions on
X, the classic papers mentioned above guarantee asymptotic normality of the coordinates
of Î²bÏ when p/n â†’ 0. In these cases, the width of confidence intervals for the coordinates of
Î² are hence only dependent asymptotically on the variance of the coordinates of Î²bÏ.
A.2 Summary of recent results on high-dimensional robust regression
We summarize in this section the key results we use from the recent papers El Karoui et al.
(2011); El Karoui et al. (2013); El Karoui (2013, 2017). The third paper is a mathematically
rigorous version of the heuristic arguments of the first two; the first paper is the long-form
version of the second one. Those papers are concerned with the asymptotic properties of
Î²bÏ when p/n â†’ Îº âˆˆ (0, 1). The predictor vectors Xi
â€™s are assumed to be random and
independent, with Xi = Î»iÎ£
1/2XËœ
i
, where XËœ
i has i.i.d (not necessarily Gaussian) entries
with mean 0 and variance 1. Î»i
â€™s are independent random variables with E

Î»
2
i

= 1. (The
  
Can We Trust the Bootstrap in High-dimensions?
nÃ—p design matrix X is full rank with probability 1. Î£ has only positive eigenvalues.) Xi
â€™s
are independent of i
â€™s.
Role of cov (Xi) = Î£ It is shown in these papers that, if Î²b(Î²; Î£) is the regression vector
corresponding to the situation where yi = X0
iÎ² + i and cov (Xi) = Î£ for all i,
Î²bÏ(Î²; Î£) = Î² + Î£âˆ’1/2Î²b(0; Idp) .
This follows from a simple change of variable. It also means that to understand the properties of Î²bÏ(Î²; Î£), it is enough to understand the â€œnull caseâ€ Î² = 0 and Î£ = Idp.
Consequence for leave-one-out-predicted errors The result we just mentioned has an important consequence for our leave-one-out predicted error, i.e Ëœei(i) = yi âˆ’ X0
iÎ²b
(i)
: Ëœei(i)
(Î²; Î£) =
eËœi(i)
(0; Idp). In other words, we can assume without loss of generality that Î² = 0 and
Î£ = Idp when working with leave-one-out-predicted errors.
A non-asymptotic and exact stochastic representation in the elliptical case When Xi
iidv Î»iÏ…i
,
where Ï…i âˆ¼ N (0, Î£) and Î»i
is a random variable independent of Ï…i
, it is shown that
Î²bÏ(Î²; Î£) L= Î² + kÎ²bÏ(0; Idp)k2Î£
âˆ’1/2u ,
where u is uniformly distributed on the unit sphere in R
p and kÎ²bÏ(0; Idp)k2 is independent
of u. kÎ²bÏ(0; Idp)k2 is simply the norm of Î²bÏ when Î² = 0 and cov (Xi) = Idp. Note that u
has the stochastic representation u
L= Zp/kZpk2, where Zp âˆ¼ N (0,Idp).
Consequence of the previous representation for large p Since kZpk2 has Ï‡p distribution, it
is clear that as p â†’ âˆ, if v is a deterministic vector,
âˆš
p
v
0
(Î²bÏ(Î²; Î£) âˆ’ Î²)
kÎ²bÏ(0; Idp)k2
=â‡’ N (0, v0Î£
âˆ’1
v) ,
where =â‡’ denotes weak convergence of distributions. Hence, provided kÎ²bÏ(0; Idp)k2 and
v
0Î£
âˆ’1v remain bounded, v
0Î²bÏ(Î²; Î£) is âˆšp-consistent for v
0Î².
Properties of kÎ²bÏ(0; Idp)k2 It is shown, under various technical assumptions, that as p and n
tend to infinity with p/n â†’ Îº, the variance of the random variable kÎ²bÏ(0; Idp)k2 goes to zero.
Hence, for practical matters, kÎ²bÏ(0; Idp)k2 can be considered non-random. In particular,
that implies that
âˆš
pv0
(Î²bÏ(Î²; Î£) âˆ’ Î²) is approximately Normal as p/n â†’ Îº .
Of great importance is the characterization of kÎ²bÏ(0; Idp)k2, since it will affect the width of
confidence intervals. It can be characterized, in the case where Î»i = 1 (see the papers for the
case Î»i 6= 1) in the following way: kÎ²bÏ(0; Idp)k2 â†’ rÏ(Îº). The non-random scalar rÏ(Îº) can
be characterized through a system of two non-linear equations, involving another constant,
c. The pair of positive and deterministic scalars (c, rÏ(Îº)) satisfy: if Ë†z =  + rÏ(Îº)Z, where
Z âˆ¼ N (0, 1) is independent of , and  has the same distribution as i
â€™s:

E ((prox(cÏ))0
(Ë†z)) = 1 âˆ’ Îº ,
Îºr2
Ï
(Îº) = E

[Ë†z âˆ’ prox(cÏ)(Ë†z)]2

.
(9)
3 
El Karoui and Purdom
In this system, prox(cÏ) refers to Moreauâ€™s proximal mapping of the convex function cÏ
- see Moreau (1965) or Hiriart-Urruty and LemarÂ´echal (2001). (The system is rigorously
shown in El Karoui (2013) under the assumption that the Xi
â€™s have i.i.d entries with mean
0 and variance 1, as well as a few other minor requirements; these assumptions are satisfied
when Xi,j have a Gaussian distribution, or are bounded, or do not have heavy tails, the
latter requiring appeal to various truncation arguments. Another proof of the validity of
this system, which first appeared in El Karoui et al. (2011), can be found in Donoho and
Montanari (2013). That proof is limited to the case of Xi
â€™s having i.i.d Gaussian entries.)
The assumptions on i
â€™s and Ï are relatively mild. See El Karoui (2017) for the latest,
handling the situation where i
â€™s have for instance a Cauchy distribution. We note that
some of the results in El Karoui (2013) are stated with Ï strongly convex (and i
â€™s having
many moments). While the proof in that paper suggests several ways of removing this
assumption, it is also possible to change Ï in to Ï+Î·x2/2 with Î· very small (e.g Î· = 10âˆ’100)
to satisfy this technical assumption and change essentially nothing to the statistical problem
at hand.
Consequences for the distribution of Î²b1 or other contrasts of interest In our simulation setup,
the previous results imply that the distribution of Î²b1 (or any other coordinates or contrasts
v
0Î²b for v deterministic) is asymptotically normal. In the case where Î£ = Idp, the variance of
âˆšp(Î²b1 âˆ’ Î²1) is roughly N (0, r2
Ï
(Îº)). See Bean et al. (2013) and its supplementary material
for a longer discussion and questions related to building confidence intervals.
Asymptotic normality questions and impact on confidence intervals: p/n â†’ Îº âˆˆ (0, 1)
Because we know that, in the Gaussian design case, the coordinates of Î²bÏ are asymptotically normal, the width of these intervals is completely determined by the variance of the
coordinates of Î²bÏ. We explain above how these variances depend on the distribution of  and
the loss function Ï: basically through kÎ²b(Ï; Id)k2 and hence rÏ(Îº). Therefore, as was the
case in the low-dimensional situation, the variance of the coordinates of Î²bÏ can be used as a
proxy for the width of the confidence interval in the high-dimensional case where p/n â†’ Îº,
0 < Îº < 1.
In (Bean et al., 2013), these asymptotic normality results are used to create confidence
intervals for v
0Î² in the Gaussian design case: if z1âˆ’Î±/2
is the (1 âˆ’ Î±/2)-quantile of the
Gaussian distribution a 100(1 âˆ’ Î±)% confidence interval for v
0Î² is
v
0Î²b Â±
z1âˆ’Î±/2
âˆšp
rË†
q
(1 âˆ’ p/n)v
0Î£bâˆ’1v ,
where Ë†r is a consistent estimator of kÎ²bÏ(0; Idp)k2. In (Bean et al., 2013), it is said without
more precision that leave-one-techniques can be used to come up with Ë†r; we propose in the
current paper estimates Ë†r based on leave-one-out predicted errors that can therefore be used
for the purpose of building those confidence intervals. (See Section 2.3 in the main paper)
Leave-one-out approximations for Î²b It is shown in the aforementioned papers that
Î²b ' Î²b
(i) +
1
n
S
âˆ’1
i XiÏˆ(ei) ,
where ' means that we are neglecting a quantity that is negligible for all our mathematical
and statistical purposes (see the papers for very precise bounds on the quantity we are
32
Can We Trust the Bootstrap in High-dimensions?
neglecting). This approximation is the key to the approximations in Equations (3) and (4)
which we use in the main paper. Recall that Si =
1
n
P
j6=i Ïˆ
0
(Ëœej(i)
)XjX0
j
.
A.3 Consequences for the residual bootstrap
We call {
âˆ—
i
}
n
i=1 the estimated errors used in the residual bootstrap. When doing a residual
bootstrap, we are effectively sampling from a model with fixed design X, â€œtrue Î²â€ taken to
be equal to Î²bÏ and i.i.d errors sampled according to the empirical distribution of the {
âˆ—
i
}
n
i=1.
As a shortcut, we call this distribution 
âˆ—
in what follows. We call Î²bâˆ—
Ï
the bootstrapped
version of Î²b.
Case p/n â†’ 0 Naturally, the classic results mentioned above imply that the distribution
of v
0
(Î²bâˆ—
Ï âˆ’ Î²bÏ) is going to be asymptotically normal (under mild conditions on X that are
satisfied in our simulations); the variance of the coordinates of Î²bâˆ—
Ï
, on the other hand depends
on
E(Ïˆ
2
(
âˆ—))
[E(Ïˆ0(
âˆ—))]2 . Hence, even if the distribution of the estimated errors 
âˆ—
is very different
from that of the â€œtrueâ€ errors, , the residual bootstrap may work very well: indeed, if 
and 
âˆ— have two very different distribution but
E

Ïˆ
2
(
âˆ—
)

[E (Ïˆ0(
âˆ—))]2
=
E

Ïˆ
2
()

[E (Ïˆ0())]2
,
using a residual bootstrap with â€œthe wrong error distributionâ€, 
âˆ—
, will give us bootstrap
confidence intervals of the right width. An important question then becomes, when p/n is
small: what class of distributions 
âˆ—
is such that E(Ïˆ
2
(
âˆ—))
[E(Ïˆ0(
âˆ—))]2 =
E(Ïˆ
2
())
[E(Ïˆ0())]2 , as this class defines
all acceptable error distributions from the point of view of our residual bootstrap.
Case p/n â†’ Îº âˆˆ (0, 1) We note that at this point in the case p/n â†’ Îº âˆˆ (0, 1) we are
not aware of central limit theorems for the coordinates of Î²b that are valid conditional on
the design matrix X. However, it is expected that such theorems will hold if the design
matrix results from a draw of a random design matrix similar to the ones we consider (with
very high-probability with respect to the sampling of the design matrix). The discussions
above make then clear that the key quantity to describe the width of the residual bootstrap
confidence intervals becomes the risk kÎ²bÏ(0; Idp; 
âˆ—
)k2, i.e the risk kÎ²bÏ(0; Idp)k2 when the
error distribution is 
âˆ—
. A â€œgoodâ€ error distribution is therefore one for which rÏ(Îº; 
âˆ—
) '
rÏ(Îº; ). (We used the notation rÏ(Îº; ) = limnâ†’âˆkÎ²bÏ(0; Idp; )k, when p/n â†’ Îº.)
The case of least squares Let us call GË†
n,p the distribution of the errors we use in our residual
bootstrap. We assume that GË†
n,p has mean 0. Let us call w
0 = v
0
(X0X)
âˆ’1X0
- where we
choose to not index v and w by p for the sake of clarity. v is a deterministic sequence of pdimensional vectors. Assume that w and GË†
n,p satisfy the conditions of the Linderberg-Feller
theorem for triangular arrays, and that limnâ†’âˆ var 
GË†
n,p
= Ïƒ
2

. Then the Lindeberg-Feller
theorem guarantees that
v
0
(Î²bâˆ— âˆ’ Î²b)
kwk
=â‡’ N (0, Ïƒ2

) .
  
El Karoui and Purdom
Note that it also guarantees, under the same assumptions on w that
v
0
(Î²b âˆ’ Î²)
kwk
=â‡’ N (0, Ïƒ2

) .
These results do not depend on the size of Îº, the limit of the ratio p/n.
Informally, what this means is that provided that the entries of w are all relatively small,
that GË†
n,p has mean 0 and var 
GË†
n,p
is close to Ïƒ
2

, then bootstrapping from the residuals
in least-squares works for approximating the distribution v
0
(Î²b âˆ’ Î²).
Conclusion for the purposes of the main paper In our discussions we use kÎ²bÏ(0; Idp; 
âˆ—
)k
and its closeness to its value under the correct error distribution, kÎ²bÏ(0; Idp; )k, as a proxy
to understand a priori the quality of residual bootstrap confidence intervals when using 
âˆ—
to sample the errors instead of . The previous discussion explains why we do so. Our
numerical work in Section 2.3 of the main text shows numerically that this yields valuable
insights. This is why our discussion in Section 2.4 is focused on understanding kÎ²b(0; Idp; )k2
for various error distributions. In particular, Theorem 1 shows that when p/n is close to 1,
if 
âˆ— has approximately the same two first moments as , kÎ²b(0; Idp; 
âˆ—
)k/kÎ²b(0; Idp; )k ' 1.
This explains why the scaled Ëœri(i)
is probably a good error distribution 
âˆ—
to use in the
residual bootstrap when Îº is close to 0 or 1. We note that when Îº is close to 1, Ëœri(i) gives an
error distribution that is in general very different from the distribution of . Our numerical
work of Section 2.3 shows that it is nonetheless a good error distribution from the point of
view of the residual bootstraps we consider.
Appendix B. Deconvolution Bootstrap
In the main text, we considered situations where our predictors Xi are i.i.d with an elliptical
distribution and assume for instance that Xi = Î»iÎ¾i
, where Î¾i âˆ¼ N (0, Î£) and Î»i are i.i.d
scalar random variables with E

Î»
2
i

= 1. As described in the main text, if X is elliptical,
eËœi(i)
is a convolution of the correct G distribution and a Normal distribution,
eËœi(i) ' i + ZËœ
i
,
where
ZËœ
i
iidv N (0, Î»2
i kÎ²b
Ï(i) âˆ’ Î²k
2
2
)
and are independent of i
.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Specifically, we proposed the
following bootstrap procedure:
1. Calculate the predicted errors, Ëœei(i)
2. Estimate |Î»i
|kÎ²b
Ï(i) âˆ’ Î²k2 (the standard deviation of the ZËœ
i)
3. Deconvolve in Ëœei(i)
the error term i from the ZËœ
i term ;
4. Use the resulting estimates of G as the estimate of GË† in residual bootstrapping.
3 
Can We Trust the Bootstrap in High-dimensions?
B.1 Estimating kÎ²bÏ âˆ’ Î²k and the Variance of the Zi
Deconvolution methods that deconvolve  from the ZËœ
i require an estimate of the variance
of the ZËœ
i
. Equation (3) gives the variance as Î»
2
i
kÎ²b
Ï(i) âˆ’ Î²k
2
2
, and we need to estimate this
quantity from the data. We use the approximation
kÎ²b
Ï(i) âˆ’ Î²k2 ' kÎ²bÏ âˆ’ Î²k2.
See AppendixA and references therein for justification of this approximation.
Furthermore, as we note in the main text, in our implementation of this deconvolution in
simulations we assume X âˆ¼ N (0, Idp) so that Î»i = 1 (see Section B.5 below for estimating
Î»i
in the elliptical case). This means we are estimating the variance of ZËœ
i as kÎ²b âˆ’ Î²k
2
2
for
all i. We estimate this as
var d(ZËœ
i) = var d(Ëœei(i)
) âˆ’ ÏƒË†
2

,
where var d(Ëœei(i)
) is the standard estimate of variance and Ë†Ïƒ
2

is the estimate of variance from
the least squares fit, Ïƒb
2
,LS, defined in the main text.
In the case where var d(Ëœei(i)
) â‰¤ ÏƒË†
2

, we do not do a deconvolution, but simply bootstrap
from the Ëœei(i)
. This is generally only the case when p/n is quite small.
B.2 Estimating GË†
We used the deconvolution algorithm in the decon package in R (Wang and Wang, 2011) to
estimate the distribution of i
. Deconvolution algorithms require selection of a bandwidth in
the kernels that make up the functional basis of the estimate. The appropriate bandwidth
parameter in deconvolution problems is tied intrinsically to the use of the estimate, with
optimal bandwidths depending on what functional of the distribution is wanted (e.g. the
pdf versus the cdf). Moreover, the optimal bandwidth depends on the distribution of ZËœ
i
with which the signal is being convolved. Ultimately, our procedure resamples from the
distribution GË†, requiring estimates of Gâˆ’1
(y), and the distribution of ZËœ
i
is Gaussian. There
is no specific theory for the optimal bandwidth in this setting (though see the work of Hall
and Lahiri (2008) for optimal bandwidth selection for estimations of the quantiles of GË†
if the ZËœ
i are distributed according to a distribution whose characteristic function decays
polynomially at infinity - see Assumption (A.11) on p.2133; this is clearly violated in our
case where ZËœ
i are normally distributed.)
We used the bandwidth estimation procedure bw.dboot2 provided in the package decon.
Delaigle (2014) outlines problems in the estimation of bandwidth parameter in decon; specifically that the implementation in decon of existing bandwidth estimation procedures does
not match their published descriptions. bw.dboot2 was not one of the bandwidth procedures with these discrepancies. However, we also compared our results with a bandwidth
selected via the bandwidth selection method of Delaigle and Gijbels (2002, 2004) and used
the R code implementation provided by the authors on http://www.ms.unimelb.edu.au/
~aurored/links.html#Code. The two different choices in bandwidth, however, had little
effect on the coverage of the bootstrap confidence intervals (Supplemental Figure 8). The
results in Figure 2 in the main text make use of the bandwidth parameter of Delaigle and
Gijbels (2002, 2004).
35
El Karoui and Purdom
0.00 0.02 0.04 0.06 0.08
Ratio (Îº)
95% CI Error Rate
â—
â—
â—
â—
â—
â—
â—
â—
0.01 0.30 0.50
â—
Delaigle
Decon
(a) L1 loss
0.00 0.02 0.04 0.06 0.08
Ratio (Îº)
95% CI Error Rate
â—
â—
â—
â—
â—
â— â—
â—
0.01 0.30 0.50
(b) Huber loss
Figure 8: Different bandwidths for Method 1: We plotted the error rate of 95% confidence intervals for the deconvolution bootstrap (Method 1) using two different
choices of bandwith: the bw.dboot2 in decon (light blue) or that of Delaigle and
Gijbels (2002, 2004) (maroon). The solid lines refer to bootstrapping by drawing
{
âˆ—
i
}
n
i=1 as i.i.d draws from GË†; the dashed lines refer to {
âˆ—
i
}
n
i=1 drawn from repeated resampling of a single draw ({Ë†i}
n
i=1) from GË†. See section B.4 below. Note
that the y-axis for these plots is different than that shown in the main text.
For both bandwidth selections, we estimated the cdf using the function DeconCdf provided in the decon package and provided the bandwidth parameters described above. We
specified the error distribution as â€˜Normalâ€™ and set the variance of ZËœ
i as described above in
Section B.1. The number of grid points for evaluating the cdf (the â€˜ngridâ€™ argument) was
set to be the number needed to get a space of 0.01 across the range of observed Ëœei(i)
, with
a lower bound of 512 grid points (the default of â€˜ngridâ€™ given by the DeconCdf function).
Other options were set to the default of DeconCdf.
B.3 Random Draws from GË†
The end result of the DeconCdf function was values of the GË† evaluated at specific grid points
x. The resulting GË†(x) was not always guaranteed to be â‰¤ 1 nor monotonically decreasing;
this is likely due to the fact that use of higher-order kernels estimates (which is standard
practice in deconvolution literature) does not constrain the estimate be a proper density.
Furthermore, the tail ends of the cdf are based on little data and unlikely to be reliable,
as well as having problems either non-monotonicity or extending beyond the boundaries of
(0, 1). We truncated the left tail of GË†(x) to be within 0.001 by finding the largest such x0
such that GË†(x0) â‰¤ 0.001 and setting GË†(x) = 0.001 for x â‰¤ x0; and we similarly trimmed
the right tail based on 1 âˆ’ 0.001. We then calculated the differences di = GË†(xi) âˆ’ GË†(xiâˆ’1)
36
Can We Trust the Bootstrap in High-dimensions?
and for di < 0 set di = 0. We then defined a monotone cdf based on the cumulative sum of
the di
,
C(xj ) = X
j
i=1
di
.
We then renormalized the values C(xj ) so that they extend from 0 to 1, giving the final
monotone estimate of GË†(xj ) as
GË†(xj ) = C(xj ) âˆ’ mini C(xi)
maxi C(xi) âˆ’ mini C(xi)
To randomly sample from GË†, we needed to be able to evaluate GË† for all x. We did this
by linearly interpolating between the GË†(xj ) values. In what follows, we consider the values
GË†(x) based on this smoothed and monotone version of the original output of the DeconCdf
function.
We create random draws from GË† by drawing random variables Ui from a Unif(0, 1) and
calculating Ei = GË†(Ui). We further centered and standardized the draws Ej from GË† to get

âˆ—
j = (Ej âˆ’ meani(Ej )) Ïƒb,LS
p
var (Ei)
so that the resulting 
âˆ—
j
have mean zero and variance Ïƒb
2
,LS. This was done because the
variance of GË† was not guaranteed to have the correct variance, despite the fact we had to
pre-specify the variance in the deconvolution call. Ensuring the correct moments of 
âˆ—
j was
a critical component for reasonable coverage of the bootstrap confidence intervals. When
we did not standardize the results and just took the draws from Ej , the resulting bootstrap
confidence intervals became more and more conservative as p/n grew. This again highlights
the results of Theorem 1 â€“ the variance of GË† is the most important feature of the distribution
in order to have accurate confidence intervals.
B.4 Bootstrap Estimates Î²bâˆ—
from GË†
We used GË† to create bootstrap errors, {
âˆ—
i
}
n
i=1 in two ways. For the first method we estimated
{
âˆ—
i
}
n
i=1 as i.i.d draws from GË†, and repeatedly drew such samples from GË†, B times. In the
second method, we drew one single estimate {Ë†i}
n
i=1 as i.i.d draws from GË† and then created
{
âˆ—
i
}
n
i=1 from resampling from the empirical distribution of the {Ë†i}
n
i=1, and repeated this
resampling from the empirical distribution of {Ë†i}
n
i=1 B times. For both methods, we then
calculated Î²bâˆ—
from the data (Xi
, yâˆ—
i
) where y
âˆ—
i = X0
iÎ²b + 
âˆ—
i
, as in the standard residual
bootstrap. The first method seems to do slightly better in simulations, see Figure 8.
B.5 Estimation of Î»
2
i
To extend the deconvolution bootstrapping method to the elliptical case when p/n â†’
Îº âˆˆ (0, 1), one needs to be able to estimate Î»i
, at least up to sign. In which case, one
could estimate individually the variance of ZËœ
i and feed these individual estimates into the
deconvolution method described above.
37
El Karoui and Purdom
We recall a simple proposal from the paper (El Karoui, 2010) to solve this problem.
Specifically, the author proposes to use
Î»bi
2
=
kX2
i
k2/p
1
p
trace 
Î£b
 =
kXik
2
2
1
n
Pn
i=1kXik
2
2
,
where Î£ = b 1
n
Pn
i=1 XiX0
i
. Under mild conditions on Î£ and Î»i
, it can be shown that when
n â†’ âˆ and p/n â†’ Îº âˆˆ (0, âˆ)
sup
1â‰¤iâ‰¤n
|Î»
2
i âˆ’ Î»c2
i
| â†’ 0 in probability.
The intuition and proof are as follows. Concentration of measure arguments (Ledoux, 2001)
show that if Î¾i âˆ¼ N (0, Î£), kÎ¾ik
2/p ' trace (Î£) /p and hence kXik
2/p ' Î»
2
i
trace (Î£) /p. The
law of large numbers and a little bit of further technical work then imply that 1
n
Pn
i=1kXik
2/p '
E

Î»
2
i

trace (Î£) /p = trace (Î£) /p.
Appendix C. Alternative Weight Distributions for Pairs Bootstrap
The formula given in Theorem 2 suggests resampling from a distribution FË† defined using
weights other than i.i.d. Poisson(1). An acceptable weight distribution is such that the
variance of the resampled estimator is equal to the variance of the sampling distribution
of the original estimator. In Section 3.2 we consider the least-squares estimator where the
variance, in the case where Î£ = Idp is asymptotically Îº/(1 âˆ’ Îº)Ïƒ
2
 /p.
In the main text, we proposed to use
wi
iidv 1 âˆ’ Î± + Î±Poisson(1)
We determined Î± numerically so that
ï£®
ï£°Îº
1
1 âˆ’ Îº âˆ’ Ewi
h
1
(1+cwi)
2
i âˆ’
1
1 âˆ’ Îº
ï£¹
ï£» =
Îº
1 âˆ’ Îº
. (10)
This was done via a simple dichotomous search for Î± over the interval [0, 1]. Our initial
Î± was .95. We specified a tolerance of 10âˆ’2
for the results reported in the paper in Table
1. This means that we stopped the algorithm when the ratio of the two terms in Equation
(10) was within 1% of 1. We used a sample size of 106
to estimate all the expectations.
Table 2 gives the values of Î±(Îº) found.
C.1 Case Î£ 6= Idp
In the case where Î£ 6= Id, both E

var 
v
0Î²bâˆ—
 and var 
v
0Î²b

depend on v
0Î£
âˆ’1v. It is
therefore natural to ask how we could estimate this quantity. If we are able to do so, it is
clear that we could follow the same strategy as above to find Î± from the data. Standard
Wishart results (Mardia et al. (1979), Theorem 3.4.7) give that
v
0Î£
âˆ’1v
v
0Î£bâˆ’1v
âˆ¼
Ï‡
2
nâˆ’p
n âˆ’ 1
â†’ (1 âˆ’ Îº)in probability.
3 
Can We Trust the Bootstrap in High-dimensions?
Îº 0.05 0.10 0.15 0.20 0.25
Î±(Îº) 0.9938 0.9875 0.9812 0.9688 0.9562
Îº 0.30 0.35 0.40 0.45 0.50
Î±(Îº) 0.9426 0.9352 0.9277 0.9222 0.9203
Table 2: Values of Î±(Îº) to use to fix the variance estimation issue in high-dimensional
pairs-bootstrap
This of course suggests using (1 âˆ’ p/n)v
0Î£bâˆ’1v as an estimator of v
0Î£
âˆ’1v and solves the
question we were discussing above.
However, we note that since
E

var 
v
0Î²bâˆ—

var 
v
0Î²b

does not depend on Î£ when the design is Gaussian or Elliptical, the same Î± should work
regardless of Î£, provided it is positive definite. In particular, an acceptable weight distribution for resampling as defined above could be computed by assuming Î£ = Idp and would
work for any positive definite Î£.
Appendix D. Description of Numerics
Here we describe the implementation of various computational numerics used in the paper.
D.1 Simulation Description
In the simulations described in the paper, we explored variations in the distribution of the
design matrix X, the error distribution, the loss function, the sample size (n), and the ratio
of Îº = p/n, detailed below.
All results in the paper were based upon 1, 000 replications of our simulation routine for
each combination of these values. Each simulation consisted of
1. Simulation of data matrix X, {i}
n
i=1 and construction of data yi = X0Î²+i
. However,
for our simulations, Î² = 0 (without loss of generality for the results, which are shift
equivariant), so yi = i
.
2. Estimate Î²Ë† using the corresponding loss function. For L2 this was via the lm command
in R, for Huber via the rlm command in the MASS package with default settings
(k = 1.345) (Venables and Ripley, 2002), and for L1 via an internal program making
use of MOSEK optimization package and accessed in R using the Rmosek package
(MOSEK, 2014). The internal L1 program was checked to give the same results as
the rq function that is part of the R package quantreg (Koenker, 2013), but was
much faster for simulations.
3. Bootstrapping according to the relevant bootstrap procedure (using the boot package)
and estimating Î²Ë†âˆ—
for each bootstrap sample. Each bootstrap resampling consisted of
39
El Karoui and Purdom
R = 1, 000 bootstrap samples, the minimum generally suggested for 95% confidence
intervals (Davison and Hinkley, 1997). For jackknife resampling and for calculating
leave-one-out prediction errors Ëœei(i)
, we wrote an internal function that left out each
observation in turn and recalculated Î²Ë†
(i)
.
4. Construction of confidence intervals for Î²Ë†
1. For bootstrap resampling, we used the
function boot.ci in the boot package to calculate confidence intervals. We calculated
â€œbasicâ€, â€œpercentileâ€, â€œnormalâ€, and â€œBCAâ€ confidence intervals (see help of boot.ci
and Davison and Hinkley (1997) for details about each of these), but all results shown
in the manuscript rely on only the percentile method. The percentile method calculates the boundaries of the confidence intervals as the estimates of 2.5% and 97.5%
percentiles of Î²Ë†âˆ—
1
(note that the estimate is not exactly the observed 2.5% and 97.5%
of Î²Ë†âˆ—
1
, since there is a correction term for estimating the percentile, again see Davison
and Hinkley (1997)). For the jackknife confidence intervals, the confidence interval
calculated was a standard normal confidence interval (Â±1.96q
var dJack(Î²Ë†
1))
D.2 Values of Simulation Parameters
Design Matrix For the design matrix X, we considered the following designs for the distribution of an element Xij of the matrix X, ensuring that the vectors Xi had covariance Idp
in all cases :
â€¢ Normal: Xij are i.i.d N(0, 1)
â€¢ Double Exp: Xij are i.i.d. double exponential with variance 1.
â€¢ Elliptical: Xij âˆ¼ Î»iZij where the Zij are i.i.d N(0, 1) and the Î»i are i.i.d according to
â€“ Î»i âˆ¼ Exp(
âˆš
2) (i.e. mean 1/
âˆš
2), so E

Î»
2
i

= 1
â€“ Î»i âˆ¼ N(0, 1)
â€“ Î»i âˆ¼ Unif(
q
12
13 0.5,
q
12
13 1.5) so that E

Î»
2
i

= 1
Error Distribution We used two different distributions for the i.i.d errors i
: N(0, 1) and
standard double exponential (with variance 2).
Dimensions We simulated from n = 100, 500, and 1, 000 though we showed only n = 500
in our results for simplicity. Except where noted, no significant difference in the results was
seen for varying sample size. The ratio Îº was simulated at 0.01, 0.1, 0.3, 0.5.
D.3 Details of Additional Numerics
In Tables A-1 to A-5 in AppendixI we give the precise numerical results from our simulations
that are plotted in both the main text and supplementary figures.
Calculating Correction Factors for Jackknife We computed these quantities using the formula we mentioned in the text and Matlab. We solve the associated regression problems
with cvx (Grant and Boyd, 2014, 2008), running Mosek (ApS, 2015) as our optimization
  
Can We Trust the Bootstrap in High-dimensions?
engine. We used n = 500 and 1, 000 simulations to compute the mean of the quantities we
were interested in.
Relative Risk (Figure 3) In Figure 3 we plot for both Huber loss and L1 loss the average
risk rÏ(Îº; Gconv) (i.e. errors given by Gconv) relative to the average risk rÏ(Îº; G) (i.e. errors
distributed according to G), where G has a double exponential distribution. We also plot
the relative average risk rÏ(Îº; Gnorm), where Gnorm = N(0, Ïƒ2

). The values for Figure 3
were generated with Matlab, using cvx and Mosek, as described above. We picked n = 500
and did 500 simulations. p was taken in (5, 10, 30, 50, 75, 100, 125, 150, 175, 200, 225,
250, 275, 300, 350, 400, 450). We used our simulations for the case of the original errors
to estimate E

kÎ²b âˆ’ Î²k2

. We used this estimate in our simulation under the convolved
error distribution. The Gaussian error simulations were made with N (0, 2) to match the
variance of the double exponential distribution.
Calculation of amount of variance overestimated in pairs bootstrap (Figure 5a)
In Figure 5a, we plot the theoretical factor by which the pairs bootstrap overestimates
the actual variance of Î²1Ï. This figure was generated by assuming Poisson(1) weights
and computing deterministically the expectations of interest. This was easy since if W âˆ¼
Poisson(1), P(W = k) = exp(âˆ’1)
k!
.
We truncated the expansion of the expectation at K = 100, so we neglected terms
of order 1/100! or lower only. The constant c was found by dichotomous search, with
tolerance 10âˆ’6
for matching the equation E (1/(1 + W c)) = 1 âˆ’ p/n. Once c was found, we
approximated the expectation in Theorem 2 in the same fashion as we just described.
Once we had computed the quantity appearing in Theorem 2, we divided it by Îº/(1âˆ’Îº).
We repeated these computations for Îº = .05 to Îº = .5 by increments of 10âˆ’3
to produce
our figure.
Appendix E. Proof of Theorem 1 (Residual bootstrap, p/n close to 1)
Proof
Recall the system describing the asymptotic limit of kÎ²bÏ âˆ’ Î²k when p/n â†’ Îº and the
design matrix has i.i.d mean 0, variance 1 entries, is, under some conditions on i
â€™s and
some mild further conditions on the design (see Section A above): kÎ²bÏ âˆ’ Î²k â†’ rÏ(Îº) and
the pair of positive and deterministic scalars (c, rÏ(Îº)) satisfy: if Ë†z =  + rÏ(Îº)Z, where
Z âˆ¼ N (0, 1) is independent of , and  has the same distribution as i
â€™s:

E ((prox(cÏ))0
(Ë†z)) = 1 âˆ’ Îº ,
Îºr2
Ï
(Îº) = E

[Ë†z âˆ’ prox(cÏ)(Ë†z)]2

.
In this system, prox(cÏ) refers to Moreauâ€™s proximal mapping of the convex function cÏ -
see Moreau (1965) or Hiriart-Urruty and LemarÂ´echal (2001).
We first give an informal argument to â€œguessâ€ the correct values of various quantities of
interest, namely c and of course, rÏ(Îº).
Note that when |x|  c, and when Ïˆ(x) âˆ¼ x at 0, prox(cÏ)(x) '
x
1+c
. Hence, x âˆ’
prox(cÏ)(x) ' xc/(1 + c). (Note that as long as Ïˆ(x) is linear near 0, we can assume
that Ïˆ(x) âˆ¼ x, since the scaling of Ï by a constant does not affect the performance of the
estimators.)
4 
El Karoui and Purdom
We see that 1 âˆ’ Îº ' 1/(1 + c), so that c ' Îº/(1 âˆ’ Îº) - assuming for a moment that we
can apply the previous approximations in the system. Hence, we have
ÎºrÏ(Îº)
2 ' (c/(1 + c))2
[rÏ(Îº)
2 + Ïƒ
2

] ' Îº
2
[rÏ(Îº)
2 + Ïƒ
2

] .
We can therefore conclude (informally at this point) that
rÏ(Îº)
2 âˆ¼
Ïƒ
2
 Îº
1 âˆ’ Îº
âˆ¼
Ïƒ
2

1 âˆ’ Îº
.
Once these values are guessed, it is easy to verify that rÏ(Îº)  c and hence all the
manipulations above are valid if we plug these two expressions in the system driving the
performance of robust regression estimators described above. We note that our argument is
not circular: we just described a way to guess the correct result. Once this has been done,
we have to make a verification argument to show that our guess was correct.
In this particular case, the verification is done as follows: we can rewrite the expectations
as integrals and split the domain of integration into (âˆ’âˆ, âˆ’sÎº), (âˆ’sÎº, sÎº), (sÎº, âˆ), with
sÎº = (1 âˆ’ Îº)
âˆ’3/4
. Using our candidate values for c and rÏ(Îº), we see that the corresponding zb has extremely low probability of falling outside the interval (âˆ’sÎº, sÎº) - recall that
1âˆ’Îº â†’ 0. Coarse bounding of the integrands outside this interval shows the corresponding
contributions to the expectations are negligible at the scales we consider. On the interval
(âˆ’sÎº, sÎº), we can on the other hand make the approximations for prox(cÏ)(x) we discussed
above and integrate them. That gives us the verification argument we need, after somewhat
tedious but simple technical arguments. (Note that the method of propagation of errors
in analysis described in (Miller, 2006) works essentially in a similar a-posteriori-verification
fashion. Also, sÎº could be picked as (1 âˆ’ Îº)
âˆ’(1/2+Î´)
for any Î´ âˆˆ (0, 1/2) and the arguments
would still go through.)
Appendix F. Proof of Theorem 2 (Expected Variance of the Pairs
Bootstrap Estimator)
In this section, we compute the expected variance of the bootstrap estimator.
We recall that for random variables T, Î“, we have
var (T) = var (E (T|Î“)) + E (var (T|Î“)) .
In our case, T = v
0Î²bw, the projection of the regression estimator Î²bw obtained using the
random weights w on the contrast vector v. Î“ represents both the design matrix and the
errors. We assume without loss of generality that kvk2 = 1.
Hence,
var 
v
0Î²bw

= var 
v
0E

Î²bw|Î“
 + E

var 
v
0Î²bw|Î“
 .
In plain English, the variance of v
0Î²bw is equal to the variance of the bagged estimator
plus the expectation of the variance of the bootstrap estimator (where we randomly weight
observation (yi
, Xi) with weight wi).
42
Can We Trust the Bootstrap in High-dimensions?
As explained in Section H, we can study without loss of generality the case where Î£ = Idp
and Î² = 0. This is what we do in this proof. Further the rotational invariance arguments
we give in Section H mean that we can focus on the case v = ep,the p-th canonical basis
vector, without loss of generality.
We consider the case where Xi
iidv N (0,Idp). This allows us to work with results in El
Karoui et al. (2011); El Karoui et al. (2013), El Karoui (2013).
Notational simplification To make the notation lighter, in what follows in this proof we use
the notation Î²b for Î²bw. There are no ambiguities as we are always using a weighted version
of the estimator and hence this simplification should not create any confusion.
In particular, we have, using the derivation of Equation (9) in El Karoui et al. (2013)
and noting that in the least-squares case all approximations in that paper are actually exact
equalities,
Î²bp = Ë†c
Pn
i=1 wiXi(p)ei,[p]
p
.
ei,[p] here are the residuals based on the first p âˆ’ 1 predictors, when Î² = 0. We note
that, under our assumptions on Xi
â€™s and wi
â€™s, Ë†c =
1
n
trace
S
âˆ’1
w

+ oL2
(1), where Sw =
1
n
Pn
i=1 wiXiX0
i
. It is known from work in random matrix theory (see e.g El Karoui (2009))
that 1
n
trace
S
âˆ’1
w

is asymptotically deterministic in the situation under investigation with
our assumptions on w and X, i.e 1
n
trace
S
âˆ’1
w

= c + oL2
(1), where c = E

1
n
trace
S
âˆ’1
w
.
We also recall the residuals representation from El Karoui et al. (2013), which are exact
in the case of least-squares : namely here,
Î²b âˆ’ Î²b
(i) =
wi
n
S
âˆ’1
i XiÏˆ(ei) ,
which implies that, with Si =
1
n
P
j6=i wjXjX0
j
,
eËœi(i) = ei + wi
X0
iS
âˆ’1
i Xi
n
Ïˆ(ei) .
In the case of least-squares, Ïˆ(x) = x, so that
ei =
eËœi(i)
1 + wici
,
where
ci =
X0
iS
âˆ’1
i Xi
n
.
These equalities also follow from simple linear algebra since we are in the least-squares
case. We note that ci = c + oP (1), where c is deterministic, as explained in e.g El Karoui
(2010), El Karoui (2013). Furthermore, here the approximation holds in L2 because of our
assumptions on wâ€™s and existence of moments for the inverse Wishart distribution - see e.g
Haff (1979). As explained in El Karoui (2013), the same is true for ci,[p] which is the same
quantity computed using the first (p âˆ’ 1) coordinates of Xi
, vectors we denote generically
by Vi
. We can rewrite
Î²bp = Ë†c
Pn
i=1 wiXi(p)
eËœi(i),[p]
1+wici,[p]
p
     
El Karoui and Purdom
Let us call bb the bagged estimate. We note that Ëœei(i),[p]
is independent of wi and so is ci,[p]
.
We have already seen that Ë†c is close to a constant, c. So taking expectation with respect to
the weights, we have, if w(i) denotes {wj}j6=i
, and using independence of the weights,
bbp =
1
p
Xn
i=1
Ewi

cwi
1 + cwi

Xi(p)Ew(i)

eËœi(i),[p]

[1 + oL2
(1)] .
Now the last term is of course the prediction error for the bagged problem, i.e
Ew(i)

eËœi(i),[p]

= i âˆ’ V
0
i
(gb(i) âˆ’ Î³)
where gb(i)
is the bagged estimate of Ë†Î³ and Ë†Î³ is the regression vector obtained by regressing
yi on the first p âˆ’ 1 coordinates of Xi
. (Recall that in these theoretical considerations we
are assuming that Î² = 0, without loss of generality.)
So we have, since we can work in the null case where Î³ = 0 (without loss of generality),
bbp =
1
p
Xn
i=1
Ewi

cwi
1 + cwi

Xi(p)

i âˆ’ V
0
i
gb(i)

(1 + oL2
(1)) .
Hence,
E

pbb
2
p

=
1
p
Xn
i=1

Ewi

cwi
1 + cwi
2
(Ïƒ
2
 + E

kgb(i)k
2
2

)(1 + o(1)) .
Now, in expectation, using e.g El Karoui (2013), E

kgb(i)k
2
2

(1 + o(1)) = E

kbbk
2
2

=
pE

bb
2
p

. The last equality comes from the fact that all coordinates play a symmetric role
in this problem, so they are all equal in law.
Now, recall that according to e.g El Karoui et al. (2013), top-right equation on p. 14562,
or El Karoui (2010)
1
n
Xn
i=1
1
1 + cwi
= 1 âˆ’
p
n
+ oL2
(1) ,
since the previous expression effectively relates trace
DwX(X0DwX)
âˆ’1X0

to n âˆ’ p, the
rank of the corresponding â€œhat matrixâ€.
Since cwi
1+cwi
= 1 âˆ’
1
1+cwi
, we see that
Ewi

cwi
1 + cwi

=
p
n
+ o(1) .
Hence, for the bagged estimate, we have the equation
E

kbbk
2
2

=
p
n

Ïƒ
2 + E

kbbk
2
2
 (1 + o(1)) .
We conclude that
E

kbbk
2
2

= (1 + o(1)) Îº
1 âˆ’ Îº
Ïƒ
2
.
Note that Îº
1âˆ’Îº
Ïƒ
2 = E

kÎ²b
sLSk
2
2

, where the latter is the standard (i.e non-weighted) least
squares estimator       
Can We Trust the Bootstrap in High-dimensions?
We note that the rotational invariance argument given in El Karoui et al. (2011);
El Karoui et al. (2013) still apply here, so that we have the
bb âˆ’ Î²
L= kbb âˆ’ Î²ku ,
where u is uniform on the sphere and independent of kbb âˆ’ Î²k (recall that this simply comes
from the fact that if Xi
is changed into OXi
, where O is orthogonal, bb is changed into
Obb - and we then apply invariance arguments coming from rotational invariance of the
distribution of Xi). Therefore,
var 
v
0
(bb âˆ’ Î²)

=
kvk
2
p
E

kbb âˆ’ Î²k
2
2

.
So we conclude that
pE

var 
v
0Î²bw|Î“
 = pvar 
v
0Î²bw

âˆ’
Îº
1 âˆ’ Îº
Ïƒ
2
kvk
2
2 + o(1) .
Now, the quantity var 
v
0Î²bw

is well understood. The rotational invariance arguments
we mentioned before give that
var 
v
0Î²bw

=
kvk
2
2
p
E

kÎ²bw âˆ’ Î²k
2
2

.
In fact, using the notation Dw for the diagonal matrix with Dw(i, i) = wi
, since
Î²bw âˆ’ Î² = (X0DwX)
âˆ’1X0Dw ,
we see that
E

kÎ²bw âˆ’ Î²k
2
2

= Ïƒ
2
 E

trace
(X0DwX)
âˆ’2X0Dw2X
 .
(Note that under mild conditions on , X and w, we also have kÎ²bwâˆ’Î²k
2
2 = E

kÎ²bw âˆ’ Î²k
2
2

+
oL2
(1) - owing to concentration results for quadratic forms of vectors with independent
entries; see Ledoux (2001).)
We now need to simplify this quantity.
Analytical simplification of trace
(X0DwX)
âˆ’2X0Dw2X

Of course,
trace
(X0DwX)
âˆ’2X0Dw2X

= trace
DwX(X0DwX)
âˆ’2X0Dw

=
Xn
i=1
w
2
i X0
i
(X0DwX)
âˆ’2Xi
.
Hence, if Î£bw =
1
n
Pn
i=1 wiXiX0
i ,
wi
n XiX0
i + Î£b
(i)
, we have
trace
(X0DwX)
âˆ’2X0Dw2X

=
1
n
Xn
i=1
w
2
i
X0
iÎ£bâˆ’2Xi
n
.
Call Î£( b z) = Î£b âˆ’ zIdp. Using the identity
(Î£b âˆ’ zIdp)(Î£b âˆ’ zIdp)
âˆ’1 = Idp      
El Karoui and Purdom
we see, after taking traces, that (Silverstein (1995))
1
n
Xn
i=1
wiX0
i
(Î£b âˆ’ zIdp)
âˆ’1Xi âˆ’ ztrace 
(Î£b âˆ’ zIdp)
âˆ’1

= p .
We call, for z âˆˆ C, c(z) = 1
n
trace 
(Î£b âˆ’ zIdp)
âˆ’1

and ci(z) = X0
i
(Î£b
(i)âˆ’zIdp)
âˆ’1Xi
, provided
z is not an eigenvalue of Î£. b
Differentiating with respect to z and taking z = 0 (we know here that Î£ is non-singular b
with probability 1, so this does not create a problem), we have
1
n
Xn
i=1
wiX0
iÎ£bâˆ’2Xi âˆ’ trace 
Î£bâˆ’1

= 0 .
Also, since, by the Sherman-Morrison-Woodbury formula (Horn and Johnson (1990)),
X0
iÎ£( b z)
âˆ’1Xi =
X0
iÎ£b
(i)
(z)
âˆ’1Xi
1 + wi
1
nX0
iÎ£b
(i)
(z)âˆ’1Xi
,
we have, after differentiating,
1
n
X0
iÎ£bâˆ’2Xi =
c
0
i
(0)
[1 + wici(0)]2
,
where of course c
0
i
(0) = X0
iÎ£bâˆ’2
(i) Xi
. Hence,
1
n
Xn
i=1
w
2
i
1
n
X0
iÎ£bâˆ’2Xi =
1
n
Xn
i=1
w
2
i
c
0
i
(0)
[1 + wici(0)]2
= c
0
(0) 1
n
Xn
i=1
w
2
i
[1 + wic(0)]2
.
(Note that the arguments given in e.g El Karoui (2010) or El Karoui and Koesters (2011)
for why ci(z) = c(z)(1 + oP (1)) extend easily to c
0
i
and c
0 given our assumptions on wâ€™s
and the fact that these functions have simple interpretations in terms of traces of powers
of inverses of certain well-behaved - under our assumptions - matrices.)
Going back to
1
n
Xn
i=1
wiX0
i
(Î£b âˆ’ zIdp)
âˆ’1Xi âˆ’ ztrace 
(Î£b âˆ’ zIdp)
âˆ’1

= p ,
and using the previously discussed identity
wi
n
X0
i
(Î£b âˆ’ zIdp)
âˆ’1Xi = 1 âˆ’
1
1 + wici(z)
,
we have
n âˆ’
Xn
i=1
1
1 + wici(z)
âˆ’ znc(z) = p .
46
Can We Trust the Bootstrap in High-dimensions?
In other words,
1 âˆ’ Îº =
1
n
Xn
i=1
1
1 + wici(z)
+ zc(z) .
Now,
c(z)
1
n
Xn
i=1
wi
1 + wic(z)
=
1
n
Xn
i=1
(1 âˆ’
1
1 + wic(z)
)
=
1
n
Xn
i=1
(1 âˆ’
1
1 + wici(z)
) + Î·(z)
= Îº + zc(z) + Î·(z) ,
where Î·(z) is such that Î·(z) = oP (1) and Î·
0
(z) = oP (1) (Î· has an explicit expression which
allows us to verify these claims). Therefore, by differentiation, and after simplifications,
1
n
X
wi
1 + wic(0)2
c
0
(0) = Îº
c
0
(0)
[c(0)]2
âˆ’ 1 + oP (1) .
Hence,
trace
(X0DwX)
âˆ’2X0Dw2X

=
ï£®
ï£°Îº
trace 
Î£bâˆ’2
w

/n
[trace 
Î£bâˆ’1
w

/n]
2
âˆ’ 1
ï£¹
ï£» + oP (1) .
The fact that we can take expectations on both sides of this equation and that oP (1) is
in fact oL2
(1) come from our assumptions about wi
â€™s - especially the fact that they are
independent and bounded away from 0 - and properties of the inverse Wishart distribution.
Conclusion We can now conclude that a consistent estimator of the expected variance
of the bootstrap estimator is
kvk
2
2
p
Ïƒ
2

ï£®
ï£°Îº
trace 
Î£bâˆ’2
w

/n
[trace 
Î£bâˆ’1
w

/n]
2
âˆ’
1
1 âˆ’ Îº
ï£¹
ï£» .
Using the fact that
1 âˆ’ Îº =
1
n
Xn
i=1
1
1 + wic(z)
+ zc(z) ,
we see that, since 1
n
trace 
Î£bâˆ’2
w

= c
0
(0),
1
n
trace 
Î£bâˆ’2
w

=
c(0)
1
n
Pn
i=1 wi/(1 + wic(0))2
.
We further note that asymptotically, when wi are i.i.d and satisfy our assumptions,
c(0) â†’ c, which solves:
Ewi

1
1 + wic

= 1 âˆ’ Îº .
4 
El Karoui and Purdom
Hence, asymptotically, when wi
â€™s are i.i.d and satisfy our assumptions, we have
trace 
Î£bâˆ’2
w

/n
[trace 
Î£bâˆ’1
w

/n]
2
â†’
1
cEwi
[wi/(1 + wic)
2]
.
Since cwi/(1 + cw2
i
) = 1/(1 + cwi) âˆ’ 1/(1 + cwi)
2
, we finally see that
cEwi

wi
(1 + wic)
2

= Ewi

1
1 + cwi

âˆ’ Ewi

1
(1 + cwi)
2

,
= 1 âˆ’ Îº âˆ’ Ewi

1
(1 + cwi)
2

.
So asymptotically, the expected bootstrap variance is equivalent to, when kvk2 = 1,
Ïƒ
2

p
ï£®
ï£°Îº
1
1 âˆ’ Îº âˆ’ E

1
(1+cwi)
2
 âˆ’
1
1 âˆ’ Îº
ï£¹
ï£» ,
where E

1
1+cwi

= 1 âˆ’ Îº.
In particular, when wi = 1, we see, unsurprisingly, that the above quantity is 0, as it
should, given that the bootstrapped estimate does not change when resampling.
We finally make note of a technical point, that is addressed in papers such as El Karoui
(2010, 2013) and on which we rely here by using those papers. Essentially, theoretical
considerations regarding quantities such as 1
p
trace 
Î£bâˆ’k
w

are easier to handle by working
rather with 1
p
trace 
(Î£bw + Ï„ Idp)
âˆ’k

, for some Ï„ > 0. In the present context, it is easy to
show (and done in those papers) that this approximation allows us to take the limit - even
in expectation - for Ï„ â†’ 0 in all the expressions we get for Ï„ > 0 and that that limit is
indeed E

1
p
trace 
Î£bâˆ’k
w
. Technical details rely on using the first resolvent identity (Kato,
1995), using moment properties of inverse Wishart distributions and using the fact that
wi
â€™s are bounded below.
F.1 Extension: Elliptical Design
In this case, we have XËœ
i = Î»iXi
, where Xi âˆ¼ N (0,Idp) and yi = i + XËœ0
iÎ². We assume
Î»i 6= 0 for all i, E

Î»
2
i

= 1, Î»i
â€™s are i.i.d and bounded away from 0.
We can go through the proof of Theorem 2 and make necessary adjustments.
Of course, we have
E

kÎ²bw âˆ’ Î²k
2
2

= Ïƒ
2
 E

trace 
(XËœ0DwXËœ)
âˆ’2XËœ0Dw2XËœ
 .
If we reformulate this expression in terms of X we get
E

kÎ²bw âˆ’ Î²k
2
2

= Ïƒ
2
 E

trace
(X0DÎ»2wX)
âˆ’2X0DÎ»2w2X
 .   
Can We Trust the Bootstrap in High-dimensions?
So this quantity is affected by the distribution of Î»i
â€™s; hence the risk of Î²bw is different in
the Gaussian and elliptical design case.
The other important part of the proof is the computation of the risk of the bagged
estimator. In this case, earlier work in random matrix theory (e.g El Karoui (2009); El
Karoui and Koesters (2011)) shows that we can use the approximations
ci ' Î»
2
i
c ,
where c = limn,pâ†’âˆ E

1
n
trace
S
âˆ’1
, where S =
1
n
Pn
i=1 Î»
2
i wiXiX0
i
, i.e S =
1
nX0DÎ»2wX.
If we call
g(Î»
2
i
) = Ewi

cÎ»2
i
1 + cÎ»2
i wi

,
we see by keeping track of changes in the earlier proof that we have asymptotically
E

k
Ë†bk
2
2

=
EÎ»i
(Î»
2
i
g
2
(Î»
2
i
))
Îº âˆ’ EÎ»i
(Î»
4
i
g
2(Î»
2
i
))Ïƒ
2

.
The same arguments we used before give that
E

g(Î»
2
i
)

= Îº .
Based on this information, we can compute E

var 
v
0Î²bâˆ—
w
 as we had in the proof of
Theorem 2 and compare it to var 
v
0Î²b

. The expressions do not seem to simplify much
further however in this case, by contrast to the Gaussian design case where Î»i = 1 for all i.
(For instance, when Î»i=1 for all iâ€™s, g(Î»i) = g(1) = Îº and we recover the results of Theorem
2.)
Importantly, the characteristics of the distribution of Î»i that affect E

var 
v
0Î²bâˆ—
w
 go
beyond E

Î»
2
i

. And hence the expression we gave in Theorem 2 wonâ€™t apply directly to
the elliptical case.
F.2 Extension: Multinomial(n, 1/n) weights
A natural question is whether the computations we have made can be extended to wi
â€™s that
are i.i.d P oisson(1) and/or Multinomial(n, 1/n), as in the standard bootstrap.
In both cases, technical issues arise because with asymptotically negligible but non-zero
probability, the matrix X0DwX may be of rank less than p. This can handled in several
ways. A simple one is to replace the weights wi by wi(Ï„ ) = Ï„ + (1 âˆ’ Ï„ )wi and study the
problem when Ï„ â†’ 0.
Beyond that technicality, an important question is whether one can handle the fact that
the weights are dependent in the multinomial case. For quantities of the type 1
n
trace
(X0DwX)
âˆ’1

,
it was argued in El Karoui (2010) that one could ignore the dependency issue and treat
the problem as if the weights where i.i.d Po(1). This type of arguments would be easy to
extend where we need them here, for instance in quantities that arise in the computation
of E

kÎ²bw âˆ’ Î²k
2
2

or to show that we can write ci = c + oP (1), where c is deterministic     
El Karoui and Purdom
The remaining question is therefore the characterization of the risk of the bagged estimator. We have, with a slight modification with respect to the case of independent weights,
Ë†bp =
1
p
Xn
i=1
Xi(p)

Ewi

cwi
1 + cwi

i âˆ’ V
0
i Ew

Î³Ë†(i)
cwi
1 + cwi
 (1 + oP (1)) .
As before, Ewi

cwi
1+cwi

= p/n + o(1). The problem is the dependence between Ë†Î³(i) and
wi
. The rotational invariance arguments we invoked before still hold, so that Ë†Î³i = kÎ³Ë†(i)k2u,
where u is uniform on the unit sphere and independent of kÎ³Ë†ik2. It is also independent of
Vi
, since Ë†Î³(i)
is the leave-one-out estimate of Î³. The same rotational invariance arguments
hold for the bagged estimate EwÎ³Ë†(i)
cwi
1+cwi
. Hence, after a little bit of work we see that
E

[V
0
i Ew

Î³Ë†(i)
cwi
1 + cwi

]
2

= E

kEw

Î³Ë†(i)
cwi
1 + cwi

k
2
2

.
Using the fact that w(i)
|wi âˆ¼ Multinomial(nâˆ’wi
, 1/(nâˆ’1)), the only real technical hurdle is
to show that Ewi
Î³Ë†(i)
is asymptotically deterministic and independent of wi
. A strategy for
this is to create a coupling: one can compare Ë†Î³(i)
to gË†(i)
, where gË†(i)
is computed using a nâˆ’1
dimensional vector of weights with distribution Multinomial(n âˆ’ 1, 1/(n âˆ’ 1)) - i.e running
wi âˆ’ 1 multinomial trials after having obtained w(i)
(the case wi = 0 is easy to handle
separately). Clearly, the distribution of gË†(i)
is independent of wi
, by construction. On the
other hand, a bit of work on top of the leave-one-observation-out expansions shows that
kgË†(i) âˆ’Î³Ë†(i)k
2
2
is roughly of size at most w
2
i
/n â†’ 0. Furthermore, kEw(gË†(i)
)âˆ’Ew(Ë†Î³(i)
)k2 â†’ 0
for the same reason. This suggests that further technical work along those lines will give
that
Ew

Î³Ë†(i)
cwi
1 + cwi

' Ew

Î³Ë†(i)

Ew

cwi
1 + cwi

,
where ' means that the approximation is valid in Euclidean norm. The same coupling
arguments will give that
kEw

Î³Ë†(i)

k ' kË†bk ,
where Ë†b is the bagged estimator. This will yield the same results as in the i.i.d Po(1) case.
Numerical results We verified that our theoretical results (i.e Theorem 2) hold for Poisson(1)
weights in limited simulations (note that in this case wi = 0 is possible). For Gaussian design
matrix, double exponential errors, and ratios Îº = .1, .3, .5 we found that the ratio of the
observed bootstrap expected variance of Î²bâˆ—
1
to our theoretical prediction using Poisson(1)
weights was 1.0027, 1.0148, and 1.0252, respectively (here n = 500, and there were R = 1000
bootstrap resamples for each of 1000 simulations).
Appendix G. Proof of Theorem 3 (Jackknife Variance)
As explained in Section H, we can study without loss of generality the case where Î£ = Idp
and Î² = 0. This is what we do in this proof.
We study it in details in the least-squares case, and postpone a detailed analysis of the
robust regression case to future studies.
  
Can We Trust the Bootstrap in High-dimensions?
According to the approximations in El Karoui et al. (2013), which are exact for least
squares, or classic results Weisberg (2014) we have:
Î²b âˆ’ Î²b
(i) =
1
n
Î£bâˆ’1
(i) Xiei
.
Recall also that
ei =
eËœi(i)
1 + 1
nX0
iÎ£bâˆ’1
(i) Xi
.
Hence,
v
0
(Î²b âˆ’ Î²b
(i)
) = 1
n
v
0Î£bâˆ’1
(i) Xi
eËœi(i)
1 + 1
nX0
iÎ£bâˆ’1
(i) Xi
.
Hence,
n
Xn
i=1
[v
0
(Î²b âˆ’ Î²b
(i)
)]2 =
1
n
Xn
i=1
[v
0Î£bâˆ’1
(i) XieËœi(i)
]
2
[1 + 1
nX0
iÎ£bâˆ’1
(i) Xi
]
2
.
Note that at the denominator, we have
1 +
1
n
X0
iÎ£bâˆ’1
(i) Xi = 1 +
1
n
trace 
Î£bâˆ’1

+ oP (1) ,
= 1 +
p
n
1
1 âˆ’ p/n + oP (1) = 1
1 âˆ’ p/n + oP (1) .
by appealing to standard results about concentration of high-dimensional Gaussian random
variables, and standard results in random matrix theory and classical multivariate statistics
(see Mardia et al. (1979); Haff (1979)). By the same arguments, this approximation works
not only for each i but for all 1 â‰¤ i â‰¤ n at once. The approximation is also valid in
expectation, using results concerning Wishart matrices found for instance in Mardia et al.
(1979).
For the numerator, we see that
Ti = v
0Î£bâˆ’1
(i) XieËœi(i) = v
0Î£bâˆ’1
(i) Xi(i âˆ’ X0
i
(Î²b
(i) âˆ’ Î²)) .
Since i
is independent of Xi and Î£b
(i)
, we see that
E

T
2
i

= E


2
i

E

(v
0Î£bâˆ’1
(i) Xi)
2

+ E

[X0
i
(Î²b
(i) âˆ’ Î²)]2
[v
0Î£bâˆ’1
(i) Xi
]
2

.
If Î± and Î² are fixed vectors, Î±
0Xi and Î²
0Xi are Gaussian random variables with covariance
Î±
0Î², since we are working under the assumption that Xi âˆ¼ N (0,Idp). It is easy to check
that if Z1 and Z2 are two Gaussian random variables with covariance Î³ and respective
variances Ïƒ
2
1
and Ïƒ
2
2
, we have
E

(Z1Z2)
2

= Ïƒ
2
1Ïƒ
2
2 + 2Î³
2
.
We conclude that
E

(a
0Xi)
2
(b
0Xi)
2

= kak
2
2kbk
2
2 + 2(a
0
b)
2
.    
El Karoui and Purdom
We note that
E

[v
0Î£bâˆ’1
(i) Xi
]
2

= E

v
0Î£bâˆ’2
(i)
v

.
Classic Wishart computations give (Haff (1979), p.536 (iii)) that as n, p â†’ âˆ,
E

Î£bâˆ’2
(i)

=

1
(1 âˆ’ p/n)
3
+ o(1)
Idp .
Hence, in our asymptotics,
E

(v
0Î£bâˆ’1
(i) Xi)
2

â†’
1
(1 âˆ’ p/n)
3
kvk
2
2
.
We also note that
E
h
(v
0Î£bâˆ’1
(i)
Î²b
(i)
)
2
i
=
1
n
v
0Î£bâˆ’3
(i)
v .
Hence,
E

(v
0Î£bâˆ’1
(i)
Î²b
(i)
)
2

= o(1)in our asymptotics .
Therefore,
E

T
2
1

=
1
(1 âˆ’ p/n)
3
kvk
2
2Ïƒ
2

(1 + p/n
1 âˆ’ p/n) + o(1)
since E

kÎ²b
(i) âˆ’ Î²k
2
2

= Ïƒ
2

p/n
1âˆ’p/n + o(1).
When v = e1, we therefore have
E

T
2
1

= Ïƒ
2

1
(1 âˆ’ p/n)
4
+ o(1) .
Therefore, in that situation,
E

n
Xn
i=1
(v
0
(Î²b
(i) âˆ’ Î²b)
2
)
!
= Ïƒ
2

1
(1 âˆ’ p/n)
2
+ o(1) .
In other words,
E
 Xn
i=1
(v
0
(Î²b
(i) âˆ’ Î²b)
2
)
!
=

1
1 âˆ’ p/n + o(1)
var 
Î²b1

G.1 Dealing with Centering
Let us call Î²b
(Â·) =
1
n
Pn
i=1 Î²b
(i)
. We have previously studied the properties of Pn
i=1([v
0
(Î²b âˆ’
Î²b
(i)
)]2
) and now need to show that the same results apply to Pn
i=1([v
0
(Î²b
(Â·) âˆ’ Î²b
(i)
)]2
).
To show that replacing Î²b by Î²b
(Â·) does not affect the result, we consider the quantity
n
2
[v
0
(Î²b âˆ’ Î²b
(Â·)
)]2
.
Since Î²b âˆ’ Î²b
(i) =
1
nÎ£bâˆ’1
(i) Xiei
, we have
Î²b âˆ’ Î²b
(Â·) =
1
n2
Xn
i=1
Î£bâˆ’1
(i) Xiei
.
  
Can We Trust the Bootstrap in High-dimensions?
Hence,
n
2
[v
0
(Î²b âˆ’ Î²b
(Â·)
)]2 =
"
1
n
Xn
i=1
v
0Î£bâˆ’1
(i) Xi(i âˆ’ X0
i
(Î²b âˆ’ Î²))#2
.
A simple variance computation gives that 1
n
Pn
i=1 v
0Î£bâˆ’1
(i) Xii â†’ 0 in L
2
, since each term has
mean 0 and the variance of the sum goes to 0.
Recall now that
Î£bâˆ’1Xi =
Î£bâˆ’1
(i) Xi
1 + ci
,
where all ci
â€™s are equal to p/n/(1 âˆ’ p/n) + oP (1). Let us call c = p/n/(1 âˆ’ p/n).
We conclude that
1
n
Xn
i=1
v
0Î£bâˆ’1
(i) XiX0
i
(Î²b âˆ’ Î²) = v
0
(Î²b âˆ’ Î²)(1 + c + o(1)) .
When v is given, we clearly have v
0
(Î²b âˆ’ Î²) = oP (p
âˆ’1/2
), given the distribution of Î²b âˆ’ Î²
under our assumptions on Xi
â€™s and i
â€™s. So we conclude that
n
2
[v
0
(Î²b âˆ’ Î²b
(Â·)
)]2 â†’ 0 in probability .
Because we have enough moments, the previous result is also true in expectation.
G.2 Putting Everything Together
The jackknife estimate of variance of v
0Î²b is up to a factor going to 1
n
n âˆ’ 1
JACK(var 
v
0Î²b

) = Xn
i=1
[(v
0Î²b
(i) âˆ’ Î²b
(Â·)
)]2
=
Xn
i=1
[(v
0Î²b
(i) âˆ’ Î²b)]2 + n[v
0
(Î²b âˆ’ Î²b
(Â·)
)]2
.
Our previous analyses therefore imply (using v = e1) that
n
n âˆ’ 1
E

JACK(var 
Î²b1

)

=

1
1 âˆ’ p/n + o(1)
var 
Î²b1

.
This completes the proof of Theorem 3
G.3 Extension: More Involved Designs and Different Loss Functions
Our approach could be used to analyze similar problems in the case of elliptical designs.
However, in that case, it seems that the factor that will appear in quantifying the amount
by which the variance is mis-estimated will depend in general on the ellipticity parameters.
We refer to El Karoui (2013) for computations of quantities such as v
0Î£bâˆ’2v in that case,
which are of course essential to measuring mis-estimation.
We obtained the possible correction we mentioned in the paper for these more general
settings following the ideas used in the rigorous proof we just gave, as well as approximation
53
El Karoui and Purdom
arguments given in El Karoui et al. (2013) and justified rigorously in El Karoui (2013).
Checking fully rigorously all the approximations we made in this Jackknife computation
would require a very large amount of technical work, and since this is tangential to our
main interests in this paper, we postpone that to a future work of a more technical nature.
It is also clear, since all these results and the proof we just gave rely on random matrix
techniques, that a similar analysis could be carried out in the case where Xi,j are i.i.d
with a non-Gaussian distribution, provided that distribution has enough moments (see e.g
Pajor and Pastur (2009) or El Karoui and Koesters (2011) for examples of such techniques,
actually going beyond the case of i.i.d entries for the design matrix). The main issues in
carrying out this program seem to be technical and not conceptual at this point, so we leave
this problem to possible future work.
Appendix H. Going from Î£ = Idp to Î£ 6= Idp
As discussed in Section A, we have
Î²bÏ(yi
; Xi
; i) âˆ’ Î² = Î£âˆ’1/2Î²bÏ(i
; Î£âˆ’1/2Xi
; i) ,
In other words, Î²bÏ(Ëœyi
; Î£âˆ’1/2Xi
; i) is the robust regression estimator in the null case where
Î² = 0 and Xi
is replaced by XËœ
i = Î£âˆ’1/2Xi
. Of course, if cov (Xi) = Î£, cov 
XËœ
i

= Idp.
H.1 Consequences for the Jackknife
Naturally the same equality applies to leave-one-out estimators. So, with the notations of
Equation (7) in the main text, we have, when span({Xi}
n
i=1) = R
p and Î£ is positive definite,
(v
0
[Î²b
(i) âˆ’ Î²Ëœ])2 = (v
0Î£
âˆ’1/2
[Î²b
(i)
(i
; Î£âˆ’1/2Xi
; i) âˆ’ Î²Ëœ(i
; Î£âˆ’1/2Xi
; i)])2
.
Let us call Î²bÏ(Î£; Î²) our robust regression estimator when cov (Xi) = Î£ and E (yi
|Xi) = X0
iÎ².
It is clear from the previous display that the properties of varJACK(v
0Î²bÏ(Î£; Î²)) are the
same as those of varJACK(v
0Î£
âˆ’1/2Î²bÏ(Idp; 0)). So understanding the null case is enough to
understand the general case, which is why we focus on the null case in our computations.
Furthermore, by the same arguments, we have
var 
v
0Î²bÏ(yi
; Xi
; i)

= var 
v
0Î£
âˆ’1/2Î²bÏ(Idp; 0)
.
So we have
varJACK(v
0Î²bÏ(Î£; Î²))
var 
v
0Î²bÏ(Î£; Î²)
 =
varJACK(v
0Î£
âˆ’1/2Î²bÏ(Idp; 0))
var 
v
0Î£âˆ’1/2Î²bÏ(Idp; 0) .
Calling u1 = Î£âˆ’1/2v/kÎ£
âˆ’1/2vk, we see that u1 is a unit vector. And we finally see that
varJACK(v
0Î²bÏ(Î£; Î²))
var 
v
0Î²bÏ(Î£; Î²)
 =
varJACK(u
0
1Î²bÏ(Idp; 0))
var 
u
0
1
Î²bÏ(Idp; 0) .
54
Can We Trust the Bootstrap in High-dimensions?
Hence, characterizing varJACK(v
0Î²bÏ(Idp;0))
var(v
0Î²bÏ(Idp;0))
for all fixed unit vectors v characterizes
varJACK(v
0Î²bÏ(Î£; Î²))
var 
v
0Î²bÏ(Î£; Î²)

for all Î² and invertible Î£. This is why our proof is focused on the null case Î£ = Idp and
Î² = 0.
H.2 Consequences for the Pairs Bootstrap
Let us call Dw the diagonal matrix with (i, i)-entry D(i, i) = wi
. We consider only the case
where wi > 0, so we do not have to consider the case where fewer than p Xi
â€™s are assigned
positive weights - which would result in Î²bÏ being ill-defined (since infinitely many solutions
would then be feasible).
In particular, for least squares, we have in our setting
Î²bw = (X0DwX)
âˆ’1X0DwY = Î² + (X0DwX)
âˆ’1X0Dw .
More generally, by a simple change of variables, since wi > 0 and span({Xi}
n
i=1) = R
p
,
when Î£ is invertible,
Î²bw,Ï(yi
; {Xi}
n
i=1; i) âˆ’ Î² = Î£âˆ’1/2Î²bw,Ï(i
; Î£âˆ’1/2Xi
; i) .
If bÏ is the corresponding bagged estimate, obtained by averaging Î²bw,Ï over wâ€™s, we also
have
bÏ(yi
; {Xi}
n
i=1; i) âˆ’ Î² = Î£âˆ’1/2
bÏ(i
; Î£âˆ’1/2Xi
; i) .
Hence, we also have
Î²bw,Ï(yi
; {Xi}
n
i=1; i) âˆ’ bÏ(yi
; {Xi}
n
i=1; i) = Î£âˆ’1/2
h
Î²bw,Ï(i
; Î£âˆ’1/2Xi
; i) âˆ’ bÏ(i
; Î£âˆ’1/2Xi
; i)
i
We further note that since yi = i + X0
iÎ², yi = i + (Î£âˆ’1/2Xi)
0Î£
1/2Î² and hence
Î²bw,Ï(yi
; Î£âˆ’1/2Xi
; i) = Î£1/2Î² + Î²bw,Ï(i
; Î£âˆ’1/2Xi
; i) .
The previous equation clearly implies that, if v is a fixed vector and u1 = Î£âˆ’1/2v
v
0
(Î²bâˆ—
Ï
(yi
; {Xi}
n
i=1; Î²) âˆ’ bÏ(yi
; {Xi}
n
i=1; Î²))
= u
0
1
h
Î²bâˆ—
Ï
(yi
; Î£âˆ’1/2Xi
; i) âˆ’ bÏ(yi
; Î£âˆ’1/2Xi
; i)
i
,
= u
0
1
h
Î²bâˆ—
Ï
(i
; Î£âˆ’1/2Xi
; i) âˆ’ bÏ(i
; Î£âˆ’1/2Xi
; i)
i
.
We note that if cov (Xi) = Î£, the last line in the previous display corresponds to the
bootstrap distribution of our estimator in the null case where Î² = 0 and Î£ = Idp, but v has
been replaced by u1 = Î£âˆ’1/2v. This shows that understanding the bootstrap properties of
v
0
(Î²bâˆ—
Ï âˆ’ bÏ) in the null case cov (Xi) = Î£ and Î² = 0 gives the result we seek in the general
case of Î£ 6= Idp and Î² 6= 0. (Here we centered our estimator around the bagged estimator,
55
El Karoui and Purdom
because it is natural when computing bootstrap variances. The arguments above show that
many other centering choices are possible, however.)
The last small issue that one needs to handle is the fact that our computations are done
for v with unit norm and u1 may not have unit norm. This is easily handled by simply
scaling by the deterministic ku1k. In particular, it is easy to see through simple scaling
arguments that
E

var 
v
0Î²bâˆ—
Ï
(Î£; Î²)

var 
v
0Î²bÏ(Î£; Î²)
 =
E

var 
uËœ1
0Î²bâˆ—
Ï
(Idp; 0)
var 
uËœ
0
1
Î²bÏ(Idp; 0) ,
where Ëœu1 = u1/ku1k has unit norm.
H.3 Rotational Invariance Arguments and Consequences
Motivated by the arguments in the previous two subsections, we now consider the null case
where Î² = 0 and cov (Xi) = Idp. Note that then yi = i
. Also, if Xi
is replaced by OXi
,
where O is an orthogonal matrix, and Î²b is replaced by OÎ²b. In other words,
Î²bÏ(i
; {OXi}
n
i=1; i) = OÎ²bÏ(i
; {Xi}
n
i=1; i) , .
Note that when the design matrix is such that OXi
L= Xi for all i (i.e the distribution of
Xi
â€™s is invariant by rotation),
Î²bÏ(i
; {OXi}
n
i=1; i)
L= Î²bÏ(i
; {Xi}
n
i=1; i) .
When wi > 0 for all i, we see that exactly the same arguments apply to Î²bw,Ï(i
; {Xi}
n
i=1; i)
and hence Î²bâˆ—
Ï
(i
; {Xi}
n
i=1; i). In particular, for any orthogonal matrix O, since Xi
L= OXi
,
E

var 
v
0Î²bâˆ—
Ï
(i
; {Xi}
n
i=1; i)
 = E

var 
v
0Î²bâˆ—
Ï
(i
; {OXi}
n
i=1; i)

= E

var 
v
0OÎ²bâˆ—
Ï
(i
; {Xi}
n
i=1; i)
 .
This implies that for any unit vector v, we have, if e1 is the first canonical basis vector,
E

var 
v
0Î²bâˆ—
Ï
(i
; Xi
; i)
 = E

var 
e
0
1Î²bâˆ—
Ï
(i
; Xi
; i)
 .
Indeed, we just need to take O to be such that O0v = e1 to prove the above result.
In the case where Xi
â€™s are i.i.d N (0,Idp), we do have Xi
L= OXi
, so the arguments above
apply. Therefore, to understand E

var 
v
0Î²bâˆ—
Ï
 in this case it is sufficient to understand
E

var 
e
0
1Î²bâˆ—
Ï
. This latter case is the case tackled in the proof of Theorem 2. (These
rotational invariance arguments are closely related to those in El Karoui et al. (2013).)
56
Can We Trust the Bootstrap in High-dimensions?
Appendix I. Supplementary Tables & Figures
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
(a) L1 loss
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
â—
â—
â—
â—
(b) Huber loss
â— â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
â—
â—
â—
â—
â—
â—
Residual
Jackknife
Pairs
Std. Residuals
(c) L2 loss
Figure A-1: Performance of 95% confidence intervals of Î²1 (double exponential
error): Here we show the coverage error rates for 95% confidence intervals
for n = 500 with the error distribution being double exponential (with Ïƒ
2 = 2)
and i.i.d. normal entries of X. See the caption of Figure 1 for more details.
57
El Karoui and Purdom
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
â—
â—
â—
â—
â—
â—
Residual
Jackknife
Pairs
Std. Residuals
(a) Normal X
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
â— â— â—
â—
(b) Ellip. X, Unif
â—
â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
â—
â—
â—
â—
(c) Ellip. X, N(0, 1)
â— â—
â—
â—
0.00 0.05 0.10 0.15 0.20
Ratio (Îº)
95% CI Error Rate
0.01 0.30 0.50
â— â—
â—
â—
(d) Ellip. X, Exp
Figure A-2: Performance of 95% confidence intervals of Î²1 for L2 loss (elliptical design X): Here we show the coverage error rates for 95% confidence intervals for n = 500 with different distributions of the design matrix X using ordinary least squares regression: (a) N(0, 1), (b) elliptical with
Î»i âˆ¼ U(
q
12
13 0.5,
q
12
13 1.5), (c) elliptical with Î»i âˆ¼ N(0, 1), and (d) elliptical
with Î»i âˆ¼ Exp(
âˆš
2). In all of these plots, the error is distributed N(0, 1) and
the loss is L2. See the caption of Figure 1 for additional details.
58
Can We Trust the Bootstrap in High-dimensions?
Residual Jackknife Pairs
Îº =0.01 0.063 0.089 0.035
Îº =0.1 0.113 0.005 0.013
Îº =0.3 0.137 0.000 0.003
Îº =0.5 0.210 0.000 0.000
(a) L1 loss
Residual Jackknife Pairs
Îº =0.01 0.057 0.054 0.054
Îº =0.1 0.068 0.037 0.041
Îº =0.3 0.090 0.015 0.004
Îº =0.5 0.198 0.002 0.000
(b) Huber loss
Residual Jackknife Pairs
Îº =0.01 0.040 0.061 0.040
Îº =0.1 0.060 0.034 0.052
Îº =0.3 0.098 0.021 0.033
Îº =0.5 0.188 0.005 0.000
(c) L2 loss
Table A-1: Error rate of 95% confidence intervals of Î²1 for n = 500 This table gives
the exact error rates plotted in Figure 1. Îº = p/n indicates the ratio of p/n
used in the simultation for this and future tables. See Figure 1â€™s caption for
more details.
Normal Ellip. Normal Ellip. Exp
Îº =0.01 1.001 1.001 1.017
Îº =0.1 1.016 1.090 1.156
Îº =0.3 1.153 1.502 1.655
Îº =0.5 1.737 3.123 3.635
Table A-2: Ratio of CI Width of Pairs compared to Standard. This table gives the
ratio of the average width of the confidence intervals from pairs bootstrapping
to the average for the standard interval given by theoretical results, i.e. using
var(Î²Ë†) = Ïƒ
2
(X0X)
âˆ’1 and creating standard confidence interval. These values
were used for Figure 4 in the text.
59
El Karoui and Purdom
Residual Std. Pred Error Deconv
Îº =0.01 0.064 0.042 0.031
Îº =0.1 0.091 0.028 0.018
Îº =0.3 0.135 0.026 0.022
Îº =0.5 0.182 0.030 0.035
(a) L1 loss
Residual Std. Pred Error Deconv
Îº =0.01 0.065 0.048 0.036
Îº =0.1 0.051 0.054 0.039
Îº =0.3 0.098 0.035 0.037
Îº =0.5 0.174 0.034 0.036
(b) Huber loss
Table A-3: Error rate of 95% confidence intervals using predicted errors. This
table gives the exact error rates plotted in Figure 2. See figure caption for more
details.
Residual Jackknife Pairs
Îº =0.01 0.064 0.073 0.032
Îº =0.1 0.091 0.002 0.005
Îº =0.3 0.135 0.001 0.001
Îº =0.5 0.182 0.000
(a) L1 loss
Residual Jackknife Pairs
Îº =0.01 0.065 0.061 0.059
Îº =0.1 0.051 0.042 0.027
Îº =0.3 0.098 0.009 0.009
Îº =0.5 0.174 0.001 0.000
(b) Huber loss
Residual Jackknife Pairs
Îº =0.01 0.052 0.052 0.052
Îº =0.1 0.056 0.036 0.045
Îº =0.3 0.114 0.018 0.022
Îº =0.5 0.155 0.008 0.002
(c) L2 loss
Table A-4: Error rate of 95% confidence intervals of Î²1 for double exponential
error This table gives the exact error rates plotted in Figure A-1. See figure
caption for more details.
60
Can We Trust the Bootstrap in High-dimensions?
Residual Jackknife Pairs
Îº =0.01 0.057 0.052 0.053
Îº =0.1 0.071 0.047 0.056
Îº =0.3 0.118 0.017 0.018
Îº =0.5 0.171 0.007 0.001
(a) Ellipical, Unif
Residual Jackknife Pairs
Îº =0.01 0.041 0.046 0.047
Îº =0.1 0.061 0.034 0.036
Îº =0.3 0.098 0.005 0.006
Îº =0.5 0.177 0.002 0.000
(b) Elliptical, Normal
Residual Jackknife Pairs
Îº =0.01 0.059 0.041 0.060
Îº =0.1 0.063 0.011 0.025
Îº =0.3 0.115 0.005 0.002
Îº =0.5 0.157 0.000 0.000
(c) Elliptical, Exp
Table A-5: Error rate of 95% confidence intervals of Î²1 for elliptical design X This
table gives the exact error rates plotted in Figure A-2. See figure caption for
more details.
L2 Huber L1
Îº =0.01 0.964 0.991 2.060
Îº =0.1 1.115 1.173 5.432
Îº =0.3 1.411 1.613 10.862
Îº =0.5 1.986 2.671 14.045
(a) Jackknife
L2 Huber L1
Îº =0.01 1.078 0.923 1.081
Îº =0.1 1.041 1.098 1.351
Îº =0.3 1.333 1.954 2.001
Îº =0.5 2.808 4.507 3.156
(b) Pairs Bootstrap
Table A-6: Over estimation of variance for Pairs bootstrap and Jackknife This
table gives the median values of the boxplots plotted in Figures 5 and 6. See
relevant figure captions for more details.