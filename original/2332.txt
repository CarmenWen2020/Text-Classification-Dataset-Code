The multi-armed restless bandit problem is studied in the case where the pay-off distributions are
stationary Ï•-mixing. This version of the problem provides a more realistic model for most real-world
applications, but cannot be optimally solved in practice, since it is known to be PSPACE-hard. The
objective of this paper is to characterize a sub-class of the problem where good approximate solutions
can be found using tractable approaches. Specifically, it is shown that under some conditions on
the Ï•-mixing coefficients, a modified version of UCB can prove effective. The main challenge is
that, unlike in the i.i.d. setting, the distributions of the sampled pay-offs may not have the same
characteristics as those of the original bandit arms. In particular, the Ï•-mixing property does not
necessarily carry over. This is overcome by carefully controlling the effect of a sampling policy on
the pay-off distributions. Some of the proof techniques developed in this paper can be more generally
used in the context of online sampling under dependence. Proposed algorithms are accompanied
with corresponding regret analysis.
1. Introduction
As one of the simplest examples of sequential optimization under uncertainty, multi-armed bandit
problems arise in various modern real-world applications, such as online advertisement, and Internet
routing. These problems are typically studied under the assumption that the pay-offs are independently and identically distributed (i.i.d.), and the arms are independent. However, this assumption
does not necessarily hold in many practical situations. Consider, for example, the problem of online
advertisement in which the aim is to garner as many clicks as possible from a user. Grouping adverts
into categories and associating with each category an arm, this problem turns into a multi-armed
bandit. There is dependence over time and across the arms since, for example, we expect a user to be
more likely to select adverts that are related to her selections in the recent past.
In this paper, we consider the multi-armed bandit problem in the case where the pay-offs are
dependent and each arm evolves over time regardless of whether or not it is played. This is an instance
of the so-called restless bandit problem (Whittle, 1988; Guha et al., 2010; Ortner et al., 2014). Since
in this setting an optimal policy can leverage the inter-dependencies between the samples and switch
between the arms at appropriate times, it can obtain an overall pay-off much higher than that given
by playing the best arm, i.e. the distribution with the highest expected pay-off, see Example 1
in (Ortner et al., 2014). However, finding the best such switching strategy is PSPACE-hard, even
in the case where the process distributions are Markovian with known dynamics (Papadimitriou
c 2019 Steffen GrÃ¼newÃ¤lder and Azadeh Khaleghi.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
http://jmlr.org/papers/v20/17-547.html.
GRÃœNEWÃ„LDER AND KHALEGHI
and Tsitsiklis, 1999). Therefore, it is useful to consider relaxations of the problem with the aim
to devise computationally tractable solutions that effectively approximate the optimal switching
strategy. Approximations of the Markovian restless bandit problem under known dynamics have
been previously considered, see, e.g. (Guha et al., 2010) and references therein. Our focus in this
paper is on a more general setting, where the rewards have unknown distributions, exhibit long-range
dependencies, and have Markov chains as a special case.
We are interested in a sub-class of the restless bandit problem, where the pay-offs have long-range
dependencies. Since the nature of the problem calls for finite-time analysis, we further require that
the dependence weakens over time, so as to make use of concentration inequalities in this setting.
To this end, a natural approach is to assume that the pay-off distributions are stationary Ï•-mixing.
The so-called Ï•-mixing coefficients Ï•n, n âˆˆ N, of a sequence of random variables hXtitâˆˆN measure
the amount of dependence between the sub-sequences of hXtitâˆˆN separated by n time-steps. A
process is said to be Ï•-mixing if this dependence vanishes with n. This notion is more formally
defined in Section 2. In Markov chains Ï•-mixing coefficients are closely related to mixing times.
The mixing time of a Markov chain is a measure of how fast its distribution approaches the stationary
distribution. In particular, it is defined to be the time that it takes for the distribution to be within
1/4
th of the stationary distribution as measured in total variation distance (Levin et al., 2008)[Sec
4.5]. A classical result by Davydov shows that the decrease in distance of the distribution of the
Markov chain to the stationary distribution is controlled (up to a factor of 1/2) by the Ï•-mixing
coefficients of the Markov chain (Doukhan, 1994)[pp.88]. While Ï•-mixing coefficients are related
to the well-studied mixing properties of Markov chains, Ï•-mixing processes correspond to a wide
variety of stochastic processes of which Markov chains are a special case (Doukhan, 1994).
As discussed earlier, the optimal, yet notoriously infeasible strategy for this version of the
problem is to switch between the arms. In this paper, we first address the question of when a
relaxation obtained by identifying the arm with the highest stationary mean would lead to a viable
approximation in this setting. For this purpose, we characterize the approximation error in terms of
the amount of dependence between the pay-offs, and show that if Ï•1 is small, the optimum of the
relaxed problem is close to that given by the optimal switching strategy. Observe that this condition
translates directly to the pay-off distributions being weakly dependent. Next, we address the question
of how an optimistic approach can be devised to identify the best arm. To this end, we propose a
UCB-type algorithm and show that it achieves logarithmic regret with respect to the highest stationary
mean. Interestingly, the amount of dependence in the form of P
i Ï•i appears in the bound, and in the
case where the pay-offs are i.i.d., we recover the regret bound of Auer et al. (2002).
A familiar real-world example for the bandit problem considered in this paper corresponds
to recommendation systems where the objective is to present personalized adverts, news-feeds or
Massive Open Online Course (MOOC) material to each user. In these cases, depending on the specific
application, each bandit arm could correspond to an appropriate subject category, indicating, for
example, a genre of products or a class of topics. Once an arm is played, an item of the corresponding
category could be selected at random and presented to the user. There are long-range dependencies
between the pay-offs as reflected by the usersâ€™ memory of their past observations. However, the
dependence naturally decays over time, while usersâ€™ short-term memory, which in turn influences the
dominant mixing coefficients, can be controlled by capping the maximum frequency at which each
specific recommendation is given to users. With minor modifications, this example carries over to a
larger class of real-world sequential decision making problems which naturally possess a dependency
structure imposed by usersâ€™ memory.
2
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Note that even this relaxed version of the problem is far from straightforward. The main challenge
lies in obtaining confidence intervals around empirical estimates of the stationary means. Since
Hoeffding-type concentration bounds exist for Ï•-mixing processes, it may be tempting to use such
inequalities directly with standard UCB algorithms designed for the i.i.d. setting, to find the best arm.
However, as we demonstrate in the paper, unlike in the i.i.d. setting, a policy in this framework may
introduce strong couplings between past and future pay-offs in such a way that the distribution of
the sampled sequence may not even be Ï•-mixing. This is the reason why a standard UCB algorithm
designed for i.i.d. settings is not suitable here, even when equipped with a concentration bound for
Ï•-mixing processes. In fact, this oversight seems to have occurred in previous literature, specifically
in the Improved-UCB-based approach of Audiffren and Ralaivola (2015). We refer to Section 4.1
for a more detailed discussion. We circumvent these difficulties by carefully taking random times1
into account and controlling the effect of a policy on the pay-off distributions. Some of our technical
results can be more generally used to address the problem of online sampling under dependence.
Finally, while the study of the multi-armed bandit problem with strongly dependent pay-offs
at its full generality is beyond the scope of this paper, we provide a complementary example for
this regime. Specifically, we consider a setting where the bandit arms are governed by stationary
Gaussian processes with slowly decaying covariance functions. Such high-dependence scenarios
are quite common in practice. For instance, the throughput of radio channels changes slowly over
time and the problem of choosing the best channel can be modeled by a bandit problem with strongly
dependent pay-offs. The intuitive reason why in this setting it may also be possible to efficiently
obtain approximately optimal solutions is that the strong dependencies can allow for the prediction of
future rewards even from scarce observations. We give a simple switching strategy for this instance
of the problem and show that it significantly outperforms a policy that aims for the best arm. Our
regret bound for this algorithm directly reflects the dependence between the pay-offs: the higher the
dependence the lower the regret.
A summary of our main contributions is listed below.
i. In an attempt to derive a computationally tractable solution for the restless bandit problem, we
first identify a case where the optimal switching strategy can be approximated by playing the
arm with the highest stationary mean. To this end, we show that the loss of settling for the
highest stationary mean as opposed to finding the best switching strategy is controlled by the
amount of inter-dependence as reflected by Ï•1. This is shown in Proposition 9.
ii. We provide a detailed example, namely Example 1, where we demonstrate the challenges
in the non-i.i.d. bandit problem. In particular, we show that a policy in this framework may
introduce strong couplings between past and future pay-offs in such a way that the resulting
pay-off sequence may have a completely different dependency structure.
iii. We develop technical machinery to circumvent the difficulties introduced by the inter-dependence
between the rewards, and allow us to control the effect of a policy on the pay-off distributions.
Some of our derivations concerning sampling under dependence can be of independent interest.
We further propose a UCB-type algorithm, namely Algorithm 1 that deploys these tools to
identify the arm with the highest stationary mean.
1. These correspond to random variables which determine the time at which an arm is sampled.
3
GRÃœNEWÃ„LDER AND KHALEGHI
iv. We provide an upper bound on the regret of Algorithm 1 (with respect to the highest stationary
mean). This is provided in Theorem 12. The regret bound is a function of the amount of
inter-dependence as reflected by the Ï•-mixing coefficients, and in the case where the pay-offs
are i.i.d., we recover the regret bound of Auer et al. (2002)[Thm. 1]. This result along with
Proposition 9 allow us to argue that in the case where dependence is low Algorithm 1 can be
used to approximate the best switching strategy.
The remainder of the paper is organized as follows. In Section 2 we introduce preliminary
notation and definitions. We formulate the problem in Section 3 and give our main results in
Section 4. We conclude in Section 5 with a discussion of open problems.
2. Preliminaries
We start with some useful notation before discussing basic definitions concerning stochastic processes.
Since the bandit problem involves multiple arms (processes), we extend the definition of a Ï•-mixing
process to what we call a jointly Ï•-mixing process, to be able to model the multi-armed bandit
problem. Indeed, in many natural settings, the process is jointly Ï•-mixing. For example, as we
demonstrate in Proposition 5, independent Markov chains are jointly Ï•-mixing.
Notation. Let N+ := {1, 2, . . .} and N := N âˆª {âˆ} denote the set and extended set of natural
numbers respectively. We introduce the abbreviation am..n, m, n âˆˆ N+, m â‰¤ n, for sequences
am, am+1, . . . , an. Given a finite subset C âŠ‚ N+ and a sequence a, we let aC := {ai
: i âˆˆ C}
denote the set of elements of a indexed by C. If XC is a sequence of random variables indexed by
C âŠ‚ N+, we denote by Ïƒ(XC) the smallest Ïƒ-algebra generated by XC.
Notion of Ï•-dependence. Part of our results concern the so-called Ï•-dependence between Ïƒalgebras defined as follows, see, e.g. (Doukhan, 1994).
Definition 1 Consider a probability space (â„¦, A, P) and let U and V denote Ïƒ-subalgebras of A
respectively. The Ï•-dependence between U and V is is given by
Ï•(U, V) := sup{|P(V ) âˆ’ P(V |U)| : U âˆˆ U, P(U) > 0, V âˆˆ V}.
If X and Y are two random variables measurable with respect to A we simplify notation by letting
Ï•(X, Y ) := Ï•(Ïƒ(X), Ïƒ(Y )) denote the Ï•-dependence between their corresponding Ïƒ-algebras;
distinction will be clear from the context. Similarly, if XA and XB are finite sequences of random
variables, with A, B âŠ‚ N+ their Ï•-dependence can be similarly defined as
Ï•(XA, XB) := Ï•(Ïƒ(XA), Ïƒ(XB)).
In words, Ï•(XA, XB) measures the maximal difference between the probability of an event V and
its conditional probability given an event U, where U and V are determined by random variables
indexed by A and B respectively. The notion of Ï•-dependence carries over from probability measures
to expectations. In particular, consider a real-valued random variable X defined on some probability
space (â„¦, A, P), and denote by G some collected information in the form of a Ïƒ-subalgebra of A.
Let E(X|G) denote Kolmogorovâ€™s conditional expectation, i.e. a G-measurable random variable Z
such that R
B
Z =
R
B X for all B âˆˆ G. As follows from Theorem 2 below, due to Bradley (2007)[vol.
1 pp. 124], the difference between E(X|G) and EX is effectively upper-bounded by Ï•(G, Ïƒ(X)).
4
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Theorem 2 (Bradley (2007)) Let (â„¦, A, P) be a probability space, let X be a real-valued random
variable with kXk1 < âˆ and let G be some Ïƒ-subalgebra of A. Then
2Ï•(G, Ïƒ(X)) = sup kE(Y |G) âˆ’ E(Y )k1/kY k1, (1)
where the supremum is taken over all Ïƒ(X)-measurable random variables Y with kY k1 < âˆ.
Furthermore, for any B âˆˆ G it holds that
Z
B
|E(X|G) âˆ’ E(X)| dP â‰¤ 2P(B)kXkâˆÏ•(G, Ïƒ(X)). (2)
Observe that because X is trivially Ïƒ(X)-measurable (1) given in the theorem implies that
kE(X|G) âˆ’ E(X)k1 â‰¤ 2kXk1Ï•(G, Ïƒ(X)).
Stochastic Processes & Ï•-mixing Properties. Let (X , BX ) be a measurable space; we let X âŠ‚
[0, 1] 2
and denote by B
(m)
X
the Borel Ïƒ-algebra on X
m, m âˆˆ N+. We denote by X âˆ the set of all
X -valued infinite sequences indexed by N+. A stochastic process can be modeled as a probability
measure over the space (X âˆ, B) where B denotes the Ïƒ-algebra on X âˆ generated by the cylinder
sets. Associated with the stochastic process is a sequence of random variables X1, X2, . . ., where
Xt
: X âˆ â†’ X is the projection onto the tâ€™th element, i.e. Xt(Ï‰) = Ï‰t for Ï‰ âˆˆ X âˆ and t âˆˆ N+.
A process Ï is stationary if Ï(X1..m âˆˆ B) = Ï(Xi+1..i+m âˆˆ B) for all Borel sets B âˆˆ B(m)
X
,
i, m âˆˆ N+. The term stochastic process refers to either the process distribution Ï or the associated
sequence of random variables Xt
, t âˆˆ N+; reference will be clear from the context.
Definition 3 (Stationary Ï•-mixing Process) Consider a stationary stochastic process hXiiiâˆˆN+
.
Its Ï•-mixing coefficients are given by
Ï•n := sup
u,vâˆˆN+
Ï•(X1..u, Xu+n..u+n+vâˆ’1), n âˆˆ N+
and measure the Ï•-dependence between Ïƒ(X1..u) and Ïƒ(Xu+n..u+n+vâˆ’1)
X1, . . . , Xu â† gap of length n â†’ Xu+n, . . . , Xu+n+vâˆ’1
The process is said to be Ï•-mixing if limnâ†’âˆ Ï•n = 0.
When modeling a bandit problem in this paper, we are concerned with some k âˆˆ N+ stochastic
processes with a joint distribution that is stationary Ï•-mixing. More specifically, for a fixed k âˆˆ N,
let (â„¦, A, P) be a probability space where â„¦ := â„¦1 Ã— . . . Ã— â„¦k with â„¦i
:= X âˆ, i âˆˆ 1..k, P a
probability measure and A obtained via the cylinder sets. Let B
(m,k)
X
denote the Borel Ïƒ-algebra on
(X
m)
k
, m âˆˆ N+. In much the same way as with the single-process described above, associated with
the joint process is a sequence of random variables hXt,j i, t âˆˆ N+, j âˆˆ 1..k where Xt,j : â„¦ â†’ X
is the projection on to the t, j-th element, i.e. Xt,j (Ï‰) = Ï‰t,j for Ï‰ âˆˆ â„¦, t âˆˆ N+, j âˆˆ 1..k. The
measure P is stationary if P(X1..m,1..k âˆˆ B) = P(Xi+1..i+m,1..k âˆˆ B) for every B âˆˆ B(m,k)
X
and
i, m âˆˆ N+. As above, the term stochastic process is used interchangeably to correspond to the
sequence of random variables hXt,j i, t âˆˆ N+, j âˆˆ 1..k or their corresponding joint measure P.
2. More generally X can be a finite set or a closed interval [a, b], for a < b, a, b âˆˆ R.
5
GRÃœNEWÃ„LDER AND KHALEGHI
Definition 4 (Jointly Stationary Ï•-mixing Processes) For a fixed k âˆˆ N+, consider a stationary
process hXt,j i, t âˆˆ N+, j âˆˆ 1..k. Its Ï•-mixing coefficients are given by
Ï•n := sup
u,vâˆˆN+
Ï•(X1..u,1..k, Xu+n..u+n+vâˆ’1,1..k), n âˆˆ N+
and measure the Ï•-dependence between Ïƒ(X1..u,1..k) and Ïƒ(Xu+n..u+n+vâˆ’1,1..k). The process is
said to be jointly Ï•-mixing if limnâ†’âˆ Ï•n = 0.
We use Ï•n to denote the Ï•-mixing coefficient corresponding to a single process or to a joint process
given by Definitions 3 and 4 respectively; the notion will be apparent from the context. Under
the assumption that the joint process is stationary Ï•-mixing, there could be dependence between
the processes, as the mixing requirement needs only to be fulfilled by the joint process. The
assumption that the process is jointly Ï•-mixing is fulfilled by a variety of well-known models,
including independent Markov chains. More generally, as follows from Proposition 5 below, if we
have k independent Ïˆ-mixing processes then the joint process is Ï•-mixing. The Ïˆ-mixing property
is defined in the same way as Ï•-mixing whereby the Ï•-dependence given by Definition 1 is replaced
with
Ïˆ(U, V) := sup{|1 âˆ’ Ï(U âˆ© V )/(Ï(V )Ï(U))| :U âˆˆ U, Ï(U) > 0, V âˆˆ V, Ï(V ) > 0},
and the Ïˆ-mixing coefficients are defined in a manner analogous to Definitions 3. A Ïˆ-mixing
process is also Ï•-mixing and the Ïˆ-mixing coefficients upper bound Ï•-mixing coefficients.
Proposition 5 Let (â„¦, A, P) be some probability space with k mutually independent processes
defined on it. If each of these processes is Ïˆ-mixing then the joint process is also Ïˆ-mixing and for
all i âˆˆ N, 1 + ÏˆËœ
i â‰¤ (1 + Ïˆi)
k
, where ÏˆËœ
i are the mixing coefficients of the joint process and the Ïˆi
are upper-bounds on the mixing coefficients of the individual processes.
The proof is provided in Appendix A.2. Using the above result, it can be shown that independent
Markov chains are jointly Ï•-mixing. More specifically, we have the following example.
Corollary 6 (Independent Markov chains are jointly Ï•-mixing.) A set of k âˆˆ N+ mutually independent, stationary ergodic, finite-state Markov processes Ïi
, i = 1..k give rise to a jointly stationary
Ï•-mixing process.
Proof By Theorem 3.1 of Bradley (2005) each process Ïi
, i = 1..k is Ïˆ-mixing. Moreover, by
Proposition 5 above, k mutually independent Ïˆ-mixing processes are jointly Ï•-mixing. As a result
Ïi
, i = 1..k, are jointly stationary Ï•-mixing.
The significance of the above observation is that jointly Ï•-mixing processes have mutually independent Markov processes as special case.
3. Problem Formulation: the Jointly Ï•-mixing Bandit Problem.
We assume in the sequel that a total of k < âˆ bandit arms are given, where for each i âˆˆ 1..k, arm i
corresponds to a stationary process that generates a time series of pay-offs X1,i, X2,i, . . . Furthermore,
we assume that the joint process over the k arms is Ï•-mixing in the sense of Definition 4, and that
its sequence of mixing coefficients is summable, i.e. kÏ•k := Pâˆ
i=1 Ï•i < âˆ. Each process has
6
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
stationary mean Âµi
, i = 1..k and we denote by Âµ
âˆ—
:= max{Âµ1, . . . , Âµk} the highest stationary mean.
We sometimes denote the arm with the highest stationary mean as the best arm. At every time-step t âˆˆ
N+, a player chooses one of k arms according to a policy Ï€t and receives a reward Xt,Ï€t
. The playerâ€™s
objective is to maximize the sum of the pay-offs received. The policy has access only to the pay-offs
gained at earlier stages and to the arms it has chosen. Let hFtitâ‰¥0 be a filtration that tracks the payoffs obtained in the past t rounds, i.e. F0 = {âˆ…, â„¦}, and Ft = Ïƒ(X1,Ï€1
, . . . , Xt,Ï€t
), t â‰¥ 1. A policy
is a sequence of mappings Ï€t
: â„¦ â†’ {1, . . . , k}, t â‰¥ 1, each of which is measurable with respect to
Ftâˆ’1. Note that the assumption that Ï€t
, t â‰¥ 1 is measurable with respect to Ftâˆ’1 is equivalent to the
assumption that the policy can be written as a function of the past pay-offs and chosen arms, see, e.g.
(Shiryaev, 1991)[Thm. 3, pp.174]. Let Î  = {Ï€ := hÏ€titâ‰¥1 : Ï€t
is Ftâˆ’1-measurable for all t â‰¥ 1}
denote the space of all possible policies. We also let Gt denote the filtration that keeps track of all the
information available up to time t (including unobserved pay-offs). More specifically, let hGtitâ‰¥0,
G0 = N and Gt = Ïƒ(
Sk
i=1
St
s=1 Ïƒ(Xs,i) âˆª N ) for t â‰¥ 1, where N is the family of measurable sets
of P-measure zero. We define the maximal value that can be achieved in n rounds as
v
âˆ—
n = sup
Ï€âˆˆÎ 
Xn
t=1
EXt,Ï€t
. (3)
The regret that builds up over n rounds for any strategy Ï€ is
RÏ€(n) := v
âˆ—
n âˆ’ E(Xt,Ï€t
). (4)
To simplify notation, we may use R(n) when the policy Ï€ is clear from the context.
Remark 7 Requiring kÏ•k < âˆ, which is standard in the literature on empirical process theory
involving mixing processes, does not lead to a strong assumption. This condition is already fulfilled
by any stationary ergodic finite-state Markov chain, see, e.g. (Bradley, 2005). In fact, as follows
from Theorem 3.4 of Bradley (2005) the Ï•-mixing coefficient corresponding to any (not necessarily
stationary) Ï•-mixing Markov process has an exponentially fast rate of decay, giving rise to a
summable sequence of Ï•-mixing coefficients. Note that our focus in the sequel is on the more general
case of Ï•-mixing (not necessarily Markov) processes.
4. Main Results
We consider the restless bandit problem in a setting where the reward distributions are jointly Ï•mixing as formulated in Section 3. Recall that, while the optimal strategy in this case is to switch
between the arms, obtaining the best switching strategy is PSPACE-hard. We address the question
of when and how a good and computationally tractable approximation of the optimal policy can
be obtained in this setting. The former question is answered in Section 4.2 where we characterize
(in terms of Ï•1) the loss of settling for the highest stationary mean as opposed to following the
best switching strategy. We show that for small Ï•-mixing coefficients, the optimum of this relaxed
problem is close to that given by v
âˆ—
n
. To answer the latter, we devise a UCB-type algorithm in
Section 4.3 to identify the arm with the highest stationary mean. The main challenge lies in building
confidence intervals around empirical estimates of the stationary means. Indeed, as we demonstrate
in Section 4.1, unlike in the i.i.d. setting, a policy in this framework may introduce strong couplings
between past and future pay-offs in such a way that the resulting pay-off sequence may not even
be Ï•-mixing. As a result, a standard UCB algorithm designed for an i.i.d. setting is not suitable
7
GRÃœNEWÃ„LDER AND KHALEGHI
here, even when equipped with a Hoeffding-type concentration bound for Ï•-mixing processes. We
circumvent these difficulties in Sections 4.2 and 4.3 by controlling the effect of a policy on the
pay-off distributions. Part of the analysis in these two sections relies on some technical results for
Ï•-mixing processes outlined in Appendix A.1, which may be of independent interest. Finally, our
results for the weakly dependent reward distributions are complemented in Section 4.4, where we
study an example of a class of strongly dependent processes and give a simple switching strategy
that significantly outperforms a policy that aims for the best arm.
4.1. Policies, Random Times, and the Ï•-mixing Property
Recall that a policy Ï€t
, t âˆˆ N+ is a function which based on the past (observed) data samples one
of k bandit arms at time-step t. Therefore, since this decision is based on samples generated by a
random process, the times at which bandit arms are played can be naturally modeled via random
times. More formally, denote by Ï„i,j : â„¦ â†’ N+ a random variable which determines the time at
which the j-th arm is sampled for the i-th time, i âˆˆ N+ and j âˆˆ 1..k, and observe that for any t âˆˆ N
the event {Ï„i,j = t} is in Ftâˆ’1. We denote by
XÏ„i,j := X
tâˆˆN+
Ï‡{Ï„i,j = t}Xt,j
the pay-off obtained from sampling arm j at random time Ï„i,j for any i âˆˆ N+, j âˆˆ 1..k, where Ï‡ is
the indicator function, i.e. Ï‡{Ï„i,j = t} is 1 for all Ï‰ âˆˆ â„¦ such that Ï„i,j (Ï‰) = t, and is 0 otherwise.
The main challenge in devising a policy for the Ï•-mixing bandit problem is that, depending on
the policy used, the dependence structure of the pay-off sequence XÏ„1,j , XÏ„2,j , . . ., for j âˆˆ 1..k may
be completely different from that of X1,j , X2,j , . . .. Note that this differs from the simpler i.i.d.
setting where the distribution of the j-th arm (with all its characteristics) carries over to that of the
sampled sequence. This is illustrated in Example 1 below.
Example 1 Consider a two-armed bandit problem where the second arm is deterministically set to
0, i.e. Xt,2 = 0, t âˆˆ N+ and the first arm has a process distribution described by a two state Markov
chain with the initial distribution and stationary distribution over the states both being (1/2 1/2)>
and with the following transition matrix,
T =

1 âˆ’  
 1 âˆ’ 

, with some  âˆˆ (0, 1).
Observe that for this process, if  is small, with high probability the Markov chain stays in its
current state. Now consider a policy Ï€, and denote by Ï„1, Ï„2, . . . the sequence of random times at
which Ï€ samples the first arm according to the following simple rule. Set Ï„1 = 1. For subsequent
random times, if XÏ„n,1 = X1,1 for n âˆˆ N+ then Ï„n+1 = Ï„n + 1. Otherwise, Ï„n+1 is set to be
significantly larger than Ï„n to guarantee that the distribution of XÏ„n+1,1 given XÏ„n,1 is close to
the stationary distribution of the Markov chain, during which time the first arm is sampled. The
sequence XÏ„1,1, XÏ„2,1, . . . so generated is highly dependent on X1,1 and is not Ï•-mixing. In fact, the
expected pay-offs given the first observation, i.e. E(XÏ„n,1|X1,1), n âˆˆ N+, are very different from
the stationary mean EX1,1 if  is small. In particular, EX1,1 = 0.5 while E(XÏ„n,1|X1,1 = 1) is
at least 1/(1 + 2) âˆ’ 1/10 due to Equation (41) on page 28 and, hence, for  â‰¤ 0.01 we have that
E(XÏ„n,1|X1,1 = 1) â‰¥ 0.8.
8
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
A more detailed treatment of the above example is given in Appendix A.4.
Indeed, it is a policyâ€™s access to the (observed) past data which can lead to strong couplings
between past and future pay-offs in this framework. This point has been overlooked in the work
of Audiffren and Ralaivola (2015) which relies on Improved-UCB (Auer and Ortner, 2010) to
identify the arm with the highest stationary mean, by eliminating potentially sub-optimal arms. The
elimination process depends on the data, and the time-steps at which a particular arm is played
depend on the remaining arms. Hence, random times and the policyâ€™s memory have to be carefully
taken into consideration, as the process distribution of the sampled sequence is different from that of
the corresponding arm. This notion has not been accounted for in their algorithm, and the confidence
intervals involved correspond to the distributions of the arms, and not to those of the sampled
sequences, and are therefore invalid in this non-i.i.d. setting.
4.2. Approximation Error
We start by translating Ï•-mixing properties to those of expectations in order to control the difference
between what a switching strategy can achieve as compared to the highest stationary mean. Prior
to delving into the bandit problem we consider a single bounded real-valued stationary Ï•-mixing
process hXtitâˆˆN+
sampled at random times Ï„1 < Ï„2 < . . ., where Ï„i
, i âˆˆ N+, is fully defined by
the past observations XÏ„1
, . . . , XÏ„iâˆ’1
. We control the difference between the mean of the sampled
process hXÏ„i
iiâˆˆN+
from the stationary mean of the original process. The following proposition
shows that the difference in the means is controlled by the Ï•-mixing coefficients and the increments
in the stopping times; the proof is provided in Appendix A.3.
Proposition 8 Assume that hXtitâˆˆN+
is a stationary Ï•-mixing process with mixing coefficients
hÏ•iiiâˆˆN+
such that EXt = Âµ, t âˆˆ N+, and suptâˆˆN+
|Xt
| â‰¤ c for some c âˆˆ [0, âˆ). Furthermore,
let Ï„1, Ï„2, . . . be a sequence of random times such that Ï„i + ` â‰¤ Ï„i+1 a.s. for some ` â‰¥ 1 and all
i âˆˆ N+, and all Ï„i+1 are Ïƒ(XÏ„1
, . . . , XÏ„i
)- measurable with Ï„1 âˆˆ N+ being a fixed time. Then for
any n âˆˆ N+



1
n
Xn
i=1
EXÏ„i âˆ’ Âµ


 â‰¤ 2cÏ•`
.
In other words, when using the sample mean of the sampled process as an estimate of the stationary
mean then the bias of the estimator is bounded through the Ï•`-mixing coefficient. This result has
further implications, in particular, it is telling us something about the leverage a switching policy
has. A switching policy selects effectively random times for each arm at which the arm is played and
this result is saying that the summed pay-off it can gather cannot be more than 2cnÏ•`
larger than the
stationary mean of the arm. Now, a policy is free to play the arms at any time and we only know that
Ï„i+1 â‰¥ Ï„i + 1 for any random time Ï„i
, i.e. ` = 1. This intuition underlies the following proposition.
Proposition 9 Consider the jointly stationary Ï•-mixing bandit problem formulated in Section 3. Let
Âµ1, . . . , Âµk be the means of the stationary distributions and let Âµ
âˆ—
:= max{Âµ1, . . . , Âµk}. Let Ï•1 be
the first Ï•-mixing coefficient as given by Definition 4. For every n â‰¥ 1 we have
v
âˆ—
n âˆ’ nÂµâˆ— â‰¤ 2nÏ•1.
9
GRÃœNEWÃ„LDER AND KHALEGHI
Proof Consider an arbitrary policy hÏ€titâˆˆN+
and an arbitrary t âˆˆ N+ then
EXt,Ï€t =
X
k
j=1
X
t
i=1
EXt,j Ã— Ï‡{Ï„i,j = t}.
Recall the definition of Gtâˆ’1 in Section 3 and observe that Ï„i,j is Gtâˆ’1-measurable. Hence, we have
with B = {Ï„i,j = t} that
EXt,j Ã— Ï‡{Ï„i,j = t} = EE(Xt,j |Gtâˆ’1) Ã— Ï‡{Ï„i,j = t} =
Z
B
E(Xt,j |Gtâˆ’1).
We can extend the Ï•-mixing property of the joint process from Ïƒ(X11, . . . , Xtk)to Gt by applying
Lemma A.2 and we get


Z
B
(E(Xt,j |Gtâˆ’1) âˆ’ EX1,j )

 â‰¤ 2Ï•1P(B).
Since the different sets {Ï„i,j = t}, j âˆˆ {1, . . . , k}, i âˆˆ {1, . . . , t} are disjoint
EXt,Ï€t âˆ’ Âµ
âˆ— â‰¤
X
k
j=1
X
t
i=1
(EXt,j Ã— Ï‡{Ï„i,j = t} âˆ’ P(Ï„i,j = t)EX1,j )
â‰¤ 2Ï•1
X
k
j=1
X
t
i=1
P(Ï„i,j = t)
â‰¤ 2Ï•1.
Observe that this relaxation introduces an inevitable linear component to the regret as shown by
Proposition 9. However, we argue that if the reward distributions are weakly dependent in the sense
that Ï•1 is small, we may settle for the best arm instead of following the best switching strategy.
4.3. An Optimistic Approach
In this section we propose a UCB-type algorithm to identify the arm with the highest stationary mean
in a jointly Ï•-mixing bandit problem. Consider the bandit problem described in Section 3, where we
have k arms each with a bounded stationary pay-off sequence such that the joint process is stationary
Ï•-mixing. Suppose that the processes are weakly dependent in the sense that Ï•1 â‰¤  for some small
. As discussed in Section 4.2, in this case a policy to settle for the best arm can serve as a good
approximation for the best switching strategy. More specifically, let
RÏ€(n) := nÂµâˆ— âˆ’
Xn
t=1
EXt,Ï€t
denote the regret of a policy Ï€ with respect to the arm with the highest stationary mean. From
Proposition 9 we have 1
n
(RÏ€(n) âˆ’ RÏ€(n)) â‰¤  and our objective in this section is to minimize RÏ€.
Recall that in light of the arguments provided in Section 4.1 it is crucial to take a policyâ€™s access to
past (observed) data into account when devising a strategy for the bandit problem in this framework.
10
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
To address the challenge induced by the inter-dependent reward sequences obtained at random times,
our approach relies on the following key observation. Suppose we obtain a sequence of m âˆˆ N+
consecutive samples XÏ„,i, XÏ„+1,i, . . . , XÏ„+m,i from arm i âˆˆ 1..k starting at a random time Ï„ . For a
long batch, i.e. large enough m, the average expectations 1
m
Pmâˆ’1
j=0 EXÏ„+j,i become close to the
stationary mean Âµi
. More formally we have Lemma 10 below.
Lemma 10 For a fixed i âˆˆ 1..k and m âˆˆ N+, consider the consecutive samples XÏ„,i, XÏ„+1,i,
. . . , XÏ„+m,i, where Ï„ : â„¦ â†’ N+ is a random time at which the i-th arm is sampled. Let Âµi denote
the stationary mean of arm i. We have






Âµi âˆ’
1
m
mXâˆ’1
j=0
EXÏ„+j,i






â‰¤
2
m
kÏ•k .
Proof For simplicity of notation we denote Xt,i by Xt and XÏ„,i by XÏ„ . Recall that Gt denotes the
filtration that keeps track of all the information available up to time t (including unobserved pay-offs).
Observe that Ï‡{Ï„ = t} is Gtâˆ’1-measurable so that the event {Ï„ = t} is in Gtâˆ’1 for all t âˆˆ N+. As a
result, for any t âˆˆ N+ and j âˆˆ 0..m âˆ’ 1 we have
E(Ï‡{Ï„ = t}Xt+j |Gtâˆ’1) = Ï‡{Ï„ = t}E(Xt+j |Gtâˆ’1), (5)
see, e.g. (Shiryaev, 1991)[pp.216]. We obtain






mÂµi âˆ’
mXâˆ’1
j=0
EXÏ„+j,i






=






X
tâˆˆN+
mXâˆ’1
j=0
E(Ï‡{Ï„ = t}Xt+j ) âˆ’ EÏ‡{Ï„ = t}EXt






(6)
=






X
tâˆˆN+
mXâˆ’1
j=0
EE(Ï‡{Ï„ = t}Xt+j |Gtâˆ’1) âˆ’ EÏ‡{Ï„ = t}EXt






(7)
=






X
tâˆˆN+
mXâˆ’1
j=0
E(Ï‡{Ï„ = t}E(Xt+j |Gtâˆ’1)) âˆ’ EÏ‡{Ï„ = t}EXt






(8)
â‰¤
X
tâˆˆN+
mXâˆ’1
j=0
E

Ï‡{Ï„ = t}|E(Xt+j |Gtâˆ’1) âˆ’ EXt
|

(9)
=
X
tâˆˆN+
mXâˆ’1
j=0
E

Ï‡{Ï„ = t}|E(Xt+j |Gtâˆ’1) âˆ’ EXt+j |

(10)
=
X
tâˆˆN+
mXâˆ’1
j=0
Z
{Ï„=t}
|E(Xt+j |Gtâˆ’1) âˆ’ EXt+j |
â‰¤
X
tâˆˆN+
mXâˆ’1
j=0
2Ï•(Gtâˆ’1, Ïƒ(Xt+j ))EÏ‡{Ï„ = t} (11)
â‰¤
X
tâˆˆN+
Xm
j=1
2Ï•jEÏ‡{Ï„ = t} (12)
â‰¤ 2 kÏ•k (13)
  
GRÃœNEWÃ„LDER AND KHALEGHI
Algorithm 1 A UCB-type Algorithm for Ï•-mixing bandits.
Input: Number k of arms; Sum kÏ•k := Pâˆ
i=1 Ï•i of mixing coefficients3
Initialization: Play each arm once and update the empirical mean for each arm
1: for i = 1..k do
2: Xi â† Xi
3: si â† 1 . si
, i = 1..k denotes the number of times arm i has been selected
Main Loop:
4: for t = 1..âˆ do
5: Arm Selection: Select the arm that maximizes the following UCB
j â† min
ï£«
ï£­argmax
uâˆˆ1..k
Xu +
s
8Î¾(
1
8 + ln t)
2
su
+
kÏ•k
2
suâˆ’1
ï£¶
ï£¸ , where Î¾ := 1 + 8 kÏ•k
. The min operator is used to give precedence to the smaller arm-index in the case of a tie.
6: Update: Play arm j for 2
sj consecutive iterations and update the empirical mean accordingly.
tj â† t t â† t + 2sj Xj â†
1
2
sj
X
tâˆ’1
t
0=tj
Xt
0
,j sj â† sj + 1
where (6) and (7) are due to stationarity and the law of total expectation respectively, (8) follows
from (5), (10) follows from stationarity, (11) follows from Theorem 2, namely Inequality (2), and
noting that kXkâˆ = 1, (12) follows directly from Definition 4, and (13) follows from the definition
of kÏ•k.
Inspired by this result, we provide Algorithm 1 which, given the number k of arms and the sum
kÏ•k of the Ï•-mixing coefficients, works as follows. First, each arm is sampled once for initialization.
Next, from t = k + 1 on, arms are played in batches of exponentially growing length. Specifically, at
each round arm j with the highest upper-confidence on its empirical mean is selected, and played for
2
sj consecutive time-steps, where sj denotes the number of times that arm j has been selected so far.
The upper confidence bound is calculated based on a Hoeffding-type bound for Ï•-mixing processes
given by Corollary 2.1 of Rio (1999). The 2
sj samples obtained by playing the selected arm are
used in turn to calculate (from scratch) the armâ€™s empirical mean. The algorithm does not require
the values of the individual Ï•-mixing coefficients, but only their sum kÏ•k. In fact, any upper-bound
Ï‘ â‰¥ kÏ•k may be used, in which case Ï‘ would replace kÏ•k in the regret bound of Theorem 12.
To analyze the regret of Algorithm 1, first recall that in an i.i.d. setting we trivially have
R(n) = nÂµâˆ— âˆ’ Âµj
Pk
j=1 ETj (n) = Pk
j=1 âˆ†jETj (n) where Tj (n) is the total number of times that
arm j is played by the algorithm in n rounds and âˆ†j := Âµ
âˆ— âˆ’ Âµj , j âˆˆ 1..k. In our framework, this
equality does not necessarily hold due to the inter-dependencies between the pay-offs. However,
as shown in Proposition 11 below, an analogous result in the form of an upper-bound holds for our
algorithm.
3. Note that only the sum of the coefficients is needed, not the individual coefficients Ï•i.
12
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Proposition 11 Consider the regret R(n) of Algorithm 1 after n rounds of play. We have,
R(n) â‰¤
X
k
j=1
âˆ†jETj (n) + 2k
 Xn
l=0
Ï•l
!
log n
Proof Denote by Ï„i,j : â„¦ â†’ N+ the random time at which the j-th arm is sampled for the i-th time.
Note that for any t âˆˆ N the event {Ï„i,j = t} is measurable with respect to the filtration Gtâˆ’1 that
keeps track of all the information available up to time t. First note that
E(Ï‡{Ï„i,j = t}Xt+l,j ) = EE(Ï‡{Ï„i,j = t}Xt+l,j |Gtâˆ’1)
= E (Ï‡{Ï„i,j = t}E (Xt+l,j |Gtâˆ’1))
â‰¥ (Âµj âˆ’ 2Ï•l) P (Ï„i,j = t) (14)
where the second equality follows from the fact that the event {Ï„i,j = t} is Gtâˆ’1-measurable and (14)
follows from Theorem 2. We have,
R(n) = nÂµâˆ— âˆ’
Xn
t=1
EXt,Ï€t
= nÂµâˆ— âˆ’
Xn
t=1
X
k
j=1
log
Xn
m=1
min{2mXâˆ’1,nâˆ’t}
l=0
EÏ‡{Ï„m,j = t}Xt+l,j
â‰¤ nÂµâˆ— âˆ’
Xn
t=1
X
k
j=1
log
Xn
m=1
min{2mXâˆ’1,nâˆ’t}
l=0
P (Ï„m,j = t) (Âµj âˆ’ 2Ï•l)
â‰¤ nÂµâˆ— âˆ’
Xn
t=1
X
k
j=1
log
Xn
m=1
min{2mXâˆ’1,nâˆ’t}
l=0
P (Ï„m,j = t) Âµj + 2k
 Xn
l=0
Ï•l
!
log n
=
X
k
j=1
âˆ†jETj (n) + 2k
 Xn
l=0
Ï•l
!
log n
where the last inequality follows from (14).
An upper-bound on R(n) is given by Theorem 12 below with proof provided in Appendix B. Recall
that âˆ†i
:= Âµ
âˆ— âˆ’ Âµi
is the difference between the highest stationary mean Âµ
âˆ—
and the stationary mean
Âµi of arm i.
Theorem 12 (Regret Bound.) For the regret R(n) of Algorithm 1 after n rounds of play. We have,
R(n) â‰¤
X
k
i=1
Âµi6=Âµ
âˆ—
32(1 + 8 kÏ•k) ln n
âˆ†i
+ (1 + 2Ï€
2
/3)(X
k
i=1
âˆ†i) + kÏ•k log n
Remark. Interestingly, kÏ•k appears in the bound of Theorem 12. Indeed, in the case where the
pay-offs are i.i.d., we recover (up to some constant) the regret bound of Auer et al. (2002)[Thm. 1].
13
GRÃœNEWÃ„LDER AND KHALEGHI
4.4. Strongly Dependent Reward Distributions: a Complementary Example
At the other end of the extreme lie bandit problems with strongly dependent pay-off distributions. Our
objective in this section is to give an example where a simple switching strategy can be obtained in
this case to leverage the strong inter-dependencies between the samples. This approach gives a much
higher overall pay-off than what would be given by settling for the best arm, and is computationally
efficient. The intuition is that in many cases strong dependencies may allow for the prediction of
future rewards even from scarce observations of a sample path.
We consider a class of stochastic processes for which we can easily control the level of dependency. A natural choice is to use stationary Gaussian processes on N+. Recall that a Gaussian
process is fully specified by its covariance function cov: N+ Ã— N+ â†’ R. Also, for any covariance
function Kolmogorovâ€™s consistency theorem guarantees the existence of a Gaussian process with this
particular covariance function, see, e.g. (GinÃ© and Nickl, 2016). A Gaussian process is stationary
if it has constant mean on N+ and its covariance can be written as cov(s) = cov(t, t + s) for all
t âˆˆ N+, s âˆˆ N. We measure the degree of dependence of the process by means of HÃ¶lder-continuity
of the covariance function. In particular, we assume that there exists some c > 0 and Î± âˆˆ (0, 1] such
that for all s, t âˆˆ N, |cov(s) âˆ’ cov(t)| â‰¤ c|s âˆ’ t|
Î± and we assume that the covariance function is
non-negative. A low c and Î± correspond to highly dependent processes since the covariance decreases
slowly over time. A slowly decreasing covariance also implies large Ï•-mixing coefficients, since
|cov(t)| /cov2
(0) â‰¤ 2(Ï•(X0, Xt)Ï•(Xt
, X0))1/2
, by Rio (1999)[Thm. 1.4]. Consider a k-armed
bandit problem where each arm is distributed according to a stationary Gaussian process with stationary mean Âµi
, i = 1..k. For simplicity, we assume that the processes are mutually independent
with the same, unknown, covariance function cov(Â·). While cov(Â·) is assumed unknown, we have
access to an upper-bound on its rate of decay. That is, we are given constants c and Î± such that
cov(Â·) is Î±, c-HÃ¶lder continuous. We further assume that an upper bound on the stationary means of
the processes is known and that cov(0) = 1. In order to obtain direct control on the regret RÏ€(n)
we would need to make inference about the best possible switching strategy. Instead, we provide
guarantees for the regret R+
Ï€
(n) of policy Ï€ with respect to the best policy that can choose arms in
hindsight, i.e. R+
Ï€
(n) := Pn
t=1 E(maxiâ‰¤k(Xt,i âˆ’ Xt,Ï€t
)). Note that R+
Ï€
(n) â‰¥ RÏ€(n).
We provide a simple algorithm, namely, Algorithm 2, that exploits the dependence between
the pay-offs. Starting from an exploration phase, the algorithm alternates between exploration
and exploitation, denoted Phase I and Phase II respectively. In Phase I it sweeps through all k
arms to observe the corresponding pay-offs. In Phase II it plays the arm with the highest observed
pay-off for m âˆ’ k rounds, where m is a (large) constant given by (1) which reflects the degree of
dependence between the samples in the processes. We need not estimate the stationary distributions
in this algorithm, as bounds on the differences between the stationary means suffice. Indeed, these
differences are of minor relevance unless they are high as compared to the dependence between the
individual processes. We have the following regret bound whose proof is given in Appendix C.2.
Proposition 13 Given âˆ† such that maxi,j |Âµi âˆ’ Âµj | â‰¤ âˆ† and Î±, c such that |1 âˆ’ cov(t)| â‰¤ ctÎ± for
all t â‰¥ 0, the regret R+(n) of Algorithm 2 after n rounds is at most,
(n + m?
)k(k âˆ’ 1)
âˆ† + âˆš
2
m?
+
am? c
1/2
8Ï€(1 âˆ’ bm? )

2Ï€
1/2 âˆ’ (1 âˆ’ âˆ†
p
bm? /4) exp 
âˆ’
âˆ†2
bm?
4
!
,
with am? = 8c(m?
)
Î±, bm? = c((m? âˆ’ k)
Î± + k
Î±).
14
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Algorithm 2 An Algorithm for highly dependent arms.
Input: a bound âˆ† on the difference between the stationary means, maxi,jâ‰¤k |Âµi âˆ’ Âµj | â‰¤ âˆ†; HÃ¶lder
coefficients Î±, c such that |1 âˆ’ cov(t)| â‰¤ ctÎ± for all t â‰¥ 0.
1:
m? â†
ï£®
ï£¯
ï£¯
ï£¯
 âˆš
Ï€(âˆ† + âˆš
2)
2Î±c3/2
! 1
1+Î±
ï£¹
ï£º
ï£º
ï£º
.
2: if m? â‰¤ k then m? â† k + 1
3: if âˆ† <
p
8(mâˆ—)
Î±/(
âˆš
2c(m? âˆ’ k)
Î± + k
Î±)) then
4:
m? â† min argmax
mâˆˆN
{m : âˆ† â‰¥
âˆš
8mÎ±/(
âˆš
2c(m âˆ’ k)
Î± + k
Î±
))}.
5: for l = 0 . . . âˆ do
6: Phase I: observe pay-offs Xlm?+1,1 = x1, . . . , Xlm?+k,k = xk
i
âˆ— â† min argmax
iâ‰¤k
{x1, . . . , xk}
7: Phase II: play arm i
âˆ—
for m? âˆ’ k steps.
To interpret the bound consider for simplicity Î± = 1 and the case of a highly dependent process
and, hence, a very small c. If we choose to play the arm with the highest stationary mean at all
rounds, then standard bounds on the normal distribution give us a bound of order n exp(âˆ’(âˆ†2/4))
on the regret. In this case, the regret of Algorithm 2 is of order
nk2
c
3/4
because bm? = cm? â‰ˆ c
1/4Ï€
1/4
((âˆ†+âˆš
2)/2)1/2
is insignificant for small c, c
1/2am? = 8c
3/2m? â‰ˆ
8c
3/4Ï€
1/4
((âˆ† + âˆš
2)/2)1/2
and the bracket on the right side is about 2Ï€
1/2 âˆ’ 1. The gap âˆ† itself is
not of high importance because in Phase I the algorithm selects the arm with highest current pay-off
and the value stays stable over a long period as c is small. Both regret bounds are linear in n because
the oracle has a significant advantage in this setting: at any given time t the oracle chooses the arm
with the highest pay-off in hindsight. However, for a moderate number of arms k
2
c
3/4
is significantly
smaller than exp(âˆ’âˆ†2/4) unless âˆ† is considerably large. The advantage of the switching algorithm
vanishes if k is large compared to the smoothness of the process, because eventually the exploration
phase (Phase I) will dominate and the smoothness of the arms cannot be exploited by this algorithm.
This example demonstrates that large dependence in the stochastic process can be exploited to build
switching algorithms that have a significant edge over algorithms that aim to select a single arm and
algorithms like Algorithm 1 are outperformed by simple switching algorithms.
5. Outlook
This paper is an initial attempt to characterize special sub-classes of the restless bandit problem
where good approximate solutions can be found using simple, computationally tractable approaches.
We provide a UCB-type algorithm to approximate the optimal strategy in the case where the pay-offs
15
GRÃœNEWÃ„LDER AND KHALEGHI
are jointly stationary Ï•-mixing and are only weakly dependent. A natural open problem here is
the derivation of a lower-bound. Moreover, while our algorithm only requires knowledge of the
sum of the Ï•-mixing coefficients kÏ•k =
P
i Ï•i as opposed to that of each individual Ï•i
, the online
estimation of the mixing coefficients can prove useful. Specifically, in light of Proposition 9, if Ï•1 is
estimated from data, the algorithm can have a real-time estimate of its maximum loss with respect
to the best switching strategy after n rounds of play. Further, the results can be strengthened if the
algorithm can adaptively estimate kÏ•k instead of relying on it as input. Another interesting regime
corresponds to strongly dependent pay-off distributions. We provide an example using stationary
Gaussian Processes where a simple switching strategy can leverage the dependencies to outperform a
best arm policy. An open problem would be to weaken the assumptions on the process distributions
and obtain results analogous to the weakly dependent case for the strongly dependent framework.

Appendix A. Proofs for Ï•-mixing Processes
A.1. Technical Results
The following key lemma allows us to control Ï•-mixing coefficients corresponding to disjoint events.
Lemma A.1 Let (â„¦, A, P) be a probability space and let B, C be two Ïƒ-subalgebras of A. If there
exists a Ï• â‰¥ 0 such that for all B âˆˆ B and C âˆˆ C it holds that |P(B)P(C) âˆ’ P(B âˆ© C)| â‰¤ Ï•P(C)
then for any disjoint sequence hBninâˆˆN, Bn âˆˆ B for all n âˆˆ N, and any C âˆˆ C, we have
Xâˆ
n=0
|P(Bn)P(C) âˆ’ P(Bn âˆ© C)| â‰¤ 2Ï•P(C).
Proof Let cn = P(Bn)P(C) âˆ’ P(Bn âˆ© C), I+ = {n âˆˆ N : cn â‰¥ 0}, Iâˆ’ = {n âˆˆ N : cn < 0}.
Now, since S
nâˆˆI+
Bn âˆˆ B and S
nâˆˆIâˆ’
Bn âˆˆ B we have
P(C)Ï• â‰¥ P
 [
nâˆˆI+
Bn

P(C) âˆ’ P
 [
nâˆˆI+
Bn

âˆ© C

=
X
nâˆˆI+
(P(Bn)P(C) âˆ’ P(Bn âˆ© C)) = X
nâˆˆI+
|P(Bn)P(C) âˆ’ P(Bn âˆ© C)| .
Similarly, P(C)Ï• â‰¥
P
nâˆˆIâˆ’
|P(Bn)P(C) âˆ’ P(Bn âˆ© C)|. Hence
2P(C)Ï• â‰¥
X
nâˆˆI+
|P(Bn)P(C) âˆ’ P(Bn âˆ© C)| +
X
nâˆˆIâˆ’
|P(Bn)P(C) âˆ’ P(Bn âˆ© C)|
=
X
nâˆˆN
|P(Bn)P(C) âˆ’ P(Bn âˆ© C)| .
16
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
The following technical lemma is used in the proof of Proposition 9.
Lemma A.2 Given some probability space (â„¦, A, P), three Ïƒ-subalgebras B, C, D âŠ‚ A and Ï• > 0
such that |P(U)P(V ) âˆ’ P(U âˆ© V )| â‰¤ Ï•P(V ) for all U âˆˆ B, V âˆˆ C and P(A) = 0 for all A âˆˆ D
then it holds that |P(U)P(V ) âˆ’ P(U âˆ© V )| â‰¤ Ï•P(V ) for all U âˆˆ B, V âˆˆ Ïƒ(C âˆª D).
Proof (i) The set
E := {A : A âˆˆ A, there exists a B âˆˆ C such that for all C âˆˆ A, P(A âˆ© C) = P(B âˆ© C)}
is a Dynkin system (Fremlin, 2010)[136A]. To see this, note that (1) âˆ… âˆˆ E since âˆ… âˆˆ C, (2) for A âˆˆ E
take any B âˆˆ C and observe that P(C âˆ©(â„¦\A)) = P(C\(C âˆ©A)) = P(C)âˆ’P(C âˆ©A) = P(C)âˆ’
P(C âˆ© B) = P(C âˆ©(â„¦\B)) for every C âˆˆ A and â„¦\B âˆˆ C, (3) let hAninâˆˆN be a disjoint sequence
in E with corresponding elements Bn âˆˆ C then P(
S
nâˆˆN An) = Pâˆ
n=0 P(An) = Pâˆ
n=0 P(Bn).
Let B0
0 = B0 and iteratively let B0
n = Bn\B0
nâˆ’1
then hB0
n
inâˆˆN is a disjoint sequence such that
P(Bn) = P(B0
n
): for any m, n âˆˆ N we have P(Bn âˆ© Bm) = P(An âˆ© Bm) = P(An âˆ© Am) = 0
and, hence, P(Bn\B0
nâˆ’1
) â‰¥ P(Bn) âˆ’
Pnâˆ’1
i=0 P(Bn âˆ© Bi) = P(Bn). Therefore, Pâˆ
P n=0 P(Bn) =
âˆ
n=0 P(B0
n
) = P(
S
nâˆˆN B0
n
) = P(
S
nâˆˆN Bn) and E is a Dynkin system.
(ii) Let D0
:= {A âˆ© B : A âˆˆ C, B âˆˆ D}. Observe that C âˆª D âˆª D0
is closed under intersection:
if A, B âˆˆ C then A âˆ© B âˆˆ C and similarly for A, B âˆˆ D. If A âˆˆ C and B âˆˆ D then A âˆ© B is in an
element of D0
. If A âˆˆ C and B is an element of D0
then B = C âˆ© D for some C âˆˆ C , D âˆˆ D and
A âˆ© B = (A âˆ© C) âˆ© D which is again an element of D0
(and equivalently for A âˆˆ D). Furthermore,
C âˆª D âˆª {A âˆ© B : A âˆˆ C, B âˆˆ D} âŠ‚ E. Note that C is a subset of E, and if B âˆˆ D then we obtain
P(âˆ… âˆ© C) = 0 = P(B âˆ© C) for any C âˆˆ A and âˆ… âˆˆ C. Finally, if we have A âˆ© B where A âˆˆ C
and B âˆˆ D then P(A âˆ© B âˆ© C) = 0 = P(âˆ… âˆ© C) for any C âˆˆ A. Hence, Ïƒ(C âˆª D) âŠ‚ E by the
monotone class theorem (Fremlin, 2010)[136B].
(iii) Now, let us consider a U âˆˆ B and V âˆˆ Ïƒ(C âˆª D). Due to steps (i) and (ii) we can select
a V
0 âˆˆ C such that P(V âˆ© C) = P(V
0 âˆ© C) for all C âˆˆ A. Then with this V
0 we have that
|P(U)P(V ) âˆ’ P(U âˆ© V )| = |P(U)P(V
0
) âˆ’ P(U âˆ© V
0
)| â‰¤ Ï•P(V
0
) = Ï•P(V ).
Another important result in this context concerns the Ï•-mixing property of a process that starts at a
random time. This result is needed to be able to use Hoeffding bounds for batches of observations
which occur in our algorithm. To state this result concisely we first define the following two families
of events
U := n
{Ï„n < âˆ} âˆ© (XÏ„n,i, . . . , XÏ„n+uâˆ’1,i)
âˆ’1
[B] : B âˆˆ B(u)
X
o
,
V := n
{Ï„n < âˆ} âˆ© (XÏ„n+u+lâˆ’1,i, . . . , XÏ„n+u+l+vâˆ’2,i)
âˆ’1
[B] : B âˆˆ B(u)
X
o
,
where the notation (XÏ„n,i, . . . , XÏ„n+uâˆ’1,i)
âˆ’1
[B] denotes the set
{Ï‰ : Ï‰ âˆˆ â„¦,(XÏ„n(Ï‰),i(Ï‰), . . . , XÏ„n(Ï‰)+uâˆ’1,i(Ï‰)) âˆˆ B}
and X is the space in which the random variables Xt,i attain values in.
Lemma 14 In the jointly-stationary Ï•-mixing bandit problem formulated in Section 3, consider an
arm i â‰¤ k and an increasing sequence of starting times hÏ„ninâˆˆN+
of batches in which arm i is played
which are almost surely finite. The following holds for all n âˆˆ N+ and 1 â‰¤ l â‰¤ 2
nâˆ’1 âˆ’ 1
sup{|P(V ) âˆ’ P(U âˆ© V )/P(U)| :U âˆˆ U, V âˆˆ V, P(U) > 0,
u, v âˆˆ N+, u + v + l âˆ’ 1 â‰¤ 2
nâˆ’1
} â‰¤ 2Ï•l
. (15)
17
GRÃœNEWÃ„LDER AND KHALEGHI
Proof For any U, V as defined in (15) there exist sequences hUtitâˆˆN+
, and hVtitâˆˆN+
, where
Ut âˆˆ Ïƒ

{(Xt,i, . . . , Xt+uâˆ’1,i)
âˆ’1
[B] âˆ© {Ï„n < âˆ} : B âˆˆ B(u)
X
}

and
Vt âˆˆ Ïƒ

{(Xt+u+lâˆ’1,i, . . . , Xt+u+l+vâˆ’2,i)
âˆ’1
[B] âˆ© {Ï„n < âˆ} : B âˆˆ B(v)
X
}

such that U =
S
tâˆˆN+
{Ï„n = t} âˆ© Ut and V =
S
tâˆˆN+
{Ï„n = t} âˆ© Vt
. Observe also that due to
stationarity P(Vt) = P(V1) for all t â‰¥ 1. Since, Ut âˆ© {Ï„n = t} âˆˆ Gt
the Ï•-mixing property gives us
|P({Ï„n = t} âˆ© Ut âˆ© Vt) âˆ’ P({Ï„n = t} âˆ© Ut)P(Vt)| â‰¤ Ï•lP({Ï„n = t} âˆ© Ut)
and since Ï„n < âˆ almost surely for each n we have
|P(V ) âˆ’ P(V1)| â‰¤ Xâˆ
t=1
|P({Ï„n = t} âˆ© Vt) âˆ’ P(Ï„n = t)P(Vt)| â‰¤ Ï•l
.
Combining these gives the result as
|P(U)P(V ) âˆ’ P(U âˆ© V )|
= |P(U)P(V ) âˆ’
Xâˆ
t=1
P({Ï„n = t} âˆ© Ut âˆ© Vt)|
â‰¤ |P(U)P(V ) âˆ’
Xâˆ
t=1
P({Ï„n = t} âˆ© Ut)P(Vt)| +
Xâˆ
t=1
Ï•lP({Ï„n = t} âˆ© Ut)
â‰¤ 2Ï•lP(U) + |P(U)P(V1) âˆ’
Xâˆ
t=1
P({Ï„n = t} âˆ© Ut)P(V1)| = 2Ï•lP(U).
A.2. Proof of Proposition 5
Let (â„¦, A, P) be some probability space with k mutually independent processes defined on it. If
each of these processes is Ïˆ-mixing then the joint process is also Ïˆ-mixing and for all i âˆˆ N,
1 + ÏˆËœ
i â‰¤ (1 + Ïˆi)
k
, where the ÏˆËœ
i are the mixing coefficients of the joint process and the Ïˆi are upper
bounds on the mixing coefficients of the individual processes.
Proof Fix some n, a, u, v > 0, a â‰¥ u+n and let us denote the individual processes with hXn,iinâˆˆN+
,
i â‰¤ k. Furthermore, let A = {1, . . . , u}, B = {a, . . . , a+vâˆ’1} and G = Ïƒ(XA,1, . . . , XA,k), H =
Ïƒ(XB,1, . . . , XB,k). The proof structure is the following: in step (i) we show for â€œsimpleâ€ events that
the Ïˆ-mixing property carries over to the joint process. In step (ii), we construct a new probability
space in which to each of these â€œsimpleâ€ events a product of events is associated. The approach is
useful since more complex events can be easily approximated by products. In step (iii) we make
use of this approximation and we relate arbitrary events to unions of products for each of which the
Ïˆ-mixing property derived in (i) applies.
18
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
(i) Consider a set U âˆˆ G of the form U =
T
iâ‰¤k Ui where Ui âˆˆ Ïƒ(XA,i) for all i â‰¤ k and a set
V âˆˆ H of the form V =
T
iâ‰¤k
Vi
, Vi âˆˆ Ïƒ(XB,i), for all i â‰¤ k. The mutual independence of the
processes implies that
|P(U)P(V ) âˆ’ P(U âˆ© V )| =



Y
k
i=1
P(Ui)P(Vi) âˆ’
Y
k
i=1
P(Ui âˆ© Vi)



â‰¤ P(U1)P(V1)



Y
k
i=2
P(Ui)P(Vi) âˆ’
Y
k
i=2
P(Ui âˆ© Vi)



+ |P(U1)P(V1) âˆ’ P(U1 âˆ© V1)|
Y
k
i=2
P(Ui âˆ© Vi)
â‰¤
Y
2
i=1
P(Ui)P(Vi)
 


Y
k
i=3
P(Ui)P(Vi) âˆ’
Y
k
i=3
P(Ui âˆ© Vi)



+ P(U1)P(V1)|P(U2)P(V2) âˆ’ P(U2 âˆ© V2)|
Y
k
i=3
P(Ui âˆ© Vi)
+ Ïˆn P(U1)P(V1)
Y
k
i=2
P(Ui âˆ© Vi)
â‰¤ Ïˆn
X
k
i=1
Y
i
j=1
P(Uj )P(Vj )
 Y
k
j=i+1
P(Uj âˆ© Vj )

â‰¤ Ïˆn P(U)P(V )
X
k
i=1
Y
k
j=i+1
P(Uj âˆ© Vj )
P(Uj )P(Vj )
â‰¤ Ïˆn P(U)P(V )
X
kâˆ’1
i=0
(1 + Ïˆn)
kâˆ’iâˆ’1 = ((1 + Ïˆn)
k âˆ’ 1) P(U)P(V ).
(ii) Next, we demonstrate the Ïˆ-mixing property for U âˆˆ G and V âˆˆ H. We use a product
measure approach. To make use of this let the index set C = A âˆª B and consider the independent
Ïƒ-algebras Ïƒ(XC,1), . . . , Ïƒ(XC,k). Let Pi be the restriction of P to Ïƒ(XC,i) for all i â‰¤ k and
define the product space (â„¦k
, âŠ—biâ‰¤kÏƒ(XC,i), Âµ), where Âµ is the product measure of P1, . . . , Pk and
âŠ—biâ‰¤kÏƒ(XC,i) denotes the product Ïƒ-algebra of Ïƒ(XC,1), . . . , Ïƒ(XC,k). The map Ï† : â„¦ â†’ â„¦
k
,
Ï†(Ï‰)(i) = Ï‰ for all i â‰¤ k, is inverse-measure preserving due to Fremlin (2010)[272J,254Xc].
In particular, P(
T
iâ‰¤k Ui) = P Ï†âˆ’1
[U1 Ã— . . . Ã— Uk] = Âµ(U1 Ã— . . . Ã— Uk) for Ui âˆˆ Ïƒ(XA,i). The
important property is the following: if U âˆˆ G then there exists an F âˆˆ âŠ—biâ‰¤kÏƒ(XA,i) âŠ† âŠ—biâ‰¤kÏƒ(XC,i)
such that U = Ï†
âˆ’1
[F]. To see this consider the set
S := {Ï†
âˆ’1
[F] : F âˆˆ âŠ—biâ‰¤kÏƒ(XA,i)}.
A standard argument shows that S is a Ïƒ-algebra, i.e. âˆ… âˆˆ S, if U âˆˆ S then â„¦\U = Ï†
âˆ’1
[â„¦k\U] âˆˆ
S and if hUninâˆˆN a sequence in S then S
nâˆˆN Un = Ï†
âˆ’1
[
S
nâˆˆN Fn] âˆˆ S for suitable sets Fn.
Furthermore, S âŠ‡ {T
iâ‰¤k Ui
: Ui âˆˆ Ïƒ(XA,i), i â‰¤ k} and the latter set is closed under intersection.
19
GRÃœNEWÃ„LDER AND KHALEGHI
Hence, the monotone class theorem tells us that G = Ïƒ{
T
iâ‰¤k Ui
: Ui âˆˆ Ïƒ(XA,i), i â‰¤ k} âŠ‚ S and
the result follows. Similarly, we can link V âˆˆ G to âŠ—biâ‰¤kÏƒ(XB,i).
(iii) Weâ€™d like to demonstrate that the Ïˆ-mixing property carries over to arbitrary elements
U âˆˆ âŠ—biâ‰¤kÏƒ(XA,i) and V âˆˆ âŠ—biâ‰¤kÏƒ(XB,i). First, observe that if U = U1 Ã— . . . Ã— Uk and V =
V1 Ã— . . . Ã— Vk, Ui âˆˆ Ïƒ(XA,i), Vi âˆˆ Ïƒ(XB,i), i â‰¤ k, then using (U1 Ã— . . . Ã— Uk) âˆ© (V1 Ã— . . . Ã— Vk) =
(U1 âˆ© V1) Ã— . . . Ã— (Uk âˆ© Vk) with the inverse-measure preserving property of Ï† and (i) it follows that
|Âµ(U1 Ã— . . . Ã— Uk)Âµ(V1 Ã— . . . Ã— Vk) âˆ’ Âµ(U1 Ã— . . . Ã— Uk âˆ© V1 Ã— . . . Ã— Vk)|
=

P
T
iâ‰¤k Ui)P
T
iâ‰¤k
Vi

âˆ’ P
T
iâ‰¤k Ui âˆ©
T
iâ‰¤k
Vi


â‰¤ ((1 + Ïˆn)
k âˆ’ 1) P
T
iâ‰¤k Ui

P
T
iâ‰¤k
Vi

= ((1 + Ïˆn)
k âˆ’ 1) Âµ(U1 Ã— . . . Ã— Uk)Âµ(V1 Ã— . . . Ã— Vk).
The advantage of the product approach is that we can approximate the sets U and V with cylinders
of the form U1 Ã— . . . Ã— Uk and this allows us to carry the Ïˆ-mixing property to G and H. As
follows from Fremlin (2010)[Thm. 251Ie, pp. 200, 251W] for every  âˆˆ (0, 1] there exist sequences
U1,1, . . . , Um1,1, . . . , U1,k, . . . , Um1,k and V1,1, . . . , Vm2,1, . . . , V1,k, . . . , Vm2,k, with Ui,j , Vi
0
,j âˆˆ
Ïƒ(XC,j ) for all i â‰¤ m1, i0 â‰¤ m2, j â‰¤ k, with the following four properties.
Âµ
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
â‰¥
X
iâ‰¤m1
Âµ(Ui,1 Ã— . . . Ã— Ui,k) âˆ’ , (16)
Âµ
 [
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k
â‰¥
X
iâ‰¤m2
Âµ(Vi,1 Ã— . . . Ã— Vi,k) âˆ’ /m1, (17)
Âµ

U4
[
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
â‰¤ , and (18)
Âµ

V 4
[
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k
â‰¤ /m1, (19)
where 4 denotes the symmetric difference between sets.
From (16) and (17) we obtain P
iâ‰¤m1
Âµ(Ui,1 Ã— . . . Ã— Ui,k) â‰¤ 1 +  and P
iâ‰¤m2
Âµ(Vi,1 Ã— . . . Ã—
Vi,k) â‰¤ 1 + /m1 respectively. Moreover, we have

Âµ(U) âˆ’ Âµ
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
 â‰¤ Âµ

U4
[
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
â‰¤  (20)
where the second inequality follows from (18). Similarly, from (19) we obtain

Âµ(V ) âˆ’ Âµ
 [
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k
 â‰¤ /m1. (21)
Furthermore, to obtain a similar result for U âˆ© V ,below, we rely on the following elementary set
manipulation. Consider sets A1, A2, A3, A4 then
(A1 âˆ© A2)4(A3 âˆ© A4) = ((A1 âˆ© A2)\(A3 âˆ© A4)) âˆª ((A3 âˆ© A4)\(A1 âˆ© A2))
= ((A1 âˆ© A2)\A3) âˆª ((A1 âˆ© A2)\A4) âˆª ((A3 âˆ© A4)\A1) âˆª ((A3 âˆ© A4)\A2)
âŠ† (A1\A3) âˆª (A2\A4) âˆª (A3\A1) âˆª (A4\A2)
= (A14A3) âˆª (A24A4).         
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
We have,

Âµ(U âˆ© V ) âˆ’ Âµ
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
âˆ©
 [
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k

â‰¤

Âµ

(U âˆ© V )4
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
âˆ©
 [
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k

â‰¤ Âµ

U4
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k + Âµ

V 4
 [
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k (23)
â‰¤  + /m1 (24)
â‰¤ 2 (25)
where, (23) is due to (22) and (24) follows from (18) and (19). Now, applying (20), (21) and (25) we
obtain,
|Âµ(U)Âµ(V ) âˆ’ Âµ(U âˆ© V )|
â‰¤ |Âµ(U)Âµ(V ) âˆ’ Âµ
 [
iâ‰¤m1
[
jâ‰¤m2
(Ui,1 Ã— . . . Ã— Ui,k) âˆ© (Vj,1 Ã— . . . Ã— Vj,k)

| + 2
â‰¤ 4 +

Âµ
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
Âµ
 [
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k
âˆ’ Âµ
 [
iâ‰¤m1
[
jâ‰¤m2
(Ui,1 Ã— . . . Ã— Ui,k) âˆ© (Vj,1 Ã— . . . Ã— Vj,k)


. (26)
Furthermore,

 X
iâ‰¤m1
X
jâ‰¤m2
Âµ(Ui,1 Ã— . . . Ã— Ui,k)Âµ(Vj,1 Ã— . . . Ã— Vj,k)
âˆ’ Âµ
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
Âµ
 [
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k

â‰¤

Âµ
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
+ 
 X
jâ‰¤m2
Âµ(Vj,1 Ã— . . . Ã— Vj,k)
âˆ’ Âµ
 [
iâ‰¤m1
Ui,1 Ã— . . . Ã— Ui,k
Âµ
 [
iâ‰¤m2
Vi,1 Ã— . . . Ã— Vi,k
â‰¤ 2 + 
2
â‰¤ 3. (27)
Let U1, . . . , Um be such that Âµ(
S
iâ‰¤m Ui) â‰¥
P
iâ‰¤m Âµ(Ui) âˆ’ . Then P
iâ‰¤m Âµ
S
j<i Uj âˆ© Ui

â‰¤ .
This is because
Âµ
 [
iâ‰¤m
Ui

= Âµ
 [
iâ‰¤m
Ui\
[
j<i
Uj âˆ© Ui

=
X
iâ‰¤m
Âµ

Ui\
[
j<i
Uj âˆ© Ui

=
X
iâ‰¤m
Âµ(Ui) âˆ’
X
iâ‰¤m
Âµ                           
GRÃœNEWÃ„LDER AND KHALEGHI
Now, for any measurable set V we have by the same argument that
Âµ
 [
iâ‰¤m
Ui âˆ© V

= Âµ
 [
iâ‰¤m
Ui âˆ© V \
[
j<i
Uj âˆ© Ui

=
X
iâ‰¤m
Âµ

Ui âˆ© V \
[
j<i
Uj âˆ© Ui

=
X
iâ‰¤m
Âµ(Ui âˆ© V ) âˆ’
X
iâ‰¤m
Âµ
[
j<i
Uj âˆ© Ui

â‰¥
X
iâ‰¤m
Âµ(Ui âˆ© V ) âˆ’ .
Hence, using (16) and (17), we can obtain

 X
iâ‰¤m1
X
jâ‰¤m2
Âµ

(Ui,1 Ã— . . . Ã— Ui,k) âˆ© (Vj,1 Ã— . . . Ã— Vj,k)

âˆ’ Âµ
 [
iâ‰¤m1
[
jâ‰¤m2
(Ui,1 Ã— . . . Ã— Ui,k) âˆ© (Vj,1 Ã— . . . Ã— Vj,k)


â‰¤
X
iâ‰¤m1
X
jâ‰¤m2
Âµ

(Ui,1 Ã— . . . Ã— Ui,k) âˆ© (Vj,1 Ã— . . . Ã— Vj,k)

âˆ’
X
iâ‰¤m1
Âµ

(Ui,1 Ã— . . . Ã— Ui,k) âˆ©
 [
jâ‰¤m2
(Vj,1 Ã— . . . Ã— Vj,k)

+ 
â‰¤  +
X
iâ‰¤m1
/m1
= 2. (28)
We obtain an bound on |Âµ(U)Âµ(V ) âˆ’ Âµ(U âˆ©V )| by using (26), (27) and (28) together with the result
from Step (i), i.e.,
|Âµ(U)Âµ(V ) âˆ’ Âµ(U âˆ© V )|
â‰¤ 9 +
X
iâ‰¤m1
X
jâ‰¤m2
|Âµ(Ui,1 Ã— . . . Ã— Ui,k)Âµ(Vj,1 Ã— . . . Ã— Vj,k)
âˆ’ Âµ

(Ui,1 Ã— . . . Ã— Ui,k) âˆ© (Vj,1 Ã— . . . Ã— Vj,k)

|
â‰¤ 9 + ((1 + Ïˆn)
k âˆ’ 1) X
iâ‰¤m1
Âµ(Ui,1 Ã— . . . Ã— Ui,k)
X
jâ‰¤m2
Âµ(Vj,1 Ã— . . . Ã— Vj,k) (29)
â‰¤ ((1 + Ïˆn)
k âˆ’ 1) Âµ(U)Âµ(V ) + 9 + 5((1 + Ïˆn)
k âˆ’ 1),
where the last inequality follows from the same arguments as used in inequalities (26) and (27).
Since  is arbitrary we derive the upper bound for arbitrary elements U âˆˆ âŠ—biâ‰¤kÏƒ(XA,i) and
V âˆˆ âŠ—biâ‰¤kÏƒ(XB,i).
Note that this last step is the only part of the proof where Ïˆ-mixing is necessary. In particular,
we could not derive (29) from the line preceding it by only assuming Ï•            
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
To conclude, if U âˆˆ G, V âˆˆ H then we know from step (ii) that there are elements E âˆˆ
âŠ—biâ‰¤kÏƒ(XA,i), F âˆˆ âŠ—biâ‰¤kÏƒ(XB,i) such that
|P(U âˆ© V ) âˆ’ P(U)P(V )|
= |P Ï†âˆ’1
[E âˆ© F] âˆ’ P Ï†âˆ’1
[E]P Ï†âˆ’1
[F]|
= |Âµ(E âˆ© F) âˆ’ Âµ(E)Âµ(F)|
â‰¤ ((1 + Ïˆn)
k âˆ’ 1)P(U)P(V ).
(iv) The joint process is Ïˆ-mixing since limnâ†’âˆ(1 âˆ’ Ïˆn)
k âˆ’ 1 = 0.
A.3. Proof of Proposition 8
In what follows, we denote the probability space by (â„¦, A, P) and let hEtitâ‰¥1 be the filtration
Et = Ïƒ{Ïƒ(X1, . . . , Xt) âˆª N } where N is the family of sets of measure zero. The random times are
Ï„i
: â„¦ â†’ N+ and they are A âˆ’ P(N+) measurable, where P(N+) denotes the power set of N+ =
{1, 2 . . .}. Furthermore, let the random variables Xi attain values in X âŠ‚ R and assume that they
are A âˆ’ BX measurable, where BX denotes the Borel Ïƒ-algebra on X . We define the random times
Ï„i
inductively together with a filtration that tracks the observed information. Let H0 = {âˆ…, â„¦}, let Ï„1
be a H0-measurable random time and let H1 = Ïƒ(XÏ„1
). Then, for given Ï„1, . . . , Ï„i and H1, . . . , Hi
let Ï„i+1 be some Hi-measurable random variable such that Ï„i+1 > Ï„i almost surely and define
Hi+1 = Ïƒ(XÏ„1
, . . . , XÏ„i+1 ). We can observe that Ïƒ(XÏ„1
, . . . , XÏ„i
) = Ïƒ(XÏ„1
, . . . , XÏ„t
, Ï„1, . . . , Ï„t).
To see this, note that Ï„1 is Ïƒ(XÏ„1
, . . . , XÏ„t
)-measurable, hence Ïƒ(XÏ„1
, . . . , XÏ„t
, Ï„1, . . . , Ï„t) =
Ïƒ(XÏ„1
, . . . , XÏ„t
, Ï„2, . . . , Ï„t). Moreover, since Ï„2 is Ïƒ(XÏ„1
, Ï„1) = Ïƒ(XÏ„1
)-measurable, we have
that Ïƒ(XÏ„1
, . . . , XÏ„t
, Ï„2, . . . , Ï„t) = Ïƒ(XÏ„1
, . . . , XÏ„t
, Ï„3, . . . , Ï„t), and the observation can simply be
verified by induction. Recall the statement of Proposition 8 as follows.
Assume that hXtitâˆˆN+
is a stationary Ï•-mixing process with mixing coefficients hÏ•iiiâˆˆN+
such
that EXt = Âµ, t âˆˆ N+, and suptâˆˆN+
|Xt
| â‰¤ c for some c âˆˆ [0, âˆ). Furthermore, let Ï„1, Ï„2, . . . be a
sequence of random times such that Ï„i + ` â‰¤ Ï„i+1 a.s. for some ` â‰¥ 1 and all i âˆˆ N+, and all Ï„i+1
are Ïƒ(XÏ„1
, . . . , XÏ„i
)- measurable with Ï„1 âˆˆ N+ being a fixed time. Then for any n âˆˆ N+



1
n
Xn
i=1
EXÏ„i âˆ’ Âµ


 â‰¤ 2cÏ•`
.
Proof The following technical result is important in this context.
Lemma A.3 For all i, t â‰¥ 1 and A âˆˆ Hiâˆ’1 we have that A âˆ© {Ï„i = t} âˆˆ Et
. In particular
{Ï„i = t} âˆˆ Et
.
Proof For i = 1 the result holds trivially since A âˆ© {Ï„i = t} is either â„¦ or âˆ… and since Et
is a Ïƒ-algebra it contains â„¦ and âˆ…. For any other i â‰¥ 2 observe that if the claim holds for all
1 â‰¤ j â‰¤ i âˆ’ 1 then since Ï„i
is Hiâˆ’1-measurable and because Ï„i > Ï„iâˆ’1 almost surely implies that
U := {Ï„i = t} âˆ© {Ï„iâˆ’1 â‰¥ t} âˆˆ N âŠ‚ Et and we have
{Ï„i = t} = U âˆª
t
[âˆ’1
s=1
{Ï„i = t} âˆ© {Ï„iâˆ’1 = s} âˆˆ Et
.
23
GRÃœNEWÃ„LDER AND KHALEGHI
We can also observe that {B âˆ© {Ï„i = t} : B âˆˆ Ïƒ(XÏ„i
)} âŠ‚ Et because
{B âˆ© {Ï„i = t} : B âˆˆ Ïƒ(XÏ„i
)} = {X
âˆ’1
t
[B
0
] âˆ© {Ï„i = t} : B
0 âˆˆ BX }
and both X
âˆ’1
t
[B0
] and {Ï„i = t} lie in Et
.
Also, for any j âˆˆ {1, . . . , i âˆ’ 1} we have {B âˆ© {Ï„i = t} : B âˆˆ Ïƒ(XÏ„j
)} âŠ‚ Et
: let U = {Ï„i =
t} âˆ© {Ï„j â‰¥ t + 1 âˆ’ (i âˆ’ j)} then U âˆˆ N and
{B âˆ© {Ï„i = t} : B âˆˆ Ïƒ(XÏ„j
)} = U âˆª
tâˆ’[
(iâˆ’j)
s=1
{Xâˆ’1
s
[B
0
] âˆ© {Ï„i = t} âˆ© {Ï„j = s} : B
0 âˆˆ BX }
and Xâˆ’1
s
[B0
] âˆ© {Ï„j = s} âˆˆ Es âŠ‚ Et
. We have shown that
n
{Ï„i = t} âˆ©
i
\âˆ’1
j=1
Bj : Bj âˆˆ Ïƒ(XÏ„j
)
o
âŠ‚ Et
.
This implies directly that
Ïƒ
n
{Ï„i = t} âˆ©
i
\âˆ’1
j=1
Bj : Bj âˆˆ Ïƒ(XÏ„j
)
o
âŠ‚ Et
.
By induction one can verify that {A âˆ© {Ï„i = t} : A âˆˆ Ïƒ(XÏ„1
, . . . , XÏ„iâˆ’1
)} is included in the left
side.
We also need the following observation: Let s, t âˆˆ N+, s < t then for any B âˆˆ Es we have
|
R
B
(E(Xt
|Es) âˆ’ E(Xt))| â‰¤ 2Ï•tâˆ’scP(B). This follows from Lemma A.2 by remembering that
Es = Ïƒ(Ïƒ(X1, . . . , Xs)âˆªN ), i.e. the Ï•-mixing property implies this upper bound for Ïƒ(X1, . . . , Xs)
instead of Es and Lemma A.2 allows us to extend this property to Ïƒ(Ïƒ(X1, . . . , Xs) âˆª N ) by using
in Lemma A.2 the following Ïƒ-algebras: B = Ïƒ(Xt), C = Ïƒ(X1, . . . , Xs) and D = N .
Now, coming to the main proof consider first Ï„1. Note that Ï„1 is independent of any Xi since for
U âˆˆ Ïƒ(Xi), V âˆˆ Ïƒ(Ï„1) = {âˆ…, â„¦} we have either V = â„¦ and P(U âˆ© V ) = P(U) = P(U)P(V ) or
V = âˆ… and P(U âˆ© V ) = 0 = P(U)P(V ). Independence and stationarity of the process hXtitâˆˆN+
give us for any t âˆˆ N+ that
E (Xt Ã— Ï‡{Ï„1 = t}) = P(Ï„1 = t)E(Xt) = P(Ï„1 = t)E(X1).
Now, since the Xt are bounded and Ï„1 attains a value in N+ we know that
Xâˆ
t=1
E (|Xt
| Ã— Ï‡{Ï„1 = t}) = Xâˆ
t=1
P(Ï„1 = t)E |X1|
is finite and B. Leviâ€™s Theorem (Fremlin, 2010)[123A] tells us that Pâˆ
t=1 |Xt
| Ã— Ï‡{Ï„1 = t} is
integrable. Also |
Pt
0
t=1 Xt Ã— Ï‡{Ï„1 = t}| is upper bounded by the integrable function Pâˆ
t=1 |Xt
| Ã—
Ï‡{Ï„1 = t} for all t
0 âˆˆ N+ and Lebesgueâ€™s Dominated Convergence Theorem (Fremlin, 2010)[123C]
gives us
E(XÏ„1
) = E
Xâˆ
t=1
Xt Ã— Ï‡{Ï„1 = t}

=
Xâˆ
t=1
E(Xt Ã— Ï‡{Ï„1 = t}) = Xâˆ
t=1
P(Ï„1 = t)E(Xt) = E(X1).
2 
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
We perform induction over Ï„i
, i â‰¥ 2. The set {Ï„i = t} is an element of Hiâˆ’1 and for any s < t,
s, t âˆˆ N+ we have that B = {Ï„i = t} âˆ© {Ï„iâˆ’1 = s} âˆˆ Hiâˆ’1. Observe that Lemma A.3 tells us that
B âˆˆ Es and, hence, that
Z
B
E(XÏ„i
|Hiâˆ’1) = Z
B
Xt =
Z
B
E(Xt
|Es).
Due to stationarity this implies that


Z
B
(E(XÏ„i
|Hiâˆ’1) âˆ’ E(Xt))

 =


Z
B
(E(Xt
|Es) âˆ’ E(X1))

 â‰¤ 2cÏ•tâˆ’sP(B). (30)
Because Ï„i
is Hiâˆ’1 measurable we can now write
E(XÏ„i
) = EE(XÏ„i
|Hiâˆ’1)
= EE(
Xâˆ
t=1
Xt Ã— Ï‡{Ï„i = t}|Hiâˆ’1) (31)
=
Xâˆ
t=1
EE(Xt Ã— Ï‡{Ï„i = t}|Hiâˆ’1) (32)
=
Xâˆ
t=1
E(E(Xt
|Hiâˆ’1) Ã— Ï‡{Ï„i = t}) (33)
=
Xâˆ
t=1
E(E(Xt
|Hiâˆ’1) Ã— Ï‡

{Ï„i = t} âˆ©
t
[âˆ’`
s=1
{Ï„iâˆ’1 = s}

)
=
Xâˆ
t=1
X
tâˆ’`
s=1
E(E(Xt
|Hiâˆ’1) Ã— Ï‡

{Ï„i = t} âˆ© {Ï„iâˆ’1 = s}

, (34)
where, with the same argument as above, using B. Leviâ€™s Theorem and Lebesgueâ€™s Dominated
Convergence Theorem for the expectation operator and for the conditional expectation operator, the
infinite summation in (31) is moved outside to give (32). Moreover, (33) is due to the fact that Ï„i
is
Hiâˆ’1-measurable. Finally, we obtain,
|E(XÏ„i
) âˆ’ E(X1)| (35)
=





Xâˆ
t=1
X
tâˆ’`
s=1
(E(E(Xt
|Hiâˆ’1) Ã— Ï‡ ({Ï„i = t} âˆ© {Ï„iâˆ’1 = s})) âˆ’ P(Ï„i = t, Ï„iâˆ’1 = s)E(X1))





(36)
â‰¤
Xâˆ
t=1
X
tâˆ’`
s=1

(E(E(Xt
|Hiâˆ’1) Ã— Ï‡ ({Ï„i = t} âˆ© {Ï„iâˆ’1 = s})) âˆ’ P(Ï„i = t, Ï„iâˆ’1 = s)E(X1))


â‰¤ 2c
Xâˆ
t=1
X
tâˆ’`
s=1
Ï•tâˆ’sP(Ï„i = t, Ï„iâˆ’1 = s) (37)
â‰¤ 2cÏ•`
Xâˆ
t=1
X
tâˆ’`
s=1
P(Ï„i = t, Ï„iâˆ’1 = s) (38)
= 2cÏ•`
,
25
GRÃœNEWÃ„LDER AND KHALEGHI
where, (36) follows from (34), (37) follows from (30) with B := {Ï„i = t} âˆ© {Ï„iâˆ’1 = s} âˆˆ Hiâˆ’1,
and (38) is due to the fact that hÏ•ninâˆˆN+
is a decreasing sequence, see, e.g. (Bradley, 2007)[vol. 1
pp. 69], and that t âˆ’ s â‰¥ `. This proves the statement.
A.4. Details of Example 1
We give in this section the details of the construction in Example 1. This example demonstrates that
the sequence hXÏ„n
inâˆˆN+
of samples of one of the arms of a two armed bandit problem does not need
to be Ï•-mixing even though the original process is Ï•-mixing. The argument uses standard results
from Markov chains as they can be found in (Levin et al., 2008) and a well known perturbation result
for Markov chains. We provide the details of the argument for completeness.
Assume we have a two arm bandit problem where the pay-off for arm two is zero at all time
and the pay-off distribution of arm one is described by a Markov chain with two states, transition
probabilities p11 = p22 = 1 âˆ’ , p12 = p21 = , for some  âˆˆ (0, 1), and probability 1/2 to be
in state 1 at time t = 0. The player gains a pay-off of 1 if the Markov chain is in state 1 and
a pay-off of 0 if the Markov chain is in state 2. The Markov chain is irreducible and aperiodic.
Furthermore, the Markov chain induces a stationary pay-off distribution. This implies that we are
dealing with a jointly stationary Ï•-mixing process with mixing coefficients Ï•k being upper bounded
by Ï•k â‰¤ (1 âˆ’ 2)
k
: one can derive the particular bound by considering an eigendecomposition
of the transition matrix
âˆš
T which yields eigenvalues Î»1 = 1, Î»2 = 1 âˆ’ 2 and eigenvectors u1 =
2(1/2 1/2)>, u2 =
âˆš
2(1/2 âˆ’ 1/2)>, i.e. with U = (u1 u2) and Î› being the diagonal
matrix with entries Î»1 and Î»2 we have T = UÎ›U
>. The stationary distribution over the states
is s = (1/2 1/2)> and for any vector v = (v1 v2)
>, v1, v2 â‰¥ 0, v1 + v2 = 1, we have
Î›
kU
>v = (1/
âˆš
2)(1 (1 âˆ’ 2)
k
(v1 âˆ’ v2))>. Hence, T
kv âˆ’ s = (1/2)(1 âˆ’ 2)
k
(v1 âˆ’ v2)(1 âˆ’
1)> and

T
kv âˆ’ s


âˆ
â‰¤ (1/2)(1 âˆ’ 2)
k
. The mixing coefficients can now be bounded in the
following way. Let Xt,i, t â‰¥ 1, i âˆˆ {1, 2}, be random variables that represent the pay-off of
arm i gained at time t. Consider a particular realization where X1,1 = x1, . . . , Xn,1 = xn and
Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+k+m for some x1, . . . , xn, xn+k+1, . . . , xn+k+m âˆˆ {0, 1}
then P(X1,1 = x1, . . . , Xn,1 = xn, Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+k+m) = P(X1,1 =
x1, . . . , Xn,1 = xn)P(Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+m+k|Xn,1 = xn) and
|P(Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+m+k|Xn,1 = xn)
âˆ’ P(Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+m+k)|
= P(Xn+k+1,1 = xn+k+1, . . . , Xn+k+m,1 = xn+m+k|Xn+k,1 = xn+k)
Ã— |P(Xn+k,1 = xn+k) âˆ’ P(Xn+k,1 = xn+k|Xn,1 = xn)|
â‰¤ (1/2)(1 âˆ’ 2)
kP(Xn+k+1,1 = xn+k+1, . . . , Xn+k+m,1 = xn+m+k|Xn+k,1 = xn+k)
= (1 âˆ’ 2)
kP(Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+m+k). (39)
since P(Xn+k,1 = xn+k) = 1/2. Let A = {1, . . . , n}, B = {n + k, . . . , n + k + m} and
consider Ïƒ(XA), Ïƒ(XB). The events in these Ïƒ-algebras are finite unions of events of the form
{X1,1 = x1, . . . , Xn,1 = xn} and {Xn+k,1 = xnk
, . . . , Xn+k+m,1 = xn+k+m}.
For any U âˆˆ Ïƒ(XA) we know that U consists at most of finitely many such events U1, . . . , Ul
,
Ui âˆ©Uj = âˆ…, for all i, j â‰¤ l. Similarly for V âˆˆ Ïƒ(XB) we know that V = V1 âˆª. . .âˆªVo, Vi âˆ©Vj = âˆ…,
26
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
for all i, j â‰¤ o. The above argument which leads to the bound (39) allows us to conclude that for any
U âˆˆ Ïƒ(XA) and V âˆˆ Ïƒ(XB)
|P(U)P(V ) âˆ’ P(U âˆ© V )| =
X
iâ‰¤l
X
jâ‰¤o
|P(Ui)P(Vj ) âˆ’ P(Ui âˆ© Vj )|
â‰¤ (1 âˆ’ 2)
kX
iâ‰¤l
X
jâ‰¤o
P(Ui)P(Vj ) â‰¤ (1 âˆ’ 2)
k
.
Hence the Markov chain is Ï•-mixing. The second arm has a constant reward and does not introduce
any dependencies in the pay-off over time. Therefore we have a jointly stationary Ï•-mixing process
with the mixing coefficient being equal to the mixing coefficients of the Markov chain.
Now fix some Î´ > 0 and consider the following policy Ï€
Î´
. At t = 1 the policy plays arm 1
receiving pay-off X1,1. Then at any other t â‰¥ 2 the arm is selected according to the following rules:
if at t âˆ’ 1 arm 1 has been played and Xtâˆ’1,1 = X1,1 then the policy chooses at t arm 1; if at t âˆ’ 1
arm 1 has been played and Xtâˆ’1,1 6= X1,1 then the policy chooses at t arm 2 and plays arm 2 for the
next k := dlog(2Î´)/ log(1 âˆ’ 2)e rounds before switching back to arm 1. Weâ€™d like to show that the
sequence of pay-offs generated by policy Ï€
Î´
at arm 1 is not Ï•-mixing. Let Ï„1, Ï„2, . . . be the sequence
of random times at which arm 1 is played (by construction Ï„1 = 1 and Ï„2 = 2). The evolution of the
process hXÏ„n
inâ‰¥1 can also be described by transition matrices. The transition probabilities to move
from a state at t = 1 to a state at t = 2 are just the probabilities summarized in T. For all other t
(t â‰¥ 2) the transition matrix is either
TËœ =

1 âˆ’  
(T
k
)21 (T
k
)22
or TËœ =

(T
k
)11 (T
k
)12
 1 âˆ’ 

(40)
depending on XÏ„1
, i.e. if XÏ„1 = 1 then the former is describing the evolution and if XÏ„1 = 0
the latter is the one describing the evolution of the Markov chain. In the following we discuss
the case that XÏ„1 = 1, but the same arguments apply to the case XÏ„1 = 0. We can observe that
kv
>(TËœ âˆ’ TË†)kâˆ â‰¤ Î´ for all v with non-negative entries and v1 + v2 = 1, where
TË† =

1 âˆ’  
1/2 1/2

in case that XÏ„1 = 1. The claim can be verified through



v
>TËœ âˆ’ v
>TË†



âˆ
=




v
>

0 0
TËœ
21 âˆ’ 1/2 TËœ
22 âˆ’ 1/2




âˆ
= v2




T
k

0
1

âˆ’

1/2
1/2




âˆ
â‰¤ v2Î´ â‰¤ Î´.
The Markov chains associated to TËœ and TË† are both irreducible and aperiodic. This implies in
particular the existence of stationary distributions, with the associated probabilities to be in state one
and two summarized in vectors s, Ëœ sË† âˆˆ [0, 1]2
, and the convergence of TËœl
, TË†l
to s, Ëœ sË† in l (measured in
kkâˆ, (Levin et al., 2008)[Thm. 4.9]). In particular, there exist constants c, Ëœ c >Ë† 0 and Î±, Ëœ Î±Ë† âˆˆ (0, 1)
such that for all l â‰¥ 1 and any vector v âˆˆ [0, 1]2 with v1 + v2 = 1
kv
>TËœl âˆ’ sËœkâˆ â‰¤ cËœÎ±Ëœ
l
and kv
>TË†l âˆ’ sË†kâˆ â‰¤ cË†Î±Ë†
l
.
27
GRÃœNEWÃ„LDER AND KHALEGHI
We can also calculate the stationary distribution of TË† explicitly to get sË† = (1/(1+ 2) 2/(1+ 2))>.
It is known that Markov chains with slightly perturbed transition matrices have similar stationary
distributions. Due to Cho and Meyer (2001) there exists a constant c > 0 that is only dependent
on TË† (and independent of TËœ) such that ksËœ âˆ’ sË†kâˆ â‰¤ ckTËœ âˆ’ TË†kâˆ,1 â‰¤ 2Î´c where kTËœ âˆ’ TË†kâˆ,1 :=
maxiâˆˆ{1,2}
P2
j=1 |TËœ
i,j âˆ’TË†
i,j | â‰¤ 2Î´. Combining these inequalities yields kv
>TËœl âˆ’sË†kâˆ â‰¤ 2Î´c+ ËœcÎ±Ëœ
l
for any v with non-negative entries and v1 + v2 = 1.
Consider now the events U = {XÏ„1,1 = 1} and V = {XÏ„n,1 = 1} for n â‰¥ 2. We have
P(U) = 1/2 = P(V ) where the second equality follows from
2P(XÏ„n,1 = 0) = P(XÏ„n,1 = 0|XÏ„1,1 = 0) + P(XÏ„n,1 = 0|XÏ„1,1 = 1)
= P(XÏ„n,1 = 1|XÏ„1,1 = 1) + P(XÏ„n,1 = 1|XÏ„1,1 = 0)
= 2P(XÏ„n,1 = 1).
Furthermore, with TËœ being the matrix defined on the left side of (40)



P(U âˆ© V )
P(U)
âˆ’
1
1 + 2


 =


(1 âˆ’  )TËœnâˆ’1

1
0

âˆ’
1
1 + 2



â‰¤


(1 âˆ’  )TËœnâˆ’1 âˆ’
1
1 + 2

1
2



âˆ
and the last term is upper bounded by 2Î´c + ËœcÎ±Ëœ
nâˆ’1
. Recalling that c does not depend on TËœ and,
hence, not on Î´ we see that we can make the term 2Î´c arbitrary small. Furthermore, by considering a
large n we can make the second term arbitrary small. In particular, let  = 1/10 then there exists a
Ï€
Î´
and an N âˆˆ N such that for all n â‰¥ N



P(U âˆ© V )
P(U)
âˆ’
1
1 + 2


 â‰¤ 1/10. (41)
Hence, for all n â‰¥ N
|P(U)P(V ) âˆ’ P(U âˆ© V )| â‰¥ P(U)



1
2
âˆ’
1
1 + 2


 âˆ’ 1/10
â‰¥ P(U)/5
and the process is not Ï•-mixing.
Appendix B. Proof of Theorem 12: Regret Bound for Ï•-mixing Bandits
For the regret R(n) of Algorithm 1 after n rounds of play. We have,
R(n) â‰¤
X
k
i=1
Âµi6=Âµ
âˆ—
32(1 + 8 kÏ•k) ln n
âˆ†i
+ (1 + 2Ï€
2
/3)(X
k
i=1
âˆ†i) + kÏ•k log n
Proof Thanks to Proposition 11, in order to bound the regret R(n) it suffices to calculate the
expected number of times Ti(n) that a suboptimal arm is played in n rounds. For any s, t âˆˆ N+,
let ct,s := p
(8Î¶((1/8) + ln t))/2
s + kÏ•k /2
sâˆ’1
, where Î¶ = 1 + 8 kÏ•k. Recall that Algorithm 1
28
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
plays its selected arms in batches of exponentially growing length so that if arm j for j âˆˆ 1..k is
selected at round t, it is played for 2
sj (t)
consecutive time-steps, where sj (t) denotes the number of
times that arm j has been selected up-to time t. Below, we denote by Xj,s := 1
2
s
Pt+2sâˆ’1
t
0=t Xt
0
,j with
s := sj (t), the Algorithmâ€™s estimate of the stationary mean of arm j selected at time t. As usual,
a superscript â€œâˆ—â€ refers to the quantities for the arm with the highest stationary mean. Fix some
l âˆˆ N+. We have,
Ti(n) = 1 + Xn
t=k+1
Ï‡{Ï€t = i}
â‰¤ 2
l+1 +
Xn
t=2l+1+1
X
log t
m=l+1
2
mÏ‡{Ï„m,i = t}
â‰¤ 2
l+1 +
Xn
t=2l+1+1
X
log t
m=l+1
2
mÏ‡
n
min
s=1.. log t
X
âˆ—
s + ct,s â‰¤
1
2mâˆ’1
t+2
Xmâˆ’1âˆ’1
u=t
Xu,i + ct,mâˆ’1
o
â‰¤ 2
l+1 +
Xâˆ
t=1
X
log t
m=l+1
X
log t
s=1
2
mÏ‡
n
X
âˆ—
s + ct,s â‰¤
1
2mâˆ’1
t+2
Xmâˆ’1âˆ’1
u=t
Xu,i + ct,mâˆ’1
o
For every t âˆˆ N and every s âˆˆ 1.. log t we have that X
âˆ—
s + ct,s â‰¤
1
2mâˆ’1
Pt+2mâˆ’1âˆ’1
u=t Xu,i + ct,mâˆ’1
implies that
X
âˆ—
s â‰¤ Âµ
âˆ— âˆ’ ct,s (42)
1
2mâˆ’1
t+2
Xmâˆ’1âˆ’1
u=t
Xu,i â‰¥ Âµi + ct,mâˆ’1 (43)
Âµ
âˆ— < Âµi + 2ct,mâˆ’1 (44)
Now, observe that for a fixed t âˆˆ 1..n we have,
P(X
âˆ—
s â‰¤ Âµ
âˆ— âˆ’ ct,s) â‰¤ P



X
âˆ—
s âˆ’ Âµ
âˆ—


 â‰¥ ct,s
â‰¤ P
ï£«
ï£­






2
Xsâˆ’1
j=0
Xâˆ—
Ï„s+j âˆ’ EXâˆ—
Ï„s+j






+






2
Xsâˆ’1
j=0
(EXâˆ—
Ï„s+j âˆ’ Âµ
âˆ—
)






â‰¥ 2
s
ct,s
ï£¶
ï£¸
â‰¤ P
ï£«
ï£­






2
Xsâˆ’1
j=0
Xâˆ—
Ï„s+j âˆ’ EXâˆ—
Ï„s+j






â‰¥ 2
s
ct,s âˆ’ 2 kÏ•k
ï£¶
ï£¸ (45)
â‰¤
âˆš
e exp
âˆ’
(2s
ct,s âˆ’ 2 kÏ•k)
2
2
s+1Î¶

(46)
â‰¤ t
âˆ’4
, (47)
where, (45) follows from Lemma 10 and (46) follows from a Hoeffding-type bound for Ï•-mixing
processes given by Corollary 2.1 of Rio (1999) which is applicable due to Lemma 14 (pp. 17).
29
GRÃœNEWÃ„LDER AND KHALEGHI
Moreover, noting that kÏ•k â‰¥ 0 we similarly obtain,
P
 1
2mâˆ’1
t+2
Xmâˆ’1âˆ’1
u=t
Xu,i â‰¥ Âµi + ct,mâˆ’1

â‰¤ P
 1
2mâˆ’1
t+2
Xmâˆ’1âˆ’1
u=t
Xu,i â‰¥ Âµi + ct,mâˆ’1 âˆ’
2
2mâˆ’1
kÏ•k

â‰¤ P

|
t+2
Xmâˆ’1âˆ’1
u=t
Xu,i âˆ’ 2
mâˆ’1Âµi
| â‰¥ 2
mâˆ’1
ct,mâˆ’1 âˆ’ 2 kÏ•k

â‰¤ t
âˆ’4
. (48)
Let L := log 32(1+4kÏ•k) ln n
âˆ†2
i
. Since, for t â‰¥ 2
L+1 and every m â‰¥ L+1 we have Âµ
âˆ—âˆ’Âµiâˆ’2ct,mâˆ’1 â‰¥
0, it follows that (44) is false for all m â‰¥ L + 1. Therefore we have,
E(Ti(n)) â‰¤ 2
L+1 +
Xâˆ
t=1
X
log t
m=L+1
X
log t
s=1
P(X
âˆ—
s â‰¤ Âµ
âˆ— âˆ’ ct,s,
1
2mâˆ’1
t+2
Xmâˆ’1âˆ’1
u=t
Xu,i â‰¥ Âµi + ct,mâˆ’1)
â‰¤ 2
L+1 +
Xâˆ
t=1
X
log t
m=L+1
X
log t
s=1
2
m+1t
âˆ’4
(49)
â‰¤
32(1 + 4 kÏ•k) ln n
âˆ†2
i
+ 1 + 2Ï€
2
/3
where (49) follows from (47) and (48).
Appendix C. Proofs for Strongly Dependent Reward Distributions
C.1. Basic Bounds
Consider two independent and normally distributed random variables X, Y with mean ÂµX > ÂµY and
variance Ïƒ
2
X, Ïƒ2
Y
. Let âˆ† = ÂµX âˆ’ÂµY > 0 and Ïƒ
2 = Ïƒ
2
X +Ïƒ
2
Y
. We derive in this section the following
bounds which are crucial for the derivation of the regret. We use the notation z
+ = max{0, z} and
Ï†(x) = (2Ï€)
âˆ’1/2
exp(âˆ’x
2/2) for the density function of the standard normal distribution.
E(Y âˆ’ X)
+ â‰¤ ÏƒÏ†(âˆ†/Ïƒ), (50)
E(Y âˆ’ X)
+ â‰¥ 0, (51)
E(X âˆ’ Y )
+ â‰¤ ÏƒÏ†(âˆ†/Ïƒ) + âˆ†, (52)
E(X âˆ’ Y )
+ â‰¥ âˆ†. (53)
The derivation is based on basic properties of Gaussian random variables. Recall that Z = X âˆ’ Y is
normally distributed with mean âˆ† and variance Ïƒ
2
. Therefore,
âˆš
2Ï€ÏƒE(X âˆ’ Y )
+ =
Z âˆ
0
z exp 
âˆ’
(z âˆ’ âˆ†)2
2Ïƒ
2

=
Z âˆ
âˆ’âˆ†
z exp 
âˆ’
z
2
2Ïƒ
2

+ âˆ† Z âˆ
âˆ’âˆ†
exp 
âˆ’
z
2
2Ïƒ
2

= Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

+ âˆ†Ïƒ
Z âˆ†/Ïƒ
âˆ’âˆ
exp 
âˆ’
z
2
2

30
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Standard bounds on the cdf, as can be found in (Dudley, 2002)[Lem. 12.1.6], lead to (53), i.e.
âˆš
2Ï€ÏƒE(X âˆ’ Y )
+ = Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

+ âˆ†Ïƒ
âˆš
2Ï€

1 âˆ’
1
âˆš
2Ï€
Z âˆ
âˆ†/Ïƒ
exp 
âˆ’
z
2
2
!
â‰¥ Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

+ âˆ†Ïƒ
âˆš
2Ï€

1 âˆ’
Ïƒ
âˆš
2Ï€âˆ†
exp 
âˆ’
âˆ†2
2Ïƒ
2

= âˆ†Ïƒ
âˆš
2Ï€.
Similarly, if we consider Z = Y âˆ’ X which has mean âˆ’âˆ†
âˆš
2Ï€ÏƒE(Y âˆ’ X)
+ =
Z âˆ
0
z exp 
âˆ’
(z + âˆ†)2
2Ïƒ
2

= Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

âˆ’ âˆ†Ïƒ
Z âˆ
âˆ†/Ïƒ
exp 
âˆ’
z
2
2

.
Applying the result from Dudley (2002)[Lem. 12.1.6] leads here to the trivial lower bound 0 and,
hence, inequality (51). We use the following inequalities to obtain upper bounds on E(Y âˆ’ X)
+ and
E(X âˆ’ Y )
+. Let Z be a standard normal random variable. Then
Pr(Z â‰¥ c) â‰¥
(
Ï†(c)/(2c) if c â‰¥ 1,
Ï†(c)(1 âˆ’ c)/2 if 0 â‰¤ c < 1.
The first bound can be found in (Dudley, 2014). The second bound is a straightforward adaptation
of the techniques used to derive the first bound. Applying these bounds we get the following upper
bounds on E(Y âˆ’ X)
+. If âˆ†/Ïƒ â‰¥ 1 then
âˆš
2Ï€ÏƒE(Y âˆ’ X)
+ â‰¤ Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

âˆ’ âˆ†Ïƒ
Ïƒ
2âˆ† exp 
âˆ’
âˆ†2
2Ïƒ
2

=
Ïƒ
2
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

and if 0 â‰¤ âˆ†/Ïƒ < 1
âˆš
2Ï€ÏƒE(Y âˆ’ X)
+ â‰¤ Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

âˆ’ âˆ†Ïƒ
(1 âˆ’ âˆ†/Ïƒ)
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

= (Ïƒ
2 âˆ’ âˆ†Ïƒ/2 + âˆ†2
/2) exp 
âˆ’
âˆ†2
2Ïƒ
2

â‰¤ Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

and the inequality (52) follows. The upper bounds on E(X âˆ’ Y )
+ are derived in the same way. For
âˆ†/Ïƒ â‰¥ 1 these are
âˆš
2Ï€ÏƒE(X âˆ’ Y )
+ â‰¤ Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

âˆ’ (Ïƒ
2
/2) exp 
âˆ’
âˆ†2
2Ïƒ
2

+ âˆ†Ïƒ
âˆš
2Ï€
=
Ïƒ
2
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

+ âˆ†Ïƒ
âˆš
2Ï€
31
GRÃœNEWÃ„LDER AND KHALEGHI
and for 0 â‰¤ âˆ†/Ïƒ â‰¤ 1
âˆš
2Ï€ÏƒE(X âˆ’ Y )
+ â‰¤ Ïƒ
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

âˆ’
âˆ†Ïƒ(1 âˆ’ âˆ†/Ïƒ)
2
exp 
âˆ’
âˆ†2
2Ïƒ
2

+ âˆ†Ïƒ
âˆš
2Ï€
= (Ïƒ
2 âˆ’ âˆ†Ïƒ/2 + âˆ†2
/2) exp 
âˆ’
âˆ†2
2Ïƒ
2

+ âˆ†Ïƒ
âˆš
2Ï€
and the inequality (50) follows.
C.2. Proof of Proposition 13
We bound the regret of the two phases individually. Some technical steps are moved further below to
streamline the discussion.
Regret of Phase I. We sweep through all k arms at times 1 to k, m + 1 to m + k, etc. During each
of these phases we build up regret. This regret can be bounded by
X
k
i=1
E(max
j6=i
Xm+i,j âˆ’ Xm+i,i)
+
â‰¤
X
k
i=1
X
j6=i
E(Xm+i,j âˆ’ Xm+i,i)
+
â‰¤ (k âˆ’ 1)X
k
i=1
max
j6=i
E(Xm+i,j âˆ’ Xm+i,i)
+
â‰¤ (k âˆ’ 1)X
k
i=1
âˆ†i +
âˆš
2(k âˆ’ 1)X
k
i=1
Ï†(âˆ†i/
âˆš
2)
â‰¤ (k âˆ’ 1)X
k
i=1
(âˆ†i +
âˆš
2), (54)
where âˆ†i = Âµ
âˆ— âˆ’ Âµi
is the difference between the highest stationary mean of the k arms and the
stationary mean of arm i. Here, we use Inequality (52) and the assumption that the variance of the
individual processes is 1. Therefore, the Ïƒ appearing in the bound (52) is âˆš
2 since Ïƒ
2
is the sum of
the variances of the two individual processes.
Regret of Phase II. To control the regret building up in the second phase we condition on the
observations in a sweep at time lm, l âˆˆ N, i.e. on the observed pay-offs x1, . . . , xk. Arm i
âˆ—
is
selected such that xi
âˆ— â‰¥ maxiâ‰¤k xi and this arm is played for m âˆ’ k steps. We need to control
E(maxiâ‰¤k(Xt
0
,i âˆ’ Xt
0
,Ï€t
0
)
+) for all lm + k + 1 â‰¤ t
0 â‰¤ (l + 1)m. Due to stationarity this is equal
to controlling E(maxiâ‰¤k(Xt,i âˆ’ Xt,Ï€t
)
+), for all k + 1 â‰¤ t â‰¤ m.
We use in the following Px1,...,xk
, Pxi,xu
for the conditional distributions given that X11 =
x1, . . . , Xkk = xk and Xii = xi
, Xuu = xu, and we use Î½ for the marginal measure. In Appendix
32
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
C.2.1 below we show that for any k + 1 â‰¤ t â‰¤ m it holds that
E(max
iâ‰¤k
(Xt,i âˆ’ Xt,Ï€t
)
+)
â‰¤
X
k
u=1
X
i6=u
Z
Px1,...,xk
(u = i
âˆ—
) Ã—
Z
(Xt,i âˆ’ Xt,u)
+dPxi,xu dÎ½(x1, . . . , xk). (55)
The inner integral in (55) can be bounded by using inequality (50)
Z
(Xt,i âˆ’ Xt,u)
+dPxi,xu â‰¤ Ïƒt,iÏ†

âˆ†t,i
Ïƒt,i 
, (56)
where âˆ†t,i = E(Xt,u âˆ’ Xt,i|Xi,i = xi
, Xu,u = xu) which equals E(Xt,u|Xu,u = xu) âˆ’
E(Xt,i|Xi,i = xi) due to the independence between arm u and i. These are posterior means
of Gaussian processes given observations xu and xi
, respectively. Observe that the posterior mean
for Xt,u is related to the posterior mean of the zero mean Gaussian process Xt,u âˆ’ Âµu through
E(Xt,u|Xu,u = xu) = Âµu + E(Xt,u âˆ’ Âµu|Xu,u âˆ’ Âµu = xu âˆ’ Âµu). The posterior equation for the
zero mean Gaussian process Xt,u âˆ’ Âµu given observation xu âˆ’ Âµu at time u is cov(t âˆ’ u)(xu âˆ’ Âµu)
since cov(0) = 1. Therefore, E(Xt,u|Xu,u = xu) = Âµu + cov(t âˆ’ u)(xu âˆ’ Âµu) and, hence,
E(Xt,u|Xu,u = xu) = (1 âˆ’ cov(t âˆ’ u))Âµu + cov(t âˆ’ u)xu. Similarly, E(Xt,i|Xi,i = xi) =
(1 âˆ’ cov(t âˆ’ i))Âµi + cov(t âˆ’ i)xi By defining âˆ†Ëœ
i = xu âˆ’ xi and by using the HÃ¶lder-continuity
assumption, we obtain the following lower bound.
âˆ†t,i = (Âµu âˆ’ Âµi)(1 âˆ’ cov(t âˆ’ k)) + âˆ†Ëœ
icov(t âˆ’ k) + Îµ(t)
â‰¥ âˆ†Ëœ
i âˆ’ |Âµu âˆ’ Âµi
||cov(0) âˆ’ cov(t âˆ’ k)| âˆ’ âˆ†Ëœ
i
|cov(0) âˆ’ cov(t âˆ’ k)| âˆ’ |Îµ(t)|
â‰¥ âˆ†Ëœ
i âˆ’ c(t âˆ’ k)
Î±
(âˆ† + âˆ†Ëœ
i) âˆ’ |Îµ(t)| (57)
with Îµ(t) being a term which can be bounded by |Îµ(t)| â‰¤ (âˆ† + âˆ†Ëœ
i)ckÎ± (see Appendix C.2.2). For
the conditional variance,
Ïƒ
2
t,i = E(X2
t,u|Xu,u = xu) âˆ’ E(Xt,u|Xu,u = xu)
2 + E(X2
t,i|Xi,i = xi) âˆ’ E(Xt,i|Xi,i = xi)
2
,
we obtain the following upper-bound
Ïƒ
2
t,i = 2 âˆ’ cov2
(t âˆ’ u) âˆ’ cov2
(t âˆ’ i) â‰¤ 2c(t âˆ’ u)
Î± + 2c(t âˆ’ i)
Î± â‰¤ 4ctÎ±
, (58)
where we have used the posterior equation for the covariance, that the covariance is by assumption
non-negative, and that, under the HÃ¶lder-continuity assumption, for a non-negative covariance
cov(l) â‰¥ max{0,(1 âˆ’ clÎ±)} and, hence,
cov2
(l) â‰¥ cov(l) max{0, 1 âˆ’ clÎ±
}
=
(
0 if clÎ± â‰¥ 1,
cov(l)(1 âˆ’ clÎ±) otherwise,
â‰¥
(
0 if clÎ± â‰¥ 1,
1 âˆ’ 2clÎ± + c
2
l
2Î± otherwise,
â‰¥ 1 âˆ’ 2clÎ±
.
33
GRÃœNEWÃ„LDER AND KHALEGHI
Combining (56), (57) and (58) we obtain
Z
(Xt,i âˆ’ Xt,u)
+dPxi,xu
â‰¤ (2/Ï€)
1/2
c
1/2
t
Î±/2
exp
âˆ’
(âˆ†Ëœ
i âˆ’ c((t âˆ’ k)
Î± + k
Î±)(âˆ†Ëœ
i + âˆ†))2
8ctÎ±
!
= (2/Ï€)
1/2
c
1/2
t
Î±/2
exp
âˆ’
âˆ†Ëœ 2
i
8ctÎ±

1 âˆ’ c((t âˆ’ k)
Î± + k
Î±
)

1 +
âˆ†
âˆ†Ëœ
i
2
!
. (59)
The bound is maximized for t = m. Substituting this bound back into (55) and after a few
manipulations (see Appendix C.2.3) we see that
Xm
t=k+1
E

max
iâ‰¤k
(Xt,i âˆ’ Xt,Ï€t
)
+

â‰¤ (m âˆ’ k)
amc
1/2k(k âˆ’ 1)
8Ï€(1 âˆ’ bm)

2Ï€
1/2 âˆ’ (1 âˆ’ âˆ†
p
bm/4) exp 
âˆ’
âˆ†2
bm
4
 (60)
if m > k and âˆ† <
âˆš
am/(bm
âˆš
2), where am = 8cmÎ± and bm = c((m âˆ’ k)
Î± + k
Î±).
Combined Regret. Combining (54) with (60) and using âˆ†i â‰¤ âˆ† we can observe that the combined
regret for any m steps is bounded by
k(k âˆ’ 1)
âˆ† + âˆš
2 + (m âˆ’ k)
amc
1/2
8Ï€(1 âˆ’ bm)

2Ï€
1/2 âˆ’ (1 âˆ’ âˆ†
p
bm/4) exp 
âˆ’
âˆ†2
bm
4
!
if m > k and âˆ† <
âˆš
am/(bm
âˆš
2). Given a time horizon n we have dn/me many iterations of Phase
I and II and the overall regret is bounded by
(n/m + 1)k(k âˆ’ 1)
âˆ† + âˆš
2 +
amc
1/2
(m âˆ’ k)
8Ï€(1 âˆ’ bm)

2Ï€
1/2 âˆ’ (1 âˆ’ âˆ†
p
bm/4) exp 
âˆ’
âˆ†2
bm
4
!
â‰¤ (n + m)k(k âˆ’ 1)
âˆ† + âˆš
2
m
+
amc
1/2
8Ï€(1 âˆ’ bm)

2Ï€
1/2 âˆ’ (1 âˆ’ âˆ†
p
bm/4) exp 
âˆ’
âˆ†2
bm
4
!
.
(61)
This is the regret bound stated in the proposition text.
Instead of trying to find the m that minimizes (61) we optimize m over the considerably simpler
expression
âˆ† + âˆš
2
m
+
2c
3/2mÎ±
âˆš
Ï€

=
âˆ† + âˆš
2
m
+
amc
1/2
4
âˆš
Ï€
!
. (62)
The (1 âˆ’ bm) term that we leave out is of minor relevance since bm is small and the negative term in
the bracket that we leave out is not larger than 1. By ignoring this latter term we lose only another
constant. Minimizing (62) with respect to m and rounding up yields
m? =
ï£®
ï£¯
ï£¯
ï£¯
 âˆš
Ï€(âˆ† + âˆš
2)
2Î±c3/2
! 1
1+Î±
ï£¹
ï£º
ï£º
ï£º
.
34
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
C.2.1. PROOF OF INEQUALITY (55)
Due to stationarity and since our policy depends only on the observations at the last sweep we have
that E(maxiâ‰¤k(Xt
0
,i âˆ’ Xt
0
,Ï€t
0
)
+) = E(maxiâ‰¤k(Xt,i âˆ’ Xt,Ï€t
)
+) for any t
0
, lm + k + 1 â‰¤ t
0 â‰¤ lm,
l âˆˆ N, and corresponding t = t
0 âˆ’ lm. Now, consider any t, k + 1 â‰¤ t â‰¤ m and, using i
âˆ—
for the
choice of arm given the observations X1,1, . . . , Xk,k, rewrite the regret in the following way
E

max
iâ‰¤k
(Xt,i âˆ’ Xt,Ï€t
)
+

=
X
k
u=1
E

max
iâ‰¤k
Ï‡{u = i
âˆ—
} Ã— (Xt,i âˆ’ Xt,iâˆ— )
+

=
X
k
u=1
E

max
i6=u
Ï‡{u = i
âˆ—
} Ã— (Xt,i âˆ’ Xt,u)
+

â‰¤
X
k
u=1
X
i6=u
E

Ï‡{u = i
âˆ—
} Ã— (Xt,i âˆ’ Xt,u)
+

=
X
k
u=1
X
i6=u
Z Z Ï‡{u = i
âˆ—
} Ã— (Xt,i âˆ’ Xt,u)
+dPx1,...,xk
dÎ½(x1, . . . , xk)
=
X
k
u=1
X
i6=u
Z
Px1,...,xk
(u = i
âˆ—
) Ã—
Z
(Xt,i âˆ’ Xt,u)
+dPx1,...,xk
dÎ½(x1, . . . , xk) (63)
=
X
k
u=1
X
i6=u
Z
Px1,...,xk
(u = i
âˆ—
) Ã—
Z
(Xt,i âˆ’ Xt,u)
+dPxi,xu dÎ½(x1, . . . , xk) (64)
where (63) follows because Ï‡{u = i
âˆ—} is independent of (Xt,iâˆ’Xt,u)
+ given X11 = x1, . . . , Xkk =
xk and (64) follows since (Xt,i âˆ’ Xt,u)
+ only depends on Xi,i and Xu,u.
C.2.2. BOUND ON Îµ(t).
The term Îµ(t) that we introduced is equal to
Îµ(t) =Âµu(cov(t âˆ’ k) âˆ’ cov(t âˆ’ u)) âˆ’ Âµi(cov(t âˆ’ k) âˆ’ cov(t âˆ’ i))
+ xu(cov(t âˆ’ u) âˆ’ cov(t âˆ’ k)) âˆ’ xi(cov(t âˆ’ i) âˆ’ cov(t âˆ’ k)).
Since, due to our HÃ¶lder assumption, |cov(t âˆ’ k) âˆ’ cov(t âˆ’ u)| â‰¤ c(k âˆ’ u)
Î± â‰¤ ckÎ± and |cov(t âˆ’
k) âˆ’ cov(t âˆ’ i)| â‰¤ ckÎ±, we have the following bound.
|Îµ(t)| â‰¤ (âˆ† + âˆ†Ëœ
i)ckÎ±
.
C.2.3. PROOF OF INEQUALITY (60)
Denote in the following Xu := Xu,u and Xi = Xi,i and recall that âˆ†Ëœ
i = Xu âˆ’ Xi
is normally
distributed with mean Âµu âˆ’ Âµi and variance 2. Writing
f(Xu, Xi) = (2/Ï€)
1/2
c
1/2mÎ±/2
exp
âˆ’
âˆ†Ëœ 2
i
8cmÎ±

1 âˆ’ c((m âˆ’ k)
Î± + k
Î±
)

1 +
âˆ†
âˆ†Ëœ
i
2
!
35
GRÃœNEWÃ„LDER AND KHALEGHI
we have
Z
Px1,...,xk
(u = i
âˆ—
) Ã—
Z
(Xt,i âˆ’ Xt,u)
+dPxi,xu dÎ½(x1, . . . , xk)
â‰¤
Z
Px1,...,xk
(Xu â‰¥ Xi) Ã—
Z
(Xt,i âˆ’ Xt,u)
+dPxi,xu dÎ½(x1, . . . , xk)
â‰¤
Z
Ï‡{xu â‰¥ xi} Ã— f(xu, xi) dÎ½(x1, . . . , xk) (65)
=
Z
Ï‡{xu âˆ’ xi â‰¥ 0} Ã— f(xu, xi) dÎ½(xu, xi), (66)
where the inequality in (65) follows because {Xu â‰¥ Xi} is only dependent on xu and xi and by
bounding the inner integral with (59). Multiplying the density of âˆ†Ëœ
i by f and using a = 8cmÎ±,
b = c((m âˆ’ k)
Î± + k
Î±), d = (ac)
1/2/(4Ï€), in the case where âˆ† <
âˆš
a/(b
âˆš
2), we obtain
Z
Ï‡{xu âˆ’ xi â‰¥ 0} Ã— f(xu, xi) dÎ½(xu, xi)
= d
Z âˆ
0
exp
âˆ’
(âˆ†Ëœ
i(1 âˆ’ b) âˆ’ âˆ†b)
2
a
âˆ’
(âˆ†Ëœ
i âˆ’ (Âµu âˆ’ Âµi))2
4
!
dâˆ†Ëœ
i
â‰¤ d
Z âˆ
0
exp
âˆ’
(âˆ†Ëœ
i(1 âˆ’ b) âˆ’ âˆ†b)
2
a
!
dâˆ†Ëœ
i
=
d
1 âˆ’ b
Z âˆ
âˆ’âˆ†b
exp
âˆ’
âˆ†Ëœ 2
i
a
!
dâˆ†Ëœ
i
=
âˆš
ad
(1 âˆ’ b)
âˆš
2
Z
âˆš
âˆš2âˆ†b
a
âˆ’âˆ
exp
âˆ’
âˆ†Ëœ 2
i
2
!
dâˆ†Ëœ
i
=
âˆš
adâˆš
2Ï€
(1 âˆ’ b)
âˆš
2

1 âˆ’
1
âˆš
2Ï€
Z âˆ
âˆš
âˆš2âˆ†b
a
exp
âˆ’
âˆ†Ëœ 2
i
2
!
dâˆ†Ëœ
i
!
â‰¤
âˆš
Ï€ad
(1 âˆ’ b)

1 âˆ’
1 âˆ’
âˆš
2âˆ†b/âˆš
a
2
âˆš
2Ï€
exp 
âˆ’
âˆ†2
b
2
a
!
=
ac1/2
4(1 âˆ’ b)
âˆš
Ï€

1 âˆ’
1 âˆ’
âˆš
2âˆ†b/âˆš
a
2
âˆš
2Ï€
exp 
âˆ’
âˆ†2
b
2
a
!
â‰¤
ac1/2
8Ï€(1 âˆ’ b)

2Ï€
1/2 âˆ’ (1 âˆ’ âˆ†b/âˆš
a) exp 
âˆ’
âˆ†2
b
2
a

and for m > k, since then a/4 â‰¥ b, this can be further bounded by
ac1/2
8Ï€(1 âˆ’ b)

2Ï€
1/2 âˆ’ (1 âˆ’ âˆ†
p
b/4) exp 
âˆ’
âˆ†2
b
4
 .