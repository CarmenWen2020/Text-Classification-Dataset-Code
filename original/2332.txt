The multi-armed restless bandit problem is studied in the case where the pay-off distributions are
stationary ϕ-mixing. This version of the problem provides a more realistic model for most real-world
applications, but cannot be optimally solved in practice, since it is known to be PSPACE-hard. The
objective of this paper is to characterize a sub-class of the problem where good approximate solutions
can be found using tractable approaches. Specifically, it is shown that under some conditions on
the ϕ-mixing coefficients, a modified version of UCB can prove effective. The main challenge is
that, unlike in the i.i.d. setting, the distributions of the sampled pay-offs may not have the same
characteristics as those of the original bandit arms. In particular, the ϕ-mixing property does not
necessarily carry over. This is overcome by carefully controlling the effect of a sampling policy on
the pay-off distributions. Some of the proof techniques developed in this paper can be more generally
used in the context of online sampling under dependence. Proposed algorithms are accompanied
with corresponding regret analysis.
1. Introduction
As one of the simplest examples of sequential optimization under uncertainty, multi-armed bandit
problems arise in various modern real-world applications, such as online advertisement, and Internet
routing. These problems are typically studied under the assumption that the pay-offs are independently and identically distributed (i.i.d.), and the arms are independent. However, this assumption
does not necessarily hold in many practical situations. Consider, for example, the problem of online
advertisement in which the aim is to garner as many clicks as possible from a user. Grouping adverts
into categories and associating with each category an arm, this problem turns into a multi-armed
bandit. There is dependence over time and across the arms since, for example, we expect a user to be
more likely to select adverts that are related to her selections in the recent past.
In this paper, we consider the multi-armed bandit problem in the case where the pay-offs are
dependent and each arm evolves over time regardless of whether or not it is played. This is an instance
of the so-called restless bandit problem (Whittle, 1988; Guha et al., 2010; Ortner et al., 2014). Since
in this setting an optimal policy can leverage the inter-dependencies between the samples and switch
between the arms at appropriate times, it can obtain an overall pay-off much higher than that given
by playing the best arm, i.e. the distribution with the highest expected pay-off, see Example 1
in (Ortner et al., 2014). However, finding the best such switching strategy is PSPACE-hard, even
in the case where the process distributions are Markovian with known dynamics (Papadimitriou
c 2019 Steffen Grünewälder and Azadeh Khaleghi.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at
http://jmlr.org/papers/v20/17-547.html.
GRÜNEWÄLDER AND KHALEGHI
and Tsitsiklis, 1999). Therefore, it is useful to consider relaxations of the problem with the aim
to devise computationally tractable solutions that effectively approximate the optimal switching
strategy. Approximations of the Markovian restless bandit problem under known dynamics have
been previously considered, see, e.g. (Guha et al., 2010) and references therein. Our focus in this
paper is on a more general setting, where the rewards have unknown distributions, exhibit long-range
dependencies, and have Markov chains as a special case.
We are interested in a sub-class of the restless bandit problem, where the pay-offs have long-range
dependencies. Since the nature of the problem calls for finite-time analysis, we further require that
the dependence weakens over time, so as to make use of concentration inequalities in this setting.
To this end, a natural approach is to assume that the pay-off distributions are stationary ϕ-mixing.
The so-called ϕ-mixing coefficients ϕn, n ∈ N, of a sequence of random variables hXtit∈N measure
the amount of dependence between the sub-sequences of hXtit∈N separated by n time-steps. A
process is said to be ϕ-mixing if this dependence vanishes with n. This notion is more formally
defined in Section 2. In Markov chains ϕ-mixing coefficients are closely related to mixing times.
The mixing time of a Markov chain is a measure of how fast its distribution approaches the stationary
distribution. In particular, it is defined to be the time that it takes for the distribution to be within
1/4
th of the stationary distribution as measured in total variation distance (Levin et al., 2008)[Sec
4.5]. A classical result by Davydov shows that the decrease in distance of the distribution of the
Markov chain to the stationary distribution is controlled (up to a factor of 1/2) by the ϕ-mixing
coefficients of the Markov chain (Doukhan, 1994)[pp.88]. While ϕ-mixing coefficients are related
to the well-studied mixing properties of Markov chains, ϕ-mixing processes correspond to a wide
variety of stochastic processes of which Markov chains are a special case (Doukhan, 1994).
As discussed earlier, the optimal, yet notoriously infeasible strategy for this version of the
problem is to switch between the arms. In this paper, we first address the question of when a
relaxation obtained by identifying the arm with the highest stationary mean would lead to a viable
approximation in this setting. For this purpose, we characterize the approximation error in terms of
the amount of dependence between the pay-offs, and show that if ϕ1 is small, the optimum of the
relaxed problem is close to that given by the optimal switching strategy. Observe that this condition
translates directly to the pay-off distributions being weakly dependent. Next, we address the question
of how an optimistic approach can be devised to identify the best arm. To this end, we propose a
UCB-type algorithm and show that it achieves logarithmic regret with respect to the highest stationary
mean. Interestingly, the amount of dependence in the form of P
i ϕi appears in the bound, and in the
case where the pay-offs are i.i.d., we recover the regret bound of Auer et al. (2002).
A familiar real-world example for the bandit problem considered in this paper corresponds
to recommendation systems where the objective is to present personalized adverts, news-feeds or
Massive Open Online Course (MOOC) material to each user. In these cases, depending on the specific
application, each bandit arm could correspond to an appropriate subject category, indicating, for
example, a genre of products or a class of topics. Once an arm is played, an item of the corresponding
category could be selected at random and presented to the user. There are long-range dependencies
between the pay-offs as reflected by the users’ memory of their past observations. However, the
dependence naturally decays over time, while users’ short-term memory, which in turn influences the
dominant mixing coefficients, can be controlled by capping the maximum frequency at which each
specific recommendation is given to users. With minor modifications, this example carries over to a
larger class of real-world sequential decision making problems which naturally possess a dependency
structure imposed by users’ memory.
2
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Note that even this relaxed version of the problem is far from straightforward. The main challenge
lies in obtaining confidence intervals around empirical estimates of the stationary means. Since
Hoeffding-type concentration bounds exist for ϕ-mixing processes, it may be tempting to use such
inequalities directly with standard UCB algorithms designed for the i.i.d. setting, to find the best arm.
However, as we demonstrate in the paper, unlike in the i.i.d. setting, a policy in this framework may
introduce strong couplings between past and future pay-offs in such a way that the distribution of
the sampled sequence may not even be ϕ-mixing. This is the reason why a standard UCB algorithm
designed for i.i.d. settings is not suitable here, even when equipped with a concentration bound for
ϕ-mixing processes. In fact, this oversight seems to have occurred in previous literature, specifically
in the Improved-UCB-based approach of Audiffren and Ralaivola (2015). We refer to Section 4.1
for a more detailed discussion. We circumvent these difficulties by carefully taking random times1
into account and controlling the effect of a policy on the pay-off distributions. Some of our technical
results can be more generally used to address the problem of online sampling under dependence.
Finally, while the study of the multi-armed bandit problem with strongly dependent pay-offs
at its full generality is beyond the scope of this paper, we provide a complementary example for
this regime. Specifically, we consider a setting where the bandit arms are governed by stationary
Gaussian processes with slowly decaying covariance functions. Such high-dependence scenarios
are quite common in practice. For instance, the throughput of radio channels changes slowly over
time and the problem of choosing the best channel can be modeled by a bandit problem with strongly
dependent pay-offs. The intuitive reason why in this setting it may also be possible to efficiently
obtain approximately optimal solutions is that the strong dependencies can allow for the prediction of
future rewards even from scarce observations. We give a simple switching strategy for this instance
of the problem and show that it significantly outperforms a policy that aims for the best arm. Our
regret bound for this algorithm directly reflects the dependence between the pay-offs: the higher the
dependence the lower the regret.
A summary of our main contributions is listed below.
i. In an attempt to derive a computationally tractable solution for the restless bandit problem, we
first identify a case where the optimal switching strategy can be approximated by playing the
arm with the highest stationary mean. To this end, we show that the loss of settling for the
highest stationary mean as opposed to finding the best switching strategy is controlled by the
amount of inter-dependence as reflected by ϕ1. This is shown in Proposition 9.
ii. We provide a detailed example, namely Example 1, where we demonstrate the challenges
in the non-i.i.d. bandit problem. In particular, we show that a policy in this framework may
introduce strong couplings between past and future pay-offs in such a way that the resulting
pay-off sequence may have a completely different dependency structure.
iii. We develop technical machinery to circumvent the difficulties introduced by the inter-dependence
between the rewards, and allow us to control the effect of a policy on the pay-off distributions.
Some of our derivations concerning sampling under dependence can be of independent interest.
We further propose a UCB-type algorithm, namely Algorithm 1 that deploys these tools to
identify the arm with the highest stationary mean.
1. These correspond to random variables which determine the time at which an arm is sampled.
3
GRÜNEWÄLDER AND KHALEGHI
iv. We provide an upper bound on the regret of Algorithm 1 (with respect to the highest stationary
mean). This is provided in Theorem 12. The regret bound is a function of the amount of
inter-dependence as reflected by the ϕ-mixing coefficients, and in the case where the pay-offs
are i.i.d., we recover the regret bound of Auer et al. (2002)[Thm. 1]. This result along with
Proposition 9 allow us to argue that in the case where dependence is low Algorithm 1 can be
used to approximate the best switching strategy.
The remainder of the paper is organized as follows. In Section 2 we introduce preliminary
notation and definitions. We formulate the problem in Section 3 and give our main results in
Section 4. We conclude in Section 5 with a discussion of open problems.
2. Preliminaries
We start with some useful notation before discussing basic definitions concerning stochastic processes.
Since the bandit problem involves multiple arms (processes), we extend the definition of a ϕ-mixing
process to what we call a jointly ϕ-mixing process, to be able to model the multi-armed bandit
problem. Indeed, in many natural settings, the process is jointly ϕ-mixing. For example, as we
demonstrate in Proposition 5, independent Markov chains are jointly ϕ-mixing.
Notation. Let N+ := {1, 2, . . .} and N := N ∪ {∞} denote the set and extended set of natural
numbers respectively. We introduce the abbreviation am..n, m, n ∈ N+, m ≤ n, for sequences
am, am+1, . . . , an. Given a finite subset C ⊂ N+ and a sequence a, we let aC := {ai
: i ∈ C}
denote the set of elements of a indexed by C. If XC is a sequence of random variables indexed by
C ⊂ N+, we denote by σ(XC) the smallest σ-algebra generated by XC.
Notion of ϕ-dependence. Part of our results concern the so-called ϕ-dependence between σalgebras defined as follows, see, e.g. (Doukhan, 1994).
Definition 1 Consider a probability space (Ω, A, P) and let U and V denote σ-subalgebras of A
respectively. The ϕ-dependence between U and V is is given by
ϕ(U, V) := sup{|P(V ) − P(V |U)| : U ∈ U, P(U) > 0, V ∈ V}.
If X and Y are two random variables measurable with respect to A we simplify notation by letting
ϕ(X, Y ) := ϕ(σ(X), σ(Y )) denote the ϕ-dependence between their corresponding σ-algebras;
distinction will be clear from the context. Similarly, if XA and XB are finite sequences of random
variables, with A, B ⊂ N+ their ϕ-dependence can be similarly defined as
ϕ(XA, XB) := ϕ(σ(XA), σ(XB)).
In words, ϕ(XA, XB) measures the maximal difference between the probability of an event V and
its conditional probability given an event U, where U and V are determined by random variables
indexed by A and B respectively. The notion of ϕ-dependence carries over from probability measures
to expectations. In particular, consider a real-valued random variable X defined on some probability
space (Ω, A, P), and denote by G some collected information in the form of a σ-subalgebra of A.
Let E(X|G) denote Kolmogorov’s conditional expectation, i.e. a G-measurable random variable Z
such that R
B
Z =
R
B X for all B ∈ G. As follows from Theorem 2 below, due to Bradley (2007)[vol.
1 pp. 124], the difference between E(X|G) and EX is effectively upper-bounded by ϕ(G, σ(X)).
4
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Theorem 2 (Bradley (2007)) Let (Ω, A, P) be a probability space, let X be a real-valued random
variable with kXk1 < ∞ and let G be some σ-subalgebra of A. Then
2ϕ(G, σ(X)) = sup kE(Y |G) − E(Y )k1/kY k1, (1)
where the supremum is taken over all σ(X)-measurable random variables Y with kY k1 < ∞.
Furthermore, for any B ∈ G it holds that
Z
B
|E(X|G) − E(X)| dP ≤ 2P(B)kXk∞ϕ(G, σ(X)). (2)
Observe that because X is trivially σ(X)-measurable (1) given in the theorem implies that
kE(X|G) − E(X)k1 ≤ 2kXk1ϕ(G, σ(X)).
Stochastic Processes & ϕ-mixing Properties. Let (X , BX ) be a measurable space; we let X ⊂
[0, 1] 2
and denote by B
(m)
X
the Borel σ-algebra on X
m, m ∈ N+. We denote by X ∞ the set of all
X -valued infinite sequences indexed by N+. A stochastic process can be modeled as a probability
measure over the space (X ∞, B) where B denotes the σ-algebra on X ∞ generated by the cylinder
sets. Associated with the stochastic process is a sequence of random variables X1, X2, . . ., where
Xt
: X ∞ → X is the projection onto the t’th element, i.e. Xt(ω) = ωt for ω ∈ X ∞ and t ∈ N+.
A process ρ is stationary if ρ(X1..m ∈ B) = ρ(Xi+1..i+m ∈ B) for all Borel sets B ∈ B(m)
X
,
i, m ∈ N+. The term stochastic process refers to either the process distribution ρ or the associated
sequence of random variables Xt
, t ∈ N+; reference will be clear from the context.
Definition 3 (Stationary ϕ-mixing Process) Consider a stationary stochastic process hXiii∈N+
.
Its ϕ-mixing coefficients are given by
ϕn := sup
u,v∈N+
ϕ(X1..u, Xu+n..u+n+v−1), n ∈ N+
and measure the ϕ-dependence between σ(X1..u) and σ(Xu+n..u+n+v−1)
X1, . . . , Xu ← gap of length n → Xu+n, . . . , Xu+n+v−1
The process is said to be ϕ-mixing if limn→∞ ϕn = 0.
When modeling a bandit problem in this paper, we are concerned with some k ∈ N+ stochastic
processes with a joint distribution that is stationary ϕ-mixing. More specifically, for a fixed k ∈ N,
let (Ω, A, P) be a probability space where Ω := Ω1 × . . . × Ωk with Ωi
:= X ∞, i ∈ 1..k, P a
probability measure and A obtained via the cylinder sets. Let B
(m,k)
X
denote the Borel σ-algebra on
(X
m)
k
, m ∈ N+. In much the same way as with the single-process described above, associated with
the joint process is a sequence of random variables hXt,j i, t ∈ N+, j ∈ 1..k where Xt,j : Ω → X
is the projection on to the t, j-th element, i.e. Xt,j (ω) = ωt,j for ω ∈ Ω, t ∈ N+, j ∈ 1..k. The
measure P is stationary if P(X1..m,1..k ∈ B) = P(Xi+1..i+m,1..k ∈ B) for every B ∈ B(m,k)
X
and
i, m ∈ N+. As above, the term stochastic process is used interchangeably to correspond to the
sequence of random variables hXt,j i, t ∈ N+, j ∈ 1..k or their corresponding joint measure P.
2. More generally X can be a finite set or a closed interval [a, b], for a < b, a, b ∈ R.
5
GRÜNEWÄLDER AND KHALEGHI
Definition 4 (Jointly Stationary ϕ-mixing Processes) For a fixed k ∈ N+, consider a stationary
process hXt,j i, t ∈ N+, j ∈ 1..k. Its ϕ-mixing coefficients are given by
ϕn := sup
u,v∈N+
ϕ(X1..u,1..k, Xu+n..u+n+v−1,1..k), n ∈ N+
and measure the ϕ-dependence between σ(X1..u,1..k) and σ(Xu+n..u+n+v−1,1..k). The process is
said to be jointly ϕ-mixing if limn→∞ ϕn = 0.
We use ϕn to denote the ϕ-mixing coefficient corresponding to a single process or to a joint process
given by Definitions 3 and 4 respectively; the notion will be apparent from the context. Under
the assumption that the joint process is stationary ϕ-mixing, there could be dependence between
the processes, as the mixing requirement needs only to be fulfilled by the joint process. The
assumption that the process is jointly ϕ-mixing is fulfilled by a variety of well-known models,
including independent Markov chains. More generally, as follows from Proposition 5 below, if we
have k independent ψ-mixing processes then the joint process is ϕ-mixing. The ψ-mixing property
is defined in the same way as ϕ-mixing whereby the ϕ-dependence given by Definition 1 is replaced
with
ψ(U, V) := sup{|1 − ρ(U ∩ V )/(ρ(V )ρ(U))| :U ∈ U, ρ(U) > 0, V ∈ V, ρ(V ) > 0},
and the ψ-mixing coefficients are defined in a manner analogous to Definitions 3. A ψ-mixing
process is also ϕ-mixing and the ψ-mixing coefficients upper bound ϕ-mixing coefficients.
Proposition 5 Let (Ω, A, P) be some probability space with k mutually independent processes
defined on it. If each of these processes is ψ-mixing then the joint process is also ψ-mixing and for
all i ∈ N, 1 + ψ˜
i ≤ (1 + ψi)
k
, where ψ˜
i are the mixing coefficients of the joint process and the ψi
are upper-bounds on the mixing coefficients of the individual processes.
The proof is provided in Appendix A.2. Using the above result, it can be shown that independent
Markov chains are jointly ϕ-mixing. More specifically, we have the following example.
Corollary 6 (Independent Markov chains are jointly ϕ-mixing.) A set of k ∈ N+ mutually independent, stationary ergodic, finite-state Markov processes ρi
, i = 1..k give rise to a jointly stationary
ϕ-mixing process.
Proof By Theorem 3.1 of Bradley (2005) each process ρi
, i = 1..k is ψ-mixing. Moreover, by
Proposition 5 above, k mutually independent ψ-mixing processes are jointly ϕ-mixing. As a result
ρi
, i = 1..k, are jointly stationary ϕ-mixing.
The significance of the above observation is that jointly ϕ-mixing processes have mutually independent Markov processes as special case.
3. Problem Formulation: the Jointly ϕ-mixing Bandit Problem.
We assume in the sequel that a total of k < ∞ bandit arms are given, where for each i ∈ 1..k, arm i
corresponds to a stationary process that generates a time series of pay-offs X1,i, X2,i, . . . Furthermore,
we assume that the joint process over the k arms is ϕ-mixing in the sense of Definition 4, and that
its sequence of mixing coefficients is summable, i.e. kϕk := P∞
i=1 ϕi < ∞. Each process has
6
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
stationary mean µi
, i = 1..k and we denote by µ
∗
:= max{µ1, . . . , µk} the highest stationary mean.
We sometimes denote the arm with the highest stationary mean as the best arm. At every time-step t ∈
N+, a player chooses one of k arms according to a policy πt and receives a reward Xt,πt
. The player’s
objective is to maximize the sum of the pay-offs received. The policy has access only to the pay-offs
gained at earlier stages and to the arms it has chosen. Let hFtit≥0 be a filtration that tracks the payoffs obtained in the past t rounds, i.e. F0 = {∅, Ω}, and Ft = σ(X1,π1
, . . . , Xt,πt
), t ≥ 1. A policy
is a sequence of mappings πt
: Ω → {1, . . . , k}, t ≥ 1, each of which is measurable with respect to
Ft−1. Note that the assumption that πt
, t ≥ 1 is measurable with respect to Ft−1 is equivalent to the
assumption that the policy can be written as a function of the past pay-offs and chosen arms, see, e.g.
(Shiryaev, 1991)[Thm. 3, pp.174]. Let Π = {π := hπtit≥1 : πt
is Ft−1-measurable for all t ≥ 1}
denote the space of all possible policies. We also let Gt denote the filtration that keeps track of all the
information available up to time t (including unobserved pay-offs). More specifically, let hGtit≥0,
G0 = N and Gt = σ(
Sk
i=1
St
s=1 σ(Xs,i) ∪ N ) for t ≥ 1, where N is the family of measurable sets
of P-measure zero. We define the maximal value that can be achieved in n rounds as
v
∗
n = sup
π∈Π
Xn
t=1
EXt,πt
. (3)
The regret that builds up over n rounds for any strategy π is
Rπ(n) := v
∗
n − E(Xt,πt
). (4)
To simplify notation, we may use R(n) when the policy π is clear from the context.
Remark 7 Requiring kϕk < ∞, which is standard in the literature on empirical process theory
involving mixing processes, does not lead to a strong assumption. This condition is already fulfilled
by any stationary ergodic finite-state Markov chain, see, e.g. (Bradley, 2005). In fact, as follows
from Theorem 3.4 of Bradley (2005) the ϕ-mixing coefficient corresponding to any (not necessarily
stationary) ϕ-mixing Markov process has an exponentially fast rate of decay, giving rise to a
summable sequence of ϕ-mixing coefficients. Note that our focus in the sequel is on the more general
case of ϕ-mixing (not necessarily Markov) processes.
4. Main Results
We consider the restless bandit problem in a setting where the reward distributions are jointly ϕmixing as formulated in Section 3. Recall that, while the optimal strategy in this case is to switch
between the arms, obtaining the best switching strategy is PSPACE-hard. We address the question
of when and how a good and computationally tractable approximation of the optimal policy can
be obtained in this setting. The former question is answered in Section 4.2 where we characterize
(in terms of ϕ1) the loss of settling for the highest stationary mean as opposed to following the
best switching strategy. We show that for small ϕ-mixing coefficients, the optimum of this relaxed
problem is close to that given by v
∗
n
. To answer the latter, we devise a UCB-type algorithm in
Section 4.3 to identify the arm with the highest stationary mean. The main challenge lies in building
confidence intervals around empirical estimates of the stationary means. Indeed, as we demonstrate
in Section 4.1, unlike in the i.i.d. setting, a policy in this framework may introduce strong couplings
between past and future pay-offs in such a way that the resulting pay-off sequence may not even
be ϕ-mixing. As a result, a standard UCB algorithm designed for an i.i.d. setting is not suitable
7
GRÜNEWÄLDER AND KHALEGHI
here, even when equipped with a Hoeffding-type concentration bound for ϕ-mixing processes. We
circumvent these difficulties in Sections 4.2 and 4.3 by controlling the effect of a policy on the
pay-off distributions. Part of the analysis in these two sections relies on some technical results for
ϕ-mixing processes outlined in Appendix A.1, which may be of independent interest. Finally, our
results for the weakly dependent reward distributions are complemented in Section 4.4, where we
study an example of a class of strongly dependent processes and give a simple switching strategy
that significantly outperforms a policy that aims for the best arm.
4.1. Policies, Random Times, and the ϕ-mixing Property
Recall that a policy πt
, t ∈ N+ is a function which based on the past (observed) data samples one
of k bandit arms at time-step t. Therefore, since this decision is based on samples generated by a
random process, the times at which bandit arms are played can be naturally modeled via random
times. More formally, denote by τi,j : Ω → N+ a random variable which determines the time at
which the j-th arm is sampled for the i-th time, i ∈ N+ and j ∈ 1..k, and observe that for any t ∈ N
the event {τi,j = t} is in Ft−1. We denote by
Xτi,j := X
t∈N+
χ{τi,j = t}Xt,j
the pay-off obtained from sampling arm j at random time τi,j for any i ∈ N+, j ∈ 1..k, where χ is
the indicator function, i.e. χ{τi,j = t} is 1 for all ω ∈ Ω such that τi,j (ω) = t, and is 0 otherwise.
The main challenge in devising a policy for the ϕ-mixing bandit problem is that, depending on
the policy used, the dependence structure of the pay-off sequence Xτ1,j , Xτ2,j , . . ., for j ∈ 1..k may
be completely different from that of X1,j , X2,j , . . .. Note that this differs from the simpler i.i.d.
setting where the distribution of the j-th arm (with all its characteristics) carries over to that of the
sampled sequence. This is illustrated in Example 1 below.
Example 1 Consider a two-armed bandit problem where the second arm is deterministically set to
0, i.e. Xt,2 = 0, t ∈ N+ and the first arm has a process distribution described by a two state Markov
chain with the initial distribution and stationary distribution over the states both being (1/2 1/2)>
and with the following transition matrix,
T =

1 −  
 1 − 

, with some  ∈ (0, 1).
Observe that for this process, if  is small, with high probability the Markov chain stays in its
current state. Now consider a policy π, and denote by τ1, τ2, . . . the sequence of random times at
which π samples the first arm according to the following simple rule. Set τ1 = 1. For subsequent
random times, if Xτn,1 = X1,1 for n ∈ N+ then τn+1 = τn + 1. Otherwise, τn+1 is set to be
significantly larger than τn to guarantee that the distribution of Xτn+1,1 given Xτn,1 is close to
the stationary distribution of the Markov chain, during which time the first arm is sampled. The
sequence Xτ1,1, Xτ2,1, . . . so generated is highly dependent on X1,1 and is not ϕ-mixing. In fact, the
expected pay-offs given the first observation, i.e. E(Xτn,1|X1,1), n ∈ N+, are very different from
the stationary mean EX1,1 if  is small. In particular, EX1,1 = 0.5 while E(Xτn,1|X1,1 = 1) is
at least 1/(1 + 2) − 1/10 due to Equation (41) on page 28 and, hence, for  ≤ 0.01 we have that
E(Xτn,1|X1,1 = 1) ≥ 0.8.
8
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
A more detailed treatment of the above example is given in Appendix A.4.
Indeed, it is a policy’s access to the (observed) past data which can lead to strong couplings
between past and future pay-offs in this framework. This point has been overlooked in the work
of Audiffren and Ralaivola (2015) which relies on Improved-UCB (Auer and Ortner, 2010) to
identify the arm with the highest stationary mean, by eliminating potentially sub-optimal arms. The
elimination process depends on the data, and the time-steps at which a particular arm is played
depend on the remaining arms. Hence, random times and the policy’s memory have to be carefully
taken into consideration, as the process distribution of the sampled sequence is different from that of
the corresponding arm. This notion has not been accounted for in their algorithm, and the confidence
intervals involved correspond to the distributions of the arms, and not to those of the sampled
sequences, and are therefore invalid in this non-i.i.d. setting.
4.2. Approximation Error
We start by translating ϕ-mixing properties to those of expectations in order to control the difference
between what a switching strategy can achieve as compared to the highest stationary mean. Prior
to delving into the bandit problem we consider a single bounded real-valued stationary ϕ-mixing
process hXtit∈N+
sampled at random times τ1 < τ2 < . . ., where τi
, i ∈ N+, is fully defined by
the past observations Xτ1
, . . . , Xτi−1
. We control the difference between the mean of the sampled
process hXτi
ii∈N+
from the stationary mean of the original process. The following proposition
shows that the difference in the means is controlled by the ϕ-mixing coefficients and the increments
in the stopping times; the proof is provided in Appendix A.3.
Proposition 8 Assume that hXtit∈N+
is a stationary ϕ-mixing process with mixing coefficients
hϕiii∈N+
such that EXt = µ, t ∈ N+, and supt∈N+
|Xt
| ≤ c for some c ∈ [0, ∞). Furthermore,
let τ1, τ2, . . . be a sequence of random times such that τi + ` ≤ τi+1 a.s. for some ` ≥ 1 and all
i ∈ N+, and all τi+1 are σ(Xτ1
, . . . , Xτi
)- measurable with τ1 ∈ N+ being a fixed time. Then for
any n ∈ N+



1
n
Xn
i=1
EXτi − µ


 ≤ 2cϕ`
.
In other words, when using the sample mean of the sampled process as an estimate of the stationary
mean then the bias of the estimator is bounded through the ϕ`-mixing coefficient. This result has
further implications, in particular, it is telling us something about the leverage a switching policy
has. A switching policy selects effectively random times for each arm at which the arm is played and
this result is saying that the summed pay-off it can gather cannot be more than 2cnϕ`
larger than the
stationary mean of the arm. Now, a policy is free to play the arms at any time and we only know that
τi+1 ≥ τi + 1 for any random time τi
, i.e. ` = 1. This intuition underlies the following proposition.
Proposition 9 Consider the jointly stationary ϕ-mixing bandit problem formulated in Section 3. Let
µ1, . . . , µk be the means of the stationary distributions and let µ
∗
:= max{µ1, . . . , µk}. Let ϕ1 be
the first ϕ-mixing coefficient as given by Definition 4. For every n ≥ 1 we have
v
∗
n − nµ∗ ≤ 2nϕ1.
9
GRÜNEWÄLDER AND KHALEGHI
Proof Consider an arbitrary policy hπtit∈N+
and an arbitrary t ∈ N+ then
EXt,πt =
X
k
j=1
X
t
i=1
EXt,j × χ{τi,j = t}.
Recall the definition of Gt−1 in Section 3 and observe that τi,j is Gt−1-measurable. Hence, we have
with B = {τi,j = t} that
EXt,j × χ{τi,j = t} = EE(Xt,j |Gt−1) × χ{τi,j = t} =
Z
B
E(Xt,j |Gt−1).
We can extend the ϕ-mixing property of the joint process from σ(X11, . . . , Xtk)to Gt by applying
Lemma A.2 and we get


Z
B
(E(Xt,j |Gt−1) − EX1,j )

 ≤ 2ϕ1P(B).
Since the different sets {τi,j = t}, j ∈ {1, . . . , k}, i ∈ {1, . . . , t} are disjoint
EXt,πt − µ
∗ ≤
X
k
j=1
X
t
i=1
(EXt,j × χ{τi,j = t} − P(τi,j = t)EX1,j )
≤ 2ϕ1
X
k
j=1
X
t
i=1
P(τi,j = t)
≤ 2ϕ1.
Observe that this relaxation introduces an inevitable linear component to the regret as shown by
Proposition 9. However, we argue that if the reward distributions are weakly dependent in the sense
that ϕ1 is small, we may settle for the best arm instead of following the best switching strategy.
4.3. An Optimistic Approach
In this section we propose a UCB-type algorithm to identify the arm with the highest stationary mean
in a jointly ϕ-mixing bandit problem. Consider the bandit problem described in Section 3, where we
have k arms each with a bounded stationary pay-off sequence such that the joint process is stationary
ϕ-mixing. Suppose that the processes are weakly dependent in the sense that ϕ1 ≤  for some small
. As discussed in Section 4.2, in this case a policy to settle for the best arm can serve as a good
approximation for the best switching strategy. More specifically, let
Rπ(n) := nµ∗ −
Xn
t=1
EXt,πt
denote the regret of a policy π with respect to the arm with the highest stationary mean. From
Proposition 9 we have 1
n
(Rπ(n) − Rπ(n)) ≤  and our objective in this section is to minimize Rπ.
Recall that in light of the arguments provided in Section 4.1 it is crucial to take a policy’s access to
past (observed) data into account when devising a strategy for the bandit problem in this framework.
10
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
To address the challenge induced by the inter-dependent reward sequences obtained at random times,
our approach relies on the following key observation. Suppose we obtain a sequence of m ∈ N+
consecutive samples Xτ,i, Xτ+1,i, . . . , Xτ+m,i from arm i ∈ 1..k starting at a random time τ . For a
long batch, i.e. large enough m, the average expectations 1
m
Pm−1
j=0 EXτ+j,i become close to the
stationary mean µi
. More formally we have Lemma 10 below.
Lemma 10 For a fixed i ∈ 1..k and m ∈ N+, consider the consecutive samples Xτ,i, Xτ+1,i,
. . . , Xτ+m,i, where τ : Ω → N+ is a random time at which the i-th arm is sampled. Let µi denote
the stationary mean of arm i. We have






µi −
1
m
mX−1
j=0
EXτ+j,i






≤
2
m
kϕk .
Proof For simplicity of notation we denote Xt,i by Xt and Xτ,i by Xτ . Recall that Gt denotes the
filtration that keeps track of all the information available up to time t (including unobserved pay-offs).
Observe that χ{τ = t} is Gt−1-measurable so that the event {τ = t} is in Gt−1 for all t ∈ N+. As a
result, for any t ∈ N+ and j ∈ 0..m − 1 we have
E(χ{τ = t}Xt+j |Gt−1) = χ{τ = t}E(Xt+j |Gt−1), (5)
see, e.g. (Shiryaev, 1991)[pp.216]. We obtain






mµi −
mX−1
j=0
EXτ+j,i






=






X
t∈N+
mX−1
j=0
E(χ{τ = t}Xt+j ) − Eχ{τ = t}EXt






(6)
=






X
t∈N+
mX−1
j=0
EE(χ{τ = t}Xt+j |Gt−1) − Eχ{τ = t}EXt






(7)
=






X
t∈N+
mX−1
j=0
E(χ{τ = t}E(Xt+j |Gt−1)) − Eχ{τ = t}EXt






(8)
≤
X
t∈N+
mX−1
j=0
E

χ{τ = t}|E(Xt+j |Gt−1) − EXt
|

(9)
=
X
t∈N+
mX−1
j=0
E

χ{τ = t}|E(Xt+j |Gt−1) − EXt+j |

(10)
=
X
t∈N+
mX−1
j=0
Z
{τ=t}
|E(Xt+j |Gt−1) − EXt+j |
≤
X
t∈N+
mX−1
j=0
2ϕ(Gt−1, σ(Xt+j ))Eχ{τ = t} (11)
≤
X
t∈N+
Xm
j=1
2ϕjEχ{τ = t} (12)
≤ 2 kϕk (13)
  
GRÜNEWÄLDER AND KHALEGHI
Algorithm 1 A UCB-type Algorithm for ϕ-mixing bandits.
Input: Number k of arms; Sum kϕk := P∞
i=1 ϕi of mixing coefficients3
Initialization: Play each arm once and update the empirical mean for each arm
1: for i = 1..k do
2: Xi ← Xi
3: si ← 1 . si
, i = 1..k denotes the number of times arm i has been selected
Main Loop:
4: for t = 1..∞ do
5: Arm Selection: Select the arm that maximizes the following UCB
j ← min

argmax
u∈1..k
Xu +
s
8ξ(
1
8 + ln t)
2
su
+
kϕk
2
su−1

 , where ξ := 1 + 8 kϕk
. The min operator is used to give precedence to the smaller arm-index in the case of a tie.
6: Update: Play arm j for 2
sj consecutive iterations and update the empirical mean accordingly.
tj ← t t ← t + 2sj Xj ←
1
2
sj
X
t−1
t
0=tj
Xt
0
,j sj ← sj + 1
where (6) and (7) are due to stationarity and the law of total expectation respectively, (8) follows
from (5), (10) follows from stationarity, (11) follows from Theorem 2, namely Inequality (2), and
noting that kXk∞ = 1, (12) follows directly from Definition 4, and (13) follows from the definition
of kϕk.
Inspired by this result, we provide Algorithm 1 which, given the number k of arms and the sum
kϕk of the ϕ-mixing coefficients, works as follows. First, each arm is sampled once for initialization.
Next, from t = k + 1 on, arms are played in batches of exponentially growing length. Specifically, at
each round arm j with the highest upper-confidence on its empirical mean is selected, and played for
2
sj consecutive time-steps, where sj denotes the number of times that arm j has been selected so far.
The upper confidence bound is calculated based on a Hoeffding-type bound for ϕ-mixing processes
given by Corollary 2.1 of Rio (1999). The 2
sj samples obtained by playing the selected arm are
used in turn to calculate (from scratch) the arm’s empirical mean. The algorithm does not require
the values of the individual ϕ-mixing coefficients, but only their sum kϕk. In fact, any upper-bound
ϑ ≥ kϕk may be used, in which case ϑ would replace kϕk in the regret bound of Theorem 12.
To analyze the regret of Algorithm 1, first recall that in an i.i.d. setting we trivially have
R(n) = nµ∗ − µj
Pk
j=1 ETj (n) = Pk
j=1 ∆jETj (n) where Tj (n) is the total number of times that
arm j is played by the algorithm in n rounds and ∆j := µ
∗ − µj , j ∈ 1..k. In our framework, this
equality does not necessarily hold due to the inter-dependencies between the pay-offs. However,
as shown in Proposition 11 below, an analogous result in the form of an upper-bound holds for our
algorithm.
3. Note that only the sum of the coefficients is needed, not the individual coefficients ϕi.
12
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Proposition 11 Consider the regret R(n) of Algorithm 1 after n rounds of play. We have,
R(n) ≤
X
k
j=1
∆jETj (n) + 2k
 Xn
l=0
ϕl
!
log n
Proof Denote by τi,j : Ω → N+ the random time at which the j-th arm is sampled for the i-th time.
Note that for any t ∈ N the event {τi,j = t} is measurable with respect to the filtration Gt−1 that
keeps track of all the information available up to time t. First note that
E(χ{τi,j = t}Xt+l,j ) = EE(χ{τi,j = t}Xt+l,j |Gt−1)
= E (χ{τi,j = t}E (Xt+l,j |Gt−1))
≥ (µj − 2ϕl) P (τi,j = t) (14)
where the second equality follows from the fact that the event {τi,j = t} is Gt−1-measurable and (14)
follows from Theorem 2. We have,
R(n) = nµ∗ −
Xn
t=1
EXt,πt
= nµ∗ −
Xn
t=1
X
k
j=1
log
Xn
m=1
min{2mX−1,n−t}
l=0
Eχ{τm,j = t}Xt+l,j
≤ nµ∗ −
Xn
t=1
X
k
j=1
log
Xn
m=1
min{2mX−1,n−t}
l=0
P (τm,j = t) (µj − 2ϕl)
≤ nµ∗ −
Xn
t=1
X
k
j=1
log
Xn
m=1
min{2mX−1,n−t}
l=0
P (τm,j = t) µj + 2k
 Xn
l=0
ϕl
!
log n
=
X
k
j=1
∆jETj (n) + 2k
 Xn
l=0
ϕl
!
log n
where the last inequality follows from (14).
An upper-bound on R(n) is given by Theorem 12 below with proof provided in Appendix B. Recall
that ∆i
:= µ
∗ − µi
is the difference between the highest stationary mean µ
∗
and the stationary mean
µi of arm i.
Theorem 12 (Regret Bound.) For the regret R(n) of Algorithm 1 after n rounds of play. We have,
R(n) ≤
X
k
i=1
µi6=µ
∗
32(1 + 8 kϕk) ln n
∆i
+ (1 + 2π
2
/3)(X
k
i=1
∆i) + kϕk log n
Remark. Interestingly, kϕk appears in the bound of Theorem 12. Indeed, in the case where the
pay-offs are i.i.d., we recover (up to some constant) the regret bound of Auer et al. (2002)[Thm. 1].
13
GRÜNEWÄLDER AND KHALEGHI
4.4. Strongly Dependent Reward Distributions: a Complementary Example
At the other end of the extreme lie bandit problems with strongly dependent pay-off distributions. Our
objective in this section is to give an example where a simple switching strategy can be obtained in
this case to leverage the strong inter-dependencies between the samples. This approach gives a much
higher overall pay-off than what would be given by settling for the best arm, and is computationally
efficient. The intuition is that in many cases strong dependencies may allow for the prediction of
future rewards even from scarce observations of a sample path.
We consider a class of stochastic processes for which we can easily control the level of dependency. A natural choice is to use stationary Gaussian processes on N+. Recall that a Gaussian
process is fully specified by its covariance function cov: N+ × N+ → R. Also, for any covariance
function Kolmogorov’s consistency theorem guarantees the existence of a Gaussian process with this
particular covariance function, see, e.g. (Giné and Nickl, 2016). A Gaussian process is stationary
if it has constant mean on N+ and its covariance can be written as cov(s) = cov(t, t + s) for all
t ∈ N+, s ∈ N. We measure the degree of dependence of the process by means of Hölder-continuity
of the covariance function. In particular, we assume that there exists some c > 0 and α ∈ (0, 1] such
that for all s, t ∈ N, |cov(s) − cov(t)| ≤ c|s − t|
α and we assume that the covariance function is
non-negative. A low c and α correspond to highly dependent processes since the covariance decreases
slowly over time. A slowly decreasing covariance also implies large ϕ-mixing coefficients, since
|cov(t)| /cov2
(0) ≤ 2(ϕ(X0, Xt)ϕ(Xt
, X0))1/2
, by Rio (1999)[Thm. 1.4]. Consider a k-armed
bandit problem where each arm is distributed according to a stationary Gaussian process with stationary mean µi
, i = 1..k. For simplicity, we assume that the processes are mutually independent
with the same, unknown, covariance function cov(·). While cov(·) is assumed unknown, we have
access to an upper-bound on its rate of decay. That is, we are given constants c and α such that
cov(·) is α, c-Hölder continuous. We further assume that an upper bound on the stationary means of
the processes is known and that cov(0) = 1. In order to obtain direct control on the regret Rπ(n)
we would need to make inference about the best possible switching strategy. Instead, we provide
guarantees for the regret R+
π
(n) of policy π with respect to the best policy that can choose arms in
hindsight, i.e. R+
π
(n) := Pn
t=1 E(maxi≤k(Xt,i − Xt,πt
)). Note that R+
π
(n) ≥ Rπ(n).
We provide a simple algorithm, namely, Algorithm 2, that exploits the dependence between
the pay-offs. Starting from an exploration phase, the algorithm alternates between exploration
and exploitation, denoted Phase I and Phase II respectively. In Phase I it sweeps through all k
arms to observe the corresponding pay-offs. In Phase II it plays the arm with the highest observed
pay-off for m − k rounds, where m is a (large) constant given by (1) which reflects the degree of
dependence between the samples in the processes. We need not estimate the stationary distributions
in this algorithm, as bounds on the differences between the stationary means suffice. Indeed, these
differences are of minor relevance unless they are high as compared to the dependence between the
individual processes. We have the following regret bound whose proof is given in Appendix C.2.
Proposition 13 Given ∆ such that maxi,j |µi − µj | ≤ ∆ and α, c such that |1 − cov(t)| ≤ ctα for
all t ≥ 0, the regret R+(n) of Algorithm 2 after n rounds is at most,
(n + m?
)k(k − 1)
∆ + √
2
m?
+
am? c
1/2
8π(1 − bm? )

2π
1/2 − (1 − ∆
p
bm? /4) exp 
−
∆2
bm?
4
!
,
with am? = 8c(m?
)
α, bm? = c((m? − k)
α + k
α).
14
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Algorithm 2 An Algorithm for highly dependent arms.
Input: a bound ∆ on the difference between the stationary means, maxi,j≤k |µi − µj | ≤ ∆; Hölder
coefficients α, c such that |1 − cov(t)| ≤ ctα for all t ≥ 0.
1:
m? ←




 √
π(∆ + √
2)
2αc3/2
! 1
1+α




.
2: if m? ≤ k then m? ← k + 1
3: if ∆ <
p
8(m∗)
α/(
√
2c(m? − k)
α + k
α)) then
4:
m? ← min argmax
m∈N
{m : ∆ ≥
√
8mα/(
√
2c(m − k)
α + k
α
))}.
5: for l = 0 . . . ∞ do
6: Phase I: observe pay-offs Xlm?+1,1 = x1, . . . , Xlm?+k,k = xk
i
∗ ← min argmax
i≤k
{x1, . . . , xk}
7: Phase II: play arm i
∗
for m? − k steps.
To interpret the bound consider for simplicity α = 1 and the case of a highly dependent process
and, hence, a very small c. If we choose to play the arm with the highest stationary mean at all
rounds, then standard bounds on the normal distribution give us a bound of order n exp(−(∆2/4))
on the regret. In this case, the regret of Algorithm 2 is of order
nk2
c
3/4
because bm? = cm? ≈ c
1/4π
1/4
((∆+√
2)/2)1/2
is insignificant for small c, c
1/2am? = 8c
3/2m? ≈
8c
3/4π
1/4
((∆ + √
2)/2)1/2
and the bracket on the right side is about 2π
1/2 − 1. The gap ∆ itself is
not of high importance because in Phase I the algorithm selects the arm with highest current pay-off
and the value stays stable over a long period as c is small. Both regret bounds are linear in n because
the oracle has a significant advantage in this setting: at any given time t the oracle chooses the arm
with the highest pay-off in hindsight. However, for a moderate number of arms k
2
c
3/4
is significantly
smaller than exp(−∆2/4) unless ∆ is considerably large. The advantage of the switching algorithm
vanishes if k is large compared to the smoothness of the process, because eventually the exploration
phase (Phase I) will dominate and the smoothness of the arms cannot be exploited by this algorithm.
This example demonstrates that large dependence in the stochastic process can be exploited to build
switching algorithms that have a significant edge over algorithms that aim to select a single arm and
algorithms like Algorithm 1 are outperformed by simple switching algorithms.
5. Outlook
This paper is an initial attempt to characterize special sub-classes of the restless bandit problem
where good approximate solutions can be found using simple, computationally tractable approaches.
We provide a UCB-type algorithm to approximate the optimal strategy in the case where the pay-offs
15
GRÜNEWÄLDER AND KHALEGHI
are jointly stationary ϕ-mixing and are only weakly dependent. A natural open problem here is
the derivation of a lower-bound. Moreover, while our algorithm only requires knowledge of the
sum of the ϕ-mixing coefficients kϕk =
P
i ϕi as opposed to that of each individual ϕi
, the online
estimation of the mixing coefficients can prove useful. Specifically, in light of Proposition 9, if ϕ1 is
estimated from data, the algorithm can have a real-time estimate of its maximum loss with respect
to the best switching strategy after n rounds of play. Further, the results can be strengthened if the
algorithm can adaptively estimate kϕk instead of relying on it as input. Another interesting regime
corresponds to strongly dependent pay-off distributions. We provide an example using stationary
Gaussian Processes where a simple switching strategy can leverage the dependencies to outperform a
best arm policy. An open problem would be to weaken the assumptions on the process distributions
and obtain results analogous to the weakly dependent case for the strongly dependent framework.

Appendix A. Proofs for ϕ-mixing Processes
A.1. Technical Results
The following key lemma allows us to control ϕ-mixing coefficients corresponding to disjoint events.
Lemma A.1 Let (Ω, A, P) be a probability space and let B, C be two σ-subalgebras of A. If there
exists a ϕ ≥ 0 such that for all B ∈ B and C ∈ C it holds that |P(B)P(C) − P(B ∩ C)| ≤ ϕP(C)
then for any disjoint sequence hBnin∈N, Bn ∈ B for all n ∈ N, and any C ∈ C, we have
X∞
n=0
|P(Bn)P(C) − P(Bn ∩ C)| ≤ 2ϕP(C).
Proof Let cn = P(Bn)P(C) − P(Bn ∩ C), I+ = {n ∈ N : cn ≥ 0}, I− = {n ∈ N : cn < 0}.
Now, since S
n∈I+
Bn ∈ B and S
n∈I−
Bn ∈ B we have
P(C)ϕ ≥ P
 [
n∈I+
Bn

P(C) − P
 [
n∈I+
Bn

∩ C

=
X
n∈I+
(P(Bn)P(C) − P(Bn ∩ C)) = X
n∈I+
|P(Bn)P(C) − P(Bn ∩ C)| .
Similarly, P(C)ϕ ≥
P
n∈I−
|P(Bn)P(C) − P(Bn ∩ C)|. Hence
2P(C)ϕ ≥
X
n∈I+
|P(Bn)P(C) − P(Bn ∩ C)| +
X
n∈I−
|P(Bn)P(C) − P(Bn ∩ C)|
=
X
n∈N
|P(Bn)P(C) − P(Bn ∩ C)| .
16
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
The following technical lemma is used in the proof of Proposition 9.
Lemma A.2 Given some probability space (Ω, A, P), three σ-subalgebras B, C, D ⊂ A and ϕ > 0
such that |P(U)P(V ) − P(U ∩ V )| ≤ ϕP(V ) for all U ∈ B, V ∈ C and P(A) = 0 for all A ∈ D
then it holds that |P(U)P(V ) − P(U ∩ V )| ≤ ϕP(V ) for all U ∈ B, V ∈ σ(C ∪ D).
Proof (i) The set
E := {A : A ∈ A, there exists a B ∈ C such that for all C ∈ A, P(A ∩ C) = P(B ∩ C)}
is a Dynkin system (Fremlin, 2010)[136A]. To see this, note that (1) ∅ ∈ E since ∅ ∈ C, (2) for A ∈ E
take any B ∈ C and observe that P(C ∩(Ω\A)) = P(C\(C ∩A)) = P(C)−P(C ∩A) = P(C)−
P(C ∩ B) = P(C ∩(Ω\B)) for every C ∈ A and Ω\B ∈ C, (3) let hAnin∈N be a disjoint sequence
in E with corresponding elements Bn ∈ C then P(
S
n∈N An) = P∞
n=0 P(An) = P∞
n=0 P(Bn).
Let B0
0 = B0 and iteratively let B0
n = Bn\B0
n−1
then hB0
n
in∈N is a disjoint sequence such that
P(Bn) = P(B0
n
): for any m, n ∈ N we have P(Bn ∩ Bm) = P(An ∩ Bm) = P(An ∩ Am) = 0
and, hence, P(Bn\B0
n−1
) ≥ P(Bn) −
Pn−1
i=0 P(Bn ∩ Bi) = P(Bn). Therefore, P∞
P n=0 P(Bn) =
∞
n=0 P(B0
n
) = P(
S
n∈N B0
n
) = P(
S
n∈N Bn) and E is a Dynkin system.
(ii) Let D0
:= {A ∩ B : A ∈ C, B ∈ D}. Observe that C ∪ D ∪ D0
is closed under intersection:
if A, B ∈ C then A ∩ B ∈ C and similarly for A, B ∈ D. If A ∈ C and B ∈ D then A ∩ B is in an
element of D0
. If A ∈ C and B is an element of D0
then B = C ∩ D for some C ∈ C , D ∈ D and
A ∩ B = (A ∩ C) ∩ D which is again an element of D0
(and equivalently for A ∈ D). Furthermore,
C ∪ D ∪ {A ∩ B : A ∈ C, B ∈ D} ⊂ E. Note that C is a subset of E, and if B ∈ D then we obtain
P(∅ ∩ C) = 0 = P(B ∩ C) for any C ∈ A and ∅ ∈ C. Finally, if we have A ∩ B where A ∈ C
and B ∈ D then P(A ∩ B ∩ C) = 0 = P(∅ ∩ C) for any C ∈ A. Hence, σ(C ∪ D) ⊂ E by the
monotone class theorem (Fremlin, 2010)[136B].
(iii) Now, let us consider a U ∈ B and V ∈ σ(C ∪ D). Due to steps (i) and (ii) we can select
a V
0 ∈ C such that P(V ∩ C) = P(V
0 ∩ C) for all C ∈ A. Then with this V
0 we have that
|P(U)P(V ) − P(U ∩ V )| = |P(U)P(V
0
) − P(U ∩ V
0
)| ≤ ϕP(V
0
) = ϕP(V ).
Another important result in this context concerns the ϕ-mixing property of a process that starts at a
random time. This result is needed to be able to use Hoeffding bounds for batches of observations
which occur in our algorithm. To state this result concisely we first define the following two families
of events
U := n
{τn < ∞} ∩ (Xτn,i, . . . , Xτn+u−1,i)
−1
[B] : B ∈ B(u)
X
o
,
V := n
{τn < ∞} ∩ (Xτn+u+l−1,i, . . . , Xτn+u+l+v−2,i)
−1
[B] : B ∈ B(u)
X
o
,
where the notation (Xτn,i, . . . , Xτn+u−1,i)
−1
[B] denotes the set
{ω : ω ∈ Ω,(Xτn(ω),i(ω), . . . , Xτn(ω)+u−1,i(ω)) ∈ B}
and X is the space in which the random variables Xt,i attain values in.
Lemma 14 In the jointly-stationary ϕ-mixing bandit problem formulated in Section 3, consider an
arm i ≤ k and an increasing sequence of starting times hτnin∈N+
of batches in which arm i is played
which are almost surely finite. The following holds for all n ∈ N+ and 1 ≤ l ≤ 2
n−1 − 1
sup{|P(V ) − P(U ∩ V )/P(U)| :U ∈ U, V ∈ V, P(U) > 0,
u, v ∈ N+, u + v + l − 1 ≤ 2
n−1
} ≤ 2ϕl
. (15)
17
GRÜNEWÄLDER AND KHALEGHI
Proof For any U, V as defined in (15) there exist sequences hUtit∈N+
, and hVtit∈N+
, where
Ut ∈ σ

{(Xt,i, . . . , Xt+u−1,i)
−1
[B] ∩ {τn < ∞} : B ∈ B(u)
X
}

and
Vt ∈ σ

{(Xt+u+l−1,i, . . . , Xt+u+l+v−2,i)
−1
[B] ∩ {τn < ∞} : B ∈ B(v)
X
}

such that U =
S
t∈N+
{τn = t} ∩ Ut and V =
S
t∈N+
{τn = t} ∩ Vt
. Observe also that due to
stationarity P(Vt) = P(V1) for all t ≥ 1. Since, Ut ∩ {τn = t} ∈ Gt
the ϕ-mixing property gives us
|P({τn = t} ∩ Ut ∩ Vt) − P({τn = t} ∩ Ut)P(Vt)| ≤ ϕlP({τn = t} ∩ Ut)
and since τn < ∞ almost surely for each n we have
|P(V ) − P(V1)| ≤ X∞
t=1
|P({τn = t} ∩ Vt) − P(τn = t)P(Vt)| ≤ ϕl
.
Combining these gives the result as
|P(U)P(V ) − P(U ∩ V )|
= |P(U)P(V ) −
X∞
t=1
P({τn = t} ∩ Ut ∩ Vt)|
≤ |P(U)P(V ) −
X∞
t=1
P({τn = t} ∩ Ut)P(Vt)| +
X∞
t=1
ϕlP({τn = t} ∩ Ut)
≤ 2ϕlP(U) + |P(U)P(V1) −
X∞
t=1
P({τn = t} ∩ Ut)P(V1)| = 2ϕlP(U).
A.2. Proof of Proposition 5
Let (Ω, A, P) be some probability space with k mutually independent processes defined on it. If
each of these processes is ψ-mixing then the joint process is also ψ-mixing and for all i ∈ N,
1 + ψ˜
i ≤ (1 + ψi)
k
, where the ψ˜
i are the mixing coefficients of the joint process and the ψi are upper
bounds on the mixing coefficients of the individual processes.
Proof Fix some n, a, u, v > 0, a ≥ u+n and let us denote the individual processes with hXn,iin∈N+
,
i ≤ k. Furthermore, let A = {1, . . . , u}, B = {a, . . . , a+v−1} and G = σ(XA,1, . . . , XA,k), H =
σ(XB,1, . . . , XB,k). The proof structure is the following: in step (i) we show for “simple” events that
the ψ-mixing property carries over to the joint process. In step (ii), we construct a new probability
space in which to each of these “simple” events a product of events is associated. The approach is
useful since more complex events can be easily approximated by products. In step (iii) we make
use of this approximation and we relate arbitrary events to unions of products for each of which the
ψ-mixing property derived in (i) applies.
18
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
(i) Consider a set U ∈ G of the form U =
T
i≤k Ui where Ui ∈ σ(XA,i) for all i ≤ k and a set
V ∈ H of the form V =
T
i≤k
Vi
, Vi ∈ σ(XB,i), for all i ≤ k. The mutual independence of the
processes implies that
|P(U)P(V ) − P(U ∩ V )| =



Y
k
i=1
P(Ui)P(Vi) −
Y
k
i=1
P(Ui ∩ Vi)



≤ P(U1)P(V1)



Y
k
i=2
P(Ui)P(Vi) −
Y
k
i=2
P(Ui ∩ Vi)



+ |P(U1)P(V1) − P(U1 ∩ V1)|
Y
k
i=2
P(Ui ∩ Vi)
≤
Y
2
i=1
P(Ui)P(Vi)
 


Y
k
i=3
P(Ui)P(Vi) −
Y
k
i=3
P(Ui ∩ Vi)



+ P(U1)P(V1)|P(U2)P(V2) − P(U2 ∩ V2)|
Y
k
i=3
P(Ui ∩ Vi)
+ ψn P(U1)P(V1)
Y
k
i=2
P(Ui ∩ Vi)
≤ ψn
X
k
i=1
Y
i
j=1
P(Uj )P(Vj )
 Y
k
j=i+1
P(Uj ∩ Vj )

≤ ψn P(U)P(V )
X
k
i=1
Y
k
j=i+1
P(Uj ∩ Vj )
P(Uj )P(Vj )
≤ ψn P(U)P(V )
X
k−1
i=0
(1 + ψn)
k−i−1 = ((1 + ψn)
k − 1) P(U)P(V ).
(ii) Next, we demonstrate the ψ-mixing property for U ∈ G and V ∈ H. We use a product
measure approach. To make use of this let the index set C = A ∪ B and consider the independent
σ-algebras σ(XC,1), . . . , σ(XC,k). Let Pi be the restriction of P to σ(XC,i) for all i ≤ k and
define the product space (Ωk
, ⊗bi≤kσ(XC,i), µ), where µ is the product measure of P1, . . . , Pk and
⊗bi≤kσ(XC,i) denotes the product σ-algebra of σ(XC,1), . . . , σ(XC,k). The map φ : Ω → Ω
k
,
φ(ω)(i) = ω for all i ≤ k, is inverse-measure preserving due to Fremlin (2010)[272J,254Xc].
In particular, P(
T
i≤k Ui) = P φ−1
[U1 × . . . × Uk] = µ(U1 × . . . × Uk) for Ui ∈ σ(XA,i). The
important property is the following: if U ∈ G then there exists an F ∈ ⊗bi≤kσ(XA,i) ⊆ ⊗bi≤kσ(XC,i)
such that U = φ
−1
[F]. To see this consider the set
S := {φ
−1
[F] : F ∈ ⊗bi≤kσ(XA,i)}.
A standard argument shows that S is a σ-algebra, i.e. ∅ ∈ S, if U ∈ S then Ω\U = φ
−1
[Ωk\U] ∈
S and if hUnin∈N a sequence in S then S
n∈N Un = φ
−1
[
S
n∈N Fn] ∈ S for suitable sets Fn.
Furthermore, S ⊇ {T
i≤k Ui
: Ui ∈ σ(XA,i), i ≤ k} and the latter set is closed under intersection.
19
GRÜNEWÄLDER AND KHALEGHI
Hence, the monotone class theorem tells us that G = σ{
T
i≤k Ui
: Ui ∈ σ(XA,i), i ≤ k} ⊂ S and
the result follows. Similarly, we can link V ∈ G to ⊗bi≤kσ(XB,i).
(iii) We’d like to demonstrate that the ψ-mixing property carries over to arbitrary elements
U ∈ ⊗bi≤kσ(XA,i) and V ∈ ⊗bi≤kσ(XB,i). First, observe that if U = U1 × . . . × Uk and V =
V1 × . . . × Vk, Ui ∈ σ(XA,i), Vi ∈ σ(XB,i), i ≤ k, then using (U1 × . . . × Uk) ∩ (V1 × . . . × Vk) =
(U1 ∩ V1) × . . . × (Uk ∩ Vk) with the inverse-measure preserving property of φ and (i) it follows that
|µ(U1 × . . . × Uk)µ(V1 × . . . × Vk) − µ(U1 × . . . × Uk ∩ V1 × . . . × Vk)|
=

P
T
i≤k Ui)P
T
i≤k
Vi

− P
T
i≤k Ui ∩
T
i≤k
Vi


≤ ((1 + ψn)
k − 1) P
T
i≤k Ui

P
T
i≤k
Vi

= ((1 + ψn)
k − 1) µ(U1 × . . . × Uk)µ(V1 × . . . × Vk).
The advantage of the product approach is that we can approximate the sets U and V with cylinders
of the form U1 × . . . × Uk and this allows us to carry the ψ-mixing property to G and H. As
follows from Fremlin (2010)[Thm. 251Ie, pp. 200, 251W] for every  ∈ (0, 1] there exist sequences
U1,1, . . . , Um1,1, . . . , U1,k, . . . , Um1,k and V1,1, . . . , Vm2,1, . . . , V1,k, . . . , Vm2,k, with Ui,j , Vi
0
,j ∈
σ(XC,j ) for all i ≤ m1, i0 ≤ m2, j ≤ k, with the following four properties.
µ
 [
i≤m1
Ui,1 × . . . × Ui,k
≥
X
i≤m1
µ(Ui,1 × . . . × Ui,k) − , (16)
µ
 [
i≤m2
Vi,1 × . . . × Vi,k
≥
X
i≤m2
µ(Vi,1 × . . . × Vi,k) − /m1, (17)
µ

U4
[
i≤m1
Ui,1 × . . . × Ui,k
≤ , and (18)
µ

V 4
[
i≤m2
Vi,1 × . . . × Vi,k
≤ /m1, (19)
where 4 denotes the symmetric difference between sets.
From (16) and (17) we obtain P
i≤m1
µ(Ui,1 × . . . × Ui,k) ≤ 1 +  and P
i≤m2
µ(Vi,1 × . . . ×
Vi,k) ≤ 1 + /m1 respectively. Moreover, we have

µ(U) − µ
 [
i≤m1
Ui,1 × . . . × Ui,k
 ≤ µ

U4
[
i≤m1
Ui,1 × . . . × Ui,k
≤  (20)
where the second inequality follows from (18). Similarly, from (19) we obtain

µ(V ) − µ
 [
i≤m2
Vi,1 × . . . × Vi,k
 ≤ /m1. (21)
Furthermore, to obtain a similar result for U ∩ V ,below, we rely on the following elementary set
manipulation. Consider sets A1, A2, A3, A4 then
(A1 ∩ A2)4(A3 ∩ A4) = ((A1 ∩ A2)\(A3 ∩ A4)) ∪ ((A3 ∩ A4)\(A1 ∩ A2))
= ((A1 ∩ A2)\A3) ∪ ((A1 ∩ A2)\A4) ∪ ((A3 ∩ A4)\A1) ∪ ((A3 ∩ A4)\A2)
⊆ (A1\A3) ∪ (A2\A4) ∪ (A3\A1) ∪ (A4\A2)
= (A14A3) ∪ (A24A4).         
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
We have,

µ(U ∩ V ) − µ
 [
i≤m1
Ui,1 × . . . × Ui,k
∩
 [
i≤m2
Vi,1 × . . . × Vi,k

≤

µ

(U ∩ V )4
 [
i≤m1
Ui,1 × . . . × Ui,k
∩
 [
i≤m2
Vi,1 × . . . × Vi,k

≤ µ

U4
 [
i≤m1
Ui,1 × . . . × Ui,k + µ

V 4
 [
i≤m2
Vi,1 × . . . × Vi,k (23)
≤  + /m1 (24)
≤ 2 (25)
where, (23) is due to (22) and (24) follows from (18) and (19). Now, applying (20), (21) and (25) we
obtain,
|µ(U)µ(V ) − µ(U ∩ V )|
≤ |µ(U)µ(V ) − µ
 [
i≤m1
[
j≤m2
(Ui,1 × . . . × Ui,k) ∩ (Vj,1 × . . . × Vj,k)

| + 2
≤ 4 +

µ
 [
i≤m1
Ui,1 × . . . × Ui,k
µ
 [
i≤m2
Vi,1 × . . . × Vi,k
− µ
 [
i≤m1
[
j≤m2
(Ui,1 × . . . × Ui,k) ∩ (Vj,1 × . . . × Vj,k)


. (26)
Furthermore,

 X
i≤m1
X
j≤m2
µ(Ui,1 × . . . × Ui,k)µ(Vj,1 × . . . × Vj,k)
− µ
 [
i≤m1
Ui,1 × . . . × Ui,k
µ
 [
i≤m2
Vi,1 × . . . × Vi,k

≤

µ
 [
i≤m1
Ui,1 × . . . × Ui,k
+ 
 X
j≤m2
µ(Vj,1 × . . . × Vj,k)
− µ
 [
i≤m1
Ui,1 × . . . × Ui,k
µ
 [
i≤m2
Vi,1 × . . . × Vi,k
≤ 2 + 
2
≤ 3. (27)
Let U1, . . . , Um be such that µ(
S
i≤m Ui) ≥
P
i≤m µ(Ui) − . Then P
i≤m µ
S
j<i Uj ∩ Ui

≤ .
This is because
µ
 [
i≤m
Ui

= µ
 [
i≤m
Ui\
[
j<i
Uj ∩ Ui

=
X
i≤m
µ

Ui\
[
j<i
Uj ∩ Ui

=
X
i≤m
µ(Ui) −
X
i≤m
µ                           
GRÜNEWÄLDER AND KHALEGHI
Now, for any measurable set V we have by the same argument that
µ
 [
i≤m
Ui ∩ V

= µ
 [
i≤m
Ui ∩ V \
[
j<i
Uj ∩ Ui

=
X
i≤m
µ

Ui ∩ V \
[
j<i
Uj ∩ Ui

=
X
i≤m
µ(Ui ∩ V ) −
X
i≤m
µ
[
j<i
Uj ∩ Ui

≥
X
i≤m
µ(Ui ∩ V ) − .
Hence, using (16) and (17), we can obtain

 X
i≤m1
X
j≤m2
µ

(Ui,1 × . . . × Ui,k) ∩ (Vj,1 × . . . × Vj,k)

− µ
 [
i≤m1
[
j≤m2
(Ui,1 × . . . × Ui,k) ∩ (Vj,1 × . . . × Vj,k)


≤
X
i≤m1
X
j≤m2
µ

(Ui,1 × . . . × Ui,k) ∩ (Vj,1 × . . . × Vj,k)

−
X
i≤m1
µ

(Ui,1 × . . . × Ui,k) ∩
 [
j≤m2
(Vj,1 × . . . × Vj,k)

+ 
≤  +
X
i≤m1
/m1
= 2. (28)
We obtain an bound on |µ(U)µ(V ) − µ(U ∩V )| by using (26), (27) and (28) together with the result
from Step (i), i.e.,
|µ(U)µ(V ) − µ(U ∩ V )|
≤ 9 +
X
i≤m1
X
j≤m2
|µ(Ui,1 × . . . × Ui,k)µ(Vj,1 × . . . × Vj,k)
− µ

(Ui,1 × . . . × Ui,k) ∩ (Vj,1 × . . . × Vj,k)

|
≤ 9 + ((1 + ψn)
k − 1) X
i≤m1
µ(Ui,1 × . . . × Ui,k)
X
j≤m2
µ(Vj,1 × . . . × Vj,k) (29)
≤ ((1 + ψn)
k − 1) µ(U)µ(V ) + 9 + 5((1 + ψn)
k − 1),
where the last inequality follows from the same arguments as used in inequalities (26) and (27).
Since  is arbitrary we derive the upper bound for arbitrary elements U ∈ ⊗bi≤kσ(XA,i) and
V ∈ ⊗bi≤kσ(XB,i).
Note that this last step is the only part of the proof where ψ-mixing is necessary. In particular,
we could not derive (29) from the line preceding it by only assuming ϕ            
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
To conclude, if U ∈ G, V ∈ H then we know from step (ii) that there are elements E ∈
⊗bi≤kσ(XA,i), F ∈ ⊗bi≤kσ(XB,i) such that
|P(U ∩ V ) − P(U)P(V )|
= |P φ−1
[E ∩ F] − P φ−1
[E]P φ−1
[F]|
= |µ(E ∩ F) − µ(E)µ(F)|
≤ ((1 + ψn)
k − 1)P(U)P(V ).
(iv) The joint process is ψ-mixing since limn→∞(1 − ψn)
k − 1 = 0.
A.3. Proof of Proposition 8
In what follows, we denote the probability space by (Ω, A, P) and let hEtit≥1 be the filtration
Et = σ{σ(X1, . . . , Xt) ∪ N } where N is the family of sets of measure zero. The random times are
τi
: Ω → N+ and they are A − P(N+) measurable, where P(N+) denotes the power set of N+ =
{1, 2 . . .}. Furthermore, let the random variables Xi attain values in X ⊂ R and assume that they
are A − BX measurable, where BX denotes the Borel σ-algebra on X . We define the random times
τi
inductively together with a filtration that tracks the observed information. Let H0 = {∅, Ω}, let τ1
be a H0-measurable random time and let H1 = σ(Xτ1
). Then, for given τ1, . . . , τi and H1, . . . , Hi
let τi+1 be some Hi-measurable random variable such that τi+1 > τi almost surely and define
Hi+1 = σ(Xτ1
, . . . , Xτi+1 ). We can observe that σ(Xτ1
, . . . , Xτi
) = σ(Xτ1
, . . . , Xτt
, τ1, . . . , τt).
To see this, note that τ1 is σ(Xτ1
, . . . , Xτt
)-measurable, hence σ(Xτ1
, . . . , Xτt
, τ1, . . . , τt) =
σ(Xτ1
, . . . , Xτt
, τ2, . . . , τt). Moreover, since τ2 is σ(Xτ1
, τ1) = σ(Xτ1
)-measurable, we have
that σ(Xτ1
, . . . , Xτt
, τ2, . . . , τt) = σ(Xτ1
, . . . , Xτt
, τ3, . . . , τt), and the observation can simply be
verified by induction. Recall the statement of Proposition 8 as follows.
Assume that hXtit∈N+
is a stationary ϕ-mixing process with mixing coefficients hϕiii∈N+
such
that EXt = µ, t ∈ N+, and supt∈N+
|Xt
| ≤ c for some c ∈ [0, ∞). Furthermore, let τ1, τ2, . . . be a
sequence of random times such that τi + ` ≤ τi+1 a.s. for some ` ≥ 1 and all i ∈ N+, and all τi+1
are σ(Xτ1
, . . . , Xτi
)- measurable with τ1 ∈ N+ being a fixed time. Then for any n ∈ N+



1
n
Xn
i=1
EXτi − µ


 ≤ 2cϕ`
.
Proof The following technical result is important in this context.
Lemma A.3 For all i, t ≥ 1 and A ∈ Hi−1 we have that A ∩ {τi = t} ∈ Et
. In particular
{τi = t} ∈ Et
.
Proof For i = 1 the result holds trivially since A ∩ {τi = t} is either Ω or ∅ and since Et
is a σ-algebra it contains Ω and ∅. For any other i ≥ 2 observe that if the claim holds for all
1 ≤ j ≤ i − 1 then since τi
is Hi−1-measurable and because τi > τi−1 almost surely implies that
U := {τi = t} ∩ {τi−1 ≥ t} ∈ N ⊂ Et and we have
{τi = t} = U ∪
t
[−1
s=1
{τi = t} ∩ {τi−1 = s} ∈ Et
.
23
GRÜNEWÄLDER AND KHALEGHI
We can also observe that {B ∩ {τi = t} : B ∈ σ(Xτi
)} ⊂ Et because
{B ∩ {τi = t} : B ∈ σ(Xτi
)} = {X
−1
t
[B
0
] ∩ {τi = t} : B
0 ∈ BX }
and both X
−1
t
[B0
] and {τi = t} lie in Et
.
Also, for any j ∈ {1, . . . , i − 1} we have {B ∩ {τi = t} : B ∈ σ(Xτj
)} ⊂ Et
: let U = {τi =
t} ∩ {τj ≥ t + 1 − (i − j)} then U ∈ N and
{B ∩ {τi = t} : B ∈ σ(Xτj
)} = U ∪
t−[
(i−j)
s=1
{X−1
s
[B
0
] ∩ {τi = t} ∩ {τj = s} : B
0 ∈ BX }
and X−1
s
[B0
] ∩ {τj = s} ∈ Es ⊂ Et
. We have shown that
n
{τi = t} ∩
i
\−1
j=1
Bj : Bj ∈ σ(Xτj
)
o
⊂ Et
.
This implies directly that
σ
n
{τi = t} ∩
i
\−1
j=1
Bj : Bj ∈ σ(Xτj
)
o
⊂ Et
.
By induction one can verify that {A ∩ {τi = t} : A ∈ σ(Xτ1
, . . . , Xτi−1
)} is included in the left
side.
We also need the following observation: Let s, t ∈ N+, s < t then for any B ∈ Es we have
|
R
B
(E(Xt
|Es) − E(Xt))| ≤ 2ϕt−scP(B). This follows from Lemma A.2 by remembering that
Es = σ(σ(X1, . . . , Xs)∪N ), i.e. the ϕ-mixing property implies this upper bound for σ(X1, . . . , Xs)
instead of Es and Lemma A.2 allows us to extend this property to σ(σ(X1, . . . , Xs) ∪ N ) by using
in Lemma A.2 the following σ-algebras: B = σ(Xt), C = σ(X1, . . . , Xs) and D = N .
Now, coming to the main proof consider first τ1. Note that τ1 is independent of any Xi since for
U ∈ σ(Xi), V ∈ σ(τ1) = {∅, Ω} we have either V = Ω and P(U ∩ V ) = P(U) = P(U)P(V ) or
V = ∅ and P(U ∩ V ) = 0 = P(U)P(V ). Independence and stationarity of the process hXtit∈N+
give us for any t ∈ N+ that
E (Xt × χ{τ1 = t}) = P(τ1 = t)E(Xt) = P(τ1 = t)E(X1).
Now, since the Xt are bounded and τ1 attains a value in N+ we know that
X∞
t=1
E (|Xt
| × χ{τ1 = t}) = X∞
t=1
P(τ1 = t)E |X1|
is finite and B. Levi’s Theorem (Fremlin, 2010)[123A] tells us that P∞
t=1 |Xt
| × χ{τ1 = t} is
integrable. Also |
Pt
0
t=1 Xt × χ{τ1 = t}| is upper bounded by the integrable function P∞
t=1 |Xt
| ×
χ{τ1 = t} for all t
0 ∈ N+ and Lebesgue’s Dominated Convergence Theorem (Fremlin, 2010)[123C]
gives us
E(Xτ1
) = E
X∞
t=1
Xt × χ{τ1 = t}

=
X∞
t=1
E(Xt × χ{τ1 = t}) = X∞
t=1
P(τ1 = t)E(Xt) = E(X1).
2 
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
We perform induction over τi
, i ≥ 2. The set {τi = t} is an element of Hi−1 and for any s < t,
s, t ∈ N+ we have that B = {τi = t} ∩ {τi−1 = s} ∈ Hi−1. Observe that Lemma A.3 tells us that
B ∈ Es and, hence, that
Z
B
E(Xτi
|Hi−1) = Z
B
Xt =
Z
B
E(Xt
|Es).
Due to stationarity this implies that


Z
B
(E(Xτi
|Hi−1) − E(Xt))

 =


Z
B
(E(Xt
|Es) − E(X1))

 ≤ 2cϕt−sP(B). (30)
Because τi
is Hi−1 measurable we can now write
E(Xτi
) = EE(Xτi
|Hi−1)
= EE(
X∞
t=1
Xt × χ{τi = t}|Hi−1) (31)
=
X∞
t=1
EE(Xt × χ{τi = t}|Hi−1) (32)
=
X∞
t=1
E(E(Xt
|Hi−1) × χ{τi = t}) (33)
=
X∞
t=1
E(E(Xt
|Hi−1) × χ

{τi = t} ∩
t
[−`
s=1
{τi−1 = s}

)
=
X∞
t=1
X
t−`
s=1
E(E(Xt
|Hi−1) × χ

{τi = t} ∩ {τi−1 = s}

, (34)
where, with the same argument as above, using B. Levi’s Theorem and Lebesgue’s Dominated
Convergence Theorem for the expectation operator and for the conditional expectation operator, the
infinite summation in (31) is moved outside to give (32). Moreover, (33) is due to the fact that τi
is
Hi−1-measurable. Finally, we obtain,
|E(Xτi
) − E(X1)| (35)
=





X∞
t=1
X
t−`
s=1
(E(E(Xt
|Hi−1) × χ ({τi = t} ∩ {τi−1 = s})) − P(τi = t, τi−1 = s)E(X1))





(36)
≤
X∞
t=1
X
t−`
s=1

(E(E(Xt
|Hi−1) × χ ({τi = t} ∩ {τi−1 = s})) − P(τi = t, τi−1 = s)E(X1))


≤ 2c
X∞
t=1
X
t−`
s=1
ϕt−sP(τi = t, τi−1 = s) (37)
≤ 2cϕ`
X∞
t=1
X
t−`
s=1
P(τi = t, τi−1 = s) (38)
= 2cϕ`
,
25
GRÜNEWÄLDER AND KHALEGHI
where, (36) follows from (34), (37) follows from (30) with B := {τi = t} ∩ {τi−1 = s} ∈ Hi−1,
and (38) is due to the fact that hϕnin∈N+
is a decreasing sequence, see, e.g. (Bradley, 2007)[vol. 1
pp. 69], and that t − s ≥ `. This proves the statement.
A.4. Details of Example 1
We give in this section the details of the construction in Example 1. This example demonstrates that
the sequence hXτn
in∈N+
of samples of one of the arms of a two armed bandit problem does not need
to be ϕ-mixing even though the original process is ϕ-mixing. The argument uses standard results
from Markov chains as they can be found in (Levin et al., 2008) and a well known perturbation result
for Markov chains. We provide the details of the argument for completeness.
Assume we have a two arm bandit problem where the pay-off for arm two is zero at all time
and the pay-off distribution of arm one is described by a Markov chain with two states, transition
probabilities p11 = p22 = 1 − , p12 = p21 = , for some  ∈ (0, 1), and probability 1/2 to be
in state 1 at time t = 0. The player gains a pay-off of 1 if the Markov chain is in state 1 and
a pay-off of 0 if the Markov chain is in state 2. The Markov chain is irreducible and aperiodic.
Furthermore, the Markov chain induces a stationary pay-off distribution. This implies that we are
dealing with a jointly stationary ϕ-mixing process with mixing coefficients ϕk being upper bounded
by ϕk ≤ (1 − 2)
k
: one can derive the particular bound by considering an eigendecomposition
of the transition matrix
√
T which yields eigenvalues λ1 = 1, λ2 = 1 − 2 and eigenvectors u1 =
2(1/2 1/2)>, u2 =
√
2(1/2 − 1/2)>, i.e. with U = (u1 u2) and Λ being the diagonal
matrix with entries λ1 and λ2 we have T = UΛU
>. The stationary distribution over the states
is s = (1/2 1/2)> and for any vector v = (v1 v2)
>, v1, v2 ≥ 0, v1 + v2 = 1, we have
Λ
kU
>v = (1/
√
2)(1 (1 − 2)
k
(v1 − v2))>. Hence, T
kv − s = (1/2)(1 − 2)
k
(v1 − v2)(1 −
1)> and

T
kv − s


∞
≤ (1/2)(1 − 2)
k
. The mixing coefficients can now be bounded in the
following way. Let Xt,i, t ≥ 1, i ∈ {1, 2}, be random variables that represent the pay-off of
arm i gained at time t. Consider a particular realization where X1,1 = x1, . . . , Xn,1 = xn and
Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+k+m for some x1, . . . , xn, xn+k+1, . . . , xn+k+m ∈ {0, 1}
then P(X1,1 = x1, . . . , Xn,1 = xn, Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+k+m) = P(X1,1 =
x1, . . . , Xn,1 = xn)P(Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+m+k|Xn,1 = xn) and
|P(Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+m+k|Xn,1 = xn)
− P(Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+m+k)|
= P(Xn+k+1,1 = xn+k+1, . . . , Xn+k+m,1 = xn+m+k|Xn+k,1 = xn+k)
× |P(Xn+k,1 = xn+k) − P(Xn+k,1 = xn+k|Xn,1 = xn)|
≤ (1/2)(1 − 2)
kP(Xn+k+1,1 = xn+k+1, . . . , Xn+k+m,1 = xn+m+k|Xn+k,1 = xn+k)
= (1 − 2)
kP(Xn+k,1 = xn+k, . . . , Xn+k+m,1 = xn+m+k). (39)
since P(Xn+k,1 = xn+k) = 1/2. Let A = {1, . . . , n}, B = {n + k, . . . , n + k + m} and
consider σ(XA), σ(XB). The events in these σ-algebras are finite unions of events of the form
{X1,1 = x1, . . . , Xn,1 = xn} and {Xn+k,1 = xnk
, . . . , Xn+k+m,1 = xn+k+m}.
For any U ∈ σ(XA) we know that U consists at most of finitely many such events U1, . . . , Ul
,
Ui ∩Uj = ∅, for all i, j ≤ l. Similarly for V ∈ σ(XB) we know that V = V1 ∪. . .∪Vo, Vi ∩Vj = ∅,
26
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
for all i, j ≤ o. The above argument which leads to the bound (39) allows us to conclude that for any
U ∈ σ(XA) and V ∈ σ(XB)
|P(U)P(V ) − P(U ∩ V )| =
X
i≤l
X
j≤o
|P(Ui)P(Vj ) − P(Ui ∩ Vj )|
≤ (1 − 2)
kX
i≤l
X
j≤o
P(Ui)P(Vj ) ≤ (1 − 2)
k
.
Hence the Markov chain is ϕ-mixing. The second arm has a constant reward and does not introduce
any dependencies in the pay-off over time. Therefore we have a jointly stationary ϕ-mixing process
with the mixing coefficient being equal to the mixing coefficients of the Markov chain.
Now fix some δ > 0 and consider the following policy π
δ
. At t = 1 the policy plays arm 1
receiving pay-off X1,1. Then at any other t ≥ 2 the arm is selected according to the following rules:
if at t − 1 arm 1 has been played and Xt−1,1 = X1,1 then the policy chooses at t arm 1; if at t − 1
arm 1 has been played and Xt−1,1 6= X1,1 then the policy chooses at t arm 2 and plays arm 2 for the
next k := dlog(2δ)/ log(1 − 2)e rounds before switching back to arm 1. We’d like to show that the
sequence of pay-offs generated by policy π
δ
at arm 1 is not ϕ-mixing. Let τ1, τ2, . . . be the sequence
of random times at which arm 1 is played (by construction τ1 = 1 and τ2 = 2). The evolution of the
process hXτn
in≥1 can also be described by transition matrices. The transition probabilities to move
from a state at t = 1 to a state at t = 2 are just the probabilities summarized in T. For all other t
(t ≥ 2) the transition matrix is either
T˜ =

1 −  
(T
k
)21 (T
k
)22
or T˜ =

(T
k
)11 (T
k
)12
 1 − 

(40)
depending on Xτ1
, i.e. if Xτ1 = 1 then the former is describing the evolution and if Xτ1 = 0
the latter is the one describing the evolution of the Markov chain. In the following we discuss
the case that Xτ1 = 1, but the same arguments apply to the case Xτ1 = 0. We can observe that
kv
>(T˜ − Tˆ)k∞ ≤ δ for all v with non-negative entries and v1 + v2 = 1, where
Tˆ =

1 −  
1/2 1/2

in case that Xτ1 = 1. The claim can be verified through



v
>T˜ − v
>Tˆ



∞
=




v
>

0 0
T˜
21 − 1/2 T˜
22 − 1/2




∞
= v2




T
k

0
1

−

1/2
1/2




∞
≤ v2δ ≤ δ.
The Markov chains associated to T˜ and Tˆ are both irreducible and aperiodic. This implies in
particular the existence of stationary distributions, with the associated probabilities to be in state one
and two summarized in vectors s, ˜ sˆ ∈ [0, 1]2
, and the convergence of T˜l
, Tˆl
to s, ˜ sˆ in l (measured in
kk∞, (Levin et al., 2008)[Thm. 4.9]). In particular, there exist constants c, ˜ c >ˆ 0 and α, ˜ αˆ ∈ (0, 1)
such that for all l ≥ 1 and any vector v ∈ [0, 1]2 with v1 + v2 = 1
kv
>T˜l − s˜k∞ ≤ c˜α˜
l
and kv
>Tˆl − sˆk∞ ≤ cˆαˆ
l
.
27
GRÜNEWÄLDER AND KHALEGHI
We can also calculate the stationary distribution of Tˆ explicitly to get sˆ = (1/(1+ 2) 2/(1+ 2))>.
It is known that Markov chains with slightly perturbed transition matrices have similar stationary
distributions. Due to Cho and Meyer (2001) there exists a constant c > 0 that is only dependent
on Tˆ (and independent of T˜) such that ks˜ − sˆk∞ ≤ ckT˜ − Tˆk∞,1 ≤ 2δc where kT˜ − Tˆk∞,1 :=
maxi∈{1,2}
P2
j=1 |T˜
i,j −Tˆ
i,j | ≤ 2δ. Combining these inequalities yields kv
>T˜l −sˆk∞ ≤ 2δc+ ˜cα˜
l
for any v with non-negative entries and v1 + v2 = 1.
Consider now the events U = {Xτ1,1 = 1} and V = {Xτn,1 = 1} for n ≥ 2. We have
P(U) = 1/2 = P(V ) where the second equality follows from
2P(Xτn,1 = 0) = P(Xτn,1 = 0|Xτ1,1 = 0) + P(Xτn,1 = 0|Xτ1,1 = 1)
= P(Xτn,1 = 1|Xτ1,1 = 1) + P(Xτn,1 = 1|Xτ1,1 = 0)
= 2P(Xτn,1 = 1).
Furthermore, with T˜ being the matrix defined on the left side of (40)



P(U ∩ V )
P(U)
−
1
1 + 2


 =


(1 −  )T˜n−1

1
0

−
1
1 + 2



≤


(1 −  )T˜n−1 −
1
1 + 2

1
2



∞
and the last term is upper bounded by 2δc + ˜cα˜
n−1
. Recalling that c does not depend on T˜ and,
hence, not on δ we see that we can make the term 2δc arbitrary small. Furthermore, by considering a
large n we can make the second term arbitrary small. In particular, let  = 1/10 then there exists a
π
δ
and an N ∈ N such that for all n ≥ N



P(U ∩ V )
P(U)
−
1
1 + 2


 ≤ 1/10. (41)
Hence, for all n ≥ N
|P(U)P(V ) − P(U ∩ V )| ≥ P(U)



1
2
−
1
1 + 2


 − 1/10
≥ P(U)/5
and the process is not ϕ-mixing.
Appendix B. Proof of Theorem 12: Regret Bound for ϕ-mixing Bandits
For the regret R(n) of Algorithm 1 after n rounds of play. We have,
R(n) ≤
X
k
i=1
µi6=µ
∗
32(1 + 8 kϕk) ln n
∆i
+ (1 + 2π
2
/3)(X
k
i=1
∆i) + kϕk log n
Proof Thanks to Proposition 11, in order to bound the regret R(n) it suffices to calculate the
expected number of times Ti(n) that a suboptimal arm is played in n rounds. For any s, t ∈ N+,
let ct,s := p
(8ζ((1/8) + ln t))/2
s + kϕk /2
s−1
, where ζ = 1 + 8 kϕk. Recall that Algorithm 1
28
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
plays its selected arms in batches of exponentially growing length so that if arm j for j ∈ 1..k is
selected at round t, it is played for 2
sj (t)
consecutive time-steps, where sj (t) denotes the number of
times that arm j has been selected up-to time t. Below, we denote by Xj,s := 1
2
s
Pt+2s−1
t
0=t Xt
0
,j with
s := sj (t), the Algorithm’s estimate of the stationary mean of arm j selected at time t. As usual,
a superscript “∗” refers to the quantities for the arm with the highest stationary mean. Fix some
l ∈ N+. We have,
Ti(n) = 1 + Xn
t=k+1
χ{πt = i}
≤ 2
l+1 +
Xn
t=2l+1+1
X
log t
m=l+1
2
mχ{τm,i = t}
≤ 2
l+1 +
Xn
t=2l+1+1
X
log t
m=l+1
2
mχ
n
min
s=1.. log t
X
∗
s + ct,s ≤
1
2m−1
t+2
Xm−1−1
u=t
Xu,i + ct,m−1
o
≤ 2
l+1 +
X∞
t=1
X
log t
m=l+1
X
log t
s=1
2
mχ
n
X
∗
s + ct,s ≤
1
2m−1
t+2
Xm−1−1
u=t
Xu,i + ct,m−1
o
For every t ∈ N and every s ∈ 1.. log t we have that X
∗
s + ct,s ≤
1
2m−1
Pt+2m−1−1
u=t Xu,i + ct,m−1
implies that
X
∗
s ≤ µ
∗ − ct,s (42)
1
2m−1
t+2
Xm−1−1
u=t
Xu,i ≥ µi + ct,m−1 (43)
µ
∗ < µi + 2ct,m−1 (44)
Now, observe that for a fixed t ∈ 1..n we have,
P(X
∗
s ≤ µ
∗ − ct,s) ≤ P



X
∗
s − µ
∗


 ≥ ct,s
≤ P








2
Xs−1
j=0
X∗
τs+j − EX∗
τs+j






+






2
Xs−1
j=0
(EX∗
τs+j − µ
∗
)






≥ 2
s
ct,s


≤ P








2
Xs−1
j=0
X∗
τs+j − EX∗
τs+j






≥ 2
s
ct,s − 2 kϕk

 (45)
≤
√
e exp
−
(2s
ct,s − 2 kϕk)
2
2
s+1ζ

(46)
≤ t
−4
, (47)
where, (45) follows from Lemma 10 and (46) follows from a Hoeffding-type bound for ϕ-mixing
processes given by Corollary 2.1 of Rio (1999) which is applicable due to Lemma 14 (pp. 17).
29
GRÜNEWÄLDER AND KHALEGHI
Moreover, noting that kϕk ≥ 0 we similarly obtain,
P
 1
2m−1
t+2
Xm−1−1
u=t
Xu,i ≥ µi + ct,m−1

≤ P
 1
2m−1
t+2
Xm−1−1
u=t
Xu,i ≥ µi + ct,m−1 −
2
2m−1
kϕk

≤ P

|
t+2
Xm−1−1
u=t
Xu,i − 2
m−1µi
| ≥ 2
m−1
ct,m−1 − 2 kϕk

≤ t
−4
. (48)
Let L := log 32(1+4kϕk) ln n
∆2
i
. Since, for t ≥ 2
L+1 and every m ≥ L+1 we have µ
∗−µi−2ct,m−1 ≥
0, it follows that (44) is false for all m ≥ L + 1. Therefore we have,
E(Ti(n)) ≤ 2
L+1 +
X∞
t=1
X
log t
m=L+1
X
log t
s=1
P(X
∗
s ≤ µ
∗ − ct,s,
1
2m−1
t+2
Xm−1−1
u=t
Xu,i ≥ µi + ct,m−1)
≤ 2
L+1 +
X∞
t=1
X
log t
m=L+1
X
log t
s=1
2
m+1t
−4
(49)
≤
32(1 + 4 kϕk) ln n
∆2
i
+ 1 + 2π
2
/3
where (49) follows from (47) and (48).
Appendix C. Proofs for Strongly Dependent Reward Distributions
C.1. Basic Bounds
Consider two independent and normally distributed random variables X, Y with mean µX > µY and
variance σ
2
X, σ2
Y
. Let ∆ = µX −µY > 0 and σ
2 = σ
2
X +σ
2
Y
. We derive in this section the following
bounds which are crucial for the derivation of the regret. We use the notation z
+ = max{0, z} and
φ(x) = (2π)
−1/2
exp(−x
2/2) for the density function of the standard normal distribution.
E(Y − X)
+ ≤ σφ(∆/σ), (50)
E(Y − X)
+ ≥ 0, (51)
E(X − Y )
+ ≤ σφ(∆/σ) + ∆, (52)
E(X − Y )
+ ≥ ∆. (53)
The derivation is based on basic properties of Gaussian random variables. Recall that Z = X − Y is
normally distributed with mean ∆ and variance σ
2
. Therefore,
√
2πσE(X − Y )
+ =
Z ∞
0
z exp 
−
(z − ∆)2
2σ
2

=
Z ∞
−∆
z exp 
−
z
2
2σ
2

+ ∆ Z ∞
−∆
exp 
−
z
2
2σ
2

= σ
2
exp 
−
∆2
2σ
2

+ ∆σ
Z ∆/σ
−∞
exp 
−
z
2
2

30
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
Standard bounds on the cdf, as can be found in (Dudley, 2002)[Lem. 12.1.6], lead to (53), i.e.
√
2πσE(X − Y )
+ = σ
2
exp 
−
∆2
2σ
2

+ ∆σ
√
2π

1 −
1
√
2π
Z ∞
∆/σ
exp 
−
z
2
2
!
≥ σ
2
exp 
−
∆2
2σ
2

+ ∆σ
√
2π

1 −
σ
√
2π∆
exp 
−
∆2
2σ
2

= ∆σ
√
2π.
Similarly, if we consider Z = Y − X which has mean −∆
√
2πσE(Y − X)
+ =
Z ∞
0
z exp 
−
(z + ∆)2
2σ
2

= σ
2
exp 
−
∆2
2σ
2

− ∆σ
Z ∞
∆/σ
exp 
−
z
2
2

.
Applying the result from Dudley (2002)[Lem. 12.1.6] leads here to the trivial lower bound 0 and,
hence, inequality (51). We use the following inequalities to obtain upper bounds on E(Y − X)
+ and
E(X − Y )
+. Let Z be a standard normal random variable. Then
Pr(Z ≥ c) ≥
(
φ(c)/(2c) if c ≥ 1,
φ(c)(1 − c)/2 if 0 ≤ c < 1.
The first bound can be found in (Dudley, 2014). The second bound is a straightforward adaptation
of the techniques used to derive the first bound. Applying these bounds we get the following upper
bounds on E(Y − X)
+. If ∆/σ ≥ 1 then
√
2πσE(Y − X)
+ ≤ σ
2
exp 
−
∆2
2σ
2

− ∆σ
σ
2∆ exp 
−
∆2
2σ
2

=
σ
2
2
exp 
−
∆2
2σ
2

and if 0 ≤ ∆/σ < 1
√
2πσE(Y − X)
+ ≤ σ
2
exp 
−
∆2
2σ
2

− ∆σ
(1 − ∆/σ)
2
exp 
−
∆2
2σ
2

= (σ
2 − ∆σ/2 + ∆2
/2) exp 
−
∆2
2σ
2

≤ σ
2
exp 
−
∆2
2σ
2

and the inequality (52) follows. The upper bounds on E(X − Y )
+ are derived in the same way. For
∆/σ ≥ 1 these are
√
2πσE(X − Y )
+ ≤ σ
2
exp 
−
∆2
2σ
2

− (σ
2
/2) exp 
−
∆2
2σ
2

+ ∆σ
√
2π
=
σ
2
2
exp 
−
∆2
2σ
2

+ ∆σ
√
2π
31
GRÜNEWÄLDER AND KHALEGHI
and for 0 ≤ ∆/σ ≤ 1
√
2πσE(X − Y )
+ ≤ σ
2
exp 
−
∆2
2σ
2

−
∆σ(1 − ∆/σ)
2
exp 
−
∆2
2σ
2

+ ∆σ
√
2π
= (σ
2 − ∆σ/2 + ∆2
/2) exp 
−
∆2
2σ
2

+ ∆σ
√
2π
and the inequality (50) follows.
C.2. Proof of Proposition 13
We bound the regret of the two phases individually. Some technical steps are moved further below to
streamline the discussion.
Regret of Phase I. We sweep through all k arms at times 1 to k, m + 1 to m + k, etc. During each
of these phases we build up regret. This regret can be bounded by
X
k
i=1
E(max
j6=i
Xm+i,j − Xm+i,i)
+
≤
X
k
i=1
X
j6=i
E(Xm+i,j − Xm+i,i)
+
≤ (k − 1)X
k
i=1
max
j6=i
E(Xm+i,j − Xm+i,i)
+
≤ (k − 1)X
k
i=1
∆i +
√
2(k − 1)X
k
i=1
φ(∆i/
√
2)
≤ (k − 1)X
k
i=1
(∆i +
√
2), (54)
where ∆i = µ
∗ − µi
is the difference between the highest stationary mean of the k arms and the
stationary mean of arm i. Here, we use Inequality (52) and the assumption that the variance of the
individual processes is 1. Therefore, the σ appearing in the bound (52) is √
2 since σ
2
is the sum of
the variances of the two individual processes.
Regret of Phase II. To control the regret building up in the second phase we condition on the
observations in a sweep at time lm, l ∈ N, i.e. on the observed pay-offs x1, . . . , xk. Arm i
∗
is
selected such that xi
∗ ≥ maxi≤k xi and this arm is played for m − k steps. We need to control
E(maxi≤k(Xt
0
,i − Xt
0
,πt
0
)
+) for all lm + k + 1 ≤ t
0 ≤ (l + 1)m. Due to stationarity this is equal
to controlling E(maxi≤k(Xt,i − Xt,πt
)
+), for all k + 1 ≤ t ≤ m.
We use in the following Px1,...,xk
, Pxi,xu
for the conditional distributions given that X11 =
x1, . . . , Xkk = xk and Xii = xi
, Xuu = xu, and we use ν for the marginal measure. In Appendix
32
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
C.2.1 below we show that for any k + 1 ≤ t ≤ m it holds that
E(max
i≤k
(Xt,i − Xt,πt
)
+)
≤
X
k
u=1
X
i6=u
Z
Px1,...,xk
(u = i
∗
) ×
Z
(Xt,i − Xt,u)
+dPxi,xu dν(x1, . . . , xk). (55)
The inner integral in (55) can be bounded by using inequality (50)
Z
(Xt,i − Xt,u)
+dPxi,xu ≤ σt,iφ

∆t,i
σt,i 
, (56)
where ∆t,i = E(Xt,u − Xt,i|Xi,i = xi
, Xu,u = xu) which equals E(Xt,u|Xu,u = xu) −
E(Xt,i|Xi,i = xi) due to the independence between arm u and i. These are posterior means
of Gaussian processes given observations xu and xi
, respectively. Observe that the posterior mean
for Xt,u is related to the posterior mean of the zero mean Gaussian process Xt,u − µu through
E(Xt,u|Xu,u = xu) = µu + E(Xt,u − µu|Xu,u − µu = xu − µu). The posterior equation for the
zero mean Gaussian process Xt,u − µu given observation xu − µu at time u is cov(t − u)(xu − µu)
since cov(0) = 1. Therefore, E(Xt,u|Xu,u = xu) = µu + cov(t − u)(xu − µu) and, hence,
E(Xt,u|Xu,u = xu) = (1 − cov(t − u))µu + cov(t − u)xu. Similarly, E(Xt,i|Xi,i = xi) =
(1 − cov(t − i))µi + cov(t − i)xi By defining ∆˜
i = xu − xi and by using the Hölder-continuity
assumption, we obtain the following lower bound.
∆t,i = (µu − µi)(1 − cov(t − k)) + ∆˜
icov(t − k) + ε(t)
≥ ∆˜
i − |µu − µi
||cov(0) − cov(t − k)| − ∆˜
i
|cov(0) − cov(t − k)| − |ε(t)|
≥ ∆˜
i − c(t − k)
α
(∆ + ∆˜
i) − |ε(t)| (57)
with ε(t) being a term which can be bounded by |ε(t)| ≤ (∆ + ∆˜
i)ckα (see Appendix C.2.2). For
the conditional variance,
σ
2
t,i = E(X2
t,u|Xu,u = xu) − E(Xt,u|Xu,u = xu)
2 + E(X2
t,i|Xi,i = xi) − E(Xt,i|Xi,i = xi)
2
,
we obtain the following upper-bound
σ
2
t,i = 2 − cov2
(t − u) − cov2
(t − i) ≤ 2c(t − u)
α + 2c(t − i)
α ≤ 4ctα
, (58)
where we have used the posterior equation for the covariance, that the covariance is by assumption
non-negative, and that, under the Hölder-continuity assumption, for a non-negative covariance
cov(l) ≥ max{0,(1 − clα)} and, hence,
cov2
(l) ≥ cov(l) max{0, 1 − clα
}
=
(
0 if clα ≥ 1,
cov(l)(1 − clα) otherwise,
≥
(
0 if clα ≥ 1,
1 − 2clα + c
2
l
2α otherwise,
≥ 1 − 2clα
.
33
GRÜNEWÄLDER AND KHALEGHI
Combining (56), (57) and (58) we obtain
Z
(Xt,i − Xt,u)
+dPxi,xu
≤ (2/π)
1/2
c
1/2
t
α/2
exp
−
(∆˜
i − c((t − k)
α + k
α)(∆˜
i + ∆))2
8ctα
!
= (2/π)
1/2
c
1/2
t
α/2
exp
−
∆˜ 2
i
8ctα

1 − c((t − k)
α + k
α
)

1 +
∆
∆˜
i
2
!
. (59)
The bound is maximized for t = m. Substituting this bound back into (55) and after a few
manipulations (see Appendix C.2.3) we see that
Xm
t=k+1
E

max
i≤k
(Xt,i − Xt,πt
)
+

≤ (m − k)
amc
1/2k(k − 1)
8π(1 − bm)

2π
1/2 − (1 − ∆
p
bm/4) exp 
−
∆2
bm
4
 (60)
if m > k and ∆ <
√
am/(bm
√
2), where am = 8cmα and bm = c((m − k)
α + k
α).
Combined Regret. Combining (54) with (60) and using ∆i ≤ ∆ we can observe that the combined
regret for any m steps is bounded by
k(k − 1)
∆ + √
2 + (m − k)
amc
1/2
8π(1 − bm)

2π
1/2 − (1 − ∆
p
bm/4) exp 
−
∆2
bm
4
!
if m > k and ∆ <
√
am/(bm
√
2). Given a time horizon n we have dn/me many iterations of Phase
I and II and the overall regret is bounded by
(n/m + 1)k(k − 1)
∆ + √
2 +
amc
1/2
(m − k)
8π(1 − bm)

2π
1/2 − (1 − ∆
p
bm/4) exp 
−
∆2
bm
4
!
≤ (n + m)k(k − 1)
∆ + √
2
m
+
amc
1/2
8π(1 − bm)

2π
1/2 − (1 − ∆
p
bm/4) exp 
−
∆2
bm
4
!
.
(61)
This is the regret bound stated in the proposition text.
Instead of trying to find the m that minimizes (61) we optimize m over the considerably simpler
expression
∆ + √
2
m
+
2c
3/2mα
√
π

=
∆ + √
2
m
+
amc
1/2
4
√
π
!
. (62)
The (1 − bm) term that we leave out is of minor relevance since bm is small and the negative term in
the bracket that we leave out is not larger than 1. By ignoring this latter term we lose only another
constant. Minimizing (62) with respect to m and rounding up yields
m? =




 √
π(∆ + √
2)
2αc3/2
! 1
1+α




.
34
APPROXIMATIONS OF THE RESTLESS BANDIT PROBLEM
C.2.1. PROOF OF INEQUALITY (55)
Due to stationarity and since our policy depends only on the observations at the last sweep we have
that E(maxi≤k(Xt
0
,i − Xt
0
,πt
0
)
+) = E(maxi≤k(Xt,i − Xt,πt
)
+) for any t
0
, lm + k + 1 ≤ t
0 ≤ lm,
l ∈ N, and corresponding t = t
0 − lm. Now, consider any t, k + 1 ≤ t ≤ m and, using i
∗
for the
choice of arm given the observations X1,1, . . . , Xk,k, rewrite the regret in the following way
E

max
i≤k
(Xt,i − Xt,πt
)
+

=
X
k
u=1
E

max
i≤k
χ{u = i
∗
} × (Xt,i − Xt,i∗ )
+

=
X
k
u=1
E

max
i6=u
χ{u = i
∗
} × (Xt,i − Xt,u)
+

≤
X
k
u=1
X
i6=u
E

χ{u = i
∗
} × (Xt,i − Xt,u)
+

=
X
k
u=1
X
i6=u
Z Z χ{u = i
∗
} × (Xt,i − Xt,u)
+dPx1,...,xk
dν(x1, . . . , xk)
=
X
k
u=1
X
i6=u
Z
Px1,...,xk
(u = i
∗
) ×
Z
(Xt,i − Xt,u)
+dPx1,...,xk
dν(x1, . . . , xk) (63)
=
X
k
u=1
X
i6=u
Z
Px1,...,xk
(u = i
∗
) ×
Z
(Xt,i − Xt,u)
+dPxi,xu dν(x1, . . . , xk) (64)
where (63) follows because χ{u = i
∗} is independent of (Xt,i−Xt,u)
+ given X11 = x1, . . . , Xkk =
xk and (64) follows since (Xt,i − Xt,u)
+ only depends on Xi,i and Xu,u.
C.2.2. BOUND ON ε(t).
The term ε(t) that we introduced is equal to
ε(t) =µu(cov(t − k) − cov(t − u)) − µi(cov(t − k) − cov(t − i))
+ xu(cov(t − u) − cov(t − k)) − xi(cov(t − i) − cov(t − k)).
Since, due to our Hölder assumption, |cov(t − k) − cov(t − u)| ≤ c(k − u)
α ≤ ckα and |cov(t −
k) − cov(t − i)| ≤ ckα, we have the following bound.
|ε(t)| ≤ (∆ + ∆˜
i)ckα
.
C.2.3. PROOF OF INEQUALITY (60)
Denote in the following Xu := Xu,u and Xi = Xi,i and recall that ∆˜
i = Xu − Xi
is normally
distributed with mean µu − µi and variance 2. Writing
f(Xu, Xi) = (2/π)
1/2
c
1/2mα/2
exp
−
∆˜ 2
i
8cmα

1 − c((m − k)
α + k
α
)

1 +
∆
∆˜
i
2
!
35
GRÜNEWÄLDER AND KHALEGHI
we have
Z
Px1,...,xk
(u = i
∗
) ×
Z
(Xt,i − Xt,u)
+dPxi,xu dν(x1, . . . , xk)
≤
Z
Px1,...,xk
(Xu ≥ Xi) ×
Z
(Xt,i − Xt,u)
+dPxi,xu dν(x1, . . . , xk)
≤
Z
χ{xu ≥ xi} × f(xu, xi) dν(x1, . . . , xk) (65)
=
Z
χ{xu − xi ≥ 0} × f(xu, xi) dν(xu, xi), (66)
where the inequality in (65) follows because {Xu ≥ Xi} is only dependent on xu and xi and by
bounding the inner integral with (59). Multiplying the density of ∆˜
i by f and using a = 8cmα,
b = c((m − k)
α + k
α), d = (ac)
1/2/(4π), in the case where ∆ <
√
a/(b
√
2), we obtain
Z
χ{xu − xi ≥ 0} × f(xu, xi) dν(xu, xi)
= d
Z ∞
0
exp
−
(∆˜
i(1 − b) − ∆b)
2
a
−
(∆˜
i − (µu − µi))2
4
!
d∆˜
i
≤ d
Z ∞
0
exp
−
(∆˜
i(1 − b) − ∆b)
2
a
!
d∆˜
i
=
d
1 − b
Z ∞
−∆b
exp
−
∆˜ 2
i
a
!
d∆˜
i
=
√
ad
(1 − b)
√
2
Z
√
√2∆b
a
−∞
exp
−
∆˜ 2
i
2
!
d∆˜
i
=
√
ad√
2π
(1 − b)
√
2

1 −
1
√
2π
Z ∞
√
√2∆b
a
exp
−
∆˜ 2
i
2
!
d∆˜
i
!
≤
√
πad
(1 − b)

1 −
1 −
√
2∆b/√
a
2
√
2π
exp 
−
∆2
b
2
a
!
=
ac1/2
4(1 − b)
√
π

1 −
1 −
√
2∆b/√
a
2
√
2π
exp 
−
∆2
b
2
a
!
≤
ac1/2
8π(1 − b)

2π
1/2 − (1 − ∆b/√
a) exp 
−
∆2
b
2
a

and for m > k, since then a/4 ≥ b, this can be further bounded by
ac1/2
8π(1 − b)

2π
1/2 − (1 − ∆
p
b/4) exp 
−
∆2
b
4
 .