This paper introduces the first strategy for stochastic bandits with unit variance Gaussian noise that
is simultaneously minimax optimal up to constant factors, asymptotically optimal, and never worse
than the classical upper confidence bound strategy up to universal constant factors. Preliminary
empirical evidence is also promising. Besides this, a conjecture on the optimal form of the regret is
shown to be false and a finite-time lower bound on the regret of any strategy is presented that very
nearly matches the finite-time upper bound of the newly proposed strategy.
Keywords Stochastic bandits, sequential decision making, regret minimisation.
1. Introduction
Let k > 1 be the number of bandits (or arms) and µ ∈ R
k be the unknown vector of mean
payoffs so that µi ∈ R is the expected payoff when playing the ith bandit (or arm). In each
round t ∈ [n] = {1, 2, . . . , n} the player chooses an arm At ∈ [k] based on past observations and
(optionally) an independent source of randomness. After making her choice, the player observes a
payoff Xt = µAt + ηt where η1, η2, . . . , ηn is a sequence of independent standard Gaussian random
variables. It is standard to minimise the expected pseudo-regret (from now on, just the regret). Let
∆i(µ) = maxj µj − µi be the suboptimality gap for the ith arm. The regret over n rounds is
Rn(µ) = E
"Xn
t=1
∆At
(µ)
#
.
Because the regret depends on the unknown payoff vector, no strategy can hope to make the regret
small for all µ simultaneously. There are a number of performance metrics in the literature, two of
which are described below along with a new one. To spoil the surprise, the strategy introduced in the
present article is simultaneously optimal with respect to all of them.
Worst-case optimality The worst-case regret of a strategy is the value of the regret it suffers when
faced with the worst possible µ.
RWC
n = sup
µ∈Rk:∆(µ)∈[0,1]k
Rn(µ).
The restriction to bounded suboptimality gaps is necessary to allow an algorithm to choose each arm at
least once without suffering arbitrarily large regret. Generally problems with small suboptimality gaps
are the most interesting. Provided that n ≥ k it is known that all algorithms suffer RWC
n = Ω(√
kn)
(Auer et al., 1995).
Asymptotic optimality The worst-case regret obscures interesting structure in the problem that
becomes relevant in practice. This motivates the study of a problem-dependent metric, which demands
that strategies have smaller regret on ‘easier’ bandit instances. A strategy is called asymptotically
optimal if
limn→∞
Rn(µ)
log(n)
=
X
i:∆i(µ)>0
2
∆i(µ)
for all µ ∈ R
d
.
The name is justified by the existence of policies satisfying the definition and lower bounds by Lai
and Robbins (1985) and Burnetas and Katehakis (1996) showing that consistent policies (those with
sub-polynomial regret on all µ) cannot do better.
The sub-UCB criteria While asymptotic analysis is quite insightful, the ultimate quantity of interest
is the finite-time regret. To make a stab at quantifying this I say an algorithm is sub-UCB if there
exist universal constants C1, C2 > 0 such that for all k, n and µ it holds that
Rn(µ) ≤ C1
X
k
i=1
∆i(µ) + C2
X
i:∆i(µ)>0
log(n)
∆i(µ)
. (1)
Of course UCB (Auer et al., 2002) satisfies Eq. (1), along with many other policies as shown in
Table 2 in Appendix E, which outlines the long history of algorithms for stochastic finite-armed
bandits. The study of this new metric can be justified in several ways. First, it provides a forgiving
finite-time analogue of asymptotic optimality. Lai and Robbins (1985) derived asymptotic optimality
by making a restriction on policies (the consistent ones). Consistency is an asymptotic notion, so it is
not surprising that the resulting lower bound is also asymptotic. The sub-UCB notion is suggested by
making a finite-time restriction on the worst case regret. Precisely, for any strategy the finite-time
instance-dependent regret can be bounded in terms of the worst case regret by
Rn(µ) ≥ sup
ε∈(0,1]
X
i:∆i(µ)>0
max



0,
2 log 
n
RWC
n

+ 2 log 
ε∆i(µ)
8

(1 + ε)∆i(µ)



(2)
for all µ with maxi ∆i(µ) ≤ 1/2 (Lattimore and Szepesvari, 2018). This means that if you demand a ´
reasonable worst case bound, then the instance-dependent regret cannot be much better than sub-UCB.
Note that the first sum in Eq. (1) is unavoidable for policies that always choose each arm at least
once, which is also necessary for any algorithm to have reasonable worst case regret. The finite-time
world is not as clean as the asymptotic and it is not easy to decide how tight Eq. (2) might be, which
justifies the additional constant-factor allowance in Eq. (1) and the removal of the (typically negative)
second logarithm term. The second justification for using Eq. (1) as a yardstick is that it is forgiving
and yet recent policies that are minimax optimal up to constant factors do not satisfy it. One of the
core contributions of this article is to correct these deficiencies. Note that Eq. (2) depends quite
weakly on the worst case regret and is meaningful as long as RWC
n = O(n
p
) for p not too close to 1.
None of these criteria are perfect by themselves. Asymptotic optimality is achievable by
policies with outrageous burn-in time and/or large minimax regret, minimax optimal policies may be
unreasonably conservative on easy problems and sub-UCB policies may be far from asymptotically
optimal.
2
REFINING THE CONFIDENCE LEVEL FOR BANDIT STRATEGIES
Contributions The main contribution is a new strategy called ADA-UCB (‘adaptive UCB’) and
analysis showing it is asymptotically optimal, minimax optimal and sub-UCB. No other algorithm is
simultaneously minimax optimal and sub-UCB (see Table 2). Results are specialised to the Gaussian
case with unit variance, but upper bounds can be generalised to subgaussian noise with known
subgaussian constant at the price of increased constants (without losing asymptotic optimality) and
longer proofs. The latter justifies the specialisation because it allows for an elegant concentration
analysis via an embedding of Gaussian random walks into Brownian motion. Also included:
(a) Finite-time lower bounds showing the new strategy is close to optimal.
(b) A conjecture by Bubeck and Cesa-Bianchi (2012) is proven false.
(c) A generic analysis for a large class of strategies simplifying the analysis for existing strategies.
Beyond the concrete results, the approach used for deriving ADA-UCB by examining lower bounds
will likely generalise to other noise models, and indeed, other sequential optimisation problems
with an exploration/exploitation flavour. The contents of this article combines the best parts of two
technical reports with improved results, intuition and analysis (Lattimore, 2015a, 2016b).
Notation For natural number n let [n] = {1, 2, . . . , n}. Binary minimums and maximums are
abbreviated by ∧ and ∨ respectively. The complement of event A is Ac
. Except where otherwise
stated, it is assumed without loss of generality that µ1 ≥ µ2 ≥ . . . ≥ µk. None of the proposed
strategies depend on the labelling of the arms, so if this is not the case the indices can simply be
re-ordered. The dependence of the suboptimality gap on the mean vector will usually be omitted
when the context is clear: ∆i = ∆i(µ) = maxj µj − µi
. Occasionally it is convenient to define
µk+1 = −∞ and ∆k+1 = ∞. Let Ti(t) = Pt
s=1 1{As=i} be the number of times arm i has
been chosen after round t and µˆi(t) = Pt
s=1 1{As=i}Xs/Ti(t) be the corresponding empirical
estimate of its return. Let µˆi,s be the empirical estimate of the mean of arm i after s samples
from that arm so that µˆi,Ti(t) = ˆµi(t). Define σ-algebra Ft = σ(ξ, X1, . . . , Xt) to contain the
information available to the strategy after round t, where ξ is an independent source of randomness
that allows for randomness in the strategy. This means that formally a strategy is a sequence of
random variables (At)t such that At
is Ft−1-measurable. It is assumed throughout that n ≥ k.
Finally, let log(x) = log((x + e) log1/2
(x + e)). A table of notation is available in Table 3 in the
appendix.
2. The strategy
The ADA-UCB strategy chooses each arm once in arbitrary order for the first k rounds and
subsequently At = arg maxi∈[k] γi(t) where the index of arm i in round t is:
γi(t) = ˆµi(t − 1) + s
2
Ti(t − 1) log 
n
Hi(t − 1)
,
with Hi(t) = Ti(t)Ki(t) and Ki(t) = X
k
j=1
min (
1,
s
Tj (t)
Ti(t)
)
.
At first sight the new index seems overly complicated. After the statement of the main regret
guarantee I show how the strategy is derived in a principled fashion from lower bounds obtained
3
LATTIMORE
from information theoretic limits of the problem. Similar approaches have been used before, for
example, by Agrawal et al. (1989) for reinforcement learning, and by Garivier and Kaufmann (2016)
for pure exploration in bandits. One interesting consequence of this approach is that ADA-UCB is
not a true index strategy in the sense that γi(t) depends on random variables associated with other
arms. An intriguing open question is whether or not there exists an index strategy for which all three
performance criteria are met. A minor observation is that it is not clear whether or not a true index
strategy should be allowed to depend on t or just on n − t and the samples from the given arm.
majority t
Lai (1987) n/Ti
Honda and Takemura (2010) t/Ti
Audibert and Bubeck (2009) n/(kTi)
Degenne and Perchet (2016) t/(kTi)
Lattimore (2015a) n/t
Table 1: Confidence levels
Relation to other algorithms The index is the same
as that used by Katehakis and Robbins (1995) except
that log(t) has been replaced by log(n/Hi(t − 1)).
The change from log(·) to log(·) is quite minor.
Such inflations of the logarithmic term are typical
for algorithms with finite-time guarantees. The main
difference between ADA-UCB and previous work is the
term inside the logarithm, often called the confidence
level. The most common choice is the current round t, which is used by various versions of UCB,
KL-UCB and BAYES-UCB (Katehakis and Robbins, 1995; Burnetas and Katehakis, 1996; Agrawal,
1995; Auer et al., 2002; Kaufmann et al., 2012; Cappe et al., 2013). Already in the early work by Lai ´
(1987) there appeared an unnamed variant of UCB for which the confidence level was n/Ti(t − 1).
Due to its similarity to KL-UCB (Cappe et al., 2013) this algorithm will be called ´ KL-UCB* from now
on. A variety of other choices have been used as shown in Table 1.
Computation A naive implementation of ADA-UCB requires a computation time that is quadratic in
the number of arms in each round. Fortunately an incremental implementation leads to an algorithm
with linear computation time by noting that:
(a) If Ti(t − 1) ≤ TAt
(t − 1), then γi(t + 1) = γi(t).
(b) For arms i with Ti(t−1) > TAt
(t−1) the value of Ki(t−1) may be computed incrementally
by Ki(t) = Ki(t − 1) −
p
(TAt
(t) − 1)/Ti(t) + p
TAt
(t)/Ti(t).
(c) The index of At can be computed trivially in order k time.
The algorithm follows by maintaining a list of arms sorted by Ti(t) and applying the above
observations to incrementally update the indices. If all details are addressed carefully, then the
computation required in round t is O(
Pk
i=1 1 {Ti(t − 1) ≥ TAt
(t − 1)}), which in the worst case is
O(k), but can be much smaller when a single arm is played significantly more often than any other.
Regret bound The theorem statement has a more complicated form than previous regret bounds
for finite-armed bandits, mainly because it correctly deals with the case where there are many
near-optimal arms that cannot be statistically identified within the time horizon. Define ki and λi by
ki =
X
k
j=1
min 
1,
∆i
∆j

and λi = 1 +
1
∆2
i
log 
n∆2
i
ki

.
The main theorem is below, which gives the best known finite-time guarantee for any strategy, as
well as all three optimality criteria defined in the introduction.
4
REFINING THE CONFIDENCE LEVEL FOR BANDIT STRATEGIES
Theorem 1 Assume that µ1 ≥ µ2 ≥ · · · ≥ µk. Then there exists a universal constant C > 0 such
that the regret of ADA-UCB is bounded by
Rn(µ) ≤ C min
i∈[k]

n∆¯
i +
X
m>i
∆mλm
!
, where ∆¯
i =
1
i
X
i
m=1
∆m . (3)
Furthermore:
(a) ADA-UCB is minimax optimal up to constant factors: RWC
n ≤ C
0
√
kn .
(b) ADA-UCB is sub-UCB: Rn(µ) ≤ C
X
m:∆m>0

∆m +
log(n)
∆m

.
(c) ADA-UCB is asymptotically optimal: limn→∞
Rn(µ)
log(n)
=
X
i:∆i>0
2
∆i
.
Remark 2 The assumption on the order of the arms is purely for cosmetic purposes. The algorithm
does not need this ordering and treats all arms symmetrically.
Intuition for bound and strategy Let µ ∈ [0, 1]k
and i be a suboptimal arm so that ∆i = ∆i(µ) >
0. Set µ
0 ∈ [0, 2]k
equal to µ except for µ
0
i = µi + 2∆i
, which means that arm i has the largest mean
for the bandit determined by µ
0
. Provided that n is sufficiently large, a sub-UCB strategy should play
arm i logarithmically often if the mean payoff vector is µ, and linearly often for µ
0
. Let E and E
0
and
P and P
0 denote the measures on the outcomes A1, X1, . . . , An, Xn when the strategy interacts with
the bandits determined by µ and µ
0
respectively. Let δ ∈ (0, 1] be such that
E[Ti(n)] = 2 log(1/δ)
(2∆i)
2
. (4)
Since µ and µ
0
are only different in the ith coordinate, the problem of minimising the regret is
essentially equivalent to a hypothesis test on the mean of the ith arm, which satisfies |µ
0
i − µi
| = 2∆i
.
Using this idea, a standard information-theoretic argument (see the section on lower bounds for formal
details) shows that P
0
(Ti(n) ≤ n/2) & δ. Abbreviate ∆0
i = ∆i(µ
0
). Since ∆0
j = µ
0
i − µ
0
j ≥ ∆i for
all j 6= i it holds that
Rn(µ
0
) ≥
n∆i
2
P
0
(Ti(n) ≤ n/2) & n∆iδ/2 . (5)
Assuming the strategy is sub-UCB, then there exists a (hopefully small) constant C > 0 such that
Rn(µ
0
) ≤ C
X
j:∆0
j>0

∆0
j +
log(n)
∆0
j
!
≈ C
X
j:∆0
j>0
log(n)
∆0
j
, (6)
where the approximation follows because ∆0
j ∈ [0, 2] has been assumed. By Eqs. (5) and (6):
δ .
2C log(n)
n∆i
X
j:∆0
j>0
1
∆0
j
=
2C log(n)
n∆2
i
X
j6=i
∆i
∆j + ∆i
≤
2Cki
log(n)
n∆2
i
.
5
LATTIMORE
The regret guarantee given in Theorem 1 is now justified up to constant factors and an extraneous
additive log log(·) term by substituting the above display into Eq. (4) and writing the regret as
Rn(µ) = Pk
i=1 ∆iE[Ti(n)]. The idea behind ADA-UCB is to use the approximation ∆
−2
i ≈ Ti(t−1).
The approximation is poor when t is small, but becomes reasonable at the critical time when
arm i should no longer be played. Specifically, if Ti(t − 1) ≈ ∆
−2
i
, then we should expect
Tj (t − 1) ≈ min{Ti(t − 1), ∆
−2
j
} ≈ min{∆
−2
i
, ∆
−2
j
}. Then
n
Hi(t − 1) =
n
Pk
j=1 min 
Ti(t − 1),
p
Ti(t − 1)Tj (t − 1)	
≈
n
Pk
j=1 min n
1
∆2
i
,
1
∆i∆j
o =
n∆2
i
ki
.
The implication is that the index dynamically tunes its confidence level using the pull counts to
loosely estimate the gaps. The ideas in this section are made formal in the proof of Theorem 1 or the
lower bound (§5).
3. Asymptotic analysis
The primary purpose of this section is to prove part (c) of Theorem 1. Along the way, a finitetime regret bound for a whole class of strategies is derived, including slightly modified versions
of KL-UCB* (Lai, 1987) and the MOSS (Minimax Optimal in the Stochastic Setting, Audibert and
Bubeck 2009). The analysis leads to an optimal worst-case analysis of MOSS and KL-UCB*, but
not ADA-UCB. The following theorem holds for the class of index strategies that choose At = t for
1 ≤ t ≤ k and subsequently maximise
γi(t) = ˆµi(t − 1) + s
2
Ti(t − 1) log 
n
Ji(t − 1)Ti(t − 1)
, (7)
where Ji(t − 1) is Ft−1-measurable and Ji(t − 1) ∈ [a, b] almost surely for constants 0 < a ≤ b.
Except for minor differences in the leading constant and the logarithmic term, this index is the same
as MOSS if Ji(t − 1) = k, KL-UCB* if Ji(t − 1) = 1 and ADA-UCB if Ji(t − 1) = Ki(t − 1).
Theorem 3 For any ε ∈ (0, 1/2) and 1 ≤ ` ≤ k, the regret of the strategy in Eq. (7) is at most
Rn(µ) ≤ n∆` +
2c1b
ε
2∆`+1
+
X
i>`

2∆i +
1
∆i

1 +
1
ε
2
+
2 log 
n∆2
i
a

(1 − 2ε)
2



 .
Furthermore:
(a) limn→∞ Rn(µ)/ log(n) = P
i:∆i>0
2
∆i
.
(b) If b ≤ k, then RWC
n ≤ C
q
nk
1 + log
k
a
 where C > 0 is a universal constant.
Before the proof a little more notation is required. For each i and ∆ > 0 let ζi(∆) be a random
variable given by
ζi(∆) = 1 + max {s : ˆµi,s > µi + ∆} . (8)  
REFINING THE CONFIDENCE LEVEL FOR BANDIT STRATEGIES
Clearly, ζi(∆) is surely monotone non-increasing in ∆ and may be upper bounded in expectation
using Lemma 13 in the appendix.
Proof [of Theorem 3] Let ∆ ∈ R be the smallest value such that
µˆ1,s +
r
2
s
log  n
bs

≤ µ1 − ∆ for all 1 ≤ s ≤ n ,
which is chosen so that γ1(t) ≥ µ1 − ∆ for all t. By part (a) of Lemma 12 with α = n/b and d = 1
and λ1 = ∞ we have for any x > 0 that
P (∆ ≥ x) ≤
c1bhλ(1/x2
)
n
=
c1b
nx2
. (9)
Define random variable
Λi = 1 + max 
1
∆2
i
, ζi(ε∆i),
2
(1 − 2ε)
2∆2
i
log 
n∆2
i
a
 .
The definitions of the policy, Λi and ∆ ensure that if ∆i > ∆/ε, then Ti(n) ≤ Λi and by Lemma 13,
E[Λi
] ≤ 2 +
1
∆2
i

1 +
1
ε
2
+
2 log 
n∆2
i
a

(1 − 2ε)
2

 . (10)
Hence the regret of the strategy maximising the index in Eq. (7) is
Rn(µ) = E
"X
k
i=1
∆iTi(n)
#
= E
"X
`
i=1
∆iTi(n)
#
+ E
" X
k
i=`+1
1

∆i ≤
∆
ε

∆iTi(n)
#
+ E
" X
k
i=`+1
1

∆i >
∆
ε

∆iTi(n)
#
≤ n∆` + E

n∆
ε
1 {∆ ≥ ε∆`+1}

+
X
i>`
∆iE [Λi
] . (11)
The last expectation in Eq. (11) is bounded using Eq. (10),
X
i>`
∆iE [Λi
] ≤
X
i>`

2∆i +
1
∆i

1 +
1
ε
2
+
2 log 
n∆2
i
a

(1 − 2ε)
2



 . (12)
Given an arbitrary random variable X and constant b ∈ R it holds that
E[X] ≤ bP (X ≥ b) + Z ∞
b
P (X ≥ x) dx .
The first expectation in Eq. (11) is bounded by combining the above display with Eq. (9),
n
ε
E [∆1 {∆ ≥ ε∆`+1}] ≤ n∆`+1P (∆ ≥ ε∆`+1) + n
ε
Z ∞
ε∆`+1
P (∆ ≥ x) dx
≤
c1b
ε
2∆`+1
+
c1b
ε
Z ∞
ε∆`+1
dx
x
2
=
2c1b
ε
2∆`+1
,
7
LATTIMORE
which together with Eq. (12) and Eq. (11) completes the proof of the first part. The asymptotic result
follows by choosing ` = max{i : ∆i = 0} and ε = log−1/4
(n). The equality follows by the lower
bound of Lai and Robbins (1985). The worst-case bound follows by choosing ε = 1/4 and tuning
the cut-off `.
The failure of MOSS Notice that if Ji(t−1) = k, then a = b = k and except for a smaller inflated
logarithm the resulting policy is the same as the variant proposed by Menard and Garivier (2017). ´
The troublesome term in the regret is the second term, which when ` = 1 is approximately k/∆2. By
contrast, for more conservative strategies this term is approximately 1/∆2, which is negligible. An
especially challenging regime is when ∆2 = 1/k and ∆i = 1 for i > 2. Suppose now that n = k
3
and k is large, then the regret of UCB on this problem should be
Rn = O (k log(k)) .
For MOSS, however, the regret on this problem is Ω(√
nk) = Ω(k
2
), which for large k is arbitrarily
worse than UCB. A vague explanation of the poor performance is that although there are k arms,
all but two of them are so suboptimal that effectively it is a two-armed bandit. And yet MOSS is
heavily tuned for the k-armed case and suffers as a consequence. A more precise argument is that
distinguishing the first and second arms requires T2(t) ≈ T1(t) ≈ 1/∆2
2 = k
2
. But after playing
these arms roughly k
2
times each the confidence level is n/(kTi(t)) ≈ 1 and the likelihood of
misidentification is large enough that the regret is Ω(n∆2) = Ω(k
2
) = Ω(√
nk). Note that for
ADA-UCB we would expect Ki(t) ≈ 2 and the confidence level is approximately n/k2 = k, which is
exactly as large as necessary.
Remark 4 An empirical study in this problem was given in a previous technical report (Lattimore,
2015a). In practice the failure does not become extreme until k is very large (approximately 1000).
4. Finite-time analysis
In this section the remainder of Theorem 1 is proven. The argument is quite long, but never terribly
complicated. The main novel challenge is to deal with the dependence of the index of one arm on the
number of plays of other arms. The usual program for analysing strategies based on upper confidence
bounds has two parts:
(a) Show that with high probability the index of the optimal arm is never much smaller than its
mean.
(b) Show that the index of each suboptimal arm drops below the mean of the optimal arm after not
too many plays with high probability.
The proof starts with (b), the main component of which is showing for suboptimal arms i that Hi(t)
grows at a reasonable rate. As discussed, this means showing that other arms are played sufficiently
often. The following definitions spell out which arms will be played reasonably often. For each arm i
8
REFINING THE CONFIDENCE LEVEL FOR BANDIT STRATEGIES
define a deterministic set of arms Vi ⊂ [k] and random subset Wi ⊆ Vi by
Vi = {j ∈ [k] : j is even or j ≥ i} . (13)
Wi = {i} ∪ (
j ∈ Vi
: min
1≤s≤∆
−2
i
µˆj,s +
s
2
s
log 
n∆i
ki
√
s

−
r
2
s
≥ µj
)
. (14)
The set Wi
is a subset of Vi
that includes arm i and those arms for which the empirical mean is
always nearly as large as the true mean. We’ll soon see that arms j ∈ Wi will be played sufficiently
often to ensure that Hi(t) grows at the right rate. The exclusion of odd arms with j < i from Vi
is
for technical reasons. In order to show that arm i is not played too often we need to show that its
index drops sufficiently fast and that the index of some other arm is sufficiently large. The separation
of the arms allows us to exploit the independence between the arms. Those arms not in Vi will be
used to show that some index is large enough. Define
δi = c2
s
ki
n∆2
i
and ` = max {i : δi > 1/4} . (15)
It is shown in Lemma 21 in the appendix that there exists a universal constant C > 0 such that `
satisfies
n∆¯
` +
X
m>`
λm∆m ≤ C min
i∈[k]

n∆¯
i +
X
m>i
∆mλm
!
, (16)
and so the rest of the proof is devoted to bounding the regret of ADA-UCB in terms of the left-hand-side
of Eq. (16). Define Fi
to be the event that the ‘mass’ of Wi
is sufficiently large.
Fi = 1



X
m∈Wi
min {1, ∆i/∆m} ≥ ki/8



. (17)
Recall that ki =
Pk
m=1 min {1, ∆i/∆m}. So Fi holds if the sum over the restricted set Wi
is at most
a factor of 8 smaller. The point is that if j ∈ Vi
, then Lemma 12 implies P (j /∈ Wi) ≤ δi ≤ 1/4.
Later this will be combined with Hoeffding’s bound to show that Fi occurs with high probability.
And now the lemmas begin. First up is to show that arms j ∈ Wi are played sufficiently often
relative to arm i. This will then be used to show that Hi(t) grows at the right rate, which leads to the
conclusion of the proof of part (b) in the outline in Lemma 7.
Lemma 5 If At = i and Hi(t − 1) ≤ ki/∆2
i and Ti(t − 1) ≥ ζi(∆i) ∨ 1/∆2
i
, then for all j ∈ Wi
,
Tj (t − 1) ≥ min



1
2∆2
j
,
Ti(t − 1)
4 log 
n
Hi(t−1)



.
Proof If i = j or Tj (t − 1) ≥ Ti(t − 1) or Tj (t − 1) ≥ 1/(2∆2
j
), then we are done, so assume from
now on that none of these are true.
ki
∆2
i
≥ Hi(t − 1) = q
Tj (t − 1) X
k
m=1
min (
Ti(t − 1)
p
Tj (t − 1)
,
s
Ti(t − 1)Tm(t − 1)
Tj (t − 1) )
≥
p
Tj (t − 1)
∆i
X
k
m=1
min (
1,
s
Tm(t − 1)
Tj (t − 1) )
=
q
Tj (t − 1)Kj (t − 1)
∆i
, (18)
9
LATTIMORE
where the first inequality is assumed in the lemma statement and the second because Ti(t − 1) ≥
Tj (t − 1) ∨ (1/∆2
i
). Therefore if j ∈ Wi
, then
µ1 +
s
2
Ti(t − 1) log 
n
Hi(t − 1)
≥ µˆi(t − 1) + s
2
Ti(t − 1) log 
n
Hi(t − 1)
= γi(t) ≥ γj (t) = ˆµj (t − 1) + s
2
Tj (t − 1) log 
n
Tj (t − 1)Kj (t − 1)
≥ µˆj (t − 1) +
vuut
2
Tj (t − 1) log
n∆i
ki
p
Tj (t − 1)!
≥ µj +
s
2
Tj (t − 1) ≥ µ1 +
s
1
2Tj (t − 1) (19)
where the first inequality follows from the assumption that Ti(t − 1) ≥ ζi(∆i), which ensures that
µ1 = µi + ∆i ≥ µˆi(t − 1). The second follows from Eq. (18). The inequalities in Eq. (19) from the
definition of j ∈ Wi and the assumption that Tj (t − 1) ≥ 1/(2∆2
j
). Therefore
Tj (t − 1) ≥
Ti(t − 1)
4 log 
n
Hi(t−1) . 
The next lemma uses the previous result to show that if Wi
is large enough (Fi holds), then
Hi(t − 1) is reasonably large at the critical point when Ti(t − 1) ≈ 1/∆2
i
.
Lemma 6 If Fi holds and Ti(t − 1) ≥ ζi(∆i) ∨ 128/∆2
i
and At = i, then
log 
n
Hi(t − 1)
≤ 2 log 
n∆2
i
ki

.
Proof If Hi(t − 1) > ki/∆2
i
, then there is nothing more to do. Otherwise, by Lemma 5
Hi(t − 1) = X
k
m=1
min n
Ti(t − 1),
p
Ti(t − 1)Tm(t − 1)o
≥
X
m∈Wi
min n
Ti(t − 1),
p
Ti(t − 1)Tm(t − 1)o
≥
X
m∈Wi
min



Ti(t − 1),
Ti(t − 1)
2 log
1
2

n
Hi(t−1),
p
Ti(t − 1)/2
∆m



(20)
≥
8
P
m∈Wi min n
1
∆2
i
,
1
∆i∆m
o
log
1
2

n
Hi(t−1) ≥
ki
∆2
i
log
1
2

n
Hi(t−1) , (21)
where Eq. (20) follows from Lemma 5. The first inequality in Eq. (21) follows because log(x) ≥ 1
and the assumption Ti(t − 1) ≥ 128/∆2
i
, and the second because Fi holds. The result follows via
10
REFINING THE CONFIDENCE LEVEL FOR BANDIT STRATEGIES
re-arrangement and some algebraic trickery with the log(·) function using Part (v) of Lemma 20 in
the appendix.
For each arm i define random variable Λi
that will be shown to be a high probability bound on
Ti(n) and approximately equal to λi
in expectation.
Λi = 1 + max 
128
∆2
i
, ζi(∆i/3),
36
∆2
i
log 
n∆2
i
ki

,
181{F
c
i
}
∆2
i
log
n∆2
i


,
where ζi(·) is defined in Eq. (8). The next lemma is a simple consequence of the previous two and
shows that if Ti(t − 1) + 1 ≥ Λi
, then either arm i is not played or its index is smaller than the mean
of the optimal arm by a margin of at least ∆i/3.
Lemma 7 If Ti(t − 1) + 1 ≥ Λi
, then either At 6= i or γi(t) ≤ µ1 − ∆i/3.
Proof If Fi does not occur, then Hi(t − 1) ≥ Ti(t − 1) ≥ 1/∆2
i
and so
γi(t) = ˆµi(t − 1) +
vuut
2 log 
n
Hi(t−1)
Ti(t − 1) ≤ µi +
∆i
3
+
s
2 log
n∆2
i

Ti(t − 1) ≤ µ1 −
∆i
3
,
where the first inequality follows because Ti(t − 1) + 1 ≥ Λi ≥ 1 + ζi(∆i/3) and the second
because Hi(t − 1) = Ti(t − 1)Ki(t − 1) ≥ Ti(t − 1) ≥ 1/∆2
i
and the definition of Λi and because
µi + ∆i/3 = µ1 − 2∆i/3. Next suppose that Fi occurs, then either At 6= i and the result is true, or
At = i and so by Lemma 6
γi(t) = ˆµi(t − 1) +
vuut
2 log 
n
Hi(t−1)
Ti(t − 1) ≤ µi +
∆i
3
+
vuut
4 log 
n∆2
i
ki

Ti(t − 1) ≤ µ1 −
∆i
3
. 
This essentially completes the first part of the proof by showing that if Ti(t − 1) + 1 ≥ Λi
, then
arm i is either not played or its index is not too large. The next step is to show that with reasonable
probability there is some near-optimal arm for which the index is big enough to prevent arm i from
being played. The value of Λi has been carefully chosen to bound the number of times arm i can be
played provided the index of some other arm is always larger than µ1 − ∆i/3. But more than this,
Λi does not depend on the rewards for odd-index arms j < i so is measurable with respect to the
σ-algebra σ(ˆµj,s : j ∈ Vi
, 1 ≤ s ≤ n). Define random variable I ∈ [k] to be the arm with the largest
mean such that there exists an odd j < I + 1 with ∆j ≤ ∆I+1/6 and
min
1≤s≤n
µˆj,s +
vuut
2
s
log
n
sI +
P
m>I min 
s, √
sΛm
	
!
> µj −
∆I+1
6
. (22)
The definition of I implies that arms i > I will not be played once their index drops far enough
below the mean of the optimal arm. We should hope that I is small with reasonably high probability,
with the best case being I = 1, which occurs when the optimal arm is always optimistic. Notice that
I is well-defined because of the convention that ∆k+1 = ∞. Let E1 be the event that I ≤ `, where `
is given in Eq. (15).
  
LATTIMORE
Lemma 8 The regret of ADA-UCB is bounded by
Rn(µ) ≤ E
"X
i>`
∆iΛi
#
+ nE
h
1{Ec
1
}∆I
i
+ E
"
1{E1}
X
`
i=1
∆iTi(n)
#
.
Proof The first task is to show that Ti(n) < Λi for all i > I, which follows by induction over rounds
k + 1 ≤ t ≤ n. Starting with the base case, note that when t = k + 1 we have Ti(t − 1) = 1 < Λi
for all i. Now suppose for t ≥ k + 1 that Ti(t − 1) < Λi for all i > I. By the definition of I there
must exist an odd j with ∆j ≤ ∆I+1/6 that satisfies Eq. (22). For this arm we have
Hj (t − 1) = X
k
m=1
min 
Tj (t − 1),
q
Tj (t − 1)Tm(t − 1)
< Tj (t − 1)I +
X
m>I
min 
Tj (t − 1),
q
Tj (t − 1)Λm

.
Therefore
γj (t) = ˆµj (t − 1) + s
2
Tj (t − 1) log 
n
Hj (t − 1)
> µj − ∆I+1/6 ≥ µ1 − ∆I+1/3 .
It follows that if i > I and Ti(t − 1) + 1 ≥ Λi
, then Lemma 7 implies At 6= i. Therefore Ti(t) < Λi
for all i > I, which completes the induction. Finally, since Pk
i=1 Ti(n) = n and using the definition
of E1 as the event that I ≤ `, the regret may be bounded by
Rn(µ) = E
"X
k
i=1
∆iTi(n)
#
≤ E
"X
i>`
∆iΛi
#
+ nE[1{Ec
1
}∆I ] + E
"
1{E1}
X
`
i=1
∆iTi(n)
#
. 
The last step in the proof of Theorem 1 is to bound each of the expectations in Lemma 8.
Lemma 9 There exists a universal C > 0 such that:
(i) E
"X
i>`
∆iΛi
#
≤ C
X
i>`
∆iλi (ii) E[1{Ec
1
}∆I ] ≤ C

n∆¯
` +
X
i>`
∆iλi
!
(iii) E
"
1{E1}
X
`
i=1
∆iTi(n)
#
≤ C

n∆¯
` +
X
i>`
∆iλi
!
.
The proof of part (i) follows shortly. The proof of part (ii) is deferred to Appendix B. Very briefly,
it follows somewhat directly from part (a) of Lemma 12 (notice the similarity between the lemma
and Eq. (22) that defines ∆I ). Part (iii) would be trivial if one assumed that E[Ti(n)] is monotone
non-increasing in i. This seems likely, but despite significant effort I was only able to show that this
is approximately true using a complicated proof (details are in Appendix C).
Proof [of Lemma 9 (i)] The proof follows by bounding E[Λi
] for each arm i > `. Naively bounding
the max in the definition of Λi by a sum shows that
E[Λi
] ≤ 1 +
128
∆2
i
+
36
∆2
i
log 
n∆2
i
ki

+ E

ζi

∆i
3
 +
18P (F
c
i
)
∆2
i
log
n∆2
i

. (23)
1 
REFINING THE CONFIDENCE LEVEL FOR BANDIT STRATEGIES
The first three terms are non-random. The second last term is bounded using Lemma 13 by
E[ζi(∆i/3)] ≤ 1 + 18/∆2
i
. For the last term we need to upper bound P (F
c
i
), where Fi
is the
event defined in Eq. (17). In order to do this we need to show that Wi
is reasonably large with high
probability. Let χj = 1 {j /∈ Wi}, which for j ∈ Vi satisfies E[χj ] ≤ δi ≤ 1/4. Then
P (F
c
i
) = P


X
j∈Wi
min 
1,
∆i
∆j

<
ki
8

 ≤ P


X
j∈Vi
(χj − E[χj ]) min 
1,
∆i
∆j

>
ki
4


≤ exp

−
k
2
i
8
P
j∈Vi min n
1,
∆i
∆j
o2

 ≤ exp 
−
ki
8

,
where the first inequality follows from the facts that E[χj ] ≤ 1/4 and P
j∈Vi min{1, ∆i/∆j} ≥ ki/2
and the second inequality follows from Hoeffding’s bound and the fact that χj are independent for
j ∈ Vi
. Therefore
P (F
c
i
) log
n∆2
i

≤ log
n∆2
i

exp (−ki/8) ≤ 4 log 
n∆2
i
ki

,
which holds because ki ≥ 2 is guaranteed for all suboptimal arms i. The proof is completed by
substituting the above display into Eq. (23) and using the definition of λi = 1 + 1
∆2
i
log 
n∆2
i
ki

.
Proof [of Theorem 1] The finite-time bound Eq. (3) follows by substituting the bounds given in
Lemma 9 into Lemma 8 and Eq. (16). Minimax and sub-UCB results are derived as corollaries of
the finite-time bound Eq. (3) via Parts (iii) and (iv) of Lemma 21 in the appendix. The asymptotic
analysis has been given already in §3.
5. A lower bound
I now formalise the intuitive argument for the regret guarantee given in §2. The results show that in a
certain sense the upper bound in Theorem 1 is very close to optimal. The following lower bound
holds for all strategies, but does not give a lower bound for all µ simultaneously. Related results
have been proven by a variety of authors (Kulkarni and Lugosi, 2000; Bubeck et al., 2013; Salomon
et al., 2013; Lattimore, 2015b), with the most related by Garivier et al. (2016b). The most significant
difference between that work and the present article is that the lower-order terms are more carefully
considered here, and besides this, the assumptions, and so also results, are different.
Theorem 10 Fix a strategy and let µ ∈ R
k be such that n∆2
i ≥ 2ki
log(n) and ∆i ≤ 1 for all i
with ∆i > 0. Then one of the following holds:
(a) Rn(µ) ≥
1
2
X
i:∆i>0
1
∆i
log 
n∆2
i
2ki
log(n)

.
(b) There exists a µ
0 ∈ R
k and i with ∆i > 0 such that Rn(µ
0
) ≥
1
4
X
i:∆0
i>0
1
∆0
i
log(n),
where µ
0
i = µi + 2∆i and µ
0
j = µj for j 6= i and ∆0
i = ∆i(µ
0
).
  
LATTIMORE
Proof Suppose that (a) does not hold, then there exists a suboptimal arm i such that
E[Ti(n)] ≤
1
2∆2
i
log 
n∆2
i
2ki
log(n)

. (24)
Let µ
0 be as defined in the second part of the lemma and write P
0
and E
0
for expectation when rewards
are sampled from µ
0
. Then by Lemmas 18 and 19 in Appendix D we have
P (Ti(n) ≥ n/2) + P
0
(Ti(n) < n/2) ≥
ki
log(n)
n∆2
i
, 2δ .
By Markov’s inequality and Eq. (24) and the fact that ki ≥ 2,
P (Ti(n) ≥ n/2) ≤
2E[Ti(n)]
n
≤
1
n∆2
i
log 
n∆2
i
2ki
log(n)

≤
ki
log(n)
2n∆2
i
= δ .
Therefore P
0
(Ti(n) < n/2) ≥ δ, which implies that
Rn(µ
0
) ≥
δn∆i
2
=
1
4
X
k
j=1
min 
1
∆i
,
1
∆j

log(n) ≥
1
4
X
j:∆0
j>0
1
∆0
j
log(n). 
A conjecture is false It was conjectured by Bubeck and Cesa-Bianchi (2012) that the optimal
regret might have approximately the following form.
Rn(µ) ≤ C
X
i:∆i>0

∆i +
1
∆i
log  n
H

for all µ and n and k , (25)
where C > 0 is a universal constant and H =
P
i:∆i>0 ∆
−2
i
is a quantity that appears in the best-arm
identification literature (Bubeck et al., 2009; Audibert and Bubeck, 2010; Jamieson et al., 2014).
Theorem 11 There does not exist a strategy for which Eq. (25) holds.
Proof Let k ≥ 2 and µ1 = 0 and µ2 = −1/k and µi = −1 for i > 2, which implies that
H = k
2 + k − 2 ≥ n. For the rest of the proof we view the horizon n = k
2
to be a function of k.
Suppose that Rn(µ) = o(k log k), which must be true for any strategy witnessing Eq. (25). Then
mini>2 E [Ti(n)] = o(log k). Let i = arg mini>2 E [Ti(n)] and define µ
0
to be equal to µ except
for the ith coordinate, which has µ
0
i = 1. Let A be the event that Ti(n) ≥ n/2 and let P and P
0 be
measures on the space of outcomes induced by the interaction between the fixed strategy and the
bandits determined by µ and µ
0
respectively. Then for all ε > 0,
Rn(µ) + Rn(µ
0
) ≥
n
2

P (A) + P
0
(A
c
)

≥
n
4
exp
− KL(P, P
0
)

=
k
2
4
exp (−2E [Ti(n)]) = ω(k
2−ε
),
By the assumption on Rn(µ) and for suitably small ε we have Rn(µ
0
) = ω(k
2−ε
). But as the
number of arms k → ∞ (and so also the horizon), this cannot be true for any policy satisfying
Eq. (25), or even Eq. (1). Therefore the conjecture is not true.
  
REFINING THE CONFIDENCE LEVEL FOR BANDIT STRATEGIES
6. Empirical evaluation
ADA-UCB is compared to UCB (Katehakis and Robbins, 1995), MOSS (Menard and Garivier, 2017), ´
THOMPSON SAMPLING (Agrawal and Goyal, 2012) and IMED (Honda and Takemura, 2015),1 where
the reference indicates the source of the algorithm. All algorithms choose each arm once and
subsequently:
A
IMED
t = arg min
i∈[k]
Ti(t − 1)
2
(ˆµi(t − 1) − max
j∈[k]
µˆj (t − 1))2 + log(Ti(t − 1)).
A
UCB
t = arg max
i∈[k]
µˆi(t − 1) + s
2 log(t)
Ti(t − 1) .
A
MOSS
t = arg max
i∈[k]
µˆi(t − 1) + s
2
Ti(t − 1) log+

n
kTi(t − 1) log2
+

1 +
n
kTi(t − 1) .
A
TS
t = arg max
i∈[k]
θi(t) with θi(t) ∼ N (ˆµi(t − 1), 1/Ti(t − 1)).
The logarithmic term used by Menard and Garivier (2017) in their version of ´ MOSS is larger than
log(·) and this negatively affects its performance. If the variant proposed in Section 3 is used instead,
then it becomes comparable to ADA-UCB on the experiments described below, but still fails on the
computationally expensive experiment given in the previous technical report (Lattimore, 2015a). For
all other algorithms I have chosen the variant for which (a) guarantees exist and (b) the empirical
performance is best. In all plots N indicates the number of independent samples per data point and
confidence bands are calculated at a 95% level. The first three plots in Figure 1 show the regret in the
worst case regime where all suboptimal arms have the same suboptimality gap. Unsurprisingly the
relative advantage of policies with well-tuned confidence levels increases with the number of arms. At
its worst, UCB suffers about three times the regret of ADA-UCB. Coincidentally, p
log(104) ≈ 3.03.
Figure 2 shows the regret as a function of the horizon n on a fixed bandit with k = 20 arms (see
caption of figure for means). The regret of ADA-UCB is again a little better than the alternatives.
7. Discussion
Anytime strategies The ADA-UCB strategy depends on the horizon n, which may sometimes be
unknown. The natural idea is to replace n by t, which indeed leads to a reasonable strategy that
enjoys the same guarantees as ADA-UCB provided the log(·) function is replaced by something
fractionally larger. The analysis, however, is significantly longer and is not included. Interested
readers may refer to the technical report (Lattimore, 2016b) for the core ideas, but more work is
required to find a clean proof.
Multiple optimal arms The finite-time bound in Theorem 1 is the first that demonstrates an
improvement when there are multiple (near-)optimal arms. The gain in terms of the expected regret is
not very large because ki (which grows as optimal arms are added) appears only in the denominator
of the logarithm. There is, however, a more significant advantage when there are many optimal
arms, which (up to a point) is an exponential decrease in the variance of most strategies. This can be
1. IMED is usually defined for bandits where the rewards have (semi-)bounded support, but Junya Honda kindly provided
unpublished details of the adaptation to the Gaussian case.
15
LATTIMORE
0 0.5 1 1.5 2
0
10
20
30
N = 104
k = 2, n = 103
∆
Regret
0 0.2 0.4 0.6 0.8 1
0
50
100
150 N = 2000
k = 10, n = 103
∆
0 0.2 0.4 0.6 0.8 1
0
1,000
2,000
3,000 N = 200
k = 100, n = 104
∆
ADA-UCB
IMED
MOSS
TS
UCB
Figure 1: The regret of various algorithms as a function of ∆ when µ = (∆, 0, . . . , 0) and the
number of arms is 2, 10 and 100 respectively. The y-axis shows the regret averaged over
N independent samples for each data point.
0 10,000
0
200
400
600
800
k = 20, n = 104
N = 2 × 103
n
Regret
ADA-UCB
IMED
MOSS
TS
UCB
Figure 2: The regret of various algorithms as a function of the horizon for the Gaussian bandit with
k = 20 arms and payoff vector µ = (0, -0.03, -0.03, -0.07, -0.07, -0.07, -0.15, -0.15, -0.15,
-0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -0.5, -1, -1). ADA-UCB is again outperforming
the competitors. Note that IMED and THOMPSON SAMPLING are so similar they cannot be
distinguished in the plot.
extracted from the analysis by observing that the high variance is caused by the possibility that an
optimal arm is not sufficiently optimistic, but the probability of this occurring drops exponentially as
the number of optimal arms increases.
Alternative noise models and other extensions The most obvious open question is how to
generalise the results to a broader class of noise models and setups. I am quite hopeful that this
is possible for noise from exponential families, though the analysis will necessarily become more
complicated because the divergences become more cumbersome to work with than the squared
distance that is the divergence in the Gaussian case. An alternative direction is to consider the
16
REFINING THE CONFIDENCE LEVEL FOR BANDIT STRATEGIES
situation where the variance is also unknown, which has seen surprisingly little attention, but is now
understood reasonably well (Honda and Takemura, 2014; Cowan et al., 2015). At the very least, the
concentration analysis using Brownian motion could be applied, but I expect an adaptive confidence
level will also yield theoretical and practical improvements. Another potential application of the
ideas presented here would be to try and port them into other bandit strategies that depend on a
confidence level such as BAYES-UCB (Kaufmann, 2016), or even linear bandits (Dani et al., 2008;
Abbasi-Yadkori et al., 2011; Lattimore and Szepesvari, 2017).