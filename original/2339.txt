We consider the problem of learning a linear subspace from data corrupted by outliers. Classical approaches are typically designed for the case in which the subspace dimension is small relative
to the ambient dimension. Our approach works with a dual representation of the subspace and
hence aims to find its orthogonal complement; as such, it is particularly suitable for subspaces
whose dimension is close to the ambient dimension (subspaces of high relative dimension). We
pose the problem of computing normal vectors to the inlier subspace as a non-convex `1 minimization problem on the sphere, which we call Dual Principal Component Pursuit (DPCP) problem.
We provide theoretical guarantees under which every global solution to DPCP is a vector in the
orthogonal complement of the inlier subspace. Moreover, we relax the non-convex DPCP problem
to a recursion of linear programs whose solutions are shown to converge in a finite number of steps
to a vector orthogonal to the subspace. In particular, when the inlier subspace is a hyperplane, the
solutions to the recursion of linear programs converge to the global minimum of the non-convex
DPCP problem in a finite number of steps. We also propose algorithms based on alternating minimization and iteratively re-weighted least squares, which are suitable for dealing with large-scale
data. Experiments on synthetic data show that the proposed methods are able to handle more outliers and higher relative dimensions than current state-of-the-art methods, while experiments in the
context of the three-view geometry problem in computer vision suggest that the proposed methods
can be a useful or even superior alternative to traditional RANSAC-based approaches for computer
vision and other applications.
Keywords: Outliers, Robust Principal Component Analysis, High Relative Dimension, `1 Minimization, Non-Convex Optimization, Linear Programming, Trifocal Tensor
1. Introduction
Principal Component Analysis (PCA) is one of the oldest (Pearson, 1901; Hotelling, 1933) and most
fundamental techniques in data analysis, with ubiquitous applications in engineering (Moore, 1981),
economics and sociology (Vyas and Kumaranayake, 2006), chemistry (Ku et al., 1995), physics
(Loyd et al., 2014) and genetics (Price et al., 2006) to name a few; see Jolliffe (2002) for more applications. Given a data matrix X ∈ R
D×L of L data points of coordinate dimension D, PCA gives
a closed form solution to the problem of finding a d-dimensional linear subspace Sˆ that is closest,
in the Euclidean sense, to the columns of X . Although the optimization problem associated with
PCA is non-convex, it does admit a simple solution by means of the Singular Value Decomposition
(SVD) of X . In fact, Sˆ is the subspace spanned by the first d left singular vectors of X .
Using Sˆ as a model for the data X is meaningful when the data are known to have an approximately linear structure of underlying dimension d, i.e., they lie close to a d-dimensional subspace
S. In practice, the principal components of X are known to be well-behaved under mild levels of
noise, i.e., the principal angles between Sˆ and S are relatively small, and in fact, Sˆ is optimal when
the noise is Gaussian (Jolliffe, 2002). However, very often in applications the data are corrupted by
outliers, i.e., the data matrix has the form X˜ = [X O]Γ, where the M columns of O ∈ R
D×M are
points of R
D whose angles from the underlying ground truth subspace S associated with the inlier
points X are large, and Γ is an unknown permutation. In such cases, the principal angles between S
and its PCA estimate Sˆ will in general be large, even when M is small. This is to be expected since,
by definition, the principal components of X˜ are orthogonal directions of maximal correlation with
all the points of X˜. This phenomenon, together with the fact that outliers are almost always present
in real datasets, has given rise to the important problem of outlier detection in PCA.
Traditionally, outlier detection has been a major area of study in robust statistics with notable methods being Influence-based Detection, Multivariate Trimming, M-Estimators, Iteratively
Reweighted Least Squares (IRLS) and Random Sampling Consensus (RANSAC) (Huber, 1981; Jolliffe, 2002). These methods are usually based on non-convex optimization problems and in practice
converge only to a local minimum. In addition, their theoretical analysis is usually limited and their
computational complexity may be large (e.g., in the case of RANSAC). Recently, two attractive
methods have appeared (Xu et al., 2012; Soltanolkotabi and Candes, 2012) that are directly based `
on convex optimization and are inspired by low-rank representation (Liu et al., 2010) and compressed sensing (Candes and Wakin, 2008). Even though both of these methods admit theoretical `
guarantees and efficient implementations, they are in principle applicable only in the case of subspaces of small relative dimension (i.e., d/D  1). On the other hand, the theoretical guarantees
of the recent REAPER method of Lerman et al. (2015) seem to suggest that the method is able to
handle any subspace dimension.
In this paper we adopt a dual approach to the problem of robust PCA in the presence of outliers,
which allows us to explicitly transcend the low relative dimension regime of modern methods such
as Xu et al. (2012) or Soltanolkotabi and Candes (2012), and even be able to handle as many as `
70% outliers for hyperplanes (subspaces of maximal relative dimension (D − 1)/D), a regime
where other modern (Lerman et al., 2015) or classic (Huber, 1981) methods fail. The key idea
of our approach comes from the fact that, in the absence of noise, the inliers X lie inside any
hyperplane H1 = Span(b1)
⊥ that contains the underlying linear subspace S associated with the
inliers. This suggests that, instead of attempting to fit directly a low-dimensional linear subspace to
the entire dataset X˜, as done e.g. in Xu et al. (2012), we can search for a maximal hyperplane H1
that contains as many points of the dataset as possible. When the inliers X are in general position
(to be made precise shortly) inside S, and the outliers O are in general position in R
D, such a
maximal hyperplane will contain the entire set of inliers together with possibly a few outliers. Then
one may remove all points that lie outside this hyperplane and be left with an easier robust PCA
problem that could potentially be addressed by existing methods. Alternatively, one can continue
by finding a second maximal hyperplane H2 = Span(b2)
⊥, with the new dual principal component
b2 perpendicular to the first one, i.e., b2 ⊥ b1, and so on, until c := D−d such maximal hyperplanes
H1, . . . , Hc have been found, leading to a Dual Principal Component Analysis (DPCA) of X˜. In
2
DUAL PRINCIPAL COMPONENT PURSUIT
such a case, the inlier subspace is precisely equal to Tc
i=1 Hi
, and a point is an outlier if and only if
it lies outside this intersection.
We formalize the problem of searching for maximal hyperplanes with respect to X˜ as an `0
cosparsity-type problem (Nam et al., 2013), which we relax to a non-convex `1 problem on the
sphere, referred to as the Dual Principal Component Pursuit (DPCP) problem. We provide theoretical guarantees under which every global solution of the DPCP problem is a vector orthogonal to the
linear subspace associated with the inliers, i.e., it is a dual principal component. Moreover, we relax
the non-convex DPCP problem to a recursion of linear programming problems and we show that,
under mild conditions, their solutions converge to a dual principal component in a finite number of
steps. In particular, when the inlier subspace is a hyperplane, then the solutions of the linear programming recursion converge to the global minimum of the non-convex problem in a finite number
of steps. Furthermore, we propose algorithms based on alternating minimization and IRLS that
are suitable for dealing with large-scale data. Extensive experiments on synthetic data show that
the proposed methods are able to handle more outliers and subspaces of higher relative dimension
d/D than state-of-the-art methods (Fischler and Bolles, 1981; Xu et al., 2012; Soltanolkotabi and
Candes, 2012; Lerman et al., 2015), while experiments with real face and object images show that `
our DPCP-based methods perform on par with state-of-the-art methods.
Notation The shorthand RHS stands for Right-Hand-Side and similarly for LHS. The notation ∼=
stands for isomorphism in whatever category the objects lying to the LHS and RHS of the symbol belong to. The notation ' denotes approximation. For any positive integer n let [n] := {1, 2, . . . , n}.
For any positive number α let dαe denote the smallest integer that is greater than α. For sets A, B,
the set A \ B is the set of all elements of A that do not belong to B. If S is a subspace of R
D, then
dim(S) denotes the dimension of S and πS : R
D → S is the orthogonal projection of R
D onto
S. For vectors b, b
0 ∈ R
D we let ∠b, b
0
be the acute angle between b and b
0
, defined as the unique
angle θ ∈ [0 90◦
] such that cos θ =

b
>b
0


. If b is a vector of R
D and S a linear subspace of R
D,
the principal angle of b from S is ∠b, πS(b). The symbol ⊕ denotes direct sum of subspaces. The
orthogonal complement of a subspace S in R
D is S
⊥. If y1
, . . . , ys
are elements of R
D, we denote
by Span(y1
, . . . , ys
) the subspace of R
D spanned by these elements. S
D−1 denotes the unit sphere
of R
D. For a vector w ∈ R
D we define wˆ := w/

w


2
, if w 6= 0, and wˆ := 0 otherwise. Given a
square matrix C, Diag(C) denotes the vector of diagonal elements of C. Given a square matrix P ,
the notation 0 ≤ P ≤ I indicates that P , I − P are positive semi-definite matrices. With a mild
abuse of notation we will be treating on several occasions matrices as sets, i.e., if X is D × N and
x a point of R
D, the notation x ∈ X signifies that x is a column of X . Similarly, if O is a D × M
matrix, the notation X ∩ O signifies the points of R
D that are common columns of X and O. The
notation Sign denotes the sign function Sign : R → {−1, 0, 1} defined as
Sign(x) = 
x/|x| if x 6= 0,
0 if x = 0.
(1)
Finally, we note that the ith entry of the subdifferential of the `1-norm kzk1 =
PD
i=1 |zi
| of a vector
z = (z1, . . . , zD)
> is a set-valued function on R
D defined as
Sgn(zi) = (
Sign(zi) if zi 6= 0,
[−1, 1] if zi = 0.
(2)
3
TSAKIRIS AND VIDAL
2. Prior Art
We begin by briefly reviewing some state-of-the-art methods for learning a linear subspace from
data X˜ = [X O]Γ in the presence of outliers. The literature on this subject is vast and our account
is far from exhaustive; with a few exceptions, we mainly focus on modern methods based on convex optimization. For methods from robust statistics see Huber (1981); Jolliffe (2002), for online
subspace learning methods see Balzano et al. (2010); Feng et al. (2013), for regression-type methods see Wang et al. (2015), while for fast and other methods the reader is referred to the excellent
literature review of Lerman and Zhang (2014) or the recent survey by Lerman and Maunu (2018).
Finally, we note that preliminary results associated with the present work have been published in
the form of a conference paper1
(Tsakiris and Vidal, 2015). While the present paper was under
review, we extended our approach to clustering data from multiple subspaces (Tsakiris and Vidal,
2017), which can also be thought of as a robust PCA problem but with structured outliers. This is a
continuation of the present work, which certainly builds on the concepts and algorithms presented
here, yet requires sufficiently distinct machinery to be fully established.
RANSAC One of the oldest and most popular outlier detection methods for PCA is Random
Sampling Consensus (RANSAC) (Fischler and Bolles, 1981). The idea behind RANSAC is simple:
alternate between randomly sampling a subset of cardinality d from the dataset and computing a
d-dimensional subspace from this subset, until a subspace Sˆ is found that maximizes the number
of points in the entire dataset that approximately lie in Sˆ within some error. RANSAC is typically
used when the ambient dimension D is small (say D ≤ 20), yielding high quality subspace estimates regardless of the subspace relative dimension d/D. However, as D increases, RANSAC
becomes inefficient for large values of d/D, except when the outlier ratio is very small, as otherwise a prohibitive number of trials may be required in order to obtain outlier-free samples and thus
furnish reliable models. Additionally, RANSAC requires as input an estimate for the dimension of
the subspace as well as a thresholding parameter, which is used to distinguish outliers from inliers;
naturally the performance of RANSAC is very sensitive to these two parameters.
`2,1-RPCA Unlike RANSAC, modern methods for outlier detection in PCA are primarily based
on convex optimization. One of the earliest and most important such methods is the `2,1-RPCA
method of Xu et al. (2012), which is in turn inspired by the Robust Principal Component Analysis
(RPCA) algorithm of Candes et al. (2011). ` `2,1-RPCA computes a (`∗ +`2,1)-norm decomposition2
of the data matrix, instead of the (`∗ +`1)-decomposition in Candes et al. (2011). More specifically, `
`2,1-RPCA solves the optimization problem
min
L,E: X˜=L+E

L


∗
+ λ

E


2,1
, (3)
which attempts to decompose the data matrix X˜ = [X O]Γ into the sum of a low-rank matrix
L, and a matrix E that has only a few non-zero columns. The idea is that L is associated with
the inliers, having the form L = [X 0D×M]Γ, and E is associated with the outliers, having the
form E = [0D×N O]Γ. The optimization problem (3) is convex and admits theoretical guarantees
1. We note that the proof of Theorem 2 in Tsakiris and Vidal (2015) contained an inaccuracy which makes its statement
incomplete. The complete statement is Theorem 11 in the present paper.
2. Here `∗ denotes the nuclear norm, which is the sum of the singular values of the matrix. Also, `2,1 is defined as the
sum of the Euclidean norms of the columns of a matrix.
4
DUAL PRINCIPAL COMPONENT PURSUIT
and efficient algorithms based on the alternating direction method of multipliers (ADMM) (Gabay
and Mercier, 1976). However, it is expected to succeed only when the intrinsic dimension d of the
inliers is small enough (otherwise [X 0D×M] will not be low-rank), and the outlier ratio is not too
large (otherwise [0D×N O] will not be column-sparse). Finally, notice that `2,1-RPCA does not
require as input the subspace dimension d, because it does not directly compute an estimate for the
subspace. Rather, the subspace can be obtained subsequently by applying classic PCA on L, and
now one does need an estimate for d.
SE-RPCA Separating outliers from low-rank inlier points can also be achieved by exploiting the
self-expressiveness (SE) property of the data matrix, a notion popularized by the work of Elhamifar
and Vidal (2011, 2013) in the area of subspace clustering (Vidal, 2011). Specifically, if a column
x˜ of X˜ is an inlier, then it can be expressed as a linear combination of d other inliers in X˜, while
if x˜ is an outlier, then in principle it requires D columns of X˜. The coefficient matrix C can be
obtained as the solution to the convex optimization problem
min
C
kCk1 s.t. X˜ = X˜C, Diag(C) = 0, (4)
where the extra constraint prevents the trivial solution C = I. Given C, and under the hypothesis
that d/D is small, a column of X˜ is declared as an outlier, if the `1 norm of the corresponding
column of C is large; see Soltanolkotabi and Candes (2012) for an explicit formula. SE-RPCA `
admits theoretical guarantees (Soltanolkotabi and Candes, 2012) and efficient ADMM implementa- `
tions (Elhamifar and Vidal, 2013). Moreover, the recent work of You et al. (2017) has demonstrated
that the information contained in the self-expressive matrix C can be further exploited to identify
the outliers by means of a random walk on the directed affinity graph defined by C, thus yielding
superior results than the simple thresholding of the norms of the columns of C. In contrast to `2,1-
RPCA, which in principle fails in the presence of a very large number of outliers, SE-RPCA is still
expected to perform well, since the existence of sparse subspace-preserving self-expressive patterns
does not depend on the number of outliers present. Also, similarly to `2,1-RPCA, SE-RPCA does
not directly require an estimate for the subspace dimension d. Nevertheless, knowledge of d is necessary if one wants to furnish an actual subspace estimate, which entails removing the outliers (a
judiciously chosen threshold would also be necessary here) and applying PCA.
REAPER Another robust subspace learning method that admits an interesting theoretical analysis
is REAPER (Lerman et al., 2015), which is conceptually associated with the optimization problem
min
P
X
L
j=1

(ID − P )x˜j


2
s.t. P is an orthogonal projection and Trace(P ) = d. (5)
Here the vector x˜j denotes the j-th column of X˜ and the matrix P denotes the orthogonal projection
onto a d-dimensional linear subspace S. Notice that P can be thought of as the product P = UU >,
where the columns of U ∈ R
D×d
form an orthonormal basis for S. Since the problem in (5) is nonconvex, Lerman et al. (2015) relaxed it to the convex semi-definite program
min
P
X
L
j=1

(ID − P )x˜j


2
s.t. 0 ≤ P ≤ ID, Trace (P ) = d, (6)
5
TSAKIRIS AND VIDAL
and obtained an approximate solution P
∗
to (5) as the rank-d orthogonal projector that is closest to
the global solution of (6), as measured by the nuclear norm. It was shown by Lerman et al. (2015)
that the orthoprojector P
∗
obtained in this way is within a neighborhood of the orthoprojector
corresponding to the true underlying inlier subspace. In practice, the semi-definite program (6) may
become prohibitively expensive to solve even for moderate values of the ambient dimension D.
As a consequence, the authors proposed an Iteratively Reweighted Least Squares (IRLS) scheme to
obtain a numerical solution of (6), whose objective value was shown to converge to a neighborhood
of the optimal objective value of problem (6).
One advantage of REAPER with respect to `2,1-RPCA and SE-RPCA, is that its theoretical
conditions allow for the subspace to have arbitrarily large relative dimension, providing that the
outlier ratio is sufficiently small. It is interesting to note here that this is precisely the condition
under which RANSAC (Huber, 1981) can handle large relative dimensions; the main difference
though between RANSAC and REAPER is that the latter employs convex optimization, and for a
fixed relative dimension and computational budget REAPER can tolerate considerably higher outlier
ratios than RANSAC (see Fig. 6, §7).
COHERENCE PURSUIT (CoP) The recent work of Rahmani and Atia (2017) analyzes a simple
yet efficient algorithm for detecting the inlier space from pairwise point coherences, hence called
Coherence Pursuit (CoP). The main insight behind CoP is that, under the hypothesis that the inliers
lie in a low-dimensional subspace, inlier points tend to have significantly higher coherence with the
rest of the points in the dataset, than outlier points. Hence the columns of the pairwise coherence
matrix X˜
>
X˜ that have large norm are expected to correspond to inliers. Indeed, CoP orders the
columns of X˜
>
X˜ in descending values for a given norm and selects sufficiently many of them from
the top, until their span yields a d-dimensional subspace. Similarly to SE-RPCA, the performance of
CoP is not expected to be significantly degraded as the number of outliers increases, as long as each
outlier remains sufficiently incoherent with the rest of the dataset. As demonstrated by Rahmani
and Atia (2017), CoP has a competitive performance and admits an extensive theoretical analysis.
L1-PCA∗ Finally, we mention the method L1-PCA∗ of Brooks et al. (2013), since it works with
the orthogonal complement of the subspace, similarly to the proposed method of the present paper.
Nevertheless, L1-PCA∗
is slightly unusual in that it learns `1 hyperplanes, i.e., hyperplanes that
minimize the `1 distance to the points, as opposed to the Euclidean distance used by methods such
as PCA and REAPER. Overall, no theoretical guarantees seem to be known for L1-PCA∗
, as far as
the subspace learning problem is concerned. In addition, L1-PCA∗
requires solving O(D2
) linear
programs, where D is the ambient dimension, which makes it computationally expensive.
3. Problem Formulation
In this section we formulate the problem addressed in this paper. We describe our data model (§3.1),
and motivate the problem at a conceptual (§3.2) and computational level (§3.3).
3.1 Data model
We employ a deterministic noise-free data model, under which the given data is
X˜ = [X O]Γ = [x˜1, . . . , x˜L] ∈ R
D×L
, (7)
6
DUAL PRINCIPAL COMPONENT PURSUIT
where the N inliers X = [x1, . . . , xN ] ∈ R
D×N lie in the intersection of the unit sphere S
D−1 with
an unknown proper subspace S of R
D of unknown dimension 1 ≤ d ≤ D − 1, and the M outliers
O = [o1, . . . , oM] ∈ R
D×M lie on the sphere S
D−1
. The unknown permutation Γ indicates that we
do not know which point is an inlier and which point is an outlier. Finally, we assume that the points
X˜ are in general position, in the sense that there are no relations among the columns of X˜ except
for those implied by the inclusions X ⊂ S and X˜ ⊂ R
D. In particular, every D-tuple of columns
of X˜ such that at most d points come from X is linearly independent. Notice that as a consequence
every d-tuple of inliers and every D-tuple of outliers are linearly independent, and also X ∩O = ∅.
Finally, to avoid degenerate situations we will assume that N ≥ d + 1 and M ≥ D − d.
3
3.2 Conceptual formulation
Given X˜, we consider the problem of partitioning its columns into those that lie in S and those
that don’t. Since we have made no assumption about the dimension of S, this problem is however
not well posed because S can be anything from a line to a (D − 1)-dimensional hyperplane, and
hence X lies inside every subspace that contains S, which in turn may contain some elements of
O. Instead, it is meaningful to search for a linear subspace of R
D that contains all of the inliers
and perhaps a few outliers. Since we do not know the intrinsic dimension d of the inliers, a natural
choice is to search for a hyperplane of R
D that contains all the inliers.
Problem 1 Given the dataset X˜ = [X O]Γ, find a hyperplane H that contains all the inliers X .
Notice that hyperplanes that contain all the inliers always exist: any non-zero vector b in the
orthogonal complement S
⊥ of the linear subspace S associated with the inliers defines a hyperplane
(with normal vector b) that contains all inliers X . Having such a hyperplane H1 at our disposal,
we can partition our dataset as X˜ = X˜1 ∪ X˜2, where X˜1 are the points of X˜ that lie in H1 and
X˜2 are the remaining points. Then by definition of H1, we know that X˜2 will consist purely of
outliers, in which case we can safely replace our original dataset X˜ with X˜1 and reconsider the
problem of robust PCA on X˜1. We emphasize that X˜1 will contain all the inliers X together with
at most D−d−1 outliers,4
a number which may be dramatically smaller than the original number of
outliers. Then one may apply existing methods such as Xu et al. (2012), Soltanolkotabi and Candes`
(2012) or Fischler and Bolles (1981) to finish the task of identifying the remaining outliers, as the
following example demonstrates.
Example 1 Suppose we have N = 1000 inliers lying in general position in a linear subspace of
R
100 of dimension d = 90. Suppose that the dataset is corrupted by M = 1000 outliers lying in
general position in R
100. Let H be a hyperplane that contains all 1000 inliers. Since the dimensionality of the inliers is 90 and the dimensionality of the hyperplane is 99, there are only 99 − 90 = 9
linearly independent directions left for the hyperplane to fit, i.e., H will contain at most 9 outliers (it
can not contain more outliers since this would violate the general position hypothesis). If we remove
the points of the dataset that do not lie in H, then we are left with 1000 inliers and at most 9 outliers.
A simple application of RANSAC is expected to identify the remaining outliers in only a few trials.
3. If the number of outliers is less than D −d, then the entire dataset is degenerate because it lies in a proper hyperplane
of the ambient space, hence we can reduce the coordinate representation of the data and eventually satisfy the stated
condition.
4. This comes from the assumption of general position.
7
TSAKIRIS AND VIDAL
Alternatively, if the true dimension d is known, one may keep working with the entire dataset
X˜ (i.e., no point removal takes place) and search for a second hyperplane H2 that contains all the
inliers, such that its normal vector b2 is linearly independent (e.g., orthogonal) from the normal
vector b1 of H1. Then H1 ∩ H2 is a linear subspace of dimension D − 2 that contains all the
inliers X , and as a consequence Span(X ) = S ⊂ H1 ∩ H2. Then a third hyperplane H3 ⊃ X
may be sought for, such that its normal vector b3 is not in Span(b1, b2), and so on. Repeating this
process c = codim S = D − d times, until c linearly independent hyperplanes5 H1, . . . , Hc have
been found, each containing X , we arrive at a situation where Tc
k=1 Hk is a subspace of dimension
d = dim S that contains S and thus it must be the case that Tc
k=1 Hk = S. Hence we may declare
a point to be an inlier if and only if the point lies in the intersection of these c hyperplanes.
3.3 Hyperplane pursuit by `1 minimization
In this section we propose an optimization framework for the computation of a hyperplane that
solves Problem 1, i.e., a hyperplane that contains all the inliers. To proceed, we need a definition.
Definition 2 A hyperplane H of R
D is called maximal with respect to the dataset X˜, if it contains
a maximal number of data points in X˜, i.e., if for any other hyperplane H† of R
D we have that
Card(X˜ ∩ H) ≥ Card(X˜ ∩ H†
).
In principle, hyperplanes that are maximal with respect to X˜, always solve Problem 1, as the next
proposition shows (see §5.1 for the proof).
Proposition 3 Suppose that N ≥ d+1 and M ≥ D−d, and let H be a hyperplane that is maximal
with respect to the dataset X˜. Then H contains all the inliers X .
In view of Proposition 3, we may restrict our search for hyperplanes that contain all the inliers X
to the subset of hyperplanes that are maximal with respect to the dataset X˜. The advantage of this
approach is immediate: the set of hyperplanes that are maximal with respect to X˜ is in principle
computable, since it is precisely the set of solutions of the following optimization problem
min
b

X˜
>
b


0
s.t. b 6= 0. (8)
The idea behind (8) is that a hyperplane H = Span(b)
⊥ contains a maximal number of columns
of X˜ if and only if its normal vector b has a maximal cosparsity level with respect to the matrix
X˜
>
, i.e., the number of non-zero entries of X˜
>
b is minimal. Since (8) is a combinatorial problem
admitting no efficient solution, we consider its natural relaxation
min
b

X˜
>
b


1
s.t.

b


2
= 1, (9)
which in our context we will be referring to as Dual Principal Component Pursuit (DPCP). A major
question that arises, to be answered in Theorem 11, is under what conditions every global solution
of (9) is orthogonal to the inlier subspace Span(X ). A second major question, raised by the nonconvexity of the constraint b ∈ S
D−1
, is how to efficiently solve (9) with theoretical guarantees.
5. By the hyperplanes being linearly independent we mean that their normal vectors are linearly independent.
8
DUAL PRINCIPAL COMPONENT PURSUIT
We emphasize here that the optimization problem (9) is far from new; interestingly, its earliest
appearance in the literature that we are aware of is in Spath and Watson (1987), where the authors ¨
proposed to solve it by means of the recursion of convex problems given by6
nk+1 := argmin
b
>nˆk=1

X˜
>
b


1
. (10)
Notice that at each iteration of (10) the problem that is solved is computationally equivalent to a
linear program; this makes the recursion (10) a very appealing candidate for solving the non-convex
(9). Even though Spath and Watson (1987) proved the very interesting result that (10) converges to ¨
a critical point of (9) in a finite number of steps (see Appendix A), there is no reason to believe that
in general (10) converges to a global minimum of (9).
Other works in which optimization problem (9) appears are Spielman et al. (2013); Qu et al.
(2014); Sun et al. (2015c,d,b,a). More specifically, Spielman et al. (2013) propose to solve (9) by
replacing the quadratic constraint b
>b = 1 with a linear constraint b
>w = 1 for some vector w.
In Qu et al. (2014); Sun et al. (2015b) (9) is approximately solved by alternating minimization,
while a Riemannian trust-region approach is employed in Sun et al. (2015a). Finally, we note that
problem (9) is closely related to the non-convex problem (5) associated with REAPER. To see this,
suppose that the REAPER orthoprojector Π appearing in (5), represents the orthogonal projection
to a hyperplane U with unit-`2 normal vector b. In such a case ID −Π = bb> and it readily follows
that problem (5) becomes identical to problem (9).
4. Dual Principal Component Pursuit Theory
In this section we establish our analysis framework and discuss our main theoretical results regarding the global optimum of the non-convex problem (9) as well as the recursion of convex relaxations
in (10). We begin our theoretical investigation in §4.1 by establishing a connection between the discrete problems (9) and (10) and certain underlying continuous problems. The continuous problems
do not depend on a finite set of inliers and outliers, rather on uniform distributions on the respective
inlier and outlier spaces, and as such, are easier to analyze. The analysis of Theorems 5 and 6 reveals that the optimal solutions of the continuous analogue of (9) are orthogonal to the inlier space,
and that the solutions of the continuous recursion corresponding to (10) converge to a normal vector
to the inlier space, respectively. This suggests that under certain conditions on the distribution of
the data, the same must be true for the discrete problem (9) and the discrete recursion (10), where
the adjective discrete refers to the fact that these problems depend on a finite set of points. Our
analysis of the discrete problems is inspired by the analysis of their continuous counterparts and the
link between the two is formally captured through certain discrepancy bounds that we introduce in
§4.2. In turn, these allow us to prove conditions under which we can characterize the global optimal
of problem (9) as well as the convergence of recursion (10); this is done in §4.3 and the main results
are Theorems 11 and 12, which are analogues of Theorems 5 and 6. These theorems suggest that
both (9) and (10) are natural formulations for computing the orthogonal complement of a linear
subspace in the presence of outliers. The proofs of all theorems as well as intermediate results are
deferred to §5.
6. Being unaware of the work of Spath and Watson (1987), we independently proposed the same recursion in (Tsakiris ¨
and Vidal, 2015).
9
TSAKIRIS AND VIDAL
4.1 Formulation and theoretical analysis of the underlying continuous problems
In this section we show that the problems of interest (9) and (10) can be viewed as discrete versions
of certain continuous problems, which are easier to analyze. To begin with, consider given outliers
O = [o1, . . . , oM] ⊂ S
D−1
and inliers X = [x1, . . . , xN ] ⊂ S ∩ S
D−1
, and recall the notation
X˜ = [X O]Γ, where Γ is an unknown permutation. Next, for any b ∈ S
D−1 define the function
fb : S
D−1 → R by fb(z) =

b
>z


. Define also discrete measures µO and µX on S
D−1
associated
with the outliers and inliers respectively, as
µO(z) = 1
M
X
M
j=1
δ(z − oj ) and µX (z) = 1
N
X
N
j=1
δ(z − xj ), (11)
where δ(·) is the Dirac function on S
D−1
, satisfying
Z
z∈SD−1
g(z)δ(z − z0)dµSD−1 = g(z0), (12)
for every g : S
D−1 → R and every z0 ∈ S
D−1
; µSD−1 is the uniform measure on S
D−1
.
With these definitions, we have that the objective function

X˜
>
b


1
appearing in (9) and (10) is
the sum of the weighted expectations of the function fb under the measures µO and µX , i.e.,

X˜
>
b


1
=

O>b


1
+

X
>b


1
=
X
M
j=1

b
>oj

 +
X
N
j=1

b
>xj


(13)
=
X
M
j=1
Z
z∈SD−1

b
>z

δ(z − oj )dµSD−1 +
X
N
j=1
Z
z∈SD−1

b
>z

δ(z − xj )dµSD−1 (14)
=
Z
z∈SD−1

b
>z

X
M
j=1
δ(z − oj )dµSD−1 +
Z
z∈SD−1

b
>z

X
N
j=1
δ(z − xj )dµSD−1 (15)
= M EµO (fb) + N EµX
(fb). (16)
Hence, the optimization problem (9), which we repeat here for convenience,
min
b

X˜
>
b


1
s.t. b
>b = 1, (17)
is equivalent to the problem
min
b
[M EµO (fb) + N EµX
(fb)] s.t. b
>b = 1. (18)
Similarly, the recursion (10), repeated here for convenience,
nk+1 = argmin
b

X˜
>
b


1
s.t. b
>nˆ k = 1, (19)
is equivalent to the recursion
nk+1 = argmin
b
[M EµO (fb) + N EµX
(fb)] s.t. b
>nˆ k = 1. (20)
10
DUAL PRINCIPAL COMPONENT PURSUIT
Now, the discrete measures µO, µX of (11), are discretizations of the continuous measures µSD−1 ,
and µSD−1∩S respectively, where the latter is the uniform measure on S
D−1 ∩ S. Hence, for the purpose of understanding the properties of the global minimizer of (18) and the limiting point of (20),
it is meaningful to replace in (18) and (20) the discrete measures µO and µX by their continuous
counterparts µSD−1 and µSD−1∩S, and study the resulting continuous problems
min
b
h
M EµSD−1
(fb) + N EµSD−1∩S
(fb)
i
s.t. b
>b = 1, (21)
nk+1 = argmin
b
h
M EµSD−1
(fb) + N EµSD−1∩S
(fb)
i
s.t. b
>nˆ k = 1. (22)
It is important to note that if these two continuous problems have the geometric properties of interest,
i.e., if every global solution of (21) is a vector orthogonal to the inlier subspace, and similarly, if
the sequence of vectors {nk} produced by (22) converges to a vector nk
∗ orthogonal to the inlier
subspace, then this correctness of the continuous problems can be viewed as a first theoretical
verification of the correctness of the discrete formulations (9) and (10). The objective of the rest of
this section is to establish that this is precisely the case.
Before discussing our main two results in this direction, we note that the continuous objective
function appearing in (21) and (22) can be re-written in a more suggestive form. To see what that
is, define cD as the average height of the unit hemisphere of R
D, directly computed as
cD := Z
z∈SD−1
|z1|dµSD−1 =
(D − 2)!!
(D − 1)!! ·
 2
π
if D even,
1 if D odd,
(23)
where z1 is the first coordinate of the vector z, and the double factorial is defined as
k!! := 
k(k − 2)(k − 4)· · · 4 · 2 if k even,
k(k − 2)(k − 4)· · · 3 · 1 if k odd.
(24)
Then we have the following result, whose proof can be found in §5.2.
Proposition 4 The objective function of the continuous problem (21) can be rewritten as:
M EµSD−1
(fb) + N EµSD−1∩S
(fb) =

b


2
(M cD + N cd cos(φ)), (25)
where φ is the principal angle between b and the subspace S.
As a consequence of this result, when b ∈ S
D−1
, the first term (the outlier term) of the objective
function becomes a constant (M cD) and hence the outliers do not affect the optimal solution of (21).
Moreover, the second term (the inlier term) of the objective function depends only on the cosine of
the principal angle between b and the subspace, which is minimized when b is orthogonal to the
subspace (φ = π/2). This leads to the following result about the continuous problem, whose proof
can be found in §5.3.
Theorem 5 Any global solution to problem (21) must be orthogonal to S.
Observe that this result is true irrespective of the weight M of the outlier term or the weight N of
the inlier term in the continuous objective function (25). Similarly, the next result, whose proof can
be found in §5.4, shows that the solutions to the continuous recursion in (22) converge to a vector
orthogonal to the inlier subspace in a finite number of steps, regardless of the outlier and inlier
weights M, N.
11
TSAKIRIS AND VIDAL
Theorem 6 Consider the sequence {nk}k≥0
generated by recursion (22), with nˆ 0 ∈ S
D−1
. Let φ0
be the principal angle of n0 from S, and define α := N cd/M cD. Then, as long as φ0 > 0, the
sequence {nk}k≥0
converges to a unit `2-norm element of S
⊥ in a finite number k
∗ of iterations,
where k
∗ = 0 if φ0 = π/2, k
∗ = 1 if tan(φ0) ≥ 1/α, and k
∗ ≤

tan−1
(1/α)−φ0
sin−1
(α sin(φ0))
+ 1 otherwise.
Notice the remarkable fact that according to Theorem 6, the continuous recursion (22) converges to
a vector orthogonal to the inlier subspace S in a finite number of steps. Moreover, if the relation
tan(φ0) ≥ 1/α =
M
N
cD
cd
, (26)
holds true, then this convergence occurs in a single step. One way to interpret (26) is to notice that
as long as the angle φ0 of the initial estimate nˆ 0 from the inlier subspace is positive, and for any
arbitrary but fixed number of outliers M, there is always a sufficiently large number N of inliers,
such that (26) is satisfied and thus convergence occurs in one step. Likewise, condition (26) can also
be satisfied if d/D is sufficiently small (so that cD/cd is small). Conversely, for any fixed number
of inliers N and outliers M, there is always a sufficiently large angle φ0 such that (26) is true, and
thus (22) again converges in a single step. More generally, even when (26) is not true, the larger
φ0, N are, the smaller the quantity

tan−1
(1/α) − φ0
sin−1
(α sin(φ0)) 
(27)
is, and thus according to Theorem 5 the faster (22) converges.
4.2 Discrepancy bounds between the continuous and discrete problems
The heart of our analysis framework is to bound the deviation of some underlying geometric quantities, which we call the average outlier and the average inlier with respect to b, from their continuous
counterparts. To begin with, recall our discrete objective function
Jdiscrete(b) =

X˜
>
b


1
=

O>b


1
+

X
>b


1
(28)
and its continuous counterpart
Jcontinuous(b) =

b


2
(M cD + N cd cos(φ)). (29)
Now, notice that the term of the discrete objective that depends on the outliers O can be written as

O>b


1
=
X
M
j=1
|o
>
j b| =
X
M
j=1
b
> Sign(o
>
j b)oj = M b
> ob, (30)
where Sign(·) is the sign function and ob is the average outlier with respect to b, defined as
ob :=
1
M
X
M
j=1
Sign(b
>oj )oj . (31)
12
DUAL PRINCIPAL COMPONENT PURSUIT
Defining a vector valued function fb
: S
D−1 → R
D by z ∈ S
D−1
fb
7−→ Sign(b
>z)z, we notice that
ob =
1
M
X
M
j=1
fb
(oj ) = 1
M
X
M
j=1
Z
z∈SD−1
fb(z)δ(z − oj )dµSD−1 =
Z
z∈SD−1
fb(z)dµO(z), (32)
where µO(z) is defined in (11), and so ob is a discrete approximation to the continuous integral
R
z∈SD−1 fb
(z)dµSD−1 , whose value is given by the next Lemma (see §5.5 for the proof).
Lemma 7 Recall the definition of cD in (23). For any b ∈ S
D−1 we have
Z
z∈SD−1
fb
(z)dµSD−1 =
Z
z∈SD−1
Sign(b
>z)zdµSD−1 = cD b. (33)
In other words, the continuous average outlier with respect to b is cDb. We define O,M to be the
maximum error between the discrete and continuous average outliers as b varies on S
D−1
, i.e.,
O,M := max
b∈SD−1

cD b − ob


2
, (34)
and we establish that the more uniformly distributed O = [o1, . . . , oM] ⊂ S
D−1
is the smaller O,M
becomes. The notion of uniformity of O that we use here is a deterministic one and is captured by
the spherical cap discrepancy of the set O, defined as (Grabner et al., 1997; Grabner and Tichy,
1993)
SD,M (O) := sup
C



1
M
X
M
j=1
IC(oj ) − µSD−1 (C)


. (35)
In (35) the supremum is taken over all spherical caps C of the sphere S
D−1
, where a spherical cap
is the intersection of S
D−1 with a half-space of R
D, and IC(·) is the indicator function of C, which
takes the value 1 inside C and zero otherwise. The spherical cap discrepancy SD,M(O) is precisely
the supremum among all errors in approximating integrals of indicator functions of spherical caps
via averages of such indicator functions on the point set O. Intuitively, SD,M(O) captures how
close the discrete measure µO (see equation (11)) associated with O is to the measure µSD−1 . We
will say that O is uniformly distributed on S
D−1
if SD,M(O) is small. We note here that as a
function of the number of points M, SD,M(O) decreases with a rate of (Dick, 2014; Beck, 1984)
p
log(M)M
− 1
2 − 1
2(D−1)
. (36)
As a consequence, to show that uniformly distributed points O correspond to small O,M, it suffices to bound the maximum integration error O,M from above by a quantity proportional to the
spherical cap discrepancy SD,M(O). Inequalities that bound from above the approximation error
of the integral of a function in terms of the variation of the function and the discrepancy of a finite
set of points (not necessarily the spherical cap discrepancy; there are several types of discrepancies) are widely known as Koksma-Hlawka inequalites (Kuipers and Niederreiter, 2012; Hlawka,
1971). Even though such inequalities exist and are well-known for integration of functions on the
unit hypercube [0, 1]D (Kuipers and Niederreiter, 2012; Hlawka, 1971; Harman, 2010), similar inequalities for integration of functions on the unit sphere S
D−1
seem not to be known in general
13
TSAKIRIS AND VIDAL
(Grabner and Tichy, 1993), except if one makes additional assumptions on the distribution of the
finite set of points (Grabner et al., 1997; Brauchart and Grabner, 2015). Nevertheless, the function
fb : z 7−→ |b
>z| that is associated with O,M is simple enough to allow for a Koksma-Hlawka
inequality of its own, as described in the next lemma, whose proof can be found in §5.6.7
Lemma 8 Let O = [o1, . . . , oM] be a finite subset of S
D−1
. Then
O,M = max
b∈SD−1

cDb − ob


2
≤
√
5SD,M (O), (37)
where cD, ob and SD,M (O) are defined in (23), (31) and (35) respectively.
We now turn our attention to the inlier term

X˜
>
b


1
of the discrete objective function (28),
which is slightly more complicated than the outlier term. We have

X
>b


1
=
X
N
j=1

x
>
j b

 =
X
N
j=1
b
> Sign(x
>
j b)xj = N b
> xb, (38)
where
xb :=
1
N
X
N
j=1
Sign(b
>xj )xj =
1
N
X
N
j=1
fb
(xj ) = Z
x∈SD−1∩S
fb(x)dµX (x) (39)
is the average inlier with respect to b. Thus, xb is a discrete approximation of the integral
Z
x∈SD−1∩S
fb
(x)dµSD−1 , (40)
whose value is given by the next lemma (see §5.7 for the proof).
Lemma 9 For any b ∈ S
D−1 we have
Z
x∈SD−1∩S
fb
(x)dµSD−1 =
Z
x∈SD−1∩S
Sign(b
>x)xdµSD−1 = cd vˆ, (41)
where cd is given by (23) after replacing D with d, and v is the orthogonal projection of b onto S.
In other words, the continuous average inlier with respect to b is cdvˆ. We define X to be the
maximum error between the discrete and continuous average inliers as b varies on S
D−1
, which is
the same as the maximum error as b varies on S
D−1 ∩ S, i.e.,
X ,N := max
b∈SD−1

cd π\S(b) − xb


2
= max
b∈SD−1∩S

cd b − xb


2
. (42)
Then an almost identical argument as the one that established Lemma 8 gives that
X ,N ≤
√
5Sd,N (X ), (43)
where now the discrepancy Sd,N (X ) of the inliers X is defined exactly as in (35) except that M is
replaced by N and the supremum is taken over all spherical caps of S
D−1 ∩ S ∼= S
d−1
.
7. The authors are grateful to Prof. Glyn Harman for pointing out that such a result is possible as well as suggesting
how to prove it.
14
DUAL PRINCIPAL COMPONENT PURSUIT
4.3 Conditions for global optimality and convergence of the discrete problems
In this section we analyze the discrete problem (9) and the associated discrete recursion (10), where
the adjective discrete refers to the fact that (9) and (10) depend on a finite set of points X˜ = [X O]Γ
sampled from the union of the space of outliers S
D−1
and the space of inliers S
D−1 ∩ S. In §4.1
we showed that these two problems are discrete versions of the continuous problems (21) and (22),
respectively. We further showed that the continuous problems possess the geometric property of
interest, i.e., every global minimizer of (21) must be an element of S
⊥ ∩ S
D−1
(Theorem 5) and
the recursion (22) produces a sequence of vectors that converges in a finite number of steps to an
element of S
⊥ ∩ S
D−1
(Theorem 6). In this section we use the discrepancy bounds of §4.2 to show
that under some conditions on the uniformity of X = [x1, . . . , xN ] and O = [o1, . . . , oM], a
similar statement holds for problems (9) and (10). We start with a definition.
Definition 10 Given a set Y = [y1
, . . . , yL] ⊂ S
D−1 and an integer K ≤ L, define RY,K to be
the maximum circumradius among all polytopes of the form
(X
K
i=1
αjiyji
: αji ∈ [−1, 1])
, (44)
where j1, . . . , jK are distinct integers in [L], and the circumradius of a bounded subset of R
D is the
infimum over the radii of all Euclidean balls of R
D that contain that subset. With that, define
RO,X := max
K1+K2<D, K2<d
(RO,K1 + RX ,K2
). (45)
The next theorem, proved in §5.8, states that if both inliers and outliers are sufficiently uniformly
distributed, i.e., if the uniformity parameters X ,N and O,M are sufficiently small, then every global
solution of (9) must be orthogonal to the inlier subspace S. More precisely,
Theorem 11 Suppose that the ratio γ of outliers to inliers satisfies
γ :=
M
N
<
1
2O,M
min 
cd − X ,N , 2

cd − X ,N −
RO,X
N

, Γ

, where (46)
Γ :=
cd − X ,N
2(3cd − X ,N )
"r
P2 + 8(3cd − X ,N )(cd − X ,N −
RO,X
N
) − P
#
, (47)
P := 2
RO,X
N
+ X ,N + cd. (48)
Then any global solution b
∗
to (9) must be orthogonal to Span(X ).
Towards interpreting Theorem 11, consider first the asymptotic case where we allow N and
M to go to infinity, while keeping the ratio γ constant. Assuming that both inliers and outliers
are perfectly well distributed in the limit, i.e., under the hypothesis that limN→∞ Sd,N (X ) = 0
and limM→∞ SD,M (O) = 0, Lemma 8 and inequality (43) give that limN→∞ X ,N = 0 and
limM→∞ O,M = 0, in which case (46) is satisfied. This suggests the interesting fact that (9)
can possibly give a normal to the inliers even for arbitrarily many outliers, and irrespectively of
the subspace dimension d. Along the same lines, for a given γ and under the point set uniformity
hypothesis, we can always increase the number of inliers and outliers (thus decreasing X ,N and
15
TSAKIRIS AND VIDAL
O,M), while keeping γ constant, until (46) is satisfied, once again indicating that (9) can possibly
yield a normal to the space of inliers irrespectively of their intrinsic dimension; this becomes evident
in the numerical evaluation of Figs. 4(a)-4(c). Notice that the intrinsic dimension d of the inliers
manifests itself through the quantity cd, which we recall is a decreasing function of d. Consequently,
the smaller d is the larger the RHS of (46) becomes, and so the easier it is to satisfy (46).
More explicitly (and less formally), because of (36) the quantities O,M, X ,N decay at an approximate rate of 1/
√
M, 1/
√
N respectively. In turn, this shows that the conditions (46) are satisfied if roughly M < O(N2
). To see this, note, e.g., that the first inequality in (46) reads
M
N
<
cd − X ,N
2O,M
, (49)
which roughly says that
constant ·
√
M ≤ constant · N − constant ·
√
N, (50)
where by constant here we mean independent of M, N. A similar conclusion can be drawn from
the rest inequalities in (46) 8
. In contrast, the analysis of the haystack model of REAPER (Lerman
et al., 2015) gives M < O(N).
A similar phenomenon holds for the recursion of convex relaxations (10). Notice that according
to Theorem 5, the continuous recursion converges in a finite number of iterations to a vector that
is orthogonal to Span(X ) = S, as long as the initialization nˆ 0 does not lie in S (equivalently
φ0 > 0). Intuitively, one should expect that in the discrete case, the conditions for the discrete
recursion (10) to be successful, should be at least as strong as the conditions of Theorem 11, and
strictly stronger than the condition φ0 > 0 of Theorem 6. Our next result, whose proof can be found
in §5.9, formalizes this intuition.
Theorem 12 Suppose that condition (46) holds true and consider the sequence {nk}k≥0
generated
by the recursion (10). Let φ0 be the principal angle of nˆ 0 from Span(X ) and suppose that
cos(φ0) <
cd − X ,N
2cd(cd + X ,N )
h
−Q +
p
Q2 + 4cd(cd − Q)
i
−
2O,M
cd + X ,N
M
N
, (51)
Q :=
RO,X
N
+ O,M
M
N
+ X ,N . (52)
Then after a finite number of iterations the sequence {nk}k≥0
converges to a unit `2-norm vector
that is orthogonal to Span(X ).
First note that if (46) is true, then the expression of (51) always defines an angle between 0 and
π/2. Moreover, Theorem 12 can be interpreted using the same asymptotic arguments as Theorem
11; notice in particular that the lower bound on the angle φ0 tends to zero as M, N go to infinity with
γ constant, i.e., the more uniformly distributed inliers and outliers are, the closer n0 is allowed to be
to Span(X ). We also emphasize that Theorem 12 asserts the correctness of the linear programming
recursions (10) as far as recovering a vector nk
∗ orthogonal to S := Span(X ) is concerned. Even
though this was our initial motivation for posing problem (9), Theorem 12 does not assert in general
that nk
∗ is a global minimizer of problem (9). However, this is indeed the case, when the inlier
subspace S is a hyperplane, i.e., d = D − 1. This is because, up to a sign, there is a unique vector
b ∈ S
D−1
that is orthogonal to S (the normal vector to the hyperplane), which, under conditions
(46) and (51), is the unique global minimizer of (9), as well as the limit point nk
∗ of Theorem 12.
8. It is the subject of ongoing research to arrive at this conclusion by more formal means.
16
DUAL PRINCIPAL COMPONENT PURSUIT
5. Proofs
In this section we provide the proofs of all claims stated in earlier sections.
5.1 Proof of Proposition 3
By the general position hypothesis on X and O, any hyperplane that does not contain X can contain
at most D − 1 points from X˜. We will show that there exists a hyperplane that contains more than
D−1 points of X˜. Indeed, take d inliers and D−d−1 outliers and let H be the hyperplane generated
by these D −1 points. Denote the normal vector to that hyperplane by b. Since H contains d inliers,
b will be orthogonal to these inliers. Since X is in general position, every d-tuple of inliers is a
basis for Span(X ). As a consequence, b will be orthogonal to Span(X ), and in particular b ⊥ X .
This implies that X ⊂ H and so H will contain N + D − d − 1 ≥ d + 1 + D − d − 1 > D − 1
points of X˜.
5.2 Proof of Proposition 4
Writing b =

b


2
ˆb, and letting R be a rotation that takes ˆb to the first standard basis vector e1, we
see that the first expectation in the LHS of (25) becomes equal to
EµSD−1
(fb) = Z
z∈SD−1
fb(z)dµSD−1 =
Z
z∈SD−1



b
>z


 dµSD−1 (53)
=

b


2
Z
z∈SD−1



ˆb
>
z


 dµSD−1 =

b


2
Z
z∈SD−1



z
>R−1Rˆb


 dµSD−1 (54)
=

b


2
Z
z∈SD−1
|z
>e1|dµSD−1 =

b


2
Z
z∈SD−1
|z1|dµSD−1 =

b


2
cD, (55)
where z = (z1, . . . , zD)
> is the coordinate representation of z. To see what the second expectation
in the LHS of (25) evaluates to, decompose b as b = πS(b) + πS⊥ (b), and note that because the
support of the measure µSD−1∩S is contained in S, we must have that
EµSD−1∩S
(fb) = Z
z∈SD−1



b
>z


 dµSD−1∩S =
Z
z∈SD−1∩S



b
>z


 dµSD−1∩S (56)
=
Z
z∈SD−1∩S


(πS(b))>
z


 dµSD−1∩S (57)
=

πS(b)


2
Z
z∈SD−1∩S





π\S(b)
>
z




dµSD−1∩S. (58)
Writing z
0
and b
0
for the coordinate representation of z and π\S(b) with respect to a basis of S, and
noting that µSD−1∩S ∼= µS
d−1 , we have that
Z
z∈SD−1∩S





π\S(b)
>
z




dµSD−1∩S =
Z
z0∈S
d−1



z
0>b
0


 dµS
d−1 = cd, (59)
where now cd is the average height of the unit hemisphere of R
d
. Finally, noting that

πS(b)


2
=

b


2
cos(φ), (60)
where φ is the principal angle of b from the subspace S, we have that
EµSD−1∩S
(fb) =

b


2
cd cos(φ). (61)
17
TSAKIRIS AND VIDAL
5.3 Proof of Theorem 5
Because of the constraint b
>b = 1 in (21), and using (25), problem (21) can be written as
min
b
[M cD + N cd cos(φ)] s.t. b
>b = 1. (62)
It is then immediate that the global minimum is equal to M cD and it is attained if and only if
φ = π/2, which corresponds to b ⊥ S.
5.4 Proof of Theorem 6
At iteration k the optimization problem associated with (22) is
min
b∈RD
J (b) =

b


2
(M cD + N cd cos(φ)) s.t. b
>nˆ k = 1, (63)
where φ is the principal angle of b from the subspace S.
Let φk be the principal angle of nˆ k from S, and let nk+1 be a global minimizer of (63), with
principal angle from S equal to φk+1. We show that φk+1 ≥ φk. To see this, note that the decrease
in the objective function at iteration k is
J (nˆ k) − J (nk+1) :=M cD

nˆ k


2
+ N cd

nˆ k


2
cos(φk)
− M cD

nk+1


2
− N cd

nk+1


2
cos(φk+1). (64)
Since n
>
k+1nˆ k = 1, we must have that

nk+1


2
≥ 1 =

nˆ k


2
. Now if φk+1 < φk, then
cos(φk+1) > cos(φk). But then (64) implies that J (nk+1) > J (nˆ k), which is a contradiction
on the optimality of nk+1. Hence it must be the case that φk+1 ≥ φk, and so the sequence {φk}k
is non-decreasing. In particular, since φ0 > 0 by hypothesis, we must also have φk > 0, i.e.,
nˆ k 6∈ S, ∀k ≥ 0.
Letting ψk be the angle of b from nˆ k, the constraint b
>nˆ k = 1 gives 0 ≤ ψk < π/2 and

b


2
= 1/ cos(ψk), and so we can write the optimization problem (63) equivalently as
min
b∈RD
M cD + N cd cos(φ)
cos(ψk)
s.t. b
>nˆ k = 1. (65)
If nˆ k is orthogonal to S, i.e., φk = π/2, then J (nˆ k) = M cD ≤ J (b), ∀b : b
>nˆ k = 1, with
equality only if b = nˆ k. As a consequence, nk
0 = nˆ k, ∀k
0 > k, and in particular if φ0 = π/2, then
k
∗ = 0.
So suppose that φk < π/2 and let nˆ
⊥
k be the normalized orthogonal projection of nˆ k onto S
⊥.
We will prove that every global minimizer of problem (65) must lie in the two-dimensional plane
H := Span(nˆ k, nˆ
⊥
k
). To see this, let b have norm 1/ cos(ψk) for some ψk < π/2. If ψk > π/2 −
φk, then such a b can not be a global minimizer of (65), as the feasible vector nˆ
⊥
k / sin(φk) ∈ H
already gives a smaller objective, since
J (nˆ
⊥
k / sin(φk)) = M cD
sin(φk)
=
M cD
cos(π/2 − φk)
<
M cD + N cd cos(φ)
cos(ψk)
= J (b). (66)
Thus, without loss of generality, we may restrict to the case where ψk ≤ π/2 − φk. Denote by hˆ
k
the normalized projection of nˆ k onto S and by nˆ
†
the vector that is obtained from nˆ k by rotating it
18
DUAL PRINCIPAL COMPONENT PURSUIT
towards nˆ
⊥
k by ψk. Note that both hˆ
k and nˆ
†
k
lie in H. Letting Ψk ∈ [0, π] be the spherical angle
between the spherical arc formed by nˆ k,
ˆb and the spherical arc formed by nˆ k, hˆ
k, the spherical law
of cosines gives
cos(∠b, hˆ
k) = cos(φk) cos(ψk) + sin(φk) sin(ψk) cos(Ψk). (67)
Now, Ψk is equal to π if and only if nˆ k, hˆ
k, b are coplanar, i.e., if and only if b ∈ H. Suppose that
b 6∈ H. Then Ψk < π, and so cos(Ψk) > −1, which implies that
cos(∠b, hˆ
k) > cos(φk) cos(ψk) − sin(φk) sin(ψk) = cos(φk + ψk). (68)
This in turn implies that the principal angle φ of b from S is strictly smaller than φk + ψk, and so
J (b) = M cD + N cd cos(φ)
cos(ψk)
>
M cD + N cd cos(φk + ψk)
cos(ψk)
= J (nˆ
†
k
/ cos(ψk)), (69)
i.e., the feasible vector nˆ
†
k
/ cos(ψk) ∈ H gives strictly smaller objective than b.
To summarize, for the case where φk < π/2, we have shown that any global minimizer b of
(65) must i) have angle ψk from nˆ k less or equal to π/2 − φk, and ii) it must lie in Span(nˆ k, nˆ
⊥
k
).
Hence, we can rewrite (65) in the equivalent form
min
ψ∈[−π/2+φk,π/2−φk]
Jk(ψ) := M cD + N cd cos(φk + ψ)
cos(ψk)
, (70)
where now ψk takes positive values as b approaches nˆ
⊥
k
and negative values as it approaches hˆ
k.
The function Jk is continuous and differentiable in the interval [−π/2+φk, π/2−φk], with derivative given by
∂Jk
∂ψ =
M cD sin(ψ) − N cd sin(φk)
cos2(ψ)
. (71)
Setting the derivative to zero gives
sin(ψ) = α sin(φk). (72)
If α sin(φk) ≥ sin(π/2 − φk) = cos(φk), or equivalently tan(φk) ≥ 1/α, then Jk is strictly
decreasing in the interval [−π/2 + φk, π/2 − φk], and so it must attain its minimum precisely at
ψ = π/2−φk, which corresponds to the choice nk+1 = nˆ
⊥
k / sin(φk). Then by an earlier argument
we must have that nˆ k
0 ⊥ S, ∀k
0 ≥ k + 1. If, on the other hand, tan(φk) < 1/α, then the equation
(72) defines an angle
ψ
∗
k
:= sin−1
(α sin(φk)) ∈ (0, π/2 − φk), (73)
at which Jk must attain its global minimum, since
∂
2Jk
∂ψ2
(ψ
∗
k
) = 1
cos(ψ
∗
k
)
> 0. (74)
19
TSAKIRIS AND VIDAL
As a consequence, if tan(φk) < 1/α, then
φk+1 = φk + sin−1
(α sin(φk)) < π/2. (75)
We then see inductively that as long as tan(φk) < 1/α, φk increases by a quantity which is bounded
from below by sin−1
(α sin(φ0)). Thus, φk will keep increasing until it becomes greater than the
solution to the equation tan(φ) = 1/α, at which point the global minimizer will be the vector
nk+1 = nˆ
⊥
k / sin(φk), and so nˆ k
0 = nˆ k+1, ∀k
0 ≥ k + 1. Finally, under the hypothesis that
φk < tan−1
(1/α), we have
φk = φ0 +
X
k−1
j=0
sin−1
(α sin(φj )) ≥ φ0 + k sin−1
(α sin(φ0)), (76)
from where it follows that the maximal number of iterations needed for φk to become larger than
tan−1
(1/α) is 
tan−1
(1/α)−φ0
sin−1
(α sin(φ0))
, at which point at most one more iteration will be needed to achieve
orthogonality to S.
5.5 Proof of Lemma 7
Letting R be a rotation that takes b to the first canonical vector e1, i.e., Rb = e1, we have that
Z
z∈SD−1
Sign(b
>z)zdµSD−1 =
Z
z∈SD−1
Sign(b
>R>Rz)zdµSD−1 (77)
=
Z
z∈SD−1
Sign(e
>
1 z)R>zdµSD−1 (78)
= R>
Z
z∈SD−1
Sign(z1)zdµSD−1 , (79)
where z1 is the first cartesian coordinate of z. Recalling the definition of cD in equation (23), we
see that
Z
z∈SD−1
Sign(z1)z1dµSD−1 =
Z
z∈SD−1
|z1| dµSD−1 = cD. (80)
Moreover, for any i > 1, we have
Z
z∈SD−1
Sign(z1)zidµSD−1 = 0. (81)
Consequently, the integral in (79) becomes
Z
z∈SD−1
Sign(b
>z)zdµSD−1 = R>
Z
z∈SD−1
Sign(z1)zdµSD−1 = R> (cDe1) = cDb. (82)
5.6 Proof of Lemma 8
For any b ∈ S
D−1 we can write
cDb − ob = ρ1b + ρ2ζ, (83)
20
DUAL PRINCIPAL COMPONENT PURSUIT
for some vector ζ ∈ S
D−1 orthogonal to b, and so it is enough to show that p
ρ
2
1 + ρ
2
2 ≤
√
5SD,M (O).
Let us first bound from above |ρ1| in terms of SD,M(O). Towards that end, observe that
ρ1 = b
>(cDb − ob) = cD −
1
M
X
M
j=1

b
>oj

 =
Z
z∈SD−1
fb(z)dµSD−1 −
1
M
X
M
j=1
fb(oj ), (84)
where the equality follows from the definition of cD in (23) and recalling that fb(z) =

b
>z


. In
other words, ρ1 is the error in approximating the integral of fb on S
D−1 by the average of fb on the
point set O.
Now, notice that each super-level set 
z ∈ S
D−1
: fb(z) ≥ α
	
for α ∈ [0, 1], is the union of
two spherical caps, and also that
sup
z∈SD−1
fb(z) − inf
z∈SD−1
fb(z) = 1 − 0 = 1. (85)
We these in mind, repeating the entire argument of the proof of Theorem 1 in (Harman, 2010) that
lead to inequality (9) in (Harman, 2010), but now for a measurable function with respect to µSD−1
(that would be fb), leads directly to
|ρ1| ≤ SD,M(O). (86)
For ρ2 we have that
ρ2 = ζ
> (cDb) − ζ
>ob (87)
=
Z
z∈SD−1
Sign 
b
>z

ζ
>zdµSD−1 −
1
M
X
M
j=1
Sign 
b
>oj

ζ
>oj (88)
=
Z
z∈SD−1
gb,ζ(z)dµSD−1 −
1
M
X
M
j=1
gb,ζ(oj ), (89)
where gb,ζ : S
D−1 → R is defined as gb,ζ(z) = Sign
b
>z

ζ
>z. Then a similar argument as for
ρ1, with the difference that now
sup
z∈SD−1
gb,ζ(z) − inf
z∈SD−1
gb,ζ(z) = 1 − (−1) = 2, (90)
leads to
|ρ2| ≤ 2SD,M(O). (91)
In view of (86), inequality (91) establishes that p
ρ
2
1 + ρ
2
2 ≤
√
5SD,M(O), which concludes the
proof of the lemma.
5.7 Proof of Lemma 9
Since x lies in S, we have fb
(x) = fv
(x) = fvˆ
(x), so that
Z
x∈SD−1∩S
Sign(b
>x)xdµSD−1 =
Z
x∈SD−1∩S
Sign(vˆ
>x)xdµSD−1 . (92)
Now express x and vˆ on a basis of S, use Lemma 7 replacing D with d, and then switch back to the
standard basis of R
D.
2 
TSAKIRIS AND VIDAL
5.8 Proof of Theorem 11
To prove the theorem we need the following lemma.
Lemma 13 For any b ∈ S
D−1 we have that
M(cD + O,M) ≥

O>b


1
≥ M(cD − O,M) (93)
N(cd + X ,N ) cos(φ) ≥

X
>b


1
≥ N(cd − X ,N ) cos(φ). (94)
Proof We only prove the second inequality as the first is even simpler. Let v 6= 0 be the orthogonal
projection of b onto S. By definition of X ,N , there exists a vector ξ ∈ S of `2 norm less or equal
to X ,N , such that
xv = xb =
1
N
X
N
j=1
Sign(b
>xj )xj = cdvˆ + ξ. (95)
Taking inner product of both sides with b gives
1
N

X
>b


1
= cd cos(φ) + b
>ξ. (96)
Now, the result follows by noting that

b
>ξ

 ≤ X ,N cos(φ), since the principal angle of b from
Span(ξ) can not be less then φ.
Now, let b
∗
be an optimal solution of (9). Then b
∗ must satisfy the first order optimality relation
0 ∈ λb
∗ + X˜ Sgn(X˜
>
b
∗
), (97)
where λ is a scalar Lagrange multiplier parameter, and Sgn is the sub-differential of the `1 norm.
For the sake of contradiction, suppose that b
∗
6⊥ S. If b
∗ ∈ S, then using Lemma 13 we have
M cD + M O ≥ min
b⊥S,b
>b=1

O>b


1
≥

O>b
∗


1
+

X
>b
∗


1
≥ M cD − M O + N cd − N X , (98)
which violates the first inequality of hypothesis (46). Hence, we can assume that b
∗
6∈ S.
By the general position hypothesis as well as Proposition 14, b
∗ will be orthogonal to precisely
D − 1 points, among which K1 points belong to O, say, without loss of generality, o1, . . . , oK1
,
and 0 ≤ K2 ≤ d − 1 points belong to X , say x1, . . . , xK2
. Then there must exist real numbers
−1 ≤ αj , βj ≤ 1, such that
λb
∗ +
X
K1
j=1
αjoj +
X
M
j=K1+1
Sign(o
>
j b
∗
)oj +
X
K2
j=1
βjxj +
X
N
j=K2+1
Sign(x
>
j b
∗
)xj = 0. (99)
Since Sign(o
>
j
b
∗
) = 0, ∀j ≤ K1 and similarly Sign(x
>
j
b
∗
) = 0, ∀j ≤ K2, we can equivalently
write
λb
∗ +
X
K1
j=1
αjoj +
X
M
j=1
Sign(o
>
j b
∗
)oj +
X
K2
j=1
βjxj +
X
N
j=1
Sign(x
>
j b
∗
)xj = 0 (100)
22
DUAL PRINCIPAL COMPONENT PURSUIT
or more compactly
λb
∗ + ξO + M ob
∗ + ξX + N xvˆ
∗ = 0, (101)
where vˆ
∗
is the normalized projection of b
∗
onto S (nonzero since b
∗
6⊥ S by hypothesis), and
ob
∗ :=
1
M
X
M
j=1
Sign(o
>
j b
∗
)oj , xvˆ
∗ :=
1
N
X
N
j=1
Sign(x
>
j vˆ
∗
)xj , (102)
ξO := X
K1
j=1
αjoj , ξX := X
K2
j=1
βjxj . (103)
From the definitions of O,M and X ,N in (34) and (42) respectively, we have that
ob
∗ = cD b
∗ + ηO, ||ηO||2 ≤ O,M (104)
xvˆ
∗ = cd vˆ
∗ + ηX , ||ηX ||2 ≤ X ,N , (105)
and so (101) becomes
λb
∗ + ξO + M cD b
∗ + M ηO + ξX + N cd vˆ
∗ + N ηX = 0. (106)
Since b
∗
6∈ S, we have that b
∗
, vˆ
∗
are linearly independent. Define the two-dimensional subspace
U := Span (b
∗
, vˆ
∗
) and project (106) onto U to get
λb
∗ + πU(ξO) + M cD b
∗ + M πU(ηO) + πU(ξX ) + N cd vˆ
∗ + N πU(ηX ) = 0. (107)
Now, very vector u in the image of πU can be written as a linear combination of b
∗
and vˆ
∗
:
u = [u]b
∗ b
∗ + [u]vˆ
∗ vˆ
∗
, with [u]b
∗ , [u]vˆ
∗ ∈ R. (108)
Taking inner product of u with b
∗
and vˆ
∗
, we get respectively
u
>b
∗ = [u]b
∗ + [u]vˆ
∗ cos(φ
∗
) (109)
u
>vˆ
∗ = [u]b
∗ cos(φ
∗
) + [u]vˆ
∗ , (110)
where φ
∗
is the angle between b
∗
and vˆ
∗
, i.e., the angle of b
∗
from S. Solving with respect to [u]vˆ
∗ ,
we obtain
[u]vˆ
∗ =
u
>vˆ
∗ − u
>b
∗
cos(φ
∗
)
1 − cos2(φ∗)
, (111)
which in turn gives an upper bound on the magnitude of [u]vˆ
∗ :
|[u]vˆ
∗ | ≤ 1 + cos(φ
∗
)
1 − cos2(φ∗)
kuk2
. (112)
Going back to (107) and writing each vector as a linear combination of b
∗
and vˆ
∗
, we obtain
λb
∗ + [πU (ξO)]b
∗ b
∗ + [πU (ξO)]vˆ
∗ vˆ
∗ + M cD b
∗ + M [πU (ηO)]b
∗ b
∗ + M [πU (ηO)]vˆ
∗ vˆ
∗+
[πU (ξX )]b
∗ b
∗ + [πU (ξX )]vˆ
∗ vˆ
∗ + N cd vˆ
∗ + N [πU (ηX )]b
∗ b
∗ + N [πU (ηX )]vˆ
∗ vˆ
∗ = 0. (113)
23
TSAKIRIS AND VIDAL
Since U is a two-dimensional space, there exists a vector ˆζ ∈ U that is orthogonal to b
∗
but not
orthogonal to vˆ
∗
. Projecting the above equation onto the line spanned by ˆζ, we obtain the onedimensional equation
([πU (ξO)]vˆ
∗ + M [πU (ηO)]vˆ
∗ + [πU (ξX )]vˆ
∗ + N cd + N [πU (ηX )]vˆ
∗ ) ·
ˆζ
>
vˆ
∗ = 0. (114)
Since ˆζ is not orthogonal to vˆ
∗
, the above equation implies that
[πU(ξO)]vˆ
∗ + M [πU(ηO)]vˆ
∗ + [πU(ξX )]vˆ
∗ + N cd + N [πU(ηX )]vˆ
∗ = 0, (115)
which, in turn, implies that
N cd ≤ |[πU(ξO)]vˆ
∗ | + M |[πU(ηO)]vˆ
∗ | + |[πU(ξX )]vˆ
∗ | + N |[πU(ηX )]vˆ
∗ | . (116)
Invoking the upper bound of (112) together with

ξO


2
≤ RO,K1
,

ξX


2
≤ RX ,K2
,

ηO


2
≤ O,M,

ηX


2
≤ X ,N , (117)
and the definition of RO,X (Definition 10), we get
N cd ≤
1 + cos(φ
∗
)
1 − cos2(φ∗)
(RO,X + M O,M + N X ,N ), (118)
or equivalently
N cd cos2
(φ
∗
) + (RO,X + M O,M + N X ,N ) cos(φ
∗
)
+ (RO,X + M O,M + N X ,N − N cd) ≥ 0. (119)
This is a quadratic polynomial in cos(φ
∗
), whose constant term is negative by the second inequality
of hypothesis (46), and thus has exactly one positive and one negative root. As consequence, this
polynomial being non-negative together with the fact that cos(φ
∗
) > 0, implies that cos(φ
∗
) must
be greater than the positive root of the polynomial, i.e.,
cos(φ
∗
) ≥
−Q +
p
Q2 + 4cd(cd − Q)
2cd
, Q :=
RO,X
N
+ O,M
M
N
+ X ,N . (120)
On the other hand, by Lemma 13 we have
M(cD + O,M) ≥ min
bˆ⊥S



X˜
>ˆb



1
≥



X˜
>
b
∗



1
≥ M(cD − O,M) + N(cd − X,N ) cos(φ
∗
), (121)
which implies that
2M O,M ≥ N(cd − X ,N )
−Q +
p
Q2 + 4cd(cd − Q)
2cd
. (122)
This latter inequality is equivalent to the inequality
2
2
O,M(3cd − X ,N )

M
N
2
+ O,M(cd − X ,N )

2
RO,X
N
+ X ,N + cd

M
N
− (cd − X ,N )
2

cd − X ,N −
RO,X
N

≥ 0, (123)
24
DUAL PRINCIPAL COMPONENT PURSUIT
whose left-hand-side we view as quadratic polynomial in M/N. By the first two inequalities of
hypothesis (46), the second term of this polynomial is positive, while the constant term is negative,
and so this inequality is equivalent to M/N being greater or equal than the unique positive root
of that polynomial. But this contradicts the third inequality of hypothesis (46). Consequently, the
initial hypothesis of the proof that b
∗
6⊥ S can not be true, and the theorem is proved.
A Geometric View of the Proof of Theorem 11. Let us provide some geometric intuition that
underlies the proof of Theorem 11. It is instructive to begin our discussion by considering the case
d = 1, D = 2, i.e. the inlier space is simply a line and the ambient space is a 2-dimensional plane.
Since all points have unit `2-norm, every column of X will be of the form xˆ or −xˆ for a fixed
vector xˆ ∈ S1
that spans the inlier space S. In this setting, let us examine a global solution b
∗
of
the optimization problem (9). We will start by assuming that such a b
∗
is not orthogonal to S, and
intuitively arrive at the conclusion that this can not be the case as long as there are sufficiently many
inliers.
We will argue on an intuitive level that if b
∗
6⊥ S, then the principal angle φ
∗ of b
∗
from S
needs to be small; this is captured precisely by (120) in the proof of the theorem. To see this,
suppose b
∗
6⊥ S; then b
∗ will be non-orthogonal to every inlier, and by Proposition14 orthogonal to
D − 1 = 1 outlier, say o1. The optimality condition (97) specializes to
α1o1 +
X
M
j=1
Sign(o
>
j b
∗
)oj
| {z }
Mob∗
+
X
N
j=1
Sign(x
>
j b
∗
)xj + λb
∗ = 0, (124)
where −1 ≤ α1 ≤ 1. Notice that the third term is simply N Sign(xˆ
>b
∗
)xˆ, and so
α1o1 + M ob
∗ + λb
∗ = −N Sign(xˆ
>b
∗
)xˆ. (125)
Now, what (125) is saying is that the point −N Sign(xˆ
>b
∗
)xˆ must lie inside the set
Conv(±o1) + {Mob
∗ } + Span(b
∗
) = {α1o1 + Mob
∗ + λb
∗
: |α1| ≤ 1, λ ∈ R} , (126)
where the + operator on sets is the Minkowski sum. Notice that the set Conv(±o1) + Mob
∗ is the
translation of the line segment (polytope) Conv(±o1) by Mob
∗ . Then (125) says that if we draw
all affine lines that originate from every point of Conv(±o1) + Mob
∗ and have direction b
∗
, then
one of these lines must meet the point −N Sign(xˆ
>b
∗
)xˆ. Let us illustrate this for the case where
M = N = 5 and say it so happens that b
∗
has a rather large angle φ
∗
from S, say φ
∗ = 45◦
. Recall
that ob
∗ is concentrated around cD b
∗
and for the case D = 2 we have cD =
2
π
. As illustrated in
Figure 1, because φ
∗
is large, the unbounded polytope Mob
∗ + Conv(±o1) + Span(b
∗
) misses the
point −N Sign(xˆ
>b
∗
)xˆ thus making the optimality equation (125) infeasible. This indicates that
critical vectors b
∗
6⊥ S having large angles from S are unlikely to exist.
On the other hand, critical points b
∗
6⊥ S may exist, but their angle φ
∗
from S needs to be small,
as illustrated in Figure 2. However, such critical points can not be global minimizers, because small
angles from S yield large objective values; this is captured precisely by equation (121) in the proof
of the theorem. Hence the only possibility that critical points b
∗
6⊥ S that are also global minimizers
do exist is that the number of inliers is significantly less than the number of outliers, i.e. N << M,
as illustrated in Figure 3. The precise notion of how many inliers should exist with respect to outliers
is captured by condition (46) of Theorem 11.
25
TSAKIRIS AND VIDAL
Mob
∗ + Conv(±o1) + Span(b
∗
)
S −N Sign(xˆ
>b
∗
)xˆ
Mob
∗ + Conv(±o1)
Mob
∗
b
∗
φ
∗
o1
Figure 1: Geometry of the optimality condition (97) and (125) for the case d = 1, D = 2, M =
N = 5. The polytope Mob
∗ + Conv(±o1) + Span(b
∗
) misses the point −N Sign(xˆ
>b
∗
)xˆ and so
the optimality condition can not be true for both b
∗
6⊥ S = Span(xˆ) and φ
∗
large.
Mob
∗ + Conv(±o1) + Span(b
∗
)
S −N Sign(xˆ
>b
∗
)xˆ
Mob
∗ + Conv(±o1)
Mob
∗ b
∗
o1
Figure 2: Geometry of the optimality condition (97) and (125) for the case d = 1, D = 2, M =
N = 5. A critical b
∗
6⊥ S exists, but its angle from S is small, so that the polytope Mob
∗ +
Conv(±o1) + Span(b
∗
) can contain the point −N Sign(xˆ
>b
∗
)xˆ. However, b
∗
can not be a global
minimizer, since small angles from S yield large objective values.
We should note here that the picture for the general setting is analogous to what we described
above, albeit harder to visualize: with reference to equation (100), the optimality condition says that
every feasible point b
∗
6⊥ S must have the following property: there exist 0 ≤ K2 ≤ d − 1 inliers
x1, . . . , xK2
and 0 ≤ K1 ≤ D − 1 − K2 outliers o1, . . . , oK1
to which b
∗
is orthogonal, and two
points ξO ∈ Conv(±o1 ± · · · ± oK1
) + ob
∗ and ξX ∈ Conv(±x1 ± · · · ± xK2
) + xb
∗ that are
joined by an affine line that is parallel to the line spanned by b
∗
. In fact in our proof of Theorem 11
we reduced this general case to the case d = 1, D = 2 described above: this reduction is precisely
taking place in equation (107), where we project the optimality equation onto the 2-dimensional
subspace U. The arguments that follow this projection consist of nothing more than a technical
treatment of the intuition given above.
5.9 Proof of Theorem 12
First note that if (46) is true, then the expression of (51) always defines an angle between 0 and π/2.
We start by establishing that nˆ k does not lie in the inlier space S. For k = 0 this is true by the
26
DUAL PRINCIPAL COMPONENT PURSUIT
Mob
∗ + Conv(±o1) + Span(b
∗
)
−N Sign(xˆ
>b
∗
)xˆ
Mob
∗ + Conv(±o1)
Mob
∗
b o ∗
1
Figure 3: Geometry of the optimality condition (97) and (125) for the case d = 1, D = 2, N << M.
Critical points b
∗
6⊥ S do exist and moreover they can have large angle from S. This is because N is
small and so the polytope Mob
∗ + Conv(±o1) + Span(b
∗
) contains the point −N Sign(xˆ
>b
∗
)xˆ.
Moreover, such critical points can be global minimizers. Condition (46) of Theorem 11 prevents
such cases from occuring.
hypothesis (51). For the sake of contradiction suppose that nˆ k ∈ S for some k > 0. Note that

X˜
>
nˆ 0


1
≥

X˜
>
n1


1
≥

X˜
>
nˆ 1


1
≥ · · · ≥

X˜
>
nˆ k


1
. (127)
Suppose first that nˆ 0 ⊥ S. Then (127) gives

O>nˆ 0


1
≥

O>nˆ k


1
+

X
>vˆk


1
, (128)
where vˆk is the normalized projection of nˆ k onto S (and since nˆ k ∈ S, these two are equal). Using
Lemma 13, we take an upper bound of the LHS and a lower bound of the RHS of (128), and obtain
M cD + M O,M ≥ M cD − M O,M + N cd − N X ,N , (129)
or equivalently
M
N
≥
cd − X ,N
2 O,M
, (130)
which contradicts the first inequality of (46). Consequently, nˆ 0 6⊥ S. Then (127) implies that

O>nˆ 0


1
+

X
>nˆ 0


1
≥

O>nˆ k


1
+

X
>nˆ k


1
, (131)
or equivalently

O>nˆ 0


1
+ cos(φ0)

X
>vˆ0


1
≥

O>nˆ k


1
+

X
>vˆk


1
, (132)
where vˆ0 is the normalized projection of nˆ 0 onto S. Once again, using Lemma 13 we obtain the
following contradiction to (51):
M cD + M O,M + (N cd + N X ,N ) cos(φ0) ≥ M cD − M O,M + N cd − N X ,N . (133)
27
TSAKIRIS AND VIDAL
Now let us complete the proof of the theorem. We know by Proposition 16 that the sequence {nk}
converges to a critical point nk
∗ of problem (9) in a finite number of steps k
∗
, and we have already
shown that nk
∗ 6∈ S. If nk
∗ is not orthogonal to S, an identical argument as in the proof of Theorem
11 (with nk
∗ in place of b
∗
) shows that the principal angle φk
∗ of nk
∗ from S satisfies
cos(φk
∗ ) ≥
−Q +
p
Q2 + 4cd(cd − Q)
2cd
, Q :=
RO,X
N
+ O,M
M
N
+ X ,N . (134)
However, due to (127) and Lemma 13, we have that
M(cD + O,M) + N(cd + X ,N ) cos(φ0) ≥ M(cD − O,M) + N(cd − X ,N ) cos(φk
∗ ), (135)
which, after substituting the lower bound (134), contradicts (51). Thus nk
∗ ⊥ S.
6. Dual Principal Component Pursuit Algorithms
We present algorithms based on the ideas discussed so far, for estimating the inlier linear subspace
in the presence of outliers. Specifically, in §6.1 we describe the main algorithmic contribution of
this paper, which is based on the implementation of the recursion (10) via linear programming. In
§6.2 we propose an alternative way of computing dual principal components based on Iteratively
Reweighted Least-Squares, which, as will be seen in §7 performs almost as well as recursion (6.1),
yet it is significantly more efficient. Finally, in §6.3 we present a variation of the DPCP optimization
problem (9) suitable for noisy data and propose a heuristic method for solving it.
6.1 DPCP via Linear Programming (DPCP-LP)
For the sake of an argument, suppose that there is no noise in the inliers, i.e., the inliers X span
a linear subspace S of dimension d. Then Theorem 12 suggests a mechanism for obtaining an
element b1 of S
⊥: run the recursion of linear programs (10) until the sequence nˆ k converges and
identify the limit point with b1. Due to computational constraints, in practice one usually terminates
the recursion when the objective value

X˜
>
nˆ k


1
converges within some small ε, or a maximal
number Tmax of iterations is reached, and obtains a normal vector b1. Having computed a vector
b1, there are two possibilities: either S is a hyperplane of dimension D − 1 or dim S < D − 1.
In the first case we can identify our subspace model with the hyperplane defined by the normal
b1. If on the other hand dim S < D − 1, we can proceed to find a second vector b2 ⊥ b1 that
is approximately orthogonal to S, and so on, until we have computed an orthogonal basis for the
orthogonal complement of S; this process naturally leads to Algorithm 1, in which c is an estimate
for the codimension D − d of the inlier subspace Span(X ).
Notice how the algorithm initializes n0: This is precisely the right singular vector of X˜
>
that
corresponds to the smallest singular value, after projection of X˜ onto Span(b1, . . . , bi−1)
⊥. As it
will be demonstrated in §7, this is a key choice, since it has the effect that the angle of n0 from the
inlier subspace is typically large, a desirable property for the success of recursion (10) (see Theorem
12). We refer to Algorithm 1 as DPCP-LP, to emphasize that the optimization problem associated
with each iteration of the recursion (10) is a linear program. In fact, at iteration k the optimization
problem is
min
b

X˜
>
b


1
s.t. b
>nˆ k−1 = 1, (136)
28
DUAL PRINCIPAL COMPONENT PURSUIT
Algorithm 1 Dual Principal Component Pursuit via Linear Programming
1: procedure DPCP-LP(X˜, c, ε, Tmax)
2: B ← ∅;
3: for i = 1 : c do
4: k ← 0;J ← 0; ∆J ← ∞;
5: nˆ 0 ← w ∈ argmin
b


2
=1, b⊥B

X˜
>
b


2
;
6: while k < Tmax and ∆J > εJ do
7: J ←

X˜
>
nˆ k


1
;
8: k ← k + 1;
9: nk ← w ∈ argminb
>nˆk−1=1,b⊥B

X˜
>
b


1
;
10: nˆ k ← nk/

nk


2
;
11: ∆J ← 
J −

X˜
>
nˆ k


1

;
12: end while
13: bi ← nˆ k;
14: B ← B ∪ {bi};
15: end for
16: return B;
17: end procedure
which can equivalently be written as a standard linear program,
min
b,u+,u−

11×N 11×N


u
+
u
−

(137)
s.t. "
IN −IN −X˜
>
01×N 01×N nˆ
>
k
# 

u
+
u
−
b

 =

0N×1
1

, u
+,u
− ≥ 0, (138)
and can be solved efficiently with an optimized general purpose linear programming solver, such as
Gurobi (Gurobi Optimization, 2015).
6.2 DPCP via Iteratively Reweighted Least-Squares (DPCP-IRLS)
Even though DPCP-LP (Algorithm 1) comes with theoretical guarantees as per Theorem 12, and
moreover will be shown to have a rather remarkable performance (at least for synthetic data, see
Fig. 6), it has the weakness that the linear programs (which are non-sparse) may become inefficient
to solve in high dimensions and for a large number of data points. Moreover, even though DPCPLP is theoretically applicable regardless of the subspace relative dimension d/D, its running time
increases with the subspace codimension c = D −d, since the c basis elements of S
⊥ are computed
sequentially. This motivates us to generalize the DPCP problem (9) to an optimization problem that
targets the entire orthogonal basis of S
⊥:
min
B∈RD×c

X˜
>B


1,2
s.t. B>B = Ic. (139)
29  
TSAKIRIS AND VIDAL
Algorithm 2 Dual Principal Component Pursuit via Iteratively Reweighted Least Squares
1: procedure DPCP-IRLS(X˜, c, ε, Tmax, δ)
2: k ← 0;J ← 0; ∆J ← ∞;
3: B0 ← W ∈ argminB∈RD×c, B>B=Ic

X˜
>B


F
;
4: while k < Tmax and ∆J > εJ do
5: J ←

X˜
>Bk


1,2
;
6: k ← k + 1;
7: Bk ← argminB∈RD×c, B>B=Ic
P
x˜∈X˜

B>x˜


2
2
/ max 
δ,

B>
k−1x˜


2
	
;
8: ∆J ←

X˜
>Bk−1


1,2
−

X˜
>Bk


1,2
;
9: end while
10: return Bk;
11: end procedure
Notice that in (139), the `1,2 matrix norm

X˜
>B


1,2
of X˜
>B is defined as the sum of the
Euclidean norms of the rows of X˜
>B, and as such, favors a solution B that results in a matrix
X˜
>B that is row-wise sparse (notice that for c = 1 (139) reduces precisely to the DPCP problem
(9)). In fact, Lerman et al. (2015) consider exactly the same problem (139), and proceed to relax
it to a semi-definite convex program, which they solve via an Iteratively Reweighted Least-Squares
(IRLS) scheme (Candes et al., 2008; Daubechies et al., 2010; Chartrand and Yin, 2008); while `
similar IRLS schemes appear in Zhang and Lerman (2014) and Lerman and Maunu (2017). Instead,
we propose to solve (139) directly via IRLS (and not a convex relaxation of it as Lerman et al.
(2015)): Given a D × c orthonormal matrix Bk−1, we define for each point x˜j a weight
wj,k :=
1
max 
δ,

B>
k−1x˜j


2
	, (140)
where δ > 0 is a small constant that prevents division by zero. Then we obtain Bk as the solution
to the quadratic problem
min
B∈RD×c
X
L
j=1
wj,k

B>x˜j


2
2
s.t. B>B = Ic, (141)
which is readily seen to be the c right singular vectors corresponding to the c smallest singular
values of the weighted data matrix WkX˜
>
, where Wk is a diagonal matrix with √wj,k at position
(j, j). We refer to the resulting Algorithm 2 as DPCP-IRLS; a study of its theoretical properties is
deferred to future work.
6.3 Denoised DPCP (DPCP-d)
Clearly, problem (9) (and (139)) is tailored for noise-free inliers, since, when the inliers X are
contaminated by noise, the vector X˜
>
b is no longer sparse, even if b is a true normal to the inlier subspace. As a consequence, it is natural to propose the following DPCP-denoised (DPCP-d)
30
DUAL PRINCIPAL COMPONENT PURSUIT
Algorithm 3 Denoised Dual Principal Component Pursuit
1: procedure DPCP-d(X˜, ε, Tmax, δ, τ )
2: Compute a Cholesky factorization LL> = X˜X˜
>
+ δID;
3: k ← 0; y0 ← 0;J ← 0; ∆J ← ∞;
4: b0 ← argminb∈RD:

b


2
=1

X˜
>
b


2
;
5: while k < Tmax and ∆J > εJ do
6: J ← τ

yk


1
+
1
2

yk − X˜
>
bk


2
2
7: yk+1 ← Sτ

X˜
>
bk

;
8: bk+1 ← solution of LL>ξ = X˜yk+1 by backward/forward propagation;
9: k ← k + 1;
10: bk ← bk/

bk


2
;
11: ∆J ← J − 
τ

yk


1
+
1
2

yk − X˜
>
bk


2
2

;
12: end while
13: return (yk
, bk);
14: end procedure
problem
min
b,y: ||b||2=1 
τ

y


1
+
1
2

y − X˜
>
b


2
2

, (142)
where now the vector variable y ∈ R
N+M is to be interpreted as a denoised vesion of the vector
X˜
>
b. Interestingly, both problems (9) and (142) appear in Qu et al. (2014), in the quite different
context of dictionary learning, where the authors propose to solve (142) via alternating minimization, in order to obtain an approximate solution to (9). Given b, the optimal y is given by Sτ

X˜
>
b

,
where Sτ is the soft-thresholding operator applied element-wise on the vector X˜
>
b. Given y the
optimal b is a solution to the quadratically constrained least-squares problem
min
b∈RD

y − X˜
>
b


2
2
s.t.

b


2
= 1. (143)
In the context of Qu et al. (2014), the coefficient matrix of the least-squares problem (X˜
>
in our
notation) has orthonormal columns. As a consequence, the solution to (143) is obtained in closed
form by projecting the solution of the unconstrained least-squares problem minb∈RD ||y −X˜
>
b||2
onto the unit sphere. However, in our context the assumption that X˜
>
has orthonormal columns
is in principle violated, so that the optimal b is no longer available in closed form. Even though
using Lagrange multipliers one ends up with a polynomial equation for the Lagrange multiplier, it
is known that computing the optimal value of the multiplier is a numerically challenging problem
(Elden, 2002; Golub and Von Matt, 1991; Gander, 1980). For this reason we leave exact approaches
for solving (143) to future investigations, and we instead propose to obtain a suboptimal b as Qu
et al. (2014) do, i.e., by projecting onto the unit sphere the solution of the unconstrained leastsquares problem. The resulting Algorithm 3 is very efficient, since the least-squares problems that
3 
TSAKIRIS AND VIDAL
appear in the various iterations have the same coefficient matrix X˜X˜
>
, a factorization of which
can be precomputed.9 Moreover, Algorithm 3 can trivially be extended to compute multiple normal
vectors, just as in Algorithm 1.
7. Experiments
In this section we evaluate the proposed algorithms experimentally. In §7.1 we investigate numerically the theoretical regime of success of recursion (10) predicted by Theorems 11 and 12. We
also show that even when these sufficient conditions are violated, (10) can still converge to a normal vector to the subspace if initialized properly. Finally, in §7.2 we compare DPCP variants with
state-of-the-art robust PCA algorithms for the purpose of outlier detection using synthetic data, and
similarly in §7.3 using real images.
7.1 Numerical evaluation of the theoretical conditions of Theorems 11 and 12
We begin with a numerical evaluation of the theoretical condition (46) of Theorem 11, under which
every global minimizer of the DPCP problem (9) is orthogonal to the inlier subspace S. We also
evaluate the initial minimal angle φ
∗
0
from S given in (51) of Theorem 12, which together with
(46) guarantee the convergence of the linear programming recursion (10) to an element of S
⊥.
As explained in the discussion of Theorems 11 and 12, for any fixed outlier ratio, condition (46)
will eventually be satisfied and also the angle φ
∗
0 will become arbitrarily small regardless of the
subspace relative dimension d/D, provided that N is sufficiently large and that both inliers and
outliers are uniformly distributed. Hence, we check whether (46) is true and also plot φ
∗
0
as we vary
N for uniformly distributed inliers and outliers. Towards that end, we fix the ambient dimension
as D = 30 and randomly sample a subspace S of varying dimension d = [5 : 5 : 25 29] so that
the relative subspace dimension d/D varies as [5/30 : 5/30 : 25/30 29/30]. We sample N inliers
uniformly at random from S ∩ S
D−1
for different values N = 500, 2000, 7000. For each value of
N we also sample M outliers uniformly at random from S
D−1
so that the percentage of outliers
varies as R := M/(N + M) = [0.1 : 0.1 : 0.7]. For each dataset instance as above, we estimate
the parameters X ,N , O,M, RO,X appearing in (46) and (51) by Monte-Carlo simulation.
The top row of Fig. 4 shows whether condition (46) is true (white) or not (black) as we vary N.
Notice that for N = 500, Fig. 4(a) shows a poor success regime. However, as we increase N to
2000, Fig. 4(b) shows that the success regime improves dramatically. Finally, as expected from our
earlier theoretical arguments, for sufficiently large N, in particular for N = 7000, Fig. 4(c) shows
that the sufficient condition (46) is satisfied regardless of outlier ratio or subspace relative dimension.
Similarly, notice how the angle φ
∗
0
, plotted in the bottom row of Fig. 4 (black for 0
◦ white for 90◦
),
uniformly decreases as we increase N across all outlier ratios and relative dimensions.
Next, we show that the recursion (10), if initialized properly, is in fact able to converge in just
a few iterations to a vector normal to the inlier subspace, even when the sufficient conditions of
Theorem 12 are not satisfied. Towards that end, we maintain the same experimental setting as
above using N = 500 and run (10) with a maximal number of iterations set to Tmax = 10 and a
convergence accuracy set to 10−3
. Fig. 5(a) is a replicate of Fig. 4(a) and serves as a reminder that
N = 500 results in a limited success regime as predicted by the theory; in particular the sufficient
9. The parameter δ in Algorithm 3 is a small positive number, typically 10−6
, which helps avoiding solving illconditioned linear systems.
32
DUAL PRINCIPAL COMPONENT PURSUIT
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(a) Check (46), N = 500
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(b) Check (46), N = 2000
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(c) Check (46), N = 7000
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(d) φ
∗
0 when N = 500
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(e) φ
∗
0 when N = 2000
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(f) φ
∗
0 when N = 7000
Figure 4: Figs. 4(a)-4(c) check whether the condition (46) is satisfied (white) or not (black) for a
fixed number N of inliers while varying the outlier ratio M/(N + M) and the subspace relative
dimension d/D. Figs. 4(d)-4(f) plot the minimum initial angle φ
∗
0
needed for convergence of the
recursion of linear programs (10) as per Theorem 12 (0
◦
corresponds to black and 90◦
corresponds
to white). Results are averaged over 10 independent trials.
condition (46) is satisfied only for a small outlier ratio or small subspace relative dimensions. Even
so, Fig. 5(b) shows that, when the recursion (10) is initialized using nˆ 0 as the left singular vector
of X˜ corresponding to the smallest singular value, then (10) converges in at most 10 iterations to
a vector nˆ
∗ whose angle φ
∗
from the subspace is precisely 90◦
. This suggests that the sufficient
condition (46) is much stronger than necessary, leaving room for future theoretical improvements.
On the other hand, Fig. 5(c) shows that when nˆ 0 is initialized uniformly at random, the recursion
(10) does not always converge to a normal vector, particularly for high outlier ratios and relative
dimensions. This reveals that initializing (10) from the SVD of the data is indeed a good strategy,
which is further supported by Fig. 5(e), which plots the angle of the initialization from the subspace
(contrast this to Fig. 5(f), which shows the angle of a random initialization from the subspace).
7.2 Comparative analysis using synthetic data
In this section we use the same synthetic experimental set-up as in §7.1 (with N = 500) to demonstrate the behavior of several methods relative to each other under uniform conditions, in the context of outlier rejection in single subspace learning. In particular, we test DPCP-LP (Algorithm
1), DPCP-IRLS (Algorithm 2), DPCP-d (Algorithm 3), RANSAC (Fischler and Bolles, 1981),
SE-RPCA (Soltanolkotabi and Candes, 2012), ` `21-RPCA (Xu et al., 2012), the IRLS version of
REAPER (Lerman et al., 2015), as well as Coherence Pursuit (CoP) (Rahmani and Atia, 2017); see
§2 for details on these last five existing methods.
33
TSAKIRIS AND VIDAL
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(a) Check if (46) is true
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(b) φ
∗
(nˆ 0 from SVD)
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(c) φ
∗
(nˆ 0 random)
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(d) φ
∗
0 of (51)
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(e) φ0 (nˆ 0 from SVD)
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(N+M)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(f) φ0 (nˆ 0 random)
Figure 5: Convergence of recursion (10) in a regime of limited theoretical guarantees (N = 500)
as concluded from Fig. 4. The number of inliers is fixed at N = 500. Fig. 5(a) plots whether the
sufficient condition (46) is satisfied (white) or not (black) for varying outlier ratios M/(N + M)
and relative dimensions d/D. Figs. 5(b)-5(c) plot the angle φ
∗
(0
◦
corresponds to black and 90◦
corresponds to white) from the inlier subspace of the vector nˆ
∗
that recursion (10) converges to,
when nˆ 0 is initialized from the SVD of the data or uniformly at random, respectively. Fig. 5(d)
plots the minimum angle φ
∗
0
needed for the convergence of (10) to a normal vector in the inlier
subspace as per Theorem 12, while Figs. 5(e)-5(f) plot the angle φ0 of nˆ 0 from the subspace, when
it is initialized from the SVD of the data or uniformly at random, respectively. The results are
averages over 10 independent trials.
For the methods that require an estimate of the subspace dimension d, such as RANSAC,
REAPER, CoP, and all DPCP variants, we provide as input the true subspace dimension. The
convergence accuracy of all methods is set to 10−3
. For REAPER we set the regularization parameter as δ = 10−6
and the maximum number of iterations equal to 100. For DPCP-d we set
τ = 1/
√
N + M as suggested in Qu et al. (2014) and the maximum number of iterations to 1000.
For RANSAC we set its thresholding parameter to 10−3
, and for fairness, we do not let it terminate
earlier than the running time of DPCP-LP, unless the theoretically required number of iterations for
a success probability 0.99 is reached (here we are using the ground truth outlier ratio). Both SERPCA and `21-RPCA are implemented with ADMM, with augmented Lagrange parameters 1000
and 100 respectively. For `21-RPCA λ is set to 3/(7√
M), as suggested in Xu et al. (2012). DPCP
variants are initialized via the SVD of the data as in Algorithm 1. CoP is implemented using the code
provided by its authors, and selects 3d points upon classic PCA gives the subspace estimate. Finally,
the linear programs in DPCP-LP are solved via the generic LP solver Gurobi (Gurobi Optimization,
2015), while the maximum number of iterations for DPCP-LP is set to Tmax = 10.
34
DUAL PRINCIPAL COMPONENT PURSUIT
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(M+N)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(a) RANSAC
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(M+N)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(b) SE-RPCA
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(M+N)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(c) `21-RPCA
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(M+N)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(d) REAPER
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(M+N)
0.16
0.33
0.5
0.67
0.83
0.97
d/D
(e) CoP
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(M+N)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(f) DPCP-LP
0.1 0.2 0.3 0.4 0.5 0.6 0.7
M/(M+N)
0.17
0.33
0.5
0.67
0.83
0.97
d/D
(g) DPCP-IRLS
Figure 6: Outlier/Inlier separation in the absence of noise over 10 independent trials. The horizontal
axis is the outlier ratio defined as M/(N + M), where M is the number of outliers and N is the
number of inliers. The vertical axis is the relative inlier subspace dimension d/D; the dimension of
the ambient space is D = 30. Success (white) is declared by the existence of a threshold that, when
applied to the output of each method, perfectly separates inliers from outliers.
Absence of Noise We first investigate the potential of each of the above methods to perfectly
distinguish outliers from inliers in the absence of noise.10 Note that each method returns a signal
α ∈ R
N+M
+ , which can be thresholded for the purpose of declaring outliers and inliers. For SERPCA, α is the `1-norm of the columns of the coefficient matrix C, while for `21-RPCA α is the
`2-norm of the columns of E. Since RANSAC, REAPER, CoP, and DPCP variants directly return
subspace models, for these methods α is simply the distances of all points to the estimated subspace.
In Fig. 6 we depict success (white) versus failure (black), where success is interpreted as the
existence of a threshold on α that perfectly separates outliers and inliers. First observe that, as
expected, RANSAC succeeds when there are very few outliers (10%) regardless of the inlier rel10. We do not include DPCP-d for this experiment, since it only approximately solves the DPCP optimization problem,
and hence it can not be expected to perfectly separate inliers from outliers, even when there is no noise (we have
confirmed this experimentally).
35
TSAKIRIS AND VIDAL
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
(a) σ = 0.05, d/D = 25/30
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
(b) σ = 0.10, d/D = 25/30
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
(c) σ = 0.20, d/D = 25/30
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
(d) σ = 0.05, d/D = 29/30
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
(e) σ = 0.10, d/D = 29/30
0 0.2 0.4 0.6 0.8 1
0.2
0.4
0.6
0.8
1
(f) σ = 0.20, d/D = 29/30
200 400 600 800 1000
point index
0.1
0.2
0.3
0.4
0.5
0.6
distance to
ˆS for DPCP-LP
(g) σ = 0.05, d/D = 25/30
200 400 600 800 1000
point index
0
0.1
0.2
0.3
0.4
0.5
distance to
ˆS for DPCP-LP
(h) σ = 0.05, d/D = 29/30
Figure 7: Figs. 7(a)-7(f) show ROC curves for varying noise standard deviation σ = 0.05, 0.1, 0.2,
and subspace relative dimension d/D = 25/30, 29/30 (number of inliers is N = 500 and outlier
ratio is M/(N + M) = 0.5). The horizontal axis is False Positives ratio and the vertical axis is
True Positives ratio. The number associated with each curve is the area above the curve; smaller
numbers reflect more accurate performance. Figs. 7(g)-7(h) show the distance of the noisy points
to the subspace estimated by DPCP-LP for noise σ = 0.05 for both relative dimensions under
consideration.
ative dimension d/D, since in such a case the probability of sampling outlier-free points is high.
Similarly, RANSAC succeeds when d/D is small regardless of the outlier ratio, since in that case
one needs only sample d points, and for a sufficient budget (say, the running time of DPCP-LP),
the probability of one of these samples being outlier-free is again high. Moving on, and again
as expected, both SE-RPCA and `21-RPCA succeed only for low to medium relative dimensions,
since both methods are meant to exploit the low-rank structure of the inlier data, and thus fail when
such a structure does not exist. Remarkably, even though CoP is a low-rank method in spirit, it
performs surprisingly better than its low-rank alternatives SE-RPCA and `21-RPCA, giving perfect
inlier/outlier separation regardless of the outlier ratio for relative dimensions d/D ≤ 2/3. Further
improvement is achieved by REAPER, which succeeds for as many as 40% outliers and as high a
36
DUAL PRINCIPAL COMPONENT PURSUIT
Table 1: Mean running time of each method in seconds over 10 independent trials for the experimental setting of §7.2. We report only the extreme regimes corresponding to d/D = 5/30, 29/30
and M/(N + M) = 0.1, 0.7. The experiment is run in MATLAB on a standard Macbook-Pro with
a dual core 2.5GHz Processor and a total of 4GB Cache memory.
d/D : M/(N + M) 5/30 : 0.1 29/30 : 0.1 5/30 : 0.7 29/30 : 0.7
RANSAC 0.097 0.410 23.31 2.83
SE-RPCA 4.485 4.519 58.94 79.07
`21-RPCA 0.048 0.014 0.185 0.180
REAPER 0.050 0.058 0.153 0.042
CoP 0.014 0.014 0.062 0.061
DPCP-LP 16.87 0.407 95.35 2.822
DPCP-IRLS 0.046 0.038 0.121 0.415
relative dimension as 25/30 ≈ 0.83. Yet, REAPER fails in the challenging case d/D = 29/30 as
soon as there are more than 20% outliers. Remarkably, DPCP-LP allows for perfect outlier rejection
across all outlier ratios and all relative dimensions, thus clearly improving the state-of-art in the high
relative dimension regime. Moreover, DPCP-IRLS does almost as well as DPCP-LP thus being the
second best method, except that it only fails in the hardest of regimes, i.e., for relative dimension
29/30 ≈ 0.97 and for more than 50% outliers.
Finally, it is important to comment on the running time of the methods. As Table 1 shows,
DPCP-LP is admittedly the slowest among the methods, particularly for low relative dimensions,
since in that case many dual principal components need to be computed. Indeed, for 10% outliers
and d/D = 5/30 DPCP-LP computes D − d = 25 dual components and thus it takes about 17
seconds, as opposed to 0.4 seconds for the same amount of outliers but d/D = 29/30, since a
single dual component is computed in this latter case. Similarly, for 70% outliers DPCP-LP takes
about 95 seconds when d/D = 5/30 as opposed to about 3 seconds for d/D = 29/30. On the other
hand, DPCP-IRLS is one order of magnitude faster than DPCP-LP and comparable to `21-RPCA
and REAPER, which overall are the second fastest methods, with CoP being the fastest of all.
Presence of Noise Next, we fix D = 30, N = 500, M/(N + M) = 0.5, and investigate the
performance of the methods, adding DPCP-d to the mix, in the presence of varying levels of noise
for two cases of high relative dimension, i.e., d/D = 25/30 and d/D = 29/30. The inliers are
corrupted by additive white Gaussian noise of zero mean and standard deviation σ = 0.05, 0.1, 0.2,
with support in the orthogonal complement of the inlier subspace. The parameters of all methods
are the same as earlier except for DPCP-d we set τ = max 
σ, 1/
√
N + M
	
, while for RANSAC
we set its threshold parameter equal to σ.
We evaluate the performance of each method by its corresponding ROC curve. Each point of an
ROC curve corresponds to a certain value of a threshold, with the vertical coordinate of the point
giving the percentage of inliers being correctly identified as inliers (True Positives), and the horizontal coordinate giving the number of outliers erroneously identified as inliers (False Positives).
As a consequence, an ideal ROC curve should be concentrated to the top left of the plot, i.e., the
area above the curve should be zero. The ROC curves11 as well as the area above each curve are
shown in Fig. 7. As expected, the low-rank methods RANSAC, SE-RPCA and `21-RPCA perform
poorly for either relative dimension with performance being close to that of a random guess (inlier
11. We note that the vertical axis of all ROC curves in this paper starts from a ratio of 0.1 True Positives.
37
TSAKIRIS AND VIDAL
vs. outlier) for relative dimension 29/30. On the other hand REAPER, CoP, DPCP-LP, DPCP-IRLS
and DPCP-d perform almost perfectly well for d/D = 25/30, while REAPER starts failing for very
high relative dimension 29/30, even for as low noise standard devision as σ = 0.05 (of course this
is to be expected because we already know from Fig. 6 that REAPER fails at this regime even in
the absence of noise), and CoP fails completely. In contrast, the DPCP variants remain robust to
noise in this challenging regime for as high noise as σ = 0.1. What is remarkable, is that both
DPCP-LP and DPCP-IRLS, which are designed for noiseless data, are surprisingly robust to noise,
and slightly outperform DPCP-d, the latter meant to handle noisy data. We attribute this fact to
the suboptimal approach we followed in solving the DPCP-d problem, as well as to the lack of a
suitable mechanism for optimally tuning its thresholding parameter.
7.3 Comparative analysis using real data: Three-view geometry
In this section we perform an experimental evaluation of the proposed methods in the context of
the three-view problem in computer vision using real data. In that setting one is given three images
of the same static scene taken from different views, and the goal is to estimate the relative view
poses, i.e., the rotations and translations that relate, say, view 1 to view 2 and 3 (e.g., see Figs 8(a)-
8(b)). This task is of fundamental importance in many computer vision applications, such as 3D
reconstruction, where a 3D model of a real-world scene is constructed from 2D images of the scene.
The trifocal tensor. The three images of the static scene may have been taken from three different cameras, or from a single moving camera. Regardless, the underlying three-view geometry is
characterized by the constraints satisfied by any points lying in views 1, 2 and 3 respectively, that
correspond to the same 3D point (e.g., see Fig. 8(b)). To describe the nature of these constraints,
we fix a coordinate system (x, y, z) for the 3D space, and identify view 1 with the plane z = 1
and its optical center with the origin 0 of the coordinate system. We refer to this view as canonical
view V. Then the projection X of a 3D point Ξ = (ξ1, ξ2, ξ3)
> onto V is the intersection point of
the plane z = 1 with the line that passes through Ξ and the origin, i.e., X = λΞ with λ = 1/ξ3.
For simplicity, we assume that views 2 (V
0
) and 3 (V
00) are rotated and translated versions of the
canonical view V, i.e., there exist rotations R0
, R00 ∈ SO(3) and translations t
0
, t
00 ∈ R
3
, such that12
V = R0
(V
0
) + t
0 = R00(V
00) + t
00
. (144)
The projection X
0 of the 3D point Ξ onto V
0
is the intersection of V
0 with the line that passes from
Ξ and the optical center −t
0 of view 2. However, in practice X
0
is only known up to local pixel
coordinates with respect to view 2. That is, we can only know the representation x
0 of X
0 with
respect to a coordinate system where view 2 is the canonical view. In such a system of coordinates
the point Ξ is represented as R0Ξ + t
0
and hence x
0 = λ
0
(R0Ξ + t
0
), where λ
0
is the inverse of the
third coordinate of the vector R0Ξ + t
0
. Substituting Ξ = (1/λ)x in this equation yields a relation
between the local representations13 x, x
0 of X , X
0
in V and V
0
respectively as follows:
1
λ0
x
0 =
1
λ
R0x + t
0
. (145)
12. This geometry corresponds to calibrated cameras, where the camera projection parameters are known.
13. Without loss of generality we take the local system of coordinates of view 1 to be the same as the global system of
coordinates.
38
DUAL PRINCIPAL COMPONENT PURSUIT
(a) Three views of the same scene.
(b) Left: Example of points viewed by all three cameras. Right: Configuration of
cameras and the same points depicted in 3D space. Color represents height.
(c) Examples of correct (yellow) and incorrect (red) point correspondences between
the three views.
Figure 8: An example of three views of a static scene along with camera configurations and point
correspondences (views 2, 4, 6 of the Model House dataset, provided by the Visual Geometry Group,
University of Oxford).
Now, for a vector v = (α, β, γ)
> ∈ R
3
, denote by [v] the skew-symmetric matrix
[v] =


0 γ −β
−γ 0 α
β −α 0

 , (146)
and note that [v]v = 0. Multiplying equation (145) from the left by [x
0
] gives
1
λ
[x
0
]R0x + [x
0
]t
0 = 0. (147)
In exactly the same way, and letting x
00 be the representation of X
00 in the canonical coordinate
system for view 3, we have a relationship
1
λ
[x
00]R00x + [x
00]t
00 = 0. (148)
39
TSAKIRIS AND VIDAL
Degenerate cases aside, equations (147)-(148) are equivalent to the condition
Rank  [x
0
]R0x [x
0
]t
0
[x
00]R00x [x
00]t
00 ≤ 1, (149)
which in turn is equivalent to the matrix equation14
[x
0
]R0x t00>[x
00]
> − [x
0
]t
0x
> R00>[x
00]
> = 03×3, (150)
or more elegantly written as
[x
0
]
X
3
i=1
xiT i

[x
00]
> = 03×3, x = (x1, x2, x3)
>, T i
:= r
0
i
t
00> − t
0
r
00>
i
, i = 1, 2, 3, (151)
where r
0
i
, r
00
i
is the ith column of R0
, R00 respectively. Equation (151) consists of 9 trilinear constraints on the local representations x, x
0
, x
00 of the imaged 3D point Ξ, among which a maximal
number of four are linearly independent (Hartley and Zisserman, 2004). The matrices (T 1, T 2, T 3)
are the slices of the so-called trifocal tensor15 T ∈ R
3×3×R
3×3×R
3×3
, which is the mathematical
object that encodes the relative geometry of the calibrated three views: indeed, up to a change of
coordinates there is a 1 − 1 correspondence between trifocal tensors and camera views V, V
0
, V
00;
see Proposition 15 and Theorem 16 in Kileel (2017).
Trifocal tensor estimation as a hyperplane learning problem. Notice that the trilinear constraints
(151) are linear in the entries of the tensor T = (T 1, T 2, T 3), which in its unfolded form can be
regarded as a vector t ∈ R
27. In fact, the space of (uncalibrated) trifocal tensors is an algebraic
variety of R
27 of dimension 18 (Alzati and Tortora, 2010; Aholt and Oeding, 2014). As already
noted, every point correspondence (x, x
0
, x
00) contributes four linearly independent equations in t;
equivalently, every point correspondence cuts the variety with four hyperplanes. As it turns out
though, only 3 of these hyperplanes are algebraically independent with respect to the variety16, i.e.,
every generic point correspondence reduces the dimension of the variety by three (Kileel, 2017).
As a result, one needs 6 correspondences to get a finite number of candidate trifocal tensors that
agree with them. Adding a 7th correspondence allows us to uniquely determine t via solving a
28 × 27 homogeneous linear system of equations, while the relative poses (R0
, t
0
) and (R00
, t
00) can
be extracted from t by, e.g., the procedure described by Hartley and Zisserman (2004).
The above discussion suggests that given a set of N0 ≥ 7 generic and exact point correspondences {(xj , x
0
j
, x
00
j
)}
N0
j=1, the coefficient vectors
c
(1)
1
, c
(1)
2
, c
(1)
3
, c
(1)
4
, . . . , c
(j)
1
, c
(j)
2
, c
(j)
3
, c
(j)
4
. . . , c
(N0
)
1
, c
(N0
)
2
, c
(N0
)
3
, c
(N0
)
4 ∈ R
27
, (152)
of the resulting N = 4N0
linear equations span a hyperplane in R
27 with normal vector t. We
will be referring to the vectors (152) as the trilinear embeddings of the point correspondences.
14. Here we have used the fact that for vectors a, b, c, d ∈ R
n
the 2n × 2 matrix 
a b
c d
has rank at most 1 if and only
if ad> − bc> = 0n×n; thanks to Tianjiao Ding for this observation.
15. In the uncalibrated case, which is more relevant in computer vision applications, the trifocal tensor has exactly the
same structure as in (151), with the only difference being that the matrices R0
, R00 are no longer rotations.
16. A more precise way to state this in algebraic-geometric language is that the ideal generated by these four equations
has depth 3 in the quotient ring of the trifocal variety.
40
DUAL PRINCIPAL COMPONENT PURSUIT
However, in practice one obtains such correspondences by matching points across images based on
the similarity of some local features, such as SIFT (Lowe, 1999). As a result, it is typically the
case that many of the produced correspondences are incorrect (see Fig. 8(c)), and the problem then
becomes that of detecting inliers lying close to a hyperplane of R
27, from a dataset corrupted by
outliers. Equivalently, one is presented with a codimension 1 subspace learning problem, for which
the proposed Dual Principal Component Pursuit (DPCP) is ideally suited; as we show next, the
method can achieve superior performance than RANSAC, the latter being the traditional and up to
date one of the most popular options in the computer vision community for such problems.
Data. We use the first three views of the datasets Model House, Corridor and Merton College III,
provided by the Visual Geometry Group at Oxford University. Each dataset contains different views
of the same static scene, together with the projection matrices of each view and high-quality (inlier) point correspondences. From each dataset we randomly pick N0 = 125 inlier correspondences
{(xj , x
0
j
, x
00
j
)}
N0
j=1. We further generate 100 · M0/(N0 + M0
)% = 30%, 40%, 50% outlier correspondences {(oj , o
0
j
, o
00
j
)}M0
j=1 as follows: For each triplet (V, V
0
, V
00) of views we sample uniformly
at random M0 points from each view, and randomly match them in M0
triplets. We normalize all
data according to Hartley (1997) and form a unit `2-norm dataset X˜ = [X O]Γ ∈ R
27×4(M0+N0
)
that consists of the trilinear embeddings (see (152)) of all inlier/outlier correspondences.
Algorithms. We compare REAPER and two variants of RANSAC (see §2) with the proposed fast
DPCP variants DPCP-IRLS (Algorithm 2) and DPCP-d (Algorithm 3) in the context of outlier detection for trifocal tensor estimation17. REAPER, DPCP-IRLS and DPCP-d receive as input the
dataset X˜ and are configured to learn a subspace of dimension 26 in R
27 (a hyperplane) that fits
X˜. So does the first RANSAC variant, called H-RANSAC (Hyperplane-RANSAC), which is the
standard RANSAC that randomly samples 26 trilinear embeddings at each trial. The second variant,
called H-G-RANSAC (Hyperplane-Group-RANSAC), is exactly as H-RANSAC except that it samples trilinear embeddings in groups of four (instead of individually), where each group is associated
with a point correspondence18. For H-RANSAC we use as threshold the maximal distance among
inlier trilinear embeddings to the hyperplane associated with the ground-truth trifocal tensor. For
H-G-RANSAC we use the maximal average such distance among inlier trilinear embeddings in the
same group. Regarding time budget, we note that the fastest method is DPCP-d, and then follows
REAPER and DPCP-IRLS. For example, for the experiment we are considering and for 40% outliers, DPCP-d needs an average of about 1msec to converge as opposed to 16msec for DPCP-IRLS.
Since RANSAC’s performance is sensitive to the allocated time budget, we explore a high time
budget regime (running time of DPCP-IRLS), as well as a low time budget regime (running time of
DPCP-d), and for fairness, we also restrict the running time of rest of the methods accordingly.
Results. We use as a metric the group precision of the algorithms that corresponds to a recall value
equal to 1: given a hyperplane estimate Hˆ, this induces an ordering of all the inlier/outlier point
17. The purely low-rank methods SE-RPCA and `2-RPCA are unsuitable for learning a hyperplane; since Coherence
Pursuit (CoP) performed much better than them with synthetic data, we also included it in our experiments, but do
not report its performance as it was not competitive with the other tested methods, i.e., RANSAC, REAPER and
DPCP. Notice from Fig. 7 that with synthetic data CoP fails precisely at the case of a hyperplane.
18. Since the trilinear embeddings also lie in a union of coordinate hyperplanes irrelevant to the trifocal tensor, taking this
grouping into consideration prevents the method from learning one of these hyperplanes. This is implicitly achieved
by the RANSAC variant penalizing the re-projection error, most commonly used for trifocal tensor estimation. We
have also tested this variant, however due to its higher complexity and the running time restrictions enforced in this
experiment, it did not perform on par with the rest of the methods and hence we do not report its performance further.
A group-based DPCP approach is the subject of current research.
41
TSAKIRIS AND VIDAL
correspondences based on increasing average distance of the corresponding 4-tuples of trilinear
embeddings to Hˆ. Letting α be the maximal such average distance that corresponds to an inlier
point correspondence, our metric is the percentage of the inliers among all correspondences with
average distance to Hˆ less or equal than α. Tables 2, 3 and 4 report the precision of the algorithms for
the three different scenes, Corridor, Model House and Merton College III respectively, for different
outlier ratios and different time budgets.
Table 2: Algorithm precision when recall value is 1 for the first three views of dataset Corridor.
Algorithm vs. 30% outliers 40% outliers 50% outliers
% of outliers & time budget high t.b. low t.b. high t.b. low t.b. high t.b. low t.b.
H-RANSAC 0.698 0.735 0.601 0.625 0.502 0.522
H-G-RANSAC 1.000 0.992 0.992 0.977 0.992 0.665
REAPER-IRLS 1.000 1.000 1.000 0.992 0.984 0.954
DPCP-IRLS 1.000 1.000 1.000 1.000 1.000 0.977
DPCP-d 1.000 1.000 1.000 1.000 1.000 1.000
Table 3: Algorithm precision when recall value is 1 for the first three views of dataset Model House.
Algorithm vs. 30% outliers 40% outliers 50% outliers
% of outliers & time budget high t.b. low t.b. high t.b. low t.b. high t.b. low t.b.
H-RANSAC 0.702 0.725 0.601 0.619 0.502 0.510
H-G-RANSAC 0.996 0.984 0.992 0.893 0.984 0.587
REAPER-IRLS 0.992 0.977 0.977 0.943 0.880 0.820
DPCP-IRLS 0.969 0.980 0.969 0.965 0.947 0.856
DPCP-d 0.984 0.984 0.977 0.977 0.954 0.954
Table 4: Algorithm precision when recall value is 1 for the three views of dataset Merton College 3.
Algorithm vs. 30% outliers 40% outliers 50% outliers
% of outliers & time budget high t.b. low t.b. high t.b. low t.b. high t.b. low t.b.
H-RANSAC 0.698 0.723 0.607 0.620 0.505 0.515
H-G-RANSAC 1.000 0.980 0.992 0.906 0.992 0.546
REAPER-IRLS 1.000 0.992 0.984 0.919 0.839 0.786
DPCP-IRLS 1.000 0.992 1.000 0.936 0.992 0.794
DPCP-d 1.000 1.000 1.000 1.000 0.992 0.992
As a first observation note that H-RANSAC essentially fails for all three datasets, with a precision not exceeding 73.5 %, even for the case of 30% outliers. Moreover, its precision is higher
for low time budget, which at first sight seems contradictory. Both phenomena are attributed to
42
DUAL PRINCIPAL COMPONENT PURSUIT
a combination of insufficient time budget together with the fact that the dataset X˜ also lies in a
union of coordinate hyperplanes irrelevant to the trifocal tensor; this is evident by inspecting the
zero structure of the trilinear embeddings, not shown here. As a result, given different time budgets
H-RANSAC identifies different hyperplanes that fit a significant portion of the data, but with none
coinciding with the trifocal tensor.
The aforementioned issue is remedied by H-G-RANSAC, which forces the estimated hyperplane to fit groups of trilinear embeddings respecting the underlying point-point-point correspondences. Indeed, this dramatically improves its performance and for the dataset Model House in
Table 3 it is the best performing method for high time budget. However, H-G-RANSAC is not able
to cope with 50% outliers at low time budget: its highest precision in that regime is only 66.5% for
the dataset Corridor in Table 2.
On the other hand, DPCP-d is not only fast, but also very robust: in the challenging regime of
50% outliers it is the only method that gives 100% precision for the dataset Corridor, 95.4% for
Model House, and 99.2% for Merton College III, while in this low time budget regime the second
best method is DPCP-IRLS with precision 97.7%, 85.6% and 79.4% respectively. Interestingly,
DPCP-d is performing uniformly better than DPCP-IRLS even if the latter is allowed to run to
convergence (high time budget); we attribute this to the fact that DPCP-d is designed to explicitly
handle noise. Overall, DPCP-d is the best performing method in low time budget across all datasets,
and the best performing method in high budget for datasets Corridor and Merton College III. Finally,
REAPER performs somewhere between DPCP-IRLS and H-G-RANSAC, outperforming DPCPIRLS only on a few occasions. This is consistent with the experiment of Fig. 6 on synthetic data,
according to which the advantage of DPCP over REAPER is precisely the codimension 1 case,
where the latter fails.
In conclusion, even though RANSAC can have a very high precision given sufficient time budget, once the latter is restricted its performance can drop dramatically. This is particularly the case
for large outlier ratios, a regime where an exponentially large time budget might be needed. Moreover, RANSAC is very sensitive to its thresholding parameter, which in the above experiment was
set using knowledge of the ground truth. Clearly, such knowledge is not available in practice and
different choices for this parameter are expected to only lead to performance degradation. On the
other hand, DPCP-d was found to be the best method in the above experiment, combining low running time, high precision and robustness to its thresholding parameter, suggesting that the proposed
Dual Principal Component Pursuit can be a useful or even superior alternative to popular approaches
such as RANSAC, for three-view geometry or other computer vision applications.
8. Conclusions
We presented and studied a solution to the problem of robust principal component analysis in the
presence of outliers, called Dual Principal Component Pursuit (DPCP). The heart of the proposed
method consisted of a non-convex `1 optimization problem on the sphere, for which a solution strategy based on a recursion of linear programs was analyzed. Rigorous mathematical analysis revealed
that DPCP is a natural method for learning the inlier subspace in the presence of outliers, even in
the challenging regime of large outlier ratios and high subspace relative dimensions. In fact, experiments on synthetic data showed that DPCP was the only method that could handle 70% outliers
inside a 30-dimensional ambient space, irrespectively of the subspace dimension. Moreover, exper43
TSAKIRIS AND VIDAL
iments with real images in the context of three-view geometry showed that DPCP can outperform
popular alternatives such as RANSAC, suggesting its potential in computer vision applications.
Appendix A. Review of existing results on Problems (9) and (10)
In this appendix we state three results that are important for our mathematical analysis, already
known in Spath and Watson (1987). For the sake of clarity and convenience, we have also taken the ¨
liberty of writing complete proofs of the statements, as not all of them can be found in Spath and ¨
Watson (1987).
Proposition 14 Let Y = [y1
, . . . , yL] ∈ D × L be full rank. Then any global solution b
∗
to
min
b
>b=1

Y
>b


1
, (153)
must be orthogonal to (D − 1) linearly independent columns of Y.
Proof Let b
∗
be an optimal solution of (153). Then b
∗ must satisfy the first order optimality relation
0 ∈ Y Sgn(Y
>b
∗
) + λb
∗
, (154)
where λ is a scalar Lagrange multiplier parameter, and Sgn is the sub-differential of the `1 norm.
Without loss of generality, let y1
, . . . , yK be the columns of Y to which b
∗
is orthogonal. Then
equation (154) implies that there exist real numbers α1, . . . , αK ∈ [−1, 1] such that
X
K
j=1
αjyj +
X
L
j=K+1
Sign(y
>
j b
∗
)yj + λb
∗ = 0. (155)
Now, suppose that the span of y1
, . . . , yK is of dimension less than D − 1. Then there exists a unit
norm vector ζ ∈ S
D−1
that is orthogonal to all y1
, . . . , yK, b
∗
, and multiplication of (155) from the
left by ζ
> gives
X
L
j=K+1
Sign(y
>
j b
∗
)ζ
>yj = 0. (156)
Furthermore, we can choose a sufficiently small ε > 0, such that
Sign(y
>
j b
∗ + εy
>
j ζ) = Sign(y
>
j b
∗
), ∀j ∈ [L]. (157)
The above equation is trivially true for all j such that y
>
j
b
∗ = 0, because in that case y
>
j
ζ = 0 by
the definition of ζ. On the other hand, if y
>
j
b
∗
6= 0, then a small perturbation  will not change the
sign of y
>
j
b
∗
. Consequently, we can write


y
>
j
(b
∗ + εζ)


 =


y
>
j b
∗


 + ε Sign(y
>
j b
∗
)y
>
j ζ, ∀j ∈ [L] (158)
and so

Y
>(b
∗ + εζ)


1
=

Y
>b
∗


1
+ ε
X
L
j=K+1
Sign(y
>
j b
∗
)ζ
>yj =

Y
>b
∗


1
. (159)
44
DUAL PRINCIPAL COMPONENT PURSUIT
However,

b
∗ + εζ


2
=
p
1 + ε
2 > 0, (160)
and normalizing b
∗+εζ to have unit `2 norm, we get a contradiction on b
∗
being a global solution.
Proposition 15 Let Y = [y1
, . . . , yL] be a D × L matrix of rank D. Then problem
min
b
>nˆk=1

Y
>b


1
(161)
admits a computable solution nk+1 that is orthogonal to (D − 1) linearly independent points of Y.
Proof Let nk+1 be a solution to minb
>nˆk=1

Y
>b


1
that is orthogonal to less than D − 1 linearly
independent points of Y. Then we can find a unit norm vector ζ that is orthogonal to the same points
of Y that nk+1 is orthogonal to, and moreover ζ ⊥ nk+1. In addition, we can find a sufficiently
small ε > 0 such that

Y
>(nk+1 + εζ)


1
=

Y
>nk+1


1
+ ε
X
j: nk+16⊥yj
Sign(y
>
j nk+1)ζ
>yj
, (162)
where
X
j: nk+16⊥yj
Sign(y
>
j nk+1)ζ
>yj ≤ 0. (163)
Since nk+1 is optimal, it must be the case that
X
j: nk+16⊥yj
Sign(y
>
j nk+1)ζ
>yj = 0, (164)
and so

Y
>(nk+1 + εζ)


1
=

Y
>nk+1


1
. (165)
By (165) we see that as we vary ε the objective remains unchanged. Notice also that varying ε preserves all zero entries appearing in the vector Y
>nk+1. Furthermore, because of (164), it is always
possible to either decrease or increase ε until an additional zero is achieved, i.e., until nk+1 + εζ
becomes orthogonal to a point of Y that nk+1 is not orthogonal to. Then we can replace nk+1 with
nk+1 + εζ and repeat the process, until we get some nk+1 that is orthogonal to D − 1 linearly
independent points of Y.
Proposition 16 Let Y = [y1
, . . . , yL] be a D×L matrix of rank D. Suppose that for each problem
(161) a solution nk+1 is chosen such that nk+1 is orthogonal to D − 1 linearly independent points
of Y, in accordance with Proposition 15. Then the sequence {nk} converges to a critical point of
problem (153) in a finite number of steps.
45
TSAKIRIS AND VIDAL
Proof If nk+1 = nˆ k, then inspection of the first order optimality conditions of the two problems,
reveals that nˆ k is a critical point of minb
>b=1

Y
>b


1
. If nk+1 6= nˆ k, then

nk+1


2
> 1, and so

Y
>nˆ k+1


1
<

Y
>nˆ k


1
. As a consequence, if nk+1 6= nˆ k, then nˆ k can not arise as a solution
for some k
0 > k. Now, because of Proposition 15, for each k, there is a finite number of candidate
directions nk+1. These last two observations imply that the sequence {nk} must converge in a finite
number of steps to a critical point of minb
>b=1

Y
>b


1