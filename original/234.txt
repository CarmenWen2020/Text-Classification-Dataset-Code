At present, although great progress has been made in automatic depression assessment, most of the recent works only concern the audio and video paralinguistic information, rather than the linguistic information from the spoken content. In this work, we argue that beside developing good audio and video features, to build reliable depression detection systems, text-based content features are also of importance to analyse depression-related textual indicators. Furthermore, to improve the performance of automatic depression assessment systems, powerful models, capable of modelling the characteristics of depression embedded in the audio, visual and text descriptors, are also required. This paper proposes new text and video features and hybridizes deep and shallow models for depression estimation and classification from audio, video and text descriptors. The proposed hybrid framework consists of three main parts: 1) A Deep Convolutional Neural Network (DCNN) and Deep Neural Network (DNN) based audio-visual multi-modal depression recognition model for estimating the Patient Health Questionnaire depression scale (PHQ-8); 2) A Paragraph Vector (PV) and Support Vector Machine (SVM) based model for inferring the physical and mental conditions of the individual from the transcripts of the interview; 3) A Random Forest (RF) model for depression classification from the estimated PHQ-8 score and the inferred conditions of the individual. In the PV-SVM model, PV embedding is used to obtain fixed-length feature vectors from transcripts of the answers to the questions associated with psychoanalytic aspects of depression, which are subsequently fed into the SVM classifiers for detecting the presence/absence of the considered psychoanalytic symptoms. To our best knowledge, this approach is the first attempt to apply PV for depression analysis. Besides, we propose a new visual descriptor - Histogram of Displacement Range (HDR) to characterize the displacement and velocity of the facial landmarks in the video segment. Experiments have been carried out on the Audio Visual Emotion Challenge (AVEC2016) depression dataset, they demonstrate that: 1) The proposed hybrid framework effectively improves the accuracies of both depression estimation and depression classification, with an average F1 measure up to 0.746, which is higher than the best result (0.724) of the depression sub-challenge of AVEC2016. 2) HDR obtains better depression recognition performance than Bag-of-Words (BoW) and Motion History Histogram (MHH) features.

SECTION 1Introduction
Depression and anxiety disorders are highly prevalent worldwide. Attention to the adverse effects of depression on patient health, as well as its associated economic burden has been warranted [1]. Depression can increase healthcare costs [2], suicide [3] and disability [4] while it decreases quality of life and physical function. It has been suggested that prompt diagnosis and treatment can help control symptoms and minimize dysfunction for patients with mental illnesses [5]. Commonly used diagnostic tools [6] include interview style assessment such as the Hamilton Rating Scale for Depression (HRSD) [7], and selfreporting tools such as the Beck Depression Inventory [8] and the Patient Health Questionnaire depression scale (PHQ-8) [9] designed to assess the intensity of symptoms associated with psychoanalytic aspects of depression, e.g., sleep disorder, feelings and social withdrawal. These assessments rate the severity of symptoms and behaviours observed in depression to give a patient a score which relates to the level of depression. Several studies demonstrated that such approaches do not capture the complexity of the phenomenon, nor do syndromal models of depression [10], moreover, many studies have raised concerns about over-diagnosis and misdiagnosis of depression [11].

To support objective depression assessment, the affective computing community engaged signal processing, computer vision and machine learning approaches for analysing verbal and non-verbal behaviour of depressed patients [12] and made predictions about what patterns should be indicative of depressed state [13], [14], [15], [16]. These studies have analysed the relationship between objective measures of voice, speech, non-verbal behaviour and clinical subjective ratings of depression severity for the purpose of automatic depression assessment. Various audio visual and text features, as well as depression classification models and estimation models, have been explored and obtained promising performance, especially in the Audio Visual Emotion Challenges (AVEC) [17], [18].

Within the past several years, despite that significant progresses have been made in audio/video depression analysis, due to the complexity of depression disorder and differences among individuals, there still exist two main challenges in the study of depression: 1) More discriminative multi-modal features, which can better reflect the characteristics of depression disorder, need to be explored and fused. Most of recent works concern the audio and video paralinguistic information, such as speaking rate, facial action units (AUs), rather than the linguistic information from the spoken content, which can directly reflect the sleep status, emotional status, feeling and other psychoanalytic symptoms of the individual. Besides the audio and video features reflecting non-verbal behaviour information, more discriminative text features are also required for automatic depression assessment. 2) Although huge progress in machine learning has been achieved recently, performances of depression assessment from the most commonly used shallow and static models, such as Support Vector Machine (SVM), Decision Trees, Gaussian Mixture Model (GMM), are still not satisfactory. More powerful models, which are capable of modelling the characteristics of depression embedded in the audio, visual and text descriptors, are needed to improve the performance of depression analysis.

For the first challenge, besides the usually used vocal prosody features and facial features like Local Phase Quantisation (LPQ), Local Gabor Binary Patterns from Three Orthogonal Planes (LGBP-TOP), and Histogram of Oriented Gradients (HOG), some researchers investigated audio and visual features based on the conclusions of cognitive psychology research. It has been proved that patients with depression disorders have psychomotor retardation (PMR), involving a slowing-down of thought and a reduction of physical movements [19], [20], [21]. Based on the above studies, Williamson et al. [22] proposed multi-scale correlation structure and timing feature sets from audio-based vocal features and video based facial action units, and won the depression sub-challenge of AVEC2014. Syed et al. [23] computed velocity and acceleration contours from facial landmarks, targeting head movement, mouth movement and eyelid movement. In this work, we consider the displacement in the horizontal and vertical directions of the facial landmarks along the video sequence in a displacement range histogram, and propose a novel visual descriptor, named Histogram of Displacement Range (HDR), to describe the movement of facial landmarks.

On the other hand, evidences show that the disorder of biochemical substances in the brain causes the sleep patterns of patients with depression are quite different from those of the controlled people. However, such physiological related information is difficult to be observed from audio and visual cues. Unlike the paralinguistic information, linguisitc information during the interview questionnaires, such as HRSD, is more direct to indicate these physiological conditions of depression. Therefore, linguistic information can also be used for indicators of psychological health [24]. Sentiment or text analysis emerged as a new approach in studying emotion-related textual indicators as well as depression symptoms such as negative schemas and self-focus ruminations. Such approaches, which have been used for analysing emotional well-being in social networks [25], [26], [27], [28], were recently combined with non-verbal behaviour for depression recognition [29], [30], [31]. In [32], the authors explored the potential of using text information to detect major depressive disorder in individuals. Application of topic and sentiment modelling was presented in [33] for online therapy for depression and anxiety. It was found that besides the discussion topic and sentiment, style and dialogue structure are also important for measuring the patient's status. In our previous research [31] on AVEC2016, we also found that linguisitc (transcript) information related to the psychoanalytic symptoms (such as sleeping disorder, feelings, personality.) greatly improves the depression classification performance. In this work, we adopt the paragraph vector (PV) [34], an extension of the Word-to-Vector (Word2Vec) model, to learn a fixed-length feature representations of a text segment.

For the second challenge, currently few studies consider both depression classification and severity level estimation at the same time. However, there is an implicit relationship between depressed/not-depressed classes and the severity level of depression. The higher the depression level (such as PHQ-8 score) is, the more likely the individual is depressed. Therefore, depression estimation and classification can be integrated together to improve the accuracy of depression assessment. Existing investigations also show that when depression classification and estimation are considered together, better performance could be obtained [22], [35], [36]. In this paper, we propose a multi-modal framework for predicting the PHQ-8 score, and classifying an individual as depressed or not depressed, by hybridizing deep models and shallow methodologies. The deep models are adopted for modelling the complex non-linear relationship between depression severity and the audio-visual non-verbal behaviour. While the PV based features, extracted from the conversational transcripts, are a kind of high level descriptor reflecting the life status (or mood) of the individual directly, shallow models are adopted to characterize the relationship between the text features and depression.

The contributions of our work are in three folds:

A hybrid depression classification and estimation framework, combining from audio, video and text descriptors, is designed. We first leverage the powerful learning ability of deep convolution neural network (DCNN) and deep neural network (DNN) to estimate the depression severity (PHQ-8 score) from audio visual modalities, then make use of a shallow learner, as SVM, along with PV embedding to infer the physical and mental conditions of the individual from text information. Finally these information are fused via a random forest model to classify the individual as depressed/not-depressed.

We leverage a PV-SVM model to infer the physical and mental conditions from the text information of interviews. We select the patient's answers to the questions associated with psychoanalytic symptoms of depression, such as insomnia, feelings, and personality, and make use of PV embedding to obtain fixed-length feature vectors, which are subsequently fed into binary SVM classifiers for detecting the presence or absence of the considered psychoanalytic symptoms. To our best knowledge, this approach is the first attempt to apply PV for depression analysis.

We propose a new visual descriptor, named as Histogram of Displacement Range, which characterizes the displacement and velocity of the facial landmarks in the video segment by a histogram. This visual descriptor shows the range distribution of the displacements of facial landmarks. Experimental results prove that the HDR features obtain promising depression estimation performance.

The remainder of the paper is organized as follows. Section 2 gives an overview of previous works on automatic depression assessment. The proposed audio visual and text features are introduced in Section 3. The hybrid architecture for depression estimation and classification is described in Section 4, including three main modules: the audio-video DCNN-DNN based depression estimation module is addressed in Section 4.2, Section 4.3 describes the psychoanalytic symptoms classification module from text information via the PV-SVM models, and the final depression classification module is described in Section 4.4. Section 5 analyses the performance of our proposed framework on the AVEC2016 depression challenge dataset. Finally, conclusions and further research work are discussed in Section 6.

SECTION 2Background and Related Work
2.1 Features
Various audio and visual features have been explored for depression recognition. With respect to audio, researchers have found that depressed subjects are prone to possess a low dynamic range of the fundamental frequency, a slow speaking rate, a slightly shorter speaking duration, and a relatively monotone delivery [37], [38] [39]. Furthermore, Cohn et al. [40] investigated the relationship between depression severity and vocal prosody such as switching pause and vocal fundamental frequency. After several weeks’ tracking and treatment, it was strongly supported that quantitative features of vocal prosody in depressed participants reveal the changes in symptom severity over the course of depression. Consequently, features describing subtle changes in speech characteristics (e.g., differences in pitch, loudness, speaking rate, articulation.) have been used as indicators of depression. In AVEC2013 and AVEC2014 [17], [41], various functionals were performed on the low-level descriptors (LLDs) and used as the baseline audio features, which were proven to be effective for depression recognition.

For the visual features, subtle expressions, facial periodical muscular movements and other body or facial movements, have been widely explored for depression analysis. In the Depression Sub-challenges of AVEC (2013-2016), LPQ, LGBP-TOP [42], and HOG features were adopted as the baseline features. Girard et al. [14] and Dhall et al. [43] investigated the relationship between nonverbal behavior and severity of depression using Facial Action Coding System (FACS) action units and head pose. They found that participants with high depression severity made fewer affiliative facial expressions (AU12 and AU15), more non-affiliative facial expressions (AU14), and diminished head motion. In [44], Scherer et al. analyzed nonverbal behaviour of subjects with psychological disorders, and concluded that face and eye gaze, as well as smile intensity and durations of smile, can discriminate between subjects. They proposed the vertical head and eye gaze directionality, smile intensity and average duration, as well as self-adaptors and leg fidgeting, as nonverbal behavior descriptors. The Depression Sub-challenge of AVEC2016 [18] also adopted facial landmark features, eye gaze and head pose as baseline visual features. Space temporal interesting point (STIP) features, describing the spatio-temporal changes by taking into account of the movements from facial area, hands, shoulder, and head, has been employed in [45], [46], [47] for depression classification, and excellent performances have been obtained.

To aggregate the audio/visual features from all the frames into a global representation, early research considered averaging the frame based features over the sequence for the subsequent regression/classification task [18]. In recent years, Bag-of-Words (BoWs) [46], Vector of Locally Aggregated Descriptors (VLAD) [48], and Fisher Vector [43] have been used. In [48], BoW and VLAD have been performed on the LGBP features or STIP features to obtain such global features. The histogram-based statistical methods are also widely adopted. In [49], Meng et al. used Motion History Histograms (MHH) to capture the movement of each pixel within the face area, describing temporal information during facial expression. In the audio process, MHH is also used to extract the change information of the vocal expression, followed by the Partial Least Square (PLS) regression to predict the depression scale. In [50], the authors proposed Histogram of Oriented Displacement (HOD) to describe the trajectories of body joints for action recognition. To construct HOD, each displacement in the trajectory votes with its length in an orientation angles histogram.

Apart from audio and visual features, some researchers analyzed depression from text information. Asgari et al. [51] explored the information from “what is said” (content) and “how it is said” (prosody). To extract features from the text, they used a published table to tag each word in an utterance with an arousal and a valence rating. Finally the speech prosody features and text features are fused to detect depression by a SVM classifier. Similar idea is also presented in [26], where Nguyen et al. extracted a variety of features: Affective features, Linguistic Inquiry and Word Count (LIWC) features, Topics and Mood tags, for analysing the textual content generated in social media. The first three features are word-based text features. Each word is assigned either an arousal/valence value or a probability of belonging to a certain category. It is obvious that analysing the whole semantics by a few words is defective. In the Mood tags text feature, the mood tag produced allows to directly access to the user sentiment. However in most cases, the tags are not provided. Therefore, a method for automatically analysing the entire sentence is in demand. In [29], the authors combined the speech-based and text-based features for depression recognition on the AVEC2014 dataset. The speech-based features include prosody features (F0, voicing probability, loudness contours) and speech-rate features. For the text-based features, they first performed automatic speech recognition using the Googles German Web Speech API. For each transcript, they generated n-grams from the word and character level, LIWC, part-of-speech (POS) tag n-grams and text-based speech-rate features. SVM has been used for depression recognition from these features. The authors also presented an extensive comparison between a text based system for depression detection and a speech-based system. For the text-based system, they found strong correlations between words and depression level.

2.2 Models
Within the past several years, various models have been investigated for depression classification and depression level estimation. Cohn et al. [16] adopted facial actions and vocal prosody features for clinical diagnosis of major depression, and adopted SVM and logistic regression to classify a person as depressed or not depressed. In [52], Alghowinem et al. compared four popular classifiers, namely GMM, SVM, Multilayer Perceptron Neural Networks (MLP), and Hierarchical Fuzzy Signature (HFS), for the task of depression detection. In [30], Pampouchidou et al. implemented a decision tree based depression classification by fusing the high level and low level features from audio, video and text modalities. Using the AVEC2014 [17] dataset for BDI-II score estimation, Jain et al. [53] focused on visual descriptors and Fisher Vector aggregation, along with GMMs for depression scores prediction, while Gupta et al. [54] adopted a support vector regression (SVR) to model the relationship between the audio, visual, linguistic information and depression scores. In [55], Mitra et al. modelled a wide range of vocal features and used a variety of approaches, including SVR, GMM and decision trees for depression recognition.

These studies, however, used relatively shallow architectures. Recently, the emotion recognition domain has highly benefited with the advent of Deep Convolutional Neural Network, Deep Neural Network and Long Short Term Memory - Recurrent Neural Network (LSTM-RNN) models. The reader is referred to the works of [56], [57], and the references therein for an overview of the main publications. Deep learning models have also been exploited for depression analysis. Using the AVEC2016 [18] dataset, Ma et al. [58] proposed a deep model to encode the depression related characteristics in the vocal channel. Mel-scale filter bank features within a speech segment are concatenated to construct a 2D representation, which is input into a convolutional neural network (CNN) model, followed by a LSTM to capture the long-range variability for depression prediction. In [59], Zhu et al. proposed a two stream fusion framework based on DCNNs. Facial appearance features and optical flow features are input into two DCNN models, separately, their output fully connected layers are then concatenated as the input into a network of two fully connected layers for depression level estimation. Dibeklioglu et al. [60] investigated depression severity measurement from facial movement dynamics, head movement dynamics, and vocal prosody. The authors recorded the interviews of 57 depressed participants (34 women, 23 men), which were conducted using the Hamilton Rating Scale for Depression. They used 3-layer Stacked Denoising Autoencoders (SDAE) [61] deep network architecture to encode per-frame representations of both facial landmark coordinates and head angles. Each SDAE was trained using the normalized and smoothed 49 facial landmark coordinates and smoothed 3 head pose orientations, respectively. By applying the learned encoding to each frame of the video, the SDAE-based outputs were combined into time series representation, from which they computed the dynamic changes over time in terms of velocity and acceleration.

It is worth noting that the methods mentioned above studied either depression classification, or depression estimation. In recent years, some works considered frameworks combining the two tasks to improve recognition accuracy. In [35], a SVM based classifier was first built to split the participants as depressed or not depressed, then for each class, a Generalized Linear Model (GLM) was trained as regressor to predict the BDI scores. Similar idea was adopted in [22], in which two Gaussian Staircase models were trained for depressed and not depressed, respectively. For a test sample, the extracted features were input into the two Gaussian Staircase models, whose output log-likelihood ratios were fused by a univariate regression model to estimate the depression levels. [36] further extended this depression estimation framework to depression classification by setting a threshold. In our previous work [31], we used the AVEC2016 dataset to predict the PHQ-8 scores from audio-visual cues with SVR and local linear regression (LLR), and analyzed the participant's depression symptoms from the text information. Then, gender specific decision-trees, combining PHQ-8 scores and text-derived depression symptoms, were built for depression classification. Experimental results showed that the frameworks combing depression classification and depression estimation obtained promising performance.

SECTION 3Audio, Video and Text Features
In this work, we use the AVEC2016 Depression Sub-Challenge dataset [18]. The dataset is organized in sessions each corresponding to a recording (audio/video/text) of the conversation between a participant and an animated virtual interviewer called Ellie. Each session consists of several segments each containing the recording of the participant answering one question from Ellie. From each segment we extract hand crafted descriptors as detailed in the following.

3.1 HDR-Based Video Features
In this paper, we exploit the 68 2D landmarks of the face provided by AVEC2016 to describe the facial motion information, and introduce a new global descriptor denoted as Histogram of Displacement Range. HDR records the range and speed of the displacements of the facial landmarks in the horizontal and vertical directions, and reflects facial dither. To construct HDR, we consider the displacement of the facial landmarks along the video sequence, each landmark displacement votes with its length, in the horizontal and vertical directions, in a displacement range histogram. The landmark displacements are described by a histogram of n equally spaced bins Ri(i=1,…,n) spanning the range [−30,30] pixels. For temporal modelling, we estimate the landmark's displacements in the horizontal and vertical directions at different time intervals Mk(k=1,…,K), and concatenate the obtained histograms. Formally, let Dx(i,j)=ji+Mk(x)−ji(x) and Dy(i,j)=ji+Mk(y)−ji(y) be the horizontal and vertical displacement of landmark j(x,y) between frame i and frame i+Mk, each bin of the histogram contains the number of occurrences of displacements in the corresponding range. Fig. 1 visualises the process of HDR, and gives an example when K=5. For each landmark, we calculate the difference between ji and ji+M, where M is the time interval. Then we make statistics on the elements falling in these six ranges and get the histogram when M=10,20,30,40 and 50. Finally, we concatenate these histograms, and obtain the HDR features as shown in the lower right corner of Fig. 1. Algorithm 1 summarizes the HDR estimation process.


Fig. 1.
The proposed HDR feature.

Show All

Algorithm 1. Histogram of Displacement Range
input:

Set time interval M := {M1, M2, M3,…,Mk};

Set the range R := {R1, R2, R3,…,Rn};

Time series features: Landmarks;

output:

Histogram of Displacement Range: N(Mk,j,Rn);

for each Mk do

for each landmark j(x,y) and

i≤totalframe−Mk do

Dx(i,j) := ji+Mk(x) - ji(x);

Dy(i,j) := ji+Mk(y) - ji(y);

for each Rn do

Numx := Dx(i,j)∈Rn;

Numy := Dy(i,j)∈Rn;

Numx := Numx/(totalframe−Mk);

Numy := Numy/(totalframe−Mk);

N(Mk,j,Rn) := [N(Mk,j,Rn) Numx Numy];

end

end

end

For our experiments, we considered 5 time intervals (Mk), as 10, 20, 30, 40, and 50 frames, respectively. We find that males show less head movements and facial landmarks movements than females. Therefore, with respect to the histogram bins, we considered 6 equally spaced bins (Ri), spanning the range [−30,30] pixels for females, and 4 equally spaced bins spanning the range [−20,20] pixels for males. In total, for each segment, we obtained 4080 HDR features for females, and 2720 for males.

3.2 Audio Features
For the audio features, considering the Geneva Minimalistic Acoustic Parameter Set (GeMAPS) [62] and the INTERSPEECH Challenges [63] feature sets, we analysed the baseline features of the challenge over the years and create a union feature set. We utilize the openSMILE toolkit [64] to extract for each speech segment, 238 low level descriptors, comprising of 211 spectral and energy related features and 27 voicing related dynamic features. The LLDs are shown in Table 1 where the numbers between brackets are the dimensions of the extracted features vectors, and △, △△ denote the first and second order derivatives, respectively. The LLDs are extracted with the frame length of 60ms and frame shift of 10ms. 25 statistical functionals and 4 regression functionals, as shown in Table 2, have been performed on the extracted LLDs, resulting in a 6902 dimensional feature vector for each speech segment.

TABLE 1 The Selected Low Level Descriptors from openSMILE (238)
Table 1- 
The Selected Low Level Descriptors from openSMILE (238)
TABLE 2 Functionals Performed on the Selected LLDs (29)

3.3 Paragraph Vector for Text Transcripts
The paragraph vector [34] model is an extension of the Word2Vec model of distributed representations of words in a vector space. It is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, or documents. PV acquires semantically meaningful representations for words and documents and represents each document by a dense vector which is trained to predict the words of the document. Compared to the Bag-of-Words (BoW) model, this approach takes into account the order of the words in a sentence. PV has been widely used in sentence classification and sentiment analysis [65], [66]. Especially, in [66] Andrew et al. provided a more thorough comparison of PV to other document modelling algorithms and concluded that PV can perform more useful semantic results than other methods, such as Latent Dirichlet Allocation (LDA), BoW and Averaged word embeddings.

The framework of learning PV is illustrated in Fig. 2. For a given word sequence (w1,w2,w3,…,wn), suppose the word wi is represented by a vector Vwi, which is a column of the matrix W=(Vw1,…,Vwi,…,Vwn), and the paragraph pj is mapped to a paragraph vector Vpj of the paragraph matrix D=(Vp1,…,Vpj,…,Vpm), the paragraph vector and word vectors are averaged or concatenated to predict the next word in a context.


Fig. 2.
Framework for learning paragraph vector (adapted from [34]).

Show All

The objective of the paragraph model is to maximize the average log probability of any given word in a sequence of training words w1,w2,w3,…,wn, conditioned on the appearance of the other words of the same sequence [34]
1n∑t=kn−klogp(wt|wt−k,…,wt+k).
View SourceRight-click on figure for MathML and additional features.

The above definition describes a prediction task, which is usually solved via the usage of a multiclass classifier such as softmax
p(wt|wt−k,…,wt+k)=eywt∑ieyi,(1)
View SourceRight-click on figure for MathML and additional features.where each term yi is the un-normalized log-probability for each output word i, computed as
y=b+Uh(wt−k,…,wt+k;W,D),(2)
View Sourcewhere U,b are the softmax parameters and the function h is constructed by a concatenation (or average) of the word vectors extracted from the matrix W and D. The hierarchical softmax [67] is preferred to softmax for fast training.

The paragraph vector can be considered as a summary of the whole paragraph. It acts as a memory that remembers what is missing from the current context, or the topic of the paragraph. An important advantage of paragraph vectors is that they are learned from unlabeled data. They also inherit an important property of the Word2Vec model, being the semantics of the words. Moreover, they take into consideration the word order.

SECTION 4Hybrid Architecture Integrating Deep and Shallow Models
4.1 Depression Estimation and Classification Framework
As described above, from the recorded segments of the dialogue between the participants and Ellie, audio, visual and text features have been extracted. The audio and visual features are descriptors of of non-verbal behaviour, while depression may cause psychomotor retardation which will influence the behaviours like speaking rate or facial movements, the relationship between depression severity and the audio visual features is very complex and implicit. Therefore for estimating the depression severity level from audio visual cues, deep models are expected to model the complex non-linear relationship between depression and the audio visual features. The recent empirical success of deep learning architectures suggest that their compositional/hierarchical structure induces increasingly invariant data representations by progressively flattening and separating the manifold-shape of the data [68]. This property allows deep learning models to have an advantage over other shallow models in analysing high-dimensional data such as video and audio. Especially, the DCNNs have been successfully applied in many computer vision tasks such as object recognition [69] and face verification [70]. Recently they have been also adopted in emotion recognition [71]. DCNN not only characterizes large data variations but also learns a compact and discriminative feature representation. On the other hand, DNNs also have the strong ability of capturing complex nonlinear relationships between variables, and have performed extremely well in a variety of regression and classification problems, such as speech recognition [72], speech synthesis [73] and emotion recognition [74]. In this work, we propose a multi-modal DCNN-DNN framework for estimating depression severity from audio and visual cues.

On the other hand, the PV based text features extracted from the answers to the questions in the interview are a kind of high level descriptors, which reflect the physical or mental states of the individual directly. Therefore shallow models are adopted to characterize the relationship between the text features and depression. In this work, we propose a PV-SVM framework to classify the psychoanalytic symptoms from the transcripts of the answers to the questions.

Finally, we propose a depression severity estimation and depressed/not-depressed classification architecture by hybridizing the deep and shallow models, as shown in Fig. 3. It consists of three modules: 1) The audio-video DCNN-DNN based depression estimation module, where hand-crafted audio/video features are first input into respective DCNNs to learn high-level features, which are then fed to a DNN for the initial estimation of PHQ-8 score. The predicted PHQ-8 scores are input to a decision fusion DNN for final estimation. 2) The psychoanalytic symptoms classification module using PV-SVM model to detect the presence or absence of the considered psychoanalytic aspects, such as sleeping disorder and feelings, from the transcripts of the participant's interviews. 3) The depression classification module, in which the estimated PHQ-8 score and psychoanalytic symptom classification results are input together into a Random Forest model to classify the individual as depressed/not-depressed. In the following sections, we will give detailed descriptions of each module.

Fig. 3. - 
The proposed hybrid framework for multi-modal depression estimation and classification.
Fig. 3.
The proposed hybrid framework for multi-modal depression estimation and classification.

Show All

4.2 DCNN and DNN Based Audio Visual Depression Estimation
4.2.1 Single Modal Depression Estimation
The audio/visual single modal DCNN-DNN framework is shown in Fig. 4. We first train the DCNN model. For each audio or video segment, we pass the corresponding feature descriptors through a DCNN, and add a fully-connected layer to produce the prediction score of PHQ-8. After training the DCNN, we freeze the weights, discard the last layer and connect the second fully connected layer to the visible layer of the DNN. Here also, the DNN is use to predict the PHQ-8 score.

Fig. 4. - 
DCNN and DNN based single-modal depression estimation framework.
Fig. 4.
DCNN and DNN based single-modal depression estimation framework.

Show All

Because of the small amount of training data, strategies have to be considered to avoid the over-fitting problem. In our current implementation, the DCNN and DNN are trained separately, without back-propagating the loss of DNN to the DCNN. This separate training manner can benefit from mitigating the complexity of the model, while updating all the parameters in DCNN and DNN in a nutshell will cause over-fitting. Moreover, in this work, the DCNN model performs as a high-level feature extractor to learn the compact and discriminative feature representation from audio/visual descriptors, therefore it is suitable to train the DCNN and DNN models separately, instead of training them together.

4.2.2 Multi-Modal Depression Estimation
With respect to the multi-modal fusion strategies, feature level fusion is a straightforward way where features of all modalities are concatenated into a single high-dimensional feature vector. However, feature fusion can exacerbate the high dimensionality, and models trained with such concatenated feature vectors are not beneficial to capture the property of each modality. On the contrary, a decision fusion approach, which fuses the outputs from the single modalities, can capture the property of each modality and achieve better performance [75].

In this work, we consider a decision fusion approach for audio visual multi-modal depression estimation, as shown in the upper part of Fig. 3. The single modal estimations of the PHQ-8 score from the output layers of the audio DNN and visual DNN are input into a fusion DNN model, whose output is the final PHQ-8 score. For training the fusion DNN, the single modal estimations of the PHQ-8 scores on the training set are used as the input layer of the DNN, along with the corresponding ground-truth PHQ-8 labels as output layer for supervision. The structural hyper-parameters of the used DCNNs and DNNs will be discussed in Section 5.2.

4.3 PV-SVM Based Psychoanalytic Symptoms Classification
The transcript files provided by the AVEC2016 dataset include time stamps and dialogue contents. An example is given in Table 3. As in our previous work [31], we conduct content analysis on the participant's answers to the questions associated with psychoanalytic aspects of depression. Five aspects have been considered in this study:

Prior depression diagnosis (Yes/No)

TABLE 3 An Example of the Question-Answer Pairs in the Transcript Files

Prior post-traumatic stress disorder (PTSD) diagnosis (Yes/No)

Sleep disorder (Yes/No)

Feelings (Bad/Good)

Personality (Extrovert/Introvert).

As illustrated in the lower part of Fig. 3, the PV descriptors of the participant's answers to the five considered topics are fed to five binary SVMs trained to automatically classify the presence/absence of the considered psychoanalytic symptoms. To train the SVMs, we first conduct content analysis of all the training data and label the considered psychoanalytic symptom as presence/absence as follows. E.g., for the answers related to “sleep disorder”, we label an answer as Yes (1) if the participant's answer contains the following expressions “not had a good sleep”, “really hard”, “kind a difficult”, “never easy”; the label is No (0) if the answer contains “no problem”, “pretty good”, “get a good night sleep”, and some answers which express the same meaning.

4.4 Multi-Modal Depression Classification
For the multi-modal depression classification, as illustrated in the right side of Fig. 3, the predicted PHQ-8 score from the audio-video cues and the binary outputs of the five SVMs transcripts classifiers are fed into a Random Forest (RF) [76] for depression classification. According to the comprehensive experiments conducted in [77], Random Forests achieve the satisfactory recognition performance in most cases, especially when dealing with high-dimensional problems.

In the Random Forest model, there is only one single tuning parameter - the forest size, therefore this model is easy to tune and not sensitive to parameters. In addition, since the features are randomly selected during constructing each decision tree and the growth of each decision tree is also random, the Random Forest model is robust. It has inherent support for multi-class problems and is capable of dealing with high dimension features [77]. At the same time, it has low computational cost and is prone to avoid over-fitting problems, which render it suitable to our classification problem.

In training the Random Forest classifier, we use the ground truth Depressed (1) or Not-Depressed (0) labels as supervision, while for testing, the Random Forest outputs a real value p∈[0,1]. A threshold (thd) is adopted for the final classification of depressed/not-depressed, as indicated in the right side of Fig. 3. A grid search method is adopted to estimate thd, which will be discussed in Section 5.2 along with the other experimental parameters.

SECTION 5Experiments and Analysis
In this section, we carried out experiments on the AVEC2016 depression dataset. We trained gender specific models, and performed depression estimation and classification experiments for female and male, respectively. Finally the results from the gender specific models were combined for the overall evaluation on all participants.

5.1 Balancing of the Depressed/Not-Depressed Training Samples
The AVEC2016 depression dataset [78] consists of 189 sessions of clinical interviews. For each session of the training and development sets, a PHQ-8 score ranging from 0 to 24, and a binary value of depressed/not-depressed are given as labels. The training set contains sessions of 31 not-depressed and 13 depressed females, 55 not-depressed and 8 depressed males. The imbalance between the depressed and not-depressed samples, as shown on the left column of Fig. 5, may decrease the recognition performance and cause over-fitting. Therefore, we propose a re-sampling strategy to balance the training samples. According to the given time-boundary information during the conversation, we first remove the “non-speaking” segments when the participant listens to Ellie but does not speak. Then for the “depressed” samples, from the “speaking” segments, the longest four segments of each session are taken as new samples for females, and the longest five segments of each session are taken as new samples for males. For the “not-depressed” samples, the longest four segments of each PHQ-8 score are taken as new samples for both females and males. This results in 46 “not-depressed” segments and 48 “depressed” segments for female. For male, 48 “not-depressed” segments and 40 “depressed” segments have been obtained, as shown in Table 4. The distribution of the re-sampled data is shown on the right column of Fig. 5. These balanced segments are used to train the depression estimation models and the depression classification model.

Fig. 5. - 
Training samples balancing.
Fig. 5.
Training samples balancing.

Show All

TABLE 4 Training Samples of Depressed/Not Depressed(in the Brackets Are After Sampling)
Table 4- 
Training Samples of Depressed/Not Depressed(in the Brackets Are After Sampling)
Moreover, in the “non-speaking” segments, even though the participants are not speaking, they still may show facial expressions. Therefore for video based depression recognition, we also tested the performance of estimating the PHQ-8 score from the “non-speaking” segments. For the “non-speaking” segments, we apply the same re-sampling method as that for the “speaking” segments, and the balanced samples are used to train the DCNN and DNN models.

5.2 Model Parameters
5.2.1 Training of the DCNN-DNN Models
In the AVEC 2016 depression dataset, for each session of the training and development sets, a single real value PHQ-8 score is given as the ground truth label. This label is assigned to all the audio/visual segments within the session. In our implementation, we use the provided segments, along with their corresponding labels, to train the DCNN and DNN models.

Most DCNNs adopt images as inputs to learn the high-level abstract features. In this work, we feed the effective hand-crafted features (audio functional features and video HDR features) to the DCNNs. By adding the domain knowledge to the DCNN model, it will help increase the performance and save the computation costs. For designing the single modal DCNN-DNN networks, we have tested several network architectures and selected the ones which provided the best PHQ-8 prediction, in terms of root mean square error (RMSE) and mean absolute error (MAE), on the development set. In this work, we did not make the greedy search to tune all the parameters deeply, but set the parameters within the following ranges. For the DCNN model with audio or HDR features as inputs, and the ground-truth labels of PHQ-8 scores as the supervision information, we evaluated architectures with 3, 4 or 5 convolutional layers, respectively. Each convolutional layer is followed by a ReLU, MaxPooling and Dropout layer, except for the last convolutional layer, which is followed by 2 fully connected layers. The convolution kernel size has been set as 1×5 and stride=1, and the number of feature maps Nmaps∈{12,24,48,64,128}. For the MaxPooling layer, the pooling kernel size was set as 2×2, with stride=2. In each Dropout layer, we evaluated dropout ratios as d∈{0.3,0.5}. For the fully connected layers, the number of hidden nodes was set as Nnodes∈{15,20,25,30,35,50,100,150}. The euclidean loss was chosen as loss function. All of the DCNNs were trained using stochastic gradient descent with batch size of 25, momentum of 0.85, weight decay of 5e-4, and a learning rate of 2e-5.

Once the audio/video DCNN model is trained, the outputs of the second fully-connected layer are fed into a DNN model, whose output layer is the PHQ-8 score. We evaluated several DNN structures, with the number of layers Ndlayer∈{3,4,5}, and for each layer, different number of hidden nodes have been tested Nnodes∈{15,20,25,30,35,50,100,150}. The ReLU was used as activation function, and the euclidean loss as loss function.

For the audio-video fusion DNN, we compared two kinds of fusion strategies: decision fusion and model fusion. The decision fusion framework is as shown in the upper part of Fig. 3, in which the single modal estimations of the PHQ-8 score are input into a fusion DNN model. We evaluated several models with number of hidden layers Ndlayer∈{2,3,4}, and the number of hidden nodes in each layer as Nnodes∈{2,4,6,8,10,15,20}. In the model fusion scheme, the DCNN features from the last fully connected layers of the audio-DCNN and HDR-DCNN are input together to a DNN for fusion. When training the model fusion scheme, we adopted the same dataset after balancing the depressed/not-depressed training samples as describe in Section 5.1, and evaluated several DNN structures: Ndlayer∈{4,5,6,7}, and for each layer, different number of hidden nodes have been tested Nnodes∈{30,40,50,100,150,200,250}. Eventually, the selected architectures which obtained the best results on the development set is Ndlayer=6, with the number of nodes for each layer as {200,200,200,200,200,30}. All of our DCNN and DNN models have been trained using Caffe [79]. In our experiments, we adopted an early-stop strategy. If the loss no longer decreased in two hundred consecutive iterations, the training process was stopped. Moreover, in the iterations of the training process, we observed the loss of the training set and development set. If the loss continued to decrease in the training set but began to increase in the development set, we thought overfitting occurred and stopped the iterations. Fig. 6 presents the temporal curves of loss during the training procedure. One can see that for both audio and video, the losses from the DCNN models or DNN models converged very quickly, both on the training set and development set, showing that overfitting did not occur in the models.


Fig. 6.
Graph of loss.

Show All

For testing (prediction), audio and visual features from the longest segment of a testing session are used as inputs for multi-modal PHQ-8 score estimation. This measure is mainly considered from three aspects: 1) Depression is a relatively stable disorder and will not have large fluctuations in a short time period, it is feasible to use the longest segment to represent the symptoms embedded in all the segments. 2) At the beginning of the interview, to accommodate quickly the participants to the environment, Ellie will chat to the participant with some other questions that are not related to the psychoanalytic symptoms. These segments can be skipped in inferring the depression status. 3) As mentioned above, in order to balance the samples, we use several longest samples to train the models. Here to keep consistent, we just select the longest segment rather than using all segments to predict the PHQ-8 score.

5.2.2 Training of the PV-SVM Models
In this work, we train the PV models with 402,325 dialogues which are collected from Friends, The Big Bang Theory and Game of Thrones. Most of these dialogues are simple and short, which are similar to the conversations between the participants and Ellie. The parameters used in training the PV models are: the dimension L of the vector for each segment, which is set as L∈{50,100,150}, and the window size S which is set as S∈{5,10}. For selecting the SVM parameters cost and gamma, we adopt a grid search approach within 2y where y∈[−12,12].

5.2.3 Training of the Random Forest Model
For the Random Forest, the only hyper-parameter in our experiment is the number of trees which is set as Ntrees∈{1,2,3,4,5,10,15,20}. The F1 score of the depression classification results on the development set has been used to select the best model. A grid search within [0.1, 0.9] with a step of 0.0005 is used to select the threshold parameter thd for depression classification. In our experiments, the best results have been obtained with a random forest of 5(2) trees and thd=0.727(0.449) for female (male).

The experiments have been implemented on a Tesla C2075 GPU machine. Due to the very limited size of the dataset, the time consumed for training is very short. The architectures and parameters, which provide the best performance on the development set, are selected as shown in Table 5, where for the convolution layers we indicate the number of feature maps per layer, and for the fully-connected layers and DNNs we indicate the number of nodes per layer.

TABLE 5 Selected (Best) Models
Table 5- 
Selected (Best) Models
5.3 DCNN-DNN Based Depression Estimation
For depression estimation, RMSE and MAE are adopted to measure the estimation accuracy of the PHQ-8 scores. Single modal PHQ-8 score estimation results from DCNN-DNNs for females and males on the development set are shown in Table 6, along with comparison to other models. For Audio-DCNN or HDR-DCNN, the DCNN model is used as an estimation model directly, applied to the hand-crafted global audio features, or HDR features. For the DCNN-SVR model, the hidden layer features from the DCNN are input to an SVR for PHQ-8 estimation. From Table 6, one can notice that: 1) For the video modality, better recognition performance is obtained on the “speaking” segments compared to the “non-speaking” segments. This is reasonable because during speech, people show richer facial expressions than when they are listening. 2) The proposed DCNN-DNN framework outperforms DCNN and DCNN-SVR models, indicating that DNN performs well for regression. It also shows that when DCNN is used as a feature extractor, the decoding ability of the deep model (DNN) is superior to shallow model (SVR). 3) For both audio and video modalities, high-level features obtained from DCNN perform better than the hand-crafted features, such as speech covariance feature (COVAREP), geometric features with principle component analysis for feature reduction (GEO-PCA), and HOG features, showing that DCNN has the strong ability of extracting the high level abstract features.

TABLE 6 Single-Modal Estimation of PHQ-8 on the Development Set

In the following experiments, all the training samples are the balanced samples from the “speaking” segments.

Table 7 compares the performance of HDR (HDR-DCNN-DNN) with those of Bag-of-Words(BoW-DCNN-DNN) and Motion History Histograms(MHH-DCNN-DNN) [49], together with the parameters (threshold (th) and the number of frames (m) for MHH, the number of clusters (K) for BoW, and the number of ranges (R) for HDR). One can see that: 1) HDR obtains the best performance among the three features. 2) For both BoW and HDR, the depression estimation performances on females outperform those on males, but opposite results are obtained in MHH. This indicates that HDR and BoW are easier to capture the landmark movements than MHH, because in our experiments, we have found that females show more head movements and facial landmarks movements than males. To further compare the performance of these features, we utilize t-SNE [83] technique for dimensionality reduction and visualization of the latent representation as shown in Fig. 7. One can see that the HDR feature is indeed more discriminative than the other two features.

Fig. 7. - 
t-SNE visualization of HDR, BOW and MHH.
Fig. 7.
t-SNE visualization of HDR, BOW and MHH.

Show All

TABLE 7 Comparing HDR with BoW, MHH on the Development Set

For the decision-fusion based audio visual multi-modal depression estimation experiments, we use the architecture as depicted on the top left part of Fig. 3, and use the output of the audio-DCNN-DNN and HDR-DCNN-DNN from the “speaking” segments to perform the final PHQ-8 prediction. In Table 8, we compare the obtained results to the model fusion scheme. One can see that the proposed decision fusion scheme obtains better performance with lower RMSE and MAE.

TABLE 8 Multi-Modal Estimation of PHQ-8 on the Development Set

Finally, considering all participants, Table 9 lists the PHQ-8 score prediction results obtained using the proposed audio visual models compared to state of art methods. From the table we can conclude that: 1) For the single modal estimation, the proposed DCNN-DNN frameworks outperform the results reported in [36] and [80] through a single feature, i.e., the proposed HDR based landmark features, rather than the three kinds of video features reported in [80]. 2) The proposed audio visual fusion model outperforms most of the state-of-art models where apart from the audio and visual features, text features and semantic information have been utilized to improve the fusion performance. 3) In multi-modal fusion framework, the work in [81] achieved better performance than our proposed decision fusion approach, however the text modality in [81] improved the performance greatly, where in our decision fusion framework, we just fused the audio and video modalities. 4) Although [31] obtained the best performance on the development set, it performed the worst on the test set, showing that the model was over-fitting seriously. On the other hand, the proposed decision fusion framework based on DCNN and DNN obtains close results on both the development and the test sets, and achieves the best overall performance.

TABLE 9 PHQ-8 Estimation for All Participants

5.4 PV-SVM Based Psychoanalytic Symptoms Classification
The classification accuracies of psychoanalytic symptoms from the paragraph vectors and SVM models are listed in Table 10, together with the dimension L of the paragraph vector and the window size S. One can see that: 1) With the parameters (L=50,S=10) and (L=100,S=5), the PV-SVM frameworks obtain the same (the highest) average classification accuracies. Considering the two main psychoanalytic aspects - prior PTSD diagnosis and depression diagnosis, the performance with (L=100,S=5) obtains the best performance, therefore we choose this parameter setting for the classification of the psychoanalytic symptoms. 2) The parameter L is more sensitive than S. With the gradual increasing of L, the average accuracy rate is decreasing, especially when L=150 where some redundant features might emerge. 3) The overall classification accuracies are satisfactory. In addition, the classification accuracies for the questions related to prior diagnosis of depression and PTSD are very high compared to the other topics (sleep, feelings, and personality), with 84.21 and 100 percent for female, 81.25 and 93.75 percent for male, respectively. This is mainly due to the fact that in the case of the prior diagnosis of depression and PTSD, the participant answers with discriminative words, such as “Yes” or “No”, while for Sleep disorder, Feeling and Personality, the answers are often more complex and ambiguous.

TABLE 10 Classification Accuracy(%) of the Psychoanalytic Symptoms on the Development Set

Additionally, to evaluate the domain transferability of the PV features, some unrelated contexts, which are quite different from the dialogue of AVEC2016, i.e., 220,000 English news and 220,000 song lyrics, are adopted to train a new PV model. The psychoanalytic symptoms’ classification accuracy obtained from the new PV-SVM model are listed in Table 11. One can notice that: 1) Although the adopted texts are quite different from AVEC2016, the classification results in Table 11 are very similar to that in Table 10, indicating that the PV based text features have an excellent ability on domain transferability. 2) In some cases, e.g., L=50,S=5, the classification accuracy is a little higher than Table 10, this may be mainly caused by the initialization of the PV model and the difference in the size of the training data.

TABLE 11 The Psychoanalytic Symptoms’ Classification Accuracy(%) Obtained from a New PV Model Trained by Unrelated Contexts

5.5 Depression Classification Results
The estimated PHQ-8 score of Section 5.3 and text-based classification results of Section 5.4 are input into a Random Forest model for depression classification. The confusion matrix on the development set is shown in Table 12. As it can be seen, among the 35 participants of the development set, only 6 participants have not been correctly classified. For females, all the “depressed” participants are successfully detected.

TABLE 12 Confusion Matrix of Depression Classification on the Development Set

The F1 score, precision, and recall for depressed and not-depressed (indicated between brackets) classes are reported in Table 13, and compared with the state-of-the-art results on the AVEC2016 depression dataset. The indicated values are as reported by the authors of the mentioned publications. We can see that on the test set, the proposed hybrid depression estimation and classification framework obtains the best results. The average F1 is up to 0.746, which is even higher than that in our previous work (0.724) [31] who won the AVEC2016 depression sub-challenge. To measure the significance levels of improvement over the state of the art methods, we let the null hypothesis affirm the equal behavior in terms of F1 between our proposed method and the competitor methods, a p-value of 0.47 percent is obtained. This small p-value rejects the null hypothesis with errors lower than 5 percent and support the alternative hypothesis that the F1 of the proposed method is significantly higher than those of the state of the art methods. Moreover, even though that [31] obtained very promising results for both depressed class and not depressed class on the development set, over-fitting problem occurred on the test set. The proposed hybrid framework of this paper achieves close results on the development and test sets, indicating to some extent that the framework avoids the over-fitting problem and has more robust performance.

TABLE 13 Depression Classification Results on the Development Set and Test Set
Table 13- 
Depression Classification Results on the Development Set and Test Set
SECTION 6Conclusions and Future Works
We presented a hybrid framework design for improving the accuracies of depression estimation and depression classification. In this hybrid framework, the DCNN-DNN models effectively improve the accuracy of depression estimation, and the PV-SVM based text classification provides a novel idea for depression analysis from text information. As far as we know, our approach is the first which applies Paragraph Vectors to textual transcripts for depression analysis. The complete DCNN-DNN and PV-SVM framework integrates the low level audio visual features and high level text information. Experimental results on the AVEC2016 depression dataset demonstrate that the proposed method effectively improves the accuracies of both depression estimation and depression classification. Further, comparing with our previous work [31], the proposed hybrid framework mitigates the impact of over-fitting.

In addition to the aforementioned contributions, we propose a novel hand-crafted visual feature named Histogram of Displacement Range to represent the range and speed of the displacements of facial landmarks. Experimental results prove that the HDR feature obtains better performance than BoW and MHH.

We conclude that our proposal of the DCNN-DNN and PV-SVM based framework provides an effective method for depression analysis. It not only makes full use of the deep and shallow models, but also integrates the advantages of depression estimation and classification. We believe that this can bring a new way for depression analysis. For the future work, we are going to collect a multi-modal depression database for Chinese patients, and improve the models for unipolar/bipolar depression classification.