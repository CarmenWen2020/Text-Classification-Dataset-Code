Structured prediction is used in areas including computer vision and natural language
processing to predict structured outputs such as segmentations or parse trees. In these
settings, prediction is performed by MAP inference or, equivalently, by solving an integer
linear program. Because of the complex scoring functions required to obtain accurate
predictions, both learning and inference typically require the use of approximate solvers.
We propose a theoretical explanation for the striking observation that approximations based
on linear programming (LP) relaxations are often tight (exact) on real-world instances.
In particular, we show that learning with LP relaxed inference encourages integrality of
training instances, and that this training tightness generalizes to test data.
1. Introduction
Many applications of machine learning can be formulated as prediction problems over
structured output spaces (Bakir et al., 2007; Nowozin et al., 2014). In such problems
output variables are predicted jointly in order to take into account mutual dependencies
between them, such as high-order correlations or structural constraints (e.g., matchings
or spanning trees). Unfortunately, the improved expressive power of these models comes
at a computational cost, and indeed, exact prediction and learning become NP-hard in
general. Despite this worst-case intractability, efficient approximations often achieve very
good performance in practice. In particular, one type of approximation which has proved
effective in many applications is based on linear programming (LP) relaxation. In this
approach the prediction problem is first cast as an integer LP (ILP), and then the integrality
constraints are relaxed to obtain a tractable program. In addition to achieving high
prediction accuracy, it has been observed that LP relaxations are often tight in practice.
That is, the solution to the relaxed program happens to be optimal for the original hard
problem (i.e., an integral solution is found). This is particularly surprising since the LPs
have complex scoring functions that are not constrained to be from any tractable family. It
has been an interesting open question to understand why these real-world instances behave
so differently from the theoretical worst case.
This paper addresses this question and aims to provide a theoretical explanation for the
frequent tightness of LP relaxations in the context of structured prediction. In particular,
we show that an approximate training objective, although designed to produce accurate
predictors, also induces tightness of the LP relaxation as a byproduct. Interestingly,
our analysis also suggests that exact training may have the opposite effect. To explain
tightness on future (i.e., test) instances, we prove several generalization bounds relating
average tightness on the training data to expected tightness with respect to the generating
distribution. Our bounds imply that if many predictions on training instances are tight,
then predictions on test instances are also likely to be tight. Moreover, if predictions on
training instances are close to integral solutions (in terms of L1 distance), then predictions
on test instances will likely be similarly close to integral solutions. Our results help to
understand previous empirical findings, and to our knowledge provide the first theoretical
justification for the widespread success of LP relaxations for structured prediction in settings
where the training data is not (algorithmically) separable.
2. Background
In this section we review the formulation of the structured prediction problem, its LP
relaxation, and the associated learning problem. Consider a prediction task where the goal is
to map a real-valued input vector x to a discrete output vector y = (y1, . . . , yn). In this work
we focus on a simple class of models based on linear classifiers.1 Particularly, in this setting
prediction is performed via a linear discriminant rule: y(x; w) = arg maxy
0 w
>φ(x, y0
), where
φ(x, y) ∈ R
d
is a function mapping input-output pairs to feature vectors, and w ∈ R
d
is
the corresponding weight vector. Since the output space is often huge (exponential in n),
it will generally be intractable to maximize over all possible outputs.
In many applications the score function has a particular structure. Specifically, we
will assume that the score decomposes as a sum of simpler score functions: w
>
P
φ(x, y) =
c w
>
c φc(x, yc), where yc is an assignment to a (non-exclusive) subset of the variables.
For example, it is common to use such a decomposition that assigns scores to single and
pairs of output variables corresponding to nodes and edges of a graph G: w
>
P
φ(x, y) =
i∈V (G) w
>
i φi(x, yi) + P
ij∈E(G) w
>
ijφij (x, yi
, yj ). Viewing this as a function of y, we can
write the prediction problem as:
max
y
X
c
θc(yc; x, w) , (1)
where θc(yc; x, w) = w
>
c φc(x, yc) (we will sometimes omit the dependence on x and w in the
sequel).
Due to its combinatorial nature, the prediction problem is generally NP-hard, but
fortunately, efficient approximations have been proposed. Here we will be particularly
interested in approximations based on LP relaxations. We begin by formulating prediction
1. Most of our results also hold more generally for non-linear models (e.g., deep neural factors).
2
Train and Test Tightness of LP Relaxations in Structured Prediction
as the following ILP:2
max
µ∈ML
µ∈{0,1}
q
X
c
X
yc
µc(yc)θc(yc) +X
i
X
yi
µi(yi)θi(yi) = θ
>µ , (2)
where ML =
(
µ ≥ 0 :
P
yc\i P
µc(yc) = µi(yi) ∀c, i ∈ c, yi
yi
µi(yi) = 1 ∀i
)
.
Here, µc(yc) is an indicator variable for a factor c and local assignment yc, and q is the total
number of factor assignments (dimension of µ). The set ML is known as the local marginal
polytope (Wainwright and Jordan, 2008), and P
yc\i
µc(yc) is the marginalization of µc
w.r.t. variable i. First, notice that there is a one-to-one correspondence between feasible
µ’s and assignments y’s, which is obtained by setting µ to indicators over local assignments
(yc and yi) consistent with y. Second, while solving ILPs is NP-hard in general, it is
easy to obtain a tractable program by relaxing the integrality constraints (µ ∈ {0, 1}
q
),
which may introduce fractional solutions to the LP. This relaxation, sometimes called the
basic linear programming relaxation (Thapper and Zivn´y, 2012), is the first level of the ˇ
Sherali-Adams hierarchy (Sherali and Adams, 1990). This hierarchy provides successively
tighter LP relaxations of an ILP. Notice that since the relaxed program is obtained by
removing constraints from the original problem, its optimal value upper bounds the ILP
optimum. Finally, we note that most of our results below also hold for cases where more
complex constraints than those in Eq. (2) are used. For example, some assignments may be
forbidden since they do not correspond to feasible global structures such as spanning trees
(e.g., Martins et al., 2009b; Koo et al., 2010).
In order to achieve high prediction accuracy, the parameters w are learned from training
data. In this supervised learning setting, the model is fit to labeled examples {(x
(m)
, y(m)
)}M
m=1,
where the goodness of fit is measured by a task-specific loss ∆(y(x
(m)
; w), y(m)
). We assume
that ∆(y, y0
) ≥ 0 for all y, y0
, and that ∆(y, y) = 0. For example, a commonly used task-loss
is the Hamming distance: ∆Hamming(y, y0
) = 1
n
P
i 1[yi 6= y
0
i
].
In the structured SVM (SSVM) framework (Taskar et al., 2003; Tsochantaridis et al.,
2004), the empirical risk is upper bounded by a convex surrogate called the structured hinge
loss, which yields the training objective:
min
w
X
m
max
y
h
w
>

φ(x
(m)
, y) − φ(x
(m)
, y(m)
)

+ ∆(y, y(m)
)
i
. (3)
For brevity, we have omitted the standard regularization term from Eq. (3), however, all of
our results below in sections 4—6 still hold with regularization.3 The objective in Eq. (3)
is a convex function of w and hence can be optimized in various ways. But, notice that
the objective includes a maximization over outputs y for each training example. This lossaugmented prediction task needs to be solved repeatedly during training (e.g., to evaluate
subgradients), which makes training intractable in general (see also Sontag et al., 2010).
Similar to prediction, LP relaxation can be applied to the structured loss (Taskar et al.,
2. For convenience we introduce singleton factors θi, which could be set to 0 if needed.
3. In particular, our bounds apply to the maximization over y in Eq. (3), so still hold when regularization
w.r.t. w is added.
3
Meshi, London, Weller, and Sontag
2003; Kulesza and Pereira, 2007), which yields the relaxed training objective:
min
w
X
m
max
µ∈ML
h
θ
>
m(µ − µm) + `m
>µ
i
, (4)
where θm ∈ R
q
is a score vector in which each entry represents w
>
c φc(x
(m)
, yc) for some c
and yc, and µm is the integral vector corresponding to y
(m)
. Assuming that the task-loss
decomposes as the model score, ∆(y, y0
) = P
c ∆c(yc, y0
c
), we define the vector `m ∈ R
q with
entries `m,c,yc = ∆c(y
(m)
c , yc) for each value yc. Notice that ` has the same dimension as µ,
and we can define ∆(y
(m)
, y) = P
c
P
y
0
c
∆c(y
(m)
c , y0
c
)µc(y
0
c
) = `m
>µ, where µ is the vector of
indicators corresponding to y (i.e., µc(y
0
c
) = 1{y
0
c = yc}). With this definition, ` generalizes
∆ to any µ ∈ ML.
After reviewing related work in Section 3, we propose a theoretical justification for
the observed tightness of LP relaxations for structured prediction. To this end, we make
two complementary arguments: in Section 4 we argue that optimizing the relaxed training
objective of Eq. (4) also has the effect of encouraging tightness of training instances; then,
in sections 5 and 6 we show that tightness generalizes from train to test data.
3. Related Work
Many structured prediction problems can be expressed as ILPs (Roth and Yih, 2005; Martins
et al., 2009a; Rush et al., 2010). Despite being NP-hard in general (Roth, 1996; Shimony,
1994), various effective approximations have been proposed. These approximations include
search-based methods (Daum´e III et al., 2009; Zhang et al., 2014) and natural LP relaxations
to the hard ILPs (Schlesinger, 1976; Koster et al., 1998; Chekuri et al., 2004; Wainwright
et al., 2005). Tightness of LP relaxations for special classes of problems has been studied
extensively in recent years and has been demonstrated by restricting either the structure
of the model or its score function. For example, the pairwise LP relaxation is known to
be tight for tree-structured models or for supermodular scores (see, e.g., Wainwright and
Jordan, 2008; Thapper and Zivn´y, 2012); certain stability conditions guarantee tightness of ˇ
LP relaxations for Ferromagnetic Potts models (Lang et al., 2018); and the cycle relaxation
(for binary pairwise models) is known to be tight both for planar Ising models with no
external field (Barahona, 1993) and for almost balanced models (Weller et al., 2016). Hybrid
conditions, combining structure and score, by forbidding signed minors have recently been
shown to also guarantee tight relaxations (Rowland et al., 2017; Weller, 2016). To facilitate
efficient prediction, one could restrict the model class to be tractable. For example, Taskar
et al. (2004) learn supermodular scores, and Meshi et al. (2013) learn tree structures.
However, the sufficient conditions mentioned above are by no means necessary, and
indeed, many score functions that are useful in practice do not satisfy them but still produce
integral solutions (Roth and Yih, 2004; Lacoste-Julien et al., 2006; Sontag et al., 2008; Finley
and Joachims, 2008; Martins et al., 2009b; Koo et al., 2010). For example, Martins et al.
(2009b) showed that predictors that are learned with LP relaxations yield tight LPs for
92.88% of the test data on a dependency parsing problem (see Table 2 therein). Koo et al.
(2010) observed similar behavior for dependency parsing on a number of languages, as can
4
LP is often tight for structured prediction! Non-Projective Dependency Parsing
Train and Test Tightness of LP Relaxations in Structured Prediction
John1 saw2 a3 movie4 today5 that6 he7 liked8
John1 saw2 a3 movie4 today5 that6 he7 liked8
Important problem in many languages.
Problem is NP-Hard for all but the simplest models.
r example, in non-projective
pendency parsing, we found that the
 relaxation is exact for over 95% of
ntences
artins et al. ACL ’09, Koo et al., EMNLP ’10)
How often do we exactly solve the problem?
 90
 92
 94
 96
 98
 100
Cze
Eng
Dan
Dut
Por
Slo
Swe
Tur
I Percentage of examples where the dual decomposition finds
an exact solution.
Language
Percentage of integral solutions
Even when the local LP relaxation is not tight, often still possible to
solve exactly and quickly (e.g., Sontag et al. ‘08, Rush & Collins ‘11)
Figure 1: Percentage of integral solutions for dependency parsing from Koo et al. (2010).
be seen in Fig. 1.4 Lacoste-Julien et al. (2006) found that about 80% of test instances had
tight LP relaxations in a quadratic assignment formulation for word alignment, with only
0.2% fractional values overall. The same phenomenon has been observed for a multi-label
classification task, where test integrality reached 100% (Finley and Joachims, 2008, Table
3).
Learning structured output predictors from labeled data was proposed in various forms
by Collins (2002); Taskar et al. (2003); Tsochantaridis et al. (2004). These formulations
generalize training methods for binary classifiers, such as the Perceptron algorithm and
support vector machines (SVMs), to the case of structured outputs. The learning algorithms repeatedly perform prediction, necessitating the use of approximate inference within
training as well as at test time. A common approach, introduced right at the inception of
structured SVMs by Taskar et al. (2003), is to use LP relaxations for this purpose.
The closest work to ours is by Kulesza and Pereira (2007), which showed that not
all approximations are equally good, and that it is important to match the inference
algorithms used at train and test time. The authors defined the concept of algorithmic
separability which refers to the case where an approximate inference algorithm achieves
zero loss on a data set. The authors studied the use of LP relaxations for structured
learning, giving generalization bounds for the true risk of LP-based prediction. However,
since the generalization bounds in Kulesza and Pereira (2007) are focused on prediction
accuracy, the only settings in which tightness on test instances can be guaranteed are
when the training data is algorithmically separable, which is seldom the case in real-world
structured prediction tasks (the models are far from perfect). In contrast, our paper’s main
result (Theorem 1), guarantees that the expected fraction of test instances for which an
LP relaxation is tight is close to that which was achieved on training data. This then
allows us to talk about the generalization of computation. For example, suppose one uses
LP relaxation-based algorithms that iteratively tighten the relaxation, such as Sontag and
Jaakkola (2008); Sontag et al. (2008), and observes that 20% of the instances in the training
data are integral using the basic relaxation and that after tightening the remaining 80% are
now integral too. Our generalization bound then guarantees that approximately the same
ratio will hold at test time (assuming sufficient training data).
4. Kindly provided by the authors.
5
Meshi, London, Weller, and Sontag
Finley and Joachims (2008) also studied the effect of various approximate inference
methods in the context of structured prediction. Their theoretical and empirical results
support the superiority of LP relaxations in this setting. Martins et al. (2009b) established
conditions which guarantee algorithmic separability for LP relaxed training, and derived risk
bounds for a learning algorithm which uses a combination of exact and relaxed inference.
Finally, Globerson et al. (2015) recently studied the performance of structured predictors
for 2D grid graphs with binary labels from an information-theoretic perspective. They
proved lower bounds on the minimum achievable expected Hamming error in this setting,
and proposed a polynomial-time algorithm that achieves this error. Our work is different
since we focus on LP relaxations as an approximation algorithm, we handle a general form of
the problem without making any assumptions on the model or error measure (except score
decomposition), and we concentrate solely on the computational aspects while ignoring any
accuracy concerns.
4. Tightness at Training
In this section we show that the relaxed training objective in Eq. (4), although designed
to achieve high accuracy, also induces tightness of the underlying LP relaxation. In fact,
although we focus here on the basic LP relaxation (first-level), the results below hold for
higher-level LP relaxations as well.5
In order to simplify notation we focus on a single training instance and drop the index
m. Denote the solutions to the relaxed and integer LPs as:
µL ∈ arg max
µ∈ML
θ
>µ µI ∈ arg max
µ∈ML
µ∈{0,1}
q
θ
>µ , (5)
respectively. Also, let µT be the integral vector corresponding to the ground-truth output
y
(m)
. Now consider the following decomposition:
θ
>(µL − µT )
relaxed-hinge
= θ
>(µL − µI )
integrality gap
+ θ
>(µI − µT )
exact-hinge
(6)
This equality states that the difference in scores between the relaxed optimum and groundtruth (relaxed-hinge) can be written as a sum of the integrality gap and the difference in
scores between the exact optimum and the ground-truth (exact-hinge); notice that all three
terms are non-negative. This simple decomposition has several interesting implications.
First, we can immediately derive the following bound on the integrality gap:
θ
>(µL − µI ) = θ
>(µL − µT ) − θ
>(µI − µT ) (7)
≤ θ
>(µL − µT ) (8)
≤ θ
>(µL − µT ) + `
>µL (9)
≤ max
µ∈ML

θ
>(µ − µT ) + `
>µ

, (10)
where Eq. (10) is precisely the relaxed training objective from Eq. (4). Therefore, optimizing
the approximate training objective of Eq. (4) minimizes an upper bound on the integrality
5. Similar analysis for SDP and quadratic relaxations is left as future work—see recent work by Lˆe-Huu
and Paragios (2018).
 
Train and Test Tightness of LP Relaxations in Structured Prediction
gap. Hence, driving down the approximate objective may also reduce the integrality gap
of training instances, although in Section 4.1 we also study cases where loose bounds can
lead to non-zero integrality gap. One case where the integrality gap becomes zero is when
the data is algorithmically separable (i.e., µL = µT , so Eq. (8) equals 0). In this case the
relaxed-hinge term vanishes (the exact-hinge must also vanish), and training integrality is
assured.
Second, Eq. (6) holds for any integral µ, and not just the ground-truth µT . In other
words, the only property of µT used here is its integrality. Indeed, in Section 7 we verify
empirically that training a model using random labels still attains the same level of tightness
as training with the ground-truth labels.6 On the other hand, accuracy drops drastically,
as expected. This analysis suggests that tightness is not coupled with accuracy of the
predictor. Finley and Joachims (2008) explained tightness of LP relaxations by noting
that fractional solutions always incur a loss during training. Our analysis suggests an
alternative explanation, emphasizing the difference in scores (Eq. (7)) rather than the loss,
and decoupling tightness from accuracy.
Third, the results above still hold in the presence of global constraints. Often such
constraints can be expressed as linear inequalities, so in this case the local polytope ML
can be redefined by adding these constraints to those in Eq. (2) to form a tighter polytope.
In particular, Eq. (8) holds since µT satisfies the global constraints.
Finally, we do not make any assumption here about the form of the model scores θ.
Therefore, these results apply more generally, even when factor scores are obtained from
non-linear functions of the inputs, such as deep neural networks (e.g., Chen et al., 2015).
4.1. When Relaxed Training is Better than Exact Training
Unfortunately, the bound in Eq. (10) might sometimes be loose. Indeed, to get the bound we
have discarded the exact-hinge term (Eq. (8)), added the task-loss (Eq. (9)), and maximized
the loss-augmented objective (Eq. (10)). At the same time, Eq. (7) provides a precise
characterization of the integrality gap. Specifically, the gap is determined by the difference
between the relaxed-hinge and the exact-hinge terms. This implies that even when the
relaxed-hinge is not zero, a small integrality gap can still be attained if the exact-hinge is
also large. In fact, the only way to get a large integrality gap is by setting the exact-hinge
much smaller than the relaxed-hinge. But when can this happen? As we now show, it is
less likely to happen when training with relaxed inference than when training with exact
inference.
A key point is that the relaxed and exact hinge terms are upper bounded by the relaxed
and exact training objectives respectively (the latter additionally depend on the task loss ∆).
Therefore, minimizing the training objective will also likely reduce the corresponding hinge
term (this is also demonstrated empirically in Section 7). Using this insight, we observe that
relaxed training reduces the relaxed-hinge term without directly reducing the exact-hinge
term, and thereby induces a small integrality gap. On the other hand, this also suggests
that exact training may actually increase the integrality gap, since it reduces the exacthinge without also reducing directly the relaxed-hinge term. This finding is consistent with
previous empirical evidence. Specifically, Martins et al. (2009b, Table 2) showed that on
6. This is not true for random models (w), which often yield loose relaxations.
7
Meshi, London, Weller, and Sontag
w
-2 -1 0 1 2 3
Hinge
0
2
4
6
8
10
Relaxed hinge
Exact hinge
Figure 2: Exact- and relaxed- hinge (see Eq. (6)) as a function of w for the learning scenario
in Section 4.1.
a dependency parsing problem, training with the relaxed objective achieved 92.9% integral
solutions, while exact training achieved only 83.5% integral solutions. An even stronger
effect was observed by Finley and Joachims (2008, Table 3) for multi-label classification,
where relaxed training resulted in 99.6% integral instances, with exact training attaining
only 17.7% (‘Yeast’ dataset).
In Section 7 we provide further empirical support for our explanation, however, we
next show its possible limitation by providing a counter-example. The counter-example
demonstrates that despite training with a relaxed objective, the exact-hinge can in some
cases actually be substantially smaller than the relaxed-hinge, leading to a loose relaxation.
Although this illustrates the limitations of the explanation above, we point out that the
corresponding learning task is far from natural.
We construct a learning scenario where relaxed training obtains zero exact-hinge and
non-zero relaxed-hinge, so the relaxation is not tight. Consider a model where x ∈ R
3
,
y ∈ {0, 1}
3
, and the prediction is given by:
y(x; w) = arg max
y

x1y1 + x2y2 + x3y3 + w [1{y1 6= y2} + 1{y1 6= y3} + 1{y2 6= y3}]

. (11)
The corresponding LP relaxation is then:
max
µ∈ML

x1µ1(1) + x2µ2(1) + x3µ3(1) (12)
+ w [µ12(01) + µ12(10) + µ13(01) + µ13(10) + µ23(01) + µ23(10)] 
.
Next, we construct a training set where the first instance is: x
(1) = (2, 2, 2), y(1) = (1, 1, 0),
and the second is: x
(2) = (0, 0, 0), y(2) = (1, 1, 0). Fig. 2 shows the relaxed and exact losses
as a function of w, obtained by plugging Eq. (11) and Eq. (12) in Eq. (3) and Eq. (4),
respectively.7 Observe that w = 1 minimizes the relaxed objective (Eq. (4)). However, with
this weight vector the relaxed-hinge for the second instance (x
(2), y(2)) is equal to 1, while
the exact-hinge for this instance is 0 (the data is separable with w = 1). Consequently,
there is an integrality gap of 1 for the second instance, and the relaxation is loose (the first
7. For simplicity we use ` = 0 in this example, but a similar result holds with ` = 1/2 · `Hamming.
8
Train and Test Tightness of LP Relaxations in Structured Prediction
ML
Figure 3: Illustration of the local marginal polytope ML, with its vertices partitioned into
integral vertices VI (blue), and fractional vertices VF (red).
instance is actually tight). Notice that in this data the same output corresponds to two
very different inputs.
5. Generalization of Tightness
Our argument in Section 4 concerns only the tightness of train instances. However, the
empirical evidence discussed above pertains to test data. To bridge this gap, in this section
we prove a generalization bound which shows that train tightness implies test tightness.
We first define a loss function which measures the lack of integrality (or, fractionality)
of the LP solution for a given instance. To this end, we consider the discrete set of vertices
of the local polytope ML (excluding its convex hull), denoting by VI and VF the sets of
fully-integral and non-integral (i.e., fractional) vertices,8
respectively (so VI ∩ VF = ∅, and
VI ∪ VF consists of all vertices of ML);9
see Fig. 3. Considering only vertices does not
reduce generality, since the solution to a linear program is always at a vertex.
Next, let
I
∗
(w, x) , max
µ∈VI
θ(x; w)
>µ and F
∗
(w, x) , max
µ∈VF
θ(x; w)
>µ (13)
denote the respective best integral and fractional scores. By convention, we set F
∗
(w, x) ,
−∞ whenever VF = ∅. The fractionality of inference with (w, x) can be measured by the
quantity
D(w, x) , F
∗
(w, x) − I
∗
(w, x). (14)
Observe that D(w, x) > 0 whenever the LP has a fractional solution that is better than the
integral solution. We can now define the integrality loss,
L0(w, x) ,
(
1 D(w, x) > 0
0 otherwise
. (15)
8. It is enough that one coordinate is fractional to belong to VF .
9. We assume that all feasible integral solutions are vertices of ML, which is the case for the type of
relaxations considered here (see Wainwright and Jordan, 2008).
9
Meshi, London, Weller, and Sontag
This loss function equals 1 if and only if the optimal fractional solution has a (strictly) higher
score than the optimal integral solution. The loss will be 0 whenever the non-integral and
integral optima are equal—that is, for our purpose we consider the relaxation to be tight in
this case. The expected integrality loss measures the probability of obtaining a fractional
LP solution (over draws of an input, x). Note that this loss ignores the ground truth
assignment.
To support our generalization analysis, we define a related loss function, which we call
the integrality ramp loss. For a predetermined margin parameter, γ, the integrality ramp
loss is given by
Lγ(w, x) ,



1 D(w, x) > 0
1 + D(w, x)/γ −γ < D(w, x) ≤ 0
0 D(w, x) ≤ −γ
. (16)
Importantly, the integrality ramp loss upper-bounds the integrality loss. For the ramp loss
to be zero, the best integral solution has to be better than the best fractional solution by a
margin of at least γ, which is a stronger requirement than mere tightness. In Appendix A
we give examples of models that are guaranteed to satisfy this requirement, and in Section
7 we also show this often happens in practice.
We point out that both L0(w, x) and Lγ(w, x) are generally hard to compute, a point
which we address in Section 6. For the time being, we are only interested in proving that
tightness is a generalizing property, so we will not worry about computational efficiency.
We are now ready to state the main theorem of this section, a generalization bound for
tightness. Our proof (deferred to Appendix B.1) uses a PAC-Bayesian analysis, similar to
London et al. (2016), though the main result is stated for a deterministic predictor.
Theorem 1. Let D denote a distribution over X . Let φ : X × Y → R
d denote a feature
mapping such that supx,y kφ(x, y)k2 ≤ B < ∞. Then, for any γ > 0, δ ∈ (0, 1) and m ≥ 1,
with probability at least 1 − δ over draws of (x
(1), . . . , x(m)
) ∈ X m, according to D
m, every
weight vector, w, with kwk2 ≤ R < ∞, satisfies
E
x∼D
[L0(w, x)] ≤
1
m
Xm
i=1
Lγ(w, x(i)
) + 8
m
+ 2s
d ln(mBR/γ) + ln 2
δ
2m
. (17)
Theorem 1 shows that if we observe high integrality (equivalently, low fractionality)
on a finite sample of training data, then it is likely that integrality of test data will not
be much lower, provided sufficient number of samples. It is worth noting that, though
we focus on linear models to simplify our presentation, it is possible to extend this result
to accommodate non-linear models (e.g., scores θ computed by a deep neural network),
provided some assumptions are made on the smoothness of the model and loss.
As the following corollary states, Theorem 1 actually applies more generally to any two
disjoint sets of vertices, and is not limited to VI and VF .
Corollary 1. Let Vα be any set of vertices of ML with at most α fractional values (where
0 ≤ α ≤ 1), and let V¯
α be the rest of the vertices of ML. Then Theorem 1 holds with Vα
and V¯
α replacing VI and VF in the definition of I
∗ and F
∗
in Eq. (13), respectively.
10
Train and Test Tightness of LP Relaxations in Structured Prediction
For example, we can set Vα to be any set of vertices with at most 10% fractional values,
and V¯
α to be the rest of the vertices of ML. This gives a different meaning to the integrality
loss, but the rest of our analysis holds unchanged. Consequently, our generalization result
implies that it is likely to observe a similar portion of instances with at most 10% fractional
values at test time as we did at training.
Moreover, Theorem 1 also holds in the presence of global constraints (e.g., spanning tree
constraints). As mentioned in Section 4, the polytope ML is replaced by its intersection
with the global constraints polytope, but the rest of our derivation remains unchanged.
Note that the loss function in Eq. (15) does not measure the actual number of fractional
values, nor their distance to integrality. In Section 6, we analyze a notion of tightness that
accounts for the L1 distance to integrality.
Compared to the generalization bound of Kulesza and Pereira (2007), our bounds only
consider the tightness of a prediction, ignoring label errors. Thus, for example, if learning
happens to settle on a set of parameters in a tractable regime (e.g., supermodular potentials
or stable instances (Makarychev et al., 2014)) for which the LP relaxation is tight for
most training instances, our generalization bound guarantees that with high probability
the LP relaxation will also be tight on most test instances. In contrast, in Kulesza and
Pereira (2007), tightness on test instances can only be guaranteed when the training data
is algorithmically separable (i.e., LP-relaxed inference predicts perfectly).
5.1. γ-Tight Relaxations
In this section we study the stronger notion of tightness required by our surrogate fractionality loss (Eq. (16)), and show examples of models that satisfy it.
Definition 1. An LP relaxation is called γ-tight if I
∗
(w, x) ≥ F
∗
(w, x) + γ (so Lγ(w, x) =
0). That is, the best integral value is larger than the best non-integral value by at least γ.
10
We focus on binary pairwise models and show two cases where the model is guaranteed
to be γ-tight. Proofs are provided in Appendix A. Our first example involves balanced
models, which are binary pairwise models that have supermodular scores, or can be made
supermodular by “flipping” a subset of the variables (for more details, see Appendix A).
Proposition 1. A balanced model with a unique optimum is (α/2)-tight, where α is the
difference between the best and second-best (integral) solutions.
This result is of particular interest when learning structured predictors where the edge
scores depend on the input. Whereas one could learn supermodular models by enforcing
linear inequalities (Taskar et al., 2004), we know of no tractable means of ensuring the model
is balanced. Instead, one could learn over the full space of models using LP relaxation. If the
learned models are balanced on the training data, Proposition 1 together with Theorem 1
tell us that the LP relaxation is likely to be tight on test data as well.
Our second example regards models with singleton scores that are much stronger than
the pairwise scores. Consider a binary pairwise model11 in minimal representation, where
¯θi are node scores and ¯θij are edge scores in this representation (see Appendix A for full
10. Notice that scaling up θ(w, x) will also increase γ, but our bound in Eq. (17) also grows with the norm
of θ(w, x) (via the term BR). Therefore, we assume here that kθ(w, x)k2
is bounded.
11. This case easily generalizes to variables with more than 2 possible values.
11
Meshi, London, Weller, and Sontag
details). Further, for each variable i, define the set of neighbors with attractive edges N
+
i =
{j ∈ Ni
|
¯θij > 0}, and the set of neighbors with repulsive edges N
−
i = {j ∈ Ni
|
¯θij < 0}.
Proposition 2. If all variables satisfy the condition:
¯θi ≥ −X
j∈N
−
i
¯θij + β, or ¯θi ≤ −X
j∈N
+
i
¯θij − β
for some β > 0, then the model is (β/2)-tight.
Finally, we point out that in both of the examples above, the conditions can be verified
efficiently and if they hold, the value of γ can be computed efficiently.
6. Analysis of the Integrality Distance
For structured prediction, the maximizer, µL, is often more important than the maximum
value, θ
>µL. That is, we do not really care whether the optimum of the relaxed problem
equals that of the integral one; we just want relaxed inference to yield the optimal integral
assignment—or, lacking that, an assignment that is “close to” the optimal integral one. If
we assume that the relaxed problem has a unique solution, then an integrality gap of zero
implies that the assignments are the same. However, lacking this assumption, there may
be multiple, disparate solutions, so the assignments may differ. In general, it is difficult to
characterize the distance between relaxed and exact assignments as a function of a nonzero
integrality gap.
Thus, in addition to studying the integrality gap, we are also interested in what we
will call the integrality distance, defined as kµL − µIk1
, which is the Manhattan distance
between the maximizers of the relaxed and exact programs. The integrality distance is
conceptually similar to persistence (see Wainwright and Jordan, 2008 for definition) in that
a persistent fractional solution will have a subset of variables with zero integrality distance.
The integrality distance is also related to the integrality gap, although the distance could
sometimes be more useful: when the integrality distance is small, the relaxed solution is
close to the exact solution, which is what we may ultimately care about; moreover, when
the integrality distance is zero, the integrality gap must also be zero, regardless of whether
we assume uniqueness.
In this section, we relate the integrality distance to several loss functions that are
commonly analyzed in the literature on structured prediction. We then show that, similar to
the integrality gap, the integrality distance also generalizes from an empirical sample to the
population average. Moreover, we show that the integrality distance is upper-bounded by a
constant multiple of the structured hinge loss—a convex loss function that is commonly used
for training. Importantly, unlike the bound in Theorem 1, this bound is computationally
tractable. Combining these results, we obtain a high-probability bound on the expected
integrality distance that can be efficiently evaluated from training data, and whose additive
error decreases with the number of examples. Finally, a simple argument shows how this
bound applies to an integral rounding of a fractional solution.
12
Train and Test Tightness of LP Relaxations in Structured Prediction
6.1. Structured Loss Functions
We will focus on the integrality distance of the singleton (i.e., node) marginals, denoted
µu , (µi)
n
i=1, since they are sufficient for decoding a labeling, y. Let
D1(µ, µ0
) ,
1
2n

µu − µ
0
u


1
(18)
denote the normalized Manhattan distance. When both inputs are integral, D1 is equivalent
to the normalized Hamming distance.
Given a model, w, an input, x, and an assignment, µ, let
L1(w, x, µ) , D1 (µ, µL(x; w)) (19)
denote the L1 loss. This loss function is a generalization of the Hamming loss, which is
commonly used to measure the prediction error of exact inference. If the third argument
is a reference (i.e., “ground truth”) labeling, µT , then L1(w, x, µT ) measures the prediction
error of approximate inference. However, if the third argument is the exact, integral MAP
state, µI , then L1(w, x, µI ) is the normalized integrality distance. This latter quantity is
what we will focus on upper-bounding.
Let
Lh(w, x, µ) , max
µ0∈ML
D1(µ, µ0
) + θ
>

µ
0 − µ

. (20)
denote a loss function commonly referred to as the (relaxed) structured hinge loss. This
loss is minimized when µ scores higher than all alternate assignments, µ
0
, by a margin that
is at least D1(µ, µ0
). Note that when µ is the exact MAP state, the structured hinge loss
computes a loss-augmented integrality gap, using Manhattan distance for loss augmentation
(see Eq. (4)).
A related loss function is the (relaxed) structured ramp loss,
Lr(w, x, µ) , max
µ0∈ML
D1(µ, µ0
) + θ
>

µ
0 − µL(x; w)

, (21)
which can be considered a normalized version of the hinge loss. Lr is bounded by [0, 1],
whereas Lh might be unbounded (depending on the features and weights).
The hinge loss is often used in max-margin training, since it is convex in w. The ramp
loss is not convex in w, but it is bounded, Lipschitz, and has a convenient relationship to
the L1 (or Hamming) and hinge losses:
L1(w, x, µ) ≤ Lr(w, x, µ) ≤ Lh(w, x, µ). (22)
Thus, the ramp loss is often used as an analytical tool to derive generalization bounds, such
as those that follow.
6.2. Generalization Bound
Similar to Section 5, we now show that the integrality distance on a training sample
generalizes to the data distribution. Like Theorem 1, the proof (in Appendix B.2) uses
PAC-Bayesian analysis.
  
Meshi, London, Weller, and Sontag
Theorem 2. Let D denote a distribution over X . Let φ : X × Y → R
d denote a feature
mapping such that supx,y kφ(x, y)k2 ≤ B < ∞; and µI is defined in Eq. (5). Then, for any
δ ∈ (0, 1) and m ≥ 1, with probability at least 1 − δ over draws of (x
(1), . . . , x(m)
) ∈ X m,
according to D
m, every weight vector, w, with kwk2 ≤ R < ∞, satisfies
E
x∼D
[L1(w, x, µI )] ≤
1
m
Xm
i=1
Lr(w, x(i)
, µ
(i)
I
) + 8
m
+ 2s
d ln(mBR) + ln 2
δ
2m
. (23)
Further, Eq. (23) holds when Lr is replaced with Lh.
Theorem 2 says that the integrality distance on the training set generalizes to future
examples. More precisely, the expected integrality distance on a random instance is upperbounded by the average integrality ramp (or hinge) loss on the training set, plus two terms
that vanish as the number of training examples grows. Thus, the more training data we
have, the better we can estimate the expected integrality distance.
Remark 1. There is nothing special about µI to Theorem 2. Indeed, we could use any
integral assignment as a reference labeling for the loss functions and the proof would be the
same. For example, we could replace µI with µT (a ground truth labeling) and obtain a risk
bound for learning with approximate inference, which is a well-studied topic (e.g., Kulesza
and Pereira, 2007; London et al., 2016).
6.3. Relationship to Max-Margin Training
In practice, computing the integrality loss is generally infeasible, since it requires exact
inference. Therefore, the upper bounds in Theorems 1 and 2 cannot be evaluated. However,
the empirical relaxed hinge loss with respect to the ground truth labels can be evaluated
efficiently. In this section, we show how minimizing this quantity actually minimizes the
integrality distance. That is, max-margin training with approximate inference—which is
commonly used anyway to learn graphical models—reduces not only the prediction error,
but also the inference approximation error.
The key insight that enables this result comes from the following technical lemma.
Lemma 1. For any w and x, if µT is the reference (ground truth) labeling of x and µI is
the exact MAP state under w, then
Lh(w, x, µI ) ≤ 2Lh(w, x, µT ), (24)
meaning the integrality hinge loss is at most twice the hinge loss with respect to the true
labeling.
Proof First, we decompose the integrality hinge loss as follows:
Lh(w, x, µI ) = max
µ∈ML
D1(µI , µ) + θ
> (µ − µI )
≤ max
µ∈ML
D1(µI , µ) + θ
> (µ − µT )
≤ D1(µI , µT ) + max
µ∈ML
D1(µT , µ) + θ
> (µ − µT )
= D1(µI , µT ) + Lh(w, x, µT ). (25)
14
Train and Test Tightness of LP Relaxations in Structured Prediction
The second term on the right-hand side is the hinge loss of the approximate predictor with
respect to the true labeling, which can be evaluated efficiently. The first term on the righthand side is the Hamming loss of exact inference, which cannot be evaluated efficiently.
However, this latter quantity can be upper-bounded as follows:
D1(µI , µT ) ≤ max
µ∈M
D1(µ, µT ) + θ
> (µ − µT )
≤ max
µ∈ML
D1(µ, µT ) + θ
> (µ − µT )
= Lh(w, x, µT ). (26)
Combining Eq. (25) and (26) completes the proof.
Note that Lemma 1 also yields an upper bound on the integrality ramp loss, since it is
upper-hounded by the integrality hinge loss.
Using Lemma 1, we thus obtain the following corollary of Theorem 2.
Corollary 2. Let D denote a distribution over X ×Y. Let φ : X ×Y → R
d denote a feature
mapping such that supx,y kφ(x, y)k2 ≤ B < ∞. Then, for any δ ∈ (0, 1) and m ≥ 1, with
probability at least 1 − δ over draws of (x
(1), y(1), . . . , x(m)
, y(m)
) ∈ (X × Y)
m, according to
D
m, every weight vector, w, with kwk2 ≤ R < ∞, satisfies
E
x∼D
[L1(w, x, µI )] ≤
2
m
Xm
i=1
Lh(w, x(i)
, µ
(i)
T
) + 8
m
+ 2s
d ln(mBR) + ln 2
δ
2m
, (27)
where µ
(i)
T
denotes the integral vector corresponding to the ground-truth labeling y
(i)
.
Corollary 2 says that max-margin training with relaxed inference directly minimizes the
integrality distance on future examples. Importantly, if the constants B and R are known,
then this bound can be efficiently evaluated from training data. It is worth noting that
Corollary 2 actually holds for any integral assignment, not just the ground truth labels.
Nonetheless, we feel the bound is more insightful when stated with respect to the ground
truth labels, which are given in the learning setup. In this case D is defined as a joint
distribution over X and Y, and the bound holds with high probability over draws of both
inputs and labels. It is also worth noting that Corollary 2 holds in the presence of global
constraints—provided the ground truth (or whichever integral assignments are used) are in
the feasible set.
6.4. Decoding a Solution
When the solution to an LP relaxation is fractional, we often round the solution to an
integral assignment. Rounding schemes have been studied extensively (e.g., Raghavan and
Tompson, 1987; Kleinberg and Tardos, 2002; Chekuri et al., 2004; Ravikumar et al., 2010).
Arguably, the simplest method is to select the local assignments µi(yi) with the highest
values. One question that arises is how far the rounding, denoted µR(x; w), is from the
exact solution; once this relationship is determined, one can apply our prior generalization
analysis to the rounding. It turns out that the distance from µR to µI can be upper-bounded
by a multiple of the integrality distance.
15
Meshi, London, Weller, and Sontag
0 50 100 150 200
0
0.5
1
Relaxed Training
Relaxed−hinge
Exact−hinge
Relaxed SSVM obj
Exact SSVM obj
0 50 100 150 200
0
0.5
1
Training epochs
Train tightness
Test tightness
Relaxed test F1
Exact test F1
0 50 100 150 200 250
0
0.5
1
1.5 Exact Training
Training epochs
0 50 100 150 200 250
0
0.5
1
−0.4 −0.2 0
0
100
200
Relaxed Training
−0.4 −0.2 0
0
100
200
Exact Training
I
*−F*
Figure 4: Training with the ‘Yeast’ multi-label classification dataset. Various quantities of
interest are shown as a function of training iterations. (Left) Training with LP
relaxation. (Middle) Training with ILP. (Right) Integrality margin (bin widths
are scaled differently).
Lemma 2. Suppose that every output variable has the same domain—i.e., Y1 = Y2 = . . . =
Yn—and that each domain has size k. If µR(x; w) is the rounding of the fractional solution,
µL(x; w), then
D1(µR, µI ) ≤ k D1(µL, µI ). (28)
Proof Consider any output variable. If the fractional solution assigns the majority of
the local belief to the “correct” label (i.e., the label chosen by exact inference), then the
rounding of that variable will be exact. However, if the fractional solution puts most of the
local belief on an “incorrect” label, then the rounding of that variable will have D1 distance
1 from the correct label. Since the incorrect label must have had a fractional value of at
least 1/k, it follows that the fractional solution has D1 distance at least 1/k, which is no less
than 1/k that of the rounding. Applying this logic to every variable completes the proof.
Lemma 2 can be combined with Corollary 2 to generate bounds on the expected integrality distance of rounding; the bound simply scales by k.
7. Experiments
In this section we present some numerical results to support our theoretical analysis. We
run experiments for a multi-label classification task and an image segmentation task. For
training we have implemented the block-coordinate Frank-Wolfe algorithm for structured
SVM (Lacoste-Julien et al., 2013), using GLPK as the LP solver. We use a standard L2
regularizer, chosen via cross-validation.
7.1. Multi-Label Classification
For multi-label classification we adopt the experimental setting of Finley and Joachims
(2008). In this setting labels are represented by binary variables, the model consists of
16
Train and Test Tightness of LP Relaxations in Structured Prediction
0 100 200 300 400 500
0
0.5
1
Relaxed Training
Relaxed−hinge
Exact−hinge
Relaxed SSVM obj
Exact SSVM obj
0 100 200 300 400 500
0
0.5
1
Training epochs
Train tightness
Test tightness
Relaxed test F1
Exact test F1
0 50 100 150 200 250 300
0
0.5
1 Exact Training
Training epochs
0 50 100 150 200 250 300
0
0.5
1
−0.2 0 0.2 0.4 0.6
0
100
200
300
Relaxed Training
−0.2 0 0.2 0.4 0.6
0
100
200
300
Exact Training
I
*−F*
Figure 5: Training with the ‘Scene’ multi-label classification dataset. Various quantities of
interest are shown as a function of training iterations. (Left) Training with LP
relaxation. (Middle) Training with ILP. (Right) Integrality margin.
singleton and pairwise factors forming a fully connected graph over the labels, and the task
loss is the normalized Hamming distance.
Fig. 4 shows relaxed and exact training iterations for the ‘Yeast’ dataset (14 labels).
We plot the relaxed and exact hinge terms (Eq. (6)), the exact and relaxed SSVM training
objectives12 (Eq. (3) and Eq. (4), respectively), fraction of train and test instances having
integral solutions, as well as test accuracy (measured by F1 score). We use a simple scheme
to round fractional solutions found with relaxed inference. First, we note that the relaxedhinge values are nicely correlated with the relaxed training objective, and likewise the
exact-hinge is correlated with the exact objective (left and middle, top). Second, observe
that with relaxed training, the relaxed-hinge and the exact-hinge are very close (left, top),
so the integrality gap, given by their difference, remains small (almost 0 here). On the
other hand, with exact training there is a large integrality gap (middle, top). Indeed,
we can see that the percentage of integral solutions is almost 100% for relaxed training
(left, bottom), and close to 0% with exact training (middle, bottom). In Fig. 4 (right) we
also show histograms of the difference between the optimal integral and fractional values,
i.e., the integrality margin (I
∗
(w, x) − F
∗
(w, x)), under the final learned w for all training
instances. It can be seen that with relaxed training this margin is positive (although small),
while exact training results in larger negative values. Finally, we note that train and test
integrality levels are very close to each other, almost indistinguishable (left and middle,
bottom), which provides empirical support to our generalization result from Section 5.
We next train a model using random labels (with similar label counts as the true data).
In this setting the learned model obtains 100% tight training instances (not shown), which
supports our observation that any integral point can be used in place of the ground-truth,
and that accuracy is not important for tightness. Finally, in order to verify that tightness is
not coincidental, we test the tightness of the relaxation induced by a random weight vector
w. We find that random models are never tight (in 20 trials), which shows that tightness
of the relaxation does not come by chance.
12. The displayed objective values are averaged over train instances and exclude regularization.
17
Meshi, London, Weller, and Sontag
0 200 400 600 800 1000
0
1
2
3
x 104 Relaxed Training
Relaxed−hinge
Exact−hinge
Relaxed SSVM obj
Exact SSVM obj
0 200 400 600 800 1000
0
0.5
1
Training epochs
Train tightness
Test tightness
Relaxed test accuracy
Exact test accuracy
0 200 400 600 800 1000
0
1
2
3
x 104 Exact Training
0 200 400 600 800 1000
0
0.5
1
Training epochs
Figure 6: Training for foreground-background segmentation with the Weizmann Horse
dataset. Various quantities of interest are shown as a function of training
iterations. (Left) Training with LP relaxation. (Right) Training with ILP.
We now proceed to perform experiments on the ‘Scene’ dataset (6 labels). The results,
in Fig. 5, differ from the ‘Yeast’ results in case of exact training (middle). Specifically,
we observe that in this case the relaxed-hinge and exact-hinge are close in value (middle,
top), as for relaxed training (left, top). As a consequence, the integrality gap is very
small and the relaxation is tight for almost all train (and test) instances. These results
illustrate that sometimes optimizing the exact objective can reduce the relaxed objective
(and relaxed-hinge) as well. Further, in this setting we observe a larger integrality margin
(right), namely the integral optimum is strictly better than the fractional one.
We conjecture that the LP instances are easy in this case due to the dominance of
the singleton scores.13 Specifically, the features provide a strong signal which allows label
assignment to be decided mostly based on the local score, with little influence coming from
the pairwise terms. To test this conjecture we inject Gaussian noise into the input features,
forcing the model to rely more on the pairwise interactions. We find that with the noisy
singleton scores the results are indeed more similar to the ‘Yeast’ dataset, where a large
integrality gap is observed and fewer instances are tight (see Appendix C in the supplement).
7.2. Image Segmentation
Finally, we conduct experiments on a foreground-background segmentation problem using
the Weizmann Horse dataset (Borenstein et al., 2004). The data consists of 328 images,
of which we use the first 50 for training and the rest for testing. Here a binary output
variable is assigned to each pixel, and there are ∼ 58K variables per image on average.
We extract singleton and pairwise features as described in Domke (2013). Fig. 6 shows
the same quantities as in the multi-label setting, except for the accuracy measure—here
we compute the percentage of correctly classified pixels rather than F1. We observe a very
similar behavior to that of the ‘Scene’ multi-label dataset (Fig. 5). Specifically, both relaxed
and exact training produce a small integrality gap and high percentage of tight instances.
13. With ILP training, the condition in Proposition 2 is satisfied for 65% of all variables, although only 1%
of the training instances satisfy it for all their variables.
18
Train and Test Tightness of LP Relaxations in Structured Prediction
Unlike the ‘Scene’ dataset, here only 1.2% of variables satisfy the condition in Proposition 2
(using LP training). In all of our experiments the learned model scores were never balanced
(Proposition 1), although for the segmentation problem we believe the models learned are
close to balanced, both for relaxed and exact training.
8. Conclusion
In this paper we present a theoretical analysis of the tightness of LP relaxations often observed in structured prediction applications. Our analysis is based on a careful examination
of the integrality gap, the integrality distance, and their relation to the training objective.
It shows how training with LP relaxations, although designed with accuracy considerations
in mind, also induces tightness of the relaxation. Our derivation also suggests that exact
training may sometimes have the opposite effect, increasing the integrality gap.
To explain tightness at test time, we show that tightness generalizes in the following
two senses: first, if most training predictions are integral, then most test instances will also
be integral; secondly, if training predictions are on average “close to” integral, then test
predictions will be similarly close to integral in expectation.