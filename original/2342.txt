We consider off-policy temporal-difference (TD) learning in discounted Markov decision
processes, where the goal is to evaluate a policy in a model-free way by using observations
of a state process generated without executing the policy. To curb the high variance issue
in off-policy TD learning, we propose a new scheme of setting the λ-parameters of TD,
based on generalized Bellman equations. Our scheme is to set λ according to the eligibility
trace iterates calculated in TD, thereby easily keeping these traces in a desired bounded
range. Compared with prior work, this scheme is more direct and flexible, and allows
much larger λ values for off-policy TD learning with bounded traces. As to its soundness,
using Markov chain theory, we prove the ergodicity of the joint state-trace process under
nonrestrictive conditions, and we show that associated with our scheme is a generalized
Bellman equation (for the policy to be evaluated) that depends on both the evolution of λ
and the unique invariant probability measure of the state-trace process. These results not
only lead immediately to a characterization of the convergence behavior of least-squares
based implementation of our scheme, but also prepare the ground for further analysis of
gradient-based implementations.
Keywords: Markov decision process, approximate policy evaluation, generalized Bellman
equation, reinforcement learning, temporal-difference method, Markov chain, randomized
stopping time
1. Introduction
We consider discounted Markov decision processes (MDPs) and off-policy temporal-difference
(TD) learning methods for approximate policy evaluation with linear function approximation. The goal is to evaluate a policy in a model-free way by using observations of a state
process generated without executing the policy. Off-policy learning is an important part of
the reinforcement learning methodology (Sutton and Barto, 1998) and has been studied in
the areas of operations research and machine learning. (For an incomplete list of references,
see e.g., Glynn and Iglehart, 1989; Precup et al., 2000, 2001; Randhawa and Juneja, 2004;
Sutton et al., 2008, 2009; Maei, 2011; Yu, 2012; Dann et al., 2014; Geist and Scherrer, 2014;
Mahadevan et al., 2014; Mahmood et al., 2014; Liu et al., 2015; Sutton et al., 2016; Dai
et al., 2018.) Available TD algorithms, however, tend to have very high variances due to
the use of importance sampling, an issue that limits their applicability in practice. The
purpose of this paper is to introduce a new TD learning scheme that can help address this
problem.
Our work is motivated by the recently proposed Retrace algorithm (Munos et al., 2016)
and ABQ algorithm (Mahmood et al., 2017), and by the Tree-Backup algorithm (Precup
et al., 2000) that existed earlier. These algorithms, as explained by Mahmood et al. (2017),
all try to use the λ-parameters of TD to curb the high variance issue in off-policy learning.
In particular, they all choose the values of λ according to the current state or state-action
pair in such a way that guarantees the boundedness of the eligibility traces in TD learning,
which can help reduce significantly the variance of the TD iterates. A limitation of these
algorithms, however, is that they tend to be over-conservative and restrict λ to small values,
whereas small λ can result in large approximation bias in TD solutions.
In this paper, we propose a new scheme of setting the λ-parameters of TD, based on
generalized Bellman equations. Our scheme is to set λ according to the eligibility trace
iterates calculated in TD, thereby easily keeping those traces in a desired bounded range.
Compared with the schemes used in the previous work just mentioned, this is a direct way
to bound the traces in TD, and it is also more flexible and allows much larger λ values for
off-policy learning.
Regarding generalized Bellman equations, in our context, they will correspond to a family of dynamic programming equations for the policy to be evaluated. These equations all
have the true value function as their unique solution, and their associated operators have
contraction properties, like the standard Bellman operator. We will refer to the associated
operators as generalized Bellman operators or Bellman operators for short. Some authors
have considered, at least conceptually, the use of an even broader class of equations for
policy evaluation. For example, Ueno et al. (2011) have considered treating the policy
evaluation problem as a parameter estimation problem in the statistical framework of estimating equations, and in their framework, any equation that has the true value function as
the unique solution can be used to estimate the value function. The family of generalized
Bellman equations we consider has a more specific structure. They generalize multistep
Bellman equations, and they are associated with randomized stopping times and arise from
the strong Markov property (see Section 3.1 for details).
Generalized Bellman equations and operators are powerful tools. In classic MDP theory they have been used in some intricate optimality analyses (e.g., Sch¨al and Sudderth,
1987). Their computational use, however, seems to emerge primarily in the field of reinforcement learning. Through the λ-parameters and eligibility traces, TD learning is naturally
connected with, not a single Bellman operator, but a family of Bellman operators, with different choices of λ or different rules of calculating the eligibility trace iterates corresponding
2
Generalized Bellman Equations and TD Learning
to different Bellman operators. Early efforts that use this aspect to broaden the scope of
TD algorithms and to analyze such algorithms include Sutton’s work (1995) on learning at
multiple timescales and Tsitsiklis’ work on generalized TD algorithms in the tabular case
(see the book by Bertsekas and Tsitsiklis, 1996, Chap. 5.3). In the context of off-policy
learning, there are more recent approaches that try to utilize this connection of TD with
generalized Bellman operators to make TD learning more efficient (Precup et al., 2000; Yu
and Bertsekas, 2012; Munos et al., 2016; Mahmood et al., 2017). This is also our aim, in
proposing the new scheme of setting the λ-parameters.
Our analyses of the new TD learning scheme will focus on its theoretical side. Using
Markov chain theory, we prove the ergodicity of the joint state and trace process under
nonrestrictive conditions (see Theorem 2.1), and we show that associated with our scheme
is a generalized Bellman equation (for the policy to be evaluated) that depends on both
the evolution of λ and the unique invariant probability measure of the state-trace process
(see Theorem 3.2 and Corollary 3.1). These results not only lead immediately to a characterization of the convergence behavior of least-squares based implementation of our scheme
(see Corollary 2.1 and Remark 3.2), but also prepare the ground for further analysis of
gradient-based implementations. (The latter analysis has been carried out recently by Yu
(2017); see Remark 3.3.)
In addition to the theoretical study, we also present the results from a preliminary
numerical study that compares several ways of setting λ for the least-squares based offpolicy algorithm. The results demonstrate the advantages of the proposed new scheme
with its greater flexibility.
We remark that although we shall focus exclusively on policy evaluation in this paper,
approximate policy evaluation methods are highly pertinent to finding near-optimal policies
in MDPs. They can be applied in approximate policy iteration, in policy-gradient algorithms
for gradient estimation or in direct policy search (see e.g., Konda, 2002; Mannor et al., 2003).
In addition to solving MDPs, they can also be used in artificial intelligence and robotics
applications as a means to generate experience-based world models (see e.g., Sutton, 2009).
It is, however, beyond the scope of this paper to discuss these applications of our results.
The rest of the paper is organized as follows. In Section 2, after a brief background introduction, we present our scheme of TD learning with bounded traces, and we establish the
ergodicity of the joint state-trace process. In Section 3, we first discuss generalized Bellman
operators associated with randomized stopping times, and we then derive the generalized
Bellman equation associated with our scheme. In Section 4, we present the experimental
results on the least-squares based implementation of our scheme. Appendices A-B include
a proof for generalized Bellman operators and materials about approximation properties of
TD solutions that are too long to include in the main text.
2. Off-Policy TD Learning with Bounded Traces
We describe the off-policy policy evaluation problem and the algorithmic form of TD learning in Section 2.1. We then present our scheme of history-dependent λ in Section 2.2, and
analyze the properties of the resulting eligibility trace iterates and the convergence of the
corresponding least-squares based algorithm in Section 2.3.
3
Yu, Mahmood, and Sutton
2.1 Preliminaries
The off-policy learning problem we consider in this paper concerns two Markov chains on a
finite state space S = {1, . . . , N}. The first chain has transition matrix P, and the second
P
o
. Whatever physical mechanisms that induce the two chains shall be denoted by π and π
o
,
and referred to as the target policy and behavior policy, respectively. The second Markov
chain we can observe; however, it is the system performance of the first Markov chain that
we want to evaluate.
Specifically, we consider a one-stage reward function rπ : S → < and an associated
discounted total reward criterion with state-dependent discount factors γ(s) ∈ [0, 1], s ∈ S.
Let Γ denote the N ×N diagonal matrix with diagonal entries γ(s). We assume that P and
P
o
satisfy the following conditions:
Condition 2.1 (Conditions on the target and behavior policies)
(i) P is such that the inverse (I − PΓ)−1
exists, and
(ii) P
o
is such that for all s, s0 ∈ S, P
o
ss0 = 0 ⇒ Pss0 = 0, and moreover, P
o
is irreducible.
The performance of π is defined as the expected discounted total rewards for each initial
state s ∈ S:
vπ(s) := E
π
s
[ rπ(S0) + P∞
t=1 γ(S1) γ(S2) · · · γ(St) · rπ(St)] , (2.1)
where the notation E
π
s means that the expectation is taken with respect to (w.r.t.) the
Markov chain {St} starting from S0 = s and induced by π (i.e., with transition matrix P).
The function vπ is well-defined under Condition 2.1(i). It is called the value function of π,
and by standard MDP theory (see e.g., Puterman, 1994), we can write it in matrix/vector
notation as
vπ = rπ + PΓ vπ, i.e., vπ = (I − PΓ)−1
rπ.
The first equation above is known as the Bellman equation (or dynamic programming
equation) for a stationary policy (cf. Footnote 2).
We compute an approximation of vπ of the form v(s) = φ(s)
>θ, s ∈ S, where θ ∈ <n
is a parameter vector and φ(s) is an n-dimensional feature representation for each state s
(here φ(s), θ are column vectors and the symbol > stands for transpose). Data available for
this computation are:
(i) a realization of the Markov chain {St} with transition matrix P
o generated by π
o
,
and
(ii) rewards Rt = r(St
, St+1) associated with state transitions, where the function r relates
to rπ(s) as rπ(s) = E
π
s
[r(s, S1)] for all s ∈ S.
1
To find a suitable parameter θ for the approximation φ(s)
>θ, we use the off-policy TD
learning scheme. Define ρ(s, s0
) = Pss0/Po
ss0 (the importance sampling ratio),2 and write
ρt = ρ(St
, St+1), γt = γ(St).
Given an initial e0 ∈ <n
, for each t ≥ 1, the eligibility trace vector et ∈ <n and the scalar
temporal-difference term δt(v) for any approximate value function v : S → < are calculated
according to
et = λt γt ρt−1 et−1 + φ(St), (2.2)
δt(v) = ρt

Rt + γt+1v(St+1) − v(St)

. (2.3)
Here λt ∈ [0, 1], t ≥ 1, are important parameters in TD learning, the choice of which we
shall elaborate on shortly.
There exist a number of TD algorithms that use et and δt to generate a sequence of
parameters θt for approximate value functions. One such algorithm is LSTD (Boyan, 1999;
Yu, 2012), which obtains θt by solving the linear equation for θ ∈ <n
,
1
t
Pt−1
k=0 ek δk(v) = 0, v = Φθ (2.4)
(if it admits a solution), where Φ is a matrix with row vectors φ(s)
>, s ∈ S. LSTD updates
the equation (2.4) iteratively by incorporating one by one the observation of (St
, St+1, Rt) at
each state transition. We will discuss primarily this algorithm in the paper, as its behavior
can be characterized directly using our subsequent analyses of the joint state-trace process.
As mentioned earlier, our analyses will also provide bases for analyzing other gradientbased TD algorithms (e.g., Sutton et al., 2008, 2009; Maei, 2011; Mahadevan et al., 2014)
by using stochastic approximation theory (Kushner and Yin, 2003; Borkar, 2008; Karmakar
and Bhatnagar, 2018). Because of the complexity of this subject, however, we will not
delve into it in the present paper, and we refer the reader to the recent work (Yu, 2017) for
details.
2.2 Our Scheme of History-dependent λ
We now come to the choices of λt
in the trace iterates (2.2). For TD with function approximation, one often lets λt be a constant or a function of St (Sutton, 1988; Tsitsiklis and Van
Roy, 1997; Sutton and Barto, 1998). If neither the behavior policy nor the λt
’s are further
constrained, {et} can have unbounded variances and is also unbounded in many natural
situations (see e.g., Yu, 2012, Section 3.1), and this makes off-policy TD learning challenging.3
If we let the behavior policy to be close enough to the target policy so that P
o ≈ P,
then variance can be reduced, but it is not a satisfactory solution, for the applicability of
off-policy learning would be seriously limited.
Without restricting the behavior policy, as mentioned earlier, the two recent papers
(Munos et al., 2016; Mahmood et al., 2017), as well as the closely related early work by Precup et al. (2000), exploit state-dependent λ’s to control variance. Their choices of λt
are such that λtγtρt−1 < 1 for all t, so that the trace iterates et are made bounded, which
can help reduce the variance of the iterates.
Motivated by this prior work, our proposal is to set λt according to et−1 directly, so that
we can keep et
in a desired range straightforwardly and at the same time, allow a much
larger range of values for the λ-parameters. As a simple example, if we use λt to scale
the vector γtρt−1et−1 to be within a ball with some given radius, then we keep et always
bounded.
In the rest of this paper, we shall focus on analyzing the iteration (2.2) with a particular
choice of λt of the kind just mentioned. We want to be more general than the preceding
simple example. However, since the dependence on the trace et−1 would make λt dependent
on the entire past history (S0, . . . , St−1), we also want to retain certain Markovian properties
that are very useful for convergence analysis. This leads us to consider λt being a certain
function of the previous trace and past states. More specifically, we will let λt be a function
of the previous trace et−1 and a certain memory state that is a summary of the states
observed so far. The formulation is as follows.
2.2.1 Formulation and Examples
We denote the memory state at time t by yt
. For simplicity, we assume that yt can only
take values from a finite set M, and its evolution is Markovian: yt = g(yt−1, St) for some
given function g. The joint process {(St
, yt)} is then a simple finite-state Markov chain.
Each yt
is a function of the history (S0, . . . , St) and y0. We further require, besides the
irreducibility of {St} (cf. Condition 2.1(ii)), that
Condition 2.2 (Evolution of memory states) Under the behavior policy π
o
, the Markov
chain {(St
, yt)} on S × M has a single recurrent class.
This recurrence condition is nonrestrictive: If the Markov chain has multiple recurrent
classes, each recurrent class can be treated separately by using the same arguments we
present in this paper. However, we remark that the finiteness assumption on M is a
simplification. We choose to work with finite M mainly for the reason that with the traces
lying in a continuous space, to study the joint state and trace process, we need to resort
to properties of Markov chains on infinite spaces. With an infinite M, we would need to
introduce more technical conditions that are not essential to our analysis and can obscure
our main arguments.
We thus let yt and λt evolve as
yt = g(yt−1, St), λt = λ(yt
, et−1) (2.5)
where λ : M × <n → [0, 1]. We require the function λ to satisfy two conditions.
Condition 2.3 (Conditions for λ(·)) For some norm k · k on <
n
, the following hold for
each memory state y ∈ M:
(i) For any e, e0 ∈ <n
, kλ(y, e) e − λ(y, e0
) e
0k ≤ ke − e
0k.
(ii) For some constant Cy, kγ(s
0
)ρ(s, s0
) · λ(y, e) ek ≤ Cy for all e ∈ <n and all possible
state transitions (s, s0
) that can lead to the memory state y.
6
Generalized Bellman Equations and TD Learning
In the above, the second condition is to restrict {et} in a desired range (as it makes ketk ≤
maxy∈M Cy + maxs∈S kφ(s)k). The first condition is about the continuity of the function
λ(y, e) e in the trace variable e for each memory state y, and it plays a key role in the
subsequent analysis, where we will use this condition to ensure that the traces et and the
states (St
, yt) jointly form a Markov chain with appealing properties. We shall defer a
further discussion on the technical roles of these conditions to the end of Section 2.3 (cf.
Remark 2.2).
Let us give a few simple examples of choosing λ that satisfy Condition 2.3. We will later
use these examples in our experimental study (Section 4).
Example 2.1 We consider again the simple scaling example mentioned earlier and describe
it using the terminologies just introduced. In this example, we let yt = (St−1, St). For each
y = (s, s0
), we define the function λ(y, ·) so that when multiplied with λ(y, e), the vector
γ(s
0
)ρ(s, s0
) e is scaled down whenever its length exceeds a given threshold Css0:
λ

y, e
=
(
1 if γ(s
0
)ρ(s, s0
)kek2 ≤ Css0;
Css0
γ(s
0)ρ(s,s0)kek2
otherwise.
(2.6)
Condition 2.3(i) is satisfied because for y = (s, s0
) with γ(s
0
)ρ(s, s0
) = 0, λ

y, e
e = e,
whereas for y = (s, s0
) with γ(s
0
)ρ(s, s0
) 6= 0, λ

y, e
e is simply the Euclidean projection
of e onto the ball (centered at the origin) with radius Css0/(γ(s
0
)ρ(s, s0
)) and is therefore
Lipschitz continuous in e with modulus 1 w.r.t. k · k2. Corresponding to (2.6), the update
rule (2.2) of et becomes
et =
(
γt ρt−1 et−1 + φ(St) if γtρt−1ket−1k2 ≤ CSt−1St
;
CSt−1St
·
et−1
ket−1k2
+ φ(St) otherwise.
(2.7)
Note that this scheme of setting λ encourages the use of large λt
: λt = 1 will be chosen
whenever possible. A variation of the scheme is to multiply the right-hand side (r.h.s.)
of (2.6) by another factor βss0 ∈ [0, 1], so that λt can be at most βSt−1St
. In particular,
one such variation is to simply multiply the r.h.s. of (2.6) by a constant β ∈ (0, 1) so that
λt ≤ β < 1 for all t.
Example 2.2 The Retrace algorithm (Munos et al., 2016) modifies the trace updates in
off-policy TD learning by truncating the importance sampling ratios by 1. In particular, for
the off-policy TD(λ) algorithm with a constant λ = β ∈ (0, 1], Retrace modifies the trace
updates to be
et = β γt
· min{1, ρt−1} · et−1 + φ(St). (2.8)
As pointed out by Mahmood et al. (2017), to retain the original interpretation of λ as a
bootstrapping parameter in TD learning, we can rewrite the above update rule of Retrace
equivalently as
et = λt γt ρt−1 et−1 + φ(St) for λt = β ·
min{1,ρt−1}
ρt−1
(with 0/0 = 0). (2.9)
Each λt here is a function of (St−1, St) only and does not depend on et−1, so this choice
of λ-parameters automatically satisfies Condition 2.3(i) with the memory states being yt =   
Yu, Mahmood, and Sutton
(St−1, St). When the discount factors γ(s) are all strictly less than 1, ketk for all t are
bounded by a deterministic constant that depends on the initial e0. Then for each initial
e0, Retrace’s choice of λ coincides with a choice in our framework, since the C-parameters
in Condition 2.3(ii) can be made vacuously large so that the condition is satisfied by all
the traces et that could be encountered by Retrace. Thus in this case our framework for
choosing λ effectively encompasses the particular choice used by Retrace.
One can make variations on Retrace’s trace update rule. For example, instead of truncating each importance sampling ratio ρ(s, s0
) by 1, one can truncate it by a constant
Kss0 ≥ 1, and then use a scaling scheme similar to Example 2.1 to bound the traces. The
simplest such variation is to choose two memory-independent positive constants K and C,
and replace the definition of λt
in (2.9) by the following: with λ˜
t =
min{K,ρt−1}
ρt−1
(where we
treat 0/0 = 0),
λt =
(
β λ˜
t
if λ˜
tγt ρt−1 ket−1k2 ≤ C;
β λ˜
t
·
C
λ˜tγt ρt−1 ket−1k2
otherwise.
(2.10)
Correspondingly, instead of (2.8), the update rule of et becomes
et =
(
β γt
· min{K, ρt−1} · et−1 + φ(St) if γt
· min{K, ρt−1} · ket−1k2 ≤ C;
β C ·
et−1
ket−1k2
+ φ(St) otherwise.
(2.11)
These variations of Retrace are similar to Example 2.1 and satisfy Condition 2.3.4
2.2.2 Comparison with Previous Work
For policy evaluation, the Retrace algorithm (Munos et al., 2016) and the ABQ algorithm
(Mahmood et al., 2017) are very similar (ABQ was actually developed independently of
Retrace before the Munos et al. (2016) paper was published, although the ABQ paper
itself was released much later). Both Retrace and ABQ include the Tree-Backup algorithm
(Precup et al., 2000) as a special case. They can use additional parameters to select λ from
a range of values, whereas Tree-Backup specifies λ, implicitly, in a particular way (which
has the advantage of requiring no knowledge of the behavior policy) and does not have the
freedom in choosing λ. Because of the relations between these algorithms, when comparing
our method to them, we will compare it with Retrace only. In the experimental study given
later in Section 4 on the performance of LSTD for various ways of setting λ, we will compare
our scheme of choosing λ with that of Retrace for β = 1, which lets Retrace use the largest
λ that it can take.
We see in Example 2.2 that the eligibility trace update rule of Retrace can be written
in two equivalent forms, (2.8) and (2.9). The second form (2.9) has the advantage that
the λ-parameters involved are shown explicitly. In TD learning, the λ-parameters directly
affect the associated Bellman operators and can be meaningfully interpreted as stopping
probabilities (see Section 3), whereas the importance sampling ratio terms in the eligibility
4. To see this, let the memory states be yt = (St−1, St). For each y = (s, s0
), let λ(y, e) be defined according
to (2.10), and let Cy in Condition 2.3(ii) be Css0 =
ρ(s,s0
)
min{K, ρ(s,s0)}
· C (treat 0/0 = 0). Then note that
kλ(y, e)e − λ(y, e0
)e
0
k2 ≤ β min  K
ρ(s,s0)
, 1
	
· ke − e
0
k2 ≤ βke − e
0
k2.
8
Generalized Bellman Equations and TD Learning
trace iterates are essentially unchanged, for they have to be there in order to correct for the
discrepancy between the behavior and target policies. For this reason, we prefer (2.9) to
(2.8) and prefer thinking in terms of the selection of λ-parameters to that of what occurs
apparently to those importance sampling ratio terms in the trace updates.
As mentioned in Example 2.2, the Munos et al. (2016) paper does not make the connection between (2.8) and (2.9). Mahmood et al. (2017) recognized the role of the λ-parameters
and made explicit use of it to derive the ABQ algorithm. However, in the ABQ paper, the
discussion and the presentation of the algorithm still emphasize the apparent changes in
those importance sampling ratio terms in the trace iterates. This is an unsatisfactory point
in that paper that we hope we have clarified with our present work.
We mentioned in the introduction that Retrace, ABQ and Tree-Backup are too conservative and tend to use too small λ values. Let us now make this statement more precise
and also explain the reason behind.
These algorithms tend to behave effectively like TD(λ) with small constant λ, despite
that they can have λt = 1 at some time steps t. This is due to the nature of TD learning
with time-varying λ, which is very different from that of TD with constant λ. For timevarying λ, a large λt at one time step need not mean that we are using the information
of the cumulative rewards over a long time horizon to estimate the value at the state St
encountered at time t. Because the next λt+1 could be very small or even zero, forcing a
TD algorithm to “bootstrap” immediately. When large λt
’s are interleaved with small ones,
we are effectively in the situation of TD with small λ. This could occur to our proposed
scheme as well if, for example, in Example 2.1 the thresholds Css0 are set too small. When we
use larger thresholds, we allow larger λ. By comparison, Retrace, ABQ, and Tree-Backup
constrain the state-dependent λ-parameters to be small enough so that all the products
λtγtρt−1 < 1, and this makes them prone to the small-λ issue just mentioned. (See the
experiments in Section 4.2 for demonstrations.)
While we consider Retrace for approximate policy evaluation, the Munos et al. (2016)
paper actually focuses primarily on finding an optimal policy for an MDP, in the tabular
case, and it has demonstrated good empirical performance of Retrace and Tree-Backup
for that purpose. Despite this, its results are not adequate yet to establish asymptotic
optimality of these algorithms in the online optimistic policy iteration setting (personal
communication with Munos), and it is still an open theoretical question whether online
TD algorithms can solve an MDP like the Q-learning algorithm (Watkins, 1989; Tsitsiklis,
1994), when positive λ (small or not) and rapidly changing target policies are involved.
We also mention that for policy evaluation, Munos et al. (2016, Section 3.1) have also
conceived the use of generalized Bellman operators, although they did not relate these
operators explicitly to history-dependent λ’s and did not study corresponding algorithms
in this general case.
2.3 Ergodicity Result
The properties of the joint state-trace process {(St
, yt
, et)} are important for understanding
and characterizing the behavior of our proposed TD learning scheme. We study them in this
subsection. Most importantly, we shall establish the ergodicity of the state-trace process.
The result will be useful in convergence analysis of several associated TD algorithms (Yu,
9
Yu, Mahmood, and Sutton
2017), although in this paper we discuss only the LSTD algorithm. In the next section we
will also use the ergodicity result when we relate the LSTD equation (2.4) to a generalized
Bellman equation for the target policy in order to interpret the LSTD solutions.
We note that to obtain the results in this subsection, we will follow similar lines of
argument used in (Yu, 2012) for analyzing off-policy LSTD with constant λ. However,
because λ is now history-dependent, some proof steps in (Yu, 2012) no longer apply. We
shall explain this in more detail after we prove the main result of this subsection.
As another side note, one can introduce nonnegative coefficients i(y) for memory states
y to weight the state features (similarly to the use of “interest” weights in the ETD algorithm (Sutton et al., 2016)) and update et according to
et = λt γt ρt−1 et−1 + i(yt) φ(St). (2.12)
The results given below apply to this update rule as well.
Let us start with two basic properties of {(St
, yt
, et)} that follow directly from our choice
of the λ function:
(i) By Condition 2.3(i), for each y, λ(y, e)e is a continuous function of e, and thus et
depends continuously on et−1. This, together with the finiteness of S × M, ensures
that {(St
, yt
, et)} is a weak Feller Markov chain.5
(ii) Then, by a property of weak Feller Markov chains (Meyn and Tweedie, 2009, Theorem 12.1.2(ii)), the boundedness of {et} ensured by Condition 2.3(ii) implies that
{(St
, yt
, et)} has at least one invariant probability measure.
The third property, given in the lemma below, concerns the behavior of {et} for different
initial e0. It is an important implication of Condition 2.3(i); actually, it is our purpose of
introducing the condition 2.3(i) in the first place. In the lemma, a.s. → stands for “converges
almost surely to.”
Lemma 2.1 Let {et} and {eˆt} be generated by the iteration (2.2) and (2.5), using the same
trajectory of states {St} and initial y0, but with different initial e0 and eˆ0, respectively. Then
under Conditions 2.1(i) and 2.3(i), et − eˆt
a.s. → 0.
Proof The proof is similar to that of (Yu, 2012, Lemma 3.2). Let ∆t = ket−eˆtk, and let Ft
denote the σ-algebra generated by Sk, k ≤ t. Note that under our assumption, in the generation of the two trace sequences {et} and {eˆt}, the states {St} and the memory states {yt} are
the same, but the λ-parameters are different. Let us denote them by {λt} and {λˆ
t} for the
two trace sequences, respectively. Then by (2.2), et − eˆt = γt ρt−1 (λtet−1 − λˆ
teˆt−1), and by
Condition 2.3(i), kλtet−1 −λˆ
teˆt−1k ≤ ket−1 −eˆt−1k. Hence ket −eˆtk ≤ γt ρt−1 ket−1 −eˆt−1k,
so E

∆t

Ft−1

≤ E

γt ρt−1

Ft−1

· ∆t−1 ≤ ∆t−1. This shows {(∆t
, Ft)} is a nonnegative
supermartingale. By the supermartingale convergence theorem (Dudley, 2002, Theorem
10.5.7 and Lemma 4.3.3), {∆t} converges a.s. to a nonnegative random variable ∆∞ with
E[∆∞] ≤ lim inft→∞ E[∆t
]. From the inequality ket −eˆtk ≤ γt ρt−1 ket−1 −eˆt−1k for all t, we
have ∆t ≤ ∆0 ·
Qt
k=1 γkρk−1, from which a direct calculation shows E

∆t

≤ ∆0 · 1
>(PΓ)t1
5. This means that for any bounded continuous function f on S × M × <n
(endowed with the usual
topology), with Xt = (St, yt, et), E

f(X1) | X0 = x

is a continuous function of x (Meyn and Tweedie,
2009, Prop. 6.1.1).
10        
Generalized Bellman Equations and TD Learning
where 1 denotes the n-dimensional vector of all 1’s. As t → ∞, (PΓ)t
converges to the zero
matrix under Condition 2.1(i). Therefore, lim inft→∞ E[∆t
] = 0 and consequently, we must
have ∆∞ = 0 a.s., i.e., ∆t
a.s. → 0.
We use Lemma 2.1 and ergodicity properties of weak Feller Markov chains (Meyn, 1989)
to prove the ergodicity theorem below. A direct application to LSTD will be discussed
immediately after the theorem, before we give its proof.
To state the result, we need some terminology and notation. For {(St
, yt
, et)} starting
from the initial condition x = (s, y, e), we write Px for its probability distribution, and we
write “Px-a.s.” for “almost surely with respect to Px.” The occupation probability measures
are denoted by {µx,t}, and they are random probability measures on S ×M × <n given by
µx,t(D) := 1
t
Pt−1
k=0 1

(Sk, yk, ek) ∈ D

∀ Borel sets D ⊂ S × M × <n
,
where 1(·) is the indicator function. We are interested in the asymptotic convergence of
these occupation probability measures in the sense of weak convergence: for probability
measures {µt} and µ on a metric space, {µt} converges weakly to µ if R
f dµt →
R
f dµ as
t → ∞, for every bounded continuous function f.
We shall also consider the Markov chain {(St
, St+1, yt
, et)}, whose occupation probability
measures are defined likewise. This Markov chain is essentially the same as {(St
, yt
, et)},
but it is more convenient for applying our ergodicity result to TD algorithms because
the temporal-difference term δt(v) involves (St
, St+1, et). Regarding invariant probability
measures of the two Markov chains, obviously, if ζ is an invariant probability measure of
{(St
, yt
, et)}, then an invariable probability measure of {(St
, St+1, yt
, et)} is the probability
measure ζ1 composed from the marginal ζ and the conditional distribution of S1 given
(S0, y0, e0) specified by P
o
; i.e.,
ζ1(D) = R P
s
0∈S P
o
ss0 1

(s, s0
, y, e) ∈ D

ζ

d(s, y, e)

∀ Borel sets D ⊂ S2 × M × <n
.
(2.13)
(In the above, we used the notation R
f(x) ζ(dx) to write the integral of f w.r.t. ζ, and the
notation ζ

d(s, y, e)

is the same as ζ(dx) with x = (s, y, e).)
Theorem 2.1 Let Conditions 2.1-2.3 hold. Then {(St
, yt
, et)} is a weak Feller Markov
chain and has a unique invariant probability measure ζ. For each initial condition x :=
(s, y, e) of (S0, y0, e0), the occupation probability measures {µx,t} converge weakly to ζ, Pxa.s.
Likewise, the same holds for {(St
, St+1, yt
, et)}, whose unique invariant probability measure is as given in (2.13).
If the initial distribution of (S0, y0, e0) is ζ, the state-trace process {(St
, yt
, et)} is stationary. Let Eζ denote expectation w.r.t. this stationary process. We now state a corollary
of the above theorem for LSTD, before we prove the theorem.
Consider the sequence of equations in v,
1
t
Pt−1
k=0 ek δk(v) = 0, appeared in (2.4) for
LSTD. From the definition (2.3) of δt(v),
δt(v) = ρt

Rt + γt+1v(St+1) − v(St)

     
Yu, Mahmood, and Sutton
we see that for fixed v, every ek δk(v) can be expressed as f(Sk, Sk+1, ek) for a continuous function f. Since the traces and hence the entire process lie in a bounded set under Condition 2.3(ii), the weak convergence of the occupation probabilities measures of
{(St
, St+1, yt
, et)} shown by Theorem 2.1 implies that this sequence of equations has an
asymptotic limit that can be expressed in terms of the stationary state-trace process as
follows.
Corollary 2.1 Let Conditions 2.1-2.3 hold. Then for each initial condition of (S0, y0, e0),
almost surely, the sequence of linear equations in v,
1
t
Pt−1
k=0 ek δk(v) = 0, tends asymptotically to Eζ [ e0 δ0(v)] = 0 (also a linear equation in v), in the sense that the random
coefficients in the former equations converge to the corresponding coefficients in the latter
equation as t → ∞.
In the rest of this section we prove Theorem 2.1. Broadly speaking, the line of argument
is as follows: We first prove the weak convergence of occupation probability measures to
the same invariant probability measure, for each initial condition. This will in turn imply
the uniqueness of the invariant probability measure.
After the proof we will first comment in Remark 2.1 on the differences between our
proof and that of a similar result in the previous work (Yu, 2012). We will then comment
in Remark 2.2 about the technical roles of Condition 2.3 (which concerns the choice of the
function λ(·)) and whether some part of that condition can be relaxed.
Proof of Theorem 2.1 As we discussed before Lemma 2.1, under Conditions 2.3,
{(St
, yt
, et)} is weak Feller and has at least one invariant probability measure ζ. Then,
by (Meyn, 1989, Prop. 4.1), there exists a set D ⊂ S × M × <n with ζ-measure 1 such
that for each initial condition x = (s, y, e) ∈ D, the occupation probability measures {µx,t}
converge weakly, Px-a.s., to an invariant probability measure µx that depends only on the
initial condition x. To prove the theorem using this result, we need to show that (i) all
these {µx | x ∈ D} are the same invariant probability measure, and (ii) for all x 6∈ D, {µx,t}
has the same weak convergence property.
To this end, we first consider an arbitrary pair (s, ys) in the recurrent class of {(St
, yt)}
(cf. Condition 2.2). Let us show that for all initial conditions x ∈ {(s, ys, e) | e ∈ <n},
{µx,t} converges weakly to the same invariant probability measure, almost surely.
Since the finite-state Markov chain {(St
, yt)} has a single recurrent class (Condition 2.2)
and its evolution is not affected by {et}, the marginal of ζ on S × M coincides with the
unique invariant probability distribution of {(St
, yt)}. So the fact that ζ(D) = 1 and (s, ys)
is a recurrent state of {(St
, yt)} implies that there exists some ˆe with (s, ys, eˆ) ∈ D. For
the initial condition ˆx = (s, ys, eˆ), by the result of (Meyn, 1989) mentioned earlier, {µx,t ˆ }
converges weakly to µxˆ, almost surely.
Now consider x = (s, ys, e) for an arbitrary e ∈ <n
. Generate iterates {eˆt} and {et}
according to (2.2), using the same trajectory {(St
, yt)} with (S0, y0) = (s, ys), but with
eˆ0 = ˆe and e0 = e. By Lemma 2.1, ˆet − et
a.s. → 0. Therefore, except on a null set of sample
12
Generalized Bellman Equations and TD Learning
paths, it holds for all bounded Lipschitz continuous functions f on S × M × <n
that6


R
f dµx,t ˆ −
R
f dµx,t

 =



1
t
Pt−1
k=0 f(Sk, yk, eˆk) −
1
t
Pt−1
k=0 f(Sk, yk, ek)



→ 0. (2.14)
By the a.s. weak convergence of µx,t ˆ to µxˆ proved earlier, except on a null set, R
R
f dµx,t ˆ →
f dµxˆ for all such functions f. Combining this with (2.14) yields that almost surely,
R
f dµx,t →
R
f dµxˆ for all such f. By (Dudley, 2002, Theorem 11.3.3), this implies that
almost surely, µx,t → µxˆ weakly.
Thus we have proved that for all initial conditions x = (s, ys, e), e ∈ <n
, {µx,t} converges
weakly, almost surely, to the same invariant probability measure µxˆ. Denote µ = µxˆ. Let
us now show that for any initial condition x, {µx,t} also converges to µ, Px-a.s.
Consider {(St
, yt
, et)} with an arbitrary initial condition ¯x = (¯s, y, ¯ e¯). Let τ = min{t |
(St
, yt) = (s, ys)} (the pair (s, ys) is as in the proof above). Note that τ < ∞ a.s., because
(s, ys) is a recurrent state of {(St
, yt)}. Define (S˜
k, y˜k) = (Sτ+k, yτ+k), ˜ek = eτ+k for k ≥ 0.
By the strong Markov property (see e.g. Nummelin, 1984, Theorem 3.3), {(S˜
k, y˜k)}k≥0
has the same probability distribution as the Markov chain {(St
, yt)} that starts from
(S0, y0) = (s, ys). Therefore, by the preceding proof, Px¯-almost surely, for all bounded
continuous functions f on S × M × <n
,
lim m→∞
1
m
Pm−1
k=0 f(S˜
k, y˜k, e˜k) = R
f dµ. (2.15)
Denote a ∧ b = min{a, b}. Using (2.15) and the fact τ < ∞ a.s., we have that Px¯-almost
surely,
lim
t→∞
1
t
Pt−1
k=0 f(Sk, yk, ek) = limt→∞ 
1
t
Pt∧(τ−1)
k=0 f(Sk, yk, ek) + 1
t
Pt−1
k=τ
f(Sk, yk, ek)

= lim
t→∞
1
t
Pt−τ−1
k=0 f(Sτ+k, yτ+k, eτ+k)
= lim m→∞
1
m
Pm−1
k=0 f(S˜
k, y˜k, e˜k) = R
f dµ.
This proves that {µx,t} converges weakly to µ almost surely, for each initial condition x.
It now follows that µ must be the unique invariant probability measure of {(St
, yt
, et)}.
To see this, suppose ζ is another invariant probability measure. For any bounded continuous
function f, by stationarity, Eζ [
1
t
Pt−1
k=0 f(Sk, yk, ek)] = R
f dζ for all t ≥ 1. On the other
hand, the preceding proof has established that for all initial conditions x,
1
t
Pt−1
k=0 f(Sk, yk, ek) = R
f dµx,t →
R
f dµ, Px-a.s.,
which implies that if ζ is the initial distribution of (S0, y0, e0), then 1
t
Pt−1
k=0
R
f(Sk, yk, ek) →
f dµ, Pζ -a.s. We thus have
R
f dζ = Eζ
h
1
t
Pt−1
k=0 f(Sk, yk, ek)
i
= lim
t→∞
Eζ
h
1
t
Pt−1
k=0 f(Sk, yk, ek)
i
= Eζ
h
lim
t→∞
1
t
Pt−1
k=0 f(Sk, yk, ek)
i
=
R
f dµ,
where the third equality follows from the bounded convergence theorem. This shows
R
f dζ =
R
f dµ for all bounded continuous functions f, and hence ζ = µ by (Dudley,
2002, Prop. 11.3.2), proving the uniqueness of the invariant probability measure.
The conclusions for the Markov chain {(St
, St+1, yt
, et)} follow from the same arguments
given above, if we replace St with (St
, St+1) and replace the set S with the set of possible
state transitions. (We could have proved the assertions for {(St
, St+1, yt
, et)} first and then
deduced as their implications the assertions for {(St
, yt
, et)}. We treated the latter first, as
it makes the notation in the proof simpler.)
Remark 2.1 (About the proof) Theorem 2.1 is similar to (Yu, 2012, Theorem 3.2) for
off-policy LSTD with constant λ (the analysis given in (Yu, 2012) also applies to statedependent λ). Some of the techniques used to prove the two theorems are also similar.
The main difference to (Yu, 2012) is that in the proof here we used an argument based
on the strong Markov property to extend the weak convergence property of {µx,t} for a
subset of initial conditions x ∈ {(s, ys, e) | e ∈ <n} to all initial conditions, whereas in
(Yu, 2012) this step was proved using a result on the convergence-in-mean of LSTD iterates
established first. The latter approach would not work here due to the dependence of λt on
the history. Indeed, due to this dependence, the proof of the convergence-in-mean of LSTD
given in (Yu, 2012) does not carry over to our case, even though that convergence does hold
as a consequence of Theorem 2.1, in view of the boundedness of traces by construction.
Compared with the proof of the ergodicity result in (Yu, 2012), the proof we gave here is
more direct and therefore better.
Regarding possible alternative proofs of Theorem 2.1, let us also mention that if we
prove first the uniqueness of the invariant probability measure, then, since {(St
, yt
, et)}t≥1
lie in a bounded set, the weak convergence of occupation probability measures will follow
immediately from (Meyn, 1989, Prop. 4.2). However, because the evolution of the λt
’s depends on both states and traces, it does not seem easy to us to prove directly the uniqueness
part first.
Remark 2.2 (About the conditions on the function λ(·)) Our proof of Theorem 2.1
relied on Lemma 2.1 and the two properties discussed preceding that lemma, namely, that
{(St
, yt
, et)} is a weak Feller Markov chain and has at least one invariant probability measure. As long as these hold when we weaken or change the conditions on the function λ(·),
the proof and the conclusions of the theorem will remain applicable.
We introduced Condition 2.3(ii) to bound the traces for algorithmic concerns. For the
ergodicity of the state-trace process, Condition 2.3(ii) is unimportant—in fact, it can be
removed from the conditions of Theorem 2.1. The reason is that we used this condition
before Lemma 2.1 to quickly infer that {(St
, yt
, et)} has at least one invariant probability
measure, but this is still true without Condition 2.3(ii), in view of (Meyn and Tweedie, 2009,
Theorem 12.1.2(ii)) and the fact that under Condition 2.1(i), {et} is bounded in probability
(the proof of this fact is straightforward and similar to the proof of (Yu, 2012, Lemma 3.1)
or (Yu, 2015, Prop. A.1)).
Condition 2.3(i) is actually two conditions combined into one. The first is the continuity
of λ(y, e)e in e for each y, which was used to ensure that the state-trace process is a weak
14
Generalized Bellman Equations and TD Learning
Feller Markov chain. To be more general, instead of letting the evolutions of the traces
and memory states be governed by the functions λ and g, one may consider letting them
be governed by stochastic kernels. Then by placing a suitable continuity condition on the
stochastic kernel λ, one can ensure that the state-trace process has the desired weak Feller
Markov property.
The second condition packed into Condition 2.3(i) is that for each y, λ(y, e)e is a Lipschitz continuous function of e with modulus 1. This condition is somewhat restrictive, and
one may consider instead allowing the function to have Lipschitz modulus greater than 1.
However, additional conditions are then needed to ensure that Lemma 2.1 holds. (If this
lemma does not hold, then the state-trace process may not be ergodic and one will need a
different approach than the one we took to characterize the sample path properties of the
state-trace process.)
From an algorithmic perspective, if it is desirable to choose even larger λt
’s or to have
greater flexibility in choosing these λ-parameters, some of the generalizations just mentioned
can be considered. For example, Condition 2.3(ii) can be replaced and stochastic kernels
can be introduced to allow for occasionally large traces et
, so that instead of having the
traces bounded, one only make their variances bounded in a desired range.
3. Generalized Bellman Equations
In this section, we continue the analysis started in Section 2.3. Recall that Corollary 2.1
established that the asymptotic limit of the linear equations (2.4) for LSTD is the linear
equation (in v):
Eζ [ e0 δ0(v)] = 0.
Our goal now is to relate this equation to a generalized Bellman equation for the target
policy π. This will then allow us to interpret solutions of (2.4) computed by LSTD as
solutions of approximate versions of that generalized Bellman equation.
To this end, we will first give a general description of randomized stopping times and
associated Bellman operators (Section 3.1). We will then use these notions to derive the
particular Bellman operators that correspond to our choices of the λ-parameters and appear
in the linear equations for LSTD (Section 3.2). We will also discuss a composite scheme of
choosing the λ-parameters as a direct application and extension of our results.
To simplify notation in subsequent derivations, we shall use the following shorthand
notation: For k ≤ m, denote S
m
k = (Sk, Sk+1, . . . Sm),
ρ
m
k =
Qm
i=k
ρi
, λm
k =
Qm
i=k
λi
, γm
k =
Qm
i=k
γi
. (3.1)
Also, we shall treat ρ
m
k = λ
m
k = γ
m
k = 1 if k > m.
3.1 Randomized Stopping Times and Associated Bellman Operators
Consider the Markov chain {St} induced by the target policy π. Let Condition 2.1(i) hold.
Recall that for the value function vπ, we have that for each state s ∈ S,
vπ(s) = E
π
s
P∞
t=0 γ
t
1
rπ(St)

(by definition)
15  
Yu, Mahmood, and Sutton
and
vπ(s) = rπ(s) + E
π
s
[γ1vπ(S1)].
The second equation is the standard one-step Bellman equation.
To write generalized Bellman equations for π, we shall make use of randomized stopping
times for {St}, a notion that generalizes naturally stopping times for {St} in that whether to
stop at time t depends not only on the past states S
t
0 but also on certain random outcomes.
A simple example is to toss a coin at each time and stop as soon as the coin lands on heads,
regardless of the history S
t
0
. (The corresponding Bellman equation is the one associated
with TD(λ) for a constant λ; cf. Example 3.1.) Of interest here is the general case where
the stopping decision does depend on the entire history.
To define a randomized stopping time formally, first, the probability space of {St} is
enlarged to take into account whatever randomization scheme that is used to make the
stopping decision. (The enlargement will be problem-dependent, as the next subsection
will demonstrate.) Then, on the enlarged space, a randomized stopping time τ for {St} is
a stopping time7
relative to some increasing sequence of σ-algebras F0 ⊂ F1 ⊂ · · · , where
the sequence {Ft} is such that
(i) for all t ≥ 0, Ft ⊃ σ(S
t
0
) (the σ-algebra generated by S
t
0
), and
(ii) relative to {Ft}, {St} remains to be a Markov chain with transition probability P,
i.e., for all s ∈ S, Prob(St+1 = s | Ft) = PSts.
See (Nummelin, 1984, Chap. 3.3); in particular, see Prop. 3.6 in p. 31-32 therein for several
equivalent definitions of randomized stopping times.
Note that if Ft = σ(S
t
0
) for all t, then the history of states S
t
0
fully determines whether
τ ≤ t and τ reduces to a stopping time for the Markov chain {St}. The properties (i)-(ii)
in the above definition encapsulate our earlier intuitive discussion about making stopping
decisions, namely, stopping decisions are made based on the history S
t
0
and additional
random outcomes that do not affect the evolution of the Markov chain.
Like stopping times, the strong Markov property also holds for randomized stopping
times for a Markov chain. This is an important basic property. It says that in the event
τ < ∞, conditioned on the σ-algebra Fτ associated with the stopping time τ relative to {Ft}
(which is the σ-algebra generated by the events that “happen before τ”), the conditional
distribution of (Sτ , Sτ+1, . . .) is the same as the probability distribution of a Markov chain
(S0, S1, . . .) with initial state S0 = Sτ (Nummelin, 1984, Theorem 3.3).
The above abstract definition of a randomized stopping time allows us to write Bellman
equations in general forms without worrying about the details of the enlarged space, which
are not important at this point. For notational simplicity, when there is no confusion, we
shall still write Pπ
for the probability measure on the enlarged probability space and use E
π
and E
π
s
to denote the expectation and conditional expectation given S0 = s, respectively,
w.r.t. Pπ
.
If τ is a randomized stopping time for {St}, the strong Markov property (Nummelin,
1984, Theorem 3.3) allows us to express vπ in terms of vπ(Sτ ) and the total discounted
7. A random time τ is called a stopping time relative to a sequence {Ft} of increasing σ-algebras if the
event {τ = t} ∈ Ft for every t.
16
Generalized Bellman Equations and TD Learning
rewards Rτ prior to stopping:
vπ(s) = E
π
s
hPτ−1
t=0 γ
t
1
rπ(St) + P∞
t=τ
γ
τ
1
· γ
t
τ+1 rπ(St)
i
= E
π
s

R
τ + γ
τ
1
vπ(Sτ )

, (3.2)
where Rτ =
Pτ−1
t=0 γ
t
1
rπ(St) for τ ∈ {0, 1, 2, . . .} ∪ {+∞}.
8 We can also write the Bellman
equation (3.2) in terms of {St} only, by taking expectation over τ :
vπ(s) = E
π
s
hP∞
t=0 
1(τ > t) · γ
t
1
rπ(St) + 1(τ = t) · γ
t
1
vπ(St)
i ,
= E
π
s
hP∞
t=0 
q
+
t
(S
t
0
) · γ
t
1
rπ(St) + qt(S
t
0
) · γ
t
1
vπ(St)
i , (3.3)
where
q
+
t
(S
t
0
) = P
π
(τ > t | S
t
0
), qt(S
t
0
) = P
π
(τ = t | S
t
0
). (3.4)
The r.h.s. of (3.2) or (3.3) defines a generalized Bellman operator T : <
N → <N associated
with τ , which has several equivalent expressions; e.g.,
(T v)(s)=E
π
s

Rτ + γ
τ
1
v(Sτ )

=E
π
s
hP∞
t=0
q
+
t
(S
t
0
) · γ
t
1
rπ(St) + qt(S
t
0
) · γ
t
1
v(St)
i, s ∈ S.
Depending on the context, one expression can be more convenient to use than the other.
For example, the first expression is convenient for defining T through the associated τ and
for deducing the contraction property of T, whereas expressions like the second will be of
interest when we want to know more explicitly the particular T for our TD learning scheme
and its dependence on the λ-parameters.
In common with one-step Bellman operator, the generalized Bellman operator T is affine
and involves a substochastic matrix. If τ ≥ 1 a.s., then the value function vπ is the unique
fixed point of T, i.e., the unique solution of v = T v, and T is a sup-norm contraction. In
fact, this can be shown for slightly more general τ :
Theorem 3.1 Let Condition 2.1(i) hold, and let the randomized stopping time τ be such
that Pπ
(τ ≥ 1 | S0 = s) > 0 for all states s ∈ S. Then vπ is the unique fixed point of the
generalized Bellman operator T associated with τ , and T is a contraction w.r.t. a weighted
sup-norm on <
N .
8. We explain the derivations in this footnote. In the case τ = 0, R
0 = 0. In the case τ = ∞, by
Condition 2.1(i), R
∞ =
P∞
t=0 γ
t
1 rπ(St) is almost surely well-defined, while the second term γ
τ
1 vπ(Sτ ) in
(3.2) is 0 because γ
∞1 := Q∞
k=1 γk = 0 a.s., under Condition 2.1(i). Equation (3.2) is derived as follows:
By the strong Markov property (Nummelin, 1984, Theorem 3.3), on {τ < ∞},
E
π
P∞
t=τ
γ
τ
1 · γ
t
τ+1 rπ(St) | Fτ

= γ
τ
1 · E
π
Sτ
P∞
t=0 γ
t
1 rπ(St)

= γ
τ
1 vπ(Sτ ).
Then, since the term E
π
s
P∞
t=τ
γ
τ
1 · γ
t
τ+1 rπ(St)

= E
π
s

1(τ < ∞) ·
P∞
t=τ
γ
τ
1 · γ
t
τ+1 rπ(St)

, we use the
property of the conditional expectation given Fτ and the fact Fτ ⊃ σ(S0) to rewrite this term as
E
π
s

1(τ < ∞) · E
π
P∞
t=τ
γ
τ
1 · γ
t
τ+1 rπ(St) | Fτ
  = E
π
s [ 1(τ < ∞) · γ
τ
1 vπ(Sτ )] = E
π
s [γ
τ
1 vπ(Sτ )],
where in the last equality we also used the fact γ
∞1 = 0 a.s. This gives (3.2).
17                
Yu, Mahmood, and Sutton
We prove this theorem in Appendix A. The proof amounts to showing that if a state
process evolves according to the substochastic matrix P˜ involved in the affine operator T,
then all the states in S are transient (equivalently, the spectral radius of P˜ is less than 1
and I − P˜ is invertible (Puterman, 1994, Appendix A.4)). From this the conclusions of the
theorem follow as a basic fact from nonnegative matrix theory (Seneta, 2006, Theorem 1.1),
and one specific choice of the weights of the sup-norm in the theorem is simply the expected
time for the process to leave S from each initial state (see e.g., the proof of (Bertsekas and
Tsitsiklis, 1996, Prop. 2.2)).
For TD algorithms that do not use history-dependent λ, the random times τ and the
corresponding Bellman operators T have simple descriptions:
Example 3.1 (TD with constant or state-dependent λ) Depending on the choice of
λ, TD(λ) algorithms are associated with different randomized stopping times τ . In the case
of constant λ, starting from time 1, we stop the system with probability 1 − λ if it has not
stopped yet; i.e.,
τ ≥ 1 and P
π
(τ = t | τ > t − 1, St
0
) = 1 − λ, ∀ t ≥ 1.
In particular, we always stop at t = 1 if λ = 0, and we never stop if λ = 1. Similarly, for
state-dependent λ where λt = λ(St), a function of the current state, the preceding stopping
probability is replaced by 1−λ(St): Pπ
(τ = t | τ > t−1, St
0
) = 1−λ(St) for t ≥ 1. In these
cases, by taking expectations over τ , the corresponding Bellman operators can be expressed
solely in terms of λ and the model parameters for the target policy.
3.2 Bellman Equation for the Proposed TD Learning Scheme
With the terminology of randomized stopping times, we are now ready to write down the
generalized Bellman equation associated with the TD learning scheme proposed in Section 2.2. It corresponds to a particular randomized stopping time. We shall first describe
this random time, from which a generalized Bellman equation follows as seen in the preceding subsection. That this is indeed the Bellman equation for our TD learning scheme will
then be proved.
Consider the Markov chain {St} under the target policy π. We define a randomized
stopping time τ for {St}:
• Let yt
, λt
, et
, t ≥ 1, evolve according to (2.5) and (2.2):
yt = g(yt−1, St), λt = λ(yt
, et−1), et = λt γt ρt−1 et−1 + φ(St), t ≥ 1.
• Let the initial (S0, y0, e0) be distributed according to ζ, the unique invariant probability measure in Theorem 2.1 for the state-trace process induced by the behavior
policy.
• At time t ≥ 1, we stop the system with probability 1 − λt
if it has not yet been
stopped. Let τ be the time when the system stops (τ = ∞ if the system never stops).
To make the dependence on the initial distribution ζ explicit, we write Pπ
ζ
for the probability
measure of this process.
18
Generalized Bellman Equations and TD Learning
Note that by definition λt and λ
t
1 =
Qt
k=1 λk are functions of the initial (y0, e0) and
states S
t
0
. From how the random time τ is defined, we have for all t ≥ 1,
P
π
ζ
(τ > t | S
t
0
, y0, e0) = λ
t
1 =: h
+
t
(y0, e0, St
0
), (3.5)
P
π
ζ
(τ = t | S
t
0
, y0, e0) = λ
t−1
1
(1 − λt) =: ht(y0, e0, St
0
), (3.6)
and hence
q
+
t
(S
t
0
) := P
π
ζ
(τ > t | S
t
0
) = Z
h
+
t
(y, e, St
0
) ζ

d(y, e) | S0

, (3.7)
qt(S
t
0
) := P
π
ζ
(τ = t | S
t
0
) = Z
ht(y, e, St
0
) ζ

d(y, e) | S0

, (3.8)
where ζ(d(y, e) | s) is the conditional distribution of (y0, e0) given S0 = s, w.r.t. the initial
distribution ζ. As before, we can write the generalized Bellman operator T associated
with τ in several equivalent forms. Let E
π
ζ
denote expectation under Pπ
ζ
. Similarly to the
derivation of (3.3), we can rewrite (3.2) in this case by taking expectation over τ conditioned
on (S
t
0
, y0, e0) to derive that for all v : S → <, s ∈ S,
(T v)(s) = E
π
ζ
hP∞
t=0 λ
t
1
γ
t
1
rπ(St) + P∞
t=1 λ
t−1
1
(1 − λt)γ
t
1
v(St) | S0 = s
i
. (3.9)
Or express T in the form of (3.3) by further integrating over (y0, e0) and using (3.7)-(3.8):
(T v)(s) = E
π
ζ
hP∞
t=0 
q
+
t
(S
t
0
) · γ
t
1
rπ(St) + qt(S
t
0
) · γ
t
1
v(St)
 
 S0 = s
i
, (3.10)
for all v : S → <, s ∈ S, where in the case t = 0, q
+
0
(S0) = 1 and q0(S0) = 0 since τ > 0 by
construction.
It will be useful later to express T V − V in terms of temporal differences. From (3.9),
by writing λ
t−1
1
(1 − λt)γ
t
1
v(St) = λ
t−1
1
γ
t
1
v(St) − λ
t
1
γ
t
1
v(St) and rearranging terms, we have
for all v : S → <, s ∈ S,
(T v)(s) − v(s) = E
π
ζ
hP∞
t=0 λ
t
1
γ
t
1
rπ(St) + P∞
t=0 λ
t
1
γ
t+1
1
v(St+1) −
P∞
t=0 λ
t
1
γ
t
1
v(St) | S0 = s
i
= E
π
ζ
hP∞
t=0 λ
t
1
γ
t
1
·

rπ(St) + γt+1 v(St+1) − v(St)
 
 S0 = s
i
. (3.11)
In a similar way, from (3.10), we can write9
(T v)(s) − v(s) = E
π
ζ
hP∞
t=0 q
+
t
(S
t
0
) · γ
t
1
·

rπ(St) + γt+1 v(St+1) − v(St)
 
 S0 = s
i
.
Remark 3.1 Comparing the two expressions (3.9) and (3.10) of T, we remark that the
expression (3.9) reflects the role of the λt
’s in determining the stopping time, whereas the
expression (3.10), which has eliminated the auxiliary variables yt and et
, shows more clearly
the dependence of the stopping time on the entire history S
t
0
. It can also be seen, from the
9. Since τ is a randomized stopping time for the Markov chain {St}, we have P
π
ζ
(τ > t | S
t+1
0
) = P
π
ζ
(τ >
t | S
t
0), so P
π
ζ
(τ > t | S
t
0) − P
π
ζ
(τ = t + 1 | S
t+1
0
) = P
π
ζ
(τ > t + 1 | S
t+1
0
), i.e., q
+
t
(S
t
0) − qt+1(S
t+1
0
) =
q
+
t+1(S
t+1
0
). Thus we can write the term qt(S
t
0) in (3.10) for t ≥ 1 as q
+
t−1
(S
t−1
0
) − q
+
t
(S
t
0), and the
expression for (T v − v)(s) then follows by rearranging terms.
  
Yu, Mahmood, and Sutton
initial distribution ζ, the dependence of λt on the traces and the dependence of the traces
on the function ρ(·) (which describes importance sampling ratios), that both the behavior
policy and the choice of the feature representation assert a significant role in determining
the Bellman operator T for the target policy. This is in contrast with off-policy TD learning
that uses a constant λ, where the behavior policy and the approximation subspace affect
only how one approximates the Bellman equation underlying TD, not the Bellman equation
itself, which is solely determined by λ (cf. Example 3.1).
Furthermore, note that as the invariant distribution of the state-trace process, ζ is
associated with the dynamic behavior of the states and traces under the behavior policy.
Generally, there is no explicit expression of ζ in terms of P
o and the parameters in the
λ function. As a result, in general we cannot express the operator T in terms of these
parameters in the learning scheme. This is different from the case of TD(λ) where λ is a
function of the present state only.
We now proceed to show how the Bellman equation v = T v given above relates to the
off-policy TD learning scheme in Section 2.2. Some notation is needed. Denote by ζS the
invariant probability measure of the Markov chain {St} induced by the behavior policy; note
that it coincides with the marginal of ζ on S. For two functions v1, v2 on S, we write
v1 ⊥ζS
v2 if P
s∈S ζS(s) v1(s) v2(s) = 0. If L is a linear subspace of functions on S and
v ⊥ζS
v
0
for all v
0 ∈ L, we write v ⊥ζS L. Recall that φ is a function that maps each
state s to an n-dimensional feature vector. Denote by Lφ the subspace spanned by the n
component functions of φ, which is the space of approximate value functions for our TD
learning scheme. Recall also that Eζ denotes expectation w.r.t. the stationary state-trace
process {(St
, yt
, et)} under the behavior policy (cf. Theorem 2.1).
Theorem 3.2 Let Conditions 2.1-2.3 hold. Then as a linear equation in v, Eζ

e0 δ0(v)

= 0
is equivalently T v − v ⊥ζS Lφ, where T is the generalized Bellman operator for π given in
(3.9) or (3.10).
Remark 3.2 (On LSTD) Note that
T v − v ⊥ζS Lφ, v ∈ Lφ
is a projected version of the generalized Bellman equation T v − v = 0 (projecting the lefthand side onto the approximation subspace Lφ w.r.t. the ζS-weighted Euclidean norm).
Theorem 3.2 and Corollary 2.1 together show that this is what LSTD solves in the limit.
Note also that although the generalized Bellman operator T is a contraction (Theorem 3.1), the composition of projection with T is in general not a contraction (cf. Example B.1 in Appendix B). Thus we cannot use contraction-based arguments to analyze
approximation properties. For that purpose, we use the oblique projection viewpoint of
Scherrer (2010). Specifically, if the preceding projected Bellman equation admits a unique
solution ¯v, then ¯v can be viewed as an oblique projection of vπ (Scherrer, 2010) and the
approximation error ¯v − vπ can be characterized as in (Yu and Bertsekas, 2010) by using
the oblique projection viewpoint. The details of these are given in Appendix B.
Remark 3.3 (On gradient-based TD) While Theorem 3.2 is about the LSTD algorithm, it also helps prepare the ground for analyzing gradient-based algorithms similar to
20  
Generalized Bellman Equations and TD Learning
those discussed in (Maei, 2011; Mahadevan et al., 2014). Like LSTD, these algorithms aim
to solve the same projected generalized Bellman equation as characterized by Theorem 3.2
(cf. Remark 3.2). Their average dynamics, which is important for analyzing their convergence using the mean ODE approach from stochastic approximation theory (Kushner and
Yin, 2003), can be studied based on the ergodicity result of Theorem 2.1, in essentially the
same way as we did in Section 2.3 for the LSTD algorithm. For details of the convergence
analysis of these gradient-based TD algorithms, see the recent work (Yu, 2017).
In the rest of this subsection, we give a corollary to Theorem 3.2, deferring the proofs
of both the theorem and the corollary to the next subsection. The corollary concerns
a composite scheme of setting λ, which is slightly more general than what Section 2.2
described. It results in a Bellman operator that is a composition of the components of
other Bellman operators, and it can be useful in practice for variance control. Let us
describe the scheme first, before explaining our motivation for it.
Partition the state space into m nonempty disjoint sets: S = ∪
m
i=1Si
. Associate each set
Si with a possibly different scheme of setting λ that is of the type described in Section 2.2,
and denote its memory states by y
(i)
t
and λ-function by λ
(i)
(·, ·). Keep m trace vectors
e
(1)
t
, . . . , e
(m)
t
, one for each set, and update them according to
e
(i)
t = λ
(i)
t
γt ρt−1 e
(i)
t−1 + φ(St) 1(St ∈ Si), 1 ≤ i ≤ m, (3.12)
where λ
(i)
t = λ
(i)

y
(i)
t
, e
(i)
t−1

. We then have m ergodic state-trace processes that share the
same state variables, St
, y
(i)
t
, e
(i)
t
	, i = 1, 2, . . . , m. Each process has a unique invariant
probability measure ζ
(i)
(Theorem 2.1) and an associated randomized stopping time τ
(i) and
generalized Bellman operator T
(i)
, as discussed in this subsection. Define now an operator
T by concatenating the component mappings of T
(i)
for Si as follows: for all v ∈ <N and
s ∈ S,
(T v)(s) := (T
(i)
v)(s) if s ∈ Si
. (3.13)
Consider an LSTD algorithm that defines the trace et to be the sum of the m trace vectors,
et =
Pm
i=1 e
(i)
t
, (3.14)
and uses the traces to form the linear equation as before,
1
t
Pt−1
k=0 ek δk(v) = 0, v = Φθ.
Note that 1
t
Pt−1
k=0 ekδk(v) = 0 is the same as Pm
i=1
1
t
Pt−1
k=0 e
(i)
k
δk(v) = 0. By Corollary 2.1,
as a linear equation in v, it tends asymptotically (as t → ∞) to the linear equation
Pm
i=1 Eζ
(i)

e
(i)
0
δ0(v)

= 0.
Corollary 3.1 Let Condition 2.1 hold. Consider the composite scheme of setting λ discussed above, and let Conditions 2.2-2.3 hold for each of the m schemes involved. Let LSTD
calculate traces according to (3.12) and (3.14). Then the limiting linear equation (in v) associated with LSTD, Pm
i=1 Eζ
(i)

e
(i)
0
δ0(v)

= 0, is equivalently T v − v ⊥ζS Lφ, where T is
the generalized Bellman operator for π given by (3.13) and has the same fixed point and
contraction properties as stated in Theorem 3.1.
      
Yu, Mahmood, and Sutton
The use of composite schemes will be demonstrated by experiments in Section 4.2.2.
Here let us explain informally our motivation for such schemes.
Remark 3.4 (About composite schemes of setting λ) Our motivation for using the
composite schemes is revealed by the equation (3.13). Typically each T
(i)
is designed to
be simple to implement in TD learning. For example, if we ignore for now the bounding
of traces introduced in Section 2.2 and just consider TD(λ) with constant λ, T
(i)
can be
the Bellman operator T
(λ)
for TD(λ) with some constant λ. A simple, extreme example
is to partition the state space into two sets, and associate one with T
(λ)
, λ = 1, and the
other with T
(λ)
, λ = 0. Using the combination (3.13) of the two operators in TD then
means that for the first set of states whose λ = 1, we want to estimate their values by
using the information about the total rewards received when starting from those states,
whereas for the second set of states whose λ = 0, we only use the information about their
one-stage rewards and how these states relate to the “neighboring” states in the transition
graph. While this way of using different kinds of information for different states is natural
and useful for TD-based policy evaluation, it cannot be realized by keeping a single trace
sequence as before and only letting λt evolve with states or histories. Indeed, in that case,
as discussed in Section 2.2.2, interleaving large and small λt
’s would make the algorithm
behave effectively like TD with small λ over the entire state space.
In the context of the more complex scheme of setting λ discussed in this paper, our
motivation and reasons for considering composite schemes are the same. Each T
(i)
can be
designed to be simple to implement, such as in the simple scaling example in Section 2.2.
The parameters in the ith scheme can be chosen so that they encourage the use of large λt
’s
throughout time or dictate the use of only small λt
’s. By combining component mappings
of T
(i)
through (3.13), composite schemes allow us to use cumulative rewards and transition
structures at different timescales for different states. This provides additional flexibility in
managing the bias-variance trade-off when estimating the value function (see Figure 9 and
Figure 11 in Section 4.2.2 for a demonstration).
Finally, we mention that for off-policy LSTD(λ) with constant λ, composite schemes
were proposed in (Yu and Bertsekas, 2012) and analyzed in (Yu, 2012, Proposition 4.5, Section 4.3). Our Corollary 3.1 extends that result. The convergence analysis of the gradientbased algorithms for the composite schemes is given in (Yu, 2017).
3.3 Proofs of Theorem 3.2 and Corollary 3.1
We divide the proof of Theorem 3.2 into two steps. The first step deals with an expression
for the trace vector, given in the following lemma. It is more subtle than the other step in
the proof, which involves mostly calculations.
We start by extending the stationary state-trace process {(St
, yt
, et)}t≥0 to t = −1,
−2, . . ., and work with a double-ended stationary process {(St
, yt
, et)}−∞<t<∞ (by Kolmogorov’s existence theorem (Dudley, 2002, Theorem 12.1.2), such a process exists). Note
that as before this is a Markov chain whose transition probability is defined by the behavior policy π
o
together with the update rules (2.2) and (2.5) for et
, yt and λt
, and the
marginal distribution of each (St
, yt
, et) is ζ. We keep using the notation Pζ and Eζ for this
double-ended stationary Markov chain.
22
Generalized Bellman Equations and TD Learning
Recall the shorthand notation (3.1) introduced at the beginning of Section 3: For k ≤ m,
ρ
m
k =
Qm
i=k
ρi
, λ
m
k =
Qm
i=k
λi
, γ
m
k =
Qm
i=k
γi
, and in addition, λ
0
1 = γ
0
1 = ρ
−1
0 = 1 by
convention.
Lemma 3.1 Pζ -almost surely, P∞
t=1 λ
0
1−t
γ
0
1−t
ρ
−1
−t φ(S−t) is well-defined and finite, and
e0 = φ(S0) + P∞
t=1 λ
0
1−t
γ
0
1−t
ρ
−1
−t φ(S−t). (3.15)
Proof First, we show Eζ
P∞
t=1 γ
0
1−t
ρ
−1
−t

< ∞. Indeed,
Eζ
P∞
t=1 γ
0
1−t
ρ
−1
−t

=
P∞
t=1 Eζ

γ
0
1−t
ρ
−1
−t

=
P∞
t=1 ζ
>
S
(PΓ)t1 < ∞,
where the first equality follows from the monotone convergence theorem, the second equality
from Condition 2.1(ii) and a direct calculation, and the last inequality follows from Condition 2.1(i) (since (I − PΓ)−1 =
P∞
t=0(PΓ)t
). This implies P∞
t=1 γ
0
1−t
ρ
−1
−t < ∞, Pζ -a.s., so
γ
0
1−t
ρ
−1
−t → 0 as t → ∞, Pζ -a.s. Since λ
0
1−t ≤ 1 for all t, it also implies that
Eζ
P∞
t=1 λ
0
1−t
γ
0
1−t
ρ
−1
−t
kφ(S−t)k

≤ maxs∈S kφ(s)k · Eζ
P∞
t=1 γ
0
1−t
ρ
−1
−t

< ∞. (3.16)
It then follows from a theorem on integration (Rudin, 1966, Theorem 1.38, p. 28-29) that
Pζ -almost surely, the infinite series P∞
t=1 λ
0
1−t
γ
0
1−t
ρ
−1
−t φ(S−t) converges to a finite limit.
We now prove the expression for e0. By unfolding the iteration (2.2) for et backwards
in time, we have for all m ≥ 1,
e0 = φ(S0) + Pm−1
t=1 λ
0
1−t
γ
0
1−t
ρ
−1
−t φ(S−t) + λ
0
1−mγ
0
1−mρ
−1
−m e−m. (3.17)
Let m → ∞ in the r.h.s. of (3.17). For the last term, the trace e−m lies in a bounded set
by Condition 2.3(ii), λ
0
1−m ≤ 1, and as we just showed, γ
0
1−mρ
−1
−m → 0, Pζ -a.s. So the last
term converges to zero Pζ -a.s. Also as we just showed, the second term converges Pζ -almost
surely to P∞
t=1 λ
0
1−t
γ
0
1−t
ρ
−1
−t φ(S−t). The expression (3.15) for e0 then follows.
Proof of Theorem 3.2 Treating λ
0
1 = γ
0
1 = ρ
−1
0 = 1, we write the expression of e0 given
in Lemma 3.1 as e0 =
P∞
t=0 λ
0
1−t
γ
0
1−t
ρ
−1
−t φ(S−t), Pζ -a.s. We use this expression to calculate
first Eζ

e0 · ρ0f(S
1
0
)

for an arbitrary function f on S × S. (Note that f is bounded and
measurable, since S is finite.) We have
Eζ

e0 · ρ0f(S
1
0
)

=
P∞
t=0 Eζ
h
λ
0
1−t
γ
0
1−t
ρ
−1
−t φ(S−t) · ρ0f(S
1
0
)
i
=
P∞
t=0 Eζ
h
λ
t
1
γ
t
1
ρ
t−1
0 φ(S0) · ρtf(S
t+1
t
)
i
=
P∞
t=0 Eζ
h
φ(S0) · Eζ

λ
t
1
γ
t
1
ρ
t
0
f(S
t+1
t
) | S0, y0, e0

i
(3.18)
where we used the stationarity of the double-ended state-trace process to derive the second
equality, and we changed the order of expectation and summation in the first equality. This
change is justified by the dominated convergence theorem (cf. (3.16)), and so are similar
interchanges of expectation and summation that will appear in the rest of this proof.
To proceed with the calculation, we relate the expectations in the summation in (3.18)
to expectations w.r.t. the process with probability measure Pπ
ζ
introduced in Section 3.2
23                
Yu, Mahmood, and Sutton
(which we recall is induced by the target policy π and involves the randomized stopping time
τ ). Let E˜π
ζ
denote expectation w.r.t. the marginal of Pπ
ζ
on the space of {(St
, yt
, et)}t≥0.
From the change of measure performed through ρ
t
0
, we have
Eζ

λ
t
1γ
t
1ρ
t
0
f(S
t+1
t
) | S0, y0, e0

= E˜π
ζ

λ
t
1γ
t
1
f(S
t+1
t
) | S0, y0, e0

, t ≥ 0. (3.19)
Combining this with (3.18) and using the fact that ζ is the marginal distribution of (S0, y0, e0)
in both processes, we obtain
Eζ

e0 · ρ0f(S
1
0
)

=
P∞
t=0 E˜π
ζ
h
φ(S0) · E˜π
ζ

λ
t
1
γ
t
1
f(S
t+1
t
) | S0, y0, e0

i
= E˜π
ζ
h
φ(S0) ·
P∞
t=0 E˜π
ζ

λ
t
1
γ
t
1
f(S
t+1
t
) | S0

i
. (3.20)
We now use (3.20) to calculate Eζ

e0 δ0(v)

for a given function v. Recall from (2.3) that
δ0(v) = ρ0 ·

r(S
1
0
) + γ1v(S1) − v(S0)

, so we let f(S
t+1
t
) = r(S
t+1
t
) + γt+1v(St+1) − v(St) in
(3.20). Since E
π
ζ
[r(S
t+1
t
) | S
t
0
] = rπ(St), we have
P∞
t=0 E˜π
ζ

λ
t
1
γ
t
1
f(S
t+1
t
) | S0

=
P∞
t=0 E˜π
ζ
h
λ
t
1
γ
t
1

rπ(St) + γt+1v(St+1) − v(St)
 


S0
i
= (T v − v)(S0),
where the last equality follows from the expression (3.11) for T V −V . Therefore, by (3.20),
Eζ

e0 δ0(v)

=
P
s∈S ζS(s) φ(s) · (T v − v)(s), (3.21)
and this shows that Eζ

e0 δ0(v)

= 0 is equivalent to T v − v ⊥ζS Lφ.
We now prove Corollary 3.1.
Proof of Corollary 3.1 We apply Theorem 3.2 to each state-trace process St
, y
(i)
t
, e
(i)
t
	
for i = 1, 2, . . . , m. Specifically, by (3.21) and the definition (3.12) of e
(i)
t
,
Eζ
(i)

e
(i)
0
δ0(v)

=
P
s∈S ζS(s) · φ(s)1(s ∈ Si) ·

T
(i)v − v

(s).
Hence
Pm
i=1 Eζ
(i)

e
(i)
0
δ0(v)

=
P
s∈S ζS(s) φ(s) ·
hPm
i=1 1(s ∈ Si) ·

T
(i)v − v

(s)
i
=
P
s∈S ζS(s) φ(s) · (T v − v)(s),
where the last equality follows from the definition (3.13) of the operator T. This shows that
the linear equation in v,
Pm
i=1 Eζ
(i)

e
(i)
0
δ0(v)

= 0, is equivalently T v − v ⊥ζS Lφ.
We now prove that T has vπ as its unique fixed point and is a contraction with respect
to a weighted sup-norm—in other words, Theorem 3.1 applies to T. For this, it suffices to
show that T satisfies the conditions of Theorem 3.1, namely, T is a generalized Bellman
operator associated with a randomized stopping time τ that satisfies Pπ
(τ ≥ 1 | S0 =
s) > 0 for all states s ∈ S. We can define such a random time τ from the randomized
stopping times τ
(i) associated with the Bellman operators T
(i)
. In particular, by enlarging                            
Generalized Bellman Equations and TD Learning
the probability space if necessary, we can regard τ
(i)
, i = 1, 2, . . . , m, as being defined on
the same probability space.10 We then let τ = τ
(i)
if S0 ∈ Si
. With this definition, we have
Pπ
(τ ≥ 1 | S0 = s) > 0 for all states s ∈ S (since τ
(i) ≥ 1 a.s. for all i). For each set Si
,
by (3.2), the component mappings of the generalized Bellman operator Tτ associated with
τ are given by
(Tτ v)(s)=E
π
s

Rτ + γ
τ
1
v(Sτ )

=E
π
s

Rτ
(i)
+ γ
τ
(i)
1
v(Sτ
(i) )

= (T
(i)v)(s), s ∈ Si
.
So Tτ = T by the definition (3.13) of T; i.e., T is the Bellman operator associated with the
randomized stopping time τ .
4. Numerical Study
In this section, we first use a toy problem to illustrate the behavior of traces calculated by
off-policy LSTD(λ) for constant λ and for λ that evolves according to a simple special case
of our proposed scheme described in Example 2.1. We then compare the behavior of LSTD
for various choices of λ, on the toy problem and on the Mountain Car problem.
4.1 Behavior of Traces
The toy problem we use in this study has 21 states, arranged as shown in Figure 1 (left).
One state is located at the centre, and the rest of the states split evenly into four groups,
indicated by the four loops in the figure. The topology of the transition graph is the same for
the target and behavior policies. We have drawn the transition graph only for the northeast
group in Figure 1 (left); the states in each of the other three groups are arranged in the
same manner and have the same transition structure. Given this symmetry, to specify the
transition matrices P and P
o
for the target and behavior policies π and π
o
respectively, it
suffices to specify the submatrices for the central state and one of the groups. If we label
the central state as state 1 and the states in the northeast group clockwise as states 2-6,
the submatrices of P and P
o
for these states are given, respectively, by
π :


0 0.25 0 0 0 0
0 0 1 0 0 0
0 0.2 0 0.8 0 0
0 0 0.2 0 0.8 0
0 0 0 0.2 0 0.8
0.8 0 0 0 0.2 0


, πo
:


0 0.25 0 0 0 0
0 0 1 0 0 0
0 0.5 0 0.5 0 0
0 0 0.5 0 0.5 0
0 0 0 0.5 0 0.5
0.5 0 0 0 0.5 0


.
Intuitively speaking, from the central state, the system enters one group of states by moving
diagonally in one of the four directions with equal probability, and after spending some time
in that group, eventually returns to the central state and the process repeats. The behavior
policy on average spends more time wandering inside each group than the target policy,
while the target policy tends to traverse clockwise through the group more quickly.
10. That we can do so is clear from the definition of each τ
(i)
as described at the beginning of Section 3.2
and from the fact that for each i, the initial distribution ζ
(i)
on (S0, y
(i)
0
, e
(i)
0
) has the same marginal on
S, which is ζS .
25    
Yu, Mahmood, and Sutton
northeast
group
central
state
southeast
group
1
1
.8/.5
.8/.5
.8/.5
.8/.5
Figure 1: The transition graph of a toy problem (left) and a cycle pattern in it (right). The
numbers appearing in the right graph indicate the importance sampling ratios for
the state transitions represented by each directed edge. From such cycle patterns
one can infer whether the trace sequence is unbounded almost surely.
All the rewards are zero except for the middle state in each group—for the northeast
group, this is the shaded state in Figure 1 (left). For the two northern (southern) groups,
their middle states have reward 1 (−1). The discount factor is γ = 0.9 for all states. As to
features, we aggregate states into 5 groups: the 4 groups mentioned earlier and the central
state forming its own group, and we let each state have 5 binary features indicating its
membership.
We now discuss and illustrate the behavior of traces in this toy problem. For comparison,
we first do this for the off-policy TD(λ) with a constant λ. It can help explain the challenges
in off-policy TD learning and our motivation for proposing the new scheme of setting λ.
4.1.1 Traces for Constant λ
In this experiment we let λ = 1 and consider the trace iterates {et} calculated by TD(1). In
general, by identifying certain cycle patterns in the transition graph, one can infer whether
{et} will be unbounded over time almost surely (Yu, 2012, Section 3.1). Figure 1 (right)
shows such a cycle of states in the transition graph of the toy problem. It consists of the
central state and the northeast group of states. Labeled on each edge of the cycle is the
importance sampling ratio for that state transition. Traversing through the cycle once from
any starting state, and multiplying together the importance sampling ratios of each edge
and the discount factors of the destination states, we get
0.8
0.5
4
· γ
6 =

0.8
0.5
4
· 0.9
6 > 1.
From this one can infer that {et} calculated by off-policy TD(1) will be unbounded in this
problem (cf. Yu, 2012, Prop. 3.1).
We plotted in the upper left graph of Figure 2 the Euclidean norm ketk of the traces
over 8 × 105
iterations for TD(1). One can see the recurring spikes and the exceptionally
large values of some of these spikes in the plot. This is consistent with the unboundedness
of {et} just discussed.
The unboundedness of {et} tells us that the invariant probability measure ζ of the
state-trace process {(St
, et)} has an unbounded support. Despite this unboundedness, {et}
is bounded in probability (Yu, 2012, Lemma 3.4) and under the invariant distribution ζ,
Eζ

ke0k

< ∞ (Yu, 2012, Prop. 3.2). The latter property implies that under the invariant
distribution, the probability of ke0k > x decreases as o(1/x) for large x. Since the empirical
    
Generalized Bellman Equations and TD Learning
0 1 2 3 4 5 6 7 8
x 105
0
1000
2000
3000
4000
5000
norm of traces
t
10 12 14 16 18 20 22 24 26 28
0
5
10
histogram for segments of traces whose norm > 100
0 20 40 60 80 100 120 140 160 180 200
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
fractions of traces whose norm > x
x
Figure 2: Statistics of traces for TD(1) in a toy problem (see the text in Section 4.1.1 for
detailed explanations).
distribution of the state-trace process converges to ζ almost surely (Yu, 2012, Theorem 3.2),
during a run of many iterations, we expect to see the fraction of traces with ketk > x drop
in a similar way as x increases.
The simulation result shown in the right part of Figure 2 agrees with the preceding
discussion. Plotted in the graph are fractions of traces with ketk > x during 8 × 105
iterations (the vertical axis indicates the fraction, and the horizontal axis indicates x). It
can be seen that despite the recurring spikes in ketk during the entire run, the fraction of
traces with large magnitude x drops sharply with the increase in x.
While only a small fraction of traces have exceptionally large magnitude, they can
occur in consecutive iterations. This is illustrated by the histogram in the lower left part of
Figure 2. The histogram concerns the excursions of the trajectory {et} outside of the ball
{e ∈ <n
| kek ≤ 100}. The horizontal axis indicates the lengths of the excursions (where
the length is the number of iterations an excursion contains), and the vertical axis indicates
how many excursions of length x occurred during the 8 × 105
iterations of the experimental
run. We plotted the histogram for lengths x > 10. It can be seen that one can have large
traces during many consecutive iterations. Such behavior, although tolerable by LSTD, is
especially detrimental to TD algorithms and can disrupt their learning. This is our main
motivation for suggesting the use of λ-parameters to bound the traces directly.
4.1.2 Traces with Evolving λ
We now proceed to illustrate the behavior of traces and LSTD for λ that evolves according
to our proposed scheme. Specifically, for this demonstration, we will use the simple scaling
example given by (2.6)-(2.7) in Example 2.1, with all the thresholds Css0 being the same
constant C. That is, the update rule for et used in this experiment is
et =
(
γt ρt−1 et−1 + φ(St) if γtρt−1ket−1k2 ≤ C;
C ·
et−1
ket−1k2
+ φ(St) otherwise.
(4.1)
27
Yu, Mahmood, and Sutton
We first simulate the state-trace process to illustrate the ergodicity of this process stated by
Theorem 2.1. We will shortly study the performance of LSTD for different values of C in
Section 4.2.1. The results of these two experiments are shown in Figure 3 and Figures 4-5,
respectively, and the details are as follows.
According to the ergodicity result of Theorem 2.1, no matter from which initial state
and trace pair (S0, e0) we generate a trajectory {(St
, et)}0≤t≤t¯ according to the behavior
policy, the empirical distribution of state and trace pairs in this trajectory should converge,
as t¯ → ∞, to the same distribution on S × <n
, which is the marginal of the invariant
probability measure ζ on that space. Since S is discrete, we can verify this fact by examining
the empirical conditional distribution of the trace given the state. In other words, for each
state s, we examine the empirical distribution of the trace for the sub-trajectory (Stk
, etk
),
k = 1, 2, . . ., where Stk = s and it is the kth visit to state s by the trajectory. We check
if this empirical distribution converges to the same one as we increase the length t¯ of the
trajectory and as we vary the initial condition (S0, e0).
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1
0 20 40 60
0
0.2
0.4
0.6
0.8
1
0 10 20 30 40 50
0
0.2
0.4
0.6
0.8
1
0 1 2 3 4
x 104
0
0.5
1
kth visit
0 2 4 6
x 104
0
0.5
1
kth visit
0 1 2 3 4
x 104
0
0.5
1
kth visit
0 1 2 3 4
x 10
4
0
0.5
1
kth visit
Figure 3: Demonstration of convergence of empirical conditional distributions on the trace
space. (See the text in Section 4.1.2 for detailed explanations.)
To give a sense of what these limiting distributions over the trace space look like, we
set the parameter C = 50 and generated a long trajectory with 8 × 105
iterations. In the
top row of Figure 3, we plotted three normalized histograms of the first trace component
for the sub-trajectories associated with three states of the toy problem, respectively: the
central state (left), the middle state of the northeast group (middle), and the first state of
the southeast group (right).
To check whether the empirical conditional distributions on the trace space converge
along the sub-trajectory {(Stk
, etk
)} for a given state, we compare the characteristic functions fk of these distributions with the characteristic function f of the empirical conditional
distribution obtained at the end of the sub-trajectory during the experimental run. In par28
Generalized Bellman Equations and TD Learning
ticular, we evaluate all these (complex-valued) characteristic functions at 500 points, which
are chosen randomly according to the multivariate normal distribution on <
5 with mean 0
and covariance matrix 2002
I. We take the maximal difference between fk and f at these
500 points as an indicator of the deviation between the two corresponding distributions.11
In the bottom row of Figure 3, the first three plots show the difference curves obtained in
the way just described, for three different states, respectively. These three states are the
same ones mentioned earlier in the description of the top row of Figure 3. The horizontal
axis of these plots indicates k, the number of visits to the corresponding state. As can
be seen, the difference curves all tend to zero as k increases, which is consistent with the
predicted convergence of the empirical conditional distributions on the trace space.
So far we compared the empirical distributions along the same trajectory. Next we
compare them against the one obtained at the end of another trajectory that starts from a
different initial condition. The difference curve for one state (the first state in the southeast
group) is plotted in the last graph in the bottom row of Figure 3, and it is the lower curve in
that graph. As can be seen, the curve tends to zero, suggesting that the limiting distribution
of these empirical distributions does not change if we vary the initial condition, which is
consistent with Theorem 2.1.
For comparison, we also plotted the difference curve when these same empirical conditional distributions are compared against the empirical conditional distribution obtained
from the same trajectory but for a different state (specifically, the middle state of the northeast group). This is the upper curve in the last graph in the bottom row of Figure 3. It
clearly indicates that for the two states, the associated limiting conditional distributions
on the trace space are different. It also shows that the characteristic function approach we
adopted in this experiment can effectively distinguish between two different distributions
(cf. Footnote 11).
4.2 LSTD with Evolving λ
We now present experiments on the LSTD algorithm.
4.2.1 A Toy Problem
Let us first continue with the toy problem of the previous subsection and show how the
LSTD algorithm performs in this problem as we vary the parameter C in the λ function for
bounding the traces. We ran LSTD for C = 10, 20, . . . , 100, using the same trajectory, for
3 × 105
iterations, and we computed the (Euclidean) distance of these LSTD solutions to
the asymptotic TD(1) solution (in the space of the θ-parameters), normalized by the norm
of the latter. We then repeat this calculation 10 times, each time with an independently
generated trajectory. Plotted in Figure 4 (left) against the values of C are the means and
standard deviations of the normalized distances of LSTD solutions thus obtained.
11. Recall that a tight sequence {pk} of probability distributions on <
m converges to a probability distribution p if and only if the characteristic functions of pk converge pointwise to the characteristic function of
p (Dudley, 2002, Lemma 9.5.5). Recall also that we are dealing with convergence in distribution here,
which is much weaker than convergence in total variation, so we cannot use total variation as a metric
on the distribution space in this case.
29
Yu, Mahmood, and Sutton
0 10 20 30 40 50 60 70 80 90 100 110
0
0.02
0.04
0.06
0.08
0.1
0.12
C
normalized distance to TD(1) solution
0.88 0.9 0.92 0.94 0.96 0.98 1
0
0.02
0.04
0.06
0.08
0.1
0.12
λ
normalized distance to TD(1) solution
Figure 4: Compare LSTD solutions with evolving λ (left) and with constant λ (right).
For constant λ, the red dot-dash curve in the right plot shows the quality of
the asymptotic TD(λ) solutions, and LSTD(λ) would approach this curve in the
limit, but due to variance issues, it can require an impractically large number of
iterations to exhibit this convergent behavior. LSTD with evolving λ outperforms
LSTD with constant λ in this case and effectively archives the quality of TD(λ)
solutions for large constant λ.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
λ
normalized distance to TD(1) solution
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.4
0.45
0.5
0.55
0.6
0.65
λ
approximation error (normalized)
Figure 5: Approximation quality of asymptotic TD(λ) solutions with constant λ for the toy
problem. Larger λ yields better approximations.
For comparison, we did the same for LSTD with a large constant λ: λ = 0.9, 0.92, . . . , 1.
Figure 4 (right) shows the result, where the dash-dot curve indicates the normalized distance
of the asymptotic TD(λ) solution—the solution that LSTD(λ) would obtain in the limit.
It can be seen that the performance of LSTD deteriorates as λ gets close to 1. We think
that this is because due to the high variance issue, the convergence of LSTD with a large
constant λ is too slow and requires far more iterations than the 3×105
iterations performed.
In comparison, LSTD with evolving λ behaves better: it works effectively for C ≥ 20, and
30
Generalized Bellman Equations and TD Learning
the approximation quality it achieved with such C is comparable to that of asymptotic
TD(λ) solutions for a large constant λ around 0.96.
Figure 5 shows the quality of the asymptotic solutions of TD(λ) with constant λ, for
the full range of λ values. Plotted in the left graph is the normalized distance of the TD(λ)
solution to the TD(1) solution. Plotted in the right graph is the normalized approximation
error of the corresponding approximate value function, where the error is measured by
the weighted Euclidean norm with weights specified by ζS (the invariant distribution on
S under the behavior policy), and the normalization is over the weighted norm kvπkζS
of
the true value function vπ of the target policy. It can be seen that using λ > 0.9 provides
considerably better approximations for this problem than using small λ.
We also found that for this problem, if we set λ according to the Retrace algorithm with
β = 1 (cf. (2.8) in Example 2.2), then the performance of LSTD is comparable to TD(λ)
with a small λ around 0.5. (Specifically, for Retrace, the normalized distance to the TD(1)
solution is 0.37, and the normalized approximation error is 0.54, which are comparable
to the numbers for TD(0.5), as Figure 5 shows.) This is not surprising, because, as we
discussed earlier in Section 2.2.2, in keeping λtρt−1 ≤ 1 always, Retrace and ABQ can be
too “conservative,” resulting in an overall effect that is like using a small λ, even though λt
may appear to be large at times. Recall also that this can happen to our proposed scheme
too. In the present experiment, for instance, this can happen when C is small; in particular,
the case C = 0 reduces to LSTD(0).
4.2.2 Mountain Car Problem
In this subsection we demonstrate LSTD with evolving λ on a problem adapted from the
well-known Mountain Car problem (Sutton and Barto, 1998). The details of this adaptation,
including the target and behavior policies involved, can be found in the report (Yu, 2016a,
Section 5.1, p. 23-26); most of these details are not crucial for our experiments, so to avoid
distraction, we only describe briefly the experimental setup here.
In Mountain Car, the goal is to drive an underpowered car to reach the top of a steep
hill, from the bottom of a valley. A state consists of the position and velocity of the car,
whose values lie in the intervals [−1.2, 0.5], [−0.07, 0.07], respectively. The position 0.5
corresponds to the desired hill top destination, while the position −π/6 (≈ −0.52) lies at
the bottom of a valley that is between the destination and a second hill peaked at −1.2 in
the opposite direction (see the illustration in Figure 6). Except for the destination state,
each state has three available actions: {back, coast, forward}, and the rewards depend
only on the action taken and are −1.5, 0 and −1 for the three actions, respectively. The
dynamics is as given in (Sutton and Barto, 1998). We consider undiscounted expected total
rewards, so the discount factor is 1 except at the destination state, where the discount factor
is 0 and from where the car enters a rewardless termination state permanently.
The target policy π is a simple but reasonably well-behaved policy. On either slopes
between the two hills, it tries to increase its energy (kinetic plus gravitational potential
energy) by accelerating in the direction of its current motion. If this brings it up to the
opposite hill (position < −1), it coasts; otherwise, if its velocity drops to near zero, it goes
forward or backward with equal probability. Figure 6 visualizes the total costs −vπ of the
31
Yu, Mahmood, and Sutton
Figure 6: Left: Illustration of the Mountain Car problem. Right: Costs −vπ for this problem are estimated and visualized as a color image, with the coloring scheme
indicated by the colorbar. (The horizontal and vertical axes of the image correspond to position and velocity, respectively, for the 2-dimensional state space of
this problem.)
Figure 7: Left: Visualization of weights on states induced by the behavior policy. Right:
The color image visualizes the approximation of −vπ obtained from a discretized
model for the Mountain Car problem, where the coloring scheme is the same as
that shown in Figure 6 (right). The quality of this approximation is close to that
of TD(0).
target policy,12 where the horizontal (vertical) axis indicates position (velocity) and the
12. The values of vπ shown in Figure 6 are estimated by simulating the target policy for each starting state
in a set of 171 × 141 points evenly spaced in the position-velocity space. In particular, the position
(velocity) interval is evenly divided into subintervals of length 0.01 (0.001), and for each stating state,
the target policy is simulated 600 times.
32
Generalized Bellman Equations and TD Learning
colorbar on the right shows the value corresponding to each color. The discontinuity of the
function in certain regions can be seen in this figure.
The behavior policy π
o
is an artificial policy that takes a random action (chosen with
equal probability from the three actions) 90% of the time, and explores the state space by
jumping to some random state 10% of the time. It also restarts when it is at the destination:
with equal probability, it either restarts near the bottom of the valley or restarts from a
random point sampled uniformly from the state space.13
We now explain how we will measure approximation qualities. Since the Mountain Car
problem has a continuous state space, it is actually not covered by our analysis, which is for
finite-state problems. Although we can treat it as essentially a finite-state problem (since
the simulation is done with finite precision in computers), the number of states would still
be too large to calculate the weights ζS. So, to measure weighted approximation errors
kv − vπk for approximate value functions v produced by various LSTD algorithms in the
subsequent experiments, we will compare v and vπ at a grid of points in the state space and
calculate a weighted Euclidean distance between the function values at these grid points,
using a set of weights precalculated by simulating the behavior policy.14 The image in
Figure 7 (left) visualizes these weights.
We use tile-coding (Sutton and Barto, 1998) to generate 145 binary features for our
experiments.15 The approximate value functions obtained with LSTD algorithms are thus
piecewise constant. For comparison, we also build a discrete approximate model by state
aggregation. The discretization is done at a resolution comparable to our tile-coding scheme,
and the dynamics and rewards of this model are calculated based on data collected under
the behavior policy. The solution of the discrete approximate model is shown in Figure 7
(right) (the coloring scheme for this and the subsequent images are the same as shown in
Figure 6). It is similar to the approximate value function calculated by LSTD(0), which
is shown in Figure 8 (first image, top row). As will be seen shortly, with positive λ, the
approximation quality of LSTD improves. Thus the discrete model approximation approach
is not as effective as the TD method in this case.
We now report the results of our experiments on the Mountain Car problem.
First Experiment: In this experiment, we compare three ways of setting λ: (i) Retrace
with β = 1 (cf. Example 2.2); (ii) our simple scaling scheme with parameter C used in
the previous experiments (cf. (4.1) and Example 2.1); and (iii) a composite scheme of the
type discussed at the end of Section 3.2, which partitions the state space into two sets16
13. The behavior policy is exactly the same as described in (Yu, 2016a, p. 24-25) except for the possibility
of restarting near the bottom of the valley whenever the destination is reached.
14. Specifically, we chose a grid of 171 × 141 points evenly spaced in the position-velocity space. We ran the
behavior policy for 8 × 105
effective iterations, where an iteration is considered to be ineffective if the
behavior policy takes an action, e.g., the restart action, that is impossible for the target policy. A visit
to a state at an effective iteration was counted as a visit to the nearest grid point. At the end of the
run, visits to a boundary point (−1.2, 0) were disregarded as they were due to boundary effects in the
dynamics of this problem, and the final counts were normalized to produce a set of weights on the grid
points that sum to 1.
15. Two tilings are used: the first (second) comprises of 64 (81) uneven-sized rectangles that cover the state
space. Together they produce a total of 145 binary features. The details of the coding scheme are as
described in (Yu, 2016a, p. 28).
16. The first set consists of those states (position, velocity) with either position ≤ −0.9 or velocity ≥ 0.04.
The rest of the states belong to the second set.
33
Yu, Mahmood, and Sutton
Figure 8: Visualized in the color images are approximations of −vπ obtained by LSTD
with different schemes of setting λ, where the coloring scheme is as that shown
in Figure 6 (right). The choices of λ for each image, from left to right and top
to bottom, are as follows. Top row: C = 0 (equivalent to LSTD(0)), C = 5,
Retrace. Middle row: C = 25, C = 125, C : (125, 25). Bottom row: C = 200,
C = 300, C : (300, 50).
and applies the simple scaling scheme with parameters C1, C2 for the first and second
set, respectively. When referring to this composite scheme in the figures, we will use the
designation C : (C1, C2).
We ran LSTD with different ways of setting λ just mentioned, on the same state trajectory generated by the behavior policy, for 6 × 105
effective iterations (cf. Footnote 14).
Some of the approximate value functions obtained at the end of the run are visualized as
images in Figure 8. It can be seen that the result of Retrace is similar to that of the simple
scaling scheme with a small C, and as we increase C, the approximation from the scaling
scheme improves.
To compare more precisely the approximation errors and see how they change over time
for each algorithm, we did 10 independent runs, each of which consists of 6 × 105
effective
34
Generalized Bellman Equations and TD Learning
C=0 5 10 15 20 25 50 75 100 125 150 175 200 225 250 275 C=300 Retrace C: (125,0) (125,25) (200,0) (200,25) (300,0) C: (300,50)
12
13
14
15
16
17
18
19
20
21
approximation error (weighted)
Figure 9: Compare the approximation error of LSTD for different schemes of setting λ.
iterations. The results are shown in Figures 9-11. Plotted in Figure 9 for each algorithm
are the mean and standard deviation of the approximation errors for the 10 approximate
value functions obtained by that algorithm at the end of the 10 runs. We can see from this
figure the improvement in approximation quality as C increases. We can also see that the
result of Retrace is in between those of C = 0 and C = 5, which is consistent with what
the images in the top row of Figure 8 tell us.
0 50 100 150 200 250 300
10
15
20
25
30
35
40
45
x2000 iterations
approximation error (weighted)
Retrace
C=5
C=25
0 50 100 150 200 250 300
0
20
40
60
80
100
120
x2000 iterations
approximation error (weighted)
Retrace
C=125
C=200
C=300
Figure 10: Compare the temporal behavior of LSTD for different schemes of setting λ.
0 10 20 30 40 50 60 70 80 90 100
10
15
20
25
30
35
40
45
50
x2000 iterations
approximation error (weighted)
Retrace
C=125
C: (125,0)
C: (125,25)
0 10 20 30 40 50 60 70 80 90 100
10
15
20
25
30
35
40
45
50
55
60
x2000 iterations
approximation error (weighted)
Retrace
C: (125,0)
C: (200,0)
C: (300,0)
0 10 20 30 40 50 60 70 80 90 100
10
15
20
25
30
35
40
45
50
55
60
x2000 iterations
approximation error (weighted)
Retrace
C: (125,25)
C: (200,25)
C: (300,50)
Figure 11: Compare the temporal behavior of LSTD for different schemes of setting λ.
35
Yu, Mahmood, and Sutton
Plotted in Figures 10-11 are the approximation errors calculated per 2000 effective iterations for each algorithm, during one of the 10 experimental runs. Figure 10 (left) shows
how Retrace compares with the simple scaling with C = 5 and C = 25. It can be seen
that the latter two achieved better approximation quality than Retrace without increases
in variance. Figure 10 (right) shows that for larger values of C, variances also became larger
initially; however, after about 5 × 104
iterations, these schemes overtook Retrace, yielding
better approximations.
Figure 11 shows how the composite scheme of setting λ performs. Comparing the plots
in this figure with the right plot in Figure 10, it can be seen that the composite schemes
helped in reducing variances, and in about 2 × 104
iterations the schemes C : (125, 0) and
C : (125, 25) overtook Retrace and yielded better approximations. Together with Figure 9,
Figure 11 shows clearly the bias-variance trade-off of using composite schemes in this problem.
Second Experiment: Similarly to the previous experiment, we now compare our proposed method with several other ways of setting λ for the LSTD algorithm as well as
with a constrained variant of LSTD: (i) Retrace with β = 1 as before; (ii) constant λ;
(iii) constrained LSTD with constant λ; and (iv) the simple scaling scheme with parameter C. For constant λ ∈ [0, 1], the constrained LSTD(λ) used in this experiment evolves
the trace vectors as off-policy LSTD(λ) does, but it forms and solves the linear equation
1
t
Pt−1
k=0[ek]50 · δk(v) = 0, v = Φθ instead, where the function [·]50 truncates each component
of the trace vector to be within the interval [−50, 50]. (Such an algorithm follows naturally
from the ergodicity of the state-trace process and the approximation of an unbounded integrable function by a bounded one. For a detailed discussion, see (Yu, 2016b, Section 3.2)
or (Yu, 2017, Section 3.3).)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Retrace C=5 25 125 200 C=300
10
12
14
16
18
20
22
24
approximation error (weighted)
LSTD(λ) constrained LSTD(λ)
Figure 12: Compare the approximation errors of several LSTD algorithms.
Figure 12 is similar to Figure 9 and shows, for each algorithm, the mean and standard
deviation of the approximation errors of 10 approximate value functions obtained at the end
of 10 independent experimental runs (each of which consists of 6 × 105
effective iterations).
The horizontal axis indicates the algorithms and their parameters. As can be seen from this
figure, for constant small λ, LSTD(λ) performed well in this problem, and LSTD(0.3) and
Retrace are comparable. For constant λ > 0.7, LSTD(λ) failed to give sensible results, and
LSTD(0.7) started to show this unreliable behavior. This behavior of LSTD(λ) is related
to what we observed in Figure 4(right) in the small toy problem, and it is, we think, due to
36
Generalized Bellman Equations and TD Learning
0 50 100 150 200 250 300
10
15
20
25
30
35
40
45
x2000 iterations
approximation error (weighted)
C=25
Retrace
LSTD(0.6)
constrained LSTD(0.8)
constrained LSTD(0.9)
constrained LSTD(1)
0 50 100 150 200 250 300
10
15
20
25
30
35
40
45
x2000 iterations
approximation error (weighted)
C=25
C=125
C=200
C=300
constrained LSTD(1)
Figure 13: Compare the temporal behavior of several LSTD algorithms.
the high variance issue, which becomes more severe as λ gets larger. Constrained LSTD(λ)
is much more reliable, and it did consistently well for all values of λ tested. LSTD with
evolving λ also did well, and with C > 125, it achieved a slightly better approximation
quality than constrained LSTD(1).
Figure 13 compares the temporal behavior of several algorithms during one experimental
run. Plotted are the approximation errors calculated per 2000 iterations for each algorithm
in the comparison. It can be seen that in this problem constrained LSTD(λ) did not suffer
from large variances even with large λ values, and compared with constrained LSTD(1),
the behavior of LSTD with evolving λ was also reasonable for large C.
Third Experiment: In this experiment we first compare the simple scaling scheme and
Retrace, where both schemes now use an additional parameter β ∈ [0, 1], as discussed in
Examples 2.1-2.2. Recall that when β = 1, they reduce to the schemes that we already
compared in the previous experiments. For both schemes, as β becomes smaller, we expect
the approximation quality to drop but the variance to get smaller.
Plotted in Figure 14 for C = 125, C = 200 and Retrace are the results obtained from a
single experimental run consisting of 3×105
effective iterations. As before the approximation
errors were calculated per 2000 iterations. The results do show the expected bias-variance
trade-off effects of the parameter β, during the initial period of the experimental run,
although the effects on Retrace turned out to be smaller and hard to discern at the scale
of the plot.
Next we test some of the variations on Retrace discussed in Example 2.2. Specifically, we
consider (2.10)-(2.11) with parameters β = 0.9, K ∈ {1.5, 2.0, 2.5, 3.0} and C ∈ {50, 125}.
Plotted in Figure 15 are the results from one experimental run of 3×105
effective iterations.
For comparison, the plots also show the behavior of Retrace and the simple scaling scheme
with the same values of C during that run (these algorithms used the same β = 0.9). As
expected and can be seen from the figure, the approximation quality improves with K and
C. While the variances also tend to increase during the initial part of the run, the variants
that truncate the importance sampling ratios by K = 1.5 performed comparably to Retrace
initially, soon overtook Retrace and achieved better approximation quality.
37
Yu, Mahmood, and Sutton
0 50 100 150
10
11
12
13
14
15
16
17
18
19
20
21
x2000 iterations
approximation error (weighted)
C = 125
β = 0.6
β = 0.7
β = 0.8
β = 0.9
β = 1
0 50 100 150
10
11
12
13
14
15
16
17
18
19
20
21
x2000 iterations
approximation error (weighted)
C = 200
β = 0.6
β = 0.7
β = 0.8
β = 0.9
β = 1
0 50 100 150
10
11
12
13
14
15
16
17
18
19
20
21
x2000 iterations
approximation error (weighted)
Retrace
β = 0.6
β = 0.7
β = 0.8
β = 0.9
β = 1
Figure 14: Compare the approximation error and temporal behavior of LSTD for different
schemes of setting λ. From left to right: C = 125, C = 200, Retrace.
0 50 100 150
10
12
14
16
18
20
22
24
26
28
30
x2000 iterations
approximation error (weighted)
Retrace
K=1.5, C=50
K=2.0, C=50
K=2.5, C=50
K=3.0, C=50
C=50
0 50 100 150
10
12
14
16
18
20
22
24
26
28
30
x2000 iterations
approximation error (weighted)
Retrace
K=1.5, C=125
K=2.0, C=125
K=2.5, C=125
K=3.0, C=125
C=125
Figure 15: Compare the approximation error and temporal behavior for some variations on
Retrace.
5. Conclusion
We developed in this paper a new scheme of setting the λ-parameters for off-policy TD
learning, using the ideas of randomized stopping times and generalized Bellman equations
for MDPs. Like the two recently proposed algorithms Retrace (Munos et al., 2016) and
ABQ (Mahmood et al., 2017), our scheme keeps the traces bounded to reduce variances,
but it is much more general and flexible. To study its theoretical properties, we analyzed
the resulting state-trace process and established convergence and solution properties for
the associated LSTD algorithm, and these results have prepared the ground for convergence analysis of the gradient-based implementation of our proposed scheme (Yu, 2017). In
addition we did a preliminary numerical study. It showed that with the proposed scheme
LSTD can outperform several existing off-policy LSTD algorithms. It also demonstrated
that in order to achieve better bias-variance trade-offs in off-policy learning, it is helpful
to have more flexibility in choosing the λ-parameters and to allow for large λ values. Future research is to conduct a more extensive numerical study of both least-squares based
and gradient-based algorithms, with more versatile ways of using the memory states and
λ-parameters, in off-policy learning applications.
