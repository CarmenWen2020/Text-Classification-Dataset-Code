We develop an approximate formula for evaluating a cross-validation estimator of predictive
likelihood for multinomial logistic regression regularized by an `1-norm. This allows us to
avoid repeated optimizations required for literally conducting cross-validation; hence, the
computational time can be significantly reduced. The formula is derived through a perturbative approach employing the largeness of the data size and the model dimensionality. An
extension to the elastic net regularization is also addressed. The usefulness of the approximate formula is demonstrated on simulated data and the ISOLET dataset from the UCI
machine learning repository. MATLAB and python codes implementing the approximate
formula are distributed in (Obuchi, 2017; Takahashi and Obuchi, 2017).
Keywords: classification, multinomial logistic regression, cross-validation, linear perturbation, self-averaging approximation
1. Introduction
Multinomial classification is a ubiquitous task. There are several ways to treat this task,
such as the naive Bayesian methods, neural networks, decision trees, and hierarchical classification schemes (Trevor et al., 2009). Among them, in this paper, we focus on multinomial
logistic regression (MLR), which is simple but powerful enough to be used in many present
day applications.
Let us denote each feature vector by xµ ∈ R
N and its class by yµ ∈ {1, · · · , L}, where
µ = 1 · · · , M denotes the index of given data. The MLR uses a linear structural model
with parameters {wa ∈ R
N }
L
a=1 and computes a class-a bias as an overlap:
uµa = x
>
µ wa. (1)
A probability such that the feature vector xµ belongs to the class a is computed through a
softmax function φ as:
φ

a


 {uµb}
L
b=1
=
e
uµa
PL
b=1 e
uµb
. (2)
These define the MLR.
The maximum likelihood estimation is usually employed to train the MLR, though the
learning result tends to be inefficient when the data size is not sufficiently larger than
the model dimensionality or noises in relevant levels are present. A common technique to
overcome this difficulty is to introduce a penalty or regularization. In this paper, we use
an `1-regularization, which induces a sparse classifier as a learning result and is accepted
to be effective. Given M data points DM ≡ {(xµ, yµ)}M
µ=1 , the `1-regularized estimator is
defined by the following optimization problem:
{wˆa(λ)}a = arg min
{wa}a
n
H

{wa}
L
a=1



DM, λo , (3)
H

{wa}
L
a=1



DM, λ
≡
X
M
µ=1
qµ

{wa}
L
a=1
+ λ
X
L
a=1
||wa||1, (4)
qµ

{wa}
L
a=1
= − ln φ

yµ



n
uµa = x
>
µ wa
oL
a=1
, (5)
where we denote the negative log-likelihood as qµ and define a regularized cost function or
Hamiltonian H.
The introduction of regularization causes another problem of model selection or hyperparameter estimation with respect to λ. A versatile framework providing a reasonable
estimate is cross-validation (CV), but it has a disadvantage in terms of the computational
cost. The literal CV requires repeated optimizations which can be a serious computational
burden when the data size and the model dimensionality are large. The purpose of this
paper is to resolve this problem by inventing an efficient approximation of CV.
Our technique is based on a perturbative expansion employing the largeness of the data
size and the model dimensionality. Similar techniques were also developed for the Bayesian
learning of simple perceptron and committee machine (Opper and Winther, 1996, 1997), for
Gaussian process and support vector machine (Opper and Winther, 2000a,b; Vapnik and
Chapelle, 2000), for linear regression with the `1-regularization (Obuchi and Kabashima,
2016; Rad and Maleki, 2018; Wang et al., 2018) and with the two-dimensional total variation (Obuchi et al., 2017). Actually, this perturbative approach is fairly general and can be
applied to a wide class of generalized linear models with simple convex regularizations. For
example in the present MLR case, it is easy to extend our result to the case where both the
`1- and `2-regularizations exist (elastic net, Zou and Hastie, 2005), which is used in a common implementation (Friedman et al., 2010). The derivation of our approximate formula
below is, however, conducted on the case of the `1-regularization only, for simplicity. The
extension to the elastic net case is stated after the derivation.
The rest of the paper is organized as follows. In sec. 2, we state our formulation and how
to derive the approximate formula. In sec. 3, we compare our approximation result with
that of the literally conducted CV on simulated data and on the ISOLET dataset from UCI
machine learning repository (Lichman, 2013). The accuracy and the computational time of
our approximate formula are reported in comparison with the literal CV. The limitation of
a simplified version of the approximation is also examined. The last section is devoted to
the conclusion.
2
Accelerating CV in Multinomial Logistic Regression
2. Formulation
In the maximum likelihood estimation framework, it is natural to employ a predictive likelihood as a criterion for model selection (Bjornstad, 1990; Ando and Tsay, 2010). We require
a good estimator of the predictive likelihood, and the CV provides a simple realization of
it. Particularly in this paper, we consider an estimator based on the leave-one-out (LOO)
CV. The LOO solution is described by
{wˆ
\µ
a
(λ)}a = arg min
{wa}a
n
H\µ

{wa}
L
a=1



DM, λo , (6)
H\µ

{wa}
L
a=1



DM, λ
≡ H 
{wa}
L
a=1



DM, λ
− qµ

{wa}
L
a=1
. (7)
Denoting the overlap of xµ with the LOO solution as ˆu
\µ
µa = x
>
µ wˆ
\µ
a , as well as that with
the full solution ˆuµa = x
>
µ wˆa, we can define the LOO estimator (LOOE) of the predictive
negative log-likelihood as:
LOO(λ) = 1
M
X
M
µ=1
qµ
n
wˆ
\µ
a
oL
a=1
= −
1
M
X
M
µ=1
ln φ

yµ


{uˆ
\µ
µa}
L
a=1
. (8)
In the following, the predictive negative log-likelihood is simply called prediction error. The
minimum of the LOOE determines the optimal value of λ though its evaluation requires us
to solve eq. (6) M times, which is computationally demanding.
2.0.1. Notations
Here, we fix the notations for a better flow of the derivation shown below. By summarizing
the class index, we introduce a vector notation of the overlap as uµ = (uµa)a ∈ R
L and an
extended vector representation of the weight vectors {wa}a as W = (wa)a ∈ R
LN . The
mth component of W can thus be decomposed into two parts as m = (mc, mf ) where
mc ∈ {1, · · · , L} denotes the class index and mf ∈ {1, · · · , N} represents the component
index of the feature vector. Namely we write Wm = wmcmf
. Correspondingly, we leverage
a matrix Xµ ∈ R
L×LN to define a repetition representation of the feature vector xµ: Each
component is defined as:
Xµ
am ≡ δamcxµmf
. (9)
This yields simple and convenient relations:
uµ = XµW, Xµ =

∂uµ
∂W
>
. (10)
Further, the class-a probability of µth data at the full solution Wˆ = (wˆa)a is denoted by:
pa|µ = φ(a|{uˆµb}b) = e
uˆµa
PL
b=1 e
uˆµb
(11)
3
Obuchi and Kabashima
These notations express the gradient and the Hessian of qµ at the full solution as:
∇qµ(Wˆ ) ≡
∂qµ
∂W




W=Wˆ
=
∂uµ
∂W
∂
∂uµ
qµ




uµ=uˆµ
= (Xµ
)
>
b
µ
, (12)
∂
2
qµ(Wˆ ) ≡
∂
2
qµ
∂W∂W0




W=W0=Wˆ
=
∂uµ
∂W

∂
2
qµ
∂uµ∂u0
µ




uµ=u0
µ=uˆµ
! 
∂u
0
µ
∂W0
>
= (Xµ
)
> F
µXµ
, (13)
where
b
µ ≡ (p1|µ − δ1yµ
, p2|µ − δ2yµ
, · · · , pL|µ − δLyµ
)
>, (14)
F
µ
ab ≡ δabpa|µ − pa|µpb|µ. (15)
In addition, we denote the cost function Hessians at the respective solutions as:
G ≡ ∂
2H(Wˆ ) = X
µ

∂
2
qµ(Wˆ )

, (16)
G
\µ ≡ ∂
2H\µ
(Wˆ \µ
) = X
ν(6=µ)

∂
2
qν(Wˆ \µ
)

. (17)
Finally, we introduce the symbol A(W) ≡ {m|Wm 6= 0} representing the index set of the
active components of W and Aˆ ≡ A(Wˆ ). Given Wˆ , we denote the active components of a
vector Y ∈ R
LN by the subscript as YAˆ. A similar notation is used for any matrix and the
symbol ∗ is assumed to represent all of the components in the corresponding dimension.
2.1. Approximate formula
For a simple derivation, it is important to consider that the w-dependence of φ appears
only in the overlap u = x
>w. Hence, it is sufficient to provide the relation between ˆuµa
and ˆu
\µ
µa in order to derive the approximate formula.
A crucial assumption to derive the formula is that the active set is “common” between
the full and LOO solutions, Wˆ = (wˆa)a and Wˆ \µ = (wˆ
\µ
a )a; namely Aˆ = Aˆ\µ ≡ A(Wˆ \µ
).
Although this assumption is literally not true, we numerically confirmed that this approximately holds. In other words, the change of the active set is small enough compared to the
size of the active set itself when considering the LOO operation when N and M are large.
Moreover, in a related problem of an `1-regularized linear regression, the so-called LASSO, it
has been shown that the contribution of the active set change vanishes in a limit N, M → ∞
keeping α = M/N = O(1) (Obuchi and Kabashima, 2016). It is expected that the same
holds in the present problem. Hence, we adopt this assumption in the following definition.
Note that this idea of the active set constantness can be found in preceding analyses of
support vector machine (Opper and Winther, 2000b; Vapnik and Chapelle, 2000).
Once the active set Aˆ is assumed to be known and unchanged by the LOO operation,
it is easy to determine the active components of the full and LOO solutions Wˆ and Wˆ \µ
.
4
Accelerating CV in Multinomial Logistic Regression
The vanishing condition of the gradient of the cost function is the determining equation:
(∇H)Aˆ = 0 ⇒ Wˆ
Aˆ, (18)

∇H\µ

Aˆ
= (∇H)Aˆ − (∇qµ)Aˆ = 0 ⇒ Wˆ
\µ
Aˆ
. (19)
The difference between the gradients is only ∇qµ, and hence the difference between Wˆ and
Wˆ \µ
is expected to be small. Denoting the difference as d
µ = Wˆ − Wˆ \µ and expanding
eq. (19) with respect to d
µ up to the first order, we obtain an equation determining d
µ
:
d
µ
Aˆ = −

G
\µ
AˆAˆ
−1 
∇qµ(Wˆ )

Aˆ
. (20)
Inserting this and eq. (12) into the definition d
µ = Wˆ − Wˆ \µ and multiplying Xµ
from
left, we obtain:
uˆ
\µ
µ ≈ uˆµ + C
\µ
µ b
µ
, (21)
C
\µ
µ ≡ X
µ
∗Aˆ

G
\µ
AˆAˆ
−1 
X
µ
∗Aˆ
>
. (22)
This equation implies that the matrix inversion operation is necessary for each µ, which
still requires a significant computational cost. To avoid this, we employ an approximation
and the Woodbury matrix inversion formula in conjunction with eqs. (13,16,17). The result
is:

G
\µ
−1
≡

∂
2H\µ
(Wˆ \µ
)
−1
≈

∂
2H\µ
(Wˆ )
−1
=

G − (Xµ
)
> F
µXµ
−1
= G
−1 − G
−1
(Xµ
)
>

−F
µ + XµG
−1
(Xµ
)
>
−1
XµG
−1
. (23)
Inserting this into eq. (21) and simplifying several factors, we obtain:
uˆ
\µ
µ ≈ uˆµ + Cµ (IL − F
µCµ)
−1
b
µ
, (24)
where
Cµ = X
µ
∗Aˆ

GAˆAˆ
−1

X
µ
∗Aˆ
>
. (25)
Now, all of the variables on the righthand side of eq. (24) can be computed from the
full solution Wˆ only, which enables us to estimate the LOOE by leveraging a one-time
optimization using all of the data DM, while avoiding repeated optimizations.
We should mention the computational cost of this approximation: it is mainly scaled
as O(ML2
|Aˆ| + ML|Aˆ|
2 + |Aˆ|
3
). The first two terms come from the construction of GAˆAˆ
and Cµ, and the last one is derived from the inverse of G. If |Aˆ| is proportional to the
feature dimensionality N, this computational cost is of the third order with respect to the
system dimensionality N and M. This is admittedly not cheap and the computational
cost for the k-fold literal CV with a moderate value of k becomes smaller than that for
our approximation in a large dimensionality limit. We, however, stress that there actually
exists a wide range of N and M values in which our approximation outperforms the literal
 
Obuchi and Kabashima
CV in terms of the computational time, as later demonstrated in sec. 3. Moreover, for
treating much larger systems, we invent a further simplified approximation based on the
above approximate formula. The computational cost of this simplified version is scaled only
linearly with respect to the system parameters N and M. Its derivation is in sec. 2.2 and
the precision comparison to the original approximation is in sec. 3.
Another sensitive issue is present in computing
GAˆAˆ
−1
. Occasionally the cost function
Hessian G has zero eigenvalues and is not invertible. We handle this problem in the next
subsection.
2.1.1. Handling zero modes
In the MLR, there is an intrinsic symmetry such that the model is invariant under the
addition of any constant vector to the weight vectors of all classes:
wa → wa + v (∀a). (26)
In this sense, the weight vectors defining the same model are “degenerated” and our MLR
is singular. For finite λ, this is not harmful because the regularization term resolves this
singularity and selects an optimal one {wˆa}a with the smallest value of ||wa||1 among the
degenerated vectors. However, this does not mean that the associated Hessian is nonsingular. The regularization term does not provide any direct contribution to the Hessian
and as a result, the Hessian tends to have some zero modes. This prevents taking the inverse
Hessian G−1
in eq. (25). How can we overcome this?
One possibility is to fix the weights of one certain class at constant values when solving
the optimization problem (4). This is termed “gauge fixing” in physics, and one convenient
gauge in the present problem will be the zero gauge in which the weights in a chosen class
are fixed at zeros. This is actually found in some earlier implementations (Krishnapuram
et al., 2005; Schmidt, 2010) and is preferable for our approximate formula because it removes the harmful zero modes of the Hessian from the beginning. However, some other
implementations which are currently well accepted do not employ such gauge fixing (Friedman et al., 2010), and moreover even with gauge fixing very small eigenvalues sometimes
accidentally emerge in the Hessian. Hence, for user convenience, we require another way of
avoiding this problem.
Another possibility is to remove the zero modes by hand. By construction, the zero
modes are associated to the model invariance. This implies that those zero modes are irrelevant and may be removed. In fact, we are only interested in the perturbations which truly
change the model, and the modes which maintain the model unchanged are unnecessary.
According to this consideration, we replace G−1
in eq. (25) with the zero-mode-removed inverse Hessian G
−1
. The computation of G
−1
is straightforward: we perform the eigenvalue
decomposition of GAˆAˆ and obtain the eigenvalues {di}
|Aˆ|
i=1 and eigenvectors {vi}
|Aˆ|
i=1, which
allows us to represent
GAˆAˆ =
X
i
diviv
>
i =
X
i∈S+
diviv
>
i
, (27)
 
Accelerating CV in Multinomial Logistic Regression
where S
+ denotes the index set of the modes with finite eigenvalues. Then, G
−1
is defined
as:
G
−1
AˆAˆ ≡
X
i∈S+
d
−1
i
viv
>
i
. (28)
Finally, we replace G−1 by G
−1
in eq. (25), and obtain:
Cµ = X
µ
∗AˆG
−1
AˆAˆ

X
µ
∗Aˆ
>
. (29)
By using this instead of eq. (25), the problem caused by the zero modes can be avoided.
2.1.2. Extension to the mixed regularization case
Let us briefly state how we can generalize the present result to the case of the mixed
regularizations of the `1- and `2-terms (elastic net, Zou and Hastie, 2005). The problem to
be solved can be defined as follows:
{wˆa(λ1, λ2)}a = arg min
{wa}a



X
M
µ=1
qµ

{wa}
L
a=1
+ λ1
X
L
a=1
||wa||1 +
λ2
2
X
L
a=1
||wa||2
2



.(30)
where || · ||2 denotes the `2 norm. Following the derivation in sec. 2.1, we realize that
the derivation is essentially the same, and the difference only appears in the cost function
Hessian:
Gmxd =
X
µ

∂
2
qµ(Wˆ )

+ λ2INL, (31)
where IK is the identity matrix of size K. As a result, we can compute the LOO solution
by leveraging the same equation as eq. (24) by replacing the definition of Cµ, eq. (25), with:
Cµ = X
µ
∗Aˆ

(Gmxd)AˆAˆ
−1

X
µ
∗Aˆ
>
. (32)
Thanks to the `2 term, the zero mode removal is not needed since the eigenvalues are lifted
up by λ2.
2.1.3. Binomial case
The binomial case L = 2 is particularly interesting in several applications and thus we write
down the specific formula for this case.
In the binomial case, it is fairly common to express the class y as a binary y = 0, 1 and
to use the following logit function:
φlogit(yµ|uµ) = δyµ1 + δyµ0e
−uµ
1 + e
−uµ
, (33)
where
uµ = x
>
µ w. (34)
 
Obuchi and Kabashima
If we identify y = 0 in this case as y = 1 in the two-class MLR case, this is nothing but
the two-class MLR with a zero gauge w1 = 0. Hence, there is no harmful zero mode in the
Hessian and we can straightforwardly apply our approximate formula. The explicit form in
this case is:
uˆ
\µ
µ ≈ uˆµ +
c
µ
1 −
∂
2qµ
∂u2
µ
c
µ
∂qµ
∂uµ
, (35)
where qµ = − ln φlogit(yµ|uµ) and
∂qµ
∂uµ
= δyµ0 −
e
−uµ
1 + e
−uµ
, (36)
∂
2
qµ
∂u2
µ
=
e
−uµ
(1 + e
−uµ )
2
, (37)
GAˆAˆ =
X
M
µ=1
∂
2
qµ
∂u2
µ

xµx
>
µ

AˆAˆ
, (38)
c
µ =

x
>
µ

Aˆ

GAˆAˆ
−1
(xµ)Aˆ , (39)
and Aˆ = {i|wˆi 6= 0} is the active set of the full solution, as before.
Note that this approximation can be easily generalized to arbitrary differentiable output
functions by replacing the logit function φlogit. Readers are thus encouraged to implement
approximate CVs in a variety of different problems.
2.2. Further simplified approximation
As mentioned above, the computational cost of our approximation is O(ML2
|Aˆ|+ML|Aˆ|
2+
|Aˆ|
3
) and should be reduced for treating larger systems. For this, we derive a further
simplified approximation based on the invented approximate formula above. We call this a
self-averaging (SA) approximation according to physics terminology.
The basic idea for simplifying our approximate formula is to assume that correlations
between Wm and Wn are sufficiently weak. The meaning of “correlation” is not evident here,
but as seen in sec. A the Hessian G can be connected to a (rescaled) covariance χ between
Wm and Wn in a statistical mechanical formulation introducing a probability distribution
of W. Our weak correlation assumption requires that the correlation between different
feature components is negligibly small; χmn(≡ (1/β)cov(Wm, Wn)) = χ(mf ,mc),(nf ,nc) =
δmfnf
(χmf
)mcnc
, where mc, nc(= 1, · · · , L) are the class indices and mf , nf (= 1, · · · , N)
are the feature component indices defined thus far, and β is the rescaling factor. In this
way, the Hessian is assumed to be expressed in a rather restricted form:

G
\µ
−1
mn
≈
(
χmf

mcnc
δmfnf
, (m, n ∈ Aˆ)
0, (otherwise) , (40)
Namely, the SA Hessian is allowed to take finite values if and only if the two indices share
the same feature vector component. The dependence on the data index µ is also assumed  
Accelerating CV in Multinomial Logistic Regression
to be negligible, implying that strong heterogeneity among feature vectors is assumed to be
absent.
To proceed with the computation, we require a closed equation to determine the L × L
matrix χi for i = 1, · · · , N. Its derivation is rather technical and is deferred to sec. A. The
result is:
(χi)Aˆ
iAˆ
i
=

λ2I
|Aˆ
i| + σ
2
x
X
M
ν=1

(IL + F
νCSA)
−1 F
ν

Aˆ
iAˆ
i
!−1
, (41)
where σ
2
x =
P
µ
P
i
x
2
µi/(NM) and Aˆ
i = {a|wˆai 6= 0} is the set of active class variables at
the feature component i; the other components of χi related to inactive variables are zeros.
The SA approximation of C
\µ
µ , CSA ∈ R
L×L, is defined by:
CSA = σ
2
x
X
N
i=1
χi
. (42)
Using the solution of eqs. (41,42), the approximate formula is now simply expressed as:
uˆ
\µ
µ ≈ uˆµ + CSAb
µ
. (43)
Note that there is no factor like (IL − F
µCµ)
−1
in contrast to eq. (24), because we directly
approximate C
\µ
µ in eq. (21).
When solving eqs. (41,42), the inverse at the right-hand side of eq. (41) becomes occasionally ill-defined again due to the presence of zero modes. In such cases, we should
remove the zero modes as eq. (28). Putting R = λ2IL + σ
2
x
PM
µ=1 
(IL + F
µCSA)
−1 F
µ

and performing the eigenvalue decomposition, we define its zero-mode-removed inverse R
−1
as:
RAˆ
iAˆ
i
=
X
j
djvjv
>
j =
X
j∈S+
djvjv
>
j ⇒ R
−1
Aˆ
iAˆ
i
=
X
j∈S+
d
−1
j
vjv
>
j
, (44)
where S
+ is the index set of the modes with finite eigenvalues. This requires a O(L
3
)
computational cost at a maximum. Leveraging this approach, a naive way to solve eqs.
(41,42) is a recursive substitution. If this converges in a constant time, irrespectively of the
system parameters N, M and L, then the computational cost of the SA approximation is
scaled as O(NL3 + ML3
). This is linear in the feature dimensionality N and the data size
M and hence, its advantage is significant.
2.3. Summary of procedures
Here, we summarize the two versions of the approximation derived thus far as algorithmic
procedures. We call the first version, based on eq. (24), the approximate CV or ACV,
and call the second one, using eq. (43), the self-averaging approximate CV or SAACV.
The procedures of ACV and SAACV are given in Alg. 1 and Alg. 2, respectively; they
are written for the case of the mixed regularization (30). Comments are added for
9
Obuchi and Kabashima
Algorithm 1 Approximate CV of the MLR
1: procedure ACV(Wˆ (λ1, λ2), DM, λ2)
2: Compute the active set Aˆ from Wˆ
3: Compute {uˆµ, Xµ
, b
µ
, Fµ}µ by eqs. (1,9), (14) and (15)
4: GAˆAˆ ←
PM
µ=1 (Xµ
)
> F
µXµ + λ2I
|Aˆ|
. O(ML|Aˆ|
2 + ML2
|Aˆ|)
5: if λ2 is large enough then . O(|Aˆ|
3
)
6: G
−1
AˆAˆ =

GAˆAˆ
−1
7: else
8: Compute G
−1
AˆAˆ by eq. (28)
9: end if
10: for µ = 1, · · · , M do . O(ML|Aˆ|
2 + ML2
|Aˆ| + ML3
)
11: Cµ ← X
µ
∗AˆG
−1
AˆAˆ

X
µ
∗Aˆ
>
12: uˆ
\µ
µ ← uˆµ + Cµ (IL − F
µCµ)
−1
b
µ
13: end for
14: Compute LOO from {u
\µ
µ }µ by eq. (8)
15: return LOO
16: end procedure
specifying the time consuming parts in the entire procedures. In Alg. 2, we describe an
actual implementation for solving CSA by recursion, which is not fully specified in sec. 2.2.
The symbol || · ||F denotes the Frobenius norm and we set the threshold θ judging the
convergence as θ = 10−6
in typical situations. We also set as 10−6
the threshold judging if
λ2 is large or not.
3. Numerical experiments
In this section, we examine the precision and actual computational time of ACV and SAACV
in numerical experiments. Both simulated and actual datasets (from UCI machine learning
repository, Lichman, 2013) are used.
For examination, we compute the errors also by literally conducting k-fold CV with
some ks, and compare it to the result of our approximate formula. In principle, we should
compare our approximate result with that of the LOO CV (k = M) because our formula
approximates it. However for large M, the literal LOO CV requires huge computational
burdens despite that the result is empirically not much different from that of the k-hold CV
with moderate ks. Hence in some of the following experiments with large M, we use the
10-hold CV instead of the LOO CV. Further, to directly check the approximation accuracy,
we also compute the normalized error difference defined as

approximate
LOO − 
literal
CV

literal
CV
, (45)
where 
literal
CV denotes the literal CV estimator of the prediction error while 
approximate
LOO is the
approximated LOOE. Moreover, as a reference, we compute the negative log-likelihood of
1 
Accelerating CV in Multinomial Logistic Regression
Algorithm 2 Self-averaging approximate CV of the MLR
1: procedure SAACV(Wˆ (λ1, λ2), DM, λ2)
2: Compute the active sets {Aˆ
i}
N
i=1 from Wˆ
3: Compute {uµ, Xµ
, bµ, Fµ}µ by eqs. (1,9), (14) and (15)
4: t ← 0 . Start initialization
5: for i = 1, · · · , N do
6: 
χ
\µ
i
(t)
← 0,
7: 
χ
\µ
i
(t)
Aˆ
iAˆ
i
← σ
−2
x
,
8: end for
9: ∆ ← 100 . End initialization
10: while ∆ > θ do . Compute CSA by recursion
11: C
(t+1)
SA ← σ
2
x
PN
i=1 
χ
\µ
i
(t)
12: R ← σ
2
x
PM
µ=1 
IL + F
µC
(t+1)
SA −1
F
µ + λ2IL . O(ML3
)
13: ∆ ← 0
14: for i = 1, · · · , N do . O(NL3
)
15: if λ2 is large enough
16: R
−1
Aˆ
iAˆ
i
=

RAˆ
iAˆ
i
−1
17: else
18: Compute R
−1
Aˆ
iAˆ
i
by eq. (44) from R
19: end if then
20: 
χ
\µ
i
(t+1)
Aˆ
iAˆ
i
← R
−1
Aˆ
iAˆ
i
21: ∆ ← ∆ +







χ
\µ
i
(t+1)
Aˆ
iAˆ
i
−

χ
\µ
i
(t)
Aˆ
iAˆ
i






F
22: end for
23: ∆ ← ∆/N
24: t ← t + 1
25: end while
26: for µ = 1, · · · , M do
27: u
\µ
µ ← uµ + C
(t)
SAb
µ
28: end for
29: Compute LOO from {u
\µ
µ }µ by eq. (8)
30: return LOO
31: end procedure
11
Obuchi and Kabashima
the full solution {wˆa}
L
a=1 as:
 =
1
M
X
M
µ=1
qµ

{wˆa}
L
a=1
, (46)
and call it the training error, hereafter. The training error is expected to be a monotonic
increasing function with respect to λ, while the prediction one is supposed to be nonmonotonic.
In all of the experiments, we used a single CPU of Intel(R) Xeon(R) E5-2630 v3 2.4GHz.
To solve the optimization problems in eqs. (4,6), we employed Glmnet (Friedman et al., 2010)
which is implemented as a MEX subroutine in MATLABR
. The two approximations were
implemented as raw codes in MATLAB. This is not the most optimized approach, because
as seen in Algs. 1,2 our approximate formula uses a number of for and while loops which are
slow in MATLAB, and hence the comparison is not necessarily fair. However, even in this
comparison there is a significant difference in the computational time between the literal
CV and our approximations, as shown below.
In Glmnet, the corresponding optimization problem is parameterized as follows:
{wˆa(λ, η ˜ )}a = arg min
{wa}a



1
M
X
M
µ=1
qµ

{wa}
L
a=1
+ λ˜

η
X
L
a=1
||wa||1 +
(1 − η)
2
X
L
a=1
||wa||2
2
!


.(47)
In the following experiments, we present the results based on this parameterization. We
basically prefer η = 1 in which the `2 term is absent, because the main contribution of the
present paper is to overcome technical difficulties stemming from the `1 term. However,
Glmnet or its employing algorithm occasionally loses its stability in some uncontrolled
manner without the `2 term. Hence, in the following experiments we adaptively choose the
value of η.
1
A sensitive point which should be noted is the convergence problem of the algorithm for
solving the present optimization problem. In Glmnet, a specialized version of coordinate
descent methods is employed, and it requires a threshold δ to judge the algorithm convergence. Unless explicitly mentioned, we set this as δ = 10−8 being tighter than the default
value. This is necessary since we treat problems of rather large sizes. A looser choice for δ
rather strongly affects the literal CV result, while it does not change the full solution or the
training error as much. As a result, our approximations employing only the full solution are
rather robust against the choice of δ compared to the literal CV. This is also demonstrated
below.
3.1. On simulated dataset
Let us start by testing with the simulated data. Suppose each “true” feature vector w0a is
independently identically drawn (i.i.d.) from the following Bernoulli-Gaussian prior:
w0a ∼
Y
N
i=1
{(1 − ρ0)δ(w0ai) + ρ0N (0, 1/ρ0)} , (48)
1. When employing our distributed codes implementing the approximate formula (Obuchi, 2017; Takahashi
and Obuchi, 2017) in conjunction with Glmnet, the parameters λ1 and λ2 are read as λ1 = Mλη˜ and
λ2 = Mλ˜(1 − η).
12
Accelerating CV in Multinomial Logistic Regression
where N (µ, σ2
) denotes a Gaussian distribution whose mean and variance are µ and σ
2
,
respectively. The resultant feature vector va becomes N ρ0(≡ K0)-sparse and its norm becomes √
N on average. Then, we choose a class yµ from {1, · · · , L} uniformly and randomly,
and generate an observed feature vector xµ by leveraging the following linear process:
xµ =
w0yµ √
N
+ ξ, (49)
where ξ is an observation noise each component of which is i.i.d. from a Gaussian N (0, σ2
N ).
For convenience, we introduce the ratio of the data size to the feature dimensionality,
α = M/N, and now obtain five parameters {N, L, α, ρ0, σ2
ξ
} characterizing the experimental
setup. It is rather heavy to obtain the dependence of all parameters and below, and hence
we mainly focus on the dependence on L, σ
2
ξ
, and N. Other parameters are set to be α = 2
and ρ0 = 0.5.
3.1.1. Result
Let us summarize the result on simulated data.
Fig. 1 shows the plots of the prediction and training errors against λ˜ for L = 4, 8, 16 at
N = 200 and σ
2
ξ = 0.01. This demonstrates that both approximations provide consistent
results with the literal LOO CV, except at small λ˜s. This inconsistency at small λ˜s is
considered to be due to a numerical instability occurring in the literal CV. Actually, for small
λ˜s, we have observed that certain small changes in the data induce large differences in the
literal CV result. This example demonstrates that our approximations provide robust curves
even in such situations. Note that as L grows the number of estimated parameters {wa}
L
a=1
increases while the data size M = αN = 400 is fixed, meaning that the problem becomes
more and more underdetermined with the growth of L. Hence, Fig. 1 demonstrates that the
developed approximations work irrespectively of how much the problem is underdetermined.
Fig. 2 exhibits the σ
2
ξ
-dependence of the errors and the approximation results for L = 8
and N = 200. For the very weak noise case (σ
2
ξ = 0.001, left), the difference between the
predictive and training errors is negligible and hence all four curves are not discriminable.
For the moderate (σ
2
ξ = 0.1, middle) and large (σ
2
ξ = 1, right) noise cases, the training curve
is very different from the predictive ones. The approximation curves are again consistent
with the literal LOO one.
Fig. 3 demonstrates how the approximation accuracy changes as the system size N
grows. For small sizes N = 50, 100, a discriminable difference exists between the results of
the approximations and the literal LOO CV, as well as the difference between the results of
the two approximations. This is expected, because our derivation relies on the largeness of
N and M. For large systems N = 400, 800, the difference among the two approximations
and the literal CV is much smaller. Considering this example in conjunction with the middle
panel of Fig. 1, we can recognize that our approximate formula becomes fairly precise for
N ≥ 200 in this parameter set. The normalized error difference corresponding to Fig. 3 is
shown in Fig. 4. We can observe that the difference tends to be smaller as the system size
increases, which is expected because the perturbation employed in our approximate formula
is justified in the large N, M limit.
Finally, let us consider the actual computational time to evaluate {wˆa}a and the approximate LOOEs, and observe its system size dependence. The left panel of Fig. 5 provides
13
Obuchi and Kabashima
10-4 10-2
10-4
10-2
100
Errors
L=4
LOO
ACV
SAACV
Training
10-4 10-2
10-4
10-2
100
Errors
L=8
LOO
ACV
SAACV
Training
10-4 10-2
10-4
10-2
100
Errors
L=16
LOO
ACV
SAACV
Training
10-4 10-2
-4
-2
0
2
4
Normalized error difference
L=4
ACV
SAACV
 1
10-4 10-2
-4
-2
0
2
4
Normalized error difference
L=8
ACV
SAACV
 1
10-4 10-2
-4
-2
0
2
4
Normalized error difference
L=16
ACV
SAACV
 1
Figure 1: (Upper) Log-log plots of the errors against λ˜ for several values of the class number
L. Other parameters are fixed at N = 200, σ
2
ξ = 0.01, α = 2 and ρ0 = 0.5. The
approximation results are consistent with the literal LOO CV results, except at
small λs, which is presumably due to a numerical instability occurring in the literal
CV at small λs. Here, η = 0.9. (Lower) The normalized error difference (45)
plotted against λ˜. The parameters of each panel are them of the corresponding
upper one. The horizontal dotted lines denote ±1 and drawn for comparison. For
small λ˜s the difference is not negligible, but the literal CV itself is not stable in
that region and hence the error difference is not reliable.
14
Accelerating CV in Multinomial Logistic Regression
10-4 10-3 10-2
10-4
10-3
10-2
10-1
Errors
2=0.001
LOO
ACV
SAACV
Training
10-4 10-3 10-2
10-2
100
Errors
2=0.1
LOO
ACV
SAACV
Training
10-4 10-3 10-2
10-2
100
Errors
2=1
LOO
ACV
SAACV
Training
10-4 10-3 10-2
-4
-2
0
2
4
Normalized error difference
2=0.001
ACV
SAACV
 1
10-4 10-3 10-2
-4
-2
0
2
4
Normalized error difference
2=0.1
ACV
SAACV
 1
10-4 10-3 10-2
-4
-2
0
2
4
Normalized error difference
2=1
ACV
SAACV
 1
Figure 2: (Upper) Log-log plots of the errors against λ˜ for several noise strengths. Other
parameters are fixed at N = 200, L = 8, α = 2 and ρ0 = 0.5. The approximation
results are consistent with the literal LOO CV, irrespectively of the noise strength.
The convergence threshold δ is set to be δ = 10−9
for the case σ
2
ξ = 1. Here,
η = 1. (Lower) The normalized error difference (45) plotted against λ˜. The
parameters of each panel are them of the corresponding upper one. In the whole
region the difference is negligibly small.
15
Obuchi and Kabashima
100
10-4
10-2
100
Errors
N=50
LOO
ACV
SAACV
Training
10-4 10-2
10-4
10-2
100
Errors
N=100
LOO
ACV
SAACV
Training
10-4 10-2
10-4
10-2
100
Errors
N=400
LOO
ACV
SAACV
Training
10-4 10-2
10-4
10-2
100
Errors
N=800
LOO
ACV
SAACV
Training
Figure 3: Log-log plots of the errors against λ˜ for several values of feature dimensionality
N. Other parameters are fixed at L = 8, σ
2
ξ = 0.01, α = 2 and ρ0 = 0.5. Here,
η = 0.9.
16
Accelerating CV in Multinomial Logistic Regression
10-4 10-2 100
-4
-2
0
2
4
Normalized error difference
N=50
ACV
SAACV
 1
10-4 10-2
-4
-2
0
2
4
Normalized error difference
N=100
ACV
SAACV
 1
10-4 10-2
-4
-2
0
2
4
Normalized error difference
N=400
ACV
SAACV
 1
10-4 10-2
-4
-2
0
2
4
Normalized error difference
N=800
ACV
SAACV
 1
Figure 4: The plot of the normalized error difference corresponding to Fig. 3. The difference
tends to be smaller as the system size increases.
17
Obuchi and Kabashima
the plot of the actual computational time against the system size. Here, the number of
examined points of λ˜ to obtain a solution path is different from size to size, and hence the
plotted time is given as the whole computational time to obtain the solution path divided
by the number of λ˜s points. The left panel of Fig. 5 clearly displays the advantage and
50 100 200 400 1000 4000
N
10-3
10-2
10-1
100
101
102
103
104
105
time (sec.)
optimization
ACV
SAACV
slope 1
10-5 10-4 10-3 10-2
10-3
10-2
10-1
Errors
Convergence threshold sensitivity
10-fold( =10-6)
10-fold( =10-8)
ACV( =10-6)
ACV( =10-8)
SAACV( =10-6)
SAACV( =10-8)
Figure 5: (Left) Actual computational time spent to find the solution of eq. (4) and that
for ACV and SAACV, plotted against the feature dimensionality N in a double
logarithmic scale. Note that the computational time for the k-fold CV is about
k times larger than that for finding the solution of eq. (4), represented by the
green asterisks. Parameters are fixed at L = 8, σ
2
ξ = 0.01, α = 2 and ρ0 = 0.5.
Here, η = 1. (Right) The errors are obtained for the two convergence thresholds
δ = 10−6 and δ = 10−8
. Error bars are omitted for visibility. For the tighter case
δ = 10−8
, the minimum value of λ˜ in the examined range is larger than that of the
case δ = 10−6
, though the systematic difference with the results of the literal LOO
CV is already clear. The training errors of these two different δ, represented by
black circles and left-pointing triangles, are completely overlapping. The system
parameters are N = 400, L = 8, σ
2
ξ = 0.01, α = 2 and ρ0 = 0.5. Here, η = 1.
disadvantage of the developed approximations. For small sizes, the computational time for
optimization to obtain {wˆa}a is shorter than the time to compute the approximate LOOEs,
and hence the literal CV is better. However, for larger systems, the optimization cost increases rapidly and for N >∼ 400 the approximate CV is better. For N >∼ 800, the ACV
cost exceeds that of SAACV. The SAACV cost behaves linearly as a function of N (see the
black dashed line), and hence for larger systems of N >∼ 800 SAACV can be a very powerful
tool. As a related issue, we mention the convergence problem of the algorithm. In the right
panel of Fig. 5, we compare the errors at two different values of the convergence threshold
δ. An important observation is that a significant difference exists in the literal CV results
while other curves do not show a strong change. This implies that our approximate formula
is rather robust and can be used with a rather loose convergence threshold or conversely,
we can use the systematic deviation between the literal CV and our approximations as an
18
Accelerating CV in Multinomial Logistic Regression
indicator to verify the tightness of the convergence threshold. This is beneficial, especially
when treating large models, for which the convergence check is a common annoying task.
3.2. On real-world dataset
Next, we test the approximate formula on a real-world dataset. As shown above, our
approximations become more precise if the model dimensionality and data size are large.
Hence, we chose the ISOLET dataset which is a relatively large problem among classification tasks collected in the UCI machine learning repository (Lichman, 2013). The feature
dimensionality, the data size, and the class number are N = 617, M = 6238, and L = 26,
respectively. Here we apply the 10-fold CV, instead of the LOO CV because of the computational reason, and our approximations to this dataset. The result is given in Fig. 6. The
10-4 10-3 10-2 10-1
10-1
100
Errors
ISOLET data, 26 classes
10-fold
ACV
SAACV
Training
10-4 10-3 10-2 10-1
-4
-2
0
2
4
Normalized error difference
ISOLET data, 26 classes
ACV
SAACV
 1
Figure 6: Approximate CV performance on the ISOLET data of L = 26 classes. The
errors are shown in the left panel and the normalized error differences between
the approximations and the 10-fold CV are in the right panel. At the estimated
minimums of the prediction error, the accuracy rate for correctly classifying the
test data is about 0.86 while the probability of recovering the training data is
about 0.98, commonly among the literal CV and the two approximations. At
the minimum value of λ˜, the leftmost point in the figure, the accuracy rates are
different among the three different methods, and are 0.83, 0.78, and 0.81 for the
literal CV, ACV and SAACV, respectively. Here, η = 1.
results of the approximations and of the 10-fold CV demonstrate a fairly good agreement,
proving the actual effectiveness of the developed approximations. In an experiment, the
actual computational time to obtain the result of the full simulation, of the 10-fold CV,
of ACV, and of SAACV were 785, 7825, 5173, and 689 seconds, respectively. The system
parameters {N, M, L} are rather large in this problem and thus the advantage of ACV is
not large, while the efficiency of SAACV stands out in such situations.
19
Obuchi and Kabashima
3.3. When does SAACV fail?
Two major factors neglected in SAACV are the correlations among feature components
and the heterogeneity among feature vectors. If these factors are strong, the approximation
accuracy of SAACV is expected to be degraded. In this section, we examine this point.
First, to test the impact of correlations in feature components, we add further constraints
to the simulated data treated in sec. 3.1 and examine the approximation performance on
the situation. Two cases are treated: the first is the case where the true feature vectors
{w0a} have common components among all the classes. The result of this case is shown
in the left panels in Fig. 7. Here, the fraction of the common components to the non-zero
components is rcommon = 0.9 and thus the overlap between feature vectors of different classes
is rather large. The other is the case where the noise vector has strong correlations among
the components. The result of this case is presented in the right panels in Fig. 7, in which
the noise strength is σ
2
ξ = 1 and the correlation coefficient of any pair of noise components is
Corr(ξi
, ξj ) = 0.9; hence the noise and the correlation are rather large. For both the cases,
the performance of the approximate formula is fairly good, implying that SAACV is likely
to perform well even when components of the feature vectors are correlated. Similar findings
were actually obtained in the case of linear models (Obuchi and Kabashima, 2016). This
is a preferable observation because it implies that the applicable limit of SAACV can be
extended to a wider class of feature vectors than that is assumed in our present derivation in
which the weakness of the correlations is assumed, as seen in sec. A. These also imply that
there possibly exists another approximation formula taking into account the correlations
but being similar to SAACV. A promising framework to derive such a formula might be the
adaptive TAP method (Opper and Winther, 2001a,b, 2005). The adaptive TAP method
itself requires a larger computational cost than that of SAACV but it is possible to reduce
the computational cost up to the linear scaling with respect to N and M by employing
an additional simplifying approximation (Kabashima and Vehkaper¨a, 2014; C¸ akmak and
Opper, 2018). This is, however, rather technical and we leave it as a future work.
Second, to examine the effect of the heterogeneity among feature vectors, we introduce
an amplifying factor Ω to control the norm of feature vectors. In particular, we multiply the factor Ω to the feature vectors of some chosen classes, as xµ → Ωxµ. Here, we
use the simulated data identical to that for the center panels in Fig. 3 of the parameters
(N, L, α, ρ0, σ2
ξ
, η) = (200, 8, 2, 0.5, 0.1, 1), except that the amplifying factor Ω = 100 is applied to the latter four classes y = 5, 6, 7, 8. The approximation performance on this dataset
is shown in Fig. 8. We also examine the impact of the same heterogeneity on a real-world
dataset in Fig. 9. Here, we treat the well-known MNIST data of handwritten digits (LeCun
et al., 1998). For simplicity, we only use the data of two digits: 0 and 1. As a preprocessing,
feature components with small variances are removed and only the N = 350 components of
the largest variances are retained; the original size of the feature vector is 784 = 28×28 and
thus almost the half of the components are discarded. Then, the usual standardization procedure is conducted. Further, we apply the amplifying factor Ω = 10 to the class of 1 (right
panels), while the case without the amplification (or Ω = 1, left panels) is also examined
for comparison. These two examples clearly show that ACV shows a consistency with the
LOO CV behavior while SAACV does not, demonstrating that SAACV gives an inaccurate
estimate of the CV error for datasets with strong heterogeneity. This kind of heterogeneity
20
Accelerating CV in Multinomial Logistic Regression
10-4 10-2
10-1
100
Errors
r
common
=0.9
LOO
ACV
SAACV
Training
10-3 10-2 10-1
100
Errors
2=1, Corr=0.9
LOO
ACV
SAACV
Training
10-4 10-3 10-2 10-1
-4
-2
0
2
4
Normalized error difference
r
common
=0.9
ACV
SAACV
 1
10-3 10-2 10-1
-4
-2
0
2
4
Normalized error difference
2=1, Corr=0.9
ACV
SAACV
 1 Figure 7: (Upper) Log-log plots of the errors against λ˜ for correlated feature vectors.
The left panel is for the case with common components in true feature vectors
while the right one is of the correlated noise case. Parameters (N, L, α, ρ0, η) =
(200, 8, 2, 0.5, 1) are common in both the cases, while the noise strengths and
convergence thresholds are different: (σ
2
ξ
, δ) = (0.1, 10−8
) (left) and (σ
2
ξ
, δ) =
(1, 10−9
) (right). (Lower) The normalized error difference (45) plotted against λ˜.
The parameters of each panel are them of the corresponding upper one.
21
Obuchi and Kabashima
10-3 10-2 10-1
0.5
1
1.5
2
Errors
=100
LOO
ACV
SAACV
Training
10-3 10-2 10-1
-4
-2
0
2
4
Normalized error difference
=100
ACV
SAACV
 1
Figure 8: (Left) Log-log plots of the errors against λ˜ with strong heterogeneity in feature
vectors. The same dataset as that of the center panels in Fig. 3 is used but the
feature vectors for the classes yµ = 5, 6, 7, 8 are amplified as xµ → Ωxµ by the
factor Ω = 100. The ACV result is consistent with the LOO CV one while that
of SAACV is not. (Right) The normalized error difference corresponding to the
left panel.
can naturally emerge in some applications: for example if we consider problems in medical
statistics, a number of biological markers can give distinguishably large values for affected
patients compared to unaffected ones, yielding larger values in norm for feature vectors
of affected patients. This consideration suspects the efficiency of SAACV. We, however,
stress that this kind of heterogeneity attributed to the belonging class can be absorbed by
rescaling the weights as {wa}a → {Ω
−1
a wa}a, where Ωa is chosen to homogenize the feature
vector norm in different classes as ||xyµ Ωa||2 ≈ const. For the `1 regularization case, this
resultantly leads to the regularization coefficients which take different values adaptively to
the belonging class as
λ
X
a
||wa||1 →
X
a
λΩa||wa||1 =
X
a
λa||wa||1. (50)
For this problem with adaptive regularization coefficients, our approximation formula can
be applied in the completely same manner, which can be convinced by seeing Algs. 1,2 where
the value of the regularization coefficient is not required as the argument. The `2-norm can
also be handled, though the codes should be extended to take into account the groupwise
coefficients as arguments. We argue that this rescaling is a natural prescription to treat
strong heterogeneity among different classes, and once employing this prescription the weak
point of SAACV is naturally cured.
As a noteworthy remark, we point out that the basic idea of SAACV is closely related
to Wahba’s generalized cross-validation (GCV) for linear regression (Golub et al., 1979). In
GCV, the heterogeneity in coefficient corresponding to C
\µ
µ in SAACV is also neglected, and
hence it shares the same weak point as SAACV, when it is regarded as an approximation
of the CV estimator to generalization errors. We stress that this kind of approximation
22
Accelerating CV in Multinomial Logistic Regression
10-4 10-3 10-2
10-3
10-2
Errors
mnist (2 class), N=350, =1
10-fold
ACV
SAACV
Training
10-4 10-3 10-2
10-3
10-2
Errors
mnist (2 class), N=350, =10
10-fold
ACV
SAACV
Training
10-4 10-3 10-2
-4
-2
0
2
4
Normalized error difference
mnist (2 class), N=350, =1
ACV
SAACV
 1
10-4 10-3 10-2
-4
-2
0
2
4
Normalized error difference
mnist (2 class), N=350, =10
ACV
SAACV
 1
Figure 9: (Upper) Log-log plots of the errors against λ˜ of mnist handwritten data with two
digits 0 and 1. The amplifying factor is not applied (or Ω = 1) in the left panels
while is applied (Ω = 10) in the right ones. The strong heterogeneity among the
classes affect the performance of SAACV. (Lower) The normalized error difference
corresponding to the upper panels.
23
Obuchi and Kabashima
reducing the computational cost is again needed because the data size and the model dimensionality are increasing rapidly in recent years.
4. Conclusion
In this paper, we have developed an approximate formula for the CV estimator of the
predictive likelihood of the multinomial logistic regression regularized by the `1-norm. An
extension to the elastic net regularization has been also stated. We have demonstrated their
advantages and disadvantages in numerical experiments using simulated and real-world
datasets. Two versions of the approximation have been defined based on the developed
formula. The first version, abbreviated as ACV, has a better performance, in terms of
computational time, for middle size problems. It will eventually become worse than the
literal k-fold CV with moderate ks as the problem size grows, because its computational
time is scaled as a third-order polynomial of the feature dimensionality and data size, N
and M, though such a tendency has not been observed in the investigated range of N. We
have also defined the second version based on ACV, the computational time of which is
just scaled linearly with respect to N and M. This second approximation is called SAACV,
and it has been demonstrated that SAACV is slow for small size problems but has a great
advantage for large size problems. Hence, we suggest leveraging the literal CV for small,
ACV for middle, and SAACV for large size problems.
Our derivation is based on the perturbation which assumes that there is a small difference
between the full and leave-one-out solutions. This assumption will not be satisfied for some
specific cases. Even with this restriction, we expect the range of application of our formula
is wide enough and we would like to encourage readers to leverage it in their own work.
We have implemented MATLAB and python codes and they are available in (Obuchi, 2017;
Takahashi and Obuchi, 2017).
The perturbative approach employed here is fairly general and can be applied to a wide
class of generalized linear models with convex regularizations. The development of practical
formulas for these cases will be of great assistance, given that we are living in the Big Data
era.