In many applications involving large dataset or online learning, stochastic gradient descent
(SGD) is a scalable algorithm to compute parameter estimates and has gained increasing
popularity due to its numerical convenience and memory efficiency. While the asymptotic
properties of SGD-based estimators have been well established, statistical inference such
as interval estimation remains much unexplored. The classical bootstrap is not directly
applicable if the data are not stored in memory. The plug-in method is not applicable
when there is no explicit formula for the covariance matrix of the estimator. In this paper,
we propose an online bootstrap procedure for the estimation of confidence intervals, which,
upon the arrival of each observation, updates the SGD estimate as well as a number of
randomly perturbed SGD estimates. The proposed method is easy to implement in practice.
We establish its theoretical properties for a general class of models that includes linear
regressions, generalized linear models, M-estimators and quantile regressions as special
cases. The finite-sample performance and numerical utility is evaluated by simulation
studies and real data applications.
Keywords: Bootstrap, Interval estimation, Generalized linear models, Large datasets,
M-estimators, Quantile regression, Resampling methods, Stochastic gradient descent
1. Introduction
Big datasets arise frequently in clinical, epidemiological, financial and sociological studies.
In such applications, many classical optimization methods for parameter estimation such as
Fisher scoring, the EM algorithm or iterated reweighted least squares (Hastie et al. 2009,
Nelder and Baker 1972) do not scale well and are computationally less attractive. Due to
its computational and memory efficiency, stochastic gradient descent (Robbins and Monro
1951)[SGD] is a scalable algorithm for parameter estimation and has recently drawn a great
deal of attention. Unlike other classical methods that evaluate the objective function involving the entire dataset, the SGD method calculates the gradient of the objective function
using only one data point at a time and recursively updates the parameter estimate. This
is also numerically appealing and particularly useful in online updating settings such as
streaming data where it may not even be feasible to store the entire dataset in memory.
Wang et al. (2016) gave a nice review on recent achievements of applying the SGD method
to big data and streaming data.
The asymptotic properties of SGD estimators such as consistency and asymptotic normality have been well established; see, for example, Ruppert (1988) and Polyak and Juditsky
(1992). However, statistical inference such as confidence interval estimation for SGD estimators has remained largely unexplored. Traditional interval estimation procedures such
as the plug-in procedure and the bootstrap are often numerically difficult in the presence
of big datasets. The bootstrap repeatedly draws samples from the entire dataset. The
plug-in procedure requires an explicit variance-covariance formula. Since the classical bootstrap is not directly applicable if the data are not stored in memory, using the deal from
the weighted bootstrap (Rubin 1981), we propose an online bootstrap procedure for the
estimation of confidence intervals.
There are only a few papers considering the statistical inference of the SGD method.
Chen et al. (2016) proposed a method called the batch-mean procedure. Although computationally efficient and theoretically sound, the batch-means procedure substantially underestimates the variance of the SGD estimator in finite-sample studies, because of the
correlations between the batch means. Li et al. (2017) presented a new method for statistical inference in M-estimation problems, based on SGD estimators with a fixed step size.
However, this method is limited to M-estimation and fixed step size. Su and Zhu (2018)
proposed a new method called HiGrad, short for Hierarchical Incremental GRAdient Descent, which estimates model parameters in an online fashion and provides a confidence
interval for the true population value. This method is also computationally efficient and
theoretically sound, but it is not applicable to vanilla SGD estimators.
In this paper, we propose an online bootstrap resampling procedure to approximate the
distribution of a SGD estimator in a general class of models that includes linear regressions,
generalized linear models, M-estimators and quantile regressions as special cases. Our
proposal, justified by asymptotic theories, provides a simple way to estimate the covariance
matrix and confidence regions. Through numerical experiments, we verify the ability of this
procedure to give accurate inference for big datasets.
The rest of the article is organized as follows. In Section 2, we introduce the proposed
online bootstrap procedure for constructing confidence regions. In Section 3, we theoretically justify the validity of our proposal for a general class of models, along with some
special cases. In Section 4, we demonstrate the performance of the proposed procedures
in finite samples via simulation studies and three real data applications. Some concluding
remarks are given in Section 5 and all the technical proofs are relegated to the Appendix.
2. The proposed resampling procedure
Parameter estimation by optimizing an objective function is often encountered in statistical
practice. Consider the general situation where the optimal model parameter Î¸0 âˆˆ Rp
is
defined to be the minimizer of the expected loss function,
Î¸0 = argmin
Î¸âˆˆÎ˜
n
L(Î¸) , E[`(Î¸;Z)]o
, (1)
2
Online Bootstrap for SGD
where `(Î¸; z) is some loss function and Z denotes one single observation and Î˜ is the
domain on which the loss function is defined, which is assumed to be open. Suppose that
the data consist of independent and identically distributed (i.i.d.) copies of Z, denoted by
DN = {Z1, . . . , ZN }. Under mild conditions, Î¸0 can be consistently estimated by
Î¸eN = argmin
Î¸âˆˆÎ˜
(
1
N
X
N
i=1
`(Î¸;Zi)
)
. (2)
However, the minimization problem (2) for large-scale datasets pose numerical challenges.
Furthermore, for applications such as online data where each sample arrives sequentially
(e.g., search queries or transactional data), it may not be necessary or feasible to store the
entire dataset, leaving alone evaluating the minimand in (2).
As a stochastic approximation method (Robbins and Monro 1951), stochastic gradient
descent is a scalable algorithm for parameter estimation with large-scale data. Given an
initial estimate Î¸b0, the SGD method recursively updates the estimate upon the arrival of
each data point Zn, n = 1, 2, . . . , N,
Î¸bn = Î¸bnâˆ’1 âˆ’ Î³nâˆ‡`(Î¸bnâˆ’1;Zn), (3)
where the learning rates are Î³n = Î³1n
âˆ’Î± with Î³1 > 0 and Î± âˆˆ (0.5, 1). As suggested by
Ruppert (1988) and Polyak and Juditsky (1992), we consider the averaging estimate,
Î¸n =
1
n
Xn
i=1
Î¸bi
, (4)
which can also be recursively updated given that Î¸n = (n âˆ’ 1)Î¸nâˆ’1/n + Î¸bn/n.
In order to conduct statistical inference with the averaging SGD estimator Î¸n at any
stage, we propose an online bootstrap resampling procedure, which recursively updates
the SGD estimate as well as a large number of randomly perturbed SGD estimates, upon
the arrival of each data point. Specifically, let W = {Wi
, i = 1, . . . , N} be a set of i.i.d.
non-negative random variables with mean and variance equal to one. In parallel with (3)
and (4), with Î¸bâˆ—
0 â‰¡ Î¸b0, upon observing data point Zn, we recursively updates randomly
perturbed SGD estimates,
Î¸bâˆ—
n = Î¸bâˆ—
nâˆ’1 âˆ’ Î³nWnâˆ‡`(Î¸bâˆ—
nâˆ’1
;Zn), (5)
Î¸
âˆ—
n =
1
n
Xn
i=1
Î¸bâˆ—
i
. (6)
We will show that âˆš
n(Î¸n âˆ’ Î¸0) and âˆš
n(Î¸
âˆ—
n âˆ’ Î¸n) converge in distribution to the same
limiting distribution. In practice, these results allow us to estimate the distribution of
âˆš
n(Î¸n âˆ’ Î¸0) by generating a large number, say B, of random samples of W. We obtain Î¸
âˆ—,b
n
by sequentially updating perturbed SGD estimates for each sample, b = 1, . . . , B,
Î¸bâˆ—,b
n = Î¸bâˆ—,b
nâˆ’1 âˆ’ Î³nWn,bâˆ‡`(Î¸bâˆ—,b
nâˆ’1
;Zn), (7)
Î¸
âˆ—,b
n =
1
n
Xn
i=1
Î¸bâˆ—,b
i
, (8)
3
Yixin Fang, Jinfeng Xu and Lei Yang
and then approximate the sampling distribution of Î¸n âˆ’ Î¸0 using the empirical distribution
of {Î¸
âˆ—,b
n âˆ’ Î¸n, b = 1, ..., B}. Specifically, the covariance matrix of Î¸n can be estimated by the
sample covariance matrix constructed from {Î¸
âˆ—,b
n
, b = 1, ..., B}. Estimating the distribution
of âˆš
n(Î¸n âˆ’ Î¸0) based on the distribution of âˆš
n(Î¸
âˆ—
n âˆ’ Î¸n)|Dn leads to the construction
of (1 âˆ’ Î±)100% confidence regions for Î¸0. The resulting inferential procedure retains the
numerical simplicity of the SGD method, only using one pass over the data. The proposed
inferential procedure scales well for datasets with millions of data points or more, and its
theoretical validity can be justified for a general class models with mild regularity conditions
as shown in the next section.
3. Theoretical Results
3.1. Main theorems
In this section, we derive some theoretical properties of Î¸
âˆ—
n
, justifying that the conditional
distribution of Î¸
âˆ—
n âˆ’ Î¸n given data Dn = {Z1, Z2, . . . , Zn} can approximate the sampling
distribution of Î¸n âˆ’ Î¸0, under the following assumptions. Let k Â· k be the Euclidean norm
for vectors and the operator norm for matrices. The proofs are presented in the Appendix.
(A1). The objective function L(Î¸) is convex, continuously differentiable over Î¸ âˆˆ Î˜, and
twice continuously differentiable at Î¸ = Î¸0, where Î¸0 is the unique minimizer of L(Î¸).
(A2). The gradient of L(Î¸), R(Î¸) = âˆ‡L(Î¸), is Lipschitz continuous with constant L1 > 0;
that is, for any Î¸1 and Î¸2, kR(Î¸1) âˆ’ R(Î¸2)k â‰¤ L1kÎ¸1 âˆ’ Î¸2k.
(A3). The Hessian matrix of L(Î¸), S(Î¸) = âˆ‡2L(Î¸), exists and is positive definite at Î¸0 with
S0 = S(Î¸0) > 0 and is Lipschitz continuous at Î¸0 with constant L2 > 0.
(A4). Let V0 = E {[âˆ‡`(Î¸0;Z)][âˆ‡`(Î¸0;Z)]T}. Assume Ekâˆ‡`(Î¸;Z)k
2 â‰¤ C(1 + kÎ¸k
2
) for some
C and Ekâˆ‡`(Î¸;Z) âˆ’ âˆ‡`(Î¸0;Z)k
2 â‰¤ Î´(kÎ¸ âˆ’ Î¸0k) for some Î´(Â·) with Î´(x) â†’ 0 as x â†’ 0.
(A5). The learning rates are chosen as Î³n = Î³1n
âˆ’Î± with Î³1 > 0 and Î± âˆˆ (0.5, 1).
(A6). The perturbation variables, W1, W2, . . ., are non-negative i.i.d. random variables satisfying that E(Wn) = Var(Wn) = 1.
Following similar arguments in Ruppert (1988) and Polyak and Juditsky (1992), we can
prove the asymptotic normality of the SGD estimator Î¸n under the above assumptions.
Lemma 1 If Assumptions A1-A5 are satisfied, then we have
âˆš
n(Î¸n âˆ’ Î¸0) â‡’ N
0, Sâˆ’1
0 V0S
âˆ’1
0

, in distribution, as n â†’ âˆ. (9)
From Lemma 1, we can conduct statistical inference based on Î¸n provided that we can
estimate the covariance matrix S
âˆ’1
0 V0S
âˆ’1
0
, or we can use some resampling procedure to
approximate the sampling distribution of âˆš
n(Î¸n âˆ’ Î¸0). We first derive the asymptotically
linear representation of Î¸
âˆ—
n
for any perturbation variables that are i.i.d. random variables
satisfying that E(Wn) = 1.
 
Online Bootstrap for SGD
Theorem 2 If Assumptions A1-A5 hold, and the perturbation variables, W1, W2, . . ., are
non-negative i.i.d. random variables satisfying that E(Wn) = 1, then we have,
âˆš
n(Î¸
âˆ—
n âˆ’ Î¸0) = âˆ’
1
âˆš
n
S
âˆ’1
0
Xn
i=1
Wiâˆ‡`(Î¸0;Zi) + op(1). (10)
By Theorem 1, letting Wn â‰¡ 1, we derive the following representation for Î¸n,
âˆš
n(Î¸n âˆ’ Î¸0) = âˆ’
1
âˆš
n
S
âˆ’1
0
Xn
i=1
âˆ‡`(Î¸0;Zi) + op(1). (11)
Then, considering the difference between (2) and (11), we have
âˆš
n(Î¸
âˆ—
n âˆ’ Î¸n) = âˆ’
1
âˆš
n
S
âˆ’1
0
Xn
i=1
(Wi âˆ’ 1)âˆ‡`(Î¸0;Zi) + op(1). (12)
Let P
âˆ— and E
âˆ— denote the conditional probability and expectation given the data. Starting from (12), we derive the following theorem.
Theorem 3 If Assumptions A1-A6 hold, then we have
sup
vâˆˆRp



P
âˆ—
âˆš
n(Î¸
âˆ—
n âˆ’ Î¸n) â‰¤ v

âˆ’ P
âˆš
n(Î¸n âˆ’ Î¸0) â‰¤ v



â†’ 0, in probability. (13)
By Theorem 2, the Kolmogorow-Smirnov distance between âˆš
n(Î¸
âˆ—
nâˆ’Î¸n) and âˆš
n(Î¸nâˆ’Î¸0)
converges to zero in probability. This validates our proposal of the perturbation-based
resampling procedure for inference with SGD. In the next section, we consider some special
cases where Assumptions A1-A4 are satisfied.
3.2. Special cases
3.2.1. Cases where `(Î¸;Z) is twice differentiable
If the loss function `(Î¸;Z) is twice differentiable, we can use the plug-in procedure to
estimate the asymptotic covariance matrix of âˆš
N(Î¸N âˆ’Î¸0), S
âˆ’1
0 V0S
âˆ’1
0
. That is, at the final
step, S0 and V0 can be estimated respectively by
SbN =
1
N
X
N
i=1
âˆ‡2
`(Î¸N ;Zi) and VbN =
1
N
X
N
i=1
[âˆ‡`(Î¸N ;Zi)][âˆ‡`(Î¸N ;Zi)]T
. (14)
However, the above final-step plug-in estimation is impractical for large-scale data or
streaming data, because it requires that the whole dataset be stored. To overcome this
problem, in practice we can estimate S0 and V0 recursively, for n = 1, 2, . . ., using
Sbn =
1
n
Xn
i=1
âˆ‡2
`(Î¸i
;Zi) and Vbn =
1
n
Xn
i=1
[âˆ‡`(Î¸i
;Zi)][âˆ‡`(Î¸i
;Zi)]T
. (15)
In the following, we examine two examples where `(Î¸;Z) is twice differentiable. Example
1 is linear regression, where the loss function `(Î¸;Z) is twice differentiable and the objective
5
Yixin Fang, Jinfeng Xu and Lei Yang
function L(Î¸) is strongly convex. Example 2 is logistic regression, where `(Î¸;Z) is twice
differentiable but L(Î¸) is non-strongly convex. They are two examples of generalized linear
models, one for quantitative outcome and the other for binary outcome. In these two
examples, both the plug-in procedures and the proposed perturbation resampling procedure
are robust to model mis-specification.
Example 1 (Linear regression) Suppose that Zn = (Yn, Xn), n = 1, 2, . . ., are i.i.d. copies
of Z = (Y, X), where Y is quantitative and X is p-dim with EkXk
2 < âˆ. Let
Î¸0 = arg min
Î¸âˆˆRp
E (Y âˆ’ XT
Î¸)
2
, (16)
where `(Î¸;Z) = (Y âˆ’ XT
Î¸)
2
is twice differentiable and L(Î¸) = E (Y âˆ’ XT
Î¸)
2
is strongly
convex. Moveover, âˆ‡`(Î¸;Z) = âˆ’2(Y âˆ’XT
Î¸)X, âˆ‡2
`(Î¸;Z) = 2XTX, âˆ‡L(Î¸) = 2E{XX T}Î¸âˆ’
2E{XY }, and âˆ‡2L(Î¸) = E{âˆ‡l(Î¸;Z)} = 2E{XX T}. Letting V0 = 4E{(Y âˆ’ XT
Î¸0)
2XX T}
and S0 = 2E{XX T}, we can easily verify that Assumptions A1-A4 hold. The SGD and
perturbed SGD updates for Î¸0, as defined in (3) and (5) respectively, are
Î¸bn = Î¸bnâˆ’1 + 2Î³n(Yn âˆ’ XT
n
Î¸bnâˆ’1)Xn, (17)
Î¸bâˆ—
n = Î¸bâˆ—
nâˆ’1 + 2Î³nWn(Yn âˆ’ XT
n
Î¸bâˆ—
nâˆ’1
)Xn. (18)
Example 2 (Logistic regression) Suppose that Zn = (Yn, Xn), n = 1, 2, . . ., are i.i.d. copies
of Z = (Y, X), where Y = Â±1 and X is p-dim with EkXk
2 < âˆ. Let
Î¸0 = arg min
Î¸âˆˆRp
E

âˆ’ log 
1
1 + exp(âˆ’Y X TÎ¸)
 , (19)
where `(Î¸;Z) = log (1 + exp(âˆ’Y X T
Î¸)) is twice differentiable and L(Î¸) = E{`(Î¸;Z)} is nonstrongly convex. Moveover, âˆ‡`(Î¸;Z) = âˆ’
1
1+exp(Y XTÎ¸)XY , âˆ‡2
`(Î¸;Z) = exp(Y XTÎ¸)
[1+exp(Y XTÎ¸)]2 XX T
,
âˆ‡L(Î¸) = E {âˆ‡`(Î¸;Z)}, and âˆ‡2L(Î¸) = E

âˆ‡2
`(Î¸;Z)
	
. Letting V0 = E
n
1
[1+exp(Y XTÎ¸)]2 XX T
o
and S0 = E
n
exp(Y XTÎ¸)
[1+exp(Y XTÎ¸)]2 XX T
o
, we can easily verify that Assumptions A1-A4 hold. The
SGD and perturbed SGD updates for Î¸0, as defined in (3) and (5) respectively, are
Î¸bn = Î¸bnâˆ’1 + Î³nXY /[1 + exp(Y X T
Î¸bnâˆ’1)], (20)
Î¸bâˆ—
n = Î¸bâˆ—
nâˆ’1 + Î³nWnXY /[1 + exp(Y X T
Î¸bâˆ—
nâˆ’1
)]. (21)
We conclude this subsection with some discussion on the strong convexity of objective
function L(Î¸), which is strongly convex in Example 1 and is non-strongly convex in Example
2. If L(Î¸) is strongly convex, i.e. there exists Âµ > 0 such that L(Î¸1) â‰¥ L(Î¸2) +âˆ‡L(Î¸2)
T
(Î¸1 âˆ’
Î¸2) + ÂµkÎ¸1 âˆ’ Î¸2k
2
for any Î¸1 and Î¸2, Moulines and Bach (2011) derived a non-asymptotic
bound for (EkÎ¸n âˆ’ Î¸0k)
1/2
. The bound of (EkÎ¸n âˆ’ Î¸0k)
1/2 has several terms; the leading
term is of order O(n
âˆ’1
) and the next two leading terms have order O(n
Î±âˆ’2
) and O(n
âˆ’2Î±),
suggesting the setting Î± = 2/3 to make them equal. If L(Î¸) is non-strongly convex, Moulines
and Bach (2011) derived a non-asymptotic bound for E[L(Î¸bn)âˆ’L(Î¸0)] and a non-asymptotic
bound for E[L(Î¸n) âˆ’ L(Î¸0)]. The bound of E[L(Î¸bn) âˆ’ L(Î¸0)] is O(max{n
Î±âˆ’1
, nâˆ’Î±/2}), also
suggesting the setting Î± = 2/3 to achieve optimal rate O(n
âˆ’1/3
). Using the Polyak-Ruppert
averaging has allowed the bound of E[L(Î¸n) âˆ’ L(Î¸0)] to go from O(max{n
Î±âˆ’1
, nâˆ’Î±/2}) to
O(n
âˆ’Î±). Therefore, we use Î± = 2/3 in the numerical results.
6
Online Bootstrap for SGD
3.2.2. Cases where `(Î¸;Z) is not twice-differentiable
If the loss function `(Î¸;Z) is not twice-differentiable, neither the final-step plug-in estimation
(14) nor the recursive plug-in estimation (15) is applicable. Fortunately, our proposal
of scalable inference based on perturbation resampling is still applicable because it only
depends on the first order derivative âˆ‡`(Î¸;Z). To understand this explicitly, consider the
following example of robust regression via Ïˆ-type M-estimator where the loss function may
be not twice-differentiable.
Example 3 (Robust regression via Ïˆ-type M-estimator) Suppose that Zn = (Yn, Xn),
n = 1, 2, . . ., are i.i.d. copies of Z = (Y, X), where Y is quantitative and X is p-dim with
EkXk
2 < âˆ. Let Ï(Â·) be some convex function with Ï(0) = 0 and we attempt to estimate
Î¸0 = arg min
Î¸âˆˆRp
EÏ (Y âˆ’ XT
Î¸), (22)
where `(Î¸;Z) = Ï(Y âˆ’XT
Î¸) and L(Î¸) = EÏ (Y âˆ’ XT
Î¸). This is robust regression via Ï-type
M-estimator. If Ï(Â·) is differentiable with derivative Ë™Ï(Â·) = Ïˆ(Â·), we can solve it via Ïˆ-type
M-estimator, solving the following equation,
E {Ïˆ (Y âˆ’ XT
Î¸0) X} = 0, (23)
where âˆ‡`(Î¸;Z) = âˆ’Ïˆ(Y âˆ’ XT
Î¸)X and âˆ‡L(Î¸) = âˆ’E {Ïˆ (Y âˆ’ XT
Î¸) X}. Hence, the SGD
and perturbed SGD updates for Î¸0, as defined in (3) and (5) respectively, are
Î¸bn = Î¸bnâˆ’1 + Î³nÏˆ(Yn âˆ’ XT
n
Î¸bnâˆ’1)Xn, (24)
Î¸bâˆ—
n = Î¸bâˆ—
nâˆ’1 + Î³nWnÏˆ(Yn âˆ’ XT
n
Î¸bâˆ—
nâˆ’1
)Xn. (25)
If Ïˆ(Â·) is not differentiable, neither the final-step plug-in estimation (14) nor the recursive
plug-in estimation (15) is applicable. However, if the corresponding `(Î¸;Z) and L(Î¸) satisfy
Assumptions A1-A4, the perturbation resampling procedure is applicable. Next we consider
a special setting where the following Assumptions B1-B4 hold.
(B1). Assume that Ï(u) is a convex function on R with the right derivative being Ïˆ+(u) and
left derivative being Ïˆâˆ’(u). Let Ïˆ(u) be a function such that Ïˆâˆ’(u) â‰¤ Ïˆ(u) â‰¤ Ïˆ+(u).
There exists constant C1 > 0 such that |Ïˆ(u)| â‰¤ C1(1 + |u|).
(B2). Let Îµn = Yn âˆ’XT
n
Î¸0. Assume that (Xn, Îµn), n = 1, 2, ..., are i.i.d. copies of (X, Îµ), with
EkXk
4 < âˆ and EkÎµk
2 < âˆ. Let V0 = E{Ïˆ
2
(Îµ)XX T} > 0.
(B3). Let Ï†(u|X) = E{Ïˆ(u + Îµ)|X}. Assume that Ï†(0|X) = 0, uÏ†(u|X) > 0 for any u 6= 0,
and Ï†(u|X) has a derivative at u = 0 with Ï†Ë™(0|X) â‰¥ Ïƒ > 0 uniformly over X. Let
S0 = E{Ï†Ë™(0|X)XX T} > 0.
(B4). Assume that Ï†Ë™(u|X) is uniformly Lipschitz at u = 0. That is, there exist constants
C2 > 0 and Î´ > 0 such that


Ï†Ë™(u|X) âˆ’ Ï†Ë™(0|X)


 â‰¤ C2|u| for |u| â‰¤ Î´ uniformly over X.
We derive the asymptotic properties of the Ïˆ-type M-estimator as follows.
7
Yixin Fang, Jinfeng Xu and Lei Yang
Lemma 4 If Assumptions B1-B4 and A5 are satisfied, then we have
âˆš
n(Î¸n âˆ’ Î¸0) â‡’ N
0, Sâˆ’1
0 V0S
âˆ’1
0

, in distribution, as n â†’ âˆ. (26)
Theorem 5 If Assumptions B1-B4 and A5-A6 are satisfied, then we have
âˆš
n(Î¸
âˆ—
n âˆ’ Î¸0) = 1
âˆš
n
S
âˆ’1
0
Xn
i=1
WiÏˆ(Îµi)Xi + op(1). (27)
From Lemma 2 we see that the plug-in procedures are not applicable for estimating the
asymptotic covariance matrix, because although they are applicable for estimating V0, they
are not applicable for estimating S0, which involves Ï†Ë™(0|X). Moreover, by Theorem 3, we
can show that the Kolmogorow-Smirnov distance between âˆš
n(Î¸
âˆ—
n âˆ’ Î¸n) and âˆš
n(Î¸n âˆ’ Î¸0)
converges to zero in probability, as stated in Theorem 2. This validates our proposal of
the perturbation-based resampling procedure for robust regression. To further understand
Assumptions B1-B4, we examine the following example of quantile regression, which is a
special case of the above robust regression.
Example 4 (Quantile regression). Assume the Ï„ -quantile of Y given X is XT
Î¸0, with
Î¸0 = arg min
Î¸âˆˆRp
EÏÏ„ (Y âˆ’ XT
Î¸), (28)
where ÏÏ„ (u) = u(Ï„ âˆ’ I(u < 0)) with a given 0 < Ï„ < 1. Let Îµ = Y âˆ’ XT
Î¸0 and ÏˆÏ„ (u) =
Ï„ âˆ’ I(u < 0). Thus E{ÏˆÏ„ (Îµ)|X} = Ï„ âˆ’ FÎµ(0|X) = 0, where FÎµ(u|X) is the conditional
distribution function of Îµ. Let pÎµ(u|X) be the conditional density function of Îµ. Note
that Ï†(u|X) = E{ÏˆÏ„ (u + Îµ)|X} = Ï„ âˆ’ FÎµ(âˆ’u|X), Ï†Ë™(0|X) = pÎµ(0|X), V0 = E{Ïˆ
2
Ï„
(Îµ)XX T} =
Ï„ (1âˆ’Ï„ )E{XX T} and S0 = E{pÎµ(0|X)XX T}. Therefore, we can easily verify that if pÎµ(0|X)
is uniformly bounded away from 0 and pÎµ(u|X) is uniformly Lipschitz continuous at u = 0,
then Assumptions B1-B4 hold. Thus, the SGD and perturbed SGD updates for Î¸0, as
defined in (3) and (5) respectively, are
Î¸bn = Î¸bnâˆ’1 + Î³n
n
Ï„ âˆ’ I(Yn âˆ’ XT
n
Î¸bnâˆ’1 < 0)o
Xn, (29)
Î¸bâˆ—
n = Î¸bâˆ—
nâˆ’1 + Î³nWn
n
Ï„ âˆ’ I(Yn âˆ’ XT
n
Î¸bâˆ—
nâˆ’1 < 0)o
Xn, (30)
and the asymptotic results stated in Lemma 2 and Theorem 3 follow directly.
4. Numerical results
4.1. Simulation studies
To assess the performance of the proposed online bootstrap resampling procedure (a.k.a.
random weighting procedure; RW) for SGD estimators, we conduct simulation studies for
those four examples discussed in Section 3. We compare the proposed procedure with the
recursive plug-in procedure (RPI).
Setting 1 (Linear regression): Consider linear regression (16), where covariates X(j)
,
j = 1, . . . , p, and residual Îµ = Y âˆ’XT
Î¸0, are independently generated from standard normal
 
Online Bootstrap for SGD
N(0, 1). Here X(j)
indicates the j-th dimension of X. Let Î¸0 = (Âµ1
T
q/2
, âˆ’Âµ1
T
q/2
, 0
T
pâˆ’q
)
T
(same
for the other three settings). Consider the corresponding SGD estimators (17) and (18).
Setting 2 (Logistic regression): Consider logistic regression (19), where covariates
X(j) are independently from N(0, 1) and response Y from Bernoulli distribution with
logit{P(Y = 1|X)} = XT
Î¸0. Consider the corresponding SGD estimators (20) and (21).
Setting 3 (LAD regression): Consider least absolute deviation (LAD) regression, which
is a special case of robust regression (22) with Ï(x) = |x| and quantile regression (28) with
Ï„ = 0.5, where covariates X(j)
, j = 1, . . . , p, are i.i.d. with N(0, 1) and residual Îµ, defined
as Y âˆ’XT
Î¸0, is independently from double exponential distribution DE(0, 1). Consider the
corresponding SGD estimators (29) and (30) with Ï„ = 0.5.
Setting 4 (LAD regression for data with outliers): Consider LAD regression for the
data generated from Setting 1 but contaminated with 10% outliers. The contaminated data
are obtained by transforming the outcome variable in the data generated from Setting 1
using Y â† Y + 10 if |X(1)| â‰¥ 1.96 and |X(2)| < 1.96; and Y â† Y âˆ’ 10 if |X(1)| < 1.96
and |X(2)| â‰¥ 1.96. In this setting, covariate vector X and residual Îµ = Y âˆ’ XT
Î¸0 are not
independent, but median{Îµ|X} = 0. Consider the corresponding SGD estimators defined
in (29) and (30) with Ï„ = 0.5.
For each simulation setting, we consider four scenarios, as described by (N, p, q, Âµ),
where sample size N = 10, 000 or 20, 000, number of covariates p = 10 or 20, number of
informative covariates q = 6, and effect size Âµ = 0.1 or 0.2. For each example, we repeat
the data generation 1000 times. For each data repetition, we use Wnb âˆ¼ exp(1) as random
weights and generate B = 200 copies of random weights whenever a new data point is read.
Then, for each data repetition, we obtain the SGD estimate (4), and apply the following
procedures to construct 95% confidence intervals: the proposed random weighting procedure
to obtain 2.5% and 97.5% quantile (RW-Q), the proposed random weighting procedure to
estimate its standard error (RW-Ïƒ) and then construct â€œestimate Â± 1.96 Ã— SEâ€, and the c
recursive plug-in procedures (RPI), if applicable, to estimate its standard error. We consider
the learning rate Î± = 2/3. When we calculate the average SGD estimators (4) and (6), the
first 2000 and 4000 estimates are excluded for N = 10, 000 and N = 20, 000, respectively.
We also obtain the empirical standard error based on 1000 repeated SGD estimates, as a
benchmark approximation to the true standard error.
The coverage probabilities of the 95% confidence interval estimates are summarized in
Table 1 for linear regression (Setting 1), Table 2 for logistic regression (Setting 2) and Table
3 for LAD regression (Settings 3-4), respectively. We only report results corresponding to
the first, fourth and seventh covariates (that is, X(1)
, X(q/2+1) and X(q+1)). The plug-in
procedures are not applicable for LAD regression in Settings 3-4.
From Tables 1 and 2, we see that, for linear regression and logistic regression, the
coverage probabilities using the RW-Q, RW-Ïƒ and RPI are close to 95%. Therefore, the
proposed random weighting procedures (both RW-Q and RW-Ïƒ) perform well for linear
regression and logistic regression, and if we choose to use the plug-in procedure, which
involves matrix inverse, we can use RPI. Since the point estimate is PN
i=N0+1 Î¸bi/(N âˆ’ N0),
where N0 is the number of excluded estimates and which involves a pass of SGD estimates,
its standard error should also involve a pass of SGD estimates, instead of involving only
the final-step estimate or the true parameter value. We can understand this clearer if
we see the â€œSEâ€ column, where the standard errors from RW-Ïƒ and RPI are close to the
9
Yixin Fang, Jinfeng Xu and Lei Yang
empirical standard error. Moveover, from Table 3, we see that, for LAD regression and for
both Settings 3 and 4, the coverage probabilities using either of the two proposed random
weighting procedures are close to 95%, and the standard errors using RW-Ïƒ are close to the
empirical standard errors.
Table 1: Coverage probabilities of 95% confidence intervals for linear regression
(N, p, q, Âµ) Method Dim 1 Dim q/2 + 1 Dim q + 1
Cover SE Cover SE Cover SE
(10000,10,6,0.1) RW-Q 0.947 âˆ’ 0.946 âˆ’ 0.950 âˆ’
RW-Ïƒ 0.956 0.012 0.950 0.012 0.959 0.012
RPI 0.953 0.011 0.946 0.011 0.956 0.011
Empirical âˆ’ 0.011 âˆ’ 0.011 âˆ’ 0.011
(10000,10,6,0.2) RW-Q 0.947 âˆ’ 0.946 âˆ’ 0.950 âˆ’
RW-Ïƒ 0.956 0.012 0.950 0.012 0.959 0.012
RPI 0.953 0.011 0.946 0.011 0.956 0.011
Empirical âˆ’ 0.011 âˆ’ 0.011 âˆ’ 0.011
(20000,20,6,0.1) RW-Q 0.939 âˆ’ 0.951 âˆ’ 0.945 âˆ’
RW-Ïƒ 0.949 0.008 0.953 0.008 0.960 0.008
RPI 0.956 0.008 0.948 0.008 0.957 0.008
Empirical âˆ’ 0.008 âˆ’ 0.008 âˆ’ 0.008
(20000,20,6,0.2) RW-Q 0.939 âˆ’ 0.951 âˆ’ 0.945 âˆ’
RW-Ïƒ 0.949 0.008 0.953 0.008 0.960 0.008
RPI 0.956 0.008 0.948 0.008 0.957 0.008
Empirical âˆ’ 0.008 âˆ’ 0.008 âˆ’ 0.008
4.2. Real data applications
In this section, we apply the proposed procedures to conduct inference for linear regression
analysis for the individual household electric power consumption data (POWER) and logistic regression analysis for the skin segmentation dataset (SKIN) and gas sensors for the
home Activity monitoring data (GAS). All the three datasets are publicly available on UCI
machine learning repository.
The POWER dataset contains 2,075,259 observations and we fit a linear model to investigate how the time of a day influences the response variable â€œsub-metering-1â€, the energy
sub-metering No. 1, in watt-hour of active energy corresponding to kitchen. The observations with missing value are deleted, and the time of a day is divided into 8 categories,
â€œTime 0-2â€, â€œTime 3-5â€, ..., and â€œTime 21-23â€. The SKIN dataset contains 245,057 observations, out of which 50,859 is the skin samples and 194,198 is non-skin samples. We
fit a logistic model to examine the relationship between the indicator of skin and three
predictors, B, G and R. The GAS dataset contains 919,438 observations and we only use
a subset containing 652,024 observations with response variable being either â€œbananaâ€ or
â€œwineâ€. We fit a logistic model to examine the association between the response variable
and 11 explanatory variables, Time, R1 to R8, Temperature and Humidity.
10
Online Bootstrap for SGD
Table 2: Coverage probabilities of 95% confidence intervals for logistic regression
(N, p, q, Âµ) Method Dim 1 Dim q/2 + 1 Dim q + 1
Cover SE Cover SE Cover SE
(10000,10,6,0.1) RW-Q 0.950 âˆ’ 0.948 âˆ’ 0.928 âˆ’
RW-Ïƒ 0.954 0.024 0.954 0.024 0.948 0.023
RPI 0.954 0.023 0.952 0.023 0.942 0.023
Empirical âˆ’ 0.022 âˆ’ 0.022 âˆ’ 0.023
(10000,10,6,0.2) RW-Q 0.945 âˆ’ 0.945 âˆ’ 0.944 âˆ’
RW-Ïƒ 0.959 0.025 0.954 0.025 0.954 0.024
RPI 0.955 0.023 0.952 0.023 0.948 0.023
Empirical âˆ’ 0.023 âˆ’ 0.023 âˆ’ 0.023
(20000,20,6,0.1) RW-Q 0.944 âˆ’ 0.939 âˆ’ 0.934 âˆ’
RW-Ïƒ 0.951 0.016 0.953 0.016 0.952 0.016
RPI 0.948 0.016 0.953 0.016 0.949 0.016
Empirical âˆ’ 0.016 âˆ’ 0.016 âˆ’ 0.016
(20000,20,6,0.2) RW-Q 0.946 âˆ’ 0.941 âˆ’ 0.939 âˆ’
RW-Ïƒ 0.958 0.017 0.952 0.017 0.957 0.016
RPI 0.950 0.016 0.944 0.016 0.952 0.015
Empirical âˆ’ 0.016 âˆ’ 0.016 âˆ’ 0.016
Although standard softwares such as SAS and R can fit linear and logistic regression to
such datasets without difficulty, for the illustration purpose, we use the SGD as in Examples
1 and 2 to fit linear and logistic regression and use the proposed online boostrap procedure
to construct confidence intervals. The point estimates and the 95% confidence intervals of
the coefficients are showed in Table 4. From the left-top panel of Table 4, we see that the
electronic power consumption from kitchen is relatively high in the evening and night. From
the left-bottom panel of Table 4, we see that variable B is positively associated with the
response while the other two variables G and R are negatively associated. From the right
panel of Table 4, we see that all the variables but R4 are statistical significantly associated
with the response. Furthermore, we display the histogram of B = 1000 perturbation-based
SGD estimates for each coefficient in Figures 4.2-4.2 for the POWER data, the SKIN data
and the GAS data, respectively. The blue triangle in each figure indicates the corresponding
point estimate the red triangles indicate 2.5 and 97.5 quantiles. From these figures, we see
the the perturbation-based procedure can be used to approximate the sampling distribution
of the corresponding SGD estimator, which might be skewed. For example, for the GAS
data, the sampling distributions are very skewed, and therefore the proposed resampling
procedure is able to display such skewness.
5. Discussion
Online updating is a useful strategy for analyzing big data and streaming data, and recently stochastic gradient decent has become a popular method for doing online updating.
Although the asymptotic properties of SGD have been well studied, there is little research
11
Yixin Fang, Jinfeng Xu and Lei Yang
Table 3: Coverage probabilities of 95% confidence intervals for LAD regression
(N, p, q, Âµ) Method Dim 1 Dim q/2 + 1 Dim q + 1
Cover SE Cover SE Cover SE
Simulation Setting 3
(10000,10,6,0.1) RW-Q 0.965 âˆ’ 0.965 âˆ’ 0.969 âˆ’
RW-Ïƒ 0.969 0.029 0.965 0.029 0.965 0.029
Empirical âˆ’ 0.026 âˆ’ 0.027 âˆ’ 0.026
(10000,10,6,0.2) RW-Q 0.972 âˆ’ 0.965 âˆ’ 0.967 âˆ’
RW-Ïƒ 0.973 0.029 0.966 0.029 0.968 0.029
Empirical âˆ’ 0.026 âˆ’ 0.027 âˆ’ 0.026
(20000,20,6,0.1) RW-Q 0.970 âˆ’ 0.973 âˆ’ 0.966 âˆ’
RW-Ïƒ 0.966 0.010 0.969 0.010 0.965 0.010
Empirical âˆ’ 0.009 âˆ’ 0.009 âˆ’ 0.009
(20000,20,6,0.2) RW-Q 0.967 âˆ’ 0.974 âˆ’ 0.966 âˆ’
RW-Ïƒ 0.966 0.010 0.969 0.010 0.969 0.010
Empirical âˆ’ 0.009 âˆ’ 0.009 âˆ’ 0.009
Simulation Setting 4
(10000,10,6,0.1) RW-Q 0.954 âˆ’ 0.971 âˆ’ 0.950 âˆ’
RW-Ïƒ 0.958 0.035 0.969 0.035 0.960 0.035
Empirical âˆ’ 0.032 âˆ’ 0.031 âˆ’ 0.033
(10000,10,6,0.2) RW-Q 0.960 âˆ’ 0.966 âˆ’ 0.955 âˆ’
RW-Ïƒ 0.958 0.035 0.966 0.035 0.956 0.035
Empirical âˆ’ 0.032 âˆ’ 0.031 âˆ’ 0.033
(20000,20,6,0.1) RW-Q 0.948 âˆ’ 0.953 âˆ’ 0.965 âˆ’
RW-Ïƒ 0.963 0.047 0.958 0.047 0.964 0.047
Empirical âˆ’ 0.043 âˆ’ 0.045 âˆ’ 0.044
(20000,20,6,0.2) RW-Q 0.944 âˆ’ 0.957 âˆ’ 0.961 âˆ’
RW-Ïƒ 0.961 0.047 0.961 0.047 0.961 0.047
Empirical âˆ’ 0.044 âˆ’ 0.045 âˆ’ 0.044
on conducting statistical inference based on SGD estimators. In this paper, we propose
the perturbation-based resampling procedure, which can be applied to estimate the sampling distribution of an SGD estimator. The offline version of perturbation-based resamping
procedure was first proposed by Rubin (1981) and was also discussed in Shao and Tu (2012).
The proposed resampling procedure is in essence an online version of the bootstrap.
Recall that the data points, Z1, Z2, . . . , ZN , are arriving one at a time and an SGD estimate updates itself from Î¸bnâˆ’1 to Î¸bn whenever a new data point Zn arrives. If we
are forced to apply the bootstrap, then we should have many bootstrap samples; the
data points of each bootstrap sample, Z
âˆ—
1
, Zâˆ—
2
, . . . , Zâˆ—
N , are assumed to be arriving one at
a time and the bootstrapped SGD estimate updates itself from Î¸bâˆ—
nâˆ’1
to Î¸bâˆ—
n whenever a
new data point Z
âˆ—
n arrives. Of course such bootstrap is impractical here because in online updating we will not obtain and store all the data points and then generate bootstrap samples. Now if we rearrange hypothetical bootstrap sample Z
âˆ—
1
, Zâˆ—
2
, . . . , Zâˆ—
N as
12
Online Bootstrap for SGD
Table 4: Point estimates and 95% confidence intervals for three real datasets; POWER data
on the left-top panel, SKIN data on the left-bottom panel and GAS data on the
right panel
Variable Est. 95% CI Variable Est. 95% CI
Time 0-2 2.265 (2.254, 2.275) Time âˆ’0.158 (âˆ’0.178, âˆ’0.139)
Time 3-5 2.045 (2.040, 2.049) R1 âˆ’0.202 (âˆ’0.215, âˆ’0.190)
Time 6-8 2.623 (2.608, 2.639) R2 0.176 (0.160, 0.191)
Time 9-11 3.323 (3.298, 3.347) R3 âˆ’0.907 (âˆ’0.932, âˆ’0.882)
Time 12-14 3.445 (3.420, 3.470) R4 âˆ’0.007 (âˆ’0.018, 0.004)
Time 15-17 3.059 (3.037, 3.082) R5 âˆ’0.450 (âˆ’0.467, âˆ’0.432)
Time 18-20 4.176 (4.143, 4.208) R6 1.772 (1.759, 1.785)
Time 21-23 4.053 (4.024, 4.082) R7 0.173 (0.139, 0.207)
B 1.501 (1.441, 1.569) R8 0.302 (0.272, 0.332)
G âˆ’0.242 (âˆ’0.319, âˆ’0.166) Temp. âˆ’0.175 (âˆ’0.191, âˆ’0.160)
R âˆ’1.956 (âˆ’1.999, âˆ’1.918) Humi. âˆ’0.551 (âˆ’0.560, âˆ’0.542)
Figure 1: Histograms of B = 1000 perturbation-based SGD estimates for the POWER data.
{K1 copies Z1, K2 copies Z2, . . . , KN copies ZN }, where Kn follows binomial distribution
B(N, 1/N), then the SGD estimator updates itself from Î¸bâˆ—
nâˆ’1
to Î¸bâˆ—
n whenever a new batch of
data points, Kn copies of Zn, arrives. Noting that binomial distribution B(N, 1/N) approximates to Poisson distribution P(1) as N â†’ âˆ, we see that the aforementioned hypothetical
13
Yixin Fang, Jinfeng Xu and Lei Yang
Figure 2: Histograms of B = 1000 perturbation-based SGD estimates for the SKIN data.
Figure 3: Histograms of B = 1000 perturbation-based SGD estimates for the GAS data.
bootstrap is equivalent to our proposed online bootstrap procedure with Wn âˆ¼ P(1), whose
mean and variance are both equal to one.
Finally, the SGD method considered in this paper is actually the explicit SGD, in contrast with the implicit SGD considered in Toulis et al (2017). We are working on extending
14
Online Bootstrap for SGD
the proposed perturbation-based resampling procedure for conducting statistical inference
for the implicit SGD.
Appendix A. technical proofs
For ease exposition of establishing asymptotic normality of SGD and perturbed SGD estimates, we present the following Proposition 1, adapted from Theorem 2 of Polyak and
Juditsky (1992; pp.841). Let R(Î¸) : Rp â†’ Rp be some unknown function and R(Î¸0) = 0.
The dataset consists of Zn, n = 1, 2, . . . , which are i.i.d. copies of Z. Stochastic gradients
are Rb(Î¸;Zi) and E{Rb(Î¸;Zi)} = R(Î¸). With an initial point Î¸b0 and learning rates Î³n, the
SGD estimate is defined as
Î¸bn = Î¸bnâˆ’1 âˆ’ Î³nRb(Î¸bnâˆ’1;Zn) = Î¸bnâˆ’1 âˆ’ Î³n

R(Î¸bnâˆ’1) âˆ’ Dn

, (31)
where Dn = R(Î¸bnâˆ’1)âˆ’Rb(Î¸bnâˆ’1;Zn) is a martingale-difference process; that is, E{Dn|Fnâˆ’1} =
0, where Fnâˆ’1 = Ïƒ(Dnâˆ’1). The regularity conditions for Proposition 1 are listed as follows.
(C1). There exists a function U(Î¸) : Rp â†’ R such that for some Î» > 0, Î´ > 0, l0 > 0, L0 > 0,
and all Î¸, Î¸0 âˆˆ Rp
, the conditions U(Î¸) â‰¥ Î»kÎ¸k
2
, kâˆ‡U(Î¸) âˆ’ âˆ‡U(Î¸
0
)k â‰¤ L0kÎ¸ âˆ’ Î¸
0k,
U(0) = 0, âˆ‡U(Î¸ âˆ’ Î¸0)
TR(Î¸) > 0 for Î¸ 6= Î¸0 hold true. Moreover, âˆ‡U(Î¸ âˆ’ Î¸0)
TR(Î¸) â‰¥
l0U(Î¸ âˆ’ Î¸0) for all kÎ¸ âˆ’ Î¸0k â‰¤ Î´.
(C2). There exists a positive definite matrix S0 âˆˆ RpÃ—p
such that for some C > 0, 0 < % â‰¤ 1,
and Î´ > 0, the condition kR(Î¸)âˆ’S0(Î¸ âˆ’Î¸0)k â‰¤ CkÎ¸ âˆ’Î¸0k
1+%
for all kÎ¸ âˆ’Î¸0k â‰¤ Î´ holds
true.
(C3). {Dn}nâ‰¥1 is a martingale difference process with E{Dn|Fnâˆ’1} = 0, and for some C > 0,
E

kDnk
2
| Fnâˆ’1
	
+ kR(Î¸bnâˆ’1)k
2 â‰¤ C

1 + kÎ¸bnâˆ’1k
2

a.s.,
for all n â‰¥ 1. Consider decomposition Dn = Dn(0) + En(Î¸bnâˆ’1), where Dn(0) =
R(Î¸0)âˆ’Rb(Î¸0;Zn) and En(Î¸bnâˆ’1) = Dn âˆ’Dn(0). Assume that E{Dn(0)|Fnâˆ’1} = 0 a.s.,
E{Dn(0)Dn(0)T
|Fnâˆ’1}
P
â†’ V0 > 0,
supnâ‰¥1 E

kDn(0)k
2
I(kDn(0)k > Î·)|Fnâˆ’1
	 P
â†’ 0, as Î· â†’ âˆ,
and there exists Î´(x) â†’ 0 as x â†’ 0 such that, for all n large enough,
E
n
kEn(Î¸bnâˆ’1)k
2
| Fnâˆ’1
o
â‰¤ Î´(kÎ¸bnâˆ’1 âˆ’ Î¸0k) a.s..
(C4). It holds that (Î³n âˆ’ Î³n+1)/Î³n = o(Î³n), Î³n > 0 for all n, and Pâˆ
n=1 Î³
(1+%)/2
n n
âˆ’1/2 < âˆ.
Assumptions C2 and C4 are implied by the following Assumptions C20
(i.e. Assumption
C2 with % = 1) and C40
(i.e. Assumption A5):
(C20
). There exists a positive definite matrix S âˆˆ RpÃ—p
such that for some C > 0 and Î´ > 0,
the condition kR(Î¸) âˆ’ S0(Î¸ âˆ’ Î¸0)k â‰¤ CkÎ¸ âˆ’ Î¸0k
2
for all kÎ¸ âˆ’ Î¸0k â‰¤ Î´ holds true.
15
Yixin Fang, Jinfeng Xu and Lei Yang
(C40
). The learning rates are chosen as Î³n = Î³1n
âˆ’Î± with Î³1 > 0 and Î± âˆˆ (0.5, 1).
Proposition 1. If Assumptions C1-C4 are satisfied, then we have Î¸n â†’ Î¸0, a.s.; and
âˆš
n(Î¸n âˆ’ Î¸0) = 1
âˆš
n
S
âˆ’1Xn
i=1
Di + op(1), (32)
which implies âˆš
n(Î¸n âˆ’ Î¸0) â‡’ N
0, Sâˆ’1
0 V0S
âˆ’1
0

, in distribution.
A1. Proof of Lemma 1
Proof. By Proposition 1, it suffices to show that Assumptions C1-C4 hold under Assumptions A1-A5. Because C2 and C4 are implied by C20 and C40
, it suffices to show that
Assumptions C1, C20
, C3 and C40 hold under Assumptions A1-A5.
Verification of Assumption C1: Recall that R(Î¸) = âˆ‡L(Î¸) and Rb(Î¸;Zi) = âˆ‡`(Î¸;Zi).
Define U(Î¸) = L(Î¸0 + Î¸) âˆ’ L(Î¸0) + Î»kÎ¸k
2
for a given Î» > 0. By definition of U(Î¸) and
Assumption A1, we have U(Î¸) â‰¥ Î»kÎ¸k
2 and U(0) = 0. For any Î¸ and Î¸
0
, since âˆ‡U(Î¸) âˆ’
âˆ‡U(Î¸
0
) = R(Î¸+Î¸0)âˆ’R(Î¸
0+Î¸0)+2Î»(Î¸âˆ’Î¸
0
), letting L0 = L1+2Î» and by Assumption A2, we
have kâˆ‡U(Î¸)âˆ’âˆ‡U(Î¸
0
)k â‰¤ L1kÎ¸âˆ’Î¸
0k. Since âˆ‡U(Î¸âˆ’Î¸0)
TR(Î¸) = kR(Î¸)k
2+Î»(Î¸âˆ’Î¸0)
TR(Î¸), by
Assumption A1, we have âˆ‡U(Î¸âˆ’Î¸0)
TR(Î¸) > 0 for any Î¸ 6= Î¸0. Last, it remains to verify there
exist l0 > 0 and Î´ > 0 such that âˆ‡U(Î¸ âˆ’Î¸0)
TR(Î¸) â‰¥ l0U(Î¸ âˆ’Î¸0) for all kÎ¸ âˆ’Î¸0k â‰¤ Î´. Noting
that U(Î¸âˆ’Î¸0) = L(Î¸+Î¸0)+Î»kÎ¸âˆ’Î¸0k
2
, by Taylor expansion and Assumption A3, we see that
there exist l1 > 0 and Î´1 such that U(Î¸âˆ’Î¸0) â‰¤ l1kÎ¸âˆ’Î¸0k
2
for all kÎ¸âˆ’Î¸0k â‰¤ Î´1. On the other
hand, noting that âˆ‡U(Î¸âˆ’Î¸0)
TR(Î¸) = kR(Î¸)k
2+Î»(Î¸âˆ’Î¸0)
TR(Î¸), by Assumption A3 also, we
see that there exist l2 > 0 and Î´2 such that (Î¸ âˆ’Î¸0)
TR(Î¸) â‰¥ l2kÎ¸ âˆ’Î¸0k
2
for all kÎ¸ âˆ’Î¸0k â‰¤ Î´2.
Selecting Î´ = min(Î´1, Î´2) and l0 = Î»l2/l2, we show that âˆ‡U(Î¸ âˆ’ Î¸0)
TR(Î¸) â‰¥ l0U(Î¸ âˆ’ Î¸0) for
all kÎ¸ âˆ’ Î¸0k â‰¤ Î´.
Verification of Assumption C20
: Recall that R(Î¸) = âˆ‡L(Î¸), S(Î¸) = âˆ‡R(Î¸), and S0 =
S(Î¸0) > 0. By Assumption A3, there exists Î´ > 0 such that for any kÎ¸ âˆ’ Î¸0k < Î´0
,
kS(Î¸) âˆ’ S(Î¸0)k < L2kÎ¸ âˆ’ Î¸0k. By mean-value theorem, kS(Î¸) âˆ’ S0(Î¸ âˆ’ Î¸0)k = kS(Î¸e)(Î¸ âˆ’
Î¸0) âˆ’ S0(Î¸ âˆ’ Î¸0)k, where Î¸e lies between Î¸ and Î¸0. Hence kS(Î¸) âˆ’ S0(Î¸ âˆ’ Î¸0)k â‰¤ L2kÎ¸ âˆ’ Î¸0k
2
,
for any kÎ¸ âˆ’ Î¸0k â‰¤ Î´. Letting C = L2, we have verified Assumption C20
.
Verification of Assumption C3: Dn = R(Î¸bnâˆ’1) âˆ’ Rb(Î¸bnâˆ’1;Zn). Consider decomposition Dn = Dn(0) + En(Î¸bnâˆ’1), where Dn(0) = âˆ’âˆ‡`(Î¸0, Zn) and En(Î¸bnâˆ’1) = [R(Î¸bnâˆ’1) âˆ’
R(Î¸0)] âˆ’ [âˆ‡`(Î¸bnâˆ’1;Zn) âˆ’ âˆ‡`(Î¸0;Zn)]. By Assumption A2, kR(Î¸bnâˆ’1)k
2 â‰¤ L
2
1
kÎ¸bnâˆ’1 âˆ’ Î¸0k
2
.
In addition, Cauchy-Schwartz inequality implies that E

kâˆ‡`(Î¸;Zn) âˆ’ âˆ‡`(Î¸0;Zn)k
2
	
=
2Ekâˆ‡`(Î¸;Zn)k
2 + 2Ekâˆ‡`(Î¸0;Zn)k
2
; we have E
n
kâˆ‡`(Î¸bnâˆ’1;Zn) âˆ’ âˆ‡`(Î¸0;Zn)k
2
|
o
â‰¤ C
0
(1 +
kÎ¸k
2
) for some C
0 > 0 by Assumption A4, . Together, we have E

kDnk
2
| Fnâˆ’1
	
+
kR(Î¸bnâˆ’1)k
2 â‰¤ C

1 + kÎ¸bnâˆ’1k
2

for some C > 0. Moreover, because Dn(0)â€™s are i.i.d., we
have E{Dn(0)Dn(0)T
|Fnâˆ’1} = V0 > 0 and supnâ‰¥1 E

kDn(0)k
2
I(kDn(0)k > Î·)|Fnâˆ’1
	 P
â†’ 0,
as Î· â†’ âˆ. Finally, note that EkEn(Î¸)k
2 â‰¤ L
2
1
kÎ¸ âˆ’ Î¸0k
2 + Ekâˆ‡`(Î¸;Zn) âˆ’ âˆ‡`(Î¸0;Zn)k
2
. By
Assumption A3, Ekâˆ‡`(Î¸;Zn) âˆ’ âˆ‡`(Î¸0;Zn)k
2 â‰¤ Î´
0
(kÎ¸ âˆ’ Î¸0k) for some Î´
0
(Â·) with Î´
0
(x) â†’ 0 as
x â†’ 0. Define Î´(x) = L
2
1x
2 + Î´
0
(x), we show EkEn(Î¸)k
2 â‰¤ Î´(kÎ¸ âˆ’ Î¸0k). This complete the
verification of Assumption C3.
1 
Online Bootstrap for SGD
Obviously Assumption A5 is the same as Assumption C40
. Therefore, we have verified
that Assumptions A1-A5 imply Assumptions C1, C20
, C3 and C40
. By Proposition 1, we
complete the proof of Lemma 1.
A2. Proof of Theorem 1
Proof. Rewrite Î¸bâˆ—
n as
Î¸bâˆ—
n = Î¸bâˆ—
nâˆ’1 âˆ’ Î³nR(Î¸bâˆ—
nâˆ’1
) + Î³nDâˆ—
n
, (33)
where Dâˆ—
n = R(Î¸bâˆ—
nâˆ’1
) âˆ’ Wnâˆ‡`(Î¸bâˆ—
nâˆ’1
;Zn). Then let F
âˆ—
nâˆ’1 be the Borel field generated
by {(Zi
, Wi), i â‰¤ n âˆ’ 1}. Since E{Wn|F
âˆ—
nâˆ’1
} = 1 and R(Î¸) = E{âˆ‡`(Î¸;Zn)}, we have
E{Dâˆ—
n
|Fnâˆ’1} = 0. Thus Dâˆ—
n
is a martingale-difference process. Let Dâˆ—
n
(Î¸) = R(Î¸) âˆ’
Wnâˆ‡`(Î¸;Zn). Consider decomposition Dâˆ—
n = Dâˆ—
n
(0) + Eâˆ—
n
(Î¸bâˆ—
nâˆ’1
), where
Dâˆ—
n
(0) = âˆ’Wnâˆ‡`(Î¸0;Zn) (34)
and
E
âˆ—
n
(Î¸) = [R(Î¸) âˆ’ R(Î¸0)] âˆ’ Wn[âˆ‡`(Î¸;Zn) âˆ’ âˆ‡`(Î¸0;Zn)]. (35)
Noting that E{Dâˆ—
n
(0)} = 0 and E(W2
n
) = 2 under Assumption A6, by Assumption A4, we
have
E{[Dâˆ—
n
(0)][Dâˆ—
n
(0)]T
} = 2E {[`(Î¸0;Zn)][`(Î¸0;Zn)]T
} = 2V0. (36)
By Cauchy-Schwartz inequality and Assumptions A2 and A4, we have
E{kE
âˆ—
n
(Î¸)k
2
} â‰¤ 2kR(Î¸)k
2 + 4E

kâˆ‡`(Î¸, Z) âˆ’ âˆ‡`(Î¸0, Z)k
2
	
â‰¤ Î´
0
(kÎ¸ âˆ’ Î¸0k), (37)
where Î´
0
(x) = 2L
2
1x
2 + 2Î´(x) satisfying that Î´
0
(x) â†’ 0 as x â†’ 0. Also by Cauchy-Schwartz
inequality, E{kEâˆ—
n
(Î¸)k
2} â‰¤ 2kR(Î¸)k
2 + 2E{kâˆ‡`(Î¸, Z)k
2}. Further, by Assumptions A2 and
A4,
E{kDâˆ—
nk
2
|Fnâˆ’1}+kR(Î¸bâˆ—
nâˆ’1
)k
2 â‰¤ 3L
2
1kÎ¸bâˆ—
nâˆ’1âˆ’Î¸0k
2+2CkÎ¸bâˆ—
nâˆ’1k
2 â‰¤ C
0
(1+kÎ¸bâˆ—
nâˆ’1âˆ’Î¸0k
2
), (38)
for some large enough C
0 > 0. Combining results (36)-(38), we have verified Assumption
C3. Moreover, noting that EWn = 1, we can easily verify that Assumptions A1 and A2
imply that Assumption C1 holds, and Assumption A3 implies that Assumption C2 holds.
By Proposition 1, we show that Î¸bâˆ—
n â†’ Î¸0 almost surely, and
âˆš
n(Î¸
âˆ—
n âˆ’ Î¸0) = 1
âˆš
n
S
âˆ’1
0
Xn
i=1
Dâˆ—
i + op(1)
= âˆ’
1
âˆš
n
S
âˆ’1
0
Xn
i=1
Wiâˆ‡`(Î¸0;Zi) + 1
âˆš
n
S
âˆ’1
0
Xn
i=1
E
âˆ—
n
(Î¸bâˆ—
nâˆ’1
) + op(1). (39)
Note that E{kEn(Î¸bâˆ—
nâˆ’1
)k
2
|Fnâˆ’1} â‰¤ Î´
0
(kÎ¸bâˆ—
nâˆ’1 âˆ’ Î¸0k), following (37). Since Î¸bâˆ—
n â†’ Î¸0 a.s., we
have Î´(kÎ¸bâˆ—
nâˆ’1 âˆ’ Î¸0k) â†’ 0 a.s.. Thus, S
âˆ’1
0
Pn
i=1 En(Î¸bâˆ—
nâˆ’1
)/
âˆš
n = op(1). Therefore, by (39),
this completes the proof of Theorem 1.
17  
Yixin Fang, Jinfeng Xu and Lei Yang
A3. Proof of Theorem 2
Proof. Let
Vn = âˆ’
1
âˆš
n
S
âˆ’1
0
(Wi âˆ’ 1)âˆ‡`(Î¸0, Zi). (40)
By Theorem 1, we have âˆš
n(Î¸
âˆ—
n âˆ’ Î¸n) = Vn + op(1). We first show that, for any Î² âˆˆ B ,
{Î² âˆˆ Rp
: kÎ²k = 1} and u âˆˆ R,
P
âˆ—
(Î²
TVn â‰¤ u) â†’ Î¦(u/ÏƒÎ²), in probability, (41)
where Î¦(u) is the distribution of N (0, 1) and ÏƒÎ² = Î²
TS
âˆ’1
0 V0S
âˆ’1
0
Î². In fact,
Î²
TVn =
1
âˆš
n
Xn
i=1
(Wi âˆ’ 1)Î¾i
, (42)
where Î¾i = âˆ’Î²
TS
âˆ’1
0 âˆ‡`(Î¸0, Zi). Note that, by Assumption A6, EWi = Var(Wi) = 1. Hence
sn =
1
n
Xn
i=1
Varâˆ—
{(Wi âˆ’ 1)Î¾i} =
1
n
Xn
i=1
Î¾
2
i â†’ Ïƒ
2
Î²
, in probability, (43)
and for any  > 0,
1
nsn
Xn
i=1
E
âˆ—

(Wi âˆ’ 1)2
Î¾
2
i
I

|(Wi âˆ’ 1)Î¾i
| >
âˆš
nsn
	 â†’ 0, in probability. (44)
Therefore, the Lindebergâ€™s condition is satisfied. By the central limit theorem, (41) holds,
which implies that for any Î² âˆˆ B,
sup
uâˆˆR
|P
âˆ—
(Î²
TVn â‰¤ u) âˆ’ Î¦(u/ÏƒÎ²)| â†’ 0, in probability. (45)
Consider B0 , {Î² âˆˆ Rp
: kÎ²k = 1,the components of Î² are rational}, which contains only
countable many Î² and is a dense subset of B. For any subsequence {n1}, by Cantorâ€™s â€œdiagonal methodâ€ used in Rao and Zhao (1992), we can show that there exists a subsequence
{n2} âŠ‚ {n1} such that, with probability one,
sup
uâˆˆR
|P
âˆ—
(Î²
TVnâˆ’1 â‰¤ u) âˆ’ Î¦(u/ÏƒÎ²)| â†’ 0, for any Î² âˆˆ B0. (46)
Hence, we show that
sup
vâˆˆRp



P
âˆ—
âˆš
n(Î¸
âˆ—
n âˆ’ Î¸n) â‰¤ v

âˆ’ P(Î¶ â‰¤ v)



â†’ 0, in probability, (47)
where Î¶ âˆ¼ N (0, Sâˆ’1
0 V0S
âˆ’1
0
). By Lemma 1, we can also show that
sup
vâˆˆRp

P
âˆš
n(Î¸n âˆ’ Î¸0) â‰¤ v

âˆ’ P(Î¶ â‰¤ v)

 â†’ 0. (48)
Combining (47) and (48), we complete the proof of Theorem 2.
1    
Online Bootstrap for SGD
A4. Proof of Lemma 2
Proof. By Proposition 1, it suffices to show that Assumptions C1, C20 and C3 hold under
Assumptions B1-B4.
Verification of Assumption C1: Define âˆ† = Î¸ âˆ’ Î¸0. Let R(âˆ†) = E{Ï†(âˆ†TX|X)X} and
Rb(âˆ†;Zn) = Ïˆ(âˆ†TXn+Îµn)Xn. Define U(âˆ†) = âˆ†Tâˆ†. By definition of U(âˆ†), U(âˆ†) â‰¥ Î»kâˆ†k
2
with Î» = 1, U(0) = 0 and âˆ‡U(âˆ†) is Lipschitz continuous. Since âˆ†TE{Ï†(âˆ†TX|X)âˆ†TX} > 0
by Assumption B3, we see that âˆ‡U(âˆ†)TR(âˆ†) > 0. By the mean-value theorem and by Assumption B3, there exists Î´ such that âˆ†TE{Ï†(âˆ†TX|X)âˆ†TX} â‰¥ âˆ†TE{Ï†Ë™(0|X)XX T}âˆ†/2 â‰¥
Î»min(S0)kâˆ†k
2/2, for any kâˆ†k â‰¤ Î´, where Î»min(S0) is the minimum eigenvalue of S0. Hence
âˆ‡U(âˆ†)TR(âˆ†) â‰¥ Î»min(S0)kâˆ†k
2 and we have verified Assumption C1.
Verification of Assumption C20
: Note that
kR(âˆ†) âˆ’ S0âˆ†k = kEÏ†(âˆ†TX|X) âˆ’ EÏ†Ë™(0|X)XX Tâˆ†k.
By the mean-value theorem and Assumption B4, there exists Î´ such that, for any kâˆ†k â‰¤ Î´,
kE{Ï†(âˆ†TX|X)X} âˆ’E{Ï†Ë™(0|X)XX Tâˆ†}k â‰¤ C2Î»max(S0)kâˆ†k
2
. This implies Assumption C20
.
Verification of Assumption C3: Let âˆ†b n = Î¸bn âˆ’Î¸0. Then Dn = R(âˆ†b nâˆ’1)âˆ’Rb(âˆ†b nâˆ’1;Zn).
Consider decomposition Dn = Dn(0)+En(âˆ†b nâˆ’1), where Dn(0) = Ïˆ(Îµn)Xn and En(âˆ†b nâˆ’1) =
E{Ïˆ(âˆ†b T
nâˆ’1X+Îµn)Xn}âˆ’[Ïˆ(âˆ†b T
nâˆ’1X+Îµn)Xnâˆ’Ïˆ(Îµn)Xn]. By Assumption B1 and the CauchySchwartz inequality, we can show that E

kDnk
2
| Fnâˆ’1
	
+kR(âˆ†b nâˆ’1)k
2 â‰¤ 2C1

1 + kâˆ†b nâˆ’1k
2

.
Moreover, by Assumption B2, Dn(0)â€™s are i.i.d., so E{Dn(0)Dn(0)T
|Fnâˆ’1} = V0 > 0 and
supnâ‰¥1 E

kDn(0)k
2
I(kDn(0)k > Î·)|Fnâˆ’1
	
â†’0, as Î· â†’ âˆ. Finally, by Cauchy-Schwartz
inequality and Assumptions B2-B3, we can show that EkEn(âˆ†)k
2 â‰¤ Î´(kâˆ†k) for some Î´(Â·)
with Î´(x) â†’ 0 as x â†’ 0. This complete the verification of Assumption C3.
Obviously Assumption A5 is the same as Assumption C40
. Therefore, we have verified
that Assumptions B1-B4 and A5 imply Assumptions C1, C20
, C3 and C40
. By Proposition
1, we complete the proof of Lemma 2.
A5. Proof of Theorem 3
Proof. Recall that R(âˆ†) = E{Ï†(âˆ†TX|X)X} and âˆ†b n = Î¸bn âˆ’ Î¸0. Rewrite Î¸bâˆ—
n as
Î¸bâˆ—
n = Î¸bâˆ—
nâˆ’1 + Î³nR(âˆ†b n) + Î³nDâˆ—
n
, (49)
where Dâˆ—
n = WnÏˆ(âˆ†b T
nXn+Îµn)Xnâˆ’R(âˆ†b n) is a martingale-difference process by Assumption
B3 and that E{Wn|Fnâˆ’1} = 1. Let Dâˆ—
n
(âˆ†) = WnÏˆ (âˆ†TXn + Îµn) Xn âˆ’ R(âˆ†) and Dâˆ—
n
(âˆ†) =
Dâˆ—
n
(0) + Eâˆ—
n
(âˆ†), where
E
âˆ—
n
(âˆ†) = Wn [Ïˆ(âˆ†TXn + Îµn) âˆ’ Ïˆ(Îµn)] Xn âˆ’ R(âˆ†). (50)
Since Dâˆ—
n
(0) = WnÏˆ(Îµn)Xn, E{Dâˆ—
n
(0)} = 0 and E{[Dâˆ—
n
(0)][Dâˆ—
n
(0)]T} = (1 + Var(W1))V0
by Assumption B2. By Cauchy-Schwartz inequality and Assumptions B2-B3, we can show
that EkEâˆ—
n
(âˆ†)k
2 â‰¤ Î´(kâˆ†k) for some Î´(Â·) with Î´(x) â†’ 0 as x â†’ 0. Therefore, using the
similar arguments as those in the proof of Lemma 2, we can verify that, under Assumptions
B1-B4, Assumptions C1-C4 are satisfied. By Proposition 1, it follows that Î¸bâˆ—
n â†’ Î¸0 almost
19 
Yixin Fang, Jinfeng Xu and Lei Yang
surely, and
âˆš
n(Î¸
âˆ—
n âˆ’ Î¸0) = 1
âˆš
n
S
âˆ’1
0
Xn
i=1
Dâˆ—
i + op(1). (51)
By the decomposition of Dâˆ—
i
, we have
âˆš
n(Î¸
âˆ—
n âˆ’ Î¸0) = 1
âˆš
n
S
âˆ’1
0
Xn
i=1
WiÏˆ(Îµi)Xi +
1
âˆš
n
S
âˆ’1
0
Xn
i=1
E
âˆ—
n
(Î¸bâˆ—
nâˆ’1 âˆ’ Î¸0) + op(1). (52)
By the definition of Î´(kâˆ†k), E{kEâˆ—
n
(Î¸bâˆ—
nâˆ’1 âˆ’ âˆ†)k
2
2
|Fnâˆ’1} = Î´(kÎ¸bâˆ—
nâˆ’1 âˆ’ Î¸0k). Since Î¸bâˆ—
n â†’ Î¸0
a.s., we have Î´(kÎ¸bâˆ—
nâˆ’1 âˆ’ Î¸0k) â†’ 0 a.s.. Thus, Pn
i=1 Eâˆ—
n
(Î¸bâˆ—
nâˆ’1 âˆ’ Î¸0)/
âˆš
n = op(1). By (52), we
complete the proof of Theorem 3.
