In many applications involving large dataset or online learning, stochastic gradient descent
(SGD) is a scalable algorithm to compute parameter estimates and has gained increasing
popularity due to its numerical convenience and memory efficiency. While the asymptotic
properties of SGD-based estimators have been well established, statistical inference such
as interval estimation remains much unexplored. The classical bootstrap is not directly
applicable if the data are not stored in memory. The plug-in method is not applicable
when there is no explicit formula for the covariance matrix of the estimator. In this paper,
we propose an online bootstrap procedure for the estimation of confidence intervals, which,
upon the arrival of each observation, updates the SGD estimate as well as a number of
randomly perturbed SGD estimates. The proposed method is easy to implement in practice.
We establish its theoretical properties for a general class of models that includes linear
regressions, generalized linear models, M-estimators and quantile regressions as special
cases. The finite-sample performance and numerical utility is evaluated by simulation
studies and real data applications.
Keywords: Bootstrap, Interval estimation, Generalized linear models, Large datasets,
M-estimators, Quantile regression, Resampling methods, Stochastic gradient descent
1. Introduction
Big datasets arise frequently in clinical, epidemiological, financial and sociological studies.
In such applications, many classical optimization methods for parameter estimation such as
Fisher scoring, the EM algorithm or iterated reweighted least squares (Hastie et al. 2009,
Nelder and Baker 1972) do not scale well and are computationally less attractive. Due to
its computational and memory efficiency, stochastic gradient descent (Robbins and Monro
1951)[SGD] is a scalable algorithm for parameter estimation and has recently drawn a great
deal of attention. Unlike other classical methods that evaluate the objective function involving the entire dataset, the SGD method calculates the gradient of the objective function
using only one data point at a time and recursively updates the parameter estimate. This
is also numerically appealing and particularly useful in online updating settings such as
streaming data where it may not even be feasible to store the entire dataset in memory.
Wang et al. (2016) gave a nice review on recent achievements of applying the SGD method
to big data and streaming data.
The asymptotic properties of SGD estimators such as consistency and asymptotic normality have been well established; see, for example, Ruppert (1988) and Polyak and Juditsky
(1992). However, statistical inference such as confidence interval estimation for SGD estimators has remained largely unexplored. Traditional interval estimation procedures such
as the plug-in procedure and the bootstrap are often numerically difficult in the presence
of big datasets. The bootstrap repeatedly draws samples from the entire dataset. The
plug-in procedure requires an explicit variance-covariance formula. Since the classical bootstrap is not directly applicable if the data are not stored in memory, using the deal from
the weighted bootstrap (Rubin 1981), we propose an online bootstrap procedure for the
estimation of confidence intervals.
There are only a few papers considering the statistical inference of the SGD method.
Chen et al. (2016) proposed a method called the batch-mean procedure. Although computationally efficient and theoretically sound, the batch-means procedure substantially underestimates the variance of the SGD estimator in finite-sample studies, because of the
correlations between the batch means. Li et al. (2017) presented a new method for statistical inference in M-estimation problems, based on SGD estimators with a fixed step size.
However, this method is limited to M-estimation and fixed step size. Su and Zhu (2018)
proposed a new method called HiGrad, short for Hierarchical Incremental GRAdient Descent, which estimates model parameters in an online fashion and provides a confidence
interval for the true population value. This method is also computationally efficient and
theoretically sound, but it is not applicable to vanilla SGD estimators.
In this paper, we propose an online bootstrap resampling procedure to approximate the
distribution of a SGD estimator in a general class of models that includes linear regressions,
generalized linear models, M-estimators and quantile regressions as special cases. Our
proposal, justified by asymptotic theories, provides a simple way to estimate the covariance
matrix and confidence regions. Through numerical experiments, we verify the ability of this
procedure to give accurate inference for big datasets.
The rest of the article is organized as follows. In Section 2, we introduce the proposed
online bootstrap procedure for constructing confidence regions. In Section 3, we theoretically justify the validity of our proposal for a general class of models, along with some
special cases. In Section 4, we demonstrate the performance of the proposed procedures
in finite samples via simulation studies and three real data applications. Some concluding
remarks are given in Section 5 and all the technical proofs are relegated to the Appendix.
2. The proposed resampling procedure
Parameter estimation by optimizing an objective function is often encountered in statistical
practice. Consider the general situation where the optimal model parameter θ0 ∈ Rp
is
defined to be the minimizer of the expected loss function,
θ0 = argmin
θ∈Θ
n
L(θ) , E[`(θ;Z)]o
, (1)
2
Online Bootstrap for SGD
where `(θ; z) is some loss function and Z denotes one single observation and Θ is the
domain on which the loss function is defined, which is assumed to be open. Suppose that
the data consist of independent and identically distributed (i.i.d.) copies of Z, denoted by
DN = {Z1, . . . , ZN }. Under mild conditions, θ0 can be consistently estimated by
θeN = argmin
θ∈Θ
(
1
N
X
N
i=1
`(θ;Zi)
)
. (2)
However, the minimization problem (2) for large-scale datasets pose numerical challenges.
Furthermore, for applications such as online data where each sample arrives sequentially
(e.g., search queries or transactional data), it may not be necessary or feasible to store the
entire dataset, leaving alone evaluating the minimand in (2).
As a stochastic approximation method (Robbins and Monro 1951), stochastic gradient
descent is a scalable algorithm for parameter estimation with large-scale data. Given an
initial estimate θb0, the SGD method recursively updates the estimate upon the arrival of
each data point Zn, n = 1, 2, . . . , N,
θbn = θbn−1 − γn∇`(θbn−1;Zn), (3)
where the learning rates are γn = γ1n
−α with γ1 > 0 and α ∈ (0.5, 1). As suggested by
Ruppert (1988) and Polyak and Juditsky (1992), we consider the averaging estimate,
θn =
1
n
Xn
i=1
θbi
, (4)
which can also be recursively updated given that θn = (n − 1)θn−1/n + θbn/n.
In order to conduct statistical inference with the averaging SGD estimator θn at any
stage, we propose an online bootstrap resampling procedure, which recursively updates
the SGD estimate as well as a large number of randomly perturbed SGD estimates, upon
the arrival of each data point. Specifically, let W = {Wi
, i = 1, . . . , N} be a set of i.i.d.
non-negative random variables with mean and variance equal to one. In parallel with (3)
and (4), with θb∗
0 ≡ θb0, upon observing data point Zn, we recursively updates randomly
perturbed SGD estimates,
θb∗
n = θb∗
n−1 − γnWn∇`(θb∗
n−1
;Zn), (5)
θ
∗
n =
1
n
Xn
i=1
θb∗
i
. (6)
We will show that √
n(θn − θ0) and √
n(θ
∗
n − θn) converge in distribution to the same
limiting distribution. In practice, these results allow us to estimate the distribution of
√
n(θn − θ0) by generating a large number, say B, of random samples of W. We obtain θ
∗,b
n
by sequentially updating perturbed SGD estimates for each sample, b = 1, . . . , B,
θb∗,b
n = θb∗,b
n−1 − γnWn,b∇`(θb∗,b
n−1
;Zn), (7)
θ
∗,b
n =
1
n
Xn
i=1
θb∗,b
i
, (8)
3
Yixin Fang, Jinfeng Xu and Lei Yang
and then approximate the sampling distribution of θn − θ0 using the empirical distribution
of {θ
∗,b
n − θn, b = 1, ..., B}. Specifically, the covariance matrix of θn can be estimated by the
sample covariance matrix constructed from {θ
∗,b
n
, b = 1, ..., B}. Estimating the distribution
of √
n(θn − θ0) based on the distribution of √
n(θ
∗
n − θn)|Dn leads to the construction
of (1 − α)100% confidence regions for θ0. The resulting inferential procedure retains the
numerical simplicity of the SGD method, only using one pass over the data. The proposed
inferential procedure scales well for datasets with millions of data points or more, and its
theoretical validity can be justified for a general class models with mild regularity conditions
as shown in the next section.
3. Theoretical Results
3.1. Main theorems
In this section, we derive some theoretical properties of θ
∗
n
, justifying that the conditional
distribution of θ
∗
n − θn given data Dn = {Z1, Z2, . . . , Zn} can approximate the sampling
distribution of θn − θ0, under the following assumptions. Let k · k be the Euclidean norm
for vectors and the operator norm for matrices. The proofs are presented in the Appendix.
(A1). The objective function L(θ) is convex, continuously differentiable over θ ∈ Θ, and
twice continuously differentiable at θ = θ0, where θ0 is the unique minimizer of L(θ).
(A2). The gradient of L(θ), R(θ) = ∇L(θ), is Lipschitz continuous with constant L1 > 0;
that is, for any θ1 and θ2, kR(θ1) − R(θ2)k ≤ L1kθ1 − θ2k.
(A3). The Hessian matrix of L(θ), S(θ) = ∇2L(θ), exists and is positive definite at θ0 with
S0 = S(θ0) > 0 and is Lipschitz continuous at θ0 with constant L2 > 0.
(A4). Let V0 = E {[∇`(θ0;Z)][∇`(θ0;Z)]T}. Assume Ek∇`(θ;Z)k
2 ≤ C(1 + kθk
2
) for some
C and Ek∇`(θ;Z) − ∇`(θ0;Z)k
2 ≤ δ(kθ − θ0k) for some δ(·) with δ(x) → 0 as x → 0.
(A5). The learning rates are chosen as γn = γ1n
−α with γ1 > 0 and α ∈ (0.5, 1).
(A6). The perturbation variables, W1, W2, . . ., are non-negative i.i.d. random variables satisfying that E(Wn) = Var(Wn) = 1.
Following similar arguments in Ruppert (1988) and Polyak and Juditsky (1992), we can
prove the asymptotic normality of the SGD estimator θn under the above assumptions.
Lemma 1 If Assumptions A1-A5 are satisfied, then we have
√
n(θn − θ0) ⇒ N
0, S−1
0 V0S
−1
0

, in distribution, as n → ∞. (9)
From Lemma 1, we can conduct statistical inference based on θn provided that we can
estimate the covariance matrix S
−1
0 V0S
−1
0
, or we can use some resampling procedure to
approximate the sampling distribution of √
n(θn − θ0). We first derive the asymptotically
linear representation of θ
∗
n
for any perturbation variables that are i.i.d. random variables
satisfying that E(Wn) = 1.
 
Online Bootstrap for SGD
Theorem 2 If Assumptions A1-A5 hold, and the perturbation variables, W1, W2, . . ., are
non-negative i.i.d. random variables satisfying that E(Wn) = 1, then we have,
√
n(θ
∗
n − θ0) = −
1
√
n
S
−1
0
Xn
i=1
Wi∇`(θ0;Zi) + op(1). (10)
By Theorem 1, letting Wn ≡ 1, we derive the following representation for θn,
√
n(θn − θ0) = −
1
√
n
S
−1
0
Xn
i=1
∇`(θ0;Zi) + op(1). (11)
Then, considering the difference between (2) and (11), we have
√
n(θ
∗
n − θn) = −
1
√
n
S
−1
0
Xn
i=1
(Wi − 1)∇`(θ0;Zi) + op(1). (12)
Let P
∗ and E
∗ denote the conditional probability and expectation given the data. Starting from (12), we derive the following theorem.
Theorem 3 If Assumptions A1-A6 hold, then we have
sup
v∈Rp



P
∗
√
n(θ
∗
n − θn) ≤ v

− P
√
n(θn − θ0) ≤ v



→ 0, in probability. (13)
By Theorem 2, the Kolmogorow-Smirnov distance between √
n(θ
∗
n−θn) and √
n(θn−θ0)
converges to zero in probability. This validates our proposal of the perturbation-based
resampling procedure for inference with SGD. In the next section, we consider some special
cases where Assumptions A1-A4 are satisfied.
3.2. Special cases
3.2.1. Cases where `(θ;Z) is twice differentiable
If the loss function `(θ;Z) is twice differentiable, we can use the plug-in procedure to
estimate the asymptotic covariance matrix of √
N(θN −θ0), S
−1
0 V0S
−1
0
. That is, at the final
step, S0 and V0 can be estimated respectively by
SbN =
1
N
X
N
i=1
∇2
`(θN ;Zi) and VbN =
1
N
X
N
i=1
[∇`(θN ;Zi)][∇`(θN ;Zi)]T
. (14)
However, the above final-step plug-in estimation is impractical for large-scale data or
streaming data, because it requires that the whole dataset be stored. To overcome this
problem, in practice we can estimate S0 and V0 recursively, for n = 1, 2, . . ., using
Sbn =
1
n
Xn
i=1
∇2
`(θi
;Zi) and Vbn =
1
n
Xn
i=1
[∇`(θi
;Zi)][∇`(θi
;Zi)]T
. (15)
In the following, we examine two examples where `(θ;Z) is twice differentiable. Example
1 is linear regression, where the loss function `(θ;Z) is twice differentiable and the objective
5
Yixin Fang, Jinfeng Xu and Lei Yang
function L(θ) is strongly convex. Example 2 is logistic regression, where `(θ;Z) is twice
differentiable but L(θ) is non-strongly convex. They are two examples of generalized linear
models, one for quantitative outcome and the other for binary outcome. In these two
examples, both the plug-in procedures and the proposed perturbation resampling procedure
are robust to model mis-specification.
Example 1 (Linear regression) Suppose that Zn = (Yn, Xn), n = 1, 2, . . ., are i.i.d. copies
of Z = (Y, X), where Y is quantitative and X is p-dim with EkXk
2 < ∞. Let
θ0 = arg min
θ∈Rp
E (Y − XT
θ)
2
, (16)
where `(θ;Z) = (Y − XT
θ)
2
is twice differentiable and L(θ) = E (Y − XT
θ)
2
is strongly
convex. Moveover, ∇`(θ;Z) = −2(Y −XT
θ)X, ∇2
`(θ;Z) = 2XTX, ∇L(θ) = 2E{XX T}θ−
2E{XY }, and ∇2L(θ) = E{∇l(θ;Z)} = 2E{XX T}. Letting V0 = 4E{(Y − XT
θ0)
2XX T}
and S0 = 2E{XX T}, we can easily verify that Assumptions A1-A4 hold. The SGD and
perturbed SGD updates for θ0, as defined in (3) and (5) respectively, are
θbn = θbn−1 + 2γn(Yn − XT
n
θbn−1)Xn, (17)
θb∗
n = θb∗
n−1 + 2γnWn(Yn − XT
n
θb∗
n−1
)Xn. (18)
Example 2 (Logistic regression) Suppose that Zn = (Yn, Xn), n = 1, 2, . . ., are i.i.d. copies
of Z = (Y, X), where Y = ±1 and X is p-dim with EkXk
2 < ∞. Let
θ0 = arg min
θ∈Rp
E

− log 
1
1 + exp(−Y X Tθ)
 , (19)
where `(θ;Z) = log (1 + exp(−Y X T
θ)) is twice differentiable and L(θ) = E{`(θ;Z)} is nonstrongly convex. Moveover, ∇`(θ;Z) = −
1
1+exp(Y XTθ)XY , ∇2
`(θ;Z) = exp(Y XTθ)
[1+exp(Y XTθ)]2 XX T
,
∇L(θ) = E {∇`(θ;Z)}, and ∇2L(θ) = E

∇2
`(θ;Z)
	
. Letting V0 = E
n
1
[1+exp(Y XTθ)]2 XX T
o
and S0 = E
n
exp(Y XTθ)
[1+exp(Y XTθ)]2 XX T
o
, we can easily verify that Assumptions A1-A4 hold. The
SGD and perturbed SGD updates for θ0, as defined in (3) and (5) respectively, are
θbn = θbn−1 + γnXY /[1 + exp(Y X T
θbn−1)], (20)
θb∗
n = θb∗
n−1 + γnWnXY /[1 + exp(Y X T
θb∗
n−1
)]. (21)
We conclude this subsection with some discussion on the strong convexity of objective
function L(θ), which is strongly convex in Example 1 and is non-strongly convex in Example
2. If L(θ) is strongly convex, i.e. there exists µ > 0 such that L(θ1) ≥ L(θ2) +∇L(θ2)
T
(θ1 −
θ2) + µkθ1 − θ2k
2
for any θ1 and θ2, Moulines and Bach (2011) derived a non-asymptotic
bound for (Ekθn − θ0k)
1/2
. The bound of (Ekθn − θ0k)
1/2 has several terms; the leading
term is of order O(n
−1
) and the next two leading terms have order O(n
α−2
) and O(n
−2α),
suggesting the setting α = 2/3 to make them equal. If L(θ) is non-strongly convex, Moulines
and Bach (2011) derived a non-asymptotic bound for E[L(θbn)−L(θ0)] and a non-asymptotic
bound for E[L(θn) − L(θ0)]. The bound of E[L(θbn) − L(θ0)] is O(max{n
α−1
, n−α/2}), also
suggesting the setting α = 2/3 to achieve optimal rate O(n
−1/3
). Using the Polyak-Ruppert
averaging has allowed the bound of E[L(θn) − L(θ0)] to go from O(max{n
α−1
, n−α/2}) to
O(n
−α). Therefore, we use α = 2/3 in the numerical results.
6
Online Bootstrap for SGD
3.2.2. Cases where `(θ;Z) is not twice-differentiable
If the loss function `(θ;Z) is not twice-differentiable, neither the final-step plug-in estimation
(14) nor the recursive plug-in estimation (15) is applicable. Fortunately, our proposal
of scalable inference based on perturbation resampling is still applicable because it only
depends on the first order derivative ∇`(θ;Z). To understand this explicitly, consider the
following example of robust regression via ψ-type M-estimator where the loss function may
be not twice-differentiable.
Example 3 (Robust regression via ψ-type M-estimator) Suppose that Zn = (Yn, Xn),
n = 1, 2, . . ., are i.i.d. copies of Z = (Y, X), where Y is quantitative and X is p-dim with
EkXk
2 < ∞. Let ρ(·) be some convex function with ρ(0) = 0 and we attempt to estimate
θ0 = arg min
θ∈Rp
Eρ (Y − XT
θ), (22)
where `(θ;Z) = ρ(Y −XT
θ) and L(θ) = Eρ (Y − XT
θ). This is robust regression via ρ-type
M-estimator. If ρ(·) is differentiable with derivative ˙ρ(·) = ψ(·), we can solve it via ψ-type
M-estimator, solving the following equation,
E {ψ (Y − XT
θ0) X} = 0, (23)
where ∇`(θ;Z) = −ψ(Y − XT
θ)X and ∇L(θ) = −E {ψ (Y − XT
θ) X}. Hence, the SGD
and perturbed SGD updates for θ0, as defined in (3) and (5) respectively, are
θbn = θbn−1 + γnψ(Yn − XT
n
θbn−1)Xn, (24)
θb∗
n = θb∗
n−1 + γnWnψ(Yn − XT
n
θb∗
n−1
)Xn. (25)
If ψ(·) is not differentiable, neither the final-step plug-in estimation (14) nor the recursive
plug-in estimation (15) is applicable. However, if the corresponding `(θ;Z) and L(θ) satisfy
Assumptions A1-A4, the perturbation resampling procedure is applicable. Next we consider
a special setting where the following Assumptions B1-B4 hold.
(B1). Assume that ρ(u) is a convex function on R with the right derivative being ψ+(u) and
left derivative being ψ−(u). Let ψ(u) be a function such that ψ−(u) ≤ ψ(u) ≤ ψ+(u).
There exists constant C1 > 0 such that |ψ(u)| ≤ C1(1 + |u|).
(B2). Let εn = Yn −XT
n
θ0. Assume that (Xn, εn), n = 1, 2, ..., are i.i.d. copies of (X, ε), with
EkXk
4 < ∞ and Ekεk
2 < ∞. Let V0 = E{ψ
2
(ε)XX T} > 0.
(B3). Let φ(u|X) = E{ψ(u + ε)|X}. Assume that φ(0|X) = 0, uφ(u|X) > 0 for any u 6= 0,
and φ(u|X) has a derivative at u = 0 with φ˙(0|X) ≥ σ > 0 uniformly over X. Let
S0 = E{φ˙(0|X)XX T} > 0.
(B4). Assume that φ˙(u|X) is uniformly Lipschitz at u = 0. That is, there exist constants
C2 > 0 and δ > 0 such that


φ˙(u|X) − φ˙(0|X)


 ≤ C2|u| for |u| ≤ δ uniformly over X.
We derive the asymptotic properties of the ψ-type M-estimator as follows.
7
Yixin Fang, Jinfeng Xu and Lei Yang
Lemma 4 If Assumptions B1-B4 and A5 are satisfied, then we have
√
n(θn − θ0) ⇒ N
0, S−1
0 V0S
−1
0

, in distribution, as n → ∞. (26)
Theorem 5 If Assumptions B1-B4 and A5-A6 are satisfied, then we have
√
n(θ
∗
n − θ0) = 1
√
n
S
−1
0
Xn
i=1
Wiψ(εi)Xi + op(1). (27)
From Lemma 2 we see that the plug-in procedures are not applicable for estimating the
asymptotic covariance matrix, because although they are applicable for estimating V0, they
are not applicable for estimating S0, which involves φ˙(0|X). Moreover, by Theorem 3, we
can show that the Kolmogorow-Smirnov distance between √
n(θ
∗
n − θn) and √
n(θn − θ0)
converges to zero in probability, as stated in Theorem 2. This validates our proposal of
the perturbation-based resampling procedure for robust regression. To further understand
Assumptions B1-B4, we examine the following example of quantile regression, which is a
special case of the above robust regression.
Example 4 (Quantile regression). Assume the τ -quantile of Y given X is XT
θ0, with
θ0 = arg min
θ∈Rp
Eρτ (Y − XT
θ), (28)
where ρτ (u) = u(τ − I(u < 0)) with a given 0 < τ < 1. Let ε = Y − XT
θ0 and ψτ (u) =
τ − I(u < 0). Thus E{ψτ (ε)|X} = τ − Fε(0|X) = 0, where Fε(u|X) is the conditional
distribution function of ε. Let pε(u|X) be the conditional density function of ε. Note
that φ(u|X) = E{ψτ (u + ε)|X} = τ − Fε(−u|X), φ˙(0|X) = pε(0|X), V0 = E{ψ
2
τ
(ε)XX T} =
τ (1−τ )E{XX T} and S0 = E{pε(0|X)XX T}. Therefore, we can easily verify that if pε(0|X)
is uniformly bounded away from 0 and pε(u|X) is uniformly Lipschitz continuous at u = 0,
then Assumptions B1-B4 hold. Thus, the SGD and perturbed SGD updates for θ0, as
defined in (3) and (5) respectively, are
θbn = θbn−1 + γn
n
τ − I(Yn − XT
n
θbn−1 < 0)o
Xn, (29)
θb∗
n = θb∗
n−1 + γnWn
n
τ − I(Yn − XT
n
θb∗
n−1 < 0)o
Xn, (30)
and the asymptotic results stated in Lemma 2 and Theorem 3 follow directly.
4. Numerical results
4.1. Simulation studies
To assess the performance of the proposed online bootstrap resampling procedure (a.k.a.
random weighting procedure; RW) for SGD estimators, we conduct simulation studies for
those four examples discussed in Section 3. We compare the proposed procedure with the
recursive plug-in procedure (RPI).
Setting 1 (Linear regression): Consider linear regression (16), where covariates X(j)
,
j = 1, . . . , p, and residual ε = Y −XT
θ0, are independently generated from standard normal
 
Online Bootstrap for SGD
N(0, 1). Here X(j)
indicates the j-th dimension of X. Let θ0 = (µ1
T
q/2
, −µ1
T
q/2
, 0
T
p−q
)
T
(same
for the other three settings). Consider the corresponding SGD estimators (17) and (18).
Setting 2 (Logistic regression): Consider logistic regression (19), where covariates
X(j) are independently from N(0, 1) and response Y from Bernoulli distribution with
logit{P(Y = 1|X)} = XT
θ0. Consider the corresponding SGD estimators (20) and (21).
Setting 3 (LAD regression): Consider least absolute deviation (LAD) regression, which
is a special case of robust regression (22) with ρ(x) = |x| and quantile regression (28) with
τ = 0.5, where covariates X(j)
, j = 1, . . . , p, are i.i.d. with N(0, 1) and residual ε, defined
as Y −XT
θ0, is independently from double exponential distribution DE(0, 1). Consider the
corresponding SGD estimators (29) and (30) with τ = 0.5.
Setting 4 (LAD regression for data with outliers): Consider LAD regression for the
data generated from Setting 1 but contaminated with 10% outliers. The contaminated data
are obtained by transforming the outcome variable in the data generated from Setting 1
using Y ← Y + 10 if |X(1)| ≥ 1.96 and |X(2)| < 1.96; and Y ← Y − 10 if |X(1)| < 1.96
and |X(2)| ≥ 1.96. In this setting, covariate vector X and residual ε = Y − XT
θ0 are not
independent, but median{ε|X} = 0. Consider the corresponding SGD estimators defined
in (29) and (30) with τ = 0.5.
For each simulation setting, we consider four scenarios, as described by (N, p, q, µ),
where sample size N = 10, 000 or 20, 000, number of covariates p = 10 or 20, number of
informative covariates q = 6, and effect size µ = 0.1 or 0.2. For each example, we repeat
the data generation 1000 times. For each data repetition, we use Wnb ∼ exp(1) as random
weights and generate B = 200 copies of random weights whenever a new data point is read.
Then, for each data repetition, we obtain the SGD estimate (4), and apply the following
procedures to construct 95% confidence intervals: the proposed random weighting procedure
to obtain 2.5% and 97.5% quantile (RW-Q), the proposed random weighting procedure to
estimate its standard error (RW-σ) and then construct “estimate ± 1.96 × SE”, and the c
recursive plug-in procedures (RPI), if applicable, to estimate its standard error. We consider
the learning rate α = 2/3. When we calculate the average SGD estimators (4) and (6), the
first 2000 and 4000 estimates are excluded for N = 10, 000 and N = 20, 000, respectively.
We also obtain the empirical standard error based on 1000 repeated SGD estimates, as a
benchmark approximation to the true standard error.
The coverage probabilities of the 95% confidence interval estimates are summarized in
Table 1 for linear regression (Setting 1), Table 2 for logistic regression (Setting 2) and Table
3 for LAD regression (Settings 3-4), respectively. We only report results corresponding to
the first, fourth and seventh covariates (that is, X(1)
, X(q/2+1) and X(q+1)). The plug-in
procedures are not applicable for LAD regression in Settings 3-4.
From Tables 1 and 2, we see that, for linear regression and logistic regression, the
coverage probabilities using the RW-Q, RW-σ and RPI are close to 95%. Therefore, the
proposed random weighting procedures (both RW-Q and RW-σ) perform well for linear
regression and logistic regression, and if we choose to use the plug-in procedure, which
involves matrix inverse, we can use RPI. Since the point estimate is PN
i=N0+1 θbi/(N − N0),
where N0 is the number of excluded estimates and which involves a pass of SGD estimates,
its standard error should also involve a pass of SGD estimates, instead of involving only
the final-step estimate or the true parameter value. We can understand this clearer if
we see the “SE” column, where the standard errors from RW-σ and RPI are close to the
9
Yixin Fang, Jinfeng Xu and Lei Yang
empirical standard error. Moveover, from Table 3, we see that, for LAD regression and for
both Settings 3 and 4, the coverage probabilities using either of the two proposed random
weighting procedures are close to 95%, and the standard errors using RW-σ are close to the
empirical standard errors.
Table 1: Coverage probabilities of 95% confidence intervals for linear regression
(N, p, q, µ) Method Dim 1 Dim q/2 + 1 Dim q + 1
Cover SE Cover SE Cover SE
(10000,10,6,0.1) RW-Q 0.947 − 0.946 − 0.950 −
RW-σ 0.956 0.012 0.950 0.012 0.959 0.012
RPI 0.953 0.011 0.946 0.011 0.956 0.011
Empirical − 0.011 − 0.011 − 0.011
(10000,10,6,0.2) RW-Q 0.947 − 0.946 − 0.950 −
RW-σ 0.956 0.012 0.950 0.012 0.959 0.012
RPI 0.953 0.011 0.946 0.011 0.956 0.011
Empirical − 0.011 − 0.011 − 0.011
(20000,20,6,0.1) RW-Q 0.939 − 0.951 − 0.945 −
RW-σ 0.949 0.008 0.953 0.008 0.960 0.008
RPI 0.956 0.008 0.948 0.008 0.957 0.008
Empirical − 0.008 − 0.008 − 0.008
(20000,20,6,0.2) RW-Q 0.939 − 0.951 − 0.945 −
RW-σ 0.949 0.008 0.953 0.008 0.960 0.008
RPI 0.956 0.008 0.948 0.008 0.957 0.008
Empirical − 0.008 − 0.008 − 0.008
4.2. Real data applications
In this section, we apply the proposed procedures to conduct inference for linear regression
analysis for the individual household electric power consumption data (POWER) and logistic regression analysis for the skin segmentation dataset (SKIN) and gas sensors for the
home Activity monitoring data (GAS). All the three datasets are publicly available on UCI
machine learning repository.
The POWER dataset contains 2,075,259 observations and we fit a linear model to investigate how the time of a day influences the response variable “sub-metering-1”, the energy
sub-metering No. 1, in watt-hour of active energy corresponding to kitchen. The observations with missing value are deleted, and the time of a day is divided into 8 categories,
“Time 0-2”, “Time 3-5”, ..., and “Time 21-23”. The SKIN dataset contains 245,057 observations, out of which 50,859 is the skin samples and 194,198 is non-skin samples. We
fit a logistic model to examine the relationship between the indicator of skin and three
predictors, B, G and R. The GAS dataset contains 919,438 observations and we only use
a subset containing 652,024 observations with response variable being either “banana” or
“wine”. We fit a logistic model to examine the association between the response variable
and 11 explanatory variables, Time, R1 to R8, Temperature and Humidity.
10
Online Bootstrap for SGD
Table 2: Coverage probabilities of 95% confidence intervals for logistic regression
(N, p, q, µ) Method Dim 1 Dim q/2 + 1 Dim q + 1
Cover SE Cover SE Cover SE
(10000,10,6,0.1) RW-Q 0.950 − 0.948 − 0.928 −
RW-σ 0.954 0.024 0.954 0.024 0.948 0.023
RPI 0.954 0.023 0.952 0.023 0.942 0.023
Empirical − 0.022 − 0.022 − 0.023
(10000,10,6,0.2) RW-Q 0.945 − 0.945 − 0.944 −
RW-σ 0.959 0.025 0.954 0.025 0.954 0.024
RPI 0.955 0.023 0.952 0.023 0.948 0.023
Empirical − 0.023 − 0.023 − 0.023
(20000,20,6,0.1) RW-Q 0.944 − 0.939 − 0.934 −
RW-σ 0.951 0.016 0.953 0.016 0.952 0.016
RPI 0.948 0.016 0.953 0.016 0.949 0.016
Empirical − 0.016 − 0.016 − 0.016
(20000,20,6,0.2) RW-Q 0.946 − 0.941 − 0.939 −
RW-σ 0.958 0.017 0.952 0.017 0.957 0.016
RPI 0.950 0.016 0.944 0.016 0.952 0.015
Empirical − 0.016 − 0.016 − 0.016
Although standard softwares such as SAS and R can fit linear and logistic regression to
such datasets without difficulty, for the illustration purpose, we use the SGD as in Examples
1 and 2 to fit linear and logistic regression and use the proposed online boostrap procedure
to construct confidence intervals. The point estimates and the 95% confidence intervals of
the coefficients are showed in Table 4. From the left-top panel of Table 4, we see that the
electronic power consumption from kitchen is relatively high in the evening and night. From
the left-bottom panel of Table 4, we see that variable B is positively associated with the
response while the other two variables G and R are negatively associated. From the right
panel of Table 4, we see that all the variables but R4 are statistical significantly associated
with the response. Furthermore, we display the histogram of B = 1000 perturbation-based
SGD estimates for each coefficient in Figures 4.2-4.2 for the POWER data, the SKIN data
and the GAS data, respectively. The blue triangle in each figure indicates the corresponding
point estimate the red triangles indicate 2.5 and 97.5 quantiles. From these figures, we see
the the perturbation-based procedure can be used to approximate the sampling distribution
of the corresponding SGD estimator, which might be skewed. For example, for the GAS
data, the sampling distributions are very skewed, and therefore the proposed resampling
procedure is able to display such skewness.
5. Discussion
Online updating is a useful strategy for analyzing big data and streaming data, and recently stochastic gradient decent has become a popular method for doing online updating.
Although the asymptotic properties of SGD have been well studied, there is little research
11
Yixin Fang, Jinfeng Xu and Lei Yang
Table 3: Coverage probabilities of 95% confidence intervals for LAD regression
(N, p, q, µ) Method Dim 1 Dim q/2 + 1 Dim q + 1
Cover SE Cover SE Cover SE
Simulation Setting 3
(10000,10,6,0.1) RW-Q 0.965 − 0.965 − 0.969 −
RW-σ 0.969 0.029 0.965 0.029 0.965 0.029
Empirical − 0.026 − 0.027 − 0.026
(10000,10,6,0.2) RW-Q 0.972 − 0.965 − 0.967 −
RW-σ 0.973 0.029 0.966 0.029 0.968 0.029
Empirical − 0.026 − 0.027 − 0.026
(20000,20,6,0.1) RW-Q 0.970 − 0.973 − 0.966 −
RW-σ 0.966 0.010 0.969 0.010 0.965 0.010
Empirical − 0.009 − 0.009 − 0.009
(20000,20,6,0.2) RW-Q 0.967 − 0.974 − 0.966 −
RW-σ 0.966 0.010 0.969 0.010 0.969 0.010
Empirical − 0.009 − 0.009 − 0.009
Simulation Setting 4
(10000,10,6,0.1) RW-Q 0.954 − 0.971 − 0.950 −
RW-σ 0.958 0.035 0.969 0.035 0.960 0.035
Empirical − 0.032 − 0.031 − 0.033
(10000,10,6,0.2) RW-Q 0.960 − 0.966 − 0.955 −
RW-σ 0.958 0.035 0.966 0.035 0.956 0.035
Empirical − 0.032 − 0.031 − 0.033
(20000,20,6,0.1) RW-Q 0.948 − 0.953 − 0.965 −
RW-σ 0.963 0.047 0.958 0.047 0.964 0.047
Empirical − 0.043 − 0.045 − 0.044
(20000,20,6,0.2) RW-Q 0.944 − 0.957 − 0.961 −
RW-σ 0.961 0.047 0.961 0.047 0.961 0.047
Empirical − 0.044 − 0.045 − 0.044
on conducting statistical inference based on SGD estimators. In this paper, we propose
the perturbation-based resampling procedure, which can be applied to estimate the sampling distribution of an SGD estimator. The offline version of perturbation-based resamping
procedure was first proposed by Rubin (1981) and was also discussed in Shao and Tu (2012).
The proposed resampling procedure is in essence an online version of the bootstrap.
Recall that the data points, Z1, Z2, . . . , ZN , are arriving one at a time and an SGD estimate updates itself from θbn−1 to θbn whenever a new data point Zn arrives. If we
are forced to apply the bootstrap, then we should have many bootstrap samples; the
data points of each bootstrap sample, Z
∗
1
, Z∗
2
, . . . , Z∗
N , are assumed to be arriving one at
a time and the bootstrapped SGD estimate updates itself from θb∗
n−1
to θb∗
n whenever a
new data point Z
∗
n arrives. Of course such bootstrap is impractical here because in online updating we will not obtain and store all the data points and then generate bootstrap samples. Now if we rearrange hypothetical bootstrap sample Z
∗
1
, Z∗
2
, . . . , Z∗
N as
12
Online Bootstrap for SGD
Table 4: Point estimates and 95% confidence intervals for three real datasets; POWER data
on the left-top panel, SKIN data on the left-bottom panel and GAS data on the
right panel
Variable Est. 95% CI Variable Est. 95% CI
Time 0-2 2.265 (2.254, 2.275) Time −0.158 (−0.178, −0.139)
Time 3-5 2.045 (2.040, 2.049) R1 −0.202 (−0.215, −0.190)
Time 6-8 2.623 (2.608, 2.639) R2 0.176 (0.160, 0.191)
Time 9-11 3.323 (3.298, 3.347) R3 −0.907 (−0.932, −0.882)
Time 12-14 3.445 (3.420, 3.470) R4 −0.007 (−0.018, 0.004)
Time 15-17 3.059 (3.037, 3.082) R5 −0.450 (−0.467, −0.432)
Time 18-20 4.176 (4.143, 4.208) R6 1.772 (1.759, 1.785)
Time 21-23 4.053 (4.024, 4.082) R7 0.173 (0.139, 0.207)
B 1.501 (1.441, 1.569) R8 0.302 (0.272, 0.332)
G −0.242 (−0.319, −0.166) Temp. −0.175 (−0.191, −0.160)
R −1.956 (−1.999, −1.918) Humi. −0.551 (−0.560, −0.542)
Figure 1: Histograms of B = 1000 perturbation-based SGD estimates for the POWER data.
{K1 copies Z1, K2 copies Z2, . . . , KN copies ZN }, where Kn follows binomial distribution
B(N, 1/N), then the SGD estimator updates itself from θb∗
n−1
to θb∗
n whenever a new batch of
data points, Kn copies of Zn, arrives. Noting that binomial distribution B(N, 1/N) approximates to Poisson distribution P(1) as N → ∞, we see that the aforementioned hypothetical
13
Yixin Fang, Jinfeng Xu and Lei Yang
Figure 2: Histograms of B = 1000 perturbation-based SGD estimates for the SKIN data.
Figure 3: Histograms of B = 1000 perturbation-based SGD estimates for the GAS data.
bootstrap is equivalent to our proposed online bootstrap procedure with Wn ∼ P(1), whose
mean and variance are both equal to one.
Finally, the SGD method considered in this paper is actually the explicit SGD, in contrast with the implicit SGD considered in Toulis et al (2017). We are working on extending
14
Online Bootstrap for SGD
the proposed perturbation-based resampling procedure for conducting statistical inference
for the implicit SGD.
Appendix A. technical proofs
For ease exposition of establishing asymptotic normality of SGD and perturbed SGD estimates, we present the following Proposition 1, adapted from Theorem 2 of Polyak and
Juditsky (1992; pp.841). Let R(θ) : Rp → Rp be some unknown function and R(θ0) = 0.
The dataset consists of Zn, n = 1, 2, . . . , which are i.i.d. copies of Z. Stochastic gradients
are Rb(θ;Zi) and E{Rb(θ;Zi)} = R(θ). With an initial point θb0 and learning rates γn, the
SGD estimate is defined as
θbn = θbn−1 − γnRb(θbn−1;Zn) = θbn−1 − γn

R(θbn−1) − Dn

, (31)
where Dn = R(θbn−1)−Rb(θbn−1;Zn) is a martingale-difference process; that is, E{Dn|Fn−1} =
0, where Fn−1 = σ(Dn−1). The regularity conditions for Proposition 1 are listed as follows.
(C1). There exists a function U(θ) : Rp → R such that for some λ > 0, δ > 0, l0 > 0, L0 > 0,
and all θ, θ0 ∈ Rp
, the conditions U(θ) ≥ λkθk
2
, k∇U(θ) − ∇U(θ
0
)k ≤ L0kθ − θ
0k,
U(0) = 0, ∇U(θ − θ0)
TR(θ) > 0 for θ 6= θ0 hold true. Moreover, ∇U(θ − θ0)
TR(θ) ≥
l0U(θ − θ0) for all kθ − θ0k ≤ δ.
(C2). There exists a positive definite matrix S0 ∈ Rp×p
such that for some C > 0, 0 < % ≤ 1,
and δ > 0, the condition kR(θ)−S0(θ −θ0)k ≤ Ckθ −θ0k
1+%
for all kθ −θ0k ≤ δ holds
true.
(C3). {Dn}n≥1 is a martingale difference process with E{Dn|Fn−1} = 0, and for some C > 0,
E

kDnk
2
| Fn−1
	
+ kR(θbn−1)k
2 ≤ C

1 + kθbn−1k
2

a.s.,
for all n ≥ 1. Consider decomposition Dn = Dn(0) + En(θbn−1), where Dn(0) =
R(θ0)−Rb(θ0;Zn) and En(θbn−1) = Dn −Dn(0). Assume that E{Dn(0)|Fn−1} = 0 a.s.,
E{Dn(0)Dn(0)T
|Fn−1}
P
→ V0 > 0,
supn≥1 E

kDn(0)k
2
I(kDn(0)k > η)|Fn−1
	 P
→ 0, as η → ∞,
and there exists δ(x) → 0 as x → 0 such that, for all n large enough,
E
n
kEn(θbn−1)k
2
| Fn−1
o
≤ δ(kθbn−1 − θ0k) a.s..
(C4). It holds that (γn − γn+1)/γn = o(γn), γn > 0 for all n, and P∞
n=1 γ
(1+%)/2
n n
−1/2 < ∞.
Assumptions C2 and C4 are implied by the following Assumptions C20
(i.e. Assumption
C2 with % = 1) and C40
(i.e. Assumption A5):
(C20
). There exists a positive definite matrix S ∈ Rp×p
such that for some C > 0 and δ > 0,
the condition kR(θ) − S0(θ − θ0)k ≤ Ckθ − θ0k
2
for all kθ − θ0k ≤ δ holds true.
15
Yixin Fang, Jinfeng Xu and Lei Yang
(C40
). The learning rates are chosen as γn = γ1n
−α with γ1 > 0 and α ∈ (0.5, 1).
Proposition 1. If Assumptions C1-C4 are satisfied, then we have θn → θ0, a.s.; and
√
n(θn − θ0) = 1
√
n
S
−1Xn
i=1
Di + op(1), (32)
which implies √
n(θn − θ0) ⇒ N
0, S−1
0 V0S
−1
0

, in distribution.
A1. Proof of Lemma 1
Proof. By Proposition 1, it suffices to show that Assumptions C1-C4 hold under Assumptions A1-A5. Because C2 and C4 are implied by C20 and C40
, it suffices to show that
Assumptions C1, C20
, C3 and C40 hold under Assumptions A1-A5.
Verification of Assumption C1: Recall that R(θ) = ∇L(θ) and Rb(θ;Zi) = ∇`(θ;Zi).
Define U(θ) = L(θ0 + θ) − L(θ0) + λkθk
2
for a given λ > 0. By definition of U(θ) and
Assumption A1, we have U(θ) ≥ λkθk
2 and U(0) = 0. For any θ and θ
0
, since ∇U(θ) −
∇U(θ
0
) = R(θ+θ0)−R(θ
0+θ0)+2λ(θ−θ
0
), letting L0 = L1+2λ and by Assumption A2, we
have k∇U(θ)−∇U(θ
0
)k ≤ L1kθ−θ
0k. Since ∇U(θ−θ0)
TR(θ) = kR(θ)k
2+λ(θ−θ0)
TR(θ), by
Assumption A1, we have ∇U(θ−θ0)
TR(θ) > 0 for any θ 6= θ0. Last, it remains to verify there
exist l0 > 0 and δ > 0 such that ∇U(θ −θ0)
TR(θ) ≥ l0U(θ −θ0) for all kθ −θ0k ≤ δ. Noting
that U(θ−θ0) = L(θ+θ0)+λkθ−θ0k
2
, by Taylor expansion and Assumption A3, we see that
there exist l1 > 0 and δ1 such that U(θ−θ0) ≤ l1kθ−θ0k
2
for all kθ−θ0k ≤ δ1. On the other
hand, noting that ∇U(θ−θ0)
TR(θ) = kR(θ)k
2+λ(θ−θ0)
TR(θ), by Assumption A3 also, we
see that there exist l2 > 0 and δ2 such that (θ −θ0)
TR(θ) ≥ l2kθ −θ0k
2
for all kθ −θ0k ≤ δ2.
Selecting δ = min(δ1, δ2) and l0 = λl2/l2, we show that ∇U(θ − θ0)
TR(θ) ≥ l0U(θ − θ0) for
all kθ − θ0k ≤ δ.
Verification of Assumption C20
: Recall that R(θ) = ∇L(θ), S(θ) = ∇R(θ), and S0 =
S(θ0) > 0. By Assumption A3, there exists δ > 0 such that for any kθ − θ0k < δ0
,
kS(θ) − S(θ0)k < L2kθ − θ0k. By mean-value theorem, kS(θ) − S0(θ − θ0)k = kS(θe)(θ −
θ0) − S0(θ − θ0)k, where θe lies between θ and θ0. Hence kS(θ) − S0(θ − θ0)k ≤ L2kθ − θ0k
2
,
for any kθ − θ0k ≤ δ. Letting C = L2, we have verified Assumption C20
.
Verification of Assumption C3: Dn = R(θbn−1) − Rb(θbn−1;Zn). Consider decomposition Dn = Dn(0) + En(θbn−1), where Dn(0) = −∇`(θ0, Zn) and En(θbn−1) = [R(θbn−1) −
R(θ0)] − [∇`(θbn−1;Zn) − ∇`(θ0;Zn)]. By Assumption A2, kR(θbn−1)k
2 ≤ L
2
1
kθbn−1 − θ0k
2
.
In addition, Cauchy-Schwartz inequality implies that E

k∇`(θ;Zn) − ∇`(θ0;Zn)k
2
	
=
2Ek∇`(θ;Zn)k
2 + 2Ek∇`(θ0;Zn)k
2
; we have E
n
k∇`(θbn−1;Zn) − ∇`(θ0;Zn)k
2
|
o
≤ C
0
(1 +
kθk
2
) for some C
0 > 0 by Assumption A4, . Together, we have E

kDnk
2
| Fn−1
	
+
kR(θbn−1)k
2 ≤ C

1 + kθbn−1k
2

for some C > 0. Moreover, because Dn(0)’s are i.i.d., we
have E{Dn(0)Dn(0)T
|Fn−1} = V0 > 0 and supn≥1 E

kDn(0)k
2
I(kDn(0)k > η)|Fn−1
	 P
→ 0,
as η → ∞. Finally, note that EkEn(θ)k
2 ≤ L
2
1
kθ − θ0k
2 + Ek∇`(θ;Zn) − ∇`(θ0;Zn)k
2
. By
Assumption A3, Ek∇`(θ;Zn) − ∇`(θ0;Zn)k
2 ≤ δ
0
(kθ − θ0k) for some δ
0
(·) with δ
0
(x) → 0 as
x → 0. Define δ(x) = L
2
1x
2 + δ
0
(x), we show EkEn(θ)k
2 ≤ δ(kθ − θ0k). This complete the
verification of Assumption C3.
1 
Online Bootstrap for SGD
Obviously Assumption A5 is the same as Assumption C40
. Therefore, we have verified
that Assumptions A1-A5 imply Assumptions C1, C20
, C3 and C40
. By Proposition 1, we
complete the proof of Lemma 1.
A2. Proof of Theorem 1
Proof. Rewrite θb∗
n as
θb∗
n = θb∗
n−1 − γnR(θb∗
n−1
) + γnD∗
n
, (33)
where D∗
n = R(θb∗
n−1
) − Wn∇`(θb∗
n−1
;Zn). Then let F
∗
n−1 be the Borel field generated
by {(Zi
, Wi), i ≤ n − 1}. Since E{Wn|F
∗
n−1
} = 1 and R(θ) = E{∇`(θ;Zn)}, we have
E{D∗
n
|Fn−1} = 0. Thus D∗
n
is a martingale-difference process. Let D∗
n
(θ) = R(θ) −
Wn∇`(θ;Zn). Consider decomposition D∗
n = D∗
n
(0) + E∗
n
(θb∗
n−1
), where
D∗
n
(0) = −Wn∇`(θ0;Zn) (34)
and
E
∗
n
(θ) = [R(θ) − R(θ0)] − Wn[∇`(θ;Zn) − ∇`(θ0;Zn)]. (35)
Noting that E{D∗
n
(0)} = 0 and E(W2
n
) = 2 under Assumption A6, by Assumption A4, we
have
E{[D∗
n
(0)][D∗
n
(0)]T
} = 2E {[`(θ0;Zn)][`(θ0;Zn)]T
} = 2V0. (36)
By Cauchy-Schwartz inequality and Assumptions A2 and A4, we have
E{kE
∗
n
(θ)k
2
} ≤ 2kR(θ)k
2 + 4E

k∇`(θ, Z) − ∇`(θ0, Z)k
2
	
≤ δ
0
(kθ − θ0k), (37)
where δ
0
(x) = 2L
2
1x
2 + 2δ(x) satisfying that δ
0
(x) → 0 as x → 0. Also by Cauchy-Schwartz
inequality, E{kE∗
n
(θ)k
2} ≤ 2kR(θ)k
2 + 2E{k∇`(θ, Z)k
2}. Further, by Assumptions A2 and
A4,
E{kD∗
nk
2
|Fn−1}+kR(θb∗
n−1
)k
2 ≤ 3L
2
1kθb∗
n−1−θ0k
2+2Ckθb∗
n−1k
2 ≤ C
0
(1+kθb∗
n−1−θ0k
2
), (38)
for some large enough C
0 > 0. Combining results (36)-(38), we have verified Assumption
C3. Moreover, noting that EWn = 1, we can easily verify that Assumptions A1 and A2
imply that Assumption C1 holds, and Assumption A3 implies that Assumption C2 holds.
By Proposition 1, we show that θb∗
n → θ0 almost surely, and
√
n(θ
∗
n − θ0) = 1
√
n
S
−1
0
Xn
i=1
D∗
i + op(1)
= −
1
√
n
S
−1
0
Xn
i=1
Wi∇`(θ0;Zi) + 1
√
n
S
−1
0
Xn
i=1
E
∗
n
(θb∗
n−1
) + op(1). (39)
Note that E{kEn(θb∗
n−1
)k
2
|Fn−1} ≤ δ
0
(kθb∗
n−1 − θ0k), following (37). Since θb∗
n → θ0 a.s., we
have δ(kθb∗
n−1 − θ0k) → 0 a.s.. Thus, S
−1
0
Pn
i=1 En(θb∗
n−1
)/
√
n = op(1). Therefore, by (39),
this completes the proof of Theorem 1.
17  
Yixin Fang, Jinfeng Xu and Lei Yang
A3. Proof of Theorem 2
Proof. Let
Vn = −
1
√
n
S
−1
0
(Wi − 1)∇`(θ0, Zi). (40)
By Theorem 1, we have √
n(θ
∗
n − θn) = Vn + op(1). We first show that, for any β ∈ B ,
{β ∈ Rp
: kβk = 1} and u ∈ R,
P
∗
(β
TVn ≤ u) → Φ(u/σβ), in probability, (41)
where Φ(u) is the distribution of N (0, 1) and σβ = β
TS
−1
0 V0S
−1
0
β. In fact,
β
TVn =
1
√
n
Xn
i=1
(Wi − 1)ξi
, (42)
where ξi = −β
TS
−1
0 ∇`(θ0, Zi). Note that, by Assumption A6, EWi = Var(Wi) = 1. Hence
sn =
1
n
Xn
i=1
Var∗
{(Wi − 1)ξi} =
1
n
Xn
i=1
ξ
2
i → σ
2
β
, in probability, (43)
and for any  > 0,
1
nsn
Xn
i=1
E
∗

(Wi − 1)2
ξ
2
i
I

|(Wi − 1)ξi
| >
√
nsn
	 → 0, in probability. (44)
Therefore, the Lindeberg’s condition is satisfied. By the central limit theorem, (41) holds,
which implies that for any β ∈ B,
sup
u∈R
|P
∗
(β
TVn ≤ u) − Φ(u/σβ)| → 0, in probability. (45)
Consider B0 , {β ∈ Rp
: kβk = 1,the components of β are rational}, which contains only
countable many β and is a dense subset of B. For any subsequence {n1}, by Cantor’s “diagonal method” used in Rao and Zhao (1992), we can show that there exists a subsequence
{n2} ⊂ {n1} such that, with probability one,
sup
u∈R
|P
∗
(β
TVn−1 ≤ u) − Φ(u/σβ)| → 0, for any β ∈ B0. (46)
Hence, we show that
sup
v∈Rp



P
∗
√
n(θ
∗
n − θn) ≤ v

− P(ζ ≤ v)



→ 0, in probability, (47)
where ζ ∼ N (0, S−1
0 V0S
−1
0
). By Lemma 1, we can also show that
sup
v∈Rp

P
√
n(θn − θ0) ≤ v

− P(ζ ≤ v)

 → 0. (48)
Combining (47) and (48), we complete the proof of Theorem 2.
1    
Online Bootstrap for SGD
A4. Proof of Lemma 2
Proof. By Proposition 1, it suffices to show that Assumptions C1, C20 and C3 hold under
Assumptions B1-B4.
Verification of Assumption C1: Define ∆ = θ − θ0. Let R(∆) = E{φ(∆TX|X)X} and
Rb(∆;Zn) = ψ(∆TXn+εn)Xn. Define U(∆) = ∆T∆. By definition of U(∆), U(∆) ≥ λk∆k
2
with λ = 1, U(0) = 0 and ∇U(∆) is Lipschitz continuous. Since ∆TE{φ(∆TX|X)∆TX} > 0
by Assumption B3, we see that ∇U(∆)TR(∆) > 0. By the mean-value theorem and by Assumption B3, there exists δ such that ∆TE{φ(∆TX|X)∆TX} ≥ ∆TE{φ˙(0|X)XX T}∆/2 ≥
λmin(S0)k∆k
2/2, for any k∆k ≤ δ, where λmin(S0) is the minimum eigenvalue of S0. Hence
∇U(∆)TR(∆) ≥ λmin(S0)k∆k
2 and we have verified Assumption C1.
Verification of Assumption C20
: Note that
kR(∆) − S0∆k = kEφ(∆TX|X) − Eφ˙(0|X)XX T∆k.
By the mean-value theorem and Assumption B4, there exists δ such that, for any k∆k ≤ δ,
kE{φ(∆TX|X)X} −E{φ˙(0|X)XX T∆}k ≤ C2λmax(S0)k∆k
2
. This implies Assumption C20
.
Verification of Assumption C3: Let ∆b n = θbn −θ0. Then Dn = R(∆b n−1)−Rb(∆b n−1;Zn).
Consider decomposition Dn = Dn(0)+En(∆b n−1), where Dn(0) = ψ(εn)Xn and En(∆b n−1) =
E{ψ(∆b T
n−1X+εn)Xn}−[ψ(∆b T
n−1X+εn)Xn−ψ(εn)Xn]. By Assumption B1 and the CauchySchwartz inequality, we can show that E

kDnk
2
| Fn−1
	
+kR(∆b n−1)k
2 ≤ 2C1

1 + k∆b n−1k
2

.
Moreover, by Assumption B2, Dn(0)’s are i.i.d., so E{Dn(0)Dn(0)T
|Fn−1} = V0 > 0 and
supn≥1 E

kDn(0)k
2
I(kDn(0)k > η)|Fn−1
	
→0, as η → ∞. Finally, by Cauchy-Schwartz
inequality and Assumptions B2-B3, we can show that EkEn(∆)k
2 ≤ δ(k∆k) for some δ(·)
with δ(x) → 0 as x → 0. This complete the verification of Assumption C3.
Obviously Assumption A5 is the same as Assumption C40
. Therefore, we have verified
that Assumptions B1-B4 and A5 imply Assumptions C1, C20
, C3 and C40
. By Proposition
1, we complete the proof of Lemma 2.
A5. Proof of Theorem 3
Proof. Recall that R(∆) = E{φ(∆TX|X)X} and ∆b n = θbn − θ0. Rewrite θb∗
n as
θb∗
n = θb∗
n−1 + γnR(∆b n) + γnD∗
n
, (49)
where D∗
n = Wnψ(∆b T
nXn+εn)Xn−R(∆b n) is a martingale-difference process by Assumption
B3 and that E{Wn|Fn−1} = 1. Let D∗
n
(∆) = Wnψ (∆TXn + εn) Xn − R(∆) and D∗
n
(∆) =
D∗
n
(0) + E∗
n
(∆), where
E
∗
n
(∆) = Wn [ψ(∆TXn + εn) − ψ(εn)] Xn − R(∆). (50)
Since D∗
n
(0) = Wnψ(εn)Xn, E{D∗
n
(0)} = 0 and E{[D∗
n
(0)][D∗
n
(0)]T} = (1 + Var(W1))V0
by Assumption B2. By Cauchy-Schwartz inequality and Assumptions B2-B3, we can show
that EkE∗
n
(∆)k
2 ≤ δ(k∆k) for some δ(·) with δ(x) → 0 as x → 0. Therefore, using the
similar arguments as those in the proof of Lemma 2, we can verify that, under Assumptions
B1-B4, Assumptions C1-C4 are satisfied. By Proposition 1, it follows that θb∗
n → θ0 almost
19 
Yixin Fang, Jinfeng Xu and Lei Yang
surely, and
√
n(θ
∗
n − θ0) = 1
√
n
S
−1
0
Xn
i=1
D∗
i + op(1). (51)
By the decomposition of D∗
i
, we have
√
n(θ
∗
n − θ0) = 1
√
n
S
−1
0
Xn
i=1
Wiψ(εi)Xi +
1
√
n
S
−1
0
Xn
i=1
E
∗
n
(θb∗
n−1 − θ0) + op(1). (52)
By the definition of δ(k∆k), E{kE∗
n
(θb∗
n−1 − ∆)k
2
2
|Fn−1} = δ(kθb∗
n−1 − θ0k). Since θb∗
n → θ0
a.s., we have δ(kθb∗
n−1 − θ0k) → 0 a.s.. Thus, Pn
i=1 E∗
n
(θb∗
n−1 − θ0)/
√
n = op(1). By (52), we
complete the proof of Theorem 3.
