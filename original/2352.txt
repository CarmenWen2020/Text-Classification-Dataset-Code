
In this paper, we study the efficiency of a Restarted SubGradient (RSG) method that
periodically restarts the standard subgradient method (SG). We show that, when applied
to a broad class of convex optimization problems, RSG method can find an -optimal
solution with a lower complexity than the SG method. In particular, we first show that
RSG can reduce the dependence of SG’s iteration complexity on the distance between the
initial solution and the optimal set to that between the -level set and the optimal set
multiplied by a logarithmic factor. Moreover, we show the advantages of RSG over SG
in solving a broad family of problems that satisfy a local error bound condition, and also
demonstrate its advantages for three specific families of convex optimization problems with
different power constants in the local error bound condition. (a) For the problems whose
epigraph is a polyhedron, RSG is shown to converge linearly. (b) For the problems with
local quadratic growth property in the -sublevel set, RSG has an O(
1

log( 1

)) iteration
complexity. (c) For the problems that admit a local Kurdyka- Lojasiewicz property with a
power constant of β ∈ [0, 1), RSG has an O(
1

2β log( 1

)) iteration complexity. The novelty
of our analysis lies at exploiting the lower bound of the first-order optimality residual at the
-level set. It is this novelty that allows us to explore the local properties of functions (e.g.,
local quadratic growth property, local Kurdyka- Lojasiewicz property, more generally local
error bound conditions) to develop the improved convergence of RSG. We also develop
a practical variant of RSG enjoying faster convergence than the SG method, which can
be run without knowing the involved parameters in the local error bound condition. We
demonstrate the effectiveness of the proposed algorithms on several machine learning tasks
including regression, classification and matrix completion.
Keywords: subgradient method, improved convergence, local error bound, machine
learning
1. Introduction
We consider the following generic optimization problem
f∗ := min
w∈Ω
f(w), (1)
∗. Correspondence
c 2018 Tianbao Yang and Qihang Lin.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v19/17-016.html.
Yang and Lin
where f : R
d → (−∞, +∞] is an extended-valued, lower semicontinuous and convex function, and Ω ⊆ R
d
is a closed convex set such that Ω ⊆ dom(f). Here, we do not assume
the smoothness of f on dom(f). During the past several decades, many fast (especially
linearly convergent) optimization algorithms have been developed for (1) when f is smooth
and/or strongly convex. On the contrary, there are relatively fewer techniques for solving
generic non-smooth and non-strongly convex optimization problems, which have many applications in machine learning, statistics, computer vision, and etc. To solve (1) with f
being potentially non-smooth and non-strongly convex, one of the simplest algorithms to
use is the subgradient (SG) 1 method. When f is Lipschitz-continuous, it is known that
SG method requires O(1/2
) iterations for obtaining an -optimal solution (Rockafellar,
1970; Nesterov, 2004). It has been shown that this iteration complexity is unimprovable
for general non-smooth and non-strongly convex problems in a black-box first-order oracle model of computation (Nemirovsky A.S. and Yudin, 1983). However, better iteration
complexity can be achieved by other first-order algorithms for certain classes of f where
additional structural information is available (Nesterov, 2005; Gilpin et al., 2012; Freund
and Lu, 2017; Renegar, 2014, 2015, 2016).
In this paper, we present a generic restarted subgradient (RSG) method for solving (1)
which runs in multiple stages with each stage warm-started by the solution from the previous
stage. Within each stage, the standard projected subgradient update is performed for a
fixed number of iterations with a constant step size. This step size is reduced geometrically
from stage to stage. With these schemes, we show that RSG can achieve a lower iteration
complexity than the classical SG method when f belongs to some classes of functions. In
particular, we summarize the main results and properties of RSG below:
• For the general problem (1), under mild assumptions (see Assumption 1 and 2), RSG
has an iteration complexity of O(
1

2 log( 0

)) which has an additional log( 0

)
2
term but
has significantly smaller constant in O(·) compared to SG. In particular, compared
with SG whose iteration complexity quadratically depends on the distance from the
initial solution to the optimal set, RSG’s iteration complexity has a quadratic dependence on the distance from the -level set to the optimal set, which is much smaller
than the distance from the initial solution to the optimal set. Its dependence on the
initial solution is through 0 - a known upper bound of the initial optimality gap,
which only scales logarithmically.
• When the epigraph of f over Ω is a polyhedron, RSG can achieve linear convergence,
i.e., an O(log( 1

)) iteration complexity.
• When f is locally quadratically growing (see Definition 10), which is a weaker condition than strong convexity, RSG can achieve an O(
1

log( 1

)) iteration complexity.
• When f admits a local Kurdyka- Lojasiewicz property (see Definition 13) with a
power desingularizing function of degree 1 − β where β ∈ [0, 1), RSG can achieve
an O(
1

2β log( 1

)) complexity.
1. In this paper, we use SG to refer deterministic subgradient method, though it is used in literature for
stochastic gradient methods.
2. 0 is a known upper bound of the initial optimality gap in terms of the objective value.
2
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
These results, except for the first one, are derived from a generic complexity of RSG for the
problem satisfying a local error condition (15), which has a close connection to the existing
error bound conditions and growth conditions in the literature (Pang, 1997, 1987; Luo and
Tseng, 1993; Necoara et al., 2015; Bolte et al., 2006). In spite of its simplicity, the analysis of
RSG provides additional insight on improving first-order methods’ iteration complexity via
restarting. It is known that restarting can improve the theoretical complexity of (stochastic)
SG method for non-smooth problems when strongly convexity is assumed (Ghadimi and
Lan, 2013; Chen et al., 2012; Hazan and Kale, 2011) but we show that restarting can be
still helpful for SG methods under other (weaker) assumptions. We would like to remark
that the key lemma (Lemma 4) developed in this work can be leveraged to develop faster
algorithms in different contexts. For example, built on the groundwork laid in this paper,
Xu et al. (2016) have developed new smoothing algorithms to improve the convergence
of Nesterov’s smoothing algorithm (Nesterov, 2005) for non-smooth optimization with a
special structure, and Xu et al. (2017) have developed new stochastic subgradient methods
to improve the convergence of standard stochastic subgradient method.
We organize the reminder of the paper as follows. Section 2 reviews some related work.
Section 3 presents some preliminaries and notations. Section 4 presents the algorithm of
RSG and the general theory of convergence. Section 5 considers several classes of nonsmooth and non-strongly convex problems and shows the improved iteration complexities
of RSG. Section 6 presents parameter-free variants of RSG. Section 8 presents some experimental results. Finally, we conclude in Section 9.
2. Related Work
Smoothness and strong convexity are two key properties of a convex optimization problem
that affect the iteration complexity of finding an -optimal solution by first-order methods.
In general, a lower iteration complexity is expected when the problem is either smooth or
strongly convex. Recently there has emerged a surge of interest in further accelerating firstorder methods for non-strongly convex or non-smooth problems that satisfy some particular
conditions (Bach and Moulines, 2013; Wang and Lin, 2014; So and Zhou, 2017; Hou et al.,
2013; Zhou et al., 2015; Gong and Ye, 2014; Gilpin et al., 2012; Freund and Lu, 2017). The
key condition for us to develop an improved complexity is a local error bound condition (15)
which is closely related to the error bound conditions in the literature (Pang, 1987, 1997;
Luo and Tseng, 1993; Necoara et al., 2015; Bolte et al., 2006; Zhang, 2016).
Various error bound conditions have been exploited in many studies to analyze the
convergence of optimization algorithms. For example, Luo and Tseng (1992a,b, 1993) established the asymptotic linear convergence of a class of feasible descent algorithms for
smooth optimization, including coordinate descent method and projected gradient method,
based on a local error bound condition. Their results on coordinate descent method were
further extended to a more general class of objective functions and constraints by Tseng
and Yun (2009a,b). Wang and Lin (2014) showed that a global error bound holds for a
family of non-strongly convex and smooth objective functions for which feasible descent
methods can achieve a global linear convergence rate. Recently, these error bounds have
been generalized and leveraged to show faster convergence for structured convex optimization that consists of a smooth function and a simple non-smooth function (Hou et al., 2013;
3
Yang and Lin
Zhou and So, 2017; Zhou et al., 2015). Recently, Necoara and Clipici (2016) considered a
generalized error bound condition, and established linear convergence of a parallel version
of a randomized (block) coordinate descent method for minimizing the sum of a partially
separable smooth convex function and a fully separable non-smooth convex function.
We would like to emphasize that the aforementioned error bounds are different from the
local error bound explored in this paper. In particular, they bound the distance of a point
to the optimal set by using the norm of the projected gradient or proximal gradient at the
point, thus requiring the (partial) smoothness of the objective function. In contrast, we
bound the distance of a point to the optimal set by its objective residual with respect to the
optimal value, covering a much broader family of functions. More recently, there have appeared many studies that consider smooth optimization or composite smooth optimization
problems whose objective functions satisfy different error bound conditions, growth conditions or other non-degeneracy conditions and established the linear convergence rates of several first-order methods including proximal-gradient method, accelerated gradient method,
prox-linear method and so on (Gong and Ye, 2014; Necoara et al., 2015; Zhang and Cheng,
2015; Zhang, 2016; Karimi et al., 2016; Drusvyatskiy and Lewis, 2018; Drusvyatskiy and
Kempton, 2016; Hou et al., 2013; Zhou et al., 2015). The relative strength and relationships between some of those conditions are studied by Necoara et al. (2015) and Zhang
(2016). For example, Necoara et al. (2015) showed that under the smoothness assumption
the second-order growth condition (i.e., the considered error bound condition in the present
work with θ = 1/2) is equivalent to the error bound condition considered by Wang and Lin
(2014). It was brought to our attention that the local error bound condition in the present
paper is closely related to metric subregularity of subdifferentials (Artacho and Geoffroy,
2008; Kruger, 2015; Drusvyatskiy et al., 2014; Mordukhovich and Ouyang, 2015).
Gilpin et al. (2012) established a polyhedral error bound condition for problems whose
epigraph is polyhedral and domain is a bounded polytope. Using this polyhedral error
bound condition, they studied a two-person zero-sum game and proposed a restarted firstorder method based on Nesterov’s smoothing technique (Nesterov, 2005) that can find the
Nash equilibrium and has linear convergence rate. The differences between Gilpin et al.
(2012)’s work and this work are: (i) we study subgradient methods instead of Nesterov’s
smoothing technique, where the former have broader applicability than Nesterov’s smoothing technique; (ii) our linear convergence can be derived for a slightly general problem where
the domain is allowed to be an unbounded polyhedron as long as the polyhedral error bound
condition in Lemma 8 holds, which is the case for many important applications; (iii) we
consider a general condition that subsumes the polyhedral error bound condition as a special case and we try to solve the general problem (1) rather than the bilinear saddle-point
problem considered by Gilpin et al. (2012).
The error bound condition that allows us to derive a linear convergence of RSG is
the same to the weak sharp minimum condition, which was first coined in 1970s (Polyak,
1979). However, it was used even earlier for studying the convergence of subgradient
method (Eremin, 1965; Polyak, 1969). Later, it was studied in many subsequent works (Polyak,
1987; Burke and Ferris., 1993; Studniarski and Ward, 1999; Ferris, 1991; Burke and Deng,
2002, 2005, 2009). Finite or linear convergence of several algorithms has been established
under the weak sharp minimum condition, including gradient projection method (Polyak,
1987), the proximal point algorithm (PPA) (Ferris, 1991), and subgradient method with a
4
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
particular choice of step size (see below) (Polyak, 1969). We would like to emphasize the
differences between the results in these works and the results in the present work that make
our results novel: (i) the gradient projection method and its finite convergence established
in (Polyak, 1987) requires the gradient of the objective function to be Lipschitz continuous,
i.e., the objective function is smooth (see Polyak, 1987, Chap. 7, pp 207, Theorem 1),
in contrast we do not assume smoothness of the objective function; (ii) the PPA studied
in (Ferris, 1991) requires solving a proximal sub-problem consisting of the original objective
function and a strongly convex function at every iteration, and therefore its finite convergence does not mean that only a finite number of subgradient evaluations is needed. In
contrast, the linear convergence in this paper was in terms of the number of subgradient
evaluations; (iii) linear convergence of a subgradient method studied by Polyak (1969) requires knowing the optimal objective value for setting its step size, and its convergence is
in terms of the distance of the iterates to the optimal set, which is weaker than our linear
convergence in terms of objective gap. In addition, our method does not require knowing
the optimal objective value. Instead the basic variant of RSG that has a linear convergence only needs to know the value of the multiplicative constant parameter in the local
error bound condition. For problems without knowing this parameter, we also develop a
practical variant of RSG that can achieve a convergence rate close to linear convergence.
In his recent work (Renegar, 2014, 2015, 2016), Renegar presented a framework of applying first-order methods to general conic optimization problems by transforming the original
problem into an equivalent convex optimization problem with only linear equality constraints and a Lipschitz-continuous objective function. This framework greatly extends the
applicability of first-order methods to the problems with general linear inequality constraints
and leads to new algorithms and new iteration complexity. One of his results related to
this work implies (Renegar, 2015, Corollary 3.4), if the objective function has a polyhedral
epigraph and the optimal objective value is known beforehand, a subgradient method can
have a linear convergence rate. Compared to this result of his, our method does not need to
know the optimal objective value. Note that Renegar’s method can be applied in a general
setting where the objective function is not necessarily polyhedral while our method obtains
improved iteration complexities under the local error bound conditions.
More recently, Freund and Lu (2017) proposed a new SG method by assuming that
a strict lower bound of f∗, denoted by fslb, is known and f satisfies a growth condition,
kw − w∗k2 ≤ G · (f(w) − fslb), where w∗
is the optimal solution closest to w and G
is a growth rate constant depending on fslb. Using a novel step size that incorporates
fslb, for non-smooth optimization, their SG method achieves an iteration complexity of
O(G
2
(
log H

0 +
1

02 )) for finding a solution wˆ such that f(wˆ ) − f∗ ≤ 
0
(f∗ − fslb), where
H =
f(w0)−fslb
f∗−fslb
and w0 is the initial solution. We note that there are several key differences
in the theoretical properties and implementations between our work and that by Freund
and Lu (2017): (i) Their growth condition has a similar form to the inequality (7) proved
for a general function but there are still noticeable differences in the both sides and the
growth constants. (ii) The convergence results established by Freund and Lu (2017) are
based on finding an solution wˆ with a relative error of 
0 while we consider absolute error.
(iii) By rewriting the convergence results of Freund and Lu (2017) in terms of absolute
accuracy  with  = 
0
(f∗ −fslb), their algorithm’s complexity depends on f∗ −fslb and may
be higher than ours if f∗ − fslb is large. However, Freund and Lu’s new SG method is still
5
Yang and Lin
attractive due to that it is a parameter free algorithm without requiring the value of the
growth constant G. We will compared our RSG method with the method of Freund and Lu
(2017) with more details in Section 7.
Restarting and multi-stage strategies have been employed to achieve the (uniformly) optimal theoretical complexity of (stochastic) SG methods when f is strongly convex (Ghadimi
and Lan, 2013; Chen et al., 2012; Hazan and Kale, 2011) or uniformly convex (Juditsky and
Nesterov, 2014). Here, we show that restarting can be still helpful even without uniform or
strong convexity. Furthermore, in all the algorithms proposed in existing works (Ghadimi
and Lan, 2013; Chen et al., 2012; Hazan and Kale, 2011; Juditsky and Nesterov, 2014), the
number of iterations per stage increases between stages while our algorithm uses the same
number of iterations in all stages. This provides a different possibility of designing restarted
algorithms for a better complexity only under a local error bound condition.
3. Preliminaries
In this section, we define some notations used in this paper and present the main assumptions needed to establish our results. We use ∂f(w) to denote the set of subgradients (the
subdifferential) of f at w. Since the objective function is not necessarily strongly convex,
the optimal solution is not necessarily unique. We denote by Ω∗ the optimal solution set
and by f∗ the unique optimal objective value. We denote by k · k2 the Euclidean norm in
R
d
.
Throughout the paper, we make the following assumption.
Assumption 1 For the convex minimization problem (1), we assume
a. For any w0 ∈ Ω, we know a constant 0 ≥ 0 such that f(w0) − f∗ ≤ 0.
b. There exists a constant G such that maxv∈∂f(w) kvk2 ≤ G for any w ∈ Ω.
We make several remarks about the above assumptions: (i) Assumption 1.a is equivalent
to assuming we know a lower bound of f∗ which is one of the assumptions made by Freund
and Lu (2017). In machine learning applications, f∗ is usually bounded below by zero, i.e.,
f∗ ≥ 0, so that 0 = f(w0) for any w0 ∈ R
d will satisfy the condition; (ii) Assumption 1.b
is a standard assumption also made in many previous subgradient-based methods.
Let w∗ denote the closest optimal solution in Ω∗ to w measured in terms of norm k · k2,
i.e.,
w∗
:= arg min
u∈Ω∗
ku − wk
2
2
.
Note that w∗
is uniquely defined for any w due to the convexity of Ω∗ and that k · k2
2
is
strongly convex. We denote by L the -level set of f(w) and by S the -sublevel set of
f(w), respectively, i.e.,
L := {w ∈ Ω : f(w) = f∗ + } and S := {w ∈ Ω : f(w) ≤ f∗ + }. (2)
Let B be the maximum distance between the points in the -level set L and the optimal
set Ω∗, i.e.,
B := max
w∈L
min
u∈Ω∗
kw − uk2 = max
w∈L
kw − w∗
k2. (3)
In the sequel, we also make the following assumption.
6
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
Assumption 2 For the convex minimization problem (1), we assume that B is finite.
Remark: B is finite when the optimal set Ω∗ is bounded (e.g., when the objective function
is a proper lower-semicontinuous convex and coercive function). This is because that the
sublevel set S must be bounded for any  ≥ 0 (Rockafellar, 1970, Corollary 8.7.1). Nevertheless, the bounded optimal set is not a necessary condition for a finite B. For example,
f(x) = max(0, x). Although its optimal set is not bounded, B = . In Section 5, we will
consider a broad family of problems with a local error bound condition, which will satisfy
the above assumption.
Let w
†
 denote the closest point in the -sublevel set to w, i.e.,
w†

:= arg min
u∈S
ku − wk
2
2
. (4)
Denote by Ω\S = {w ∈ Ω : w 6∈ S}. It is easy to show that w
†
 ∈ L when w ∈ Ω\S (using
the optimality condition of 4).
Given w ∈ Ω, we denote the normal cone of Ω at w by NΩ(w). Formally, NΩ(w) =
{v ∈ R
d
: v
>(u − w) ≤ 0, ∀u ∈ Ω}. Define dist(0, f(w) + NΩ(w)) as
dist(0, f(w) + NΩ(w)) := min
g∈∂f(w),v∈NΩ(w)
kg + vk2. (5)
Note that w ∈ Ω∗ if and only if dist(0, f(w)+NΩ(w)) = 0. Therefore, we call dist(0, f(w)+
NΩ(w)) the first-order optimality residual of (1) at w ∈ Ω. Given any  > 0 such that
L 6= ∅, we define a constant ρ as
ρ := min
w∈L
dist(0, f(w) + NΩ(w)). (6)
Given the notations above, we provide the following lemma which is the key to our
analysis.
Lemma 1 For any  > 0 such that L 6= ∅ and any w ∈ Ω, we have
kw − w†
k2 ≤
1
ρ
(f(w) − f(w†

)). (7)
Proof Since the conclusion holds trivially if w ∈ S (so that w
†
 = w), we assume w ∈ Ω\S.
According to the first-order optimality conditions of (4), there exist a scalar ζ ≥ 0 (the
Lagrangian multiplier of the constraint f(u) ≤ f∗ +  in 4), a subgradient g ∈ ∂f(w
†
 ) and
a vector v ∈ NΩ(w
†
 ) such that
w†
 − w + ζg + v = 0. (8)
The definition of normal cone leads to (w
†
 − w)
>v ≥ 0. This inequality and the convexity
of f(·) imply
ζ

f(w) − f(w†

)

≥ ζ(w − w†

)
>g ≥ (w − w†

)
> (ζg + v) = kw − w†
k
2
2
,
7
Yang and Lin
where the equality is due to (8). Since w ∈ Ω\S, we must have kw − w
†
k2 > 0 so that
ζ > 0. Therefore, w
†
 ∈ L by complementary slackness. Dividing the inequality above by
ζ gives
f(w) − f(w†

) ≥
kw − w
†
k
2
2
ζ
= kw − w†
k2kg + v/ζk2 ≥ ρkw − w†
k2, (9)
where the equality is due to (8) and the last inequality is due to the definition of ρ in (6).
The lemma is then proved.
The inequality in (7) is the key to achieve improved convergence by RSG, which hinges
on the condition that the first-order optimality residual on the -level set is lower bounded.
It is important to note that (i) the above result depends on f rather than the optimization
algorithm applied; and (ii) the above result can be generalized to using other norms such
as the p-norm kwkp (p ∈ (1, 2]) to measure the distance between w and w
†
 and using the
corresponding dual norm to define the lower bound of the residual in (5) and (6). This
generalization allows one to design mirror decent (Nemirovski et al., 2009) variant of RSG.
To our best knowledge, this is the first work that leverages the lower bound of the optimal
residual to improve the convergence for non-smooth convex optimization.
In the next several sections, we will exhibit the value of ρ for different classes of problems
and discuss its impact on the convergence. In the sequel, we abuse the Big O notation
T = O(h()) to mean that there exists a constant C > 0 independent of  such that
T ≤ Ch().
4. Restarted SubGradient (RSG) Method and Its Generic Complexity
for General Problem
In this section, we present a framework of restarted subgradient (RSG) method and prove
its general convergence result using Lemma 1. It will be noticed that the algorithmic results
developed in this section is less interesting from the viewpoint of practice. However, it will
exhibit the insights for the improvements and provide the template for the developments in
next several sections, where we will present improved convergence of RSG for problems of
different classes.
The steps of RSG are presented in Algorithm 2 where SG is a subroutine of projected
subgradient method given in Algorithm 1 and ΠΩ[w] is defined as
ΠΩ[w] = arg min
u∈Ω
ku − wk
2
2
.
The values of K and t in RSG will be revealed later for proving the convergence of RSG to
an 2-optimal solution. The number of iterations t is the only varying parameter in RSG
that depends on the classes of problems. The parameter α could be any value larger than
1 (e.g., 2) and it only has a small influence on the iteration complexity.
We emphasize that (i) RSG is a generic algorithm that is applicable to a broad family
of non-smooth and/or non-strongly convex problems without changing updating schemes
except for one tuning parameter, the number of iterations per stage, whose best value
varies with problems; (ii) RSG has different variants with different subroutines in stages. In
8
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
Algorithm 1 SG: wb T = SG(w1, η, T)
1: Input: a step size η, the number of iterations T, and the initial solution w1 ∈ Ω
2: for τ = 1, . . . , T do
3: Query the subgradient oracle to obtain G(wτ ) ∈ ∂f(wτ )
4: Update wτ+1 = ΠΩ[wτ − ηG(wτ )]
5: end for
6: Output: wb T =
PT
τ=1
wτ
T
Algorithm 2 RSG: wK = RSG(w0, K, t, α)
1: Input: the number of stages K and the number of iterations t per-stage, w0 ∈ Ω, and
α > 1.
2: Set η1 = 0/(αG2
), where 0 is from Assumption 1.a
3: for k = 1, . . . , K do
4: Call subroutine SG to obtain wk = SG(wk−1, ηk, t)
5: Set ηk+1 = ηk/α
6: end for
7: Output: wK
fact, we can use other optimization algorithms than SG as the subroutine in Algorithm 2,
as long as a similar convergence result to Lemma 2 is guaranteed. Examples include dual
averaging (Nesterov, 2009) and the regularized dual averaging (Chen et al., 2012) in the nonEuclidean space. In the following discussions, we will focus on using SG as the subroutine.
Next, we establish the convergence of RSG. It relies on the convergence result of the SG
subroutine which is given in the lemma below.
Lemma 2 (Zinkevich, 2003; Nesterov, 2004) If Algorithm 1 runs for T iterations, we have,
for any w ∈ Ω,
f(wb T ) − f(w) ≤
G2η
2
+
kw1 − wk
2
2
2ηT
.
We omit the proof because it follows a standard analysis and can be found in cited papers.
With the above lemma, we can prove the following convergence of RSG.
Theorem 3 Suppose Assumption 1 and 2 holds. If t ≥
α
2G2
ρ
2

and K = dlogα(
0

)e in
Algorithm 2, with at most K stages, Algorithm 2 returns a solution wK such that f(wK) −
f∗ ≤ 2. The total number of iterations for Algorithm 2 to find an 2-optimal solution is at
most T = tdlogα(
0

)e where t ≥
α
2G2
ρ
2

.
Remark: If t also satisfies t = O

α
2G2
ρ
2


, then the iteration complexity of Algorithm 2 for
finding an -optimal solution is O

α
2G2
ρ
2

dlogα(
0

)e

.
Proof
Let w
†
k, denote the closest point to wk in the -sublevel set. Let k := 0
αk so that
ηk = k/G2 because η1 = 0/(αG2
) and ηk+1 = ηk/α. We will show by induction that
f(wk) − f∗ ≤ k + , (10)
9
Yang and Lin
for k = 0, 1, . . . , K which leads to our conclusion if we let k = K.
Note that (10) holds obviously for k = 0. Suppose it holds for k −1, namely, f(wk−1)−
f∗ ≤ k−1 + . We want to prove (10) for k. We apply Lemma 2 to the k-th stage of
Algorithm 2 and get
f(wk) − f(w
†
k−1,
) ≤
G2ηk
2
+
kwk−1 − w
†
k−1,
k
2
2
2ηkt
. (11)
We now consider two cases for wk−1. First, assume f(wk−1)−f∗ ≤ , i.e., wk−1 ∈ S. Then
w
†
k−1, = wk−1 and f(wk) − f(w
†
k−1,
) ≤
G2ηk
2 =
k
2
. As a result,
f(wk) − f∗ ≤ f(w
†
k−1,
) − f∗ +
k
2
≤  + k.
Next, we consider the case that f(wk−1)−f∗ > , i.e., wk−1 6∈ S. Then we have f(w
†
k−1,
) =
f∗ + . By Lemma 1, we have
kwk−1 − w
†
k−1,
k2 ≤
1
ρ
(f(wk−1) − f(w
†
k−1,
)) =
f(wk−1) − f∗ + (f∗ − f(w
†
k−1,
))
ρ
≤
k−1 +  − 
ρ
.
(12)
Combining (11) and (12) and using the facts that ηk =
k
G2
and t ≥
α
2G2
ρ
2

, we have
f(wk) − f(w
†
k−1,
) ≤
k
2
+

2
k−1
2kα2
= k,
which, together with the fact that f(w
†
k−1,
) = f∗ + , implies (10) for k. Therefore, by
induction, we have (10) holds for k = 1, 2, . . . , K so that
f(wK) − f∗ ≤ K +  =
0
αK
+  ≤ 2,
where the last inequality is due to the definition of K.
In Theorem 3, the iteration complexity of RSG for the general problem (1) is given
in terms of ρ. Next, we show that ρ ≥

B
, which allows us to leverage the local error
bound condition in next sections to upper bound B to obtain specialized and more practical
algorithms for different classes of problems.
Lemma 4 For any  > 0 such that L 6= ∅, we have ρ ≥

B
, where B is defined in (3),
and for any w ∈ Ω
kw − w†
k2 ≤
kw
†
 − w∗
 k2

(f(w) − f(w†

)) ≤
B

(f(w) − f(w†

)), (13)
where w∗

is the closest point in Ω∗ to w
†
 .
10
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
ǫ
w
†
ǫ
w
B
B ≤ dist(w
†
ǫ
, Ω∗)
kw − w
†
ǫ
k
A ≤ f(w) − f(w
†
ǫ
)
w∗
θ
ǫ
dist(w
†
ǫ
,Ω∗)
≤
ǫ
B
= tan θ =
A
kw−w
†
ǫ
k
≤
f(w)−f(w
†
ǫ
)
kw−w
†
ǫ
k
f(·)
Figure 1: A geometric illustration of the inequality (13), where dist(w
†
 , Ω∗) = |w
†
 − w
∗

|.
Proof Given any u ∈ L, let gu be any subgradient in ∂f(u) and vu be any vector in
NΩ(u). By the convexity of f(·) and the definition of normal cone, we have
f(u
∗
) − f(u) ≥ (u
∗ − u)
>gu ≥ (u
∗ − u)
> (gu + vu),
where u
∗
is the closest point in Ω∗ to u. This inequality further implies
ku
∗ − uk2kgu + vuk2 ≥ f(u) − f(u
∗
) = , ∀gu ∈ ∂f(u) and vu ∈ NΩ(u), (14)
where the equality is because u ∈ L. By (14) and the definition of B, we obtain
Bkgu + vuk2 ≥  =⇒ kgu + vuk2 ≥ /B.
Since gu + vu can be any element in ∂f(u) + NΩ(u), we have ρ ≥

B
by the definition (6).
To prove (13), we assume w ∈ Ω\S and thus w
†
 ∈ L; otherwise it is trivial. In the
proof of Lemma 1, we have shown that (see (9)) there exists g ∈ ∂f(w
†
 ) and v ∈ NΩ(w
†
 )
such that f(w) − f(w
†
 ) ≥ kw − w
†
k2kg + v/ζk2, which, according to (14) with u = w
†
 ,
gu = g and vu = v/ζ, leads to (13).
A geometric explanation of the inequality (13) in one dimension is shown in Figure 1. With
Lemma 4, the iteration complexity of RSG can be stated in terms of B in the following
corollary of Theorem 3.
Corollary 5 Suppose Assumption 1 holds. The iteration complexity of RSG for obtaining
an 2-optimal solution is O(
α
2G2B2


2 dlogα(
0

)e) provided t =
α
2G2B2


2 and K = dlogα(
0

)e.
11
Yang and Lin
We will compare this result with SG in Section 7. Compared to the standard SG, the above
improved result of RSG does require knowing strong knolwedge about f. In particular, one
issue is that the above improved complexity is obtained by choosing t =
α
2G2B2


2 , which
requires knowing the order of magnitude of B, if not its exact value. To address the issue
of unknown B for general problems, in the next section, we consider different families
of problems that admit a local error bound condition and show that the requirement of
knowing B is relaxed to knowing some particular parameters related to the local error
bound.
5. RSG for Some Classes of Non-smooth Non-strongly Convex
Optimization
In this section, we consider a particular family of problems that admit local error bounds
and show the improved iteration complexities of RSG compared to standard SG method.
5.1 Complexity for the Problems with Local Error Bounds
We first define a local error bound condition of the objective function.
Definition 6 We say f(·) admits a local error bound on the -sublevel set S if
kw − w∗
k2 ≤ c(f(w) − f∗)
θ
, ∀w ∈ S, (15)
where w∗
is the closet point in Ω∗ to w, θ ∈ (0, 1] and 0 < c < ∞ are constants.
Because S2 ⊂ S1
for 2 ≤ 1, if (15) holds for some , it will always hold when  decreases
to zero with the same θ and c. Indeed, a smaller  may induce a smaller value of c. It is
notable that the local error bound condition has been extensively studied in the community of optimization, mathematical programming and variational analysis (Yang, 2009; Li,
2010, 2013; Artacho and Geoffroy, 2008; Kruger, 2015; Drusvyatskiy et al., 2014; Li and
Mordukhovich, 2012; Hou et al., 2013; Zhou and So, 2017; Zhou et al., 2015), to name just
a few of them. The value of θ has been exhibited for many problems. For certain problems,
the value of c is also computable (see Bolte et al., 2017).
If the problem admits a local error bound like (15), RSG can achieve a better iteration
complexity than O(1/2
). In particular, the property (15) implies
B ≤ cθ
. (16)
Replacing B in Corollary 5 by this upper bound and choosing t =
α
2G2
c
2

2(1−θ)
in RSG if c and
θ are known, we obtain the following complexity of RSG.
Corollary 7 Suppose Assumption 1 holds and f(·) admits a local error bound on S. The
iteration complexity of RSG for obtaining an 2-optimal solution is O

α
2G2
c
2

2(1−θ)
logα

0



provided t =
α
2G2
c
2

2(1−θ) and K = dlogα(
0

)e.
Remark: If t = Θ( α
2G2

2(1−θ)
) >
α
2G2
c
2

2(1−θ)
, then the same order of iteration complexity remains.
If one aims to find a point w such that kw−w∗k2 ≤ , we can apply RSG to find a solution w
1 
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
such that f(w) − f∗ ≤ (/c)
1/θ ≤  (where the last inequality is due to θ ≤ 1 and assuming
c ≥ 1 without loss of generality). Then under the local error bound condition, we have
kw−w∗k2 ≤ c(f(w)−f∗)
θ ≤ . For finding a solution w such that f(w)−f∗ ≤ (/c)
1/θ ≤ ,
RSG requires an iteration complexity of Oe(
1

2(1−θ)/θ ). Therefore, in order to find a solution
w such that kw − w∗k2 ≤ , the iteration complexity of RSG is Oe(
1

2(1−θ)/θ ).
Next, we will consider different convex optimization problems that admit a local error
bound on S with different θ and show the faster convergence of RSG when applied to these
problems.
5.2 Linear Convergence for Polyhedral Convex Optimization
In this subsection, we consider a special family of non-smooth and non-strongly convex
problems where the epigraph of f(·) over Ω is a polyhedron. In this case, we call (1) a
polyhedral convex minimization problem. We show that, in polyhedral convex minimization problem, f(·) has a linear growth property and admits a local error bound with
θ = 1 so that B ≤ c for a constant c < ∞.
Lemma 8 (Polyhedral Error Bound Condition) Suppose Ω is a polyhedron and the
epigraph of f(·) is also polyhedron. There exists a constant κ > 0 such that
kw − w∗
k2 ≤
f(w) − f∗
κ
, ∀w ∈ Ω.
Thus, f(·) admits a local error bound on S with θ = 1 and c =
1
κ
3
(so B ≤

κ
) for any
 > 0.
Remark: The above inequality is also known as weak sharp minimum condition in literature (Burke and Ferris., 1993; Studniarski and Ward, 1999; Ferris, 1991; Burke and Deng,
2002, 2005, 2009). A proof of Lemma 8 is given by Burke and Ferris. (1993). We also provide a proof (see Yang and Lin, 2016). We remark that the above result can be extended to
any valid norm to measure the distance between w and w∗. Lemma 8 generalizes Lemma
4 of Gilpin et al. (2012), which requires Ω to be a bounded polyhedron, to a similar result
where Ω can be an unbounded polyhedron. This generalization is simple but useful because
it helps the development of efficient algorithms based on this error bound for unconstrained
problems without artificially including a box constraint.
Lemma 8 provides the basis for RSG to achieve a linear convergence for the polyhedral
convex minimization problems. In fact, the following linear convergence of RSG can be
obtained if we plugin the values of θ = 1 and c =
1
κ
into Corollary 7.
Corollary 9 Suppose Assumption 1 holds and (1) is a polyhedral convex minimization problem. The iteration complexity of RSG for obtaining an -optimal solution is O(
α
2G2
κ2 dlogα(
0

)e)
provided t =
α
2G2
κ2 and K = dlogα(
0

)e.
We want to point out that Corollary 9 can be proved directly by replacing w
†
k−1,
by w∗
k−1
and replacing ρ by κ in the proof of Theorem 3. Here, we derive it as a corollary of a
3. In fact, this property of f(·) is a global error bound on Ω.
13
Yang and Lin
more general result. We also want to mention that, as shown by Renegar (2015), the linear
convergence rate in Corollary 9 can be also obtained by a SG method for the historically
best solution, provided f∗ is known.
5.2.1 Examples
Many non-smooth and non-strongly convex machine learning problems satisfy the assumptions of Corollary 9, for example, `1 or `∞ constrained or regularized piecewise linear
loss minimization. In many machine learning tasks (e.g., classification and regression),
there exists a set of data {(xi
, yi)}i=1,2,...,n and one often needs to solve the following empirical risk minimization problem
min
w∈Rd
f(w) ,
1
n
Xn
i=1
`(w>xi
, yi) + R(w),
where R(w) is a regularization term and `(z, y) denotes a loss function. We consider a
special case where (a) R(w) is a `1 regularizer, `∞ regularizer or an indicator function of a
`1/`∞ ball centered at zero; and (b) `(z, y) is any piecewise linear loss function, including
hinge loss `(z, y) = max(0, 1 − yz), absolute loss `(z, y) = |z − y|, -insensitive loss `(z, y) =
max(|z −y| −, 0), and etc (Yang et al., 2014). It is easy to show that the epigraph of f(w)
is a polyhedron if f(w) is defined as a sum of any of these regularization terms and any of
these loss functions. In fact, a piecewise linear loss functions can be generally written as
`(w>x, y) = max
1≤j≤m
ajw>x + bj , (17)
where (aj , bj ) for j = 1, 2, . . . , m are finitely many pairs of scalars. The formulation (17)
indicates that `(w>x, y) is a piecewise affine function so that its epigraph is a polyhedron.
In addition, the `1 or `∞ norm is also a polyhedral function because we can represent them
as
kwk1 =
X
d
i=1
max(wi
, −wi), kwk∞ = max
1≤i≤d
|wi
| = max
1≤i≤d
max(wi
, −wi).
Since the sum of finitely many polyhedral functions is also a polyhedral function, the epigraph of f(w) is a polyhedron.
Another important family of problems whose objective function has a polyhedral epigraph is submodular function minimization. Let V = {1, . . . , d} be a set and 2V
denote its power set. A submodular function F(A) : 2V → R is a set function such that
F(A)+F(B) ≥ F(A∪B)+F(A∩B) for all subsets A, B ⊆ V and F(∅) = 0. A submodular
function minimization can be cast into a non-smooth convex optimization using the Lov´asz
extension (Bach, 2013). In particular, let the base polyhedron B(F) be defined as
B(F) = {s ∈ R
d
, s(V ) = F(V ), ∀A ⊆ V, s(A) ≤ F(A)},
where s(A) = P
i∈A
si
. Then the Lov´asz extension of F(A) is f(w) = maxs∈B(F) w>s,
and minA⊆V F(A) = minw∈[0,1]d f(w). As a result, a submodular function minimization is
essentially a non-smooth and non-strongly convex optimization with a polyhedral epigraph.
14
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
5.3 Improved Convergence for Locally Semi-Strongly Convex Problems
First, we give a definition of local semi-strong convexity.
Definition 10 A function f(w) is semi-strongly convex on the -sublevel set S if there
exists λ > 0 such that
λ
2
kw − w∗
k
2
2 ≤ f(w) − f(w∗
), ∀w ∈ S, (18)
where w∗
is the closest point to w in the optimal set.
We refer to the property (18) as local semi-strong convexity when S 6= Ω. The two papers
(Gong and Ye, 2014; Necoara et al., 2015) have explored the semi-strong convexity on the
whole domain Ω to prove linear convergence of smooth optimization problems. In some
literature (Necoara et al., 2015), the inequality (18) is also called second-order growth
property. Necoara et al. (2015) have also shown that a class of problems satisfy (18) (see
examples given below). The inequality (18) indicates that f(·) admits a local error bound
on S with θ =
1
2
and c =
q
2
λ
, which leads to the following the corollary about the iteration
complexity of RSG for locally semi-strongly convex problems.
Corollary 11 Suppose Assumption 1 holds and f(w) is semi-strongly convex on S. Then
B ≤
q
2
λ
4and the iteration complexity of RSG for obtaining an 2-optimal solution is
O(
2α
2G2
λ dlogα(
0

)e) provided t =
2α
2G2
λ and K = dlogα(
0

)e.
Remark: Here, we obtain an Oe(1/) iteration complexity (Oe(·) suppresses constants and
logarithmic terms) only with local semi-strong convexity. It is obvious that strong convexity
implies local semi-strong convexity (Hazan and Kale, 2011) but not vice versa.
For examples, let us consider a family of functions in the form of f(w) = h(Xw)+r(w),
where X ∈ R
n×d
, h(·) is strongly convex on any compact set and r(·) has a polyhedral
epigraph. According to (Gong and Ye, 2014; Necoara et al., 2015), such a function f(w)
satisfies (18) for any  ≤ 0 with a constant value for λ. Although smoothness is assumed
for h(·) in (Gong and Ye, 2014; Necoara et al., 2015), we find that it is not necessary for
proving (18). We state this result as the lemma below.
Lemma 12 Suppose Assumption 1 holds, Ω = {w ∈ R
d
|Cw ≤ b} with C ∈ R
k×d and
b ∈ R
k
, and f(w) = h(Xw) + r(w) where h : R
n → R satisfies dom(h) = R
k and is a
strongly convex function on any compact set in R
n
, and r(w) has a polyhedral epigraph.
Then, f(w) satisfies (18) for any  ≤ 0.
The proof of this lemma can be duplicated following analysis in some existing works (Gong
and Ye, 2014; Necoara et al., 2015; Necoara and Clipici, 2016). For example, it is almost
identical to the proof of Lemma 1 by Gong and Ye (2014) which assumes h(·) is smooth.
However, a similar result holds without the smoothness of h(·).
4. Recall (16).
15
Yang and Lin
The function of this type covers some commonly used loss functions and regularization
terms in machine learning and statistics. For example, we can consider robust regression
with/without `1 regularizer (Xu et al., 2010; Bertsimas and Copenhaver, 2014):
min
w∈Ω
1
n
Xn
i=1
|x
>
i w − yi
|
p + λkwk1, (19)
where p ∈ (1, 2), xi ∈ R
d denotes the feature vector and yi
is the target output. The objective function is in the form of h(Xw) + r(w) where X is a n × d matrix with x1, x2, . . . , xn
being its rows and h(u) := Pn
i=1 |ui − yi
|
p
. According to Goebel and Rockafellar (2007),
h(u) is a strongly convex function on any compact set so that the objective function above
is semi-strongly convex on S for any  ≤ 0.
5.4 Improved Convergence for Convex Problems with KL property
Lastly, we consider a family of non-smooth functions with a local Kurdyka- Lojasiewicz (KL)
property. The definition of KL property is given below.
Definition 13 The function f(w) has the Kurdyka - Lojasiewicz (KL) property at w¯ if
there exist η ∈ (0, ∞], a neighborhood Uw¯ of w¯ and a continuous concave function ϕ :
[0, η) → R+ such that (i) ϕ(0) = 0; (ii) ϕ is continuous on (0, η); (iii) for all s ∈ (0, η),
ϕ
0
(s) > 0; (iv) and for all w ∈ Uw¯ ∩ {w : f(w¯ ) < f(w) < f(w¯ ) + η}, the Kurdyka -
 Lojasiewicz (KL) inequality holds
ϕ
0
(f(w) − f(w¯ ))k∂f(w)k2 ≥ 1, (20)
where k∂f(w)k2 := ming∈∂f(w) kgk2.
The function ϕ is called the desingularizing function of f at w¯ , which sharpens the
function f(w) by reparameterization. An important desingularizing function is in the form
of ϕ(s) = cs1−β
for some c > 0 and β ∈ [0, 1), by which, (20) gives the KL inequality
k∂f(w)k2 ≥
1
c(1 − β)
(f(w) − f(w¯ ))β
.
Note that all semi-algebraic functions satisfy the KL property at any point (Bolte et al.,
2014). Indeed, all the concrete examples given before satisfy the Kurdyka - Lojasiewicz
property. For more discussions about the KL property, we refer readers to some previous
works (Bolte et al., 2014, 2007; Schneider and Uschmajew, 2015; Attouch et al., 2013;
Bolte et al., 2006). The following corollary states the iteration complexity of RSG for
unconstrained problems that have the KL property at each w¯ ∈ Ω∗ .
Corollary 14 Suppose Assumption 1 holds, f(w) satisfies a (uniform) Kurdyka - Lojasiewicz
property at any w¯ ∈ Ω∗ with the same desingularizing function ϕ and constant η, and
S ⊂ ∪w¯ ∈Ω∗
[Uw¯ ∩ {w : f(w¯ ) < f(w) < f(w¯ ) + η}] . (21)
RSG has an iteration complexity of O

α
2G2
(
ϕ()

)
2
dlogα(
0

)e

for obtaining an 2-optimal
solution provided t = α
2G2
(ϕ()/)
2
. In addition, if ϕ(s) = cs1−β
for some c > 0 and
β ∈ [0, 1), the iteration complexity of RSG is O(
α
2G2
c
2
(1−β)
2

2β dlogα(
0

)e) provided t =
α
2G2
c
2

2β
and K = dlogα(
0

)e.
16
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
Proof We can prove the above corollary following a result by Bolte et al. (2017) as presented
in Proposition 1 in the Appendix. According to Proposition 1, if f(·) satisfies the KL
property at w¯ , then for all w ∈ Uw¯ ∩ {w : f(w¯ ) < f(w) < f(w¯ ) + η} it holds that
kw − w∗k2 ≤ ϕ(f(w) − f(w¯ )). It then, under the uniform condition in (21), implies that,
for any w ∈ S
kw − w∗
k2 ≤ ϕ(f(w) − f∗) ≤ ϕ(),
where we use the monotonic property of ϕ. Then the first conclusion follows similarly as
Corollary 5 by noting B ≤ ϕ(). The second conclusion immediately follows by setting
ϕ(s) = cs1−β
in the first conclusion. Please note that the above inequality implies the local
error bound condition with θ = 1 − β for ϕ(s) = cs1−β
.
While the conclusion in Corollary 14 hinges on a condition in (21) for certain Uw¯ and η,
in practice many convex functions (e.g., continuous semi-algebraic or subanalytic functions)
satisfy the KL property with U = R
d and any finite η < ∞ (Attouch et al., 2010; Bolte
et al., 2017; Li, 2010).
It is worth mentioning that to our best knowledge, the present work is the first to
leverage the KL property for developing improved subgradient methods, though it has
been explored in non-convex and convex optimization for deterministic descent methods for
smooth optimization (Bolte et al., 2017, 2014; Attouch et al., 2010; Karimi et al., 2016). For
example, Bolte et al. (2017) studied the convergence of subgradient descent sequence
for minimizing a convex function under an error bound condition. A sequence {xk} is
called a subgradient descent sequence if there exist a > 0, b > 0 it satisfies two conditions,
namely sufficient decrease condition f(xk) + akxk − xk−1k
2
2 ≤ f(xk−1), and relative error
condition, i.e., there exists ωk ∈ ∂f(xk) such that kωkk2 ≤ bkxk − xk−1k2. However, for
a general non-smooth function f(x), the sequence generated by subgradient method, i.e.,
xk = xk−1−ηk∂∇f(xk−1) do not necessarily satisfy the above two conditions. Instead, Bolte
et al. (2017) considered proximal gradient method that only applies to a smaller family of
functions consisting of a smooth component and a non-smooth component by assuming the
proximal mapping for the non-smooth component can be efficiently computed. In contrast,
our algorithm and analysis are developed for much general non-smooth functions.
6. Variants of RSG without knowing the constant c and the exponent θ
in the local error bound
In Section 5, we have discussed the local error bound and presented several classes of
problems to reveal the magnitude of B, i.e., B = cθ
. For some problems, the value of
θ is exhibited. However, the value of the constant c could be still difficult to estimate,
which renders it challenging to set the appropriate value t =
α
2
c
2G2

2(1−θ)
for inner iterations of
RSG. In practice, one might use a sufficiently large c to set up the value of t. However,
such an approach might be vulnerable to both over-estimation and under-estimation of t.
Over-estimating the value of t leads to a waste of iterations while under-estimation leads to
an less accurate solution that might not reach to the target accuracy level. In addition, for
some problems the value of θ is still an open problem. One interesting family of objective
functions in machine learning is the sum of piecewise linear loss over training data and a
17
Yang and Lin
nuclear norm regularizer or an overlapped or non-overlapped group lasso regularizer. In this
section, we present variants of RSG that can be implemented without knowing the value
of c in the local error bound condition and even the value of exponent θ, and prove their
improved convergence over the SG method.
6.1 RSG without knowing c
The key idea is to use an increasing sequence of t and another level of restarting for RSG.
The detailed steps are presented in Algorithm 3, to which we refer as R2SG. With large
enough t1 in R2SG, the complexity of R2SG for finding an  solution is given by the theorem
below.
Theorem 15 Suppose  ≤ 0/4 and K = dlogα(0/)e. Let t1 in Algorithm 3 be large
enough so that there exists ˆ1 ∈ (, 0/2), with which f(·) satisfies a local error bound
condition on Sˆ1 with θ ∈ (0, 1) and the constant cˆ, and t1 =
α
2
cˆ
2G2
ˆ
2(1−θ)
1
. Then, with at
most S = dlog2
(ˆ1/)e + 1 calls of RSG in Algorithm 3, we find a solution wS
such that
f(wS
) − f∗ ≤ 2. The total number of iterations of R2SG for obtaining 2-optimal solution
is upper bounded by TS = O

cˆ
2G2

2(1−θ) dlogα(
0

)e

.
Proof Since K = dlogα(0/)e ≥ dlogα(0/ˆ1)e and t1 =
α
2
cˆ
2G2
ˆ
2(1−θ)
1
, we can apply Corollary 7
with  = ˆ1 to the first call of RSG in Algorithm 3 so that the output w1
satisfies
f(w1
) − f∗ ≤ 2ˆ1. (22)
Then, we consider the second call of RSG with the initial solution w1
satisfying (22). By
the setup K = dlogα(0/)e ≥ dlogα(2ˆ1/(ˆ1/2))e and t2 = t12
2(1−θ) =
cˆ
2G2
(ˆ1/2)2(1−θ)
, we can
apply Corollary 7 with  = ˆ1/2 and 0 = 2ˆ1 so that the output w2 of the second call
satisfies f(w2
) − f∗ ≤ ˆ1. By repeating this argument for all the subsequent calls of RSG,
with at most S = dlog2
(ˆ1/)e + 1 calls, Algorithm 3 ensures that
f(wS
) − f∗ ≤ 2ˆ1/2
S−1 ≤ 2.
The total number of iterations during the S calls of RSG is bounded by
TS = K
X
S
s=1
ts = K
X
S
s=1
t12
2(s−1)(1−θ) = Kt12
2(S−1)(1−θ)X
S
s=1

1
2
2(1−θ)
S−s
≤
Kt12
2(S−1)(1−θ)
1 − 1/2
2(1−θ)
≤ O

Kt1

ˆ1

2(1−θ)
!
= O

cˆ
2G2

2(1−θ)
dlogα(
0

)e

.
Remark: We make several remarks about Algorithm 3 and Theorem 15: (i) Theorem 15
applies only when θ ∈ (0, 1). If θ = 1, in order to have an increasing sequence of ts, we can
set θ in Algorithm 3 to a little smaller value than 1 in practical implementation, and the
18
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
Algorithm 3 RSG with restarting: R2SG
1: Input: the number of iterations t1 in each stage of the first call of RSG and the number
of stages K in each call of RSG
2: Initialization: w0 ∈ Ω;
3: for s = 1, 2 . . . , S do
4: Let ws = RSG(ws−1
, K, ts, α)
5: Let ts+1 = ts2
2(1−θ)
6: end for
iteration complexity in Theorem 15 implies that R2SG can enjoy a convergence rate close
to linear convergence for problems satisfying the weak sharp minimum condition. (ii) the 0
in the implementation of RSG (Algorthm 2) can be re-calibrated for s ≥ 2 to improve the
performance (e.g., one can use the relationship f(ws−1) − f∗ = f(ws−2) − f∗ + f(ws−1) −
f(ws−2) to do re-calibration); (iii) as a tradeoff, the exiting criterion of R2SG is not as
automatic as RSG. In fact, the total number of calls S of RSG for obtaining an 2-optimal
solution depends on an unknown parameter (namely ˆ1). In practice, one could use other
stopping criteria to terminate the algorithm. For example, in machine learning applications
one can monitor the performance on the validation data set to terminate the algorithm.
(vi) The quantities ˆ1, S in the proof above are implicitly determined by t1 and one does
not need to compute ˆ1 and S in order to apply Algorithm 3. Finally, we note that when
a local strong convexity condition holds on Sˆ1 with ˆ1 ≥  one might derive an iteration
complexity of O(1/) for SG by first showing that SG converges to Sˆ1 with a number of
iterations independent of , then showing that the iterates stay within Sˆ1
and converge to
an -level set with an iteration complexity of O(1/) following existing analysis of SG for
strongly convex functions, e.g., (Lacoste-Julien et al., 2012). However, it still needs to know
the value of the local strong convexity parameter unlike our result in Theorem 15 that does
not need to known the local strong convexity parameter.
6.2 RSG for unknown θ and c
Without knowing θ ∈ (0, 1] and c to get a sharper local error bound, we can simply let
θ = 0 and c = B
0 with 
0 ≥ , which still render the inequaity (15) hold (c.f. Definition 6).
Then we can employ the same trick to increase the values of t. In particular, we start with
a sufficiently large value of t and run RSG with K = dlogα(0/)e stages, and then increase
the value of t by a factor of 4 and repeat the process.
Theorem 16 Let θ = 0 in Algorithm 3 and suppose  ≤ 0/4 and K = dlogα(0/)e.
Assume t1 in Algorithm 3 is large enough so that there exists ˆ1 ∈ (, 0/2] giving t1 =
α
2B2
ˆ1
G2
ˆ
2
1
. Then, with at most S = dlog2
(ˆ1/)e + 1 calls of RSG in Algorithm 3, we find
a solution wS
such that f(wS
) − f∗ ≤ 2. The total number of iterations of R2SG for
obtaining 2-optimal solution is upper bounded by TS = O

B2
ˆ1
G2

2 dlogα(
0

)e

.
19
Yang and Lin
Remark: Since B/ is a monotonically decreasing function in  (Xu et al., 2017, Lemma
7), such a t1 in Theorem 16 exists. Note that if the problem satisfies a KL property as in
Corollary 14 and the value of β is unknown, the above theorem still holds.
Proof The proof is similar to that of Theorem 15 except that we let c = Bˆ1
and θ = 0.
Since K = dlogα(0/)e ≥ dlogα(0/ˆ1)e and t1 =
α
2B2
ˆ1
G2
ˆ
2
1
, we can apply Corollary 5 with
 = ˆ1 to the first call of RSG in Algorithm 3 so that the output w1
satisfies
f(w1
) − f∗ ≤ 2ˆ1. (23)
Then, we consider the second call of RSG with the initial solution w1
satisfying (23). By
the setup K = dlogα(0/)e ≥ dlogα(2ˆ1/(ˆ1/2))e and t2 = t12
2 =
B2
ˆ1
G2
(ˆ1/2)2 , we can apply
Corollary 5 with  = ˆ1/2 and 0 = 2ˆ1 (noting that Bˆ1 > Bˆ1/2
) so that the output w2 of
the second call satisfies f(w2
) − f∗ ≤ ˆ1. By repeating this argument for all the subsequent
calls of RSG, with at most S = dlog2
(ˆ1/)e + 1 calls, Algorithm 3 ensures that
f(wS
) − f∗ ≤ 2ˆ1/2
S−1 ≤ 2.
The total number of iterations during the S calls of RSG is bounded by
TS = K
X
S
s=1
ts = K
X
S
s=1
t12
2(s−1) = Kt12
2(S−1)X
S
s=1

1
2
2
S−s
≤
Kt12
2(S−1)
1 − 1/2
2
≤ O

Kt1

ˆ1

2
!
= O

B2
ˆ1
G2

2
dlogα(
0

)e
!
.
7. Discussions and Comparisons
In this section, we further discuss the obtained results and compare them with existing
results.
7.1 Comparison with the standard SG
The standard SG’s iteration complexity is known as O(
G2kw0−w∗
0
k
2
2

2 ) for achieving an 2-
optimal solution. By assuming t is appropriately set in RSG according to Corollary 5, its
iteration complexity is O(
G2B2


2 log(0/)), which depends on B2

instead of kw0 − w∗
0
k
2
2
and
only has a logarithmic dependence on 0, the upper bound of f(w0) − f∗. When the initial
solution is far from the optimal set so that B2
  kw0 − w∗
0
k
2
2
, RSG could have a lower
worst-case complexity. Even if t is not appropriately set up to be larger than α
2G2B2
 /2
,
Theorem 16 guarantees that the proposed R2SG could still has a lower iteration complexity
than that of SG as long as t1 is sufficiently large. In some special cases, e.g., when f satisfies
the local error bound condition (15) with θ ∈ (0, 1], RSG only needs O

1

2(1−θ)
log
1



iterations (see Corollary 7 and Theorem 15), which has a better dependency on  than the
complexity of standard SG method.
2 
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
7.2 Comparison with the SG method by Freund and Lu (2017)
Freund and Lu (2017) introduced a similar but different growth condition:
kw − w∗
k2 ≤ G · (f(w) − fslb), ∀w ∈ Ω, (24)
where fslb is a strict lower bound of f∗. The main differences from our key condition (7)
are: the left-hand side is the distance of w to the optimal set in (24) while it is the distance
of w to the -sublevel set in (7); the right-hand side is the objective gap with respect to fslb
in (24) and it is the objective gap with respect to f∗ in (7); the growth constant G in (24)
varies with fslb and ρ in (7) may depend on  in general.
Freund and Lu’s SG method has an iteration complexity of O(G2G
2
(
log H

0 +
1

02 )) for
finding a solution wˆ such that f(wˆ ) − f∗ ≤ 
0
(f∗ − fslb), where fslb and G are defined in
(24) and H =
f(w0)−fslb
f∗−fslb
. In comparison, our RSG can be better if f∗ − fslb is large. To see
this, we represent the complexity of the method by Freund and Lu (2017) in terms of the
absolute error  with  = 
0
(f∗ − fslb) and obtain O(G2G
2
(
(f∗−fslb) log H
 +
(f∗−fslb)
2

2 )). If the
gap f∗ − fslb is large, e.g., O(f(w0) − fslb), the second term is dominating, which is at least
Ω( G2kw0−w∗
0
k
2
2

2 ) due to the definition of G in (24). This complexity has the same order of
magnitude as the standard SG method so that RSG can be better due to the reasoning in
last paragraph. More generally, the iteration complexity of Freund and Lu’s SG method can
be reduced to O(
G2B2
f∗−fslb

2 ) by choosing the best G in the proof of Theorem 1.1 in Freund
and Lu (2017)’s paper, which depends on f∗ − fslb. In comparison, RSG could have a
lower complexity if f∗ − fslb is larger than  as in Corollary 5 or ˆ1 as in Theorem 15. Our
experiments in subsection 8.4 also corroborate this point. In addition, RSG can leverage
the local error bound condition to enjoy a lower iteration complexity than O(1/2
).
7.3 Comparison with the method by Juditsky and Nesterov (2014)
Juditsky and Nesterov (2014) considered primal-dual subgradient methods for solving the
problem (1) with f being uniformly convex, namely,
f(αw + (1 − α)v) ≤ αf(w) + (1 − α)f(v) −
1
2
µα(1 − α)[α
ρ−1 + (1 − α)
ρ−1
]kw − vk
ρ
2
,
for any w and v in Ω and any α ∈ [0, 1] 5
, where ρ ∈ [2, +∞] and µ ≥ 0. In this case, the
method by (Juditsky and Nesterov, 2014) has an iteration complexity of O

G2
µ2/ρ
2(ρ−1)/ρ 
.
The uniform convexity of f further implies f(w) − f∗ ≥
1
2
µkw − w∗k
ρ
2
for any w ∈ Ω so
that f(·) admits a local error bound on the -sublevel set S with c =

2
µ
 1
ρ
and θ =
1
ρ
.
Therefore, our RSG has a complexity of O

G2
µ2/ρ
2(ρ−1)/ρ log( 0

)

according to Corollary 7.
Compared to the result of Juditsky and Nesterov (2014), our complexity is higher by a
logarithmic factor. However, we only require the local error bound property of f that is
weaker than uniform convexity and also covers much broader family of functions. Note that
the above comparison is fair, since for achieving a target -optimal solution the algorithms
5. The Euclidean norm in the definition here can be replaced by a general norm as in (Juditsky and
Nesterov, 2014).
21
Yang and Lin
0 10 20
3
3.5
4
4.5
objective
robust regression (p=1)
0 10 20 −25
−20
−15
−10
−5
0
log(level−gap)
#of stages
t=102
t=103
t=104
(a) different t
0 10 20
8
10
12
14
16
18
20
objective
robust regression (p=1.5)
0 10 20 −25
−20
−15
−10
−5
0
5
log(level−gap)
#of stages
t=102
t=103
t=104
(b) different t
0 2 4 6 8 10
x 104
−15
−10
−5
0
#of iterations
log(objective gap)
robust regression (p=1)
SG
RSG (t=103
)
R
2SG (t1=103
)
RSG (t=104
)
(c) different algorithms
0 5 10 15
x 104
−40
−30
−20
−10
0
10
#of iterations
log(objective gap)
robust regression (p=1.5)
SG
RSG (t=103
)
R
2SG (t1=103
)
RSG (t=104
)
(d) different algorithms
Figure 2: Comparison of RSG with different t and of different algorithms on the housing
data. One iteration means one subgradient update in all algorithms. (t1 for R2SG
represents the initial value of t in the first call of RSG.)
proposed by Juditsky and Nesterov (2014) do need the knowledge of uniform convexity
parameter ρ and the parameter µ. It is worth mentioning that Juditsky and Nesterov
(2014) also presented algorithms with a fixed number of iterations T as input that achieve
adaptive rates without knowledge of ρ and µ. However, they only considered the case when
ρ >= 2, which corresponds to θ ≤ 1/2 in our notations, while our methods can be applied
also when θ > 1/2.
8. Experiments
In this section, we present some experiments to demonstrate the effectiveness of RSG. We
first consider several applications in machine learning, in particular regression, classification
and matrix completion, and focus on the comparison between RSG and SG. Then we make
comparison between RSG with Freund & Lu’s SG variant for solving regression problems. In
experiments, all compared algorithms use the same initial solution unless otherwise specified.
22
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
0 10 20
0.1
0.15
0.2
0.25
0.3
0.35
objective
robust regression (p=1)
0 10 20 −25
−20
−15
−10
−5
0
log(level−gap)
#of stages
t=102
t=103
t=104
(a) different t
0 10 20
0.05
0.1
0.15
0.2
0.25
objective
robust regression (p=1.5)
#of stages
0 10 20 −25
−20
−15
−10
−5
0
log(level−gap)
t=102
t=103
t=104
(b) different t
0 0.5 1 1.5 2
x 105
−30
−25
−20
−15
−10
−5
0
#of iterations
log(objective gap)
robust regression (p=1)
SG
RSG (t=103
)
R
2SG (t1=103
)
RSG (t=104
)
(c) different algorithms
0 0.5 1 1.5 2
x 105
−30
−25
−20
−15
−10
−5
0
#of iterations
log(objective gap)
robust regression (p=1.5)
SG
RSG (t=103
)
R
2SG (t1=103
)
RSG (t=104
)
(d) different algorithms
Figure 3: Comparison of RSG with different t and of different algorithms on the space-ga
data. One iteration means one subgradient update in all algorithms.
8.1 Robust Regression
The regression problem is to predict an output y based on a feature vector x ∈ R
d
. Given
a set of training examples (xi
, yi), i = 1, . . . , n, a linear regression model can be found by
solving the optimization problem in (19).
We solve two instances of the problem with p = 1 and p = 1.5 and λ = 0. We conduct
experiments on two data sets from libsvm website 6
, namely housing (n = 506 and d = 13)
and space-ga (n = 3107 and d = 6). We first examine the convergence behavior of RSG
with different values for the number of iterations per-stage t = 102
, 103
, and 104
. The value
of α is set to 2 in all experiments. The initial step size of RSG is set to be proportional to
0/2 with the same scaling parameter for different variants. We plot the results on housing
data in Figure 2 (a,b) and on space-ga data in Figure 3 (a,b). In each figure, we plot the
objective value vs number of stages and the log difference between the objective value and
the converged value (to which we refer as level gap). We can clearly see that with different
values of t RSG converges to an -level set and the convergence rate is linear in terms of
the number of stages, which is consistent with our theory.
Secondly, we compare with SG to verify the effectiveness of RSG. The baseline SG is
implemented with a decreasing step size proportional to 1/
√
τ , where τ is the iteration
6. https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/
23
Yang and Lin
0 0.5 1 1.5 2
x 105
0.34
0.36
0.38
0.4
0.42
0.44
#of iterations
Objective
R
2SG
SG
(a) comparison between different algorithms
0 0.5 1 1.5 2
x 105
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
#of iterations
Objective
R
2SG (initial 1)
R
2SG (initial 2)
SG (initial 1)
SG (initial 2)
(b) sensitivity to initial solutions
Figure 4: Results for solving SVM classification with GFlassso regularizer. In (b), the
objective values of the two initial solutions are 1 and 70.35. One iteration means
one subgradient update in all algorithms.
index. The initial step size of SG is tuned in a wide range to give the fastest convergence.
The initial step size of RSG is also tuned around the best initial step size of SG. The results
are shown in Figure 2(c,d) and Figure 3(c,d), where we show RSG with two different values
of t and also R2SG with an increasing sequence of t. In implementing R2SG, we restart RSG
for every 5 stages, and increase the number of iterations by a certain factor. In particular,we
increase t by a factor of 1.15 and 1.5 respectively for p = 1 and p = 1.5. From the results,
we can see that (i) RSG with a smaller value of t = 103
can quickly converge to an -level,
which is less accurate than SG after running a sufficiently large number of iterations; (ii)
RSG with a relatively large value t = 104
can converge to a much more accurate solution;
(iv) R2SG converges much faster than SG and can bridge the gap between RSG-t = 103
and RSG-t = 104
.
8.2 SVM Classification with a graph-guided fused lasso
The classification problem is to predict a binary class label y ∈ {1, −1} based on a feature
vector x ∈ R
d
. Given a set of training examples (xi
, yi), i = 1, . . . , n, the problem of training
a linear classification model can be cast into
min
w∈Rd
F(w) := 1
n
Xn
i=1
`(w>xi
, yi) + R(w).
Here we consider the hinge loss `(z, y) = max(0, 1−yz) as in support vector machine (SVM)
and a graph-guided fused lasso (GFlasso) regularizer R(w) = λkF wk1 (Kim et al., 2009),
where F = [Fij ]m×d ∈ R
m×d
encodes the edge information between variables. Suppose
there is a graph G = {V, E} where nodes V are the attributes and each edge is assigned a
weight sij that represents some kind of similarity between attribute i and attribute j. Let
E = {e1, . . . , em} denote a set of m edges, where an edge eτ = (iτ , jτ ) consists of a tuple
of two attributes. Then the τ -th row of F matrix can be formed by setting Fτ,iτ = siτ ,jτ
and Fτ,jτ = −siτ ,jτ
for (iτ , jτ ) ∈ E, and zeros for other entries. Then the GFlasso becomes
24
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
0 5 10 15
#of iterations 10 4
0
1
2
3
4
objective
absolute loss
SG
R
2SG
0 5 10 15
#of iterations 10 4
0
2
4
6
8
10
objective
hinge loss
SG
R
2SG
Figure 5: Results for solving low rank matrix completion with different loss functions.
R(w) = λ
P
(i,j)∈E sij |wi − wj |. Previous studies have found that a carefully designed
GFlasso regularization helps in reducing the risk of over-fitting. In this experiment, we
follow (Ouyang et al., 2013) to generate a dependency graph by sparse inverse covariance
selection (Friedman et al., 2008). To this end, we first generate a sparse inverse covariance
matrix using the method in (Friedman et al., 2008) and then assign an equal weight sij = 1
to all edges that have non-zero entries in the resulting inverse covariance matrix. We
conduct the experiment on the dna data (n = 2000 and d = 180) from the libsvm website,
which has three class labels. We solve the above problem to classify class 3 versus the rest.
The comparison between different algorithms starting from an initial solution with all zero
entries for solving the above problem with λ = 0.1 is presented in Figure 4(a). For R2SG,
we start from t1 = 103 and restart it every 10 stages with t increased by a factor of 1.15.
The initial step sizes for all algorithms are tuned.
We also compare the dependence of R2SG’s convergence on the initial solution with
that of SG. We use two different initial solutions (the first initial solution w0 = 0 and the
second initial solution w0 is generated once from a normal Gaussian distribution). The
convergence curves of the two algorithms from the two different initial solutions are plotted
in Figure 4(b). Note that the initial step sizes of SG and R2SG are separately tuned for
each initial solution. We can see that R2SG is much less sensitive to a bad initial solution
than SG consistent with our theory.
8.3 Matrix Completion for Collaborative Filtering
In this subsection, we consider low rank matrix completion problems to demonstrate the
effectiveness of R2SG without having the knowledge of c and θ in the local error bound
condition. We consider a movie recommendation data set, namely MovieLens 100k data 7
,
which contains 100, 000 ratings from m = 943 users on n = 1682 movies. We formulate
the problem as a task of recovering a full user-movie rating matrix X from the partially
observed matrix Y . The objective is composed of a loss function measuring the difference
between X and Y on the observed entries and a nuclear norm regularizer on X for enforcing
7. https://grouplens.org/datasets/movielens/
25
Yang and Lin
a low rank, i.e.,
min
X∈Rm×n
1
N
X
(i,j)∈Σ
`(Xij , Yij ) + λkXk∗, (25)
where Σ is a set of user-movie pairs that denote the observed entries, `(·, ·) denote a loss
function, kXk∗ denotes the nuclear norm, N = |Σ| and λ > 0 is a regularization parameter.
We consider two loss functions, i.e, the hinge loss and the absolute loss. For absolute
loss, we set `(a, b) = |a − b|. For hinge loss, we follow Rennie and Srebro (2005) by
introducing four thresholds θ1,2,3,4 due to there are five distinct ratings in {1, 2, 3, 4, 5}
that can be assigned to each movie, and defining `(a, b) = P4
r=1 max(0, 1 − T
r
i,j (θr − Xij )),
where T
r
ij =

1 if r ≥ Yij
0 otherwise . In our experiment, we set θ1,2,3,4 = (0, 3, 6, 9) and λ = 10−5
following (Yang et al., 2014). Since the loss function and the nuclear norm are both semialgebraic functions (Yang et al., 2016; Bolte et al., 2014), then the problem (25) satisfies an
error bound condition on any compact set (Bolte et al., 2017). However, it remains an open
problem what are the proper values of c and θ to make local error bound condition hold.
Hence, we run R2SG by setting θ = 0. To compare with SG, we simply set t1 = 10 - the
number of iterations of each stage of the first call of RSG. The baseline SG is implemented
in the same way as before. The results of the objective values vs the number of iterations
are plotted in Figure 5. We can see that R2SG converges much faster than SG, verifying
the effectiveness of R2SG predicted by Theorem 16.
8.4 Comparison with Freund & Lu’s SG
In this subsection, we compare the proposed RSG with Freund & Lu’ SG algorithm empirically. The later algorithm is designed with a fixed relative accuracy 
0
such that f(xt)−f∗
f∗−fslb
≤ 
0
,
where fslb is a strict lower bound of f∗, and requires to maintain the best solution in terms
of the objective value during the optimization. For fair comparison, we run RSG with a
fixed t and then vary 
0
for Freund & Lu’s SG algorithm that is an input parameter, and
then plot the objective values versus the running time and the number of iterations for both
algorithms. The experiments are conducted on the two classification data sets as used in
subsection 8.1, namely the housing data and the space-ga data, for solving robust regression problems (19) with p = 1 and p = 1.5. The strict lower bound fslb in Freund & Lu’s
algorithm is set to 0. The results are shown in Figure 6 and Figure 7, where SGR refers to
Freund & Lu’s SG algorithm with a specified relative accuracy. For each problem instance
(a data set and a particular value of p), we report two results comparing the objective values
vs. running time and the number of iterations. We can see that RSG is very competitive
in performance in terms of running time and converge faster than Freund & Lu’s algorithm
with a small 
0 = 10−4
for achieving the same accurate solution (e.g., with objective gap
less than 10−10).
9. Conclusion
In this work, we have proposed a novel restarted subgradient method for non-smooth and/or
non-strongly convex optimization for obtaining an -optimal solution. By leveraging the
26
RSG: Beating Subgradient Method without Smoothness and Strong Convexity
0 5 10 15 20 25 −15
−10
−5
0
5
running time (s)
log(objective gap)
robust regression (p=1)
RSG (t=104
)
SGR (ε’=10−1)
SGR (ε’=10−2)
SGR (ε’=10−3)
SGR (ε’=10−4)
0 2 4 6 8 10
x 104
−15
−10
−5
0
5
#iterations
log(objective gap)
robust regression (p=1)
RSG (t=104
)
SGR (ε’=10−1)
SGR (ε’=10−2)
SGR (ε’=10−3)
SGR (ε’=10−4)
0 5 10 15 20 25 30 35 −40
−30
−20
−10
0
10
running time (s)
log(objective gap)
robust regression (p=1.5)
RSG (t=104
)
SGR (ε’=10−1)
SGR (ε’=10−2)
SGR (ε’=10−3)
SGR (ε’=10−4)
0 2 4 6 8 10
x 104
−40
−30
−20
−10
0
10
#iterations
log(objective gap)
robust regression (p=1.5)
RSG (t=104
)
SGR (ε’=10−1)
SGR (ε’=10−2)
SGR (ε’=10−3)
SGR (ε’=10−4)
Figure 6: Comparison of RSG with Freund & Lu’s SG algorithm (SGR) on the housing
data.
0 20 40 60 80 100 −20
−15
−10
−5
0
running time (s)
log (objective gap)
robust regression (p=1)
RSG (t=104
)
SGR (ε’=10−1)
SGR (ε’=10−2)
SGR (ε’=10−3)
SGR (ε’=10−4)
0 2 4 6 8 10
x 104
−20
−15
−10
−5
0
#iterations
log(objective gap)
robust regression (p=1)
RSG (t=104
)
SGR (ε’=10−1)
SGR (ε’=10−2)
SGR (ε’=10−3)
SGR (ε’=10−4)
0 50 100 150 −25
−20
−15
−10
−5
0
running time (s)
log (objective gap)
robust regression (p=1.5)
RSG (t=104
)
SGR (ε’=10−1)
SGR (ε’=10−2)
SGR (ε’=10−3)
SGR (ε’=10−4)
0 2 4 6 8 10
x 104
−25
−20
−15
−10
−5
0
#iterations
log(objective gap)
robust regression (p=1.5)
RSG (t=104
)
SGR (ε’=10−1)
SGR (ε’=10−2)
SGR (ε’=10−3)
SGR (ε’=10−4)
Figure 7: Comparison of RSG with Freund & Lu’s SG algorithm (SGR) on the space-ga
data.
lower bound of the first-order optimality residual, we establish a generic complexity of RSG
that improves over standard subgradient method. We have also considered several classes
of non-smooth and non-strongly convex problems that admit a local error bound condition
and derived the improved order of iteration complexities for RSG. Several extensions have
been made to design a parameter-free variant of RSG without requiring the knowledge of
the constants in the local error bound condition. Experimental results on several machine
learning tasks have demonstrated the effectiveness of the proposed algorithms in comparison
to the subgradient method.
