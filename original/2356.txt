Graphical models with change-points are computationally challenging to fit, particularly
in cases where the number of observation points and the number of nodes in the graph
are large. Focusing on Gaussian graphical models, we introduce an approximate majorizeminimize (MM) algorithm that can be useful for computing change-points in large graphical
models. The proposed algorithm is an order of magnitude faster than a brute force search.
Under some regularity conditions on the data generating process, we show that with high
probability, the algorithm converges to a value that is within statistical error of the true
change-point. A fast implementation of the algorithm using Markov Chain Monte Carlo is
also introduced. The performances of the proposed algorithms are evaluated on synthetic
data sets and the algorithm is also used to analyze structural changes in the S&P 500 over
the period 2000-2016.
Keywords: change-points, Gaussian graphical models, proximal gradient, simulated annealing, stochastic optimization
1. Introduction
Networks are fundamental structures that are commonly used to describe interactions between sets of actors or nodes. In many applications, the behaviors of the actors are observed
over time and one is interested in recovering the underlying network connecting these actors.
High-dimensional versions of this problem where the number of actors is large (compared
to the number of time points) is of special interest. In the statistics and machine learning
literature, this problem is typically framed as fitting large graphical models with sparse
parameters, and significant progress has been made recently, both in terms of the statistical theory (Meinshausen and Buhlmann, 2006; Yuan and Lin, 2007; Banerjee et al., 2008;
Ravikumar et al., 2011; Hastie et al., 2015), and practical algorithms (Friedman et al.,
2007; HÂ¨ofling and Tibshirani, 2009; Atchade et al., 2017).
In many problems arising in areas such as biology, finance, and political sciences, it
is well-accepted that the underlying networks of interest are not static, but can undergo
changes over time. Graphical models with change-points (or piecewise constant graphical
models) are simple, yet powerful models that are particularly well-suited for such problems,
and different versions have been explored in the literature. In this work, similarly to
c 2018 Leland Bybee and Yves AtchadÂ´e.
License: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v19/17-218.html.
Bybee and AtchadeÂ´
Zhou et al. (2009); Kolar et al. (2010); Roy et al. (2017), we focus on settings where the
change occurring at a given change-point is global in the sense that it affects the joint
distribution of all nodes. This differs from the approach of Kolar and Xing (2012) where
at a given change-point only the conditional distribution of a single node sees a change.
Which framework is more appropriate depends in general on the application. For instance
in biological applications where interests are often on single biomolecules, nodewise changepoint analysis might be preferred, whereas in many social science problems global structural
changes in the network is often of interest. We also mention the alternative approach of Liu
et al. (2013) which has an original parametrization that focuses directly on the occurring
change. Although we work within the joint-change framework, we stress that our proposed
algorithms can be easily adapted to work with other alternative models.
Despite their conceptual simplicity, graphical models with change-points are computationally challenging to fit. For instance a full grid search approach to locate a single
change-point in a Gaussian graphical model with a lasso penalty (glasso) requires solving
O(T) glasso sub-problems, where T is the number of time points. Most algorithms for
the glasso problem scale like O(p
3
) or worst1
, where p is the number of nodes. Hence
when p and T are large, fitting a high-dimensional Gaussian graphical model with a single
change-point has a taxing computational cost of O(T p3
) per iteration.
The literature addressing the computational aspects of model-based change-point models is rather sparse. A large portion of change-point detection procedures are based on
cumulative sums (CUSUM) or similar statistic-monitoring approaches (LÂ´evy-Leduc and
Roueff, 2009; Aue et al., 2009; Fryzlewicz, 2014; Chen and Zhang, 2015; Cho and Fryzlewicz, 2015, and the references therein). By and large, these change-point detection procedures can be efficiently implemented, and the computational difficulty aforementioned
can be avoided. However in problems where one wishes to detect structural changes in
large networks, a CUSUM-based or a statistic-based approach can be difficult to employ,
since it requires knowledge of the pertinent statistics to monitor. Furthermore the estimation of the parameters in a model-based change-point models can provide new insight in
the underlying phenomenon driving the changes. Hence CUSUM-based approaches may
not be appropriate in applications where the main driving forces of the network changes
are poorly understood, and/or are of prime interest.
Specific works addressing computational issues in model-based change-point estimation
include Roy et al. (2017); Leonardi and BÂ¨uhlmann (2016). In Roy et al. (2017) the authors considered a discrete graphical model with change-point and proposed a two-steps
algorithm for computation. However the success of their algorithm depends crucially on
the choice of the coarse and refined grids, and there is limited insight on how to choose
these. A related work is Leonardi and BÂ¨uhlmann (2016) where the authors considered a
high-dimensional linear regression model with change-points and proposed a dynamic programming approach to compute the change points. In the case of a single change-point
their algorithm corresponds to the brute force (full-grid search) approach mentioned above.
In this work we propose an approximate majorize-minimize (MM) algorithm for fitting
piecewise constant high-dimensional models. The algorithm can be applied more broadly.
However to focus the idea we limit our discuss to Gaussian graphical models with an elastic net penalty. In this specific setting, the algorithm takes the form of a block update
algorithm that alternates between a proximal gradient update of the graphical model parameters followed by a line search of the change-point. The proposed algorithm only solves
for a single change-point. We extend it to multiple change-points by binary segmentation.
We study the convergence of the algorithm and show under some regularity conditions on
the data generating mechanism that the algorithm is stable, and produces values in the
1. Furthermore the constant in the big-O is typically problem dependent and can be large
2
Change-Point Computation for Large Graphical Models
vicinity of the true change-point (under the assumption that one such true change-point
exists).
Each iteration of the proposed algorithm has a computational cost of O(T p2 + p
3
).
Although this cost is one order of magnitude smaller than the O(T p3
) cost of the brute
force approach, it can still be large when p and T are both large. As a solution we propose a
stochastic version of the algorithm where the line search performed to update the changepoint is replaced by a Markov Chain Monte Carlo (MCMC)-based simulated annealing.
The simulated annealing update is cheap (its computational cost per iteration is O(p
2
))
and is used as a stochastic approximation of the full line search. We show by simulation
that the stochastic algorithm behaves remarkably well, and as expected outperforms the
deterministic algorithm is terms of computing time.
The paper is organized as follows. Section 2 contains a presentation of the Gaussian
graphical model with change-points, followed by a detailed presentation of the proposed
algorithms. We performed extensive numerical experiments to investigate the behavior
of the proposed algorithms. We also use the algorithm to analyze structural changes in
the Standard & Poors (S&P) 500 over the period 2000-2016. The results are reported in
Section 3. We gather some of the technical proofs in Section 4.
We end this introduction with some notation that we shall use throughout the paper.
We denote Mp the set of all symmetric elements of R
pÃ—p
equipped with its Frobenius norm
kÂ·kF
and associated inner product
hA, BiF
def =
X
1â‰¤iâ‰¤jâ‰¤p
AijBij .
We denote M+
p
the subset of Mp of positive definite elements. For 0 < a < A â‰¤ +âˆ, let
M+
p
(a, A) denote the subset of M+
p of matrices Î¸ such that Î»min(Î¸) â‰¥ a, and Î»max(Î¸) â‰¤
A, where Î»min(M) (resp. Î»max(M)) denotes the smallest eigenvalue (resp. the largest
eigenvalue) of M.
If u âˆˆ R
p
, and q âˆˆ [1, âˆ], we define kukq
def = (Pp
j=1 |uj |
q
)
1/q (kukâˆ
def
= max1â‰¤jâ‰¤p |uj |).
For a matrix Î¸ âˆˆ R
pÃ—p and q âˆˆ [1, âˆ] \ {2}, we define kÎ¸kq similarly by viewing Î¸ as a R
p
2
vector. For q = 2, kÎ¸k2 denotes the spectral norm (operator norm) of Î¸.
2. Fitting Gaussian Graphical Models with a Single Change-Point
Let {X(t)
, 1 â‰¤ t â‰¤ T} be a sequence of p-dimensional random vectors. The grid over
which the change-points are searched is denoted T
def = {n0, . . . , T âˆ’ n0}, for some integer
1 â‰¤ n0 < T. We define
S1(Ï„ )
def =
1
Ï„
XÏ„
t=1
X(t)X(t)
0
, S2(Ï„ )
def =
1
T âˆ’ Ï„
X
T
t=Ï„+1
X(t)X(t)
0
, Ï„ âˆˆ T .
We define the regularization function as
â„˜(Î¸)
def = Î±kÎ¸k1 +
1 âˆ’ Î±
2
kÎ¸k
2
F
, Î¸ âˆˆ Mp, (1)
where Î± âˆˆ [0, 1) is a given constant, and kÎ¸k1
def =
Pp
iâ‰¤j
|Î¸ij |. Then we define
g1,Ï„ (Î¸) =  1
2
Ï„
T
[âˆ’ log det(Î¸) + Tr(Î¸S1(Ï„ ))] if Î¸ âˆˆ M+
p
,
+âˆ otherwise,
, Ï„ âˆˆ T ,
3
Bybee and AtchadeÂ´
where Tr(A) (resp. det(A)) denotes the trace (resp. the determinant) of A, and
g2,Ï„ (Î¸) =  1
2

1 âˆ’
Ï„
T

[âˆ’ log det(Î¸) + Tr(Î¸S2(Ï„ ))] if Î¸ âˆˆ M+
p
,
+âˆ otherwise,
, Ï„ âˆˆ T .
For j âˆˆ {1, 2}, we set
Ë†Î¸j,Ï„
def = Argmin Ï‘âˆˆM+
p
[gj,Ï„ (Ï‘) + Î»j,Ï„â„˜(Ï‘)] , (2)
for regularization parameters Î»1,Ï„ > 0, Î»2,Ï„ > 0, that we assume fixed throughout. Note
that due to the quadratic term in the elastic-net regularization (1), each of these minimization problems (2) is strongly convex. Hence for each Ï„ âˆˆ T , and j âˆˆ {1, 2},
Ë†Î¸j,Ï„ is
well-defined. We consider the problem of computing the change point estimate Ë†Ï„ defined
as
Ï„Ë† = Argmin Ï„âˆˆT h
g1,Ï„ (
Ë†Î¸1,Ï„ ) + Î»1,Ï„â„˜(
Ë†Î¸1,Ï„ ) + g2,Ï„ (
Ë†Î¸2,Ï„ ) + Î»2,Ï„â„˜(
Ë†Î¸2,Ï„ )
i
. (3)
If the minimization problem in (3) has more than one solution, then Ë†Ï„ denotes any one of
these solutions. The quantity Ë†Ï„ is the maximum likelihood estimate of a change point Ï„ in
the model which assumes that X(1), . . . , X(Ï„) are independent with common distribution
N(0, Î¸âˆ’1
1
), and X(Ï„+1), . . . , X(T) are independent with common distribution N(0, Î¸âˆ’1
2
), for
an unknown change-point Ï„ , and unknown precision matrices Î¸1 6= Î¸2.
The problem of computing the graphical lasso (glasso) estimators Ë†Î¸j,Ï„ in (2) has received
a lot of attention in the literature, and several efficient algorithms have been developed for
this purpose (see for instance AtchadÂ´e et al., 2015, and the references therein). Hence in
principle, using any of these available glasso algorithms, the change-point problem in (3)
can be solved by solving T âˆ’ 2n0 + 1 = O(T) glasso sub-problems. A similar algorithm is
advocated in Leonardi and BÂ¨uhlmann (2016) for fitting a high-dimensional linear regression
model with change-points. However this brute force approach can be very time-consuming
in cases where p and T are large. For instance, one of the most cost-efficient algorithm
for solving the glasso problem in high-dimensional cases is the standard proximal gradient
algorithm (Rolfs et al., 2012; AtchadÂ´e et al., 2015), which has a computational cost of
O(p
3
cond(
Ë†Î¸)
2
log(1/Î´)) to deliver a Î´-accurate solution (that is kÎ¸âˆ’Ë†Î¸kF â‰¤ Î´), where cond(A)
denotes the condition number of A, that is the ratio of the largest eigenvalue over the
smallest eigenvalue of A. Hence when p and T are large the computational cost of the
brute force approach for computing (3) is of order O

T p3
cond(
Ë†Î¸j,Ï„ )
2
log(1/Î´)

, which can
become prohibitively large.
We propose an algorithm that we show has a better computational complexity. To
motivate the algorithm we first introduce a majorize-minimize (MM) algorithm for solving (3). We refer the reader to Wu and Lange (2010) for a general introduction to MM
algorithms. Let
G(t)
def = g1,t(
Ë†Î¸1,t) + Î»1,tâ„˜(
Ë†Î¸1,t) + g2,t(
Ë†Î¸2,t) + Î»2,Ï„â„˜(
Ë†Î¸2,t), t âˆˆ T
denote the objective function of the minimization problem in (3). For Î¸1, Î¸2 âˆˆ Mp, we also
define
H(Ï„ |Î¸1, Î¸2)
def = g1,Ï„ (Î¸1) + Î»1,Ï„â„˜(Î¸1) + g2,Ï„ (Î¸2) + Î»2,Ï„â„˜(Î¸2), Ï„ âˆˆ T . (4)
Instead of the brute force approach that requires solving (2) for each value Ï„ âˆˆ T , consider
the following algorithm.
Algorithm 1 (MM algorithm) Pick Ï„
(0) âˆˆ T , and for k = 1, . . . , K, repeat the following steps.
 
Change-Point Computation for Large Graphical Models
1. Given Ï„
(kâˆ’1) âˆˆ T , compute Ë†Î¸1,Ï„ (kâˆ’1) and Ë†Î¸2,Ï„ (kâˆ’1) , and minimize the function H(t|
Ë†Î¸1,Ï„ (kâˆ’1) ,
Ë†Î¸2,Ï„ (kâˆ’1) )
to get Ï„
(k)
:
Ï„
(k) = ArgmintâˆˆT H(t|
Ë†Î¸1,Ï„ (kâˆ’1) ,
Ë†Î¸2,Ï„ (kâˆ’1) ).

By definition of Ë†Î¸j,Ï„ in (2), we have G(t) â‰¤ H(t|
Ë†Î¸1,Ï„ (kâˆ’1) ,
Ë†Î¸2,Ï„ (kâˆ’1) ) for all t âˆˆ T .
Furthermore G(Ï„
(kâˆ’1)) = H(Ï„
(kâˆ’1)|
Ë†Î¸1,Ï„ (kâˆ’1) ,
Ë†Î¸2,Ï„ (kâˆ’1) ). Therefore, for all k â‰¥ 1,
G(Ï„
(k)
) â‰¤ H(Ï„
(k)
|
Ë†Î¸1,Ï„ (kâˆ’1) ,
Ë†Î¸2,Ï„ (kâˆ’1) ) â‰¤ H(Ï„
(kâˆ’1)|
Ë†Î¸1,Ï„ (kâˆ’1) ,
Ë†Î¸2,Ï„ (kâˆ’1) ) = G(Ï„
(kâˆ’1)).
Hence the objective function G is non-increasing along the iterates of Algorithm 1. Note
that this algorithm is already potentially faster than the brute force approach, particular
when T is large, since we compute the graphical-lasso solutions Ë†Î¸j,Ï„ (k) only for time points
visited along the iterations. We propose to further reduce the computational cost by
computing the solutions Ë†Î¸j,Ï„ (k) only approximately, by simple gradient updates.
Given Î³ > 0, and a matrix Î¸ âˆˆ R
pÃ—p
, define ProxÎ³(Î¸) (the proximal map with respect
to the penalty function â„˜(Î¸) = Î±kÎ¸k1 + (1âˆ’Î±) kÎ¸k
2
F
/2) as the symmetric R
pÃ—p matrix such
that for 1 â‰¤ i, j â‰¤ p,
(ProxÎ³(Î¸))ij =
ï£±
ï£´ï£²
ï£´ï£³
0 if |Î¸ij | < Î±Î³
Î¸ijâˆ’Î±Î³
1+(1âˆ’Î±)Î³
if Î¸ij â‰¥ Î±Î³
Î¸ij+Î±Î³
1+(1âˆ’Î±)Î³
if Î¸ij â‰¤ âˆ’Î±Î³ .
We consider the following algorithm.
Algorithm 2 [Approximate MM algorithm] Fix a step-size Î³ > 0. Pick some initial value Ï„
(0) âˆˆ T ,
Î¸
(0)
1
, Î¸(0)
2 âˆˆ M+
p
. Repeat for k = 1, . . . , K. Given (Ï„
(kâˆ’1)
, Î¸
(kâˆ’1)
1
, Î¸
(kâˆ’1)
2
), do the following:
1. Compute
Î¸
(k)
1 = ProxÎ³Î»1,Ï„(kâˆ’1) 
Î¸
(kâˆ’1)
1 âˆ’ Î³

S1(Ï„
(kâˆ’1)) âˆ’ (Î¸
(kâˆ’1)
1
)
âˆ’1
 ,
2. compute
Î¸
(k)
2 = ProxÎ³Î»2,Ï„(kâˆ’1) 
Î¸
(kâˆ’1)
2 âˆ’ Î³

S2(Ï„
(kâˆ’1)) âˆ’ (Î¸
(kâˆ’1)
2
)
âˆ’1
 ,
3. compute
Ï„
(k) def = ArgmintâˆˆT H

t|Î¸
(k)
1
, Î¸(k)
2

.

Note that, if instead of a single proximal gradient update in Step (1)-(2), we do a large
number proximal gradient updates (an infinite number for the sake of the argument), we
recover exactly Algorithm 1. Hence Algorithm 2 is an approximate version of Algorithm 1.
Remark 1 1. Notice that one can easily compute H(Ï„ + 1|Î¸1, Î¸2) from H(Ï„ |Î¸1, Î¸2) by a rank-one
update in O(p
2
) number of operations. Hence the computational cost of Step (3) is O(T p2
).
And the total computational cost of one iteration of Algorithm 2 is O(p
3 + T p2
).
5  
Bybee and AtchadeÂ´
2. In practice, and as with any gradient descent algorithm, one needs to exercise some care in
choosing the step-size Î³. Clearly, too small values of Î³ lead to slow convergence. However,
choosing Î³ too large might cause the algorithm to diverge. Another (related) issue is how
to guarantee that the matrices Î¸
(k)
1
and Î¸
(k)
2 maintain positive definiteness throughout the
iterations. What we show below is that positive definiteness is automatically guaranteed if
the step-size Î³ is taken small enough. A nice trade-off that works well from the software
engineering viewpoint is to start with a large value of Î³ and to re-initialize the algorithm
with a smaller Î³ if at some point positive definiteness is lost. This issue is discussed more
extensively in AtchadÂ´e et al. (2015).
As suggested in the remark above, Algorithm 2 raises two basic questions. The first
question is whether the algorithm is stable, where here by stability we mean whether the
algorithm runs without Î¸
(kâˆ’1)
1
or Î¸
(kâˆ’1)
2
losing positive definiteness. Indeed we notice that
Steps (1 and 2) involve taking the inverse of the matrices Î¸
(kâˆ’1)
1
, and Î¸
(kâˆ’1)
2
, but there
is no guarantee a priori that these matrices are non-singular. Using results established in
AtchadÂ´e et al. (2015), we answer this question by showing below that if the step-size Î³ is
small enough then the algorithm is actually stable. The second basic question is whether
the algorithm converges to the optimal value. We address this question below.
For j âˆˆ {1, 2}, we set
Î»j
def = min
Ï„âˆˆT
Î»j,Ï„ , Î»Â¯
j
def
= max
Ï„âˆˆT
Î»j,Ï„ , Âµj
def
= max
Ï„âˆˆT 
1
2
kSj (Ï„ )k2 + Î±pÎ»j,Ï„ 
,
bj
def =
âˆ’Âµj +
q
Âµ
2
j + 2Î»Â¯
j (1 âˆ’ Î±)
n0
T
2(1 âˆ’ Î±)Î»Â¯
j
, Bj
def =
Âµj +
q
Âµ
2
j + 2Î»j
(1 âˆ’ Î±)
2(1 âˆ’ Î±)Î»j
.
Lemma 2 Fix j âˆˆ {1, 2}. For all Ï„ âˆˆ T ,
Ë†Î¸j,Ï„ âˆˆ M+
p
(bj , +âˆ). Let {(Î¸
(k)
1
, Î¸(k)
2
), k â‰¥ 0} be
the output of Algorithm 2. If the step-size Î³ satisfies Î³ âˆˆ (0, b
2
j
], and Î¸
(0)
j âˆˆ M+
p
(bj , Bj ), then
Î¸
(k)
j âˆˆ M+
p
(bj , Bj ), for all k â‰¥ 0.
Proof We present the proof for j = 1, the case j = 2 being similar. Note that Ë†Î¸1,Ï„ is the
graphical elastic-net estimate based on data X(1), . . . , X(Ï„)
. The fact that Ë†Î¸1,Ï„ exists (and
is unique) and satisfies the spectral bound Î»min(
Ë†Î¸1,Ï„ ) â‰¥ b1 then follows from known results
on the graphical elastic-net (see for instance Lemma 1 of AtchadÂ´e et al., 2015).
The second part of the lemma is similar to Lemma 2 of AtchadÂ´e et al. (2015). The
idea is to show that if Î¸
(k)
1 âˆˆ M+
p
(b1, B1) then Î¸
(k+1)
1 âˆˆ M+
p
(b1, B1). This is proved as
follows. Suppose that Î¸
(k)
1 âˆˆ M+
p
(b1, B1). Hence Î¸
(k)
1
is non-singular. It is well-known (see
for instance Parikh and Boyd, 2013, Section 4.2) that we can write Î¸
(k+1)
1
as
Î¸
(k+1)
1 = ArgminuâˆˆMp
D
âˆ‡g1,Ï„ (k) (Î¸
(k)
1
), u âˆ’ Î¸
(k)
1
E
+
1
2Î³



u âˆ’ Î¸
(k)
1



2
F
+ Î»1,Ï„ (k)â„˜(u)

.
The optimality conditions of this problem implies that there exists Z âˆˆ R
pÃ—p
, where Zij âˆˆ
[âˆ’1, 1] for all i, j such that
âˆ‡g1,Ï„ (k) (Î¸
(k)
1
) + 1
Î³

Î¸
(k+1)
1 âˆ’ Î¸
(k)
1

+ Î»1,Ï„ (k)

Î±Z + (1 âˆ’ Î±)Î¸
(k+1)
1

= 0.
6
Change-Point Computation for Large Graphical Models
Since âˆ‡g1,Ï„ (Î¸) = Ï„
2T
(S1(Ï„ ) âˆ’ Î¸
âˆ’1
), we re-arrange this optimality condition into:

1 + (1 âˆ’ Î±)Î»1,Ï„ (k) Î³

Î¸
(k+1)
1 = Î¸
(k)
1 +
Î³Ï„ (k)
2T

Î¸
(k)
1
âˆ’1
âˆ’ Î³

Ï„
(k)
2T
S1(Ï„
(k)
) + Î±Î»1,Ï„ (k)Z

.
Hence, if Î»min(Î¸
(k)
1
) â‰¥ b1, and b
2
1 â‰¥ Î³Ï„ /(2T) (which holds true if Î³ â‰¤ 2b
2
1
), and using the
fact that Î»min(A + B) â‰¥ Î»min(A) + Î»min(B), we get
Î»min(Î¸
(k+1)
1
) â‰¥
1
1 + (1 âˆ’ Î±)Î»Â¯
1Î³

b1 +
Î³n0
2T
1
b1
âˆ’ Î³Âµ1

, (5)
where Âµ1 = maxÏ„âˆˆT
1
2
kS1(Ï„ )k2 + Î±pÎ»1,Ï„
, using the fact that kZk2 â‰¤ p. We note that as
chosen, b1 satisfies
(1 âˆ’ Î±)Î»Â¯
1b
2
1 + Âµ1b1 âˆ’
n0
2T
= 0,
and this (with some easy algebra) implies that the right hand side of (5) is equal to b1.
Hence Î»min(Î¸
(k+1)
1
) â‰¥ b1. Similarly, if Î»max(Î¸
(k)
1
) â‰¤ B1, then
Î»max(Î¸
(k+1)
1
) â‰¤
1
1 + (1 âˆ’ Î±)Î»1Î³

B1 +
Î³
2
1
B1
+ Î³Âµ1

= B1,
where the last equality follows from the fact that we have chosen B1 such that
(1 âˆ’ Î±)Î»1B
2
1 âˆ’ Âµ1B1 âˆ’
1
2
= 0.
This completes the proof.
Remark 3 The first statement of Lemma 2 implies that the change-point problem (3) has at least
one solution. The second part shows that when the step-size Î³ is small enough, all the iterates of the
algorithm remains positive definite. We note that the fact that Î± < 1 is crucial in the arguments.
The result remains true where Î± = 1, however the arguments is slightly more involved (see AtchadÂ´e
et al., 2015, Lemma 2). For simplicity we focus in this paper on the case Î± âˆˆ [0, 1).
We now address the issue of convergence. Clearly the function t 7â†’ H(t|Î¸1, Î¸2) is not
smooth, nor convex. This implies that Algorithm 2 cannot be analyzed using standard optimization tools. And indeed, we will not be able to establish that the output of Algorithm
2 converges to the minimizer Ë†Ï„ . Rather, we introduce a containment assumption (Assumption H1) and we show that when it holds, then the output of Algorithm 2 converges to
some neighborhood of the true change-point (the existence of this true change-point is part
of the assumption).
H1 There exist  > 0, c â‰¥ 0, Îº âˆˆ [0, 1), and Ï„? âˆˆ T such that the following holds. For any Ï„ âˆˆ T ,
and for any Î¸1, Î¸2 âˆˆ M+
p
such that



Î¸1 âˆ’ Ë†Î¸1,Ï„



F
+



Î¸2 âˆ’ Ë†Î¸2,Ï„



F
â‰¤  we have
|Argmin tâˆˆT H(t|Î¸1, Î¸2) âˆ’ Ï„?| â‰¤ Îº|Ï„ âˆ’ Ï„?| + c. (6)
Remark 4 Plainly, what is imposed in H1 is the existence of a time point Ï„? âˆˆ T (that we can view
as the true change-point), such that anytime we take Ï„ âˆˆ T that is far from Ï„? in the sense that
   
Bybee and AtchadeÂ´
|Ï„ âˆ’ Ï„?| > c/(1 âˆ’ Îº), if Î¸1, Î¸2 are sufficiently close to the solutions Ë†Î¸1,Ï„ and Ë†Î¸2,Ï„ respectively, then
computing Argmin tâˆˆT H(t|Î¸1, Î¸2) brings us closer to Ï„?:
|Argmin tâˆˆT H(t|Î¸1, Î¸2) âˆ’ Ï„?| â‰¤ Îº|Ï„ âˆ’ Ï„?| + c < |Ï„ âˆ’ Ï„?|.
This containment assumption is akin to a curvature assumption on the function t 7â†’ H(t|Î¸1, Î¸2)
when Î¸1 and Î¸2 are reasonably close to Ë†Î¸1,Ï„ ,
Ë†Î¸2,Ï„ , respectively. The assumption seems realistic in
settings where the data X(1:T)
is indeed drawn from a Gaussian graphical model with true changepoint Ï„?, and parameters Î¸?,1, Î¸?,2. Indeed in this case, and if T is large enough, for any Ï„ that
is not too close to the boundaries, one expects Ë†Î¸1,Ï„ and Ë†Î¸2,Ï„ to be good estimates of Î¸?,1 and Î¸?,2,
respectively. Therefore if



Î¸1 âˆ’ Ë†Î¸1,Ï„



F
+



Î¸2 âˆ’ Ë†Î¸2,Ï„



F
â‰¤  for  small enough, one expect as well
Î¸1 and Î¸2 to be close to Î¸?,1 and Î¸?,2 respectively. Hence Argmin tâˆˆT H(t|Î¸1, Î¸2) should be close
to Argmin tâˆˆT H(t|Î¸?,1, Î¸?,2), which in turn should be close to Ï„?. Theorem 9 below will make this
intuition precise.

In the next result we will see that in fact the iterates Î¸
(k)
1
and Î¸
(k)
2
closely track Î¸1,Ï„ (k) and
Î¸2,Ï„ (k) respectively. Hence, when H1 holds Equation (6) guarantees that the sequence Ï„
(k)
remains close to Ï„?.
Theorem 5 Suppose that Î³ âˆˆ (0, b
2
1 âˆ§ b
2
2
], and Î¸
(0)
j âˆˆ M+
p
(bj , Bj ), for j = 1, 2. Then
lim
k



Î¸
(k)
1 âˆ’ Ë†Î¸1,Ï„ (k)



F
= 0, lim
k



Î¸
(k)
2 âˆ’ Ë†Î¸2,Ï„ (k)



F
= 0.
Furthermore, if H1 holds then
lim sup
kâ†’âˆ


Ï„
(k) âˆ’ Ï„?


 â‰¤
c
1 âˆ’ Îº
.
Proof See Section 4.1
Remark 6 Note that the theorem does not guarantee that Ï„
(k)
converges to Ï„?, but rather its conclusion is that for k large Ï„
(k)
stays within c/(1 âˆ’ Îº) of Ï„?.
We now address the question whether H1 is a realistic assumption. More precisely we
will show that the argument highlighted in Remark 4 holds true under some regularity conditions. Suppose that X(1:T) def = (X(1), . . . , X(T)
) are p-dimensional independent random
variables such that
X(1), . . . , X(Ï„?) i.i.d. âˆ¼ N(0, Î¸âˆ’1
?,1
), and X(Ï„?+1), . . . , X(T) i.i.d. âˆ¼ N(0, Î¸âˆ’1
?,2
), (7)
for some unknown change-point Ï„?, and unknown symmetric positive definite precision
matrices Î¸?,1 6= Î¸?,2. We set Î£?,j
def = Î¸
âˆ’1
?,j , and we let sj denote the number of nonzero entries of Î¸?,j , j = 1, 2. For an integer Î¹ âˆˆ {1, . . . , p}, we define the Î¹-th restricted
eigenvalues of Î£?,j as
Îºj
(Î¹)
def = inf {u
0
(Î£?,j )u, kuk2 = 1, kuk0 â‰¤ Î¹} ,
ÎºÂ¯j (Î¹)
def
= sup {u
0
(Î£?,j )u, kuk2 = 1, kuk0 â‰¤ Î¹} .
8 
Change-Point Computation for Large Graphical Models
We set s
def = max(s1, s2), Â¯Îº
def = max (Â¯Îº1(2), ÎºÂ¯2(2)), Îº
def = min (Îº1
(2), Îº2
(2)), and we set
the regularization parameter Î»j,Ï„ as
Î»1,Ï„
def =
ÎºÂ¯
Î±T
p
48Ï„ log(pT), Î»2,Ï„
def =
ÎºÂ¯
Î±T
p
48(T âˆ’ Ï„ ) log(pT), Ï„ âˆˆ T . (8)
We need to assume that the parameter Î± âˆˆ [0, 1) in the regularization term is large enough
to produce approximately sparse solutions in (2). To that end, we assume that
Î±
1 âˆ’ Î±
â‰¥ max (kÎ¸?,1kâˆ, kÎ¸?,2kâˆ). (9)
Finally, we assume that the search domain T is such that for all Ï„ âˆˆ T ,
min (Ï„, T âˆ’ Ï„ ) â‰¥ A
2
1
log(pT), (10)
where
A1
def
= max
2

ÎºÂ¯
Îº
2
,(1280)s
1/2ÎºÂ¯(kÎ¸?,1k2 âˆ¨ kÎ¸?,2k2)
!
,
and
ÎºÂ¯
p
Ï„ log(pT) â‰¥
1
2
âˆš
3
(Ï„ âˆ’ Ï„?)+kÎ¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
?,1
kâˆ,
and Â¯Îº
p
(T âˆ’ Ï„ ) log(pT) â‰¥
1
2
âˆš
3
(Ï„? âˆ’ Ï„ )+kÎ¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
?,1
kâˆ, (11)
where x+
def = max(x, 0).
Remark 7 Assumption (10) is a minimum sample size requirement. See for instance Ravikumar
et al. (2011) Theorem 1, and 2 for similar conditions in standard Gaussian graphical model estimation. Here we require to have T such that min(Ï„, T âˆ’Ï„ ) = O(s log(pT)) for all Ï„ âˆˆ T . This obviously
implies that we need T to be at least O(s log(p)). It is unclear whether the large constant 1280 in
(10) is tight or simply an artifact of our proof techniques.
To understand Assumption (11), note that for Ï„ > Ï„?, the estimator Ë†Î¸1,Ï„ in (2) is based on
misspecified data X(Ï„?+1), . . . , X(Ï„)
. Hence if Ï„ > Ï„? is too far away from Ï„?, the estimators Ë†Î¸1,Ï„
may behave poorly, particularly if Î¸?,1 are Î¸?,2 are very different. Assumption (11) rules out such
settings, by requiring the search domains T to be roughly a âˆš
T neighborhood of Ï„?. Indeed, suppose
that Ï„? = Ï?T, for some Ï? âˆˆ (0, 1). Then it can be easily checked that any search domain of the
form (Ï„? âˆ’ r1T
1/2
, Ï„? + r2T
1/2
), satisfies (10) and (11) for T large enough, provided that
0 < r1 â‰¤
2
âˆš
3Â¯Îº
p
Ï? log(pT)
kÎ¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
?,1
kâˆ
, and 0 < r2 â‰¤
2
âˆš
3Â¯Îº
p
(1 âˆ’ Ï?) log(pT)
kÎ¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
?,1
kâˆ
.
Of course, this search domain is difficult to use in practice since it depends on Ï„?. In practice, we
have found that taking T of the form (rT,(1 âˆ’ r)T) for r â‰¤ 0.1 works well, even though it is much
wider than what is prescribed by our theory.

For Ï„ âˆˆ T , let
r1,Ï„
def = A2ÎºÂ¯kÎ¸?,1k
2
2
r
s1 log(pT)
Ï„
, r2,Ï„
def = A2ÎºÂ¯kÎ¸?,2k
2
2
r
s2 log(pT)
T âˆ’ Ï„
,
9 
Bybee and AtchadeÂ´
where A2 is an absolute constant that can be taken as 16 Ã— 20 Ã—
âˆš
48. We set b
def =
min(Î»min(Î¸?,1), Î»min(Î¸?,2)), and B
def = max(Î»max(Î¸?,1), Î»max(Î¸?,2)). We assume that for j =
1, 2, and for Ï„ âˆˆ T ,
rj,Ï„ â‰¤ min
Î»min(Î¸?,j )
4
,
kÎ¸?,jkâˆ
2
,
kÎ¸?,jk1
1 + 8s
1/2
j
!
, rj,Ï„ â‰¤
kÎ¸?,2 âˆ’ Î¸?,1kF
2(1 + 8s
1/2)
and rj,Ï„ â‰¤ A2

b
B
4
kÎ¸?,jk1
s
1/2
j
. (12)
Remark 8 Condition (12) is mostly technical. As we will see below in Lemma 16, the term rj,Ï„
is the convergence rate toward Î¸?,j of the estimator Ë†Î¸j,Ï„ , and is expected to converge to 0 with p, T
(which implies that the sample size T cannot be too small compared to kÎ¸?,jk
4
2
sj log(pT)). Hence
according to (12) the matrices Î¸?,1 and Î¸?,2 need to be such that the terms on the right-hand sides
do no vanish faster than the rate rj,Ï„ . In particular Î¸?,1 and Î¸?,2 should be well-conditioned so that
Î»min(Î¸?,j ) and the ratio b/B do not decay too fast.
Theorem 9 Consider the output {(Î¸
(k)
1
, Î¸(k)
2
), k â‰¥ 0} of Algorithm 2. Suppose that Î³ âˆˆ (0, b
2
1 âˆ§b
2
2
],
and Î¸
(0)
j âˆˆ M+
p
(bj , Bj ), for j = 1, 2. Suppose that the statistical model underlying the data X(1:T)
is
as in (7), and that (8)-(12) hold. Suppose also that
kÎ¸?,2 âˆ’ Î¸?,1kF â‰¥ 8A2 max "
Î»min(Î¸?,1)
Î»max(Î¸?,1)
2
kÎ¸?,1k1
s
1/2
1
,

Î»min(Î¸?,2)
Î»max(Î¸?,2)
2
kÎ¸?,2k1
s
1/2
2
#
. (13)
Then with probability at least 1 âˆ’
8
pT âˆ’
4
p2(1âˆ’eâˆ’C0 )
, H1 holds with  = (1/
âˆšp) minÏ„âˆˆT (r1,Ï„ âˆ§ r2,Ï„ ),
Îº = 0, and c = 4 log(p)/C0, where
C0
def = min 
kÎ¸?,2 âˆ’ Î¸?,1k
4
F
128B4kÎ¸?,2 âˆ’ Î¸?,1k
2
1
,
Îº
ÎºÂ¯
4

.
In particular, with probability at least 1 âˆ’
8
pT âˆ’
4
p2(1âˆ’eâˆ’C0 )
we have
lim sup
kâ†’âˆ


Ï„
(k) âˆ’ Ï„?


 â‰¤
4
C0
log(p), (14)
Proof See Section 4.2.
Remark 10 The main point of the theorem is that under the assumptions and data generation
mechanism described above, the containment assumption H1 holds with probability as least 1 âˆ’
8
pT âˆ’
4
p2(1âˆ’eâˆ’C0 )
, and where  can be taken as minÏ„ r1,Ï„ âˆ§r2,Ï„ /
âˆšp, Îº = 0, and c = 4 log(p)/C0. Conclusion
(14) is then simply a consequence of Theorem 5. One should view (14) as saying that for k large,
the output of Algorithm 2 fluctuates around Ï„?, and the size of the fluctuation is O(log(p)), under
the assumed data generating mechanism. And we should stress that Algorithm 2 is not stochastic.
Hence the randomness expressed in the theorem is with respect to the data generating mechanism.
Remark 11 We note that the bound in (14) grows with p. In classical change-point problems where
p is fixed, and T â†’ âˆ, it is known (see e.g. Bai, 1997) that with a fixed-magnitude change, the
10
Change-Point Computation for Large Graphical Models
best one can achieve in estimating Ï„ is O(1). The rate in Theorem 9 suggests that in the highdimensional setting where p grows the estimation rate for Ï„ if of order O(log(p)) (see also Roy et al.,
2017). We believe that it is not possible to remove the additional log(p) factor, although to the best
of our knowledge this question is still open. Note that it is customary in the change-point literature
to take a re-scaled viewpoint and to define the change point as a? âˆˆ (0, 1) such that Ï„? = a?T. In
that setting the estimation rate for a? is O(1/T) in the classical fixed-dimensional fixed-magnitude
change setting, and O(log(p)/T) in our setting.
2.1 A Stochastic Version
When T is much larger than p, Step 3 of Algorithm 2 becomes costly. In such cases, one can
gain in efficiency by replacing Step 3 by a Monte Carlo approximation. We explore the use
of simulated annealing to approximately solve Step 3 of Algorithm 2. Given Î¸1, Î¸2 âˆˆ Mp,
and Î² > 0, let Ï€Î²,Î¸1,Î¸2 denote the probability distribution on T defined as
Ï€Î²,Î¸1,Î¸2
(Ï„ ) = 1
ZÎ²,Î¸1,Î¸2
exp 
âˆ’
H(Ï„ |Î¸1, Î¸2)
Î²

, Ï„ âˆˆ T .
Here, ZÎ²,Î¸1,Î¸2
is the normalizing constant, and Î² > 0 is the cooling parameter, that we
shall drive down to zero with the iteration to increase the accuracy of the Monte Carlo
approximation. Direct sampling from Ï€Î²,Î¸1,Î¸2
is typically possible, but this has the same
computational cost as Step 3 of Algorithm 2. We will use a Markov Chain Monte Carlo
approach which will allow us to make only a small number of calls of the function H, per
iteration. Let KÎ²,Î¸1,Î¸2 denote a Markov kernel on T with invariant distribution Ï€Î²,Î¸1,Î¸2
.
Typically we will choose KÎ²,Î¸1,Î¸2
as a Metropolis-Hastings Markov kernel (we give examples
below).
We consider the following algorithm. As in Algorithm 2, Î³ is a given step-size. We
choose a decrease sequence of temperature Î²
(k)
that we use along the iterations.
Algorithm 3 Fix a step-size Î³ > 0, and a cooling sequence {Î²
(k)}. Pick some initial value Ï„
(0) âˆˆ T ,
Î¸
(0)
1
, Î¸(0)
2 âˆˆ M+
p
. Repeat for k = 1, . . . , K. Given (Ï„
(kâˆ’1)
, Î¸
(kâˆ’1)
1
, Î¸
(kâˆ’1)
2
), do the following:
1. Compute
Î¸
(k)
1 = ProxÎ³Î»1,Ï„(kâˆ’1) 
Î¸
(kâˆ’1)
1 âˆ’ Î³

S1(Ï„
(kâˆ’1)) âˆ’ (Î¸
(kâˆ’1)
1
)
âˆ’1
 ,
2. compute
Î¸
(k)
2 = ProxÎ³Î»2,Ï„(kâˆ’1) 
Î¸
(kâˆ’1)
2 âˆ’ Î³

S2(Ï„
(kâˆ’1)) âˆ’ (Î¸
(kâˆ’1)
2
)
âˆ’1
 ,
3. draw
Ï„
(k) âˆ¼ KÎ²(k),Î¸(k)
1
,Î¸(k)
2
(Ï„
(kâˆ’1)
, Â·).

For most commonly used MCMC kernels, each iteration of Algorithm 3 has a computational cost of O(p
3
), which is better than O(p
3 +T p2
) needed by Algorithm 2, when T â‰¥ p.
However Algorithm 3 travels along the change-point space T more slowly. Hence overall,
a larger number of iterations would typically be needed for Algorithm 3 to converge. Even
after accounting for this slow convergence, Algorithm 3 is still substantially faster than Algorithm 2, as shown in Table 1 and 2. A rigorous analysis of the convergence of Algorithm
3 is beyond the scope of this work, and it left as a possible future research.
11 
Bybee and AtchadeÂ´
2.2 Extension to Multiple Change-Points
We extend the method to multiple change-points by binary segmentation. Binary segmentation is a standard method for detecting multiple change-points. The method proceeds by
first searching for a single change-point. When a change-point is found the data is split into
the two parts defined by the detected change-point. A similar search is then performed on
each segment which can result in further splits. This recursive procedure continues until a
certain stopping criterion is satisfied. Here we stop the recursion if
`Ï„ + Cp â‰¥ `F ,
where `Ï„ is the penalized negative log-likelihood obtained with the additional changepoint Ï„ , and `F is the penalized negative log-likelihood without the change-point. The
term Cp is a penalty term for model complexity, where C is a user-defined regularization
parameter that controls the sparsity of the change-point model (the number of changepoints). To the best of our knowledge there is no easy and principled approach for choosing
C. We identify this as an important issue where more research is needed. Since C controls
the number of change-points, in practice one ad-hoc approach is to set C such that the
number of detected change-points is reasonable. This is the approach that we use in the
real data analysis. Here we rely on simulation. We explore various scenarios by simulation
and found that values of C between (0, 4) produce the best results in our setting.
The binary segmentation algorithm can be defined more precisely as follows. Let us
call J (X, t0, t1) the (single) change-point output either by Algorithm 3 or Algorithm 4
when applied to data set X using sample Xt0
, . . . , Xt1
, for some t0, t1 âˆˆ T , t0 < t1.
Let L(X, t0, t1) denote the (penalized) minimum negative log-likelihood achieved on data
Xt0
, . . . , Xt1
. That is,
L(X, t0, t1) = min
Î¸0
"
âˆ’ log det(Î¸) + Tr
Î¸

1
t1 âˆ’ t0 + 1
Xt1
t=t0
X(t)X(t)
0
!! + Î»â„˜(Î¸)
#
.
Then the binary-segmentation algorithm B(X, t0, t1) can be written recursively as follows:
Algorithm 4 Binary Segmentation
1: function B(X, t0, t1)
2: Ï„ = J (X, t0, t1) (apply either algorithm 3 or 4 to data Xt0
, . . . , Xt1
)
3: `Ï„ = L(X, t0, Ï„ ) + L(X, Ï„ + 1, t1)
4: `F = L(X, t0, t1)
5: if `Ï„ + Cp â‰¥ `F then
6: return Null
7: else
8: return {Ï„, B(X, t0, Ï„ ), B(X, Ï„ + 1, t1)}
9: end if
10: end function
We end this section with some words of caution. Binary segmentation is well-known
to be a sub-optimal procedure and can perform poorly in some settings (see for instance
Fryzlewicz, 2014). The issue is that at each step, binary segmentation is actually fitting a
possibly misspecified modelâ€”one with a single change-pointâ€”to data with possibly multiple change-points. One approach is overcoming this limitation is to extend our proposed
algorithms so as to handle directly multiple change-points. We leave this as an important
future work.
12
Change-Point Computation for Large Graphical Models
3. Numerical Experiments
We investigate the different algorithms presented here in a variety of settings. For all
the algorithms investigated the choice of the step-size Î³ and the regularizing parameter
Î» are important. For all experiments, and as suggested by (8), we found that setting
Î»1,Ï„ = Î»
q
log{p}
Ï„
and Î»2,Ï„ = Î»
qlog{p}
T âˆ’Ï„ worked well. For the time-comparison in Section
3.1 we used Î» = 0.1 and Î³ = 3.5 when T = 1000, and we used Î» = 0.01 and Î³ = 3.5 when
T = 500. For the remainder of the experiments we set Î» = 0.13 and Î³ = 0.25. For all the
experiments the search domain T is taken as {n0, . . . , T âˆ’ n0}, for a minimum sample size
n0 from {0.01T, 0.05T, 0.1T}.
We initialize Ï„
(0) to a randomly selected value in T . The initial value Î¸
(0)
1
and Î¸
(0)
2
are taken as Î¸
(0)
j = (Sj (Ï„
(0)) + I)
âˆ’1 where  is a constant chosen to maintain positive
definiteness. For cases where p < Ï„ and p < T âˆ’ Ï„ we used  = 0, while for larger values of
p we set  = 0.2.
For the data generation in the simulations, we typically choose Ï„? = T /2 unless otherwise specified, and unless otherwise specified, we generate independently the matrices Î¸?,1
and Î¸?,2 as follows. First we generate a random symmetric sparse matrix M such that the
proportion of non-zero entries is 0.25. We add 4 to all positive entries and subtract 4 from
all negative entries. Then we set the actual precision matrix as Î¸?,j = M + (1âˆ’Î»min(M))Ip
where Î»min(M) is the smallest eigenvalue of M. The resulting precision matrices contain
roughly 25% non-zero off-diagonal elements. For each simulation a new pair of precision
matrices was generated as well as the corresponding data set.
For Algorithm 3 we also experimented with a number of MCMC kernel KÎ²,Î¸1,Î¸2
. We
experiment with the independence Metropolis sampler with proposal U(n0, T âˆ’ n0). We
also tried a Random Walk Metropolis with a truncated Gaussian proposal N(Ï„
(kâˆ’1), Ïƒ2
),
for some scale parameter Ïƒ > 0. Finally, we also experimented with a mixture of these two
Metropolis-Hastings kernels. We found that for our simulations the Independent Metropolis
kernel works best, although the mixture kernel also performed well. For the cooling schedule
of simulated annealing we use Î²
(0) = 1, and a geometric decay Î²
(n) = Î±Î²(nâˆ’1) with
Î± =

Î²
(M)
Î²(0) 1/M
where Î²
(M) = 0.001, and M is the maximum number of iterations.
An implementation of the algorithms presented here for the Gaussian graphical model
context is available in the changepointsHD package, Bybee (2017), available on the Comprehensive R Archive Network (CRAN).
3.1 Time Comparison
First we compare the running times of the proposed algorithms and the brute force approach. We consider two settings: (p = 100, T = 1000) and (p = 500, T = 500). In the
setting (p = 100, T = 1000), 100 independent runs of Algorithms 2 and 3 are performed
and the average run-times are reported in Table 1. In the setting (p = 500, T = 500) 10
independent runs of Algorithms 2 and 3 are used, and the results are presented in Table
2. We compare these times to results from one simulation run of the brute-force approach,
the results of which are given in the description (caption) of Tables 1 and 2.
We consider two stopping criteria for Algorithm 2 or 3. The first criterion stops the
iterations if
1
T
|Ï„
(k) âˆ’ Ï„?| < 0.005 and kÎ¸
(k)
1 âˆ’ Ë†Î¸1kF
k
Ë†Î¸1kF
+
kÎ¸
(k)
2 âˆ’ Ë†Î¸2kF
k
Ë†Î¸2kF
< 0.05, (V1)
where Ë†Î¸1 and Ë†Î¸2 are obtained by performing 1000 proximal-gradient steps at the true Ï„
value. An interesting feature of the proposed approximate MM algorithms is that the
13
Bybee and AtchadeÂ´
Variant Approx. MM Simulated Annealing
(V1) Time (Seconds) 195.95 (48.94) 3.03 (0.40)
Iterations 658.68 (82.93) 662.62 (88.51)
(V2) Time (Seconds) 0.39 (0.10) 0.48 (0.46)
Iterations 1.03 (0.17) 101.96 (100.29)
Table 1: Run-times of Algorithm 2 and 3 for (p = 100, T = 1000). For comparison the
run-time of the brute force algorithm for this problem is 2374.82.
Variant Approx. MM Simulated Annealing
(V1) Time (Seconds) 3554.30 (404.24) 94.64 (5.50)
Iterations 939.70 (11.03) 941.70 (16.23)
(V2) Time (Seconds) 4.27 (1.10) 10.96 (8.26)
Iterations 1.10 (0.32) 111.20 (90.71)
Table 2: Run-times of Algorithm 2 and 3 for (p = 500, T = 500). For comparison the
run-time of the brute force algorithm for this problem is 10854.44.
change-point sequence Ï„
(k)
can converge well before Î¸
(k)
1
and Î¸
(k)
2
. To illustrate this, we
also explore the alternative approach of stopping the iterations only based on Ï„
(k)
, namely
when
1
T
|Ï„
(k) âˆ’ Ï„?| < 0.005. (V2)
Finally, we note that we implement the brute force approach by running 500 proximalgradient steps for each possible value of Ï„ . Note that 500 iterations is typically smaller
than the number of iterations needed to satisfy (V1).
Tables 1 and 2 highlight the benefits of Algorithm 2 and Algorithm 3 as the run-time
is several orders of magnitude lower than the brute force approach. Additionally, while
Algorithm 3 requires more iterations than Algorithm 2 its run-time is typically smaller. The
benefits of Algorithm 3 are particularly clear for large values of p and T (under stopping
criterion (V1)). The stopping criteria (V2) highlights the fact that the Ï„
(k)
sequence in the
proposed algorithms can converge well before the Î¸-sequences.
3.2 Behavior of the Algorithm when the Change-Point is at the Edge
We investigate how the brute force algorithm, Algorithm 2, and Algorithm 3 perform
when change-points are non-existent or close to the edges. The results for the brute force
algorithm are presented in Figure 1, the results for Algorithm 2 are presented on Figure 2
and the results for Algorithm 3 are presented on Figure 3. For Algorithm 2 and Algorithm
3 the figure contains two subfigures, the first showing the sequences {Ï„
(k)} of solutions
produced by the algorithm (trace plots) for all 200 replications, and the second showing
a histogram of the final change-point estimate, based on 200 replications. Additionally, a
line is included to show the location of the true Ï„ . The trace plots show how quickly each
algorithm converges under the various settings. For the brute force algorithm the trace plot
is not relevant since the brute force algorithm is not an iterative algorithm. The results
suggest that Algorithm 2 and Algorithm 3 have more trouble when the true Ï„ is close to
the edge of the sample. For Ï„ = 0.1T, Algorithm 3 performed slightly better, with 136
simulations ending within 5 units of the true Ï„ compared to 90 for Algorithm 2.
14
Change-Point Computation for Large Graphical Models
(a) No change-point (b) Change-point at Ï„ = 0.1T
(c) Change-point at Ï„ = 0.25T (d) Change-point at Ï„ = 0.5T
Figure 1: Behavior of the brute force approach as the location of the true change-point
is varied. Each plot is a histogram of the change-point estimates based on 200
replications.
15
Bybee and AtchadeÂ´
(a) No change-point (b) Change-point at Ï„ = 0.1T
(c) Change-point at Ï„ = 0.25T (d) Change-point at Ï„ = 0.5T
Figure 2: Behavior of Algorithm 2 as the location of the true change-point is varied.
Each plot gives a trace plot of produced estimates, and a histogram of the final
change-point estimate. Based on 200 replications.
16
Change-Point Computation for Large Graphical Models
(a) No change-point (b) Change-point at Ï„ = 0.1T
(c) Change-point at Ï„ = 0.25T (d) Change-point at Ï„ = 0.5T
Figure 3: Behavior of Algorithm 3 as the location of the true change-point is varied.
Each plot gives a trace plot of produced estimates, and a histogram of the final
change-point estimate. Based on 200 replications.
17
Bybee and AtchadeÂ´
(a) q = 25, p = 0 (b) q = 17.5, p = 7.5
(c) q = 10, p = 15 (d) q = 0, p = 25
Figure 4: Behavior of the brute force approach for varying signals. Each plot is a histogram
of the final change-point estimate. Based on 200 replications.
3.3 Behavior of the Algorithms when Î¸1 and Î¸2 are Similar
As Î¸1 and Î¸2 get increasingly similar, the location of the change-point becomes increasingly
more difficult to find. We investigate the behavior of the proposed algorithms in such
settings. We generate the true precision matrices Î¸1 and Î¸2 as follows. We draw a random
precision matrix Î¸ with q% non-zero off-diagonal elements, and C1 and C2 two random
precision matrix with p% non-zero off-diagonal elements. We choose C1 and C2 to have
the same diagonal elements. Then we set Î¸1 = Î¸ + C1 and Î¸2 = Î¸ + C2, which are then
used to generate the data set for the experiment. The ratio p/q is a rough indication of the
signal. Figure 4-6 show the behavior of the three algorithms for different values of q and
p. For Algorithms 2 and 3 we found that similar precision matrices sometimes leads the
algorithm to converge to the edge of the search domain. This makes sense, since a strong
similarity between the two precision matrices implies a weak signal-to-noise ratio, which
makes the model with no change-point more attractive. Putting the estimated changepoint at the boundary of the search domain is roughly equivalent to fitting a model with
no change-point.
18
Change-Point Computation for Large Graphical Models
(a) q = 25, p = 0 (b) q = 17.5, p = 7.5
(c) q = 10, p = 15 (d) q = 0, p = 25
Figure 5: Behavior of Algorithm 2 for varying signals. Each plot gives a trace plot of
produced estimates, and a histogram of the final change-point estimate. Based
on 200 replications.
19
Bybee and AtchadeÂ´
(a) q = 25, p = 0 (b) q = 17.5, p = 7.5
(c) q = 10, p = 15 (d) q = 0, p = 25
Figure 6: Behavior of Algorithm 3 for varying signals. Each plot gives a trace plot of
produced estimates, and a histogram of the final change-point estimate. Based
on 200 replications.
20
Change-Point Computation for Large Graphical Models
3.4 Sensitivity to the Stopping Criteria in Binary Segmentation
This section considers the stopping condition for the binary segmentation algorithm (see
Section 2.2) and how it performs with different configurations. A condition is required for
determining when the binary segmentation splitting should reject a change-point and stop
running. The stopping condition that we use is the following, stop if
`Ï„ + Cp â‰¥ `F ,
where `Ï„ is the penalized negative log-likelihood obtained with the additional changepoint Ï„ , and `F is the penalized negative log-likelihood without the change-point. The
term C is a user-defined parameter.
As mentioned above, the proposed algorithms can diverge when the step-size Î³ is not
appropriately selected. In particular the appropriate value of Î³ is highly dependent on the
length of the data set, and the binary segmentation splittings of the data can result in
data segments with very different lengths. We use this feature to our advantage. We have
chosen not to tune Î³ to the data segment, and to stop the binary segmentation splitting if
the sequence Ë†Î¸
(k)
1
or Ë†Î¸
(k)
2
appear to diverge. This has the effect of constraining the lengths
of the change-point segments from being too small. We achieve this result without directly
setting a minimum length constraintâ€”which be hard to do in practice. We found that
stopping the algorithm when ||Ë†Î¸
(k)
i
||2
2 > 2 Ã— 103 was sufficient for our data.
In the binary segmentation, since the estimates of Î¸1 and Î¸2 may not have converged by
the end of the search for Ï„ it may be worth continuing the estimation procedure for Î¸1 and
Î¸2 so that the resulting penalized log-likelihoods are comparable. Hence after each split
from the binary segmentation search, we perform an additional 500 iterations to estimate
Î¸1 and Î¸2 at the resulting Ï„ .
See Figure 7 for a series of heatmaps showing how often the binary segmentation
method finds a given number of change-points for different values of C. These results
suggest that the choice of C in the interval (0, 4) is reasonable. These results are produced
using Algorithm 3 for speed, however, the results are identical for the other two algorithms
considered. Note that since an additional change-point should always improve the loglikelihood, when C â‰¤ 0 we only stop on the secondary stopping condition that ||Ë†Î¸
(k)
i
||2
2 >
2 Ã— 103
.
3.5 High Dimensional Experiments
We also investigate the behavior of the proposed algorithms for larger values of p. We performed several (100) runs of Algorithm 3 for T = 1000, and p âˆˆ {100, 500, 750, 1000}.
From these 100 runs we estimate the distributions of the iterates (by boxplots) after
10, 100, 200, . . . , 1000 iterations. The results are presented in Figure 8. The results show
again a very quick convergence toward Ï„? and this convergence persists even as p gets large.
3.6 A Real Data Analysis
In finance and econometrics there is considerable interest in regime-switching models in
the context of volatility, particularly because these switches may correspond to real events
in the economy (Banerjee and Urga, 2005; Beltratti and Morana, 2006; GÂ¨unay, 2014; Choi
et al., 2010). However, much of the literature is limited to the low dimensional case, due
to the difficulty involved in estimating change-points for higher dimensions. We are able
to use our method to estimate change-points in the covariance structure of the Standard
& Poorâ€™s (S&P) 500â€”an American stock market index.
21
Bybee and AtchadeÂ´
Figure 7: Number of change-points detected by binary segmentation as function of the
cost multiplier C. The set of true change-points is indicated on top of the plots.
22
Change-Point Computation for Large Graphical Models
Figure 8: Boxplots of the iterates produced by Algorithm 4. Based on 100 replications.
23
Bybee and AtchadeÂ´
Data from the S&P 500 was collected for the period from 2000-01-01 to 2016-03-03.
From this initial sample a subset of stocks (or tickers) was selected for which at least
3000 corresponding observations exist. This produced a sample extending from 2004-02-06
to 2016-03-03, consisting of 3039 observations and 436 stocks. We follow a similar data
cleaning procedure to Lafferty et al. (2012), who investigate a comparable problem without
change-points. For each stock we generate the log returns, log Xt
Xtâˆ’1
, and standarize the
resulting returns. Following Lafferty et al. (2012), we then truncate (or clip) all observations
beyond three standard deviations of the same mean, thereby limiting unwanted outliers in
our sample. The reason for this cleaning procedure is that these outliers often correspond
to stock splits instead of meaningful price changes.
For our setting Î» = 0.002 and Î³ = 0.5. We initialize Ë†Î¸
(0) = (S(Ï„
(0)) + I)
âˆ’1 where
 = 10âˆ’4 and Ï„
(0) is selected randomly. After the simulated annealing run the proximal
gradient algorithm was run an additional 2000 steps, to produces estimates of Î¸1 and Î¸2.
Here we increase the step-size to Î³ = 350 to accelerate the convergence. For the binary
segmentation we found that selecting the threshold constant, C = 0.005, found a reasonable
set of change-points. We found the choice of parameters important in this application, in
particular, variation from the values used here can lead the algorithm to diverge. We use
the same stopping criterion as with the prior binary-segmentation simulations. That is, a)
stop when `Ï„ + Cp â‰¥ `F or b) stop when ||Ë†Î¸
(k)
i
||2
2 > 2 Ã— 103
.
Figure 9 presents the results of the change-point analysis using binary segmentation
with Algorithm 4. As a reference we also present the results obtained using binary segmentation together with the brute force approach. For the brute force approach, we set Î³ = 35
and ran 10 iterations for each possible change-point, before running 2000 steps at Î³ = 350
to get the estimates for Î¸1 and Î¸2. The brute force approach took approximately an hour
to run one layer of the search, while simulated annealing took approximately 15 minutes.
Figure 9-(a) shows the trace plots from simulated annealing based on 100 replications. The
red lines mark the detected time segments. Figure 9-(b) shows the resulting segmentation
of the data. We note that simulated annealing and brute force produce slightly different sets of change-points. This brings up an important point: the resulting solution is a
local optimum. Binary segmentation does introduce an element of path dependency to
the results so there may be more than one viable set of change-pointsâ€”in this particular
case, the brute force approach starts with the first change-point on August 19th 2011 while
simulated annealing starts with January 11th 2008.
We next look at how well the estimated change-points correspond to real world events.
Our change-point set seems to do a good job of capturing both the Great Recession and
a fall in stock prices during August of 2011 related to the European debt crisis and the
downgrading of United Stateâ€™s credit-rating. The first change-point in our set is January
11th 2008. The National Bureau of Economic Research (NBER) identifies December of
2007 as the beginning of the Great Recession, which this change-point seems to capture.
Additionally, 10 days after the change-point, the Financial Times Stock Exchange (FTSE)
would experience its biggest fall since September 11th 2001. The brute force approach
places this first change-point earlier in the series on July 23rd 2007, possibly capturing
a relatively positive time in the economy before the downturn. The Second change-point
occurred on September 15th 2008, the day on which Lehman Brothers filed for bankruptcy
protection, one of the key events of the Great Recession (both methods agree on this
change-point). The third change-point takes place on March 16th 2009, corresponding
to the end of the bear market in the United States. For bthe brute force approach, this
change-point is June 2nd 2009â€”June of 2009 was when the NBER officially declared the
end of the recession. The fourth change-point, on June 1st 2011, and the fifth change-point,
on December 21st 2011, likely capture a period of heightened concerns over the possible
24
Change-Point Computation for Large Graphical Models
(a) Simulated Annealing trace plots from 100 replications. The red lines represent the prior set of relevant
change-points.
(b) Simulated annealing (top) and brute force segmentations of the data.
Figure 9: Change-points analysis of the S&P 500 data set over the period 2004-02-06 to
2016-03-03.
spread of the European debt crisis to Spain and Italy, during August of 2011. This period
also saw the downgrading of the S&Pâ€™s credit rating of the United States from AAA to
25
Bybee and AtchadeÂ´
Figure 10: Adjacency matrices between stocks based on estimated precision matrices Ë†Î¸
for each time segment. A black dot represents an edge between two stocks.
AA+. The August 19th 2011 brute force change-point more precisely identifies this August
downturn.
Given that the change-point set identified seems sensible, we then investigate what
the corresponding Ë†Î¸ estimates look like, and whether any interesting conclusions can be
drawn from our estimates. Here we focus only on the simulated annealing change-point
set. See Figure 10 for a plot of the adjacency matrix for each Ë†Î¸ estimate. The black
squares correspond to non-zero edges and he yellow boxes correspond to Global Industry
Classification Standard (GICS) sectors. These results tell an intuitive story about how the
economy behaves during financial crises. Following both the collapse of Lehamn Brotherâ€™s
and the events of August 2011, we see a dramatic increase in connectivity between returns
even outside of GICS sectors. To get a better sense of this see Figure 11 for a similar
series of plots where edges are summed over each sector. Figure 12 gives an expanded
version of the summed edge plot for the first Ë†Î¸ estimate, as well as the corresponding
sector labels for reference. Again, we can see that during periods of crisis, the off diagonal
elementsâ€”corresponding to edges between different sectorsâ€”become more significant than
during periods of general stability.
From these figures we can get a sense of which sectors are most affected during times
of crisis. To expand upon this some, see Figure 12 for the edge count between each sector
26
Change-Point Computation for Large Graphical Models
Figure 11: Adjacency matrices between sectors for each time segment. Based on the
number of edges going from stocks of one sector to another as given by the
estimated precision matrices Ë†Î¸.
and the Financial sector for each Ë†Î¸ estimate. We can see that during times of crisis,
there is considerable connection between Industrials, Information Technology, Consumer
Discretionary, and to a lesser extend Healthcare, and the Financial sector. Consumer
Staples, Utilities, and Materials appear to be more stable during these periods and do not
experience as much correlation with Financials. This might suggest that our method could
be used as a tool to identify investment strategies that are likely to be resilient to periods
of crisis in the market.
4. Proofs
4.1 Proof of Theorem 5
We will need the following lemma.
Lemma 12 Set
g(Î¸)
def = âˆ’ log det(Î¸) + Tr(Î¸S),
and Ï†(Î¸)
def = g(Î¸) + Î»

Î±kÎ¸k1 +
1 âˆ’ Î±
2
kÎ¸k
2
F

, Î¸ âˆˆ M+
p
,
for some symmetric matrix S, Î± âˆˆ (0, 1), and Î» > 0. Fix 0 < b < B â‰¤ âˆ.
1. For Î¸, Ï‘ âˆˆ M+
p
(b, B), we have
g(Î¸) + hâˆ‡g(Î¸), Ï‘ âˆ’ Î¸i +
1
2B2
kÏ‘ âˆ’ Î¸k
2
F â‰¤ g(Ï‘)
â‰¤ g(Î¸) + hâˆ‡g(Î¸), Ï‘ âˆ’ Î¸i +
1
2b
2
kÏ‘ âˆ’ Î¸k
2
F
.
27
Bybee and AtchadeÂ´
Figure 12: Number of edges between the financial sector and the remaining sectors, for
each time segment. Based on the estimated precision matrices Ë†Î¸.
More generally, If Î¸, Ï‘ âˆˆ M+
p
, then
g(Ï‘) âˆ’ g(Î¸) âˆ’ hâˆ‡g(Î¸), Ï‘ âˆ’ Î¸i â‰¥ kÏ‘ âˆ’ Î¸k
2
F
4kÎ¸k2

kÎ¸k2 +
1
2
kÏ‘ âˆ’ Î¸kF
 .
2. Let Î³ âˆˆ (0, b2
], and Î¸, Â¯Î¸, Î¸0 âˆˆ M+
p
(b, B). Suppose that
Â¯Î¸ = ProxÎ³Î»
Î¸ âˆ’ Î³(S âˆ’ Î¸
âˆ’1
)

,
then
2Î³

Ï†(
Â¯Î¸) âˆ’ Ï†(Î¸0)

+

Â¯Î¸ âˆ’ Î¸0


2
F
â‰¤

1 âˆ’
Î³
B2

kÎ¸ âˆ’ Î¸0k
2
F
.
Proof The first part of (1) is Lemma 12 of AtchadÂ´e et al. (2015), and Part (2) is Lemma
14 of AtchadÂ´e et al. (2015). The second part of (1) can be proved along similar lines. For
completeness we give the details below.
Take Î¸0, Î¸1 âˆˆ M+
p
. By Taylor expansion we have
g(Î¸1) âˆ’ g(Î¸0) âˆ’ hâˆ‡g(Î¸0), Î¸1 âˆ’ Î¸0i = âˆ’
Z 1
0


(Î¸0 + tH)
âˆ’1 âˆ’ Î¸
âˆ’1
0
, H
dt,
where H
def = Î¸1 âˆ’ Î¸0. We have (Î¸0 + tH)
âˆ’1 âˆ’ Î¸
âˆ’1
0 = âˆ’tÎ¸âˆ’1
0 H(Î¸0 + tH)
âˆ’1
, which leads to
g(Î¸1) âˆ’ g(Î¸0) âˆ’ hâˆ‡g(Î¸0), Î¸1 âˆ’ Î¸0i =
Z 1
0
Tr
Î¸
âˆ’1
0 H(Î¸0 + tH)
âˆ’1H

tdt.    
Change-Point Computation for Large Graphical Models
If Î¸0 =
Pp
i=1 Ïjuju
0
j
is the eigendecomposition of Î¸0, we see that Tr
Î¸
âˆ’1
0 H(Î¸0 + tH)
âˆ’1H

= Pp
j=1
1
Ïj
u
0
jH(Î¸0 + tH)
âˆ’1Huj . Hence
g(Î¸1) âˆ’ g(Î¸0) âˆ’ hâˆ‡g(Î¸0), Î¸1 âˆ’ Î¸0i â‰¥ Xp
j=1
kHujk
2
2
Z 1
0
tdt
kÎ¸0k2 (kÎ¸0k2 + tkHkF)
â‰¥
Pp
j=1 kHujk
2
2
4kÎ¸0k2

kÎ¸0k2 +
1
2
kHkF
 ,
and the result follows by noting that Pp
j=1 kHujk
2
2 = kHk
2
F
.
Set
F(Ï„, Î¸1, Î¸2) = g1,Ï„ (Î¸1) + Î»1,Ï„ p(Î¸) + g2,Ï„ (Î¸2) + Î»2,Ï„ p(Î¸2),
F = F(Ë†Ï„, Ë†Î¸1,Ï„Ë†,
Ë†Î¸1,Ï„Ë†) the value of Problem (3), and Fk = F(Ï„
(k)
, Î¸(k)
1
, Î¸(k)
2
) âˆ’ F.
Lemma 13 Suppose that Î³ âˆˆ (0, b
2
1âˆ§b
2
2
], and for j = 1, 2, Î¸
(0)
j âˆˆ M+
p
(bj , Bj ). Then limk



Î¸
(k)
1 âˆ’ Ë†Î¸1,Ï„ (k)



F
=
0, limk



Î¸
(k)
2 âˆ’ Ë†Î¸2,Ï„ (k)



F
= 0. Furthermore the sequence {Fk} is non-increasing, and limk Fk exists.
Proof We know from Lemma 2 that for Î³ âˆˆ (0, b
2
1 âˆ§ b
2
2
], and Î¸
(0)
j âˆˆ M+
p
(bj , Bj ), we have
Î¸
(k)
j âˆˆ M+
p
(bj , Bj ) for all k â‰¥ 0, for j = 1, 2. We have,
Fk+1 âˆ’ Fk = F(Ï„
(k+1), Î¸(k+1)
1
, Î¸(k+1)
2
) âˆ’ F(Ï„
(k)
, Î¸(k+1)
1
, Î¸(k+1)
2
)
+ F(Ï„
(k)
, Î¸(k+1)
1
, Î¸(k+1)
2
) âˆ’ F(Ï„
(k)
, Î¸(k)
1
, Î¸(k)
2
).
By definition, F(Ï„
(k+1), Î¸(k+1)
1
, Î¸(k+1)
2
) âˆ’ F(Ï„
(k)
, Î¸(k+1)
1
, Î¸(k+1)
2
) â‰¤ 0, and by Lemma 12-
Part(2),
F(Ï„
(k)
, Î¸(k+1)
1
, Î¸(k+1)
2
) âˆ’ F(Ï„
(k)
, Î¸(k)
1
, Î¸(k)
2
)
â‰¤ âˆ’
1
2Î³



Î¸
(k+1)
1 âˆ’ Î¸
(k)
1



2
F
âˆ’
1
2Î³



Î¸
(k+1)
2 âˆ’ Î¸
(k)
2



2
F
It follows that
Fk+1 â‰¤ Fk âˆ’
1
2Î³



Î¸
(k+1)
1 âˆ’ Î¸
(k)
1



2
F
âˆ’
1
2Î³



Î¸
(k+1)
2 âˆ’ Î¸
(k)
2



2
F
,
which implies that
lim
k



Î¸
(k+1)
1 âˆ’ Î¸
(k)
1



F
= 0, and lim
k



Î¸
(k+1)
2 âˆ’ Î¸
(k)
2



F
= 0. (15)
It also implies that the sequence {Fk} is non-increasing and bounded from below by 0.
Hence converges. Another application of Lemma 12 gives
2Î³

F(Ï„
(k)
, Î¸(k+1)
1
, Î¸(k+1)
2
) âˆ’ F(Ï„
(k)
,
Ë†Î¸1,Ï„ (k) ,
Ë†Î¸2,Ï„ (k) )

+



Î¸
(k+1)
1 âˆ’ Ë†Î¸1,Ï„ (k)



2
F
+



Î¸
(k+1)
2 âˆ’ Ë†Î¸2,Ï„ (k)



2
F
â‰¤

1 âˆ’
Î³
B
2
1
 


Î¸
(k)
1 âˆ’ Ë†Î¸1,Ï„ (k)



2
F
+

1 âˆ’
Î³
B
2
2
 


Î¸
(k)
2 âˆ’ Ë†Î¸2,Ï„ (k)



2
F
.
  
Bybee and AtchadeÂ´
And notice that F(Ï„
(k)
, Î¸(k+1)
1
, Î¸(k+1)
2
) âˆ’ F(Ï„
(k)
,
Ë†Î¸1,Ï„ (k) ,
Ë†Î¸2,Ï„ (k) ) â‰¥ 0. Hence



Î¸
(k+1)
1 âˆ’ Ë†Î¸1,Ï„ (k)



2
F
+



Î¸
(k+1)
2 âˆ’ Ë†Î¸2,Ï„ (k)



2
F
â‰¤

1 âˆ’
Î³
B
2
1
 


Î¸
(k)
1 âˆ’ Ë†Î¸1,Ï„ (k)



2
F
+

1 âˆ’
Î³
B
2
2
 


Î¸
(k)
2 âˆ’ Ë†Î¸2,Ï„ (k)



2
F
,
which can be written as
Î³
B
2
1



Î¸
(k)
1 âˆ’ Ë†Î¸1,Ï„ (k)



2
F
+
Î³
B
2
2



Î¸
(k)
2 âˆ’ Ë†Î¸2,Ï„ (k)



2
F
â‰¤



Î¸
(k+1)
1 âˆ’ Î¸
(k)
1



2
F
+



Î¸
(k+1)
2 âˆ’ Î¸
(k)
2



2
F
âˆ’ 2
D
Î¸
(k+1)
1 âˆ’ Î¸
(k)
1
, Î¸(k+1)
1 âˆ’ Ë†Î¸1,Ï„ (k)
E
âˆ’ 2
D
Î¸
(k+1)
2 âˆ’ Î¸
(k)
2
, Î¸(k+1)
2 âˆ’ Ë†Î¸2,Ï„ (k)
E
.
Since {Î¸
(k)
1
}, {Î¸
(k)
2
} {Ë†Î¸1,Ï„ (k) }, and {
Ë†Î¸2,Ï„ (k) } are bounded sequence, and given (15), letting
k â†’ âˆ, we conclude that
lim
k



Î¸
(k)
1 âˆ’ Ë†Î¸1,Ï„ (k)



F
= 0, and lim
k



Î¸
(k)
2 âˆ’ Ë†Î¸2,Ï„ (k)



F
= 0.
Proof of Theorem 5 Let  > 0 as in H1. By Lemma 13, there exist k0 â‰¥ 1 such that for
all k â‰¥ k0,



Î¸
(k+1)
1 âˆ’ Ë†Î¸1,Ï„ (k)



F
â‰¤ , and



Î¸
(k+1)
2 âˆ’ Ë†Î¸2,Ï„ (k)



F
â‰¤ . Since
Ï„
(k+1) = ArgmintâˆˆT H

t|Î¸
(k+1)
1
, Î¸(k+1)
2

,
using H1 we conclude that for all k â‰¥ k0,


Ï„
(k+1) âˆ’ Ï„?


 â‰¤ Îº


Ï„
(k) âˆ’ Ï„?


 + c â‰¤ Îº
kâˆ’k0+1


Ï„
(k0) âˆ’ Ï„?


 +
c
1 âˆ’ Îº
,
which implies the stated result.
4.2 Proof of Theorem 9
We introduce some more notation. Given M âˆˆ R
pÃ—p
the sparsity structure of M is the
matrix Î´ âˆˆ {0, 1}
pÃ—p
such that Î´jk = 1{|Mjk|>0}. In particular we will write Î´?,j (j = 1, 2)
to denote the sparsity structure of Î¸?,j . Given matrices A âˆˆ R
pÃ—p
, and Î´ âˆˆ {0, 1}
pÃ—p
, we
will use the notation AÎ´ (resp. AÎ´
c ) to denote the component-wise product of A and Î´
(resp A and 1 âˆ’ Î´). Given j âˆˆ {1, 2}, we define
Cj
def =
n
M âˆˆ Mp : kMÎ´
c
?,j
k1 â‰¤ 7kMÎ´?,j k1.
o
. (16)
We will need the following deviation bound.
Lemma 14 Suppose that Xi
indâˆ¼ N(0, Î¸âˆ’1
i
), i = 1, . . . , N, where Î¸i âˆˆ M+
p
. We set Î£i
def = Î¸
âˆ’1
i
, and
define
Îºi
(2) def = inf {u
0Î£iu, kuk2 = 1, kuk0 â‰¤ 2} , ÎºÂ¯i(2) def
= sup {u
0Î£iu, kuk2 = 1, kuk0 â‰¤ 2} ,
30
Change-Point Computation for Large Graphical Models
and suppose that Îºi
(2) > 0 for i = 1, . . . , N. Set GN
def = N âˆ’1 PN
i=1(XiX0
i âˆ’ Î¸
âˆ’1
i
). Then for
0 < Î´ â‰¤ 2

mink Îºk
(2)
maxk ÎºÂ¯k(2)2
, we have
P

kGN kâˆ >

max
k
ÎºÂ¯k(2)
Î´

â‰¤ 4p
2
e
âˆ’ NÎ´2
4 .
Proof The proof is similar to the proof of Lemma 1 of Ravikumar et al. (2010), which
itself builds on Bickel and Levina (2008). For 1 â‰¤ i, j â‰¤ p, arbitrary, set Z
(k)
ij = Xk,iXk,j ,
and Ïƒ
(k)
ij = Î£k,ij , so that the (i, j)-th component of GN is N âˆ’1 PN
k=1(Z
(k)
ij âˆ’Ïƒ
(k)
ij ). Suppose
that i 6= j. The case i = j is simpler. It is easy to check that
X
N
k=1
h
Z
(k)
ij âˆ’ Ïƒ
(k)
ij i
=
1
4
X
N
k=1
h
(Xk,i + Xk,j )
2 âˆ’ Ïƒ
(k)
ii âˆ’ Ïƒ
(k)
jj âˆ’ 2Ïƒ
(k)
ij i
âˆ’
1
4
X
N
k=1
h
(Xk,i âˆ’ Xk,j )
2 âˆ’ Ïƒ
(k)
ii âˆ’ Ïƒ
(k)
jj + 2Ïƒ
(k)
ij i
.
Notice that Xk,i+Xk,j âˆ¼ N(0, Ïƒ
(k)
ii +Ïƒ
(k)
jj +2Ïƒ
(k)
ij ), and Xk,iâˆ’Xk,j âˆ¼ N(0, Ïƒ
(k)
ii +Ïƒ
(k)
jj âˆ’2Ïƒ
(k)
ij ).
It follows that for all x â‰¥ 0,
P
"




X
N
k=1
h
Z
(k)
ij âˆ’ Ïƒ
(k)
ij i





> x#
â‰¤ P
"




X
N
k=1
a
(k)
ij (Wk âˆ’ 1)





> 2x
#
+ P
"




X
N
k=1
b
(k)
ij (Wk âˆ’ 1)





> 2x
#
,
where W1:N
i.i.d. âˆ¼ Ï‡
2
1
, a
(k)
ij = Ïƒ
(k)
ii +Ïƒ
(k)
jj + 2Ïƒ
(k)
ij , and b
(k)
ij = Ïƒ
(k)
ii +Ïƒ
(k)
jj âˆ’2Ïƒ
(k)
ij . For any x â‰¥ 0
and a sequence a = (a1, . . . , aN ) of positive numbers, with |a|âˆ = maxi
|ai
|, |a|2 =
pP
i
a
2
i
,
we write
2x = 2|a|2

x
2|a|2

+ 2|a|âˆ

4|a|
2
2
2x|a|âˆ
  x
2|a|2
2
.
Therefore if 2x|a|âˆ â‰¤ 4|a|
2
2
, we can apply Lemma 1 of Laurent and Massart (2000) to
conclude that
P
 




X
N
k=1
ak(Wk âˆ’ 1)





â‰¥ 2x
!
â‰¤ 2e
âˆ’ x
2
4|a|
2
2 .
In particular, we can apply the above bound with x = |a|âˆNÎ´ for Î´ âˆˆ (0,
2 minj a
2
i
maxi a
2
i
] to get
that
P
 




X
N
k=1
ak(Wk âˆ’ 1)





â‰¥ 2|a|âˆNÎ´!
â‰¤ 2e
âˆ’ NÎ´2
4 .
In the particular case above, a
(k)
ij = Ïƒ
(k)
ii + Ïƒ
(k)
jj + 2Ïƒ
(k)
ij = u
0Î£
(k)u, where ui = uj = 1,
and ur = 0 for r /âˆˆ {i, j}. And
mink u
0Î£
(k)u
maxk u
0Î£(k)u
â‰¥
mink Îºk
(2)
maxk ÎºÂ¯(2) .
A similar bound holds for b
(k)
ij . The lemma follows from a standard union-sum argument.
31
Bybee and AtchadeÂ´
The following event plays an important role in the analysis.
En
def =
\
Ï„âˆˆT

1
Î»1,Ï„
kâˆ‡g1,Ï„ (Î¸?,1)kâˆ â‰¤
Î±
2
, and 1
Î»2,Ï„
kâˆ‡g2,Ï„ (Î¸?,2)kâˆ â‰¤
Î±
2

, (17)
Lemma 15 Under the assumptions of the theorem
P(En) â‰¥ 1 âˆ’
8
pT
.
Proof We have
P(E
c
n
) â‰¤ P

max
Ï„âˆˆT
1
Î»1,Ï„
kâˆ‡g1,Ï„ (Î¸?,1)kâˆ >
Î±
2

+ P

max
Ï„âˆˆT
1
Î»2,Ï„
kâˆ‡g2,Ï„ (Î¸?,2)kâˆ >
Î±
2

.
We show how to bound the first term. A similar bound follows for g2,Ï„ by working on
the reversed sequence X(T)
, . . . , X(1). We have âˆ‡g1,Ï„ (Î¸) = Ï„
2T
(S1(Ï„ ) âˆ’ Î¸
âˆ’1
). Setting
U
(t) def = X(t)
(X(t)
)
0 âˆ’ E

X(t)
(X(t)
)
0

, we can write
âˆ‡g1,Ï„ (Î¸?,1) = 1
2T
XÏ„
t=1
U
(t) +
(Ï„ âˆ’ Ï„?)+
2T
(Î¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
?,1
),
where a+
def = max(a, 0). Hence by a standard union-bound argument,
P

max
Ï„âˆˆT
1
Î»1,Ï„
kâˆ‡g1,Ï„ (Î¸?,1)kâˆ >
Î±
2

â‰¤
X
Ï„âˆˆT
P
 




XÏ„
t=1
U
(t)





âˆ
> Î±Î»1,Ï„T âˆ’ (Ï„ âˆ’ Ï„?)+kÎ¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
?,1
kâˆ
!
.
Given the choice of Î»1,Ï„ in (8), Î±Î»1,Ï„T /2 = 2âˆš
3Â¯Îº
p
Ï„ log(pT) â‰¥ (Ï„ âˆ’ Ï„?)+kÎ¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
?,1
kâˆ,
by assumption (11). In view of (10) we can apply Lemma 14 to deduce that
P

max
Ï„âˆˆT
1
Î»1,Ï„
kâˆ‡g1,Ï„ (Î¸?,1)kâˆ >
Î±
2

â‰¤
X
Ï„âˆˆT
P
 




1
Ï„
XÏ„
t=1
U
(t)





âˆ
>
Î±Î»1,Ï„T
2Ï„
!
â‰¤ 4T p2
e
âˆ’ Ï„
4
 Î±Î»1,Ï„ T
2Ï„ÎºÂ¯
2
â‰¤ 4 exp (2 log(pT) âˆ’ 3 log(pT)) â‰¤
4
pT
.
Lemma 16 Under the assumptions of the theorem, and on the event En, we have



Ë†Î¸1,Ï„ âˆ’ Î¸?,1



F
â‰¤ AÎºÂ¯kÎ¸?,1k
2
2
r
s1 log(pT)
Ï„
,
and



Ë†Î¸2,Ï„ âˆ’ Î¸?,2



F
â‰¤ AÎºÂ¯kÎ¸?,2k
2
2
r
s2 log(pT)
T âˆ’ Ï„
,
for all Ï„ âˆˆ T , where A is an absolute constant that can be taken as A = 16 Ã— 20 Ã—
âˆš
48.
3 
Change-Point Computation for Large Graphical Models
Proof Fix j âˆˆ {1, 2}, and Ï„ âˆˆ T . Set Â¯gj,Ï„ (Î¸)
def = gj,Ï„ (Î¸) + (1 âˆ’ Î±)Î»j,Ï„ kÎ¸kF
/2, and recall
that Ï†j,Ï„ (Î¸)
def = gj,Ï„ (Î¸) + Î»j,Ï„â„˜(Î¸). Hence Ï†j,Ï„ (Î¸) = Â¯gj,Ï„ (Î¸) + Î±Î»j,Ï„ kÎ¸k1. By a very standard
argument that can be found for instance in Negahban et al. (2012), it is known that on
the event En, and if Î± satisfies (9) then we have Ë†Î¸j,Ï„ âˆ’ Î¸?,j âˆˆ Cj , where the cones Cj are as
defined in (16). We write
Ï†j,Ï„ (
Ë†Î¸j,Ï„ ) âˆ’ Ï†j,Ï„ (Î¸?,j ) = D
âˆ‡gj,Ï„ (Î¸?,j ) + (1 âˆ’ Î±)Î»j,Ï„ Î¸?,j ,
Ë†Î¸j,Ï„ âˆ’ Î¸?,jE
+Â¯gj,Ï„ (
Ë†Î¸j,Ï„ ) âˆ’ gÂ¯j,Ï„ (Î¸?,j ) âˆ’
D
âˆ‡gÂ¯j,Ï„ (Î¸?,j ),
Ë†Î¸j,Ï„ âˆ’ Î¸?,jE
+Î±Î»j,Ï„ 
k
Ë†Î¸j,Ï„ k1 âˆ’ kÎ¸?,jk1

.
On En,
Ë†Î¸j,Ï„ âˆ’ Î¸?,j âˆˆ Cj . Therefore
Î±Î»j,Ï„


k
Ë†Î¸j,Ï„ k1 âˆ’ kÎ¸?,jk1


 â‰¤ Î±Î»j,Ï„



Ë†Î¸j,Ï„ âˆ’ Î¸?,j



1
â‰¤ 8Î±Î»j,Ï„âˆš
sj



Ë†Î¸j,Ï„ âˆ’ Î¸?,j



F
,
and



D
âˆ‡gj,Ï„ (Î¸?,j ) + (1 âˆ’ Î±)Î»j,Ï„ Î¸?,j ,
Ë†Î¸j,Ï„ âˆ’ Î¸?,jE


â‰¤
Î»j,Ï„
2
(Î± + 2(1 âˆ’ Î±)kÎ¸?,jkâˆ)



Ë†Î¸j,Ï„ âˆ’ Î¸?,j



1
â‰¤ 4Î»j,Ï„ (Î± + 2(1 âˆ’ Î±)kÎ¸?,jkâˆ)
âˆš
sj



Ë†Î¸j,Ï„ âˆ’ Î¸?,j



F
.
Suppose j = 1. The case j = 2 is similar. We then set âˆ†1,Ï„
def = Ë†Î¸1,Ï„ âˆ’ Î¸?,1, and use the
second part of Lemma 12 (1) to deduce that
gÂ¯1,Ï„ (
Ë†Î¸1,Ï„ ) âˆ’ gÂ¯1,Ï„ (Î¸?,1) âˆ’
D
âˆ‡gÂ¯1,Ï„ (Î¸?,1),
Ë†Î¸1,Ï„ âˆ’ Î¸?,1
E
â‰¥ g1,Ï„ (
Ë†Î¸1,Ï„ ) âˆ’ g1,Ï„ (Î¸?,1) âˆ’
D
âˆ‡g1,Ï„ (Î¸?,1),
Ë†Î¸1,Ï„ âˆ’ Î¸?,1
E
â‰¥
Ï„
2T
kâˆ†1,Ï„ k
2
F
2kÎ¸?,1k2 (2kÎ¸?,1k2 + kâˆ†1,Ï„ kF)
.
Set c1 =
Ï„
4TkÎ¸?,1k
2
2
, c2 = 4Î»1,Ï„âˆš
s1 (3Î± + 2(1 âˆ’ Î±)kÎ¸?,1kâˆ). Since Ï†1,Ï„ (
Ë†Î¸1,Ï„ )âˆ’Ï†1,Ï„ (Î¸?,1) â‰¤ 0,
the above derivation shows that on the event En,
c1 kâˆ†1,Ï„ k
2
F
2 + 1
kÎ¸?,1k2
kâˆ†1,Ï„ kF
âˆ’ c2 kâˆ†1,Ï„ kF â‰¤ 0,
Under the assumption that c1 â‰¥ 2c2/kÎ¸?,1k2 (which we impose in (10)), this implies that
kâˆ†1,Ï„ kF â‰¤
4c2
c1
â‰¤ AÎºÂ¯kÎ¸?,1k
2
2
r
s1 log(pT)
Ï„
,
where A = 16 Ã— 20 Ã—
âˆš
48, as claimed.
Proof of Theorem 9 For Ï„ âˆˆ T , let
r1,Ï„
def = AÎºÂ¯kÎ¸?,1k
2
2
r
s1 log(pT)
Ï„
, r2,Ï„
def = AÎºÂ¯kÎ¸?,2k
2
2
r
s2 log(pT)
T âˆ’ Ï„
,
33
Bybee and AtchadeÂ´
be the convergence rates obtained in Lemma 16. Let  > 0 be given by

def = min
Ï„âˆˆT
(r1,Ï„ âˆ§ r1,Ï„ ).
For j = 1, 2, let Î¸j âˆˆ M+
p be such that kÎ¸j âˆ’ Ë†Î¸Ï„,jk1 â‰¤ . Set Ë‡Ï„ = ArgmintâˆˆT H(t|Î¸1, Î¸2),
where H is as defined in (4). Set
C0 = min 
kÎ¸?,2 âˆ’ Î¸?,1k
4
F
128B4kÎ¸?,2 âˆ’ Î¸?,1k
2
1
,
Îº
ÎºÂ¯
4

.
We will show below that
P

|Ï„Ë‡ âˆ’ Ï„?| >
4 log(p)
C0

â‰¤
8
pT
+
4
p
2 (1 âˆ’ eâˆ’C0 )
. (18)
This implies that with probability at least 1âˆ’
8
pT âˆ’
4
p2(1âˆ’eâˆ’C0 )
, Assumption H1 holds (with
 â† /âˆšp, Îº = 0, and c = (4/C0) log(p)). The theorem then follows by applying Theorem
5.
Given Î¸j âˆˆ M+
p be such that kÎ¸j âˆ’ Ë†Î¸Ï„,jk1 â‰¤ , we will now show that (18) holds. We
shall bound P(Ë‡Ï„ > Ï„? + Î´), Î´ = (4/C0) log(p). The bound on P(Ë‡Ï„ < Ï„? âˆ’ Î´) follows similarly
by working with the reversed sequence X(T)
, . . . , X(1)
.
Note that Î¸j can be written as
Î¸j = (Î¸j âˆ’ Ë†Î¸Ï„,j ) + (Ë†Î¸Ï„,j âˆ’ Î¸?,j ) + Î¸?,j . (19)
This implies that on En, for  â‰¤ rj,Ï„ , and rj,Ï„ â‰¤ min 
Î»min(Î¸?,j )
4
,
kÎ¸?,j kâˆ
2
,
kÎ¸?,j k1
1+8s
1/2
j

, we have
Î»min(Î¸j ) â‰¥
1
2
Î»min(Î¸?,j ), Î»max(Î¸j ) â‰¤ 2Î»max(Î¸?,j ),
kÎ¸jkâˆ â‰¤ 2kÎ¸?,jkâˆ, and kÎ¸jk1 â‰¤ 2kÎ¸?,jk1. (20)
Using the event En introduced in (17), we have
P (Ë‡Ï„ > Ï„? + Î´) â‰¤ P(E
c
n
) + X
jâ‰¥0: Ï„?+Î´+jâˆˆT
P (En, Ï„Ë‡ = Ï„? + Î´ + j)
â‰¤ P(E
c
n
) + X
jâ‰¥0: Ï„?+Î´+jâˆˆT
P (En, Ï†1,Ï„?+Î´+j (Î¸1) + Ï†2,Ï„?+Î´+j (Î¸2) â‰¤ Ï†1,Ï„?
(Î¸1) + Ï†2,Ï„?
(Î¸2)), (21)
where Ï†j,Ï„ (Î¸)
def = gj,Ï„ (Î¸) + Î»j,Ï„â„˜(Î¸). First we are going to bound the probability
P (En, Ï†1,Ï„ (Î¸1) + Ï†2,Ï„ (Î¸2) â‰¤ Ï†1,Ï„?
(Î¸1) + Ï†2,Ï„?
(Î¸2)),
for some arbitrary Ï„ âˆˆ T , Ï„ > Ï„?. A simple calculation shows that
2T
Ï„ âˆ’ Ï„?
[Ï†1,Ï„ (Î¸1) + Ï†2,,Ï„ (Î¸2) âˆ’ Ï†1,Ï„?
(Î¸1) âˆ’ Ï†2,Ï„?
(Î¸2)] = âˆ’ log det(Î¸1) + log det(Î¸2)
+


Î¸1 âˆ’ Î¸2, Î¸âˆ’1
?,2

+
*
Î¸1 âˆ’ Î¸2,
1
Ï„ âˆ’ Ï„?
XÏ„
t=Ï„?+1

X(t)X(t)
0
âˆ’ Î¸
âˆ’1
?,2

+
+ 2T

Î»1,Ï„ âˆ’ Î»1,Ï„?
Ï„ âˆ’ Ï„?
 1 âˆ’ Î±
2
kÎ¸1k
2
F + Î±kÎ¸1k1

+ 2T

Î»2,Ï„ âˆ’ Î»2,Ï„?
Ï„ âˆ’ Ï„?
 1 âˆ’ Î±
2
kÎ¸2k
2
F + Î±kÎ¸2k1

.
34
Change-Point Computation for Large Graphical Models
We have 2T

Î»1,Ï„ âˆ’Î»1,Ï„?
Ï„âˆ’Ï„?

1âˆ’Î±
2
kÎ¸1k
2
F + Î±kÎ¸1k1

â‰¥ 0, and
2T




Î»2,Ï„ âˆ’ Î»2,Ï„?
Ï„ âˆ’ Ï„?




â‰¤
ÎºÂ¯
Î±
r
48 log(pT)
T âˆ’ Ï„
=
c0r2,Ï„
Î±s
1/2
2
kÎ¸?,2k
2
2
,
for some absolute constant c0. Using the infinity-norm and 1-norm bounds in (20) together
with (9), we have
1 âˆ’ Î±
2
kÎ¸2k
2
F + Î±kÎ¸2k1 = Î±

1 âˆ’ Î±
2Î±
kÎ¸2kâˆ + 1
kÎ¸2k1 â‰¤ 4Î±kÎ¸?,2k1,
and it follows that
2T




Î»2,Ï„ âˆ’ Î»2,Ï„?
Ï„ âˆ’ Ï„?





1 âˆ’ Î±
2
kÎ¸2k
2
F + Î±kÎ¸2k1

â‰¤ CÏ„
def =

4c0kÎ¸?,2k1
s
1/2
2
kÎ¸?,2k
2
2
!
r2,Ï„ .
Set
b
def = min (Î»min(Î¸?,1), Î»min(Î¸?,2)), B def = max (kÎ¸?,1k2, kÎ¸?,2k2).
By the strong convexity of log det (Lemma 12 Part(1)) we have:
âˆ’ log det(Î¸1) + log det(Î¸2) + 

Î¸1 âˆ’ Î¸2, Î¸âˆ’1
?,2

â‰¥


Î¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
2
, Î¸1 âˆ’ Î¸2

+
1
2B2
kÎ¸1 âˆ’ Î¸2k
2
F
.
Since Î¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
2 = Î¸
âˆ’1
?,2
(Î¸2 âˆ’ Î¸?,2)Î¸
âˆ’1
2
, and using the fact that kABkF â‰¤ kAk2kBkF, we have
that on En,




Î¸
âˆ’1
?,2 âˆ’ Î¸
âˆ’1
2
, Î¸1 âˆ’ Î¸2

 â‰¤ 2r2,Ï„ kÎ¸
âˆ’1
?,2
k2kÎ¸
âˆ’1
2
k2kÎ¸2 âˆ’ Î¸1kF â‰¤ 4r2,Ï„ kÎ¸
âˆ’1
?,2
k
2
2kÎ¸2 âˆ’ Î¸1kF.
We conclude that on En,
2T
Ï„ âˆ’ Ï„?
[Ï†1,Ï„ (Î¸1) + Ï†2,Ï„ (Î¸2) âˆ’ Ï†1,Ï„?
(Î¸1) âˆ’ Ï†2,Ï„?
(Î¸2)] â‰¥
*
Î¸1 âˆ’ Î¸2,
1
Ï„ âˆ’ Ï„?
XÏ„
t=Ï„?+1

X(t)X(t)
0
âˆ’ Î¸
âˆ’1
?,2

+
âˆ’ CÏ„ âˆ’ 4r2,Ï„ kÎ¸
âˆ’1
?,2
k
2
2kÎ¸2 âˆ’ Î¸1kF +
1
2B2
kÎ¸1 âˆ’ Î¸2k
2
F
.
Under the assumption (12) imposed on rj,Ï„ and for  â‰¤ r1,Ï„ âˆ§ r2,Ï„ , it can be shown that on
En, and for kÎ¸?,2 âˆ’ Î¸?,1kF â‰¥
8c0kÎ¸?,2k1
s
1/2
2
kÎ¸?,2k
2
2
kÎ¸
âˆ’1
?,2
k
2
2
, we have
âˆ’CÏ„ âˆ’ 2 ( + r2,Ï„ ) kÎ¸
âˆ’1
?,2
k
2
2kÎ¸2 âˆ’ Î¸1kF +
1
4B2
kÎ¸1 âˆ’ Î¸2k
2
F â‰¥ 0. (22)
To see this, note that (22) holds if kÎ¸2âˆ’Î¸1kF â‰¥ 8B2
r2,Ï„ kÎ¸
âˆ’1
?,2
k
2
2+2B
q
CÏ„ + 16B2kÎ¸
âˆ’1
?,2
k
4
2
r
2
2,Ï„ .
Then it can be checked that if r2,Ï„ â‰¤
c0kÎ¸?,2k1
16B2s
1/2
2
kÎ¸?,2k
2
2
kÎ¸
âˆ’1
?,2
k
4
2
, then
8B
2
kÎ¸
âˆ’1
?,2
k
2
2
r2,Ï„ â‰¤
CÏ„
2kÎ¸
âˆ’1
?,2
k
2
2
r2,Ï„
, and 4B
p
CÏ„ â‰¤
CÏ„
2kÎ¸
âˆ’1
?,2
k
2
2
r2,Ï„
.
3 
Bybee and AtchadeÂ´
Therefore, (22) holds if
kÎ¸2 âˆ’ Î¸1kF â‰¥
CÏ„
kÎ¸
âˆ’1
?,2
k
2
2
r2,Ï„
=
4c0kÎ¸?,2k1
s
1/2
2
kÎ¸?,2k
2
2
kÎ¸
âˆ’1
?,2
k
2
2
.
Now we write
Î¸2 âˆ’ Î¸1 = (Î¸2 âˆ’ Ë†Î¸Ï„,2) + (Ë†Î¸Ï„,2 âˆ’ Î¸?,2) + (Î¸?,2 âˆ’ Î¸?,1) + (Î¸?,1 âˆ’ Ë†Î¸Ï„,1) + (Ë†Î¸Ï„,1 âˆ’ Î¸1),
and use the fact that  â‰¤ r1,Ï„ âˆ§ r2,Ï„ , and rj,Ï„ â‰¤ kÎ¸?,2 âˆ’ Î¸?,1kF/8 to deduce that on En,
kÎ¸2 âˆ’ Î¸1kF â‰¥ kÎ¸?,2 âˆ’ Î¸?,1kF/2, and this completes the proof of the claim.
It follows from the above that
P (En; Ï†1,Ï„ (Î¸1) + Ï†2,Ï„ (Î¸2) âˆ’ Ï†1,Ï„?
(Î¸1) âˆ’ Ï†2,Ï„?
(Î¸2) â‰¤ 0)
â‰¤ P
ï£«
ï£­





1
Ï„ âˆ’ Ï„?
XÏ„
t=Ï„?+1

X(t)X(t)
0
âˆ’ Î¸
âˆ’1
?,2






âˆ
>
kÎ¸2 âˆ’ Î¸1k
2
F
4B2kÎ¸2 âˆ’ Î¸1k1
ï£¶
ï£¸ . (23)
Proceeding as above, it is easy to see that if  â‰¤ r1,Ï„ âˆ§ r2,Ï„ , and rj,Ï„ â‰¤
kÎ¸?,2âˆ’Î¸?,1kF
2(1+8s
1/2)
, then
kÎ¸2 âˆ’ Î¸1k
2
F
4B2kÎ¸2 âˆ’ Î¸1k1
â‰¥
kÎ¸?,2 âˆ’ Î¸?,1k
2
F
32B2kÎ¸?,2 âˆ’ Î¸?,1k1
.
Using this, and by Lemma 15, it follows that the probability on the right-hand side of (23)
is upper-bounded by
4p
2
exp 
âˆ’(Ï„ âˆ’ Ï„?) min 
kÎ¸?,2 âˆ’ Î¸?,1k
4
F
128B4kÎ¸?,2 âˆ’ Î¸?,1k
2
1
,
Îº
ÎºÂ¯
4
 .
We apply this to (21) to get:
P(Ë‡Ï„ > Ï„? + Î´) â‰¤ P(E
c
n
) +X
jâ‰¥0
4p
2
e
âˆ’C0(Î´+j) â‰¤
8
pT
+
4
p
2(1 âˆ’ eâˆ’C0 )
,
where C0 = min h
kÎ¸?,2âˆ’Î¸?,1k
4
F
128B4kÎ¸?,2âˆ’Î¸?,1k
2
1
,
 Îº
ÎºÂ¯
4
i
, and by taking Î´ = 4 log(p)/C0. This completes
the proof.