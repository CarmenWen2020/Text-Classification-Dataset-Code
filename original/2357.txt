In solving hard computational problems, semidefinite program (SDP) relaxations often play an
important role because they come with a guarantee of optimality. Here, we focus on a popular
semidefinite relaxation of K-means clustering which yields the same solution as the non-convex
original formulation for well segregated datasets. We report an unexpected finding: when data
contains (greater than zero-dimensional) manifolds, the SDP solution captures such geometrical
structures. Unlike traditional manifold embedding techniques, our approach does not rely on manually defining a kernel but rather enforces locality via a nonnegativity constraint. We thus call our
approach NOnnegative MAnifold Disentangling, or NOMAD. To build an intuitive understanding
of its manifold learning capabilities, we develop a theoretical analysis of NOMAD on idealized
datasets. While NOMAD is convex and the globally optimal solution can be found by generic SDP
solvers with polynomial time complexity, they are too slow for modern datasets. To address this
problem, we analyze a non-convex heuristic and present a new, convex and yet efficient, algorithm,
based on the conditional gradient method. Our results render NOMAD a versatile, understandable,
and powerful tool for manifold learning.
Keywords: K-means, semidefinite programming, manifolds, conditional gradient method
1. Introduction
In the quest for an algorithmic theory of biological neural networks, some of the authors have
recently proposed a soft K-means clustering network that may model insect olfactory processing
and other computations (Pehlevan et al., 2017). This network was derived by performing online
optimization on the non-convex K-means objective function. Whereas the network dynamics and
learning rules are biologically plausible, the nonconvexity of the objective makes it difficult to
analyze the solutions and algorithm convergence.
Here, to understand the solutions computed by the clustering neural network, we consider a
convex SDP relaxation of K-means (Kulis et al., 2007; Peng and Wei, 2007; Awasthi et al., 2015).
Given data points {xi}
n
i=1 we define the Gramian matrix, D, such that (D)ij = xi
>xj . Then, we
TEPPER, SENGUPTA, CHKLOVSKII
2
0
2
2
0
2
2
0
2
Input dataset
Input Gramian D
Q (K = 16) 2D embedding
(a)
Input dataset Input Gramian D
Zoom
Q (K = 64)
(enhanced contrast)
Input dataset
(b)
Figure 1: NOMAD, originally introduced as a convex relaxation of K-means clustering, surprisingly learns manifold structures in the data. (a) Learning the manifold of a trefoil knot, which cannot
be “untied” in 3D without cutting it. NOMAD understands that this is a closed manifold, yielding a
circulant matrix Q, which can be “unfolded” in 2D. (b) Learning multiple manifolds with NOMAD.
Although they are linearly non-separable, NOMAD correctly finds two submatrices, one for each
manifold (for visual clarity, we enhance the contrast of Q).
search for a cluster co-association matrix Q, such that (Q)ij = 1 if points i and j belong to the
same cluster and (Q)ij = 0 if they do not. The optimum Q∗ can be found by solving the following
optimization problem (the acronym will be explained below):
Q∗ = argmax
Q∈Rn×n
Tr (DQ) s.t. Q1 = 1, Tr (Q) = K, Q  0, Q ≥ 0. (NOMAD)
Its link with the original K-means clustering formulation is explained in Appendix A.1
First, we focus on the question: what does NOMAD compute? Until now, theoretical efforts
have concentrated on showing that NOMAD is a good surrogate for K-means. Awasthi et al. (2015)
study its solutions on datasets consisting of linearly separable clusters and demonstrate that they
reproduce hard-clustering assignments of K-means. Moreover, the solution to NOMAD achieves
hard clustering even for some datasets on which Lloyd’s algorithm (Lloyd, 1982) fails (i.e., Iguchi
et al. (2015); Mixon et al. (2016)). Related problems have been studied by Amini and Levina (2014);
Javanmard et al. (2015); Yu et al. (2012).
In this work, we analyze NOMAD in a different regime than previous studies. Instead of focusing on cases and parameter settings where it approximates the original K-means formulation,
we concentrate on alternative settings and discover that NOMAD is not merely a convex K-means
imitator. NOMAD finds the manifold structure in the data, even discriminating different manifolds.
Fig. 1 shows two examples of this unexpected behavior where NOMAD dissects the geometry of
1. Notation. (X)ij , (X):j , (X)i: denote the (i, j)-th entry of matrix X, the j-th column of X, and the i-th row of
X, respectively. For vectors, we employ lowercase and we use a similar notation but with a single index. We write
X ≥ 0 if a matrix X is entry-wise nonnegative and X  0 if it is positive semidefinite.
2
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
the data. Because of this and of the central role played by the nonnegativity constraint in the SDP
we call it a NOnnegative MAnifold Disentangling (NOMAD).
The next question is: how can we compute these solutions? Despite the theoretical advantages
of convex optimization, in practice, the use of SDPs for clustering has remained limited. This is
mainly due to the lack of efficient algorithms to solve the convex optimization problem. We address
this issue by presenting an efficient convex solver for NOMAD, based on the conditional gradient
method. The new algorithm can handle large datasets, extending the applicability of NOMAD to
more interesting and challenging scenarios.
Organization. We first study the behavior of NOMAD theoretically by analyzing its solution
for a simple synthetic example of a regular manifold with symmetry (Sec. 2). In this context, we
demonstrate how NOMAD departs from standard K-means. Building on this analysis, we suggest
that NOMAD has non-trivial manifold learning capabilities (Sec. 3) and demonstrate numerically
NOMAD’s good performance in non-trivial examples, including synthetic and real datasets. Then,
motivated by the relatively slow performance of standard SDP solvers, we focus on scaling NOMAD
to large modern datasets. In Sec. 4, we study both theoretically and experimentally an heuristic nonconvex Burer-Monteiro-style algorithm (Kulis et al., 2007). Finally, we present a new convex and
yet efficient algorithm for NOMAD. This algorithm allows us, for the first time, to study provable
solutions of NOMAD on large datasets. Our software is publicly available at https://github.
com/simonsfoundation/sdp_kmeans.
2. Theoretical analysis of manifold learning capabilities of NOMAD
Starting with the appearance of Isomap (Tenenbaum et al., 2000) and locally-linear embedding
(LLE) (Roweis and Saul, 2000), there has been outstanding progress in the area of manifold learning
(e.g., Belkin and Niyogi, 2003; Hadsell et al., 2006; Weinberger and Saul, 2006; Weiss et al., 2008).
For a data matrix X = [xi
]
n
i=1 of column-vectors/points xi ∈ R
d
, the majority of these modern
methods have three steps:
1. Determine the neighbors of each point. This can be done in two ways: (1) keep all point
within some fixed radius ρ or (2) compute κ nearest neighbors.
2. Construct a weighting matrix W, where (W)ij = 0 if points i and j are not neighbors, and
(W)ij is inversely proportional to the distance between points i and j otherwise.
3. Compute an embedding from W that is locally isometric to X.
For the third step, many different and powerful approaches have been proposed, from computing
shortest paths on a graph (Tenenbaum et al., 2000), to using graph spectral methods (Belkin and
Niyogi, 2003), to using neural networks (Hadsell et al., 2006).
However, the success of these techniques depends critically on the ability to capture the data
structure in the first two steps. Correctly setting either ρ or κ is a non-trivial task that is left to
the user of these techniques. Furthermore, a kernel (most commonly an RBF) is often involved in
the second step, adding an additional parameter (the kernel width/scale) to the user to determine.
Expectedly, the optimal selection of these parameters plays a critical role in the overall success of
the manifold learning process.
NOMAD departs drastically from this setup as no kernel selection nor nearest neighbor search
are involved. Yet, the solution Q∗ is effectively a kernel which is automatically learned from the
data. Because Q∗ is positive semidefinite it can be factorized as Q∗ = Y>Y, defining a feature
map from X to Y.
3
TEPPER, SENGUPTA, CHKLOVSKII
1.0 0.5 0.0 0.5 1.0
1.0
0.5
0.0
0.5
1.0
x0
Input dataset
0 3
4
3
2
2
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Squared Euclidean distances to x0
0 3
4
3
2
2
1.0
0.5
0.0
0.5
1.0
Dot products with x0
NOMAD kernel function
Figure 2: Correspondence between using a kernel/threshold in distance-space and nonnegativity of
Gramian-based representations. In this toy example, the constraint Q ≥ 0 in NOMAD is equivalent
to setting to zero distances that are greater than √
2 (squared distances greater than 2). We use x0
as a reference but rotational symmetry makes this argument valid for all points in the dataset.
To illustrate intuitively the differences and similarities with prior work on manifold learning we
use LLE (Roweis and Saul, 2000) as an example. LLE optimizes the cost function
Φ(Y) = Tr 
(I − W)
>
(I − W)

Y>Y
 , (1)
where W is the adjacency matrix of a weighted nearest-neighbors graph. The key to finding a
matrix Y that is locally isometric to X, while unwrapping the data manifold, is to remove from W
the connections between distant points (X):i and (X):j . This is done with some technique to find
nearest neighbors.
NOMAD also tries to align the output Gramian, Q, to the input Gramian, D, but discards distant
data points differently. As negative entries in D cannot be matched because Q is nonnegative, the
best option would be to set the corresponding element of Q to zero. This effectively discards pairs
of input data points whose inner product is negative thus enforcing locality in the angular space
(Cho and Saul, 2009), see Fig. 2. In fact, this argument can be taken further by noting that the
constraint Q1 = 1 allows us to replace the Gramian D with the negative squared distance matrix,
−
1
2
X
ij
kxi − xjk
2
2
(Q)ij = −
X
i
(D)iiX
j
(Q)ij + Tr (DQ) = − Tr (D) + Tr (DQ). (2)
Finally, the constraint Tr (Q) = K allows further control of the neighborhood size of NOMAD
(modulating the actual width of its kernel function, see Fig. 2). Next, we develop further intuition
about the manifold-learning capabilities of NOMAD by analyzing theoretically the dataset in Fig. 2.
As we mentioned before, the SDP formulation of Peng and Wei (2007) was developed as a
clustering algorithm. Whether this method actually delivers a clustered solution depends on the
geometry of the dataset. When the dataset consists of well-segregated clusters, the resulting Q∗
has block diagonal structure. We empirically observe that, when the dataset is sampled from a
regular manifold, the solution Q∗ does not break down the dataset into artificial clusters and actually
preserves the manifold structure (see Sec. 3). In a simple example, where the manifold exhibits a
high degree of symmetry, we demonstrate analytically that this behavior occurs. The following
sections are devoted to this task.
4
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
2.1. Analysis of NOMAD on a 2D ring dataset
We analyze the case in which the input data to NOMAD possess rotational symmetry, i.e., data are
arranged uniformly on a ring, see Fig. 2. In this case, we can write the SDP as a linear program (LP)
in the circular Fourier basis. This new representation allows to visualize that NOMAD lifts the data
into a high-dimensional space, with K controlling its dimensionality.
In the example in Fig. 2, the entries of D can be described by (D)ij = xi
>xj = cos(αi − αj ),
where αi
, αj are the angles of points xi
, xj , respectively (Fig. 2). Since the points are uniformly
distributed over the ring, D is a circulant matrix, i.e., cos(αi − αj ) = cos(αi+k − αk+k). The
solution Q∗ to NOMAD is circulant too (Bachoc et al., 2012). Being circulant matrices, D and Q∗
are diagonalized by the discrete Fourier transform (DFT), i.e.,
D = F diag(d)F
H, Q∗ = F diag(q)F
H, (3)
where q, d ≥ 0 respectively are vectors containing the eigenvalues of D and Q∗, F
H is a Hermitian
conjugate of F, and F ∈ C
n×n
is the unitary DFT matrix, with entries (p, k = 0, . . . , n − 1)
(F)pk = √
1
n
exp
−i2πp k
n

. (4)
Hence, and in accord with the constraint Q∗1 = 1, we have that (F)0: = √
1
n
1 and (q)0 = 1.
2.2. A linear program on the data manifold
We express the objective function and the constraints of NOMAD in terms of d and q, i.e.,
Tr (DQ∗) = d
>q, (5)
Tr (Q) = 1
>q = K, (6)
(Q)kk0 = (F)k: diag(q)(F
H):k
0 =
nX−1
p=0
(q)p
n
cos 
2πp k
0−k
n

≥ 0. (7)
This reformulation allows us to rewrite NOMAD as a linear program
max
q
d
>q s.t. (∀τ ) cτ
>q ≥ 0, 1
>q = K, q ≥ 0, (q)0 = 1, (8)
where (cτ )p =
1
n
cos
2πp τ
n

.
Problem (8) sheds light on the inner workings of NOMAD. First, the constraint 1
>q = K
ensures that q does not grow to infinity and acts as a budget constraint. Let us assume for a moment
that we remove the constraint cτ
>q ≥ 0 (the equivalent of Q ≥ 0). Then, the program will try to
set to K the entry of q corresponding to the largest eigenvalue of d; this q will violate as K gets
bigger the removed constraint (since (cτ )p is a sinusoid). Then the effect of this constraint is to
spread the allocated budget among several eigenvalues (instead of just the largest). The experiment
in Fig. 3 confirms this: the number of active eigenvalues of Q∗ grows with K. We can interpret this
as increasing the intrinsic dimensionality of the problem in such a way that only local interactions
are considered.
Interpretation of K. The circulant property of Q∗ for the 2D ring sheds further light on the meaning of K. In Fig. 3(c), we observe that the number of significant elements in each of Q∗ is dn/Ke.  
TEPPER, SENGUPTA, CHKLOVSKII
K = 1 K = 12
K = 25 K = 100
(a)
10 20 30 40 50 60 70 80 90 100
K
0.0
0.2
0.4
0.6
0.8
1.0
Eigenvalues
Mean
Median
(b)
10 20 30 40 50 60 70 80 90 100
K
0.0
0.2
0.4
0.6
0.8
1.0
V
alu
e
alo
n
g
h-dia
g
o
n
al
h=0
h=1
h=2
h=3
h=4
h=5
h=6
h=7
h=8
h=9
(c)
(d)
Figure 3: Evolution of the NOMAD solution for the 2D ring dataset (with 100 points, see Fig. 2)
with increasing parameter K. (a) As K increases, the solution, Q∗, concentrates more and more
towards the diagonal. (b) As K increases, the number of active eigenvalues in the solution, Q∗,
grows resulting in the more uniform distribution of eigenvalues and greater mean/median (notice
that the mean being linear comes from the trace constraint). (c) We define the h-diagonal of Q∗
as the entries (i, j) for which i − j = h. As Q∗ is a circulant matrix, each h-diagonal contains
a single repeated value. We plot these values, assigning a different color to each h. The effect of
the scaling constraint Tr (Q) = K becomes evident: when one h-diagonal becomes inactive, all
remaining h
0
-diagonals need to be upscaled. (d) The eigenvectors of Q∗ form a high-dimensional
cone (cartoon representation, cone axis in red and eigenvectors in green).
Thus, we can interpret K as a parameter that effectively sets the size of the local neighborhood on
the manifold. In standard manifold learning methods this size is set by a combination of the number
of nearest neighbors and the shape and scale of the kernel function. In NOMAD, all these variables are incorporated into a single parameter and balanced with the help of the remaining problem
constraints.
In general, for non-symmetric and irregularly sampled manifolds, K is chosen to capture the
manifold underlying the dataset: the neighborhood size needs to be small enough to capture the
desired manifold features, but big enough to avoid capturing unwanted structure (e.g., noise). If
sampling density differs in different areas, the size will adjust locally as needed.
2.3. Lifting the ring to a high-dimensional cone
Here, we show that NOMAD effectively embeds the data manifold into a space where its structure,
i.e., rotational symmetry, is preserved. We now make use of the half-wave symmetry in Q∗, noting
that they can be fully represented with only one half of the Fourier basis. We can then decompose it
with the real Fourier basis
Q∗ = F˜ diag(˜q)F˜>, (9)
where ˜q = [(q)0,(q)1,(q)1, . . . ,(q)n−1,(q)n−1]
>
and F˜ ∈ R
n×n has entries (p, k = 0, . . . , n−1)
(F˜)pk =
(
√
2
n
cos
2πp k
n

if k is even,
√
2
n
sin
2πp k−1
n

if k is odd.
(10)  
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
Let Y˜ = diag(q)
1/2F˜>. Notice that hY˜:i/



Y˜:i



F
, F˜:0i = hF˜:i
, F˜:0i =
4
n
, meaning that the
vectors Y˜:i are the extreme rays of a right circular cone with the eigenvector F˜:0 = √
2
n
[1, 0, . . . , 0]>
as its symmetry axis, see Fig. 3(d). Thus, we can interpret the solution to NOMAD as lifting the
2D ring structure into a cone. As mentioned before, this cone is high-dimensional, with as many
directions as needed to preserve the nonnegativity of Q.
We identify the rank of the solution Q with the number of active eigenvalues. The bigger the
K, the higher the rank. The constraint Q1 = 1 in NOMAD leads to a fanning-out effect in the data
representation. Intuitively, this fan-out effect is key to the disentanglement of datasets with complex
topologies. Spin-model-inspired SDPs for community detection (Javanmard et al., 2015) achieve a
similar fanning-out by dropping the constraint Q1 = 1 and adding the related term −γ1
>Q1 to
the objective function.
With the LP framework and the geometric picture in place, we can begin to understand how the
solution evolves as the parameter K increases from 1 to n. At K = 1, only the eigenvalue (q)0 is
active and every vector (Y˜ ):i
is the same with each entry equal to 1/n. When K slightly above 1,
the eigenvalue (q)1 becomes active (nonzero), introducing the first nontrivial Fourier component.
Geometrically, the vectors {(Y˜ ):i} now open up into a narrow cone. As K increases, the cone
widens and, at some point, the angle between two of the vectors reaches π/2 (this activates the
nonnegativity constraint in Eq. (7)). Further increase of K necessitates use of a larger number of
Fourier modes. Finally, at K = n all modes are active and all vectors {(Y˜ ):i} become orthogonal
to each other. Fig. 3(b) depicts the progression with K of the number of active modes.
Summary. Previous studies (Kulis et al., 2007; Peng and Wei, 2007; Awasthi et al., 2015), focus
solely on cases where NOMAD exhibits K-means-like solutions (i.e., hard-clustering). Sec. 2 provides a characterization of the NOMAD solutions on a simple example with a high degree of symmetry, showing that they are drastically different from K-means. These solutions connect neighboring points, with the neighborhood size determined by K. These neighborhoods overlap, as they
would in soft-clustering, in a way that preserves global features of the manifold, including its symmetry. This is a feature sought after by manifold learning methods and help place NOMAD among
reliable manifold analysis techniques.
3. Analyzing data manifolds with NOMAD: Experimental results
In the previous section, we showed that NOMAD recovers the data manifold in an idealized 2D
ring dataset. Here, we extend this observation numerically to more complex datasets for which
analytical form of the transformation that diagonalizes Q∗ (nor D) is not known, see figs. 1, 5
and 7. We visualize the solution Q∗ by embedding it in a low-dimensional space. While our goal is
not dimensionality reduction, we learn the data manifold with NOMAD, and use standard spectral
dimensionality reduction to visualize the results.
Recovering multiple manifolds. K-means cannot effectively recover multiple distinct manifolds
(although in some very particular cases, with well separated and linearly separable manifolds, it
may group the points correctly). Interestingly, NOMAD does not inherit this limitation. Of course,
if we set the NOMAD parameter K to the number of manifolds that we want to recover, there is
no hope in the general case to obtain a result substantially better than the one obtained with Lloyd’s
algorithm (Lloyd, 1982). However, setting the NOMAD parameter K to be higher than the number
of manifolds leads to a correct identification and characterization of their structures. Note that
7
TEPPER, SENGUPTA, CHKLOVSKII
Input dataset
(a)
Input Gramian
1
0
1
(b)
Q (K = 16)
0.1
0.0
0.1
0.1
0.0
0.1
0.1
0.0
0.1
0.1
0.0
0.1
(c)
(d)
Figure 4: Solution of NOMAD on the dataset consisting of two 2D rings. (a) Two-ring dataset. (b)
Input Gramian, D, and its two eigenvectors (D has rank 2). Note that the eigenvectors of D do not
segregate the rings. (c) The solution, Q, of NOMAD contains two sets of eigenvectors with disjoint
support: one set describing the points in each ring (we show all eigenvectors and a detail on the first
3 within each set). (d) The eigenvectors of Q form two orthogonal high-dimensional cones: one
cone for each ring (cartoon representation, cone axis in red and eigenvectors in green). Notice how
these cones become linearly-separable.
setting a similarly large K would not help K-means, as it is designed to partition the data, thus
breaking each manifold into several pieces.
An example with two rings is presented in Fig. 4. We can expect that, as the single ring in
Sec. 2 is described by Fourier modes, NOMAD describes two rings with two sets of Fourier modes
with disjoint support; the solution is now arranged as two orthogonal high-dimensional cones, see
Fig. 4(d). In a sense, the manifold learning problem is already solved, as there are two circulant
submatrices, one for each manifold, with no interactions between them. If the user desires a hard
assignment of points to manifolds, we can simply consider Q∗ as the adjacency matrix of a weighted
graph and compute its connected components.
Discussion of the experimental results. To demonstrate the manifold-learning capabilities of the
NOMAD, we present several examples, both synthetic and real-world. The trefoil knot in Fig. 1(a)
is a 1D manifold in 3D; it is the simplest example of a nontrivial knot, meaning that it is not possible
to “untie” it in three dimensions without cutting it. However, the manifold learning procedure in
Sec. 3 learns a closed 1D manifold. We also present examples using real-world high-dimensional
datasets, recovering in every case structures of interest, see Fig. 6. In figs. 6(a) to 6(c), NOMAD
respectively uncovers the camera rotation, the orientation of the lighting source, and specific handwriting features.
To demonstrate the multi-manifold learning and manifold-disentangling capabilities of NOMAD, we use several standard synthetic datasets, see figs. 1(b), 4 and 5. In all of these examples,
NOMAD is able to disentangle clusters that are not linearly separable. We also present results for
a real-world dataset (Fig. 7) which is similar to the one in Fig. 6(a) but with two objects. NOMAD
8
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
Input dataset Input Gramian D
Q (K = 16)
(enhanced contrast)
Figure 5: Learning multiple manifolds with NOMAD. The points are arranged in two semicircular manifolds and contaminated with Gaussian noise. Although the manifolds are linearly nonseparable, NOMAD correctly finds two submatrices, one for each manifold (for visual clarity, we
enhance the contrast of Q).
(a) Teapots (K = 20) (b) Yale Faces (K = 16) (c) MNIST digits (K = 16)
Figure 6: Finding two-dimensional embeddings with NOMAD. (a) 100 images obtained by viewing
a teapot from different angles in a plane. The input vectors size is 23028 (76 × 101 pixels, 3
color channels). The manifold uncovers the change in orientation. (b) 256 images from 4 different
subjects (each subject is marked with a different color in the figure), obtained by changing the
position of the illumination source. The input vectors size is 32256 (192×168 pixels). The manifold
uncovers the change in illumination (from frontal, to half-illuminated, to dark faces, and back). (c)
500 images handwritten instances of the same digit. The input vectors size is 784 (28 × 28 pixels).
On the left and on the right, images of the digits 1 and 2, respectively. The manifold of 1s uncovers
their orientation, while the manifold of 2s parameterizes features like size, slant, and line thickness.
Details are better perceived by zooming on the plots.
Input Gramian
Q (K = 35)
(enhanced contrast)
‘Horse’ manifold ‘Lamp’ manifold
Figure 7: 144 images obtained by viewing a lamp and a horse figurine from different angles in a
plane. The input vectors size is 589824 (384 × 512 pixels, 3 color channels). We plot the input data
using a 2D spectral embedding (the points corresponding to each object are colored differently).
NOMAD correctly finds two submatrices, one for each manifold (for visual clarity, we enhance the
contrast of Q); furthermore, NOMAD recovers closed manifolds.
9
TEPPER, SENGUPTA, CHKLOVSKII
Input dataset Input Gramian D Q (K = 32) 2D embedding
(a)
(b)
Figure 8: Learning a 2D manifold embedded in a 10-dimensional ambient space; the first two
dimensions are regular samples on a 2D grid (shown on the left) and the remaining ones are Gaussian
noise. (a) NOMAD recovers the right 2D structure (of course, some distortion is expected along the
edges). (b) We show a few columns of Q on top of the data itself: the red level indicates the value of
the corresponding entry in Q (red maps to high values, white maps to a zero). NOMAD effectively
tiles the dataset with a collections of overlapping local neighborhoods centered at each data point.
These patches contain all the information necessary to reconstruct the intrinsic manifold geometry.
recovers two closed manifolds, each of which containing the viewpoints of one object. The structure
of the solution is similar to the one in Fig. 4(d).
Finally, we include an example in which NOMAD captures the structure of a 2D manifold living
in a 10-dimensional space, see Fig. 8. NOMAD assigns a local patch to each data point (non-zero
values for neighboring points, zeros elsewhere). These local patches tile the manifold with overlap
(as in soft-clustering), allowing to recover its grid structure. Such tiling takes place in all of the
examples included in the paper.
3.1. Manifold disentangling with multi-layer NOMAD
The recursive application of NOMAD, with successively decreasing values of K, enhances its
manifold-disentangling capabilities. The pseudocode is as follows:
1 D1 ← X>X;
2 for l = 1, 2, . . . do
3 Choose Kl (for all l > 1 we require Kl ≤ Kl−1);
4 Find the solution Ql of NOMAD with input matrix Dl and parameter Kl
;
5 Dl+1 ← Ql
;
6 return {Ql}l=1,2,...
10
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
Input Gramian
L1
Q (K = 16)
L2
Q (K = 8)
L3
Q (K = 4)
L4
Q (K = 2)
Input Gramian
L1
Q (K = 16)
L2
Q (K = 8)
L3
Q (K = 4)
L4
Q (K = 2)
Input Gramian
L1
Q (K = 64)
L2
Q (K = 32)
L3
Q (K = 16)
L4
Q (K = 8)
L5
Q (K = 4)
L6
Q (K = 2)
Figure 9: Results of recursive NOMAD application (multi-layer NOMAD). For each example, we
show matrices Q∗ computed by the successive application of the algorithm. Multi-layer NOMAD
untangles these linearly non-separable manifolds and, in the final layer, assigns each manifold to
one cluster.
In Fig. 9, we present the evolution of successive matrices Ql
. In all of these examples, multilayer NOMAD is able to correctly identify clusters that are not linearly separable, something
unattainable with single-layer NOMAD or with K-means clustering. Interestingly, we find that
the manifolds are already segregated after one application of NOMAD in the direction of the leading eigenvectors of Q (see Fig. 4). The rest of the NOMAD layers little-by-little sieve out the
(unwanted) smaller eigenvalues in an unsupervised fashion.
To turn this algorithm into a general data-analysis tool, we need an automated selection of the
values {Kl} which is a non-trivial task in general. Additional results, using different sequences
{Kl}, can be found in Appendix C. Further research is needed to develop such algorithm and fully
understand multi-layer NOMAD’s interesting behavior.
3.2. Geodesic-distance preservation: NOMAD versus existing manifold learning techniques
In order to compare different methods for manifold discovery, we need to agree upon appropriate
metrics, which itself is an active area of research (e.g., Zhang et al., 2012). In particular, we want a
metric that allows fair comparison among outputs of methods with very different objectives. Since
our method is not explicitly geared towards dimension reduction, or towards variance maximization,
we prefer metrics that emphasize the preservation of intrinsic structure of the manifold. In particular,
we hope to preserve the ordering of intrinsic distances along the manifold, something that guarantees
that the neighborhood structure remain similar.
Concretely, (1) we compute N nearest neighbors for each dataset point and build a weighted
graph with these distances as edges; (2) we use this graph to compute geodesic distances using
11
TEPPER, SENGUPTA, CHKLOVSKII
No noise Noise 0.05 Noise 0.10
One ring (Fig. 2)
0 10 20 30 40 50
Data percentile
0.75
0.80
0.85
0.90
0.95
1.00
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
One half-ring
0 10 20 30 40 50
Data percentile
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
Teapots (Fig. 6(a))
0 10 20 30 40 50
Data percentile
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
(a) One manifold: NOMAD is among the best-in-class methods.
No noise Noise 0.05 Noise 0.10
Two rings (Fig. 4(a))
0 10 20 30 40 50
Data percentile
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
Two half-rings (Fig. 5)
0 10 20 30 40 50
Data percentile
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
0 10 20 30 40 50
Data percentile
0.4
0.5
0.6
0.7
0.8
0.9
1.0
intersection ratio
Method
NOMAD
ISOMAP
LLE
LLE-MOD
LTSA
Spectral
NO-EMB
(b) Two manifolds: NOMAD outperforms all considered methods.
Figure 10: Comparison of the robustness of geodesic distances to the addition of noise for different
manifold learning methods. The description of the experimental protocol is in Sec. 3.2. The method
termed NO-EMB directly computes distances on the noisy data.
12
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
Dijkstra’s algorithm; (3) we finally sort the distances in increasing order. We consider this ordering
our ground truth.
We then add noise to the each point in the dataset. Noise was added by creating 5d additional
dimensions that contain Gaussian noise with standard deviation 0.05 and 0.10. Using the noisy
dataset, we run the geodesic-distance-sorting process on the embeddings produced by different
manifold learning algorithms using N nearest neighbors. For NOMAD, instead of resorting to
nearest neighbors, we set K = n/N and use the non-zero entries in Q to determine the graph
connectivity. As NOMAD yields a similarity matrix Q, we derive distances from it with the formula
(Q)ii + (Q)jj − 2(Q)ij . Using this weighted graph, we compute and sort the geodesic distances.
For our distance-preservation measure, we use a bullseye score: for each method, we count the
fraction of points in the top p percentile of distances that are also present in the top p percentile of
ground truth distances.
As seen in Fig. 10, when the data is sampled from a single manifold, NOMAD performs very
well, on par with the best algorithms included in our comparison. However, in the two-manifolds
case, NOMAD clearly outperforms all other methods, nearly matching the performance of direct
distance computations on the noisy data.
4. Heuristic non-convex solvers for large-scale NOMAD
Standard SDPs involve O(n
2
) variables and their resulting time complexity is often O(n
3
). Consequently, standard solvers (O’Donoghue et al., 2016a) will struggle with large datasets. NOMAD
lends itself to a fast and big-data-friendly implementation (Kulis et al., 2007). This is done by
posing a related problem
max
Y∈Rr×n
Tr 
DY>Y

s.t. Tr 
Y>Y

= K, Y>Y1 = 1, Y ≥ 0. (11)
In this new problem, we have forgone convexity in exchange of reducing the number of unknowns
from O(n
2
) to rn. For example, Kulis et al. (2007) set r = K. The problematic constraint Y>Y ≥
0, involving O(n
2
) terms, has been replaced by the much stronger but easier to enforce Y ≥ 0. The
speed gain is shown in Fig. 15. See Appendix E for a description of the algorithm.
However, strictly speaking, the new constraint is equivalent to the old one only if Q is completely positive. An n × n matrix A is called completely positive (CP) if there exists B ≥ 0 such
that A = B>B. The least possible number of rows of B is called the cp-rank of A. Whereas matrix
A is doubly nonnegative (DN), i.e. A ≥ 0 and A  0, not every DN matrix (with n > 4) is CP
(Maxfield and Minc, 1962).
We are thus interested in two questions. First, is the solution Q∗ to NOMAD completely positive? Answering this question in the affirmative would allow for theoretically sound and fast implementations of NOMAD. Whereas the set of CP matrices forms a convex cone, the problems of
determining whether a matrix is inside the set and of projecting a matrix into the set are NP-hard
leading us to the second question: What is the cp-rank of Q∗? This issue is critical because it
determines the number of unknowns. For example, if cp-rank(Q∗) ≤ K, (11) would be easier to
solve. These questions are difficult only when NOMAD produces a soft-clustering Q∗, as in all
of the examples in this paper. Indeed, it is not hard to prove that, whenever NOMAD produces a
hard-clustering Q∗, Q∗ is CP (see Awasthi et al., 2015, for such conditions).
Let us now go back to the example in Sec. 2 (points arranged regularly on a ring). For this
example, we can establish a simple sufficient condition on K, for Q∗ to be CP. Recall that if D is
13
TEPPER, SENGUPTA, CHKLOVSKII
K K
Figure 11: We empirically study the cp-rank of Q∗. As a proxy of the exact nonnegative decomposition, we compute the rank-r symmetric NMF Q∗ ≈ Y+
>Y+ for different values of r. We show
the mean plus/minus two standard deviations of the relative error

Q∗ − Y+
>Y+


F
/ kQ∗kF
computed from 50 different SNMFs for each r (their differences stem from the random initialization).
Both datasets have 200 points. Clearly, setting r = K is not enough to properly reconstruct Q∗.
circulant, Q∗ is circulant (Bachoc et al., 2012). In Proposition 2 of Appendix B, we prove that if the
solution Q∗ to NOMAD is a circulant matrix, then it is CP for every K ≤ 3/2 or K ≥
n
2
. Naturally,
more theory is needed to shed light onto this problem in general scenarios as it is unclear whether
similar results exist.
Complementarily, we have studied the questions raised in this section from an experimental
viewpoint. We use the symmetric nonnegative matrix factorization (SNMF) of Q∗, see Appendix D,
as a proxy for checking whether Q∗ is CP. The rationale is that if the approximation with SNMF is
very tight, it is highly likely that Q∗ is CP. These experiments are presented in Fig. 11. We found
that, with a properly chosen rank r, SNMF can indeed accurately approximate Q∗. However, setting
r = K is in general not enough and leads to a poor reconstruction. These two facts support the idea
that Q∗ is CP, but has a cp-rank much higher than K.
Our experiments with the non-convex algorithm in Appendix E lead to similar conclusions as
those with SNMF, see Fig. 12. Setting r = K, leads to a poor approximation of Q∗ and, as observed
by Kulis et al. (2007), to hard-clustering. Setting r  K leads to much improved reconstructions,
at the expense of speed.
5. A fast and convex algorithm for NOMAD
The Burer-Monteiro solver forgoes convexity in favor of speed. However, as discussed in the previous section, this conversion carries theoretical and practical difficulties that are not easily overcome.
In this section, we propose an algorithm for NOMAD that is fast and yet convex.
5.1. Augmented Lagrangian formulation
First, we redefine the variables in NOMAD by setting P = Q − En, where En =
1
n
11>. Then,
max
P
Tr (DP) s.t. P1 = 0, Tr (P) = K − 1, P  0, P + En ≥ 0. (12)
As usual in the optimization literature, we handle this constraint with an augmented Lagrangian
method. The augmented Lagrangian of Problem (12) with respect to the constraint P + En ≥ 0 is
g(P,Γ) = − Tr (DP) + Tr (Γ(P + En)) + γ
2

[P + En]−


2
F
, (13)
14
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
Q ¤
69.11%
YY
>(r = 8)
31.24%
YY
>(r = 16)
9.55%
YY
>(r = 32)
3.57%
YY
>(r = 64)
5.91%
YY
>(r = 128)
4.94%
YY
>(r = 256)
Q ¤
66.88%
YY
>(r = 20)
20.13%
YY
>(r = 40)
19.34%
YY
>(r = 60)
12.59%
YY
>(r = 80)
10.82%
YY
>(r = 100)
13.94%
YY
>(r = 120)
Figure 12: Comparison of the results obtained with a standard SDP solver (first column) and with
a low-rank non-convex approach (remaining columns, r denotes the rank of the obtained solution).
(Top) Dataset in Fig. 4; we set K = 8 in all cases. (Bottom) Dataset in Fig. 6(a); we set K = 20
in all cases. In each case, we also display the relative error between the matrix Y>Y and Q∗.
Interestingly, setting r = K produces hard clustering solutions (see the block diagonal structure of
the matrices on the second column), while increasing r produces “softer” solutions. This suggests
that the cp-rank of Q∗ is (much) greater than K.
where Γ ≥ 0 is the associated Lagrange multiplier, and [·]− = min(·, 0) is the projection operator
onto the negative orthant. We can then pose Problem (12) as
min
P
max
Γ≥0
g(P,Γ) s.t. P1 = 0, Tr (P) = K − 1, P  0. (14)
We solve it using the method of multipliers, i.e.,
Pt+1 = argmin
P
g(P,Γt) s.t. P1 = 0, Tr (P) = K − 1, P  0, (15a)
Γt+1 = [Γt + τ (Pt+1 + En)]− . (15b)
5.2. A conditional gradient method for SDPs with an orthogonality constraint
In this section, we introduce a very efficient algorithm to solve
max
Z
f(Z) s.t. Z  0, Tr (Z) = s, Zb = 0. (16)
of which Problem (15a) is an instance.
To this end we modify an algorithm to efficiently solve the SDP (Hazan, 2008)
max
Z
f(Z) s.t. Z  0, Tr (Z) = s, (17)
where function f is differentiable and concave. The iterative algorithm consists, at each iteration
t = 0 . . . , of the following steps:
1. Let vt be the largest algebraic eigenvector of ∇f(Zt).
2. Zt+1 = (1 − α)Zt + αsvtvt
> with α = 2/(t + 2).
15
TEPPER, SENGUPTA, CHKLOVSKII
Algorithm 1: Conditional gradient algorithm for SDPs with an orthogonality constraint
input : function f to minimize, scale parameter s.
output : solution Zt+1 ∈ Ps to Problem (17).
1 Initialize Z0 = 0;
2 for t = 0, . . . , ∞ do
3 Let v be the largest algebraic eigenvector of ∇f(Z) such that v
>b = 0;
4 α ← 2/(t + 2);
5 Zt+1 ← (1 − α)Zt + αsvv>;
6 if converged then break ;
This algorithm is an instance of the Frank-Wolfe/conditional-gradient algorithm (Frank and Wolfe,
1956). As such it provides a solution without performing any projections. First, Zt+1 is a nonnegative linear combination of two positive semidefinite matrices, and is thus positive semidefinite
itself. Second, the iterations maintain the invariant Tr (Zt) = s as Tr (Zt+1) = (1 − α) Tr (Zt) +
αs Tr
vtvt
>

= (1 − α) Tr (Zt) + αs.
We now show how to extend this algorithm to handle an orthogonality constraint. Let Ps be the
convex cone of positive semidefinite matrices with trace s that are orthogonal to a given vector b,
i.e.,
Ps = {Z  0, Tr (Z) = s, Zb = 0} . (18)
Notice that setting b = 1 yields the constraints of Problem (15a). We seek to solve
max
Z
f(Z) s.t. Z ∈ Ps. (19)
Fortunately, we can push the constraint Zb = 0 into the eigenvector computation. We begin by
noticing that the final solution is a weighted sum of the matrices vtvt
>. It then suffices to require
that, for every t, vtvt
>b = 0, which reduces to vt
>b = 0. This naturally yields a new iterative
method, summarized in Alg. 1. This algorithm has the same performance guarantee as Hazan’s
(2008), given by the following proposition, which we prove in Appendix F.
Proposition 1 Let X, Z ∈ Ps and Y = X + α(Z − X) and α ∈ R. The curvature constant of f is
Cf
:= sup
X,Z,α
1
α2 [f(X) − f(Y) + (Y − X) • ∇f(X)]. (20)
Let Z
? be the solution to Problem (19). The iterates Zt of Alg. 1 satisfy for all t > 1
f(Z
?
) − f(Zt) ≤
8Cf
t+2 . (21)
5.3. A conditional gradient algorithm for NOMAD
Alg. 2 summarizes the proposed method of multipliers, see iterations (15a) and (15b), to solve
Problem (12). The inner problem (15a) is solved using Alg. 1. A few remarks are in order:
• When using the method of multipliers, it is often not necessary (nor desirable) to solve the
inner problem to a high precision (Goldstein and Osher, 2009). In our implementation we set
Ninner = 10.
1 
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
Algorithm 2: Conditional gradient algorithm for NOMAD
input : matrix D, scale parameter k.
output : solution Q to NOMAD.
1 Initialize P0 = 0; Γ ← 0; γ = 1;
2 for t = 1, . . . , ∞ do
3 for tinner = 1, . . . , Ninner do
4 Let ∇g(P,Γ) = −D + Γ + γ [P + En]−;
5 Let A = (I −
1
n
11>)∇g(P,Γ)(I −
1
n
11>);
6 Let v be the smallest algebraic eigenvector of A, such that v
>1 = 0;
7 H ← (K − 1)vv>;
8 α ← 2/(t + tinner + 2);
9 P ← (1 − α)P + αH;
10 Γ ← [Γ + τ (P + En)]−;
11 if converged then break ;
12 Q ← P + En
• There is no need to need for a highly accurate eigenvector computation (Hazan, 2008). We
use the Lanczos algorithm and set its accuracy to (t + 1)−1
.
• Alg. 1 solves a maximization problem and requires the eigenvector with the largest algebraic
eigenvalue. To solve the minimization problem (15a), we simply compute the eigenvector
with the smallest algebraic eigenvalue (Jaggi, 2013).
• As b = 1, we can enforce the orthogonality constraint vt
>1 = 0 by computing the maximum
eigenvalue of A = (I −
1
n
11>)∇g(P,Γ)(I −
1
n
11>). This operation can be carried out very
efficiently.
Complexity. The complexity of Alg. 1 is similar to that of Hazan’s (2008), plus an additional
factor to compute A. From Proposition 1, Alg. 1 yields a solution with accuracy ε, i.e., f(Zt) ≥
f(Z
?
) − ε, in 4Cf
ε − 1 iterations. Computing ∇g(P,Γ), A, and Ht require n
2 operations. Let TEIG
be the number of iterations of the eigensolver, each iteration taking O(n
2
) operations. Additional
operations require O(n) time. Then, the overall complexity of Alg. 1 is
O

Cf
ε

n + n
2 + n
2TEIG

. (22)
For the Lanczos algorithm, and our accuracy setting of (t+ 1)−1
, we have TEIG = O((t+ 1) log n).
In this case, the complexity per iteration is O(n
2
log n). As a comparison, standard SDP solvers
have a complexity of O(n
3
) per iteration. These solvers also involve significant memory usage,
while our algorithm has an optimal space complexity of O(n
2
).
5.4. Experimental analysis
Throughout the iterations of Alg. 2, P ∈ Pk−1, see Eq. (18). Thus, we only need to keep track of
the constraint Q = P + En ≥ 0 and of the value of the objective Tr (DP).
We illustrate with two typical examples the empirical convergence of these values in Fig. 13.
the convergence the objective value is clearly superlinear, while we observe a linear convergence
for the nonnegativity constraint. Accelerating the latter rate is an interesting line of future research.
17  
TEPPER, SENGUPTA, CHKLOVSKII
10
0 10
1 10
2 10
3
Iterations
10 5
10 4
10 3
10 2
RMSE
10
0 10
1 10
2 10
3
Iterations
100
200
300
400
500
600
700
800
900
Objective value
10
0 10
1 10
2 10
3
Iterations
10 5
10 4
10 3
10 2
RMSE
10
0 10
1 10
2 10
3
Iterations
100
150
200
250
300
350
Objective value
Figure 13: Prototypical examples of the behavior of the proposed conditional gradient NOMAD
solver as its iterations progress. On the left plots, we show the RMSE of [Q]− = [P + En]−,
with the average computed over its non-zero entries. After about 10 iterations, the RMSE drops
linearly, as usual for the method of multipliers. On the right plots, we display the objective value
Tr (DP), which usually converges in a few hundred iterations. In each case, as a reference, we
show in orange the values returned by the standard SDP solver. The proposed algorithm enforces
the nonnegativity constraint in NOMAD less accurately (although accurate enough for practical
purposes), while exactly enforcing all the other constraints.
We can see in Fig. 13 that standard solvers enforce the nonnegativity constraint more accurately.
However, they do not exactly enforce P ∈ Pk−1. There is a trade-off between what can be enforced
up to which precision, making the solutions sometimes not exactly comparable.
We show the suitability of the proposed NOMAD solver in Fig. 14. In the vast majority of cases
the solutions are the same. While the proposed method enforces the nonnegativity constraint less
accurately than the standard solver, it enforces all the other constraints exactly. This is why in the
teapot example, bottom left of Fig. 14, the solution of the proposed method looks less jagged than
the one of the standard solver: the constraint Q1 = 1 is more accurately enforced, resulting in a
more “circulant” representation.
In Fig. 15, we present the speed comparison of computing NOMAD with three different methods: two state-of-the-art SDP solvers, SCS (O’Donoghue et al., 2016b) and SDPNAL+ (Yang et al.,
2015), the low-rank Burer-Monteiro solver (discussed in Sec. 4), and the proposed conditional gradient method. The Burer-Monteiro method is the fastest. Keep in mind that the latter does not
guarantee convergence to the global optimal solution; this is particularly true specially in its fastest
setting, i.e., by keeping r relatively small, see Sec. 4. Among solvers that solve a convex problem,
for very small problems (up to 250 points), standard SDP solvers are the fastest. For larger problems the proposed solver is significantly faster. It is important to point out that, in theory, the speed
difference grows significantly larger. This is hard to show in practice as standard solvers either run
out of memory very quickly (SCS) or are implemented to time out for big instances (SDPNAL+);
the proposed solver has a much more efficient use of memory.
We highlight the extended computational capabilities of the proposed conditional gradient method
with an example that cannot be handled by standard SDP solvers. We use as input the 9603 × 9603
Gramian formed by all (vectorized) images of the digit zero in MNIST. The proposed algorithm is
able to compute a solution to NOMAD with ease for a problem size about 100 times larger than the
upper size limit for standard solvers. In the 2D embedding of the solution (see Sec. 3 for details
about its computation), shown in Fig. 16, we can clearly see that the images are organized by their
intrinsic characteristics (elongation and rotation).
18
CLUSTERING IS SEMIDEFINITELY NOT THAT HARD
Input dataset Input Gramian D Standard: Q (K = 16) CGM: Q (K = 16)
Input dataset Input Gramian D Standard: Q (K = 16) CGM: Q (K = 16)
2
0
2
2
0
2
2
0
2
Input dataset Input Gramian D Standard: Q (K = 64) CGM: Q (K = 64)
Input dataset Input Gramian D Standard: Q (K = 16) CGM: Q (K = 16)
Input dataset Input Gramian D Standard: Q (K = 6) CGM: Q (K = 6)
Input dataset Input Gramian D Standard: Q (K = 20) CGM: Q (K = 20)
Figure 14: Comparison of the standard SDP solver with the proposed conditional gradient solver
(CGM) for NOMAD on different datasets. In most cases, the results are practically indistinguishable
while being delivered much faster.
10
2 10
3 10
4
Dataset size (n)
10
1
10
0
10
1
10
2
10
3
10
4
Time (s)
SCS
SDPNAL+
conditional gradient solver
non-convex solver (r = 64)
non-convex solver (r = 128)
Figure 15: Running time comparison (smaller is better) of different NOMAD solvers for K =
16 (SCS (O’Donoghue et al., 2016b) and SDPNAL+ (Yang et al., 2015) are written in a highly
optimized C/C++ code, while we use our non-optimized Python code for the others). The nonconvex solver is much faster than the convex ones. Unfortunately, it may yield different results, see
Fig. 12, and may not converge to the global maximum. The conditional gradient algorithm proposed
in this paper is much faster than SCS and SDPNAL+ (about three times faster for n = 103
) but
guarantees converging to the global optimum. Additionally, the proposed algorithm handles large
problems seamlessly: in our desktop with 128GB of RAM, SCS (running under CVXPY) runs out
of memory with instances larger than n = 1200) while SDPNAL+ times out before converging for
instances larger than n = 4000.
19
TEPPER, SENGUPTA, CHKLOVSKII
Figure 16: We show the 2D embedding of the digit 0 in MNIST, computed in the same fashion as
in Fig. 6. In this case, we use all 9603 images of the digit and obtain a 9603 × 9603 matrix. We
compute the solution of NOMAD with K = 128 using the proposed conditional gradient method
(Alg. 2). In contrast, traditional SDP-solvers can only handle dense matrices approximately 100
times smaller. As in Fig. 6, the data gets organized according to different visual characteristics of
the hand-written digit (e.g., orientation and elongation).
6. Conclusions
In this work, we showed that NOMAD can learn multiple low-dimensional data manifolds in highdimensional spaces. An SDP instance, it is convex and can be solved in polynomial-time. Unlike
most manifold learning algorithms, the user does not need to select/use a kernel and no nearestneighbors searches are involved.
We also studied the computational performance of NOMAD. We first focused on a non-convex
Burer-Monteiro-style algorithm and performed both theoretical and empirical analysis. Finally, we
presented a new algorithm for NOMAD based on the conditional gradient method. The proposed
algorithm is convex and yet efficient. This algorithm allows us, for the first time, to analyze the
behavior of NOMAD on large datasets.
Related and future work. It has not escaped our attention that NOMAD can be considered as
an instance of kernel alignment (Cristianini et al., 2002). In supervised setting, kernel alignment
has been previously formulated as an SDP (e.g., Lanckriet et al., 2004; Cortes et al., 2012). Even
beyond the distinction between the supervised and unsupervised scenarios, this body of work differs significantly from NOMAD. Its goal is to optimally combine pre-computed kernel matrices,
whereas NOMAD learns such a matrix from scratch. Nonetheless, we find this connection with
kernel learning very promising and plan to investigate it further in the future.
