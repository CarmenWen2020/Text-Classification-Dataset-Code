Robust PCA is a widely used statistical procedure to recover an underlying low-rank matrix
with grossly corrupted observations. This work considers the problem of robust PCA as a
nonconvex optimization problem on the manifold of low-rank matrices and proposes two
algorithms based on manifold optimization. It is shown that, with a properly designed
initialization, the proposed algorithms are guaranteed to converge to the underlying lowrank matrix linearly. Compared with a previous work based on the factorization of low-rank
matrices Yi et al. (2016), the proposed algorithms reduce the dependence on the condition
number of the underlying low-rank matrix theoretically. Simulations and real data examples
confirm the competitive performance of our method.
Keywords: principal component analysis, low-rank modeling, manifold of low-rank matrices.
1. Introduction
In many problems, the underlying data matrix is assumed to be approximately low-rank.
Examples include problems in computer vision Epstein et al. (1995); Ho et al. (2003),
machine learning Deerwester et al. (1990), and bioinformatics Price et al. (2006). For
such problems, principal component analysis (PCA) is a standard statistical procedure to
recover the underlying low-rank matrix. However, PCA is highly sensitive to outliers in
the data, and robust PCA Cand`es et al. (2011); Chandrasekaran et al. (2011); Clarkson
and Woodruff (2013); Frieze et al. (2004); Bhojanapalli et al. (2015); Yi et al. (2016);
Chen and Wainwright (2015); Gu et al. (2016); Cherapanamjeri et al. (2016); Netrapalli
et al. (2014) is hence proposed as a modification to handle grossly corrupted observations.
Mathematically, the robust PCA problem is formulated as follows: given a data matrix
Y âˆˆ R
n1Ã—n2 that can be written as the sum of a low-rank matrix L
âˆ—
(signal) and a sparse
matrix S
âˆ—
(corruption) with only a few nonzero entries, can we recover both components
accurately? Robust PCA has been shown to have applications in many real-life applications

Zhang and Yang
including background detection Li et al. (2004), face recognition Basri and Jacobs (2003),
ranking, and collaborative filtering Cand`es et al. (2011).
Since the set of all low-rank matrices is nonconvex, it is generally difficult to obtain
an algorithm with theoretical guarantee since there is no tractable optimization algorithm
for the nonconvex problem. Here we review a few carefully designed algorithms such that
the theoretical guarantee on the recovery of underlying low-rank matrix exists. The works
Cand`es et al. (2011); Chandrasekaran et al. (2011) consider the convex relaxation of the
original problem instead:
min
L,S
kLkâˆ— + kSk1, s.t. Y = L + S, (1)
where kLkâˆ— represents the nuclear norm (i.e., Schatten 1-norm) of L, defined by the sum
of its singular values and kSk1 represents the sum of the absolute values of all entries of
S. Since this problem is convex, the solution to (1) can be solved in polynomial time. In
addition, it is shown that the solution recovers the correct low-rank matrix when S
âˆ— has
at most Î³
âˆ— = O(1/Âµ2
r) fraction of corrupted non-zero entries, where r is the rank of L
âˆ—
and Âµ is the incoherence level of L
âˆ— Hsu et al. (2011). If the sparsity of S
âˆ—
is assumed
to be random, then Cand`es et al. (2011) shows that the algorithm succeeds with high
probability, even when the percentage of corruption can be in the order of O(1) while the
rank r = O(min(n1, n2)/Âµ log2 max(n1, n2)), where Âµ is a coherence parameter of the lowrank matrix L
âˆ—
(this work defines Âµ slightly differently compared to Cand`es et al. (2011)
and (16) in this work, but the value is comparable).
However, the aforementioned algorithms based on convex relaxation have a computational complexity of O(n1n2 min(n1, n2)) per iteration, which could be prohibitive when n1
and n2 are very large. Alternatively, some faster algorithms are proposed based on nonconvex optimization. In particular, the work by Kyrillidis and Cevher (2012) proposes a
method based on the projected gradient method. However, it assumes that the sparsity
pattern of S
âˆ—
is random, and the algorithm still has the same computational complexity
as the convex methods. Netrapalli et al. (2014) proposes a method based on the alternating projecting, which allows Î³
âˆ— â‰¤
1
Âµ2r
, with a computational complexity of O(r
2n1n2)
per iteration. Chen and Wainwright (2015) assumes that L
âˆ—
is positive semidefinite and
applies the gradient descent method on the Cholesky decomposition factor of L
âˆ—
, but the
positive semidefinite assumption is not satisfied in many applications. Gu et al. (2016)
factorizes L
âˆ—
into the product of two matrices and performs alternating minimization over
both matrices. It shows that the algorithm allows Î³
âˆ— = O(1/Âµ2/3
r
2/3 min(n1, n2)) and has
the complexity of O(r
2n1n2) per iteration. Yi et al. (2016) applies a similar factorization
and applies an alternating gradient descent algorithm with a complexity of O(rn1n2) per
iteration and allows Î³
âˆ— = O(1/Îº2Âµr3/2
), where Îº is the condition number of the underlying
low-rank matrix. There is another line of works that further reduces the complexity of the
algorithm by subsampling the entries of the observation matrix Y, including Mackey et al.
(2011); Li and Haupt (2015); Rahmani and Atia (2017); Cherapanamjeri et al. (2016) and
(Yi et al., 2016, Algorithm 2), which will also be discussed in this paper as the partially
observed case.
The common idea shared by Gu et al. (2016) and Yi et al. (2016) is as follows. Since
any low-rank matrix L âˆˆ R
n1Ã—n2 with rank r can be written as the product of two low-rank
2
Robust PCA by Manifold Optimization
matrices by L = UVT with U âˆˆ R
n1Ã—r and V âˆˆ R
n2Ã—r
, we can optimize the pair (U, V)
instead of L, and a smaller computational cost is expected since (U, V) has (n1 + n2)r
parameters, which is smaller than n1n2, the number of parameters in L. In fact, such a
re-parametrization technique has a long history Ruhe (1974), and has been popularized by
Burer and Monteiro Burer and Monteiro (2003, 2005) for solving semi-definite programs
(SDPs). The same idea has been used in other low-rank matrix estimation problems such
as dictionary learning Sun et al. (2017), phase synchronization Boumal (2016), community
detection Bandeira et al. (2016), matrix completion Jain et al. (2013), recovering matrix
from linear measurements Tu et al. (2016), and even general problems Chen and Wainwright
(2015); Wang et al. (2017); Park et al. (2016); Wang et al. (2017); Park et al. (2017). In
addition, the property of associated stochastic gradient descent algorithm is studied in De Sa
et al. (2015).
The main contribution of this work is a novel robust PCA algorithm based on the gradient descent algorithm on the manifold of low-rank matrices, with a theoretical guarantee on
the exact recovery of the underlying low-rank matrix. Compared with Yi et al. (2016), the
proposed algorithm utilizes the tool of manifold optimization, which leads to a simpler and
more naturally structured algorithm with a stronger theoretical guarantee. In particular,
with a proper initialization, our method can still succeed with Î³
âˆ— = O(1/ÎºÂµr3/2
), which
means that it can tolerate more corruption than Yi et al. (2016) by a factor of Îº. In addition, the theoretical convergence rate is also faster than Yi et al. (2016) by a factor of
Îº. Simulations also verified the advantage of the proposed algorithm over Yi et al. (2016).
We remark that while manifold optimization has been applied to robust PCA in Cambier
and Absil (2016), our work studies a different algorithm and gives theoretical guarantees.
Considering the popularity of the methods based on the factorization of low-rank matrices,
it is expected that manifold optimization could be applied to other low-rank matrix estimation problems. In addition, we implement our method in an efficient and user-friendly
R package morpca, which is available at https://github.com/emeryyi/morpca.
The paper is organized as follows. We first present the algorithm in Section 2, and
explain how the proposed algorithms are derived in Section 3. Their theoretical properties
are studied and compared with previous algorithms in Section 4. In Section 5, simulations
and real data analysis on the Shoppingmall dataset show that the proposed algorithms
are competitive in many scenarios and have superior performances to the algorithm based
on matrix factorization. A discussion about the proposed algorithms is then presented in
Section 6, followed by the proofs of the results in Appendix.
2. Algorithm
In this work, we consider the robust PCA problem in two settings: fully observed setting and
partially observed setting. The problem under the fully observed setting can be formulated
as follows: given Y = L
âˆ—+S
âˆ—
, where L
âˆ—
is a low-rank matrix and S
âˆ—
is a sparse matrix, then
can we recover L
âˆ—
from Y? To recover L
âˆ—
, we solve the following optimization problem:
Lb = arg min
rank(L)=r
f(L), where f(L) = 1
2
kF(L âˆ’ Y)k
2
F
, (2)
3
Zhang and Yang
where F : R
n1Ã—n2 â†’ R
n1Ã—n2
is a hard thresholding procedure defined in (3):
Fij (A) = (
0, if |Aij | > |Ai,Â·
|
[Î³] and |Aij | > |AÂ·,j |
[Î³]
Aij , otherwise.
(3)
Here Ai,Â· represents the i-th row of the matrix A, and AÂ·,j represents the j-th column of A.
|Ai,Â·
|
[Î³] and |AÂ·,j |
[Î³]
represent the (1 âˆ’ Î³)-th percentile of the absolute values of the entries
of Ai,Â· and AÂ·,j for Î³ âˆˆ [0, 1). In other words, what are removed are the entries that are
simultaneously among the largest Î³-fraction in the corresponding row and column of A in
terms of the absolute values. The threshold Î³ is set by users. If some entries of Ai,Â· or AÂ·,j
have the entries with identical absolute values, the ties can be broken down arbitrarily.
The motivation is that, if S
âˆ—
is sparse in the sense that the percentage of nonzero
entries in each row and each column is smaller than Î³, then F(L
âˆ— âˆ’ Y) = F(âˆ’S
âˆ—
) is zero
by definition thus f(L
âˆ—
) is zero. Since f is nonnegative, L
âˆ—
is the solution to (2). To solve
(2), we propose Algorithm 1 based on manifold optimization, with its derivation deferred
to Section 3.3.1.
Algorithm 1 Gradient descent on the manifold under the fully observed setting.
Input: Observation Y âˆˆ R
n1Ã—n2
; Rank r; Thresholding value Î³; Step size Î·.
Initialization: Set k = 0; Initialize L
(0) using the rank-r approximation to F(Y).
Loop: Iterate Steps 1â€“4 until convergence:
1: Let L
(k) = U(k)Î£(k)V(k) T
.
2: Let D(k) = F(L
(k) âˆ’ Y).
3(a): (Option 1) Let â„¦(k) = U(k)U(k) T D(k) + D(k)V(k)V(k) T âˆ’ U(k)U(k) T D(k)V(k)V(k) T
,
and let U(k+1) âˆˆ R
n1Ã—r
, Î£(k+1) âˆˆ R
rÃ—r
, and V(k+1) âˆˆ R
n2Ã—r be matrices consist of the top
r left singular vectors/singular values/right singular vectors of L
(k) âˆ’ Î·â„¦(k)
.
3(b): (Option 2) Let Q1, R1 be the QR decomposition of (L
(k) âˆ’ Î·D(k)
)
T U(k) and Q2, R2
be the QR decomposition of (L
(k) âˆ’ Î·D(k)
)V(k)
. Then U(k+1) = Q2, V(k+1) = Q1 and
Î£(k+1) = R2[U(k) T
(L
(k) âˆ’ Î·D(k)
)V(k)
]
âˆ’1RT
1
.
4: k := k + 1.
Output: Estimation of the low-rank matrix L
âˆ—
, given by limkâ†’âˆž L
(k)
.
Under the partially observed setting, in addition to gross corruption S
âˆ—
, the observed
matrix Y has a large number of missing values, i.e., many entries of Y are not observed.
We denote the set of all observed entries by Î¦ = {(i, j)|Yij is observed}, and define FËœ :
R
n1Ã—n2 â†’ R
n1Ã—n2
FËœ
ij (A) = (
0, if |Aij | > |Ai,Â·
|
[Î³,Î¦] and |Aij | > |AÂ·,j |
[Î³,Î¦]
Aij , otherwise.
(4)
Here |Ai,Â·
|
[Î³,Î¦] and |AÂ·,j |
[Î³,Î¦]
represent the (1 âˆ’ Î³)-th percentile of the absolute values of
the observed entries of Ai,Â· and AÂ·,j of the matrix A respectively.
As a generalization of Algorithm 1, we propose to solve
arg min
rank(L)=r
Ëœf(L),
Ëœf(L) = 1
2
X
(i,j)âˆˆÎ¦
FËœ
ij (L âˆ’ Y)
2
, (5)
4
Robust PCA by Manifold Optimization
Algorithm 2 Gradient descent on the manifold under the partially observed setting.
Input: Observation Y âˆˆ R
n1Ã—n2
; Set of all observed entries by Î¦; Rank r; Thresholding
value Î³; Step size Î·.
Initialization: Set k = 0; Initialize L
(0) using the rank-r approximation to FËœ(Y).
Loop: Iterate Steps 1â€“4 until convergence:
1: Let L
(k) be a sparse matrix with support Î¦, with nonzero entries given by the corresponding entries of U(k)Î£(k)V(k) T
.
2: Let D(k) = FËœ(L
(k) âˆ’ Y).
3(a): (Option 1) Let â„¦(k) = U(k)U(k) T D(k) + D(k)V(k)V(k) T âˆ’ U(k)U(k) T D(k)V(k)V(k) T
,
and let U(k+1) âˆˆ R
n1Ã—r
, Î£(k+1) âˆˆ R
rÃ—r
, and V(k+1) âˆˆ R
n2Ã—r be matrices consists of the top
r left singular vectors/singular values/right singular vectors of L
(k) âˆ’ Î·â„¦(k)
.
3(b): (Option 2) Let Q1, R1 be the QR decomposition of (L
(k) âˆ’ Î·D(k)
)
T U(k) and Q2, R2
be the QR decomposition of (L
(k) âˆ’ Î·D(k)
)V(k)
. Then U(k+1) = Q2, V(k+1) = Q1 and
Î£(k+1) = R2[U(k) T
(L
(k) âˆ’ Î·D(k)
)V(k)
]
âˆ’1RT
1
.
4: k := k + 1.
Output: Estimation of the low-rank matrix L
âˆ—
, given by limkâ†’âˆž L
(k)
.
which is similar to (2) but only the observed entries are considered. The implementation is
presented in Algorithm 2 and its derivation is deferred to Section 3.3.2.
For Algorithm 1, its memory usage is O(n1n2) due to the storage of Y. For Algorithm 2,
storing Y and L
(k)
requires O(|Î¦|) and storing U(k) and V(k)
requires O(r(n1+n2)). Adding
them together, the memory usage is O(|Î¦| + r(n1 + n2)).
For both Algorithm 1 and Algorithm 2 with Option 1, the singular value decomposition
is the most computationally intensive step and as a result, the complexity per iteration is
O(rn1n2). For Algorithm 1 and Algorithm 2 with Option 2, their computational complexities per iteration are in the order of O(rn1n2) and O(r
2
(n1 + n2) + r|Î¦|) respectively.
3. Derivation of the Proposed Algorithms
This section gives the derivations of Algorithms 1 and 2. Since they are derived from
manifold optimization, we first give a review of manifold optimization in Section 3.1 and
the geometry of the manifold of low-rank matrices in Section 3.2.
3.1. Manifold optimization
The purpose of this section is to review the framework of the gradient descent method on
manifolds. It summarizes mostly the framework used in Vandereycken (2013); Shalit et al.
(2012); Absil et al. (2009), and we refer readers to these work for more details.
Given a smooth manifold M âŠ‚ R
n and a differentiable function f : M â†’ R, the
procedure of the gradient descent algorithm for solving minxâˆˆM f(x) is as follows:
Step 1. Consider f(x) as a differentiable function from R
n
to R and calculate the Euclidean
gradient âˆ‡f(x).
5
Zhang and Yang
Figure 1: The visualization of gradient descent algorithms on the manifold M. The black
solid line is the Euclidean gradient. The blue solid line is the projection of the
Euclidean gradient to the tangent space. The red solid line represents the orthographic retraction, while the red dashed line represents the projective retraction.
Step 2. Calculate its Riemannian gradient, which is the direction of steepest ascent of f(x)
among all directions in the tangent space TxM. This direction is given by PTxMâˆ‡f(x),
where PTxM is the projection operator to the tangent space TxM.
Step 3. Define a retraction Rx that maps the tangent space back to the manifold, i.e.
Rx : TxM â†’ M, where Rx needs to satisfy the conditions in (Vandereycken, 2013,
Definition 2.2). In particular, Rx(0) = x, Rx(y) = x + y + O(kyk
2
) as y â†’ 0, and Rx
needs to be smooth. Then the update of the gradient descent algorithm x
+ is defined
by
x
+ = Rx(âˆ’Î·PTxMâˆ‡f(x)), (6)
where Î· is the step size.
We remark that in differential geometry, the standard â€œretractionâ€ is the exponential map
from the tangent space to the manifold. However, in this work (as well as many works on
manifold optimization) it is used to represent a generic mapping from the tangent plane
to the manifold. As a result, the definition of retraction is not unique in this work. In
Figure 1, we visualize the gradient descent method on the manifold M with two different
kinds of retractions (orthographic and projective). We will discuss the details of those two
retractions in Section 3.2.
6
Robust PCA by Manifold Optimization
3.2. The geometry of the manifold of low-rank matrices
To apply the gradient descent algorithm in Section 3.1 to the manifold of the low-rank
matrices, the projection PTxM and the retraction Rx need to be defined. In this section, we
let M be the manifold of all R
n1Ã—n2 matrices with rank r and X âˆˆ M be a matrix of rank
r and will find the explicit expressions of PTxM and Rx.
The tangent space TXM and the retraction RX of the manifold of the low-rank matrices
have been well-studied Absil and Oseledets (2015): Assume that the SVD decomposition
of X is X = UÎ£VT
, then the tangent space TXM can be defined by TXM = {AVVT +
UUT B : for A âˆˆ R
n1Ã—n1
, B âˆˆ R
n2Ã—n2 } according to Absil and Oseledets (2015). The
explicit formula for the projection PTXM is given in (Absil and Oseledets, 2015, (9)):
PTXM(D) = UUT D + DVVT âˆ’ UUT DVVT
, D âˆˆ R
n1Ã—n2
. (7)
For completeness, a proof of (7) is presented in Appendix.
There are various ways of defining retractions for the manifold of low-rank matrices,
and we refer the reader to Absil and Oseledets (2015) for more details. In this work, we
consider two types of retractions. One is called the projective retraction Shalit et al. (2012);
Vandereycken (2013), Given any Î´ âˆˆ TXM, the retraction is defined as the nearest low-rank
matrix to X + Î´ in terms of Frobenius norm:
R
(1)
X (Î´) = arg min
ZâˆˆM
kX + Î´ âˆ’ ZkF . (8)
The solution is the rank-r approximation of X + Î´ (for any matrix W, its rank-r approximation is given by Pr
i=1 Ïƒiuiv
T
i
, where Ïƒi
, ui
, vi are the ordered singular values and vectors
of W).
In order to further improve computation efficiency, we also consider the orthographic
retraction Absil and Oseledets (2015). Denoted by R
(2)
X (Î´), it is the nearest rank-r matrix
to X + Î´ that their difference is orthogonal to the tangent space TXM:
R
(2)
X (Î´) = arg min
ZâˆˆM
kX + Î´ âˆ’ ZkF , s.t. hR
(2)
X (Î´) âˆ’ (X + Î´), ZiF = 0 for all Z âˆˆ TXM, (9)
and its explicit solution of (9) is given in (Absil and Oseledets, 2015, Section 3.2),
R
(2)
X (Î´) = (X + Î´)V[UT
(X + Î´)V]
âˆ’1UT
(X + Î´), (10)
and a proof is given in Appendix.
3.3. Derivation of the proposed algorithms
3.3.1. Derivation of Algorithm 1
The gradient descent algorithm (6) for solving (2) can be written as
L
(k+1) = RL(k) (âˆ’Î·PT
L(k)âˆ‡f(L
(k)
)), (11)
where PT
L(k)
is defined in (7) and RL(k) is defined in (8) or (10). To derive the explicit
algorithm, it remains to find the gradient âˆ‡f.
7
Zhang and Yang
If the absolute values of all entries of A are different, then we have
âˆ‡f(L) = F(L âˆ’ Y). (12)
The proof of (12) is deferred to Appendix. When some entries of A are equivalent and
there is a tie in generating F(L âˆ’ Y), the objective function could be non-differentiable.
However, it can be shown that by arbitrarily breaking the tie, F(L âˆ’ Y) is a subgradient
of f(L).
The corresponding gradient descent method (or subgradient method when f is not
differentiable) with projective retraction can be written as follows:
L
(k+1) := rank-r approximation of h
L
(k) âˆ’ Î·PT
L(k)F(L
(k) âˆ’ Y)
i
, (13)
where the rank-r approximation has been defined after (8). This leads to Algorithm 1 with
Option 1.
For the orthographic retraction, i.e., RL(k) defined according to (10), by writing D =
F(L
(k) âˆ’ Y), the update formula (11) can be simplified to
L
(k+1) := (L
(k) âˆ’ Î·D)V(k)
[U(k)T
(L
(k) âˆ’ Î·D)V(k)
]
âˆ’1U(k)T
(L
(k) âˆ’ Î·D), (14)
where U(k) âˆˆ R
n1Ã—r
is any matrix such that its column space is the same as the column
space of L
(k)
; and V(k) âˆˆ R
n2Ã—r
is any matrix such that its column space is the same as
the row space of L
(k)
. The derivation of (14) is deferred to Appendix, and it can be shown
that the implementation of (14) leads to Algorithm 1 with Option 2.
3.3.2. Derivation of Algorithm 2
By a similar argument as in the previous section, we can conclude that when all entries of
|L âˆ’ Y| are different from each other, then applying the same procedure of deriving (12),
we have
âˆ‡ Ëœf(L) = FËœ(L âˆ’ Y);
and FËœ(Lâˆ’Y) is a subgradient when Ëœf(L) is not differentiable. Based on this observation, the
algorithm under the partially observed setting is identical to (13) or (14), with F replaced
by FËœ. This gives the implementation of Algorithm 2.
3.3.3. Basic convergence properties of Algorithms 1 and 2
An interesting topic is that, can we still expect the algorithm to have reasonable basic properties, such as convergence to a critical point? Unfortunately, it is impossible to have such
a theoretical guarantee if a fixed step size Î· is chosen: in general, the subgradient method
with fixed step size does not have the convergence guarantee if the objective function is
non-differentiable. However, if we choose step size with line search, then any accumulation
point of L
(k)
, Lb, would have the property that either the objective function is not differentiable at Lb, or it is a critical point in the sense that its Riemannian gradient is zero. For
example, the line search strategy for Algorithm 1 can be described as follows: start the step
size Î·k with a relatively large value, and repeatedly shrinks it by a factor of Î² âˆˆ (0, 1) such
that the following condition is satisfied: for L
(k+1) = RL(k) (âˆ’Î·kPT
L(k)âˆ‡f(L
(k)
)),
f(L
(k)
) âˆ’ f(L
(k+1)) > cÎ·kkPT
L(k)âˆ‡f(L
(k)
)k
2
,
8
Robust PCA by Manifold Optimization
where c âˆˆ (0, 1) is prespecified. The argument for convergence follows from the same
argument as the proof of (Absil et al., 2009, Theorem 4.3.1).
3.4. Prior works on manifold optimization
The idea of optimization on manifolds has been well investigated in the literature Vandereycken (2013); Shalit et al. (2012); Absil et al. (2009). For example, Absil et al. Absil
et al. (2009) give a summary of many advances in the field of optimization on manifolds.
Manifold optimization has been applied to many matrix estimation problems, including recovering a low rank matrix from its partial entries, i.e., matrix completion Keshavan et al.
(2010); Vandereycken (2013); Wei et al. (2016) and robust matrix completion in Cambier
and Absil (2016). In fact, the problem studied in this work can be reformulated to the
problem analyzed in Cambier and Absil (2016). In comparison, our work studies a different
algorithm and gives additional theoretical guarantees.
In another aspect, while Wei et al. (2016) studies matrix completion, it shares some
similarities with this work: both works study manifold optimization algorithms and have
theoretical guarantees showing that the proposed algorithms can recover the underlying
low-rank matrix exactly. In fact, Wei et al. (2016) can be considered as our problem under
the partially observed setting, without corruption S
âˆ—
. It proposes to solve
arg min
LâˆˆRn1Ã—n2 ,rank(L)=r
X
(i,j)âˆˆÎ¦
(Yij âˆ’ Lij )
2
,
which can be considered as Ëœf in (5) when Î³ = 0.
4. Theoretical Analysis
In this section, we analyze the theoretical properties of Algorithms 1 and 2 and compare
them with previous algorithms. Since the goal is to recover the low-rank matrix L
âˆ— and
the sparse matrix S
âˆ—
from Y = L
âˆ— + S
âˆ—
, to avoid identifiability issues, we need to assume
that L
âˆ—
can not be both low-rank and sparse. Specifically, we make the following standard
assumptions on L
âˆ— and S
âˆ—
:
Assumption 1 Each row of S
âˆ—
contains at most Î³
âˆ—n2 nonzero entries and each column of
S
âˆ—
contains at most Î³
âˆ—n1 nonzero entries. In other words, for Î³
âˆ— âˆˆ [0, 1), assume S
âˆ— âˆˆ SÎ³
âˆ—
where
SÎ³
âˆ— :=

A âˆˆ R
n1Ã—n2
| kAi,Â·k0 â‰¤ Î³
âˆ—n2, for 1 â‰¤ i â‰¤ n1; kAÂ·,jk0 â‰¤ Î³
âˆ—n1, for 1 â‰¤ j â‰¤ n2
	
.
(15)
Assumption 2 The low-rank matrix L
âˆ—
is not near-sparse. To achieve this, we require that
L
âˆ— must be Âµ-coherent. Given the singular value decomposition (SVD) L
âˆ— = Uâˆ—Î£âˆ—Vâˆ—T
,
where Uâˆ— âˆˆ R
n1Ã—r and Vâˆ— âˆˆ R
n2Ã—r
, we assume there exists an incoherence parameter Âµ
such that
kUâˆ—
k2,âˆž â‰¤
rÂµr
n1
, kVâˆ—
k2,âˆž â‰¤
rÂµr
n2
, (16)
where the norm k Â· k2,âˆž is defined by kAk2,âˆž = maxkzk2=1 kAzkâˆž and kxkâˆž = maxi
|xi
|.
9
Zhang and Yang
4.1. Analysis of Algorithm 1
With Assumption 1 and 2, we have the following theoretical results regarding the convergence rate, initialization, and stability of Algorithm 1:
Theorem 1 (Linear convergence rate, fully observed case) Suppose that kL
(0)âˆ’L
âˆ—kF â‰¤
aÏƒr(L
âˆ—
), where Ïƒr(L
âˆ—
) is the r-th largest singular value of L
âˆ—
, a â‰¤ 1/2, Î³ > 2Î³
âˆ— and
C1 =
q
4(Î³ + 2Î³
âˆ—)Âµr + 4 Î³
âˆ—
Î³âˆ’Î³
âˆ— + a
2 <
1
2
, then there exists Î·0 = Î·0(C1, a) > 0 that does not
depend on k, such that for all Î· â‰¤ Î·0,
kL
(k) âˆ’ L
âˆ—
kF â‰¤

1 âˆ’
1 âˆ’ 2C1
8
Î·
k
kL
(0) âˆ’ L
âˆ—
kF .
Remark 2 (Choices of parameters). It is shown in the proof that Î·0 can be set to the
solution of the equation
Î·0(1 + C1)
2

1
2
+
a
2
1 âˆ’ Î·0(1 + C1)a

=
1
8
(1 âˆ’ 2C1).
Since the LHS is an increasing function of Î·0 and is zero when Î·0 = 0, and its RHS is a
positive number.
While C1 < 1/2 requires p
4Î³
âˆ—/(Î³ âˆ’ Î³
âˆ—) < 1/2, i.e., Î³ > 17Î³
âˆ—
. In practice a much
smaller Î³ can be used. In Section 5, Î³ = 1.5Î³
âˆ—
is used and works well for a large number of
examples. It suggests that some constants in Theorem 1 might be due to the technicalities
in the proof and can be potentially improved.
Remark 3 (Simplified choices of parameters) There exists c1 and c2 such that if a < c1,
Î³
âˆ—Âµr < c2 and Î³ = 65Î³
âˆ—
, then one can choose Î·0 = 1/8. In this sense, if the initialization
of the algorithm is good, then the algorithm can handle Î³
âˆ— as large as O(1/Âµr). In addition,
it requires O(log(1/)) iterations to achieve kL
(k) âˆ’ L
âˆ—kF /kL
(0) âˆ’ L
âˆ—kF < .
Since the statements require proper initializations (i.e., small a), the question arises as to
how to choose proper initializations. The work by Yi et al. (2016) shows that if the rank-r
approximation to F(Y) is used as the initialization L
(0), then such initialization has the
upper bound kL
(0) âˆ’L
âˆ—k according to the proofs of (Yi et al., 2016, Theorems 1 and 3) (we
borrow this estimation along with the fact that kL
(0) âˆ’ L
âˆ—kF â‰¤
âˆš
2rkL
(0) âˆ’ L
âˆ—k).
Theorem 4 (Initialization, fully observed case) If Î³ > Î³âˆ— and we initialize L
(0) using
the rank-r approximation to F(Y), then
kL
(0) âˆ’ L
âˆ—
kF â‰¤ 8Î³Âµrâˆš
2rÏƒ1(L
âˆ—
).
The combination of Theorem 1, 4 and the fact that Î³ = O(Î³
âˆ—
) implies that under the
fully observed setting, the tolerance of the proposed algorithms to corruption is at most
Î³
âˆ— = O(
1
Âµrâˆš
rÎº
), where Îº = Ïƒ1(L
âˆ—
)/Ïƒr(L
âˆ—
) is the condition number of L
âˆ—
. We also study
the stability of Algorithm 1 in the following statement.
10
Robust PCA by Manifold Optimization
Theorem 5 (Stability, fully observed case) Let L be the current value, and let L
+ be
the next update by applying Algorithm 1 to L for one iteration. Assuming Y = L
âˆ— + S
âˆ— +
Nâˆ—
, where Nâˆ—
is a random Gaussian noise i.i.d. sampled from N(0, Ïƒ2
), Î³ > 10Î³
âˆ— and
(Î³ + 2Î³
âˆ—
)Âµr < 1/64, then there exist C, a, c, Î·0 > 0 such that when Î· < Î·0,
P

kL
+ âˆ’ L
âˆ—
kF â‰¤ (1 âˆ’ cÎ·) kL âˆ’ L
âˆ—
kF for all L âˆˆ Î“

â†’1, as n1, n2 â†’ âˆž, (17)
where
Î“ = {L âˆˆ R
n1Ã—n2
: rank(L) = r, CÏƒp
(n1 + n2)r ln(n1n2) â‰¤ kL âˆ’ L
âˆ—
kF â‰¤ aÏƒr(L
âˆ—
)k,
Since 1 âˆ’ cÎ· < 1, and Theorem 5 shows that when the observation Y is contaminated with
a random Gaussian noise, if L
(0) is properly initialized such that kL
(0) âˆ’ L
âˆ—kF < aÏƒr(L
âˆ—
),
Algorithms 1 will converge to a neighborhood of L
âˆ— given by
{L : kL âˆ’ L
âˆ—
kF â‰¤ CÏƒp
(n1 + n2)r ln(n1n2)}
in [âˆ’ log(kL
(0) âˆ’ L
âˆ—kF ) + log(CÏƒp
(n1 + n2)r ln(n1n2))]/ log(1 âˆ’ cÎ·) iterations, with probability goes to 1 as n1, n2 â†’ âˆž.
4.2. Analysis of Algorithm 2
For the partially observed setting, we assume that each entry of Y = L
âˆ— + S
âˆ—
is observed
with probability p. That is, for any 1 â‰¤ i â‰¤ n1 and 1 â‰¤ j â‰¤ n2, Pr((i, j) âˆˆ Î¦) = p. Then
we have the following statement on convergence:
Theorem 6 (Linear convergence rate, partially observed case) There exists c > 0
such that for n = max(n1, n2), if p â‰¥ max(cÂµr log(n)/2 min(n1, n2),
56
3
log n
Î³ min(n1,n2)
), then
with probability 1 âˆ’ 2n
âˆ’3 âˆ’ 6n
âˆ’1
,
kL
(k) âˆ’ L
âˆ—kF
kL(0) âˆ’ Lâˆ—kF
â‰¤
h
s
1 âˆ’ p
2(1 âˆ’ )
2

2Î·

1 âˆ’ CËœ
1 âˆ’
ap(1 + )
2(1 âˆ’ a)
(1 + CËœ
1)

âˆ’ Î·
2(1 + CËœ
1)
2

+
Î·
2a
2
(p + p)
2
(1 + CËœ
1)
2
1 âˆ’ Î·a(p + p)(1 + CËœ
1)
ik
(18)
for
CËœ
1 =
1
p(1 âˆ’ )
h
6(Î³ + 2Î³
âˆ—
)pÂµr + 4 3Î³
âˆ—
Î³ âˆ’ 3Î³
âˆ—
(
p
p(1 + ) + a
2
)
2 + a
2
i
.
Remark 7 (Choice of parameters) Note that when Î· is small, the RHS of (18) is in the
order of
1 âˆ’ p
2
(1 âˆ’ )
2

1 âˆ’ CËœ
1 âˆ’
ap(1 + )
2(1 âˆ’ a)
(1 + CËœ
1)

Î· + O(Î·
2
).
As a result, to make sure that the RHS of (18) to be smaller than 1 for small Î·, we assume
that
1 âˆ’ CËœ
1 âˆ’
ap(1 + )
2(1 âˆ’ a)
(1 + CËœ
1) > 0. (19)
For example, when ap(1 + ) = 4(1 âˆ’ a), it requires that CËœ
1 < 1/3. If (19) holds, then there
exists Î·0 = Î·0(CËœ
1, p, , a) such that for all Î· â‰¤ Î·0, the RHS of (18) is smaller than 1.
The practical choices of Î· and Î³ will be discussed in Section 5.
1 
Zhang and Yang
Remark 8 (Simplified choice of parameters) There exists {ci}
4
i=1 > 0 such that when  <
1/2, a < c1p, Î³
âˆ—Âµr < c2 and Î³ = c3Î³
âˆ—
, then when Î· < 1/8,
kL
(k) âˆ’ L
âˆ—kF
kL(0) âˆ’ Lâˆ—kF
â‰¤ (1 âˆ’ c4Î·p2
)
k
.
Compared with the result in Theorem 1, the addition parameter p appears in both the initialization requirement a < c1p as well as the convergence rate. This makes the result weaker,
but we suspect that the dependence on the subsampling ratio p could be improved through
a better estimation in (39) and the estimation of CËœ
1 in Lemma 16, and we leave it as a
possible future direction.
We present a method of obtaining a proper initialization in Theorem 9. Combining it with
Theorem 6, Algorithm 2 allows the corruption level Î³
âˆ—
to be in the order of O(
p
Âµrâˆš
rÎº
).
Theorem 9 (Initialization, partially observed case) There exists c1, c2, c3 > 0 such
that if Î³ > 2Î³
âˆ—
, and p â‰¥ c2(
Âµr2

2 +
1
Î±
) log n/ min(n1, n2), and we initialize L
(0) using the
rank-r approximation to F(Y), then
kL
(0) âˆ’ L
âˆ—
kF â‰¤ 16Î³ÂµrÏƒ1(L
âˆ—
)
âˆš
2r + 2âˆš
2c1Ïƒ1(L
âˆ—
)
with probability at least 1 âˆ’ c3n
âˆ’1
, where Ïƒ1(L
âˆ—
) is the largest singular value of L
âˆ—
.
4.3. Comparison with Alternating Gradient Descent
Since our objective functions are equivalent to the objective functions of the alternating
gradient descent (AGD) in Yi et al. (2016), it would be interesting to compare these two
works. The only difference of these two works lies in the algorithmic implementation: our
methods use the gradient descent on the manifold of low-rank matrices, while the methods
in Yi et al. (2016) use alternating gradient descent on the factors of the low-rank matrix.
In the following we compare the results of both works from four aspects:
1. Accuracy of initialization. What is the largest value t that the algorithm can
tolerate, such that for any initialization L
(0) satisfying kL
(0)âˆ’L
âˆ—kF â‰¤ t, the algorithm
is guaranteed to converge to L
âˆ—
?
2. Convergence rate. What is the smallest number of iteration steps k such that the
algorithm reaches a given convergence criterion , i.e. kL
(k) âˆ’L
âˆ—kF /kL
(0) âˆ’L
âˆ—kF < ?
3. Corruption level (perfect initialization). Suppose that the initialization is in a
sufficiently small neighborhood of L
âˆ—
(i.e. there exists a very small 0 > 0 such that
L
(0) satisfies kL
(0) âˆ’ L
âˆ—kF < 0), what is the maximum corruption level that can be
tolerated in the convergence analysis?
4. Corruption level (proper initialization). Suppose that the initialization is given
by the procedure in Theorem 4 (for the partially observed case) and 9 (for the fully
observed case), what is the maximum corruption level that can be tolerated?
12
Robust PCA by Manifold Optimization
These comparisons are summarized in Table 1. We can see that under the full observed
setting, our results remove or reduce the dependence on the condition number Îº, while
keeping other values unchanged. Under the partially observed setting our results still have
the advantage of less dependence on Îº, but sometimes require an additional dependence on
the subsampling ratio p. The simulation results discussed in the next section also verify
that when Îº is large our algorithms have better performance, while that the slowing effect
of p under the partially observed setting is not significant. As discussed after Theorem 6,
we suspect that this dependence could be removed after a more careful analysis (or more
assumptions).
Criterion Accuracy of Convergence Max corruption Max corruption
initialization rate (perfect init.) (proper init.)
Algorithm 1 O(Ïƒr(L
âˆ—
)) O(log( 1

)) O(
1
Âµr
) O(
1
Âµr1.5Îº
)
APG (full) O(
Ïƒr(Lâˆ—)
âˆš
Îº
) O(Îº log( 1

)) O(
1
Îº2Âµr
) O(
1
max(Âµr1.5Îº1.5,Îº2Âµr)
)
Algorithm 2 O(
âˆšpÏƒr(L
âˆ—
)) O(log( 1

)/p2
) O(
1
Âµr
) O(
âˆšp
Âµr1.5Îº
)
APG (partial) O(
Ïƒr(Lâˆ—)
Îº
) O(ÎºÂµr log( 1

) O(
1
Îº2Âµr
) O(
1
max(Âµr1.5Îº1.5,Îº2Âµr)
)
Table 1: Comparison of the theoretical guarantees in our work and the alternating gradient descent
algorithm in Yi et al. (2016). The four criteria are explained in details in Section 4.3.
Here we use a simple example to give some intuition of why our proposed methods work
better than gradient descent method based on factorization. Let us consider the following
simple optimization problem:
arg min
zâˆˆRm
f(z), where z = Ax + By,
where x, y âˆˆ R
n and A, B âˆˆ R
mÃ—n
. In this example, (x, y) can be considered as the
â€œfactorsâ€ of z. The gradient descent method on the factors (x, y) is then given by
x
+ = x âˆ’ Î·AT
f
0
(z), y
+ = y âˆ’ Î·B
T
f
0
(z). (20)
Writing the update formula (20) in terms of z, it becomes
z
+ = Ax+ + By+ = z âˆ’ Î·(AAT + BBT
)f
0
(z).
As a result, the â€œgradient descent on factors (x, y)â€ has a direction of âˆ’(AAT +
BBT
)f
0
(z). In comparison, the gradient descent on the variable z has a direction of âˆ’f
0
(z),
which is the direction that f decreases fastest. If AAT + BBT
is a matrix with a large condition number, then we expect that the gradient descent method on factors (x, y) would not
work well. This example shows that generally, compared to applying the gradient descent
method to the factors of a variable, it is better to apply it to the variable itself. Similar
to this example, our method applies gradient descent on the L itself while Yi et al. (2016)
applies gradient descent to the factors of L.
13
Zhang and Yang
4.4. Comparison with other robust PCA algorithms
In this section we compare our result with other robust PCA methods and summarize them
in Table 2. Some criterion in Table 1 are not included since they do not apply. For example, (Netrapalli et al., 2014, Alternating Projection) only analyzes the algorithm with
specific initialization, and the criterion 1 and 3 in Table 1 do not apply to this work. As
a result, we only compare the maximum corruption ratio that these methods can handle, and the computational complexity per iteration in Table 2. As for the convergence
rate, it depends on assumptions on parameters such as the coherence parameter, rank,
and the size of the matrix: The alternating projection Netrapalli et al. (2014) requires
10 log(4n1Âµ
2
rkY âˆ’ L
(0)k2/âˆš
n1n2) iterations to achieve an accuracy , under the assumptions that Î³
âˆ— < 1/512Âµr2 and a tuning parameter is chosen to be 4Âµ
2
r
âˆš
n1n2. The alternating minimization method Gu et al. (2016) have the guarantee that if kL
(1) âˆ’L
âˆ—k2 â‰¤ Ïƒ1(L
âˆ—
),
then
kL
(k+1) âˆ’ L
âˆ—
kF â‰¤ Ïƒ1

96âˆš
2Î½Âµâˆš
r(s
âˆ—/d)
3/2ÎºÏƒ1
1 âˆ’ 24âˆš
2Î½Âµâˆš
r(s
âˆ—/d)
3/2ÎºÏƒ1
!k
kL
(1) âˆ’ L
âˆ—
kF ,
where Î½ is a parameter concerning the coherence of L
âˆ—
, s
âˆ—
is the number of nonzero entries
in S
âˆ—
, d = min(n1, n2). As a result s
âˆ—/d is approximately Î³
âˆ— max(n1, n2) in our notation.
If Î½ is in the order of O(1), then this results requires that Âµ
âˆš
rÎºÏƒ1 max(n1, n2)
3/2Î³
âˆ— 3/2 â‰¤
O(1), which is more restrictive than our assumption in Theorem 1 that Î³
âˆ—Âµr â‰¤ O(1).
Convex methods usually have convergence rate guarantees based on convexity, for example,
the accelerated proximal gradient method Toh and Yun (2010) has a convergence rate of
O(1/k2
). While it is a slower convergence rate compared to the result in Theorem 1 in this
work or the results in Netrapalli et al. (2014); Gu et al. (2016) and it does not necessarily
converge to the correct solution, this result does not depend on any assumption on the
low-rank matrix and the corruption ratio.
Criterion Maximum Complexity
corruption level per iteration
Algorithm 1 O(1/ÎºÂµr3/2
) O(rn1n2)
Convex methods O(1/Âµ2
r) O(n1n2 min(n1, n2))
Netrapalli et al. (2014) 1/512Âµ
2
r O(r
2n1n2)
Gu et al. (2016) O(1/Âµ2/3
r
2/3 min(n1, n2)) O(r
2n1n2)
Yi et al. (2016) O(1/Îº2Âµr3/2
) O(rn1n2)
Table 2: Comparison of the theoretical guarantees in our work and some other robust PCA algorithms.
The stability in Theorem 5 is comparable to analysis in Netrapalli et al. (2014) (the
works Gu et al. (2016) and (Yi et al., 2016, Alternating Gradient Descent) do not have
stability analysis). The work Netrapalli et al. (2014) assumes that kNâˆ—kâˆž < Ïƒr(L
âˆ—
)/100n2
and proves that the output of their algorithm Lb satisfies
kLb âˆ’ L
âˆ—
kF â‰¤  + 2Âµ
2
r

7kNâˆ—
k2 +
8
âˆš
n1n2
âˆš
r
kNâˆ—
kâˆž

,
14
Robust PCA by Manifold Optimization
where  is the error of the algorithm when there is no noise. If Nâˆ—
is i.i.d. sampled from
N(0, Ïƒ2
), this result suggests that kLb âˆ’ L
âˆ—kF is bounded above by  + O(Âµ
2âˆš
rn1n2Ïƒ). In
comparison, Theorem 5 suggests that after a few iterations, kL
(k) âˆ’ L
âˆ—kF is bounded above
by CÏƒp
(n1 + n2)r ln(n1n2) with high probability, which is a tighter upper bound.
5. Simulations
In this section, we test the performance of the proposed algorithms by simulated data sets
and real data sets. The MATLAB implementation of our algorithm used in this section
is available at https://sciences.ucf.edu/math/tengz/. For simulated data sets, we
generate L
âˆ— by UÎ£VT
, where U âˆˆ R
n1Ã—r and V âˆˆ R
n2Ã—r are random matrices that i.i.d.
sampled from N(0, 1), and Î£ âˆˆ R
n1Ã—r
is an diagonal matrix. As for S
âˆ— âˆˆ R
n1Ã—n2
, each
entry is sampled from N(0, 100) with probability q, and is zero with probability 1 âˆ’ q.
That is, q represents the level of sparsity in the sparse matrix S
âˆ—
. It measures the overall
corruption level of Y and is associated with the corruption level Î³
âˆ—
(Î³
âˆ— measures the row
and column-wise corruption level). For the partially observed case, we assume that each
entry of Y is observed with probability p.
5.1. Choice of parameters
We first investigate the performance of the proposed algorithms, in particular, the dependence on the parameters Î· and Î³. In simulations, we let [n1, n2] = [500, 600], r = 3, Î£ = I,
and q = 0.02. For the partially observed case, we let p = 0.2.
The first simulation investigates the following questions:
â€¢ Should we use the Algorithms 1 and 2 with Option 1 or Option 2?
â€¢ What is the appropriate choice of the step size Î·?
The simulation results for Option 1 an 2 with various step sizes are visualized in Figure 2,
which show that the two options perform similarly. Usually the algorithms converge faster
when the step size is larger. However, if the step size is too large then it might diverge.
As a result, we use the step size Î· = 0.7 for Algorithm 1 and 0.7/p for Algorithm 2 for the
following simulations.
The second simulation concerns the choice of Î³. We test Î³ = cÎ³âˆ—
for a few choices of c
(Î³
âˆ—
can be calculated from the zero pattern of S). Figure 5.1 shows that if Î³ is too small,
for example, 0.5Î³
âˆ—
, then the algorithm fail to converge to the correct solution; and if Î³ is
too large, then the convergence is slow. Following these observations, we use Î³ = 1.5Î³
âˆ— as
the default choice of the following experiments, which is also used in Yi et al. (2016).
5.2. Performance of the proposed algorithm
In this section, we analyze the convergence behavior as the parameters (overall ratio of
corrupted entries q, condition number Îº, rank r, subsampling ratio p) changes and visualized
the result in Figure 4.
Figure 4(a) shows the simulation for corruptions level q, we use the setting in Section 5.1,
but replace the corruption level q by q = 0.1, 0.2, 0.3, 0.4. Figure 4 shows that the algorithm
15
Zhang and Yang
0 20 40 60 80 100
Iterations
10-10
10-8
10-6
10-4
10-2
100
102
104
Error
(a)
2 = 0.2
2 = 0.4
2 = 0.7
2 = 1
2 = 1.5
2 = 2.5
0 20 40 60 80 100
Iterations
10-10
10-8
10-6
10-4
10-2
100
102
104
Error
(b)
2 = 0.2
2 = 0.4
2 = 0.7
2 = 1
2 = 1.5
2 = 2.5
0 20 40 60 80 100
Iterations
10-10
10-8
10-6
10-4
10-2
100
102
104
Error
(c)
2 = 0.2/p
2 = 0.4/p
2 = 0.7/p
2 = 1/p
2 = 1.5/p
2 = 2.5/p
0 20 40 60 80 100
Iterations
10-10
10-8
10-6
10-4
10-2
100
102
104
Error
(d)
2 = 0.2/p
2 = 0.4/p
2 = 0.7/p
2 = 1/p
2 = 1.5/p
2 = 2.5/p
Figure 2: The dependence of the estimation error on the number of iterations for different step sizes Î· (a) Algorithm 1 (Option 1); (b) Algorithm 1 (Option 2); (c)
Algorithm 2 (Option 1); (d) Algorithm 2 (Option 2).
0 100 200 300 400 500
Iterations
10-8
10-6
10-4
10-2
100
102
Error
(a)
. = 0.5.
*
. = .
*
. = 1.5.
*
. = 2.
*
. = 3.
*
. = 4.
*
. = 6.
*
0 100 200 300 400 500
Iterations
10-3
10-2
10-1
100
101
102
103
Error
(b)
. = 0.5.
*
. = .
*
. = 1.5.
*
. = 2.
*
. = 3.
*
. = 4.
*
. = 6.
*
Figure 3: The convergence of the algorithm depending on the choice of Î³. (a) fully observed
setting; (b) partially observed setting.
16
Robust PCA by Manifold Optimization
converges slower with more corruption, which is expected since there is fewer information
available. However, the algorithm still converges even with an overall corruption level at
0.4.
Figure 4(b) shows the simulation for rank r, we use the setting in Section 5.1, but replace
r by r = 3, 10, 30, 100, 300 respectively. Simulations show that the algorithm works fine for
rank r = 3, 10, 30, 100 and it converges slower for rank r = 300.
Figure 4(c) shows the simulation for condition number Îº of L, we use the setting in
Section 5.1, but replace Î£ by Î£ = diag(1, 1, 1/Îº) and try various values of Îº. While the
algorithm converges for Îº up to 30 in the simulation, for larger Îº the algorithm converges
slowly at the beginning, and then decreases quickly to zero. We suspect that the initialization is not sufficiently good and it takes a while for the algorithm to reach the â€œlocal
neighborhood of convergenceâ€. We also remark that L with a very large condition number,
e.g. Îº = 100, is generally challenging for any nonconvex optimization algorithm, as shown
in Figure 5, Setting 4. It is because that when Îº is large, the solution is close to a matrix
with rank less than r â€“ a singular point on the manifold of the matrices of rank r, which
gives a geometry of manifold that is not â€œsmoothâ€ enough. We observe that when Îº = 100,
our algorithm performs well if the rank r is set to 2 (instead of the true value 3)â€”in fact,
when Îº = 100, the underlying matrix is approximately of rank 2 since the third singular
value is very small.
We test the algorithm with various matrix sizes using the setting in Section 5.1 and
set [n1, n2] = [1000, 1200], [5000, 6000], [10000, 12000]. Figure 4(d) shows that Algorithm 1
converges quickly for all of the choices within a few iterations.
In the last simulation, we test Algorithm 2 under the setting in Section 5.1 with various
choices of the subsampling ratio p. Figure 4(e) shows suggest that the algorithm converges
for p as small as 0.1, though the convergence rate is slow for small p.
5.3. Comparison with other robust PCA algorithms
In this section, we compare our algorithm with the accelerated proximal gradient method
(APG) and the alternating direction method of multiplier (ADMM) based on convex relaxation (1); the robust matrix completion algorithm (RMC) Cambier and Absil (2016) based
on manifold optimization problem
arg min
rank(L)=r
X
(i,j)âˆˆÎ¦
kLij âˆ’ Yijk + Î»
X
(i,j)6âˆˆÎ¦
L
2
ij ,
as well as the alternating gradient descent method (AGD) in Yi et al. (2016) that solves the
same optimization as this work, but with an implementation based on matrix factorization
rather than manifold optimization. We use the implementation of APG from Toh and Yun
(2010) and the implementation of ADMM from https://github.com/dlaptev/RobustPCA.
In these two algorithms, we use the choice of parameter Î» = 1/
p
max(n1, n2), which is the
default choice in the implementation Toh and Yun (2010) and the theoretical analysis in
Cand`es et al. (2011). For ADMM, the augmented Lagrangian parameter is set by default
as 10Î». For RMC and GD, we use their default setting of parameters. Since the setting
of Algorithm 2 does not apply to the implementations of APG/ADMM, we compare them
under the fully observed setting. We compare them in the following four settings:
17
Zhang and Yang
0 500 1000 1500
Iterations
10-10
10-8
10-6
10-4
10-2
100
102
104
Error
(a)
q = 0.4
q = 0.3
q = 0.2
q = 0.1
0 50 100 150
Iterations
10-10
10-5
100
Error
(b)
rank = 3
rank = 10
rank = 30
rank = 100
rank = 300
0 500 1000 1500 2000 2500 3000
Iterations
10-10
10-5
100
Error
(c)
5 = 1
5 = 3
5 = 10
5 = 30
5 = 100
0 10 20 30 40 50
Iterations
10-10
10-8
10-6
10-4
10-2
100
102
104
Error
(d)
[n1
,n2
] = [500,600]
[n1
,n2
] = [1000,1200]
[n1
,n2
] = [5000,6000]
[n1
,n2
] = [10000,12000]
0 200 400 600 800 1000
Iterations
10-10
10-5
100
Error
(e)
p = 1
p = 0.5
p = 0.2
p = 0.1
p = 0.05
Figure 4: Dependence of the estimation error on the number of iterations for different (a)
Overall ratios of corrupted entries q (Algorithm 1); (b) Ranks r (Algorithm 1);
(c) Condition numbers Îº (Algorithm 1); (d) Matrix sizes [n1, n2] (Algorithm 1);
(e) Subsampling ratio p (Algorithm 2).
18
Robust PCA by Manifold Optimization
0 20 40 60 80 100 120
Running Time
10-10
10-5
100
Error
Setting 1
RMC
APG
ADMM
Algorithm 1
AGD
0 5 10 15 20
Running Time
10-15
10-10
10-5
100
105
Error
Setting 2
RMC
APG
ADMM
Algorithm 1
AGD
0 200 400 600 800 1000
Running Time
10-15
10-10
10-5
100
105
Error
Setting 3
RMC
APG
ADMM
Algorithm 1
AGD
0 1 2 3 4
Running Time
10-4
10-3
10-2
10-1
100
101
102
103
Error
Setting 4
RMC
APG
ADMM
Algorithm 1
AGD
Figure 5: The comparison of the performance of the algorithms under the fully observed
setting. The running time is measured in seconds.
â€¢ Setting 1: same setting as in Section 5.1.
â€¢ Setting 2 (large condition number): replace Î£ by diag(1, 1, 0.1) in Setting 1.
â€¢ Setting 3 (large matrix): replace n1 and n2 by 3000 and 4000 in Setting 1.
â€¢ Setting 4 (large condition number): replace Î£ by diag(1, 1, 0.01) in Setting 1.
Figure 5 shows that under Setting 1, 2 and 3, Algorithm 1 converges faster than other
algorithms. In particular, the advantage over the AGD algorithm is very clear under Setting
2, where the condition number is larger. This verifies our theoretical analysis, where the
convergence rate is faster than the analysis in Yi et al. (2016) by a factor of âˆš
Îº. In Setting 3,
the algorithms RMC, AGD and Algorithm 1 converge much faster than APG and ADMM,
which verifies the computational advantage of nonconvex algorithms when the matrix size
is large. However, in Setting 4, the convex algorithms converge to the correct solution
while the nonconvex algorithms converge to a local minimizer that is different than the
correct solution. This is due to the fact that the nonconvex algorithms have more than one
minimizer, and if it is not initialized well then it could get trapped in local minimizers. In
19
Zhang and Yang
0 1 2 3 4 5
Running Time
10-8
10-6
10-4
10-2
100
102
104
Error
Comparison of robust PCA algorithms, partially observed setting
RMC
Algorithm 2
AGD
Figure 6: The comparison of the performances of the algorithms under the partially observed setting. The running time is measured in seconds.
practice, we observed that if the initialization is well chosen and close to the true L
âˆ—
, then
Algorithm 1 converges quickly to the correct solution.
We also compare the performance of RMC, AGD and Algorithm 2 under the partially
observed setting. We use Setting 1 with p = 0.3 and visualize the result in Figure 6.
The results are similar to that of the fully observed setting: AGD and Algorithm 2 are
comparable and RMC converges faster at the beginning, but then does not achieve higher
accuracy, possibly due to their choice of the regularization parameter.
We also test the proposed algorithms in a real data application for video background
subtraction. We adopt the public data set Shoppingmall studied in Yi et al. (2016),1 A few
frames are visualized in the first column of Figure 7. There are 1000 frames in this video
sequence, represented by a matrix of size 81920 Ã— 1000, where each column corresponds
to a frame of the video and each row corresponds to a pixel of the video. We apply our
algorithms with r = 3 and Î³
âˆ— = 0.1, p = 0.5 for the partially observed case, the step size
Î· = 0.7. We stop the algorithm after 100 iterations. Figure 7 shows that our algorithms
obtain desirable low-rank approximations within 100 iterations.
In Figure 8, we compare our algorithms with APG in terms of the convergence of the
objective function value. In this figure, the relative error is defined as kF(Lâˆ’ Y)kF /kYkF ,
a scaled objective value. A smaller relative error implies a better low-rank approximation.
Figure 8 shows out that our algorithms can obtain smaller objective values within 100
iterations under both fully observed and partially observed cases.
1. The data set is originally from http://perception.i2r.a-star.edu.sg/bk_model/bk_index.html, and
is available at https://sciences.ucf.edu/math/tengz/.
20
Robust PCA by Manifold Optimization
Figure 7: The performance of Algorithms 1 and 2 in video background subtraction, with
three rows representing three frames in the video sequence. For Algorithm 2, a
subsampling ratio of p = 0.5 is used.
0 5 10 15 20 25
Iterations
0
0.05
0.1
0.15
0.2
0.25
0.3
Relative Error
Algorithm 1, fully observed case
AGD, fully observed case
Algorithm 2, partially observed case
AGD, partially observed case
Figure 8: The relative error of Algorithms 1, 2, and AGD with respect to the iterations,
for both fully observed case and partially observed case in the experiment of
background subtraction.
21
Zhang and Yang
6. Conclusion
This paper proposes two robust PCA algorithms (one for fully observed case and one for
partially observed case) based on the gradient descent algorithm on the manifold of lowrank matrices. Theoretically, compared with the gradient descent algorithm with matrix
factorization, our approach has a faster convergence rate, better tolerance of the initialization accuracy and corruption level. The approach removes or reduces the dependence of
the algorithms on the condition number of the underlying low-rank matrix. Numerically,
the proposed algorithms performance is less sensitive to the choice of step sizes. We also
find that under the partially observed setting, the performance of the proposed algorithm
is not significantly affected by the presence of the additional dependence on the observation
probability. Considering the popularity of the methods based on matrix factorization, it
is an interesting future direction to apply manifold optimization to other low-rank matrix
estimation problems.
Acknowledgements
The authors thank the editor, an associate editor and referee for their helpful comments
and suggestions. The authors also thank David Fleischer for providing helps on the coding.
Yangâ€™s research is partially supported by NSERC RGPIN-2016-05174. Zhangâ€™s research is
partially supported by National Science Foundation (NSF) grant CNS-1739736.
22
Robust PCA by Manifold Optimization
Appendix for â€œRobust PCA by Manifold Optimization"
A. Technical Derivations in Section 3
Verification of (7). Formula (7) can be verified as follows. Let hÂ·iF be the Frobenius
inner product of two matrices, then
hD âˆ’ PTXM(D), AVVT
iF = h(I âˆ’ UUT
)D(I âˆ’ VVT
), AVVT
iF
=h(I âˆ’ UUT
)D(I âˆ’ VVT
)VVT
, AiF = h0, AiF = 0
and similarly hD âˆ’ PTXM(D), UUT BiF = 0. As a result, hD âˆ’ PTXM(D), AVVT +
UUT BiF = 0 for all A âˆˆ R
n1Ã—n1 and B âˆˆ R
n2Ã—n2
, which verifies formula (7) by showing
that D âˆ’ PTXM(D) is orthogonal to TXM.
Verification of (10). It is clear that R
(2)
X (Î´) defined in (10) has rank r; and to show that
hR
(2)
X (Î´)âˆ’(X+Î´), ZiF = 0 for all Z âˆˆ TXM, we first write this property as [R
(2)
X (Î´)âˆ’(X+
Î´)] âŠ¥ TXM for simplicity, and since TXM = {AVVT + UUT B : for A âˆˆ R
n1Ã—n1
, B âˆˆ
R
n2Ã—n2 }, we just need to show that hR
(2)
X (Î´) âˆ’ (X + Î´), AVVT + UUT BiF = 0 for all
A âˆˆ R
n1Ã—n1 and B âˆˆ R
n2Ã—n2
. This is easy to verify, because we have R
(2)
X (Î´)V = (X+Î´)V,
hR
(2)
X (Î´) âˆ’ (X + Î´), AVVT
iF = h(R
(2)
X (Î´)V âˆ’ (X + Î´)V)VT
, AiF = h0, AiF = 0, (21)
Similarly, we can easily verify that UT R
(2)
X (Î´) = UT
(X + Î´), we have hR
(2)
X (Î´) âˆ’ (X +
Î´), UUT BiF = 0, and therefore [R
(2)
X (Î´) âˆ’ (X + Î´)] âŠ¥ TXM. As a result, there exists a
unique R
(2)
X such that rank(R
(2)
X ) = r and [R
(2)
X (Î´) âˆ’ (X + Î´)] âŠ¥ TXM.
Verification of (12). We first define the operator S : R
n1Ã—n2 â†’ R
n1Ã—n2 such that F(A) =
S(A) â—¦ A (â—¦ represents the elementwise product), i.e.,
S(A) = (
0, if |Aij | > |Ai,Â·
|
[Î³] and |Aij | > |AÂ·,j |
[Î³]
,
1, otherwise.
Then if the absolute values of all entries of A are different, the sparsity pattern does
not change under a small perturbation, i.e., S(A) = S(A + âˆ†). Then by definition of f(Â·),
f(L + âˆ†) âˆ’ f(L) = 1
2
kS(L âˆ’ Y + âˆ†) â—¦ (L âˆ’ Y + âˆ†)k
2
F âˆ’
1
2
kS(L âˆ’ Y) â—¦ (L âˆ’ Y)k
2
F
=
1
2
kS(L âˆ’ Y) â—¦ (L âˆ’ Y + âˆ†)k
2
F âˆ’
1
2
kS(L âˆ’ Y) â—¦ (L âˆ’ Y)k
2
F
=hS(L âˆ’ Y) â—¦ (L âˆ’ Y), âˆ†iF + O(kâˆ†k
2
F
),
where â—¦ represents the Hadamard product, i.e., the elementwise product between matrices.
Verification of (14). It is sufficient to prove the case where U(k) and V(k) are given by
the SVD decomposition L
(k) = U(k)Î£(k)V(k)T
. Denote D = âˆ‡f(L
(k)
) = F(L
(k) âˆ’ Y). Set
X = L
(k) and Î´ = âˆ’Î·PT
L(k)M(D) in (10), we have
L
(k+1) := (L
(k) âˆ’ Î·PT
L(k)M(D))V(k)
[U(k)T
(L
(k) âˆ’ Î·PT
L(k)M(D))V(k)
]
âˆ’1
(22)
U(k)T
(L
(k) âˆ’ Î·PT
L(k)M(D))
23
Zhang and Yang
On the other hand, from (7) we have the projection
PT
L(k)M(D) = U(k)U(k)T D + DV(k)V(k)T âˆ’ U(k)U(k)T DV(k)V(k)T
.
As a result
PT
L(k)M(D)V(k) = [U(k)U(k)T D + DV(k)V(k)T âˆ’ U(k)U(k)T DV(k)V(k)T
]V(k)
= U(k)U(k)T DV(k) + DV(k)V(k)T V(k) âˆ’ U(k)U(k)T DV(k)V(k)T V(k) = DV(k)
(23)
and similarly,
U(k)TPT
L(k)M(D) = U(k)T D. (24)
Combining (23), (24) with (22), the update formula (14) is verified.
B. Proof of Theorem 1
In this proof, we will investigate kL
+ âˆ’ L
âˆ—kF , where
L
+ = RL(âˆ’Î·PTL F(L âˆ’ Y)).
It is sufficient to prove that when kL âˆ’ L
âˆ—k â‰¤ aÏƒr(L
âˆ—
) with the value a satisfying the
conditions in Theorem 1, then
kL
+ âˆ’ L
âˆ—
kF â‰¤

1 âˆ’
1 âˆ’ 2C1
8
Î·

kL âˆ’ L
âˆ—
kF . (25)
To prove (25), we first introduce three auxiliary lemmas.
Lemma 10 (a) Let D = L âˆ’ L
âˆ— âˆ’ F(L âˆ’ Y) = L âˆ’ L
âˆ— âˆ’ F(L âˆ’ L
âˆ— âˆ’ S
âˆ—
), then
kDk
2
F â‰¤ C
2
1 kL âˆ’ L
âˆ—
k
2
F
. (26)
(b) For the noisy setting where Y = L
âˆ— + S
âˆ— + Nâˆ—
, and D0 = L âˆ’ L
âˆ— âˆ’ Nâˆ— âˆ’ F(L âˆ’ Y),
we have
kD0
k
2
F â‰¤ 2C
2
1 kL âˆ’ L
âˆ—
k
2
F + 2(Î³ + 5Î³
âˆ—
)N1, (27)
where N1 = n2
Pn1
i=1 |Nâˆ—
i,Â·
|
max + n1
Pn2
j=1 |Nâˆ—
Â·,j |
max
.
Lemma 11 If kL âˆ’ L
âˆ—kF â‰¤ aÏƒr(L
âˆ—
) and a â‰¤ 1, then
k(L âˆ’ L
âˆ—
) âˆ’ PTL
(L âˆ’ L
âˆ—
)kF â‰¤
a
2(1 âˆ’ a)
kL âˆ’ L
âˆ—
kF , (28)
k(L âˆ’ L
âˆ—
) âˆ’ PTLâˆ— (L âˆ’ L
âˆ—
)kF â‰¤
a
2
kL âˆ’ L
âˆ—
kF . (29)
Lemma 12 For X âˆˆ TLM, then
kR
(i)
L
(X) âˆ’ (L + X)kF â‰¤
kXk
2
F
2(Ïƒr(L) âˆ’ kXk)
, for either i = 1 or 2.
24
Robust PCA by Manifold Optimization
To prove (25), first we note that
kL âˆ’ L
âˆ—
k
2
F âˆ’ kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ L
âˆ—
k
2
F
=kL âˆ’ L
âˆ—
k
2
F âˆ’ kL âˆ’ L
âˆ—
k
2
F + 2Î·hL âˆ’ L
âˆ—
, PTL F(L âˆ’ Y)iF âˆ’ kÎ·PTL F(L âˆ’ Y)k
2
F
=2Î·hL âˆ’ L
âˆ—
, PTL F(L âˆ’ Y)iF âˆ’ kÎ·PTL F(L âˆ’ Y)k
2
F
=2Î·hL âˆ’ L
âˆ—
, PTL
(L âˆ’ L
âˆ—
) âˆ’ PTL
(L âˆ’ L
âˆ— âˆ’ F(L âˆ’ Y))iF âˆ’ Î·
2
kPTL F(L âˆ’ Y)k
2
F
=2Î·hPTL
(L âˆ’ L
âˆ—
), PTL
(L âˆ’ L
âˆ—
) âˆ’ PTLDiF âˆ’ Î·
2
kPTL F(L âˆ’ Y)k
2
F
â‰¥2Î·(kPTL
(L âˆ’ L
âˆ—
)k
2
F âˆ’ kDkF kPTL
(L âˆ’ L
âˆ—
)kF ) âˆ’ Î·
2
(kL âˆ’ L
âˆ—
kF + kDkF )
2
. (30)
The fourth line is obtained by PTL
(L âˆ’ L
âˆ— âˆ’ F(L âˆ’ Y)) = L âˆ’ L
âˆ— âˆ’ PTL F(L âˆ’ Y)iF .
The fifth line is because L âˆ’ L
âˆ— = PTL
(L âˆ’ L
âˆ—
) + P
âŠ¥
TL
(L âˆ’ L
âˆ—
). The last line uses CauchySchwarz inequality hPTL
(Lâˆ’L
âˆ—
), PTLDiF â‰¤ kDkF kPTL
(Lâˆ’L
âˆ—
)kF and triangular inequality
kPTL F(L âˆ’ Y)kF â‰¤ kL âˆ’ L
âˆ—kF + kPTL
(D)kF â‰¤ kL âˆ’ L
âˆ—kF + kDkF . Lemma 11 and the
assumptions kL âˆ’ L
âˆ—kF â‰¤ aÏƒr(L
âˆ—
) and q
1 âˆ’ (
a
2(1âˆ’a)
)
2 >
1
2
imply
kPTL
(L âˆ’ L
âˆ—
)kF â‰¥
1
2
kL âˆ’ L
âˆ—
kF . (31)
Combining it with the estimation of kDkF in Lemma 10, we have
kL âˆ’ L
âˆ—
k
2
F âˆ’ kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ L
âˆ—
k
2
F
â‰¥ Î·(
1
2
âˆ’ C1)kL âˆ’ L
âˆ—
k
2
F âˆ’ Î·
2
(1 + C1)
2
kL âˆ’ L
âˆ—
k
2
F
. (32)
When ths RHS of (32) is positive (i.e., when (1 âˆ’ 2C1) â‰¥ 2Î·(1 + C1)
2
), (32) implies kL âˆ’
L
âˆ—kF > kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ L
âˆ—kF and
kL âˆ’ L
âˆ—
kF âˆ’ kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ L
âˆ—
kF
â‰¥
Î·(
1
2 âˆ’ C1)kL âˆ’ L
âˆ—k
2
F âˆ’ Î·
2
(1 + C1)
2kL âˆ’ L
âˆ—k
2
F
kL âˆ’ Lâˆ—kF + kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ Lâˆ—kF
â‰¥
1
2

Î·(
1
2
âˆ’ C1) âˆ’ Î·
2
(1 + C1)
2

kL âˆ’ L
âˆ—
kF . (33)
In addition,
kPTL F(L âˆ’ Y)kF â‰¤ kF(L âˆ’ Y)kF = kL âˆ’ L
âˆ—
kF + kDkF â‰¤ (1 + C1)kL âˆ’ L
âˆ—
kF (34)
and Lemma 12 give
kL
+ âˆ’ L
âˆ—
kF âˆ’ kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ L
âˆ—
kF â‰¤ kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ L
+kF
â‰¤
Î·
2kPTL F(L âˆ’ Y)k
2
F
Ïƒr(Lâˆ—) âˆ’ Î·kPTL F(L âˆ’ Y)kF
â‰¤
Î·
2a
2
(1 + C1)
2
1 âˆ’ Î·a(1 + C1)
kL âˆ’ L
âˆ—
kF . (35)
Combining (33) and (35),
kL âˆ’ L
âˆ—kF âˆ’ kL
+ âˆ’ L
âˆ—kF
kL âˆ’ Lâˆ—kF
â‰¥
1
4
Î·(1 âˆ’ 2C1) âˆ’ Î·
2
(1 + C1)
2

1
2
+
a
2
1 âˆ’ Î·(1 + C1)a

.
Therefore, Theorem 1 is proved when C1 < 1/2, and Î·0 is chosen such that
Î·0(1 + C1)
2

1
2
+
a
2
1 âˆ’ Î·0(1 + C1)a

â‰¤
1
8
(1 âˆ’ 2C1).
25
Zhang and Yang
C. Proof of Theorem 5
The proof of the noisy case also follows similarly from the proofs of Theorem 1 and 6. Note
that
F(L âˆ’ Y) = L âˆ’ L
âˆ— âˆ’ Nâˆ— âˆ’ D0
,
and define Q = PTL
(Lâˆ’L
âˆ—
), then following the proof of Theorem 1 and applying Lemma 10
(b), we have
kL âˆ’ L
âˆ—
k
2
F âˆ’ kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ L
âˆ—
k
2
F
=2Î·hL âˆ’ L
âˆ—
, PTL F(L âˆ’ Y)iF + O(Î·
2
) = 2Î·hPTL
(L âˆ’ L
âˆ—
), PTL F(L âˆ’ Y)iF + O(Î·
2
)
=2Î·hPTL
(L âˆ’ L
âˆ—
), PTL
(L âˆ’ L
âˆ— âˆ’ Nâˆ— âˆ’ D0
)iF + O(Î·
2
)
â‰¥2Î·

kQk
2
F âˆ’ hNâˆ—
, QiF âˆ’ kQkF
q
2C2
1
kL âˆ’ Lâˆ—k
2
F + 2(Î³ + 5Î³
âˆ—)N1

+ O(Î·
2
).
In addition, (35) gives

kL
+ âˆ’ LkF âˆ’ kL âˆ’ Î·PTL F(L âˆ’ Y) âˆ’ L
âˆ—
kF

 = O(Î·
2
).
Combining it with the estimation of C1, N1, and hNâˆ—
, QiF in Lemma 13 and the fact that
(1 âˆ’
a
2(1âˆ’a)
)kL âˆ’ L
âˆ—kF â‰¤ kQkF â‰¤ (1 + a
2(1âˆ’a)
)kL âˆ’ L
âˆ—kF (which follows from Lemma 11),
the Theorem is proved.
Lemma 13 If Nâˆ— âˆˆ R
n1Ã—n2
is elementwisely i.i.d. sampled from N(0, Ïƒ2
), then
(a) with probability 1 âˆ’
4
n
7
1n
7
2
,
Pn1
i=1(|Nâˆ—
i,Â·
|
max)
2 â‰¤ 16Ïƒ
2n1 ln(n1n2), and Pn2
j=1(|Nâˆ—
Â·,j |
max)
2 â‰¤
16Ïƒ
2n2 ln(n1n2), and as a result, N1 â‰¤ 32Ïƒ
2n1n2 ln(n1n2).
(b) There exists C6 > 0 such that as n1 + n2 â†’ âˆž, the probability that
hNâˆ—
, PTL
(L âˆ’ L
âˆ—
)iF â‰¤
1
4
kPTL
(L âˆ’ L
âˆ—
)k
2
F
(36)
holds for all {L : C6Ïƒ
p
(n1 + n2)r ln(n1n2) â‰¤ kL âˆ’ L
âˆ—kF â‰¤ aÏƒr(L
âˆ—
)} converges to 1.
D. Proof of Theorem 6
This proof borrows two lemmas from (Yi et al., 2016, Lemmas 9, 10) as follows.
Lemma 14 (Yi et al., 2016, Lemma 9) There exists c > 0 such that for all 0 <  < 1,
if p â‰¥ cÂµr log(n)/2 min(n1, n2), then with probability at least 1 âˆ’ 2n
âˆ’3
, for all X in the
tangent plane TLâˆ— , i.e., all X that can be written as L
âˆ—A + BLâˆ—
, where A âˆˆ R
n2Ã—n2 and
B âˆˆ R
n1Ã—n1
,
(1 âˆ’ )kXk
2
F â‰¤
1
p
kPÎ¦Xk
2
F â‰¤ (1 + )kXk
2
F
.
Lemma 15 (Yi et al., 2016, Lemma 10) If p â‰¥
56
3
log n
Î³ min(n1,n2)
, the with probability at least
1 âˆ’ 6n
âˆ’1
, the number of entries in Î¦ per row is in the interval [pn2/2, 3pn2/2], and the
number of entries in Î¦ per column is in [pn1/2, 3pn1/2].
Then we introduce the following lemma parallel to Lemma 10:
26
Robust PCA by Manifold Optimization
Lemma 16 When the events in Lemmas 14 and 15 hold, for DËœ = PÎ¦[L âˆ’ L
âˆ— âˆ’ FËœ(L âˆ’ Y)]
we have
kDËœ k
2
F â‰¤ CËœ2
1 kL âˆ’ L
âˆ—
k
2
F
, (37)
with
CËœ
1 =
1
p(1 âˆ’ )
h
6(Î³ + 2Î³
âˆ—
)pÂµr + 4 3Î³
âˆ—
Î³ âˆ’ 3Î³
âˆ—
(
p
p(1 + ) + a
2
)
2 + a
2
i
.
The proof of Theorem 6 is parallel to the proof of Theorem 1, with L
+ defined slightly
differently by
L
+ = RL(âˆ’Î·PTL FËœ(L âˆ’ Y)).
Defining PÎ¦ : R
n1Ã—n2 â†’ R
n1Ã—n2 by
[PÎ¦X]ij =
(
Xij , if (i, j) âˆˆ Î¦,
0, if (i, j) âˆˆ/ Î¦.
Then FËœ(L âˆ’ Y) = PÎ¦FËœ(L âˆ’ Y). Following a similar analysis as (30),
kL âˆ’ L
âˆ—
k
2
F âˆ’ kL âˆ’ Î·PTL PÎ¦FËœ(L âˆ’ Y) âˆ’ L
âˆ—
k
2
F
=2Î·hL âˆ’ L
âˆ—
, PTL PÎ¦FËœ(L âˆ’ Y)iF âˆ’ kÎ·PTL PÎ¦FËœ(L âˆ’ Y)k
2
F
â‰¥2Î·hPÎ¦PTL
(L âˆ’ L
âˆ—
), PÎ¦FËœ(L âˆ’ Y)iF âˆ’ kÎ·PÎ¦FËœ(L âˆ’ Y)k
2
F
â‰¥2Î·hPÎ¦(L âˆ’ L
âˆ—
) âˆ’ PÎ¦P
âŠ¥
TL
(L âˆ’ L
âˆ—
), PÎ¦(L âˆ’ L
âˆ—
) âˆ’ DËœ iF âˆ’ Î·
2
(kPÎ¦(L âˆ’ L
âˆ—
)kF + kDËœ kF )
2
,
(38)
here P
âŠ¥
TL
represents the projector to the subspace orthogonal to TL. Lemma 11 and
Lemma 14 imply
kPÎ¦P
âŠ¥
TL
(L âˆ’ L
âˆ—
)kF
kPÎ¦(L âˆ’ Lâˆ—)kF
â‰¤
kP
âŠ¥
TL
(L âˆ’ L
âˆ—
)kF
kPÎ¦(L âˆ’ Lâˆ—)kF
â‰¤
ap(1 + )
2(1 âˆ’ a)
, (39)
and combining it with the estimation of DËœ in Lemma 16, the RHS of (38) is larger than
kPÎ¦(L âˆ’ L
âˆ—
)k
2
F

2Î·

1 âˆ’ CËœ
1 âˆ’
ap(1 + )
2(1 âˆ’ a)
(1 + CËœ
1)

âˆ’ Î·
2
(1 + CËœ
1)
2

. (40)
In addition, Lemma 14 implies
kPÎ¦FËœ(L âˆ’ Y)kF â‰¤ kPÎ¦(L âˆ’ L
âˆ—
)kF + kPÎ¦DËœ kF
â‰¤ (1 + CËœ
1)kPÎ¦(L âˆ’ L
âˆ—
)kF ,
â‰¤ (1 + CËœ
1)p(1 + )kL âˆ’ L
âˆ—
k
and combining it with Lemma 12,
kL
+ âˆ’ L
âˆ—
kF âˆ’ kL âˆ’ Î·PTL PÎ¦FËœ(L âˆ’ Y) âˆ’ L
âˆ—
kF â‰¤
Î·
2a
2
(p + p)
2
(1 + CËœ
1)
2
1 âˆ’ Î·a(p + p)(1 + CËœ
1)
kL âˆ’ L
âˆ—
kF .
27
Zhang and Yang
Combining it with (40) and Lemma 11, we have
kL
+ âˆ’ L
âˆ—kF
kL âˆ’ Lâˆ—kF
â‰¤
s
1 âˆ’ p
2(1 âˆ’ )
2

2Î·

1 âˆ’ CËœ
1 âˆ’
ap(1 + )
2(1 âˆ’ a)
(1 + CËœ
1)

âˆ’ Î·
2(1 + CËœ
1)
2

+
Î·
2a
2
(p + p)
2
(1 + CËœ
1)
2
1 âˆ’ Î·a(p + p)(1 + CËœ
1)
,
and Theorem 6 is proved.
E. Proof of Lemmas
Lemma 10(a) Proof By the definition of F, for any matrix A, A âˆ’ F(A) is a sparse
matrix, therefore
D = L âˆ’ L
âˆ— âˆ’ S
âˆ— âˆ’ F(L âˆ’ L
âˆ— âˆ’ S
âˆ—
) + S
âˆ—
is a sparse matrix. Denote the locations of the nonzero entries of D by S, and divide it into
two sets S1 âˆª S2 defined as follows:
S1 = {(i, j) : |[Lâˆ’L
âˆ—âˆ’S
âˆ—
]ij | > |[Lâˆ’L
âˆ—âˆ’S
âˆ—
]i,Â·
|
[Î³]
and |[Lâˆ’L
âˆ—âˆ’S
âˆ—
]ij | > |[Lâˆ’L
âˆ—âˆ’S
âˆ—
]Â·,j |
[Î³]
},
and
S2 = {(i, j) âˆˆ S / 1 : Dij = [L âˆ’ L
âˆ—
]ij âˆ’ F(L âˆ’ L
âˆ— âˆ’ S
âˆ—
)ij 6= 0}.
For (i, j) âˆˆ S1, [F(L âˆ’ L
âˆ— âˆ’ S
âˆ—
)]ij = 0. As a result, Dij = [L âˆ’ L
âˆ—
]ij . In addition, by
definition of F(Â·), each row or column of D has at most Î³ percentage of points in S1.
For (i, j) âˆˆ S2, since [F(L âˆ’ L
âˆ— âˆ’ S
âˆ—
)]ij = [L âˆ’ L
âˆ— âˆ’ S
âˆ—
]ij , we have Dij = S
âˆ—
ij 6= 0. By
Assumption 1, therefore, for each row or column of D, at most Î³
âˆ— percentage of points lie
in S2.
Combine the results
|[L âˆ’ L
âˆ— âˆ’ S
âˆ—
]i,Â·
|
[Î³] â‰¤ {|[L âˆ’ L
âˆ—
]i,Â·
| + |[S
âˆ—
]i,Â·
|}[Î³] â‰¤ |[L âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—]
,
|[L âˆ’ L
âˆ— âˆ’ S
âˆ—
]j,Â·
|
[Î³] â‰¤ {|[L âˆ’ L
âˆ—
]j,Â·
| + |[S
âˆ—
]j,Â·
|}[Î³] â‰¤ |[L âˆ’ L
âˆ—
]j,Â·
|
[Î³âˆ’Î³
âˆ—]
,
with [F(L âˆ’ L
âˆ— âˆ’ S
âˆ—
)]ij = [L âˆ’ L
âˆ— âˆ’ S
âˆ—
]ij , we have for (i, j) âˆˆ S2
|Dij | =|[L âˆ’ L
âˆ— âˆ’ F(L âˆ’ L
âˆ— âˆ’ S
âˆ—
)]ij |
â‰¤|[L âˆ’ L
âˆ—
]ij | + |F(L âˆ’ L
âˆ— âˆ’ S
âˆ—
)]ij |
â‰¤|[L âˆ’ L
âˆ—
]ij | + max(|[L âˆ’ L
âˆ— âˆ’ S
âˆ—
]i,Â·
|
[Î³]
, |[L âˆ’ L
âˆ— âˆ’ S
âˆ—
]Â·,j |
[Î³]
)
â‰¤|[L âˆ’ L
âˆ—
]ij | + max(|[L âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—]
, |[L âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—]
).
Applying the estimations above, and repeatedly use the fact that (x + y)
2 â‰¤ 2x
2 + 2y
2
, we
have
28
Robust PCA by Manifold Optimization
kDk
2
F =
X
(i,j)âˆˆS
D2
ij =
X
(i,j)âˆˆS1
D2
ij +
X
(i,j)âˆˆS2
D2
ij â‰¤
X
(i,j)âˆˆS1
[L âˆ’ L
âˆ—
]
2
ij
+
X
(i,j)âˆˆS2
n
|[L âˆ’ L
âˆ—
]ij | + max(|[L âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—]
, |[L âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—]
)
o2
â‰¤
X
(i,j)âˆˆS1
[L âˆ’ L
âˆ—
]
2
ij + 2 X
(i,j)âˆˆS2
[L âˆ’ L
âˆ—
]
2
ij + 2 X
(i,j)âˆˆS2
max{|[L âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—]
, |[L âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—]
}
2
â‰¤
X
(i,j)âˆˆS1
[L âˆ’ L
âˆ—
]
2
ij + 2 X
(i,j)âˆˆS2
[L âˆ’ L
âˆ—
]
2
ij + 2 X
(i,j)âˆˆS2
{|[L âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—] + |[L âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—]
}
2
â‰¤
X
(i,j)âˆˆS1
[L âˆ’ L
âˆ—
]
2
ij + 2 X
(i,j)âˆˆS2
[L âˆ’ L
âˆ—
]
2
ij + 4 X
(i,j)âˆˆS2
{|[L âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—]
}
2 + {|[L âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—]
}
2
â‰¤
X
(i,j)âˆˆS
[L âˆ’ L
âˆ—
]
2
ij +
X
(i,j)âˆˆS2
[L âˆ’ L
âˆ—
]
2
ij + 4 Î³
âˆ—
Î³ âˆ’ Î³
âˆ—
kL âˆ’ L
âˆ—
k
2
F
â‰¤2
X
(i,j)âˆˆS
[PTLâˆ— (L âˆ’ L
âˆ—
)]2
ij + 2 X
(i,j)âˆˆS2
[PTLâˆ— (L âˆ’ L
âˆ—
)]2
ij + 4 Î³
âˆ—
Î³ âˆ’ Î³
âˆ—
kL âˆ’ L
âˆ—
k
2
F
+ 2 X
(i,j)âˆˆS
[L âˆ’ L
âˆ— âˆ’ PTLâˆ— (L âˆ’ L
âˆ—
)]2
ij + 2 X
(i,j)âˆˆS2
[L âˆ’ L
âˆ— âˆ’ PTLâˆ— (L âˆ’ L
âˆ—
)]2
ij
â‰¤2
X
(i,j)âˆˆS
[PTLâˆ— (L âˆ’ L
âˆ—
)]2
ij + 2 X
(i,j)âˆˆS2
[PTLâˆ— (L âˆ’ L
âˆ—
)]2
ij + 4 Î³
âˆ—
Î³ âˆ’ Î³
âˆ—
kL âˆ’ L
âˆ—
k
2
F
+ 4kL âˆ’ L
âˆ— âˆ’ PTLâˆ— (L âˆ’ L
âˆ—
)k
2
F
. (41)
Note that from line 5 to line 6, we used the fact that for x âˆˆ R
n
, and k â‰¤ n
k(x(k)
)
2 â‰¤ (x(k)
)
2 + (x(k+1))
2 + Â· Â· Â· + (x(nâˆ’1)) + (x(n)
) â‰¤ (x(1)) + Â· Â· Â· + (x(n)
) = kxk
2
F
,
where x(k)
is the k-th order statistics of x1, . . . , xn, i.e. the k-th smallest value. This gives
us
(Î³ âˆ’ Î³
âˆ—
)n2|[L âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—] â‰¤ k[L âˆ’ L
âˆ—
]i,Â·k
2
2
; (Î³ âˆ’ Î³
âˆ—
)n2|[L âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—] â‰¤ k[L âˆ’ L
âˆ—
]Â·,jk
2
2
.
Therefore
X
(i,j)âˆˆS2
|[L âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—] â‰¤
Î³
âˆ—n2
(Î³ âˆ’ Î³
âˆ—)n2
k[L âˆ’ L
âˆ—
]i,Â·k
2
F
; (42)
X
(i,j)âˆˆS2
|[L âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—] â‰¤
Î³
âˆ—n1
(Î³ âˆ’ Î³
âˆ—)n1
k[L âˆ’ L
âˆ—
]Â·,jk
2
F
. (43)
The values Î³
âˆ—n2 and Î³
âˆ—n1 in the numerator of the right hand sides in 42 and 43 are due to
the fact that, in each row or column of D, at most Î³
âˆ— percentage of points lie in S2.
On the other hand, Lemma 11 implies
kL âˆ’ L
âˆ— âˆ’ PTLâˆ— (L âˆ’ L
âˆ—
)kF â‰¤
a
2
kL âˆ’ L
âˆ—
kF . (44)
29
Zhang and Yang
In addition, using the fact that there exists A âˆˆ R
n1Ã—r and B âˆˆ R
n2Ã—r
, such that
PTLâˆ— (L âˆ’ L
âˆ—
) = AVT + UBT and kPTLâˆ— (L âˆ’ L
âˆ—
)k
2
F = kAVT k
2
F + kUBT k
2
F
, and that for
each row or column, at most Î³ + Î³
âˆ— percentage of points lie in S, we have
X
(i,j)âˆˆS
[PTLâˆ— (L âˆ’ L
âˆ—
)]2
ij â‰¤ 2
X
(i,j)âˆˆS
[k(AVT
)ijk
2 + k(UBT
)ijk
2
]
â‰¤2(Î³ + Î³
âˆ—
)Âµr X
1â‰¤iâ‰¤n1,1â‰¤jâ‰¤n2
[k(AVT
)ijk
2 + k(UBT
)ijk
2
] = 2(Î³ + Î³
âˆ—
)ÂµrkPTLâˆ— (L âˆ’ L
âˆ—
)k
2
F
â‰¤2(Î³ + Î³
âˆ—
)ÂµrkL âˆ’ L
âˆ—
k
2
F
. (45)
Similarly, kAk2,âˆž = maxkzk2=1 kAzkâˆž
X
(i,j)âˆˆS2
[PTLâˆ— (L âˆ’ L
âˆ—
)]2
ij â‰¤ 2Î³
âˆ—ÂµrkL âˆ’ L
âˆ—
k
2
F
, (46)
Combining (41)-(46), (26) is proved.
Lemma 10(b) Proof Let L
0 = L âˆ’ Nâˆ—
, then applying the fact that for any x, y âˆˆ R
n
,
|[x + y]|
[Î³] â‰¤ |[x]|
[Î³] + |[x]|
max
,
where |[x]|
max represents the largest value of |[x]|. We have
kD0
k
2
F â‰¤
X
(i,j)âˆˆS
[L
0 âˆ’ L
âˆ—
]
2
ij +
X
(i,j)âˆˆS2
[L
0 âˆ’ L
âˆ—
]
2
ij
+ 2 X
(i,j)âˆˆS2
{(|[L
0 âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—]
)
2 + (|[L
0 âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—]
)
2
}
â‰¤ 2
ï£«
ï£­
X
(i,j)âˆˆS
[L âˆ’ L
âˆ—
]
2
ij + |Nâˆ—
ij |
2 +
X
(i,j)âˆˆS2
[L âˆ’ L
âˆ—
]
2
ij + |Nâˆ—
ij |
2
ï£¶
ï£¸
+ 4 X
(i,j)âˆˆS2
{(|[L
0 âˆ’ L
âˆ—
]i,Â·
|
[Î³âˆ’Î³
âˆ—]
)
2 + (|Nâˆ—
i,Â·
|
max)
2 + (|[L
0 âˆ’ L
âˆ—
]Â·,j |
[Î³âˆ’Î³
âˆ—]
)
2 + (|Nâˆ—
Â·,j |
max)
2
}
â‰¤ 2C
2
1 kL âˆ’ L
âˆ—
k
2
F + 2(Î³ + 5Î³
âˆ—
)N1,
where the last inequality follows from the proof of part (a) and the definition of N1.
Lemma 11 Proof Let the SVD decomposition of L
âˆ— be L
âˆ— = UÎ£V, UâŠ¥ and VâŠ¥ be
orthogonal matrices of sizes R
n1Ã—(n1âˆ’r) and R
n2Ã—(n2âˆ’r)
such that Col(UâŠ¥) âŠ¥ Col(U) and
Col(VâŠ¥) âŠ¥ Col(V) (here Col(U) represents the subspace spanned by the columns of U).
Let
L
âˆ—
(1,1) â‰¡ UTL
âˆ—V, L
âˆ—
(1,2) â‰¡ UTL
âˆ—VâŠ¥,
L
âˆ—
(2,1) â‰¡ UâŠ¥TL
âˆ—V, L
âˆ—
(2,2) â‰¡ UâŠ¥TL
âˆ—VâŠ¥.
30
Robust PCA by Manifold Optimization
Since rank(L
âˆ—
) = r, we have
L
âˆ—
(2,2) = L
âˆ—
(2,1)L
âˆ—
(1,1)
âˆ’1L
âˆ—
(1,2).
Since all singular values of L
âˆ—
(1,1) are larger than (1 âˆ’ a)Ïƒr(L
âˆ—
), if the singular value decomposition of L
âˆ—
(1,1)
âˆ’1
is given by
L
âˆ—
(1,1)
âˆ’1 = U0Î£0VT
0
,
then the kÎ£0k â‰¤ 1/(1 âˆ’ a)Ïƒr(L
âˆ—
). Applying
kABk
2
F â‰¤ kAk
2
F kBk
2
F
and the fact that for a square, diagonal matrix Î£, |[XÎ£]ij | = |XijÎ£jj | â‰¤ kÎ£k|Xij |, we have
kL
âˆ—
(2,2)kF = kL
âˆ—
(2,1)U0Î£0VT
0 L
âˆ—
(1,2)kF
â‰¤ kL
âˆ—
(2,1)U0Î£0kF kVT
0 L
âˆ—
(1,2)kF
â‰¤
1
(1 âˆ’ a)Ïƒr(Lâˆ—)
kL
âˆ—
(2,1)U0kF kVT
0 L
âˆ—
(1,2)kF
â‰¤
1
(1 âˆ’ a)Ïƒr(Lâˆ—)
kL
âˆ—
(2,1)kF kL
âˆ—
(1,2)kF
â‰¤
1
(1 âˆ’ a)Ïƒr(Lâˆ—)
kL
âˆ—
(2,1)k
2
F + kL
âˆ—
(1,2)k
2
F
2

â‰¤
1
(1 âˆ’ a)Ïƒr(Lâˆ—)
a
2Ïƒr(L
âˆ—
)
2
2

â‰¤
a
2
2(1 âˆ’ a)
Ïƒr(L
âˆ—
), (47)
and (28) is proved. The proof of (29) is similar.
Lemma 12 Proof Let the SVD decomposition of L be L = UÎ£V, and
L(1,1) = UT
(X + L)V,L(1,2) = UT
(X + L)VâŠ¥
= UT XVâŠ¥,L(2,1) = UâŠ¥T
(X + L)V = UâŠ¥T XV,
then it is clear that
R
(2)
L
(X) = L + X + UâŠ¥L(2,1)L(1,1)
âˆ’1L(1,2)VâŠ¥T
,
and using the same argument as in (47),
kL(2,1)L(1,1)
âˆ’1L(1,2)kF â‰¤
1
Ïƒr(Lâˆ—
(1,1))
kL(1,2)kF kL(2,1)kF
â‰¤
1
Ïƒr(L) âˆ’ kXk
kL(2,1)k
2
F + kL(1,2)k
2
F
2

â‰¤
1
Ïƒr(L) âˆ’ kXk
kXk
2
F
2
.
31
Zhang and Yang
So Lemma 12 is proved for R
(2)
L
(X).
By definition, R
(1)
L
(X) is the closest matrix to L + X that has rank r, so kR
(1)
L
(X) âˆ’
(L + X)kF â‰¤ R
(2)
L
(X) âˆ’ (L + X)kF and Lemma 12 is also proved for R
(1)
L
(X).
Lemma 13 Proof WLOG, we assume Ïƒ = 1 and the generic cases can be proved
similarly.
(a) It follows from the estimation of the distribution of the maximum of n1 i.i.d. Gaussian
variables {gi}
n1
i=1:
Pr{ max
1â‰¤iâ‰¤n1
|gi
| â‰¤ 4
p
ln(n1n2)} â‰¥ 
1 âˆ’ 2 exp 
âˆ’
(4p
ln(n1n2))2
2
n1
â‰¥1 âˆ’ 2n1 exp 
âˆ’
(4p
ln(n1n2))2
2

= 1 âˆ’ 2n
âˆ’7
1 n
âˆ’8
2
,
where the first inequality applies the estimation of the cumulative distribution function of
the Gaussian distribution (Ledoux and Talagrand, 1991, pg 8).
Combining this estimation for each column of Nâˆ— and applying the union bound, the second inequality in part (a) holds with probability 1âˆ’2n
âˆ’7
1 n
âˆ’7
2
. Similarly, the first inequality
in part (a) holds with the same probability.
(b) First, we parameterize L by g(L) = PLâˆ— (L âˆ’ L
âˆ—
). Then we claim that, for any L
and L
0
such that kL âˆ’ L
âˆ—kF , kL
0 âˆ’ L
âˆ—kF â‰¤ aÏƒr(L
âˆ—
), there exists C0 depending on a such
that
kPTL
(L âˆ’ L
âˆ—
) âˆ’ PTL0
(L
0 âˆ’ L
âˆ—
)kF â‰¤ C0kg(L) âˆ’ g(L
0
)kF . (48)
To prove (48), apply (29) and obtain
kL âˆ’ L
0
kF â‰¤
1
1 âˆ’
a
2
kg(L) âˆ’ g(L
0
)kF . (49)
Since PTL = ULUT
L + VLVT
L âˆ’ ULUT
LVLVT
L
, and using Davis-Kahan theorem Davis
and Kahan (1970) and the assumption kL âˆ’ L
âˆ—kF â‰¤ aÏƒr(L
âˆ—
), there exists c1, c2 depending
on a such that
kULUT
L âˆ’ UL0UT
L0kF â‰¤ c1, kVLVT
L âˆ’ VL0VT
L0kF â‰¤ c2,
so there exists C
0 depending on a such that
kPTL
(L âˆ’ L
âˆ—
) âˆ’ PTL0
(L
0 âˆ’ L
âˆ—
)kF (50)
=k[PTL0
(L âˆ’ L
âˆ—
) âˆ’ PTL0
(L
0 âˆ’ L
âˆ—
)] + [PTL
(L âˆ’ L
âˆ—
) âˆ’ PTL0
(L âˆ’ L
âˆ—
)]kF
â‰¤kL âˆ’ L
0
kF + C
0
kL âˆ’ L
0
kF .
Combining (49) and (50), (48) is proved.
Second, based on (48), we will apply an -net covering argument to finish the proof that
combines probabilistic estimation for each L and a union bound (-net covering argument
is a standard argument in probabilistic estimation Vershynin (2012)). Use the estimation of
32
Robust PCA by Manifold Optimization
the cumulative distribution function of the Gaussian distribution (Ledoux and Talagrand,
1991, pg 8), for any L
0
,
Pr 
hNâˆ—
, PTL0
(L
0 âˆ’ L
âˆ—
)iF â‰¥ tkPTL0
(L
0 âˆ’ L
âˆ—
)kF
	
â‰¤
1
2
exp 
âˆ’
t
2
2

.
For any L such that kg(L
0
) âˆ’ g(L)kF < , applying (48),
Pr {hNâˆ—
, PTL
(L âˆ’ L
âˆ—
)iF â‰¥ tkPTL
(L âˆ’ L
âˆ—
)kF + C0(kNâˆ—
kF + t)} â‰¤ 1
2
exp 
âˆ’
t
2
2

.
Using union bound, there is an -net of the set {g(L) : kg(L)kF = x} with at most
(C5x/)
n1r+n2râˆ’r
2
points. Therefore, for all L such that x âˆ’  â‰¤ kPTL
(L âˆ’ L
âˆ—
)kF â‰¤ x + ,
Pr {hNâˆ—
, PTL
(L âˆ’ L
âˆ—
)iF â‰¥ tkPTL
(L âˆ’ L
âˆ—
)kF + 2C0(kNâˆ—
kF + t)}
â‰¤
1
2
exp 
âˆ’
t
2
2

Â·
C5x

n1r+n2râˆ’r
2
. (51)
Let t = x/8 and  = x/16C0kNâˆ—kF , then when kNâˆ—kF â‰¥ 1 (which holds with high probability as n1n2 goes to infinity), then using C0 â‰¥ 1 we have  â‰¤ x/16, and when x â‰¥ 4,
tkPTL
(L âˆ’ L
âˆ—
)kF + 2C0(kNâˆ—
kF + t) â‰¤
x
8
(x + ) + x
8kNâˆ—kF
(kNâˆ—
kF + t)
=
x
8
(x + ) + x
8
+
x
2
64kNâˆ—kF
â‰¤
x
2
8
17
16
+
x
8
+
x
2
64
â‰¤
x
2
8
17
16
+
x
2
32
+
x
2
64
â‰¤
1
4
(x âˆ’ )
2
â‰¤
1
4
kPTL
(L âˆ’ L
âˆ—
)k
2
F
, (52)
where the last inequality applies the assumption x âˆ’  â‰¤ kPTL
(L âˆ’ L
âˆ—
)kF . Combining (51)
and (52) and recall that t = x/8, we have that for all L such that x âˆ’ x/16C0kNâˆ—kF â‰¤
kPTL
(L âˆ’ L
âˆ—
)kF â‰¤ x + x/16C0kNâˆ—kF ,
Pr(
hNâˆ—
, PTL
(L âˆ’ L
âˆ—
)iF â‰¥
1
4
kPTL
(L âˆ’ L
âˆ—
)k
2
F
,
for all L s.t.


kPTL
(L âˆ’ L
âˆ—
)kF âˆ’ x


 â‰¤
x
16C0kNâˆ—kF
)
â‰¤
1
2
exp 
âˆ’
x
2
128
Â·

16C5C0kNâˆ—
kF
n1r+n2râˆ’r
2
. (53)
33
Zhang and Yang
Let xi =
p
n1 + n2 + 128(n1r + n2r âˆ’ r
2) ln(16C5C0kNâˆ—kF )(1 + 1/16C0kNâˆ—kF )
i with i =
1, 2, ..., then
Xâˆž
i=1
exp 
âˆ’
x
2
i
128
Â·

16C5C0kNâˆ—
kF
n1r+n2râˆ’r
2
â‰¤ exp(âˆ’
n1 + n2
128
)
Xâˆž
i=1
exp(âˆ’(1 + 1/16C0kNâˆ—
kF )
2i
)
â‰¤ exp(âˆ’
n1 + n2
128
)
Xâˆž
i=1
exp(âˆ’1 âˆ’ i/8C0kNâˆ—
kF )
= exp(âˆ’
n1 + n2
128
âˆ’ 1) exp(âˆ’1/8C0kNâˆ—kF )
1 âˆ’ exp(âˆ’1/8C0kNâˆ—kF )
â‰¤ 8C0kNâˆ—
kF exp(âˆ’
n1 + n2
128
âˆ’ 1), (54)
where the last inequality uses exp(âˆ’c) â‰¤ 1 âˆ’ c when c â‰¥ 0. Clearly, the RHS goes to 0 as
n1 + n2 â†’ âˆž.
Combining the estimation (53) for {xi}âˆž
i=1, with probability 1âˆ’8C0kNâˆ—kF exp(âˆ’
n1+n2
128 âˆ’
1), the event (36) holds for all L such that
kg(L)kF â‰¥ max(p
n1 + n2 + 128(n1r + n2r âˆ’ r
2) ln(16C5C0kNâˆ—kF ), 4).
Combining it with (29), the event (36) holds for all for all L such that
aÏƒr(L
âˆ—
) â‰¥ kL âˆ’ L
âˆ—
kF
â‰¥
1
1 âˆ’
a
2
max(p
n1 + n2 + 128(n1r + n2r âˆ’ r
2) ln(16C5C0kNâˆ—kF ), 4).
Considering that p
n1 + n2 + 128(n1r + n2r âˆ’ r
2) ln(16C5C0kNâˆ—kF ) is the dominant term
when n1, n2 â†’ âˆž, Lemma 13(b) is proved.
Lemma 16 Proof Following (41) and the proof of Lemma 10[a], and note that Lemma 15
means that Î³
âˆ— and Î³ are replaced by arbitrary numbers in the intervals [0.5pÎ³âˆ—
, 1.5pÎ³âˆ—
] and
[0.5pÎ³, 1.5pÎ³], we have
kDËœ k
2
F â‰¤ 6(Î³ + 2Î³
âˆ—
)pÂµrkL âˆ’ L
âˆ—
k
2
F + 4 3Î³
âˆ—
Î³ âˆ’ 3Î³
âˆ—
kPÎ¦(L âˆ’ L
âˆ—
)k
2
F + a
2
kL âˆ’ L
âˆ—
k
2
F
.
Applying Lemma 11 and (44), we have
kPÎ¦(L âˆ’ L
âˆ—
)kF â‰¤ kPÎ¦PTLâˆ— (L âˆ’ L
âˆ—
)kF + kPÎ¦P
âŠ¥
TLâˆ—
(L âˆ’ L
âˆ—
)kF
â‰¤
p
p(1 + )kL âˆ’ L
âˆ—
kF +
a
2
kL âˆ’ L
âˆ—
kF .
Combining it with the estimation of kPÎ¦(L âˆ’ L
âˆ—
)kF in Lemma 11, we have kDËœ kF â‰¤
CËœ
1kPÎ¦(L âˆ’ L
âˆ—
)kF with
CËœ
1 =
1
p(1 âˆ’ )
h
6(Î³ + 2Î³
âˆ—
)pÂµr + 4 3Î³
âˆ—
Î³ âˆ’ 3Î³
âˆ—
(
p
p(1 + ) + a
2
)
2 + a
2
i
.
34
Robust PCA by Manifold Optimization
