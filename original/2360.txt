We propose a new class of semiparametric exponential family graphical models for the analysis of high dimensional mixed data. Different from the existing mixed graphical models, we
allow the nodewise conditional distributions to be semiparametric generalized linear models
with unspecified base measure functions. Thus, one advantage of our method is that it is
unnecessary to specify the type of each node and the method is more convenient to apply
in practice. Under the proposed model, we consider both problems of parameter estimation
and hypothesis testing in high dimensions. In particular, we propose a symmetric pairwise
score test for the presence of a single edge in the graph. Compared to the existing methods
for hypothesis tests, our approach takes into account of the symmetry of the parameters,
such that the inferential results are invariant with respect to the different parametrizations
of the same edge. Thorough numerical simulations and a real data example are provided
to back up our theoretical results.
Keywords: Graphical Models, Exponential Family, High Dimensional Inference
1. Introduction
Given a d-dimensional random vector X = (X1, . . . , Xd)
T
, inferring the conditional independence among X and quantifying its uncertainty are important tasks in statistics. We
propose a unified framework for modeling, estimation, and uncertainty assessment for a
new type of graphical model, named as semiparametric exponential family graphical model.
Let G = (V, E) be an undirected graph with node set V = {1, 2, . . . , d} and edge set
E ⊆ {(j, k): 1 ≤ j < k ≤ d}. The semiparametric exponential family graphical model
specifies the joint distribution of X such that for each j ∈ V, the conditional distribution
of Xj given X\j
:= (X1, . . . , Xj−1, Xj+1, . . . , Xd)
T
is of the form
p(xj | x\j
) = exp
ηj (x\j
) · xj + fj (xj ) − bj (ηj , fj )

, (1)

where x\j = (x1, . . . , xj−1, xj+1, . . . , xd), ηj (x\j
) = αj +
P
k6=j
βjkxk is the canonical parameter, fj (·) is an unknown base measure function, and bj (·, ·) is the log-partition function.
Besides, we assume βjk = βkj for all j 6= k. By definition, the unknown parameter contains {(αj , βjk, fj ): 1 ≤ j < k ≤ d}. To make the model identifiable, we set αj = 0 and
absorb the term αjxj into fj (xj ). By the Hammersley-Clifford theorem (Besag, 1974), we
have βjk 6= 0 if and only if Xj and Xk are conditionally independent given {X`
: ` 6= j, k}.
Therefore, we set (j, k) ∈ E if and only if βjk 6= 0. The graph G thus characterizes the
conditional independence relationship among the high dimensional distribution of X. The
key feature of the proposed model is that (1) it is a general semiparametric model and (2)
it can be used to handle mixed data, which means that X may contain both continuous
and discrete random variables. Unlike the existing mixed graphical models, we allow the
nodewise conditional distributions to be semiparametric generalized linear models with unspecified base measure functions. Thus, our method does not need to specify the type of
each node and is more convenient to apply in practice. In addition to the proposed new
model, our paper has the following two novel contributions.
First, for the purpose of estimating βjk, we extend the multistage relaxation algorithm
(Zhang, 2010) and conduct a localized analysis for a more sophisticated loss function obtained by a statistical chromatography method (Liang and Qin, 2000; Diao et al., 2012;
Chan, 2012; Ning et al., 2017b). The gradient and Hessian matrix of the loss function are
nonlinear U-statistics with unbounded kernel functions. This makes our technical analysis more challenging than that in Zhang (2010). Under the assumption that the sparse
eigenvalue condition holds locally, we prove the same optimal statistical rates for parameter
estimation as in high dimensional linear models.
Second, we propose a symmetric pairwise score test for the null hypothesis H0 : βjk =
0. This is equivalent to testing whether Xj and Xk are conditionally independent given
{X`
: ` 6= j, k}. Compared with Ning et al. (2017b), the novelty of our method is that
we consider a more sophisticated cross type inference which incorporates the symmetry
of the parameter, i.e., βjk = βkj . By considering this unique structure of the graphical
model, our proposed method achieves the invariance property of the inferential results.
That means the same p-values are obtained for testing βjk = 0 and βkj = 0. In contrast,
the asymmetric method in Ning et al. (2017b) may lead to different conclusions for testing
these two equivalent null hypotheses.
1.1. Related Works
There is a huge literature on estimating undirected graphical models (Lauritzen, 1996;
Edwards, 2000; Whittaker, 2009). For modeling continuous data, the most commonly used
methods are Gaussian graphical models (Yuan and Lin, 2007; Banerjee et al., 2008; Friedman
et al., 2008; Ravikumar et al., 2011; Rothman et al., 2008; Lam and Fan, 2009; Shen et al.,
2012; Yuan, 2010; Cai et al., 2011; Sun and Zhang, 2013; Guo et al., 2011; Danaher et al.,
2014; Mohan et al., 2014; Meinshausen and B¨uhlmann, 2006; Peng et al., 2009; Friedman
et al., 2010). To relax the Gaussian assumption, Liu et al. (2009); Xue et al. (2012b); Liu
et al. (2012); Ning and Liu (2013) propose the Gaussian copula model and Voorman et al.
(2014) study the joint additive models for graph estimation. For modeling binary data,
the Ising graphical model is considered by Lee et al. (2006); H¨ofling and Tibshirani (2009);
2
On Semiparametric Exponential Family Graphical Models
Ravikumar et al. (2010); Xue et al. (2012a); Cheng et al. (2014). In addition to binary
data, Allen and Liu (2012) and Yang et al. (2013b) consider the Poisson data and Guo
et al. (2015) consider the ordinal data. Moreover, Yang et al. (2013a) propose exponential
family graphical models, and Tan et al. (2014) propose a general framework for graphical
models with hubs.
Recently, modeling the mixed data attracts increasing interests (Lee and Hastie, 2015;
Fellinghauer et al., 2013; Cheng et al., 2017; Chen et al., 2015; Fan et al., 2017; Yang et al.,
2014). Compared with Lee and Hastie (2015); Cheng et al. (2017); Chen et al. (2015); Yang
et al. (2014), our model has the following two main advantages. First, it is a semiparametric
model, which does not need to specify the parametric conditional distribution for each node.
Therefore, it provides a more flexible modeling framework than the existing ones. Second,
under our proposed model, the estimation and inference methods are easier to implement.
Unlike these existing methods, we propose a unified estimation and inference procedure,
which does not need to distinguish whether the node satisfies the Gaussian distribution
or the Bernoulli distribution. In addition, our estimation and inference methods are more
efficient than the nonparametric approach in Fellinghauer et al. (2013). Finally, our method
is more convenient for modeling the count data than the latent Gaussian copula approach
in Fan et al. (2017).
Though significant progress has been made towards developing new graph estimation
procedures, the research on uncertainty assessment of the estimated graph lags behind.
In low dimensions, Drton et al. (2007); Drton and Perlman (2008) establish confidence
subgraph of Gaussian graphical models. In high dimensions, Ren et al. (2015); Jankov´a
and van de Geer (2015); Gu et al. (2015) study the confidence interval for a single edge
under Gaussian (copula) graphical models and Liu et al. (2013) study the false discovery
rate control. However, all these methods rely on the Gaussian or sub-Gaussian assumption
and cannot be easily applied to the discrete data and more generally the mixed data in high
dimensions.
1.2. Notation
We adopt the following notation throughout this paper. For any vector v = (v1, . . . , vd)
T ∈
R
d
, we define its support as supp(v) = {t: vt 6= 0}. We define its `0-norm, `p-norm, and `∞-
norm as kvk0 = |supp(v)|, kvkp = (P
j∈[d]
|vj |
p
)
1/p and kvk∞ = maxj∈[d]
|vj |, respectively,
where p > 1. Let v
⊗2 = vvT be the Kronecker product of a vector v and itself. We write
v ◦ u = (v1u1, . . . , vdud)
T as the Hadamard product of two vectors u, v ∈ R
d
. In addition,
we use |v| = (|v1|, . . . , |vd|)
T
to denote the elementwise absolute value of vector v and define
kvkmin = minj∈[d]
|vj |. For any matrix A = [ajk] ∈ R
d1×d2
, let AS1S2 =[ajk]j∈S1,k∈S2 be the
submatrix of A with indices in S1×S2; let Aj\j = [ajk]k6=j
. Besides, let kAk2, kAk1, kAk∞,
kAk`p be the spectral norm, elementwise `1-norm, elementwise `∞-norm, and operator `pnorm of A, respectively. Furthermore, for two matrices A1 and A2, we write A1  A2 if
A2−A1 is positive semidefinite and write A1 ≤ A2 if every entry of A2−A1 is nonnegative.
For a function f(x): R
d → R, we write ∇f(x), ∇Sf(x), ∇2f(x) and ∂f(x) as the gradient
of f(x), the gradient of f(x) with respect to xS, the Hessian of f(x), and the subgradient of
f(x), respectively. Moreover, we write {1, 2, . . . , d} as [d]. For a sequence of random vectors
{Yi}i≥1 and a random vector Y , we write Yi Y if {Yi}i≥1 converges to Y in distribution.
3
Yang, Ning, and Liu
Finally, for functions f(n) and g(n), we write f(n) . g(n) to denote that f(n)≤cg(n) for a
universal constant c∈(0,+∞) and we write f(n)  g(n) when f(n) . g(n) and g(n) . f(n)
hold simultaneously.
1.3. Paper Organization
The rest of this paper is organized as follows. In §2 we introduce the semiparametric
exponential family graphical models. In §3 we present our methods for graph estimation
and uncertainty assessment. In §4 we lay out the assumptions and main theoretical results.
We study the finite-sample performance of our method on both simulated and real-world
datasets in §5 and conclude the paper in §6 with some discussion.
2. Semiparametric Exponential Family Graphical Models
The semiparametric exponential family graphical models are defined by specifying the conditional distribution of each variable Xj given the rest of the variables {Xk : k 6= j}.
Definition 1 (Semiparametric exponential family graphical model) A d-dimensional
random vector X = (X1, . . . , Xd)
T ∈ R
d
follows a semiparametric exponential graphical
model with graph G = (V, E) if for any node j ∈ V, the conditional density of Xj given X\j
satisfies
p(xj |x\j
) = exp
xj (β
T
j x\j
) + fj (xj ) − bj (βj , fj )

, (2)
where fj (·) is an unknown base measure function and bj (·, ·) is a known log-partition function. In particular, (j, k) ∈ E if and only if βjk 6= 0.
This model is semiparametric since we treat both βj = (βj1, . . . , βjj−1, βjj+1, . . . , βjd)
T ∈
R
d−1 and the univariate function fj (·) as parameters, where βj and fj (·) are the parametric
and nonparametric components, respectively. Because the model in Definition 1 is only
specified by the conditional distributions of each variable, it is important to understand
the conditions under which a valid joint distribution of X exists. This problem has been
addressed by Chen et al. (2015). As shown in their Proposition 1, one sufficient condition
for the existence of joint distribution of X is that, (i) βjk = βkj for 1 ≤ j, k ≤ d and (ii)
g(x) := expP
j<k βjkxjxk +
Pd
j=1 fj (xj )

is integrable.
Hereafter, we assume that the above two conditions hold. Thus, there exists a joint
probability distribution for the model defined in (2), whose density has the form of
p(x) = expX
k<`
βk`xkx` +
X
d
j=1
fj (xj ) − A

{βi
, fi}i∈[d]


, (3)
where βk` 6= 0 if and only if (k, `) ∈ E. Here A(·) is the log-partition function given by
A

{βi
, fi}i∈[d]

:= logZ
Rd
expX
k<`
βk`xkx` +
X
d
j=1
fj (xj )

ν(dx)

, (4)
where ν(·) is a product measure satisfying ν(dx) = Q
j∈[d]
νj (dxj ), and each νj is either a
Lebesgue or a counting measure on the domain of Xj , depending whether Xj is discrete or      
On Semiparametric Exponential Family Graphical Models
continuous. Since βk` = β`k for all pairs of nodes (k, `), in the sequel, we will use βk` and
β`k interchangeably for notational simplicity.
Furthermore, we remark that, without the knowledge of {fj}j∈[d]
, estimating parameters
{βj}j∈[d]
is insufficient to learn the distribution of X. In this paper, we focus on the
statistical inference of the underlying conditional independence graph specified by {βj}j∈[d]
.
In the next section, by adopting a loss function for {βj}j∈[d]
that is free of the base measures,
we obtain estimators of these parameters, which are used to construct an estimator of the
underlying graph. Moreover, by further considering the hypothesis testing problem for each
βjk, we are able to assess the uncertainty of the estimated graph.
2.1. Examples
We provide some widely used parametric examples in the class of semiparametric exponential family graphical models.
Gaussian Graphical Models: The Gaussian graphical models assume that X ∈ R
d
follows a multivariate Gaussian distribution N(0, Θ−1
), where Θ ∈ R
d×d
is the precision
matrix satisfying Θjj = 1 for j ∈ [d]. The conditional distribution of Xj given X\j
satisfies
Xj | X\j = α
T
j X\j + j with j ∼ N(0, 1),
where αj = Θ\j,j . The conditional density is given by
p(xj | x\j
) = p
1/(2π) exp
−xj (ΘT
\j,jx\j
) − 1/2 · x
2
j − 1/2 · (ΘT
\j,jx\j
)
2

.
Compared with (2), we obtain βj = −Θ\j,j , fj (x) = −x
2/2 and bj (βj , fj ) = (β
T
j x\j
)
2/2 +
log(2π)/2.
Ising Models: In an Ising model with no external field, X takes value in {0, 1}
d and the
joint probability mass function p(x) ∝ exp(P
j<k θjkxjxk). Let θj = (θj1, . . . , θj,j−1, θj,j+1, . . . , θjd)
T
.
The conditional distribution of Xj given X\j
is of the form
p(xj | x\j
) = expP
k<` θk`xkx`

P
xj∈{0,1}
expP
k<` θk`xkx`
 = expn
xj

θ
T
j x\j

− log
1 + exp(θ
T
j x\j
)
o
.
Therefore, in this case we have βj = θj , fj (x) = 0 and bj (βj , fj ) = log[1 + exp(β
T
j x\j
)].
Exponential Graphical Models: For exponential graphical models, X takes values in
[0, +∞)
d and the joint probability density satisfies p(x) ∝ exp(−
Pd
j=1 φjxj−
P
k<` θk`xkx`).
In order to ensure that this probability distribution is normalizable, we require that φj >
0, θjk ≥ 0 for all j, k ∈ [d]. Then we obtain the following conditional probability density of
Xj given X\j
:
p(xj | x\j
) = exp
−
X
d
k=1
φkxk −
X
k<`
θk`xkx`
Z
xj≥0
exp 
−
X
d
k=1
φkxk −
X
k<`
θk`xkx`

dxj
= exp
−xj

φj + θ
T
j x\j

− log
φj + θ
T
j x\j
.
Thus, we have βj = −θj , fj (x) = −φjx and bj (βj , fj ) = log(β
T
j x\j + φj            
Yang, Ning, and Liu
Poisson Graphical Models: In a Poisson graphical model, every node Xj is a discrete
random variable taking values in N = {0, 1, 2, . . .}. The joint probability mass function is
given by
p(x) ∝ expX
d
j=1
φjxj −
X
d
j=1
log(xj !) +X
k<`
θk`xkx`

.
Similar to the exponential graphical models, we also need to impose some restrictions on
the parameters so that the probability mass function is normalizable. Here we require that
θjk ≤ 0 for all j, k ∈ [d]. By direct computation, the conditional probability mass function
of Xj given X\j
is given by
p(xj | x\j
) = exp
xj

θ
T
j x\j

+ φjxj − log(xj !) − bj (θj , fj )

,
where we have βj = θj , fj (x) = φjx − log(x!) and bj (βj , fj ) = logP∞
y=0 exp
y(β
T
j x\j
) +
fj (y)
	.
3. Graph Estimation and Uncertainty Assessment
In this section, we lay out the procedures for graph estimation and uncertainty assessment.
Throughout our analysis, we use {β
∗
i
, f ∗
i
}i∈[d]
to denote the true parameters, and E(·) to
denote the expectation with respect to the joint density in (3) with the true parameters. We
first introduce a pseudo-likelihood loss function for the parametric components {βj}
d
j=1 that
is invariant to the nuisance parameters {fj}j∈[d]
. Based on such a loss function, we present
an Adaptive Multi-stage Convex Relaxation algorithm to estimate each β
∗
j
by minimizing
the loss function regularized by a nonconvex penalty function. We then proceed to introduce
the inferential procedure for accessing the uncertainty of a given edge in the graph.
3.1. A Nuisance-Free Loss Function
For graph estimation, we treat βj as the parameter of interest and the base measures fj (·)
as nuisance parameter. Let X1, . . . , Xn be n i.i.d. copies of X. Due to the presence of
fj (·), finding the conditional maximum likelihood estimator of βj is intractable. To solve
this problem, we exploit a pseudo-likelihood loss function proposed in Ning et al. (2017b)
that is invariant to the nuisance parameters {fj}j∈[d]
. This pseudo-likelihood loss is based
on pairwise local order statistics, which have been previously studied in Liang and Qin
(2000); Diao et al. (2012); Chan (2012) for semiparametric regression models. More details
are presented as follows.
Let x1, x2, . . . , xn be n data points that are realizations of X1, X2, . . . , Xn. For any
1 ≤ i < i0 ≤ n, let
A
j
ii0
:=

(Xij , Xi
0j ) = (xij , xi
0j ), Xi\j = xi\j
, Xi
0\j = xi
0\j
	
be the event that we observe Xi\j = xi\j and Xi
0\j = xi
0\j and the order statistics of Xij and
Xi
0j (but not the relative ranks of Xij and Xi
0j ). More specifically, we denote max{Xij , Xi
0j}
and min{Xij , Xi
0j} by O1 and O2, and let o1 and o2 be the observed values of O1 and O2.
Then A
j
ii0 can be equivalently written as {O1 = o1, O2 = o2, Xi\j = xi\j
, Xi
0\j = xi
0\j}.
     
On Semiparametric Exponential Family Graphical Models
Let R ∈ {(1, 2),(2, 1)} be the relative rank of Xij and Xi
0j
, and r be the observed value.
Then, by definition, we have
P

Xij = xij , Xi
0j = xi
0j

 Xi\j = xi\j
, Xi
0\j = xi
0\j

= P

O1 = o1, O2 = o2

 Xi\j = xi\j
, Xi
0\j = xi
0\j

· P

R = r

 A
j
ii0

.
Furthermore, we have
P

R = r

 A
j
ii0

=
"
1 +
P(Xij = xi
0j
, Xi
0j = xij

 A
j
ii0)
P(Xij = xij , Xi
0j = xi
0j

 A
j
ii0)
#−1
=
"
1 +
P(Xij = xi
0j
, Xi
0j = xij

 Xi\j = xi\j
, Xi
0\j = xi
0\j
)
P(Xij = xij , Xi
0j = xi
0j

 Xi\j = xi\j
, Xi
0\j = xi
0\j
)
#−1
=

1 + R
j
ii0(βj )
−1
,
(5)
where R
j
ii0(βj ) := exp[−(xij − xi
0j )βj
T
(xi\j − xi
0\j
)]. Based on the conditional likelihood
in (5), we construct the following pseudo-likelihood loss function for βj :
Lj (βj ) := 2
n(n − 1)
X
1≤i<i0≤n
log
1 + R
j
ii0(βj )

. (6)
Obviously, Lj (·) only involves βj . Since its form resembles the logistic loss, to find a
minimizer of this loss function, we could readily apply any logistic regression solver.
3.2. Adaptive Multi-stage Convex Relaxation Algorithm
Now we are ready to present the algorithm for parameter estimation. For high dimensional
sparse estimation, to promote sparsity, we minimize the sum of the loss functions Lj (βj ) and
some penalty function. Two of the most prevalent methods are the LASSO (`1-penalization)
(Tibshirani, 1996) and the folded concave penalization (Fan et al., 2014). Although the `1-
penalization enjoys good computational properties as a convex optimization problem, it is
known to incur significant estimation bias for parameters with large absolute values (Zhang
and Huang, 2008). In contrast, nonconvex penalties such as smoothly clipped absolute
deviation (SCAD) penalty, minimax concave penalty (MCP) and capped-`1 penalty can
eliminate such bias and attain improved rates of convergence. Therefore, we consider the
nonconvex optimization problem
βbj = argmin
Rd−1

Lj (βj ) +X
k6=j
pλ(|βjk|)

, (7)
where λ > 0 is a regularization parameter and pλ(·) : [0, +∞) → [0, +∞) is a penalty
function satisfying the following three conditions:
(C.1) The penalty function pλ(u) is continuously nondecreasing and concave with pλ(0) = 0.
(C.2) The right-hand derivative at u = 0 satisfies p
0
λ
(0) = p
0
λ
(0+) = λ        
Yang, Ning, and Liu
(C.3) There exist constants c1 ∈ [0, 1] and c2 ∈ (0, +∞) such that p
0
λ
(u+) ≥ c1λ for u ∈
[0, c2λ].
Note that we only require the penalty function to be right-differentiable. In what follows, we
denote by p
0
λ
(u) the right-hand derivative. By (C.1), p
0
λ
(u) is nonincreasing and nonnegative
in [0, ∞). It is easy to verify that SCAD, MCP and capped-`1 penalty all satisfy (C.1)–(C.3).
Due to the penalty term, the optimization problem in (7) is nonconvex and may have
multiple local solutions. To overcome such difficulty, we exploit the local linear approximation algorithm (Zou and Li, 2008; Fan et al., 2014) or equivalently, the multi-stage convex
relaxation (Zhang, 2010; Zhang et al., 2013; Fan et al., 2018) to attain an estimator of
β
∗
j
. Compared with previous works that mainly focus on sparse linear regression, our loss
function Lj (βj ) is a U-statistics based logistic loss, which requires nontrivial extensions of
the existing theoretical analysis.
We present the proposed adaptive multi-stage convex relaxation method in Algorithm 1.
Our algorithm solves a sequence of convex optimization problems corresponding to finer and
finer convex relaxations of the original nonconvex optimization problem. More specifically,
for each j = 1, . . . , d, in the first iteration, step 4 of Algorithm 1 is equivalent to a `1-
regularized optimization problem and we obtain the first-step solution βb(1)
j
. Then, in each
subsequent iteration, we solve an adaptive `1-regularized optimization problem where the
weights of the penalty depend on the solution of the previous step. For example, in the
`-th iteration, the regularization parameter λ
(`−1)
jk in (8) is updated using the (` − 1)-th
step estimator βb(`−1)
j
. Note that p
0
λ

|β
(`)
jk |

is the right-hand derivative of pλ(u) evaluated
at u = |β
(`)
jk |.
Since the optimization problem in step 4 is convex, our method is computationally
efficient. Besides, note that (8) with ` = 1 corresponds to the `1-regularized problem.
Hence, our approach can be viewed as a refinement of LASSO. As we will show in §4.1,
the estimator βbj of β
∗
j
constructed by Algorithm 1 attains the optimal statistical rates of
convergence for parameter estimation.
Algorithm 1 Adaptive Multi-stage Convex Relaxation algorithm for parameter estimation
1: Initialize λ
(0)
jk = λ for 1 ≤ j, k ≤ d.
2: for j= 1,2,. . . ,d do
3: for ` = 1, 2, . . . , until convergence do
4: Solve the convex optimization problem
βb(`)
j = argmin
Rd−1
n
Lj (βj ) +X
k6=j
λ
(`−1)
jk |βjk|
o
. (8)
5: Update λ
(`)
jk by λ
(`)
jk = p
0
λ
(|βb(`)
jk |) for 1 ≤ k ≤ d, k 6= j.
6: end for
7: Output βbj =βb(`)
j
, where ` is the number of iterations until convergence is attained.
8: end for
 
On Semiparametric Exponential Family Graphical Models
3.3. Graph Inference: Composite Pairwise Score Test
For any given 1 ≤ j < k ≤ d, we are interested in testing if (j, k) ∈ E, i.e., we consider
the hypothesis testing problem H0 : β
∗
jk = 0 versus H1 : β
∗
jk 6= 0. To simplify the notation,
we write βj\k = (βj1, . . . , βjj−1, βjj+1, . . . , βjk−1, βjk+1, . . . , βjd)
T ∈ R
d−2 and denote the
parameters associated with node j and node k by βj∨k :=

βjk; β
T
j\k
, β
T
k\j
T
∈ R
2d−3
. In
addition, let Hj
:= E

∇2Lj (β
∗
j
)

be the expected Hessian of Lj (βj ) evaluated at β
∗
j
. We
define two submatrices H
j
jk,j\k
and H
j
j\k,j\k
of Hj as
H
j
jk,j\k
:=
"
E
∂
2Lj (β
∗
j
)
∂βjk∂βjv #
v6=k
∈ R
d−2
and H
j
j\k,j\k
:=
"
E
∂
2Lj (β
∗
j
)
∂βju∂βjv #
u6=k,v6=k
∈ R
(d−2)×(d−2)
,
and we define Hk
jk,k\j
and Hk
k\j,k\j
similarly. Furthermore, we define
w∗
j,k = H
j
jk,j\k

H
j
j\k,j\k
−1
and w∗
k,j = Hk
jk,k\j

Hk
k\j,k\j
−1
. (9)
Following the general approach in Ning et al. (2017a); Neykov et al. (2018), the composite
pairwise score function for parameter βjk is defined as
Sjk(βj∨k) = ∇jkLj (βj ) + ∇jkLk(βk) − w∗
j,k
T ∇j\kLj (βj ) − w∗
k,j
T ∇k\jLk(βk). (10)
where we write ∇jkLj (βj ) = ∂Lj (βj )/∂βjk and ∇j\kLj (βj ) = ∂Lj (βj )/∂βj\k
. Here, the
last two terms in (10) are constructed to reduce the effect of nuisance parameters βj\k and
βk\j on assessing the uncertainty of β
∗
jk, which is the parameter of interest. A key feature
of Sjk(βj∨k) is that the symmetry of βjk and βkj (i.e., βjk = βkj ) is taken into account,
which is distinct from the existing works such as Ren et al. (2015); Jankov´a and van de
Geer (2015); Liu et al. (2013) for Gaussian graphical models and Ning et al. (2017b) in the
regression setup.
Note that both w∗
j,k and w∗
k,j are computed from H, which is unknown. We estimate
them using the Dantzig-type estimators (Cand´es et al., 2007). Specifically, we define the
empirical versions of H
j
jk,j\k
and H
j
j\k,j\k
as
∇2
jk,j\kLj (βj ) = "
∂
2Lj (βj )
∂βjk∂βjv #
v6=k
and ∇2
j\k,j\kLj (βj ) = "
∂
2Lj (βj )
∂βju∂βjv #
u6=k,v6=k
.
We also define ∇2
jk,k\j
Lk(βk) and ∇2
k\j,k\j
Lk(βk) similarly. Then we estimate w∗
j,k by
solving
wbj,k = argmin kwk1 such that

∇2
jk,j\kLj (0, βb
j\k
) − wT ∇2
j\k,j\kLj (0, βb
j\k
)


∞
≤ λD,
(11)
where βbj is the estimator of β
∗
j
obtained from Algorithm 1 and λD is a regularization
parameter. An estimator wbk,j of w∗
k,j can be similarly obtained. Based on wbj,k and wbk,j ,
we construct the composite pairwise score statistic for β
∗
jk by
Sbjk = ∇jkLj (0, βb
j\k
) + ∇jkLk

0, βb
k\j

− wb
T
j,k∇j\kLj

0, βb
j\k

− wb
T
k,j∇k\jLk

0, βb
k\j

.
(12          
Yang, Ning, and Liu
Comparing (10) and (12), we see that Sbjk is obtained by replacing βj and βk in (10) by
(0, βb
j\k
) and (0, βb
k\j
) respectively and replacing w∗
j,k and w∗
k,j in (10) by wbj,k and wbk,j .
To obtain a valid hypothesis test, we need to establish the limiting distribution of Sbjk
under the null hypothesis. Note that Sbjk is a linear combination of entries of ∇Lj (βj ) and
∇Lk(βk), both of which are U-statistics. In the next section, we prove the asymptotic normality of Sbjk. More specifically, under the null hypothesis, we have √
nSbjk/2 N(0, σ2
jk),
where the limiting variance can be estimated consistently by σb
2
jk (More details will be explained in the following section). With a significance level α ∈ (0, 1), the test function
ψjk(α) is defined as
ψjk(α) = (
1 if


√
nSbjk
(2σbjk)

 > Φ
−1
(1 − α/2)
0 if


√
nSbjk
(2σbjk)

 ≤ Φ
−1
(1 − α/2)
, (13)
where Φ(t) is the cumulative distribution function of a standard normal random variable.
In sum, the composite pairwise score test for the null hypothesis H0 : β
∗
jk = 0 consists
of the following four steps: (i) Calculate βbj and βb
k from Algorithm 1; (ii) Obtain wbj,k
and wbk,j by solving two Dantzig-type problems defined in (11); (iii) Compute the limiting
variance σb
2
jk; (iv) Evaluate the test function (13).
4. Theoretical Properties
In this section, we present our theoretical results. We first prove that the proposed procedure
attains the optimal rate of convergence for parameter estimation. Then, we provide theory
for the composite pairwise score test.
4.1. Theoretical Results for Parameter Estimation
We first establish the rates of convergence of the adaptive multi-stage convex relaxation
estimator. We begin by listing several required assumptions. The first is about moment
conditions of {Xj} and the local smoothness of the log-partition function A(·) defined in
(4). This assumption also appears in Yang et al. (2013a) and Chen et al. (2015) as a pivotal
technical condition for theoretical analysis.
Assumption 2 For all j ∈ [d], we assume that the first two moments of Xj are bounded.
That is, there exist two constants κm and κv such that |E(Xj )| ≤ κm and E(X2
j
) ≤ κv.
Denote the true parameters by {β
∗
j
, f ∗
j
}j∈[d] and define d univariate functions A¯
j (·): R → R
as
A¯
j (u) := logZ
Rd
exp
uxj +
X
k<`
β
∗
k`xkx` +
X
d
i=1
f
∗
i
(xi)

dν(x)

, j ∈ [d].
We assume that there exists a constant κh such that maxu: |u|≤1 A¯00
j
(u) ≤ κh for all j ∈ [d].
Unlike the Ising graphical models, {Xj}j∈[d] are not bounded in general for semiparametric exponential family graphical models. Instead, we impose mild conditions as in
10
On Semiparametric Exponential Family Graphical Models
Assumption 2 to obtain a loose control of the tail behaviors of the distribution of X. As
shown in Yang et al. (2013a), Assumption 2 implies that for all j ∈ [d],
max
log E[exp(Xj )], log E[exp(−Xj )]	
≤ κm + κh/2.
Markov inequality implies for any x > 0,
P

|Xj | ≥ x

≤ 2 exp(κm + κh/2) · exp(−x). (14)
Thus, by setting x = C log d in (14) with constant C sufficiently large, we have kXk∞ ≤
C log d with high probability. In addition to Assumption 2, we also impose conditions to
control the curvature of function Lj (·).
Definition 3 (Sparse eigenvalue condition) For any j, s ∈ [d], we define the s-sparse
eigenvalues of E[∇2Lj (β
∗
j
)] as
ρ
∗
j+(s) := sup
v∈Rd−1

v
TE

∇2Lj (β
∗
j
)

v: kvk0 ≤ s, kvk2 = 1	
;
ρ
∗
j−(s) := inf
v∈Rd−1

v
TE

∇2Lj (β
∗
j
)

v: kvk0 ≤ s, kvk2 = 1	
.
Assumption 4 Let s
∗ = maxj∈[d] kβ
∗
j
k0. We assume that for any j ∈ [d], there exist an
integer k
∗ ≥ 2s
∗
satisfying limn→∞
k
∗
(log9
d/n)
1/2 = 0 and a positive number ρ∗ such that the
sparse eigenvalues of E[∇2Lj (β
∗
j
)] satisfy
0 < ρ∗ ≤ ρ
∗
j−(2s
∗+ 2k
∗
) < ρ∗
j+(k
∗
) < +∞ and
ρ
∗
j+(k
∗
)

ρ
∗
j−(2s
∗+ 2k
∗
) ≤ 1 + 0.2k
∗
/s∗
for any j ∈ [d].
The condition ρ
∗
j+(k
∗
)

ρ
∗
j−(2s
∗+2k
∗
) ≤ 1+0.2k
∗/s∗
requires the eigenvalue ratio ρ
∗
j+(k)/ρ∗
j−(2k+
2s
∗
) to grow sub-linearly in k. Assumption 4 is commonly referred to as sparse eigenvalue
condition, which is standard for sparse estimation problems and has been studied by Bickel
et al. (2009); Raskutti et al. (2010); Zhang (2010); Negahban et al. (2012); Xiao and Zhang
(2013); Loh and Wainwright (2015) and Wang et al. (2014). Our assumption is similar to
that in Zhang (2010) and is weaker than the restricted isometry property (RIP) proposed
in Cand´es and Tao (2005). We claim that this assumption is true in general and will be
verified for Gaussian graphical models in the appendix.
Now we are ready to present the main theorem of this section. Recall that the penalty
function pλ(u) satisfies conditions (C.1)–(C.3) in §3.2. We use p
0
λ
(u) to denote its right-hand
derivative. For convenience, we will set p
0
λ
(u) = 1 when u < 0.
Theorem 5 (`2- and `1-rates of convergence) For all j ∈ [d], we define the support of
β
∗
j
as Sj
:=

(j, k): β
∗
jk 6= 0, k ∈ [d]
	
and let s
∗ = maxj∈[d] kβ
∗
j
k0. Let ρ∗ > 0 be defined in
Assumption 4. Under Assumptions 2 and 4, there exists an absolute constant K > 0 such
that k∇Lj (β
∗
j
)k∞ ≤ K
p
log d/n, ∀j ∈ [d] with probability at least 1 − (2d)
−1
. Moreover,
the penalty function pλ(·) in (7) satisfies (C.1)–(C.3) listed in §3.2 with c1 = 0.91 and
c2 ≥ 24/ρ∗ for condition (C.3). We set the regulization parameter λ = C
p
log d/n with
C ≥ 25K. We denote constants % = c2(c2ρ∗ −11)−1
, A1 = 22%, A2 = 2.2c2, B1 = 32%,
1     
Yang, Ning, and Liu
B2 = 3.2c2, γ = 11c
−1
2
ρ
−1
∗ < 1, and define Υj
:= [P
(j,k)∈Sj
p
0
λ
(|β
∗
jk|−c2λ)
2
]
1/2
. Then, with
probability at least 1−d
−1
, we have the following statistical rates of convergence:

βb(`)
j − β
∗
j


2
≤ A1

∇SjLj (β
∗
j
)


2
+ Υj

+ A2
√
s
∗λγ`
and (15)

βb(`)
j − β
∗
j


1
≤ B1
√
s
∗

∇SjLj (β
∗
j
)


2
+ Υj

+ B2s
∗λγ`
, ∀j ∈ [d]. (16)
By Theorem 5, the statistical rates are dominated by the second term if p
0
λ
(|β
∗
jk|−c2λ)
is not negligible. If the signal strength is large enough such that p
0
λ
(β −c2λ) = 0 where
β =min(j,k)∈Sj
|β
∗
jk|, after sufficient number of iterations, the statistical rates will be of the
order

βb(`)
j − β
∗
j


2
= OP

∇SjLj (β
∗
j
)


2

and

βb(`)
j − β
∗
j


1
= OP
√
s
∗

∇SjLj (β
∗
j
)


2

.
However, if the signals are uniformly small such that p
0
λ

|β
∗
jk|−c2λ

> 0 for all (j, k) ∈ Sj ,
the rates of convergence will be of the order

βb(`)
j − β
∗
j


2
= OP
√
s
∗λ

and

βb(`)
j − β
∗
j


1
= OP

s
∗λ

,
which are identical to the `2- and `1-rates of the LASSO estimator, respectively (Ning
et al., 2017b). Thus c2λ can be viewed as the threshold of signal strength. Therefore, after
sufficient numbers of iterations, the final estimator βbj obtained by Algorithm 1 attains the
following more refined rates of convergence:

βbj −β
∗
j


2
= OP

∇SjLj (β
∗
j
)


2
+Υj

and

βbj −β
∗
j


1
= OP
√
s
∗

∇SjLj (β
∗
j
)


2
+Υj


.
These statistical rates of convergence are optimal in the sense that they cannot be improved
in terms of the order.
Finally, we comment that the sparsity level s
∗
in (15) and (16) can be replaced by
the sparsity level of each β
∗
j
. Let s
∗
j = kβ
∗
j
k0 be the sparsity level of β
∗
j
and λj be the
regularization parameter for optimization problem (7) such that λj  k∇Lj (β
∗
j
)k∞. The
statistical rates of convergence for each βb(`)
j
can be improved to

βb(`)
j − β
∗
j


2
= OP
q
s
∗
j
λj

and

βb(`)
j − β
∗
j


1
= OP

s
∗
jλj

.
We use the uniform sparsity level s
∗ =maxj∈[d] s
∗
j
and the same regularization parameter λ
for simplicity, but the proof can be easily adapted to individual s
∗
j
and λj for each j ∈[d].
4.2. Theoretical Results for Composite Pairwise Score Test
In the composite pairwise score test for the null hypothesis H0 : β
∗
jk = 0, we construct the
test statistic by combining the loss functions Lj (·) and Lk(·) together because βjk appears
in both Lj (βj ) and Lk(βk) (recall that we use βjk and βkj interchangeably). In the sequel,
we present the theoretical results that guarantee the validity of the proposed inferential
method.
Recall that we define the pairwise score function Sjk(βj∨k) and the pairwise score statistic Sbjk in (10) and (12) respectively. According to a fixed pair of nodes (j, k), entries             
On Semiparametric Exponential Family Graphical Models
βj and βk can be categorized into three types: (i) βjk, (ii) βj\k = (βj`; ` 6= k)
T
, and (iii)
βk\j = (βk`; ` 6= j)
T
. Recall that we write βj∨k = (βjk, β
T
j\k
, β
T
k\j
)
T
for notational simplicity.
Moreover, letting Ljk
βj∨k

:= Lj (βj ) + Lk(βj ), the entries of ∇Ljk
βj∨k

are given by
∇jkLjk(βj∨k) = ∇jkLj (βj ) + ∇kjLk(βk); ∇j\kLjk(βj∨k) = ∇j\kLj (βj ), and
∇k\jLjk(βj∨k) = ∇k\jLk(βk).
Let βbj and βb
k be the estimators of β
∗
j
and β
∗
k
obtained from Algorithm 1. Note that we
can write the pairwise score function Sjk(·) and the test statistic Sbjk as
Sjk(βj∨k) = ∇jkLjk
βj∨k

− w∗
j,k
T ∇j\kLjk
βj∨k

− w∗
k,j
T ∇k\jLjk
βj∨k

and (17)
Sbjk = ∇jkLjk
βb0
j∨k

− wb
T
j,k∇j\kLjk
βb0
j∨k

− wb
T
k,j∇k\jLjk
βb0
j∨k

, (18)
where we write βb0
j∨k
:=

0, βbT
j\k
, βbT
k\j
)
T
, w∗
j,k and w∗
k,j are defined in (9), wbj,k is obtained
from the Dantzig-type problem in (11), and wbk,j can be obtained similarly. To derive
the asymptotic distribution of Sbjk under the null hypothesis, we first show that √
n

Sbjk −
Sjk(β
∗
j∨k
)

= oP(1). Then the problem is reduced to finding the limiting distribution of
Sjk(β
∗
j∨k
) under H0. Thanks to its structure of being a U-statistics, we can characterize
the limiting distribution of Sjk(β
∗
j∨k
) using the method of H´ajek projection (Van der Vaart,
2000), which approximates a U-statistic with a sum of independent random variables.
To begin with, we denote the kernel functions of ∇Lj (βj ), ∇Lk(βk) and ∇Ljk(βj∨k) as
h
j
ii0(βj ), h
k
ii0(βk) and h
jk
ii0(βj∨k) respectively. It can be shown that E[h
j
ii0(β
∗
j
)] = E[h
k
ii0(β
∗
k
)] =
0; hence h
jk
ii0(β
∗
j∨k
) is also centered. We define
gjk(Xi) := n/2 · E

∇Ljk
β
∗
j∨k

Xi

= E

h
jk
ii0

β
∗
j∨k

Xi

and (19)
Ujk :=
2
n
Xn
i=1
gjk(Xi) = Xn
i=1
E

∇Ljk
β
∗
j∨k

Xi

. (20)
Thus 2/n · gjk
Xi

is the projection of ∇Ljk
β
∗
j∨k

onto the σ-filed generated by Xi and
Ujk is the H´ajek projection of ∇Ljk
β
∗
j∨k

. Under mild conditions, Ujk in (20) is a good
approximation of ∇Ljk
β
∗
j∨k

, which enables us to characterize the limiting distribution
of Sjk(β
∗
j∨k
). We present the following assumption that guarantees the non-degeneracy of
gjk
Xi

.
Assumption 6 Under Assumption 2, for gjk(Xi) defined in (19), we denote the covariance
matrix of gjk(Xi) as Σjk := E[gjk(Xi)gjk(Xi)
T
]. We assume that there exists a constant
cΣ > 0 such that λmin(Σjk) ≥ cΣ for all 1≤j < k≤d.
Assumption 6 requires the minimum eigenvalue of Σjk to be bounded away from 0, which
implies Var(v
T Ujk) ≥ 4cΣ for all v ∈ R
2d−3 with kvk2 = 1. Thus, this assumption guarantees the asymptotic variance of √
nSjk(β
∗
j∨k
) is bounded away from 0. We also present the
following assumption that specifies the scaling of the Dantzig selector pro                         
Yang, Ning, and Liu
Assumption 7 We assume that Hj
is invertible for all j ∈ [d]. In addition, we assume that
there exist an integer s
?
0
and a positive number w0 such that kw∗
j,kk0 ≤ s
?
0 −1 and kw∗
j,kk1 ≤
w0. Besides, the regularization parameter λD in (11) satisfies λD  max{1, w0}s
∗λ log2
d.
Moreover, we assume that
limn→∞
(1+w0+w
2
0
)s
∗λ log2
d = 0, limn→∞
(1+w0)s
?
0λD = 0, and limn→∞
√
n(s
∗+s
?
0
)λλD = 0.
(21)
In addition, recall that we denote the s-sparse eigenvalues of E[∇2Lj (β
∗
j
)] by ρ
∗
j−(s) and
ρ
∗
j+(s). We further assume that there exist an integer k
?
0 ≥ s
?
0
and a positive number ν∗
such that
limn→∞
k
?
0

log9
d/n1/2 = 0, 0 < ν∗ ≤ ρ
∗
j−(s
?
0 + k
?
0
) < ρ∗
j+(k
?
0
) ≤

1 + 0.5k
?
0/s?
0

ν∗, 1 ≤ j ≤ d.
If we can treat w0 as a constant, and k
∗ and k
?
0
is of the same order of s
∗ and s
?
0
,
respectively, Assumption 7 is reduced to λD  s
∗λ log2
d, s?
0λD = o(1), s
∗λ log2
d = o(1),
and (s
∗+s
?
)λλD = o(n
1/2
). Since λ 
p
log d/n, we can choose λD = Cs∗
(log5
d/n)
1/2 with
a sufficiently large C, provided (s
∗ + s
?
0
)(log9
d/n)
1/2 = o(1), s
?
0
s
∗
(log5
d/n)
1/2 = o(1), and
(s
∗ + s
?
0
)s
∗
log3
d/n = o(n
−1/2
). Hence this condition is fulfilled if
log d = o

min
(
√
n/s∗
2/9
,(
√
n/s?
0
)
2/9
,(
√
n/s∗2
)
1/3
,(
√
n/s∗
s
?
)
1/3
	

.
Now we are ready to present the main theorem of composite pairwise score test.
Theorem 8 Under the Assumptions 2, 4, 6 and 7, it holds uniformly for all j 6= k and
j, k ∈ [d] that √
nSbjk =
√
nSjk
β
∗
j∨k

+ oP(1). Furthermore, we let βb0
j∨k = (0, βbT
j\k
, βbT
k\j
)
T
and define Σbjk := n
−1 Pn
i=1
(n−1)−1 P
i
06=i h
jk
ii0(βb0
j∨k
)
	⊗2
, where h
jk
ii0

βj∨k

is the kernel
function of the second-order U-statistic ∇Ljk(βj∨k). In addition, we define σbjk by
σb
2
jk := Σbjk
jk,jk − 2Σbjk
jk,j\kwbj,k − 2Σbjk
jk,k\jwbk,j + wb
T
j,kΣbjk
j\k,j\kwbj,k + wb
T
k,jΣbjk
k\j,k\jwbk,j .
Then, under the null hypothesis H0 : β
∗
jk = 0, we have √
nSbjk
(2σbjk) N(0, 1).
By Theorem 8, to test the null hypothesis H0 : β
∗
jk = 0 against the alternative hypothesis H1 : β
∗
jk 6= 0, we reject H0 if the studentized test statistic √
nSbjk
(2σbjk) is too
extreme. Recall that the test function of the composite pairwise score test with significance level α is deboted by ψjk(α) in (13). The associated p-value is defined as p
jk
ψ
:=
2

1 − Φ


√
nSbjk
(2σbjk)


. By Theorem 8, under H0, we have
limn→∞
P

ψjk(α) = 1 | H0

= α and p
jk
ψ Unif[0, 1] under H0,
where Unif[0, 1] is the uniform distribution over [0, 1].
We note that our inferential approach is still valid if we replace βb0
j∨k
in (18) by other
estimators of β
∗
j∨k
, provided such an estimator converges to β
∗
j∨k
at an appropriate statistical rate. Our theory still holds after simple modification on the proof when controlling
the order of the remainder term        
On Semiparametric Exponential Family Graphical Models
Remark 9 There are a number of recent works on the uncertainty assessment for high dimensional linear models or generalized linear models with `1-penalty; see Lee et al. (2016);
Lockhart et al. (2014); Belloni et al. (2012, 2013); Zhang and Zhang (2014); Javanmard
and Montanari (2014); van de Geer et al. (2014). These works utilize the convexity and
the Karush-Kuhn-Tuker conditions of the LASSO problem. Compared with these works,
our pairwise score test is constructed using a nonconvex penalty function and is applicable
to a larger model class. Ning et al. (2017b) consider the score test for `1-penalized semiparametric generalized linear models in the regression setting. Compared with this work, we
adopt a composite score test with a nonconvex penalty and relax many technical assumptions including the bounded covariate assumption. For nonconvex penalizations, Fan and Lv
(2011); Bradic et al. (2011) establish the asymptotic normality for the low dimensional and
nonzero parameters in high dimensional models based on the oracle properties. However,
their approach depends on the minimal signal strength assumption, which is not needed in
our approach.
5. Numerical Results
In this section we study the finite-sample performance of the proposed graph inference
methods on both simulated and real-world datasets.
5.1. Simulation Studies
We first examine the numerical performance of the proposed pairwise score tests for the
null hypothesis H0 : β
∗
jk = 0. We simulate data from the following three settings:
(i) Gaussian graphical model. We set n = 100 and d = 200. The graph structure is
a 4-nearest-neighbor graph, that is, for j, k ∈ [d], j 6= k, node j is connected with
node k if |j − k| = 1, 2, d − 2, d − 1. More specifically, we sample X1, . . . , Xn from a
Gaussian distribution Nd(0, Σ). For the precision matrix Θ = Σ−1
, we set Θjj = 1,

Θjk

 = µ ∈ [0, 0.25) for |j − k| = 1, 2, d−2, d−1 and Θjk = 0 for 2 ≤ |j − k|≤ d − 2.
Note that µ denotes the signal strength of the graph inference problem and µ ≤ 0.25
ensures that Θ is diagonal dominant and invertible.
(ii) Ising graphical model. We set n = 100 and d = 200. The graph structure is a
10 × 20 grid with the sparsity level s
∗ = 4. We use Markov Chain Monte Carlo
method (MCMC) to simulate n data from an Ising model with joint distribution
p(x) ∝ expP
j6=k
β
∗
jkxjxk

(using the package IsingSampler (Epskamp, 2015)). We
set |β
∗
jk| = µ ∈ [0, 1] if there exists an edge connecting node j and node k, and β
∗
jk = 0
otherwise.
(iii) Mixed graphical model. We set n = 100 and d = 200. The graph structure is a
10×10×2 grid with the sparsity level s
∗ = 5. We set the nodes in the first layer to
be binomial and nodes in the second layer to be Gaussian. We set |β
∗
jk| = µ ∈ [0, 1] if
there exists an edge connecting node j and node k, and β
∗
jk = 0 otherwise. We refer
to Lee and Hastie (2015) for details.
We denote the true parameters of the graphical models as {β
∗
jk, j 6= k}. We also denote
β
∗
j = (β
∗
j1
, . . . , β∗
jd)
T
. For the Gaussian graphical model, we have β
∗
jk = Θjk. We first
1 
Yang, Ning, and Liu
obtain a point estimate of β
∗
j
by solving (7) using Algorithm 1 with the capped-`1 penalty
pλ(u) = λ min{u, λ}. The parameter λ is chosen by 10-fold cross validation as suggested by
Ning et al. (2017b).
Recall that the form of the loss function Lj (βj ) is exactly the loss function for logistic regression, where we use Rademacher random variables yii0 as response and yii0(xij −
xi
0j )β
T
j
(xi\j − xi
0\j
) as covariates, Algorithm 1 can be easily implemented by using the
`1-regularized logistic regression such as the PICASSO package (Ge et al., 2017). In particular, the algorithm converges quickly after a few iterations, indicating that it attains a good
balance between computational efficiency and statistical accuracy. Once βbj is obtained, we
solve the Dantzig-type problem (11) using βbj as input. We set the regularization parameter
λD to be 1. In practice, the performance of the proposed method is not very sensitive to
the choice of λD.
To examine the performance of our semiparametric modeling approach, we compare
the pairwise score test with the desparsity method in van de Geer et al. (2014). Although
this method is proposed for hypothesis tests in generalized linear models (GLMs), it can
be adapted for graphical models by performing nodewise regression, assuming the base
measures {fj}j∈[d] are correctly specified. When testing H0 : β
∗
jk = 0 with j < k, we apply
the desparsity method with Xj and X\j being the response and covariates, respectively.
Furthermore, to show that combining both Lj (βj ) and Lk(βk) is beneficial for inferring
β
∗
jk, we also compare our method with the asymmetric score test, which constructs a score
test statistic similar to that in (12) based solely on Lj (βj ).
To examine the validity of our method, we test H0 : β
∗
jk = 0 versus H1 : β
∗
jk 6= 0 for
all (j, k). Recall that β
∗
jk = µ when there is an edge. Here, we let µ increase from 0 to a
sufficiently large number. We calculate the type I errors and powers as
Type I error = the number of rejected hypotheses when there is no edge
d(d − 1)/2 − the total number of edges ,
Power = the number of rejected hypotheses when there is an edge
the total number of edges .
We report the type I errors and powers of the hypothesis tests at the 0.05 significance level
in Figure 1 and Figure 2, respectively. The simulation is repeated 100 times. As revealed
in Figure 1, both the asymmetric and the pairwise score test achieve accurate type I errors,
which is comparable to the desparsity method. Moreover, in terms of the power of the test,
in Figure 2, the two score tests based on the loss function defined in (6) are less powerful
than the desparsity method, which shows the loss of efficiency by only considering the
relative rank. However, as shown in Figure 2-(b) and (c), the two score tests are nearly as
powerful as the desparsity method in the Ising and mixed graphical models. In addition, we
emphasize that for mixed graphical models the desparsity method needs to know the type
(or distribution) of each nodes as a priori. Such phenomenon suggests that we may sacrifice
little efficiency for model generality/robustness. Furthermore, comparing the performances
of these two score tests, we see that the pairwise score test achieves uniformly higher power
than the asymmetric one, which perfectly illustrates that taking into consideration of the
symmetry of β
∗
jk and β
∗
kj may improve the inference accuracy.
16
On Semiparametric Exponential Family Graphical Models
0.05 0.10 0.15 0.20
0.02 0.03 0.04 0.05 0.06
● Asymmetric Score
Pairwise Score
Desparsity
µ
type−I errors
0.05 0.10 0.15 0.20 0.25
0.02 0.03 0.04 0.05 0.06
● Asymmetric Score
Pairwise Score
Desparsity
µ
type−I errors
0.05 0.10 0.15 0.20
0.02 0.03 0.04 0.05 0.06
● Asymmetric Score
Pairwise Score
Desparsity
µ
type−I errors
(a). Gaussian graphical model. (b). Ising model. (c). Mixed graphical model.
Figure 1: Type-I errors of the composite pairwise score test, asymmetric score test, and the
desparsity method for the three graphical models at the 0.05 significance level.
These figures are based on 100 independent simulations.
0.05 0.10 0.15 0.20
0.0 0.1 0.2 0.3 0.4 0.5 0.6
● Asymmetric Score
Pairwise Score
Desparsity
µ
powers
●
●
●
●
●
●
●
●
●
0.05 0.10 0.15 0.20 0.25
0.0 0.2 0.4 0.6 0.8
● Asymmetric Score
Pairwise Score
Desparsity
µ
powers
●
●
●
●
●
●
●
0.05 0.10 0.15 0.20
0.00 0.05 0.10 0.15 0.20 0.25 0.30
● Asymmetric Score
Pairwise Score
Desparsity
µ
powers
(a). Gaussian graphical model. (b). Ising model. (c). Mixed graphical model.
Figure 2: Powers of the composite pairwise score test, asymmetric score test, and the
desparsity method for the three graphical models at the 0.05 significance level.
These figures are based on 100 independent simulations.
5.2. Real Data Analysis
We then apply the proposed methods to analyze a publicly available dataset named Computer
Audition Lab 500-Song (CAL500) dataset (Turnbull et al., 2008). The data can be obtained from the Mulan database (Tsoumakas et al., 2011). The CAL500 dataset consists
of 502 popular music tracks each of which is annotated by at least three listeners. The
attributes of this dataset include two subsets: (i) continuous numerical features extracted
from the time series of the audio signal and (ii) discrete binary labels assigned by human
listeners to give semantic descriptions of the song. For each music track, short time Fourier
transform is implemented for a sequence of half-overlapping 23ms time windows over the
song’s digital audio file. This procedure generates four types of continuous features: spectral
centroids, spectral flux, zero crossings and a time series of Mel-frequency cepstral coefficient
17
Yang, Ning, and Liu
(MFCC). For the MFCC vectors, every consecutive 502 short time windows are grouped
together as a block window to produce the following four types of features: (i) overall mean
of MFCC vectors in each block window, (ii) mean of standard deviations of MFCC vectors in each block window, (iii) standard deviation of the means of MFCC vectors in each
block window, and (iv) standard deviation of the standard deviations of MFCC vectors
in each block window. More details on the feature extraction can be found in Tzanetakis
and Cook (2002). In addition to these continuous variables, binary variables in the CAL500
dataset include a 174-dimensional array indicating the existence of each annotation. These
174 annotations can be grouped into six categories: emotions (36 variables), instruments
(33), usages (15), genres (47), song characteristics (27) and vocal types (16). Our goal is
to infer the association between these different types of variables using graphical models.
This dataset has been analyzed in Cheng et al. (2017) where they exploit a nodewise groupLASSO regression to estimate the graph structure. In what follows, we use the proposed
pairwise score test to examine the graph structure.
Similar to Turnbull et al. (2008) and Cheng et al. (2017), we only keep the MFCC
features because they can be interpreted as the amplitude of the audio signal and the other
continuous features are not readily interpretable. Unlike Cheng et al. (2017), we keep all
the binary labels. Thus the processed dataset has n = 502 data points of dimension d = 226
with 52 continuous variables and 174 binary variables. We apply the pairwise score test to
each pair of variables to determine the presence of an edge between them. The p-values
for the null hypothesis that two variables are conditionally independence given the rest of
variables are calculated. We then apply the Bonferroni correction to control the familywise
error rate at 0.05. We set the nonconvex penalty function in optimization problem (7) to
be capped-`1 penalty pλ(u) = λ min{u, λ} with the regularization parameter λ selected by
10-fold cross-validation as in the previous section.
We compare the pairwise score test with the desparsity method and the asymmetric score
test, which are constructed in the same way as in the simulation. We present the fitted
graphs obtained by these three methods in Figure 3-(a)–(c), where we plot the connected
components and omit the singletons. Moreover, in Figure 3-(d), we plot the intersection of
these three graphs. To better display the graphical structure, we use a square to represent
each type of 13 MFCC features respectively. If a node is connected to any node within the
group of variables in a MFCC node, then we draw an edge. We use circles to represent the
binary variables and use different colors to indicate their categories. The obtained graphs
have some interesting properties. While all three tests create different graphs, the graphs
obtained by the pairwise score test and the asymmetric score test have more common edges,
which agrees with our simulation results. Indeed, our test can correct the inconsistency of
the asymmetric score test, in the sense that the asymmetric score tests for β
∗
jk = 0 and
β
∗
kj = 0 may yield different test results. To show this inconsistency problem, we also plot
the graph obtained by the asymmetric score test based on the loss function Lk(βk) in Figure
4 in the appendix. Comparing with Figure 3-(b), we can see that the asymmetric score test
indeed leads to many contradictory edges.
In Figure 3, both the pairwise score test and this asymmetric score test discover that
songs that are danceable (circle 92) are suited for parties (circle 93), but such a connection
is not found by the desparsity method. This is also true for the connection between the
rapping vocals (circle 119) and the rap genre (circle 48) and the edge between strong vocals
18
On Semiparametric Exponential Family Graphical Models
(circle 122) and songs with strong emotions (circle 19). Moreover, in all three graphs, the
continuous features are densely connected within themselves, which is similar to the results
in Cheng et al. (2017). All three tests find that the noisiness of the music (square 4) is
connected with the quality of songs (circle 85). Furthermore, the common edges connecting
two binary variables also display interesting patterns. For instance, we find that awakening
emotions (circle 6) are connected with soothing emotions (circle 8); laid-back emotions
(circle 14) are connected with songs with high energy (circle 32); sad emotions (circle 20)
are connected with songs with positive feelings (circle 84); songs with female lead vocals
(circle 62) are connected with those with male lead vocals (circle 66). In addition, songs
using drum sets (circle 59) are connected with the electronica genre (circle 46), which is
also connected with the acoustic texture (circle 88). All these edges have fairly intuitive
explanations.
In summary, the proposed method reveals some interesting associations between these
variables and can be used as a useful complement to analyze high dimensional datasets with
more complex distributions.
6. Conclusion
We propose an integrated framework for uncertainty assessment of a new semiparametric
exponential family graphical model. The novelty of our model is that the base measures of
each nodewise conditional distribution are treated as unknown nuisance functions. Towards
the goal of uncertainty assessment, we first adopt the adaptive multi-stage relaxation algorithm to perform the parameter estimation. Then we propose a composite pairwise score
test of the graph structure. Our method provides a rigorous justification for the uncertainty
assessment, and is further supported by extensive numerical results. In a followup paper
(Tan et al., 2016), the proposed model is further extended to account for the unobserved
latent variables in the graphical model.
Acknowledgments
The authors are grateful for the support of NSF CAREER Award DMS1454377, NSF
IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841.
19
Yang, Ning, and Liu
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
● ●
●
● ●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
● ●
●
●
●
1
2
3 4
5
6
8
9
10
11
12 13
14
15
16
17
18
19
20
21
22
23
27
40
41
46 48
50
51
52
53
54
55
56
57 58
59
60
61
62
66
68 71
73
78 80
81
82
83
84
85
86
88
89
90
91
92
93
96
99
113
119
122
123
124
125
128
132 134
136
138
139
●
●
●
●
●
●
mean of mean
mean of std
std of std
std of std
emotion
genre
vocals
instruments
usage
characteristics
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
1
2 3
4
5
6
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
27
40
45
46
49
50
51
52
53
54
56
57 58
59
60
61
62
66
68
73
78
80
81
82
83
84
85
86
87
88
89
90
96
108
113
123
124
125
128
132
136
138
139
142
●
●
●
●
●
●
mean of mean
mean of std
std of std
std of std
emotion
genre
vocals
instruments
usage
characteristics
(a). Pairwise score test. (b). Asymmetric score test.
●
●
●
●
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
1
2
3
4
5
6
8
9
12
14
16
17
18
20
21
22
27
40
45
46
49
51
52
54
55
56
57
58 59
61
62
66
69
71
73
80
81
82
84
85
87
88
89
90
96
99 105
122
●
●
●
●
●
●
mean of mean
mean of std
std of std
std of std
emotion
genre
vocals
instruments
usage
characteristics
●
●
●
●
●
●
● ●
● ●
●
●
●
1
2
3
4
6
8
14
20
46
59
62 66
82
84
85
88
89
●
●
●
●
●
●
mean of mean
mean of std
std of std
std of std
emotion
genre
vocals
instruments
usage
characteristics
(c). Desparsity method. (d). The common edges.
Figure 3: Estimated graphs in the CAL500 dataset inferred by the pairwise score test, asymmetric score test, and the desparsity method. We plot the connected components
of the estimated graph. In (a)-(c) we plot the graphs obtained by these three approaches, respectively, and plot the common edges in (d). For better illustration,
we only plot the connected components, combine the same type of continuous
variables, display them as a square and draw each binary variable as a circle.
The edges of the estimated graph show the association between these variables.
20
On Semiparametric Exponential Family Graphical Models
Appendix A. Proof of the Main Results
In this appendix we lay out the proof of the main results. In §A.1 we prove the result of
parameter estimation. The proof is based an induction argument that Algorithm 1 keeps
penalizing most of the irrelevant features and gradually reduces the bias in relevant features.
A.1. Proof of Theorem 5
Proof We only need to prove the theorem for one node j ∈ [d], the proof is identical for
the rest. To begin with, we first define a few index sets that play a significant role in our
analysis. For all j ∈ [d], we let Sj
:= {(j, k): β
∗
jk 6= 0, k ∈ [d]} be the support of β
∗
j
. For
the number of iterations ` = 1, 2, . . ., let G`
j
:=

(j, k) ∈/ Sj : λ
(`−1)
jk ≥ p
0
λ
(c2λ), k ∈ [d]
	
.
By condition (C.3) of the penalty function pλ(u) (see §3.2), we have p
0
λ
(c2λ) ≥ 0.91λ. In
addition, we let J
`
j
be the largest k
∗
components of
βb(`)
j

G`
j
in absolute value where k
∗
is defined in Assumption 4. In addition, we let I
`
j = (G`
j
)
c ∪ J
`
j
. Moreover, for notational
simplicity, we denote
βj

G`
j
,

βj

G`
j
and
βj

I
`
j
as βG`
j
, βJ
`
j
and βI
`
j
respectively when no
ambiguity arises.
The key point of the proof is to show that the complement of G`
j
is not too large. To be
more specific, we show that

(G`
j
)
c

 ≤ 2s
∗
for ` ≥ 1. Since Sj ⊂ (G`
j
)
c
, (G`
j
)
c ≤ 2s
∗
implies

(G`
j
)
c − Sj

 ≤ s
∗
. Note that G`
j
is the set of irrelevant features that are heavily penalized
in the `-th iteration of the algorithm, (G`
j
)
c − S being a small set indicates that the most of
the irrelevant features are heavily penalized in each step. We show that

(G`
)
c

 ≤ 2s
∗
for
each ` ≥ 1 by induction.
For ` = 1, we have G1
j = S
c
j
because λ
(0)
jk = λ for all j, k ∈ [d]. Hence

(G1
j
)
c

 ≤ s
∗
. Now
we assume that |

G`
j
c
| ≤ 2s
∗
for some integer ` and our goal is to prove that |

G
`+1
j
c
| ≤ 2s
∗
.
Our proof is based on three technical lemmas. The first lemma shows that the regularization
parameter λ in (7) is of the same order as k∇Lj (β
∗
j
)k∞.
Lemma 10 Under Assumptions 2 and 4, there exists a positive constants K such that, it
holds with probability at least 1 − (2d)
−1
that

∇Lj (β
∗
j
)


∞
≤ K
p
log d/n, ∀j ∈ [d]. (22)
Proof See §C.1 for a proof.
By this lemma, we conclude that the regularization parameter λ ≥ 25

∇Lj (β
∗
j
)


∞
with
high probability. The following lemma bounds the `1- and `2-norms of βb(`)
j −β
∗
j
by the
norms of its subvector under the induction assumption that

(G`
j
)
c

 ≤ 2s
∗
.
Lemma 11 Letting the index sets Sj , G`
j
, J`
j
and I
`
j
be defined as above, we denote Ge`
j
:=
G`
j
c
. Under the assumption that |G`
j
| ≤ 2s
∗
, we have

βb(`)
j − β
∗
j


2
≤ 2.2

βb(`)
I
`
j
− β
∗
I
`
j


2
and

βb(`)
j − β
∗
j


1
≤ 2.2

βb(`)
Ge`
j
− β
∗
Ge`
j


1
. (23)
Proof See §C.2 for a detailed proof.           
Yang, Ning, and Liu
The next lemma guarantees that βb(`)
j
stays in the `1-ball centered at β
∗
j with radius r for
` ≥ 1 where r appears in Assumption 4. Moreover, by showing this property of Algorithm
1 , we obtain a crude rate for parameter estimation. We summarized this result in the next
lemma.
Lemma 12 For ` ≥ 1 and j ∈ [d], we denote λ
(`)
Sj
:= (λ
(`)
jk ,(j, k) ∈ Sj )
T
. Assuming that

(G`
j
)
c

 ≤ 2s
∗
, it holds with probability at least 1 − d
−1
that, for all j ∈ [d], the estimators
βb(`)
j
obtained in each iteration of Algorithm 1 satisfy

βb(`)
I
`
j
− β
∗
I
`
j


2
≤ 10ρ
−1
∗
h
∇Ge`
j
Lj (β
∗
j
)


2
+

λ
(`−1)
Sj


2
i
, Ge`
j
:= (G
`
j
)
c
. (24)
This implies the following crude rates of convergence for βb(`)
j
:

βb(`)
j − β
∗
j


2
≤ 24ρ
−1
∗
√
s
∗λ and

βb(`)
j − β
∗
j


1
≤ 33ρ
−1
∗
s
∗λ. (25)
Proof See §C.3 for a detailed proof.
Now we show that Ge`+1
j = (G
`+1
j
)
c
satisfies |Ge`+1
j
| ≤ 2s
∗
, which concludes our induction.
Letting A := (G
`+1
j
)
c −Sj , by the definition of G
`+1
j
, (j, k) ∈ A implies that (j, k) ∈/ Sj
and p
0
λ

βb(`)
jk



≤ p
0
λ
(c2λ). Hence by the concavity of pλ(·), for any (j, k) ∈ A,

βb(`)
jk

 ≥ c2λ.
Therefore we have
p
|A| ≤

βb(`)
A


2

(c2λ) =

βb(`)
A − β
∗
A


2

(c2λ) ≤ 24ρ
−1
∗
√
s
∗

c2 ≤
√
s
∗, (26)
where the first inequality follows from |A| ≤ P
(j,k)∈A

βb(`)
jk


2
(c2λ)
2
. Note that (26) implies
that

(G
`+1
j
)
c

 ≤ 2s
∗
. Therefore by induction,

(G`
j
)
c

 ≤ 2s
∗
for any ` ≥ 1.
Now we have shown that for ` ≥ 1 and j ∈ [d],

(G`
j
)
c

 ≤ 2s
∗ and the crude statistical
rates (25) hold. In what follows, we derive the more refined rates (15) and (16).
A refined bound for

βb(`)
j − β
∗
j


2
and

βb(`)
j − β
∗
j


1
: For notional simplicity, we let
δ
(`) = βb(`)
j − β
∗
j
and omit subscript j in Sj , G`
j
, J`
j
and I
`
j
. We also denote Ge`
:= (G`
)
c
. We
first derive a recursive bound that links kδ
(`)
I
` k2 to kδ
(`−1)
I
`−1 k2. Note that by (23), kδ
(`)k1 ≤
2.2kδ
(`)
Ge`
k1 ≤ 2.2
√
2s
∗kδ
(`)
Ge`
k2. Hence we only need to control kδ
(`)
I
` k2 to obtain the statistical
rates of convergence for βb(`)
j
. By triangle inequality,

∇Ge`Lj (β
∗
j
)


2
≤

∇SLj (β
∗
j
)


2
+
q
|Ge` − S|

∇Lj (β
∗
j
)


∞
.
Since λ > 25

∇Lj (β
∗
j
)


∞
, (26) implies that

∇Ge`Lj (β
∗
j
)


2
≤

∇SLj (β
∗
j
)


2
+

δ
(`−1)
A


2

(25c2), (27)
where A := (G`
)
c − S ⊂ I
`
. Thus (27) can be written as

∇Ge`Lj (β
∗
j
)


2
≤

∇SLj (β
∗
j
)


2
+

δ
(`−1)
I
`


2

(25c2). (28)
2 
On Semiparametric Exponential Family Graphical Models
Also notice that ∀βjk ∈ R, if |βjk − β
∗
jk| ≥ c2λ,
p
0
λ
(|βjk|) ≤ λ ≤ |βjk − β
∗
jk|

c2;
otherwise we have |β
∗
jk| − |βjk| ≤ |βjk − β
∗
jk| < c2λ and thus p
0
λ
(|βjk|) ≤ p
0
λ

|β
∗
jk|−c2λ

by
the concavity of pλ(·). Hence the following inequality always holds:
p
0
λ
(|βjk|) ≤ p
0
λ

|β
∗
jk|−c2λ

+ |βjk − β
∗
jk|

c2. (29)
Applying (29) to βb(`−1)
j we have

λ
(`−1)
S


2
≤
 X
(j,k)∈S
p
0
λ

|β
∗
jk|−c2λ
2
1/2
+
 X
(j,k)∈S
|βb(`−1)
jk −β
∗
jk|
2
1/2
.
c2,
which leads to

λ
(`−1)
S


2
≤
h X
(j,k)∈S
p
0
λ

|β
∗
jk|−c2λ
2
i1/2
+

δ
(`−1)
I
`−1


2

c2. (30)
By (24), (28) and (30) we obtain

δ
(`)
I
`


2
≤ 10ρ
−1
∗

∇SLj (β
∗
j
)


2
+ Υj

+ γ

δ
(`−1)
I
`−1 k2,
where γ := 11(c2ρ∗)
−1 and we define Υj
:=
P
(j,k)∈S
p
0
λ

|β
∗
jk|−c2λ
2
1/2
for notational
simplicity. Note that since c2 ≥ 24ρ
−1
∗
, we have γ < 1. By recursion we obtain

δ
(`)
I
`


2
≤ 10%

∇SLj (β
∗
j
)


2
+ Υj

+ γ
`−1

δ
(1)
I
1


2
, (31)
where % := ρ
−1
∗
· (1 − γ)
−1 = c2(c2ρ∗ − 11)−1
. Using

βb(`)
j − β
∗
j


2
≤ 2.2

βb(`)
I
`
j
− β
∗
I
`
j


2
, we
can bound

βb(`)
j − β
∗
j


2
by

βb(`)
j − β
∗
j


2
≤ 22%

∇SjLj (β
∗
j
)


2
+ Υj

+ 2.2γ
`−1

δ
(1)
I
1
j


2
.
Note that for ` = 1, by (24) we have

δ
(1)
I
1
j


2
≤ 10ρ
−1
∗
√
s
∗

λ +
√
2

∇Lj (β
∗
j
)


∞

≤ 11ρ
−1
∗
√
s
∗λ = c2γ
√
s
∗λ. (32)
then we establish the following bound for

βb(`)
j − β
∗
j


2
:

βb(`)
j − β
∗
j


2
≤ 22%

∇SjLj (β
∗
j
)


2
+ Υj

+ 2.2c2
√
s
∗λγ`
. (33)
Similarly, by

βb(`)
j −β
∗
j


1
≤ 2.2
√
2s
∗

βb(`)
I
`
j
−β
∗
I
`
j


2
, we obtain a bound on

βb(`)
j −β
∗
j


1
:

βb(`)
j − β
∗
j


1
≤ 32√
s
∗%

∇SjLj (β
∗
j
)


2
+ Υj

+ 2.2γ
`−1
√
2s
∗

δ
(1)
I
1
j


2
. (34                   
Yang, Ning, and Liu
By (32) we have 2.2
√
2s
∗

δ
(1)
I
1
j


2
≤ 3.2c2γs∗λ, then the right-hand side of (34) can be
bounded by

βb(`)
j − β
∗
j


1
≤ 32√
s
∗%

∇SjLj (β
∗
j
)


2
+ Υj

+ 3.2c2s
∗λγ`
. (35)
Therefore (15) and (16) can be implied by (33) and (35) respectively. Moreover, by Lemma
12, we conclude that the statistical rates (33) and (35) hold for all j ∈ [d] with probability
at least 1 − d
−1
.
A.2. Proof of Theorem 8
Proof We first remind the reader that, for 1 ≤ j 6= k ≤ d, we denote
βj\k = (βj1, . . . , βjj−1, βjj+1, . . . , βjk−1, βjk+1, . . . , βjd)
T ∈ R
d−2
,
βj∨k = (βjk, βj\k
, βk\j
)
T ∈ R
2d−3 and βb0
j∨k = (0, βb
j\k
, βb
k\j
)
T
. In addition, we define σ
2
jk =
Σ
jk
jk,jk − 2Σ
jk
jk,j\kw∗
j,k − 2Σ
jk
jk,k\jw∗
k,j + w∗
j,k
T Σ
jk
j\k,j\kw∗
j,k + w∗
k,j
T Σ
jk
k\j,k\jw∗
k,j . To prove the
theorem our goal is to prove the following two arguments:
limn→∞
max
j<k
√
n

Sbjk−Sjk(β
∗
j∨k
)

 = 0 and limn→∞
max
j<k
|σbjk − σjk| = 0. (36)
Note that by Lemma 14, σ
2
jk is the asymptotic variance of √
n/2·Sjk(β
∗
j∨k
). Thus combining
(36) and Slutsky’s theorem yields the theorem. By the the expression of Sjk(β
∗
j∨k
) and
Sbjk in (17) and (18), under null hypothesis, for a fixed pair of nodes j and k, we have
Sbjk−Sjk(β
∗
j∨k
)=I1j+I2j+I1k+I2k where I1j and I2j are defined as
I1j
:=

∇jkLj (βb0
j
) − ∇jkLj (β
∗
j
)

− wb
T
j,k
∇j\kLj (βb0
j
) − ∇j\kLj (β
∗
j
)

and
I2j
:= (w∗
j,k − wbj,k)
T ∇j\kLj (β
∗
j
);
whereas I1k and I2k are defined by interchanging j and k in I1j and I2j :
I1k :=

∇kjLk(βb0
k
) − ∇jkLk(β
∗
k
)

− wb
T
k,j
∇k\jLk(βb0
k
) − ∇k\jLk(β
∗
k
)

and
I2k := (w∗
k,j − wbk,j )
T ∇k\jLj (β
∗
k
).
We first bound I1j . Recall that βb0
j = (0, βb
j\k
)
T
. Note that under the null hypothesis, β
∗
jk = 0,
by the Mean-Value Theorem, there exists a βe
j\k ∈ R
d−2
in the line segment between βb
j\k
and β
∗
j\k
such that
I1j =

Λe
jk,j\k − wb
T
j,kΛe
j\k,j\k
βb
j\k − β
∗
j\k

,
where Λe := ∇2Lj (0, βe
j\k
). We let δ := βb0
j − β
∗
j
and denote ∇2Lj (βb0
j
) and ∇2
(β
∗
j
) as Λ and
Λ∗
respectively. From the definition of Dantzig selector we obtain
|I1j | ≤ kΛjk,j\k − wb
T Λj\k,j\kk∞kδj\kk1
| {z }
I11
+ kΛjk,j\k − Λe
jk,j\kk∞kδj\kk1
| {z }
I12
+ kwb
T
(Λj\k,j\k − Λe
j\k,j\k
)δj\kk∞
| {z }
I13
.
2             
On Semiparametric Exponential Family Graphical Models
Theorem 5 implies that

δk1 ≤ Cs∗λ with probability tending to 1 for some constant C > 0.
Then by the definition of Dantzig selector, I11 ≤ Cs∗λλD. with high probability. Moreover,
the constant C is the same for all (j, k). By assumption 7, I11 = o(n
−1/2
) with probability
tending to one.
For term I12, H¨older’s inequality implies that
I12 ≤ kΛjk,j\k − Λe
jk,j\kk∞kδj\kk1. (37)
By Lemma 26 we obtain
kΛ − Λek∞ ≤ kΛ − Λ
∗
k∞ + kΛ
∗ − Λek∞ ≤ 2Cs∗λ log2
d. (38)
Therefore combining (37) and (38) we have
I12 ≤ 2Cs∗2
λ
2
log2
d . s
∗λλD uniformly for 1 ≤ j < k ≤ d.
Similarly by H¨older’s inequality, we have
I13 ≤ kwbj,kk1kΛ − Λek∞kδk1. (39)
Notice that by the optimality of wbj,k, kwbj,kk1 ≤ kw∗
j,kk1 ≤ w0. Combining (39) and (38)
we have
I13 ≤ Cw0s
∗2
λ
2
log2
d . s
∗λλD uniformly for 1 ≤ j < k ≤ d.
where we use the fact that λD & max{1, w0}s
∗λ log2
d. Therefore we conclude that for
all j ∈ [d], |I1j | . s
∗λλD = oP(n
−1/2
). For I2j , H¨older’s inequality implies that |I2j | ≤
kw∗
j,k − wbj,kk1

∇Lj (β
∗
j
)


∞
. To control kw∗
j,k − wbj,kk1, we need to the following lemma to
obtain the estimation error of the Dantzig selector wbj,k.
Lemma 13 For 1 ≤ j 6= k ≤ d, let wbj,k be the solution of the Dantzig-type optimization
problem (11) and let w∗
j,k = H
j
jk,j\k
(H
j
j\k,j\k
)
−1
. Under Assumptions 2, 4, 6 and 7, with
probability tending to one, we have
kwbj,k − w∗
j,kk1 ≤ 37ν
−1
∗
s
?
0λD for all 1 ≤ j 6= k ≤ d.
Proof See §D.2 for a detailed proof.
Now combining Lemma 13 and Theorem 10 we obtain that
|I2j | ≤ 37ν
−1
∗ K1s
?
0λD
p
log d/n  s
?
0λλD = o(n
−1/2
).
Therefore we have shown that I1j + I2j = o(n
−1/2
) with high probability. Similarly, we also
have I1k + I2k = o(n
−1/2
) with high probability. Moreover, since the bounds for |I1j | and
|I2j | is independent of the choice of (j, k) ∈ {(j, k): 1 ≤ j 6= k ≤ d}, we conclude that
√
n

Sbjk − Sjk
β
∗
j∨k
 = oP(1) uniformly for 1 ≤ j < k ≤ d.
Our next lemma characterizes the limiting distribution of ∇Ljk
β
∗
j∨k

and is pivotal for
establishing the validity of the composite pairwise score test.
    
Yang, Ning, and Liu
Lemma 14 For any b ∈ R
2d−3 with kbk2 = 1 and |bk0 ≤ se, if limn→∞
se

n= 0, we have
√
n/2 · b
T ∇Ljk
β
∗
j∨k
) N

0, b
T Σjkb

. (40)
By Lemma 14 we obtain
√
n/2 · S

β
∗
j∨k

= ∇jkLjk
β
∗
j∨k

− w∗
j,k
T ∇j\kLjk
β
∗
j∨k

− w∗
k,j
T ∇j\kLjk
β
∗
j∨k

 N(0, σ2
jk),
where the asymptotic variance σ
2
jk is given by
σ
2
jk = Σ
jk
jk,jk − 2Σ
jk
jk,j\kw∗
j,k − 2Σjk,k\jw∗
k,j + w∗
j,k
T Σ
jk
j\k,j\kw∗
j,k + w∗
k,j
T Σ
jk
k\j,k\jw∗
k,j .
For a more accurate estimation of Sbjk − Sjk
β
∗
j∨k

, we have
√
n

Sbjk − Sjk
β
∗
j∨k

 ≤
√
n

|I1| + |I2|

.
√
n(s
∗+s
?
0
)λλD. (41)
Finally, the following lemma, whose proof is deferred to the supplementary material, shows
that σbjk is a consistent estimator of σjk.
Lemma 15 For 1 ≤ j 6= k ≤ d, we denote the asymptotic variance of √
n/2 · Sjk(β
∗
j∨k
) as
σ
2
jk. Under Assumptions 2, 4, 6 and 7, the estimator σbjk satisfies limn→∞
max
j<k
|σbjk − σjk| = 0.
Proof See §D.3 for a proof.
Since σbjk is consistent for σjk by Lemma 15 and σjk is bounded away from zero by Assumption 6, Slutsky’s theorem implies that √
nSbjk/(2σbjk) N(0, 1).
Appendix B. Additional Estimation Results
We present the additional results of parameter estimation. In §B.1 we verify the sparse
eigenvalue condition for Gaussian graphical models, which justifies Assumption 4 in our
paper. In §B.2 we derive a more refined statistical rates of convergence for the iterates of
Algorithm 1.
B.1. Verify the Sparse Eigenvalue Condition for Gaussian Graphical Models
In this subsection, we verify the sparse eigenvalue condition for Gaussian graphical models.
Moreover, we show that such condition holds uniformly over a `1-ball centered at the true
parameter β
∗
j
.
Proposition 16 Suppose X ∼ N(0, Σ) is a Gaussian graphical model and let Θ = Σ−1
be the precision matrix. For all j ∈ [d], the conditional distribution of Xj given X\j
is a
normal distribution with mean β
∗
j
TX\j and variance Θ−1
jj , where β
∗
j = Θj\j
. Let Lj (·) be
the loss function defined in (6). We assume that there exist positive constants D, cλ and
Cλ such that kΣk∞ ≤ D and cλ ≤ λmin(Σ) ≤ λmax(Σ) ≤ Cλ. We let s
∗ = maxj∈[d] kβ
∗
j
k0
and also assume that there exists a constant Cβ > 0 such that kβ
∗
j
k2 ≤ Cβ for all j ∈         
On Semiparametric Exponential Family Graphical Models
Suppose r > 0 is a real number such that r = O(1/
√
s
∗). Then, there exist ρ∗, ρ∗ > 0 such
that for all j ∈ [d], and s = 1, . . . , d − 1,
ρ∗ ≤ ρ−

E

∇2Lj

, β
∗
j
; s, r
≤ ρ+

E

∇2Lj

, β
∗
j
; s, r
≤ ρ
∗
.
Proof We prove this lemma in two steps. For any βj ∈ R
d−1
such that kβj − β
∗
j
k1 ≤ r
and any v ∈ R
d−1
such that kvk2 = 1, we first give a lower bound for v
TE

∇2Lj (βj )

v by
truncation. Then we give an upper bound in the second step.
Step (i): Lower Bound of vTE

∇2Lj (βj )

v. We denote Bj (r) :=

β ∈ R
d−1
: kβ −
β
∗
j
k1 ≤ r
	
. For two truncation levels τ > 0 and R > 0, we denote Aii0 :=

|Xij | ≤ τ
	
∩

|Xi
0j
| ≤ τ
	
, Bi
:=

XT
i\j
βj

 ≤ R, ∀βj ∈ Bj (r)
	
and Bi
0 :=

XT
i
0\j
β
∗
j

 ≤ R, ∀βj ∈ Bj (r)
	
.
The values of R and τ will be determined later. By the definition of Lj (·), for any βj ∈ Bj (r)
and any v ∈ R
d−1 with kvk2 = 1, we have
v
T ∇2Lj (βj )v ≥
2C1(R, τ )
n(n − 1)
X
i<i0

Xij − Xi
0j
2
Xi\j − Xi
0\j
T
v
2
I(Bi)T(Bi
0)I(Aii0), (42)
where C1(R, τ ) := exp(−4Rτ )

1 + exp(−4Rτ )
−2
. For notational simplicity, we denote the
right-hand side of (42) as C1(R, τ )v
T∆v. By the properties of Gaussian graphical models,
the conditional density of Xij given Ii
:=

Xi\j = xi\j
	
∩ Bi
is
p

xij |Ii) = p(xi
|Bi)
. Z
R
p(xi
|Bi)dxij = p(xij |xi\j
),
where we use the fact that p(xi
|Bi) = p(xi)/P(Bi) and that P(Bi) is a constant. Recall that
p(xij |Xi\j
) = q
Θjj
(2π) exp
−Θjj/2(xij − XT
i\jβ
∗
j
)
2

where β
∗
j = Θj\j
.
Thus the conditional expectation of (Xij − Xi
0j )
2
I(Aij ) given Ii and Ii
0 is
E
h
(Xij − Xi
0j )
2
I(Aii0)



Ii ∩ Ii
0
i
= Θjj/(2π)
Z τ
−τ
Z τ
−τ
(xij − xij )
2
expn
−Θjj/2

(xij − β
T
j xi\j
)
2 + (xi
0j − β
T
j xi
0\j
)
2
o
dxijdxi
0j
.
Note that on event Ii
, |β
T
j Xi\j
| ≤ R, hence the expression above can be lower-bounded by
E
h
(Xij − Xi
0j )
2
I(Aii0)



Ii ∩ Ii
0
i
≥ Θjj/(2π)
Z τ
−τ
Z τ
−τ
(xij − xi
0j )
2
expn
−Θjj/2

x
2
ij + x
2
i
0j + 2R
2 + 2R(|xij | + |xi
0j
|)
o
dxijdxi
0j
.
The last expression is positive and we denote it as C2(R, τ ) for simplicity. Thus by the law
of total expectation we obtain
v
TE(∆)v = v
TE

E(∆

 ∩
n
i=1 Ii)

v ≥ C2(R, τ )E
n
(Xi\j − Xi
0\j
)
T v
2
I(Bi)I(Bj )
o
                           
Yang, Ning, and Liu
By Cauchy-Schwarz inequality we have
E
n
(Xi\j − Xi
0\j
)
T v
2

1 − I(Bi)I(Bi
0)
o
≤
q
E[(Xi\j − Xi
0\j
)
T v
4
q
P

B
c
i ∪ Bc
i
0

. (43)
Note that for Gaussian graphical model, the marginal distribution of X\j
is N(0, Σ\j\j
).
If we denote Σ\j\j as Σ1, we have (Xi\j −Xi
0\j
)
T v ∼ N(0, σ2
v
), XT
i\j
β
∗
j ∼ N(0, σ2
1
) and
XT
i\j
β ∼ N(0, σ2
2
) where σ
2
v = 2v
T Σ1v, σ2
1 = β
∗
j
T Σ1β
∗
j
and σ
2
2 = β
T
j Σ1βj . Hence we have
E

(Xi\j − Xi
0\j
)
T v
4 = 3σ
4
v
. Because the maximum eigenvalue of Σ1 is upper bounded by
Cλ, we have σ
2
1 ≤ CλC
2
β
and σ
2
v ≤ 2Cλ. Note that σ
2
2−σ
2
1 =β
T
j Σ1βj−β
∗
j
T Σ1β
∗
j
, the following
lemma in linear algebra bounds this type of error.
Lemma 17 Let M ∈ R
d×d
be a symmetric matrix and vectors v1 and v2 ∈ R
d
, then

v
T
1 Mv1 − v
T
2 Mv2

 ≤ kMk∞kv1 − v2k
2
1 + 2kMv2k∞kv1 − v2k1.
Proof Note that v
T
1 Mv1 − v
T
2 Mv2 = (v1−v2)
TM(v1−v2) + 2v
T
2 M(v1−v2), H¨older’s
inequality implies

v
T
1 Mv1 − v
T
2 Mv2

 ≤

(v1−v2)
TM(v1−v2)

 + 2

v
T
2 M(v1−v2)


≤ kMk∞kv1 − v2k
2
1 + 2kMv2k∞kv1 − v2k1.
Hence, we conclude the proof of Lemma 17.
By Lemma 17, we have
σ
2
2 − σ
2
1 ≤ kΣ1k∞kβj − β
∗
j


2
1
+ 2kΣ1β
∗
j k∞kβj − β
∗
j k1. (44)
By H¨older’s inequality and the relation between `1-norm and `2-norm of a vector, we have
kΣ1β
∗
j
k∞ ≤ kΣ1k∞kβ
∗
j
k1 ≤
√
s
∗CβD. Therefore the right-hand side of (44) can be bounded
by
σ
2
2 − σ
2
1 ≤ r
2D + 2√
s
∗rCβD,
which shows that σ
2
2
is also bounded because r = O(1√
s
∗). In addition, by the bound
1 − Φ(x) ≤ exp(−x
2/2)/(x
√
2π) for the standard normal distribution function, we obtain
that
P

B
c
i

≤ P

XT
i\jβ
∗
j > R
+ P

XT
i\jβj > R
≤ cσ1 exp
−R
2
/(2σ
2
1
)

/R + cσ2 exp
−R
2
/(2σ
2
2
)

/R,
where the constant c = 1/
√
2π. We denote the last expression as C3(R), then the righthand side of (43) can be upper-bounded by p
3σ
4
v
p
2C3(R) ≤ 2
p
6C3(R)Cλ. Hence we
can choose a sufficiently large R such that 2p
6C3(R)Cλ = λmin(Σ) and we denote this
particular choice of R as R0.
Now we have
E
n
(Xi\j − Xi
0\j
)
T v
2

1 − I(Bi)I(Bi
0)
o
≤ λmin(Σ)                   
On Semiparametric Exponential Family Graphical Models
Note that E

[(Xi\j − Xi
0\j
)
T v]
2
	
= σ
2
v ≥ 2λmin(Σ), we obtain that
v
TE

∇2Lj (βj )

v ≥ C1(R0, τ )C2(R0, τ )λmin(Σ) for all τ ∈ R.
Therefore we conclude that for all βj ∈ R
d−1
such that kβj − β
∗
j
k1 ≤ r,
v
TE

∇2Lj (βj )

v ≥ max
τ∈R

C1(R0, τ )C2(R0, τ )
	
λmin(Σ). (45)
Step (ii): Upper Bound of vTE

∇2Lj (βj )

v. For any βj ∈ R
d−1
such that kβj −β
∗
j
k1 ≤
r and for any v ∈ R
d−1 with kvk2 = 1, by the definition of ∇2Lj (βj ) we have
v
T ∇2Lj (βj )v ≤ (Xij − Xi
0j )
2

(Xi\j − Xi
0\j
)
T v
2
. (46)
Notice that conditioning on Xi\j
, Xij ∼ N

XT
i\j
β
∗
j
, Θ−1
jj 
, hence
E

(Xij − Xi
0j )
2

Xi\j
, Xi
0\j

=

(Xi\j − Xi
0\j
)
Tβ
∗
j
2 + 2Θ−1
jj . (47)
Combining (46) and (47) we obtain
E

v
T ∇2Lj (βj )v

≤ E
n
E

(Xij − Xi
0j )
2

Xi\j
, Xi
0\j

·

(Xi\j − Xi
0\j
)
T v
2
o
≤ 2Θ−1
jj E

(Xi\j − Xi
0\j
)
T v
2 + E
n
(Xi\j − Xi
0\j
)
Tβ
∗
j
2

(Xi\j − Xi
0\j
)
T v
2
o
. (48)
Because Xi\j ∼ N(0, Σ1) where Σ1 := Σ\j,\j
, and also note that the maximum eigenvalue
of Σ1 is upper bounded by Cλ, we have
E

(Xi\j − Xi
0\j
)
T v
2 = 2v
T Σ1v ≤ 2Cλ.
Moreover, by inequality 2ab ≤ a
2 + b
2 we obtain
2E
n
(Xi\j − Xi
0\j
)
Tβ
∗
j
2

(Xi\j − Xi
0\j
)
T v
2
o
≤ E

(Xi\j − Xi
0\j
)
Tβ
∗
j
4 + E

(Xi\j − Xi
0\j
)
T v
4
.
Since (Xi\j − Xi
0\j
)
T v ∼ N(0, σ2
v
) and (Xi\j − Xi
0\j
)
Tβ
∗
j ∼ N(0, 2σ
2
1
) where σ
2
v and σ
2
1
are
defined as 2v
T Σ1v and β
∗
j
T Σ1β
∗
j
respectively, we obtain
E

(Xi\j − Xi
0\j
)
Tβ
∗
j
4 = 3σ
4
v ≤ 12C
2
λ
and E

(Xi\j − Xi
0\j
)
T v
4 = 12σ
4
1 ≤ 12CλC
2
β
.
Therefore we can bound the right-hand side of (48) by
E

v
T ∇2Lj (βj )v

≤ 4Θ−1
jj Cλ + 6C
2
λ + 6CλC
2
β
. (49)
Combining (45) and (49) we conclude that Proposition 16 holds with
ρ∗ = max
τ∈R

C1(R0, τ )C2(R0, τ )
	
λmin(Σ) and ρ
∗ = 4Θ−1
jj Cλ + 6C
2
λ + 6CλC
2
β
.
Therefore, we conclude the proof of Proposition 16.
                                        
Yang, Ning, and Liu
B.2. Refined Statistical Rates of Parameter Estimation
In this subsection we show more refined statistical rates of convergence for the proposed
estimators. In specific, we consider the case where β
∗
j
contains nonzero elements with both
strong and week magnitudes.
Theorem 18 (Refined statistical rates of convergence) Under Assumptions 2 and 4,
we let K1 and K2 be the constants defined in Theorem 10 and also let ρ∗ > 0 and r > 0 be
defined in Assumption 4. For all j ∈ [d], we define the support of β
∗
j
as Sj
:={(j, k): β
∗
jk 6=
0, k ∈ [d]} and let s
∗ = maxj∈[d] kβ
∗
j
k0. The penalty function pλ(u) : [0, +∞) → [0, +∞) in
(7) satisfies regularity conditions (C.1), (C.2) and (C.3) listed in §3.2 with c1 = 0.91 and
c2 ≥ 24/ρ∗ for condition (C.3). We set the regularity parameter λ = C
p
log d/n such that
C ≥25K1. Moreover, we assume that the penalty function pλ(u) satisfies an extra condition
(C.4): there exists a constant c3 > 0 such that p
0
λ
(u)= 0 for u∈

c3λ, +∞

. Suppose that the
support of β
∗
j
can be partitioned into Sj = S1j ∪ S2j where S1j =

(j, k): |β
∗
jk| ≥ (c2+c3)λ
	
and S2j = Sj −S1j . We denote constants A1 = 22%, A2 = 2.2c2, B1 = 32%, B2 = 3.2c2,
% = c2(c2ρ∗−11)−1
, γ = 11c
−1
2
ρ
−1
∗ < 1 and a = 1.04; we let s
∗
1j = |S1j | and s
∗
2j = |S2j |. With
probability at least 1−d
−1
, we have the following more refined rates of convergence:

βb(`)
j − β
∗
j


2
≤ A1
n
∇S1jLj (β
∗
j
)


2
+ a
q
s
∗
2j
λ
o
+ A2
√
s
∗λγ`
and (50)

βb(`)
j − β
∗
j


1
≤ B1
n
∇S1jLj (β
∗
j
)


2
+ a
q
s
∗
2j
λ
o
+ B2s
∗λγ`
, ∀j ∈ [d]. (51)
Proof Let Sj = {(j, k): β
∗
jk 6= 0, k∈[d]} be the support of β
∗
j
and let index set G`
j
, J`
j
and
I
`
j
be the same as defined in the proof of Theorem 5. For notational simplicity, we omit the
subscript j in these index sets which stands for the j-th node of the graph; we simply write
them as G`
, J` and I
`
. Moreover, we let δ
(`) = βb(`)
j − β
∗
j
, it is shown in Lemma 12 that

δ
(`)
I
`


2
≤ 10ρ
−1
∗

∇Ge`Lj (β
∗
j
)


2
+

λ
(`−1)
Sj


2

; Ge` = (G
`
)
c
. (52)
In the proof of Theorem 5, we show that |Ge`
| ≤ 2s
∗
for all j ∈ [d] and ` ≥ 1. Because
Sj = S1j ∪ S2j where S1j =

(j, k): |β
∗
jk| ≥ (c2 + c3)λ
	
and S2j = Sj−S1j , then by triangle
inequality we have

∇SjLj (β
∗
j
)


2
≤

∇S1jLj (β
∗
j
)


2
+
q
s
∗
2j

∇S2jLj (β
∗
j
)


∞
.
Since λ ≥ 25

∇Lj (β
∗
j
)


∞
with high probability, by (28), we further have

∇Ge`Lj (β
∗
j
)


2
≤

∇S1jLj (β
∗
j
)


2
+
q
s
∗
2j
λ

25 +

δ
(`−1)
I
`−1


2
.
(25c2). (53)
Note that by the definition of S1j , for any (j, k)∈S1j , p
0
λ

|βjk|−c2λ

≤ p
0
λ
(c3λ) = 0, then
we have
Υj
:= λ
h X
(j,k)∈Sj
p
0
λ

|β
∗
jk| − c2λ
2
i1/2
= λ
h X
(j,k)∈S2j
p
0
λ
(|β
∗
jk| − c2λ)
2
i1/2
≤
q
s
∗
2j
λ.
   
On Semiparametric Exponential Family Graphical Models
Therefore (30) is reduced to

λ
(`−1)
Sj


2
≤ Υj +

δ
(`−1)
I
`−1


2

c2 ≤
q
s
∗
2j
λ +

δ
(`−1)
I
`−1


2
.
c2. (54)
Combining (52), (53) and (54) we obtain

δ
(`)
I
`


2
≤ 10ρ
−1
∗
n
∇S1jLj (β
∗
j
)


2
+ 1.04q
s
∗
2j
λ + 1.04

δ
(`−1)
I
`−1


2

c2
o
.
Then by recursion, we obtain the following estimation error:

δ
(`)
I
`


2
≤ 10%
n
∇S1jLj (β
∗
j
)


2
+ 1.04q
s
∗
2j
λ
o
+ γ
`−1

δ
(1)
I
1


2
,
where γ := 11c
−1
2
ρ
−1
∗ and % := c2(c2ρ∗ − 11)−1
. Note that we assume c2 ≥ 24ρ
−1
∗
, for k = 1
by (32) we have
2.2

δ
(1)
I1


2
≤ 2.2c2γ
√
s
∗λ and 2.2
√
2s
∗

δ
(1)
I1


2
≤ 3.2c2γs∗λ.
Therefore, using the original notation, we obtain the refined rates of convergence by (23):

βb(`)
j − β
∗
j


2
≤ 22%
n
∇S1jLj (β
∗
j
)


2
+ 1.04q
s
∗
2j
λ
o
+ 2.2c2γ
`
√
s
∗λ and

βb(`)
j − β
∗
j


1
≤ 32%
√
s
∗
n
∇S1jLj (β
∗
j
)


2
+ 1.04q
s
∗
2j
λ
o
+ 3.2c2γ
`
s
∗λ,
where s
∗
2j = |S2j |. Moreover, it is easy to see that, with probability at least 1−d
−1
, these
convergence rates hold for all j ∈ [d].
Appendix C. Proof of the Auxiliary Results for Estimation
In this appendix, we prove the main results for estimation results presented in §4.1. In this
appendix, we prove the auxiliary results for estimation. In specific, we give detailed proofs
of Lemmas 10, 11, and 12, which are pivotal for the proof of Theorem5. We first prove
Lemmas 10, which gives an upper bound for k∇Lj (β
∗
j
)k∞.
C.1. Proof of Lemma 10
Proof By definition, ∇Lj (β
∗
j
) is a centered second-order U-statistic with kernel function
h
j
ii0(β
∗
j
)∈R
d−1
, whose entries are given by

h
j
ii0(β
∗
j
)

jk =
R
j
ii0(β
∗
j
)(Xij − Xi
0j )(Xik − Xi
0k)
1 + R
j
ii0(β
∗
j
)
.
By the tail probability bound in (14), for any i ∈ [n] and j ∈ [d], we have
P

|Xij | > x, ∀i ∈ [n], ∀j ∈ [d]

≤
X
i∈[n],j∈[d]
P(|Xij | > x)
≤ 2 exp(κm + κh/2) exp(−x + log d + log n). (55)
3   
Yang, Ning, and Liu
By setting x = C log d for some constant C, we conclude that event E := {|Xij | ≤
C log d, ∀i ∈ [n], ∀j ∈ [d]} holds with probability at least 1 − (4d)
−1
. Following from
the same argument as in Ning et al. (2017b), it is easy to show that, conditioning on E,
h
j
ii0(β
∗
j
) is also centered. Note that conditioning on event E,

h
j
ii0(β
∗
j
)


∞
≤ C log2
d for
some generic constant C and for all i, i0 ∈ [d] and j ∈ [d]. The following Bernstein’s inequality for U-statistics, presented in Arcones (1995), gives an upper bound for the tail
probability of ∇Lj (β
∗
j
).
Lemma 19 (Bernstein’s inequality for U-statistics) Given n i.i.d. random variables
Z1, . . . Zn taking values in a measurable space (S, B) and a symmetric and measurable kernel
function h: S
m → R, we define the U-statistics with kernel h as
U :=

n
m
−1
X
i1<...<im
h(Zi1
, . . . , Zim).
Suppose that E[h(Zi1
, . . . , Zim)] = 0, E

E[h(Zi1
, . . . , Zim) | Zi1
]
	2 = σ
2
, and khk∞ ≤ b for
some positive σ and b. There exists an absolute constant K(m) > 0 that only depends on
m such that
P(|U| > t) ≤ 4 exp
−nt2
/[2m2σ
2 + K(m)bt]
	
, ∀t > 0. (56)
Note that by (14), the fourth moment of X is bounded, which implies that E[h
j
ii0(β
∗
j
)]2
is uniformly bounded by an absolute constant for all j ∈ [d]. By Lemma 19, setting
b = C log2
d in (56) yields that
P

∇jkLj (β
∗
j
)

 > t

E

≤ 4 exph
−nt2

(C1 + C2 log2
d · t)
i
(57)
for some generic constants C1 and C2. Taking a union bound over {(j, k): j, k ∈ [d], k 6= j}
we obtain
max
j∈[d]
n
P

∇Lj (β
∗
j
)


∞
> t

E
o
. d
2
· exp
−nt2

(C1 + C2 log2
d · t)

. (58)
Under Assumption 4 and conditioning on E, by setting t = K1
p
log d/n for a sufficiently
large K1 > 0, it holds probability greater than 1 − (4d)
−1
that

∇Lj (β
∗
j
)


∞
≤ K1
p
log d/n ∀j ∈ [d].
Note that E holds with probability at least 1−(4d)
−1
, we conclude the proof of Lemma 10.
C.2. Proof of Lemma 11
Proof In what follows, for notational simplicity and readability, we omit j in the subscript
and ` in the superscript by simply writing Sj , G`
j
, J`
j
and I
`
j
as S, G, J an I respectively. By
the definition of G,

λ
(`−1)
G`


min ≥ p
0
λ
(θ) ≥ 0.91λ > 22.75k∇Lj (β
∗
j
)k∞. We prove this lemma
    
On Semiparametric Exponential Family Graphical Models
in two steps. In the first step we show that

βb(`)
j − β
∗
j


1
≤ 2.2

βb`
Gc − β
∗
Gc


1
. Suppose
that βb(`)
j
is the solution in the `-th iteration and we denote ∇jkLj (βj ) = ∂Lj (βj )

∂βjk,
the Karush-Kuhn-Tucker condition implies that
∇jkLj (βb(`)
j
) + λ
(`−1)
jk sign(βb(`)
jk ) = 0 if βb(`)
jk 6= 0;
∇jkLj (βb(`)
j
) + λ
(`−1)
jk ξ
(`)
jk = 0, ξ(`)
jk ∈ [−1, 1] if βb(`)
jk = 0.
The above Karush-Kuhn-Tuker condition can be written in a compact form as
∇Lj (βb(`)
j
) + λ
(`−1)
j
◦ ξ
(`)
j = 0, (59)
where ξ
(`)
j ∈∂

βb(`)
j


1
and λ
(`−1)
j =

λ
(`−1)
j1
, . . . , λ(`−1)
jj−1
, λ(`−1)
jj+1 , . . . , λ(`−1)
jd T
∈ R
d−1
.
For notational simplicity, we let δ = βb(`)
j − β
∗
j ∈ R
d−1 and omit the superscript ` and
subscript j in both λ
(`−1)
j
and ξ
(`)
j
by writing them as λ and ξ. By definition, I = Gc ∪ J.
Note that we denote the support of β
∗
j
as S; we define H := Gc−S, then S, H and G is a
partition of 
(j, k): k∈[d], k 6=j
	
.
By the Mean-Value theorem, there exists an α∈[0, 1] such that βej
:=αβ
∗
j+(1 − α)βb(`)
j ∈
R
d−1
satisfies
∇Lj (βbj ) − ∇Lj (β
∗
j
) = ∇2Lj (βej )δ.
Then (59) implies that
0 ≤ δ
T ∇2Lj (βej )δ = −


δ,λ ◦ ξ

| {z }
(i)
−


∇Lj (β
∗
j
), δ

| {z }
(ii)
. (60)
For term (ii) in (60), H¨older’s inequality implies that
(ii) ≥ −

∇Lj (β
∗
j
)


∞
kδk1. (61)
For term (i) in (60), recall that we denote |v| as the vector that takes entrywise absolute
value for v. By the fact that ξ
(`)
jk βb(`)
jk = |βb(`)
jk |, we have ξG ◦ δG = |δG| and ξH ◦ δH = |δH|.
Since δSc = βb(`)
Sc . H¨older’s inequality implies that


δ,λ ◦ ξ

=


δS,(λ ◦ ξ)S

+


|δH|,λH

+


|δG|,λG

≥ −kδSk1kλSk∞ + kδGk1kλGkmin + kδHk1kλHkmin. (62)
Combining (60), (61) and (62) we have
−kδSk1kλSk∞ + kδGk1kλGkmin + kδHk1kλHkmin −

∇Lj (β
∗
j
)


∞
kδk1 ≤ 0. (63)
By the definition of G, we have kλGkmin ≥ p
0
λ
(c2λ) ≥ 0.91λ. Rearranging terms in (63) we
have
p
0
λ
(c2λ)kδGk1 ≤ kδGk1kλGkmin ≤

∇Lj (β
∗
j
)


∞
kδk1 + kδSk1kλSk∞.
3 
Yang, Ning, and Liu
Using the decomposability of the `1-norm, we have
h
p
0
λ
(c2λ) −

∇Lj (β
∗
j
)


∞
i
kδGk1 ≤
h
kλSk∞ + k∇Lj (β
∗
j
)k∞
i
kδGc k1 (64)
Recall that λ > 25

∇Lj (β
∗
j
)


∞
and p
0
λ
(θ) ≥ 0.91λ, (64) implies

δG


1
≤
λ +

∇Lj (β
∗
j
)


∞
p
0
λ
(c2λ) −

∇Lj (β
∗
j
)


∞
kδGc k1 ≤ 1.2kδGc k1, (65)
where we use the fact that
λ +

∇Lj (β
∗
j
)


∞
p
0
λ
(c2λ) −

∇Lj (β
∗
j
)


∞
≤
λ + 0.04λ
0.91λ − 0.04λ
≤ 1.2.
Going back to the original notation, (65) is equivalent to

βb(`)
j − β
∗
j


1
≤ 2.2

βb(`)
Ge`
j
− β
∗
Ge`
j


1
.
Now we show in the second step that

βb(`)
j − β
∗
j


2
≤ 2.2

βb(`)
I
`
j
− β
∗
I
`
j


2
. Recall that J
is the largest k
∗
components of βb(`)
G
in absolute value where we omit the subscript j and
superscript ` in the sets G`
j
, J`
j
and I
`
j
. By the definition of J we obtain that
kδI
c k∞ ≤ kδJ k1

k
∗ ≤ kδGk1

k
∗
, where δ = βb(`)
j − β
∗
j
.
By inequality (65) and the fact that Gc ⊂ I, we further have
kδI
c k∞ ≤ 1.2/k∗
· kδGc k1 ≤ 1.2/k∗
· kδIk1. (66)
Then by H¨older’ inequality and (66) we obtain that
kδI
c k2 ≤

kδI
c k1kδI
c k∞
1/2 ≤ (1.2/k∗
)
1/2

kδIk1kδI
c k1
1/2
. (67)
By the definition of index sets G and I, we have I
c ⊂ G and Gc ⊂ I. Then by (65) and (67)
we obtain
kδI
c k2 ≤ (1.2/k∗
)
1/2

kδGc k1kδGk1
1/2 ≤ 1.2kδGc k1
√
k
∗.
By the norm inequality between `1-norm and `2-norm, we have
kδI
c k2 ≤ 1.2kδGc k1/
√
k
∗ ≤ 1.2
p
2s
∗/k∗kδGc k2 ≤ 1.2kδIk2,
where we use k
∗ ≥ 2s
∗ and the induction assumption that |G| ≤ 2s
∗
. Then triangle inequality for `2-norm yields that
kδk2 ≤ kδI
c k2 + kδIk2 ≤ 2.2kδIk2. (68)
Note that (65) and (68) are equivalent to

βb(`)
j − β
∗
j


2
≤ 2.2

βb(`)
I
`
j
− β
∗
I
`
j


2
and

βb(`)
j − β
∗
j


1
≤ 2.2

βb(`)
Ge`
j
− β
∗
Ge`
j


1
,
where Ge`
j =

G`
j
c
, which concludes the proof.    
On Semiparametric Exponential Family Graphical Models
C.3. Proof of Lemma 12
Proof We first show that βb(`)
j
stays in the `1-ball centered at β
∗
j with radius r =
Cρs
∗p
log d/n, where Cρ ≥ 33ρ
−1
∗
. For notational simplicity, we denote δ = βb(`)
j − βbj
and write Sj , G`
j
, J`
j
and I
`
j
as S, G, J an I respectively. We prove by contradiction. Suppose that kδk1 > r, then we define βej = β
∗
j + t(βb(`)
j − β
∗
j
) ∈ R
d−1 with t ∈ [0, 1] such that

βej − β
∗
j


1
≤ r. Letting δe := βej − β
∗
j
, by (68) we obtain
kδek2 = tkδk2 ≤ 2.2tkδIk2 = 2.2kδe
Ik2. (69)
Moreover, by Lemma (11) and the relation between `1- and `2-norms we have
kδek1 = tkδk1 ≤ 2.2tkδGc k1 ≤ 2.2
√
2s
∗kδe
Ik2, (70)
where we use the fact that Gc ⊂ I and the induction assumption that |Gc
| ≤ 2s
∗
. By
Mean-Value theorem, there exists a γ ∈ [0, 1] such that ∇Lj (βej ) − ∇Lj (β
∗
j
) = ∇2Lj (β1)δe,
where β1 := γβ
∗
j + (1 − γ)βej ∈ R
d−1
. In what follows we will derive an upper bound for
kδe
Ik2 from δeT ∇2Lj (β1)δe. Before doing that, we present two lemmas. The first one shows
that the restricted correlation coefficients defined as follows are closely related to the sparse
eigenvalues. This lemma also appear in Zhang (2010) and Zhang et al. (2013) for `2-loss.
Lemma 20 (Local sparse eigenvalues and restricted correlation coefficients) Let m be a
positive integer and M(·): R
m → S
m be a mapping from R
m to the space of m×m symmetric
matrices. We define the s-sparse eigenvalues of M(·) over the `1-ball centered at u0 ∈ R
m
with radius r as
ρ+
M, u0; s, r
= sup
v,u∈Rm

v
TM(u)v: kvk0 ≤ s, kvk2 = 1, ku − u0k1 ≤ r
	
;
ρ−
M, u0; s, r
= inf
v,u∈Rm

v
TM(u)v: kvk0 ≤ s, kvk2 = 1, ku − u0k1 ≤ r
	
.
In addition, we define the restricted correlation coefficients of M over the `1-ball centered
at u0 with radius r as
π
M, u0; s, k, r
:= sup
v,w,u∈Rm

v
T
I M(u)wJ

vI


2
v
T
I M(u)vI

wJ


∞
: I∩J =∅, |I|≤s, |J|≤k,

u−u0


1
≤r

.
Suppose that the local sparse eigenvalue ρ−
M, u0; s+k, r
> 0, then we have the following
upper bound on the restricted correlation coefficient π(M, u0; s, k):
π
M, u0; s, k, r
≤
√
k
2
r
ρ+
M, u0; k, r.
ρ−
M, u0; s+k, r
−1.
Proof See §E.1.1 for a detailed pro       
Yang, Ning, and Liu
We denote the restricted correlation coefficients of ∇2Lj (·) over the `1-ball centered at
β
∗
j with radius r as πj (s1, s2) := π

∇2Lj , β
∗
j
; s1, s2, r
and denote the s-sparse eigenvalues ρ−

∇2Lj , β
∗
j
; s, r
and ρ+

∇2Lj , β
∗
j
; s, r
as ρj−(s) and ρj+(s) respectively. Applying
Lemma 20 to πj (2s
∗+k
∗
, k∗
) we obtain
πj (2s
∗+k
∗
, k∗
) ≤ k
∗1/2
/2 ·
q
ρj+(k
∗)/ρj−(2s
∗+2k
∗) − 1. (71)
By the law of large numbers, if the sample size n is sufficiently large such that ∇2Lj is close
to its expectation E

∇2Lj

. When βj is close to β
∗
j
, by Assumption 4, we expect that the
sparse eigenvalue condition also holds for ∇2Lj (βj ) with high probability. The following
lemma justifies this intuition.
Lemma 21 Recall that we define the sparse eigenvalues of E

∇2Lj (β
∗
j
)

in Definition 3.
Under Assumptions 2 and 4, if n is sufficiently large such that ρ∗ & k
∗λ log2
d, with probability at least 1−(2d)
−1
, for all j ∈ [d], there exists a constant Cρ ≥ 33ρ
−1
∗
such that
ρ
∗
j−(2s
∗+2k
∗
) − 0.05ρ∗ ≤ ρj−(2s
∗+2k
∗
) < ρj+(k
∗
) ≤ ρ
∗
j+(k
∗
) + 0.05ρ∗, and
ρj+(k
∗
)

ρj−(2s
∗+2k
∗
) ≤ 1 + 0.27k
∗
/s∗
,
where we denote the local sparse eigenvalues ρ−

∇2Lj , β
∗
j
; s, r
and ρ+

∇2Lj , β
∗
j
; s, r
with
r = Cρ
p
log d/n as ρj−(s) and ρj+(s), respectively.
Proof See §E.1.2 for a detailed proof.
Thus by Lemma 21 we have
πj (2s
∗+k
∗
, k∗
) ≤ 0.5
q
0.27k
∗2

s
∗. (72)
By (65), (72) and Gc ⊂ I we obtain
1 − 2πj (2s
∗+k
∗
, k∗
)k
∗−1
kδeGk1/kδe
Ik2 ≥ 1 − 1.2
√
0.54 := κ1, (73)
where we denote κ1 := 1 − 1.2
√
0.54 ≥ 0.11. Now we use the second lemma to get an lower
bound of δeT ∇2Lj (β1)δe, which implies an upper bound for kδe
Ik2.
Lemma 22 Let M: R
m → S
m be a mapping from R
m to the space of m × m-symmetric
matrices. Suppose that the sparse eigenvalue ρ−
M, u0; s + k, r
> 0, let the restricted
correlation coefficients of M(·) be defined in Lemma 20. We denote the restricted correlation
coefficients π
M, u0; s, k, r
and s-sparse eigenvalue ρ−
M, u0; s, r
as π(s, k) and ρ−(s)
respectively for notational simplicity. For any v ∈ R
d
, let F be any index set such that
|F
c
| ≤ s, let J be the set of indices of the largest k entries of vF in absolute value and
let I = F
c ∪ J. For any u ∈ R
d
such that ku − u0k2 ≤ r and any v ∈ R
d
satisfying
1 − 2π(s+k, k)kvF k1/kvIk2 > 0 we have
v
TM(u)v ≥ ρ−(s+k)

kvIk2 − 2π(s+k, k)kvF k1

k

kv              
On Semiparametric Exponential Family Graphical Models
Proof See §E.1.3 for a detailed proof.
Now applying Lemma 22 to ∇2Lj (·) with F =G, s= 2s
∗ and k=k
∗ we obtain
δeT ∇2Lj (β1)δe ≥ ρj−(2s
∗+k
∗
)kδe
Ik2

kδe
Ik2 − 2πj (2s
∗+k
∗
, k∗
)/k∗
kδeGk1

. (74)
Then by (73), the right-hand side of (74) can be lower bounded by
δeT ∇2
`(β1)δe ≥ κ1ρj−(2s
∗+k
∗
)kδe
Ik
2
2 ≥ 0.95κ1ρ∗kδe
Ik
2
2 = κ2ρ∗kδe
Ik
2
2
, (75)
where we let κ2 := 0.95κ1 ≥ 0.1. Now we derive an upper bound for δeT ∇2Lj (β1)δe. We
define the symmetric Bregman divergence of Lj (βj ) as Dj (β1, β2) :=


β1 − β2, ∇Lj (β1) −
∇Lj (β2)

, where β1, β2 ∈ R
d−1
. Then by definition, δeT ∇2
`(β1)δe = Dj (βej , β
∗
j
). The following lemma relates Dj (βej , β
∗
j
) with Dj (βbj , β
∗
j
).
Lemma 23 Let Dj (β1, β2) :=


β1 − β2, ∇Lj (β1) − ∇L(β2)

, β(t) = β1 + t(β2 − β1),
t ∈ (0, 1) be any point on the line segment between β1 and β2. Then we have
Dj (β(t), β1) ≤ tDj (β2, β1)
Proof See §E.1.4 for a detailed proof.
By Lemma 23 and (60),
Dj (βej , β
∗
j
) ≤ tDj (βbj , β
∗
j
) ≤ −t


∇Lj (β
∗
j
), δ

| {z }
(i)
−t


δ,λj ◦ ξj

| {z }
(ii)
. (76)
For term (i) in (76), by H¨older’s inequality we have
−t


∇Lj (β
∗
j
), δ

≤ t

∇GcLj (β
∗
j
)


2
kδGc k2 + t

∇GLj (β
∗
j
)


∞
kδGk1
≤

∇GcLj (β
∗
j
)


2
kδe
Ik2 +

∇GLj (β
∗
j
)


∞
kδeGk1, (77)
where the inequality follows from Gc ⊂ I. For term (ii) in (76), by (62) and H¨older’s
inequality we have
−t


δ,λj ◦ ξj

≤ −

δS,(λj ◦ ξj )S

−


|δeG|,λG

≤ kλSk2kδe
Ik2 − p
0
λ
(c2λ)kδeGk1, (78)
where we use the H¨older’s inequality and the definition of G. Combining (75),(77) and (78)
we obtain that
κ2ρ∗

δe
I


2
2
≤

∇GcLj (β
∗
j
)


2
+ kλSk2

kδe
Ik2 +

∇Lj (β
∗
j
)


∞
− p
0
λ
(c2λ)

kδeGk1
≤

∇GcLj (β
∗
j
)


2
+ kλSk2

δe
I


2
,
where the second inequality follows from p
0
λ
(c2λ) >

∇Lj (β
∗
j
)


∞
. From the inequality above
and the induction assumption |Gc
| ≤ 2s
∗ we obtain that

δe
I


2
≤ 10ρ
−1
∗

∇GcLj (β
∗
j
)


2
+ kλSk2

≤ 10ρ
−1
∗
√
s
∗
√
2

∇Lj (β
∗
j
)


∞
+ λ

. (79)        
Yang, Ning, and Liu
Thus (70), (79) and the the fact that 25

∇Lj (β
∗
j
)


∞
≤ λ imply that
kδek1 ≤ 22√
2ρ
−1
∗
(1 + √
2/25)s
∗λ < 33ρ
−1
∗
s
∗λ ≤ r, (80)
where the last inequality follows from the definition of λ. Notice that (80) contradicts our
assumption that kδek1 = r, the reason for this contradiction is because we assume that

βb(`)
j − β
∗
j


1
> r, hence

βb(`)
j − β
∗
j


1
≤ r and βej = βb(`)
j
. This means that βb(`)
j
stays in the
`1-ball centered at β
∗
j with radius r in each iteration.
Moreover, by (68) and (79), we obtain the following upper bound for kδIk2 :
kδk2 ≤ 22ρ
−1
∗

∇GcLj (β
∗
j
)


2
+ kλSk2

≤ 24ρ
−1
∗
√
s
∗λ,
where we use the condition that λ ≥ 25

∇Lj (β
∗
j
)


∞
. In addition, by(65) and (79) we obtain
the following bound on kδk1
kδk1 ≤ 2.2kδGc k1 ≤ 22√
2s
∗ρ
−1
∗

∇GcLj (β
∗
j
)


2
+ kλSk2

≤ 33ρ
−1
∗
s
∗λ, (81)
Therefore going back to the original notations, note that κ2 ≥ 0.1, we establish the following
crude rates of convergence for ` ≥ 1:

βb(`)
j − β
∗
j


2
≤ 24ρ
−1
∗
√
s
∗λ and

βb(`)
j − β
∗
j


1
≤ 33ρ
−1
∗
s
∗λ. (82)
And (79) is equivalent to

βb(`)
I
`
j
− β
∗
I
`
j


2
≤ 10ρ
−1
∗

∇Ge`
j
Lj (β
∗
j
)


2
+

λ
(`−1)
Sj


2

, Ge`
j
:= (G
`
j
)
c
. (83)
Note that we use Lemmas 10 and 21, hence (83) and (83) hold with probability at least
1 − d
−1
for all j ∈ [d].
Appendix D. Proof of Auxiliary Results for Asymptotic Inference
We prove the auxiliary results for asymptotic inference. More specifically, we first prove
Lemma 14, which is pivotal for deriving the limiting distribution of the pairwise score
statistic. Then we prove the lemmas presented in the proof of Theorem 8.
D.1. Proof of Lemma 14
Proof Before proving this lemma, we first let ∇2Ljk
βj∨k

be the Hessian of Ljk
βj∨k

and define Hjk := E

∇2Ljk
β
∗
j∨k
. We also define
Σjk := E

gjk(Xi)gjk(Xi)
T

and Θjk := E

h
jk
ii0(β
∗
)h
jk
ii0(β
∗
)
T

.
Under Assumption 2, we first show that there exists a positive constant D such that for
any j, k ∈ d, j 6= k, max
Σjk

∞
,

Hjk

∞
,

Θjk

∞
	
≤ D. The reason is as follows.
Note that H¨older’s inequality imply

Hjk

∞
. max
j∈[d]
E|Xij − Xi
0j
|
4 . max
j∈[d]
E|Xj |
4
for any j, k ∈ [d], j 6= k.          
On Semiparametric Exponential Family Graphical Models
Similarly, for Θjk
, we also have

Θjk

∞
. max
j∈[d]
E|Xj |
4
. By (14) we have
E|Xj |
4 =
Z ∞
0
P(|X4|
4 > t)dt ≤
Z ∞
0
c exp(−t
1/4
)dt = 24c, c = 2 exp(κm + κh/2).
Moreover, note that by the law of total variance, the diagonal elements of Σjk are no
larger than the corresponding diagonal elements of Θjk; then by Cauchy-Schwarz inequality,
kΣjkk∞ ≤ kΘjkk∞. Therefore there exists a constant D that does not depend on (s
∗
, n, d)
such that
max 
kHjkk∞, kΣjkk∞, kΘjkk∞
	
≤ D, 1≤ j < k≤ d. (84)
Now we are ready to prove the lemma. Recall that ∇Ljk
βj∨k

is a U-statistic with kernel function h
jk
ii0(βj∨k). Because h
jk
ii0

β
∗
j∨k

is centered, the law of total expectation implies
that E

gjk(Xi)

= 0. Note that the left-hand side of (40) can be written as
√
n
2
b
T ∇Ljk
β
∗
j∨k

=
√
n
2
b
T Ujk +
√
n
2
b
T

∇Ljk
β
∗
j∨k

− Ujk
=
1
√
n
Xn
i=1
b
T gjk(Xi)
| {z }
I1
+
√
n
2
b
T

∇Ljk
β
∗
j∨k

− Ujk
| {z }
I2
.
Notice that I1 is a weighted sum of i.i.d. random variables with the mean and variance
given by
E

b
T gjk(Xi)

= 0 and Var
b
T gjk(Xi)

= b
T Σjkb.
Central limit theorem implies that I1 N(0, b
T Σjkb). In what follows we use hii0 and
hii0
|i
to denote h
jk
ii0

β
∗
j∨k

and E

h
jk
ii0

β
∗
j∨k

Xi

= gjk(Xi). Thus we can write I2 as
I2 =
1
√
n(n − 1)
X
i<i0
b
T χii0, where χii0 = (hii0 − hii0
|i − hii0
|i
0).
Then E(I
2
2
) can be expanded as
E(I
2
2
) = 1
n(n − 1)2
X
i<i0
,s<s0
b
TE(χii0χ
T
ss0)b. (85)
By the definition of χii0, we have
E(χii0χ
T
ss0) = E(hii0h
T
ss0) − E(hii0h
T
ss0
|s
) − E(hii0h
T
ss0
|s
0) − E(hii0
|ih
T
ss0)
+ E(hii0
|ih
T
ss0
|s
) + E(hii0
|ih
T
ss0
|s
0) − E(hii0
|i
0h
T
ss0) + E(hii0
|i
0h
T
ss0
|s
) + E(hii0
|i
0h
T
ss0
|s
0). (86)
Therefore, for i 6= s, s0 and i
0 6= s, s0
, law of total expectation implies that E(χii0χ
T
ss0) = 0.
Similarly, if exactly one of i, i0
is identical to one of s, s0
, say i = s, then (86) becomes
E(χii0χ
T
ii00) = E(hii0h
T
ii00) − E(hii0h
T
ii00|i
) − E(hii0
|ih
T
ii00) + E(hii0
|ih
T
ii00|i
), i6=i
0
6=i
00                   
Yang, Ning, and Liu
Note that by the law of total expectation, for each term in (86) we have
E(hii0h
T
ii00) = E(hii0h
T
ii00|i
) = E(hii0
|ih
T
ii00) = E(hii0
|ih
T
ii00|i
).
Therefore, E(χii0χ
T
ii00) = 0. Finally, if i = s and i
0 = s
0
, by the law of total expectation,
(86) can be further reduced to E(χii0χ
T
ii0) = E(hii0h
T
ii0) − E(hii0
|ih
T
ii0
|i
) − E(hii0
|i
0h
T
ii0
|i
0) =
Θjk − 2Σjk
. Thus by triangle inequality we have

E(χii0χ
T
ii0)


∞
≤

E(hii0h
T
ii0)


∞
+

E(hii0
|ih
T
ii0
|i
)


∞
+

E(hii0
|jh
T
ii0
|j
)


∞
≤ 3D,
where the last inequality follows from Assumption 6. Then equation (85) can be reduced
to
E(I
2
2
) = 1
n(n − 1)2
X
i<i0
,s<s0
b
TE(χii0χ
T
ss)b =
1
n(n − 1)2
X
i<i0
b
TE

χii0χ
T
ii0

b.
By H¨older’s inequality we obtain
E(I
2
2
) ≤
1
2(n − 1)kbk1

E(χii0χ
T
ii0)b


∞
≤
1
2(n − 1)kbk
2
1

E(χii0χ
T
ii0)


∞
≤
3D
2(n − 1)kbk
2
1
. (87)
Since kbk0 ≤ se, by the relationship between `1-norm and `2-norm, we can further bound
the right-hand side of (87) by E(I
2
2
) ≤ 1.5sD/ e (n − 1)→ 0, where we use the condition that
limn→∞
s/n e = 0. Therefore, we conclude the proof of Lemma 14.
D.2. Proof of Lemma 13
Proof By the definition of w∗
j,k we have H
j
jk,j\k = w∗
j,k
T H
j
j\k,j\k
. We let βb0
j = (0, βb
j\k
) and
denote ∇2Lj (βb0
j
) and ∇2Lj (β
∗
j
) as Λ and Λ∗
respectively. In addition, we write Hj
, w∗
j,k
and wbj,k as H, w∗ and wb respectively for notational simplicity. Triangle inequality implies
that
kΛjk,j\k−w∗T Λj\k,j\kk∞ ≤ kHjk,j\k−Λjk,j\kk∞+kw∗T
(Hj\k,j\k−Λj\k,j\k
)k∞.
H¨older’s inequality implies that
kΛjk,j\k−w∗T Λj\k,j\kk∞ ≤ kΛ−Hk∞(1 + kw∗
k1). (88)
Under null hypothesis, β
∗
jk = 0. By Lemma 26, we have kΛ−Hk∞ . s
∗λ log2
d. Then the
right-hand side of (88) is bounded by
kΛjk,j\k−w∗T Λj\k,j\kk∞ . (w0 + 1)s
∗λ log2
d.
Therefore, by the assumption that λD & max{1, w0}s
∗λ log2
d we can ensure that w∗
is in
the feasible region of the Dantzig selector problem (11), hence we have kwb k1 ≤ kw∗k1 ≤ w0
4 
On Semiparametric Exponential Family Graphical Models
by the optimality of wb . Let J be the support set of w∗
, that is, J := {(j, `): [w∗
j,k]j` 6=
0, `∈[d], `6=j}; the optimality of w∗
is equivalent to kwb Jc k1 + kwb J k1 ≤ kw∗
J
k1. By triangle
inequality, we have
kwb Jc−w∗
Jc k1 = kwb Jc k1 ≤ kw∗
J k1−kwb J k1 ≤ kwb J −w∗
J k1, (89)
where J
c
:= {(j, `): (j, `) ∈/ J, j fixed}. Letting ωb = wb − w∗
, inequality (89) is equivalent
to kωbJc k1 ≤ kωbJ k1. Moreover, triangle inequality yields that
kΛj\k,j\kωbk∞ ≤ kΛjk,j\k − Λj\k,j\kwb k∞ + kΛjk,j\k − Λj\k,j\kw∗
k∞ ≤ 2λD,
where the last inequality follows from that both w∗ and wb are feasible for the Dantzig
selector problem (11). Then triangle inequality implies that
|ωb
T Λj\k,j\kωb| ≤ |ωb
T
J ΛJ,j\kωb|
| {z }
A1
+ |ω
T
JcΛJc,j\kωb|
| {z }
A2
.
By H¨older’s inequality and inequality between `1-norm and `2-norms, we obtain that
A1 ≤ 2λDkωbJ k1 ≤ 2
p
s
?
0λDkωbJ k2 and A2 ≤ 2λDkωbJc k1 ≤ 2λDkωbJ k1 ≤ 2
p
s
?
0λDkωbJ k2.
Hence we conclude that |ωb
T Λj\k,j\kωb| ≤ 4
p
s
?
0λDkωbJ k2.
We let J1 be the set of indices of the largest k
?
0
component of ωbJc in absolute value
and let I = J1 ∪ J, then |I| ≤ s
?
0 + k
?
0
. Under the null hypothesis, kβb0
j −β
∗
j
k1 = kβb
j\k −
β
∗
j\k
k1 ≤ 33ρ
−1
∗
s
∗λ. We denote the s-sparse eigenvalue of ∇2
j\k,j\k
Lj (βj ) over the `1-ball
centered at β
∗
j with radius r as ρ
0
j+(s) and ρ
0
j−(s) respectively and denote the corresponding
restricted correlation coefficients as π
0
j
(s1, s2). And we denote these quantities of ∇2Lj (β
∗
j
)
as ρj−(s), ρj+(s) and πj (s1, s2). By definition, we immediately have ρj−(s) ≤ ρ
0
j−(s) ≤
ρ
0
j+(s) ≤ ρj+(s).
By Lemma 22 we have
|ωb
T Λj\k,j\kωb| ≥ ρ
0
j−

k
?+s
?
kωbIk2 − 2π
0
j
(s
?+k
?
0
, s?
0
)kωbJc k1

k
?

kωbIk2. (90)
The following lemma relates the sparse eigenvalues of ∇2Lj (βj ) to those of E∇2Lj (β
∗
j
).
Lemma 24 Under Assumptions 2, 4 and 7, if n is sufficiently large such that ρ∗ &
s
∗λ log2
d, with probability at least 1−(2d)
−1
, for all j ∈ [d], there exists a constant Cρ ≥ 33ρ
−1
∗
such that
ρ
∗
j−(2s
?
0+2k
?
0
) − 0.05ν∗ ≤ ρj−(2s
?
0+2k
?
0
) < ρj+(k
?
0
) ≤ ρ
∗
j+(k
?
0
) + 0.05ν∗, and
ρj+(k
?
0
)

ρj−(2s
?
0+2k
?
0
) ≤ 1 + 0.58k
?
0/s?
0
,
where we denote the local sparse eigenvalues ρ−

∇2Lj , β
∗
j
; s, r
and ρ+

∇2Lj , β
∗
j
; s, r
with
r = Cρ
p
log d/n as ρj−(s) and ρj+(s), respectively.
Proof The proof is similar to that of Lemma 3, hence is omitted here.     
Yang, Ning, and Liu
By kωbJc k1 ≤ kωbJ k1 ≤
p
s
?
0
kωbJ k2 and Lemma 24, the right-hand side of (90) can be
reduced to
|ωb
T Λj\k,j\kωb| ≥ 0.95ν∗

kωbIk2 − 2π
0
j
(s
?
0+k
?
0
, s?
)kωbJ k2
p
s
?
0
/k?
0
)kωbIk2. (91)
Using Lemma 20 we obtain
2π
0
j
(s
?
0+k
?
0
, k?
0
)
p
s
?
0
/k?
0 ≤
q
s
?
0
/k?
0
q
ρ
0
j+(k
?
0
)/ρ0
j−(s
?
0+2k
?
0
) − 1
≤
q
s
?
0
/k?
0
q
ρj+(k
?
0
)/ρj−(s
?
0+2k
?
0
) − 1 ≤
q
s
?
0
/k?
0
q
0.58k
?
0
/s?
0 ≤ 0.76.
Thus the right-hand side of (91) can be reduced to
|ωb
T Λj\k,j\kωb| ≥ 0.95ν∗(1 − 0.76kωbJ k2

kωbIk2)kωbIk
2
2 ≥ ν∗κkωbIk
2
2
, (92)
where κ = 0.22. This inequality holds because J ⊂ I. By (92) we have
ν∗κkωbIk
2
2 ≤ 4
p
s
?
0λdkωbJ k2 ≤ 4
p
s
?
0λdkωbIk2, which implies kωbIk2 ≤ 4ν
−1
∗ κ
−1p
s
?
0λD.
Therefore the estimation error of wbj,k can be bounded by
kωbk1 ≤ 2kωbJ k1 ≤ 2
√
s
?kωbJ k2 ≤ 8ν
−1
∗ κ
−1
s
?
0λD ≤ 37ν
−1
∗
s
?
0λD.
Returning to the original notations, we conclude that kwbj,k − w∗
j,kk1 ≤ 37ν
−1
∗
s
?
0λD for all
(j, k) such that j, k∈[d], j 6= k.
D.3. Proof of Lemma 15
Proof We only need to show that σb
2
jk is a consistent estimator of σ
2
jk, which is equivalent
to showing that limn→∞
|σb
2
jk − σ
2
jk| = 0. To begin with, triangle inequality implies that
|σb
2
jk − σ
2
jk| ≤

Σbjk
jk,jk − Σ
jk
jk,jk


| {z }
I1
+2

wb
T
j,kΣbjk
j\k,jk − w∗
j,k
T Σ
jk
j\k,jk


| {z }
I2j
+

wb
T
j,kΣbjk
j\k,j\kwbj,k − w∗
j,k
T Σ
jk
j\k,j\kw∗
j,k


| {z }
I3j
+ 2

wb
T
k,jΣbjk
k\j,jk − w∗
k,j
T Σ
jk
k\j,jk


| {z }
I2k
+

wbk,j
T Σbjk
k\j,k\jwbk,j − w∗
k,j
T Σ
jk
k\j,k\jw∗
k,j


| {z }
I3k
,
where Σbjk = Σbjk
βb0
j∨k

and Σbjk
βj∨k

is defined as
Σbjk(βj∨k) = 1
n
Xn
i=1
n 1
n − 1
X
i
06=i
h
jk
ii0(βj∨k)
o⊗2
. (93)
To prove the consistency of σb
2
jk, we need the following theorem to show that Σbjk is a
consistent estimator of Σjk in the sense that

Σbjk − Σjk

∞
is negligible.   
On Semiparametric Exponential Family Graphical Models
Lemma 25 For 1≤ j < k≤ d, let Σbjk
βj∨k

be defined as (93). Suppose βbj and βb
k are the
estimators of β
∗
j
and β
∗
k
obtained from Algorithm 1 and we denote βb
j∨k = (βb
jk, βbT
j\k
, βbT
k\j
)
T
.
Then Σbjk(βb
j∨k) is a consistent estimator of Σjk
. There exists a constant CΣ that does not
depend on (j, k) such that, with probability tending to one,

Σbjk(βb
j∨k) − Σjk

∞
≤ CΣs
∗λ log2
d for 1≤ j < k≤ d.
Proof See §E.2.1 for a detailed proof.
In the rest of the proof, we will omit the superscripts in both Σbjk and Σjk for notational
simplicity. By Lemma 25,
I1 ≤ kΣb − Σk∞ ≤ OP

s
∗λ log2
d

. (94)
By triangle inequality, we have the following inequality for I2 :
I2j ≤

(wbj,k−w∗
j,k)
T

Σb
j\k,jk−Σj\k,jk

| {z }
I21
+

(wbj,k−w∗
j,k)
T Σj\k,jk


| {z }
I22
+

w∗
j,k
T

Σb
j\k,jk − Σj\k,jk

| {z }
I23
.
By H¨older’s inequality, Lemma 25 and the estimation error of wbj,k, we obtain an upperbound for I21 as follows:
I21 ≤ kwbj,k−w∗
j,kk1kΣb −Σk∞ = OP

s
∗
s
?
0λDλ log2
d

. (95)
Similarly, for I22, H¨older’s inequality implies that
I22 ≤ kwbj,k−w∗
j,kk1kΣk∞ = OP

s
?
0λDD

, (96)
where the constant D appears in (84). For I23, by H¨older’s inequality and 25 we obtain
I23 ≤ kw∗
j,kk1kΣb − Σk∞ = OP

w0s
∗λ log2
d

. (97)
Combining (95), (96) and (97) we have
I2j . (w0 + s
?
0λD)s
∗λ log2
d + s
?
0λD. (98)
For I3j , by triangle inequality we have
I3j ≤

wb
T
j,k
Σb
j\k,j\k−Σj\k,j\k

wbj,k


| {z }
I31
+

wb
T
j,kΣj\k,j\kwbj,k−w∗
j,k
T Σj\k,j\kw∗
j,k


| {z }
I32
.
For term I31, H¨older’s inequality and the optimality of wb implies that
I31 ≤ kwbj,kk
2
1kΣb
j\k,j\k−Σj\k,j\kk∞ ≤ CΣw
2
0
s
∗λ log2
d. (99)
For term I32, Lemma 17 implies that
I32 ≤ kΣj\k,j\kk∞kwbj,k − w∗
j,kk
2
1 + kΣj\k,j\kw∗
j,kk∞kwbj,k − w∗
j,kk1
≤

Dω0s
?
0λD + Ds?
0
2
λ
2
D

,          
Yang, Ning, and Liu
where we use H¨older’s inequality kΣj\k,j\kw∗
j,kk∞ ≤ kw∗
j,kk1kΣk∞ ≤ Dw0. By (99), (100)
and λD & w0s
∗λ log2
d, we obtain
I3j . w
2
0
s
∗λ log2
d +

Dω0s
?
0λD + Ds?
0
2
λ
2
D

. (101)
Therefore combining (94), (98) and (101) we obtain I1 + I2j + I3j = oP(1). We can show
similarly that I2k + I3k = oP(1). Thus limn→∞
max
j<k

σb
2
jk − σ
2
jk

 = 0 with probability converging
to one.
Appendix E. Proof of Technical Lemmas
Finally, we prove the technical lemmas in this appendix. Specifically, we prove the lemmas
introduced to derive the auxiliary results.
E.1. Proof of Technical Lemmas in §C
In this subsection we prove the technical lemmas we use to prove the auxiliary results of
estimation. These lemmas are standard for high-dimensional linear regression, but proving
them for our logistic-type loss function needs nontrivial extensions.
E.1.1. Proof of Lemma 20
Proof Let I and J be two index sets with I ∩ J = ∅, |I| ≤ s, |J| ≤ k, for any u ∈ R
d with
ku − u0k2 ≤ r and any v, w ∈ R
d
, let θ = vI + αwJ with some α ∈ R, then by definition,
kθk0 ≤ s + k. For notational simplicity, we denote s-sparse eigenvalues ρ+
M, u0; s, r) and
ρ−
M, u0; s, r) as ρ−(s) and ρ+(s) respectively. By definition, we have
ρ−(s+k)kθk
2
2 ≤ θ
TM(u)θ = v
T
I M(u)vI
| {z }
A1
+2α v
T
I M(u)wJ
| {z }
A2
+α
2 wT
J M(u)wJ
| {z }
A3
. (102)
Since kθk
2
2 = kvIk
2
2 + α
2kwJ k
2
2
. Rearranging the terms in (102) we have

A3−ρ−(s+k)kwJ k
2
2

α
2 + 2A2α +

A1−ρ−(s+k)kvIk
2
2

≥ 0 for all α ∈ R. (103)
Note that the left-hand side (103) is a univariate quadratic function in α, thus (103) implies
that

A1−ρ−(s+k)kvIk
2
2
A3−ρ−(s+k)kwJ k
2
2

≥ A
2
2
. (104)
Therefore by multiplying 4kvIk∞
2

(A2
1
kwJ k
2
2
) to both sides of (104) we have
4A2
2
kvIk
2
2
A2
1
kwJ


2
2
≤
4kvIk
2
2
A1kwJ k
2
2

A1−ρ−(s+k)

vI


2
2
A1


A3−ρ−(s+k)kwJ k
2
2

. (105)
By the inequality of arithmetic and geometric means, we have
ρ−(s+k)kvIk
2
2
A1

A1−ρ−(s+k)kvIk
2
2
A1

≤
1
4
.             
On Semiparametric Exponential Family Graphical Models
Then the right-hand side of (104) can be bounded by
4A2
2
kvIk
2
2
A2
1
kwJ k
2
2
≤
A3 − ρ−(s+k)kwJ k
2
2
ρ−(s+k)kwJ k
2
2
≤
ρ+(k)
ρ−(s+k)
− 1,
where the last inequality follows from A3 ≤ ρ+(k)kwJ k
2
2
. Note that by the relationship
between `2- and `∞ norm, we have kwJ k2 ≤
√
kkwJ k∞, which further implies that
v
T
I M(u)wJ kvIk2
v
T
I M(u)vIkwJ k∞
≤
√
kv
T
I M(u)wJ kvIk2
v
T
I M(u)vIkwJ k2
=
√
kA2kvIk2
A1kwJ k2
≤
√
k
2
p
ρ+(k)/ρ−(s + k) − 1.
Taking supremum over v, w ∈ R
d finally yields Lemma 20.
E.1.2. Proof of lemma 21
Proof Under Assumption 4, for any βj ∈ R
d−1
such that kβj −β
∗
j
k2 ≤ r and any v ∈ R
d−1
with kvk0 ≤ 2s
∗+ 2k
∗
, we denote ∇2Lj (βj ) − ∇2Lj (β
∗
j
) and ∇2Lj (βj ) − E

∇2Lj (β
∗
j
)

as
Λ1 and Λ2 respectively. Our goal is to show that both |v
T Λ1v| and |v
T Λ2v| are negligible. H¨older’s inequality implies that

v
T Λ2v

 ≤ kvk1kΛ2vk∞ ≤ kvk
2
1
kΛ2k∞. We use the
following lemma to control |v
>Λ1v| and kΛ2k∞.
Lemma 26 We denote s
∗ = maxj∈[d] kβ
∗
j
k0. Let r1(s
∗
, n, d) > 0 be a real number depending on s
∗
, n, and d that satisfy limn→∞
r1(s
∗
, n, d) log2
d = 0. We define Bj (r1) :=

βj ∈
R
d−1
:

βj−β
∗
j


1
≤ r1(s
∗
, n, d)
	
as the `1-ball centered at β
∗
j with radius r1(s
∗
, n, d). Under
Assumptions 2 and 4, there exist absolute constants Ch, Cr > 0 such that, with probability
at least 1 − (2d)
−1
, for all j ∈ [d], βj ∈ Bj (r1) and v ∈ R
d
, it holds that,

∇2Lj (β
∗
j
) − E

∇2Lj (β
∗
j
)


∞
≤ Ch
p
log d/n, (106)

∇2Lj (βj ) − ∇2Lj (β
∗
j
)


∞
≤ Crr1(s
∗
, n, d) · log2
d, (107)

v
T

∇2Lj (βj ) − ∇2Lj (β
∗
j
)v

 ≤ Crr1(s
∗
, n, d) · kvk
2
2
. (108)
Proof See §E.3 for a detailed proof.
Lemma 26 implies that kΛ2k∞ ≤ Ch
p
log d/n with probability at least 1 − (2d)
−1
. By
the relation between `1- and `2-norms, we have

v
T Λ2v

 ≤ (2s
∗+ 2k
∗
)kvk
2
2kΛk∞ ≤ (2s
∗+ 2k
∗
)Ch
p
log d/n.
Moreover, setting r = Cρs
∗p
log d/n with Cρ ≥ 33ρ
−1
∗
, we have
|v
T Λ1v| ≤ CrCρkvk
2
1 ≤ CrCρ(2s
∗+ 2k
∗
)
p
log d/n.
By Assumption 4, if n is large enough such that (2s
∗+ 2k
∗
)(CrCρ +Ch)
p
log d/n
≤ 0.05ρ∗,
then we have
0.95ρ∗ ≤ ρ
∗
j−(2s
∗+ 2k
∗
) − 0.05ρ∗ ≤ ρj−(2s
∗+ 2k
∗
) < ρj+(k
∗
) ≤ ρ
∗
j+(k
∗
) + 0.05ρ∗,
45      
Yang, Ning, and Liu
where we denote the s-sparse eigenvalues ρ−

∇2Lj , β
∗
j
; s, r
and ρ+

∇2Lj , β
∗
j
; s, r
as ρj−(s)
and ρj+(s) respectively. Under Assumption 4, ρ
∗
j+(k
∗
)

ρ
∗
j−(2s
∗+ 2k
∗
) ≤ 1 + 0.2k
∗/s∗ and
k
∗ ≥ 2s
∗
, simple computation yields that
ρj+(k
∗
)
ρj−(2s
∗+ 2k
∗)
≤
ρ
∗
j+(k
∗
) + 0.05ρ∗
ρ
∗
j−(2s
∗+ 2k
∗) − 0.05ρ∗
≤
ρ
∗
j+(k
∗
) + 0.05ρ
∗
j−(2s
∗+ 2k
∗
)
0.95ρ
∗
j−(2s
∗+ 2k
∗)
≤ 1 + 0.27k
∗
/s∗
.
Thus, we conclude the proof of Lemma 26.
E.1.3. Proof of Lemma 22
Proof For v = (v1, . . . , vd)
T ∈ R
d
, without loss of generality, we assume that F
c = [s1]
where s1 =|F
c
| ≤ s. In addition, we assume that when j > s1, vj is arranged in descending
order of |vj |. That is, we rearrange the components of v such that |vj | ≥ |vj+1| for all
j ≥ s1. Let J0 = [s1] and Ji = {s1+ (i − 1)k+1, . . . , min(s1 + ik, d)}. By definition, we
have J = J1 and I = J0 ∪ J1. Moreover, we have kvJi
k∞ ≤ kvJi−1
k1

k when i ≥ 2 because
by the definition of Ji
, we have P
i≥2
kvJi
k∞ ≤ kvF k1

k. Note that by the definition of
index sets I and Ji
, |Ji
| ≤ k and |I| = k+s1 ≤ k+s. We denote the restricted correlation
coefficients π(M, u0; s, k, r) as π(s, k), then by the definition of π(s+k, k) we have

v
T
I M(u)vJi

≤ π(s+k, k)

v
T
I M(u)vI

kvJi
k∞

kvIk2.
Thus we have the following upper bound for

v
T
I M(u)vI
c


:

v
T
I M(u)vI
c

 ≤
X
i≥2

v
T
I M(u)vJi

 ≤ π(s+k, k)kvIk
−1
2

v
T
I M(u)vI
X
i≥2
kvJi
k∞
≤ π(s+k, k)kvIk
−1
2

v
T
I M(u)vI

kvF k1

k. (109)
Because v
TM(u)v ≥ v
T
I M(u)vI + 2v
T
I M(u)vI
c , by (109) we have
v
TM(u)v ≥ v
T
I M(u)vI − 2π(s+k, k)kvIk
−1
2

v
T
I M(u)vI

kvF k1

k
=

v
T
I M(u)vI
1 − 2π(s+k, k)kvIk
−1
2
kvF k1

k

.
Thus we can bound the right-hand side of the last formula using the sparse eigenvalue
condition
v
TM(u)v ≥ ρ−(s+k)

1 − 2π(s+k, k)k
−1
kvIk
−1
2
kvF k1

kvIk
2
2
, (110)
where we denote s-sparse eigenvalue ρ−(M, u0; s, r) as ρ−(s + k) for the simplicity of notations. Inequality (110) concludes the proof of Lemma 22.
E.1.4. Proof of Lemma 23
Proof Let F(t) = Lj

β(t)

− Lj (β1) −


∇Lj (β1), β(t) − β1

. Since the derivative of
Lj

β(t)

with respect to t is 

∇Lj

β(t)

, β2 − β1

, the derivative of F is given by
F
0
(t) = 

∇Lj

β(t)

− ∇Lj (β1), β2 − β1
                    
On Semiparametric Exponential Family Graphical Models
Therefore the Bregman divergence Dj

β(t), β1

can be written as
Dj

β(t), β1

=


∇Lj [β(t)] − ∇Lj (β1), t(β2 − β1)

= tF0
(t).
By definition, it is easy to see that F
0
(1) = Dj (β2, β1). To derive Lemma 23, it suffices to
show that F(t) is convex, which implies that F
0
(t) is non-decreasing and Dj

β(t), β1

=
tF0
(t) ≤ tF0
(1) = tDj (β2, β1).
For ∀t1, t2 ∈ R+, t1 + t2 = 1, x, y ∈ (0, 1), by the linearity of β(t), β(t1x + t2y) =
t1β(x) + t2β(y). Then we have


∇Lj (β1), β(t1x + t2y) − β1

= t1


∇Lj (β1), β(x) − β1

+ t2


∇Lj (β1), β(y) − β1

. (111)
In addition, by convexity of function Lj (·), we obtain
Lj

β(t1x + t2y)

≤ t1Lj

β(x)

+ t2Lj

β(y)

. (112)
Adding (111) and (112) we obtain
F

t1x + t2y

≤ t1F(x) + t2F(y).
Therefore F(t) is convex, thus we have Dj (β(t), β1) ≤ tDj (β2, β1).
E.2. Proof of Technical Lemmas in §D
Now we prove the lemmas that supports the auxiliary inferential results. We first prove
Lemma 25, which implies that the σb
2
jk is a consistent estimator of the asymptotic variance
of σjk.
E.2.1. Proof of lemma 25
Proof Recall that we denote βj∨k = (βjk, βj\k
, βk\j
) and Ljk
βj∨k

= Lj (βj )+Lk(βk). We
denote the kernel function of the second-order U-statistic ∇Ljk
βj∨k

as h
jk
ii0

βj∨k

where
the subscripts i, i0
indicate that h
jk
ii0(·) depends on Xi and Xi
0. We define V
jk
ii0
i
00
βj∨k

:=
h
jk
ii0

βj∨k

h
jk
ii0

βj∨k
T
. Then by definition, Σbjk
βj\k

can be written as
Σbjk
βj∨k

=
1
n(n − 1)2
Xn
i=1
X
i
06=i,i006=i
V
jk
ii0
i
00
βj∨k

.
Note that Σbjk
βj∨k

− Σjk = Σbjk
βj∨k

− Σbjk
β
∗
j∨k

| {z }
I1
+ Σbjk
β
∗
j∨k

− Σjk
| {z }
I2
.
We first consider I2. For notational simplicity, we use hii0 and hii0
|i
to denote h
jk
ij
β
∗
j∨k

and h
jk
ii0
|i

β
∗
j∨k

:= E

h
jk
ij
β
∗
j∨k

Xi

respectively. As shown in §D.1, for i 6= i
0 6= i
00
,
E

hii0h
T
ii00
= E

hii0h
T
ii00

Xi

= E

hii0
|ih
T
ii00|i

= Σjk and E

hijh
T
ij
= Θjk                             
Yang, Ning, and Liu
I2 =
n − 2
n − 1
n
3
−X
1
i<i0<i00

Vii0
i
00−E(Vii0
i
00)


| {z }
I21
+
1
n − 1
n
2
−X
1
i<i0

Vii0
i
0−E(Vii0
i
0)


| {z }
I22
+
1
n − 1

Θjk−Σjk
,
where we use Vii0
i
00 to denote V
jk
ii0
i
00(β
∗
j∨k
). Observing that I21 is a centered third order
U-statistic, for x large enough such that x
4 ≥

E

Vijk(β
∗
j∨k
)


∞
and for any (a, b),(c, d) ∈

(p, q): p, q ∈ {j, k}
	
we have
P
V
jk
ii0
i
00(βj∨k)

ab,cd > 2x
4

≤ P

(Xia − Xi
0a)(Xib − Xi
0b)(Xic − Xi
00c)(Xid − Xi
00d) > x4

≤ 8 exp(2κm + κh) exp(−x).
Thus there exist constants c1 and C1 that does not depend on n or d or (j, k) such that for
any x ∈ R, any i, i0
, i00 ∈ [n] and any j, k ∈ [d],
P

[V
jk
ii0
i
00(β
∗
j∨k
)]ab,cd > x
≤ C1 exp(c1x
1/4
). (113)
This implies that there exists some generic constant C such that kV
jk
ii0
i
00(β
∗
j∨k
)k∞ ≤ C log4
d
for all j, k ∈ [d] and i, i0 ∈ [n] with probability tending to one. Similar to the method we
use in §E.3, we define E :=

kV
jk
ii0
i
00(β
∗
j∨k
)k∞ ≤ C log4
d, ∀i, i0
, i00 ∈ [n], j, k ∈ [d]
	
. By
Bernstein’s inequality for U-statistics (Lemma 19) with b = C log4
d in (56), for some
generic constants C, it holds with high probability that

n
2
−1
X
i<i0

Vii0
i
0−E(Vii0
i
0|E)

≤ C
p
log d/n, ∀j, k ∈ [d], i, i0
, i00 ∈ [n]. (114)
Moreover, by (113), we have
E

[Vii0
i
0(β
∗
j∨k
)]ab,cd|E	
− E

[Vii0
i
00(β
∗
j∨k
)]ab,cd	
≤
Z ∞
C log4 d
P


[V
jk
ii0
i
00(β
∗
j∨k
)]ab,cd

 > x	
≤ c1 log3
d · exp(−c2 log d) (115)
for some absolute constant c1 and c2. Since (115) holds uniformly, we have

n
2
−1
X
i<i0

E(Vii0
i
0|E) − E(Vii0
i
00)

≤ log3
d · exp(−c2 log d) .
p
log d/n. (116)
Combining (114) and (116) we obtain that
kI21k∞ = OP
p
log d/n
uniformly for 1 ≤ j < k ≤ n. (117)
For the second part I22, noting that it is a U-statistic of order 2, because (113) also holds
for Vii0
i
00(β
∗
j∨k
), applying the same technique, we have kI21k∞ = OP
p
log d/n
uniforml                   
On Semiparametric Exponential Family Graphical Models
for 1 ≤ j < k ≤ n. Combining with (117), we conclude that, for some absolute constant C,
we have

Σbjk
β
∗
j∨k

− Σjk

∞
≤ C
p
log d/n, ∀1 ≤ j < k ≤ n. (118)
Now we turn to I1. For any βj , βk ∈ R
d−1
such that kβj − β
∗
j
k1 ≤ r(s
∗
, n, d) and
kβk − β
∗
k
k1 ≤ r(s
∗
, n, d), we denote ω
j
ii0
:= exp
−(Xij − Xi
0j )(βj − β
∗
j
)
T
(Xi\j − Xi
0\j
)

and
denote ω
k
ii0 similarly. Recall that we denote R
j
ii0(βj ) = exp
−(xij − xi
0j )βj
T
(xi\j − xi
0\j
)

.
Hence by definition we have R
j
ii0(βj ) = ω
j
ii0R
j
ii0(β
∗
j
). As shown in §E.3, we have
min{1, ω
j
ii0, ωk
ii0}h
jk
ii0(β
∗
j∨k
) ≤ h
jk
ii0(βj∨k) ≤ max{1, ω
j
ii0, ωk
ii0}h
jk
ii0(β
∗
j∨k
), (119)
where the inequality is taken elementwisely. We denote b := maxi,i0∈[n];j∈[d] r(s
∗
, n, d)

(Xij−
Xi
0j )(Xi\j−Xi
0\j
)


∞
. Note that when kβj−β
∗
j
k1 ≤ r(s
∗
, n, d) and kβk−β
∗
k
k1 ≤ r(s
∗
, n, d),
we have ω
j
ii0, ωk
ii0 ∈ [exp(−b), exp(b)]. Therefore by (119) and the definition of V
jk
ii0
i
00
βj\k

,
we obtain the following elementwise inequality
exp(−2b)V
jk
ii0
i
00
β
∗
j\k

≤ V
jk
ii0
i
00
βj\k

≤ exp(2b)V
jk
ii0
i
00
β
∗
j\k

,
which implies that

Σbjk
βj∨k

− Σbjk
β
∗
j∨k


∞
≤ max
1 − exp(−2b), exp(2b) − 1
	
Σbjk
β
∗
j∨k


∞
. (120)
As we show in §E.3, b ≤ Cr(s
∗
, n, d) log2
d with high probability for some absolute constant
C > 0. Since limn→∞
r(s
∗
, n, d) log2
d = 0, by (120) we have

Σbjk
βj∨k

− Σbjk
β
∗
j∨k


∞
. b

Σbjk
β
∗
j∨k


∞


∞
≤ b

Σbjk
β
∗
j∨k

− Σjk

∞
+ bkΣjkk∞.
Note that we show kI2k∞ =

Σbjk
β
∗
j∨k

− Σjk

∞
= OP
p
log d/n
, which converges to
zero asymptotically. Thus we conclude that

Σbjk
βj∨k

− Σbjk
β
∗
j∨k


∞
= OP

r(s
∗
, n, d) log2
d

. (121)
Combining (118) and (121), we have the following error bound for Σbjk
βj∨k

:

Σbjk
βj∨k

− Σjk

∞
= OP

r(s
∗
, n, d) log2
d +
p
log d/n
for all (j, k). (122)
Finally, by the fact that maxj∈[d] kβbj − β
∗
j
k1 . s
∗λ, we conclude the proof of Lemma 25 by
setting r = Cs∗λ.
E.3. Proof of Lemma 26
Now we turn to the last unproven result, namely Lemma 26, which characterizes the perturbation                       
Yang, Ning, and Liu
Proof Note that ∇2Lj (βj ) is a second-order U-statistic. Hence ∇2Lj (βj ) − E

∇2Lj (βj )

is a centered U-statistic. We denote its kernel as Tii0(βj ), then
∇2Lj (βj ) − E

∇2Lj (βj )

=
2
n(n − 1)
X
i<i0
Tii0(βj ).
Note that

E[Tii0(βj )]


∞
is bounded for all βj ∈ R
d−1 because
max
u∈Rd−1

E[Tii0(βj )]


∞
. max
j∈[d]
E|Xij − Xi
0j
|
4 . max
j∈[d],i∈[n]
E|Xij |
4 ≤
Z ∞
0
c exp(−t
1/4
)dt = 24c,
where c = 2 exp(κm + κh/2). Here the last inequality follows from (14). Let ∇2
jk,j`Lj (βj ) =
∂
2Lj (βj )
∂βjk∂βj`
and let
Tii0(βj )

k` be the corresponding kernel function. That is,
∇2
jk,j`Lj (βj ) =
n
2
−1 P
i<i0

Tii0(βj )

k`. For x > 0 such that x
4 > 24c and k, ` 6= j, we have
P


[Tii0(β
∗
j
)]k`

 > 2x
4
	
≤ P

(Xij − Xi
0j )
2
(Xik − Xi
0k)(Xi` − Xi
0`) > x4

≤ P

|Xij − Xi
0j
| > x
+ P

|Xik − Xi
0k| > x
+ P

|Xi` − Xi
0`
| > x
. (123)
As a direct implication of Assumption 2, we have P

|Xij − Xij | > x
≤ 2 exp(2κm +
κk) exp(−x) for all j ∈ [d]. Then we can bound the right-hand side of (123) by
P


[Tii0(β
∗
j
)]k`

 > 2x
4
	
≤ 6 exp(2κm + κh) exp(−x) when x
4 > 48 exp(κm + κh/2).
Letting CT = maxn
6 exp(2κm + κh), exp
[48 exp(κm + κh/2)]1/4
	o
, it holds that
P


[Tii0(β
∗
j
)]k`

 > x	
≤ CT exp(−2
−1/4x
1/4
) for all x > 0. (124)
Thus by a union bound, we conclude that there exists some generic constant C such that
kTii0(β
∗
j
)k∞ ≤ C log4
d for all j ∈ [d] and i, i0 ∈ [n] with probability at least 1 − (8d)
−1
.
We define an event E :=

kTii0(β
∗
j
)k∞ ≤ C log4
d, ∀i, i0 ∈ [n], j ∈ [d]
	
. By (124), it is easy
to see that Tii0(β
∗
j
) is `2-integrable. By Bernstein’s inequality for U-statistics (Lemma 19)
with b = C log4
d in (56), for some generic constants C1 and C2, we obtain that
P

∇2Lj (βj ) − E1

∇2Lj (βj )

> t

E

≤ 4 exp
−nt2

(C1 + C2 log4
·t)

, ∀j ∈ [d]. (125)
Here we use E1

∇2Lj (βj )

to denote E

∇2Lj (βj )

E

. Thus under Assumption 4 we obtain
that, conditioning on event E,

∇2Lj (βj ) − E1

∇2Lj (βj )


∞
≤ C
p
log d/n, ∀j ∈ [d] (126)
with probability at least 1 − (8d)
−1
. Moreover, by (124) we obtain that
E

[Tii0(β
∗
j
)]k`

E
	
−E

[Tii0(β
∗
j
)]k`	
≤
Z ∞
C log4 d
P


[Tii0(β
∗
j
)]k`

 > x	
≤ c1 log3
d·exp(−c2 log d)
for some absolute constant c1 and c2. Therefore we have

E1

∇2Lj (βj )

− E

∇2Lj (βj )


∞
. log3
d · exp(−c2 log d) .
p
log d/n. (12                              
On Semiparametric Exponential Family Graphical Models
Combining (126) and (127) we show that, with probability at least 1−(4d)
−1
,

∇2Lj (β
∗
j
)−
E[∇2Lj (β
∗
j
)]


∞
≤ Ch
p
log d/n for all j ∈ [d].
For the second argument (107), let ∆ = βj − β
∗
j where βj ∈ R
d−1
lies in the `1-ball
centered at β
∗
j with radius r1(s
∗
, n, d), that is,

βj−β
∗
j


1
≤ r1(s
∗
, n, d). By the independence
between Xi and Xi
0, Assumption 2 implies that
maxn
log E

exp(Xij − Xi
0j )

, log E

exp(Xi
0j − Xij )
o
≤ 2κm + κh,
which further implies that for any x > 0
P

(Xij − Xi
0j )

 > x
≤ 2 exp(2κm + κh) exp(−x), ∀j ∈ [d].
Hence for any x > 0 and j, k ∈ [d], a union bound implies that
P

(Xij − Xi
0j )(Xik − Xi
0k)

 > x2

≤ P

(Xij − Xi
0j )

 > x
+ P

(Xik − Xi
0k)

 > x
≤ 4 exp(2κm + κh) exp(−x). (128)
Taking a union bound over 1≤ j < k≤ d and 1 ≤ i < i0 ≤ n we obtain that
P
h
max
i,i0∈[n];j∈[d]

(Xij − Xi
0j )(Xi\j − Xi
0\j
)


∞
> x2
i
. n
2
d
2
exp(−x).
If we denote b := maxi,i0∈[n];j∈[d] r1(s
∗
, n, d)

(Xij − Xi
0j )(Xi\j − Xi
0\j
)


∞
, then we obtain
that b ≤ Cr1(s
∗
, n, d) log2
d with probability at least 1 − (4d)
−1
for some constant C > 0.
Denoting ωii0 := exp
−(Xij − Xi
0j )∆T
(Xi\j − Xi
0\j
)
	
, by definition,
R
j
ii0(βj ) = exp
−(Xij − Xi
0j )(∆ + β
∗
j
)
T
(Xi\j − Xi
0\j
)
	
= ωii0R
j
ii0(β
∗
j
).
Thus we can write ∇2Lj (βj ) as:
∇2Lj (βj ) = 2
n(n − 1)
X
i<i0
R
j
ii0(β
∗
)(Xij − Xi
0j )
2
(Xi\j − Xi
0\j
)
⊗2

1 + R
j
ii0(β∗)
2
ωii0

1 + R
j
ii0(β
∗
)
2

1 + ωii0R
j
ii0(β∗)
2
.
(129)
If ωii0 ≥ 1, then (ωii0)
−2 ≤

1 + R
j
ii0(β
∗
)
21 + ωii0R
j
ii0(β
∗
)
2 ≤ 1; otherwise we have
1 ≤

1 + R
j
ii0(β)
2
/

1 + ωii0R
j
ii0(β
∗
)
2 ≤ (ωii0)
−2
. This observation implies
min
ωii0, 1/ωii0
	
≤
ωii0

1 + R
j
ii0(β)
2

1 + ωii0R
j
ii0(β∗)
2 ≤ max
ωii0, 1/ωii0
	
. (130)
By the definition of ωii0, H¨older’s inequality implies that

(Xij −Xi
0j )∆T
(Xi\j −Xi
0\j
)

 ≤ b,
thus we have
exp(−b) ≤ min
ωii01/ωii0
	
≤ max
ωii0, 1/ωii0
	
≤ exp(b).                    
Yang, Ning, and Liu
Combining (129),(130) and (131) we obtain
exp(−b)∇2Lj (β
∗
j
) ≤ ∇2Lj (βj ) ≤ exp(b)∇2Lj (β
∗
j
). (132)
Then by (132), since limn→∞
r1(s
∗
, n, d) log2
d = 0, we have

∇2Lj (βj ) − ∇2Lj (β
∗
j
)


∞
≤ max
1 − exp(−b), exp(b) − 1
	
∇2Lj (β
∗
j
)


∞
. b

∇2Lj (β
∗
j
)


∞
.
Notice that under Assumption 2, as shown in §D.1, we can assume that

E

∇2Lj (β
∗
j
)


∞
≤
D where D appears in (84). By triangle inequality,

∇2Lj (β
∗
j
)


∞
≤

∇2Lj (β
∗
j
)−E

∇2Lj (β
∗
j
)


∞
+

E

∇2Lj (β
∗
j
)


∞
≤ D+Ch
p
log d/n ≤ 2D
with probability at least 1 − (4d)
−1
, where the last inequality follows from the fact that

log9
d/n1/2
tends to zero as n goes to infinity. Then we obtain that

∇2Lj (βj ) − ∇2Lj (β
∗
j
)


∞
≤ Crr1(s
∗
, n, d) log2
d
holds for some absolute constant Cr > 0 and uniformly for all j ∈ [d] and βj ∈ Bj (r1) with
probability at least 1 − (2d)
−1
.
Finally, for the last argument (108), for any v ∈ R
d−1
, by (132) we have
exp(−b)v
T ∇2Lj (β
∗
j
)v ≤ v
T ∇2Lj (βj )v ≤ exp(b)v
T ∇2Lj (β
∗
j
)v.
Thus we have

v
T

∇2Lj (βj ) − ∇2Lj (β
∗
j
)

v

 . b

v
T ∇2Lj (β
∗
j
)v

 ≤ bkvk
2
1

∇2Lj (β
∗
j
)


∞
,
which implies (108).
