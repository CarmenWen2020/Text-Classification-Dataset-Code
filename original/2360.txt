We propose a new class of semiparametric exponential family graphical models for the analysis of high dimensional mixed data. Different from the existing mixed graphical models, we
allow the nodewise conditional distributions to be semiparametric generalized linear models
with unspecified base measure functions. Thus, one advantage of our method is that it is
unnecessary to specify the type of each node and the method is more convenient to apply
in practice. Under the proposed model, we consider both problems of parameter estimation
and hypothesis testing in high dimensions. In particular, we propose a symmetric pairwise
score test for the presence of a single edge in the graph. Compared to the existing methods
for hypothesis tests, our approach takes into account of the symmetry of the parameters,
such that the inferential results are invariant with respect to the different parametrizations
of the same edge. Thorough numerical simulations and a real data example are provided
to back up our theoretical results.
Keywords: Graphical Models, Exponential Family, High Dimensional Inference
1. Introduction
Given a d-dimensional random vector X = (X1, . . . , Xd)
T
, inferring the conditional independence among X and quantifying its uncertainty are important tasks in statistics. We
propose a unified framework for modeling, estimation, and uncertainty assessment for a
new type of graphical model, named as semiparametric exponential family graphical model.
Let G = (V, E) be an undirected graph with node set V = {1, 2, . . . , d} and edge set
E âŠ† {(j, k): 1 â‰¤ j < k â‰¤ d}. The semiparametric exponential family graphical model
specifies the joint distribution of X such that for each j âˆˆ V, the conditional distribution
of Xj given X\j
:= (X1, . . . , Xjâˆ’1, Xj+1, . . . , Xd)
T
is of the form
p(xj | x\j
) = exp
Î·j (x\j
) Â· xj + fj (xj ) âˆ’ bj (Î·j , fj )

, (1)

where x\j = (x1, . . . , xjâˆ’1, xj+1, . . . , xd), Î·j (x\j
) = Î±j +
P
k6=j
Î²jkxk is the canonical parameter, fj (Â·) is an unknown base measure function, and bj (Â·, Â·) is the log-partition function.
Besides, we assume Î²jk = Î²kj for all j 6= k. By definition, the unknown parameter contains {(Î±j , Î²jk, fj ): 1 â‰¤ j < k â‰¤ d}. To make the model identifiable, we set Î±j = 0 and
absorb the term Î±jxj into fj (xj ). By the Hammersley-Clifford theorem (Besag, 1974), we
have Î²jk 6= 0 if and only if Xj and Xk are conditionally independent given {X`
: ` 6= j, k}.
Therefore, we set (j, k) âˆˆ E if and only if Î²jk 6= 0. The graph G thus characterizes the
conditional independence relationship among the high dimensional distribution of X. The
key feature of the proposed model is that (1) it is a general semiparametric model and (2)
it can be used to handle mixed data, which means that X may contain both continuous
and discrete random variables. Unlike the existing mixed graphical models, we allow the
nodewise conditional distributions to be semiparametric generalized linear models with unspecified base measure functions. Thus, our method does not need to specify the type of
each node and is more convenient to apply in practice. In addition to the proposed new
model, our paper has the following two novel contributions.
First, for the purpose of estimating Î²jk, we extend the multistage relaxation algorithm
(Zhang, 2010) and conduct a localized analysis for a more sophisticated loss function obtained by a statistical chromatography method (Liang and Qin, 2000; Diao et al., 2012;
Chan, 2012; Ning et al., 2017b). The gradient and Hessian matrix of the loss function are
nonlinear U-statistics with unbounded kernel functions. This makes our technical analysis more challenging than that in Zhang (2010). Under the assumption that the sparse
eigenvalue condition holds locally, we prove the same optimal statistical rates for parameter
estimation as in high dimensional linear models.
Second, we propose a symmetric pairwise score test for the null hypothesis H0 : Î²jk =
0. This is equivalent to testing whether Xj and Xk are conditionally independent given
{X`
: ` 6= j, k}. Compared with Ning et al. (2017b), the novelty of our method is that
we consider a more sophisticated cross type inference which incorporates the symmetry
of the parameter, i.e., Î²jk = Î²kj . By considering this unique structure of the graphical
model, our proposed method achieves the invariance property of the inferential results.
That means the same p-values are obtained for testing Î²jk = 0 and Î²kj = 0. In contrast,
the asymmetric method in Ning et al. (2017b) may lead to different conclusions for testing
these two equivalent null hypotheses.
1.1. Related Works
There is a huge literature on estimating undirected graphical models (Lauritzen, 1996;
Edwards, 2000; Whittaker, 2009). For modeling continuous data, the most commonly used
methods are Gaussian graphical models (Yuan and Lin, 2007; Banerjee et al., 2008; Friedman
et al., 2008; Ravikumar et al., 2011; Rothman et al., 2008; Lam and Fan, 2009; Shen et al.,
2012; Yuan, 2010; Cai et al., 2011; Sun and Zhang, 2013; Guo et al., 2011; Danaher et al.,
2014; Mohan et al., 2014; Meinshausen and BÂ¨uhlmann, 2006; Peng et al., 2009; Friedman
et al., 2010). To relax the Gaussian assumption, Liu et al. (2009); Xue et al. (2012b); Liu
et al. (2012); Ning and Liu (2013) propose the Gaussian copula model and Voorman et al.
(2014) study the joint additive models for graph estimation. For modeling binary data,
the Ising graphical model is considered by Lee et al. (2006); HÂ¨ofling and Tibshirani (2009);
2
On Semiparametric Exponential Family Graphical Models
Ravikumar et al. (2010); Xue et al. (2012a); Cheng et al. (2014). In addition to binary
data, Allen and Liu (2012) and Yang et al. (2013b) consider the Poisson data and Guo
et al. (2015) consider the ordinal data. Moreover, Yang et al. (2013a) propose exponential
family graphical models, and Tan et al. (2014) propose a general framework for graphical
models with hubs.
Recently, modeling the mixed data attracts increasing interests (Lee and Hastie, 2015;
Fellinghauer et al., 2013; Cheng et al., 2017; Chen et al., 2015; Fan et al., 2017; Yang et al.,
2014). Compared with Lee and Hastie (2015); Cheng et al. (2017); Chen et al. (2015); Yang
et al. (2014), our model has the following two main advantages. First, it is a semiparametric
model, which does not need to specify the parametric conditional distribution for each node.
Therefore, it provides a more flexible modeling framework than the existing ones. Second,
under our proposed model, the estimation and inference methods are easier to implement.
Unlike these existing methods, we propose a unified estimation and inference procedure,
which does not need to distinguish whether the node satisfies the Gaussian distribution
or the Bernoulli distribution. In addition, our estimation and inference methods are more
efficient than the nonparametric approach in Fellinghauer et al. (2013). Finally, our method
is more convenient for modeling the count data than the latent Gaussian copula approach
in Fan et al. (2017).
Though significant progress has been made towards developing new graph estimation
procedures, the research on uncertainty assessment of the estimated graph lags behind.
In low dimensions, Drton et al. (2007); Drton and Perlman (2008) establish confidence
subgraph of Gaussian graphical models. In high dimensions, Ren et al. (2015); JankovÂ´a
and van de Geer (2015); Gu et al. (2015) study the confidence interval for a single edge
under Gaussian (copula) graphical models and Liu et al. (2013) study the false discovery
rate control. However, all these methods rely on the Gaussian or sub-Gaussian assumption
and cannot be easily applied to the discrete data and more generally the mixed data in high
dimensions.
1.2. Notation
We adopt the following notation throughout this paper. For any vector v = (v1, . . . , vd)
T âˆˆ
R
d
, we define its support as supp(v) = {t: vt 6= 0}. We define its `0-norm, `p-norm, and `âˆ-
norm as kvk0 = |supp(v)|, kvkp = (P
jâˆˆ[d]
|vj |
p
)
1/p and kvkâˆ = maxjâˆˆ[d]
|vj |, respectively,
where p > 1. Let v
âŠ—2 = vvT be the Kronecker product of a vector v and itself. We write
v â—¦ u = (v1u1, . . . , vdud)
T as the Hadamard product of two vectors u, v âˆˆ R
d
. In addition,
we use |v| = (|v1|, . . . , |vd|)
T
to denote the elementwise absolute value of vector v and define
kvkmin = minjâˆˆ[d]
|vj |. For any matrix A = [ajk] âˆˆ R
d1Ã—d2
, let AS1S2 =[ajk]jâˆˆS1,kâˆˆS2 be the
submatrix of A with indices in S1Ã—S2; let Aj\j = [ajk]k6=j
. Besides, let kAk2, kAk1, kAkâˆ,
kAk`p be the spectral norm, elementwise `1-norm, elementwise `âˆ-norm, and operator `pnorm of A, respectively. Furthermore, for two matrices A1 and A2, we write A1  A2 if
A2âˆ’A1 is positive semidefinite and write A1 â‰¤ A2 if every entry of A2âˆ’A1 is nonnegative.
For a function f(x): R
d â†’ R, we write âˆ‡f(x), âˆ‡Sf(x), âˆ‡2f(x) and âˆ‚f(x) as the gradient
of f(x), the gradient of f(x) with respect to xS, the Hessian of f(x), and the subgradient of
f(x), respectively. Moreover, we write {1, 2, . . . , d} as [d]. For a sequence of random vectors
{Yi}iâ‰¥1 and a random vector Y , we write Yi Y if {Yi}iâ‰¥1 converges to Y in distribution.
3
Yang, Ning, and Liu
Finally, for functions f(n) and g(n), we write f(n) . g(n) to denote that f(n)â‰¤cg(n) for a
universal constant câˆˆ(0,+âˆ) and we write f(n)  g(n) when f(n) . g(n) and g(n) . f(n)
hold simultaneously.
1.3. Paper Organization
The rest of this paper is organized as follows. In Â§2 we introduce the semiparametric
exponential family graphical models. In Â§3 we present our methods for graph estimation
and uncertainty assessment. In Â§4 we lay out the assumptions and main theoretical results.
We study the finite-sample performance of our method on both simulated and real-world
datasets in Â§5 and conclude the paper in Â§6 with some discussion.
2. Semiparametric Exponential Family Graphical Models
The semiparametric exponential family graphical models are defined by specifying the conditional distribution of each variable Xj given the rest of the variables {Xk : k 6= j}.
Definition 1 (Semiparametric exponential family graphical model) A d-dimensional
random vector X = (X1, . . . , Xd)
T âˆˆ R
d
follows a semiparametric exponential graphical
model with graph G = (V, E) if for any node j âˆˆ V, the conditional density of Xj given X\j
satisfies
p(xj |x\j
) = exp
xj (Î²
T
j x\j
) + fj (xj ) âˆ’ bj (Î²j , fj )

, (2)
where fj (Â·) is an unknown base measure function and bj (Â·, Â·) is a known log-partition function. In particular, (j, k) âˆˆ E if and only if Î²jk 6= 0.
This model is semiparametric since we treat both Î²j = (Î²j1, . . . , Î²jjâˆ’1, Î²jj+1, . . . , Î²jd)
T âˆˆ
R
dâˆ’1 and the univariate function fj (Â·) as parameters, where Î²j and fj (Â·) are the parametric
and nonparametric components, respectively. Because the model in Definition 1 is only
specified by the conditional distributions of each variable, it is important to understand
the conditions under which a valid joint distribution of X exists. This problem has been
addressed by Chen et al. (2015). As shown in their Proposition 1, one sufficient condition
for the existence of joint distribution of X is that, (i) Î²jk = Î²kj for 1 â‰¤ j, k â‰¤ d and (ii)
g(x) := expP
j<k Î²jkxjxk +
Pd
j=1 fj (xj )

is integrable.
Hereafter, we assume that the above two conditions hold. Thus, there exists a joint
probability distribution for the model defined in (2), whose density has the form of
p(x) = expX
k<`
Î²k`xkx` +
X
d
j=1
fj (xj ) âˆ’ A

{Î²i
, fi}iâˆˆ[d]


, (3)
where Î²k` 6= 0 if and only if (k, `) âˆˆ E. Here A(Â·) is the log-partition function given by
A

{Î²i
, fi}iâˆˆ[d]

:= logZ
Rd
expX
k<`
Î²k`xkx` +
X
d
j=1
fj (xj )

Î½(dx)

, (4)
where Î½(Â·) is a product measure satisfying Î½(dx) = Q
jâˆˆ[d]
Î½j (dxj ), and each Î½j is either a
Lebesgue or a counting measure on the domain of Xj , depending whether Xj is discrete or      
On Semiparametric Exponential Family Graphical Models
continuous. Since Î²k` = Î²`k for all pairs of nodes (k, `), in the sequel, we will use Î²k` and
Î²`k interchangeably for notational simplicity.
Furthermore, we remark that, without the knowledge of {fj}jâˆˆ[d]
, estimating parameters
{Î²j}jâˆˆ[d]
is insufficient to learn the distribution of X. In this paper, we focus on the
statistical inference of the underlying conditional independence graph specified by {Î²j}jâˆˆ[d]
.
In the next section, by adopting a loss function for {Î²j}jâˆˆ[d]
that is free of the base measures,
we obtain estimators of these parameters, which are used to construct an estimator of the
underlying graph. Moreover, by further considering the hypothesis testing problem for each
Î²jk, we are able to assess the uncertainty of the estimated graph.
2.1. Examples
We provide some widely used parametric examples in the class of semiparametric exponential family graphical models.
Gaussian Graphical Models: The Gaussian graphical models assume that X âˆˆ R
d
follows a multivariate Gaussian distribution N(0, Î˜âˆ’1
), where Î˜ âˆˆ R
dÃ—d
is the precision
matrix satisfying Î˜jj = 1 for j âˆˆ [d]. The conditional distribution of Xj given X\j
satisfies
Xj | X\j = Î±
T
j X\j + j with j âˆ¼ N(0, 1),
where Î±j = Î˜\j,j . The conditional density is given by
p(xj | x\j
) = p
1/(2Ï€) exp
âˆ’xj (Î˜T
\j,jx\j
) âˆ’ 1/2 Â· x
2
j âˆ’ 1/2 Â· (Î˜T
\j,jx\j
)
2

.
Compared with (2), we obtain Î²j = âˆ’Î˜\j,j , fj (x) = âˆ’x
2/2 and bj (Î²j , fj ) = (Î²
T
j x\j
)
2/2 +
log(2Ï€)/2.
Ising Models: In an Ising model with no external field, X takes value in {0, 1}
d and the
joint probability mass function p(x) âˆ exp(P
j<k Î¸jkxjxk). Let Î¸j = (Î¸j1, . . . , Î¸j,jâˆ’1, Î¸j,j+1, . . . , Î¸jd)
T
.
The conditional distribution of Xj given X\j
is of the form
p(xj | x\j
) = expP
k<` Î¸k`xkx`

P
xjâˆˆ{0,1}
expP
k<` Î¸k`xkx`
 = expn
xj

Î¸
T
j x\j

âˆ’ log
1 + exp(Î¸
T
j x\j
)
o
.
Therefore, in this case we have Î²j = Î¸j , fj (x) = 0 and bj (Î²j , fj ) = log[1 + exp(Î²
T
j x\j
)].
Exponential Graphical Models: For exponential graphical models, X takes values in
[0, +âˆ)
d and the joint probability density satisfies p(x) âˆ exp(âˆ’
Pd
j=1 Ï†jxjâˆ’
P
k<` Î¸k`xkx`).
In order to ensure that this probability distribution is normalizable, we require that Ï†j >
0, Î¸jk â‰¥ 0 for all j, k âˆˆ [d]. Then we obtain the following conditional probability density of
Xj given X\j
:
p(xj | x\j
) = exp
âˆ’
X
d
k=1
Ï†kxk âˆ’
X
k<`
Î¸k`xkx`
Z
xjâ‰¥0
exp 
âˆ’
X
d
k=1
Ï†kxk âˆ’
X
k<`
Î¸k`xkx`

dxj
= exp
âˆ’xj

Ï†j + Î¸
T
j x\j

âˆ’ log
Ï†j + Î¸
T
j x\j
.
Thus, we have Î²j = âˆ’Î¸j , fj (x) = âˆ’Ï†jx and bj (Î²j , fj ) = log(Î²
T
j x\j + Ï†j            
Yang, Ning, and Liu
Poisson Graphical Models: In a Poisson graphical model, every node Xj is a discrete
random variable taking values in N = {0, 1, 2, . . .}. The joint probability mass function is
given by
p(x) âˆ expX
d
j=1
Ï†jxj âˆ’
X
d
j=1
log(xj !) +X
k<`
Î¸k`xkx`

.
Similar to the exponential graphical models, we also need to impose some restrictions on
the parameters so that the probability mass function is normalizable. Here we require that
Î¸jk â‰¤ 0 for all j, k âˆˆ [d]. By direct computation, the conditional probability mass function
of Xj given X\j
is given by
p(xj | x\j
) = exp
xj

Î¸
T
j x\j

+ Ï†jxj âˆ’ log(xj !) âˆ’ bj (Î¸j , fj )

,
where we have Î²j = Î¸j , fj (x) = Ï†jx âˆ’ log(x!) and bj (Î²j , fj ) = logPâˆ
y=0 exp
y(Î²
T
j x\j
) +
fj (y)
	.
3. Graph Estimation and Uncertainty Assessment
In this section, we lay out the procedures for graph estimation and uncertainty assessment.
Throughout our analysis, we use {Î²
âˆ—
i
, f âˆ—
i
}iâˆˆ[d]
to denote the true parameters, and E(Â·) to
denote the expectation with respect to the joint density in (3) with the true parameters. We
first introduce a pseudo-likelihood loss function for the parametric components {Î²j}
d
j=1 that
is invariant to the nuisance parameters {fj}jâˆˆ[d]
. Based on such a loss function, we present
an Adaptive Multi-stage Convex Relaxation algorithm to estimate each Î²
âˆ—
j
by minimizing
the loss function regularized by a nonconvex penalty function. We then proceed to introduce
the inferential procedure for accessing the uncertainty of a given edge in the graph.
3.1. A Nuisance-Free Loss Function
For graph estimation, we treat Î²j as the parameter of interest and the base measures fj (Â·)
as nuisance parameter. Let X1, . . . , Xn be n i.i.d. copies of X. Due to the presence of
fj (Â·), finding the conditional maximum likelihood estimator of Î²j is intractable. To solve
this problem, we exploit a pseudo-likelihood loss function proposed in Ning et al. (2017b)
that is invariant to the nuisance parameters {fj}jâˆˆ[d]
. This pseudo-likelihood loss is based
on pairwise local order statistics, which have been previously studied in Liang and Qin
(2000); Diao et al. (2012); Chan (2012) for semiparametric regression models. More details
are presented as follows.
Let x1, x2, . . . , xn be n data points that are realizations of X1, X2, . . . , Xn. For any
1 â‰¤ i < i0 â‰¤ n, let
A
j
ii0
:=

(Xij , Xi
0j ) = (xij , xi
0j ), Xi\j = xi\j
, Xi
0\j = xi
0\j
	
be the event that we observe Xi\j = xi\j and Xi
0\j = xi
0\j and the order statistics of Xij and
Xi
0j (but not the relative ranks of Xij and Xi
0j ). More specifically, we denote max{Xij , Xi
0j}
and min{Xij , Xi
0j} by O1 and O2, and let o1 and o2 be the observed values of O1 and O2.
Then A
j
ii0 can be equivalently written as {O1 = o1, O2 = o2, Xi\j = xi\j
, Xi
0\j = xi
0\j}.
     
On Semiparametric Exponential Family Graphical Models
Let R âˆˆ {(1, 2),(2, 1)} be the relative rank of Xij and Xi
0j
, and r be the observed value.
Then, by definition, we have
P

Xij = xij , Xi
0j = xi
0j

 Xi\j = xi\j
, Xi
0\j = xi
0\j

= P

O1 = o1, O2 = o2

 Xi\j = xi\j
, Xi
0\j = xi
0\j

Â· P

R = r

 A
j
ii0

.
Furthermore, we have
P

R = r

 A
j
ii0

=
"
1 +
P(Xij = xi
0j
, Xi
0j = xij

 A
j
ii0)
P(Xij = xij , Xi
0j = xi
0j

 A
j
ii0)
#âˆ’1
=
"
1 +
P(Xij = xi
0j
, Xi
0j = xij

 Xi\j = xi\j
, Xi
0\j = xi
0\j
)
P(Xij = xij , Xi
0j = xi
0j

 Xi\j = xi\j
, Xi
0\j = xi
0\j
)
#âˆ’1
=

1 + R
j
ii0(Î²j )
âˆ’1
,
(5)
where R
j
ii0(Î²j ) := exp[âˆ’(xij âˆ’ xi
0j )Î²j
T
(xi\j âˆ’ xi
0\j
)]. Based on the conditional likelihood
in (5), we construct the following pseudo-likelihood loss function for Î²j :
Lj (Î²j ) := 2
n(n âˆ’ 1)
X
1â‰¤i<i0â‰¤n
log
1 + R
j
ii0(Î²j )

. (6)
Obviously, Lj (Â·) only involves Î²j . Since its form resembles the logistic loss, to find a
minimizer of this loss function, we could readily apply any logistic regression solver.
3.2. Adaptive Multi-stage Convex Relaxation Algorithm
Now we are ready to present the algorithm for parameter estimation. For high dimensional
sparse estimation, to promote sparsity, we minimize the sum of the loss functions Lj (Î²j ) and
some penalty function. Two of the most prevalent methods are the LASSO (`1-penalization)
(Tibshirani, 1996) and the folded concave penalization (Fan et al., 2014). Although the `1-
penalization enjoys good computational properties as a convex optimization problem, it is
known to incur significant estimation bias for parameters with large absolute values (Zhang
and Huang, 2008). In contrast, nonconvex penalties such as smoothly clipped absolute
deviation (SCAD) penalty, minimax concave penalty (MCP) and capped-`1 penalty can
eliminate such bias and attain improved rates of convergence. Therefore, we consider the
nonconvex optimization problem
Î²bj = argmin
Rdâˆ’1

Lj (Î²j ) +X
k6=j
pÎ»(|Î²jk|)

, (7)
where Î» > 0 is a regularization parameter and pÎ»(Â·) : [0, +âˆ) â†’ [0, +âˆ) is a penalty
function satisfying the following three conditions:
(C.1) The penalty function pÎ»(u) is continuously nondecreasing and concave with pÎ»(0) = 0.
(C.2) The right-hand derivative at u = 0 satisfies p
0
Î»
(0) = p
0
Î»
(0+) = Î»        
Yang, Ning, and Liu
(C.3) There exist constants c1 âˆˆ [0, 1] and c2 âˆˆ (0, +âˆ) such that p
0
Î»
(u+) â‰¥ c1Î» for u âˆˆ
[0, c2Î»].
Note that we only require the penalty function to be right-differentiable. In what follows, we
denote by p
0
Î»
(u) the right-hand derivative. By (C.1), p
0
Î»
(u) is nonincreasing and nonnegative
in [0, âˆ). It is easy to verify that SCAD, MCP and capped-`1 penalty all satisfy (C.1)â€“(C.3).
Due to the penalty term, the optimization problem in (7) is nonconvex and may have
multiple local solutions. To overcome such difficulty, we exploit the local linear approximation algorithm (Zou and Li, 2008; Fan et al., 2014) or equivalently, the multi-stage convex
relaxation (Zhang, 2010; Zhang et al., 2013; Fan et al., 2018) to attain an estimator of
Î²
âˆ—
j
. Compared with previous works that mainly focus on sparse linear regression, our loss
function Lj (Î²j ) is a U-statistics based logistic loss, which requires nontrivial extensions of
the existing theoretical analysis.
We present the proposed adaptive multi-stage convex relaxation method in Algorithm 1.
Our algorithm solves a sequence of convex optimization problems corresponding to finer and
finer convex relaxations of the original nonconvex optimization problem. More specifically,
for each j = 1, . . . , d, in the first iteration, step 4 of Algorithm 1 is equivalent to a `1-
regularized optimization problem and we obtain the first-step solution Î²b(1)
j
. Then, in each
subsequent iteration, we solve an adaptive `1-regularized optimization problem where the
weights of the penalty depend on the solution of the previous step. For example, in the
`-th iteration, the regularization parameter Î»
(`âˆ’1)
jk in (8) is updated using the (` âˆ’ 1)-th
step estimator Î²b(`âˆ’1)
j
. Note that p
0
Î»

|Î²
(`)
jk |

is the right-hand derivative of pÎ»(u) evaluated
at u = |Î²
(`)
jk |.
Since the optimization problem in step 4 is convex, our method is computationally
efficient. Besides, note that (8) with ` = 1 corresponds to the `1-regularized problem.
Hence, our approach can be viewed as a refinement of LASSO. As we will show in Â§4.1,
the estimator Î²bj of Î²
âˆ—
j
constructed by Algorithm 1 attains the optimal statistical rates of
convergence for parameter estimation.
Algorithm 1 Adaptive Multi-stage Convex Relaxation algorithm for parameter estimation
1: Initialize Î»
(0)
jk = Î» for 1 â‰¤ j, k â‰¤ d.
2: for j= 1,2,. . . ,d do
3: for ` = 1, 2, . . . , until convergence do
4: Solve the convex optimization problem
Î²b(`)
j = argmin
Rdâˆ’1
n
Lj (Î²j ) +X
k6=j
Î»
(`âˆ’1)
jk |Î²jk|
o
. (8)
5: Update Î»
(`)
jk by Î»
(`)
jk = p
0
Î»
(|Î²b(`)
jk |) for 1 â‰¤ k â‰¤ d, k 6= j.
6: end for
7: Output Î²bj =Î²b(`)
j
, where ` is the number of iterations until convergence is attained.
8: end for
 
On Semiparametric Exponential Family Graphical Models
3.3. Graph Inference: Composite Pairwise Score Test
For any given 1 â‰¤ j < k â‰¤ d, we are interested in testing if (j, k) âˆˆ E, i.e., we consider
the hypothesis testing problem H0 : Î²
âˆ—
jk = 0 versus H1 : Î²
âˆ—
jk 6= 0. To simplify the notation,
we write Î²j\k = (Î²j1, . . . , Î²jjâˆ’1, Î²jj+1, . . . , Î²jkâˆ’1, Î²jk+1, . . . , Î²jd)
T âˆˆ R
dâˆ’2 and denote the
parameters associated with node j and node k by Î²jâˆ¨k :=

Î²jk; Î²
T
j\k
, Î²
T
k\j
T
âˆˆ R
2dâˆ’3
. In
addition, let Hj
:= E

âˆ‡2Lj (Î²
âˆ—
j
)

be the expected Hessian of Lj (Î²j ) evaluated at Î²
âˆ—
j
. We
define two submatrices H
j
jk,j\k
and H
j
j\k,j\k
of Hj as
H
j
jk,j\k
:=
"
E
âˆ‚
2Lj (Î²
âˆ—
j
)
âˆ‚Î²jkâˆ‚Î²jv #
v6=k
âˆˆ R
dâˆ’2
and H
j
j\k,j\k
:=
"
E
âˆ‚
2Lj (Î²
âˆ—
j
)
âˆ‚Î²juâˆ‚Î²jv #
u6=k,v6=k
âˆˆ R
(dâˆ’2)Ã—(dâˆ’2)
,
and we define Hk
jk,k\j
and Hk
k\j,k\j
similarly. Furthermore, we define
wâˆ—
j,k = H
j
jk,j\k

H
j
j\k,j\k
âˆ’1
and wâˆ—
k,j = Hk
jk,k\j

Hk
k\j,k\j
âˆ’1
. (9)
Following the general approach in Ning et al. (2017a); Neykov et al. (2018), the composite
pairwise score function for parameter Î²jk is defined as
Sjk(Î²jâˆ¨k) = âˆ‡jkLj (Î²j ) + âˆ‡jkLk(Î²k) âˆ’ wâˆ—
j,k
T âˆ‡j\kLj (Î²j ) âˆ’ wâˆ—
k,j
T âˆ‡k\jLk(Î²k). (10)
where we write âˆ‡jkLj (Î²j ) = âˆ‚Lj (Î²j )/âˆ‚Î²jk and âˆ‡j\kLj (Î²j ) = âˆ‚Lj (Î²j )/âˆ‚Î²j\k
. Here, the
last two terms in (10) are constructed to reduce the effect of nuisance parameters Î²j\k and
Î²k\j on assessing the uncertainty of Î²
âˆ—
jk, which is the parameter of interest. A key feature
of Sjk(Î²jâˆ¨k) is that the symmetry of Î²jk and Î²kj (i.e., Î²jk = Î²kj ) is taken into account,
which is distinct from the existing works such as Ren et al. (2015); JankovÂ´a and van de
Geer (2015); Liu et al. (2013) for Gaussian graphical models and Ning et al. (2017b) in the
regression setup.
Note that both wâˆ—
j,k and wâˆ—
k,j are computed from H, which is unknown. We estimate
them using the Dantzig-type estimators (CandÂ´es et al., 2007). Specifically, we define the
empirical versions of H
j
jk,j\k
and H
j
j\k,j\k
as
âˆ‡2
jk,j\kLj (Î²j ) = "
âˆ‚
2Lj (Î²j )
âˆ‚Î²jkâˆ‚Î²jv #
v6=k
and âˆ‡2
j\k,j\kLj (Î²j ) = "
âˆ‚
2Lj (Î²j )
âˆ‚Î²juâˆ‚Î²jv #
u6=k,v6=k
.
We also define âˆ‡2
jk,k\j
Lk(Î²k) and âˆ‡2
k\j,k\j
Lk(Î²k) similarly. Then we estimate wâˆ—
j,k by
solving
wbj,k = argmin kwk1 such that

âˆ‡2
jk,j\kLj (0, Î²b
j\k
) âˆ’ wT âˆ‡2
j\k,j\kLj (0, Î²b
j\k
)


âˆ
â‰¤ Î»D,
(11)
where Î²bj is the estimator of Î²
âˆ—
j
obtained from Algorithm 1 and Î»D is a regularization
parameter. An estimator wbk,j of wâˆ—
k,j can be similarly obtained. Based on wbj,k and wbk,j ,
we construct the composite pairwise score statistic for Î²
âˆ—
jk by
Sbjk = âˆ‡jkLj (0, Î²b
j\k
) + âˆ‡jkLk

0, Î²b
k\j

âˆ’ wb
T
j,kâˆ‡j\kLj

0, Î²b
j\k

âˆ’ wb
T
k,jâˆ‡k\jLk

0, Î²b
k\j

.
(12          
Yang, Ning, and Liu
Comparing (10) and (12), we see that Sbjk is obtained by replacing Î²j and Î²k in (10) by
(0, Î²b
j\k
) and (0, Î²b
k\j
) respectively and replacing wâˆ—
j,k and wâˆ—
k,j in (10) by wbj,k and wbk,j .
To obtain a valid hypothesis test, we need to establish the limiting distribution of Sbjk
under the null hypothesis. Note that Sbjk is a linear combination of entries of âˆ‡Lj (Î²j ) and
âˆ‡Lk(Î²k), both of which are U-statistics. In the next section, we prove the asymptotic normality of Sbjk. More specifically, under the null hypothesis, we have âˆš
nSbjk/2 N(0, Ïƒ2
jk),
where the limiting variance can be estimated consistently by Ïƒb
2
jk (More details will be explained in the following section). With a significance level Î± âˆˆ (0, 1), the test function
Ïˆjk(Î±) is defined as
Ïˆjk(Î±) = (
1 if


âˆš
nSbjk
(2Ïƒbjk)

 > Î¦
âˆ’1
(1 âˆ’ Î±/2)
0 if


âˆš
nSbjk
(2Ïƒbjk)

 â‰¤ Î¦
âˆ’1
(1 âˆ’ Î±/2)
, (13)
where Î¦(t) is the cumulative distribution function of a standard normal random variable.
In sum, the composite pairwise score test for the null hypothesis H0 : Î²
âˆ—
jk = 0 consists
of the following four steps: (i) Calculate Î²bj and Î²b
k from Algorithm 1; (ii) Obtain wbj,k
and wbk,j by solving two Dantzig-type problems defined in (11); (iii) Compute the limiting
variance Ïƒb
2
jk; (iv) Evaluate the test function (13).
4. Theoretical Properties
In this section, we present our theoretical results. We first prove that the proposed procedure
attains the optimal rate of convergence for parameter estimation. Then, we provide theory
for the composite pairwise score test.
4.1. Theoretical Results for Parameter Estimation
We first establish the rates of convergence of the adaptive multi-stage convex relaxation
estimator. We begin by listing several required assumptions. The first is about moment
conditions of {Xj} and the local smoothness of the log-partition function A(Â·) defined in
(4). This assumption also appears in Yang et al. (2013a) and Chen et al. (2015) as a pivotal
technical condition for theoretical analysis.
Assumption 2 For all j âˆˆ [d], we assume that the first two moments of Xj are bounded.
That is, there exist two constants Îºm and Îºv such that |E(Xj )| â‰¤ Îºm and E(X2
j
) â‰¤ Îºv.
Denote the true parameters by {Î²
âˆ—
j
, f âˆ—
j
}jâˆˆ[d] and define d univariate functions AÂ¯
j (Â·): R â†’ R
as
AÂ¯
j (u) := logZ
Rd
exp
uxj +
X
k<`
Î²
âˆ—
k`xkx` +
X
d
i=1
f
âˆ—
i
(xi)

dÎ½(x)

, j âˆˆ [d].
We assume that there exists a constant Îºh such that maxu: |u|â‰¤1 AÂ¯00
j
(u) â‰¤ Îºh for all j âˆˆ [d].
Unlike the Ising graphical models, {Xj}jâˆˆ[d] are not bounded in general for semiparametric exponential family graphical models. Instead, we impose mild conditions as in
10
On Semiparametric Exponential Family Graphical Models
Assumption 2 to obtain a loose control of the tail behaviors of the distribution of X. As
shown in Yang et al. (2013a), Assumption 2 implies that for all j âˆˆ [d],
max
log E[exp(Xj )], log E[exp(âˆ’Xj )]	
â‰¤ Îºm + Îºh/2.
Markov inequality implies for any x > 0,
P

|Xj | â‰¥ x

â‰¤ 2 exp(Îºm + Îºh/2) Â· exp(âˆ’x). (14)
Thus, by setting x = C log d in (14) with constant C sufficiently large, we have kXkâˆ â‰¤
C log d with high probability. In addition to Assumption 2, we also impose conditions to
control the curvature of function Lj (Â·).
Definition 3 (Sparse eigenvalue condition) For any j, s âˆˆ [d], we define the s-sparse
eigenvalues of E[âˆ‡2Lj (Î²
âˆ—
j
)] as
Ï
âˆ—
j+(s) := sup
vâˆˆRdâˆ’1

v
TE

âˆ‡2Lj (Î²
âˆ—
j
)

v: kvk0 â‰¤ s, kvk2 = 1	
;
Ï
âˆ—
jâˆ’(s) := inf
vâˆˆRdâˆ’1

v
TE

âˆ‡2Lj (Î²
âˆ—
j
)

v: kvk0 â‰¤ s, kvk2 = 1	
.
Assumption 4 Let s
âˆ— = maxjâˆˆ[d] kÎ²
âˆ—
j
k0. We assume that for any j âˆˆ [d], there exist an
integer k
âˆ— â‰¥ 2s
âˆ—
satisfying limnâ†’âˆ
k
âˆ—
(log9
d/n)
1/2 = 0 and a positive number Ïâˆ— such that the
sparse eigenvalues of E[âˆ‡2Lj (Î²
âˆ—
j
)] satisfy
0 < Ïâˆ— â‰¤ Ï
âˆ—
jâˆ’(2s
âˆ—+ 2k
âˆ—
) < Ïâˆ—
j+(k
âˆ—
) < +âˆ and
Ï
âˆ—
j+(k
âˆ—
)

Ï
âˆ—
jâˆ’(2s
âˆ—+ 2k
âˆ—
) â‰¤ 1 + 0.2k
âˆ—
/sâˆ—
for any j âˆˆ [d].
The condition Ï
âˆ—
j+(k
âˆ—
)

Ï
âˆ—
jâˆ’(2s
âˆ—+2k
âˆ—
) â‰¤ 1+0.2k
âˆ—/sâˆ—
requires the eigenvalue ratio Ï
âˆ—
j+(k)/Ïâˆ—
jâˆ’(2k+
2s
âˆ—
) to grow sub-linearly in k. Assumption 4 is commonly referred to as sparse eigenvalue
condition, which is standard for sparse estimation problems and has been studied by Bickel
et al. (2009); Raskutti et al. (2010); Zhang (2010); Negahban et al. (2012); Xiao and Zhang
(2013); Loh and Wainwright (2015) and Wang et al. (2014). Our assumption is similar to
that in Zhang (2010) and is weaker than the restricted isometry property (RIP) proposed
in CandÂ´es and Tao (2005). We claim that this assumption is true in general and will be
verified for Gaussian graphical models in the appendix.
Now we are ready to present the main theorem of this section. Recall that the penalty
function pÎ»(u) satisfies conditions (C.1)â€“(C.3) in Â§3.2. We use p
0
Î»
(u) to denote its right-hand
derivative. For convenience, we will set p
0
Î»
(u) = 1 when u < 0.
Theorem 5 (`2- and `1-rates of convergence) For all j âˆˆ [d], we define the support of
Î²
âˆ—
j
as Sj
:=

(j, k): Î²
âˆ—
jk 6= 0, k âˆˆ [d]
	
and let s
âˆ— = maxjâˆˆ[d] kÎ²
âˆ—
j
k0. Let Ïâˆ— > 0 be defined in
Assumption 4. Under Assumptions 2 and 4, there exists an absolute constant K > 0 such
that kâˆ‡Lj (Î²
âˆ—
j
)kâˆ â‰¤ K
p
log d/n, âˆ€j âˆˆ [d] with probability at least 1 âˆ’ (2d)
âˆ’1
. Moreover,
the penalty function pÎ»(Â·) in (7) satisfies (C.1)â€“(C.3) listed in Â§3.2 with c1 = 0.91 and
c2 â‰¥ 24/Ïâˆ— for condition (C.3). We set the regulization parameter Î» = C
p
log d/n with
C â‰¥ 25K. We denote constants % = c2(c2Ïâˆ— âˆ’11)âˆ’1
, A1 = 22%, A2 = 2.2c2, B1 = 32%,
1     
Yang, Ning, and Liu
B2 = 3.2c2, Î³ = 11c
âˆ’1
2
Ï
âˆ’1
âˆ— < 1, and define Î¥j
:= [P
(j,k)âˆˆSj
p
0
Î»
(|Î²
âˆ—
jk|âˆ’c2Î»)
2
]
1/2
. Then, with
probability at least 1âˆ’d
âˆ’1
, we have the following statistical rates of convergence:

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ A1

âˆ‡SjLj (Î²
âˆ—
j
)


2
+ Î¥j

+ A2
âˆš
s
âˆ—Î»Î³`
and (15)

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ B1
âˆš
s
âˆ—

âˆ‡SjLj (Î²
âˆ—
j
)


2
+ Î¥j

+ B2s
âˆ—Î»Î³`
, âˆ€j âˆˆ [d]. (16)
By Theorem 5, the statistical rates are dominated by the second term if p
0
Î»
(|Î²
âˆ—
jk|âˆ’c2Î»)
is not negligible. If the signal strength is large enough such that p
0
Î»
(Î² âˆ’c2Î») = 0 where
Î² =min(j,k)âˆˆSj
|Î²
âˆ—
jk|, after sufficient number of iterations, the statistical rates will be of the
order

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
= OP

âˆ‡SjLj (Î²
âˆ—
j
)


2

and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
= OP
âˆš
s
âˆ—

âˆ‡SjLj (Î²
âˆ—
j
)


2

.
However, if the signals are uniformly small such that p
0
Î»

|Î²
âˆ—
jk|âˆ’c2Î»

> 0 for all (j, k) âˆˆ Sj ,
the rates of convergence will be of the order

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
= OP
âˆš
s
âˆ—Î»

and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
= OP

s
âˆ—Î»

,
which are identical to the `2- and `1-rates of the LASSO estimator, respectively (Ning
et al., 2017b). Thus c2Î» can be viewed as the threshold of signal strength. Therefore, after
sufficient numbers of iterations, the final estimator Î²bj obtained by Algorithm 1 attains the
following more refined rates of convergence:

Î²bj âˆ’Î²
âˆ—
j


2
= OP

âˆ‡SjLj (Î²
âˆ—
j
)


2
+Î¥j

and

Î²bj âˆ’Î²
âˆ—
j


1
= OP
âˆš
s
âˆ—

âˆ‡SjLj (Î²
âˆ—
j
)


2
+Î¥j


.
These statistical rates of convergence are optimal in the sense that they cannot be improved
in terms of the order.
Finally, we comment that the sparsity level s
âˆ—
in (15) and (16) can be replaced by
the sparsity level of each Î²
âˆ—
j
. Let s
âˆ—
j = kÎ²
âˆ—
j
k0 be the sparsity level of Î²
âˆ—
j
and Î»j be the
regularization parameter for optimization problem (7) such that Î»j  kâˆ‡Lj (Î²
âˆ—
j
)kâˆ. The
statistical rates of convergence for each Î²b(`)
j
can be improved to

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
= OP
q
s
âˆ—
j
Î»j

and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
= OP

s
âˆ—
jÎ»j

.
We use the uniform sparsity level s
âˆ— =maxjâˆˆ[d] s
âˆ—
j
and the same regularization parameter Î»
for simplicity, but the proof can be easily adapted to individual s
âˆ—
j
and Î»j for each j âˆˆ[d].
4.2. Theoretical Results for Composite Pairwise Score Test
In the composite pairwise score test for the null hypothesis H0 : Î²
âˆ—
jk = 0, we construct the
test statistic by combining the loss functions Lj (Â·) and Lk(Â·) together because Î²jk appears
in both Lj (Î²j ) and Lk(Î²k) (recall that we use Î²jk and Î²kj interchangeably). In the sequel,
we present the theoretical results that guarantee the validity of the proposed inferential
method.
Recall that we define the pairwise score function Sjk(Î²jâˆ¨k) and the pairwise score statistic Sbjk in (10) and (12) respectively. According to a fixed pair of nodes (j, k), entries             
On Semiparametric Exponential Family Graphical Models
Î²j and Î²k can be categorized into three types: (i) Î²jk, (ii) Î²j\k = (Î²j`; ` 6= k)
T
, and (iii)
Î²k\j = (Î²k`; ` 6= j)
T
. Recall that we write Î²jâˆ¨k = (Î²jk, Î²
T
j\k
, Î²
T
k\j
)
T
for notational simplicity.
Moreover, letting Ljk
Î²jâˆ¨k

:= Lj (Î²j ) + Lk(Î²j ), the entries of âˆ‡Ljk
Î²jâˆ¨k

are given by
âˆ‡jkLjk(Î²jâˆ¨k) = âˆ‡jkLj (Î²j ) + âˆ‡kjLk(Î²k); âˆ‡j\kLjk(Î²jâˆ¨k) = âˆ‡j\kLj (Î²j ), and
âˆ‡k\jLjk(Î²jâˆ¨k) = âˆ‡k\jLk(Î²k).
Let Î²bj and Î²b
k be the estimators of Î²
âˆ—
j
and Î²
âˆ—
k
obtained from Algorithm 1. Note that we
can write the pairwise score function Sjk(Â·) and the test statistic Sbjk as
Sjk(Î²jâˆ¨k) = âˆ‡jkLjk
Î²jâˆ¨k

âˆ’ wâˆ—
j,k
T âˆ‡j\kLjk
Î²jâˆ¨k

âˆ’ wâˆ—
k,j
T âˆ‡k\jLjk
Î²jâˆ¨k

and (17)
Sbjk = âˆ‡jkLjk
Î²b0
jâˆ¨k

âˆ’ wb
T
j,kâˆ‡j\kLjk
Î²b0
jâˆ¨k

âˆ’ wb
T
k,jâˆ‡k\jLjk
Î²b0
jâˆ¨k

, (18)
where we write Î²b0
jâˆ¨k
:=

0, Î²bT
j\k
, Î²bT
k\j
)
T
, wâˆ—
j,k and wâˆ—
k,j are defined in (9), wbj,k is obtained
from the Dantzig-type problem in (11), and wbk,j can be obtained similarly. To derive
the asymptotic distribution of Sbjk under the null hypothesis, we first show that âˆš
n

Sbjk âˆ’
Sjk(Î²
âˆ—
jâˆ¨k
)

= oP(1). Then the problem is reduced to finding the limiting distribution of
Sjk(Î²
âˆ—
jâˆ¨k
) under H0. Thanks to its structure of being a U-statistics, we can characterize
the limiting distribution of Sjk(Î²
âˆ—
jâˆ¨k
) using the method of HÂ´ajek projection (Van der Vaart,
2000), which approximates a U-statistic with a sum of independent random variables.
To begin with, we denote the kernel functions of âˆ‡Lj (Î²j ), âˆ‡Lk(Î²k) and âˆ‡Ljk(Î²jâˆ¨k) as
h
j
ii0(Î²j ), h
k
ii0(Î²k) and h
jk
ii0(Î²jâˆ¨k) respectively. It can be shown that E[h
j
ii0(Î²
âˆ—
j
)] = E[h
k
ii0(Î²
âˆ—
k
)] =
0; hence h
jk
ii0(Î²
âˆ—
jâˆ¨k
) is also centered. We define
gjk(Xi) := n/2 Â· E

âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

Xi

= E

h
jk
ii0

Î²
âˆ—
jâˆ¨k

Xi

and (19)
Ujk :=
2
n
Xn
i=1
gjk(Xi) = Xn
i=1
E

âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

Xi

. (20)
Thus 2/n Â· gjk
Xi

is the projection of âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

onto the Ïƒ-filed generated by Xi and
Ujk is the HÂ´ajek projection of âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

. Under mild conditions, Ujk in (20) is a good
approximation of âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

, which enables us to characterize the limiting distribution
of Sjk(Î²
âˆ—
jâˆ¨k
). We present the following assumption that guarantees the non-degeneracy of
gjk
Xi

.
Assumption 6 Under Assumption 2, for gjk(Xi) defined in (19), we denote the covariance
matrix of gjk(Xi) as Î£jk := E[gjk(Xi)gjk(Xi)
T
]. We assume that there exists a constant
cÎ£ > 0 such that Î»min(Î£jk) â‰¥ cÎ£ for all 1â‰¤j < kâ‰¤d.
Assumption 6 requires the minimum eigenvalue of Î£jk to be bounded away from 0, which
implies Var(v
T Ujk) â‰¥ 4cÎ£ for all v âˆˆ R
2dâˆ’3 with kvk2 = 1. Thus, this assumption guarantees the asymptotic variance of âˆš
nSjk(Î²
âˆ—
jâˆ¨k
) is bounded away from 0. We also present the
following assumption that specifies the scaling of the Dantzig selector pro                         
Yang, Ning, and Liu
Assumption 7 We assume that Hj
is invertible for all j âˆˆ [d]. In addition, we assume that
there exist an integer s
?
0
and a positive number w0 such that kwâˆ—
j,kk0 â‰¤ s
?
0 âˆ’1 and kwâˆ—
j,kk1 â‰¤
w0. Besides, the regularization parameter Î»D in (11) satisfies Î»D  max{1, w0}s
âˆ—Î» log2
d.
Moreover, we assume that
limnâ†’âˆ
(1+w0+w
2
0
)s
âˆ—Î» log2
d = 0, limnâ†’âˆ
(1+w0)s
?
0Î»D = 0, and limnâ†’âˆ
âˆš
n(s
âˆ—+s
?
0
)Î»Î»D = 0.
(21)
In addition, recall that we denote the s-sparse eigenvalues of E[âˆ‡2Lj (Î²
âˆ—
j
)] by Ï
âˆ—
jâˆ’(s) and
Ï
âˆ—
j+(s). We further assume that there exist an integer k
?
0 â‰¥ s
?
0
and a positive number Î½âˆ—
such that
limnâ†’âˆ
k
?
0

log9
d/n1/2 = 0, 0 < Î½âˆ— â‰¤ Ï
âˆ—
jâˆ’(s
?
0 + k
?
0
) < Ïâˆ—
j+(k
?
0
) â‰¤

1 + 0.5k
?
0/s?
0

Î½âˆ—, 1 â‰¤ j â‰¤ d.
If we can treat w0 as a constant, and k
âˆ— and k
?
0
is of the same order of s
âˆ— and s
?
0
,
respectively, Assumption 7 is reduced to Î»D  s
âˆ—Î» log2
d, s?
0Î»D = o(1), s
âˆ—Î» log2
d = o(1),
and (s
âˆ—+s
?
)Î»Î»D = o(n
1/2
). Since Î» 
p
log d/n, we can choose Î»D = Csâˆ—
(log5
d/n)
1/2 with
a sufficiently large C, provided (s
âˆ— + s
?
0
)(log9
d/n)
1/2 = o(1), s
?
0
s
âˆ—
(log5
d/n)
1/2 = o(1), and
(s
âˆ— + s
?
0
)s
âˆ—
log3
d/n = o(n
âˆ’1/2
). Hence this condition is fulfilled if
log d = o

min
(
âˆš
n/sâˆ—
2/9
,(
âˆš
n/s?
0
)
2/9
,(
âˆš
n/sâˆ—2
)
1/3
,(
âˆš
n/sâˆ—
s
?
)
1/3
	

.
Now we are ready to present the main theorem of composite pairwise score test.
Theorem 8 Under the Assumptions 2, 4, 6 and 7, it holds uniformly for all j 6= k and
j, k âˆˆ [d] that âˆš
nSbjk =
âˆš
nSjk
Î²
âˆ—
jâˆ¨k

+ oP(1). Furthermore, we let Î²b0
jâˆ¨k = (0, Î²bT
j\k
, Î²bT
k\j
)
T
and define Î£bjk := n
âˆ’1 Pn
i=1
(nâˆ’1)âˆ’1 P
i
06=i h
jk
ii0(Î²b0
jâˆ¨k
)
	âŠ—2
, where h
jk
ii0

Î²jâˆ¨k

is the kernel
function of the second-order U-statistic âˆ‡Ljk(Î²jâˆ¨k). In addition, we define Ïƒbjk by
Ïƒb
2
jk := Î£bjk
jk,jk âˆ’ 2Î£bjk
jk,j\kwbj,k âˆ’ 2Î£bjk
jk,k\jwbk,j + wb
T
j,kÎ£bjk
j\k,j\kwbj,k + wb
T
k,jÎ£bjk
k\j,k\jwbk,j .
Then, under the null hypothesis H0 : Î²
âˆ—
jk = 0, we have âˆš
nSbjk
(2Ïƒbjk) N(0, 1).
By Theorem 8, to test the null hypothesis H0 : Î²
âˆ—
jk = 0 against the alternative hypothesis H1 : Î²
âˆ—
jk 6= 0, we reject H0 if the studentized test statistic âˆš
nSbjk
(2Ïƒbjk) is too
extreme. Recall that the test function of the composite pairwise score test with significance level Î± is deboted by Ïˆjk(Î±) in (13). The associated p-value is defined as p
jk
Ïˆ
:=
2

1 âˆ’ Î¦


âˆš
nSbjk
(2Ïƒbjk)


. By Theorem 8, under H0, we have
limnâ†’âˆ
P

Ïˆjk(Î±) = 1 | H0

= Î± and p
jk
Ïˆ Unif[0, 1] under H0,
where Unif[0, 1] is the uniform distribution over [0, 1].
We note that our inferential approach is still valid if we replace Î²b0
jâˆ¨k
in (18) by other
estimators of Î²
âˆ—
jâˆ¨k
, provided such an estimator converges to Î²
âˆ—
jâˆ¨k
at an appropriate statistical rate. Our theory still holds after simple modification on the proof when controlling
the order of the remainder term        
On Semiparametric Exponential Family Graphical Models
Remark 9 There are a number of recent works on the uncertainty assessment for high dimensional linear models or generalized linear models with `1-penalty; see Lee et al. (2016);
Lockhart et al. (2014); Belloni et al. (2012, 2013); Zhang and Zhang (2014); Javanmard
and Montanari (2014); van de Geer et al. (2014). These works utilize the convexity and
the Karush-Kuhn-Tuker conditions of the LASSO problem. Compared with these works,
our pairwise score test is constructed using a nonconvex penalty function and is applicable
to a larger model class. Ning et al. (2017b) consider the score test for `1-penalized semiparametric generalized linear models in the regression setting. Compared with this work, we
adopt a composite score test with a nonconvex penalty and relax many technical assumptions including the bounded covariate assumption. For nonconvex penalizations, Fan and Lv
(2011); Bradic et al. (2011) establish the asymptotic normality for the low dimensional and
nonzero parameters in high dimensional models based on the oracle properties. However,
their approach depends on the minimal signal strength assumption, which is not needed in
our approach.
5. Numerical Results
In this section we study the finite-sample performance of the proposed graph inference
methods on both simulated and real-world datasets.
5.1. Simulation Studies
We first examine the numerical performance of the proposed pairwise score tests for the
null hypothesis H0 : Î²
âˆ—
jk = 0. We simulate data from the following three settings:
(i) Gaussian graphical model. We set n = 100 and d = 200. The graph structure is
a 4-nearest-neighbor graph, that is, for j, k âˆˆ [d], j 6= k, node j is connected with
node k if |j âˆ’ k| = 1, 2, d âˆ’ 2, d âˆ’ 1. More specifically, we sample X1, . . . , Xn from a
Gaussian distribution Nd(0, Î£). For the precision matrix Î˜ = Î£âˆ’1
, we set Î˜jj = 1,

Î˜jk

 = Âµ âˆˆ [0, 0.25) for |j âˆ’ k| = 1, 2, dâˆ’2, dâˆ’1 and Î˜jk = 0 for 2 â‰¤ |j âˆ’ k|â‰¤ d âˆ’ 2.
Note that Âµ denotes the signal strength of the graph inference problem and Âµ â‰¤ 0.25
ensures that Î˜ is diagonal dominant and invertible.
(ii) Ising graphical model. We set n = 100 and d = 200. The graph structure is a
10 Ã— 20 grid with the sparsity level s
âˆ— = 4. We use Markov Chain Monte Carlo
method (MCMC) to simulate n data from an Ising model with joint distribution
p(x) âˆ expP
j6=k
Î²
âˆ—
jkxjxk

(using the package IsingSampler (Epskamp, 2015)). We
set |Î²
âˆ—
jk| = Âµ âˆˆ [0, 1] if there exists an edge connecting node j and node k, and Î²
âˆ—
jk = 0
otherwise.
(iii) Mixed graphical model. We set n = 100 and d = 200. The graph structure is a
10Ã—10Ã—2 grid with the sparsity level s
âˆ— = 5. We set the nodes in the first layer to
be binomial and nodes in the second layer to be Gaussian. We set |Î²
âˆ—
jk| = Âµ âˆˆ [0, 1] if
there exists an edge connecting node j and node k, and Î²
âˆ—
jk = 0 otherwise. We refer
to Lee and Hastie (2015) for details.
We denote the true parameters of the graphical models as {Î²
âˆ—
jk, j 6= k}. We also denote
Î²
âˆ—
j = (Î²
âˆ—
j1
, . . . , Î²âˆ—
jd)
T
. For the Gaussian graphical model, we have Î²
âˆ—
jk = Î˜jk. We first
1 
Yang, Ning, and Liu
obtain a point estimate of Î²
âˆ—
j
by solving (7) using Algorithm 1 with the capped-`1 penalty
pÎ»(u) = Î» min{u, Î»}. The parameter Î» is chosen by 10-fold cross validation as suggested by
Ning et al. (2017b).
Recall that the form of the loss function Lj (Î²j ) is exactly the loss function for logistic regression, where we use Rademacher random variables yii0 as response and yii0(xij âˆ’
xi
0j )Î²
T
j
(xi\j âˆ’ xi
0\j
) as covariates, Algorithm 1 can be easily implemented by using the
`1-regularized logistic regression such as the PICASSO package (Ge et al., 2017). In particular, the algorithm converges quickly after a few iterations, indicating that it attains a good
balance between computational efficiency and statistical accuracy. Once Î²bj is obtained, we
solve the Dantzig-type problem (11) using Î²bj as input. We set the regularization parameter
Î»D to be 1. In practice, the performance of the proposed method is not very sensitive to
the choice of Î»D.
To examine the performance of our semiparametric modeling approach, we compare
the pairwise score test with the desparsity method in van de Geer et al. (2014). Although
this method is proposed for hypothesis tests in generalized linear models (GLMs), it can
be adapted for graphical models by performing nodewise regression, assuming the base
measures {fj}jâˆˆ[d] are correctly specified. When testing H0 : Î²
âˆ—
jk = 0 with j < k, we apply
the desparsity method with Xj and X\j being the response and covariates, respectively.
Furthermore, to show that combining both Lj (Î²j ) and Lk(Î²k) is beneficial for inferring
Î²
âˆ—
jk, we also compare our method with the asymmetric score test, which constructs a score
test statistic similar to that in (12) based solely on Lj (Î²j ).
To examine the validity of our method, we test H0 : Î²
âˆ—
jk = 0 versus H1 : Î²
âˆ—
jk 6= 0 for
all (j, k). Recall that Î²
âˆ—
jk = Âµ when there is an edge. Here, we let Âµ increase from 0 to a
sufficiently large number. We calculate the type I errors and powers as
Type I error = the number of rejected hypotheses when there is no edge
d(d âˆ’ 1)/2 âˆ’ the total number of edges ,
Power = the number of rejected hypotheses when there is an edge
the total number of edges .
We report the type I errors and powers of the hypothesis tests at the 0.05 significance level
in Figure 1 and Figure 2, respectively. The simulation is repeated 100 times. As revealed
in Figure 1, both the asymmetric and the pairwise score test achieve accurate type I errors,
which is comparable to the desparsity method. Moreover, in terms of the power of the test,
in Figure 2, the two score tests based on the loss function defined in (6) are less powerful
than the desparsity method, which shows the loss of efficiency by only considering the
relative rank. However, as shown in Figure 2-(b) and (c), the two score tests are nearly as
powerful as the desparsity method in the Ising and mixed graphical models. In addition, we
emphasize that for mixed graphical models the desparsity method needs to know the type
(or distribution) of each nodes as a priori. Such phenomenon suggests that we may sacrifice
little efficiency for model generality/robustness. Furthermore, comparing the performances
of these two score tests, we see that the pairwise score test achieves uniformly higher power
than the asymmetric one, which perfectly illustrates that taking into consideration of the
symmetry of Î²
âˆ—
jk and Î²
âˆ—
kj may improve the inference accuracy.
16
On Semiparametric Exponential Family Graphical Models
0.05 0.10 0.15 0.20
0.02 0.03 0.04 0.05 0.06
â— Asymmetric Score
Pairwise Score
Desparsity
Âµ
typeâˆ’I errors
0.05 0.10 0.15 0.20 0.25
0.02 0.03 0.04 0.05 0.06
â— Asymmetric Score
Pairwise Score
Desparsity
Âµ
typeâˆ’I errors
0.05 0.10 0.15 0.20
0.02 0.03 0.04 0.05 0.06
â— Asymmetric Score
Pairwise Score
Desparsity
Âµ
typeâˆ’I errors
(a). Gaussian graphical model. (b). Ising model. (c). Mixed graphical model.
Figure 1: Type-I errors of the composite pairwise score test, asymmetric score test, and the
desparsity method for the three graphical models at the 0.05 significance level.
These figures are based on 100 independent simulations.
0.05 0.10 0.15 0.20
0.0 0.1 0.2 0.3 0.4 0.5 0.6
â— Asymmetric Score
Pairwise Score
Desparsity
Âµ
powers
â—
â—
â—
â—
â—
â—
â—
â—
â—
0.05 0.10 0.15 0.20 0.25
0.0 0.2 0.4 0.6 0.8
â— Asymmetric Score
Pairwise Score
Desparsity
Âµ
powers
â—
â—
â—
â—
â—
â—
â—
0.05 0.10 0.15 0.20
0.00 0.05 0.10 0.15 0.20 0.25 0.30
â— Asymmetric Score
Pairwise Score
Desparsity
Âµ
powers
(a). Gaussian graphical model. (b). Ising model. (c). Mixed graphical model.
Figure 2: Powers of the composite pairwise score test, asymmetric score test, and the
desparsity method for the three graphical models at the 0.05 significance level.
These figures are based on 100 independent simulations.
5.2. Real Data Analysis
We then apply the proposed methods to analyze a publicly available dataset named Computer
Audition Lab 500-Song (CAL500) dataset (Turnbull et al., 2008). The data can be obtained from the Mulan database (Tsoumakas et al., 2011). The CAL500 dataset consists
of 502 popular music tracks each of which is annotated by at least three listeners. The
attributes of this dataset include two subsets: (i) continuous numerical features extracted
from the time series of the audio signal and (ii) discrete binary labels assigned by human
listeners to give semantic descriptions of the song. For each music track, short time Fourier
transform is implemented for a sequence of half-overlapping 23ms time windows over the
songâ€™s digital audio file. This procedure generates four types of continuous features: spectral
centroids, spectral flux, zero crossings and a time series of Mel-frequency cepstral coefficient
17
Yang, Ning, and Liu
(MFCC). For the MFCC vectors, every consecutive 502 short time windows are grouped
together as a block window to produce the following four types of features: (i) overall mean
of MFCC vectors in each block window, (ii) mean of standard deviations of MFCC vectors in each block window, (iii) standard deviation of the means of MFCC vectors in each
block window, and (iv) standard deviation of the standard deviations of MFCC vectors
in each block window. More details on the feature extraction can be found in Tzanetakis
and Cook (2002). In addition to these continuous variables, binary variables in the CAL500
dataset include a 174-dimensional array indicating the existence of each annotation. These
174 annotations can be grouped into six categories: emotions (36 variables), instruments
(33), usages (15), genres (47), song characteristics (27) and vocal types (16). Our goal is
to infer the association between these different types of variables using graphical models.
This dataset has been analyzed in Cheng et al. (2017) where they exploit a nodewise groupLASSO regression to estimate the graph structure. In what follows, we use the proposed
pairwise score test to examine the graph structure.
Similar to Turnbull et al. (2008) and Cheng et al. (2017), we only keep the MFCC
features because they can be interpreted as the amplitude of the audio signal and the other
continuous features are not readily interpretable. Unlike Cheng et al. (2017), we keep all
the binary labels. Thus the processed dataset has n = 502 data points of dimension d = 226
with 52 continuous variables and 174 binary variables. We apply the pairwise score test to
each pair of variables to determine the presence of an edge between them. The p-values
for the null hypothesis that two variables are conditionally independence given the rest of
variables are calculated. We then apply the Bonferroni correction to control the familywise
error rate at 0.05. We set the nonconvex penalty function in optimization problem (7) to
be capped-`1 penalty pÎ»(u) = Î» min{u, Î»} with the regularization parameter Î» selected by
10-fold cross-validation as in the previous section.
We compare the pairwise score test with the desparsity method and the asymmetric score
test, which are constructed in the same way as in the simulation. We present the fitted
graphs obtained by these three methods in Figure 3-(a)â€“(c), where we plot the connected
components and omit the singletons. Moreover, in Figure 3-(d), we plot the intersection of
these three graphs. To better display the graphical structure, we use a square to represent
each type of 13 MFCC features respectively. If a node is connected to any node within the
group of variables in a MFCC node, then we draw an edge. We use circles to represent the
binary variables and use different colors to indicate their categories. The obtained graphs
have some interesting properties. While all three tests create different graphs, the graphs
obtained by the pairwise score test and the asymmetric score test have more common edges,
which agrees with our simulation results. Indeed, our test can correct the inconsistency of
the asymmetric score test, in the sense that the asymmetric score tests for Î²
âˆ—
jk = 0 and
Î²
âˆ—
kj = 0 may yield different test results. To show this inconsistency problem, we also plot
the graph obtained by the asymmetric score test based on the loss function Lk(Î²k) in Figure
4 in the appendix. Comparing with Figure 3-(b), we can see that the asymmetric score test
indeed leads to many contradictory edges.
In Figure 3, both the pairwise score test and this asymmetric score test discover that
songs that are danceable (circle 92) are suited for parties (circle 93), but such a connection
is not found by the desparsity method. This is also true for the connection between the
rapping vocals (circle 119) and the rap genre (circle 48) and the edge between strong vocals
18
On Semiparametric Exponential Family Graphical Models
(circle 122) and songs with strong emotions (circle 19). Moreover, in all three graphs, the
continuous features are densely connected within themselves, which is similar to the results
in Cheng et al. (2017). All three tests find that the noisiness of the music (square 4) is
connected with the quality of songs (circle 85). Furthermore, the common edges connecting
two binary variables also display interesting patterns. For instance, we find that awakening
emotions (circle 6) are connected with soothing emotions (circle 8); laid-back emotions
(circle 14) are connected with songs with high energy (circle 32); sad emotions (circle 20)
are connected with songs with positive feelings (circle 84); songs with female lead vocals
(circle 62) are connected with those with male lead vocals (circle 66). In addition, songs
using drum sets (circle 59) are connected with the electronica genre (circle 46), which is
also connected with the acoustic texture (circle 88). All these edges have fairly intuitive
explanations.
In summary, the proposed method reveals some interesting associations between these
variables and can be used as a useful complement to analyze high dimensional datasets with
more complex distributions.
6. Conclusion
We propose an integrated framework for uncertainty assessment of a new semiparametric
exponential family graphical model. The novelty of our model is that the base measures of
each nodewise conditional distribution are treated as unknown nuisance functions. Towards
the goal of uncertainty assessment, we first adopt the adaptive multi-stage relaxation algorithm to perform the parameter estimation. Then we propose a composite pairwise score
test of the graph structure. Our method provides a rigorous justification for the uncertainty
assessment, and is further supported by extensive numerical results. In a followup paper
(Tan et al., 2016), the proposed model is further extended to account for the unobserved
latent variables in the graphical model.
Acknowledgments
The authors are grateful for the support of NSF CAREER Award DMS1454377, NSF
IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841.
19
Yang, Ning, and Liu
â—
â—
â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—
â— â—
â—
â— â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â— â—
â—
â— â—
â—
â—
â—
1
2
3 4
5
6
8
9
10
11
12 13
14
15
16
17
18
19
20
21
22
23
27
40
41
46 48
50
51
52
53
54
55
56
57 58
59
60
61
62
66
68 71
73
78 80
81
82
83
84
85
86
88
89
90
91
92
93
96
99
113
119
122
123
124
125
128
132 134
136
138
139
â—
â—
â—
â—
â—
â—
mean of mean
mean of std
std of std
std of std
emotion
genre
vocals
instruments
usage
characteristics
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
1
2 3
4
5
6
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
27
40
45
46
49
50
51
52
53
54
56
57 58
59
60
61
62
66
68
73
78
80
81
82
83
84
85
86
87
88
89
90
96
108
113
123
124
125
128
132
136
138
139
142
â—
â—
â—
â—
â—
â—
mean of mean
mean of std
std of std
std of std
emotion
genre
vocals
instruments
usage
characteristics
(a). Pairwise score test. (b). Asymmetric score test.
â—
â—
â—
â—
â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â— â—
â—
â—
â—
â— â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â— â— â—
â—
1
2
3
4
5
6
8
9
12
14
16
17
18
20
21
22
27
40
45
46
49
51
52
54
55
56
57
58 59
61
62
66
69
71
73
80
81
82
84
85
87
88
89
90
96
99 105
122
â—
â—
â—
â—
â—
â—
mean of mean
mean of std
std of std
std of std
emotion
genre
vocals
instruments
usage
characteristics
â—
â—
â—
â—
â—
â—
â— â—
â— â—
â—
â—
â—
1
2
3
4
6
8
14
20
46
59
62 66
82
84
85
88
89
â—
â—
â—
â—
â—
â—
mean of mean
mean of std
std of std
std of std
emotion
genre
vocals
instruments
usage
characteristics
(c). Desparsity method. (d). The common edges.
Figure 3: Estimated graphs in the CAL500 dataset inferred by the pairwise score test, asymmetric score test, and the desparsity method. We plot the connected components
of the estimated graph. In (a)-(c) we plot the graphs obtained by these three approaches, respectively, and plot the common edges in (d). For better illustration,
we only plot the connected components, combine the same type of continuous
variables, display them as a square and draw each binary variable as a circle.
The edges of the estimated graph show the association between these variables.
20
On Semiparametric Exponential Family Graphical Models
Appendix A. Proof of the Main Results
In this appendix we lay out the proof of the main results. In Â§A.1 we prove the result of
parameter estimation. The proof is based an induction argument that Algorithm 1 keeps
penalizing most of the irrelevant features and gradually reduces the bias in relevant features.
A.1. Proof of Theorem 5
Proof We only need to prove the theorem for one node j âˆˆ [d], the proof is identical for
the rest. To begin with, we first define a few index sets that play a significant role in our
analysis. For all j âˆˆ [d], we let Sj
:= {(j, k): Î²
âˆ—
jk 6= 0, k âˆˆ [d]} be the support of Î²
âˆ—
j
. For
the number of iterations ` = 1, 2, . . ., let G`
j
:=

(j, k) âˆˆ/ Sj : Î»
(`âˆ’1)
jk â‰¥ p
0
Î»
(c2Î»), k âˆˆ [d]
	
.
By condition (C.3) of the penalty function pÎ»(u) (see Â§3.2), we have p
0
Î»
(c2Î») â‰¥ 0.91Î». In
addition, we let J
`
j
be the largest k
âˆ—
components of
Î²b(`)
j

G`
j
in absolute value where k
âˆ—
is defined in Assumption 4. In addition, we let I
`
j = (G`
j
)
c âˆª J
`
j
. Moreover, for notational
simplicity, we denote
Î²j

G`
j
,

Î²j

G`
j
and
Î²j

I
`
j
as Î²G`
j
, Î²J
`
j
and Î²I
`
j
respectively when no
ambiguity arises.
The key point of the proof is to show that the complement of G`
j
is not too large. To be
more specific, we show that

(G`
j
)
c

 â‰¤ 2s
âˆ—
for ` â‰¥ 1. Since Sj âŠ‚ (G`
j
)
c
, (G`
j
)
c â‰¤ 2s
âˆ—
implies

(G`
j
)
c âˆ’ Sj

 â‰¤ s
âˆ—
. Note that G`
j
is the set of irrelevant features that are heavily penalized
in the `-th iteration of the algorithm, (G`
j
)
c âˆ’ S being a small set indicates that the most of
the irrelevant features are heavily penalized in each step. We show that

(G`
)
c

 â‰¤ 2s
âˆ—
for
each ` â‰¥ 1 by induction.
For ` = 1, we have G1
j = S
c
j
because Î»
(0)
jk = Î» for all j, k âˆˆ [d]. Hence

(G1
j
)
c

 â‰¤ s
âˆ—
. Now
we assume that |

G`
j
c
| â‰¤ 2s
âˆ—
for some integer ` and our goal is to prove that |

G
`+1
j
c
| â‰¤ 2s
âˆ—
.
Our proof is based on three technical lemmas. The first lemma shows that the regularization
parameter Î» in (7) is of the same order as kâˆ‡Lj (Î²
âˆ—
j
)kâˆ.
Lemma 10 Under Assumptions 2 and 4, there exists a positive constants K such that, it
holds with probability at least 1 âˆ’ (2d)
âˆ’1
that

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
â‰¤ K
p
log d/n, âˆ€j âˆˆ [d]. (22)
Proof See Â§C.1 for a proof.
By this lemma, we conclude that the regularization parameter Î» â‰¥ 25

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
with
high probability. The following lemma bounds the `1- and `2-norms of Î²b(`)
j âˆ’Î²
âˆ—
j
by the
norms of its subvector under the induction assumption that

(G`
j
)
c

 â‰¤ 2s
âˆ—
.
Lemma 11 Letting the index sets Sj , G`
j
, J`
j
and I
`
j
be defined as above, we denote Ge`
j
:=
G`
j
c
. Under the assumption that |G`
j
| â‰¤ 2s
âˆ—
, we have

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 2.2

Î²b(`)
I
`
j
âˆ’ Î²
âˆ—
I
`
j


2
and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 2.2

Î²b(`)
Ge`
j
âˆ’ Î²
âˆ—
Ge`
j


1
. (23)
Proof See Â§C.2 for a detailed proof.           
Yang, Ning, and Liu
The next lemma guarantees that Î²b(`)
j
stays in the `1-ball centered at Î²
âˆ—
j with radius r for
` â‰¥ 1 where r appears in Assumption 4. Moreover, by showing this property of Algorithm
1 , we obtain a crude rate for parameter estimation. We summarized this result in the next
lemma.
Lemma 12 For ` â‰¥ 1 and j âˆˆ [d], we denote Î»
(`)
Sj
:= (Î»
(`)
jk ,(j, k) âˆˆ Sj )
T
. Assuming that

(G`
j
)
c

 â‰¤ 2s
âˆ—
, it holds with probability at least 1 âˆ’ d
âˆ’1
that, for all j âˆˆ [d], the estimators
Î²b(`)
j
obtained in each iteration of Algorithm 1 satisfy

Î²b(`)
I
`
j
âˆ’ Î²
âˆ—
I
`
j


2
â‰¤ 10Ï
âˆ’1
âˆ—
h
âˆ‡Ge`
j
Lj (Î²
âˆ—
j
)


2
+

Î»
(`âˆ’1)
Sj


2
i
, Ge`
j
:= (G
`
j
)
c
. (24)
This implies the following crude rates of convergence for Î²b(`)
j
:

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 24Ï
âˆ’1
âˆ—
âˆš
s
âˆ—Î» and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 33Ï
âˆ’1
âˆ—
s
âˆ—Î». (25)
Proof See Â§C.3 for a detailed proof.
Now we show that Ge`+1
j = (G
`+1
j
)
c
satisfies |Ge`+1
j
| â‰¤ 2s
âˆ—
, which concludes our induction.
Letting A := (G
`+1
j
)
c âˆ’Sj , by the definition of G
`+1
j
, (j, k) âˆˆ A implies that (j, k) âˆˆ/ Sj
and p
0
Î»

Î²b(`)
jk



â‰¤ p
0
Î»
(c2Î»). Hence by the concavity of pÎ»(Â·), for any (j, k) âˆˆ A,

Î²b(`)
jk

 â‰¥ c2Î».
Therefore we have
p
|A| â‰¤

Î²b(`)
A


2

(c2Î») =

Î²b(`)
A âˆ’ Î²
âˆ—
A


2

(c2Î») â‰¤ 24Ï
âˆ’1
âˆ—
âˆš
s
âˆ—

c2 â‰¤
âˆš
s
âˆ—, (26)
where the first inequality follows from |A| â‰¤ P
(j,k)âˆˆA

Î²b(`)
jk


2
(c2Î»)
2
. Note that (26) implies
that

(G
`+1
j
)
c

 â‰¤ 2s
âˆ—
. Therefore by induction,

(G`
j
)
c

 â‰¤ 2s
âˆ—
for any ` â‰¥ 1.
Now we have shown that for ` â‰¥ 1 and j âˆˆ [d],

(G`
j
)
c

 â‰¤ 2s
âˆ— and the crude statistical
rates (25) hold. In what follows, we derive the more refined rates (15) and (16).
A refined bound for

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
: For notional simplicity, we let
Î´
(`) = Î²b(`)
j âˆ’ Î²
âˆ—
j
and omit subscript j in Sj , G`
j
, J`
j
and I
`
j
. We also denote Ge`
:= (G`
)
c
. We
first derive a recursive bound that links kÎ´
(`)
I
` k2 to kÎ´
(`âˆ’1)
I
`âˆ’1 k2. Note that by (23), kÎ´
(`)k1 â‰¤
2.2kÎ´
(`)
Ge`
k1 â‰¤ 2.2
âˆš
2s
âˆ—kÎ´
(`)
Ge`
k2. Hence we only need to control kÎ´
(`)
I
` k2 to obtain the statistical
rates of convergence for Î²b(`)
j
. By triangle inequality,

âˆ‡Ge`Lj (Î²
âˆ—
j
)


2
â‰¤

âˆ‡SLj (Î²
âˆ—
j
)


2
+
q
|Ge` âˆ’ S|

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
.
Since Î» > 25

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
, (26) implies that

âˆ‡Ge`Lj (Î²
âˆ—
j
)


2
â‰¤

âˆ‡SLj (Î²
âˆ—
j
)


2
+

Î´
(`âˆ’1)
A


2

(25c2), (27)
where A := (G`
)
c âˆ’ S âŠ‚ I
`
. Thus (27) can be written as

âˆ‡Ge`Lj (Î²
âˆ—
j
)


2
â‰¤

âˆ‡SLj (Î²
âˆ—
j
)


2
+

Î´
(`âˆ’1)
I
`


2

(25c2). (28)
2 
On Semiparametric Exponential Family Graphical Models
Also notice that âˆ€Î²jk âˆˆ R, if |Î²jk âˆ’ Î²
âˆ—
jk| â‰¥ c2Î»,
p
0
Î»
(|Î²jk|) â‰¤ Î» â‰¤ |Î²jk âˆ’ Î²
âˆ—
jk|

c2;
otherwise we have |Î²
âˆ—
jk| âˆ’ |Î²jk| â‰¤ |Î²jk âˆ’ Î²
âˆ—
jk| < c2Î» and thus p
0
Î»
(|Î²jk|) â‰¤ p
0
Î»

|Î²
âˆ—
jk|âˆ’c2Î»

by
the concavity of pÎ»(Â·). Hence the following inequality always holds:
p
0
Î»
(|Î²jk|) â‰¤ p
0
Î»

|Î²
âˆ—
jk|âˆ’c2Î»

+ |Î²jk âˆ’ Î²
âˆ—
jk|

c2. (29)
Applying (29) to Î²b(`âˆ’1)
j we have

Î»
(`âˆ’1)
S


2
â‰¤
 X
(j,k)âˆˆS
p
0
Î»

|Î²
âˆ—
jk|âˆ’c2Î»
2
1/2
+
 X
(j,k)âˆˆS
|Î²b(`âˆ’1)
jk âˆ’Î²
âˆ—
jk|
2
1/2
.
c2,
which leads to

Î»
(`âˆ’1)
S


2
â‰¤
h X
(j,k)âˆˆS
p
0
Î»

|Î²
âˆ—
jk|âˆ’c2Î»
2
i1/2
+

Î´
(`âˆ’1)
I
`âˆ’1


2

c2. (30)
By (24), (28) and (30) we obtain

Î´
(`)
I
`


2
â‰¤ 10Ï
âˆ’1
âˆ—

âˆ‡SLj (Î²
âˆ—
j
)


2
+ Î¥j

+ Î³

Î´
(`âˆ’1)
I
`âˆ’1 k2,
where Î³ := 11(c2Ïâˆ—)
âˆ’1 and we define Î¥j
:=
P
(j,k)âˆˆS
p
0
Î»

|Î²
âˆ—
jk|âˆ’c2Î»
2
1/2
for notational
simplicity. Note that since c2 â‰¥ 24Ï
âˆ’1
âˆ—
, we have Î³ < 1. By recursion we obtain

Î´
(`)
I
`


2
â‰¤ 10%

âˆ‡SLj (Î²
âˆ—
j
)


2
+ Î¥j

+ Î³
`âˆ’1

Î´
(1)
I
1


2
, (31)
where % := Ï
âˆ’1
âˆ—
Â· (1 âˆ’ Î³)
âˆ’1 = c2(c2Ïâˆ— âˆ’ 11)âˆ’1
. Using

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 2.2

Î²b(`)
I
`
j
âˆ’ Î²
âˆ—
I
`
j


2
, we
can bound

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
by

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 22%

âˆ‡SjLj (Î²
âˆ—
j
)


2
+ Î¥j

+ 2.2Î³
`âˆ’1

Î´
(1)
I
1
j


2
.
Note that for ` = 1, by (24) we have

Î´
(1)
I
1
j


2
â‰¤ 10Ï
âˆ’1
âˆ—
âˆš
s
âˆ—

Î» +
âˆš
2

âˆ‡Lj (Î²
âˆ—
j
)


âˆ

â‰¤ 11Ï
âˆ’1
âˆ—
âˆš
s
âˆ—Î» = c2Î³
âˆš
s
âˆ—Î». (32)
then we establish the following bound for

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
:

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 22%

âˆ‡SjLj (Î²
âˆ—
j
)


2
+ Î¥j

+ 2.2c2
âˆš
s
âˆ—Î»Î³`
. (33)
Similarly, by

Î²b(`)
j âˆ’Î²
âˆ—
j


1
â‰¤ 2.2
âˆš
2s
âˆ—

Î²b(`)
I
`
j
âˆ’Î²
âˆ—
I
`
j


2
, we obtain a bound on

Î²b(`)
j âˆ’Î²
âˆ—
j


1
:

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 32âˆš
s
âˆ—%

âˆ‡SjLj (Î²
âˆ—
j
)


2
+ Î¥j

+ 2.2Î³
`âˆ’1
âˆš
2s
âˆ—

Î´
(1)
I
1
j


2
. (34                   
Yang, Ning, and Liu
By (32) we have 2.2
âˆš
2s
âˆ—

Î´
(1)
I
1
j


2
â‰¤ 3.2c2Î³sâˆ—Î», then the right-hand side of (34) can be
bounded by

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 32âˆš
s
âˆ—%

âˆ‡SjLj (Î²
âˆ—
j
)


2
+ Î¥j

+ 3.2c2s
âˆ—Î»Î³`
. (35)
Therefore (15) and (16) can be implied by (33) and (35) respectively. Moreover, by Lemma
12, we conclude that the statistical rates (33) and (35) hold for all j âˆˆ [d] with probability
at least 1 âˆ’ d
âˆ’1
.
A.2. Proof of Theorem 8
Proof We first remind the reader that, for 1 â‰¤ j 6= k â‰¤ d, we denote
Î²j\k = (Î²j1, . . . , Î²jjâˆ’1, Î²jj+1, . . . , Î²jkâˆ’1, Î²jk+1, . . . , Î²jd)
T âˆˆ R
dâˆ’2
,
Î²jâˆ¨k = (Î²jk, Î²j\k
, Î²k\j
)
T âˆˆ R
2dâˆ’3 and Î²b0
jâˆ¨k = (0, Î²b
j\k
, Î²b
k\j
)
T
. In addition, we define Ïƒ
2
jk =
Î£
jk
jk,jk âˆ’ 2Î£
jk
jk,j\kwâˆ—
j,k âˆ’ 2Î£
jk
jk,k\jwâˆ—
k,j + wâˆ—
j,k
T Î£
jk
j\k,j\kwâˆ—
j,k + wâˆ—
k,j
T Î£
jk
k\j,k\jwâˆ—
k,j . To prove the
theorem our goal is to prove the following two arguments:
limnâ†’âˆ
max
j<k
âˆš
n

Sbjkâˆ’Sjk(Î²
âˆ—
jâˆ¨k
)

 = 0 and limnâ†’âˆ
max
j<k
|Ïƒbjk âˆ’ Ïƒjk| = 0. (36)
Note that by Lemma 14, Ïƒ
2
jk is the asymptotic variance of âˆš
n/2Â·Sjk(Î²
âˆ—
jâˆ¨k
). Thus combining
(36) and Slutskyâ€™s theorem yields the theorem. By the the expression of Sjk(Î²
âˆ—
jâˆ¨k
) and
Sbjk in (17) and (18), under null hypothesis, for a fixed pair of nodes j and k, we have
Sbjkâˆ’Sjk(Î²
âˆ—
jâˆ¨k
)=I1j+I2j+I1k+I2k where I1j and I2j are defined as
I1j
:=

âˆ‡jkLj (Î²b0
j
) âˆ’ âˆ‡jkLj (Î²
âˆ—
j
)

âˆ’ wb
T
j,k
âˆ‡j\kLj (Î²b0
j
) âˆ’ âˆ‡j\kLj (Î²
âˆ—
j
)

and
I2j
:= (wâˆ—
j,k âˆ’ wbj,k)
T âˆ‡j\kLj (Î²
âˆ—
j
);
whereas I1k and I2k are defined by interchanging j and k in I1j and I2j :
I1k :=

âˆ‡kjLk(Î²b0
k
) âˆ’ âˆ‡jkLk(Î²
âˆ—
k
)

âˆ’ wb
T
k,j
âˆ‡k\jLk(Î²b0
k
) âˆ’ âˆ‡k\jLk(Î²
âˆ—
k
)

and
I2k := (wâˆ—
k,j âˆ’ wbk,j )
T âˆ‡k\jLj (Î²
âˆ—
k
).
We first bound I1j . Recall that Î²b0
j = (0, Î²b
j\k
)
T
. Note that under the null hypothesis, Î²
âˆ—
jk = 0,
by the Mean-Value Theorem, there exists a Î²e
j\k âˆˆ R
dâˆ’2
in the line segment between Î²b
j\k
and Î²
âˆ—
j\k
such that
I1j =

Î›e
jk,j\k âˆ’ wb
T
j,kÎ›e
j\k,j\k
Î²b
j\k âˆ’ Î²
âˆ—
j\k

,
where Î›e := âˆ‡2Lj (0, Î²e
j\k
). We let Î´ := Î²b0
j âˆ’ Î²
âˆ—
j
and denote âˆ‡2Lj (Î²b0
j
) and âˆ‡2
(Î²
âˆ—
j
) as Î› and
Î›âˆ—
respectively. From the definition of Dantzig selector we obtain
|I1j | â‰¤ kÎ›jk,j\k âˆ’ wb
T Î›j\k,j\kkâˆkÎ´j\kk1
| {z }
I11
+ kÎ›jk,j\k âˆ’ Î›e
jk,j\kkâˆkÎ´j\kk1
| {z }
I12
+ kwb
T
(Î›j\k,j\k âˆ’ Î›e
j\k,j\k
)Î´j\kkâˆ
| {z }
I13
.
2             
On Semiparametric Exponential Family Graphical Models
Theorem 5 implies that

Î´k1 â‰¤ Csâˆ—Î» with probability tending to 1 for some constant C > 0.
Then by the definition of Dantzig selector, I11 â‰¤ Csâˆ—Î»Î»D. with high probability. Moreover,
the constant C is the same for all (j, k). By assumption 7, I11 = o(n
âˆ’1/2
) with probability
tending to one.
For term I12, HÂ¨olderâ€™s inequality implies that
I12 â‰¤ kÎ›jk,j\k âˆ’ Î›e
jk,j\kkâˆkÎ´j\kk1. (37)
By Lemma 26 we obtain
kÎ› âˆ’ Î›ekâˆ â‰¤ kÎ› âˆ’ Î›
âˆ—
kâˆ + kÎ›
âˆ— âˆ’ Î›ekâˆ â‰¤ 2Csâˆ—Î» log2
d. (38)
Therefore combining (37) and (38) we have
I12 â‰¤ 2Csâˆ—2
Î»
2
log2
d . s
âˆ—Î»Î»D uniformly for 1 â‰¤ j < k â‰¤ d.
Similarly by HÂ¨olderâ€™s inequality, we have
I13 â‰¤ kwbj,kk1kÎ› âˆ’ Î›ekâˆkÎ´k1. (39)
Notice that by the optimality of wbj,k, kwbj,kk1 â‰¤ kwâˆ—
j,kk1 â‰¤ w0. Combining (39) and (38)
we have
I13 â‰¤ Cw0s
âˆ—2
Î»
2
log2
d . s
âˆ—Î»Î»D uniformly for 1 â‰¤ j < k â‰¤ d.
where we use the fact that Î»D & max{1, w0}s
âˆ—Î» log2
d. Therefore we conclude that for
all j âˆˆ [d], |I1j | . s
âˆ—Î»Î»D = oP(n
âˆ’1/2
). For I2j , HÂ¨olderâ€™s inequality implies that |I2j | â‰¤
kwâˆ—
j,k âˆ’ wbj,kk1

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
. To control kwâˆ—
j,k âˆ’ wbj,kk1, we need to the following lemma to
obtain the estimation error of the Dantzig selector wbj,k.
Lemma 13 For 1 â‰¤ j 6= k â‰¤ d, let wbj,k be the solution of the Dantzig-type optimization
problem (11) and let wâˆ—
j,k = H
j
jk,j\k
(H
j
j\k,j\k
)
âˆ’1
. Under Assumptions 2, 4, 6 and 7, with
probability tending to one, we have
kwbj,k âˆ’ wâˆ—
j,kk1 â‰¤ 37Î½
âˆ’1
âˆ—
s
?
0Î»D for all 1 â‰¤ j 6= k â‰¤ d.
Proof See Â§D.2 for a detailed proof.
Now combining Lemma 13 and Theorem 10 we obtain that
|I2j | â‰¤ 37Î½
âˆ’1
âˆ— K1s
?
0Î»D
p
log d/n  s
?
0Î»Î»D = o(n
âˆ’1/2
).
Therefore we have shown that I1j + I2j = o(n
âˆ’1/2
) with high probability. Similarly, we also
have I1k + I2k = o(n
âˆ’1/2
) with high probability. Moreover, since the bounds for |I1j | and
|I2j | is independent of the choice of (j, k) âˆˆ {(j, k): 1 â‰¤ j 6= k â‰¤ d}, we conclude that
âˆš
n

Sbjk âˆ’ Sjk
Î²
âˆ—
jâˆ¨k
 = oP(1) uniformly for 1 â‰¤ j < k â‰¤ d.
Our next lemma characterizes the limiting distribution of âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

and is pivotal for
establishing the validity of the composite pairwise score test.
    
Yang, Ning, and Liu
Lemma 14 For any b âˆˆ R
2dâˆ’3 with kbk2 = 1 and |bk0 â‰¤ se, if limnâ†’âˆ
se

n= 0, we have
âˆš
n/2 Â· b
T âˆ‡Ljk
Î²
âˆ—
jâˆ¨k
) N

0, b
T Î£jkb

. (40)
By Lemma 14 we obtain
âˆš
n/2 Â· S

Î²
âˆ—
jâˆ¨k

= âˆ‡jkLjk
Î²
âˆ—
jâˆ¨k

âˆ’ wâˆ—
j,k
T âˆ‡j\kLjk
Î²
âˆ—
jâˆ¨k

âˆ’ wâˆ—
k,j
T âˆ‡j\kLjk
Î²
âˆ—
jâˆ¨k

 N(0, Ïƒ2
jk),
where the asymptotic variance Ïƒ
2
jk is given by
Ïƒ
2
jk = Î£
jk
jk,jk âˆ’ 2Î£
jk
jk,j\kwâˆ—
j,k âˆ’ 2Î£jk,k\jwâˆ—
k,j + wâˆ—
j,k
T Î£
jk
j\k,j\kwâˆ—
j,k + wâˆ—
k,j
T Î£
jk
k\j,k\jwâˆ—
k,j .
For a more accurate estimation of Sbjk âˆ’ Sjk
Î²
âˆ—
jâˆ¨k

, we have
âˆš
n

Sbjk âˆ’ Sjk
Î²
âˆ—
jâˆ¨k

 â‰¤
âˆš
n

|I1| + |I2|

.
âˆš
n(s
âˆ—+s
?
0
)Î»Î»D. (41)
Finally, the following lemma, whose proof is deferred to the supplementary material, shows
that Ïƒbjk is a consistent estimator of Ïƒjk.
Lemma 15 For 1 â‰¤ j 6= k â‰¤ d, we denote the asymptotic variance of âˆš
n/2 Â· Sjk(Î²
âˆ—
jâˆ¨k
) as
Ïƒ
2
jk. Under Assumptions 2, 4, 6 and 7, the estimator Ïƒbjk satisfies limnâ†’âˆ
max
j<k
|Ïƒbjk âˆ’ Ïƒjk| = 0.
Proof See Â§D.3 for a proof.
Since Ïƒbjk is consistent for Ïƒjk by Lemma 15 and Ïƒjk is bounded away from zero by Assumption 6, Slutskyâ€™s theorem implies that âˆš
nSbjk/(2Ïƒbjk) N(0, 1).
Appendix B. Additional Estimation Results
We present the additional results of parameter estimation. In Â§B.1 we verify the sparse
eigenvalue condition for Gaussian graphical models, which justifies Assumption 4 in our
paper. In Â§B.2 we derive a more refined statistical rates of convergence for the iterates of
Algorithm 1.
B.1. Verify the Sparse Eigenvalue Condition for Gaussian Graphical Models
In this subsection, we verify the sparse eigenvalue condition for Gaussian graphical models.
Moreover, we show that such condition holds uniformly over a `1-ball centered at the true
parameter Î²
âˆ—
j
.
Proposition 16 Suppose X âˆ¼ N(0, Î£) is a Gaussian graphical model and let Î˜ = Î£âˆ’1
be the precision matrix. For all j âˆˆ [d], the conditional distribution of Xj given X\j
is a
normal distribution with mean Î²
âˆ—
j
TX\j and variance Î˜âˆ’1
jj , where Î²
âˆ—
j = Î˜j\j
. Let Lj (Â·) be
the loss function defined in (6). We assume that there exist positive constants D, cÎ» and
CÎ» such that kÎ£kâˆ â‰¤ D and cÎ» â‰¤ Î»min(Î£) â‰¤ Î»max(Î£) â‰¤ CÎ». We let s
âˆ— = maxjâˆˆ[d] kÎ²
âˆ—
j
k0
and also assume that there exists a constant CÎ² > 0 such that kÎ²
âˆ—
j
k2 â‰¤ CÎ² for all j âˆˆ         
On Semiparametric Exponential Family Graphical Models
Suppose r > 0 is a real number such that r = O(1/
âˆš
s
âˆ—). Then, there exist Ïâˆ—, Ïâˆ— > 0 such
that for all j âˆˆ [d], and s = 1, . . . , d âˆ’ 1,
Ïâˆ— â‰¤ Ïâˆ’

E

âˆ‡2Lj

, Î²
âˆ—
j
; s, r
â‰¤ Ï+

E

âˆ‡2Lj

, Î²
âˆ—
j
; s, r
â‰¤ Ï
âˆ—
.
Proof We prove this lemma in two steps. For any Î²j âˆˆ R
dâˆ’1
such that kÎ²j âˆ’ Î²
âˆ—
j
k1 â‰¤ r
and any v âˆˆ R
dâˆ’1
such that kvk2 = 1, we first give a lower bound for v
TE

âˆ‡2Lj (Î²j )

v by
truncation. Then we give an upper bound in the second step.
Step (i): Lower Bound of vTE

âˆ‡2Lj (Î²j )

v. We denote Bj (r) :=

Î² âˆˆ R
dâˆ’1
: kÎ² âˆ’
Î²
âˆ—
j
k1 â‰¤ r
	
. For two truncation levels Ï„ > 0 and R > 0, we denote Aii0 :=

|Xij | â‰¤ Ï„
	
âˆ©

|Xi
0j
| â‰¤ Ï„
	
, Bi
:=

XT
i\j
Î²j

 â‰¤ R, âˆ€Î²j âˆˆ Bj (r)
	
and Bi
0 :=

XT
i
0\j
Î²
âˆ—
j

 â‰¤ R, âˆ€Î²j âˆˆ Bj (r)
	
.
The values of R and Ï„ will be determined later. By the definition of Lj (Â·), for any Î²j âˆˆ Bj (r)
and any v âˆˆ R
dâˆ’1 with kvk2 = 1, we have
v
T âˆ‡2Lj (Î²j )v â‰¥
2C1(R, Ï„ )
n(n âˆ’ 1)
X
i<i0

Xij âˆ’ Xi
0j
2
Xi\j âˆ’ Xi
0\j
T
v
2
I(Bi)T(Bi
0)I(Aii0), (42)
where C1(R, Ï„ ) := exp(âˆ’4RÏ„ )

1 + exp(âˆ’4RÏ„ )
âˆ’2
. For notational simplicity, we denote the
right-hand side of (42) as C1(R, Ï„ )v
Tâˆ†v. By the properties of Gaussian graphical models,
the conditional density of Xij given Ii
:=

Xi\j = xi\j
	
âˆ© Bi
is
p

xij |Ii) = p(xi
|Bi)
. Z
R
p(xi
|Bi)dxij = p(xij |xi\j
),
where we use the fact that p(xi
|Bi) = p(xi)/P(Bi) and that P(Bi) is a constant. Recall that
p(xij |Xi\j
) = q
Î˜jj
(2Ï€) exp
âˆ’Î˜jj/2(xij âˆ’ XT
i\jÎ²
âˆ—
j
)
2

where Î²
âˆ—
j = Î˜j\j
.
Thus the conditional expectation of (Xij âˆ’ Xi
0j )
2
I(Aij ) given Ii and Ii
0 is
E
h
(Xij âˆ’ Xi
0j )
2
I(Aii0)



Ii âˆ© Ii
0
i
= Î˜jj/(2Ï€)
Z Ï„
âˆ’Ï„
Z Ï„
âˆ’Ï„
(xij âˆ’ xij )
2
expn
âˆ’Î˜jj/2

(xij âˆ’ Î²
T
j xi\j
)
2 + (xi
0j âˆ’ Î²
T
j xi
0\j
)
2
o
dxijdxi
0j
.
Note that on event Ii
, |Î²
T
j Xi\j
| â‰¤ R, hence the expression above can be lower-bounded by
E
h
(Xij âˆ’ Xi
0j )
2
I(Aii0)



Ii âˆ© Ii
0
i
â‰¥ Î˜jj/(2Ï€)
Z Ï„
âˆ’Ï„
Z Ï„
âˆ’Ï„
(xij âˆ’ xi
0j )
2
expn
âˆ’Î˜jj/2

x
2
ij + x
2
i
0j + 2R
2 + 2R(|xij | + |xi
0j
|)
o
dxijdxi
0j
.
The last expression is positive and we denote it as C2(R, Ï„ ) for simplicity. Thus by the law
of total expectation we obtain
v
TE(âˆ†)v = v
TE

E(âˆ†

 âˆ©
n
i=1 Ii)

v â‰¥ C2(R, Ï„ )E
n
(Xi\j âˆ’ Xi
0\j
)
T v
2
I(Bi)I(Bj )
o
                           
Yang, Ning, and Liu
By Cauchy-Schwarz inequality we have
E
n
(Xi\j âˆ’ Xi
0\j
)
T v
2

1 âˆ’ I(Bi)I(Bi
0)
o
â‰¤
q
E[(Xi\j âˆ’ Xi
0\j
)
T v
4
q
P

B
c
i âˆª Bc
i
0

. (43)
Note that for Gaussian graphical model, the marginal distribution of X\j
is N(0, Î£\j\j
).
If we denote Î£\j\j as Î£1, we have (Xi\j âˆ’Xi
0\j
)
T v âˆ¼ N(0, Ïƒ2
v
), XT
i\j
Î²
âˆ—
j âˆ¼ N(0, Ïƒ2
1
) and
XT
i\j
Î² âˆ¼ N(0, Ïƒ2
2
) where Ïƒ
2
v = 2v
T Î£1v, Ïƒ2
1 = Î²
âˆ—
j
T Î£1Î²
âˆ—
j
and Ïƒ
2
2 = Î²
T
j Î£1Î²j . Hence we have
E

(Xi\j âˆ’ Xi
0\j
)
T v
4 = 3Ïƒ
4
v
. Because the maximum eigenvalue of Î£1 is upper bounded by
CÎ», we have Ïƒ
2
1 â‰¤ CÎ»C
2
Î²
and Ïƒ
2
v â‰¤ 2CÎ». Note that Ïƒ
2
2âˆ’Ïƒ
2
1 =Î²
T
j Î£1Î²jâˆ’Î²
âˆ—
j
T Î£1Î²
âˆ—
j
, the following
lemma in linear algebra bounds this type of error.
Lemma 17 Let M âˆˆ R
dÃ—d
be a symmetric matrix and vectors v1 and v2 âˆˆ R
d
, then

v
T
1 Mv1 âˆ’ v
T
2 Mv2

 â‰¤ kMkâˆkv1 âˆ’ v2k
2
1 + 2kMv2kâˆkv1 âˆ’ v2k1.
Proof Note that v
T
1 Mv1 âˆ’ v
T
2 Mv2 = (v1âˆ’v2)
TM(v1âˆ’v2) + 2v
T
2 M(v1âˆ’v2), HÂ¨olderâ€™s
inequality implies

v
T
1 Mv1 âˆ’ v
T
2 Mv2

 â‰¤

(v1âˆ’v2)
TM(v1âˆ’v2)

 + 2

v
T
2 M(v1âˆ’v2)


â‰¤ kMkâˆkv1 âˆ’ v2k
2
1 + 2kMv2kâˆkv1 âˆ’ v2k1.
Hence, we conclude the proof of Lemma 17.
By Lemma 17, we have
Ïƒ
2
2 âˆ’ Ïƒ
2
1 â‰¤ kÎ£1kâˆkÎ²j âˆ’ Î²
âˆ—
j


2
1
+ 2kÎ£1Î²
âˆ—
j kâˆkÎ²j âˆ’ Î²
âˆ—
j k1. (44)
By HÂ¨olderâ€™s inequality and the relation between `1-norm and `2-norm of a vector, we have
kÎ£1Î²
âˆ—
j
kâˆ â‰¤ kÎ£1kâˆkÎ²
âˆ—
j
k1 â‰¤
âˆš
s
âˆ—CÎ²D. Therefore the right-hand side of (44) can be bounded
by
Ïƒ
2
2 âˆ’ Ïƒ
2
1 â‰¤ r
2D + 2âˆš
s
âˆ—rCÎ²D,
which shows that Ïƒ
2
2
is also bounded because r = O(1âˆš
s
âˆ—). In addition, by the bound
1 âˆ’ Î¦(x) â‰¤ exp(âˆ’x
2/2)/(x
âˆš
2Ï€) for the standard normal distribution function, we obtain
that
P

B
c
i

â‰¤ P

XT
i\jÎ²
âˆ—
j > R
+ P

XT
i\jÎ²j > R
â‰¤ cÏƒ1 exp
âˆ’R
2
/(2Ïƒ
2
1
)

/R + cÏƒ2 exp
âˆ’R
2
/(2Ïƒ
2
2
)

/R,
where the constant c = 1/
âˆš
2Ï€. We denote the last expression as C3(R), then the righthand side of (43) can be upper-bounded by p
3Ïƒ
4
v
p
2C3(R) â‰¤ 2
p
6C3(R)CÎ». Hence we
can choose a sufficiently large R such that 2p
6C3(R)CÎ» = Î»min(Î£) and we denote this
particular choice of R as R0.
Now we have
E
n
(Xi\j âˆ’ Xi
0\j
)
T v
2

1 âˆ’ I(Bi)I(Bi
0)
o
â‰¤ Î»min(Î£)                   
On Semiparametric Exponential Family Graphical Models
Note that E

[(Xi\j âˆ’ Xi
0\j
)
T v]
2
	
= Ïƒ
2
v â‰¥ 2Î»min(Î£), we obtain that
v
TE

âˆ‡2Lj (Î²j )

v â‰¥ C1(R0, Ï„ )C2(R0, Ï„ )Î»min(Î£) for all Ï„ âˆˆ R.
Therefore we conclude that for all Î²j âˆˆ R
dâˆ’1
such that kÎ²j âˆ’ Î²
âˆ—
j
k1 â‰¤ r,
v
TE

âˆ‡2Lj (Î²j )

v â‰¥ max
Ï„âˆˆR

C1(R0, Ï„ )C2(R0, Ï„ )
	
Î»min(Î£). (45)
Step (ii): Upper Bound of vTE

âˆ‡2Lj (Î²j )

v. For any Î²j âˆˆ R
dâˆ’1
such that kÎ²j âˆ’Î²
âˆ—
j
k1 â‰¤
r and for any v âˆˆ R
dâˆ’1 with kvk2 = 1, by the definition of âˆ‡2Lj (Î²j ) we have
v
T âˆ‡2Lj (Î²j )v â‰¤ (Xij âˆ’ Xi
0j )
2

(Xi\j âˆ’ Xi
0\j
)
T v
2
. (46)
Notice that conditioning on Xi\j
, Xij âˆ¼ N

XT
i\j
Î²
âˆ—
j
, Î˜âˆ’1
jj 
, hence
E

(Xij âˆ’ Xi
0j )
2

Xi\j
, Xi
0\j

=

(Xi\j âˆ’ Xi
0\j
)
TÎ²
âˆ—
j
2 + 2Î˜âˆ’1
jj . (47)
Combining (46) and (47) we obtain
E

v
T âˆ‡2Lj (Î²j )v

â‰¤ E
n
E

(Xij âˆ’ Xi
0j )
2

Xi\j
, Xi
0\j

Â·

(Xi\j âˆ’ Xi
0\j
)
T v
2
o
â‰¤ 2Î˜âˆ’1
jj E

(Xi\j âˆ’ Xi
0\j
)
T v
2 + E
n
(Xi\j âˆ’ Xi
0\j
)
TÎ²
âˆ—
j
2

(Xi\j âˆ’ Xi
0\j
)
T v
2
o
. (48)
Because Xi\j âˆ¼ N(0, Î£1) where Î£1 := Î£\j,\j
, and also note that the maximum eigenvalue
of Î£1 is upper bounded by CÎ», we have
E

(Xi\j âˆ’ Xi
0\j
)
T v
2 = 2v
T Î£1v â‰¤ 2CÎ».
Moreover, by inequality 2ab â‰¤ a
2 + b
2 we obtain
2E
n
(Xi\j âˆ’ Xi
0\j
)
TÎ²
âˆ—
j
2

(Xi\j âˆ’ Xi
0\j
)
T v
2
o
â‰¤ E

(Xi\j âˆ’ Xi
0\j
)
TÎ²
âˆ—
j
4 + E

(Xi\j âˆ’ Xi
0\j
)
T v
4
.
Since (Xi\j âˆ’ Xi
0\j
)
T v âˆ¼ N(0, Ïƒ2
v
) and (Xi\j âˆ’ Xi
0\j
)
TÎ²
âˆ—
j âˆ¼ N(0, 2Ïƒ
2
1
) where Ïƒ
2
v and Ïƒ
2
1
are
defined as 2v
T Î£1v and Î²
âˆ—
j
T Î£1Î²
âˆ—
j
respectively, we obtain
E

(Xi\j âˆ’ Xi
0\j
)
TÎ²
âˆ—
j
4 = 3Ïƒ
4
v â‰¤ 12C
2
Î»
and E

(Xi\j âˆ’ Xi
0\j
)
T v
4 = 12Ïƒ
4
1 â‰¤ 12CÎ»C
2
Î²
.
Therefore we can bound the right-hand side of (48) by
E

v
T âˆ‡2Lj (Î²j )v

â‰¤ 4Î˜âˆ’1
jj CÎ» + 6C
2
Î» + 6CÎ»C
2
Î²
. (49)
Combining (45) and (49) we conclude that Proposition 16 holds with
Ïâˆ— = max
Ï„âˆˆR

C1(R0, Ï„ )C2(R0, Ï„ )
	
Î»min(Î£) and Ï
âˆ— = 4Î˜âˆ’1
jj CÎ» + 6C
2
Î» + 6CÎ»C
2
Î²
.
Therefore, we conclude the proof of Proposition 16.
                                        
Yang, Ning, and Liu
B.2. Refined Statistical Rates of Parameter Estimation
In this subsection we show more refined statistical rates of convergence for the proposed
estimators. In specific, we consider the case where Î²
âˆ—
j
contains nonzero elements with both
strong and week magnitudes.
Theorem 18 (Refined statistical rates of convergence) Under Assumptions 2 and 4,
we let K1 and K2 be the constants defined in Theorem 10 and also let Ïâˆ— > 0 and r > 0 be
defined in Assumption 4. For all j âˆˆ [d], we define the support of Î²
âˆ—
j
as Sj
:={(j, k): Î²
âˆ—
jk 6=
0, k âˆˆ [d]} and let s
âˆ— = maxjâˆˆ[d] kÎ²
âˆ—
j
k0. The penalty function pÎ»(u) : [0, +âˆ) â†’ [0, +âˆ) in
(7) satisfies regularity conditions (C.1), (C.2) and (C.3) listed in Â§3.2 with c1 = 0.91 and
c2 â‰¥ 24/Ïâˆ— for condition (C.3). We set the regularity parameter Î» = C
p
log d/n such that
C â‰¥25K1. Moreover, we assume that the penalty function pÎ»(u) satisfies an extra condition
(C.4): there exists a constant c3 > 0 such that p
0
Î»
(u)= 0 for uâˆˆ

c3Î», +âˆ

. Suppose that the
support of Î²
âˆ—
j
can be partitioned into Sj = S1j âˆª S2j where S1j =

(j, k): |Î²
âˆ—
jk| â‰¥ (c2+c3)Î»
	
and S2j = Sj âˆ’S1j . We denote constants A1 = 22%, A2 = 2.2c2, B1 = 32%, B2 = 3.2c2,
% = c2(c2Ïâˆ—âˆ’11)âˆ’1
, Î³ = 11c
âˆ’1
2
Ï
âˆ’1
âˆ— < 1 and a = 1.04; we let s
âˆ—
1j = |S1j | and s
âˆ—
2j = |S2j |. With
probability at least 1âˆ’d
âˆ’1
, we have the following more refined rates of convergence:

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ A1
n
âˆ‡S1jLj (Î²
âˆ—
j
)


2
+ a
q
s
âˆ—
2j
Î»
o
+ A2
âˆš
s
âˆ—Î»Î³`
and (50)

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ B1
n
âˆ‡S1jLj (Î²
âˆ—
j
)


2
+ a
q
s
âˆ—
2j
Î»
o
+ B2s
âˆ—Î»Î³`
, âˆ€j âˆˆ [d]. (51)
Proof Let Sj = {(j, k): Î²
âˆ—
jk 6= 0, kâˆˆ[d]} be the support of Î²
âˆ—
j
and let index set G`
j
, J`
j
and
I
`
j
be the same as defined in the proof of Theorem 5. For notational simplicity, we omit the
subscript j in these index sets which stands for the j-th node of the graph; we simply write
them as G`
, J` and I
`
. Moreover, we let Î´
(`) = Î²b(`)
j âˆ’ Î²
âˆ—
j
, it is shown in Lemma 12 that

Î´
(`)
I
`


2
â‰¤ 10Ï
âˆ’1
âˆ—

âˆ‡Ge`Lj (Î²
âˆ—
j
)


2
+

Î»
(`âˆ’1)
Sj


2

; Ge` = (G
`
)
c
. (52)
In the proof of Theorem 5, we show that |Ge`
| â‰¤ 2s
âˆ—
for all j âˆˆ [d] and ` â‰¥ 1. Because
Sj = S1j âˆª S2j where S1j =

(j, k): |Î²
âˆ—
jk| â‰¥ (c2 + c3)Î»
	
and S2j = Sjâˆ’S1j , then by triangle
inequality we have

âˆ‡SjLj (Î²
âˆ—
j
)


2
â‰¤

âˆ‡S1jLj (Î²
âˆ—
j
)


2
+
q
s
âˆ—
2j

âˆ‡S2jLj (Î²
âˆ—
j
)


âˆ
.
Since Î» â‰¥ 25

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
with high probability, by (28), we further have

âˆ‡Ge`Lj (Î²
âˆ—
j
)


2
â‰¤

âˆ‡S1jLj (Î²
âˆ—
j
)


2
+
q
s
âˆ—
2j
Î»

25 +

Î´
(`âˆ’1)
I
`âˆ’1


2
.
(25c2). (53)
Note that by the definition of S1j , for any (j, k)âˆˆS1j , p
0
Î»

|Î²jk|âˆ’c2Î»

â‰¤ p
0
Î»
(c3Î») = 0, then
we have
Î¥j
:= Î»
h X
(j,k)âˆˆSj
p
0
Î»

|Î²
âˆ—
jk| âˆ’ c2Î»
2
i1/2
= Î»
h X
(j,k)âˆˆS2j
p
0
Î»
(|Î²
âˆ—
jk| âˆ’ c2Î»)
2
i1/2
â‰¤
q
s
âˆ—
2j
Î».
   
On Semiparametric Exponential Family Graphical Models
Therefore (30) is reduced to

Î»
(`âˆ’1)
Sj


2
â‰¤ Î¥j +

Î´
(`âˆ’1)
I
`âˆ’1


2

c2 â‰¤
q
s
âˆ—
2j
Î» +

Î´
(`âˆ’1)
I
`âˆ’1


2
.
c2. (54)
Combining (52), (53) and (54) we obtain

Î´
(`)
I
`


2
â‰¤ 10Ï
âˆ’1
âˆ—
n
âˆ‡S1jLj (Î²
âˆ—
j
)


2
+ 1.04q
s
âˆ—
2j
Î» + 1.04

Î´
(`âˆ’1)
I
`âˆ’1


2

c2
o
.
Then by recursion, we obtain the following estimation error:

Î´
(`)
I
`


2
â‰¤ 10%
n
âˆ‡S1jLj (Î²
âˆ—
j
)


2
+ 1.04q
s
âˆ—
2j
Î»
o
+ Î³
`âˆ’1

Î´
(1)
I
1


2
,
where Î³ := 11c
âˆ’1
2
Ï
âˆ’1
âˆ— and % := c2(c2Ïâˆ— âˆ’ 11)âˆ’1
. Note that we assume c2 â‰¥ 24Ï
âˆ’1
âˆ—
, for k = 1
by (32) we have
2.2

Î´
(1)
I1


2
â‰¤ 2.2c2Î³
âˆš
s
âˆ—Î» and 2.2
âˆš
2s
âˆ—

Î´
(1)
I1


2
â‰¤ 3.2c2Î³sâˆ—Î».
Therefore, using the original notation, we obtain the refined rates of convergence by (23):

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 22%
n
âˆ‡S1jLj (Î²
âˆ—
j
)


2
+ 1.04q
s
âˆ—
2j
Î»
o
+ 2.2c2Î³
`
âˆš
s
âˆ—Î» and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 32%
âˆš
s
âˆ—
n
âˆ‡S1jLj (Î²
âˆ—
j
)


2
+ 1.04q
s
âˆ—
2j
Î»
o
+ 3.2c2Î³
`
s
âˆ—Î»,
where s
âˆ—
2j = |S2j |. Moreover, it is easy to see that, with probability at least 1âˆ’d
âˆ’1
, these
convergence rates hold for all j âˆˆ [d].
Appendix C. Proof of the Auxiliary Results for Estimation
In this appendix, we prove the main results for estimation results presented in Â§4.1. In this
appendix, we prove the auxiliary results for estimation. In specific, we give detailed proofs
of Lemmas 10, 11, and 12, which are pivotal for the proof of Theorem5. We first prove
Lemmas 10, which gives an upper bound for kâˆ‡Lj (Î²
âˆ—
j
)kâˆ.
C.1. Proof of Lemma 10
Proof By definition, âˆ‡Lj (Î²
âˆ—
j
) is a centered second-order U-statistic with kernel function
h
j
ii0(Î²
âˆ—
j
)âˆˆR
dâˆ’1
, whose entries are given by

h
j
ii0(Î²
âˆ—
j
)

jk =
R
j
ii0(Î²
âˆ—
j
)(Xij âˆ’ Xi
0j )(Xik âˆ’ Xi
0k)
1 + R
j
ii0(Î²
âˆ—
j
)
.
By the tail probability bound in (14), for any i âˆˆ [n] and j âˆˆ [d], we have
P

|Xij | > x, âˆ€i âˆˆ [n], âˆ€j âˆˆ [d]

â‰¤
X
iâˆˆ[n],jâˆˆ[d]
P(|Xij | > x)
â‰¤ 2 exp(Îºm + Îºh/2) exp(âˆ’x + log d + log n). (55)
3   
Yang, Ning, and Liu
By setting x = C log d for some constant C, we conclude that event E := {|Xij | â‰¤
C log d, âˆ€i âˆˆ [n], âˆ€j âˆˆ [d]} holds with probability at least 1 âˆ’ (4d)
âˆ’1
. Following from
the same argument as in Ning et al. (2017b), it is easy to show that, conditioning on E,
h
j
ii0(Î²
âˆ—
j
) is also centered. Note that conditioning on event E,

h
j
ii0(Î²
âˆ—
j
)


âˆ
â‰¤ C log2
d for
some generic constant C and for all i, i0 âˆˆ [d] and j âˆˆ [d]. The following Bernsteinâ€™s inequality for U-statistics, presented in Arcones (1995), gives an upper bound for the tail
probability of âˆ‡Lj (Î²
âˆ—
j
).
Lemma 19 (Bernsteinâ€™s inequality for U-statistics) Given n i.i.d. random variables
Z1, . . . Zn taking values in a measurable space (S, B) and a symmetric and measurable kernel
function h: S
m â†’ R, we define the U-statistics with kernel h as
U :=

n
m
âˆ’1
X
i1<...<im
h(Zi1
, . . . , Zim).
Suppose that E[h(Zi1
, . . . , Zim)] = 0, E

E[h(Zi1
, . . . , Zim) | Zi1
]
	2 = Ïƒ
2
, and khkâˆ â‰¤ b for
some positive Ïƒ and b. There exists an absolute constant K(m) > 0 that only depends on
m such that
P(|U| > t) â‰¤ 4 exp
âˆ’nt2
/[2m2Ïƒ
2 + K(m)bt]
	
, âˆ€t > 0. (56)
Note that by (14), the fourth moment of X is bounded, which implies that E[h
j
ii0(Î²
âˆ—
j
)]2
is uniformly bounded by an absolute constant for all j âˆˆ [d]. By Lemma 19, setting
b = C log2
d in (56) yields that
P

âˆ‡jkLj (Î²
âˆ—
j
)

 > t

E

â‰¤ 4 exph
âˆ’nt2

(C1 + C2 log2
d Â· t)
i
(57)
for some generic constants C1 and C2. Taking a union bound over {(j, k): j, k âˆˆ [d], k 6= j}
we obtain
max
jâˆˆ[d]
n
P

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
> t

E
o
. d
2
Â· exp
âˆ’nt2

(C1 + C2 log2
d Â· t)

. (58)
Under Assumption 4 and conditioning on E, by setting t = K1
p
log d/n for a sufficiently
large K1 > 0, it holds probability greater than 1 âˆ’ (4d)
âˆ’1
that

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
â‰¤ K1
p
log d/n âˆ€j âˆˆ [d].
Note that E holds with probability at least 1âˆ’(4d)
âˆ’1
, we conclude the proof of Lemma 10.
C.2. Proof of Lemma 11
Proof In what follows, for notational simplicity and readability, we omit j in the subscript
and ` in the superscript by simply writing Sj , G`
j
, J`
j
and I
`
j
as S, G, J an I respectively. By
the definition of G,

Î»
(`âˆ’1)
G`


min â‰¥ p
0
Î»
(Î¸) â‰¥ 0.91Î» > 22.75kâˆ‡Lj (Î²
âˆ—
j
)kâˆ. We prove this lemma
    
On Semiparametric Exponential Family Graphical Models
in two steps. In the first step we show that

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 2.2

Î²b`
Gc âˆ’ Î²
âˆ—
Gc


1
. Suppose
that Î²b(`)
j
is the solution in the `-th iteration and we denote âˆ‡jkLj (Î²j ) = âˆ‚Lj (Î²j )

âˆ‚Î²jk,
the Karush-Kuhn-Tucker condition implies that
âˆ‡jkLj (Î²b(`)
j
) + Î»
(`âˆ’1)
jk sign(Î²b(`)
jk ) = 0 if Î²b(`)
jk 6= 0;
âˆ‡jkLj (Î²b(`)
j
) + Î»
(`âˆ’1)
jk Î¾
(`)
jk = 0, Î¾(`)
jk âˆˆ [âˆ’1, 1] if Î²b(`)
jk = 0.
The above Karush-Kuhn-Tuker condition can be written in a compact form as
âˆ‡Lj (Î²b(`)
j
) + Î»
(`âˆ’1)
j
â—¦ Î¾
(`)
j = 0, (59)
where Î¾
(`)
j âˆˆâˆ‚

Î²b(`)
j


1
and Î»
(`âˆ’1)
j =

Î»
(`âˆ’1)
j1
, . . . , Î»(`âˆ’1)
jjâˆ’1
, Î»(`âˆ’1)
jj+1 , . . . , Î»(`âˆ’1)
jd T
âˆˆ R
dâˆ’1
.
For notational simplicity, we let Î´ = Î²b(`)
j âˆ’ Î²
âˆ—
j âˆˆ R
dâˆ’1 and omit the superscript ` and
subscript j in both Î»
(`âˆ’1)
j
and Î¾
(`)
j
by writing them as Î» and Î¾. By definition, I = Gc âˆª J.
Note that we denote the support of Î²
âˆ—
j
as S; we define H := Gcâˆ’S, then S, H and G is a
partition of 
(j, k): kâˆˆ[d], k 6=j
	
.
By the Mean-Value theorem, there exists an Î±âˆˆ[0, 1] such that Î²ej
:=Î±Î²
âˆ—
j+(1 âˆ’ Î±)Î²b(`)
j âˆˆ
R
dâˆ’1
satisfies
âˆ‡Lj (Î²bj ) âˆ’ âˆ‡Lj (Î²
âˆ—
j
) = âˆ‡2Lj (Î²ej )Î´.
Then (59) implies that
0 â‰¤ Î´
T âˆ‡2Lj (Î²ej )Î´ = âˆ’


Î´,Î» â—¦ Î¾

| {z }
(i)
âˆ’


âˆ‡Lj (Î²
âˆ—
j
), Î´

| {z }
(ii)
. (60)
For term (ii) in (60), HÂ¨olderâ€™s inequality implies that
(ii) â‰¥ âˆ’

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
kÎ´k1. (61)
For term (i) in (60), recall that we denote |v| as the vector that takes entrywise absolute
value for v. By the fact that Î¾
(`)
jk Î²b(`)
jk = |Î²b(`)
jk |, we have Î¾G â—¦ Î´G = |Î´G| and Î¾H â—¦ Î´H = |Î´H|.
Since Î´Sc = Î²b(`)
Sc . HÂ¨olderâ€™s inequality implies that


Î´,Î» â—¦ Î¾

=


Î´S,(Î» â—¦ Î¾)S

+


|Î´H|,Î»H

+


|Î´G|,Î»G

â‰¥ âˆ’kÎ´Sk1kÎ»Skâˆ + kÎ´Gk1kÎ»Gkmin + kÎ´Hk1kÎ»Hkmin. (62)
Combining (60), (61) and (62) we have
âˆ’kÎ´Sk1kÎ»Skâˆ + kÎ´Gk1kÎ»Gkmin + kÎ´Hk1kÎ»Hkmin âˆ’

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
kÎ´k1 â‰¤ 0. (63)
By the definition of G, we have kÎ»Gkmin â‰¥ p
0
Î»
(c2Î») â‰¥ 0.91Î». Rearranging terms in (63) we
have
p
0
Î»
(c2Î»)kÎ´Gk1 â‰¤ kÎ´Gk1kÎ»Gkmin â‰¤

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
kÎ´k1 + kÎ´Sk1kÎ»Skâˆ.
3 
Yang, Ning, and Liu
Using the decomposability of the `1-norm, we have
h
p
0
Î»
(c2Î») âˆ’

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
i
kÎ´Gk1 â‰¤
h
kÎ»Skâˆ + kâˆ‡Lj (Î²
âˆ—
j
)kâˆ
i
kÎ´Gc k1 (64)
Recall that Î» > 25

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
and p
0
Î»
(Î¸) â‰¥ 0.91Î», (64) implies

Î´G


1
â‰¤
Î» +

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
p
0
Î»
(c2Î») âˆ’

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
kÎ´Gc k1 â‰¤ 1.2kÎ´Gc k1, (65)
where we use the fact that
Î» +

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
p
0
Î»
(c2Î») âˆ’

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
â‰¤
Î» + 0.04Î»
0.91Î» âˆ’ 0.04Î»
â‰¤ 1.2.
Going back to the original notation, (65) is equivalent to

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 2.2

Î²b(`)
Ge`
j
âˆ’ Î²
âˆ—
Ge`
j


1
.
Now we show in the second step that

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 2.2

Î²b(`)
I
`
j
âˆ’ Î²
âˆ—
I
`
j


2
. Recall that J
is the largest k
âˆ—
components of Î²b(`)
G
in absolute value where we omit the subscript j and
superscript ` in the sets G`
j
, J`
j
and I
`
j
. By the definition of J we obtain that
kÎ´I
c kâˆ â‰¤ kÎ´J k1

k
âˆ— â‰¤ kÎ´Gk1

k
âˆ—
, where Î´ = Î²b(`)
j âˆ’ Î²
âˆ—
j
.
By inequality (65) and the fact that Gc âŠ‚ I, we further have
kÎ´I
c kâˆ â‰¤ 1.2/kâˆ—
Â· kÎ´Gc k1 â‰¤ 1.2/kâˆ—
Â· kÎ´Ik1. (66)
Then by HÂ¨olderâ€™ inequality and (66) we obtain that
kÎ´I
c k2 â‰¤

kÎ´I
c k1kÎ´I
c kâˆ
1/2 â‰¤ (1.2/kâˆ—
)
1/2

kÎ´Ik1kÎ´I
c k1
1/2
. (67)
By the definition of index sets G and I, we have I
c âŠ‚ G and Gc âŠ‚ I. Then by (65) and (67)
we obtain
kÎ´I
c k2 â‰¤ (1.2/kâˆ—
)
1/2

kÎ´Gc k1kÎ´Gk1
1/2 â‰¤ 1.2kÎ´Gc k1
âˆš
k
âˆ—.
By the norm inequality between `1-norm and `2-norm, we have
kÎ´I
c k2 â‰¤ 1.2kÎ´Gc k1/
âˆš
k
âˆ— â‰¤ 1.2
p
2s
âˆ—/kâˆ—kÎ´Gc k2 â‰¤ 1.2kÎ´Ik2,
where we use k
âˆ— â‰¥ 2s
âˆ— and the induction assumption that |G| â‰¤ 2s
âˆ—
. Then triangle inequality for `2-norm yields that
kÎ´k2 â‰¤ kÎ´I
c k2 + kÎ´Ik2 â‰¤ 2.2kÎ´Ik2. (68)
Note that (65) and (68) are equivalent to

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 2.2

Î²b(`)
I
`
j
âˆ’ Î²
âˆ—
I
`
j


2
and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 2.2

Î²b(`)
Ge`
j
âˆ’ Î²
âˆ—
Ge`
j


1
,
where Ge`
j =

G`
j
c
, which concludes the proof.    
On Semiparametric Exponential Family Graphical Models
C.3. Proof of Lemma 12
Proof We first show that Î²b(`)
j
stays in the `1-ball centered at Î²
âˆ—
j with radius r =
CÏs
âˆ—p
log d/n, where CÏ â‰¥ 33Ï
âˆ’1
âˆ—
. For notational simplicity, we denote Î´ = Î²b(`)
j âˆ’ Î²bj
and write Sj , G`
j
, J`
j
and I
`
j
as S, G, J an I respectively. We prove by contradiction. Suppose that kÎ´k1 > r, then we define Î²ej = Î²
âˆ—
j + t(Î²b(`)
j âˆ’ Î²
âˆ—
j
) âˆˆ R
dâˆ’1 with t âˆˆ [0, 1] such that

Î²ej âˆ’ Î²
âˆ—
j


1
â‰¤ r. Letting Î´e := Î²ej âˆ’ Î²
âˆ—
j
, by (68) we obtain
kÎ´ek2 = tkÎ´k2 â‰¤ 2.2tkÎ´Ik2 = 2.2kÎ´e
Ik2. (69)
Moreover, by Lemma (11) and the relation between `1- and `2-norms we have
kÎ´ek1 = tkÎ´k1 â‰¤ 2.2tkÎ´Gc k1 â‰¤ 2.2
âˆš
2s
âˆ—kÎ´e
Ik2, (70)
where we use the fact that Gc âŠ‚ I and the induction assumption that |Gc
| â‰¤ 2s
âˆ—
. By
Mean-Value theorem, there exists a Î³ âˆˆ [0, 1] such that âˆ‡Lj (Î²ej ) âˆ’ âˆ‡Lj (Î²
âˆ—
j
) = âˆ‡2Lj (Î²1)Î´e,
where Î²1 := Î³Î²
âˆ—
j + (1 âˆ’ Î³)Î²ej âˆˆ R
dâˆ’1
. In what follows we will derive an upper bound for
kÎ´e
Ik2 from Î´eT âˆ‡2Lj (Î²1)Î´e. Before doing that, we present two lemmas. The first one shows
that the restricted correlation coefficients defined as follows are closely related to the sparse
eigenvalues. This lemma also appear in Zhang (2010) and Zhang et al. (2013) for `2-loss.
Lemma 20 (Local sparse eigenvalues and restricted correlation coefficients) Let m be a
positive integer and M(Â·): R
m â†’ S
m be a mapping from R
m to the space of mÃ—m symmetric
matrices. We define the s-sparse eigenvalues of M(Â·) over the `1-ball centered at u0 âˆˆ R
m
with radius r as
Ï+
M, u0; s, r
= sup
v,uâˆˆRm

v
TM(u)v: kvk0 â‰¤ s, kvk2 = 1, ku âˆ’ u0k1 â‰¤ r
	
;
Ïâˆ’
M, u0; s, r
= inf
v,uâˆˆRm

v
TM(u)v: kvk0 â‰¤ s, kvk2 = 1, ku âˆ’ u0k1 â‰¤ r
	
.
In addition, we define the restricted correlation coefficients of M over the `1-ball centered
at u0 with radius r as
Ï€
M, u0; s, k, r
:= sup
v,w,uâˆˆRm

v
T
I M(u)wJ

vI


2
v
T
I M(u)vI

wJ


âˆ
: Iâˆ©J =âˆ…, |I|â‰¤s, |J|â‰¤k,

uâˆ’u0


1
â‰¤r

.
Suppose that the local sparse eigenvalue Ïâˆ’
M, u0; s+k, r
> 0, then we have the following
upper bound on the restricted correlation coefficient Ï€(M, u0; s, k):
Ï€
M, u0; s, k, r
â‰¤
âˆš
k
2
r
Ï+
M, u0; k, r.
Ïâˆ’
M, u0; s+k, r
âˆ’1.
Proof See Â§E.1.1 for a detailed pro       
Yang, Ning, and Liu
We denote the restricted correlation coefficients of âˆ‡2Lj (Â·) over the `1-ball centered at
Î²
âˆ—
j with radius r as Ï€j (s1, s2) := Ï€

âˆ‡2Lj , Î²
âˆ—
j
; s1, s2, r
and denote the s-sparse eigenvalues Ïâˆ’

âˆ‡2Lj , Î²
âˆ—
j
; s, r
and Ï+

âˆ‡2Lj , Î²
âˆ—
j
; s, r
as Ïjâˆ’(s) and Ïj+(s) respectively. Applying
Lemma 20 to Ï€j (2s
âˆ—+k
âˆ—
, kâˆ—
) we obtain
Ï€j (2s
âˆ—+k
âˆ—
, kâˆ—
) â‰¤ k
âˆ—1/2
/2 Â·
q
Ïj+(k
âˆ—)/Ïjâˆ’(2s
âˆ—+2k
âˆ—) âˆ’ 1. (71)
By the law of large numbers, if the sample size n is sufficiently large such that âˆ‡2Lj is close
to its expectation E

âˆ‡2Lj

. When Î²j is close to Î²
âˆ—
j
, by Assumption 4, we expect that the
sparse eigenvalue condition also holds for âˆ‡2Lj (Î²j ) with high probability. The following
lemma justifies this intuition.
Lemma 21 Recall that we define the sparse eigenvalues of E

âˆ‡2Lj (Î²
âˆ—
j
)

in Definition 3.
Under Assumptions 2 and 4, if n is sufficiently large such that Ïâˆ— & k
âˆ—Î» log2
d, with probability at least 1âˆ’(2d)
âˆ’1
, for all j âˆˆ [d], there exists a constant CÏ â‰¥ 33Ï
âˆ’1
âˆ—
such that
Ï
âˆ—
jâˆ’(2s
âˆ—+2k
âˆ—
) âˆ’ 0.05Ïâˆ— â‰¤ Ïjâˆ’(2s
âˆ—+2k
âˆ—
) < Ïj+(k
âˆ—
) â‰¤ Ï
âˆ—
j+(k
âˆ—
) + 0.05Ïâˆ—, and
Ïj+(k
âˆ—
)

Ïjâˆ’(2s
âˆ—+2k
âˆ—
) â‰¤ 1 + 0.27k
âˆ—
/sâˆ—
,
where we denote the local sparse eigenvalues Ïâˆ’

âˆ‡2Lj , Î²
âˆ—
j
; s, r
and Ï+

âˆ‡2Lj , Î²
âˆ—
j
; s, r
with
r = CÏ
p
log d/n as Ïjâˆ’(s) and Ïj+(s), respectively.
Proof See Â§E.1.2 for a detailed proof.
Thus by Lemma 21 we have
Ï€j (2s
âˆ—+k
âˆ—
, kâˆ—
) â‰¤ 0.5
q
0.27k
âˆ—2

s
âˆ—. (72)
By (65), (72) and Gc âŠ‚ I we obtain
1 âˆ’ 2Ï€j (2s
âˆ—+k
âˆ—
, kâˆ—
)k
âˆ—âˆ’1
kÎ´eGk1/kÎ´e
Ik2 â‰¥ 1 âˆ’ 1.2
âˆš
0.54 := Îº1, (73)
where we denote Îº1 := 1 âˆ’ 1.2
âˆš
0.54 â‰¥ 0.11. Now we use the second lemma to get an lower
bound of Î´eT âˆ‡2Lj (Î²1)Î´e, which implies an upper bound for kÎ´e
Ik2.
Lemma 22 Let M: R
m â†’ S
m be a mapping from R
m to the space of m Ã— m-symmetric
matrices. Suppose that the sparse eigenvalue Ïâˆ’
M, u0; s + k, r
> 0, let the restricted
correlation coefficients of M(Â·) be defined in Lemma 20. We denote the restricted correlation
coefficients Ï€
M, u0; s, k, r
and s-sparse eigenvalue Ïâˆ’
M, u0; s, r
as Ï€(s, k) and Ïâˆ’(s)
respectively for notational simplicity. For any v âˆˆ R
d
, let F be any index set such that
|F
c
| â‰¤ s, let J be the set of indices of the largest k entries of vF in absolute value and
let I = F
c âˆª J. For any u âˆˆ R
d
such that ku âˆ’ u0k2 â‰¤ r and any v âˆˆ R
d
satisfying
1 âˆ’ 2Ï€(s+k, k)kvF k1/kvIk2 > 0 we have
v
TM(u)v â‰¥ Ïâˆ’(s+k)

kvIk2 âˆ’ 2Ï€(s+k, k)kvF k1

k

kv              
On Semiparametric Exponential Family Graphical Models
Proof See Â§E.1.3 for a detailed proof.
Now applying Lemma 22 to âˆ‡2Lj (Â·) with F =G, s= 2s
âˆ— and k=k
âˆ— we obtain
Î´eT âˆ‡2Lj (Î²1)Î´e â‰¥ Ïjâˆ’(2s
âˆ—+k
âˆ—
)kÎ´e
Ik2

kÎ´e
Ik2 âˆ’ 2Ï€j (2s
âˆ—+k
âˆ—
, kâˆ—
)/kâˆ—
kÎ´eGk1

. (74)
Then by (73), the right-hand side of (74) can be lower bounded by
Î´eT âˆ‡2
`(Î²1)Î´e â‰¥ Îº1Ïjâˆ’(2s
âˆ—+k
âˆ—
)kÎ´e
Ik
2
2 â‰¥ 0.95Îº1Ïâˆ—kÎ´e
Ik
2
2 = Îº2Ïâˆ—kÎ´e
Ik
2
2
, (75)
where we let Îº2 := 0.95Îº1 â‰¥ 0.1. Now we derive an upper bound for Î´eT âˆ‡2Lj (Î²1)Î´e. We
define the symmetric Bregman divergence of Lj (Î²j ) as Dj (Î²1, Î²2) :=


Î²1 âˆ’ Î²2, âˆ‡Lj (Î²1) âˆ’
âˆ‡Lj (Î²2)

, where Î²1, Î²2 âˆˆ R
dâˆ’1
. Then by definition, Î´eT âˆ‡2
`(Î²1)Î´e = Dj (Î²ej , Î²
âˆ—
j
). The following lemma relates Dj (Î²ej , Î²
âˆ—
j
) with Dj (Î²bj , Î²
âˆ—
j
).
Lemma 23 Let Dj (Î²1, Î²2) :=


Î²1 âˆ’ Î²2, âˆ‡Lj (Î²1) âˆ’ âˆ‡L(Î²2)

, Î²(t) = Î²1 + t(Î²2 âˆ’ Î²1),
t âˆˆ (0, 1) be any point on the line segment between Î²1 and Î²2. Then we have
Dj (Î²(t), Î²1) â‰¤ tDj (Î²2, Î²1)
Proof See Â§E.1.4 for a detailed proof.
By Lemma 23 and (60),
Dj (Î²ej , Î²
âˆ—
j
) â‰¤ tDj (Î²bj , Î²
âˆ—
j
) â‰¤ âˆ’t


âˆ‡Lj (Î²
âˆ—
j
), Î´

| {z }
(i)
âˆ’t


Î´,Î»j â—¦ Î¾j

| {z }
(ii)
. (76)
For term (i) in (76), by HÂ¨olderâ€™s inequality we have
âˆ’t


âˆ‡Lj (Î²
âˆ—
j
), Î´

â‰¤ t

âˆ‡GcLj (Î²
âˆ—
j
)


2
kÎ´Gc k2 + t

âˆ‡GLj (Î²
âˆ—
j
)


âˆ
kÎ´Gk1
â‰¤

âˆ‡GcLj (Î²
âˆ—
j
)


2
kÎ´e
Ik2 +

âˆ‡GLj (Î²
âˆ—
j
)


âˆ
kÎ´eGk1, (77)
where the inequality follows from Gc âŠ‚ I. For term (ii) in (76), by (62) and HÂ¨olderâ€™s
inequality we have
âˆ’t


Î´,Î»j â—¦ Î¾j

â‰¤ âˆ’

Î´S,(Î»j â—¦ Î¾j )S

âˆ’


|Î´eG|,Î»G

â‰¤ kÎ»Sk2kÎ´e
Ik2 âˆ’ p
0
Î»
(c2Î»)kÎ´eGk1, (78)
where we use the HÂ¨olderâ€™s inequality and the definition of G. Combining (75),(77) and (78)
we obtain that
Îº2Ïâˆ—

Î´e
I


2
2
â‰¤

âˆ‡GcLj (Î²
âˆ—
j
)


2
+ kÎ»Sk2

kÎ´e
Ik2 +

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
âˆ’ p
0
Î»
(c2Î»)

kÎ´eGk1
â‰¤

âˆ‡GcLj (Î²
âˆ—
j
)


2
+ kÎ»Sk2

Î´e
I


2
,
where the second inequality follows from p
0
Î»
(c2Î») >

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
. From the inequality above
and the induction assumption |Gc
| â‰¤ 2s
âˆ— we obtain that

Î´e
I


2
â‰¤ 10Ï
âˆ’1
âˆ—

âˆ‡GcLj (Î²
âˆ—
j
)


2
+ kÎ»Sk2

â‰¤ 10Ï
âˆ’1
âˆ—
âˆš
s
âˆ—
âˆš
2

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
+ Î»

. (79)        
Yang, Ning, and Liu
Thus (70), (79) and the the fact that 25

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
â‰¤ Î» imply that
kÎ´ek1 â‰¤ 22âˆš
2Ï
âˆ’1
âˆ—
(1 + âˆš
2/25)s
âˆ—Î» < 33Ï
âˆ’1
âˆ—
s
âˆ—Î» â‰¤ r, (80)
where the last inequality follows from the definition of Î». Notice that (80) contradicts our
assumption that kÎ´ek1 = r, the reason for this contradiction is because we assume that

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
> r, hence

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ r and Î²ej = Î²b(`)
j
. This means that Î²b(`)
j
stays in the
`1-ball centered at Î²
âˆ—
j with radius r in each iteration.
Moreover, by (68) and (79), we obtain the following upper bound for kÎ´Ik2 :
kÎ´k2 â‰¤ 22Ï
âˆ’1
âˆ—

âˆ‡GcLj (Î²
âˆ—
j
)


2
+ kÎ»Sk2

â‰¤ 24Ï
âˆ’1
âˆ—
âˆš
s
âˆ—Î»,
where we use the condition that Î» â‰¥ 25

âˆ‡Lj (Î²
âˆ—
j
)


âˆ
. In addition, by(65) and (79) we obtain
the following bound on kÎ´k1
kÎ´k1 â‰¤ 2.2kÎ´Gc k1 â‰¤ 22âˆš
2s
âˆ—Ï
âˆ’1
âˆ—

âˆ‡GcLj (Î²
âˆ—
j
)


2
+ kÎ»Sk2

â‰¤ 33Ï
âˆ’1
âˆ—
s
âˆ—Î», (81)
Therefore going back to the original notations, note that Îº2 â‰¥ 0.1, we establish the following
crude rates of convergence for ` â‰¥ 1:

Î²b(`)
j âˆ’ Î²
âˆ—
j


2
â‰¤ 24Ï
âˆ’1
âˆ—
âˆš
s
âˆ—Î» and

Î²b(`)
j âˆ’ Î²
âˆ—
j


1
â‰¤ 33Ï
âˆ’1
âˆ—
s
âˆ—Î». (82)
And (79) is equivalent to

Î²b(`)
I
`
j
âˆ’ Î²
âˆ—
I
`
j


2
â‰¤ 10Ï
âˆ’1
âˆ—

âˆ‡Ge`
j
Lj (Î²
âˆ—
j
)


2
+

Î»
(`âˆ’1)
Sj


2

, Ge`
j
:= (G
`
j
)
c
. (83)
Note that we use Lemmas 10 and 21, hence (83) and (83) hold with probability at least
1 âˆ’ d
âˆ’1
for all j âˆˆ [d].
Appendix D. Proof of Auxiliary Results for Asymptotic Inference
We prove the auxiliary results for asymptotic inference. More specifically, we first prove
Lemma 14, which is pivotal for deriving the limiting distribution of the pairwise score
statistic. Then we prove the lemmas presented in the proof of Theorem 8.
D.1. Proof of Lemma 14
Proof Before proving this lemma, we first let âˆ‡2Ljk
Î²jâˆ¨k

be the Hessian of Ljk
Î²jâˆ¨k

and define Hjk := E

âˆ‡2Ljk
Î²
âˆ—
jâˆ¨k
. We also define
Î£jk := E

gjk(Xi)gjk(Xi)
T

and Î˜jk := E

h
jk
ii0(Î²
âˆ—
)h
jk
ii0(Î²
âˆ—
)
T

.
Under Assumption 2, we first show that there exists a positive constant D such that for
any j, k âˆˆ d, j 6= k, max
Î£jk

âˆ
,

Hjk

âˆ
,

Î˜jk

âˆ
	
â‰¤ D. The reason is as follows.
Note that HÂ¨olderâ€™s inequality imply

Hjk

âˆ
. max
jâˆˆ[d]
E|Xij âˆ’ Xi
0j
|
4 . max
jâˆˆ[d]
E|Xj |
4
for any j, k âˆˆ [d], j 6= k.          
On Semiparametric Exponential Family Graphical Models
Similarly, for Î˜jk
, we also have

Î˜jk

âˆ
. max
jâˆˆ[d]
E|Xj |
4
. By (14) we have
E|Xj |
4 =
Z âˆ
0
P(|X4|
4 > t)dt â‰¤
Z âˆ
0
c exp(âˆ’t
1/4
)dt = 24c, c = 2 exp(Îºm + Îºh/2).
Moreover, note that by the law of total variance, the diagonal elements of Î£jk are no
larger than the corresponding diagonal elements of Î˜jk; then by Cauchy-Schwarz inequality,
kÎ£jkkâˆ â‰¤ kÎ˜jkkâˆ. Therefore there exists a constant D that does not depend on (s
âˆ—
, n, d)
such that
max 
kHjkkâˆ, kÎ£jkkâˆ, kÎ˜jkkâˆ
	
â‰¤ D, 1â‰¤ j < kâ‰¤ d. (84)
Now we are ready to prove the lemma. Recall that âˆ‡Ljk
Î²jâˆ¨k

is a U-statistic with kernel function h
jk
ii0(Î²jâˆ¨k). Because h
jk
ii0

Î²
âˆ—
jâˆ¨k

is centered, the law of total expectation implies
that E

gjk(Xi)

= 0. Note that the left-hand side of (40) can be written as
âˆš
n
2
b
T âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

=
âˆš
n
2
b
T Ujk +
âˆš
n
2
b
T

âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

âˆ’ Ujk
=
1
âˆš
n
Xn
i=1
b
T gjk(Xi)
| {z }
I1
+
âˆš
n
2
b
T

âˆ‡Ljk
Î²
âˆ—
jâˆ¨k

âˆ’ Ujk
| {z }
I2
.
Notice that I1 is a weighted sum of i.i.d. random variables with the mean and variance
given by
E

b
T gjk(Xi)

= 0 and Var
b
T gjk(Xi)

= b
T Î£jkb.
Central limit theorem implies that I1 N(0, b
T Î£jkb). In what follows we use hii0 and
hii0
|i
to denote h
jk
ii0

Î²
âˆ—
jâˆ¨k

and E

h
jk
ii0

Î²
âˆ—
jâˆ¨k

Xi

= gjk(Xi). Thus we can write I2 as
I2 =
1
âˆš
n(n âˆ’ 1)
X
i<i0
b
T Ï‡ii0, where Ï‡ii0 = (hii0 âˆ’ hii0
|i âˆ’ hii0
|i
0).
Then E(I
2
2
) can be expanded as
E(I
2
2
) = 1
n(n âˆ’ 1)2
X
i<i0
,s<s0
b
TE(Ï‡ii0Ï‡
T
ss0)b. (85)
By the definition of Ï‡ii0, we have
E(Ï‡ii0Ï‡
T
ss0) = E(hii0h
T
ss0) âˆ’ E(hii0h
T
ss0
|s
) âˆ’ E(hii0h
T
ss0
|s
0) âˆ’ E(hii0
|ih
T
ss0)
+ E(hii0
|ih
T
ss0
|s
) + E(hii0
|ih
T
ss0
|s
0) âˆ’ E(hii0
|i
0h
T
ss0) + E(hii0
|i
0h
T
ss0
|s
) + E(hii0
|i
0h
T
ss0
|s
0). (86)
Therefore, for i 6= s, s0 and i
0 6= s, s0
, law of total expectation implies that E(Ï‡ii0Ï‡
T
ss0) = 0.
Similarly, if exactly one of i, i0
is identical to one of s, s0
, say i = s, then (86) becomes
E(Ï‡ii0Ï‡
T
ii00) = E(hii0h
T
ii00) âˆ’ E(hii0h
T
ii00|i
) âˆ’ E(hii0
|ih
T
ii00) + E(hii0
|ih
T
ii00|i
), i6=i
0
6=i
00                   
Yang, Ning, and Liu
Note that by the law of total expectation, for each term in (86) we have
E(hii0h
T
ii00) = E(hii0h
T
ii00|i
) = E(hii0
|ih
T
ii00) = E(hii0
|ih
T
ii00|i
).
Therefore, E(Ï‡ii0Ï‡
T
ii00) = 0. Finally, if i = s and i
0 = s
0
, by the law of total expectation,
(86) can be further reduced to E(Ï‡ii0Ï‡
T
ii0) = E(hii0h
T
ii0) âˆ’ E(hii0
|ih
T
ii0
|i
) âˆ’ E(hii0
|i
0h
T
ii0
|i
0) =
Î˜jk âˆ’ 2Î£jk
. Thus by triangle inequality we have

E(Ï‡ii0Ï‡
T
ii0)


âˆ
â‰¤

E(hii0h
T
ii0)


âˆ
+

E(hii0
|ih
T
ii0
|i
)


âˆ
+

E(hii0
|jh
T
ii0
|j
)


âˆ
â‰¤ 3D,
where the last inequality follows from Assumption 6. Then equation (85) can be reduced
to
E(I
2
2
) = 1
n(n âˆ’ 1)2
X
i<i0
,s<s0
b
TE(Ï‡ii0Ï‡
T
ss)b =
1
n(n âˆ’ 1)2
X
i<i0
b
TE

Ï‡ii0Ï‡
T
ii0

b.
By HÂ¨olderâ€™s inequality we obtain
E(I
2
2
) â‰¤
1
2(n âˆ’ 1)kbk1

E(Ï‡ii0Ï‡
T
ii0)b


âˆ
â‰¤
1
2(n âˆ’ 1)kbk
2
1

E(Ï‡ii0Ï‡
T
ii0)


âˆ
â‰¤
3D
2(n âˆ’ 1)kbk
2
1
. (87)
Since kbk0 â‰¤ se, by the relationship between `1-norm and `2-norm, we can further bound
the right-hand side of (87) by E(I
2
2
) â‰¤ 1.5sD/ e (n âˆ’ 1)â†’ 0, where we use the condition that
limnâ†’âˆ
s/n e = 0. Therefore, we conclude the proof of Lemma 14.
D.2. Proof of Lemma 13
Proof By the definition of wâˆ—
j,k we have H
j
jk,j\k = wâˆ—
j,k
T H
j
j\k,j\k
. We let Î²b0
j = (0, Î²b
j\k
) and
denote âˆ‡2Lj (Î²b0
j
) and âˆ‡2Lj (Î²
âˆ—
j
) as Î› and Î›âˆ—
respectively. In addition, we write Hj
, wâˆ—
j,k
and wbj,k as H, wâˆ— and wb respectively for notational simplicity. Triangle inequality implies
that
kÎ›jk,j\kâˆ’wâˆ—T Î›j\k,j\kkâˆ â‰¤ kHjk,j\kâˆ’Î›jk,j\kkâˆ+kwâˆ—T
(Hj\k,j\kâˆ’Î›j\k,j\k
)kâˆ.
HÂ¨olderâ€™s inequality implies that
kÎ›jk,j\kâˆ’wâˆ—T Î›j\k,j\kkâˆ â‰¤ kÎ›âˆ’Hkâˆ(1 + kwâˆ—
k1). (88)
Under null hypothesis, Î²
âˆ—
jk = 0. By Lemma 26, we have kÎ›âˆ’Hkâˆ . s
âˆ—Î» log2
d. Then the
right-hand side of (88) is bounded by
kÎ›jk,j\kâˆ’wâˆ—T Î›j\k,j\kkâˆ . (w0 + 1)s
âˆ—Î» log2
d.
Therefore, by the assumption that Î»D & max{1, w0}s
âˆ—Î» log2
d we can ensure that wâˆ—
is in
the feasible region of the Dantzig selector problem (11), hence we have kwb k1 â‰¤ kwâˆ—k1 â‰¤ w0
4 
On Semiparametric Exponential Family Graphical Models
by the optimality of wb . Let J be the support set of wâˆ—
, that is, J := {(j, `): [wâˆ—
j,k]j` 6=
0, `âˆˆ[d], `6=j}; the optimality of wâˆ—
is equivalent to kwb Jc k1 + kwb J k1 â‰¤ kwâˆ—
J
k1. By triangle
inequality, we have
kwb Jcâˆ’wâˆ—
Jc k1 = kwb Jc k1 â‰¤ kwâˆ—
J k1âˆ’kwb J k1 â‰¤ kwb J âˆ’wâˆ—
J k1, (89)
where J
c
:= {(j, `): (j, `) âˆˆ/ J, j fixed}. Letting Ï‰b = wb âˆ’ wâˆ—
, inequality (89) is equivalent
to kÏ‰bJc k1 â‰¤ kÏ‰bJ k1. Moreover, triangle inequality yields that
kÎ›j\k,j\kÏ‰bkâˆ â‰¤ kÎ›jk,j\k âˆ’ Î›j\k,j\kwb kâˆ + kÎ›jk,j\k âˆ’ Î›j\k,j\kwâˆ—
kâˆ â‰¤ 2Î»D,
where the last inequality follows from that both wâˆ— and wb are feasible for the Dantzig
selector problem (11). Then triangle inequality implies that
|Ï‰b
T Î›j\k,j\kÏ‰b| â‰¤ |Ï‰b
T
J Î›J,j\kÏ‰b|
| {z }
A1
+ |Ï‰
T
JcÎ›Jc,j\kÏ‰b|
| {z }
A2
.
By HÂ¨olderâ€™s inequality and inequality between `1-norm and `2-norms, we obtain that
A1 â‰¤ 2Î»DkÏ‰bJ k1 â‰¤ 2
p
s
?
0Î»DkÏ‰bJ k2 and A2 â‰¤ 2Î»DkÏ‰bJc k1 â‰¤ 2Î»DkÏ‰bJ k1 â‰¤ 2
p
s
?
0Î»DkÏ‰bJ k2.
Hence we conclude that |Ï‰b
T Î›j\k,j\kÏ‰b| â‰¤ 4
p
s
?
0Î»DkÏ‰bJ k2.
We let J1 be the set of indices of the largest k
?
0
component of Ï‰bJc in absolute value
and let I = J1 âˆª J, then |I| â‰¤ s
?
0 + k
?
0
. Under the null hypothesis, kÎ²b0
j âˆ’Î²
âˆ—
j
k1 = kÎ²b
j\k âˆ’
Î²
âˆ—
j\k
k1 â‰¤ 33Ï
âˆ’1
âˆ—
s
âˆ—Î». We denote the s-sparse eigenvalue of âˆ‡2
j\k,j\k
Lj (Î²j ) over the `1-ball
centered at Î²
âˆ—
j with radius r as Ï
0
j+(s) and Ï
0
jâˆ’(s) respectively and denote the corresponding
restricted correlation coefficients as Ï€
0
j
(s1, s2). And we denote these quantities of âˆ‡2Lj (Î²
âˆ—
j
)
as Ïjâˆ’(s), Ïj+(s) and Ï€j (s1, s2). By definition, we immediately have Ïjâˆ’(s) â‰¤ Ï
0
jâˆ’(s) â‰¤
Ï
0
j+(s) â‰¤ Ïj+(s).
By Lemma 22 we have
|Ï‰b
T Î›j\k,j\kÏ‰b| â‰¥ Ï
0
jâˆ’

k
?+s
?
kÏ‰bIk2 âˆ’ 2Ï€
0
j
(s
?+k
?
0
, s?
0
)kÏ‰bJc k1

k
?

kÏ‰bIk2. (90)
The following lemma relates the sparse eigenvalues of âˆ‡2Lj (Î²j ) to those of Eâˆ‡2Lj (Î²
âˆ—
j
).
Lemma 24 Under Assumptions 2, 4 and 7, if n is sufficiently large such that Ïâˆ— &
s
âˆ—Î» log2
d, with probability at least 1âˆ’(2d)
âˆ’1
, for all j âˆˆ [d], there exists a constant CÏ â‰¥ 33Ï
âˆ’1
âˆ—
such that
Ï
âˆ—
jâˆ’(2s
?
0+2k
?
0
) âˆ’ 0.05Î½âˆ— â‰¤ Ïjâˆ’(2s
?
0+2k
?
0
) < Ïj+(k
?
0
) â‰¤ Ï
âˆ—
j+(k
?
0
) + 0.05Î½âˆ—, and
Ïj+(k
?
0
)

Ïjâˆ’(2s
?
0+2k
?
0
) â‰¤ 1 + 0.58k
?
0/s?
0
,
where we denote the local sparse eigenvalues Ïâˆ’

âˆ‡2Lj , Î²
âˆ—
j
; s, r
and Ï+

âˆ‡2Lj , Î²
âˆ—
j
; s, r
with
r = CÏ
p
log d/n as Ïjâˆ’(s) and Ïj+(s), respectively.
Proof The proof is similar to that of Lemma 3, hence is omitted here.     
Yang, Ning, and Liu
By kÏ‰bJc k1 â‰¤ kÏ‰bJ k1 â‰¤
p
s
?
0
kÏ‰bJ k2 and Lemma 24, the right-hand side of (90) can be
reduced to
|Ï‰b
T Î›j\k,j\kÏ‰b| â‰¥ 0.95Î½âˆ—

kÏ‰bIk2 âˆ’ 2Ï€
0
j
(s
?
0+k
?
0
, s?
)kÏ‰bJ k2
p
s
?
0
/k?
0
)kÏ‰bIk2. (91)
Using Lemma 20 we obtain
2Ï€
0
j
(s
?
0+k
?
0
, k?
0
)
p
s
?
0
/k?
0 â‰¤
q
s
?
0
/k?
0
q
Ï
0
j+(k
?
0
)/Ï0
jâˆ’(s
?
0+2k
?
0
) âˆ’ 1
â‰¤
q
s
?
0
/k?
0
q
Ïj+(k
?
0
)/Ïjâˆ’(s
?
0+2k
?
0
) âˆ’ 1 â‰¤
q
s
?
0
/k?
0
q
0.58k
?
0
/s?
0 â‰¤ 0.76.
Thus the right-hand side of (91) can be reduced to
|Ï‰b
T Î›j\k,j\kÏ‰b| â‰¥ 0.95Î½âˆ—(1 âˆ’ 0.76kÏ‰bJ k2

kÏ‰bIk2)kÏ‰bIk
2
2 â‰¥ Î½âˆ—ÎºkÏ‰bIk
2
2
, (92)
where Îº = 0.22. This inequality holds because J âŠ‚ I. By (92) we have
Î½âˆ—ÎºkÏ‰bIk
2
2 â‰¤ 4
p
s
?
0Î»dkÏ‰bJ k2 â‰¤ 4
p
s
?
0Î»dkÏ‰bIk2, which implies kÏ‰bIk2 â‰¤ 4Î½
âˆ’1
âˆ— Îº
âˆ’1p
s
?
0Î»D.
Therefore the estimation error of wbj,k can be bounded by
kÏ‰bk1 â‰¤ 2kÏ‰bJ k1 â‰¤ 2
âˆš
s
?kÏ‰bJ k2 â‰¤ 8Î½
âˆ’1
âˆ— Îº
âˆ’1
s
?
0Î»D â‰¤ 37Î½
âˆ’1
âˆ—
s
?
0Î»D.
Returning to the original notations, we conclude that kwbj,k âˆ’ wâˆ—
j,kk1 â‰¤ 37Î½
âˆ’1
âˆ—
s
?
0Î»D for all
(j, k) such that j, kâˆˆ[d], j 6= k.
D.3. Proof of Lemma 15
Proof We only need to show that Ïƒb
2
jk is a consistent estimator of Ïƒ
2
jk, which is equivalent
to showing that limnâ†’âˆ
|Ïƒb
2
jk âˆ’ Ïƒ
2
jk| = 0. To begin with, triangle inequality implies that
|Ïƒb
2
jk âˆ’ Ïƒ
2
jk| â‰¤

Î£bjk
jk,jk âˆ’ Î£
jk
jk,jk


| {z }
I1
+2

wb
T
j,kÎ£bjk
j\k,jk âˆ’ wâˆ—
j,k
T Î£
jk
j\k,jk


| {z }
I2j
+

wb
T
j,kÎ£bjk
j\k,j\kwbj,k âˆ’ wâˆ—
j,k
T Î£
jk
j\k,j\kwâˆ—
j,k


| {z }
I3j
+ 2

wb
T
k,jÎ£bjk
k\j,jk âˆ’ wâˆ—
k,j
T Î£
jk
k\j,jk


| {z }
I2k
+

wbk,j
T Î£bjk
k\j,k\jwbk,j âˆ’ wâˆ—
k,j
T Î£
jk
k\j,k\jwâˆ—
k,j


| {z }
I3k
,
where Î£bjk = Î£bjk
Î²b0
jâˆ¨k

and Î£bjk
Î²jâˆ¨k

is defined as
Î£bjk(Î²jâˆ¨k) = 1
n
Xn
i=1
n 1
n âˆ’ 1
X
i
06=i
h
jk
ii0(Î²jâˆ¨k)
oâŠ—2
. (93)
To prove the consistency of Ïƒb
2
jk, we need the following theorem to show that Î£bjk is a
consistent estimator of Î£jk in the sense that

Î£bjk âˆ’ Î£jk

âˆ
is negligible.   
On Semiparametric Exponential Family Graphical Models
Lemma 25 For 1â‰¤ j < kâ‰¤ d, let Î£bjk
Î²jâˆ¨k

be defined as (93). Suppose Î²bj and Î²b
k are the
estimators of Î²
âˆ—
j
and Î²
âˆ—
k
obtained from Algorithm 1 and we denote Î²b
jâˆ¨k = (Î²b
jk, Î²bT
j\k
, Î²bT
k\j
)
T
.
Then Î£bjk(Î²b
jâˆ¨k) is a consistent estimator of Î£jk
. There exists a constant CÎ£ that does not
depend on (j, k) such that, with probability tending to one,

Î£bjk(Î²b
jâˆ¨k) âˆ’ Î£jk

âˆ
â‰¤ CÎ£s
âˆ—Î» log2
d for 1â‰¤ j < kâ‰¤ d.
Proof See Â§E.2.1 for a detailed proof.
In the rest of the proof, we will omit the superscripts in both Î£bjk and Î£jk for notational
simplicity. By Lemma 25,
I1 â‰¤ kÎ£b âˆ’ Î£kâˆ â‰¤ OP

s
âˆ—Î» log2
d

. (94)
By triangle inequality, we have the following inequality for I2 :
I2j â‰¤

(wbj,kâˆ’wâˆ—
j,k)
T

Î£b
j\k,jkâˆ’Î£j\k,jk

| {z }
I21
+

(wbj,kâˆ’wâˆ—
j,k)
T Î£j\k,jk


| {z }
I22
+

wâˆ—
j,k
T

Î£b
j\k,jk âˆ’ Î£j\k,jk

| {z }
I23
.
By HÂ¨olderâ€™s inequality, Lemma 25 and the estimation error of wbj,k, we obtain an upperbound for I21 as follows:
I21 â‰¤ kwbj,kâˆ’wâˆ—
j,kk1kÎ£b âˆ’Î£kâˆ = OP

s
âˆ—
s
?
0Î»DÎ» log2
d

. (95)
Similarly, for I22, HÂ¨olderâ€™s inequality implies that
I22 â‰¤ kwbj,kâˆ’wâˆ—
j,kk1kÎ£kâˆ = OP

s
?
0Î»DD

, (96)
where the constant D appears in (84). For I23, by HÂ¨olderâ€™s inequality and 25 we obtain
I23 â‰¤ kwâˆ—
j,kk1kÎ£b âˆ’ Î£kâˆ = OP

w0s
âˆ—Î» log2
d

. (97)
Combining (95), (96) and (97) we have
I2j . (w0 + s
?
0Î»D)s
âˆ—Î» log2
d + s
?
0Î»D. (98)
For I3j , by triangle inequality we have
I3j â‰¤

wb
T
j,k
Î£b
j\k,j\kâˆ’Î£j\k,j\k

wbj,k


| {z }
I31
+

wb
T
j,kÎ£j\k,j\kwbj,kâˆ’wâˆ—
j,k
T Î£j\k,j\kwâˆ—
j,k


| {z }
I32
.
For term I31, HÂ¨olderâ€™s inequality and the optimality of wb implies that
I31 â‰¤ kwbj,kk
2
1kÎ£b
j\k,j\kâˆ’Î£j\k,j\kkâˆ â‰¤ CÎ£w
2
0
s
âˆ—Î» log2
d. (99)
For term I32, Lemma 17 implies that
I32 â‰¤ kÎ£j\k,j\kkâˆkwbj,k âˆ’ wâˆ—
j,kk
2
1 + kÎ£j\k,j\kwâˆ—
j,kkâˆkwbj,k âˆ’ wâˆ—
j,kk1
â‰¤

DÏ‰0s
?
0Î»D + Ds?
0
2
Î»
2
D

,          
Yang, Ning, and Liu
where we use HÂ¨olderâ€™s inequality kÎ£j\k,j\kwâˆ—
j,kkâˆ â‰¤ kwâˆ—
j,kk1kÎ£kâˆ â‰¤ Dw0. By (99), (100)
and Î»D & w0s
âˆ—Î» log2
d, we obtain
I3j . w
2
0
s
âˆ—Î» log2
d +

DÏ‰0s
?
0Î»D + Ds?
0
2
Î»
2
D

. (101)
Therefore combining (94), (98) and (101) we obtain I1 + I2j + I3j = oP(1). We can show
similarly that I2k + I3k = oP(1). Thus limnâ†’âˆ
max
j<k

Ïƒb
2
jk âˆ’ Ïƒ
2
jk

 = 0 with probability converging
to one.
Appendix E. Proof of Technical Lemmas
Finally, we prove the technical lemmas in this appendix. Specifically, we prove the lemmas
introduced to derive the auxiliary results.
E.1. Proof of Technical Lemmas in Â§C
In this subsection we prove the technical lemmas we use to prove the auxiliary results of
estimation. These lemmas are standard for high-dimensional linear regression, but proving
them for our logistic-type loss function needs nontrivial extensions.
E.1.1. Proof of Lemma 20
Proof Let I and J be two index sets with I âˆ© J = âˆ…, |I| â‰¤ s, |J| â‰¤ k, for any u âˆˆ R
d with
ku âˆ’ u0k2 â‰¤ r and any v, w âˆˆ R
d
, let Î¸ = vI + Î±wJ with some Î± âˆˆ R, then by definition,
kÎ¸k0 â‰¤ s + k. For notational simplicity, we denote s-sparse eigenvalues Ï+
M, u0; s, r) and
Ïâˆ’
M, u0; s, r) as Ïâˆ’(s) and Ï+(s) respectively. By definition, we have
Ïâˆ’(s+k)kÎ¸k
2
2 â‰¤ Î¸
TM(u)Î¸ = v
T
I M(u)vI
| {z }
A1
+2Î± v
T
I M(u)wJ
| {z }
A2
+Î±
2 wT
J M(u)wJ
| {z }
A3
. (102)
Since kÎ¸k
2
2 = kvIk
2
2 + Î±
2kwJ k
2
2
. Rearranging the terms in (102) we have

A3âˆ’Ïâˆ’(s+k)kwJ k
2
2

Î±
2 + 2A2Î± +

A1âˆ’Ïâˆ’(s+k)kvIk
2
2

â‰¥ 0 for all Î± âˆˆ R. (103)
Note that the left-hand side (103) is a univariate quadratic function in Î±, thus (103) implies
that

A1âˆ’Ïâˆ’(s+k)kvIk
2
2
A3âˆ’Ïâˆ’(s+k)kwJ k
2
2

â‰¥ A
2
2
. (104)
Therefore by multiplying 4kvIkâˆ
2

(A2
1
kwJ k
2
2
) to both sides of (104) we have
4A2
2
kvIk
2
2
A2
1
kwJ


2
2
â‰¤
4kvIk
2
2
A1kwJ k
2
2

A1âˆ’Ïâˆ’(s+k)

vI


2
2
A1


A3âˆ’Ïâˆ’(s+k)kwJ k
2
2

. (105)
By the inequality of arithmetic and geometric means, we have
Ïâˆ’(s+k)kvIk
2
2
A1

A1âˆ’Ïâˆ’(s+k)kvIk
2
2
A1

â‰¤
1
4
.             
On Semiparametric Exponential Family Graphical Models
Then the right-hand side of (104) can be bounded by
4A2
2
kvIk
2
2
A2
1
kwJ k
2
2
â‰¤
A3 âˆ’ Ïâˆ’(s+k)kwJ k
2
2
Ïâˆ’(s+k)kwJ k
2
2
â‰¤
Ï+(k)
Ïâˆ’(s+k)
âˆ’ 1,
where the last inequality follows from A3 â‰¤ Ï+(k)kwJ k
2
2
. Note that by the relationship
between `2- and `âˆ norm, we have kwJ k2 â‰¤
âˆš
kkwJ kâˆ, which further implies that
v
T
I M(u)wJ kvIk2
v
T
I M(u)vIkwJ kâˆ
â‰¤
âˆš
kv
T
I M(u)wJ kvIk2
v
T
I M(u)vIkwJ k2
=
âˆš
kA2kvIk2
A1kwJ k2
â‰¤
âˆš
k
2
p
Ï+(k)/Ïâˆ’(s + k) âˆ’ 1.
Taking supremum over v, w âˆˆ R
d finally yields Lemma 20.
E.1.2. Proof of lemma 21
Proof Under Assumption 4, for any Î²j âˆˆ R
dâˆ’1
such that kÎ²j âˆ’Î²
âˆ—
j
k2 â‰¤ r and any v âˆˆ R
dâˆ’1
with kvk0 â‰¤ 2s
âˆ—+ 2k
âˆ—
, we denote âˆ‡2Lj (Î²j ) âˆ’ âˆ‡2Lj (Î²
âˆ—
j
) and âˆ‡2Lj (Î²j ) âˆ’ E

âˆ‡2Lj (Î²
âˆ—
j
)

as
Î›1 and Î›2 respectively. Our goal is to show that both |v
T Î›1v| and |v
T Î›2v| are negligible. HÂ¨olderâ€™s inequality implies that

v
T Î›2v

 â‰¤ kvk1kÎ›2vkâˆ â‰¤ kvk
2
1
kÎ›2kâˆ. We use the
following lemma to control |v
>Î›1v| and kÎ›2kâˆ.
Lemma 26 We denote s
âˆ— = maxjâˆˆ[d] kÎ²
âˆ—
j
k0. Let r1(s
âˆ—
, n, d) > 0 be a real number depending on s
âˆ—
, n, and d that satisfy limnâ†’âˆ
r1(s
âˆ—
, n, d) log2
d = 0. We define Bj (r1) :=

Î²j âˆˆ
R
dâˆ’1
:

Î²jâˆ’Î²
âˆ—
j


1
â‰¤ r1(s
âˆ—
, n, d)
	
as the `1-ball centered at Î²
âˆ—
j with radius r1(s
âˆ—
, n, d). Under
Assumptions 2 and 4, there exist absolute constants Ch, Cr > 0 such that, with probability
at least 1 âˆ’ (2d)
âˆ’1
, for all j âˆˆ [d], Î²j âˆˆ Bj (r1) and v âˆˆ R
d
, it holds that,

âˆ‡2Lj (Î²
âˆ—
j
) âˆ’ E

âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
â‰¤ Ch
p
log d/n, (106)

âˆ‡2Lj (Î²j ) âˆ’ âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
â‰¤ Crr1(s
âˆ—
, n, d) Â· log2
d, (107)

v
T

âˆ‡2Lj (Î²j ) âˆ’ âˆ‡2Lj (Î²
âˆ—
j
)v

 â‰¤ Crr1(s
âˆ—
, n, d) Â· kvk
2
2
. (108)
Proof See Â§E.3 for a detailed proof.
Lemma 26 implies that kÎ›2kâˆ â‰¤ Ch
p
log d/n with probability at least 1 âˆ’ (2d)
âˆ’1
. By
the relation between `1- and `2-norms, we have

v
T Î›2v

 â‰¤ (2s
âˆ—+ 2k
âˆ—
)kvk
2
2kÎ›kâˆ â‰¤ (2s
âˆ—+ 2k
âˆ—
)Ch
p
log d/n.
Moreover, setting r = CÏs
âˆ—p
log d/n with CÏ â‰¥ 33Ï
âˆ’1
âˆ—
, we have
|v
T Î›1v| â‰¤ CrCÏkvk
2
1 â‰¤ CrCÏ(2s
âˆ—+ 2k
âˆ—
)
p
log d/n.
By Assumption 4, if n is large enough such that (2s
âˆ—+ 2k
âˆ—
)(CrCÏ +Ch)
p
log d/n
â‰¤ 0.05Ïâˆ—,
then we have
0.95Ïâˆ— â‰¤ Ï
âˆ—
jâˆ’(2s
âˆ—+ 2k
âˆ—
) âˆ’ 0.05Ïâˆ— â‰¤ Ïjâˆ’(2s
âˆ—+ 2k
âˆ—
) < Ïj+(k
âˆ—
) â‰¤ Ï
âˆ—
j+(k
âˆ—
) + 0.05Ïâˆ—,
45      
Yang, Ning, and Liu
where we denote the s-sparse eigenvalues Ïâˆ’

âˆ‡2Lj , Î²
âˆ—
j
; s, r
and Ï+

âˆ‡2Lj , Î²
âˆ—
j
; s, r
as Ïjâˆ’(s)
and Ïj+(s) respectively. Under Assumption 4, Ï
âˆ—
j+(k
âˆ—
)

Ï
âˆ—
jâˆ’(2s
âˆ—+ 2k
âˆ—
) â‰¤ 1 + 0.2k
âˆ—/sâˆ— and
k
âˆ— â‰¥ 2s
âˆ—
, simple computation yields that
Ïj+(k
âˆ—
)
Ïjâˆ’(2s
âˆ—+ 2k
âˆ—)
â‰¤
Ï
âˆ—
j+(k
âˆ—
) + 0.05Ïâˆ—
Ï
âˆ—
jâˆ’(2s
âˆ—+ 2k
âˆ—) âˆ’ 0.05Ïâˆ—
â‰¤
Ï
âˆ—
j+(k
âˆ—
) + 0.05Ï
âˆ—
jâˆ’(2s
âˆ—+ 2k
âˆ—
)
0.95Ï
âˆ—
jâˆ’(2s
âˆ—+ 2k
âˆ—)
â‰¤ 1 + 0.27k
âˆ—
/sâˆ—
.
Thus, we conclude the proof of Lemma 26.
E.1.3. Proof of Lemma 22
Proof For v = (v1, . . . , vd)
T âˆˆ R
d
, without loss of generality, we assume that F
c = [s1]
where s1 =|F
c
| â‰¤ s. In addition, we assume that when j > s1, vj is arranged in descending
order of |vj |. That is, we rearrange the components of v such that |vj | â‰¥ |vj+1| for all
j â‰¥ s1. Let J0 = [s1] and Ji = {s1+ (i âˆ’ 1)k+1, . . . , min(s1 + ik, d)}. By definition, we
have J = J1 and I = J0 âˆª J1. Moreover, we have kvJi
kâˆ â‰¤ kvJiâˆ’1
k1

k when i â‰¥ 2 because
by the definition of Ji
, we have P
iâ‰¥2
kvJi
kâˆ â‰¤ kvF k1

k. Note that by the definition of
index sets I and Ji
, |Ji
| â‰¤ k and |I| = k+s1 â‰¤ k+s. We denote the restricted correlation
coefficients Ï€(M, u0; s, k, r) as Ï€(s, k), then by the definition of Ï€(s+k, k) we have

v
T
I M(u)vJi

â‰¤ Ï€(s+k, k)

v
T
I M(u)vI

kvJi
kâˆ

kvIk2.
Thus we have the following upper bound for

v
T
I M(u)vI
c


:

v
T
I M(u)vI
c

 â‰¤
X
iâ‰¥2

v
T
I M(u)vJi

 â‰¤ Ï€(s+k, k)kvIk
âˆ’1
2

v
T
I M(u)vI
X
iâ‰¥2
kvJi
kâˆ
â‰¤ Ï€(s+k, k)kvIk
âˆ’1
2

v
T
I M(u)vI

kvF k1

k. (109)
Because v
TM(u)v â‰¥ v
T
I M(u)vI + 2v
T
I M(u)vI
c , by (109) we have
v
TM(u)v â‰¥ v
T
I M(u)vI âˆ’ 2Ï€(s+k, k)kvIk
âˆ’1
2

v
T
I M(u)vI

kvF k1

k
=

v
T
I M(u)vI
1 âˆ’ 2Ï€(s+k, k)kvIk
âˆ’1
2
kvF k1

k

.
Thus we can bound the right-hand side of the last formula using the sparse eigenvalue
condition
v
TM(u)v â‰¥ Ïâˆ’(s+k)

1 âˆ’ 2Ï€(s+k, k)k
âˆ’1
kvIk
âˆ’1
2
kvF k1

kvIk
2
2
, (110)
where we denote s-sparse eigenvalue Ïâˆ’(M, u0; s, r) as Ïâˆ’(s + k) for the simplicity of notations. Inequality (110) concludes the proof of Lemma 22.
E.1.4. Proof of Lemma 23
Proof Let F(t) = Lj

Î²(t)

âˆ’ Lj (Î²1) âˆ’


âˆ‡Lj (Î²1), Î²(t) âˆ’ Î²1

. Since the derivative of
Lj

Î²(t)

with respect to t is 

âˆ‡Lj

Î²(t)

, Î²2 âˆ’ Î²1

, the derivative of F is given by
F
0
(t) = 

âˆ‡Lj

Î²(t)

âˆ’ âˆ‡Lj (Î²1), Î²2 âˆ’ Î²1
                    
On Semiparametric Exponential Family Graphical Models
Therefore the Bregman divergence Dj

Î²(t), Î²1

can be written as
Dj

Î²(t), Î²1

=


âˆ‡Lj [Î²(t)] âˆ’ âˆ‡Lj (Î²1), t(Î²2 âˆ’ Î²1)

= tF0
(t).
By definition, it is easy to see that F
0
(1) = Dj (Î²2, Î²1). To derive Lemma 23, it suffices to
show that F(t) is convex, which implies that F
0
(t) is non-decreasing and Dj

Î²(t), Î²1

=
tF0
(t) â‰¤ tF0
(1) = tDj (Î²2, Î²1).
For âˆ€t1, t2 âˆˆ R+, t1 + t2 = 1, x, y âˆˆ (0, 1), by the linearity of Î²(t), Î²(t1x + t2y) =
t1Î²(x) + t2Î²(y). Then we have


âˆ‡Lj (Î²1), Î²(t1x + t2y) âˆ’ Î²1

= t1


âˆ‡Lj (Î²1), Î²(x) âˆ’ Î²1

+ t2


âˆ‡Lj (Î²1), Î²(y) âˆ’ Î²1

. (111)
In addition, by convexity of function Lj (Â·), we obtain
Lj

Î²(t1x + t2y)

â‰¤ t1Lj

Î²(x)

+ t2Lj

Î²(y)

. (112)
Adding (111) and (112) we obtain
F

t1x + t2y

â‰¤ t1F(x) + t2F(y).
Therefore F(t) is convex, thus we have Dj (Î²(t), Î²1) â‰¤ tDj (Î²2, Î²1).
E.2. Proof of Technical Lemmas in Â§D
Now we prove the lemmas that supports the auxiliary inferential results. We first prove
Lemma 25, which implies that the Ïƒb
2
jk is a consistent estimator of the asymptotic variance
of Ïƒjk.
E.2.1. Proof of lemma 25
Proof Recall that we denote Î²jâˆ¨k = (Î²jk, Î²j\k
, Î²k\j
) and Ljk
Î²jâˆ¨k

= Lj (Î²j )+Lk(Î²k). We
denote the kernel function of the second-order U-statistic âˆ‡Ljk
Î²jâˆ¨k

as h
jk
ii0

Î²jâˆ¨k

where
the subscripts i, i0
indicate that h
jk
ii0(Â·) depends on Xi and Xi
0. We define V
jk
ii0
i
00
Î²jâˆ¨k

:=
h
jk
ii0

Î²jâˆ¨k

h
jk
ii0

Î²jâˆ¨k
T
. Then by definition, Î£bjk
Î²j\k

can be written as
Î£bjk
Î²jâˆ¨k

=
1
n(n âˆ’ 1)2
Xn
i=1
X
i
06=i,i006=i
V
jk
ii0
i
00
Î²jâˆ¨k

.
Note that Î£bjk
Î²jâˆ¨k

âˆ’ Î£jk = Î£bjk
Î²jâˆ¨k

âˆ’ Î£bjk
Î²
âˆ—
jâˆ¨k

| {z }
I1
+ Î£bjk
Î²
âˆ—
jâˆ¨k

âˆ’ Î£jk
| {z }
I2
.
We first consider I2. For notational simplicity, we use hii0 and hii0
|i
to denote h
jk
ij
Î²
âˆ—
jâˆ¨k

and h
jk
ii0
|i

Î²
âˆ—
jâˆ¨k

:= E

h
jk
ij
Î²
âˆ—
jâˆ¨k

Xi

respectively. As shown in Â§D.1, for i 6= i
0 6= i
00
,
E

hii0h
T
ii00
= E

hii0h
T
ii00

Xi

= E

hii0
|ih
T
ii00|i

= Î£jk and E

hijh
T
ij
= Î˜jk                             
Yang, Ning, and Liu
I2 =
n âˆ’ 2
n âˆ’ 1
n
3
âˆ’X
1
i<i0<i00

Vii0
i
00âˆ’E(Vii0
i
00)


| {z }
I21
+
1
n âˆ’ 1
n
2
âˆ’X
1
i<i0

Vii0
i
0âˆ’E(Vii0
i
0)


| {z }
I22
+
1
n âˆ’ 1

Î˜jkâˆ’Î£jk
,
where we use Vii0
i
00 to denote V
jk
ii0
i
00(Î²
âˆ—
jâˆ¨k
). Observing that I21 is a centered third order
U-statistic, for x large enough such that x
4 â‰¥

E

Vijk(Î²
âˆ—
jâˆ¨k
)


âˆ
and for any (a, b),(c, d) âˆˆ

(p, q): p, q âˆˆ {j, k}
	
we have
P
V
jk
ii0
i
00(Î²jâˆ¨k)

ab,cd > 2x
4

â‰¤ P

(Xia âˆ’ Xi
0a)(Xib âˆ’ Xi
0b)(Xic âˆ’ Xi
00c)(Xid âˆ’ Xi
00d) > x4

â‰¤ 8 exp(2Îºm + Îºh) exp(âˆ’x).
Thus there exist constants c1 and C1 that does not depend on n or d or (j, k) such that for
any x âˆˆ R, any i, i0
, i00 âˆˆ [n] and any j, k âˆˆ [d],
P

[V
jk
ii0
i
00(Î²
âˆ—
jâˆ¨k
)]ab,cd > x
â‰¤ C1 exp(c1x
1/4
). (113)
This implies that there exists some generic constant C such that kV
jk
ii0
i
00(Î²
âˆ—
jâˆ¨k
)kâˆ â‰¤ C log4
d
for all j, k âˆˆ [d] and i, i0 âˆˆ [n] with probability tending to one. Similar to the method we
use in Â§E.3, we define E :=

kV
jk
ii0
i
00(Î²
âˆ—
jâˆ¨k
)kâˆ â‰¤ C log4
d, âˆ€i, i0
, i00 âˆˆ [n], j, k âˆˆ [d]
	
. By
Bernsteinâ€™s inequality for U-statistics (Lemma 19) with b = C log4
d in (56), for some
generic constants C, it holds with high probability that

n
2
âˆ’1
X
i<i0

Vii0
i
0âˆ’E(Vii0
i
0|E)

â‰¤ C
p
log d/n, âˆ€j, k âˆˆ [d], i, i0
, i00 âˆˆ [n]. (114)
Moreover, by (113), we have
E

[Vii0
i
0(Î²
âˆ—
jâˆ¨k
)]ab,cd|E	
âˆ’ E

[Vii0
i
00(Î²
âˆ—
jâˆ¨k
)]ab,cd	
â‰¤
Z âˆ
C log4 d
P


[V
jk
ii0
i
00(Î²
âˆ—
jâˆ¨k
)]ab,cd

 > x	
â‰¤ c1 log3
d Â· exp(âˆ’c2 log d) (115)
for some absolute constant c1 and c2. Since (115) holds uniformly, we have

n
2
âˆ’1
X
i<i0

E(Vii0
i
0|E) âˆ’ E(Vii0
i
00)

â‰¤ log3
d Â· exp(âˆ’c2 log d) .
p
log d/n. (116)
Combining (114) and (116) we obtain that
kI21kâˆ = OP
p
log d/n
uniformly for 1 â‰¤ j < k â‰¤ n. (117)
For the second part I22, noting that it is a U-statistic of order 2, because (113) also holds
for Vii0
i
00(Î²
âˆ—
jâˆ¨k
), applying the same technique, we have kI21kâˆ = OP
p
log d/n
uniforml                   
On Semiparametric Exponential Family Graphical Models
for 1 â‰¤ j < k â‰¤ n. Combining with (117), we conclude that, for some absolute constant C,
we have

Î£bjk
Î²
âˆ—
jâˆ¨k

âˆ’ Î£jk

âˆ
â‰¤ C
p
log d/n, âˆ€1 â‰¤ j < k â‰¤ n. (118)
Now we turn to I1. For any Î²j , Î²k âˆˆ R
dâˆ’1
such that kÎ²j âˆ’ Î²
âˆ—
j
k1 â‰¤ r(s
âˆ—
, n, d) and
kÎ²k âˆ’ Î²
âˆ—
k
k1 â‰¤ r(s
âˆ—
, n, d), we denote Ï‰
j
ii0
:= exp
âˆ’(Xij âˆ’ Xi
0j )(Î²j âˆ’ Î²
âˆ—
j
)
T
(Xi\j âˆ’ Xi
0\j
)

and
denote Ï‰
k
ii0 similarly. Recall that we denote R
j
ii0(Î²j ) = exp
âˆ’(xij âˆ’ xi
0j )Î²j
T
(xi\j âˆ’ xi
0\j
)

.
Hence by definition we have R
j
ii0(Î²j ) = Ï‰
j
ii0R
j
ii0(Î²
âˆ—
j
). As shown in Â§E.3, we have
min{1, Ï‰
j
ii0, Ï‰k
ii0}h
jk
ii0(Î²
âˆ—
jâˆ¨k
) â‰¤ h
jk
ii0(Î²jâˆ¨k) â‰¤ max{1, Ï‰
j
ii0, Ï‰k
ii0}h
jk
ii0(Î²
âˆ—
jâˆ¨k
), (119)
where the inequality is taken elementwisely. We denote b := maxi,i0âˆˆ[n];jâˆˆ[d] r(s
âˆ—
, n, d)

(Xijâˆ’
Xi
0j )(Xi\jâˆ’Xi
0\j
)


âˆ
. Note that when kÎ²jâˆ’Î²
âˆ—
j
k1 â‰¤ r(s
âˆ—
, n, d) and kÎ²kâˆ’Î²
âˆ—
k
k1 â‰¤ r(s
âˆ—
, n, d),
we have Ï‰
j
ii0, Ï‰k
ii0 âˆˆ [exp(âˆ’b), exp(b)]. Therefore by (119) and the definition of V
jk
ii0
i
00
Î²j\k

,
we obtain the following elementwise inequality
exp(âˆ’2b)V
jk
ii0
i
00
Î²
âˆ—
j\k

â‰¤ V
jk
ii0
i
00
Î²j\k

â‰¤ exp(2b)V
jk
ii0
i
00
Î²
âˆ—
j\k

,
which implies that

Î£bjk
Î²jâˆ¨k

âˆ’ Î£bjk
Î²
âˆ—
jâˆ¨k


âˆ
â‰¤ max
1 âˆ’ exp(âˆ’2b), exp(2b) âˆ’ 1
	
Î£bjk
Î²
âˆ—
jâˆ¨k


âˆ
. (120)
As we show in Â§E.3, b â‰¤ Cr(s
âˆ—
, n, d) log2
d with high probability for some absolute constant
C > 0. Since limnâ†’âˆ
r(s
âˆ—
, n, d) log2
d = 0, by (120) we have

Î£bjk
Î²jâˆ¨k

âˆ’ Î£bjk
Î²
âˆ—
jâˆ¨k


âˆ
. b

Î£bjk
Î²
âˆ—
jâˆ¨k


âˆ


âˆ
â‰¤ b

Î£bjk
Î²
âˆ—
jâˆ¨k

âˆ’ Î£jk

âˆ
+ bkÎ£jkkâˆ.
Note that we show kI2kâˆ =

Î£bjk
Î²
âˆ—
jâˆ¨k

âˆ’ Î£jk

âˆ
= OP
p
log d/n
, which converges to
zero asymptotically. Thus we conclude that

Î£bjk
Î²jâˆ¨k

âˆ’ Î£bjk
Î²
âˆ—
jâˆ¨k


âˆ
= OP

r(s
âˆ—
, n, d) log2
d

. (121)
Combining (118) and (121), we have the following error bound for Î£bjk
Î²jâˆ¨k

:

Î£bjk
Î²jâˆ¨k

âˆ’ Î£jk

âˆ
= OP

r(s
âˆ—
, n, d) log2
d +
p
log d/n
for all (j, k). (122)
Finally, by the fact that maxjâˆˆ[d] kÎ²bj âˆ’ Î²
âˆ—
j
k1 . s
âˆ—Î», we conclude the proof of Lemma 25 by
setting r = Csâˆ—Î».
E.3. Proof of Lemma 26
Now we turn to the last unproven result, namely Lemma 26, which characterizes the perturbation                       
Yang, Ning, and Liu
Proof Note that âˆ‡2Lj (Î²j ) is a second-order U-statistic. Hence âˆ‡2Lj (Î²j ) âˆ’ E

âˆ‡2Lj (Î²j )

is a centered U-statistic. We denote its kernel as Tii0(Î²j ), then
âˆ‡2Lj (Î²j ) âˆ’ E

âˆ‡2Lj (Î²j )

=
2
n(n âˆ’ 1)
X
i<i0
Tii0(Î²j ).
Note that

E[Tii0(Î²j )]


âˆ
is bounded for all Î²j âˆˆ R
dâˆ’1 because
max
uâˆˆRdâˆ’1

E[Tii0(Î²j )]


âˆ
. max
jâˆˆ[d]
E|Xij âˆ’ Xi
0j
|
4 . max
jâˆˆ[d],iâˆˆ[n]
E|Xij |
4 â‰¤
Z âˆ
0
c exp(âˆ’t
1/4
)dt = 24c,
where c = 2 exp(Îºm + Îºh/2). Here the last inequality follows from (14). Let âˆ‡2
jk,j`Lj (Î²j ) =
âˆ‚
2Lj (Î²j )
âˆ‚Î²jkâˆ‚Î²j`
and let
Tii0(Î²j )

k` be the corresponding kernel function. That is,
âˆ‡2
jk,j`Lj (Î²j ) =
n
2
âˆ’1 P
i<i0

Tii0(Î²j )

k`. For x > 0 such that x
4 > 24c and k, ` 6= j, we have
P


[Tii0(Î²
âˆ—
j
)]k`

 > 2x
4
	
â‰¤ P

(Xij âˆ’ Xi
0j )
2
(Xik âˆ’ Xi
0k)(Xi` âˆ’ Xi
0`) > x4

â‰¤ P

|Xij âˆ’ Xi
0j
| > x
+ P

|Xik âˆ’ Xi
0k| > x
+ P

|Xi` âˆ’ Xi
0`
| > x
. (123)
As a direct implication of Assumption 2, we have P

|Xij âˆ’ Xij | > x
â‰¤ 2 exp(2Îºm +
Îºk) exp(âˆ’x) for all j âˆˆ [d]. Then we can bound the right-hand side of (123) by
P


[Tii0(Î²
âˆ—
j
)]k`

 > 2x
4
	
â‰¤ 6 exp(2Îºm + Îºh) exp(âˆ’x) when x
4 > 48 exp(Îºm + Îºh/2).
Letting CT = maxn
6 exp(2Îºm + Îºh), exp
[48 exp(Îºm + Îºh/2)]1/4
	o
, it holds that
P


[Tii0(Î²
âˆ—
j
)]k`

 > x	
â‰¤ CT exp(âˆ’2
âˆ’1/4x
1/4
) for all x > 0. (124)
Thus by a union bound, we conclude that there exists some generic constant C such that
kTii0(Î²
âˆ—
j
)kâˆ â‰¤ C log4
d for all j âˆˆ [d] and i, i0 âˆˆ [n] with probability at least 1 âˆ’ (8d)
âˆ’1
.
We define an event E :=

kTii0(Î²
âˆ—
j
)kâˆ â‰¤ C log4
d, âˆ€i, i0 âˆˆ [n], j âˆˆ [d]
	
. By (124), it is easy
to see that Tii0(Î²
âˆ—
j
) is `2-integrable. By Bernsteinâ€™s inequality for U-statistics (Lemma 19)
with b = C log4
d in (56), for some generic constants C1 and C2, we obtain that
P

âˆ‡2Lj (Î²j ) âˆ’ E1

âˆ‡2Lj (Î²j )

> t

E

â‰¤ 4 exp
âˆ’nt2

(C1 + C2 log4
Â·t)

, âˆ€j âˆˆ [d]. (125)
Here we use E1

âˆ‡2Lj (Î²j )

to denote E

âˆ‡2Lj (Î²j )

E

. Thus under Assumption 4 we obtain
that, conditioning on event E,

âˆ‡2Lj (Î²j ) âˆ’ E1

âˆ‡2Lj (Î²j )


âˆ
â‰¤ C
p
log d/n, âˆ€j âˆˆ [d] (126)
with probability at least 1 âˆ’ (8d)
âˆ’1
. Moreover, by (124) we obtain that
E

[Tii0(Î²
âˆ—
j
)]k`

E
	
âˆ’E

[Tii0(Î²
âˆ—
j
)]k`	
â‰¤
Z âˆ
C log4 d
P


[Tii0(Î²
âˆ—
j
)]k`

 > x	
â‰¤ c1 log3
dÂ·exp(âˆ’c2 log d)
for some absolute constant c1 and c2. Therefore we have

E1

âˆ‡2Lj (Î²j )

âˆ’ E

âˆ‡2Lj (Î²j )


âˆ
. log3
d Â· exp(âˆ’c2 log d) .
p
log d/n. (12                              
On Semiparametric Exponential Family Graphical Models
Combining (126) and (127) we show that, with probability at least 1âˆ’(4d)
âˆ’1
,

âˆ‡2Lj (Î²
âˆ—
j
)âˆ’
E[âˆ‡2Lj (Î²
âˆ—
j
)]


âˆ
â‰¤ Ch
p
log d/n for all j âˆˆ [d].
For the second argument (107), let âˆ† = Î²j âˆ’ Î²
âˆ—
j where Î²j âˆˆ R
dâˆ’1
lies in the `1-ball
centered at Î²
âˆ—
j with radius r1(s
âˆ—
, n, d), that is,

Î²jâˆ’Î²
âˆ—
j


1
â‰¤ r1(s
âˆ—
, n, d). By the independence
between Xi and Xi
0, Assumption 2 implies that
maxn
log E

exp(Xij âˆ’ Xi
0j )

, log E

exp(Xi
0j âˆ’ Xij )
o
â‰¤ 2Îºm + Îºh,
which further implies that for any x > 0
P

(Xij âˆ’ Xi
0j )

 > x
â‰¤ 2 exp(2Îºm + Îºh) exp(âˆ’x), âˆ€j âˆˆ [d].
Hence for any x > 0 and j, k âˆˆ [d], a union bound implies that
P

(Xij âˆ’ Xi
0j )(Xik âˆ’ Xi
0k)

 > x2

â‰¤ P

(Xij âˆ’ Xi
0j )

 > x
+ P

(Xik âˆ’ Xi
0k)

 > x
â‰¤ 4 exp(2Îºm + Îºh) exp(âˆ’x). (128)
Taking a union bound over 1â‰¤ j < kâ‰¤ d and 1 â‰¤ i < i0 â‰¤ n we obtain that
P
h
max
i,i0âˆˆ[n];jâˆˆ[d]

(Xij âˆ’ Xi
0j )(Xi\j âˆ’ Xi
0\j
)


âˆ
> x2
i
. n
2
d
2
exp(âˆ’x).
If we denote b := maxi,i0âˆˆ[n];jâˆˆ[d] r1(s
âˆ—
, n, d)

(Xij âˆ’ Xi
0j )(Xi\j âˆ’ Xi
0\j
)


âˆ
, then we obtain
that b â‰¤ Cr1(s
âˆ—
, n, d) log2
d with probability at least 1 âˆ’ (4d)
âˆ’1
for some constant C > 0.
Denoting Ï‰ii0 := exp
âˆ’(Xij âˆ’ Xi
0j )âˆ†T
(Xi\j âˆ’ Xi
0\j
)
	
, by definition,
R
j
ii0(Î²j ) = exp
âˆ’(Xij âˆ’ Xi
0j )(âˆ† + Î²
âˆ—
j
)
T
(Xi\j âˆ’ Xi
0\j
)
	
= Ï‰ii0R
j
ii0(Î²
âˆ—
j
).
Thus we can write âˆ‡2Lj (Î²j ) as:
âˆ‡2Lj (Î²j ) = 2
n(n âˆ’ 1)
X
i<i0
R
j
ii0(Î²
âˆ—
)(Xij âˆ’ Xi
0j )
2
(Xi\j âˆ’ Xi
0\j
)
âŠ—2

1 + R
j
ii0(Î²âˆ—)
2
Ï‰ii0

1 + R
j
ii0(Î²
âˆ—
)
2

1 + Ï‰ii0R
j
ii0(Î²âˆ—)
2
.
(129)
If Ï‰ii0 â‰¥ 1, then (Ï‰ii0)
âˆ’2 â‰¤

1 + R
j
ii0(Î²
âˆ—
)
21 + Ï‰ii0R
j
ii0(Î²
âˆ—
)
2 â‰¤ 1; otherwise we have
1 â‰¤

1 + R
j
ii0(Î²)
2
/

1 + Ï‰ii0R
j
ii0(Î²
âˆ—
)
2 â‰¤ (Ï‰ii0)
âˆ’2
. This observation implies
min
Ï‰ii0, 1/Ï‰ii0
	
â‰¤
Ï‰ii0

1 + R
j
ii0(Î²)
2

1 + Ï‰ii0R
j
ii0(Î²âˆ—)
2 â‰¤ max
Ï‰ii0, 1/Ï‰ii0
	
. (130)
By the definition of Ï‰ii0, HÂ¨olderâ€™s inequality implies that

(Xij âˆ’Xi
0j )âˆ†T
(Xi\j âˆ’Xi
0\j
)

 â‰¤ b,
thus we have
exp(âˆ’b) â‰¤ min
Ï‰ii01/Ï‰ii0
	
â‰¤ max
Ï‰ii0, 1/Ï‰ii0
	
â‰¤ exp(b).                    
Yang, Ning, and Liu
Combining (129),(130) and (131) we obtain
exp(âˆ’b)âˆ‡2Lj (Î²
âˆ—
j
) â‰¤ âˆ‡2Lj (Î²j ) â‰¤ exp(b)âˆ‡2Lj (Î²
âˆ—
j
). (132)
Then by (132), since limnâ†’âˆ
r1(s
âˆ—
, n, d) log2
d = 0, we have

âˆ‡2Lj (Î²j ) âˆ’ âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
â‰¤ max
1 âˆ’ exp(âˆ’b), exp(b) âˆ’ 1
	
âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
. b

âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
.
Notice that under Assumption 2, as shown in Â§D.1, we can assume that

E

âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
â‰¤
D where D appears in (84). By triangle inequality,

âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
â‰¤

âˆ‡2Lj (Î²
âˆ—
j
)âˆ’E

âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
+

E

âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
â‰¤ D+Ch
p
log d/n â‰¤ 2D
with probability at least 1 âˆ’ (4d)
âˆ’1
, where the last inequality follows from the fact that

log9
d/n1/2
tends to zero as n goes to infinity. Then we obtain that

âˆ‡2Lj (Î²j ) âˆ’ âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
â‰¤ Crr1(s
âˆ—
, n, d) log2
d
holds for some absolute constant Cr > 0 and uniformly for all j âˆˆ [d] and Î²j âˆˆ Bj (r1) with
probability at least 1 âˆ’ (2d)
âˆ’1
.
Finally, for the last argument (108), for any v âˆˆ R
dâˆ’1
, by (132) we have
exp(âˆ’b)v
T âˆ‡2Lj (Î²
âˆ—
j
)v â‰¤ v
T âˆ‡2Lj (Î²j )v â‰¤ exp(b)v
T âˆ‡2Lj (Î²
âˆ—
j
)v.
Thus we have

v
T

âˆ‡2Lj (Î²j ) âˆ’ âˆ‡2Lj (Î²
âˆ—
j
)

v

 . b

v
T âˆ‡2Lj (Î²
âˆ—
j
)v

 â‰¤ bkvk
2
1

âˆ‡2Lj (Î²
âˆ—
j
)


âˆ
,
which implies (108).
