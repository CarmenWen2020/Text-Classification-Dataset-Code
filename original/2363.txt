We study the following basic machine learning task: Given a fixed set of input points in R
d
for a
linear regression problem, we wish to predict a hidden response value for each of the points. We can
only afford to attain the responses for a small subset of the points that are then used to construct
linear predictions for all points in the dataset. The performance of the predictions is evaluated by the
total square loss on all responses (the attained as well as the remaining hidden ones). We show that
a good approximate solution to this least squares problem can be obtained from just dimension d
many responses by using a joint sampling technique called volume sampling. Moreover, the least
squares solution obtained for the volume sampled subproblem is an unbiased estimator of optimal
solution based on all n responses. This unbiasedness is a desirable property that is not shared by
other common subset selection techniques.
Motivated by these basic properties, we develop a theoretical framework for studying volume
sampling, resulting in a number of new matrix expectation equalities and statistical guarantees which
are of importance not only to least squares regression but also to numerical linear algebra in general.
Our methods also lead to a regularized variant of volume sampling, and we propose the first efficient
algorithm for volume sampling which makes this technique a practical tool in the machine learning
toolbox. Finally, we provide experimental evidence which confirms our theoretical findings.
Keywords: volume sampling, linear regression, row sampling, active learning, optimal design
1. Introduction
As an introductory case, consider linear regression in one dimension. We are given n non-zero points
xi
. Each point has a hidden real response (or target value) yi
. Assume that obtaining the responses is
expensive and the learner can afford to request the responses yi for only a small number of indices i.
After receiving the requested responses, the learner determines an approximate linear least squares
solution. In the one dimensional homogeneous case, this is just a single weight. How many response
values does the learner need to request so that the total square loss of its approximate solution on
all n points is “close” to the total loss of the optimal linear least squares solution found with the
knowledge of all responses? We will show here that just one response suffices if the index i is chosen
proportional to x
2
i
. When the learner uses the approximate solution w
∗
i =
yi
xi
, then its expected loss
equals 2 times the loss of the optimum w
∗
that is computed based on all responses (See Figure 1).
Moreover, the approximate solution w
∗
i
is an unbiased estimator for the optimum w
∗
:
Ei
X
j
(xj
yi
xi
− yj )
2

= 2
X
j
(xjw
∗ − yj )
2
and Ei

yi
xi

= w
∗
, when P(i) ∼ x
2
i
.
∗. This paper is an expanded version of two conference papers (Derezinski and Warmuth, 2017, 2018). ´

DEREZINSKI AND ´ WARMUTH
We will extend these formulas to higher dimensions and to sampling more responses by making use of
a joint sampling distribution called volume sampling. We next break down our contributions into four
parts.
x
y
xi
Figure 1: The expected loss of w
∗
i =
yi
xi
(blue line) based on one response yi
is twice
the loss of the optimum w
∗
(green line).
Least squares with dimension many responses. Consider
the case when the points xi
lie in R
d
. Let X denote a full
rank n × d matrix that has the n transposed points x
>
i
as
rows, and let y ∈ R
n be the vector of responses. Now the
goal is to minimize the (total) square loss,
L(w) = Xn
i=1
(x
>
i w − yi)
2 = kXw − yk
2
,
over all linear weight vectors w ∈ R
d
. Let w∗ denote
the optimal such weight vector. We want to minimize
the square loss based on a small number of responses we
attained for a subset of rows. Again, the learner is initially
given the fixed set of n rows (i.e. fixed design), but none
of the responses. It is then allowed to choose a random
subset of d indices, S ⊆ {1..n}, and obtains the responses for the corresponding d rows. The
learner proceeds to find the optimal linear least squares solution w∗
(S) for the subproblem (XS, yS).
where XS is the subset of d rows of X indexed by S and yS the corresponding d responses from
the response vector y. As a generalization of the one-dimensional distribution that chooses an
index based on the squared length, set S of size d is chosen proportional to the squared volume of
the parallelepiped spanned by the rows of XS. This squared volume equals det(X>
S XS). Using
elementary linear algebra, we will show that volume sampling the set S assures that w∗
(S) is a good
approximation to w∗
in the following sense: In expectation, the square loss (on all n row response
pairs) of w∗
(S) is equal d + 1 times the square loss of w∗
(when X is in general position):
E[L(w∗
(S))] = (d + 1)L(w∗
), when P(S) ∼ det(X>
S XS).
Furthermore, we will show that for any sampling procedure that attains less than d responses, the
ratio between the expected loss and the loss of the optimum cannot be bounded by a constant.
Unbiased pseudoinverse estimator. There is a direct connection between solving linear least squares
problems and the pseudoinverse X+ of matrix X: For an n−dimensional response vector y, the
optimal solution is w∗ = argminw ||Xw − y||2 = X+y. Similarly w∗
(S) = (XS)
+yS is the
solution for the subproblem (XS, yS). We propose a new implementation of volume sampling
called reverse iterative sampling which enables a novel proof technique for obtaining elementary
expectation formulas for pseudoinverses based on volume sampling.
x
>
i
n
d
S
s XS
(XS)
+
X IS ISX X+ (ISX)
+
Figure 2: Shapes of the matrices. The indices of S may not be consecutive.
2
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Suppose that our goal is to estimate the pseudoinverse X+ based on the pseudoinverse of a subset
of rows. Recall that for a subset S ⊆ {1..n} of s row indices (where the size s is fixed and s ≥ d),
we let XS be the submatrix of the s rows indexed by S (see Figure 2). Consider a version of X in
which all but the rows of S are zero. This matrix equals ISX, where the selection matrix IS is an
n-dimensional diagonal matrix with (IS)ii = 1 if i ∈ S and 0 otherwise.
For the set S of fixed size s ≥ d row indices chosen proportional to det(X>
S XS), we can prove
the following two expectation formulas (for the second equality, X must be in general position):
E[(ISX)
+] = X+ and E[ (X>
S XS)
−1
| {z }
(ISX)+(ISX)+>
] = n − d + 1
s − d + 1
(X>X)
−1
| {z }
X+X+>
.
Note that (ISX)
+ has the d × n shape of X+ where the s columns indexed by S contain (XS)
+ and
the remaining n − s columns are zero. The expectation of this matrix is X+ even though (XS)
+ is
clearly not a submatrix of X+. This expectation formula now implies that for any size s ≥ d, if S of
size s is drawn by volume sampling, then w∗
(S) is an unbiased estimator1
for w∗
, i.e.
E[w∗
(S)] = E[(XS)
+yS] = E[(ISX)
+y] = E[(ISX)
+] y = X+y = w∗
.
The second expectation formula can be viewed as a second moment of the pseudoinverse estimator
(ISX)
+, and it can be used to compute a useful notion of matrix variance with applications in random
matrix theory:
E[(ISX)
+(ISX)
+>] − E[(ISX)
+]E[(ISX)
+]
> =
n − s
s − d + 1
X+X+>.
Regularized volume sampling. We also develop a new regularized variant of volume sampling, which
extends reverse iterative sampling to selecting subsets of size smaller than d, and leads to a useful
extension of the above matrix variance formula. Namely, for any λ ≥ 0, our λ-regularized procedure
for sampling subsets S of size s satisfies
E

(X>
S XS + λI)
−1


n − dλ + 1
s − dλ + 1
(X>X + λI)
−1
,
where dλ
def = tr(X(X>X + λI)
−1X>) ≤ d is a standard notion of statistical dimension. Crucially,
the above bound holds for subset sizes s ≥ dλ, which can be much smaller than the dimension d.
Under the additional assumption that response vector y is generated by a linear transformation
distorted with bounded white noise, the expected bound on (X>
S XS + λI)
−1
leads to strong variance
bounds for the ridge regression estimator. Specifically, we prove that when y = Xwe + ξ, with
ξ having mean zero and bounded variance Var[ξ]  σ
2
I, then if S is sampled according to λregularized volume sampling with λ ≤
σ
2
kwe k
2 , we can obtain the following bound on the mean
squared prediction error (MSPE):
ESEξ

1
n
kX(w∗
λ
(S) − we )k
2

≤
σ
2dλ
s − dλ + 1
,
1. For size s = d volume sampling, the fact that E[w∗
(S)] = w∗
can be found in an early paper (Ben-Tal and Teboulle,
1990). They give a direct proof based on Cramer’s rule.
3  
DEREZINSKI AND ´ WARMUTH
where w∗
λ
(S) = (X>
S XS + λI)
−1X>
S yS is the ridge regression estimator for the subproblem
(XS, yS). Our new lower bounds show that the above upper bound for regularized volume sampling
is essentially optimal with respect to the choice of a subsampling procedure.
Algorithms and experiments. The only known polynomial time algorithm for size s > d volume
sampling was recently proposed by Li et al. (2017) with time complexity O(n
4
s). In this paper
we give two new algorithms using our general framework of reverse iterative sampling: one with
deterministic runtime of O((n−s+d)nd), and a second one that with high probability finishes in
time O(nd2
). Thus both algorithms improve on the state-of-the-art by a factor of at least n
2
and
make volume sampling nearly as efficient as the comparable i.i.d. sampling technique called leverage
score sampling. Our experiments on real datasets confirm the efficiency of our algorithms and show
that for small sample sizes s, volume sampling is more effective than leverage score sampling for the
task of subset selection for linear regression.
1.1 Related Work
Volume sampling is a type of determinantal point process (DPP) (Kulesza and Taskar, 2012). DPP’s
have been given a lot of attention in the literature with many applications to machine learning,
including recommendation systems (Gartrell et al., 2016) and clustering (Kang, 2013). Many exact
and approximate methods for efficiently generating samples from this distribution have been proposed
(Deshpande and Rademacher, 2010; Kulesza and Taskar, 2011), making it a useful tool in the design
of randomized algorithms. Most of those methods focus on sampling s ≤ d elements. In this paper,
we study volume sampling sets of size s ≥ d, which was proposed by Avron and Boutsidis (2013)
and motivated with applications in graph theory, linear regression, matrix approximation and more.
The problem of selecting a subset of the rows of the input matrix for solving a linear regression
task has been extensively studied in statistics literature under the terms optimal design (Fedorov,
1972) and pool-based active learning (Sugiyama and Nakajima, 2009). Various criteria for subset
selection have been proposed, like A-optimality and D-optimality. For example, A-optimality seeks
to minimize tr((X>
S XS)
−1
), which is combinatorially hard to optimize exactly. We show that for
size s ≥ d volume sampling, E[(X>
S XS)
−1
] = n−d+1
s−d+1 (X>X)
−1
, which provides an approximate
randomized solution of the sampled inverse covariance matrix rather than just its trace.
In the field of computational geometry a variant of volume sampling was used to obtain optimal
bounds for low-rank matrix approximation. In this task, the goal is to select a small subset of rows of
a matrix X ∈ R
n×d
(much fewer than the rank of X, which is bounded by d), so that a good low-rank
approximation of X can be constructed from those rows. Deshpande et al. (2006) showed that volume
sampling of size s < d index sets obtains optimal multiplicative bounds for this task and polynomial
time algorithms for size s < d volume sampling were given in Deshpande and Rademacher (2010)
and Guruswami and Sinop (2012). We show in this paper that for linear regression, fewer than rank
many rows do not suffice to obtain multiplicative bounds. This is why we focus on volume sampling
sets of size s ≥ d (recall that, for simplicity, we assume that X is full rank).
Computing approximate solutions to linear regression has been explored in the domain of
numerical linear algebra (see Mahoney (2011) for an overview). Here, multiplicative bounds on the
loss of the approximate solution can be achieved via two approaches. The first approach relies on
sketching the input matrix X and the response vector y by multiplying both by the same suitably
chosen random matrix. Algorithms which use sketching to generate a smaller input matrix for a
given linear regression problem are computationally efficient (Sarlos, 2006; Clarkson and Woodruff,
4
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
2013), but they require all of the responses from the original problem to generate the sketch and are
thus not suitable for the goal of using as few response values as possible. The second approach is
based on subsampling the rows of the input matrix and only asking for the responses of the sampled
rows. The learner optimally solves the sampled subproblem2
and then uses the obtained weight
vector for its prediction on all rows. The selected subproblem is known under the term “b-agnostic
minimal coreset” in (Boutsidis et al., 2013; Drineas et al., 2008) since it is selected without knowing
the response vector (denoted as the vector b). The second approach coincides with the goals of this
paper but the focus here is different in a number of ways. First, we focus on the smallest sample
size for which a multiplicative loss bound is possible: Just d volume sampled rows are sufficient to
achieve a multiplicative bound with a fixed factor, while d − 1 are not sufficient. A second focus
here is the efficiency and the combinatorics of volume sampling. The previous work is mostly based
on i.i.d. sampling using the statistical leverage scores (Drineas et al., 2012). As we show in this
paper, leverage scores are the marginals of volume sampling and any i.i.d. sampling method requires
sample size Ω(d log d) to achieve multiplicative loss bounds for linear regression. On the other hand,
the rows obtained from volume sampling are selected jointly and this makes the chosen subset more
informative and brings the required sample size down to d. Third, we focus on the fact that the
estimators produced from volume sampling are unbiased and therefore can be averaged to get more
accurate estimators. Using our methods, averaging immediately leads to an unbiased estimator with
expected loss 1 +  times the optimum based on sampling d
2/ responses in total. We leave it as an
open problem to construct a 1 +  factor unbiased estimator from sampling only O(d/) responses.
If unbiasedness is not a concern, then such an estimator has recently been found (Chen and Price,
2017).
1.2 Outline of the Paper
In the next section, we define volume sampling as an instance of a more general procedure we call
reverse iterative sampling, and we use this methodology to prove closed form matrix expressions
for the expectation of the pseudoinverse estimator (ISX)
+ and its square (ISX)
+(ISX)
+>, when
S is sampled by volume sampling. Central to volume sampling is the Cauchy-Binet formula for
determinants. As a side, we produce a number of short self-contained proofs for this formula and
show that leverage scores are the marginals of volume sampling. Then in Section 3 we formulate the
problem of solving linear regression from a small number of responses, and state the upper bound for
the expected square loss of the volume sampled least squares estimator (Theorem 8), followed by a
discussion and related lower bounds. In Section 3.3, we prove Theorem 8 and an additional related
matrix expectation formula. We next discuss in Section 3.4 how unbiased estimators can easily
be averaged for improving the expected loss and discuss open problems for constructing unbiased
estimators. A new regularized variant of volume sampling is proposed in Section 4, along with
the statistical guarantees it offers for computing subsampled ridge regression estimators. Next, we
present efficient volume sampling algorithms in Section 5, based on the reverse iterative sampling
paradigm, which are then experimentally evaluated in Section 6. Finally, Section 7 concludes the
paper by suggesting a future research direction.
2. Note that those methods typically require additional rescaling of the subproblem, whereas the techniques proposed in
this paper do not require any rescaling.
5
DEREZINSKI AND ´ WARMUTH
2. Reverse Iterative Sampling
Let n be an integer dimension. For each subset S ⊆ {1..n} of size s we are given a matrix formula
F(S). Our goal is to sample set S of size s using some sampling process and then develop concise
expressions for ES:|S|=s
[F(S)]. Examples of formula classes F(S) will be given below.
{1..n}
S
S−i
P(S−i|S)
size
n
n−1
s
s−1
d
Figure 3: Reverse iterative sampling.
We represent the sampling by a directed acyclic graph
(DAG), with a single root node corresponding to the full
set {1..n}. Starting from the root, we proceed along the
edges of the graph, iteratively removing elements from the
set S (see Figure 3). Concretely, consider a DAG with levels
s = n, n − 1, ..., d. Level s contains
n
s

nodes for sets
S ⊆ {1..n} of size s. Every node S at level s > d has s
directed edges to the nodes S − {i} (also denoted S−i) at the
next lower level. These edges are labeled with a conditional
probability vector P(S−i
|S), where the event S occurs if the
sampling process visits node S as it traces a (directed) path in
the DAG from the root node {1..n} to a node at level d. Such
paths have n − d edges. It is natural to assign probabilities
to shorter paths as well going from any node to a node at a lower level. The probability of such a
path is again the product of its edge probabilities. It also follows that the probability P(S) of visiting
node S (via a path from the root) is the sum of the probabilities of all paths from root to S. Finally,
the probability P({1..n}) of the root node is 1 and more generally, the total probability of all nodes
at each layer is 1.
We associate a formula F(S) with each set node S in the DAG. The following key equality lets
us compute expectations.
Lemma 1 If for all S ⊆ {1..n} of size greater than d we have
F(S) = X
i∈S
P(S−i
|S)F(S−i),
then for any s ∈ {d..n}: ES:|S|=s
[F(S)] = P
S:|S|=s P(S)F(S) = F({1..n}).
Proof Suffices to show that expectations at successive layers s and s − 1 are equal for s > d:
X
S:|S|=s
P(S) F(S) =
X
S:|S|=s
P(S)
X
i∈S
P(S−i
|S) F(S−i) =
X
S:|S|=s
X
i∈S
P(S)P(S−i
|S)F(S−i)
=
X
T:|T|=s−1
X
j /∈T
P(T+j )P(T|T+j )
| {z }
P(T)
F(T).
Note that the r.h.s. of the first line has one summand per edge leaving level s, and the r.h.s. of the
second line has one summand per edge arriving at level s − 1. Now the last equality holds because
the edges leaving level s are exactly those arriving at level s − 1, and the summand for each edge in
both expressions is equivalent.
 
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
2.1 Volume Sampling
Given a tall full rank matrix X ∈ R
n×d
and a sample size s ∈ {d..n}, volume sampling chooses
subset S ⊆ {1..n} of size s with probability proportional to squared volume spanned by the columns
of submatrix3 XS and this squared volume equals det(X>
S XS). The following theorem uses the
above DAG setup to compute the normalization constant for this distribution. Note that all subsets S
of volume 0 will be ignored, since they are unreachable in the proposed sampling procedure.
Theorem 2 Let X ∈ R
n×d
, where d ≤ n and det(X>X) > 0. For any set S of size s > d for
which det(X>
S XS) > 0, define the probability of the edge from S to S−i
for i ∈ S as:
P(S−i
|S)
def
=
det(X>
S−iXS−i
)
(s−d) det(X>
S XS)
=
1−x
>
i
(X>
S XS)
−1xi
s−d
, (reverse iterative volume sampling)
where x
>
i
is the i-th row of X. In this case P(S−i
|S) is a proper probability distribution. If
det(X>
S XS) = 0, then simply set P(S−i
|S) to 1
s
. With these definitions, P
S:|S|=s P(S) = 1 for all
s ∈ {d..n} and the probability of all paths from the root to any subset S of size at least d is
P(S) = det(X>
S XS)

n−d
s−d

det(X>X)
. (volume sampling)
The rewrite of the ratio
det(X>
S−i
XS−i
)
det(X>
S XS)
as 1 − x
>
i
(X>
S XS)
−1xi
is Sylvester’s Theorem for determinants. Incidentally, this is the only property of determinants used in this section.
The theorem also implies a generalization of the Cauchy-Binet formula to size s ≥ d sets:
X
S:|S|=s
det(X>
S XS) = 
n − d
s − d

det(X>X). (1)
When s = d, then the binomial coefficient is 1 and the above becomes the vanilla Cauchy-Binet
formula. The below proof of the theorem thus results in a minimalist proof of this classical formula
as well. The proof uses the reverse iterative sampling (Figure 3) and the fact that all paths from the
root to node S have the same probability. For the sake of completeness we also give a more direct
inductive proof of the above generalized Cauchy-Binet formula in Appendix A.
Proof First, for any node S s.t. s > d and det(X>
S XS) > 0, the probabilities out of S sum to 1:
X
i∈S
P(S−i
|S) = X
i∈S
1 − tr((X>
S XS)
−1xix
>
i
)
s − d
=
s − tr((X>
S XS)
−1X>
S XS)
s − d
=
s − d
s − d
= 1.
It remains to show the formula for the probability P(S) of all paths ending at node S. If
det(X>
S XS) = 0, then one edge on any path from the root to S has probability 0. This edge goes
from a superset of S with positive volume to a superset of S that has volume 0. Since all paths have
probability 0, P(S) = 0 in this case.
Now assume det(X>
S XS) > 0 and consider any path from the root {1..n} to S. There are
(n − s)! such paths all going through sets with positive volume. The fractions of determinants in the
3. For sample size s = d, the rows and columns of XS have the same length and det(X>
S XS) is also the squared volume
spanned by the rows XS.
 
DEREZINSKI AND ´ WARMUTH
probabilities along each path telescope and the additional factors accumulate to the same product. So
the probability of all paths from the root to S is the same and the total probability into S is
(n − s)!
(n − d). . .(s − d + 1)
det(X>
S XS)
det(X>X)
=
1

n−d
s−d

det(X>
S XS)
det(X>X)
.
An immediate consequence of the above sampling procedure is the following composition
property of volume sampling, which states that this distribution is closed under subsampling. We
also give a direct proof to highlight the combinatorics of volume sampling.
Corollary 3 For any X ∈ R
n×d and n ≥ t > s ≥ d, the following hierarchical sampling procedure:
T
t∼ X (size t volume sampling from X),
S
s∼ XT (size s volume sampling from XT )
returns a set S which is distributed according to size s volume sampling from X.
Proof We start with the Law of Total Probability and then use the probability formula for volume
sampling from the above theorem. Here P(T ∩ S) means the probability of all paths going through
node T at level t and ending up at the final node S at level s. If S 6⊆ T, then P(T ∩ S) = 0.
P(S) = X
T: S⊆T
P(T ∩S)
z }| {
P(S | T) P(T)
=
X
T: S⊆T
det(X>
S XS)

t−d
s−d

✘✘✘✘✘✘
det(X>
T XT )
✘✘✘✘✘✘
det(X>
T XT )

n−d
t−d

det(X>X)
=

n − s
t − s

det(X>
S XS)

t−d
s−d

n−d
t−d

det(X>X)
=
det(X>
S XS)

n−d
s−d

det(X>X)
.
Note that for all sets T containing S, the probability P(T ∩ S) is the same, and there are
n−s
t−s

such
sets.
The main competitor of volume sampling is i.i.d. sampling of the rows of X w.r.t. the statistical
leverage scores. For an input matrix X ∈ R
n×d
, the leverage score of the i-th row x
>
i
of X is defined
as
li
def
= x
>
i
(X>X)
−1xi
.
Recall that this quantity appeared in the definition of conditional probability P(S−i
|S) in Theorem
2, where the leverage score was computed w.r.t. the submatrix XS. In fact, there is a more basic
relationship between leverage scores and volume sampling: If set S is sampled according to size
s = d volume sampling, then the leverage score li of row i is the marginal probability P(i ∈ S) of
selecting i-th row into S. A general formula for the marginals of size s volume sampling is given in
the following proposit       
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Proposition 4 Let X ∈ R
n×d be a full rank matrix and s ∈ {d..n}. If S ⊆ {1..n} is sampled
according to size s volume sampling, then for any i ∈ {1..n},
P(i ∈ S) = s − d
n − d
+
n − s
n − d
li
z }| {
x
>
i
(X>X)
−1xi
.
Proof Instead of P(i ∈ S) we will first compute P(i /∈ S):
P(i /∈ S) = X
S:|S|=s,i /∈S
det(X>
S XS)

n−d
s−d

det(X>X)
=
X
S:|S|=s,i /∈S
P
T ⊆S:|T|=d
det(X>
T XT )

n−d
s−d

det(X>X)
=

n−d−1
s−d

det(X>
−iX−i)
z X }| {
T:|T|=d,i /∈T
det((X−i)
>
T
(X−i)T )

n−d
s−d

det(X>X)
=
n − s
n − d

1 − x
>
i
(X>X)
−1xi

,
where we used Cauchy-Binet twice and the fact that every set T : |T| = d, i /∈ T appears in

n−d−1
s−d

sets S : |S| = s, i /∈ S. Now, the marginal probability follows from the fact that
P(i ∈ S) = 1 − P(i /∈ S).
2.2 Expectation Formulas for Volume Sampling
All expectations in the remainder of the paper are w.r.t. volume sampling. We use the short-hand
E[F(S)] for expectation with volume sampling where the size of the sampled set is fixed to s. The
expectation formulas for two choices of F(S) are proven in Theorems 5 and 6. By Lemma 1 it
suffices to show F(S) = P
i∈S P(S−i
|S)F(S−i) for volume sampling. We also present a related
expectation formula (Theorem 7), which is proven later using different techniques.
Recall that XS is the submatrix of rows indexed by S ⊆ {1..n}. We also use a version of X in
which all but the rows of S are zeroed out. This matrix equals ISX where IS is an n-dimensional
diagonal matrix with (IS)ii = 1 if i ∈ S and 0 otherwise (see Figure 2).
Theorem 5 Let X ∈ R
n×d be a tall full rank matrix (i.e. n ≥ d). For s ∈ {d..n}, let S ⊆ {1..n}
be a size s volume sampled set over X. Then
E[(ISX)
+] = X+.
For the special case of s = d, this fact was known in the linear algebra literature (Ben-Tal and
Teboulle, 1990; Ben-Israel, 1992). It was shown there using elementary properties of the determinant
such as Cramer’s rule.4 The proof methodology developed here based on reverse iterative volume
4. Using the composition property of volume sampling (Corollary 3), the s > d case of the theorem can be reduced to
the s = d case. However, in this paper we give a different self-contained proof for Theorem      
DEREZINSKI AND ´ WARMUTH
sampling is very different. We believe that this fundamental formula lies at the core of why volume
sampling is important in many applications. In this work, we focus on its application to linear
regression. However, Avron and Boutsidis (2013) discuss many problems where controlling the
pseudoinverse of a submatrix is essential. For those applications, it is important to establish variance
bounds for the above expectation and volume sampling once again offers very concrete guarantees.
We obtain them by showing the following formula, which can be viewed as a second moment for this
estimator.
Theorem 6 Let X ∈ R
n×d be a full rank matrix and s ∈ {d..n}. If size s volume sampling over X
has full support, then
E[ (X>
S XS)
−1
| {z }
(ISX)+(ISX)+>
] = n − d + 1
s − d + 1
(X>X)
−1
| {z }
X+X+>
.
In the case when volume sampling does not have full support, then the matrix equality “=” above is
replaced by the positive-definite inequality “”.
The condition that size s volume sampling over X has full support is equivalent to det(X>
S XS) > 0
for all S ⊆ {1..n} of size s. Note that if size s volume sampling has full support, then size t > s
also has full support. So full support for the smallest size d (often phrased as X being in general
position) implies that volume sampling w.r.t. any size s ≥ d has full support.
The above theorem immediately gives an expectation formula for the Frobenius norm k(ISX)
+kF
of the estimator:
E

k(ISX)
+k
2
F

= E[tr((ISX)
+(ISX)
+>)] = n − d + 1
s − d + 1
kX+k
2
F
. (2)
This norm formula was shown by Avron and Boutsidis (2013), with numerous applications. Theorem
6 can be viewed as a much stronger pre-trace version of the known norm formula. Also our proof
techniques are quite different and much simpler. Note that if size s volume sampling for X does not
have full support, then (2) becomes an inequality.
We now mention a second application of the above theorem in the context of linear regression for
the case when the response vector y is modeled as a noisy linear transformation, i.e., y = Xwe + ξ
for some we ∈ R
d
and a random noise vector ξ ∈ R
n
(detailed discussion in Section 4). In this
case the matrix (X>
S XS)
−1
can be interpreted as the covariance matrix of least-squares estimator
w∗
(S) (for a fixed set S) and Theorem 6 gives an exact formula for the covariance matrix of w∗
(S)
under volume sampling. In Section 4, we give an extended version of this result which provides even
stronger guarantees for regularized least-squares estimators under this model (Theorem 16).
Note that except for the above application, all results in this paper hold for arbitrary response
vectors y. By combining Theorems 5 and 6, we can also obtain a covariance-type formula5
for the
pseudoinverse matrix estimator:
E[((ISX)
+ − E[(ISX)
+]) ((ISX)
+ − E[(ISX)
+])>]
= E[(ISX)
+(ISX)
+>] − E[(ISX)
+] E[(ISX)
+]
>
=
n − d + 1
s − d + 1
X+X+> − X+X+> =
n − s
s − d + 1
X+X+>. (3)
5. This notion of “covariance” is used in random matrix theory, i.e. for a random matrix M, we analyze E[(M −
E[M])(M − E[M])>]. See for example Tropp (2012).
10  
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
We now give the background for a third matrix expectation formula for volume sampling.
Pseudoinverses can be used to compute the projection matrix onto the span of columns of matrix X,
which is defined as follows:
PX
def
= X
X+
z }| {
(X>X)
−1X> .
Applying Theorem 5 leads us immediately to the following unbiased matrix estimator for the
projection matrix:
E[X(ISX)
+] = X E[(ISX)
+] = XX+ = PX.
Note that this matrix estimator X(ISX)
+ is closely connected to linear regression: It can be used to
transform the response vector y into the prediction vector yb(S) of subsampled least squares solution
w∗
(S) as follows:
yb(S) = X (ISX)
+y
| {z }
w∗(S)
.
In this case, volume sampling once again provides a covariance-type matrix expectation formula.
Theorem 7 Let X ∈ R
n×d be a full rank matrix. If matrix X is in general position and S ⊆ {1..n}
is sampled according to size d volume sampling, then
E[ (X(ISX)
+)
2
| {z }
(ISX)+>X>X(ISX)+
] − PX = d (I − PX).
If X is not in general position, then the matrix equality “=” is replaced by the positive-definite
inequality “”.
Note that this third expectation formula is limited to sample size s = d. It is a direct consequence of
Theorem 8 given in the next section which relates the expected loss of a subsampled least squares
estimator to the loss of the optimum least squares estimator. Unlike the first two formulas given in
theorems 5 and 6, its proof does not rely on the methodology of Lemma 1, i.e., on showing that the
expectations at all levels of a certain DAG associated with the sampling process are the same. We
defer the proof of this third expectation formula to the end of Section 3.3. No extension of this third
formula to sample size s > d is known.
Proof of Theorem 5 We apply Lemma 1 with F(S) = (ISX)
+. It suffices to show F(S) =
P
i∈S P(S−i
|S)F(S−i) for P(S−i
|S) = 1−x>
i
(X>
S XS)−1xi
s−d
, i.e.:
(ISX)
+ =
X
i∈S
1 − x
>
i
(X>
S XS)
−1xi
s − d
(IS−iX)
+
| {z }
(X>
S−i
XS−i
)−1(IS−iX)>
.
We first apply Sherman-Morrison to (X>
S−iXS−i
)
−1 = (X>
S XS − xix
>
i
)
−1 on the r.h.s. of the
above:
X
i
1 − x
>
i
(X>
S XS)
−1xi
s − d

(X>
S XS)
−1 +
(X>
S XS)
−1xix
>
i
(X>
S XS)
−1
1 − x
>
i
(X>
S XS)−1xi

((ISX)
> − xie
>
i
).
11
DEREZINSKI AND ´ WARMUTH
Next we expand the last two factors into 4 terms. The expectation of the first (X>
S XS)
−1
(ISX)
> is
(ISX)
+ (which is the l.h.s.) and the expectations of the remaining three terms times s − d sum to 0:
−
X
i∈S
(1 − x
>
i
(X>
S XS)
−1xi) (X>
S XS)
−1xie
>
i +✘✘✘✘✘ (X>
S XS)
−1
✚
✚
✚
X ✚✚
i∈S
xix
>
i
(X>
S XS)
−1
(ISX)
>
−
X
i∈S
(X>
S XS)
−1xi (x
>
i
(X>
S XS)
−1xi)e
>
i = 0.
In Appendix B we give an alternate proof using a derivative argument.
Proof of Theorem 6 Choose F(S) = s−d+1
n−d+1 (X>
S XS)
−1
P
. By Lemma 1 it suffices to show F(S) =
i∈S P(S−i
|S)F(S−i) for volume sampling:
s − d + 1
✭✭✭✭✭ n − d + 1
(X>
S XS)
−1 =
X
i∈S
1 − x
>
i
(X>
S XS)
−1xi
✘s −✘✘d
✘s −✘✘d
✭✭✭✭✭ n − d + 1
(X>
S−iXS−i
)
−1
.
To show this we apply Sherman-Morrison to (X>
S−iXS−i
)
−1 on the r.h.s.:
X
i∈S
(1 − x
>
i
(X>
S XS)
−1xi)

(X>
S XS)
−1 +
(X>
S XS)
−1xix
>
i
(X>
S XS)
−1
1 − x
>
i
(X>
S XS)−1xi

= (s − d)(X>
S XS)
−1 +✘✘✘✘✘ (X>
S XS)
−1
✚
✚
✚
X ✚✚
i∈S
xix
>
i
(X>
S XS)
−1 = (s − d + 1) (X>
S XS)
−1
.
If some denominators 1 − x
>
i
(X>
S XS)
−1xi are zero, then we only sum over i for which the denominators are positive. In this case the above matrix equality becomes a positive-definite inequality .
3. Linear Regression with Smallest Number of Responses
Our main motivation for studying volume sampling came from asking the following simple question.
Suppose we want to solve a d-dimensional linear regression problem with an input matrix X of n
rows in R
d
and a response vector y ∈ R
n
, i.e. find w ∈ R
d
that minimizes the least squares loss
kXw − yk
2 on all n rows. We use L(w) to denote this loss. The optimal weight vector minimizes
L(w), i.e.
w∗ def = argmin
w∈Rd
L(w) = X+y.
Computing it requires access to the input matrix X and the response vector y. Assume we are
given X but the access to response vector y is restricted. We are allowed to pick a random subset
S ⊆ {1..n} of fixed size s for which the responses yS for the submatrix XS are revealed to us, and
then must produce a weight vector w(X, S, yS) ∈ R
d
from a subset of row indices S of the input
matrix X and the corresponding responses yS. Our goal in this paper is to find a distribution on the
subsets S of size s and a weight function w(X, S, yS) s.t.6
∀ (X, y) ∈ R
n×d × R
n×1
: E [L(w(X, S, yS))] ≤ (1 + c)L(w∗
),
6. Since the learner is given X, it is natural to define the optimal multiplicative constant specialized for each X:
cX,s = minc minP (·),w(·) maxy EP [L(w(X, S, yS))] ≤ (1 + c)L(w∗
), where the domain for distribution P(·)
and weight function w(·) are sets of size s. Showing specialized bounds for cX,s is left for future research.
12
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
where c must be a fixed constant (that is independent of X and y). Throughout the paper we use
the one argument shorthand w(S) for the weight function w(X, S, yS). We assume that attaining
response values is expensive and ask the question: What is the smallest number of responses (i.e.
smallest size of S) for which such a multiplicative bound is possible? We will use volume sampling
to show that attaining d response values is sufficient and show that less than d responses is not.
L(·)
L(w∗
)
E[L(w∗
(S))]
L(w∗
(Si))
L(w∗
(Sj ))
w∗
(Si) w∗ w (Sj )
∗ = E[w∗
(S)]
d L(w∗
)
Figure 4: Unbiased estimator w∗
(S) in expectation suffers loss (d + 1)L(w∗
).
Before we state our main upper bound based on
volume sampling, we make the following key observation: If for the subproblem (XS, yS) there is a
weight vector w(S) that has loss zero, then the algorithm has to predict with such a consistent weight
vector. This is because in that case the responses yS
can be extended to a response vector y for all of X s.t.
L(w∗
) = 0. Thus since we aim for a multiplicative
loss bound, we force the algorithm to predict with the
optimum solution w∗
(S)
def = (XS)
+yS whenever the
subproblem (XS, yS) has loss 0. In particular, when
|S| = d and XS has full rank, then there is a unique
consistent solution w∗
(S) for the subproblem and the
learner must use the weight function w(S) = w∗
(S).
Theorem 8 If the input matrix X ∈ R
n×d
is in general position, then for any response vector
y ∈ R
n
, the expected square loss (on all n rows of X) of the optimal solution w∗
(S) for the
subproblem (XS, yS), with the d-element set S obtained from volume sampling, is given by
E[L(w∗
(S))] = (d + 1) L(w∗
).
If X is not in general position, then the expected loss is upper-bounded by (d + 1) L(w∗
).
There are no range restrictions on the n points and response values in this bound. Also, as discussed
in the introduction, this bound is already non-obvious for dimension 1, when the multiplicative factor
is 2 (See Figure 1 for a visualization). Note that if there is a bias term in dimension 1, then the factor
becomes 3.
In dimension d, it is instructive to look at the case when the square loss of the optimum solution
is zero, i.e. there is a weight vector w∗ ∈ R
d
s.t. Xw∗ = y. In this case the response values of any d
linearly independent rows of X determine the optimum solution and the multiplicative loss formula
of the theorem clearly holds. The formula specifies how noise-free case generalizes gracefully to
the noisy case in that for volume sampling, the expected square loss of the solution obtained from d
row response pairs is always by a factor of at most d + 1 larger than the square loss of the optimum
solution. Moreover, since E[w∗
(S)] = w∗
and the loss function L(·) is convex, we have by Jensen’s
inequality that
E

L(w∗
(S))

≥ L

E[w∗
(S)]

= L(w∗
).
The above theorem now states that the gap E[L(w∗
(S))] − L(w∗
) in Jensen’s inequality (which
coincides with the “regret” of the estimator) equals d L(w∗
), when the expectation is w.r.t. size d
volume sampling and X is in general position (See Figure 4 for a schematic). As we will show in
Section 3.4, this gap also equals the variance E[kXw∗
(S) − Xw∗k
2
] of the predictions since the
estimator is unbiased. In summary:
1   
DEREZINSKI AND ´ WARMUTH
E

L(w∗
(S))

− L(
E[w∗(S)]
z}|{
w∗
)
| {z }
regret
= d L(w∗
)
| {z }
gap in Jensen’s
= E

kXw∗
(S) − Xw∗
k
2

| {z }
variance
.
We now make a number of observations and present some lower bounds that highlight the upper
bound of the above theorem. Then, in Section 3.3 we prove the theorem and a matrix expectation
formula implied by it.
3.1 When X is not in General Position
The above theorem gives an equality for the expected loss of a volume-sampled solution. However,
this equality is only guaranteed to hold when matrix X is in general position. We give a minimal
example problem where the matrix X is not in general position and the equality of Theorem 8 turns
into a strict inequality. This shows that for the equality, the general position assumption is necessary.
If we apply even an infinitesimal additive perturbation to the matrix X of the example problem,
then the resulting matrix X is in general position and the equality holds. Note that even though the
optimum loss L(w∗
) does not change significantly under such a perturbation, the expected sampling
loss E[L(w∗
(S))] has to jump sufficiently to close the gap in the inequality. In our minimal example
problem, n = 3 and d = 2, and
X =


1 1
1 1
1 0

 , y =


1
0
0

 .
We have three 2-element subsets to sample from: S1 = {1, 2}, S2 = {2, 3}, S3 = {1, 3}. Notice
that the first two rows of X are identical, which means that the probability of sampling set S1 is
0 in the volume sampling process. The other two subsets, S2 and S3, form identical submatrices
XS2 = XS3
. Therefore they are equally probable. The optimal weight vectors for these sets are
w∗
(S2) = (0, 0)> and w∗
(S3) = (0, 1)>. Also w∗ = (0,
1
2
)
> and the expected loss is bounded as:
E[L(w∗
(S))] = 1
2
1
z }| {
L(w∗
(S2)) +1
2
1
z }| {
L(w∗
(S3))
| {z }
1
<
3
z }| {
(d + 1)
1/2
z }| {
L(w∗
)
| {z }
3/2
.
Now consider a slightly perturbed input matrix
X =


1 1 + 
1 1
1 0

 ,
where  > 0 is arbitrarily small (We keep the response vector y the same). Now, there is no d × d
submatrix that is singular, so the upper bound from Theorem 8 must be tight. The reason is that
even though subset S1 still has very small probability, its loss is very large, so the expectation
is significantly affected by this component, no matter how small  is. We see this directly in the
calculations. Let w∗
and w∗
(Si) be the corresponding solutions for the perturbed problem and its
subproblems. The volumes of the subproblems and their losses are:
det(X>
S1XS1
) = 
2 L(w∗
(S1)) = 
−2
det(X>
S2XS2
) = 1 L(w∗
(S2)) = 1
det(X>
S3XS3
) = (1 + )
2 L(w∗
(S3)) = (1 + )
−2
L(w∗
) = 1
2(1 +  + 
2)
.
14    
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Note that for each subproblem, the product of volume times loss is equal to 1. Now the expected loss
can be easily computed, and we can see that the gap in the bound disappears (the denominator is the
normalizing constant for volume sampling):
E[L(w∗
(S))] = 1 + 1 + 1

2 + 1 + (1 + )
2
= (d + 1) L(w∗
).
3.2 Lower Bounds and the Importance of Joint Sampling
The factor d + 1 in Theorem 8 cannot, in general, be improved when selecting only d responses:
Proposition 9 For any d, there exists a least squares problem (X, y) with d + 1 rows in R
d
such
that for every d-element index set S ⊆ {1 .. d +1}, we have
L(w∗
(S)) = (d + 1) L(w∗
).
Proof Choose the input vectors xi (and rows x
>
i
) as the d + 1 corners of any simplex in R
d
centered
at the origin and choose all d + 1 responses as the same non-zero value α. For any α, the optimal
solution w∗ will be the all-zeros vector with loss
L(w∗
) = (d + 1) α
2
.
On the other hand, taking any size d subset of indices S ⊆ {1 .. d +1}, the subproblem solution
w∗
(S) will only produce loss on the left out input vector xi
, indexed with i 6∈ S. To obtain the
prediction on xi
, we use a simple geometric argument. Observe that since the simplex is centered,
we can write the origin of R
d
in terms of the corners of the simplex as
0 =
X
k
xk = xi + d x¯−i
, where x¯−i
def
=
1
d
X
k6=i
xk.
Thus, the left out input vector xi equals −d x¯−i
. The prediction of w∗
(S) on this vector is
ybi = x
>
i w∗
(S) = −d

1
d
X
k6=i
x
>
k

w∗
(S) = −
X
k6=i
x
>
k w∗
(S) = −dα.
It follows that the loss of w∗
(S) equals
L(w∗
(S)) = (ybi − yi)
2 = (−dα − α)
2 = (d + 1)2α
2 = (d + 1)L(w∗
).
Moreover, it is easy to show that no deterministic algorithm for selecting d rows (without knowing
the responses) can guarantee a multiplicative loss bound with a factor less than n/d (Boutsidis et al.,
2013). For the sake of completeness, we show this here for d = 1:
Proposition 10 For any n×1 input matrix X of all 1’s and any deterministic algorithm that chooses
some singleton set S = {i}, there is a response vector y for which the loss of the subproblem and
the optimal loss are related as follows:
L(w∗
(S)) = n L(w∗
).
15
DEREZINSKI AND ´ WARMUTH
Proof If the response vector y is the vector of n 1’s except for a single 0 at index i, then we have
L(
0
z }| {
w∗
({i})
| {z }
n−1
) = n L(
n−1
n
z}|{
w∗
)
| {z }
n−1
n
.
Note that for the 1-dimensional example used in the proof, volume sampling would pick the
set S uniformly. For this distribution, the multiplicative factor drops from n down to 2, that is
E[L(w∗
(S))] = 1
n
(n − 1) + n−1
n
1 = 2 L(w∗
).
The importance of joint sampling. Three properties of volume sampling play a crucial role in
achieving a multiplicative loss bound:
a) Randomness. No deterministic algorithm guarantees such a bound (see Proposition 10).
b) The chosen submatrices must have full rank. Choosing any rank deficient submatrix with
positive probability, does not allow for a multiplicative bound (see Propositions 11 and 12).
c) Jointness. No i.i.d. sampling procedure can achieve a multiplicative loss bound with O(d)
responses (see Corollary 13).
By jointly selecting subset S, volume sampling ensures that the corresponding input vectors xi
are well spread out in the input space R
d
. In particular, volume sampling does not put any probability
mass on sets S such that the rank of submatrix XS is less than d. Intuitively, selecting rank deficient
row subsets should not be effective, since such a choice leads to an under-determined least squares
problem. We make this simple statement more precise by showing that any randomized algorithm,
that with positive probability selects a rank deficient row subset, cannot achieve a multiplicative loss
bound. Intuitively if the algorithm picks a rank deficient subset then it is not clear how it should select
the weight vector w(S) given input matrix X, subset S and responses yS. We reasoned before that
w(S) must have loss 0 on the subproblem (XS, yS). However if rank(XS) < d, then the choice
of the weight vector w(S) with loss 0 is not unique and this causes positive loss for some response
vector y.
Proposition 11 If for any input matrix X, the algorithm samples a rank deficient subset S of rows
with positive probability, then the expected loss of the algorithm cannot be bounded by a constant
times the optimum loss for all response vectors y.
Note that this means in particular that if X has rank d, then sampling d − 1 size subsets with positive
probability does not allow for a constant factor approximation.
Proof Let S be a rank deficient subset chosen with probability P(S) > 0. Since in our setup the
bound has to hold for all response vectors y we can imagine an adversary choosing a worst-case
y. This adversary gives all rows of XS the response value zero. Let w(S) be the plane produced
by the algorithm when choosing S and receiving the responses 0 for XS. Let i ∈ {1..n} s.t.
x
>
i
6∈ row-span(XS) and let w∗ be any weight vector that gives response value 0 to all rows of XS
and response value x
>
i w(S) + Y to xi
. The adversary chooses y as Xw∗
, i.e. it gives all points xj
not indexed by S and different from xi
the response values x
>
j w∗
as well. Now w∗ has total loss 0
but w(S) has loss Y
2 on xi and the algorithm’s expected total loss is ≥ P(S) Y
2
.
16
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
We now strengthen the above proposition in that whenever the sample S is rank deficient then
the loss of the optimum is zero while the loss of the algorithm is positive. However note that this
proposition is weaker than the above in that it only holds for specific input matrices.
Proposition 12 Let d ≤ n and let X be any input matrix of rank d consisting of n standard basis
row vectors in R
d
. Then for any randomized learning algorithm that with probability p selects a
subset S s.t. rank(XS) < d and any weight function w(·), there is a response vector y, satisfying:
L(w∗
) = 0, and L(w(S)) > 0 with probability at least p.
Proof Let Q = {1, 2, . . . , 2
n}. The adversarial response vector y is constructed by carefully
selecting one of the weight vectors w∗ ∈ Qd
, and setting the response vector y to Xw∗
. This ensures
that L(w∗
) = 0 and since X consists of standard basis row vectors, the components of y lie in Q as
well. Note that if the learner does not discover w∗
exactly, it will incur positive loss. Let H be the
set of all rank deficient sets in X, i.e. those that lack at least one of the standard basis vectors:
H = {S ⊆ {1..n} : rank(XS) < d}.
Suppose that given matrix X, the learner uses weight function w(S, yS). (Note that for the sake of
concreteness we stopped using the single argument shorthand for the weight function during this
proof.) We will count the number of possible inputs to this function, when S is a rank deficient index
set of the rows of X and the response vector yS is consistent with some w∗ ∈ Qd
. For any fixed
rank deficient set S, let t be the number of distinct basis vectors appearing in XS. Clearly t ≤ d − 1.
Fix a subset T ⊆ S of size t s.t. XT contains all t basis vectors of XS exactly once (Thus the basis
vectors in XS\T are all duplicates). Since y ∈ Qn
, the components of yS also lie in Q and yS is
determined by the responses of yT . Clearly there are at most |Q|
d−1
choices for yT . It follows that
the number of possible input pairs (S, yS) for function w(·, ·) under the above restrictions can be
bounded as



n
(S, yS) : [S ∈ H] and [yS = XSw∗
for w∗ ∈ Q
d
]
o

 ≤ |H|
|{z}
<2n
max
S∈H
|{XSw∗
: w∗ ∈ Q
d
}|
| {z }
≤|Q|
d−1
< 2
n
|Q|
d−1 = |Q
d
|.
So for every weight function w(·, ·), there exists w∗ ∈ Qd
that is not present in the set {w(S, yS) :
S ∈ H}. Selecting y = Xw∗
for the adversarial response vector, we guarantee that the learner picks
the wrong solution for every rank deficient set S and therefore receives positive loss w.p. at least p.
Using Proposition 12, we show that any i.i.d. row sampling distribution (like for example leverage
score sampling) requires Ω(d log d) samples to get any multiplicative loss bound, either with high
probability or in expectation.
Corollary 13 Let d ≤ n and let X be any input matrix of rank d consisting of n standard basis
row vectors in R
d
. Then for any randomized learning algorithm which selects a random multiset
S ⊆ {1..n} of size |S| ≤ (d − 1) ln(d) via i.i.d. sampling from any distribution and uses any weight
function w(S), there is a response vector y satisfying:
L(w∗
) = 0, and L(w(S)) > 0 with probability at least 1/2.
17
DEREZINSKI AND ´ WARMUTH
Proof Any i.i.d. sample of size at most (d − 1) ln(d) with probability at least 1/2 does not contain
all of the unique standard basis vectors (Coupon Collector Problem7
). Thus, with probability at least
1/2 submatrix XS has rank less than d. Now, for any such algorithm we can use Proposition 12
to select a consistent adversarial response vector y such that with probability at least 1/2 the loss
L(w(S)) is positive.
Note that the corollary requires X to be of a restricted form that contains a lot of duplicate rows. It is
open whether this corollary still holds when X is an arbitrary full rank matrix.
3.3 Proof of the Loss Expectation Formula
First, we discuss several key connections between linear regression and volume, which are used
in the proof. Note that the loss L(w∗
) suffered by the optimum weight vector can be written as
kyb − yk
2
, the squared Euclidean distance between prediction vector yb = Xw∗
and the response
vector y. Since yb is minimizing the distance from y to the subspace of R
n
spanning the feature
vectors {f1, . . . ,fd} (columns of X), it has to be the projection of y onto that subspace (see Figure
5). We denote this projection as PX y, as defined in Section 2.2. Note that PX is a linear mapping
from R
n onto the column span of the matrix X such that
for u ∈ span(X) u = PX y ⇔ PX (u − y) = 0 ⇔ X>(u − y) = 0. (4)
We next give a second geometric interpretation of the length kyb−yk
2
. Let P be the parallelepiped
formed by the d column/feature vectors of the input matrix X. Furthermore, consider the extended
input matrix produced by adding the response vector y to X as an extra column:
Xe
def = (X, y) ∈ R
n×(d+1)
. (5)
y
y
f1
f2
L(w*)
Figure 5: Prediction vector yb is a projection of y
onto the span of feature vectors fi
.
Using the “base × height” formula we can relate the
volume of P to the volume of Pe, the parallelepiped
formed by the d + 1 columns of Xe . Observe that Pe
has P as one of its faces, with the response vector y
representing the edge that protrudes from that face.
Hence the volume of Pe is the product of the volume
of P and the distance between y and span(X). This
distance equals kyb − yk, since as discussed above, yb
is the projection of y onto span(X). Thus we have
det(Xe >Xe ) = det(X>X)L(w∗
). (6)
Next, we present a proposition whose corollary is key to proving Theorem 8. Suppose that we
select one test row from the input matrix and use the remaining n − 1 row response pairs as the
training set. The proposition relates the loss of the obtained solution on the test row to the total
leave-one-out loss an all rows.
Proposition 14 For any index i ∈ {1..n}, let w∗
(−i) be the solution to the reduced linear regression
problem (X−i
, y−i). Then
7. This was proven for uniform sampling in Theorem 1.24 of Auger and Doerr (2011). It can be shown that uniform
sampling is the best case for Coupon Collector Problem (Holst, 2001), so the bound holds for any i.i.d. sampling.
18
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
L(w∗
(−i)) − L(w∗
) =
det(X>X)−det(X>
−iX−i
)
det(X>X)
z }| {
x
>
i
(X>X)
−1xi `i(w∗
(−i)),
where `i(w)
def = (x
>
i w − yi)
2
is the square loss of w on the i-th point.
An algebraic proof of this proposition essentially appears in the proof of Theorem 11.7 in CesaBianchi and Lugosi (2006). For the sake of completeness we give a new geometric proof of this
proposition in Appendix C using basic properties of volume, thus stressing the connection to volume
sampling.
Note that if matrix X has exactly n = d + 1 rows and the training matrix X−i
is full rank, then
w∗
(−i) has loss zero on all training rows. In this case we obtain a simpler relationship than the
proposition.
Corollary 15 If X has d + 1 rows and rank(X−i) = d, then defining Xe as in (5), we have
det(Xe >Xe ) = det(X>
−iX−i) `i(w∗
(−i)).
Proof By Proposition 14 and the fact that L(w∗
(−i)) = `i(w∗
(−i)), we have
det(X>X) L(w∗
) = det(X>
−iX−i) `i(w∗
(−i)).
The corollary now follows from the “base × height” formula for volume.
We are now ready to present the proof of Theorem 8. Recall that our goal is to find the expected
loss E[L(w∗
(S))], where S is a size d volume sampled set.
Proof of Theorem 8 First, we rewrite the expectation as follows:
E[L(w∗
(S))] = X
S,|S|=d
P(S)L(w∗
(S)) = X
S,|S|=d
P(S)
Xn
j=1
`j (w∗
(S))
=
X
S,|S|=d
X
j /∈S
P(S) `j (w∗
(S)) = X
T,|T|=d+1
X
j∈T
P(T−j ) `j (w∗
(T−j )). (7)
We now use Corollary 15 on the matrix XT and test row x
>
j
(assuming rank(XT−j
) = d):
P(T−j ) `j (w∗
(T−j )) =
det(X>
T−j
XT−j
)
det(X>X)
`j (w∗
(T−j )) = det(Xe >
T Xe T )
det(X>X)
. (8)
Since the summand does not depend on the index j ∈ T, the inner summation in (7) becomes a
multiplication by d + 1. This lets us write the expected loss as:
E[L(w∗
(S))] = d + 1
det(X>X)
X
T,|T|=d+1
det(Xe >
T Xe T )
(1) = (d + 1)det(Xe >Xe )
det(X>X)
(2) = (d + 1)L(w∗
), (9)
where (1) follows from the Cauchy-Binet formula and (2) is an application of the “base × height”
formula. If X is not in general position, then for some summands in (8), rank(XT−j
) < d and
19
DEREZINSKI AND ´ WARMUTH
P(T−j ) = 0. Thus the left-hand side of (8) is 0, while the right-hand side is non-negative, so (9)
becomes an inequality, completing the proof of Theorem 8.
Lifting expectations to matrix form. We can now show the matrix expectation formula of Theorem
7 as a corollary to the loss expectation formula of Theorem 8. The key observation is that the loss
formula holds for arbitrary response vector y, which allows us to “lift” it to the matrix form.
Proof of Theorem 7 Note, that the loss of least squares estimator can be written in terms of the
projection matrix PX:
L(w∗
) = ky − ybk
2 = k(I − PX)yk
2 = y
>(I − PX)
2y
(∗)
= y
>(I − PX) y,
where in (∗) we used the following property of a projection matrix: P2
X = PX. Writing the loss
expectation of the subsampled estimator in the same form, we obtain:
E[L(w∗
(S))] = E[ky − yb(S)k
2
] = E[k(I − X(ISX)
+)yk
2
]
= E[y
>(I − X(ISX)
+)
2 y] = y
>E[(I − X(ISX)
+)
2
] y.
Crucially, we are able to extract the response vector y out of the expectation formula, which allows
us to write the formula from Theorem 8 as follows:
y
> E[(I − X(ISX)
+)
2
] y = y
>(d + 1)(I − PX) y, ∀ y ∈ R
n
.
We now use the following elementary fact: If for two symmetric matrices A and B, we have
y
>Ay = y
>By, ∀y ∈ R
n
, then A = B.
8 This gives the matrix expectation formula:
E[(I − X(ISX)
+)
2
] = (d + 1)(I − PX).
Expanding square on the l.h.s. of the above and applying Theorem 5, we obtain the covariance-type
equivalent form stated in Theorem 7:
I − 2
PX
z }| {
E[X(ISX)
+] +E[(X(ISX)
+)
2
] = (d + 1)(I − PX)
⇐⇒ E[(X(ISX)
+)
2
] − PX = d (I − PX).
3.4 Averaging Unbiased Estimators and the Open Problem for Worst-case Responses
As discussed at the beginning of Section 3, our goal is to find a way to sample a small index set S and
construct a weight function w(S) which uses responses yS so that E[L(w(S))] ≤ (1 + c) L(w∗
),
where the multiplicative factor 1 + c is bounded for all input matrices X and all response vectors
y. Recall that L(·) denotes the square loss on all rows and w∗
is the optimal solution based on all
responses. We show in the previous subsections that the smallest size of S for which this goal can be
achieved is d (There is no sampling procedure for sets of size less than d and weight function w(S)
for which this factor is finite). We also prove that when sets S of size d are drawn proportional to
8. Similarly, if y
>Ay ≤ y
>By, ∀y ∈ R
n
, then the positive-definite inequality A  B holds for the matrices.
20
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
the squared volume of XS (i.e. det(X>
S XS)), then E[L(w∗
(S))] ≤ (d + 1)L(w∗
), where the factor
d + 1 is optimal for some X and y. Here w∗
(S) denotes the linear least squares solution for the
subproblem (XS, yS).
A natural more general goal is to get arbitrarily close to the optimum loss. That is, for any ,
what is the smallest sample size |S| = s for which there is a sampling distribution over subsets
S and a weight function w(S) built from X and yS, such that E[L(w(S))] ≤ (1 + )L(w∗
). A
related bound for i.i.d. leverage score sampling states that a sample size of O(d log d +
d

) suffices to
achieve a 1 +  factor with high probability (Hsu, 2017; Derezinski, 2018), however this does not ´
imply multiplicative bounds in expectation.
9
We conjecture that some form of volume sampling can be used to achieve the 1 +  factor
with sample size O(
d

), in expectation. How close can we get with the techniques presented in
this paper? We showed that size d volume sampling achieves a factor of 1 + d, but we do not
know how to generalize this proof to sample size larger than d. However, one unique property of
the volume-sampled estimator w∗
(S) that can be useful here is that it is an unbiased estimator
of w∗
. As we shall see now, this basic property has many benefits. For any unbiased estimator
(i.e. E[w(S)] = w∗
) and optimal prediction vector yb = Xw∗
, consider the following rudimentary
version of a bias-variance decomposition:
E kX w(S) − yk
2
| {z }
L(w(S))
= E kX w(S) − yb + yb − yk
2 = E kX w(S) − ybk
2 + kyb − yk
2
| {z }
L(w∗)
. (10)
The unbiasedness of the estimator assures that the cross term (X
w∗
z }| {
E[w(S)] −yb)
>(yb − y) is 0.
Therefore a 1 + c factor loss bound is equivalent to a c factor variance bound, i.e.
loss bound
z }| {
E[L(w(S))] ≤ (1 + c)L(w∗
) ⇐⇒
variance bound
z }| {
E kX w(S) − ybk
2 ≤ c L(w∗
). (11)
To reduce the variance of any unbiased estimator w(S) (i.e. E[w(S)] = w∗
) with sample size s, we
can draw k independent samples S1, . . . , Sk of size s each and predict with the average estimator
1
k
Pk
j=1 w(Sj ). If the loss bound from (11) holds for w(S), then the average estimator satisfies
E

L
1
k
X
j
w(Sj )


≤

1 +
c
k

L(w∗
).
Setting k = c/, we need t = s c/ responses to get a 1 +  approximation. We showed that size
s = d volume sampling achieves factor c = d. So with our current proof techniques, we need
t = d
2/ responses to get a 1 +  factor approximation, for  ∈ (0, d].
10
9. Also, the weight vectors produced from i.i.d. leverage score sampling are not unbiased.
10. Thus when averaging the estimators of k = t/d independent volume sampled sets of size d,
E

L

1
k
X
j
w
∗
(Sj )

− L(w
∗
)
| {z }
regret
=
d
2L(w∗
)
t
| {z }
prediction variance
, when X is in general position.
21
DEREZINSKI AND ´ WARMUTH
The basic open problem for worst-case responses is the following: Is there a size O(d/)
unbiased estimator that achieves a 1 +  factor approximation?11 By the above averaging method
this is equivalent to the following question: Is there a size O(d) unbiased estimator that achieves a
constant factor? This is because once we have an unbiased estimator that achieves a constant factor,
then by averaging 1/ copies, we get the 1 + O() factor. Ideally the special unbiased estimators
resulting from a version of volume sampling can achieve this feat. We conclude this section with
our favorite open problem: Is there a version of O(d) size volume sampling that achieves a constant
factor approximation?
In the next section we make some minimal statistical assumptions on the response vector which
let us prove much stronger bounds: We assume that the response vector is linear plus bounded noise
of mean zero. In particular we show that with this noise model, O(d) size volume sampling achieves
a constant factor approximation.
4. Regularized Volume Sampling for Learning with Noisy Responses
Algorithm 1 λ-regularized v. sampling
S ← {1..n}
while |S| > s
∀i∈S : hi ←
det(X>
S−i
XS−i+λI)
det(X>
S XS+λI)
Sample i ∝ hi out of S
S ← S − {i}
end
return S
Volume sampling, as defined in Section 2.1, has certain
fundamental limitations. Namely, it is undefined whenever
matrix X is not full rank or if we wish to sample a subset S
of size smaller than the dimension d. Motivated by these
limitations, we propose a regularized variant, called λregularized volume sampling, which we define through a
generalization of the reverse iterative sampling procedure:
P(S−i
| S) ∝
det(X>
S−iXS−i + λI)
det(X>
S XS + λI)
. (12)
The normalization factor of this conditional probability (i.e. the sum of (12) over i ∈ S) can be
computed using Sylvester’s theorem:
X
i∈S
det(X>
S−iXS−i + λI)
det(X>
S XS + λI)
=
X
i∈S

1 − x
>
i
(X>
S XS + λI)
−1xi

= |S| − tr
XS(X>
S XS + λI)
−1X>
S
)
= |S| − d + λ tr
(X>
S XS + λI)
−1

. (13)
Note that in the special case of no regularization (i.e. λ = 0) the last trace vanishes and (13) is equal
to |S| − d, so we recover volume sampling from Section 2.1. However, when λ > 0, then the last
term is non-zero and depends on the entire matrix XS. This makes regularized volume sampling
more complicated and certain equalities proven in previous sections for λ = 0 no longer hold. In
particular, the analogous closed form of the sampling probability P(S) given in Theorem 2 is not
recovered because the paths from node {1..n} to node S in the graph of Figure 3 do not all have the
same probability. However, the proof technique we developed for reverse iterative sampling can still
be applied, resulting in the following extension of the variance formula of Theorem 6:
11. In a recent paper (Chen and Price, 2017) a 1 +  factor approximation has been achieved with O(d/) examples (for
 ∈ (0, 1]), but the guarantee holds with high probability (and not in expectation) and the estimator is not unbiased.   
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Theorem 16 For any X ∈ R
n×d
, λ ≥ 0, let S be sampled according to λ-regularized size s volume
sampling from X. Then,
E

(X>
S XS + λI)
−1


n − dλ + 1
s − dλ + 1
(X>X + λI)
−1
for any s ≥ dλ
def = tr(X(X>X + λI)
−1X>).
Constant dλ is a common notion of statistical dimension often referred to as the effective degrees
of freedom. If λi are the eigenvalues of X>X, then dλ =
Pd
i=1
λi
λi+λ
. Note that dλ is decreasing
with λ and, when X is full rank, d0 = d. Thus, unlike Theorem 6, the above result offers meaningful
bounds for sampling sets S of size smaller than d.
Proof To obtain Theorem 16, we use essentially the same methodology as described in Lemma
1, except in the regularized case equality is replaced with inequality. Recall that using Sylvester’s
theorem we can compute the unnormalized conditional probability from (12) as:
hi =
det(X>
S−iXS−i + λI)
det(X>
S XS + λI)
= 1 − x
>
i
(X>
S XS + λI)
−1xi
.
From now on, we will use Zλ(S) = X>
S XS + λI as a shorthand in the proofs. Next, letting
M =
P
i∈S
hi
, we compute unnormalized expectation by applying the Sherman-Morrison formula:
M E

(X>
S−iXS−i + λI)
−1
| S

=
X
i∈S
hiZλ(S−i)
−1 =
X
i∈S
hi

Zλ(S)
−1 +
Zλ(S)
−1xix
>
i Zλ(S)
−1
1 − x
>
i Zλ(S)
−1xi

= M Zλ(S)
−1 + Zλ(S)
−1
X
i∈S
xix
>
i

Zλ(S)
−1
= M Zλ(S)
−1 + Zλ(S)
−1
(Zλ(S) − λI)Zλ(S)
−1
= M Zλ(S)
−1 + Zλ(S)
−1 − λZλ(S)
−2
 (M + 1) Zλ(S)
−1
.
Finally, the normalization factor M (which we already computed in (13)) can be lower-bounded
using the λ-statistical dimension dλ of matrix X:
M =
X
i∈S
(1 − x
>
i Zλ(S)
−1xi) = s − d + λ tr(Zλ(S)
−1
) ≥ s −

d−λ tr(Zλ({1..n})
−1
)
| {z }
dλ

.
Putting the bounds together, we obtain that:
E

(X>
S−iXS−i + λI)
−1
| S


s − dλ + 1
s − dλ
(X>
S XS + λI)
−1
.
To prove Theorem 16 it remains to chain the conditional expectations along the sequence of subsets
obtained by λ-regularized volume sampling:
E

Zλ(S)
−1


 Yn
t=s+1
t − dλ + 1
t − dλ
!
Zλ({1..n})
−1 =
n − dλ + 1
s − dλ + 1
(X>X + λI)
−1
.
2         
DEREZINSKI AND ´ WARMUTH
4.1 Ridge Regression with Noisy Responses
We apply the above result to obtain statistical guarantees for subsampling with regularized estimators.
Given a matrix X ∈ R
n×d
, we consider the task of fitting a linear model to a vector of responses
y = Xwe + ξ, where we ∈ R
d
and the noise ξ ∈ R
n
is a mean zero random vector with covariance
matrix Var[ξ]  σ
2
I for some σ > 0. A classical solution to this task is the ridge estimator:
w∗
λ = argmin
w∈Rd
kXw − yk
2 + λkwk
2 = (X>X + λI)
−1X>y.
As a consequence of Theorem 16, we show that if S is sampled with λ-regularized volume sampling
from X, then the ridge estimator for the subproblem (XS, yS)
w∗
λ
(S) = (X>
S XS + λI)
−1X>
S yS
has strong generalization properties with respect to the full problem (X, y) in terms of the mean
squared prediction error (MSPE) and mean squared error (MSE).
Theorem 17 Let X ∈ R
n×d and we ∈ R
d
, and suppose that y = Xwe + ξ, where ξ is a mean
zero vector with Var[ξ]  σ
2
I. Let S be sampled according to λ-regularized size s ≥ dλ volume
sampling from X and w∗
λ
(S) be the λ-ridge estimator of we computed from subproblem (XS, yS).
Then, if λ ≤
σ
2
kwe k
2 , we have
(mean squared prediction error) ESEξ
h
1
n
kX(w∗
λ
(S) − we )k
2
i
≤
σ
2dλ
s − dλ + 1
,
(mean squared error) ESEξ

kw∗
λ
(S) − we k
2

≤
σ
2n tr((X>X + λI)
−1
)
s − dλ + 1
.
Next, we present two lower bounds for MSPE of a subsampled ridge estimator which show that
the statistical guarantees achieved by regularized volume sampling are nearly optimal for s  dλ
and better than standard approaches for s = O(dλ). In particular, we show that non-i.i.d. nature
of volume sampling is essential if we want to achieve good generalization when the number of
responses is close to dλ. Namely for certain data matrices, any i.i.d. subsampling procedure (such as
i.i.d. leverage score sampling) requires at least dλ ln(dλ) responses to achieve MSPE below σ
2
. In
contrast volume sampling obtains that bound for any matrix with 2dλ responses.
Theorem 18 For any p ≥ 1 and σ ≥ 0, there is d ≥ p such that for any sufficiently large n divisible
by d, there exists a matrix X ∈ R
n×d
such that
dλ(X) ≥ p for any 0 ≤ λ ≤ σ
2
,
and for each of the following two statements there is a vector we ∈ R
d
for which the corresponding
regression problem y = Xwe + ξ with Var[ξ] = σ
2
I satisfies that statement:
a) For any subset S ⊆ {1..n} of size s,
Eξ
h
1
n
kX(w∗
λ
(S) − we )k
2
i
≥
σ
2dλ
s + dλ
;
24  
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
b) For multiset S ⊆ {1..n} of size s ≤ (dλ−1) ln(dλ), sampled i.i.d. from any distribution over
{1..n},
ESEξ
h
1
n
kX(w∗
λ
(S) − we )k
2
i
≥ σ
2
.
Proof of Theorem 17 Standard analysis for the ridge regression estimator follows by performing
bias-variance decomposition of the error, and then selecting λ so that bias can be appropriately
bounded. We will recall this calculation for a fixed subproblem (XS, yS). First, we compute the bias
of the ridge estimator for a fixed set S (recall the shorthand Zλ(S) = X>
S XS + λI):
Biasξ[w∗
λ
(S)] = E[w∗
λ
(S)] − we = Eξ [Zλ(S)
−1X>
S yS] − we
= Zλ(S)
−1X>
S
(XSwe +✘E ✘✘ ξ[ξS
]) − we
= (Zλ(S)
−1X>
S XS − I)we = −λ Zλ(S)
−1we .
Similarly, the covariance matrix of w∗
λ
(S) is given by:
Varξ[w∗
λ
(S)] = Zλ(S)
−1X>
S Varξ[ξS
]XSZλ(S)
−1
 σ
2Zλ(S)
−1X>
S XSZλ(S)
−1 = σ
2
(Zλ(S)
−1 − λ Zλ(S)
−2
).
Mean squared error of the ridge estimator for a fixed subset S can now be bounded by:
Eξ

kw∗
λ
(S) − we k
2

= tr(Varξ[w∗
λ
(S)]) + kBiasξ[w∗
λ
(S)]k
2
≤ σ
2
tr(Zλ(S)
−1− λZλ(S)
−2
) + λ
2
tr(Zλ(S)
−2we we
>)
≤ σ
2
tr(Zλ(S)
−1
) + λtr(Zλ(S)
−2
)(λkwe k
2− σ
2
) (14)
≤ σ
2
tr(Zλ(S)
−1
), (15)
where in (14) we applied Cauchy-Schwartz inequality for matrix trace, and in (15) we used the
assumption that λ ≤
σ
2
kwe k
2 . Thus, taking expectation over the sampling of set S, we get
ESEξ

kw∗
λ
(S) − we k
2

≤ σ
2ES

tr(Zλ(S)
−1
)

(Theorem 16) ≤ σ
2 n − dλ + 1
s − dλ + 1
tr(Zλ({1..n})
−1
) (16)
≤
σ
2 n tr((X>X + λI)
−1
)
s − dλ + 1
.
Next, we bound the mean squared prediction error. As before, we start with the standard bias-variance
decomposition for fixed set S:
Eξ

kX(w∗
λ
(S)−we )k
2

= tr(Varξ[Xw∗
λ
(S)]) + kX(Eξ[w∗
λ
(S)] − we )k
2
≤ σ
2
tr(X(Zλ(S)
−1−λ Zλ(S)
−2
)X>) + λ
2
tr(Zλ(S)
−1X>XZλ(S)
−1we we
>)
≤ σ
2
tr(XZλ(S)
−1X>) + λ tr(XZλ(S)
−2X>)(λkwe k
2 − σ
2
)
≤ σ
2
tr(XZλ(S)
−1X>).
25        
DEREZINSKI AND ´ WARMUTH
Once again, taking expectation over subset S, we have
ESEξ
h
1
n
kX(w∗
λ
(S) − we )k
2
i
≤
σ
2
n
ES

tr(XZλ(S)
−1X>)

=
σ
2
n
tr(X ES[Zλ(S)
−1
] X>)
(Theorem 16) ≤
σ
2
n
n − dλ + 1
s − dλ + 1
tr(XZλ({1..n})
−1X>) ≤
σ
2dλ
s − dλ + 1
. (17)
The key part of proving both bounds is the application of Theorem 16. For MSE, we only used the
trace version of the inequality (see (16)), however to obtain the bound on MSPE we used the more
general positive semi-definite inequality in (17).
Proof of Theorem 18 Let d = dpe + 1 and n ≥ dσ
2
ed(d − 1) be divisible by d. We define
X
def = [I, ..., I]
> ∈ R
n×d
, we
> def = [aσ, ..., aσ] ∈ R
d
for some a > 0. For any λ ≤ σ
2
, the λ-statistical dimension of X is
dλ = tr(X Zλ({1..n})
−1X>) ≥
dσ
2
ed(d − 1)
dσ
2e(d − 1) + λ
≥
d(d − 1)
d − 1 + 1
≥ p.
Let S ⊆ {1..n} be any set of size s, and for i ∈ {1..d} let si
def
= |{i ∈ S : xi = ei}|. The prediction
variance of estimator w∗
λ
(S) is equal to
tr
Varξ[Xw∗
λ
(S)]

= σ
2
tr(X(Zλ(S)
−1− λZλ(S)
−2
)X>)
=
σ
2n
d
X
d
i=1

1
si + λ
−
λ
(si + λ)
2

=
σ
2n
d
X
d
i=1
si
(si + λ)
2
.
The prediction bias of estimator w∗
λ
(S) is equal to
kX(Eξ[w∗
λ
(S)] − we )k
2 = λ
2we
>Zλ(S)
−1X>XZλ(S)
−1we
=
λ
2a
2σ
2n
d
tr
Zλ(S)
−2

=
λ
2a
2σ
2n
d
X
d
i=1
1
(si + λ)
2
.
Thus, MSPE of estimator w∗
λ
(S) is given by:
Eξ
h
1
n
kX(w∗
λ
(S) − we )k
2
i
=
1
n
tr
Varξ[Xw∗
λ
(S)]

+
1
n
kX(Eξ[w∗
λ
(S)] − we )k
2
=
σ
2
d
X
d
i=1

si
(si + λ)
2
+
a
2λ
2
(si + λ)
2

=
σ
2
d
X
d
i=1
si + a
2λ
2
(si + λ)
2
.
Next, we find the λ that minimizes this expression. Taking the derivative with respect to λ we get:
∂
∂λ
σ
2
d
X
d
i=1
si + a
2λ
2
(si + λ)
2
!
=
σ
2
d
X
d
i=1
2si(λ − a
−2
)
(si + λ)
3
.     
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Thus, since at least one si has to be greater than 0, for any set S the derivative is negative for
λ < a−2
and positive for λ > a−2
, and the unique minimum of MSPE is achieved at λ = a
−2
,
regardless of which subset S is chosen. So, as we are seeking a lower bound, we can focus on the
case of λ = a
−2
.
Proof of part a. Let a = 1. As shown above, we can assume that λ = 1. In this case the formula
simplifies to:
Eξ
h
1
n
kX(w∗
λ
(S) − we )k
2
i
=
σ
2
d
X
d
i=1
si + 1
(si + 1)2
=
σ
2
d
X
d
i=1
1
si + 1
(∗)
≥
σ
2
s
d + 1
=
σ
2d
s + d
≥
σ
2dλ
s + dλ
,
where (∗) follows by applying Jensen’s inequality to convex function φ(x) = 1
x+1 .
Proof of part b. Let a =
√
2d. As shown above, we can assume that λ = 1/(2d). Suppose that
multiset S is sampled i.i.d. from some distribution over set {1..n}. Similarly as in Corollary 13, we
exploit the Coupon Collector’s problem, i.e. that if |S| ≤ (d − 1) ln(d), then with probability at least
1/2 there is i ∈ {1..d} such that si = 0 (i.e., one of the unit vectors ei was never selected). Thus,
MSPE can be lower-bounded as follows:
ESEξ
h
1
n
kX(w∗
λ
(S) − we )k
2
i
≥
1
2
σ
2
d
si + a
2λ
2
(si + λ)
2
=
σ
2
2d
2dλ2
λ2
= σ
2
.
5. Efficient Algorithms for Volume Sampling
In this section we propose algorithms for efficiently performing volume sampling. This addresses
the question posed by Avron and Boutsidis (2013), asking for a polynomial-time algorithm for
the case when the size of set S is s > d. Deshpande and Rademacher (2010) gave an algorithm
for the case when s = d, which was later improved by Guruswami and Sinop (2012), running in
time O(nd3
). Recently, Li et al. (2017) offered an algorithm for arbitrary s, which has complexity
O(n
4
s). We propose two new methods, which use our reverse iterative sampling technique to achieve
faster running times for volume sampling of any size s. Both algorithms apply to the more general
setting of λ-regularized volume sampling (described in Section 4), and produce standard volume
sampling as a special case for λ = 0 and s ≥ d. The first algorithm has a deterministic runtime
of O((n−s+d)nd), whereas the second one is an accelerated version which with high probability
finishes in time O(nd2
). Thus, we obtain a direct improvement over Li et al. (2017) by a factor of
at least n
2
, and in the special case of s = d, by a factor of d over the algorithm of Guruswami and
Sinop (2012).
Our algorithms implement reverse iterative sampling from Theorem 2. We start with the full
index set S = {1..n}. In one step of the algorithm, we remove one row from set S. After removing
q rows, we are left with the index set of size n − q that is distributed according to volume sampling
for row set size n − q, and we proceed until our set S has the desired size s. The primary cost of the
procedure is updating the conditional distribution P(S−i
|S) at every step. It is convenient to store it
using the unnormalized weights defined in (12) which, via Sylvester’s theorem, can be computed as
hi = 1 − x
>
i
(X>
S XS + λI)
−1xi (For the sake of generality we state the methods for λ-regularized
27
DEREZINSKI AND ´ WARMUTH
volume sampling). Doing this naively, we would first compute (X>
S XS + λI)
−1 which takes O(nd2
)
time12. After that for each i, we would multiply this matrix by xi
in time O(d
2
) to get the hi’s. The
overall runtime of this naive method becomes:
n−s
z }| {
# of steps × (
O(nd2
)
z }| {
compute (X>
S XS + λI)
−1 +
≤n
z }| {
# of weights ×
O(d
2
)
z }| {
compute hi ) = O((n − s)nd2
).
Both the computation of matrix inverse and the weights hi can be made more efficient. First, the
matrix (X>
S XS + λI)
−1
can be computed from the one obtained in the previous step by using the
Sherman-Morrison formula. This lets us update it in O(d
2
) time instead of O(nd2
). Furthermore,
we propose two strategies for dealing with the cost of maintaining the weights:
a) Update all hi’s at every step using Sherman-Morrison;
b) Use rejection sampling and only compute the hi’s needed for the rejection trials (This avoids
computing all hi’s, but makes the computation of each needed hi more expensive).
As we can see, there is a trade-off between those strategies. In the following lemma, we will show
that updating the value of hi
, given its value in the previous step only costs O(d) time as opposed
to O(d
2
). However, the number of hi’s that need to be computed for rejection sampling (explained
shortly) can be far smaller.
Lemma 19 For any matrix X ∈ R
n×d
, set S ⊆ {1..n} and two distinct indices i, j ∈ S, we have
1 − x
>
j
(X>
S−iXS−i+ λI)
−1xj = hj − (x
>
j v)
2
,
where hj = 1 − x
>
j
(X>
S XS + λI)
−1xj and v = √
1
hi
(X>
S XS + λI)
−1xi
.
Proof Letting Zλ(S) = X>
S XS + λI, we have
hj − (x
>
j v)
2 = 1 − x
>
j Zλ(S)
−1xj −
(x
>
j Zλ(S)
−1xi)
2
1 − x
>
i Zλ(S)
−1xi
= 1 − x
>
j Zλ(S)
−1xj −
x
>
j Zλ(S)
−1xix
>
i Zλ(S)
−1xj
1 − x
>
i Zλ(S)
−1xi
= 1 − x
>
j

Zλ(S)
−1 +
Zλ(S)
−1xix
>
i Zλ(S)
−1
1 − x
>
i Zλ(S)
−1xi

xj
(∗)
= 1 − x
>
j
(X>
S−iXS−i + λI)
−1xj ,
where (∗) follows from the Sherman-Morrison formula.
Thus the overall time complexity of reverse iterative sampling when using the first strategy goes
down by a factor of d compared to the naive version (except for an initialization cost which stays at
O(nd2
)).
Theorem 20 Algorithm RegVol produces an index set S of rows distributed according to λ-regularized
size s volume sampling over X in time O((n−s+d)nd).
12. We are primarily interested in the case where n ≥ d and we state our time bounds under that assumption. However,
when λ > 0, our techniques can be easily adapted to the case of n < d.
28
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Proof Using Lemma 19 for hi and the Sherman-Morrison formula for Z, the following invariants
hold at the beginning of the while loop:
hi = 1 − x
>
i
(X>
S XS + λI)
−1xi and Z = (X>
S XS + λI)
−1
.
Runtime: Computing the initial Z = (X>X + λI)
−1
takes O(nd2
), as does computing the initial
values of hj ’s. Inside the while loop, updating hj ’s takes O(|S|d) = O(nd) and updating Z takes
O(d
2
). The overall runtime becomes O(nd2 + (n − s)nd) = O((n − s + d)nd).
Algorithm 2 RegVol(X, s, λ)
1: Z ← (X>X + λI)
−1
2: ∀i∈{1..n} hi ← 1 − x
>
i Zxi
3: S ← {1..n}
4: while |S| > s
5: Sample i ∝ hi out of S
6: S ← S − {i}
7: v ← Zxi/
√
hi
8: ∀j∈S hj ← hj − (x
>
j v)
2
9: Z ← Z + vv>
10: end
11: return S
Algorithm 3 FastRegVol(X, s, λ)
1: Z ← (X>X + λI)
−1
2: S ← {1..n}
3: while |S| > max{s, 2d}
4: repeat
5: Sample i uniformly out of S
6: hi ← 1 − x
>
i Zxi
7: Sample A ∼ Bernoulli(hi)
8: until A = 1
9: S ← S − {i}
10: Z ← Z + h
−1
i Zxix
>
i Z
11: end
12: if s < 2d, S ← RegVol(XS, s, λ) end
13: return S
Next we present algorithm FastRegVol, which is based on the rejection sampling strategy. Our
key observation is that updating the full conditional distribution P(S−i
|S) is wasteful, since the
distribution changes very slowly throughout the procedure. Moreover, the unnormalized weights hi
,
which are computed in the process are all bounded by 1. Thus, to sample from the correct distribution
at any given iteration, we can employ rejection sampling as follows:
1: Sample i uniformly from set S,
2: Compute hi
,
3: Accept with probability hi
,
4: Otherwise, draw another sample.
Note that this rejection sampling can be employed locally, within each iteration of the algorithm.
Thus, one rejection does not revert us back to the beginning of the algorithm. Moreover, if the
probability of acceptance is high, then this strategy requires computing only a small number of
weights per iteration of the algorithm, as opposed to updating all of them. This turns out to be
the case for a majority of the steps of the algorithm, except at the very end (for s ≤ 2d), were the
conditional probabilities start changing more drastically. At that point, it becomes more efficient to
use the first algorithm, RegVol.
Theorem 21 For any λ, s ≥ 0, and δ ∈ (0, 1), algorithm FastRegVol samples according to λregularized size s volume sampling, and with probability at least 1 − δ runs in time
O

n + log
n/d
log
1/δ

d
2

.
  
DEREZINSKI AND ´ WARMUTH
Proof We analyze the efficiency of rejection sampling in FastRegVol. Let Rt be a random variable
corresponding to the number of trials needed in the repeat loop from line 4 in FastRegVol at the
point when |S| = t. Note that conditioning on the algorithm’s history, Rt
is distributed according to
geometric distribution Ge(qt) with success probability:
qt =
1
t
X
i∈S

1 − x
>
i
(X>
S XS + λI)
−1xi

≥
t − d
t
≥
1
2
.
Thus, even though variables Rt are not themselves independent, they can be upper-bounded by a
sequence of independent variables Rbt ∼ Ge(
t−d
t
). The expectation of the total number of trials in
FastRegVol, R¯ =
P
t Rt
, can thus be bounded as follows:
E[R¯] ≤
Xn
t=2d
E[Rbt
] = Xn
t=2d
t
t − d
≤ 2n.
Next, we will obtain a similar bound with high probability instead of in expectation. Here, we
will have to use the fact that the variables Rbt are independent, which means that we can upper-bound
their sum with high probability using standard concentration bounds for geometric distribution. For
example, using Corollary 2.2 from Janson (2018) one can immediately show that with probability
at least 1 − δ we have R¯ = O(n ln δ
−1
). However, more careful analysis shows an even better
dependence on δ.
Lemma 22 Let Rbt ∼ Ge(
t−d
t
) be independent random variables. Then, w.p. at least 1 − δ,
Xn
t=2d
Rbt = O

n + log
n/d
log
1/δ

.
Each trial of rejection sampling requires computing one weight hi
in time O(d
2
). The overall time
complexity of FastRegVol thus includes computation and updating of matrix Z (in time O(nd2
)),
rejection sampling which takes O
n + log
n
d

log
1
δ
 d
2

time, and (if s < 2d) the RegVol
portion, taking O(d
3
).
Proof of Lemma 22 As observed by Janson (2018), tail-bounds for the sum of geometric random
variables depend on the minimum acceptance probability among those variables. Note that for the
vast majority of Rbt’s the acceptance probability is very close to 1, so intuitively we should be able
to take advantage of this to improve our tail bounds. To that end, we partition the variables into
groups of roughly similar acceptance probability and then separately bound the sum of variables
in each group. Let J = log(n
d
) (w.l.o.g. assume that J is an integer). For 1 ≤ j ≤ J, let
Ij = {d2
j
, d2
j + 1, .., d2
j+1} represent the j-th partition. We use the following notation for each
partition:
R¯
j
def
=
X
t∈Ij
Rt
, µj
def
= E[R¯
j ], rj
def = min
t∈Ij
t − d
t
, γj
def
=
log(δ
−1
)
d2
j−2
+       
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Now, we apply Theorem 2.3 of Janson (2018) to R¯
j , obtaining
P(R¯
j ≥ γjµj ) ≤ γ
−1
j
(1 − rj )
(γj−1−ln γj )µj
(1)
≤ (1 − rj )
γjµj/4
(2)
≤ 2
−jγjd2
j−2
,
where (1) follows since γj ≥ 3, and (2) holds because µj ≥ d2
j
and rj ≥ 1 − 2
−j
. Moreover, for
the chosen γj we have
jγjd2
j−2 = j log(δ
−1
) + 3jd2
j−2 ≥ log(δ
−1
) + j = log(2j
δ
−1
).
Let A denote the event that R¯
j ≤ γjµj for all j ≤ J. Applying union bound, we get
P(A) ≥ 1 −
X
J
j=1
P(R¯
j ≥ γjµj ) ≥ 1 −
X
J
j=1
2
− log(2j
δ−1
) = 1 −
X
J
j=1
δ
2
j
≥ 1 − δ.
If A holds, then we obtain the desired bound:
Xn
t=2d
Rbt ≤
X
J
j=1
γjµj ≤
X
J
j=1

log(δ
−1
)
d2
j−2
+ 3
d2
j+1 = 8J log(δ
−1
) + 6X
J
j=1
d2
j
= O

log
n/d
log
1/δ
+ n

.
6. Experiments
In this section we experimentally evaluate the proposed volume sampling algorithms in terms of
runtime and in the task of subsampling for linear regression. We use regularization both for sampling
and for prediction, as discussed in Section 4. The list of implemented algorithms is:
a) Regularized volume sampling (algorithms FastRegVol and RegVol),
b) Leverage score sampling13 (LSS) – a popular i.i.d. sampling technique (Mahoney, 2011),
where examples are selected w.p. P(i) = (x
>
i
(X>X)
−1xi)/d.
Dataset n × d RegVol FastRegVol LSS
cadata 21k×8 33.5s 0.9s 0.1s
MSD 464k×90 >24hr 39s 12s
cpusmall 8k×12 1.7s 0.4s 0.07s
abalone 4k×8 0.5s 0.2s 0.03s
Table 1: List of regression datasets with runtime comparison
between RegVol and FastRegVol. We also provide the runtime
for i.i.d. sampling with exact leverage scores (LSS).
The experiments were performed
on several benchmark linear regression datasets from the libsvm repository
(Chang and Lin, 2011). Table 1 lists those
datasets along with running times for sampling dimension many columns with each
method. Dataset MSD was too big for
RegVol to finish in reasonable time, however FastRegVol finished in less than 40
seconds. In Figure 6 we plot the runtime against varying values of n (using portions of the datasets), to
compare how FastRegVol and RegVol scale with respect to the data size. We observe that FastRegVol
exhibits linear dependence on n, thus it is much better suited for running on large datasets.
13. Regularized variants of leverage scores have also been considered in context of kernel ridge regression Alaoui and
Mahoney (2015). However, in our experiments regularizing leverage scores did not provide any improvements.
  
DEREZINSKI AND ´ WARMUTH
0 2000 4000 6000 8000 10000
Data Size n
0
2
4
6
8
Time (seconds)
cadata
RegVol
FastRegVol
1000 2000 3000 4000 5000 6000
Data Size n
0
10
20
30
Time (seconds)
MSD
RegVol
FastRegVol
1000 2000 3000 4000 5000 6000 7000 8000
Data Size n
0
0.5
1
1.5
Time (seconds)
cpusmall
RegVol
FastRegVol
1500 2000 2500 3000 3500 4000
Data Size n
0.1
0.2
0.3
0.4
Time (seconds)
abalone
RegVol
FastRegVol
Figure 6: Comparison of runtime between FastRegVol and RegVol on four libsvm regression datasets (Chang
and Lin, 2011), with the methods ran on data subsets of varying size (n).
6.1 Subset Selection for Ridge Regression
We applied volume sampling to the task of subset selection for linear regression, by evaluating the
subsampled ridge estimator w∗
λ
(S) using the average loss over the full dataset, i.e.,
Average Loss:
1
n
kXw∗
λ
(S) − yk
2
, where w∗
λ
(S) = (X>
S XS + λI)
−1X>
S yS.
We evaluated the estimators for a range of subset sizes and values of λ, when the subsets are
sampled according to λ-regularized volume sampling14 and leverage score sampling. The results
were averaged over 20 runs of each experiment. For clarity, Figure 7 shows the results only with one
value of λ for each dataset, chosen so that the subsampled ridge estimator performed best (on average
over all samples of preselected size s). Note that for leverage scores we did the appropriate rescaling
of the instances before solving for w∗
λ
(S) for the sampled subproblems (see Mahoney (2011) for
details). Volume sampling does not require any rescaling. The results on all datasets show that when
only a small number of responses s is obtainable, then regularized volume sampling offers better
estimators than leverage score sampling (as predicted by Theorems 17 and 18). The lower bound
from Theorem 18b can be observed for dataset cpusmall, where d = 12 and d log d ≈ 30.
7. Conclusions
Volume sampling is a joint sampling procedures that produces more diverse samples than i.i.d.
sampling. We developed a method for proving exact matrix expectation formulas for volume
sampling giving further credence to the fact that this is a fundamental sampling procedure. We also
made significant progress on finding an efficient implementation of this sampling procedure: Our
14. Our experiments suggest that using the same λ for sampling and for computing the ridge estimator works best.
32
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Figure 7: Comparison of loss of the subsampled ridge estimator when using regularized volume sampling vs
using leverage score sampling on four datasets.
new reverse iterative volume sampling algorithm runs in time O(nd2
). Note that this running time is
within a constant factor of i.i.d. sampling with exact leverage scores and is a remarkable feat since
volume sampling was only recently shown to be polynomial (that is O(n
4
s) in Li et al. (2017)).
A final long ranging question is how to generalize volume sampling and the exact matrix
expectation formulas to higher order tensors.
Acknowledgments
Thanks to Daniel Hsu and Wojciech Kotłowski for many valuable discussions. This research was
supported by NSF grant IIS-1619271.
Appendix A. Inductive Proof of Cauchy-Binet
The most common form of the Cauchy-Binet equation deals with two real
P
n × d matrices A, B:
S : |S|=d
det(A>
S BS) = det(A>B). It is easy to generalize volume sampling and Theorem 2 to
this “asymmetric” version. Here we give an alternate inductive proof.
For i ∈ {1..n}, let ai
, bi denote the i-th row of A, B, respectively. For S ⊆ {1..n}, AS consists
of all rows indexed by S, and A−i
, all except for the i-th row.
Theorem 23 For A, B ∈ R
n×d and n − 1 ≥ s ≥ d :
det(A>B) = 1

n−d
s−d

X
S : |S|=s
det(A>
S BS).
3 
DEREZINSKI AND ´ WARMUTH
Proof S is a size s subset of a set of size n. We rewrite the range restriction n − 1 ≥ s ≥ d for size
s as 1 ≤ n−s ≤ n−d and induct on n − s. For the base case, n−s = 1 or s = n−1, we need to
show that
det(A>B) = 1
n − d
Xn
i=1
det(A>
−iB−i).
This clearly holds if det(A>B) = 0. Otherwise, by Sylvester’s Theorem
Xn
i=1
det(
A>B−aib>
i
z }| {
A>
−iB−i )
det(A>B)
=
Xn
i=1
(1 − a
>
i
(A>B)
−1bi) = n −
d
z }| {
tr((A>B)
−1A>B).
Induction: Assume 2 ≤ n − s ≤ n − d.
det(A>B)
base
case
=
1
n − d
Xn
i=1
det(A>
−iB−i)
ind.
step
=
1
n − d
Xn
i=1
X
S : |S|=s, i /∈S
1

n−1−d
s−d
 det(A>
S BS)
=
n − s
n − d
1

n−1−d
s−d

| {z }
1
(
n−d
s−d)
X
S : |S|=s
det(A>
S BS).
Note that for the induction step, S is a subset of size s from a set of size n − 1 and we have the range
restriction 1 ≤ n−1−s ≤ n−1−d. Clearly, n−1−s is one smaller than n−s. For the last equality,
notice that each set S : |S| = s is counted n − s times in the double sum.
Appendix B. Alternate Proof of Theorem 5
We make use of the following derivative for determinants by Petersen and Pedersen (2012):
For symmetric C:
∂ det(X>CX)
∂X
= 2 det(X>CX)CX(X>CX)
−1
.
The proof begins with generalized Cauchy-Binet for size s volume sampling:
X
S
det(X>ISX) = 
n − d
s − d

det(X>X).
Now, we take a derivative w.r.t. X on both sides
X
S
2 det(X>ISX) (ISX)
+> =

n − d
s − d

2 det(X>X) X+>
⇐⇒ X
S
det(X>ISX)

n−d
s−d

det(X>X)
(ISX)
+>
| {z }
E[(ISX)+>]
= X+>.   
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION
Appendix C. Proof of Proposition 14
The main idea behind the proof is to construct variants of the input matrix X and relate their volumes.
We use the following standard properties of the determinant:
Proposition 24 For any matrix M, det(M>M) = det(Mf>Mf) where Mf is produced from M
through the following operations:
a) Mf equals M except that column mj is replaced by mj + αmi
, where mi
is another column
of M;
b) Mf equals M except that two rows are swapped.
Recall that our goal is to prove the following formula for any X, y and i ∈ {1..n}:
det(X>X)

L(w∗
(−i)) − L(w∗
)

=

det(X>X) − det(X>
−iX−i)

`i(w∗
(−i)).
By Proposition 24b, we can assume w.l.o.g. that i = n, i.e. that the test row in Proposition 14 is the
last row of X. As discussed in Section 3.3, the columns of X are the feature vectors, denoted by
f1, . . . ,fd. Moreover, the optimal prediction vector on the full dataset, yb = Xw∗
, is a projection of
y onto the subspace spanned by the features/columns of X, denoted as yb = PX y. Let us define a
vector y as
y
> def = ( yb
>
−n
, yn), (18)
where yb−n
def
= X−nw∗
(−n) is the optimal prediction vector for the training problem (X−n, y−n).
Note, that if rank(X−n) < d, then w∗
(−n) may not be unique, but we can pick any weight vector as
long as it minimizes the loss on the training set {1..n−1}. Next, we show the following lemma:
Lemma 25 The best achievable loss for the problem (X, y) can be decomposed as follows:
L(w∗
) = L(w∗
(−n)) − `n(w∗
(−n)) + ky − ybk
2
. (19)
Proof First, we will show that y is the projection of y onto the subspace spanned by all features
and the unit vector en ∈ R
n
(where n corresponds to the test row). That is, we want to show that
y = P(X,en) y. Denote ye as that projection. Observe that yen = yn, because if this was not true,
we could construct a vector ye + (yn − yen)en that is closer to y than ye and lies in span(X, en).
Thus, the projection does not incur any loss along the n-th dimension and can be reduced to the
remaining n − 1 dimensions, which corresponds to solving the training problem (X−n, y−n). Using
the definition of y in (18), this shows that ye = P(X,en) y equals y.
Next, we will show that yb is the projection of y onto span(X), i.e. that PX y = yb. By the
linearity of projection, we have
PX y = PX(y − y + y)
= PX(y − y) + PX y
= PX(y − y) + yb.
We already showed that y = P(X,en) y. Therefore, the vector y − y is orthogonal to the column
vectors of X, and thus PX(y − y) = 0. This shows that PX y = yb.
  
DEREZINSKI AND ´ WARMUTH
Finally, note that since y is the projection of y onto span(X, en) and yb ∈ span(X, en), vector
y − y is orthogonal to vector y − yb and by the Pythagorean Theorem we have
kyb − yk
2 = ky − yk
2 + ky − ybk
2
.
Using the definition of y in (18), we have
ky − yk
2 = kyb−n − y−nk
2 = L(w∗
(−n)) − `n(w∗
(−n)),
concluding the proof of the lemma.
Proof of Proposition 14 We construct a matrix X, adding vector y as an extra column to matrix X:
X
def = (X , y) =


X−n yb−n
x
>
n
yn

 . (20)
Applying “base × height” and Lemma 25, we compute the volume spanned by X:
det(X
>X) = det(X>X) ky − ybk
2 = det(X>X) (L(w∗
) − L(w∗
(−n)) + `n(w∗
(−n))). (21)
Next, we use the fact that volume is preserved under elementary column operations (Proposition 24a).
Note, that prediction vector yb−n is a linear combination of the columns of X−n, with the coefficients
given by w∗
(−n). Therefore, looking at the block structure of X (see (20)), we observe that
performing column operations on the last column of X with coefficients given by negative w∗
(−n),
we can zero out that column except for its last element:
y − X w∗
(−n) = r en,
where r
def
= yn − x
>
n w∗
(−n) (see transformation (a) in (22)). Now, we consider two cases, depending
on whether or not r equals zero. If r 6= 0, then we further transform the matrix by a second
transformation (b), which zeros out the last row (the test row) using column operations. The entire
sequence of operations, resulting in a matrix we call X0, is shown below:
X =


X−n yb−n
x
>
n
yn


(a) →


X−n 0
x
>
n
r


(b) →


X−n 0
0 r

 = X0. (22)
Note, that due to the block-diagonal structure of X0, its volume can be easily described by the “base
× height” formula:
det(X
>
0 X0) = det(X>
−nX−n) r
2 = det(X>
−nX−n) `n(w∗
(−n)). (23)
Since det(X
>X) = det(X
>
0 X0), we can combine (21) and (23) to obtain the desired result.
Finally, if r = 0 we cannot perform transformation (b). However, in this case matrix X has
volume 0, and moreover, `n(w∗
(−n)) = r
2 = 0, so once again we have
det(X
>X) = 0 = det(X>
−nX−n) `n(w∗
(−n)),
which concludes the proof of Proposition 14.
36
REVERSE ITERATIVE VOLUME SAMPLING FOR LINEAR REGRESSION