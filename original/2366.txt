Scalable machine learning over big data is an important problem that is receiving a lot of
attention in recent years. On popular distributed environments such as Hadoop running on
a cluster of commodity machines, communication costs are substantial and algorithms need
to be designed suitably considering those costs. In this paper we give a novel approach to the
distributed training of linear classifiers (involving smooth losses and L2 regularization) that
is designed to reduce the total communication costs. At each iteration, the nodes minimize
locally formed approximate objective functions; then the resulting minimizers are combined
to form a descent direction to move. Our approach gives a lot of freedom in the formation
of the approximate objective function as well as in the choice of methods to solve them.
The method is shown to have O(log(1/)) time convergence. The method can be viewed as
an iterative parameter mixing method. A special instantiation yields a parallel stochastic
gradient descent method with strong convergence. When communication times between
nodes are large, our method is much faster than the Terascale method (Agarwal et al.,
2011), which is a state of the art distributed solver based on the statistical query model (Chu
et al., 2006) that computes function and gradient values in a distributed fashion. We also
evaluate against other recent distributed methods and demonstrate superior performance
of our method.
Keywords: Distributed learning, Example partitioning, L2 regularization
1. Introduction
In recent years, machine learning over Big Data has become an important problem, not
only in web related applications, but also more commonly in other applications, e.g., in
the data mining over huge amounts of user logs. The data in such applications are usually
collected and stored in a decentralized fashion over a cluster of commodity machines (nodes)
where communication times between nodes are significantly large. Examples of applications
involving high communication costs include: Click prediction on advertisement data where
the number of examples is huge and features are words which can run in to billions, and
Geo distributed data across different countries/continents where cross data-center speeds
are extremely slow. In such a settings, it is natural for the examples to be partitioned over
the nodes.
Distributed machine learning algorithms that operate on such data are usually iterative.
Each iteration involves some computation that happens locally in each node. In each iteration, there is also communication of information between nodes and this is special to the
distributed nature of solution. Distributed systems such as those based on the Map-Reduce
framework (Dean and Ghemawat, 2008) involve additional special operations per iteration,
such as the loading of data from disk to RAM. Recent frameworks such as Spark (Zaharia
et al., 2010) and REEF (Weimer et al., 2015) avoid such unnecessary repeated loading of
data from disk. Still, communication between nodes in each iteration is unavoidable and,
its cost can be substantial when working with Big Data. Therefore, the development of efficient distributed machine learning algorithms that minimize communication between nodes
is an important problem. The key is to come up with algorithms that minimize the number
of iterations.
In this paper we consider the distributed batch training of linear classifiers in which:
(a) both, the number of examples and the number of features are large; (b) the data matrix
is sparse; (c) the examples are partitioned over the nodes; (d) the loss function is convex
and differentiable; and, (e) the L2 regularizer is employed. This problem involves the large
scale unconstrained minimization of a convex, differentiable objective function f(w) where
w is the weight vector. The minimization is usually performed using an iterative descent
method in which an iteration starts from a point w
r
, computes a direction d
r
that satisfies
sufficient angle of descent: −g
r
, dr ≤ θ (1)
where g
r = g(w
r
), g(w) = ∇f(w), a, b is the angle between vectors a and b, and 0 ≤
θ < π/2, and then performs a line search along the direction d
r
to find the next point,
w
r+1 = w
r + tdr
. Let w
? = arg minw f(w). A key side contribution of this paper is the
proof that, when f is convex and satisfies some additional weak assumptions, the method has
global linear rate of convergence (glrc)
1 and so it finds a point w
r
satisfying f(w
r
)−f(w
?
) ≤ 
in O(log(1/)) iterations. The main theme of this paper is that the flexibility offered by this
method with strong convergence properties allows us to build a class of useful distributed
learning methods with good computation and communication trade-off capabilities.
Take one of the most effective distributed methods, viz., SQM (Statistical Query Model)
(Chu et al., 2006; Agarwal et al., 2011), which is a batch, gradient-based descent method.
The gradient is computed in a distributed way with each node computing the gradient
1. We say a method has glrc if ∃ 0 < δ < 1 such that (f(w
r+1) − f(w
?
)) ≤ δ(f(w
r
) − f(w
?
)) ∀r.
2
An efficient distributed learning algorithm
component corresponding to its set of examples. This is followed by an aggregation of the
components. We are interested in systems in which the communication time between nodes
is large relative to the computation time in each node.2 For iterative algorithms such as
SQM, the total training time is given by
Training time = (T
cmp + T
com) T
iter (2)
where T
cmp and T
com are respectively, the computation time and the communication time
per iteration and T
iter is the total number of iterations. When T
com is large, it is not
optimal to work with an algorithm such as SQM that has T
cmp small and due to which,
T
iter is large. In such a scenario, it is useful to ask: Q1. In each iteration, can we do
more computation in each node so that the number of iterations and hence the number of
communication passes are decreased, thus reducing the total computing time?
There have been some efforts in the literature to reduce the amount of communication.
In one class of such methods, the current w
r
is first passed on to all the nodes. Then,
each node p forms an approximation ˜fp of f using only its examples, followed by several
optimization iterations (local passes over its examples) to decrease ˜fp and reach a point wp.
The wp ∀p are averaged to form the next iterate w
r+1. One can stop after just one major
iteration (going from r = 0 to r = 1); such a method is referred to as parameter mixing
(PM) (Mann et al., 2009). Alternatively, one can do many major iterations; such a method
is referred to as iterative parameter mixing (IPM) (Hall et al., 2010). Convergence theory
for such methods is inadequate (Mann et al., 2009; McDonald et al., 2010), which prompts
us to ask: Q2. Is it possible to devise an IPM method that produces {w
r} → w
??
In another class of methods, the dual problem is solved in a distributed fashion (Pechyony et al., 2011; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014). Let αp denote the dual
vector associated with the examples in node p. The basic idea is to optimize {αp} in parallel
and then use a combination of the individual directions thus generated to take an overall
step. In practice these methods tend to have slow convergence; see Section 4 for details.
We make a novel and simple use of the iterative descent method mentioned at the
beginning of this section to design a distributed algorithm that answers Q1-Q2 positively.
The main idea is to use distributed computation for generating a good search direction
d
r and not just for forming the gradient as in SQM. At iteration r, let us say each node
p has the current iterate w
r and the gradient g
r
. This information can be used together
with the examples in the node to form a function ˆfp(·) that approximates f(·) and satisfies
∇ ˆfp(w
r
) = g
r
. One simple and effective suggestion is:
ˆfp(w) = fp(w) + (g
r − ∇fp(w
r
)) · (w − w
r
) (3)
where fp is the part of f that does not depend on examples outside node p; the second
term in (3) can be viewed as an approximation of the objective function part associated
with data from the other nodes. In Section 3 we give other suggestions for forming ˆfp. Now
ˆfp can be optimized within node p using any method M which has glrc, e.g., Trust region
method, L-BFGS, etc. There is no need to optimize ˆfp fully. We show (see Section 3) that,
in a constant number of local passes over examples in node p, an approximate minimizer
2. This is the case when feature dimension is huge. Many applications gain performance when the feature
space is expanded, say, via feature combinations, explicit expansion of nonlinear kernels etc.
3
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
wp of ˆfp can be found such that the direction dp = wp − w
r
satisfies the sufficient angle
of descent condition, (1). A convex combination of the set of directions generated in the
nodes, {dp} forms the overall direction d
r
for iteration r. Note that d
r also satisfies (1).
The result is an overall distributed method that finds a point w satisfying f(w)−f(w
?
) ≤ 
in O(log(1/)) time. This answers Q2.
The method also reduces the number of communication passes over the examples compared with SQM, thus also answering Q1. The intuition here is that, if each ˆfp is a good
approximation of f, then d
r will be a good global direction for minimizing f at w
r
, and so
the method will move towards w
? much faster than SQM.
In summary, the paper makes the following contributions. First, for convex f we establish glrc for a general iterative descent method. Second, and more important, we propose a
distributed learning algorithm that: (a) converges in O(log(1/)) time, thus leading to an
IPM method with strong convergence; (b) is more efficient than SQM when communication
costs are high; and (c) flexible in terms of the local optimization method M that can be
used in the nodes.
There is also another interesting side contribution associated with our method. It is
known that example-wise methods such as stochastic gradient descent (SGD) are inherently
sequential and hard to parallelize (Zinkevich et al., 2010). By employing SGD as M,
the local optimizer for ˜fp in our method, we obtain a parallel SGD method with good
performance as well as strong convergence properties. This contribution is covered in detail
in Mahajan et al. (2013b); we give a summarized view in Subsection 3.5.
Experiments (Section 4) validate our theory as well as show the benefits of our method
for large dimensional datasets where communication is the bottleneck. We give a discussion
on unexplored possibilities for extending our distributed learning method in Section 5 and
conclude the paper in Section 6.
2. Basic descent method
Let f ∈ C1
, the class of continuously differentiable functions3
, f be convex, and the gradient
g satisfy the following assumptions.
A1. g is Lipschitz continuous, i.e., ∃ L > 0 such that kg(w) − g( ˜w)k ≤ Lkw − w˜k ∀ w, w˜.
A2. ∃ σ > 0 such that (g(w) − g( ˜w)) · (w − w˜) ≥ σkw − w˜k
2 ∀ w, w˜.
A1 and A2 are essentially second order conditions: if f happens to be twice continuously
differentiable, then L and σ can be viewed as upper and lower bounds on the eigenvalues of
the Hessian of f. A convex function f is said to be σ- strongly convex if f(w) −
σ
2
kwk
2
is
convex. In machine learning, all convex risk functionals in C
1 having the L2 regularization
term, λ
2
kwk
2 are σ- strongly convex with σ = λ. It can be shown (Smola and Vishwanathan,
2008) that, if f is σ-strongly convex, then f satisfies assumption A2.
Let f
r = f(w
r
), g
r = g(w
r
) and w
r+1 = w
r + tdr
. Consider the following standard line
search conditions.
Armijo: f
r+1 ≤ f
r + αgr
· (w
r+1 − w
r
) (4)
Wolfe: g
r+1
· d
r ≥ βgr
· d
r
(5)
3. It would be interesting future work to extend all the theory developed in this paper to non-differentiable
convex functions, using sub-gradients.
4
An efficient distributed learning algorithm
where 0 < α < β < 1.
Algorithm 1: Descent method for f
Choose w
0
;
for r = 0, 1 . . . do
1. Exit if g
r = 0;
2. Choose a direction d
r
satisfying (1);
3. Do line search to choose t > 0 so that w
r+1 = w
r + tdr
satisfies the
Armijo-Wolfe conditions (4) and (5);
end
Let us now consider the general descent method in Algorithm 1 for minimizing f. The
following result shows that the algorithm is well-posed. A proof is given in the appendix B.
Lemma 1. Suppose g
r
· d
r < 0. Then {t : (4) and (5) hold for w
r+1 = w
r + tdr} = [tβ, tα],
where 0 < tβ < tα, and tβ, tα are the unique roots of
g(w
r + tβd
r
) · d
r = βgr
· d
r
, (6)
f(w
r + tαd
r
) = f
r + tααgr
· d
r
, tα > 0. (7)
Theorem 2. Let w
? = arg minw f(w) and f
? = f(w
?
).4 Then {w
r} → w
?
. Also, we
have glrc, i.e., ∃ δ satisfying 0 < δ < 1 such that (f
r+1 − f
?
) ≤ δ (f
r − f
?
) ∀ r ≥ 0,
and, f
r − f
? ≤  is reached after at most log((f
0−f
?)/)
log(1/δ)
iterations. An upper bound on δ is
(1 − 2α(1 − β)
σ
2
L2 cos2
θ).
A proof of Theorem 2 is given in the appendix B. If one is interested only in proving
convergence, it is easy to establish under the assumptions made; such theory goes back to
the classical works of Wolfe (Wolfe, 1969, 1971). But proving glrc is harder. There exist
proofs for special cases such as the gradient descent method (Boyd and Vandenberghe,
2004). The glrc result in Wang and Lin (2013) is only applicable to descent methods that
are “close” (see equations (7) and (8) in Wang and Lin (2013)) to the gradient descent
method. Though Theorem 2 is not entirely surprising, as far as we know, such a result does
not exist in the literature.
Remark 1. It is important to note that the rate of convergence indicated by the upper
bound on δ given in Theorem 2 is pessimistic since it is given for a very general descent
algorithm that includes plain batch gradient descent which is known to have a slow rate of
convergence. Therefore, it should not be misconstrued that the indicated slow convergence
rate would hold for all methods falling into the class of methods covered here. Depending
on the method used for choosing d
r
, the actual rate of convergence can be a lot better. For
example, we observe very good rates for our distributed method, FADL; see the empirical
results in Section 4 and also the comments made in Remark 2 of Section 3.
3. Distributed training
In this section we discuss full details of our distributed training algorithm. Let {xi
, yi} be
the training set associated with a binary classification problem (yi ∈ {1, −1}). Consider a
4. Assumption A2 implies that w
?
is unique.
5
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
linear classification model, y = sgn(w
T x). Let l(w·xi
, yi) be a continuously differentiable loss
function that has Lipschitz continuous gradient. This allows us to consider loss functions
such as least squares (l(s, y) = (s − y)
2
), logistic loss (l(s, y) = log(1 + exp(−sy)) and
squared hinge loss (l(s, y) = max{0, 1−ys}
2
). Hinge loss is not covered by our theory since
it is non-differentiable.
Suppose the training examples are distributed in P nodes. Let: Ip be the set of indices i
such that (xi
, yi) sits in the p-th node; Lp(w) = P
i∈Ip
l(w; xi
, yi) be the total loss associated
with node p; and, L(w) = P
p Lp(w) be the total loss over all nodes. Our aim is to minimize
the regularized risk functional f(w) given by
f(w) = λ
2
kwk
2 + L(w) = λ
2
kwk
2 +
X
p
Lp(w), (8)
where λ > 0 is the regularization constant. It is easy to check that g = ∇f is Lipschitz
continuous.
3.1. Our approach
Our distributed method is based on the descent method in Algorithm 1. We use a masterslave architecture.5 Let the examples be partitioned over P slave nodes. Distributed computing is used to compute the gradient g
r as well as the direction d
r
. In the r-th iteration,
let us say that the master has the current w
r and gradient g
r
. One can communicate these
to all P (slave) nodes. The direction d
r
is formed as follows. Each node p constructs an
approximation of f(w) using only information that is available in that node, call it ˆfp(w),
and (approximately) optimizes it (starting from w
r
) to get the point wp. Let dp = wp − w
r
.
Then d
r
is chosen to be any convex combination of dp ∀p. Doing line search along the d
r
direction completes the r-th iteration. Line search involves distributed computation, but it
is inexpensive; we give details in Subsection 3.4.
We want to point out that ˆfp can change with r, i.e., one is allowed to use a different ˆfp
in each outer iteration. We just don’t mention it as ˆf
r
p
to avoid clumsiness of notation. In
fact, all the choices for ˆfp that we discuss below in Subsection 3.2 are such that ˆfp depends
on the current iterate, w
r
.
3.2. Choosing ˆfp
Our method offers great flexibility in choosing ˆfp and the method used to optimize it. We
only require ˆfp to satisfy the following.
A3. ˆfp is σ-strongly convex, has Lipschitz continuous gradient and satisfies gradient consistency at w
r
: ∇ ˆfp(w
r
) = g
r
.
Below we give several ways of forming ˆfp. The σ-strongly convex condition is easily
taken care of by making sure that the L2 regularizer is always a part of ˆfp. This condition
implies that
ˆfp(wp) ≥ ˆfp(w
r
) + ∇ ˆfp(w
r
) · (wp − w
r
) + σ
2
kwp − w
r
k
2
. (9)
5. An AllReduce arrangement of nodes (Agarwal et al., 2011) may also be used.
6
An efficient distributed learning algorithm
The gradient consistency condition is motivated by the need to satisfy the angle condition
(1). Since wp is obtained by starting from w
r and optimizing ˆfp, it is reasonable to assume
that ˆfp(wp) < ˆfp(w
r
). Using these in (9) gives −g
r
·dp > 0. Since d
r
is a convex combination
of the dp it follows that −g
r
· d
r > 0. Later we will formalize this to yield (1) precisely.
A general way of choosing the approximating functional ˆfp is
ˆfp(w) = λ
2
kwk
2 + L˜
p(w) + Lˆ
p(w), (10)
where L˜
p is an approximation of Lp and Lˆ
p(w) is an approximation of L(w)−Lp(w) = P
q6=p
Lq(w). A natural choice for L˜
p is Lp itself since it uses only the examples within node p;
but there are other possibilities too. To maintain communication efficiency, we would like
to design an Lˆ
p such that it does not explicitly require any examples outside node p. To
satisfy A3 we need Lˆ
p to have Lipschitz continuous gradient. Also, to aid in satisfying
gradient consistency, appropriate linear terms are added. We now suggest five choices for
ˆfp.
Linear Approximation. Set L˜
p = Lp and choose Lˆ
p based on the first order Taylor series.
Thus,
L˜
p(w) = Lp(w), Lˆ
p(w) = (∇L(w
r
) − ∇Lp(w
r
)) · (w − w
r
). (11)
(The zeroth order term needed to get f(w
r
) = ˆf(w
r
) is omitted everywhere because it is a
constant that plays no role in the optimization.) Note that ∇L(w
r
) = g
r − λwr and so it is
locally computable in node p; this comment also holds for the methods below.
Hybrid approximation. This is an improvement over the linear approximation where
we add a quadratic term to Lˆ
p. Since the loss term in (8) is total loss (and not averaged
loss), this is done by using (P − 1) copies of the quadratic term of Lp to approximate the
quadratic term of P
q6=p Lq.
L˜
p(w) = Lp(w), (12)
Lˆ
p(w) = (∇L(w
r
) − ∇Lp(w
r
)) · (w − w
r
) + P − 1
2
(w − w
r
)
T Hr
p
(w − w
r
), (13)
where Hr
p
is the Hessian of Lp at w
r
. This corresponds to using subsampling to approximate the Hessian of L(w) − Lp(w) at w
r utilizing only the local examples. Subsampling
based Hessian approximation is known to be very effective in optimization for machine
learning (Byrd et al., 2012).
Quadratic approximation. This is a pure quadratic variant where a second order approximation is used for L˜
p too.
L˜
p(w) = ∇Lp(w
r
) · (w − w
r
) + 1
2
(w − w
r
)
T Hr
p
(w − w
r
), (14)
Lˆ
p(w) = (∇L(w
r
) − ∇Lp(w
r
)) · (w − w
r
) + P − 1
2
(w − w
r
)
T Hr
p
(w − w
r
). (15)
The comment made earlier on the goodness of subsampling based Hessian for the Hybrid
approximation applies here too.
7
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Nonlinear approximation.
P
Here the idea is to use P − 1 copies of Lp to approximate
q6=p Lq.
L˜
p(w) = Lp(w), (16)
Lˆ
p(w) = (∇L(w
r
) − P ∇Lp(w
r
)) · (w − w
r
) + (P − 1)Lp(w). (17)
A somewhat similar approximation is used in Sharir et al. (2014). But the main algorithm
where it is used does not have deterministic monotone descent like our algorithm. The
gradient consistency condition, which is essential for establishing function descent, is not
respected in that algorithm. In Section 4 we compare our methods against the method
in Sharir et al. (2014).
BFGS approximation. For L˜
p we can either use Lp or a second order approximation,
like in the approximations given above. For Lˆ
p we can use a second order term, 1
2
(w −
w
r
) · H(w − w
r
) where H is a positive semi-definite matrix; for H we can use a diagonal
approximation or keep a limited history of gradients and form a BFGS approximation of
L − Lp.
Remark 2. The distributed method described above is an instance of Algorithm 1
and so Theorem 2 can be used. In Theorem 2 we mentioned a convergence rate, δ. For
cos θ = σ/L this yields the rate δ = (1 − 2α(1 − β)( σ
L
)
4
). This rate is obviously pessimistic
given that it applies to general choices of ˆfp satisfying minimal assumptions. Actual rates
of convergence depend a lot on the choice made for ˆfp. Suppose we choose ˆfp via Hybrid,
Quadratic or Nonlinear approximation choices mentioned in Subsection 3.2 and minimize
ˆfp exactly in the inner optimization. These approximations are invariant to coordinate
transformations such as w
0 = Bw, where B is a positive definite matrix. Note that ArmijoWolfe line search conditions are also unaffected by such transformations. What this means
is that, at each iteration, we can, without changing the algorithm, choose for analysis a
coordinate transformation that gives the best rate. The Linear approximation choice for
ˆfp does not enjoy this property. This explains why the Hybrid, Quadratic and Nonlinear
approximations perform so well and give great rates of convergence in practice; see the
experiments in Section 4.7 and Subsection 4.9.1. Proving much better convergence rates for
FADL using these approximations would be interesting future work.
In Section 4 we evaluate some of these approximations in detail.
3.3. Convergence theory
In practice, exactly minimizing ˆfp is infeasible. For convergence, it is not necessary for wp
to be the minimizer of ˆfp; we only need to find wp such that the direction dp = wp − w
r
satisfies (1). The angle θ needs to be chosen right. Let us discuss this first. Let ˆw
?
p be the
minimizer of ˆfp. It can be shown (see appendix B) that wˆ
?
p − w
r
, −g
r ≤ cos−1 σ
L
. To allow
for wp being an approximation of ˆw
?
p
, we choose θ such that
π
2
> θ > cos−1 σ
L
. (18)
The following result shows that if an optimizer with glrc is used to minimize ˆfp, then, only
a constant number of iterations is needed to satisfy the sufficient angle of descent condition.
8
An efficient distributed learning algorithm
Lemma 3. Assume g
r 6= 0. Suppose we minimize ˆfp using an optimizer M that starts from
v
0 = w
r and generates a sequence {v
k} having glrc, i.e., ˆfp(v
k+1) − ˆf
?
p ≤ δ(
ˆfp(v
k
) − ˆf
?
p
),
where ˆf
?
p = ˆfp( ˆw
?
p
). Then, there exists ˆk (which depends only on σ and L) such that
−g
r
, vk − w
r ≤ θ ∀k ≥ ˆk.
Lemma 3 can be combined with Theorem 2 to yield the following convergence theorem.
Theorem 4. Suppose θ satisfies (18), M is as in Lemma 3 and, in each iteration r and for
each p,
ˆk or more iterations of M are applied to minimize ˆfp (starting from w
r
) and get
wp. Then the distributed method converges to a point w satisfying f(w) − f(w
?
) ≤  in
O(log(1/)) time.
Proofs of Lemma 3 and Theorem 4 are given in appendix B.
3.4. Practical implementation.
We refer to our method by the acronym, FADL - Function Approximation based Distributed
Learning. Going with the practice in numerical optimization, we replace (1) by the condition, −g
r
· d
r > 0 and use α = 10−4
, β = 0.9 in (4) and (5). In actual usage, Algorithm 1
can be terminated when kg
rk ≤ gkg
0k is satisfied at some r. Let us take line search next.
On w = w
r +tdr
, the loss has the form l(zi +tei
, yi) where zi = w
r
·xi and ei = d
r
·xi
. Once
we have computed zi ∀i and ei ∀i, the distributed computation of f(w
r +tdr
) and its derivative with respect to t is cheap as it does not involve any computation involving the data,
{xi}. Thus, many t values can be explored cheaply. Since d
r
is determined by approximate
optimization, t = 1 is expected to give a decent starting point. We first identify an interval
[t1, t2] ⊂ [tβ, tα] (see Lemma 1) by starting from t = 1 and doing forward and backward
stepping. Then we check if t1 or t2 is the minimizer of f(w
r + tdr
) on [t1, t2]; if not, we do
several bracketing steps in (t1, t2) to locate the minimizer approximately. Finally, when using method M, we terminate it after a fixed number of steps, ˆk; we have found that, setting
ˆk to match computation and communication costs works effectively. Algorithm 2 gives all
the steps of FADL while also mentioning the distributed communications and computations
involved.
Choices for M. There are many good methods having (deterministic) glrc: L-BFGS,
TRON (Lin et al., 2008), Primal coordinate descent (Chang et al., 2008), etc. One could also
use methods with glrc in the expectation sense (in which case, the convergence in Theorem
4 should be interpreted in some probabilistic sense). This nicely connects our method with
recent literature on parallel SGD. We discuss this in the next subsection only briefly as it
is outside the scope of the current paper. See our related work (Mahajan et al., 2013b) for
details. For the experiments of this paper we do the following. For optimizing the quadratic
approximation in (10) with (14)-(15), we used the conjugate-gradient method (Shewchuk,
1994). For all other (non-quadratic) approximations of FADL as well as all nonlinear solvers
needed by other methods, we used TRON.
3.5. Connections with parallel SGD
For large scale learning on a single machine, example-wise methods6
such as stochastic
gradient descent (SGD) and its variations (Bottou, 2010; Johnson and Zhang, 2013) and dual
6. These methods update w after scanning each example.
9
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Algorithm 2: FADL - Function Approximation based Distributed Learning. com:
communication; cmp: = computation; agg: aggregation. M is the optimizer used for
minimizing ˆfp.
Choose w
0
;
for r = 0, 1 . . . do
1. Compute g
r
(com: w
r
; cmp: Two passes over data; agg: g
r
); By-product:
{zi = w
r
· xi};
2. Exit if kg
rk ≤ gkg
0k;
3. for p = 1, . . . , P (in parallel) do
4. Set v
0 = w
r
;
5. for k = 0, 1, . . . ,
ˆk do
6. Find v
k+1 using one iteration of M;
end
7. Set wp = v
kˆ+1;
end
8. Set d
r as any convex combination of {wp} (agg: wp);
9. Compute {ei = d
r
· xi} (com: d
r
; cmp: One pass over data);
10. Do line search to find t (for each t: com: t; cmp: l and ∂l/∂t agg: f(w
r + tdr
)
and its derivative wrt t);
11. Set w
r+1 = w
r + tdr
;
end
coordinate ascent (Hsieh et al., 2008) perform quite well. However, example-wise methods
are inherently sequential. If one employs a method such as SGD as M, the local optimizer
for ˜fp, the result is, in essence, a parallel SGD method. However, with parameter mixing
and iterative parameter mixing methods (Mann et al., 2009; Hall et al., 2010; McDonald
et al., 2010) (we briefly discussed these methods in Section 1) that do not do line search,
convergence theory is limited, even that requiring a complicated analysis (Zinkevich et al.,
2010); see also Mann et al. (2009) for some limited results. Thus, the following has been
an unanswered question: Q3. Can we form a parallel SGD method with strong convergence
properties such as glrc?
As one special instantiation of our distributed method, we can use, for the local optimization method M, any variation of SGD with glrc (in expectation), e.g., the one in Johnson and Zhang (2013). For this case, in a related work of ours (Mahajan et al., 2013b)
we show that our method has O(log(1/)) time convergence in a probabilistic sense. The
result is a strongly convergent parallel SGD method, which answers Q3. An interesting
side observation is that, the single machine version of this instantiation is very close to the
variance-reducing SGD method in Johnson and Zhang (2013). We discuss this next.
Connection with SVRG. Let us take the ˆfp in (11). Let np = |Ip| be the number of
examples in node p. Define ψi(w) = npl(w · xi
, yi) + λ
2
kwk
2
. It is easy to check that
∇ ˆfp(w) = 1
np
X
i∈Ip
(∇ψi(w) − ∇ψi(w
r
) + g
r
). (19)
10
An efficient distributed learning algorithm
Thus, plain SGD updates applied to ˆfp has the form
w = w − η(∇ψi(w) − ∇ψi(w
r
) + g
r
), (20)
which is precisely the update in SVRG. In particular, the single node (P = 1) implementation of our method using plain SGD updates for optimizing ˆfp is very close to the
SVRG method.7 While Johnson and Zhang (2013) motivate the update in terms of variance
reduction, we derive it from a functional approximation viewpoint.
3.6. Computation-Communication tradeoff
In this subsection we do a rough analysis to understand the conditions under which our
method (FADL) is faster than the SQM method (Chu et al., 2006; Agarwal et al., 2011)
(see Section 1). This analysis is only for understanding the role of various parameters and
not for getting any precise comparison of the speed of the two methods.
Compared to the SQM method, FADL does a lot more computation (optimize ˆfp) in
each node. On the other hand FADL reaches a good solution using a much smaller number
of outer iterations. Clearly, FADL will be attractive for problems with high communication
costs, e.g., problems with a large feature dimension. For a given distributed computing
environment and specific implementation choices, it is easy to do a rough analysis to understand the conditions in which FADL will be more efficient than SQM. Consider a distributed
grid of nodes in an AllReduce tree. Let us use a common method such as TRON for implementing SQM as well as for M in FADL. Assuming that T
outer
SQM > 3.0T
outer
FADL (where T
outer
FADL
and T
outer
SQM are the number of outer iterations required by SQM and FADL), we can do a
rough analysis of the costs of SQM and FADL (see appendix A for details) to show that
FADL will be faster when the following condition is satisfied.
nz
m
<
γP
2
ˆk
(21)
where: nz is the number of nonzero elements in the data, i.e., {xi}; m is the feature
dimension; γ is the relative cost of communication to computation (e.g. 100 − 1000); P is
the number of nodes; and ˆk is the number of inner iterations of FADL. Thus, the larger
the dimension (m) is, and the higher the sparsity in the data is, FADL will be better than
SQM.
4. Experiments
In this section, we demonstrate the effectiveness of our method by comparing it against
several existing distributed training methods on five large data sets. We first discuss our
experimental setup. We then briefly list each method considered and then do experiments
to decide the best overall setting for each method. This applies to our method too, for
which the setting is mainly decided by the choice made for the function approximation,
ˆfp; see Subsection 3.2 for details of these choices. Finally, we compare, in detail, all the
methods under their best settings. This study clearly demonstrates scenarios under which
our method performs better than other methods.
7. Note the subtle point that applying SVRG method on ˆfp is different from doing (20), which corresponds
to plain SGD. It is the former that assures glrc (in expectation).
11
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
4.1. Experimental Setup
We ran all our experiments on a Hadoop cluster with 379 nodes and 10 Gbit interconnect
speed. Each node has Intel (R) Xeon (R) E5-2450L (2 processors) running at 1.8 GHz.
Since iterations in traditional MapReduce are slower (because of job setup and disk access
costs), as in Agarwal et al. (Agarwal et al., 2011), we build an AllReduce binary tree between
the mappers8
. The communication bandwidth is 1 Gbps (gigabits per sec).
Dataset #Examples (n) #Features (m) #Non-zeros (nz) λ/n
kdd2010 8.41 × 106 20.21 × 106 0.31 × 109 1.25 × 10−6
url 1.91 × 106 3.23 × 106 0.22 × 109 0.11 × 10−6
webspam 0.35 × 106 16.6 × 106 0.98 × 109 1.0 × 10−4
mnist8m 8.1 × 106 784 6.35 × 109 1.0 × 10−4
rcv 0.5 × 106 47236 0.50 × 108 1.0 × 10−4
Table 1: Properties of datasets.
Data Sets. We consider the following publicly available datasets having a large number
of examples:9
kdd2010, url, webspam, mnist8m and rcv. Table 1 shows the numbers of
examples, features, nonzero in data matrix and the values of regularizer λ used. The
regularizer for each dataset is chosen to be the optimal value that gives the best performance
on a small validation set. We use these datasets mainly to illustrate the validity of theory,
and its utility to distributed machine learning. In real scenarios of Big data, the datasets
are typically much larger. Note that kdd2010, url and webspam are large dimensional (m is
large) while mnist8m and rcv are low/medium dimensional (m is not high). This division
of the datasets is useful because communication cost in example-partitioned distributed
methods is mainly dependent on m (see Appendix A) and so these datasets somewhat help
to see the effect of communications cost.
We use the squared-hinge loss function for all the experiments. Unless stated differently,
for all numerical optimizations we use the Trust Region Newton method (TRON) proposed
in Lin et al. (2008).
Evaluation Criteria. We use the relative difference to the optimal function value and
the Area under Precision-Recall Curve (AUPRC) (Sonnenburg and Franc, 2010; Agarwal
et al., 2013)10 as the evaluation criteria. The former is calculated as (f − f
∗
)/f ∗
in log
scale, where f
∗
is the optimal function value obtained by running the TERA algorithm (see
below) for a very large number of iterations.
4.2. Methods for comparison
We compare the following methods.
8. Note that we do not use the pipelined version and hence we incur an extra multiplicative logP cost in
communication.
9. These datasets are available at: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/. For
mnist8m we solve the binary problem of separating class “3” from others.
10. We employed AUPRC instead of AUC because it differentiates methods more finely.
12
An efficient distributed learning algorithm
• TERA: The Terascale method (TERA) (Agarwal et al., 2011) is the best representative
method from the SQM class (Chu et al., 2006). It can be considered as the state-ofthe-art distributed solver and therefore an important baseline.
• ADMM: We use the example partitioning formulation of the Alternating Direction
Method of Multipliers (ADMM) (Boyd et al., 2011; Zhang et al., 2012). ADMM is
a dual method which is very different from our primal method; however, like our
method, it solves approximate problems in the nodes and iteratively reaches the full
batch solution.
• CoCoA: This method (Jaggi et al., 2014) represents the class of distributed dual
methods (Pechyony et al., 2011; Yang, 2013; Yang et al., 2013; Jaggi et al., 2014) that,
in each outer iteration, solve (in parallel) several local dual optimization problems.
• DANE: This is the Newton based method described in Sharir et al. (2014) that uses
a function approximation similar to FADL.
• DiSCO: This (Zhang and Xiao, 2015) is a distributed Newton method designed with
communication-efficiency in mind.
• Our method (FADL): This is our method described in detail in Section 3 and more
specifically, in Algorithm 2.
4.3. Study of TERA
A key attractive property of TERA is that the number of outer iterations pretty much
remains constant with respect to the number of distributed nodes used. As recommended
by Agarwal et al. (2011), we find a local weight vector per node by minimizing the local
objective function (based only on the examples in that node) using five epochs of SGD (Bottou, 2010). (The optimal step size is chosen by running SGD on a subset of data.) We then
average the weights from all the nodes (on a per-feature basis as explained in Agarwal et al.
(2011)) and use the averaged weight vector to warm start TERA.
11 Agarwal et al. (2011)
use the LBFGS method as the trainer whereas we use TRON. To make sure that this does
not lead to bias, we try both, TERA-LBFGS and TERA-TRON. Figure 1 compares the
progress of objective function for these two choices. Clearly, TERA-TRON is superior. We
observe similar behavior on the other datasets also. Given this, we restrict ourselves to
TERA-TRON and simply refer to it as TERA.
4.4. Study of ADMM
The ADMM objective function (Boyd et al., 2011) has a quadratic proximal term called
augmented Lagrangian with a penalty parameter ρ multiplying it. In general, the performance of ADMM is very sensitive to the value of ρ and hence making a good choice for it
is crucial. We consider three methods for choosing ρ.
11. We use this inexpensive initialization for FADL and ADMM too. It is not applicable to CoCoA. Because
of this, CoCoA starts with a different primal objective function value than others.
13
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Time (in secs.)
0 1000 2000 3000 4000 5000 6000 7000
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
TERA-LBFGS
TERA-TRON (a) kdd2010 - 8 nodes
Time (in secs.)
0 1000 2000 3000 4000 5000 6000 7000
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
TERA-LBFGS
TERA-TRON (b) kdd2010 - 128 nodes
Figure 1: Plots showing the time efficiency of TERA methods for kdd2010.
Even though there is no supporting theory, Boyd et al (Boyd et al., 2011) suggest an
approach by which ρ is adapted in each iteration; see Equation (3.13) in Section 3.4.1 of
that paper. We will refer to this choice as Adap.
Recently, Deng and Yin (2012) proved a linear rate of convergence for ADMM under
assumptions A1 and A2 (see Section 2) on ADMM functions. As a result, their analysis
also hold for the objective function in (8). They also give an analytical formula to set ρ in
order to get the best theoretical linear rate constant. We will refer to this choice of ρ as
Analytic.
We also consider a third choice, ADMM-Search in which, we start with the value of ρ
given by Analytic, choose several values of ρ in its neighborhood and select the best ρ by
running ADMM for 10 iterations and looking at the objective function value. Note that
this step takes additional time and causes a late start of ADMM.
Figure 2 compares the progress of the training objective function for the three choices
on kdd2010 for P = 8 and P = 128. Analytic is an order of magnitude slower than the
other two choices. Search works well. However, a significant amount of initial time is spent
on finding a good value for ρ, thus making the overall approach slow. Adap comes out to be
the best performer among the three choices. Similar observations hold for other datasets
and other choices of P. So, for ADMM, we will fix Adap as the way of choosing ρ and refer
to ADMM-Adap simply as ADMM.
It is also worth commenting on methods related to ADMM. Apart from ADMM, Bertsekas and Tsitsiklis (1997) discuss several other classic optimization methods for separable
convex programming, based on proximal and Augmented Lagrangian ideas which can be
used for distributed training of linear classifiers. ADMM represents the best of these methods. Also, Gauss-Seidel and Jacobi methods given in Bertsekas and Tsitsiklis (1997) are
related to feature partitioning, which is very different from the example partitioning scenario
studied in this paper. Therefore we do not consider these methods.
14
An efficient distributed learning algorithm
Time (in secs.)
0 1000 2000 3000 4000 5000
Log Rel. Func. Value Diff.
-5
-4
-3
-2
-1
0
ADMM-Adap
ADMM-Search
ADMM-Analytic
(a) kdd2010 - 8 nodes
Time (in secs.)
0 1000 2000 3000 4000 5000
Log Rel. Func. Value Diff.
-5
-4
-3
-2
-1
0
ADMM-Adap
ADMM-Search
ADMM-Analytic
(b) kdd2010 - 128 nodes
Figure 2: Plots showing the time efficiency of ADMM methods for kdd2010.
4.5. Study of CoCoA
In CoCoA (Jaggi et al., 2014) the key parameter is the approximation level of the inner
iterations used to solve each projected dual sub-problem. The number of epochs of coordinate dual ascent inner iterations plays a crucial role. We try the following choices for it:
0.1, 1 and 10.12 Figure 3 compares the progress of the objective function on kdd2010 for
two choices of nodes, P = 8 and P = 128. We find the choice of 1 epoch to work well
reasonably consistently over all the five datasets and varying number of nodes. So we fix
this choice and refer to the resulting method simply as CoCoA. Note in Figure 3 that the
(primal) objective function does not decrease continuously with time. This is because it is
a dual method and so monotone descent of the objective function is not assured.13
4.6. Study of DANE
In our detailed comparison study later in this section, we will also include another method
called DANE, which has some resemblance to FADL. This is the Newton based method
described in Sharir et al. (2014). Even though this method is very different in spirit from
our method, it uses a function approximation similar to our Nonlinear approximation idea;
we briefly discussed this in Subsection 3.2. DANE is a non-monotone method that is based
on fixed step sizes, with a probabilistic convergence theory to support it.
DANE has two parameters, µ and η in the function approximation; µ is the coefficient
for the proximal term and η is used for defining the direction. Sharir et al. (2014) do not
prove convergence for any possible choices of µ and η values; their practical recommendation
is to use µ = 3λ and η = 1. The choice of µ in particular, turns out to be quite sub-optimal.
We found that there was no single value of µ that is good for all datasets, and so it needs
to be tuned for each dataset. So, to improve DANE, we included an initial µ-tuning step.
12. The examples were randomly shuffled for presentation. When the number of epochs is a fraction, e.g.,
0.1, the inner iteration was stopped after 10% of the examples were presented.
13. The same comment holds for ADMM which is also a dual method; see for example, the jumps in objective
function values for ADMM in Figures 6 and 8 for kdd2010.
15
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Time (in secs.)
0 500 1000 1500 2000
Log Rel. Func. Value Diff.
-5
-4
-3
-2
-1
0
CoCoA0.1
CoCoA1
CoCoA10
(a) kdd2010 - 8 nodes
Time (in secs.)
0 500 1000 1500 2000
Log Rel. Func. Value Diff.
-5
-4
-3
-2
-1
0
CoCoA0.1
CoCoA1
CoCoA10
(b) kdd2010 - 128 nodes
Figure 3: Plots showing the time efficiency of CoCoA settings for kdd2010.
We start with µ = 3 ∗ λ and use an initial set of four outer iterations to choose the µ value
that gives the best improvement in objective function. Starting from µ = 3 ∗ λ we try
several values of µ in the direction of µ change that leads to improvement. After the first
four outer iterations, we fix µ at the chosen best value for the remaining iterations. (Note
that the cost associated with tuning µ has to be included in the overall cost of DANE.) As
we will see later in Subsection 4.9, in spite of all this tuning, DANE did not converge in
several situations. Essentially, the issue is that µ has to be adaptive - DANE needs to use
different choices of µ in the early, middle and end stages of one training. But then, this is
nearly akin to doing a kind of line search in each outer iteration. As opposed to DANE,
note that FADL is a monotone method directly based on line search, and it does not need
the proximal term to restrict step-sizes. Also, unlike DANE, all the parameters of FADL
are fixed to default values for all datasets.
The choice of inner optimizer for DANE is crucial for its efficiency. We tried
SVRG (Johnson and Zhang, 2013) and also TRON (Lin et al., 2008). Both did well, but
SVRG was better, especially for small number of nodes. So we did a more detailed study of
SVRG. We tried two choices. (a) Fix the number of epochs for the local (inner) optimization
to some good value, e.g., 10. (b) Always choose the number of epochs for the local (inner)
optimization to be such that the cost of local computation in a node is matched with the
communications cost. Choice (b) gave a better solution. So we have used it for DANE.
4.7. Study of FADL
Recall from Subsection 3.2 the various choices that we suggested for ˆfp. We are yet to
implement and study the BFGS approximation; we leave it out for future work. Our
original work (Mahajan et al., 2013a) focused on the Linear approximation, but we found
the Quadratic, Hybrid and Nonlinear choices to work much better. So we study only these
three methods. The implementation of these methods is as described in Subsection 3.4 and
Algorithm 2.
16
An efficient distributed learning algorithm
Time (in secs.)
0 500 1000 1500 2000 2500 3000
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
FADL-Quad
FADL-Hybrid
FADL-Nonlin
(a) kdd2010 - 8 nodes
Time (in secs.)
0 500 1000 1500 2000 2500 3000
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
FADL-Quad
FADL-Hybrid
FADL-Nonlin
(b) kdd2010 - 128 nodes
Figure 4: Plots showing the time efficiency of the three function approximations of FADL
for kdd2010.
Figure 4 compares the progress of the training objective function for various choices
of ˆfp. Among the choices, the quadratic approximation for ˆfp gives the best performance,
although the Hybrid and Nonlinear approximations also do quite well. We observe this
reasonably consistently in other datasets too. Hence, from the set of methods considered in
this subsection we choose FADL-Quadratic approximation as the only method for further
analysis, and simply refer to this method as FADL hereafter.
Why does the quadratic approximation do better than hybrid and nonlinear approximations? We do not have a precise answer to this question, but we give some arguments in
support. In each outer iteration, the function approximation idea is mainly used to get a
good direction. Recall from Subsection 3.2 that different choices use different approximations for L˜
p and Lˆ
p. Using the same “type” (meaning linear, nonlinear or quadratic) for
both, L˜
p and Lˆ
p is possibly better for direction finding. Second, the direction finding could
be more sensitive to the nonlinear approximation compared to the quadratic approximation;
this could become more severe as the number of nodes becomes larger. Literature shows
that quadratic approximations have good robustness properties; for example, subsampling
in Hessian computation (Byrd et al., 2012) doesn’t worsen direction finding much.
4.8. Comparison of FADL against DiSCO
DiSCO (Zhang and Xiao, 2015) is an inexact damped Newton method, where the inexact
Newton steps are computed using a distributed preconditioned conjugate gradient method.
Like TERA, it belongs to the SQM class (Chu et al., 2006). The DiSCO method as described
in Zhang and Xiao (2015) is not designed for squared hinge loss. Therefore, in this subsection
we separately compare FADL against DiSCO using the logistic loss. With communication
efficiency in mind we compare the two methods in terms of the number of communication
passes. Figure 5 gives the progress in objective function as a function of the number
of communication passes for the two methods. FADL is clearly better than DiSCO, the
17
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
difference being quite large on datasets such as kdd2010. In terms of total computing
time, the superiority of FADL turns out to be even better than what is seen in the plots
of communication passes. This is because DiSCO requires more extensive computational
steps than FADL within one communication pass.
4.9. Comparison of FADL against TERA, ADMM, CoCoA and DANE
Having made the best choice of settings for the methods, we now evaluate FADL against
TERA, ADMM, CoCoA and DANE in more detail. We do this using three sets of plots.
We give details only for P = 8 and P = 128 to give an idea of how performance varies for
small and large number of nodes.
1. Communication passes. We plot the variation of the training objective function as a
function of the number of communication passes. For the x-axis we prefer the number
of communication passes instead of the number of outer iterations since the latter
has a different meaning for different methods while the former is quite uniform for
all methods. Figures 6 and 7 give the plots respectively for the large dimensional
(m large) datasets (kdd2010, url and webspam) and medium/small dimensional (m
medium/small) datasets (mnist8m and rcv).
2. Time. We plot the variation of the training objective function as a function of the actual solution time. Figures 8 and 9 give the plots respectively for the large dimensional
and medium/small dimensional datasets.
3. Speed-up over TERA. TERA is an established strong baseline method. So it is useful
to ask how other methods fare relative to TERA and study this as a function of the
number of nodes. For doing this we need to represent each method by one or two real
numbers that indicate performance. Since generalization performance is finally the
quantity of interest, we stop a method when it reaches within 0.1% of the steady state
AUPRC value achieved by full, perfect training of (8) and record the following two
measures: the total number of communication passes and the total time taken. For
each measure, we plot the ratio of the measure’s value for TERA to the corresponding
measure’s value for a method, as a function of the number of nodes, and repeat this
for each method. Larger this ratio, better is a method; also, ratio greater than one
means a method is faster than TERA. Figures 10 and 11 give the plots for all the five
datasets.
Let us now use these plots to compare the methods. In the plots, DANE starts later than
other methods due to the extra work needed for tuning the proximal parameter µ.
4.9.1. Rate of Convergence
Analysis of the rate of convergence is better done by studying the behavior of the training
objective function with respect to the number of communication passes. So it is useful to
look at Figures 6 and 7. Clearly, as predicted by theory, the rate of convergence is linear
for all methods.
18
An efficient distributed learning algorithm
Communication Passes
0 10 20 30 40 50 60
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
DISCO (a) kdd2010 - 8 nodes
Communication Passes
0 20 40 60 80
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
DISCO (b) kdd2010 - 128 nodes
Communication Passes
0 10 20 30 40
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
DISCO
(c) url - 8 nodes
Communication Passes
0 10 20 30 40 50 60
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
DISCO
(d) url - 128 nodes
Communication Passes
0 10 20 30
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
DISCO
(e) webspam - 8 nodes
Communication Passes
0 10 20 30
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
DISCO
(f) webspam - 128 nodes
Figure 5: Plots comparing the number of communication passes of DiSCO and FADL on
three datasets.
19
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Communication Passes
0 20 40 60 80 100
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(a) kdd2010 - 8 nodes
Communication Passes
0 100 200 300 400 500
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(b) kdd2010 - 128 nodes
Communication Passes
0 20 40 60 80
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(c) url - 8 nodes
Communication Passes
0 100 200 300 400
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(d) url - 128 nodes
Communication Passes
0 10 20 30 40 50 60
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(e) webspam - 8 nodes
Communication Passes
0 50 100 150
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(f) webspam - 128 nodes
Figure 6: Plots showing the rate of convergence of various methods for the three high dimensional datasets.
20
An efficient distributed learning algorithm
Communication Passes
0 10 20 30 40 50
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(a) mnist8m - 8 nodes
Communication Passes
0 10 20 30 40 50
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(b) mnist8m - 128 nodes
Communication Passes
0 10 20 30 40 50
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(c) rcv - 8 nodes
Communication Passes
0 50 100 150 200
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(d) rcv - 128 nodes
Figure 7: Plots showing the linear convergence of various methods for the two low/medium
dimensional datasets.
TERA uses distributed computation only to compute the gradient and so the plots
should be unaffected by P. But in the plots we do see differences between the plots for
P = 8 and P = 128. This is because of their different initialization (average of one pass
SGD solutions from nodes): the initialization with lower number of nodes is better due
its smaller variance; note also the better starting objective function value at the start (left
most point) for P = 8.
For FADL, the rate is steeper for P = 8 than for P = 128. This steeper behavior
for lower number of nodes is expected because the functional approximation in each node
becomes better as the number of nodes decreases.
Recall from Section 1 that, our main aim behind the design of the function approximation based methods is to reduce the number of communication passes significantly. The
plots in Figures 6 and 7 clearly confirm such a reduction.
21
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Even though the end convergence rate of ADMM is slow, it generally shows good rates of
convergence in the initial stages of training. This is a useful behavior because generalization
measures such as AUPRC tend to achieve steady state values quickly in the early stages.
This usefulness is seen in Figure 10 too.
DANE has good rates of convergence when P is small (P = 8), but the cost associated
with tuning µ makes it poor; note that, if µ is not tuned, that will affect the rate of
convergence. DANE tends to be unstable for large P values.
Overall, FADL gives much better rates of convergence (both, in the early training stage
as well as in the end stage) compared to TERA, CoCoA, ADMM and DANE methods.14
FADL shows a large reduction in the number of communication passes over TERA, especially when the number of nodes is small. Against CoCoA the trend is the other way:
FADL needs a much smaller number of communication passes than CoCoA especially when
the number of nodes is large. These observations can also be seen from Figure 10. Clearly
CoCoA seems to be very slow with increasing number of nodes.
4.9.2. Time Taken
In the previous analysis we ignored computation costs within each iteration. But these
costs play a key role when we analyze overall efficiency in terms of the actual time taken.
We study this next. Figures 8 and 9 are relevant for this study. FADL, ADMM, CoCoA
and DANE involve much more extensive computations in the inner iterations than TERA;
this is especially true when the number of nodes is small because of the large amount of
local data in each node. TERA fares much better in the time analysis than what we saw
while studying using communication passes only. Thus the gap between TERA and other
methods becomes a lot smaller in the time analysis. Compare, for example, TERA and
ADMM with respect to communication passes and time. Although ADMM is much more
efficient than TERA with respect to the number of communication passes, TERA catches
up nicely with ADMM on the time taken.
CoCoA does well sometimes; for example, on kdd2010 and url, when the number of
nodes is small, say P = 8. But it is slow otherwise, especially when the number of nodes is
large.
FADL is uniformly better than ADMM with respect to the total time taken. Overall,
FADL shows the best performance, performing equally or much better than other methods in
different situations. With medium/low dimensional datasets (see Figure 9), communication
time is less of an issue and so the expectation is that FADL is less of value for them. Even
on these datasets, FADL does equally or better than TERA. DANE is not competitive due
to the time involved for tuning µ.
14. Here, it is useful to recall the comments made in Remark 2 of Section 3 on the goodness of the convergence
rate of FADL.
22
An efficient distributed learning algorithm
Time (in secs.)
0 1000 2000 3000 4000 5000 6000
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(a) kdd2010 - 8 nodes
Time (in secs.)
0 1000 2000 3000 4000 5000 6000
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(b) kdd2010 - 128 nodes
Time (in secs.)
0 100 200 300 400
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(c) url - 8 nodes
Time (in secs.)
0 100 200 300 400
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(d) url - 128 nodes
Time (in secs.)
0 200 400 600 800
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(e) webspam - 8 nodes
Time (in secs.)
0 200 400 600 800
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(f) webspam - 128 nodes
Figure 8: Plots showing the time efficiency of various methods for the three high dimensional
datasets.
23
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Time (in secs.)
0 20 40 60 80 100 120
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(a) mnist8m - 8 nodes
Time (in secs.)
0 20 40 60 80 100 120
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(b) mnist8m - 128 nodes
Time (in secs.)
0 5 10 15 20 25 30
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(c) rcv - 8 nodes
Time (in secs.)
0 5 10 15 20 25 30
Log Rel. Func. Value Diff.
-7
-6
-5
-4
-3
-2
-1
0
FADL
CoCoA
TERA
ADMM
DANE
(d) rcv - 128 nodes
Figure 9: Plots showing the time efficiency of various methods for the two low/medium
dimensional datasets. For rcv and mnist8m - 8 nodes, the plot of DANE is invisible
since the time taken for tuning µ is larger than the time window displayed.
4.9.3. Relative performance of the methods
Figure 11 is relevant for this study. CoCoA shows impressive speed-up over TERA on
kdd2010 but it is much slower on all the other datasets. It is unclear why CoCoA fares so
well on kdd2010 but not on the other datasets. ADMM gives an overall decent performance
when compared to TERA. FADL is consistently faster than TERA, with speed-ups ranging
anywhere from 1-10. In communication-heavy scenarios where reducing the number of
communication passes is most important, methods such as FADL and ADMM have great
value (see Figure 10), with the possibility of getting even higher speed-ups over TERA.
Except for kdd2010 for which FADL is slower than CoCoA for small number of nodes, it is
generally the fastest method. DANE does not seem to have any special scenario where it is
better than others.
24
An efficient distributed learning algorithm
Number of Nodes
20 40 60 80 100 120
TERA Comm. Pass / Comm. Pass 0
10
20
30
40
50
60 FADL
CoCoA
TERA
ADMM
DANE
(a) kdd2010
Number of Nodes
20 40 60 80 100 120
TERA Comm. Pass / Comm. Pass 0
5
10
15
FADL
CoCoA
TERA
ADMM
DANE
(b) url
Number of Nodes
20 40 60 80 100 120
TERA Comm. Pass / Comm. Pass 0
5
10
15
FADL
CoCoA
TERA
ADMM
DANE
(c) webspam
Number of Nodes
20 40 60 80 100 120
TERA Comm. Pass / Comm. Pass 0
2
4
6
8
10
12
14
FADL
CoCoA
TERA
ADMM
DANE
(d) mnist8m
Number of Nodes
20 40 60 80 100 120
TERA Comm. Pass / Comm. Pass 0
1
2
3
4
5
6 FADL
CoCoA
TERA
ADMM
DANE
(e) rcv
Figure 10: Plots showing communication passes (relative to TERA) as a function of the
number of nodes. Each method was terminated when it reached within 0.1% of
the steady state AUPRC value achieved by full, perfect training of (8). For rcv,
the FADL and ADMM curves coincide.
25
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Number of Nodes
20 40 60 80 100 120
TERA Time / Time
0
5
10
15
FADL
CoCoA
TERA
ADMM
DANE
(a) kdd2010
Number of Nodes
20 40 60 80 100 120
TERA Time / Time
0
0.5
1
1.5
2
2.5
3
3.5 FADL
CoCoA
TERA
ADMM
DANE
(b) url
Number of Nodes
20 40 60 80 100 120
TERA Time / Time
0
0.5
1
1.5
2
2.5
3
FADL
CoCoA
TERA
ADMM
DANE
(c) webspam
Number of Nodes
20 40 60 80 100 120
TERA Time / Time
0
1
2
3
4 FADL
CoCoA
TERA
ADMM
DANE
(d) mnist8m
Number of Nodes
20 40 60 80 100 120
TERA Time / Time
0
0.5
1
1.5
2
FADL
CoCoA
TERA
ADMM
DANE
(e) rcv
Figure 11: Plots showing time (relative to TERA) as a function of the number of nodes.
Each method was terminated when it reached within 0.1% of the steady state
AUPRC value achieved by full, perfect training of (8).
26
An efficient distributed learning algorithm
4.9.4. Speed-up as a function of P
Let us revisit Figure 8 and look at the plots corresponding to kdd2010 for FADL.15 It can
be observed that the time needed for reaching a certain tolerance, say Log Rel. Func. Value
Diff. = -3, is two times smaller for P = 8 than for P = 128. This means that using a large
number of nodes is not useful, which prompts the question: Is a distributed solution really
necessary? There are two answers to this question. First, as we already mentioned, when
the training data is huge16 and the data is generated and forced to reside in distributed nodes
(moving data between machines is not an efficient option), the right question to ask is not
whether we get great speed-up, but to ask which method is the fastest. Second, for a given
dataset and method, if the time taken to reach a certain approximate stopping tolerance
(e.g., based on AUPRC) is plotted as a function of P, it usually has a minimum at a value
P > 1. Given this, it is appropriate to choose a P optimally to minimize training time. A
large fraction of Big data machine learning applications involve periodically repeated model
training involving newly added data. For example, in Advertising, logistic regression based
click probability models are retrained on a daily basis on incrementally varying datasets.
In such scenarios it is worthwhile to spend time to tune P in an early deployment phase to
minimize time, and then use this choice of P for future runs.
4.9.5. Computation and Communication Costs
Table 2 shows the ratio of computational cost to communication cost for the three high
dimensional datasets for all the methods.17 Note that the ratio is small for TERA and
so communication cost dominates the time for it. On the other hand, both the costs are
well balanced for FADL. Note that ratio varies in the range of 0.625 − 2.845. This clearly
shows that FADL trades-off computation with communication, while significantly reducing
the number of communication passes (Figures 6 and 7) and time (Figures 8 and 9).
FADL CoCoA TERA ADMM
kdd2010 1.6333 0.1416 0.1422 1.8499
url 1.3650 0.1040 0.2986 3.4886
webspam 1.2082 0.1570 0.2423 1.2543
Table 2: Ratio of the total computation cost to the total communication cost for various
methods which were terminated when AUPRC reached within 0.1% of the AUPRC
value for 128 nodes.
15. We choose FADL as an example, but the comments made in the discussion apply to other methods too.
16. The datasets, kdd2010, url and webspam are really not huge in the Big data sense. In this paper we used
them only because of lack of availability of much bigger public datasets.
17. For the medium/low dimensional datasets rcv and mnist8m, communication latencies, line search cost
etc. also play a key role and an analysis of computation cost versus communication cost does not provide
any great insight.
27
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
4.10. Experiment on a much larger dataset
To verify the goodness of FADL, we also did an experiment evaluating FADL against other
methods, on a large dataset that is more than an order of magnitude bigger than the largest
dataset in Table 1. The dataset is the Splice site recognition dataset from the bioinformatics
domain (Sonnenburg and Franc, 2010). In this dataset each example is a sequence; we
considered all positional features upto 9 grams. This led to a dataset of 49 million features
and 50 million examples. The size of the dataset is larger than 0.65 Terabytes. We employed
a cluster of 100 nodes to solve this problem. Figure 12 compares the various methods on
(a) the reduction of the objective function as a function of communication passes; (b)
the reduction of the objective function as a function of time; and (c) the improvement of
generalization performance (AUPRC) as a function of time. It is clear that FADL makes
great reductions over other methods, on the number of communication passes. FADL is
also the best performer when we measure by the time taken. On clusters with slower
communication speeds and iteration set-up times, the value of FADL over other methods
will be even higher. Interestingly, CoCoA comes out as the next best performer. CoCoA
has slower end convergence than FADL, but it shows up equally well in plot (c) on the
improvement of generalization performance. As we saw earlier with other datasets, the
value of CoCoA varies a lot; for the current scenario of solving the splice site recognition
on 100 nodes it seems to be well-suited. FADL, on the other hand, is uniformly good in
varying scenarios of several datasets, number of nodes etc.
4.11. Summary
It is useful to summarize the findings of the empirical study.
• FADL gives a great reduction in the number of communication passes, making it
clearly superior to other methods in communication heavy settings.
• In spite of higher computational costs per iteration FADL shows the overall best performance on the total time taken. This is true even for medium and low dimensional
datasets.
• FADL shows a speed-up of 1-10 over TERA, the actual speed-up depending on the
dataset and the setting.
• FADL nicely balances computation and communication costs.
5. Discussion
In this section, we discuss briefly, other different distributed settings made possible by our
algorithm. The aim is to show the flexibility and generality of our approach while ensuring
glrc.
Section 3 considered example partitioning where examples are distributed across the
nodes. First, it is worth mentioning that, due to the gradient consistency condition, partitioning is not a necessary constraint; our theory allows examples to be resampled, i.e., each
example is allowed to be a part of any number of nodes arbitrarily. For example, to reduce
the number of outer iterations, it helps to have more examples in each node.
28
An efficient distributed learning algorithm
Communication Passes
0 50 100 150 200 250 300
Log Rel. Func. Value Diff.
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0
FADL
CoCoA
ADMM
TERA
DANE
(a) Communication efficiency (objective function)
Time(in secs.)
0 2000 4000 6000 8000
Log Rel. Func. Value Diff.
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
0
FADL
CoCoA
ADMM
TERA
DANE
(b) Time efficiency (objective function)
Time(in secs.)
0 1000 2000 3000 4000 5000
AUPRC
0.46
0.48
0.5
0.52
0.54
0.56
FADL
CoCoA
ADMM
TERA
DANE
(c) Time efficiency (AUPRC)
Figure 12: Plots comparing various methods on the Splice site recognition dataset.
Second, the theory proposed in Section 3 holds for feature partitioning also. Suppose,
in each node p we restrict ourselves to a subset of features, Jp ⊂ {1, . . . , d}, i.e., include the
constraint, wp ∈ {w : w(j) = w
r
(j) ∀r 6∈ Jp}, where w(j) denotes the weight of the j
th
feature. Note that we do not need {Jp} to form a partition. This is useful since important
features can be included in all the nodes.
Gradient sub-consistency. Given w
r and Jp we say that ˆfp(w) has gradient subconsistency with f at w
r on Jp if ∂f
∂w(j)
(w
r
) = ∂fˆ
∂w(j)
(w
r
) ∀ j ∈ Jp.
Under the above condition, we can modify the algorithm proposed in Section 3 to come
up with a feature decomposition algorithm with glrc.
Several feature decomposition based approaches (Richt´arik and Tak´ac, 2012; Patriksson,
1998b) have been proposed in the literature. The one closest to our method is the work
by Patrikkson on a synchronized parallel algorithm (Patriksson, 1998b) which extends a
generic cost approximation algorithm (Patriksson, 1998a) that is similar to our functional
approximation. The sub-problems on the partitions are solved in parallel. Although the
objective function is not assumed to be convex, the cost approximation is required to satisfy
29
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
a monotone property, implying that the approximation is convex. The algorithm only has
asymptotic linear rate of convergence and it requires the feature partitions to be disjoint.
In contrast, our method has glrc and works even if features overlap in partitions. Moreover,
there does not exist any counterpart of our example partitioning based distributed algorithm
discussed in Section 3.
Recently Mairal (2013) has developed an algorithm called MISO. The main idea of MISO
(which is in the spirit of the EM algorithm) is to build majorization approximations with
good properties so that line search can be avoided, which is interesting. MISO is a serial
method. Developing a distributed version of MISO is an interesting future direction; but,
given that line search is inexpensive communication-wise, it is unclear if such a method
would give great benefits.
Our approach can be easily generalized to joint example-feature partitioning as well as
non-convex settings.18 The exact details of all the extensions mentioned above and related
experiments are left for future work.
Recently, a powerful divide and conquer approach Hsieh et al. (2014) has been suggested
for training kernel methods. The idea is to partition the input space such that the restrictions of training on the partitioned input spaces are as decoupled as possible. If, in FADL,
we had the ability to choose the parts of data that are placed in the nodes, then we would
also gain by choosing decoupled partitions. However, in the distributed case, this requires
pre-processing as well as shuffling of data, which are expensive.
6. Conclusion
To conclude, we have proposed FADL, a novel functional approximation based distributed
algorithm with provable global linear rate of convergence. The algorithm is general and
flexible in the sense of allowing different local approximations at the node level, different
algorithms for optimizing the local approximation, early stopping and general data usage
in the nodes. We also established the superior efficiency of FADL by evaluating it against
key existing distributed methods. We believe that FADL has great potential for solving
machine learning problems arising in Big data.
Appendix A: Complexity analysis
Let us use the notations of section 3 given around (21). We define the overall cost of any
distributed algorithm as
[(c1
nz
P
+ c2m)T
inner + c3γm]T
outer
, (22)
where T
outer is the number of outer iterations, T
inner is the number of inner iterations at
each node before communication happens and c1 and c2 denote the number of passes over
the data and m-dimensional dot products per inner iteration respectively. For communication, we assume an AllReduce binary tree as described in Agarwal et al. (2011) with
18. For non-convex settings glrc is hard to establish, but proving a simpler convergence theory is quite
possible.
30
An efficient distributed learning algorithm
pipelining. As a result, we do not have a multiplicative factor of log2P in our cost19
. γ
is the relative computation to communication speed in the given distributed system; more
precisely, it is the ratio of the times associated with communicating a floating point number
and performing one floating point operation; γ is usually much larger than 1. c3 is the
number of m-dimensional vectors (gradients, Hessian-vector computations etc.) we need to
communicate.
Method c1 c2 c3 T
inner
SQM 2 ≈ 5 − 10 1 1
FADL 2 ≈ 5 − 7 2 ˆk
Table 3: Value of cost parameters
The values of different parameters for SQM and FADL are given in Table 3. T
outer
SQM is
the number of overall conjugate gradient iterations plus gradient computations. ˆk is the
average number of conjugate gradient iterations (for the inner minimization of ˆfp using
TRON) required per outer iteration in FADL. Typically ˆk is between 5 and 20.
Since dense dot products are extremely fast c2m is small compared to c1nz/P for both
the approaches, we ignore it from (22) for simplicity. Now for FADL to have lesser cost
than TERA, we can use (22) to get the condition,
2.0(ˆkTouter
FADL − T
outer
SQM )
nz
P
≤ (T
outer
SQM − 2T
outer
FADL)γm (23)
Let us ignore T
outer
SQM on the left side of this inequality (in favor of SQM) and rearrange to
get the looser condition,
nz
m
≤
γP
ˆk
1
2.0
(
T
outer
SQM
T
outer
FADL
− 2) (24)
Assuming T
outer
SQM > 3.0T
outer
FADL, we arrive at the final condition in (21).
Appendix B: Proofs
Proofs of the results in section 2
Let us now consider the establishment of the convergence theory given in section 2.
Proof of Lemma 1. Let ρ(t) = f(w
r + tdr
) and γ(t) = ρ(t) − ρ(0) − αtρ0
(0). Note
the following connections with quantities involved in Lemma 1: ρ(t) = f
r+1
, ρ(0) = f
r
,
ρ
0
(t) = g
r+1
· d
r and γ(t) = f
r+1 − f
r − αgr
·(w
r+1 − w
r
). (4) corresponds to the condition
γ(t) ≤ 0 and (5) corresponds to the condition ρ
0
(t) ≥ βρ0
(0).
γ
0
(t) = ρ
0
(t) − αρ0
(0). ρ
0
(0) < 0. ρ
0
is strictly monotone increasing because, by assumption A2,
ρ
0
(t) − ρ
0
(t˜) ≥ σ(t − t˜)kd
r
k
2 ∀ t,t˜ (25)
This implies that γ
0
is also strictly monotone increasing and, all four, ρ, ρ
0
, γ
0 and γ tend
to infinity as t tends to infinity.
19. Actually, there is another communication term, γb log2P, where b is the size of first block of communicated doubles in the pipeline. However, typically b  m and hence we ignore it.
31
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
Let tβ be the point at which ρ
0
(t) = βρ0
(0). Since ρ
0
(0) < 0 and ρ
0
is strictly monotone
increasing, tβ is unique and tβ > 0. This validates the definition in (6). Monotonicity of ρ
0
implies that (5) is satisfied iff t ≥ tβ.
Note that γ(0) = 0 and γ
0
(0) < 0. Also, since γ
0
is monotone increasing and γ(t) → ∞
as t → ∞, there exists a unique tα > 0 such that γ(tα) = 0, which validates the definition
in (7). It is easily checked that γ(t) ≤ 0 iff t ∈ [0, tα].
The properties also imply γ
0
(tα) > 0, which means ρ
0
(tα) ≥ αρ0
(0). By the monotonicity
of ρ
0 we get tα > tβ, proving the lemma.
Proof of Theorem 2. Using (5) and A1,
(β − 1)g
r
· d
r ≤ (g
r+1 − g
r
) · d
r ≤ Ltkd
r
k
2
(26)
This gives a lower bound on t:
t ≥
(1 − β)
Lkd
rk
2
(−g
r
· d
r
) (27)
Using (4), (27) and (1) we get
f
r+1 ≤ f
r + αtgr
· d
r ≤ f
r −
α(1 − β)
Lkd
rk
2
(−g
r
· d
r
)
2 ≤ f
r −
α(1 − β)
L
cos2
θkg
r
k
2
(28)
Subtracting f
? gives
(f
r+1 − f
?
) ≤ (f
r − f
?
) −
α(1 − β)
L
cos2
θkg
r
k
2
(29)
A2 together with g(w
?
) = 0 implies kg
rk
2 ≥ σ
2kw
r − w
?k
2
. Also A1 implies f
r − f
? ≤
L
2
kw
r − w
?k
2 Smola and Vishwanathan (2008). Using these in (29) gives
(f
r+1 − f
?
) ≤ (f
r − f
?
) − 2α(1 − β)
σ
2
L2
cos2
θ(f
r − f
?
)
≤ (1 − 2α(1 − β)
σ
2
L2
cos2
θ)(f
r − f
?
) (30)
Let δ = (1 − 2α(1 − β)
σ
2
L2 cos2
θ). Clearly 0 < δ < 1. Theorem 2 follows.
Proofs of the results in section 3
Let us now consider the establishment of the convergence theory given in section 3. We
begin by establishing that the exact minimizer of ˆfp makes a sufficient angle of descent at
w
r
.
Lemma 5. Let ˆw
?
p be the minimizer of ˆfp. Let dp = ( ˆw
?
p − w
r
). Then
−g
r
· dp ≥ (σ/L)kg
r
kkdpk (31)
Proof. First note, using gradient consistency and ∇fp( ˆw
?
p
) = 0 that
kg
r
k = k∇ ˆfp(w
r
) − ∇ ˆfp( ˆw
?
p
)k ≤ Lkdpk (32)
32
An efficient distributed learning algorithm
Now,
−g
r
· dp = (∇ ˆfp(w
r
) − ∇ ˆfp( ˆw
?
p
))T
(w
r − wˆ
?
p
) ≥ σkdpk
2 = σkg
rkkdpk
kdpk
kg
rk ≥
σ
L
kg
rkkdpk(33)
where the second line comes from σ-strong convexity and the fourth line follows from (32).
Proof of Lemma 3. Let us now turn to the question of approximate stopping and establish
Lemma 3. Given θ satisfying (18) let us choose ζ ∈ (0, 1) such that
π
2
> θ > cos−1 σ
L
+ cos−1
ζ (34)
Figure 13: Construction used in the proof of Lemma 3.
By A3 and equations (3.16) and (3.22) in Smola and Vishwanathan (2008), we get
σ
2
kv − wˆ
?
pk
2 ≤ ˆfp(v) − ˆf
?
p ≤
L
2
kv − wˆ
?
pk
2
(35)
After k iterations we have
ˆfp(v
k
) − ˆf
?
p ≤ δ
k
(
ˆfp(w
r
) − ˆf
?
p
) (36)
We can use these to get
kv
k − wˆ
?
pk
2 ≤
2( ˆfp(v
k
) − ˆf
?
p
)
σ
≤
2δ
k
(
ˆfp(w
r
) − ˆf
?
p
)
σ
≤
δ
kL
σ
kw
r − wˆ
?
pk
2 def = (r
k
)
2
(37)
33
Mahajan, Agrawal, Keerthi, Sellamanickam and Bottou
For now let us assume the following:
kv
k − wˆ
?
pk
2 ≤ kw
r − wˆ
?
pk
2
(38)
Using (37) note that (38) holds if
δ
kL
σ
≤ 1 (39)
Let S
k be the sphere, S
k = {v : kv −wˆ
?
pk
2 ≤ (r
k
)
2}. By (37) we have v
k ∈ S
k
. See Figure 6.
Therefore,
φ
k ≤ max
v∈Sk
φ(v) (40)
where φ
k
is the angle between ˆw
?
p−w
r and v
k−w
r
, and φ(v) is the angle between v−w
r and
wˆ
?
p−w
r
. Given the simple geometry, it is easy to see that maxv∈Sk φ(v) is attained by a point
vˆ lying on the boundary of S
k
(i.e., kvˆ − wˆ
?
pk
2 = (r
k
)
2
) and satisfying (ˆv − wˆ
?
p
) ⊥ (ˆv − w
r
).
This geometry yields
cos2 φ(ˆv) = kvˆ − w
rk
2
kwˆ
?
p − wrk
2
=
kwˆ
?
p − w
rk
2 − (r
k
)
2
kwˆ
?
p − wrk
2
= 1 −
(r
k
)
2
kwˆ
?
p − wrk
2
= 1 −
δ
kL
σ
(41)
Since φ
k ≤ φ(ˆv),
cos2 φ
k ≥ 1 −
δ
kL
σ
(42)
Thus, if
1 −
δ
kL
σ
≥ ζ
2
(43)
then
cos φ
k ≥ ζ ∀k ≥ ˆk (44)
holds. By (34) this yields −g
r
, vk − w
r ≤ θ, the result needed in Lemma 3. Since ζ > 0,
(43) implies (39), so (38) holds and there is no need to separately satisfy it. Now (43) holds
if
k ≥ ˆk
def =
log(L/(σ(1 − ζ
2
)))
log(1/δ)
(45)
which proves the lemma.
Proof of Theorem 4. It trivially follows from a combination of Lemma 3 and Theorem
2.