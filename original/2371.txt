k nearest neighbor (kNN) method is a popular classification method in data mining and statistics because of its simple implementation and significant classification performance. However, it is impractical for traditional kNN methods to assign a fixed k value (even though set by experts) to all test samples. Previous solutions assign different k values to different test samples by the cross validation method but are usually time-consuming. This paper proposes a kTree method to learn different optimal k values for different test/new samples, by involving a training stage in the kNN classification. Specifically, in the training stage, kTree method first learns optimal k values for all training samples by a new sparse reconstruction model, and then constructs a decision tree (namely, kTree) using training samples and the learned optimal k values. In the test stage, the kTree fast outputs the optimal k value for each test sample, and then, the kNN classification can be conducted using the learned optimal k value and all training samples. As a result, the proposed kTree method has a similar running cost but higher classification accuracy, compared with traditional kNN methods, which assign a fixed k value to all test samples. Moreover, the proposed kTree method needs less running cost but achieves similar classification accuracy, compared with the newly kNN methods, which assign different k values to different test samples. This paper further proposes an improvement version of kTree method (namely, k*Tree method) to speed its test stage by extra storing the information of the training samples in the leaf nodes of kTree, such as the training samples located in the leaf nodes, their kNNs, and the nearest neighbor of these kNNs. We call the resulting decision tree as k*Tree, which enables to conduct kNN classification using a subset of the training samples in the leaf nodes rather than all training samples used in the newly
SECTION I.Introduction
Different from model-based methods which first learn models from training samples and then predict test samples with the learned model [1]–[2][3][4][5][6], the model-free k nearest neighbors (kNNs) method does not have training stage and conducts classification tasks by first calculating the distance between the test sample and all training samples to obtain its nearest neighbors and then conducting kNN classification1 (which assigns the test samples with labels by the majority rule on the labels of selected nearest neighbors). Because of its simple implementation and significant classification performance, kNN method is a very popular method in data mining and statistics and thus was voted as one of top ten data mining algorithms [7]–[8][9][10][11][12][13].

Previous kNN methods include: 1) assigning an optimal k value with a fixed expert-predefined value for all test samples [14]–[15][16][17][18][19] and 2) assigning different optimal k values for different test samples [18], [20], [21]. For example, Lall and Sharma [19] indicated that the fixed optimal-k -value for all test samples should be k=n−−√ (where n>100 and n is the number of training samples), while Zhu et al. [21] proposed to select different optimal k values for test samples via tenfold cross validation method. However, the traditional kNN method, which assigns fixed kNNs to all test samples (fixed kNN methods for short), has been shown to be impractical in real applications. As a consequence, setting an optimal-k -value for each test sample to conduct kNN classification (varied kNN methods for short) has been becoming a very interesting research topic in data mining and machine learning [22]–[23][24][25][26][27][28][29].

A lot of efforts have been focused on the varied kNN methods, which efficiently set different optimal-k -values to different samples [20], [30], [31]. For example, Li et al. [32] proposed to use different numbers of nearest neighbors for different categories and Sahigara et al. [33] proposed to employ the Monte Carlo validation method to select an optimal smoothing parameter k fore each test sample. Recently, Cheng et al. [20] proposed a sparse-based kNN method to learn an optimal-k -value for each test sample and Zhang et al. [30] studied the kNN method by learning a suitable k value for each test sample based on a reconstruction framework [34]. Previous varied kNN methods usually first learn an individual optimal-k -value for each test sample and then employ the traditional kNN classification (i.e., majority rule on k training samples) to predict test samples by the learned optimal-k -value. However, either the process of learning an optimal-k -value for each test sample or the process of scanning all training samples for finding nearest neighbors of each test sample is time-consuming. Therefore, it is challenging for simultaneously addressing these issues of kNN method, i.e., optimal-k -values learning for different samples, time cost reduction, and performance improvement.

To address aforementioned issues of kNN methods, in this paper, we first propose a kTree2 method for fast learning an optimal-k -value for each test sample, by adding a training stage into the traditional kNN method and thus outputting a training model, i.e., building a decision tree (namely, kTree) to predict the optimal-k -values for all test samples. Specifically, in the training stage, we first propose to reconstruct each training sample by all training samples via designing a sparse-based reconstruction model, which outputs an optimal-k -value for each training sample. We then construct a decision tree using training samples and their corresponding optimal-k -values, i.e., regarding the learned optimal-k -value of each training sample as the label. The training stage is offline and each leaf node stores an optimal-k -value in the constructed kTree. In the test stage, given a test sample, we first search for the constructed kTree (i.e., the learning model) from the root node to a leaf node, whose optimal-k -value is assigned to this test sample so that using traditional kNN classification to assign it with a label by the majority rule.

There are two distinguished differences between the previous kNN methods [20], [30] and our proposed kTree method. First, the previous kNN methods (e.g., fixed kNN methods and varied kNN methods) have no training stage, while our kTree method has a sparse-based training stage, whose time complexity is O(n2) (where n is the sample size). It is noteworthy that the training stage of our kTree method is offline. Second, even though both the varied kNN methods and our proposed kTree method (which can be regarded as one of varied kNN methods) first search the optimal-k -values and then conduct traditional kNN classification to classify the test sample with the learned optimal-k -values, the previous methods need at least O(n2) time complexity to obtain the optimal-k -values due to involving a sparse-based learning process, while our kTree method only needs O(log(d)+n) (where d is the dimensions of the features) to do that via the learned model, i.e., the kTree. It is also noteworthy that the process of traditional fixed kNN method assigning a fixed k value to all test samples needs at least O(n2d) via scanning all training samples for each test sample.

Although our kTree method enables to obtain optimal-k -values for test samples, it still needs to scan all training samples to conduct kNN classification, which is also a time-consuming process, i.e., at least O(n2d) . We further extend our proposed kTree method to its improvement version (namely, k*Tree method) to speed test stage, by only storing extra information of training samples in the left nodes, such as the training samples, their kNNs, and the nearest neighbors of these nearest neighbors. We call the resulting decision tree as k*Tree. That is, there is one difference between the kTree method and the k*Tree method in the training stage, i.e., the optimal-k -values in the leaf nodes for the kTree, while the optimal-k -values and the information of training samples for the k*Tree. In the test stage, given a test sample, the k*Tree outputs its optimal-k -value and the information of its nearest neighbors in this leaf node, so the traditional kNN classification is conducted using the optimal-k -value and a subset of training samples in the left node (i.e., kNNs of the nearest neighbor of the test sample and their corresponding nearest neighbors of these kNNs). In this way, the number of used training samples s is less than the sample size n , i.e., s≪n , thus reducing the running cost of test stage.

The rest of this paper is organized as follows. We briefly recall the state-of-the-art kNN methods and describe the detail of the proposed method, respectively, in Sections II and III. We then analyze our experimental results in Section IV and give our conclusion in Section V.

SECTION II.Related Work
While kNN method enables to output remarkable performance and has been proved to approximately achieve to the error rate of Bayes optimization under very mild conditions, it has widely been applied to many kinds of applications, such as regression, classification, and missing value imputation [35]–[36][37][38][39][40][41]. The performance of kNN method can be affected by a lot of issues, such as the selection of the k value and the selection of distance measures. To address these issues, a large amount of machine learning techniques have been developed.

Previous study of kNN method mainly focused on searching for an optimal-k -value for all test samples. For example, Zhang et al. [42] incorporated certainty factor measure to conduct kNN classification with a fixed k value for all samples [43], while Song et al. [44] proposed to select a subset of most informative sample from neighborhoods. Vincent and Bengio [45] designed a k -local hyperplane distance and Wang et al. [24] defined a new similarity between two data points [46] for conducting kNN classification. Recently, Liu et al. [47] proposed an enhanced fuzzy kNN method to adaptively specify the optimal-k -values by the particle swarm optimization approach. Gou et al. [48] developed a dual weighted voting scheme for kNN to overcome the sensitivity of the optimal-k -values. Premachandran and Kakarala [49] proposed to select a robust neighborhood using the consensus of multiple rounds of kNNs.

As it is impractical for applying for a fixed k value for all test samples in data mining and machine learning, a number of efforts have been focused on designing different k values for different samples. For example, Li et al. [16] demonstrated to use different numbers of nearest neighbors for different categories, rather than a fixed number across all categories. Góra and Wojna [15] proposed to combine two widely used empirical approaches, i.e., rule induction and instance-based learning, respectively, to learn the optimal-k -values. Guo et al. [50] proposed to construct a kNN model to automatically determine the optimal-k -value for each sample. Based on statistical confidence, Wang et al. [18] proposed to locally adjust the number of nearest neighbors. Manocha and Girolami [51] proposed a probabilistic nearest neighbor method for inferring the number of neighbors, i.e., optimal-k -values. Sun and Huang [52] also proposed an adaptive kNN algorithm, for each test sample, by setting the optimal-k -value as the optimal k of its nearest neighbor in the training set.

Although the above methods solved the fixed k value problem, their complexity is high for learning the optimal-k -value for each test sample.

SECTION III.Approach
A. Denotations
In this paper, we denote the matrix, the vector, and the scalar, respectively, as a boldface uppercase letter, a boldface lowercase letter, and a normal italic letter. For a matrix X=[xij] , its i th row and j th column are denoted as xi and xj , respectively. We denote the Frobenius norms of X , ℓ2− norm, ℓ1− norm, and ℓ21− norm, respectively, as ||X||F=(∑j||xj||22)1/2 (matrix norms here are entrywise norm), ||X||2=(∑i∑j|xij|2)1/2 , ||X||1=∑i∑j|xij| , and ||X||21=∑i(∑jx2ij)1/2 . We further denote the transpose operator, the trace operator, and the inverse of a matrix X , respectively, as XT , tr(X) and X−1 .

B. Framework
In this section, we describe the proposed kTree method and k*Tree method in detail. Specifically, we first interpret the reconstruction process to learn the optimal-k -values for training samples in Section III-C. We then describe the kTree method and the k*Tree method, respectively, in Sections III-D and III-E. Figs. 1 and 2 illustrate the flowcharts of the proposed methods.

Fig. 1. - Flowchart of the proposed kTree method.
Fig. 1.
Flowchart of the proposed kTree method.

Show All

Fig. 2. - Flowchart of the proposed k*Tree method.
Fig. 2.
Flowchart of the proposed k*Tree method.

Show All

C. Reconstruction
Denote by training samples X∈Rd×n=[x1,…,xn] , where n and d , respectively, represent the number of training samples and features, in this section, we design to use training samples X to reconstruct themselves, i.e., reconstruct each training sample xi , with the goal that the distance between Xwi and xi (where wi∈Rn denotes the reconstruction coefficient matrix) is as small as possible. To do this, we use a least square loss function [53] as follows:
minW∑i=1n||Xwi−xi||22=minW||XW−X||2F(1)
View SourceRight-click on figure for MathML and additional features.where W=[w1,…,wn]∈Rn×n denotes the reconstruction coefficient or the correlations between training samples and themselves.

In real applications, an ℓ2− norm regularization term is often added into (1) for avoiding the issue of singularity of XTX , that is
minW||XW−X||2F+ρ||W||22(2)
View SourceRight-click on figure for MathML and additional features.where ||W||2 is an ℓ2− norm regularization term and ρ is a tuning parameter. Usually, (2) is called ridge regres-sion [23], [31] with a close solution W=(XTX+ρI)−1XTX . However, (2) does not output sparse results. In this paper, we expect to generate the sparse reconstruction coefficient (i.e., W ) to select parts of training samples to represent each test sample. Following previous literature [34], [54], we employ the following sparse objective function:
minW||XW−X||2F+ρ1||W||1,W⪰0(3)
View SourceRight-click on figure for MathML and additional features.where ||W||1 is an ℓ1− norm regularization term [55] and W⪰0 means that each element of W is nonnegative. Equation (3) has been proved to result in sparse W [2], [56] and is also called the least absolute shrinkage and selection operator [9], [53]. Moreover, (3) generates the elementwise sparsity, i.e., irregular sparsity in the elements of W . The larger the value of ρ1 , the more sparse the W .

Since we use training samples to reconstruct themselves, it is natural to expect that there exist relations among features or samples. Generally, if two features are highly related to each other, then it is reasonable to have the corresponding predictions also related [43], [53]. To this end, we devise a regularization term with the assumption that, if some features, e.g., xi and xj are involved in regressing, then the corresponding predictions are also related to each other. Thus, their corresponding predictions (i.e., yi=xiW and yj=xjW ) should have the same or similar relation. To utilize such relation, we penalize the loss function with the similarity between yi and yj . Specifically, we impose the relation between two training samples in X to be reflected in the relation between their predictions by defining the following embedding function [43]:
12∑i,jdsij∥xiW−xjW∥22(4)
View SourceRight-click on figure for MathML and additional features.where sij denotes an element in the feature similarity matrix S=[sij]∈Rd×d which encodes the relation between feature vectors. With respect to the similarity measure between vectors of a and b , throughout this paper, we first use a radial basis function kernel as defined as follows:
f(a,b)=exp(−∥a−b∥222σ2)(5)
View SourceRight-click on figure for MathML and additional features.where σ denotes a kernel width. As for the similarity matrix S , we first construct a data adjacency graph by regarding each feature as a node and using kNNs along with a heat kernel function defined in (5) to compute the edge weights, i.e., similarities. For example, if a feature xj is selected as one of the kNNs of a feature xi , then the similarity sij between these two features or nodes is set to the value of f(xi,xj) ; otherwise, their similarity is set to zero, i.e., sij=0 [43], [53], [57].

After simple mathematical transformation, we obtain the following regularization term:
R(W)=Tr(WTXTLXW)(6)
View SourceRight-click on figure for MathML and additional features.where L∈Rd×d is a Laplacian matrix. We should note that the definition of L is different from [43] and [58], whose Laplacian matrix indicates the relational information between samples, while our Laplacian matrix indicates the relational information between features, which has been successfully used in many manifold learning methods [50], [57], [58]. Finally, we define the final objective function for the reconstruction process as follows:
minW||XW−X||2F+ρ1||W||1+ρ2R(W),W⪰0.(7)
View SourceRight-click on figure for MathML and additional features.

Equation (7) sequentially includes the least square loss function, the ℓ1 -norm regularization term, the graph Laplacian regularization term, and the nonnegative constraint. According to [56] and [59], both the least square loss function and the graph Laplacian regularization term are convex and smooth, while either the ℓ1 -norm regularization term or the nonnegative constraint is convex but not differentiable in all the range of W and thus being convex but nonsmooth. Therefore, our final objective function is convex but nonsmooth. According to [30] and [59], we can use an iterative method to optimize (7). As the objective function (7) is convex, the W satisfying (7) is a global optimum solution. Moreover, it will converge to the global optimum of the objective function (7). In this paper, we omit the proof, since it can be directly obtained according to [30, Th. 1].

After optimizing (7), we obtain the optimal solution W∗ , i.e., the weight matrix or the correlations between training samples and themselves. The element wij of W∗ denotes the correlation between the i th training sample and the j th training sample. The positive weight (i.e., wij>0 ) indicates that the i th training sample and the j th training sample are positively correlated, and the negative weight (i.e., wij<0 ) means that their correlation is negative. In particular, the zero weight (i.e., wij=0 ) means that there is no correlation between the i th training sample and the j th training sample. In other words, the i th training sample should not be used for predicting the j th training sample. Consequently, we only use those correlated training samples (i.e., the training samples with nonzero coefficient) to predict each training sample, rather than using all training samples.

To better understand the characteristics of the proposed reconstruction method, we assume the optimal solution W∗∈R5×5 as follows:
W∗=⎡⎣⎢⎢⎢⎢⎢⎢0.2000.10.020.050.70.020.30000.900.100.600.8000.1000.3⎤⎦⎥⎥⎥⎥⎥⎥.
View SourceRight-click on figure for MathML and additional features.In this example, we have five training samples. According to our proposed method, the values in the first column of W∗ indicate the correlations between the first training sample and all five training samples. Due to that there are only three nonzero values in the first column, i.e., w11 , w41 , and w51 , the first training sample is only related to the last two training samples except itself, i.e., the fourth training sample and the fifth training sample. More specifically, in the kNN classification step, we only need to regard the last two training samples as the nearest neighbors of the first training sample, i.e., the corresponding optimal-k -value is 2. Meanwhile, according to the values of the second column of W∗ , we only need to regard three training samples as the nearest neighbors of the second training sample, i.e., the corresponding optimal-k -value is 3. Obviously, for the third training sample, it should be predicted by the fifth training sample. The corresponding optimal-k -value is 1. In this way, the nearest neighbors of each training sample are obtained by learning the proposed reconstruction model. Moreover, the optimal-k -value in the kNN algorithm are different for different samples. Hence, (7) takes the distribution of data and prior knowledge into account for selecting the optimal-k -value for each training sample.

D. kTree Method
The kNN based on graph sparse reconstruction (GS-kNN) method in [30] used (7) to reconstruct test samples by training samples to yield good performance. However, it is time-consuming, i.e., at least O(n2) for predicting each test sample, where n is the number of training samples. To overcome this, we propose a training stage to construct a k -decision tree (namely, kTree) between training samples and their corresponding optimal-k -values. The motivation of our method is that we expect to find the relationship between training samples and their optimal-k -values so that the learned kTree enables to output an optimal-k -value for a test sample in the test stage. In this way, our test stage with time complexity O(log (d)+n) is faster than both the GS-kNN method in [30] and the fixed kNN method, with the time complexity at least O(n2d) . It should be noteworthy that our proposed method thus results in a training stage involving two steps, i.e., optimizing (7) to yield the optimal-k -values for all training samples and constructing the kTree, respectively. Fortunately, both of them are offline.

In the training stage, our kTree method first uses (7) to learn the optimal W to obtain optimal-k -values of training samples, i.e., the numbers of nonzero coefficients in each column of W for each training sample. Then, we regard the learned optimal-k -values as labels to construct a kTree between training samples and their corresponding optimal-k -values. That is, we follow the idea of the state-of-the-art methods such as ID3 [12], [60], [61] to greedily construct a top-down recursive divide-and-conquer decision tree. The difference between our method and ID3 method is that our kTree regards the optimal-k -values of training samples as their labels, while ID3 method uses the labels of training samples to construct its decision tree. This results in different items being stored in the leaf nodes, where ID3 stores the labels of training samples and our kTree stores the optimal-k -values of training samples. We illustrate their difference in Fig. 3(a) and (b).

Fig. 3. - Illustration of three different kinds of decision trees, i.e., ID3 method, kTree method, and k*Tree method, respectively. Note that these three decision trees are constructed with the same rule but storing different items in leaf nodes. Specifically, ID3 and kTree, respectively, store the class labels and the optimal-
$k$
-values, of training samples in the leaf node, while k*Tree stores the optimal-
$k$
-value (i.e., 
${k}$
 value), a subset of training samples (i.e., 
$\mathbf {e}_{1}, \ldots, \mathbf {e}_{m}$
, the corresponding nearest neighbors of these training samples (e.g., the kNNs 
$\mathbf {e}_{i1}, \ldots \mathbf {e}_{ij},\ldots, \mathbf {e}_{ik}$
 of training sample 
$\mathbf {e}_{i}$
, 
$i = 1, \ldots, m$
), and the nearest neighbor of these nearest neighbors (e.g., the nearest neighbor 
$\mathbf {e}_{ij_{NN}}$
 of 
$\mathbf {e}_{ij}$
, 
$i = 1, \ldots, m$
 and, 
$j = 1, \ldots, k$
). (a) Decision tree. (b) kTree. (c) k*Tree.
Fig. 3.
Illustration of three different kinds of decision trees, i.e., ID3 method, kTree method, and k*Tree method, respectively. Note that these three decision trees are constructed with the same rule but storing different items in leaf nodes. Specifically, ID3 and kTree, respectively, store the class labels and the optimal-k -values, of training samples in the leaf node, while k*Tree stores the optimal-k -value (i.e., k value), a subset of training samples (i.e., e1,…,em , the corresponding nearest neighbors of these training samples (e.g., the kNNs ei1,…eij,…,eik of training sample ei , i=1,…,m ), and the nearest neighbor of these nearest neighbors (e.g., the nearest neighbor eijNN of eij , i=1,…,m and, j=1,…,k ). (a) Decision tree. (b) kTree. (c) k*Tree.

Show All

In the test stage, we easily obtain the optimal-k -values of test samples in the leaf nodes of kTree. Then, we conduct the kNN classification step to classify test samples between training samples and the learned optimal-k -values of test samples. Such a test process only needs O(log (d)+n) and is faster than both the varied kNN methods (such as GS-kNN method in [30]) and the fixed kNN method with the time complexity at least O(n2d) . We list the pseudo of the proposed kTree method in Algorithm 1.

Algorithm 1 Pseudo of the Proposed kTree Method
training samples X , test samples Y

Class labels of Y

* Training Stage *

Learning the optimal-k -values of all training samples by Eq. (7);

Using ID3 method to construct kTree with training samples and their corresponding optimal-k -values;

Storing the optimal-k -values of training samples in leaf nodes;

* Test Stage *

Obtaining the optimal-k -values of test samples (i.e., k ) using kTree;

Predicting test labels using traditional kNN method with learnt optimal-k -values on all training samples;

Although our kTree method enables to obtain optimal-k -values for test samples, it still needs to conduct kNN classification on all training samples. To further reduce time complexity, we extend our kTree method to an improvement version (namely, k*Tree method) with the time complexity of test stage O(log (d)+s) , where s is the cardinality of a subset of training samples, i.e., s≪n .

E. k*Tree Classification
In the training stage, the proposed k*Tree method constructs the decision tree (namely, k*Tree) by using the same steps of kTree described in Section III-D. Their difference is the information in the leaf nodes. That is, kTree stores the optimal-k -value in leaf nodes, while k*Tree stores the optimal-k -value as well as other information in the leaf nodes, including a subset of training samples located in this leaf node, the kNNs of each sample in this subset, and the nearest neighbor of each of these kNNs. Specifically, in the constructed k*Tree, each leaf node contains an optimal-k -value (e.g., kj ) and a subset of training samples (i.e., X′={x′1,…,x′m} ) which regard kj as their optimal-k -values. Besides these, we also store the kj nearest neighbors of each sample in X′ , denoted as X′i={x′i1,…,x′ikj} (where i=1,…,m ), and the nearest neighbor of each x′ik as x′′ik (k=1,…,kj ), denoted as X′′i={x′′i1,…,x′′ikj} . In this way, each leaf node contains the optimal-k -values, X′ , {X′1,…,X′m} , and {X′′1,…,X′′m} . We list the illustration in Fig. 3(c).

In the test stage, given a test sample (e.g., xt ), the proposed k*Tree method first searches the constructed k*Tree to output its optimal-k -value (e.g., kt ) as well as its nearest neighbors in the leaf node (e.g., x′t ). With these, the proposed k*Tree method selects kt nearest neighbors from the subset of training samples, including x′t , its kt nearest neighbors X′t={x′t1,…,x′tkt} , and the nearest neighbors of X′t , i.e., X′′t={x′′t1,…,x′′tkt} , and further assigns xt with a label according to the majority rule of kt nearest neighbors. In the proposed k*Tree method, the kNN classification is conducted by select nearest neighbors from the set S={x′t,X′t,X′′t} . We denoted the cardinality of S as s , i.e., s≤2×kt+1 in this example. The pseudo of k*Tree method is presented in Algorithm 2.

Algorithm 2 Pseudo of the Proposed k*Tree Method
training samples X , test samples Y

Class labels of Y

* Training Stage *

Learning the optimal-k -values of all training samples by Eq. (7);

Using ID3 method to construct k*Tree with training samples and their corresponding optimal-k -values;

Storing the optimal-k -values of training samples, X′ , {X′1,…,X′m} , and {X′′1,…,X′′m} , in leaf nodes;

* Test Stage *

Obtaining the optimal-k -values of test samples (i.e., k ) using k*Tree ;

Predicting test labels using traditional kNN method with learnt optimal-k -values on X′ , {X′1,..,X′m} and {X′′1,…,X′′m} ;

The principle of kNN method is based on the intuitive assumption that samples in the same class should be closer in the feature space [19]. As a result, for a given test sample of unknown class, we can simply compute the distance between this test sample and all the training samples, and assign the class determined by kNNs of this test sample. In the proposed k*Tree method, we reduce the training set from all training samples to its subset, i.e., the neighbors of the nearest neighbors of the test sample (i.e., X′t ) and the nearest neighbor of all neighbors of the test samples (i.e., X′′t ). In this way, we expect that the set S almost includes all the nearest neighbors in the whole training samples. Actually, our experimental results listed in Section IV verified this assumption, since the proposed k*Tree method achieved similar classification accuracy to the kTree method and the traditional kNN method.

The training complexity of k*Tree method is the same as the kTree method, i.e., O(n2) . In the test stage, the k*Tree method conducts kNN classification on a subset of training samples, i.e., S={x′t,X′t,X′′t} , thus resulting in the time complexity of test stage as at most O(log (d)+s) (where s≪n ).

SECTION IV.Experiments
A. Experimental Setting
We used 20 public data sets from UCI Repository of Machine Learning Data sets,3 whose data sets have been widely used for academic research, to evaluate the proposed methods and the competing methods on the classification task, in terms of classification accuracy and running cost. These data sets include all different types of data, such as low-dimensional data set and high-dimensional data set, binary data sets and multiclass data sets, and imbalance data sets, and are used to evaluate the robust of the proposed methods. In our experiments, we used ten of them (e.g., Abalone, Balance, Blood, Car, Breast, Australian, Climate, and German) for the experiments of different sample size, while the rest (e.g., Madelon, LSVT, CNAE, Gisette, Hill, Libras, Dbworld, and Arcene) for the experiments of different feature numbers. Among of them, both Climate data set containing 46 positive samples and 494 negative samples and German data set including 700 positive samples and 300 negative samples can be regarded as imbalance data sets.

We employed the tenfold cross validation method on all methods. Specifically, we first randomly partitioned the whole data set into ten subsets and then selected one subset for testing and the remaining nine subsets for training. We repeated the whole process ten times to avoid the possible bias during data set partitioning for cross-validation. The final result was computed by averaging results from all experiments. For the model selection of our method, we considered the parameter spaces of ρ1∈{10−5,10−4,…,101} and ρ2∈{10−5,10−4,…,10−1} in (7).

B. Competing Methods
In this paper, we selected the state-of-the-art methods, including kNN [19], kNN-based applicability domain approach (AD-kNN) [33], kNN method based on sparse learning (S-kNN) [20], GS-kNN [30], filtered attribute subspace-based bagging with injected randomness (FASBIR), ensembles of nearest neighbor classifiers) [62], [63], and Landmark-based spectral Clustering kNN (LC-kNN) [64] as the competing methods. We list their details as follows.

k -Nearest Neighbor: kNN is a classical classification method. Following the literature in [19], we set k=1 , 5, 10, 20, and the squared root of sample size, respectively, and reported the best result.

kNN-Based Applicability Domain Approach [33]: AD-kNN integrates salient features of the kNN approach and adaptive kernel methods for conducting probability density estimation. Following the literature [33], we set the parameter k of AD-kNN with the Monte Carlo validation method by setting the maximum number of neighbors as 20.

kNN Method Based on Sparse Learning [20]: S-kNN learns different k values for different test samples by sparse learning, where a least square loss function is applied to achieve the minimal reconstruction error, an ℓ1− norm regularization term is utilized to obtain the elementwise sparsity, and a Laplacian regularization term is used to preserve the local structures of data. Following the literature in [20], we used the cross validation method to conduct model selection by setting β1 and β2 in the ranges of {10−5,10−4,…,101} .

kNN Based on Graph Sparse Reconstruction [30]: GS-kNN first uses training samples to reconstruct test samples to obtain the optimal k values, and then uses the traditional kNN method to conduct classification tasks. Following the literature in [30], we used the cross validation method to conduct model selection by setting the parameter spaces of γ1∈{10−5,10−4,…,101} , γ2∈{10−5,10−4,…,10−1} and γ3∈{10−5,10−4,…,10−2} .

FASBIR [62], [63] was designed for building ensembles of nearest neighbor classifiers. FASBIR works through integrating the perturbations on the training data, input attributes and learning parameters together.

Landmark-based spectral Clustering kNN (LC-kNN) [64] was proposed to first conduct k -means clustering to separate the whole data set into several parts and then select the nearest cluster as the training samples for conducting the kNN classification.

C. Experimental Results on Different Sample Sizes
In this section, we conducted classification tasks with all methods at different sample size on ten UCI data sets, aim at avoiding the bias of imbalanced sample size. We reported the classification accuracy (i.e., the averaging classification accuracy of ten iterations) of all methods in Fig. 4, where the horizontal axis indicates the sample size and the vertical axis represents the classification accuracy. We also listed the running cost (in time) of all methods in each of iteration in Figs. 5–14, where the horizontal axis indicates the number of iterations and the vertical axis represents the running cost. We also list the accuracy and the running cost in Table I.

TABLE I Result of Classification Accuracy/Running Cost (Mean)
Table I- 
Result of Classification Accuracy/Running Cost (Mean)
Fig. 4. - Classification accuracy on ten data sets with different sample size. (a) Abalone (
${\rho _{1}} = {10^{-4}}, {\rho _{2}} = {10^{-5}}$
). (b) Balance (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-5}}$
). (c) Blood (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-4}}$
). (d) Car (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-1}}$
). (e) BreastOri (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-5}}$
). (f) Australian (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-3}}$
). (g) Climate (
${\rho _{1}} = {10^{-4}}, {\rho _{2}} = {10^{-5}}$
). (h) German (
${\rho _{1}} = {10^{-4}}, {\rho _{2}} = {10^{-4}}$
). (i) DDCclients (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-5}}$
). (j) MicePE (
${\rho _{1}} = {10^{-4}}, {\rho _{2}} = {10^{-4}}$
);.
Fig. 4.
Classification accuracy on ten data sets with different sample size. (a) Abalone (ρ1=10−4,ρ2=10−5 ). (b) Balance (ρ1=10−3,ρ2=10−5 ). (c) Blood (ρ1=10−3,ρ2=10−4 ). (d) Car (ρ1=10−3,ρ2=10−1 ). (e) BreastOri (ρ1=10−3,ρ2=10−5 ). (f) Australian (ρ1=10−3,ρ2=10−3 ). (g) Climate (ρ1=10−4,ρ2=10−5 ). (h) German (ρ1=10−4,ρ2=10−4 ). (i) DDCclients (ρ1=10−3,ρ2=10−5 ). (j) MicePE (ρ1=10−4,ρ2=10−4 );.

Show All

Fig. 5. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Abalone with a sample size of 4000.
Fig. 5.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Abalone with a sample size of 4000.

Show All

Fig. 6. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Balance with a sample size of 600.
Fig. 6.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Balance with a sample size of 600.

Show All

Fig. 7. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Blood with a sample size of 720.
Fig. 7.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Blood with a sample size of 720.

Show All

Fig. 8. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Car with a sample size of 1600.
Fig. 8.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Car with a sample size of 1600.

Show All

Fig. 9. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Breast with a sample size of 680.
Fig. 9.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Breast with a sample size of 680.

Show All

Fig. 10. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Australian with a sample size of 690.
Fig. 10.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Australian with a sample size of 690.

Show All

Fig. 11. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Climate with a sample size of 540.
Fig. 11.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Climate with a sample size of 540.

Show All

Fig. 12. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on German with a sample size of 1000.
Fig. 12.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on German with a sample size of 1000.

Show All

Fig. 13. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on DDCclients with a sample size of 4000.
Fig. 13.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on DDCclients with a sample size of 4000.

Show All

Fig. 14. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on MicePE with a sample size of 1000.
Fig. 14.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on MicePE with a sample size of 1000.

Show All

From Fig. 4 and Table I, we knew that: 1) the proposed methods (i.e., the kTree method and the k*Tree method) improved the classification accuracies on average by 4% (versus kNN), 1.5% (versus AD-kNN), 1.8% (versus FASBIR), and 4.5% (versus LC-kNN), on all ten data sets, while our methods (i.e., the kTree method and the k*Tree method) have almost the same accuracy with GS-kNN and S-kNN and 2) the methods learning optimal-k -values for different samples (e.g., our k*Tree method, our kTree method, S-kNN, GS-kNN, and AD-kNN) outperformed the method with fixed expert-predefined value (e.g., kNN). For example, kNN method reduced on average the classification accuracy on ten data sets by 2.6% (versus AD-kNN), which is the worst method learning optimal-k -values for different samples in our experiments.

Regarding the running cost in Figs. 5–14, we have the following observations.

Our k*Tree method achieved the minimal running cost, followed by LC-kNN, kNN, our kTree, FASBIR, AD-kNN, GS-kNN, and S-kNN. For example, the k*Tree method was four times faster than kNN on Abalone data set in our experiments. The reason is that the proposed k*Tree method scanned a small subset of training samples to conduct kNN classification, while both kNN and the kTree method conducted kNN classification by scanning all training samples. It is noteworthy that the running cost of LC-kNN is similar to our k*Tree, since LC-kNN conducts k -means clustering to separate the whole data set into several parts, i.e., only scanning a subset of training data set. However, our k*Tree outperformed LC-kNN in terms of classification accuracy. Moreover, it is very difficult for LC-kNN to find a suitable number of the parts so that achieving the similar performance as the standard kNN [64].

The methods (such as S-kNN, GS-kNN, and AD-kNN) took more running cost than either our methods (e.g., the kTree method and the k*Tree method) or kNN, since both S-kNN and GS-kNN must use training samples to reconstruct test samples to obtain the optimal k values, while AD-kNN took expensive cost to calculate AD of training samples and verified if test samples were inside or outside the AD.

From Table I, our proposed kTree and k*Tree outperformed kNN, AD-kNN and FASBIR in terms of classification accuracy, and also took less running cost. For example, k*Tree improved about 5.7% (versus kNN), 2% (versus AD-kNN), and 3.2% (versus FASBIR), respectively, on DDCclients data set, while k*Tree was about 5 times, 1000 times and 10 times, faster than kNN, AD-kNN, and FASBIR, respectively, in terms of running cost. On the other hand, our k*Tree did not achieve the similar performance as either GS-kNN or S-kNN, but was faster than each of them in terms of running cost. For example, k*Tree was on average about 15000 times faster than either GS-kNN or S-kNN, but only reducing the classification accuracy by about 0.6% on all data sets. The reason is that the proposed k*Tree method only scanned a small subset of training samples to conduct kNN classification, while bo GS-kNN and S-kNN scanned all training samples.

D. Experimental Results on Different Feature Number
In this section, we first employed the state-of-the-art method Fisher score [65] to rank all features of the data, and then selected the most informative features for kNN classification. Our goal is to analysis the robustness of all methods with different feature numbers. Fig. 15 listed the classification accuracy of all methods on ten data sets and Figs. 16–25 reported the running cost of each iteration for all methods. We also list the accuracy and the running cost in Table II.

TABLE II Result of Classification Accuracy/Running Cost (Mean)
Table II- 
Result of Classification Accuracy/Running Cost (Mean)
Fig. 15. - Classification accuracy on ten data sets with different number of features. (a) Madelon (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-3}}$
). (b) LSVT (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-3}}$
). (c) CNAE (
${\rho _{1}} = {10^{-2}}, {\rho _{2}} = {10^{-2}}$
). (d) Gisette (
${\rho _{1}} = {10^{-1}}, {\rho _{2}} = {10^{-5}}$
). (e) Hill (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-3}}$
). (f) Libras (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-4}}$
). (g) DBworld (
${\rho _{1}} = {10^{-4}}, {\rho _{2}} = {10^{-3}}$
). (h) Arcene (
${\rho _{1}} = {10^{-2}}, {\rho _{2}} = {10^{-3}}$
). (i) Musk (
${\rho _{1}} = {10^{-4}}, {\rho _{2}} = {10^{-4}}$
). (j) Arrhythmia (
${\rho _{1}} = {10^{-3}}, {\rho _{2}} = {10^{-5}}$
).
Fig. 15.
Classification accuracy on ten data sets with different number of features. (a) Madelon (ρ1=10−3,ρ2=10−3 ). (b) LSVT (ρ1=10−3,ρ2=10−3 ). (c) CNAE (ρ1=10−2,ρ2=10−2 ). (d) Gisette (ρ1=10−1,ρ2=10−5 ). (e) Hill (ρ1=10−3,ρ2=10−3 ). (f) Libras (ρ1=10−3,ρ2=10−4 ). (g) DBworld (ρ1=10−4,ρ2=10−3 ). (h) Arcene (ρ1=10−2,ρ2=10−3 ). (i) Musk (ρ1=10−4,ρ2=10−4 ). (j) Arrhythmia (ρ1=10−3,ρ2=10−5 ).

Show All

Fig. 16. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Madelon with a feature number of 450.
Fig. 16.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Madelon with a feature number of 450.

Show All

Fig. 17. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on LSVT with a feature number of 300.
Fig. 17.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on LSVT with a feature number of 300.

Show All

Fig. 18. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on CNAE with a feature number of 840.
Fig. 18.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on CNAE with a feature number of 840.

Show All

Fig. 19. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Gisette with a feature number of 5000.
Fig. 19.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Gisette with a feature number of 5000.

Show All

Fig. 20. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Hill with a feature number of 100.
Fig. 20.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Hill with a feature number of 100.

Show All

Fig. 21. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Libras with a feature number of 90.
Fig. 21.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Libras with a feature number of 90.

Show All

Fig. 22. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on DBworld with a feature number of 4700.
Fig. 22.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on DBworld with a feature number of 4700.

Show All

Fig. 23. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Arcene with a feature number of 10 000.
Fig. 23.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Arcene with a feature number of 10 000.

Show All

Fig. 24. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Musk with a feature number of 160.
Fig. 24.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Musk with a feature number of 160.

Show All

Fig. 25. - Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Arrhythmia with a feature number of 270.
Fig. 25.
Running cost of all methods (left) and five methods zoomed in from the left subfigure (right) on Arrhythmia with a feature number of 270.

Show All

Fig. 15 and Table II clearly indicated that our methods (i.e., the kTree method and the k*Tree method) still achieved the best classification accuracy, compared with other comparison methods. For example, our k*Tree method improved the classification accuracies on average over ten data sets by 2.6% (versus AD-kNN), 4.2% (versus kNN), 2.3% (versus FASBIR), and 5.1% (versus LC-kNN). Moreover, the proposed kTree method and the proposed k*Tree method had similar results with GS-kNN and S-kNN, in terms of classification accuracy.

Figs. 16–25 intuitively showed that the k*Tree method is faster than the kTree method and kNN on Madelon data set, while slower than kNN on data sets, such as CNAE and Musk. The reason is that when the dimensions of the data sets are small and the number of training samples is big, i.e., n≫d , the k*Tree method is much faster than kNN. While the dimensions are large, or even larger than the number of samples, i.e., d≥n , the k*Tree method will be slower than kNN.

From Table II, our proposed kTree and k*Tree still outperformed kNN, AD-kNN, and FASBIR with different feature numbers. Moreover, our k*Tree did not achieve the similar performance as either GS-kNN or S-kNN, but was faster than each of them in terms of running cost. The reason is that the proposed k*Tree method only scanned a small subset of training samples to conduct kNN classification, while both GS-kNN and S-kNN scanned all training samples. In particular, although both k*Tree and LC-kNN were designed to scan a subset of training samples, our k*Tree increased by on average 4.9% (classification accuracy) and also was faster twice than LC-kNN. The reason is that it is difficult for LC-kNN to find a suitable number of the parts, which was concluded in [64]. All the above experimental results showed that the proposed method k*Tree can be used to improve the performance of kNN method in terms of classification accuracy and running cost.

SECTION V.Conclusion and Future Work
In this paper, we have proposed two new kNN classification algorithms, i.e., the kTree and the k*Tree methods, to select optimal-k -value for each test sample for efficient and effective kNN classification. The key idea of our proposed methods is to design a training stage for reducing the running cost of test stage and improving the classification performance. Two set of experiments have been conducted to evaluate the proposed methods with the competing methods, and the experimental results indicated that our methods outperformed the competing methods in terms of classification accuracy and running cost.

In future, we will focus on improving the performance of the proposed methods on high-dimensional data [11], [66]–[67][68].