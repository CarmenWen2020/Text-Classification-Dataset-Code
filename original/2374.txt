Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This article provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to many applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.
SECTION I.Introduction
The field of natural language processing (NLP) encompasses a variety of topics, which involves the computational processing and understanding of human languages. Since the 1980s, the field has increasingly relied on data-driven computation involving statistics, probability, and machine learning [1], [2]. Recent increases in computational power and parallelization, harnessed by graphical processing units (GPUs) [3], [4], now allow for “deep learning,” which utilizes artificial neural networks (ANNs), sometimes with billions of trainable parameters [5]. In addition, the contemporary availability of large data sets, facilitated by sophisticated data collection processes, enables the training of such deep architectures [6]–[7][8].

In recent years, researchers and practitioners in NLP have leveraged the power of modern ANNs with many propitious results, beginning in large part with the pioneering work of Collobert et al. [9]. In the very recent past, the use of deep learning has considerably upsurged [10], [11]. This has led to significant advances both in core areas of NLP and in areas in which it is directly applied to achieve practical and useful objectives. This article provides a brief introduction to both NLP and deep neural networks (DNNs) and then presents an extensive discussion on how deep learning is being used to solve current problems in NLP. While several other articles and books on the topic have been published [10], [12], none of them have extensively covered the state of the art in as many areas within it. Furthermore, no other survey has examined not only the applications of deep learning to computational linguistics but also the underlying theory and traditional NLP tasks. In addition to the discussion of recent revolutionary developments in the field, this article will be useful to readers who want to familiarize themselves quickly with the current state of the art before embarking upon further advanced research and practice.

The topics of NLP and AI, including deep learning, are introduced in Section II. The ways in which deep learning has been used to solve problems in core areas of NLP are presented in Section III. The section is broken down into several subsections, namely natural language modeling (Section III-A), morphology (Section III-B), parsing (Section III-C), and semantics (Section III-D). Applications of deep learning to more practical areas are discussed in Section IV. Specifically discussed are information retrieval (IR) (Section IV-A), information extraction Section (IV-B), text classification (Section IV-C), text generation (Section IV-D), summarization (Section IV-E), question ans- wering (QA) (Section IV-F), and machine translation (Section IV-G). Conclusions are then drawn in Section V with a brief summary of the state of the art as well as predictions, suggestions, and other thoughts on the future of this dynamically evolving area.

SECTION II.Overview of NLP and Deep Learning
In this section, significant issues that draw the attention of researchers and practitioners are introduced, followed by a brisk explanation of the deep learning architectures commonly used in the field.

A. Natural Language Processing
The field of NLP, also known as computational linguistics, involves the engineering of computational models and processes to solve practical problems in understanding human languages. These solutions are used to build useful software. Work in NLP can be divided into two broad subareas: core areas and applications, although it is sometimes difficult to distinguish clearly to which areas issues belong. The core areas address fundamental problems such as language modeling, which underscores quantifying associations among naturally occurring words; morphological processing, dealing with segmentation of meaningful components of words and identifying the true parts of speech (POSs) of words as used; syntactic processing, or parsing, which builds sentence diagrams as possible precursors to semantic processing; and semantic processing, which attempts to distill meaning of words, phrases, and higher level components in text. The application areas involve topics, such as extraction of useful information (e.g., named entities and relations), translation of text between and among languages, summarization of written works, automatic answering of questions by inferring answers, and classification and clustering of documents. Often, one needs to handle one or more of the core issues successfully and apply those ideas and procedures to solve practical problems.

Currently, NLP is primarily a data-driven field using statistical and probabilistic computations along with machine learning. In the past, machine learning approaches, such as naïve Bayes, k -nearest neighbors, hidden Markov models, conditional random fields (CRFs), decision trees, random forests, and support vector machines, were widely used. However, during the past several years, there has been a wholesale transformation, and these approaches have been entirely replaced, or at least enhanced, by neural models, discussed next.

B. Neural Networks and Deep Learning
Neural networks are composed of interconnected nodes, or neurons, each receiving some number of inputs and supplying an output. Each of the nodes in the output layers performs weighted sum computation on the values they receive from the input nodes and then generate outputs using simple nonlinear transformation functions on these summations. Corrections to the weights are made in response to individual errors or losses that the networks exhibit at the output nodes. Such corrections are usually made in modern networks using stochastic gradient descent, considering the derivatives of errors at the nodes, an approach called backpropagation [13]. The main factors that distinguish different types of networks from each other are how the nodes are connected and the number of layers. Basic networks in which all nodes can be organized into sequential layers, with every node receiving inputs only from nodes in earlier layers, are known as feedforward neural networks (FFNNs). While there is no clear consensus on exactly what defines a DNN, generally, networks with multiple hidden layers are considered deep and those with many layers are considered very deep [7].

1) Convolutional Neural Networks:
Convolutional neural networks (CNNs) [14], [15], built upon Fukashima’s neocognitron [16], [17], derive the name from the convolution operation in mathematics and signal processing. CNNs use functions, known as filters, allowing for simultaneous analysis of different features in the data [18], [19]. CNNs are extensively used in image and video processing, as well as speech and NLP [20]–[21][22][23]. Often, it is not important precisely where certain features occur, but rather whether or not they appear in particular localities. Therefore, pooling operations can be used to minimize the size of feature maps (the outputs of the convolutional filters). The sizes of such pools are generally small to prevent the loss of too much precision.

2) Recursive Neural Networks:
Much like CNNs, recursive networks [24], [25] use a form of weight sharing to minimize training. However, whereas CNNs share weights horizontally (within a layer), recursive nets share weights vertically (between layers). This is particularly appealing, as it allows for easy modeling of structures such as parse trees. In recursive networks, a single tensor (or a generalized matrix) of weights can be used at a low level in the tree and then used recursively at successively higher levels [26].

3) Recurrent Neural Networks and Long Short-Term Memory Networks:
A type of recursive neural network that has been used heavily is the recurrent neural network (RNN) [27], [28]. Since much of NLP is dependent on the order of words or other elements such as phonemes or sentences, it is useful to have memory of the previous elements when processing new ones [29]–[30][31]. Sometimes, backward dependencies exist, i.e., correct processing of some words may depend on words that follow. Thus, it is beneficial to look at sentences in both directions, forward and backward, using two RNN layers and combining their outputs. This arrangement of RNNs is called a bidirectional RNN. It may also lead to a better final representation if there is a sequence of RNN layers. This may allow the effect of an input to linger longer than a single RNN layer, allowing for longer term effects. This setup of sequential RNN cells is called an RNN stack [32], [33].

One highly engineered RNN is the long short-term memory (LSTM) network [34], [35]. In LSTMs, the recursive nodes are composed of several individual neurons connected in a manner designed to retain, forget, or expose specific information. Whereas generic RNNs with single neurons feeding back to themselves technically have some memory of long passed results, these results are diluted with each successive iteration. Oftentimes, it is important to remember information from the distant past, while at the same time, other very recent information may not be important. By using LSTM blocks, this important information can be retained much longer, while irrelevant information can be forgotten. A slightly simpler variant of the LSTM, called the gated recurrent unit (GRU), has been shown to perform as well as or better than standard LSTMs in many tasks [36], [37].

4) Attention Mechanisms and Transformer:
For tasks such as machine translation, text summarization, or captioning, the output is in textual form. Typically, this is done through the use of encoder–decoder pairs. An encoding ANN is used to produce a vector of a particular length and a decoding ANN is used to return variable-length text based on this vector. The problem with this scheme, which is shown in Fig. 1(a), is that the RNN is forced to encode an entire sequence to a finite length vector, without regard to whether or not any of the inputs are more important than others.

Fig. 1. - Encoder–decoder architectures. While there are multiple options of encoders and decoders available, RNN variants are a common choice for each, particularly the latter. Such a network is shown in (a). Attention mechanisms, such as that present in (b), allow the decoder to determine which portions of the encoding are most relevant at each output step.
Fig. 1.
Encoder–decoder architectures. While there are multiple options of encoders and decoders available, RNN variants are a common choice for each, particularly the latter. Such a network is shown in (a). Attention mechanisms, such as that present in (b), allow the decoder to determine which portions of the encoding are most relevant at each output step.

Show All

A robust solution to this is that of attention. The first noted use of an attention mechanism [38] used a dense layer for annotated weighting of an RNN’s hidden state, allowing the network to learn what to pay attention to in accordance with the current hidden state and annotation. Such a mechanism is present in Fig. 1(b). Variants of the mechanism have been introduced, popular ones including convolutional [39], intratemporal [40], gated [41], and self-attention [42]. Self-attention involves providing attention to words in the same sentence. For example, during encoding a word in an input sentence, it is beneficial to project variable amounts of attention to other words in the sentence. During decoding to produce a resulting sentence, it makes sense to provide appropriate attention to words that have already been produced. Self-attention, in particular, has become widely used in a state-of-the-art encoder–decoder model called transformer [42]. The transformer model, shown in Fig. 2, has a number of encoders and decoders stacked on top of each other, self-attention in each of the encoder and decoder units, and cross attention between the encoders and the decoders. It uses multiple instances of attention in parallel and eschews the use of recurrences and convolutions. The transformer has become a quintessential component in most state-of-the-art neural networks for NLP.

Fig. 2. - Transformer model. (a) Transformer with four “encoders” followed by four “decoders,” all following a “positional encoder.” (b) Inner workings of each “encoder,” which contains a self-attention layer followed by a feed forward layer. (c) Inner workings of each “decoder,” which contains a self-attention layer followed by an attentional encoder–decoder layer and then a feed forward layer.
Fig. 2.
Transformer model. (a) Transformer with four “encoders” followed by four “decoders,” all following a “positional encoder.” (b) Inner workings of each “encoder,” which contains a self-attention layer followed by a feed forward layer. (c) Inner workings of each “decoder,” which contains a self-attention layer followed by an attentional encoder–decoder layer and then a feed forward layer.

Show All

5) Residual Connections and Dropout:
In deep networks, trained via backpropagation [13], the gradients used to correct for error often vanish or explode [43]. This can be mitigated by choosing activation functions, such as the rectified linear unit (ReLU) [44], which do not exhibit regions that are arêtically steep or have bosonically small gradients. Also, in response to this issue, as well as others [45], residual connections are often used. Such connections are simply those that skip layers (usually one). If used in every alternating layer, this cuts in half the number of layers through which the gradient must backpropagate. Such a network is known as a residual network (ResNet). A number of variants exist, including highway networks [46] and DenseNets [47].

Another important method used in training ANNs is dropout. In dropout, some connections and maybe even nodes are deactivated, usually randomly, for each training batch (small set of examples), varying which nodes are deactivated each batch. This forces the network to distribute its memory across multiple paths, helping with generalization and lessening the likelihood of overfitting to the training data.

SECTION III.Deep Learning in Core Areas of NLP
The core issues are those that are inherently present in any computational linguistic system. To perform translation, text summarization, image captioning, or any other linguistic task, there must be some understanding of the underlying language. This understanding can be broken down into at least four main areas: language modeling, morphology, parsing, and semantics. The number of scholarly works in each area over the last decade is shown in Fig. 3.

Fig. 3. - Publication volume for core areas of NLP. The number of publications, indexed by Google Scholar, relating to each topic over the last decade is shown. While all areas have experienced growth, language modeling has grown the most.
Fig. 3.
Publication volume for core areas of NLP. The number of publications, indexed by Google Scholar, relating to each topic over the last decade is shown. While all areas have experienced growth, language modeling has grown the most.

Show All

Language modeling can be viewed in two ways. First, it determines which words follow which. By extension, however, this can be viewed as determining what words mean, as individual words are only weakly meaningful, deriving their full value only from their interactions with other words. Morphology is the study of how words themselves are formed. It considers the roots of words and the use of prefixes and suffixes, compounds, and other intraword devices, to display tense, gender, plurality, and a other linguistic constructs. Parsing considers which words modify others, forming constituents, leading to a sentential structure. The area of semantics is the study of what words mean. It considers the meanings of the individual words and how they relate to and modify others, as well as the contexts these words appear in and some degree of world knowledge, i.e., “common sense.”

There is a significant amount of overlap between each of these areas. Therefore, many models analyzed can be classified as belonging in multiple sections. As such, they are discussed in the most relevant sections with logical connections to those other places where they also interact.

A. Language Modeling and Word Embeddings
Arguably, the most important task in NLP is that of language modeling. Language modeling is an essential piece of almost any application of NLP. Language modeling is the process of creating a model to predict words or simple linguistic components given previous words or components [48]. This is useful for applications in which a user types input, to provide predictive ability for fast text entry. However, its power and versatility emanate from the fact that it can implicitly capture syntactic and semantic relationships among words or components in a linear neighborhood, making it useful for tasks such as machine translation or text summarization. Using prediction, such programs can generate more relevant, human-sounding sentences.

1) Neural Language Modeling:
A problem with statistical language models was the inability to deal well with synonyms or out-of-vocabulary (OOV) words that were not present in the training corpus. Progress was made in solving the problems with the introduction of the neural language model [49]. While much of NLP took another decade to begin to use ANNs heavily, the language modeling community immediately took advantage of them and continued to develop sophisticated models, many of which were summarized by De Mulder et al. [50].

2) Evaluation of Language Models:
While neural networks have made breakthroughs in the language modeling field, it is hard to quantify improvements. It is desirable to evaluate language models independently of the applications in which they appear. A number of metrics have been proposed, but no perfect solution has yet been found. [51]–[52][53] The most commonly used metric is perplexity, which is the inverse probability of a test set normalized by the number of words. Perplexity is a reasonable measurement for language modelings trained on the same data sets, but when they are trained on different vocabularies, the metric becomes less meaningful. Luckily, there are several benchmark data sets that are used in the field, allowing for comparison. Two such data sets are the Penn Treebank (PTB) [54] and the Billion Word Benchmark [55].

3) Memory Networks and Attention Mechanisms in Language Modeling:
Daniluk et al. [56] tested several networks using variations of attention mechanisms. The first network had a simple attention mechanism, which was not fully connected, having a window length of five. They hypothesized that using a single value to predict the next token, encode information for the attentional unit, and decode the information in the attentional unit hinders a network, as it is difficult to train a single parameter to perform three distinct tasks simultaneously. Therefore, in the second network, they designed each node to have two outputs: one to encode and decode the information in the attentional unit, and another to predict the next tokens explicitly. In the third network, they further separated the outputs, using separate values to encode the information entering the attentional unit and decode the information being retrieved from it. Tests on a Wikipedia corpus showed that the attention mechanism improved perplexity compared to the baseline and that successively adding the second and third parameters led to further increases. It was also noted that only the previous five or so tokens carried much value (hence the selection of the window size of five). Therefore, they tested a fourth network that simply used residual connections from each of the previous five units. It was found that this network also provided results comparable to many larger RNNs and LSTMs, suggesting that reasonable results can be achieved using simpler networks.

Another recent study was done on the usage of residual memory networks (RMNs) for language modeling [57]. The authors found that residual connections skipping two layers were most effective, followed closely by those skipping a single layer. In particular, a residual connection was present between the first layer and the fourth layer, as was between the fifth layer and the eighth, and between the ninth and the twelfth. It was found that increasing network depth improved results, but that when using large batch sizes, memory constraints were encountered. Network width was not found to be of particular importance for performance; however, wide networks were found to be harder to train. It was found that RMNs are capable of outperforming LSTMs of similar size.

4) Convolutional Neural Networks in Language Modeling:
A CNN used recently in language modeling replaced the pooling layers with fully connected layers [58]. These layers allowed the feature maps to be reduced to lower dimensional spaces just like the pooling layers. However, whereas any references to the location of such features are lost in pooling layers, fully connected layers somewhat retain this information. Three different architectures were implemented: a multilayer perceptron CNN (MLPConv) in which the filters were not simply linear, but instead small MLPs [59]; a multilayer CNN (ML-CNN) in which multiple convolutional layers were stacked on top of each other; and a combination of these networks called COM, in which kernel sizes for filters varied (in this case, they were three and five). The results showed that stacking convolutional layers was detrimental in language modeling, but both MLPConv and COM reduced perplexity. Combining MLPConv with the varying kernel sizes of COM provided even better results. Analysis showed that the networks learned specific patterns of words, such as, “as... as.” Finally, this study showed that CNNs can be used to capture long-term dependencies in sentences. Closer words were found to be of greatest importance, but words located farther away were of some significance as well.

5) Character-Aware Neural Language Models:
While most CNNs used in NLP receive word embeddings (see Section III-A6) as input, recent networks have analyzed character-level input instead. For example, the network of Kim et al. [60], unlike previous networks [61], accepted only character-level input, rather than combining it with word embeddings. A CNN was used to process the character-level input to provide the representations of the words. In a manner similar to how word embeddings usually are these representations were then fed into an encoder–decoder pair composed of a highway network (a gated network resembling an LSTM) [46] and an LSTM. They trained the network on the English PTB, as well as on data sets for Czech, German, Spanish, French, Russian, and Arabic. For every non-English language except Russian, the network outperformed previously published results [61] in both the large and small data sets. On the PTB, the results were produced on par with the existing state of the art [62]. However, the network had only 19 million trainable parameters, which is considerably lower than others. Since the network focused on morphological similarities produced by character-level analysis, it was more capable than previous models of handling rare words. Analysis showed that without the use of highway layers, many words had nearest neighbors that were orthographically similar but not necessarily semantically similar. In addition, the network was capable of recognizing misspelled words or words not spelled in the standard way (e.g., looooook instead of look) and of recognizing out of vocabulary words. The analysis also showed that the network was capable of identifying prefixes, roots, and suffixes, as well as understanding hyphenated words, making it a robust model.

Jozefowicz et al. [63] tested a number of architectures producing character-level outputs [55], [64]–[65][66]. While many of these models had only been tested on small-scale language modeling, this study tested them on a large scale, testing them with the Billion Word Benchmark. The most effective model, achieving a state-of-the-art (for single models) perplexity of 30.0 with 1.04 billion trainable parameters (compared to a previous best by a single model of 51.3 with 20 billion parameters [55]), was a large LSTM using a character-level CNN as an input network. The best performance, however, was achieved using an ensemble of ten LSTMs. This ensemble, with a perplexity of 23.7, far surpassed the previous state-of-the-art ensemble [65], which had a perplexity of 41.0.

6) Development of Word Embeddings:
Not only do neural language models allow for the prediction of unseen synonymous words, but also they allow for modeling the relationships between the words [67], [68]. Vectors with numeric components, representing individual words, obtained by language modeling techniques are called embeddings. This is usually done either by the use of principle component analysis or by capturing internal states in a neural language model. (Note that these are not standard language modelings, but rather are language modelings constructed specifically for this purpose.) Typically, word embeddings have between 50 and 300 dimensions. An overused example is that of the distributed representations of the words king, queen, man, and woman. If one takes the embedding vectors for each of these words, computation can be performed to obtain highly sensible results. If the vectors representing these words are, respectively, represented as k⃗  , q⃗  , m⃗  , and w⃗  , it can be observed that k⃗ −q⃗ ≈m⃗ −w⃗  , which is extremely intuitive to human reasoning. In recent years, word embeddings have been the standard form of input to NLP systems.

7) Recent Advances and Challenges:
Language modeling has been evolving on a weekly basis, beginning with the works of Radford et al. [69] and Peters et al. [70]. Radford et al. [69] introduced generative pretraining (GPT) which pretrained a language model based on the transformer model [42] (Section IV-G), learning dependencies of words in sentences and longer segments of text, rather than just the immediately surrounding words. Peters et al. [70] incorporated bidirectionalism to capture backward context in addition to the forward context, in their Embeddings from Language Models (ELMo). In addition, they captured the vectorizations at multiple levels, rather than just the final layer. This allowed for multiple encodings of the same information to be captured, which was empirically shown to significantly boost the performance.

Devlin et al. [71] added the additional unsupervised training tasks of random masked neighbor word prediction and next-sentence-prediction (NSP), in which given a sentence (or other continuous segment of text), another sentence was predicted to either be the next sentence or not. These Bidirectional Encoder Representations from Transformers (BERT) were further built upon by Liu et al. [72] to create multitask DNN (MT-DNN) representations, which are the current state of the art in language modeling. The model used a stochastic answer network (SAN) [73], [74] ontop of a BERT-like model. After pretraining, the model was trained on a number of different tasks before being fine-tuned to the task at hand. Using MT-DNN as the language modeling, they achieved state-of-the-art results on ten out of eleven of the attempted tasks.

While these pretrained models have made excellent headway in “understanding” language, as is required for some tasks such as entailment inference, it has been hypothesized by some that these models are learning templates or syntactic patterns present within the data sets, unrelated to logic or inference. When new data sets are created by removing such patterns carefully, the models do not perform well [75]. In addition, while there has been recent work on cross-language modeling and universal language modeling, the amount and level of work need to pick up to address low-resource languages.

B. Morphology
Morphology is concerned with finding segments within single words, including roots and stems, prefixes, suffixes, and—in some languages—infixes. Affixes (prefixes, suffixes, and infixes) are used to overtly modify stems for gender, number, person, and so on.

Luong et al. [76] constructed a morphologically aware language modeling. An RvNN was used to model the morphological structure. A neural language model was then placed on top of the RvNN. The model was trained on the WordSim-353 data set [77], and segmentation was performed using Morfessor [78]. Two models were constructed—one using context and one not. It was found that the model that was insensitive to context overaccounted for certain morphological structures. In particular, words with the same stem were clustered together even if they were antonyms. The context-sensitive model performed better, noting the relationships between the stems but also accounting for other features such as the prefix “un.” The model was also tested on several other popular data sets [79]–[80][81], significantly outperforming previous embedding models on all.

A good morphological analyzer is often important for many NLP tasks. As such, one recent study by Belinkov et al. [82] examined the extent to which morphology was learned and used by a variety of neural machine translation (NMT) models. A number of translation models were constructed, all translating from English to French, German, Czech, Arabic, or Hebrew. Encoders and decoders were LSTM-based models (some with attention mechanisms) or character aware CNNs, and the models were trained on the WIT3 corpus [83], [84]. The decoders were then replaced with POS taggers and morphological taggers, fixing the weights of the encoders to preserve the internal representations. The effects of the encoders were examined as were the effects of the decoders attached during training. The study concluded that the use of attention mechanisms decreases the performance of encoders but increases the performance of decoders. Furthermore, it was found that character-aware models are superior to others for learning morphology and that the output language affects the performance of the encoders. Specifically, the more morphologically rich the output language, the worse the representations created by the encoders.

Morita et al. [85] analyzed a new morphological language model for unsegmented languages such as Japanese. They constructed an RNN-based model with a beam search decoder and trained it on an automatically labeled [86] corpus and a manually labeled corpus. The model performed a number of tasks jointly, including morphological analysis, POS tagging, and lemmatization. The model was then tested on the Kyoto Text Corpus [87] and the Kyoto University Web Document Leads Corpus [88], outperforming all baselines on all tasks.

A recent line of work in morphology is universal morphology. This task considers the relationships between the morphologies of different languages and how they relate to each other, aiming toward the ultimate goal of a single morphological analyzer. However, to the authors’ knowledge, there has been only a single study applying deep learning to this area [89] and, even then, only as a supporting task to universal parsing (Section III-C4). For those wishing to apply deep learning to this task, several data sets are already available, including one from a CoNLL shared task [90].

In addition to universal morphology, the development of morphological embeddings, which considers the structures of words could aid in multilanguage processing. They could possibly be used across cognate languages, which would be valuable when some languages are more resourced than others. In addition, morphological structures may be important in handling specialized language, such as that used in biomedical literature. Since deep learning has become quite entrenched in NLP, better handling of morphological components is likely to improve the performance of overall models.

C. Parsing
Parsing examines how different words and phrases relate to each other within a sentence. There are at least two distinct forms of parsing: constituency parsing and dependency parsing [48]. In constituency parsing, phrasal constituents are extracted from a sentence in a hierarchical fashion. Dependency parsing looks at the relationships between the pairs of individual words.

Most recent uses of deep learning in parsing have been in dependency parsing, within which there exists another major divide in types of solutions. Graph-based parsing constructs a number of parse trees that are then searched to find the correct one. Most graph-based approaches are generative models, in which a formal grammar, based on the natural language, is used to construct the trees [48]. More popular in recent years than graph-based approaches have been transition-based approaches that usually construct only one parse tree. While a number of modifications have been proposed, the standard method of transition-based dependency parsing is to create a buffer containing all of the words in the sentence and stack containing only the ROOT label. Words are then pushed onto the stack, where connections, known as arcs, are made between the top two items. Once dependencies have been determined, words are popped off the stack. The process continues until the buffer is empty and only the ROOT label remains on the stack. Three major approaches are used to regulate the conditions in which each of the previously described actions takes place. In the arc-standard approach [91], [92], all dependents are connected to a word before the word is connected to its parent. In the arc-eager approach [91], [92], words are connected to their parents as soon as possible, regardless of whether or not their children are all connected to them. Finally, in the swap-lazy approach [93], the arc-standard approach is modified to allow swapping of positions on the stack. This makes the graphing of nonprojective edges possible.

1) Early Neural Parsing:
One early application of deep learning to NLP, that of Socher et al. [94], [95], included the use of RNNs with probabilistic context-free grammars (PCFGs) [96], [97]. As far as the authors are aware, the first neural model to achieve state-of-the-art performance in parsing was that of Le and Zuidema [98]. Such performance was achieved on the PTB for both labeled attachment score (LAS) and unlabeled attachment score (UAS) by using an inside-out recursive neural network, which used two vector representations (an inner and an outer) to allow both top-down and bottom-up flows of data. Vinyals et al. [99] created an LSTM with an attention mechanism in a syntactic constituency parser, which they tested on data from domains different from those of the test data (the English Web Treebank [100] and the Question Treebank [101] as opposed to the Wall Street Journal portion of the PTB [54]), showing that neural models can generalize between domains. Embeddings were first used in dependency parsing by Stenetorp [102]. This approach used an RNN to create a directed acyclic graph. While this model did produce results within 2% of the state of the art (on the Wall Street Journal portion of the CoNLL 2008 Shared Task data set [103]), by the time it reached the end of a sentence, it seemed to have difficulty in remembering phrases from early in the sentence.

2) Transition-Based Dependency Parsing:
Chen and Manning [104] pushed the state of the art in both UAS and LAS on both English and Chinese data sets on the English PTB. They accomplished this by using a simple FFNN as the decision-maker in a transition-based parser. By doing so, they were able to subvert the problem of sparsity persistent in the statistical models.

Chen and Manning used a simple greedy search, which was replaced by Zhou et al. [105] with a beam search, achieving a significant improvement. Weiss et al. [106] improved upon Chen and Manning’s work by using a deeper neural network with residual connections and a perceptron layer placed after the softmax layer. They were able to train on significantly more examples than typical by using tritraining [107], a process in which potential data samples are fed to two other parsers, and those samples upon which both of the parsers agree are used for training the primary parser.

Another model was produced using an LSTM instead of a feedforward network [108]. Unlike previous models, this model was given knowledge of the entire buffer and the entire stack and had knowledge of the entire history of transition decisions. This allowed for better predictions, generating state-of-the-art scores on the Stanford Dependency Treebank [109], as well as state-of-the-art results on the CTB5 Chinese data set [110]. Finally, Andor et al. [111] used a feedforward network with global normalization on a number of tasks, including POS tagging, sentence compression, and dependency parsing. State-of-the-art results were obtained on all tasks on the Wall Street Journal data set. Notably, their model required significantly less computation than comparable models.

Much like Stenentorp [102], Wang et al. [112] used an alternative algorithm to produce directed acyclic graphs, for a task called semantic parsing, where deeper relationships between the words are found. The task seeks to identify what types of actions are taking place and how words modify each other. In addition to the typical stack and buffer used in transition-based parsing, the algorithm employed a deque. This allowed for the representation of multiparented words, which although rare in English, are common in many natural languages. Furthermore, it allowed for multiple children of the ROOT label. In addition to producing said graphs, this article is novel in its use of two new LSTM-based techniques: Bi-LSTM subtraction and incremental Tree-LSTM. Bi-LSTM subtraction built on previous work [41], [113] to represent the buffer as a subtraction of the vectors from the head and tail of the LSTM, in addition to using an additional LSTM to represent the deque. Incremental Tree-LSTM is an extension of Tree-LSTM [114], modified for directed acyclic graphs, by connecting children to parents incrementally, rather than connecting all children to a parent simultaneously. The model achieved the best published scores at the time for 14 of the 16 evaluation metrics used on SemEval-2015 Task 18 (English) [115] and SemEval-2016 Task 9 (Chinese) [116]. While deep learning had been applied to semantic parsing in particular domains, such as QA [117], [118], to the authors’ knowledge, this was the first time it was applied in large scale to semantic parsing as a whole.

3) Generative Dependency and Constituent Parsing:
Dyer et al. [119] proposed a model that used RNN grammars for parsing and language modeling. While most approaches take a bottom–up approach to parsing, this took a top–down approach, taking as input the full sentence in addition to the current parse tree. This allowed the sentence to be viewed as a whole, rather than simply allowing local phrases within it to be considered. This model achieved the best results in English generative parsing as well as in single sentence language modeling. It also attained results close to the best in Chinese generative parsing.

Choe and Charniak [120] treated parsing as a language modeling problem and used an LSTM to assign probabilities to the parse trees, achieving state of the art. Fried et al. [121] wanted to determine whether the power of the models came from the reranking process or simply from the combined power of two models. They found that while using one parser for producing candidate trees and another for ranking them was superior to a single parser approach, combining two parsers explicitly was preferable. They used two parsers to both select the candidates and rerank them, achieving state-of-the-art results. They extended this model to use three parsers, achieving even better results. Finally, an ensemble of eight such models (using two parsers) was constructed and achieved the best results on PTB at the time.

A model created by Dozat and Manning [122] used a graph-based approach with a self-attentive network. Similarly, Tan et al. [123] used a self-attentional model for semantic role labeling and a subtask of semantic parsing, achieving excellent results. They experimented with recurrent and convolutional replacements to the feedforward portions of the self-attention mechanism, finding that the feedforward variant had the best performance. Another novel approach is that of Duong et al. [124], who used active learning. While not perfect, this is a possible solution to one of the biggest problems in semantic parsing—the availability of data.

4) Universal Parsing:
Much like universal morphology, universal dependency parsing, or universal parsing, is the relatively new task of parsing language using a standardized set of tags and relationships across all languages. While current parsing varies drastically from language to language, this attempts to make it uniform between them, in order to allow for easier processing between and among them. Nivre [125] discussed the recent development of universal grammar and presented the challenges that lie ahead, mainly the development of tree banks in more languages and the consistency of labeling between tree banks in different (and even the same) languages. This task has gained traction in large part because it has been a CoNLL shared task for the past two years [126]. A number of approaches from the 2018 task included using deep transition parsing [127], graph-based neural parsing [128], and a competitive model, which used only a single neural model, rather than an ensemble [129]. The task has begun to be examined outside of CoNLL, with Liu et al. [130] applying universal dependencies to the parsing of tweets, using an ensemble of bidirectional LSTM.

5) Remaining Challenges:
Outside of universal parsing, a parsing challenge that needs to be further investigated is the building of syntactic structures without the use of treebanks for training. Attempts have been made using attention scores and Tree-LSTMs, as well as “outside-inside” autoencoders. If such approaches are successful, they have the potential use in many environments, including in the context of low-resource languages and out-of-domain scenarios. While a number of other challenges remain, these are the largest and are expected to receive the most focus.

D. Semantics
Semantic processing involves understanding the meaning of words, phrases, sentences, or documents at some level. Word embeddings, such as Word2Vec [67], [68] and GloVe [131], claim to capture meanings of words, following the distributional hypothesis of meaning [132]. As a corollary, when vectors corresponding to phrases, sentences, or other components of text are processed using a neural network, a representation that can be loosely thought to be semantically representative is computed compositionally. In this section, neural semantic processing research is separated into two distinct areas: work on comparing the semantic similarity of two portions of text and work on capturing and transferring meaning in high-level constituents, particularly sentences.

1) Semantic Comparison:
One way to test the efficacy of an approach to computing semantics is to see if two similar phrases, sentences, or documents, judged by humans to have similar meaning also are judged similarly by a program.

Hu et al. [133] proposed two CNNs to perform a semantic comparison task. The first model, ARC-I, inspired by Bordes et al. [134], used a Siamese network, in which two CNNs sharing weights evaluated two sentences in parallel. In the second network, connections were placed between the two, allowing for sharing before the final states of the CNNs. The approach outperformed a number of existing models in tasks in English and Chinese.

Building on prior work [21], [26], [133], Yin and Schütze [135] proposed a Bi-CNN-MI (MI for multigranular interaction features), consisting of a pretrained CNN sentence model, a CNN interaction model, and a logistic regressor. They modified a Siamese network using dynamic CNNs [21] (Section III-D2). In addition, the feature maps from each level were used in the comparison, rather than simply the top-level feature maps. They achieved state-of-the-art results on the Microsoft Research Paraphrase Corpus (MSRP) [136].

He et al. [137] constructed feature maps, which were then compared using a “similarity measurement layer” followed by a fully connected layer and then a log-softmax output layer within a CNN. The windows used in the convolutional layers ranged in length from one to four. The network was trained and evaluated on three data sets: MSRP, the Sentences Involving Compositional Knowledge (SICK) data set [138], and the Microsoft Video Paraphrase Corpus (MSRVID) [139]. State-of-the-art results were achieved on the first and the third.

Tai et al. [114] concocted a model using an RvNN with LSTM-like nodes called a Tree-LSTM. Two variations were examined (constituency- and dependency-based) and tested on both the SICK data set and Stanford Sentiment Treebank [94]. The constituency-based model achieved state-of-the-art results on the Stanford Sentiment Treebank and the dependency-based one achieved state-of-the-art results on SICK.

He and Lin [140] presented another model, which outperformed that of Tai et al. on SICK. The model formed a matrix of the two sentences before applying a “similarity focus layer” and then a 19-layer CNN followed by dense layers with a softmax output. The similarity focus layer matched semantically similar pairs of words from the input sentences and applied weights to the matrix locations representing the relations between the words in each pair. They also obtained state-of-the-art resuults on MSRVID, SemEval 2014 Task 10 [141], WikiQA [142], and TreeQA [143] data sets.

2) Sentence Modeling:
Extending from neural language modeling, sentence modeling attempts to capture the meaning of sentences in vectors. Taking this a step further are models, such as that of Le and Mikolov [144], which attempt to model paragraphs or larger bodies of text in this way.

Kalchbrenner et al. [21] generated the representations of sentences using a dynamic convolutional neural network (DCNN), which used a number of filters and dynamic k -max-pooling layers. Due to dynamic pooling, features of different types and lengths could be identified in sentences with varying structures without padding of the input. This allowed not only short-range dependencies but also long-range dependencies to be identified. The DCNN was tested in applied tasks that require semantic understanding. It outperformed all comparison models in predicting sentiment of movie reviews in the Stanford Sentiment Treebank [95] and in identification of sentiment in tweets [145]. It was also one of the top performers in classifying types of questions using the TREC database [146].

Between their requirement for such understanding and their ease of examination due to the typical encoder–decoder structure they use, NMT systems (Section IV-G) are splendid testbeds for researching internal semantic representations. Poliak et al. [147] trained encoders on four different language pairs: English and Arabic, English and Spanish, English and Chinese, and English and German. The decoding classifiers were trained on four distinct data sets: Multi NLI [148], which is an expanded version of SNLI [149], as well as three recast data sets from the JHU Decompositional Semantics Initiative [150] (FrameNet Plus or FN+ [151], Definite Pronoun Resolution or DPR [152], and Semantic Proto-Roles or SPR [153]). None of the results were particularly strong, although they were strongest in SPR. This led to the conclusion that NMT models do a poor job of capturing paraphrased information and fail to capture inferences that help in anaphora resolution (e.g., resolving gender). They did, however, find that the models learn about protoroles (e.g., who or what is the recipient of an action). A concurrent work [154] analyzed the quality of many data sets used for natural language inference.

Herzig and Berant [155] found that training semantic parsers on a single domain, as is often done, is less effective than training across many domains. This conclusion was drawn after testing three LSTM-based models. The first model was a one-to-one model, in which a single encoder and a single decoder were used, requiring the network itself to determine the domain of the input. In the second model, a many-to-many model, a decoder was used for each domain, as were two encoders: the domain-specific encoder and a multidomain encoder. The third model was a one-to-many model, using a single encoder but separate decoders for each domain. Each model was trained on the “OVERNIGHT” data set [156]. Exceptional results were achieved for all models, with a state-of-the-art performance exhibited by the one-to-one model.

Similar conclusions were drawn by Brunner et al. [157]. who created several LSTM-based encoder–decoder networks and analyzed the embedding vectors produced. A single encoder accepting English sentences as input was used, as were four different decoders. The first such decoder was a replicating decoder, which reproduced the original English input. The second and third decoders translated the text into German and French. Finally, the fourth decoder was a POS tagger. Different combinations of decoders were used; one model had only the replicating decoder, while others had two, three, or all four. Sentences of 14 different structures from the EuroParl data set [158] were used to train the networks. A set of test sentences were then fed to the encoders and their output analyzed. In all cases, 14 clusters were formed, each corresponding to one of the sentence structures. Analysis showed that adding more decoders led to more correct and more definitive clusters. In particular, using all four of the decoders led to zero error. Furthermore, the researchers confirmed a hypothesis that just as logical arithmetic can be performed on word embeddings, so can it be performed on sentence embeddings.

3) Semantic Challenges:
In addition to the challenges already mentioned, researchers believe that being able to solve tasks well does not indicate actual understanding. Integrating deep networks with general word graphs (e.g., WordNet [159]) or knowledge graphs (e.g., DBPedia [160]) may be able to endow a sense of understanding. Graph embedding is an active area of research [161], and work on integrating language-based models and graph models has only recently begun to take off, giving hope for better machine understanding.

E. Summary of Core Issues
Deep learning has generally performed very well, surpassing existing states of the art in many individual core NLP tasks and has thus created the foundation on which useful natural language applications can and are being built. However, it is clear from examining the research reviewed here that natural language is an enigmatically complex topic, with myriad core or basic tasks, of which deep learning has only grazed the surface. It is also not clear how architectures for ably executing individual core tasks can be synthesized to build a common edifice, possibly a much more complex distributed neural architecture, to show competence in multiple or “all” core tasks. More fundamentally, it is also not clear, how mastering of basic tasks, may lead to superior performance in applied tasks, which are the ultimate engineering goals, especially in the context of building effective and efficient deep learning models. Many, if not most, successful deep learning architectures for applied tasks, discussed in Section IV, seem to forgo explicit architectural components for core tasks and learn such tasks implicitly. Thus, some researchers argue that the relevance of the large amount of work on core issues is not fully justified, while others argue that further extensive research in such areas is necessary to better understand and develop systems which more perfectly perform these tasks, whether explicitly or implicitly.

SECTION IV.Applications of NLP Using Deep Learning
While the study of core areas of NLP is important to understanding how neural models work, it is meaningless in and of itself from an engineering perspective, which values the applications that benefit humanity, not pure philosophical and scientific inquiry. Current approaches to solving several immediately useful NLP tasks are summarized here. Note that the issues included here are only those involving the processing of text, not the processing of verbal speech. Because speech processing [162], [163] requires expertise on several other topics including acoustic processing, it is generally considered another field of its own, sharing many commonalities with the field of NLP. The number of studies in each discussed area over the last decade is shown in Fig. 4.

Fig. 4. - Publication volume for applied areas of NLP. All areas of applied NLP discussed have witnessed growth in recent years, with the largest growth occurring in the last two to three years.
Fig. 4.
Publication volume for applied areas of NLP. All areas of applied NLP discussed have witnessed growth in recent years, with the largest growth occurring in the last two to three years.

Show All

A. Information Retrieval
The purpose of IR systems is to help people find the right (most useful) information in the right (most convenient) format at the right time (when they need it) [164]. Among many issues in IR, a primary problem that needs addressing pertains to ranking documents with respect to a query string in terms of relevance scores for ad hoc retrieval tasks, similar to what happens in a search engine.

Deep learning models for ad hoc retrieval match texts of queries to texts of documents to obtain relevance scores. Thus, such models have to focus on producing representations of the interactions among individual words in the query and the documents. Some representation-focused approaches build deep learning models to produce good representations for the texts and then match the representations straightforwardly [133], [165], [166], whereas interaction-focused approaches first build local interactions directly and then use DNNs to learn how the two pieces of text match based on word interactions [133], [167], [168]. When matching a long document to a short query, the relevant portion can potentially occur anywhere in the long document and may also be distributed, thus, finding how each word in the query relates to portions of the document is helpful.

Mindful of the specific needs for IR, Guo et al. [169] built a neural architecture called DRMM, enhancing an interaction-focused model that feeds quantized histograms of the local interaction intensities to an MLP for matching. In parallel, the query terms go through a small subnetwork on their own to establish term importance and term dependencies. The outputs of the two parallel networks are mixed at the top so that the relevance of the document to the query can be better learned. DRMM achieved the state-of-the-art performance for its time.

Most current neural IR models are not end-to-end relevance rankers, but are rerankers for documents a first-stage efficient traditional ranker has deemed relevant to a query. The representations that the neural rerankers learn are dense for both documents and queries, i.e., most documents in a collection seem to be relevant to a query, making it impossible to use such ANNs for ranking an entire collection of documents. In contrast, Zamani et al. [170] presented a standalone neural ranking model called SNRM_PRF that learned sparse representations for both queries and documents, mimicking what traditional approaches do. Since queries are much shorter than documents and queries contain much less information than documents, it makes sense for query representations to be denser. This was achieved by using, during training, a sparsity objective combined with hinge loss. In particular, an n -gram representation for queries and documents was used. It passed the embedding of each word separately through an individual MLP and performed average pooling on top. During training, the approach used pseudorelevant documents obtained by retrieving documents using the existing models such as TF-IDF and BM25, because of the lack of enough correctly labeled documents to train large ANN models. The approach created a 20 000-bit-long inverted index for each document using the trained network, just like a traditional end-to-end approach. For retrieval, a dot product was computed between query and document representations to obtain the retrieval relevance score. The SNRM_PRF system obtained the best metrics (measured by MAP, P@20, nDCG@20, and Recall) across the board for two large data sets, Robust and ClueWeb.

MacAveney et al. [171] extracted query term representations from two pretrained contextualized language models, ELMo [70] and BERT [71], and used the representations to augment three existing competitive neural ranking architectures for ad hoc document ranking, one of them being DRMM [169]. They also presented a joint model that combined BERT’s classification vector with these architectures to get benefits from both approaches. MacAveney’s system called contextualized embeddings for document ranking (CEDR) improved the performance of all three prior models and produced state-of-the-art results using BERT’s token representations.

B. Information Extraction
Information extraction extracts explicit or implicit information from the text. The outputs of systems vary, but often, the extracted data and the relationships within it are saved in relational databases [172]. Commonly extracted information includes named entities and relations, events and their participants, temporal information, and tuples of facts.

1) Named Entity Recognition:
Named entity recognition (NER) refers to the identification of proper nouns as well as information such as dates, times, prices, and product IDs. The multitask approach of Collobert et al. [9] included the task although no results were reported. In their approach, a simple feedforward network was used, having a context with a fixed-sized window around each word. Presumably, this made it difficult to capture long-distance relations between the words.

LSTMs were first used for NER by Hammerton [173]. The model, which was ahead of its time, had a small network due to the lack of available computing power at the time. In addition, sophisticated numeric vector models for words were not yet available. Results were slightly better than the baseline for English and much better than the baseline for German. Dos Santos et al. [174] used a DNN architecture, known as CharWNN, which jointly used word-level and character-level inputs to perform sequential classification. In this article, a number of experiments were performed using the HAREM I annotated Portuguese corpus [175], and the SPA CoNLL2002 annotated Spanish corpus [176]. For the Portuguese corpus, CharWNN outperformed the previous state-of-the-art system across ten named entity classes. It also achieved state-of-the-art performance in Spanish. The authors noted that when used alone, neither word embeddings nor character level embeddings worked. This revalidated a fact long known: joint use of word-level and character-level features is important to effective NER performance.

Chiu and Nichols [177] used a bidirectional LSTM with a character-level CNN resembling those used by dos Santos and Guimarães [174]. Without using any private lexicons, detailed information about linked entities, or elaborate hand-crafted features they produced state-of-the-art results on the CoNLL-2003 [178] and OntoNotes [179], [180] data sets.

Lample et al. [181] developed an architecture based on bidirectional LSTMs and conditional random fields. The model used both character-level inputs and word embeddings. The inputs were combined and then fed to a bidirectional LSTM, whose outputs were in turn fed to a layer that performed CRF computations [182]. The model, when trained using dropout, obtained state-of-the-art performance in both German and Spanish. The LSTM-CRF model was also very close in both English and Dutch. The main claim of this study was that state-of-the-art results were achieved without the use of any hand-engineered features or gazetteers.

Akbik et al. [183] achieved the state-of-the-art performance in German and English NER using a pretrained bidirectional character language model. They retrieved for each word a contextual embedding that they passed into a BiLSTM-CRF sequence labeler to perform NER.

2) Event Extraction:
Event extraction is concerned with identifying words or phrases that refer to the occurrence of events, along with participants such as agents, objects, recipients, and times of occurrence. Event extraction usually deals with four subtasks: identifying event mentions, or phrases that describe events; identifying event triggers, which are the main words—usually verbs or gerunds—that specify the occurrence of the events; identifying arguments of the events; and identifying arguments’ roles in the events.

Chen et al. [184] argued that CNNs that use max-pooling are likely to capture only the most important information in a sentence, and as a result, might miss valuable facts when considering sentences that refer to several events. To address this drawback, they divided the feature map into three parts, and instead of using one maximum value, kept the maximum value of each part. In the first stage, they classified each word as either being a trigger word or nontrigger word. If triggers were found, the second stage aligned the roles of arguments. Results showed that this approach significantly outperformed other state-of-the-art methods of the time. The following year, Nguyen et al. [185] used an RNN-based encoder–decoder pair to identify event triggers and roles, exceeding earlier results. Liu et al. [186] presented a latent variable neural model to induce event schemas and extract open domain events, achieving the best results on a data set they created and released.

3) Relationship Extraction:
Another important type of information extracted from the text is that of relationships. These may be possessive, antonymous, or synonymous relationships, or more natural, familial, or geographic relationships. The first deep learning approach was that of Zeng et al. [23], who used a simple CNN to classify a number of relationships between the elements in sentences. Using only two layers, a window size of three and word embeddings with only 50 dimensions, they attained better results than any prior approach. Further work, by Zheng et al. [187], used a bidirectional LSTM and a CNN for relationship classification as well as entity recognition. More recently, Sun et al. [188] used an attention-based GRU model with a copy mechanism. This network was novel in its use of a data structure known as a coverage mechanism [189], which helped ensure that all important information was extracted the correct number of times. Lin et al. [190] achieved the state-of-the-art performance in clinical temporal relation extraction using the pretrained BERT [71] model with supervised training on a biomedical data set.

C. Text Classification
Another classic application for NLP is text classification or the assignment of free-text documents to predefined classes. Document classification has numerous applications.

Kim [20] was the first to use pretrained word vectors in a CNN for sentence-level classification. Kim’s work was motivating, and showed that simple CNNs, with one convolutional layer followed by a dense layer with dropout and softmax output, could achieve excellent results on multiple benchmarks using little hyperparameter tuning. The CNN models proposed were able to improve upon the state of the art on four out of seven different tasks cast as sentence classification, including sentiment analysis and question classification. Conneau et al. [191] later showed that networks that employ a large number of convolutional layers work well for document classification.

Jiang et al. [192] used a hybrid architecture combining a deep belief network [193] and softmax regression [194]. (A deep belief network is a feedforward network where pairs of hidden layers are designed to resemble restricted Boltzmann machines [195], which are trained using unsupervised learning and are designed to increase or decrease dimensionality of data.) This was achieved by making passes over the data using forward and backward propagation many times until a minimum engery-based loss was found. This process was independent of the labeled or classification portion of the task and was therefore initially trained without the softmax regression output layer. Once both sections of the architecture were pretrained, they were combined and trained such as a regular deep neural net with backpropagation and quasi-Newton methods [196].

Adhikari et al. [197] used BERT [71] to obtain state-of-the-art classification results on four document data sets.

While deep learning is promising for many areas of NLP, including text classification, it is not necessarily the end-all-be-all, and many hurdles are still present. Worsham and Kalita [198] found that for the task of classifying long full-length books by genre, gradient boosting trees are superior to neural networks, including both CNNs and LSTMs.

D. Text Generation
Many NLP tasks require the generation of human-like language. Summarization and machine translation convert one text to another in a sequence-to-sequence (seq2seq) fashion. Other tasks, such as image and video captioning and automatic weather and sports reporting, convert nontextual data to text. Some tasks, however, produce text without any input data to convert (or with only small amounts used as a topic or guide). These tasks include poetry generation, joke generation, and story generation.

1) Poetry Generation:
Poetry generation is arguably the hardest of the generation subtasks, as in addition to producing creative content, the content must be delivered in an esthetic manner, usually following a specific structure. As with most tasks requiring textual output, recurrent models are the standard. However, while recurrent networks are great at learning internal language models, they do a poor job of producing structured output or adhering to any single style. Wei et al. [199] addressed the style issue by training using particular poets and controlling for style in Chinese poetry. They found that with enough training data, adequate results could be achieved. The structure problem was addressed by Hopkins and Kiela [200], who generated rhythmic poetry by training the network on only a single type of poem to ensure the produced poems adhered to a single rhythmic structure. Human evaluators judged poems produced to be of lower quality than, but indistinguishable from, human-produced poems.

Another approach to poetry generation, beginning this year, has been to use pretrained language models. Specifically, Radford et al.’s GPT-2 model [201], the successor of the GPT model (Section III-A7), has been used. Radford et al. [201] hypothesized that alongside sequence-to-sequence learning and attention, language models can inherently start to learn text generation while training over a vast data set. As of late 2019, these pretrained GPT-2 models are arguably the most effective and prolific neural natural language generators. Bena and Kalita [202] used the 774 million parameter GPT-2 model to generate high-quality poems in English, demonstrating and eliciting emotional response in readers. (Two other GPT-2 models are available: 355 million parameters, and as of Novemeber 2019, 1.5 billion parameters.) Tucker and Kalita [203] generated poems in several languages—English, Spanish, Ukrainian, Hindi, Bengali, and Assamese—using the 774 M model as well. This study provided astonishing results in the fact that GPT-2 was pretrained on a large English corpus, yet with further training on only a few hundred poems in another language, it turns into a believable generator in that language, even for poetry.

2) Joke and Pun Generation:
Another area, which has received little attention, is the use of deep learning for joke and pun generation. Yu et al. [204] generated homographic puns (puns that use multiple meanings of the same written word) using a small LSTM. The network produced sentences in which ambiguities were introduced by words with multiple meanings although it did a poor job of making the puns humorous. The generated puns were classified by human evaluators as machine generated a majority of the time. The authors noted that training on pun data alone is not sufficient for generating good puns. Ren and Yang [205] used an LSTM to generate jokes, training on two data sets, one of which was a collection of short jokes from Conan O’Brien. Since many of these jokes pertain to current events, the network was also trained on a set of news articles. This gave context to the example jokes. Chippada and Saha [206] generated jokes, quotes, and tweets using the same neural network, using an additional input to specify which should be produced. It was found that providing more general knowledge of other types of language, and examples of nonjokes, increased the quality of the jokes produced.

3) Story Generation:
While poetry and especially humor generation have not gained much traction, story generation has seen a recent rise in interest. Jain et al. [207] used RNN variants with attention to produce short stories from “one-liner” story descriptions. Another recent study of interest is that by Peng et al. [208], who used LSTMs to generate stories, providing an input to specify whether the story should have a happy or sad ending. Their model successfully did so while at the same time providing better coherence than noncontrolled stories. More recent attempts at the task have used special mechanisms focusing on the “events” (or actions) in the stories [209] or on the entities (characters and important objects) [210]. Even with such constraints, generated stories generally become incoherent or lose direction rather shortly. Xu et al. [211] addressed this by using a “skeleton”-based model to build general sentences and fill in important information. This did a great job of capturing only the most important information but still provided only modest end results in human evaluation. Drissi et al. [212] followed a similar approach.

The strongest models to date focus on creating high-level overviews of stories before breaking them down into smaller components to convert to text. Huang et al. [213] generated short stories from images using a two-tiered network. The first constructed a conceptual overview, while the second converted the overview into words. Fan et al. [214] used a hierarchical approach, based on CNNs, which beat out the nonhierarchical approach in blind comparison by human evaluators. In addition, they found that self-attention leads to better perplexity. They also developed a fusion model with a pretrained language model, leading to greater improvements. These results concur with those of an older study by Li et al. [215] who read documents in a hierarchical fashion and reproduced them in a hierarchical fashion, achieving great results.

4) Text Generation With GANs:
In order to make stories seem more human-like, Lin et al. [216] used generative adversarial networks (GANs) to measure human likeness of generated text, forcing the network toward more natural reading output. GANs are based on the concept of a minimax two-player game, in which a generative network and a discriminative network are designed to work against each other with the discriminator attempting to determine whether examples are from the generative network or the training set, and the generator trying to maximize the number of mistakes made by the discriminator. RankGAN, the GAN used in the study, measured differences in embedding space, rather than in output tokens. This meant that the story content was evaluated more directly, without respect to the specific words and grammars used to tell it. Rather than simply using standard metrics and minimizing loss, Tambwekar et al. [217] used reinforcement learning to train a text generation model. This taught the model to not only attempt to optimize metrics but also to generate stories that humans evaluated to be meaningful. Zhang et al. [218] used another modified GAN, referred to as textGAN, for text generation, employing an LSTM generator and a CNN discriminator, achieving a promising bilingual evaluation understudy (BLEU) score and a high tendency to reproduce realistic-looking sentences. GANs have seen increasing use in text generation recently [219], [220].

5) Text Generation With VAEs:
Another interesting type of network is the variational autoencoder (VAE) [221]. While GANs attempt to produce output indistinguishable (at least to the model’s discriminator) from actual samples, VAEs attempt to create output similar to samples in the training set [222]. Several recent studies have used VAEs for text generation [223], [224], including Wang et al. [225], who adapted it by adding a module for learning a guiding topic for sequence generation, producing good results.

6) Summary of Text Generation:
Humor and poetry generation are still understudied topics. As machine-generated texts improve, the desire for more character, personality, and color in the texts will almost certainly emerge. Hence, it can be expected that research in these areas will increase.

While story generation is improving, coherence is still a major problem, especially for longer stories. This has been addressed in part, by Haltzman et al. [226], who have proposed “nucleus sampling” to help counteract this problem, performing their experiments using the GPT-2 model.

In addition to issues with lack of creativity and coherence, creating metrics to measure any sort of creative task is difficult, and therefore, human evaluations are the norm, often utilizing Amazon’s Mechanical Turk. However, recent works have proposed metrics that make a large step toward reliable automatic evaluation of generated text [227], [228]. In addition to the more creative tasks surveyed here, a number of others were previously discussed by Gatt and Krahmer [229]. The use of deep learning for image captioning has been surveyed very recently [230], [231], and tasks that generate text given textual inputs are discussed in Sections IV-E–IV-G.

E. Summarization
Summarization finds elements of interest in documents in order to produce an encapsulation of the most important content. There are two primary types of summarization: extractive and abstractive. The first focuses on sentence extraction, simplification, reordering, and concatenation to relay the important information in documents using text taken directly from the documents. Abstractive summaries rely on expressing documents’ contents through generation-style abstraction, possibly using words never seen in the documents [48].

Rush et al. [39] introduced deep learning to summarization, using an FFNN. The language model used an encoder and a generative beam search decoder. The initial input was given directly to both the language model and the convolutional attention-based encoder, which determined contextual importance surrounding the summary sentences and phrases. The performance of the model was comparable to other state-of-the-art models of the time.

As in other areas, attention mechanisms have improved the performance of encoder–decoder models. Krantz and Kalita [232] compared various attention models for abstractive summarization. A state-of-the-art approach developed by Paulus et al. [40] used a multiple intratemporal attention encoder mechanism that considered not only the input text tokens but also the output tokens used by the decoder for previously generated words. They also used similar hybrid cross-entropy loss functions to those proposed by Ranzato et al. [233], which led to decreases in training and execution by orders of magnitude. Finally, they recommended using strategies seen in reinforcement learning to modify gradients and reduce exposure bias, which has been noted in models trained exclusively via supervised learning. The use of attention also boosted accuracy in the fully convolutional model proposed by Gehring et al. [234], who implemented an attention mechanism for each layer.

Zhang et al. [235] proposed an encoder–decoder framework, which generated an output sequence based on an input sequence in a two-stage manner. They encoded the input sequence using BERT [71]. The decoder had two stages. In the first stage, a transformer-based decoder generated a draft output sequence. In the second stage, they masked each word of the draft sequence and fed it to BERT, and then by combining the input sequence and the draft representation generated by BERT, they used a transformer-based decoder to predict the refined word for each masked position. Their model achieved state-of-the-art performance on the CNN/Daily Mail and New York Times data sets.

F. Question Answering
Similar to summarization and information extraction, question answering (QA) gathers relevant words, phrases, or sentences from a document. QA coherently returns this information in response to a request. Current methods resemble those of summarization.

Wang et al. [41] used a gated attention-based recurrent network to match the question with an answer-containing passage. A self-matching attention mechanism was used to refine the machine representation by mapping the entire passage. Pointer networks were used to predict the location and boundary of an answer. These networks used attention-pooling vector representations of passages, as well as the words being analyzed, to model the critical tokens or phrases necessary.

Multicolumn CNNs were used by Dong et al. [236] to automatically analyze questions from multiple viewpoints. Parallel networks were used to extract pertinent information from input questions. Separate networks were used to find context information and relationships and to determine which forms of answers should be returned. The output of these networks was combined and used to rank possible answers.

Santoro et al. [237] used relational networks (RNs) for summarization. First proposed by Raposo et al. [238], RNs are built upon an MLP architecture, with a focus on relational reasoning, i.e., defining relationships among entities in the data. These feedforward networks implement a similar function among all pairs of objects in order to aggregate correlations among them. For input, the RNs took final LSTM representations of document sentences. These inputs were further paired with a representation of the information request given [237].

BERT [71] achieved state of the art in QA experiments on the SQuAD 1.1 and SQuAD 2.0 data sets. Yang et al. [239] demonstrated an end-to-end QA system that integrates BERT with the open-source Anserini IR toolkit. This system can identify answers from a large corpus of Wikipedia articles in an end-to-end fashion, obtaining the best results on a standard benchmark test collection.

G. Machine Translation
Machine translation is the quintessential application of NLP. It involves the use of mathematical and algorithmic techniques to translate the documents in one language to another. Performing effective translation is intrinsically onerous even for humans, requiring proficiency in areas such as morphology, syntax, and semantics, as well as an adept understanding and discernment of cultural sensitivities, for both of the languages (and associated societies) under consideration [48].

The first attempt at NMT was that by Schwenk [240], although neural models had previously been used for the similar task of transliteration, converting certain parts of text, such as proper nouns, into different languages [241]. Schwenk used a feedforward network with seven-word inputs and outputs, padding and trimming when necessary. The ability to translate from a sentence of one length to a sentence of another length came about with the introduction of encoder–decoder models.

The first use of such a model, by Kalchbrenner and Blumson [242], stemmed from the success of continuous recurrent representations in capturing syntax, semantics, and morphology [243] in addition to the ability of RNNs to build robust language models [29]. This original NMT encoder–decoder model used a combination of generative convolutional and recurrent layers to encode and optimize a source language model and cast this into a target language. The model was quickly reworked and further studied by Cho et al. [244], and numerous novel and effective advances to this model have since been made [38], [245]. Encoder–decoder models have continuously defined the state of the art, being expanded to contain dozens of layers, with residual connections, attention mechanisms, and even residual attention mechanisms allowing the final decoding layer to attend to the first encoding layer [246]. State-of-the-art results have also been achieved by using numerous convolutional layers in both the encoder and decoder, allowing information to be viewed in several hierarchical layers rather than a multitude of recurrent steps [234]. Such derived models are continually improving, finding answers to the shortcomings of their predecessors and overcoming any need for hand engineering [247]. Recent progress includes effective initialization of decoder hidden states, use of conditional gated attentional cells, removal of bias in embedding layers, use of alternative decoding phases, factorization of embeddings, and test time use of the beam search algorithm [248], [249].

The standard initialization for the decoder state is that proposed by Bahdanau et al. [38], using the last backward encoder state. However, as noted by Britz et al. [247], using the average of the embedding or annotation layer seems to lead to the best translations. Gated recurrent cells have been the gold standard for sequence-to-sequence tasks, a variation of which is a conditional GRU (cGRU) [248], most effectively utilized with an attention mechanism. A cGRU cell consists of three key components: two GRU transition blocks and an attention mechanism between them. These three blocks combine the previous hidden state, along with the attention context window to generate the next hidden state. Altering the decoding process [38] from look at input, generate output token, update hidden representation to a process of look, update, and generate can simplify the final decoding. Adding further source attributes, such as morphological segmentation labels, POS tags, and syntactic dependency labels, improves models, and concatenating or factorizing these with embeddings increases robustness further [248], [250]. For remembering long-term dependencies, vertically stacked recurrent units have been the standard, with the optimum number of layers having been determined to be roughly between 2 and 16 [247], depending on the desired input length as well as the presence and density of residual connections. At test time, a beam search algorithm can be used beside the final softmax layer for considering multiple target predictions in a greedy fashion, allowing the best predictions to be found without looking through the entire hypothesis space [249].

In a direction diverging from previous work, Vaswani et al. [42] and Ahmed et al. [251] proposed discarding the large number of recurrent and convolutional layers and instead focusing exclusively on attention mechanisms to encode a language globally from input to output. Preferring such “self-attention” mechanisms over traditional layers is motivated by the following three principles: reducing the complexity of computations required per layer, minimizing sequential training steps, and, finally, abating the path length from input to output and its handicap on the learning of the long-range dependencies that are necessary in many sequencing tasks [252]. Apart from increased accuracy across translation tasks, self-attention models allow more parallelization throughout architectures, decreasing the training times and minimizing necessary sequential steps. At time of writing, the state-of-the-art model generating the best results for English to German and English to French on the International Workshop on Spoken Language Translation (IWSLT) 2014 test corpus [253] is that of Medina and Kalita [254], which modified the model proposed by Vaswani to use parallel self-attention mechanisms, rather than stacking them as was done in the original model. In addition to improving BLEU scores [255], this also reduced training times. Ghazvininejad et al. [256] recently applied BERT to the machine translation task using constant-time models. They were able to achieve relatively competitive performance in a fraction of the time. Lample and Conneau [257] attained state-of-the-art results, performing unsupervised machine translation using multiple languages in their language model pretraining.

Several of the recent state-of-the-art models were examined by Chen et al. [258]. The models were picked apart to determine which features were truly responsible for their strength and to provide a fair comparison. Hybrid models were then created using this knowledge, and incorporating the best parts of each previous model, outperforming the previous models. In addition to creating two models with both a self-attentive component and a recurrent component (in one model, they were stacked, in the other parallel), they determined four techniques that they believe should always be employed, as they are crucial to some models, at best, and neutral to all models examined, at worst. These are label smoothing, multihead attention, layer normalization, and synchronous training. Another study, by Denkowski and Neubig [259], examined a number of other techniques, recommending three: using Adam optimization, restarting multiple times, with learning rate annealing; performing subword translation; and using an ensemble of decoders. Furthermore, they tested a number of common techniques on models that were strong to begin and determined that three of the four provided no additional benefits to, or actually hurt, the model, those three being lexicon bias (priming the outputs with directly translated words), pretranslation (using translations from another model, usually of lower quality, as additional input), and dropout. They did find, however, that data bootstrapping (using phrases that are parts of training examples as additional independent smaller samples) was advantageous even to models that are already high performing. They recommended that future developments be tested on top-performing models in order to determine their realm of effectiveness.

In addition to studies presenting recommendations, one study has listed a number of challenges facing the field [260]. While neural machine translation models are superior to other forms of statistical machine translation models (as well as rule-based models), they require significantly more data, perform poorly outside of the domain in which they are trained, fail to handle rare words adequately, and do not do well with long sentences (more than about 60 words). Furthermore, attention mechanisms do not perform as well as their statistical counterparts for aligning words, and beam searches used for decoding only work when the search space is small. Surely, these six drawbacks will be, or in some cases, will continue to be, the focus of much research in the coming years. In addition, as mentioned in Section III-D2, NMT models still struggle with some semantic concepts, which will also be a likely area of focus in the upcoming years. While examining some of these failings of NMT can help, predicting the future of research and development in the field is nearly impossible.

New models and methods are being reported daily with far too many advancements to survey, and state-of-the-art practices are becoming outdated in a matter of months. Notable recent advancements include using caching to provide networks with greater context than simply the individual sentences being translated [261], the ability to better handle rare words [262], [263], and the ability to translate to and from understudied languages, such as those that are polysynthetic [264]. In addition, work has been conducted on the selection, sensitivity, and tuning of hyperparameters [265], denoising of data [266], and a number of other important topics surrounding NMT. Finally, a new branch of machine translation has been opened up by groundbreaking research: multilingual translation.

A fairly recent study [267] showed that a single, simple (but large) neural network could be trained to convert a number (up to at least 12) of different languages to each other, automatically recognizing the source language and simply needing an input token to identify the output language. Furthermore, the model was found to be capable of understanding, at least somewhat, multilingual input, and of producing mixed outputs when multiple language tokens are given, sometimes even in languages related to, but not actually, those selected. This suggests that DNNs may be capable of learning universal representations for information, independent of language, and even more, that they might possibly be capable of learning some etymology and relationships between and among families of different languages.

H. Summary of Deep Learning NLP Applications
Numerous other applications of NLP exist including grammar correction, as seen in word processors, and author mimicking, which, given sufficient data, generates text replicating the style of a particular writer. Many of these applications are infrequently used, understudied, or not yet exposed to deep learning. However, the area of sentiment analysis should be noted, as it is becoming increasingly popular and utilizing deep learning. In large part a semantic task, it is the extraction of a writer’s sentiment—their positive, negative, or neutral inclination toward some subject or idea [268]. Applications are varied, including product research, futures prediction, social media analysis, and classification of spam [269], [270]. The current state of the art uses an ensemble, including both LSTMs and CNNs [271].

This section has provided a number of select examples of the applied usages of deep learning in NLP. Countless studies have been conducted in these and similar areas, chronicling the ways in which deep learning has facilitated the successful use of natural language in a wide variety of applications. Only a minuscule fraction of such work has been referred to in this survey.

While more specific recommendations for practitioners have been discussed in some individual sections, the current trend in state-of-the-art models in all application areas is to use pretrained stacks of transformer units in some configuration, whether in encoder–decoder configurations or just as encoders. Thus, self-attention, which is the mainstay of transformer, has become the norm, along with cross attention between the encoder and decoder units, if decoders are present. In fact, in many recent articles, if not most, transformers have begun to replace LSTM units that were preponderant just a few months ago. Pretraining of these large transformer models has also become the accepted way to endow a model with generalized knowledge of language. Models such as BERT, which have been trained on corpora of billions of words, are available for download, thus providing a practitioner with a model that possesses a great amount of general knowledge of language already. A practitioner can further train it with one’s own general corpora, if desired, but such training is not always necessary, considering the enormous sizes of the pretraining that downloaded models have received. To train a model to perform a certain task well, the last step that a practitioner must go through is to use available downloadable task-specific corpora or build one’s own task-specific corpus. This last training step is usually supervised. It is also recommended that if several tasks are to be performed, multitask training is used wherever possible.

SECTION V.Conclusion
Early applications of NLP included a well-acclaimed but simpleminded algebra word problem solver program called STUDENT [272], as well as interesting but severely constrained conversational systems such as Eliza, which acted as a “psychotherapist” [273], and another that conversed about manipulating blocks in a microworld [274]. Nowadays, highly advanced applications of NLP are ubiquitous. These include Google’s and Microsoft’s machine translators, which translate more or less competently from a language to scores of other languages, as well as a number of devices which process voice commands and respond in like. The emergence of these sophisticated applications, particularly in deployed settings, acts as a testament to the impressive accomplishments that have been made in this domain over the last sixty or so years. Without a doubt, incredible progress has taken place, particularly in the last several years.

As has been shown, this recent progress has a clear causal relationship with the remarkable advances in ANNs. Considered an “old” technology just a decade ago, these machine learning constructs have ushered in progress at an unprecedented rate, breaking performance records in myriad tasks in miscellaneous fields. In particular, deep neural architectures have instilled models with higher performance in natural language tasks, in terms of “imperfect” metrics. Consolidating the analysis of all the models surveyed, a few general trends can be surmized. Both convolutional and recurrent specimens had contributed to the state of the art in the recent past; however, of very late, stacks of attention-powered transformer units as encoders and often decoders have consistently produced superior results across the rich and varying terrain of the NLP field. These models are generally heavily pretrained on general language knowledge in an unsupervised or supervised manner and somewhat lightly trained on specific tasks in a supervised fashion. Second, attention mechanisms alone, without recurrences or convolutions, seem to provide the best connections between encoders and decoders. Third, forcing networks to examine different features (by performing multiple tasks) usually improves results. Finally, while highly engineering networks usually optimizes results, there is no substitute for cultivating networks with large quantities of high-quality data, although pretraining on large generic corpora seems to help immensely. Following from this final observation, it may be useful to direct more research effort toward pretraining methodologies, rather than developing highly specialized components to squeeze the last drops of performance from complex models.

While the numerous stellar architectures being proposed each month are highly competitive, muddling the process of identifying a winning architecture, the methods of evaluation used add just as much complexity to the problem. Data sets used to evaluate new models are often generated specifically for those models and are then used only several more times, if at all, although consolidated data sets encompassing several tasks such as GLUE [275] have started to emerge. As the features and sizes of these data sets are highly variable, this makes comparison difficult. Most subfields of NLP, as well as the field as a whole, would benefit from extensive, large-scale discussions regarding the necessary contents of such data sets, followed by the compilation of such sets. In addition to high variability in evaluation data, there are numerous metrics used to evaluate performance on each task. Oftentimes, comparing similar models is difficult because different metrics are reported for each. Agreement on particular sets of metrics would go a long way toward ensuring clear comparisons in the field.

Furthermore, metrics are usually only reported for the best case, with few mentions of average cases and variability, or of worst cases. While it is important to understand the possible performance of new models, it is just as important to understand the standard performance. If models produce highly variable results, they may take many attempts to train to the cutting-edge levels reported. In most cases, this is undesirable, and models that can be consistently trained to relatively high levels of performance are preferable. While increasingly large numbers of randomized parameters do reduce variation in performance, some variance will always exist, necessitating the reporting of more than just best case metrics.

One final recommendation for future work is that it is directed toward a wider variety of languages than it is at present. Currently, the vast majority of research in NLP is conducted on the English language, with another sizeable portion using Mandarin Chinese. In translation tasks, English is almost always either the input or output language, with the other end usually being one of a dozen major European or Eastern Asian languages. This neglects entire families of languages, as well as the people who speak them. Many linguistic intricacies may not be expressed in any of the languages used and, therefore, are not captured in current NLP software. Furthermore, there are thousands of languages spoken throughout the world, with at least 80 spoken by more than 10 million people, meaning that current research excludes an immense segment of humankind. Collection and validation of data in underanalyzed languages, as well as testing NLP models using such data, will be a tremendous contribution to not only the field of NLP but also to human society as a whole.

Due to the small amounts of data available in many languages, the authors do not foresee the complete usurpation of traditional NLP models by deep learning any time in the near future. Deep learning models (and even shallow ANNs) are extremely data hungry. Contrastingly, many traditional models require only relatively small amounts of training data. However, looking further forward, it can be anticipated that deep learning models will become the norm in computational linguistics, with pretraining and transfer learning playing highly impactful roles. Collobert et al. [9] sparked the deep learning revolution in NLP, although one of the key contributions of their work—that of a single unified model—was not realized widely. Instead, neural networks were introduced into traditional NLP tasks and are only now reconnecting. In the field of parsing, for example, most models continue to implement nonneural structures, simply using ANNs on the side to make the decisions that were previously done using rules and probability models. While more versatile and general architectures are obviously becoming more and more of a reality, understanding the abstract concepts handled by such networks is important to understanding how to build and train better networks. Furthermore, as abstraction is a hallmark of human intelligence, understanding of the abstractions that take place inside an ANN may aid in the understanding of human intelligence and the processes that underlie it. Just as human linguistic ability is only a piece of our sentience, so is linguistic processing just a small piece of artificial intelligence. Understanding how such components are interrelated is important in constructing more complete AI systems, and creating a unified NLP architecture is another step toward making such a system a reality.

This goal will also be aided by further advances in computational equipment. While GPUs have significantly improved the ability to train deep networks, they are only a step in the right direction [276]. The next step is the wider availability of chips designed specifically for this purpose, such as Google’s Tensor Processing Unit (TPU), Microsoft’s Catapult, and Intel’s Lake Crest [277]. Ultimately, ANNs implemented in traditional von Neumann style computers may not be able to reach their full potential. Luckily, another old line of work in computer science and engineering has seen a resurgance in recent years: neuromorphic computing. With neuromorphic chips, which implement neural structures at the hardware level, expected much more widely in the coming years [278], the continuation of deep learning and the longevity of its success can be highly anticipated, ensuring the opportunity for sustained progress in NLP.