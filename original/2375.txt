This article presents a visual navigation and landing control paradigm for an unmanned aerial vehicle (UAV) to land on a moving autonomous surface vehicle (ASV). Therein, an adaptive learning navigation rule with a multilayer nested guidance is designed to pinpoint the position of the ASV and to guide and control the UAV to fulfill horizontal tracking and vertical descending in a narrow landing region of the ASV by means of merely relative position feedback. To ensure the feasibility of the proposed control law, asymptotical stability conditions are derived based on Lyapunov stability theory. Landing experimental results are reported for a UAV-ASV system consisting of an M-100 UAV and a self-developed three-meters-long HUSTER-30 ASV on a lake to substantiate the efficacy of the proposed landing control method.
SECTION I.Introduction
In recent years, coordinated motion analysis and control of multiagent systems (MASs) have attracted extensive attentions in the areas of control engineering, systems science, and so on [1]–[2][3][4][5], motivated by the extensive applications of unmanned systems, smart grids, wireless sensor networks, traffic networks, industrial robots, wind farms, etc. The most fascinating feature of MAS collaboration lies in the emergence of high coordination merely via simple protocols of inter-agent interaction, with high efficiency and low cost desirable for numerous applications [6]–[7][8][9][10][11][12].

In parallel to the MAS research, the development of unmanned systems also boomed nourished by the significant advances in high-performance integrated circuits and chips, embedded systems, and sensors and actuators. As a representative of unmanned systems, autonomous surface vehicles (ASVs) play a more and more essential role in maritime operations, such as collective resource exploration, environmental monitoring, offshore patrolling, collective rescue, escort, hunting, mine sweeping, and so on [13]–[14][15][16][17][18][19][20][21][22][23][24][25]. However, the inherent horizontal 2-D water surface movement areas of multi-ASV groups largely limit their detectable ranges and operational regions. Specifically, for the unreachable areas, like sky and shoals, ASV-borne unmanned aerial vehicles (UAVs) will substantially extend the detectable and operational regions of ASVs from 2-D to 3-D, including maritime-air space survey, reconnaissance, target acquisition, shoal water extraction, and water quality monitoring. Therefore, these years have witnessed the emergence of investigations on UAV-ASV formation cooperation, which are expected to endow the cross-domain groups with high efficiently operational capability in 3-D maritime-air areas.

In the coordination technology of the UAV-ASV system, one of the most challenging issues is the landing of an ASV-borne UAV to a moving ASV platform. The arduousness of the mission lies in the high precision and efficiency requirements for the anti-wind disturbance control, autonomous navigation, and landing control during all the sequential three steps, i.e., tracking, approaching, and landing. Most of the existing studies were conducted for autonomous navigation and security landing space resolving with the assistance of embedded laser/navigation radars [26], [27] and RGPS [28], [29]. However, such radar-based landing methods require sufficiently large installation space on the UAV and high maintenance cost as well, which hinder their further applications, especially for civil use. Accordingly, it becomes an urgent yet challenging mission to develop a more convenient and less costly autonomous landing scheme to facilitate the cross-domain coordination of UAV-ASV groups.

In this pursuit, with the development of opto-electronic technology, a kind of promising vision-based navigation methods have been intensively explored for UAV-ASV landing. Due to the challenging of motional targets, initial researches focused on a stationary landing platform. As one of the pioneer works, Lin et al. [30] designed a monocular camera-based target identification method for a drone. Kim et al. [31] developed a guidance-oriented vision processing algorithm, which provides a proper bearing angle for the guidance system. However, with the rapid development of ocean engineering, the existing stationary target landing methods could not fulfill the requirements of the more and more sophisticated water area missions. Hence, more recent efforts have been devoted to the more desirable landing scenarios on motional surface targets. For instance, Serra et al. [32] addressed the landing problem on moving platforms via a dynamic image-based visual servo control. Beul et al. [33] proposed a camera-based scheme for landing on moving platforms. Tzoumanikas et al. [34] designed a visual–inertial estimation-based model predictive controller (MPC) to land a UAV on a moving platform. Baca et al. [35] generated a reference trajectory for a UAV to land on a moving car. Rodriguez et al. [36] utilized a versatile Gazebo-based reinforcement learning framework to fulfill a continuous UAV landing task. Wu et al. [37] designed a dynamic thresholding method for the topological-pattern landmark and took landing experiments on a moving car.

However, the aforementioned studies just focus on ground landing targets (e.g., ground vehicles), which is less challenging than ASV-landing as the latter has unneglectable external disturbances induced by uncertain wind, wave, and tide on water surfaces. Additionally, vision sensor feedback defections induced by vision reflections, vibrations, and bad weathers will even intensify the landing arduousness. Among the few studies on UAVs’ landing on water surface vehicles, Lee et al. [38] proposed a sliding-mode control scheme for a moving vehicle trajectory tracking and landing. Xia et al. [39] addressed a fixed-time control problem of ASV-landing procedures of vertical take-off and landing UAVs suffered by external disturbances. Meng et al. [40] proposed a visual/inertial-integrated guidance method for UAV vehicle-board landing. Afterward, Tan et al. [41] developed an invariant ellipsoid-based control approach for autonomous vehicle deck landing systems with wind disturbances and measurement noises. Meng et al. [42] designed a vision/radar/inertia navigation system for shipboard landing, where the imbedded camera on the UAV tracks and locks the ASV landmarks to facilitate autonomous landing.

However, due to the practical implementation challenges like wind/wave/tide disturbances, limited sensor range/resolution, surface light reflections, feedback image jittering, and bad weathers, most of the closely-relevant works [38]–[39][40][41] still focus on numerical simulation investigation. So far, only a few experimental results [38] of visual navigation precise and secure landing control are conducted with the UAV-ASV cooperation system via only four infrared targets, which could not always guarantee the precise landing of the UAV since the infrared targets are easy to be missed when the UAV-ASV relative height reduces too low. Additionally, sufficiently large landing areas are generally required as well, which is not feasible for small-sized ASVs. Apart from the landing procedure, the capability of autonomous navigation and guidance systems inevitably influences the performance of the landing procedures. Consequently, it becomes an urgent yet challenging mission to propose a niche protocol for guaranteeing autonomous navigation and accurate landing of cross-domain UAV-ASV cooperation systems.

To this end, in this study, a vision-based landing control framework is proposed for a cross-domain autonomous UAV-ASV cooperation equipment composed of an M-100 UAV and a three-meter-long HUSTER-30 ASV located at our testbed lake in Dongguan City, Guangdong Province, China. More precisely, during the landing control procedure, an adaptive learning navigation method is designed with the assistance of a multilayer nested guidance module and a difference GPS device to feedback the position of the ASV, where the UAV could lock the landing area irrelevant of the UAV-ASV height upon detecting any one of the three markers. Moreover, the secure and precise landing could be guaranteed along decreasing height by smoothly switching the makers. Thereby, the UAV fulfills the ASV moving trajectory tracking task via position-feedback regulation governing the UAV-ASV horizontal error to zero. Upon approaching the ASV, with an Adam learning-based landing mark identification and locking procedure, the UAV begins to implement a vision-based horizontal tracking and vertical regulation protocol to land on the moving ASV.

In summary, the contribution of this article is twofold as follows.

Proposing an autonomous adaptive learning-based landing control framework for a UAV to land on a moving ASV, which consists of a navigator, an ASV trajectory tracking controller, and a landing height regulator.

Establishing a lake-based landing platform consisting of an ASV dock, an M-100 UAV and a three-meter-long HUSTER-30 ASV with a NovAtel-OEM615-typed differential GPS station to substantiate the effectiveness of the proposed control scheme.

The remainder of this article is organized as follows. Section II introduces the preliminaries and the landing control problem for ASV-UAV cooperation systems. Section III proposes a navigation, control, and regulation framework for a UAV to land on an ASV in motion, and afterward derives the conditions guaranteeing asymptotical stability for the closed-loop system. Extensive landing experiments are conducted to substantiate the effectiveness of the present landing scheme in Section IV. Conclusion is finally drawn in Section V.

SECTION II.Problem Formulation
Throughout this article, R and R+ denote the real number and positive real number, respectively. Rn denotes the n -dimensional Euclidean space and ∥∗∥ is the Euclidean norm of a vector ∗ . e∗ denotes exponential power of a square matrix ∗ . ∗−1 denotes the inversion of a square matrix ∗ . ∗T represents the transpose of a matrix ∗ .

Consider an autonomous motional landing system consisting of a UAV and an ASV. In these years, there are significant achievements on the UAV control [43], [44]. Therein, a representative practical method is to simplify a dynamic UAV as a moving particle approximated by the following second-order dynamics in the Cartesian coordinates (see [45], [46])
q˙(t)=p˙(t)=p(t)u(t)(1)
View SourceRight-click on figure for MathML and additional features.with q(t)=[qx,qy,qz]T,p(t)=[px,py,pz]T∈R3 denoting the position and velocity of the UAV, respectively, and u(t)=[ux,uy,uz]T∈R3 being the control input. Specifically, in the landing scenario, qd(t) is the center of the ASV landing mark. Meanwhile, a moving ASV with a constant velocity can be regarded as a second-order dynamic with a two-level hierarchical framework, see [18] as
q˙d(t)=p˙d(t)=pd(t)0(2)
View SourceRight-click on figure for MathML and additional features.with qd(t)=[qxd,qyd,qzd]T,pd(t)=[pxd,pyd,pzd]T∈R3 representing the position and velocity of the ASV, respectively.

In real landing procedure of the UAV-ASV system, the ASV position qd is usually detected by GPS and afterward be transmitted to the UAV, which inevitably introduces positioning inaccuracy due to the inherent GPS measurement error. As a remedy, a landing guidance is thus equipped onboard and identified by a camera embedded in the UAV, which provides a more accurate position feedback of the landing area. However, due to the limited detectable range and resolution, the camera could just catch the guidance with a sufficiently small UAV-ASV distance. Meanwhile, the external disturbances and measurement noises of the ASV like winds, waves, tides, surface reflections, image jittering, and bad weather also influence the identification performance of the landing-oriented guidance module, which thus poses a challenge to access the actual ASV position qd(t) . It is noted that only the position qd(t) is available to the guidance module and GPS sensors, but not the velocity pd(t) .

Recall the objective of landing UAV on a motional ASV, it is necessary to provide the following two definitions first.

Definition 1 (Horizontal Tracking):
A UAV (1) tracks a moving ASV (2) in a horizontal plane with bounded errors and maintains the initial ASV-UAV relative height if
Δqx:=∥∥qx(t)−qxd(t)∥∥≤Δqy:=∥∥qy(t)−qyd(t)∥∥≤Δqz:=∥∥qz(t)−qzd(t)∥∥=δx∀t>Tδy∀t>T∥∥qz(0)−qzd(0)∥∥(3)
View SourceRight-click on figure for MathML and additional features.where δx,δy∈R+ are the tolerated position errors of the landing area, and T∈R+ a finite constant time.

Definition 1 describes a practical situation in real landing applications, since the landing time is finite and the landing area for the UAV is just slightly larger than the UAV itself.

Definition 2 (Vertical Regulation):
The ASV-UAV relative height settles if
Δqz(t)=0∀t>Tz(4)
View SourceRight-click on figure for MathML and additional features.with a finite constant time Tz∈R+ .

The two-step landing procedure is illustrated more vividly in Fig. 1. Therein, with GPS navigation, Step 1 of horizontal tracking (see Definition 1) is conducted by the UAV until the horizontal tracking error
Δqx,y:=∥∥[qx(t)−qxd(t),qy(t)−qyd(t)]T∥∥≤σ
View SourceRight-click on figure for MathML and additional features.with a small threshold σ>(δ2x+δ2y)1/2>0 . Afterward, with the assistance of adaptive learning-based visual navigation, the ASV landing mark central qd(t) becomes detectable and lockable by the UAV, and Step 2 of vertical regulation (see Definition 2) is thus triggered together with horizontal tracking for the final touch down.

Fig. 1. - Two-step landing procedure, step 1: GPS navigation tracking (black trajectory); step 2: Visual navigation landing (red trajectory).
Fig. 1.
Two-step landing procedure, step 1: GPS navigation tracking (black trajectory); step 2: Visual navigation landing (red trajectory).

Show All

Now, it is ready to introduce the three main problems addressed in this article as follows.

Problem 1:
Design a control law
ux,y:=f(qx,qy,px,py,qxd,qyd)
View SourceRight-click on figure for MathML and additional features.for the UAV of the UAV-ASV system governed by (1) and (2) to fulfill the horizontal tracking mission in Definition 1.

Problem 2:
Design an adaptive learning rule to calculate changes for pinpointing the landing position of ASV qd as
qd:={2-D visual-guidance barcode,camera}
View SourceRight-click on figure for MathML and additional features.for the visual navigation of UAV in the UAV-ASV system.

Problem 3:
Design a control law
uz:=f(qz,pz,qzd)
View SourceRight-click on figure for MathML and additional features.for the UAV of the UAV-ASV system governed by (1) and (2) to achieve the vertical regulation status in Definition 2.

It is noted that, due to the decoupling property of the simplified UAV dynamic in (1), Problems 1 and 3 can be both addressed by separately designing 3-D control signals ux,uy,uz .

SECTION III.Theoretical Results
In this section, the three parts of the landing control procedure in Fig. 1 will be derived, i.e., horizontal tracking control, adaptive learning-based navigation, and vertical regulation.

A. Horizontal Tracking
We are now ready to propose a tracking control law to tackle Problem 1, i.e., to fulfill Step 1 of the landing procedure in Fig. 1. Before presenting the detailed derivation, a lemma of typical output regulation theory is given first.

Lemma 1 [47]:
Consider a linear time-invariant system governed by
x˙=e=ν˙=Ax+Bu+EνCx+Du+FνSν
View SourceRight-click on figure for MathML and additional features.and a dynamic state feedback controller
u=z˙=K1x+K2zG1z+G2 e
View SourceRight-click on figure for MathML and additional features.where x∈Rn,u∈Rm,e∈Rp,z∈Rnz,v∈Rq the system state, input state, regulated output, internal model state, and exosystem signal, respectively and A∈Rn×n,B∈Rn×m,C∈Rp×n,D∈Rp×m,E∈Rn×q,F∈Rp×q,S∈Rq×q,K1∈Rm×n,K2∈Rm×nz,G1∈Rnz×nz,G2∈Rnz×p . Assume S has no eigenvalue with negative real parts. If (G1,G2) incorporates a p-copy internal model of S and the matrix
Ac=[A+BK1G2(C+DK1)BK2G1+G2DK2]
View Sourceis Hurwitz, then the following Sylvester matrix equation has a unique solution X∈Rn×q,Z∈Rnz×q as
XcS=0=AcXc+BcCcXc+Dc
View SourceRight-click on figure for MathML and additional features.with
Xc=[XZ],Bc=[EG2F], Cc=[CD], Dc=[F].
View SourceRight-click on figure for MathML and additional features.

To achieve horizontal tracking, it suffices to consider the horizontal plane dynamics of the UAV-ASV system as
q¯¯˙(t)=p¯¯¯˙(t)=p¯¯¯(t)u¯¯¯(t)(5)
View SourceRight-click on figure for MathML and additional features.with q¯¯(t)=[qx,qy]T,p¯¯¯=[px,py]T,u(t)=[ux,uy]T∈R2 , and
q¯¯˙d(t)=p¯¯¯˙d(t)=p¯¯¯d(t)0(6)
View SourceRight-click on figure for MathML and additional features.with q¯¯d(t):=[qxd,qyd]T,p¯¯¯d:=[pxd,pyd]T∈R2 . Note that, in Step 1 of the landing procedure, ASV position qd is just roughly detected by a differential NovAtel-OEM615-typed GPS with ±0.4 m error, which however suffices the horizontal tracking.

Define the horizontal plane position error between the UAV and the ASV as follows:
e¯¯¯(t):=q¯¯(t)−q¯¯d(t).(7)
View Source

It follows from (5) and (6) that
e¯¯¯˙(t)=p¯¯¯(t)−p¯¯¯d(t).(8)
View Source

A control law for (5) in the horizontal plane is thus designed as
u¯¯¯=ξˆ˙=−[k1k2]⋅ξ¯¯−[k3k4]⋅ξˆG1ξˆ+G2e¯¯¯(9)
View SourceRight-click on figure for MathML and additional features.where ξ¯¯:=[q¯¯,p¯¯¯]T,ξˆ:=[qˆ,pˆ]T∈R2 are the states of the designed internal model, and k1,k2,k3,k4∈R+ are the control gains to be specified afterward, and
G1=⎡⎣⎢0010⎤⎦⎥,G2=⎡⎣⎢01⎤⎦⎥.(10)
View SourceRight-click on figure for MathML and additional features.

Now, we are ready to present the main technical result concerning Problem 1 as follows.

Theorem 1:
The closed-loop system composed of (5)–(7) and (9) is able to achieve the horizontal tracking status in Problem 1, if k1,k2,k3,k4∈R+ satisfy
k2k1−k4>(k2k1−k4)k4−k22k3>00.(11)
View Source

Proof:
Substituting u¯¯¯ in (9) into (5) yields a compact form of (5) as
ξ¯¯˙=Aξ¯¯+Bξˆ(12)
View SourceRight-click on figure for MathML and additional features.with
A=⎡⎣⎢0−k11−k2⎤⎦⎥,B=⎡⎣⎢0−k30−k4⎤⎦⎥.(13)
View SourceRight-click on figure for MathML and additional features.

Analogously, define ξ¯¯d:=[q¯¯d,p¯¯¯d]T , one has the horizontal plane dynamic equation of the ASV in (6) as
ξ¯¯˙d=Sξ¯¯d(14)
View Sourcewith
S=⎡⎣⎢0010⎤⎦⎥.
View Source

Then, e¯¯¯ can be rewritten via ξ¯¯,ξ¯¯d as
e¯¯¯(t)=Cξ¯¯+Dξ¯¯d(15)
View Sourcewith
C=[10],D=[−10].(16)
View SourceRight-click on figure for MathML and additional features.

Let ς=[ξ¯¯,ξˆ]T , it follows from (9), (12), (15) that the regulation system becomes
ς˙=ξ¯¯˙d=e¯¯¯=Acς+Bcξ¯¯dSξ¯¯dCcς+Dξ¯¯d(17)
View SourceRight-click on figure for MathML and additional features.with
Ac=[AG2CBG1],Bc=[0G2D], Cc=[C0].
View Source

Due to the minimum characteristic polynomial of matrix λ(S)=λ2 , one has that G1,G2 incorporate a p -copy internal model of matrix S in (14) [47]. Moreover, with A,B,G1,G2,C defined in (10), (13), (16), it can be derived that
Ac=⎡⎣⎢⎢⎢⎢⎢⎢0−k1011−k2000−k3000−k410⎤⎦⎥⎥⎥⎥⎥⎥(18)
View Sourcewhich can be rewritten as a controllable canonical form [48]
A¯¯¯¯c=⎡⎣⎢⎢⎢⎢⎢⎢000−k3100−k4010−k1001−k2⎤⎦⎥⎥⎥⎥⎥⎥.(19)
View SourceRight-click on figure for MathML and additional features.

With the assistance of Routh stability criterion [49], the matrix A¯¯¯¯c is Hurwitz if k1,k2,k3,k4∈R+ satisfy
k2k1−k4>0,(k2k1−k4)k4−k22k3>0(20)
View SourceRight-click on figure for MathML and additional features.which implies that Ac is Hurwitz as well. According to Lemma 1, one has that the regulation equation of (17) has a solution Σ as
ΣS=0=AcΣ+Bc,CcΣ+D.(21)
View Source

Define ς˜:=ς−Σξ¯¯d as the state errors, the temporal derivative of ς˜ along the trajectories of (17) and (21) writes
ς˜˙===Acς+Bcξ¯¯d−ΣSξ¯¯dAcς˜+(AcΣ+Bc−ΣS)ξ¯¯dAcς˜(22)
View SourceRight-click on figure for MathML and additional features.which implies that limt→∞ς˜(t)=0 exponentially due to the Hurwitz matrix Ac . On the other hand, considering (21), it can be derived that
e¯¯¯===Ccς+Dξ¯¯dCcς˜+CcΣξ¯¯d+Dξ¯¯dCcς˜(23)
View SourceRight-click on figure for MathML and additional features.which implies that limt→∞e¯¯¯(t)=0 exponentially if limt→∞ς˜(t)=0 . Then, it derives that these exists a finite time T1∈R+ such that Δqx(t)≤δx and Δqy(t)≤δy,∀t>T1 , i.e., Problem 1 is solved. The proof is thus completed.

Remark 1:
The proposed ξˆ in (9) is designed based on an internal model principle (IMP) [47], which tracks the moving ASV merely with rough GPS position feedback signal qd . Moreover, IMP substantially eliminates the influence of external disturbances on the UAV like winds and surface light reflects, which endows the controller (9) with desirable robustness in real landing applications.

Remark 2:
For the low speed ASV tracking scenario, the UAV tackles Problem 1 with a low frequency, which thus implies the dynamic matrix G1 in (10) can be ignored as 0. In accordance, the internal model ξˆ can be simplified as ξˆ˙=G2e¯¯¯ , which simplifies the control law (9) into
u¯¯¯=−[k1k2]⋅ξ¯¯−[k3k4]∫G2e¯¯¯(s)ds.(24)
View SourceRight-click on figure for MathML and additional features.

In this way, the output regulation controller (9) is simplified to a proportional-integral (PI) form, which facilitates the control parameters turning in real UAV-ASV landing applications.

B. Learning-Based Navigation
To fulfill the high-precision visual navigation mission, i.e., Step 2, of the landing procedure in Fig. 1, Problem 2 should be addressed first. To this end, a guidance 2-D barcode as show in Fig. 2(a), namely AprilTag2 [50], i.e., an 80×80 cm2 rectangle containing multilayer nested block information will be identified by the onboard DJI Zenmuse Z3 camera of the UAV. More precisely, as show in Fig. 2(b), the side length of marker 2 (medium altitude: 1.2–5 m) is 1/5 of that of marker 1 (high altitude: 5–15 m), and so does the side length proportion of marker 3 (low altitude: ≤1.2 m ) to marker 2. In this way, the precise landing position can be locked along by smoothly switching the makers when reducing UAV-ASV relative height e¯¯¯z .


Fig. 2.
(a) Multilayer nested 2-D barcode consisting of high-, medium-, and low-altitude markers. (b) Different-size guidance recognition during the landing process.

Show All

During the visual navigation, the UAV camera firstly detects the rectangle of the 2-D barcode in Fig. 2 with an edge detection approach. Afterward, the UAV is to judge whether the rectangle is the landing marker. Different from a typical image segmentation method, an adaptive learning-based convolution neural network (CNN) is proposed to fulfill the real-time requirement of UAV-ASV landing (see [51]), which consists of a backbone network, a feature pyramid, and a classification layer with an Adam learning algorithm in Fig. 3. The number of training and validation pictures are 1532 and 383, respectively, where the accuracies of the proposed CNN in training and validation data are given as 99.4% and 98.3%, respectively.

Fig. 3. - Architecture of the adaptive learning-based CNN, where the matrix block represents the feature map, 
$H$
 and 
$W$
 are the height and width of feature map, respectively. “
$/s$
” (s = 8, 16, 32) denotes the down-sampling ratio of the feature maps at the level to the input image. “Conv” and “interp” denote the convolution and the bilinear interpolation, respectively. “Heads” represents a general name of several convolutions after feature fusion. “
$\times 4$
” represents convolution four times.
Fig. 3.
Architecture of the adaptive learning-based CNN, where the matrix block represents the feature map, H and W are the height and width of feature map, respectively. “/s ” (s = 8, 16, 32) denotes the down-sampling ratio of the feature maps at the level to the input image. “Conv” and “interp” denote the convolution and the bilinear interpolation, respectively. “Heads” represents a general name of several convolutions after feature fusion. “×4 ” represents convolution four times.

Show All

To train the parameters ω∈Rv in the proposed CNN, a cost function containing the training error is given as follows:
E(ω)=∑i=1P∑j=1NL(oLj,i−yj,i)2(25)
View SourceRight-click on figure for MathML and additional features.where P is the number of patterns of the marker from different angle perspectives, NL is the number of neurons in output layer, oLj,i is the output of neuron j in the i th pattern, and yj,i is the target in the i th pattern.

Then, the Adam learning algorithm [52] is utilized to minimize E(ω) in (25). To this end, define gt:=∇ωE(ω) , one has
ωt=ωt−1−αnˆt−−√+ϵmˆt(26)
View Sourcewith a constant parameter ϵ∈R+ , α∈R+ is the step size, and bias-corrected estimates mˆt,nˆt are derived as
mˆt=nˆt=mt=nt=mt1−βt1nt1−βt2β1mt−1+(1−β1)gtβ2nt−1+(1−β2)g2t
View SourceRight-click on figure for MathML and additional features.with moving averages of the gradient mt , squared gradient nt , and parameters β1,β2,βt1,βt2∈[0,1 ).

With the identified marker, the essential four corners of the 2-D barcode are calculated as a pixel μi=[ηi,ωi,1]T,i=1,2,3,4 for visual navigation, where ηi and ωi denote the horizontal and vertical coordinate, respectively. More precisely, define the corner positions in the camera coordinate as ς¯i:=[X¯¯¯¯i,Y¯¯¯¯i,Z¯¯¯¯i]T∈R3 , thus the relationship between μi and ς¯i can be formulated as [53]
Z¯¯¯¯iμi=Kς¯i(27)
View SourceRight-click on figure for MathML and additional features.with the camera intrinsics matrix
K=⎡⎣⎢⎢⎢⎢fx000fy0cxcy1⎤⎦⎥⎥⎥⎥(28)
View SourceRight-click on figure for MathML and additional features.and constants fx,fy,cx,cy .

Define the corner positions in earth coordinate as ςi:=[Xi,Yi,Zi]T∈R3 , then the relationship between ς¯i and ςi becomes
ς¯i=Rςi+φ(29)
View Sourcewhere R∈R3×3 and φ∈R3×1 are the rotation and the translation matrices from earth coordinate to camera coordinate, respectively. It follows from (27) and (29) that the coordinate transformation from 3-D to 2-D coordinate writes
Z¯¯¯¯iμi=K(Rςi+φ).(30)
View SourceRight-click on figure for MathML and additional features.

Design a cost function of the re-projection error of the four corners as
V(R,φ)=12∑i=14∥∥∥μi−1Z¯¯¯¯iK(Rςi+φ)∥∥∥2.(31)
View SourceRight-click on figure for MathML and additional features.

According to Lie algebra [54], it is noted that Rςi=eψ(χ)ςi , where ψ(⋅):R3→R3×3 is a function with an input vector χ=[χ1,χ2,χ3]T∈R3 as
ψ(χ)=⎡⎣⎢⎢⎢0χ3−χ2−χ30χ1χ2−χ10⎤⎦⎥⎥⎥.(32)
View Source

Then, the index (31) is rewritten as an unconstrained optimization index
V(χ,φ)=12∑i=14∥∥∥μi−1Z¯¯¯¯iK(eψ(χ)ςi+φ)∥∥∥2.(33)
View SourceRight-click on figure for MathML and additional features.

To facilitate a real-time optimization with (33) in the typical Perspective-n-Points (PnP) problem, a neurodynamic optimization method [55], [56] is utilized as
ϵddt(χ,φ)=−∇(χ,φ)V(χ,φ)(34)
View SourceRight-click on figure for MathML and additional features.with ϵ∈R+ a time constant. It is noted that the convergence time of the optimal solution in (34) is proportional to ϵ . To this end, (χ∗,φ∗)=arg minχ,φV(χ,φ) can be attained.

With the optimized χ∗,φ∗ yielded, R and φ can be calculated as
R=eψ(χ∗), φ=φ∗.(35)
View SourceRight-click on figure for MathML and additional features.

Define the position in the camera homogeneous coordinate as τ¯¯¯c:=[0,0,0,1]T , the relative position τ˜c=[X˜c,Y˜c,Z˜c,1]∈R4 between the camera and the ASV in the world homogeneous coordinate is calculates as
τ˜c=T−1τ¯¯¯c(36)
View SourceRight-click on figure for MathML and additional features.with a coordinate transformation matrix
T=⎡⎣⎢R0Tφ1⎤⎦⎥∈R4×4.(37)
View SourceRight-click on figure for MathML and additional features.

It implies the relative position τ¯¯¯c=[X˜c,Y˜c,Z˜c]∈R3 in the world coordinate. Combined with the position q(t) captured by the onboard GPS of M-100 UAV, the accurate position of the target qd=τ¯¯¯c+q is derived according to an Adam-based navigation algorithm summarized in Algorithm 1. Along the decreased UAV height during the landing procedure, the high-, medium-, and low-altitude 2-D barcode markers in Fig. 2 can be identified and locked for accurate vision guidance to realize Step 2 of the landing procedure in Fig. 1. The merit of the proposed Algorithm 1 lies in its high computational efficiency and nonstationary optimization capability with noisy gradients induced by the vibrations of the UAV and the ASV moving in the windy and wavy water area.

Algorithm 1 Adam Adaptive Learning-Based Navigation Algorithm
Detect the rectangle of the marker in Fig. 2 (a) with an edge detection approach;

Identify the marker and four corner positions μi in Fig. 2 (a) by using an Adam learning-based CNN structure;

Calculate R=eψ(χ∗),φ=φ∗ in (35) in the PnP problem via a neurodynamic optimization method;

Pinpoint the position qd=τ¯¯¯c+q with position q captured by the onboard GPS of M-100 UAV.

Remark 3:
In previous visual navigation methods [40], [42], only four infrared targets are equipped on the platform. These schemes could not always guarantee the precise landing of the UAV since the infrared targets could be missed when the UAV-ASV relative height reduces too low (less than 1.2 m for example). Additionally, it usually requires large landing area, which is not feasible for small-sized ASVs like the present case. As a remedy, the present study designs multilayer nested markers instead, the UAV could lock the landing area irrelevant of the UAV-ASV height once detecting any one marker of the three markers in Fig. 2. In this way, secure and precise landing could be guaranteed along decreasing height by smoothly switching the makers. Therefore, the proposed algorithm is more accurate, effective, and secure compared with [40], [42].

C. Vertical Regulation
Upon completing Step 1 of the landing procedure in Fig. 1, one has limt→∞e¯¯¯(t)=0 in accordance to Theorem 1. In other words, for any prescribed bound σ∈R+ , there exists a time period Tp>0 such that ∥e(t)∥≤σ if t>Tp . Afterward, we can address the vertical regulation in Problem 3 with the visually identified landing position pd resulted from Algorithm 1.

Unlike subsection III-A, only longitudinal-axis dynamic of the UAV-ASV system is considered in Problem 3 due to the decoupling property of the simplified UAV dynamic, which are formulated as
q˙z(t)=p˙z(t)=pz(t)uz(t)(38)
View SourceRight-click on figure for MathML and additional features.and
q˙zd(t)=p˙zd(t)=pzd(t)0(39)
View SourceRight-click on figure for MathML and additional features.by virtue of (1) and (2). Let e¯¯¯z be the relative UAV-ASV height shown in Fig. 1 as
e¯¯¯z=qz−qzd.(40)
View Source

It then follows from (38) and (39) that
e¯¯¯˙z=pz−q˙zd.(41)
View Source

A control law to achieve vertical regulation is thus designed as follows:
uz=−k5e¯¯¯˙z−k6(k5e¯¯¯z+pz−q˙zd)+q¨zd(42)
View SourceRight-click on figure for MathML and additional features.where k5,k6∈R+ are the control gains to be specified afterward. Now, we are ready to present the main technical result concerning Problem 3 as below.

Theorem 2:
The closed-loop system composed of (38), (39) and (42) is able to achieve the relative vertical regulation in Problem 3.

Proof:
We first denote hz as a desired height
hz=k5e¯¯¯z+pz−q˙zd.(43)
View SourceRight-click on figure for MathML and additional features.

Using the fact that pz−q˙zd=hz−k5e¯¯¯z , it follows from (41) that
e¯¯¯˙z=−k5e¯¯¯z+hz.(44)
View SourceRight-click on figure for MathML and additional features.

Let a Lyapunov function candidate be
V2(e¯¯¯z)=12e¯¯¯2z.(45)
View SourceRight-click on figure for MathML and additional features.

Its time derivative of V2(e¯¯¯z) along the trajectories of (44) is
V˙2(e¯¯¯z)==≤≤e¯¯¯ze¯¯¯˙ze¯¯¯z(−k5e¯¯¯z+hz)−k5(1−c2)e¯¯¯2z+|e¯¯¯z|(|hz|−k5c2|e¯¯¯z|)0(46)
View Sourcewith c2∈ (0,1) . Then, one has V˙2(e¯¯¯z)≤−k5(1−c2)e¯¯¯2z,∀|e¯¯¯z|≥|hz|/(c2k5) . If |e¯¯¯z(0)|≥|hz|/(c2k5) , then V˙2(e¯¯¯z)≤0 , which implies that |e¯¯¯z| converges into the ball of radius |hz|/(c2k5) . If |e¯¯¯z(0)|<|hz|/(c2k5) . It is clear that |e¯¯¯z(0)| has already been in the ball of radius |hz|/(c2k5) . Both cases satisfy the definition of input-to-state stability (ISS) [53], which implies that |e¯¯¯z(0)| could be randomly picked. Accordingly, the closed-loop system (44) is input-to-state stable (ISS) [57] with respect to |hz| satisfying
e¯¯¯z(t)=β(|e¯¯¯z(0)|,t)+κ2supτ≥0|hz(τ)|∀t>0(47)
View SourceRight-click on figure for MathML and additional features.where κ2=1/(k5c2) is the gain of the system (see [58]). Then, the temporal derivative of hz along the trajectory of (38) becomes
h˙z=k5e¯¯¯˙z+uz−q¨zd.(48)
View SourceRight-click on figure for MathML and additional features.

Substituting the controller (42) into (48) yields
h˙z==k5e¯¯¯˙z−k5e¯¯¯˙z−k6hz+q¨zd−q¨zd−k6hz(49)
View Sourcewhich implies that limt→∞hz(t)=0 , exponentially. It follows from (47) that limt→∞e¯¯¯z(t)=0 , exponentially. Note that Δqz=∥e¯¯¯z∥ in virtue of (3) and (40). Analogously, these exists a finite time T2∈R+ and a small constant δz∈R+ such that Δqz(t)≤δz,∀t>T2 . Afterward, the UAV settles on the landing platform safely (i.e., Problem 3 is solved). The proof is thus completed.

Remark 4:
When the ASV moves at a sufficiently low velocity, the influence of the surfaces waves and tides can be neglected, which implies the ASV moves in a horizontal plane. It can be assumed that qzd is a constant, and hence q˙zd=0,q¨zd=0 . In such a situation, the control law (42) can be approximated as
uz=≈−k5e¯¯¯˙z−k6(k5e¯¯¯z+pz−q˙zd)+q¨zd−(k5+k6)e¯¯¯˙z−k6k5e¯¯¯z.(50)
View SourceRight-click on figure for MathML and additional features.

In this way, the output control law (9) is simplified in a proportional-differential (PD) form, which facilitates the control parameter turning in landing applications.

SECTION IV.Experimental Results
In this section, we elaborate on our experimental platform, setup, and results on the learning-based navigation and landing control on established cross-domain UAV-ASV system located on Songshan Lake, Dongguan, Guangdong, China.

A. UAV-ASV Experimental Platform
As shown in Fig. 4(a), our established UAV-ASV platform consists of a DJI Matrice-100 (M-100) UAV and a HUSTER-30 ASV. The UAV is 65.0 cm long in wheelbase, which is equipped with four rotor modules (DJI 3510), a GPS module (A2 GPS PRO PLUS), a camera (DJI Zenmuse Z3), a 5 g Wi-Fi (VM5g), and a battery (TB47D). All modules are monitored by a control module (MANIFOLD V1). Our developed HUSTER-30 ASV is 300 cm in length, 185 cm in width, and 53 cm in height, which consists of two motor drivers (SEAKING-V3), a battery (RUIYI-200), two antennas (Harxon GPS500), a WiFI (VM5g), an embedded controller (STM32F407VGT6), and a landing platform. It is noted that the landing platform consists of a multilayer nested 2-D barcode (80 cm ×80 cm) in Fig. 2 and a metal frame (180 cm ×120 cm). Table I shows the features of the essential modules in the cross-domain UAV-ASV platform.

TABLE I Main Modules in the UAV-ASV Platform
Table I- 
Main Modules in the UAV-ASV Platform
Fig. 4. - Our cross-domain UAV-ASV cooperative platform. (a) Components of the UAV-ASV system. (b) Operating procedure of the UAV-ASV system.
Fig. 4.
Our cross-domain UAV-ASV cooperative platform. (a) Components of the UAV-ASV system. (b) Operating procedure of the UAV-ASV system.

Show All

As shown in Fig. 4(b), with the VM5g Wi-Fi module embedded on both ASV and UAV, the rough ASV position detected by the GPS module is transferred to the UAV at a frequency of 1 Hz. Upon catching the ASV, or completing Step 1 in Fig. 1, the UAV-ASV relative displacement is calculated by the Z3 camera with the assistance of the identified precise position pd of the landing barcode in Fig. 2. Thereafter, with the received distance and landing position, the embedded Minifold V1 controller generates the execution signal to control the rotors to fulfill the landing mission, or Step 2 in Fig. 1. Meanwhile, the status trajectories and the controller parameters are all recorded by the Minifold V1 controller for data analysis.

B. Setup and Results
In the experiment, the parameters of the camera intrinsics K in (28) are chosen as fx=1048.9125,cx=662.2501,fy=1045.4800,cy=370.6075 . The positions of four corners in the world coordinate are set as q1=[−40,−40,0]T cm, q2=[−40,40,0]T cm, q3=[40,40,0]T cm, q4=[40,−40,0]T cm. To satisfy the inequality (11), the parameters of the controller (9) are set as k1=2,k2=2,k3=0.5,k4=1 for horizontal tracking. In Step 2 of vertical regulation, the parameters of the controller (42) are set as k5=1,k6=10 . The control signal of (9) and (42) is updated by the embedded controller at a sampling and updating frequency of 20 Hz. As for the HUSTER-30 ASV, the initial position of the HUSTER-30 ASV is randomly set and the velocity of pd in (2) is regulated as a constant pd=[1.2,0]T m/s with the two-level controller proposed in [18], [21].

The UAV takes off initially from the lake shore and hovers at a desired height. Fig. 5 shows the landing trajectories of the UAV on a moving ASV with the learning-based vision navigation, where the landing process are separated into Steps 1 and 2 as shown in Fig. 1. To illustrate the landing process more vividly, Fig. 6 shows four consecutive snapshots. Fig. 7 shows that both the relative horizontal position error e¯¯¯=[e¯¯¯x,e¯¯¯y]T and altitude error e¯¯¯z shrink to zeros in less than 28 seconds. Moreover, It is noted that the M-100 UAV conducts horizontal tracking in only the first nine seconds before driving the relative horizontal position error inside the prescribed threshold ∥σ∥≤1.5 m. Afterward, the UAV begins to fulfill the vertical regulation mission illustrated in Fig. 7. As shown in Fig. 2, the three-layer nested 2-D barcode is detected by the Z3 camera, where the visual guidance from large- to medium- and to small-sized markers are identified along decreasing relative UAV-ASV height. The real landing experiments are conducted on the ASV-UAV system as shown in Fig. 6. The feasibility of the proposed relative position feedback control method is thus verified on the UAV-ASV landing mission with horizontal tracking and vertical descending.

Fig. 5. - 3-D view of the detailed UAV-ASV landing process, where the solid black and red curves denote respectively steps 1, and 2 of the UAV, and the dashed red curves denote the trajectories of the ASV.
Fig. 5.
3-D view of the detailed UAV-ASV landing process, where the solid black and red curves denote respectively steps 1, and 2 of the UAV, and the dashed red curves denote the trajectories of the ASV.

Show All

Fig. 6. - Four consecutive landing snapshots of the M-100 UAV on a moving HUSTER-30 ASV, where the red rectangle highlights M-100 UAV. (a) Initial positions. (b) Horizontal tracking. (c) Vertical regulation. (d) Landing on the ASV.
Fig. 6.
Four consecutive landing snapshots of the M-100 UAV on a moving HUSTER-30 ASV, where the red rectangle highlights M-100 UAV. (a) Initial positions. (b) Horizontal tracking. (c) Vertical regulation. (d) Landing on the ASV.

Show All

Fig. 7. - Transient relative position errors 
$\overline {e}_{x},\overline {e}_{y}, \overline {e}_{z}$
 toward zeros in the horizontal tracking and vertical regulation.
Fig. 7.
Transient relative position errors e¯¯¯x,e¯¯¯y,e¯¯¯z toward zeros in the horizontal tracking and vertical regulation.

Show All

SECTION V.Conclusion
In this article, we present a practical paradigm on the visual navigation and landing control of a UAV to land on a moving ASV with unknown velocity. The proposed landing procedure consists of two consecutive missions; i.e., GPS navigation horizontal tracking and visual navigation vertical/horizontal regulation. We derive the asymptotical stability conditions for the closed-loop UAV-ASV landing system. We also substantiate the effectiveness of the proposed landing scheme with experimental results based on our lake-based platform composed of an M-100 UAV and a self-developed HUSTER-30 ASV. Future work will focus on in-depth theoretical analysis, cohesive system designs, and practical applications of UAV-ASV coordinated systems in complex aquatic environments, such as the distributed landing control of multiUAV on multiple motional ASVs.