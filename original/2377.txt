This article aims to tackle the problem of group activity recognition in the multiple-person scene. To model the group activity with multiple persons, most long short-term memory (LSTM)-based methods first learn the person-level action representations by several LSTMs and then integrate all the person-level action representations into the following LSTM to learn the group-level activity representation. This type of solution is a two-stage strategy, which neglects the “host–parasite” relationship between the group-level activity (“host”) and person-level actions (“parasite”) in spatiotemporal space. To this end, we propose a novel graph LSTM-in-LSTM (GLIL) for group activity recognition by modeling the person-level actions and the group-level activity simultaneously. GLIL is a “host–parasite” architecture, which can be seen as several person LSTMs (P-LSTMs) in the local view or a graph LSTM (G-LSTM) in the global view. Specifically, P-LSTMs model the person-level actions based on the interactions among persons. Meanwhile, G-LSTM models the group-level activity, where the person-level motion information in multiple P-LSTMs is selectively integrated and stored into G-LSTM based on their contributions to the inference of the group activity class. Furthermore, to use the person-level temporal features instead of the person-level static features as the input of GLIL, we introduce a residual LSTM with the residual connection to learn the person-level residual features, consisting of temporal features and static features. Experimental results on two public data sets illustrate the effectiveness of the proposed GLIL compared with state-of-the-art methods.
SECTION I.Introduction
Single-person action recognition, aiming to understand the action performed by a single person (e.g., running and jumping), has achieved great progress for the past decades [1]–[2][3][4]. Compared with the single-person action, a group/collective activity usually indicates a more complex activity scene involving at least two persons’ actions, e.g., people are talking and people are queuing. Since a group activity contains several person-level actions from two or more persons, group activity recognition becomes more challenging than single-person action recognition [5]–[6][7][8].

In the early stages, researchers used various graphical models to tackle the problem of group activity recognition, e.g., hierarchical graphical models [9], AND–OR graphs [10], dynamic Bayesian networks [11], and classical neural networks [12]. Recently, witnessing the success of recurrent neural network (RNN) [13] and long short-term memory (LSTM) [14] in modeling the sequence data, researchers attempted to use RNN to address the problem of the group activity recognition [15]–[16][17][18]. A common two-stage solution is that it first learns person-level action representation by several LSTMs and then integrates all the person-level action representations to learn the group-level activity representation by another LSTM. Such a two-stage solution achieves a significant improvement of the recognition accuracy compared with traditional methods on group activity recognition.

However, the abovementioned two-stage solution ignores the important “host–parasite” relationship between the group-level activity (“host”) and the person-level actions (“parasite”). Obviously, in an activity scene within multiple persons, the person-level actions and group-level activity are co-occurrence over time. Thus, the persons-level actions of the individuals and group-level activity of the scene should be simultaneously modeled by multiple RNNs. In the local view, most of the person-level actions participate in the group-level activity and decide the class of group-level activity. In the global view, a group-level activity involves several person-level actions and binds several person-level actions to a specific activity. For example, in a “walking” activity, most persons, who are walking together, decide the class “walking” of this activity. In turn, the “walking” activity involves most of the “walking” persons. We can see that the group-level activity and person-level actions in an activity scene constitute a host–parasite relationship, which cannot be simulated by the two-stage solution.

Therefore, considering the host–parasite relationship in the group activity, we propose a novel graph LSTM-in-LSTM (GLIL) to simultaneously model the person-level actions and the group-level activity in the spatiotemporal space, as shown in Fig. 1. GLIL becomes a graph LSTM (G-LSTM) in the global view and also becomes several person LSTMs (P-LSTMs)with the interactions in the local view. Specifically, P-LSTMs target to model the person-level actions with interaction among persons; meanwhile, G-LSTM targets to model the group-level activity. G-LSTM and P-LSTMs constitute a host–parasite architecture of GLIL in the spatiotemporal space, which reveals the “host–parasite” relationship between the group-level activity and person-level actions.

Fig. 1. - Idea of the proposed GLIL for modeling a group activity. GLIL becomes P-LSTMs in the local view that models the person-level actions with the interactions among persons and becomes a G-LSTM in the global view that models the group-level activity at the same time. The G-LSTM and P-LSTMs constitute a host–parasite architecture in spatiotemporal space, which simulates the “host–parasite” relationship between the group-level activity (“host”) and person-level actions (“parasite”).
Fig. 1.
Idea of the proposed GLIL for modeling a group activity. GLIL becomes P-LSTMs in the local view that models the person-level actions with the interactions among persons and becomes a G-LSTM in the global view that models the group-level activity at the same time. The G-LSTM and P-LSTMs constitute a host–parasite architecture in spatiotemporal space, which simulates the “host–parasite” relationship between the group-level activity (“host”) and person-level actions (“parasite”).

Show All

The training framework of GLIL is shown in Fig. 2, which stacks a pretrained CNN, a residual LSTM (R-LSTM), the GLIL, and a softmax layer in a bottom–up way. First, we employ a pretrained CNN to extract the static features (i.e., CNN features) of each person on the person’s bounding boxes. Second, we extend an R-LSTM to learn the person-level residual features of each person from their static features. Third, followed by R-LSTM, P-LSTM in GLIL learns and updates the person-level motion state of one person under the interaction with other persons, while a G-LSTM in GLIL selectively aggregates the person-level motion information from P-LSTM into a new group-level memory cell over time. Finally, we feed the group-level activity representation output from GLIL into the softmax layer at each time step and then average the outputs of all the softmax classifiers to infer the class of group activity. This means to perform an average classification score on all the frames over time.

Fig. 2. - Framework of the proposed GLIL for group activity recognition. For each frame, we first feed the CNN features of each person to an R-LSTM unit. Then, R-LSTM learns the person-level residual features of each person, which are treated as the inputs of the following GLIL. For a GLTL unit, one P-LSTM unit learns and updates the person-level motion state with the interactions among persons, while one G-LSTM unit selectively aggregates and stores the person-level motion information from P-LSTMs into a new group-level memory cell. Finally, the group-level activity representation output from the group-level memory cell at each time step is input to the softmax layer, and the averaged softmax score at all time steps is the prediction probability vector of group activity class.
Fig. 2.
Framework of the proposed GLIL for group activity recognition. For each frame, we first feed the CNN features of each person to an R-LSTM unit. Then, R-LSTM learns the person-level residual features of each person, which are treated as the inputs of the following GLIL. For a GLTL unit, one P-LSTM unit learns and updates the person-level motion state with the interactions among persons, while one G-LSTM unit selectively aggregates and stores the person-level motion information from P-LSTMs into a new group-level memory cell. Finally, the group-level activity representation output from the group-level memory cell at each time step is input to the softmax layer, and the averaged softmax score at all time steps is the prediction probability vector of group activity class.

Show All

Overall, the main contributions of this article are summarized as follows.

To address the problem of group activity recognition, we propose a novel GLIL framework by simultaneously modeling the person-level actions and group-level activity, where the architecture of GLIL simulates the “host–parasite” relationship between the group-level activity and the person-level actions.

We design several P-LSTMs to learn the person-level action representations by considering the interactions among persons under a new interaction gate and design a G-LSTM to learn the group-level activity representations.

We conduct experiments on two public data sets (Volleyball data set (VD) [15] and Collective Activity data set (CAD) [6]) to illustrate the effectiveness of the proposed GLIL compared with the state-of-the-art methods.

The rest of this article is organized as follows. Section II reviews some works related to RNN-based action recognition and group activity recognition. Section III introduces some preliminary works. Section IV details the proposed framework. Experiments are conducted in Section V, followed by the conclusions in Section VII.

SECTION II.Related Work
In this section, we briefly review some works related to the RNN-based action recognition and group activity recognition.

A. RNN-Based Action Recognition
Action recognition aims to recognize human action in videos [5], [19]–[20][21]. In the early stages, various spatiotemporal feature learning and feature extraction methods, e.g., histogram of oriented gradients (HOG) [22], histogram of optical flow (HOF) [23], dense trajectories [20], and 3-D SIFT [24], were proposed to represent the human action in videos.

For the last few years, RNNs [13] and LSTM [14] have made great progress in action recognition, due to the powerful ability for handling sequential data with variable length [15], [19], [25]–[26][27][28]. For example, Donahue et al. [19] proposed a long-term recurrent convolutional network for action recognition by stacking the CNN layer and RNN/LSTM layer in a bottom–up way. Subsequently, some works utilized RNN/LSTM to model the spatial relationship among data when modeling human action. For example, Wang and Wang et al. [29] proposed a two-stream architecture, including a temporal RNN and a spatial RNN to model temporal motions of individuals over time and spatial relation among skeleton joints.

In the meantime, kinds of RNN architectures were built to model human action based on various ideas [25], [30]–[31][32]. For example, to capture the change degree of motion information between two consecutive frames, Veeriah et al. [27] designed a derivative of the motion state between the gates in LSTM. Moreover, Shahroudy et al. [30] and Liu et al. [28] proposed to divide the memory cell in LSTM into multiple subcells corresponding to different human skeleton parts, which models the motion of skeleton parts over time.

B. Group Activity Recognition
Group activity recognition aims to automatically understand an activity performed by at least two persons, which has been developed into an attractive topic [8], [9], [33]–[34][35]. In the early stages, researchers designed various handcrafted features to represent person-level actions or group-level activities [6], [8], [36], [37]. Since deep learning has achieved great progress in image/video classification tasks [38]–[39][40], many deep learning-based activity recognition methods have been proposed in recent years [15], [17], [33], [41]. As one of the most representative works, a hierarchical LSTM model in [15] is proposed to first utilize several LSTMs to learn the person-level action representations over time and then integrate the person-level action representations of all persons into the following LSTM to learn the group-level activity representation over time.

Subsequently, some deep learning-based activity recognition methods assumed that persons are not independent in the group activity and considered modeling interactions among persons in a group activity [16], [18], [31], [42]. For example, Wang et al. [18] extended an RNN-based hierarchical framework to learn three-level motions in a step-by-step way, i.e., person-level actions, person–person interactions, and group-level activity corresponding to the individuals, multiple persons, and the activity scene. Considering the different contributions of individuals in a group activity, Tang et al. [31] proposed a coherence-constrained G-LSTM with spatiotemporal context coherence constraint and a global context coherence to effectively recognize group activity by modeling the relevant motions of individuals while suppressing the irrelevant motions. However, there methods model the person-level action and group-level activity in a step-by-step way, which ignores the fact that person-level action and group-level activity happen at the same time.

Therefore, some works considered modeling the person-level action and group-level activity simultaneously [33], [43], [44]. For example, Deng et al. [43] regarded all persons and the whole activity scene as the nodes of a graph and further exploited multiple RNNs to model the person-level action of persons and the group-level activity of the scene in a graph model. This method regards the group-level activity as a graph node, which is equal to the person-level action. In fact, in an activity scene, the person-level action and the group-level activity are not equivalent, while the truth is that the person-level actions participate in the group-level activity. A reasonable assumption is that the group-level activity and the person-level actions constitute a host–parasite relationship, as discussed in Section I. Therefore, we consider designing a new architecture of LSTM to simulate such host–parasite relationship for modeling group activity well.

Recently, Wu et al. [45] and Azar et al. [46] also explored the activity recognition and achieved the state-of-the-art performance. The difference between the proposed GLIL and these two recent methods is detailed in the following.

For the idea, both of Wu et al. [45] and Azar et al. [46] considered aggregating all predictions made at the level of single persons into the prediction made at the level of the whole group. GLIL directly outputs the prediction made at the level of the whole group by aggregating the single-person memories instead of the single-person predictions.

For the model formulation, the previous models proposed by Azar et al. [46] and Wu et al. [45] are based on the convolution network. These models only consider the spatial relationships among persons in the spatial space. Different from them, GLIL is originally based on both of the LSTM and graph structure, where the graph structure considers the spatiotemporal relation among persons in both of the temporal and spatial spaces.

For the architecture design, convolutional relational machine proposed by Azar et al. [46] can be seen as a new convolutional neural network, and Wu et al. [45] constructed actor relation graphs that implement on the existing graph convolutional network. Different from them, GLIL designs a bioinspired host–parasite G-LSTM, which can be seen as a new LSTM architecture.

Since generative adversarial network (GAN) has become beneficial in generating the data/feature and learning the loss function at the same time, Gammulle et al. [47] first utilized GAN to learn the “action code” for the group activity, which is the same as to the ground-truth label. Even so, Azar et al. [46] employed LSTM as the core module to learn the temporal action representation over time. Compared with LSTM, GLIL is a relatively flexible and superior architecture, which can also be embedded into the GAN framework.

SECTION III.Preliminary
This section introduces some preliminary works, such as LSTM and graph construction, which can provide basic knowledge and background.

A. Long Short-Term Memory
Given a video clip {xt|t=1,…,T} with T frames, where xt is the static feature (such as CNN feature [48]) of the t th frame, we use the standard LSTM [14] to learn a sequence of hidden states {ht|t=1,…,T} to describe the dynamic of this video clip. The standard LSTM mainly consists of an input gate, forget gate, output gate, input modulation gate, and memory cell state, and one common LSTM unit at time step t can be repressed as follows:
it=ft=ot=gt=ct=ht=σ(Wix⋅xt+Wih⋅ht−1+bi);σ(Wfx⋅xt+Wfh⋅ht−1+bf);σ(Wox⋅xt+Woh⋅ht−1+bo);φ(Wgx⋅xt+Wgh⋅ht−1+bg);fst⊙ct−1+it⊙gt;ot⊙φ(ct),(1)(2)(3)(4)(5)(6)
View SourceRight-click on figure for MathML and additional features.where it , ft , ot , gt , and ct are the input gate, forget gate, output gate, input modulation gate, and memory cell state, respectively; σ(⋅) is a sigmoid function; ⊙ denotes elementwise product, φ(⋅) is a hyperbolic tangent tanh(⋅) ; W∗x and W∗h are weight matrices; and b∗ is bias vector. Specifically, the input gate it controls the contributions of the newly arrived input data at time step t for updating the memory cell, while the forget gate ft determines how much the contents of the previous state ct−1 contribute to deriving the current state ct . The output gate ot learns how the output of the LSTM unit at time step t should be derived from the current state of the memory cell ct . More details can be found in [14].

B. Graph Construction
This article aims to understand the complex group activity and recognizing different group activities by considering the participating degree. For a group activity, each video frame contains multiple-persons’ motion information, which is interrelated in both the spatial space and temporal space. In this article, we consider constructing a graph to explore such relations among persons’ motion. Specifically, the nodes of the graph can represent the state of data, and the edges can capture the spatiotemporal interactions among nodes.

Specifically, given a video clip with T frames describing a group activity within p persons, we construct a relational graph Gt={V,ES,ET} for the t th frame by connecting a set of graph nodes {vts|s=1,2,…,p,andt=1,2,…,T} with the graph edges ES and ET in the spatial space and the temporal space, respectively. Here, vts denotes the feature of the s th person’s motion at time step t . For each node vts , there are two temporal edges connecting to the previous node vt−1s and the subsequent node vt+1s in temporal space and p−1 edges connecting to its neighboring nodes {vtjs}js∈Ω(s) in spatial space, where Ω(s)={1,2,…,s−1,s+1,…,p} . Fig. 3 shows a toy example of graph construction for one activity within three persons.

Fig. 3. - Toy example of graph construction for one activity within three persons.
Fig. 3.
Toy example of graph construction for one activity within three persons.

Show All

SECTION IV.Proposed Framework
The framework of GLIL for modeling group activity is shown in Fig. 2, which stacks a pretrained CNN, a new R-LSTM, the GLIL (P-LSTMs or G-LSTM), and a softmax layer in a bottom–top way. These components will be introduced in this section.

A. Residual LSTM
For a video clip describing a group activity within p persons, we first employ a pretrained CNN [49] to extract the person-level static features (i.e., CNN features) of each person on person bounding box at each time step, denoted by {xts}Tt=1 , where s=1,2,…,p . Subsequently, for the s th person, we learn the person-level temporal features {x~ts}Tt=1 over time by an LSTM with the residual connection, called R-LSTM in this article. Here, witnessing the success of the deep residual network [50], we add a residual connection across the input and the output of LSTM [51], [52]. Such residual connection can provide better flexibility to deal with the gradient vanishing or exploding in the learning process [50], [53]. R-LSTM combines the person-level static features {xts}Tt=1 and the person-level temporal features {x~ts}Tt=1 into the person-level residual features {dts}Tt=1 , namely dts≜xts+x~ts , s=1,2,…,p . Finally, the obtained person-level residual features {dts}Tt=1 can be seen as the node vts in Gt , which is fed into the proposed GLIL.

B. Parasite Architecture of GLIL
Based on the constructed graph Gt , we build the architecture of GLIL, as shown in Fig. 4. Here, GLIL becomes several P-LSTMs in the local view, where the architecture of one P-LSTM is shown in Fig. 4(a). In the global view, GLIL becomes a G-LSTM, as shown in Fig. 4(b), where one P-LSTM acts as a graph node. G-LSTM and P-LSTMs constitute a “host–parasite” architecture. In Fig. 4(b), the s th P-LSTM has an input gate its , forget gate fts , output gate ots , and the neighboring forget gates ftjs at time step t . These gates are decided by the input feature dts (output from the R-LSTM) at the current time step, a person-level motion state ht−1s at time step (t−1) , and a neighboring motion state h¯t−1s from its spatial neighbors at time step (t−1) , respectively. Formally, the s th P-LSTM at time step t is formulated as follows:
its=fts=ftjs=ots=σ(Wisdts+Uisht−1s+Vish¯t−1s+bis)σ(Wfsdts+Ufsht−1s+Vfsh¯t−1s+bfs)σ(Wfjsdts+Ufjsht−1s+Vfjsh¯t−1s+bfjs), js∈Ω(s)σ(Wosdts+Uosht−1s+Vosh¯t−1s+bos)(7)(8)(9)(10)
View SourceRight-click on figure for MathML and additional features.where W∗∗ , U∗∗ , and V∗∗ are the weight matrices, and b∗∗ is the bias vector.

Fig. 4. - Host–parasite architecture of the proposed GLIL. In the local view, GLIL becomes P-LSTMs that model the person-level motions by the neighboring interaction under an interaction gate. In the global view, GLIL becomes a G-LSTM that models group-level motion by integrating all person-level memory cells under a role gate. Such role gate checks the importance of one person-level motion for inferring the class of group-level activity at each time step. (a) P-LSTM (parasite architecture) in GLIL. (b) G-LSTM (host architecture) in GLIL.
Fig. 4.
Host–parasite architecture of the proposed GLIL. In the local view, GLIL becomes P-LSTMs that model the person-level motions by the neighboring interaction under an interaction gate. In the global view, GLIL becomes a G-LSTM that models group-level motion by integrating all person-level memory cells under a role gate. Such role gate checks the importance of one person-level motion for inferring the class of group-level activity at each time step. (a) P-LSTM (parasite architecture) in GLIL. (b) G-LSTM (host architecture) in GLIL.

Show All

1) Person-Level Action Representation:
The output of P-LSTM at time step t is a person-level motion state hts of the s th person (i.e., the person-level action representation hts of the s th person at time step t ), which can be computed as follows:
gts=cts=hts=φ(Wgsdts+Ugsht−1s+Vgsh¯t−1s+bgs)its⊙gts+fts⊙ct−1s+∑js∈Ω(s)fjs⊙ctjsots⊙φ(cts)(11)(12)(13)
View SourceRight-click on figure for MathML and additional features.where cts is the person-level memory cell at time step t . Equation (7)–(13) represent the basic model of P-LSTM. In the following, we will introduce some new components in P-LSTM compared with the conventional LSTM.

2) Neighboring Motion State:
It is noted that the neighboring motion state h¯t−1s in (7)–(13) is averaged by the persons-level motion states of all neighboring persons of the s th person as follows:
h¯t−1s=∑js∈Ω(s)rtjsht−1js.(14)
View SourceRight-click on figure for MathML and additional features.In (14), rtjs denotes an interaction gate of the js th neighboring person corresponding to the s th person.

3) Interaction Gate:
As mentioned earlier, two persons usually interact over time in a group activity. Specifically, for the s th person’s motion at time step t , different neighboring persons interact with different degrees over time. In most cases, if two persons are standing closely, and performing similar motions, they are intensively interacting. Therefore, we can simultaneously use the feature similarity and location similarity of two persons to measure their interaction. Specifically, we use the central position of person’s bounding box to denote the person’s location. Let ats and atjs denote the locations of the s th person and her/his js th neighboring person, respectively, and we design an interaction gate rtjs to quantify the interaction between two persons at time step t as follows:
rtjs===λ⋅SimFeaturet(s,js)+(1−λ)⋅SimLocationt(s,js);SimFeaturet(s,js)X(∣∣∣∣ht−1s−ht−1js∣∣∣∣2)∑j∈Ω(s)X(∣∣∣∣ht−1s−ht−1js∣∣∣∣2);SimLocationt(s,js)X(∣∣∣∣ats−atjs∣∣∣∣2)∑js∈Ω(s)X(∣∣∣∣ats−atjs∣∣∣∣2)(15)
View SourceRight-click on figure for MathML and additional features.where X(⋅)=1/exp(⋅) , and λ is a coefficient to balance two terms.

C. Host Architecture of GLIL
In Fig. 4(b), G-LSTM resembles a host that fosters several P-LSTMs. In G-LSTM, the person-level motion information in all person-level memory cells cts of P-LSTM is integrated into a new graph-level memory cell ct of G-LSTM. The group-level memory cell combines all person-level motions into the group-level motion. Although all person-level motion information contributes to the inference of the group activity class, their contributions are different. Thus, we hope the group-level memory cell can selectively integrate and store the useful person-level motion information when cooking group-level motion. Similar to a previous work [42], we can set a gate to control what types of person-level motion information would enter or leave the group-level memory cell over time. Here, we design a new role gate πts at time step t to allow the person-level motion of the s th person to enter or level group-level memory cell.

1) Role Gate:
To design the role gate πts , we need to answer a question: what type of person-level motion is useful to infer the class of group activity? Obviously, if we only use one person-level action representation at the previous time step to accurately infer the class of group activity, the person-level motion at this time step is useful and to be allowed into the group-level memory cell. Therefore, we measure the consistency of the label inference of group-level activity representation and person-level action representation at the previous time step. Formally, the role gate πts at time step t is defined as follows:
πts=σ(Wπsqt−1+Uπsqt−1s+bπs),s∈{1,2,…,p}(16)
View SourceRight-click on figure for MathML and additional features.where qt−1 and qt−1s denote two predicted label vectors of the activity class, which are obtained via group-level activity representation ht−1 [defined in (22)] and person-level action representation ht−1s , respectively. Specifically, if we let L denote the class number of group activity, qt−1=[qt−11,…,qt−1l,…,qt−1L]T and qt−1s=[qt−1s1,…,qt−1sl,…,qt−1sL]T are obtained as follows:
qt−1l=qt−1sl=exp(zt−1l)∑Li=1exp(zt−1i)exp(zt−1sl)∑Li=1exp(zt−1si)(17)(18)
View SourceRight-click on figure for MathML and additional features.where zt−1=[zt−11,…,zt−1l,…,zt−1L]T and zt−1s=[zt−1s1,⋯,zt−1sl,…,zt−1sL]T are the confidence score vectors of group-level action representation ht−1 and person-level action representation ht−1s at time step (t−1) , respectively, that is
zt−1=zt−1s=φ(Wzht−1+bz)φ(Wzsht−1s+bzs),s∈{1,2,…,p}.(19)(20)
View SourceRight-click on figure for MathML and additional features.

2) Group-Level Memory Cell:
When we obtain the role gate πts , we selectively integrate all person-level memory cells {cts}ps=1 into the group-level memory cell ct via the corresponding role gates πts , as follows:
ct=∑s=1pπts⊙cts.(21)
View SourceRight-click on figure for MathML and additional features.

3) Group-Level Activity Representation:
In G-LSTM, its output at time step t is the group-level activity state ht (i.e., group-level activity representation ht at time step t ), that is
ht=ot⊙φ(ct)(22)
View SourceRight-click on figure for MathML and additional features.where ot is the output gate of G-LSTM, which is defined as follows:
ot=σ(∑s=1pGoshts+Goht−1+bo)(23)
View SourceRight-click on figure for MathML and additional features.where G∗∗ is the weight matrix.

Finally, we compute the confidence score vector zt of the group activity class by (19), and feed zt (t=1,2,…,T ) into a softmax layer, that is
yt=softmax(zt),t=1,2,…,T.(24)
View SourceRight-click on figure for MathML and additional features.The outputs of all the softmax classifiers corresponding to all frames are averaged to obtain the probability class vector of group activity.

SECTION V.Experiments
In the experiments, we evaluate the performance of the proposed GLIL compared with the state-of-the-art methods on two public data sets, i.e., CAD [6], and VD [15].

A. Data Set
Two data sets used in the experiments are described as follows.

CAD [6]: It contains 44 video clips with five types of activities, i.e., “crossing,” “waiting,” “queuing,” “walking,” and “talking.” Similar to [36] and [58], we select two-thirds of the video clips from each activity class to form the training set, and the rest are used for testing. The person bounding boxes (tracklets) used in the experiments are provided in [9]. Since the number of persons is varying in a range [1, 12], we randomly select five effective persons for each frame and regard them as a group activity.1 Here, if the number of persons in one frame is less than five or some humans dynamically exiting the scene/group at one time step, we take a full-zero matrix as a new person bounding box. Following the experimental setting in [17], [18], and [46], we merge class “walking” and “crossing” as “moving” due to the imbalanced test set.

VD [15]: It consists of 55 videos within 4830 annotated frames. This data set provides the person bounding box of each person in each frame, and a group activity class for each video clip, e.g., “left pass,” “right pass,” “left set,” “right set,” “left spike,” “right spike,” “left win,” or “right win.” Following the setting in [15], two-thirds of the annotated frames are selected for training, while the rest ones for testing. In this data set, each group activity contains two subgroups corresponding to two teams. Similar to [15] and [42], all person-level residual features in one subgroup are first input to a GLIL for learning the representations of subgroup-level activity. Then, we concatenate the activity representations of two subgroups as the representation of the whole group-level activity at each time step.

B. Experimental Setting
We use Torch toolbox and Tensorflow [59] as the deep learning platform to conduct the experiments. Following in [60], we employ the VGG16 pretrained on ImageNet to extract the person-level CNN features (on the FC-15 layer of VGG16) based on the person bounding box at each time step. In the VD and CAD data sets, we consider ten frames from each video without any resampling; namely, the length T of time steps for a video clip is set to 10. We also use the other recent networks to extract the person-level CNN features for fair comparison with prior methods. In the configuration of R-LSTM, the number of input nodes and the number of output nodes are set to 4096 for the residual connection. The number of output nodes in P-LSTM and the number of nodes in G-LSTM are set to 1024. We use the Adam algorithm [61] as the optimizer. The learning rate, momentum, and decay rate are set to 0.5×10−3 , 0.9, and 0.95, respectively. For the network initialization, such as traditional deep neural networks, we use the random normal distribution (mean = 0 and variance = 0.01) to initialize the parameters of GLIL. We select the parameter λ in (15) from the values of {0.1, 0.3, 0.5, 0.7, 0.9}. Through the experimental validation, we ultimately set lambda = 0.7 as the optimal value. We use the mean per-class accuracy (MPCA) as the performance metric.

C. Time Complexity
Let d denote the feature dimension of the person-level CNN feature. In R-LSTM, the number of the output nodes is also set d for the skip connection. For the forward propagation of R-LSTM, the time complexity mainly comes from the matrix computation in input gate, forget gate, output gate, and input modulation gate, namely O(ResidualLSTM)=O(4pd2Tn) , where p , T , and N are the number of persons, the number of time steps, and the number of video clips, respectively. Since we use backpropagation through time (BPTT) to minimize the loss function, the time complexity of backpropagation is equal to that of forward propagation. Thus, the time complexity of R-LSTM is O(ResidualLSTM)=2O(4pd2Tn) in total. Likewise, the time complexity of P-LSTM in input gate, forget gate, output gate, input modulation gate, and neighboring forget gates is O(PersonLSTM)=2(O(4pd2Tn)+(p−1)O(pd2Tn)) . Let m denote the number of output nodes of P-LSTM and the number of output nodes of G-LSTM, and the time complexity of G-LSTM in role gate and output gate is O(GraphLSTM)=2(pO(2LmTn)+(p+1)O(LmTn)+O((p+1)m2Tn)) , where L denotes the number of the activity classes. Thus, the time complexity of the proposed GLIL is O(GLIL)=2E(2O(4pd2Tn)+(p−1)O(pd2Tn)+pO(2LmTn))+2E((p+1)O(LmTn)+O((p+1)m2Tn)) , where E denotes the number of epochs. In the experiments, the training of GLIL begins to converge after about 60 and 110 epochs on the CAD and VD data sets, respectively. Then, the time consumption for training GLIL on CAD and VD requires about 10 and 55 h, respectively.

D. Baselines
In the experiments, several baselines are defined to illustrate the novelty of the proposed GLIL.

One LSTM: This baseline treats all person-level actions as a whole to directly learn the group-level activity via an LSTM. First, multiple-person bounding boxes at one time step are merged into a bigger bounding box. Second, the CNN feature is extracted on this “bigger” bounding box at each time step. Third, we use the CNN features as inputs to train an LSTM.

Multiple LSTMs: This baseline learns the person-level actions by multiple LSTMs. First, the CNN features of each person are fed into an LSTM to learn the person-level action representation. Third, all person-level action representations at one time step are concatenated into the group-level activity representation.

Hierarchical LSTM: This baseline is a two-stage solution in a hierarchical way. It first learns the person-level action representations of all persons by multiple LSTMs and then learns the group-level activity representations by the following LSTM in a hierarchical way. The idea of this baseline is the same as Ibrahim et al. [15].

Hierarchical R-LSTM: This baseline is a two-stage solution in a hierarchical way. The CNN features of each person are input to the R-LSTM, followed by a conventional LSTM. This baseline aims to test the contribution of R-LSTM compared with B3.

GLIL Without R-LSTM: This baseline throws out the R-LSTM in the proposed GLIL. Thus, the CNN features of each person are directly fed into GLIL, followed by the softmax layer at each time step. This baseline aims to illustrate the power of the GLIL network.

E. Results on CAD
1) Ablation Studies:
We first illustrate the novelty of the proposed GLIL by comparing it with several baselines. As shown in Table I, GLIL achieves the best performance compared with all baselines. Since B3–B5 model group-level activity in a hierarchical way, they perform better than both B1 and B2. B4 outperforms B3, which illustrates that training the stacked LSTMs in a hierarchical way benefits from the residual connection. Compared with B5, B4 with the R-LSTM and hierarchical architecture can be learned by setting a greater number of epochs. This ensures that B4 gains more discriminative power for some activity, which has few outlier persons (noise), such as the queuing activity. However, for some activities within a certain number of outlier persons (noise), such as the waiting activity, B5 with the “host–parasite” architecture is very useful to remove these outlier persons. This is because the host–parasite architecture of GLIL (in B5) revealing the consistence of the group-level activity and person-level actions can filter the outlier persons. When we further use the person-level residual features instead of the person-level static feature, GLIL improves 0.68% again compared with B5. The confusion matrix of GLIL on CAD is shown in Fig. 5. We can see that “waiting” and “talking” activities are more confusing since they are visually similar to each other.

TABLE I Comparison Among Different Methods on the CAD Data Set
Table I- 
Comparison Among Different Methods on the CAD Data Set
Fig. 5. - Confusion matrix of the proposed GLIL (VGG16 as the backbone) on CAD.
Fig. 5.
Confusion matrix of the proposed GLIL (VGG16 as the backbone) on CAD.

Show All

2) Comparison With the State of the Art:
We also compare the recognition accuracy of the proposed GLIL with several competitive methods, including nondeep learning-based methods (Choi et al. [6], Wang et al. [18], Lan et al. [36], Zhou et al. [54], and Antic and Ommer [63], Kong et al. [64] and deep learning methods (Ibrahim et al. [15], Donahue et al. [19], Deng et al. [33], Shu et al. [42], Deng et al. [43], Wu et al. [45], Azar et al. [46], Gammulle et al. [47], Hajimirsadeghi and Mori [55], Li and Chuah [56], and Qi et al. [60]). The recognition accuracies obtained by these methods are shown in Table I. GLIL achieves the best average accuracy compared with alternatives. Specifically, GLIL improves approximately 7% improvements compared with one of the most representative hierarchical LSTM methods (the two-stage solution) [15] and approximately 23% improvements compared with one classical method (releases this data set) [6]. Not only that, GLIL also performs better than some semantic-based methods (e.g., Li and Chuah [56], and Qi et al. [60]) that use the person-level action label information (as the external prior information) to learn the network. Here, we do not use the person-level label information to learn the GLIL model. If we set the same backbone, the proposed GLIL is comparable to the state-of-the-art methods (e.g., Wu et al. [45], Azar et al. [46], and Gammulle et al. [47]). Some recognition results obtained by GLIL are shown in Fig. 6(a).

Fig. 6. - Some recognition results obtained by the proposed GLIL on data sets. On CAD, no more than five persons are randomly selected to represent the whole activity. The bounding box on (a) CAD. (b) VD.
Fig. 6.
Some recognition results obtained by the proposed GLIL on data sets. On CAD, no more than five persons are randomly selected to represent the whole activity. The bounding box on (a) CAD. (b) VD.

Show All

F. Results on VD
1) Ablation Studies:
The recognition accuracy of the proposed GLIL compared with the baselines is shown in Table II. GLIL achieves the best average accuracy performance on all activity classes. Compared with B1 and B2, the two-stage solution (i.e., B3 and B4) has achieved a significant improvement in recognition accuracy. Specifically, the recognition results obtained by B3 and B4 successfully validate the importance of the residual connection for learning the hierarchical LSTMs. In comparison to B3 and B4, the improvement obtained by B5 demonstrates that simultaneous capturing person-level motion and group-level motion are effective for recognizing group activities. Specifically, in the lpass, rpass, lset, and rset activity scenes within some outlier persons, B5 performs much better than B4 due to the “host–parasite” architecture of B5. Furthermore, we push GLIL into a hierarchical framework within an R-LSTM and obtain better recognition accuracy. The confusion matrix of the proposed GLIL on the VD is shown in Fig. 7. We find that the confusion occurs due to visually similar motions, e.g., “right set” and “right pass.”

TABLE II Comparison Among Different Methods on the VD [15]. “Lpass,” “Rpass,” “Lset,” “Rset,” “Lspike,” “Rspike,” “Lwinpoint,” and “Rwin Point” Denote “Left Pass,” “Right Pass,” “Left Set,” “Right Set,” “Left Spike,” “Right Spike,” “Left Winpoint,” and “Right Winpoint,” Respectively
Table II- 
Comparison Among Different Methods on the VD [15]. “Lpass,” “Rpass,” “Lset,” “Rset,” “Lspike,” “Rspike,” “Lwinpoint,” and “Rwin Point” Denote “Left Pass,” “Right Pass,” “Left Set,” “Right Set,” “Left Spike,” “Right Spike,” “Left Winpoint,” and “Right Winpoint,” Respectively
Fig. 7. - Confusion matrix of the proposed GLIL (VGG16 as the backbone) on VD.
Fig. 7.
Confusion matrix of the proposed GLIL (VGG16 as the backbone) on VD.

Show All

2) Comparison With the State of the Art:
We compare the performance of the proposed GLIL with several competitive methods. The recognition accuracies obtained by different methods are shown in Table II. Among these methods, Shu et al. [16], Bagautdinov et al. [34], Biswas and Gall [44], Tang et al. [57], and Ibrahim et al. [62] do not provide the recognition accuracy of each class, and Li and Chuah [56] ignores the classes of “left winpoint” and “right winpoint.” As expected, GLIL achieves the best performance on average. In particular, GLIL achieves approximately 8% improvement compared with one original work [15] that releases the VD. More importantly, the MPCA obtained by GLIL is comparable to the state-of-the-art methods (e.g., Wu et al. [45], Azar et al. [46], and Gammulle et al. [47]). These results demonstrate that GLIL with a host–parasite architecture can effectively simulate the relationship between the person-level actions and group activity. Finally, we show some recognition results obtained by GLIL in Fig. 6(b).

SECTION VI.Discussion
In this article, we build a novel deep network architecture, called GLIL, by simultaneously modeling the person-level actions and group-level activity. In the architecture of GLIL, several P-LSTMs locate inside the G-LSTM, where the memory cells of P-LSTMs link to a common graph-level memory cell of graph-LSTM. It can be seen as that GLIL constructs a new “host–parasite” architecture. Such new architecture is different from the traditional hierarchical architecture, e.g., Ibrahim et al. [15], Yan et al. [17], and Wang et al. [18]. For traditional graph architecture, there are two nodes that are linked by edge, and several nodes construct a graph. For GLIL, two P-LSTMs are not directly linked by an edge, and all P-LSTMs are linked to a common graph-LSTM, as shown in Fig. 3(b). Thus, GLIL is also different from the Graph RNN/LSTM, Tang et al. [31], Deng et al. [33], Deng et al. [43], and Biswas and Gall [44]. To the best of our knowledge, GLIL is a new architecture for modeling the person-level actions and group-level activity.

SECTION VII.Conclusion and Future Work
In this article, to address the problem of group activity recognition, we propose a novel GLIL framework by modeling the person-level actions and the group-level activity simultaneously. In the host–parasite architecture of GLIL, several P-LSTMs in the local view model the person-level actions (parasites) based on the interactions among persons, while a G-LSTM in the global view models the group-level activity (host). Furthermore, to utilize the temporal features instead of the static features as the input of GLIL, we extend an R-LSTM to learn the person-level residual features (including static features and temporal features) of each person, in which a residual connection can avoid the gradient vanishing or exploding in the learning process to some extent. Experimental results on two public data sets demonstrate that the proposed GLIL has improved the recognition accuracy compared with the state-of-the-art methods. In future work, we will embed the proposed GLIL into the GAN framework to train an activity recognizer in a few-shot learning way.