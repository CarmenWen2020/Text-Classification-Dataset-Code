The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.
SECTION I.Introduction
In the era of big data, the vast quantity of information poses the demand for effective and efficient analysis, interpretation, and prediction to explore the benefits lie ahead. Because of the big data, the machine learning community tells many success stories [1]–[2][3][4] while still leaving challenges. The Gaussian process regression (GPR) [5], also known as Kriging in geostatistics [6] and surrogates or emulators in computer experiments [7], is a nonparametric statistical model used in various scenarios, e.g., active learning [8], multitask learning [9], [10], manifold learning [11], and optimization [12].

Big data in the GP community mainly refers to one of the 5V challenges [13]: the volume that represents the huge amount of data points to be stored, processed, and analyzed, incurring high computational complexity for current GP paradigms. It is worth noting that this review mainly focuses on scalable GPs for large-scale regression but not on all forms of GPs or other machine learning models.

Given n training points X={xi∈Rd}ni=1 and their observations y={yi=y(xi)∈R}ni=1 , GP seeks to infer the latent function f:Rd↦R in the functional space GP(m(x),k(x,x′)) defined by the mean m(.) and the kernel k(.,.) . The most prominent weakness of standard GP is the cubic time complexity O(n3) due to the inversion and determinant of the kernel matrix Knn=k(X,X)∈Rn×n . This limits the scalability of GP and makes it unaffordable for large-scale data sets.

Hence, scalable GPs devote to improving the scalability of full GP while retaining the favorable prediction quality for big data. The extensive literature review summarized in Fig. 1 classifies scalable GPs into two main categories.

Global Approximations: It approximates the kernel matrix Knn through global distillation. The distillation can be achieved by the following:

a subset of the training data with m (m≪n ) points (subset of data (SoD) [14]), resulting in a smaller kernel matrix Kmm ;

the remove of uncorrelated entries in Knn (sparse kernels [15]), resulting in a sparse kernel matrix K~nn with many zero entries;

the low-rank representation measured between m inducing points and n training points (sparse approximations [1], [16]–[17][18]), resulting in the Nyström approximation Knn≈KnmK−1mmKmn .

Local Approximations: It follows the divide-and-conquer (D&C) idea to focus on the local subsets of training data. Efficiently, local approximations only need to tackle a local expert with m0 (m0≪n ) data points at each time [19], [20]. In addition, to produce smooth predictions equipped with valid uncertainty, modeling averaging has been employed through mixture or product of experts [21]–[22][23][24][25][26][27][28].

Fig. 1. - Percentages of the categories for (a) scalable GPs including (b) global approximations and (c) local approximations in the literature surveyed.
Fig. 1.
Percentages of the categories for (a) scalable GPs including (b) global approximations and (c) local approximations in the literature surveyed.

Show All

As depicted in Fig. 2, in terms of scalability, most of the sparse approximations using m inducing points and the local approximations using m0=m data points for each expert have the same training complexity as O(nm2) . They can further speed up through parallel/distributed computing [20], [29]–[30][31][32][33]. When organizing the inducing points into the Kronecker structure, sparse approximations can further reduce the complexity to O(n) [18], [34]. In the meantime, by reorganizing the variational lower bound, stochastic optimization is available for sparse approximations with a remarkable complexity of O(m3) [1], [35], [36], enabling the regression with million- and even billion-sized data points [36], [37].

Fig. 2. - Comparison of scalable GPs regarding scalability and model capability, where 
$0 < \alpha < 1$
 and 
$m$
 is the inducing size for sparse approximations and the subset size for SoD and local approximations.
Fig. 2.
Comparison of scalable GPs regarding scalability and model capability, where 0<α<1 and m is the inducing size for sparse approximations and the subset size for SoD and local approximations.

Show All

It is notable that we welcome GPs with high scalability but require favorable predictions, i.e., good model capability. For example, though showing a remarkable complexity of O(m3) , we cannot expect the SoD to perform well with increasing n . In terms of model capability, global approximations are capable of capturing the global patterns (long-term spatial correlations) but often filter out the local patterns due to the limited global inducing set. In contrast, due to the localization, local approximations favor capturing local patterns (nonstationary features), enabling them to outperform global approximations for complicated tasks (see the solar example in [38]). The drawback, however, is that they ignore the global patterns to risk discontinuous predictions and overfitting. Recently, attempts have been made to improve the model capability through, for example, the interdomain strategy [39], hierarchical structure [40], and hybrid of global & local approximations or neural networks (NNs) [34], [41], [42], showcasing the state-of-the-art performance [34], [43].

The development and success of scalable GPs pose the demand for comprehensive review, including the methodological characteristics and comparisons for better understanding. To the best of our knowledge, a detailed survey on various scalable GPs has not been conducted in the literature before.1 Thus, such a review in the GP community is timely and important due to the explosion of data size.

To this end, we consider a skeletal overview in Fig. 3 to classify, review, and analyze state-of-the-art scalable GPs. Specifically, with a quick introduction of standard GP regression in Section II, the two main categories of scalable GPs, global and local approximations, are then comprehensively reviewed in Sections III and IV. Moreover, Section V reviews the improvements for scalable GPs in terms of scalability and capability. Thereafter, Section VI discusses the extensions of scalable GPs in different scenarios to highlight potential research avenues. Finally, Section VII offers concluding remarks.

Fig. 3. - Skeletal overview of scalable GPs.
Fig. 3.
Skeletal overview of scalable GPs.

Show All

SECTION II.GP Regression Revisited
The nonparametric GPR places a GP prior over the latent function as f(x)∼GP(m(x),k(x,x′)) [5]. The mean function m(x) is often taken as zero. The kernel k(x,x′) controls the smoothness and is often taken as the squared exponential (SE) function equipped with automatic relevance determination (ARD)
kSE(x,x′)=σ2fexp(−0.5(x−x′)TΔ−1(x−x′))(1)
View Sourcewhere Δ=diag[l21,…,l2d] comprises the length scales along d dimensions and σ2f is the signal variance. For other conventional kernels, e.g., the Matérn kernel, please refer to [5].

Given the training data D={X,y} , where y(xi)=f(xi)+ϵ with the i.i.d. noise ϵ∼N(0,σ2ϵ) , we obtain the model evidence (marginal likelihood) p(y|θ)=∫p(y|f)p(f)df=N(y|0,Kϵnn) , where Kϵnn=Knn+σ2ϵIn . The hyperparameters θ could be inferred by maximizing
logp(y)=−n2log2π−12log∣∣Kϵnn∣∣−12yT(Kϵnn)−1y(2)
View SourceRight-click on figure for MathML and additional features.which automatically achieves the bias-variance tradeoff. We omit θ from the conditioning of distribution for clarity.

Thereafter, the predictive distribution p(f∗|D,x∗)=N(f∗|μ(x∗),σ2∗(x∗)) at a test point x∗ has the mean and variance, respectively, expressed as
μ(x∗)=σ2(x∗)=k∗n(Kϵnn)−1yk∗∗−k∗n(Kϵnn)−1kn∗(3a)(3b)
View Sourcewhere k∗n=k(x∗,X) and k∗∗=k(x∗,x∗) . For y∗ , we need to consider the noise such that p(y∗|D,x∗)=N(y∗|μ(x∗),σ2∗(x∗)+σ2ϵ) .

Alternatively, we can interpret the GP from the weight-space view as an extension of the Bayesian linear model as
f(x)=ϕ(x)Tw,y(x)=f(x)+ϵ(4)
View SourceRight-click on figure for MathML and additional features.where the Gaussian prior is placed on the weights as p(w)=N(w|0,Σ) , and ϕ(x)=[ϕ1(x),…,ϕv(x)]T maps the d -dimensional input x into a v -dimensional feature space. Equivalently, we derive the kernel as k(x,x′)=ϕ(x)TΣϕ(x′) . Particularly, the SE kernel (1) can be recovered from an infinite number (v→∞ ) of the Gaussian-shaped basis functions {ϕc(x)}vc=1 centered everywhere.

The computational bottleneck in (2) is solving the linear system (Kϵnn)−1y and the determinant |Kϵnn| . Traditionally, we use the O(n3) Cholesky decomposition Kϵnn=LLT such that (Kϵnn)−1y=LT∖(L∖y) and log|Kϵnn|=2∑ni=1logLii . As for predictions in (3), the mean costs O(n) and the variance costs O(n2) per test case through precomputations.

The scalable GPs have been extensively presented and studied in recent years to improve the scalability for big data. In the following, we classify the scalable GPs into global approximations and local approximations and comprehensively analyze them to showcase the methodological characteristics.

SECTION III.Global Approximations
Global approximations achieve the sparsity of the full kernel matrix Knn , which is crucial for scalability, through: 1) using a subset of the training data (SoD); 2) removing the entries of Knn with low correlations (sparse kernels); and 3) employing a low-rank representation (sparse approximations).

A. Subset of Data
SoD is the simplest strategy to approximate the full GP by using a subset Dsod of the training data D . Hence, the SoD retains the standard GP inference at a lower time complexity of O(m3) since it operates on Kmm , which only comprises m (m≪n ) data points. A recent theoretical work [45] analyzes the error bounds for the prediction and generalization of SoD through a graphon-based framework, indicating a better speed-accuracy tradeoff in comparison to other approximations reviewed below when n is sufficiently large. Though SoD produces reasonable prediction mean for the case with redundant data, it struggles to produce overconfident prediction variance due to the limited subset.

Regarding the selection of Dsod , one could randomly choose m points from D , use the clustering techniques, e.g., k -means and KD tree [46], to partition the data into m subsets and choose their centroids as subset points, and employ active learning criteria, e.g., differential entropy [47], information gain [48], and matching pursuit [49], to sequentially query data points with, however, higher computing cost.

B. Sparse Kernels
Sparse kernels [50] attempt to directly achieve a sparse representation K~nn of Knn via the particularly designed compactly supported (CS) kernel, which imposes k(xi,xj)=0 when |xi−xj| exceeds a certain threshold. Therefore, only the nonzero elements in K~nn are involved in the calculation. As a result, the GP using the CS kernel scales as O(αn3) with 0<α<1 . The main challenge in constructing valid CS kernels is to build a positive semidefinite (PSD) K~nn , i.e., vTK~nnv≥0 ∀v∈Rn [15], [50]–[51][52]. Besides, the GP using the CS kernel is potential for capturing local patterns due to the truncation property.

C. Sparse Approximations
Typically, the eigendecomposition helps choose the first m eigenvalues to approximate the full-rank kernel matrix as Knn≈UnmΛmmUTnm . The inversion then can be calculated via the Sherman–Morrison–Woodbury formula
(Kϵnn)−1≈σ−2ϵIn+σ−2ϵUnm(σ2ϵΛ−1mm+UTnmUnm)−1UTnm
View Sourceand the determinant using the Sylvester determinant theorem
∣∣Kϵnn∣∣≈|Λmm|∣∣σ2ϵΛ−1mm+UTnmUnm∣∣
View SourceRight-click on figure for MathML and additional features.resulting in the complexity of O(nm2) . However, the eigendecomposition is of limited interest since itself is an O(n3) operation. Hence, we approximate the eigenfunctions of Knn using m data points, leading to the Nyström approximation
Knn≈Qnn=KnmK−1mmKTnm
View Sourcewhich greatly improves the large-scale kernel learning [53] and enables the naive Nyström GP [54]. This scalable GP, however, may produce negative prediction variances [55] since it is not a complete generative probabilistic model as the Nyström approximation is only imposed on the training data and it cannot guarantee the PSD of kernel matrix.

Inspired by the influential Nyström approximation, sparse approximations build a generative probabilistic model, which achieves the sparsity via m inducing points (also referred to as support points, active set, or pseudopoints) to optimally summarize the dependence of the whole training data. We introduce a set of inducing pairs (Xm,fm) . The inducing variables fm akin to f follow the same GP prior p(fm)=N(0,Kmm) . Besides, fm is assumed to be a sufficient statistic for f , i.e., for any variables z , it holds p(z|f,fm)=p(z|fm) . We could recover the joint prior p(f,f∗) by marginalizing out fm as p(f,f∗)=∫p(f,f∗|fm)p(fm)dfm .

The sparse approximations have three main categories.

Prior Approximations: It approximates the prior but performs exact inference.

Posterior Approximations: It retains exact prior but performs approximate inference.

Structured Sparse Approximations: It exploits specific structures in the kernel matrix.

1) Prior Approximations:
Prior approximations [16] modify the joint prior, which is the origin of the cubic complexity, using the independence assumption f⊥f∗|fm such that
p(f,f∗)=∫p(f|fm)p(f∗|fm)p(fm)dfm(5)
View Sourcewhere the training and test conditionals write, given a Nyström notation Qab=KamK−1mmKmb
p(f|fm)=p(f∗|fm)=N(f|KnmK−1mmfm,Knn−Qnn)N(f∗|k∗mK−1mmfm,k∗∗−Q∗∗)(6a)(6b)
View Sourcewhere fm is called inducing variables since the dependencies between f and f∗ are induced by fm . To obtain computational gains, we modify the training and test conditionals as
q(f|fm)=q(f∗|fm)=N(f|KnmK−1mmfm,Q~nn)N(f∗|k∗mK−1mmfm,Q~∗∗).(7a)(7b)
View SourceRight-click on figure for MathML and additional features.Then, logp(y) is approximated by logq(y) as
logq(y)=−n2log2π−12log∣∣Q~nn+Qnn+σ2ϵIn∣∣−12yT(Q~nn+Qnn+σ2ϵIn)−1y.(8)
View SourceIt is found that specific selections of Q~nn enable calculating |Q~nn+Qnn+σ2ϵIn| and (Q~nn+Qnn+σ2ϵIn)−1 with a substantially reduced complexity of O(nm2) .

Particularly, the subset of regressors (SoR) [56], also called deterministic-inducing conditional (DIC), imposes deterministic training and test conditionals, i.e., Q~nn=0 and Q~∗∗=0 , as
qSoR(f|fm)=qSoR(f∗|fm)=N(f|KnmK−1mmfm,0)N(f∗|k∗mK−1mmfm,0).(9a)(9b)
View SourceRight-click on figure for MathML and additional features.This is equivalent to applying the Nyström approximation to both training and test data, resulting in a degenerate2 GP with a rank (at most) m kernel
kSoR(xi,xj)=k(xi,Xm)K−1mmk(Xm,xj).
View SourceRight-click on figure for MathML and additional features.

Alternatively, we could interpret the SoR from the weight-space view. It is known that the GP using a kernel with an infinite expansion of the input x in the feature space defined by dense basis functions {ϕc(x)}vc=1 is equivalent to a Bayesian linear model in (4) with infinite weights. Hence, the relevance vector machine (RVM) [57] uses only m basis functions ϕm(x)=[ϕ1(x),…,ϕm(x)]T for approximation
p(f|w)=N(f|Φnmw,Knn−ΦnmΣmmΦTnm)(10)
View Sourcewhere Φnm=[ϕm(x1),…,ϕm(xn)]T and p(w)=N(w|0,Σmm) . As a consequence, the RVM is a GP from the function-space view with the kernel
kRVM(xi,xj)=ϕT(xi)Σmmϕ(xj)
View SourceRight-click on figure for MathML and additional features.which recovers kSoR when Σmm=Im and ϕm(x)=LTkT(x,Xm) , where LLT=K−1mm [36].3 However, as depicted in Fig. 4, the SoR approximation and the RVM-type models [57]–[58][59] impose too restrictive assumptions to the training and test data such that they produce overconfident prediction variances when leaving the training data.4

Fig. 4. - Illustration of sparse approximations on a 1-D toy example with 
$y(x) = \mathrm {sinc}(x) + \epsilon $
, where 
$\epsilon \sim \mathcal {N}(0, 0.04)$
. + symbols represent 120 training points. Top circles represent the initial locations of inducing points, whereas the bottom triangles represent the optimized locations of inducing points. Green dotted curves represent the prediction mean of full GP and green curves represent 95% confidence interval of the full GP predictions. Red curves represent the prediction mean of sparse approximations. The shaded regions represent 95% confidence interval of the predictions of sparse approximations. For SKI, it does not optimize over the positions of inducing points. It is found that among the three prior approximations: 1) the SoR suffers from overconfident prediction variance when leaving the training data; 2) the FITC captures heteroscedasticity in variance; and 3) all of them are not guaranteed to converge to the full GP, indicated by the overlapped inducing points. Differently, the VFE and its stochastic variant SVGP approximate the full GP well due to the posterior approximation. Finally, though greatly reducing the time complexity by structured inducing set, the SKI may produce discontinuous predictions.
Fig. 4.
Illustration of sparse approximations on a 1-D toy example with y(x)=sinc(x)+ϵ , where ϵ∼N(0,0.04) . + symbols represent 120 training points. Top circles represent the initial locations of inducing points, whereas the bottom triangles represent the optimized locations of inducing points. Green dotted curves represent the prediction mean of full GP and green curves represent 95% confidence interval of the full GP predictions. Red curves represent the prediction mean of sparse approximations. The shaded regions represent 95% confidence interval of the predictions of sparse approximations. For SKI, it does not optimize over the positions of inducing points. It is found that among the three prior approximations: 1) the SoR suffers from overconfident prediction variance when leaving the training data; 2) the FITC captures heteroscedasticity in variance; and 3) all of them are not guaranteed to converge to the full GP, indicated by the overlapped inducing points. Differently, the VFE and its stochastic variant SVGP approximate the full GP well due to the posterior approximation. Finally, though greatly reducing the time complexity by structured inducing set, the SKI may produce discontinuous predictions.

Show All

To reverse the uncertainty behavior of SoR, the RVM is healed through augmenting the basis functions at x∗ with, however, higher computing cost [60]. This augmentation by including f∗ into fm was also studied in [16]. Alternatively, the sparse spectrum GP (SSGP) [61] and its variational variants [62]–[63][64] elegantly address this issue by reconstructing the Bayesian linear model from the spectral representation (Fourier features), resulting in the stationary kernel
k(xi,xj)=σ20mϕTm(xi)ϕm(xj)=σ20m∑r=1mcos(2πsTr(xi−xj))
View SourceRight-click on figure for MathML and additional features.where sr∈Rd represents the spectral frequencies.

Another way is imposing more informative assumption to Q~nn and Q~∗∗ . For instance, the deterministic training conditional (DTC) [65], [66] has this training conditional
qDTC(f|fm)=N(f|KnmK−1mmfm,0)(11)
View SourceRight-click on figure for MathML and additional features.but retains the exact test conditional. Hence, the prediction mean is the same as that of SoR, but the prediction variance is always larger than that of SoR and grows to the prior when leaving the inducing points (see Fig. 4). Notably, due to the inconsistent conditionals in (11), the DTC is not an exact GP. Besides, the DTC and SoR often perform not so well due to the restrictive prior assumption Q~nn=0 .

Alternatively, the fully independent training conditional (FITC) [67] removes the dependence among {fi}ni=1 such that given Vnn=Knn−Qnn , the training conditional qFITC(f|fm)
:=∏i=1np(fi|fm)=N(f|KnmK−1mmfm,diag[Vnn])(12)
View Sourcewhereas the test conditional retains exact. It is found that the variances of (12) are identical to that of p(f|fm) due to the correlation Q~nn=diag[Vnn] . Hence, compared to SoR and DTC that throw away the uncertainty in (9) and (11), FITC partially retains it, leading to a closer approximation to the prior p(f,f∗) . Moreover, the full independence assumption can be extended to q(f∗|fm) to derive the fully independent conditional (FIC) model5 [16], which stands as a nondegenerate GP with the kernel
kFIC(xi,xj)=kSoR(xi,xj)+δij[k(xi,xj)−kSoR(xi,xj)]
View SourceRight-click on figure for MathML and additional features.where δij is the Kronecker delta. Note that kFIC has a constant prior variance but is not stationary. Alternatively, the approximation (12) can be derived from minimizing the Kullback–Leibler (KL) divergence KL(p(f,fm)||q(fm)∏ni=1q(fi|fm)) [68], which quantifies the similarity between the exact and approximated joint prior.

To improve FIT(C), the partially independent training conditional (PITC) [16] has the training conditional qPITC(f|fm)
:=∏i=1Mp(fi|fm)=N(f|KnmK−1mmfm,blkdiag[Vnn]).(13)
View SourceRight-click on figure for MathML and additional features.This is equivalent to partitioning the training data D into M independent subsets (blocks) {Di}Mi=1 and taking into account the joint distribution of fi in each subset. However, it is argued that though being a closer approximation to p(f|fm) , the blocking qPITC(f|fm) brings little improvements over FITC [41]. This issue can be addressed by the extended partially independent conditional (PIC) [41] discussed in Section V-B.

Particularly, the FITC produces the prediction mean and variance at x∗ as μ(x∗)=k∗mΨKmnΛ−1y and σ2(x∗)=k∗∗−Q∗∗+k∗mΨkm∗ , where Ψ−1=Kmm+KmnΞ−1Knm and Ξ=diag[Vnn]+σ2ϵIn . It is found that the diagonal correlation diag[Vnn] represents the posterior variances of f given fm . Hence, these varying variances, which are zeros exactly at Xm , enable FITC to capture the noise heteroscedasticity (see Fig. 4) at the cost of producing an invalidate estimation (nearly zero) of the noise variance σ2ϵ and sacrificing the accuracy of prediction mean [69].

Finally, the heteroscedasticity of FITC raises another finding that this approximation attempts to achieve a desirable predictive accuracy at low computing cost rather than faithfully recovering the standard GP with increasing m . Indeed, the prior approximations recover the full GP when Xm=X . However, this configuration is not the global optimum when maximizing logq(y) , which makes them philosophically troubling. Besides, learning inducing points via the optimization of (8) may produce poor predictions [17]. These issues will be addressed by the posterior approximations reviewed in the following.

2) Posterior Approximations:
Different from the prior approximations, the posterior approximations [1], [17] retain exact prior but perform approximate inference. The most well-known method is the elegant variational free energy (VFE) proposed by Titsias [17] by using variational inference (VI) [70]. Instead of modifying the prior p(f,f∗) , VFE directly approximates the posterior p(f,fm|y) , the learning of which is a central task in statistical models, by introducing a variational distribution q(f,fm|y) . Then, we have their KL divergence KL(q(f,fm|y)||p(f,fm|y))
:=logp(y)−⟨logp(y,f,fm)q(f,fm|y)⟩q(f,fm|y)=logp(y)−Fq(14)
View SourceRight-click on figure for MathML and additional features.where ⟨.⟩q(.) represents the expectation over the distribution q(.) .6 It is found that minimizing the rigorously defined KL(q||p)≥0 is equivalent to maximizing Fq , since logp(y) is constant for q(f,fm|y) . Thus, Fq is called evidence lower bound (ELBO) or VFE, which permits us to jointly optimize the variational parameters and hyperparameters. It is observed that maximizing Fq with respect to the hyperparameters directly improves Fq , while maximizing Fq with respect to the variational parameters implicitly drives the approximation to match both the posterior p(f,fm|y) and the evidence p(y) .

To derive a tighter bound, the calculus of variations finds the optimal variational distribution q∗(fm|y) to remove the dependence of Fq on q(fm|y) by taking the relevant derivative to be zero, leading to the “collapsed” bound
FVFE=logqDTC(y)−12σ2ϵtr[Vnn]≥Fq.(15)
View SourceNote that FVFE differs with logqDTC only by a trace term, which, however, substantially improves the inference quality. In order to maximize FVFE , we should decrease the trace tr[Vnn]≥0 , which represents the total variance of predicting the latent variables f given fm . Particularly, tr[Vnn]=0 means fm=f and we recover the full GP. Hence, the trace term is a regularizer guards against over-fitting, seeks to deliver a good inducing set, and always improves Fq with increasing m (see the theoretical analysis in [69] and [71]). The third property implies that given enough resources, the VFE will recover the full GP (see Fig. 4). In contrast, without this trace term, the DTC often risks overfitting [72].

Regarding the improvements of VFE, it was extended to continuous and discrete inputs through an efficient QR factorization-based optimization over both inducing points and hyperparameters [73]. The estimation of inducing points has also been improved in an augmented feature space [74], which is similar to the interdomain strategy [39]. The authors argued that the similarity of inducing points measured in the Euclidean space is inconsistent to that measured by the kernel. Hence, they assigned a mixture prior on X in the latent feature space and derived a regularized bound for choosing good inducing points in the kernel space. Besides, Matthews et al. [71] and Matthews [74] bridged the gap between the variational inducing-point framework and the more general KL divergence between the stochastic processes. Using this new interpretation, Bui et al. [76] approximated the general, infinite joint prior p(fm,f≠fm,y)=p(fm,f≠fm|y)p(y) , which comprises two inferential objects of interest: posterior distribution and model evidence. Minimizing their KL divergence thus encourages direct approximation to both posterior and evidence. Hence, the FITC and VFE are interpreted jointly as
logqPEP(y)=logq(y)−1−α2αtr[log(In+ασ2ϵVnn)](16)
View Sourcewhere logq(y) takes the form (8) with Q~nn=αdiag[Vnn] . By varying α∈(0,1] , we recover FITC when α=1 and VFE when α→0 . Besides, a hybrid approximation using a moderate α , e.g., α=0.5 , often produces better predictions.

To further improve the scalability of VFE, Hensman et al. [1] retained the variational distribution q(fm|y)=N(fm|m,S) in Fq to obtain a relaxed bound
Fq=⟨logp(y|f)⟩p(f|fm)q(fm|y)−KL(q(fm|y)||p(fm)).(17)
View SourceRight-click on figure for MathML and additional features.The first term in the right-hand side of Fq is the sum of n terms due to the i.i.d. observation noises, i.e., p(y|f)=∏ni=1p(yi|fi) . Hence, the stochastic gradient descent (SGD) [77], which encourages large-scale learning, could be employed to obtain an unbiased estimation of Fq using a minibatch {Xb,yb} as
Fq≈n|yb|∑yi∈yb∫q(fm|y)p(fi|fm)logp(yi|fi)dfidfm−KL(q(fm|y)||p(fm)).(18)
View SourceRight-click on figure for MathML and additional features.Due to the difficulty of optimizing the variational parameters m and S in the Euclidean space, one can employ the stochastic VI (SVI) [78] using natural gradients,7 resulting in a remarkable complexity of O(m3) when |yb|=1 and, more interestingly, the online or anytime learning fashion.

Therefore, a crucial property of the stochastic variational GP (SVGP) is that it trains a sparse GP at any time with a small subset of the training data in each iteration [35]. Though showing high scalability and desirable approximation, the SVGP has some drawbacks: 1) the bound Fq is less tight than FVFE because q(fm|y) is not optimally eliminated; 2) it optimizes over q(fm|y) with a huge number of variational parameters, thus requiring much time to complete one epoch of training; and 3) the introduction of SVI brings the empirical requirement of carefully turning the parameters of SGD.

Inspired by the idea of Hensman, Peng et al. [36] derived the similar factorized variational bound for GPs by taking the weight-space augmentation in (10). The weight-space view allows using flexible basis functions to incorporate various low-rank structures and provides a composite nonconvex bound enabling the speedup using an asynchronous proximal gradient-based algorithm [80]. By deploying the variational model in a distributed machine learning platform PARAMETERSERVER [81], the authors have first scaled GP up to billions of data points. Similarly, Cheng and Boots [82] also derived a stochastic variational framework from the weight-space view with the difference being that the mean and variance of p(f|w) , respectively, use the decoupled basis function sets ϕa and ϕb , leading to more flexible inference. Besides, a recent interesting work [35] presents a novel unifying, anytime variational framework akin to Hensman’s variational framework for accommodating the existing sparse approximations, e.g., SoR, DTC, FIT(C), and PIT(C), such that they can be trained via the efficient SGD, which achieves asymptotic convergence to the predictive distribution of the chosen sparse model. The key is to conduct a reverse VI in which “reverse” means that we can find a prior p(fm)=N(fm|ν,Λ) (not the conventional GP prior) such that the variational distribution q∗(fm|y)=p(fm|y) for FI(T)C and PI(T)C is the maximum of ELBO.8 Finally, the scalability of SVGP can be further reduced to nearly O(m) by introducing Kronecker structures for inducing points and the variance of q(fm|y) [83], [84].

Titsias and Hensman’s models have been further improved by using: 1) the Bayesian treatment of hyperparameters [85]–[86][87] rather than traditional point estimation which risks overfitting when the number of hyperparameters is small and 2) the non-Gaussian likeli-hoods [86], [88], [89].

3) Structured Sparse Approximations:
A direct speedup to solve (Kϵnn)−1y in standard GP can be achieved through fast matrix-vector multiplication (MVM) [90], [91], which iteratively solves the linear system using conjugate gradients (CGs) with s (s≪n ) iterations,9 resulting in a time complexity of O(sn2) . It was argued in [14] that the original MVM has some open issues, e.g., the determination of s , the lack of meaningful speedups, and the badly conditioned kernel matrix. Alternatively, the preconditioned CG (PCG) [92] employs a preconditioning matrix through, for example, the Nyström approximation to improve the conditioning of kernel matrix and accelerate the CG convergence.

More interestingly, when the kernel matrix Knn itself has some algebraic structure, the MVM provides massive scalability. For example, the Kronecker methods [93], [94] exploit the multivariate grid inputs x∈Ω1×⋯×Ωd and the tensor product kernel with the form k(xi,xj)=∏dt=1k(xti,xtj) .10 Then, the kernel matrix decomposes to a Kronecker product Knn=K1⊗⋯⊗Kd , which eases the eigendecomposition with a greatly reduced time complexity of O(dn¯¯¯d+1) , where n¯¯¯=n−−√d for d>1 .11 Another one is the Toeplitz methods [95]—complementary to the Kronecker methods—that exploit the kernel matrix built from regularly spaced 1-D points, resulting in the time complexity of O(dn¯¯¯dlogn¯¯¯) . The severe limitation of the Kronecker and Toeplitz methods is that they require grid inputs, preventing them from being applied to the general arbitrary data points.12

To handle arbitrary data while retaining the efficient Kronecker structure, the structured kernel interpolation (SKI) [18] imposes the grid constraint on the inducing points. Hence, the matrix Kmm admits the Kronecker structure for d>1 and the Toeplitz structure for d=1 , whereas the cross kernel matrix Knm is approximated, for example by a local linear interpolation using the adjacent grid inducing points as
k(xi,uj)≈wik(ua,uj)+(1−wi)k(ub,uj)(19)
View Sourcewhere ua and ub are two inducing points most closely bound xi , and wi is the interpolation weight. Inserting the approximation (19) back into Qnn , we have
Qnn≈WnmK−1mmWTnm(20)
View SourceRight-click on figure for MathML and additional features.where the weight matrix W is extremely sparse since it only has two nonzero entires per row for local linear interpolation, leading to an impressive time complexity of O(n+dm¯¯¯¯¯d+1) with m¯¯¯¯¯=m−−√d for solving (Kϵnn)−1y . Also, the sparse W incurs the prediction mean with constant-time complexity O(1) and the prediction variance with complexity O(m) after precomputing. Furthermore, Pleiss et al. [97] derived a constant-time prediction variance using the Lanczos approximation, which admits s iterations of the MVM for calculation.

The original SKI has two main drawbacks. First, the number m of grid inducing points grows exponentially with dimensionality d , making it impractical for d>5 . To address this issue, one could use dimensionality reduction or manifold learning to map the inducing points into a p -dimensional (p≪d ) latent space [98], or more interestingly, one can use the hierarchical structure of NNs to extract the latent low-dimensional feature space [34], [99]. Furthermore, continual efforts [84], [100], [101] have been made to directly reduce the time complexity to be linear with d by exploiting the row-partitioned Khatri–Rao structure of Knm or imposing tensor train decomposition and Kronecker product to the mean and variance of q(fm|y) in Hensman’s variational framework. The linear complexity with d permits the use of numerous inducing points, e.g., m=10d .

Second, the SKI may produce discontinuous predictions due to the local weight interpolation and provide overconfident prediction variance when leaving the training data due to the restrictive SoR framework (see Fig. 4). To smooth the predictions, Evans and Nair [100] exploited the row-partitioned Khatri–Rao structure of Knm rather than using local weight interpolation. To have sensible uncertainty, a diagonal correlation akin to that of FITC has been considered [100], [102].

Finally, note that the usage of many inducing points is expected to improve the model capability. However, due to the grid constraint, the structured sparse approximations use fixed inducing points, resort to dimensionality reduction for tackling high-dimensional tasks, and place the vast majority of inducing points on the domain boundary with increasing d , which in turn may degenerate the model capability.

Choosing Inducing Points: So far, we have reviewed state-of-the-art sparse approximations. Regarding their implementations, the number and location of inducing points is crucial. As for the inducing size, theoretical analysis [103] indicates that the KL divergence between the variational approximation and the posterior can be arbitrarily small when m grows more slowly than n for regression with normally distributed inputs and the SE kernel. As for the location of inducing points, alternatively, we could use clustering techniques to select a finite set of space-filling inducing points from D or employ some querying criteria [49], [56], [66], [104], [105] to sequentially choose informative inducing points. More flexibly, inducing points are regarded as parameters to be optimized together with other hyperparameters [67], which additionally introduces m×d parameters and turns the inference into a high-dimensional optimization task. Besides, with increasing m , the benefits brought by the optimization over the simple selection from training data vanish. Interestingly, a recent work [106] shows the first attempt to simultaneously determine the number and locations of inducing points in the Bayesian framework by placing a prior on Xm .

SECTION IV.Local Approximations
Inspired by D&C, local approximations use localized experts to improve the scalability of GP. Besides, compared to global approximations, the localization enables capturing nonstationary features. In what follows, we comprehensively review the naive local experts that directly employs the pure local experts for prediction, and the mixture of experts (MoEs) and product of experts (PoEs) that inherit the advantages of naive local experts but boost the predictions through model averaging.

A. Naive Local Experts
It is known that a pair of points far away from each other is lowly correlated. Hence, the local experts trained on subsets of D is expected to produce sensible predictions with low time complexity. Particularly, the simple naive local experts (NLEs) [107], [108] let the local expert Mi be completely responsible for the subregion Ωi defined by Xi . Mathematically, we predict at x∗∈Ωi as p(y∗|D,x∗)≈pi(y∗|Di,x∗) .

According to the partition of D , we classify NLE into two main categories: 1) inductive NLE, which first partitions the input space and trains all the experts and then chooses an appropriate one for predicting at x∗ and 2) transductive NLE, which particularly chooses a neighborhood subset D∗ around x∗ and trains the relevant expert M∗ for predicting at x∗ .

Inductive NLE employs a static partition of D using clustering techniques, e.g., the Voronoi tessellations [107] and trees [109], [110], and trains independent local GP experts, resulting in O(nm20) , where m0=n/M is the training size for each expert. The partition and the experts are usually learned separately, or they can be learned jointly with Bayesian treatment [111]. In contrast, transductive NLE, e.g., the nearest neighbors (NeNe) [112] which could induce a valid stochastic process [108], [113], employs a dynamic partition to choose m0 neighbor points around x∗ , resulting in O(ntm30) complexity that relies on the test size nt . A key issue in transductive NLE is to choose the neighborhood set D∗ around x∗ . The simplest way is to use the geometric closeness criteria13 for selection, which, however, are not optimal without considering the spatial correlation [114]. Hence, some GP-based active learning methods have been employed to sequentially update the neighborhood set [20], [32], [115], [116].

While enjoying the capability of capturing nonstationary features due to the localized structure, the NLE produces the discontinuous predictions on the boundaries of subregions and suffers from poor generalization capability since it misses the long-term spatial correlations, as shown in Fig. 5. To address the discontinuity issue, the patched GPs [117], [118] impose continuity conditions such that two adjacent local GPs are patched to share the nearly identical predictions on the boundary. However, the patched GPs suffer from inconsistent and even negative prediction variance, and are only available in low dimensions [106], [118]. More popularly, the model averaging strategy, which is accomplished by the mixture/product of local GPs elaborated next, well smooths the predictions from multiple experts. To address the generalization issue, it is possible to 1) share the hyperparameters across experts, such as [26]; or 2) combine local approximations with global approximations, which will be reviewed in Section V-B.


Fig. 5.
Illustration of local approximations using six individual experts on the toy example. Note that for MoE, we did not jointly learn the experts and gating functions. For simplicity, we use the individual experts and the differential entropy as βi in the softmax gating function. It is found that the NLE suffers from discontinuity and poor generalization. The PoE produces poor prediction mean and overconfident prediction variance due to the inability of suppressing poor experts. To alleviate this issue, we could either use gating functions, such as the MoE, to provide desirable predictions; or use input-dependent weights, such as GPoE, to boost the predictions.

Show All

B. Mixture of Experts
The MoE devotes to combining the local and diverse experts owning individual parameters for improving the overall accuracy and reliability [21], [22].14 As shown in Fig. 6, MoE generally expresses the combination as a Gaussian mixture model (GMM) [120]
p(y|x)=∑i=1Mgi(x)pi(y|x)(21)
View SourceRight-click on figure for MathML and additional features.where gi(x) is the gating function, which usually takes a parametric form such as the softmax or probit function [120], [121], and can be thought as the probability p(z=i)=πi that the expert indicator z is i , i.e., x is assigned to expert Mi ; pi(y|x) comes from Mi and is responsible for component i of the mixture. The gates in (21) manage the mixture through probabilistic partition of the input space for defining the subregions where the individual experts are responsible for. The experts can be a variety of machine learning models, e.g., the linear model and the support vector machines [122], [123].

Fig. 6. - Illustration of MoE.
Fig. 6.
Illustration of MoE.

Show All

The training of MoE usually assumes that the data are i.i.d. such that we maximize the factorized log likelihood ∑nt=1logp(yt|xt) to learn the gating functions and the experts simultaneously by the gradient-based optimizers [124] and, more popularly, the EM algorithm [120], [122], [125], [126]. The joint learning permits probabilistic (soft) partition of the input space via both the data and the experts themselves, and diverse experts specified for different but overlapped subregions. Finally, the predictive distribution at x∗ is
p(y∗|D,x∗)=∑i=1Mgi(x∗|D)pi(y∗|D,x∗)(22)
View Sourcewhere gi(x∗|D) can be regarded as the posterior probability p(z∗=i|D) , called responsibility.

To advance the MoE, the single-layer model in Fig. 6 is extended to a hierarchical architecture [125]; the Bayesian approach is employed instead of the maximum likelihood to get rid of overfitting and noise-level underestimation [127]; the t -distribution is considered to handle the outliers [128]; and finally, instead of following the conditional mixture (21), the input distribution p(x) is considered to form the joint likelihood p(y,x) for better assignment of experts [122].

Regarding the mixture of GP experts for big data, it is observed that the original MoE is designed for multimodal (nonstationary) modeling, and the individual global experts are responsible for all the data points, leading to high complexity; the i.i.d. data assumption does not hold here since GP fits the data dependences through joint distribution; and the parametric gating function gi is not favored in the Bayesian nonparametric framework. In 2001, Tresp [129] first introduced the mixture of GP experts. It employs 3M GP experts to, respectively, capture the mean, the noise variance, and the gate parameters with nearly O(3Mn3) complexity, which is unattractive for big data. The mixture of GP experts for big data should address two issues: 1) how to reduce the computational complexity (model complexity) and 2) how to determine the number of experts (model selection).

To address the model complexity issue, there are three core threads. The first is the localization of experts. For instance, the infinite mixture of GP experts (iMGPE) [23] uses a localized likelihood to get rid of the i.i.d. assumption as
p(y|X)=∑z∈Zp(z|X)∏ip(yi|z,Xi).(23)
View SourceRight-click on figure for MathML and additional features.Given an instance of the expert indicators z=[z1,…,zn]T , the likelihood factorizes over local experts, resulting in O(nm20) when each expert has the same training size m0 . Similar to [122], the iMGPE model was further improved by employing the joint distribution p(y,X) as [130]
p(y,X)=∑z∈Zp(z)∏ip(yi|z,Xi)p(Xi|z).(24)
View SourceRight-click on figure for MathML and additional features.The fully generative model is capable of handling partially specified data and providing inverse functional mappings. However, the inference over (23) and (24) should resort to the expensive Markov chain Monte Carlo (MCMC) sampling. Alternatively, the localization can be achieved by the hard-cut EM algorithm using a truncation representation, in which the E-step assigns the data to experts through maximum a posteriori (MAP) of the expert indicators z or a threshold value [42], [131]–[132][133]. Thereafter, the M-step only operates on small subsets.

The second thread is to combine global experts with the sparse approximations reviewed in Section III-C under the variational EM framework. The dependence among inputs is broken to make VI feasible by: 1) interpreting GP as the finite Bayesian linear model in (10) [24], [134] or 2) using the FITC experts that factorize over f given the inducing set fm [42], [133]. With m inducing points for each expert, the complexity is O(nm2M) , which can be further reduced to O(nm2) with the hard-cut EM [42], [133].

Note that the first two threads assign the data dynamically according to the data property and the experts’ performance. Hence, they are denoted as a mixture of implicitly localized experts (MILE) [22]. The implicit partition determines the optimal allocation of experts, thus capturing the interaction among the experts. This advantage encourages the application on data association [135], [136]. The main drawback of MILE, however, is that in the competitive learning process, some experts may fail due to the zero-coefficient problem caused by unreasonable initial parameters [137].

To relieve the problem of MILE, the third thread is to prepartition the input space by clustering techniques and assign points to the experts before training [138], [139]. The mixture of explicitly localized experts (MELE) [22] reduces the model complexity as well and explicitly determines the architecture of MoE and poses distinct local experts. In the meantime, the drawback of MELE is that the clustering-based partition misses the information from data labels and experts such that it cannot capture the interaction among experts.

Finally, to address the model selection issue, the Akaike information criterion [140] and the synchronously balancing criterion [141] have been employed to choose over a set of candidate M values. More elegantly, the input-dependent Dirichlet process (DP) [23], the Polya urn distribution [130], or the more general Pitman–Yor process [142] is introduced over the expert indicators z to automatically infer the number of experts from data. The model inference, however, has to resort to a stick-breaking representation of DP [24], [133] due to the complex prior and the infinite M .

C. Product of Experts
Different from the MoE that employs a weighted sum of several probability distributions (experts) via an “OR” operation, the PoEs [25] multiplies these probability distributions, which sidesteps the weight assignment in MoE and is similar to an “AND” operation as
p(y|x)=1Z∏i=1Mpi(y|x)(25)
View Sourcewhere Z is a normalizer, which, however, makes the inference intractable when maximizing the likelihood ∑ni=1logp(yi|xi) [25].

Fortunately, the GP experts sidestep this issue since pi(y|x) in (25) is a Gaussian distribution. Hence, the product of multiple Gaussians is still a Gaussian distribution, resulting in a factorized marginal likelihood over experts
p(y|X)=∏i=1Mp(yi|Xi)(26)
View SourceRight-click on figure for MathML and additional features.where pi(yi|Xi)∼N(yi|0,Ki+σ2ϵ,iIni) with Ki=k(Xi,Xi)∈Rni×ni and ni being the training size of expert Mi . This factorization degenerates the full kernel matrix Knn into a diagonal block matrix diag[K1,⋯,KM] , leading to K−1nn≈diag[K−11,⋯,K−1M] . Hence, the complexity is substantially reduced to O(nm20) given ni=m0 .

It is found that the PoE likelihood (26) is a “point estimation” of the MoE likelihood (23); the MoE likelihood averages the PoE likelihood over possible configurations of the expert indicators z . Consequently, the joint learning of gating functions and experts makes MoE achieve the optimal allocation of experts [143]. Generally, due to the weighted sum form (21), the MoE will never be sharper than the sharpest expert; on the contrary, due to the product form (25), the PoE can be sharper than any of the experts. This can be confirmed in Fig. 5; the PoE produces poor prediction mean and overconfident prediction variance by aggregating the predictions from six independent experts, due to the inability of suppressing poor experts; on the contrary, the MoE provides the desirable predictions through gating functions.

Hence, in order to improve PoE, we retain the effective training process but modify the predicting process. Instead of following the simple product rule to aggregate the experts’ predictions, various aggregation criteria have been proposed to weaken the votes of poor experts.15 Particularly, the aggregations are expected to have several properties [143]: 1) the aggregated prediction is sensible in terms of probability and 2) the aggregated prediction is robust to weak experts.

Given the GP experts {Mi}Mi=1 with predictive distributions {p(y∗|Di,x∗)=N(μi(x∗),σ2i(x∗))}Mi=1 at x∗ , the PoEs [25], [143]–[144][145] aggregate the experts’ predictions through a modified product rule as
p(y∗|D,x∗)=∏i=1Mpβ∗ii(y∗|Di,x∗)(27)
View SourceRight-click on figure for MathML and additional features.where β∗i is a weight quantifying the contribution of pi(y∗|Di,x∗) at x∗ . Using (27), we can derive the aggregated prediction mean and variance with closed-form expressions. The original product-rule aggregation [25] employs a constant weight β∗i=1 , resulting in the aggregated precision σ−2(x∗)=∑Mi=1σ−2i(x∗) , which will explode rapidly with increasing M . To alleviate the overconfident uncertainty, the generalized PoE (GPoE) [143] introduces a varying weight β∗i , which is defined as the difference in the differential entropy between the expert’s prior and posterior, to increase or decrease the importance of experts based on their prediction uncertainty. However, with this flexible weight, the GPoE produces explosive prediction variance when leaving the training data [28]. To address this issue, we can impose a constraint ∑Mi=1β∗i=1 (see the favorable predictions in Fig. 5) or we employ a simple weight β∗i=1/M such that the GPoE recovers the GP prior when leaving X , at the cost of, however, producing underconfident prediction variance [26].16

Alternatively, the Bayesian committee machine (BCM) [26], [146]–[147][148] aggregates the experts’ predictions from another point of view by imposing a conditional independence assumption p(y|y∗)≈∏Mi=1p(yi|y∗) , which in turn explicitly introduces a common prior p(y∗|θ) for the experts.17 Thereafter, by using the Bayes rule, we have
p(y∗|D,x∗,θ)=∏Mi=1pβ∗ii(y∗|Di,x∗,θ)p∑Mi=1β∗i−1(y∗|θ).(28)
View SourceRight-click on figure for MathML and additional features.The prior correlation term helps BCM recover the GP prior when leaving X , and the varying β∗i akin to that of GPoE helps produce robust BCM (RBCM) predictions within X [26]. The BCM, however, will produce unreliable prediction mean when leaving X , which has been observed and analyzed in [26] and [28]. Notably, unlike PoE, the common prior in BCM requires that all the experts should share the hyperparameters. That is why we explicitly write (28) conditioned on θ .

It has been pointed out that the conventional PoEs and BCMs are inconsistent [28], [149], that is, their aggregated predictions cannot recover that of full GP when n→∞ . To raise consistent aggregation, the nested pointwise aggregation of experts (NPAE) [27] removes the independence assumption by assuming that yi has not yet been observed such that μi(x∗) is a random variable. The NPAE provides theoretically consistent predictions at the cost of requiring much higher time complexity due to the inversion of a new M×M kernel matrix at each test point. To be efficient while retaining consistent predictions, the generalized RBCM (GRBCM) [28] introduces a global communication expert Mc rather than the fixed GP prior to perform correction, i.e., acting as a base expert, and considers the covariance between global and local experts to support consistent predictions when n→∞ . It results in the aggregation
p(y∗|D,x∗)=∏Mi=2pβ∗i+i(y∗|D+i,x∗,)p∑Mi=2β∗i−1c(y∗|Dc,x∗)(29)
View Sourcewhere p+i(y∗|D+i,x∗) is the predictive distribution of the expert M+i trained on the augmented data set D+i={Di,Dc} .

Note that these transductive aggregations usually share the hyperparameters across experts [26], i.e., θi=θ , because 1) it achieves automatic regularization and eases the inference due to fewer hyperparameters; 2) it allows to temporarily ignore the noise term of GP in aggregation, i.e., using pi(f∗|Di,x∗) instead of pi(y∗|Di,x∗) such as [26], to relieve the inconsistency of typical aggregations; and finally 3) the BCMs cannot support the experts owning individual hyperparameters as discussed before. However, the shared hyperparameters limit the capability of capturing nonstationary features, which is the superiority of local approximations.18 Besides, another main drawback of aggregations is the Kolmogorov inconsistency [150] induced by the separation of training and predicting such that it is not a unifying probabilistic framework. That is, when we extend the predictive distributions at multiple test points, e.g., x∗ and x′∗ , we have p(y∗|D)≠∫p(y∗,y′∗|D)dy′∗ .

SECTION V.Improvements Over Scalable GPs
A. Scalability
The global approximations, especially the sparse approximations, have generally reduced the standard cubic complexity to O(nm2) through m inducing points. Moreover, their complexity can be further reduced through SVI [1] (with O(m3) ) and the exploitation of structured data [18] (with O(n+dlogm1+1/d) ). Sparse approximations, however, are still computationally impractical in the scenarios requiring real-time predictions, for example, environmental sensing and monitoring [151]. Alternatively, we can implement sparse approximations using advanced computing infrastructure, e.g., graphics processing units (GPUs) and distributed clusters/processors, to further speed up the computation.

Actually, the exact GP using GPU and distributed clusters has been investigated [152]–[153][154][155][156] in the regime of distributed learning [157]. It implements parallel and fast linear algebra algorithms, e.g., the HODLR algorithm [154] and the MVM algorithm, with modern distributed memory and multicore/multi-GPU hardware. For instance, Wang et al. [156] successfully trained an MVM-based exact GP over a million data points in 3 days through eight GPUs.

In the meantime, the GPU-accelerated sparse GPs have been explored. Since most of the terms in (15) can be factorized over data points, the inference can be parallelized and accelerated by GPU [29], [30]. Moreover, by further using the relaxed ELBO (17) or grid-inducing points in (20), the TensorFlow-based GPflow library [31] and the PyTorch-based GPyTorch library [158] have been developed to exploit the usage of GPU hardwares.

Besides, the parallel sparse GPs, e.g., the parallel PIT(C) and incomplete Cholesky factorization (ICF), have been developed using the message passing interface framework to distribute computations over multiple machines [159], [160]. Ideally, the parallelization can achieve a speedup factor close to the number of machines in comparison to the centralized counterparts. Recently, a unifying framework, which distributes conventional sparse GPs, including DTC, FI(T)C, PI(T)C, and low-rank-cum-Markov approximation (LMA) [161], have been built via varying correlated noise structure [43]. Impressively, Peng et al. [36] first implemented the sparse GPs in a distributed computing platform using up to one billion training points and trained the model successfully within 2 h.

The local approximations generally have the same complexity as the global approximations if m0=m . The local opinion naturally encourages the parallel/distributed implementations to further reduce computational complexity (see [20], [32], [33]).

B. Capability
Originated from the low-rank Nyström approximation, the global sparse approximations have been found to work well for approximating slow-varying features with high spatial correlations. This is because in this case, the spectral expansion of the kernel matrix is dominated by a few large eigenvectors. In contrast, when the latent function f has quick-varying (nonstationary) features, e.g., the complicated time-series tasks [162], the limited global inducing set is hard to exploit the local patterns. The D&C inspired local approximations, however, are capable of capturing local patterns but suffer from the inability of describing global patterns. Hence, in order to enhance the representational capability of scalable GPs, the hybrid approximations are a straightforward thread by combining global and local approximations in tandem.

Alternatively, the hybrid approximations can be accomplished through an additive process [41], [163], [164]. For instance, after partitioning the input space into subregions, the PIC [41] and its stochastic and distributed variants [35], [43], [87] extend PITC by retaining the conditional independence of training and test, i.e., f⊥f∗|fm , for all the subregions except for the one containing the test point x∗ . Thus, it enables the integration of local and global approximations in a transductive pattern. Mathematically, suppose that x∗∈Ωj , and we have
q(f,f∗|fm)=p(fj,f∗|fm)∏i≠jMp(fi|fm).(30)
View SourceRight-click on figure for MathML and additional features.This model corresponds to an exact GP with an additive kernel
kPIC(xi,xj)=kSoR(xi,xj)+ψij[k(xi,xj)−kSoR(xi,xj)]
View Sourcewhere ψij=1 when xi and xj belong to the same block; otherwise, ψij=0 . Note that PIC recovers FIC by taking all the subregion sizes to one; PIC becomes the purely local GPs by taking the inducing size to zero. The additive kernel similar to kPIC has also been employed in [163]–[164][165] by combining the CS kernel [15], [52] and the sparse approximation. Furthermore, as an extension of PIC, the tree-structured GP [166] ignores most of the intersubregion dependences of inducing points, but it concentrates on the dependence of the adjacent subregions lying on a chained tree structure. The almost purely localized model reduces the time complexity to be linear to n and allows using many inducing points.

Hybrid approximations can also be conducted through a coarse-to-fine process, resulting in a hierarchical structure with multiple layers yielding multiresolution [40], [167]–[168][169]. For example, Lee et al. [40] extended the work of [166] into a hierarchically partitioned GP approximation. This model has multiple layers, with the root layer being localized GPs. Particularly, each layer owns the individual kernel, the configuration of which is determined by the density of inducing points; the adjacent layers share a cross-covariance function that is convolved from two relevant kernels, such as [170]. Similarly, Park and Choi [167] presented a two-layer model, in which a GP is placed over the centroids of the subsets as g(c)∼GP(0,kg(c,c′)) to construct a rough global approximation in the top layer; then in each subregion of the root layer, a local GP is trained by using the global-level GP as the mean prior fi(x)∼GP(g(x),ki(x,x′)) . This model has also been improved into multilayer structure [168].

Inevitably, the combination of local approximations may induce discontinuous predictions and inaccurate uncertainties on the boundaries of subregions. For example, the tree-structured GPs [40], [166] completely adopt a localized predictive distribution, which suffers from severe discontinuity. The predictions could be smoothed by placing inducing points on the boundaries of subregions [40], which, however, is hard to implement. The PIC predictive distribution is composed of both global and local terms [41], which partially alleviate the discontinuity. To completely address the discontinuity, Nguyen and Bonilla [42] and Nguyen et al. [133] combined sparse approximations with model averaging strategies, e.g., MoE.

Finally, despite the hybrid approximations, the representational capability of sparse approximations can be enhanced through a more powerful probabilistic framework, for instance, the interdomain GP [39], which employs an idea similar to the convolution process in multioutput GP [10], [170] and high-dimensional GP [171]. Particularly, it uses a linear integral transform g(z)=∫w(x,z)f(x)dx to map the inducing points into another domain with possibly different dimensions.19 The inducing variables in the new domain can own a new kernel and induce richer dependences in the old domain. The interdomain idea has also been applied to the posterior approximations [71], [76], [172]. Besides, from the weight-space view in (10), it is encouraged to employ different configurations for the basis functions to capture slow- and quick-varying features using different scales [173], [174]. This kind of weight-space nonstationary GP indeed can be derived from the interdomain view (see [39]).

Alternatively, unlike the standard GP using a homoscedastic noise ϵ∼N(0,σ2ϵ) , the FITC has been extended by a varying noise as p(ϵ)=N(ϵ|0,diag[h]) where h=[σ2ϵ(x1),⋯,σ2ϵ(xn)]T [175]. Moreover, Hoang et al. [43] employed a B -th order Markov property on the correlated noise process p(ϵ)=N(ϵ|0,Kϵ) in a distributed variational framework. The unifying framework accommodates the existing sparse approximations, e.g., DTC and PIC, by varying the Markov order and noise structure. Yu et al. [87] further extended this article through Bayesian treatment of hyperparameters to guard against overfitting. More elegantly, Almosallam et al. [176] derived a scalable heteroscedastic Bayesian model from the weight-space view by adopting an additional log GP, which is analogous to [177] and [178], to account for noise variance as σ2ϵ(xi)=exp(ϕ(xi)w+b) . Differently, Liu et al. [179] derived the stochastic and distributed variants of [177] for scalable heteroscedastic regression. The distributed variant using experts with hybrid parameters improves both scalability and capability, while the stochastic variant using global inducing set may sacrifice the prediction mean for describing the heteroscedastic noise. Finally, the heteroscedasticity can also be encoded though augmenting the inputs with latent variables [180], [181]. This has also been exploited in the regime of density estimation [182].

SECTION VI.Extensions and Open Issues
A. Scalable Manifold GP
In scalable GP literature, we usually focus on the scenario in which the training size n is large (e.g., n≥104 ), whereas the number d of inputs is modest (e.g., up to hundreds of dimensions). However, in practice, we may need to handle the task with comparable n and d or even d≫n , leading to the demand of high-dimensional scalable GP. In practice, we often impose low-dimensional constraints to restrict the high-dimensional problems, i.e., the inputs often lie in a p -dimensional (p<d ) manifold embedded in the original d -dimensional space. This is because high-dimensional statistical inference is solvable only when the input size d is compatible with the statistical power based on the training size n [183].

Hence, various manifold GPs [11], [175], which are expressed as
y=f(Υx)+ϵ(31)
View SourceRight-click on figure for MathML and additional features.where Υ∈Rp×d is a mapping matrix, have been developed to tackle high-dimensional big data through linear/nonlinear dimensionality reduction [175], [184], [185] or NN-like input transformation [186]. As a result, the kernel operates the data in a lower dimensional space as k(Υx,Υx′) . Note that the mapping matrix and the scalable regression are learned jointly in the Bayesian framework for producing favorable results. Particularly, the true dimensionality of the manifold can be estimated using the Bayesian mixture models [187], which, however, induce a heavy computational budget.

A recent exciting theoretical finding [183] turns out that the learning of the intrinsic manifold can be bypassed since the GP learned in the original high-dimensional space can achieve the optimal rate when f is not highly smooth. This motivates the usage of Bayesian model averaging based on random compression over various configurations in order to reduce computational demands [188].

Continual theoretical and empirical efforts are required for designing specific components, e.g., the convolutional kernel [171], for scalable manifold GPs, because of the urgent demands in various fields, e.g., computer vision (CV).

B. Scalable Deep GP
Motivated by the enormous success of deep learning in various fields, the scalable deep GPs (DGPs) [34], [189] have been investigated in recent years.20 A simple representative is to combine the structural NNs and the flexible nonparametric GP together, in which NNs map the original input space to the feature space for extracting nonstationary/recurrent features, and the last-layer sparse GP conducts standard regression over the latent space [34], [99], [186], [192]. The parameters of NNs and GP are jointly learned by maximizing the marginal likelihood in order to guard against overfitting. The NNs+GP structure produces sensible uncertainties and is found to be robust to adversarial examples in CV tasks [193]. Differently, Cremanns and Roos [194] employed the NNs to learn the input-dependent hyperparameters for the additive kernels. Then, the NeNe algorithm is employed to ease the GP inference. Besides, Iwata and Ghahramani [195] used the outputs of NNs as prior mean for SVGP.

More elegantly, inspired by deep learning, the DGP [189] and its variants [196]–[197][198][199][200], which employ the hierarchical and functional composite
y(x)=fl(fl−1(⋯f1(x)))+ϵ(32)
View SourceRight-click on figure for MathML and additional features.to stack multiple layers of latent variable model (LVM)21 [11], [201] for extracting features. The DGP showcases great flexibility in (un)supervised scenarios, resulting in, however, a nonstandard GP. The recently developed convolutional kernel [171] opens up the way of DGP for CV tasks [202]. Interestingly, the finite layer-to-layer transformation of inputs in (32) can be extended to infinitely deep, but infinitesimal, differential fields governed by stochastic differential equations [203].

Note that the inference in DGP is intractable and expensive. Thus, efficient training requires a sophisticated approximate inference via inducing points [198], [204], which in turn may limit the capability. Easier inference without loss of prediction accuracy has always been a big challenge for DGP to completely show its potential beyond regression.

C. Scalable Multitask GP
Due to the multitask problems that have arose in various fields, e.g., environmental sensor networks and structure design, multitask GP (MTGP) [9], [10], also known as multioutput GP, seeks to learn the latent T correlated tasks f=[f1,…,fT]T:Rd↦RT simultaneously as
f(x)∼GP(0,kMTGP(x,x′)),y(x)=f(x)+ϵ(33)
View SourceRight-click on figure for MathML and additional features.where ϵ=[ϵ1,…,ϵT]T is the individual noises. The crucial in MTGP is the construction of a valid multitask kernel kMTGP(x,x′)∈RT×T , which can be built through, for example, the linear model of coregionalization [205] and the convolution process [170]. Compared to individual modeling of tasks which loses valuable information, the joint learning of tasks enables boosting predictions by exploiting the task correlations and leveraging information across tasks.

Given that each of the T tasks has n training points, MTGP collects the data from all the tasks and fuses them in an entire kernel matrix, leading to a much higher complexity O(T3n3) . Hence, since the inference in most MTGPs follows the standard process, the above-reviewed sparse approximations and local approximations have been applied to MTGPs [170], [206]–[207][208] to improve the scalability.

To date, scalable MTGPs are mainly studied in the scenario where the tasks have well-defined labels and share the input space with modest dimensions. Many efforts are required for extending current MTGPs to handle the 4V challenges in the regime of multitask (multioutput) learning [209].

D. Scalable Online GP
Typically, it is assumed that the entire data D are available a priori to conduct the off-line training. We, however, should consider the scenario where the data arrives sequentially, i.e., online or streaming data, in small unknown batches. For the complicated online regression, the model [65] should have real-time adaptation to the streaming data and handle large-scale case since the new data are continuously arriving.

Sparse GPs are extensible for online learning since they employ a small inducing set to summarize the whole training data [210]–[211][212]. As a result, the arrived new data only interact with the inducing points to enhance fast online learning. This is reasonable since the updates of μ(x∗) and σ2(x∗) of FITC and PITC only rely on the inducing set and new data [213], [214]. Moreover, the stochastic variants naturally showcase the online structure [215], since the bound in (17) supports minibatch learning by stochastic optimization.

However, there are two issues for scalable online GPs. First, some of them [211], [213], [214] fix the hyperparameters to obtain constant complexity per update. It is empirically argued that the optimization improves the model significantly in the first few iterations [215]. Hence, with the advanced computing power and the demand for accurate predictions, it could update the hyperparameters online over a small number of iterations.

Second, the scalable online GPs implicitly assume that the new data and the old data are drawn from the same input distribution. This, however, is not the case in tasks with complex trajectories, e.g., an evolving time-series process [216]. To address the evolving online learning, Nguyen et al. [217] presented a simple and intuitive idea using local approximations. This method maintains multiple local GPs and either uses the new data to update the specific GP when they fall into the relevant local region or uses the new data to train a new local GP when they are far away from the old data, resulting in, however, information loss from available training data. As the extension of [65] and [218], Bui et al. [216] deployed an elegant probabilistic framework to update the posterior distributions and hyperparameters in an online fashion, where the interaction happens between the old and new inducing points. The primary theoretical bounds for this Bayesian online model were also provided in [219].

E. Scalable Recurrent GP
There exist various tasks, e.g., speech recognition, system identification, energy forecasting, and robotics, in which the data sets are sequential and the ordering matters. Here, we focus on recurrent GP [220], [221] to handle sequential data.

The popular recurrent GP is GP-based nonlinear autoregressive models with exogenous inputs (GP-NARX) [220]. It is generally expressed as
xt=[yt−1,…,yt−Ly,ut−1,…,ut−Lu],yt=f(xt)+ϵyt(34)
View SourceRight-click on figure for MathML and additional features.where ut is the external input, yt is the output observation at time t , Ly and Lu are the lagged parameters that, respectively, indicate the numbers of delayed outputs and inputs to form the regression vector xt , f is the emission function, and ϵyt accounts for the observation noise. Note that the observations yt−1,…,yt−Ly here are considered to be deterministic. The transformed input xt comprising the previous observations and external inputs enables using standard scalable GPs, e.g., sparse approximations, to train the GP-NARX. Due to the simplicity and applicability, GP-NARX has been well studied and extended to achieve robust predictions against outliers [222], local modeling by incorporating prior information [223], and higher order frequency response functions [224]. A main drawback of GP-NARX, however, is that it cannot account for the observation noise in xt , leading to the errors-in-variables problem. To address this issue, we could conduct data preprocessing to remove the noise from data [225], adopt GPs considering input noise [204], and employ the more powerful state-space models (SSMs) [221] introduced next.

The GP-SSM employs a more general recurrent structure as
xt=g(xt−1,ut−1)+ϵxt,yt=f(xt)+ϵyt(35)
View SourceRight-click on figure for MathML and additional features.where xt is the state of the system that acts as an internal memory, g is the transition function, f is the emission function, ϵxt is the transition noise, and finally, ϵyt is the emission noise. Note that GP-NARX is a simplified GP-SSM model with observable state. The GP-SSM considers the transition noise and brings the flexibility in requiring no lagged parameters. However, this model suffers from intractable inference since we need to marginalize out all the latent variables, thus requiring approximate inference [221], [226], [227].

Finally, we again see the trend in combining recurrent GPs with NNs. For instance, the deep recurrent GP [228] attempts to mimic the well-known recurrent NNs (RNNs), with each layer modeled by a GP. Similar to DGP [189], the inference in deep recurrent GP is intractable and requires sophisticated approximations, such as [229]. Hence, to keep the model as simple as possible while retaining the recurrent capability, the long short-term memory (LSTM) model [230] is directly combined with scalable GPs, resulting in analytical inference and desirable results [192].

F. Scalable GP Classification
Different from the regression tasks mainly reviewed by this article with continuous real observations, the classification has discrete class labels. To this end, the binary GP classification (GPC) model [231] with y∈{0,1} is usually formulated as
f∼GP(0,k),p(y|f)=Bernoulli(π(f))(36)
View SourceRight-click on figure for MathML and additional features.where π(.)∈[0,1] is an inverse link function22 that squashes f into the class probability space. Differently, the multiclass GPC (MGPC) [232] with y∈{1,⋯,C} is
fc∼GP(0,kc),p(y|f)=Categorical(π(f))(37)
View Sourcewhere {fc}Cc=1 are independent latent functions23 for C classes, and f=[f1,⋯,fC]T:Rd↦RC .24 Due to the non-Gaussian likelihood, exact inference for GPC, however, is intractable, thus requiring approximate inference, the key of which approximates the non-Gaussian posterior p(f|y)∝p(y|f)p(f) with a Gaussian q(f|y) [231].

Motivated by the success of scalable GPR, we could directly treat GPC as a regression task [235] or solve it as GPR by a transformation that interprets class labels as the outputs of a Dirichlet distribution [236]. This sidesteps the non-Gaussian likelihood. A more principled way, however, is adopting GPCs in (36) and (37), and combining approximate inference, e.g., Laplace approximation, EP and VI, with the sparse strategies in Section III-C to derive scalable GPCs [86], [237]–[238][239][240][241].25

The main challenges of scalable GPC, especially MGPC, are: 1) the intractable inference and posterior and 2) the high training complexity for large C . For the first issue, the stochastic GPC derives the model evidence expressed as the integration over a 1-D Gaussian distribution, which can be adequately calculated using the Gaussian–Hermite quadrature [75], [238]. Furthermore, the GPC equipped with the FITC assumption owns a completely analytical model evidence [239], [240]. Particularly, when taking the logit/softmax inverse link function, the Pòlya–Gamma data augmentation [242] offers analytical inference and posterior for GPC [241], [243]. A recent work [244] presents a noisy framework to derive analytical ELBOs for scalable binary/MGPCs using conventional likelihoods. For the second issue, since the complexity of MGPC is linear to the number C of classes, alternatively, we may formulate the model evidence as a sum over classes such as [234], thus allowing efficient stochastic training.

SECTION VII.Conclusion
Although the GP itself has a long history, the nonparametric flexibility and the high interpretability make it popular yet posing many challenges in the era of big data. In this article, we have attempted to summarize the state of scalable GPs in order to well understand the state of the art and attain insights into new problems and discoveries. The extensive review seeks to uncover the applicability of scalable GPs to real-world large-scale tasks, which in turn present new challenges, models, and theory in the GP community.

Appendix Libraries and Data Sets
Table I summarizes the primary libraries that implement representative scale GPs and are well known for both academia and industry.26 It is observed that Python and hardware acceleration are becoming popular for the GP community. Note that some specific scalable GP packages implementing the advanced models reviewed in Sections V and VI are not listed here. They can be found in the relevant researchers’ webpage.

TABLE I List of Primary Libraries Supporting Representative Scalable GPs
Table I- 
List of Primary Libraries Supporting Representative Scalable GPs
Besides, except the well-known UCI27 regression data sets with n∈[15,4.18×106] and d∈[3,4.8×105] , and the LIBSVM28 regression data sets with n∈[152,2.06×104] and d∈[6,4.27×106] , Table II summarizes the regression data sets (n≥104 ) occurred in the scalable GP literature. It is found that researchers have assessed scalable GPs with up to about one billion training points [36].

TABLE II Big Regression Data Sets ( n≥104 ) in the Literature
Table II- 
Big Regression Data Sets (
$n \ge10^{4}$
) in the Literature