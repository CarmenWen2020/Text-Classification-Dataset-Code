Recent evidence in mental health assessment have demonstrated that facial appearance could be highly indicative of depressive disorder. While previous methods based on the facial analysis promise to advance clinical diagnosis of depressive disorder in a more efficient and objective manner, challenges in visual representation of complex depression pattern prevent widespread practice of automated depression diagnosis. In this paper, we present a deep regression network termed DepressNet to learn a depression representation with visual explanation. Specifically, a deep convolutional neural network equipped with a global average pooling layer is first trained with facial depression data, which allows for identifying salient regions of input image in terms of its severity score based on the generated depression activation map (DAM). We then propose a multi-region DepressNet, with which multiple local deep regression models for different face regions are jointly leaned and their responses are fused to improve the overall recognition performance. We evaluate our method on two benchmark datasets, and the results show that our method significantly boosts state-of-the-art performance of the visual-based depression recognition. Most importantly, the DAM induced by our learned deep model may help reveal the visual depression pattern on faces and understand the insights of automated depression diagnosis.

SECTION 1Introduction
As a mental disorder, major depressive disorder (MDD) (also simply known as depression) can negatively affect a person's mental health. Typically, people having a major depressive episode often exhibit low mood presented at various situations [1]. Nowadays, more than 250 million people (about 3.6 percent of the world's population in 2013) are affected by MDD [2]. While proper antidepressant medication and psychological counseling can be helpful for treatment of MDD, diagnosis of MDD may be delayed or missed when the signs and symptoms are interpreted as normal moodiness. This issue is even more marked in developing countries.

In practice, clinical diagnosis of MDD is usually conducted by experienced experts such as psychiatrist and psychologist, which perform comprehensive assessment of patient's current symptoms, life circumstances, biographical history, and etc. This diagnosis procedure, however, is usually labor intensive and highly depends on the specialist's subjective observation. With the increasing amount of people suffering from depression around the world, approaches to automated depression diagnosis (ADD) are highly desired to facilitate its objective assessment and efficient diagnosis. Recently, ADD based on machine learning techniques has been paid increasing attention [3], [4]. Effectiveness of the verbal cues for ADD has ever been verified in various depression detection/recognition methods [5], [6], [7], [8], [9], [10]. Besides the verbal cues, nonverbal cues like facial expression could also be highly indicative in prediction of the depression severity [11], [12], [13], [14]. In this paper, we focus on the visual-based ADD, which aims to predict the depression severity for a given subject in videos based on the Beck Depression Inventory-II (BDI-II [15]). To illustrate the challenges of the ADD from facial images, Fig. 1 depicts some cases of visual-based nonverbal behavior [3], which are collected in wild conditions using a webcam and the subjects were free to move during the record. The cases presented in Fig. 1 may not be easy to diagnose in terms of severity level for a person without a professional medical background and even for a junior medical specialist.


Fig. 1.
Example cases of facial images with different depression levels (BDI-II scores of 0, 15, 19, 32 and 34 from left to right) from AVEC 2013 [3].

Show All

To achieve depression recognition from facial images, conventional solutions to a visual-based ADD system typically consist of two consecutive steps: 1) feature extraction, and 2) classification (or regression). The task of feature extraction involves designing an effective depression representation robust to irrelevant factors, such as age, gender, pose, illumination and so on. Based on the extracted features, regression (or classification) methods [16], [17] are utilized to predict the depression severity for a given subject in video. In practice, these two steps are addressed independently and then integrated together for the overall performance fine-tuning. In previous works, more efforts on feature extraction have been made for visual representation of depression characteristic [7], [8], [11], [12], [13], [14]. In general, the feature representation for visual-based ADD can be categorized into hand-crafted [3], [4], [6], [8], [18], [19], [20], [21] and learning-based [12], [14]. For the hand-crafted feature descriptors [22], [23], [24], [25], the extraction of effective depression features often involves a set of complicated image processing steps, and thus it depends heavily on the expertise knowledge. Most importantly, these low-level feature descriptors are not discriminative enough and hard to capture the high-level semantic structures encoded in depression faces. In contrast to hand-crafted descriptors, learning-based methods exploit some prior knowledge such as the sparsity [26], [27], compactness [28] and nonlinear hierarchy [29] for data-driven representation learning, leading to more accurate and robust recognition performance of video-based ADD [12], [14].

While very few attempts were made so far to develop deep learning techniques for ADD, deep convolutional neural network (DCNN) based deep learning methods could potentially change the computing paradigm of the conventional visual-based ADD framework in two aspects. On one hand, deep feature learning can reveal and capture high-level semantics features from the visual training data, and hence the learned deep facial features can be more discriminative for prediction of depression severity than conventional hand-crafted features [14]. On the other hand, the two steps of feature extraction and classification (regression) are jointly formulated in a deep architecture. With such a unified framework, the recognition performance of an ADD system could be tuned more easily in a systematic manner, and hence the great effort of feature engineering can be significantly alleviated.

In this paper we present a deep regression network termed DepressNet to learn a depression representation with visual explanation that facilitates automated prediction of depression severity from facial images. Specifically, a DCNN equipped with a global average pooling (GAP) layer is introduced to learn a deep regression model on the visual depression data, which not only facilitates accurate prediction of the depression severity, but also enables the ADD system to localize the salient regions of facial image in terms of its severity level based on the generated depression activation map (DAM). We also propose a multi-region DepressNet (MR-DepressNet), with which multiple local deep regression models for different face regions are jointly leaned and their responses are combined to improve the overall recognition performance. We evaluate our proposed method on two benchmark depression datasets against several state-of-the-art alternative methods, and the results demonstrate the superior prediction performance of our method. Most importantly, the DAM induced by our learned deep model may help reveal the visual depression pattern on human faces, and understand the insights of visual-based depression recognition (i.e., why the learned deep model works), which has not yet been addressed in previous visual-based depression recognition.

The remainder of this paper is organized as follows. We first briefly review the related work in Section 2, and Section 3 details the proposed depression recognition method with the DepressNet (and MR-DepressNet) architecture. Experimental settings, results and discussions are presented in Section 4, and Section 5 concludes the paper.

SECTION 2Related Work
In recent years, increasing attention has been paid to depression recognition from behavioral signals, including the speech and visual cues of human communication. A significant amount of work in the literature is dedicated to this research topic. Here, we focus on the visual-based depression recognition. The Audio-Visual Emotion Challenge and Workshop (AVEC 2013 and 2014) fundamentally promotes the research on effective computing and machine learning methods for automated depression recognition. In the AVEC 2013 competition on depression recognition [3], the local phase quantization (LPQ) [22] was employed as a baseline facial descriptor, by which the dense features extracted from facial regions at each video frame were fed into the support vector regression (SVR) for learning and prediction. In [6], Cummins et al. proposed to use two descriptors, i.e., the space-time interest points (STIPs) [23] and pyramid of histogram of gradients (PHOG) [24], to extract behavioural signals from videos for depression recognition. Meng et al. [8] adopted motion history histogram (MHH) [25] based feature to encode motion cue from videos, and the partial least squares (PLS) [30] was employed for regressor learning. In [12], the temporal cue is extracted by LPQ-TOP descriptor from sub-volumes of the detected facial region, on which a sparse coding scheme is introduced for learning of the behavior pattern dictionary.

In the AVEC 2014 competition on depression recognition [4], the local dynamics descriptor LGBP-TOP [18] was adopted as the baseline feature for video description, and the SVR was again adopted as the prediction model. In [19], several motion related local features were extracted from sub-volumes of the detected facial region, which were then fed into a SVR for learning and prediction. Jan et al. [20] introduced an 1D MHH computed on the feature vector sequence of some local descriptors (e.g., LBP and LPQ), and the PLS regressor is employed for learning and prediction. In [21], the baseline features LGBP-TOP were combined with LPQ for video representation, and they obtained an ensemble of regional linear regressors via Moore-Penrose generalized inverse (MPGI) for depression prediction.

The depression recognition methods discussed above are typically based on the hand-crafted image descriptors, which rely heavily on expert knowledge and are difficult to capture high-level semantic structures. Most recently, Zhu et al. [14] proposed to use a two-stream DCNN architecture [31] with two full-connected (FC) layers for joint learning of the depression features of facial appearance and dynamics in videos, and they reported promising performance in their experiments when compared to previous visual-based solutions. Jan et al. [32] proposed an depression recognition scheme based on visual and vocal expressions. In their solution, the feature dynamic history histogram (FDHH) and audio features, including spectral low-level descriptors (LLD) and Mel-frequency cepstral coefficients (MFCCs), are fused through regression to predict the BDI-II scales. It is well-known that, most existing CNN-based image recognition systems are built up based on a certain standard deep architectures like AlexNet [29], VGG [33], GoogLeNet [31] and ResNet [34], [35], and the experimental results reported in the literature have demonstrated the superiority of this deep learning paradigm. The recent top performing solutions for visual- or audio-based ADD systems [10], [14], [32] further witnessed that CNN-based deep models play a critical role in depression recognition. Nevertheless, these CNN-based depression recognition approaches are generally hard for clinical practitioners to understand the insight of depression feature learning and, most importantly, they cannot explain which region of the facial image is salient and discriminative for prediction of its severity level.

A increasing number of techniques and applications have been developed to understand the visually interpretable representations of CNN. The representation ability of a CNN can often be visualized by using backpropagation-like algorithms to identify salient image regions [36], [37], [38]. In this work, we introduce the concept of DAM and show how to generate the DAM of an input facial image to highlight the salient regions in terms of its depression score. Our idea of generating the DAM for ADD with visual explanation is mainly inspired by the work [39], [40], in which class activation map is introduced to characterize discriminative regions of the input image in terms of its output category. Our DAM differs from the class activation map in that DAM is generated in the context of regression rather than classification, and it is focused on the ADD problem.

SECTION 3Our Approach
3.1 DepressNet
Recently, CNN-based deep feature learning has achieved great success in computer vision [29], [31], [33], [34]. It can model high-level semantic structures in data related to specific vision task. Typically, CNN is built up for deep representation learning based on a multiple layers neural network, where the data can be represented by a nonlinear features from low-level to high-level in a hierarchical manner. In this deep architecture, multiple layers of data processing such as convolution and pooling are stacked to reveal the salient image signals in different scales. Most importantly, in CNN, feature extraction and prediction can be jointly formulated and optimized in a unified framework, leading to more discriminative feature representation and more robust prediction performance compared to conventional learning paradigm. In this paper, we propose a deep CNN architecture termed DepressNet, and derive a deep regression learning algorithm for visual depression feature learning and prediction of the depression severity level given a set of face images from a single subject in video. Fig. 2 illustrates the pipeline of our DepressNet-based ADD scheme. For a video input to the DepressNet, face detection and cropping are first performed to obtained the three-dimensional face subvolumes. Then, these facial images are fed into the DepressNet for deep depression feature learning. Finally, the DepressNet outputs the depression score, as well as the salient regions in terms of its severity score.


Fig. 2.
The pipeline of our proposed visual-based ADD method with DepressNet.

Show All

In the line of research on face recognition and analysis, CNN has proved to be very effective for face representation learning given a huge number of face training samples [33], [41], [42], [43]. However, compared to some large scale image datasets (e.g., ImageNet [44]) for visual recognition [29], the size of the depression datasets [3], [4] for visual-based depression is quite limited due to the privacy concerns and involved time and human costs in data collection. Lack of labeled training data makes the model learning with deep CNN prone to overfitting in practice. To tackle this issue, a deep residual network ResNet50 [35] is first pre-trained for face recognition with a large scale face dataset (e.g., CASIA-WebFace [45]), which contains total of 453,453 images from 10,575 subjects after face detection.

As this pre-trained deep model is trained for representation of face images with different identities, it cannot be directly exploited for the task of depression analysis. Based on the pre-trained ResNet, DepressNet is built up by replacing the softmax layer with a regression output layer which is directly connected to the global average pooling layer, as shown in Fig. 3. Specifically, DepressNet has four blocks, and they contain 3, 4, 6 and 3 bottleneck structures, respectively. Bottleneck structure is illustrated in Fig. 4. Then, the DepressNet is fine-tuned for the task of depression recognition with the depression datasets. Since depression recognition (estimation) can be regarded as a regression problem, the square loss (other than the softmax loss used for face recognition in the pre-training step) is adopted as the loss function for DepressNet. Mathematically, the square loss for our depression recognition task can be formally written by:
L=12N∑i=1N(f(xi)−ℓi)2,(1)
View SourceRight-click on figure for MathML and additional features.where N is the batch size (i.e., total number of face samples in a minibatch), f(xi) and ℓi are the prediction output of the DepressNet and the ground truth depression score for the ith input face sample xi, respectively.

Fig. 3. - 
Network architecture of the proposed DepressNet. There are $K=2048$K=2048 feature maps (size = $7 \times 7$7×7) at the last convolutional layer, followed by a global average pooling (GAP) layer connecting to the regression ouput.
Fig. 3.
Network architecture of the proposed DepressNet. There are K=2048 feature maps (size = 7×7) at the last convolutional layer, followed by a global average pooling (GAP) layer connecting to the regression ouput.

Show All


Fig. 4.
Bottleneck structure of a block in DepressNet.

Show All

As shown in Fig. 3, the well-designed GAP structure of DepressNet allows for localization of the salient regions of input facial image in terms of its depression severity level. This prominent property of the proposed deep regression model is highly desirable in practice for depression recognition, as the focuses of clinical practitioners are not only on the high prediction performance of the ADD system, but also on the visual explanation for understanding the insight of the visual-based ADD and why the learned deep model works. In what follows we will elaborate how to generate the depression activation map that highlights the salient regions of the input facial image.

3.2 Visual Explanation for Learned Depression Feature
Hierarchical computer vision pipeline has long been followed for visual analysis of simple to complex features, especially for CNN-based deep architectures. This is mainly motivated by the discovery of the receptive field [46], through which the visual system works in a hierarchical and feed-forward manner. Conceptually, convolution operators at each convolutional layers of CNN can be viewed as visual detectors discovering low-level features (e.g., edges) to high-level semantic structures (e.g., full face). For the task of depression recognition, however, the use of FC layers in ResNet would make it hard to understand and identify the contributions of different convolutional units in predicting depression score. By using the GAP layer (rather than FC layers) followed by a linear connection to the regression output, our DepressNet allows for visualizing the salient regions of the input facial image in terms of its predicted score. The salient regions are identified by none other than DAM in DepressNet.

Suppose that there are K feature maps in the last convolutional layer of the DepressNet. For an input image to DepressNet, let Ik(i,j) denote the activation of the kth neuron of the last convolutional layer at spatial location (i,j), where k=1,2,…,K. By definition, each neuron gk of the GAP layer is a spatial average of the activation of the last convolutional layer,
gk=∑i,jIk(i,j),(2)
View Sourcefor k=1,2,…,K. Accordingly, the final depression prediction can be computed by
ℓ^=∑k=1Kwkgk,(3)
View SourceRight-click on figure for MathML and additional features.where wk is the weight of the kth neuron for the output of the GAP layer. By substituting Eq.(2) into Eq.(3), we have
ℓ^=∑k=1Kwk∑i,jIk(i,j)=∑i,j∑k=1KwkIk(i,j)(4)
View SourceRight-click on figure for MathML and additional features.Based on the DepressNet architecture, the depression activation map for a facial image input can be defined by
D(i,j)=∑k=1KwkIk(i,j).(5)
View SourceTherefore, the prediction output can also be expressed by ℓ^=∑i,jD(i,j), and essentially, the DAM D(i,j) indicates the contribution of the activation at spatial location (i,j) to the final depression prediction ℓ^.

Conceptually, in our DepressNet, each neuron of the last convolutional layer is activated by a certain visual depression pattern in its receptive field, and the feature map Ik reflects the presence of this visual depression pattern. DAM is considered as a weighted sum of the presence of these depression patterns at different image coordinations. Finally, by resizing the depression activation map to the size of the input facial image, one can locate the salient image regions that are discriminative in terms of its output score of depression severity. One example of the DAM output with the DepressNet is depicted in Fig. 2. We can see that the salient regions (around the eyes and forehead) of the input facial image for its depression scores are clearly identified.

3.3 Multi-Region DepressNet
Based on the DepressNet, we propose a multi-region DepressNet, by which multiple local deep regression models for different face regions are jointly leaned and their responses are combined to further improve the overall recognition performance. The basic idea behind this local learning strategy is that, CNNs are typically less powerful to model geometric transformations due to their fixed geometric structures in the network. Instead of making a prediction on one single global regression model, fusion of responses from local region-based deep models would make the ADD system more robust to various types of face misalignment. The architecture of the proposed MR-DepressNet is shown in Fig. 5.

Fig. 5. - 
Schematic illustration of the proposed MR-DepressNet.
Fig. 5.
Schematic illustration of the proposed MR-DepressNet.

Show All

For each face image input to the MR-DepressNet, we divide it into three overlapping regions (top, central, and bottom). Together with the full face image, these local image regions are fed into a four-stream DepressNet to learn four local regression models for different face regions. The output responses of the four sub-networks are integrated in the cost function layer, so that these local deep regression models can be jointly learned and mutually complementary for more robust prediction. Accordingly, the loss for MR-DepressNet can be written by:
L=12N∑i=1N(1M∑m=1Mfm(xi)−ℓi)2,(6)
View SourceRight-click on figure for MathML and additional features.where N is the batch size, M is the total number of the image regions, fm(xi) is the prediction output of the mth sub-network, and ℓi is the ground truth depression score for the ith input sample xi. In the multi-region scheme, the prediction output for an image input is computed by 1M∑Mm=1fm(xi).

As illustrated in Fig. 5, our MR-DepressNet can generate several depression activation maps for different face regions. Among these DAMs, one generated by the sub-network associated with the full face region can be chosen as the final DAM, although fusion of these DAMs can be an alternative solution. With MR-DepressNet, these local deep models for different face regions are jointly learned and optimized in a unified framework, and hence generation of the DAM for the full face region has already exploited the complementary information from other local deep models.

SECTION 4Experiments
4.1 Datasets
The training examples for our deep model learning are from the AVEC 2013 and 2014 datasets [3], [4].

There are 340 videos from 292 subjects in the AVEC 2013 depression dataset, and a subset (150 videos from 82 subjects) of the audio-visual depressive language corpus (AVid-Corpus) from this dataset is used in this work. The video clips in this subset are collected by a webcam and a microphone, and there is only one subject in each video performing the questionnaire task via human-computer interaction. The age of the subjects in the dataset ranges from 18 to 63 years old, with an average age of 31.5 years old. The average length of the video clips is about 25 minutes. The depression dataset is split into three subsets: training, development and test, each of which contains 50 videos. Each video has a label corresponding its depression severity level, which is assessed based on the BDI-II questionnaire [15]. BDI-II contains 21 standardized questions, and each answer is scored on a scale of 0 to 3. Higher total scores correspond to higher severity levels or symptoms. The standardized cutoffs used are summarized in Table 1. For the AVEC 2014 dataset, two tasks referred to as Freeform and Northwind from the 12 tasks of AViD-Corpus are used for in this work. There are 150 video clips for both tasks, and they are equally split into three partitions: training, development, and test, respectively.

TABLE 1 The Relation Between the BDI-II Cut-off Scores and the Depression Severity Level
Table 1- 
The Relation Between the BDI-II Cut-off Scores and the Depression Severity Level
4.2 Experimental Settings
In the proposed ADD framework, the first step is to perform face detection for each video frame. This is implemented using the machine learning toolkit Dlib [47] in our experiments. The facial region in each video frame is then cropped and aligned according to its eye positions, and finally, it is resized to 224×224 as the image input to the DepressNet for model training or testing. As for the MR-DepressNet, each image input is divided into three overlapping blocks (top, central, and bottom) and these local blocks have the same width and height, as shown in Fig. 5. Due to the presence of high temporal redundancy in videos, the total number of the image frames for each video input can be significantly reduced by a simple frame sampling scheme. Considering the video length as well as the the variation frequency of facial appearance of the two depression datasets, we empirically extract one frame out of every 100 frames for AVEC 2013 and 10 frames for AVEC 2014, respectively. As a result, we obtain about 400,000 image frames for AVEC 2013, and 50,000 image frames for AVEC 2014 in our experiments.

Network Training. The network structure of our DepressNet is schematically illustrated in Fig. 2, which differs from some conventional CNNs like AlexNet [29] and VGG [33] in that our DepressNet uses the GAP layer instead of the FC layers to directly connect the last convolutional layer and the regression output. In MR-DepressNet, each sub-network has the similar structure to the DepressNet, and the output responses from different sub-networks are jointly modeled in the cost function layer, as shown in Fig. 5. All the facial images input to the network are resized to 224 × 224 with RGB color channels. As the depression estimation is considered as a regression problem, there is only one neuron in the output layer giving the depression score. Specifically, DepressNet is pre-trained on the CASIA-WebFace dataset [45] that contains total of 453,453 images from 10,575 subjects after face detection, and fine-tuned on the AVEC 2013 and 2014 depression datasets that contain about 400,000 and 50,000 images from 150 subjects, respectively. In the fine-tuning step, the batch size is fixed at 32 per GPU, and the learning rate is set to 0.001. The number of iterations is empirically set to 100,000. To achieve fast convergence, the ADAM optimizer is adopted for training of our DepressNet and MR-DepressNet, which has been proved to be an effective practice to train deep model on relatively small datasets. β1, β2, and ϵ of the ADAM optimizer are set to the default values of 0.9, 0.999, and 1e-8, respectively. To enhance the generalization ability, we also use L2 weight decay with factor 0.0005 for all the network layers in the experiments. All the model training for different deep architectures are performed on a popular deep learning platform TensorFlow [48] with two Tesla-K80 GPUs (each with 12 GB global memory).

Evaluation Metrics. During testing, the depression score for a video input is defined as an average of the predicted scores for all the sampled frames from this video. The recognition (regression) performance of an ADD algorithm can be measured by two commonly used evaluation metrics: mean absolute error (MAE) and root mean square error (RMSE). The MAE ϵm and RMSE ϵr are respectively defined by:
ϵm=1N∑i=1N∣∣ℓ^i−ℓi∣∣(7)
View SourceRight-click on figure for MathML and additional features.
ϵr=1N∑i=1N(ℓ^i−ℓi)2−−−−−−−−−−−−−⎷,(8)
View SourceRight-click on figure for MathML and additional features.where N denotes the total number of video samples, ℓ^i and ℓi are the prediction output and the ground truth depression score for the ith video, respectively.

4.3 Analysis on Recognition Performance
To demonstrate the effectiveness and superiority of our visual-based ADD methods, we conduct experiments to evaluate our method on the AVEC 2013 [3] and 2014 [4] depression sub-challenge datasets. We implement several baseline ADD algorithms based on popular deep architectures (e.g., AlexNet, VGG and GoogleNet), and compare our method with these baselines as well as several state-of-the-art visual-based ADD methods.

Comparison with Previous Methods. All the compared methods use the same training and test sets in the experiments. We first compare our MR-DepressNet based ADD method (hereafter referred to as MR-DepressNet) with five previous visual-based methods [3], [6], [8], [12], [14] on the AVEC 2013 dataset. The recognition results of different methods are listed in Table 2. From this table we can see that MR-DepressNet achieves the best recognition performance (i.e., the lowest MAE/RMSE of 6.20/8.28) among all the visual-based solutions. We also compare our MR-DepressNet with seven existing visual-based solutions [4], [14], [19], [20], [21], [32], [49] on the AVEC 2014 dataset, and the results are summarized in Table 3. Our method achieves MAE/RMSE of 6.21/8.39 and outperforms six out of seven visual-based solutions by a significant margin. As shown in Table 3, our method achieves comparable recognition performance to Jan et al. method [32], where the best performing visual feature extracted by the VGG-Face produces the MAE/RMSE of 6.68/8.04. It should be noted that, in the work [14], a two-stream CNN was trained to fuse the deep features of facial appearance and dynamics for more accurate prediction. As can be seen in Tables 2 and 3, our MR-DepressNet achieves significant performance gain over the two-stream CNN solution on the two datasets, even only exploiting the visual information from facial appearance.

TABLE 2 Comparison of Different Visual-Based ADD Methods on the AVEC 2013 Dataset
Table 2- 
Comparison of Different Visual-Based ADD Methods on the AVEC 2013 Dataset
TABLE 3 Comparison of Different Visual-Based ADD Methods on the AVEC 2014 Dataset
Table 3- 
Comparison of Different Visual-Based ADD Methods on the AVEC 2014 Dataset
It is well-known that combination of multiple modalities of depression cues (e.g., audio, visual and dynamics) can be helpful to further improve the overall recognition performance of an ADD system. While the focus of this work is mainly on learning of visual depression feature for ADD, we are also interested in the comparison of recognition performance between our method and several state-of-the-art multimodal solutions (i.e., audio and visual cues) on the two depression datasets. Specifically, two competitive methods that exploit both visual and audio cues are compared with our method on the AVEC 2013 dataset, and the comparison results are depicted in Fig. 6. On the AVEC 2014 dataset, nine competitive methods that exploit both audio and visual cues are chosen for comparison with our method, and the comparison results are depicted in Fig. 6. We can see that, even only exploiting the visual cue for ADD, our method outperforms several competitive multimodal methods and achieves comparable performance to the best multimodal solutions [4], [5] on the two depression datasets.

Fig. 6. - 
Comparison with multimodal depression recognition methods on the AVEC 2013 (left) and 2014 (right) datasets. (A) and (V) in brackets denote the data modalities of audio and video used for depression recognition, respectively.
Fig. 6.
Comparison with multimodal depression recognition methods on the AVEC 2013 (left) and 2014 (right) datasets. (A) and (V) in brackets denote the data modalities of audio and video used for depression recognition, respectively.

Show All

The superiority of our method can be attributed to the well-designed DepressNet architecture that facilitates representation learning of visual depression feature from facial appearance. On one hand, compared to the hand-crafted depression features, visual depression feature learned by our DepressNet can reveal and capture high-level semantics structure from the visual training data, and hence the learned deep features can be more discriminative for prediction of depression severity than conventional hand-crafted features. On the other hand, joint learning of multiple local deep models for different facial regions further enhances the generalization ability of our method, leading to significant performance gain against most other CNN-based solutions.

Comparison of Different Deep Architectures. We also implement several CNN-based algorithms for the ADD problem based on various popular deep architectures, and compare our DepressNet with these popular deep architectures in terms of recognition performance on the two depression datasets. More specifically, these compared algorithms are implemented based on the following network architectures:

AlexNet. We train an AlexNet (pre-trained on CASIA-WebFace) for our ADD problem with the depression dataset. The detailed network configuration of AlexNet can be found in the literature [29].

AlexNet-GAP. AlexNet-GAP is first built up based on the original AlexNet (pre-trained on CASIA-WebFace), where the FC layers are replaced with a GAP layer that is directly connected to the final convolutional layer and the regression output. We then train the AlexNet-GAP with the depression dataset.

VGG-A. We train a 11-layers VGG (pre-trained on CASIA-WebFace) for our ADD problem with the depression dataset. The detailed network structure of VGG-A can be found in the literature [33].

VGG-A-GAP. VGG-A-GAP is first built up based on the original VGG-A (pre-trained on CASIA-WebFace), where the FC layers are replaced with a GAP layer that is directly connected to the final convolutional layer and the regression output layer. We then train the VGG-A-GAP with the depression dataset.

GoogleNet. We train a GoogleNet (pre-trained on CASIA-WebFace) with the depression dataset, which refers to the network structure introduced in the work [14], where only the visual modality of facial appearance is exploited for model training.

DepressNet-Full, DepressNet-Top, DepressNet-Central and DepressNet-Bottom. They refer to the sub-networks (built on ResNet50) of the MR-DepressNet taking the full, top, central, and bottom facial regions as the image input, respectively.

For fair comparison with our MR-DepressNet, all the standard CNN architectures being compared, i.e., AlexNet, VGG, GoogleNet, are also trained with the multi-region strategy (denoted by the prefix ’MR-’). They are first pre-trained on the CASIA-WebFace dataset, and then fine-tuned on the AVEC dataset. The comparison results of the different deep architectures on the two depression datasets are listed in Tables 4 and 5, respectively. As shown in the two tables, our MR-DepressNet outperforms most of the compared deep networks in terms of recognition performance with a significant margin. From the above experimental results we can make the following observations:

DepressNet generally performs better than the other popular deep architectures (e.g., AlexNet, VGG, and GoogleNet) for the visual-based ADD. The residual learning framework makes the training of very deep networks more easy, and hence can achieve more robust and accurate recognition performance for the ADD problem.

TABLE 4 Comparison of Different Deep Architectures for Visual-Based ADD on the AVEC 2013 Dataset

TABLE 5 Comparison of Different Deep Architectures for Visual-Based ADD on the AVEC 2014 Dataset

The use of the GAP layer instead of the FC layers in deep networks reduces the number of the network parameters significantly, while achieving comparable even better recognition performance in ADD.

Compared to a single local deep model for ADD, joint learning of multiple local deep models that are trained with the image data of different facial regions can effectively exploit complementary information from different facial regions to boost the overall recognition performance of the ADD system.

Among the local DepressNet models, DepressNet-Central achieves the best recognition performance. The result implies that top-central face region can be more discriminative and indicative than other regions for practical depression diagnosis. This can be visually explained by the DAM induced by our learned deep regression model.

4.4 Analysis on DAM
We present the depression activation maps generated by DepressNet, and give a brief discussion on how the DAM can help understand the insight of learned deep model for visual-based ADD.

Fig. 7 shows some example DAMs from the input facial images of size 224×224 and their corresponding depression scores (ground truths and predictions). These DAMs are able to identify salient regions that may have contributions to the final depression score prediction. Specifically, Figs. 7a and 7b depict the DAMs from two facial images with minimal depression severity level (i.e., BDI-II scores ranging from 0 to 13), and Figs. 7c, 7d, 7e, 7f, 7g, and 7h present the DAMs from facial images with mild, moderate and severe depression levels, respectively. From Fig. 7 we have the following key observations:

For the patients with mild, moderate or severe depression levels, DAM learns to identify the salient regions distributed over the top-central blocks of face. More specifically, the DAMs for these depression levels often discover those regions around the eyes and forehead, as shown in Figs. 7c, 7d, 7e, 7f, 7g, and 7h.

Fig. 7. - 
Depression activation maps for the facial images with different depression levels. The color bars on the right indicate the saliency weights (from 0.0 to 1.0).
Fig. 7.
Depression activation maps for the facial images with different depression levels. The color bars on the right indicate the saliency weights (from 0.0 to 1.0).

Show All

When the patient has minimal depression level and the score predicted by the deep model also falls into this level, the generated DAMs identify the salient regions uniformly distributed over the full face except for the regions around the eyes and forehead, as shown in Figs. 7a and 7b.

The above observations appear to be consistent with the general clinical manifestations of MDD that the patients with depression rarely smile and often wear a worried frown with glazed eyes. Based on the above observations, DAM provides reasonable transparency on our deep learning model to see why and how it makes the prediction. The visual explanation of DAM may assist the clinicians to quickly identify the salient face regions of depression.

SECTION 5Discussion and Conclusion
In this paper, we have presented a deep regression network called DepressNet to learn a depression representation with visual explanation that facilitates clinical prediction of depression severity from facial images. Specifically, a deep residual network equipped with a GAP layer was introduced to learn a deep regression model on the visual depression data, which not only facilitates accurate prediction of the depression severity, but also allows for identifying salient regions of the input facial image in terms of its severity level by generating the depression activation map. We also proposed a multi-region DepressNet, by which multiple local deep regression models for different face regions are jointly leaned and their responses are fused to improve the overall recognition performance. We evaluated our proposed method on two benchmark depression datasets, and the results showed that our method significantly boosts the state-of-the-art performance of visual-based depression recognition. We believe the DAM induced by our deep regression model may help discover visual depression pattern on human faces and understand why the learned deep model works.

We also observed in the experiments that the proposed deep regression model may perform the conservative prediction in ADD. It means that the predicted depression score is often lower than the ground truth, especially for the cases that the patients are with minimal, moderate or server depression levels. The prediction bias can be intuitively observed from the Fig. 8. One reasonable interpretation is that the training dataset has considerably larger number of normal facial images (with minimal depression level) than that of depressive facial images (with mild, moderate or server depression level). This imbalanced distribution of training samples in the two depression datasets is illustrated in Fig. 9.

Fig. 8. - 
Prediction bias of the proposed deep regression model on the AVEC 2013 and 2014 datasets.
Fig. 8.
Prediction bias of the proposed deep regression model on the AVEC 2013 and 2014 datasets.

Show All

Fig. 9. - 
Depression level distribution of the training samples in the AVEC 2013 and 2014 datasets.
Fig. 9.
Depression level distribution of the training samples in the AVEC 2013 and 2014 datasets.

Show All

In future work we are interested in investigating the depression recognition with multimodal depression data (e.g., audio, video, and facial dynamics) to further improve the overall prediction performance, although our proposed visual-based solution has achieved better performance than most existing multimodal solutions. Moreover, we plan to evaluate our method on the databases of other mental health institutions to consolidate the results as well as to enhance the learned deep depression representation that would benefit from being trained with a larger number of depression data.