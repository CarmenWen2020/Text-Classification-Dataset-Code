We investigate anomaly detection in an unsupervised framework and introduce long short-term memory (LSTM) neural network-based algorithms. In particular, given variable length data sequences, we first pass these sequences through our LSTM-based structure and obtain fixed-length sequences. We then find a decision function for our anomaly detectors based on the one-class support vector machines (OC-SVMs) and support vector data description (SVDD) algorithms. As the first time in the literature, we jointly train and optimize the parameters of the LSTM architecture and the OC-SVM (or SVDD) algorithm using highly effective gradient and quadratic programming-based training methods. To apply the gradient-based training method, we modify the original objective criteria of the OC-SVM and SVDD algorithms, where we prove the convergence of the modified objective criteria to the original criteria. We also provide extensions of our unsupervised formulation to the semisupervised and fully supervised frameworks. Thus, we obtain anomaly detection algorithms that can process variable length data sequences while providing high performance, especially for time series data. Our approach is generic so that we also apply this approach to the gated recurrent unit (GRU) architecture by directly replacing our LSTM-based structure with the GRU-based structure. In our experiments, we illustrate significant performance gains achieved by our algorithms with respect to the conventional methods.
SECTION I.Introduction
A. Preliminaries
Anomaly detection [1] has attracted significant interest in the contemporary learning literature due to its applications in a wide range of engineering problems [2]–[3][4]. In this article, we study the variable length anomaly detection problem in an unsupervised framework, where we seek to find a function to decide whether or not each unlabeled variable length sequence in a given data set is anomalous. Note that although this problem is extensively studied in the literature and there exist different methods, e.g., supervised (or semisupervised) methods, that require the knowledge of data labels, we employ an unsupervised method due to the high cost of obtaining accurate labels in most real-life applications [1]. However, we also extend our derivations to the semisupervised and fully supervised frameworks for completeness.

In the current literature, a common and widely used approach for anomaly detection is to find a decision function that defines the model of normality [1], [5]. In this approach, one first defines a certain decision function and then optimizes the parameters of this function with respect to a predefined objective criterion, e.g., the one-class support vector machines (OC-SVMs) and support vector data description (SVDD) algorithms [6], [7]. However, algorithms based on this approach examine time series data over a sufficiently long time window to achieve an acceptable performance [1], [8], [9]. Thus, their performances significantly depend on the length of this time window so that this approach requires careful selection for the length of the time window to provide a satisfactory performance [8], [10]. To enhance performance for time series data, Fisher kernel and generative models are introduced [11]–[12][13][14]. However, the main drawback of the Fisher kernel model is that it requires the inversion of the Fisher information matrix, which has a high computational complexity [11], [12]. On the other hand, in order to obtain an adequate performance from a generative model such as a hidden Markov model (HMM), one should carefully select its structural parameters, e.g., the number of states and topology of the model [13], [14]. Furthermore, the type of training algorithm has also considerable effects on the performance of generative models, which limits their usage in real-life applications [14]. Thus, neural networks, especially recurrent neural networks (RNNs)-based approaches are introduced, thanks to their inherent memory structure that can store “time” or “state” information [1], [15]. However, since the basic RNN architecture does not have control structures (gates) to regulate the amount of information to be stored [16], [17], a more advanced RNN architecture with several control structures, i.e., the long short-term memory (LSTM) network, is introduced [17], [18]. However, neural networks-based approaches cannot directly optimize an objective criterion for anomaly detection due to the lack of data labels in an unsupervised framework [1], [19]. Hence, they first predict a sequence from its past samples and then determine whether the sequence is an anomaly or not based on the prediction error, i.e., an anomaly is an event, which cannot be predicted from the past nominal data [1]. Thus, they require a probabilistic model for the prediction error and a threshold on the probabilistic model to detect anomalies, which results in challenging optimization problems and restricts their performance accordingly [1], [19], [20]. Furthermore, both the common and neural networks-based approaches can process only fixed-length vector sequences, which significantly limits their usage in real-life applications [1].

In order to circumvent these issues, we introduce novel LSTM-based anomaly detection algorithms for variable length data sequences. In particular, we first pass variable length data sequences through an LSTM-based structure to obtain fixed-length representations. We then apply our OC-SVM [6]-based algorithm and SVDD [7]-based algorithm for detecting anomalies in the extracted fixed-length vectors as illustrated in Fig. 1. Unlike the previous approaches in the literature [1], we jointly train the parameters of the LSTM architecture and the OC-SVM (or SVDD) formulation to maximize the detection performance. For this joint optimization, we propose two different training methods, i.e., a quadratic programming-based algorithm and gradient-based algorithm, where the merits of each different approach are detailed in the article. For our gradient-based training method, we modify the original OC-SVM and SVDD formulations and then provide the convergence results of the modified formulations to the original ones. Thus, instead of following the prediction-based approaches [1], [19], [20] in the current literature, we define proper objective functions for anomaly detection using the LSTM architecture and optimize the parameters of the LSTM architecture via these well-defined objective functions. Hence, our anomaly detection algorithms are able to process variable length sequences and provide high performance for time series data. Furthermore, since we introduce a generic approach in the sense that it can be applied to any RNN architecture, we also apply our approach to the gated recurrent unit (GRU) architecture [21], i.e., an advanced RNN architecture as the LSTM architecture, in our simulations. Through an extensive set of experiments, we demonstrate significant performance gains with respect to the conventional methods [6], [7], [10].


Fig. 1.
Overall structure of our anomaly detection approach.

Show All

B. Prior Art and Comparisons
Several different methods have been introduced for the anomaly detection problem [1]. Among these methods, the OC-SVM [6] and SVDD [7] algorithms are generally employed due their high performance in real-life applications [22]. However, these algorithms provide inadequate performance for time series data due to their inability to capture time dependencies [8], [9]. In order to improve the performances of these algorithms for time series data, in [9], Zhang et al. convert time series data into a set of vectors by replicating each sample so that they obtain 2-D vector sequences. However, even though they obtain 2-D vector sequences, the second dimension does not provide additional information such that this approach still provides inadequate performance for time series data [8]. As another approach, the OC-SVM-based method in [8] acquires a set of vectors from time series data by unfolding the data into a phase space using a time delay embedding process [23]. More specifically, for a certain sample, they create an E dimensional vector by using the previous E−1 samples along with the sample itself [8]. However, in order to obtain satisfactory performance from this approach, the dimensionality, i.e., E , should be carefully tuned, which restricts its usage in real-life applications [24]. On the other hand, even though LSTM-based algorithms provide high performance for time series data, we have to solve highly complex optimization problems to get adequate performance [1]. For example, the LSTM-based anomaly detection algorithms in [10] and [25] first predict time series data and then fit a multivariate Gaussian distribution to the error, where they also select a threshold for this distribution. Here, they allocate a different set of sequences to learn the parameters of the distribution and threshold via the maximum likelihood estimation technique [10], [25]. Thus, the conventional LSTM-based approaches require careful selection of several additional parameters, which significantly degrades their performance in real-life [1], [10]. Furthermore, both the OC-SVM- (or SVDD) and LSTM-based methods are able to process only fixed-length sequences [6], [7], [10]. To circumvent these issues, we introduce generic LSTM-based anomaly detectors for variable length data sequences, where we jointly train the parameters of the LSTM architecture and the OC-SVM (or SVDD) formulation via a predefined objective function. Therefore, we not only obtain high performance for time series data but also enjoy joint and effective optimization of the parameters with respect to a well-defined objective function.

C. Contributions
Our main contributions are as follows.

We introduce LSTM-based anomaly detection algorithms in an unsupervised framework, where we also extend our derivations to the semisupervised and fully supervised frameworks.

As the first time in the literature, we jointly train the parameters of the LSTM architecture and the OC-SVM (or SVDD) formulation via a well-defined objective function, where we introduce two different joint optimization methods. For our gradient-based joint optimization method, we modify the OC-SVM and SVDD formulations and then prove the convergence of the modified formulations to the original ones.

Thanks to our LSTM-based structure, the introduced methods are able to process variable length data sequences. In addition, unlike the conventional methods [6], [7], our methods effectively detect anomalies in time series data without requiring any preprocessing.

Through an extensive set of experiments involving real and simulated data, we illustrate significant performance improvements achieved by our algorithms with respect to the conventional methods [6], [7], [10]. Moreover, since our approach is generic, we also apply it to the recently proposed GRU architecture [21] in our experiments.

D. Organization of the Article
The organization of this article is as follows. In Section II, we first describe the variable length anomaly detection problem and then introduce our LSTM-based structure. In Section III-A, we introduce anomaly detection algorithms based on the OC-SVM formulation, where we also propose two different joint training methods in order to learn the LSTM and SVM parameters. The merits of each different approach are also detailed. In a similar manner, we introduce anomaly detection algorithms based on the SVDD formulation and provide two different joint training methods to learn the parameters in Section III-B. In Section IV, we demonstrate performance improvements over several real-life data sets. Thanks to our generic approach, we also introduce GRU-based anomaly detection algorithms. Finally, we provide concluding remarks in Section V.

SECTION II.Model and Problem Description
In this article, all vectors are column vectors and denoted by boldface lower case letters. Matrices are represented by boldface uppercase letters. For a vector a , aT is its ordinary transpose and ||a||=aTa−−−−√ is the ℓ2 -norm. The time index is given as subscript, e.g., ai is the i th vector. Here, 1 (and 0 ) is a vector of all ones (and zeros) and I represents the identity matrix, where the sizes are understood from the context.

We observe data sequences {Xi}ni=1 , i.e., defined as
Xi=[xi,1xi,2…xi,di]
View SourceRight-click on figure for MathML and additional features.where xi,j∈Rp , ∀j∈{1,2,…di} and di∈Z+ is the number of columns in Xi , which can vary with respect to i . Here, we assume that the bulk of the observed sequences are normal and the remaining sequences are anomalous. Our aim is to find a scoring (or decision) function to determine whether Xi is anomalous or not based on the observed data, where +1 and −1 represent the outputs of the desired scoring function for nominal and anomalous data, respectively. As an example application for this framework, in host-based intrusion detection [1], the system handles operating system call traces, where the data consist of system calls that are generated by users or programs. All traces contain system calls that belong to the same alphabet; however, the co-occurrence of the system calls is the key issue in detecting anomalies [1]. For different programs, these system calls are executed in different sequences, where the length of the sequence may vary for each program. Binary encoding of a sample set of call sequences can be X1=101011 , X2=1010 , and X3=1011001 for n=3 case [1]. After observing such a set of call sequences, our aim is to find a scoring function that successfully distinguishes the anomalous call sequences from the normal sequences.

In order to find a scoring function l(⋅) such that
l(Xi)={−1,+1,if Xi is anomalousotherwise 
View Sourceone can use the OC-SVM algorithm [6] to find a hyperplane that separates the anomalies from the normal data or the SVDD algorithm [7] to find a hypersphere enclosing the normal data while leaving the anomalies outside the hypersphere. However, these algorithms can only process fixed-length sequences. Hence, we use the LSTM architecture [18] to obtain a fixed-length vector representation for each Xi as we previously introduced in [26]. Although there exist several different versions of LSTM architecture, we use the most widely employed architecture, i.e., the LSTM architecture without peephole connections [17]. We first feed Xi to the LSTM architecture as demonstrated in Fig. 2, where the internal LSTM equations are as follows [18]:
zi,j=si,j=fi,j=ci,j=oi,j=hi,j=g(W(z)xi,j+R(z)hi,j−1+b(z))σ(W(s)xi,j+R(s)hi,j−1+b(s))σ(W(f)xi,j+R(f)hi,j−1+b(f))si,j⊙zi,j+fi,j⊙ci,j−1σ(W(o)xi,j+R(o)hi,j−1+b(o))oi,j⊙g(ci,j)(1)(2)(3)(4)(5)(6)
View SourceRight-click on figure for MathML and additional features.where ci,j∈Rm is the state vector, xi,j∈Rp is the input vector, and hi,j∈Rm is the output vector for the j th LSTM unit in Fig. 2. In addition, si,j , fi,j , and oi,j is the input, forget, and output gates, respectively. Here, g(⋅) is set to the hyperbolic tangent function, i.e., tanh , and applies to input vectors pointwise. Similarly, σ(⋅) is set to the sigmoid function. ⊙ is the operation for elementwise multiplication of two same-sized vectors. Furthermore, W(⋅) , R(⋅) , and b(⋅) are the parameters of the LSTM architecture, where the size of each is selected according to the dimensionality of the input and output vectors. Basically, in our LSTM architecture, ci,j−1 represents the cell state of the network from the previous LSTM block. This cell state provides an information flow between consecutive LSTM blocks. For the LSTM architecture, it is important to determine how much information we should keep in the cell state. Thus, in order to determine the amount of information to be kept, we use fi,j , which outputs a number between 0 and 1, and scales the cell state in (4). The next step is to determine how much new information we should learn from the data. For this purpose, we compute zi,j , which contains new candidate values, via a tanh layer, where we control the amount of learning through si,j . We then generate a new cell state information by multiplying old and new information with the forget and input gates, respectively, as in (4). Finally, we need to determine what we should output. In order to obtain the output, we use ci,j . However, we also need to determine which parts of the cell state we should keep for the output. Thus, we first compute oi,j to filter certain parts of the cell state. Then, we push the cell state through a tanh layer and multiply it with the output gate to obtain the final output of an LSTM block as in (6).


Fig. 2.
Our LSTM-based structure for obtaining fixed-length sequences. Note that each LSTM block has the same parameters; however, we represent them as separate blocks for presentation simplicity.

Show All

After applying the LSTM architecture to each column of our data sequences as illustrated in Fig. 2, we take the average of the LSTM outputs for each data sequence, i.e., the mean pooling method. Through this, we obtain a new set of fixed-length sequences, i.e., denoted as {h¯i}ni=1 , h¯i∈Rm . Note that we also use the same procedure to obtain the state information c¯i∈Rm for each Xi as demonstrated in Fig. 2. We emphasize that even though we do not use the mean state vector c¯i explicitly in Section III, all the calculations that include h¯i also requires the computation c¯i via the mean pooling method.

Remark 1:
We use the mean pooling method in order to obtain the fixed-length sequences as h¯i=(1/di)∑dij=1hi,j . However, we can also use the other pooling methods. For example, for the last and max pooling methods, we use h¯i=hi,di and h¯i=maxjhi,j , ∀i∈{1,2,…n} , respectively. Our derivations can be straightforwardly extended to these different pooling methods.

SECTION III.Novel Anomaly Detection Algorithms
In this section, we first formulate the anomaly detection approaches based on the OC-SVM and SVDD algorithms. We then provide joint optimization updates to train the parameters of the overall structure.

A. Anomaly Detection With the OC-SVM Algorithm
In this section, we provide an anomaly detection algorithm based on the OC-SVM formulation and derive the joint updates for both the LSTM and SVM parameters. For the training, we first provide a quadratic programming-based algorithm and then introduce a gradient-based training algorithm. To apply the gradient-based training method, we smoothly approximate the original OC-SVM formulation and then prove the convergence of the approximated formulation to the actual one in Section III-A2.

In the OC-SVM algorithm, our aim is to find a hyperplane that separates the anomalies from the normal data [6]. We formulate the OC-SVM optimization problem for the sequence {h¯i}ni=1 as follows [6]:
minθ∈Rnθ,w∈Rm,ξ∈R,ρ∈R∥w∥22+1nλ∑i=1nξi−ρs. t.: wTh¯i≥ρ−ξi,ξi≥0 ∀iW(⋅)TW(⋅)=I,R(⋅)TR(⋅)=Iand b(⋅)Tb(⋅)=1(7)(8)(9)
View Sourcewhere ρ and w are the parameters of the separating hyperplane, λ>0 is a regularization parameter, ξ is a slack variable to penalize misclassified instances, and we group the LSTM parameters {W(z) , R(z) , b(z) , W(s) , R(s) , b(s) , W(f) , R(f) , b(f) , W(o) , R(o) , b(o)} into θ∈Rnθ , where nθ=4m(m+p+1) . Since the LSTM parameters are unknown and h¯i is a function of these parameters, we also minimize the cost function in (7) with respect to θ .

After solving the optimization problem in (7)–(9), we use the scoring function
l(Xi)=sgn(wTh¯i−ρ)(10)
View SourceRight-click on figure for MathML and additional features.to detect the anomalous data, where the sgn(⋅) function returns the sign of its input.

We emphasize that while minimizing (7) with respect to θ , we might suffer from overfitting and impotent learning of time dependencies on the data [27], i.e., forcing the parameters to null values, e.g., θ=0 . To circumvent these issues, we introduce (9), which constraints the norm of θ to avoid overfitting and trivial solutions, e.g., θ=0 , while boosting the ability of the LSTM architecture to capture time dependencies [27], [28].

Remark 2:
In (9), we use an orthogonality constraint for each LSTM parameter. However, we can also use other constraints instead of (9) and solve the optimization problem in (7)–(9) in the same manner. For example, a common choice of constraint for neural networks is the Frobenius norm [29], i.e., defined as
∥A∥F=∑i∑jA2ij(11)
View Sourcefor a real matrix A , where Aij represents the element at the i th column and j th row of A . In this case, we can directly replace (9) with a Frobenius norm constraint for each LSTM parameter as in (11) and then solve the optimization problem in the same manner. Such approaches only aim to regularize the parameters [28]. However, for RNNs, we may also encounter exponential growth or decay in the norm of the gradients while training the parameters, which significantly degrades capabilities of these architectures to capture time dependencies [27], [28]. Moreover, (9) also regularizes the parameters by bounding the norm of each column of the coefficient matrices as one. Thus, in this article, we put the constraint (9) in order to regularize the parameters while improving the capabilities of the LSTM architecture in capturing time dependencies [27], [28].

1) Quadratic Programming-Based Training Algorithm:
Here, we introduce a training approach based on quadratic programming for the optimization problem in (7)–(9), where we perform consecutive updates for the LSTM and SVM parameters. For this purpose, we first convert the optimization problem to a dual form in the following. We then provide the consecutive updates for each parameter.

We have the following Lagrangian for the SVM parameters:
L(w,ξ,ρ,ν,α)=∥w∥22+1nλ∑i=1nξi−ρ−∑i=1nνiξi−∑i=1nαi(wTh¯i−ρ+ξi)(12)
View SourceRight-click on figure for MathML and additional features.where νi , αi≥0 are the Lagrange multipliers. Taking derivative of (12) with respect to w , ξ , and ρ and then setting the derivatives to zero give
w=∑i=1nαi=∑i=1nαih¯i1and αi=1/(nλ)−νi∀i.(13)(14)
View SourceNote that at the optimum, the inequalities in (8) become equalities if αi and νi are nonzero, i.e., 0<αi<1/(nλ) [6]. With this relation, we compute ρ as
ρ=∑j=1nαjh¯Tjh¯ifor  0<αi<1/(nλ).(15)
View Source

By substituting (13) and (14) into (12), we obtain the following dual problem for the constrained minimization in (7)–(9):
minθ∈Rnθ,α∈Rn12∑i=1n∑j=1nαiαjh¯Tih¯js. t.:  ∑i=1nαi=1and 0≤αi≤1/(nλ)∀iW(⋅)TW(⋅)=I,R(⋅)TR(⋅)=Iand b(⋅)Tb(⋅)=1(16)(17)(18)
View SourceRight-click on figure for MathML and additional features.where α∈Rn is a vector representation for αi ’s. Since the LSTM parameters are unknown, we also put the minimization term for θ into (16) as in (7). By substituting (13) into (10), we have the following scoring function for the dual problem:
l(Xi)=sgn(∑j=1nαjh¯Tjh¯i−ρ)(19)
View SourceRight-click on figure for MathML and additional features.where we calculate ρ using (15).

In order to find the optimal θ and α for the optimization problem in (16)–(18), we employ the following procedure. We first select a certain set of the LSTM parameters, i.e., θ0 . Based on θ0 , we find the minimizing α values, i.e., α1 , using the sequential minimal optimization (SMO) algorithm [30]. Now, we fix α as α1 and then update θ from θ0 to θ1 using the algorithm for optimization with orthogonality constraints in [31]. We repeat these consecutive update procedures until α and θ converge [32]. Then, we use the converged values in order to evaluate (19). Although the convergence of the algorithm is not guaranteed, it can be obtained by carefully tuning certain parameters, e.g., the learning rate, in most of real-life applications [32]. In the following, we explain these procedures in detail.

Based on θk , i.e., the LSTM parameter vector at the k th iteration, we update αk , i.e., the α vector at the k th iteration, using the SMO algorithm due to its efficiency in solving quadratic constrained optimization problems [30]. In the SMO algorithm, we choose a subset of parameters to minimize and fix the rest of parameters. In the extreme case, we choose only one parameter to minimize, however, due to (17), we must choose at least two parameters. To illustrate how the SMO algorithm works in our case, we choose α1 and α2 to update and fix the rest of the parameters in (16). From (17), we have
α1=1−S−α2,where  S=∑i=3nαi.(20)
View SourceWe first replace α1 in (16) with (20). We then take the derivative of (16) with respect to α2 and equate the derivative to zero. Thus, we obtain the following update for α2 at the k th iteration:
αk+1,2=(αk,1+αk,2)(K11−K12)+M1−M2K11+K22−2K12(21)
View Sourcewhere Kij≜h¯Tih¯j , Mi≜∑nj=3αk,jKij and αk,i represents the i th element of αk . Due to (17), if the updated value of α2 is outside of the region [0,1/(nλ)] , we project it to this region. Once α2 is updated as αk+1,2 , we obtain αk+1,1 using (20). For the rest of the parameters, we repeat the same procedure, which eventually converges to a certain set of parameters [30]. In this way, we obtain αk+1 , i.e., the minimizing α for θk .

Following the update of α , we update θ based on the updated αk+1 vector. For this purpose, we employ the optimization method in [31]. Since we have αk+1 that satisfies (17), we reduce the dual problem to
minθκ(θ,αk+1)=12∑i=1n∑j=1nαk+1,iαk+1,jh¯Tih¯js.t.:W(⋅)TW(⋅)=I,R(⋅)TR(⋅)=Iand b(⋅)Tb(⋅)=1.(22)(23)
View SourceRight-click on figure for MathML and additional features.For (22) and (23), we update W(⋅) as follows:
W(⋅)k+1=(I+μ2Ak)−1(I−μ2Ak)W(⋅)k(24)
View Sourcewhere the subscripts represent the current iteration index, μ is the learning rate, Ak=Gk(W(⋅)k)T−W(⋅)kGTk , and the element at the i th row and the j th column of G is defined as
Gij≜∂κ(θ,αk+1)∂W(⋅)ij.(25)
View SourceRight-click on figure for MathML and additional features.

Remark 3:
For R(⋅) and b(⋅) , we first compute the gradient of the objective function with respect to the chosen parameter as in (25). We then obtain Ak according to the chosen parameter. Using Ak , we update the chosen parameter as in (24).

With these updates, we obtain a quadratic programming-based training algorithm (see Algorithm 1 for the pseudocode) for our LSTM-based anomaly detector.

Algorithm 1 Quadratic Programming-Based Training for the Anomaly Detection Algorithm Based on OC-SVM
Initialize the LSTM parameters as θ0 and the dual OC-SVM parameters as α0

Determine a threshold ϵ as convergence criterion

k=−1

do

k=k+1

Using θk , obtain {h¯}ni=1 according to Fig. 2

Find optimal αk+1 for {h¯}ni=1 using (20) and (21)

Based on αk+1 , obtain θk+1 using (24) and Remark 3

while (κ(θk+1,αk+1)−κ(θk,αk))2>ϵ

Detect anomalies using (19) evaluated at θk and αk

2) Gradient-Based Training Algorithm:
Although the quadratic programming-based training algorithm directly optimizes the original OC-SVM formulation without requiring any approximation, since it depends on the separated consecutive updates of the LSTM and OC-SVM parameters, it might not converge to even a local minimum [32]. In order to resolve this issue, in this section, we introduce a training method based on only the first-order gradients, which updates the parameters at the same time. However, since we require an approximation to the original OC-SVM formulation to apply this method, we also prove the convergence of the approximated formulation to the original OC-SVM formulation in this section.

Considering (8), we write the slack variable in a different form as follows:
G(βw,ρ(h¯i))≜max{0,βw,ρ(h¯i)}∀i(26)
View Sourcewhere
βw,ρ(h¯i)≜ρ−wTh¯i.
View SourceBy substituting (26) into (7), we remove the constraint (8) and obtain the following optimization problem:
minw∈Rm,ρ∈R,θ∈Rnθ∥w∥22+1nλ∑i=1nG(βw,ρ(h¯i))−ρs.t.: W(⋅)TW(⋅)=I,R(⋅)TR(⋅)=Iand b(⋅)Tb(⋅)=1.(27)(28)
View SourceRight-click on figure for MathML and additional features.Since (26) is not a differentiable function, we are unable to solve the optimization problem in (27) using gradient-based optimization algorithms. Hence, we employ a differentiable function
Sτ(βw,ρ(h¯i))=1τlog(1+eτβw,ρ(h¯i))(29)
View SourceRight-click on figure for MathML and additional features.to smoothly approximate (26), where τ>0 is the smoothing parameter and log represents the natural logarithm. In (29), as τ increases, Sτ(⋅) converges to G(⋅) (see Fig. 3); hence, we choose a large value for τ .


Fig. 3.
Comparison of (26) with its smooth approximations.

Show All

Proposition 1:
As τ increases, Sτ(βw,ρ(h¯i)) uniformly converges to G(βw,ρ(h¯i)) . As a consequence, our approximation Fτ(w,ρ,θ) converges to the SVM objective function F(w,ρ,θ) , i.e., defined as
F(w,ρ,θ)≜∥w∥22+1nλ∑i=1nG(βw,ρ(h¯i))−ρ.
View Source

Proof of Proposition I:
The proof of the proposition is given in Appendix A.

With (29), we modify our optimization problem as follows:
minw∈Rm,ρ∈R,θ∈RnθFτ(w,ρ,θ)s.t.: W(⋅)TW(⋅)=I,R(⋅)TR(⋅)=Iand b(⋅)Tb(⋅)=1(30)(31)
View SourceRight-click on figure for MathML and additional features.where Fτ(⋅,⋅,⋅) is the objective function of our optimization problem and defined as
Fτ(w,ρ,θ)≜∥w∥22+1nλ∑i=1nSτ(βw,ρ(h¯i))−ρ.
View SourceTo obtain the optimal parameters for (30) and (31), we update w , ρ and θ until they converge to a local or global optimum [31], [33]. For the update of w and ρ , we use the gradient descent algorithm [33], where we compute the first-order gradient of the objective function with respect to each parameter. We first compute the gradient for w as follows:
∇wFτ(w,ρ,θ)=w+1nλ∑i=1n−h¯ieτβw,ρ(h¯i)1+eτβw,ρ(h¯i).(32)
View SourceUsing (32), we update w as
wk+1=wk−μ∇wFτ(w,ρ,θ)|w=wkρ=ρkθ=θk(33)
View Sourcewhere the subscript k indicates the value of any parameter at the k th iteration. Similarly, we calculate the derivative of the objective function with respect to ρ as follows:
∂Fτ(w,ρ,θ)∂ρ=1nλ∑i=1neτβw,ρ(h¯i)1+eτβw,ρ(h¯i)−1.(34)
View SourceRight-click on figure for MathML and additional features.Using (34), we update ρ as
ρk+1=ρk−μ∂Fτ(w,ρ,θ)∂ρ|w=wkρ=ρkθ=θk.(35)
View SourceRight-click on figure for MathML and additional features.For the LSTM parameters, we use the method for optimization with orthogonality constraints in [31] due to (31). To update W(⋅) , we calculate the gradient of the objective function as
∂Fτ(w,ρ,θ)∂W(⋅)ij=1nλ∑i=1n−wT(∂h¯i/∂W(⋅)ij)eτβw,ρ(h¯i)1+eτβw,ρ(h¯i).(36)
View SourceWe then update W(⋅) using (36) as
W(⋅)k+1=(I+μ2Bk)−1(I−μ2Bk)W(⋅)k(37)
View Sourcewhere Bk=Mk(W(⋅)k)T−W(⋅)kMTk and
Mij≜∂Fτ(w,ρ,θ)∂W(⋅)ij.(38)
View SourceRight-click on figure for MathML and additional features.

Remark 4:
For R(⋅) and b(⋅) , we first compute the gradient of the objective function with respect to the chosen parameter as in (38). We then obtain Bk according to the chosen parameter. Using Bk , we update the chosen parameter as in (37).

Remark 5:
In the semisupervised framework, we have the following optimization problem for our SVM-based algorithms [34]:
minθ,w,ξ,η,γ,ρ(∑li=1ηi+∑l+kj=l+1min(γj,ξj)(1/C))+∥w∥s.t.: yi(wTh¯i+ρ)≥1−ηi,ηi≥0, i=1,…,lwTh¯j−ρ≥1−ξj,  ξj≥0 j=l+1,…,l+k−wTh¯j+ρ≥1−γj, γj≥0j=l+1,…,l+kW(⋅)TW(⋅)=I,R(⋅)TR(⋅)=Iand  b(⋅)Tb(⋅)=1(39)(40)(41)(42)(43)
View Sourcewhere γ∈R and η∈R are slack variables as ξ , C is a tradeoff parameter, l and k are the number of the labeled and unlabeled data instances, respectively, and yi∈{−1,+1} represents the label of the i th data instance.

For the application of quadratic programming-based training method in the semisupervised case, we apply all the steps from (12) to (25) for the optimization problem in XS (39)–(43). Similarly, we modify the equations from (26) to (38) according to (39)–(43) in order to get the gradient-based training method in the semisupervised framework. For the supervised implementations, we follow the same procedures with the semisupervised implementations for k=0 case.

Hence, we complete the required updates for each parameter. The complete algorithm is also provided in Algorithm 2 as a pseudocode. Moreover, we illustrate the convergence of our approximation (29)–(26) in Proposition 1. Using Proposition 1, we then demonstrate the convergence of the optimal values for our objective function (30) to the optimal values of the actual SVM objective function (27) in Theorem 1.

Algorithm 2 Gradient-Based Training for the Anomaly Detection Algorithm Based on OC-SVM
Initialize the LSTM parameters as θ0 and the OC-SVM parameters as α0 and ϵ

Determine a threshold ϵ as convergence criterion

k=−1

do

k=k+1

Using θk , obtain {h¯}ni=1 according to Fig. 2

Obtain wk+1 , ρk+1 and θk+1 using (33), (35), (37) and Remark 4

while (Fτ(wk+1,ρk+1,θk+1)−Fτ(wk,ρk,θk))2>ϵ

Detect anomalies using (10) evaluated at wk , ρk and θk

Theorem 1:
Let wτ and ρτ be the solutions of (30) for any fixed θ . Then, wτ and ρτ are unique and Fτ(wτ,ρτ,θ) converges to the minimum of F(w,ρ,θ) .

Proof of Theorem 1:
The proof of the theorem is given in Appendix B.

B. Anomaly Detection With the SVDD Algorithm
In this section, we introduce an anomaly detection algorithm based on the SVDD formulation and provide the joint updates in order to learn both the LSTM and SVDD parameters. However, since the generic formulation is the same with the OC-SVM case, we only provide the required and distinct updates for the parameters and proof for the convergence of the approximated SVDD formulation to the actual one.

In the SVDD algorithm, we aim to find a hypersphere that encloses the normal data while leaving the anomalous data outside the hypersphere [7]. For the sequence {h¯i}ni=1 , we have the following SVDD optimization problem [7]:
minθ∈Rnθ,c~∈Rm,ξ∈R,R∈RR2+1nλ∑i=1nξis. t.: ∥h¯i−c~∥2−R2≤ξi,ξi≥0 ∀iW(⋅)TW(⋅)=I,R(⋅)TR(⋅)=I and  b(⋅)Tb(⋅)=1(44)(45)(46)
View Sourcewhere λ>0 is a tradeoff parameter between R2 and the total misclassification error, R is the radius of the hypersphere, and c~ is the center of the hypersphere. In addition, θ and ξ represent the LSTM parameters and the slack variable, respectively, as in the OC-SVM case. After solving the constrained optimization problem in (44)–(46), we detect anomalies using the following scoring function:
l(Xi)=sgn(R2−∥h¯i−c~∥2).(47)
View SourceRight-click on figure for MathML and additional features.

1) Quadratic Programming-Based Training Algorithm:
In this section, we introduce a training algorithm based on quadratic programming for (44)–(46). As in the OC-SVM case, we first assume that the LSTM parameters are fixed and then perform optimization over the SVDD parameters based on the fixed LSTM parameters. For (44) and (45), we have the following Lagrangian:
L(c~,ξ,R,ν,α)=R2+1nλ∑i=1nξi−∑i=1nνiξi−∑i=1nαi(ξi−∥h¯i−c~∥2+R2)(48)
View Sourcewhere νi , αi≥0 are the Lagrange multipliers. Taking derivative of (48) with respect to c~ , ξ , and R and then setting the derivatives to zero yields
c~=∑i=1nαi=∑i=1nαih¯i1and αi=1/(nλ)−νi∀i.(49)(50)
View SourceRight-click on figure for MathML and additional features.Putting (49) and (50) into (48), we obtain a dual form for (44) and (45) as follows:
minθ∈Rnθ,α∈Rn∑i=1n∑j=1nαiαjh¯Tih¯j−∑i=1nαih¯Tih¯is. t.: ∑i=1nαi=1and 0≤αi≤1/(nλ)∀iW(⋅)TW(⋅)=I,R(⋅)TR(⋅)=Iand b(⋅)Tb(⋅)=1.(51)(52)(53)
View SourceRight-click on figure for MathML and additional features.Using (49), we modify (47) as
l(Xi)=sgn(R2−∑k=1n∑j=1nαkαjh¯Tkh¯j+2∑j=1nαjh¯Tjh¯i−h¯Tih¯i).(54)
View SourceRight-click on figure for MathML and additional features.In order to solve the constrained optimization problem in (51)–(53), we employ the same approach as in the OC-SVM case. We first fix a certain set of the LSTM parameters θ . Based on these parameters, we find the optimal α using the SMO algorithm. After that, we fix α to update θ using the algorithm for optimization with orthogonality constraints. We repeat these procedures until we reach convergence. Finally, we evaluate (54) based on the converged parameters.

Remark 6:
In the SVDD case, we apply the SMO algorithm using the same procedures with the OC-SVM case. In particular, we first choose two parameters, e.g., α1 and α2 , to minimize and fix the other parameters. Due to (52), the chosen parameters must obey (20). Hence, we have the following update rule for α2 at the k th iteration:
αk+1,2=2(1−S)(K11−K12)+K22−K11+M1−M22(K11+K22−2K12)
View SourceRight-click on figure for MathML and additional features.where S=∑nj=3αk,j and the other definitions are the same with the OC-SVM case. We then obtain αk+1,1 using (20). By this, we obtain the updated values αk+1,2 and αk+1,1 . For the remaining parameters, we repeat this procedure until reaching convergence.

Remark 7:
For the SVDD case, we update W(⋅) at the k th iteration as in (24). However, instead of (25), we have the following definition for G :
Gij=∂π(θ,αk+1)∂W(⋅)ij
View Sourcewhere
π(θ,αk+1)≜∑i=1n∑j=1nαk+1,iαk+1,jh¯Tih¯j−∑i=1nαk+1,ih¯Tih¯i
View Sourceat the k th iteration. For the remaining parameters, we follow the procedure in Remark 3.

Hence, we obtain a quadratic programming-based training algorithm for our LSTM-based anomaly detector, which is also described in Algorithm 3 as a pseudocode.

Algorithm 3 Quadratic Programming-Based Training for the Anomaly Detection Algorithm Based on SVDD
Initialize the LSTM parameters as θ0 and the dual SVDD parameters as α0

Determine a threshold ϵ as convergence criterion

k=−1

do

k=k+1

Using θk , obtain {h¯}ni=1 according to Fig. 2

Find optimal αk+1 for {h¯}ni=1 using the procedure in Remark 6

Based on αk+1 , obtain θk+1 using Remark 7

while (π(θk+1,αk+1)−π(θk,αk))2>ϵ

Detect anomalies using (54) evaluated at θk and αk

2) Gradient-Based Training Algorithm:
In this section, we introduce a training algorithm based on only the first-order gradients for (44)–(46). We again use the G(⋅) function in (26) in order to eliminate the constraint in (45) as follows:
minθ∈Rnθ,c~∈Rm,R∈RR2+1nλ∑i=1nG(ΨR,c~(h¯i))s.t.: W(⋅)TW(⋅)=I,R(⋅)TR(⋅)=Iand b(⋅)Tb(⋅)=1(55)(56)
View SourceRight-click on figure for MathML and additional features.where
ΨR,c~(h¯i)≜∥h¯i−c~∥2−R2.
View SourceSince the gradient-based methods cannot optimize (55) due to the nondifferentiable function G(⋅) , we employ Sτ(⋅) instead of G(⋅) and modify (55) as
minθ∈Rnθ,c~∈Rm,R∈RFτ(c~,R,θ)=R2+1nλ∑i=1nSτ(ΨR,c~(h¯i))s.t.: W(⋅)TW(⋅)=I,R(⋅)TR(⋅)=I and b(⋅)Tb(⋅)=1(57)(58)
View Sourcewhere Fτ(⋅,⋅,⋅) is the objective function of (57). To obtain the optimal values for (57) and (58), we update c~ , R , and θ till we reach either a local or a global optimum. For the updates of c~ and R , we employ the gradient descent algorithm, where we use the following gradient calculations. We first compute the gradient of c~ as
∇c~Fτ(c~,R,θ)=1nλ∑i=1n2(c~−h¯i)eτΨc~,R(h¯i)1+eτΨc~,R(h¯i).(59)
View SourceRight-click on figure for MathML and additional features.Using (59), we have the following update:
c~k+1=c~k−μ∇c~Fτ(c~,R,θ)∣∣c~=c~kR2=R2kθ=θk(60)
View SourceRight-click on figure for MathML and additional features.where the subscript k represents the iteration number. Likewise, we compute the derivative of the objective function with respect to R2 as
∂Fτ(c~,R,θ)∂R2=1+1nλ∑i=1n−eτΨc~,R(h¯i)1+eτΨc~,R(h¯i).(61)
View SourceWith (61), we update R2 as
R2k+1=R2k−μ∂Fτ(c~,R,θ)∂R2∣∣c~=c~kR2=R2kθ=θk.(62)
View SourceFor θ , the gradient calculation is as follows:
∂Fτ(c~,R,θ)∂W(⋅)ij=∑i=1n2(∂h¯i/∂W(⋅)ij)T(h¯i−c~)eτΨc~,R(h¯i)nλ(1+eτΨc~,R(h¯i)).(63)
View SourceRight-click on figure for MathML and additional features.Using (63), we have the following update:
W(⋅)k+1=(I+μ2Bk)−1(I−μ2Bk)W(⋅)k(64)
View SourceRight-click on figure for MathML and additional features.where Bk=Mk(W(⋅)k)T−W(⋅)kMTk and
Mij≜∂Fτ(c~,R,θ)∂W(⋅)ij.(65)
View SourceRight-click on figure for MathML and additional features.

Remark 8:
For R(⋅) and b(⋅) , we first compute the gradient of the objective function with respect to the chosen parameter as in (65). We then obtain Bk according to the chosen parameter. Using Bk , we update the chosen parameter as in (64).

Remark 9:
In the semisupervised framework, we have the following optimization problem for our SVDD-based algorithms [35]:
minθ,c~,R,ξ,γ,ηR2−C1γ+C2∑i=1lξi+C3∑j=l+1l+kηjs.t.: ∥h¯i−c~∥2−R2≤ξi,ξi≥0 ∀li=1yj(∥h¯j−c~∥2−R2)≤−γ+ηjηj≥0 ∀l+kj=l+1W(⋅)TW(⋅)=I,R(⋅)TR(⋅)=I and b(⋅)Tb(⋅)=1(66)(67)(68)(69)
View SourceRight-click on figure for MathML and additional features.where η∈R is a slack variable as ξ , γ∈R is the margin of the labeled data instances, C1 , C2 , and C3 are tradeoff parameters, k and l are the number of the labeled and unlabeled data instances, respectively, and yj∈{−1,+1} represents the label of the j th data instance.

For the quadratic programming-based training method, we modify all the steps from (48) to (54), Remark 6 and Remark 7 with respect to (66)–(69). In a similar manner, we modify the equations from (55) to (65) according to (66)–(69) in order to obtain the gradient-based training method in the semisupervised framework. For the supervised implementations, we follow the same procedures with the semisupervised implementations for l=0 case.

The complete algorithm is provided in Algorithm 4. In the following, we provide the convergence proof as in the OC-SVM case.

Algorithm 4 Gradient-Based Training for the Anomaly Detection Algorithm Based on SVDD
Initialize the LSTM parameters as θ0 and the SVDD parameters as c~0 and R02

Determine a threshold ϵ as convergence criterion

k=−1

do

k=k+1

Using θk , obtain {h¯}ni=1 according to Fig. 2

Obtain c~k+1 , R2k+1 and θk+1 using (60), (62), (64) and Remark 8

while (Fτ(c~k+1,Rk+1,θk+1)−Fτ(c~k,Rk,θk))2>ϵ

Detect anomalies using (47) evaluated at c~k , R2k and θk

Theorem 2:
Let c~τ and R2τ be the solutions of (57) for any fixed θ . Then, c~τ and R2τ are unique and Fτ(c~τ,Rτ,θ) converges to the minimum of F(c~,R,θ) , i.e., defined as
F(c~,R,θ)≜R2+1nλ∑i=1nG(ΨR,c~(h¯i)).
View Source

Proof of Theorem 2:
The proof of the theorem is given in Appendix C.

SECTION IV.Simulations
In this section, we demonstrate the performances of the algorithms on several different data sets. We first evaluate the performances on a data set that contains variable length data sequences, i.e., the digit data set [36]. We then compare the anomaly detection performances on several different benchmark real data sets such as the occupancy [37], Hong Kong Exchange (HKE) rate [38], http [39], and Alcoa stock price [40] data sets. While performing experiments on real benchmark data sets, we also include the GRU-based algorithms in order to compare their performances with the LSTM-based ones. Moreover, we also measure the training times of the algorithms and perform an experiment to observe the effects of the orthogonality constraint in this section. Note that since the introduced algorithms have bounded functions, e.g., the sigmoid function in the LSTM architecture, for all the experiments in this section, we normalize each dimension of the data sets into [−1, 1].

Throughout this section, we denote the LSTM-based OC-SVM anomaly detectors that are trained with the gradient and quadratic programming-based algorithms as “LSTM-GSVM” and “LSTM-QPSVM,” respectively. In a similar manner, we use “LSTM-GSVDD” and “LSTM-QPSVDD” for the SVDD-based anomaly detectors. Moreover, for the labels of the GRU-based algorithms, we replace the LSTM prefix with GRU.

A. Anomaly Detection for Variable Length Data Sequences
In this section, we first evaluate the performances of the introduced anomaly detectors on the digit data set [36]. In this data set, we have the pixel samples of digits, which were written on a tablet by several different authors [36]. Since the speed of writing varies from person to person, the number of samples for a certain digit might significantly differ. The introduced algorithms are able to process such kind of sequences, thanks to their generic structure in Fig. 2. However, the conventional OC-SVM and SVDD algorithms cannot directly process these sequences [6], [7]. For these algorithms, we take the mean of each sequence to obtain a fixed-length vector sequence, i.e., 2-D in this case (two coordinates of a pixel). In order to evaluate the performances, we first choose a digit as normal and another digit as an anomaly. We emphasize that we randomly choose digits for illustration and obtain similar performances for the other digits. We then divide the samples of these digits into training and test parts, where we allocate 60% of the samples for the training part and 40% for the test part. In both the training and test parts, we select the samples so that 10% of the samples are anomalies. Then, using the training part, we optimize the parameters of each algorithm using twofold cross validation, where we also select a certain crucial parameter, e.g., μ . This procedure results in μ=0.05 , 0.001, 0.05, and 0.01 for LSTM-GSVM, LSTM-QPSVM, LSTM-GSVDD, and LSTM-QPSVDD, respectively. Furthermore, we select the output dimension of the LSTM architecture as m=2 and the regularization parameter as λ=0.5 for all the algorithms. For the implementation of the conventional OC-SVM and SVDD algorithms, we use the LIBSVM library and their parameters are selected in a similar manner via built-in optimization tools of LIBSVM [41].

Here, we use the area under the receiver operating characteristic (ROC) curve as a performance metric [42]. In a ROC curve, we plot a true positive rate (TPR) as a function of false positive rate (FPR). Area under this curve, i.e., also known as AUC, is a well-known performance measure for anomaly detection tasks [42]. In Fig. 4(a) and (b), we illustrate the ROC curves and provide the corresponding AUC scores, where we label digit “0” and “9” as normal and anomaly, respectively. For the OC-SVM and SVDD algorithms, since we directly take the mean of variable length data sequences to obtain fixed-length sequences, they achieve significantly lower AUC scores compared to the introduced LSTM-based methods. Among the LSTM-based methods, LSTM-GSVM slightly outperforms LSTM-QPSVM. On the other hand, LSTM-GSVDD achieves significantly higher AUC than LSTM-QPSVDD. Since the quadratic programming-based training method depends on the separated consecutive updates of the LSTM and SVM (or SVDD) parameters, it might not converge to even a local minimum. However, the gradient-based method can guarantee convergence to at least a local minimum given a proper choice of the learning rate [33]. Thus, although these methods might provide similar performances as in Fig. 4(a), it is also expected to obtain much higher performance from the gradient-based method for certain cases as shown in Fig. 4(b). However, overall, the introduced algorithms provide significantly higher AUC than the conventional methods.

Fig. 4. - ROC curves of the algorithms for the digit data set, where we consider digit “0” as normal and digit “9” as anomaly (a) for the SVM-based algorithms and (b) for the SVDD-based algorithms.
Fig. 4.
ROC curves of the algorithms for the digit data set, where we consider digit “0” as normal and digit “9” as anomaly (a) for the SVM-based algorithms and (b) for the SVDD-based algorithms.

Show All

Besides the previous scenario, we also consider a scenario, where we label digit “1” and “7” as normal and anomaly, respectively. In Fig. 5(a) and (b), we illustrate the ROC curves and provide the corresponding AUC scores. As in the previous scenario, for both the SVM and SVDD cases, the introduced algorithms achieve higher AUC scores than the conventional algorithms. Among the introduced algorithms, LSTM-GSVM and LSTM-GSVDD achieve the highest AUC scores for the SVM and SVDD cases, respectively. Furthermore, the AUC score of each algorithm is much lower compared to the previous case due to the similarity between digits “1” and “7.”


Fig. 5.
ROC curves of the algorithms for the digit data set, where we consider digit “1” as normal and digit “7” as anomaly (a) for the SVM-based algorithms and (b) for the SVDD-based algorithms.

Show All

In addition to the digit data set, we perform another experiment that handles variable length data sequences. In this experiment, we evaluate the anomaly detection performances of the algorithms on a financial data set, i.e., the Ford stock price data set [43]. Here, we have daily stock price values. For our anomaly detection framework, we first artificially introduce anomalies via a Gaussian distribution with the mean and ten times the variance of the training data. We then select certain parts of the time series data by applying a variable length time windowing operation, thus we obtain variable length data sequences. Moreover, unlike the previous cases, we choose μ=0.01,0.001,0.001 , and 0.005 for LSTM-GSVM, LSTM-QPSVM, LSTM-GSVDD, and LSTM-QPSVDD, respectively.

In Fig. 6, we observe that the LSTM-based algorithms achieve considerably higher AUC scores than the SVM and SVDD algorithms. Among the LSTM-based methods, LSTM-GSVM slightly outperforms LSTM-QPSVM. Similarly, LSTM-GSVDD achieves slightly higher AUC than LSTM-QPSVDD. Moreover, as in the previous experiments, the gradient-based training method provides higher performance compared to the quadratic programming-based method, thanks to its learning capabilities.

Fig. 6. - ROC curves of the stock price data set for (a) SVM-based algorithms and (b) SVDD-based algorithms.
Fig. 6.
ROC curves of the stock price data set for (a) SVM-based algorithms and (b) SVDD-based algorithms.

Show All

B. Benchmark Real Data sets
In this section, we compare the AUC scores of each algorithm on several different real benchmark data sets. Moreover, we provide the training times and evaluate the effects of the orthogonality constraint on these data sets. Since our approach in this article is generic, in addition to the LSTM-based algorithms, we also implement our approach on the recently introduced RNN architecture, i.e., the GRU architecture, which is defined by the following equations [21]:
z~i,j=ri,j=h~i,j=hi,j=σ(W(z~)xi,j+R(z~)hi,j−1)σ(W(r)xi,j+R(r)hi,j−1)g(W(h~)xi,j+ri,j⊙(R(h~)hi,j−1))h~i,j⊙z~i,j+hi,j−1⊙(1−z~i,j)(70)(71)(72)(73)
View SourceRight-click on figure for MathML and additional features.where hi,j∈Rm is the output vector and xi,j∈Rp is the input vector. Furthermore, W(⋅) and R(⋅) are the parameters of the GRU, where the sizes are selected according to the dimensionality of the input and output vectors. We then replace (1)–(6) with (70)–(73) in Fig. 2 to obtain GRU-based anomaly detectors. Note that in this section, we also include the LSTM-based anomaly detection approach in [10] and [25] as another benchmark performance criterion, especially for the experiments with time series data.

1) Occupancy Detection:
We first evaluate the performances of the algorithms on the occupancy data set [37]. In this data set, we have five features, which are relative humidity percentage, light (in lux), carbon dioxide level (in ppm), temperature (in Celsius), and humidity ratio, and our aim is to determine whether an office room is occupied or not based on the features. Here, we use the same procedure with Section IV-A to separate the test and training data. Moreover, using the training data, we select μ=0.05 , 0.05, 0.001, and 0.01 for LSTM-GSVM, LSTM-QPSVM, LSTM-GSVDD, and LSTM-QPSVDD, respectively. Note that, for the GRU-based algorithms in this section, we use the same parameter setting with the LSTM-based algorithms. Furthermore, we choose m=5 and λ=0.5 for all of the experiments in this section in order to maximize the performances of the algorithms.

As can be seen in Table I, due to their inherent memory, both the LSTM- and GRU-based algorithms achieve considerably high-AUC scores compared to the conventional SVM and SVDD algorithms. Moreover, GRU-GSVDD achieves the highest AUC score among all the algorithms, where the LSTM-based algorithms (LSTM-GSVM and LSTM-QPSVM) also provide comparable AUC scores. Here, we also observe that the gradient-based training method provides higher AUC scores compared to the quadratic programming-based training method, which might stem from its separated update procedure that does not guarantee convergence to a certain local minimum.

TABLE I AUC Scores of the Algorithms for the Occupancy, HKE Rate, Http, and Alcoa Stock Price Data sets

2) Anomalous Exchange Rate Detection:
Other than the occupancy data set, we also perform an experiment on the HKE rate data set in order to examine the performances for a real-life financial scenario. In this data set, we have the amount of Hong Kong dollars that one can buy for one US dollar each day. In order to introduce anomalies to this data set, we artificially add samples from a Gaussian distribution with the mean and ten times the variance of the training data. Furthermore, using the training data, we select μ=0.01 , 0.005, 0.05, and 0.05 for LSTM-GSVM, LSTM-QPSVM, LSTM-GSVDD, and LSTM-QPSVDD, respectively.

In Table I, we illustrate the AUC scores of the algorithms on the HKE rate data set. Since we have time-series data, both the LSTM- and GRU-based algorithms naturally outperform the conventional methods, thanks to their inherent memory, which preserves sequential information. Moreover, since the LSTM architecture also controls its memory content via an output gate unlike the GRU architecture [21], we obtain the highest AUC scores from LSTM-GSVM. As in the previous cases, the gradient-based training method provides better performance than the quadratic programming-based training.

3) Network Anomaly Detection:
We also evaluate the AUC scores of the algorithms on the http data set [39]. In this data set, we have 4 features, which are duration (number of seconds of the connection), network service, number of bytes from source to destination and from destination to source. Using these features, we aim to distinguish normal connections from network attacks. In this experiment, we select μ=0.01 , 0.05, 0.001, and 0.01 for LSTM-GSVM, LSTM-QPSVM, LSTM-GSVDD, and LSTM-QPSVDD, respectively.

We demonstrate the performances of the algorithms on the http data set in Table I. Even though all the algorithms achieve high-AUC scores on this data set, we still observe that the LSTM- and GRU-based algorithms have higher AUC scores than the conventional SVM and SVDD methods. Overall, GRU-QPSVDD achieves the highest AUC score and the quadratic programming-based training methods perform better than the gradient-based training method on this data set. However, since the AUC scores are very high and close to each other, we observe only slight performance improvement for our algorithms in this case.

4) Anomalous Stock Price Detection:
As the last experiment, we evaluate the anomaly detection performances of the algorithms on another financial data set, i.e., the Alcoa stock price data set [40]. In this data set, we have daily stock price values. As in the HKE rate data set, we again artificially introduce anomalies via a Gaussian distribution with the mean and ten times the variance of the training data. Moreover, we choose μ=0.01,0.001,0.001 , and 0.005 for LSTM-GSVM, LSTM-QPSVM, LSTM-GSVDD, and LSTM-QPSVDD, respectively.

In Table I, we illustrate the AUC scores of the algorithms on the Alcoa stock price data set. Here, we observe that the GRU- and LSTM-based algorithms achieve considerably higher AUC scores than the conventional methods, thanks to their memory structure. Although the LSTM-based algorithms have higher AUC scores in general, we obtain the highest AUC score from GRU-QPSVDD. Moreover, as in the previous experiments, the gradient-based training method provides higher performance compared to the quadratic programming-based method thanks to its learning capabilities.

5) Constraint and Time Complexity Analysis:
In Table II, we compare the performance of LSTM-GSVM under three different scenarios, i.e., using the orthogonality constraint, using the conventional ℓ2 norm regularization constraint and a case without constraint. Note that since LSTM-GSVM provides high AUC scores for all the experiments, we choose it to perform this experiment. We observe that the case with the orthogonality constraint outperforms the other cases. Thus, we use it to improve our detection performance in this article. In addition to this, we measure the training times of the algorithms for all the data sets. In Table III, we observe that the gradient-based algorithms achieve significantly faster training performance compared to the quadratic programming-based methods due to the highly complicated structure of the quadratic programming optimization method.

TABLE II AUC Scores of LSTM-GSVM for the Orthogonality Constraint in (9), ℓ2 Norm Regularization Constraint in (11), and no Constraint Cases

TABLE III Training Times (in Seconds) of the Algorithms. For This Experiment, We Use a Computer That has i5-6400 Processor, 2.7 GHz CPU, and 16 GB RAM
Table III- 
Training Times (in Seconds) of the Algorithms. For This Experiment, We Use a Computer That has i5-6400 Processor, 2.7 GHz CPU, and 16 GB RAM
SECTION V.Concluding Remarks
In this article, we study anomaly detection in an unsupervised framework and introduce LSTM-based algorithms. In particular, we have introduced a generic LSTM-based structure in order to process variable-length data sequences. After obtaining fixed-length sequences via our LSTM-based structure, we introduce a scoring function for our anomaly detectors based on the OC-SVM [6] and SVDD [7] algorithms. As the first time in the literature, we jointly optimize the parameters of both the LSTM architecture and the final scoring function of the OC-SVM (or SVDD) formulation. To jointly optimize the parameters of our algorithms, we have also introduced gradient and quadratic programming-based training methods with different algorithmic merits, where we extend our derivations for these algorithms to the semisupervised and fully supervised frameworks. In order to apply the gradient-based training method, we modify the OC-SVM and SVDD formulations and then provide the convergence results of the modified formulations to the actual ones. Therefore, we obtain highly effective anomaly detection algorithms, especially for time series data, that are able to process variable length data sequences. In our simulations, due to the generic structure of our approach, we have also introduced GRU-based anomaly detection algorithms. Through an extensive set of experiments, we illustrate significant performance improvements achieved by our algorithms with respect to the conventional methods [6], [7], [10] over several different real and simulated data sets.

Appendix A Proof of Proposition I
In order to simplify our notation, for any given w , θ , Xi , and ρ , we denote βw,ρ(h¯i) as Ω . We first show that Sτ(Ω)≥G(Ω) , ∀τ>0 . Since
Sτ(Ω)=≥=1τlog(1+eτΩ)1τlog(eτΩ)Ω
View SourceRight-click on figure for MathML and additional features.and Sτ(Ω)≥0 , we have Sτ(Ω)≥G(Ω)=max{0,Ω} . Then, for any Ω≥0 , we have
∂Sτ(Ω)∂τ=<≤−1τ2log(1+eτΩ)+1τΩeτΩ1+eτΩ−1τΩ+1τΩeτΩ1+eτΩ0
View Sourceand for any Ω<0 , we have
∂Sτ(Ω)∂τ=<−1τ2log(1+eτΩ)+1τΩeτΩ1+eτΩ0,
View Sourcethus, we conclude that Sτ(Ω) is a monotonically decreasing function of τ . As the last step, we derive an upper bound for the difference Sτ(Ω)−G(Ω) . For Ω≥0 , the derivative of the difference is as follows:
∂(Sτ(Ω)−G(Ω))∂Ω=eτΩ1+eτΩ−1<0,
View SourceRight-click on figure for MathML and additional features.hence, the difference is a decreasing function of Ω for Ω≥0 . Therefore, the maximum value is log(2)/τ and it occurs at Ω=0 . Similarly, for Ω<0 , the derivative of the difference is positive, which shows that the maximum for the difference occurs at Ω=0 . With this result, we obtain the following bound:
log(2)τ=maxΩ(Sτ(Ω)−G(Ω)).(74)
View SourceRight-click on figure for MathML and additional features.Using (74), for any ϵ>0 , we can choose τ sufficiently large so that Sτ(Ω)−G(Ω)<ϵ . Hence, as τ increases, Sτ(Ω) uniformly converges to G(Ω) . By averaging (74) over all the data points and multiplying with 1/λ , we obtain
log(2)λτ=maxw,ρ,θ(Fτ(w,ρ,θ)−F(w,ρ,θ))
View SourceRight-click on figure for MathML and additional features.which proves the uniform convergence of Fτ(⋅,⋅,⋅) to F(⋅,⋅,⋅) .

Appendix B Proof of Theorem I
We have the following Hessian matrix of Fτ(w,ρ,θ) with respect to w :
∇2wFτ(w,ρ,θ)=I+τnλ∑i=1neτβw,ρ(h¯i)(1+eτβw,ρ(h¯i))2h¯ih¯Ti,
View SourceRight-click on figure for MathML and additional features.which satisfies vT∇2wFτ(w,ρ,θ)v>0 for any nonzero column vector v . Hence, the Hessian matrix is positive definite, which shows that Fτ(w,ρ,θ) is strictly convex function of w . Consequently, the solution wτ is both global and unique given any ρ and θ . In addition, we have the following second-order derivative for ρ :
∂2 Fτ(w,ρ,θ)∂ρ2=τnλ∑i=1neτβw,ρ(h¯i)(1+eτβw,ρ(h¯i))2>0,
View SourceRight-click on figure for MathML and additional features.which implies that Fτ(w,ρ,θ) is strictly convex function of ρ . As a result, the solution ρτ is both global and unique for any given w and θ .

Let w∗ and ρ∗ be the solutions of (27) for any fixed θ . From the proof of Proposition 1, we have
Fτ(w∗,ρ∗,θ)≥Fτ(wτ,ρτ,θ)≥≥F(wτ,ρτ,θ)F(w∗,ρ∗,θ).(75)
View SourceRight-click on figure for MathML and additional features.Using the convergence result in Proposition 1 and (75), we have
limτ→∞Fτ(wτ,ρτ,θ)≤limτ→∞Fτ(wτ,ρτ,θ)≥limτ→∞Fτ(w∗,ρ∗,θ)=F(w∗,ρ∗,θ)F(w∗,ρ∗,θ)
View SourceRight-click on figure for MathML and additional features.which proves the following equality:
limτ→∞Fτ(wτ,ρτ,θ)=F(w∗,ρ∗,θ).
View SourceRight-click on figure for MathML and additional features.

Appendix C Proof of Theorem II
We have the following Hessian matrix of Fτ(c~,R,θ) with respect to c~ :
∇2c~Fτ(c~,R,θ)=∑i=1n2I(Ωi+Ω2i)+4τΩi(c~−h¯i)(c~−h¯i)Tnλ(1+Ωi)2,
View SourceRight-click on figure for MathML and additional features.where Ωi=eτΨc~,R(h¯i) , which implies vT∇2c~Fτ(c~,R,θ)v>0 for any nonzero column vector v . Thus, the Hessian matrix is positive definite, which shows that Fτ(w,ρ,θ) is strictly convex function of c~ . As a result, the solution c~τ is both global and unique given any R and θ . In addition to this, we have the following second-order derivative for R2 :
∂2 Fτ(c~,R,θ)∂(R2)2=τnλ∑i=1neτΨc~,R(h¯i)(1+eτΨc~,R(h¯i))2>0,
View SourceRight-click on figure for MathML and additional features.which implies that Fτ(c~,R,θ) is strictly convex function of R2 . Therefore, the solution R2τ is both global and unique for any given c~ and θ .

The convergence proof directly follows the proof of Theorem 1.