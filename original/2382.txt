Nowadays vehicle type classification is a fundamental part of intelligent transportation systems (ITSs) and is widely used in various applications like traffic flow monitoring, security enforcement, and autonomous driving, etc. However, vehicle classification is usually used in supervised learning, which greatly limits the applicability for real ITS. This article proposes a semisupervised vehicle type classification scheme via ensemble broad learning for ITS. This presented method contains two main parts. In the first part, a collection of base broad learning system (BLS) classifiers is trained by semisupervised learning to avoid time-consuming training process and alleviate the increasingly unlabeled samples burden. In the second part, a dynamic ensemble structure constructed by trained classifier groups with different characteristics obtains the highest type probability and determine which the vehicle belongs, so as to achieve superior generalization performance than a single base classifier. Several experiments conducted on the pubic BIT-Vehicle dataset and MIO-TCD dataset demonstrate that the proposed method outperforms single BLS classifier and some mainstream methods on effectiveness and efficiency.

SECTION I.Introduction
In recent years, with rapid growth of population and city capacity, the government has paid more attention on intelligent traffic system (ITS). Vehicle type classification is one of the essential and important parts in ITS. There are abundant researches for vehicle type classification over the past several decades [1]–[2][3][4][5][6][7]. All of them can be mainly classified into traditional feature-based vehicle type classification method [8]–[9][10][11][12][13][14][15][16] and newly developed deep neural network (DNN)-based vehicle type classification method [17]–[18][19][20][21][22]. The former mainly extracts apparent feature information of the captured traffic images or videos, then uses traditional machine learning method to do classification. In this case, the difficulty lies in overdependant on prior feature extraction, hand-crafted features might absent enough discrimination for classification. In view of this bottleneck, the more advanced DNN-based approaches have been proposed to satisfy various ITS tasks. Whereas, existing DNN-based vehicle type classification methods usually have significant defects, such as costly training process and complex network structure. In the real world, unlabeled vehicle images can be more easily obtained than labeled ones in surveillance system, so it is labor intensive and unrealistic of the manual labeling with a large-scale vehicle images. To address these issues, a novel semisupervised vehicle type classification scheme via broad learning system (BLS) [23] ensemble is proposed in this article, which trains a collection of different base classifiers by semisupervised learning (SSL) to avoid big labeled burden, and a dynamic BLS classifiers ensemble is constructed to achieve better generalization performance than a single classifier. Our specific contributions are described as follows: 1) we creatively apply BLS model for vehicle type classification to avoid costly training process; 2) we use BLS as the base classifier to form the ensemble model, it can effectively improve classification accuracy; and 3) we propose a dynamic BLS ensemble structure to promote the generalization ability than a single classifier in this task.

The remainder of this article is organized as follows. We briefly introduce basic concepts and related work in Section II, such as the BLS, SSL, and ensemble learning. Section III details the proposed ensemble broad learning scheme for semisupervised vehicle type classification task. Experimental results and discussions are presented in Section IV, conclusion and the future work are prospected in Section V.

SECTION II.Related Work
A. Principle of the BLS
BLS is a novel flat network framework proposed by Chen and Liu [23]. It inherits a majority of features and superiority of the random vector functional-link neural network (RVFLNN) [24], [25]. The network parameters of BLS can be simply randomly selected, instead of DNN consumes too much training time of a large number of parameters. Many researchers made some work of BLS in recent years [26]–[27][28][29][30], this article is also inspired by those work. The architecture of BLS is shown in Fig. 1, it is formulated as follows [23], [30]. Assume the given input training data is X and output vector is Y , respectively. In BLS model, the input samples are first mapped into feature nodes layer by mapping functions as formula (1), that is
Zi=ϕi(XWei+βei),i=1,2,…,N(1)
View SourceRight-click on figure for MathML and additional features.where the weights Wei and the bias term βei are randomly determined with reasonable dimensions. After that, the feature nodes layer is defined as ZN=[Z1,Z2,…,Zn] , here N means groups of feature nodes. All of enhancement nodes are generated and connected by feature nodes ZN , the outputs of the j th group of enhancement nodes are defined by
Hj=φj(ZNWhj),j=1,2,…,m(2)
View SourceRight-click on figure for MathML and additional features.where φj is a nonlinear activation function. The enhancement layer are denoted by Hm=[H1,H2,…,Hm] . The randomly initialized weight matrix Whj are tuned, and the output Y^ of BLS has been obtained by Y^=[ZN,Hm]W=AW . Here A=[Zn,Hm] , and W is the output weights connecting the feature nodes and enhancement nodes to the output layer, it can be inferred by minimizing the objective function
argminWJBLS=||Y−AW||2+λ||W||2(3)
View SourceRight-click on figure for MathML and additional features.where ||Y−AW||2 means the errors, λ||W||2 is a regularization term which constrains the solution of this model, λ is a regularization parameter to balance the error term and the regularization term [30]. BLS avoids time-consuming hyperparameter tuning instead of simple network structure, it achieves fast training and high testing accuracy than many DNN methods.

Fig. 1. - Basic architecture of BLS [23].
Fig. 1.
Basic architecture of BLS [23].

Show All

B. SSL
Generally speaking, a large scale of unlabeled samples can be easily obtained but labeled samples are small number due to high labeling costs. As we all known, SSL is a good way to solve this limited labeling problem [18], [31], [32]. The SSL can be roughly divided into generation-based method, difference-based method, discriminant-based method, and graph-based method. Self-training method is the most straightforward generation-based SSL model where the final model is incrementally trained by additional self-labeled samples given by the model’s own predictions with high confidence. At the conceptual stage, self-training described in the following steps. Denote D={DL,DU}ND , where DL is the labeled data, DU is the unlabeled data, and ND is the size of data. The process of self-training SSL method can be illustrated in Fig. 2.

Fig. 2. - Illustration of self-training SSL framework.
Fig. 2.
Illustration of self-training SSL framework.

Show All

Step 1: Split the labeled instances DL into training set and testing set. Then a classification model is trained by the labeled training data.

Step 2: Use the trained classification model to predict class labels for all unlabeled data instances DU . For all predicted class labels, the ones with the highest probability of being correct are adopted as “pseudolabels.”

Step 3: Concatenate the “pseudolabeled” data with the labeled training data. Retrain the classifier on the combined “pseudolabeled” and labeled training data.

Step 4: Use the trained classifier to predict class labels for the labeled testing instances. Evaluate classifier performance using metric of choice.

(Steps 1–4 can be repeated until no more predicted class labels from Step 2 meet a specific probability threshold, or until no more unlabeled data remains.)

From self-training SSL, the learning process propagates the already existed labels to the rest unlabeled instances by this iterative process.

C. Inspiration of Ensemble Learning
Ensemble learning is a hot topic in machine learning where multiple learners (e.g., classifiers) are combined to achieve better generalization performance than single learner by collective decision [33]–[34][35][36][37]. The reason is that single classifier tends to produce the bias in terms of fixed parameters, but ensemble learning paradigm can greatly reduce such bias.

The basic framework of ensemble learning is shown in Fig. 3. And then, the base learners are combined to produce the final decision by voting or weighted averaging strategy.

Fig. 3. - Basic framework of ensemble learning.
Fig. 3.
Basic framework of ensemble learning.

Show All

Given the input with n examples and m features D={(xi,yi)}(|D|=n,xi∈Rm,yi∈R) . First, K base classifiers (c1,c2,…,ck ) are trained by input samples. Suppose an ensemble learning model φ uses an aggregation function O that aggregates K base classifiers toward predicting a single output as follows:
yi=φ(xi)=O(c1,c2,…,ck)(4)
View SourceRight-click on figure for MathML and additional features.where y^∈R for regression problems and y^i∈Z for classification problems.

In this framework, two main popular used ensemble learning methods are as follows.

The Dependent Method: To emphasize those misclassified training samples from previous models, a classifiers ensemble is incrementally built by iteratively training a new model. AdaBoost [38], [39] is the most well-known dependent algorithm for building an ensemble model. The motivation is to use dependence of various base classifiers, and more weight wrong samples to improve performance.

The Independent Method (Such as Bagging [40] and Random Forest [34]): It combines various prediction results of independently trained models to improve the classification accuracy. In recent years, many researchers developed some new ensemble methods, such as DNN-based ensemble methods [41]–[42][43] and semisupervised ensemble methods [43], [44]–[45][46][47].

Those methods can achieve good performance but they still exist some problems, such as time-consuming base classifier training, ensemble structure is obsolete, and so on.
For these drawbacks, a dynamic ensemble method with BLS base classifiers is presented in this article.

SECTION III.Our Proposed Classification Scheme
A. Architecture
In this section, we describe the proposed BLS ensemble scheme for semisupervised vehicle type classification. It is graphically illustrated in Fig. 4, which includes three main stages: data preparation; training a collection of BLS base classifiers; and testing unlabeled data with the trained model. Firstly, all of the input data are preprocessed from RGB channels to gray-scale and resized into 64×64 pixels for saving computation burden. A large number of unlabeled samples are divided into many groups or subsets {x1,x2,…,xn−1} . Then, a set of BLS base classifiers {BLS1,BLS2,…,BLSn} is trained respectively in a SSL manner. In the training stage, the BLS1 base classifier is trained on a small number of original labeled samples S1 , the labels of unlabeled subsets are predicted by the generated BLS classifiers, and different BLS classifiers are produced by training different input samples so as to improve discrimination of different base classifiers. This is an iterative process given in Table I. Finally, new samples are applied on the trained BLS ensemble model to get the highest probabilities (determine which class of test samples belongs) in the testing stage. This scheme adopts ensemble learning to improve predication accuracy and uses BLS as base ensemble classifiers to reduce hyperparameters tuning and simplify network architecture than DNN-base classifiers. The training of different BLS base classifiers to form ensemble structure is a key part of the proposed scheme, this process is described in Table I.

TABLE I Producing Process of Different BLS Base Classifiers
Table I- 
Producing Process of Different BLS Base Classifiers
Fig. 4. - Graphical illustration of the proposed vehicle type classification scheme.
Fig. 4.
Graphical illustration of the proposed vehicle type classification scheme.

Show All

B. Training of Base Classifiers by SSL
As illustrated in Fig. 4, it is necessary to train different BLS base classifiers by SSL after data preparation. At first, the base classifier BLS1 was trained with a small number of labeled samples S1 . The labels of unlabeled sample subset x1 were predicted by the trained BLS1, the unlabeled x1 is changed to labeled S2 (includes x1 and the corresponding predicted label y^1 ). Then, the subclassifier BLS2 was trained by labeled samples S1 and S2 . As described in Table I, all unlabeled samples can be predicted pseudolabels and all BLS base classifiers are completely trained by this iterative process. Due to different training samples, the obtained base classifiers have large discrimination, that is a good preparation to meet the ensemble learning demand in the next testing stage.

C. Testing With Dynamic Ensemble Model
In the real world, the unlabeled data will continuously increase in ITS. In order to improve generalization capability of real vehicle type classification, we propose a dynamic ensemble model in testing stage as blue shadowed in Fig. 4. It is very different from traditional ensemble learning methods (bagging, boosting and stacking), we adopt a sliding window to perform dynamic updating of ensemble structure. The basic explanation of this dynamic ensemble method is shown in Fig. 5(a) and (b).

Fig. 5. - Graphical explanation of the proposed ensemble methods. (a) Framework of the proposed ensemble structure. (b) Detailed illustration of the dynamic ensemble method.
Fig. 5.
Graphical explanation of the proposed ensemble methods. (a) Framework of the proposed ensemble structure. (b) Detailed illustration of the dynamic ensemble method.

Show All

Fig. 5(a) is a general illustration of the proposed dynamic ensemble method. The top represents different groups of BLS base classifiers (from left to right is three groups to n groups), it is corresponding to the blue shadowed part in Fig. 4. Those G-BLSs are produced by SSL with different number of samples in the training process. Each group contains three BLS base classifiers with different training (marked in purple, red, and blue), the middle red one is the chosen base classifier with best optimal trained parameters. The left purple one and the right blue one in each group represent the subchoice classifiers with different parameters almost close to the best optimal. This structure will improve discrimination of a base classifiers group. Each base classifier produces type probability values whose types are included, and each group produces three sets of probability values. Suppose vehicle type is 4, each base classifier will produce four probability values, each group (G-BLS) will produce 12 probability values, and each ensemble (three G-BLSs) will give 36 probabilities. After that, these 36 probability values are merged as a whole feature (ensemble feature) and sent to the metaclassifier. It learns this ensemble feature, the weight of the metaclassifier is artificially set to learn the feature. At last, this process forms the presented dynamic ensemble model, so as to produce final probability values through softmax and derive which type included.

Now, let us pay a attention to the proposed dynamic ensemble process. As shown in Fig. 5(b), the first row named Ensemble feature 1, it represents the merged features (probabilities) of three BLS base classifiers (BLS1, BLS2, and BLS3). The second row is Ensemble feature 2, it is the features (probabilities) merged of three base classifiers (BLS2 to BLS4). Similar to that, when the number of base classifiers up to n , the group of this n base classifiers is named G-BLS n−2 , etc. In this ensemble structure, when new unlabeled samples are continuously input, the number of BLS classifiers will also increase. By sliding window method, we always choose the last three groups of base classifiers (those base classifiers have relative high learning ability) each time to form an integral new feature. Suppose there is a four categories classification task, each group includes three base classifiers with different training, then 12 sets of probability values are obtained. An ensemble structure contains three groups, the produced 36 probability values can be treated as a new ensemble feature [as shown in Fig. 5(b)]. In general situation, more training samples will produce higher learning performance. We adopt sliding window in this ensemble structure to choose groups of base classifier, the groups of base classifier will update in each time, again and again. With more and more unlabeled data adding in the real ITS, n base classifiers are produce, it will gradually generate (n−2 ) groups. As the rule of choosing high learning performance, the 36 probabilities included in the (n−2 )th ensemble group are integrated as a new feature and provided to a meta classifier. At last, this process forms the presented dynamic ensemble model.

SECTION IV.Experimental Results
A. Data Preparation and Experimental Settings
To demonstrate the effectiveness of our proposed scheme, it is thoroughly evaluated on BIT-vehicle dataset [18] and MIO-TCD classification challenge dataset [48], respectively. Some vehicle image examples are shown in Fig. 6.

Fig. 6. - Some examples in two available public datasets. (a) MIO-TCD classification challenge dataset. (b) BIT-Vehicle dataset.
Fig. 6.
Some examples in two available public datasets. (a) MIO-TCD classification challenge dataset. (b) BIT-Vehicle dataset.

Show All

1) Dataset Preparation:
MIO-TCD classification challenge dataset is a large benchmark traffic dataset acquired at different time and periods by traffic cameras. It contains 648 959 samples and 11 categories (Bus, Car, Articulated truck, Pedestrian, Background, Bicycle, etc.) with highly imbalanced distribution. Total training sample is 519 164 and the training number for each category ranges from 1751 to 260 518.

BIT-Vehicle dataset is highly imbalanced as well, it contains 9850 high-resolution (1600×1200 and 1920×1080 ) vehicle images captured from two cameras at different time and places. All vehicles in this dataset are divided into six categories: Bus, Microbus, Minivan, Sedan, SUV, and Truck. The number are 558, 883, 476, 5922, 1392, and 822, respectively.

In order to avoid highly imbalanced problem, we choose relative balanced samples from two public datasets to built Dataset 1 and Dataset 2, respectively. Dataset 1 is built from MIO-TCD dataset. It contains four balanced categories (Articulated truck, Bus, Car, and Pickup truck). Each type contains 1000 labeled samples, the total labeled samples with four categories are 4000. The unlabeled samples are 28 000, each type of unlabeled samples are 7000. In self-training process, 4000 unlabeled samples (each type is 1000 samples) are added each time. The testing unlabeled samples are 8000, each type contains 2000 samples, respectively. Dataset 2 selects six categories (Bus, Microbus, Minivan, Sedan, SUV, and Truck) from BIT-Vehicle. In training process, the labeled samples for BLS1 are 300 (each type contain 50), the unlabeled samples are 2100 (each type is 350). The unlabeled samples are added 300 each time in self-training process, each type is 50. In testing stage, the testing data number is 300 (each type is 50).

2) Experimental Environment:
This presented method can be easily performed on a laptop, a high-performance computer with GPU is not a necessary. All experiments are conducted on the laptop configured by NVIDIA GeForce GTX 1080Ti, 11G memory, Intel Xeon E5-1650 CPU 3.60-GHz processor, the operating system is Window 10 64 bit, and compiling environment is MATLAB R2016b.

B. Results and Discussion
In this section, we comprehensively evaluate superior performance of the proposed method in many aspects, such as optimal parameter selection, better generalization performance of the presented dynamic ensemble model, and comparative results with some mainstream classification methods. All parameters of comparative methods are tuned to achieve the best performance.

1) Parameter Determination of Base Classifiers:
Optimal parameter selection is important for a classification task. In this section, we give optimal parameters determination of BLS base classifiers. In this experiment, initial base classifier BLS1 are trained by 4000 labeled samples on Dataset 1. After that, 4000 unlabeled samples are putted into BLS1 and produce their pseudolabels. These 8000 labeled samples are merged to train the base classifier BLS2. By this iteration, each base classifier can be trained with 4000 additional samples than the previous one in each time. In the left part of Fig. 7(a), when the initial enhancement node is 1000 of eight BLS base classifiers, the best optimal feature nodes number of eight base classifiers is 120-110-90-100-130-100-130-120, respectively. In the right part of Fig. 7(a), we can see that the best testing accuracy of each base classifier occurs in the enhancement nodes number is 10 000-6000-11 000–10 000-12 000–14 000-14 000–14 000, respectively. For result of Dataset 2 in Fig. 7(b), the optimal feature nodes number is 25-35-45-35-30-35-30-40 and the enhancement nodes number is 40-80-60-140-140-160-180-160, respectively. The testing result validate that those parameters selection can reach to the better classification performance. As can be seen from Fig. 7, the classification accuracy improve gradually with increasing number of base classifiers (e.g., for Dataset 1, the highest accuracy of BLS 1 is 85.73%, while it is 93.93% of BLS 8), we think it is caused by new labeled samples are added in each subsequent base classifier training, new information of samples will produce better generalization result.

Fig. 7. - Parameters determination of all base classifiers in training stage. (a) Optimal parameters selection on Dataset 1. (b) Optimal parameters selection on Dataset 2.
Fig. 7.
Parameters determination of all base classifiers in training stage. (a) Optimal parameters selection on Dataset 1. (b) Optimal parameters selection on Dataset 2.

Show All

2) Performance of the Dynamic Ensemble Model:
This part gives performance of the proposed dynamic ensemble model applied on Datasets 1 and 2, respectively. The left part of Fig. 8 shows testing accuracy with different single base BLS (BLS1–BLS8) trained by the best optimal parameters [here selected BLS is the middle red one of G-BLS shown in Fig. 5(a)]. The right subgraphs show testing accuracy with different dynamic ensembles (ensembles 1–6). As shown in Fig. 5(b), each ensemble is constructed by the probabilities of three G-BLSs. According to sliding window backup moving, finally eight base BLS classifiers will produce the dynamic ensemble of ensemble 6. For example, ensemble 1 is constructed by G-BLS1, G-BLS2, and G-BLS3. By analogy, ensemble 6 is constructed by G-BLS6, G-BLS7, and G-BLS8.

Fig. 8. - Performance validation of a single classifier and our proposed dynamic ensemble model. (a) Results of each single classifier and different classifiers ensemble on Dataset 1. (b) Results of each single classifier and different classifiers ensemble on Dataset 2.
Fig. 8.
Performance validation of a single classifier and our proposed dynamic ensemble model. (a) Results of each single classifier and different classifiers ensemble on Dataset 1. (b) Results of each single classifier and different classifiers ensemble on Dataset 2.

Show All

In Fig. 8(a), the left highest accuracy is BLS8 (93.93%), and the right highest accuracy is Ensemble 6 (94.63%) on Dataset 1. In Fig. 8(b), the left highest accuracy is BLS8 (93.13%), and the right highest accuracy is ensemble 6 (93.23%) on Dataset 2. These results show that the proposed dynamic ensemble model outperforms higher accuracy than any single classifier.

3) Multiclass Imbalanced Data Classification:
MIO-TCD is a serious imbalanced vehicle dataset, this situation is usually existed in the real traffic surveillance. In this section, we will deal with the situation of multiclass imbalanced data classification. All of eight vehicle categories (Articulated truck, Bus, Car, Pickup truck, bicycle, motorcycle, single_unit_truck, work_van) are used in this experiment. Each four categories (articulated_truck, bus, Car and Pickup truck) are randomly select 10 000 from MIO-TCD, respectively. Other four categories (bicycle, motorcycle, single_unit_truck and work_van) are performed by data augmentation [49] to obtain 10 000 samples for each type. All of those images are downsampled to 64×64 pixels and gray-scaled for saving computation. This new dataset is Dataset 3. It is partitioned into a training set containing 64 000 images and a testing set including 16 000 images. The labeled samples for BLS1 are 16 000 (each type is 2000). The total unlabeled samples are 56 000 (each type is 7000). In self-training process, the unlabeled samples are added 8000 (each category is 1000) each time to predict the corresponding pseudolabels, so as to produce base classifiers BLS2–BLS8. The testing samples are 16 000 (each type is 2000). In the left of Fig. 9(a), the initial enhancement nodes is set as 1000, optimal feature nodes of eight base BLS classifiers is 120-120-110-90-90-100-90-90, respectively. Then, the optimal enhancement nodes of eight BLS base classifiers is 14 000–10 000-14 000–14 000-12 000–16 000-16 000–18 000 as indicated in the right of Fig. 9(a). The comparative performance of different ensembles and single base classifier are clearly shown in Fig. 9(b). The left part shows the base classifier BLS8 achieves the highest at 92.68%, it outperforms other single base classifiers. The right part of Fig. 9(b) reflects that ensemble 6 (contains three groups from G-BLS6 to G-BLS8) reaches the highest accuracy at 93.39%.

Fig. 9. - Performance evaluated on Dataset 3 (multiclass imbalanced data). (a) Optimal parameter selection on multiclass unbalanced data classification (eight vehicle types). (b) Comparative results of single different base classifier and different ensembles.
Fig. 9.
Performance evaluated on Dataset 3 (multiclass imbalanced data). (a) Optimal parameter selection on multiclass unbalanced data classification (eight vehicle types). (b) Comparative results of single different base classifier and different ensembles.

Show All

Above experiments prove that the proposed dynamic ensemble model effectively improves superior generalization ability than any single base BLS classifier.

C. Comparison With Other Mainstream Methods
We further compare the presented method with other state-of-the-art methods, such as sparse automated encoder (SAE), deep belief network (DBN), convolutional neural network (CNN), support vector machine (SVM), SL-BLS, and SL-ELM proposed in [26]. SAE, DBN, and CNN are conducted on MATLAB (deep learning toolbox). The comparative result is given in Table II. Note that all of the comparative methods are tuned with optimal parameters as follows.

SAE: Structure is 2000-1000-200-100, 100-50-20-10 and 2500-1200-600-200-50 on Datasets 1–3, respectively. All of the learning rate is 0.1, the epoch is 100-100-150 on Datasets 1–3.

CNN (Layer is 5): The learning rate is 0.1, epoch is 100-100-120 on Datasets 1–3, respectively.

DBN: Structure is 2000-1000-200-100, 200-100-100-20 and 2000-1000-100-500 on Datasets 1–3, respectively, the learning rate on three datasets is 0.1; the epoch is 100-100-120 on three datasets, respectively.

VGG16: The epoch is 40-30-60 on three datasets, respectively. And the learning rate is 0.1.

SVM (RBF Kernel): Parameter c is 0.1-0.2-0.1 and the parameter g of kernel function is 0.2-0.1-0.2 on Datasets 1–3, respectively.

TABLE II Comparative Result of the Presented Method and Other Mainstream Methods
Table II- 
Comparative Result of the Presented Method and Other Mainstream Methods
In addition, structure of SL-ELM is 4000-100-8000 on Datasets 1–3, respectively. For SL-BLS, the feature nodes are 100-45-70 and enhancement nodes are 14 000-110-15 000 on Datasets 1–3, respectively.

The performance of each method is compared under the same data samples. As can be seen from Table II, the training times of SVM and DNN-based methods (SAE, CNN, DBN, and VGG-16 [50]) are too much long than other methods. SS-ELM and SS-BLS take on faster training time on three datasets. For the proposed method, although it takes a bit of more training time than those two SS methods, but its accuracy can obviously achieve the highest ones (94.63%, 91.66%, and 93.39% on evaluated datasets), respectively.

All in all, it is not difficult to see that accuracy of the proposed method is surprisingly outperforms other main-stream methods, and its efficiency is much higher than DNN-based methods. Furthermore, under the same number of labeled training samples or unlabeled samples, the proposed method can meet the requirement of semisupervised classification with superior accuracy and acceptable training time. Above experiment results show that the presented method has better classification performance.

SECTION V.Conclusion and Future Work
To our knowledge, this article is the first report to employ ensemble broad learning scheme for semisupervised vehicle type classification. This scheme faced increasingly unlabeled problem in the real world, it can reduce annotation burden and surprisingly improve classification accuracy. What is more, a dynamic ensemble model is presented in this scheme to greatly increase generalization capability. The experiments conducted on the benchmark datasets proved that the proposed method achieved high efficiency and better effectiveness than many other popular methods. In future work, we will further explore the relationship of unlabeled samples and base classifier number, ensemble strategy, even extend this method to many complicated applications.