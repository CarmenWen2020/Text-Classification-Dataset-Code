Essential decision-making tasks such as power management in future vehicles will benefit from the development of artificial intelligence technology for safe and energy-efficient operations. To develop the technique of using neural network and deep learning in energy management of the plug-in hybrid vehicle and evaluate its advantage, this article proposes a new adaptive learning network that incorporates a deep deterministic policy gradient (DDPG) network with an adaptive neuro-fuzzy inference system (ANFIS) network. First, the ANFIS network is built using a new global K-fold fuzzy learning (GKFL) method for real-time implementation of the offline dynamic programming result. Then, the DDPG network is developed to regulate the input of the ANFIS network with the real-world reinforcement signal. The ANFIS and DDPG networks are integrated to maximize the control utility (CU), which is a function of the vehicle’s energy efficiency and the battery state-of-charge. Experimental studies are conducted to testify the performance and robustness of the DDPG-ANFIS network. It has shown that the studied vehicle with the DDPG-ANFIS network achieves 8% higher CU than using the MATLAB ANFIS toolbox on the studied vehicle. In five simulated real-world driving conditions, the DDPG-ANFIS network increased the maximum mean CU value by 138% over the ANFIS-only network and 5% over the DDPG-only network.

SECTION I.Introduction
Recent advances in artificial intelligence (AI) and informatics have significantly promoted the development of connected and autonomous vehicles [1]. AI techniques will be equipped to future vehicles for enhancing safety and achieving the optimal energy efficiency. On the contrary, electrification brings another revolution to the automotive industry. By harnessing conventional thermal propulsion with electric drives, the hybridized vehicles can achieve high efficiency and low emissions simultaneously. Hybrid electric vehicles, as a mainstream ultralow emission solution, will account for more than 60% of the world automotive market by 2030 according to predictions from the International Energy Agency [2].

The power management systems (PMSs) regulate the energy flows between the power units (e.g., engine and battery) within the hybrid vehicle. The optimization of energy efficiency in the PMS is one of the most challenging decision-making tasks because of the uncertainties in real-world driving and constraints in operations [3], [4]. PMSs are expected to be optimized such that vehicles comply with the stricter regulations in fuel consumption and emissions. Taking Europe as an example, the new European driving cycle (NEDC) for road vehicles has been replaced by the worldwide-harmonized light-duty testing cycle (WLTC), where an increasing number of transient operation points is included to evaluate energy efficiency and emissions [5]. New legislations on examining real-world driving emissions (RDEs) have been enforced [6]–[7][8], which bring more uncertain transient operation conditions to be considered for vehicle development.

Offline optimization of the power management strategy under testing cycles is essential to help automakers comply with legislations. Offline optimization determines the optimal settings that achieve the maximum energy efficiency [9], where dynamic programming (DP) is considered as the benchmark method. However, DP requires large computational efforts and, therefore, is not feasible to be implemented in real-time control directly [10], [11].

Implementing the DP results in real-time control is critical to enable optimal power management [12]. This will be achieved by finding the optimal parameters in power management control models that achieve the minimum mean square error (MSE) with the DP results. Meta-heuristic algorithms, e.g., particle swarm optimization (PSO) [13]–[14][15][16] and genetic algorithms (GAs) [17], [18], have been developed to minimize the MSEs between model data and testing data. The learning performance heavily depends on the data used in training and validation. Khayyam and Bab-Hadiashar [18] modeled a fuzzy logic power management controller using five groups of datasets, while the size of each dataset is 30k. Tian et al. [19] used 1120 datasets to train a fuzzy power management controller, while the size of each dataset is more than 4k. Xing et al. [20] used 10k data to train recurrent neural networks for driver behavior prediction. These articles demonstrated model learning based on a huge amount of data, but such approaches are time consuming and may cause overfitting.

Building precise and robust control models with limited source data is challenging for knowledge implementation of the offline optimization results. Ideally, the optimal model built from knowledge implementation will use the data collected from the test cycle (e.g., WLTC and RTS-95) [8]. Cross-validation is a statistical method to estimate the robustness of machine-learning models [21]. It divides the source data into the training dataset and the validation dataset. The K-fold cross validation is widely used for learning with labeled data [21]. Lv et al. [22] implemented a fivefold cross validation to train a neural network for driver intention prediction. Zuo et al. [23] developed a fivefold method to train a fuzzy model in solving regression problems. Tivive used a tenfold method to train a convolutional neural network for pattern recognition [24]. However, using K-fold cross-validation methods for power management has not been reported.

Online optimization is necessary for hybrid powertrain control because only limited future-trip information is available in real-world driving. Model predictive control (MPC) has been applied in online optimization of energy management strategies [25]–[26][27][28]. It works on a rolling basis to generate the optimal control policy based on the vehicle model [29]. Because the vehicle model is normally fixed with offline calibrated results, MPC is less adaptive in noncalibrated conditions, e.g., real-world driving [30], [31].

Reinforcement learning (RL) is an emerging and promising technology for online optimal control [32]. It is a plant-model-free method based on Bellman’s theory [33], which updates its knowledge based on reinforcement information to fulfill online optimization in unknown environments [10]. The effectiveness of RL has been demonstrated in various vehicle-control-related applications [34]. Remarkable improvements in vehicle energy efficiency have been achieved by RL methods, e.g., Q-learning [35], deep Q-learning [36], double Q-learning [37], and multiple-step Q-learning [38]. Most research on RL-based power management control focus on learning from scratch [39], [40]. However, this approach requires a long time to develop a proper control policy, so it is not practical in real-world applications [41].

The learning speed and performance of the RL can be theoretically improved if the offline optimization knowledge can be translated into online learning. This can be formulated as a transfer learning paradigm, which leverages the previously acquired knowledge (e.g., offline optimization) to improve the efficiency and accuracy of learning in another domain (e.g., real-world driving). The transferred knowledge include characteristics [42], feature representations [43], model parameters [44], and relational information [45]. Recently, adaptive neuro-fuzzy inference systems (ANFIS) have elaborated superiorities in transfer learning as they incorporate heuristic human knowledge with data for accurate modeling of uncertainties [46]–[47][48]. There has been a big volume of research on applying transfer learning to solve classification problems [49]. However, employing transfer learning to improve the real-time control performance is scarcely reported.

To enable knowledge implementation and transfer in power management of the plug-in hybrid electric vehicle (PHEV), this article proposes an adaptive learning network, which incorporates a deep deterministic policy gradient (DDPG) network [50] with an ANFIS network. Experimental evaluations are conducted to show how advanced artificial neural networks and learning systems help improve control performance in real-world driving. This work has two main contributions:

A new method named global K-fold fuzzy learning (GKFL) is proposed to build the ANFIS network that is used to implement the knowledge learned from offline DP to real-time control.

The DDPG network is combined with the ANFIS to optimize the control performance in real-world driving with the capability of online knowledge reinforcement.

The rest of the article is organized as follows. Section II describes the energy flow within a PHEV PMS, and Section III proposes the adaptive learning network for power management. Experimental evaluations are conducted in Section IV. Conclusions are summarized in Section V.
SECTION II.Energy Flow Optimization and Power Management
This section formulates the optimization problem in power management based on energy flow modeling. Afterward, a baseline PMS for the PHEV is introduced.

A. Energy Flow Optimization
In general, a PHEV consists of two power units (e.g., battery package and engine-generator) to match the power demand for vehicle operations. The power flows of power units are shown in Fig. 1, where Pdem is the power demand for vehicle operation; Pppu is the power output from the battery pack; the battery is discharging when Pppu>0 , and is charging when Pppu<0 ; Papu is the power output from the engine generator. The battery package works as the primary power unit of the PHEV. The engine generator is the alternative power unit for maintaining the battery’s state-of-charge (SoC) to ensure longer driving distance. The PHEV has three working modes including the pure electric drive mode, the hybrid power mode, and the engine-generator power mode. When the battery SoC is relatively high, the powertrain is solely driven by the battery (e.g., the pure electric drive mode). When the battery SoC is relatively low, the engine-generator works at the rated power to guarantee that the battery is not over-discharged (e.g., the engine-generator power mode). Otherwise, both the battery package and engine generator supply power to the PHEV (e.g., the hybrid power mode).

Fig. 1. - Power flow of a hybrid powertrain system.
Fig. 1.
Power flow of a hybrid powertrain system.

Show All

1) Energy Flow Model:
From the perspective of energy transmission, the power flow in the PHEV is expressed as
Pdem(t)=Pppu(t)+Papu(t).(1)
View SourceRight-click on figure for MathML and additional features.

Both power units have energy losses when generating electricity. The power losses of the battery and the engine generator can be modeled by
Pppu_loss(t)=Rloss(SoC)⋅Ibatt(ubatt(t))2Papu_loss(t)=m˙f(uegu(t))⋅Hf−Papu(t)}(2)
View SourceRight-click on figure for MathML and additional features.where Rloss is the battery internal resistance; Ibatt is the battery current; ubatt is the battery control signal; uegu is the engine-generator control signal; m˙f is the fuel mass flow rate; and Hf is the heat value of gasoline.

Achieving maximum vehicle energy efficiency is the primary objective for power management. The energy efficiency of the PHEV is defined as
η=∑tτt=t0Pdem(t)⋅Δt∑tτt=t0Pdem(t)⋅Δt+∑tτt=t0Ploss(t)⋅Δt(3)
View SourceRight-click on figure for MathML and additional features.where η is the vehicle energy efficiency; t0 and tt are the starting and terminal time of a driving cycle; Δt is the sampling time; and Ploss(t)=Pppu_loss(t)+Papu_loss(t) is the total power loss.

Maintaining the battery SoC is a critical constraint to be met in power management. The battery SoC at time tl is
SoC(tl)=SoC(tl−1)−Ibatt(ubatt(tl))Qbatt⋅Δt(4)
View SourceRight-click on figure for MathML and additional features.where Qbatt and Ibatt are the capacity and current of the battery, respectively.

2) Energy Flow Optimization Problem Formulation:
To achieve the maximum vehicle energy efficiency while maintaining the battery SoC, a control utility (CU) function is defined by introducing p(SoC(t)) to the denominator of (3) as
U=∑tτt=t1Pdem(t)⋅Δt∑tτt=t1Pdem(t)⋅Δt+∑tτt=t1(Ploss(t)⋅Δt+p(SoC(t)))(5)
View SourceRight-click on figure for MathML and additional features.where p(SoC(t))=β⋅eα⋅(SoC(t)−SoC+−SoC−) is the penalty function that is defined based on the degradation of the battery SoC [36]; and SoC+ and SoC− are the higher and lower boundaries of battery SoC for the hybrid power mode.

The optimization problem can be formulated by
max[ubattuegu]U(ubat,uegu,Pdem) s.t. ⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪Pppu_loss(t)=Rloss(SoC)⋅Ibatt(ubatt(t))2Papu_loss(t)=m˙f(uegu(t))⋅Hf−Papu(t)SoC(tl)=SoC(tl−1)−Ibatt(ubatt(tl))Qbatt⋅ΔtSoC−<SoC(t)<SoC+t1≤t≤tτ(6)
View SourceRight-click on figure for MathML and additional features.where ubatt=[ubatt(t1),ubatt(t2),…,ubatt(tτ)] and uegu=[uegu(t1),uegu(t2),…,uegu(tτ)] are vectors of control signals in a driving cycle; and Pdem=[Pdem(t1),Pdem(t2),…,Pdem(tτ)] is a vector of power demands in a driving cycle.

B. PMS of the PHEV
For the series PHEV, power from the battery and the engine generator yields
Pdem(t)=ubatt(t)⋅Pppu_max+uegu(t)⋅Papu_max.(7)
View SourceRight-click on figure for MathML and additional features.

The battery command ubatt can be derived from (7) by
ubatt(t)=Pdem(t)−uegu(t)⋅Papu_maxubatt(t)⋅Pppu_max(8)
View SourceRight-click on figure for MathML and additional features.where Pppu_max and Papu_max are the maximum powers that can be provided by the battery and engine generator, respectively.

1) Double-Input Single-Output Energy Management System:
Typically, energy management of series plug-in hybrid powertrains uses power demand and battery SoC as system inputs to determine the control command of the engine-generator unit uegu(t) [25], [37], [38]
uegu(t)=M(Pdem(t),SoC(t),C)(9)
View SourceRight-click on figure for MathML and additional features.where M(⋅) is a nonlinear mapping function that projects the inputs of Pdem(t) and SoC(t) to the relevant control command uegu(t) ; C is a vector of parameters for model M(⋅) . Then, the battery control command ubatt(t) can be calculated using (8).

2) Takagi–Sugeno Fuzzy Inference Network for Energy Management:
The energy management strategy is based on a Takagi–Sugeno model, as shown in Fig. 2. This strategy is easy to be implemented with data-driven learning [51]. The battery SoC and power demand from the PHEV are gathered as an input vector x=[SoC(t),Pdem(t)]T in the input layer, and the control command uegu(t)=y is generated in the output layer. The output y is calculated in three hidden layers based on x .

Fig. 2. - Takagi–Sugeno fuzzy model for energy management.
Fig. 2.
Takagi–Sugeno fuzzy model for energy management.

Show All

The first hidden layer fuzzifies the inputs with triangular membership functions, F1,i and F2,i , such that
F1,i(x1,v1,i)=max(min(x1−v1,i(1)v1,i(2)−v1,i(1)v1,i(3)−x1v1,i(3)−v1,i(2)),0)F2,j(x2,v2,j)=max(min(x2−v2,j(1)v2,j(2)−v2,j(1)v2,j(3)−x2v2,j(3)−v2,j(2)),0)⎫⎭⎬⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪(10)
View SourceRight-click on figure for MathML and additional features.where i=1,2,…,n ; j=1,2,…,m ; x1 and x2 are the first and second element of x ; F1,i is the i th membership function for the first input; F2,j is the j th membership function for the second input; and v(k) , k=1 ,2,3, is the k th element of v .

The second hidden layer connects the outputs of the input membership functions based on fuzzy rules. Each fuzzy rule applies the following linguistic logic:
 If x(1) is F1,i(x(1),v1,i) and x(2) is F2,j(x(2),v2,j) then y is L(x,ai,j)(11)
View SourceRight-click on figure for MathML and additional features.where L(x,ai,j) is the output membership function that maps the x and ai,j to a constant [11]; and ai,j is a scaling factor.

The third hidden layer uses a vector of weighting values W=[w1.1,w1.2,…,w1.n,w2.1,…,w2.n,…,wm.1,…,wm.n] to scale the outputs of fuzzy rules
y=∑j=1m∑i=1n{min(F1,i,F2,j)⋅L(x,ai,j)⋅wi,j}(12)
View SourceRight-click on figure for MathML and additional features.where wi.j∈[0,1],i=1,2,…,n,j=1,2,…,m .

For the generic controller, the model parameter vector C is
C=[V1,V2,A,W](13)
View SourceRight-click on figure for MathML and additional features.where V1=[v1,1,v1,2,…,v1,n] and V2=[v2,1,v2,2,…,v2,m] are the parameter vectors of the inputs membership functions; A1=[a1,1,…,a1,n,a2,1,…,a2,n,…,am,n1,…,am,n] is a vector of parameter in the output membership functions. It should be noticed that the vector C cannot be determined using conventional gradient-based algorithms because the ANFIS model includes components that are not derivable. Therefore, C should be obtained by using the derivative-free algorithm.

SECTION III.Adaptive Learning Network for Power Management
This section introduces the development procedure for the adaptive learning network, as shown in Fig. 3. This procedure incorporates a DDPG network with an ANFIS network to enable knowledge implementation and transfer for real-time energy management with two stages. In Stage 1, a new GKFL method is developed to implement the offline optimization results in the ANFIS-based generic control model. In Stage 2, the DDPG network is developed for transfer learning, which regulates the inputs of the ANFIS network with the reinforcement signals from real-world driving. Integration of the development in both stages ensures robust and efficient online learning that can be conducted in the onboard power management controller. Therefore, the vehicle’s CU can be continuously improved in real-world driving.

Fig. 3. - Adaptive learning network in power management of the plug-in hybrid vehicle.
Fig. 3.
Adaptive learning network in power management of the plug-in hybrid vehicle.

Show All

A. GKFL for Implementation of Offline Optimization Knowledge in Real-Time Control
The GKFL method is proposed to implement the offline optimization knowledge in a control model Mkf . It conducts k-fold cross validation for model learning with the offline DP results to achieve the maximum CU value, U(Mkf) , in real-time.

Remark 1:
The proposed GKFL method can simultaneously determine the optimal energy management model Mkf and the optimal setting, κ∗ , by conducting a global search with all possible κ values (e.g., κ=2,3,4,…,10 ). Therefore, the study on GKFL is important to contribute know-how on selection of K values for cross-validation-based model learning because it is scarcely studied in this field. The working procedure of the proposed GKFL is illustrated in Fig. 4.

Fig. 4. - Procedure of GKFL for implementing the offline optimization result into real-time control.
Fig. 4.
Procedure of GKFL for implementing the offline optimization result into real-time control.

Show All

After an initialization of the K value by setting κ=2 , a rotational model learning process will be conducted by repeating Steps 1–4:

The dataset of the offline DP results under a given driving cycle, D=[x,y]T , is divided into κ folds randomly, i.e., D1,D2,…,Dκ , with similar data size.

The parameter vector C in fuzzy model M is optimized using the derivative-free algorithm (e.g., Generic Algorithm) in κ rounds. For each round, Drtrn=[D1,D2,…,Dr−1,Dr+1,…,Dκ] (r=1,2,3,…,κ ) is used for training and Drtst=Dr is used for testing.

A fuzzy model Mκ(.,.,Cκ) is selected based on the results from Step 2, and it has the minimum cross-validation MSE (CV-MSE)
CVMSE(κ)=1κ⋅∑r=1κ∑τ′t=1(Mr(xrtst(t))−yrtst(t))2τ′(14)
View SourceRight-click on figure for MathML and additional features.where Mr(xrtst(t)) is the model output at time t using the model learned from training data Drtrn during round r ; and xrtst(t) and yrtst(t) are, respectively, the model input and output at time t in testing dataset Drtst during round r .

The fuzzy model Mκ is implemented for real-time energy management control under the given driving cycle. The CU value, U(Mκ) , is collected as an indicator to select the optimal learning result.

Once the termination term is met (i.e., κ>10 ), the rotational process stops. Thereafter, the optimal setting κ∗ is extracted, and the optimal model Mkf is generated to satisfy
U(Mkf)=U(Mκ∗)≥U(Mκ),κ∈[2,10](15)
View SourceRight-click on figure for MathML and additional features.where U(Mκ∗) is the CU value that the vehicle achieved under a driving cycle with the optimal fuzzy model Mκ∗ (κ=κ∗ ).

B. DDPG Network for Adaptive Knowledge Transfer
To make the ANFIS network more adaptive to new driving conditions that has not been used for model learning, the input signals of the ANFIS need to be regulated. This will be achieved by developing a nonlinear input space mapping network that has the capability of online RL with real-time feedback.

Remark 2:
Online adaptive knowledge transfer is enabled by the DDPG network, which has the capability of self-learning based on real-time reinforcement signals. As illustrated in Fig. 5, the DDPG network will allow the generic model Mkfgen to adapt the new driving scenarios by regulating the power demand x2 to a corresponding level φ .

Fig. 5. - DDPG network for adaptive knowledge transfer.
Fig. 5.
DDPG network for adaptive knowledge transfer.

Show All

The DDPG network uses a deep critic network Q to optimize the deep actor-network Φ online. The actor-network Φ maps a deterministic action execution policy, and a noise NE is added to enable the exploration, such that
φ(t)=Φ(x(t)|θΦ)+NE(16)
View SourceRight-click on figure for MathML and additional features.where φ(t) is the regulated state variable at time t ; x(t) is a vector of inputs at time t ; θΦ is a vector of parameters in the neural network Φ ; and NE∼N(0,σE2) is the exploration noise generated by a random number generator in MATLAB, and it enables more global exploration at the early stage of the learning process and guarantee convergence of the control policy. The derivation value σE of the exploration noise decreases along with the online learning progress [52]
σE=σ0−Δσ⋅E(17)
View SourceRight-click on figure for MathML and additional features.where σ0 is the initial variation value; Δσ is the variance decay rate; and E is the number of epis ParaFirstLine-Indodes that the learning agent experienced in online learning. Once the power demand signal is regulated by (16), the control command is represented by
uegu(t)=∑j=1m∑i=1n{min(F1,i(SoC(t)),F2,j(φ(t)))⋅L([SoC(t),φ(t)]T,ai,j)⋅wi,j}.(18)
View SourceRight-click on figure for MathML and additional features.

Subsequently, the vehicle’s fuel mass flow rate and battery’s open-circuit voltage (for battery SoC estimation) are measured and used as feedback to the controller. To guarantee the convergence of the algorithm, the reinforcement signal at each time step is defined based on (6)
r(t)=−(Ploss(t)⋅Δt+β⋅eα⋅(SoC(t)−SoC+−SoC−)).(19)
View SourceRight-click on figure for MathML and additional features.

Multiple signals are restored in a replay buffer R , including the real-time state signals x(t)=[SoC(t),Pdem(t)]T , the action signal φ(t) , and the reinforcement signal r(t) . In each time interval, a minibatch of N transitions (xi,φi,ri,xi+1) is created by random sampling from R to train both the actor and critic networks.

The parameter vector in the critic network, θQ , is optimized by minimizing the loss, L , such that
L=∑Ni=1(Q^i−Q(xi,φi|θQ))N(20)
View SourceRight-click on figure for MathML and additional features.where Q(xi,φi|θQ) is a deep critic network with parameter vector θQ , which calculates the merit function value using vehicle state, xi , and action, φi . Based on Bellman’s theory [33], the estimated merit function value Q^i is calculated with the reinforcement signal, such that
Q^i=ri+γ⋅Q′(xi+1,Φ′(xi+1|θΦ′)|θQ′)(21)
View SourceRight-click on figure for MathML and additional features.where Q′ with its parameter vector θQ′ is an estimated target critic network; Φ′ with its parameter vector θΦ′ is an estimated target actor network; and γ is the learning rate.

The parameter vector in actor network, θΦ , is optimized based on the deterministic policy gradient theory [52], such that
θΦ←θΦ+α⋅∇θΦJ(Φ(x|θΦ))(22)
View SourceRight-click on figure for MathML and additional features.where ∇θΦJ(Φ(x|θΦ)) is the policy gradient, i.e., the gradient of vehicle performance J using the parameter vector θΦ in the actor network Φ ; α is the learning rate. The policy gradient can be estimated using the minibatch data as
∇θΦJ(Φ(x|θΦ))=∑Ni=1[∇θΦΦ(xi|θΦ)⋅∇Φ(xi|θΦ)Q(xi,Φ(xi|θΦ)|θQ)]N(23)
View SourceRight-click on figure for MathML and additional features.where ∇θΦΦ(xi|θΦ) is the gradient of the actor network Φ with regard to θΦ as variables; and ∇Φ(x(i)|θΦ)Q(xi,Φ(xi|θΦ)|θQ) is the gradient of the critic network Q with regard to Φ(xi|θΦ) as variables. The pseudo-code for the DDPG algorithm for knowledge transfer is shown in Fig. 6.

Fig. 6. - Pseudocode of adaptive knowledge transfer with DDPG.
Fig. 6.
Pseudocode of adaptive knowledge transfer with DDPG.

Show All

SECTION IV.Experimental Evaluations
The experimental evaluations were conducted on a plug-in hybrid passenger car, which has a 36.6-kW engine generator with a 0.65L engine, a 125-kW electric motor, and a 360-V high-volt battery with the capacity of 22 kWh. The vehicle is modeled using the Simulink Powertrain Toolbox based on the dynamometer data. The inputs of the vehicle model are the desired vehicle speed and the power management control signals. The model outputs are the battery SoC, battery voltage/current, fuel mass flow rate, and power demand for vehicle operation. The key parameters are listed in Table I.

TABLE I Key Parameters for Vehicle Plant Modeling
Table I- 
Key Parameters for Vehicle Plant Modeling
Both offline software-in-the-loop (SiL) and online hardware-in-the-loop (HiL) testing platforms were used in experimental evaluations. The offline evaluation was conducted in MATLAB 2020a on a PC with an i7 CPU and a 16-GB RAM. A Speedgoat real-time target machine (Intel Core i7 2.5 GHz with 4 GB RAM) is used for online HiL testing, as shown in Fig. 7. The control prototype and the real-time vehicle plant model are compiled on a host PC, downloaded onto the Speedgoat target machine through Ethernet, and physically connected using controller area networks (CANs). The proposed DDPG-ANFIS network was implement in the prototype controller with two steps: 1) the network was built with MATLAB/Simulink with its inputs/outputs connected to the CAN interface blocks that are provided in the Simulink real-time block set and 2) the Simulink model was compiled into executive code with the Simulink code generator for real-time control. The prototype controllers are developed in the same way with the ANFIS-only network, DDPG-only network, and a lookup table system (built with DP result).

Fig. 7. - Online HiL testing platform.
Fig. 7.
Online HiL testing platform.

Show All

A. Knowledge Implementation From Offline Optimization
Experimental evaluation of the knowledge implementation performance was conducted under the WLTC, which is currently used for vehicle certification. The benchmark power management result was obtained by DP, which is a dataset containing 1800 data pairs. A total of 70% of DP data was used for learning and 30% was for verification. The advantage of the proposed GKFL method was demonstrated by comparing the model learning performance with the conventional method (default in MATLAB ANFIS toolbox). The conventional method used the whole learning data, whereas GKFL further divided the learning data into κ folds (κ=2,3,…,10 ) of training and validation data pairs. To ensure the effectiveness of learning results, GA and PSO algorithms were both used for model learning and the results are compared in Tables II and III.

TABLE II Knowledge Implementation Performance With GA

TABLE III Knowledge Implementation Performance With PSO
Table III- 
Knowledge Implementation Performance With PSO
Three performance metrics were used to testify the robustness of the proposed GKFL method including the learning performance, the verification MSE (Veri. MSE), and the real-time CU value. Training MSEs (Train. MSEs) and minimum CV-MSE (Min. CV-MSE) were used to evaluate the learning performance of the conventional method and the GKFL methods (with different κ values), respectively. The experimental evaluation suggests that the optimal power management control model under the WLTC can be obtained by the proposed GKFL method when κ=9 . It achieves the minimal Veri. MSE and highest CU values. This indicates that the result of fuzzy model learning using both GA and PSO is robust. The number of folds for cross-validation-based statistical learning should be carefully chosen to robustly achieve the optimal result. In this study, fuzzy learning based on the widely used fivefold cross validation can achieve acceptable learning performance, which achieves higher CU value compared to the conventional method. However, the performance using another widely used method, i.e., tenfold cross validation, is not as good as expected because it achieves a less CU value than the conventional method.

By implementing the control models in the HiL testing platform, the real-time performance of the fuzzy model obtained by GKFL-9-GA (κ=9 , using GA for learning) is compared in Fig. 8 with both benchmark strategy (obtained by DP) and the model obtained by conventional Conv-PSO (ANFIS toolbox with PSO). The PMS generates control commands for the engine generator unit (EGU) as shown in Fig. 8(b). It is to satisfy the power demand in Fig. 8(a) while maintaining the battery SoC at a certain level as shown in Fig. 8(c). It optimizes the vehicle energy efficiency by minimizing fuel consumption. The power management real-time control model obtained by GKFL-9-GA achieves 2.8% lower fuel consumption than using the benchmark strategy. However, this approach achieves 3.4% lower CU value because it has 5.9% less remaining battery SoC. The fuzzy model obtained by GKFL-9-GA achieves the highest CU value compared to other learning methods. The fuzzy model is chosen to implement the offline optimization knowledge for the rest of this article.


Fig. 8.
Real-time performance under WLTC. (a) Power demand for vehicle operation. (b) EGU control command. (c) battery SoC. (d) Fuel consumptions.

Show All

B. Knowledge Transfer Across Different Testing Cycles
The experimental evaluation on knowledge transfer performance is conducted under the RTS-95 cycle, which is recommended by the EU commission to emulate real-world driving conditions [8]. The practical significance of this study is to show how advanced learning networks help guarantee vehicle compliance with different legislations. DP was conducted to obtain the benchmark CU over the RTS-95 cycle.

The ANFIS network obtained by GKFL-9-GA was chosen as the generic energy management model, and a DDPG network was connected in front of the ANFIS network as described in Section III-B. Two baseline energy management systems developed based on a DDPG-only network and an ANFIS-only network, respectively, were used for comparison of the learning progresses shown in Fig. 9. Each learning episode (containing 800 steps with 1 s step length) runs repetitively under the RTS-95 cycle with an initial battery SoC of 40%. Because online learning contains random factors, the learning of using both DDPG–ANFIS and DDPG-only networks is repeated ten times independently. This is to examine their CU value at the end of each episode.

Fig. 9. - Online learning progress for knowledge transfer.
Fig. 9.
Online learning progress for knowledge transfer.

Show All

The blue-colored area covers all the CU values that were achieved in each episode of the ten trials when using the DDPG–ANFIS network for power management. The blue dot-curve shows the average CU values achieved by the DDPG–ANFIS network. The yellow-colored area covers the CU values achieved by the DDPG-only network and the yellow dot-curve comprises their mean values. The DDPG–ANFIS network has a greater opportunity to gain better CU values in the early stage with the transferred knowledge. Both DDPG–ANFIS and DDPG-only networks develop their policy to achieve better CU from real-world interactions. They can converge to the same CU level that is closed to the DP result. The converged CU is 38% higher than that obtained by the ANFIS-only network.

To monitor how the DDPG–ANFIS network enables adaptive transfer learning in the RTS-95 cycle, the original power demand and the regulated power demand (the output of the DDPG subnetwork) are illustrated in Fig. 10. The data were collected at the 50th episode, where the DDPG–ANFIS network had developed a proper control policy for power management. Because the ANFIS subnetwork transfers the knowledge learned from the WLTC cycle, it achieves the maximum performance in the scenarios where the power demands are between 0 and 40 kW. The proposed DDPG subnetwork has shown a capability to map the power demand into the domains where the ANFIS achieves its optimal performance.

Fig. 10. - Regulation of power demand in the RTS-95 cycle.
Fig. 10.
Regulation of power demand in the RTS-95 cycle.

Show All

C. Online Learning in Simulated Real-World Conditions
To investigate the feasibility of the proposed DDPG–ANFIS network in real-world learning, evaluations were conducted under simulated real-world driving conditions (SRDCs) with HiL testing. The baseline power management controllers were developed based on DDPG-only network and ANFIS-only network, respectively, for comparison. Five SRDC cycles (SRDC1-5) were generated using the AVL Random Driving Cycle Generator [53], which assume that the speed profiles are collected from different drivers and driving scenarios. The control utilities are measured in 200 learning episodes. Each episode contains 800 learning steps (with 1 s step length), which equals to the RTS-95 cycle. Battery SoC is randomly initialized between 35% and 40%.

The CU achieved by the DDPG–ANFIS network at each episode under SRDC-1 are compared with the ANFIS-only network and the DDPG-only network in Fig. 11(a). To illustrate the trends of CU improvements, moving averages of the control utilities in every 30 episodes are illustrated in Fig. 11(b). In SRDC-1, the moving average of the CU achieved by the ANFIS-only network remains at the same level because the ANFIS subnetwork only transfers the knowledge learned from WLTC but without the online learning capability. The moving average of the CU achieved by both DDPG–ANFIS and DDPG-only networks tends to be continuously improved. Both DDPG–ANFIS and DDPG-only networks achieve similar CU values when they acquired enough experience in real world, but DDPG–ANFIS network performs much better at the early stage.

Fig. 11. - Learning Performance in the SRDC-1. (a) Row data. (b) Moving average of every 30 episodes.
Fig. 11.
Learning Performance in the SRDC-1. (a) Row data. (b) Moving average of every 30 episodes.

Show All

To testify the robustness of the proposed DDPG–ANFIS network in real-time control, a comparison study with ANFIS-only and DDPG-only networks was conducted under five simulated real-world conditions. The statistical results of the online learning performance are summarized in Table IV. With mean power demands and standard derivations listed in Table IV, SRDC1 simulates a highway drive, which has a relatively higher power demand; SRDC2 simulates a suburban drive, which has combined highway and urban sections; and SRDC3-5 simulates urban driving with varying traffic congestion.

TABLE IV Online Learning Performance in SRDCs
Table IV- 
Online Learning Performance in SRDCs
To measure the online performance at different stages, the mean CU values were calculated with 1) all 200 episodes’ data for global performance; 2) the first 50 episodes’ data for the initial stage; and 3) the last 50 episodes’ data for the final stage. Generally, the DDPG–ANFIS network robustly outperforms both ANFIS-only and DDPG-only networks by achieving higher mean CU values under the five SRDC cycles. The highest mean CU value is achieved under SRDC4, which is 138% higher than the ANFIS-only network and 5.2% higher than the DDPG-only network. Because of the prior knowledge implemented in the ANFIS subnetwork and online learning capability enabled by DDPG subnetwork, the DDPG–ANFIS network achieves at least 4.9% higher mean CU values for the first 50 episodes than DDPG-only network. Also, the proposed method achieves more than 4.5% higher mean CU values for the last 50 episodes than the ANFIS-only network.

SECTION V.Conclusion
This article proposes an adaptive learning network to enable adaptive knowledge implementation and transfer in the power management of a plug-in hybrid vehicle. This network combines a DDPG network with an ANFIS network and the superiorities over both individual networks have been demonstrated. Experimental evaluations have been conducted on both SiL and HiL platforms. The conclusions drawn from this work are as follows.

The proposed GKFL for building ANFIS network is effective in the implementation of offline optimization knowledge. The highest CU has been achieved with ninefold fuzzy learning for the studied vehicle, which is 8% higher than using conventional fuzzy learning in the WLTC condition.

In the study under RTS-95 cycle representing new legislation conditions, the proposed DDPG–ANFIS network benefits from online reinforcement signals so that it achieved 38% higher CU compared to the ANFIS-only network.

In five SRDCs, the proposed DDPG–ANFIS network achieves higher control utilities, which is up to 138% higher than the ANFIS-only network and 5.2% higher than the DDPG-only network.

The planned future work will develop online multiple-objective optimization method to improve energy efficiency, safety, and drivability in autonomous driving. The parameter sensitivity in DDPG-based online learning will also be studied to guarantee the robustness.