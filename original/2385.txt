The roundabout is a typical changeable, interactive scenario in which automated vehicles should make adaptive and safe decisions. In this article, an optimization embedded reinforcement learning (OERL) is proposed to achieve adaptive decision-making under the roundabout. The promotion is the modified actor of the Actor–Critic framework, which embeds the model-based optimization method in reinforcement learning to explore continuous behaviors in action space directly. Therefore, the proposed method can determine the macroscale behavior (change lane or not) and medium-scale behaviors of desired acceleration and action time simultaneously with high sample efficiency. When scenarios change, medium-scale behaviors can be adjusted timely by the embedded direct search method, promoting the adaptability of decision-making. More notably, the modified actor matches human drivers’ behaviors, macroscale behavior captures the human mind’s jump, and medium-scale behaviors are preferentially adjusted through driving skills. To enable the agent adapts to different types of the roundabout, task representation is designed to restructure the policy network. In experiments, the algorithm efficiency and the learned driving strategy are compared with decision-making containing macroscale behavior and constant medium-scale behaviors of the desired acceleration and action time. To investigate the adaptability, the performance under an untrained type of roundabout and two more dangerous situations are simulated to verify that the proposed method changes the decisions with changeable scenarios accordingly. The results show that the proposed method has high algorithm efficiency and better system performance.

SECTION I.Introduction
With the development of automated vehicles, decision-making methods face significant challenges in actively adapting to complex, changeable, and interactive scenarios [1]. Aside from the uncertainty with perceptive technology, the factors that the behaviors of other drivers cannot be precisely predicted and are highly influenced by personal characteristics limit the algorithm to make a trustworthy and active decision [2]. To ensure safety, current driving strategies are conservative and sometimes different from human behaviors, which decreases the acceptability for passengers. Although intelligent vehicles have already equipped with L2 or L3 autopilot systems, vehicles driven with humans and driven with the machine will mix in the near future [3]. Therefore, decision-making methods for automated vehicles should improve its acceptability and performance under the assurance of safety [4].

A. State-of-the-Art Review and Challenges
The literature on decision-making and reinforcement learning (RL) is reviewed because RL is supposed as a favorable choice and is more adaptive to interactive environments among decision-making methods [5].

1) Decision-Making:
Related studies employ rule-based and learning-based methods to model driving strategies as the switch in scenario-based state machines [6]–[7][8], the Markov decision process [9]–[10][11][12], and so on. The similarity lies in that the decisions are defined as specific and discrete human’s driving behaviors (such as overtaking, lane-maintaining, lane-changing, and right-hand turning) with exhaustion [13]. Thus, the decisions need to be predefined by experts and have difficulty in generalizing to nonpredefined behaviors or scenarios [14]. Besides, once the driving scenarios become complex, the controller has complicated switch rules to consider feasible and safety actions although finite driving behaviors are considered [15].

To increase the scalability of automated vehicles, the decisions combine more detailed information, such as lateral and longitudinal goals [16], and the action space is discretized around 104 as a tradeoff between huge exploration space and better generalization. In [17], the decisions are decomposed with some parameters that indicate physical quantities of macroscale behaviors and medium-scale behaviors (such as terminal relative lateral position and terminal heading angle), and its values finally form the decision. As these parameters involve a finite set, and each of them is bounded within a specific range [18], [19], the process of decision-making is transformed into finding the optimal values about these parameters. The parameter-based decision-making is more suitable for optimization, as well as increases the scalability to multiple scenarios.

More notably, under the hierarchical control framework of automated vehicles, better performance can be achieved by designing the decision-making module and trajectory planning module jointly together with employing information of medium-scale continuous behaviors [20].

2) Reinforcement Learning:
The model-free RL lacks high efficiency and stability in continuous problems [21]–[22][23][24]. Several techniques, such as asynchronously updated strategy [25], reward shaping [26], and pretraining [27], [28], are put forward to promote algorithm efficiency and stability. Deterministic policy methods have higher algorithm efficiency and stability when a model-based controller is enabled for sampling [29]–[30][31][32][33].

For the particularity of the problem in automotive control, close attention must be paid to the changeable scenarios when the behaviors of surrounding vehicles are not precisely predicted and may suddenly change [34]. During this period, the driving strategy is different from the normal case and the former decision that the host agent has made, which also only sustains temporarily. Consequently, the decision-making method requires adaptability to behave accordingly in changeable and interactive scenarios [35]. The model-based approach can be used to explore action effectively, and the driving strategy can be fast iterated [36].

B. Work and Contributions
In this work, an optimization embedded RL (OERL) is proposed to achieve adaptive decision-making for automated vehicles in a typical changeable and interactive scenario, roundabout. This method determines macroscale behavior, terminal relative lateral position, medium-scale behaviors, desired acceleration, and action time, simultaneously, in which the desired acceleration and action time are different between human drivers and are seldom considered in previous studies. To achieve adaptive decision-making, the actor in the Actor–Critic (AC) framework is modified with the following improvements. First, in the state design, except for the state vector for environment representation (ER), the state vector for task representation (TR) is added. Second, the policy network is restructured to balance different dimensions in ER and TR. Third, a neural network empirical model is established to model the execution ability of the trajectory planning module, which will be used by the embedded optimization method. Fourth, the direct search that is known as a model-based optimization method is embedded to perform action exploration for the continuous action to promise high algorithm efficiency and adaptability in changeable and interactive scenarios.

The innovation and contribution of this article can be summarized as follows.

The proposed method matches human’s behavior in the driving process. For instance, under changeable scenarios, the medium-scale behaviors can be preferentially adjusted, such as continuous adjustment of pedal and steering wheel through driving skills before reaching the threshold in mind and jump for another macroscale driving behavior. In addition, medium-scale behaviors are different from human drivers.

The proposed method uses the model reasonably to improve the efficiency of RL methods. By embedding the model-based optimization method into the RL method, the proposed method keeps algorithm efficiency comparable with decision-making containing discrete macroscale behavior of terminal relative lateral position and constant medium-scale behaviors of desired acceleration and action time.

By considering TR and restructuring the policy network, the driving strategy can timely recognize different situations and adapt to a different type of roundabout.

With the embedded optimization method, adaptive decisions can be achieved by adjusting the desired acceleration and action time online to avoid emergencies in the interactive scenario and obtain better performance.

The rest of this article is organized as follows. In Section II, first, the driving scenario and the overall system are described. Subsequently, the parameter-based decision-making problem is formulated. In Section III, the OERL is illustrated, which contains a modified actor network (AN), direct search method, and a neural network empirical model. In Section IV, the effectiveness of the proposed method is evaluated by simulations. Conclusions are given in Section V.

SECTION II.Parameter-Based Decision-Making Problem Formulation
In this section, roundabout, a complex and typical driving encounter scenario, and the overall control system are depicted. The driving behavior is modeled as Markov’s decision processes (MDPs). As a destination and multivehicles interaction are included in this problem, the state space is specially designed to behave efficiently. Moreover, action space is formulated basing on the parameter-based decision.

A. Driving Scenario and System Description
For automated vehicles in urban traffic, the scenarios are more complex and contain many driving encounters. Except for completing the routing task from the navigation layer, a vehicle will also interact with multivehicles and keep driving safely [37]. Beyond numerous scenarios, the roundabout scenario is a typical complex driving encounter scenario that contains the abovementioned two typical situations: a destination and multivehicles interaction. A diagram of a roundabout is shown in Fig. 1. In the roundabout, there are several vehicles on the current lane and both neighbor lanes.

Fig. 1. - Diagram of the roundabout and the overall control system.
Fig. 1.
Diagram of the roundabout and the overall control system.

Show All

The nonconservative decisions can be implemented with a determinative and parameter-based description [16], [17]. For example, the action time should be short, or the acceleration should be high in some active lane-change behaviors. The parameter decisions are the terminal relative lateral offset to current lane, action time, and desired acceleration, which contains numerous kinds of lane-keeping and lane-change in a roundabout.

In the overall control system, the simulation environment is established in Prescan, and a destination is decided in routing planning. Then, in the parameter-based decision-making module, the proposed OERL is applied to obtain decision parameters’ values. After that, in the trajectory planning and motion control module, the trajectory is directly optimized with nonlinear model predictive control. Finally, the motion control variables are output to the actuator control module.

B. MDPs Modeling
In this work, the parameter-based decision-making problem is modeled as MDPs in RL. The continuous state space S , the action space A , which contains both discrete and continuous variables, and the reward function can be designed as followed sections.

1) State Design:
Differently, the state representation is divided into two parts: ER and TR. The ER helps the agent make a safe decision, and the TR makes the agent accomplish the routing task. This change is also consistent with a new AN, which will be introduced in Section III-B.

Concerning ER, in a roundabout, the surrounding vehicles can be divided into two parts shown and numbered, as shown in Fig. 2. The ranges of different positions are given in Table I. One part is the vehicles that are close to the host vehicle and should be paid great attention, which is marked in light-blue and has seven potential positions (P1,P2,…,P7 ). In order to fully depict each of them, the relative lane ΔLn(k) , the relative velocity Δvn(k) , the acceleration an(k) , the relative distance dn(k) , and the intention of the surrounding vehicle In(k) are taken into consideration, in which the subscript n indicates the corresponding potential position with a specific vehicle. Here, the relative lane ΔLn(k)=Ln(k)−Lh(k) can be calculated by the current lane of surrounding vehicle Ln(k) and host vehicle Lh(k) . The relative velocity Δvn(k)=vn(k)−vh(k) can be calculated by the velocity of surrounding vehicle vn(k) and host vehicle vh(k) . The intention of the surrounding vehicle In(k)∈{−1,0,1} can be predicted by our previous work. Meanwhile, a human driver will also choose a fluent lane rather than a blocked lane. Thus, the near traffic flows in the neighbor lanes that are marked with light-blue in Fig. 2 (such as P8,P9,…,P12 ) are taken into consideration as another part of the ER. Here, the state of near traffic flow in the front and behind of neighbor lanes can be the average relative velocity Δv¯n(k) and the average time headway between these vehicles TH¯n(k) in the marked area, respectively. Here, THn,j(k)=dn,j(k)/vn,j(k) is the j th vehicle’s time headway in the near traffic flow numbered as Pn . Thus, the state vector in positions P1 –P7 can be expressed as
Spn(k)=(Fn(k),ΔLn(k),Δvn(k),an(k),dn(k),In(k))T(1)
View SourceRight-click on figure for MathML and additional features.where Fn∈{1,0} represents that the corresponding position (P1,…,P7) is a feasible lane or not. The state vector in position P8−P12 can be expressed as
Spn(k)=(Δv¯n(k),TH¯n(k))T.(2)
View SourceRight-click on figure for MathML and additional features.Thus, the state vector of ER part can be expressed as
SER(k)=(SP1(k),SP8(k),SP2(k),SP3(k),SP11(k),SP4(k),SP9(k),SP5(k),SP10(k),SP6(k),SP7(k),SP12(k))T.(3)
View SourceRight-click on figure for MathML and additional features.

TABLE I Range of Different Positions
Table I- 
Range of Different Positions
Fig. 2. - Diagram of the decision scenario.
Fig. 2.
Diagram of the decision scenario.

Show All

Respect to TR, in a roundabout, there is an expected exit with route planning, whose angle, radius, and lane are αE , DE , and LE , respectively. The relative longitudinal distance Δlh and the lane ΔLh between the host vehicle and expected exit are taken into consideration of the state vector of TR. Here, the relative lane ΔLh(k)=LE−Lh(k) can be calculated by the exit lane LE and the current lane of host vehicle Lh(k) . The relative longitudinal distance can be expressed as
Δlh(k)==Δαh(k)(DE+Dh(k))/2(αE−αh(k))(DE+Dh(k))/2(4)
View SourceRight-click on figure for MathML and additional features.where Δαh is the relative angle between the host vehicle and expected exit, and αh(k) and Dh(k) are the angle and radius of host vehicle at time step k . Thus, the state vector of TR part can be expressed as
STR(k)=(Δlh(k),ΔLh(k))T.(5)
View SourceRight-click on figure for MathML and additional features.Finally, the state vector is S=(SER,STR) .

2) Action Design:
As the parameter-based decision-making framework is used, a more sophisticated decision is decided, which will be applied in trajectory planning and change the trajectory form [17]. Thus, the action space contains three parts, which can be expressed as
a(k)=(Ty(k),atar(k),ta(k))T(6)
View SourceRight-click on figure for MathML and additional features.where Ty(k)∈{−L,0,L} is the lateral target offset of the current lane, and L is the distance between two neighboring traffic lane lines. In this method, we assume that the terminal relative lateral offset of the current lane at time step k and a short distance ahead can be known by perceptive technology. atar(k) is the target acceleration, and ta(k) is the expected action duration that changes with different tasks and situations. The action vector can precisely describe the decision, and the values of these parameters change with different decisions. To be more specific, some examples are listed in Table II. The decision with different decision parameters in the lane-change scenario can be described as the same human behavior, such as lane keep (with acceleration, maintain, and deceleration), lane-change to the right (with acceleration, maintain, and deceleration) in fast, moderate, and mild modes, and lane-change to the left (with acceleration, maintain, and deceleration) in fast, moderate, mild modes.

TABLE II Examples of Decisions With Different Values of Parameter
Table II- 
Examples of Decisions With Different Values of Parameter
3) Reward Design:
Here, the safety reward rs , task reward rt , and execution reward re are mainly considered. When calculating safety reward rs , the surrounding vehicle in the front of current lane Lh(k) and in the target lane Ltar(k)=Lh(k)+sign(Ty(k)) are considered, which also contains the surrounding vehicle that will change to these two lanes in the next 5 s. When sign(Ty(k))=0 , only the vehicle in the position P4 is considered. When the host vehicle executes lane-change, take sign(Ty(k))=−1 ; for example, the surrounding vehicle in the positions P1,P2,P3 , and P4 will be considered. Assume that the distance between vehicles in the corresponding position Pn and the host vehicle in its lane is dn(k) . Considering these positions, the incremental equation of safety reward rs can be respected as
rs=⎧⎩⎨rs−10rs−100rsfor dn(k)<defor dn(k)<dcelse(7)
View SourceRight-click on figure for MathML and additional features.where de=3 is the emergency distance and dc=1 is the collision distance.

The task reward rt can be divided into two categories. One is consistent with its position, and the incremental equation can be expressed as
rt=⎧⎩⎨rt+100rt−100rtfor |Δlh(k)|<3 & ΔLh(k)=0for |Δlh(k)|<3 & ΔLh(k)≠0else.(8)
View SourceRight-click on figure for MathML and additional features.The other is consistent with the action. As the inner lane has more advantages in speed-ability, the host vehicle trends to go inside. The expected lane can be roughly calculated as
ΔLexp(k)=⌊αE/αlc⌋−ΔLh(k)(9)
View SourceRight-click on figure for MathML and additional features.where αE and αlc are the angles to the exit and estimation of angle change about a lane-change. Then, the incremental equation of task reward rt can be expressed as
rt=⎧⎩⎨⎪⎪⎪⎪⎪⎪rt+0.25rt+0.25rt−0.25rtfor ΔLexp(k)>2 & Ty(k)<0for ΔLexp(k)<0 & Ty(k)>0for ΔLexp(k)<0 & Ty(k)<0else.(10)
View SourceRight-click on figure for MathML and additional features.Furthermore, when a lane-change action is selected, the preceding vehicle and the traffic flow are compared as well. Assume that the vehicle in the target lane and current lane is P1 and P4 . The reward can be expressed as
rt,1=rt,2=rt,3=rt,4=⎧⎩⎨1−10for  v1(k)>v4(k)for  v1(k)<v4(k)else⎧⎩⎨1−10for TH1(k)>TH4(k)for TH1(k)<TH4(k)else⎧⎩⎨1−10for  TH¯1(k)>TH¯4(k) for  TH¯1(k)<TH¯4(k)else⎧⎩⎨1−10for  d1(k)>d4(k)for  d1(k)<d4(k) else.(11a)(11b)(11c)(11d)
View SourceRight-click on figure for MathML and additional features.The corresponding incremental equation can be expressed as
rt=k1rt,1+k2rt,2+k3rt,3+k4rt,4(12)
View SourceRight-click on figure for MathML and additional features.where k1=0.04 , k2=0.03 , k3=0.02 , and k4=0.1 are the coefficients. Finally, the execution reward re can be calculated as
re={−k5k6(LT−Lh(k))for Ty(k)≠0for Ty(k)=0(13)
View SourceRight-click on figure for MathML and additional features.where k5=0.01 and k6=0.04 are the coefficients. LT is the total lane in a roundabout. The total reward can be expressed as
r=rs+rt+re.(14)
View SourceRight-click on figure for MathML and additional features.

SECTION III.Optimization Embedded Reinforcement Learning
In this section, OERL is summarized in Section III-A, in which several promotions are made in the actor of the AC frame. Then, these promotions are detailed from Sections III-B–III-D. Eventually, the proposed method increases sample efficiency and deals with changeable scenarios, which goes beyond the pure learning method.

A. Promotion and RL Algorithm Design
In this work, the action space mixes discrete action and continuous action. To obtain equal sample efficiency with the discrete action space, the model-based optimization method is embedded in RL. As the decision-making process has no physical model, only the designed trajectory planning controller can provide some prior knowledge, which can reflect the action’s execution. Thus, an empirical model can be established to simulate the state change of the host vehicle. After that, a good sample of the continuous values of the decision parameter can be found, which can significantly promote the efficiency of exploration in continuous action space and accelerate the learning process.

The diagram of the AC frame is shown in Fig. 3. The improvements are made in the actor of the AC frame, as summarized in the following. First, the AN for discrete action Ty is modified to strengthen TR to behavior efficiently. Second, three neural network empirical models are established to replicate the designed trajectory planning controller’s execution. Then, the direct search method, the simplex search algorithm, is used to guide the search values of continuous decision parameters.

Fig. 3. - Diagram of the AC frame and the proposed algorithm.
Fig. 3.
Diagram of the AC frame and the proposed algorithm.

Show All

The OERL is shown in Algorithm 1. As shown in Algorithm 1 and Fig. 3, the discrete action Ty is decided by modified AN. The weights in the modified AN and critic network are updated at the end of one episode with bootstrapping from the last state [25]. Meanwhile, AN 1 (AN1) and AN 2 (AN2) compute the initial point for continuous action. The weights for AN1 and AN2 are updated by supervised learning with action atar(k),ta(k) that is calculated from empirical model-based direct search. In each episode, an experienced good policy will be updated, which compares the current policy with history best policy and records a better policy.

Algorithm 1 OERL
//Initialize policy with weights θ1,θ2,θ3 and value approximation V(s;ω) , learning rates α0,β0 , reply number n

Repeat

Initialize environment

With probability ε select a random action Ty(k)

Otherwise select Ty(k) with π(S(k);θ1) or reply the best policy in history every n episode

Obtain initial points from π(S(k);θ2) , π(S(k);θ3) to calculate action atar(k),ta(k) with Empirical Model-based Direct Search

Observe next state S(k+1) and reward r(k)

until S(k+1) is terminal state or task failed

Bootstrap from the last state to update θ1 and ω [25]

Perform supervised learning at θ2 and θ3

Update the best policy in history

until Convergence

B. Actor Network
In decision-making, the feature of the task (TR) plays an equal role compared with ER. For example, in the preliminary stage of a task, the agent has more freedom to choose higher rewards action, while, in the later period, it has to consider more about the success of the task. This leads to a different decision with a similar ER. However, TR only has two dimensions, and ER has 52 dimensions. Thus, TR will fade when a fully connected neural network is applied. This situation will also bring great difficulty in function approximation when these two kinds of representation have significantly different dimensions.

In this work, the structure of the AN about terminal relative lateral offset Ty is changed, as shown in Fig. 4. It has two hidden layers. To reserve this feature and behavior efficiently, the TR is duplicated, and these vectors are reinput in the input layer and the first hidden layer. Thus, there are 104 nodes in the input layer, whose half is state representation, and half is duplicated TR. In the first hidden layer, the conventional nodes are 32. The TR is duplicated to form equally 32 nodes to reinput to the first hidden layer. The second hidden layer only has conventional nodes, which number is 16.

Fig. 4. - Diagram of the whole decision-making and trajectory planning process.
Fig. 4.
Diagram of the whole decision-making and trajectory planning process.

Show All

C. Neural Network Empirical Model
Neural network empirical models are established to simulate the state change of the host vehicle under different decision parameter values with several movement points by BP neural network learning.

First, the trajectory data are collected. As the values of the decision parameter are within a fixed range, several parallel experiments are implemented to obtain its execution in different decisions. The designed trajectory planning controller under parameter decisions D=(ta,atar) can be expressed as
minJ=∫ta0(k1δ2f+k2a2+k3(a−atar)2+k4Δδ2f+k5Δa2)dts.t. x˙(t)=f(x(t),u(t),t)amin≤a≤amax,δf,min≤δf≤δf,max⎧⎩⎨⎪⎪vy(ta)=0,wr(ta)=0,φ(ta)=0,yl(ta)=yl,f.for P(t_a)in R_acφ(ta)=φr,yl(ta)=yl,f.for P(t_a) in R_cd(15)
View SourceRight-click on figure for MathML and additional features.where x=[X,Y,φ,vx,vy,wr]T is the vector of the state; u=[a,δf]T is the vector of the control variable and a changes the longitudinal velocity vx ; a simple equation about the change of longitudinal velocity vx is considered; and a lower level tracking controller is designed to follow the expected a , which can simplify the motion control model. P(ta)=(X(ta),Y(ta)) is the position at the end of predictive horizon. Rac and Rcd are the straight and curved road segments, respectively. The motion control model x˙(t)=f(x(t),u(t),t) is attached in APPENDIX. V. In parallel experiments, decision parameters expected action duration ta∈[2,4] when Ty≠0 and expected acceleration atar∈[−2,2] , respectively. The optimized trajectory under different decisions D=(ta,atar) can be obtained as a database, and part of them is shown in Fig. 5. As can be seen in Fig. 5, the medium-scale variables, such as expected acceleration atar and action duration ta , can have a significant influence on the trajectory of automated vehicles.

Fig. 5. - Change of trajectories with different values of decision parameters. Here, T2, T3, and T4 indicate 
$t_{a}=2,3$
, and 4, respectively. A1, A2, A3, A4, and A5 indicate 
$a_{\mathrm{ tar}}=-2,-1,0,1$
, and 2, respectively.
Fig. 5.
Change of trajectories with different values of decision parameters. Here, T2, T3, and T4 indicate ta=2,3 , and 4, respectively. A1, A2, A3, A4, and A5 indicate atar=−2,−1,0,1 , and 2, respectively.

Show All

Then, each trajectory can be represented with the coefficient lx , sequences of lateral movement coefficient ly,1,ly,2,…,ly,5 , and the time coefficient lt,1,lt,2,…,lt,5 . We take one of the trajectory as an example. The diagram of the abovementioned characteristics about the trajectory can be represented, as shown in Fig. 6. In a trajectory, the coefficient lx can be expressed as
lx=sxvh,0∗ta(16)
View SourceRight-click on figure for MathML and additional features.where sx is the longitudinal displacement, and vh,0 is the initial velocity of the host vehicle. Meanwhile, the coefficient lx will also be influenced by the expected acceleration atar . In lateral movement, five points in this trajectory are recorded, of which the longitudinal displacement is 1/8 lx , 1/4 lx , 1/2 lx , 3/4 lx , and 7/8 lx , respectively. The corresponding lateral movement coefficient ly,i and the time coefficient lt,i are calculated, which can be expressed as
ly,i=lt,i=sy,iLts,ita.(17a)(17b)
View SourceRight-click on figure for MathML and additional features.Finally, the coefficient lx , sequences of lateral movement coefficient ly,1,ly,2,…,ly,5 , and the time coefficient lt,1,lt,2,…,lt,5 are learned with three neural networks as the empirical models, which can reflect the characteristic of each kind of trajectory planning. The input vector contains the initial velocity of the host vehicle vh,0 , the expected acceleration atar , and the expected action duration ta .

Fig. 6. - Diagram of characteristics of the trajectory example.
Fig. 6.
Diagram of characteristics of the trajectory example.

Show All

For avoiding the local minimum value while learning, the genetic algorithm combines the Levenberg–Marquardt training method in BP neural network learning. All the neural network empirical model has two hidden layers with 128 nodes and 64 nodes in the first hidden layer and the second hidden layer, respectively. The numbers of samples for the training set and test set are 322 and 18, respectively. The average training step is about 500, which will vary with different initial weights. The threshold for training error is 1e−6 . One situation of the learned neural network empirical model that vh,0=10m/s2 is shown in Fig. 7.

Fig. 7. - Output of the learned neural network empirical model when 
$v_{h,0}$
 = 10 m/s2. Fig. 7(a)–(f) are the output about longitudinal displacement coefficient 
$l_{x}$
, lateral movement coefficient 
$l_{y,1}$
, 
$l_{y,2}$
, 
$l_{y,3}$
, 
$l_{y,4}$
, 
$l_{y,5}$
, respectively.
Fig. 7.
Output of the learned neural network empirical model when vh,0 = 10 m/s2. Fig. 7(a)–(f) are the output about longitudinal displacement coefficient lx , lateral movement coefficient ly,1 , ly,2 , ly,3 , ly,4 , ly,5 , respectively.

Show All

D. Direct Search With Empirical Model for Continuous Action
The optimization algorithm is used to find the optimal expected acceleration atar and expected action duration ta based on the neural network model and surrounding vehicles’ movement prediction. The prediction of the surrounding vehicles’ movement will not be discussed in detail. As the mathematical model is replaced with a neural network experienced model, which reserves the characteristic of trajectory planning, the gradients are not available to optimize the values of the decision parameter. The direct search methods rely exclusively on values of the objective function and replace the actual gradient with an estimated gradient. In continuous action space RL, the efficiency of the algorithm is consistent with action exploration. Once good action can be explored, the efficiency of the algorithm can be significantly improved. In this work, the simplex search method, the Nelder–Mean method, is used to optimize the values of the decision parameter. With the aforementioned neural network empirical models, longitudinal displacement coefficient lx , the sequence of time step Ts , and the lateral displacement Sy when the host vehicle’s longitudinal displacement is at Sx can be obtained, which can be expressed as
Sx=Sy=Ts=[0,1/8 sx,1/4 sx,1/2 sx,3/4 sx,7/8 sx,sx][0,ly,1Ty,ly,2Ty,ly,3Ty,ly,4Ty,ly,5Ty,Ty][k,k+lt,1ta,k+lt,2ta,k+lt,3ta,k+lt,4ta,k+lt,5ta,k+ta](18a)(18b)(18c)
View SourceRight-click on figure for MathML and additional features.where sx is calculated by (16), and sequences of lateral movement coefficient ly,1,ly,2,…,ly,5 and the time coefficient lt,1,lt,2,…,lt,5 are calculated by (17).

Then, the vehicles in the current lane (Ln=Lh ) and the target lane (Lh+sign(Ty)=Ln ) are selected. When the decision Ty=0 , the nearest vehicle in the position P4 or P9 is mainly considered, and the expected action duration ta is set to 1 sec. The target acceleration atar is optimized to maintain the time headway between the host vehicle and the preceding vehicle. When the decision Ty≠0 , take Ty=L , for example, the vehicle in the position P4 that d4<2de . The vehicles in the positions P1 , P2 , and P3 that |di|<2de are considered, and the expected action duration ta and the target acceleration atar are optimized to keep the distance between the host vehicle and the preceding vehicle as large as possible. The trajectories of the surrounding vehicles are generated, the position of surrounding vehicles Pn in the j step in time sequence Ts can be predicted as (sn,x(Ts(j)),sn,y(Ts(j)) . Here, the sequences of sn,x and sn,y are represented as Sn,x and Sn,y , respectively. The accuracy of intention and trajectory prediction is not discussed, which already has numerous previous works. Thus, the objective function is incremental calculated and can be expressed as
J=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪J+k6∑7j=1(Sx(j)−Sn,x(j))2+(Sy(j)−Sn,y(j))2for−2de<dn<2de & Ty≠0J+k7(THn−THexp)2for Ty=0Jelse.(19)
View SourceRight-click on figure for MathML and additional features.Then, the expected values of expected action duration ta and the target acceleration atar are also considered in the incremental equation of the objective function
J=J+k8(ta−ta,exp)2+k9(atar−atar,exp)2.(20)
View SourceRight-click on figure for MathML and additional features.With the neural network empirical model and simplex direct search method, the values of the decision D=(ta,atar) can be found. Efficient action exploration can be greatly improved. Meanwhile, in continuous RL, the policy is approximated by a neural network as well. Thus, a trained policy can provide a good initial simplex vertex in turn, which will influence the number of iterations in the direct search method.

SECTION IV.Simulation Results in Complex Driving Encounter Scenario
First, some parameters in the direct search method are evaluated, and the learning efficiency is compared. Then, the learned decision-making strategy is shown in the parallel simulations and is compared with the fixed lane-change time and no longitudinal velocity change. Finally, the performance under an untrained type of roundabout and two more dangerous situations are simulated to further evaluate the proposed method for its better adaptability.

A. Learning Phase Evaluation
In this work, we use the direct search method to obtain continuous action in mixed action space, which can greatly improve algorithm efficiency. First, the effectiveness of the direct search method is evaluated. The initial point I and step dI in the designed initial simplex IS=(I−dI,I,I+dI) are the influencing factor in experiments. Here, the initial point I is (atar,ta) that are output by ANs 1 and 2. The step dI is the variation of initial point I . The iteration step is recorded when the initial point is approaching the optimal point I with a different step dI . As can be seen in Fig. 8, the initial point I , which is more close to the optimal points, has fewer iteration steps. Moreover, different steps dI , which are selected in an appropriate range, have little influence on the number of iterations. Thus, the initial point I is output by the AN, which will be updated in the learning process, and the step dI is set manually.

Fig. 8. - Comparison of the influential factors: initial point 
$I$
 and step 
$dI$
 in the direct search method.
Fig. 8.
Comparison of the influential factors: initial point I and step dI in the direct search method.

Show All

In this work, the direct search method can provide a good sample in continuous action space and acquire similar algorithm efficiency with discrete action learning. In the training phase, as the algorithm in continuous action space has very low sample efficiency, we only compare the learning effect between the proposed learning in mixed action space and normal learning in discrete action space. In the discrete action space problem, the target acceleration and the expected action duration are set in a constant, which can be shown in Table III. Several parallel experiments have been done and the whole training epoch Nt is recorded.

TABLE III Comparison in the Discrete and Mixed Action Spaces
Table III- 
Comparison in the Discrete and Mixed Action Spaces
The average training epoch Nt¯ and average reward Rt¯ are calculated, which are shown in Table III. As can be seen in Table III, the sample efficiency has a similar tendency in these two kinds of learning that the proposed learning method can realize more elaborate decision and acquire higher rewards. Meanwhile, the learning phase of a regular framework of AN is compared with the reconstructed AN. In the reconstructed AN, the numbers of nodes in the hidden layers are 32 and 16. In the regular AN, the numbers of nodes in the hidden layer are 64 and 32. The total training iteration of a regular framework of AN is about three times more than the reconstructed AN, and the structure is also much more complicated.

B. Driving Strategy Evaluation
First, simulation results of the learned driving strategy are shown in Fig. 9(a) and (b). In these simulations, we assume that the intention of the surrounding vehicle can be predicted precisely. The trajectory is predicted with the known intention and vehicle kinematics model under high precision. The situation that the intention of the surrounding vehicle is not predicted precisely will be discussed in Section IV-C. As shown in the diagram of the roundabout in Fig. 1, based on the routing planning, the host vehicle enters at the entrance E1 and exits at the entrance E3. In each scenario, the learned driving strategy is compared with decision-making containing discrete macroscopic behavior of terminal position and constant microscopic behavior of acceleration and action time.

Fig. 9. - Simulation results in scenarios N1 and N2. (1) Initial position and velocity of surrounding vehicles and host vehicle. (2) Trajectory profiles of the host vehicle with varied decision parameter values and constant parameter values and specific surrounding vehicles (
$v_{n}$
). (3) Minimal distance between the host vehicle and surrounding vehicles. (4) Decision action parameter values: the target relative lateral offset of the current lane 
$T_{y}$
. (5) Decision action parameter values: target acceleration 
$a_{\mathrm{ tar}}$
. (6) Decision action parameter values: expected action duration 
$t_{a}$
. (a) Simulation results in scenario N1. (b) Simulation results in scenario N2.
Fig. 9.
Simulation results in scenarios N1 and N2. (1) Initial position and velocity of surrounding vehicles and host vehicle. (2) Trajectory profiles of the host vehicle with varied decision parameter values and constant parameter values and specific surrounding vehicles (vn ). (3) Minimal distance between the host vehicle and surrounding vehicles. (4) Decision action parameter values: the target relative lateral offset of the current lane Ty . (5) Decision action parameter values: target acceleration atar . (6) Decision action parameter values: expected action duration ta . (a) Simulation results in scenario N1. (b) Simulation results in scenario N2.

Show All

In scenario N1, there are 15 surroundings vehicles in these scenarios, and the initial position and velocity are shown in subfigure (1) of Fig. 9(a). There is a surrounding vehicle v10 in the current lane and no other surrounding vehicles in the inner lane. Thus, the host vehicle changes to the clearer inner lane and switch back to the outside lane when approaching the entrance E3. The total simulation times are 9.97 and 11.6 s, respectively, with varied decision parameter values and constant decision parameter values. Trajectory profiles of part surrounding vehicles are shown in subfigure (2) of Fig. 9(a). The trajectory profiles of some surrounding vehicles and the host vehicle with varied decision parameter values and constant decision parameter values (as shown in Table III) are shown in subfigure (2) of Fig. 9(a). The minimal distance with surrounding vehicles is shown in subfigures (3) of Fig. 9(a). The decision action parameter values are shown in subfigures (4)–(6) of Fig. 9(a). As can be seen in these subfigures, these two kinds of decision methods cause different lane-change time step in the roundabout, as well as different target accelerations atar and expected action durations ta . The vehicle that uses changeable parameter decisions can smoothly execute lane-change to the inner lane and pass the roundabout in a shorter time.

In scenario N2, the abovementioned results are compared as well. As shown in subfigures (1) of Fig. 9(b), a surrounding vehicle v10 runs at the inner lane of its current lane, and the distance with surrounding vehicles in the current lane is larger than the inner lane. The host vehicle with changeable parameter decisions will accelerate and lengthen the action time of lane-change to implement lane-change rather than lane-keeping as the host vehicle with constant parameter decisions. Meanwhile, in this process, the minimal distance with other surrounding vehicles are maintained in a safe situation. The total simulation times are 9.7 and 12.0 s, respectively, with varied decision parameter values and constant decision parameter values.

In scenario T1, a totally different roundabout scenario that is untrained is tested. The roundabout only has three lanes, and the host vehicle needs to enter and exit in the E1 and E3. The radius is much smaller than the original roundabout, which is 33, 29, and 25 m, respectively. As shown in subfigures (4)–(6) of Fig. 10, after entering the inner lane, the host vehicle only keeps the lane for two time steps and then changes to the outer lane to exit the roundabout.

Fig. 10. - Simulation results in scenario T1. (1) Initial position and velocity of surrounding vehicles and host vehicle. (2) Trajectory profiles of the host vehicle with varied decision parameter values and constant parameter values and specific surrounding vehicles (
$v_{n}$
). (3) Minimal distance between the host vehicle and surrounding vehicles. (4) Decision action parameter values: the target relative lateral offset of the current lane 
$T_{y}$
. (5) Decision action parameter values: target acceleration 
$a_{\mathrm{ tar}}$
. (6) Decision action parameter values: expected action duration 
$t_{a}$
.
Fig. 10.
Simulation results in scenario T1. (1) Initial position and velocity of surrounding vehicles and host vehicle. (2) Trajectory profiles of the host vehicle with varied decision parameter values and constant parameter values and specific surrounding vehicles (vn ). (3) Minimal distance between the host vehicle and surrounding vehicles. (4) Decision action parameter values: the target relative lateral offset of the current lane Ty . (5) Decision action parameter values: target acceleration atar . (6) Decision action parameter values: expected action duration ta .

Show All

In these simulations, we can see that the host vehicle has learned an active driving strategy, which indicates that the host vehicle will try to overtaking although there are no potential dangers. Furthermore, compared with driving strategy in discrete action space, the host vehicle has better decision performance. Also, the trained driving strategy is not limited to the trained roundabout, and the adaptability is shown in the third scenario.

C. Interactive Scenario Evaluation
Some more complicated and dangerous situations may occur, as shown in Fig. 11. When the host vehicle changes the lane, the surrounding vehicle in the next neighbor lane (gray vehicle) may also change lanes to the target lane and cause emergence conditions or fluctuation in the decision. The reason causes this dangerous situation may have the following reasons. First, the intention cannot be precisely predicted and can change suddenly. When the surrounding vehicle’s motion is not predicted correctly, the host vehicle’s decision may change accordingly and eventually lead to fluctuations or even cause danger. As illustrated in Section II-B, the surrounding vehicle in the next neighbor lane is actually out of the decision-making corresponding area (the light green area, as shown in Fig. 12). The surrounding vehicle in this area is hard to be perceived and not considered in the normal decision-making but also causes fluctuation or even danger. Furthermore, extend the decision-making corresponding area blindly will increase the complexity of the problem and not reasonable for decision-making.

Fig. 11. - Diagram of the complex and dangerous situations.
Fig. 11.
Diagram of the complex and dangerous situations.

Show All

Fig. 12. - Examples of two complex and dangerous situations in a roundabout. (a) Left lane-change (E1). (b) Right lane-change (E2).
Fig. 12.
Examples of two complex and dangerous situations in a roundabout. (a) Left lane-change (E1). (b) Right lane-change (E2).

Show All

For the abovementioned two main reasons, the proposed method has another two advantages to further promote the safety and stability of the decision-making process. First, as shown in Fig. 5, decision parameters can influence the planned trajectory. Once the surrounding vehicle in the next neighbor lane did change the lane to the same target lane as the host vehicle, different decision parameters can be applied to change the trajectory form and keep a safe distance from other surrounding vehicles. Meanwhile, the optimal values of the decision parameters in this specific situation can be provided by the direct search method timely. Thus, the host vehicle can adjust to the changeable environment accordingly based on the current decision first and make as little change as possible. In this way, the host vehicle can change the driving strategy temporarily. The host vehicle will not consider other kinds of decisions unless the safety is greatly influenced.

Thus, we show the results when this happens in two situations, as shown in Fig. 12. In each situation, three comparison experiments are done: the fixed decision (A:−L,3,0) , the original driving strategy without change when the red vehicle is considered (B) , and the decision changes timely with consideration of the red vehicle (C) . The distance between the two vehicles, which evaluates safety, the host vehicle’s (dH) and red vehicle’s (dE) distance to the target lane, the control variables, steering angle δ , target acceleration a , the optimized results, expected action duration ta , and the target acceleration atar are shown in Figs. 13 and 14. As can be seen in the results, when the decision changes timely with consideration of the red vehicle (C) , the distance is controlled to promise safety (a) without causing fluctuations in control (c)–(f). Furthermore, it also helps the host vehicle increase the possibility of getting the right of way (b) for less time to change to the target lane.

Fig. 13. - Results of comparison experiments in situation E1. (a) Distance between the two vehicles. (b) Host vehicle’s (dH) and red vehicle’s (dE) distance to the target lane. (c) Steering angle 
$\delta $
. (d) Target acceleration 
$a$
. (e) Expected action duration 
$t_{a}$
. (f) Target acceleration 
$a_{\mathrm{ tar}}$
.
Fig. 13.
Results of comparison experiments in situation E1. (a) Distance between the two vehicles. (b) Host vehicle’s (dH) and red vehicle’s (dE) distance to the target lane. (c) Steering angle δ . (d) Target acceleration a . (e) Expected action duration ta . (f) Target acceleration atar .

Show All

Fig. 14. - Results of comparison experiments in situation E2. (a) Distance between the two vehicle. (b) Host vehicle’s (dH) and red vehicle’s (dE) distance to the target lane. (c) Steering angle 
$\delta $
. (d) Target acceleration 
$a$
. (e) Expected action duration 
$t_{a}$
. (f) Target acceleration 
$a_{\mathrm{ tar}}$
.
Fig. 14.
Results of comparison experiments in situation E2. (a) Distance between the two vehicle. (b) Host vehicle’s (dH) and red vehicle’s (dE) distance to the target lane. (c) Steering angle δ . (d) Target acceleration a . (e) Expected action duration ta . (f) Target acceleration atar .

Show All

SECTION V.Conclusion
In this article, adaptive decision-making is achieved by the proposed OERL, which is verified in a typical changeable and interactive scenario, roundabout. The modified method matches human drivers’ behavior and realizes adaptive decision-making, as well as promotes algorithm efficiency significantly. Correspondingly, the state, the action, and the frame of the actor, along with the policy network, are specially designed. By experiments, it is verified that the proposed method achieves better performance with reserved continuous varied medium-scale behaviors, together with comparable sample efficiency with decision-making containing discrete macroscopic behavior of terminal position and constant microscopic behavior of acceleration and action time. Besides high sample efficiency, this method adapts the driving policy to a different type of roundabout and changeable scenarios fast that ensures safety.

In our future work, the extension of this method under multiple scenarios will be investigated. The prediction of other traffic participants will be more precisely considered to future promote the safety of the control system.

Appendix Nonlinear Motion Control Model
In the trajectory planning controller, a nonlinear motion control model is used [17]. This model contains a nonlinear vehicle model that considers the longitudinal and lateral dynamics simultaneously, as well as a kinematic equation. As the longitudinal dynamic is considered, the single-track vehicle dynamics model is transformed into a nonlinear vehicle model for model predictive control when a changeable longitudinal velocity is considered. Here, a rear-wheel-driven, front-wheel-steered vehicle is considered. With the consideration of vehicle kinematics with a geometric relationship in the global coordinate system, the nonlinear motion control model can be established as
x˙=f(x,u)=f(x,u),⎡⎣⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢vxcosφ−vysinφvxsinφ+vycosφwralfIzFyf−lrIzFyr1MFyf+1MFyr−vxwr⎤⎦⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥(21a)(21b)
View SourceRight-click on figure for MathML and additional features.where x=[X,Y,φ,vx,vy,wr]T is the vector of the state; u=[a,δf]T is the vector of the control variable. A simple equation about the change of longitudinal velocity vx is considered, and a lower level tracking controller is designed to follow the expected a , which can simplify the motion control model. δf is the steering-wheel angle. X and Y are the coordinates of directions x and y in the global coordinate system; φ is the heading angle in the global coordinate system; M is the mass of the vehicle; vy is the lateral velocity; wr is the yaw rate; Iz is the moment of inertia of the vehicle about the z -axis; and lf and lr are the distances from the center of gravity (CoG) to the front and rear axles, respectively. A concise tire model is considered, the tire slip angle in the front wheel αf and rear wheel αr can be linearized because of the small slip angle, and the lateral tire forces Fyf and Fyr on the front and each rear tire can be written as
Fyf≈Fyr≈−Cf(vy+lfwrvx−δf)=Fyf(vy,wr,vx,δf)−Cfvy−lrwrvx=Fyr(vy,wr,vx)(22a)(22b)
View SourceRight-click on figure for MathML and additional features.where Cr and Cf are the cornering stiffness values of the front and rear tires, respectively.