This paper reviews the current state of the art on reinforcement learning (RL)-based feedback control solutions to optimal regulation and tracking of single and multiagent systems. Existing RL solutions to both optimal H 2 and H ∞ control problems, as well as graphical games, will be reviewed. RL methods learn the solution to optimal control and game problems online and using measured data along the system trajectories. We discuss Q-learning and the integral RL algorithm as core algorithms for discrete-time (DT) and continuous-time (CT) systems, respectively. Moreover, we discuss a new direction of off-policy RL for both CT and DT systems. Finally, we review several applications.
Published in: IEEE Transactions on Neural Networks and Learning Systems ( Volume: 29, Issue: 6, June 2018)

SECTION I.Introduction
Optimal control theory [1]–[2][3][4][5][6] is a mature mathematical discipline that finds optimal control policies for dynamical systems by optimizing user-defined cost functionals that capture desired design objectives. The two main principles for solving such problems are the Pontryagin’s maximum principle (PMP) and the dynamic programming (DP) principle. PMP provides a necessary condition for optimality. On the other hand, DP provides a sufficient condition for optimality by solving a partial differential equation, known as the Hamilton–Jacobi–Bellman (HJB) equation [7], [8]. Classical optimal control solutions are offline and require complete knowledge of the system dynamics. Therefore, they are not able to cope with uncertainties and changes in dynamics.

Machine learning [9]–[10][11][12][13] has been used for enabling adaptive autonomy. Machine learning is grouped in supervised, unsupervised, or reinforcement, depending on the amount and quality of feedback about the system or task. In supervised learning, the feedback information provided to learning algorithms is a labeled training data set, and the objective is to build the system model representing the learned relation between the input, output, and system parameters. In unsupervised learning, no feedback information is provided to the algorithm and the objective is to classify the sample sets to different groups based on the similarity between the input samples. Finally, reinforcement learning (RL) is a goal-oriented learning tool wherein the agent or decision maker learns a policy to optimize a long-term reward by interacting with the environment. At each step, an RL agent gets evaluative feedback about the performance of its action, allowing it to improve the performance of subsequent actions [14]–[15][16][17][18][19]. The term of RL includes all work done in all areas such as psychology, computer science, economic, and so on. A more modern formulation of RL is called approximate DP (ADP).

In a control engineering context, RL and ADP bridge the gap between traditional optimal control and adaptive control algorithms [20]–[21][22][23][24][25][26]. The goal is to learn the optimal policy and value function for a potentially uncertain physical system. Unlike traditional optimal control, RL finds the solution to the HJB equation online in real time. On the other hand, unlike traditional adaptive controllers that are not usually designed to be optimal in the sense of minimizing cost functionals, RL algorithms are optimal. This has motivated control system researchers to enable adaptive autonomy in an optimal manner by developing RL-based controllers.

A. Related Theoretical Work
The origin of RL is rooted in computer science and has attracted increasing attention since the seminal work of [27] and [28]. The interest in RL in control society dates back to the work of [15] and [29]–[30][31]. Watkins’ Q-learning algorithm [32] has also made an impact by considering totally unknown environments. Extending RL algorithms to continuous-time (CT) and continuous-state systems was first performed in [33]. The work of [33] used the knowledge of the system models to learn the optimal control policy. The work of [34]–[35][36] formulated and developed such ideas in a control-theoretic framework for CT systems. The control of switching and hybrid systems using ADP is considered in [37]–[38][39][40][41].

There are generally two basic tasks in RL algorithms. One is called policy evaluation and the other is called policy improvement. Policy evaluation calculates the cost or value function related to the current policy, and policy improvement assesses the obtained value function and updates the current policy. Two main classes of RL algorithms that are used for performing these two steps are known as policy iteration (PI) [14] and value iteration (VI). PI and VI algorithms iteratively perform policy evaluation and policy improvement until an optimal solution is found. PI methods start with an admissible control policy [42], [43] and solve a sequence of Bellman equations to find the optimal control policy. In contrast to PI methods, VI methods do not require an initial stabilizing control policy. While most of RL-based control algorithms are PI, some VI algorithms have also been developed to learn optimal control solutions. We mostly survey PI-based RL algorithms that are used for feedback control design.

RL algorithms in control context have been mainly used to solve: 1) optimal regulation and optimal tracking of single-agent systems [1] and 2) optimal coordination of multiagent systems [44]. The objective of the optimal regulation problem is to design an optimal controller to assure the states or outputs of the systems converge to zero, or close to zero, while in the optimal tracking control problem, it is desired that the optimal controllers make the states or outputs of the systems track a desired reference trajectory. The goal in optimal coordination of multiagent systems is to design distributed control protocols based only on available local information of agents so that agents achieve some team objectives. This paper reviews existing RL-based algorithms in solving optimal regulation and tracking of single-agent systems and game-based coordination of multiagent systems.

Finally, RL algorithms that are used to solve optimal control problems are categorized in two classes of learning control methods, namely, on-policy and off-policy methods [14]. On-policy methods evaluate or improve the same policy as the one that is used to make decisions. In off-policy methods, these two functions are separated. The policy used to generate data, called the behavior policy, may in fact be unrelated to the policy that is evaluated and improved, called the estimation policy or target policy. The learning process for the target policy is online, but the data used in this step can be obtained offline by applying the behavior policy to the system dynamics. The off-policy methods are data efficient and fast since a stream of experiences obtained from executing a behavior policy is reused to update several value functions corresponding to different estimation policies. Moreover, off-policy algorithms take into account the effect of probing noise needed for exploration. Fig. 1 shows the schematic of on-policy and off-policy RL.

Fig. 1. - Two different categories of RL. In on-policy RL, the policy that is applied to the system (behavior policy) to generate data for learning is the same as the policy that is being learned (learned policy) to find the optimal control solution. In off-policy RL, on the other hand, these two policies are separated and can be different. (a) Off-policy RL diagram. (b) On-policy RL diagram.
Fig. 1.
Two different categories of RL. In on-policy RL, the policy that is applied to the system (behavior policy) to generate data for learning is the same as the policy that is being learned (learned policy) to find the optimal control solution. In off-policy RL, on the other hand, these two policies are separated and can be different. (a) Off-policy RL diagram. (b) On-policy RL diagram.

Show All

B. Structure
This paper surveys the literature on RL and autonomy. In Section II, we present the optimal control problems for discrete-time (DT) dynamical systems and their online solutions using RL algorithms. This section includes optimal regulation problem, tracking control problem, and H∞ problem for both linear and nonlinear DT systems. In Section III, we discuss several recent developments of using RL for designing the optimal controllers for CT dynamical systems. Nash game problems and their online solutions are discussed at the end of this section. The RL solution to games on graphs is presented in Section IV. Finally, we talk about applications and provide open research directions in Sections V and VI.

SECTION II.Optimal Control of DT Systems and Online Solutions
Consider the nonlinear time-invariant system given as
x(k+1)=y(k)=f(x(k))+g(x(k))u(k)l(x(k))(1)
View Sourcewhere x(k)∈Rn , u(k)∈Rm , and y(k)∈Rp represent the state of the system, the control input, and the output of the system, respectively. f(x(k))∈Rn is the drift dynamics, g(x(k))∈Rn×m is the input dynamics, and l(x(k))∈Rp is the output dynamics. It is assumed that f(0)=0 and f(x(k))+g(x(k))u(k) is locally Lipschitz and the system is stabilizable. This is a standard assumption to make sure the solution x(t) of the system (1) is unique for any finite initial condition.

A. Optimal Regulation Problem
The goal of optimal regulation is to design an optimal control input to stabilize the system in (1) while minimizing a predefined cost functional. Such energy-related cost functional can be defined as
J=∑i=0∞U(x(i),u(i))≡∑i=0∞(Q(x)+uT(i)Ru(i))
View Sourcewhere Q(x)⪰0 and R=RT≻0 . Hence, the problem to be solved can be defined as
V(x(k))=minu[∑i=k∞(Q(x)+uT(i)Ru(i))],∀x(k).
View Source

The value function given u(k) can be defined as
V(x(k))=≡∑i=k∞U(x(i),u(i))∑i=k∞(Q(x)+uT(i)Ru(i)),∀x.(2)
View SourceAn equivalent to (2) is the Bellman equation
V(x(k))=U(x(k),u(k))+V(x(k+1)).(3)
View SourceThe associated Hamiltonian is defined by
H(x(k),u(k),V)  =U(x(k),u(k))+V(x(k+1))−V(x(k)),∀x,u.
View SourceThe Bellman optimality principle [1] gives the optimal value function as
V⋆(x(k))=minu[U(x(k),u(k))+V⋆(x(k+1))]
View Sourcewhich is termed the DT HJB equation. The optimal control is derived to be
u⋆(x(k))==argminu[U(x(k),u(k))+V⋆(x(k+1))]−12R−1gT(x)(∂V⋆(x(k+1)∂x(k+1)).
View Source

B. Special Case
For the DT linear systems, the dynamics (1) become
x(k+1)=Ax(k)+Bu(k)y(k)=Cx(k)(4)
View Sourcewhere A , B , and C are constant matrices with appropriate dimensions.

By assuming that Q(x)=xT(k)Qx(k),Q≻0 in the Bellman equation, the value function is quadratic in the current state so that
V(x(k))=xT(k)Px(k),∀x.(5)
View SourceThen, the DT HJB becomes the DT algebraic Riccati equation (DARE)
Q−P+ATPA−ATPB(R+BTPB)−1BTPA=0
View Sourceand the optimal control input is
u⋆(k)=−(R+BTPB)−1BTPAx(k),∀x.
View Source

C. Approximate Solution Using RL
The HJB equation is generally extremely difficult or even impossible to solve analytically and one needs to approximate its solution. Existing methods for approximating the HJB equation and DARE require complete knowledge of the system dynamics. The following PI algorithm can be used to approximate the HJB equation and DARE solutions by performing successive iterations.

1) Offline PI Algorithm:
Algorithm 1 presents an offline solution to the DT HJB equation but requires complete knowledge of the system dynamics.

Algorithm 1 PI Algorithm to Find the Solution of HJB
procedure

Given admissible policy u0(k)

for j=0,1,… given uj , solve for the value Vj+1(x) using Bellman equation
Vj+1(x(k))=Q(x)+uTj(k)Ruj(k)+Vj+1(x(k+1)),
View Sourceon convergence, set Vj+1(x(k))=Vj(x(k)) .

Update the control policy uj+1(k) using
uj+1(k)=−12R−1gT(x)(∂Vj+1(x(k+1))∂x(k+1)).
View Source

Go to 3

end procedure

2) Actor and Critic Approximators:
To approximate the solution of the HJB equation and obviate the requirement of the complete knowledge about the system dynamics, actor-critic structure has been widely presented to find the online solution to the HJB. The critic approximator estimates the value function and is updated to minimize the Bellman error. The actor approximator approximates the control policy and is updated to minimize the value function [15], [29], [30], [45].

The value function is represented at each step as
V^j(x(k))=W^Tcjϕc(x(k))=∑i=1Ncw^icjϕci(x(k)),∀x(6)
View Sourceand the control input as
u^j(k)=W^Tajσa(x(k))=∑i=1Naw^iajσai(x(k)),∀x(7)
View Sourcewhere ϕci(x(k)) and σai(x(k)) are the basis functions, W^cj=[w^1cjw^2cj…w^Nccj]T is the weight vector of the critic approximator with Nc the number of basis functions used, and W^aj=[w^1ajw^2aj…w^Naaj]T is the weight vector of the actor approximator with Na the number of basis functions used.

The Bellman equation (3) in terms of the critic approximator (6) is written as
W^Tc(j+1)ϕc(x(k))=U(x(k),u^j(k))+W^Tc(j+1)ϕc(x(k+1)).
View Source

The gradient descent tuning laws for the critic and actor approximators are given as
W^l+1c(j+1)=W^l+1a(j+1)=W^lc(j+1)−α1ϕc(x(k))((W^lc(j+1))Tϕc(x(k))−U(x(k),u^j(k)))W^la(j+1)−α2σa(x(k))×(2R(Wla(j+1))T×σa(x(k))+g(x(k))T∂ϕ(x(k+1))∂x(k+1)W^c(j+1))T(8)(9)
View Sourcewhere α1,α2>0 are the tuning gains. Note that using actor-critic structure to evaluate the value function and improve the policy for nonlinear systems does not need complete knowledge of the system dynamics. In [46], synchronous methods are given to tune the actor and critic approximators. In [47], an online approximator approach is used to find the solution HJB equation without requiring the knowledge of the internal system dynamics. In [48], a greedy iterative heuristic DP is introduced to obtain the optimal saturated controller using three neural networks to approximate the value function, the optimal control policy, and the model of the unknown plant.

Remark 1:
Note that the structure of the value function approximator is an important factor in convergence and performance of Algorithm 1. If an inappropriate value function approximator is chosen, the algorithm may never converge to an optimal solution. For linear systems, the form of the value function is known to be quadratic, and, therefore, there would be no error in its approximation. Consequently, Algorithm 1 converges to the global optimal solution for the linear systems. That is, for linear systems, Algorithm 1 gives the exact solution to the DARE. For nonlinear system, using single-layer neural networks for value function approximation may require a large number of activation functions to assure a good approximation error, and consequently, a near-optimal solution. Two-layer neural networks are used in [49] to achieve a better approximation error with lesser number of activation functions. Moreover, error-tolerant ADP-based algorithms are presented for DT systems [50] that guarantees stability in the presence of approximation error in the value function approximation.

3) Event-Triggered RL [51]:
In order to reduce the communication between the controller and the plant, one needs to use an event-triggered control algorithm. The event-triggering mechanism determines when the control signal has to be transmitted so that the resulting event-based control executions still achieve some degree of performance and stabilize the system. For the nonlinear DT systems, three neural networks are used to find the event-triggered-based approximation of the HJB equation.

The nonlinear system can be represented as
x(k+1)=W⋆Tmϕm(xm(k))+ϵm(10)
View Sourcewhere W⋆m is the target weights of model network from the hidden layer to the output layer, xm(k) is the input vector of the hidden layer, and ϵm is the bounded approximation error.

The system (10) using current estimates W^m of the ideal weights W⋆m is given as
x^(k+1)=∑i=1Nmhw2misi(k)
View SourceRight-click on figure for MathML and additional features.with
si(k)=hi(k)=1−e−hi(k)1+e−hi(k)i=1,…,Nmh∑j=1n+mw1mi,jxmj(k)i=1,…,Nmh
View Sourcewhere w1m and w2m are the weight matrices. hi is the input of the i th hidden node, and si is its corresponding output. Nmh is the number of hidden neurons. xm(k) is the input of the model network that includes the sampled state vector and the corresponding control law.

The gradient descent can be used to update the weights of the network to minimize em=x^(k+1)−x(k+1) . The value function can be approximated by a network similar to (6). The gradient descent can be used to update the weights of the network to minimize
ec(k)=J(x(ki))−[J(x^(ki+1))+U(k)]
View Sourcewhere J is the output of critic network and defined as
J(x(ki))=∑l=1Nchw2clql(k)
View Sourcewith
ql(k)=pl(k)=1−e−pl(k)1+e−pl(k)l=1,…,Nch∑j=1nw1cl,jxcj(ki)l=1,…,Nch
View Sourcewhere pl and ql are the input and the output of the hidden nodes of the critic network, respectively, and Nch is the total number of the hidden nodes. The input for the critic network, xc(ki) , is the sampled state vector only, so there are n input nodes in the critic network.

The sampled state x(ki) is used as input to learn the event-triggered control law, which is defined as
μ(x(ki))=∑l=1Nahw2alvl(k)
View Sourcewith
vl(k)=tl(k)=1−e−tl(k)1+e−tl(k)l=1,…,Nah∑j=1nw1al,jxaj(ki)l=1,…,Nah
View Sourcewhere tl and vl are the input and the output of the hidden nodes of the action network, respectively. Nah is the total number of the hidden nodes in the action network.

The gradient descent can be used to update the weights of the actor network. In [52], the event-triggered finite-time optimal control scheme is designed for an uncertain nonlinear DT system.

4) Q Function for the DT Linear Quadratic Regulation:
For linear systems, the work of [32] proposed an action-dependent function (Q-function) instead of value function in the Bellman equation to avoid knowing the system dynamics. This is termed in the literature as Q-learning [53], [54].

Based on (3) and (5), the DT Q-function is defined as
Q(x(k),u(k))=xT(k)Qx(k)+uT(k)Ru(k)+xT(k+1)Px(k+1),∀u,x.(11)
View SourceUsing the dynamics (4), the Q-function (11) becomes
Q(x(k),u(k))=[x(k)u(k)]T[Q+ATPABTPAATPBR+BTPB][x(k)u(k)].
View SourceDefine
Q(x(k),u(k))==[x(k)u(k)]T[SxxSuxSxuSuu][x(k)u(k)]ZT(k)SZ(k)
View Sourcefor a kernel matrix S .

By applying the stationarity condition (∂Q(x(k),u(k))/ ∂u(k))=0 , one has
u⋆(k)=−(R+BTPB)−1BTPAx(k)
View Sourceand
u⋆(k)=−S−1uuSuxx(k).
View Source

Algorithm 2 is a model-free learning approach. This algorithm converges to the global optimal solution, on condition that a persistence of excitation (PE) condition is satisfied [55]. The PE condition guarantees the uniqueness of the policy evaluation step at each iteration. However, full information of the states of the system is needed. In [56], output-feedback (OPFB) RL algorithms are derived for linear systems. These algorithms do not require any knowledge of the system dynamics and, as such, are similar to Q-learning and they have an added advantage of requiring only measurements of input/output data and not the full system state.

Algorithm 2 Q-Learning Algorithm for DARE
procedure

Given admissible policy u0(k)

for j=0,1,… given uj , solve for the value Sj+1 using Bellman equation
ZT(k)Sj+1Z(k)=xT(k)Qx(k)+uTj(k)Ruj(k)+ZT(k+1)Sj+1Z(k+1),
View Sourceon convergence, set Sj+1=Sj .

Update the control policy uj+1(k) using
uj+1(k)=−(Suu)−1j+1(Sux)j+1x(k).
View Source

go to 3

end procedure

D. Optimal Tracking Problem
The goal now is to design an optimal control input to make the states of the system x(k) follow a desired reference trajectory xd(k) . Let us now define the tracking error e(k) as
e(k)=x(k)−xd(k).
View SourceIn the tracking problem, the control input consists of two terms: a feedforward term that guarantees tracking and a feedback term that stabilizes the system.

The feedforward term can be obtained using the dynamics inversion concept as
ud(k)=g(xd(k))−1(xd(k+1)−f(xd(k))).
View SourceConsider the following cost functional:
J(e(k),ue(k))=∑i=k∞(eT(i)Qee(i)+uTe(i)Reue(i))
View Sourcewhere Qe⪰0 and Re=RTe≻0 . The feedback input can be found by applying the following stationarity condition, ∂J(e,ue)/∂ue=0 as:
u⋆e(k)=−12R−1egT(k)∂J(e(k+1))∂e(k+1).
View SourceThen, the optimal control input including both feedback and feedforward terms is
u⋆(k)=ud(k)+u⋆e(k).
View Source

Obtaining the feedforward part of the control input needs complete knowledge of the system dynamics and the reference trajectory dynamics. In [57] and [58], a new formulation is developed that gives both feedback and feedforward parts of the control input simultaneously and thus enables RL algorithms to solve the tracking problems without requiring the complete knowledge of the system dynamics.

Assume now that the reference trajectory is generated by the following command generator model:
xd(k+1)=ψ(xd(k))
View Sourcewhere xd(k)∈Rn . Then, an augmented system can be constructed in terms of the tracking error e(k) and the reference trajectory xd(k) as
=≡[e(k+1)xd(k+1)][f(e(k)+xd(k))−ψ(xd(k))ψ(xd(k))]+[g(e(k)+xd(k))0]u(k)F(X(k))+G(X(k))u(k)
View Sourcewhere the augmented state is
X(k)=[e(k)xd(k)].
View Source

The new cost functional is defined as
J(x(0),xd(0),u(k))=∑i=0∞γi−k×((x(k)−xd(k))TQ(x(k)−xd(k))+uT(i)Ru(i))
View Sourcewhere Q⪰0 and R=RT≻0 . The value function in terms of the states of the augmented system is written as
V(X(k))==∑i=k∞γi−k(U(X(k),u(k))∑i=k∞γi−k(XT(i)QTX(i)+uT(i)Ru(i))(12)
View Sourcewhere
QT=[Q000]
View Sourceand 0<γ≤1 is the discount factor.

Remark 2:
Note that it is essential to use a discounted performance function for the proposed formulation. This is because if the reference trajectory does not go to zero, which is the case of most real applications, then the value is infinite, without the discount factor as the control input contains a feedforward part that depends on the reference trajectory and thus uT(k)Ru(k) does not go to zero as time goes to infinity.

A difference equivalent to (12) is
V(X(k))=U(X(k),u(k)+γV(X(k+1)).
View SourceThe Hamiltonian for this problem is given as
H(X(k),u(k),V)=XT(k)QTX(k)+uT(k)Ru(k) +γV(x(k+1))−V(x(k)).
View SourceThe optimal value can be found [59]
V⋆(X(k))=minu[U(X(k),u(k))+γV⋆(X(k+1))]
View Sourcewhich is just the DT HJB equation. The optimal control is then given as
u⋆(X(k))==argminu[U(X(k),u(k))+γV⋆(X(k+1))]−γ2R−1GT(X)(∂V⋆(X(k+1)∂X(k+1)).
View Source

E. Approximate Solution Using RL
The solution of the DT HJB tracking equation can be approximated as follows.

1) Offline PI Algorithm:
The PI algorithm is used to find the solution of DT HJB tracking by iterating on the solution of the Bellman equation.

The augmented system dynamics must be known in order to update the control input in Algorithm 3. Convergence properties of Algorithm 3 are similar to Algorithm 1 and are not discussed here.

Algorithm 3 PI Algorithm to Find the Solution of Tracking HJB
procedure

Given admissible policy u0(k)

for j=0,1,… given uj , solve for the value Vj+1(x) using Bellman equation
Vj+1(X(k))=XT(k)QTX(k)+uTj(k)Ruj(k)+γVj+1(X(k+1))
View Sourceon convergence, set Vj+1(X(k))=Vj(X(k)) .

Update the control policy uj+1(k) using
uj+1(X(k))=−γ2R−1GT(X)(∂Vj+1(X(k+1))∂X(k+1)).
View SourceRight-click on figure for MathML and additional features.

Go to 3

end procedure

2) Online Actor and Critic Approximators:
To obviate the requirement of complete knowledge of the system dynamics or reference trajectory dynamics, an actor-critic structure, similar to (6) and (7), is developed in [57] for solving the nonlinear optimal tracking problem. In [58], the Q-learning is used to find the optimal solution for linear systems. Kiumarsi et al. [60] presented PI and VI algorithms to solve the linear quadratic tracker (LQT) ARE online without requiring any knowledge of the system dynamics and information of the states of the system only using the measured input and output data.

F. H∞ Control of DT Systems
The H∞ control problem can be considered as a zero-sum game, where the controller and the disturbance inputs are considered as minimizing and maximizing players, respectively [61]–[62][63][64]. In the linear systems, the optimal solution to the zero-sum game problem leads to solving the GARE.

Consider now the dynamics with an added disturbance input
x(k+1)=f(x(k))+g(x(k))u(k)+h(x(k))d(k)(13)
View Sourcewhere x(k)∈Rn is a measurable state vector, u(k)∈Rm is the control input, d(k)∈Rq is the disturbance input, f(x(k))∈Rn is the drift dynamics, g(x(k))∈Rn×m is the input dynamics, and h(x(k))∈Rn×q is the disturbance input dynamics.

Define the cost functional to be optimized as
J(x(0),u(k),d(k))=∑i=0∞(Q(x)+uT(i)Ru(i)−β2dT(i)d(i))
View Sourcewhere Q(x)⪰0 , R=RT≻0 , and β≥β⋆≥0 with β⋆ the smallest β such that the system is stabilized. The value function with feedback control and disturbance policies can be defined as
V(x(k),u(k),d(k))=∑i=k∞(Q(x)+uT(i)Ru(i)−β2dT(i)d(i)).
View Source

A difference equivalent to this is
V(x(k))=Q(x)+uT(k)Ru(k)−β2dT(k)d(k)+V(x(k+1)).
View SourceRight-click on figure for MathML and additional features.We can find the optimal value, by solving a zero-sum differential game as
V⋆(x(k))=minumaxdJ(x(0),u,d)
View SourceRight-click on figure for MathML and additional features.subject to (13). It is worth noting that u(k) is the minimizing player while d(k) is the maximizing one.

In order to solve this zero-sum game, one needs to solve the following Hamilton–Jacobi–Isaacs (HJI) equation:
V⋆(x(k))=Q(x)+u⋆T(k)Ru⋆(k)−β2d⋆T(k)d⋆(k)+V⋆(x(k+1)).
View SourceGiven a solution V⋆ to this equation, one can find the optimal control and the worst case disturbance as
u⋆(k)=−12R−1gT(x)(∂V⋆(x(k+1))∂x(k+1))
View Sourceand
d⋆(k)=12β2hT(x)(∂V⋆(x(k+1))∂x(k+1))
View SourceRight-click on figure for MathML and additional features.respectively.

G. Approximate Solution Using RL
In the following sections, on-policy and off-policy RL algorithms are presented to solve the HJI and GARE solutions by successive approximations to the Bellman equations.

1) On-Policy RL to Solve H∞ Control of DT Systems:
The following on-policy RL algorithm is given to solve the HJI.

The PI algorithm (Algorithm 4) is an offline algorithm and requires complete knowledge of the system dynamics. The actor-critic structure can be used to design an online PI algorithm that simultaneously updates the value function and policies and does not require the knowledge of the drift dynamics. The value function and control input are approximated as (6) and (7), respectively. The disturbance input is approximated as
d^j(k)=W^Tdjσd(x(k))=∑i=1Ndw^idjσdi(x(k))
View Sourcewhere σdi(x(k)) is the basis function for the approximator, W^dj=[w^1djw^2dj…w^Nddj]T is the weight vector, and Nd is the number of basis functions used.

Algorithm 4 PI Algorithm to Solve the HJI
procedure

Given admissible policies u0(k) and d0(k)

for j=0,1,… given uj and dj , solve for the value Vj+1(x(k)) using Bellman equation
Vj+1(x(k))=Q(x)+uTj(k)Ruj(k)−β2dTj(k)dj(k)+Vj+1(x(k+1)),
View Sourceon convergence, set Vj+1(x(k))=Vj(x(k)) .

Update the control policy uj+1(k) and disturbance policy dj+1(k) using
uj+1(k)=−12R−1gT(x)(∂Vj+1(k+1)∂x(k+1)),
View SourceRight-click on figure for MathML and additional features.
dj+1(k)=12β2hT(x)(∂Vj+1(x(k+1))∂x(k+1)).
View SourceRight-click on figure for MathML and additional features.

Go to 3.

end procedure

The weights of the value function and control input approximators are updated using gradient descent as (8) and (8), respectively. The gradient descent can also be used to update the weights of the disturbance input approximator.

A Q-learning algorithm is presented in [65] to find the optimal control input and worst case disturbance input for linear systems without requiring any knowledge of the system dynamics. However, the disturbance input needs to be updated in a prescribed manner. An off-policy RL algorithm is presented in [66] that does not need any knowledge of the system dynamics and the disturbance input does not need to be updated in a prescribed manner. Fig. 2 shows a schematic of off-policy RL for solving the zero-sum game problem.


Fig. 2.
Off-policy RL for H∞ control. The behavior disturbance policy is the actual disturbance from the environment that cannot be specified. On the other hand, the learned disturbance policy uses the collected data to learn the worst case policy.

Show All

2) Off-Policy RL for Solving Zero-Sum Game Problem of DT Systems [66]:
Consider the following system description:
x(k+1)=Ax(k)+Bu(k)+Dd(k)(14)
View Sourcewhere x∈Rn, u∈Rm , and d∈Rq are the state, control, and disturbance inputs, and A,B , and D are constant matrices of appropriate dimensions. To derive an off-policy RL algorithm, the original system (14) is rewritten as
x(k+1)=Akx(k)+B(K1jx(k)+u(k))+D(K2jx(k)+d(k))(15)
View Sourcewhere Ak=A−BK1j−DK2j .

In (15), the estimation policies are uj(k)=−K1jx(k) and dj(k)=−K2jx(k) . By contrast, u(k) and d(k) are the behavior policies that are actually applied to (14) to generate the data for learning.

Using the Taylor expansion of the value function V(x(k)) at point x(k+1) for (14), the value function (5) yields
=xT(k)Pj+1x(k)−xT(k+1)Pj+1x(k+1)xT(k)Qx(k)+xT(k)(K1j)TRK1jx(k)−β2xT(k)(K2j)T×K2jx(k)−(u(k)+K1jx(k))TBTPj+1x(k+1)−(u(k)+K1jx(k))TBTPj+1Akx(k)−(K2jx(k)+d(k))TDTPj+1x(k+1)−(K2jx(k)+d(k))TDTPj+1Akx(k).(16)
View SourceRight-click on figure for MathML and additional features.

Remark 3:
Note that (16) does not explicitly depend on K1j+1 and K2j+1 . Also, complete knowledge of the system dynamics is required for solving (16). In the following, it is shown how to find (Pj+1,K1j+1,K2j+1) simultaneously by (16) without knowing any knowledge of the system dynamics.

Given any matrix M∈Rn×m , vec(M)∈Rnm×1 is transpose of a vector formed by stacking the rows of matrix M .

Using aTWb=(bT⊗aT)vec(W) , (14), and Ak=A−BK1j−DK2j , the off-policy Bellman equation (16) can be rewritten as
(xT(k)⊗xT(k))vec(L1j+1)−(xT(k+1)⊗xT(k+1))vec(L1j+1)+2(xT(k)⊗(u(k)+K1jx(k))T)vec(L2j+1)−((K1jx(k)−u(k))T⊗(u(k)+K1jx(k))T)vec(L3j+1)+2(xT(k)⊗(d(k)+K2jx(k))T)vec(L4j+1)−((K1jx(k)−u(k))T⊗(d(k)+K2jx(k))T)vec(L5j+1)+((d(k)−K2jx(k))T⊗(u(k)+K1jx(k))T)vec(L6j+1)+((d(k)−K2jx(k))T⊗(d(k)+K2jx(k))T)vec(L7j+1) =xT(k)Qx(k)+xT(k)(K1j)TRK1jx(k)  −β2xT(k)(K2j)TK2jx(k)(17)
View SourceRight-click on figure for MathML and additional features.with L1j+1=Pj+1 , L2j+1=BTPj+1A , L3j+1=BTPj+1B , L4j+1=DTPj+1A , L5j+1=DTPj+1B , L6j+1=BTPj+1D , and L7j+1=DTPj+1D that are the unknown variables in the Bellman equation (17). Then, using (17), one has
ψj[vec(L1j+1)Tvec(L2j+1)Tvec(L3j+1)Tvec(L4j+1)Tvec(L5j+1)Tvec(L6j+1)Tvec(L7j+1)T]T=ϕj(18)
View Sourcewhere ϕj=[(ϕ1j)T…(ϕsj)T]T and ψj=[(ψ1j)T…(ψsj)T]T with ϕij and ψij defined as in [66]. One can solve (18) using a least-squares (LS) method.

Algorithm 5 Off-Policy PI to Find the Solution of GARE
procedure

Set the iteration number j=0 and start with a admissible control policy u(k)=−K1x(k)+e(k) where e(k) is probing noise.

while ∥K1j+1−K1j∥≥ϵ and ∥K2j+1−K2j∥≥ϵ do

For j=0,1,2,… , solve (18) using LS
ψj[vec(L1j+1)Tvec(L2j+1)Tvec(L3j+1)Tvec(L4j+1)Tvec(L5j+1)Tvec(L6j+1)Tvec(L7j+1)T]T=ϕj.
View Source

Update the control and disturbance gains as
K1j+1=−(R+L3j+1+L6j+1(β2I−L7j+1)−1L5j+1)−1×(L2j+1+L6j+1(β2I−L7j+1)−1L4j+1),K2j+1=(L7j+1−β2I−L5j+1(R+L3j+1)−1L6j+1)−1×(L4j+1−L5j+1(R+L3j+1)−1L2j+1).
View Source

j=j+1

end while

end procedure

SECTION III.Optimal Control of CT Systems and Online Solutions
Consider the nonlinear time-invariant system given as
x˙(t)=f(x(t))+g(x(t))u(t)y(t)=l(x(t))(19)
View SourceRight-click on figure for MathML and additional features.where x(t)∈Rn , u(t)∈Rm , and y(t)∈Rp represent the state of the system, the control input, and the output of the system, respectively. f(x(t))∈Rn is the drift dynamics, g(x(t))∈Rn×m is the input dynamics, and l(x(t))∈Rp is the output dynamics. It is assumed that f(0)=0 and f(x(t))+g(x(t))u(t) is locally Lipschitz and the system is stabilizable.

A. Optimal Regulation Problem
The goal in optimal regulation is to design an optimal control input to assure the states of the system (19) converge to zero by minimizing a cost functional. The cost functional is defined as
J(x(0),u)=∫∞0r(x,u)dt≡∫∞0(Q(x)+uTRu))dt
View SourceRight-click on figure for MathML and additional features.where Q(x)⪰0 and R=RT≻0 . The value function for the admissible control policy can be defined as
V(x,u)=∫∞tr(x,u)dτ≡∫∞t(Q(x)+uTRu))dτ.
View SourceA differential equivalent to this is
r(x,u)+∂V∂xT(f(x)+g(x)u))=0,V(0)=0
View Sourceand the Hamiltonian is given by
H(x,u,∂V∂xT)=r(x,u)+∂V∂xT(f(x)+g(x)u).
View SourceRight-click on figure for MathML and additional features.The optimal value is given by the Bellman optimality equation
r(x,u⋆)+∂V⋆∂xT(f(x)+g(x)u⋆)=0(20)
View SourceRight-click on figure for MathML and additional features.which is just the CT HJB equation. The optimal control is then given as
u⋆(t)==argminu(r(x,u)+∂V⋆∂xT(f(x)+g(x)u))−12R−1gT(x)(∂V⋆∂x).(21)
View Source

B. Approximate Solution Using RL
The following on-policy and off-policy integral RL (IRL) algorithms are presented to approximate HJB solution by iterating on the Bellman equations.

1) On-Policy IRL:
In [35] and [67], an equivalent formulation of the Bellman equation that does not involve the dynamics is found to be
V(x(t))=∫t+Tt(Q(x(τ))+uT(τ)Ru(τ))dτ+V(x(t+T))(22)
View SourceRight-click on figure for MathML and additional features.for any time t≥0 and time interval T>0 . This equation is called IRL Bellman equation. The following PI algorithm can be implemented by iterating on the above IRL Bellman equation and updating the control policy.

The IRL algorithm (Algorithm 6) is online and does not require the knowledge of the drift dynamics. To implement Step 3 of Algorithm 6, a neural-network-type structure, similar to (6) and (7), is used in [67] to approximate the value function. This algorithm is a sequential RL algorithm in the sense that the actor (policy improvement) and critic (policy evaluation) are updated sequentially. Synchronous update laws for actor and critic were first introduced in [36] to update both actor and critic simultaneously while assuring system stability. Later, in [68] and [69], synchronous actor-critic structure was augmented with system identification to avoid complete knowledge of the system dynamics.

Algorithm 6 On-Policy IRL Algorithm to Find the Solution of HJB
procedure

Given admissible policy u0

for j=0,1,… given uj , solve for the value Vj+1(x) using Bellman equation
Vj+1(x(t))=∫t+Tt(Q(x)+uTjRuj))dτ+Vj+1(x(t+T)),
View Sourceon convergence, set Vj+1(x)=Vj(x) .

Update the control policy uj+1(k) using
uj+1(t)=−12R−1gT(x)(∂Vj+1(x)∂x).
View Source

Go to 3.

end procedure

2) On-Policy IRL With Experience Replay Learning Technique [70]–[71][72]:
To speed up and obtain an easy-to-check condition for the convergence of the IRL algorithm, the recent transition samples are stored and repeatedly presented to the gradient-based update rule. A similar condition was proposed in [73] for adaptive control systems. This is a gradient-decent algorithm that does not only minimize the instantaneous temporal difference (TD) error but also minimize the TD errors for the stored transition samples. Assume now that the value function V(x) can be uniformly approximated as in the IRL Bellman equation (22)
V^(x)=W^Tcϕ1(x),∀x
View SourceRight-click on figure for MathML and additional features.where ϕ1(x):Rn→RN is the basis function vector and N is the number of basis functions. Therefore, the approximate IRL Bellman equation becomes
eB=Δϕ1(t)TW^c+∫tt−T(Q(x)+u^TRu^)dτ
View SourceRight-click on figure for MathML and additional features.where Δϕ1(t)=ϕ1(t)−ϕ1(t−T) and eB is the TD error after using current critic approximator weights. To collect data in the history stack, consider Δϕ1(tj) as evaluated values of Δϕ1 at the recorded time tj . Then, define the Bellman equation error (TD error) at the recorded time tj using the current critic weights estimation W^c as
(eB)j=Δϕ1(tj)TW^c+∫tjtj−T(Q(x(τ))+u^T(τ)Ru^(τ))dτ.
View Source

The experience replay-based gradient-decent algorithm for the critic NN is now given as
W^˙c=−αcΔϕ1(t)(Δϕ1(t)TΔϕ1(t)+1)2eB−αc∑j=1lΔϕ1(tj)(Δϕ1(tj)TΔϕ1(tj)+1)2(eB)j.
View SourceThe first term is a gradient update law for the TD error and the last term minimizes its stored samples in the history stack.

3) Event-Triggered On-Policy RL [74]:
The event-triggered version of the optimal controller uses the sampled state information instead of the true one and (21) becomes
u⋆(x^i)=−12R−1gT(x^i)(∂V⋆(x^i)∂x)∀t∈(ri−1,ri] and i∈N(23)
View SourceRight-click on figure for MathML and additional features.where ri is the i th consecutive sampling instant and x^i=x(ri) .

Using the event-triggered controller (23), the HJB equation (20) becomes ∀x,x^i∈Rn
∂V⋆∂xT(f(x)−12g(x)R−1gT(x^i)(∂V⋆(x^i)∂x))+Q(x)+14∂V⋆(x^i)∂xTg(x^i)(R−1)TR−1g(x^i)T∂V⋆(x^i)∂x =(u⋆(x^i)−u⋆c)TR(u⋆(x^i)−u⋆c)(24)
View Sourcewhere u⋆c is given by (21).

To solve the event-triggered HJB equation (24), the value function is approximated on a compact set Ω as
V⋆(x)=W⋆Tcϕ(x)+ϵc(x),∀x∈Rn(25)
View Sourcewhere ϕ(x):Rn→Rk is the basis function set vector, k is the number of basis functions, and ϵc(x) is the approximation error. Based on this, the optimal event-triggered controller in (23) can be rewritten as
u⋆(x^i)=−12R−1gT(x^i)(∂ϕ(x^i)∂xTW⋆c+∂ϵc(x^i)∂x) t∈(ri−1,ri1].(26)
View SourceThe optimal event-triggered controller (26) can be approximated by the actor for all t∈(ri−1,ri] as
u⋆(x^i)=W⋆Tuϕu(x^i)+ϵu(x^i),∀x^i, i∈N(27)
View SourceRight-click on figure for MathML and additional features.where ϕu(x^i):Rn→Rh is the basis function set vector, h is the number of basis functions, and ϵu(x^i) is the approximation error.

The value function (25) and the optimal policy (27) using current estimates W^c and W^u , respectively, of the ideal weights W⋆c and W⋆u are given by the following critic and actor approximators:
V^(x)=u^(x^i)=W^Tcϕ(x),∀xW^Tuϕu(x^i),∀x^i.
View SourceThe Bellman error is defined as
ec=W^Tc∂ϕ∂x(f(x)+g(x)u^(x^i)))+r(x,u^)
View Sourcewith r(x,u^)=Q(x)+u^(x^i)TRu^(x^i) .

The weights W^c are tuned to minimize K=(1/2)eTcec as
Wc^˙=−αc∂K∂W^c=−αcw(wTw+1)2(wTW^c+r(x,u^))
View Sourcewith w=(∂ϕ/∂x)(f(x)+g(x)u^(x^i))) .

In order to find the update law for the actor approximator, the following error is defined:
eu=W^Tuϕu(x^i)+12R−1gT(x^i)(∂ϕ(x^i)∂x)TW^c,∀x^i.
View SourceThe weights W^u are tuned to minimize Eu=(1/2)eTueu as
Wu^˙=0,for t∈(ri−1,ri]
View SourceRight-click on figure for MathML and additional features.and the jump equation to compute W^u(r+j) given by
W^+u=W^u(t)−αuϕu(x(t))×(W^Tuϕu(x(t))+12R−1gT(x(t))∂ϕ(x(t))∂xTW^c)Tfor t=ri.
View SourceRight-click on figure for MathML and additional features.

The convergence and stability are proved in [74]. The work of [75] extended [74] to systems with input constraints and without requiring the knowledge of the system dynamics. Event-based RL approaches are also presented in [76]–[77][78] for interconnected systems.

4) Off-Policy IRL [79], [80]:
In order to develop the off-policy IRL algorithm, the system dynamics (19) is rewritten as
x˙(t)=f(x(t))+g(x(t))uj(t)+g(x(t))(u(t)−uj(t))(28)
View SourceRight-click on figure for MathML and additional features.where uj(t) is the policy to be updated. By contrast, u(t) is the behavior policy that is actually applied to the system dynamics to generate the data for learning.

Differentiating V(x) along with the system dynamics (28) and using uj+1(t)=−(1/2)R−1gT(x)(∂Vj(x)/∂x) give
V˙j==(∂Vj(x)∂x)T(f+guj)+(∂Vj(x)∂x)Tg(u−uj)−Q(x)−ujTRuj−2uj+1TR(u−uj).
View SourceIntegrating from both sides of the above equation yields the off-policy IRL Bellman equation
Vj(x(t+T))−Vj(x(t))=∫t+Tt(−Q(x)−uTjRuj−2uj+1TR(u−uj))dτ.(29)
View Source

Iterating on the IRL Bellman equation (29) yields the following off-policy IRL algorithm.

Remark 4:
Note that for a fixed control policy u(t) (the policy that is applied to the system), the off-policy IRL Bellman equation (29) can be solved for both value function Vj and updated policy uj+1 , simultaneously without requiring any knowledge about the system dynamics.

To implement the off-policy IRL algorithm (Algorithm 7), the actor-critic structure, similar to (6) and (7), is used to approximate the value function and control policy [79].

Algorithm 7 Off-Policy IRL Algorithm to Find the Solution of HJB
procedure

Given admissible policy u0

for j=0,1,… given uj , solve for the value Vj and uj+1 using off-policy Bellman equation
Vj(x(t+T))−Vj(x(t))=∫t+Tt(−Q(x)−uTjRuj−2uj+1TR(u−uj))dτ.
View Sourceon convergence, set Vj+1=Vj .

Go to 3.

end procedure

For a linear system, the off-policy IRL Bellman equation (see [81]) is given as
xT(t+T)Pjx(t+T)−xT(t)Pjx(t)=∫t+Tt(−xTQx−uTjRuj−2uj+1TR(u−uj))dτ.(30)
View SourceRight-click on figure for MathML and additional features.

Iterating on the Bellman equation (30) yields the off-policy IRL algorithm for the linear systems.

5) Robust Off-Policy IRL [79]:
Consider the following uncertain nonlinear system:
x˙(t)=w˙(t)=f(x(t))+g(x(t))[u(t)+Δ(x,w)]Δw(x,w)(31)
View SourceRight-click on figure for MathML and additional features.where x(t)∈Rn is the measured component of the state available for feedback control, w(t)∈Rp is the unmeasurable part of the state with unknown order p , u(t)∈R is the control input, Δw∈Rp and Δ∈R are unknown locally Lipschitz functions, and f(x(t))∈Rn and g(x(t))∈Rn are the drift dynamics and input dynamics, respectively. In order to develop the robust off-policy IRL algorithm, the system dynamics (31) is rewritten as
x˙(t)=f(x(t))+g(x(t))uj(t)+g(x(t))vj(t))
View Sourcewhere vj=u+Δ−uj .

Differentiating V(x) along (31) and using uj+1(t)=−(1/2)R−1gT(x)(∂Vj(x)/∂x) give
V˙j==(∂Vj(x)∂x)T(f+guj+gvj)−Q(x)−ujTRuj−2uj+1TRvj.
View SourceIntegrating both sides of the above equation yields the robust off-policy IRL Bellman equation
Vj(x(t+T))−Vj(x(t))=∫t+Tt(−Q(x)−uTjRuj−2uj+1TRvj)dτ.(32)
View Source

Using an actor-critic structure, similar to (6) and (7), for Vj and uj+1 in (32), yields
∑i=1Ncw^icj[ϕci(x(t+T))−ϕci(x(t))]=∫t+Tt⎛⎝−Q(x)−u^TjRu^j−2(∑i=1Naw^iajσai(x(k)))TRvj⎞⎠dτ.(33)
View SourceRight-click on figure for MathML and additional features.Iterating on the IRL Bellman equation (33) provides the following robust off-policy IRL algorithm.

Algorithm 8 Robust Off-Policy IRL Algorithm
procedure

Given admissible policy u0

for j=0,1,… given uj , solve for the value Vj and uj+1 using off-policy Bellman equation
∑i=1Ncw^icj[ϕci(x(t+T))−ϕci(x(t))]=∫t+Tt⎛⎝−Q(x)−u^TjRu^j−2(∑i=1Naw^iajσai(x(k)))TRvj⎞⎠dτ,
View Sourceon convergence, set Vj+1=Vj .

Go to 3.

end procedure

C. Optimal Tracking Problem
The goal here is to design an optimal control input to make the states of the system x(t) track a desired reference trajectory xd(t) .

Define the tracking error as
e(t)=x(t)−xd(t).
View Source

Similar to DT systems, standard techniques find the feedback and feedforward parts of the control input separately using complete knowledge of the system dynamics. In [82] and [83], a new formulation is developed that gives both feedback and feedforward parts of the control input simultaneously and thus enables RL algorithms to solve the tracking problems without requiring complete knowledge of the system dynamics.

Assume that the reference trajectory is generated by the command generator model
x˙d(t)=hd(xd(t))
View SourceRight-click on figure for MathML and additional features.where xd(t)∈Rn . An augmented system can be constructed in terms of the tracking error e(t) and the reference trajectory xd(t) as
X˙(t)=[e˙(t)x˙d(t)]=[f(e(t)+xd(t))−hd(xd(t))hd(xd(t))]+[g(e(t)+xd(t))0]u(t)≡F(X(t))+G(X(t))u(t)(34)
View Sourcewhere the augmented state is
X(t)=[e(t)xd(t)].
View SourceThe cost functional is defined as
=J(x(0),xd(0),u(t))∫0∞e−η(τ−t)×((x(τ)−xd(τ))TQ(x(τ)−xd(τ)) +uT(τ)Ru(τ))dτ
View Sourcewhere Q⪰0 and R=RT≻0 . The value function in terms of the states of the augmented system yields
V(X(t))==∫∞te−η(τ−t)r(X,u)dτ(XT(τ)QTX(τ)+uT(τ)Ru(τ))dτ(35)
View Sourcewhere
QT=[Q000]
View Sourceand η≥0 is the discount factor.

A differential equivalent to this is the Bellman equation
r(X,u)−ηV+∂V∂XT(F(x)+G(X)u)=0,V(0)=0
View SourceRight-click on figure for MathML and additional features.and the Hamiltonian is given by
H(X,u,∂V∂XT)=r(X,u)−ηV+∂V∂XT(F(X)+G(X)u).
View SourceThe optimal value is given by
r(X,u⋆)−ηV⋆+∂V⋆∂XT(F(X)+G(X)u⋆)=0
View Sourcewhich is just the CT HJB tracking equation. The optimal control is then given as
u⋆(t)==argminu(r(X,u)−ηV⋆+∂V⋆∂XT(F(X)+G(X)u))−12R−1GT(x)(∂V⋆∂X).
View Source

D. Approximate Solution Using RL
Using the value function (35) and in a similar manner to the off-policy and on-policy IRL algorithms for the optimal regulation, the following off-policy and on-policy IRL algorithms are developed to find the optimal solution of CT HJB tracking equation.

1) On-Policy IRL:
Similar to Algorithm 6 for the optimal regulation, the following on-policy IRL algorithm is presented in [83] to solve the CT HJB equation online and without requiring the knowledge of the internal system dynamics.

The IRL algorithm (Algorithm 9) is an online algorithm and does not require the knowledge of the internal system dynamics. In [83], an actor-critic structure, similar to (6) and (7), is used to implement Algorithm 9. In [82], the quadratic form of value function, xT(t)Px(t) , is used in the IRL Bellman equation for the linear systems to find the solution of LQT ARE.

Algorithm 9 On-Policy IRL Algorithm to Find the Solution of HJB
procedure

Given admissible policy u(0)

for j=0,1,… given uj , solve for the value Vj+1(X) using Bellman equation
Vj+1(X(t))=∫t+Tte−ητ(XT(τ)QTX(τ)+uTj(τ)Ruj(τ))dτ+e−ηTVj+1(X(t+T)),
View SourceRight-click on figure for MathML and additional features.on convergence, set Vj+1(X)=Vj(X) .

Update the control policy uj+1(t) using
uj+1(t)=−12R−1gT(X)(∂Vj+1(X)∂X).
View SourceRight-click on figure for MathML and additional features.

Go to 3.

end procedure

2) Off-Policy IRL:
Similar to off-policy IRL for optimal regulation, for the augmented system (34) and the discounted value function (35), the following off-policy IRL algorithm is developed in [84] to avoid the requirement of the knowledge of the system dynamics.

To implement off-policy IRL algorithm (Algorithm 10), the actor-critic structure, similar to (6) and (7), is used to approximate the value function and control policy [84].

Algorithm 10 Off-Policy IRL Algorithm to Find the Solution of HJB
procedure

Given admissible policy u0

for j=0,1,… given uj , solve for the value Vj+1 and uj+1 using off-policy Bellman equation
e−ηTVj+1(X(t+T))−Vj+1(X(t))=∫t+Tte−ητ(−Q(X)−uTjRuj−2uj+1TR(u−uj))dτ,
View Sourceon convergence, set Vj+1=Vj .

Go to 3.

end procedure

Remark 5:
Most of the existing solutions to optimal control problems are presented for affine systems. Extensions of RL algorithms for nonaffine systems are considered in [85] for optimal regulation and in [86] for optimal tracking control problems.

E. H∞ Control of CT Systems
The CT system dynamics in the presence of disturbance input is considered as
x˙(t)=f(x(t))+g(x(t))u(t)+h(x(t))d(t)(36)
View SourceRight-click on figure for MathML and additional features.where x(t)∈Rn is a measurable state vector, u(t)∈Rm is the control input, d(t)∈Rq is the disturbance input, f(x(t))∈Rn is the drift dynamics, g(x(t))∈Rn×m is the input dynamics, and h(x(t))∈Rn×q is the disturbance input dynamics. The performance index to be optimized is defined as
J(x(0),u,d)=∫∞t(Q(x)+uTRu−β2dTd)dτ
View Sourcewhere Q(x)⪰0 , R=RT≻0 , and β≥β⋆≥0 with β⋆ the smallest β such that the system is stabilized. The value function can be defined as
V(x(t),u,d)=∫∞t(Q(x)+uTRu−β2dTd)dτ.
View Source

A differential equivalent to this is the Bellman equation
Q(x)+uTRu−β2dTd  +∂V∂xT(f(x)+g(x)u+h(x)d)=0V(0)=0
View Sourceand the Hamiltonian is given by
H(x,u,d,∂V∂xT)=Q(x)+uTRu−β2dTd+∂V∂xT(f(x)+g(x)u+h(x)d).
View Source

Define the two-player zero-sum differential game as
V⋆(x(t))=minumaxdJ(x(0),u,d)
View Sourcesubject to (36).

In order to solve this zero-sum game, one needs to to solve the following HJI equation:
Q(x)+(u⋆)TRu⋆−β2(d⋆)Td⋆+∂V⋆∂xT(f(x)+g(x)u⋆+h(x)d⋆)=0.
View SourceRight-click on figure for MathML and additional features.Given a solution to this equation, one has
u⋆=d⋆=−12R−1gT(x)∂V⋆∂x12β2hT(x)∂V⋆∂x.
View SourceRight-click on figure for MathML and additional features.

F. Approximate Solution Using RL
The following PI algorithms can be used to approximate the HJI solution by iterating on the Bellman equation.

1) On-Policy IRL:
In [87], an equivalent formulation of the Bellman equation that does not involve the dynamics is found to be
V(x(t−T))=∫tt−T(r(x(τ),u(τ),d(τ))dτ+V(x(t))
View Sourcefor any time t≥0 and time interval T>0 . This equation is called the IRL Bellman equation.

The PI algorithm (Algorithm 11) is an offline algorithm and requires complete knowledge of the system dynamics. Actor-critic structure as (6) and (7) can be used to implement an online PI algorithm that simultaneously updates the value function and policies and does not require the knowledge of the drift dynamics. Various update laws are developed for learning actor, critic, and disturbance approximators’ weights by minimizing the Bellman error [88]–[89][90].

Algorithm 11 On-Policy IRL for Regulation in Zero-Sum Games (H∞ Control)
procedure

Given admissible policy u0

for j=0,1,… given uj

for i=0,1,… set d0=0 , solve for the value V(i)j(x) using Bellman’s equation
Q(x)+(∂Vij∂x)T(f(x)+g(x)uj+h(x)di)+uTjRuj−β2(di)Tdi=0,Vij(0)=0,di+1=12β2hT(x)(∂Vij∂x),
View SourceRight-click on figure for MathML and additional features.on convergence, set Vj+1(x)=Vij(x) .

Update the control policy uj+1 using
uj+1=−12R−1gT(x)(∂Vj+1∂x).
View SourceRight-click on figure for MathML and additional features.

Go to 3.

end procedure

2) Off-Policy IRL [80]:
In order to develop the off-policy IRL algorithm for the H∞ control, the system dynamics (36) is rewritten as
x˙(t)=f(x(t))+g(x(t))uj(t)+g(x(t))(u(t)−uj(t)) +h(x(t))dj(t)+g(x(t))(d(t)−dj(t))
View Sourcewhere uj(t) and dj(t) are the policies to be updated. By contrast, u(t) and d(t) are the behavior policies that are actually applied to the system dynamics to generate the data for learning.

Differentiating V(x) along with the system dynamics (28) and using uj+1(t)=−(1/2)R−1gT(x)(∂Vj(x)/∂x) and dj+1(t)=(1/2β2)hT(x)(∂Vj(x)/∂x) give
V˙j=(∂Vj(x)∂x)T(f+guj+hdj)+(∂Vj(x)∂x)Tg(u−uj)+(∂Vj(x)∂x)Th(d−dj)=−Q(x)−ujTRuj+β2djTdj−2uj+1TR(u−uj)+2β2dj+1T(d−dj).
View SourceIntegrating from both sides of the above equation yields the H∞ off-policy IRL Bellman equation
Vj(x(t+T))−Vj(x(t))=∫t+Tt(−Q(x)−ujTRuj+β2djTdj−2uTj+1R(u−uj)+2β2dTj+1(d−dj))dτ.(37)
View Source

Iterating on the IRL Bellman equation (37) yields the following off-policy IRL algorithm.

Note that for a fixed control policy u(t) and the actual disturbance (the policies that are applied to the system), the off-policy IRL Bellman equation (37) can be solved for value function Vj , learned control policy uj+1 , and learned disturbance policy dj+1 simultaneously, and without requiring any knowledge about the system dynamics.

To implement the off-policy IRL algorithm (Algorithm 12), the actor-critic structure, similar to (6) and (7), is used to approximate the value function, control, and disturbance policies [80].

Algorithm 12 Off-Policy IRL Algorithm to Find the Solution of HJI
procedure

Given admissible policy u0

for j=0,1,… given uj and dj , solve for the value Vj , uj+1 and dj+1 using off-policy Bellman equation
Vj(x(t+T))−Vj(x(t))=∫t+Tt(−Q(x)−ujTRuj+β2djTdj−2uj+1TR(u−uj)+2β2dj+1T(d−dj))dτ,
View SourceRight-click on figure for MathML and additional features.on convergence, set Vj+1=Vj .

Go to 3.

end procedure

In [88], an off-policy IRL algorithm is presented to find the solution of optimal tracking control problem without requiring any knowledge about the system dynamics and reference trajectory dynamics.

G. Nash Games
In this section, online RL-based solutions to noncooperative games, played among N agents, are considered.

Consider the N -player nonlinear time-invariant differential game
x˙(t)=f(x(t))+∑j=1Ngj(x(t))uj(t)
View Sourcewhere x∈Rn is a measurable state vector, uj(t)∈Rmj are the control inputs, f(x)∈Rn is the drift dynamics, and gj(x)∈Rn×mj is the input dynamics. It is assumed that f(0)=0 and f(x)+∑Nj=1gj(x)uj is locally Lipschitz and that the system is stabilizable.

The cost functional associated with each player is defined as
Ji(x(0),u1,u2,…,uN)=≡∫∞0(ri(x,u1,…,uN))dt∫∞0(Qi(x)+∑j=1NuTjRijuj)dt∀i∈N:={1,2,…,N}
View SourceRight-click on figure for MathML and additional features.where Qi(⋅)⪰0 , Rii=RTii≻0,∀i∈N , and Rij=RTij⪰0, ∀j≠i∈N with N:={1,2,…,N} .

The value for each player can be defined as
Vi(x,u1,u2,…,uN)=∫∞t(ri(x,u1,…,uN))dτ,∀i∈N, ∀x,u1,u2,…,uN.
View SourceRight-click on figure for MathML and additional features.

Differential equivalents to each value function are given by the following Bellman equations:
ri(x,u1,…,uN)+∂Vi∂xT(f(x)+∑j=1Ngj(x)uj)=0Vi(0)=0,∀i∈N.
View Source

The Hamiltonian functions are defined as
=Hi(x,u1,…,uN,∂Vi∂xT)ri(x,u1,…,uN)+∂Vi∂xT(f(x)+∑j=1Ngj(x)uj),∀i∈N.
View SourceBy applying the stationarity conditions, the associated feedback control policies are given by
u⋆i==argminuiHi(x,u1,…,uN,∂V⋆i∂xT)−12R−1iigTi(x)∂Vi∂x,∀i∈N.
View Source

After substituting the feedback control policies into the Hamiltonian, one has the coupled Hamilton–Jacobi (HJ) equations
0=Qi(x)+14∑j=1N∂Vj∂xTgj(x)R−TjjRijR−1jjgTj(x)∂Vj∂x+∂Vi∂xT(f(x)−12∑j=1Ngj(x)R−1jjgTj(x)∂Vj∂x),∀i∈N.(38)
View Source

H. Approximate Solution Using RL
A PI algorithm is now developed by iterating on the Bellman equation to approximate the solution to the coupled HJ equations.

Algorithm 13 PI for Regulation in Nonzero-Sum Games
procedure

Given N -tuple of admissible policies μki(0), ∀i∈N

while ∥Vμ(k)i−Vμ(k−1)i∥≥ϵiac,∀i∈N do

Solve for the N -tuple of costs Vki(x) using the coupled Bellman equations
Qi(x)+∂Vki∂xT(f(x)+∑i=1Ngi(x)μki)+μkiTRiiμki+∑j=1NμkjTRijμkj=0,Vμki(0)=0.
View SourceRight-click on figure for MathML and additional features.

Update the N -tuple of control policies μk+1i, ∀i∈N using
μk+1i=−12R−1iigTi(x)∂Vki∂xT.
View SourceRight-click on figure for MathML and additional features.

k=k+1

end while

end procedure

1) On-Policy IRL:
It is obvious that (38) requires the complete knowledge of the system dynamics. In [87], an equivalent formulation of the coupled IRL Bellman equation that does not involve the dynamics is given as
Vi(x(t−T))=∫tt−Tri(x(τ),u1(τ),…,uN(τ))dτ+Vi(x(t)),∀i∈N
View SourceRight-click on figure for MathML and additional features.for any time t≥0 and time interval T>0 .

Let the value functions V⋆i be approximated on a compact set Ω as
V⋆i(x)=WTiϕi(x)+ϵi(x),∀x, ∀i∈N
View Sourcewhere ϕi(x):Rn→RKi, ∀i∈N , are the basis function basis set vectors, Ki, ∀i∈N , is the number of basis functions, and ϵi(x) is the approximation error.

Assuming current weight estimates W^ic,∀i∈N , the outputs of the critic are given by
V^i(x)=W^Ticϕi(x),∀x, i∈N.
View Source

Using a similar procedure as used for zero-sum games, the update laws can be rewritten as
=W˙ic−αiΔϕi(t)(Δϕi(t))TΔϕi(t)+1)2×(Δϕi(t)TW^ic+∫tt−T(Qi(x)+u^TiRiiu^i+∑j=1Nu^TjRiju^j)dτ),∀i∈N
View Sourceand
W^˙iu=−αiu((FiW^iu−LiΔϕi(t)TW^ic)−14∑j=1N(∂ϕi∂xgi(x)R−TiiRijR−1iigi(x)T∂ϕi∂xT)W^iuΔϕi(t)T(Δϕi(t)TΔϕi(t)+1)2W^jc),∀i∈N
View SourceRight-click on figure for MathML and additional features.respectively, with Δϕi(t):=ϕi(t)−ϕi(t−T) .

In [91], a model-free solution to nonzero-sum Nash games is presented. To obviate the requirement of complete knowledge of the system dynamics, system identification is used in [92].

SECTION IV.Graphical Games
Interactions among agents are modeled by a fixed strongly connected graph G=(VG,EG) defined by a finite set VG={n1,…,nN} of N agents and a set of edges EG⊆VG×VG that represent interagent information exchange links. The set of neighbors of a node ni is defined as the set of nodes with edges incoming to ni and is denoted by Ni={nj:(nj,ni)∈EG} . The adjacency matrix is AG=[αij]N×N with weights αij>0 if (nj,ni)∈EG and zero otherwise. The diagonal degree matrix D of the graph G is defined as D=diag(di) with the weighted degree di=∑j∈Niαij .

Let the agents dynamics be modeled as
x˙i(t)=Axi(t)+Biui(t),xi(0)=xi0, t≥0
View Sourcewhere xi(t)∈Rn is a measurable state vector, ui(t)∈Rmi, i∈N:={1,…,N} , is each control input (or player), and A∈Rn×n and Bi∈Rn×mi, i∈N , are the plant and input matrices, respectively.

The leader node/exosystem dynamics are
x˙0=Ax0.
View Source

The agents in the network seek to cooperatively asymptotically track the state of a leader node/exosystem, i.e., xi(t)→x0(t),∀i∈N while simultaneously satisfying distributed cost functional.

Define the neighborhood tracking error for every agent as
δi:=∑j∈Niαij(xi−xj)+gi(xi−x0),∀i∈N(39)
View Sourcewhere gi is the pinning gain. gi≠0 if agent is pinned to the leader node. The dynamics of (39) are given by
δ˙i=Aδi+(di+gi)Biui−∑j∈NiαijBjuj,∀i∈N(40)
View SourceRight-click on figure for MathML and additional features.with δi∈Rn .

The cost functional associated to each agent i∈N has the following form:
=≡Ji(δi(0);ui,uNi)12∫∞0ri(δi,ui,uNi)dt,∀i∈N12∫∞0⎛⎝δTiQiδi+uTiRiiui+∑j∈NiuTjRijuj⎞⎠dt,∀i∈N
View SourceRight-click on figure for MathML and additional features.with user-defined matrices Qi⪰0 , Rii≻0 , and Rij⪰0, ∀i,j∈N , of appropriate dimensions and (Q−−√i,A), ∀i∈N , are detectable.

It is desired to find a graphical Nash equilibrium [93] u⋆i for all agents i∈N in the sense that
Ji(δi(0);u⋆i,u⋆Ni)≤Ji(δi(0);ui,u⋆Ni),∀ui, i∈N.
View Source

This can be expressed by the following coupled distributed minimization problems:
Ji(δi(0);u⋆i,u⋆Ni)=minuiJi(δi(0);ui,u⋆Ni),∀i∈N
View Sourcewith the dynamics given in (40).

The ultimate goal is to find the distributed optimal value functions V⋆i, ∀i∈N defined by
V⋆i(δi(t)):=minui∫∞t12(δTiQiδi+uTiRiiui +∑j∈NiuTjRijuj)dt,∀t,δi,i∈N.(41)
View SourceRight-click on figure for MathML and additional features.One can define the Hamiltonians associated with each agent’s neighborhood tracking error (40) and each V⋆i given in (41) as follows:
=Hi(δi,ui,uNi,∂V⋆i∂δi)∂V⋆i∂δiT⎛⎝Aδi+(di+gi)Biui−∑j∈NiαijBjuj⎞⎠+12δTiQiδi+12uTiRiiui+12∑j∈NiuTjRijuj,∀δi,ui,∀i∈N.
View SourceAccording to the stationarity condition, the optimal control for each i∈N can be found to be
u⋆i(δi)==argminuiHi(δi,ui,uNi,∂V⋆i∂δi)−(di+gi)R−1iiBTi∂V⋆i∂δi,∀δi(42)
View Sourcethat should satisfy the appropriate distributed coupled HJ equations
Hi(δi,u⋆i,u⋆Ni,∂V⋆i∂δi)=0,∀i∈N.
View Source

Assume that the value functions are quadratic in the neighborhood tracking error, i.e., V⋆i(δi):Rn→R
V⋆i(δi)=12δTiPiδi,∀δi,∀i∈N(43)
View Sourcewhere Pi=PTi>0∈Rn×n, ∀i∈N , are the unique matrices that solve the following complicated distributed coupled equations:
δTiPi(Aδi−(di+gi)2BiR−1iiBTiPiδi+∑j∈Nαij(dj+gj)BjR−1jjBTjPjδj)+(Aδi−(di+gi)2BiR−1iiBTiPiδi+∑j∈Nαij(dj+gj)BjR−1jjBTjPjδj)T×Piδi+∑j∈Ni(dj+gj)2δTjPjBjR−TjjRijR−1jjBTjPjδj+(di+gi)2δTiPiBiRiiBTiPiδi+δTiQiδi=0,∀i∈N.
View SourceRight-click on figure for MathML and additional features.Using (43), the optimal control (42) for every player i∈N can be written as
u⋆i(δi)=−(di+gi)R−1iiBTiPiδi,∀δi.
View Source

A. Approximate Solution Using RL
A PI algorithm is now developed to solve distributed coupled HJ equations as follows.

The PI algorithm (Algorithm 14) is an offline algorithm. We now show how to develop an online RL-based algorithm for solving the coupled HJ equations.

Algorithm 14 PI for Regulation in Graphical Games
procedure

Given N -tuple of admissible policies μki(0), ∀i∈N

while ∥Vμ(k)i(δi)−Vμ(k−1)i(δi)∥≥ϵiac, ∀i∈N do

Solve for the N -tuple of costs Vki(x) using the coupled Bellman equations,
δTiQiiδi+∂Vki∂δiT⎛⎝Aδi+(di+gi)Biμki−∑j∈NiαijBjμkj⎞⎠+μkiTRiiμki+∑j∈NiμkjTRijμkj=0,Vμki(0)=0.
View Source

Update the N -tuple of control policies μk+1i,∀i∈N using
μk+1i=−(di+gi)R−1iiBTi∂Vki∂δiT.
View Source

k=k+1

end while

end procedure

Assume there exist constant weights Wi and such that the value functions V⋆i are approximated on a compact set Ω as
V⋆i(δi)=WTiϕi(δi)+ϵi(δi),∀δi,i∈N
View SourceRight-click on figure for MathML and additional features.where ϕi(δi):Rn→RKi, ∀i∈N , are the basis function set vectors, Ki is the number of basis functions, and ϵi(δi) is the approximation error.

Assuming current weight estimates W^ic,∀i∈N , the outputs of the critic are given by
V^i(δi)=W^Ticϕi(δi),∀i∈N.
View Source

Using the current weight estimates, the approximate Lyapunov-like equations are given by
H^i(⋅)==ri(δi(t),u^i,u^Ni)+∂Vi∂δiT⎛⎝Aδi+(di+gi)Biu^i−∑jNiαijBju^j⎞⎠ei,∀i∈N(44)
View SourceRight-click on figure for MathML and additional features.where ei∈R,∀i∈N , are the residual errors due to approximation and
u^i=−(di+gi)R−1iiBTi∂ϕi∂δiTW^iu,∀i∈N
View SourceRight-click on figure for MathML and additional features.where W^iu denote the current estimated values of the ideal weights Wi .

The critic weights are updated to minimize the square residual errors ei in order to guarantee that W^ic→Wi, ∀i∈N . Hence, the tuning law for the critic is given as
=W˙ic−αi∂ϕi∂δi(Aδi+(di+gi)Biu^i−∑j∈NiαijBju^j)Δ2×(⎛⎝∂ϕi∂δi⎛⎝Aδi+(di+gi)Biu^i−∑j∈NiαijBju^j⎞⎠⎞⎠T×W^ic+12δTiQiδi+u^TiRiiu^i+∑j∈Niu^TjRiju^),∀i∈N
View SourceRight-click on figure for MathML and additional features.where αi>0 is a tuning gain, and for the actor is given as
=W^˙iu−αiu(FiW^iu−Li(∂ϕi∂δi(Aδi+(di+gi)Biu^i−∑j∈NiαijBju^j)TW^ic)−14∑j∈Ni⎛⎝∂ϕi∂δiBiR−TiiRijR−1iiBTj∂ϕi∂δiT⎞⎠W^iu×⎛⎝⎜∂ϕi∂δi(Aδi+(di+gi)Biu^i−∑j∈NiαijBju^j)Δ⎞⎠⎟TW^ic)∀i∈N
View Sourcewhere αiu>0 is a tuning gain, and Fi,Li≻0 are matrices that guarantee stability of the system and
Δ=(⎛⎝∂ϕi∂δi⎛⎝Aδi+(di+gi)Biu^i−∑j∈NiαijBju^j⎞⎠⎞⎠T×∂ϕi∂δi⎛⎝Aδi+(di+gi)Biu^i−∑j∈NiαijBju^j⎞⎠+1)2.
View Source

SECTION V.Applications
RL has been successfully applied in many fields. Two of such fields are presented below.

A. Robot Control and Navigation
In the last decade or so, the application of RL in robotics has increased steadily. In [94], RL is used to design a kinematic and dynamic tracking control scheme for a nonholonomic wheeled mobile robot. Reference [95] uses the RL in the network-based tracking control system of the two-wheeled mobile robot, Pioneer 2-DX. The neural network RL is used to design a controller for an active simulated three-link biped robot in [96]. In [97], an RL based actor-critic framework is used in control of a fully actuated six degrees-of-freedom autonomous underwater vehicle. Luy et al. [98] use RL to design robust adaptive tracking control laws with optimality for multiwheeled mobile robots’ synchronization in communication graph. Reference [99] presents an optimal adaptive consensus-based formation control scheme over finite horizon for networked mobile robots or agents in the presence of uncertain robot/agent dynamics. In [100], IRL approach is employed to design a novel adaptive impedance control for robotic exoskeleton with adjustable robot bebavior. IRL is used in control of a surface marine craft with unknown hydrodynamic parameters in [101]. In [102], RL is used along with a stabilizing feedback controller such as PID or linear quadratic regulation to improve the performance of the trajectory tracking in robot manipulators.

A mobile robot navigation task refers to plan a path with obstacle avoidance to a specified goal. RL-based mobile robot navigation mainly includes robot learning from expert demonstrations and robot self-learning and autonomous navigation. In the former, the examples of standard behaviors are provided to the robot and it learns from these data and generalize over all potential situations that are not given in the examples [103]–[104][105][106][107][108][109][110][111][112]. In the later, RL techniques are used to train the robot via interaction with the surrounding environment. Stability is not a concern in this type of the task. Yang et al. [113] used continuous Q-learning for autonomous navigation of mobile robots. Lagoudakis and Parr [114] proposed the LS PI for robot navigation task. In this paper, and some other similar works [115]–[116][117], neural networks are used to approximate the value function and the environment is assumed static. An autonomous agent needs to adapt its strategies to cope with changing surroundings and solve challenging navigation tasks [118], [119]. The methods in [115] and [116] are designed for environments with dynamic obstacle.

B. Power Systems
There have existed several applications of RL in power systems. The work of [120] proposes the PI technique based on actor-critic structure for automatic voltage regulator system for its both models neglecting and including sensor dynamics. In [121], a game-theory-based distributed controller is designed to provide the desired voltage magnitude and frequency at the load. The optimal switching between different typologies in step-down dc–dc voltage converters is investigated in [122]. The control of boost converter is presented in [123] using RL-based nonlinear control strategy at attaining a constant output voltage. In [124], RL is used for distributed control of dc microgrids to establish coordination among active loads to collectively respond to any load change. In [125], RL is applied to the control of an electric water heater with 50 temperature sensors.

SECTION VI.Conclusion
In this paper, we have reviewed several RL techniques for solving optimal control problems in real time using data measured along the system trajectories. We have presented families of online RL-based solutions for optimal regulation, optimal tracking, Nash, and graphical games. The complete dynamics of the systems do not need to be known for these RL-based online solution techniques. As another approach to speed up the learning by reuse of data, experience replay technique used in RL was discussed. These algorithms are fast and data efficient because of reuse of the data for learning.

The design of static RL-based OPFB controllers for nonlinear systems has not been investigated yet. Also existing RL-based H∞ controllers require the disturbance to be measured during learning. Another interesting and important open research direction is to develop novel deep RL approaches for feedback control of nonlinear systems with high-dimensional inputs and unstructured input data. Deep neural networks can approximate a more accurate structure of the value function and avoid divergence of the RL algorithm and consequently instability of the feedback control system. Deep RL for feedback control, however, requires developing new learning algorithms that assure the stability of the feedback system in the sense of Lyapunov during the learning.