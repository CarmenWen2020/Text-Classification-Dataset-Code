The vehicle platoon will be the most dominant driving mode on future roads. To the best of our knowledge, few reinforcement learning (RL) algorithms have been applied in vehicle platoon control, which has large-scale action and state spaces. Some RL-based methods were applied to solve single-agent problems. If we need to tackle multiagent problems, we will use multiagent RL algorithms since the parameters space grows exponentially with the increasing number of agents involved. Previous multiagent RL algorithms generally may provide redundant information to agents, indicating a large amount of useless or unrelated information, which may cause to be difficult for convergence training and pattern extractions from shared information. Also, random actions usually contribute to crashes, especially at the beginning of training. In this study, a communication proximal policy optimization (CommPPO) algorithm was proposed to tackle the above issues. In specific, the CommPPO model adopts a parameter-sharing structure to allow the dynamic variation of agent numbers, which can well handle various platoon dynamics, including splitting and merging. The communication protocol of the CommPPO consists of two parts. In the state part, the widely used predecessor–leader follower typology in the platoon is adopted to transmit global and local state information to agents. In the reward part, a new reward communication channel is proposed to solve the spurious reward and “lazy agent” problems in some existing multiagent RLs. Moreover, a curriculum learning approach is adopted to reduce crashes and speed up training. To validate the proposed strategy for platoon control, two existing multiagent RLs and a traditional platoon control strategy were applied in the same scenarios for comparison. Results showed that the CommPPO algorithm gained more rewards and achieved the largest fuel consumption reduction (11.6%).

SECTION I.Introduction
Transportation is one of the main sources of energy consumption and greenhouse gas emission. In Europe, transportation is responsible for 33% of energy consumption and 23% of total emissions [1]. Road transport represents 72.8% of total greenhouse gas emissions and 73.4% of transport energy demand [2]. Therefore, reducing transportation-related fuel consumption is a critical issue.

With the development of vehicle-to-vehicle (V2V) communication technology, the vehicle platoon will be the most dominant driving mode on both urban and freeway roads [2], [3]. The vehicle platoon is a fleet of vehicles traveling together based on a specific information typology. Most existing studies adopted the optimal control theory [e.g., model predictive control (MPC)] to improve the fuel efficiency of a platoon, which was formulated by the cost function of fuel consumption minimization and solved by heuristic algorithms to find the optimal action of each vehicle [4]–[5][6]. However, a large platoon size causes a high dimension of solution space, which significantly increases the computational complexity and reduces the feasibility of real-world applications.

In recent years, the reinforcement learning (RL) approaches have been applied in various microscopic traffic scenarios (e.g., autonomous vehicles) and have attracted significant attention because of its good performance [7], [8]. To the best of our knowledge, a few deep RL algorithms have been applied to vehicle platoon control. To control a vehicle platoon, the accelerations (or speeds) of all vehicles should be decided. If simply adopting a single-agent RL to obtain all vehicles’ joint action, the number of actions and states exponentially increases with the increase of platoon size, which makes the problem intractable. Hence, the centralized RL is infeasible for a large-scale platoon control due to the extremely high dimension of the joint action space [9], [10].

Previously, numerous multiagent RL algorithms were proposed to solve the large-scale action problem. Some early researchers adopted an independent learner to optimize for the team reward [11], [12]. Since from a single agent’s perspective, the environment is only partially observed, agents may receive spurious reward signals that originate from their teammates’ (unobserved) behavior. The independent RL is often unsuccessful due to the so-called nonstationary problem [13]. Later, some researchers adopted N parallel actors with the guide of a centralized critic to control N agents. The structure of centralized training with decentralized execution makes it possible for RL in a multiagent context [14], [15]. However, training the fully centralized critic becomes impractical when there are more than a handful of agents according to [16].

To further improve the performance of multiagent RL, some communication protocol was proposed to enhance the cooperation between agents [3], [17]–[18][19]. Cooperating agents know the actions of other agents through communication. However, when there is a large number of agents, sharing the information of all agents and communication might not be helpful since it is hard for the agent to differentiate the valuable information from the shared information [10]. In addition, it usually spends enormous computational resources with a complex neural network and information typology [9], [10].

In this article, a communication proximal policy optimization (CommPPO) algorithm was proposed to control the vehicle platoon. There are three main contributions as follows.

This study constructs a multiagent RL framework to control vehicle platoons for achieving a goal with guaranteed traffic safety and driving efficiency.

By carefully balancing two crucial elements (traffic safety and driving efficiency), the proposed RL-based platoon control framework ensures a reasonable driving state of vehicle platoon even in mixed traffic with connected autonomous vehicles (CAVs) and human-driven vehicles (HDVs). We need a large exploration rate to help for generating better performance, but it usually brings plenty of crashes, especially in a large-size vehicle platoon. Hence, we adopt curriculum learning to train a small-size platoon at the beginning period and speed up training: learn a simple task (i.e., a platoon with small size) first and then build on that knowledge to solve the difficult task (i.e., a larger platoon).

A novel information communication protocol is designed to enhance cooperation among agents for improving the performance of multiagent system.

There are two information transmission channels in the proposed communication protocol. One is a state transmission channel: based on full observations, the widely used predecessor–leader follower typology in-vehicle platoon is adapted to transmit the carefully selected state information to agents, which addresses the issue of the nonstationary problem caused by partial observation and avoids inefficient information transmission. Another one is a reward transmission channel: instead of directly adopting a local or global reward, a new reward propagation channel is proposed to allocate a more explicit and representative reward for each agent, avoiding the “lazy agent” problem.

A parameter-sharing RL structure is adopted to reduce computational complexity and control agents with a variable agent number.

Our proposed CommPPO algorithm adopts a parameter-sharing structure where each agent shares the weights of a single network to decrease the computational complexity in a large system. Due to various platoon dynamics, including platoon splitting and merging, the number of agents varies with agents coming and going in the environment. The parameter-sharing structure handles the issues well as different agents use one set of parameters.

In this study, we aim at improving energy efficiency by vehicle platoon control. High fuel consumption usually occurs in traffic oscillation scenarios with frequent accelerations and decelerations, concurrently on both freeways and urban roads. Hence, various traffic oscillation scenarios are simulated to test the performance of CommPPO by comparing it with existing algorithms.

The rest of this article is organized as follows. Section II reviews the related work. Section III presents the methodology of the CommPPO-based vehicle platoon control strategy. Section IV introduces the simulation platform and experimental design. Section V presents the experiment results. Section VI concludes and discusses the future directions.

SECTION II.Related Work
A. Existing Vehicle Platoon Control Strategies
In a vehicle platoon, following vehicles can know what the leader would do, which helps them proactively make a reaction for the dynamics of the leading vehicle. In addition, based on a predefined communication typology, the adjacent two vehicles can effectively adjust their inner gap by cooperated driving. Hence, the vehicle platoon will be increasingly prevailing on future roads. In fact, the platooning strategies have been widely applied in energy management system [2], [20].

Early studies proposed an off-line speed advice approach to reduce energy consumption, while most recent studies applied the optimal control theory (e.g., MPC) algorithm to calculate the optimal speed at each step [4], [6], [21]. The optimal control problem is formulated by the cost function of fuel consumption minimization and solved by heuristic algorithms to find the optimal action of each vehicle. Though such approaches can theoretically obtain the mathematical optimal solutions of the fuel consumption minimization problem, it is formulated based on an assumption that the predecessor of platoon moves at constant speed during the whole horizon, which is unrealistic since it may accelerate or decelerate in the real-world scenarios. Hence, the optimal solution is predecessor trajectory-specific and may not work well when the predecessor moves with a large disturbance. In addition, the MPC algorithm contains large online computing workload, which decays the feasibility in large platoon.

B. RL Applications in Autonomous Vehicle Control
In recent years, various RL algorithms have been applied to autonomous vehicle controls, which are mainly divided into three categories [22]. The first category focuses on a better driving state, by training the autonomous vehicles to learn a smooth and efficient driving policy for car following or lane change maneuvers [23], [24]. The second category uses V2V and vehicle-to-infrastructure communications to derive data-driven driving policies for autonomous vehicles. The primary approaches adopt the actor–critic-based RL algorithms to formulate a human-like car-following model [25], [26]. The third category controls the autonomous vehicles’ movement in complex urban scenarios. The recent studies include modeling the autonomous vehicles’ decision on stop/go at a traffic light and train the autonomous vehicle to explore in a parking lot and avoid objects [8], [27], [28].

C. Multiagent RL Algorithms
The studies on multiagent RL have a long history since the 1990s. Some early studies adopted a decentralized learner (e.g., independent Q -learning) to control the large-scale agent system, where each agent accessed its local observation, and all agents try to optimize a joint (i.e., team) reward [11]. Agents share the same network structure and corresponding parameters. Each agent considers the rest of the agents as part of the environment, making it possible to be applied in an extensive system. Later, with the development of Deep RL, some studies trained the independent learner with a deep Q network (DQN) or deep recurrent Q network (DRQN), and better performance was obtained [12], [29]. More recently, the parameter-sharing structure was applied to the continuous action field by combining with the trust region policy optimization (TRPO) algorithm. Empirical results on three multiagent learning problems demonstrated that parameter-sharing TRPO (PS-TRPO) could scale to systems with large numbers of agents and performed well in domains with continuous action spaces [30]. In general, each agent is faced with a nonstationary learning problem because the dynamics of the environment effectively changes as its teammates change their behaviors through learning. Due to the unstable environment, an agent tends to receive a spurious reward originated from its teammates [13].

To address the nonstationary issue, most multiagent RL algorithms adopt a structure named centralized training with decentralized execution to achieve the coordination of agents. Centralized training with decentralized execution allows agents to learn and construct individual action-value functions, such that optimization at the individual level leads to optimization of the joint action-value function [29]. Agents learn strategy with the guide of a centralized critic, while each agent produces actions independently in execution. Sunehag et al. [13] proposed a value-decomposition network (VDN) algorithm to decompose the total value function for each agent. Intuitively, VDN measures each agent’s impact on the observed joint reward, assuming that the joint action-value function can be additively decomposed into N Q -functions for N agents, where each Q -function only relies on the local state-action history. Later, Q with mixing network (Qmix) and Q transformation (QTRAN) relaxed the assumption by using a neural network to express the relationship between the local Q value and the global Q value [16], [29]. Some recent studies applied centralized training with decentralized execution to the continuous action field [e.g., multiagent deep deterministic policy gradient (MADDPG) and counterfactual multiagent (COMA)] and gained better performance in various cooperative and competitive environments [14], [15].

To enhance the cooperation between agents, various communication protocols were integrated into a multiagent system. Representative solutions include the differentiable interagent learning (DIAL) [31] and the CommNet [17]. To get the knowledge of other agents’ actions, they adopted a single network in the multiagent setting, passing the averaged message over the agent modules between layers. Later, a policy gradient algorithm, a multiagent bidirectionally coordinated network (BiCNet), was proposed [18]. It used a bidirectional recurrent neural network such that heterogeneous agents can communicate with different sets of parameters. Results showed that BiCNet was able to discover several effective ways to collaborate during the combat game. More recent works, such as targeted multiagent communication (TarMAC) and nearly decomposable Q -functions (NDQs), tried other methods to adjust the communication channel for a better information transmission [32], [33].

Previous multiagent RL algorithms generally may provide redundant information to agents, indicating a large amount of useless or unrelated information, which may cause to be difficult for convergence training and pattern extractions from shared information. This limitation inspired us for proposing the CommPPO algorithm to enhance the communication efficiency between agents.

SECTION III.Preliminaries and Method
A. Preliminary Knowledge
1) Reinforcement Learning:
RL is a learning tool where agents interact with the environment to learn how to behave in the environment without having any prior knowledge by learning to maximize a numerically predefined reward (or to minimize a penalty) [22]. After taking action, the RL agent receives feedback from the environment at each time step t about the performance of its action. Using this feedback (reward or penalty), it iteratively updates its action policy to reach an optimum control policy. The successful application of deep RL in chess-playing, such as Alpha GO [34], inspired a significant amount of engineering applications of deep RL, including intelligent transportation and automated vehicles.

As a classical RL algorithm, Q value-based RL (e.g., deep Q network) performed well in discrete action spaces, while it could not handle continuous action spaces. To resolve such difficulties, a policy gradient approach was introduced to enable agents to update policies in a continuous space as policy-based RL algorithms (e.g., policy proximal optimization) formulate policy as a continuous function [35].

In the policy gradient algorithm, the objective function that decides the updating direction of policy can be written as follows:
LPG=Et[logπθ(at|st)At^](1)
View SourceRight-click on figure for MathML and additional features.where Et[⋯] is the average value over a finite batch of samples, at is the action at time step t , st is the state at time step t , πθ is the policy or actor with the parameter θ at time step t , and A^t is an estimation of the advantage function at time step t .

By defining a probability ratio rt(θ)=(πθ(at|st))/(πθold(at|st)) representing the probability of policy πθ , the parameter θ can be updated according to (2). When A^t>0 , πθ(at|st) will be encouraged and vice versa
L(θ)=Et[πθ(at∣st)πθold(at∣st)A^t].(2)
View SourceRight-click on figure for MathML and additional features.

However, the minimization of L(θ) would lead to an excessively large policy update if without constraints. To penalize changes of policy and ensure the efficiency of importance sampling, Schulman et al. [35] proposed a clipped PPO as presented in (3), which was adopted in this article
LKLPEN=Eˆt[min(rt(θ)A^t,clip(rt(θ),1−ϵ,1+ϵ)A^t)](3)
View SourceRight-click on figure for MathML and additional features.where ϵ is a hyperparameter to be trained.

2) Predecessor–Leader Follower Typology:
The predecessor–leader follower typology might be the most widely used communication protocol in the vehicle platoon, which is also adopted in this study to enhance the cooperation between agents. As can be seen from the traffic network part in Fig. 1, a platoon with N CAVs follows an HDV. It is assumed that: each CAV has an in-vehicle sensor to detect the speed and location of its predecessor and 2) the V2V communication technology enables the leading CAV broadcast information to other CAVs in the platoon. Note that the assumptions are well accepted by previous studies [36], [37]. Based on the assumptions above, a modified predecessor–leader follower typology is applied for state communication, and a CAV receives two kinds of information: 1) the traffic state of its predecessor (the yellow dashed line) and 2) the traffic state of the platoon’s predecessor (the blue dashed line). In our current study, the main focus is to propose an RL framework that can find the optimal vehicle platoon control strategy to reduce energy. Thus, we did not consider the communication failure.

Fig. 1. - Framework of CommPPO-based vehicle platoon strategy.
Fig. 1.
Framework of CommPPO-based vehicle platoon strategy.

Show All

B. Communication Proximal Policy Optimization Algorithm
The nonstationary nature of the multiagent system makes experience replay samples obsolete and negatively impact training. One possible way to address this problem is disabling experience replay and instead relying on asynchronous training. Hence, the online updated RL algorithm, PPO, was adopted in this article. This section details the principle of the proposed CommPPO.

1) Communication Protocol:
The vehicle platoon is a multiagent system with an explicit interaction relationship. In other words, the influences of one agent on other agents are clear and already known. More concretely, since vehicle platoons can only move in one direction, only a preceding agent influences its following agents, and the influence can be calculated based on its ordinal number. For example, a leading vehicle influences all vehicles. However, it does not suffer from the influence of other vehicles, while the last vehicle in a platoon is influenced by its preceding vehicles and has no influence on other vehicles. For this kind of system, we have argued that the algorithm’s performance with a general communication typology can be further improved by adopting a specialized communication channel.

As can be seen from the CommPPO structure part in Fig. 1, each agent with shared parameter control one corresponding vehicle. In a vehicle platoon, only the first vehicle can detect the global state, which influences the whole platoon. It broadcasts the preliminary information to other agents. Also, each agent is influenced by its predecessor. Hence, for agent i , it receives a global state oglobal from agent 1 and a local state oi_global from agent i−1 . The respective index is also included in the state vector so that different agents can produce different action values, even receiving the same global and local state.

Besides the actor part, the critic part is also formed as a parameter-sharing structure. A potential drawback of only using the global reward is the “lazy agent” problem that one agent learns a useful policy, while other agents are discouraged from learning [13]. In fact, each agent tends to have its own objective to drive the collaboration. If simply allocating the local reward for each agent, agents may receive a spurious reward signal that is originated from other agents. Hence, a compound reward reflecting the interactions between agents is proposed using a reward propagation channel (see Fig. 1). More specifically, in an N -agents platoon, agent m influences its following vehicles, so the real reward of agent m can be calculated by the following equation:
Rm=∑i=mNri∗discount(i−m)(4)
View SourceRight-click on figure for MathML and additional features.where ri is the reward of agent i calculated by its traffic state, Rm is the reward of agent m for updating the critic, and discount is the discounted factor. Note that the effect of the preceding vehicle might be related to vehicle order, speed, relative speed, acceleration, distance, and so on. We make the assumption that it is only influenced by the order as the influence evaluation is not the main focus in this study. In addition, the influence of preceding agents attenuates through the vehicular string and keeps a constant discounted factor.

2) Algorithm With Curriculum Learning:
The actions of RL are likely to be randomly distributed in the predefined range, especially at the beginning of training. When the random actions are implemented to control a vehicle platoon, vehicles may collide with each other, and also training will be interrupted. In most cases, traffic collisions occur promptly with random actions, especially in large-platoon scenarios, so it is hard for agents to learn the optimal strategy. Hence, a curriculum learning approach was adopted in this study for better training: learn a simple task first and then build on that knowledge to solve the difficult task. More specifically, a two-CAV platoon is trained, and only two agents sample data at the beginning. After they move stably, a four-CAVs platoon is trained, then eight, sixteen… By a simple curriculum learning approach, the number of agents doubles until the target agent number.

Algorithm 1 describes the parameter-sharing training approach based on single-agent PPO. We first initialize the actor and critic networks and set the step size parameter. The agent number increases gradually based on the curriculum learning. At each iteration of the algorithm, the sampled trajectories from different agents (S,A,R, and Snext ) are collected together for updating parameters in network. Unlike the distributed PPO (DPPO) [38], where each worker computes the gradient from its own experience and passes them to the central PPO, the CommPPO computes the gradient from all trajectories by maximizing the following objective for more robust performance:
L(θ)=Ei∼[1,m][min(rt(θ)A^t,clip(rt(θ),1−ϵ,1+ϵ)A^t)](5)
View SourceRight-click on figure for MathML and additional features.where i is the agent index ranging from 1 to the current platoon size m .

Algorithm 1 CommPPO
Initial policy critic network V(s,ω) and actor πθ(s,a)

for agent number m=2,4,8,16,… do

for iteration k=0,1,2,… do

Rollout trajectories for all agents

Compute advantages value Aθk(o,a) using all trajectories

Find πθk+1 maximizing Eq. 5

In (5), the term in the bracket is the same as the objective function in clipped PPO proposed by OpenAI [35], which reflects the advantages of policy πθ with parameter vector θ . The first term insides the min(.) function is rt(θ)A^t , which is the advantage of policy πθ . A better policy can be obtained by maximizing rt(θ)A^t . Without the constraint, it would lead to an excessively large policy update, decaying the updating stability of the algorithm. Hence, the second term, the formula clip(rt(θ),1−ϵ,1+ϵ )A^t , is introduced to modify the objective function by clipping the probability ratio, which removes the incentive for moving rt(θ) outside of the interval [1−ϵ , 1+ϵ ]. Finally, the objective is a lower bound by choosing the minimum value. In this study, we gather all transition pairs (st,at,rt , and st+1 ) sampled by m parameter-sharing agents together to update parameter θ in (5). This selection makes each agent evenly contributes to the central updating, avoiding the “lazy agent” problem.

3) Crucial Elements:
Fig. 1 shows the interaction between the CommPPO algorithm and the traffic network. There are three crucial elements in RL for a sequential decision-making problem: state, action, and reward. The CommPPO agents perceive the state value st from the traffic simulation network and take actions at to control the behaviors of platoons. After implementing actions, agents receive the reward value rt from the traffic simulation environment and then obtain the next state value st+1 . Each agent receives a transition (st , at , rt , and st+1 ) at each time step. After a period of time, we collect all transitions sampled by each agent, and the series of transitions from one agent are called one trajectory. Eventually, the CommPPO computes the gradient based on trajectories collected by all agents. Hence, agents have an equal contribution to the policy gradient. It reduces the correlation between sampled data to ensure algorithm stability. The optimal policy benefits all agents, and it converges to the collective goal eventually.

a) Action:
Each agent controls its corresponding CAV by adjusting the acceleration, which is considered as the action in the CommPPO-based platoon control strategy. A general principle of CAV control recommended in previous studies is to give drivers a more comfortable driving experience. Thus, previous studies have applied an acceleration rage of −3 and 3 m/s2 for CAV control (see [23], [25]), which is also adopted in our study.

b) State:
In the CommPPO algorithm, the state space is represented by several variables reflecting the traffic state of the platoon. For CAV i , the state vector consists of five variables based on the communication protocol: 1) the speed difference between the HDV and CAV i (the former minus the latter); 2) the speed difference between CAV i and its predecessor, CAV i−1 (the latter minus the former); 3) the speed of CAV i ; 4) the gap between CAV i and its predecessor, CAV i−1 ; and 5) the ordinal number, i . It is worth mentioning that for CAV 1, its predecessor is the HDV, so the first value is equal to the second value.

c) Reward:
This article aims at minimizing fuel consumption in traffic oscillation scenarios with guaranteed traffic safety and efficiency. According to [8], a low-speed driving state with frequent accelerations and decelerations is the main factor resulting in high fuel consumption. Hence, agents receive an acceleration penalty defined as the square norm of the ratio of the real acceleration and the acceleration bound.

In real-world scenarios, vehicles cannot entirely sacrifice traffic safety or driving speed for fuel efficiency. Otherwise, some abnormal driving behaviors may occur (e.g., crashes). To ensure an acceptable driving speed and avoid low-speed driving state, a penalty is added when the gap ahead is more significant than a gap threshold, Lthreshold . According to [39], the vehicle moves at a free flow state when the gap ahead is larger than 120 m, so the Lthreshold value is set as 120 m to ensure vehicles keep the car-following mode.

Moreover, to avoid crashes, a maximum conflict acceleration is introduced. Traffic conflict is a traffic safety index representing crash risks [40], [41]. A traffic collision is likely to occur when traffic conflict index exceeds the predefined threshold. Hence, vehicles can decelerate proactively to avoid the crashes when a traffic conflict occurs. Deceleration rate to avoid a crash (DRAC) is a widely used traffic conflict index in existing studies [40], [42], especially in evaluating rear-end crash risks. DRAC is defined as minimum deceleration rate required by the following vehicle to match the velocity of the leading vehicle to avoid a crash, as written in as follows:
DRACi=⎧⎩⎨⎪⎪(vi−vi−1)2xi−1−xi−Li−1,0,if vi>vi−1else (6)
View SourceRight-click on figure for MathML and additional features.where v is the velocity of vehicle (i is the following vehicle and i−1 is the preceding vehicle), x is the position of the vehicle, and L is the vehicle length.

DRAC is based on the assumption of reaction time, and the computational time for the control signal of CAV could be considered as the CAV’s reaction time to some extent. In a real-world scenario, one CAV needs to decelerate instantaneously to avoid crashes, if the DRAC is more extensive than its threshold. Thus, we introduce DRAC as an index reflecting traffic safety state, which can be calculated based on an instant traffic state obtained from the V2V communication technology, without the knowledge of the human-drivers’ reaction. To judge whether a traffic conflict occurs, a maximum threshold index, maximum available deceleration rate (MADR), is applied. The following vehicle is deemed to conflict with the leading vehicle if DRAC > MADR. In the literature, a widely accepted MADR value, 1.4 m/s2, was adopted in this study. Hence, the maximum acceleration without traffic conflict (aconflict ) can be calculated accordingly. A penalty is added when the real acceleration a is larger than aconflict . The acceleration of the vehicle is set as the minimum value of a and aconflict . Overall, an agent is penalized when it escapes from the car-following mode or has a traffic conflict with its predecessor. The reward value of agent i can be calculated as follows:
ri=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪−(aamax)2−1,−(aamax)2, if gap>L_text thresholdor a>aconflictelse(7)
View SourceRight-click on figure for MathML and additional features.where a is the instant acceleration of vehicle i , gap is the gap ahead of vehicle i , and amax is the acceleration bound.

d) Neural Network:
Each agent uses two kinds of neural networks, the actor for policy generation and the critic for policy improvement. There are one critic network for computing policy advantage and two actor networks for constructing two policies before and after parameter updating. In the critic part, a three-layer fully connected neural network with 100 neurons in the middle layer is adopted. The input layer is the state variable (five neurons), and the output layer is the Q value of the corresponding state (one neuron). In the actor part, two three-layer fully connected neural networks with 100 neurons in the middle layer are adopted. The input layers in the two networks are both the state variables (five neurons), while the output layers are the mean value (one neuron) and standard deviation (one neuron) of the action distribution, respectively.

C. Existing Approaches
1) Existing Multiagent RL Algorithms:
To test the performance of the proposed CommPPO, two existing multiagent RL strategies, PS-PPO and BiCNet, are introduced for comparison [18], [30]. In fact, PS-PPO is originated from PS-TRPO, which adopts independent learners to train a parameter-sharing agent. To focus on the comparison with different structures, we replaced the TRPO as PPO.

The BiCNet can be applied in a multiagent system with a variable number of agents, which is suitable for vehicle platoon control. In BiCNet, communication takes place in the latent space (i.e., in the hidden layers). It also uses the structure of parameter sharing. However, it proposes bidirectional recurrent neural networks to model the actor and critic networks of their model and gained a good performance in a large-scale system. Readers can refer to [18] and [30] for more details.

2) Traditional Platoon Control Strategy:
To further validate the CommPPO in terms of energy efficiency, the most widely used platoon control strategy, MPC, was applied in the same scenarios. This MPC algorithm is briefly introduced in this section, and more details refer to previous studies [4], [43], [44]. A distributed MPC platoon control strategy is adopted for an acceptable computational speed according to [4]. In the MPC-based platoon control strategy, the state-space function can be written as follows:
x˙(t)=Ax(t)+Bu(t)(8)
View SourceRight-click on figure for MathML and additional features.where
A=x(t)=(0011),B=(−τ−1)(si−1−si−(vi∗τ+L)vi−1−vi),u(t)=ai.
View SourceRight-click on figure for MathML and additional features.

The objective function can be written as follows:
J=∑k=tt+N{x(k)′Qx(k)+u(k)′Ru(k)+ωf(k)v}(9)
View SourceRight-click on figure for MathML and additional features.with the following constraints: 1) state-space function: x˙(t)=Ax(t)+Bu(t) and 2) variable constraints: 0≤v≤vmax , amin≤a≤amax , si−1−si>li+L , where
Q=(2001),R=2, ω
View SourceRight-click on figure for MathML and additional features.are the weighting coefficient, si,vi,ai, and li denote the location, speed, acceleration, and vehicle length of vehicle i , respectively, amin,amax,vmax,L, and τ,N are the minimum acceleration, maximum acceleration, maximum speed, the minimum spacing, and the constant time gap and the prediction horizon, respectively. The parameters are set as follows: ω=10,amin=−3m/s2;amax=3m/s2;vmax=33m/s,L=2m;andτ=2s,N=10 . Note that a relatively large headway is adopted for better oscillation mitigating performance.

In the MPC-based control strategy, CAVs are controlled by the MPC algorithms, while HDVs are controlled by the intelligent driver model (IDM). Here, u(k) can be calculated by solving the optimal function (9) at each time step, which is the optimal acceleration to control each vehicle. This point can be referred via [4], [6] for more information.

SECTION IV.Experiment Design
A. Development of Simulation Platform
A traffic simulation software, the simulation of urban mobility (SUMO), was adopted to simulate the dynamics of the vehicle platoon in this article. SUMO has been an emerging and widely used traffic simulation platform in various existing studies [45]–[46][47]. Compared with other microscopic simulations, SUMO provides numerous car-following and lane-changing models that can satisfy the research needs. More importantly, as an open-source software, SUMO gives users higher privileges to do further and deeper developments. It also enables communications and interactions with other software, such as importing python packages and connecting to veins [47]–[48][49].

In the future road, CAV and HDV may exist simultaneously, and the mixed traffic scenarios last for an extended period. In this study, a widely used car-following model (namely IDM) was adopted to simulate the movement of HDV. We selected this model because the parameters in the IDM can be quickly and accurately calibrated. In addition, it can well simulate the reaction mode of HDV with collision-free behaviors [50], [51]. Readers can refer to [52] and [53] for more details and model parameters.

B. Training Environment and Parameter Settings
The traffic oscillation is the so-called stop-and-go wave, which is a common phenomenon on both freeways and urban roads [8], [41]. The stop-and-go behavior indicates that one vehicle decelerates for a while and then accelerates to its original speed. Then, its following vehicles conduct stop-and-go behaviors sequentially. The series of stop-and-go behaviors contribute to a large number of traffic disturbances. Furthermore, the frequent acceleration and deceleration largely contribute to high fuel consumption according to [8]. Hence, we tried to apply the CommPPO in traffic oscillation scenarios for fuel consumption minimization.

In this scenario, a leading vehicle followed by a 16-vehicle platoon is loaded on a one-lane freeway (note that the platoon size increases gradually). The initial headway and speed are 2 s and 20 m/s, respectively. The leading vehicle keeps its original speed for the 40 s, followed by a 30-s deceleration (−1 m/s2), and a period of acceleration (1 m/s2) until the original speed. Then, it keeps the speed to the ending of the simulation. The total running time is 150 s. It is worth mentioning that an episode ends when the simulation time runs out.

C. Fuel Consumption
In this study, we attempt to test the effect of the CommPPO-based platoon control strategy in the different oscillation scenarios. To ensure a fair comparison, the CAV and HDV are assumed to have the same physical and aerodynamic properties. In other words, they are equal in terms of calculating fuel consumption. According to [54], the resistance forces include aerodynamic, rolling, and slope forces. The sum of force, Fw , can be calculated by the following equation:
Fw=Fair=Fr=FG=Fair+Fr+FG1/2ρairAfCDv2mgCrcosαmgsinα(10)(11)(12)(13)
View SourceRight-click on figure for MathML and additional features.where Fair , Fr , and FG refer to the air drag force, the rolling resistance force, and the gravity force a vehicle suffers, respectively, and v is the instant speed of the vehicle.

The physical and aerodynamic property values are listed in Table I according to [8]. An exact derivation of the fuel consumption equation could be very complex and is not the main objective here. Instead, an approximate and differentiable function of velocity and control input is enough to develop the algorithm for ecological driving. For typical vehicles, if Fw is nonnegative, the fuel consumption in milliliters per second can be represented as follows:
fV=b0+b1v+b2v2+b3v3.(14)
View SourceRight-click on figure for MathML and additional features.Otherwise, the fuel consumption can be represented as
fV=b0+b1v+b2v2+b3v3+a(c0+c1v+c2v2)(15)
View SourceRight-click on figure for MathML and additional features.where a is the acceleration of the vehicle and b0 , b1 , b2 , b3 , c0 , c1 , and c2 are the model coefficient obtained through the curve-fitting process according to [54]: b0=0.1569 , b1=2.450×10−2 , b2=−7.415×10−4 , b3=5.975×10−5 , c0=0.07224 , c1=9.681×10−2 , and c2=1.075×10−3 .

TABLE I Physical and Aerodynamic Property Values
Table I- 
Physical and Aerodynamic Property Values
SECTION V.Results of Simulation Analysis
A. Training Results
The CommPPO-based vehicle platoon was trained through the interaction with SUMO. To improve the training effect, a simple curriculum learning was adopted in this study, and the agent number increased when the gain of reward was positive and became sufficiently small [55]. The final training parameters are shown in Table II. We have conducted a sensitivity analysis for six values of the discount factor ranging from 0 to 1.0. Each experiment is repeated three times, and the results show averaged values. With the increase of the discount factor, reward values present a convex trend, and it gains the most reward (−0.23) when the discount factor is equal to 0.4, which determines the reward function. Since RL is a heuristic algorithm, the performance can be further improved by more precise adjustments of the discount factor. We compared the convergence performance of three multiagent RL algorithms by plotting the reward against the number of training episodes (see Fig. 2). It was found that the CommPPO gained more rewards than the other two algorithms. Using curriculum learning, the reward gradually increased with agent number, and it outperformed other algorithms during the training process.

TABLE II Hyperparameters and Corresponding Descriptions
Table II- 
Hyperparameters and Corresponding Descriptions
Fig. 2. - Reward curve.
Fig. 2.
Reward curve.

Show All

To further test the performance of algorithms in vehicle platoon control, the time–space diagram of vehicle trajectories is plotted in Fig. 3. This figure showed the change in vehicle position and speed. A vehicle moves at a low speed with a purple color and a high speed with a yellow color. In the baseline scenario where vehicles are controlled by the traditional IDM model in SUMO, an oscillation was formed well and propagated continuously to the upstream. Each vehicle experiences an emergent deceleration. By contrast, with the CommPPO-based control, the vehicle platoon almost dampened the oscillation. As can be seen from the trajectories, the vehicle with CommPPO-based control proactively slows down to a suitable speed to keep a relatively large gap ahead, cutting the propagation of oscillations. Results showed that the fuel consumption with and without CommPPO-based control was 0.343 and 0.388 Ah/km, respectively, and an 11.6% reduction was achieved. However, in the scenarios with BiCNet and PS-PPO control, the oscillations were only partly dampened, and vehicles tried to keep more significant gaps with their predecessor after experiencing the oscillation. Results showed that only 2.79% and 3.45% fuel consumption were reduced. The reason might be that the communication channels in CommPPO give more valuable information to agents so that vehicles can know what the leader would do and take actions proactively. To explore the real driving state of vehicles controlled by different algorithms, the acceleration curves of the last vehicle were also plotted in Fig. 3. It was found that only in the scenario controlled by CommPPO, the maximum absolute value of acceleration (0.95 m/s) was less than the initial absolute acceleration of the leading vehicle (1 m/s). Hence, the traffic oscillation was dampened and vehicles moved smoothly.

Fig. 3. - Time–space diagram of vehicle trajectories in different strategies.
Fig. 3.
Time–space diagram of vehicle trajectories in different strategies.

Show All

One can notice that vehicles recover to the original state for the baseline case, while they do not in other cases for a more fuel-efficient driving mode. It is not necessary to always keep the highest speed (or ideal speed limit) as a relatively low speed tends to bring less fuel consumption according to the fuel consumption function (concave curve with respect to speed). Thus, vehicles are not likely to recover the initial speed during a short period. At the same time, vehicles tend to keep a relatively large gap with their predecessors to alleviate possible future oscillations, which contributes to the difference of traffic state before and after experiencing the oscillation. Actually, RL is a heuristic algorithm, so the system with the control of the multiagent RL does not have a uniform equilibrium state. In other words, the intervehicle gap of platoons under RL control does not necessarily recover to the initial value. In the previous single-agent RL-based oscillation mitigating strategy proposed in [8], the traffic states before and after experiencing oscillation are also not always the same, reflecting the same property of RL algorithm that the equilibrium state may change if influenced by disturbances.

To test the performance of CommPPO in different speed scenarios, a high speed (30 m/s) and a low speed (15 m/s) scenarios were simulated. As can be seen from the fuel consumption curves in Fig. 4, the instant fuel consumption controlled by CommPPO almost kept the lowest value all the time. To explore the reasons, the traffic disturbance curves were also plotted in Fig. 4. Traffic disturbance was defined as the average square value of accelerations. It was found that the disturbances kept a relatively low value in CommPPO, especially in the time after the oscillation was triggered (after 40 s). Since more traffic disturbances reflected more frequent acceleration and deceleration of vehicles, the fuel consumption values were higher in scenarios controlled by other algorithms. Note that vehicle speed is also an impact factor for fuel consumption. Since vehicles in CommPPO kept a relatively high speed while vehicles in other algorithms kept an extremely low speed in a short period around 80 s, fuel consumption value in CommPPO was the highest in this time.


Fig. 4.
Instant fuel consumption and disturbance comparison in different speed scenarios.

Show All

B. Testing the CommPPO-Based Strategy in Mixed Traffic
The mixed CAV and HDV traffic scenarios will last for an extended period in the future. Moreover, the ratio of CAV and HDV on different roads varies. Generally speaking, a large platoon is not likely to form an environment with a low CAV penetration rate. In mixed traffic, CAV and HDV are randomly distributed on the freeway, so the penetration rate of CAVs is not necessarily to be 100%. To test the influence of the CommPPO-based vehicle platoon on the whole traffic, several scenarios with different CAV penetration rates were simulated, and the widely used vehicle platoon control algorithm, MPC, is applied for comparison.

In this scenario, a leading vehicle with a predefined trajectory (shown below) was loaded on a freeway, and 32 following vehicles are inserted later with 2-s headway. The trajectory of the leading vehicle was defined as follows: 1) keeps a constant speed (20 m/s) for 50 s; 2) decelerates (−1 m/s2) for 30 s (if the speed decreases to zero, vehicle stops, and the deceleration is zero); and 3) accelerates (1 m/s2) to the original speed and then keeps the constant speed until 200 s.

We tested the fuel consumption in five scenarios with different CAV penetration rates: 0%, 25%, 50%, 75%, and 100% and the results were listed in Table III. Results showed that the two strategies both brought more fuel consumption with the increase of the CAV penetration rate. The reason was that in a low penetration rate scenario, CAVs could not perform well with sparse information obtained from other CAVs and vice versa. The CommPPO-based strategy contributed to over double reduction of fuel consumption compared with the MPC-based strategy. In addition, in an environment with a low CAV penetration rate, the proposed strategy still presented an acceptable performance (10.25% reduction) than the widely used MPC-based platoon control strategy (3.42% reduction).

TABLE III Fuel Consumption Comparison Between Different Strategies
Table III- 
Fuel Consumption Comparison Between Different Strategies
The average travel time was calculated and shown in Table IV. Traffic efficiency may decrease if the speeds of trailing vehicles are not the ideal speed limit. Accordingly, we penalize vehicles with a large gap ahead in the reward function to keep vehicles in a car-following state [see (7)]. Then, traffic disturbances and fuel consumption (16.63% reduction) are reduced well with slightly sacrificed traffic efficiency (4.94% reduction) (see Table IV).

TABLE IV Average Travel Speed Comparison Between Different Strategies
Table IV- 
Average Travel Speed Comparison Between Different Strategies
When there are several (⩾2 ) CAVs navigating together, and their intervehicle gap is smaller than the gap threshold Lthreshold , they belong to the same platoon, and the first vehicle is the leader of the platoon. In typical scenarios (without oscillations), CAVs follow their preceding vehicles in the platoon and keep a relatively constant speed to reduce unnecessary fuel consumption. In oscillation scenarios, when an oscillation is triggered and vehicles ahead conduct stop-and-go behaviors sequentially, CAVs in a platoon can obtain the oscillation information and proactively take actions to reduce traffic disturbances. In specific, the leading CAV in the platoon broadcasts the disturbances information to its following vehicles, and the following vehicles decelerate to a different extent to keep a gap with their predecessor, which largely alleviates traffic disturbances. The actions of the CAVs aim to minimize fuel consumption and traffic disturbances.

In this article, we analyzed the computational burden based on the computational time. To validate the proposed strategy, we calculated the averaged running time in each time step and compared with existing strategies. Results showed that the average running time surged with increased CAV penetration rate in scenarios with the MPC-based strategy control, while it fluctuated around the initial value in scenarios with the CommPPO-based strategy control (see Fig. 5). In addition, the CommPPO-based control strategy spent less time than the MPC-based strategy in all scenarios (e.g., 0.62 s averagely at each time step in the environment with 50% CAV). The reason was that in MPC, the decision variable exponentially increased with the increased number of CAVs, and the objective function with hundreds of decision variables and constraints should be solved in each time step. By contrast, in CommPPO, the neural networks with well-trained parameters directly produce the actions in running time without recursive iteration. In addition, the parallel agents make actions independently, so the computational speed does not increase with the penetration rate. According to [56], the real-time deployability of vehicle platoons requires that given the traffic state so that the well-trained agents can produce actions promptly (i.e., in much less than 0.1 s). We computed the average computational time of the CommPPO algorithm at different platoon sizes, and the result showed that the maximum computational time at each step was 0.04 s. Hence, the parameter-sharing structure satisfies the needs of real-time operation in CAV control statistically.

Fig. 5. - Running time comparison in mixed traffic (in the boxplot, the five values from the top to the bottom in a box denote the maximum, the third quartile, sample median, the first quartile, and the minimum value of the data set).
Fig. 5.
Running time comparison in mixed traffic (in the boxplot, the five values from the top to the bottom in a box denote the maximum, the third quartile, sample median, the first quartile, and the minimum value of the data set).

Show All

Note that on the one hand, MPC can effectively solve a complex optimization problem even in the industrial process with delays, so it is applicable in platoons with communication or reaction delays. On the other hand, due to the enormous computational complexity, MPC is more suitable for systems with extensive delays rather than the platoon control scenario with the needs of real-time operations. With the development of advanced communication technologies such as 5G and dedicated short-range communications, there will be a limited delay and can be neglected in some scenarios [57]. Many previous pieces of research on deep RL model development also assumed the ideal environment and did not consider the information delay [8], [23], so it was also not considered in this research.

C. Testing the Influence of Vehicle Platoon Size
Due to the parameter-sharing structure, a well-trained CommPPO algorithm can be directly applied to a vehicle’s platoon of any platoon size (i.e., agent number). In this section, the performance of CommPPO-based vehicle platoons of different platoon sizes was compared. Different from previous scenarios, a more severe traffic oscillation was simulated to test the oscillation mitigating performance. The trajectory of the leading vehicle is defined as follows: 1) keeps a constant speed (30 m/s) for 50 s; 2) decelerates (−1 m/s2) for 50 s (if the speed decreases to zero, vehicle stops, and the deceleration is zero); and 3) accelerates (1 m/s2) to the original speed and then keeps the constant speed until 200 s.

The CAV penetration rate is set as 50% all the time for consistent, and four scenarios with different platoon sizes are simulated: 1) one 32-vehicle platoon; 2) two 16-vehicle platoons; 3) four 8-vehicle platoons; and 4) eight 4-vehicle platoons. They are evenly distributed on the traffic, and the time–space diagrams of vehicle trajectories are shown in Fig. 6. As can be seen from the figure, the last vehicle’s trajectory went more smoothly with the decrease of platoon size. Statistically, it was found that small platoons separately distributed in the mixed traffic achieved the most fuel consumption reduction (8.89%). The reason might be that in mixed traffic with various unpredictable factors (e.g., emergent deceleration in HDV), small vehicle platoons are more sensitive to traffic disturbances and they can make actions more promptly. Hence, the platoon decomposition was able to dampen more severe oscillations.

Fig. 6. - Time–space heatmap in different platoon sizes.
Fig. 6.
Time–space heatmap in different platoon sizes.

Show All

SECTION VI.Conclusion and Discussion
This study proposed a multiagent RL algorithm, CommPPO, to control vehicle platoons for improving energy efficiency in traffic oscillations. One agent with shared parameters controls each vehicle, so the CommPPO can handle the system with a variable agent number. Two different communication protocols were proposed for state and reward communication: 1) the widely used predecessor–leader follower typology transmitted the local and global state information to each agent and 2) the reward propagation channel was adopted to avoid the problem of spurious reward and “lazy agent.” Moreover, a simple curriculum learning approach was adopted for speeding up training. The CommPPO agents interacted with SUMO recursively to learn and explore the optimal actions. With the communication channels, vehicles in the CommPPO-based platoon are able to proactively react to the dynamics of the leading vehicles, avoiding emergent decelerations. Hence, more traffic disturbances and fuel consumption were reduced when compared with other multiagent RL algorithms and the MPC algorithm. In mixed traffic, the fuel consumption decreased with the CAV penetration rate since CAVs perform better in environment with abundant information obtained from other CAVs. Moreover, several small platoons separately distributed in the mixed traffic achieved better fuel efficiency when compared with a large platoon.

This study tried to shed light on how to apply multiagent RL to vehicle platoons with guaranteed safety and driving efficiency. It has been proved that vehicles controlled by several parallel agents can achieve a collective goal with a predefined communication typology. In the proposed CommPPO algorithm, two communication channels can address some critical issues on existing multiagent RL (e.g., nonstationary problem, spurious reward, and “lazy agent”). The fast computational speed and good performance in mixed traffic make the algorithm more applicable on future roads. In this article, the relationship between state and reward is a relative relationship instead of an absolute one. For example, given the same state value, the agent may receive different reward values and even takes the same actions.

Nowadays, autonomous vehicles and related roadside communication devices are not so popular in an urban environment, especially those for commercial use. Hence, the proposed control strategy might not be applicable in some scenarios without advanced information transmission devices. Recently, numerous advanced telematic technologies are developed to support autonomous vehicle control (e.g., cellular base station) in the testing fields [57], [58], so the proposed control strategy might be applicable in scenarios with the technologies. Moreover, the vehicle platoon is a unique multiagent system with explicit interaction relationships (e.g., one agent only has an influence on its following agents), so the CommPPO might not be practical in systems with partial or limited observation.