Broad Learning System (BLS) that aims to offer an alternative way of learning in deep structure is proposed in this paper. Deep structure and learning suffer from a time-consuming training process because of a large number of connecting parameters in filters and layers. Moreover, it encounters a complete retraining process if the structure is not sufficient to model the system. The BLS is established in the form of a flat network, where the original inputs are transferred and placed as “mapped features” in feature nodes and the structure is expanded in wide sense in the “enhancement nodes.” The incremental learning algorithms are developed for fast remodeling in broad expansion without a retraining process if the network deems to be expanded. Two incremental learning algorithms are given for both the increment of the feature nodes (or filters in deep structure) and the increment of the enhancement nodes. The designed model and algorithms are very versatile for selecting a model rapidly. In addition, another incremental learning is developed for a system that has been modeled encounters a new incoming input. Specifically, the system can be remodeled in an incremental way without the entire retraining from the beginning. Satisfactory result for model reduction using singular value decomposition is conducted to simplify the final structure. Compared with existing deep neural networks, experimental results on the Modified National Institute of Standards and Technology database and NYU NORB object recognition dataset benchmark data demonstrate the effectiveness of the proposed BLS.

SECTION I.Introduction
Deep structure neural networks and learnings have been applied in many fields and have achieved breakthrough successes in a great number of applications [1], [2], particularly in large scale data processing [3], [4]. Among them, the most popular deep networks are the deep belief networks (DBN) [5], [6], the deep Boltzmann machines (DBM) [7], and the convolutional neural networks (CNN) [8], [9]. Although the deep structure has been so powerful, most of networks suffer from the time-consuming training process because a great number of hyperparameters and complicated structures are involved. Moreover, this complication makes it so difficult to analyze the deep structure theoretically that most work span in turning the parameters or stacking more layers for better accuracy. To achieve this mission, more and more powerful computing resource has been involved. Recently, some variations in hierarchical structure [10]–[11][12] or ensembles [13]–[14][15][16][17], are proposed to improve the training performance.

Single layer feedforward neural networks (SLFN) have been widely applied to solve problems such as classification and regression because of their universal approximation capability [18]–[19][20][21]. Conventional methods for training SLFN are gradient-descent-based learning algorithms [22], [23]. The generalization performance of them is more sensitive to the parameter settings such as learning rate. Similarly, they usually suffer from slow convergence and trap in a local minimum. The random vector functional-link neural network (RVFLNN), proposed in [19]–[20][21], offers a different learning method.

RVFLNN effectively eliminates the drawback of the long training process and also it provides the generalization capability in function approximation [20], [24]. It has been proven that RVFLNN is a universal approximation for continuous functions on compact sets with fast learning property. Therefore, RVFLNN has been employed to solve problems in diverse domains, including the context of modeling and control [25]. Although RVFLNN enhances the performance of the perception significantly, this technique could not work well on remodeling high-volume and time-variety data in modern large data era [26]. To model moderate data size, a dynamic step-wise updating algorithm was proposed in [27] to update the output weights of the RVFLNN for both a new added pattern and a new added enhancement node. This paper paves a path for remodeling the system that has been modeled and encounters a new incoming data.

Nowadays, in addition to the growth of data in size, the data dimensions also increase tremendously. Taking the raw data with high dimension directly to a neural network, the system cannot sustain its accessibility anymore. The challenge for solving high dimensional data problem becomes imperative recently. Two common practices to alleviate this problem are dimension reduction and feature extraction. Feature extraction is to seek, possible, the optimal transformation from the input data into the feature vectors. Common approaches, which have the advantage of easy implementation and outstanding efficiency, for feature selection include variable ranking [28], feature subset selection [29], penalized least squares [30], random feature extractions such as nonadaptive random projections [31] and random forest [32], or convolution-based input mapping, to name a few.

Likewise for the feature extraction, the RVFLNN can take mapped features as its input. The proposed Broad Learning System (BLS) is designed based on the idea of RVFLNN. In addition, the BLS can effectively and efficiently update the systems (or relearn) incrementally when it deems necessary. The BLS is designed as, first, the mapped features are generated from the input data to form the feature nodes. Second, the mapped features are enhanced as enhancement nodes with randomly generated weights. The connections of all the mapped features and the enhancement nodes are fed into the output. Ridge regression of the pseudoinverse is designed to find the desired connection weights. Broad Learning algorithms are designed for the network through the broad expansion in both the feature nodes and the enhancement nodes. And the incremental learning algorithms are developed for fast remodeling in broad expansion without a retraining process if the network deems to be expanded.

It should be noted that once the learning system has completed its modeling, it may consist of some redundancy due to the broad expansion. The system can be simplified more using low-rank approximations. Low-rank approximation has been established as a new tool in scientific computing to address large-scale linear and multilinear algebra problems, which would be intractable by classical techniques. In [33] and [34], comprehensive exposition of the theory, algorithms, and applications of structured low-rank approximations are presented. Among the various algorithms, the singular value decomposition (SVD) and nonnegative matrix factorization (NMF) [35] are widely used for exploratory data analysis. By embedding these classical low-rank algorithms into the proposed broad learning network, we also design an SVD-based structure to simplify broad learning algorithm. This simplification process can be done by compressing the system on-the-fly right after the moment when additional feature nodes are inserted or right after additional enhancement nodes are inserted or after both together. One can also choose to compress the whole structure after the learning is completed. The algorithms are designed so versatile such that one can select number of equivalent neural nodes in the final structure. This approach lays out an excellent approach for model selection.

This paper is organized as follows. In Section II, preliminaries of RVFLNN, ridge regression approach of pseudoinverse, sparse autoencoder, and SVD are given. Section III presents the BLS and gives the details of the proposed broad learning algorithms. Section IV compares the performance in Modified National Institute of Standards and Technology database (MNIST) classification and NYU object recognition benchmark (NORB) classification with those from various deep systems. Also the performance analysis of the proposal broad learning algorithms is addressed here. Finally, discussions and conclusions are given in section V.

SECTION II.Preliminaries
Pao and Takefuji [19] and Igelnik and Pao [21] give a classical mathematic discussion of the advantages of the functional-link network in terms of training speed and its generalization property over the general feedforward networks [20]. The capabilities and universal approximation properties of RVFLNN have been clearly shown and, hence, are omitted in this section. The illustration of the flatted characteristics of the functional-link network is shown again in Figs. 1 and 2. In this section, first, the proposed broad learning is introduced based on the rapid and dynamic learning features of the functional-link network developed by Chen and Wan [27] and Chen [36]. Second, the ridge regression approximation algorithm of the pesudoinverse is recalled. After that, sparse autoencoder and SVD are briefly discussed.


Fig. 1.
Functional-link neural network [27].

Show All


Fig. 2.
Same functional-link network redrawn from Fig. 1 [27].

Show All

A. Dynamic Stepwise Updating Algorithm for Functional-Link Neural Networks
For a general network in usual classification task, referring to Figs. 1 and 2, denoted by A the matrix [X|ξ(XWh+βh)] , where A is the expanded input matrix consisting of all input vectors combined with enhancement components. The functional-link network model is illustrated in Fig. 2. In [27], a dynamic version model was proposed to update the weights of the system instantly for both a new added pattern and a new added enhancement node. Compared with the classic model, this model is simple, fast, and easy to update. Generally, this model was inspired by the pseudoinverse of a partitioned matrix described in [37] and [38].

Denote An be the n×m pattern matrix. In this section, we will only introduce the stepwise updating algorithm for adding a new enhancement node to the network as shown in Fig. 3. In this case, it is equivalent to add a new column to the input matrix An . Denote An+1≜[An|a] . Then the pseudoinverse of the new A+n+1 equals
[A+n−dbTbT]
View SourceRight-click on figure for MathML and additional features.where d=A+na
bT={(c)+(1+dTd)−1dTA+nif c≠0if c=0
View SourceRight-click on figure for MathML and additional features.and c=a−And .

Fig. 3. - Illustration of stepwise update algorithm [27].
Fig. 3.
Illustration of stepwise update algorithm [27].

Show All

Again, the new weights are
Wn+1=[Wn−dbTYnbTYn]
View SourceRight-click on figure for MathML and additional features.where Wn+1 and Wn are the weights after and before new enhancement nodes are added, respectively. In this way, the new weights can be updated easily by only computing the pseudoinverse of the corresponding added node. It should be noted that if An is of the full rank, then c=0 which will make a fast updating of the pseudoinverse A+n and the weight Wn .

B. Pseudoinverse and Ridge Regression Learning Algorithms
In a flatted network, pseudoinverse can be considered as a very convenient approach to solve the output-layer weights of a neural network. Different methods can be used to calculate this generalized inverse, such as orthogonal projection method, orthogonalization method, iterative method, and SVD [39], [40]. However, a direct solution is too expensive, especially the training samples and input patterns are suffered from high volume, high velocity, and/or high variety [26]. Also, pseudoinverse, which is the least square estimator of linear equations, is aimed to reach the output weights with the smallest training errors, but may not true for generalization errors, especially for the ill condition problems. In fact, the following kind of optimal problems is an alternative way to solve the pseudoinverse:
argminW:∥AW−Y∥σ1v+λ∥W∥σ2u(1)
View SourceRight-click on figure for MathML and additional features.where σ1>0 , σ2>0 , and u , v are typically kinds of norm regularization. By taking σ1=σ2=u=v=2 , the above optimal problem is setting as regular l2 norm regularization, which is convex and has a better generalization performance. The value λ denotes the further constraints on the sum of the squared weights, W . This solution is equivalent with the ridge regression theory, which gives an approximation to the Moore–Penrose generalized inverse by adding a positive number to the diagonal of ATA or AAT [41]. Theoretically, if λ=0 , the inverse problem degenerates into the least square problem and leads the solution to original pesuedoinverse. On the other hand, if λ→∞ , the solution is heavily constrained and tends to 0 . Consequently, we have
W=(λI+AAT)−1ATY.(2)
View SourceRight-click on figure for MathML and additional features.

Specifically, we have that
A+=limλ→0(λI+AAT)−1AT.(3)
View SourceRight-click on figure for MathML and additional features.

C. Sparse Autoencoder
Supervised learning tasks, such as classifications, usually need a good feature representation of the input to achieve an outstanding performance. Feature representation is not only an efficient way of data representation, but also, more importantly, it captures the characteristics of the data. Usually, intractable mathematic derivation is used or an easy random initialization to generate a set of random features can be populated. However, randomness suffers from unpredictability and needs guidance. To overcome the randomness nature, the sparse autoencoder could be regarded as an important tool to slightly fine-tune the random features to a set of sparse and compact features. Specifically, sparse feature learning models have been attractive that could explore essential characterization [12], [42], [43].

To extract the sparse features from the given training data, X can be considered to solve the optimization problem [44]. Notice that it is equivalent to the optimization problem in (1) if we set σ2=u=1 , and σ1=v=2
argminW^:∥ZW^−X∥22+λ∥W^∥1(4)
View SourceRight-click on figure for MathML and additional features.where W^ is the sparse autoencoder solution and Z is the desired output of the given linear equation, i.e., XW=Z .

The above problem denoted by lasso in [45] is convex. Consequently, the approximation problem in (4) can be solved by dozens of ways, such as orthogonal matching pursuit [46], K-SVD [47], alternating direction method of multipliers (ADMM) [48], and fast iterative shrinkage-thresholding algorithm (FISTA) [49]. Among them, the ADMM method is actually designed for general decomposition methods and decentralized algorithms in the optimization problems. Moreover, it has been shown that many state-of-art algorithms for l1 norm involved problems could be derived by ADMM [50], such as FISTA. Hence, a brief review of typical approach for lasso is presented below.

First, (4) can be equivalently considered as the following general problem:
argminw:f(w)+g(w),w∈Rn(5)
View SourceRight-click on figure for MathML and additional features.where f(w)=∥Zw−x∥22 and g(w)=λ∥w∥1 . In ADMM form, the above problem could be rewritten as
argminw:f(w)+g(o),s.t. w−o=0.(6)
View SourceRight-click on figure for MathML and additional features.

Therefore, the proximal problem could be solved by the following iterative steps:
⎧⎩⎨⎪⎪wk+1:=(ZTZ+ρI)−1(ZTx+ρ(ok−uk))ok+1:=Sλρ(wk+1+uk)uk+1:=uk+(wk+1−ok+1)(7)
View SourceRight-click on figure for MathML and additional features.where ρ>0 and S is the soft threshholding operator which is defined as
Sκ(a)=⎧⎩⎨a−κ,0,a+κ,a>κ|a|≤κa<−κ.(8)
View SourceRight-click on figure for MathML and additional features.

D. Singular Value Decomposition
Now comes a highlight of linear algebra. Any real m×n matrix A can be factored as
A=UΣVT
View SourceRight-click on figure for MathML and additional features.where U is an m×m orthogonal matrix whose columns are the eigenvectors of AAT , V is an n×n orthogonal matrix whose columns are the eigenvectors of ATA , and Σ is an m×n diagonal matrix of the form
Σ=diag{σ1,…,σr,0,…,0}
View SourceRight-click on figure for MathML and additional features.with σ1⩾σ2⩾⋯⩾σr>0 and r=rank(A) . Moreover, in the above, σ1,…,σr are the square roots of the eigenvalues of ATA . They are called the singular values of A . Therefore, we achieve a decomposition of matrix A , which is one of a number of effective numerical analysis tools used to analyze matrices. In our algorithms, two different ways to reduce the size of the matrix are involved. In the first, the threshold parameter η is set as 0<η≤1 , which means that the components associate with an eigenvalue σi≥ησ1 are kept. The second case is to select a fixed l singularities, while l is smaller than n . Define a threshold value ε , which is η for case 1 and l for case 2. In the real practice, both of the cases may be happened depending on various requirements. An SVD technique is known in its advantage in feature selection.

SECTION III.Broad Learning System
In this section, the details of the proposed BLS are given. First, the model construction that is suitable for broad expansion is introduced, and the second part is the incremental learning for the dynamic expansion of the model. The two characteristics result in a complete system. At last, model simplification using SVD is presented.

A. Broad Learning Model
The proposed BLS is constructed based on the traditional RVFLNN. However, unlike the traditional RVFLNN that takes the input directly and establishes the enhancement nodes, we first map the inputs to construct a set of mapped features. In addition, we also develop incremental learning algorithms that can update the system dynamically.

Assume that we present the input data X and project the data, using ϕi(XWei+βei) , to become the i th mapped features, Zi , where Wei is the random weights with the proper dimensions. Denote Zi≡[Z1,…,Zi] , which is the concatenation of all the first i groups of mapping features. Similarly, the j th group of enhancement nodes, ξj(ZiWhj+βhj) is denoted as Hj , and the concatenation of all the first j groups of enhancement nodes are denoted as Hj≡[H1,…,Hj] . In practice, i and j can be selected differently depending upon the complexity of the modeling tasks. Furthermore, ϕi and ϕk can be different functions for i≠k . Similarly, ξj and ξr can be different functions for j≠r . Without loss of generality, the subscripts of the i th random mappings ϕi and the j th random mappings ξj are omitted in this paper.

In our BLS, to take the advantages of sparse autoencoder characteristics, we apply the linear inverse problem in (7) and fine-tune the initial Wei to obtain better features. Next, the details of the algorithm are given below.

Assume the input data set X , which equips with N samples, each with M dimensions, and Y is the output matrix which belongs to RN×C . For n feature mappings, each mapping generates k nodes, can be represented as the equation of the form
Zi=ϕ(XWei+βei),i=1,…,n(9)
View SourceRight-click on figure for MathML and additional features.where Wei and βei are randomly generated. Denote all the feature nodes as Zn≡[Z1,…,Zn] , and denote the m th group of enhancement nodes as
Hm≡ξ(ZnWhm+βhm).(10)
View SourceRight-click on figure for MathML and additional features.

Hence, the broad model can be represented as the equation of the form
Y=== [Z1,…,Zn|ξ(ZnWh1+βh1),…,ξ(ZnWhm+βhm)]Wm [Z1,…,Zn|H1,…,Hm]Wm [Zn|Hm]Wm
View SourceRight-click on figure for MathML and additional features.where the Wm=[Zn|Hm]+Y . Wm are the connecting weights for the broad structure and can be easily computed through the ridge regression approximation of [Zn|Hm]+ using (3). Fig. 4(a) shows the above broad learning network.

Fig. 4. - Illustration of BLS. (a) BLS. (b) BLS with an alternative enhancement nodes establishment.
Fig. 4.
Illustration of BLS. (a) BLS. (b) BLS with an alternative enhancement nodes establishment.

Show All

B. Alternative Enhancement Nodes Establishment
In the previous section, the broad expansion of enhancement nodes is added synchronously with the connections from mapped features. Here, a different construction can be done by connecting each group of mapped features to a group of enhancement nodes. Details are described below.

For input data set, X , for n mapped features and for n enhancement groups, the new construction is
Y=≜ [Z1,ξ(Z1Wh1+βh1)|…Zn,ξ(ZnWhn+βhn)]Wn [Z1,…,Zn|ξ(Z1Wh1+βh1),…,ξ(ZnWhn+βhn)]Wn(11)(12)
View SourceRight-click on figure for MathML and additional features.where Zi , i=1,…,n , are N×α dimensional mapping features achieved by (9) and Whj∈Rα×γ . This model structure is illustrated in Fig. 4(b).

It is obvious that the main difference between two constructions, in Fig. 4(a) and (b), is in the connections of the enhancement nodes. The following theorem proves that the above two different connections in the enhancement nodes are actually equivalent.

Theorem 1:
For the model in Section III-A [or Fig. 4(a)], the feature dimension of Z(a)i is k , for i=1,…,n , and the dimension of H(a)j is q , for j=1,…,m . Respectively, for the model in Section III-B [or Fig. 4(b)] the feature dimension of Z(b)i is k , for i=1,…,n , and the dimension of H(b)j is γ , for j=1,…,n . Then if mq=nγ , and H(a) and H(b) are normalized, the two networks are exactly equivalent.

Consequently, no matter which kind of establishment of enhancement nodes is adapted, the network is essentially the same, as long as the total number of feature nodes and enhancement nodes are equal. Hereby, only the model in Section III-A [or Fig. 4(a)] will be considered in the rest of this paper. The proof is as follows.

Proof:
Suppose that the elements of Wei , Whj , βei , and βhj are randomly drawn independently of the same distribution ρ(w) . For the first model, treat the nk feature mapping together as Z(a)∈RN×nk whose weights are W(a)e , and separately, the mq enhancement nodes are denoted together as H(a)∈RN×mq whose weights are W(a)h . Respectively, for the second model, treat the nk feature mapping together as Z(b)∈RN×nk whose weights are W(b)e , and separately, the nγ enhancement nodes are denoted together as H(b)∈RN×nγ whose weights are W(b)h . Obviously, W(b)e and W(a)e , that is with the same dimension, are exactly equivalent because the entrances of the two matrices are generated from the same distribution.

As for the enhancement nodes part, first we have that H(a) and H(b) are of the same size if mq=nγ . Therefore, we need to prove that their elements are equivalent. For any sample xl chosen from the data set, denote the columns of W(a)e as waei∈RN,i=1,…,nk and the columns of W(a)h as wahj∈Rnk,j=1,…,mq .

Hence, the j th enhancement node associate with the sample xl should be
H(a)lj===≈= ξ(ϕ(XW(a)e+β(a)e)W(a)h+β(a)h)lj ξ([ϕ(xlwae1+βael1),…,ϕ(xlwaenk+βaelnk)]wahj+βahj) ∑i=1nkξ(ϕ(xlwaei+βali)wahli+βahli) nkE[ξ(ϕ(xlwae+βae)wah+βah)] nkE[ξ(ϕ(xlwe+βe)wh+βh)].
View SourceRight-click on figure for MathML and additional features.Here E stands for the expectation of distribution, we is the N dimension random vector drawn from the distribution density ρ(w) , and wh is the scaler number sampled from ρ(w) .

Similarly, for the columns of W(b)e as wbei∈RN,i=1,…,nk and the columns of W(b)h as wbhj∈Rk,j=1,…,nγ , it could be deduced that for the second model we have
H(b)lj===≈= ξ(ϕ(XW(b)e+β(b)e)W(b)h+β(b)h)lj ξ([ϕ(xe1wb1+βbel1),…,ϕ(xelwbek+βbelk)]wbhj+βbj) ∑i=1nkξ(ϕ(xlwbei+βbeli)wbhli+βbhij) kE[ξ(ϕ(xlwbe+βbe)whb+βbh)] kE[ξ(ϕ(xlwe+βe)wh+βh)].
View SourceSince all the we , wh and βe , βh are drawn from the same distribution, the expectations of the above two composite distributions are obviously the same. Hence, it is clear that
H(b)lj≜1nH(a)lj.
View SourceRight-click on figure for MathML and additional features.Therefore, we could conclude that under the given assumption, H(a) and H(b) are also equivalent if the normalization operator is applied.

C. Incremental Learning for Broad Expansion: Increment of Additional Enhancement Nodes
In some cases, if the learning cannot reach the desired accuracy, one solution is to insert additional enhancement nodes to achieve a better performance. Next, we will detail the broad expansion method for adding p enhancement nodes. Denote Am=[Zn|Hm] and Am+1 as
Am+1≡[Am|ξ(ZnWhm+1+βhm+1)]
View SourceRight-click on figure for MathML and additional features.where Whm+1∈Rnk×p , and βhm+1∈Rp . The connecting weights and biases from mapped features to the p additional enhancement nodes, are randomly generated.

By the discussion in Section II, we could deduce the pseudoinverse of the new matrix as
(Am+1)+=[(Am)+−DBTBT](13)
View Sourcewhere D=(Am)+ξ(ZnWhm+1+βhm+1)
BT={(C)+(1+DTD)−1BT(Am)+if C≠0if C=0(14)
View Sourceand C=ξ(ZnWhm+1+βhm+1)−AmD .

Again, the new weights are
Wm+1=[Wm−DBTYBTY].(15)
View SourceRight-click on figure for MathML and additional features.

The broad learning construction model and learning procedure is listed in Algorithm 1, meanwhile, the structure is illustrated in Fig. 5. Notice that all the peseudoinverse of the involved matrix are calculated by the regularization approach in (3). Specifically, this algorithm only needs to compute the pseudoinverse of the additional enhancement nodes instead of computations of the entire (Am+1) and thus results in fast incremental learning.


Fig. 5.
Broad learning: increment of p additional enhancement nodes.

Show All


Algorithm 1
Broad learning: Increment of p Additional Enhancement Nodes

Show All

D. Incremental Learning for Broad Expansion: Increment of the Feature Mapping Nodes
In various applications, with the selected feature mappings, the dynamic increment of the enhancement nodes may not be good enough for learning. This may be caused from the insufficient feature mapping nodes that may not extract enough underlying variation factors which define the structure of the input data.

In popular deep structure networks, when the existing model could not learn the task well, general practices are to either increase the number of the filter (or window) or increase the number of layer. The procedures suffer from tedious learning by resetting the parameters for new structures.

Instead, in the proposed BLS, if the increment of a new feature mapping is needed, the whole structure can be easily constructed and the incremental learning is applied without retraining the whole network from the beginning.

Here, let us consider the incremental learning for newly incremental feature nodes. Assume that the initial structure consists of n groups feature mapping nodes and m groups broad enhancement nodes. Considering that the (n+1) th feature mapping group nodes are added and denoted as
Zn+1=ϕ(XWen+1+βen+1).(16)
View SourceRight-click on figure for MathML and additional features.The corresponding enhancement nodes are randomly generated as follows:
Hexm=[ξ(Zn+1Wex1+βex1),…,ξ(Zn+1Wexm+βexm)](17)
View SourceRight-click on figure for MathML and additional features.where Wexi and βexi are randomly generated. Denote Amn+1=[Amn|Zn+1|Hexm] , which is the upgrade of new mapped features and the corresponding enhancement nodes. The relatively upgraded pseudoinverse matrix should be achieved as follows:
(Amn+1)+=[(Amn)+−DBTBT](18)
View SourceRight-click on figure for MathML and additional features.where D=(Amn)+[Zn+1|Hexm]
BT={(C)+(1+DTD)−1DT(Amn)+if C≠0if C=0(19)
View Sourceand C=[Zn+1|Hexm]−AmnD .

Again, the new weights are
Wmn+1=[Wmn−DBTYBTY].(20)
View Source

Specifically, this algorithm only needs to compute the pseudoinverse of the additional mapped features instead of computing of the entire Amn+1 , thus resulting in fast incremental learning.

The incremental algorithm of the increment feature mapping is shown in Algorithm 2. And the incremental network for additional (n+1) feature mapping as well as p enhancement nodes is shown in Fig. 6.


Fig. 6.
Broad learning: increment of n+1 mapped features.

Show All

Algorithm 2 - Broad learning: Increment of 
$n+1$
 Mapped Features
Algorithm 2
Broad learning: Increment of n+1 Mapped Features

Show All

E. Incremental Learning for the Increment of Input Data
Now let us come to the cases that the input training samples keep entering. Often, once a system modeling is completed and if a new input with a corresponding output enters to model, the model should be updated to reflect the additional samples. The algorithm in this section is designed to update the weights easily without an entire training cycle.

Denote Xa as the new inputs added into the neural network, and denote Amn as the n groups of feature mapping nodes and m groups of enhancement nodes of the initial network. The respectively increment of mapped feature nodes and the enhancement nodes are formulated as follows:
Ax= [ϕ(XaWe1+βe1),…,ϕ(XaWen+βen)|  ξ(ZnxWh1+βh1),…,ξ(ZnxWhm+βhm)](21)(22)
View SourceRight-click on figure for MathML and additional features.where Zxn=[ϕ(XaWe1+βe1),…,ϕ(XaWen+βen)] is the group of the incremental features updated by Xa . The Wei , Whj and βei,βhj are randomly generated during the initial of the network. Hence, we have the updating matrix
xAmn=[AmnATx].
View SourceThe associated pseudoinverse updating algorithm could be deduced as follows:
(xAmn)+=[(Amn)+−BDT|B](23)
View SourceRight-click on figure for MathML and additional features.where DT=ATxAmn+
BT={(C)+(1+DTD)−1(Amn)+Dif C≠0if C=0(24)
View SourceRight-click on figure for MathML and additional features.and C=ATx−DTAmn .

Therefore the updated weights are
xWmn=Wmn+(YTa−ATxWmn)B(25)
View Sourcewhere Ya are the respectiv labels of additional Xa .

Similarly, the input nodes updating algorithm is shown in Algorithm 3. And the network illustration is checked in Fig. 7. Again, this incremental learning saves time for only computing necessary pseudoinverse. This particular scheme is perfect for incremental learning for new incoming input data.


Fig. 7.
Broad learning: increment of input data.

Show All

Algorithm 3 - Broad Learning: Increment of Feature-Mapping Nodes, Enhancement Nodes, and New Inputs
Algorithm 3
Broad Learning: Increment of Feature-Mapping Nodes, Enhancement Nodes, and New Inputs

Show All

Remark 1:
So far a general framework of the proposed BLS is presented, while the selection of functions for the feature mapping deserves attention. Theoretically, functions ϕi(⋅) have no explicit restrictions, which means that the common choices such as kernel mappings, nonlinear transformations, or convolutional functions are acceptable. Specifically, if the function in the feature mapping uses convolutional functions, the broad learning network structure is very similar to that of classical CNN structure except that the broad learning network has additional connecting links between the convolutional layers and the output layer. Furthermore, additional connections, either laterally or stacking, among the feature nodes can be explored.

Remark 2:
The proposed BLS is constructed based on the characteristics of the flatted functional-link networks. In general, such flatted functional extensions and incremental learning algorithms could be used in various networks such as support vector machine or RBF. The pseudoinverse computation is used here. This can be replaced with an iterative algorithm if desired. Gradient descent approach can be used to find the weights of enhancement nodes as well if it is desired.

F. Broad Learning: Structure Simplification and Low-Rank Learning
After the broad expansion with added mapped features and enhancement nodes via incremental learning, the structure may have a risk of being redundant due to poor initialization or redundancy in the input data.

Generally, the structure can be simplified by a series of low-rank approximation methods. In this section, we adapt the classical SVD as a conservative choice to offer the structure simplification for the proposed broad model.

The simplification can be done in different ways: 1) during the generation of mapped features; 2) during the generation of enhancement nodes; or 3) in the completion of broad learning.

1) SVD Simplification of Mapping Features:
Let us begin with the random initial network with n groups of feature nodes that can be represented as the equation of the following form:
Y== [ϕ(XWe1+βe1),…,ϕ(XWen+βen)]W0n [Z1,…,Zn]W0n.
View SourceRight-click on figure for MathML and additional features.Similarly, from the previous sections, we denote by A0n=[Z1,…,Zn] , which yields
Y=A0nW0n.
View SourceRight-click on figure for MathML and additional features.To explore the characteristics of the matrix A0n , we apply SVD to Zi , i=1,…,n as follows:
Zi=== UZiΣPZiVZiT UZi⋅[ΣPZi|ΣQZi]⋅[VPZi|VQZi]T UZiΣPZiVPZiT+UZiΣQZiVQZiT=ZPi+ZQi(26)(27)(28)
View Sourcewhere ΣP and ΣQ are divided by the order of singularities, under the parameter ε .

Remember that our motivation is to achieve a satisfactory reduction of the numbers of nodes. The idea is to compress Zi by the principal portion, ZPi . The equation between Zi and ZPi is derived as follows:
ZPiVPZi==== UZiΣPZiVPZiTVPZi UZiΣPZiVPZiTVPZi+UZiΣPZiVQZiTVPZi UZi⋅[ΣPZi|ΣQZi]⋅[VPZi|VQZi]T⋅VPZi ZiVPZi.
View SourceRight-click on figure for MathML and additional features.As for the original model, define
W0n≜[W{0,n}Z1|…|W{0,n}Zn]T
View Sourcewe have that
Y====== A0nW0n [Z1,…,Zn]W0n Z1W{0,n}Z1+⋯+ZnW{0,n}Zn Z1VPZ1VPZ1TW{0,n}Z1+⋯+ZnVPZnVPZnTW{0,n}Zn [Z1VPZ1,…,ZnVPZn]⎡⎣⎢⎢⎢VPZ1TW{0,n}Z1⋯VPZnTW{0,n}Zn⎤⎦⎥⎥⎥ A{0,n}FW{0,n}F
View Sourcewhere
W{0,n}F=⎡⎣⎢⎢⎢VPZ1TW{0,n}Z1⋯VPZnTW{0,n}Zn⎤⎦⎥⎥⎥.
View SourceRight-click on figure for MathML and additional features.

Finally, by solving a least square linear equation, the model is refined to
Y=A{0,n}FW{0,n}F(29)
View SourceRight-click on figure for MathML and additional features.where
W{0,n}F=(A{0,n}F)+Y.(30)
View SourceHere, (A{0,n}F)+ is the pseudoinverse of A{0,n}F . In this way, the original A0n is simplified to A{0,n}F .

2) SVD Simplification of Enhancement Nodes:
We are able to simplify the structure after adding a new group of enhancement nodes to the network. Suppose that the n groups of feature mapping nodes and m groups of enhancement nodes have been added, and the network is
Y=A{m,n}FW{m,n}F
View Sourcewhere
A{m,n}F=[Z1VPZ1,…,ZnVPZn|H1VPH1,…,HmVPHm]
View SourceRight-click on figure for MathML and additional features.and
Hj=ξ([Z1VPZ1,…,ZnVPZn]Whj+βhj).
View SourceRight-click on figure for MathML and additional features.In the above equations, HPjs , j=1,…,m , are obtained by the same way as ZPi , which means
Hj=== UHjΣPHjVHjT UHj⋅[ΣPHj|ΣQHj]⋅[VPHj|VQHj]T UHjΣPHjVPHjT+UHjΣQHjVQHjT=HPj+HQj.(31)(32)(33)
View SourceSimilarly, the simplified structure is obtained by substituting Hj by HjVPHj .

3) SVD Simplification of Inserting Additional p Enhancement Nodes:
Without loss of generality, based on the above assumptions, we deduce an SVD simplification for inserting additional p enhancement nodes as follows:
A{m+1,n}=[A{m,n}F|Hm+1]
View Sourcewhere Hm+1=ξ([Z1VPZ1,…,ZnVPZn]Whm+1+βhm+1) .

Similarly, as the SVD implement in the previous steps, we have that
A{m+1,n}F=[A{m,n}F|Hm+1VPHm+1].
View SourceRight-click on figure for MathML and additional features.To update the pseudoinverse of A{m+1,n}F , similar to (13)–(15), we conclude that
(A{m+1,n}F)+=[(A{m,n}F)+−DBTBT](34)
View Sourcewhere D=(A{m,n}F)+Hm+1VPHm+1
BT={(C)+(1+DTD)−1DT(A{m,n}F)+if C≠0if C=0(35)
View Sourceand C=Hm+1VPHm+1−A{m,n}FD . The new weights are
W{m+1,n}F=[W{m,n}F−DBTYBTY].(36)
View SourceRight-click on figure for MathML and additional features.Here, W{m+1,n}F is the least square solution of the following model:
Y=A{m+1,n}FW{m+1,n}F.
View Source

4) SVD Simplification of Completion of Broad Learning:
Now, it seems that a complete network is built; however, there is a need to simplify more. A possible solution is to cut off more small singular values components.

Therefore, we have that
A{m,n}F==== UFΣFVTF UF⋅[ΣPF|ΣQF]⋅[VPF|VQF]T UFΣPFVPFT+UFΣQFVQFT A{m,n}FP+A{m,n}FQ.
View SourceRight-click on figure for MathML and additional features.Similar to the beginning of the algorithm, set
AF=A{m,n}FVPF
View SourceRight-click on figure for MathML and additional features.we have an approximation of the matrix as follows:
Y=AFWF(37)
View Sourcewhere
WF=AF+Y.(38)
View SourceFinally, the structure simplified broad learning algorithm is given in Algorithm 4. Generally, the number of the neural nodes could be significantly reduced depending on the threshold values εe , εh , and ε , for simplifying the feature mapping nodes, the enhancement nodes, and the final structure, respectively.

Algorithm 4 - Broad Learning: SVD Structure Simplification
Algorithm 4
Broad Learning: SVD Structure Simplification

Show All

SECTION IV.Experiment and Discussion
In this section, experimental results are given to verify the proposed system. To confirm the effectiveness of the proposed system, classification experiments are applied on popular MNIST and NORB data. To prove the effectiveness of BLS, we will compare the classification ability of our method with existing mainstream methods, including stacked auto encoders (SAE) [6], another version of stacked autoencoder (SDA) [52], DBN [5], multilayer perceptron-based methods (MLP) [53], DBM [7], two kinds of extremely learning machine (ELM)-based multilayer structure, which denoted as MLELM [54] and HELM [10], respectively. The above comparing experiments are tested on MATLAB software platform under a laptop that equips with Intel-i7 2.4 GHz CPU, 16 GB memory. The classification results of the above methods are cited from [10].

Furthermore, we compare our results with an extended fuzzy restricted Boltzmann machine (FRBM) [13] which offers a more reasonable, solid, and theoretical foundation for establishing FRBMs. The one-layer FRBM and our proposed broad learning results are tested on a 3.40-GHz Intel i7–6700 CPU processor PC with MATLAB platform. It should be noted that duplicate experiments are tested in the sever computer equips with 2.30 GHz Intel Xeon E5–2650 CPU processor and the testing accuracy and the training time are given with a special superscript ∗ .

A state-of-art deep CNN [3] has achieved extremely good results even on the ImageNet challenge. However, this is generally done with the help of a very deep structure or the ensembles of various operations. In this paper, only the comparison of the original deep CNN (LeNet5) in [22] is presented here for fair comparison because the proposed BLS only uses linear feature mapping. Related experiments about CNN are tested in the above server computer on Theano platform.

Generally, all the methods we mentioned above, except for HELM and MLELM, are deep structures and the hyperparameters are tuned based on the back propagation whose initial learning rates are set as 0.1 and the decay rate for each learning epoch is set as 0.95. As for the ELM-based networks, the regularization parameters of MLELM are set as 10−1, 103, and 108, respectively, while the penalty parameter of HELM is 108. Other detailed parameters could be checked in [10]. In our proposed BLS, the regularization parameter λ for ridge regression is set as 10−8, meanwhile, the one layer linear feature mapping with one step fine-tune to emphasize the selected features is adopted. In addition, the associated parameters Wei and βei , for i=1,…,n are drawn from the standard uniform distributions on the interval [−1, 1]. For the enhancement nodes, the sigmoid function is chosen to establish BLS. At the end of this section, experimental results based on the proposed broad learning algorithms are given.

A. MNIST Data
In this section, a series of experiments is focused on the classical MNIST handwritten digital images [8]. This data set consists of 70 000 handwritten digits partitioned into a training set containing 60 000 samples and a test set of 10 000 samples. Every digit is represented by an image with the size of 28×28 gray-scaled pixels. Typically, images are shown in Fig. 8.


Fig. 8.
MNIST data set [51].

Show All

To test the accuracy and efficiency of our proposed broad learning structures in the classification, we give a priori knowledge about the numbers of feature nodes and enhancement nodes. However, this is exactly the normal practice for building the network in the deep learning neural networks, which is in fact the most challenging task of the whole learning process. In our experiments, the network is constructed by total 10×10 feature nodes and 1×11000 enhancement nodes. For reference, the deep structures of SAE, DBN, DBM, MLELM, and HELM is 1000 – 500 – 25 – 30, 500 – 500 – 2000, 500–500–1000, 700–700–15 000, and 300–300–12 000, respectively. The testing accuracies of mentioned methods and our proposed method are shown in Table I. Although the 98.74% is not the best one, (in fact, the performance of the broad learning is still better than SAE and MLP), the training time in the server is very fast at a surprising level which is 29.9157 seconds, while the testing time in the server is 1.0783 s. Moreover, it should be noticed that the number of feature mapping nodes is only 100 here. This result accords with scholar’s intuition in large scale learning that the information in the practical applications is usually redundancy. More results with different mapped features are given in Table II.

TABLE I Classification Accuracy on MNIST Data Set
Table I- 
Classification Accuracy on MNIST Data Set
TABLE II Classification Accuracy on MNIST Data Set With Different Numbers of Enhancement Nodes
Table II- 
Classification Accuracy on MNIST Data Set With Different Numbers of Enhancement Nodes
Next, we will show the fast and effectiveness of the incremental learning system. And the associated experiments are implemented in the sever computer mentioned above. Let the final structure consists of 100 feature nodes and 11 000 enhancement nodes. Two different initial networks are used to test the incremental learning here.

First, suppose the initial network is set as 10×10 feature nodes and 9000 enhancement nodes. Then, Algorithm 1 is applied to dynamically increase 500 enhancement nodes each time until it reaches 11 000.

Second, three dynamic increments are used here to increase dynamically: 1) the feature nodes; 2) the corresponding enhancement nodes; and 3) the additional enhancement nodes. Scenario is shown in Fig. 6. The network is initially set to have 10×6 feature nodes and 7000 enhancement nodes at the beginning of the Algorithm 3. Then, the feature nodes are dynamically increased from 60 to 100 at the step of 10 in each update, the corresponding enhancement nodes for the additional features are increased at 250 each, and the additional enhancement nodes are increased at 750 each. Or equivalently, at each step, 10 feature nodes and 1000 enhancement nodes are inserted to the network. Table III presents the performance of the above two different dynamic constructions for MNIST classification compared with the results in Table I. The incremental versions have the similar performance as the one-shot construction. It is surprising that the dynamic increments on both feature nodes and enhancement nodes perform the best. This may be caused by the randomness nature of the feature nodes and the enhancement nodes. This implies that dynamic update of the model using incremental learning could present a compatible result; meanwhile, it provides the opportunities to adjust structure and accuracy for the system to match the desired performance.

TABLE III Classification Accuracy on MNIST Data Set Using Incremental Learning
Table III- 
Classification Accuracy on MNIST Data Set Using Incremental Learning
Additional experiment is designed to test the elapse time of the incremental learning. The initial network is set as 10×6 feature nodes and 3000 enhancement nodes. Similarly as the previous case, the feature nodes are dynamically increased from 60 to 100 at the step of 10 in each update, the corresponding enhancement nodes for the additional features are increased at 750 each, and the additional enhancement nodes are increased at 1250 each. The training times and results of each update are presented in Table IV. The result is very competitive to that of the one-shot solution when the network reaches 100 feature nodes and 11 000 enhancement nodes as shown in Table I. It is proven that the incremental learning algorithms are very effective.

TABLE IV Snapshot Results of MNIST Classification Using Incremental Learning
Table IV- 
Snapshot Results of MNIST Classification Using Incremental Learning
Finally, incremental broad learning algorithms on added inputs are tested. On the one hand, suppose the initial network is trained under the first 10 000 training samples in the MNIST data set. Then, incremental algorithm is applied to add dynamically 10 000 input patterns each time until all the 60 000 training samples are fed. The structure of the tested BLS is set as 10×10 feature nodes and 5000 enhancement nodes, and the snapshot results of each update are shown in Table V. On the other hand, experiments for the increment of input patterns and enhancement nodes are tested. First, the network is initially set to have 10×10 feature nodes and 5000 enhancement nodes. Then, the additional enhancement nodes are increased dynamically at 250 each, and the additional input input patterns are increased at 10 000 each. The attractive results of each update could be checked in Table VI.

TABLE V Snapshot Results of MNIST Classification Using Incremental Learning: Increment of Input Patterns
Table V- 
Snapshot Results of MNIST Classification Using Incremental Learning: Increment of Input Patterns
TABLE VI Snapshot Results of MNIST Classification Using Incremental Learning: Increment of Input Patterns and Enhancement Nodes
Table VI- 
Snapshot Results of MNIST Classification Using Incremental Learning: Increment of Input Patterns and Enhancement Nodes
B. NORB Data
NORB data set [55] is a more complicated data set compared with MNIST data set; in a total of 48 600 images, each has 2×32×32 pixels. The NORB contains images of 50 different 3-D toy objects belonging to five distinct categories: 1) animals; 2) humans; 3) airplanes; 4) trucks; and 5) cars, as shown in Figs. 9 and 10. The sampled objects are imaged under various lighting conditions, elevations, and azimuths. The training set contains 24 300 stereo image of 25 objects (five per class) as shown in Fig. 9, while the testing set contains 24 300 image of the remaining 25 objects, as shown in Fig. 10. In our experiments, the network was constructed by the one-shot model which consists of 100×10 feature nodes and 1×9000 enhancement nodes. Compared with the deep and complex structure of DBN and HELM, which is 4000 – 4000 – 4000 and 3000 – 3000 – 15000, respectively, the proposed BLS presents faster training time. The testing results, shown in Table VII, present a pleasant performance, especially the training time of the proposed broad learning. Similar to the MNIST cases, although the accuracy is not the best one, the performance matches with the previous work with a testing time of 6.0299 s in the server. Considering the superfast speed in computation, which is the best among the existing methods, the proposed broad learning and network is very attractive.

TABLE VII Classification Accuracy on NORB Data Set

Fig. 9. - Examples for training figures.
Fig. 9.
Examples for training figures.

Show All

Fig. 10. - Examples for testing figures.
Fig. 10.
Examples for testing figures.

Show All

C. SVD-Based Structure Simplification
In this part, we run simulations using SVD to simplify the structure after the model is constructed. The experiments are tested in MNIST data set. During the experiments, the threshold εe=εh=1 , and ε=N are set, which means that there is no simplification on feature nodes and enhancement nodes generation, but only to keep the first N important principle components in the final simplified network, i.e., apply SVD operation to A{m,n}F . As shown in Table VIII, N is selected as 500, 600, 800, 1000, 1500, 2000, 2500, and 3000.

TABLE VIII Network Compression Result Using SVD Broad Learning Algorithm
Table VIII- 
Network Compression Result Using SVD Broad Learning Algorithm
The Ω in table denotes the network structures of BLS, where the first is the number of feature nodes and the second is the number of the enhancement nodes. Or specifically, summation of the numbers lie in the Ω column is the total number of nodes in the broad network. In the “BLSVD” column of the table, SVD operation is applied to the network and the network is compressed to the desired N nodes. The tests are to compare with the Restrict Boltzmann Machine (RBM) and the original BLS. Parameters of RBM are referred to [5], i.e., the learning rate is 0.05 and the weight decay is 0.001. All of the three methods are repeated ten times. In the table, the minimal test error (MTE) and the average test error (ATE) under all the ten experiments are shown in percentage.

From the table we could obviously observe that when the number of nodes exceeds 1000, both the BLS models have better results than RBM. Moreover, the models selected by SVD significantly improve the accuracy. Specifically, the RBM is in fact trapped in around 3% accuracy no matter how many nodes are added to the network. In addition, the result of the proposed SVD-based broad learning varies in a narrow range than RBM.

D. Performance Analysis
Based on the experiments above, BLS obviously outperforms the existing deep structure neural networks in terms of training speed. Furthermore, compared with other MLP training methods, BLS leads to a promising performance in classification accuracy and learning speed. Compared with hours or days for training and hundreds of epochs of iteration with high-performance computers in deep structure, the BLS can be easily constructed in a few minutes, even in a regular PC.

In addition, it should be mentioned that, from Tables III and IV, the incremental version of broad learning does not lose the accuracy of the classification, even better in the MNIST case.

Furthermore, the broad structure of our system could be simplified by applying series of low-rank approximations. In this paper, only the classical SVD method is discussed and compared with one-layer RBM, the proposed SVD-based broad learning is more stable. A fast structure reduction using different low-rank algorithms can be developed if the SVD is considered as not so efficient.

SECTION V.Conclusion
BLS is proposed in this paper, which aims to offer an alternative way for deep learning and structure. The establishment of the system is based on the idea of RVFLNN.

The designed model can be expanded in wide fashion when new feature nodes and enhancement nodes are needed. The corresponding incremental learning algorithms are also designed. The incremental learnings are developed for fast remodeling in broad expansion without a retraining process if the network deems to be expanded. From the incremental experimental results presented in Table IV, it is shown that the incremental learnings can rapidly update and remodel the system. It is observed that learning time of a one-shot structure is smaller than step-by-step increments version to reach the final structure. However, this incremental learning offers an approach for system remodeling and for model selection, especially in modeling high-volume time-varying systems.

The experiments on MNIST and NORB data confirm the dynamic update properties of the proposed BLS. Finally, the SVD approach is applied to simplify the structure. It is also indicated that the simplified networks demonstrate promising results.

Lastly, with proper arrangement in the feature nodes, the proposed broad learning algorithms and incremental learnings (see Section III) can be applied to a flat network or to a network that only needs to compute the connecting weights of the last layer, such as ELM.