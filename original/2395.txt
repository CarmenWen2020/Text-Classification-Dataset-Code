Federated learning obtains a central model on the server by aggregating models trained locally on clients. As a result, federated learning does not require clients to upload their data to the server, thereby preserving the data privacy of the clients. One challenge in federated learning is to reduce the client-server communication since the end devices typically have very limited communication bandwidth. This article presents an enhanced federated learning technique by proposing an asynchronous learning strategy on the clients and a temporally weighted aggregation of the local models on the server. In the asynchronous learning strategy, different layers of the deep neural networks (DNNs) are categorized into shallow and deep layers, and the parameters of the deep layers are updated less frequently than those of the shallow layers. Furthermore, a temporally weighted aggregation strategy is introduced on the server to make use of the previously trained local models, thereby enhancing the accuracy and convergence of the central model. The proposed algorithm is empirically on two data sets with different DNNs. Our results demonstrate that the proposed asynchronous federated deep learning outperforms the baseline algorithm both in terms of communication cost and model accuracy.

SECTION I.Introduction
Smartphones, wearable gadgets, and distributed wireless sensors usually generate huge volumes of privacy-sensitive data. In many cases, service providers are interested in mining information from these data to provide personalized services, for example, to make more relevant recommendations to clients. However, the clients are usually not willing to allow the service provider to access the data for privacy reasons.

Federated learning is a recently proposed privacy-preserving machine learning framework [1]. The main idea is to train local models on the clients, send the model parameters to the server, and then aggregate the local models on the server. Since all local models are trained upon data that are locally stored in clients, data privacy can be preserved. The whole process of the typical federated learning is divided into communication rounds, in which the local models on the clients are trained on the local data sets. For the k th client, where k∈S , and S refers to the participating subset of m clients, its training samples are denoted as Pk , and the trained local model is represented by the model parameter vector ωk . In each communication round, only models of the clients belonging to the subset S will download the parameters of the central model from the server and use them as the initial values of the local models. Once the local training is completed, the participating clients send the updated parameters back to the server. Consequently, the central model can be updated by aggregating the updated local models, i.e., ω=Agg(ωk) [1]–[2][3].

In this setting, the local models of each client can be any type of machine learning models, which can be chosen according to the task to be accomplished. In most existing work on federated learning [1], deep neural networks (DNNs), e.g., long short-term memory (LSTM), are employed to conduct text-word/text-character prediction tasks. In recent years, DNNs have been successfully applied to many complex problem-solving, including text classification, image classification, and speech recognition [4]–[5][6]. Therefore, DNNs are widely adopted as the local model in federated learning, and the stochastic gradient descent (SGD) is the most popular learning algorithm for training the local models.

As aforementioned, one communication round includes parameter download (on clients), local training (on clients), trained parameter upload (on clients), and model aggregation (on the server). Such a framework appears to be similar to a distributed machine learning algorithm [7]–[8][9][10][11][12]. In federated learning, however, only the models’ parameters are uploaded and downloaded between the clients and server, and the data of local clients are not uploaded to the server or exchanged between the clients. Accordingly, the data privacy of each client can be preserved.

Compared with other machine learning paradigms, federated learning is subject to the following challenges [1], [13].

Unbalanced Data: The data amount on different clients may be highly imbalanced because there are light and heavy users.

Non-IID Data: The data on the clients may be strongly non-independent and identically distributed (IID) because of the different preferences of different users. As a result, local data sets are not able to represent the overall data distribution, and the local distributions are different from each other, too. The IID assumption in distributed learning that training data are distributed over local clients uniformly at random [14] usually does not hold in federated learning.

Massively Distributed Data: The number of clients is large. For example, the clients may be mobile phone users [2], which can be enormous.

Unreliable Participating Clients: It is common that a large portion of participating clients is often offline or on unreliable connections. Again in case the clients are mobile phone users, their communication state can vary a lot and, thus, cannot ensure their participation in each round of learning [1].

Apart from the above-mentioned challenges, the total communication cost is often used as an overall performance indicator of federated learning due to the limited bandwidth and battery capacity of mobile phones. Of course, like other learning algorithms, the learning accuracy, which is mainly determined by the local training and the aggregation strategy, is also of great importance. Accordingly, the motivation of our article is to reduce the communication cost and improve the accuracy of the central model, assuming that DNNs are used as the local learning models. Inspired by interesting observations in fine-tuning of DNNs [15], a layerwise asynchronous update strategy for local model updating and aggregation is proposed to improve the communication efficiency in each round. Note that in the field of evolutionary optimization, evolutionary algorithms have also adopted asynchronous strategies for enhancing computational efficiency, e.g., parallel asynchronous particle swarm optimization (PAPSO) [16]. Concretely, motivations behind our layerwise asynchronous strategy and the one adopted by PAPSO, i.e., higher communication efficiency versus computational efficiency, make the essential difference. Driven by these aforementioned motivations, our proposed algorithm addresses the parameters from shallow and deeps layers in an asynchronous manner, while PAPSO updates particle positions and velocities in parallel and asynchronously.

The main contributions of this article are as follows. First, an asynchronous strategy that aggregates and updates the parameters in the shallow and deep layers of DNNs at different frequencies is proposed to reduce the number of parameters to be communicated between the server and clients. Second, a temporally weighted aggregation strategy is suggested to more efficiently integrate information of the previously trained local models in model aggregation to enhance the learning performance.

The remainder of this article is organized as follows. In Section II, related work is briefly reviewed. The detail of the proposed algorithm, especially the asynchronous strategy, the temporally weighted aggregation, and the overall framework are described in Section III. Section IV presents the experimental results and discussions. Finally, conclusions are drawn in Section V.

SECTION II.Related Work
Konečný et al. [2] developed the first framework of federated learning and also experimentally proved that existing machine learning algorithms are not suited for this setting. Konečný et al. [3] proposed two ways to reduce the uplink communication costs, i.e., structured updates and sketched updates, using data compression/reconstruction techniques. A more recent version of federated learning, federated averaging (FedAVG), was reported in [1], which was developed for obtaining a central prediction model of Google’s Gboard app and can be embedded in a mobile phone to protect the user’s privacy. The pseudocode of FedAVG is provided in Algorithm 1 [1].

SECTION Algorithm 1FedAVG [1]
function SERVEREXECUTION ▹ Run on the server

initialize w0

for each round t=1,2,… do

m← max(C⋅K,1 )

St← (random set of m clients)

for each client k∈St in parallel do

wkt+1← ClientUpdate(k,wt )

end for

wt+1←∑Kk=1nknwkt+1

end for

end function

function CLIENTUPDATE(k,w ) ▹ Run on client k

B← (split Pk into batches of size B )

for each local epoch i from 1 to E do

for batch b∈B do

w←w−η▽ℓ(w;b) )

end for

end for

return w to server

end function

In the following, we briefly explain the main components of FedAVG.

Server Execution consists of the initialization and communication rounds.

Initialization: Line 2 initializes parameter ω0 .

Communication Rounds: Line 4 obtains m , the number of participating clients; K indicates the number of local clients, and C corresponds to the fraction of participating clients per round, according to which line 5 randomly selects participating subset St . In lines 6–8, subfunction ClientUpdate is called in parallel to get ωkt+1 . Line 9 executes aggregation to update ωt+1 .

Client Update: The subfunction gets k and ω . B and E are the local mini-batch size and the number of local epochs, respectively. η is the learning rate. Line 14 splits data into batches; where Pk corresponds to the set of indices of the data points on client k , with nk=|Pk| . Lines 15–19 execute the local SGD on batches. Line 20 returns the local parameters.

The equation in line 9, ωt+1←∑Kk=1(nk/n)∗ωkt+1 , described the aggregation strategy, where nk is the sample size of the k th client and n is the total number of training samples. ωkt+1 comes from client k in round t+1 ; however, it is not always updated in the current communication round. For clients that do not participate, their models remain unchanged until they are chosen to participate. In model aggregation, the parameters uploaded from clients in the current round and those in previous ones contribute equally to the central model. In FedAVG, parameters in the shallow and deep layers are always synchronously updated. The parameters uploaded from the clients not participating in the current round are the same as in the previous rounds.

Apart from reducing communication costs, other studies of federated learning have focused on protocol security. For example, to tackle differential attacks, Gayer et al. [17] proposed an algorithm incorporating a preserving mechanism into federated learning for client-sided differential privacy. Bonawitz et al. [18] designed an efficient and robust transfer protocol for secure aggregation of high-dimensional data.

While privacy preserving and reduction of communication costs are two important aspects to take into account in federated learning, the main concern remains to be the enhancement of learning performance of the central model on all data. The loss function of federated learning is defined as F(ω)=∑Kk=1(nk/n)fk(ω) , where fk(ω) is the loss function of the k th client model. Clearly, the performance of federated learning heavily depends on the model aggregation strategy.

Not much work has been reported on reducing the communication cost by reducing the number of parameters to be uploaded and downloaded between clients and the server except for some recent work reported most recently [19]. In this article, we present a layerwise asynchronous model learning model that updates only part of the model parameters to reduce communication and a temporally weighted aggregation strategy that gives a higher weight on more recent models in aggregation to enhance learning performance.

SECTION III.Layerwise Asynchronous Model Update and Temporally Weighted Aggregation
A. Layerwise Asynchronous Model Update
The most intrinsic requirement for decreasing the communication cost of federated learning is to upload/download as little data as possible without deteriorating the performance of the central model. To this end, we present in this article a layerwise asynchronous model update strategy that updates only part of the local model parameters to reduce the amount of data to be uploaded or downloaded.

Our idea was primarily inspired by the following interesting observations made in fine-tuning DNNs [15], [20].

Shallow layers in a DNN learn the general features that are applicable to different tasks and data sets, meaning that a relatively small part of the parameters in DNNs (those in the shallow layers) represent features general to different data sets.

By contrast, deep layers in a DNN learn ad hoc features related to specific data sets, and a large number of parameters focus on learning features in specific data.

These above-mentioned observations indicate that the relatively smaller number of parameters in the shallow layers is more pivotal for the performance of the central model in federated learning. Accordingly, parameters of the shallow layers should be updated more frequently than those parameters in the deep layers. Therefore, the parameters in the shallow and deep layers in the models can be updated asynchronously, thereby reducing the amount of data to be sent between the server and clients. We term this layerwise asynchronous model update.

The DNN employed for the local models on the clients is shown in Fig. 1. As shown in Fig. 1, it can be separated into shallow layers for learning general features and deep layers for learning specific-feature layers features, which are denoted as ωg and ωs , respectively. The sizes of ωg and ωs are denoted as Sg and Ss , respectively, and typically, Sg≪Ss . In the proposed asynchronous learning strategy, ωg will be updated and uploaded/downloaded more frequently than ωs . Assume that the whole federated process is divided into loops, and each loop has T rounds of model updates. In each loop, ωg will be updated and communicated in every round, while ωs will be updated and communicated in only fe rounds, where fe<T . As a result, the number of reduced parameters to be exchanged between the server and clients, i.e., the reduced communication cost, will be fe∗Ss . This way, the communication cost can be significantly reduced, since Ss is usually very large in DNNs.


Fig. 1.
Illustration of shallow and deep layers of a DNN.

Show All

An example is given in Fig. 2 to show the asynchronous learning strategy. The abscissa and ordinate denote the communication round and the local client, respectively. In this example, there are five local devices, i.e., {A,B,C,D,E} , and a server. Point (A,t ) indicates that client A is participating in updating the global model in round t .


Fig. 2.
Parameter exchange. (a) Synchronous model update strategy. (b) Asynchronous model update strategy.

Show All

Fig. 2(a) provides an illustration of a conventional synchronous aggregation strategy, where both ωg and ωs are uploaded/downloaded in each round. The gray rounded rectangles in the bottom represent the aggregation, in which both shallow and deep layers participate in all rounds. By contrast, Fig. 2(b) shows the proposed asynchronous learning strategy. In this example, there are six computing rounds (t−5,t−4,…,t ) in the loop, and the parameters of deep layers are exchanged in rounds t−1 and t only. As a result, the number of reduced parameters to be communicated is 2/3∗Ss .

B. Temporally Weighted Aggregation
In federated learning, the aggregation strategy (line 9 in Algorithm 1) usually weights the importance of each local model based on the size of the training data on the corresponding client. That is, the larger nk is, the more the local on client k will contribute to the central model, as shown in Fig. 3. In that example, the blue diamonds represent the previously trained local models, which do not take part in the t+1 th update of the central model, and only the orange dots are the most recently updated local model on each client and will participate in the current round of aggregation. All participating local models (orange dots in Fig. 3) will be weighted by their data size only regardless the computing round in which these models are updated. In other words, local models updated in round t−p are as important as those updated in round t , which might not be reasonable.


Fig. 3.
Conventional aggregation strategy.

Show All

In federated learning, however, the training data on each participating client are changing in each round and, therefore, models that are more recently updated should have a larger weight in the aggregation. Accordingly, the following model aggregation method taking into account of timeliness of the local models is proposed.
ωt+1←∑k=1Knkn∗(e/2)−(t−timestampk)∗ωk(1)
View Sourcewhere e is the natural logarithm used to depict the time effect, t means the current round, and timestampk is the round in which the newest ωk was updated. The proposed temporally weighted aggregation is shown in Fig. 4. Similar to the setting used in Fig. 3, all clients participating in the aggregation are denoted by an orange dot, while others are represented by blue diamonds. The darker the color is, the larger the weight of this local model will have in the aggregation.


Fig. 4.
Illustration of the temporally weighted aggregation.

Show All

C. Framework
The framework of the proposed federated learning with a layerwise asynchronous model update and temporally weighted aggregation is shown in Fig. 5. A detailed description of the main components will be given in Section III-D.


Fig. 5.
Federated learning with a layerwise asynchronous model update and temporally weighted aggregation.

Show All

D. Temporally Weighted Asynchronous Federated Learning
The pseudocode for the two main components of the proposed temporally weighted aggregation asynchronous (ASTW) federated learning ASTW_FedAVG, one implemented on the server, and the other on the clients is given in Algorithms 2 and 3, respectively.

Algorithm 2 Server Component of ASTW_FedAVG
function SERVEREXECUTION ▹ Run on the server

initialize ω0

for each client k∈{1,2,…,K} do

timestampkg←0

timestampks←0

end for

for each round t=1,2,… do

if t mod rounds_in_loop∈setES then§

flag← True

else

flag← False

end if

m← max(C∗K,1 )

St← (random set of m clients)

for each client k∈St in parallel do

if flag then

ωk← ClientUpdate(k,ωt,flag )

timestampkg←t

timestampks←t

else

ωkg← ClientUpdate(k,ωg,t,flag )

timestampkg←t

end if

end for

ωg,t+1←∑Kk=1nkn∗fg(t,k)∗ωkg†

if flag then

ωs,t+1←∑Kk=1nkn∗fs(t,k)∗ωks‡

end if

end for

end function

§rounds_in_loop=15 and setES={11,12,13,14,0}

†fg(t,k)=a−(t−timestampkg)

‡fs(t,k)=a−(t−timestampks)

Algorithm 3 Client Component of ASTW_FedAVG
function CLIENTUPDATE(k,w,flag ) ▹ Run on client k

B← (split Pk into batches of size B )

if flag then

ω←ω

else

ωs←ω

end if

for each local epoch i from 1 to E do

for batch b∈B do

ω←ω−η∗▽ℓ(w;b)

end for

end for

if flag then

return ω to server

else

return ωs to server

end if

end function

The part to be implemented on the server consists of an initialization step followed by a number of communication rounds. In initialization (Algorithm 2, lines 2–6), the central model ω0 , timestamps timestampg , and timestamps are initialized. Timestamps are stored and to be used to weight the timeliness of corresponding parameters in aggregation.

The training process is divided into loops. Lines 8–12 in Algorithm 2 set flag to be true in the last 1/freq rounds in each loop. Assume there are rounds_in_loop rounds in each loop. Lines 13 and 14 randomly select a participating subset St of the clients according to C , which is the fraction of participating clients per round. In lines 15–24, subfunction ClientUpdate is called in parallel to get ωk/ωkg , and the corresponding timestamps are updated. flag specified whether all layers or the shallow layers only will be updated and communicated. Then in lines 25–28, the aggregation is performed to update ωg . Note that compared with 1, a parameter a is introduced into the weighting function in line 25 or line 27 to examine the influence of different weightings in the experiments. In this article, a is set to e or e/2 .

The implementation of the local model update (Algorithm 3) is controlled by three parameters, k , ω , and flag , where k is the index of the selected client, flag indicates whether all layers or the shallow layers will be updated. B and E denote the local mini-batch size and the local epoch, respectively. In Algorithm 3, line 2 splits data into batches, whereas lines 3–7 set all layers or shallow layers of the local model to be downloaded according to flag . In lines 8–12, local SGD is performed. Lines 13–17 return local parameters.

SECTION IV.Experimental Results and Analyses
A. Experimental Design
We perform a set of experiments using two popular DNN models, i.e., convolution neural networks (CNNs) for image processing and LSTM for human activity recognition (HAR), respectively, to empirically examine the learning performance and communication cost of the proposed ASTW_FedAVG. A CNN with two stacked convolution layers and one fully connected layer [21], [22] is applied on the Modified National Institute of Standards and Technology (MNIST) handwritten digit recognition data set, and an LSTM with two stacked LSTM layers and one fully connected layer is chosen to accomplish the HAR task [23], [24]. Both the MNIST and the HAR data sets are adapted to test the performance of the proposed federated learning framework for different real-world scenarios, e.g., non-IID distribution, unbalanced amount, and massively decentralized data sets, which will be discussed in detail in Section IV-B.

FedAVG [1] is selected as the baseline algorithm since it is the state-of-the-art approach. The proposed ASTW_FedAVG is also compared with two variants, namely, temporally weighted federated learning (TWFL) that adopts temporally weighted aggregation without the layerwise asynchronous model update, and AS_FedAVG that employs the layerwise asynchronous model update without using temporally weighted aggregation. Thus, four algorithms, i.e., FedAVG, ASTW_FedAVG, TW_FedAVG, and AS_FedAVG will be compared in the following experiments.

The most important parameters in the proposed algorithm are listed in Table I. The parameter freq controls the frequency for updating and exchanging the parameters in the deep layers ωs between the server and the local clients in a loop. For instance, freq=5/15 means that only in the last 5 of the 15 rounds, the parameters in the deep layers ωs will be uploaded/downloaded between the server and clients. a is a parameter for adjusting the time effect in model aggregation. K and m are environmental parameters controlling the scale or complexity of the experiments, K denotes the number of local clients, and m is the number of participating clients per round.

TABLE I Parameter Settings

B. Settings on Data Sets
As discussed in Section I, the federated learning framework has its particular challenges, such as non-IID, unbalanced, and massively decentralized data sets. Therefore, the data sets used in our experiments should be designed to reflect these challenges. The generation of the client data set is described in detail in Algorithm 4, which is controlled by four parameters Labels , Nc , Smin , and Smax , where Nc controls the number of classes in each local data set, Smin and Smax specify the range of the size of the local data, and Labels indicates the names of classes involved in the corresponding tasks.

Algorithm 4 Generation of Local Data Sets.
Labels , NC , Smin , and Smax

non-IID and unbalanced local dataset Pk

classes← Choices(Labels ,NC )

L← Len(Labels )

weights← Zeros(L )

P← Zeros(L )

for each class∈classes do

weightsclass← Random(0,1)

end for

sum←∑Lclass=1weightsclass

num← Random(Smin,Smax )

for each class∈classes do

Pclass←weightsclasssum×num

end for

Pk←P

1) Handwritten Digit Recognition Using CNN:
The MNIST data set has ten different kinds of digits and one digit is a 28×28 pixel gray-scale image. To partition the data over local clients, we first sort them by their digit labels and divide them into ten shards, i.e., 0, 1, … , 9. Then, Algorithm 4 is performed to compute Pk , which is the k th client’s partition coefficient corresponding to these shards. In this task, Labels={0,1,2,…,9} ; Nc is randomly chosen from {2, 3}, given K=20 , Smin=1000 and Smax=1600 . For the sake of easy analyses, five partitions/local data sets, namely, 1@MNIST, 2@MNIST, … , 5@MNIST are predefined. Their corresponding 3-D column charts are plotted in Fig. 6.


Fig. 6.
3-D column charts of pregenerated data sets. (a) 1@MNIST. (b) 2@MNIST. (c) 3@MNIST. (d) 4@MNIST. (e) 5@MNIST.

Show All

The architecture of the CNN used for the MNIST task has two 5×5 convolution layers (the first one has 32 channels and the other has 64) and 2×2 max-pooling layers. Then, a fully connected layer with 512 units and ReLu activation is followed. An output layer is a softmax unit. The parameter settings for CNN are listed in Table II.

TABLE II Parameters Settings for the CNN

2) Human Activity Recognition Using LSTM:
In the HAR data set, each data is a sequence of images with a label out of six activities. Similar operations are applied on the HAR data set to divide the data set over local clients. Here, Labels={0,1,2,…,5} ; Nc is randomly chosen from {2, 3}, given K=20 , Smin=250 and Smax=500 .

The architecture of the LSTM used in this article has two 5×5 LSTM layers (the first one with cell_size=20 and time_steps=128 and the other with cell_size=10 and the same time_steps ), a fully connected layer with 128 units and the ReLu activation, and a softmax output layer. The corresponding parameters of the LSTM is given in Table III.

TABLE III Parameter Settings for the LSTM

C. Results and Analysis
Two sets of experiments are performed in this subsection. The first set of experiments examines the influence of the most important parameters, freq , a , K , and m , on the performance of the two strategies using the CNN for the 1@MNIST data set. The second set of experiments compares four algorithms in terms of the communication cost and learning accuracy using the LSTM on the HAR data set since the HAR is believed to be more challenging than the MNIST data set.

1) Effect of the Parameters:
The experiments here are not meant for a detailed sensitivity analysis. These experiments and related discussions aim to offer a basic understanding of parameter settings that may be helpful in practice. We mainly conduct research on the parameters listed in Table I.

In investigating the influence of a particular parameter, all others are set to be their default value. Experiment on freq are carried out for freq={3/15,5/15,7/15} , given a=e/2 , K=20 , and m=2 .

Two metrics are adopted in this article for measuring the performance of the compared algorithms. One is the best accuracy of the central model within 200 rounds, and the other is to the required rounds before the central model’s accuracy reaches 95.0%. Note that the same computing rounds mean the same communication cost. All experiments are independently run for ten times, and their average (AVG) and standard deviation (STDEV) values are presented in Tables IV–VI. In Tables IV–VI, the average value is listed before the standard deviation in parenthesis.

TABLE IV Experimental Results on freq

TABLE V Experimental Results on a

TABLE VI Experimental Results on the Scalability of the Algorithm

Based on the results in Tables IV–VI, the following observations can be made.

Analysis on freq: From the results presented in Table IV, we can conclude that the lower the exchange frequency is, the fewer communication costs will be required, which is very much expected. However, a too low freq will deteriorate the accuracy of the central model.

Analysis on a : a is a parameter that controls the influence of time effect on the model aggregation. When a is set as e , the more recently updated local models are more heavily weighted in the aggregated model, and a takes the value e/2 , the previously updated local models will have a greater impact on the central model. The results in Table V indicate that e/2 is a better option for CNN on the 1@MNIST data set. When a=1 , the algorithm is reduced to AS_FedAVG, meaning that the parameters uploaded in different rounds will be of equal importance in the aggregation.

Both parameters, K and m , leverage the scalability of federated learning. The AVG and STDEV values are calculated based on the recognition accuracy of the CNN on the five predefined data sets. Different combinations of K and m are separately assessed. Based on the results presented in Table VI, the following three conclusions can be made.

First, a larger number of involved clients (a larger m ) leads to higher recognition accuracy. Second, ASTW_FedAVG outperforms FedAVG in most cases, as indicated in the results in the first, third, fourth, and fifth rows in Table VI. Third, FedAVG is slightly better when K , i.e., the total number of clients, is smaller and C is higher, as shown by the results listed in the second row of the table. This implies that the advantage of the proposed algorithm over the traditional federated learning will become more apparent as the number of clients increases. This is encouraging since, in most real-world problems, the number of clients is typically very large.

2) Comparison on Accuracy and Communication Cost:
The following experiments are conducted for testing the overall performance of the algorithms under comparison using the default values of the parameters. Five local data sets are predefined for MNIST and HAR, respectively. For instance, local data set 1 of task MNIST is termed as 1@MNIST. The importance of the temporal weighting is first demonstrated by comparing the changes in the accuracy over the computing rounds. The baseline FedAVG and TW_FedAVG without the asynchronous update are compared, and the results on the MNIST and HAR data sets are shown in Figs. 7 and 8, respectively.


Fig. 7.
Comparative studies on temporally weighted aggregation on data set MNIST using the CNN. (a) 1@MNIST. (b) 2@MNIST. (c) 3@MNIST. (d) 4@MNIST. (e) 5@MNIST.

Show All


Fig. 8.
Comparative studies on temporally weighted aggregation on data set HAR using the LSTM. (a) 1@HAR. (b) 2@HAR. (c) 3@HAR. (d) 4@HAR. (e) 5@HAR.

Show All

The following conclusions can be reached from the results in Figs. 7 and 8. First, the proposed temporally weighted aggregation helps the central model to converge to an acceptable accuracy. On the 1@MNIST and 2@MNIST data sets, TW_FedAVG needs about 30 communication rounds to achieve an accuracy reaches to 95.0%, while the traditional FedAVG needs about 75 rounds to achieve similar accuracy, leading to a reduction of 40% communication cost. Similar conclusions can also be drawn on data sets 3@MNIST, 4@MNIST, and 5@MNIST, although the accuracy of TW_FedAVG becomes more fluctuating. Second, on the HAR data set, a more difficult task, TW_FedAVG can mostly achieve higher accuracy than FedAVG except on 5@HAR. Notably, TW_FedAVG only needs about 75 communication rounds to achieve an accuracy of 90%, while FedAVG requires about 750 rounds, resulting in a nearly 90% reduction of the communication cost. Even on 5@HAR, TW_FedAVG shows a much faster convergence than FedAVG in the early stage. Finally, the temporally weighted aggregation may result in some fluctuations in the learning performance, which may be attributed to the fact that the contributions of some high-quality local models are less weighted in some communication rounds.

Finally, the comparative results of the four algorithms on ten test cases generated from MNIST and HAR tasks are given in Table VII. The listed metrics include the number of rounds needed, the classification accuracy (listed in parenthesis), and total communication cost (Cost. C for short). From these results, the following observations can be made.

Both ASTW_FedAVG and TW_FedAVG outperform FedAVG in most cases in terms of the total number of rounds, the best accuracy, and the total communication cost.

TW_FedAVG achieves the best performance on most tasks in terms of the total number of rounds and the best accuracy. The temporally weighted aggregation strategy accelerates the convergence of the learning and improved the learning performance.

ASTW_FedAVG performs slightly better than TW_FedAVG on MNIST in terms of the total communication cost, while TW_FedAVG works better than ASTW_FedAVG on the HAR data sets. The layerwise asynchronous model update strategy significantly contributes to reducing the communication cost per round.

AS_FedAVG performs the worst among the four compared algorithms. When comparing the performance of the AS_FedAVG only adopting the asynchronous strategy and the ASTW_FedAVG using both of them, the asynchronous one always needs the help of the other strategy.

TABLE VII Experiments on Performance

SECTION V.Conclusion and Future Work
This article aims to reduce the communication costs and improve the learning performance of federated learning by suggesting an asynchronous model update strategy and a temporally weighted aggregation method. Empirical studies comparing the performance and communication costs of the canonical federated learning and the proposed federated learning on the MNIST and human action recognition data sets demonstrate that the proposed asynchronous federated learning with temporally weighted aggregation outperforms the canonical one in terms of both learning performance and communication costs.

This article follows the assumption that all local models adopt the same neural network architecture and share the same hyperparameters, such as the learning rate of SGD. In future research, we are going to develop new federated learning algorithms allowing clients to evolve their local models to further improve the learning performance and reduce communication costs.

