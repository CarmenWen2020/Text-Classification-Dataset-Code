This article addresses the problem of path following for underactuated unmanned surface vessels (USVs) formation via a modified deep reinforcement learning with random braking (DRLRB). A formation control model based on deep reinforcement learning (DRL) is constructed to urge USVs to form a preset formation. Specifically, an efficient reward function is designed from the perspective of velocity and error distance of each USV related to the given formation, and then a novel random braking mechanism is formulated to prevent the training of the decision-making network from falling into the local optimum and failing to achieve the training objectives. Following that, a virtual leader-based path-following guidance system is developed for the USV formation problem. Wherein, with the aid of DRLRB, our proposed system can adjust formation automatically and flexibly even when some USVs deviate from the formation. Simulation verifies the effectiveness and superiority of our formation and path-following control strategy.
Published in: IEEE Transactions on Neural Networks and Learning Systems ( Volume: 32, Issue: 12, Dec. 2021)
Page(s): 5468 - 5478
Date of Publication: 01 April 2021 
ISSN Information:
PubMed ID: 33793404
DOI: 10.1109/TNNLS.2021.3068762
Publisher: IEEE
Funding Agency:
SECTION I.Introduction
Over the last decade, generous achievements have emerged in the application domain of unmanned surface vessel (USV). Meanwhile, to undertake arduous tasks, in general, several underactuated USVs can jointly accomplish missions, including search and rescue, reconnaissance, data collection, etc., [1]–[2][3]. To complete the abovementioned task effectively, the first and foremost for the multiple underactuated USV is the path-following issue [4]–[5][6]. Path following is one of the fundamental issues in the USV control field [7]–[8][9]. Referring to the multiple USVs formation path following, its main purpose is to guarantee USVs to keep a certain formation and navigate on a predetermined path. Following that, we focus on designing an effective formation path-following method for multiple underactuated USVs.

Excellent work has been carried out in the field of path following for the multiple USV formation problems [10]. A decentralized leader-follower formation control problem for a group of fully actuated USVs was addressed in [11], on the condition that each USV should follow its reference trajectory with the prescribed performance constraints. When it comes to the path-following problem for ships, the path-following controllers were designed for each underactuated ship to follow a reference path in [12]. By combining a path-following controller and a path variable synchronization for coordination, a cooperative path-following problem for autonomous underwater vehicles (AUVs) was reported in [13]. It is worth noting that the abovementioned approaches in [11]–[12][13] were dedicated to put forward an individual reference path for each vehicle. However, referring to the USV path-following problem in practical engineering applications, it is rather impossible to conceive the complex path-following controller and achieve that all vehicles follow the complicated paths.

At the same time, the virtual target has been employed to resolve the USV formation path-following problem. Based on the virtual target strategy, the guidance-control structure, and the passivity-based coordination method, a coordination algorithm was proposed in [14] for the path-following issue. The virtual reference target was utilized to keep the multiple nonholonomic mobile robots formation in [15]. Suppose the leader can be treated as a target, a leader-follower error model was built in [16] for the multiple vehicles formation control, and then the formation control problem was converted into the tracking problem. A localization approach based on consensus-unscented particle filter algorithm is proposed in [17] to eliminate the accumulated errors of multi-AUV systems, in which the key information is exchanged with leaders or neighbor followers. A virtual leader was introduced in [18] to achieve the multiple autonomous surface vehicles path following, and the vehicles moved along parameterized paths. Referring to the formation control of a group of underactuated surface vessels with partially known control input functions, the leader-follower and line-of-sight strategies were adopted in [19]. It can be observed that most of the advanced path-following techniques were presented for the fixed formations, while little attention has been paid to real-time dynamic formation transformation in literature.

With the breakthrough of machine learning, the algorithms including deep learning (DL), reinforcement learning (RL), and their combination deep RL (DRL) have been utilized for resolving the path-following problem of underactuated USV formation [20]–[21][22][23][24][25]. To overcome the low usability of general control methods, a concise DRL algorithm with a deep Q network was proposed for obstacle avoidance problems in [20]. The problem of the formation control with collision avoidance of vehicle in a leader-follower structure was addressed in [21] via DRL. A two-stage training framework (imitation learning and RL), long short-term memory network and transfer training approach were adopted. Refer to works on improving DRL-based USV path following, [22]–[23][24][25] have made outstanding contributions. Woo et al. [22] proposed a path-following controller for USV through DRL, the controller has a self-learning capability for path following. In [23], the path-following problem of autoassembly robot was considered. A generalized path-integral-control approach was introduced to ensure fast and reliable training convergence and a novel parameterization method using Lyapunov techniques was introduced to ease practical application. By using a deep deterministic policy gradient (DDPG) model and a reward system, a DRL algorithm was applied for the USV path planning problem in [24]. In which, the usability problem caused by the complicated control law of the traditional method was considered. An adaptive neuro-fuzzy PID controller was designed using twin delayed DDPG in [25], where the neural network was embedded into fuzzy PID controller structure. The combination of fuzzy PID controller and DRL not only reduces the training difficulty of the neural network but also enable to automatically optimize the parameters of controller. It is meaningful that the algorithms deduced from machine learning offer excellent and positive references for the multiple USV formation path-following problem.

Inspired by the abovementioned observations, this article aims at resolving the multiple underactuated USVs path-following problem by employing the DRL algorithm. Some highlights can be: 1) based on DDPG, a path-following strategy named DRL with random braking (DRLRB) is introduced to solve the multiple USV formation path-following problem; 2) to cope with the failure of USVs and other emergency dispatching, the USVs can quit from the formation dynamically; and 3) a novel random brake mechanism is designed for the training of DDPG, and then an excellent performance in formation keeping and path following can be reached. Finally, simulation results validate the effectiveness of our multiple underactuated USVs path-following method DRLRB.

The remainder of this article is organized as follows. Section II formulates the USV formulation path-following problem. Section III details the DRLRB based cooperative path-following method. Section IV presents an illustrative path-following simulation for the multiple USV formulation using our DRLRB. Section V concludes this article.

SECTION II.Problem Formulation
A. USV Motion Model
R3 represents n -dimensional Euclid space. ∥⋅∥ represents Euclidean norm. USVi represents the i th USV in the formation.

Suppose there exists N USV in the group of USVs, and each USV owns the same mechanical structure. As shown in Fig. 1, the motion model [26] of USVi is
η˙i=J(ψci)=Mi=Ci=c13i=J(ψci)νiMiν˙i+Ci(νi)νi+Di(νi)νi+gi(νi)=τi⎡⎣⎢cosψcisinψci0−sinψcicosψci0001⎤⎦⎥⎡⎣⎢m11i000m22im32i0m23im33i⎤⎦⎥⎡⎣⎢00−c13i 0 0 −c23i c13i c23i 0⎤⎦⎥, D=⎡⎣⎢d11i00 0 d22i d32i 0 d23i d33i⎤⎦⎥−m22iv−12(m23i+m32i)ri,c23i=m11iui(1)(2)
View Sourcewhere ηi=[xci,yci,ψci]T is the position vector of USVi in the earth-fixed inertial frame XOY , term (xci,yci) is the position of USVi, ψci is the heading angle. νi=[ui,vi,ri]T is the velocity vector of USVi in the body-fixed frame xbobyb , term ui,vi , and ri are the linear velocities and angular velocity in surge, sway, and yaw directions, respectively. J(ψci) is the rotation matrix between the body-fixed frame and earth-fixed frame. Mi=MTi is the inertia matrix, Ci(νi)∈R3×3 , Di(νi)∈R3×3 and gi(νi)∈R3 denote the Coriolis and centripetal terms matrix, the damping matrix and unmodeled dynamics, respectively. τi=[τui,0,τri]T is the control input of USVi.


Fig. 1.
USV motion model.

Show All

B. USV Formation Path Following
Referring to the multiple USV formation, the virtual leader is introduced to calibrate the reference position of each USV in formation, and its position represents the center position of the formation. As shown in Fig. 2, there are three USVs (USVm,USVj,USVk) in the formation, the virtual leader is USVl and its expected position is pv(xv,yv) . The real position of USVl is pc(xc,yc) . The expected position and real position of USVm are pvm(xvm,yvm) and pcm(xcm,ycm) . ed represents the cross-track error between pc and the point pp(xp,yp) on the preset path, pp(xp,yp) is the projection of pc on the preset path. The position of USVm is determined by βm and lm . xb−pv−yb is a coordinate frame that follows the virtual leader, and the xb -axis is along the tangent direction of the preset path. Then, the expected position (xvm,yvm) of USVm can be
⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪xvm|θ=θ0=xv|θ=θ0+lmcos(arctan(dydx|θ=θ0)+βm)yvm|θ=θ0=yv|θ=θ0+lmsin(arctan(dydx|θ=θ0)+βm).(3)
View Source


Fig. 2.
Cooperation schematic of USVs.

Show All

Following that, assume that there are z USVs in the formation, the error ef between the actual formation and the expected formation can be
ef=∑m+zm(xvm−xcm)2+(yvm−ycm)2−−−−−−−−−−−−−−−−−−−−−−√z.(4)
View Source

To guarantee the multiple USVs formation follows the preset path, the virtual leader should move along the parameterized preset path (xp(θ),yp(θ)) successfully. For this purpose, a set of control inputs are utilized for USVs to decrease the cross-track error between the center of USV formation and the preset path. Thus, the cross-track error ed between the center of USV formation and the preset path can be
ed=(xc(θ)−xp(θ))2+(yc(θ)−yp(θ))2−−−−−−−−−−−−−−−−−−−−−−−−−−−√.(5)
View Source

Suppose all states of USVs can be measured directly. Consequently, the objective of the multiple USV formation path following can be
{limt→∞ef=0limt→∞ed=0.(6)
View Source

It is self-evident that the virtual leader is critical for the multiple USVs formation keeping and path following. Fig. 3 illustrates the updated path-following strategy of virtual leader pv , the central position of the formation pc would be changing dynamically, and the projection pp of pc on the preset path will change correspondingly.


Fig. 3.
Path-following strategy for multiple USVs formation.

Show All

And then, the virtual leader pv is selected on the preset path to keep a distance λ from the pp point. It can be observed that the position update of the virtual leader is also related to the cross-track error ed of path following. Accordingly, to guarantee the effect of the formation keeping and path following, the virtual leader can update continuously with the movement of USV formation. As shown in Fig. 2, once the position of the virtual leader and the relative angle and distance are set, it is easy to obtain the position of USVs in formation. When the position of formation is updated, the DRLRB would make USVs navigate to the expected positions to form the formation.

SECTION III.USV Formation Path-Following Control Model DRLRB
To fulfill the path following of USV formation, the DRLRB is proposed by combining the random braking mechanism in the training. First, state representation and action space are introduced. Second, the reward functions for path following of USV formation are designed. Then, the training objectives and training process of the model are summarized. After the model training, the path-following strategy is designed for the virtual leader to realize the path following of USV formation.

A. DRL Structure for USV Path Following
Fig. 4 presents the path-following control model based on DDPG for the multiple USVs formation. The training model of the path following is a process of interacting with the environment iteratively [27]. Based on state observation from the environment, the USVs can make reasonable decisions. By combining task objective and state observation, the action selection of multiple underactuated USVs formation path-following models can be evaluated. Following that, according to the evaluation value, the multiple underactuated USVs formation path-following model can be trained. Finally, the model would perform the selected action to the corresponding updated environment and regain the observation state. Repeat the above process till the path-following model can select the expected action. Ultimately, the model can learn to make ideal control decisions for the multiple USVs to form the formation and follow the path.


Fig. 4.
USV formation path-following control model based on DDPG.

Show All

Referring to the path-following DRL model, the exploration problem under the unknown environment must be concerned [28]. As shown in Fig. 5, the decision-making network outputs actions for each USV, each USV will transform to a new state after performing the action. In the simulation, the states of USVs can observe and measure. By concatenating states of all USVs, the observed states are obtained. Then, the reward is produced by measuring the behavior of each USV. In the end, all those data are saved to the experience pool, and following that experience data can be selected to form the training data set. By utilizing the training data set, the USV formation control model is trained iteratively. Experience pool is used to store the experience records of USV navigation. The states data involved in an interaction, the actions from the decision-making network, and the corresponding reward are formed an experience record. All experience records stored in the experience pool together constitute training data set.


Fig. 5.
Environment exploration and the collection of the training data set.

Show All

When it comes to the decision-making network, multiple USVs can be trained as a formation. When training the decision-making network, the environment is explored by the above USVs jointly. It is evident that each USV starts to explore from a different initial state, and then the repetition rate of exploration experience can be reduced in essence. Following that, different motion states are explored faster by the multiple USVs, and the training data in the experience pool can be accumulated faster. It can be observed that multiple USVs can be trained together by employing the same decision-making network, and then the consistency of the behavior of multiple USVs in the formation can be guaranteed.

It is worth noting that, the USV formation control model is trained by the cooperation of all USVs in the formation. In the training process, the environment is explored cooperatively by all USVs in the formation, which means that the training is processed under the multi-USVs formation. Meanwhile, exploration experiences from all USVs in the formation are store in the same experience pool together, and the training data are sampled from the whole experience pool. Consequently, the proposed DRLRB is suitable for a multi-USVs formation. Then, in theory, the number of USV in formation is limited by water space. However, in reality, it is also indispensable to consider the efficiency of algorithm execution and hardware performance.

B. State Representation and Action Space
As elucidated in Section III-A, with the aid of the multiple underactuated USV formation path-following model, the decisions that control USVs can be made according to observation states. Wherein the observation states describe the relations of each USV with the environment and other USVs. It is worth noting that the state representation is pivotal for the performance of the proposed formation path-following model.

After probing into the multiple underactuated USVs formation path-following task, it can be divided into two periods, including the multiple underactuated USVs formation period and the path-following period. Referring to the USV formation period, several factors impact its speed and standard, including the state of each USV, and the relations among USVs, the virtual leader and other USVs. As shown in Fig. 1, states of USVs are characterized by their position (xci,yci) , heading ψci and velocity νi(ui,vi,ri) . The position of virtual leader (xv,yv) is indispensable to calculate the expected positions (xvi,yvi) of USVs. ef represents the performance of USV formation. During the path-following period, the relation between the USV formation and preset path is characterized by the cross-track error ed . Consequently, the observation state of N USVs is described by s=(xc1,yc1,ψc1,u1,v1,r1,xv1,yv1,…,xcN,ycN,ψcN,uN , vN,rN,xvN,yvN,xv,yv,ef,ed ).

When it comes to the energy level of the USV, it is not represented in the observed state. On the one hand, in the path-following issue, it has not been addressed separately as an attribute of individual USV in this article, the multi-USVs formation performs path following as a whole. The energy constraints need to be considered in the path planning phase rather than the path-following phase [29]. On the other hand, referring to the energy level of the USV in the formatting, the energy consumption is related to some factors, where speed, voyage, and displacement are the main determinants [30], [31]. In this article, with the consideration of the different specifications of USVs in formation, and then the formation transformation is designed correspondingly. Once the energies of USVs with low energy level are exhausting, these USVs would withdraw from formation autonomously, and then the USV formation would be reformatted dynamically. The above mechanism of this work will ease the dilemma arising from different energy levels reasonably. As shown in [30], the energy consumption is mainly related to three parameters, including speed, voyage, and displacement. It can be found that in this article, voyages and displacements of USVs in the formation are known, and the speed can be observed within our observation state. Following that, the energy consumption can be calculated and achieved indirectly. Thus, the energy state has not been included in the observated state but can be reflected in our formation control model.

By utilizing the neural decision network, with the observation state input, the corresponding decision action ai=(τui,τri) can be reached, where τui is the trust in surge direction and τri is the moment in yaw. τui and τri would affect the surge velocity ui , and the heading ψci , respectively. Taken into account that, referring to the underactuated USV, the trust in sway direction is leak and can be set to 0. It is noteworthy that the decision actions are distributed in continuous space.

C. Reward Scheme
It is significant that, by employing our reward function, referring to the USV formation mission, USVs can quickly navigate to their corresponding reference coordinate points. To urge USVs to quickly form established formations, velocities of the USVs are embodied in the design of our reward function. In the reward function, when sailing toward the reference points, the speed of USV should be maximized, and lateral deviation speed should be minimized
Rv=νicosβi−νisinβi.(7)
View Source

In the training process, to get the maximum reward, in general, USV will navigate with increasing speed as fast as possible. Following that, USV would be farther and farther away from the reference coordinate point. Consequently, the formation control model would fall into an extremely adverse local optimal. To avoid the above situation, it is indispensable that the distance between USV and reference coordinate point should be stressed when it comes to the design of reward function
Rd=−efef−max.(8)
View Source

Following that, the reward function can be
R=kvRv+kdRd(9)
View Sourcewhere kv and kd are the weights of factors of speed and distance, respectively.

Accordingly, there is a trial and error learning mechanism in the training of DDPG. An Ornstein–Uhlenbeck (OU) process shown in (10) is introduced into DDPG to explore the environment. OU noise is used in disturbing the action from the USV formation control model, and then USVs perform random actions. Through these random actions, the unknown environment and observation states would be discovered. Suppose δ is the mean value of the noise, W is a stochastic noise produced by Brownian motion, and σ is the weight of W . Following that, the OU process can be
dat=θ×(δ−at)dt+σdW.(10)
View Source

The essence of the OU process can be a mean regression with random noise. When the decision-making network makes a decision and reached the corresponding output action at=μ(st) , and then a noise N is added to the above action and can be ano-brake=μ(st)+N . Wherein N can be generated by using an OU process with a mean value of 0.

Remark 1:
When the reward function shown in (9) is utilized to train the formation control model, USV would move forward to the referenced coordinate at the maximum speed. It is evident that to maintain the maximum speed the highest reward can be obtained. However, USV cannot learn the ability to slow down. When USV reaches the referenced coordinate, it will be circling the referenced target point at the maximum speed. Consequently, owing to the existence of this phenomenon, USV is unable to reach the reference coordinate point accurately and cannot stop with ease. Meanwhile, if add the brake action into the action space directly, the braking action would not be selected due to the reduction of the USV speed and the reward value. Therefore, it is significant to force the USV formation control model to select the brake action. In general, the constant braking action would generate adverse experience data from time to time, and then the training is prone to fall in the local optimal situation. Consequently, the random braking strategy is proposed for the model training, and then the above shortcomings can be dissolved effectively.

As shown in Fig. 6, the random braking is introduced after the decision-making network completed the action selection. By employing the random braking mechanism, USV can accelerate quickly when it is far away from the referenced coordinate and can brake properly when it is close to the reference coordinate.


Fig. 6.
Action selection process of our DRLRB model with random braking mechanism.

Show All

D. Learning Objective and Learning Algorithm
Our proposed USV formation control model aims at learning a shared policy a=μ(s|ϖμ) . For this purpose, the actor network is designed to approximate the policy, where ϖμ in actor network is updated by the gradient descent method, and the gradient can be
∇ϖμJ(μ)=Es∼ρ[∇aQ(s,a|ϖQ′)⋅∇ϖμμ(s|ϖμ)|a=μ(s)](11)
View Sourcewhere ρ is the probability distribution, and E represents the expect value of ∇aQ⋅∇ϖμμ . In the network training process, the mini-batch are usually sampled randomly as training data, and then the descent can be
∇ϖμJ(μ)=1N∑i(∇aQ(s,a|ϖQ′i)⋅∇ϖμμ(s|ϖμi)|s=si,a=μ(si)).(12)
View Source

In (12), N is the size of mini-batch and ϖQ′ is the parameter in target critic network. Suppose C(s,a) can concatenate state vector s and action vector a . Q is referred as the target critic and can be
Q(s,a)=(ϖQ′)TC(s,a)(13)
View Sourcewhich can be updated by optimized loss the following equation:
L(ϖ)=E[(r(s,a)+γQ(s′,a′;ϖQ′)−Q(s,a;ϖQ))2](14)
View Sourcewhere ϖQ is the parameter in online critic network, s′ refers to the next state after taking action a at state s , a′ is the action taken at s′ by the policy μ , and r is the reward. The parameter ϖQ in online critic network is updated by the stochastic gradient descent method, and the corresponding gradient can be
∇ϖL(ϖ)=E[(r(s,a)+γQ(s′,a′;ϖQ′)−Q(s,a;ϖQ))∇ϖQ(s,a;ϖQ)].(15)
View Source

Consequently, by employing DDPG, our DRLRB is presented in Algorithm 1.

Algorithm 1 DRLRB
Input:

The velocity vector ν0=[u0,v0,r0]T , the position vector η0=[xc0,yc0,ψc0]T , number of episodes M , number of steps for each episode T , replay memory size R , batch size N , learning rates for decision-making network γ , updated period C of target network.

Output:

The parameters of network ϖQ and ϖμ .

Initialize online critic network Q(s,a|ϖQ) and online actor network μ(s|ϖμ) with weight ϖQ and ϖμ .

Initialize target critic network Q(s,a|ϖQ′) and target actor network μ(s|ϖμ′) with weight ϖQ′ and ϖμ′ .

Initialize experience pool .

for do

Initialize a random process and for state exploration and random braking, respectively.

Reset the initial state .

for do

Select action according to current policy and exploration noise.

Select action according to brake probability.

Execute action , obtain reward and observe new state .

Store transition in .

Sample a random mini-batch of transitions from .

if episode terminates at step then

Set .

else

.

end if

Update critic by minimizing loss .

Update actor policy with sampled policy gradient according to Eq. (12).

Update the target network every stepsand every steps.

end for

end for

E. Design of USV Formation Path-Following Method
Within our DRLRB algorithm, after training the USV formation control model based on the DDPG algorithm, the USV formation control model is combined with the path-following strategy to complete the formation path-following task. As shown in Fig. 7, at first the observation state is obtained according to Section III-B. Then, action decisions that can be utilized for generating commands to control the USV actuator, are reached by using our DRLRB. Following that, the USV actuator executes the above control commands. Accordingly, the formation error of USV can be obtained and used for judging whether the USV formation formed or not. Repeating the above steps till the USV formation is formed. Consequently, the cross-track error of the USV formation path following is calculated, and the virtual leader is updated according to and . By repeating the above steps, and then the USV formation path-following task would be fulfilled eventually.


Fig. 7.
USV formation path-following method.

Show All

Remark 2:
Suppose each USV can communicate with any other USV directly or indirectly. Benefited from our designed superior formation strategy, each USV in formation can dynamically change its position to cope with the adjustment of formation scale and the fault of USVs. In each control cycle, USV performs the process of environmental awareness, decision-making, and action execution sequentially. Referring to environmental awareness, the status of USVs in the USV formation can be observed, and then according to the virtual leader, the referenced position of the USV formation can be updated. If there exists a USV that fails in formation, with our DRLRB the formation can reorganize in time to realize the dynamic transformation of the formation.

SECTION IV.Simulation
A. Simulation Parameters
By employing our DRLRB, a series of simulations for multiple underactuated USV formation path following is performed. Suppose action inputs of our DRLRB algorithm are parameterized as a vector . The sampling period is 60 s, and the control period is 1 s. Details of parameters are given in Table I.

TABLE I Details of Parameters in Simulation

is the discount factor, it is used to regulate short and long-term effects. is the capacity of the experience pool. is the batch size, and are update delay of actor-network and critic network respectively, they are used to deferred update the target networks.

The preset paths in the form of polygon can be
View Source

And the preset paths in the form of curve can be
View Source

B. Results Analysis
To validate the effectiveness of our DRLRB algorithm, this section elucidates simulation results for USV formation from four aspects, including formation process, formation path following with polygonal and curvilinear paths, and formation transformation.

Fig. 8 presents the formation progress with five USVs. The USVs start from different starting positions and directions, and follow their virtual leader to form a formation in Fig. 8(a). The time-varying speeds and courses of five USVs are given in Fig. 8(b) and (c), respectively. It can be observed that, almost after 45 s, a formation has been reached, on condition that all those USVs learn to slow down and their speeds reduce to 0 with the aid of our DRLRB. Furthermore, in Fig. 8(d), it shows that the formation errors of all USV gradually decrease, and finally reach 0 to form a formation. This case verifies that the formation with a number of USVs can be reached successfully by using our DRLRB.


Fig. 8.
Progress of formation with five USVs. (a) Following trajectories of formation with polygonal paths. (b) Speeds variation of formation. (c) Courses variation of formation. (d) Formation error variation curves of formation.

Show All

In the polygonal path-following simulation, the USVs keep the formation and follow the preset path after the formation of three USVs is formed. Fig. 9 shows the results with a random braking mechanism in the training, and Fig. 10 shows that without random braking mechanism. Figs. 9(a) and 10(a) show the trajectories of all USVs in formation, and the formation remains intact when the path is traced to the end. However, it can be seen from Fig. 10(a) that when there is no random braking mechanism during training, USV3 has not learned to slow down, resulting in a turning left near . Figs. 9(b) and 10(b) show the preset path, the trajectory of the virtual leader and the trajectory of the center of formation, from which it can be seen that the three maintain perfect consistency. Figs. 9(c) and (d) and 10(c) and (d) show the speed and course angle curves of the USVs. The comparison of the algorithm before [Fig. 10(c)] and after [Fig. 9(c)] adding brake mechanisms are reflected on the variations of the speed and course. As shown in Fig. 10(c), before adding brake mechanism, there exist frequent fluctuations of the speed in 0–100 s and 300–500 s. The USVs do not have the braking capacity, which result in the USVs can only navigate at a low speed of about 1.6 m/s. As for Fig. 9(c), after adding brake mechanism, the speed curves are smooth and regular. Benefit from the adding of brake mechanism, USVs can navigate at a high speed about 4 m/s. From Figs. 9(c) and 10(c), it is obvious that the speeds of USVs with random braking mechanism has regular changes, but irregular change for the speeds of USVs without random braking mechanism. Meanwhile, benefit by the random braking mechanism, the speed in Fig. 9(c) can be increased to be larger than that in Fig. 10(c), under the premise of ensuring the path-following effect. The course curves illustrate that USVs navigate more stable after adding brake mechanism. As shown in Fig. 10(d), the courses vibrate violently in 0–100 s and 300–450 s. As for Fig. 9(d), the courses change smoothly and regularly in 0–100 s and 170–270 s. This indicates that USVs navigate more stably and safely with our DRLRB. As shown in Figs. 9 and 10, to maintain the formation, the speeds and course angle of USVs are adjusted dynamically and steady, which ensures the navigation safety of USVs. It is proved that the multi-USVs formation path-following method designed in this article has a significant effect on the polygonal path.


Fig. 9.
Polygonal path following of formation with three USVs with brake. (a) Following trajectories of formation with polygonal paths. (b) Following trajectories of the center of formation. (c) Speeds variation of formation following with polygonal paths. (d) Courses variation of formation following with polygonal paths.

Show All


Fig. 10.
Polygonal path following of formation with three USVs without brake. (a) Following trajectories of formation with polygonal paths. (b) Following trajectories of the center of formation. (c) Speeds variation of formation following with polygonal paths. (d) Courses variation of formation following with polygonal paths.

Show All

For the sake of enhancing the persuasion of the USV formation path-following method, a more complex simulation experiment of stepped curvilinear path following is designed, the results are shown in Figs. 11 and 12. Comparing Fig. 11(a) with Fig. 12(a), it can be found that the trajectories of USVs with random brake mechanism are smoother and that without random brake mechanism are rugged. Figs. 11(b) and 12(b) show the preset path, the trajectory of virtual leader and the trajectory of the center of formation, from which it can be seen that the three maintain excellent consistency. Figs. 11(c) and 12(c) describe the speed curves of the USVs. The speeds of USVs with random braking mechanism has regular changes, but irregular change for the speeds of USVs without random braking mechanism. Meanwhile, benefit by the random braking mechanism, the speed in Fig. 11(c) can be increased to be larger than that in Fig. 12(c), under the premise of ensuring the path-following effect. Figs. 11(d) and 12(d) show that the course angle curves of the USVs are adjusted dynamically and steady, which ensures the navigation safety of USVs.


Fig. 11.
Stepped curvilinear path following of formation with three USVs with brake. (a) Following trajectories of formation with stepped curvilinear paths. (b) Following trajectories of the center of formation. (c) Speeds variation of formation following with stepped curvilinear paths. (d) Courses variation of formation following with stepped curvilinear paths.

Show All


Fig. 12.
Stepped curvilinear path following of formation with three USVs without brake. (a) Following trajectories of formation with stepped curvilinear paths. (b) Following trajectories of the center of formation. (c) Speeds variation of formation following with stepped curvilinear paths. (d) Courses variation of formation following with stepped curvilinear paths.

Show All

In addition, the multi-USV formation path-following method designed in this article can dynamically adjust the formation. As shown in Fig. 13, this article designed the simulation of transforming the formation of five USVs to three USVs. Fig. 13(a) shows the trajectories of five USVs, five USVs keep the formation until around , USV4 and USV5 exit the formation, the other three USVs regroup and continue to complete the path-following task. Fig. 13(b) shows the preset path, the trajectory of virtual leader, and the trajectory of the center of formation, from which it can be seen that the three maintain perfect consistency. It can be seen that only when the formation changes near , the center of formation breaks away from the preset path. After the formation transformation is completed, the center of formation returns to the preset path. It is proved that the dynamic formation transformation function of the multi-USV formation path-following method designed in this article can ensure the path-following effect when the USV formation is abnormal. The formation transformation is easy to implement by our DRLRB. When the USV formation navigates by narrow waters, a linear formation is adaptable to pass by narrow waters.


Fig. 13.
Curvilinear path following of USV formation from five USVs to three USVs. (a) Following trajectories of formation transform of USVs. (b) Following trajectories of the center of formation.

Show All

Fig. 14 shows the cross-track errors in the simulations of polygonal path following, curvilinear path following and formation transformation. In the process of polygonal path following, it can be seen that the cross-track error is always small, and only fluctuates in the middle of 200–250 s. The reason is that when USV formation sails near the turning point of the polygonal path, the navigation direction of the virtual leader has changed, and USV regroups. In the process of curvilinear path following, the cross-track error tends to 0. Near 180 s, the curve fluctuates due to the larger curvature of the path. In the simulation experiment of the formation transformation, the cross-track error is always kept in the range of 0–3 m. During 300–350 s, due to the formation transformation, the cross-track error gets larger.


Fig. 14.
Cross-track errors of polygonal, curvilinear path following and stepped curvilinear path following.

Show All

Fig. 15 shows the cross-track errors in the simulations of polygonal path following, curvilinear path following, and stepped curvilinear path following without random brake mechanism. Compared with Fig. 14, the cross-track error of path following of USVs trained with random brake mechanism is smoother and more stable. This proves that the multi-USV formation path-following method with a random brake mechanism designed in this article has an excellent effect.


Fig. 15.
Cross-track errors of polygonal, curvilinear path following and stepped curvilinear path following without brake.

Show All

SECTION V.Conclusion
To resolve the USV formation path-following problem, this study proposed an algorithm named DRLRB. Referring to the DRLRB model, a virtual leader-follower formation strategy is integrated to achieve the path following of USV formation. Based on the DDPG framework, a reward function of formation control is smartly designed to realize the rapid formatting. A random brake mechanism is incorporated to train USV to slow down and stop. Simulation validates the effectiveness of our DRLRB model. By using this algorithm, even under the adverse condition that some USVs failed in formation, the fast formation transformation can be fulfilled, and then the USV formation path-following issue has been solved successfully.

In the future, it is significant to develop the path-following application for the USV formation by employing our DRLRB control model.