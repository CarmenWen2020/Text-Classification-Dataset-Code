Emotions have an important role in daily life, not only in human interaction, but also in decision-making processes, and in the perception of the world around us. Due to the recent interest shown by the research community in establishing emotional interactions between humans and computers, the identification of the emotional state of the former became a need. This can be achieved through multiple measures, such as subjective self-reports, autonomic and neurophysiological measurements. In the last years, Electroencephalography (EEG) received considerable attention from researchers, since it can provide a simple, cheap, portable, and ease-to-use solution for identifying emotions. In this paper, we present a survey of the neurophysiological research performed from 2009 to 2016, providing a comprehensive overview of the existing works in emotion recognition using EEG signals. We focus our analysis in the main aspects involved in the recognition process (e.g., subjects, features extracted, classifiers), and compare the works per them. From this analysis, we propose a set of good practice recommendations that researchers must follow to achieve reproducible, replicable, well-validated and high-quality results. We intend this survey to be useful for the research community working on emotion recognition through EEG signals, and in particular for those entering this field of research, since it offers a structured starting point.
SECTION 1Introduction
Emotions are fundamental in the daily life of human beings as they play an important role in human cognition, namely in rational decision-making, perception, human interaction, and human intelligence [1]. However, emotions have been largely ignored, in particular in the field of Human-Computer Interaction (HCI).

Affective Computing has emerged to fulfill this gap by converging technology and emotions into HCI. It aims to model emotional interactions between a human and a computer by measuring the emotional state of a user [2]. A person's inner emotional state may become apparent by subjective experiences (how the person feels), internal/inward expressions (physiological signals), and external/outward expressions (audio/visual signals) [3]. Subjective self-reports about how the person is feeling can provide valuable information but there are issues with validity and corroboration [4]. Participants may not answer exactly how they are feeling but rather as they feel others would answer.

Physiological signals can assist in obtaining a better understanding of the participants’ underlying responses expressed at the time of the observations. These correspond to multichannel recordings from both the central and the autonomic nervous systems.

The central nervous system comprises the brain and spinal cord, while the autonomic nervous system is a control system that acts unconsciously and regulates bodily functions such as the heart rate, pupillary response, and sexual arousal. The signals commonly used to measure emotions are the Galvanic Skin Response (GSR), which increases linearly with a person's level of arousal; Electromyography (EMG) (frequency of muscle tension), which is correlated with negatively valenced emotions; Heart Rate (HR), which increases with negatively valenced emotions such as fear; and Respiration Rate (RR) (how deep and fast the breath is), which becomes irregular with more aroused emotions like anger. Measurements recorded over the brain also enable the observation of the emotions felt [3].

Functional neuroimaging techniques such as Electroencephalography (EEG), functional Magnetic Resonance Imaging (fMRI), or Positron Emission Tomography (PET) can be used. Although EEG has a poor spatial resolution and requires many electrodes placed at various sites on the head, it provides great time resolution, allowing researchers to study phase changes in response to emotional stimuli. Furthermore, the use of EEG is noninvasive, fast, and inexpensive, making it a preferred method in studying the brain's responses to emotional stimuli [5]. Nowadays, due to their wearability, price, portability and ease-of-use, new wireless EEG devices are coming to the market. Thus, it is now possible to use EEG-based emotion recognition in different areas such as entertainment, e-learning, virtual worlds, or e-healthcare applications [6], [7]. It may be used for many purposes, such as instant messaging, online games, assisting therapists and psychologists while doing their job.

In this paper, we review works that present approaches for recognizing emotions based on EEG signals. Our analysis was done upon two different perspectives: one more general, concerning a set of recommendations to avoid common pitfalls that tend to be performed in this area of research; and another more specific covering the different steps of the process of recognizing emotions from EEG signals. The latter focuses on the number and gender of the participants, set of emotions recognized, the stimuli used to elicit them (images, videos, etc.), EEG device used and location of the electrodes, EEG features extracted and the methods used to extract those features, and finally the classifiers used.

SECTION 2Methodology
We performed the following queries on Google Scholar,1 Pubmed,2 and IEEE Xplore3 websites to collect the papers for the survey: EEG+Emotions+Recognition and EEG+Emotions+Identification. Then, we carefully identified those published between 2009 and 2016 belonging to the EEG-based emotion recognition group. We also identified similar works cited by these, but in general they were already retrieved by our initial queries. This first selection resulted in 155 papers, which we grouped by author and then removed those that were incremental contributions. This resulted in a new list of 142 papers.

In the next step, we analyzed the quality of the papers, by considering the number of citations. For each year, we chose the papers whose number of citations was bigger than the median of citations for that year. Only eighty-eight (of the 142) complied with this quality metric. Note that for the year 2016, since the median was zero we kept all the papers. Given that a small number of citations may not be enough to consider a paper as not good, we analyzed the content and novelty of each of the papers below the threshold. The final list of papers was composed of 99 papers.

These papers were further analyzed according to two perspectives. First, we reviewed all the papers according to the six recommendations (with 14 key points) defined by Brouwer et al. [8]. Second, we performed a more specific analysis on a subset of the 99 papers. This subset contains the works that complied with at least 9 of the 14 key points.

SECTION 3Background
In the following paragraphs, we shortly introduce the definition and representation of emotions, as well as the main characteristics of the EEG signals, to give some context to the reader.

3.1 Emotions
An emotion is a complex psychological state that involves three distinct components: a subjective experience, a physiological response, and a behavioral or expressive response [9], [10]. Emotions have been described as discrete and consistent responses to events (external or internal) with significance for the organism [11]. They are brief in duration and correspond to a coordinated set of responses, which may include verbal, behavioral, physiological and neural mechanisms. In affective neuroscience, the emotion concept can be differentiated from similar constructs like feelings, moods and affects. Feelings can be viewed as a subjective representation of emotions. Moods are diffuse affective states that generally last for much longer durations than emotions and are also usually less intense than emotions. Finally, affect is an encompassing term, used to describe the topics of emotions, feelings, and moods all-together.

There are two different perspectives towards emotion representation. The first one (categorial) indicates that basic emotions have evolved through natural selection. Plutchik proposed eight basic emotions: anger, fear, sadness, disgust, surprise, curiosity, acceptance, and joy [12]. All the other emotions can be formed by these basic ones (e.g., disappointment is composed of surprise and sadness). Ekman, following a Darwinian tradition, based his work in the relationship between facial expressions and emotions derived from a number of universal basic emotions: anger, disgust, fear, happiness, sadness, and surprise [13]. In the second perspective (dimensional), based on cognition, the emotions are mapped into the Valence, Arousal, and Dominance (VAD) dimensions. Valence goes from very positive feelings to very negative (or unpleasure to pleasure); arousal (also called activation) goes from states like sleepy to excited; and finally, dominance correspond to the strength of the emotion [13], [14]. The most common model used is the Circumplex Model of Affect, which only uses valence and arousal [15].

3.2 Electroencephalography (EEG)
The largest portion of the human brain, the cortex, is divided into the frontal, temporal, parietal, and occipital lobes (See Fig. 1) [16]. The frontal lobe is responsible for the conscious thought. The temporal lobe is responsible for the senses of smell and sound, and the processing of complex stimuli such as faces and scenes. The parietal lobe is responsible for integrating sensory information from various senses, as well as the manipulation of objects. Finally, the occipital lobe is responsible for the sense of sight.


Fig. 1.
The cortex subdivided into the frontal, temporal, parietal, and occipital lobes. Adapted from [17] (best seen in color).

Show All

EEG is a medical imaging technique that reads scalp electrical activity generated by brain structures, i.e., it measures voltage fluctuations resulting from ionic current flows within the neurons of the brain. A typical adult EEG signal, when measured from the scalp, is about 10-100 μV [18]. These signals observed in the scalp are divided into specific ranges that are more prominent in certain states of mind, namely the delta (1-4 Hz), theta (4-7 Hz), alpha (8-13 Hz), beta (13-30 Hz), and gamma (>30 Hz) bands [19] (see Fig. 2). The beginning and the end of the bands varies a few Hertz among different authors.

Fig. 2. - 
The five brain waves: Delta, theta, alpha, beta, and gamma.
Fig. 2.
The five brain waves: Delta, theta, alpha, beta, and gamma.

Show All

Delta waves are associated with the unconscious mind, and occur during a deep dreamless sleep. Theta brain waves are associated with the subconscious mind, for instance with activities such as sleeping and dreaming. Alpha waves are typically associated to a relaxed mental state, yet aware, and are more visible over the parietal and occipital lobes. High alpha activity has been correlated to brain inactivation. Beta waves are related to an active state of mind, more prominent in the frontal cortex and over other areas during intense focused mental activity. Finally, gamma waves are associated with an hyper brain activity [20].

In the following paragraphs, we present both the electrodes positioning to gather the EEG signals and the paradigms used to evaluate them.

3.2.1 EEG Electrodes Location
In order to produce replicable setups, there are standardized sets of locations for electrodes on the skull, such as the International 10/20 System (IS) (see Fig. 3) [21]. This system is based on the relationship between the location of an electrode and the underlying area of the cerebral cortex. The numbers 10 and 20 indicate the distance between adjacent electrodes (10 or 20 percent of the total front-back or right-left distance of the skull). Extra positions can be added by the utilization of the existing empty spaces.

Fig. 3. - 
The International 10/20 system (best seen in color).
Fig. 3.
The International 10/20 system (best seen in color).

Show All

Each site has a letter to identify the lobe and a number to identify the hemisphere location. F stands for Frontal, T for Temporal, C for Central (although there is no central lobe, C letter is used for identification purposes), P for Parietal, and O for Occipital. z (zero) refer to an electrode placed on the mid line. Even numbers refer to electrode positions on the right hemisphere, while odd numbers refers to the left one. Four anatomical landmarks are used for the correct positioning of the electrodes: nasion (the point between the forehead and nose), inion (the lowest point of the skull from the back of the head, indicated by a prominent bump), and the pre auricular points anterior to the ear.

Electrodes can be monopolar or bipolar. The first record the potential difference, compared to a neutral electrode connected to an ear lobe or mastoid. The second shows the potential difference between two paired electrodes. With the use of high-density electrodes, multiple sources of noise that can disrupt EEG recordings arise, such as muscle activity near the active sites, eye movements and blinks. Eye movement artifacts can have profound effects on frontal brain sites, specifically mid-frontal sites (F3 & F4), commonly used in studying emotional reactivity [5], [20], [22].

3.2.2 EEG Paradigms
In order to understand how the changes that occur in the electrical brain activity can be evaluated, we present the paradigms most commonly used: Sensory Evoked Potentials (SEP) [23], Event-Related Potentials (ERP) [24], and Event-Related De/Synchronizations (ERD/ERS) [25].

An evoked potential corresponds to an electrical potential signal recorded after the presentation of a stimulus. There are three types: Auditory Evoked Potentials (AEP), Visual Evoked Potentials (VEP), and the Somatosensory Evoked Potentials (SsEP), that differ by the elicitation method used [26]. AEP are elicited by a click or tone stimulus presented through earphones, VEP by a flashing light or changing pattern on a monitor (Steady State Visually Evoked Potential (SSVEP) if it is elicited by a periodic stimulus [27], [28]), and SsEP by electrical stimulation of the peripheral nerve.

ERP have a very high temporal resolution that allows the measurement of immediate responses to short stimuli. They are usually measured as latencies and amplitudes of positive and negative potentials at specific millisecond intervals following a stimulus. The ERP components can be encapsulated in the following order: P100, N100, N200, P200, P300, and Slow Cortical Potential (SCP). N100 is characterized by a negative deflection in voltage with a delay between stimulus and response (latency) of 100 ms after the stimulus, while P100 is the equivalent but with a positive deflection. N200 and P200 are analogous to N100 and P100, with a latency of about 200 ms instead of 100 ms (varying between 150 and 275 ms). P300 is thought to reflect processes involved in stimulus evaluation or categorization, and it is characterized by a positive deflection in voltage with a latency of roughly 250 to 500 ms. SCP can occur from 300 ms to over several seconds.

ERD/ERS analysis allows for the evaluation of power changes within specified frequency bands with a high temporal resolution. They measure rapid changes of power within defined frequency band ranges in order to assess responses that occur within milliseconds of a stimulus presentation. The increased power within a frequency band after the presentation of a stimulus is defined as an ERS, while ERD corresponds to the decrease of the power within a frequency band [25]. It is appropriate for measuring the existing reactions to affective communications as they occur.

3.3 Emotions in the Brain
In the last decade, a high number of neuropsychological studies have reported correlations between EEG signals and emotions. There are two main areas of the brain correlated with emotional activity: the amygdala (located close to the hippocampus, in the frontal portion of the temporal lobe); and the pre-frontal cortex (covers part of the frontal lobe). Although there is no consensus about a possible lateralization of the amygdala, its activation seems to be more related to negative emotions than positive ones [29].

Changes in alpha power and asymmetry between the hemispheres of the brain are related to emotions. A relative right frontal activation is associated with withdrawal stimuli or negative emotions, such as fear or disgust. A relatively greater left frontal activation is associated with an approach stimuli or positive emotions, such as joy or happiness. Thus, the asymmetrical frontal EEG activity may reflect changes on the valence [29], [30], [31], [32]. Beta bands are also related to valence [32]. Pre-frontal and parietal asymmetry in the alpha band and temporal asymmetry in gamma band are present for valence recognition, while pre-frontal asymmetry in alpha band and temporal asymmetry in the gamma band are observable for arousal recognition [33].

Changes in the gamma band are related with the emotions happiness and sadness, and so is the decrease in the alpha wave in different sides of the temporal lobe (left for sadness, and right for happiness) [34], [35]. Finally, the ERP components of short (N100 and P100) to middle (N200 and P200) latencies have been shown to correlate with valence, whereas the components of middle to long (P300 and SCP) latencies have been shown to correlate with arousal [36].

Previous studies have suggested that men and women process emotional stimuli differently. They suggest that men rely on the recall of past emotional experiences to evaluate current emotional experiences, whereas women seemed to engage the emotional system more readily [37]. There is also some evidence that women share more similar EEG patterns among them when emotions are evoked, while men have more individual differences among their EEG patterns [38].

In summary, we can conclude that the frontal and parietal lobes are the most informative about the emotional states, while the alpha, gamma and beta waves appear to be the most discriminative. The gender-related findings are consistent with the common belief that women are more emotional than men, which suggests possible gender-related neural responses to emotional stimuli.

SECTION 4Brouwer's Recommendations
The recognition of emotions through neurophysiological signals such as the EEG, as well as the creation of applications that exploit this information, requires knowledge from different areas. For example, a researcher needs expertise in engineering, experimental design, knowledge of the targeted user group, mathematical modeling, psychophysiology, sensor technology, signal processing, and systems design. Therefore, this is a highly interdisciplinary field that is difficult to perform, but also to analyze (both by experts and readers). In fact, the common pitfalls enumerated in this section mainly occur in interdisciplinary regions that link experimental psychology, human factors, machine learning, and neurophysiology (see Fig. 4). Experimental psychology provides methods to assess mental states. Human factors are needed to create and test applications. Machine learning provides advanced classification algorithms. Neurophysiology offer the knowledge about the functioning of the nervous system and how it can be measured.

Fig. 4. - 
Overview of five of the six recommendations in relation to their major underlying fields. Recommendation 3 is interweaved with all of the other recommendations [8] (best seen in color).
Fig. 4.
Overview of five of the six recommendations in relation to their major underlying fields. Recommendation 3 is interweaved with all of the other recommendations [8] (best seen in color).

Show All

Brouwer et al. presented six recommendations (see Table 1) to avoid the common pitfalls that are related to the use of neurophysiological signals that reflect cognitive or affective states [8]. These recommendations are related with the definition of the state of interest, neurophysiological processes expected to be involved in the state of interest, confounding factors, “cheating” on the results through classification analysis (although not on purpose), insight on what underlies successful state estimation, and finally, the added value of neurophysiological measures in the context of an application. They may help to improve the design and execution of new studies, and could work as a checklist for reading and evaluating studies. Following, we present our analysis of the works carried out in the field, since 2009 until 2016, according to these recommendations (see Table 2). We also describe in detail each of the recommendations and how we consider that each work comply (or not) with a given key point.

TABLE 1 The Recommendations, Proposed by [8], to Avoid Common Pitfalls While Using Neurophysiological Signals that Reflect Cognitive or Affective States

TABLE 2 Analysis of the Works in Accordance with the Six Recommendations (and Key Points)



4.1 R1 - Define State of Interest and Ground Truth
A given concept may have multiple interpretations among the community (e.g., there are plenty of different sets of emotions, although all of them are under the umbrella of the emotion concept). To prevent confusions, it is important to clarify which are the mental states addressed by the authors, as well as discuss how it was addressed in previous studies and the definition in use. It is also very important to connect the mental state of interest to its operationalization in the work, since it reflects what should be consider as ground truth (e.g., behavioral measures such as button press accuracy, subjective measures such as responses on known scales like Self Assessment Manikin (SAM) [131], or knowledge about the condition that individuals are currently in).

As we can see in Table 2, about 74 percent of the works met the first recommendation, i.e., they satisfy both key points of this recommendation. Considering key point 1.1, the works usually present the problem they intend to solve (recognition of emotions) and how they will get the ground truth data: collect the emotional ratings from the users or use already known standardized datasets (97 percent). However, a smaller number of works comply with key point 1.2 (73.7 percent). Although it is common the authors collect both the EEG signals and emotional evaluation of what the subject felt during the stimulus’ exposure, some of the works only collect the signal data, assuming that the stimulus effectively elicited the emotions expected. However, the emotion could not be successfully elicited, meaning that this assumption may affect the quality of the recognizers, leading authors to present incorrect or inadequate conclusions.

4.2 R2 - Connect State of Interest to Neurophysiology
One key aspect when trying to estimate affective (or cognitive) states based on neurophysiological signals is to connect a given psychological state to certain physiological signals (in our particular case, EEG signals). Thus, findings in the literature should be used to formulate hypotheses about the way the neurophysiological measures used are expected to vary (and how) with the mental state of interest. With this, researchers are able to identify useful variables/features for the training step of the mental state estimation classification model, as well as to validate if the mental state estimation model is functioning as expected.

Recommendation 2 addresses these aspects, and according to our analysis only 34.3 percent of the works comply with it. Researchers tend to present only the methods they used to extract the EEG features, as well as the features themselves, without providing any explanation of the relationship between the emotions they intend to recognize and the features they used.

4.3 R3 - Eliminate Confounding Factors
Confounding factors are particularly important, since they can affect the neurophysiological study. In the particular case of EEG, involuntary movements made by the subject may cause artificial artifacts in the collected data. The best way to avoid them is by properly design the study. However, it is difficult to totally eliminate the existence of confounds. In these situations, where the confounds cannot be avoided, we should examine the data to verify their existence and, more importantly, to check if the neurophysiological variables vary with the mental state of interest or due to confounds.

Most of the works (87.9 percent) attempt to use the proper design of the study to avoid confounding factors (key point 3.1). For example, habituation time is provided to subjects to get them used to the device, as well as a relaxed environment with ideal conditions of temperature, light, and comfort. Less common is the verification of the data to find confounds and remove them if they exist (key points 3.2 and 3.3 - 67.7 percent each). One potential reason for this is the fact that researchers working with EEG signals apply artifact removal techniques. Hence, authors believe there is no need to observe the data and to manually remove them. This reason may also justify the very small number of works that comply with key point 3.4 (5.1 percent). For further information about EEG artifact removal, please see [132].

4.4 R4 - Adhere to Good Classification Practice
Classification analysis is used to estimate mental states, especially with high dimensional signals (such as the EEG). Usually, supervised classification models are trained with samples from data collected and labeled according to the state of interest. Following, the trained models are used to label unseen neurophysiological data. Then, by comparing the labels from the known and unseen data, the performance of the classifier can be determined. To ensure that the classification accuracy is not inflated, the pre-processing and parameter settings should be carefully chosen, and be independent of the test set.

Around 49 percent of the works fulfill all the key points from recommendation four. Key points 4.2 and 4.3 are fulfilled for almost all the works (more than 96 percent), while key point 4.1 is complied only in 49.5 percent of the reviewed works. This is mainly due to the fact that some authors do not provide any information about this, or use data from the same session/subject for training and testing. Given the dependency between the data collected for training and test, it is not guaranteed that the results obtained are not due to the dependency relationship: overly optimistic results may arise. In the case of data from the same subject, authors tend to not generalize the findings gathered.

4.5 R5 - Insight into the Cause of Classification Success
Classification performance provides insight about how well a trained model can estimate the mental state of interest of unseen neurophysiological data. Besides presenting the classification results, it is also important to present information about the way the neurophysiological processes underlying the different features (and combinations of features) differ.

It is common that authors extract various features from EEG signals, and then train classifiers with those features, or combinations of them (key point 5.2-68.7 percent). However, they only report the results achieved, without any explanation or insights about the results or why some sets of features perform better than others (key point 5.1-32.3 percent).

4.6 R6 - Added Value of Using Neurophysiology
Only part of the works explained the advantages of the EEG signals over other physiological measures that can also be used to capture the emotions felt by a person (key point 6.1-61.6 percent). A larger number of works explain the type of applications that will benefit from this kind of recognizers, and what added value they can bring to those applications (key point 6.2-77.8 percent).

4.7 Discussion
In summary, part of the recommendations have already been adopted in the revised works. Authors present the state of interest, and the expected gains that the recognition of emotions through physiological data can bring to the scientific community, as well as the general public who will benefit from its application. They also present the classification methods used, and explain how they used the data collected both for training and for test. The selection of pre-processing and classification techniques appear to be independent of the validation process.

Future works should provide more information about how the EEG signals (and the features used in classification) vary depending on the state of interest, since it may affect the presentation of the results. The authors should present the advantages of using EEG signals (and the devices selected) over other physiological measures more often, as well as make an effort to minimize the existence of confounding factors. The use of techniques for artifact removal should not replace the validation of the signals being collected. More information about the different parameters used for the classification methods should be provided to increase the reproducibility and replicability of the works, as well as to increase comparison among different works.

SECTION 5Emotion Recognition from EEG
Over the last years, emotion recognition from EEG signals has received much interest. To recognize emotions using EEG signals we need to perform the following steps (see Fig. 5): i) the user must be exposed to the stimulus being tested; ii) the voltage changes observed in the brain of the user are recorded; iii) the noise and artifacts from the recorded signals are removed; iv) the resulting data is analyzed and the relevant features are extracted; v) a classifier is trained based on a training set and using the computed features, leading to the interpretation of the original raw brain signals [20].


Fig. 5.
Process of emotions recognition using EEG. Adapted from [20].

Show All

We performed the comparisons among the 63 works that satisfy 9 of the 14 key points according to the following criteria: subjects, stimuli (and duration of the stimulus), emotions to be elicited, EEG equipment (with the sampling frequency), electrodes location, artifact filtering, EEG features extracted, the methods for the feature extraction, classifiers used, offline versus online training/testing, user-dependent or user-independent data, and finally, the accuracies achieved.

5.1 Test Protocol
In the following paragraphs, we present the analysis performed considering the type of stimulus used and the correspondent duration, the number of subjects, their gender, and finally, the emotions to be recognized (see Table 3).

TABLE 3 Analysis of the Works Considering the Test Protocol Phase
Table 3- 
Analysis of the Works Considering the Test Protocol Phase
5.1.1 Subjects
The number of subjects used in each of the works varies considerably, from 1 to 161 subjects, with a median of only 15 subjects. When the number of participants is this small, it is difficult to verify the accuracy and the meaningfulness of the data and results presented. It is evident that the majority of the works do not use a statistically significant number of participants to provide a good level of experimental reliability and validity, with 47 percent of the works reviewed using less than 15 subjects each, and only about 27 percent using at least 30 subjects.

Regarding the gender of the participants, in 24 percent of the works it was omitted. Since men and women might perceive emotional stimuli in different ways, it is important that the number of subjects from each gender is balanced. Only 23 percent of the works satisfy this. A minority of the works focus in only one of the genders: none used only female subjects, while 7 percent uses only male subjects. The remaining works, mainly used an unbalanced number of subjects, with more men in the sample than women (68 percent).

5.1.2 Stimulus
There are two approaches for emotion elicitation: subject- and event-elicited. In the first one, emotions can be generated by asking the participants to remember past emotional episodes of their life or act as if they were feeling a given emotion. In the second, it is possible to use different modalities including the visual, auditory, tactile, or odor stimulation. These emotional stimuli are usually selected to cover the desired arousal levels and valence states (or the basic emotions). Emotion elicitation is influenced by the complexity and number of targeted emotions [29], [36].

The ground truth of the emotional state induced by a stimulus is secured by exploiting the self-ratings of subjects or using standard stimulus sets such as the International Affective Picture System (IAPS) [133] and Geneva Affective PicturE Database (GAPED) [134] for images, and International Affective Digitized Sound System (IADS) [135] for sound. The duration of an affective phenomenon can be used to define time categories that range from “full blown emotions” (lasting for some seconds or minutes) to traits, lasting for years if not a lifetime.

Almost 26 percent of the works used images as the stimuli. The majority of them (56.3 percent) used images from the IAPS, 12.5 percent from Pictures of Facial Affect (POFA), 6.25 percent from GAPED, 6.25 percent from the Ekman's Picture Set, and another 6.25 percent from the Chinese Affective Picture System (CAPS) [136]. The remaining do not provide information about the source of the images. The average duration of the stimulus presentation was 11.97 seconds, varying between 1.5 and 48 seconds.

In the case of the 23.8 percent of the works that used video as the stimuli, the majority do not provide information about the source of the videos (93.33 percent), while the remaining used the Stanford emotional clips from Stanford. Regarding the duration of the stimulus, in 40 percent of the works there was no fixed time for each video (ranged from 0.5 seconds to 5 minutes). The works that provide information about the duration had an average duration of 171.6 seconds, with 30 seconds being the minimum and 288 the maximum duration used.

There were 17.5 percent of the works that used music as stimuli, with 18 percent using the IADS, and the remaining do not provide information about the source (82 percent). The average duration was 57.1 seconds, varying from 15 to 180 seconds.

A considerable part of the works used existing datasets that provide both physiological data and emotional evaluations made by users following exposure to stimulus (22.2 percent). The majority used the dataset for emotion analysis using EEG, physiological and video signals (DEAP) [137], and the remaining the Mahnob HCI dataset [70].

The remaining works used subjects own memories (duration not reported), tetris game (5 minutes), performing movements (8 minutes), odors (8 seconds), live performing (duration not available), IAPS and music videos (60 seconds), music videos (1 to 2 minutes), and finally GAPED with music (2 minutes).

5.1.3 Emotions
Around 46 percent of the works try to identify basic emotions, with the most common emotions being sad/sadness (62.1 percent), happy/happiness (48.3 percent), anger/angry (44.8 percent), fear (44.8 percent), joy/joyful (27.6 percent), surprise (27.6 percent), disgust (24.1 percent), pleasant (20.1 percent), and neutral (13.8 percent).

Valence and arousal were identified in about 30 percent of the works, with three of them also identifying control or dominance dimensions. Other emotional states were identified in the remaining works, such as positive and negative (29.4 percent), positive, negative, and neutral (17.6 percent), calm-neutral and negatively excited (11.8 percent), calm, positively excited and negatively excited (11.8 percent), and like/dislike (11.8 percent). Note that multiple works started with a large set of emotions, but due to the poor results achieved, they ended up reducing to one or two emotions only.

5.2 EEG Recordings
The number of electrodes used (and the equipment) assumes a leading role due to the time needed to set up the EEG device, the comfort level of the users who wear the device, and the amount of features to process. For these reasons, ideally, the number of electrodes should be reduced. However, as we will present in the following paragraphs, most of the current works still require a relatively big number of electrodes, and expensive clinical devices (see Table 4).

TABLE 4 Analysis of the Works Considering the EEG Recording Phase
Table 4- 
Analysis of the Works Considering the EEG Recording Phase
5.2.1 Equipments
There were 17 different EEG equipments used in the reviewed works that provided this information. The majority were commercial and only one was developed by the authors of the work. The most used were the Biosemi Active Two4 (37.1 percent), Emotiv wireless headset5 (16.1 percent), EEG module from Neuroscan, Inc.6 (14.5 percent), and g.MOBIlab7 (4.8 percent). From these devices, the most portable and easy to use is the Emotiv wireless headset. One work does not provide information about the device used, and another one indicated the device used but does not specify the sampling rate used.

For the remaining, the most used sampling frequencies were 512 Hz (21.3 percent), 256 Hz (19.7 percent), and 500 Hz (13.1 percent). Considering the most used devices, Biosemi Active Two was used to collect the EEG signals with sampling frequencies of 512 Hz (56.5 percent), 256 Hz (17.4 percent), 1024 Hz (17.4 percent), and 2048 Hz (8.7 percent); Emotiv with 128 Hz (56.6 percent), and 2048 Hz (44.4 percent); g.MOBIlab was always used with a sampling frequency of 256 Hz; and finally the EEG module from Neuroscan, Inc was used with a sampling frequency of 500 Hz.

5.2.2 Electrodes
The majority of the works provides information about both the electrodes used and their positioning. However, 11.1 percent do not provide any information at all regarding the positioning, while only 3.17 percent do not provide the number of electrodes used to collect the EEG signals. In the case of the works that do not indicate information about the positioning of the electrodes, but indicate the number of electrodes, it varies from 14 to 64 electrodes, with an average of 52 electrodes. The 10-20 system (also known as IS) was applied in 32.14 percent of the works, with the minimum number of electrodes being 1, the maximum 64, and the average 41 electrodes. The 10-10 system was applied in 5.4 percent of the works, always with 64 electrodes. From all the works that indicate the number of electrodes used and the location of each electrode, the average was 14, ranging from 1 to 32.

Among all the works, 69 different electrodes covering the whole scalp were used (see Fig. 6). The FTC1, FTC2, TCP1, and TCP2 do not appear in the image presented but were used in the works reviewed (less than 3 percent each). In 2006, a modification to the 10/10 electrode positioning was introduced [138], [139]. The inconsistent T3/T4 and T5/T6 terms were replaced by the consistent T7/T8 and P7/P8. With this, almost all positions along the same sagittal line have the same post-scripted number and all with the same letter(s) are on the same coronal line.


Fig. 6.
Electrodes positioning for the 10-10 system. Adapted from [140]. The color information is based on the values we collected: Red indicates that an electrode was used in more that 75 percent of the works, orange between 50 and 75 percent, yellow between 25 and 50 percent, and green less than 25 percent (best seen in color).

Show All

The exceptions are the FP1/FP2 and O1/O2 positions. Since in the works both terminologies were used, we decided to keep the original ones in the tables, and sum up the occurrences of each pair of old and new terminology for evaluation purposes only. The most commonly used were F4 (82.9 percent), F3 (77.14 percent), T7 (65.7 percent), FP1 (65.7 percent), FP2 (60 percent), T8 (60 percent), F7 (60 percent), F8 (60 percent), O1 (54.3 percent), P7 (54.3 percent), P8 (51.4 percent), O2 (51.4 percent), FC5 (40 percent), FC6 (40 percent), C4 (40 percent), C3 (34.3 percent), AF3 (34.3 percent), AF4 (34.3 percent), P3 (28.6 percent), P4 (25.7 percent), and Pz(25.7 percent). AF stands for anterior frontal, C for central, F for frontal, FC for frontocentral, FP for frontopolar, FT for frontotemporal, O for occipital, P for parietal, T for temporal, and z for zero.

As we can see, the most used electrodes are the ones placed at the frontal lobe (considering the electrodes represented by the red and orange colors), which is in agreement with the findings that relate the emotions and this lobe.

5.3 Artifact Filtering
Although authors try to avoid artifacts in the EEG signals collected (such as eye blinks) by providing information to participants about their posture, they may still occur. In Table 5, we can see that 24 percent of the works manually removed some of the data due to different types of artifacts associated to the participant. In addition to the works that removed this information manually, methods such as Blind Source Separation (BSS) (19.3 percent) and Independent Component Analysis (ICA) (8.8 percent) were applied to remove eye movements, blinks, muscle, heart and line noise. Around 30 percent of the works re-referenced the electrodes using methods such as the Common Average Reference (CAR) (58.9 percent), Laplacian (23.6 percent), or Average Mean Reference (AMR) (5.9 percent).

TABLE 5 Analysis of the Works Considering the Artifact Filtering Phase

Since not all the frequencies collected are useful for the emotion recognition problem, approximately 84 percent of the works used some bandpass filters. Although 24 frequency ranges were used across all the works, the most commonly used were the 4-45 Hz (33.3 percent), 1-100 Hz (6.25 percent), 8-30 Hz (6.25 percent), 2-42 Hz (6.25 percent). The Notch filter was also applied in 16.58 percent of the works (mainly at 50 and 60 Hz). Finally, 43.9 percent of the works downsampled their original EEG signals: 52 percent to 128 Hz, 16 percent to 206 Hz, 12 percent to 256 Hz, 4 percent to 512 Hz, 4 percent to 500 Hz, 4 percent to 300 Hz, 4 percent to 250 Hz, and 4 percent to 32 Hz.

5.4 Feature Extraction
In the following paragraphs, we present the most common features extracted from the EEG signals, as well as the methods used to perform it (see Table 6).

TABLE 6 Analysis of the Works Considering the Feature Extraction Phase
Table 6- 
Analysis of the Works Considering the Feature Extraction Phase
5.4.1 EEG Features
Regarding the types of EEG features that authors used, around 10 percent of the works do not provide any information, while the remaining used mainly the delta, theta, alpha, beta, and gamma bands (89.4 percent). Almost 37 percent of these used all the bands together, while the remaining selected only some of them, such as alpha, beta, theta, and gamma (13.7 percent), alpha and beta (7.8 percent), alpha, beta, and gamma (7.8 percent), delta, theta, alpha, and beta (3.92 percent), alpha, beta, gamma (3.92 percent), among other combinations.

The remaining features used were the Event-Related De/Synchronizations (ERD/ERS), Event-Related Potentials (ERP), and fixed frequency bandwidths (e.g., 0.5-30 Hz, 1-10 Hz, 1-46 Hz, and 2-30 Hz).

5.4.2 Methods
The feature extraction process can be handled using various methods (for further information please see [36], [141]). In the works reviewed, there were 42 different methods used. More than 47.6 percent of the works used more than one method, although in the end only one was selected as the best one.

The most used methods were the Fourier Transform such as the Short-time Fourier Transform (STFT) or Discrete Fourier Transform (DFT) (25.4 percent), statistical (23.8 percent), Power Spectral Density (PSD) (22.2 percent), Wavelet Transform (WT) (19.1 percent), Entropy such as the Approximate Entropy (AE), Differential Entropy (DE), Sample Entropy (SE), or Wavelet Entropy (WE) (15.9 percent), Higher Order Crossings (HOC) (9.5 percent), Common Spatial Patterns (CSP) (7.9 percent), Fractal Dimensions (mainly the Higuchi Fractal Dimension (HFD)) (7.9 percent), and Asymmetry Index (AI) (4.8 percent).

5.5 Classification
In the field of recognition of emotions we have a large number of classifiers’ families that are commonly used: bayesian, support vector machines, decision trees, among others. In the following paragraphs, we present the most used classifiers, the type of classification (offline versus online), and the type of data used to train and test the classifiers (see Table 7). We remember that an emotion recognition system has a training phase that should use data that is different from the data used in the test phase. Due to the large number of differences between the works, it is complicated to make comparisons between them, hence infer conclusions about the quality of the results. Therefore, we will not discuss the accuracies achieved.

TABLE 7 Analysis of the Works Considering the Classification Phase
Table 7- 
Analysis of the Works Considering the Classification Phase
5.5.1 Classifiers
Since the majority of the works applied more than one classifier, and choose only one for the final configuration of the recognizer, we focus our analysis in the final one. Twenty-six different classifiers were selected as the best ones.

In almost 59 percent of the cases, Support Vector Machines (SVM) was used, with different kernels being applied: Radial Basis Function (RBF) (29.7 percent), linear (16.2 percent), polynomial (8.1 percent), gaussian (5.4 percent), and pearson (2.7 percent). Variations such as adaptive SVM, Multi-class Support Vector Machine (ML-SVM) or Least Squares Support Vector Machine (LS-SVM), were used in 8 percent of these works. Twenty-nine percent of the works that used Support Vector Machines (SVM) do not specify the kernel used. The k-Nearest Neighbors (kNN) was selected by almost 14 percent of the works; some works do not specify the value of k (44.4 percent), while in the others it varies from k = 2 to 8. Linear Discriminant Analysis (LDA) was used by 6.3 percent of the authors, while Quadratic Discriminant Analysis (QDA) was selected by 3.2 percent. Finally, the Naive Bayes (NB) and Multi-Layer Percepton Back Propagation (MLP-BP) were selected by 6.35 percent of the authors (3.17 percent each).

5.5.2 Offline versus Online
EEG signals are always changing its nature with time. This non-stationary nature of the signals can lead to classification models, built using specific physiological data, to not reflect the changes that have already occurred to the EEG signals. Most of the classification methods are based on the idea that the data comes from a stationary distribution [39]. Due to this, the classification accuracy is expected to degrade with time unless the model is adapted to reflect the changes occurring in the EEG signals. However, 90 percent of the works reviewed applied offline classification methods, with only 8 percent applying online classification (more suitable for real-time scenarios). One work applied both online and offline techniques.

5.5.3 User-Dependent / Independent
Another important aspect of the classification process is if the classifier was trained with user-dependent data or not. In the case of user-dependent data, a new model is generated for each user and the testing step is also done with this user data. Typically, better results are obtained, however at the cost of a lack of generalization. In the case of an user-independent model, the data of multiple users are used both for training and testing purposes. This leads to an easier applicability of the model to new users, since there is no need to create a new model. In the works reviewed, 46.8 percent of them use user-independent data and 43.5 percent user-dependent data. Around 8 percent used classifiers trained with models of both types. The rest of the works do not provide any information about their data being user-dependent or user-independent.

5.6 Discussion
Most of the works provide information about the number of subjects, and their gender, that were used to collect the EEG data and validate the work. Regarding the number of subjects used, few authors performed studies involving a statistically significant number of participants (30). Moreover, there is not a fair distribution of the gender of the subjects, since most of the studies were performed mostly with men.

The authors mainly resorted to images or videos as the stimuli used to elicit emotions. However, only in the case of images the authors used well-known datasets. Furthermore, there is no agreement among the set of emotions to be recognized, with the majority of the works intending to identify basic emotions (or subsets of them), and the remaining focusing on the valence and arousal levels. When the number of emotions to be recognized increases, the accuracy tends to diminish.

Various devices to collect the data have been used, with different sampling frequencies, as well as different sets of electrodes. There is no consensus among the authors about the number of electrodes that must be used as well as their positioning. The authors mainly used brain waves as features, and used different methods for their extraction. Further explanations between the relationship of the features used and emotions that the work aims to recognize would be an asset to understand the results presented.

Most works apply artifact removal techniques to improve the quality of the collected signals. Multiple classifiers were used, with a large set of the authors training various classifiers and selecting the best one. It is recommended that authors present more detail about the parameters of the classifier, and to perform manual validation of the pre-processed EEG signals, to ensure that the techniques applied are sufficient to remove the existing noise.

SECTION 6Best Practice Recommendations
In this section, we present a set of best practice recommendations concerning both the applicability and steps that compose an EEG-based Emotion Recognizer. For this, we considered the recommendations from Brouwer et al. and our analysis of each of their key points presented in Section 4, as well as the analysis described in Section 5.

Applicability
Explain the advantages of the use of the EEG over other physiological measures;

Present the applicability of EGG-based emotion recognizers to real-world problems, and what can these recognizers bring to applications on those fields.

Test Protocol
To get statistical and meaningful results, use at least 30 subjects in the study. In case authors use subjects of both genders, the number of subjects should be balanced;

Collect information besides the EEG signal (e.g., subjective evaluation, facial expressions to validate the subjective evaluation, other physiological measure) to use as ground-truth.

Ensure that the time used to present the stimulus to the subject is enough to elicit an emotional reaction, but not too long to provide habituation to the stimulus (which may affect self-evaluations made by the subject);

Whenever possible use stimulus from existing datasets, such as IAPS, IADS, or DEAP;

Present the set of emotions to be identified, and how they are suppose to vary with the EEG signals collected;

Whenever possible use one of the sets of emotions already presented in the literature, leading to comparable studies (e.g., Ekman, Plutchik);

Design the study with a high level of comfort and instructions (e.g., provide relaxing time between images, good illumination and temperature; instruct the user to avoid moving/blinking during image visualization).

EEG Recordings
Describe the device used to collect the physiological signals, and its sampling rate;

Identify the positioning system, and the electrode positions used to gather the EEG signals.

Artifact Filtering
Artifact remotion should be performed to remove known artifacts (EOG, muscle, etc.) that may arise even if a proper design is applied;

Validate if an existing variation in the signal when visualizing a stimulus occur due to the successfully elicitation of the emotion or due to the confounds, and if so, manually evaluate the signal to remove them.

Feature Extraction
In case authors do not use data collected from all the electrodes, indicate which ones were used;

Present the features extracted from the signal;

Provide information about the computational methods used to extract the features, but, more importantly, detail how the features are supposed to relate with the emotions to be identified, i.e., what is the expected behavior of the methods if a given emotion is successfully elicited and the signal is free of noise.

Classification
Provide details about the classifiers used, in particular, which was used and the parameters used to train it (e.g., many authors only indicate the use of SVM but do not indicate the kernel used);

Collect data from each subject in different sessions over time to avoid dependency between the training and test data;

Present information about the type of recognizer: offline or online;

Identify if the system is user-dependent or independent, since the results differ considerably among them (better accuracies in the user-dependent recognizers are usually achieved);

Guarantee that the pre-processing and classification procedures are independent of the validation data;

Explain the metrics used to evaluate the recognizer performance. If more than one emotion is recognized, provide the individual performance metrics (ideally, a confusion matrix), and not only the final average;

Examine multiple features and combinations among them;

Present, and explain the results considering existing relations between the features and the neurophysiological processes in use (e.g., a given feature or set of features is supposed to perform better with a set of electrodes to identify specific emotions).

SECTION 7Conclusions
In this paper, we present an analysis of the works, from 2009 to 2016, that propose novel methods for the recognition of emotions through EEG signals. Our analysis draws on two perspectives: one more general that takes into consideration a set of recommendations to avoid the common pitfalls of this research area, and another more specific that considers the aspects involved in the process of recognizing emotions from EEG (e.g., subjects, features extracted, classifiers, etc.).

As a result of the analysis and together with the recommendations from Brouwer, we derive a set of best practice recommendations to help researchers produce well-validated and high-quality works, able to be reproducible and replicable. We hope that this analysis will be useful for the research community, and in particularly for those who are entering this field of research.