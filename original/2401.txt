Federated learning (FL) is currently the most widely adopted framework for collaborative training of (deep) machine learning models under privacy constraints. Albeit its popularity, it has been observed that FL yields suboptimal results if the local clients' data distributions diverge. To address this issue, we present clustered FL (CFL), a novel federated multitask learning (FMTL) framework, which exploits geometric properties of the FL loss surface to group the client population into clusters with jointly trainable data distributions. In contrast to existing FMTL approaches, CFL does not require any modifications to the FL communication protocol to be made, is applicable to general nonconvex objectives (in particular, deep neural networks), does not require the number of clusters to be known a priori, and comes with strong mathematical guarantees on the clustering quality. CFL is flexible enough to handle client populations that vary over time and can be implemented in a privacy-preserving way. As clustering is only performed after FL has converged to a stationary point, CFL can be viewed as a postprocessing method that will always achieve greater or equal performance than conventional FL by allowing clients to arrive at more specialized models. We verify our theoretical analysis in experiments with deep convolutional and recurrent neural networks on commonly used FL data sets.

SECTION I.Introduction
Federated learning (FL) [1]–[2][3][4][5] is a distributed training framework, which allows multiple clients (typically, mobile or the IoT devices) to jointly train a single deep learning model on their combined data in a communication-efficient way, without requiring any of the participants to reveal their private training data to a centralized entity or to each other. FL realizes this goal via an iterative three-step protocol where, in every communication round t , the clients first synchronize with the server by downloading the latest master model θt . Every client then proceeds to improve the downloaded model by performing multiple iterations of stochastic gradient descent with minibatches sampled from its local data Di , resulting in a weight-update vector
Δθt+1i=SGD(θt,Di)−θt,i=1,…,M.(1)
View SourceFinally, all clients upload their computed weight-updates to the server, where they are aggregated by weighted averaging according to
θt+1=θt+∑i=1M|Di||D|Δθt+1i(2)
View Sourceto create the next master model (see [1]). The procedure is summarized in Algorithm 2.


Algorithm 2
FL

Show All

FL implicitly makes the assumption that it is possible for one single model to fit all client’s data generating distributions φi at the same time. Given a model fθ:X→Y parameterized by θ∈Θ and a loss function l:Y×Y→R≥0 , we can formally state this assumption as follows.

Assumption 1 (FL):
There exists a parameter configuration θ∗∈Θ that (locally) minimizes the risk on all clients’ data generating distributions at the same time
Ri(θ∗)≤minθ∥θ−θ∗∥<εRi(θ),i=1,…,M(3)
View Sourcefor some ε>0 . Hereby
Ri(θ)=∫l(fθ(x),y)dφi(x,y)(4)
View Sourceis the risk function associated with distribution φi .

It is easy to see that this assumption is not always satisfied. Concretely, it is violated if either: 1) the model fθ is not expressive enough to fit all distributions at the same time or 2) clients have disagreeing conditional distributions φi(y|x)≠φj(y|x) . Simple counterexamples for both cases are presented in Fig. 1.


Fig. 1.
Two toy cases in which the FL assumption is violated. Blue points belong to clients that follow φ1 , while orange points belong to clients that follow φ2 . Left: FL assumes that a single model can fit all clients’ data distributions at the same time. Middle: federated XOR-problem. An insufficiently complex model is not capable of fitting all clients’ data distributions at the same time. Right: if different clients’ conditional distributions diverge, no single model can fit all distributions at the same time. In all cases, however, the data of clients belonging to the same cluster can be easily separated.

Show All

In the following, we will call a set of clients and their data generating distributions φ congruent (with respect to f and l ) if they satisfy Assumption 1 and incongruent if they do not.

In this work, we argue that Assumption 1 is frequently violated in real FL applications, especially given the fact that, in FL, clients: 1) can hold arbitrary non-i.i.d. data, which cannot be audited by the centralized server due to privacy constraints and 2) typically run on limited hardware which puts restrictions on the model complexity. For illustration, consider the following practical scenarios.

Varying Preferences: Assume a scenario where every client holds a local data set of images of human faces and the goal is to train an “attractiveness” classifier on the joint data of all clients. Naturally, different clients will have varying opinions about the attractiveness of certain individuals, which corresponds to disagreeing conditional distributions on all clients’ data. Assume, for instance, that one half of the client population thinks that people wearing glasses are attractive, while the other half thinks that those people are unattractive. In this situation, one single model will never be able to accurately predict the attractiveness of glasses-wearing people for all clients at the same time (confer also Fig. 1, right).

Limited Model Complexity: Assume that a number of clients are trying to jointly train a language model for next-word prediction on private text messages. In this scenario, the statistics of a client’s text messages will likely vary a lot based on demographic factors, interests, and so on. For instance, text messages composed by teenagers will typically exhibit different statistics than those composed by elderly people. An insufficiently expressive model will not be able to fit the data of all clients at the same time (see Fig. 1, middle).

Presence of Adversaries: A special case of incongruence is given if a subset of the client population behaves in an adversarial manner. In this scenario, the adversaries could deliberately alter their local data distribution in order to encode arbitrary behavior into the jointly trained model, thus affecting the model decisions on all other clients and causing potential harm [6].

Federated Multitask Learning: The goal in federated multitask learning (FMTL) is to provide every client with a model that optimally fits its local data distribution. In all the above-described situations, the ordinary FL framework, in which all clients are treated equally and only one single global model is learned, is not capable of achieving this goal.

In order to incorporate the above-presented problems with incongruent data generating distributions, we suggest generalizing the conventional FL assumption.

Assumption 2 (CFL):
There exists a partitioning C={c1,…,cK} , ⋃˙Kk=1ck={1,…,M} of the client population, such that every subset of clients c∈C satisfies the conventional FL assumption.

In this work, we present clustered FL (CFL), a novel algorithmic framework that is able to deal with FMTL problems that satisfy Assumption 2. By identifying the hidden clustering structure, CFL allows clients with similar data to profit from one another while minimizing the harmful interference between clients with dissimilar data. Our main contributions are given as follows.

Contributions:

We highlight an important practical limitation of conventional FL, namely, incongruent client data distributions (see Section I).

We derive a computationally efficient tool based on the cosine similarity between the clients’ gradient updates that provably allows us to infer whether two members of the client population have different data generating distributions, thus making it possible for us to infer the clustering structure C (see Section II-A).

We address the question of when to cluster and derive a criterion that ensures that clustering only takes place in the incongruent case, where benefits can be expected (see Section II-B).

Based on these theoretical insights, we present the CFL algorithm (see Section II-C).

We investigate several practical concerns (varying client populations, training with formal privacy guarantees, and communication of weight-updates instead of gradients) and demonstrate that CFL can seamlessly adapt to these conditions/constraints (see Section IV).

We evaluate our theoretical findings on large-scale deep neural networks and demonstrate vast performance improvements over the conventional FL algorithm when optimizing over incongruent clients (see Section V).

A summary of the most important symbols used throughout this article is provided in Table I. We additionally refer the reader to Table II, which gives a detailed comparison between our proposed method and the related methods for FMTL.

TABLE I Summary of Symbols

TABLE II Qualitative Comparison Between the Methods for FMTL

SECTION II.Clustered Federated Learning
In this section, we address the question of how to solve distributed learning problems that satisfy Assumption 2 (which generalizes the FL Assumption 1). This will require us to identify the correct partitioning C , which, at first glance, seems like a daunting task, as, under the FL paradigm, the server does not have access to the clients’ data, their data generating distributions, or any meta information thereof.

A. Cosine Similarity-Based Bipartitioning
An easier task than trying to directly infer the entire clustering structure C is to find a correct bipartitioning in the sense of the following definition.

Definition 1:
Let M≥K≥2 and
I:{1,…,M}→{1,…,K},i↦I(i)(5)
View Sourcebe the mapping that assigns a client i to its data generating distribution φI(i) . Then, we call a bipartitioning c1∪˙c2={1,…,M} with c1≠∅ and c2≠∅ correct if and only if
I(i)≠I(j)∀i∈c1,j∈c2.(6)
View SourceIn other words, we call a bipartitioning correct if clients with the same data generating distribution end up in the same cluster. It is easy to see that the clustering C={c1,…,ck} can be obtained after exactly K−1 correct bipartitions.

In the following, we will demonstrate that there exists an explicit criterion based on which a correct bipartitioning can be inferred. To see this, let us first look at the following simplified FL setting with M clients, in which the data on every client were sampled from one of two data generating distributions φ1 and φ2 such that
Di∼φI(i)(x,y).(7)
View SourceEvery client is associated with an empirical risk function
ri(θ)=∑(x,y)∈Dilθ(f(x),y)(8)
View Sourcewhich approximates the true risk arbitrarily well if the number of data points on every client is sufficiently large
ri(θ)≈RI(i)(θ).(9)
View SourceFor demonstration purposes, let us first assume equality in (9). Then, the FL objective becomes
F(θ):=∑i=1M|Di||D|ri(θ)=a1R1(θ)+a2R2(θ)(10)
View Sourcewith a1=∑i,I(i)=1|Di|/|D| , a2=∑i,I(i)=2|Di|/|D| , and D=⋃i=1,…,MDi . Under standard assumptions, it has been shown [7], [8] that the FL optimization protocol described in (1) and (2) converges to a stationary point θ∗ of the FL objective. In this point, it holds that
0=∇F(θ∗)=a1∇R1(θ∗)+a2∇R2(θ∗).(11)
View SourceNow, we are in one of the two situations. Either it holds that ∇R1(θ∗)=∇R2(θ∗)=0 ; in that case, we have simultaneously minimized the risk of all clients. This means that φ1 and φ2 are congruent, and we have solved the distributed learning problem, or, otherwise, it has to hold
∇R1(θ∗)=−a2a1∇R2(θ∗)≠0(12)
View Sourceand φ1 and φ2 are incongruent. In this situation, the cosine similarity between the gradient updates of any two clients is given by
αi,j:===α(∇ri(θ∗),∇rj(θ∗)):=⟨∇ri(θ∗),∇rj(θ∗)⟩∥∇ri(θ∗)∥∥∇rj(θ∗)∥⟨∇RI(i)(θ∗),∇RI(j)(θ∗)⟩∥∇RI(i)(θ∗)∥∥∇RI(j)(θ∗)∥{1,−1,if I(i)=I(j)if I(i)≠I(j).(13)
View SourceConsequently, a correct bipartitioning is given by
c1={i|αi,0=1},c2={i|αi,0=−1}.(14)
View SourceThis consideration provides us with the insight that, in a stationary solution of the FL objective θ∗ , we can distinguish clients based on their hidden data generating distribution by inspecting the cosine similarity between their gradient updates. For a visual illustration of the result, we refer to Fig. 2.


Fig. 2.
Left: optimization path of FL with four clients, belonging to two different clusters with incongruent data distributions. FL converges to a stationary solution of the FL objective θ∗ where the gradients of the two clients are of the positive norm and point into opposite directions (13). While the cosine similarity between gradient updates from the same cluster stays more or less constant throughout the federated training process, the cosine similarity between the gradient updates from different clusters quickly decreases.

Show All

If we drop the equality assumption in (9) and allow for an arbitrary number of data generating distributions K , we obtain the following generalized version of the result (13).

Theorem 1 (Separation Theorem):
Let D1,…,DM be the local training data of M different clients, each data set sampled from one of K different data generating distributions φ1,…,φK such that Di∼φI(i)(x,y) . Let the empirical risk on every client approximate the true risk at every stationary solution of the FL objective θ∗ subject to
∥∇RI(i)(θ∗)∥>∥∇RI(i)(θ∗)−∇ri(θ∗)∥(15)
View Sourceand define
γi:=∥∇RI(i)(θ∗)−∇ri(θ∗)∥∥∇RI(i)(θ∗)∥∈[0,1).(16)
View SourceThen, there exists a bipartitioning c∗1∪˙c∗2={1,…,M} of the client population such that the maximum similarity between the updates from any two clients from different clusters can be bounded from above according to
αmaxcross:==≤minc1∪˙c2={1,…,M}maxi∈c1,j∈c2α(∇ri(θ∗),∇rj(θ∗))maxi∈c∗1,j∈c∗2α(∇ri(θ∗),∇rj(θ∗))⎧⎩⎨⎪⎪⎪⎪⎪⎪cos(πK−1)Hi,j+sin(πK−1)1−H2i,j−−−−−−√if H≥cos(πK−1)1else(17)
View Sourcewith
Hi,j=−γiγj+1−γ2i−−−−−√1−γ2j−−−−−√∈(−1,1].(18)
View SourceAt the same time, the similarity between the updates from clients that share the same data generating distribution can be bounded from below by
αminintra:=mini,jI(i)=I(j)α(∇θri(θ∗),∇θrj(θ∗))≥mini,jI(i)=I(j)Hi,j.(19)
View SourceProof of Theorem 1 can be found in the Appendix.

Remark 1:
In the case with two data generating distributions (K=2 ), the result simplifies to
αmaxcross=maxi∈c∗1,j∈c∗2α(∇θri(θ∗),∇θrj(θ∗))≤maxi∈c∗1,j∈c∗2−Hi,j(20)
View Sourcefor a certain partitioning, respective
αminintra=mini,jI(i)=I(j)α(∇θri(θ∗),∇θrj(θ∗))≥mini,jI(i)=I(j)Hi,j(21)
View Sourcefor two clients from the same cluster. If additionally γi=0 for all i=1,…,M , then Hi,j=1 , and we retain result (13). From Theorem 1, we can directly deduce a correct separation rule.

Corollary 1:
If, in Theorem 1, K and γi , i=1,…,M , are in such a way that
αminintra>αmaxcross(22)
View Sourcethen the partitioning
c1,c2←argminc1∪˙c2=c(maxi∈c1,j∈c2αi,j)(23)
View Sourceis always correct in the sense of Definition 1.

Proof:
Set
c1,c2←argminc1∪˙c2=c(maxi∈c1,j∈c2αi,j)(24)
View Sourceand let i∈c1 and j∈c2 ; then
αi,j≤αmaxcross<αminintra=mini,jI(i)=I(j)αi,j(25)
View Sourceand hence, i and j cannot have the same data generating distribution.

This consideration leads us to the notion of the separation gap.

Definition 2 (Separation Gap):
Given a cosine-similarity matrix α and a mapping from client to data generating distribution I , we define the notion of a separation gap
g(α):==αminintra−αmaxcrossmini,jI(i)=I(j)αi,j−minc1∪˙c2=c(maxi∈c1,j∈c2αi,j).(26)(27)
View Source

Remark 2:
By Corollary 1, the bipartitioning (23) will be correct in the sense of Definition 1 if the separation gap g(α) is greater than zero.

Theorem 1 gives an estimate for the similarities in the absolute worst case. In practice, αminintra typically will be much larger, and αmaxcross typically will be much smaller, especially if the parameter dimension d:=dim(Θ) is large. For instance, if we set d=102 (which is still many orders of magnitude smaller than typical modern neural networks), M=3K , and assume ∇RI(i)(θ∗) and ∇RI(i)(θ∗)−∇ri(θ∗) to be normally distributed for all i=1,…,M ; then, experimentally, we find (see Fig. 3) that
P[‘‘Correct Clustering′′]≥P[g(α)>0]≈1(28)
View Sourceeven for large values of K>10 and
γmax:=maxi=1,…,Mγi>1.(29)
View SourceThis suggests that using the cosine similarity criterion (23), we can readily find a correct bipartitioning c1,c2 even if the number of data generating distributions is high, and the empirical risk on every client’s data is only a very loose approximation of the true risk.


Fig. 3.
Clustering quality as a function of the number of data generating distributions K and the relative approximation noise γ . For all values of K and γ in the green area, CFL will always correctly separate the clients (by Theorem 1). For all values of K and γ in the blue area, we find empirically that CFL will correctly separate the clients with probability close to 1.

Show All

B. Distinguishing Congruent and Incongruent Clients
In order to appropriately generalize the classical FL setting, we need to make sure to only split up clients with incongruent data distributions. In the classical congruent non-i.i.d. FL setting described in [1] where one single model can be learned, performance will typically degrade if clients with varying distributions are separated into different clusters due to the restricted knowledge transfer between clients in different clusters. Luckily, we have a criterion at hand to distinguish the two cases. To realize this, we have to inspect the gradients computed by the clients at a stationary point θ∗ . When client distributions are incongruent, the stationary solution of the FL objective by definition cannot be stationary for the individual clients. Hence, the norm of the clients’ gradients has to be strictly greater than zero. If, conversely, the client distributions are congruent, federated optimization will be able to jointly optimize all clients’ local risk functions, and hence, the norm of the clients’ gradients will tend toward zero as we are approaching the stationary point. Based on this observation, we can formulate the following criteria that allow us to make the decision whether to split or not: Splitting should only take place if it holds that both: 1) we are close to a stationary point of the FL objective
0≤∥∥∥∥∑i=1,…,MDi|D|∇θri(θ∗)∥∥∥∥<ε1(30)
View Sourceand 2) the individual clients are far from a stationary point of their local empirical risk
maxi=1,…,M∥∇θri(θ∗)∥>ε2>0.(31)
View SourceWe will also experimentally verify the clustering criteria (30) and (31) and give recommendations for the selection of the hyperparameters ε1 and ε2 in Section V-B.

In practice, we have another viable option to distinguish the congruent from the incongruent case. As splitting is only performed after FL has converged to a stationary point θ∗ , the conventional FL solution is always computed as part of CFL. This means that if, after splitting up the clients, a degradation in the model performance is observed, it is always possible to fall back to the FL solution. In this sense, CFL will always improve the FL performance (or perform equally well at worst).

C. Algorithm
CFL recursively bipartitions the client population in a top–down way: starting from an initial set of clients c={1,…,M} and a parameter initialization θ0 , CFL performs FL according to Algorithm 2 in order to obtain a stationary solution θ∗ of the FL objective. After FL has converged, the stopping criterion
0≤maxi∈c∥∇θri(θ∗)∥<ε2(32)
View Sourceis evaluated. If criterion (32) is satisfied, we know that all clients are sufficiently close to a stationary solution of their local risk and, consequently, CFL terminates, returning the FL solution θ∗ . If, on the other hand, criterion (32) is violated, this means that the clients are incongruent, and the server computes the pairwise cosine similarities α between the clients’ latest transmitted updates according to (13). Next, the server separates the clients into two clusters in such a way that the maximum similarity between clients from different clusters is minimized
c1,c2←argminc1∪˙c2=c(maxi∈c1,j∈c2αi,j).(33)
View SourceThis optimal bipartitioning problem at the core of CFL can be solved in O(M3) using Algorithm 1. Since, in FL, it is assumed that the server has far greater computational power than the clients, the overhead of clustering will typically be negligible.


Algorithm 1
Optimal Bipartition

Show All

As derived in Section II-A, a correct bipartitioning can always be ensured if it holds that
αminintra>αmaxcross.
View SourceWhile the optimal cross-cluster similarity αmaxcross can be easily computed in practice, computation of the intra cluster similarity requires knowledge of the clustering structure, and hence, αminintra can only be estimated using Theorem 1 according to
αminintra≥≥mini,jI(i)=I(j)−γiγj+1−γ2i−−−−−√1−γ2j−−−−−√1−2γ2max.(34)(35)
View SourceConsequently, we know that the bipartitioning will be correct if
γmax<1−αmaxcross2−−−−−−−−√(36)
View Sourceindependent of the number of data generating distributions K . This criterion allows us to reject bipartitionings, based on our assumptions on the approximation noise γmax (which is an interpretable hyperparameter).

If criterion (36) is satisfied, CFL is recursively reapplied to each of the two separate groups starting from the stationary solution θ∗ . Splitting recursively continues on until (after at most K−1 recursions) none of the subclusters violate the stopping criterion anymore; at that point, all groups of mutually congruent clients C={c1,…,cK} have been identified, and the CFL problem characterized by Assumption 2 is solved. The communication burden of CFL, thus, increases at most linearly with the number of splits only after the FL solution is reached. The entire recursive procedure is presented in Algorithm 3. A schematic illustration is given in Fig. 4.


Fig. 4.
Schematic overview over the CFL algorithm. By recursively bipartitioning the client population into subgroups of maximum dissimilarity, CFL produces a hierarchy of models of increasing specificity.

Show All


Algorithm 3
CFL

Show All

SECTION III.Related Work
FL [1], [2], [4], [5], [11], [12] is currently the dominant framework for distributed training of machine learning models under communication and privacy constraints. FL assumes the clients to be congruent, i.e., that one central model can fit all client’s distributions at the same time. Different authors have investigated the convergence properties of FL in congruent i.i.d. and non-i.i.d. scenarios: Lin et al. [13], Sattler et al. [14], [15], and Zhao et al. [16] performd an empirical investigation and Li et al. [7], Sahu et al. [8], Jiang and Agrawal [17], and Yu et al. [18] proved convergence guarantees. As argued in Section I, conventional FL is not able to deal with the challenges of incongruent data distributions. Other distributed training frameworks [19]–[20][21][22] are facing the same issues.

The natural framework for dealing with incongruent data is multitask learning [23]–[24][25]. An overview of recent techniques for multitask learning in deep neural networks can be found in [26]. However, all of these techniques are applied in a centralized setting in which all data reside at one location and the server has full control over and knowledge about the optimization process. Smith et al. [9] presented MOCHA that extends the multitask learning approach to the FL setting, by explicitly modeling client similarity via a correlation matrix. However, their method relies on alternating biconvex optimization and is, thus, only applicable to convex objective functions and limited in its ability to scale to massive client populations. Corinzia and Buhmann [27] modeled the connectivity structure between clients and server as a Bayesian network and performed variational inference during learning. Although their method can handle nonconvex models, it is expensive to generalize to large federated networks as the client models are refined sequentially.

Finally, Ghosh et al. [10] proposed a clustering approach, similar to the one presented in this article. However, their method differs from ours in the following key aspects: most significantly, they use l2 -distance instead of cosine similarity to determine the distribution similarity of the clients. This approach has the severe limitation that it only works if the client’s risk functions are convex and the minima of different clusters are well separated. Their method also is not able to distinguish congruent from incongruent settings. This means that the method will incorrectly split up clients in the conventional congruent non-i.i.d. setting described in [1]. Furthermore, their approach is not adaptive in the sense that the decision of whether to cluster or not is made already after the first communication round. In contrast, our method can be applied to arbitrary FL problems with nonconvex objective functions. We also note that we have provided theoretical considerations that allow a systematic understanding of the novel CFL framework. For a detailed qualitative comparison between FMTL methods, we refer to Table II.

SECTION IV.Implementation Considerations
In this section, we consider the practical implementation details of our method. Concretely, we will demonstrate that CFL can be implemented without making modifications to the FL communication protocol and without compromising the privacy of the participating clients. We will also demonstrate that our method is flexible enough to handle client populations that vary over time.

A. Weight-Updates as Generalized Gradients
Theorem 1 makes a statement about the cosine similarity between the gradients of the empirical risk function. In FL, however, due to constraints on the communication budged of the client devices, instead, commonly weight-updates, as defined in (1), are computed and communicated [1]. In order to deviate as little as possible from the classical FL algorithm, it would, hence, be desirable to generalize result 1 to weight-updates. It is commonly conjectured (see [28]) that accumulated minibatch gradients approximate the full-batch gradient of the objective function. Indeed, for a sufficiently smooth loss function and low learning rate, a weight-update computed over one epoch approximates the direction of the true gradient by the Taylor expansion. Given a split
⋃τ=0,…,nb−1Dτ=D(37)
View Sourceof a clients’ data D into nb disjoint batches, we have, after one epoch of SGD
Δθ==≈SGD(θ0,D)−θ0−∑τ=0nb−1η∇θr(θτ,Dτ)−∑τ=0nb−1η∇θr(θ0,Dτ)=−η∇θr(θ0,D)(38)
View Sourcewith
θτ=θτ−1−η∇θr(θτ−1,Dτ−1),τ>0.(39)
View SourceIn the remainder of this work, we will compute cosine similarities between weight-updates instead of gradients according to
αi,j:=⟨Δθi,Δθj⟩∥Δθi∥∥Δθj∥,i,j∈c.(40)
View SourceOur experiments in Section V will demonstrate that computing cosine similarities based on weight-updates, in practice, surprisingly achieves even better separations than computing cosine similarities based on gradients.

B. Preserving Privacy
Every machine learning model carries information about the data it has been trained on. For example, the bias term in the last layer of a neural network will typically carry information about the label distribution of the training data. Different authors have demonstrated that information about a client’s input data (“x ”) can be inferred from the weight-updates that it sends to the server via model inversion attacks [29]–[30][31][32][33]. In privacy-sensitive situations, it might be necessary to prevent this type of information leakage from clients to the server with mechanisms, such as the ones presented in [3]. Luckily, CFL can be easily augmented with an encryption mechanism that achieves this end. As both the cosine similarity between two clients’ weight-updates and the norms of these updates are invariant to orthonormal transformations P (such as the permutation of the indices)
⟨Δθi,Δθj⟩∥Δθi∥∥Δθj∥=⟨PΔθi,PΔθj⟩∥PΔθi∥∥PΔθj∥(41)
View Sourcea simple remedy is, for all clients, to apply such a transformation operator to their updates before communicating them to the server. After the server has averaged the updates from all clients and broadcasted the average back to the clients, they simply apply the inverse operation
1|c|∑i∈cΔθi=P−1(1|c|∑i∈cPΔθi)(42)
View Sourceand the FL protocol can resume unchanged. Other multitask learning approaches require direct access to the client’s data and, hence, cannot be used together with encryption, which means that CFL has a distinct advantage in privacy-sensitive situations.

C. Varying Client Populations and Parameter Trees
Up until now, we always made the assumption that all clients participate from the beginning of training. However, CFL is flexible enough to handle client populations that vary over time.

In order to incorporate this functionality, the server, while running CFL, needs to build a parameter tree T=(V,E) with the following properties.

The tree contains a node v∈V for every (intermediate) cluster cv computed by CFL.

Both cv and the corresponding stationary solution θ∗v obtained by running the FL Algorithm 2 on cluster cv are cached at node v .

At the root of the tree. vroot resides the FL solution over the entire client population with cvroot={1,…,M} .

If the cluster cvchild was created by bipartitioning the cluster cvparent in CFL, then the nodes vparent and vchild are connected via a directed edge e∈E .

At every edge e(vparent→vchild) , the presplit weight-updates of the children clients
Δe={SGD(θ∗vparent,Di)−θv∗parent|i∈cvchild}(43)
View Sourceare cached.

An exemplary parameter tree is shown in Fig. 5. When a new client joins the training, it can get assigned to a leaf cluster by iteratively traversing the parameter tree from the root to a leaf, always moving to the branch that contains the more similar client updates according to Algorithm 4.

Fig. 5.
Exemplary parameter tree created by CFL. At the root node resides, the conventional FL model, obtained by converging to a stationary point θ∗ of the FL objective over all clients {1,…,M} . In the next layer, the client population has been split up into two groups, according to their cosine similarities, and every subgroup has again converged to a stationary point θ∗0 respective θ∗1 . Branching continues recursively until no stationary solution satisfies the splitting criteria. In order to quickly assign new clients to a leaf model, at each edge e of the tree, the server caches the presplit weight-updates Δe of all clients belonging to the two different subbranches. This way, the new client can be moved down the tree along the path of the highest similarity.

Show All


Algorithm 4
Assigning New Clients to a Cluster

Show All

Another feature of building a parameter tree is that it allows the server to provide every client with multiple models at varying specificity. On the path from the root to leaf, the models get more specialized with the most general model being the FL model at the root. Depending on the application and context, a CFL client could switch between the models of different generality. Furthermore, a parameter tree allows us to ensemble multiple models of different specificity together. We believe that investigations along those lines are a promising direction for future research.

Putting all pieces from the previous sections together, we arrive at a protocol for general privacy-preserving CFL that is described in Algorithm 5.


Algorithm 5
With Privacy Preservation and Weight-Updates

Show All

SECTION V.Experiments
A. Practical Considerations
In Section II-A, we showed that the cosine similarity criterion does distinguish different incongruent clients under three conditions: 1) FL has converged to a stationary point θ∗ ; 2) every client holds enough data subject to the empirical risk approximates the true risk; and 3) cosine similarity is computed between the full gradients of the empirical risk. In this section, we will demonstrate that, in practical problems, none of these conditions have to be fully satisfied. Instead, we will find that CFL is able to correctly infer the clustering structure even if clients only hold small data sets and are trained to an approximately stationary solution of the FL objective. Furthermore, we will see that cosine similarity can be computed between weight-updates instead of full gradients, which even improves the performance.

In the experiments of this section, we consider the following FL setup. All experiments are performed on either the MNIST [34] or CIFAR-10 [35] data set using M=20 clients, each of which belonging to one of K=4 clusters. Every client is assigned an equally sized random subset of the total training data. To simulate an incongruent clustering structure, every clients’ data are then modified by randomly swapping out two labels, depending on which cluster a client belongs to. For example, in all clients belonging to the first cluster, data points labeled as “1” could be relabeled as “7” and vice versa, in all clients belonging to the second cluster “3” and “5,” could be switched out in the same way, and so on. This relabeling ensures that both φ(x) and φ(y) are approximately the same across all clients, but the conditionals φ(y|x) diverge between different clusters. We will refer to this as “label-swap augmentation” in the following. In all experiments, we train multilayer convolutional neural networks and adopt a standard FL strategy with three local epochs of training. We report the separation gap (see Definition 2)
g(α):=αminintra−αmaxcross(44)
View Sourcewhich, according to Corollary 1, tells us whether CFL will correctly bipartition the clients
g(α)>0⇒‘‘Correct Clustering.′′(45)
View Source

Number of Data Points: We start out by investigating the effects of data set size on the cosine similarity. We randomly subsample from each client’s training data to vary the number of data points on every client between 10 and 200 for MNIST and 100 and 2400 for CIFAR. For every different local data set size, we run FL for 50 communication rounds; after that, training progress has come mostly to halt, and we can expect to be close to a stationary point. After round 50, we compute the pairwise cosine similarities between the weight-updates and the separation gap g(α) . The results are shown in Fig. 6. As expected, g(α) grows monotonically with increasing data set size. On the MNIST problem, as little as 20 data points on every client are sufficient to achieve correct bipartitioning in the sense of Definition 1. On the more difficult CIFAR problem, a higher number of around 500 data points is necessary to achieve correct bipartitioning.


Fig. 6.
Separation gap g(α) as a function of the number of data points on every client for the label-swap problem on MNIST and CIFAR. From Corollary 1, we know that CFL will always find a correct bipartitioning if g(α)>0 . On MNIST, this is already satisfied if clients hold as little as 20 data points if weight-updates are used for the computation of the similarity α .

Show All

Proximity to Stationary Solution: Next, we investigate the importance of proximity to a stationary point θ∗ for the clustering. Under the same setting as in the previous experiment, we reduce the number of data points on every client to 100 for MNIST and 1500 for CIFAR and compute the pairwise cosine similarities and the separation gap after each of the first 50 communication rounds. The results are shown in Fig. 7. Again, we see that the separation quality monotonically increases with the number of communication rounds. On MNIST and CIFAR, as little as 10 communication rounds are necessary to obtain a correct clustering.


Fig. 7.
Separation gap g(α) as a function of the number of communication rounds for the label-swap problem on MNIST and CIFAR. The separation quality monotonically increases with the number of communication rounds of FL. Correct separation in both cases is already achieved after around ten communication rounds if α is computed using weight-updates.

Show All

Weight-Updates Instead of Gradients: In both the abovementioned experiments, we computed the cosine similarities α based on either the full gradients
αi,j=⟨∇θri(θ),∇θrj(θ)⟩∥∇θri(θ)∥∥∇θrj(θ)∥(‘‘Gradient′′)(46)
View Sourceor federated weight-updates
αi,j=⟨Δθi,Δθj⟩∥Δθi∥∥Δθj∥(‘‘Weight−Update′′)(47)
View Sourceover three epochs. Interestingly, weight-updates seem to provide even better separation g(α) with fewer data points and at a greater distance to a stationary solution. This comes in very handy as it allows us to leave the FL communication protocol unchanged. In all following experiments, we will compute cosine similarities based on weight-updates instead of gradients.

B. Distinguishing Congruent and Incongruent Clients
In this section, we experimentally verify the validity of the clustering criteria (30) and (31) in an FL experiment on MNIST with two clients holding data from incongruent and congruent distributions. In the congruent case, client one holds all training digits “0” to “4,” and client two holds all training digits “5” to “9.” In the incongruent case, both clients hold a random subset of the training data, but the distributions are modified according to the “label swap” rule described earlier. Fig. 8 shows the development of the average update norm [see (30)] and the maximum client norm [see (31)] over the course of 1000 communication rounds. As predicted by the theory, in the congruent case, the maximum client norm converges to zero, while, in the incongruent case, it stagnates and even increases over time. In both cases, the average update norm tends to zero, indicating convergence to a stationary point (see Fig. 8).


Fig. 8.
Experimental verification of the norm criteria (31) and (30). Displayed is the development of gradient norms over the course of 1000 communication rounds of FL with two clients holding data from incongruent (left) and congruent distributions (right). In both cases, FL converges to a stationary point of F(θ) , and the average update norm (30) goes to zero. In the congruent case, the maximum norm of the client updates (31) decreases along with the server update norm, while, in contrast in the incongruent case, it stagnates and even increases.

Show All

The considerations in this section lead us to the following recommendations regarding the selection of the hyperparameters ε1 and ε2 .

As the quality of the clustering improves with proximity to a stationary solution (see Fig. 7), the value for ε1 should be set as small as the run-time restrictions allow. A good rule of thumb is to set it to around a tenth of the maximum average update norm ε1≈maxt∥Δθtc∥/10 .

The value of ε2 should be set in accordance with the number of available clients and/or prior knowledge on the heterogeneity of the client data. The smaller the value of ε2 , the more likely it is that the client population will be separated by CFL. In our experiments, we obtained good results by setting ε2∈[ε1,10ε1] .

C. Clustered Federated Learning
In this section, we apply CFL as described in Algorithm 5 to different FL setups that are inspired by our motivating examples in the introduction. In all experiments, the clients perform three epochs of local training at a batch size of 100 in every communication round.

Image Classification on CIFAR-10: We split the CIFAR-10 training data randomly and evenly among M=20 clients, which we group into K=4 different clusters. All clients belonging to the same cluster apply the same random permutation Pc(i) to their labels such that their modified training and test data are given by
Di^={(x,PI(i)(y))|(x,y)∈Di}(48)
View Sourcerespective
Dtesti^={(x,PI(i)(y))|(x,y)∈Dtest}.(49)
View SourceThe clients then jointly train a five-layer convolutional neural network on the modified data using CFL with three epochs of local training at a batch size of 100. Fig. 9 (top) shows the joint training progression: In the first 50 communication rounds, all clients train one single model together, following the conventional FL protocol. After these initial 50 rounds, training has converged to a stationary point of the FL objective, and the client test accuracies stagnate at around 20%. Conventional FL would be finalized at this point. At the same time, we observe (see Fig. 9, bottom) that a distinct gap g(α)=αminintra−αmaxcross has developed (①), indicating an underlying clustering structure. In communication round 50, the client population is, therefore, split up for the first time, which leads to an immediate 25% increase in validation accuracy for all clients belonging to the “purple” cluster that was separated out ②. Splitting is repeated in communication rounds 100 and 150 until all clusters have been separated, and g(α) has dropped to below zero in all clusters (③), which indicates that clustering is finalized. At this point, the accuracy of all clients has more than doubled the one achieved by the FL solution and is now at close to 60% ④. This underlines that, after standard FL, our novel CFL can detect the necessity for subsequent splitting and clustering that enable arriving at a significantly higher performance. In addition, the cluster structure found can potentially be illuminating as it provides interesting insight into the composition of the complex underlying data distribution.


Fig. 9.
CFL applied to the “permuted labels problem” on CIFAR with 20 clients and four different permutations. Top: accuracy of the trained model(s) on their corresponding validation sets. Bottom: separation gaps g(α) for all different clusters. After an initial 50 communication rounds, a large separation gap has developed, and a first split separates the purple group of clients, which leads to an immediate drastic increase of accuracy for these clients. In communication rounds 100 and 150, this step is repeated until all clients with incongruent distributions have been separated. After the third split, the model accuracy for all clients has more than doubled, and the separation gaps in all clusters have dropped to below zero, which indicates that the clustering is finalized.

Show All

Language Modeling on Ag-News: The Ag-News corpus is a collection of 120000 news articles belonging to one of the four topics “World,” “Sports,” “Business,” and “Sci/Tech.” We split the corpus into 20 different subcorpora of the same size, with every subcorpus containing only articles from one topic and assign every corpus to one client. Consequently, the clients form four clusters based on what type of articles they hold. Every client trains a two-layer LSTM network to predict the next word on its local corpus of articles. Fig. 10 shows 100 communication rounds of multistage CFL applied to this distributed learning problem. As we can see, FL again converges to a stationary solution after around 30 communication rounds. At this solution, all clients achieve a perplexity of around 43 on their local test set. After the client population has been split up in communication rounds 30, 60, and 90, the four true underlying clusters are discovered. After the 100th communication round, the perplexity of all clients has dropped to less than 36. The FL solution, trained over the same amount of communication rounds, still stagnates at an average perplexity of 42.


Fig. 10.
CFL applied to the Ag-News problem. Top: perplexity achieved by the different clients on their local test set (lower is better). The clients are separated in communication rounds 30, 60, and 90. After the final separation, the perplexity of all clients on their local test set has dropped to less than 36, while the FL solution (black dotted) still stagnates at a perplexity of 42.

Show All

SECTION VI.Conclusion
In this article, we presented CFL, a framework for FMTL that can improve any existing FL framework by enabling the participating clients to learn more specialized models. CFL makes use of our theoretical finding that (at any stationary solution of the FL objective) the cosine similarity between the weight-updates of different clients is highly indicative of the similarity of their data distributions. This crucial insight allows us to provide strong mathematical guarantees on the clustering quality under mild assumptions on the clients and their data, even for arbitrary nonconvex objectives.

We demonstrated that CFL can be implemented in a privacy-preserving way and without having to modify the FL communication protocol. Moreover, CFL is able to distinguish situations in which a single model can be learned from the clients’ data from those in which this is not possible and only separates clients in the latter case.

Our experiments on convolutional and recurrent deep neural networks show that CFL can achieve drastic improvements over the FL baseline in terms of classification accuracy/perplexity in situations where the clients’ data exhibit a clustering structure.

Finally, we note that our work also exposes a new privacy issue in FL as it demonstrates that the information about client data similarity can be inferred from their weight-updates. We argue that the privacy loss inflicted is tolerable in most situations as the mere knowledge of client similarity does not reveal anything about the clients’ data. Nevertheless, this fact should be considered when implementing FL for privacy-sensitive applications.

In future work, we will explore to what extent CFL can be used in conjunction with differential privacy mechanisms [36], [37], as well as parameter update compression methods [21], [38]–[39][40]. Furthermore, it would be interesting to study explainable AI techniques [41], [42] also in the context of (clustered) FL.
