Modern autonomous vehicles are required to perform various visual perception tasks for scene construction and motion decision. The multiobject tracking and instance segmentation (MOTS) are the main tasks since they directly influence the steering and braking of the car. Implementing both tasks using a multitask learning neural network presents significant challenges in performance and complexity. Current work on MOTS devotes to improve the precision of the network with a two-stage tracking by detection model, which is difficult to satisfy the real-time requirement of autonomous vehicles. In this article, a real-time multitask network named YolTrack based on one-stage instance segmentation model is proposed to perform the MOTS task, achieving an inference speed of 29.5 frames per second (fps) with slight accuracy and precision drop. The YolTrack uses ShuffleNet V2 with feature pyramid network (FPN) as a backbone, from which two decoders are extended to generate instance segments and embedding vectors. Segmentation masks are used to improve the tracking performance by performing logic AND operation with feature maps, proving that foreground segmentation plays an important role in object tracking. The different scales of multiple tasks are balanced by the optimized geometric mean loss during the training phase. Experimental results on the KITTI MOTS data set show that YolTrack outperforms other state-of-the-art MOTS architectures in real-time aspect and is appropriate for deployment in autonomous vehicles.
SECTION I.Introduction
Autonomous vehicle is a rapidly advancing application area benefiting from advances in computer vision, decision fusion, and control [1]–[2][3]. As one of the core technologies of autonomous vehicle, environment perception is responsible for analyzing the surrounding road traffic conditions and provides data support for planning and decision-making, which requires high performance of perception in precision, real-time performance, power consumption, and other aspects. Meanwhile, with the increase in the number of on-board sensors, multisource sensor data fusion, embedded deployment, fault diagnosis, and tolerance are the next problems to be solved for autonomous vehicle environment perception [4]–[5][6]. As the dominant means of perception, computer vision consists of tasks, such as object detection, semantic segmentation, depth estimation, and motion detection [7]–[8][9]. In recent years, convolutional neural networks (CNNs) have been widely used in computer vision and obtained satisfactory results. For a single specific application, such as object detection, CNNs can generate bounding boxes on the object and label the category, which has been deployed in actual autonomous vehicles.

In the computer vision community, most efforts are devoted to improving the performance of network on a single task. However, autonomous vehicle is responsible for a series of tasks simultaneously, which presents a challenge to the performance and power consumption of the vehicle platform. Multitask learning (MTL) has been proposed in recent years, processing multiple tasks in a unified way. MultiNet++ [10] proposed a multistream multitask network for joint learning segmentation, depth, and motion with high accuracy. Siam et al. [11] trained a two-stream MTL architecture named MODNet for detection and motion segmentation with RGB images and optical flow input. Joint learning of all tasks is unrealistic since the precision of the networks decreases as tasks increases [12]. Not all tasks can be learned jointly; otherwise, the training results may be perishing. Multitask learning requires correlation between tasks to ensure that tasks can reinforce each other [13]–[14][15]. Instance segmentation and object tracking both need similar features partially, which has been proved in previous work [12].

This article focuses on the task of multiobject tracking and segmentation extended on the basis of MOT, considering detection, segmentation, and tracking together. Unlike detection based on bounding box, pixel-level instance segmentation is not disturbed by target stacking or overlapping, which can effectively improve the tracking performance. Although the performance of detection is nearly perfect, there are still significant challenges in the face of complex urban road traffic scene. Autonomous vehicles require real-time detection and tracking of a large number of targets with high precision simultaneously and resist the interference brought by complex weather conditions.

Most of the proposed multiobject tracking and instance segmentation (MOTS) methods are based on a two-stage neural network model, and the real-time approach has rarely been explored. Voigtlaender et al. [16] extended Mask R-CNN by 3-D convolutions to fuse temporal context and the association embedding to generate association vectors for object tracking and also extended the KITTI tracking data set and MOTChallenge data set to MOTS data set, providing bounding-box-based tracklets and segmentation mask for cars and pedestrians. MOTSNet [17] added a tracking head on Mask R-CNN and proposed mask pooling to extract foreground information of ROI Align features to improve tracking performance. PointTrack [18] regarded 2-D image pixels as 2-D point clouds and used 3-D point cloud processing methods to learn instance embeddings, achieving great improvement on inference speed and execution efficiency.

In this article, a real-time MOTS model called YolTrack is proposed for environment perception and recognition of autonomous vehicles. Different from the two-stage model mentioned above, YolTrack can directly regress the location and category information of the objects to reduce the delay. Bounding-box regression and embedding vectors generation are decoded from the same feature map, and segmentation masks are generated by linearly combining the prototype masks and mask coefficients, which has been shown to produce masks with high quality in YOLACT [19]. The train balance among different tasks is also considered, and a simple and effective loss strategy is proposed to keep the rate of convergence of each task at a close level.

This article shows the high-precision and real-time test results on the KITTI MOTS data set, which proves the practicability of YolTrack in the field of autonomous vehicles. The key contributions of this article are given in the following.

A novel neural network model named YolTrack for multiobject tracking and segmentation is proposed to satisfy the requirements of high-precision and real-time perception of autonomous vehicles.

The MTL theory is applied to MOTS tasks, and an optimized geometric mean multitask loss is proposed for the joint training of object detection, segmentation, and tracking.

Evaluation on the KITTI MOTS data set and Apollo MOTS data set shows that the YolTrack has a huge improvement in real-time compared with the existing methods, as shown in Fig. 1.


Fig. 1.
sMOTSA-speed performance for YolTrack and the state-of-the-art MOTS solutions trained on the KITTI MOTS data set. YolTrack is one of the few methods that achieve real-time performance while ensuring accuracy.

Show All

The rest of this article is organized as follows. Section II introduces the research progress in the MOTS field and MTL field, as well as data sets dedicated to MOTS. Section III elaborates the core methods, including the structure of YolTrack, the loss function for each task, and the MTL strategy. The detailed training and test process and parameter configuration are explained in Section IV with an in-depth analysis of the results. The conclusion is put in Section V to summarize the methods and experimental results of this article.

SECTION II.Related Works
A. Multiobject Tracking and Segmentation
Multiobject tracking and segmentation are both based on the profile information of the object such that the learned features will overlap to some extent, which may lead to better performance for the network on the basis of the theory mentioned in UberNet [12].

Instance segmentation is one of the main tasks for autonomous vehicle perception, including object detection [20] and semantic segmentation. The proposed two-stage CNN approaches first locate the instances from the image and then mark each pixel within the range of different instances [21], which makes the whole process very time-consuming. The idea of one-stage instance segmentation is to provide an embedding while generating the bounding box and combine the two parts to create the final mask [19], [22]–[23][24]. Gao et al. [25] and Bai and Urtasun [26] transformed the instance segmentation task to finding object contour, but it is despairing to solve the problem of overlapping and truncation. Anchor free methods [27] have been proposed and achieve good performance in recent years.

Multiobject tracking involves tracking multiple targets simultaneously in a video [28]. The works in [29] adopt both optical flow and scene flow to track the movement of the objects in 2-D and 3-D. Since tracking involves associating objects at different times, recurrent neural networks and long short-term memory networks are used to learn the matching similarity between the historical information of the tracklets and the current detection [30].

Previous works have proved that detection [8], [31]–[32][33] and segmentation masks can improve tracking results. Tracking based on object detection first locates the interest regions of the objects and then uses the data association algorithm to associate the current object with the historical track. Tracking based on segmentation solves the problem of overlapping and occlusion among objects, using pixel-level masks to locate object for more accurate association. Object tracking based on instance segmentation has more advantages than single tracking in MOTS. MOTSNet [17] proposes mask pooling to extract foreground features using instance segmentation results. PointTrack [18] uses masks to separate the foreground and environment and treat them as unordered 2-D point clouds to compute embedding vectors. In this article, the foreground features are extracted by logic AND operation between the instance segmentation mask and feature map.

B. Multitask Learning
MTL has been widely studied in natural language processing, medical treatment, and other fields. In recent years, robots and autonomous vehicles have emerged [34], and using a single network to perform multiple tasks has become one of the focuses of research [35], especially in the field of environment perception and vision problems. The work in [7] learns object detection and distance prediction jointly using MTL. MTAN is proposed in [14], sharing features between image-to-image predictions and classification tasks by attention mechanism. MultiNet++ [10] and MODNet [11] try to learn object detection and motion jointly in the autonomous driving scenario, where MultiNet++ uses continuous video frames as input and MODNet uses RGB images and optical flow.

A typical MTL network mainly consists of two parts: encoder and decoder. Shared parameters act as the encoder for learning common features among multiple tasks and task-specific parameters play the role of decoder for performing specialized tasks. On the basis of the weight sharing strategy, MTL networks are divided into hard parameter sharing and soft parameter sharing [36]. For hard parameter sharing, as shown in Fig. 2, the parameters of encoder are shared completely among all tasks. For soft parameter sharing, tasks are allowed to have their own encoder or parameters of encoder can be unique to some extent for different tasks. Embedded applications mostly use hard parameter sharing because of simpler structure and higher execution efficiency.


Fig. 2.
Hard parameter sharing. Generally, the share layers are shared by multiple tasks as feature extractors (Encoder), and each task expands to a branch with independent weights (decoder).

Show All

Due to the complexities, uncertainties, or losses in different tasks, the phenomenon of training imbalance is often observed in the training phase of MTL networks. The convergence rate may vary greatly among tasks or some tasks may be dominant, which pose threats to the performance of the network [37], [38]. Different loss strategies are proposed to solve these problems.

In the early MTL architectures, arithmetic sum was often used to get the multitask loss [39], [40]. Many complex heuristics have emerged later to alleviate the problem of training imbalance. Handcrafted task weighting, a method of using weighted summation, finds the weight manually by experiment to bring the losses to the same scale. Kendall et al. [41] proposed a method to compute the task weights adaptively by using homoskedasticity uncertainty, and the weights will converge to constant values during training. The problem is that the constant weights may not fully adapt to the complex training process and balance tasks all the time, so the dynamic weight updating method is proposed [42]. GradNorm [43] attributed the training imbalance of multitask networks to the imbalance of gradient magnitudes backpropagated, normalizing the gradients to update and optimize the loss weights during each training epoch. MultiNet++ proposed the geometric loss strategy to minimal training imbalance by expressing the total loss as the geometric mean of individual task losses. These methods have been proved to be effective and give the explicit expressions of multitask loss function by task heuristics while increasing the complexity of training. Therefore, a simple and effective loss weight strategy to reduce the impact on network training complexity is imperative.

C. MOTS Data Sets
The data sets of MTL networks require the ground truth corresponding to all tasks. For multiobject tracking, objects in each frame of the video sequence are identified by a bounding box with ID and can be associated in different frames. For instance segmentation, objects need to be located precisely by pixel-level segmentation masks. The MOTS data sets need to have the segmentation masks and ID annotations of every object in a video. Voigtlaender et al. [16] extended the existing MOT data set to KITTI MOTS data set, using the semiautomatic annotation procedure, where the full convolution neural network is used to automatically generate the segmentation masks on the bounding boxes and then produces accurate pixel-level annotation through manual adjustment. The KITTI MOTS data set has 21 video sequences containing cars and pedestrians, including 8008 images with 38 319 pixel masks for 749 distinct objects, about 10.6% of which are manually labeled, while the rest were manually corrected on the basis of automatic annotation by neural network. Fig. 3 shows a video sequence in the KITTI MOTS dataset, along with the corresponding segmentation mask and tracking label. The MOTS Challenge data set also provided in [16] contains four video sequences with total 26 894 pixel masks for 228 pedestrians.


Fig. 3.
Short sequence from the KITTI MOTS data set (left) with its ground truth (right).

Show All

Porzi et al. [17] also used a semiautomated annotation method to generate MOTS labels on the KITTI Raw data set relying on images and GPS data. The authors first use the seamless scene segmentation method to produce a set of instance segments for each video sequence and then train a neural network based on optical flow to obtain the tracklets. The ApolloScape data set provides more crowded scenes compared with the KITTI MOTS, and Apollo MOTS is built in [18] using semiautomatic annotation and has the same metric as KITTI MOTS. Since the KITTI MOTS data set is the widest road traffic scenario covered in the existing open data set, the evaluation in this article is mainly carried out on it.

SECTION III.YolTrack
In this section, a real-time architecture named YolTrack for multiobject tracking and segmentation is introduced. The goal is to design a one-stage (without extra-regional proposal step) to generate the segmentation mask and ID corresponding to the targets in the input video sequence in real time.

YolTrack’s architecture is shown in Fig. 4. The ShuffleNet V2 with the feature pyramid network (FPN) is used as the backbone of YolTrack, and the output multiscale features are fed into the instance segmentation decoder and tracking decoder, which generates the pixelwise segmentation masks and embedding vectors for tracking. The instance segmentation decoder follows the implementation of YOLACT introduced in [19], and its results are fed into tracking decoder to optimize object tracking. The number of weights of YolTrack is approximately 2.64M, and the number of operations required to perform the forward propagation once is 1.409 \times 10^{10} FLOPs using the KITTI MOTS data set.


Fig. 4.
YolTrack overview. The arrows represent the transfer direction of the feature maps, and the block represents the network layers. The encoder is based on ShuffleNet V2 with FPN, which outputs three feature maps of different scales. The instance segmentation decoder generates instance masks fed into tracking decoder to produce embedding vector for every instance.

Show All

A. Encoder
In autonomous driving scenarios, in addition to the model precision, model scale and memory access cost (MAC) are also crucial factors to be considered when designing the network, which directly affect the real-time performance of the algorithm. To reduce the network scale and improve the inference speed without precision falling, the backbone of ShuffleNet V2 (without the global pool layer and FC layer) is selected as the feature extraction structure of YolTrack. Excessive group convolution will increase MAC [44], and thus, the input feature channels are split into two branches. Only one branch uses depthwise convolution, and the other remains as identity, which is the basic block of stages, as shown in Fig. 5. Also, channel shuffle operation is used to blend the features learned from the two branches. The difference with the original ShuffleNet V2 is that the max pooling is replaced by convolution. The encoder performs downsampling operation to the original image four times, and the feature maps obtained in the last three times are used as the input of the FPN to obtain multiscale features.


Fig. 5.
Basic block of ShuffleNet V2.

Show All

B. Instance Segmentation Decoder
Two-stage instance segmentation methods require locating the object region before bounding box and mask generation to get the spatial coherence of the features, which brings serious interference to the real-time performance. The one-stage approach used in YOLACT divides the mask generation process into two parts, one for generating “prototype masks” that are independent of the instance and the other for generating mask coefficients to correlate each instance anchor with the prototype mask space, and finally uses simple matrix multiplication to generate masks for instances left after nonmaximum suppression (NMS). The details are presented in [19]. This paper removes the crop operation of YOLACT since the bounding box is the intermediate result.

The instance segmentation decoder consists of three parallel detection branches and an independent segmentation branch. The detection branches detect all objects in the feature maps of all scales generated by the backbone by adding an extra mask coefficient prediction branch on the basis of general object detection structure, that is, the output feature map has 5 + {c} + {k} channels per anchor, where 5 represents the regressors of the bounding box, {c} means classification number, and {k} is the number of mask coefficients’ channels corresponding to the k-channel prototype masks provided by segmentation branch. Only the output of the Stage2 branch of the backbone is fed into the segmentation branch since the mask obtained by deeper backbone features is more robust to the interference of uncertain factors. The size of “prototype masks” provided by the segmentation branch is a quarter of the original images, and bilinear interpolation is used to upsample the combined masks to obtain the instance segmentation masks with the same size as the original images.

C. Tracking by Instance Segmentation
The goal of tracking is to associate the same object in different frames, which requires the identity information of each object in each frame. Inspired by the person reidentification [45], [46], embedding vectors can play the role in object association. In the embedding space, the embedding vectors of two instances are closest to each other when the instances have the same identity and far away from each other if they are different instances. The distance between embedding vectors can be calculated by the Euclidean distance or cosine distance \begin{equation*}d(a, b)=\|a-b\| \quad \text {or} \quad d(a, b)=\frac {a \cdot b}{|a|| b|}.\tag{1}\end{equation*}
View Source

Each real instance needs to be assigned an embedding vector that contains the feature information of the instance. Therefore, each real instance should be extracted before embedding vector calculation. Due to the instance segmentation task, the tracking encoder can directly get the location and mask of the real instance. The segmentation masks and bounding box are fed to the tracking encoder together with the feature map, where the bounding box can get the location of the instance in feature map, and the segmentation mask can better extract the foreground feature and remove the background influence. The bounding-box region contains the background information as well as other potential objects, which will cause that the embedding vectors could not represent the object information accurately. Logic AND operation is carried out first to the instance-located feature map region and the corresponding segmentation mask because the background information of the instance-located region is useless for associating embedding vectors. The masks of the background region are set to zero during operation, and thus, the background information does not contribute to the results. Then, two fully connected layers are followed to generate the final embedding vectors.

Objects may overlap each other in some frames, resulting in masks of multiple objects in a bounding box. In such case, the mask is set to zero not only for the background but also for the region that is different from the object identified by the bounding box. Though some features of the object may be lost due to overlap, as shown in Fig. 6, the association can be done accurately by setting an appropriate distance threshold.


Fig. 6.
Overlap among objects. The front object blocks a portion of the rear object, even dividing it into two parts.

Show All

Tracking by instance segmentation requires the accuracy of object detection and segmentation. Only accurate location and foreground extraction can generate reliable embedding vectors. YolTrack is pretrained on the COCO data set (only including cars and pedestrians) without tracking encoder, and the tracking decoder is added when fine-tuning on KITTI MOTS after the model is able to provide high-precision instance segments. During training, the ground truth masks and predicted bounding boxes are fed to the training encoder and switch to the predicted instance segments while inference.

During inference, all instances (real objects) of the current frame and instances of the previous {T} frames need to do the distance calculation of embedding vectors using (1). The Hungarian algorithm is used to match the instances cross frames. The two instances are associated only if they belong to the same class and are closest to each other as well as the distance is less than the distance threshold. While there is no instance to associate with in {t} frames, the object is considered to have left the field of view and the tracking stops. When a new instance appears, a new ID is assigned to it and tracking begins.

D. MTL Loss
1) Instance Segmentation Loss:
The instance segmentation has three loss functions: segmentation loss {L}_{s} , classification loss {L}_{c} , and bounding-box regression loss {L}_{b} . The computation of the single loss function uses the same approach as YOLACT, that is, binary cross entropy for {L}_{s} , smooth L1 for {L}_{b} , and cross entropy for {L}_{c} . The weight of each loss function differs from that in YOLACT, which is described in Section III-D3.

2) Tracking Loss:
For the tracking decoder, the loss function is used to narrow the distance between instances with the same ID and to separate instances with different IDs as much as possible. The triplet loss is an effective solution [47]–[48][49]. Assuming that {a} (anchor) is a sample randomly selected from the training data set, {p} (positive) is a sample belonging to the same class as {a} , and {n} (negative) is a sample having different class from {a} , the expression of the triplet loss is as follows:\begin{equation*}\sum _{i}^{N} \max \left ({\left \|{f\left ({x_{i}^{a}}\right)-f\left ({x_{i}^{p}}\right)}\right \|-\left \|{f\left ({x_{i}^{a}}\right)-f\left ({x_{i}^{n}}\right)}\right \|+\alpha, 0}\right)\tag{2}\end{equation*}
View Sourcewhere f(*) is the embedded vector of the model output with input * and \alpha is a margin used to adjust convergence and measure similarity. The loss is zero when the distance between an anchor and the negative sample is greater than that between the anchor and positive sample plus \alpha . Otherwise, it indicates that the model cannot associate the same samples correctly and the loss needs to be optimized. The small value of \alpha can accelerate the convergence rate and the loss will go down to zero quickly but bring a difficulty to distinguish different samples. A large value of \alpha may improve the performance of distinguishing similar samples, but the loss can be so large that causes the network does not converge.

For a video sequence, denote \mathcal {T} as all the instances in the video, and {t} is one of the instance with embedding {e} that satisfies t \in \mathcal {T} . \mathcal {P} represents the set of instances that have the same ID as {t} , and \mathcal {N} is the set of the other instances in \mathcal {T} ; then, the loss function of the tracking decoder {L}_{t} can be defined by the following formula:\begin{equation*}\frac {1}{|T|} \sum _{t \in \mathcal {T}} \max \left ({\max _{p \in \mathcal {P}}\left \|{e_{t}-e_{p}}\right \|-\min _{n \in \mathcal {N}}\left \|{e_{t}-e_{n}}\right \|+\alpha, 0}\right).\tag{3}\end{equation*}
View Source

3) MTL Loss:
The importance of loss function for MTL has been shown in Section II-B. A simple and effective multitask loss function is needed to reduce the burden of training phase. In general, whether a multitask loss strategy is simple depends on whether it is necessary to add a new structure to the original model. The multitask loss function is generally considered to find a set of optimal weights \Omega for (4) to make the model obtain the best performance, where {M} is the number of tasks and loss weights \omega ~\in ~\Omega . The commonly used loss strategy is the arithmetic mean, which does not consider the scale of each task. The task weights can be assigned manually, but finding the optimal weights is fiendishly difficult. Using a learning algorithm to optimize weights is a satisfactory way, such as dynamic weight average [42], but the additional structure increases the complexity of the training phase\begin{equation*}L_{\text {total}}=\sum _{i}^{M} w_{i}L_{i}. \tag{4}\end{equation*}
View Source

Inspired by MultiNet++, the geometric mean is used in this article and in view of the actual task has been optimized. The geometric mean does not consider the distribution of the weight of loss function and get better performance with minimal effort. The model in this article is trained by minimizing the following loss function:\begin{equation*}L = \sqrt [{3}]{L_{t} \times ((L_{c} + L_{b})/2) \times L_{s}}.\tag{5}\end{equation*}
View Source

The tasks of classification and bounding-box regression have a similar scale, and they have a relatively high correlation. Therefore, the arithmetic mean of the two losses is calculated, and the geometric mean is calculated with the loss of the multiobject tracking. This approach considers both the relationship between tasks and requires minimal effort during the training phase.

E. Association Algorithm
In the inference phase, we need to associate the same object in continuous frames to generate tracklets in real time. The input video sequence is processed by YolTrack to output the instance segmentation masks, as well as the embedding vector for each object in each frame. The association algorithm is to get a series of sets that contain objects whose embedding vectors’ distance is less than a threshold to obtain the final tracking results.

First, assign an ID to each object in the first frame of the video, that is, each object consists of three parts of information, the embedding vector, class, and ID. For the second frame, calculate the distance between the embedding vectors of the objects in the current frame and previous frame using (1), and the Hungarian algorithm is applied for the maximum association and assign the same ID to the object associated with each other. Repeat this for each newly inputted frame and a series sets of tracklets made up of the same objects are obtained.

Some objects may not be associated with other existing tracklets, and there are two possible scenarios: the one is that the object is a fake object generated by an incorrect instance segmentation result which should be removed and the other is that the object is new and should be assigned a new ID and start a new track. A threshold \beta is set to determine which of the above cases belongs to. When the number of times the object is detected in continuous {t} frames is less than \beta , it is considered as a fake object. The execution logic of the association algorithm is shown in Algorithm 1.


Algorithm 1
Association Algorithm

Show All

SECTION IV.Experiments
The advantages of YolTrack in real-time MOTS task are demonstrated by validating it on the MOTS data sets and comparing it to state-of-the-art methods. This section introduces the metrics used to measure the performance of the network in MOTS, as well as the training details and parameter configuration during evaluation. The experiment is divided into two parts: one is to test the performance of YolTrack and the other is to analyze the effectiveness of segmentation mask on object tracking performance.

A. Evaluation Metrics
In the MOT task, the CLEAR MOT metrics, including multiple object tracking accuracy (MOTA) and multiple object tracking precision (MOTP), are metrics for evaluating the ability of the algorithm to continuously track the objects, where MOTA reflects the accuracy of determining the number of objects and object-related attributes, while MOTP reflects the precision of objects positioning. The MOTS metrics are extended from the CLEAR MOT metrics in [16], which is defined as \begin{align*}{~\text {MOTSA }}=&\frac {|\text {T P}|-|\text {F P}|-|\text {I D} S|}{|\text {GT}|}\tag{6}\\ \text {sMOTSA}=&\frac {\widetilde {\text {T P}}-|\text {F P}|-|\text {I D} S|}{|\text {GT}|}\tag{7}\\ \text {MOTSP}=&\frac {\widetilde {\text {T P}}}{|\text {TP}|}.\tag{8}\end{align*}
View Source

TP is the set of true positives that correctly matched and the false positives set is defined as FP. The ground-truth masks belonging to a predicted tracklet whose ID is different from the tracklet in the previous frame make up the set IDS. The GT represents the set of ground truth and \widetilde {\text {T P}} means the IoU between the ground-truth mask and the predicted mask.

In the same way, the MOTSA and MOTSP measure the precision and accuracy of the model. The soft multiple object and segmentation accuracy (sMOTSA) represents object detection, segmentation, and tracking performance.

B. Experimental Setup
The YolTrack is built on YOLACT by extending a tracking branch and replacing the backbone with ShuffleNet V2. The model is pretrained on the COCO data set for 800 000 steps by an SGD optimizer without the tracking decoder to learn object detection and segmentation. Note that only the classes of the car and pedestrian in COCO are used for pretrain since the KITTI MOTS data set only has these two classes. The learning rate is initialized as 10−3 and warmed up for the first 500 steps using (9), and then, the learning rate is decreased by 0.1^{n} where n~\in [1, 2, 3, 4] at step 280 000, 600 000, 700 000, and 7500 000. Due to the lack of tracking decoder, the loss function in the pretraining phase is degraded from (5) to (10)\begin{align*} \textit {lr}=&10^{-4} + \frac {(\textit {lr} - 10^{-4}) \times \text {step}}{500} \tag{9}\\ L '=&\sqrt [{2}]{((L_{c} + L_{b})/2) \times L_{s}}.\tag{10}\end{align*}
View Source

Then, fine-tune the model with tracking decoder on the KITTI MOTS data set at which phase the input mask of tracking decoder is not the output of instance segmentation decoder but the ground truth. The purpose is to ensure that the tracking decoder can obtain precise instance information to optimize the model parameters during training. YolTrack is trained for 35 epochs on eight graphics cards with a mini-batch 8, which means that during every iteration, eight consecutive images belonging to a video sequence are fed into one of the graphics cards. The data augmentation algorithm is applied to the input images to weaken the overfitting. The tracking decoder generates the embedding vectors of all the actual objects in the eight frames, and then, the loss is calculated using the triplet loss defined by (3) for the embedding vectors of the entire sequence and the total loss is defined by (5). The learning rate is set to 2 \times 10^{-3} , optimized with a 10−3 weight decay using the standard Adam optimizer.

Different from the training, the mask and bounding box provided by the instance segmentation decoder are fed into the tracking decoder during the inference phase. {n} consecutive frames are fed into the network, and the network finally outputs corresponding instance segments and embedding vectors. Note that the bounding box does not serve as the output of the network but rather as an intermediate input to the tracking decoder; at the same time, the input mask is switched to the output of instance segmentation decoder. All that remains is to associate the same objects in the video sequence and generate the tracklets. We compute Euclidean distances between embedding vectors and follow the Hungarian algorithm for optimal matching. Some rules for generating the tracklets are set up to deal with possible problems.

Stop tracking an object that is not detected in consecutive {t} frames, which is considered out of sight. The parameter {t} is set to prevent the object from being lost that is caused by the object being temporarily out of sight or obscured by other objects.

Set the object with a high confidence level that does not match any instances as a new tracking object.

Discard the tracklets that have segments less than the threshold {m} . The very short tracklets are likely due to detection errors or matching errors that are not real tracklets.

All experimental results are obtained by following the above procedure and parameter settings on eight RTX 2080Ti graphics cards. Please refer to Section IV-C for experimental results and analysis.

C. Results on KITTI MOTS
Table I compares the results of YolTrack and several state-of-the-art MOTS methods, including TrackR-CNN, MOTSNet, MOTSFusion, and PointTrack on KITTI MOTS validation data sets. The results of these four networks for comparison have been summarized in [18], which are cited directly in Table I. Speed, sMOTSA, MOTSA, and MOTSP are used here to measure the performance of the network. For YolTrack, results under different configurations are listed (with or without segmentation masks, and different backbones are shown in Table II). The main results show that YolTrack is top-performing on speed with a slight drop in accuracy. The “Frames” column shows the number of frames that the network processes per second, where YolTrack achieves real-time object tracking and segmentation at 29.5 frames per second (fps), ahead of the top-speed PointTrack 7.5 frames. Certainly, increased speed brings an inevitable reduction in accuracy, which is directly reflected in the MOTS metrics, but it is still within the bounds of accuracy. The visualized results are shown in Fig. 7.

TABLE I Results on the Kitti Mots Validation Data Set

TABLE II Results on the Kitti Mots Validation Data Set With Different Backbones


Fig. 7.
Visualized results on the KITTI MOTS data set. Objects of the same color have the same ID.

Show All

TrackR-CNN and MOTSNet use 2-D images for object tracking and segmentation, both of which are based on the structure of Mask R-CNN that need to generate propositional regions before tracking and segmentation. Track-RCNN extends 3-D convolutions to correlate temporal context, and MOTSNet adds an attention mechanism named mask pooling to separate the foreground from the background. MOTSFusion first uses 2-D optical flow to generate short tracklets and then fuses them into 3-D object reconstruction to merge tracklets into long-term tracks, in which process the RRC detector is used to object detection. Such structures are time-consuming, even though they can achieve high accuracy. PointTrack uses a 3-D point cloud processing approach to process 2-D foreground images extracted by a lightweight instance segmentation model, providing a significant speed boost. YolTrack achieves better results in speed using a one-stage lightweight model and the accuracy is second only to PointTrack and higher than TrackR-CNN, which indicates that the high-precision one-stage model can replace the two-stage model to achieve better speed performance. Although a balance between speed and precision is found in this article, the precision has decreased to some extent. In addition to lightweight structure, finding more efficient data processing and calculation methods is the key to further improve the model performance.

The results for three different backbones are shown in Table II. The accuracy of the network will increase as the scale grows, and lightweight model will have better speed performance with no precision loss. After a tradeoff between speed and precision, ShuffleNet V2 is selected as the backbone of YolTrack that will gain better performance.

D. Results on Apollo MOTS
The YolTrack is also validated on the Apollo MOTS, a data set built by Baidu for MOTS task, which has higher instance density and more crowded scenes than the KITTI MOTS data set. It comprises 169 video sequences containing 22 480 frames, which is divided into the train, validation, and test set according to the ratio of 3:2:5. Since Baidu still does not provide the verification platform for the test set, YolTrack is only tested on the validation set. The results are shown in Table III. Some of the data in Table III are from [18].

TABLE III Results on the Apollo Mots Validation Data Set

The results show that the YolTrack has higher performance than DeepSort and TrackRCNN. PointTrack achieves the state-of-the-art performance on the Apollo MOTS data set, and the reason is that PointTrack first generates high-precision segmentation masks using an instance segmentation network and then performs a tracking network according to the masks. By using an independent instance segmentation network, the precision result in PointTrack is better than YolTrack proposed in this article, but there is a certain loss in speed. Meanwhile, two separate models would make it more difficult for deployment. YolTrack is better suited for deployment in autonomous vehicles since its lightweight MTL architecture while meeting real-time and precision requirements.

E. Effectiveness of Segmentation Mask
We verify the effectiveness of the segmentation mask on object tracking by removing the mask from the input of the tracking decoder during validation, which means that only feature maps and bounding-box information are used for object tracking. The last two rows of Table I shows the impact of the segmentation mask on the performance of object tracking, showing a decline in each metric. In terms of sMOTSA, cars and pedestrians have 8.5 and 11.6 drop, respectively, which indicates a significant decrease in performance. The speed increases slightly from 29.5 to 30.1 fps since logic AND operations consume almost no running time. One of the reasons for the drop in network performance is that background information is mixed into the embedding vectors, and the other reason is the bounding box may contain other objects that do not belong here due to the overlap among objects, resulting in inaccurate object association. The difference in performance degradation between cars and pedestrians shows that segmentation mask is more effective in tracking irregular objects with large deformation, which is consistent with the conclusion in PointTrack.

SECTION V.Conclusion
In this article, a real-time MOTS network called YolTrack is presented for environment perception and recognition of autonomous vehicles. The network uses a one-stage instance segmentation lightweight model as the backbone, and the bounding boxes and masks provided by which are fed into the tracking decoder to generate embedding vectors that can associate the same object cross the video sequence. An optimized geometric mean loss for MOTS is proposed to balance the inconsistency of gradient descent among multiple tasks during training. The network is evaluated on the main MOTS data sets, and the results show that YolTrack has top-speed performance among the existing methods. Moreover, the comparison experiment proves that the foreground extraction by logic AND operation between segmentation mask and feature map has a significant effect on tracking performance improvement. In the future, we will explore methods to improve the network accuracy without speed drop and test the network on more data sets to adapt to complex traffic scenarios. The adaptive method to learn task weight is also considered in the next step so that the network can adjust parameters online according to the actual scenarios to achieve the best performance.
