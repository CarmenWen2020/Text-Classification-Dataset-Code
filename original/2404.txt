Due to object detection's close relationship with video analysis and image understanding, it has attracted much research attention in recent years. Traditional object detection methods are built on handcrafted features and shallow trainable architectures. Their performance easily stagnates by constructing complex ensembles that combine multiple low-level image features with high-level context from object detectors and scene classifiers. With the rapid development in deep learning, more powerful tools, which are able to learn semantic, high-level, deeper features, are introduced to address the problems existing in traditional architectures. These models behave differently in network architecture, training strategy, and optimization function. In this paper, we provide a review of deep learning-based object detection frameworks. Our review begins with a brief introduction on the history of deep learning and its representative tool, namely, the convolutional neural network. Then, we focus on typical generic object detection architectures along with some modifications and useful tricks to improve detection performance further. As distinct specific detection tasks exhibit different characteristics, we also briefly survey several specific tasks, including salient object detection, face detection, and pedestrian detection. Experimental analyses are also provided to compare various methods and draw some meaningful conclusions. Finally, several promising directions and tasks are provided to serve as guidelines for future work in both object detection and relevant neural network-based learning systems.

SECTION I.Introduction
To gain a complete image understanding, we should not only concentrate on classifying different images but also try to precisely estimate the concepts and locations of objects contained in each image. This task is referred as object detection [1], [S1], which usually consists of different subtasks such as face detection [2], [S2], pedestrian detection [3], [S2], and skeleton detection [4], [S3]. As one of the fundamental computer vision problems, object detection is able to provide valuable information for semantic understanding of images and videos and is related to many applications, including image classification [5], [6], human behavior analysis [7], [S4], face recognition [8], [S5], and autonomous driving [9], [10]. Meanwhile, inheriting from neural networks and related learning systems, the progress in these fields will develop neural network algorithms and will also have great impacts on object detection techniques that can be considered as learning systems [11]–[12][13][14], [S6]. However, due to large variations in viewpoints, poses, occlusions, and lighting conditions, it is difficult to perfectly accomplish object detection with an additional object localization task. Therefore, much attention has been attracted to this field in recent years [15]–[16][17][18].

The problem definition of object detection is to determine where objects are located in a given image (object localization) and which category each object belongs to (object classification). Therefore, the pipeline of traditional object detection models can be mainly divided into three stages: informative region selection, feature extraction, and classification.

A. Informative Region Selection
As different objects may appear in any positions of the image and have different aspect ratios or sizes, it is a natural choice to scan the whole image with a multiscale sliding window. Although this exhaustive strategy can find out all possible positions of the objects, its shortcomings are also obvious. Due to a large number of candidate windows, it is computationally expensive and produces too many redundant windows. However, if only a fixed number of sliding window templates is applied, unsatisfactory regions may be produced.

B. Feature Extraction
To recognize different objects, we need to extract visual features that can provide a semantic and robust representation. Scale-invariant feature transform [19], histograms of oriented gradients (HOG) [20], and Haar-like [21] features are the representative ones. This is due to the fact that these features can produce representations associated with complex cells in human brain [19]. However, due to the diversity of appearances, illumination conditions, and backgrounds, it is difficult to manually design a robust feature descriptor to perfectly describe all kinds of objects.

C. Classification
Besides, a classifier is needed to distinguish a target object from all the other categories and to make the representations more hierarchical, semantic, and informative for visual recognition. Usually, the supported vector machine (SVM) [22], AdaBoost [23], and deformable part-based model (DPM) [24] are good choices. Among these classifiers, the DPM is a flexible model by combining object parts with deformation cost to handle severe deformations. In DPM, with the aid of a graphical model, carefully designed low-level features and kinematically inspired part decompositions are combined. Discriminative learning of graphical models allows for building high-precision part-based models for a variety of object classes.

Based on these discriminant local feature descriptors and shallow learnable architectures, state-of-the-art results have been obtained on PASCAL visual object classes (VOC) object detection competition [25] and real-time embedded systems have been obtained with a low burden on hardware. However, small gains are obtained during 2010–2012 by only building ensemble systems and employing minor variants of successful methods [15]. This fact is due to the following reasons: 1) the generation of candidate bounding boxes (BBs) with a sliding window strategy is redundant, inefficient, and inaccurate and 2) the semantic gap cannot be bridged by the combination of manually engineered low-level descriptors and discriminatively trained shallow models.

Thanks to the emergency of deep neural networks (DNNs) [6], [26], [S7], a more significant gain is obtained with the introduction of regions with convolutional neural network (CNN) features (R-CNN) [15]. DNNs, or the most representative CNNs, act in a quite different way from traditional approaches. They have deeper architectures with the capacity to learn more complex features than the shallow ones. Also, the expressivity and robust training algorithms allow to learn informative object representations without the need to design features manually [27].

Since the proposal of R-CNN, a great deal of improved models have been suggested, including fast R-CNN that jointly optimizes classification and bounding box regression tasks [16], faster R-CNN that takes an additional subnetwork to generate region proposals [17], and you only look once (YOLO) that accomplishes object detection via a fixed-grid regression [18]. All of them bring different degrees of detection performance improvements over the primary R-CNN and make real-time and accurate object detection more achievable.

In this paper, a systematic review is provided to summarize representative models and their different characteristics in several application domains, including generic object detection [15]–[16][17], salient object detection [28], [29], face detection [30]–[31][32], and pedestrian detection [33], [34]. Their relationships are depicted in Fig. 1. Based on basic CNN architectures, the generic object detection is achieved with bounding box regression, while salient object detection is accomplished with local contrast enhancement and pixel-level segmentation. Face detection and pedestrian detection are closely related to generic object detection and mainly accomplished with multiscale adaption and multifeature fusion/boosting forest, respectively. The dotted lines indicate that the corresponding domains are associated with each other under certain conditions. It should be noticed that the covered domains are diversified. Pedestrian and face images have regular structures, while general objects and scene images have more complex variations in geometric structures and layouts. Therefore, different deep models are required by various images.


Fig. 1.
Application domains of object detection.

Show All

There has been a relevant pioneer effort [35] which mainly focuses on relevant software tools to implement deep learning techniques for image classification and object detection but pays little attention on detailing specific algorithms. Different from it, our work not only reviews deep learning-based object detection models and algorithms covering different application domains in detail but also provides their corresponding experimental comparisons and meaningful analyses.

The rest of this paper is organized as follows. In Section II, a brief introduction on the history of deep learning and the basic architecture of CNN is provided. Generic object detection architectures are presented in Section III. Then, reviews of CNN applied in several specific tasks, including salient object detection, face detection, and pedestrian detection, are exhibited in Section IV–VI, respectively. Several promising future directions are proposed in Section VII. At last, some concluding remarks are presented in Section VIII.

SECTION II.Brief Overview of Deep Learning
Prior to an overview on deep learning-based object detection approaches, we provide a review on the history of deep learning along with an introduction on the basic architecture and advantages of CNN.

A. History: Birth, Decline, and Prosperity
Deep models can be referred to as neural networks with deep structures. The history of neural networks can date back to the 1940s [36], and the original intention was to simulate the human brain system to solve general learning problems in a principled way. It was popular in the 1980s and 1990s with the proposal of the back-propagation algorithm by Rumelhart et al. [37]. However, due to the overfitting of training, lack of large-scale training data, limited computation power, and insignificance in performance compared with other machine learning tools, neural networks fell out of fashion in the early 2000s.

Deep learning has become popular since 2006 [26], [S7], with a breakthrough in speech recognition [38]. The recovery of deep learning can be attributed to the following factors.

The emergence of large-scale annotated training data, such as ImageNet [39], to fully exhibit its very large learning capacity.

Fast development of high-performance parallel computing systems, such as GPU clusters.

Significant advances in the design of network structures and training strategies. With unsupervised and layerwise pretraining guided by autoencoder [40] or restricted Boltzmann machine [41], a good initialization is provided. With dropout and data augmentation, the overfitting problem in training has been relieved [6], [42]. With batch normalization (BN), the training of very DNNs becomes quite efficient [43]. Meanwhile, various network structures, such as AlexNet [6], Overfeat [44], GoogLeNet [45], Visual Geometry Group (VGG) [46], and Residual Net (ResNet) [47], have been extensively studied to improve the performance.

What prompts deep learning to have a huge impact on the entire academic community? It may owe to the contribution of Hinton’s group, whose continuous efforts have demonstrated that deep learning would bring a revolutionary breakthrough on grand challenges rather than just obvious improvements on small data sets. Their success results from training a large CNN on 1.2 million labeled images together with a few techniques [6] [e.g., rectified linear unit (ReLU) operation [48] and “dropout” regularization].

B. Architecture and Advantages of CNN
CNN is the most representative model of deep learning [27]. A typical CNN architecture, which is referred to as VGG16, can be found in Fig. S1 in the supplementary material. Each layer of CNN is known as a feature map. The feature map of the input layer is a 3-D matrix of pixel intensities for different color channels (e.g., RGB). The feature map of any internal layer is an induced multichannel image, whose “pixel” can be viewed as a specific feature. Every neuron is connected with a small portion of adjacent neurons from the previous layer (receptive field). Different types of transformations [6], [49], [50] can be conducted on feature maps, such as filtering and pooling. Filtering (convolution) operation convolutes a filter matrix (learned weights) with the values of a receptive field of neurons and takes a nonlinear function (such as sigmoid [51], ReLU) to obtain final responses. Pooling operation, such as max pooling, average pooling, L2-pooling, and local contrast normalization [52], summarizes the responses of a receptive field into one value to produce more robust feature descriptions.

With an interleave between convolution and pooling, an initial feature hierarchy is constructed, which can be fine-tuned in a supervised manner by adding several fully connected (FC) layers to adapt to different visual tasks. According to the tasks involved, the final layer with different activation functions [6] is added to get a specific conditional probability for each output neuron. The whole network can be optimized on an objective function (e.g., mean squared error or cross-entropy loss) via the stochastic gradient descent (SGD) method. The typical VGG16 has totally 13 convolutional (conv) layers, 3 FC layers, 3 max-pooling layers, and a softmax classification layer. The conv feature maps are produced by convoluting 3*3 filter windows, and feature map resolutions are reduced with 2 stride max-pooling layers. An arbitrary test image of the same size as training samples can be processed with the trained network. Rescaling or cropping operations may be needed if different sizes are provided [6].

The advantages of CNN against traditional methods can be summarized as follows.

Hierarchical feature representation, which is the multilevel representations from pixel to high-level semantic features learned by a hierarchical multistage structure [15], [53], can be learned from data automatically and hidden factors of input data can be disentangled through multilevel nonlinear mappings.

Compared with traditional shallow models, a deeper architecture provides an exponentially increased expressive capability.

The architecture of CNN provides an opportunity to jointly optimize several related tasks together (e.g., fast R-CNN combines classification and bounding box regression into a multitask learning manner).

Benefitting from the large learning capacity of deep CNNs, some classical computer vision challenges can be recast as high-dimensional data transform problems and solved from a different viewpoint.

Due to these advantages, CNN has been widely applied into many research fields, such as image superresolution reconstruction [54], [55], image classification [5], [56], image retrieval [57], [58], face recognition [8], [S5], pedestrian detection [59]–[60][61], and video analysis [62], [63].

SECTION III.Generic Object Detection
Generic object detection aims at locating and classifying existing objects in any one image and labeling them with rectangular BBs to show the confidences of existence. The frameworks of generic object detection methods can mainly be categorized into two types (see Fig. 2). One follows the traditional object detection pipeline, generating region proposals at first and then classifying each proposal into different object categories. The other regards object detection as a regression or classification problem, adopting a unified framework to achieve final results (categories and locations) directly. The region proposal-based methods mainly include R-CNN [15], spatial pyramid pooling (SPP)-net [64], Fast R-CNN [16], Faster R-CNN [17], region-based fully convolutional network (R-FCN) [65], feature pyramid networks (FPN) [66], and Mask R-CNN [67], some of which are correlated with each other (e.g., SPP-net modifies R-CNN with an SPP layer). The regression/classification-based methods mainly include MultiBox [68], AttentionNet [69], G-CNN [70], YOLO [18], Single Shot MultiBox Detector (SSD) [71], YOLOv2 [72], deconvolutional single shot detector (DSSD) [73], and deeply supervised object detectors (DSOD) [74]. The correlations between these two pipelines are bridged by the anchors introduced in Faster R-CNN. Details of these methods are as follows.


Fig. 2.
Two types of frameworks: region proposal based and regression/classification based. SPP: spatial pyramid pooling [64], FRCN: faster R-CNN [16], RPN: region proposal network [17], FCN: fully convolutional network [65], BN: batch normalization [43], and Deconv layers: deconvolution layers [54]

Show All

A. Region Proposal-Based Framework
The region proposal-based framework, a two-step process, matches the attentional mechanism of the human brain to some extent, which gives a coarse scan of the whole scenario first and then focuses on regions of interest (RoIs). Among the prerelated works [44], [75], [76], the most representative one is Overfeat [44]. This model inserts CNN into the sliding window method, which predicts BBs directly from locations of the topmost feature map after obtaining the confidences of underlying object categories.

1) R-CNN:
It is of significance to improve the quality of candidate BBs and to take a deep architecture to extract high-level features. To solve these problems, R-CNN was proposed by Girshick et al. [15] and obtained a mean average precision (mAP) of 53.3% with more than 30% improvement over the previous best result (DPM histograms of sparse codes [77]) on PASCAL VOC 2012. Fig. 3 shows the flowchart of R-CNN, which can be divided into three stages as follows.


Fig. 3.
Flowchart of R-CNN [15], which consists of three stages: 1) extracts BU region proposals, 2) computes features for each proposal using a CNN, and then 3) classifies each region with class-specific linear SVMs.

Show All

a) Region Proposal Generation:
The R-CNN adopts selective search [78] to generate about 2000 region proposals for each image. The selective search method relies on simple bottom-up (BU) grouping and saliency cues to provide more accurate candidate boxes of arbitrary sizes quickly and to reduce the searching space in object detection [24], [39].

b) CNN-Based Deep Feature Extraction:
In this stage, each region proposal is warped or cropped into a fixed resolution, and the CNN module in [6] is utilized to extract a 4096-dimensional feature as the final representation. Due to large learning capacity, dominant expressive power, and hierarchical structure of CNNs, a high-level, semantic, and robust feature representation for each region proposal can be obtained.

c) Classification and Localization:
With pretrained category-specific linear SVMs for multiple classes, different region proposals are scored on a set of positive regions and background (negative) regions. The scored regions are then adjusted with bounding box regression and filtered with a greedy nonmaximum suppression (NMS) to produce final BBs for preserved object locations.

When there are scarce or insufficient labeled data, pretraining is usually conducted. Instead of unsupervised pretraining [79], R-CNN first conducts supervised pretraining on ImageNet Large-Scale Visual Recognition Competition, a very large auxiliary data set, and then takes a domain-specific fine-tuning. This scheme has been adopted by most of the subsequent approaches [16], [17].

In spite of its improvements over traditional methods and significance in bringing CNN into practical object detection, there are still some disadvantages.

Due to the existence of FC layers, the CNN requires a fixed size (e.g., 227×227 ) input image, which directly leads to the recomputation of the whole CNN for each evaluated region, taking a great deal of time in the testing period.

Training of R-CNN is a multistage pipeline. At first, a convolutional network (ConvNet) on object proposals is fine-tuned. Then, the softmax classifier learned by fine-tuning is replaced by SVMs to fit in with ConvNet features. Finally, bounding-box regressors are trained.

Training is expensive in space and time. Features are extracted from different region proposals and stored on the disk. It will take a long time to process a relatively small training set with very deep networks, such as VGG16. At the same time, the storage memory required by these features should also be a matter of concern.

Although selective search can generate region proposals with relatively high recalls, the obtained region proposals are still redundant and this procedure is time-consuming (around 2 s to extract 2000 region proposals).

To solve these problems, many methods have been proposed. Geodesic object proposals [80] takes a much faster geodesic-based segmentation to replace traditional graph cuts. Mutiscale combinatorial grouping [81] searches different scales of the image for multiple hierarchical segmentations and combinatorially groups different regions to produce proposals. Instead of extracting visually distinct segments, the edge boxes method [82] adopts the idea that objects are more likely to exist in BBs with fewer contours straggling their boundaries. Also, some studies tried to rerank or refine preextracted region proposals to remove unnecessary ones and obtained a limited number of valuable ones, such as DeepBox [83] and SharpMask [84].

In addition, there are some improvements to solve the problem of inaccurate localization. Zhang et al. [85] utilized a Bayesian optimization-based search algorithm to guide the regressions of different BBs sequentially and trained class-specific CNN classifiers with a structured loss to penalize the localization inaccuracy explicitly. Gupta et al. [86] improved object detection for RGB-D images with semantically rich image and depth features and learned a new geocentric embedding for depth images to encode each pixel. The combination of object detectors and superpixel classification framework gains a promising result on the semantic scene segmentation task. Ouyang et al. [87] proposed a deformable deep CNN (DeepID-Net) that introduces a novel deformation constrained pooling (def-pooling) layer to impose geometric penalty on the deformation of various object parts and makes an ensemble of models with different settings. Lenc and Vedaldi [88] provided an analysis on the role of proposal generation in CNN-based detectors and tried to replace this stage with a constant and trivial region generation scheme. The goal is achieved by biasing sampling to match the statistics of the ground truth BBs with K -means clustering. However, more candidate boxes are required to achieve comparable results to those of R-CNN.

2) SPP-Net:
FC layers must take a fixed-size input. That is why R-CNN chooses to warp or crop each region proposal into the same size. However, the object may exist partly in the cropped region and unwanted geometric distortion may be produced due to the warping operation. These content losses or distortions will reduce recognition accuracy, especially when the scales of objects vary.

To solve this problem, He et al. [64] took the theory of spatial pyramid matching (SPM) [89], [90] into consideration and proposed a novel CNN architecture named SPP-net. SPM takes several finer to coarser scales to partition the image into a number of divisions and aggregates quantized local features into mid-level representations.

The architecture of SPP-net for object detection can be found in Fig. 4. Different from R-CNN, SPP-net reuses feature maps of the fifth conv layer (conv5) to project region proposals of arbitrary sizes to fixed-length feature vectors. The feasibility of the reusability of these feature maps is due to the fact that the feature maps not only involve the strength of local responses but also have relationships with their spatial positions [64]. The layer after the final conv layer is referred to as the SPP layer. If the number of feature maps in conv5 is 256, taking a three-level pyramid, the final feature vector for each region proposal obtained after the SPP layer has a dimension of 256×(12+22+42)=5376 .


Fig. 4.
Architecture of SPP-net for object detection [64].

Show All

SPP-net not only gains better results with a correct estimation of different region proposals in their corresponding scales but also improves detection efficiency in the testing period with the sharing of computation cost before SPP layer among different proposals.

3) Fast R-CNN:
Although SPP-net has achieved impressive improvements in both accuracy and efficiency over R-CNN, it still has some notable drawbacks. SPP-net takes almost the same multistage pipeline as R-CNN, including feature extraction, network fine-tuning, SVM training, and bounding-box regressor fitting. Therefore, an additional expense on storage space is still required. In addition, the conv layers preceding the SPP layer cannot be updated with the fine-tuning algorithm introduced in [64]. As a result, an accuracy drop of very deep networks is unsurprising. To this end, Girshick [16] introduced a multitask loss on classification and bounding box regression and proposed a novel CNN architecture named Fast R-CNN.

The architecture of Fast R-CNN is exhibited in Fig. 5. Similar to SPP-net, the whole image is processed with conv layers to produce feature maps. Then, a fixed-length feature vector is extracted from each region proposal with an RoI pooling layer. The RoI pooling layer is a special case of the SPP layer, which has only one pyramid level. Each feature vector is then fed into a sequence of FC layers before finally branching into two sibling output layers. One output layer is responsible for producing softmax probabilities for all C+1 categories (C object classes plus one “background” class) and the other output layer encodes refined bounding-box positions with four real-valued numbers. All parameters in these procedures (except the generation of region proposals) are optimized via a multitask loss in an end-to-end way.


Fig. 5.
Architecture of Fast R-CNN [16].

Show All

The multitasks loss L is defined in the following to jointly train classification and bounding-box regression:
L(p,u,tu,v)=Lcls(p,u)+λ[u≥1]Lloc(tu,v)(1)
View Sourcewhere Lcls(p,u)=−logpu calculates the log loss for ground truth class u , and pu is driven from the discrete probability distribution p=(p0,⋯,pC) over the C+1 outputs from the last FC layer. Lloc(tu,v) is defined over the predicted offsets tu=(tux,tuy,tuw,tuh) and ground-truth bounding-box regression targets v=(vx,vy,vw,vh) , where x,y,w , and h denote the two coordinates of the box center, width, and height, respectively. Each tu adopts the parameter settings in [15] to specify an object proposal with a log-space height/width shift and scale-invariant translation. The Iverson bracket indicator function [u≥1] is employed to omit all background RoIs. To provide more robustness against outliers and eliminate the sensitivity in exploding gradients, a smooth L1 loss is adopted to fit bounding-box regressors as follows:
Lloc(tu,v)=∑i∈x,y,w,hsmoothL1(tui−vi)(2)
View Sourcewhere
smoothL1(x)={0.5x2|x|−0.5if |x|<1otherwise.(3)
View Source

To accelerate the pipeline of Fast R-CNN, another two tricks are of necessity. On the one hand, if training samples (i.e., RoIs) come from different images, backpropagation through the SPP layer becomes highly inefficient. Fast R-CNN samples minibatches hierarchically, namely, N images sampled randomly at first and then R/N RoIs sampled in each image, where R represents the number of RoIs. Critically, computation and memory are shared by RoIs from the same image in the forward and backward pass. On the other hand, much time is spent in computing the FC layers during the forward pass [16]. The truncated singular value decomposition (SVD) [91] can be utilized to compress large FC layers and to accelerate the testing procedure.

In the Fast R-CNN, regardless of region proposal generation, the training of all network layers can be processed in a single stage with a multitask loss. It saves the additional expense on storage space and improves both accuracy and efficiency with more reasonable training schemes.

4) Faster R-CNN:
Despite the attempt to generate candidate boxes with biased sampling [88], state-of-the-art object detection networks mainly rely on additional methods, such as selective search and Edgebox, to generate a candidate pool of isolated region proposals. Region proposal computation is also a bottleneck in improving efficiency. To solve this problem, Ren et al. [17], [92] introduced an additional region proposal network (RPN), which acts in a nearly cost-free way by sharing full-image conv features with detection network.

RPN is achieved with an FCN, which has the ability to predict object bounds and scores at each position simultaneously. Similar to [78], RPN takes an image of arbitrary size to generate a set of rectangular object proposals. RPN operates on a specific conv layer with the preceding layers shared with the object detection network.

The architecture of RPN is shown in Fig. 6. The network slides over the conv feature map and fully connects to an n×n spatial window. A low-dimensional vector (512-dimensional for VGG16) is obtained in each sliding window and fed into two sibling FC layers, namely, box-classification layer (cls) and box-regression layer (reg). This architecture is implemented with an n×n conv layer followed by two sibling 1×1 conv layers. To increase nonlinearity, ReLU is applied to the output of the n×n conv layer.


Fig. 6.
RPN in Faster R-CNN [17]. K predefined anchor boxes are convoluted with each sliding window to produce fixed-length vectors which are taken by cls and reg layer to obtain corresponding outputs.

Show All

The regressions toward true BBs are achieved by comparing proposals relative to reference boxes (anchors). In the Faster R-CNN, anchors of three scales and three aspect ratios are adopted. The loss function is similar to (1)
L(pi,ti)=1Ncls∑iLcls(pi,p∗i)+λ1Nreg∑ip∗iLreg(ti,t∗i)(4)
View Sourcewhere pi is the predicted probability of the i th anchor being an object. The ground truth label p∗i is 1 if the anchor is positive, otherwise 0. ti stores four parameterized coordinates of the predicted bounding box while t∗i is related to the ground-truth box overlapping with a positive anchor. Lcls is a binary log loss and Lreg is a smoothed L1 loss similar to (2). These two terms are normalized with the minibatch size (Ncls ) and the number of anchor locations (Nreg ), respectively. In the form of FCNs, Faster R-CNN can be trained end-to-end by backpropagation and SGD in an alternate training manner.

With the proposal of Faster R-CNN, region proposal-based CNN architectures for object detection can really be trained in an end-to-end way. Also, a frame rate of 5 frames per second (fps) on a GPU is achieved with the state-of-the-art object detection accuracy on PASCAL VOC 2007 and 2012. However, the alternate training algorithm is very time-consuming and RPN produces objectlike regions (including backgrounds) instead of object instances and is not skilled in dealing with objects with extreme scales or shapes.

5) R-FCN:
Divided by the RoI pooling layer, a prevalent family [16], [17] of deep networks for object detection is composed of two subnetworks: a shared fully convolutional subnetwork (independent of RoIs) and an unshared RoI-wise subnetwork. This decomposition originates from pioneering classification architectures (e.g., AlexNet [6] and VGG16 [46]) which consist of a convolutional subnetwork and several FC layers separated by a specific spatial pooling layer.

Recent state-of-the-art image classification networks, such as ResNets [47] and GoogLeNets [45], [93], are fully convolutional. To adapt to these architectures, it is natural to construct a fully convolutional object detection network without RoI-wise subnetwork. However, it turns out to be inferior with such a naive solution [47]. This inconsistency is due to the dilemma of respecting translation variance in object detection compared with increasing translation invariance in image classification. In other words, shifting an object inside an image should be indiscriminative in image classification while any translation of an object in a bounding box may be meaningful in object detection. A manual insertion of the RoI pooling layer into convolutions can break down translation invariance at the expense of additional unshared regionwise layers. Therefore, Dai et al. [65] proposed an R-FCNs (see Fig. S2 in the supplementary material).

Different from Faster R-CNN, for each category, the last conv layer of R-FCN produces a total of k2 position-sensitive score maps with a fixed grid of k×k first and a position-sensitive RoI pooling layer is then appended to aggregate the responses from these score maps. Finally, in each RoI, k2 position-sensitive scores are averaged to produce a C+1 -d vector and softmax responses across categories are computed. Another 4k2 -d conv layer is appended to obtain class-agnostic BBs.

With R-FCN, more powerful classification networks can be adopted to accomplish object detection in a fully convolutional architecture by sharing nearly all the layers, and the state-of-the-art results are obtained on both PASCAL VOC and Microsoft COCO [94] data sets at a test speed of 170 ms per image.

6) FPN:
Feature pyramids built upon image pyramids (featurized image pyramids) have been widely applied in many object detection systems to improve scale invariance [24], [64] [Fig. 7(a)]. However, training time and memory consumption increase rapidly. To this end, some techniques take only a single input scale to represent high-level semantics and increase the robustness to scale changes [Fig. 7(b)], and image pyramids are built at test time which results in an inconsistency between train/test-time inferences [16], [17]. The in-network feature hierarchy in a deep ConvNet produces feature maps of different spatial resolutions while introduces large semantic gaps caused by different depths [Fig. 7(c)]. To avoid using low-level features, pioneer works [71], [95] usually build the pyramid starting from middle layers or just sum transformed feature responses, missing the higher resolution maps of the feature hierarchy.


Fig. 7.
Main concern of FPN [66]. (a) It is slow to use an image pyramid to build a feature pyramid. (b) Only single-scale features are adopted for faster detection. (c) Alternative to the featurized image pyramid is to reuse the pyramidal feature hierarchy computed by a ConvNet. (d) FPN integrates both (b) and (c). Blue outlines indicate feature maps and thicker outlines denote semantically stronger features.

Show All

Different from these approaches, FPN [66] holds an architecture with a BU pathway, a top-down (TD) pathway and several lateral connections to combine low-resolution and semantically strong features with high-resolution and semantically weak features [Fig. 7(d)]. The BU pathway, which is the basic forward backbone ConvNet, produces a feature hierarchy by downsampling the corresponding feature maps with a stride of 2. The layers owning the same size of output maps are grouped into the same network stage and the output of the last layer of each stage is chosen as the reference set of feature maps to build the following TD pathway.

To build the TD pathway, feature maps from higher network stages are upsampled at first and then enhanced with those of the same spatial size from the BU pathway via lateral connections. A 1×1 conv layer is appended to the upsampled map to reduce channel dimensions and the mergence is achieved by elementwise addition. Finally, a 3×3 convolution is also appended to each merged map to reduce the aliasing effect of upsampling and the final feature map is generated. This process is iterated until the finest resolution map is generated.

As feature pyramid can extract rich semantics from all levels and be trained end to end with all scales, the state-of-the-art representation can be obtained without sacrificing speed and memory. Meanwhile, FPN is independent of the backbone CNN architectures and can be applied to different stages of object detection (e.g., region proposal generation) and to many other computer vision tasks (e.g., instance segmentation).

7) Mask R-CNN:
Instance segmentation [96] is a challenging task which requires detecting all objects in an image and segmenting each instance (semantic segmentation [97]). These two tasks are usually regarded as two independent processes. The multitask scheme will create spurious edge and exhibit systematic errors on overlapping instances [98]. To solve this problem, parallel to the existing branches in Faster R-CNN for classification and bounding box regression, the Mask R-CNN [67] adds a branch to predict segmentation masks in a pixel-to-pixel manner (Fig. 8).


Fig. 8.
Mask R-CNN framework for instance segmentation [67].

Show All

Different from the other two branches that are inevitably collapsed into short output vectors by FC layers, the segmentation mask branch encodes an m×m mask to maintain the explicit object spatial layout. This kind of fully convolutional representation requires fewer parameters but is more accurate than that in [97]. Formally, besides the two losses in (1) for classification and bounding box regression, an additional loss for segmentation mask branch is defined to reach a multitask loss. This loss is only associated with ground-truth class and relies on the classification branch to predict the category.

Because RoI pooling, the core operation in Faster R-CNN, performs a coarse spatial quantization for feature extraction, misalignment is introduced between the RoI and the features. It affects classification little because of its robustness to small translations. However, it has a large negative effect on pixel-to-pixel mask prediction. To solve this problem, Mask R-CNN adopts a simple and quantization-free layer, namely, RoIAlign, to preserve the explicit per-pixel spatial correspondence faithfully. RoIAlign is achieved by replacing the harsh quantization of RoI pooling with bilinear interpolation [99], computing the exact values of the input features at four regularly sampled locations in each RoI bin. In spite of its simplicity, this seemingly minor change improves mask accuracy greatly, especially under strict localization metrics.

Given the Faster R-CNN framework, the mask branch only adds a small computational burden and its cooperation with other tasks provides complementary information for object detection. As a result, Mask R-CNN is simple to implement with promising instance segmentation and object detection results. In a word, Mask R-CNN is a flexible and efficient framework for instance-level recognition, which can be easily generalized to other tasks (e.g., human pose estimation [7], [S4]) with minimal modification.

8) Multitask Learning, Multiscale Representation, and Contextual Modeling:
Although the Faster R-CNN gets promising results with several hundred proposals, it still struggles in small-size object detection and localization, mainly due to the coarseness of its feature maps and limited information provided in particular candidate boxes. The phenomenon is more obvious on the Microsoft COCO data set which consists of objects at a broad range of scales, less prototypical images, and requires more precise localization. To tackle these problems, it is of necessity to accomplish object detection with multitask learning [100], multiscale representation [95], and context modeling [101] to combine complementary information from multiple sources.

Multitask learning learns a useful representation for multiple correlated tasks from the same input [102], [103]. Brahmbhatt et al. [100] introduced conv features trained for object segmentation and “stuff” (amorphous categories such as ground and water) to guide accurate object detection of small objects (StuffNet). Dai et al. [97] presented multitask network cascades of three networks, namely, class-agnostic region proposal generation, pixel-level instance segmentation, and regional instance classification. Li et al. [104] incorporated the weakly supervised object segmentation cues and region-based object detection into a multistage architecture to fully exploit the learned segmentation features.

Multiscale representation combines activations from multiple layers with skip-layer connections to provide semantic information of different spatial resolutions [66]. Cai et al. [105] proposed the multiscale CNN (MS-CNN) to ease the inconsistency between the sizes of objects and receptive fields with multiple scale-independent output layers. Yang et al. [34] investigated two strategies, namely, scale-dependent pooling (SDP) and layerwise cascaded rejection classifiers (CRCs), to exploit appropriate scale-dependent conv features. Kong et al. [101] proposed the HyperNet to calculate the shared features between RPN and object detection network by aggregating and compressing hierarchical feature maps from different resolutions into a uniform space.

Contextual modeling improves detection performance by exploiting features from or around RoIs of different support regions and resolutions to deal with occlusions and local similarities [95]. Zhu et al. [106] proposed the SegDeepM to exploit object segmentation which reduces the dependency on initial candidate boxes with the Markov random field. Moysset et al. [108] took advantage of four directional 2-D long short-term memories (LSTMs) [107] to convey global context between different local regions and reduced trainable parameters with local parameter sharing. Zeng et al. [109] proposed a novel gated bidirectional-net (GBD-Net) by introducing gated functions to control message transmission between different support regions.

The combination incorporates different components above into the same model to improve detection performance further. Gidaris and Komodakis [110] proposed the multiregion CNN (MR-CNN) model to capture different aspects of an object, the distinct appearances of various object parts, and semantic segmentation-aware features. To obtain contextual and multiscale representations, Bell et al. [95] proposed the inside–outside net (ION) by exploiting information both inside and outside the RoI with spatial recurrent neural networks [111] and skip pooling [101]. Zagoruyko et al. [112] proposed the MultiPath architecture by introducing three modifications to the Fast R-CNN, including multiscale skip connections [95], a modified foveal structure [110], and a novel loss function summing different intersection over union (IoU) losses.

9) Thinking in Deep Learning-Based Object Detection:
Apart from the above-mentioned approaches, there are still many important factors for continued progress.

There is a large imbalance between the number of annotated objects and background examples. To address this problem, Shrivastava et al. [113] proposed an effective online mining algorithm (OHEM) for automatic selection of the hard examples, which leads to a more effective and efficient training.

Instead of concentrating on feature extraction, Ren et al. [114] made a detailed analysis on object classifiers and found that it is of particular importance for object detection to construct a deep and convolutional per-region classifier carefully, especially for ResNets [47] and GoogLeNets [45].

Traditional CNN framework for object detection is not skilled in handling significant scale variation, occlusion, or truncation, especially when only 2-D object detection is involved. To address this problem, Xiang et al. [60] proposed a novel subcategory-aware RPN, which guides the generation of region proposals with subcategory information related to object poses and jointly optimize object detection and subcategory classification.

Ouyang et al. [115] found that the samples from different classes follow a long-tailed distribution, which indicates that different classes with distinct numbers of samples have different degrees of impacts on feature learning. To this end, objects are first clustered into visually similar class groups, and then, a hierarchical feature learning scheme is adopted to learn deep representations for each group separately.

In order to minimize the computational cost and achieve the state-of-the-art performance, with the “deep and thin” design principle and following the pipeline of Fast R-CNN, Hong et al. [116] proposed the architecture of PVANET, which adopts some building blocks including concatenated ReLU [117], Inception [45], and HyperNet [101] to reduce the expense on multiscale feature extraction and trains the network with BN [43], residual connections [47], and learning rate scheduling based on plateau detection [47]. The PVANET achieves the state-of-the-art performance and can be processed in real time on Titan X GPU (21 fps).

B. Regression/Classification-Based Framework
Region proposal-based frameworks are composed of several correlated stages, including region proposal generation, feature extraction with CNN, classification, and bounding box regression, which are usually trained separately. Even in the recent end-to-end module Faster R-CNN, an alternative training is still required to obtain shared convolution parameters between RPN and detection network. As a result, the time spent in handling different components becomes the bottleneck in the real-time application.

One-step frameworks based on global regression/ classification, mapping straightly from image pixels to bounding box coordinates and class probabilities, can reduce time expense. We first review some pioneer CNN models and then focus on two significant frameworks, namely, YOLO [18] and SSD [71].

1) Pioneer Works:
Previous to YOLO and SSD, many researchers have already tried to model object detection as a regression or classification task.

Szegedy et al. [118] formulated the object detection task as a DNN-based regression, generating a binary mask for the test image and extracting detections with a simple bounding box inference. However, the model has difficulty in handling overlapping objects, and BBs generated by direct upsampling is far from perfect.

Pinheiro et al. [119] proposed a CNN model with two branches: one generates class agnostic segmentation masks and the other predicts the likelihood of a given patch centered on an object. Inference is efficient since class scores and segmentation can be obtained in a single model with most of the CNN operations shared.

Erhan et al. [68] and Szegedy et al. [120] proposed the regression-based MultiBox to produce scored class-agnostic region proposals. A unified loss was introduced to bias both localization and confidences of multiple components to predict the coordinates of class-agnostic BBs. However, a large number of additional parameters are introduced to the final layer.

Yoo et al. [69] adopted an iterative classification approach to handle object detection and proposed an impressive end-to-end CNN architecture named AttentionNet. Starting from the top-left and bottom-right corners of an image, Attention-Net points to a target object by generating quantized weak directions and converges to an accurate object boundary box with an ensemble of iterative predictions. However, the model becomes quite inefficient when handling multiple categories with a progressive two-step procedure.

Najibi et al. [70] proposed a proposal-free iterative grid-based object detector (G-CNN), which models object detection as finding a path from a fixed grid to boxes tightly surrounding the objects [70]. Starting with a fixed multiscale bounding box grid, G-CNN trains a regressor to move and scale elements of the grid toward objects iteratively. However, G-CNN has a difficulty in dealing with small or highly overlapping objects.

2) YOLO:
Redmon et al. [18] proposed a novel framework called YOLO, which makes the use of the whole topmost feature map to predict both confidences for multiple categories and BBs. The basic idea of YOLO is exhibited in Fig. 9. YOLO divides the input image into an S × S grid and each grid cell is responsible for predicting the object centered in that grid cell. Each grid cell predicts B BBs and their corresponding confidence scores. Formally, confidence scores are defined as Pr(Object)∗IOUtruthpred , which indicates how likely there exist objects (Pr(Object)≥0 ) and shows confidences of its prediction (IOUtruthpred ). At the same time, regardless of the number of boxes, C conditional class probabilities (Pr(Classi|Object) ) should also be predicted in each grid cell. It should be noticed that only the contribution from the grid cell containing an object is calculated.


Fig. 9.
Main idea of YOLO [18].

Show All

At test time, class-specific confidence scores for each box are achieved by multiplying the individual box confidence predictions with the conditional class probabilities as follows:
Pr(Object)∗IOUtruthpred∗Pr(Classi|Object)=Pr(Classi)∗IOUtruthpred(5)
View Sourcewhere the existing probability of class-specific objects in the box and the fitness between the predicted box and the object are both taken into consideration.

During training, the following loss function is optimized:
λcoord∑i=0S2∑j=0B1objij[(xi−xi^)2+(yi−yi^)2]+λcoord∑i=0S2∑j=0B1objij[(wi−−√−wi^−−√)2+(hi−−√−hi^−−√ )2]+∑i=0S2∑j=0B1objij(Ci−Ci^)2+λnoobj∑i=0S2∑j=0B1noobjij(Ci−Ci^)2+∑i=0S21obji∑c∈classes(pi(c)−p^i(c))2.(6)
View SourceIn a certain cell i , (xi,yi) denote the center of the box relative to the bounds of the grid cell, (wi,hi) are the normalized width and height relative to the image size, Ci represents the confidence scores, 1obji indicates the existence of objects, and 1objij denotes that the prediction is conducted by the j th bounding box predictor. Note that only when an object is present in that grid cell, the loss function penalizes classification errors. Similarly, when the predictor is “responsible” for the ground truth box (i.e., the highest IoU of any predictor in that grid cell is achieved), bounding box coordinate errors are penalized.

The YOLO consists of 24 conv layers and 2 FC layers, of which some conv layers construct ensembles of inception modules with 1×1 reduction layers followed by 3×3 conv layers. The network can process images in real time at 45 fps and a simplified version Fast YOLO can reach 155 fps with better results than other real-time detectors. Furthermore, YOLO produces fewer false positives on the background, which makes the cooperation with Fast R-CNN become possible. An improved version, YOLOv2, was later proposed in [72], which adopts several impressive strategies, such as BN, anchor boxes, dimension cluster, and multiscale training.

3) SSD:
YOLO has a difficulty in dealing with small objects in groups, which is caused by strong spatial constraints imposed on bounding box predictions [18]. Meanwhile, YOLO struggles to generalize to objects in new/unusual aspect ratios/configurations and produces relatively coarse features due to multiple downsampling operations.

Aiming at these problems, Liu et al. [71] proposed an SSD, which was inspired by the anchors adopted in MultiBox [68], RPN [17], and multiscale representation [95]. Given a specific feature map, instead of fixed grids adopted in YOLO, the SSD takes the advantage of a set of default anchor boxes with different aspect ratios and scales to discretize the output space of BBs. To handle objects with various sizes, the network fuses predictions from multiple feature maps with different resolutions.

The architecture of SSD is demonstrated in Fig. 10. Given the VGG16 backbone architecture, SSD adds several feature layers to the end of the network, which are responsible for predicting the offsets to default boxes with different scales and aspect ratios and their associated confidences. The network is trained with a weighted sum of localization loss (e.g., Smooth L1) and confidence loss (e.g., Softmax), which is similar to (1). Final detection results are obtained by conducting NMS on multiscale refined BBs.


Fig. 10.
Architecture of SSD 300 [71]. SSD adds several feature layers to the end of VGG16 backbone network to predict the offsets to default anchor boxes and their associated confidences. Final detection results are obtained by conducting NMS on multiscale refined BBs.

Show All

Integrating with hard negative mining, data augmentation, and a larger number of carefully chosen default anchors, SSD significantly outperforms the Faster R-CNN in terms of accuracy on PASCAL VOC and COCO while being three times faster. The SSD300 (input image size is 300×300 ) runs at 59 fps, which is more accurate and efficient than YOLO. However, SSD is not skilled at dealing with small objects, which can be relieved by adopting better feature extractor backbone (e.g., ResNet101), adding deconvolution layers with skip connections to introduce additional large-scale context [73], and designing better network structure (e.g., stem block and dense block) [74].

C. Experimental Evaluation
We compare various object detection methods on three benchmark data sets, including PASCAL VOC 2007 [25], PASCAL VOC 2012 [121], and Microsoft COCO [94]. The evaluated approaches include R-CNN [15], SPP-net [64], Fast R-CNN [16], networks on convolutional feature maps (NOC) [114], Bayes [85], MR-CNN& S-CNN [105], Faster R-CNN [17], HyperNet [101], ION [95], MS-GR [104], StuffNet [100], SSD300 [71], SSD512 [71], OHEM [113], SDP+CRC [34], G-CNN [70], SubCNN [60], GBD-Net [109], PVANET [116], YOLO [18], YOLOv2 [72], R-FCN [65], FPN [66], Mask R-CNN [67], DSSD [73], and DSOD [74]. If no specific instructions for the adopted framework are provided, the utilized model is a VGG16 [46] pretrained on 1000-way ImageNet classification task [39]. Due to the limitation of the paper length, we only provide an overview, including proposal, learning method, loss function, programing language, and platform, of the prominent architectures in Table I. Detailed experimental settings, which can be found in the original papers, are missed. In addition to the comparisons of detection accuracy, another comparison is provided to evaluate their test consumption on PASCAL VOC 2007.

TABLE I Overview of Prominent Generic Object Detection Architectures

1) PASCAL VOC 2007/2012:
PASCAL VOC 2007 and 2012 data sets consist of 20 categories. The evaluation terms are AP in each single category and mAP across all the 20 categories. Comparative results are exhibited in Tables II and III, from which the following remarks can be obtained.

If incorporated with a proper way, more powerful backbone CNN models can definitely improve the object detection performance (the comparison among R-CNN with AlexNet, R-CNN with VGG16 and SPP-net with ZF-Net [122]).

With the introduction of the SPP layer (SPP-net), end-to-end multitask architecture (FRCN), and RPN (Faster R-CNN), object detection performance is improved gradually and apparently.

Due to a large number of trainable parameters, in order to obtain multilevel robust features, data augmentation is very important for deep learning-based models (Faster R-CNN with “07,” “07+12 ,” and “07+12+coco ”).

Apart from basic models, there are still many other factors affecting object detection performance, such as multiscale and multiregion feature extraction (e.g., MR-CNN), modified classification networks (e.g., NOC), additional information from other correlated tasks (e.g., StuffNet, HyperNet), multiscale representation (e.g., ION), and mining of hard negative samples (e.g., OHEM).

As YOLO is not skilled in producing object localizations of high IoU, it obtains a very poor result on VOC 2012. However, with the complementary information from Fast R-CNN (YOLO+FRCN) and the aid of other strategies, such as anchor boxes, BN, and fine-grained features, the localization errors are corrected (YOLOv2).

By combining many recent tricks and modeling the whole network as a fully convolutional one, R-FCN achieves a more obvious improvement of detection performance over other approaches.

TABLE II Comparative Results on VOC 2007 Test Set (%)

TABLE III Comparative Results on VOC 2012 Test Set (%)

2) Microsoft COCO:
Microsoft COCO is composed of 300 000 fully segmented images, in which each image has an average of 7 object instances from a total of 80 categories. As there are a lot of less iconic objects with a broad range of scales and a stricter requirement on object localization, this data set is more challenging than PASCAL 2012. Object detection performance is evaluated by AP computed under different degrees of IoUs and on different object sizes. The results are given in Table IV.

TABLE IV Comparative Results on Microsoft COCO Test Dev Set (%)

Besides similar remarks to those of PASCAL VOC, some other conclusions can be drawn as follows from Table IV.

Multiscale training and test are beneficial in improving object detection performance, which provide additional information in different resolutions (R-FCN). FPN and DSSD provide some better ways to build feature pyramids to achieve multiscale representation. The complementary information from other related tasks is also helpful for accurate object localization (Mask R-CNN with instance segmentation task).

Overall, region proposal-based methods, such as Faster R-CNN and R-FCN, perform better than regression/ classification-based approaches, namely, YOLO and SSD, due to the fact that quite a lot of localization errors are produced by regression/classification-based approaches.

Context modeling is helpful to locate small objects, which provides additional information by consulting nearby objects and surroundings (GBD-Net and multipath).

Due to the existence of a large number of nonstandard small objects, the results on this data set are much worse than those of VOC 2007/2012. With the introduction of other powerful frameworks (e.g., ResNeXt [123]) and useful strategies (e.g., multitask learning [67], [124]), the performance can be improved.

The success of DSOD in training from scratch stresses the importance of the network design to release the requirements for perfect pretrained classifiers on relevant tasks and a large number of annotated samples.

3) Timing Analysis:
Timing analysis (Table V) is conducted on Intel i7-6700K CPU with a single core and NVIDIA Titan X GPU. Except for “SS” which is processed with CPU, the other procedures related to CNN are all evaluated on GPU. From Table V, we can draw some conclusions as follows.

By computing CNN features on shared feature maps (SPP-net), test consumption is reduced largely. Test time is further reduced with the unified multitask learning (FRCN) and removal of additional region proposal generation stage (Faster R-CNN). It is also helpful to compress the parameters of FC layers with SVD [91] (PAVNET and FRCN).

It takes additional test time to extract multiscale features and contextual information (ION and MR-RCNN& S-RCNN).

It takes more time to train a more complex and deeper network (ResNet101 against VGG16) and this time consumption can be reduced by adding as many layers into shared fully convolutional layers as possible (FRCN).

Regression-based models can usually be processed in real time at the cost of a drop in accuracy compared with region proposal-based models. Also, region proposal-based models can be modified into real-time systems with the introduction of other tricks [116] (PVANET), such as BN [43] and residual connections [123].

TABLE V Comparison of Testing Consumption on VOC 07 Test Set

SECTION IV.Salient Object Detection
Visual saliency detection, one of the most important and challenging tasks in computer vision, aims to highlight the most dominant object regions in an image. Numerous applications incorporate the visual saliency to improve their performance, such as image cropping [125] and segmentation [126], image retrieval [57], and object detection [66].

Broadly, there are two branches of approaches in salient object detection, namely, BU [127] and TD [128]. Local feature contrast plays the central role in BU salient object detection, regardless of the semantic contents of the scene. To learn local feature contrast, various local and global features are extracted from pixels, e.g., edges [129] and spatial information [130]. However, high-level and multiscale semantic information cannot be explored with these low-level features. As a result, low-contrast salient maps instead of salient objects are obtained. TD salient object detection is task-oriented and takes prior knowledge about object categories to guide the generation of salient maps. Taking semantic segmentation as an example, a saliency map is generated in the segmentation to assign pixels to particular object categories via a TD approach [131]. In a word, TD saliency can be viewed as a focus-of-attention mechanism, which prunes BU salient points that are unlikely to be parts of the object [132].

A. Deep Learning in Salient Object Detection
Due to the significance for providing high-level and multiscale feature representation and the successful applications in many correlated computer vision tasks, such as semantic segmentation [131], edge detection [133], and generic object detection [16], it is feasible and necessary to extend CNN to salient object detection.

The early work by Vig et al. [29] follows a completely automatic data-driven approach to perform a large-scale search for optimal features, namely, an ensemble of deep networks with different layers and parameters. To address the problem of limited training data, Kummerer et al. [134] proposed the Deep Gaze by transferring from the AlexNet to generate a high-dimensional feature space and create a saliency map. A similar architecture was proposed by Huang et al. [135] to integrate saliency prediction into pretrained object recognition DNNs. The transfer is accomplished by fine-tuning DNNs’ weights with an objective function based on the saliency evaluation metrics, such as similarity, KL-divergence, and normalized scanpath saliency.

Some works combined local and global visual clues to improve salient object detection performance. Wang et al. [136] trained two independent deep CNNs (DNN-L and DNN-G) to capture local information and global contrast and predicted saliency maps by integrating both local estimation and global search. Cholakkal et al. [137] proposed a weakly supervised saliency detection framework to combine visual saliency from BU and TD saliency maps and refined the results with a multiscale superpixel-averaging. Zhao et al. [138] proposed a multicontext deep learning framework, which utilizes a unified learning framework to model global and local context jointly with the aid of superpixel segmentation. To predict saliency in videos, Bak et al. [139] fused two static saliency models, namely, spatial stream net and temporal stream net, into a two-stream framework with a novel empirically grounded data augmentation technique.

Complementary information from semantic segmentation and context modeling is beneficial. To learn internal representations of saliency efficiently, He et al. [140] proposed a novel superpixelwise CNN approach called SuperCNN, in which salient object detection is formulated as a binary labeling problem. Based on a fully CNN, Li et al. [141] proposed a multitask deep saliency model, in which intrinsic correlations between saliency detection and semantic segmentation are set up. However, due to the conv layers with large receptive fields and pooling layers, blurry object boundaries and coarse saliency maps are produced. Tang and Wu [142] proposed a novel saliency detection framework (CRPSD) [142], which combines the region-level saliency estimation and pixel-level saliency prediction together with three closely related CNNs. Li and Yu [143] proposed a deep contrast network to combine segmentwise spatial pooling and pixel-level fully convolutional streams [143].

The proper integration of multiscale feature maps is also of significance for improving detection performance. Based on Fast R-CNN, Wang et al. [144] proposed the RegionNet by performing salient object detection with end-to-end edge preserving and multiscale contextual modeling. Liu et al. [28] proposed a multiresolution CNN (Mr-CNN) to predict eye fixations, which is achieved by learning both BU visual saliency and TD visual factors from raw image data simultaneously. Cornia et al. [145] proposed an architecture that combines features extracted at different levels of the CNN. Li and Yu [146] proposed a multiscale deep CNN framework to extract three scales of deep contrast features, namely, the mean-subtracted region, the bounding box of its immediate neighboring regions, and the masked entire image, from each candidate region.

It is efficient and accurate to train a direct pixelwise CNN architecture to predict salient objects with the aids of recurrent neural networks and deconvolution networks. Pan et al. [147] formulated saliency prediction as a minimization optimization on the Euclidean distance between the predicted saliency map and the ground truth and proposed two kinds of architectures: a shallow one trained from scratch and a deeper one adapted from a deconvoluted VGG network. Asconvolutional–deconvolution networks are not expert in recognizing objects of multiple scales, Kuen et al. [148] proposed a recurrent attentional convolutional–deconvolution network with several spatial transformer and recurrent network units to conquer this problem. To fuse local, global, and contextual information of salient objects, Tang et al. [149] developed a deeply supervised recurrent CNN to perform a full image-to-image saliency detection.

B. Experimental Evaluation
Four representative data sets, including Evaluation on Complex Scene Saliency Dataset (ECSSD) [156], HKU-IS [146], PASCALS [157], and SOD [158], are used to evaluate several state-of-the-art methods. ECSSD consists of 1000 structurally complex but semantically meaningful natural images. HKU-IS is a large-scale data set containing over 4000 challenging images. Most of these images have more than one salient object and own low contrast. PASCALS is a subset chosen from the validation set of PASCAL VOC 2010 segmentation data set and is composed of 850 natural images. The SOD data set possesses 300 images containing multiple salient objects. The training and validation sets for different data sets are kept the same as those in [152].

Two standard metrics, namely, F-measure and the mean absolute error (MAE), are utilized to evaluate the quality of a saliency map. Given precision and recall values precomputed on the union of generated binary mask B and ground truth Z , F-measure is defined as follows:
Fβ=(1+β2)Presion×Recallβ2Presion+Recall(7)
View Sourcewhere β2 is set to 0.3 in order to stress the importance of the precision value.

The MAE score is computed with the following equation:
MAE=1H×W∑i=1H∑j=1W|S^(i,j)=Z^(i,j)|(8)
View Sourcewhere Z^ and S^ represent the ground truth and the continuous saliency map, respectively. W and H are the width and height of the salient area, respectively. This score stresses the importance of successfully detected salient objects over detected nonsalient pixels [159].

The following approaches are evaluated: contextual hypergraph modeling (CHM) [150], RC [151], discriminative regional feature integration (DRFI) [152], MC [138], multiscale deep CNN features (MDF) [146], local estimation and global search (LEGS) [136], DSR [149], multi-task deep neural network [141], CRPSD [142], deep contrast learning (DCL) [143], encoded low level distance (ELD) [153], nonlocal deep features (NLDF) [154], and deep supervision with short connections (DSSC) [155]. Among these methods, CHM, RC, and DRFI are classical ones with the best performance [159], while the other methods are all associated with CNN. F-measure and MAE scores are given in Table VI.

TABLE VI Comparison Between State-of-the-Art Methods

From Table VI, we can find that CNN-based methods perform better than classic methods. MC and MDF combine the information from local and global context to reach a more accurate saliency. ELD refers to low-level handcrafted features for complementary information. LEGS adopts generic region proposals to provide initial salient regions, which may be insufficient for salient detection. DSR and MT act in different ways by introducing a recurrent network and semantic segmentation, which provide insights for future improvements. CRPSD, DCL, NLDF, and DSSC are all based on multiscale representations and superpixel segmentation, which provide robust salient regions and smooth boundaries. DCL, NLDF, and DSSC perform the best on these four data sets. DSSC earns the best performance by modeling scale-to-scale short connections.

Overall, as CNN mainly provides salient information in local regions, most of the CNN-based methods need to model visual saliency along region boundaries with the aid of superpixel segmentation. Meanwhile, the extraction of multiscale deep CNN features is of significance for measuring local conspicuity. Finally, it is necessary to strengthen local connections between different CNN layers as well as to utilize complementary information from local and global context.

SECTION V.Face Detection
Face detection is essential to many face applications and acts as an important preprocessing procedure to face recognition [160]–[161][162], face synthesis [163], [164], and facial expression analysis [165]. Different from generic object detection, this task is to recognize and locate face regions covering a very large range of scales (30–300 pts versus 10–1000 pts). At the same time, faces have their unique object structural configurations (e.g., the distribution of different face parts) and characteristics (e.g., skin color). All these differences lead to special attention to this task. However, large visual variations of faces, such as occlusions, pose variations, and illumination changes, impose great challenges for this task in real applications.

The most famous face detector proposed by Viola and Jones [166] trains cascaded classifiers with Haar-like features and AdaBoost, achieving good performance with real-time efficiency. However, this detector may degrade significantly in real-world applications due to larger visual variations of human faces. Different from this cascade structure, Felzenszwalb et al. [24] proposed a deformable part model (DPM) for face detection. However, for these traditional face detection methods, high computational expenses and large quantities of annotations are required to achieve a reasonable result. In addition, their performance is greatly restricted by manually designed features and shallow architecture.

A. Deep Learning in Face Detection
Recently, some CNN-based face detection approaches have been proposed [167]–[168][169]. As less accurate localization results from independent regressions of object coordinates, Yu et al. [167] proposed a novel IoU loss function for predicting the four bounds of box jointly. Farfade et al. [168] proposed a deep dense face detector (DDFD) to conduct multiview face detection, which is able to detect faces in a wide range of orientations without the requirement of pose/landmark annotations. Yang et al. [169] proposed a novel deep learning-based face detection framework, which collects the responses from local facial parts (e.g., eyes, nose, and mouths) to address face detection under severe occlusions and unconstrained pose variations. Yang et al. [170] proposed a scale-friendly detection network named ScaleFace, which splits a large range of target scales into smaller subranges. Different specialized subnetworks are constructed on these subscales and combined into a single one to conduct end-to-end optimization. Hao et al. [171] designed an efficient CNN to predict the scale distribution histogram of the faces and took this histogram to guide the zoomed-in view and zoomed-out view of the image. Since the faces are approximately in uniform scale after zoom, compared with other state-of-the-art baselines, better performance is achieved with a less computation cost. In addition, some generic detection frameworks are extended to face detection with different modifications, e.g., Faster R-CNN [30], [172], [173].

Some authors trained CNNs with other complementary tasks, such as 3-D modeling and face landmarks, in a multitask learning manner. Huang et al. [174] proposed a unified end-to-end FCN framework called DenseBox to jointly conduct face detection and landmark localization. Li et al. [175] proposed a multitask discriminative learning framework that integrates a ConvNet with a fixed 3-D mean face model in an end-to-end manner. In the framework, two issues are addressed to transfer from generic object detection to face detection, namely, eliminating predefined anchor boxes by a 3-D mean face model and replacing RoI pooling layer with a configuration pooling layer. Zhang et al. [176] proposed a deep cascaded multitask framework named multitask cascaded convolutional networks (MTCNN) which exploits the inherent correlations between face detection and alignment in the unconstrained environment to boost up detection performance in a coarse-to-fine manner.

Reducing computational expenses is of necessity in real applications. To achieve real-time detection on the mobile platform, Kalinovskii and Spitsyn [177] proposed a new solution of frontal face detection based on compact CNN cascades. This method takes a cascade of three simple CNNs to generate, classify, and refine candidate object positions progressively. To reduce the effects of large pose variations, Chen et al. [32] proposed a cascaded CNN denoted by supervised transformer network. This network takes a multitask RPN to predict candidate face regions along with associated facial landmarks simultaneously and adopts a generic R-CNN to verify the existence of valid faces. Yang and Nevatia [8] proposed a three-stage cascade structure based on FCNs, while in each stage, a multiscale FCN is utilized to refine the positions of possible faces. Qin et al. [178] proposed a unified framework that achieves better results with the complementary information from different jointly trained CNNs.

B. Experimental Evaluation
The FDDB [179] data set has a total of 2845 pictures in which 5171 faces are annotated with an elliptical shape. Two types of evaluations are used: the discrete score and continuous score. By varying the threshold of the decision rule, the receiver operating characteristic (ROC) curve for the discrete scores can reflect the dependence of the detected face fractions on the number of false alarms. Compared with annotations, any detection with an IoU ratio exceeding 0.5 is treated as positive. Each annotation is only associated with one detection. The ROC curve for the continuous scores is the reflection of face localization quality.

The evaluated models cover DDFD [168], Cascade-CNN [180], aggregate channel features (ACF)-multiscale [181], Pico [182], Head-Hunter [183], Joint Cascade [31], SURF-multiview [184], Viola–Jones [166], NPDFace [185], Faceness [169], convolutional channel features (CCF) [186], MTCNN [176], Conv3-D [175], Hyperface [187], UnitBox [167], locally decorrelated channel features (LDCF+) [S2], DeepIR [173], hybrid-resolution model with elliptical regressor (HR-ER) [188], Face-R-CNN [172], and ScaleFace [170]. ACF-multiscale, Pico, HeadHunter, Joint Cascade, SURF-multiview, Viola-Jones, NPDFace, and LDCF+ are built on classic hand-crafted features while the rest methods are based on deep CNN features. The ROC curves are shown in Fig. 11.


Fig. 11.
ROC curves of state-of-the-art methods on FDDB. (a) Discrete ROC curves. (b) Continuous ROC curves.

Show All

In Fig. 11(a), in spite of relatively competitive results produced by LDCF+, it can be observed that most of the classic methods perform with similar results and are outperformed by CNN-based methods by a significant margin. In Fig. 11(b), it can be observed that most of the CNN-based methods earn similar true positive rates between 60% and 70% while DeepIR and HR-ER perform much better than them. Among classic methods, Joint Cascade is still competitive. As earlier works, DDFD and CCF directly make use of generated feature maps and obtain relatively poor results. CascadeCNN builds cascaded CNNs to locate face regions, which is efficient but inaccurate. Faceness combines the decisions from different part detectors, resulting in precise face localizations while being time-consuming. The outstanding performance of MTCNN, Conv3-D, and Hyperface proves the effectiveness of multitask learning. HR-ER and ScaleFace adaptively detect faces of different scales and make a balance between accuracy and efficiency. DeepIR and Face-R-CNN are two extensions of the Faster R-CNN architecture to face detection, which validate the significance and effectiveness of Faster R-CNN. Unitbox provides an alternative choice for performance improvements by carefully designing optimization loss.

From these results, we can draw the conclusion that CNN-based methods are in the leading position. The performance can be improved by the following strategies: designing novel optimization loss, modifying generic detection pipelines, building meaningful network cascades, adapting scale-aware detection, and learning multitask shared CNN features.

SECTION VI.Pedestrian Detection
Recently, pedestrian detection has been intensively studied, which has a close relationship to pedestrian tracking [189], [190], person reidentification [191], [192], and robot navigation [193], [194]. Prior to the recent progress in deep CNN (DCNN)-based methods [195], [196], some researchers combined boosted decision forests with hand-crafted features to obtain pedestrian detectors [197]–[198][199]. At the same time, to explicitly model the deformation and occlusion, part-based models [200] and explicit occlusion handling [201], [202] are of concern.

As there are many pedestrian instances of small sizes in typical scenarios of pedestrian detection (e.g., automatic driving and intelligent surveillance), the application of RoI pooling layer in generic object detection pipeline may result in “plain” features due to collapsing bins. In the meantime, the main source of false predictions in pedestrian detection is the confusion of hard background instances, which is in contrast to the interference from multiple categories in generic object detection. As a result, different configurations and components are required to accomplish accurate pedestrian detection.

A. Deep Learning in Pedestrian Detection
Although DCNNs have obtained excellent performance on generic object detection [16], [72], none of these approaches have achieved better results than the best hand-crafted feature-based method [198] for a long time, even when part-based information and occlusion handling are incorporated [202]. Thereby, some studies have been conducted to analyze the reasons. Zhang et al. [203] attempted to adapt generic Faster R-CNN [17] to pedestrian detection. They modified the downstream classifier by adding boosted forests to shared, high-resolution conv feature maps and taking an RPN to handle small instances and hard negative examples. To deal with complex occlusions existing in pedestrian images, inspired by DPM [24], Tian et al. [204] proposed a deep learning framework called DeepParts, which makes decisions based on an ensemble of extensive part detectors. DeepParts has advantages in dealing with weakly labeled data, low IoU positive proposals, and partial occlusion.

Other researchers also tried to combine complementary information from multiple data sources. CompACT-Deep adopts a complexity-aware cascade to combine hand-crafted features and fine-tuned DCNNs [195]. Based on Faster R-CNN, Liu et al. [205] proposed multispectral DNNs for pedestrian detection to combine complementary information from color and thermal images. Tian et al. [206] proposed a task-assistant CNN to jointly learn multiple tasks with multiple data sources and to combine pedestrian attributes with semantic scene attributes together. Du et al. [207] proposed a DNN fusion architecture for fast and robust pedestrian detection. Based on the candidate BBs generated with SSD detectors [71], multiple binary classifiers are processed parallelly to conduct soft-rejection-based network fusion by consulting their aggregated degree of confidences.

However, most of these approaches are much more sophisticated than the standard R-CNN framework. CompACT-Deep consists of a variety of hand-crafted features, a small CNN model, and a large VGG16 model [195]. DeepParts contains 45 fine-tuned DCNN models, and a set of strategies, including bounding box shifting handling and part selection, are required to arrive at the reported results [204]. Therefore, the modification and simplification are of significance to reduce the burden on both software and hardware to satisfy real-time detection demand. Tome et al. [59] proposed a novel solution to adapt generic object detection pipeline to pedestrian detection by optimizing most of its stages. Hu et al. [208] trained an ensemble of boosted decision models by reusing the conv feature maps, and a further improvement was gained with simple pixel labeling and additional complementary hand-crafted features. Tome et al. [209] proposed a reduced memory region-based deep CNN architecture, which fuses regional responses from both ACF detectors and SVM classifiers into R-CNN. Ribeiro et al. [33] addressed the problem of human-aware navigation and proposed a vision-based person tracking system guided by multiple camera sensors.

B. Experimental Evaluation
The evaluation is conducted on the most popular Caltech Pedestrian data set [3]. The data set was collected from the videos of a vehicle driving through an urban environment and consists of 250 000 frames with about 2300 unique pedestrians and 350 000 annotated BBs. Three kinds of labels, namely, “Person (clear identifications),” “Person? (unclear identifications),” and “People (large group of individuals),” are assigned to different BBs. The performance is measured with the log-average miss rate (L-AMR) which is computed evenly spaced in log-space in the range 10−2 to 1 by averaging miss rate at the rate of nine false positives per image [3]. According to the differences in the height and visible part of the BBs, a total of nine popular settings are adopted to evaluate different properties of these models. Details of these settings are as in [3].

Evaluated methods include Checkerboards+ [198], LDCF++ [S2], SCF+AlexNet [210], SA-FastRCNN [211], MS-CNN [105], DeepParts [204], CompACT-Deep [195], RPN+BF [203], and F-DNN+SS [207]. The first two methods are based on hand-crafted features while the rest ones rely on deep CNN features. All results are exhibited in Table VII. From this table, we observe that different from other tasks, classic handcrafted features can still earn competitive results with boosted decision forests [203], ACF [197], and HOG+LUV channels [S2]. As an early attempt to adapt CNN to pedestrian detection, the features generated by SCF+AlexNet are not so discriminant and produce relatively poor results. Based on multiple CNNs, DeepParts and CompACT-Deep accomplish detection tasks via different strategies, namely, local part integration and cascade network. The responses from different local part detectors make DeepParts robust to partial occlusions. However, due to complexity, it is too time-consuming to achieve real-time detection. The multiscale representation of MS-CNN improves the accuracy of pedestrian locations. SA-FastRCNN extends Fast R-CNN to automatically detect pedestrians according to their different scales, which has trouble when there are partial occlusions. RPN+BF combines the detectors produced by Faster R-CNN with boosting decision forest to accurately locate different pedestrians. F-DNN+SS, which is composed of multiple parallel classifiers with soft rejections, performs the best followed by RPN+BF, SA-FastRCNN, and MS-CNN.

TABLE VII Detailed Breakdown Performance Comparisons of State-of-the-Art Models on Caltech Pedestrian Data Set. All Numbers are Reported in L-AMR

In short, CNN-based methods can provide more accurate candidate boxes and multilevel semantic information for identifying and locating pedestrians. Meanwhile, handcrafted features are complementary and can be combined with CNN to achieve better results. The improvements over existing CNN methods can be obtained by carefully designing the framework and classifiers, extracting multiscale and part-based semantic information and searching for complementary information from other related tasks, such as segmentation.

SECTION VII.Promising Future Directions and Tasks
In spite of rapid development and achieved promising progress of object detection, there are still many open issues for the future work.

The first one is small object detection such as occurring in COCO data set and in face detection task. To improve localization accuracy on small objects under partial occlusions, it is necessary to modify network architectures from the following aspects.

Multitask Joint Optimization and Multimodal Information Fusion: Due to the correlations between different tasks within and outside object detection, multitask joint optimization has already been studied by many researchers [16], [17]. However, apart from the tasks mentioned in Section III-A8, it is desirable to think over the characteristics of different subtasks of object detection (e.g., superpixel semantic segmentation in salient object detection) and extend multitask optimization to other applications such as instance segmentation [66], multiobject tracking [202], and multiperson pose estimation [S4]. In addition, given a specific application, the information from different modalities, such as text [212], thermal data [205], and images [65], can be fused together to achieve a more discriminant network.

Scale Adaption: Objects usually exist in different scales, which are more apparent in face detection and pedestrian detection. To increase the robustness to scale changes, it is demanded to train scale-invariant, multiscale or scale-adaptive detectors. For scale-invariant detectors, more powerful backbone architectures (e.g., ResNext [123]), negative sample mining [113], reverse connection [213], and subcategory modeling [60] are all beneficial. For multiscale detectors, both the FPN [66] that produces multiscale feature maps and the generative adversarial network [214] that narrows representation differences between small objects and the large ones with a low-cost architecture provide insights into generating meaningful feature pyramid. For scale-adaptive detectors, it is useful to combine knowledge graph [215], attentional mechanism [216], cascade network [180], and scale distribution estimation [171] to detect objects adaptively.

Spatial Correlations and Contextual Modeling: Spatial distribution plays an important role in object detection. Therefore, region proposal generation and grid regression are taken to obtain probable object locations. However, the correlations between multiple proposals and object categories are ignored. In addition, the global structure information is abandoned by the position-sensitive score maps in R-FCN. To solve these problems, we can refer to diverse subset selection [217] and sequential reasoning tasks [218] for possible solutions. It is also meaningful to mask salient parts and couple them with the global structure in a joint-learning manner [219].

The second one is to release the burden on manual labor and accomplish real-time object detection, with the emergence of the large-scale image and video data. The following three aspects can be taken into account.

Cascade Network: In a cascade network, a cascade of detectors is built in different stages or layers [180], [220]. Easily distinguishable examples are rejected at shallow layers so that features and classifiers at later stages can handle more difficult samples with the aid of the decisions from previous stages. However, current cascades are built in a greedy manner, where previous stages in cascade are fixed when training a new stage. Therefore, the optimizations of different CNNs are isolated, which stresses the necessity of end-to-end optimization for CNN cascade. At the same time, it is also a matter of concern to build contextual associated cascade networks with existing layers.

Unsupervised and Weakly Supervised Learning: It is very time-consuming to manually draw large quantities of BBs. To release this burden, semantic prior [55], unsupervised object discovery [221], multiple instance learning [222], and DNN prediction [47] can be integrated to make the best use of image-level supervision to assign object category tags to corresponding object regions and refine object boundaries. Furthermore, weakly annotations (e.g., center-click annotations [223]) are also helpful for achieving high-quality detectors with modest annotation efforts, especially aided by the mobile platform.

Network Optimization: Given specific applications and platforms, it is significant to make a balance among speed, memory, and accuracy by selecting an optimal detection architecture [116], [224]. However, despite that detection accuracy is reduced, it is more meaningful to learn compact models with a fewer number of parameters [209]. This situation can be relieved by introducing better pretraining schemes [225], knowledge distillation [226], and hint learning [227]. DSOD also provides a promising guideline to train from scratch to bridge the gap between different image sources and tasks [74].

The third one is to extend typical methods for 2-D object detection to adapt 3-D object detection and video object detection, with the requirements from autonomous driving, intelligent transportation, and intelligent surveillance.

3-D Object Detection: With the applications of 3-D sensors (e.g., Light Detection and Ranging and camera), additional depth information can be utilized to better understand the images in 2-D and extend the image-level knowledge to the real world. However, seldom of these 3-D-aware techniques aim to place correct 3-D BBs around detected objects. To achieve better bounding results, multiview representation [181] and 3-D proposal network [228] may provide some guidelines to encode depth information with the aid of inertial sensors (accelerometer and gyrometer) [229].

Video Object Detection: Temporal information across different frames plays an important role in understanding the behaviors of different objects. However, the accuracy suffers from degenerated object appearances (e.g., motion blur and video defocus) in videos and the network is usually not trained end to end. To this end, spatiotemporal tubelets [230], optical flow [199], and LSTM [107] should be considered to fundamentally model object associations between consecutive frames.

SECTION VIII.Conclusion
Due to its powerful learning ability and advantages in dealing with occlusion, scale transformation, and background switches, deep learning-based object detection has been a research hotspot in recent years. This paper provides a detailed review on deep learning-based object detection frameworks that handle different subproblems, such as occlusion, clutter, and low resolution, with different degrees of modifications on R-CNN. The review starts on generic object detection pipelines which provide base architectures for other related tasks. Then, three other common tasks, namely, salient object detection, face detection, and pedestrian detection, are also briefly reviewed. Finally, we propose several promising future directions to gain a thorough understanding of the object detection landscape. This review is also meaningful for the developments in neural networks and related learning systems, which provides valuable insights and guidelines for future progress.
