Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ 1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.

SECTION I.Introduction
During the past years, extreme learning machine (ELM) [1] has been becoming an increasingly significant research topic for machine learning and artificial intelligence, due to its unique characteristics, i.e., extremely fast training, good generalization, and universal approximation/classification capability. ELM is an effective solution for the single hidden layer feedforward networks (SLFNs), and has been demonstrated to have excellent learning accuracy/speed in various applications, such as face classification [2], image segmentation [3], and human action recognition [4].

Unlike the other traditional learning algorithms, e.g., back propagation (BP)-based neural networks (NNs), or support vector machine (SVM), the parameters of hidden layers of the ELM are randomly generated and need not to be tuned, thus the hidden nodes could be established before the training samples are acquired. Theoretically, Huang et al. [5], [6] have proved that the SLFNs with randomly generated hidden neurons and the output weights tuned by regularized least square maintain its universal approximation capability, even without updating the parameters of hidden layers. In addition, solving the regularized least squares problem in ELM is faster than that of the quadratic programming problem in standard SVM or gradient method in traditional BP-based NNs. Thus, ELM tends to achieve faster and better generalization performance than those of NNs and SVM [1], [5], [7].

Recently, ELM has been extensively studied, and remarkable contributions have been made both in theories and applications. The universal capability of ELM has been extended into kernel learning, and it turns out that the ELM is suitable for a wide type of feature mappings, rather than the classical ones [1]. An incremental ELM was proposed in [5], where the hidden nodes are added incrementally and the output weights are determined analytically. ELM has also been transformed in an online sequential version [8], and the original data can be input one-by-one or chunk-by-chunk (a block of data) with fixed or varying chunk size. Huang et al. [9] extend the ELM for both semisupervised and unsupervised tasks based on the manifold regularization, and the unlabeled or partially labeled samples are clustered using ELM.

However, the aforementioned works still face some issues when dealing with natural scenes (e.g., visual and acoustic signals), or practical applications (e.g., image classification, voice recognition, and so on). That is, the original ELM and/or its variants mainly focus on classification. However, feature learning is often required before classification is conducted in many applications. Thus, multilayer solutions are usually needed. Kasun et al. [10] attempt to develop a multilayer learning architecture using ELM-based autoencoder as its building block. The original inputs are decomposed into multiple hidden layers, and the outputs of the previous layer are used as the inputs of the current one. The autoencoder in [10] is simply stacked layer by layer in the hierarchical structure. Before the supervised least mean square optimization, the encoded outputs are directly fed into the last layer for decision making, without random feature mapping. This framework has not well exploited the advantages of ELM theories, as Huang et al. [5] have demonstrated that the universal approximation capability of ELM cannot be guaranteed without random projections of the inputs. Therefore, it is obvious that the potential in multilayer implementation of the ELM has not been fully deployed, and further research on an ELM-based multilayer perceptron (MLP) is badly in need, toward faster training speed and better feature learning and classification performance.

Meanwhile, another leading trend for hierarchical learning is called deep learning (DL) [11], or deep NNs (DNNs) [12]. Similarly, the deep architecture extracts features by a multilayer feature representation framework, and the higher layers represent more abstract information than those from the lower ones. From concept point of view [11]–[12][13][14][15], DL is a BP learning for multilayer networks with unsupervised initialization instead of the conventional random initialization. In other words, DL considers multilayer as a whole with unsupervised initialization, and after such initialization the entire network will be trained by BP-based NNs, and all of the layers are hard coded together. It should be mentioned that all the hidden parameters in DL framework need to be fine-tuned multiple times, for the entire system. And thus the training of DL is too cumbersome and time consuming.

Seen from the above analysis, the existing learning algorithms (including the ELM-based and DL-based ones) for MLPs cannot achieve excellent generalization performance with fast learning speed. In this paper, inspired by the MLP theories, we extend the ELM and propose a hierarchical ELM (H-ELM) framework for MLPs. The proposed H-ELM further improves the learning performance of the original ELM, while maintaining its advantages of training efficiency. The contributions of the proposed work are as follows.

A new ELM-based autoencoder is developed via ℓ1 -norm optimization. Unlike the existing autoencoders used in DL, i.e., BP-based NNs, the input weights and hidden node biases of the proposed ELM autoencoder are established by searching the path back from a random space. The ELM theory in [5] has proved that random feature mapping (with almost any nonlinear piecewise activation function) can provide universal approximation capability, and by doing so, more important information can be exploited for hidden layer feature representation. As compared with the ℓ2 -norm ELM autoencoder in [10], here the ℓ1 penalty is applied to generate more sparse and meaningful hidden features.

A new H-ELM framework is proposed for an effective and efficient MLP. The proposed H-ELM consists of two main components: a) unsupervised multilayer feature encoding and b) supervised feature classification. For feature representation or extraction, the proposed ELM sparse autoencoder is utilized, and each layer in the stack architecture can be considered as a separate part (or an autonomous subsystem/submodule); for feature classification, the obtained high-level features are first scattered by a random matrix, and then the original ELM is applied for final decision making. Note that based on ELM theory [5], high dimensional nonlinear transform of extracted features can further improve feature classification precision. Moreover, different from the greedy layerwise learning in DL framework, feature extraction and classification are two separate autonomous parts in the proposed H-ELM framework, and parameter fine-tuning is not required for the entire system combined by feature extraction and classification, and thus the training speed can be much faster than the traditional BP-based DL.

To demonstrate the advantages of the proposed H-ELM framework, several H-ELM-based feature extraction and classification algorithms are developed for practical computer vision applications, such as object detection, recognition, and tracking. The obtained results are quite promising, and further confirm the generality and capability of the H-ELM.

The remainder of this paper is organized as follows. Section II introduces the related works, including the fundamental concepts and theories of ELM. Section III describes the proposed H-ELM framework and its related ELM sparse autoencoder. Section IV compares the performance of H-ELM with those of ELM and other relevant state-of-the-art MLP learning algorithms on various testing data sets. Section V presents several real-world applications incorporating the proposed H-ELM, including car detection, gesture recognition, and real-time object tracking. Finally, the conclusion is drawn in Section VI.

SECTION II.Related Works
In this paper, ELM is to be extended for MLPs. To facilitate the understanding of the proposed algorithm, this section briefly reviews the related concepts/theories of ELM, including the fundamental ideas and capabilities of the original ELM and ELM-based autoencoder.

A. ELM Theory
Suppose that SLFNs with L hidden nodes can be represented by the following equation:
fL(x)=∑i=1LGi(x,ai,bi)⋅βi,ai∈Rd,bi,βi∈R(1)
View Sourcewhere Gi(⋅) denotes the i th hidden node activation function, ai is the input weight vector connecting the input layer to the i th hidden layer, bi is the bias weight of the i th hidden layer, and βi is the output weight. For additive nodes with activation function g , Gi is defined as follows:
Gi(x,ai,bi)=g(ai⋅x+bi)(2)
View Sourceand for radial basis function (RBF) nodes with activation function g , Gi is defined as
Gi(x,ai,bi)=g(bi||x−ai||).(3)
View Source

Huang et al. [5] have proved that the SLFNs are able to approximate any continuous target functions over any compact subset X∈Rd with above random initialized adaptive or RBF nodes. Let L2(X) be a space of functions f on a compact subset X in the d -dimensional Euclidean space Rd such that |f|2 is integrable, that is, ∫X|f(x)|2dx<∞ . For u,v∈L2(X) , the inner product ⟨u,v⟩ is defined by
⟨u,v⟩=∫Xu(x)v(x)dx.(4)
View Source

The norm in L2(X) space is denoted as ||⋅|| , and the closeness between network function fn and the target function f is measured by the L2(X) distance
||fL−f||=(∫X|fn(x)−f(x)|2dx)2.(5)
View Source

Theorem 2.1:
Given any bounded nonconstant piecewise continuous function g : R→R , if span {G(a,b,x):(a,b)∈Rd×R} is dense in L2 , for any target function f and any function sequence gL(x)=G(aL,bL,x) randomly generated based on any continuous sampling distribution, limn→∞||f−fn||=0 holds with probability one if the output weights βi are determined by ordinary least square to minimize ||f(x)−∑Li=1βigi(x)|| .

The theorem above [5], [16], [17] shows that randomly generated networks with the outputs being solved by least mean square are able to maintain the universal approximation capability, if and only if the activation function g is nonconstant piecewise and span {G(a,b,x):(a,b)∈Rd×R} is dense in L2 . Based on this theorem, ELM can be established for fast learning, which will be described in detail in the next section.

B. ELM Learning Algorithm
According to the Theorem 2.1, the ELM can be built with randomly initialized hidden nodes. Given a training set {(xi,ti)|xi∈Rd,ti∈Rm, i=1,…,N} , where xi is the training data vector, ti represents the target of each sample, and L denotes the number of hidden nodes.

From the learning point of view, unlike traditional learning algorithms (see the related works referred to in [7]), ELM theory aims to reach the smallest training error but also the smallest norm of output weights [1], [7]
Minimize:∥β∥σ1u+λ∥Hβ−T∥σ2v(6)
View Sourcewhere σ1>0, σ2>0, u,v=0,(1/2),1,2,…,+∞ , H is the hidden layer output matrix (randomized matrix)
H=⎡⎣⎢⎢h(x1)⋮h(xN)⎤⎦⎥⎥=⎡⎣⎢⎢⎢h1(x1)⋮h1(xN)⋯⋮⋮hL(x1)⋮hL(xN)⎤⎦⎥⎥⎥(7)
View Sourceand  T is the training data target matrix
 T=⎡⎣⎢⎢tT1⋮tTN⎤⎦⎥⎥=⎡⎣⎢⎢t11⋮tN1⋯⋮⋯t1m⋮tNm⎤⎦⎥⎥.(8)
View Source

The ELM training algorithm can be summarized as follows [1].

Randomly assign the hidden node parameters, e.g., the input weights ai and biases bi for additive hidden nodes, i=1,…,L .

Calculate the hidden layer output matrix H .

Obtain the output weight vector
β=H†T(9)
View Sourcewhere T=[t1,…,tN]T,H† is the Moore–Penrose generalized inverse of matrix H .

The orthogonal projection method can be efficiently used for the calculation of MP inverse: H†=(HTH)−1HT , if HTH is nonsingular; or H†=HT(HTH)−1 , if HHT is nonsingular. According to the ridge regression theory, it was suggested that a positive value (1/λ) is added to the diagonal of HTH or HHT in the calculation of the output weights β . By doing so, according to [1] and [7], the resultant solution is equivalent to the ELM optimization solution with σ1=σ2=u=v=2 , which is more stable and has better generalization performance. That is, in order to improve the stability of ELM, we can have
β=HT(1λ+HHT)−1T(10)
View Sourceand the corresponding output function of ELM is
f(x)=h(x)β=h(x)HT(1λ+HHT)−1T(11)
View Sourceor we can have
β=(1λ+HHT)−1HTT(12)
View Sourceand the corresponding output function of ELM is
f(x)=h(x)β=h(x)(1λ+HHT)−1HTT.(13)
View Source

C. ELM-Based Autoencoder
Apart from the ELM-based SLFNs, the ELM theory has also been applied to build an autoencoder for an MLP. Conceptwise, the autoencoder [15] functions as some sort of feature extractor in a multilayer learning framework. It uses the encoded outputs to approximate the original inputs by minimizing the reconstruction errors. Mathematically, the autoencoder maps the input data x to a higher level representation, and then uses latent representation y through a deterministic mapping y=hθ(x)=g(A⋅x+b) , parameterized by θ={A,b} , where g(⋅) is the activation function, A is a d′×d weight matrix and b is a bias vector. The resulting latent representation y is then mapped back to a reconstructed vector z in the input space z=hθ′(y)=g(A′⋅y+b) with θ′={A′,b′} .

Using randomly mapped outputs as latent representation y , one can easily build the ELM-based autoencoder as shown in [10]. Then, the reconstruction of x can be regarded as an ELM learning problem, in which A′ is obtained by solving regularized least mean square optimization. However, due to the use of ℓ2 penalty in the original ELM, the extracted features by the ELM autoencoder in [10] tend to be dense and may have redundancy. In this case, a more sparse solution is preferred.

SECTION III.Proposed Learning Algorithm
In this section, we propose a new H-ELM framework for MLPs. The overall architecture of the proposed H-ELM is to be introduced in detail, and a new ELM sparse autoencoder is also presented, which is utilized as the basic elements of H-ELM.

A. H-ELM Framework
The proposed H-ELM is built in a multilayer manner, as shown in Fig. 1. Unlike the greedy layerwise training of the traditional DL frameworks [11], [13], one can see that the H-ELM training architecture is structurally divided into two separate phases: 1) unsupervised hierarchical feature representation and 2) supervised feature classification. For the former phase, a new ELM-based autoencoder is developed to extract multilayer sparse features of the input data, which is to be discussed in the next section; while for the latter one, the original ELM-based regression is performed for final decision making.


Fig. 1.
Proposed H-ELM learning algorithm. (a) Overall framework of H-ELM, which is divided into two phases: multilayer forward encoding followed by the original ELM-based regression. (b) Implementation of ELM-autoencoder. (c) Layout of one single layer inside the H-ELM.

Show All

In the following, we will present a detailed description of H-ELM, as well as its advantages against the existing DL and multilayer ELM (ML-ELM) algorithms. Before unsupervised feature learning, the input raw data should be transformed into an ELM random feature space, which can help to exploit hidden information among training samples. Then, a N -layer unsupervised learning is performed to eventually obtain the high-level sparse features. Mathematically, the output of each hidden layer can be represented as
Hi=g(Hi−1⋅β)(14)
View Sourcewhere Hi is the output of the i th layer (i∈[1,K] ), Hi−1 is the output of the (i−1) th layer, g(⋅) denotes the activation function of the hidden layers, and β represents the output weights. Note that here each hidden layer of H-ELM is an independent module, and functions as a separated feature extractor. As the layers increasing, the resulting feature becomes more compact. Once the feature of the previous hidden layer is extracted, the weights or parameters of the current hidden layer will be fixed, and need not be fine-tuned. This is totally different from the existing DL frameworks [11]–[12][13][14][15], where all the hidden layers are put together as a whole system, with unsupervised initialization. The whole system needs to be retrained iteratively using BP-based NNs. Thus, the training of H-ELM would be much faster than that of the DL.

From Fig. 1(a), we can also see that after unsupervised hierarchical training in the H-ELM, the resultant outputs of the K th layer, i.e., HK , are viewed as the high-level features extracted from the input data. When used for classification, they are randomly perturbed, and then utilized as the inputs of the supervised ELM-based regression to obtain the final results of the whole network. The reason for the perturbation of HK is that random projection of the inputs is required to maintain the universal approximation capability of ELM.

Conceptually, to expedite the learning speed, the proposed H-ELM framework is developed based upon random feature mapping and fully exploits the universal approximation capability of ELM, both in feature learning and feature classification. According to Theorem 2.1, using random mapped features as the inputs of the output weights, the hierarchical network is able to approximate or classify any input data in theory. On the other hand, it is also worth mentioning that H-ELM differentiates itself from the existing ML-ELM scheme [10] in threefold: 1) ML-ELM uses a simple stacked layer-by-layer architecture, and is just a straightforward replacement of DL autoencoder, with ELM autoencoder. In contrast, the proposed H-ELM considers MLP in a more comprehensive way, by dividing the whole network into two separated subsystems (i.e., unsupervised feature extraction/representation, and supervised feature classification, respectively), and using random projections of feature extraction results as the inputs of feature classification subsystem; 2) instead of ℓ2 -norm ELM autoencoder used in the ML-ELM, ℓ1 penalty is applied in H-ELM, to obtain more compact and sparse hidden information; and 3) the orthogonal initialization of ML-ELM is avoided, since the orthogonal constraint is not reasonable when the number of the input nodes is different from that of the output ones.

B. ELM Sparse Autoencoder
As introduced in the previous section, the proposed H-ELM briefly consists of two separate parts: 1) unsupervised and 2) supervised training. Since the supervised training is implemented by the original ELM, in this section, we will focus on how to train the unsupervised building blocks (i.e., autoencoder) of the H-ELM architecture. From Section II-C, it is known that the autoencoder aims to learn a function hθ(x)≃x , where θ={A,b} , A are the hidden weights, and b is the bias. In other words, the autoencoder tries to approximate the input data to make the reconstructed outputs being similar to the inputs.

In this paper, the universal approximation capability of the ELM is exploited for the design of autoencoder, and moreover, sparse constraint is added upon the autoencoder optimization, and therefore, we term it as ELM sparse autoencoder. The implementation of ELM sparse autoencoder is demonstrated in Fig. 1(b). It can be seen that unlike the autoencoders (i.e., BP-based NNs) used in the traditional DL algorithms, the input weights of the proposed ELM sparse autoencoder are established by searching the path back from a random space. The ELM theory [10] has demonstrated that the training of ELM with random mapped input weights is efficient enough to approximate any input data. That is to say, if we train the autoencoder following the concept of ELM, once the autoencoder is initialized, the fine-tuning is not required.

In addition, in order to generate more sparse and compact features of the inputs, ℓ1 optimization is performed for the establishment of ELM autoencoder, and this is different from the ELM autoencoder proposed in [10], where ℓ2 -norm singular values are calculated for feature representation. The optimization model of the proposed ELM sparse autoencoder can be denoted as the following equation:
Oβ=argminβ{||Hβ−X||2+||β||ℓ1}(15)
View Sourcewhere X represents the input data, H denotes the random mapping output, and β is the hidden layer weight to be obtained. In the existing DL algorithms, X is usually the encoding outputs of the bases β , which need to be adjusted during the iterations of optimization. However, in the proposed autoencoder, as we are to utilize random mapping for hidden layer feature representation, X is the original data and H is the random initialized output which need not to be optimized [1]. Furthermore, the experiments in the next section will show that it would not only help to improve the training time but also the learning accuracy.

Hereinafter, we will describe the optimization algorithm for the ℓ1 optimization problem. For clear representation, we rewrite the object function in (12) as
Oβ=p(β)+q(β)(16)
View Sourcewhere p(β)=||Hβ−X||2 , and q(β)=||β||ℓ1 is the ℓ1 penalty term of the training model.

A fast iterative shrinkage-thresholding algorithm (FISTA) [18] is adopted to solve the problem in (13). The FISTA minimizes a smooth convex function with complexity of O(1/j2) , where j denotes the iteration times. The implementation details of FISTA are as follows [18].

Calculate the Lipschitz constant γ of the gradient of smooth convex function ∇p .

Begin the iteration by taking y1=β0∈Rn , t1=1 as the initial points. Then, for j(j≥1) the following holds.

βj=sγ(yj) , where sγ is given by
sγ=argminβ{γ2||β−(βj−1−1γ∇p(β(j−1))||2+q(β)}.(17)
View Source

tj+1=1+1+4tj2√2 .

yj+1=βj+(tj−1tj+1)(βj−βj−1) .

By computing the iterative steps above, we could manage to perfectly recover the data from the corrupted ones. Using the resultant bases β as the weights of the proposed autoencoder, the inner product of the inputs and learned features would reflect the compact representations of the original data. In addition, as the autoencoder is adopted as the building block of the proposed H-ELM, higher level feature representations can be generated by layerwise comparison. In addition, compared with the existing ELM autoencoder [10] which simply uses the inputs as outputs with the traditional least mean square method, the ℓ1 optimization has been proved [18], [19] to be a better solution for data recovery and other applications. It will help to reduce the number of neural nodes and thus further improve the testing time of H-ELM.

SECTION IV.Performance Evaluation and Analysis
Extensive experiments are conducted to verify the effectiveness and efficiency of the proposed H-ELM framework, which is compared with the original ELM and other existing relevant state-of-the-art MLP algorithms.

In all the simulations below, the testing hardware and software conditions are listed as follows: Laptop, Intel-i7 2.4G CPU, 16G DDR3 RAM, Windows 7, MATLAB R2013b.

A. Comparison Between H-ELM and ELM
1) Selection of Hyper Parameters:
In this section, we will demonstrate the selection of hyperparameters in H-ELM and ELM schemes. Compared with the traditional NNs training algorithms, much less parameters need to be chosen in the training process of H-ELM and ELM. From Section II, we can see that two user specified parameters are required, i.e., the parameter C for the regularized least mean square calculation, and the number of hidden nodes L . In the simulations, C is set as {10−10,10−9,…,109,1010} , and L is set as {100,200,…,2000} .

Fig. 2(a) and (b) shows the learning accuracies of H-ELM and ELM in the L subspace, where the parameter C is prefixed. It can be seen that H-ELM seems to follow a similar convergence property of ELM but with higher testing accuracy, and the performances tend to be quite stable in a wide range of L . Meanwhile, in Fig. 2(c) and (d), the performance of H-ELM is more sensitive to the parameter C than that of the ELM. As C increases, the accuracy of H-ELM first slightly changes, and then increases rapidly until convergence to a better performance than ELM.



Fig. 2.
Testing accuracy in (C,L) subspace for the H-ELM and ELM. (a) Accuracy of H-ELM in terms of L . (b) Accuracy of ELM in terms of L . (c) Accuracy of H-ELM in terms of C . (d) Accuracy of ELM in terms of C . (e) Accuracy curve of H-ELM in terms of (C,L) . (f) Accuracy curve of ELM in terms of (C,L) .

Show All

It should also be mentioned that the impacts of C and L on the performance of H-ELM are different. As shown in Fig. 2(a), a proper C will make the accuracy curve more smooth (with less jitter) as L increases. On the other hand, from Fig. 2(c), one can see that different values of L have different impacts on the learning performance, but the shapes of accuracy curves are quite similar with different values of L .

To facilitate understanding, the 3-D accuracy curves of H-ELM and ELM are further shown in Fig. 2(e) and (f), respectively. Note that the learning performance of H-ELM is better than that of ELM, and they all affected by the parameters C and L .

Practically, C needs to be selected carefully, while L should be large enough. We can use a small number of nodes to select C , as L will not affect the trend of the accuracy curve.

2) Performance of Learning Accuracy:
As mentioned in Section III, the H-ELM briefly consists of two basic components: 1) unsupervised feature learning and 2) supervised feature classification. The major difference between H-ELM and the original ELM is that before ELM-based feature classification, H-ELM uses hierarchical training to obtain multilayer sparse representation of the input raw data; while in ELM scheme, the raw data is used for regression and classification. Generally speaking, the compact features can help to remove redundancy of the original inputs, and thus improve the overall learning performance.

The classification accuracies of the H-ELM and ELM are compared in Table I. For H-ELM, three-layer feature extraction is performed, and the resulting features are randomly projected before ELM-based classification; for ELM, the randomly mapped raw data is input for classification. For fair comparison, in the feature classification stage, the number of the hidden nodes of H-ELM is set to be equal to that of the ELM. In addition, both H-ELM and ELM are with the sigmoid activation function. The widely used binary-class and multiple-class data sets are tested, respectively. The experiments were repeated for 50 times, and the averaging results were obtained for comparison.

TABLE I Performance Comparison Using Different Methods Over Binary and Multiple Classed Benchmarks

One can easily observe that, the proposed H-ELM has a remarkable improvement against the ELM with most of the testing data sets, in terms of learning accuracy. The results show that by hierarchical feature encoding, the classification performance of ELM has indeed been enhanced. The reason is that for H-ELM, the input of ELM-based feature classification is changed from the original data to a more compact and sparse one. The ELM sparse autoencoder of H-ELM helps to generate a better performance by providing more robust features extract from the data itself.

B. Comparison With State-of-the-Art MLP Algorithms
In this section, we used more complicated data sets with images patches to verify the learning performance of H-ELM over DL algorithms [including Stacked Auto Encoders (SAE) [14], stacked autoencoder (SDA) [15], Deep Belief Networks (DBN) [13], Deep Boltzmann Machines (DBM) [20], and MLP-BP [21]] and ML-ELM [10]. Since they are also fully connected MLP, we use the term MLPs to denote them. Note that in the experiments, the effects of data preprocessing techniques (e.g., data augmentation) are avoided, and we mainly focused on the verification of learning capability of different MLP training schemes. For BP-based MLP training algorithms (SAE, SDA, DBN, DBM, and MLP-BP), the initial learning rate is set as 0.1 with a decay rate 0.95 for each learning epoch. The pretraining and fine-tuning are set as 100 and 200 epochs, respectively. Besides, the input corruption rate of SDA is set at 0.5 with a dropout rate 0.2 for hidden layers. The ℓ2 penalty parameters of the three-layer ML-ELM are set as 10−1 , 103 , and 108 , respectively.

1) MINST:
The Mixed National Institute of Standards and Technology (MNIST) handwriting data set [22] consists of 60 000 training images and 10 000 testing images, and the samples are digits 0–9 with 28×28 pixels in grayscale and have uniform backgrounds. As different digits have their unique shapes and different people write the number in their own ways, the MNIST is an ideal data set to test the effectiveness of the H-ELM. The original image patches are input into the training algorithms without preprocessing.

The testing accuracies of different MLP methods on MINST are shown in Table II. It can be seen that compared with other time consuming MLP training methods, the proposed H-ELM achieves 99.13% accuracy with hundreds times faster training time.

TABLE II Comparison of Learning Accuracy on MNIST Data Set

2) NORB:
To further demonstrate the advantages of H-ELM, another experiment was conducted using NYU Object Recognition Benchmark (NORB) data set [23], which is more complicated than the MNIST. The NORB contains images of 50 different 3-D toy objects with ten objects in each of five generic classes: 1) cars; 2) trucks; 3) planes; 4) animals; and 5) humans. Each image is obtained with different viewpoints and various lighting conditions. The training set contains 24 300 stereo image pairs of 25 objects (five per class), while the testing set contains the remaining image pairs of 25 objects. The testing results are shown in Table III, and it can be found that the proposed H-ELM again achieves the highest accuracy and fastest training time among the state-of-the-art MLP training algorithms, which is consistent with the results on MNIST.

TABLE III Comparison of Learning Accuracy on NORB Data Set

C. Performance Analysis
Based on the experiments above, we can see that the H-ELM obviously outperforms the original single-layer ELM, in terms of learning and classification accuracy. Furthermore, compared with other MLP training methods, H-ELM leads to a promising better performance with extremely faster training speed. While most of the state-of-the-art MLP training algorithms take hours or days for training by hundreds of epoches of iteration even with high performance computers, the H-ELM can be easily utilized with a laptop in a few minutes. In addition, it should be mentioned that the implementation of H-ELM only uses the MATLAB scripts, and therefore, the efficiency could be improved in more than tens or hundreds times with object-oriented programming techniques, e.g., cpp codes on Graphics Processing Unit or server.

SECTION V.Applications
To demonstrate the generality and capability of the H-ELM framework, in this section, three different computer vision applications (object detection, recognition, and tracking) are presented following the concept of H-ELM. In each application, H-ELM is used for feature extraction and classification, and the H-ELM-based methods are compared with state-of-the-art schemes, including the relevant DL-based SDA ones, where H-ELM is straightforwardly replaced by SDA. The testing hardware and software environments remain the same in Section IV.

A. Car Detection
Car detection is a classical application in computer vision. In our simulations, we used University of Illinois Urbana-Champaign car data set [24], [28] for training and testing. Various small training images of cars/backgrounds are used, and each of the testing images contains at least one car. The testing images uses the single-scale set where the cars to be detected are roughly the same size (100×40 pixels) like those in the training images.

The proposed H-ELM-based car detection algorithm is shown in Fig. 3, and one can see that a sliding window is used to extract a fixed-size image patch, which are grouped and input into the H-ELM for training and testing. The number of training samples is 1050, and before training, the images are normalized and histogram equalized to avoid the effects of uneven lighting and intensity. The H-ELM is set as the following network structure: three-layer feature learning with two-hidden-node ELM feature classification.



Fig. 3.
H-ELM-based car detection approach.

Show All

The testing results are shown in Tables IV and V, it can be seen that the recall at equal-error rates (EERs) (recall equals the precision) of the proposed H-ELM is 95.5%. Compared with the other existing methods shown in Table IV, the proposed car detection method achieves the best performance. It is worth noting that in order to remove the effects of background samples, the existing methods [25]–[26][27] adopted preprocessing before car detection. In contrast, the H-ELM-based detection is performed without any additional preprocessing, and achieves a pretty good performance. Furthermore, from Table V, we can see that the training time of the H-ELM-based method is much less than that of the DL-based SDA one, even with higher EER.

TABLE IV Car Detection Performance Comparison

TABLE V Performance Comparison Between H-ELM- and DL-Based SDA for Car Detection

B. Gesture Recognition
Gesture recognition is also an important topic in computer vision due to its wide ranges of applications, such as human–computer interfaces, sign language interpretation, and visual surveillance. In the experiments, the Cambridge-gesture data set [29] is used. It consists of 900 image sequences of nine hand gesture classes, which are divided by three primitive hand shapes and three primitive motions, as is shown in Fig. 4(a). Each class contains 100 image sequences includes five different illumination backgrounds, and each of the sequence was recorded in front of a fixed camera which roughly isolated gestures in space and time.


Fig. 4.
Original gesture samples and difference images. (a) Cambridge gesture data set. (b) Difference images of flat/leftward gestures in Cambridge gesture data set.

Show All

In the testing, all the video sequences are initially resized to 60×80×10 in pixels for fast implementation. Then, the first frame of each sequence is used for initialization, and the other frames subtract the corresponding value of the first frame, and the resultant differences are combined to one single fused image [shown in Fig. 4(b)]. By doing so, we obtain 900×60×80 difference images which include the motion information of the hands and partially exclude the illumination effects.

The difference images are directly fed into the H-ELM-based classifier to recognize each of the gesture. Moreover, as some of the images are not at the centre, we further extend the data set with its random skewing version. In this case, each of the images is randomly moved with 3 and 5 pixels in horizontal and vertical directions (i.e., the data are augmented two times). This would help to make recognition more robust and invariant to small shifting in space.

Based on these settings, we use the proposed ELM sparse autoencoder for feature learning, and it is stacked twice to get more deep representation. The output is then used to initialize the ELM classifier in the H-ELM, which has 5000 hidden nodes. In our simulations, we use the above-mentioned data set with fivefolds cross-validation. The H-ELM framework was performed for 20 times, and achieves an averaging performance with 99.4% testing accuracy. The performance of other methods is listed in Tables VI and VII. As can be seen, the proposed method achieves the best result over these traditional methods, where the handcrafted feature extractors or other data analysis methods are used. Unlike these methods, the H-ELM uses unsupervised hierarchical feature extractors to obtain higher level feature representations, which lead to a notable improvement and better testing accuracy. In addition, it should also be noted that the H-ELM outperforms DL-based SDA, in terms of training time, as well as learning accuracy.

TABLE VI Gesture Recognition Performance Comparison

TABLE VII Performance Comparison Between H-ELM- and DL-Based SDA for Gesture Recognition

C. Online Incremental Tracking
Online incremental tracking refers to the tracking method that aims to learn and adapt a representation to reflect the appearance changes of the target. The appearance changes of the specified target include pose variation and shape deformation. Apart from these, extrinsic illumination change, camera motion, viewpoint, and occlusions inevitably cause large appearance variation. Modeling such appearance variation is always one of the nature problems of tracking [32].

In this section, based on the proposed H-ELM, we build a novel incremental tracking method. Unlike the conventional online tracking method, more robust features are extracted using H-ELM, and the classification performance is far better compared with the probability models used in the conventional methods. In addition, as demonstrated in the previous section, the training time of H-ELM is much less than other MLP architectures (e.g., DL-based SDA), and this ensures that H-ELM has obvious advantages in the case of real-time tracking.

In the H-ELM-based tracking framework, as the training is incremental and need to be online updated, an ELM variant, i.e., online sequential ELM (OS-ELM) [8], is applied for feature classification and updating. The tracking algorithm is shown in Fig. 5, which can be summarized as follows.

Two sets of image patches are sampled around the target in the first frame, then the images with d<r1 are used as positive training samples, while the images with r2<d<r3 are the negative ones, where d denotes the Euclidean distance between the target and the sampling location, r1,r2, and r3 are the parameters with the settings of 5, 10, and 20.

The H-ELM classifier is trained with the obtained image samples. For frame n(n≥2) , the following is applied.

The object features are extracted by the ELM sparse autoencoder, and the feature classification is performed in frame (n ), using OS-ELM.

The location with the maximum response is used as the target location, and then go to Step 1).

The training samples are updated, and the new samples are used for updating OS-ELM.



Fig. 5.
H-ELM-based online tracking framework. (a) Updating with n -frame. (b) Tracking on (n+1) -frame.

Show All

Note that we adopted the same sampling scheme as that in compressive tracking (CT) [33], which is a popular real-time online tracking approach. For H-ELM tracking, the same initial position is labeled in the first frame, and the tracking results are obtained from the maximum values of the classification responses. In the simulations, we compared the feature extraction, feature updating, and classification performances of H-ELM, CT, and SDA, under the same conditions.

The well-used David and Trellis video sequences are tested. The tracking location errors of H-ELM, CT, and SDA are presented in Fig. 6, and one can see that the error of H-ELM is lower than those of CT and SDA ones, and the tracking results of H-ELM are with less fluctuations among different frames.



Fig. 6.
Comparison of tracking location error using H-ELM, CT, and SDA on different data sets. (a) David Indoor . (b) Trellis .

Show All

Moreover, the testing scenes are shown in Fig. 7, where Rows 1 and 4 are the results obtained from H-ELM, and the other rows are the results from CT and SDA. As can be seen in Rows 1–3 (David Indoor data set), the tracking locations of H-ELM is more accurate than those of CT and SDA. The tracking windows of CT and SDA more or less have some variations, while the window of H-ELM seems pretty stable. For the Trellis video sequence, the CT and SDA lost tracking of the target when the illumination on face has obvious change, while the H-ELM still gives a robust tracking. Besides, by the advantages of fast training of H-ELM framework, H-ELM achieves real-time tracking (over 15 frames/s), while the result for SDA is only 1–2 frames/s.



Fig. 7.
Tracking performance comparison of H-ELM, CT, and SDA in David Indoor and Trellis data sets. Rows 1 and 4: results of H-ELM. Rows 2 and 5: results of CT. Rows 3 and 6: results of SDA.

Show All

SECTION VI.Conclusion
In this paper, we have proposed a novel MLP training scheme, which is based on the universal approximation capability of the original ELM. The proposed H-ELM achieves high level representation with layerwise encoding, and outperforms the original ELM in various simulations. Moreover, compared with other MLP training methods, the training of H-ELM is much faster and achieves higher learning accuracy. We also verified the generality and capability of H-ELM for practical computer vision applications. In theses applications, H-ELM functions as a feature extractor and classifier, and it achieves more robust and better performance than relevant state-of-the-art methods.