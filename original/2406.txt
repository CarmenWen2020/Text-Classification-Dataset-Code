Multiagent reinforcement learning (MARL) has been extensively used in many applications for its tractable implementation and task distribution. Learning automata, which can be classified under MARL in the category of independent learner, are used to obtain the optimal joint action or some type of equilibrium. Learning automata have the following advantages. First, learning automata do not require any agent to observe the action of any other agent. Second, learning automata are simple in structure and easy to be implemented. Learning automata have been applied to function optimization, image processing, data clustering, recommender systems, and wireless sensor networks. However, a few learning automata-based algorithms have been proposed for optimization of cooperative repeated games and stochastic games. We propose an algorithm known as learning automata for optimization of cooperative agents (LA-OCA). To make learning automata applicable to cooperative tasks, we transform the environment to a P-model by introducing an indicator variable whose value is one when the maximal reward is obtained and is zero otherwise. Theoretical analysis shows that all the strict optimal joint actions are stable critical points of the model of LA-OCA in cooperative repeated games with an arbitrary finite number of players and actions. Simulation results show that LA-OCA obtains the pure optimal joint strategy with a success rate of 100% in all of the three cooperative tasks and outperforms the other algorithms in terms of learning speed.

SECTION I.Introduction
Reinforcement learning (RL) is a prevalent solution to Markov decision processes (MDPs). An agent in single-agent settings receives a numerical reward from the environment after executing an action and uses the reward to improve its behavior to obtain the maximal expected reward. In multiagent settings, central learning becomes infeasible because of the exponentially growing joint action space and the limited range of sensing. Multiagent reinforcement learning (MARL) [1], [2] has the advantages of tractable implementation and task distribution. Some MARL algorithms, known as independent learner, do not require any agent to observe the actions of the other agents. This type of MARL algorithm reduces the communications between agents and alleviates the problem of the growing joint action space.

A learning automaton is an intelligent computation entity that determines the optimal action. After an action is selected and executed, the environment responds to the learning automaton in the form of a numeric value known as the reward. The learning automaton then uses the reward to update its strategy. By continuing this process, the learning automaton gradually learns the best strategy that can obtain the most desired response from the environment. Many learning automata-based algorithms use more than one automaton. These algorithms can be classified under MARL in the category of independent learner.

Learning automata have the following advantages [3]. First, learning automata have all the merits of independent learner. Second, learning automata require a simple reinforcement signal from the environment. Third, learning automata need a few mathematics operations in each time step, which makes it suitable for real-time applications. Fourth, learning automata are simple in structure and easy to be implemented.

Learning automata have been used in a variety of applications, such as function optimization [4]–[5][6][7][8], image processing [9], data clustering [10], [11], recommender systems [12], and wireless sensor networks [13]. However, a few learning automata-based algorithms have been proposed for optimization of cooperative repeated games and stochastic games.

We propose an MARL algorithm known as learning automata for optimization of cooperative agents (LA-OCA). The main contributions are as follows. First, the LA-OCA algorithm uses a few mathematic operations in each time step. To benefit from the simple strategy updating rule of learning automata, we transform the environment to a P-model (the reinforcement signal in a P-model environment can only choose the value of one and zero) by introducing an indicator variable whose value is one when the maximal reward is obtained and is zero otherwise. Second, we analyze the model of the LA-OCA algorithm in cooperative repeated games with an arbitrary finite number of players and actions and prove that all the strict optimal joint actions are stable critical points. Third, the LA-OCA algorithm obtains a success rate of 100% in all of the three cooperative stochastic games, which are known as the load balance task, the distributed sensor network (DSN) task, and the robot leaving a room task. Besides, the LA-OCA algorithm has a higher learning speed than the other MARL algorithms.

The remaining sections are organized as follows. Section II reviews the related MARL algorithms. Section III introduces the background of stochastic games, repeated games, the notation used in this article, and learning automata. Section IV elaborates the LA-OCA algorithm and theoretically analyzes its models in cooperative repeated games. Section V studies the efficacy of the LA-OCA algorithm in repeated games and three cooperative tasks—the load balance task, the DSN task, and the robots leaving a room task. Section VI summarizes the conclusions.

SECTION II.Related Work
A. MARL Algorithms for Repeated Games
The MARL algorithms for repeated games have limited applications, but they provide valuable theoretical results and inspiration for multiagent learning for stochastic games. Some MARL algorithms for stochastic games are extended from the algorithms for repeated games [2].

The first category of MARL algorithms is to play cooperative repeated games. Their goal is to obtain the maximal immediate reward. Joint action learner (JAL) [14] requires each agent to construct an opponent model by using the observation of the other agents’ actions and immediate rewards. Frequency maximum Q value (FMQ) [15] uses the maximal reward and the frequency of obtaining the maximal reward to update each agent’s strategy. Exploring actions according to Q value ratios (EAQR) [16] uses the frequency of obtaining the maximal reward to update each agent’s strategy. Policy gradient potential (PGP) [17] uses the potential of the gradient of the system performance index to update each agent’s strategy. The models of EAQR and PGP in some type of cooperative games with an arbitrary finite number of agents and actions have been analyzed in [16] and [17], respectively.

The second category of MARL algorithms is for general-sum games. These algorithms concern whether some type of equilibrium can be obtained. Infinitesimal gradient ascent (IGA) [18], win-or-learn-fast IGA (WoLF-IGA) [19], generalized IGA (GIGA) [20], and GIGA-WoLF [21] require each agent to use the gradient of its own immediate reward to update its strategy. IGA and WoLF-IGA can be applied to repeated games with two agents and two actions, whereas GIGA and GIGA-WoLF allow for more agents and more actions. Independent Q-learning (IQL) is an extension of Q-learning. The dynamics of IQL with Boltzmann exploration and ε -greedy exploration in repeated games with two agents and two actions has been analyzed in [22]–[23][24][25] and [26], respectively. Linear reward inaction (LRI) [27], [28] is a type of learning automata-based algorithm. It has been proved that LRI converges to the Nash equilibrium (NE) if at least one strict pure NE exists.

B. MARL Algorithms for Stochastic Games
The MARL algorithms for stochastic games have been used in a wide variety of applications, such as control of traffic lights [29], robotic team navigation [30], optimal consensus control [31], and resource management [32]. Their learning goal depends on the type of task.

The first category of MARL algorithms aims to obtain the optimal joint strategy in cooperative tasks. In optimal adaptive learning (OAL) [33], each agent builds the model of each of the other agents and uses the model to learn the optimal action of every virtual game constructed on top of each stage of the stochastic games. In frequency of the maximum reward Q-learning (FMRQ) [34], each agent uses the frequency of obtaining the maximal reward to update its strategy without the observation of the actions of the other agents. In probability of maximal reward based on estimated gradient ascent (PMR-EGA) [35], each agent uses the gradient of the global cumulative reward to update its strategy. The gradient information is estimated by using the Q value function of the joint actions. The actor-critic method has been applied to cooperative tasks. Counterfactual multiagent (COMA) [36] uses a centralized critic network and decentralized actor networks to optimize each agent’s strategy. Multiagent deep deterministic policy gradient (MADDPG) [37] trains the decentralized critic networks for each agent with the centralized method. Consensus-based methods have been used for optimization of the leader–follower multiagent systems [38], [39] and MDPs [40], [41]. Chiang et al. [38] used the actor-critic network and the deterministic policy gradient method to realize the optimal consensus controller for the leader–follower multiagent systems. Abouheaf et al. [39] used RL to optimize the controller for the multiagent systems modeled as dynamic graphical games and validated the effectiveness through a leader-synchronization case. The methods in [40] and [41] use policy consensus to optimize an MDP in which each agent can visit only partial states.

The second category of MARL algorithms aims to obtain an NE in a wider variety of tasks. Nash-Q [30] uses quadratic programming to obtain NEs at each state. To avoid Q value function sharing, Nash-Q requires each agent to observe the previous rewards and actions of the other agents. Negotiation-based Q-learning (NegoQ) [42] is an efficient MARL algorithm without sharing value functions. NegoQ avoids the expensive computation of finding mixed NEs by learning three types of pure strategies. NegoQ proposes the multistep negotiation process to find these pure strategies, avoiding sharing value functions between the agents. Win or learn fast policy hill climbing (WoLF-PHC) [19], policy gradient ascent with approximate policy prediction (PGA-APP) [43], and weighted policy learner (WPL) [44] use the gradient information to update each agent’s strategy. Exponential moving average Q-learning (EMAQ) [45] employs the exponential moving average mechanism and the greedy action of each agent to update agents’ strategies. Simulation results show that EMAQ outperforms PGA-APP and WPL in a range of situations concerning convergence to an NE. LRI lagging anchor (LRILA) [1] is an extension of LRI that can be applied to stochastic games. Simulation results show that it can converge to an NE of the grid-world game. The NE can also be obtained by consensus-based algorithms [46]–[47][48]. Xue et al. [46] proposed an event-triggered asynchronous cellular learning automata algorithm to facilitate the convergence to the NE for the second-order multiagent systems. Ye and Hu [47] and Li and Ding [48] used the consensus and the gradient method to obtain an NE. They differ in that each agent in [47] needs to observe the actions of its neighbors and each agent in [48] needs to access the states of its neighbors.

The LA-OCA algorithm belongs to the first category of MARL algorithms and has the following characteristics. First, the LA-OCA algorithm alleviates the problem of the exponentially growing joint action space. Compared with JAL such as COMA, MADDPG, and PMR-EGA, the LA-OCA algorithm, which belongs to independent learner, does not require any agent to observe the action of any of the other agents. Second, compared with FMRQ, EAQR, and PMR-EGA, SGJA-UQO requires less memory. FMRQ, EAQR, and PMR-EGA need to maintain a large number of auxiliary variables to estimate the probability of obtaining the maximal reward for each agent. The LA-OCA algorithm, which benefits from the simple strategy updating rule of learning automata, needs much less memory and a few mathematics operations in each time step.

SECTION III.Preliminaries and Notation
This section introduces preliminaries of stochastic games, repeated games, the notation used in this article, and learning automata.

A. Stochastic Games
In a stochastic game [49], [50], S is the finite set of states, Ai(t) is the set of agent i ’s available actions at time t for i=1 , 2, … , n , and A(t)=A1(t)×A2(t) , … , ×An(t) is the set of all possible joint actions that consist of the actions of all agents at time t . If a joint action a∈A(t) is selected in state s∈S , a state transition occurs and the new state s′∈S is determined by the conditional probability p(s′|s,a) . If there always exists some state s ’ that satisfies p(s′|s,a)=1 for any state and joint action pair (s , a ), then this a deterministic stochastic game. After each state transition, each agent gains a numerical reward according to its own reward function ri(t) : S×A1(t)×A2(t) , … , ×An(t)×S→ R. In a cooperative stochastic game, the values of ri are the sum of all agent’s immediate reward for i=1 , 2, … , n at any time t . The goal of each agent is to maximize its own cumulative reward from any time t
==Ji(t)ri(t+1)+γri(t+2)+γ2ri(t+3)+⋯γKri(t+K+1)∑k=0Kγkri(t+k+1)(1)
View Sourcewhere γ∈(0,1) is the discount factor that emphasizes the importance of the rewards received shortly and K is the ending time of the game playing.

B. Repeated Games
In a stateless game [2], each agent selects an action according to its strategy at the same time and receives a numerical reward determined by the payoff matrix, and the game ends. Thus, there is no state transition during the game. A repeated game [51], [52] is a repetition of stateless games. To obtain the maximal reward, each agent improves its strategy based on its experience obtained from game playing. A strategy is a probability distribution of its actions. A pure strategy is an assignment that the probability of selecting some action is one. A mixed strategy is an assignment that each action can be selected with a probability. A totally mixed strategy is an assignment that each action can be selected with a positive probability. A joint strategy is composed of the strategy of every agent. A joint strategy is pure if it is composed of pure strategies and is totally mixed if it is composed of totally mixed strategies.

Fig. 1 shows the payoff matrix of a cooperative repeated game with two agents and three actions. Each agent receives an identical reward after each game. The agents try to collaborate to obtain the maximal reward 5 that are the numbers within the parentheses. The optimal joint strategy is that both agents select the first action or the second action at the same time. Thus, the optimal joint action is pure. There always exists at least one pure optimal joint strategy in a cooperative repeated game.


Fig. 1.
Payoff matrix of a cooperative repeated game with two agents and three actions.

Show All

C. Notation
Consider a repeated game with n agents whose available actions are fixed. Define indicator functions di : A1×A2 , … , ×An→ {0, 1}, for i=1 , 2, … , n , by
di(a1,a2,…,an)={1,0,if ri=ri_maxif ri≠ri_max(2)
View Sourcewhere ai is the action selected by agent i , ri is the reward obtained after a game, and ri_max is the maximal reward of agent i in the payoff matrix. The strategy of agent i is a probability vector pi=(pi1,pi2,…,pi|Ai|) . Agent i selects action j with probability pij . A pure strategy for agent i can be defined as a vector ei , where the i th component is one and the others are zero. The joint strategy is denoted by
P=(p1,p2…,pn).(3)
View Source

Define functions liw for i=1 ,2, … , n , w=1 , 2, … , |Ai| by
=1≤=liw(P)p(ri=ri_max|agent j used strategy pj,j≤n,j≠i, and agent i selected action w)∑j1,…,ji−1,ji+1,…,jndi(j1,…,ji−1,w,ji+1,…,jn)∏w≠ipwjw(4)
View Sourcewhich is the conditional probability of obtaining the maximal reward if agent i selects action w given the joint strategy P .

Define functions gi for i=1 , 2, … , n by
gi(P)==p(ri=ri_max|the joint strategyP was used)∑j1,…,jndi(j1,…,jn)∏k=1npkjk(5)
View Sourcewhich is the conditional probability of obtaining the maximal reward given the joint strategy P . From (4) and (5), we have
∑w=1|Ai|liw(P)piw=gi(P).(6)
View SourceIn a cooperative repeated game, ri denotes the sum of all agents’ rewards, and ri_max denotes the maximal sum of all agents’ rewards for i=1 , 2, … , n . Thus, we have that ∀i,j,gi(P)=gj(P) . We define the following function:
g(P)=gi(P),i=1,2,…,n(7)
View Sourcefor cooperative repeated games.

Definition 3.1:
In a cooperative repeated game, the optimal joint action (a∗1,a∗2,…,a∗n) is said to be strict if for each agent i , 1≤i≤n , we have
>d(a∗1,…,a∗i−1,a∗i,a∗i+1,…,a∗n)d(a∗1,…,a∗i−1,ai,a∗i+1,…,a∗n)∀ai∈Ai and ai≠a∗i.(8)
View Source

D. Learning Automata
According to the category of the reward, environments can be classified into three classes: P-model, Q-model, and S-model. A P-model environment returns a reward of zero or one to represent failure or success. The reward from a Q-model environment takes a finite number of values in the interval [0, 1]. The reward from an S-model environment takes any value in the interval [0, 1].

The Q-model and the S-model are appropriate for general-sum repeated games. Using the S-model, learning automata can obtain the NE in repeated games. One classical learning automata-based algorithm is LRI [27]. Each agent i for i=1 , 2, … , n updates its strategy pi according to
pig(k+1)=pig(k)+bri(k)(1−pig(k)),if aigis selected at time stepkpij(k+1)=pij(k)−bri(k)pij(k),for all aij≠aig(9)(10)
View Sourcewhere aij denotes the j th action of agent i , b∈(0,1) is the learning rate, and ri∈[0,1] is the reward received by agent i . It has been proved that this algorithm can converge to an NE.

The P-model is appropriate for optimization of some performance index. Some cooperative tasks can be modeled as deterministic stochastic games to obtain the maximum cumulative reward. We can reward each agent with one when the maximum cumulative reward is obtained and zero otherwise. Thus, a P-model is desired in this situation.

SECTION IV.LA-OCA Algorithm
A. Formulation of the LA-OCA Algorithm
We propose an MARL algorithm known as LA-OCA. To obtain the maximal sum of all agents’ rewards, we transform the environment to a P-model by defining the indicator variable as follows:
ci(k)={1,0,if ri(k)≥ri_max(k) otherwise(11)
View Sourcewhere ri(k) denotes the sum of all agents’ rewards at time step k and ri_max(k) denotes the maximal sum of all agents’ rewards by time step k . The pseudocode of the LA-OCA algorithm is presented in Algorithm 1. Each agent i for i=1 , 2, … , n updates its strategy pi according to the following updating rules:
pig(k+1)=pig(k)+bci(k)(1−pig(k))if aig is selected at time step k, pij(k+1)=pij(k)−bci(k)pij(k) for all aij≠aig.(12)(13)
View Sourcewhere b∈(0,1) is the learning rate. It will be proved that all the strict optimal joint actions are stable critical points if b is infinitely small. Thus, b should be set to a small number for actual use. In the meanwhile, the probability of each action has to be strictly greater than zero to guarantee that each joint action always has a chance to be during learning.

Algorithm 1 LA-OCA Algorithm for Repeated Games
Initialize strategy pi for each agent i with the uniform distribution for i=1 , 2, … , n.

Repeat for each game

For each agent i , do

Select an action according to pi .

End for each agent

For each agent i , do

Observe the reward ri .

If ri≥ri_max

ci=1

ri_max=ri

Else

ci=0

End if

Update pi according to (12) and (13).

End for each agent

Until the predefined number of games have been played

Return pi for each agent

B. Analysis of the LA-OCA Algorithm
The LA-OCA algorithm can be represented as
P(k)=P(k)+bG(P(k),a(k),R(k))i=1,2,…,n,j=1,2,…|Ai|(14)
View Sourcewhere P(k) denotes the joint strategy, a(k) is the joint action, R(k)= (r1 , r2 , … , rn ) are the obtained rewards, and G (.,.,.) denotes the updating specified by (12) and (13). Define function f by
f(P)=E[G(P(k),a(k),R(k))|P(k)=P].(15)
View Source

According to [27, Th. 3.1], if the learning rate b is infinitely small, the model of the LA-OCA algorithm can be represented by an ordinary differential equation as follows:
dPdt=f(P).(16)
View SourceBesides, if every joint action has been visited, ri_max(k) becomes a constant, namely the maximal reward in the payoff matrix. Specifically, the component equation about the probability of the j th action of agent i is as follows:
=====dpijdtfij(P)E[ci|P,ai=aij,ri=ri_max]pijlij(P)(1−pij)+E[ci|P,ai=aij,ri≠ri_max]pij(1−lij(P))(1−pij)+∑w≠jE[ci|P,ai=aiw,ri=ri_max]piwliw(P)(−pij)+∑w≠jE[ci|P,ai=aiw,ri≠ri_max]piw(1−liw(P))(−pij)pijlij(P)(1−pij)+0+∑w≠jpiwliw(P)(−pij)+0pij∑wpiw[lij(P)−liw(P)] pij[lij(P)−gi(P)] (17)
View Sourcewhere the last equation holds because of (6).

The following theorems describe the characteristics of the model of LA-OCA in cooperative repeated games.

Theorem 1:
For the model of the LA-OCA algorithm in a cooperative repeated game, all the strict optimal joint actions are locally asymptotically stable critical points.

Proof:
For the component action aik of each optimal joint action, we have that lik(P)−gi(P)=1−1=0 . Besides, we have that the probability of the other actions piw=0,∀w≠k . Thus, pij[lij(P)−gi(P)]=0 for j=1 , 2, … , |Ai| , i=1 , 2, … , n , which means that all the optimal joint actions (including all the strict optimal joint actions) are critical points.

Let (a∗1,…,a∗n) be a strict optimal joint action. To use the Lyapunov theory to analyze the stability of the strict optimal joint actions, we need to perform the transformation, P→P¯ , defined by
pij={p¯ij,1−p¯ij,ifaij≠a∗iifaij=a∗i.(18)
View SourceThen, the critical point corresponding to (a∗1,…,a∗n) is p¯∗ij=0,∀i,j . Since ∑jp¯ij=1,∀i , we choose pij,aij≠a∗i as the independent variables. Then, we can get
dp¯ijdt=Hij(P¯)+second and higher order terms(19)
View Sourcewhere by Taylor expansion from (17) and the notation defined by (2), (4), and (5)
==Hij(P¯)fij(P¯∗)+∑k,l[(p¯kl−p¯∗kl)∂fkl(P¯)∂p¯kl∣∣∣p¯kl=p¯∗kl]p¯ij⎡⎣⎢⎢di(a∗1,…,a∗i−1,aij,a∗i+1,…,a∗n)−di(a∗1,…,a∗n)⎤⎦⎥⎥.(20)
View SourceConsider the following Lyapunov function:
V(P¯)=∑i,j,aij≠a∗ip¯ij.(21)
View SourceWe have V(P¯)>0 for all P¯≠0 , V(P¯)=0 only at the point p¯∗ij=0 for all i and j , and
dV(P¯)dt==∑i,j,aij≠a∗idp¯ijdt∑i,j,aij≠a∗ip¯ij[di(a∗1,…,a∗i−1,aij,a∗i+1,…,a∗n)−di(a∗1,…,a∗n)]+higher order terms.(22)
View Source

By Definition 3.1, we have
di(a∗1,…,a∗i−1,aij,a∗i+1,…,a∗n)−di(a∗1,…,a∗n)<0
View Sourcewhen aij≠a∗i for all i and j . Then, we obtain (dV(P¯)/dt)<0 for all P¯≠0 in a sufficiently small neighborhood around the origin. Thus, all the strict optimal joint actions are stable critical points.

Theorem 2:
For the model of the LA-OCA algorithm in a cooperative repeated game, if all the optimal joint actions are strict, any mixed joint strategy P that has 0<g(P)<1 is an unstable critical point or not a critical point.

Proof:
Suppose that there are n agents and m (m≥1 ) strict optimal joint actions (aj1,…ajn),j=1,…,m , where aji denote agent i ’s component action of the j th optimal joint action. Let pji denote the probability of agent i selecting the action aji . Since only the strict optimal joint actions exist in the game, if an action is not a component action of any strict optimal joint action, according to (17), its probability decreases to zero. Thus, we only need to consider the component actions of each of the m strict optimal joint actions. The action probability of agent i can be defined as (p1i,…,pmi) . If m=1 , it is clear that the joint strategy will converge to the unique optimal joint action. Thus, we only need to consider the situations for m≥2 . If P is a critical point, it must satisfy
pji(∏k=1,k≠inpjk−g(P))=0,i=1,…,n, j=1,…,m.(23)
View Source

Then, we have
pj1=pj2=⋯=pjn,j=1,…,m.(24)
View SourceConsider the mixed joint strategy P(0<g(P)<1) of which one of the component strategies is pure, namely, ∃i,j,pji=1 . Suppose that P is a critical point. According to (24), we have pj1=pj2=⋯=pjn=1 . Then, we have
pji(∏k=1,k≠inpjk−g(P))=1⋅(1−g(P))>0(25)
View Sourcewhich is contradictory to (23). Thus, if 0<g(P)<1 , a critical point does not allow a pure component strategy. Based on this fact and (23), we have that if there are m1 actions that have the same probability of 1/m1−−−−−√n and m2 actions that have zero probability for each agent (m=m1+m2,2≤m1≤m ), and P satisfies (24), then the mixed joint strategy P that has 0<g(P)<1 is a critical point. Otherwise, P is not a critical point.

Then, we examine the stability of the critical points that have 0<g(P)<1 . We can perform a transformation according to any critical point to be analyzed, which is defined by
pji={p,p∈{pj1i,…,pjm1i},q,q∈{pjm1+1i,…,pjm2i},if 0<pji<1if pji=0.(26)
View SourceThe action probability of agent i can be rewritten as (pj1i,…,pjm1i,pjm1+1i,…,pjm1+m2i) . Then, we can derive the Jacobin matrix J∈Rmn×mn at the critical point. It can be verified that the trace of J is −g(P)n−g(P)nm2 , which is also the sum of all eigenvalues of J . By determinant transformation and extracting common factors from the lines of the determinant of J−λI , we find n+nm2+1 eigenvalues of −g(P) (more negative eigenvalues may exist, but we do not need to find all of them.). Thus, at least one positive eigenvalue exists, which means that any critical point that has 0<g(P)<1 is unstable.

C. LA-OCA for Cooperative Stochastic Games
In a cooperative stochastic game, the LA-OCA algorithm requires each agent to update its strategy according to
pig(s)=pig(s)+bci(s)(1−pig(s)),if aig is selected under state spij(s)=pij(s)−bci(s)pij(s),for all aij≠aig(27)(28)
View Sourcewhere agent i selects the j th action under state s with pij(s) . The value of ci is determined by
ci(s)={1,0,if Ji(s)≥Ji_max(s) otherwise(29)
View Sourcewhere Ji(s) is the cumulative reward received by agent i from state s and Ji_max(s) is the maximal cumulative reward received by agent i from state s .

The pseudocode is presented in Algorithm 2. We introduce a threshold parameter h to prevent premature convergence. It can be seen from lines 22–28 in Algorithm 2 that an appropriate value of h restrains the probability of each action. The upper bound should be larger than the lower bound for each agent. Thus, h<1−1/|Ai| should be satisfied for i=1,2,…n .

Algorithm 2 LA-OCA Algorithm for Stochastic Games
Initialize strategy pi(s) for each agent i for i=1 , 2, … , n , the learning rate b , and the threshold h .

Repeat

Repeat

For each agent i , do

Select an action ai according to pi(s) under the current state s.

End for each agent

For each agent i , do

Observe the next state s′ , the immediate reward ri

Record the tuple <s,ai,s′,ri>

End for each agent

Until the episode ends

For each agent i , do

For each experienced state s in the last episode

Evaluate Ji(s) by (1) and the related tuples <s,ai,s′,ri>

If Ji(s)>=Ji_max(s)

Ji_max(s)=Ji(s)

ci(s)=1

Else

ci(s)=0

End if

Update pi(s) according to (IV-C) and (IV-C)

For each action aij∈|Ai(s)|

If pij(s)≤h|Ai(s)|−1

pij(s)=h|Ai(s)|−1

Else if pij(s)≥1−h

pij(s)=1−h

End if

End for each action

Normalize pi(s)

End for each ex perienced state

End for each agent

Until the predefined number of episodes have been played

Return pi(s) for each agent

SECTION V.Empirical Studies
In this section, the efficacy of the LA-OCA algorithm is verified through empirical studies. Algorithm 1 is employed to play cooperative repeated games. Two scenarios are considered. In the first scenario, all the optimal joint actions are strict. In the second scenario, some of the optimal joint actions are not strict. Algorithm 2 is compared with the other MARL algorithms in three cooperative tasks—the load balance task, the DSN task, and the robot leaving a room task. All the three tasks have discrete state space, discrete joint action space, and finite steps. They differ in that the load balance task and the robot leaving a room task are deterministic stochastic games, whereas the DSN task is not a deterministic stochastic game. In the load balance task and the DSN task, each episode starts from a random state, whereas in the robots leaving a room task, each episode starts from the same state. Among the comparison algorithms, PMR-EGA and EAQR are for cooperative tasks only. WoLF-PHC and EMAQ are selected as baselines because they might converge to the optimal NE in some type of cooperative task.

A. Repeated Games
The payoff matrices of the repeated games are generated according to the following rules.

The optimal joint actions are selected randomly from the joint action space.

Each optimal joint action obtains the maximum reward 10 000, and each of the other joint actions obtains a random reward within [0, 1000].

The results are averaged on 50 runs. In each run, the strategy of each agent is initialized randomly. A run ends if the strategy of each agent becomes pure (a component strategy is considered to be pure if some action probability is no less than 0.999.). If the converged joint action is optimal, we consider that this run is successful.

In the first scenario, repeated games with n=2,3,4,5,6 agents and m=2,3,4,5 actions are used as test cases. The number of optimal joint actions is m . The learning rate b is 0.01. It is noted that the success rate is 100% except for four cases in Table I. In the failed runs of the four cases, ri_max has never reached the value of the actual maximal reward in the payoff matrix. Insufficient exploration and premature convergence are two possible causes. We have taken countermeasures in the LA-OCA algorithm for stochastic games.

TABLE I Success Rate in Repeated Games in Which All the Optimal Joint Actions are Strict (Runs = 50)

In the second scenario, repeated games with n=4,5,6 agents and m=3,4,5 actions are used as test cases. The number of optimal joint actions is 0.1mn in each case, which means that 10% of the joint actions are optimal. Thus, some optimal joint actions are not strict. The other settings are the same as the first scenario. At this time, a success rate of 100% is obtained in all cases in Table II.

TABLE II Success Rate in Repeated Games in Which Some Optimal Joint Actions are Not Strict (Runs = 50)

B. Task 1: Load Balance
The goal of the load balance task [53] is to distribute four agents evenly in minimal time steps. As shown in Fig. 2, a polygon has 12 vertices that are represented by circles. A solid circle means that this vertex has been occupied by an agent. A hollow circle means that this vertex is not occupied by any agent. To lift the polygon, each agent has to move to the desired vertex until the four agents are evenly distributed on the polygon. The positions of all agents constitute the state space of the task. Each agent knows the positions of all agents at each time step. The state space of this task contains 11 808 elements (not including the absorbing states). Each agent has three actions—moving clockwise, moving anticlockwise, or standing still. The number of joint actions is 34 = 81. All agents execute actions simultaneously. A collision occurs if two agents move to the same unoccupied vertex, two neighbor agents move in the opposite direction, or one agent moves to an occupied vertex. An agent moves successfully if no collision occurs. An episode starts at a random state and ends when all agents are evenly distributed or 100 time steps have elapsed. If all agents are evenly distributed, each agent receives a reward of 10. Otherwise, each agent receives a reward of −1 after each time step.


Fig. 2.
Load balance task.

Show All

For all the algorithms, the discount factor γ is set to 0.9. For the LA-OCA algorithm, the learning rate b follows:
b=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪0.05,0.08,0.1,0.15,0.2,1≤n≤0.2L0.2L<n≤0.4L0.4L<n≤0.6L0.6L<n≤0.8L0.8L<n≤L(30)
View Sourceand the threshold h follows:
h=⎧⎩⎨⎪⎪⎪⎪0.6,0.4,0.2,0,1≤n≤0.4L0.4L<n≤0.6L0.6L<n≤0.8L0.8L<n≤L.(31)
View Sourcewhere n is the current number of episodes and L is the total number of episodes. For PMR-EGA, the learning rate for the Q value function of the joint actions α=0.9 , the learning rate for each agent i (i=1 , 2, 3, 4)αi=50 , and the sample period N=200 . The Q value of each joint action is initialized to 0. The Q value of each agent’s own action is initialized to 8.0. The scale coefficient c is as follows:
c=⎧⎩⎨⎪⎪⎪⎪0.05,0.10,0.15,0.2,1≤n≤0.4L0.4L<n≤0.6L0.6L<n≤0.8L0.8L<n≤L.(32)
View SourceFor EAQR, the sample period Ns=50 . The learning rate α decreases gradually as follows:
α=αini−αinin1.05L(33)
View Sourcewhere the initial learning rate αini=0.7 . The exploration rate ε follows:
ε=⎧⎩⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪0.9,0.8,0.7,0.6,0.5,1≤n≤0.2L0.2L<n≤0.4L0.4L<n≤0.6L0.6L<n≤0.8L0.8L<n≤L.(34)
View Source

For WoLF-PHC, δw=0.003,δl=0.01 , ε=0.8 , and α follows (33) with αini=0.7 . For EMAQ, ε=0.2,k=2 , ηw=(1/10+0.2n) , ηl=0.001ηw , and α follows (33) with αini=0.7 .

All results are averaged on 50 runs. Each run contains L learning episodes and 50000 evaluation episodes. In each evaluation episode, each agent fixes its strategy and selects the greedy action at each time step. The experiments for different values of L are independent. The initial positions of agents at each episode for all the algorithms are the same for fairness. We design an algorithm that can determine the minimal time steps given the positions of all agents as input. A successful episode uses minimal time steps. Table III shows the success rate. The LA-OCA algorithm with L= 1 500 000 beats the other algorithms. The LA-OCA algorithm obtains a success rate of 100% when L= 6 000 000. As shown in Tables IV and V, the LA-OCA algorithm also obtains the minimal average number of steps and the maximal average cumulative reward of all algorithms. PMR-EGA exhibits unstable performance, whereas EAQR improves slowly as the value of L increases. These results indicate that the LA-OCA algorithm is superior to PMR-EGA and EAQR in terms of learning speed. It is noted that for the LA-OCA algorithm, the standard deviation is not zero when L= 6 000 000. This is because a minimum number of time steps for different episodes may vary with the initial positions of the agents.

TABLE III Success Rate for the Load Balance Task (Runs = 50)

TABLE IV Average Steps and Standard Deviation for the Load Balance Task (Runs = 50)

TABLE V Average Cumulative Reward and Standard Deviation for the Load Balance Task (Runs = 50)

To examine the strategy obtained by the LA-OCA algorithm, we perform one run with L= 6 000 000 learning episodes and four evaluation episodes. Each of the four evaluation episodes starts from a unique state and proceeds according to the joint strategy learned by the LA-OCA algorithm. As shown in Fig. 3, all the evaluation episodes use minimal time steps to solve the problem. Only one optimal joint action exists at step 0 of episodes 3 and 4. More than one optimal joint action exists at steps 0 and 1 of episode 1. For example, at step 0 of episode 1, an alternative optimal joint action can be that agent 1 moves clockwise, agent 2 stays, and agents 3 and 4 move anticlockwise. This indicates that the LA-OCA algorithm can converge to the optimal joint action when multiple optimal joint actions exist. As shown in Fig. 3, some redundant actions exist in episode 1. For the optimal joint strategy, the redundant actions could exist if they do not affect the maximum cumulative reward.


Fig. 3.
(a)–(d) Illustrations of four evaluation episodes of load balance using the joint strategy obtained by the LA-OCA algorithm (after 6 000 000 learning episodes).

Show All

C. Task 2: Distributed Sensor Network
The goal of the DSN task [54] is to capture both the targets and gain the maximal cumulative reward. The original DSN task contains eight sensors and three cells. To level up the difficulty of the task, we add four sensors and three cells to the network. As shown in Fig. 4, six cells are surrounded by a network of 12 sensors. Two targets wander in the cells. The positions of the targets and the number of live targets constitute the state space of the DSN task. The energy of the targets cannot be perceived by the sensors and is not part of the state unless the energy value is zero. Each sensor can perceive the positions and the number of live targets at each time step. The number of states is C26×2+C16×2+1=43 . Each target has four available actions—up, down, left, and right. Each sensor is viewed as an agent. Each sensor can choose the action of no focus or focus on one of the neighbor cells at a time step. The total number of joint actions is 291 600.


Fig. 4.
DSN with 12 sensors. ⊗ and two targets ∙ .

Show All

At the beginning of an episode, each target has an energy of 3 and is randomly located at a cell. One cell can be occupied by only one target at a time step. Every sensor executes an action at the same time. If a target is focused by three or more sensors, then its energy decreases by one. This is called a hit. Besides, the action of focus is always rewarded with −1, and the action of no focus always produces a reward of 0. Then, it is the turn for the targets to move. If the energy of a target decreases to zero, it is captured and is wiped out of the cells. Otherwise, they select a random action sequentially. If three sensors are involved in the capture, each of them receives a reward of 10. If more than three sensors are involved in the capture, only the ones with three maximal indexes are rewarded. An episode ends if both the targets are captured or 300 time steps have elapsed.

For all the algorithms, the discount factor γ is set to 0.9. For the LA-OCA algorithm, b=0.08 , and the threshold h follows:
h=⎧⎩⎨0.25,0.2,0.1,1≤n≤0.2L0.2L<n≤0.4L0.4L<n≤L(35)
View Sourcewhere n is the current number of episodes and L is the total number of episodes. For EAQR, Ns=50 , α follows (33) with αini=0.7 , and the exploration rate ε follows (34). For PMR-EGA, α=0.9 , αi=15 , N=1200 , and c follows (32). For WoLF-PHC, δw=0.003,δl=0.01 , ε=0.2 , and α follows (33) with αini=0.7 . For EMAQ, ε=0.2 , k=2 , ηw=0.1 , ηl=0.001ηw , and α follows (33) with αini=0.7 .

All the algorithms are performed for 50 runs. Each run consists of L learning episodes and 50 000 evaluation episodes. The maximal cumulative reward of 42 and the minimal steps of three can be obtained if each group of three sensors focuses on one target and the rest sensors select no focus. An evaluation episode is considered to be successful if a cumulative reward of 42 is obtained within three steps. The success rate is obtained by considering all the evaluation episodes of 50 runs. As shown in Table VI, both the success rates of WoLF-PHC and EMAQ are zero for all values of L . LA-OCA obtains a success rate of 100% when L= 3 000 000, which can also be inferred from Tables VII and VIII. These results indicate that LA-OCA has learned the optimal strategy in each of the 50 runs. EAQR obtains an average cumulative reward of 41.99 and an average step of 3.44, which indicates that the joint action of no focus is selected in some states. As for PMR-EGA, some redundant actions of focus are taken during the task, though the minimal steps of 3 are used when L= 1 000 000 and L= 2 000 000. The results for the worst run are presented in Tables IX and X. The LA-OCA algorithm is more reliable than the other algorithms.

TABLE VI Success Rate for the DSN Task (Runs = 50)

TABLE VII Average Cumulative Reward and Standard Deviation for the DSN Task (Runs = 50)

TABLE VIII Average Steps and Standard Deviation for the DSN Task (Runs = 50)

TABLE IX Minimal Cumulative Reward for the DSN Task (Runs = 50)

TABLE X Maximal Steps for the DSN Task (Runs = 50)

To examine the strategy obtained by the LA-OCA algorithm, we perform one run with L= 3 000 000 learning episodes and three evaluation episodes. As shown in Fig. 5, the dish with the number i represents the i th target. The energy value is shown by the energy bar above each target. It can be seen that each target is focused by three sensors at each time step, and both the targets are eliminated at the same time. With this joint strategy, the sensors can obtain a cumulative reward of 42 and eliminate both targets in three time steps.


Fig. 5.
Illustrations of using the strategy obtained by the LA-OCA algorithm to capture the targets (after 3 000 000 learning episodes).

Show All

D. Task 3: Robots Leaving a Room
The robots leaving a room task is introduced in [55]. The goal is to let the robots leave a room without any collision in the shortest time. As shown in Fig. 6, the door of the room is allowed to pass by only one robot at a time. The original task contains three robots. To increase the difficulty, we add three robots to the room. The coordinate of the center of the door is (x , y)= (5, 0). The initial distance from each robot to the center of the door is six units. Each robot has two actions at each time step—move directly toward the door with one unit and stay where it is. The number of joint actions is 26 = 64. The coordinates of the six robots constitute the state space of the task. The number of states is 76 = 117 649. All robots take action simultaneously. A collision occurs if the distance between the centers of two robots is less than or equal to 1.6 units. If a robot collides with the other robots, it stays where it is and receives a reward of −200. If a robot leaves the room, it receives a reward of 30. Otherwise, each robot obtains a reward of −1 after each step. An episode ends when all the robots have left the room or 300 time steps have elapsed.


Fig. 6.
Diagram of the robots leaving a room task.

Show All

For all the algorithms, the discount factor γ is set to 0.9. For the LA-OCA algorithm, the learning rate b is set to 0.2, and the threshold h follows:
h=hini−hinin1.05L(36)
View Sourcewhere the initial threshold hini=0.015 . For PMR-EGA, α=0.9 , αi=50,N=600, and c follows (32). For EAQR, α=0.7 , Ns=50 , and ε=0.05 . For WoLF-PHC, ε=0.05 , δw=0.003 , δl=0.01 , and α follows (33) with αini=0.7 . For EMAQ, ε=0.05 , k=2 , ηw=0.1 , ηl=0.001ηw , and α follows (33) with αini=0.7 .

The success rate, the average steps, and the average cumulative reward are shown in Tables XI–XIII, respectively. The results are averaged on 50 runs. Each run consists of L learning episodes and one evaluation episode. An evaluation episode is considered to be successful if a cumulative reward of 120 is obtained in 16 time steps. The LA-OCA algorithm has the highest convergence speed of all algorithms. Both LA-OCA and PMR-EGA obtain a success rate of 100% when L=48000 . EAQR might achieve higher performance if more learning episodes are experienced. Neither WoLF-PHC nor EMAQ learns the optimal joint strategy in any of the 50 runs. PMR-EGA performs poorly in terms of average cumulative reward when L=12000 and L=24000 because it has not converged yet, which causes many collisions in each evaluation episode. It converges to the optimal joint strategy when L=48000 . The cumulative reward of 120 indicates that no collision occurs. The results of the worst run are presented in Tables XIV and XV. EAQR obtains a cumulative reward of −1800 in the worst runs when L=24000 and L=48000 . We find that no collision occurs in these runs, and all the robots stop moving at some early time step.

TABLE XI Success Rate for the Robots Leaving a Room Task (Runs = 50)

TABLE XII Average Steps and Standard Deviation for the Robots Leaving a Room Task (Runs = 50)

TABLE XIII Average Cumulative Reward and Standard Deviation for the Robots Leaving a Room Task (Runs = 50)

TABLE XIV Maximal Steps for the Robots Leaving a Room Task (Runs = 50)

TABLE XV Minimal Cumulative Reward for the Robots Leaving a Room Task (Runs = 50)

To examine the strategies obtained by the LA-OCA algorithm, we perform 50 runs, each of which consists of L= 160 000 learning episodes and one evaluation episode. Since each episode starts with the same state, we perform and record only one evaluation episode. Fig. 7 shows the evaluation episode of the first run (this diagram cannot be used to judge whether a collision occurs because the solid circles represent only the centers of the robots.). The dotted circles indicate the joint actions at the current time step. If a robot selects the other action and the other robots remain their actions, it might collide with other agents, which are indicated by the numbers in the parentheses. For example, if robot 2 moves to the door at step 5 and the other robots remain their actions, it will collide with robots 1 and 3. In this task, multiple optimal joint actions could exist. For example, robots 2 and 4 can also choose the other action together in step 11. After analyzing the obtained strategies for each of the 50 runs, we notice that the obtained strategies are different from each other, though all of them use 16 steps to obtain a cumulative reward of 120 without any collision. This means that the LA-OCA algorithm can help us find out a variety of high-quality strategies.


Fig. 7.
Illustrations of using the strategy obtained by the LA-OCA algorithm to evacuate the robots (after 12 000 learning episodes).

Show All

SECTION VI.Conclusion
This article proposes a learning automata-based algorithm known as LA-OCA to optimize cooperative repeated games and stochastic games. The main results are as follows. First, every strict optimal joint action is a stable critical point of the model of LA-OCA in cooperative repeated games with an arbitrary finite number of agents and actions. Second, if all the optimal joint actions are strict, any mixed strategy that obtains the maximal global reward with the probability p∈(0,1) is not a critical point or an unstable critical point. Empirical evidence in repeated games shows that LA-OCA converges to the optimal joint action given enough exploration. As for stochastic games, LA-OCA obtains the optimal strategy in all of the three tasks with a success rate of 100% and outperforms the other algorithms in terms of learning speed. In the future, we will study the stability of the remaining critical points of the model of LA-OCA in repeated games and its convergence in stochastic games.
