Reinforcement learning with safety constraints is promising for autonomous vehicles, of which various failures may result in disastrous losses. In general, a safe policy is trained by constrained optimization algorithms, in which the average constraint return as a function of states and actions should be lower than a predefined bound. However, most existing safe learning-based algorithms capture states via multiple high-precision sensors, which complicates the hardware systems and is power-consuming. This article is focused on safe motion planning with the stability guarantee for autonomous vehicles with limited size and power. To this end, the risk-identification method and the Lyapunov function are integrated with the well-known soft actor–critic (SAC) algorithm. By borrowing the concept of Lyapunov functions in the control theory, the learned policy can theoretically guarantee that the state trajectory always stays in a safe area. A novel risk-sensitive learning-based algorithm with the stability guarantee is proposed to train policies for the motion planning of autonomous vehicles. The learned policy is implemented on a differential drive vehicle in a simulation environment. The experimental results show that the proposed algorithm achieves a higher 
SECTION I.Introduction
Classical motion planning algorithms, such as artificial potential field [1], rapidly exploring random trees (RRT) [2], and RRT* [3], have been successfully applied in many fields, including autonomous vehicles and manipulators. Such nonlearning algorithms, however, face difficulties when dealing with the high-dimensional motion planning problem. A recent research trend is to apply machine learning to motion planning, particularly reinforcement learning (RL) [4], which has achieved great advances in the field of robotics, such as robotic manipulators [5]–[6][7] and autonomous vehicles [8]–[9][10][11][12][13][14]. Both in simulation experiments and real-world applications, numerous successful examples have been reported in motion planning of autonomous vehicles based on RL. Most of the abovementioned studies utilize the methods based on deep deterministic policy gradient (DDPG) [15] to train policies. However, the soft actor–critic (SAC) [16] algorithm, which achieves decent performance among the existing RL methods, is rarely utilized in the motion planning of autonomous vehicles.

In addition, some of the aforementioned algorithms utilized in autonomous vehicles do not consider safety, which is a topic of great importance in RL [17]–[18][19][20]. There exists some impressive progress in the past decade allowing for applications of the safe RL in autonomous vehicles, e.g., [21], [22]. When the RL algorithm is applied to the motion planning of autonomous vehicles, the trained policy is required to be sensitive to risk, which is associated with the fact that even an optimal policy may perform poorly in some cases due to the uncertain behaviors of obstacles [23]. Thus, it is essential for the trained policy to be aware of the risk accurately and timely in motion planning. Currently, some methods address this problem by capturing the position and trajectory of obstacles with images and range information [12], [13], which requires robots to be equipped with multiple sensors and high-performance computational units. However, these methods are difficult to be implemented in autonomous vehicles with low cost and low computation capability.

Existing studies on safe RL for risk reduction can be broadly classified into three categories, including risk-sensitive criterion, constrained criterion, and worst case criterion [24]. The risk-sensitive criterion requires the policy to strike a balance between getting more rewards and reducing risks [23]. The constrained criterion aims to maximize the return subject to one or more constraints [25], [26]. As for the worst case criterion, the goal is to attain the policy that can tolerate the worst case scenario [27]. Although the aforementioned categories can be utilized to solve the problem of safe motion planning in autonomous vehicles, the worst case criterion is difficult to be applied due to the challenge of designing worst case scenarios for training the policy.

A natural way to ensure the safety of autonomous vehicles in motion planning is to apply constraints over states and actions. Typically, a well-known formulation for RL with constraints is the constrained Markov decision process (MDP) (CMDP) framework [28], where the discounted sum of safety cost should be under a certain bound. However, such a safety evaluation metric may not train an infallible policy. The optimization criterion cannot guarantee that the system always stays inside the safe region confined by the safety cost, e.g., when the variance of policy is high, violation of safety constraints can occur with high probability. Furthermore, robustness in the presence of environment uncertainties and perturbations is often missing in the safe learning-based control problems. Though the objective of RL is to maximize the long-term return [29], the agent may temporally take undesirable actions, which drives the agent to a dangerous zone. Under such circumstances, autonomous vehicles are expected to be able to return to a safe area. This ability to recover can be formulated as the stability of closed-loop systems, which is one of the most important properties in the control theory [30]. However, learning a policy that stabilizes the system is still a challenging field in model-free RL manners [31].

Motivated by the above observations, in this article, a novel learning-based algorithm called Lyapunov-based SAC with collision probability prediction (LSAC-CPP) algorithm is proposed for the safe motion planning of autonomous vehicles. Based on the SAC that is proven to improve the capability of exploration, the proposed algorithm defines the risk according to the probability of collision for learning the safe policy [32]. Furthermore, this article generalizes the definition of stability in control theory for safe RL and borrows the concept of the Lyapunov function to guarantee the stability of the system. Specifically, a risk-sensitivity parameter (RSP) is introduced to strike a balance between maximizing the return and minimizing the cumulative risk, and a data-driven method based on the generalized Lyapunov function is introduced to analyze the stability of the system.

The main contributions of this article are summarized as follows: 1) an approach is proposed to predict the probability of collision, which utilizes a sequence of states and actions; 2) a practical learning-based algorithm is designed to train the safe policy for motion planning of autonomous vehicles; and 3) a novel learning-based method is proposed to construct Lyapunov functions in a model-free framework to further ensure the safety of the system.

Notations: In this article, Rn denotes the n -dimensional Euclidean space; ∥⋅∥ refers to the Euclidean vector norm; and let st and at denote the state and action of the autonomous vehicle at time step t∈[0,T] . The relative position to the target is denoted by pt=[ptx,pty]⊺∈R2 , and the 10-D sparse range findings are part of the collected LiDAR data, which are denoted by dt∈R10 . Let Eπ[⋅] denote the expectation that follows the policy π ; ρπ denotes the trajectory distribution of (st,at) followed by policy π . Let D denote the dataset. Edge set Δ≐{s|cπ(s)≥η} is the set of edge states that are unsafe or close to the unsafe region.

SECTION II.Preliminaries
A. Motion Planning
The motion planning of the autonomous vehicle is formulated as the optimal decision problem [9], [12]. The goal is to maximize the return ∑Tt=0γtr(st),γ∈[0,1] to reach the target by a policy π , which generates actions through the observations, and the problem can be presented as follows:
argmaxπ(s) Eπ[rt+1+γrt+2+⋯+γT−1rT|st=s]s.t. mindt≥d0∥ptg−pg∥2≤r~pt=pt−1+△tπ(st)(1)(2)(3)(4)
View Sourcewhere d0 is the threshold value for colliding with obstacles, r~ is the threshold value for reaching the target, and ptg and pg are the final position and the target position of the autonomous vehicle, respectively. (2) is the collision avoidance constraint, (3) is the goal constraint, and (4) is the general kinematics of the autonomous vehicle.

B. Stability in Control Theory
Uniformly ultimately bounded (UUB) stability, among various types of stability, is found particularly useful for the general learning-based control problems. A system is said to be UUB stable in mean square with an ultimate bound d if there exist positive constants b and d ∀ϵ<b , ∃T(ϵ,d) , such that ||st0||<ϵ⟹Est||st||<d∀t>t0+T [33].

The classical definition of UUB stability generally states that, for trajectories starting from a point with the norm of state less than b , the expectation of state will eventually enter and stay inside the set where ||s||≤d after T time steps. If the safety cost in CMDP is defined by the Euclidean norm of state and the CMDP is UUB stable with an ultimate bound d¯ , the autonomous vehicle will either remain safe or first recover and then remain safe.

Generally, the classical definition of UUB stability limits the application of CMDP tasks since, for autonomous vehicles, the safety cost is not always defined by the Euclidean norm. For example, it may be defined as the distance from the working area to the unsafe area or other nonlinear metrics, such as kinetic energy, whose representation consists of the square of velocity. Therefore, this article extends the classical definition of UUB stability to more general cases and introduces the definition of UUB stability in terms of safety cost. If there exist positive constants b and d ∀ϵ<b , ∃T(ϵ,d) , such that cπ(st0)<ϵ⟹Estcπ(st)<d∀t>t0+T , the CMDP task is said to be UUB stable in mean safety cost.

The extended definition of UUB stability is illustrated in Fig. 1. In this case, satisfying UUB stability in terms of safety cost implies that, if the autonomous vehicle is accidentally disturbed and runs into a risky area, it will return to the working area within a finite time.


Fig. 1.
Demonstration of UUB stability in time domain (the case of d=d¯ ).

Show All

C. RL for Markov Decision Process
RL is one of the machine learning methods for solving sequential decision problems with unknown state transition dynamics. In general, a sequential decision problem can be modeled as an MDP [9], [26]. An MDP is denoted by (S,A,P,r,ρ) , where S is the state space, A is the action space, P is the transition probability distribution, r:S×A→R is the reward function, and ρ⊆P(S) is the distribution of the initial state s0⊆S . A policy is a mapping π:S→P(A) that specifies the distribution of the action. There is a cost function c(st,at) to measure how good or bad a state–action pair is. The cost function under the policy π is defined as cπ(s)≐Ea∼πc(s,a) , which is required be lower than a certain safety threshold d¯ in this study.

By detailing each of these elements in MDP and relating them to (1)–(4), the RL formulation of collision avoidance is provided as follows.

Action Space: The action at∈A is constructed by linear and angular velocities, at=[vt,wt]⊺∈R2 with vt∈[0,vmax] and ∥wt∥2≤wmax , where vmax and wmax are the maximum linear and angular velocities, respectively.

State Space: The state st∈S is constructed by 10-D sparse range findings, the action at time step t−1 , and the relative position to the target, st=[dt,at,pt]⊺∈R14 .

Reward Function: There are three different conditions for the reward to encourage the autonomous vehicle to reach targets
r(st,at)=⎧⎩⎨rcoll,rarr,kr(Dt−1−Dt),mindt<d0Dt≤r~o.w.(5)
View Sourcewhere Dt is the relative distance between the autonomous vehicle and the target position, and kr is a hyperparameter. If the robot arrives at the target position through distance threshold checking, a positive reward rarr is acquired. By contrast, if the robot collides with the obstacle, i.e., the minimum distance between the robot and obstacles is smaller than a constant d0 , a negative reward rcoll is acquired. Otherwise, the reward is the difference in distance Dt−1−Dt multiplied by a hyperparameter kr , where Dt is the distance from the target position.

State Transition Probability: The autonomous vehicle in state st takes action at , and the state st transfers to state st+1 . The probabilistic state transition model that is determined by the kinematics as defined in (4) is denoted as P(st+1,st|at) .

Value Function: The objective is to find the optimal value function
V∗(s0)=Eπ [∑t=0Tγtr(st,π∗(st))|st=s0](6)
View Sourcewhere γ∈[0,1] is a discount factor.

D. Risk-Sensitive RL
In risk-sensitive RL (RSRL) problems, the goal of the autonomous vehicle can be presented to solve the following problem [24]:
maxθ∈Θ J(θ)=Eπθ[∑t=0Tγt(r(st,at)+ξωt)]s.t. mindt≥d0∥ptg−pg∥2≤r~pt=pt−1+△tπ(st)(7)
View Sourcewhere ξ∈R is the RSP, and ω refers to the risk. Note that ξ allows the desired level of risk to be controlled: if ξ>0 , the policy implies a risk-seeking preference; if ξ<0 , the policy implies a risk-aversion preference; and if ξ=0 , the policy implies risk neutrality.

In (7), the ω refers to the risk of the autonomous vehicle in the environment, such as the probability of collision, the minimum distance to obstacles, and the probability of entering unsafe states. In this article, the risk is defined by the probability of collision.

E. Safe RL With Stability Guarantee
In this article, the Lyapunov function is utilized to provide a stability guarantee. This tool has been used in the control theory for stability analysis and controller design [34], mostly exploited along with a deterministic or probabilistic model (see [35] and [36]). The Lyapunov function is a class of positive definite functions L:S→R+ . The general idea of exploiting the Lyapunov function is to ensure that the difference (or derivative) of the Lyapunov function along the state trajectory is seminegative definite so that the state goes in the direction of the decreasing value of the Lyapunov function and eventually converges to the origin or a sublevel set of the Lyapunov function. A Lyapunov-based and data-driven method is proposed for analyzing the stability of the system.

Theorem [37]:
For a given safety constraint d¯ , if there exists a function L(s):S→R+ and positive constants α1,α2,α3,η , and e , such that
η<e−α1η≤Et0∼P(t0|ρ,π),P(s|π,ρ,t0)[L(s)]<α1cπ(s)≤L(s)≤d¯α3d¯eα2cπ(s)∀s∈S(8)(9)(10)(11)
View Sourceand
Es∼μ(s)[Es′∼Pπ[L(s′)−L(s)]]≤−α3Es∼μ(s)[cπ(s)](12)
View Sourcewhere μ(s)≐(1/N)∑NT=0Et0∼P(t0|ρ,π)P(s|π,ρ,t0+T) , s∈Δ is the average probability of being in s during the time in edge set Δ under policy π . Then, the system is UUB stable in mean safety cost with ultimate bound d¯ .

The proof of Theorem 1 is given in [37], and some discussion and explanation for the above theorem are as follows. The energy decreasing condition in (12) requires the Lyapunov value to decrease in the edge set Δ , which is also the key in all Lyapunov methods. A fairly wide range of parameterizations for the Lyapunov function is confined in (11). The sum of quadratic polynomials is extensively used, e.g., L(s)=sTQs , where Q is a positive definite matrix. Such Lyapunov functions can be found effectively by the semidefinite programming solvers. In addition, the sum of the safety cost over a finite time can be chosen as a Lyapunov function, i.e., L(s)=∑t+NtE[cπ(st)] . In principle, the value function evaluates the longest time horizon and results in better performance. On the contrary, for the choice of a cost function, though the approximation converges considerably fast, the agent may suffer from the short-sighted behavior.

SECTION III.Algorithm
In this study, a novel learning-based algorithm called LSAC-CPP is proposed to solve the RSRL problem mentioned in Section II-D. The architecture of the proposed algorithm is shown in Fig. 2. This section first describes a strategy for acquiring the probability of collision. Following this, the algorithm called SAC-CPP combines the probabilistic prediction strategy with the SAC to train safe policies, which can be applied to obstacle avoidance scenarios for autonomous vehicles. Finally, the proposed algorithm LSAC-CPP based on the SAC-CPP includes the Lyapunov critic function, which trains safe policies with the stability guarantee.


Fig. 2.
Architecture of the LSAC-CPP. An end-to-end policy is trained to navigate a differential drive vehicle to targets. The planner is trained in the virtual environment based on sparse 10-D range findings, 2-D previous velocity, and 2-D relative target position.

Show All

A. Collision Probability Prediction
In this section, the goal is to attain the probability of collision, which is related to both the relative distance to obstacles and the actions [38]. In comparison to [23], a neural network with the parameter φ is expected to output the collision probability Pcoll(⋅)
Pcoll(⋅)=fφ(dt−l:t−1,dt,at−l:t−1,at)(13)
View Sourcewhere fφ is a fully connected neural network, dt−l:t−1 is the history of range findings in the last l time steps, dt is the current range finding, and at−l:t−1 is a concatenation of past actions.

The collision label Pcoll is True if a collision occurred during the episode and False otherwise. Thus, the neural network fφ can be trained as a binary classification network. Generally, cross-entropy is chosen as the loss function to train the network, which is a classical loss function in binary classification [39]. According to this, the loss function for training the neural network fφ is given by
L(⋅)=−E[P^logPcoll(⋅)+(1−P^)log(1−Pcoll(⋅))](14)
View Sourcewhere P^ is the set of the label of tuples {dt,at−1,at}i . To train this neural network, the training tuples {dt,at−1,at,P^collt}i are sampled randomly from the dataset, which is precollected. Since the P^collt is labeled as True when the agent turns to the end and else as False, the number of P^collt labeled as True is far fewer than that labeled as False. To increase the number of positive samples, P^collt is labeled as True, and {P^collT−△T,…,P^collT}i in the i th sample tuple are labeled as True, where T is the steps of an episode and △T is the number of positive samples. In this study, △T is utilized to regulate the conservatism of the collision probability prediction model. If the larger value of △T is assigned, more tuples will be labeled unsafe. As a result, the model will output a higher probability of collision in collision-free cases. By setting an appropriate value of △T , the neural network can be trained to predict the probability of collision.

B. Soft Actor–Critic With Collision Probability Prediction
The soft actor–critic with collision probability prediction (SAC-CPP) algorithm has the same form as the SAC, which outperforms a series of RL methods on the continuous control benchmarks. Given an action-value network, the agent can navigate to a target position by maximizing a one-step look-ahead action-value, as outlined in Algorithm 1. The SAC-CPP aims to maximize the trajectory return and the entropy of action distributions and minimize the cumulative risk. For this purpose, by recalling the optimal objective in (7), the optimal objective is given as
J(π)=∑t=0TE(st,at)∼ρπ[r(st,at)+ξωt+αH(π(⋅|st))](15)
View Sourcewhere α is a hyperparameter and H(⋅) is the entropy.

Algorithm 1 SAC-CPP Algorithm
Restore the probability prediction model fφ(⋅)

Initialize replay buffer D

Randomly initialize ϕ1 , ϕ2 , θ

Initialize target network with ϕ¯¯¯1←ϕ1 , ϕ¯¯¯2←ϕ2

for each iteration do

Sample s0 according to ρ

for each time step do

Sample at from π(s) and step forward

Obtain Pcollt from fφ(dt−l:t−1,dt,at−l:t−1,at)

Observe st+1 , rt

Update risk-sensitive reward by crt←rt+ξPcollt

Store the tuple D←D∪{(st,at,ct,st+1)}

st←st+1

end for

for each gradient step do

ϕi←ϕi−lc∇ϕiJQ(ϕi)fori∈{1,2}

ϕ¯¯¯i←(1−τ)ϕ¯¯¯i+τϕifori∈{1,2}

θ←θ−la∇θJπ(θ)

end for

end for

By recalling the RSRL problem in Section II-D, the risk is defined as the probability of collision. Thus, the risk at each time step can be included to the reward, and the reward at time t is redefined as
cr(st,at)≐r(st,at)+ξfφ(dt−l:t−1,dt,at−l:t−1,at).(16)
View Source

1) Update of Critic Network:
The Q -function parameters of the critic network can be trained by minimizing the soft Bellman residual
JQ(ϕ)=E(st,at)∼D[12(Qϕ(st,at)−(cr(st,at)+γEst+1∼ρ[Vϕ¯¯¯(st+1)]))2](17)
View Sourcewhere
Vϕ¯¯¯(st)=Eat∼π[Qϕ¯¯¯(st,at)−logπ(at|st)].(18)
View SourceThe optimal objective function can be optimized with stochastic gradient descent
∇ϕJQ(ϕ)=∇ϕQϕ(st,at)(Qϕ(st,at)−cr(st,at)−γVϕ¯¯¯(st+1)).(19)
View SourceThe Q -function is updated by double Q -networks that are verified to improve the performance of RL algorithms [40]. Given an initial ϕ0 , the parameter ϕk is updated, as shown in lines 16 and 17 in Algorithm 1.

2) Update of Actor Network:
The actor network of SAC-CPP is presented with a parameter θ , which is the same as SAC [16], and the approximate expression of the objective optimal function is written as
Jπ(θ)=Est∼D[Eat∼πθ[αlog(πθ(at|st))−Qθ(st,at)]].(20)
View SourceThe gradient of (20) can be approximated by
∇θJπ(θ)=∇θαlog(πθ(at,st))+(∇atαlog(πθ(at|st))−∇atQ(st,at))∇θfθ(ϵt;st)(21)
View Sourcewhere ϵt is the input noise vector. Given an initial θ0 , the parameter θk is updated by the gradient descent method, which is shown in line 18 in Algorithm 1.

C. Training the Policy
The practical RL algorithm with the stability guarantee is given in Section II. Based on SAC-CPP, an off-policy RL algorithm called LSAC-CPP is proposed in this section.

Off-policy algorithms are capable of learning from past experience and, thus, possess higher sample efficiency compared with on-policy methods. The DDPG [15] is a well-known off-policy actor-critic method, which is capable of dealing with a large class of continuous control tasks. A lately proposed soft actor–critic method [16], based on the maximum entropy framework, outperforms the DDPG and other policy gradient methods in a series of complex control tasks.

By recalling (7) and the CMDP in Section II-C, the cost function c(st,at) at time t is defined as
c(st,at)=max(0,d0−mindt).(22)
View Source

Based on the maximum entropy and minimum risk actor-critic framework, the Lyapunov function is utilized as the critic in the policy gradient formulation [37], [41]. In the LSAC-CPP, a Lyapunov critic function Lc is needed, which satisfies Lc(s)=Ea∼πLc(s,a) . The objective function J(π) is given as follows:
J(π)=ED[β[log(πθ(fθ(ϵ,s)|s))]−Q(s,fθ(ϵ,s))+ξωt]+λEμ,π,Pπ(Lc(s′,fθ(ϵ,s′))−Lc(s,a)+α3c(s,a))+νEP(s|ρ,π,t0),P(t0|ρ,π),π(Lc(s,a)−e)(23)
View Sourcewhere πθ is parameterized by a neural network fθ and ϵ is an input vector consisted of the Gaussian noise. In the above objective function, α and λ are positive Lagrangian multipliers that tune the weights of policy entropy and safety constraints, and they are adjusted through gradient method. In (23), D≐{(s,a,s′,r,c)} is the replay buffer for storage of CMDP tuples. To train the Lyapunov critic function, a separate edge replay buffer DΔ is needed, which samples and stores the tuples at different instants Ts . The pseudocode of LSAC-CPP is shown in Algorithm 2.

Algorithm 2 LSAC-CPP Algorithm
Restore the probability prediction model fφ(⋅)

Input hyperparameters, learning rates αϕQ,αϕLc,αθ , parameters regarding UUB theorem α3

Randomly initialize a critic network Q(s,a) , Lyapunov critic network Lc(s,a) , actor π(a|s) with parameters ϕQ , ϕLc , θ and the Lagrangian multipliers λ

Initialize the parameters of target networks with ϕ¯Q←ϕQ , ϕ¯Lc←ϕLc , θ¯←θ

for each iteration do

Sample s0 according to ρ

for each time step do

Sample at from π(s) and step forward

Observe st+1 , rt , ct

Update risk-sensitive reward by crt←rt+ξPcollt

Store (st,at,crt,ct,st+1) in D

If st∈δ , store (st,at,crt,ct,st+1) in DΔ

end for

for each update step do

Sample minibatches of tuples from D and DΔ and update Q , Lc , π and Lagrangian multipliers

Update the target networks with soft replacement:

ϕ¯Q←τϕQ+(1−τ)ϕ¯Q

ϕ¯Lc←τϕLc+(1−τ)ϕ¯Lc

θ¯←τθ+(1−τ)θ¯

end for

end for

The update details of the Q function and the Lyapunov function, as well as β , λ , and ν , are referred to Appendix A.

Remark:
It can be seen that the proposed algorithm is an off-policy RL algorithm, which inherits the advantages of the SAC. Compared with other learning-based algorithms, e.g., the proximal policy optimization method, the sample efficiency is higher, and the performance is superior due to the introduction of the maximum entropy.

SECTION IV.Experiments
In this section, the accuracy of prediction for collision and the success rate of the proposed algorithm are evaluated, respectively. First, an experiment illustrates how to identify risk in an environment. Second, in an outdoor simulation environment, the success rate of the proposed algorithm is compared with the SAC during the training process. Finally, the proposed algorithm shows the performance with different values of RSPs and the generalization performance.

A. Experimental Setup
Gym-gazebo [42], a 3-D simulation software, is chosen to build the environment for testifying the proposed algorithm in this study. This software combines ROS and gazebo with OpenAI gym, making it possible for the direct transfer of trained agents from virtual environment to the real world. As shown in Fig. 3, an outdoor environment is constructed in gym-gazebo to train the policy that navigates the autonomous vehicle to a random target position. The laser sensor has a view of 270° and a scanning range from 0.01 to 15 m. Jackal, a four-wheeled ground differential robot, is used as the physical platform for general validation. In this experiment, the obstacles in the environment are assumed to be static.


Fig. 3.
Virtual training environment is constructed in gym-gazebo. A 15×8m2 outdoor environments with walls and obstacles are built. The agent is equipped with a single-wire LiDAR. The agent and target are shown as in the figure with red circles, the bricks are stationary obstacles, and the white squares are obstacles whose positions are randomly initialized in each episode.

Show All

According to the mechanical characteristics of real robots, bounds for the action are chosen as vmax=1.0m/s and wmax=0.6rad/s . At the beginning of each episode, the autonomous vehicle is positioned randomly, and the targets are initialized randomly in the positive x -axis of the outdoor environment. An episode ends if the autonomous vehicle collides with an obstacle or runs for more than 300 steps, which can prevent the agent from falling into local optimizations. Policies are trained on a desktop machine equipped with an Intel i7-8700k CPU, 16-GB memory, and an Nvidia GeForce RTX 2070 GPU with 8-GB memory. SAC, SAC-CPP, and LSAC-CPP algorithms are implemented with TensorFlow in Python [43], and each training process has a maximum episode number of 4000.

B. Evaluation on Collision Probability Prediction Model
The probability of collision can be predicted by a neural network. As shown in Fig. 4, the input vector is transferred to collision probability through three fully connected neural network layers with 256 nodes. A sigmoid function is used as the activation function at the last layer to satisfy the probability constraint Pcollt(⋅)∈[0,1] .


Fig. 4.
Network structure of the RL with collision probability prediction. Every layer is represented by its type, dimension, and activation function. The Dense layer here means a fully connected neural network.

Show All

To generate the dataset for training the probability prediction model, the actions of the autonomous vehicle are generated randomly and independent of the observed obstacles. Before saving the observation state and the collision label in the training dataset, these tuples are labeled by the way mentioned in Section III-A. The last few sets {P^collT−△T,…,P^collT}i are all labeled as True, and ΔT is set to be 4. When the value of ΔT is getting larger, the trained policy will become more conservative, and vice versa. During the training process, 272 000 tuples have been collected with the batch size N=1024 , the learning rate lp=3e−4 , and the history states’ length l=2 . In order to prevent the model from overfitting, the early stop technique is introduced [44], and the result of training loss is shown in Fig. 5(a). The accuracy of the collision probability prediction model mentioned above is tested by the dataset generated by the policy, which is trained by the DDPG. The test dataset contains 100 000 tuples and △T∈{1,2,3} . When the output is larger than 0.6, the input tuple is recognized as True. Fig. 5(b) shows the accuracy arrives maximization around 90% with △T=1 .


Fig. 5.
(a) Training loss of the collision probability network. (b) Performance of the collision probability prediction model.

Show All

C. Evaluation on the SAC-CPP
In this section, the experiments are carried out with the SAC and the SAC-CPP, respectively. The network structure is shown in Fig. 4. Both algorithms have the same hyperparameter γ=0.99 , batch size N=256 , and the learning rates are initialized as lractor=1e−4 , lrcritic=2e−4 , and lrα=1e−4 . In the training process, the learning rates are multiplied by a decay factor to ensure the actor network converges to a safe policy.

1) Performance During Training:
As shown in Fig. 6, the policy trained by the SAC-CPP has a lower collision probability in most time compared with the SAC. Fig. 7(a) shows that the SAC-CPP attains a higher average success rate than the SAC during the training process. The performance of SAC-CPP during the training process is evaluated by the average success rate, which is calculated for every 100 episodes. As shown in Fig. 7(a), the average success rate of SAC-CPP is around 80% after 1000 episodes, while the SAC is approximately 75%. In the training process, the SAC-CPP calculates the probability of collision at every step and treats it as a risk. Consequently, the policy trained by the SAC-CPP generates safer actions and achieves a higher success rate.


Fig. 6.
Average probability of collision probability to six different scenarios with random targets. The x -axis is the step number to targets, and the y -axis is the average probability of five experiments.

Show All


Fig. 7.
(a) Average success rate of SAC and SAC-CPP. (b) Average success rate of SAC-CPP with different values of RSPs.

Show All

2) Comparison With Different Values of RSPs:
As mentioned in Section II-D, when the value of RSP is negative, the policy tends to risk-aversion. In this article, a negative RSP is chosen to train the policy, which implements risk-aversion actions. Fig. 7(b) shows the average success rate of the SAC-CPP with different values of RSPs. In the case of less risk consideration, the absolute value of RSP is small, and vice versa. Since the safe zone is different between various initial positions and target positions, the randomly initialized RSP value may make the autonomous vehicle drive into the obstacle. As shown in Fig. 7(b), the policy is trained with ξ∈{−5,−10,−15} , respectively. The policy with ξ=−15 obtains a higher success rate in the training process than policies with other values of ξ . When the larger value of ξ is set, the policy attains a lower success rate in the early stages.

D. Evaluation on the LSAC-CPP
In this section, the performance of LSAC-CPP is evaluated and compared with the SAC and the SAC-CPP. The undiscounted sum of safety costs of episodes is utilized as the measure of safety. The goal is to suppress this measure to zero, i.e., zero violation of the state constraints. Since both the SAC and the SAC-CPP do not restrict the state trajectory of the policy within the constraint of the sum of safety costs, their state trajectories are more likely to enter a dangerous zone. As a consequence, the LSAC-CPP takes fewer global steps than the SAC and the SAC-CPP to reach convergence. As shown in Fig. 8, the proposed algorithm quickly converges to a safe policy while maintaining a higher average return compared with the SAC and the SAC-CPP, which infers that the proposed algorithm has a higher success rate.


Fig. 8.
Average return of SAC, SAC-CPP, and LSAC-CPP after 100 000 steps, where the shaded areas show the 1-SD confidence intervals over ten random seeds. The x -axis indicates the total time steps in 1000.

Show All

SECTION V.Conclusion
In this article, a safe motion planning algorithm for autonomous vehicles is developed by combining the method, which utilizes a neural network to predict collision probability with the SAC. Different from the SAC, the proposed algorithm considers the probability of collision in the reward to strike a balance between maximizing the return and minimizing the cumulative risk in the training process. In addition, a Lyapunov-based method is introduced to further ensure safety. Based on this, an end-to-end algorithm is proposed, which directly maps the raw data to actions. The proposed algorithm and the SAC are implemented on a differential drive vehicle in gym-gazebo, respectively. The experimental results demonstrate several advantages of the proposed algorithm in terms of success rate, collision avoidance performance, and average return. Future work includes: 1) applying the proposed algorithm to navigation in the real world and 2) considering the motion planning of autonomous vehicles in uncertain and dynamic environments.

Appendix Lyapunov-Based Soft Actor Critic With Collision Probability Prediction
As in [16], the soft value function V in this article can be defined as
V(s)=Ea∼π[Q(s,a)−αlog(π(a|s))](A.1)
View Sourcewhich augments a standard value function with the entropy of policy at the given state. In practice, the Q -functions are updated by minimizing
J(Q)=ED12[r(s,a)+γV(s′)−Q(s,a)+ξfφ(dt−l:t−1,dt,at−l:t−1,at)]2.(A.2)
View Source

In our implementation, two soft Q -functions Q1 and Q2 are used and the target networks Q′1 and Q′2 are constructed accordingly. Following the objective function of actor (23), the gradient estimator in the double Q -function implementation is obtained as
∇θJ(π)=ED[∇θβlog(πθ(a|s))+∇at(βlog(πθ(a|s)+ξfφ(dt−l:t−1,dt,at−l:t−1,at)−miniQi(s,a)∇θfθ(ϵ′,s)]+EDΔ[λ∇a′Lc(s,a)∇θfθ(ϵ,s′)]+νEP(s|ρ,π,t0),P(t0|ρ,π),π∇aLc(s,a)∇θfθ(ϵ,s).(A.3)
View Source

The policy gradient is composed of two parts, the gradient estimated by the Q-function based on the samples from replay buffer D and that estimated by the Lyapunov critic based on the samples from the edge buffer DΔ . In many tasks, the violation of constraints only happens after the agent mastering a certain level of skills. This implies that, with a randomly initialized policy, the content of DΔ hardly grows, while D is ready for the updates. To overcome this inconvenience, an initial objective function J^(π) is set by setting ∇ and ν to zero until DΔ has stored enough tuples. In practice, it is data-efficient and does not endanger the agent. Another point to be noted is that the policy gradient is estimated by the Q-function with a lower value, which is found to be useful in mitigating performance degradation caused by the bias in the value estimation [16].

For the learning of the Lyapunov critic, the objective function is used as follows:
J(Lc)=E(s,a)∼D[12(Lc(s,a)−Ltarget(s,a))2](A.4)
View Sourcewhere Ltarget is the approximation target for Lc . For the case of using value function as the Lyapunov function
Ltarget(s,a)=c(s,a)+γL′c(s′,f(ϵ′,s′))(A.5)
View Sourcewhere L′c is the target Lyapunov critic function as typically used in the actor critic methods [15], [16], which has the same structure with Lc , but the parameter is updated through the exponentially moving average of weights of Lc controlled by a hyperparameter τ .

The values of Lagrangian multipliers β , λ , and ν are adjusted by the gradient ascent method to maximize the following objectives:
J(β)=J(λ)=J(ν)=βE(s,a)∼D[log(πθ(a|s)+H)]λE(s,a)∼D[Lc(s′,fθ(ϵ,s′))−Lc(s,a)+α3c(s,a)]νEP(s|ρ,π,t0),P(t0|ρ,π),π(Lc(s,a)−e).(A.6)(A.7)(A.8)
View Source

