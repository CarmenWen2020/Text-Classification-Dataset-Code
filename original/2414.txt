Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.

SECTION I.Introduction
In the past few years, the computer vision community has witnessed a significant amount of applications in video search and retrieval, surveillance, robotics, and so on. Regular machine learning approaches [1]–[2][3][4][5][6][7] have achieved promising results under the major assumption that the training and testing data stay in the same feature space or share the same distribution. However, in real-world applications, due to the high price of human manual labeling and environmental restrictions, sufficient training data belonging to the same feature space or the same distribution as the testing data may not always be available. Typical examples are [8]–[9][10][11], where only one action template is provided for each action class for training, and [12], where training samples are captured from a different viewpoint. In such situations, regular machine learning techniques are very likely to fail. This reminds us of the capability of the human vision system. Given the gigantic geometric and intraclass variabilities of objects, humans are able to learn tens of thousands of visual categories in their life, which leads to the hypothesis that humans achieve such a capability by accumulated information and knowledge [13]. It is estimated that there are about 10–30 thousands object classes in the world [14] and children can learn 4–5 object classes per day [13]. Due to the limitation of objects that a child can see within a day, learning new object classes from large amounts of corresponding object data is not possible. Thus, it is believed that the existing knowledge gained from previous known objects assists the new learning process through their connections with the new object categories. For example, assuming we did not know what a watermelon is, we would only need one training sample of watermelons together with our previous knowledge on melons-circular shapes, the green color, and so on, to remember the new object category watermelon. Transfer learning mimics the human vision system by making use of sufficient amounts of prior knowledge in other related domains when executing new tasks in the given domain. In transfer learning, both the training data and the testing data can contribute to two types of domains: 1) the target domain and 2) the source domain. The target domain contains the testing instances, which are the task of the categorization system, and the source domain contains training instances, which are under a different distribution with the target domain data. In most cases, there is only one target domain for a transfer learning task, while either single or multiple source domains can exist. For example, in [15], action recognition is conducted across data sets from different domains, where the KTH data set [16], which has a clean background and limited viewpoint and scale changes, is set as the source data set, and the Microsoft research action data set1 and the TRECVID surveillance data [17], which are captured from realistic scenarios, are used as the target data set. In [18], the source and target data sets are chosen from different TV program channels for the task of video concept detection.

Transfer learning can be considered as a special learning paradigm where partial/all training data used are under a different distribution with the testing data. To understand the significance of knowledge transfer in terms of visual learning problems, the literature, (see [19]–[20][21]) has concluded three general issues regarding the transfer process: 1) when to transfer; 2) what to transfer; and 3) how to transfer. First, when to transfer includes the issues whether transfer learning is necessary for specific learning tasks and whether the source domain data are related to the target domain data. In the scenarios of [22]–[23][24], where training samples are sufficient and impressive performance can be achieved, while being constrained in the target domains, including another domain as the source domain becomes superfluous. A variety of divergence levels exist across different pairs of source domain and target domain data, brute-forcing the knowledge from the source domain into the target domain irrespective of their divergence would cause certain performance degeneration, or, in even worse cases, it would break the original data consistency in the target domain. Second, the answer to what to transfer can be concluded in three aspects: 1) inductive transfer learning, where all the source domain instances and their corresponding labels are used for knowledge transfer; 2) instance transfer learning, where only the source domain instances are used; and 3) parameter transfer learning, in addition to the source domain instances and labels, some parameters of prelearned models from the source domain are utilized to help improve the performance in the target domain. Finally, how to transfer includes all the specific transfer learning techniques, and it is also the most important part that has been studied in the transfer learning literature. Many transfer learning techniques have been proposed, e.g., in [25]–[26][27], where knowledge transfer is based on the non-negative matrix trifactorization framework, and in [28], where the transfer learning phase is via dimensionality reduction. We illustrate the basic frameworks of traditional machine learning approaches and knowledge transfer approaches in Fig. 1. For traditional machine learning approaches, the ideal choice of the training set to predict a testing instance car should contain cars. However, in the case of knowledge transfer, the training set can just contain some relevant categories rather than cars, e.g., wheels, which are similar to the wheels of cars; bicycles, which share the knowledge of wheels with the car wheels, or even some irrelevant objects, e.g., laptops and birds, which seem to have no connections with cars, but actually share certain edges or geometrical layouts with local parts of a car image.



Fig. 1.
Basic frameworks of traditional machine learning approaches and knowledge transfer approaches. For regular machine learning approaches, the learning system can only handle the situation that testing samples and training samples are under the same distribution. On the other hand, transfer learning approaches have to deal with the data distribution mismatch problem through specific knowledge transfer methods, e.g., mining the shared patterns from data across different domains.

Show All

As the age of big data has come, transfer learning can provide more benefits to solve the target problem with more relevant data. Thus, it is believed that more applications on transfer learning will emerge in future research. This survey aims to give a comprehensive overview of transfer learning techniques on visual categorization tasks, so that readers could potentially use the analysis and discussions in this survey to understand how transfer learning can be applied to visual categorization tasks or to solve their problem with a suitable transfer learning method. The visual categorization tasks possess some unique characteristics due to certain visual properties that can be potentially used in the training process, e.g., the appearance or shape of an object part, the local symmetries of an object, and the structural. All these unique properties can be employed when designing transfer learning algorithms, which makes our work different from that of [19] and [29], where the former focuses on classification, regression and clustering problems related to data mining tasks and the latter focuses on reinforcement learning, which addresses problems with only limited environmental feedback rather than correctly labeled examples.

The remaining part of this survey is structured as follows. An overview is given in Section II. In Sections III and IV, two transfer learning categories, which execute knowledge transfer through feature representations and classifiers, are discussed in detail, respectively, answering the problems of what to transfer and how to transfer. In Section V, the model selection methods from multiple source domains, i.e., when to transfer, are discussed. Evaluation, analysis, and discussions on the stated transfer learning methods are given in Section VI. Finally, the conclusions are drawn in Section VII.

SECTION II.Overview
A. Developing Interests on Transfer Learning
Dating from the raising of its notion in the last century, transfer learning (also known as, cross-domain learning, domain transfer, and domain adaptation) has a long history of being studied as a particular machine learning technique. In recent years, with the information explosion on the Internet, (e.g., audio, images, and videos) and the growing demands for target tasks in terms of accuracies, data scales, and computational efficiencies, transfer learning approaches begin to attract increasing interests from all research areas in pattern recognition and machine learning. When regular machine learning techniques reach their limits, transfer learning opens the flow of a new stream that could fundamentally change the way of how we used to learn things and how we used to treat classification or regression tasks. Along with the flow, some workshops and tutorial have been held (such as the NIPS 1995 postconference workshop2 in machine learning and data mining areas and another transfer learning survey is given in [29] for reinforcement learning). In this survey, we focus on the applications of transfer learning techniques to visual categorization, including action recognition, object recognition, and image classification.

B. Notations and Issues
Some general notations are defined as follows for later usage: let DT=DTl∪DTu denote the target domain data, where the partially labeled parts are denoted by DTl and the unlabeled parts are denoted by DTu . In addition to the target domain data, a set of auxiliary data is seen as the source domain data, which is semilabeled or fully labeled and has the representation Ds={(xi,yi)}ai=1 in a single source case, and Ds1,Ds2,…,DsM with Dsk={(xki,yki)}Naki=1 in a multiple source case. Here, xi∈Rd is the ith feature vector, where d denotes the data dimension, and yi denotes the class label of the i th sample.

According to prior proposals, common issues regarding knowledge transfer are twofold. First, the auxiliary samples are typically treated without accounting for their mutual dependency during adaptation, which may cause the adapted data to be arbitrarily distributed and the structural information beyond single data samples of the auxiliary data may become undermined. Second, during adaptation, noises, and particularly possible outliers from the auxiliary domains are blindly forced to the target domain [30].

When transferring knowledge from the auxiliary domains to the target domain, it is crucial to know the distribution similarities between the target domain data and each source domain data. So far, the most common criterion to measure the distribution similarity of two domains is a nonparametric distances metric named maximum mean discrepancy (MMD). The MMD is proposed in [31], and it compares data distributions in the reproducing kernel Hilbert space
Distk(Ds,DT)=∥∥∥1ns∑i=1nsϕ(xsi)−1nT∑i=1nTϕ(xTi)∥∥∥2(1)
View Sourcewhere ϕ(⋅) is the feature space mapping function.

In the literature, transfer learning techniques are categorized according to a variety of taxonomies. In [19], considering tasks allocated to the target domain and auxiliary domains and the availability of sample labels within the target domain and auxiliary domains, transfer learning techniques are first grouped as inductive transfer learning, transductive transfer learning, and unsupervised transfer learning, upon which they are further categorized as instance-transfer, feature representation transfer, parameter transfer, and relational knowledge transfer within each initial partition. Fig. 2 shows five ways of differentiating existing knowledge transfer approaches for visual categorization. In this survey, inheriting the concepts from the computer vision community, we simply categorize transfer learning techniques into feature representation level knowledge transfer and classifier level knowledge transfer.



Fig. 2.
Different ways of differentiating existing knowledge transfer approaches.

Show All

SECTION III.Feature Representation Transfer
Feature representation level knowledge transfer is a popular transfer learning category that maps the target domain to the source domains exploiting a set of meticulously manufactured features. Through this type of feature representation level knowledge transfer, data divergence between the target domain and the source domains can be significantly reduced so that the performance of the task in the target domain is improved. Most existing transductive features are designed for specific domains and would not perform optimally across different data types. Thus, we review the feature level knowledge transfer techniques according to two data types: 1) cross-domain knowledge transfer and 2) cross-view knowledge transfer.

A. Cross-Domain Knowledge Transfer
In the cross-domain setting, the gap between the source domain data and the target domain data varies from images to videos and from objects to edges. According to the degree of data divergence, different approaches are proposed. In [15], knowledge transfer is made between the KTH data set [16], the TRECVID data set [17] and the Microsoft research action data set II (MSRII), where the KTH data set is seen as the target domain and both the TRECVID data set and the MSRII data set are used as the source domains. The KTH data set is limited to clean backgrounds and a single actor and each video sequence exhibits one individual action from the beginning to the end. On the other hand, the TRECVID data set and the MSRII data set are captured from realistic scenarios, with cluttered backgrounds and multiple actors in each video sequence. To take advantage of the labeled training data from both the target domain and the source domain, Daumé [32] proposed the feature replication (FR) method using augmented feature for training. Inspired by [33], which applies the Gaussian mixture model (GMM) to model the visual similarities between images or videos, the work in [15] models the spatial temporal interests points (STIPs) with the GMM and introduces a prior distribution of the GMM parameters to generate probabilistic representations of the original STIPs. Such representations can accomplish the adaptation from the source domains to the target domain. The basic setting of [34] assumes that there are labeled training data in the source domain, but no labeled training data in the target domain. Furthermore, the activities in the source domain and the target domain do not overlap, so that traditional supervised learning methods cannot be applied in this scenario. Utilizing the Web pages returned by search engines to mine similarities across the domains, the labeled data in the source domain are then interpreted by the label space of the target domain. In some extreme cases, the source domain data may not be relevant to the target domain data.

Sparseness has gained tremendous attention in various scientific fields, and computer vision is a dominant part of this trend. Sparse models can find their applications in a wide range of computer vision techniques, e.g., dictionary learning (DL) [35]–[36][37] and transfer learning. Raina et al. [38] apply sparse coding to unlabeled data to break the tremendous amount of data in the source domain into basic patterns, (e.g., edges in the task of image classification) so that knowledge can be transferred through the bottom level to form a higher level representation of the training samples in the target domain, in which case the source domain data do not necessarily need to be relevant to the target domain data. Since in the regular transfer learning formalism, the source domain data have to be relevant with the target domain data, such a knowledge transfer method is named self-taught learning rather than transfer learning. Zhu and Shao [39] present a discriminative cross-domain DL (DCDDL) framework that utilizes relevant data from other visual domains as auxiliary knowledge for enhancing the learning system in the target domain. The objective function is designed to encourage similar visual patterns across different domains to possess identical representations after being encoded by a learned dictionary pair. In the part-of-speech (POS) tagging tasks, shared patterns from auxiliary categorization tasks are extracted as pivot features, which represent the frequent words emerged in the speech and are themselves indicative of their corresponding categories [40]. While the pivot features are sensitive to the POS tagging tasks, pivot visual words do not exist in typical local histogram-based low-level visual features, which indicates that no single feature dimension of the histogram bins is discriminative enough to represent the difference of the visual categories [41].

On the other hand, some works also target to identify a new lower-dimensional feature space such that the auxiliary domain and the target domain manifest some shared characteristics [42]–[43][44], instead of transferring the entire knowledge across the target domain and auxiliary domains making such an assumption that the smoothness property (i.e., those data points close to each other are more likely to share the same label) is satisfied in low-dimension subspaces [41].

B. Cross-View Knowledge Transfer
Cross-view knowledge transfer can be seen as a special case of cross-domain knowledge transfer, where the divergences across domains are caused by view-point changes. The task is to recognize action classes in the target view using training samples from one or more different views. Generating view-invariant features to address the cross-view visual pattern recognition problems attracts significant attention in the computer vision field, especially for cross-view action recognition. The bottom of Fig. 3 shows the cross-view knowledge transfer scenario on the multiview IXMAS [45] data set. The typical setting is to use samples captured in one view (the source view) as training data to predict the labels of samples captured from a different view (the target view). The core methodology of approaches that tackle visual categorization problems with changes in the observer’s viewpoint is to discover the shared knowledge irrespective to such viewpoint changes. One common approach to attack the cross-view feature representation diversity problem is to infer 3-D scene structure for cross-view feature adaptation, where the derived features can be adapted from one view to another utilizing geometric reasoning [46]–[47][48][49]. Another family of approaches is to explore visual pattern properties, e.g., affine [50], projective [51], epipolar geometry [52]–[53][54], to compute such cross-view feature representations. On the other hand, Junejo et al. [55] applied a self-similarity matrix to store distances between different pairs of actions for a view-invariant representation. Spatial-temporal features of a video sequence that are insensitive to changes in view angle are studied in [12], [51], and [56]–[57][58].



Fig. 3.
Top row: cross-domain knowledge transfer scenario. In the target domain, the walking action performed by a single player in clean backgrounds comes from the KTH data set, while in the target domain, the walking action captured from much more complicated backgrounds with multiple players comes from the TRECVID data set. Bottom row: cross-view knowledge transfer scenario, where the target view data and the source view data are the same action captured from two different views of the IXMAS data set.

Show All

In [12], a bipartite graph is built via unsupervised co-clustering to measure the visual-word to visual-word relationship across the target view and the source view so that a high-level semantic feature that bridges the semantic gap between the two vocabularies can be generated. Beyond the bag-of-visual-words representation, which have been successfully applied to natural language processing, information retrieval, and computer vision, the proposed bag-of-bilingual-words representation discovers the shared set of common action concepts between two different views, even though the two view domains are highly independent. Similar to the work of [12], Li and Zickler [58] captured the conceptual idea of virtual views construction to represent an action descriptor continuously from one observer’s viewpoint to another. Another family of approaches is proposed in [59] and [60], where a pair of over-complete dictionaries are constructed utilizing correspondence samples across two view domains. Encouraged by the learned dictionary pair, the labeled source view data and unlabeled target view data are forced to the same feature space that satisfies the smoothness assumption.

We summarize the main characteristics of the feature representation level knowledge transfer approaches according to their adaptation methods, the target domain label, the source domain label, adaptation data types and applications, and list them in Table I. Among these approaches, [38], [59], and [61] utilize the sparseness property to generate sparse representations for data adaptation.

TABLE I Main Characteristics of Listed Feature Representation Level Knowledge Transfer Approaches. Availability of Both Target Domain Labels, Adaptation Type, and Applications of All Stated Feature Representation Level Knowledge Transfer Methods are Listed

SECTION IV.Classifier-Based Knowledge Transfer
Similar as the feature representation level knowledge transfer, classifier-based knowledge transfer is another significant part of existing visual transfer learning techniques and it has attracted much attention in recent years. However, unlike the feature representation level knowledge transfer techniques, where only the training samples themselves in the source domain are adapted to the target learning framework, classifier-based knowledge transfer methods share the common trait that the learned source domain models are utilized as prior knowledge in addition to the training samples when learning the target model. Instead of minimizing the cross-domain dissimilarity by updating instances’ representations, classifier-based knowledge transfer methods aim to learn a new model that minimizes the generalization error in the target domain via provided training instances from both domains and the learned model. We structure this section according to the following categories of classifier-based knowledge transfer techniques.

A. SVM-Based
Support Vector Machine (SVM) is a supervised learning method for solving classification and regression problems, and the majority of existing work on classifier-based knowledge transfer are constructed from the original SVM classifier. As a direct application of SVM, adaptive-SVM (A-SVM) [18], and projective model transfer SVM (PMT-SVM) [63] learn from the source model ws by regularizing the distance between the target model wt and the learned model ws . The A-SVM uses the following objective function:
LA=minwt,b∥wt−Γws∥2+C∑iNl(xi,yi;wt,b)(2)
View Sourcewhere yi∈{−1,1} indicates the corresponding labels, l(xi,yi;wt,b)=max(0,1−yi(wt⊤xi+b)) is the hinge loss, C controls the weight of the loss function, and Γ controls the amount of transfer regularization. By regularizing the distances between the two models, knowledge transfer for A-SVM is like a spring between Γws and wt , which is equivalent to providing samples from the source classes. By expanding the regularization term
∥wt−Γws∥2=∥wt∥2−2Γ∥wt∥cosθ+Γ2(3)
View Sourcewhere ∥w∥2 provides the margin maximization as in regular SVM and the second term −2Γ∥w∥cosθ induces the transfer by maximizing cosθ , i.e., by minimizing the angle θ between wt and ws . Instead of maximizing the term cosθ , knowledge transfer can be induced by minimizing the projection of wt onto the separating hyperplane orthogonal to ws for PMT-SVM using the following objective function:
LPMT=minwt,b∥wt∥2+Γ∥Pwt∥2+C∑iNl(xi,yi;wt,b)s.t.:w⊤tws≥0(4)
View Sourcewhere P=I−(wsw⊤s)/(w⊤sws) is the projection matrix. Compared with A-SVM, PMT-SVM can increase the amount of transfer (Γ) without penalizing margin maximization.

Opposed to the rigid transfer methods A-SVM and PMT-SVM, the deformable adaptive SVM (DA-SVM) [63] provides more flexible transfer regularization through a deformable source template, where small local deformations can be tolerated for the template fit of the source domain to the target domain. Aytar and Zisserman [63] used a simple example to explain such visual deformation in knowledge transfer that the wheel part of a motorbike template can be increased in radius and reduced in thickness when fitting to a bicycle wheel template. The DA-SVM can also be seen as the generalization form of the rigid A-SVM by replacing ws in (2) with τ(ws)
LDA=minf,wt,b∥wt−Γτ(ws)∥2+C∑iNl(xi,yi;wt,b)+λ(∑i≠jM,Mf2i,jdi,j+∑iM(1−fii)2d)(5)
View Sourcewhere di,j is the spatial distance between the i th and j th cell, d is the penalization for the additional flow from the i th source cell to the i th target cell, and τ(ws)i=∑Mjfijwsj is the flow transformation, where the parameter fij denotes the amount of transfer from the j th cell in the source template to the i th cell in the transformed template. The cells are extracted from local image regions, on which local descriptors, (e.g., HOG [64] and SIFT [65]) are computed. Thus, different from other classifier-based knowledge transfer techniques, DA-SVM has such a constraint that it has to be constructed using low-level visual features that measure the geometrical information of local image parts.

Tommasi et al. [66] proposed a discriminative transfer learning method based on least squares support vector machine (LS-SVM) that learns the new category through adaptation. By replacing the regularization term in classical LS-SVM, the new learning objective function for knowledge transfer is formulated as
LKTLS=minwt,b12∥wt−θws∥2+C2∑i=1l[yi−wtϕ(xi)−b]2(6)
View Sourcewhere θ is a scaling factor in the range of (0, 1) to control the degree of transfer across the learned model ws and the target model wt . When being extended to multimodel knowledge transfer (multi-KT), the scaling factor θ is substituted with the vector Θ={θ1,θ2,…,θk} , where each θj is the weight of a corresponding prior model. Thus, (6) can be rewritten as
LMulti-KT=minwt,b∥∥∥∥wt−∑j=1kθjwsj∥∥∥∥2+C2∑i=1lζi(yi−wt⋅ϕ(xi)−b)2.(7)
View SourceThe ζi in (7) is used for resampling the data so that training samples are balanced. Taking the advantage of LS-SVM that the leave-one-out (LOO) error, which measures the proper amount of knowledge to be transferred, can be written in a closed form [67], the best values of θj are those that minimize the LOO error.

Typically, the kernel functions need to be specified in advance to learning and the associated kernel parameters, (e.g., the mean and variance in the Gaussian kernel) are determined during optimization. On top of the various kernel learning methods [68]–[69][70][71], the domain transfer SVM (DT-SVM) [72] unified the cross-domain learning framework by searching for the SVM decision function f(x)=w′ϕ(x)+b as well as the kernel function simultaneously instead of the two-step approaches [28], [73]. In general, DT-SVM achieves cross-domain classification by reaching two objective criteria: 1) DT-SVM minimizes the data distribution mismatch between the target domain and source domains using the MMD criterion mentioned in Section II and 2) DT-SVM pursues better classification performance by minimizing the structural risk of SVM. By meeting both criteria, an effective kernel function can be learned for better separation performance in linear space over different domains, and thus samples from the source domains are infused to the target domain to improve the classification performance of the SVM classifier.

B. Tradaboost
Adaptive boosting (AdaBoost) [74] is a popular boosting algorithm, which has been used in conjunction with a wide range of other machine learning algorithms to enhance their performance. At every iteration, AdaBoost increases the accuracy of the selection of the next weak classifier by carefully adjusting the weights on the training instances. Thus, more importance is given to misclassified instances since they are believed to be the most informative for the next selection. The transfer learning AdaBoost (TrAdaBoost) is introduced in [21] to extend AdaBoost for transfer learning by weighting less on the different-distribution data, which are considered as dissimilar to the same-distribution data in each boosting iteration. The goal of TrAdaBoost is to reduce the weighted training error on the different-distribution data, and meanwhile preserving the properties of AdaBoost. Since the quality of different-distribution data is not certain, the performance of TrAdaBoost cannot be always guaranteed to outperform AdaBoost.

C. Generative Models
The learning to learn concept via rich generative models has emerged as one promising research area in both computer vision and machine learning. Recently, researchers have begun developing new approaches to deal with transfer learning problems using generative models. One workshop in conjunction with NIPS 2010 was held specifically for the discussion of transfer learning via rich generative models. In general, the generative knowledge transfer methods can lead to higher-impact transfer, including more information than those discriminative approaches and they can be more adaptive to a single specific task.

Fei-Fei et al. [75] proposed a Bayesian-based unsupervised one-shot learning object categorization framework that learns a new object category using a single example (or just a few). Since Bayesian methods allow us to incorporate prior information about objects into a prior probability density function when observations become available, general information coming from previously learnt unrelated categories is represented with a suitable prior probability density function on the parameters of the probabilistic models. Thus, priors can be formed from unrelated object categories. For example, when learning the category motorbikes, priors can be obtained by averaging the learnt model parameters from other three categories spotted cats, faces, and airplanes, so that the hyperparameters of the priors are then estimated from the parameters of the existing category models. Yu and Aloimonos [76] applied the generative author-topic [77] model to learn the probabilistic distribution of image features-based object attributes. Since object attributes can represent common properties across different categories, they are used to transfer knowledge from source categories to target categories. Both the zero-shot learning problem and the one-shot learning problem are addressed, where in the first problem, the attribute model learned from the source domain categories is used to generate synthesized target training examples through the generative process, and in the second problem, the learned attribute model is used to reduce the uncertainty of parameters of the Dirichelt priors.

D. Fuzzy System-Based Models
Transfer learning also finds its application in fuzzy systems. Deng et al. [78] and [79] proposed two knowledge-leverage-based fuzzy system models, respectively. The former is based on the Takagi–Sugeno–Kang fuzzy system, and the latter is based on the reduced set density estimator-based Mamdani–Larsen-Type fuzzy system. In both works, the training set is decomposed to training data of the current scene and model parameters of reference scenes. The same knowledge leverage strategy is adopted by both works, where model parameters obtained from the reference scenes are fed to the current scene for parameter approximation. The knowledge leverage strategy is performed through a unified objective function, which emphasizes on both learning from the data of the current scene and transferring model parameters from reference scenes.

1) Discussion:
The stated SVM-based knowledge transfer methods can act as a plug in to the SVM training process. A common trait shared amid these methods according to their objective functions is that they all include a regularization term that measures the similarity between the learned model and the target model. In A-SVM, PMT-SVM, and DA-SVM, Γ is the tradeoff parameter between margin maximization and knowledge transfer, so it defines the amount of transfer regularization. The DA-SVM is specialized in dealing with the transfer of visually deformable templates, while A-SVM and PMT-SVM are more likely to be generalized. The advantage of PMT-SVM over A-SVM is that it can increase the amount of transfer without penalizing margin maximization, while A-SVM encourages ∥w∥ to be larger when increasing Γ . A large ∥w∥ indicates small margins to the hyperplane, and thus the generalization error of the classifier fails to gain an optimal bound. In general, PMT-SVM is expected to outperform A-SVM.

Compared with SVM-based approaches, the boosting-based method, TrAdaBoost, is simpler in terms of implementation, and it does not require the parameters from the prelearned models. Like other boosting-based techniques, TrAdaBoost has a fairly strong generalization ability. However, TrAdaBoost relies heavily on the relevance of the source domain data to the target domain data, thus it is vulnerable to negative transfers. In addition, TrAdaBoost can easily overfit in the presence of noise in either domain. The generative models are more adaptive to a specific task, however, but computationally more complex.

SECTION V.Model Selection in Knowledge Transfer
In real-world applications, knowledge transfer techniques have to consider more complicated scenarios than adapting the samples or prelearned models from a single source domain to obtain the target learner. In the first case, more than one source domains are available yet we have no idea which source domain contains more useful information that potentially improves the target learner or whether the knowledge in a specific domain is against the smoothness property in the target domain. On the other hand, in visual categorization tasks, the shared information across the two domains can be hidden in different visual forms, e.g., appearance, local symmetry, and layout, which can be captured by different feature descriptors. A fusion strategy is required to mine the most helpful knowledge from multiple features. The third case is that some knowledge transfer techniques are constructed from prelearned models, e.g., a learned bicycle classifier or a learned bird classifier, and these models can lead to different scales of contributions to the target model. In advance to knowledge transfer, the bad prelearned models need to be filtered out so that the good models can achieve more effective transfer. All the above three cases generalize the common many-to-one adaptation situations in knowledge transfer, and they can all be deemed as the model selection problem. Fig. 4 shows a typical example of multisource binary classification. A straightforward approach to reduce such prediction ambiguity is to measure the model similarity between each auxiliary domain and the target domain, and apply the closest model for prediction in the target domain, i.e., if auxiliary domain 1 is more similar with the target domain, the decision boundary in Fig. 4(c) will inherit the decision boundary in Fig. 4(a). However, data in auxiliary domain 2, which also contain useful information for the prediction of target domain data, are abandoned.


Fig. 4.
Knowledge transfer from multiple auxiliary domains. (a) Auxiliary domain 1. (b) Auxiliary domain 2. (c) Target domain. The two subfigures on the left denote the two different auxiliary domain data and their corresponding decision boundaries, where auxiliary domain 1 is partitioned by a horizontal line and auxiliary domain 2 is partitioned by a vertical line. By brutally combining the decision boundaries from the two auxiliary domains, ambiguous predictions will be caused in the top-left region and the bottom-right region of the target domain.

Show All

In general, extending the existing single-source knowledge transfer techniques to the multiple-source scenario can evoke two challenges: 1) how to leverage the distribution differences among multiple source-domains to promote the prediction performance on the target domain task? and 2) how to extend the single-source knowledge transfer techniques to a distributed algorithm, while only sharing some statistical data of all source domains instead of revealing the full contents? Since most existing multiple-source knowledge transfer methods are extended from their corresponding single-source algorithms, we structure this section in a similar manner as Sections III and IV.

A. SVM-Based
In the one-to-one adaptation scenario of A-SVM [18], the new target classifier fT(x) is adapted from the existing source classifier fs(x) using the form
fT(x)=fs(x)+△f(x)(8)
View Sourcewhere the perturbation function △f(x) is learned using the labeled data DTl from the target domain. Intuitively, when encountering with multiple source domains Ds1,Ds2,…,DsM , which are assumed to possess similar distributions to the primary domain Dt , the adapted classifier can be constructed using the ensemble of all the source domain classifiers fs1(x),fs2(x),…,fsM(x)
fT(x)=∑k=1Mγkfsk(x)+△f(x)(9)
View Sourcewhere γk∈(0,1) is the predefined weight of each source classifier fsk(x) , which sums to one: ∑mk=1γk=1 . The MMD criterion can be applied for obtaining the value of γk . The perturbation function can be formulated as △f(x)=∑nli=1αTiyTik(xTi,x) , where αTi is the coefficient of the i th labeled pattern in the target domain and k(⋅,⋅) is a kernel function induced from the nonlinear feature mapping ϕ(⋅) . When applying the same kernel function to the source classifiers, (9) can be expanded as
fT(x)=∑sγs∑i=1nlαsiysik(xTi,x)+∑i=1nlαTiyTik(xTi,x),(10)
View Sourcewhich is the sum of a set of weighted kernel evaluations between the test pattern x and all labeled patterns xTi and xsi , respectively, from the target domain and all the source domains. Obviously, the learning process is inefficient when being applied to large-scale data sets, which is the first disadvantage of A-SVM on the many-to-one adaptation application. The second disadvantage of A-SVM is its failure on using the unlabeled target domain data DTu .

Duan et al. [72] proposed the domain adaptation machine (DAM) to overcome the two disadvantages of A-SVM. To utilize the unlabeled target domain data DTu , a data-dependent regularizer is defined for the target classifier fT
Ω(fTu)=12∑s=1Sγs∑i=1m(fTi−fsi)2(11)
View Sourcewhere fTu=[fTnl+1,…,fTnT]′ and fsu=[fsnl+1,…,fsnT]′ are defined as the decision values from the target classifier and the s th source classifier, respectively. Based on the smoothness assumption for domain adaptation, DAM minimizes the structural risk function of LS-SVM as well as the data-dependent regularizer simultaneously. DAM is formulated as
minfTΩ(fT)+12∑i=1nl(fTi−yTi)2+ΩD(fTu)(12)
View Sourcewhere Ω(fT) is a regularizer to control the complexity of the target classifier fT . Since the target classifier in DAM is learned in a sparse representation, the computation inefficiency problem of A-SVM is overcome.

By arguing that it is more beneficial to transfer from a few relevant source domains rather than using all the source domains as in A-SVM and DAM, Duan et al. [80] further design a new data-dependant regularizer in domain selection machine (DSM) for source domain selection
Ω(f)=12∑s=1Sds∑i=1m(fTi−fsi)2.(13)
View SourceSimilar as γs in (11), which is a predefined weight measuring the relevance between the s th source domain and the target domain, ds∈{0,1} in (13) is a domain selection indicator for the s th source domain. When the objective function is optimized, the value of ds is 1 if the s th source domain is relevant to the target domain, and the value of ds is 0 otherwise. Another advantage of DSM over most existing transfer learning methods is its ability to work when the source domains and the target domain are represented by different types of features, e.g., using static 2-D SIFT features to represent the source domain data and 3-D spatio-temporal (ST) features to represent the target domain data. The learning function of DSM can be formulated as
f(x)=f2D(x)+f3D(x)=∑s=1Sdsβsfs(x)+w′φ(x)+b
View Sourcewhere f2D(x)=∑Ss=1dsβsfs(x) is a weighted combination of source classifiers based on SIFT features, βs is a real-valued weight for the s th source domain, f3D(x)=w′φ(x)+b is the adaptation error function of space-time features, φ(⋅) is a feature mapping function that maps x into φ(x) , w is a weight vector, and b is a bias term.

B. Boosting-Based
As discussed in Section IV-B, TrAdaBoost relies only on one source domain, which makes it intrinsically vulnerable to negative samples in the source domain. To avoid such a problem, Yao and Doretto [81] proposed two boosting approaches multisource-TrAdaBoost and task-TrAdaBoost for knowledge transfer with multiple source domains.

Multisource-TrAdaBoost is an extension of TrAdaBoost to multiple source domains. Instead of searching for a weak classifier by leveraging a single source domain, a mechanism is introduced to apply all the weak classifiers in the selected source domain that appears to be the most relevant to the target domain at the current iteration. Specifically, the training data of each source domain are combined with the training data in the target domain to generate a candidate weak classifier at each iteration, while all the source domains are considered independent from each other. Thus, the multisource-TrAdaBoost approach significantly reduces the effects of negative transfer caused by the imposition to knowledge transfer from a single source domain, which is potentially not relevant to the target domain.

On the other hand, task-TrAdaBoost is a parameter-transfer approach, that tries to identify which parameters that come from various source domains can be used. Task-TrAdaBoost is constituted of two separate phases. In phase-I, traditional AdaBoost is employed to extract suitable weak classifiers from each source domain, respectively, under the assumption that some parameters are shared between the source domain and the target domain. Thus, the source domain is described explicitly rather than implicitly with only the labeled source domain data. Phase-II runs the AdaBoost loop again over the target training data using the collection of all the candidate weak classifiers obtained from phase-I. At each iteration, the weak classifier with the lowest classification error on the target training data is picked out to ensure the knowledge being transferred is more relevant to the target task. In addition, the update of the weights on the target training data drives the search of the most helpful candidate classifiers in the next round for boosting the target classifier.

C. Multikernel Learning
There are many types of hidden knowledge that can be transferred across different visual domains, for example, the appearance or shape of an object part, (e.g., the shape of a wheel), local symmetries between parts, (e.g., the symmetry between front- and back-legs for quadrupeds), and the partially shared layout, (e.g., the layout of torso and limbs of a human). When employing knowledge transfer between the visual domains, though the shared knowledge exists among the target data and the source data, the exact type of knowledge that needs to be transferred is uncertain. Alternately, since these different types of knowledge can be represented by different features or different prior models, all types of knowledge can be considered by fusing these features or prior models when constructing the target model. Instead of using predefined weights for all the features or prior models, multikernel learning provides a more appropriate solution by learning the linear combination of coefficients of the prelearned classifiers to assure the minimization of domain mismatches.

Motivated by A-SVM, Duan et al. [82] proposed an adaptive multiple kernel learning (A-MKL) method to cope with the considerable variation in feature distributions between videos from two domains. As described above, in A-SVM, the target classifier is adapted from an existing classifier trained with the source domain data. When A-SVM employs multiple source classifiers, those classifiers are fused with fixed weights. Different from A-SVM, A-MKL learns the optimal combination of coefficients corresponding to each prelearned classifier to minimize the mismatch between the data distributions of two domains under the MMD criterion.

The multimodel knowledge transfer (multi-KT) [66] method modifies the l2 -norm regularizer in the LS-SVM objective function and constrains the new hyperplane w to be close to hyperplanes of F prior models. The regularization term is given as ∥w−∑Fj=1βjμj∥ , where μj is the hyperplane of the j th model, and βj determines the amount of transfer from each model, while subjecting to the constraint that ∥β∥2≤1 . For a sample x , the decision function is given by
s(x)=w⋅ϕ(x)+∑j=1Fβjμj⋅ϕ(x).(14)
View SourceWhile the solution to multi-KT is through two separate optimization problems, Jie et al. [83] proposed a multiple kernel transfer learning (MKTL) method that learns the best hyperplanes and corresponding weights assigned to each prior model in a unified optimization process. The MKTL utilizes the prior knowledge as experts evaluating the new query instances and addresses such a knowledge transfer problem with a multikernel learning solver. In addition to the training sample xi , the prediction score sp(xi,z),z=1,…,F (F is the total number of classes), predicted by the prior models are considered when learning the new model. The intuition behind such an idea is that if prior knowledge of a bicycle gives a high prediction score to images of a motorbike, this information may also be useful for the new model of motorbikes, since certain visual parts, (e.g., the wheels) are shared between the two categories. Priors are built over multiple features instead of only one, and meanwhile, different learning methods are considered.

D. Cross-View Multiple Source Adaptation
For the cross-view action recognition problem, some shared visual patterns (either spatial or ST) can exist in actions captured from more than one view-points, thus transferring knowledge from multiple source views to the target view is more beneficial rather than transferring from a single view.

Liu et al. [12] apply the locally weighted ensemble (LWE) approach introduced in [45] to fuse the multiple classification models. Specifically, for a set of prelearned models f1,f2,…,fk , the general Bayesian model averaging approach computes the posterior distribution of y as P(y|x)=∑ki=1P(y|x,D,fi)P(fi|D) , where P(y|x,D,fi)=P(y|x,fi) is the prediction made by each model and P(fi|D) is the posterior of model fi after observing the training set D . Considering the data distribution mismatch across the target domain and the source domains, the model prior for P(fi|T) is incorporated, where T is the test set. By replacing P(fi|D) with P(fi|T) , the difference between the target and the source domains are considered during learning
P(y|x)=∑i=1kwfi,xP(y|x,fi)(15)
View Sourcewhere wfi,x=P(fi|x) is the true model weight that is locally adjusted for x representing the model’s effectiveness on the target data.

Li and Zickler [58] achieve multiview fusion by aggregating the response values from the w MKL-SVM [69] classifiers on their corresponding cross-view features x^ , beyond which a binary decision is made. Similar as the idea in MKTL [83], MKL-SVM solves a standard SVM optimization problem, where the kernel is defined as a linear combination of multiple kernels.

1) Discussion:
The multiple source A-SVM is an intuitive extension of A-SVM that it assembles all the source domain classifiers by allocating a weight γk to each source classifier. The DAM and DSM are proposed to overcome the disadvantages of multiple source A-SVM in both inefficiency and the failure of using unlabeled target domain data, where DSM precedes over DAM by filtering out those less relevant source domain data.

By introducing multiple source domains rather than one in both multisource-TrAdaBoost and task-TrAdaBoost, the first imperfection of TrAdaBoost has been compensated. The convergence properties of multisource-TrAdaBoost can be inherited directly from TrAdaBoost [21], whereas for task-TrAdaBoost they can be inherited directly from AdaBoost [74]. It has been proved in [81] that since the convergence rate of task-TrAdaBoost has a reduced upper bound compared with multisource-TrAdaBoost, it requires fewer iterations to converge.

Compared with A-SVM, the unlabeled data in the target domain are used in the MMD criterion of A-MKL, and the weights in the target classifier are learned automatically together with the optimal kernel combination. Calling the theorem in [84], for the binary-class classification of multi-KT, multi-KT is equivalent to multiple source A-SVM based on the Mahalanobis distance measure [85]. Since the relationship between A-SVM and PMT-SVM is demonstrated in (2)–(4), the connection between multi-KT and PMT-SVM can be naturally discovered.

SECTION VI.Evaluation, Analysis, and Discussion
In general, there are three types of benefits that transfer learning can provide for performance improvements [66], [86], including: 1) higher start—improved performance at the initial points; 2) higher slope—more rapid growth of performance; and 3) higher asymptote—leading to improved final performance. In the following, several simple experiments are conducted with some selected representative knowledge transfer techniques discussed above to make a comparison between these methods and to see whether they can meet the stated criteria.

A. Feature-Level Knowledge Transfer Methods
Comparison between different feature representation cross-view transfer learning methods is given in Tables II and III, where experiments are conducted on every possible pairwise view combination of the IXMAS data set (i.e., twenty combinations in total) and columns demonstrate the results of target views, while rows demonstrate the results of auxiliary training views. According to previous cross-view action recognition works, there are two different experimental settings, which are the correspondence mode and the partially labeled mode. In the correspondence mode, the leave-one-action-class-out scheme is applied, where one action class is considered as the orphan action in the target view, while all action videos of the selected class are excluded when establishing the correspondences. Approximately 30% of the nonorphan samples are randomly selected to serve as the correspondences, and none of these correspondences are labeled. On the other hand, there are a small set of samples labeled in the partially labeled mode. We list the performance comparison of the above mentioned methods of the correspondence mode in Table II and of the partially labeled mode in Table III, respectively. Seven pairwise view scenarios are shown in Table II: 1) without (WO) transfer learning techniques [12]; 2) using the method in [12] with bilingual-words (BW); 3) using the method in [62] with quantized aspect (QA); 4) using the method in [55] with self-similarity metrics (SS); 5) using the method in [87] with continuous model of aspect (CV); 6) using the method in [58] with discriminative virtual views (VV); and 7) using the transferable dictionary pair in [59] constructed by DL. According to Table II, DL significantly outperforms the other methods and its most significant improvement over WO is 87% when treating Camera 0 as the source view and Camera 3 as the target view. Loosening the experimental restrictions by abandoning the correspondence instances from both views, while adding a small set of labeled training instances in the target view, comparisons between SVMSUT, AUGSVM, MIXSVM [88], VV, and DL are given in Table III, where DL still achieves the best results with the most significant improvement of 88.7% over WO when treating Camera 0 as the source view and Camera 3 as the target view. In general, Camera 4 has relatively weak performance. The reason is that Camera 4 is set above the actors, so that actions are captured in a totally different view. On the other hand, the performance involving Camera 4 can effectively demonstrate the capability of a transfer learning system. The BW, VV, and DL significantly outperform QA, SS, and CV. However, one limitation for the former three lies in that they implicitly assume that the target view is known for a query sequence.

TABLE II Comparison Between Different Feature Representation Cross-View Transfer Learning Methods in the Correspondence Mode. Results are Reported on Every Possible Pairwise View Combination of the IXMAS Data Set, Where Columns Correspond to the Target Views and Rows Correspond to Source Views

TABLE III Comparison Between Cross-View Knowledge Transfer Methods in the Partially Labeled Mode. Results Are Reported on Every Possible Pairwise View Combination of the IXMAS Data Set, Where Columns Correspond to the Target Views and Rows Correspond to Source Views

B. Classifier-Level Knowledge Transfer Methods
We conduct experiments on both image classification and action recognition tasks, where the PASCAL VOC 2007 data set [89] is used for image classification and the UCF YouTube and HMDB51 data set [90] are used for action recognition. The PASCAL VOC 2007 data set contains 20 object classes, including bird, bicycle, motorbike, and so on, among which we choose samples from the bicycle class and the motorbike class as positive samples of the target domain and the source domain, respectively, and samples from the remaining classes as negative testing samples in the target domain. The histogram of oriented gradients (HOG) features are extracted from each image by dividing each image into eight cells. The task is to learn a bicycle classifier to achieve a binary decision over whether the test sample belongs to the bicycle category or a different category. The target classifier is learned by transferring information from a motorbike classifier via the guidance of a few bicycle samples. We compare the methods of nontransfer SVM, A-SVM, PMT-SVM, DA-SVM, and MKTL in Table IV with different numbers of training examples that vary from 1 to 25 with the interval of 3. Among these methods, DA-SVM achieves the best performance in terms of higher start and higher slope, while the PMT-SVM achieves the best final performance.

TABLE IV Performance Comparison on the Image Classification Task Between SVM, A-SVM, PMT-SVM, DA-SVM, and MKTL. Models are Learned with Different Numbers of Training Examples of the Bicycle Class and the Motorbike Class as the Source Domain. First Row Indicates the Number of Training Samples Used in the Source Domain

The UCF YouTube action data set is a realistic data set that contains camera shaking, cluttered background, variations in actors’ scale, variations in illumination, and view point changes. There are 11 actions contained in the UCF YouTube data set, including biking, diving, golf swinging, and so on. The binary action recognition task aims at distinguishing actions between the biking class and the diving class with corresponding source domain actions from the HMDB51 data set, which is an even more challenging data set. Dense trajectories [91] are extracted from raw action video sequences with eight spatial scales spaced by a factor of 1/2–√ , and feature points are sampled on a grid spaced by five pixels and tracked in each scale, separately. Each point at frame t is tracked to the next frame t+1 by median filtering in a dense optical flow field. To avoid the drifting problem, the length of a trajectory is limited to 15 frames. The HOG-HOF [92] and MBH [93] are computed within a 32×32×15 volume along the dense trajectories, where each volume is subdivided into a ST grid of size 2×2×3 to impose more structural information in the representation. The LLC coding scheme [94] is applied to the low-level local dense trajectory features. We compare the methods of nontransfer SVM, A-SVM, PMT-SVM, and MKTL in Table V. Obviously, the overall performance when transferring knowledge from the motorbike class to the bicycle class on the PASCAL VOC 2007 data set significantly outperforms the performance for transferring knowledge from the biking class to diving class on the UCF YouTube data set. This is due to that the relevance between motorbike and bicycle is much higher than the relevance between the actions biking and diving. In addition, the shared visual commons in video sequences are more difficult to capture than those in images. Compared with the results demonstrated in the image classification task, adding more training samples in the action recognition task leads to more significant improvements. As discussed in Section III, PMT-SVM is expected to outperform A-SVM in general. As shown in Tables IV and V, A-SVM outperforms PMT-SVM when a single or a few training instances are available, while PMT-SVM outperforms A-SVM in most cases when sufficient training instances are available. This can be explained as that PMT-SVM is relatively more sensitive to bad training samples.

TABLE V Performance Comparison on the Action Recognition Task Between SVM, A-SVM, PMT-SVM, and MKTL Models Are Learned With Different Numbers of Training Examples of the Biking Class and the Diving Class as the Source Domain on the UCF YouTube Data Set. First Row Indicates the Number of Training Samples Used in the Source Domain

We additionally conduct experiments on the action recognition task to compare the performance between the feature-level knowledge transfer techniques (FR and DCDDL), the classifier-level knowledge transfer technique (A-SVM) and nonknowledge transfer techniques (LLC and K-SVD). The experiments are conducted using the same setting as described above on the UCF YouTube data set and the HMDB51 data set. The results are demonstrated in Table VI. By comparing the results of nonknowledge transfer techniques LLC and K-SVD by brutally using the source domain data to the same techniques without the source domain data, we can conclude that brutal forcing the source domain data into the target task could degrade the performance of original learning systems. Among the listed techniques, the recently proposed cross-domain DL method DCDDL achieves the best performance.

TABLE VI Recognition Results on the UCF YouTube Data Set When Using the HMDB51 Data Set as the Source Domain

C. Knowledge Transfer From Multiple Source Domains
To demonstrate the performance comparisons between multisource knowledge transfer methods, we quote the experimental results in [80] and [82] for cross-domain multisource knowledge transfer, and the results in [12] and [58] for cross-view multisource knowledge transfer.

Duan et al. [80] chose the two large-scale image data sets to construct multiple source domains, where the first data set is the NUS-WIDE data set [96], which consists of 269, 648 images downloaded from the Flickr, and the second data set is collected from the photo forum called photosig.com, which contains about 1.3 million images. Three real-world consumer video data sets, the Kodak data set [82], the YouTube data set [80], and the CCV data set [97], are used as the target domains for performance evaluation, where the former contains 195 videos from six event classes, (e.g., birthday, picnic, parade, show, sports, and wedding), the middle is collected from YouTube using the same event classes as in the Kodak data set, and the latter contains 2726 videos for the same event classes. In the source domain, one hundred thousand training images are randomly selected from the two image sources and SIFT features are extracted from each image. After that, five source domains are constructed by randomly sampling 100 relevant images and 100 irrelevant images for clustering. In the target domain, both static SIFT features and space-time features are extracted from each video sequence in all the three data sets, where space-time interest point (STIP) feature and the Mel-frequency cepstral coefficients audio feature are extracted from the CCV data set, and three types of space-time features, HOG, HOF, and MBH, are extracted from Kodak and YouTube data sets. Since the standard SVM and DASVM cannot handle the domain adaptation problem when the data from the source and the target domain are with different feature types, only static SIFT features are used to learn classifiers in the target domain. The MAPs of SVM, DASVM, DAM, DSMsim , and DSM methods on the three data sets are show in Table VII, where DSMsim , as a simplified version of DSM, only considers the ST features in the target domain. Compared with the standard SVM, DASVM, and DAM achieve worse or equivalent performance on all three data sets, which indicate that the source domain data are not successfully used by these two methods. Based on the observation that the DSMsim consistently outperforms the related DAM method, it clearly demonstrates the benefits of employing the selected relevant source domains rather than using all the source domains. The DSM method achieves the best performance on all three data sets, which further demonstrates the effectiveness of integrating static SIFT features and ST features. Experiments are also conducted in [82] using the Kodak data set and videos downloaded from YouTube using keywords-based search to evaluate the performance of A-SVM, DTSVM, and A-MKL by transferring knowledge from the source image domains to the target video domain. Table VIII reports the means and the standard deviations of MAPs over all six events for the standard SVM, MKL, DTSVM, and A-MKL methods in the three cases, which are: 1) classifiers learned based on SIFT features; 2) classifiers learned based on ST features; and 3) classifiers learned based on both SIFT and ST features. Two forms of the standard SVM method, SVM-AT and SVM-T, are evaluated, where SVM-AT is learned based on samples from both the target domain and the source domain and SVM-T is learned based on samples only from the target domain. SVM-T outperforms SVM-AT in both cases 2) and 3), which indicates that directly, including the source domain knowledge may degrade the event recognition performances in the target domain. For all methods, the MAPs based on SIFT features are better than those based on ST features. This is consistent with our evaluation for classifier level knowledge transfer methods, which indicates that the shared commons are more difficult to be captured in ST features than in static SIFT features. The effectiveness of fusing average classifiers and multiple base kernels is proved in A-MKL by providing the best performances for all cases.

TABLE VII Mean Average Precisions (MAPs) of SVM, DASVM, DAM, DSMsim, and DSM Methods on Kodak, YouTube, and CCV Data Sets

TABLE VIII Means and Standard Deviations of MAPs Over Six Events for Methods in Three Cases: 1) Classifiers Learned Based on SIFT Features; 2) Classifiers Learned Based on ST Features; and 3) Classifiers Learned Based on Both SIFT and ST Features

The LWE fusing approach [12] and MKL-SVM approach [58] are compared with the SVMSUT, AUGSVM, MIXSVM methods on both the correspondence mode and the partially labeled mode in Table IX for cross-view multisource knowledge transfer. The overall results in the correspondence mode significantly outperforms the results in the partially labeled mode. In the correspondence mode, LWE and MKL-SVM achieve equivalent performance, while in the partially labeled mode, MKL-SVM consistently leads to the best performance.

TABLE IX Cross-View Transfer Learning Action Recognition with Multiple Auxiliary Views Under Both Correspondence Mode and Partially Labeled Mode

SECTION VII.Conclusion
In this survey, we have reviewed transfer learning techniques on visual categorization tasks. There are three types of knowledge that are useful for knowledge transfer: 1) source domain features; 2) source domain features and the corresponding labels; and 3) parameters of the prelearned source domain models, which indicate instance-based transfer learning, inductive transfer learning and parameter-based transfer learning, respectively. Through the performance comparisons between knowledge transfer techniques and nonknowledge transfer techniques, we can conclude that brutal forcing the source domain data for learning can degrade the performance of the original learning system, which demonstrates the significance of knowledge transfer. To transfer the source domain knowledge to the target domain, methods are designed from either the feature representation level or the classifier level. In general, the feature representation level knowledge transfer aims to unify the mismatched data in different visual domains to the same feature space and the classifier level knowledge transfer aims to learn a target classifier based on the parameters of prelearned source domain models, while considering the data smoothness in the target domain. Thus, the feature representation level knowledge transfer techniques belong to either instance-based transfer or inductive transfer, while most classifier level knowledge transfer techniques belong to the parameter-based transfer. To avoid transferring the negative knowledge and deal with the many-to-one adaptation problem, many strategies are proposed to learn a set of weights for each source domain to achieve multiple source domain knowledge fusion.

Transfer learning is a tool for improving the performance of the target domain model only in the case that the target domain labeled data are not sufficient, otherwise the knowledge transfer is meaningless. So far, most research on transfer learning only focuses on small scale data, which cannot well reflect the potential advantage of transfer learning over regular machine learning techniques. The future challenges of transfer learning should lie in two aspects: 1) how to mine the information that would be helpful for the target domain from highly noisy source domain data and 2) how to extend the existing transfer learning methods to deal with large-scale source domain data.