Dynamical models estimate and predict the temporal evolution of physical systems. State-space models (SSMs) in particular represent the system dynamics with many desirable properties, such as being able to model uncertainty in both the model and measurements, and optimal (in the Bayesian sense) recursive formulations, e.g., the Kalman filter. However, they require significant domain knowledge to derive the parametric form and considerable hand tuning to correctly set all the parameters. Data-driven techniques, e.g., recurrent neural networks, have emerged as compelling alternatives to SSMs with wide success across a number of challenging tasks, in part due to their impressive capability to extract relevant features from rich inputs. They, however, lack interpretability and robustness to unseen conditions. Thus, data-driven models are hard to be applied in safety-critical applications, such as self-driving vehicles. In this work, we present DynaNet, a hybrid deep learning and time-varying SSM, which can be trained end-to-end. Our neural Kalman dynamical model allows us to exploit the relative merits of both SSM and deep neural networks. We demonstrate its effectiveness in the estimation and prediction on a number of physically challenging tasks, including visual odometry, sensor fusion for visual-inertial navigation, and motion prediction. In addition, we show how DynaNet can indicate failures through investigation of properties, such as the rate of innovation (Kalman gain).

SECTION I.Introduction
From catching a ball to tracking the motion of planets across the celestial sphere, the ability to estimate and predict the future trajectory of moving objects is key for interaction with our physical world. With ever increasing automation, e.g., self-driving vehicles and mobile robotics, the ability to not only estimate system states based on sensor data but also to reason about latent dynamics and therefore predict states with partial or even without any observation is of huge importance to the safety and reliability of intelligent systems [1].

Newtonian/classical mechanics has been developed as an explicit mathematical model, which can be used to predict future motion and infer how an object has moved in the past. This is commonly captured in a state-space model (SSM) that describes the temporal relationship and evolution of states through first-order differential equations. For example, in the task of estimating egomotion from visual sensors, also known as visual odometry (VO) [2]–[3][4], velocity, position, and orientation are usually chosen as physically attributable states for mobile robots. These models are typically handcrafted based on domain knowledge and require significant expertise to develop and tune their hyperparameters. Simplifying assumptions are often made, for example, to treat the system as being linear, time-invariant with uncertainty being additive and Gaussian. A canonical example of an optimal Bayesian filter for linear systems is the Kalman filter [5], which is an optimal linear quadratic estimator. Although capable of controlling sophisticated mechanical systems (e.g., the Apollo 11 lander used a 21-state Kalman Filter [6]), it becomes more challenging to use in complex, nonlinear systems, giving rise to alternative variants, such as the sequential Monte Carlo [7] or nonlinear graph optimization [8]. However, even when using these sophisticated approaches, imperfections in model parameters and measurement errors from sensory data contribute to issues, such as accumulative drift in visual navigation systems. Furthermore, there is a disconnect between the complexities of rich sensor data, e.g., images and derived states.

Identifying the underlying mechanism governing the motion of an object is a hard problem to solve for dynamical systems operating in real world. As a consequence, in recent years, there has been an explosive growth in applying deep neural networks (DNNs) for motion estimation [9]–[10][11][12][13][14][15][16][17]. These learning-based approaches can extract useful representations from high-dimensional raw data to estimate the key motion states in an end-to-end fashion.

Although these learned models demonstrate good performance in both accuracy and robustness, they are “black-boxes” regressing measurements with implicit latent states and difficult to interpret. In contrast to neural networks, SSMs are able to construct a parametric model description and offer an explicit transition relation that describes the evolution of system states and uncertainty into the future. They can also optimally fuse measurements from multiple sensors based on their innovation gain, rather than simply stacking them as in a neural network.

In this work, we propose DynaNet—neural Kalman dynamical models, combining the respective advantages of DNNs (powerful feature representation) and state-space models (SSMs; explicit modeling of physical processes). As shown in Fig. 1, our proposed end-to-end model enables automatic feature extraction from raw data and constructs a time-varying transition model. The recursive Kalman filter is incorporated to provide optimal filtering on the feature state space and estimate process covariance to reason about system behavior and stability. This allows for accurate system identification in an end-to-end manner without human effort.


Fig. 1.
Concept figure of our proposed DynaNet that combines DNNs and SSMs to infer latent system states. In this case, the motion dynamics of a driving car is modeled by a linear-like dynamical model, with DNNs to extract useful features from visual observations and SSMs to estimate and predict system states with or even without observations.

Show All

Our DynaNet can learn a linear-like SSM directly from raw data. This linear-like structure is a drop-in replacement for the typical recurrent neural network (RNN) estimator. Specifically, our contributions are threefold.

We propose a novel hybrid model with differentiable Kalman filter that is adopted on the feature level instead of system states level for latent state inference.

We design a strategy to ensure the stability of learned dynamical model by resampling the transition matrix from Dirichlet distribution, in which the system proves to be stable with a probability of one.

With the design of neural emission model to connect observations with full states, our DynaNet can cope with a number of challenging situations, e.g., when only partial/corrupted observations are available or even without any observations.

To demonstrate the effectiveness of the proposed technique, we conducted extensive experiments and a systematic study on challenging real-world motion estimation and prediction tasks, including VO, visual-inertial odometry (VIO), and motion prediction. We show how the proposed method outperforms the state-of-the-art deep-learning techniques while yielding valuable interpretable information about model performance. The interpretability analysis discovers the interesting relation between sensor data quality and the explicit model terms.

The rest of this article is organized as follows. Section II reviews the relevant work. Section III presents our proposed neural Kalman dynamical model; Section IV evaluates our proposed model applied to three different tasks, i.e., visual egomotion estimation and prediction, visual-inertial navigation, and motion prediction through extensive experiments. Finally, Section V discusses conclusions.

SECTION II.Related Work
A. State-Space Models
SSM is a convenient and compact way to represent and predict system dynamics. In classical control engineering, system identification techniques are widely employed to build parametric SSMs [18], [19]. In order to alleviate the effort of analytic description, data-driven methods, such as Gaussian processes [20] or expectation–maximization (EM) [21], emerged as alternatives to identify nonlinear models. Linear dynamic model, e.g., Kalman filtering, has been explored to combine with RNN that ensures the convergence of neural network training [22]. With advances in DNNs, deep SSMs have been recently studied to handle very complex nonlinearity. Specifically, Backprop KF [23] and DPF [24] used DNNs to extract effective features and feed them to a predefined physical model (i.e., conditioned on algorithmic priors) to improve filtering algorithms. Karkus et al. [25] incorporated a particle filter as an algorithmic prior into the neural network for visual localization. Besides feature extraction, DNNs have also been used in reparameterizing the transition matrix in SSMs from raw data [26]–[27][28][29]. Unlike prior art, our work exploits recent findings on stable dynamical models [30] and uses resampling to generate a transition matrix from the Dirichlet distribution, whose concentration is learned via a neural network. The specific Dirichlet distribution ensures the stability of dynamic systems, which is an important yet absent property of previous DNN-based SSMs.

B. Motion Estimation
Motion estimation has been studied for decades and plays a central role in robotics and autonomous driving. Conventional VO/SLAM methods rely on multiple-view geometry to estimate motion displacement between images [2]–[3][4], [31]–[32][33][34]. Due to the huge availability and complementary property of inertial and visual sensors, integrating these two sensor modalities has raised increasing attention to give more robust and accurate motion estimates [35]. A large portion of work in this direction is VIO, where filtering [36], [37] and nonlinear optimization [35], [38], [39] are two mainstream model-based methods for sensor fusion. Meanwhile, recent studies also found that the methods using data-driven DNNs are able to provide competitive robustness and accuracy over some model-based methods. These deep learning methods often use convolutional neural networks (ConvNets) to discover useful geometry features from images for effective odometry estimation [11], [16], [17], [40], [41] and/or employ RNNs to model the temporary dependence in motion dynamics [10], [12], [42], [43]. Besides self-motion estimation in robotics and autonomous driving, RNNs have also been introduced to model human dynamics and address the problem of human-skeleton motion prediction [44], [45]. To improve the capacity of long-term prediction, Tang et al. [46] leveraged temporal attention mechanism to predict next step motion based on all historical information. Furthermore, Shu et al. [47] considered both the spatial and temporal relations of human-skeleton motions and proposed a novel skeleton-joint attention with RNNs to achieve a better performance in the task of human motion prediction. Nevertheless, DNN-based methods are hard to interpret or expect/modulate their behaviors [1]. Motivated by this, our DynaNet aims to bridge the gap of performance and interpretability through a deeply coupled framework of model- and DNN-based methods.

SECTION III.Neural Kalman Dynamic Models
We consider a time-dependent dynamical system, governed by a complex evolving function f
zt=f(zt−1,wt)(1)
View Sourcewhere z∈Rd is d -dimensional latent state, t is the current timestep, and w is a random variable capturing system and measurement noise. The evolving function f is assumed to be Markovian, describing the state-dependent relation between latent states zt and zt−1 . The model in (1) can be reformulated as a linear-like structure, i.e., the state-dependent coefficient (SDC) form, with a time-varying transition matrix A
zt=Atzt−1.(2)
View Source

Notably, the system nonlinearity is not restricted by this linear-like structure, as there always exists an SDC form f(z)=A(z)z to express any continuous differentiable function f with f(0)=0 [48].

In this regard, our problem of the dynamic model is how to recover the latent states z and their time-varying transition relation A from high-dimensional measurements x (e.g., a sequence of images), without resorting to a handcrafted physical model.

This work aims to construct and reparameterize this dynamic model using the expressive power of DNNs and explicit state-SSMs. Fig. 2 shows the main framework, which will be discussed in detail in the following. To avoid confusion, in the rest of this article, latent states z are exclusively used for dynamical models and hidden states h exclusively represent the neurons in a DNN. In the meantime, we will use sensor measurements and observations interchangeably.


Fig. 2.
DynaNet framework consists of the neural observation model to extract latent states z , the neural transition model to generate the evolving relation A , and a recursive Kalman filter to infer and predict system states. P is the covariance matrix of latent states, and Q and R are process noise matrix and observation noise matrix, respectively.

Show All

A. Neural Emission Model
Intuitively, the system states containing useful information often lie in a latent space that is different from the original measurements. For example, given a sequence of images (the sensor measurements), the key system states of VO are velocity, orientation, and position. Nevertheless, it is nontrivial for conventional models to formulate a temporal linear model that can precisely describe the relation between these physical representations.

Rather than explicitly specifying physical states as in a classical SSM, we use a DNN to automatically extract the latent state features while forcing them to follow the linear-like relation in (2). This can be achieved automatically by optimizing the model via stochastic gradient descent and backpropagation algorithms. This linearization is particularly useful as it allows us to directly use a Kalman filter for state feature inference. Note that the differentiable Kalman filtering in our DynaNet model performs on the high-dimensional latent feature space rather than the physical state space (e.g., velocity, orientation, and position) as in Backprop KF [23] and DPF [24].

In our neural emission model, an encoder fencoder is used to extract both features at and an estimation of uncertainty σat from the observations xt at timestep t
at,σt=fencoder(xt).(3)
View Source

The features a act as observations of the latent feature state space. The coupled uncertainties σ represent the measurement belief that is transformed into the observation noise R in a Kalman filter. a and σ are further used in the update stage of a differentiable Kalman filter in (14), which forces them to follow the distribution of KF parameters during end-to-end optimization. Thus, unlike VAE [29], [49], the two terms are not directly and explicitly constrained in (3) to follow a prior distribution but leave the learning model to search for suitable latent states and uncertainty estimation that can construct a linear-like structure and exploit the full capacity of differentiable Kalman filter. However, the observations a are sometimes unable to provide sufficient information for all latent states z in a dynamical system, for example, the occasional absences of sensory data. Hence, a deterministic emission matrix H is defined to connect with the full latent states z
at=Hzt.(4)
View Source

In a practical setting, when the extracted features contain all the information for dynamical systems, the emission matrix H is set to d -dimensional identity matrix Id as features and states are identical at this moment. On the other side, the identity matrix needs to adapt to H=[Im,0m×(d−m)] when observations only give rise to m features. In this case, the rest (d−m) latent states will be attained from historical states. Our experiment in Section IV-C demonstrates the superiority of this neural emission model in addressing the issue of observation absences for sensor fusion in VIO.

B. Neural Transition Model
In an SSM, the temporal evolution of latent states is determined by the transition matrix A as in (2). Obviously, the transition matrix is of considerable importance as it directly describes the governing mechanism of a system. Nevertheless, such a matrix is difficult to specify manually, especially when it is time-varying. Fig. 3 shows two methods we propose to estimate A on the fly, based on prior system states: 1) a deterministic way to learn it end-to-end from raw data and 2) a stochastic way to resample it from distribution, e.g., the Dirichlet distribution in this work. The deterministic transition and resampled transition are two individual strategies to generate a transition matrix. The parameters of long short-term memory (LSTM) in these two modules are separately learned from data. We will explain them accordingly in what follows.


Fig. 3.
Two transition generation strategies—(a) deterministic transition or (b) resampled transition are proposed to achieve the desired system behaviors. The transition matrix is generated by an RNN, i.e., LSTM in this work conditioned on the previous latent system states.

Show All

1) Deterministic Transition:
Intuitively, a movement change depends on historical system states, which are encoded in the latent states z0:t−1 . Prior works mostly apply an RNN to specify dynamic weights for choosing and interpolating between a fixed number of different transition modes [28], [29]. Inspired by [50], our model generates the transition matrix A directly from the history of latent states z .

In this deterministic transition model, the dependence of the transition matrix on historical latent states is specified by an RNN. This RNN recursively processes previous hidden states (zt−1,ht−1) of the dynamic model and LSTM [51], [52] cell and outputs the time-dependent transition matrix At via
At=LSTM(zt−1,ht−1)(5)
View Sourcewhere zt−1 is the latent states of dynamical system at the timestep (t−1) , ht−1 is the hidden states of LSTM module, and At is the learned transition matrix at current timestep.

2) Resampled Transition:
Powerful DNNs are able to approximate dynamical models effectively from data. However, these learned “black-box” models are difficult to be interpreted or modulated. Especially, in safety-critical applications (e.g., self-driving vehicles), the lack of model behavioral indicator and the absence of system reliability largely limit the adoption of these learning models. The stability of a dynamical system is essential and fundamental to autonomous systems, as it guarantees that the predictions of dynamic models will not change abruptly, given slightly perturbed inputs. Unfortunately, this desirable property is not ensured by most pure DNN models. Therefore, we here aim to ensure the stability of learned dynamical model by resampling the transition matrix from a specific distribution.

The linear-like SSM structure in (2) allows for a quadratic Lyapunov stability analysis, while advances in stochastic optimization allow to construct neural probabilistic models [53]. We thus propose to resample the transition matrix from a predefined probability distribution to enforce the desired stability. Based on the findings in [30], if the state transition follows a Dirichlet distribution in a positive system, it will lead to a model being asymptotically stable, i.e., it will be bounded-input–bounded-output (BIBO) stable. Constructing the stochastic variable and determining the parameters of the distribution are easy to achieve with the widely used reparameterization trick [54].

To further reduce handcrafted engineering, the concentration α of the Dirichlet distribution in our framework is generated from historical system states via an LSTM-based RNN
α=LSTM(zt−1,ht−1)(6)
View Sourcewhere zt−1 is the latent states of dynamical system at the timestep (t−1) and ht−1 is the hidden states of LSTM module. A small Gaussian random noise is also added in this process to improve model robustness. At each timestep, a realization of the transition matrix A is drawn from the constructed Dirichlet distribution
At∼Dirichlet(α).(7)
View Source

Note that in DynaNet, the transition states are in the latent feature space rather than the final target states (e.g., the states of orientation and position in VO). The latent features are extracted by the encoder, which ensures the transition states strictly positive through a rectified linear unit (ReLU) activation and a tiny random positive number on the last layer of the DNN-based encoder.

Our DynaNet extends the work [30] of nearest neighbor method-based stable system to a DNN-based approach, for modeling more complex nonlinear dynamics. Analytically, the following proof supports our proposed system: given a DNN-based dynamical system zt+1=A(α)zt , where zt∈Rd is d -dimensional system state at the timestep t , extracted by a neural network from raw data and A(α)∈Rd×d is the transition matrix generated by a neural network with a concentration α , if the transition matrix A is constructed from a Dirichlet distributionA∼Dir(θ(α)) , this dynamical system is asymptotically stable.

Proof:
By resampling the transition matrix A∈Rd×d from a Dirichlet distribution, the elements inside A satisfy
A(i,j)>0,A(i,j)<1∀i,j=1,…,d,and ∑i=1d∑j=1dA(i,j)=1.(8)
View Source

We can have
∑j=1dA(i,j)<1∀i⟹maxi=1:d∑j=1dA(i,j)<1.(9)
View Source

If all elements inside A are strictly positive, then the maximum absolute row sum norm ∥A∥∞ follows:
∥A∥∞=maxi=1:d∑j=1d|A(i,j)|<1.(10)
View Source

The hidden feature states zk+M at the timestep t+M is derived from the system states zk at the timestep t and M consecutive transition matrix via
∥zk+M∥∞=∥∥∥∥∏m=1MAmzk∥∥∥∥∞≤≤∏m=1M∥Am∥∞∥z∥∞(maxm∥Am∥∞)M∥zk∥∞.(11)
View Source

As the M is infinite, the system states zk+M will become
∥zk+M∥∞−→−−−M→∞0.(12)
View Source

Therefore, the system is stable with a probability of one.

C. Prediction and Inference With a Kalman Filter
The neural emission model estimates system states from noisy sensor measurements, while the generated transition model describes the system evolution and predicts the system states with previous ones. However, uncertainties exist in both of them and motivate us to integrate a Kalman filter into our framework. The Kalman filter recursively deals with the uncertainties and produces a weighted average of the state predictions and fresh observations. With the aforementioned neural emission and transition models, the prediction and inference are performed on the feature state space and follow a standard Kalman filtering pipeline. We also note that the Kalman filter’s gain controls how much to update the residual error (i.e., the difference between prediction and observation), which is a useful metric to represent the relative quality of measurements (as shown in Section IV-E).

More specifically, the Kalman filter consists of two blocks: prediction and update. In the prediction stage, prior estimates of the mean value and covariance (zt|t−1,Pt|t−1) at the current timestep are derived from the posterior state estimates (zt−1|t−1,Pt−1|t−1) in the previous timestep
zt|t−1=Pt|t−1=Atzt−1|t−1AtPt−1|t−1ATt+Qt.(13)
View Source

When current observations at are available, the update process allows us to produce a posterior mean and covariance of hidden states (zt|t,Pt|t) as follows:
rt=St=Kt=zt|t=Pt|t=at−Htzt|t−1Rt+HtPt|t−1HTtPt|t−1HTtS−1tzt|t−1+Ktrt(I−KtHt)Pt|t−1(14)
View Sourcewhere r is the residual error (also known as innovation), S is the residual covariance, and K is the Kalman gain. In contrast to hand-tuning process noise Q and measurement noise R in a conventional KF, these two terms are jointly learned by our proposed neural dynamical model. Finally, the predictor (e.g., an FC network) outputs the target values yt from the estimated optimal hidden states zt|t
y~t=fpredictor(zt|t).(15)
View Source

In the case that current measurements, i.e., at , are unavailable, the reconstructed values y^t are inferred from the prior estimate zt|t−1
y^t=fpredictor(zt|t−1).(16)
View Source

All parameters θ in our model are end-to-end learned with a mean square loss function. This loss function jointly compares the ground truth yt with posterior predictions y~t and prior prediction y^t
L(θ)=1T∑t=1T(||yt−y~t||2+||yt−y^t||2).(17)
View Source

SECTION IV.Experiments
We systematically evaluate our system through extensive experiments including pose estimation for VO in Section IV-B, pose estimation for VIO in Section IV-C, and motion prediction without observations or with partial observations in Section IV-D. Moreover, an interpretability study is also conducted in Section IV-E.

A. Datasets, Baselines, and Experiment Setups
1) Datasets:
To evaluate our proposed DynaNet models, we used public datasets to conduct experiments: KITTI Odometry dataset [55] for visual pose estimation and prediction, and KITTI Raw Dataset [55] for visual-inertial pose estimation and prediction.

KITTI Odometry Dataset [55]: It is a commonly used benchmark dataset that contains 11 sequences (00–10) with images collected by car-mounted cameras and ground-truth trajectories provided by GPS. We used it for VO experiment, with Sequences 00–08 for training and Sequences 09 and 10 for testing. The images and ground truth are collected at 10 Hz. We chose the sequence length as 5 and thus generated a total of 20 373 subsequences from the training set to train neural models.

KITTI Raw Dataset [55]: It contains both raw images (10 Hz) and high-frequency inertial data (100 Hz). Since inertial data are only available in the unsynced data packages, we selected the raw files with the corresponding KITTI Odometry Dataset. Inertial data and images are manually synchronized according to their timestamps. We adopted the same data split mentioned above, discarding Sequence 03 as its raw data is unavailable. Thus, in this experiment, we used Sequences 00, 01, 02, 04, 05, 06, 07, and 08 for training and Sequences 09 and 10 for testing. We chose the sequence length as 5 and thus generated 20 373 subsequences from the training set to train neural models. We chose the sequence length as 5 and thus generated a total of 20 361 subsequences from the training set to train DNNs.

2) Baselines:
In the VO experiment, we compare our DynaNet models with three representative three deep learning-based VO models, i.e., SfmLearner [11], Bian et al. [56], and DeepVO [10]. DeepVO shares the same architecture as in our models including the encoder and predictor but uses the two-layer LSTM [52] to estimate system latent states. This can be viewed as an ablation study, and we keep their dimension of hidden states (128) the same as our models for a fair comparison. Except learning-based baselines, our model is also compared with two classical monocular VO system, i.e., ORB-SLAM [34] and VISO2 [57]. ORB-SLAM [34] is a monocular visual SLAM algorithm based on handcrafted features and multiview geometry. Its loop closing module is disabled for a fair comparison with odometry estimation. VISO2 [57] is a monocular VO algorithm and implemented as an official baseline on the KITTI dataset.

In the VIO experiment, we chose a state-of-the-art learning-based VIO approach, i.e., VINet [12] as our baseline. VINet shares a similar structure as in our models, but it uses a two-layer LSTM rather than our DynaNet module. A popular classical model-based VIO system, i.e., mono-VINS [39], is also adopted here as baseline. Mono-VINS [39] is a tightly coupled sliding window-based optimization approach for VIO, which achieves the state-of-the-art performance on several VIO datasets.

In the motion prediction task, we compare our models with LSTM-based approaches. All the other modules for LSTMs, including encoder and predictor, and the dimension of hidden states (128) are kept the same as in our proposed models for a fair comparison. Besides, we implemented an attention-based LSTM approach to show how attention mechanism improves the motion prediction in future steps. As the approach is an end-to-end model that directly deals with high-dimensional raw images, at each timestep, the attention mechanism aggregates the features extracted from visual data or visual/inertial data and then feeds the updated features to the LSTM module. The rest modules are kept the same to be compared fairly.

3) Experiment Setups:
We implemented the proposed framework with Pytorch and trained on a NVIDIA Titan X GPU. The implementation details of DynaNet frameworks can be found in the Appendix. All of our models are trained with the Adam optimizer with a batch size of 32 and a learning rate of 1e−4 . The trained models are tested on the test set, in which data have never been seen in the training set.

B. Visual Odometry
Our evaluation starts with a set of visual egomotion (VO) experiments for 6-DoF pose estimation. Here, pose estimation means that our model produces 6-DoF pose given sensor data (i.e., images). In this experiment, a sequence of raw images are given to models to produce pose transformations, i.e., translation and rotation.

Table I reports the performance of our proposed DynaNet models, comparing with other learning-based approaches and classical VO algorithm. All neural networks were trained above the KITTI Odometry dataset with Sequences 00–08 and tested with two new sequences (Sequences 09 and 10). The motion transformations from models are integrated into global trajectories, and then, we evaluated them according to the official KITTI metrics, commonly adopted to evaluate VO algorithms, which calculates the average root-mean-square errors (RMSEs) of the translation and rotation for all the subsequences in the lengths 100, 200,…,800 m. This evaluation metrics can capture both the global and local drifts of VO systems.

TABLE I Performance of VO on the KITTI Odometry Dataset for Motion Estimation, Reported in the RMSE of Translation (%) and Orientation (°)

As shown in Table I, our proposed models clearly outperform the baselines of ORB-SLAM [34], VISO2 [57], SfmLearner [11], Bian et al. [56], and DeepVO [11], and the largest gain is achieved by our Dirichlet model. Note that the only difference between our models and DeepVO is the state estimation part, and we keep the model hyperparameters, e.g., the dimension of hidden states, the same for a fair comparison. By replacing the LSTM module in DeepVO with our proposed DynaNet, our deterministic model improves the performance of DeepVO around 10.64% in translation and 16.73% in orientation, and our Dirichlet model further improves DeepVO around 14.99% in translation and 22.91% in orientation. This indicates that incorporating the physical prior into neural network benefits learning state estimation from data. It also implies that the nonlinearities of VO systems are not lost despite the linear-like structures inside our models.

Fig. 4 shows the trajectories of Sequences 09 and 10 predicted by our models. Sequence 09 and 10 are difficult scenarios, as the driving car experienced large movement in height. Our DynaNet models, especially the Dirichlet model, are still capable of providing robust results, which are closer to the ground-truth trajectories, and consistently show competitive performance over LSTM-based DeepVO.


Fig. 4.
Testing trajectories on (a) Sequence 09 and (b) Sequence 10 of the KITTI dataset indicate that our models produce robust and accurate pose estimates in VO.

Show All

It must be pointing out that the performance of learning model depends on its training process. Both underfitting and overfitting should be avoided for machine learning methods or their testing performance will be degraded in such circumstances. Fig. 5 shows the trajectories of our proposed DynaNet model with Dirichlet distribution on Sequence 9 of the KITTI dataset in three different training stages. Our model is trained for a total of 100 epochs. When the DynaNet model is trained under the well-fitting condition (Epoch 89), the testing trajectory is comparable closer to the ground truth. However, when our DynaNet is in the underfitting (Epoch 50) or overfitting (Epoch 90) condition, it sees larger drifts in the corresponding trajectory.


Fig. 5.
Testing trajectories of our proposed DynaNet model (Dirichlet) on Sequence 9 of the KITTI dataset in the underfitting, well-fitting, and overfitting conditions.

Show All

C. Visual-Inertial Odometry
How to effectively integrate and fuse two modalities to provide accurate and robust pose remains a challenging problem. In this experiment, we demonstrate that our proposed models can learn a compact SSM for sensor fusion from two modalities, i.e., visual and inertial data. We also show that our DynaNet models enable robust prediction under the circumstances with partial observations in Section IV-D. When the training model is underfitting or overfitting,

1) Hyperparameters Setup:
Initially, a visual encoder and an inertial encoder extract m -dimensional visual features avisual∈Rm and n -dimensional inertial features ainertial∈Rn separately. These two features are then concatenated together as a=[avisual,ainertial]∈Rm+n . Notably, our emission matrix is defined as identity matrix H=Im+n when both modalities are available. If visual or inertial cues are absent, the emission matrix is changed to H=[Im,0m×n] or H=[0n×m,In] . The training and testing of the visual-inertial dynamic model follows the same procedures as in VO.

2) VIO Pose Estimation:
In this experiment, we adopt the official KITTI evaluation metric to evaluate our models and baseline, which is the same as in VO experiments. Table II reports the RMSE of the translation and orientation of the proposed DynaNet models, a classical model-based VIO algorithm, i.e., VINS-Mono [39] and a learning-based approach, i.e., VINet [12]. Due to the problem of loosely time-synchronization between visual and inertial sensors in the KITTI dataset, the performance of VINS-mono is not as good as learning-based methods. This supports the claim that learning-based approaches perform more robustly than hand-designed systems. From Table II, our proposed models outperform VINet with two-layer LSTMs, when given both visual and inertial observations. VINet shares the same framework and hyperparameters as our models, except that it uses LSTM rather than differentiable Kalman filtering. Our deterministic DynaNet [Ours (Deter.)] further reduces the RMSE of VINet from 6.44% to 5.47% in translation and from 1.70° to 1.63°. This demonstrates that our proposed models excel at fusing multiple sensor modalities for more accurate state estimates than the LSTM-based VIO model.

TABLE II Performance of VIO on the KITTI Raw Dataset for Motion Estimation, Reported in the RMSE of Translation (%) and Orientation (°)

D. Motion Prediction
In this experiment, we show the evaluation of DynaNet models on pose prediction that offers pose without sensor data (i.e., future states prediction).

1) VO Pose Prediction:
We first fed VO neural models a sequence of five images for initialization and then let them predict the next five and ten states without any further observations (i.e., trajectory prediction). All models, including baselines, were trained on the training set of the KITTI dataset and tested on the subsequences of 10 or 15 frames, generated from the testing set. In order to compare with LSTM baselines fairly, the structures and hyperparameters of baseline models are kept the same as our models except for the DynaNet module.

Table III shows the quantitative results of our approaches, comparing with LSTM- and “LSTM+Attention”-based models. We report the RMSE of relative positions of the next five and ten steps predicted by neural networks. As shown in Table III, it is clear that our proposed models perform better than both LSTM- and “LSTM+Attention”-based models in visual egomotion prediction. Especially, our Dirichlet model outperforms others by a large margin. This is because the resampled transition matrix from the Dirichlet distribution ensures the learned dynamical model to be stable and hence gives rise to long-term prediction in higher accuracy.

TABLE III VO on the KITTI Odometry Dataset for Motion Prediction [Translation RMSE (0.01 m)]

We then demonstrate the qualitative results of our methods. As straight driving routes dominate driving behaviors for autonomous vehicles, we thus begin our performance report with them first. Fig. 6(a) and (d) shows the best case and worst case of line prediction. Clearly, in both cases, the predicted trajectories from our proposed Dirichlet model are much closer to the ground-truth trajectories. Without observations, in the best case, it can even provide the same predictions [the small circles in Fig. 6(a)] as ground-truth values (the small crosses). Fig. 7 shows the error bar of predicted locations for straight driving, with the best and worst results among all tested segments to show the variance of the model predictions. In both cases, our proposed Dirichlet-based model consistently outperforms other baselines by providing more accurate and robust location predictions.


Fig. 6.
For future poses prediction without observations, our Dirichlet-based model clearly outperforms others when predicting the straight driving. Predicted locations in future steps from our proposed Dirichlet-based DynaNet model are closer to the ground truth, compared with other baselines in (a) good case and (d) bad case. In turning, the future poses are estimated in a tangent direction with or without the aid of inertial data in (b) and (c) good case and (e) and (f) bad case.

Show All


Fig. 7.
Error bar of line predictions.

Show All

Fig. 6(b) and (e) further shows the prediction performance in turning. As we can see, without timely observation, it is hard for DNNs to estimate accurate orientation changes, but they predict the future poses in a tangent direction in both good and bad cases. In the bad case, the motion predictions are relatively more far away from the ground truth. We will soon show how to integrate inertial information to aid the turning prediction in VIO pose prediction.

2) VIO Pose Prediction:
We also evaluate our models on pose prediction using visual and inertial sensor data. This experiment is conducted in scenarios of prediction with visual-only observations, inertial-only observations, and no observations, in which all models are given a sequence of five images for initialization and need to predict the next five poses.

As shown in Table IV, our models, including deterministic and Dirichlet approaches, greatly outperform the comparable LSTM and “LSTM+Attention” approaches. When no input is available, although attention mechanism improves the prediction performance over LSTM, our DynaNet still outperforms this “LSTM+attention” baseline. Specifically, the Dirichlet model shows better performance than all other approaches. This is consistent with the results in visual pose prediction and validates that the model stability will allow more accurate long-term prediction.

TABLE IV Visual-Inertial Navigation on the KITTI Raw Dataset for Motion Prediction [Translation RMSE (0.01 m)]

Fig. 6(c) and (f) shows the predicted trajectories with only inertial data when no visual observation is given. Clearly, in the good case, our models are capable of predicting future pose evolution accurately and robustly. We note that robust fusion is important for safe operation with missing sensor inputs, e.g., for self-driving cars. In the bad case, though the results are not desirable, the motion predictions still indicate the tendency of turning.

E. Toward Model Interpretability
We are now in a position to discuss model interpretability. Recall that in the update process (see Section III-C), the Kalman gain is an adaptive weight that balances the observations and the model predictions. If there is a high confidence in measurements, the Kalman gain will increase to selectively upweight measurement innovation and vice versa. This property gives us a unique opportunity to analyze model behaviors from the value changes of the Kalman gain.

To this end, we deliberately fed our Dirichlet model with degraded images and use the Kalman gain to capture the belief in measurements. This experiment generated 113 sequences with 15 frames of images from Sequence 09 in the KITTI dataset. For data degradation, a block was blanked on a sequence of 15 frames of images. The size of blank blocks gradually increases as time evolves until all pixels on an image are blank. Specifically, as shown in Fig. 8, in each sequence, the images are corrupted with an increasing size of blanked blocks on timesteps 1–5 (no blanked block), 6 and 7 (a blanked block with 192 pixels ×192 pixels), 8 and 9 (a blanked block with 192 pixels ×320 pixels), 10 and 11 (a blanked block with 192 pixels ×480 pixels), 12 and 13 (a blanked block with 192 pixels ×520 pixels), and 14 and 15 (a blanked block with 192 pixels ×640 pixels). The position of the blanked block is randomly selected on each image. We then test our model with this modified dataset in the same fashion as the VO experiment described in Section IV-B.


Fig. 8.
Sample images from the generated subsequences degraded with increasing size of blanked block in the interpretability experiment.

Show All

The Frobenius norm (2-norm) of the Kalman gain matrix is calculated and averaged across all sequences as an aggregated indicator of changes in the Kalman gain matrix. Fig. 9(a) shows that compared to the case with no degradation, this indicator gradually decreases with growing data corruption. It implies that our model can adaptively place more trust on model predictions when observing low-quality data and signal to higher control layers that estimation is becoming more uncertain. Similarly, we further visualize other explicit parameters inside our DynaNet model, i.e., the observation noise matrix R , process noise matrix Q , and residual noise matrix r . As shown in Fig. 9(b)–(d), these parameters are also able to capture and reflect the model uncertainty: with respect to the increasing data corruption, the observation noise rises up as well, indicating that the model is more uncertain about the observations. On the contrary, the process noise goes down, showing that the model has to place more belief in the prediction process when the observations are uncertain. Last but not least, the increasing residual noise also aligns with the uncertainty in observations. It is critical to note that data corrupted in this way have never been seen in the training phase.


Fig. 9.
Model Interpretation. (a) Kalman gain reflects the measurement qualities, which decreases with the rising degree of data corruption. (b) Increasing observation noise and (d) residual noise reflect the rise of observation uncertainty. (c) Decreasing process noise indicates that the model places more trust in process prediction when observations are uncertain.

Show All

SECTION V.Conclusion
DynaNet, a neural Kalman dynamical model, was introduced in this article to learn temporary linear-like structure on latent states. Through deeply coupled DNNs and SSMs, DynaNet can scale to high-dimensional data as well as model very complex motion dynamics in real world. By using the Kalman filter on feature space, DynaNet is able to reason about latent system states, allowing reliable inference and predictions even with missing observations. Furthermore, the transition matrix in our model is sampled from the Dirichlet distribution learned by an RNN, which ensures system stability in the long run. DynaNet is evaluated on a variety of challenging motion-estimation tasks, including single-modality estimation under data corruption, multiple sensor fusion under data absence, and future motion prediction. Experimental results demonstrate the superiority of our approach in accuracy, robustness, and interpretability.

In the future work, it would be interesting to further explore the application of proposed DynaNet model in motion prediction, e.g., evaluating and visualizing future steps predictions in various environments and measuring the robustness and reliability of a learned stable dynamical model compared with an unstable model.

Appendix Implementation Details of DynaNet Models
This appendix illustrates the implementation details of the experiments in VO and VIO. All the models are trained with 100 epochs.

SECTION A.Visual Odometry
Table V reports the framework for the VO, consisting of visual encoder to extract features a and observation noise matrix Q , neural transition models (deterministic or resampled) to generate transition matrix A and process noise matrix R , the Kalman filter pipeline to predict and update system states z , and pose predictor to output six-state poses ([Translation, Euler angles]) from systems states. Specifically, our visual encoder used the encoder structure of the FlowNetS architecture.

TABLE V Implementation Details for the VO Experiment

SECTION B.Visual-Inertial Odometry
As shown in Table VI, the framework for VIO used the same neural transition model, Kalman filter, and pose predictor as in VO, except the encoders and sensor fusion part. Table II shows that the 64-D visual and inertial features are extracted from data by visual and inertial encoders, respectively, and concatenated as a 128-D observation in sensor fusion. The inertial encoder used one-layer bidirectional LSTM to process inertial data.

TABLE VI Implementation Details for VIO. ⊕ Denotes a Concatenation Operation