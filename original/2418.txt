In this article, we propose a Dual Relation-aware Attention Network (DRANet) to handle the task of scene segmentation. How to efficiently exploit context is essential for pixel-level recognition. To address the issue, we adaptively capture contextual information based on the relation-aware attention mechanism. Especially, we append two types of attention modules on the top of the dilated fully convolutional network (FCN), which model the contextual dependencies in spatial and channel dimensions, respectively. In the attention modules, we adopt a self-attention mechanism to model semantic associations between any two pixels or channels. Each pixel or channel can adaptively aggregate context from all pixels or channels according to their correlations. To reduce the high cost of computation and memory caused by the abovementioned pairwise association computation, we further design two types of compact attention modules. In the compact attention modules, each pixel or channel is built into association only with a few numbers of gathering centers and obtains corresponding context aggregation over these gathering centers. Meanwhile, we add a cross-level gating decoder to selectively enhance spatial details that boost the performance of the network. We conduct extensive experiments to validate the effectiveness of our network and achieve new state-of-the-art segmentation performance on four challenging scene segmentation data sets, i.e., Cityscapes, ADE20K, PASCAL Context, and COCO Stuff data sets. In particular, a Mean IoU score of 82.9% on the Cityscapes test set is achieved without using extra coarse annotated data.

SECTION I.Introduction
Scene segmentation is one of the most fundamental yet challenging problems in computer vision, whose goal is to segment and parse a scene image into different image regions associated with semantic categories, including both stuff (e.g., sky, road, and grass) and discrete objects (e.g., person, car, and bicycle). The study of this task can be applied to potential applications, such as autonomous driving, robot sensing, and image editing. As a pioneer work, fully convolutional networks (FCNs) [1] adopt the pretrained classification networks [2]–[3][4][5] to obtain high-level semantic features for pixel-level classification and achieve an important breakthrough in semantic segmentation task. However, the structure of FCNs, with capturing limited receptive field and successive downsampling, leads to the problems of inconsistent prediction in large regions and the loss of the spatial details. Some examples are shown in Fig. 1 to illustrate the problems involved in the task.


Fig. 1.
Goal of scene segmentation is to recognize each pixel including stuff and diverse objects. The various scales, occlusion, and illumination changing of objects/stuff make it challenging to parse each pixel.

Show All

Recently, state-of-the-art methods based on FCNs [1] have been proposed to address the abovementioned issues. One way is to utilize multiscale context fusion. For example, some works [6]–[7][8] aggregate multiscale context via combining feature maps generated by different dilated convolutions and pooling operations. Other works [9], [10] capture global context information by enlarging the kernel size with a decomposed structure or introducing an effective encoding layer including image-level information. In addition, the encoder–decoder structures [11]–[12][13][14] are proposed to fuse low- and high-level semantic features to enhance spatial details. Although multiscale context fusion helps to capture objects in different scales, it ignores the relations between objects. Each object has different relationships with others, and the relations can be leveraged for better context modeling. Another type of method [15], [16] captures the long-range dependencies with recurrent neural networks, thus modeling relational context among pixels. However, the effectiveness of these methods relies heavily on the learning outcome of long-term memorization.

To address the abovementioned challenges, in this article, we propose a framework, called Dual Relation-aware Attention Network (DRANet), for natural scene image segmentation. By modeling a relation-aware attention mechanism, we can obtain guidance with a saliency map to adaptively select features to fuse together. Such an effective and efficient context fusion enhances feature representation, thus improving pixel recognition. Inspired by self-attention mechanism [17] that updates the feature at a position in a sequence by aggregating features at all positions with a weighted sum, we model contextual dependencies between any two pixels or channels (the feature maps in channel dimension of a feature tensor) and update the features. Adaptive context fusion is implemented for each pixel and channel by a position attention module (PAM) and a channel attention module (CAM), respectively. For the PAM, the feature of each position is updated via aggregating features at all positions with a weighted sum, where the weights are decided by the feature similarities between the corresponding two positions. An example can illustrate this process. When we identify a pixel from a car, we collect some contextual information associated with the pixel, thus allowing the pixel to obtain more information about cars. For the CAM, we use a similar mechanism to capture the channel dependencies between any two channels and update each channel adaptively.

Directly modeling relationships between any two pixels or channels may be redundant and brings a heavy burden on computation and memory if the number of pixels or channels is huge. In order to alleviate the problem, we further propose two types of compact attention modules (the compact means that the models’ relations are compact) in the spatial and channel dimensions, respectively. To be specific, we propose a compact PAM (CPAM), and it adopts several pooling kernels with different scales to obtain spatial gathering centers and then adaptively fuse them to each pixel according to semantic associations. For example, when we identify a pixel from a car, we collect information about the regions related to the pixel. For the compact CAM (CCAM), we adopt a dimension-reduced convolution layer to obtain channel gathering centers, and then, we update each channel with them. The two compact attention modules construct the relationships between each pixel/channel and few gathering centers instead of pairwise correlations, thus reducing computational complexity. In addition, we add a simple decoder structure to fuse high-level features and low-level features, thus enhancing spatial details. A cross-level gate is designed in the decoder structure to selectively refine the low-level features from the backbone.

With the abovementioned designs, our proposed DRANet captures rich context effectively and achieves top performance on four popular benchmarks, including the Cityscapes data set [18], the ADE20K data set [19], the PASCAL Context data set [20], and the COCO Stuff data set [21]. We also make the code and trained models publicly available.1

Our main contributions can be summarized as follows.

We propose a DRANet to model contextual dependencies in spatial and channel dimensions for accurate pixel-level recognition.

Two types of compact attention modules are designed to construct compact relationships that bring a performance boost with less cost on computation and memory.

We propose a simple yet effective decoder structure with a cross-level gate to refine low-level features, thus highlighting spatial details and obtaining accurate prediction.

Extensive experimental evaluations demonstrate that the proposed DRANet achieves new state-of-the-art performance on all four benchmarks.

The rest of this article is organized as follows. In Section II, we mainly review the related works about scene segmentation and scan some methods about attention mechanism. In Section III, we will introduce our proposed DRANet, including the design details of two types of attention modules, corresponding compact attention modules, the integration of two attention modules, and a cross-level gating decoder. To verify the effectiveness of our work, the experimental evaluations and necessary analysis are presented in Section IV. Finally, we summarize our work in Section V.
SECTION II.Related Work
A. Semantic/Scene Segmentation
FCN-based methods have made great progress in semantic/ scene segmentation; however, the architecture of FCNs results in the problems of rough edges and inconsistent predictions. Recently, many works try to alleviate these problems.

Many works retain spatial information by reducing downsampling operations and focus on utilizing global or multiscale context to improve semantic recognition. For example, ParseNet [22] exploits image-level information by concatenating global features to each pixel. PSPNet [8] designs a pyramid pooling module to collect the effective contextual prior, containing information of different scales. EncNet [10] proposes a Context Encoding Module to capture the semantic context of scenes and selectively highlight class-dependent feature maps. GCN [9] adopts the large kernel with a decomposed structure to obtain the global reception field. Deeplabv2 [6] and Deeplabv3 [7] adopt atrous spatial pyramid pooling to embed contextual information, which consists of parallel dilated convolutions with different dilated rates. Although these methods are effective, they suffer low efficiency of multiscale context fusion. In this work, we adopt a relation-aware attention mechanism to overcome this shortage and obtain performance improvement.

In addition, some works have noticed the importance of contextual dependencies and employ recurrent neural networks to exploit long-range dependencies. For example, the method [15] based on the 2-D LSTM networks is proposed to capture complex spatial dependencies on labels. DAG-RNN [16] adopts a generalization of RNNs to explicitly pass local context based on the directed acyclic graph, thus capturing rich contextual dependencies over image regions. MSCI [23] merges features from adjacent layers via LSTM chains to obtain powerful features. Compared with these RNN-based methods limited to long-term memorization constraints, we effectively model contextual dependencies by directly building the semantic associations in spatial and channel dimensions.

B. Attention Modeling
Visual attention modeling is an important research topic that has been explored in recent decades. The work [24] adopts explicit computational models of bottom–up attention to exploit attentive features. The representative works [25], [26] focus on biologically plausible computational models that emphasis on bottom–up control of attentional deployment. These efforts have inspired many explorations of attention modeling [27]. Different from the early works exploring computational mechanisms, our work aims at adopting attention mechanisms to provide a guidance to update features, thus adaptively enhancing relevant features and suppressing irrelevant features.

Some methods adopt attention modeling to update features with learned weights. For example, SENet [5] utilizes the global pooling feature to generate channel weights that are used to selectively highlight the channel map. CBAM [28] expands squeeze-and-excitation module to spatial dimension to emphasize meaningful regions. The work [29] adopts a spatial attention mechanism to enlarge the receptive fields of the network. The works [30] and [31] employ an attention mechanism with a spatial pyramid structure to emphasize the important patches at multiple spatial scales. The work [32] employs a parameter-free spatial attention mechanism to capture different significant detailed action information. In contrast to these methods, we update features by integrating relation-aware weighted features.

The self-attention mechanism has been widely explored in some applications. The work [17] is the first to propose the self-attention mechanism to draw global dependencies of inputs and apply it in machine translation. The work [33] adopts a self-attention mechanism to learn an image generator for image generation. The work [34] mainly explores the effectiveness of the nonlocal operation in space–time dimension for videos and images. The work [35] employs an adapted attention module to model the relative geometry between objects for object detection. DANet [36] and OCNet [37] have demonstrated the success of the self-attention mechanism in semantic segmentation tasks. Recently, many works are proposed to reduce the computational complexity of self-attention module. A^2Net [38] adopts weighted global pooling and weighted broadcasting to gather and distribute contextual information. CCNet [39] stacks two crisscross attention modules in a recurrent fashion. EMANet [40] adopts the EM algorithm to obtain a compact basis set and distribute the set in an iterative manner. Different from these methods, in this work, we propose a relation-aware attention model in spatial and channel dimensions. On the basis of the work DANet, we go one step further and design two compact attention modules to model the relationships between each pixel/channel and gathering centers, where gathering centers are obtained by spatial pyramid pooling in spatial dimension and dimension-reduction convolution operation in channel dimension.

C. Our Previous Method
A preliminary version of this work was published in CVPR 2019 [36]. In our previous version, we adopt the self-attention mechanism to capture contextual dependencies between any two pixels/channels. Such a relation-aware attention mechanism effectively improves the context fusion, thus achieving significant performance improvement. However, the attention modeling brings a heavy burden on computation and memory if the number of pixels/channels is huge. This journal is based on the conference version and introduces the attention modules in the work of the conference version. More importantly, this journal extension differs from the conference version in the following three aspects. First, we go one step further and design two compact attention modules to construct relationships between each pixel/channel and gathering centers instead of pairwise correlations. Each pixel/channel can benefit from the compact attention modules with less cost on computation and memory. Second, we add a simple yet effective decoder network with a cross-level gating mechanism. It exploits the relationships between low- and high-level features to selectively enhance spatial details. Third, comprehensive empirical results verify the effectiveness of our proposed method, and we achieve new state-of-the-art performance on all four well-known benchmarks.

SECTION III.Our Approach
A. Overview
In this article, we propose a novel network called DRANet that aims to improve feature representation for scene segmentation by modeling semantic associations through an attention mechanism.

As shown in Fig. 2, we design two types of attention modules to draw global context over local features generated from a dilated residual network and add a simple decoder network for high-resolution dense prediction. To be specific, we follow well-known methods [7], [8], [10] and employ a pretrained residual network (e.g., ResNet-50 and ResNet-101) with the dilated strategy [6], [10] as the backbone. Note that we remove the downsampling operations and employ dilated convolutions in the last two ResNet blocks, thus enlarging the size of the final feature map size to 1/8 of the input image. It retains more details without adding extra parameters. Then, the features from the dilated residual network are fed into two parallel attention modules that aim at capturing the global contextual dependencies in spatial and channel dimensions, respectively. In the attention modules, we build the relationships between any two pixels/channels (PAM/CAM) or between each pixel/channel and gathering centers (CPAM/CAM) and then employ them to selectively fuse contextual information. Next, we aggregate the outputs from the two attention modules to obtain better feature representations. Finally, we apply a simple yet effective decoder network to gradually recover the spatial details. In this way, DRANet not only enhances context aggregation for better recognition but also refines coarse results, thus producing accurate semantic predictions.


Fig. 2.
Overall architecture of the DRANet. The PAM/CAM focuses on building the relationships between any two pixels/channels, while the CPAM/CAM captures semantic association between gathering centers and each pixel/channel. (Best viewed in color.)

Show All

B. Position Attention Module
Many works [8], [9] suggest that local features generated by traditional FCNs could not capture rich context, which may lead to misclassification of objects and stuff. In order to model rich contextual relationships over local features, we introduce a PAM. The PAM selectively encodes a wider range of contextual information into each pixel according to semantic-correlated relations, thus enhancing discrimination power for dense predictions.

To be specific, we directly construct the semantic associations between any two pixels. Each pixel collects effective context information from semantic-correlated pixels of the entire image. As shown in Fig. 3(a), given a local feature A∈RC×H×W , we first feed it into two dimensionality-reduction convolution layers to generate two new features B and C , respectively, where {B,C}∈RC¯×H×W . Then, we reshape them to RC¯×N , where N=H×W is the number of pixels. Next, we perform a matrix multiplication between the transpose of C and B and apply a softmax layer to calculate the spatial attention map S∈RN×N
sji=exp(Bj⋅Ci)∑Ni=1exp(Bj⋅Ci)(1)
View Sourcewhere sji measures the ith pixel’s impact on jth pixel. The more similar feature representation of the two positions contributes to the greater correlation between them.


Fig. 3.
Details of (a) PAM and (b) CPAM. Gray rectangle represents a 1×1 convolution layer, cyan rectangle represents a fully connected layer, and yellow rectangle represents a multiscale pooling layer followed by a 1×1 convolution layer. (Best viewed in color.)

Show All

Meanwhile, we feed the feature A into a convolution layer to generate a new feature D∈RC×H×W and reshape it to RC×N . Then, we perform a matrix multiplication between D and the transpose of S and reshape the result to RC×H×W . Finally, we multiply it by a scale parameter α and perform an elementwise sum operation with the feature A to obtain the final output E∈RC×H×W as follows:
Ej=α∑i=1N(sjiDi)+Aj(2)
View Sourcewhere α is initialized as 0 and gradually learns to assign more weight [33]. It can be inferred from (2) that the resulting feature E at each position is a weighted sum of the features at all positions and original features. Therefore, it selectively aggregates contexts according to the spatial attention map in a global view. The information of each pixel would pass to its semantic-correlated pixels, which improves semantic consistency.

C. Compact Position Attention Module
Since we need get the relationships between any two pixels by the vectorial internal product, it requires expensive GPU memory storage and computational cost if the number of pixels is huge. In order to alleviate the problem, we propose a CPAM that constructs the relationships between each pixel and a few numbers of gathering centers. The gathering centers are formally defined as a compact feature vector by gathering feature vectors from a pixel subset in the input tensor. They are implemented by a spatial pyramid pooling scheme that provides context information from different spatial scales.

As shown in Fig. 3(b), we first feed the feature A∈RC×H×W into a multiscale pooling layer followed by a 1×1 convolution layer and obtain several pooled features with varied bin sizes of 1×1,2×2,3×3 , and 6×6 , respectively. The features with a bin size of 6×6 are omitted for brevity in Fig. 3(b). Then, we regard each bin of the pooled features as a gathering center and reshape the features with a bin size of L×L to RC×L2 . Finally, the gathering centers F∈RC×M are obtained by cascading the bins of all pooled features, where M is the sum of the number of bins of all pooled features.

Next, we adaptively integrate the gathering centers to each pixel according to semantic correlations. Especially, we feed the features A and F into a 1×1 convolution layer and a fully connected layers to obtain B and C , respectively, where B∈RC¯×H×W and C∈RC¯×M . Similar to Fig. 3(a), matrix multiplication and a softmax layer are used to obtain the spatial attention map S∈RN×M
sji=exp(Bj⋅Ci)∑Mi=1exp(Bj⋅Ci)(3)
View Sourcewhere sji measures the relationship between the ith center’s and the jth pixels. Then, we feed the gathering centers F into a fully connected layer to obtain the features D∈RC×M . After obtaining the features S and D , the output feature E can be computed as follows:
Ej=α∑i=1M(sjiDi)+Aj.(4)
View Source

Different from the works [40] and [41] that capture global descriptors by learned parameter matrix and project global features back to the original pixels by reverse parameter matrix, we capture multiple gathering centers with various context by simple pooling operations and enhance each spatial pixel with a relation-aware weighted sum of the centers.

D. Channel Attention Module
Each channel map of high-level features can be regarded as a class-specific response, and different semantic responses are associated with each other. By exploiting the dependencies between any two channel maps, we improve the feature representation of relevant semantics. Therefore, we build a CAM to explicitly model dependencies between any two channels.

The structure of CAM is shown in Fig. 4(a). Different from the PAM, we directly calculate the channel attention map X∈RC×C from the original feature A∈RC×H×W . Especially, we reshape A to RC×N . Then, we perform a matrix multiplication between A and the transpose of A . Finally, we apply a softmax layer to obtain the channel attention map X∈RC×C
xji=exp(Aj⋅Ai)∑Ci=1exp(Aj⋅Ai)(5)
View Sourcewhere xji measures the ith channel’s impact on the jth channel in the feature A . In addition, we perform a matrix multiplication between the transpose of X and A and reshape their result to RC×H×W . Then, we multiply the result by a scale parameter β and perform an elementwise sum operation with A to obtain the final output E∈RC×H×W
Ej=β∑i=1C(xjiAi)+Aj(6)
View Sourcewhere β gradually learns a weight from 0. Equation (6) shows that the final feature of each channel is a weighted sum of all channels and the original feature, which models the long-range semantic interdependencies between channels.


Fig. 4.
Details of (a) CAM and (b) CCAM. Gray rectangle represents a 1×1 convolution layer. (Best viewed in color.)

Show All

E. Compact Channel Attention Module
The problem of computational burden needs to be paid attention to if the number of channel map is large. To solve this issue, we propose a CCAM that builds relationships between each channel map and channel gathering centers.

As shown in Fig. 4(b), we define gathering centers as a compact channel map by gathering channel maps from the input tensor. Especially, we adopt a 1×1 convolution layer to reduce channel dimensions of the feature A , obtaining the feature F∈RK×H×W . Each channel map of F is viewed as a gathering center. Then, we calculate the channel attention map X∈RC×K as follows:
xji=exp(Aj⋅Fi)∑Ki=1exp(Aj⋅Fi)(7)
View Sourcewhere xji measures the ith center’s impact on the jth channel of feature A . Then, we selectively integrate the channel gathering centers into each channel of feature A to obtain the output feature E as follows:
Ej=β∑i=1K(xjiFi)+Aj.(8)
View Source

Note that we do not employ convolution layers to transform features before computing relationships along the channel dimension since it can maintain relationships between different channel maps. Different from recent works [10] and [42] that explore channel relationships by a global pooling or encoding layer, we exploit spatial information at all corresponding positions to model channel correlations.

F. Attention Module Embedding With Networks
In order to take full advantage of long-range contextual information, we aggregate the features from the two attention modules by a concatenation fusion module. Especially, we transform the outputs of two attention modules by a convolution layer and concatenate them together and apply a dimensionality-reduction convolution layer to obtain fused features. Moreover, we also discuss different fusion methods (i.e., sum or concat and cascade or parallel) in experiments and finally adopt a parallel structure with concatenation fusion. Note that our attention modules are simple and can be directly inserted in the existing FCN pipeline. They do not increase too many parameters yet strengthen feature representations effectively.

G. Cross-Level Gating Decoder
Many works [12], [43], [44] suggest that low-level visual information improves accurate prediction on the boundaries and details, but its local receptive fields also may lead to misclassification on other regions. Different from the works [12] and [45] that fuse the low- and high-level features directly, we adopt a cross-level gating mechanism to provide a guidance to selectively enhance spatial details and suppress other regions, thus obtaining better cross-level feature fusion. Therefore, we propose a simple yet effective decoder structure with a cross-level gate, as shown in Fig. 5.


Fig. 5.
Details of cross-level gating decoder. (Best viewed in color.)

Show All

Especially, the outputs from the fusion module perform bilinear upsampling by a factor of 2 to generate the features H . Meanwhile, we apply a 3×3 convolution layer with 32 kernels (Conv+BN+ReLU) on the outputs of ResNet block1 to obtain the low-level features L . In order to fuse low-level features L and the features H effectively, we feed them into a cross-level gate to refine the low-level features. Especially, we concatenate the two features and feed them into a 1×1 convolution layer with one kernel followed by a sigmoid layer to obtain a spatial gating map. Then, we reweight the low-level features L according to the gating map. The boundaries or details are different between the two features, and the spatial gating map can be obtained to emphasize the region of boundaries and details. Then, we reweight the low-level features L according to the gating map, making the region of boundaries and details more responsive After obtaining refined low-level features M , we concatenate it and H together followed by two 3×3 convolution layers with 256 kernels (Conv+BN+ReLU). Finally, we adopt a convolution layer for pixel-level classification and perform bilinear upsampling by a factor of 4 for high-resolution outputs.

SECTION IV.Experiments
To evaluate the proposed method, we carry out comprehensive experiments on the Cityscapes data set [18], the ADE20K [19], the PASCAL Context data set [20], and the COCO Stuff data set [21]. Experimental results demonstrate that DRANet achieves state-of-the-art performance on all data sets. In the following, we first introduce the data sets and implementation details, and then, we perform a series of ablation experiments on the Cityscapes data set. Finally, we present a comparison of our solution with some state-of-the-arts on the four data sets.

A. Experimental Setting
1) Data Set:
a) Cityscapes:
The data set has 5000 images captured from 50 different cities. Each image has 2048×1024 pixels that have high-quality pixel-level labels of 19 semantic classes. There are 2975 images in the training set, 500 images in the validation set, and 1525 images in the testing set. We do not use coarse data in our experiments.

b) Series ADE20K:
The data set is a new scene parsing benchmark containing 20 210 images for training, 2000 images for validation, and 3352 images for testing. There are 150 labels totally in this data set, including 35 stuff concepts and 115 discrete objects. The unbalanced distribution of labels and diverse scales of images makes the data set more challenging.

c) PASCAL context:
The data set provides detailed semantic labels for whole scenes, which contains 4998 images for training and 5105 images for testing. Following [10] and [36], we evaluate the method on the most frequent 59 classes along with one background category (60 classes in total).

d) COCO stuff:
The data set contains 9000 images for training and 1000 images for testing. Following [11], we report our results on 171 categories including 80 objects and 91 stuffs annotated to each pixel.

2) Implementation Details:
We implement our method based on Pytorch and employ a “poly” learning rate policy where the initial learning rate is multiplied by (1(iter/total_iter))0.9 after each iteration. The base learning rate is set to 0.01 for the Cityscapes data set. Momentum and weight decay coefficients are set to 0.9 and 0.0001, respectively. We train our model with Synchronized BN [10]. We follow the practice of works [7] and [10] and set the batch size to 8 for Cityscapes and 16 for other data sets. When adopting multiscale augmentation, we set the base learning rate to 0.005 and the training time to 240 epochs for all data sets. For data augmentation, we apply random cropping (crop size 768) and random left–right flipping during training in the ablation study for the Cityscapes data sets.

B. Ablation Study
We perform the ablation study on Cityscapes val set to verify the effectiveness of each component of our method. All experiments are conducted using ResNet-50 as the backbone.

1) Position/Channel Attention Modules:
To evaluate the two attention modules, we conduct experiments by adding them individually on the top of the dilated residual network.

As shown in Tables I and II, the attention modules improve the performance remarkably. Compared with the baseline dilated FCN, employing a PAM yields a result of 75.74% in Mean IoU, which brings 5.71% improvements. Table II shows that employing a CAM individually also outperforms the baseline by 4.25%. These results indicate that learning the semantic correlations between any two pixels or channels helps dense classification. We also report the increment of GFLOPs2 and GPU memory usage3 when adding the two modules, respectively. Especially, although PAM improves the performance significantly, the usage of GFLOPs and GPU memory increase by 103.81 and 725.75 MB. CAM also brings an extra 9.66 GFLOPs and 54.00-MB GPU memory compared with the baseline network.

TABLE I Ablation Study on Cityscapes Val Set. PAM Represents Position Attention Module, and CPAM Represents Compact Position Attention Module. ( n ) and ( mn ) Denote PooledFeature Maps of Bin Sizes {n×n} and {n×n,m×m} , Respectively

TABLE II Ablation Study on Cityscapes Val Set. CAM Represents Channel Attention Module, and CCAM Represents Compact Channel Attention Module. ( n ) Denotesthe Channel Dimension of the Feature F

2) Compact Position/Channel Attention Modules:
To reduce the computational cost and memory usage, we further design CPAM and CCAM. We evaluate the two compact attention modules as follows.

As shown in Table I, we adopt several settings of gathering centers, including pooling with a single scale and multiple scales. Especially, when we adopt the single scale pooling in CPAM(2), the increments of GFLOPs and GPU memory usage are significantly reduced to 0.65 and 19.03 MB. When we use multiscale pooling to obtain 50 gathering centers (the last row), we could achieve comparable performance with PAM and save computation cost and GPU memory simultaneously. Therefore, four-scales gathering centers [CPAM(1236)] are adopted finally.

In addition, we provide the per-class IoU to evaluate the effect of PAM and CPAM(1236). As shown in Table III, although the two methods have similar overall performance, they have different performance in some classes. Especially, PAM performs better than CPAM(1236) in some classes containing a lot of spatial details (e.g., fence, pole, and traffic light), while CPAM(1236) obtains comparable or better performance on some classes containing mainly big objects (e.g., truck, bus, and wall). It may be that the feature D in PAM could have more local representations than the pooled feature D in CPAM(1236), and the details can benefit from them. Some example results of different settings are shown in Fig. 6.

TABLE III Per-Class Results on Cityscapes Val Set. PAM Represents Position Attention Module, and CPAM Represents Compact Position Attention Module


Fig. 6.
Example results of four settings in Table I. We observe that in the red box, the results of CPAM(1236) are improved compared with that of Dilated FCN. The red box contains big objects and stuff, which benefits from the gathering centers. In addition, the gathering centers in CPAM(1236) are pooled features and potentially less discriminative to local regions related to some details, resulting in our worse performance on some boundary area.

Show All

We also explore the number of gathering centers (the channels of the feature F ) in CCAM. As listed in Table II, when we adopt CCAM with 32 gathering centers, we could achieve a better performance of 74.63% with less usage of GFLOPs (0.90). The results indicate that our structure building compact relationships can help the performance of the network, but too few channels may not fully capture channel interdependencies. Therefore, the number of gathering centers is finally set to 32. Interestingly, we can find that the GPU memory of CCAM(32) is slightly higher than that of CAM, so we compare GPU memory between CCAM(32) and CAM in detail. Although CCAM(32) reduces the memory of the feature X from 2.94 to 0.19 MB, its extra feature F and other convolution layers occupy 3.38-MB memory.

In our attention modules, the resolution of the input feature has a great influence on the GFLOPs and GPU memory. We provide a comparison of GFLOPs and GPU memory between PAM and CPAM(1236), and CAM and CCAM(32). As shown in Fig. 7, compared with CAM and PAM, the advantages of CPAM(1236) and CCAM(32) are more significant at a higher resolution. In addition, the memory usage of CCAM(32) is almost the same as that of CAM.


Fig. 7.
Comparison of PAM and CPAM(1236), and CAM and CCAM(32). (a) GFLOPs as a function of resolution H×W . (b) Memory usage as a function of resolution H×W .

Show All

Note that we will adopt the compact attention modules in the following experiments as defaults due to the balance among the performance, GFLOPS, and memory usage.

3) Comparisons With Multiscale and Attention Methods:
In order to evaluate our model effectively, we conduct experiments, including multiscale methods (e.g., PPM [8] and ASPP [7]) and other attention methods related to self-attention module (e.g., OCNet [37], A^2Net [40], EMANet [38], and CCNet [39]). In order to ensure fair comparison, we adopt ResNet-50 as backbone and the same training and testing settings as CPAM(1236) to evaluate these method.

The results are shown in Table IV. Compared with well-known multiscale methods, such as PPM and ASPP, CPAM(1236) can adaptively fuse the multiscale features for each pixel considering their relationships and also obtain higher performance with less memory occupancy. Meanwhile, OCNet obtains a higher performance with more cost on GLOPs and memory since it combines self-attention and ASPP modules. By reducing connections with each pixel, the methods (e.g., A^2Net, EMANet, and CCNet) obtain less the computational cost compared with self-attention modules (e.g., PAM and OCNet). Since we adopt simple pooling operations to obtain multiscale gathering centers, we can bring less cost to GFLOPs and memory.

TABLE IV Comparisons With Multiscale and Attention Methods

4) Visualization of Attention Module:
For the PAM, the overall self-attention map is in the size of (H×W)×(H×W) , which means that there is a corresponding subattention map whose size is (H×W) for each specific point in the image. For compact position attention, the overall attention map is in the size of (H×W)×(M) , and we can obtain subattention map with the size of (1)×(M) for each specific point and split it into multiple maps with different scale bins. The different bins reflect the influence of different scales on the point. As shown in Fig. 8, for each input image, we select one point and show their corresponding subattention map in PAM and multiple maps corresponding to pooled features with bin sizes of 1×1,2×2,3×3 , and 6×6 in CPAM(1236). In the first row, the red point is marked on a road, and its attention map in PAM (column 2) highlights most of the areas where the road lies. In the second row, the same holds for the “rider” in the global region even though the number of corresponding pixels is less. It implies that PAM could capture associations with clear semantic similarity. We also provide multiple maps (from columns 3–6) corresponding to the red point in CPAM(1236). We can find that the pooled features with bin sizes of 1×1,2×2,3×3 have more similarity on the stuff (“road”) than small objects (“person”), and pooled features with a bin size of 6×6 have a high response on small objects, which indicates our multiscale pooled features can deal with different scale objects.


Fig. 8.
Visualization results of attention modules on Cityscapes val set. Top: input image, the subattention map (H×W) corresponding to the points marked in the input image from PAM, and multiple attention bins corresponding to the points marked in the input image from CPAM(1236). Bottom: two channel maps from the outputs of CAM and CCAM(32), respectively, and corresponding ground truth.

Show All

For channel attention, it is hard to give comprehensible visualization about the attention map directly. Instead, we show some attended channels to see whether they highlight clear semantic areas in CAM and CCAM(32). In the first picture, we provide the channel map responding to the “road” class in outputs of CAM and CCAM(32), respectively, while the channel map responding to the “tree” class is provided in the second picture. In short, these visualizations further demonstrate our modules could capture long-range dependencies for improving feature representation in scene segmentation.

5) Embedding Attention Modules:
After obtaining CPAM and CCAM, we explore the different fusion implementation of the two attention modules. Especially, the two compact attention modules can be arranged in a cascade or parallel manner. As shown in Table V, the parallel structures outperform the cascade structures. In parallel structures, the fusion using concatenate operation (Concat) outperforms that using the elementwise sum operation (Sum). Therefore, the parallel structure (Concat) is adopted.

TABLE V Ablation Study of Different Fusion Implementation of the Two Compact Attention Modules on Cityscapes Val Set. (CP-CC) Indicates that CPAM is Before of CCAM, and (CC-CP) is the Opposite

6) Cross-Level Gating Decoder:
To verify the effectiveness of the decoder structure, we conduct experiments with different settings, including a decoder structure with and without the cross-level gate.

As listed in Table VI, our method without using the decoder yields a performance of 75.88%. Adding a decoder without the cross-level gate brings extra 0.59% improvement. When we further adopt the cross-level gate to refine low-level features, we achieve the best performance of 77.19%, which shows the effectiveness of our cross-level gating decoder.

TABLE VI Ablation Study on Cityscapes Val Set. Decoder (w/o Gate) Represents the Decoder Network Without Cross-Level Gate, and Decoder (w/ Gate) Represents the Decoder Network With the Cross-Level Gate

Furthermore, we evaluate the segmentation accuracy with the trimap experiment [59], [60] to quantify the accuracy of the decoder structure. Especially, we compute the Mean IoU for those pixels locate close to the boundary of objects (the dilated band along the boundary is also called trimap). As shown in Fig. 9, the boundary of objects benefits from a decoder structure. The cross-level gate brings extra improvement when the dilated band is large. In addition, we provide an example of a visualization of the gating map. We can find that the regions containing spatial details (e.g., “pole” and “traffic sign”) are emphasized, while the regions containing some stuff (e.g., “building”) are suppressed. It indicates that the gate gives the network a guidance to pay attention to important details.


Fig. 9.
(a) Visualization result and (b) Mean IoU as a function of the trimap along the object boundary. Gate: cross-level gate.

Show All

We also provide the computation time comparison of different components in Table VII.

TABLE VII Computation Time Comparison for an Input Image of Size ( 1×3×768×768 ) During Inference. CPAM & CCAM Represents the Proposed Compact Attention Modules, and GatingDer Represents the Proposed Decoder

7) Some Common Improvement Strategies:
For comparison with other state-of-art models, we follow [7], [8], [39], and [47] and adopt several common strategies to improve the performance of our model. The results are shown in Table VIII. First, we adopt a deeper pretrained network (ResNet-101) as our backbone, which improves the performance to 78.39%. Then, we apply a hierarchy of grids of different sizes (4, 8, 16) in the last ResNet block, which brings an extra 0.99% improvement. Next, data augmentation with random scaling (from 0.5 to 2.2) improves the performance by 1.58% further. Moreover, OHEM [37] improves the performance to 81.63%. Finally, we adopt multiscale testing during inference that promotes the final performance to 82.90%. We provide visualization results of our best model on the validation set in Fig. 10, which shows that our model can obtain high-quality segmentation results.

TABLE VIII Performance Comparison Between Different Strategies on Cityscape Val Set. DRANet-101 Represents DRANet With BaseNet ResNet-101, and DA Represents Data Augmentation With Random Scaling. Multigrid Represents Employing Multigrid Method, and OHEM Represents Online Hard Example Mining. MS Represents Multiscale Inputs During Inference


Fig. 10.
Example results between different methods. The images in each column from top to bottom are (a) input image, (b) PSPNet [8], (c) DANet [36], (d) DRANet, and (e) ground truth. (Best viewed in color.)

Show All

C. Comparing With State-of-the-Art
1) Results on Cityscapes Data Set:
We further compare our method with existing methods on the Cityscapes testing set. Especially, we train our DRANet-101 with only fine annotated data and submit our test results to the official evaluation server. Results are shown in Table IX. Our single model achieves a new state-of-the-art performance of 82.9%,4 which outperforms recent existing approaches by a large margin. In particular, our model outperforms DANet [36] by a large margin. Compared with the work [57], we adopt a multilevel feature to obtain a gating map and achieve higher performance. In addition, we also perform better than the works modeling long-range dependencies with graph convolution [52] or strip pooling [54]. Moreover, it also surpasses DenseASPP [49] that uses more powerful pretrained models.

TABLE IX Per-Class Results on Cityscapes Testing Set. DRANet Outperforms Existing Approaches and Achieves 82.9% in Mean IoU

2) Results on ADE20K Data Set:
We conduct experiments on the ADE20K data set to validate the effectiveness of our method. Following previous works [8], [10], [38], and [62], we adopt data augmentation with random scaling and multiscale testing in training and testing phases. Especially, we set the initial learning rate to 0.005, the crop size to 608×608 , the batch size to 16, and train models for 240 epochs. The model is trained and tested with multiscale schemes (0.5, 0.75, 1.0, 1.25, 1.5, 1.75). We evaluate DRANet by the standard metrics of pixel accuracy (pixAcc) and mean intersection of union (Mean IoU). Quantitative results are shown in Table X. DRANet-50 obtains 45.20%/81.48% in terms of Mean IoU and PixelAcc. When we employ a deeper backbone ResNet101, DRANet-101 attains the best performance of 46.18%/81.91% on the validation set, which outperforms all previous state-of-the-art methods. Different from [8], [10], [39], and [62], our method focuses on capturing the pixel-aware contexts from the features of different granularity according to the semantic association. Some examples of the ADE20K validation set are shown in Fig. 10.

TABLE X Segmentation Results on the ADE20K Validation Set

In addition, we also follow [10] and fine-tune our best model of DRANet-101 with train+val data. We set the initial learning rate to 0.0001 and the training iterations to 40 epochs and obtain our test results with the abovementioned multiscale testing schemes. The results are shown in Table XI. Our results are submitted to the official evaluation server and evaluated by the mean of Mean IoU and Pixel Acc (final score). DRANet-101 gets the final score of 56.72%5 and ranks the first as a single model performance.

TABLE XI Segmentation Results on the ADE20K Testing Set

3) Results on PASCAL Context Data Set:
In this section, we carry out experiments on the PASCAL Context data set to further evaluate the effectiveness of our method. Especially, we set the initial learning rate to 0.001, the crop size to 544×544 , the batch size to 16, and the train models for 240 epochs. Quantitative results of PASCAL Context are shown in Table XII. DRANet-101 achieves a Mean IoU 55.4%, which outperforms previous methods by a large margin. Among previous works, RefineNet [12] adopts a deeper model (ResNet152) to improve their segmentation results. DANet [36] and CFNet [63] adopt self-attention mechanism to capture long-range dependencies with much computational cost. Different from previous methods, we employ relation-aware attention modules to model contextual dependencies between pixels/channels with the gathering centers, and the proposed method can achieve better performance.

TABLE XII Segmentation Results on the PASCAL Context Testing Set

4) Results on COCO Stuff Data Set:
We also conduct experiments on the COCO Stuff data set to verify the generalization of our proposed network. Especially, we set the initial learning rate to 0.001, the crop size to 544×544 , the batch size to 16, and the train models for 240 epochs. Comparisons with previous state-of-the-art methods are reported in Table XIII. The results show that our model achieves 41.2% in Mean IoU, which outperforms these methods by a large margin. Among the compared methods, DAG-RNN [16] utilizes chain-RNNs for 2-D images to model rich spatial dependencies, and Ding et al. [11] adopted a gating mechanism in the decoder stage for improving inconspicuous objects and background stuff segmentation. Our method could capture long-range contextual information more effectively and learn better feature representation in scene segmentation.

TABLE XIII Segmentation Results on the COCO Stuff Testing Set

SECTION V.Conclusion
In this article, we present a DRANet for scene segmentation, which adaptively integrates contextual information via an attention mechanism. Especially, we adopt a PAM and a CAM to capture global dependencies in the spatial and channel dimensions, respectively. The PAM computes a semantic similarity map for each pixel and updates the feature of the pixel with the weighted summation of the features of all the pixels according to the similarity map. Such an adaptive feature fusion enables each pixel to obtain semantic associated contexts. Similar to the PAM, the CAM updates each channel map by aggregating all channel maps with a weighted sum. It explicitly models dependencies between any two channels. Moreover, we design two types of compact attention modules to reduce the huge computational cost and memory occupancy. By condensing all pixels/channels to a few spatial/channel gathering centers, we can build compact relationships between gathering centers and pixels/channels. Since the number of gathering centers is far less than the number of pixels, the computational cost and memory occupancy of the similarity map can be reduced significantly. Meanwhile, a cross-level gating decoder is proposed to enhance spatial details. We employ the low- and high-level features to obtain a spatial gating map and then emphasize the region of boundaries and details by reweighting the low-level features according to the gating map. The ablation experiments show that our method captures contextual information effectively and gives more precise segmentation results compared with Dilated FCN. Our proposed DRANet achieves outstanding performance consistently on four scene segmentation data sets, i.e., Cityscapes, ADE20K, PASCAL Context, and COCO Stuff.