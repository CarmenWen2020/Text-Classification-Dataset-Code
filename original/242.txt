Multimedia collections usually induce multiple emotions in audiences. The data distribution of multiple emotions can be leveraged to facilitate the learning process of emotion tagging, yet has not been thoroughly explored. To address this, we propose adversarial learning to fully capture emotion distributions for emotion tagging of multimedia data. The proposed multimedia emotion tagging approach includes an emotion classifier and a discriminator. The emotion classifier predicts emotion labels of multimedia data from their content. The discriminator distinguishes the predicted emotion labels from the ground truth labels. The emotion classifier and the discriminator are trained simultaneously in competition with each other. By jointly minimizing the traditional supervised loss and maximizing the distribution similarity between the predicted emotion labels and the ground truth emotion labels, the proposed multimedia emotion tagging approach successfully captures both the mapping function between multimedia content and emotion labels as well as prior distribution in emotion labels, and thus achieves state-of-the-art performance for multiple emotion tagging, as demonstrated by the experimental results on four benchmark databases.
SECTION 1Introduction
People are inundated with a large amount of digital multimedia, which are easily accessed from the Internet by tablet, smart-phone, and other user-friendly devices. These multimedia collections including image, music, video, voice, and text, are not only good carriers for information and knowledge, but also common ways for communication, entertainment, and emotional interaction. Automatic multimedia emotion tagging is one basic task in the field of affective computing [1], which has recently attracted increasing attention [2], [3], [4], [5], [6], [7], [8], as it benefits both multimedia data providers and consumers. With the help of automatic multimedia emotion tagging, the former could accurately recommend data for target populations, and the latter could quickly find the specific data they want.

Current multimedia emotion tagging work assumes that a medium only has one emotion tag, and thus totally ignores the dependencies among multiple emotions. However, most multimedia data induce several emotions from an audience. Some emotions are frequently found simultaneously, while some rarely happen together. For examples, as shown in Fig. 1a, two frames from a raffle variety show probably cause audiences to feel joy as well as tenderness, but not fear or disgust. On the contrary, audiences may feel fear and disgust, but not joy and tenderness, while watching a thriller movie, i.e., “Se7en”, as shown in Fig. 1b. It is unlikely that joy and/or tenderness will coincide with fear and/or disgust. This label distribution inherent in multiple emotions can be leveraged for multimedia emotion tagging.


Fig. 1.
Image samples from the FilmStim database inducing a mixture of emotions instead of one single emotion.

Show All

Some recent works have started to explore dependencies among emotions for multimedia emotion tagging. They enforce the predicted emotions to satisfy the first-order [9], second-order [10], or higher-order [11] dependencies that are extracted from ground-truth emotion labels, and demonstrate the significance of emotion dependencies for detecting emotions from multimedia content. Although their explored dependencies can represent emotion label distributions to some extent, they are far from enough to represent emotion distributions, which contain tens of thousands of kinds of label dependencies and their probabilities. Current multiple emotion tagging works fail to explicitly address the statistical aspect of the predicted emotion labels, so the predicted emotion labels are still statistically different from ground truth emotion labels. The distributions must be measured in a certain way to enforce statistical similarity between the predicted labels and ground truth labels, and this is not a trivial task. Some works have tried to capture joint label distribution for multiple emotion tagging, such as Bayesian network (BN) [3] and restricted boltzmann machine (RBM) [4]. However, they need some assumptions. For limiting the complexity of network, BN assumes that some nodes are conditional independent, and thus not linked directly. RBM assumes a specific form of joint label distribution, which is not applicable in some cases.

To address the statistical similarity between the predicted emotion labels and ground-truth emotion labels without any assumptions, we propose a novel emotion tagging approach through adversarial learning. Specifically, the proposed emotion tagging approach consists of an emotional tag classifier C and a discriminator D. In addition to minimizing the traditional supervised loss, another adversarial loss from discriminator D is incorporate to enforce statistical similarity between the distribution inherent in the predicted and the ground truth emotion tags. The discriminator D tries to distinguish the ground truth emotion labels from the emotion labels predicted from C, and C tries to let discriminator D make a mistake while minimizing the supervised loss. Through the competition between the classifier C and the discriminator D, the proposed multimedia emotion tagging approach can successfully leverage the data distribution inherent in ground-truth emotion labels for facilitating the learning process of emotion tag classifier. Experiments on four benchmark databases demonstrate the superiority of the proposed approach, which incorporates the emotion distributions into the multiple emotion tagging, as compared to the state-of-the-art works.

SECTION 2Related Work
2.1 Multiple Emotion Tagging of Multimedia
Multimedia emotion tagging of music, videos, and images has attracted increasing attention due to its great potential in the digital media consumer market. For example, Wang et al. [12] and Wang et al. [13] reviewed research on emotional semantic image retrieval. Joshi et al. [14] extensively surveyed several key aspects of computational inference of aesthetics and emotion from images. Wang and Ji [15] summarized video affective content analysis into two categories: direct approaches and indirect approaches. Kim et al. [16] explored the methods that use contextual text information (e.g., websites, tags, and lyrics) and music content for music emotion recognition. Yang et al. [2] provided a comprehensive review of machine learning methods that have been proposed for music emotion recognition. Although images, videos, and music are different modalities, emotion tagging for these three kinds of media follows similar framework, i.e., feature extraction and classification. Therefore, several works consider emotion tagging of three media simultaneously. For example, Wang et al. [17] summarized researches conducted on emotional semantic detection from images, videos, and music.

Most research on multimedia emotion tagging assumes one emotion for one media. However, this assumption is not valid when detecting emotion from music, images, and videos, since media collections usually induce several emotions simultaneously. Li and Ogihara [18] may have been the first to realize the fallacy of the single emotion assumption. They decomposed the problem of multiple emotion detection from music into a set of binary classification problems, ignoring the dependencies among labels. Later, Trohidis et al. [19] compared seven multi-label classification algorithms for emotion detection from music, including binary relevance (BR) [9], label powerset (LP), random k-label sets (RAKEL) [20], multi-label k-nearest neighbor (MLkNN) [21], ranking by pairwise comparison (RPC) [22], calibrated label ranking (CLR) [10], and multi-label back-propagation (BPMLL) [23]. These methods explore first-order, second-order, and higher-order label dependencies from target labels or with the help of features and hypotheses, and demonstrate the potential of multi-label modeling for emotion detection from music. However, these works focus on certain kinds of label dependencies and fail to explicitly explore the label distributions existing in multiple emotions.

More recently, Wang et al. [3] proposed a framework of multi-label multimedia emotion tagging for music as well as images and videos. Specifically, they proposed a BN to capture the label distribution directly from the target emotion labels. However, this model can only capture the pairwise dependencies among multiple emotions due to the first-order markov assumption of BNs. To mitigate this limitation, Wang et al. [4] proposed a three-layer RBM (TRBM) model to capture the higher-order label distribution among multiple emotions. Instead of modeling dependencies among emotion labels only, Wu et al. [5] modeled multiple emotions’ dependencies in both feature and label spaces. They proposed two models: a multi-task three-layer RBM (MT-TRBM) and a four-layer RBM (FRBM). The MT-TRBM models the dependencies among labels and the dependencies among features independently, while FRBM models them dependently. The above works adopt probabilistic graphical models to explicitly capture label distributions embedded in multiple emotions. However, they assume an explicit form of joint label distribution, i.e., the joint probabilities of the adopted graphical model. These explicit forms of joint label distribution may not be consistent with the ground truth emotion distributions.

To address this, we propose to close the joint distribution inherent in predicted and ground truth emotion labels through an adversarial learning framework, which requires no assumptions of distribution form. It's important to note that the joint label distribution we consider is different from the label distribution used in label distribution learning (LDL) [24]. In our work, one instance has several binary labels, while in [24], one instance is assigned several real values, representing the degree to each label which describes the instance. The sum of all real values is equal to one. Therefore, the label distribution we consider is the joint distribution of all labels, since the population contains all training instances. The label distribution in [24] is only for one instance.

2.2 Multi-Label Classification
When each instance has more than one label, it is a multi-label classification problem. This type of classification is concerned with exploring label dependencies. Current multi-label classification methods can be roughly divided into two types: problem transformation methods and algorithm adaptation methods.

Problem transformation methods tackle the multi-label learning problems by transforming it into other well-established learning scenarios [25]. Commonly used methods include first-order methods like BR, which ignore the correlations among labels; second-order methods, such as LP, RPC and CLR, which fix the relations as combination of pairwise or subset labels in the training data; and higher-order methods like RAkEL and ensemble classifier chain (ECC) [11], which consider all other labels’ influences imposed on each label.

Algorithm adaptation methods tackle the multi-label learning problems by adapting popular learning techniques to deal with multi-label data directly [25]. For example, a first-order method multi-label decision tree (MLDT) [26] adapts a decision tree technique. The first-order method MLkNN extends the k-nearest neighbor algorithm. The second-order method BPMLL adapts the back-propagation algorithm. These methods extend specific machine learning algorithms with the help of features and hypotheses to handle multi-label data. Modified hypotheses allow these methods to model the flexible label dependencies to some extent. However, algorithm adaptation methods are limited to the specific learning methods, and their computational costs are usually higher than problem transformation methods.

Although most multi-label classification methods try to capture certain kinds of label dependencies, they may fail to explicitly model label distributions. Therefore, the predicted labels may still be statistically different from ground-truth target labels. Recently, several works turned to probabilistic graphic models for label distribution modeling. Zhang et al. [27] used a BN model to encode the dependencies of labels as well as the feature set. Wang et al. [28] proposed a BN to capture the dependencies among labels directly, without the help of features. The drawback of these works is that they assume the label distributions follow certain forms, although this assumption may be violated by the ground truth distributions. An extensive review of multi-label classification can be found in [25], [29], [30], [31], [32], [33].

In this paper, we propose a novel multi-label classification method, which considers dependencies among emotions by modeling joint emotion label distributions, but not only capture certain kinds of label dependencies. We achieve the similarity between the joint distribution inherent in predicted labels and ground truth labels through an adversarial mechanism, but not directly learn the distribution with a certain form.

SECTION 3Problem Statement
In this section, we describe the problem statement and give the notations. Let Ω={xn,yn}Nn=1 denote the training set, where N is the number of instances, xn∈Rd denotes the d dimensional feature vector of one training instance and yn∈{0,1}q is the corresponding ground truth label. q is the number of labels. X={xn}Nn=1 stores all feature vectors and Y={yn}Nn=1 stores all ground truth labels. Given the training set Ω, our goal is to train a multiple emotion tag classifier C:Rd→{1,0}q by minimizing the following problem:
minΘV(C)=(1−α)∗E(x,y)∼ΩLsup(C(x;Θ),y)+α∗d(Pr,Pt),(1)
View Source

Where Θ is the parameters of emotion tag classifier C, and Lsup is the traditional supervised loss between the predicted labels and the ground truth labels. Considering the strong dependencies among emotion tags, label distribution is an additionally important constraint for learning emotion tag classifiers. Equation (1) introduces the second term d(Pr,Pt) representing the distance between the distribution of the predicted labels Pr and the distribution of the ground truth labels Pt to take into account the distribution of emotional labels. α is the trade-off between Lsup and d.

Note that minimizing the d(Pr,Pt) is an abstract expression. We will not fit Pr and Pt and compute one certain distance between them, since the distribution of predicted labels is difficult to model and there may be errors in the distribution's modeling procedure. Instead, we use an adversarial model to close the two distributions.

SECTION 4The Proposed Approach
Goodfellow et al. [34] introduced the generative adversarial network (GAN), which consists of a generator G and a discriminator D. Through the competition between the generator and discriminator, G can synthesize realistic-looking images that can “fool” D. It has been proven in [34] that the objective of GAN has a global optimum when the distribution of the synthetic samples and the distribution of the ground truth samples are the same.

Inspired by the GAN, we propose a novel multimedia emotion tagging approach as shown in Fig. 2. Specifically, we replace the generator G with the multiple emotional tag classifier C, and the discriminator D is retained. The predicted emotion labels classified by C from feature x are adversarial samples regarded as “fake”, while the ground truth labels (denoted by y′ in Fig. 2) directly sampled from Y are regarded as “real”. Through the competition between C and D, we can achieve the goal of making two distributions close in Section 3. Therefore, we use the following objective function to minimize the d(Pr,Pt) in Equation (1).
Ladv(C,D)=Ey′∼YlogD(y′)+Ex∼Xlog(1−D(C(x))).(2)
View Source


Fig. 2.
The framework of the proposed multi-emotion tagging model. C is the multiple emotion tag classifier, and D is the multiple emotion tag discriminator. C(x) is the multiple emotion prediction for feature vector x. y is the ground truth multiple emotion label of x. y′ is the real multiple emotion label from ground truth label set. The dotted line indicates that there is an one-to-one correspondence between C(x) and y. See text for details.

Show All

As the loss Lsup between C(x) and the corresponding ground truth label is the basis of our objective, we combine the adversarial loss Ladv and the supervised loss Lsup with a trade-off rate α and solve the minmax problem shown in Equation (3). More specifically, we rewrite Equation (3) as Equation (4).
minCmaxDV(C,D)=αLadv(C,D)+(1−α)Lsup(C)(3)
View Source
minCmaxDV(C,D)=α[Ey′∼YlogD(y′)+Ex∼Xlog(1−D(C(x)))]+(1−α)E(x,y)∼ΩLsup(C(x),y).(4)
View Source

From the above equation, we find that when α=0, the optimization problem learns the multiple emotion tag classifier C without the constraint of the label distribution. In order to find the individual objective for C and D, we rewrite the objectives as Equation (5).
minCmaxDV(C,D)=αEy′∼YlogD(y′)+E(x,y)∼Ω[αlog(1−D(C(x)))+(1−α)Lsup(C(x),y)].(5)
View SourceRight-click on figure for MathML and additional features.

Following the standard procedure for GAN training, C and D are updated alternately: (1) fix the classifier C and update the discriminator D (2) fix the discriminator D and update the classifier C.

The goal of discriminator D is to classify the ground truth labels as “real” and classify the predicted labels as “fake”. We can observe from Equation (4) that only the first term contains the discriminator D, so we take this term as the objective of D, which is shown in Equation (6).
minDVD(C,D)=−[Ey′∼YlogD(y′)+Ex∼Xlog(1−D(C(x)))].(6)
View Source

The goal of classifier C is to let discriminator make a mistake, (i.e., classifying the predicted labels as “real”). Furthermore, minimizing the supervised loss Lsup is another important objective. There must be a balance between the two objectives with the hyperparameter α. Equation (5) shows that only the second term contains the classifier C, so we take this term as the overall objective of C, which is shown in Equation (7). In practice, it's better for C to minimize −logD(C(x)) rather than minimizing log(1−D(C(x))), as this could avoid the problem of vanishing emotion tag classifier gradients [35].
minCVC(C,D)=−E(x,y)∼Ω[αlogD(C(x))−(1−α)Lsup(C(x),y)].(7)
View SourceRight-click on figure for MathML and additional features.

In our work, we use the cross-entropy loss for Lsup, which is shown in Equation (8).
Lsup(C(x),y)=−[y⊤logC(x)+(1−y)⊤log(1−C(x))].(8)
View SourceRight-click on figure for MathML and additional features.

The detailed training process is described in Algorithm 1.

Any binary classifier can be used in our emotion tagging model. A three-layer feed-forward net is used for the structure of the discriminator D. For the structure of the classifier C, multi-layer perceptions are used. The number of layers varies depending on different database. Specifically, in the experiments, we use four-layer feed-forward net on the Music Emotion database and three-layer feed-forward net on other three databases. We implement the proposed method using the TensorFlow framework [36]. Any gradient-based learning rule could be used to update parameters for the optimization method. We use the ADAM [37] algorithm to optimize both the classifier and the discriminator in our experiments. Other hyper parameters such as learning rate, sampling size p, and max number of training steps M, are determined by a validation set.

SECTION Algorithm 1.Adversarial Emotion Tagging Learning
Input: The training set Ω, maximum number of training steps M, number of C updated per step (LC), number of D updates per step (LD), sampling size p, hyper parameter α.

Output: The multiple emotion tag classifier C.

Initialize parameters of multiple emotional tag classifier ΘC and parameters of tag discriminator ΘD.

for m=1,2,…,M do

for lD=1,…,LD do

Sample mini-batch of p samples {x1,x2,…,xp} from feature instance set X.

Sample mini-batch of p labels {y′1,y′2,…,y′p} from the ground truth label set Y.

Update discriminator by descending its gradient:
∇ΘD(−1p∑i=1p[logD(y′i)+log(1−D(C(xi)))])
View Source

end for

for lC=1,…,LC do

Sample mini-batch of p samples {(x1,y1),(x2,y2) ,…,(xp,yp)} from training set Ω.

Update the classifier by descending its gradient:
∇ΘC(−1p∑i=1p[αlogD(C(xi))−(1−α)Lsup(C(xi),yi)])
View SourceRight-click on figure for MathML and additional features.

end for

end for

Algorithm 

SECTION 5Experiments
5.1 Experimental Conditions
5.1.1 Databases
Four emotional multimedia databases are used in our experiments: the Music Emotion database [38], the NVIE database [39], the FilmStim database [40], and the Memorability database [41].

The Music Emotion database contains 593 pieces of music. Each piece of music can be classified into one or more of the following six emotional categories: amazed-surprised (amazement), happy-pleased (happiness), relaxing-calm (relaxation), quiet-still (quietness), sad-lonely (sadness), and angry-fearful (anger). We cannot obtain the original music clips due to copyright issues, but 72 features of each sample are provided by the database constructers, including 8 rhythmic features and 64 timbre features. The detailed information of these features can be found in [38]. We use all of the features.

The NVIE database [39] contains 72 videos, each of which is labeled with at least one of seven emotion labels: happiness, anger, sadness, fear, disgust, surprise, or valance. We adopt the same features in [42], including three visual features, i.e., lighting key, energy color and visual excitement, and 31 widely used audio features [43], i.e., average energy, average energy intensity, spectrum flux, zero crossing rate (ZCR), standard deviation of ZCR, twelve Mel-frequency Cepstral Coefficients (MFCCs), log energy of MFCCs, and the standard deviations of the thirteen MFCCs.

The FilmStim database [40] contains 64 videos, each of which is labeled with one or more of six emotion labels: tenderness, fear, anger, joy, sadness, or disgust. We examine the same visual and audio features used for the NVIE database.

The Memorability database [41] consists of 2,222 natural images selected from the SUN database [44]. There are 923 attributes for each image. We select 12 emotion labels from these attributes: frightening, arousing, funny, engaging, peaceful, exciting, interesting, mysterious, strange, striking, makes you happy, and makes you sad. We use several features provided by the Memorability database, including GIST feature, HoG features, dense SIFT features, sparse SIFT histograms, SSIM features, tiny images, line features, texton histograms, color histograms, geometric probability maps and geometry-specific histograms. The number of samples for each emotion on four databases are shown in Fig. 3.


Fig. 3.
Data distribution within the used four databases. The figure at the top of each bar is the number of samples for each emotion in the database.

Show All

5.1.2 Settings
In order to analyze the contribution of the adversarial loss, we conduct an ablation study by setting the parameter α=0 to learn the model without adversarial loss. We conduct experiments with two methods: the method without the emotion labels distribution constraint (Ours-w), and the method with the emotion labels distribution constraint (Ours).

For the tuning of the hyper parameters, we conduct the experiments with ten-fold cross-validation on all four databases, keeping a consist experimental setting with the compared works [3], [4], [5], [19]. Specifically, the dataset is randomly partitioned into ten equal sized subsets. One subset is used as testing set, and the remaining nine subsets are used as training set. After the partition of training set and testing set, we extract a quarter of the training set as the validation set in each fold. All hyper parameters are determined by grid search and the parameters which achieve the best performance on validation set are used. For the tuning of α, we first select it from the candidate set {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}, we find the performance of the proposed method decreases gradually as α increases, and the performance with α = 0.1 is better than that with α= 0. We then select α from {0.0001, 0.001, 0.01, 0.1}.

5.1.3 Metrics
To sufficiently evaluate the performances of our method, we adopt ten widely used evaluation metrics, including four example-based evaluation metrics (accuracy, precision, recall, and F1-score), and six label-based evaluation metrics (micro-precision, micro-recall, micro-F1, macro-precision, macro-recall, and macro-F1).

Let T={(xi,yi)|1≤i≤m} denote the testing set, and zi denote the predicted labels corresponding to xi. These evaluation metrics are defined as follows [33]:
Accuracy=1m∑i=1m|yi∩zi||yi∪zi|
View SourceRight-click on figure for MathML and additional features.
Precision=1m∑i=1m|yi∩zi||zi|
View Source
Recall=1m∑i=1m|yi∩zi||yi|
View Source
F1=1m∑i=1m2|yi∩zi||yi|+|zi|
View Source
Micro−Precision=∑qj=1∑mi=1yjizji∑qj=1∑mi=1zji
View SourceRight-click on figure for MathML and additional features.
Micro−Recall=∑qj=1∑mi=1yjizji∑qj=1∑mi=1yji
View SourceRight-click on figure for MathML and additional features.
Micro−F1=2∑qj=1∑mi=1yjizji∑qj=1∑mi=1yji+∑qj=1∑mi=1zji
View Source
Macro−Precision=1q∑j=1q∑mi=1yjizji∑mi=1zji
View Source
Macro−Recall=1q∑j=1q∑mi=1yjizji∑mi=1yji
View SourceRight-click on figure for MathML and additional features.
Macro−F1=1q∑j=1q2∑mi=1yjizji∑mi=1yji+∑mi=1zji.
View SourceRight-click on figure for MathML and additional features.

Where yji and zji are the jth component of yi and zi, respectively. For ten metrics above, higher values represent better performance.

5.2 Evaluation of Adversarial Learning
In this section, we evaluated the role of adversarial learning. First, we validate that the proposed adversarial learning increases the closeness of the distribution inherent in ground truth labels and predicted labels. Due to the difficulty of visualizing data distribution in high-dimensional space, we simply evaluate the conditional dependencies between two labels as shown in Fig. 4. The conditional dependencies between two labels appear as coexistent and mutually exclusive relations among the labels. Fig. 4a shows the co-occurrence table computed from ground truth labels for the Music Emotion database. Each entry represents the conditional probability of one label under another, i.e., P(emotion1 = 1 | emotion2 = 1). This table shows some coexistent label combinations, including (1) relaxation, quietness, and sadness; (2) happiness and relaxation; (3) amazement and anger. There are also some mutually exclusive label pairs, including (1) amazement and quietness; (2) happiness and sadness; (3) anger and relaxation; (4) anger and quietness.


Fig. 4.
Evaluation of adversarial learning. (a) The dependencies between the emotion labels on the Music Emotion database. (Each entry represents the conditional probability of one label under another.) (b) and (c) The absolute difference between the conditional distribution in the ground truth labels and conditional distribution in the predicted labels. ((b) Ours-w, (c) Ours).

Show All

We compute the co-occurrence tables with the predicted labels, and compute the absolute differences between those tables and Fig. 4a. This is shown in Figs. 4b and 4c, which correspond to the methods Ours-w and Ours, respectively. From the Figs. 4b and 4c, we can find that the absolute differences of Ours are lower than those of Ours-w, indicating that conditional distribution among labels predicted by Ours is closer to the distribution in Fig. 4a than that predicted by Ours-w. More specifically, the average absolute difference of coexistent label pairs in Ours is 0.1062, for Ours-w, the absolute difference is 0.1406. Similarly, for Ours, the average absolute difference of mutually exclusive label pairs is 0.0080, for Ours-w, the absolute difference is 0.0250. For the whole absolute differences table, the average value of Fig. 4c (excluding diagonal) is 0.0565, and the average value of Fig. 4b is 0.0820. All of these demonstrate that using adversarial loss can make the two distributions closer.

Furthermore, to show the impact of adversarial learning, we analyze the trend of cross-entropy loss on the validation set as iteration increases, which is shown as Fig. 5. From Fig. 5, we can find that as iteration increases, the losses of Ours on validation set are lower than that of Ours-w, further demonstrating the positive impact of adversarial learning. On the NVIE database, the loss of Ours-w gradually decreases at first but slightly increases in the end. It may indicate Ours-w starts to overfit, when the iteration number is very large. While the loss of Ours decreases all the way. It demonstrates that the adversarial loss can prevent overfitting.

Fig. 5. - 
The trend of cross-entropy loss as iteration increases on four databases. (abscissa axis: iteration, vertical axis: loss).
Fig. 5.
The trend of cross-entropy loss as iteration increases on four databases. (abscissa axis: iteration, vertical axis: loss).

Show All

5.3 Experimental Results of Emotion Tagging
To evaluate the effectiveness of emotion label distribution on emotion tagging, we compare two methods, Ours and Ours-w, in terms of ten evaluation metrics. The results are illustrated in Table 1. We find the performances of Ours are higher than those of Ours-w in terms of all evaluation metrics on the four databases. For example, on the NVIE database, the experimental results of Ours are 13.16 and 12.82 percent higher than those of Ours-w in accuracy and F1-score respectively. For label-based metrics, the experimental results of Ours are 9.71 and 10.12 percent higher than those of Ours-w in micro-F1 and macro-F1 respectively. On other databases, Ours also achieve great improvements in all evaluation metrics compared to Ours-w. Ours-w ignores crucial dependencies among emotion labels, while we incorporate emotion distributions into emotion tagging through an adversarial learning process which has been demonstrated to be effective in Section 5.2.

TABLE 1 Experimental Results on the Four Databases
Table 1- 
Experimental Results on the Four Databases
For showing better performances of Ours more intuitively, we list several examples of emotion tagging results from four databases in Table 2. For example, when a piece of music from the Music Emotion database evokes complicated emotions, Ours-w can only recognize one or part of these emotions, while the results of Ours are more complete. Ours successfully recognizes music with both happiness and relaxation, which are a coexistent emotion pair according to the Section 5.2, Ours-w only recognizes relaxation and omits happiness. For another piece of music exhibiting quietness and sadness, Ours-w only recognizes sadness. Ours recognizes quietness and sadness but wrongly assigned another emotion: relaxation. This could be because these three emotions are coexistent emotional combinations. On the FilmStim database, for one video with four emotions (fear, anger, sadness, and disgust), Ours successfully recognizes all emotions. It's surprising that Ours-w should have wrongly recognized the joy with fear and sadness emotions. Joy and sadness are mutually exclusive emotion pairs, and will rarely appear together in tagging results of Ours. All of these examples demonstrate the superiority of the proposed method, which leverages emotion distributions (containing co-existent and mutually exclusive relations) through adversarial learning to improve the multimedia emotion tagging performances.

TABLE 2 Some Examples of Emotion Tagging Results of Two Methods on the Four Databases

5.4 Comparisons with Related Work
To thoroughly show the effectiveness of the proposed method, we compare our work with other multi-label classification methods and emotion tagging methods. For multi-label classification methods, we compare our method with problem transformation methods, including the first-order method BR, the second-order method CLR and the high-order method ECC, and algorithm adapting methods, including the first-order method BPMLL and the second-order method MLkNN. Recently, Zhu et al. [45] proposed a state-of-the-art method exploiting global and local label correlations simultaneously (GLOCAL). They captured label dependencies through the label manifold regularizer, including the global manifold regularizer and the local manifold regularizer. Specifically, the more positively correlated two labels are, the closer the corresponding classifier outputs should be, and vice versa [45]. So, we also compare the proposed method with GLOCAL. Since Trohidis et al. [19] compared seven methods on the Music Emotion database, we directly copy the results of BR, CLR, BPMLL, and MLkNN. The results of ECC on the Music Emotion database and the results of BR, CLR, ECC, BPMLL, and MLkNN on the other three databases are obtained using the implementation in the Mulan package [46].1 The results of GLOCAL are obtained using the provided code by Zhu et al.2

For emotion tagging methods, we compare our methods with BN, TRBM, MT-TRBM, and FRBM. The results of BN are from [3] and [4]. The results of TRBM, MT-TRBM and FRBM are from [4] and [5], except the results of MT-TRBM and FRBM on the Memorability database, which are obtained using the provided codes, since they did not conduct experiments on this database. The comparison results are listed in Table 1.

From Table 1, we can obtain following observations: First, Ours achieves better performances on all databases compared to the state-of-the-art method GLOCAL. GLOCAL only captures pairwise label dependencies (although it considers all label pairs), while we model the joint distribution of all emotion labels, which contain all kinds of label dependencies, to improve the performance of emotion tagging.

Second, we compare several multi-label classification methods. For problem transformation methods, CLR outperforms BR on three databases, and ECC outperforms CLR on all databases. BR ignores label correlations. CLR can only address the pairwise relations between labels. ECC capture high-order dependencies. For algorithm adapting methods, BPMLL outperforms MLkNN on all database, BPMLL is a second-order method while MLkNN is a first-order method. From the comparisons among these multi-label classification methods, it's easy to find that high-order methods achieve better performances than low-order methods, since high-order methods can capture more kinds of label dependencies for learning multiple emotion tags classifier.

Third, when comparing emotion tagging methods exploring label distribution, (e.g., BN, TRBM, MT-TRBM, and FRBM) to multi-label classification methods capturing certain kinds of label dependencies, (e.g., BR, CLR, ECC, BPMLL, and MLkNN), the methods exploring label distribution achieve better performances on most evaluation metrics. The exception is BN, which only captures pairwise label dependencies. This demonstrates that in emotion tagging, joint emotion distribution is more effective for learning than capturing certain kinds of label dependencies.

Fourth, when comparing Ours with emotion tagging methods including BN, TRBM, MT-TRBM, and FRBM, Ours achieves better performances on most evaluation metrics on the four databases. For example, Ours achieves higher performance than BN on all evaluation metrics on all databases and achieves higher performance than FRBM on all evaluation metrics on the Memorability database, demonstrating the superiority of the proposed method in modeling emotion distributions for emotion tagging. BN cannot effectively use label dependencies since it only captures pairwise dependencies among labels. Although TRBM, MT-TRBM and FRBM can model the joint emotion distribution based on the RBM model, they must assume the form of distribution and need a complex inference algorithm, such as Gibbs sampling or a belief propagation algorithm. While Ours learns the model without the need for this assumption or inferential process. Additionally, TRBM and MT-TRBM have to obtain the measurement trained by traditional supervised learning and then refine the prediction of measurement with the emotion distributions, leading to the potential for hypercorrection. We train an “end-to-end” framework without a measurement and we seek an optimal balance between the supervised loss and the adversarial loss as discussed in Section 5.5. In some cases, the performance of the proposed method is not the best compared to these four methods. For example, TRBM achieves the best recall, micro-recall, and macro-recall on the Music, NVIE, and FilmStim database, FRBM achieves the best accuracy on the NVIE database. However, compared to accuracy, F1-based metrics are more appropriate for multi-label classification problem, since the samples are usually unbalanced for a certain label. Furthermore, F1-based metrics consider both recall and precision. For the three used F1 scores (example-based F1, micro-F1, and macro-F1) on four databases, the proposed method achieves the highest value eleven times.

We evaluate the proposed method with ten evaluation metrics. Compared to all other methods, the propose method performs best in most metrics. While there are some combinations of metric and database wherein the previous methods perform better than the proposed method, like samples mentioned in the last paragraph. According to the performances in twelve F1 scores, the proposed method performs best since F1-based metrics are more appropriate for our problem and the proposed method achieves the highest value ten times. To further demonstrate the superiority of the proposed method, we sort the performance of each method from high to bottom in each metric on each database, and the mean ranking of each method is shown in the last column of Table 1. We can see that the proposed method has the highest mean ranking.

5.5 Evaluating the Effect of α Parameter
α, i.e., the weight of the adversarial loss, is an important parameter in our model as it balances the supervised loss and the adversarial loss. Previously, we set α=0 to analyze the contribution of the adversarial loss. To evaluate the impact of the parameter α, we have run the same set of experiments in this section by varying the α from 0.0001 to 0.9. The experimental results of four representative evaluation metrics on the four databases are shown in Fig. 6. The proposed method achieves optimal performance when α is between 0 and 1, which combines information from the corresponding ground truth labels and the emotion distributions. We have also observed that the α should only be slightly higher than 0. The optimal α parameter for all evaluation metrics on the four databases is 0.01, 0.01, 0.1, and 0.01 (marked by a vertical line) respectively. When using larger α, like α>0.5, the performance will decrease rapidly. This indicates that the traditional supervised loss is still more important than the adversarial loss. We can use the adversarial loss to improve the performance of emotion tagging but cannot reduce the rule of the supervised loss.


Fig. 6.
Emotion tagging results of four metrics varying the α parameter in the range from 0.0001 to 0.9.

Show All

Fig. 6 indicates that adversarial loss assigned with a small weight can improve the emotion tagging performances on four databases. This may be because in the whole objective, the supervised loss is the main objective, which utilizes the supervisory information from ground truth labels, while the adversarial loss is a regularization term, which utilizes the weak supervisory information from label distribution. Table 3 lists the supervised loss, adversarial loss, and their ratio in the experiments. For example, on the NVIE database, the supervised loss is about 0.22, the adversarial loss is about 0.18. After multiplied the weight parameter, the adversarial loss is about 1/12 of supervised loss. We believe this ratio is within a reasonable range.

TABLE 3 The Supervised Loss, Adversarial Loss and Their Ratio in the Experiments

SECTION 6Conclusion
Most current multimedia emotion tagging methods that consider label dependencies either only explore certain kinds of label dependencies or assume the form of the joint emotion distribution. In order to explore joint distribution of multiple emotion labels to facilitate the learning process of emotion tagging, we propose an adversarial framework to fully capture label distribution without any assumptions of distribution form. We take into account the traditional supervised loss and the adversarial loss simultaneously and achieve a good balance between the supervised loss and the adversarial loss with a trade-off rate. Better experimental results on the four databases demonstrate the superiority of the proposed emotion tagging method as compared to the state-of-the-art methods.