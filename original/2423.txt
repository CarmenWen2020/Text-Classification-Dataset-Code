Correlation filters (CF) based tracking methods have attracted considerable attentions for their competitive performance. However, the inherent issues of boundary effect and filter degradation, as well as the scale variation, degrade the tracking accuracy. In addition, the frame-by-frame updating strategy limits the tracking speed, especially in those deep features-based CF trackers. To address these issues, we propose a novel tracker, namely Accelerated Duality-aware Correlation Filters (ADCF), in this paper. In the proposed tracker, dual correlation filters, i.e., translation filter and scale filter, are designed for target localization and scale estimation, respectively. A spatio-temporal regularization term is employed to suppress the boundary effect and filter degradation. Moreover, a model updating strategy named Sparse learning-based Average Peak-to-Correlation Energy (S-APCE) is proposed to accelerate the tracking speed. Finally, an Alternating Direction Method of Multipliers (ADMM) formulation is developed to optimize the ADCF efficiently. Extensive experimental results over six tracking benchmarks prove that the proposed tracker outperforms the state-of-the-art (SOTA) trackers in tracking accuracy and speed.

Introduction
Visual tracking is a fundamental problem in computer vision, and it has been applied in many applications, e.g., video retrieval [2], robotic perception [22] and human-machine interaction [13]. Despite significant progress in recent years, visual tracking remains challenging due to numerous complicating factors in real scenarios [29, 36], e.g., illuminations, background clutter, and scale variation.

Generally, current visual tracking models are divided into two categories, namely generative models and discriminative models [36]. In generative models, the tracking task is implemented via searching the best-matched window, while discriminative models discriminate the target patch from the background by learning a discriminative classifier. Among the discriminative models, correlation filters (CF)-based trackers [3, 7, 17, 21] have drawn extensive attention. The advantage of the CF lies in a circulant matrix structure exploited, which can be calculated effectively by point-to-point operations and Fast Fourier Transform (FFT).

Bolme et al. [3] pioneered CF in visual tracking by learning a Minimum Output Sum of Squared Error (MOSSE) between multiple training image patches and the ideal correlation response template specified by the user. Galoogahi et al. [14] proposed an improved version of MOSSE named Multi-Channel Correlation Filters (MCCF), which utilizes features in multiple channels to boost the tracking performance. The part-based CF [34, 37] and scale-adaptive CF [6, 12] were proposed to handle the occlusion and size change. In addition, the features in CF were studied, and more discriminative features are proposed, e.g., Color Names (CN) [7, 23], HOG [9, 15] and deep features [39, 44]. Moreover, model update strategies [25, 26] were proposed to improve the tracking accuracy and robustness in the multimedia environment.

Although CF has achieved great success in visual tracking, it remains a challenge to gain satisfying performance in unconstrained scenarios due to the inherent issues, i.e., boundary effect [9, 51] and filter degradation [5, 23, 51]. Meanwhile, the scale variation of the target has severe impacts on the tracking accuracy, and this problem is far from solved [6, 40, 47]. Moreover, although deep features have been adopted in CF to promote the tracking accuracy, as explored in [8, 10, 20, 43], the tracking speed of these trackers is reduced significantly.

To address the issues above, we propose an efficient yet effective tracker, namely Accelerated Duality-aware Correlation Filters (ADCF). The main contributions are summarized as follows.

1.
A duality-aware CF model which consists of a translation filter and a scale filter is built to improve the tracking performance. The former translation filter ensembles deep and handcrafted features to localize the target accurately, and the latter scale filter exploits handcrafted features to estimate the scale efficiently. Meanwhile, a spatio-temporal regularization term that employs prior information is introduced to suppress the boundary effect and filter degradation.

2.
An effective model updating strategy named Sparse learning-based Average Peak-to-Correlation Energy (S-APCE) is proposed to accelerate the tracking speed.

3.
An ADMM formulation is developed to optimize the ADCF model efficiently, in which each subproblem is ensured a closed-form solution.

4.
Extensive evaluations on six challenging tracking benchmarks are conducted, and experimental results demonstrate the competitive performance of the proposed tracker compared with more than 20 SOTA trackers.

The rest of this work is organized as follows. In Sect. 2, an overview of the relevant prior work is presented. In Sect.3, the ADCF model is proposed, and an ADMM formulation is developed to optimize this model. Meanwhile, the scale estimation and model updating strategy are presented. In Sect. 4, extensive evaluations of the proposed tracker with SOTA trackers are presented. Finally, this work is concluded in Sect. 5.

Related work
We introduce the related work in a problem-orientated manner, i.e., boundary effect, filter degradation, and scale variation.

Boundary effect
CF utilizes FFT in the training and tracking process with the underlying periodic assumption. Although this assumption guarantees the strategy of dense sampling to construct the circulant matrix from a training sample patch, leading to unwanted boundary effect [9]. To suppress the unwanted boundary effect, Danelljan et al. [9] put forward the Spatially Regularized Discriminative Correlation Filters (SRDCF) method, in which a spatial regularization component was added for penalizing the model coefficient with a predefined weight constraint. Unlike SRDCF method in which negative examples are limited to circular shifted patches, Background-Aware Correlation Filters (BACF) [15] multiplied the filter with a binary matrix directly to generate real negative and positive training examples for tracker training. Furthermore, Context-Aware Correlation Filters (CACF) [32] model incorporated the global context information and takes advantage of more negative samples to alleviate the unwanted boundary effect.

Filter degradation
The filter updated by the linear interpolation cannot adjust to ubiquitous appearance changes, leading to filter degradation [23]. The filter degradation can be tackled from many aspects. In terms of training set management, Danelljan et al. [11] put forward Efficient Convolution Operators (ECO) for tracking model. ECO model formulated a compact generative model of the training sample distribution. In terms of temporal restriction, Li et al. [20] proposed Spatial-Temporal Regularized Correlation Filters (STRCF). In terms of overfitting alleviation, Sun et al. [33] adopted the concept of Region Of Interest (ROI)-based pooling and proposed an ROI Pooled Correlation Filters (RPCF) tracker with equality constraints. In terms of tracking confidence verification, Wang et al. proposed Large Margin object tracking with Circulant Feature maps (LMCF) [35] tracker using a multimodal target detection method. Among these strategies, temporal regularization has been proved to be an effective and efficient way [23].

Scale variation
The scale variation of the target has a serious impact on tracking accuracy [6]. To tackle this problem, some recent trackers adopted either multi-scale spatial pyramid [6, 28] or part-based multiple filters [27] to estimate the optimal scale. Moreover, several SOTA trackers [15, 20, 43, 44] attempted to introduce more complex scale models to further improve the tracking accuracy. However, the computational efficiency drops sharply due to multiple filtering operations performed in a single frame. Especially for the trackers with deep features [8, 20, 43], they are extremely time-consuming because the scale estimation requires multi-scale convolutional features.

Proposed method
In this section, we firstly review the formulation of STRCF [20], which is the foundation of the proposed model. Then, we propose the ADCF model and develop ADMM [4] formulation to optimize it efficiently. Finally, the target localization and scale estimation method, and the model update strategy are presented.

Revisit STRCF
The goal of spatial-temporal regularized correlation filter (STRCF) [20] is to learn a correlation filter ğŸâˆˆâ„ğ‘€Ã—ğ‘Ã—ğ¾ with ğ‘€Ã—ğ‘ size and K channels at the tth frame, from the sample ğ±âˆˆâ„ğ‘€Ã—ğ‘Ã—ğ¾. The desired output ğ²âˆˆâ„ğ‘€Ã—ğ‘ is the Gaussian-shaped response, which includes a label for each location in the sample. The correlation filter can be trained by minimizing the following objective function,

argminğ‘“12â€–â€–â€–â€–ğ‘¦âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¥ğ‘˜ğ‘¡âˆ—ğ‘“ğ‘˜ğ‘¡ğ¹â€–â€–â€–â€–2+12âˆ‘ğ‘˜=1ğ¾â€–â€–ğ‘¤ğ‘¡âŠ™ğ‘“ğ‘˜ğ‘¡â€–â€–2ğ¹+ğœ‡2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ‘“ğ‘˜ğ‘¡âˆ’ğ‘“ğ‘˜ğ‘¡âˆ’1â€–â€–2ğ¹,
(1)
where âˆ— and âŠ™ denote the circular convolution and point-wise multiplication, respectively. k is the channel index. The spatial weight ğ° acts on the filter ğŸğ‘¡ to alleviate boundary effect. â€–â€–ğŸğ‘˜ğ‘¡âˆ’ğŸğ‘˜ğ‘¡âˆ’1â€–â€–2ğ¹ is the temporal regularization term to restrict abrupt changes of the filter by penalizing the difference between the current (tth frame) and previous (ğ‘¡âˆ’1th frame) filters. ğœ‡ is the temporal regularization parameter.

In STRCF, however, both the target localization and scale estimation are performed on the same feature space. The computational cost of such a tracking strategy is extremely expensive when using deep features [20]. Moreover, the STRCF model is updated in a frame-by-frame manner, resulting in a low frame rate, which is not suitable for real-time scenarios.

Accelerated duality-aware correlation filter model
Based on STRCF, we propose an ADCF tracking model, as shown in Fig. 1. The ADCF tracker consists of a translation filter and a scale filter. The translation filter exploits ensembles of deep and handcrafted features for accurate target localization, while the scale filter exploits handcrafted features for efficient scale estimation. Meanwhile, a developed spatio-temporal regularization term that employs prior information is introduced to suppress the boundary effect and filter degradation. Moreover, an efficient and effective model update strategy named S-APCE is proposed to accelerate the tracking speed. The overall process of the ADCF tracker is divided into two portions, namely training and detection.


Training. Features are extracted by the pre-trained VGG-Net and HOG at the tth frame. The translation filter and scale filter are trained in a duality-aware manner, sharing portion features, spatio-temporal regularization and desired output. Both filters are optimized by the developed ADMM formulation.


Detection. The translation filter and scale filter from the training frame (tth frame), and the feature maps from the detection frame (ğ‘¡+1th frame) are combined to calculate the cross-correlation. In this stage, the translation filter is designed for accurate target localization, while the scale filter is used for fast scale estimation based on the maximum value of the response map. Finally, the S-APCE strategy is proposed to update the learning rate (ğœ‚) of the appearance model.

The objective function of the ADCF can be formulated as follows,

argminğ‘“12â€–â€–â€–â€–ğ‘¦âˆ’âˆ‘ğ‘˜=1ğ¾ğ‘¥ğ‘˜ğ‘¡âˆ—ğ‘“ğ‘˜ğ‘¡â€–â€–â€–â€–2ğ¹+ğœ†12âˆ‘ğ‘˜=1ğ¾â€–â€–ğ‘¤ğ‘¡âŠ™ğ‘“ğ‘˜ğ‘¡â€–â€–2ğ¹+ğœ†22â€–â€–ğ‘¤ğ‘¡âˆ’ğ‘¤ğ‘¡âˆ’1â€–â€–2ğ¹+ğœ‡2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ‘“ğ‘˜ğ‘¡âˆ’ğ‘“ğ‘˜ğ‘¡âˆ’1â€–â€–2ğ¹
(2)
Where the first term is the least square term. The second term introduces a spatial regularization term to alleviate the boundary effect. The third item introduces prior information of the spatial weight ğ° to avoid its abrupt changes and degradation. The fourth term introduces a temporal regularization term to avoid filter degradation. ğœ†1 and ğœ†2 are the spatial regularization parameters, and ğœ‡ is the temporal regularization parameter.

Fig. 1
figure 1
Tracking framework of the proposed ADCF tracker

Full size image
To optimize the ADCF model, we introduce an auxiliary variable ğ Ë†=ğ‘‡â€¾â€¾âˆšğ…ğŸ. Here, the symbol .Ë† denotes the discrete Fourier transform (DFT) of a signal, ğ‘‡=ğ‘€Ã—ğ‘ is the feature length, and ğ…âˆˆâ„‚ğ‘‡Ã—ğ‘‡ is an orthonormal matrix to map any T dimensional vector to the Fourier domain. Then, Eq. (2) can be transformed into the Fourier domain,

argminğŸ12â€–â€–â€–â€–ğ²Ë†âˆ’âˆ‘ğ‘˜=1ğ¾ğ±Ë†ğ‘˜ğ‘¡âŠ™ğ Ë†ğ‘˜ğ‘¡â€–â€–â€–â€–2ğ¹+ğœ†12âˆ‘ğ‘˜=1ğ¾â€–â€–ğ°ğ‘¡âŠ™ğŸğ‘˜ğ‘¡â€–â€–2ğ¹+ğœ†22â€–â€–ğ°ğ‘¡âˆ’ğ°ğ‘¡âˆ’1â€–â€–2ğ¹+ğœ‡2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ Ë†ğ‘˜ğ‘¡âˆ’ğ Ë†ğ‘˜ğ‘¡âˆ’1â€–â€–2ğ¹,s.t.,ğ Ë†ğ‘˜ğ‘¡=ğ‘‡â€¾â€¾âˆšğ…ğŸğ‘˜ğ‘¡,ğ‘˜=1,2,â€¦,ğ¾.
(3)
By minimizing Eq. (3), the optimal solution is obtained by ADMM formulation [4]. The augmented Lagrangian form of Eq. (3) can be formulated as,

îˆ¸=12â€–â€–â€–â€–ğ²Ë†âˆ’âˆ‘ğ‘˜=1ğ¾ğ±Ë†ğ‘˜ğ‘¡âŠ™ğ Ë†ğ‘˜ğ‘¡â€–â€–â€–â€–2ğ¹+ğœ†12âˆ‘ğ‘˜=1ğ¾â€–â€–ğ°ğ‘¡âŠ™ğŸğ‘˜ğ‘¡â€–â€–2ğ¹+ğœ†22â€–â€–ğ°ğ‘¡âˆ’ğ°ğ‘¡âˆ’1â€–â€–2ğ¹+ğœ‡2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ Ë†ğ‘˜ğ‘¡âˆ’ğ Ë†ğ‘˜ğ‘¡âˆ’1â€–â€–2ğ¹+ğ›¾2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ Ë†ğ‘˜ğ‘¡âˆ’ğ‘‡â€¾â€¾âˆšğ…ğŸğ‘˜ğ‘¡â€–â€–2ğ¹+âˆ‘ğ‘˜=1ğ¾(ğ¯Ë†ğ‘˜ğ‘¡)T(ğ Ë†ğ‘˜ğ‘¡âˆ’ğ‘‡â€¾â€¾âˆšğ…ğŸğ‘˜ğ‘¡),
(4)
where ğ›¾ controls the step size for regularization, and ğ¯ denotes the Lagrange multiplier. The superscript .T on a complex vector or matrix indicates conjugate transpose operation. By assigning ğ¬=1ğ›¾ğ¯, the optimization of Eq. (4) can be reformulated as,

îˆ¸=12â€–â€–â€–â€–ğ²Ë†âˆ’âˆ‘ğ‘˜=1ğ¾ğ±Ë†ğ‘˜ğ‘¡âŠ™ğ Ë†ğ‘˜ğ‘¡â€–â€–â€–â€–2ğ¹+ğœ†12âˆ‘ğ‘˜=1ğ¾â€–â€–ğ°ğ‘¡âŠ™ğŸğ‘˜ğ‘¡â€–â€–2ğ¹+ğœ†22â€–â€–ğ°ğ‘¡âˆ’ğ°ğ‘¡âˆ’1â€–â€–2ğ¹+ğœ‡2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ Ë†ğ‘˜ğ‘¡âˆ’ğ Ë†ğ‘˜ğ‘¡âˆ’1â€–â€–2ğ¹+ğ›¾2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ Ë†ğ‘˜ğ‘¡âˆ’ğ‘‡â€¾â€¾âˆšğ…ğŸğ‘˜ğ‘¡+ğ¬Ë†ğ‘˜ğ‘¡â€–â€–2ğ¹.
(5)
Then, the ADMM formulation is adopted by solving the following subproblems alternately.

Subproblem ğ Ë†: If ğŸ, ğ° and ğ¬Ë† are given, the optimal ğ Ë†âˆ— can be estimated by solving the optimization problem as,

ğ Ë†âˆ—=argminğ Ë†â§â©â¨âªâª12â€–â€–â€–â€–ğ²âˆ’âˆ‘ğ‘˜=1ğ¾ğ±Ë†ğ‘˜ğ‘¡âŠ™ğ Ë†ğ‘˜ğ‘¡â€–â€–â€–â€–2ğ¹+ğœ‡2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ Ë†ğ‘˜ğ‘¡âˆ’ğ Ë†ğ‘˜ğ‘¡âˆ’1â€–â€–2ğ¹+ğ›¾2âˆ‘ğ‘˜=1ğ¾â€–â€–ğ Ë†ğ‘˜ğ‘¡âˆ’ğ‘‡â€¾â€¾âˆšğ…ğŸğ‘˜ğ‘¡+ğ¬Ë†ğ‘˜ğ‘¡â€–â€–2ğ¹}.
(6)
However, it is hard to optimize Eq. (6) because of the high computational complexity. So, We consider processing all K channels of each pixel j to simplify formulation as,

î‰‚âˆ—ğ‘—(ğ Ë†)=argminğ Ë†{12â€–â€–ğ²Ë†ğ‘—âˆ’î‰‚ğ‘—(ğ±Ë†ğ‘¡)Tî‰‚ğ‘—(ğ Ë†ğ‘¡)â€–â€–2ğ¹+ğœ‡2â€–â€–î‰‚ğ‘—(ğ Ë†ğ‘¡)âˆ’î‰‚ğ‘—(ğ Ë†ğ‘¡âˆ’1)â€–â€–2ğ¹+ğ›¾2â€–â€–î‰‚ğ‘—(ğ Ë†ğ‘¡)âˆ’î‰‚ğ‘—(ğ‘‡â€¾â€¾âˆšğ…ğŸğ‘¡)+î‰‚ğ‘—(ğ¬Ë†ğ‘¡)â€–â€–2ğ¹},
(7)
where î‰‚ğ‘—(ğ±Ë†ğ‘¡)âˆˆâ„‚ğ¾Ã—1 denotes the vector containing values of ğ±Ë†ğ‘¡ on pixel j. The solution of Eq. (7) is obtained as,

î‰‚âˆ—(ğ Ë†)=1ğœ‡+ğ›¾[ğˆâˆ’î‰‚ğ‘—(ğ±Ë†ğ‘¡)î‰‚ğ‘—(ğ±Ë†ğ‘¡)Tğœ‡+ğ›¾+î‰‚ğ‘—(ğ±Ë†ğ‘¡)Tî‰‚ğ‘—(ğ±Ë†ğ‘¡)]ğ©,
(8)
where ğˆ is an identity matrix,

ğ©=î‰‚ğ‘—(ğ±Ë†ğ‘¡)ğ²Ë†ğ‘—+ğœ‡[î‰‚ğ‘—(ğ Ë†ğ‘¡âˆ’1)]+ğ›¾[î‰‚ğ‘—(ğ‘‡â€¾â€¾âˆšğ…ğŸğ‘¡)âˆ’î‰‚ğ‘—(ğ¬Ë†ğ‘¡)].
(9)
The derivation of Eq. (8) adopts the Sherman Morrsion formulation [30],

(ğ€+ğ®ğ¯T)âˆ’1=ğ€âˆ’1âˆ’ğ€âˆ’1ğ®ğ¯Tğ€âˆ’11+ğ¯Tğ€âˆ’1ğ®,
(10)
where ğ® and ğ¯ are two column vectors, and ğ®ğ¯T is a rank-one matrix.

Subproblem ğŸ: If ğ Ë†, ğ° and ğ¬Ë† are given, the optimal ğŸâˆ— is determined as,

ğŸâˆ—=argminğŸ{ğœ†12â€–â€–ğ°ğ‘¡âŠ™ğŸğ‘˜ğ‘¡â€–â€–2ğ¹+ğ›¾2â€–â€–ğ Ë†ğ‘˜ğ‘¡âˆ’ğ‘‡â€¾â€¾âˆšğ…ğŸğ‘˜ğ‘¡+ğ¬Ë†ğ‘˜ğ‘¡â€–â€–2ğ¹}=[ğœ†1ğ–Tğ‘¡ğ–ğ‘¡+ğ›¾ğ‘‡ğˆ]âˆ’1ğ›¾ğ‘‡(ğ ğ‘˜ğ‘¡+ğ¬ğ‘˜ğ‘¡)=ğ›¾ğ‘‡(ğ ğ‘˜ğ‘¡+ğ¬ğ‘˜ğ‘¡)ğœ†1(ğ°ğ‘¡âŠ™ğ°ğ‘¡)+ğ›¾ğ‘‡,
(11)
where ğ–ğ‘¡=diag(ğ°ğ‘¡)âˆˆâ„ğ‘‡Ã—ğ‘‡ denotes diagonal matrix. ğ ğ‘˜ğ‘¡ and ğ¬ğ‘˜ğ‘¡ can be obtained by inverse discrete Fourier transform (IDFT) (i.e., ğ ğ‘˜ğ‘¡=1ğ‘‡âˆšğ…Tğ ğ‘˜ğ‘¡Ë† and ğ¬ğ‘˜ğ‘¡=1ğ‘‡âˆšğ…Tğ¬ğ‘˜ğ‘¡Ë†).

Subproblem ğ°: Given ğŸ, ğ Ë† and ğ¬Ë†, the optimal ğ°âˆ— can be obtained as,

ğ°âˆ—=argminğ°{ğœ†12âˆ‘ğ‘˜=1ğ¾â€–â€–ğ°âŠ™ğŸğ‘˜ğ‘¡â€–â€–2ğ¹+ğœ†22â€–ğ°ğ‘¡âˆ’ğ°ğ‘¡âˆ’1â€–2ğ¹}=[ğœ†1âˆ‘ğ‘˜=1ğ¾(ğğ‘˜ğ‘¡)Tğğ‘˜ğ‘¡+ğœ†2ğˆ]âˆ’1ğœ†2ğ°ğ‘¡âˆ’1=ğœ†2ğ°ğ‘¡âˆ’1ğœ†1âˆ‘ğ¾ğ‘˜=1ğŸğ‘˜ğ‘¡âŠ™ğŸğ‘˜ğ‘¡+ğœ†2ğˆ,
(12)
where ğğ‘˜ğ‘¡=diag(ğŸğ‘˜ğ‘¡)âˆˆâ„ğ‘‡Ã—ğ‘‡.

Lagrangian multiplier update: The Lagrange multiplier can be updated as,

ğ¬Ë†ğ‘–+1=ğ¬Ë†ğ‘–+ğ Ë†ğ‘–+1âˆ’ğŸË†ğ‘–+1,
(13)
where i is the iteration index.

By solving the subproblems above iteratively, the objective function can be optimized effectively. Then the optimal filter ğ Ë†ğ‘¡ is utilized for detecting at the ğ‘¡+1th frame.

Target localization and scale estimation
The response map of the target in the Fourier domain is defined as,

ğ‘Ë†ğ‘¡=âˆ‘ğ‘˜=1ğ¾ğ±Ë†ğ‘˜âŠ™ğ Ë†ğ‘˜ğ‘¡âˆ’1,
(14)
where ğ±ğ‘˜ denotes candidate area, and ğ ğ‘˜ğ‘¡âˆ’1 denotes trained filter from last frame.

ADCF is designed in a duality-aware manner in which the translation filter is designed for target localization, and the scale filter is for scale estimation. The translation filter is trained on ensembles of deep CNN features and handcrafted features (HOG feature in this work). Although the feature extraction is time-consuming, it merely executes on a single-scale search region during the tracking process. After obtaining the translation response map using Eq. (14), then, the target is localized by the maximum value of the response map.

Unlike the translation filter, which ensembles deep and handcrafted features, the scale filter exploits handcrafted features (HOG feature in this work) to estimate the scale efficiently. We apply the scale filter on five scale search regions and obtain their response maps using Eq. (14). Then, the best scale is the scale corresponding to the maximum value of the scale response maps. This proposed strategy can reduce the computational complexity efficiently under the premise of high tracking accuracy. This inference is verified in the ablation study in Sect. 4.5.

Model update
Most existing trackers update their models without considering the accuracy of detection [8, 9, 15, 20]. When the target is detected inaccurately, it will lead to a deterministic failure, e.g., in the case of severely occluded.

Fig. 2
figure 2
The ideal response map (top), and non-ideal response map (bottom)

Full size image
Generally, the peak value and the fluctuation of the response map can reflect the confidence of the tracking results to a certain extent [35]. To this aim, we depict the ideal response map and the non-ideal response map of a target in Fig. 2. It depicts that when the detected target is completely matched with the correct target, the ideal response map should be unimodal, and other areas are smooth. Wang et al. [35] first proposed a high-confidence update strategy by adopting the maximum response value and an Average Peak-to-Correlation Energy (APCE) measure in LMCF tracker. The maximum response value is defined as,

ğ‘ğ‘šğ‘ğ‘¥=maxîˆ²âˆ’1(ğ‘Ë†),
(15)
where îˆ²âˆ’1 denotes the inverse Fourier transform. The APCE measure is defined as,

ğ´ğ‘ƒğ¶ğ¸=|ğ‘maxâˆ’ğ‘min|2mean[âˆ‘ğ‘¤,â„(ğ‘ğ‘¤,â„âˆ’ğ‘min)2],
(16)
where ğ‘max, ğ‘min and ğ‘ğ‘¤,â„ denote the maximum, minimum and the wth row hth column elements of response ğ‘, respectively.

APCE reflects the smoothness of the response maps and the confidence level of the detected targets. The APCE value will drop significantly when the target encounters aberrance (i.e., aberrant training samples) such as occlusion and background clutter [35]. According to the description of the high-confidence update strategy proposed by the LMCF, if the judgment condition "the maximum response value and APCE value both reach the threshold" is satisfied, the model will be updated. Assuming that multiple consecutive frames satisfy the judgment conditions, a continual update strategy will be activated, which results in a lower frame rate and robustness degradation due to overfitting the recent frames. On the contrary, if the model is not updated for a long time, which will lead to model degradation.

To this end, we propose a sparse update strategy based on APCE measure. This model update strategy not only inherits the advantages of APCE measure, but also considers the effectiveness of training samples and the efficiency of model update. Specifically, APCE value and maximum response value are used to control the learning rate of the appearance model to ensure the effectiveness of the current sample. The model is updated at the interval of ğ‘ğ‘  frames after initialization. Where, the appearance model ğ— of the ADCF is updated as,

ğ—ğ‘¡=(1âˆ’ğœ‚)ğ—ğ‘¡âˆ’1+ğœ‚ğ—ğ‘¡,ğœ‚={ğœ‚âˆ—0if(ğ´ğ‘ƒğ¶ğ¸>ğœğ´ğ‘ƒğ¶ğ¸hm)âˆ©(ğ‘max>ğœğ‘hm)otherwise
(17)
where ğœ‚ and ğœ‚âˆ— are the learning rate and the learning rate when the training samples are high quality, respectively. ğ´ğ‘ƒğ¶ğ¸â„ğ‘š and ğ‘hm denote the historical mean value of APCE and ğ‘, respectively. ğœ is a threshold parameter. It is worth noting that although the model is updated in a sparse manner, the appearance model is constantly updated to adjust the appearance changes of the target. This model update strategy is verified in Section 4.5. The overall tracking algorithm of the ADCF tracker is summarized in Algorithm 1.

figure a
Experimental results and discussion
In this section, we evaluate the ADCF tracker on six tracking benchmarks, i.e., OTB2013 [41], OTB2015 [42], TC128 [24], UAV123 [31], UAV123@10 fps [31] and VOT2016 [19]. First, the implementation details and evaluation metrics are described. Then, quantitative and qualitative evaluations of the proposed tracker with the SOTA trackers are presented. Meanwhile, a more sophisticated analysis of the ADCF tracker is proved through ablation studies.

Experimental setup
The proposed tracker is implemented using the mixed programming of MATLAB2017a with the MatConvNet toolboxFootnote1 on a PC with CPU (Intel i7 9700k) and GPU (NVIDIA GTX 1080Ti).The Parameters of ADCF are set as follows.

(1)
For the translation CF, the spatial regularization parameters are set as ğœ†1=1.2 and ğœ†2=0.001. The temporal regularization parameter is set as ğœ‡=0.01. One-scale CNN (Conv4-3 from VGG-16) and HOG feature map are exploited for target localization.

(2)
For the scale CF, the spatial regularization parameters are set as ğœ†1=1.2 and ğœ†2=0.001. The temporal regularization parameter is set as ğœ‡=0.01. Five-scale HOG feature maps are adopted for scale estimation.

(3)
We set model update interval ğ‘ğ‘ =5, the iteration of ADMM optimization ğ‘=3, the threshold parameter ğœ = 0.7, and the learning rate ğœ‚âˆ— = 0.02. The step size parameter ğ›¾ is initialized to 1 and updated by ğ›¾ğ‘–+1=min(ğ›¾max,ğ›½ğ›¾ğ‘–)(ğ›½=10,ğ›¾max=10,000) [15].

To make a fair comparison, the parameters of the ADCF model are fixed, and the compared trackers employ the public codes or results provided by the original publications.

Evaluation metrics
For OTB2013 [41], OTB2015 [42], TC128 [24], UAV123@10fps [31] and UAV123 [31] tracking benchmarks, success rate and precision are utilized under the rule of One Pass Evaluation (OPE) [41, 42]. The success rate denotes the percentage of frames in which the Intersection Over Union (IOU) exceeds a threshold (Note that the IOU is sometimes called overlap). Given the tracked bounding box ğ‘Ÿğ‘¡ and the ground truth bounding box ğ‘Ÿğ‘”, the IOU is defined as,

ğ¼ğ‘‚ğ‘ˆ=âˆ£âˆ£ğ‘Ÿğ‘¡â‹‚ğ‘Ÿğ‘”âˆ£âˆ£âˆ£âˆ£ğ‘Ÿğ‘¡â‹ƒğ‘Ÿğ‘”âˆ£âˆ£,
(18)
where â‹‚ and â‹ƒ represent the intersection and union of two regions, respectively. |â‹…| denotes the number of pixels in the region. The precision denotes the percentage of frames where the Center Location Error (CLE) is under a threshold. Area Under Curve (AUC) in success rate and Distance Precision (DP, denoted by the percentage of frames whose CLE â‰¤ 20 pixels) in precision are adopted as the evaluation metrics to rank the success rate and precision of different trackers. In this work, accuracy evaluation on these benchmarks is based on the One Pass Evaluation (OPE) rule [42]. Moreover, the tracking speed is measured by Frames Per Second (FPS).

For VOT2016 [19] tracking benchmark, three primary evaluation metrics, namely Accuracy (A), Robustness (R) and Expected Average Overlap (EAO), are adopted to evaluate the tracking performance. A is calculated as the average overlap during successful tracking periods, and R is the total number of failures. The EAO metric combines the raw values of per-frame accuracy and the failures in a principled manner and has a clear practical interpretation. Meanwhile, it measures the expected no-reset overlap of a tracker running on a short-term sequence, and addresses the problem of increased variance and bias of the average overlap due to variable sequence lengths on practical datasets.

Quantitative evaluations
Evaluation on OTB2013 and OTB2015

OTB2013 [41] tracking benchmark contains 50 fully annotated sequences with substantial variations. OTB2015 tracking benchmark [42] is the extension of OTB2013, which contains 100 sequences. These two OTB tracking benchmarks are annotated with 11 attributes, i.e., out-plane rotation (OPR), illumination variation (IV), out-of-view (OV), scale variation (SV), background clutter (BC), in-plane rotation (IPR), deformation (DEF), motion blur (MB), occlusion (OCC), low resolution (LR) and fast motion (FM).

We compare the ADCF tracker with the recent SOTA trackers, including ECO [11], DeepSTRCF [20], MCCT [37], STRCF [20], MCPF [48], LADCF-HC [44], CFWCR [16], ADNet [45], MCCT-H [37], BACF [15], ECO-HC [11], UDT [38], ARCF [18], ARCF-H [18], UDT+ [38], AutoTrack [23], STAPLE_CA [32], fDSST [12], RSST [49] and RaF [46]. The success and precision plots of the evaluated trackers on these two OTB tracking benchmarks are shown in Fig. 3. The comparative results indicate that the ADCF performs competitively against all the SOTA trackers. It achieves the best score in AUC and DP, respectively. Accuracy and speed comparisons of the top-5 trackers on OTB2013 and OTB2015 are shown in Table 1 and Table 2, respectively. The comparative results show that ADCF achieves the fastest speed on OTB2013 and OTB2015 among the GPU-based trackers, which benefits from the proposed model update strategy.

Fig. 3
figure 3
Success and precision plots of the evaluated trackers on OTB2013 and OTB2015

Full size image
Table 1 Comparative results of top-5 trackers on OTB2013 benchmark in accuracy and speed.
Full size table
Table 2 Comparative results of top-5 trackers on OTB2015 benchmark in accuracy and speed.
Full size table
To analyze the performance of the trackers in handling different challenges, the attribute-based evaluations are performed. Some representative results are depicted in Fig. 4. For the sequences with the appearance variation of the target itself, i.e., DEF, OPR and SV attributes, ADCF achieves 0.656, 0.673 and 0.661 AUC scores. For the sequences with environmental challenge scenarios, i.e., BC, IV and OCC attributes, the target encounters partial or complete disappearance, which adversely affects the tracking accuracy. ADCF achieves 0.684, 0.701 and 0.670 AUC scores in these attributes, which surpass the second-best tracker by 3.5%, 2.5% and 0.4%, respectively. The improvement can be attributed to two factors. On the one hand, the temporal regularization in ADCF achieves a balance between the current filter ğŸğ‘¡ and the latest filter ğŸğ‘¡âˆ’1 to prevent filter degradation when the appearance of the target changes drastically. On the other hand, the spatial regularization establishes a balance between the current weight ğ°ğ‘¡ and the latest weight ğ°ğ‘¡âˆ’1. Thus, it enables the ğ°ğ‘¡ close to the ğ°ğ‘¡âˆ’1 to avoid abrupt changes and degradation. Particularly, ADCF achieves the best AUC score (0.661) in SV attribute. This can be attributed to the design of the scale filter, which avoid the loss of some detailed information in the feature description due to the pooling operation.

Fig. 4
figure 4
Success plots of the evaluated trackers under 6 challenging attributes on OTB2015. The title of each sub-figure indicates the number of sequences marked with their attributes

Full size image
Evaluation on TC128
TC128 benchmark [24] consists of 128 challenging color sequences. We provide a comprehensive comparison of the proposed ADCF with SOTA trackers, including ECO [11], ASRCF [5], MCCT [37], LADCF-HC [44], MCCT-H [37], STRCF [20], ECO-HC [11], CFWCR [16], MCPF [48], UDT+ [38], ARCF [18], UDT [38], AutoTrack [23], STAPLE_CA [32], ARCF-H [18], SAMF_CA [32], BACF [15], RSST [49], fDSST [12], DCF_CA [32], RaF [46] and MOSSE_CA [32]. Fig. 5 shows the comparative results. It shows that ADCF achieves the best score both in AUC and DP. It is worth mentioning that the ASRCF tracker also designs a scale filter. Different from ASRCF which only uses spatial regularization for translation filter, the proposed ADCF introduces spatio-temporal regularization into both translation filter and scale filter. Meanwhile, the S-APCE update strategy in ADCF results in greater accuracy and faster speed.

Fig. 5
figure 5
Success and precision plots of the evaluated trackers on TC128

Full size image
We calculate the accuracy and speed of the top-5 trackers on TC128 benchmark, and the results are depicted in Table 3. Among these top-ranked trackers, the ADCF achieves the best accuracy and the fastest speed simultaneously.

Table 3 Comparative results of top-5 trackers on TC128 benchmark in accuracy and speed
Full size table
Evaluation on UAV123 and UAV123@10fps
UAV123 tracking benchmark [31] contains 123 aerial sequences with more than 110K frames. This benchmark is more complex and challenging than other benchmarks as it includes significant changes in aspect ratio, abrupt camera motion, and severe occlusion. Meanwhile, UAV123@10fps [31] benchmark is a temporarily down-sampled version of the UAV123. Compared with the original UAV123 benchmark, UAV123@10fps is more challenging because the displacement of moving objects becomes bigger.

To further verify the performance of the ADCF tracker, comparative experiments are performed on UAV123 and UAV123@10fps benchmarks with SOTA trackers including ECO [11], DeepSTRCF [20], ASRCF [5], UDT+ [38], MCCT [37], ECO-HC [11], LADCF-HC [44], STRCF [20], UDT [38], AutoTrack [23], ARCF [18], RSST [49], BACF [15], MCCT-H [37], ARCF-H [18], STAPLE_CA [32], SAMF_CA [32], fDSST [12], DCF_CA [32], MOSSE_CA [32] and RaF [46]. The success and precision plots of these evaluated trackers are shown in Fig. 6. Meanwhile, the comparison of top-5 trackers in accuracy and speed are presented in Tables4 and 5, respectively. Generally, the AUC and DP scores of all the trackers on these two UAV benchmarks are worse than that on OTB [41, 42] and TC128 [24]. However, ADCF performs favorably against most trackers. Although the proposed tracker slightly underperforms ECO [11] and DeepSTRCF [20] in terms of accuracy, it is more than 12 times faster than ECO and 4 times faster than DeepSTRCF, respectively.

Fig. 6
figure 6
Success and precision plots of the evaluated trackers on UAV123 and UAV123@10fps

Full size image
Table 4 Comparative results of top-5 trackers on UAV123 benchmark in accuracy and speed
Full size table
Table 5 Comparative results of top-5 trackers on UAV123@10fps benchmark in accuracy and speed
Full size table
Evaluation on VOT2016

VOT2016 [19] tracking benchmark contains 60 challenging sequences. We compare the ADCF with top-10 trackers which are publicly listed in the VOT2016 official report [19], including C-COT [10], EBT [50], TCNN, Staple [1], STAPLE+, SSAT, MLDF, DDC, SRBT and DNT. The comparative results are shown in Table 6. As indicated in the VOT2016 report [19], the strict SOTA bound is 0.251 under EAO metrics, and the trackers whose EAO score exceeds this bound will be considered as SOTA trackers. Table 6 shows that the EAO score of the ADCF is 0.317 indicating that the ADCF is a SOTA tracker. Moreover, Table 6 shows that ADCF ranks second in accuracy and fourth in EAO and robustness, respectively.

Table 6 Comparison between ADCF and the top-10 trackers on VOT2016 benchmark in expected average overlap (EAO), accuracy (A) and robustness (R)
Full size table
Qualitative evaluations
The qualitative evaluations of the ADCF with nine SOTA trackers, i.e., ARCF [18], ECO [11], BACF [15], AutoTrack [23], fDSST [12], UDT+ [38], MCCT [37], STRCF [20] and LADCF-HC [44] are shown in Fig. 7. Due to space constraints, we present representative results on ten challenging sequences from OTB2013 [41], OTB2015 [42], TC128 [24], UAV123 [31] and UAV123@10fps [31] tracking benchmarks. As shown in Fig. 7, the AutoTrack, ARCF and fDSST are less effective in handling scenarios with occlusion (Biker, Lemming and Ball_ce2) attributes. The LADCF-HC, STRCF, BACF and UDT+ trackers achieve poor performance on sequences with fast motion (Bird1, Skiing and uav1) attributes. Although MCCT and ECO perform well on most sequences, they are less effective when dealing with scale variation (Surf_ce1 and car18). The qualitative evaluation demonstrates that the proposed ADCF is more competitive than other trackers.

Fig. 7
figure 7
Qualitative evaluation of sample sequences from OTB2013, OTB2015, TC128, UAV123 and UAV123@10fps (from top to bottom: Lemming, Skiing, Biker, Bird1, Ball_ce2, Surf_ce1, car1_s, person8, car18 and uav1). The indices of the frames are marked on the top-left of each figure

Full size image
Ablation studies
First, we conduct an ablation study to demonstrate the effectiveness of the critical components in the ADCF tracker. The overall evaluation is shown in Table 7. The basic concepts are as follows. (1) "Baseline" refers to the method which does not adopt the spatio-temporal regularization, scale filter and S-APCE update strategy. (2) "Baseline+(STR)" denotes the baseline method by adding spatio-temporal regularization. (3) "Baseline+(SF)" denotes the baseline method by adding scale filter. (4) "Baseline+(S-APCE)" represents the baseline method by adopting the S-APCE update strategy. (5) "Baseline+(STR)+(SF)+(S-APCE)" is the final ADCF tracker.

As shown in Table 7, all the critical components, i.e., scale filter, S-APCE update strategy and spatio-temporal regularization, contribute to the substantial improvement of the baseline method in terms of AUC and DP. Especially, the components of the scale filter and S-APCE update strategy boost the FPS by 38.4% and 83.7%. The final tracker improves the baseline method by 6.4%, 5.4% and 129.1% in AUC, DP and FPS, respectively.

Table 7 Ablation analysis of the key components in ADCF on OTB2015 benchmark
Full size table
Second, we conduct an ablation study on the feature configurations in ADCF tracker. The tracking performance with different feature configurations are compared in terms of AUC and DP metrics on OTB2015 benchmark. As shown in Table 8, the fused features of HOG and Conv-4 outperform other feature configurations.

Table 8 Tracking performance on OTB2015 with different feature configurations
Full size table
Conclusion
In this paper, we propose an accelerated duality-aware correlation filters (ADCF) model to improve the tracking performance. In the ADCF model, the translation filter exploits deep features to localize the target accurately, while the scale filter exploits handcrafted features to estimate the scale efficiently. Meanwhile, the spatial and temporal constraints are introduced into ADCF model to suppress the boundary effect and filter degradation simultaneously. Moreover, a model update strategy, namely sparse learning-based average peak-to-correlation energy (S-APCE), is proposed to update the ADCF model by the response map adaptively. Finally, an ADMM formulation is developed to optimize the ADCF model. Experiments are conducted on six challenging tracking benchmarks, i.e., OTB2013, OTB2015, TC128, UAV123, UAV123@10fps and VOT2016. The qualitative and quantitative experimental results demonstrate the superiority of the proposed method against SOTA trackers in terms of tracking accuracy and speed.