Rapid progress in adversarial learning has enabled the generation of realistic-looking fake visual content. To distinguish between fake and real visual content, several detection techniques have been proposed. The performance of most of these techniques however drops off significantly if the test and the training data are sampled from different distributions. This motivates efforts towards improving the generalization of fake detectors. Since current fake content generation techniques do not accurately model the frequency spectrum of the natural images, we observe that the frequency spectrum of the fake visual data contains discriminative characteristics that can be used to detect fake content. We also observe that the information captured in the frequency spectrum is different from that of the spatial domain. Using these insights, we propose to complement frequency and spatial domain features using a two-stream convolutional neural network architecture called TwoStreamNet. We demonstrate the improved generalization of the proposed two-stream network to several unseen generation architectures, datasets, and techniques. The proposed detector has demonstrated significant performance improvement compared to the current state-of-the-art fake content detectors with the fusing of frequency and spatial domain streams also improving the generalization of the detector.

Introduction
Recent technological advancements in artificial intelligence (AI) have led to various beneficial applications in vision, language, and speech processing. However, at the same time, the power of these technologies may be exploited by adversaries for illegal or harmful uses. For example, Deepfakes‚Äîa portmanteau of the terms ‚Äúdeep learning‚Äù and ‚Äúfake‚Äù‚Äîmay be used to produce or alter photo-realistic audio-visual content with the help of deep learning for an illegal or harmful purpose. Deepfake technology enables one to effectively synthesize realistic-looking fake audio or video of a real person speaking and performing in any arbitrary way [1]. The term Deepfake was first coined by a Reddit community for synthetically replacing the face of a person with the face of another person. The term expanded with time to include similar techniques such as Lip-Sync [2, 3], facial expression reenactment [4,5,6], full-body and background manipulation as well as audio synthesis [7,8,9,10,11,12].

The rise of technology such as Deepfake has eroded the traditional confidence in the authenticity of audio and video as any digital content (audio, video, text) can be easily subverted using advanced deep learning techniques for synthesizing images trained on readily accessible public videos and images [13,14,15,16]. The gravity and urgency of the Deepfake threat can be gauged by noting that in recent times a CEO was scammed using Deepfake audio for $243,000 [17] and a fake video of the president of Gabon has resulted in a failed coup attempt. Other potential effects of the Deepfake threat include danger to journalism and democratic norms because elections can be manipulated and democratic discourse may be disrupted by creating fake speeches of contending leaders [1, 3]. Unfortunately, most of the current research focuses on creating and improving Deepfakes, and there is a lack of focus on reliable Deepfake detection. For instance, among those papers uploaded to arXiv in 2018, 902 papers focused on Generative Adversarial Networks (GANs), a common method for Deepfake generation, while only 25 papers related to anti-forgery related topics [16].

Recent research shows that neural networks can be used for detecting fake content [18,19,20,21,22]. These methods however require a large amount of fake and real training data to accurately learn the data distributions of both classes. The performance of these methods drops significantly on the unseen fake data if sampled from a different distribution or a different generation process. It is because the underlying model may over-fit the available training data and therefore lose its ability to generalize to unseen data. To enable the model to classify previously unseen data will require a large amount of data from the new distribution which may not always be available in such problems. Attackers and defenders are continuously improving their approaches and rolling out new attacks and defenses. Therefore, it may be very difficult to collect a large amount of fake data for new manipulation techniques. Ideally, for such scenarios, a fake content detector is needed that should be able to detect fake data without explicit training on that particular type of fake content.

Nataraj et al. [23] proposed to improve the detectors for fake images by using hand-crafted co-occurrence matrices as input features. They can produce good results on only one unseen test set, however, their approach did not perform well on other types [24]. Zhang et al. [25] discovered that classifiers do not generalize well between GAN models and proposed to use the Discrete Fourier Transform (DFT) spectrum of full images as an input to the deep learning models to detect fake images. In contrast, we propose to calculate DFT on 8x8 blocks of the images and to combine these with Discrete Wavelet Transform (DWT) features. Furthermore, instead of using only the information from the frequency domain, we also propose to combine the artifacts from the spatial domain and show through extensive experiments that this technique generalizes well on many unseen test sets.

In the current work, we propose a two-stream network for fake visual content detection. The first stream called ‚ÄòSpatial Stream‚Äô detects the fake data employing RGB images while the second stream dubbed as ‚ÄòFrequency Stream‚Äô utilizes a combination of DFT and DWT for discriminating fake and real visual content. The frequency stream exploits the fact that the distribution of the frequency spectrum of the fake visual data remains distinct from the distribution of the real data frequency spectrum. This is illustrated in Fig. 1, which shows the DFT-magnitude spectrum for a sample of real and fake images. It can be seen that the frequency spectrum has patterns that are different from that of real images. These differences are used to classify the fake versus real content. To elaborate the frequency spectrum differences further we have shown the average spectra of the fake and real images from 11 different GAN generators. Following the method used by [25], we used all the images available in our test set to calculate the spectrum on the high passed filtered images and then took the average (Fig. 2). Since the information captured by the frequency stream is different from the information captured by the spatial stream, both these streams complement each other, and fusing them can provide better performance and generalization to unseen fake data detection. To the best of our knowledge, this is the first work that studies cross-modal information fusion to improve fake content detection generalization.

Fig. 1
figure 1
DFT-magnitude spectrum for fake and real images has discriminative features which can be exploited for improved fake detection performance

Full size image
Fig. 2
figure 2
We show that average spectrum (calculated on high-pass filtered image, similar to Zhang et al. [25]) for fake and real images have discriminative features which can be exploited for improved fake detection performance. Using the method of Zhang et al. [25], we first calculate the spectra of all the images in the test set and then take average of all

Full size image
The main contributions of this paper are summarized next.

1
A novel two-stream architecture for fake visual content detection consisting of a Spatial Stream (SS) and a Frequency Stream (FS) is proposed. The SS learns the difference between the distributions of real and fake visual content in the spatial space using RGB images, while the FS learns to discriminate between the distributions of real and fake content in the frequency domain. The coefficients of the stationary frequencies are captured using DFT, while the coefficients of spatially varying multi-scale frequencies are captured using Haar Wavelet transform. The spatial and frequency information complement each other and therefore their fusion improves fake visual content detection.

2
The proposed two-stream network comprising a frequency and a spatial domain stream has outperformed the state-of-the-art fake detection methods with a significant margin. A detailed analysis of the proposed approach is performed and we empirically demonstrate that the proposed approach is robust across different quality JPEG compression and blurriness artifacts.

In Sect. 2, we discuss the related work and cover the traditional image forensics techniques and the latest deep learning-based image forensics algorithms with a prime focus on generalization. In Sect. 3, we present our proposed methodology with pre-processing schemes, training, and testing procedures. In Sect. 4, we introduce the datasets used for evaluating and providing the results of our experiments. Section 5 critically evaluates the performance and the generalization of the proposed methodology by performing an ablation study. Finally, Sect. 6 concludes the paper and also points towards future directions.

Related work
In this section, we briefly review recent works needed to understand the state-of-the-art solutions in image forensics. We have divided this section into four subsections. We begin with a brief overview of the hand-crafted image forensic techniques followed by a discussion on deep-learning-based image forensic approaches. After that, we discuss methods that focus on improving generalization. Finally, we conclude the section by covering the state-of-the-art frequency-domain techniques that are specifically designed for image forensic applications.

Hand-crafted image forensics
A variety of methods are available in the literature for detecting traditional image manipulation techniques. Most of these manipulations are designed with the help of image editing tools. The traditional techniques make use of hand-crafted features to detect specific clues that are created as a result of different manipulations. For example, several blind noise estimation algorithms have been proposed to detect region splicing forgeries [26, 27]. Popescu et al. [28] detected the image forgeries by estimating the re-sampling in the images. Haodong et al. [29] integrated tampering possibility maps to improve forgery localization. Yuanfang et al. [30] identified potential artifacts in hue, saturation, dark and bright channels of fake colorized images, and developed detection methods based on histograms and feature encoding. Similarly, Peng et al. [31] used contact information of the standing objects and their supporting planes extracted from their reconstructed 3D poses to detect splicing forgeries. However, these techniques are unable to provide comparable performance to that of pixel-based methods in realistic situations. In recent works, learning-based techniques have become the preferred method compared to traditional image forensics for achieving state-of-the-art detection performance [32,33,34,35].

Deep learning based image forensics
Due to the success of deep learning in different fields, several researchers have recently leveraged deep learning approaches for fake visual content detection. Yan et al. [36] proposed an algorithm based on difference images (DIs) and illuminant map (IM) as feature extractors to detect re-colorized images. Quan et al. [37] designed a deep CNN network with two cascaded convolutional layers to detect computer-generated images. McCloskey et al. [38] detected fake images by exploiting artifacts in the color cues, whereas Li et al. [39] used face warping artifacts for the forgery detection. Li et al. [40] noticed that eye blinking in fake videos is different from the natural videos and used this fact to expose the fake videos. Similarly, Yang et al. [41] have detected Deepfakes by identifying the inconsistent head poses. Recently, Afchar et al. [22] proposed two compact forgery detection networks (Meso-4 and MesoInception-4) in which forgery detection is done by analyzing the mesoscopic propertiesFootnote1. of Deepfake videos. Similarly, Nataraj et al. [23] have shown that features extracted from the co-occurrence matrix can help improve fake data detection. Wang et al. [42] proposed an anomaly detector-based approach that uses pre-trained face detectors as a feature extractor. Yang et al. [43] proposed the use of saliency maps to distinguish between real and fake images. Guo et al. [44] proposed a procedure for identifying fake face images by exploiting the GAN-generated artifacts in the Iris of the eye. Most of the aforementioned fake image detection techniques fail to distinguish between real and fake images if the visual data is sampled from a different distribution.

Methods focused on generalization
In this subsection, we briefly describe the fake detection approaches focused on generalization. Cozzolino et al. [45] proposed an autoencoder-based method to improve the performance of the model where learned weights are transferred for a different generation method. Zhang et al. [25] proposed a generalizable architecture named AutoGAN and evaluated its generalization ability on two types of generative networks. Xuan et al. [46] proposed that by using Gaussian Blur or Gaussian noise, one can destroy unstable low-level noise cues and force models to learn more intrinsic features to improve the generalization ability of the model. Similarly, Wang et al. [24] suggested that careful pre-and post-processing with data augmentation (such as blur and JPEG compression) improves the generalization ability. They have also shown improved fake detection results on multiple test sets by training on just one image generation network.

Frequency domain methods
Gueguen et al. [47] extracted features from the frequency domain to perform classification tasks on images. Ehrlich et al. [48] proposed an algorithm to convert the convolutional neural network (CNN) models from the spatial domain to the frequency domain. Xu et al. [49] proposed learning in the frequency domain and have shown that the performance of object detection and segmentation tasks gets improved in the frequency domain as compared to using the spatial RGB domain. Durall et al. [50] have shown that fake images have a difference in high-frequency coefficients compared to the natural images which he used for fake detection. Wang et al. [24] have shown that the artifacts in the frequency spectrum of fake images can be detected. Zhang et al. [25] proposed that if instead of raw pixels, frequency spectrum (2D-DCT on all 3 channels) is used as an input to the fake image detector, the performance of the detector improves. These frequency response base detectors target specific properties of the image generation process therefore, their performance degrades when fake images from unseen distributions are tested. In contrast to these existing methods, the proposed algorithm fuses information from the spatial domain and the frequency domain to achieve improved generalization. Also, we propose to fuse DFT with Wavelet Transform to improve the discrimination in the frequency domain. These innovations have resulted in significant improvement in fake content detection compared to the existing methods.

Methodology
Improving the generalizability of a fake detection model is critical for its success in real-world applications where the fake content may be generated by unknown processes. We propose a generalizable fake detection model based on a two-stream convolutional network architecture shown in Fig. 3.

Fig. 3
figure 3
Proposed two-stream convolutional neural network (TwoStreamNet). The two network streams capture spatial and frequency domain artifacts separately, and their outputs are fused at the end of the network to produce classification scores

Full size image
The proposed architecture is motivated by the excellent performance of two-stream networks in action recognition in videos. To the best of our knowledge, the proposed network performs quite well on both seen and unseen data and has outperformed existing state-of-the-art (SOTA) methods in a wide range of experiments as we shall discuss in later sections. Our proposed two-stream network is novel and such a combination of frequency stream and the spatial stream has not been proposed before. In the following, we discuss the RGB to YCbCr conversion, DFT, DWT, and the proposed architecture in more detail.

The RGB to YCbCr transformation
The three channels in RGB color space are correlated with each other. We consider an orthogonal color space for improved representation. In our experiments, we have used YCbCr that has performed better than RGB space. As recommended in previous research [51, 52], the following formulas are used to convert from RGB to YCbCr color space:

ùëå=ùêæry.ùëÖ+ùêægy.ùê∫+ùêæby.ùêµ, Cr=ùêµ‚àíùëå,Cb = R‚àíY ùêæry+ùêægy+ùêæby=1, 
(1)
where, ùêæry,ùêægy, and ùêæby are the coefficients for color conversion whose values are specified in Table 1 according to the standards. In our implementation, we used ITU601 [53].

Table 1 Coefficients ùêæry and ùêæby of color conversion from RGB to YCbCr
Full size table
A review of frequency domain transforms
To fully capture the frequency information from a YCbCr image, we compute DFT and DWT for each image.Discrete Fourier Transform (DFT): Using DFT, one can decompose a signal into sinusoidal components of various frequencies ranging from 0 to maximum value possible based on the spatial resolution. For two dimensional data, i.e., images of size ùëä√óùêª, the DFT can be computed using the following formula:

ùëãùë§,‚Ñé=‚àëùëõ=0ùëä‚àí1‚àëùëö=0ùêª‚àí1ùë•ùë§,‚Ñéùëí‚àíùëñ2ùúãùëÅùë§ùëõùëí‚àíùëñ2ùúãùëÄ‚Ñéùëö,(2)
where w is the horizontal spatial frequency, h is the vertical spatial frequency, ùë•ùë§,‚Ñé is the pixel value at coordinates (w, h), and ùëãùë§,‚Ñé carries the magnitude and phase information of frequency at coordinates (w, h). Discrete Wavelet Transform (DWT): Wavelet transform decomposes an image into four different sub-band images. High and low pass filters are applied at each row (column), and then they are down-sampled by 2 to get the high and low-frequency components of each row (column) separately. In this way, the original image is converted into four sub-band images: High-high (HH), High-low (HL), Low-high (LH), and Low-low (LL). Each sub-band image preserves different features: HH region preserves high-frequency components in both horizontal and vertical direction; HL preserves high-frequency components in the horizontal direction and low-frequency components in the vertical direction; LH preserves low-frequency components in the vertical direction and high-frequency components in the horizontal direction; and finally, LL preserves low-frequency components in the vertical direction and low-frequency components in the horizontal direction.

Frequency stream
In this stream, two different types of the frequency spectrum are fused to get improved frequency domain representation which can better discriminate between the real and the fake visual content. An overview of the frequency spectrum fusion is shown in Fig. 4.

Fig. 4
figure 4
Proposed pre-processing pipeline: the input image is first converted to YCbCr color space and then transformed to the frequency domain by applying DFT and Wavelet Transforms (DWT). After DFT, we get real (R) and imaginary (I) channels, and after WT we get four channels: HH, HL, LH, and LL. The resulting channels are concatenated to form 3D cubes which are then provided as input to the frequency stream for further processing

Full size image
The three YCbCr channels are then transformed to the frequency domain using two different types of transformations, including DFT and DWT. Each channel is divided into a non-overlapping block of size 8 √ó 8 pixels and a transformation is applied on each block independently. The resulting coefficients are then concatenated back to obtain the arrays of the original image size. The output of the DFT converts one input channel into two output channels corresponding to real and imaginary coefficients. Similarly, the DWT converts one input channel into 4 output channels corresponding to low frequencies (LL), high and low frequencies (HL), high frequencies (HH), and low and high frequencies (LH). For three input channels (YCbCr), we obtain 18 output channels, 6 from DFT and 12 from DWT. All of these frequency output channels are concatenated to form 3D cubes of size ùêª√óùëä√óùê∂, where H is the height and W is the width of the image, and C=18 are the number of channels. We empirically observe that both DFT and DWT are necessary to capture essential information in the frequency domain at varying scales for improving the generalization ability of the proposed network.

Spatial stream
In this stream, RGB channels of the image are passed as input to the ResNet50 [56] as the classifier. RGB images are augmented in a special way using JPEG compression and Gaussian Blur as recommended by Wang et al. [24]. This stream is trained individually and plugged in the TwoStreamNet at the test time.

Two stream network architecture
The proposed two-stream network architecture is shown in Fig. 3. ResNet50 network is used as a backbone in both of the streams of the proposed architecture. Since the number of input channels in the frequency stream is larger as compared to the spatial stream, therefore the first layer of FS is accordingly modified. Both streams are independently trained, and the output of both streams is fused using the class probability averaging fusion method. In this fusion scheme, both streams contribute equally to the output to produce the final classification probability. The performance of the combined scores is significantly better than the performance of the individual streams.

Experiments and results
Training Dataset Following the protocol used by [24], the proposed two-stream network is trained using the fake images generated by ProGAN [57] and tested on the images generated by many other GANs. ProGAN has 20 different officially released models trained on different object categories of the LSUN dataset, which is a large-scale image dataset containing around one million labeled images for each of the 10 scene categories and 20 object categoriesFootnote2 [58]. We choose 15 (airplane, bird, boat, bottle, bus, car, cat, chair, dog, horse, motorbike, person, sofa, train, and tv monitor) out of 20 models to create our validation and training set. We generated 10k fake images for training and 500 fake images for validation using each of the 15 models. For each of these 15 categories of fake images, we collect 10k of real images for training and 500 for validation randomly from the LSUN dataset [58]. In total, we have 300K training images and 15K validation images. For real images, we center crop the images equal to the size of the shorter edge and then resize the images to 256 √ó 256.

Testing Dataset Testing dataset images were generated using completely unseen generators as described in Table 4. To remain consistent with the current state of the art, the same generators are selected as that of [24]. The real images for testing purposes are obtained from the repository for each generator.

Implementation details
For training the FS, we use the Adam [71] optimizer with an initial learning rate of 0.0001, weight decay of 0.0005, and a batch size of 24. For all the training sets, we train the proposed network for 24 epochs. Large training data has helped the model to converge quickly. Lastly, we select the best model based on the validation set. While training each stream, data augmentation based on Gaussian blur and JPEG compression with 10% probability is used.

Evaluation metrics
We have used following metrics in our evaluation:

F1-Score F1-Score is the harmonic mean of precision and recall and is calculated below as:

ùêπ1=2√óPrecision√óRecallPrecision + Recall,
where Precision is the number of true positives divided by the summation of true positives and false positives and the recall is the number of true positive results divided by the number of all samples that should have been identified as positive.

Accuracy Accuracy is defined as the ratio of the correct predictions over the total number of predictions made. And is calculated as below:

ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶=ùëáùëÉ+ùëáùëÅùëáùëÉ+ùëáùëÅ+ùêπùëÉ+ùêπùëÅ,(4)
where TP, FP, TN and FN represents is the number of true positives, false positives, true negatives and false negative respectively.

Comparison with the existing state-of-the-art algorithms
We thoroughly evaluated the performance of the proposed method on the test dataset and compared it with the existing state-of-the-art [24]. We also compared the robustness analysis of our approach against some common real-world perturbations. In Table 2, we have shown a comparison of our results with the best results of Wang et al. [24] ([Blur+JPEG(0.1)]). Their results from their official web linkFootnote3. Results show that our FS approach performs very well on the unseen manipulations and outperforms the state-of-the-art on several test sets while having competitive performance on the remaining. Results of the two-stream architecture demonstrate that our complete approach outperformed the state-of-the-art in almost all of the cases. Analysis of the results shows that when both spatial and frequency streams are combined into a two-stream architecture, they complement each other in a way that their combined accuracy is greater than any of them individually. This clearly shows that FS ConvNet has learned distinctive features that were not learned by SS ConvNet. Overall, the combination of FS and SS plays a vital role in improving the generalization ability of the fake image detectors.

Table 2 Comparison of the proposed Frequency Stream (FS) and two-stream network with the state-of-the-art method [24] using average accuracy. Best results of Wang et al. [24] with data augmentation using blur and JPEG (0.1) are reported where 0.1 mean JPEG compression is applied on 10% images. The same augmentation is also used in the proposed approaches. Both our approach and that of Wang et al. are trained using ProGAN only and tested on the data generated by 12 unseen generation processes mentioned in the top row
Full size table
In Table 3 we compare our method to four different models [Cyc-Im, Cyc-Spec, Auto-Im, Auto-Spec] of Zhang et al. [25]. They released four models with two kinds of variations, first they used two datasets generated using two different GAN architectures CycleGAN and AutoGAN referenced as (Cyc and Auto) in Table 3, and as a second variation, they passed images as input to one model and frequency spectrum in the other model referenced as (Im and Spec) in Table 3. Results of our approach show that our two-stream architecture outperformed all models of Zhang et al. 3 in almost all of the test sets (Fig. 5).

Table 3 Comparison of the proposed two-stream network with Zhang et al. [25] using accuracy. We compare the proposed network with 4 models released by [25], each one was trained using one of two image sources CycleGAN (Cyc) and AutoGAN (Auto), as well as one of two image representations images (Im) and spectrum (Spec). The blue text shows the same training and testing data
Full size table
Fig. 5
figure 5
Robustness comparison of the proposed algorithm with Wang et al. [24] for Gaussian Blur and JPEG Compression artifacts. In most experiments, the proposed two-stream net. We apply Gaussian Blur and JPEG Compression of different sizes on the test sets and measure the effect on the accuracy of our model. Our model performs near to the best for all the cross-modal datasets even when a large blurring effect is applied. Results show that our proposed solution is more robust as compared to the state of the art

Full size image
In Fig. 6A, we have shown samples of the fake images which are misclassified by the state-of-the-art and are correctly classified by our proposed two-stream approach. These results demonstrate the ability of the proposed approach to detect high-quality fake images which are even very hard to discriminate by humans. Figure 6B shows the fake images which are misclassified by both Wang et al. [24] and us.

Fig. 6
figure 6
A Examples of fake images correctly detected by the proposed two-stream network, however, misclassified by Wang et al. [24]; B Examples of fake image misclassified by both our proposed method and that of Wang et al. [24]

Full size image
Robustness Analysis In real-world settings, fake images may undergo several post-processing operations like compression, smoothness, etc. Therefore, we have evaluated the performance of the proposed model on the images which are post-processed using JPEG compression and Gaussian blur. Specifically, we apply Gaussian blur with different standard deviations including [3, 5, 7, 9, 11] and JPEG compression with JPEG image quality factor of [85, 87, 90, 92, 95]. Results in Fig. 5 show that our approach is robust to common perturbations. For most of the models, the proposed approach significantly outperformed the state-of-the-art method at varying blur levels. Similarly, the proposed approach also performed better than the state-of-the-art methods for a wide range of JPEG compression.

Ablation study
In this section, we thoroughly validate the different components of the proposed approach by performing an ablation study (Table 4).

Table 4 Details of the testing dataset
Full size table
Combining DFT and DWT
As shown in Fig. 3, we propose to combine DWT and DFT for better feature representation and robust fake content detection. To verify the effectiveness of using both transformations, while keeping all the experimental settings the same, we experimented with DFT and DWT separately. After training for 20 epochs, the best epoch is chosen based on validation data accuracy. The results shown in Table 5 demonstrate that a combination of DFT and DWT is essential to produce robust feature representation for fake image detection.

Table 5 Evaluation of DFT and DWT combination for fake image detection. Percentage accuracy is reported for the full image using only DFT, only DWT, and the combination DFT + DWT
Full size table
Effect of block size
We study the effect of using different block sizes instead of computing DFT over the whole image. In Table 6, we have shown results of computing DFT on the block size of 8√ó8, 16√ó16, 32√ó32, and 256√ó256 (Full-Image size). Note that the block size experiments are performed while keeping identical experimental settings. Results demonstrate that 8√ó8 block size has consistently outperformed other block sizes. Therefore, transforming the image to the frequency domain using 8√ó8 blocks for DFT is more effective for fake image detection.

Table 6 Fake image detection accuracy variation by varying block sizes for DFT transform. The block size 8√ó8 has produced best results and is therefore used in our experiments
Full size table
Effect of color-space
We evaluate the effectiveness of converting images into YCbCr color space before frequency transformations. We performed two experiments using the same settings to compare the performance of RGB with YCbCr color space. Results in Table 7 show that converting an image to YCbCr colorspace adds more discriminative features in the frequency domain and helps in better fake image detection.

Table 7 The compassion of fake detection performance using RGB and YCbCr Colorspace. YCbCr color space has performed better than RGB color space
Full size table
Limitations
The computation of DFT and DWT is computationally expensive. Therefore to implement it for fake content detection in real-time applications using video data may be a potential limitation. However, this limitation can be overcome by using parallel computation of DFT and DWT. Figure 6B shows failure cases of the proposed algorithm which are the result of non-discriminative frequency domain features. Please note that these fake images are high quality and very hard to discriminate, even for humans.

Conclusions
This paper addresses the problem of fake image detection. For this purpose, a two-stream network is proposed consisting of a spatial stream and a frequency stream. The proposed method generalizes to unseen fake image generator distributions much better than the current state-of-the-art approaches. The proposed method is also found to be more robust to the common image perturbations including blur and JPEG compression artifacts. The improved performance is leveraged by combining two types of frequency domain transformations, namely, Discrete Fourier Transform (DFT) and Discrete Wavelet Transform (DWT). Both transformations are applied upon YCbCr color-space, and different frequency domain channels are concatenated to discriminate fake images from the real ones. By exploiting the differences between the real and the fake image frequency responses, improved fake detection performance is achieved. In the future, we aim to extend this work for fake video and audio detection.