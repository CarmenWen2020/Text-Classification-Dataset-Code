In recent years, considering a balanced accuracy and efficiency, Fully-Convolutional Siamese network (SiamFC) is widely used in the field of visual tracking. Although SiamFC has achieved great success, it is still frustrated in discrimination especially in the discriminative scene. The main reason for the poor discrimination ability of SiamFC is that during the training process, it pays more attention to fitting the whole dataset than learning discrimination ability to similar objects. In terms of this issue, we propose Ensemble Siamese networks (ESiamFC) for tracking by introducing ensemble learning into SiamFC. In detail, firstly, we map the training dataset ILSVRC2015 into embedded space. Secondly, we use balanced k-means to cluster video features. Thirdly, in each cluster, we apply transfer learning into SiamFC to obtain k base trackers with their preferences. Last but not least, to leverage the diversity of base trackers, we propose a Cluster Weight fusion module which can automatically assign fusion weight to base trackers according to the semantic information of the tracking object. Extensive experiments on multiple benchmarks demonstrate that our tracker outperforms SiamFC in precision with a relative increase of 7.1%, 8.6%, 6.7% on Tcolor128, DTB70, LaSOT, respectively.

Introduction
Visual tracking is a fundamental problem in computer vision and is widely used in unmanned vehicles, video surveillance, humanâ€“computer interaction, and so on [1,2,3]. Given the position and scale of the target in the first frame, the tracker tracks the target and predicts its position and scale information in subsequent frames. During the tracking process, the tracker is often affected by occlusion, illumination changing, motion blur, and out of view making visual tracking a challenging problem.

SiamFC [4] converts the visual tracking process into a template matching problem. A template and detection image are mapped into a newly embedded space by the same convolutional neural network (CNN), and they compare the similarity with each other by correlation. Thanks to CNN's powerful appearance representation capability and the simplicity of Siamese structure, SiamFC shows well tracking performance while reaching up to 120 frames per second (FPS). Although SiamFC has achieved outstanding progress in visual tracking, its discrimination ability is still so weak that it is often attracted by distractions leading to tracking drift and even failure, especially in complex backgrounds.

Differing from online regression-based MDNet [5], FCNT [6] and correlation filter-based MOSSE [7], ECO [8], SRDCF [9], SiamFC does not update the tracking model during the tracking process. Thus, it is impossible to extract interferer information from the background and suppress it. During the training process of SiamFC, stochastic gradient descent (SGD) directly applies to the whole dataset, and the final obtained model emphasizes more on fitting the whole dataset while neglecting the learning of discrimination ability on distractors similar to the target object.

Since the discrimination ability of SiamFC depends entirely on the embedded space corresponding to its backbone network, we only need to increase the distance of similar target objects in the embedded space to improve the discrimination ability of SiamFC. However, the backbone network of SiamFC has limited discrimination ability, and if we emphasize some of these similar training samples, it will weaken the networkâ€™s discriminative ability on other samples. [10,11,12] show that ensemble learning [13] can effectively improve the discrimination ability of CNN. We introduce the ensemble learning method into SiamFC and obtain ESiamFC. Ensemble learning aims to combine multiple weaker learners to obtain a strong learner. The greater the differentiation these weak learners have, the stronger performance of the combined learners will be [13]. In contrast, to obtain a discriminative tracker, we first need to obtain multiple base trackers with divergences. In our work, this diversity is reflected in the training set. We cluster the training set based on the similarity between samples and subsequently train the base trackers on these clusters obtained by clustering. Since the individual base trackers train on a specific subset of the training set, they have a stronger discrimination ability for the target object in that subset, i.e., similar targets. And then, the individual training subsets are independent of each other and therefore have diversity. Subsequently, we introduce a fusion strategy to combine these base trackers into a strong tracker.

The training sample of SiamFC is video sequences. In order to measure the similarity between video sequences, we conduct a video feature extraction method to obtain the representation of video sequences in the embedded space. CNN is data-urged, while the traditional k-means [14] is difficult to obtain clusters with uniformly distributed sample sizes which directly affects the performance of the base trackers with fewer cluster samples. In order to obtain a balanced clustering result, we introduce proportional-integral (PI) control theory [15] into the k-means to balance the cluster while reducing the sum error of the clustering result. During the tracking process, the semantic information of distractors and the target object in the scene is relevant in distribution. We use this relevance to design the cluster weight fusion module (CW), which assigns appropriate fusion weights to each base tracker based on the semantic information of the target object. The main contributions of this article are listed as follows:

We introduce ensemble learning into SiamFC, named ESiamFC, for enhancing the robustness and tracking accuracy of SiamFC.

We bring PI control theory into the k-means to obtain a balanced clustering algorithm with uniform distribution of cluster samples size.

A cluster weight fusion module is designed, which adaptively assigns weights to each base tracker according to the target object semantic information and fuses the base trackers into one strong tracker.

Extensive experiments on LaSOT [16], TColor128 [3], DTB70 [1] benchmark demonstrate that our tracker outperforms the base tracker and performs as a counterpart with other state-of-art trackers.

The remainder of this paper is organized as follows. In Sect. 2, the background related to this work is presented. A detailed description of the proposed tracking method is introduced in Sect. 3. Extensive experiments are conducted and explicit result analysis is exhibited in Sect. 4. Section 5 gives a conclusion of this work.

Related tracking methods
Deep tracker
Handcrafted features [17,18,19] are always limited to low-dimensional texture and local color statistics. Through extensive experiments, CNN Features [20] has demonstrated that the convolutional layer output features of CNNs trained on classification tasks can be well used in classification, detection, and image indexing. CF2 [21] first introduces deep features to visual tracking. Then CFWCF [22], CSR-DCF [23], DMSRDCF [24], UPTD [25] investigate the deep feature weighting strategies under different convolutional layers and effects of different types of deep feature in tracking. Although the studies mentioned above successfully applied deep features into visual tracking, such tracker is still limited to correlation filter framework. Unlike deep trackers that use CNN features, CNT [26] uses convolutional network structures to characterize the target and contexts without offline training. FCNT [6] employs CNN for feature selection and updates the CNN online through regression. MDNet [5] treats tracking as a foreground and background classification problem and gets a general representation of the target using multi-domain learning. Due to the lack of video sequence datasets, all of the aforementioned trackers use transfer learning to fit the network in the tracking task and fail to benefit from the end-to-end training property. The advent of visual object tracking datasets ALOV300â€‰+â€‰[27], ILSVRC2015 [28], GOT10K [29], and LaSOT [16] has changed this situation. GOTURN [30] regards tracking as a regression problem and performs end-to-end training on sample pairs from the video sequences. SINT [31] views tracking as an instance search problem and locates the target by region of interest polling (RoI pooling). SiamFC [4] replaces the RoI pooling in SINT with correlation, allowing the network to further learn a general representation of target objects. The most common way to enhance the representation of deep models is to use deeper networks. However, directly introducing deeper networks such as VGG [32], ResNet [33] into SiamFC reduces tracking performance. SiamDW [34], SiamRPN++â€‰[35] shows that the padding structure in networks breaks the translation invariance of the feature extraction network. SiamDW introduces ResNet into the Siamese structure by cropping out the padding affected features. SiamRPN++â€‰uses training sample pairs where the target is uniformly distributed over spatial position to avoid the effects of the padding structure. In order to give a more accurate description of the target object's bounding box, SiamRPN introduces regional proposal networks (RPN) into SiamFC. Based on the SiamRPN, SiamCAR [36] uses anchor-free proposal networks to reduce the number of hyperparameters while achieving more accurate bounding box predictions. SiamAtt [37] introduces an attention module that weights the results of SiamRPN++â€™s classification branch to increase the network's discrimination ability on positive and negative samples and introduces an offset module for more accurate target localization. This paper proposes an ensemble learning-based tracker that neither uses deeper backbone network structures for enhanced feature representation as SiamDW and SiamRPN++â€‰nor uses additional structures to give a more accurate description of the target object like SiamRPN SiamCAR, and SiamAtt. We fully leverage the information contained in the training dataset by utilizing ensemble learning to obtain an ensemble tracker with improved discrimination ability of similar distractors.

Ensemble tracker
With the development of visual tracking, various types of trackers have been developed. Such as correlation filter-based MOSSE [7], KCF [38], DSST [39]; colour statistical-based DAT [19]; depth regression-based MDNet [5], GOTURN [30]; template matching-based SiamFC, SINT. These diverse trackers have facilitated the creation of ensemble trackers. Ensemble learning combines multiple divergent base learners into a strong learner. The â€œbias-varianceâ€“covarianceâ€ theory [13] demonstrates that the diversity of base classifiers is the key to the success of ensemble learning. There are three ways to introduce diversity: data diversity, parameter diversity, and structural diversity. Thanks to the stacking structure of the deep neural network, it is convenient to introduce ensemble learning into the neural network. BranchOut [40] introduces structural diversity through multiple FCNs with different lengths and parameter diversity through random updating. CNNET [41] uses the output of VGG-NET in different convolutional layers as features to build multiple weak KCF trackers with data diversity and utilizes the consistency of target features on two adjacent frames as weights to integrate the weak trackers into a robust tracker. ET [42] trains a new classifier under each frame during the tracking process to obtain diversity trackers while adapting to changes in the appearance of the target object. CET [43] clusters on the feature and parameter spaces to obtain diverse base classifiers. These base classifiers are integrated into a robust classifier for foreground and background classification through the consistency of base classifiers and ensemble classifier results. DCET [44] integrates correlation filter and deep classifier into a robust tracker through multi-cue metrics. ACFN [45] obtains 260 KCF base trackers by different kernel functions, search scales, and update methods. During the tracking process, the attention network predicts the tracking results of these base trackers and selects the appropriate base trackers for tracking. In order to solve the problem of the large overlap in the template space of base trackers during the tracking process, DEDT [46] improves the diversity of base trackers by generating Artificial Data for the training of base trackers. DET [47] takes the local outputs of each convolutional layer of the SiamFC backbone network as the features of the matchers and integrates these base matches into a strong matcher according to the similarity score of each base matcher. MKCFup [48] exploits the complementary properties of target object features in the reproducing kernel Hilbert space corresponding to different kernel functions to construct a multi-core KCF and integrates these trackers by optimizing least-squares regression. PTAV [49] integrates SINT and fDSST [50] into an efficient and accurate tracker using a tracking-validation framework. MCCT [51] combines HOG [17], ColorNaming [18], and VGG-19 in different convolutional layers to obtain multiple sets of weak trackers with different feature types and constructs a self-evaluation and pair-evaluation system to integrate these weak trackers into a robust tracker. RET [52] treats tracking as a fitting process to a non-stationary distribution, constructs weak classifiers on different patches and weights the base classifiers with random variables that satisfy the reliability distribution into a robust classifier. Obli-RaF [53] replaces each node of random forest with proximal SVM, which enables incremental update to the random forest and thus introduces random forest into visual tracking. EST [54] integrates the appearance features of several recent frames on SINT and takes the integrated appearance features as a template to enable SINT to adapt to the appearance changes of the target object. Unlike previous methods, we map the training set into the embedded space for clustering and obtain the training sets with different appearance features for train base trackers. Then these base trackers are integrated into a robust tracker by a cluster weight fusion module.

Ensemble SiamFC
Baseline
SiamFC converts the visual tracking problem into a similarity matching problem. The target is mapped into the embedded space by convolution neural network (usually AlexNet [55]). In this embedded space, sample and search image are matched by similarity. According to the matching result, the position and scale information corresponding to the instance with the highest similarity score is taken as the tracking result.

Specifically, the Siamese network consists of two branches: the template branch and the detection branch. They have similar network structures and share weights, i.e., the weights are the same. The template branch takes the template image ğ‘§ (usually the given target in the first frame) as input and maps the sample to the embedded space through the CNN to obtain sample ğœ‘(ğ‘§) in the embedded space. Similarly, the detection branch maps the detection image ğ‘¥ to obtain the feature map ğœ‘(ğ‘¥) of the detection image. Subsequently, the template is correlated with the detection image in the embedded space to get the response map. The whole network is expressed as:

ğ‘“(ğ‘§,ğ‘¥)=ğ‘”(ğœ‘(ğ‘§),ğœ‘(ğ‘¥))
(1)
where ğ‘“(â‹…) denotes the Siamese network and ğ‘”(â‹…) denotes the correlation operation. SiamFC performs a multi-scale search on the detection image ğ‘¥ and determines the location and scale of the target based on those of the maximum score of the response map.

Thanks to the powerful representation capability of convolutional neural networks, SiamFC has made a breakthrough in visual tracking. However, in complex contexts, especially in the presence of distractors with similar semantic information to the target object, SiamFC is vulnerable to interference and leading to tracking drift or even tracking failure.

Although SiamFC learns on a large-scale (ILSVRC15) dataset, the discrimination ability on distractors is still incompetent. The fundamental reason for the weak discrimination of SiamFC is that similar objects are very close in the embedded space. Therefore, in the response map, the distractor produces a high similarity score, even surpassing the score of the target object. To solve this problem, we introduce ensemble learning into SiamFC to improve the discrimination ability of the tracker by integrating multiple base trackers with strong discrimination ability to identify specific similar target objects into a robust tracker. Figure 1 gives an overall flowchart of the proposed method. First of all, we extract the features of the whole dataset and cluster the target objects according to their distances in the embedded space to obtain k clusters. Then, on these clusters, we perform transfer learning on SiamFC to obtain k base trackers with specific preferences in discrimination ability. In order to better integrate these base trackers with diversity, we design a Cluster Weight fusion module (CW). This module can predict the reliability of each base tracker in the current scene based on the semantic information of the target object and adaptively assign weights to each base tracker. The final response map is obtained by weighted summation of the base trackersâ€™ response maps, and the scale and position of the target are determined according to the maximum response score of the final response map.

Fig. 1
figure 1
An overall flowchart of the proposed method

Full size image
PIk-means
Given a sample set ğƒ={ğ¹1,ğ¹2,â€¦,ğ¹ğ‘š}, the samples are divided into ğ‘˜ clusters ğ‚={ğœ1,ğœ2,â€¦,ğœk}. The ultimate goal of k-means is to minimize the clustering error presented as follows:

ğ¸=âˆ‘ğ‘–=1ğ‘˜âˆ‘ğ¹âˆˆğœğ‘–dist(ğ¹,ğœ‡ğ‘–)
(2)
where ğœ‡ğ‘– denotes the cluster center of ğœi, and ğ‘‘ğ‘–ğ‘ ğ‘¡(â‹…) denotes the distance function to measure the distance between two samples in the embedded space. In SiamFC, the cross-correlation between the template and detection image is taken as the response result. So, the correlation result between the sample and cluster center is taken as the distance measurement factor in our work. Since the higher similarity between the template and the detected image share, the higher the correlation response score will be. Therefore, the correlation result needs to map to a reasonable range for measuring the distance between samples. The distance function is shown as:

dist(ğ¹,ğœ‡ğ‘–)=âˆ’ln(0.5Ã—(ğ‘”(ğ¹,ğœ‡ğ‘–))+1)
(3)
Equation (2) is optimized by iteration. During the iteration, the distance between every sample and each cluster center is calculated, and the samples are grouped into the cluster with the smallest distance. Then, the centers of ğ‘˜ clusters are recalculated for the next iteration. The iteration process stops when the results of two consecutive iterations are identical.

figure a
Although the k-means can efficiently cluster the samples into ğ‘˜ clusters, the number of samples of each cluster is significantly different. As shown in Fig. 3, over 7000 target objects in the ILSVRC15 dataset are used for clustering, and the cluster with the lowest number of samples in the clustering result only contains less than 200 target objects. Despite its powerful fitting ability, the CNN network also introduces huge generalization errors [56]. Serious over-fitting problems will occur if the number of the training dataset is too small.

Inspired by control theory, we introduce proportional-integral (PI) control into the clustering process. Specifically, we take the difference between the number of samples of each cluster in the clustering result and the desired number as the error, which is adjusted by PI control. The controller acts directly on the distance function, which in turn controls the number of samples in each cluster. After introducing PI control into Eq. (2), the improved distance function is as follow:

distğ‘¡(ğ¹,ğœ‡ğ‘–)=âˆ’ln(0.5Ã—(ğ‘”(ğ¹,ğœ‡ğ‘–)+1))+ğ‘ƒ(ğ‘›ğ‘¡ğ‘–âˆ’ğ‘›ğ‘˜)+ğ¼âˆ«ğ‘—=1ğ‘¡ğ‘›ğ‘—ğ‘–âˆ’ğ‘›ğ‘˜
(4)
where ğ‘¡ denotes the ğ‘¡-th iteration, and ğ‘› denotes the total number of samples used for clustering. ğ‘ƒ denotes the proportional control coefficient, and ğ¼ denotes the integral control coefficient.

As shown in Fig. 3, after introducing PI control to the k-means (PIk-means), the training set is clustered successfully into ğ‘˜ clusters with similar sample size. The overall algorithm of PIk-means is summarized into Algorithm 1.

Video feature
The clustering sample ğ¹ğ‘– of PIk-means is video sequence, while the feature extraction network of SiamFC outputs feature {ğ‘“ğ‘–,1,ğ‘“ğ‘–,2,â€¦,ğ‘“ğ‘–,ğ‘š} of a single frame, where ğ‘– denotes the ğ‘–-th video, and ğ‘š denotes the video sequence length. Therefore, a mapping method {ğ‘“ğ‘–,1,ğ‘“ğ‘–,2,â€¦,ğ‘“ğ‘–,ğ‘š}â†’ğ¹ğ‘– is needed to reflect the video feature by the features of video frames.

Since the correlation operation is linear satisfying the superposition theorem, we could directly superimpose the features of all frames of the whole video sequence in the embedded space to get the video feature ğ¹ğ‘–=1/ğ‘šâˆ‘ğ‘šğ‘—=1ğ‘“ğ‘–,ğ‘—. However, the video sequences used for training have distortions such as occlusion, motion blur, and out of view. Directly using linear superposition will introduce irrelevant or noisy samples, which will deteriorate the video feature and affect the clustering results. To eliminate the downside effect of noisy frames, we introduce a filtering strategy. A distance covariance matrix ğ‚ğ¨ğ¯ğ‘–âˆˆğ‘mÃ—m among all frame features in the video sequence ğ‘– is constructed, in which the element of row ğ‘ and column ğ‘ is ğ‘ğ‘œğ‘£ğ‘,ğ‘=ğ‘‘ğ‘–ğ‘ ğ‘¡(ğ‘“ğ‘–,ğ‘,ğ‘“ğ‘–,ğ‘). The matrix ğ‚ğ¨ğ¯ğ‘– Ci Ci is then summed by rows, the summation result of the row ğ‘— is ğ‘ğ‘œğ‘£ğ‘—=âˆ‘ğ‘šğ‘=1ğ‘ğ‘œğ‘£ğ‘—,ğ‘. The physical meaning of ğ‘ğ‘œğ‘£ğ‘— can be interpreted as the similarity between the ğ‘—-th frame and all the other frames in the whole video. {ğ‘ğ‘œğ‘£1,ğ‘ğ‘œğ‘£2,â€¦,ğ‘ğ‘œğ‘£ğ‘š} is sorted and then the features of the first ğ›½ğ‘›,ğ›½âˆˆ(0,1] frames are selected and summed as the video feature ğ¹ğ‘– of the i-th video.

Figure 2 shows the â€œ00,001,008â€ video sequence in the ILSVRC15 dataset. In this video sequence, the target object (the turtle) gradually moves behind to the occlusion during the frames 37â€“44. The covariance matrix heat map shows that frames 40â€“47 have a considerable distance from other frames in the video sequence. From the right row summation bar, we can see that frames 40â€“47 have a significant response value. Therefore, this method can effectively remove the contaminated frames in the video sequence.

Fig. 2
figure 2
Video feature covariance matrix, the right bar is row summation

Full size image
As shown in Fig. 3, the PIk-means successfully solves the unbalanced number of cluster samples. To better demonstrate the difference in clustering results after the introduction of PI control, we reduce the video feature data to three dimensional via principal component analysis (PCA). The clustering results obtained by traditional methods are dominated by the number of samples in some clusters. In contrast by PIk-means, the number of samples in each cluster is evenly distributed, with a reasonable degree of aggregation within clusters and noticeable inter-cluster spacing. In the initialization stage, we randomly select ğ‘˜ samples for initialization. The results of clustering carried out by the traditional method can be significantly affected by random initialization. The sum error of the clustering results will lie in a wide range of intervals. In contrast, the sum error of the PIk-means is unaffected by the initialization method. Detailed analysis is discussed in the following experiment section.

Fig. 3
figure 3
Cluster result and sum error variation during the clustering process

Full size image
It is worth noting that, compared with the traditional k-means, the sum error of PIk-means is reduced. In our opinion, the traditional k-means directly searches the declining direction of the sum error for clustering, which is easy to fall into local optimal solutions. After introducing cluster number loss, it is equivalent to introducing a disturbance in the calculating process of clustering loss. Similar to the annealing mechanism of the Boltzmann machine [57], the introduction of disturbance gives the optimization process of Eq. (2) the ability to jump out of the optimal local solution. The sum error-iteration time curve shown in Fig. 3 also confirms the conjecture.

Multi-branch fusion
Using PIk-means proposed in the previous section, we get ğ‘˜ clusters. And transfer learning is performed on these clusters to get ğ‘˜ base trackers. Although the tracking ability of these base trackers is weaker than that of the original SiamFC, they are divergent and have specific preferences for different categories of target objects. In order to integrate these diverse base trackers to get a strong tracker, we design a cluster weight module (CW), which adaptively assigns appropriate weights to each base tracker by the semantic information of the target.

Although visual tracking is a complex problem, the semantic information of the tracked object does not change during the tracking process, and the background information only changes within a specific range. The target object of a specific category in the natural environment always correlates to background and interferences with specific distribution patterns. For example, the main disturbances of fish swimming in the water are water grass, rocks, other fish, etc.; the main disturbances of cars on the road are other vehicles, road signs, bridge shadows, etc. Since there is a specific distribution relationship among the semantic information of the target, the tracking scenes and distractors, and each base tracker prefers specific classes of targets. Therefore, we can design a cluster weight module (CW) that takes the target's features as input, and its output assigns corresponding weights to each base tracker.

As shown in Fig. 4, the CW module first extracts the semantic information of the target object by CNN. Then the weights of each base tracker are obtained by regression of the semantic information of the target object with a fully connected layer. Specifically, the CW module takes the target under conv2 as input and then passes it through three convolutional layers. After regularization and rectified linear unit (ReLU) activation in each convolution layer, a vector of dimension 1Ã—1024 is obtained, which contains the semantic information of the target object. Then, the vector is input into the full connection layer of 1024Ã—k for regression, and the regression result is made to meet the probability distribution by SoftMax function. Finally, the CW module outputs k weights satisfying the probability distribution, which are weights on the response maps of each base tracker and then accumulates these weighted response maps to get the final response map.

Fig. 4
figure 4
CW weights and the success rate of each base tracker on seven challenging video sequences

Full size image
Figure 4 gives the tracking success rate of each base tracker under some video sequences, as well as the weight assigned by the CW module. From the success rate of the base tracker, it can be seen that the tracking results of different sub-trackers under the same video sequence have significant differences, which makes it possible to build a powerful integrated tracker. It can be seen from the weights assigned to each base tracker by CW module that the CW module is able to assign appropriate weights to each base tracker based on the semantic information of the target object.

The CW module is trained on the network structure shown in Fig. 5. During the tracking process, the weights of shared branches and base trackers are fixed. The loss function under each base tracker is obtained by logistic regression loss function of the base tracker response map ğ‘€ğ‘–âˆˆ{ğ‘€1,ğ‘€2,â€¦,ğ‘€ğ‘˜} and the desired response map ğ‘€ğ‘”ğ‘¡:

â„“ğ‘ğ‘–=â„“(ğ‘€ğ‘–,ğ‘€ğ‘”ğ‘¡)=1|ğ‘€ğ‘–|âˆ‘ğ‘,ğ‘âˆˆğ‘¢ğ‘¢log(1+exp(âˆ’ğ‘€ğ‘–(ğ‘,ğ‘)Ã—ğ‘€ğ‘”ğ‘¡(ğ‘¥,ğ‘¦)))
(5)
where ğ® denotes the coordinates set of all pixels in the response map, and (ğ‘,ğ‘) denotes the coordinate in the response map. |ğ‘€ğ‘–| denotes the number of pixels in the response map ğ‘€ğ‘– . The value of true label ğ‘€ğ‘”ğ‘¡(ğ‘,ğ‘)âˆˆ{1,âˆ’1} is determined by the Euclidean distance between the location of the pixel point (ğ‘,ğ‘) and the center coordinate of the response map (ğ‘ğ‘,ğ‘ğ‘).

ğ‘€ğ‘”ğ‘¡(ğ‘,ğ‘)={1,(ğ‘âˆ’ğ‘ğ‘)2+(ğ‘âˆ’ğ‘ğ‘)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš<thr10,(ğ‘âˆ’ğ‘ğ‘)2+(ğ‘âˆ’ğ‘ğ‘)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšâ‰¥thr1
(6)
where ğ‘¡â„ğ‘Ÿ1 is the distance threshold. On ğ‘˜ base tracker's response maps, we obtain ğ‘˜ logistic regression loss â„“ğ‘={â„“ğ‘1,â€¦,â„“ğ‘ğ‘˜}. Based on these regression losses, the label of CW module is constructed.

ğ‘¤ğ‘”ğ‘–={1,â„“ğ‘ğ‘–=minâ„“ğ‘0,otherwise
(7)
Fig. 5
figure 5
Flowchart of ESiamFC during the inference phase

Full size image
The ğ‘¤ğ‘”ğ‘– under all response maps are stacked as a vector ğ°ğ =[ğ‘¤ğ‘”1,ğ‘¤ğ‘”2,â€¦,ğ‘¤ğ‘”ğ‘˜], which is used as the output label of CW module. We take the mean square error of CW module's output ğœğ° and ğ°ğ  as the loss function of CW module, then the loss function of CW module is:

â„“ğ‘ğ‘¤=MSE(ğœğ°,ğ°ğ )
(8)
In which the MSE(â‹…) denotes the mean square error. During the training process, the CW module is optimized iteratively by SGD directly on the whole training set. Algorithm 2 provides a brief outline of the training phase of ESiamFC.

figure b
Inference phase
The inference phase consists of two steps: Initialization and Tracking. A detailed flow chart of the inference phase is given in Fig. 5. In the initialization step, model parameters of ESiamFC are initialized according to the given target object. In the tracking step, the tracker tracks the target in subsequent frames and gives the positions of the target object. The inference phase of ESiamFC is specified as below.

In the Initialization step, the sample image ğ‘§ is cropped from the first frame ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’1 according to the bounding box ğ‘ğ‘ğ‘œğ‘¥1 of the given target object. Then, ğ‘§ is passed through the CW module and the template branch of each base tracker to obtain ğ‘˜ template feature maps {ğœ‘1(ğ‘§),ğœ‘2(ğ‘§),â€¦,ğœ‘ğ‘˜(ğ‘§)} and k weights {ğ‘ğ‘¤1,ğ‘ğ‘¤2,â€¦,ğ‘ğ‘¤ğ‘˜}.

In the Tracking step, firstly, for a given image ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ğ‘– of the i-th frame, the detection images ğ‘¥1,ğ‘¥2,ğ‘¥3 are cropped in 3 scales according to the position where the target object is located in the previous frame. Secondly, the detection image ğ‘¥ğ‘– is input into the detection branch of each base tracker to obtain the detection feature maps {ğœ‘1(ğ‘¥ğ‘–),ğœ‘2(ğ‘¥ğ‘–),â€¦,ğœ‘ğ‘˜(ğ‘¥ğ‘–)}. Thirdly, the response maps {ğ‘€ğ‘–,1,ğ‘€ğ‘–,2,â€¦,ğ‘€ğ‘–,ğ‘˜} of all base trackers at this scale are obtained by Eq. (1), then the final response map in this scale is:

ğ‘€ğ‘“ğ‘–=âˆ‘ğ‘—=1ğ‘˜ğ‘€ğ‘–,ğ‘—,ğ‘ğ‘¤ğ‘—
(9)
Accordingly, we obtain three final response maps ğ‘€ğ‘“1,ğ‘€ğ‘“2,ğ‘€ğ‘“3 under three scales. Finally, the targetâ€™s bounding box ğ‘ğ‘ğ‘œğ‘¥ğ‘– in ğ‘“ğ‘Ÿğ‘ğ‘šğ‘’ğ‘– is determined based on the position and scale corresponding to the maximum response score of the final response maps.

Experiments
Experiment setup
The performance of ESiamFC is comprehensively evaluated on three benchmarks: TColor128, DTB70, LaSOT. TColor128 contains newly 78 color video sequences over the 50 video sequences of OTB50. Various circumstances such as railway stations, airports, highways, etc. are involved, largely increasing the diversity and difficulty over the OTB50. DTB70 contains 70 diverse video sequences captured by drone cameras, emphasizing the tracking problems arising from the changing geometric relationship between the imaging device and the tracked target in different observation scenarios. LaSOT focuses on long-term tracking and is the largest target tracking benchmark as we know. LaSOT contains 1400 video sequences of 70 categories, with an average video sequence length of 2500 frames, enabling a comprehensive description of the trackerâ€™s comprehensive metrics in all aspects.

In the following, all trackers are evaluated by the precision plots and success plots in one-pass evaluation (OPE). The precision plot computes the percentage of frames where the center location error (CLE) is within a given threshold distance of the ground-truth positions. Given the tracked bounding box ğ‘Ÿğ‘¡ and the ground-truth bounding box ğ‘Ÿ0 of the target object, the success plot is defined as the overlap score calculated by |ğ‘Ÿğ‘¡âˆ©ğ‘Ÿ0|/|ğ‘Ÿğ‘¡âˆªğ‘Ÿ0|.

Implementation details and parameters
Network structure: Similar to SiamFC, ESiamFC uses AlexNet as the backbone for feature extraction. MDNet and SiamMask show that there are still many common characteristics among different categories of data, so as shown in Fig. 5, we fix the conv1 and conv2 layers of AlexNet as the shared layers of ESiamFC to extract common features of the target objects. The transfer learning is implemented for conv3, conv4, and conv5 under each sample set to obtain k base trackers.

Training setting: ILSVRC15 is selected as the training set, which is divided into training set and testing set according to the ratio of 9:1 to observe the fitting degree of data during the training process. In each training epoch of base trackers, 53,200 sample pairs are randomly selected for training. The batch size is set to 64, and 50 epochs are trained in total. The training process is carried out by using SGD, and the learning rate is set to 1e-3. Each base tracker is trained independently, and the weights of conv1 and conv2 are fixed during the training process. The network weights of each base tracker are fixed during the training process of the CW module, and the loss function is Eq. (8). The learning rate is set to 1e-4 with batches of size 32. A total of 50 epochs are trained, and the network is fitted using SGD.

Hyperparameter setting: For PIk-means, we use elbow method to determine the value of ğ‘˜. A significant inflection point occurs when ğ‘˜ is 6â€“8, considering the computational cost, the number of ğ‘˜ selected in this paper is 6. We set the proportional control coefficient to ğ‘ƒ=0.005, and the integral control coefficient is set to ğ¼=0.003. Consistent with SiamFC, we search for the target object on three scales, and the search scale step is set to 1.0375. The trackers only take the first frame as sample and make no update.

The tracker proposed in this paper is implemented in PyCharm platform through PyTorch tool. All experiments are done on the GOT10k toolbox, and the results of other tracker under LaSOT benchmark are officially given by LaSOT. The proposed method is trained and tested on i7 9700 k CPU and RTX2080Ti GPU. The source code of the algorithm in this paper is available at https://github.com/conquerhuang/ESiamFC.

TColor128 dataset
We provide a comparison of the proposed ESiamFC with 9 representative trackers: Staple [58], BACF [59], CF2, CSR_DCF, DSST, KCF, SRDCF, SiamFC, SRDCFdecon [60]. Among them, CF2, CSR_DCF, SiamFC are deep learning-based tracker; Staple uses complementary features and different update rates to achieve long-term tracking; BACF, SRDCFdecom are correlation filter-based tracker.

Figure 6 shows the success plot and accuracy plot of OPE on TColor128 benchmark. Among all trackers, our ESiamFC provides the best performance in both plots and with a significantly improves by 3.5%, 4.0% over the base tracker SiamFC on average success rate and average precision. In order to make a clear comparison, we perform an attributes-based analysis of our approach on TColor128, which is shown in Tables 1 and 2.

Fig. 6
figure 6
Success plot and precision plot for TColor128

Full size image
Table 1 Average precision of ESiamFC and other 9 trackers on 11 attributes over all 128 sequences
Full size table
Table 2 Average success rate of ESiamFC and other 9 trackers on 11 attributes over all 128 sequences
Full size table
To enable further performance analysis of trackers, TColor128 labels each sequence with 14 attributes, namely illumination variation (IV), out-of-plane rotation (OPR), scale variation (SV), occlusion (OCC), deformation (DEF), motion blur (MB), fast motion (FM), in-plane rotation (IPR), out of view (OV), background clutter (BC), and low resolution (LR). The average accuracy and average success rate of the 10 trackers used for the evaluation under these 11 attributes are given in Tables 1 and 2, respectively. In comparison, our ESiamFC achieves the highest average precision and average success rate in 6 challenges attributes. Thanks to the representation ability of CNN on high-dimensional semantic information, deep feature-based trackers provide strong robustness to rotation change, so CF2 and ESiamFC achieve good results on both OPR and IPR. Since ESiamFC only uses the first frame as a template, it can perfectly avoid model degradation when there are scenarios such as occlusion during tracking. Therefore, ESiamFC is far better than other trackers under the challenge of OCC and achieves a gain of 5.5%, 3.9% on average precision and success rate, respectively, compared to the second-best methods. Specifically, our ESiamFC applies ensemble learning to further improve the discrimination ability of the SiamFC in embedded space. We obtain 6.6% and 5.1% improvement over SiamFC in terms of precision score and success rate, respectively, in BC challenges. Overall, the proposed ESiamFC achieves great improvement on baseline SiamFC and competitive with state-of-art trackers.

DTB70 benchmark
We further evaluate the proposed ESiamFC against 13 representative trackers: Staple, BACF, DeepSTRCF [61], CF2, CSR_DCF, DSST, KCF, ECO_HC, ECO, SRDCF, SiamFC, MDNet, SRDCFdecon on DTB70 benchmark. Among them, DeepSTRCF is a deep feature-based correlation tracker with spatialâ€“temporal regularization to suppress the modelâ€™s coefficients; ECO implicitly encodes the spatial discrete deep features into a continuous spatial domain and uses sparse updates to improve the accuracy and efficiency; MDNet is an end-to-end deep structure tracker that uses a multi-domain learning approach to learn a general representation to fit in visual tracking.

Figure 7 shows the average precision and average success rate of the 13 trackers used for comparison on the DTB70 benchmark. It can be seen from the figure that ESiamFC on DTB70 has improved 4.6% and 4.1% in the average precision and average success rate, respectively, compared to SiamFC. On DTB70 benchmark with 11 attributes, ESiamFC takes first place on 9 attributes in terms of average precision score and first place in average success rate on 10 attributes. Specially, in IV and FM attributes, which present significant challenges to the discrimination ability of the tracker, ESiamFC achieves 7.0% and 6.2% improvement in average precision score than SiamFC. Overall, our ESiamFC can easily cope with complex changing scenarios such as illumination variation and fast motion. The excellent performance of ESiamFC in IV and FM can be attributed to the introduce of ensemble learning and CW fusion strategy.

Fig. 7
figure 7
Attribute-based evaluation of the proposed tracker against 11 state-of-art trackers on DTB70 benchmark

Full size image
LaSOT benchmark
LaSOT contains two protocols, and protocol 1 uses all 1400 sequences to evaluate tracking performance. Protocol 2 splits LaSOT into training and testing subsets, and the testing set consists of 280 sequences for tracking evaluation. On these two protocols we compare ESiamFC with 19 representative trackers including ASRCF, BACF, CFNet, CN, CSK, CSR_DCF, DSST, ECO_HC, fDSST, KCF, MEEM, PTAV, SAMF [62], SiamFC, SINT, SRDCF, Staple D-STRCF, SiamDW [34]. Since LaSOT does not give tracking results of SiamDW and D-STRCF on protocol 1, the three scores for these two trackers in protocol 1 are null.

Table 3 shows average precision, average success rate and success rate at the overlap threshold of 0.5 on LaSOT. It clearly illustrates that our tracker achieves the highest score under all three evaluation methodologies on both protocols. Compared with the baseline tracker SiamFC, the average precision score and the average success rate under protocol 2 have relatively gained 3.3% and 6.7%, respectively. For detailed performance analysis, we also report the results on various challenges in LaSOT. In detail LaSOT contains a total of 14 attributes. Unlike TColor128 and DTB70, LaSOT divides occlusion (OCC) into partial occlusion (POC) and full occlusion (FOC), combines in-plane rotation (IPR) and out-of-plane rotation into rotation ROT, and additionally introduces three new challenges: abrupt motion of the camera (CM), viewpoint affects target appearance significantly (VC), and aspect ratio changing (ARC). From Table 4, it can be seen that our ESiamFC obtains the top two rankings under 9 attributes and top 3 rankings under 13 attributes. Specially, in IV and FM attributes, ESiamFC shows a relative increase of 9.7% and 22.3% compared to SiamFC. In BC challenge, which requires higher discrimination ability to distractors, ESiamFC relatively improves by 5.2% compared to SiamFC. In general, our ESiamFC achieves significant improvement over the baseline tracker SiamFC and performs as a counterpart with other state-of-art trackers.

Table 3 Performance comparisons on LaSOT
Full size table
Table 4 Average success rate of 20 trackers under LaSOT protocol 2 on 14 attributes
Full size table
Ablation research
This section analyzes the sample size distribution and sum error distribution of clustering results. To verify the validity of the clustering results, we use k-means and PIk-means to cluster the video feature data set 2000 times and analyze the distribution of the clustering results. In order to ensure the reliability of the results, the two clustering algorithms adopt the same random sample for initialization in each clustering process. As shown in Fig. 8, the sample numbers of each cluster in the PIk-means clustering results is close to each other, while the sample number of each cluster in the traditional k-means is clustered at 1000, 3000, 5000. This confirms the effectiveness of our proposed PIk-means. From the sum error distribution of the clustering results, we can see that the traditional k-means clustering results sum error is distributed over a large interval range, while the PIk-means clustering results sum error converges to a smaller interval. This indicates that the traditional k-means is badly affected by the initialization method, and the sum errors of clustering results distribute over a large range. In contrast, the PIk-means is less affected by the initialization method and always converges to a specific set of solutions. It can be seen from the sum errors of the clustering results that the sum errors of the PIk-means are smaller than that of the traditional method. We attribute this improvement to the introduction of PI control in the distance function. Although it increases on the sum error of local clustering results during the iterative process, this increase in sum error gives the clustering algorithm the ability to jump out of the optimal local solution, making the final clustering results converge to a better range.

Fig. 8
figure 8
Cluster sample size distribution, sum error distribution, JC, RI, FMI distribution of clustering result

Full size image
JC, RI, FMI analysis: To quantitative analysis the convergence of the two clustering methods. We analyze three validity indexes: Jaccard coefficient (JC), FM index (FMI), and Rand index (RI). These validity indexes are used to measure the similarity of the two results. The same dataset is clustered twice, for which the sample pair (ğ¹ğ‘,ğ¹ğ‘) consists of two samples ğ¹ğ‘, ğ¹ğ‘. If ğ¹ğ‘ and ğ¹ğ‘ are in the same cluster in both clustering results, this pair belongs to the set SS SS. If ğ¹ğ‘ and ğ¹ğ‘ are in the same cluster for the first time and in the different cluster for the second time, this pair belongs to the set ğ’ğƒ, if ğ¹ğ‘ and ğ¹ğ‘ in different cluster for the first time and in the same cluster for the second time, this pair belongs to the set ğƒğ’. If ğ¹ğ‘ and ğ¹ğ‘ in different cluster for both times, this pair belongs to the set ğƒğƒ. The number of samples in the sets ğ’ğ’, ğ’ğƒ, ğƒğ’, ğƒğƒ are a, b, c, d, respectively, then we have:

ğ½ğ¶=ğ‘ğ‘ğ‘ğ‘+ğ‘ğ‘+ğ‘ğ‘,ğ¹ğ‘€ğ¼=ğ‘ğ‘ğ‘ğ‘+ğ‘ğ‘â‹…ğ‘ğ‘ğ‘ğ‘+ğ‘ğ‘â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš,ğ‘…ğ¼=2(ğ‘ğ‘+ğ‘‘ğ‘‘)ğ‘›ğ‘›(ğ‘›ğ‘›âˆ’1)
(10)
where ğ‘› denotes the total number of samples. The values of these validity indexes are on the interval [0,1] [0,1], and the larger they are the more similar the two clustering results is. The distribution of each validity index is illustrated in Fig. 8. It clearly illustrates that our PIk-means has better convergence than that of k-means.

CW module analysis: Fig. 9 shows the precision and success plot of ESiamFC, SiamFC, and each base tracker on LaSOT protocol 2. It can be seen from the figure that the precision of base trackers learned from the data set corresponding to each cluster is somewhat lower than that of the original SiamFC. As can be seen from Tables 5 and 6, although the base trackersâ€™ precision and success rate of the base trackers decrease, each base tracker has outstanding performance in specific attributes. After integrating the base trackers through the CW module, the ESiamFC achieves first place on most attributes. This indicates that our CW module can integrate these diversity base trackers into one robust tracker.

Fig. 9
figure 9
Precision plot and success plot of ESiamFC, SiamFC and base trackers on LaSOT benchmark under protocol 2

Full size image
Table 5 Precision score of ESiamFC, SiamFC and base trackers within the location error threshold of 20 pixels on LaSOT benchmark under protocol 2
Full size table
Table 6 Success rate of ESiamFC, SiamFC and base trackers within the overlap threshold of 0.5 on LaSOT benchmark under protocol 2
Full size table
Qualitative evaluation
To visualize the difference among the trackers for comparison more intuitively, Fig. 10 gives a qualitative comparison on 12 challenging video sequences. Those sequences are BMX4, SnowBoarding4, Horse1, Bustation_ce1, MountainBike5, Girlmov, RcCar4, Kobe_ce, Matrix, Motorbike_ce, MotorRolling, Railwaystation. Different trackers are marked with rectangular boxes of different colors, only the top 7 trackers are shown for clarity. Our proposed ESiamFC has a good performance in the video sequences with multiple challenges.

Fig. 10
figure 10
Qualitative evaluation of proposed method with comparison to 6 trackers on 12 challenging video sequences

Full size image
Occlusion In Girlmov, Bustation_ce1, Railwaystation, and other 8 video sequences, occlusion is caused by the targetâ€™s motion or distractors. In Bustation_ce1, the occlusions and the target object belong to the same class. Trackers ECO, DeepSTRCF, and MDNet use the object detection network for feature extraction failed to track the target object since the same target object class shares the same category label during the training process. Thanks to the further discrimination of similar objects in each base tracker, the proposed ESiamFC can track the target among distractors with similar semantics as the target.

Motion blur Motion blur exists in the captured images due to the fast movements of the camera or target. Motion blur causes the features of the target object to be less distinct, which in turn drafts the tracker to distractors with similar features to the target in the background. In Kobe_ce, all trackers except ESiamFC are drawn to similar athletes in the background due to motion blur caused by fast motion. ECO, CSR_DCF, DeepSTRCF introduce an online template update mechanism, through which the tracker can adapt to the target's appearance changes online. However, when distractors attract the tracker, the update mechanism tends to introduce contaminated samples to cause model degradation. ESiamFC and SiamFC, for which no model update is performed avoid this degradation problem. Though SiamFC avoids the model degradation problem, it is difficult to re-track the target after being attracted by distractors due to its weak discrimination ability. This indicates that the proposed ESiamFC has a stronger discrimination capability and can robustly cope with the motion blur problem.

Rotation In the Girlmov sequence, the target undergoes the challenge of out-of-plane rotation when the girl turns around, and the viewpoint changes from front to side. All trackers other than ESiamFC are attracted to distractors in the background that are similar to the positive side of the target, leading to tracking drift. In the MotorRolling sequence, the target undergoes in-plane rotation. The tracker's robustness to the rotational challenge lies in its feature representation ability. The features of ECO, DeepSTRCF, and CSR_DCF are robust to rotational changes. However, their discrimination for intra-class targets depends on the learned correlation filters, which significantly limits their robustness under rotational challenges. The proposed ESiamFC takes video sequences as training samples enabling strong discrimination ability for intra-class targets, and thanks to the integration of multiple diverse base trackers, it can easily cope with out-of-plane rotating challenge compared with SiamFC. In summary, our proposed tracker successfully overcomes this challenge and tracks the target stably. This indicates that the proposed ESiamFC achieves a robust target representation.

Conclusion and further work
In this article, we propose an ensemble Siamese networks (ESiamFC) for tracking. During the offline training process, we use PIk-means to cluster the training dataset in the embedded space into ğ‘˜ subsets and get k k k diverse base trackers with their own preference. Then a CW fusion module is then proposed which can adaptively assign weights to integrate base trackers into a strong tracker. Extensive experiments on TColor128, DTB70, and LaSOT demonstrate that this ensemble strategy can significant boost the tracker's discriminative power and performs favorably against the state-of-art trackers in terms of accuracy and robustness.