The emerging recurrent visual attention models mostly utilize a sensor to continuously capture features from the input, which requires a suited design for the sensor. Researchers usually need a number of attempts to determine optimal structures for the sensor and corresponding modules. In this work, an adaptive multi-sensor visual attention model (AM-MA) is proposed to enhance the recurrent visual attention model. The proposed model uses several sensors to observe the original input recurrently, while the number of sensors can be added adaptively. Each sensor generates a hidden state and is followed by a location network to provide the deployment scheme. We design a self-evaluation mechanism for AM-MA, by which it can decide whether to add new sensors during training. Besides, the proposed AM-MA leverages a fine-tune mechanism to avoid a lengthy training process. AM-MA is a parameter-insensitive model. That is, there is no need for researchers to pre-train the model for finding the optimal structure in the case of unknown complexity. Experimental results show that the proposed AM-MA not only outperforms the renowned sensor-based attention model on image classification tasks, but also achieves satisfactory results when given an inappropriate structure.

Introduction
The attention mechanism [24] is imitating the characteristics of human perception, which is concentrating attention on certain parts selectively and over time combining the information of different fixations to obtain information when needed. In the field of computer vision, attention mechanisms can be realized in various forms, which can be roughly divided into soft attention [25] and hard attention [21]. Most research on deep learning just focuses on soft attention mechanisms, in which the parameters for the attention module can be updated by backpropagation directly. Different from soft attention, the existed hard attention modules are generally non-differentiable. When introducing hard attention methods into supervised learning and unsupervised learning, extra technologies such as reinforcement learning are required to update hard attention modules.

The emerging attention mechanism greatly improves machine learning technologies. For example, the deep attention selective network (dasNet) [23] dynamically alters convolutional filter sensitivities on classification tasks and allows the network to focus its internal attention iteratively on some of the filters, and it significantly enhances deep convolutional neural network (CNN) on classification performance [10]. Xiao et al. [27] proposed a two-level attention model in CNN, which can apply visual attention to fine-grained classification tasks. Not only in the field of image processing, the attention mechanism still performs well after being introduced into the field of natural language processing [1]. Kumar et al. [8] introduced the dynamic memory network into the question answering (QA) problems, and the end-to-end trained model obtains state-of-the-art results on several types of tasks and datasets. Peng et al. [33] improved bidirectional long short-term memory (Bi-LSTM) through an additional attention layer, by which the long-term preference can be captured more efficiently. Ji et al. [7] proposed a cross-attention based encoder-decoder model under the Siamese framework (CASNet) to preserve the saliency correlation and improve intraframe salient detection consistency for video salient object detection.

With the attention mechanism being applied to many scenarios of machine learning, more and more attention models have been proposed for different tasks [11]. For example, the stacked attention model, as its name implies, stacks multiple attention models and takes the output of the previous attention model as the input of the next attention model, and it is proposed for image-text matching [9]. To process high-resolution images, the multi-attention convolutional neural network (MA-CNN) also performs well for fine-grained image recognition [32]. As a typical work of visual attention, the recurrent attention model (RAM) proposed by Google Deepmind [15] uses a recurrent neural network (RNN) to process inputs sequentially, attends to different positions in an image one at a time, and combines information from these fixations to build up a dynamic internal representation incrementally. The whole model consists of three small networks: the glimpse network, the core network, and the action network. Several experiments show that it significantly performs better than a convolutional neural network on cluttered images, and it can learn to track a simple object without an explicit training signal on the agent system with visual information [3, 12].

The existing visual attention models are hard to be applied to complex image processing tasks, because most of them are parameter-sensitive. That is, their structures cannot be too deep or shallow. A too deep structure may result in a redundant training process, while a too shallow structure cannot efficiently capture the features. For this problem, a researcher needs to attempt lots of times to design the structure for an attention model. When designing a large-scale end-to-end model for some specific scenarios, attention models are usually embedded together with other modules [6, 34], and the cost for these attempts is unaffordable. Although researchers can give a feasible design according to their experiences, the attention mechanism community still needs an adaptive model for tasks with unknown complexities.

In this work, an adaptive multi-sensor visual attention model (AM-MA) is proposed. The AM-MA can constantly increase the number of sensors to extract features from the input. Each sensor has its private glimpse network, core network, and location network. The glimpse network transforms the perception of the sensor into retina-like representations, and these retina-like representations are fed to the core networks to generate latent states. The location network is used to deploy the sensor to get recurrent glimpses. At the end of the AM-MA, an action network is built for further reasoning. A self-evaluation mechanism is proposed for AM-MA to add sensors adaptively. If the model adds a sensor, the corresponding networks are constructed simultaneously. To avoid a lengthy training process, we leverage a fine-tune process to build networks for the added sensor [18]. The model is parameter-insensitive. In the case of unknown complexity, AM-MA has the ability to adjust its number of sensors to extract features more efficiently.

The main contributions of this paper are summarized as follows:

This work proposes an adaptive attention model, which needs no pre-training and experience to determine its structure. The self-evaluation mechanism provides a feasible direction for designing parameter-insensitive attention models.

To avoid redundant training after adding a new sensor, we leverage a fine-tune process to initialize the new sensor and its corresponding networks. With that, the training process for AM-MA can keep stabilization.

We compare the proposed AM-MA and baselines on two datasets and investigate the adaptability of our model. We analyze AM-MA‚Äôs superiority in both theory and experiment. Then, we conduct several ablation experiments to validate our model. The experimental results show that the proposed AM-MA can capture the features more effectively than the other sensor-based models.

The remainder of this paper is organized as follows: In the second section, we present an overview of previous research related to our work. Next, in the third section, we describe our model in detail. The fourth section interprets and compares the results of the experiments. Finally, we conclude in the fifth section and give directions for future research.

Related work
Most of the research on the combination of deep learning and visual attention mechanism has focused on using masks to form attention. The principle of the mask is identifying the key features for the image data through a set of new weights [26]. In training, a deep neural network with visual attention mechanism learns to select the regions that need attention. This idea has evolved into two different types: soft attention and hard attention. The key point of soft attention is that the model learns alignment weights, and these weights are placed over the patches in the source image [1]. The soft attention is differentiable, so the parameters of the attention module can be learned by backpropagation directly [31]. The hard attention mechanism only chooses one image block at a time, which is a random prediction process that emphasizes dynamic change over the image. Hard attention has less computation than soft attention, but the model is not differentiable and requires more complex technology for training, such as reinforcement learning [13].

Fig. 1
figure 1
The structure of RAM. In ùúå(ùë•ùë°,ùëôùë°‚àí1), the region with a darker color has a higher resolution. The ùëôùë° and ùëéùë° are output as parameterized distribution

Full size image
A variety of visual attention models have been proposed, and these works introduced attention mechanisms into the field of computer vision in different ways [28]. The RAM is one of the most famous visual attention models. It is proposed for the sequential decision process of a goal-directed agent interacting with a visual environment. As shown in Fig. 1, at each time step, the agent only observes a part of the input through a sensor and extracts multiple regions with different resolutions. Although the information available at each time is limited, the agent can recurrently observe the environment by deploying the coordinate of the sensor. When the agent decides to stop the glimpse, the sensor is no longer deployed, and the information processed through RNN is regarded as the internal state of the agent. In this model, the agent needs to integrate information over time to determine how to act and how to deploy its sensor more effectively. At each time step, the agent receives a reward from the environment, and the goal of the agent is to maximize the sum of these rewards. The model can be used for object detection and visual control, and it has been proven to perform well in these tasks [15].

Although RAM has proven its worth in many scenarios, a problem remains: For different tasks, researchers need to constantly adjust the setting for sensors before the training process because the final result is highly correlated with the setting. This problem results in unaffordable computational costs. For other more complex sensor-based models [16], the models‚Äô performances are more sensitive to the design of the sensor, so that the hard attention community is in sore need of high-adaptability models.

Adaptive multi-sensor visual attention model
This work proposes a parameter-insensitive attention model, adaptive multi-sensor visual attention model (AM-MA), to enhance the visual attention model. In this section, we first give the framework of the proposed AM-MA, and then, the self-evaluation mechanism and fine-tune process for AM-MA are described. Finally, we give a concrete introduction to the training for AM-MA.

AM-MA framework
The proposed AM-MA is designed to improve the visual attention model. AM-MA can not only capture the features of the input more efficiently by using several sensors to extract retina-like representation cooperatively, but also add sensors adaptively so that it needs no pre-training to look for optimal parameters and structure. For the added sensor, its networks‚Äô parameters are initialized by a fine-tune process to avoid the lengthy training caused by the incessant change of network structure.

Figure 2 shows the structure of AM-MA, in which multiple smaller sensors are used to extract features cooperatively. At each time step, these sensors glimpse the input x at the same time. For the i-th sensor, ùëôùëñùë°‚àí1 is the center of its perception. As shown in Fig. 3, the retina-like representation extracted by the i-th sensor is ùúåùëñ(ùë•,ùëôùëñùë°‚àí1), which contains several regions with different resolutions. Then, ùúåùëñ(ùë•,ùëôùëñùë°‚àí1) is fed to the glimpse network ùëìùëñùëî(‚ãÖ), which converts it into a feature vector ùëîùëñùë°.

Fig. 2
figure 2
The structure of AM-MA, which has three sensors in model now, and each sensor glimpse input image with three regions in different resolutions

Full size image
Fig. 3
figure 3
The framework of a glimpse sensor in AM-MA. In retina-like representation ùúå(ùë•ùë°,ùëôùë°‚àí1), the darker color region has, the higher its resolution is

Full size image
In this work, the followed core networks are different from RNN. Each internal state ‚Ñéùëñùë° output through ùëìùëñùëê(‚ãÖ;ùúÉùëñùëê) is processed by a linear layer, and then, all internal states are integrated by a full-connected layer, which produces a hidden state ‚Ñéùë°. For each core network, the hidden state at time ùë°‚àí1 is fed to it:

‚Ñéùëñùë°=ùëìùëñùëê(‚Ñéùë°‚àí1,ùëîùë°;ùúÉùëñùëê),‚Ñéùë°=Rect(‚àëùëñ Linear (‚Ñéùëñùë°)).
(1)
That is designed to share information among sensors, by which different sensors can avoid glimpse the same region. The location networks are used to deploy the sensors, and it is fed with ‚Ñéùë°. The i-th location network provides the i-th sensor with the coordinate to glimpse recurrently, and it also determines whether to stop the glimpse for the i-th sensor.

In the design for the action networks, each sensor does not correspond to an independent action network as the design for the core network and location network. A full-connected network is designed as action work ùëìùëé(‚ãÖ;ùúÉùëé), and the input of this network is the ‚Ñéùë° mentioned above. The action network is used for further reasoning, which is the classification layer in image classification tasks and action selection in a deep reinforcement learning scenario.

Self-evaluation
The proposed AM-MA can adaptively add sensors for gaining better features, which happens when the model enters the flat area of the error curved surface or the result has no improvement for a long time. Then, the model creates an experience batch to record the output of the location networks every time the sensors stop glimpse.

The output of the location network ùëìùëñùëô(‚ãÖ;ùúÉùëñùëô) is divided into two parts, one is a parameterized distribution to deploy the i-th sensor at time ùë°:ùëôùëñùë°‚àºùëù(‚ãÖ‚à£ùëìùëñùëô(‚Ñéùë°;ùúÉùëñùëô)), the other one is an independent Boolean ùë†ùëñùë° processed by threshold function. And if there are several sensors exist, the multiple sensors proactively stop glimpse in the case of ùë†1ùë°=ùë†2ùë°=‚ãØ=ùë†ùëõùë°=1 or they have glimpsed the maximum number of times, where n is the current number of sensors. When the multiple sensors stop glimpse, the experience batch stores a tuple <‚Ñéùë°,ùëô1ùë°,ùë†1ùë°,ùëô2ùë°,ùë†2ùë°,‚Ä¶,ùëôùëõùë°,ùë†ùëõùë°>.

Two conditions are required for adding a new sensor:

The multiple sensors always glimpse the maximum number of times, which is consistent with Eq. (2):

1ùëö‚àëùëó=1ùëö(‚àèùëñ=1ùëõùë†ùëñùëó)‚â§ùúÄ,ùúÄ‚àà[0,1],
(2)
where m is the number of tuples in the experience batch, and the larger ùúÄ is used to control the strictness for the first condition. different values of ùúÄ impact how fast a new sensor will be added, but the final results will not be impacted.

There is a sensor i whose ùëôùëñùëó always has multiple local maximums in each tuple. The local maximum in ùëôùë° and the highlight coordinate are defined below.

The set of coordinates which a sensor can be located in for the next glimpse is ùê∂= {ùëê1,ùëê2,‚Ä¶,ùëêùëö}. The neighbor coordinates of ùëêùëó is defined as ùëê¬Øùëó, which contains all coordinates adjacent to ùëêùëó. For each coordinate, it has four neighbor coordinates around it, and all its neighbor coordinates are not adjacent to each other. Make ùëôùëñùë°(ùëó)=ùëù(ùëêùëó‚à£ùëìùëñùëô(‚Ñéùë°;ùúÉùëñùëô)) denotes the probability of deploying the i-th sensor on ùëêùëó, ùëù¬Øùëñ=1ùëö‚ãÖ‚àëùëóùëôùëñùë°(ùëó). When ùëêùëó meets the conditions that ‚àÄùëêùëò‚ààùëê¬Øùëó,ùëôùëñùë°(ùëò)‚â§ùëôùëñùë°(ùëó) and [ùëôùëñùë°(ùëó)‚àíùëù¬Øùëñ]2>1ùëö‚ãÖ‚àëùëê‚Ñé‚ààùëê¬Øùëó(ùëôùëñùë°(‚Ñé)‚àíùëù¬Øùëñ)2,ùëêùëñ can be called as one of the highlight coordinates. The former means a highlight coordinate has higher probability than its neighbor coordinates, while the latter ensures the highlight coordinate has a high enough probability.

Fig. 4
figure 4
In a, the agent is learning to play Atari with AM-MA, and there are two sensors in the model. The probabilities of coordinates are represented by columns. In b AM-MA is used to classify images. There is only one sensor in the model, but it is difficult for a single sensor to observe enough information while the number of glimpses is limited, so its deployment has two highlight coordinates marked red (colour figure online)

Full size image
If the state space is small or the input image is simple, the policy learned by the location networks is more unambiguous. Whereas, on complex tasks, the agent needs to select actions after surveying more regions, so several coordinates apparently have higher probabilities of locating the sensor than their neighbors, which is shown in Fig. 4.

Fine-tune process
Transfer learning [22] is reusing the knowledge of a trained model for new models to speed up the training. Considering that most of the data or tasks are correlated, the parameters of the trained model (which can also be understood as the knowledge learned by the model) are shared to the new model in some ways is a feasible method to accelerate and optimize the learning efficiency [18].

When using CNN to solve image processing tasks, the fine-tune process freezes partial layers in the pre-training model and only trains the remaining convolution layers or fully connected layers that are usually closed to the output layer. When the source domain and the target domain are similar, fine-tuning usually achieve excellent results by modifying only the last layer or several layers in the case that there is a small amount of data [14, 17].

When the experience batch meets the two conditions for adding a sensor, the sensor i that fulfills the second condition is selected as the prototype for the added sensor (the (ùëõ+1)-th sensor). Then, the new glimpse network, core network, and location network are constructed for the (ùëõ+1)-th sensor, and the parameters of these networks are first initialized as the same as those for the prototypical sensor:

ùúÉùëõ+1ùëî=ùúÉùëñùëî,ùúÉùëõ+1ùëê=ùúÉùëñùëê,ùúÉùëõ+1ùëô=ùúÉùëñùëô.
(3)
Then, the prototypical sensor and the added sensor change their parameters through fine-tune. This process is divided into two phases, the first is decomposing the i-th sensor‚Äôs deployment schemes ùëôùëñùë°‚àºùëù(‚ãÖ‚à£ùëìùëñùëô(‚Ñéùë°;ùúÉùëñùëô)) in the experience batch, which is named proliferation, and the other phase is to fine-tune the networks according to the results of proliferation.

Once the conditions of adding a new sensor is met, that is, for each tuple <‚Ñéùë°,ùëô1ùë°,ùë†1ùë°,ùëô2ùë°,ùë†2ùë°,‚Ä¶,ùëôùëõùë°,ùë†ùëõùë°>, there are at least two highlight coordinates in ùëôùëñùë°. If there are more than two highlight coordinates, the two highlight coordinates with maximum Euclidean distance are selected. For the two selected coordinates ùëêùëé and ùëêùëè, we suppose ùëôùëñùë°(ùëé)‚â•ùëôùëñùë°(ùëè). Then, two distributions are generated from ùëôùëñùë°, in which each one just has one highlight coordinate. Initializing ùëôùëéùë°=ùëôùëñùë° and ùëôùëèùë°=ùëôùëñùë°, we can erase the redundant highlight coordinate in each of them. The detailed process of that is shown as Algorithm 13, and ùëôùëèùë° can be obtained with the same way.

figure a
For example, the probability distribution in Fig. 4b can proliferate to generate two probability distributions ùëôùëéùë° and ùëôùëèùë°, which are shown in Fig. 5. Each distribution ùëôùëñùë° in the experience batch for the prototypical sensor is decomposed into ùëôùëéùë° and ùëôùëèùë° in this way. Then, a set ùêø={<‚Ñé1,ùëôùëé1,ùëôùëè1>,<‚Ñé2,ùëôùëé2,ùëôùëè2>,‚Ä¶} is built to fine-tune the networks for the prototypical sensor and the added sensor.

Fig. 5
figure 5
The proliferation of probability distributions in Fig. 4b. The red columns are the highlight coordinates in the original distribution

Full size image
For the prototypical sensor and the added sensor, their glimpse networks and core networks are frozen, and their location networks are updated with the set L [29]. The location network ùëìùëñùëô(ùúÉùëñùëô) for the prototypical sensor i is updated by set {<‚Ñé1,ùëôùëé1>,<‚Ñé2,ùëôùëé2>,‚Ä¶}, while the location network ùëìùëõ+1ùëô(ùúÉùëõ+1ùëô) for the added sensor is updated by set {<‚Ñé1,ùëôùëè1>,<‚Ñé2,ùëôùëè2>,‚Ä¶}. In this process, the glimpse networks and core networks for both the prototypical and added sensor are frozen, which is shown in Fig. 6. Then, a linear layer is constructed for the added sensor with the same parameters as the prototypical sensor. The original parameters of the fully connected layer for the prototypical sensor is ùë§ùëñ:

ùë§ùëñ=[ùë§ùëñ,1,ùë§ùëñ,2,‚Ä¶]ùëá.
(4)
To change the output as little as possible, the original parameters can be set proportionally to the weights of the new neurons. That is, the added sensor‚Äôs fully connected layer is constructed as:

ùë§ùëõ+1=[0.5‚àóùë§ùëñ,1,0.5‚àóùë§ùëñ,2,‚Ä¶]ùëá,
(5)
while that for the prototypical sensor is the same:

ùë§ùëñ=[0.5‚àóùë§ùëñ,1,0.5‚àóùë§ùëñ,2,‚Ä¶]ùëá.
(6)
Fig. 6
figure 6
The glimpse networks and core networks of the two sensors are frozen, and the location networks of the prototypical sensor and the added sensor are updated by backpropagation

Full size image
Training
In this work, reinforcement learning is used to train the location networks, in which the whole training process can be regarded as a multi-agent reinforcement learning.

At time t, the internal state ‚Ñéùëñùë° output by the i-th sensor‚Äôs core network summarizes information extracted from the historical perceptions and current perception. For the deployment of the multi-sensor, due to the internal state ‚Ñéùëñùë°=ùëìùëñùëê(‚Ñéùë°‚àí1,ùëîùëñùë°;ùúÉùëñùëê), and ‚Ñéùë° is the integration of all internal states, the environment is no-stability. In this work, we leverage the independent learning to train the multiple location network, that is, each location network only pays attention to its own action in training without considering the impact of other location networks. When all sensors stop glimpse after the T-th recurrence and action network output the result, a reward r is given, which is designed according to the task. For the i-th sensor, the t-th deployment strategy output by ùëìùëñùëô(‚ãÖ;ùúÉùëñùëô) is ùúãùë°ùëñ(‚Ñéùë°;ùúÉùëñùëô), and we can calculate its gradient through:

‚àáùúÉùëñùëôùêΩ(ùúÉùëñùëô)=ùê∏ùúãùëñ[‚àáùúÉùëñùëôlogùúãùë°ùëñ(‚Ñéùë°;ùúÉùëñùëô)ùõæùëá‚àíùë°ùëü],
(7)
where ùõæ is the discount factor.

Experiments
In this section, we first compare our method with the traditional sensor-based visual attention model, and then, we discuss the performance of AM-MA in the case of inapposite structure and parameters.

Datasets
We evaluate our model on two public datasets: Cifar-10 and CUB-200. The former is a simple dataset, while the latter is used to investigate our method‚Äôs performance in recognizing fine-grained images. Figure 7a shows several samples in Cifar-10, while the samples for CUB-200 are given in Fig. 7b.

Fig. 7
figure 7
Several samples in Cifar-10 and CUB200 datasets

Full size image
Cifar-10 is a typical image classification dataset that consists of 50000 training images and 10000 testing images [4]. The images in Cifar-10 are three-channel, and the size of it is 32*32. These images are divided into ten categories, of which three are 6000 images for each category.

CUB (Caltech-UCSD Birds) 200 dataset [19] contains 11,788 bird images from 200 different categories. Each image in CUB 200 provides the label of its category, the bounding box of the bird, the information for the key part of the bird, and the bird‚Äôs attribution.

Baselines
In the following paragraph, three sensor-based models are introduced and used as baselines to evaluate the proposed AM-MA model‚Äôs performance.

RAM Recurrent attention model (RAM) [15] is a typical sensor-based model with the hard attention mechanism. It use only one sensor to glimpse the input recurrently, in which the hard attention is formed by extracting retina-like representation. RAM has been widely used in various scenarios.

Saccader As a new sensor-based model, Saccader [5] leverages pre-training process to reflect the degree of relevance to the classification task. For the policy gradient optimization, it also utilities the reinforcement learning to update its parameters.

DDRAM Deep Differentiable Recurrent Attention Model (DDRAM) [20] is an end-to-end sensor-based model. It uses a two-layer recurrent neural network for learning to jointly localize and classify objects, which is a feasible substitute for the reinforcement learning processing in RAM.

Experiment setup
In these experiments, the maximum number of glimpses is set to 8. The different resolutions of regions observed by the sensor are set to 8√ó8,10√ó10 and 12√ó12, which is designed to highlight the self-adaptability more clearly. A 100 size queue is created as the experience batch to determine whether the AM-MA model meets the two conditions for creating a sensor. There are 16 (4*4) coordinates around ùëôùë°‚àí1 for the sensor to deploy, and the deployment strategy is a parameterized distribution output by the location network after processed by Softmax. ùúÄ=0.1. When using reinforcement learning to train the location networks, we set the discount factor ùõæ=0.9 and the learning rate is 0.001. We use the negative loss of the classification result (Mean Square Error) as the reward r for the location networks. The linear layers used to process retina-like representation ùúå(ùë•ùë°,ùëôùë°‚àí1) and ùëôùë°‚àí1 is 144√ó128 and 2√ó128. The linear layer for each hidden state ‚Ñéùëñùë° has 256 units, followed by a fully connected layer with 256√ó128 units. The details of these networks in AM-MA are shown in Table 1.

Table 1 The details of AM-MA model
Full size table
Results and analysis
We first compare the proposed AM-MA with the standard RAM on Cifar-10. To highlight the adaptability of AM-MA, we use three RAM models as baselines: RAM with one-sensor, RAM with two-sensor, and RAM with three-sensor. The results are shown in Figs. 8and 9. In the training process, the proposed AM-MA executes the self-evaluation at set intervals. When the existing number of sensors cannot capture enough features from the visual input, the demand for more features makes the model meet those conditions, and then, a new sensor is constructed. When AM-MA has only one sensor (0‚àº146 Epoch), its performance is far from satisfactory because the observation is smaller than that in the RAM models. But when AM-MA adds the second sensor (146‚àº357 Epoch), the accuracy increases faster. After the third sensor is added (357‚àº600 Epoch), the performance of AM-MA is much better than the RAM models with one sensor and two sensors. The training accuracy rate is 81% with training through AM-MA, and the test accuracy rate is 80%. The training accuracy is 80% and the testing accuracy is 78% when training through the RAM model has three sensors. In addition, since fine-tune is used to initialize the network of the added sensors, it is not necessary to re-train the whole network. As can be seen from Fig. 8, after adding a new sensor, the model can be stable without too much training.

Fig. 8
figure 8
The training accuracy (a) and the testing accuracy (b) in the training process. When training used AM-MA, sensors were added to the model when the number of training epochs was 148 and 359, respectively

Full size image
Fig. 9
figure 9
The result of training accuracy (a) and the testing accuracy (b) on Cifar-10

Full size image
Figure 10 shows the trajectories of sensors in AM-MA. Comparing Fig. 10a and b, we can know that the multiple sensors have the ability to glimpse the input cooperatively, and the number of glimpses in the case of multiple sensors is obviously less than that in the case of a single sensor. Another things needing to be noticed is that the trajectories of these multiple sensors have less overlaps in Fig. 10b, which suggests these sensor can share information by the fully connected network following the linear layers. Moreover, the trajectories in both Fig. 10 a and b suggest that sensors can focus on the important regions of the visual input through reinforcement learning. When there are several sensors, these sensors can cooperative with each other to avoid repeated retina-line presentations.

Fig. 10
figure 10
a The trajectory of the sensor when there is only one sensor in AM-MA. b The trajectories of the sensors when there are three sensors in AM-MA

Full size image
Then, we investigate the proposed AM-MA on recognizing the fine-grained categories. Recognizing fine-grained categories is a very challenging task and has attracted extensive attention [2, 30]. AM-MA and the three baselines are used to classify the CUB 200 dataset. In this experiment, we construct two AM-MA models with different settings, by which the impact of structure can be investigated. In the first AM-MA model, each sensor can glimpse 8 times, and the resolutions of regions observed by it are set to 6*6, 8*8, and 10*10. In the other AM-MA model, there just 6 allowed glimpses for a sensor, and the observed regions are 4*4, 8*8, and 12*12. The first AM-MA has deeper networks and more advanced sensors, while the other one is on the contrary. The results of the two AM-MA models and baselines are shown in Table 2, from which we know that our AM-MA can capture features more effectively on recognizing the fine-grained categories than baselines, while the AM-MA models with different structures have similar performances. The traditional RAM achieves the worst results, because it is limited by the fixed number of glimpses on fine-grained categories. Saccader cannot capture the relevance degree in complicated dataset. As for DDRAM, it is better than RAM and Saccader due to its deeper network, but our AM-MA still outperforms it.

Table 2 The Accuracy of two AM-MA Models and Baselines on CUB 200
Full size table
Additional discussions
The proposed AM-MA is a parameter-insensitive, that is, its parameter and setup do not have much effect on the training result. To confirm this, we use four AM-MA models with different sensors and networks to process the Cifar-10 dataset, in which some of them have inappropriate settings. Table 3 is the comparison among the four models, where the AM-MA 1 has the most advanced sensor and deepest networks, while the sensors for the AM-MA 3 and AM-MA 4 are too small to observe the images in Cifar-10.

Table 3 The comparison among the four AM-MA models
Full size table
As known from Fig. 11 and Table 4, although the structure of the AM-MA 4 is straightforward and the sensor can only observe regions with small resolutions, it can improve performance by adding sensors adaptively. At the beginning of training, the AM-MA 1 achieved the best performance with its more complex structure and advanced sensor, but with the addition of sensors in the other models, the performances of the four models were almost equal. The AM-MA 2 only added one sensor in training. The result shows that in the case of inappropriate model structure and simple sensor setting, the performance of AM-MA will not be significantly reduced, which avoids frequent adjustment to the model due to the uncertain complexity of the task.

Fig. 11
figure 11
The results of using four AM-MA models with different structures to classify Cifar-10, where a dotted line represents the corresponding model adds a new sensor

Full size image
Table 4 The accuracy in different phrases of training, where Ep denotes the last epoch the sensor has the current number of sensor(s)
Full size table
The proposed AM-MA has two hyper-parameters, strictness degree ùúÄ and the reward r, that may impact the training results. Then, we conduct an experiment to investigate their effects on the training. In the above experiments on CUB200, ùúÄ is set to 0.1, and the reward r is the negative loss. There are three models used as the control group in this experiment: AM-MA with ùúÄ=0.05, AM-MA with ùúÄ=0.2, and AM-MA with a hard reward function. In the first two, we adjust the strictness coefficient to investigate how they impact the training results, and the corresponding models are termed AM-MA (0.05-SC) and AM-MA (0.2-SC). For the last model, which is termed AM-MA (HR), we set a hard reward function. Specifically, the reward is 1 if the model gives a correct classification result. Otherwise, the reward is -1. All models used in this experiment have the same network and sensor settings as the AM-MA 1 model in the CUB200 experiment.

Figure 12 gives the results of this. In the early process of the training, AM-MA (0.2-SC) is the first model to add a new sensor, while AM-MA (0.05-SC) cannot add new sensors til 275 epoch. This phenomenon is consistent with our intention. The larger ùúÄ is, the easier the model adds sensors. However, although AM-MA (0.2-SC) and AM-MA (0.05-SC) have different training curves with AM-MA, they get similar final accuracy. The accuracy for AM-MA (0.2-SC) is 67.6, while that for AM-MA (0.2-SC) is 67.3. When a hard reward function is used, the training curve becomes unsteady. The shadow area for AM-MA (HR) is larger than others. This is because the hard reward function cannot reflect how much the classification results are close to the objective results. We introduced the reinforcement learning technique to optimize location networks, so that negative training loss is more suitable to evaluate the deployment strategy. From the final results, we can know that the hard reward function will not influence the final accuracy, but the model needs more epoch to find the flat region of the error curved surface.

Fig. 12
figure 12
The results of models with different hyper-parameters

Full size image
Conclusion
To enhance the existing sensor-based hard attention model while avoiding adjusting the structure for the existing sensor-based attention models frequently, we propose a parameter-insensitive hard attention model. The proposed AM-MA can achieve better learning results by adding new sensors one by one when the complexity of the task is unknown or given an inapposite setting. Besides, this work also guarantees that the model will not be trained for a long time through a fine-tune process. The deployment strategies for the sensors are trained by reinforcement learning. Several experiments show that the AM-MA model is not only a parameter-insensitive model, but also captures the features of input more effectively by multiple sensors. Our proposed method could serve as a useful component in image processing and computer vision, especially in tasks with unknown complexity. In the future, the training efficiency of AM-MA will be promoted, and we plan to apply it to more complex scenarios.

Keywords
Attention mechanism
Visual attention model
Neural network
