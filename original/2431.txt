The computational cost of evaluating a neural network usually only depends on design choices such as the number of layers or the number of units in each layer and not on the actual input. In this work, we build upon deep Residual Networks (ResNets) and use their properties to design a more efficient adaptive neural network building block. We propose a new architecture, which replaces the sequential layers with an iterative structure where weights are reused multiple times for a single input image, reducing the storage requirements drastically. In addition, we incorporate an adaptive computation module that allows the network to adjust its computational cost at run time for each input sample independently. We experimentally validate our models on image classification, object detection and semantic segmentation tasks and show that our models only use their full capacity for the hardest input samples and are more efficient on average.

Introduction
A neural network typically defines a fixed pipeline of operations where each input sample undergoes the exact same operations to obtain the result. As a consequence, each input has exactly the same computational cost regardless of the complexity (the content) of the input. When designing a neural network architecture, we aim to have the best possible performance averaged over all samples in the dataset. This means that we take the worst-case approach to designing networks. Our network has enough capacity to handle the hardest inputs even though a smaller network would work fine for the majority of inputs [1]. The human visual system on the other hand does not have a constant processing speed. Recognizing familiar objects is done almost instantaneously while more complex or unfamiliar scenes require more time [2]. In this work, we build networks with similar properties. Our networks learn during training to adapt their computational cost to the complexity of the input image and only use their full capacity for the hardest input samples. We experimentally validate our approach on image classification, object detection and semantic segmentation tasks and show that our models have a lower number of parameters and are able to adjust their computational cost to the complexity of the input image. We also qualitatively analyze what types of inputs are the hardest according to the model and show that this corresponds well with our human understanding of difficulty.

The remainder of this paper is structured as follows: We begin in section 2 with an overview of previous approaches for resource-efficient deep neural networks. We focus on adaptive computation and residual networks since these form the basis for our approach. We explain our iterative network architecture in section 3 and validate it for different computer vision tasks on default benchmark datasets in section 4. We conclude in section 5 and give some pointers for future work.

This paper is an extension of a previous workshop paper [3]. We now extend this work with a much more thorough explanation of the architecture, an investigation into the role of different design choices and experiments to show the performance on object detection and semantic segmentation tasks, in addition to the image classification tasks.

Related work
Deep neural networks (DNNs) have achieved breakthrough results in the past decade for applications such as speech recognition [4], language translation [5], robotics [6] and playing complex games [7]. Arguably the biggest success story can be found in the field of computer vision. Around 2011 deep convolutional neural networks (CNNs) trained on graphics processing units (GPUs) started to achieve state-of-the-art results on different benchmark datasets [8,9,10]. When Krizhevsky et al. used CNNs to win the ImageNet competition in 2012, beating the competitors by a large margin [11], the world took notice and CNNs replaced shallow architectures for most computer vision tasks in the next years. Deep Neural Networks are so powerful because they are able to extract hierarchies of complex features from large amounts of raw data instead of having to rely on hand-crafted features defined by human experts. Combined with the large amounts of compute power provided by GPUs, this allows us to build increasingly large and powerful models. The success is not limited to image classification. Deep Learning is now the state-of-the-art technique for object detection [12], semantic segmentation [13], image super-resolution [14] as well as for less traditional tasks such as style transfer [15], image generation [16] or image captioning [17]. For a more in-depth overview of the history of Deep Learning we refer to [18] and [19].

Resource efficient deep learning
The impressive accuracy of DNNs comes at a cost. Since the models are trained from scratch without any prior information, they need large amounts of (labelled) data which is often costly to obtain. In this article, we focus, however, on the computational cost. Processing a single input sample with a modern architecture easily requires billions of floating point operations (FLOPs) and hundreds of Megabytes of memory. Training a model involves multiple passes over the training set and can take weeks. However, training a model is typically done offline on a large compute infrastructure in the cloud where distributed training algorithms can use hundreds of machines to train models relatively quickly [20, 21].

Once trained, these models are deployed for inference. Inference is less computationally demanding than training but for many interesting applications we need to use the models on resource constrained devices such as mobile phones, drones, sensor nodes or wearables. Here intrinsic limitations such as energy consumption or limited network bandwidth makes it challenging to use DNNs for real-world problems. Many works have proposed techniques to reduce the computational cost of DNNs for inference.

A popular technique is to use reduced precision operations to speed up inference. A typical implementation uses 32-bit floating-point operations but most networks will work fine with 16-bit floating point numbers. Several studies have shown that it is also possible to reduce the precision to 8 bit [22], 4 bit or even 2 bit [23]. In the extreme case, it is possible to use 1 bit (binary) weights and activations [24] although this incurs a heavy penalty on the classification accuracy for real-world tasks.

Other works have shown that neural networks are typically overparameterized and that it is possible to prune the networks to remove unnecessary weights. Many heuristics such as second-order gradient information [25] or the magnitude of the weights [26] have been proposed to identify these redundant weights. Other more complicated techniques rely on Bayesian statistics [27] or on reinforcement learning [28]. In all of these techniques, the pruned weights are set to zero. By storing the weights as sparse weight matrices, the size of the network is reduced, making it more efficient to download and store. Because many weights are zero, this approach also has the potential to speed up inference since the multiplications involving zero weights can be skipped. Doing this efficiently, however, requires custom hardware support [29].

Adaptive computation in deep neural networks
There has been some interest in designing adaptive network architectures that can scale their computational cost with the complexity of the input. One technique is to include an early stopping mechanism which allows the network to return a prediction early if the network is confident enough [1, 30,31,32]. Other techniques use reinforcement learning to selectively activate only parts of the network [33] or use different networks, each with their own computational cost-accuracy trade-off, where a big network is only used if a small network is unable to give a confident prediction [34,35,36].

Other approaches use an input-dependent block that spatially transforms feature maps, conditional on the feature map itself. These Spatial Transformer Networks [37] learn to perform a transformation (rotation, scaling, cropping, ...) specific for the current input sample. A similar idea can also be applied to the input image. Recasens et al. propose a differentiable layer that can be added to the network that effectively learns to sample from high-resolution input [38]. It selects a part of the input data that contains the most relevant information and avoids evaluating the network on the entire image or on a naively downsampled version. An intelligent downsampling is also extremely useful for semantic segmentation tasks [39].

In this work, we design a new architecture that uses the Adaptive Computation Time (ACT) algorithm introduced in [40]. This algorithm was first used in combination with recurrent neural networks (RNNs) allowing the network to learn how many computational steps to take between receiving an input and emitting an output. ACT is an interesting approach because it requires minimal changes to the network and is fully deterministic and differentiable. ACT was later extended to sSpatially adaptive computation time (SACT) [41] which allows a network to use different amounts of computation for different parts of the input image. A similar approach was developed by Verelst et al. [42]. Their dynamic convolution blocks use a small gating branch to dynamically apply convolutions conditioned on the input image. Thanks to an efficient CUDA implementation, they are even able to outperform efficient models such as MobileNetV2 or ShuffleNetV2.

Residual networks
Deep Residual Networks (ResNets) [43] won the ImageNet 2015 competition and have proven themselves to be one of the most important breakthroughs of the last years. Even though ResNets use the same basic operations as other convolutional neural networks they are different in that a single layer does not completely transform its input (ùë¶=ùêπ(ùë•)). Instead, a layer learns a residual to add to its input (ùë¶=ùë•+ùêπ(ùë•)). This seemingly minor difference has important implications. ResNets can be much deeper (up to a thousand layers [44]) and can still be trained with stochastic gradient descent.

In addition to supporting much deeper networks, these residual connections also make the network more robust against deleting or reordering layers of the network at test time which destroys other CNNs [45]. It is even possible to share weights between different layers [46, 47] or to drop random layers during training and use all layers together for inference [48].

One explanation for these observations is given by the unraveled view of a ResNet [45]. Because each layer has two parallel paths, a ResNet architecture with i residual layers has 2ùëñ different paths between the input and the output. When we delete a layer from a trained model, we corrupt certain paths but other paths remain intact. More traditional architectures only have a single path between input and output and are completely corrupted by this operation. Because of these many parallel paths, a ResNet can be interpreted as an ensemble of many smaller networks [45].

A second possible explanation for the special behaviour of ResNets can be found in the iterative estimation viewpoint [49]. Here, it is argued that a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. The first layer of a block already computes a rough estimate of the feature representation, which is then iteratively refined by the successive layers. This also explains the robustness of ResNets against deleting or reordering layers. This iterative refinement behaviour was further formalized in [47].

ResNets are closely related to highway networks that also have shortcut connections although highway networks have gating functions that regulate the information flow [50] making them similar to long short term memory networks (LSTMs) [51].

There is an interesting relationship between residual networks, recurrent neural networks (RNNS), and the primate visual cortex. In [52], Liao et al. make the observation that a recurrent neural network is equivalent to a ResNet with weight sharing among the layers (as is done in our IterativeNets). They provide a generalization of RNNs and ResNets and show that this is a biologically plausible model of the ventral stream in the primate visual cortex. We go one step further by also adding an adaptive computation time mechanism to our models. This allows us to use a more or less computational steps to recognize an input, similar to how the primate visual cortex also is able to recognize certain objects faster than other, less familiar objects [2].

Iterative neural network architecture
In this section, we introduce our novel Iterative Neural Network building block. We start by explaining how a Residual Network is designed and then show how certain properties of these architectures can be exploited to design a more efficient iterative structure.

The basic building block of a Residual Network is shown in Figure 1. This block contains three convolutional layers (with Batchnorm [53] and a ReLU [54] activation after each convolution). The block receives input with 4n channels. The first 1√ó1 convolution reduces this to a tensor with n channels for the 3√ó3 convolutional layer to work on. The last 1√ó1 convolution expands the number of channels back to 4n. The input and output of a block have the same dimensions and are summed together before passing the result on to the next block in the network. A ResNet is built by stacking many of these blocks, grouped together in stages (see Figure 2). Within a single stage, all residual blocks will have the same number of convolutional filters (n) and will work on the same spatial size. The first block of a stage downsamples the input using a convolution with stride 2, which means that each stage works on inputs with a spatial size four times smaller than the previous stage. A typical ResNet uses four stages and the number of residual blocks in each stage determines the size of the network. ResNet101, for example, has 3, 4, 23, and 3 residual blocks in each stage, respectively, while ResNet152 has 3, 8, 36, and 3, respectively. The number of channels (n) doubles in each stage and is typically 64, 128, 256, and 512. A full ResNet is show in Figure 2. The input is first passed through a single convolutional layer (64 filters, again with Batchnorm and ReLU activation) before going through the four stages. As the last step, the network applies global average pooling and a fully connected layer, which returns the predicted output.

Fig. 1
figure 1
The basic building block of a ResNet with a bottleneck architecture and an expansion rate of four

Full size image
Fig. 2
figure 2
A ResNet is built by stacking many Residual Blocks. These blocks are grouped in four stages. Within a stage, each block has the same input size and number of filters

Full size image
We propose to use the Iterative Estimation property of ResNets [49] to reduce the memory footprint and computational cost. Within a single stage, the first residual block makes a rough estimate of the feature representation that the stage will extract. The next blocks do not completely change this but instead make small refinements. This means that all the layers of a stage look for similar features and have the same behavior. We take this to the extreme and reuse the same block multiple times within a single stage. In our architecture, each stage collapses to a single block that is used over and over again. Hence, we replaced the sequential layers with an iterative structure, which reduces the model size drastically. Our proposed building block is shown in Figure 3. We keep the same three convolutional layers and each layer has the same number of filters as in a normal ResNet. Instead of having a shortcut connection, we keep track of a ‚Äústate‚Äù (s) . This state is initialized with a copy of the input of the block (Equation 1.1). Each iteration, we pass the state through the block (Equation 1.2) and add the output of the last convolutional layer to the state (Equation 1.3).

Each iteration, we refine the previously extracted features. Not all inputs are equally hard to recognize. For some easy inputs, a rough estimate of the features is good enough to accurately classify the input. For other inputs, a more refined feature estimation is needed. To support this, we include an Adaptive Computation Time (ACT) block in the network. This is a small two-layer fully connected network that looks at the current state and the last output (Equation 1.4) to determine if we need to continue iterating or if we can move on to the next stage. This Adaptive Computation block allows to adaptation of the number of iterations based on the current input.

ùë†0=ùêºùëõùëùùë¢ùë°
(1)
ùë¶ùëñ=ùêπ(ùë†ùëñ‚àí1)
(2)
ùë†ùëñ=ùë†ùëñ‚àí1+ùë¶ùëñ
(3)
‚Ñéùëñ=ùê¥ùê∂ùëá(ùë†ùëñ,ùë¶ùëñ)
(4)
Equation 1: The operations performed by the ACT block

Fig. 3
figure 3
The basic building block of our Iterative Networks

Full size image
Our building block works as described in Algorithm 1 (which closely follows the ACT algorithm from [41]). The block is configured to allow at most L iterations. The task of the ACT network is to predict a halting score ‚Ñéùëñ, a scalar value in the range [0, 1] for each iteration i. We keep track of the cumulative sum of the halting scores c and a remainder value R. As soon as the cumulative sum of the halting scores reaches one, we stop iterating within this block and move on to the next block in the architecture. The output of a block is defined to be the weighted sum of the state at each iteration. The weight is the halting score at the corresponding iteration except for the last iteration where we use the remainder value as the weight. This ensures that the weights sum to one.

To force the network to use as few iterations as possible, we introduce a ponder cost. This is equal to the number of used iterations plus the remainder value. Minimizing the ponder cost increases the halting scores, making it more likely that the computation would stop earlier. We train the network end-to-end with gradient descent to simultaneously minimize the cross-entropy classification loss and the ponder cost.

The ACT network is a small two-layer fully connected network with 64 hidden neurons. We apply global average pooling to the current state and to the output of the last convolution and concatenate these vectors before passing this to the ACT network. By applying global average pooling we reduce each feature map to a single number that indicates how strong each feature map was activated. The ACT network returns a single value (the halting score ‚Ñéùëñ) and uses a sigmoid activation to ensure that the halting score is in the range [0,1]. The weights of the ACT network are optimized simultaneously with the other parameters in the network. We initialize the bias value of the output neuron to a small value to force the network to use the maximum number of iterations at the beginning of the training phase.

Just like in the original ResNet implementations, we use BatchNorm [53] after each convolutional layer. We found that it is not possible to use the same batchnorm statistics for multiple iterations, instead, we use a different set of statistics in each iteration. The same approach was used in [47] in their experiments with shared weights.

figure a
Our iterative models have a lower memory footprint than conventional ResNets thanks to the weight sharing. The adaptive computation mechanism allows to the model to automatically adjust its computational cost to the complexity of the input image. As we will show in section 4, this makes our models more efficient on average compared to the conventional ResNets it was based on. The weight sharing could also reduce the energy consumption of evaluating the model. The energy consumption of a model is rarely reported in literature as it is difficult to estimate or measure and depends strongly on the hardware platform [55]. The total energy consumption of a forward pass through the model is dominated by memory access operations (retrieving weights and feature maps from DRAM). Memory access can easily require 200 times more energy than a multiply accumulate operation [56]. By reusing weights multiple times, we could potentially reduce the energy consumption since the cost of loading the weights from memory will be amortized over multiple iterations [57]. Doing this efficiently, however, will likely require custom hardware support.

Additional design choices for the iterative block
We follow the same bottleneck structure with three convolutional layers as a normal ResNet. A first 1√ó1 convolution reduces the number of channels, a 3√ó3 convolution looks for spatial features and a second 1√ó1 convolution goes back to the original number of channels. This bottleneck structure is more efficient because the expensive 3√ó3 convolution now works on smaller input data. The difference in the number of channels between the input and the intermediate number of channels is called the expansion rate and is typically 4 (as in Figure 1). This is, however, an arbitrary number and it is possible to use other factors to scale the model. We explore the impact of the expansion rate in our experiments in Section 4.1.

A second technique that can be used to reduce the computational cost is to use depthwise separable convolutions. A normal convolution works by taking an input volume with n channels and sliding a convolutional kernel over this volume. The kernel has a certain width and height, (e.g., 3) and has the same number of channels as the input volume. A single kernel generates a single output plane and a convolutional layer typically applies multiple kernels in parallel to obtain an output volume with multiple channels. A single convolutional kernel actually performs two tasks simultaneously, it looks for spatial features and it combines information from multiple channels. Depthwise separable convolutions explicitly perform these two tasks independently. First, a 3√ó3 depthwise convolution looks for spatial features in each input channel separately. Then, a 1√ó1 pointwise convolution combines information from the different channels and generates an output volume. Depthwise separable convolutions use fewer parameters and operations than a normal convolution and are, therefore, a very popular building block for resource-efficient architectures[58]. We experiment with replacing the 3√ó3 convolutions with depthwise convolutions in section 4.1 and report the impact on classification accuracy, the number of parameters, and computational cost.

When we introduced our architecture in the previous section, we stayed close to the original ResNet architecture and replaced each stage with a single iterative block. We can also use multiple iterative blocks to replace a single ResNet stage. This increases the capacity of the network but also increases the number of parameters and computational costs. We explore this trade-off in section 4.1.

Experiments
In this section, we experimentally validate our approach on different benchmark datasets. We start with image classification on the CIFAR10 and CIFAR100 datasets to explore the impact of different design choices on the accuracy, memory footprint and computational cost of the models. We empirically show that our models are able to adapt their computational cost based on the complexity of the input image. We also report results for a large-scale image classification task on the ImageNet dataset.

ResNets are one of the most common network architectures right now and are often used as building blocks for networks trained for other tasks such as object detection or semantic segmentation. In section 4.2 and 4.3, we take existing models for object detection and image segmentation and replace the ResNet backbone with our iterative network. We again report the impact on the accuracy, memory footprint and computational cost and show that for these tasks the computational cost of the models also scales with the complexity of the input image.

Image classification
As explained in section 3, differently sized ResNets can be designed by changing the number of residual blocks in each stage. Table 1 shows four typically used ResNet variants together with the number of parameters, the computational cost (number of multiply accumulate operations: MAC), and their accuracy on the CIFAR10 dataset [59], respectively. All these networks were trained with stochastic gradient descent (SGD) with momentum [60] for 150 epochs with initial learning rate of 0.01. The learning rate was reduced by a factor of ten every 50 epochs. We used random horizontal flips of the images during the training. For all models, the training took 2.5 hours on a single NVIDIA GTX1080 GPU.

In all our experiments, we report the theoretical number of multiply accumulate operations as calculated by a PyTorch Flop counter toolFootnote1. This gives an objective metric to compare the computational cost of different metrics but is far from perfect in practice. Two models with the same number of multiply accumulate operations might have a different memory footprint, throughput, or energy consumption in practice, depending on the size of the activations and the parallelism of the hardware.

We designed an iterative network that can use the same number of iterations in each stage as the number of residual blocks in the corresponding stage of ResNet152. This means that our network can use 3, 8, 36, and 3 iterations in each of the four stages, respectively.

The second part of Table 1 shows the results on the CIFAR10 dataset. We trained four versions, each with a different expansion rate (1-4), and showed the number of parameters and the accuracy of these models. We repeated these experiments five times and report the average accuracy. The standard deviation of the accuracy over multiple runs was low (around 0.1%). Compared to ResNet152 (the ResNet variant that our architecture was modelled after), we use drastically fewer parameters. Our model with expansion rate 1 uses 4M parameters, which is more than a tenfold reduction compared to the 58M parameters of ResNet152. A model with an expansion rate of 4 doubles the memory footprint to 9M, still only 15% of the original model size. Somewhat surprisingly, our models actually achieve a higher test accuracy than the ResNet models. This is most likely because the weight sharing acts as a regularizer and because the ResNet models are probably oversized for a task like CIFAR10.

ResNet152 requires 3.7 GMAC to process a single image. In our architecture, the computational cost is not fixed but depends on the input itself. The second column of Table 1 shows the measured number of multiply accumulate operations averaged overall images of the test set. The third column shows the lower and upper bounds. The upper bound is slightly higher than the computational cost of ResNet152 even though it uses the same number of convolutions. This is because of the overhead of the ACT block that is executed every iteration. On average, however, our architecture still uses much fewer computations than the ResNet. Measured on an Intel(R) Xeon(R) 2.10GHz single CPU core, the actual forward time for our iterative models ranges from 40% to 80% of the ResNet152 baseline model, it was based on.

The last part of Table 1 shows the results of the same architectures but now using depthwise separable convolutions. This immediately reduces the number of parameters and computational cost even further while still having a higher accuracy than the baseline ResNet models. It is interesting to note that the average number of MAC operations is now much closer to the upper bounds compared to the previous experiment. This suggests that depthwise separable convolutions cannot capture the same level of complexity as normal convolutions and that more iterations are needed to converge to a good feature estimation. While depthwise separable convolutions clearly reduce the number of parameters and theoretical computational cost, they do not improve the forward time in practice.

Table 1 Accuracy, computational cost, number of parameters and forward time measured on a CPU for standard ResNets and Iterative Networks trained on CIFAR10
Full size table
Table 2 Accuracy, computational cost and number of parameters for standard ResNets and Iterative Networks trained on CIFAR100
Full size table
Table 3 Accuracy, computational cost and number of parameters for different ImageNet networks
Full size table
Table 2 shows the same experiments but now using the CIFAR100 dataset. This dataset is more complicated since it contains images from 100 different classes. We can make similar observations as in the CIFAR10 experiments. Our models require fewer parameters and operations than the baseline ResNet models while still obtaining a very good classification accuracy. We again see a drop in accuracy for the models using depthwise separable convolutions. This drop is more severe because CIFAR100 is a more difficult task than CIFAR10.

Finally, we also trained models on the challenging ImageNet dataset [77]. This dataset contains large real-world images grouped in a thousand classes. There are around 1.2 million training, 50.000 validation, and 150.000 testing images.

We summarize the number of parameters, computational cost, and accuracy of some commonly used networks in the first part of Table 3. In the second part of the table, we list state-of-the-art models that were designed to have a low computational cost and/or memory footprint. They use depthwise separable convolutions [58] or replace expensive convolutions with shuffling operations [72]. Other architectures [73] are the result of extensive automated architecture search procedures. The third part of the table lists the default ResNet models. These architectures vary in size between 11M and 60M parameters (44Mb and 240Mb, respectively if stored as 32 bit floating point numbers). The smallest network (ResNet18) only requires 1.8 GMAC, the largest ResNet (ResNet152) requires six times more computations. To be able to make a fair comparison to our Iterative models, we report the accuracies obtained by training these ResNets from scratch ourselves. We used the exact same data preprocessing for the ResNets and IterativeNets. We rescaled all images to have the smallest dimension of 256 and took random 224*224 crops and random horizontal flips during training. For testing, we used a single 224*224 center crop of the rescaled images. The inputs were normalized to have zero mean and unit variance. All our networks were trained with stochastic gradient descent (SGD) with momentum [60] for 90 epochs with initial learning rate of 0.01. The learning rate was reduced by a factor of ten every 30 epochs. Note that our reported ResNet accuracies differ slightly from the ones reported in [43]. For ResNet50 for example, we report an accuracy of 75.2% while the original authors reported 75.3% and for ResNet152, we report 77.1% compared to 77% in the original paper. For ResNet101, we obtain the exact same result. We trained three iterative networks, summarized in the last part of the table. The first network (IterativeNet152) is modeled exactly after ResNet152. It only uses 9M parameters compared to the 60M of ResNet152. The computational cost of our iterative network is 5 GMAC averaged over all test samples, a reduction of 56%. The accuracy drops from 78.3% to 70.2% (94,1% to 90,0% top 5). The second iterative network (IterativeNet152_small) uses an expansion rate of two instead of four. This reduces the number of parameters further to 6M and halves the computational cost. The accuracy drops to 65,6%. The last model (IterativeNet152_double) replaces each stage from a ResNet with two iterative blocks instead of one. This allows the network to learn more complex features which improve the accuracy but also almost double the model size. Somewhat surprisingly, the average computational cost is only slightly higher since the model now uses less iterations in each block.

Our motivation was to design architectures that can automatically allocate their resources based on the complexity of the input. It is hard to objectively measure the ‚Äúcomplexity‚Äù of an input. In this work, we do not impose a predefined idea of what makes an image complex or not to the network but instead give the network the ability to figure this out during training. Images that are complicated for the network are not necessarily complicated for us and vice versa. This approach gives us some insight into the behavior of deep neural networks and how they learn to perform certain tasks. Table 4 shows some typical test samples for the three data sets. The first row shows images that require the smallest number of iterations, subsequent rows require more iterations. These results show that indeed the model is able to adjust its computational cost based on the complexity of the input image. It seems that the difficulty of an input according to the models correlates well with our human definition of difficulty. Empirically, we found that the easy images typically contain a single object in the center of the image while the hard images are typically dark or unclear. For the ImageNet dataset, the hardest images typically show landscapes or scenes with very small objects.

We also analyzed the error rate as a function of the number of iterations that the model has decided to use for a given test image. These results are summarized for the CIFAR10 dataset in figure 4. The other datasets showed the same behavior. This figure shows that the error rate is very low for the images that required a small amount of total iterations. For the images where the model decided to use more iterations, the error rate also increases. This supports our hypothesis that the model allocates more resources to those inputs that are harder to recognize (and that have a higher chance of being misclassified). The horizontal line in the graph shows the total error rate of the model.

Table 4 Some typical examples of images that require less iterations (top) and images that require more iterations (bottom) for the three image classification datasets
Full size table
Fig. 4
figure 4
The error rate of the CIFAR10 model for inputs where the model decided to use a certain number of iterations. As the required number of iterations increases, the error rate also increases, indicating that these inputs are harder for the model to recognize. The horizontal line shows the total error rate of the model

Full size image
Object detection
In the previous section, we focussed on image classification where the task is to predict a single class for the entire image. Here we investigate the more complicated task of object detection where the input image may contain more than one object and each object needs to be localized (annotated with a bounding box) and classified. The number of objects varies between images.

In this experiment, we take an existing implementation of Faster R-CNN [78] and adapted this implementation to use our iterative network. Faster R-CNN is one of the most commonly used networks for object detection. Like most object detection networks, Faster R-CNN uses a pretrained convolutional neural network as a backbone to extract features from the input image. These features are then used to predict Regions Of Interest (ROI), regions that potentially contain objects. Each of these regions is checked and all predictions are combined to obtain the predictions of the entire image. The backbone network is usually a VGG [61] or ResNet network but we replaced it with the pretrained IterativeNet152 from the previous section and fine-tuned it for the new task.

For this task, we used the PASCAL Visual Object Classes Challenge 2007 (VOC2007) [79] dataset. This dataset contains 20 different types of objects. There are around 5000 train and test samples within total 24,640 annotated objects. We used an input image size of 600.

Table 5 shows the number of parameters, computational cost and mean Average Precision (mAP) for a Faster R-CNN architecture trained with different backbone networks. Compared to a ResNet152 backend, our model has 77% less parameters and requires only 60% of the computational cost on average. The mean average precision is 68.9%, comparable to a model with a VGG16 backend that has ten times more parameters.

Figure 5 shows typical test images together with the predicted bounding box and labels. The first row shows the images that require the smallest amount of iterations, subsequent rows need more iterations. We again find that the required number of iterations correlates well with our human understanding of complexity. The easy images typically contain large objects, centered in view while the hardest images typically contain many small objects. We further analyze this in Figure 6. We show the relationship between the required number of iterations (y axis) and the number of ground truth objects (x axis) for the test samples in an image (Figure 6a). Although this relationship is noisy, we find that images with a large number of ground truth objects typically require more computation time. In Figure 6b, we plot the required number of iterations as a function of the average ground-truth object size. Here, we find that the computational cost decreases as the average size increases, confirming our intuitive observation that images with a small number of big objects are easier for the network to process than images with many small objects. With a ResNet backbone, the R-CNN network has the same computational cost for all, regardless of the number of visible objects or the difficulty of the input image.

Table 5 Accuracy, computational cost and number of parameters for a Faster-RCNN model trained with different backend networks
Full size table
Fig. 5
figure 5
Some typical examples of images that require less iterations (top) and images that require more iterations (bottom) for the object detection task. Images that contain many small objects are the hardest for the network

Full size image
Fig. 6
figure 6
Correlation between the number of ground truth objects and the required number of iterations (a) and between the average ground truth object size and the required number of iterations (b). The model is able to scale its computational cost with the complexity of the input image

Full size image
Semantic segmentation
Semantic segmentation is the task of predicting a label for every pixel in the input image. This again is a much more complex task than image classification where a single label is predicted for the entire image. Different network architectures were designed for this task that typically follow an encoder-decoder architecture where the encoder first extracts features and creates a compressed representation of the image. The decoder then uses this representation to predict a label for each pixel. The encoder is usually based on a network for image classification. The decoder uses upsampling operations and transposed convolutions. We adapt the UPerNet architecture [84] to use the pretrained IterativeNet152 from Section 4.1 as the encoder.

We trained our models on the MIT ADE20K scene parsing dataset [85, 86]. ADE20K is the largest open source dataset for semantic segmentation and scene parsing. It contains images of indoor and outdoor scenes and the goal is to segment an image into different image regions associated with one of 150 semantic categories, such as sky, road, or person. There are 20.210 training images and 2.000 validation images.

Table 6 shows that compared to a model with a ResNet152 backend our model has less than half the number of parameters and uses 20% less computations on average. Figure 7 shows some typical validation images sorted from easy (top) to hard (bottom). For each row, we show the input image, the ground truth segmentation and the output of the model from left to right, respectively. Somewhat surprisingly, the images that require the least amount of iterations show cluttered scenes with many different types of objects. We would have expected that these images are hard for the network to classify. We attribute this to the fact that the objects in these scenes have clear boundaries between each other and the background, making it easier for the network to make a decision. The hardest images according to the model are all landscapes or natural scenes where it is difficult to identify sharp boundaries between objects.

Table 6 mean Intersection-over-Union (IoU), computational cost and number of parameters for the semantic segmentation task
Full size table
Table 7 Some typical examples of images that require less iterations (top) and images that require more iterations (bottom) for the segmentation task. For each row, we show the input, the ground truth and the output of the model from left to right, respectively. The network struggles the most with natural scenes where it is hard to distinguish clear boundaries between objects.
Full size table
Conclusion and future work
In this work, we exploited the iterative refinement property of ResNets to design a more efficient neural network building block. By reusing the same weights multiple times, we can reduce the memory footprint dramatically. We validated our approach on different benchmark datasets for image classification, object detection and semantic segmentation and found that our models are able to adjust their computational cost at runtime based on the complexity of the input data and use less resources on average.

We believe the adaptive computation mechanism is an interesting approach that to explore further, possibly in video processing applications where less computation might be needed if a frame is similar to a previous frame. In this work, we stayed very close to the original ResNet architecture which is probably not the optimal solutions. We would, therefore, like to explore the use of adaptive computation in combination with other efficient architectures such as ShuffleNet to see if this can improve the average computational cost for these architectures as well.

In this paper, we compared the computational cost by calculating the number of floating-point operations needed to evaluate each network. This does not necessarily correlate perfectly with the actual runtimes when measured on a device. Further research is needed to implement algorithms like ACT efficiently on different hardware platforms to fully exploit its potential. Tools like TVM [93] that compile deep learning models into optimized code for specific hardware platforms will be crucial to deploy neural networks in the real world. While they optimize the low-level operations for a specific hardware platform, we follow an orthogonal approach and optimize the architecture of the network itself. Both approaches should be compatible, our networks will still benefit from optimizations such as operator fusion or layout transformation.

Keywords
Efficient deep neural networks
Inference on the edge
Adaptive computation
Resource-constrained deep learning