Currently, graph convolutional networks (GCN) have achieved significant progress in recommender systems, due to its remarkable capability on representation learning and the ability to integrate complex auxiliary information. However, the graph convolution operation is prone to cause over-smoothing due to the use of the graph Laplacian operator, so that the node embeddings become very similar after the multi-layer graph convolution, which leads to a decrease in recommendation performance. The recently proposed model based on simplified GCN can relieve this issue to a certain extent; however, they still only design the model from the viewpoint of GCN. Inspired by the recent developments of label propagation algorithms (LPA), in this paper, we propose a new recommender model that unifies graph convolutional networks and label propagation algorithms. Specifically, we utilize the GCN to build a basic recommendation prediction model, and unify the LPA to provide regularization of training edge weights, which has been proven to effectively alleviate the over-smoothing problem. In addition, we introduce an attention network to capture the attention weight of each user-item pair, which takes into account the fact that users attach different degrees of importance to various relationships of items. Extensive experiments on three real-world datasets demonstrate that the proposed algorithm has a significant improvement over other state-of-the-art recommendation algorithms.

Introduction
Recommender Systems (RSs) play a key role in today’s internet as they bring economic benefits to businesses while also offering personalized suggestions [1]. The collaborative filtering is the most widely used method in RSs [2], which produce new recommendations based on past interactions between users and items. This method only utilizes the implicit user feedback as input, and often suffers from inability of modeling auxiliary information (e.g., item attributes). In order to merge such information, a popular method is to convert them into feature vectors, and feed them into a supervised learning model with user-item ID. For example, factorization machine (FM) [3], NFM [4], and FNFM [5], etc.

Existing feature-based supervised learning methods treat each interaction as a separate data instance and fail to consider the high-order connectivity [6]. For example, the user u watched the movie v directed by the director e. Collaborative filtering methods emphasize users who have also watched movie v, while feature-based supervised learning methods focus on the other movies directed by director e. In real situations, the identity of director e may also be an actor, screenwriter, etc., while the movies corresponding to such identities of e have not been explored by these methods.

To tackle this problem, GCN-based models [6,7,8] have achieved prominent progress due to the remarkable capability on representation learning, and the ability to integrate complex auxiliary information (e.g., social network and knowledge graph). The core idea of GCN-based method is to iteratively aggregate the neighbors of the target node and express it as the embedding of the target node, which has been proven to be an effective method for fully mining the high-order connectivity of the graph, thereby enriching the representation of users and items. For example, KGAT [6] leverages the attentive embedding propagation layers to measure the importance of information, and recursively mine high-order connected information. However, the graph convolution technique employs a type of graph Laplacian, which is easy to cause over-smoothing (i.e., the node representation becomes very similar after the multi-layer graph convolution) [7]. To put it simply, such operations will homogenize user preferences, decreasing recommendation performance. [9].

In order to solve the above limitations, there is an influx of recommender models based on simplified GCN [10,11,12]. For example, LightGCN [10] empirically demonstrates that feature transformations and nonlinear activations not only increase the difficulty of training, but also have no positive effect on the model. Similarly, LRGCN [11] has been empirically demonstrated that when more graph convolution layers are stacked, the model’s performance degrades. Therefore, the above models usually solve these problems by reducing the number of network layers or removing nonlinear transformations; however, they still only design models from the perspective of graph convolution. As we know, GCN propagates and transforms node feature information along the edges of the graph [13], while label propagation algorithm (LPA) [14] propagates node label information. Therefore, models based solely on GCN are prone to be limited by the feature influences, and lack node information from other perspectives.

To solve these problems, we investigate the theoretical relationship between GCN and LPA, and propose a new recommender model named GCNLP. Different from existing GCN-based models, we integrate LPA to assist GCN-based models to alleviate the over-smoothing problem easily caused by GCN. Specially, we utilize GCN for basic prediction, and LPA provides regularization of training edge weights. In addition, we introduce an attention network to capture the attention weight of each user-item pair, which considers the fact that users assign different degrees of importance to various relationships of items. To evaluate the effectiveness of our proposed model, we conduct comprehensive experiments to compare our model with several baselines on Movielens-1M dataset, LastFM dataset and Book-Crossing dataset. Experiment results show that our algorithm outperforms the state-of-the-art recommendation algorithm. To sum up, the main contributions of this paper are as follows.

(1)
We propose a novel algorithm integrating label propagation with graph convolutional networks for recommendation. Leveraging LPA assist GCN to regularize edge weight, which can effectively alleviate the over-smoothing problem.

(2)
We introduce an attention network to capture the attention weight of each user-item pair, which takes into account the fact that users attach different degrees of importance to the various relationships of items.

(3)
We perform extensive experiments on three public datasets to prove the superiority of our proposed recommender method. Experimental results show that our algorithm outperforms state-of-the-art algorithms.

Related work
Existing recommender models incorporated with knowledge graph (KG) can be roughly divided into three types, embedding-based methods, path-based methods and GCN-based methods.

Embedding-based methods usually apply knowledge graph embedding (KGE) algorithms to encode the knowledge graph to represent items, and then integrate the item auxiliary information into the recommendation framework [15]. For instance, Zhang et al. [16] designed the CKE recommendation model, which applies TransE [17] on KG triplets, and feed the knowledge-aware embedding of items into matrix factorization (MF) [18]. In order to further capture users’ dynamic interests, Wang et al. [19] proposed the DKN model by integrating entity embedding and word embedding together with the CNN [20] framework for news recommendation, which also aggregates the embedding of historical clicked sequence to learn users representation. Subsequently, Wang et al. [21] proposed the SHINE algorithm, which treats the recommendation task as link prediction between entities. Another trend that follows this strategy is to adopt multi-task joint learning. For example, Cao et al. [22] designed the KTUP model, which simultaneously complete the task of recommendation and knowledge graph completion.

Path-based methods investigate various connections among items in knowledge graph to provide additional guidance for recommendation systems. For example, Yu et al. [23] proposed the HeteMF model, which extracts different meta-paths and computes item-item similarities in each path, then refines representation of uses and items. Luo et al. [24] proposed the HeteCF model by further considering the user-user similarity and user-item similarity. Yu et al. [25] designed the HeteRec model, which directly uses the meta-path similarity to enhance the user-item interaction matrix, therefore users/items can be represented more comprehensively. Subsequently, Yu et al. [26] designed the HeteRec model, which takes into account that the relevance of distinct paths depending on each user. To break through the limitation of meta-path expression ability, Zhao et al. [27] proposed the FMG model by substituting the meta-graph for the meta-path, which contains richer connectivity information than meta-path and captures the similarity between entities more accurately. In order to reduce the number of meta-path selections, Ma et al. [28] proposed the RuleRec model, which learns relations among connected items by exploring items’ connectivity.

GCN-based methods mainly focus on the information aggregation mechanism. Specifically, it incorporates information from the neighbor nodes to update the representations of ego nodes; When such propagation is performing recursively, information from the multi-hop nodes can be encoded in the embedding. Therefore, these methods are able to model high-order connectivity. For example, KGAT [6] connects user-item interactions and KG as a heterogeneous graph, and then employs the aggregation mechanism on it. Recently, inspired by the recent theories in simplifying GCNs [12], He et al. [10] verified that feature transformations and nonlinear activations of GCN have negative effects on recommendation performance, and proposed LightGCN that achieves the SOTA performance. Recently, CKAN [29] leverages two different strategies to propagate collaborative signals and knowledge awareness signals, respectively. And NIRec [30] is designed to merge path-based and GCN-based models, which propagate interaction patterns between two nodes through neighborhoods guided by meta-paths.

To sum up, embedding-based methods apply KGE methods to preprocess the KG for obtaining the embeddings of entities, and further integrate them into the recommendation framework. However, the informative connectivity of the KG is ignored in these methods, and few of them can explain the recommendation results. On the other hand, path-based methods utilize KG in a more natural and direct way, but they need massive manually designed meta-paths, which are very labor-intensive and time-consuming. GCN-based methods benefit from both the KGE techniques and the semantic path pattern. However, models based solely on GCN are prone to cause over-smoothing. Although some latest solutions can alleviate the problem by simplifying GCN, we still deem that they lack node information from other perspectives. We argue that integrating LPA into GCN can alleviate the over-smoothing. Therefore, we propose a new recommender model integrating LPA with GCN.

Method
We first introduce the recommendation problem, formulate the label propagate algorithm and graph convolutional network; then in the message propagation process, we explore the relationship between GCN and LPA, and prove the necessity of graph convolution operation to improve recommendation performance. Finally, according to the above theoretical guidance, we integrate LPA with GCN to design a new recommender model.

Problem formulation
The problem of recommender systems based on knowledge graphs is described as follows. Let 𝑈={𝑢1,𝑢2,…} and 𝑉={𝑣1,𝑣2,…} denote the sets of users and items, respectively. The user-item interaction matrix 𝑌={𝑦𝑢𝑣|𝑢∈𝑈,𝑣∈𝑉} is defined based on the users’ implicit feedback. We define the user–item interaction matrix as a

𝑦𝑢𝑣={1,ifinteraction(𝑢,𝑣)isobserved;0,otherwise.
(1)
here the value of 1 for 𝑦𝑢𝑣 denotes that there is an interaction between user u and item i; otherwise 𝑦𝑢𝑣=0. And we have a knowledge graph G available, which consists of massive entity-relation-entity triplets (h, r, t), where ℎ∈𝜙, 𝑟∈𝜑, and 𝑡∈𝜙 denote the head, relation, and tail of knowledge triple, respectively; 𝜙 and 𝜑 denote the set of entities and relations in G, respectively. Our goal is to predict the probability of each user clicking on any items. For example, in the movie recommendation scene, the problem formulation is shown in Fig. 1.

Fig. 1
figure 1
Top: A small example knowledge graph. Bottom: The learning objective is predicting the probability of 𝑢𝑖 clicking on 𝑚𝑖

Full size image
Label propagation algorithm
Label Propagation Algorithm (LPA) is based on the assumption that connected nodes are more likely to have similar labels. For a graph 𝐺=(𝑉,𝐴,𝑋,𝑃), where 𝑉={𝑣1,𝑣2,..,𝑣𝑛} denotes the set of nodes, A is the adjacency matrix, 𝑎𝑖𝑗 (the ij-th entry of A ) represents the weight of the edge between 𝑣𝑖 and 𝑣𝑗. N(v) is the set of direct neighbors of node v. P denotes the feature matrix of nodes and X is labels of nodes, in more detail, 𝑋(𝑘)=[𝑥1(𝑘),…,𝑥𝑛(𝑘)]𝑇 as the label matrix, where the i-th row 𝑥𝑖(𝑘)𝑇 represents the predicted label of node 𝑣𝑖 in iteration k. The initial label matrix 𝑋(0)=[𝑥1(0),…,𝑥𝑛(0)]𝑇 denotes one-hot label vectors (if node is labeled) or zero vectors (if node is not labeled). D is the diagonal degree matrix for A. And m represents the number of labeled nodes. Then LPA in step k is expressed as Eq. (2):

𝑋(𝑘+1)=𝐷(−1)𝐴𝑋(𝑘),𝑥𝑖(𝑘+1)=𝑥𝑖(0),∀𝑖≤𝑚.
(2)
𝑥𝑖∞=∑𝑗∈𝑁(𝑖)𝑎𝑖𝑗𝑥𝑗∞.
(3)
Equation (3) indicates that the final representation of the node is the weighted average of its neighbor nodes. In addition, Fig. 2 shows a demo of label propagation process.

Fig. 2
figure 2
A demo of label propagation process. Assume that labels are propagated for three steps. The blue nodes mean labeled, while the white nodes mean unlabeled. Information can only be propagated from labeled nodes to unlabeled nodes, and then the starting node 𝑣𝑎 is reset to its initial value for the next propagation, finally we can count all possible paths from node 𝑣𝑎 to node 𝑣𝑏

Full size image
Graph convolutional network
Graph convolutional network (GCN) is a multi-layer feedforward network, and it can propagates and transforms node features in the graph. Which can be formulated as: 𝑃(𝑘+1)=𝜎(𝐷−1/2𝐴𝐷−1/2𝑃𝑘𝑊(𝑘)), where 𝑊(𝑘) denotes weight matrix in the k-th layer, 𝜎() represents an activation function, and 𝑃(𝑘)=[𝑝1(𝑘),…,𝑝𝑛(𝑘)] are the k-th layer node representations with 𝑃(0)=𝑃. To be consistent with LPA, we treat 𝐷(−1)𝐴 as the normalized adjacency matrix instead of 𝐷−1/2𝐴𝐷−1/2. Thus, the feature propagation of GCN in the layer k can be formulated as:

𝑃(𝑘+1)=𝜎(𝐷−1𝐴𝑃𝑘𝑊(𝑘)).
(4)
Similar to LPA, the target node representation is the weighted sum of its neighbor nodes:

𝑝𝑖∞=∑𝑗∈𝑁(𝑖)𝑎𝑖𝑗𝑝𝑗∞
(5)
Relationship between LPA and GCN
Assume that two nodes 𝑣1 and 𝑣2 in a graph, 𝑣2 is labeled and 𝑣1 is unlabeled, we investigate the relationship between GCN and LPA from the perspective of influence. In particular, if the initial feature/label appears, how the output feature/label of 𝑣1 will change, and 𝑣2 will slightly change. Technically speaking, the feature/label influence is measured by the Jacobian/gradient of the output feature/label of 𝑣1 with respect to the initial feature/label of 𝑣2 [13]. 𝑝1(𝑘) denotes the k-th layer embedding of 𝑣1 in GCN, and 𝑝2 represents the initial embedding of 𝑣2. We calculate the feature influence of 𝑣2 on 𝑣1 as follows:

𝐼𝑓(𝑣1,𝑣2;𝑘)=||𝐸[∂𝑝1(𝑘)/∂𝑝2]||.
(6)
On the other hand, we compute the label influence of node 𝑣2 on node 𝑣1 in LPA as Eq. (7). Since LPA is an iterative algorithm, the influence of 𝑥2 on 𝑥1 is the cumulative influence as Eq. (8).

𝐼𝑙(𝑣1,𝑣2;𝑘)=∂𝑥1(𝑘)/∂𝑥2
(7)
𝐼𝑙(𝑣1,𝑣2;𝑘)=∑𝑗=0𝑘−1∂𝑥1(𝑘)/∂𝑥2(𝑗)
(8)
We set ReLU as the activation function of GCN, and 𝛽 denotes the fraction of node. Finally, we get that the feature influence and label influence satisfy the following equation:

𝐸[𝐼𝑙(𝑣1,𝑣2;𝑘)]=∑𝑗=1𝑘𝛽𝑗𝐼̃ 𝑓(𝑣1,𝑣2;𝑗).
(9)
The details of the proof of this equation can be referred to paper [31]. And in Eq. (9), 𝐼̃ 𝑓(𝑣1,𝑣2;𝑗) is the normalize feature influence, which can be computed as:

𝐼̃ 𝑓(𝑣1,𝑣2;𝑘)=𝐼𝑓(𝑣1,𝑣2;𝑘)∑𝑣𝑖∈𝑉𝐼𝑓(𝑣1,𝑣𝑖;𝑘),
(10)
Equation (9) proves that if 𝑣2 has a higher label influence on 𝑣1, the initial feature vector of 𝑣2 will also affect the output feature vector of 𝑣1 to a large extent. Intuitively, Eq. (9) indicates that when feature influence is required, label influence can be used instead. Therefore, LPA has the potential to replace GCN training edge weights.

Effect of graph convolution operation
For node 𝑣𝑖, the update formula of GCN as follows:

𝑝𝑖(𝑘+1)=𝜎⎛⎝⎜⎜∑𝑣𝑗∈𝑁(𝑣𝑖)𝑎𝑖𝑗𝑝𝑗(𝑘)𝑊(𝑘)⎞⎠⎟⎟.
(11)
Equation (11) can be decomposed into the following two steps:

(1)
In aggregation step, we compute the aggregated embedding 𝑠𝑖(𝑘) of neighborhoods 𝑁(𝑣𝑖):

𝑠𝑖(𝑘)=∑𝑣𝑗∈𝑁(𝑣𝑖)𝑎𝑖𝑗𝑝(𝑘)𝑗.
(12)
(2)
In transformation step, the aggregated embedding 𝑠𝑖(𝑘) is converted to the final representation:

𝑝𝑖(𝑘+1)=𝜎(𝑠𝑖(𝑘)𝑊(𝑘)).
(13)
We prove that through the aggregation step, the distance between the connected nodes in the embedding space can be reduced. We define 𝑄(𝑥)=12∑𝑣𝑖,𝑣𝑗𝑎𝑖𝑗||𝑝𝑖−𝑝𝑗||22 to be the distance metric between 𝑣𝑖 and 𝑣𝑗. Finally, we will have

𝑄(𝑠(𝑘))≤𝑄(𝑝(𝑘)).
(14)
The proof process is as follows. Firstly, we compute the Hessian of Q(p) as:

∇2𝑄(𝑝)=⎡⎣⎢⎢⎢⎢1−𝑎11−𝑎21⋮−𝑎𝑛1−𝑎121−𝑎22⋮−𝑎𝑛2⋯⋯⋱⋯−𝑎1𝑛−𝑎2𝑛⋮1−𝑎𝑛𝑛⎤⎦⎥⎥⎥⎥=𝐸−𝑄−1𝐴
(15)
It’s obvious that 2𝐸−∇2𝑄(𝑥)=𝐸+𝑄−1𝐴. Since 𝑄−1𝐴 is Markove matrix, its eigenvalues are within the range [−1,1], and the eigenvalues of 𝐸+𝑄−1𝐴 are within the range [0, 2]. Therefore, 𝐸+𝑄−1𝐴 is a positive semi-definite matrix, we have ∇2𝑄(𝑥)≤2𝐸. Then we can also deduce that

𝑄(𝑠(𝑘))==≤=𝑄(𝑝(𝑘))+∇𝑄(𝑝(𝑘))⊤(𝑠(𝑘)−𝑝(𝑘))+12(𝑠(𝑘)−𝑝(𝑘))⊤∇2𝑄(𝑥)(𝑠(𝑘)−𝑝(𝑘))𝑄(𝑝(𝑘))−∇𝑄(𝑝(𝑘))⊤∇𝑄(𝑝(𝑘))+12∇𝑄(𝑝(𝑘))⊤∇2𝑄(𝑝)∇𝑄(𝑝(𝑘))𝑄(𝑝(𝑘))−∇𝑄(𝑝(𝑘))⊤∇𝑄(𝑝(𝑘))+∇𝑄(𝑝(𝑘))⊤∇𝑄(𝑝(𝑘))𝑄(𝑝(𝑘)).
(16)
Equation (16) states that after an aggregation step, the total distance between connected nodes is reduced. In other words, the aggregation process combines together nodes that belong to the same class. As a result, GCN potentially categorize items that users are interested in into the same category, assisting the model in improving recommendation performance.

Unified model
We present the overall framework of GCNLP model, shown in Fig. 3. Firstly, we utilize the KG triples of <𝑢𝑠𝑒𝑟,𝑖𝑡𝑒𝑚,𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛> as the input, and assign them with initial embeddings. Secondly, we utilize the inner product of user embeddings and relation embeddings to indicate the importance of the relationship to each user, select neighbors for the target node according to the attention weight, and transform the KG into a weighted graph (this step is called attention layer in Fig. 3). Thirdly, we feed the weighted graph into GCN for training to generate new entity embeddings as item embeddings. At the same time, we perform label propagation on the weighted graph, get another item embeddings. Finally, we leverage the inner product of user embeddings and item embeddings to denote the final prediction. We will describe the architecture in detail in the following subsections.

Fig. 3
figure 3
An illustration of GCNLP model architecture. The first step is to select neighbors for the target node according to the attention weight, and transform the KG into a weighted graph. Secondly, we feed the weighted graph into GCN for training to generate a new entity embeddings as item embeddings. At the same time, we perform label propagation on the weighted graph, get another item embeddings. Finally, we leverage the inner product of user embeddings and item embeddings to denote the final prediction

Full size image
The early phase of the method is to convert the heterogeneous KG into weighted graphs to characterize users preferences. To this purpose, we introduce a user-specific relation-aware attention scoring function 𝑐𝑟𝑢, which provides the user u with the importance of relation r. According to the formula:

𝑐𝐫𝐮=𝐮T𝐫,
(17)
where 𝐮 and 𝐫 denote the user embeddings and relation embeddings. Intuitively, 𝑐𝑟𝑢 denotes the importance of relation r to user u. The original knowledge graph is an unweighted graph, where the edges only represent the relationship type, and the weights are not displayed. After introducing 𝑐𝑟𝑢, the original knowledge graph will be converted into a weighted graph. In the real scene, the scale of the weighted graph is usually very large. To alleviate the computational burden, we introduce receptive fields to determine the number of nodes. In particular, we uniformly sample a set of neighbors of a fixed-size k for each entity, where the priority of neighbor selection always choose fixed k neighbors of the highest weights. The neighborhood of v is defined as

𝑣𝑢𝑁(𝑣)=∑𝐞∈𝑁(𝑣)𝑐̂ 𝐫𝐮𝐞,
(18)
where 𝐞 denotes embedding of entity e, N(v) denotes the selected neighbor set of v, and 𝑐̂ 𝑟𝑢 is the normalized user-relation score as

𝑐̂ 𝐫𝐮=𝑒𝑥𝑝(𝑐𝐫𝐮)∑𝑒∈𝑁(𝑣)𝑒𝑥𝑝(𝑐𝐫𝐮),
(19)
We can also treat the weighted graph as an adjacency matrix 𝐴𝑢, where the (i, j)-entry 𝐴𝑢𝑖𝑗=𝑐𝑢(𝑟𝑒𝑖,𝑒𝑗), and 𝑟𝑒𝑖,𝑒𝑗 represents the relation between entities 𝑒𝑖 and 𝑒𝑗 in KG. We define the original feature matrix of entities as 𝐸∈𝑅|𝜙|×|𝑑0|, where 𝑑0 is the dimension of original entity features. Then, we leverage multiple network layers to update the entity representation. In particular, the layer-wise propagation can be formulated as

𝐻1𝐻2…𝐻𝑙=𝜎(𝐷−1/2𝑢𝐴𝑢𝐷−1/2𝑢𝐻0𝑊0)=𝜎(𝐷−1/2𝑢𝐴𝑢𝐷−1/2𝑢𝐻1𝑊1)=𝜎(𝐷−1/2𝑢𝐴𝑢𝐷−1/2𝑢𝐻𝑙−1𝑊𝑙−1),
(20)
where 𝐻𝑙 denotes the matrix of representations of entities in layer l, each row is the embedding of each item denoted as ℎ𝑙, and 𝐻0=𝐸, 𝐴𝑢 represents the weighted sub-graph mentioned above. 𝐷𝑢 is a diagonal degree matrix with entries 𝐷𝑖𝑖𝑢=∑𝑗𝐴𝑖𝑗𝑢, therefore, 𝐷−1/2𝑢 is used to normalize 𝐴𝑢 and maintain the stability of entity representation matrix 𝐻𝑙. 𝑊𝑙∈𝑅𝑑𝑙×𝑑𝑙+1 denotes the layer-specific weight matrix, 𝜎 is a nonlinear activation function, and l represents the number of layers. The GCN prediction is defined as the inner product of user and the final representation of items:

𝑦̂ 𝑢𝑣=𝑢Tℎ𝑙,
(21)
where u represents the user embedding, which is obtained from the user id through the embedding lookup layer, and 𝑦̂ 𝑢𝑣 represents the ranking score for recommendation generation.

Unlike traditional GCN-based methods, which take edge weights of the input data as constants, edge weights 𝐷−1/2𝑢𝐴𝑢𝐷−1/2𝑢 in Eq. (20) are trainable in our model since we introduce the user-relation score function Eq. (17). Though this operation strengthens the model’s fitting power, it also makes the optimization process increasingly prone to cause over-smoothing, because the sole source of supervised signal is user-item interactions. Furthermore, edge weights play a crucial role in graph representation learning. As a result, more regularization on edge weights is required to facilitate the training of entity representations, and more likely to seek items that interest users. We demonstrate that LPA has the potential to replace GCN training edge weights in section 3.4. Therefore, we update 𝐴𝑢 with the LPA algorithm, and use it to regularize edge weights for better performance.

Now we take out an item v and remove its label, and predict the unmarked item by using the remaining labeled items. The prediction process is shown as Eq. (2), the Label Propagation prediction is also defined as the inner product of user and item:

𝑙̂ 𝑢𝑣=𝑢T𝑦,
(22)
then we update the edge weight of 𝐴𝑢 by reducing the difference between the label 𝑦𝑢𝑣 of the real item v and the predicted label 𝑙̂ 𝑢𝑣.

The formulation of the above steps as shown in Algorithm 1. H represents the depth of receptive field. For each user-item pair (line 2), we compute the receptive field M of v (line 3, 21-29), and obtain the neighborhood representation(line 4). Then we feed them into GCN (line 5) and finally get 𝑦̂ 𝑢𝑣 (line 10). The calculation method of LPA is similar to GCN, and finally obtain 𝑙̂ 𝑢𝑣 (line 20). Unifying GCN and LPA, we get the following complete loss function:

𝑙𝑜𝑠𝑠=∑𝑢,𝑣𝐽(𝑦𝑢𝑣,𝑦̂ 𝑢𝑣)+𝜆(∑𝑢,𝑣𝐽(𝑦𝑢𝑣,𝑙̂ 𝑢𝑣))+𝛾‖𝜃‖22,
(23)
where J is the cross-entropy loss function, ‖𝜃‖22 is the L2-regularizer, 𝜆 and 𝛾 are balancing hyper-parameters. In Eq. (23), the first term corresponds the part of GCN that learns the edge weights A and the trainable matrix W. While the second term corresponds the part of label propagation that can be treated as adding constraint on edge weights A. We can regard the second term as a regularization on A to assist GCN in learning edge weights.

figure a
Time complexity analysis
As can be seen, the layer-wise propagation is the core operation. For the l-th layer, the computational complexity of matrix multiplication is 𝑂(|𝑅+|𝑑𝑙𝑑𝑙−1), where 𝑅+ represents the training dataset involving the observed interactions, 𝑑𝑙 and 𝑑𝑙−1 denote the current and previous transformation size. For the prediction layer and the attention layer, the time complexity of inner product is 𝑂(∑𝐿𝑙=1|𝑅+|𝑑𝑙). As a result, the overall time complexity of our proposed model is 𝑂(∑𝐿𝑙=1|𝑅+|𝑑𝑙𝑑𝑙−1+2∑𝐿𝑙=1|𝑅+|𝑑𝑙).

Experiments
In this section, we conduct extensive experiments to evaluate our proposed algorithm. We aim to answer the following research questions:

RQ1
How does our GCNLP algorithm perform compared with other state-of-the-art recommendation algorithms?

RQ2
How do different components (LPA, different GCNs) affect the performance of GCNLP.

RQ3
How does our proposed GCNLP algorithm perform in the data sparse scenarios?

Evaluation datasets
To evaluate the effectiveness of the GCNLP algorithm, we conduct comprehensive experiments on three benchmark datasets: Movilens-1M, Last-FM, and Book-Crossing, respectively.

Movielens-1MFootnote1: This movie rating dataset has been widely used to evaluate recommendation algorithms. We employ the version containing one million ratings, where each user has at least 20 ratings.

LastFMFootnote2: This dataset contains musician listening information from a set of 2 thousand users from LastFM online music system, which provides the listening count as weight for each user-item interaction.

Book-CrossingFootnote3: This dataset contains 1 million ratings (ranging from 0 to 10) of books in the book-crossing community.

Since the above three datasets are explicit feedback, we convert them to implicit data, where each item is marked as 0 or 1, indicating whether the user interacts with the item. In addition to the user-item interactions, we also need to construct item knowledge for each dataset. In addition to the user-item interaction data, we also construct item-side KG for each dataset. Specially, we leverage Microsoft Satori to construct the knowledge graph for each dataset. We first sample a subset of triples from the entire KG. For the sub-graph, we match the dataset with triples tail. Then, we match the product ID to the beginning of all triples and sample all well-matched triples. The basic statistics of the three datasets are shown in Table 1.

Table 1 Statistics of the three datasets
Full size table
Experimental settings
Evaluation metrics
For each user, we randomly hold out one item that the user has interacted with (user liked), and sample 100 unobserved (user disliked) or negative items to form the test sets. The performance of the rank-list is judged by Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). HR is a way of calculating how many “hits” a user has in an k-sized list of ranked items, and it is formulated as

𝐻𝑅@𝐾=𝑁𝑢𝑚𝑏𝑒𝑟𝑜𝑓𝐻𝑖𝑡𝑠@𝐾|𝐺𝑇|,
(24)
where GT is the collection of all test datasets. NDCG accumulated at a particular rank position K, and it highlights the retrieval of relevant results. The metric of DCG is defined as

𝐷𝐶𝐺@𝐾=∑𝑖=1𝑁2𝑟𝑒𝑙𝑖−1log2(𝑖+1),
(25)
where 𝑟𝑒𝑙𝑖 is the graded relevance of the recommendation result at position i. Then the NDCG@K of user u is defined as

𝑁𝐷𝐶𝐺𝑢@𝐾=𝐷𝐶𝐺𝑢@𝐾𝐼𝐷𝐶𝐺𝑢,
(26)
where 𝐼𝐷𝐶𝐺𝑢 represents the best recommendation list returned by a user. Then the average metric of NDCG is defined as

𝑁𝐷𝐶𝐺@𝐾=∑𝑢∈𝑈𝑁𝐷𝐶𝐺𝑢@𝐾𝐼𝐷𝐶𝐺𝑢.
(27)
Additionally, to evaluate the homogeneity of user preferences, the metric of Coverage is defined as

𝐶𝑜𝑣𝑒𝑟𝑎𝑔𝑒@𝐾=|𝑈𝑢∈𝑈𝑅(𝑢)||𝐼|,
(28)
where I represents the total item set, U is the user set, and R(u) indicates that the recommender system recommends an item list of length K to the user u.

Baselines
We implement our GCNLP algorithm using the python library of tensorflowFootnote4, and compare it with the following baselines:

BPRMF [18]: This method optimizes the MF model with a pairwise ranking loss, which is tailored to learn from implicit feedback. We employ a fixed learning rate and change other parameters to report the best results.

NeuMF [32]: It is a typical recommendation algorithm based on deep learning. It combines traditional matrix factorization and multi-layer perceptron, which can extract low-dimensional and high-dimensional features at the same time, and has a good recommendation effect.

RippleNet [33]: This is an end-to-end framework that utilize KG for recommendation. RippleNet adopts the preference propagation in KG to continuously and automatically discover users’ potential hierarchical interests.

NGCF [7]: This method uses the bipartite neural network to encode the historical interaction information of user-item into embedding to improve the recommendation effect. More importantly, NGCF explicitly considers the high-level connectivity between user-items to further enhance the representation ability of embedding.

LRGCN [11]: This method eliminates the nonlinearity in the GCN, thus simplifying the network structure and introduced a residual network structure to alleviate the problem of over-smoothing. Which has achieved a substantial improvement recommended accuracy on NGCF.

Parameter settings
In our experiments, we randomly select 70% of each dataset as training set, the rest 30% as the test set. We consider latent dimension d from {2,4,6,8,16,32} for all models. For baseline methods, the model parameters are set according to settings mentioned in the author’s paper. For GCNLP model, the batchsize and learning rate (lr) for the three datasets are set to: 𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒=2048, 𝑙𝑟=0.02 on Movielens-1M dataset; 𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒=128, 𝑙𝑟=0.0005 on LastFM dataset; 𝑏𝑎𝑡𝑐ℎ𝑠𝑖𝑧𝑒=256, 𝑙𝑟=0.0002 on Book-Crossing dataset. We set the size of neighbor and the number of hops as: 8 and 4 for Movielens-1M and Book-Crossing; 4 and 2 for LastFM.

Performance comparison(RQ1)
Figure 4 shows the performance (HR@10 and NDCG@10) of all the competitive algorithms with respect to different numbers of latent dimension d. We test the values of dimension as [2, 4, 6, 8, 16, 32], respectively. With the latent dimension changes, the performance of all models fluctuates within a small range. From Fig. 4, it is easy to see that GCN-based models perform better than other models. It is worth mentioning that LRGCN is a strong baseline which beats RippleNet and NGCF on the three datasets. GCNLP and LRGCN algorithms are superior to all other algorithms on adopted three datasets. On the Book-Crossing dataset, our GCNLP algorithm is weaker to the LRGCN algorithm in HR@10 metric, but our GCNLP algorithm performs better than the LRGCN algorithm in NDCG@10 metric. In the metric of NDCG@10, the GCNLP and LRGCN algorithms are significantly outperform all other algorithms on LastFM and BookCrossing datasets. And in the metric of HR@10, GCN-based models perform significantly stronger than other models on LastFM datasets. To sum up, compared with the other baselines, our GCNLP algorithm has achieved the best experimental results on the three datasets.

Fig. 4
figure 4
The HR@10 and NDCG@10 performance of our GCNLP algorithm compared with other baselines on three datasets, with latent dimension ranging from 2 to 32

Full size image
Graph convolution operation is easy to cause over-smooth, and homogenizes user preferences. Figure 5 shows the performance (Coverage@10) of GCN-based models on adopted three datasets. We can find that GCNLP and LRGCN always perform best on the three datasets, which means that they perform better in alleviating the homogeneity of user preferences. The Coverage@10 of GCNLP on the three datasets are 0.362, 0.462, 0.493, respectively. Compared with the Coverage@10 of LRGCN on the three datasets are 0.354, 0.422, 0.472, our algorithm has increased by 2.9%, 9.5%, and 4.2%, respectively. To sum up, compared to other GCN-based baselines, our GCNLP algorithm always performs best on the three data sets, which fully reflects the superiority of our algorithm.

Fig. 5
figure 5
Results of Coverage@10 on the three datasets

Full size image
Figure 6 shows the performance of Top-K recommended lists where the ranking position K ranges from 1 to 10. It can be seen, GCNLP and LRGCN algorithms demonstrate consistent improvements over other methods across positions. We further conducted one-sample paired t-tests, verifying that all improvements are statistically significant for 𝑝<0.01. For other baseline algorithms, we can see that the gap between these algorithms is very small, and their performance on the three datasets is comparable. It’s worth pointing out that the NDCG value of the GCN-based model is significantly better than other models on the BookCrossing dataset. It can be seen that fusion of auxiliary information is very effective in improving the accuracy of the recommendation system.

Fig. 6
figure 6
Evaluation of Top-K item recommendation where K ranges from 1 to 10 on the three datasets

Full size image
In this section, we compare the GCNLP algorithm to several state-of-the-art baselines on Movielens-1M, LastFM and Book-Crossing datasets. From the experimental results of Figs. 4,5 and 6, our GCNLP algorithms typically cope better in almost all cases. In particular, the best baseline is consistently LRGCN algorithm. However, it still underperforms our GCNLP algorithm in most cases. The key reason is that the LRGCN only by simplifying GCN (i.e., reduces the number of network layers and removes nonlinear transformations) to improve performance, where the adjacency matrix A is determined by the user-item interaction matrix, thus A is always a constant. As a result, LRGCN fails to consider the importance of users to various relationships. In contrast, in the GCNLP algorithm, the (i, j)-entry 𝐴𝑢𝑖𝑗=𝑐𝑢(𝑟𝑒𝑖,𝑒𝑗) by learning edge weights to characterize the importance of users to various relationships. In particular, we leverage these embeddings as attention weights of users and items, and aggregate them to accurately capture user preferences, which shows that our algorithm combining KG and the attention mechanism is effective in improving the recommendation performance. On the other hand, our model combines the label propagation algorithm to alleviate the over smooth problem caused by GCN, so compared with the model that only uses GCN, the performance of our model is significantly improved. It is also worth mentioning that the HR@10 of LRGCN on Book-Crossing is higher than that of GCNLP, the main reason is that the types of relations we set on Book-Crossing are not enough (see Table 1), so that GCNLP’s performance on Book-Crossing is limited to some extent.

Ablation studies(RQ2)
We conduct ablation studies on GCNLP by showing how do the LPA and different GCNs affect its performance. In addition, we seek to explain why these designs are so efficient.

The influence of LPA on the GCNLP
In order to further study the influence of the label propagation algorithm (LPA) on GCNLP, we have done a lot of ablation experiments, removing the LPA of the model and comparing it with GCNLP.

Figure 7 shows the HR@10 performance of GCNLP compared with GCN. We can clearly see that GCNLP always performs better than GCN on the three datasets. The HR@10 of GCNLP on the three datasets are 0.660, 0.617, 0.406, respectively. Compared with the HR@10 of GCN on the three datasets are 0.622,0.582, 0.372, GCNLP has increased by 5.9%, 6.1%,and 9.1%, respectively.

Fig. 7
figure 7
The HR@10 performance of GCNLP compared with GCN on the three datasets

Full size image
Figure 8 shows the NDCG@10 performance of GCNLP compared with GCN. We have almost the same conclusion, GCNLP always performs better than GCN on the three datasets. The NDCG@10 of GCNLP on the three datasets are 0.388, 0.391, 0.340, respectively. Compared with the NDCG@10 of GCN on the three datasets are 0.355, 0.360, 0.291, GCNLP has increased by 9.2%, 8.3%,and 15.1%, respectively.

Fig. 8
figure 8
The NDCG@10 performance of GCNLP compared with GCN on the three datasets

Full size image
Figure 9 shows the Coverage@10 performance of GCNLP compared with GCN. As we expected, GCNLP always performs better than GCN on the three datasets. The Coverage@10 of GCNLP on the three datasets are 0.362, 0.463, 0.493 respectively. Compared with the Coeverage@10 of GCN on the three datasets are 0.293, 0.370, 0.385, GCNLP has increased by 23.5%, 24.4%,and 28.0%, respectively.

Fig. 9
figure 9
The Coverage@10 performance of GCNLP compared with GCN on the three datasets

Full size image
To sum up, for HR@10 and NDCG@10, the improvement of GCNLP compared with GCN is almost within 10%. Only in the BookCrossing dataset, the improvement rate of NDCG@10 is 15.1%. But for Coverage@10, the improvement rate of GCNLP compared with GCN exceeds 20% on the three datasets. This means that LPA not only has a certain degree of positive impact on improving the accuracy of the model, but also has a significant increase in the coverage rate of the model, and can significantly alleviate the over smoothing caused by GCN.

The influence of different GCNs on recommender model
To investigate the influence of different GCNs on recommender model, we also conduct experiments combining traditional GCN and LPA. It is worth noting that in the traditional GCN-based method, the adjacency matrix A is directly determined by the user-item interaction matrix Y, which as shown in Eq. (29), and the rest parts of the model remain the same. We abbreviate the combination model as T-GCNLP. For fair comparison, we fix all hyper-parameters (e.g., size of embedding, learning rate and batchsize, etc.). And we report the results of HR@20 and NDCG@20 on the three benchmark datasets in Table 2.

𝐴=(0𝑌𝑇𝑌0)
(29)
It is shown in Table 2 that compared to T-GCNLP, our model has a significant improvement. Intuitively, this is due to the fact that our GCN is superior to traditional GCN. The key reason is that in our model, the (i, j)-entry 𝐴𝑢𝑖𝑗=𝑐𝑢(𝑟𝑒𝑖,𝑒𝑗) by learning edge weights to characterize the importance of users to various relationships. However, in traditional GCN, A is always a constant, the relations between entities are not explicitly modeled, and fails to consider the fact that users attach different degrees of importance to various relations of items. As a result, GCNLP performs more effective than T-GCNLP.

Table 2 The comparison of performance (HR@20 and NDCG@20) between T-GCNLP and GCNLP
Full size table
Sparse scenarios analysis (RQ3)
Table 3 shows the experimental results with different training set (10% to 90%) ratios on Movielens-1M, the rest datasets as test set, while keeping other parameters fixed. We omit the results on LastFM and Book-Crossing because we reached the same conclusion: Integrating KG into recommender models can alleviate the data sparsity to a certain extent. From Table3, we observe that the performance of all algorithms deteriorates with the reduce of the training set. When r=10%, HR@20 decreases by 7.6%, 7.2%, 4.5%, 4.2%, 4.4%, 5.2% for BPRMF, NeuMF, RippleNet, NGCF, LRGCN, and GCNLP, respectively, compared with the case ofr=90%. This experimental result shows that even in the case of sparse user-item interaction, the performance of the GCN-based model is far superior to other algorithms. We also notice that the GCNLP algorithm is more sensitive to the density of user-item interactions, but in general, it performs better than the LRGCN algorithm in data sparse scenarios.

Table 3 Results of HR@20 on MovieLens-1M with different ratios of training set
Full size table
Overall, the data sparsity is a critical challenge in the recommendation scenario. If we solely utilize user ratings as input data, it is tough to use more information to recommend items that satisfy users in data sparse scenarios. Leveraging KG as auxiliary information is a effective method to improve performance in data sparse scenarios. The KG is useful solution to address this issue due to it provides rich semantic information. By comparing the experimental results of our GCNLP algorithm under the data sparse scenarios, it is shown that the GCNLP algorithm is effective in addressing the data sparsity to large extent.

Conclusion
In this work, we developed a recommender model unifying graph convolutional networks and label propagation algorithms. Our key argument is that GCN-based methods benefit from both the semantic embedding of KG and the semantic path pattern, which naturally fit the process of embedding propagation. However, existing GCN-based methods are easy to cause over-smoothing, and make the preference of different users become homogeneous. To address this problem, we utilize the label propagation algorithm to assist the GCN model to regularize edge weights. In addition, we introduce an attention network to capture the attention weight of each user-item pair, which takes into account the fact that users attach different degrees of importance to the various relationships of items. We conduct comprehensive experiments on three public datasets to demonstrate the effectiveness of our GCNLP algorithm. Experimental results show that our algorithm outperforms state-of-the-art algorithms.

As future work, we are particularly interested in exploring dynamic recommendation. Although the GCN-based recommendation model has achieved prominent progress due to the remarkable ability on representation learning, the training process is still time-consuming. In many circumstances,, such as online shopping, news recommendations, and forums, users’ preferences may be affected by social activities in a short period of time [34]. In this case, recommendations using static preference modeling may not capture the user’s current interest. An effective way to solve this problem is to explore group interests and exploit the connections between group users to collect a comparably compact candidate set of potential media-user pairs [35]. Another solution is to leverage a dynamic graph network to capture users’ ever-changing preferences by combining long term and short term interests of friends [36]. As a result, we deem that integrating other sorts of auxiliary information and from the KG for dynamic recommendation is the future trend.

Keywords
Recommender system
Over-smoothing problem
Graph convolutional networks
Label propagation algorithms
