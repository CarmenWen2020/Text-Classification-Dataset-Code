Relation extraction aims to identify semantic relations between entities in text. In recent years, this task has been extended to the joint extraction of entities and relations, which requires the simultaneous identification of entities and their relations from sentences. However, existing methods, limited by the existing tagging scheme, fail to identify more complex entities, which in turn limits the performance of the joint extraction task. This article presents a joint extraction model for entities and relations called MLRA-LSTM-CRF that uses multi-label tagging and relational alignment to transform this task into a multi-label tag recognition problem. The proposed model first tags the entities and their relations according to the multi-label tagging scheme and then uses a joint entity and relation extraction module with a multi-layer attention mechanism to extract the triplets in the sentence. Finally, the relational alignment module is used to align the predicted relation classification results. Experimental results on the New York Times and Wiki-KBP datasets indicate that MLRA-LSTM-CRF is significantly better than that of several state-of-the-art models and baseline.

Introduction
The extraction of entities and their semantic relations is the core task of information extraction (IE) [1]. The main goal of IE is to detect entities from text and identify the semantic relations between them. IE can support many downstream applications, such as search engines and question answering systems. Existing methods can be divided into two categories: 1) pipeline methods and 2) joint extraction methods. A pipeline method first identifies entity mentions using named entity recognition (NER) [2, 3], and then uses relation classification (RC) to extract the relations between the entity mentions [4, 5]. This separated framework is flexible, which facilitates task handling. However, it ignores the correlation between the NER and RC subtasks, which leads to erroneous delivery [6].

In contrast to pipeline methods, joint extraction methods integrate the information of entities and relations and reduce the propagation of error. Most joint extraction methods are based on feature [7,8,9] or neural networks (NNs) [10,11,12,13,14,15]. Feature-based methods rely on complicated feature engineering, and it is difficult for these methods to exploit global features. In contrast, NN-based methods can automatically learn non-local features. However, NN-based methods first identify the entities and then classify the relations. In this approach, there is a large amount of redundant information for entities that have no relation. To address the issue of redundant information, Zheng et al. [16] proposed a tagging scheme to convert the joint extraction task into a tagging problem. The granularity of their tagging schemes has become smaller, ranging from the early IO and BIO tagging scheme [17] to the current BIOES tagging scheme [16]. Using the tagging scheme, they employ an end-to-end model [18] to jointly extract entities and relations. However, these models fail to extract relations when the tagging scheme is limited, and the entity is relatively complex.

Fig. 1
figure 1
Standard example sentences extracted from the Wiki-KBP and New York Times (NYT) datasets using the joint extraction of entities and relations

Full size image
In practice, entities in a sentence can be in one of three situations: 1) flat, 2) nested, or 3) overlapping. A flat entity situation assumes that only one label is needed for each entity and the problem of relation extraction can be solved using the existing tagging scheme. The latter two situations are often more complicated, as shown in Fig. 1. First, one entity is nested with another entity. In Fig. 1 for example, “Dale Earnhardt Jr.” and “Dale Earnhardt” in the first sentence are nested. Second, an entity can appear in multiple triplets. For example, the second sentence contains three entities: “Bobby Fischer,” “Reykjavik,” and “Iceland,” which can appear in different triplets. It is unreasonable to use one label to mark every instance of an entity. In addition, the equivalence relationship problem may reduce the accuracy of triplet extraction. For example, (“India,” “Bihar”) has two equivalence relations, “contains” and “administrative_divisions.” If the extraction model predicts that the relation of the entity pair is “contains” whereas the true value of the test set is “administrative_divisions,” this prediction result will be considered incorrect. However, these two relations are semantically equivalent, and if predicted, either of them should be considered correct.

To solve the above problem, this article presents a joint extraction model using multi-label tagging and relational alignment, namely MLRA-LSTM-CRF. Specifically, we model the task as a multi-label tagging problem. The model first identifies all possible entities through the multi-label tagging schema. Then, a multi-layer attention (MLA) model is used to learn the correlation between entities and their relations. Finally, relational alignment is used to improve the accuracy of triplet extraction. Experiments on two public datasets demonstrated that MLRA-LSTM-CRF can substantially outperform baseline systems, achieving state-of-the-art performance. In summary, the contributions of this article are threefold.

A multi-label tagging scheme is proposed for labeling each word in a sentence. Its aims to be able to distinguish flat entities, nested entities, and overlapping entities.

Based on the multi-label tagging scheme, an MLA mechanism is incorporated into the joint extraction module to extract triplets in a sentence. The encoding layer attention learns the dependencies within the sentence, and the decoding layer attention decodes word information and extracts candidate entities related to the target relation, thus reducing the interference of irrelevant entities.

A relational alignment module is introduced for gathering all equivalent relations from a relation set. Its aims to align the equivalence relations in the triplets and improve the accuracy of triplet extraction.

The remainder of the article is organized as follows. Section 2 briefly reviews related work. Section 3 presents our proposed MLRA-LSTM-CRF model for the joint extraction of entities and relations. Section 4 describes the experimental settings and analyzes the experimental results. Finally, Section 5 concludes the paper and discusses directions for future work.

Related work
In this section, we provide a brief review of the pipeline and joint extraction methods. How the related work motivates our developed model is also discussed.

Pipeline method
Pipeline methods decomposes the extraction of entities and relations into two separate subtasks: NER [2] and RC [4].

Named entity recognition The classic NER model is a linear statistical model. For example, hidden Markov model (HMM) [19] and conditional random fields (CRF) [20] are NER models that consider the correlations among entity tags. NN models are also used to perform the NER task. For example, Collobert et al. [21] used Convolutional Neural Networks (CNNs) combined with a CRF to extract related features at the word and sentence levels to complete the NER task. However, the training of NNs typically requires large quantities of sample data. A small amount of data can easily lead to overfitting of the model and a lack of generalization ability. To address this problem, the attention mechanism [3, 22] was proposed. For example, Rei et al. [22] used an attention mechanism to improve the splicing of word vectors and character vectors to improve the NER performance. Fei et al. [3] proposed a dispatched attention-based NN model with multi-task learning for detecting nested entities.

Relation classification RC can be approached using two types of methods, handcrafted feature-based methods, and NN-based methods. Handcrafted feature-based methods [4, 23] are relatively simple and easy to implement. However, they require complex feature engineering processing and heavily depend on natural language processing (NLP) tools. To reduce the manual effort involved in feature extraction, NN-based methods include RNN-based methods [24,25,26,27], CNN-based methods[28,29,30], and combined methods [31] have been used. NN-based methods can learn related features in an unsupervised manner from a given sentence without the need for complicated handcrafted feature engineering. However, these methods fail to fully utilize the position and semantic information of an entity in a sentence, resulting in low RC accuracy. Recently, end-to-end NNs with attention mechanisms [5, 32] have been proposed to improve RC performance.

The pipeline method uses a separate framework and treats each subtask as an independent model, making the relation extraction easy to handle. However, it ignores the correlation between the two subtasks. Moreover, the pipeline method will generate entities without relations under this framework, which affects the performance of entity and relation extraction.

Joint extraction method
The joint extraction method considers the latent semantic features and relevance of entities and relations, and the extraction performance is superior to that of the pipeline method [33]. Joint extraction methods can be subdivided into feature-based methods and NN-based methods [7,8,9]. Traditional methods mainly use feature-based methods, their performance heavily relies on complex feature engineering, and it is difficult for them to use global features. In contrast, the NN-based end-to-end method [10, 12] automatically learns global features and obtains better experimental results. For example, Miwa and Bansal [10] captured word sequence and dependency tree substructure information by stacking a bidirectional tree-structured LSTM on bidirectional sequential LSTM units. Zheng et al. [12] used the bidirectional LSTM (Bi-LSTM) for feature extraction in the encoding layer, LSTM to extract entity features, and a CNN to extract relational features in the decoding layer. In a subsequent study, end-to-end NNs and attention mechanisms were also introduced into the joint learning task. Katiyar and Cardie [34] proposed an attention mechanism and Bi-LSTM to obtain richer contextual information for the joint extraction of entities and relations.

NN-based methods share parameters through the underlying model. Because both tasks update the shared parameters through backward propagation during model training, this method incorporates the dependencies between the two tasks. However, this method separates the two subtasks of NER and RC and still generates redundant information. To resolve this problem, A method based on sequence tagging has been proposed [15, 16, 35]. For example, Zheng et al. [16] proposed a tagging scheme and an end-to-end model with a biased objective function to jointly extract entities and their relations; Tan et al. [35] found multiple triplets in sentences through a transfer mechanism and sorting, and Dai et al. [15] proposed a unified position-attentive sequence labeling framework for the joint extraction of entities and relations. The framework of these joint learning methods combines NER and RC tasks within a unified model, which effectively aggregates entity and relation information and obtains better extraction results. However, these models fail to achieve satisfactory performance when the tagging scheme is limited and the entities are relatively complex.

Inspired by these methods, this article presents a joint entity and relation extraction model that uses multi-label tagging and relational alignment. In our proposed model, a multi-label tagging scheme is adopted for tagging entities and relations simultaneously, a joint entity and relation extraction model fused with an MLA mechanism is introduced for extracting triplets in sentences, and a relational alignment module is introduced for aligning the prediction results of the relations.

Proposed method
We first define the joint entity and relation extraction problem. Given a sentence 𝑆=(𝑤1,𝑤2,…,𝑤n) and a set of predefined relations =(𝑟1,𝑟2,…,𝑟𝑛), the aim of the task is to extract the relation 𝑟𝑒𝑖,𝑒𝑗∈ of the entity pair (𝑒𝑖,𝑒𝑗) from the sentence.


In this section, we introduce our proposed model MLRA-LSTM-CRF in detail, which comprises three key modules: (1) a tagging module, (2) an extraction module, and (3) an alignment module.

Fig. 2
figure 2
Overview of the proposed MLRA-LSTM-CRF for the joint extraction of entities and relations. It consists of a tagging module, an extraction module, and an alignment module

Full size image
The workflow of MLRA-LSTM-CRF is illustrated in Fig. 2, and consists of the following three steps:

Step 1: For the input sentence sequence containing entity pairs, a multi-label tagging scheme is used to assign one or more labels to each word in the sentence to obtain the tag sequence.

Step 2: The joint entity and relation extraction model, with an MLA mechanism, is used to make a preliminary prediction of the triplets in the tag sequence.

Step 3: The relation set formed by the relation embedding is used to align the relation extracted by the extraction module to ensure that the relation of the entity pair is accurate.

The design and implementation details of the submodules in MLRA-LSTM-CRF are introduced in the following subsections.

Tagging module
The traditional tagging scheme [16] assumes that each word has a label that consists of three parts: (1) a relation type, (2) an entity role, and (3) an entity position.

Relation type is obtained from a set of predefined relations. For example, when a (person, location) entity pair and a residence relation are available, we can use “𝑝𝑙𝑎𝑐𝑒_𝑙𝑖𝑣𝑒𝑑” to represent the relation between the entity pairs.

Entity role is represented by “𝐸1” and “𝐸2,” where “𝐸1” indicates that the word belongs to entity 1, and “𝐸2” indicates that the word belongs to entity 2.

Entity position used BIOES to identify the position of each word in the entity, where “B” means the word is located at the beginning of the entity, “I” means the word is located in the middle of the entity, “O” indicates a non-entity, “E” means the word is located at the end of the entity, and “S” indicates a single-word entity.

However, the above-mentioned tagging scheme cannot effectively identify an entity when the existence of the entity is relatively complex. In practice, entities may be nested within each other [36], and entities may also appear in multiple triplets. As shown in Fig. 3, we found that “Dale Earnhardt Jr.” overlaps with “Dale Earnhardt.” According to the tagging scheme of Zheng et al. [16], only one entity in the sentence can be assigned a label, which makes the model unable to effectively identify the other entity. This requires the model to have a strong ability to capture entities of any length. Second, an entity may appear in different triplets. As shown in Fig. 4, for a sentence that contains three entities and four predefined relation types, each entity may appear in a different triplet. According to the tagging scheme of Zheng et al. [16], each word in this sentence can only be assigned one label. However, these words have different semantics in different triplets. Therefore, this requires the model to have a strong ability to capture the dependencies between different entities.

To solve the above problems, we integrate the multi-label tagging scheme into our model for sequence tagging. For example, in Fig. 3, “Dale Earnhardt Jr.” and “Dale Earnhardt” have the relation type “parents,” and “Dale Earnhardt” and “Dale Earnhardt Jr.” have the relation type “children.” Therefore, the labels of the word “Dale” in the nested entity “Dale Earnhardt” are set to “𝑝𝑎𝑟𝑒𝑛𝑡𝑠_𝐸2𝐵” and “𝑐ℎ𝑖𝑙𝑑𝑟𝑒𝑛_𝐸1𝐵,” and the labels of “Earnhardt” are set to “𝑝𝑎𝑟𝑒𝑛𝑡𝑠_𝐸2𝐼” and “𝑐ℎ𝑖𝑙𝑑𝑟𝑒𝑛_𝐸1𝐿.” The labels of the word “Dale” in the nested entity “Dale Earnhardt Jr.” are set to “𝑝𝑎𝑟𝑒𝑛𝑡𝑠_𝐸1𝐵,” and “𝑐ℎ𝑖𝑙𝑑𝑟𝑒𝑛_𝐸2𝐵,” the labels of ‘Earnhardt” are “𝑝𝑎𝑟𝑒𝑛𝑡𝑠_𝐸1𝐼” and “𝑐ℎ𝑖𝑙𝑑𝑟𝑒𝑛_𝐸2𝐼,” and the labels of “Jr.” are “𝑝𝑎𝑟𝑒𝑛𝑡𝑠_𝐸1𝐿” and “𝑐ℎ𝑖𝑙𝑑𝑟𝑒𝑛_𝐸2𝐿.” In Fig. 4, “Bobby Fischer” and “Iceland” have a relation type “nationality,” and “Bobby Fischer” and “Reykjavik” have a relation type “𝑝𝑙𝑎𝑐𝑒_𝑜𝑓_𝑑𝑒𝑎𝑡ℎ.” We can assign two different labels to each word in the “Bobby Fischer” entity, corresponding to different triplets. Among them, the labels of “Bobby” are set to “𝑝𝑙𝑎𝑐𝑒_𝑜𝑓_𝑑𝑒𝑎𝑡ℎ_𝐸1𝐵,” and “𝑛𝑎𝑡𝑖𝑜𝑛𝑎𝑙𝑖𝑡𝑦_𝐸1𝐵,” and the labels of “Fischer” are set to “𝑝𝑙𝑎𝑐𝑒_𝑜𝑓_𝑑𝑒𝑎𝑡ℎ_𝐸1𝐿” and “𝑛𝑎𝑡𝑖𝑜𝑛𝑎𝑙𝑖𝑡𝑦_𝐸1𝐿.” The single word entity “Reykjavik” corresponding to “Bobby Fischer” is set to “𝑝𝑙𝑎𝑐𝑒_𝑜𝑓_𝑑𝑒𝑎𝑡ℎ_𝐸2𝑆.” “Iceland” is set to “𝑛𝑎𝑡𝑖𝑜𝑛𝑎𝑙𝑖𝑡𝑦_𝐸2𝑆.”

Fig. 3
figure 3
Multi-label tagging scheme for nested entity recognition on the Wiki-KBP dataset

Full size image
Fig. 4
figure 4
Multi-label tagging scheme for overlapping entity recognition on the NYT dataset

Full size image
The main advantage of multi-label tagging is that the function of having multiple labels for one word is realized. In a complex semantic environment, a multi-label tagging scheme is more advantageous. It can also be used for identifying nested entities and overlapping entities. Our next task is to extract the relation triplets based on the tag sequence. Because the tag of the entity contains the predefined relation that exists in the entity, the model only needs to merge the entities with the same relation type tags into a triplet. If an entity contains multiple labels, it can be used multiple times to form multiple triplets. For example, “Bobby Fischer” is used twice, each time appearing in a different triplet with a different label. However, when two entities with the same label are presented in a sentence, the multi-label tagging scheme cannot be used to distinguish them, the actual position in the sentence must be used to distinguish them instead. Therefore, in the feature representation of the input layer in Section 3.2, we distinguish entities with the same labels using an absolute position embedding vector.

Extraction module
Based on the multi-label tagging scheme, we employ an encoder-decoder framework to build a joint entity and relation extraction module that incorporates the MLA mechanism. Figure 5 illustrates the details of the extraction module including the input layer, encoding layer, decoding layer, and output layer. Specifically, the word embedding vector and the position embedding vector are concatenated in the input layer, and the concatenated vector is input to the encoding layer. Second, the rich context information of the input token is jointly learned through the Bi-LSTM and the attention mechanism in the encoding layer. Then, all the encoded information is decoded through the CRF and the attention mechanism in the decoding layer, and the head and tail entities with the same relationship type are selected. Finally, triplets are outputted through the output layer. The goal of the extraction module is to identify the entities and the relations between the entities contained in the input sentence. A detailed description of each layer is presented in the following sections.

Fig. 5
figure 5
Schematic of the extraction module using MLA. The abbreviations POD and NA stand for 𝑝𝑙𝑎𝑐𝑒_𝑜𝑓_𝑑𝑒𝑎𝑡ℎ and nationality, respectively

Full size image
Input layer
In this layer, We construct an input embedding for each word in the sentence. The embedding consists of two parts: word embedding vector W and position embedding vector P. A word embedding vector W is used to capture the semantic feature of the input words, and these vectors are generated by running the word2vec [37] model on the training corpus. A position embedding vector P determines the position of each word or the distance between different words in the sequence, and these vectors represent the local or even global structure [38]. We use sine and cosine functions to implement position encoding. With this approach, not only can we obtain the absolute position information of the word, but we also include the relative position information. The absolute position information is determined using the following equations:

𝑃𝐸(𝑝𝑜𝑠,2𝑖)=sin(𝑝𝑜𝑠/100002𝑖/𝑑𝑚𝑜𝑑𝑒𝑙),
(1)
𝑃𝐸(𝑝𝑜𝑠,2𝑖+1)=cos(𝑝𝑜𝑠/100002𝑖/𝑑𝑚𝑜𝑑𝑒𝑙),
(2)
where pos is the position of the word (𝑝𝑜𝑠=0,1,…,𝐿−1, where L is the length of the sentence) and i is the specified dimension of the vector (when 𝑑𝑚𝑜𝑑𝑒𝑙=512, 𝑖=0,1,…,255). Through the above equations, we can generate a 𝑑𝑚𝑜𝑑𝑒𝑙-dimensional position vector for each position. For an even position, we use sine encoding, and its value is PE(pos, 2i). For odd position, we use cosine encoding, and its value is 𝑃𝐸(𝑝𝑜𝑠,2𝑖+1).

Using this approach, each position of the input sequence can be numbered. Each number corresponds with a position embedding vector P, which is combined with the word embedding vector W to obtain the input embedding vector 𝑖𝑛𝑝𝑢𝑡=[𝑊,𝑃]. By repeating the above operation for each word in the input sequence, a matrix of dimensions (input_length)∗(emb_dim) is returned to the encoding layer, where input_length is the length of the input sequence and emb_dim is the dimension of the input embedding.

Encoding layer
We use a single-layer Bi-LSTM as the encoding layer, which is used to capture sequence features [39]. It comprises a forward LSTM layer, backward LSTM layer, and connection layer. For each word 𝑊𝑡, the forward LSTM layer encodes 𝑊𝑡 by considering information from words 𝑊1 to 𝑊𝑡, labeled as ℎ𝑡→. Similarly, the backward LSTM layer will encode 𝑊𝑡 based on information from 𝑊𝑛 to 𝑊𝑡, labeled as ℎ𝑡←. Finally, we connect ℎ𝑡→ and ℎ𝑡← to represent the 𝑊𝑡 encoding information, which is expressed as ℎ𝑡=[ℎ𝑡→,ℎ𝑡←]. The forward LSTM layer can be described as ℎ𝑡→,𝑐𝑡→=𝐿𝑆𝑇𝑀(𝑋𝑡,ℎ𝑡−1−→−,𝑐𝑡−1−→−). The backward LSTM layer can be described as ℎ𝑡←,𝑐𝑡←=𝐿𝑆𝑇𝑀(𝑋𝑡,ℎ𝑡+1←−−,𝑐𝑡+1←−−).

The forward LSTM operations are defined as follows:

𝑓𝑡=𝜎(𝑤𝑥𝑓𝑥𝑡+𝑤ℎ𝑓ℎ𝑡−1−→−+𝑤𝑐𝑓𝑐𝑡−1−→−+𝑏𝑓),
(3)
𝑖𝑡=𝜎(𝑤𝑥𝑖𝑥𝑡+𝑤ℎ𝑖ℎ𝑡−1−→−+𝑤𝑐𝑖𝑐𝑡−1−→−+𝑏𝑖),
(4)
𝑜𝑡=𝜎(𝑤𝑥𝑜𝑥𝑡+𝑤ℎ𝑜ℎ𝑡−1−→−+𝑤𝑐𝑜𝑐𝑡−1−→−+𝑏𝑜),
(5)
𝑐𝑡˜=tanh(𝑤𝑥𝑐𝑥𝑡+𝑤ℎ𝑐ℎ𝑡−1−→−+𝑏𝑐),
(6)
𝑐𝑡→=𝑓𝑡∗𝑐𝑡−1−→−+𝑖𝑡∗𝑐𝑡˜,
(7)
ℎ𝑡→=𝑜𝑡∗tanh(𝑐𝑡→),
(8)
where 𝑓𝑡, 𝑖𝑡, and 𝑜𝑡 are the output of the forget gate, input gate, and output gate, respectively. Further, b is the bias term, c is the memory unit, w is the parameter, h is the hidden-layer vector. The forget gate must identify both historical and new information to determine which information needs to be forgotten. The input gate can prevent the current irrelevant content from entering the memory unit. The output gate controls the effect of long-term memory on the current output. The output value of the gate is a real vector of elements that range between 0 and 1.

Decoding layer
CRF [40] considers the dependency between adjacent elements and the current elements. For a given random independent variable x, the dependent variable y is obtained using a probabilistic undirected graph model. This model is applied to the IE task, and a series of features are provided to predict the label of each word. Because CRF considers the input state feature function and label transfer feature function, it can solve the serialized tag problem by making full use of text features. The main idea of this model comes from ME and HMM, and it can arbitrarily select features and perform global normalization to obtain the global optimal solution. CRF not only retains the advantages of the conditional probability framework but also solves the tagging offset problem of ME. Moreover, this model has higher precision than an HMM [41]. CRF treats each tagging result as a complete path and then selects the most appropriate path from these paths based on the context association.

We use a linear chain CRF as the decoding layer, which treats the data sequence tagging problem as a probability distribution problem. Here, P(y|x) simulates a probability distribution, where x represents the observation sequence and y represents the tag sequence, and it is used to label the word boundary in a sentence [42]. Both x and y have the same chain structure and the same sequence length, and P(y|x) can be obtained from the following equation:

𝑃(𝑦|𝑥)=1𝑍(𝑥)exp(∑𝑖,𝑘𝜆𝑘𝑓𝑘(𝑦𝑖−1,𝑦𝑖,𝑥)+∑𝑖,𝑙𝛼𝑙𝑔𝑙(𝑦𝑖,𝑥)),
(9)
𝑍(𝑥)=∑𝑦exp(∑𝑖,𝑘𝜆𝑘𝑓𝑘(𝑦𝑖−1,𝑦𝑖,𝑥)+∑𝑖,𝑙𝛼𝑙𝑔𝑙(𝑦𝑖,𝑥)),
(10)
where Z(x) is the normalization factor, 𝑓𝑘(𝑦𝑖−1,𝑦𝑖,𝑥) represents the feature transfer function at two adjacent label positions, K is the total number of transitional feature functions, 𝑔𝑙(𝑦𝑖,𝑥) represents the state feature function at position i of the observation sequence, L is the total number of state feature functions, and 𝜆𝑘 and 𝛼𝑙 indicate the weights of the feature function.

Attention layer
Attention mechanisms were originally proposed for learning the alignment between source and target words in different languages in RNN-based neural machine translation models [43]. An attention mechanism normally employs an attention generation function to re-allocate alignment weights and employ a weighted-sum operation for feature recalibration [44].

The attention used in the method proposed in this article is a type of self-attention, which is also called intra-attention, and is a special form of attention mechanism for intra-relation reasoning between tokens within both the encoder and decoder parts [38]. The attention layer used in the proposed method can be divided into two layers: encoding layer attention and decoding layer attention. Encoding layer attention learns the dependencies within the sentence. Decoding layer attention reduces interference caused by irrelevant entities. In the following text, we consider three aspects of each type of attention: the calculation area, structure attention information, and calculation process.

(1)
Encoding layer attention

Calculation area. Encoding layer attention uses global attention, and it calculates the weighted probability of all words in the sentence. Each word is given a final corresponding weight, and this is a global calculation method.

Attention information. Encoding layer attention focuses on the features of the words in the current sentence, and it can tag the words in the sentence with appropriate labels according to the degree to which the current word and the label match.

Calculation process. Encoding layer attention uses a multi-head self-attention mechanism [38], using multiple queries to perform multiple attentions on the original text. Each query focuses on different parts of the original text. Finally, the results of multiple attentions are stitched together.

The multi-head attention weight in the attention is calculated using the following formulas:

MultiHead(𝑄,𝐾,𝑉)=Concat(head1,head2,…,headℎ)𝑊0,
(11)
head𝑖=Attention(𝑄𝑊𝑄𝑖,𝐾𝑊𝐾𝑖,𝑉𝑊𝑉𝑖),
(12)
Attention(𝑄𝑊𝑄𝑖,𝐾𝑊𝐾𝑖,𝑉𝑊𝑉𝑖)=∑𝐿𝑥𝑖=1𝑎𝑖∗𝑉𝑖,
(13)
𝑎𝑖=softmax(Sim𝑖)=𝑒Sim𝑖∑𝐿𝑥𝑗=1𝑒Sim𝑗,
(14)
where Multihead(𝑄,𝐾,𝑉) represents the result of multi-head stitching, and head𝑖 represents the calculation results of different attention sublayers. Each head can be calculated from the results of Attention(𝑄𝑊𝑄𝑖,𝐾𝑊𝐾𝑖,𝑉𝑊𝑉𝑖). Moreover, each head maintains an independent query weight matrix 𝑊𝑞𝑖, key weight matrix 𝑊𝑘𝑖, and value weight matrix 𝑊𝑣𝑖 to generate the different query, key, and value matrices. The sequence length of each sub-layer is represented by 𝐿𝑥, and 𝑎𝑖 is the weight coefficient. The similarity Sim𝑖 is calculated from the scaled dot product attention between 𝑄𝑡 and 𝐾𝑖.

The calculated values Sim𝑖 of the encoding layer and the decoding layer need to be distinguished. All keys in the encoding layer participate in the calculation. The similarity calculation formula for the encoding layer is as follows:

Sim(𝑄𝑡,𝐾𝑖)=𝑄𝑡∗𝐾𝑖𝑇dk‾‾√,
(15)
where 𝑑𝑘‾‾‾√ is the temperature factor.

(2)
Decoding layer attention

Calculation area. Decoding layer attention uses local attention, which only calculates attention over the area in a window. First, for a point at a certain location, we obtain a window area with this point as the center, and then we calculate the attention within this small area.

Attention information. The decoding layer attention mechanism focuses on words with the same relation type. The head and tail entities can be located using the relation type in each word label, and then triplets can be extracted. The entities unrelated to the current triplet are neglected to improve the extraction performance of the triplet.

Calculation process. The key with the same relation type is involved in the calculation.

The similarity calculation formula for the decoding layer is as follows:

Sim(𝑄𝑡,𝐾𝑖)=𝑄𝑡∗𝐾𝑖𝑇∗𝐹(𝑇𝑎𝑔)dk‾‾√,
(16)
where F(Tag) is a switch function used to distinguish entities that are related or unrelated to the target triple, Tag is the relation type label of the target triplet, and tag(i) is used to obtain the relation type of the i-th word label.

𝐹(𝑇𝑎𝑔)={1 𝑡𝑎𝑔(𝑖)=‘‘𝑇𝑎𝑔″0 𝑡𝑎𝑔(𝑖)≠‘‘𝑇𝑎𝑔″.
(17)
Alignment module
Inspired by KG2E [45], we propose a relational alignment module based on a relation set to align the relations in the triplet predicted by the extraction model and reduce the prediction loss of the entity pair (𝐸1, 𝐸2) in RC. We used the triplet score function of KG2E to identify all similar relations between two entities and then aggregate them into a relation set for alignment. The relational alignment embedding space is shown in Fig. 6.

Fig. 6
figure 6
Relational alignment embedding space

Full size image
Assuming that the head entity and the tail entity are independent of certain specific relationships, The conversion from head entity to tail entity is similar to the expression of a certain relationship. Use ℎ−𝑡 to express this entity conversion, which is expressed by Gaussian distribution 𝑃𝑒, and the relation is expressed by Gaussian distribution 𝑃𝑟. The triplet score can be evaluated by the similarity of these two distributions. The probability distributions of 𝑃𝑒 and 𝑃𝑟 are expressed as

𝑃𝑒∼𝑁(𝜇ℎ−𝜇𝑡,∑ℎ+∑𝑡),
(18)
𝑃𝑟∼𝑁(𝜇𝑟,∑𝑟),
(19)
where (𝜇ℎ,𝜇𝑡), and 𝜇𝑟 are the mean of the Gaussian distribution of the entity or relation, respectively, which represents the center position of the entity or relation in the semantic space. (∑ℎ,∑𝑡) and ∑𝑟 are the covariances of the Gaussian distributions of the entities and relation, respectively, and they represent the degree of uncertainty of the entity or relation.

Given a triplet (𝐸1,𝑟1,𝐸2), the triplet score can be estimated by calculating the similarity of the entities and relation by expected probability, and the score function can be expressed as

𝑓𝑟1(𝐸1,𝐸2)=∫𝑥∈𝑅𝑘𝑒𝑁(𝑥;𝜇𝑟1,∑𝑟1)𝑁(𝑥;𝜇𝑒,∑𝑒)𝑑𝑥.
(20)
If there is another triplet (𝐸1,𝑟2,𝐸2), its score function is expressed as

𝑓𝑟2(𝐸1,𝐸2)=∫𝑥∈𝑅𝑘𝑒𝑁(𝑥;𝜇𝑟2,∑𝑟2)𝑁(𝑥;𝜇𝑒,∑𝑒)𝑑𝑥.
(21)
Through the above two functions, the following constraints can be obtained: if 𝑓(𝐸1,𝑟1,𝐸2)≈𝑓(𝐸1,𝑟2,𝐸2), then 𝑟1≈𝑟2. Similar relations that satisfy the above constraints can be formed into a relation set 𝑅={𝑟1,𝑟2,…,𝑟𝑛}, and this set is used to align the extraction results. If the RC prediction result of the entity pair is included in the relation set, the relation of the entity pair can be expressed as any kind of representation in the relation set to avoid the problem of decreasing the triplet extraction accuracy caused by a single predicted relation.

MLRA-LSTM-CRF algorithm
We present the pseudocode of the whole framework in Algorithm 1. The algorithm first uses the training set sentence sequences, entity set, and triplets to construct a training sample tag sequence set; uses batch training and the RMSProp optimization algorithm to train the model; uses the trained prediction model to predict the triplets of the test sample set, and finally uses the relational alignment set to align the relations of the predicted triplets.

figure a
Experiments and analysis
Experiment settings
Datasets
To evaluate the effectiveness of the proposed model, we evaluated its performance on two datasets, namely the NYT and Wiki-KBP datasets. Concretely, NYTFootnote1 is a news corpus. It contains 294,000 sentences extracted from New York Times news articles. The training data [9] were automatically labeled using distant supervision. The test data [46] were annotated using human annotation. Wiki-KBPFootnote2: contains 780,000 sentences extracted from Wikipedia articles and a 2013KBP corpus [47]. The training data [48] were automatically labeled through distant supervision and handcrafted mode. The test data [9] are sentences selected from the human annotation of the 2013KBP slot-filling evaluation result. The statistical information of the dataset is shown in Table 1.

Table 1 Statistics of datasets used in experiments. Without “None” means that the triplets whose entity relations are “None” have been deleted
Full size table
Baselines
To evaluate the effectiveness of the proposed model, we compared it with the following baselines:

MultiR [46]: This method is a typical distant supervision method based on multi-instance and multi-label learning algorithms, which can effectively deal with the noise problem caused by distant supervision training data.

CoType [9]: This method is a domain-independent framework that embeds candidate entities, candidate relationships, and text features into entity space and relationship space and performs joint modeling of the entities and relations.

ReHession [48]: This model employs heterogeneous supervision using a knowledge base and domain heuristic mode.

LSTM-CRF [49]: This model uses Bi-LSTM to encode the input sentence, and CRF predicts the entity annotated sequence.

LSTM-LSTM-Bias [16]: This method uses a tagging scheme to jointly extract entities and relations, and simultaneously, through the use of a biased objective function, it enhances the correlation between entities and reduces the impact of invalid tagging.

PA-LSTM-CRF [15]: This method employs a unified position-attentive sequence labeling framework for jointly extracting entities and overlapping relations.

Evaluation metrics
For the performance evaluation, in our experiment, we adopted precision (Prec.), recall (Rec.), and F-measure (F1) as evaluation metrics. When the relation type and two entities are both correct, the extracted triplet is regarded as correct. When the head and tail entity offsets are both correct, the extracted entities are regarded as correct. We created a validation set by randomly sampling 10% of the data from the test set and used the remaining data for the evaluation. Based on the suggestion in [9, 16], we performed 10 runs for each experiment to ensure the reliability of the experimental results. The average results and their standard deviations are shown in Table 6.

Implementation details
Experimental environment The experimental environment consisted of an Intel Xeon E5-2650 v4 CPU running the Ubuntu 16.04 operating system. The popular deep learning library Keras was used to build the model for our experiment. All the models were trained and tested on a TITAN V GPU with 12GB graphic memory.

Hyperparameter In our experiments, we adopted the minibatch mechanism and the RMSprop optimizer [50] to train our model. The optimizer with an initial learning rate of 0.0001, a rho of 0.9, and an epsilon of 1×10−6. In the input layer, we fixed the dimensions of the word embedding vector and the position embedding vector to 𝑑=300. In the encoding layer, we fixed the size of the LSTM in both directions to 𝑑=300. We used dropout [51] to regularize our network. Dropout was applied in the embedding output and in the encoding layer. We selected the learning rate from dropout∈{0.2,0.3,0.4,0.5}, the numbers of the multi-head attention heads and the head size from multi−head∈{(1,600),(2,300),(3,200),(4,150),(5,120),(6,100)}, and the training batch size in minibatch∈{64,128,256}. We selected the best hyperparameters according to the results in the validation set. More details about the effect of each hyperparameter on model performance are given in Tables 2, 3, 4, and 5.

Table 2 Model performance for different embedding output dropout values
Full size table
Table 3 Model performance for different LSTM layer dropout values
Full size table
Table 4 Model performance for different numbers of multi-head attention heads and the dimensions of each head
Full size table
Table 5 Model performance for different minibatch size values
Full size table
Tables 2 and 3 show the performance of our model on the test set for different values of embedding output dropout and LSTM layer dropout, respectively. We vary one hyperparameter at a time to assess the effect of each one. In Table 4, we report the results for different numbers of multi-head attention heads and the dimension of each head, and in Table 5, we report the results for different minibatch sizes. The reported results show that different hyperparameters settings lead to a noticeable performance difference.

Main results
To evaluate the performance of the proposed MLRA-LSTM-CRF, we compared it with the baseline model. Table 6 lists the experimental results of the different models on the two public data sets NYT and Wiki-KBP. It can be observed that MLRA-LSTM-CRF performs better than all the baseline models and achieves a state-of-the-art F1 score on the two datasets. The F1 scores of MLRA-LSTM-CRF on NYT and Wiki-KBP reached 56.5% (95% confidence interval: 55.5–57.5%) and 48.9% (95% confidence interval: 48.3–49.5%), respectively. Most importantly, we can see that MLRA-LSTM-CRF achieves improvements of 2.7% and 4.5% in F1 score when compared with the best baseline PA-LSTM-CRF [22]. We note that the precision of MLRA-LSTM-CRF is much higher than that of all other models on the NYT dataset. Although the recall of MLRA-LSTM-CRF on the NYT dataset as well as the accuracy and recall on the Wiki-KBP is not the best, its F1-value performance is better than the baseline and achieves competitive performance on NYT and Wiki-KBP datasets. These results prove the effectiveness of MLRA-LSTM-CRF on the joint entity and relation extraction tasks.

Table 6 Quantitative results of our proposed model compared with results for state-of-the-art methods on two benchmark datasets. Boldface indicates the best result for each dataset
Full size table
The main reason for this performance is that through the multi-label tagging scheme, the number of valid labels for entities is increased. At the same time, Encoding layer attention extracts long-distance relations by enhancing the connections between entities. Decoding layer attention enhances the effectiveness of triplet extraction and reduces invalid extraction. Finally, the extraction results are aligned using relational alignment, which improves the accuracy of triplet extraction.

Error analysis
Our model focuses on extracting triplets composed of two entities and their semantic relations. To explore the factors that affect the model extraction results, we separately predicted the performance of each element in the triple, as shown in Table 7. Here, 𝐸1 and 𝐸2 represent the performance of predicting each respective entity. If the offset of the first entity is correct, the instance of 𝐸1 is correct, and the same is true for 𝐸2. Regardless of the relation type, if the offsets of the two corresponding entities are both correct, the instance of (𝐸1,𝐸2) is correct.

Table 7 Predicted results of triplet elements on the NYT dataset
Full size table
As shown in Table 7, (𝐸1,𝐸2) leads to a higher precision than 𝐸1 or 𝐸2 alone, but the recall is slightly lower. There are two main reasons this could happen:

(1)
The head entity boundary is found, but the tail entity boundary is not found.

(2)
The tail entity boundary is found, but the head entity boundary is not found.

These two situations result in many single entities and fewer triplets. Moreover, when considering the type of relation, a lower F1 is shown in Table 6 than in Table 7. This means that in many triplets, the boundaries of the head and tail entities are correct, and the relation type is predicted to be wrong. This result is generally caused by the following two situations:

(1)
An entity pair often has multiple relations and the true value of the test set in the experiment is often marked with only one relation. The lack of relational alignment often results in the relation type of the triplet being predicted to be wrong. For example, the relation tags “/location/location/contains,” “/location/country/administrative_divisions,” and “/location/administrative_division/country” are equivalent relations. However, when the true value of the test set is marked as one of the relations and the model predicts one of the other relations, the model considers the prediction result to be incorrect. To this end, we add the function of relational alignment to the prediction results of the RC and align the triplet extraction results to adapt to the multi-label annotation of entity pairs (𝐸1,𝐸2) and mitigate the prediction loss of (𝐸1,𝐸2) in the RC.

(2)
Some relations of an entity pair cannot be simply aligned. For example, the relation tags “/location/location/contains” and “/location/country/capital” cannot be considered as the same relation type. Because the relation between the two entities is the “capital” relation, they must have the “contains” relation. In contrast, this is not the case. These issues will be studied in future work.

Ablation studies
In this section, we present the results of an ablation analysis on the key parts, i.e., the multi-label tagging scheme, MLA, CRF, and relational alignment, to validate the effectiveness of our proposed model.

Table 8 Ablation study of MLRA-LSTM-CRF on the NYT dataset
Full size table
Table 8 lists the quantitative results of the ablation study. It can be observed that all the tested components contribute to the performance of MLRA-LSTM-CRF. When we removed multi-label tagging and replaced it with single label tagging, the F1 score decreased by 4.1%. When we removed the relational alignment module, the F1 score decreased by 4.0%. This demonstrates that the multi-label tagging scheme and the relational alignment module are effective parts of the joint extraction task. In addition, when we remove the encoding layer or decoding layer attention, we find that the performance is decreased by 6.6% and 7.7%, respectively. Further, the decoding layer attention is more important than the encoding layer attention. This is mainly because their roles in the model are different. The encoding layer attention focuses on all the words in the sentence and obtains rich context information. The decoding layer attention focuses on head and tail entities with the same relation to form effective triplets and avoid generating invalid triplets. From the results in Table 8, we can also observe that CRF plays an important role in improving extraction performance. When we removed the CRF layer and replaced it with a softmax layer, the F1 score decreased by 6.2%. This is mainly because the softmax layer assumes that the label of each token is independently distributed, which degrades the performance of the joint extraction task. In contrast, CRF can capture the strong label dependency in the dataset, rather than just assuming that the label of each token is independent of the labels of adjacent tokens.

Table 9 Triplet elements ablation study on the NYT dataset
Full size table
Table 9 shows the results of the ablation experiments for each element in the triplet. It can be seen that when we removed any component from the model, the performance of each element in the triplet shows a downward trend. In addition, we can observe which component contributes most to the element. For 𝐸1, we found that removing the CRF module has a greater impact than removing the other components, the F1 score decreased by 7.6%. For 𝐸2 and (𝐸1, 𝐸2), we found that removing MLA has a greater impact than removing the other components. We note that when the decoding layer attention was removed, the F1 value decreased by 6.3% and 9.3%, respectively.

Case study
In this section, we observed the prediction results of the end-to-end models, and then list a few representative examples in Table 10 to illustrate the advantages and disadvantages of the model. The first and second examples represent a situation in which the head and tail entities are far and close, respectively. The third example is a negative example, showing that the models incorrectly predict the entity or the relation role of the entity. Each example contains four rows, the first row is the gold standard, the second and the third rows are the extracted results of model LSTM-LSTM and LSTM-LSTM-Bias respectively. The fourth row is the extraction result of our model.

Table 10 Output from different models on the NYT dataset. Sentence 𝑆𝑖 represents the gold standard of sentence i
Full size table
In the first sentence, the LSTM-LSTM model can only extract a head entity “Florida.” Both LSTM-LSTM-Bias and MLRA-LSTM-CRF can increase the correlation between entities, extracting two related head and tail entities: “Florida” and “Panama City Beach.” However, the implemented mechanism of the two models is different. LSTM-LSTM-Bias enhances the correlation between entities using the bias objective function. MLRA-LSTM-CRF enhances the correlation between entities using the encoding layer attention.

In the second sentence, LSTM-LSTM and LSTM-LSTM-Bias incorrectly predict a tail entity “Middle Ages.” MLRA-LSTM-CRF successfully extracts two related head and tail entities “Germany” and “Nuremberg.” This shows that MLRA-LSTM-CRF can obtain rich context information through the encoding layer attention, thereby effectively identifying the head entity and the tail entity. At the same time, the interference of irrelevant entities can also be reduced by the decoding layer attention.

In the third sentence, LSTM-LSTM treats both “Stephen A. Schwarzman” and “Blackstone Group” as head entities and cannot find their corresponding tail entities. LSTM-LSTM-Bias can determine the entities pair (𝐸1, 𝐸2), but it reverses the roles of the head entity and tail entity. MLRA-LSTM-CRF incorrectly treats both “Stephen A. Schwarzman” and “Blackstone Group” as tail entities and marks them as different relations. The main reason for this situation is that there is an inverse relationship between some relations. For example, the relation between “/business/company/founders” and “/business/person/company” is an inverse relation. This demonstrates that MLRA-LSTM-CRF still lacks a relational reasoning mechanism, and its ability to distinguish multiple relations between two entities needs to be improved.

Conclusion
This article presented an end-to-end NN model for the joint extraction of entities and relations called MLRA-LSTM-CRF. The model uses a multi-label tagging scheme to identify the entities and their relations in a sentence, and it extracts the triplets in the sentences by integrating the MLA mechanism into the extraction module. In addition, the relation in each triplet is aligned through relational alignment, thereby improving the accuracy of triplet extraction. Experiments conducted on two public datasets, NYT and Wiki-KBP, demonstrate that our model achieves promising performance in comparison with state-of-the-art methods. In addition, MLRA-LSTM-CRF can also be combined with pre-trained word vectors from various fields such as medicine or the law and applied to text mining on documents in these fields.

In future work, we plan to research to address the following issues. (1) We plan to explore the effectiveness of cross-attention in the extraction module. This approach has been proven beneficial in the work of Ji et al. [44] for object-detection in videos. (2) MLRA-LSTM-CRF still has certain difficulties predicting multiple relations. We will combine relational reasoning and relational alignment to further improve its performance on the multi-relation extraction task. (3) For relation types that do not appear in the training set, the existing model framework cannot predict the correct relation type to which the entity pair belongs accurately. We plan to consider the use of transfer learning to solve the out-of-vocabulary problem of relational types.

Keywords
Joint extraction
Multi-label tagging
Multi-layer attention
Relational alignment