Increasing photovoltaic (PV) instalments could affect the stability of the electrical grid as the PV produces weather-dependent electricity. However, prediction of the power output of the PV panels or incoming radiation could help to tackle this problem. It has been concluded within the European Actions “Weather Intelligence for Renewable Energies” framework that more research is needed on short-term energy forecasting using different models, locations and data for a complete overview of all possible scenarios around the world representing all possible meteorological conditions. On the other hand, for the Mediterranean region, there is a need for studies that cover a larger spectrum of forecasting algorithms. This study focuses on forecasting short-term GHI for Kalkanli, Northern Cyprus, while aiming to contribute to ongoing research on developing prediction models by testing different hybrid forecasting algorithms. Three different hybrid models are proposed using convolutional neural network (CNN), long short-term memory (LSTM) and support vector regression (SVR), and the proposed hybrid models are compared with the performance of stand-alone models, i.e. CNN, LSTM and SVR, for the short-term GHI estimation. We present our results with several evaluation metrics and statistical analysis. This is the first time such a study conducted for GHI prediction.

Access provided by University of Auckland Library

Introduction
As a result of global warming caused by greenhouse gas emissions from conventional energy sources, many countries have been focusing on renewable energy sources (RES) to meet the increasing energy demand [1]. RES play a crucial role in combating global warming by reducing the energy produced from conventional sources [2]. Additionally, RES is suggested to be taking part in increasing life quality as well as contributing to the development of the economy [2]. More countries are expected to integrate renewable energy sources, specifically solar energy, into their energy supply in the coming years [1, 3,4,5].

Among the renewable energy sources, solar energy is the main focus of interest as there is tremendous growth in solar photovoltaic (PV) system installation in many countries [1, 4, 6], including Northern Cyprus. PV panels convert sunlight, i.e. solar radiation, into electricity. However, the PV output is very intermittent and unstable as it is a weather-dependent energy source [5, 7]. As a result of the intermittency and the instability, increasing PV panel instalments jeopardise the frequency of the alternating current in the electrical grids [8]. For a continuous growth in the PV instalments as well as minimal usage of conventional energy sources in power generation, it is of utter importance for a grid manager to possess information on the energy production of PV panels, hence knowledge on received radiation. PV cannot produce electricity in the absence of sunlight. However, the production quantity varies due to variables such as the variation of the ambient temperature, humidity, cloud movements, etc. [5, 9]. As its energy output is unstable, it makes the difficult task of balancing demand and supply of electricity in the isolated electrical grids, e.g. like in islands, even more challenging [3, 6, 9,10,11,12]. Additionally, Voyant et al. [13] stated that a grid manager should know about a PV production at least one hour ahead due to a delay in starting a power generation system.

Extensive integration of solar energy to the existing or future electricity grids enhances the need for radiation forecasting as it helps mitigate the intermittency by giving information about future energy production [3,4,5,6, 10, 14,15,16,17,18]. Forecasting solar radiation is a crucial and ongoing task on different time horizons for various power system applications, as stated by many researchers [3, 5, 9, 11, 12, 18,19,20,21]. In addition to the power system applications, knowledge in solar radiation data could be utilised in infrastructure and maintenance planning [14, 18, 22, 23], solar energy-related policymaking [16] and energy storage options that depend on the knowledge of solar energy production [3, 11]. Moreover, the prediction of the energy output of local production of PV panels and other renewable thermal energy resources could be crucial in connecting the local sources with the most important loads in the case of a blackout [24]. Additionally, apart from utilisation in energy-related areas, radiation information is needed in various tools to access climate impacts on agriculture [25].

Solar radiation that reaches the PV panel is classified into four categories. Direct Normal Irradiance (DNI) is the radiation type that reaches the surface without disruption, while Diffuse Horizontal Irradiance (DHI) is scattered by the atmosphere, e.g. by the clouds. The other type, Ground Reflected Irradiance (GRI), is reflected from the ground as the name suggests. The last type is the Global Horizontal Irradiance (GHI), which is the amount of radiation reaching a horizontal surface. DNI is a useful component for concentrating solar technologies. On the other hand, for PV panels, GHI is the most commonly predicted radiation type.

There are two approaches to radiation forecasting adopted in the literature: traditional annual forecasting and novel seasonal forecasting. In annual forecasting, all data points in a dataset are used to train and test the forecasting algorithm. On the other hand, in seasonal forecasting, designed models are trained and tested with separate sub-datasets that are created depending on the months of the season. Recent studies mostly conducted in the Mediterranean region have been utilising the seasonal forecasting method.

Additionally, two different datasets can be used in GHI prediction. The first dataset is called time-series or historical data, which is a sequence of observations ordered through equally spaced time intervals. Time-series data is commonly used in the literature for short-term GHI prediction [22]. The second dataset includes meteorological variables, i.e. features. These variables include but are not limited to sunshine duration, ambient temperature, relative humidity, wind speed, wind direction, pressure, date, time and so on [17]. Seldom geographical variables, i.e. longitude, altitude and elevation, are used along with meteorological features in GHI forecasting. Forecasting models can be based on time-series datasets or meteorological and geographical datasets. They can also be based on a hybrid dataset that considers both radiation and meteorological datasets as input features [3]. Aggarwal and Saini [26] refer to time-series forecasting, which uses past data of GHI values as input data, as endogenous forecasting and hybrid forecasting as exogenous forecasting, which is also called multivariate forecasting. In this study, datasets will be represented either as endogenous or exogenous.

In our previous work [27], we used a one-dimensional CNN (1D-CNN) and an LSTM network separately for the GHI prediction for Kalkanli, Northern Cyprus. These stand-alone 1D-CNN and LSTM models are fed with exogenous and endogenous datasets for GHI forecasting. In this study, we have developed different hybrid deep learning models for the short-term GHI prediction for Kalkanli, Northern Cyprus. Sperati et al. [30] also concluded in their benchmarking study within the European Actions “Weather Intelligence for Renewable Energies” framework that more research is needed on short-term energy forecasting using different models, locations and data for a complete overview of all possible scenarios around the world representing all possible meteorological conditions. However, as it can be seen from Table 1, there is a very limited number of studies conducted for the Mediterranean region. Moreover, the performances of different hybrid models need to be investigated. Three hybrid models are constructed using 1D-CNN, LSTM and SVR, and proposed hybrid models are compared with the stand-alone models.

Table 1 Summary of the research related to the GHI forecasting for various parts of the world
Full size table
CNN and LSTM algorithms recently started attracting attention for GHI forecasting [1, 5, 11, 12, 28,29,30]. Researchers have been testing performances of these algorithms, as single and hybrid. CNN and LSTM algorithms have significant potential in terms of GHI prediction. Besides these two algorithms, SVR provides a great generalisation capabilities while its computational complexity is independent of the input dimensionality. It is also very effective in modelling energy-related applications [31, 32]. Additionally, support vector machine combined with deep learning algorithms is proven to increase performances in classification problems [33, 34]. Since SVM improves classification performance after combining with deep learning algorithms, SVR may also improve regression performance after combining with a CNN or an LSTM network. Therefore, this study employs SVR along with CNN and LSTM for short-term GHI prediction in Northern Cyprus.

Although several variations of hybrid CNN and LSTM algorithms are employed in the literature, to the best of our knowledge, the performances of hybrid 1D-CNN and LSTM and hybrid 1D-CNN, LSTM and SVR are not studied yet. Their performances are presented in this paper for the first time in the Mediterranean region with extensive evaluation and comparisons. The research on the performances of the aforementioned models, especially for the Mediterranean region, is important. Thus, this study aims to provide accurate short-term GHI prediction models to contribute to the literature on the performances of various forecasting algorithms. In our work, the effects of seasonality on forecasting are also investigated. The performances of seasonal forecasting are compared with the performances of annual forecasting models.

The study is composed of six sections. In Sect. 2, a brief review of the related recent studies is given. In Sect. 3, the materials, i.e. study area and data, algorithms and performance evaluation metrics used in the study, are elaborated. Data pre-processing steps and forecasting model construction are explained in detail in Sect. 4. Section 5 provides the results of the modelled forecasting networks, comparison and discussion. Finally, conclusions and recommendations are presented in Sect. 6.

Literature review
There is a vast extent of radiation forecasting studies in the literature, as mentioned earlier. For instance, some focuses on forecasting GHI, while others focus on DNI prediction. Furthermore, the methods used for radiation forecasting also differ significantly, e.g. numerical correlations, image classification, DL algorithms, etc. The literature review of this study focuses exclusively on machine learning (ML) employing studies.

For the GHI forecasting, there are various machine learning algorithms employed in the literature. These algorithms include but are not limited to artificial neural network (ANN), support vector regression (SVR), convolutional neural network (CNN) and long short-term memory (LSTM). In recent years, hybrid models of CNN and LSTM gained popularity as they give better performance compared to other algorithms.

There are several recent studies conducted for different parts of the world including the Mediterranean region and employing various machine learning algorithms using endogenous or exogenous datasets. Further information on these studies is listed in Table 1.

The main concern regarding the studies available in the literature is that they do not cover all of the weather patterns. Another problem faced with the studies in the literature is that the results presented in these studies are not totally comprehensible as they tend to report only numerical values. However, as the GHI values range varies in different parts of the world, it becomes more difficult to compare between different studies.

There are not many studies for the Mediterranean region. We need to investigate promising GHI forecasting algorithms for the Mediterranean region. Our work concentrates on forecasting short-term GHI for Kalkanli, Northern Cyprus, by testing different hybrid forecasting algorithms. There is a need to investigate hybrid forecasting algorithms for GHI prediction. Particularly the hybrid 1D-CNN and LSTM, and hybrid 1D-CNN, LSTM and SVR models were not studied yet. Extensive evaluation and comparisons show that these hybrid models are effective in short-term GHI prediction.

Materials
Information on the study area, data used and the algorithms employed is presented in detail in the following subsections.

Study area and data
In this study, the GHI prediction is conducted for Kalkanli, Northern Cyprus, which is located in the Eastern Mediterranean sea at 35°N and 33°E. Mediterranean climate dominates over the island, resulting in a semi-arid climate with average temperatures of 30°C and 13°C in summer and winter, respectively. The summers are dry and mostly sunny on the island, while the winters are rainy and cloudy. Cyprus has excellent potential for receiving solar radiation with a yearly average GHI potential of 5.4 kWh m−2 [37]. In Northern Cyprus, the majority of the electricity demand has been supplied by conventional energy sources. In recent years, the developments and affordability of PV panels have resulted in many households installing PV panels over their rooftops. There are also two PV farms in Northern Cyprus.

The parameters in the dataset, including the input and the target, used for this study are real data and recorded by sensors in Middle East Technical University Northern Cyprus Campus (METU NCC), Kalkanli. All data are recorded at 10-min intervals. The dataset consists of radiation (GHI), relative humidity (RH), pressure (P), temperature (T), wind speed (WS), wind direction (WD), with the corresponding month (M), day (D), hour (H) and minute (M) information between 2013 and 2017. The average GHI is 409 W m−2 over the whole dataset, while it is 510 W m−2 for the summer season and 275 W m−2 for the winter season. Minimum and maximum available data points for each variable are listed in Table 2.

Table 2 Input variables’ units and data ranges in METU NCC dataset
Full size table
Figure 1 demonstrates the temporal distribution of GHI data points over the whole METU NCC dataset in three dimensions. In the summer season, during noon, the highest GHI values are obtained. Figure 1 illustrates that the change in GHI has higher fluctuations during the winter and spring seasons, while a smoother curve is obtained during the summer to fall seasons. GHI fluctuations are occurring due to overcast sky conditions prominently observed in the winter and spring seasons.

Fig. 1
figure 1
Temporal distribution of GHI throughout the dataset

Full size image
Two-dimensional representations of the change in GHI values over a week in February, March, July and October months to represent all four seasons are illustrated in Fig. 2a–d, respectively. The x-axes show the continuous number of data points in a 10-min time interval, whereas the y-axes show the change in GHI. The fluctuations observed in Fig. 1 in the winter and spring seasons, i.e. November to May, are clearly illustrated on a daily basis in Fig. 2a and b. The smooth curves observed in Fig. 1 in the summer and fall seasons, i.e. June to October, on the other hand, are shown in detail in Fig. 2c and d, which are a result of clear sky condition. These variations in seasons are characteristic to the Eastern Mediterranean region.

Fig. 2
figure 2
Change in GHI values over a week in a February, b March, c June and d October

Full size image
Methodology
Convolutional neural networks (CNN)
CNN is one of the most popular ANN algorithms commonly used in the deep learning field [38]. One of the main advantages of using CNN is its powerful ability to achieve nonlinear feature extraction [39]. Various architectures of CNN are available in the literature, resulting from different combinations and numbers of layers that mainly consist of convolutional, pooling and fully connected layer, i.e. dense layer [40]. In the convolutional layer, feature maps are generated from the input data using filters, i.e. neurons composed of kernels [1]. Each feature map is created by a kernel which represents a different weight matrix. During the training phase, the weight values and a bias term are updated. The mathematical formula of the convolutional layer is shown in Eq. (1) [18, 41],

𝑦𝑙𝑖,𝑗,𝑘=𝐹((𝑤𝑙𝑘)𝑇𝑥𝑙𝑖𝑗+𝑏𝑙𝑘)
(1)
where the weight and bias of kth convolutional kernel in the lth layer are represented as 𝑤𝑙𝑘 and 𝑏𝑙𝑘, respectively. 𝑥𝑙𝑖𝑗 is the input patch in the lth layer, concentrated at the location (i,j). F() represents the activation function, e.g. ReLu, Sigmoid, tanh, etc., to achieve nonlinearity. In Fig. 3a, the generation of feature maps in the convolutional layer is shown.

Fig. 3
figure 3
The architecture of a general 1D-CNN

Full size image
The primary purpose of the pooling layer is to decrease the resolution of the feature map. It is commonly placed between two convolutional layers. The mathematical representation of the pooling layer is shown in Eq. (2) [18, 41],

𝑃𝑙𝑖,𝑗,𝑘=𝑃𝑜𝑜𝑙(𝑦𝑙𝑚,𝑛,𝑘)
(2)
where (m,n) ∈ Ri,j which represent the region around the location (i,j). Pool () describes the type of pooling operation, e.g. average pooling, max-pooling, etc., used in the layer. The pooling layer usually increases network accuracy while decreasing the training time by reducing the number of parameters in the network [41]. Both the pooling function block and the downsized matrices are presented in Fig. 3b.

The fully connected layer, also called the dense layer, aims to perform high-level reasoning by transporting the learned feature in the network to one space [40] by connecting each neuron from the previous layers to every neuron in the current layer, as shown in Fig. 3c. Typically one or more dense layers exist in CNN models after convolution and pooling layers [41]. The last dense layer generates the network output.

Different types of CNN are available. However, two-dimensional CNN (2D-CNN) and one-dimensional CNN (1D-CNN) are commonly used in the literature. 2D-CNN is usually used in text and image analyses, while 1D-CNN is widely associated with numerical data such as energy production and meteorological variables. Nevertheless, both types consist of the same main layers where the main difference is in the filter’s dimension [39]. A sample for a 1D-CNN is illustrated in Fig. 3 with all four main layers. In this study, various 1D-CNN algorithms are constructed for predictions using 1D convolutional and 1D pooling layers available in the Keras library [42] and are called CNN for convenience. In 1D-CNN, convolution involves sliding the filter over the input data, which performs a shift-multiply-sum procedure. In our implementation, this is done with cross-correlation (used in typical CNNs). The output data length is made equal to the length of the input data using padding operation in our 1D-Convolutional layer implementation.

Long short-term memory (LSTM)
Recurrent neural network (RNN) algorithms are capable of recognising the relations in the consecutive data. However, they lack this ability in the long-term patterns. Therefore, LSTM, which belongs to the RNN algorithms, has been introduced to overcome the gradient vanishing and exploding problems by a memory cell first introduced by Hochreiter and Schmidhuber [18, 43] and extra forget gate brought by Gers et al. [44]. The memory blocks in the LSTM provide the algorithm with the ability to update and control the flow of information in a separate block, hence the ability to define the underlying long-term relations [1].

Figure 4 illustrates a sample structure for an LSTM block. Forget gate ft, input gate it, intermediate state gt and output gate ot formulated in Eqs. (3)–(6), respectively,

𝑓𝑡=𝜎(𝑊𝑓𝑥𝑋𝑡+𝑊𝑓ℎℎ𝑡−1+𝑏𝑓)
(3)
𝑖𝑡=𝜎(𝑊𝑖𝑥𝑋𝑡+𝑊𝑖ℎℎ𝑡−1+𝑏𝑖)
(4)
𝑔𝑡=𝑅𝑒𝐿𝑢(𝑊𝑔𝑥𝑋𝑡+𝑊𝑔ℎℎ𝑡−1+𝑏𝑔)
(5)
𝑜𝑡=𝜎(𝑊𝑜𝑥𝑋𝑡+𝑊𝑜ℎℎ𝑡−1+𝑏𝑜)
(6)
where the nonlinear activation function (sigmoid function) noted as σ, Wx and Wh is the weight matrices, and b is the bias of the relevant gates, Xt refers to input of the current time step while the output of the previous time step is ht-1. Forget gate decides which information to keep from the previous memory cell (mt-1), while the information to preserve in the current memory cell (mt) is determined by the input gate. mt is then calculated as given in Eq. (7)

𝑚𝑡=𝑔𝑡⊙𝑖𝑡+𝑚𝑡−1⊙𝑓𝑡
(7)
where ⊙ refers to Hadamard product. Then, the output gate decides of which memory cell to pass as output (ht) as formulated in Eq. (8)

ℎ𝑡=𝑅𝑒𝐿𝑢(𝑚𝑡)⊙𝑜𝑡
(8)
Fig. 4
figure 4
Sample structure of LSTM unit

Full size image
The same process that is shown in Eqs. (3) to (8) takes place in the following time steps. Additionally, the weights and the biases are adjusted in the training phase by decreasing the difference between the predicted and the actual value. The predicted output of the LSTM (𝑦⎯⎯⎯𝑡) is calculated by Eq. (9)

𝑦⎯⎯⎯𝑡=𝑊𝑦ℎ𝑡
(9)
Support vector regression (SVR)
Support vector machine (SVM) used for classification is the famous form of support vector algorithms. However, Vapnik–Chervonenkis developed a version of SVM for regression applications named support vector regression (SVR). SVR uses Vapnik’s ξ-insensitive loss function which penalties all wrong predictions that cross a specific threshold defined based on the ξ value. Various kernels could be used with Vapnik’s ξ-insensitive loss function, including RBF, poly, linear and sigmoid. The efficiency of each function is based on the data and the noise distribution [45].

In SVR, the input vectors are mapped into high-dimensional feature space by nonlinear mapping Φ where linear regression occurs to find a relationship between input and output vectors [46]. SVR algorithm assigns a linear hyperplane, called a decision boundary, to estimate input and output data relation, which is then used to predict future values represented in Eq. (10) [47],

𝑓(𝑥)=𝑤⋅𝜙(𝑥)+𝑏
(10)
where w denotes the learned weight vector, b is the threshold, Φ(x) is the mapping function and f(x) represents the predicted value.

There is a trade-off between minimising the training error and good generalisation behaviour. The algorithm aims to maximise the distance between the decision boundary and data points in order to control the trade-off and obtain a hyperplane with a good regression performance [48]. Equation (11) illustrates compound risk Rreg(f) to balance the trade-off [48],

𝑅𝑟𝑒𝑔(𝑓)=𝐶𝑁∑𝑖=1𝑁𝐿𝜀(𝑓(𝑥𝑖),𝑦𝑖)+12𝑤2
(11)
where C denotes to regularisation parameter, N is the sample size, Lε (f(xi),yi) refers to Vapnik’s ε-insensitive loss function while ||w||2 is the complexity term related to the complexity of the model. Rreg(f) results from model complexity and training errors and should be kept as low as possible. Vapnik’s ε-insensitive loss function is defined by Eq. (12) [46].

𝐿𝜀(𝑓(𝑥)−𝑦)={|𝑓(𝑥)−𝑦|−𝜀0for |𝑓(𝑥)−𝑦|≥𝜀otherwise
(12)
where ε is the maximum error specified by the user to achieve the model’s desired error, f(x) is the predicted, and y is the actual value. When SVR is training, it solves Eqs. (13) and (14) [49].

minimise 𝐶𝑁∑𝑖=1𝑁(𝜉∗𝑖+𝜉𝑖)+12𝑤2
(13)
subject to {𝑦𝑖−𝑤,𝑥𝑖−𝑏≤𝑒+𝜉∗𝑖𝑤,𝑥𝑖+𝑏−𝑦𝑖≤𝑒+𝜉𝑖
(14)
where ξi is the distance between the bounds, which are defined by ε, and the predicted values outside the bounds.

Model evaluation metrics
Various most common score matrices in regression model evaluation are adapted in this study to evaluate the performance of the prediction models. These evaluation models are mean absolute error (MAE) in Wm−2, mean absolute percentage error (MAPE) in %, root mean square error (RMSE) in Wm−2, normalised RMSE (n-RMSE) and coefficient of determination (R2), the mathematical formulation of whom are illustrated in Eqs. (15) to (19), respectively [1].

MAE=1𝑁∑𝑖=1𝑁∣∣GHI𝑟,𝑖−GHI𝑝,𝑖∣∣
(15)
MAPE=1𝑁∑𝑖=1𝑁∣∣∣GHI𝑟,𝑖−GHI𝑝,𝑖GHI𝑟,𝑖∣∣∣
(16)
RMSE=1𝑁∑𝑖=1𝑁(GHI𝑟,𝑖−GHI𝑝,𝑖)2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾⎷
(17)
nRMSE=1GHI𝑟,𝑖1𝑁∑𝑖=1𝑁(GHI𝑟,𝑖−GHI𝑝,𝑖)2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾⎷
(18)
𝑅2=∑𝑁𝑖=1(GHI𝑝,𝑖−GHI𝑚,𝑖)2∑𝑁𝑖=1(GHI𝑟,𝑖−GHI𝑚,𝑖)2
(19)
where GHIr, GHIp and GHIm are the ith measured (real), predicted and mean GHI values, respectively, while N is the number of data points.

In forecasting problems where only small errors are accepted, RMSE is employed instead of its predecessor [5]. nRMSE is frequently calculated instead of RMSE for a meaningful comparison. Mohammadi et al. [50] defined ranges for nRMSE in order to measure a model’s performance. The ranges are tabulated in Table 3.

Table 3 Model performance classified according to nRMSE value
Full size table
RMSE and MAE being close to each other as value means that the forecast model has only small deviations from the real data [51]. Additionally, R2 measures how well the prediction fits the data. In other words, it illustrates the difference between the predicted values and the variance of the errors [52]. The value of R2 varies between zero and one, where zero means that the regression forecasting poorly fit the data while one means perfect fit. Although it is commonly used, MAPE has many disadvantages argued by several researchers [53,54,55]. Resulting in biased and underestimated results and its inability to deal with zero predictions are the most prominent disadvantages of MAPE. Therefore, in this study, we will not consider MAPE in our model evaluation methods.

In addition to error metrics, the absolute difference between prediction and forecast value, i.e. absolute prediction error (APE), will be evaluated through histograms. APE is calculated by Eq. (20).

APE=∣∣GHIr,i−GHIp,i∣∣
(20)
The input data is usually divided into three sets, which are called training, validation and testing sets. The training set is used to fit the model, and the validation set is used for parameter tuning, while the testing set is used to evaluate the final model [56]. Both training and testing sets are evaluated with an error metric. The resulting difference between the two error values gives an idea of the model’s overall performance.

We also compute hypothesis test statistics for statistical analysis of results in Sect. 5.3. Two-sample t-test is employed in our experiments. We check whether there is a significant means difference between the forecasting GHI values and the actual GHI data. We also conduct hypothesis test between models to observe differences in results.

Model construction
Data preprocessing
Preprocessing is transforming the data so that the algorithm can easily interpret the features of the data. Prior to the construction of the learning algorithms, each dataset is preprocessed in accordance with the steps shown in Fig. 5.

Fig. 5
figure 5
Flowchart for the dataset preprocessing procedure

Full size image
In the data cleaning step, missing GHI and temperature values are filled using the average of previous years on the same date if several days of data are missing. The remaining points are filled by linear interpolation. Following the data cleaning step, the non-useful data, i.e. night hours where GHI values are recorded as zero, are removed from the dataset. This step is particularly important as removing night hours leave only the meaningful data improving the prediction model’s performance [5]. Hence, data is trimmed between 6–7 am and 5–7 pm, depending on the season. Next, the variables used as input for the algorithms are finalised in the variable removing step, which reduces the dimensionality of the dataset to increase the performance of the model. The removing inutile variables step is carried out manually, taking similar studies from the literature as a reference point. For this purpose, year data and wind speed variables at 60, 50 and 40 m are removed from the dataset since they have similar values as wind speed at 30 m.

The variable removing step is followed by splitting the input dataset into training and testing datasets. The training dataset consists of 80% of the whole dataset, while the remaining 20% is left for the testing set, following the rule of thumb. Additionally, 10% of the training set is used as a validation set in the training process to guide the parameter optimisation in each algorithm. Validation is used to prevent overfitting and adjust parameters such as weights and biases. The validation and testing sets are used instead of cross-validation methods since these methods tend to have high computational costs on deep learning algorithms. This rule is applied to all algorithms.

The dataset is used as exogenous, i.e. weather variables and radiation, and endogenous, i.e. only radiation data, in two different prediction methods. The first method is a widely used annual prediction method. The second method is seasonal prediction, recently gaining attention from researchers to forecast GHI, specifically in the Mediterranean region. In seasonal forecasting, unlike annual forecasting, the years in the dataset are separated into seasons. It is observed from the studies in the literature that this method mostly results in better prediction results in the Mediterranean region. In annual forecasting, the performance of stand-alone models will be compared to hybrid models’ performance. On the other hand, only hybrid models will be developed for seasonal forecasting.

The dimension of the input tensor of all training and testing sets is in the form of (sample size, time step, features). Detailed information on the input dimensions for the exogenous dataset is given in Table 4. In the endogenous dataset, the number of features is one that refers to the GHI time series. The output of all algorithms is a scalar value.

Table 4 Input tensor dimensions of each dataset
Full size table
A sample set of the variables in the preprocessed METU NCC dataset is illustrated in Table 5 where M, D, H, Min, WS30, WD, T, RH, P and GHI stand for month, day, hour, minute, wind speed at 30 m, wind direction, temperature, relative humidity, pressure and global horizontal irradiation, respectively. The first and last five rows of data are shown for a complete representation. It should be noted that the time is in the 24-h format.

Table 5 A sample set from the METU NCC dataset
Full size table
Construction of learning models
In this section, the construction of the prediction models and the finalised model parameters is discussed. The algorithms are implemented in Python 3.7 [57], using freely available Keras [58], tensor flow [59] and Sklearn [60] libraries on Windows 10 operating system. There are two methods of GHI prediction, as mentioned before, namely annual and seasonal forecasting. Exogenous and endogenous datasets formed from METU NCC data are used in both methods. In annual forecasting, all data points in the dataset are used for training and testing. However, in seasonal forecasting, data points are separated according to seasons, and the algorithms are built for each season. The following sections cover algorithm construction of stand-alone CNN, LSTM and SVR algorithms as well as hybrid CNN-LSTM and CNN-LSTM-SVR algorithms for annual forecasting followed by hybrid algorithms for the seasonal forecasting. Each model, except stand-alone SVR which receives only two-dimensional input data, is trained on multiple time horizons, i.e. 10 min, 30 min and 60 min, to estimate the coming 10th-minute value. In other words, with a 10-min lead time, the algorithm predicts the 20th minute, while with the 30-min horizon, it predicts the coming 40th minute.

The parameters, layers and neurons of each algorithm are configurated through grid search. Table 6 lists the search space of the hyperparameters used in grid search for all algorithms constructed in this study.

Table 6 Hyperparameters used in the grid search in each layer for the constructed forecasting models
Full size table
In the training process for deep learning algorithms, MSE is used for parameter tuning, while the MAE is used as the objective function to be minimised in each dataset. Additionally, the learning rate for all the algorithms is kept at 0.01 while the activation function in CNN and LSTM layers are selected as ReLU. Also, the pooling layer in CNN is not added to the constructed algorithms since it decreased the algorithms’ performance considerably. Finally, the optimiser for all learning models is chosen to be Adam optimiser, i.e. adaptive moment estimation, after trying other optimiser types as listed in Table 6. Adam optimiser results in the lowest computation time with low error, which is also stated in [61].

Annual forecasting
In this forecasting method, all samples in the dataset are divided into training, testing and validation sets. Stand-alone algorithms of CNN, LSTM and SVR are designed in order to compare with the performances of the hybrid algorithms. CNN model is developed for the exogenous METU NCC dataset which is designed to have three convolutional layers connected to three fully connected layers. Table 7 illustrates examples of the input and the corresponding output of the CNN model on 30-min time interval. M, D, H, Min, WS30, WD, T, RH, P and GHI stand for month, day, hour, minute, wind speed at 30 m, wind direction, temperature, relative humidity, pressure and global horizontal irradiation, respectively. The units of each variable are given Table 2.

Table 7 Input and output of CNN model on 30-min interval
Full size table
Next, LSTM model is developed with the endogenous dataset. It contains two LSTM layers with two dense layers. Similarly, Table 8 illustrates examples of the input and the corresponding output of the LSTM model on 30-min time interval where GHI stands for global horizontal irradiation.

Table 8 Input and output of LSTM model on 30-min interval
Full size table
The number of epochs and batch size in both models are set as 100 and 300, respectively. Hyperparameters in each layer and parameters for CNN and LSTM are listed in Table 9.

Table 9 Training parameters and input data information for CNN and LSTM algorithms of annual forecasting
Full size table
Finally, the SVR model is developed for the endogenous dataset. Table 10 illustrates the input and output of the SVR model. C and ε are selected as 10 and 0.05, and the kernel is set as RBF.

Table 10 Input and output of SVR model on 10-min interval
Full size table
Hybrid algorithms are the main objective of this study. The first hybrid algorithm is a combination of CNN and LSTM, which is named C-LSTM for convenience. In this algorithm, CNN layers are used for the feature extraction step before LSTM layers. There are three convolutional layers connected to two LSTM layers by a time distributed layer. Two fully connected layers follow LSTM layers to give the prediction output. Convolutional layers are fed with the exogenous dataset. The batch size and epochs are chosen as 150 and 100, respectively.

The second hybrid algorithm is designed to feed CNN only exogenous data while feeding endogenous data to LSTM and feed their output to dense layers for final output, as illustrated in Fig. 6. The algorithm is called CN-M for convenience. The CNN is composed of four convolutional layers, while LSTM has three layers. The outputs of both layers are merged and fed to two dense layers. In this algorithm, batch size and epochs are optimised as 150 and 300, respectively. Hyperparameters in each layer of both hybrid forecasting algorithms are summarised in Table 11.

Fig. 6
figure 6
Flowchart showing the hybrid CN-M algorithm

Full size image
Table 11 Training parameters and input data information for the hybrid algorithms, C-LSTM and CN-M, of annual forecasting
Full size table
The final hybrid algorithm is designed similar to CN-M algorithm, except CNN and LSTM are constructed as algorithms, and their output is merged and fed to an SVR model named CM-SVR. A flowchart simply illustrating the construction of the CM-SVR algorithm is presented in Fig. 7. CNN algorithm is made of three convolutional layers. The output of the convolutional layer is then flattened and fed to two fully connected layers. The batch size and number of epochs of CNN are set as 500 and 100, respectively. LSTM algorithm consists of two LSTM layers, followed by two fully connected layers. The number of epochs and batch size optimised as 100 and 300. These two algorithms are trained, and the resulting predictions are combined in an array to feed to SVR algorithm. In SVR algorithms, hyperparameters C and ε are set as 14 and 0.05, respectively, while the kernel is chosen as RBF. Table 12 lists the details on hyperparameters of each layer in all algorithms.

Fig. 7
figure 7
Flowchart showing the hybrid CM-SVR algorithm

Full size image
Table 12 Training parameters and input data information for the hybrid algorithm, CM-SVR, of annual forecasting
Full size table
Seasonal forecasting
In the seasonal forecasting method, the METU NCC dataset is divided into four sub-datasets. The division is based on months depending on the seasons, namely summer, fall, winter and spring. For seasonal forecasting, only CM-SVR models are developed for each season, and each algorithm is built separately with the corresponding season. CNN algorithm is fed with the exogenous dataset, while the LSTM is fed with the endogenous dataset. Finally, SVR model is fed with an array prepared by combining outputs of CNN and LSTM.

The summer season dataset is made of the months June, July and August. CNN algorithm is made of three convolutional layers. The output of the convolutional layer is then flattened and fed to three dense layers. LSTM algorithm consists of two layers, followed by two fully connected layers. Finally, in SVR model, the hyperparameters C and ε are set as 2 and 0.05, respectively, while the kernel is chosen as RBF. For the fall season, only September, October and November months are included in the dataset. CNN, LSTM and SVR algorithms are designed similar to the algorithms for the summer season. CNN algorithm is made of four convolutional layers. The output of the convolutional layer is then flattened and fed to two fully connected layers. There are two layers in LSTM algorithm, which are followed by two fully connected layers. Finally, in SVR model, the hyperparameters C and ε are set as 1 and 0.05, respectively, while the kernel is chosen as RBF. In the winter season, December, January and February months are selected in the dataset. There are three convolutional and two dense layers in CNN, while LSTM algorithm has three LSTM layers and two dense layers. C and ε in SVR are set as 10 and 0.03, respectively. Finally, the spring dataset is composed of March, April and May. CNN algorithm is composed of three convolutional layers and three dense layers. LSTM algorithm as well has three LSTM layers and three dense layers. C and ε in SVR are set as 4 and 0.05, respectively. A detailed list of all hyperparameters in each layer in all algorithms is presented in Table 13.

Table 13 Layers in hybrid CM-SVR algorithm for seasonal forecasting
Full size table
Results and discussion
GHI and meteorological data with a 10-min time interval for METU NCC, Kalkanli, are used in this study. The dataset is preprocessed, and two datasets are created from it. The exogenous dataset consists of meteorological variables and GHI data, whereas the endogenous dataset is made of only GHI data. In this section, first we present performances of models, described and constructed in Sect. 4, for GHI prediction. Then, model performances with respect to different optimisers are provided, and finally hypothesis test is conducted for statistical analysis of results.

GHI forecasting performances
CNN, LSTM and SVR algorithms are employed for the GHI forecasting. There are two different forecasting methods followed in this study, i.e. annual and seasonal forecasting, hybrid algorithms of CNN, LSTM and SVR, namely C-LSTM, CN-M and CM-SVR, are created and compared with the performance of stand-alone algorithms, i.e. CNN, LSTM and SVR, in the annual forecasting part. In the seasonal forecasting part, the performance of models for each season is evaluated through the hybrid algorithm CM-SVR. After the construction of the algorithms, the training is initialised. The constructed algorithms are trained on several time horizons, i.e. time intervals of 10 min, 30 min and 60 min. The hyperparameter of the algorithms is optimised by manual grid search due to high computational cost. Αfter training the models, the results of the trained models were evaluated in the testing phase using MAE, RMSE, n-RMSE and R2. A flow diagram summarising the process of GHI forecasting from dataset preprocessing to the final result is illustrated in Fig. 8.

Fig. 8
figure 8
Flowchart of the forecasting procedure

Full size image
Testing results of stand-alone and hybrid prediction algorithms are listed in Table 14 with corresponding evaluation metric scores in three forecasting horizons, where the best results are shown in bold. The reported evaluation results are the mean of five separate runs for each model where data is randomly partitioned into training, validation and test set. In terms of evaluation metrics’ results, the models perform similarly. The models also have a good model precision, according to Table 3, with n-RMSE changing between 0.14 and 0.17. CN-M achieves better results with respect to MAE among the hybrid algorithms. CN-M also has an MAE of 28.3 and 37.8 W m−2 for training and validation sets, respectively. Additionally, R2 results illustrate that all models fit the data quite well. On the other hand, all evaluation results suggest that CM-SVR performs poorly in 10-min forecasting lead.

Table 14 Summary of GHI prediction model performances in different time leads for annual forecasting, best results are shown in bold
Full size table
Figures 9 and 10 demonstrate the frequency of APE on a histogram for stand-alone and hybrid models, respectively. Although the models performed similarly in terms of evaluation metrics, stand-alone CNN and SVR result in poor prediction outputs. On the other hand, stand-alone LSTM has a high MAE result in the 30-min horizon, indicating a high average error magnitude. However, LSTM histogram shows that more than 40% of the APE is 10 W m−2. Among the hybrid models, CN-M and CM-SVR result in approximately 45% of APE less than 10 W m−2, performing better than the rest of the models when evaluation metrics’ results are also considered. However, model preference could be made based on computation time which is approximately 20 s in the CM-SVR, while 18 s in the CN-M model for cases where a high error in prediction is insignificant.

Fig. 9
figure 9
APE frequency histograms generated by the results of the testing set for the stand-alone models over 30-min forecasting horizon, a CNN—endogenous, b LSTM—exogenous, c SVR—exogenous

Full size image
Fig. 10
figure 10
APE frequency histograms generated by the results of the testing set for the hybrid models, a C-LSTM, b CN-M, c CM-SVR

Full size image
We use SVR in CM-SVR because we observed that after combining SVR with CNN and LSTM, high error prediction values are significantly eliminated. This means that SVR contributes to the combination (CNN, LSTM and SVR combination). We also replaced the SVR with other machine learning models in this combination. Such as we tested linear regression combined with CNN and LSTM, GradientBoost combined with CNN and LSTM, and decision tree combined with CNN and LSTM. The results show that SVR combined with CNN and LSTM (CM-SVR) achieve better results than others. CM-SVR has MAE = 49.3, R2 = 0.88, and nRMSE = 0.24. On the other hand, linear regression + CNN + LSTM has MAE = 55.1, R2 = 0.89, nRMSE = 0.25, GradientBoost + CNN + LSTM has MAE = 53.9, R2 = 0.88, nRMSE = 0.24, and Decision Tree + CNN + LSTM has MAE = 76.8, R2 = 0.76, nRMSE = 0.35.

Evaluation metric results of the testing set on CM-SVR for the seasonal prediction are presented in Table 15 with corresponding evaluation metric scores in three forecasting horizons. The CM-SVR model for the summer season produces the best results among all seasons as expected since clear sky conditions occur most prominently in the summer months, as illustrated in Fig. 11. On that note, the n-RMSE of the summer model has excellent precision, according to Table 3. On the other hand, the model of the winter season results in higher error outputs as a result of high fluctuations in GHI, i.e. overcast sky condition, as shown in Fig. 12. Hence, the model has a fair precision with an n-RMSE of 0.24. Additionally, the forecasting model for the spring season behaves similar to the model of the summer season, while the fall model performs closer to the winter model. Similar patterns are observed in R2 results that the summer and spring models fit almost perfectly on the actual data.

Table 15 Summary of GHI prediction model performances in different time leads for seasonal forecasting
Full size table
Fig. 11
figure 11
CM-SVR model prediction fitted over the actual data for the summer season for a clear-sky condition

Full size image
Fig. 12
figure 12
CM-SVR model prediction fitted over the actual data for the winter season for a scattered clouds sky condition

Full size image
Figure 13 demonstrates the frequency of APE on histograms for each season separately. The evaluation metric results comply with APE results for the winter season. Also, the models for the summer and spring predict 40% of the results with a deviation of less than 10 W m−2, performing better than the rest of the seasons.

Fig. 13
figure 13
APE frequency histograms generated by the results of the testing set for the seasonal forecasting with CM-SVR models, a summer, b fall, c winter, d spring

Full size image
The evaluation results of seasonal forecasting models are averaged and presented in Table 16 in order to compare the annual and seasonal forecasting models. On average, the seasonal forecasting models performs similar to the annual forecasting algorithm. In addition to the performances, all models in seasonal forecasting compute results in 1 s, making them the fastest forecasting models compared to stand-alone and hybrid models of annual forecasting. Hence, seasonal forecasting could be preferable to annual forecasting for the Eastern Mediterranean region, where the seasons have distinct patterns in GHI fluctuations.

Table 16 Averaged evaluation metrics’ results of all seasons in different forecasting horizons
Full size table
The literature could guide the researcher to build their forecasting model to achieve the highest accuracy in GHI forecasting. However, optimising the model’s hyperparameters for specific data and areas is challenging and could be time-consuming. In addition to difficulties with hyperparameter tuning, measuring the performances of the regression models in a comprehensible way is a challenge faced not only in the studies available in the literature but also in this study. In this study, we overcome this problem by applying histograms in addition to traditional regression performance matrices.

The models developed in this study have relatively high performances with low computational power compared to the studies around the world. Furthermore, the proposed models show exceptional performances compared with the studies available for the Mediterranean region. Moreover, this study proposes different types of the model using different sets of data which makes them applicable on a wider range.

Forecasting model performances with respect to different optimisers
As we have described in Sect. 4 and Table 16, we determine hyperparameters and optimiser of the models through the grid search. Among the optimisers, we experimented Adam, Adadelta, Adamax, Nadam and SGD. The results show that Adam optimiser gives relatively better results than the other optimisers. Performances with respect to different optimisers for three different time leads (10 mins, 30 mins and 60 mins) are given in Tables 17, 18 and 19, respectively. We illustrate performances with three different metrics in these tables, namely MAE, n-RMSE and R2. From the tables, it is observed that models perform their best results with Adam optimiser.

Table 17 GHI prediction model performances in 10-min time leads for annual forecasting with respect to different optimisers
Full size table
Table 18 GHI prediction model performances in 30-min time leads for annual forecasting with respect to different optimisers
Full size table
Table 19 GHI prediction model performances in 60-min time leads for annual forecasting with respect to different optimisers
Full size table
Statistical analysis
The hypothesis test statistics [62] is also computed to prove that there is no significant means difference between the forecasting GHI values and the actual GHI data. Two-sample t-test is used in our experiments. There are two hypotheses, the null hypothesis (H0) and the alternative hypothesis (H1). Let μ1 be the mean of actual data, and μ2 be the mean of predicted values of the forecasting model, then the hypothesis is defined by

𝐻0:𝜇1−𝜇2=0(𝑛𝑢𝑙𝑙ℎ𝑦𝑝𝑜𝑡ℎ𝑒𝑠𝑖𝑠)
(21)
𝐻1:𝜇1−𝜇2≠0(𝑎𝑙𝑡𝑒𝑟𝑛𝑎𝑡𝑖𝑣𝑒ℎ𝑦𝑝𝑜𝑡ℎ𝑒𝑠𝑖𝑠)
(22)
The test statistic with unknown but equal variance is computed as:

𝑡=𝜇1−𝜇2𝑠21+𝑠22𝑛‾‾‾‾‾√
(23)
where 𝑠1 and 𝑠2 are sample standard deviations, and n is the sample size. The t disribution has 2𝑛−2 the degrees of fredoom. We compute the p-value at 5% significant level. If the p-value is high, it shows that the test fails to reject the null hypothesis. (Performances are significantly similar.) If the hypothesis test fails to reject the null hypothesis, it indicates that there is no significant means difference between the forecasting values and the actual data. In this case, the forecasting model is appropriate to represent and predict the real situation.

Table 20 shows the results of two-sample t-test. The t-test is used to compare the actual GHI data with the six forecasting models which have been presented, namely CNN, LSTM, SVR, C-LSTM, CN-M and CM-SVR. The test is done for three different time leads (10 mins, 30 mins and 60 mins). The p-value calculated after the t-test is used to ensure that there is no statistically significant difference of means between the actual data and the forecast values of models. If the p-value is greater than 0.05, we assume that the p-value is high and the performance of the model is good. The results show that C-LSTM and CV-SVR have significantly high p-values for all time leads. The p-value of CN-M is high for 10-mins and 60-min time leads. For LSTM, the p-value is high for 10-min and 30-min time leads, CNN has a high p-value for 60-min time lead, and SVR also has a high p-value for 10-min time lead. Table 21 shows p-values for seasonal forecasting using CM-SVR for 10-min, 30-min and 60-min time leads. It is observed that p-values are high for all seasons and time leads.

Table 20 Two-sample t-test p-values for each model. P-values are calculated between actual GHI data and forecasted GHI values
Full size table
Table 21 Two sample t-test p-values for seasonal forecasting using CM-SVR
Full size table
We also conduct hypothesis test between models to observe differences in performance. For example, when comparing CNN with C-LSTM (CNN combined with LSTM), the p-value indicates whether the performance of CNN significantly changed after combination or not. So it gives us information whether the hybrid model changes the performance of stand-alone model or not. Such as if the p-value is very small, it shows that we reject the null hypothesis (performances are significantly different). On the other hand, if the p-value is very high, it indicates that the sand-alone model contributes to hybrid model after combination. This statistical analysis are conducted between two different models including different stand-alone models and different hybrid models for 10-min time lead. Table 22 shows the p-values between each model. The “NaN” in the table means that a method cannot be compared with itself. If the p-value is less than 0.05, we assume that the p-value is small, and the performances of two models are different. It is observed that the performance of CNN is different from all other models since p-values are small. This also indicates that combining CNN with LSTM (C-LSTM and CN-M) changes the performance of CNN. C-LSTM and CN-M hybrid models have better performance than CNN as can be seen in Table 22 (performances with metrics). Similarly, combining CNN with LSTM and SVR (CM-SVR) also changes the performance of CNN. As shown in Table 22, CM-SVR hybrid model also optimises the performance of stand-alone CNN. The LSTM performance is similar to the hybrid models since p-values are high. This result indicates that the LSTM model has significant contribution to the hybrid models when it is used in combinations. SVR also has significant contribution to the hybrid model CM-SVR since the p-value is high in this combination.

Table 22 Comparing two different models with two-sample t-test p-values
Full size table
Conclusion
In this work, it is aimed to contribute to ongoing research on developing prediction models to accurately estimate solar radiation and energy production by testing different hybrid forecasting algorithms. This study provides theoretical forecasting models to predict short-term GHI using real-world data. The models are constructed using data collected in Kalkanli region in Northern Cyprus, where there are limited energy production sources and a high rate of PV installation over rooftops.

In the study, two different forecasting methods are followed, i.e. annual forecasting and seasonal forecasting. In annual forecasting, all data are used for training and testing. Overall, six different models are designed for annual forecasting; three stand-alone models, i.e. CNN, LSTM and SVR, and three hybrid models, i.e. C-LSTM, CN-M and CM-SVR. Among the models created, CN-M performs relatively better than the remaining models with a lower computational cost. However, the CM-SVR model showed a significant improvement in large margin errors compared with the other models, including CN-M. In seasonal forecasting, four sub-datasets are created based on the seasons. For each season, a CM-SVR model is designed. The evaluation results suggest that the summer model achieves the lowest error, while the winter model results in the highest error. For the Mediterranean region, where seasons have distinct sky condition patterns, it could be preferable to have different separate models for each season. When compared to the performance of annual forecasting models, seasonal models perform similarly on average. However, low computation cost makes the seasonal models desirable.

We successfully constructed effective stand-alone and particularly hybrid models for short-term GHI forecasting. The importance of this study is that it illustrates the performances of these models predicting GHI, which is the main parameter on PV power generation. The information on PV power output enables the power generation utility to maximise the use of PV panels and to decrease the use of conventional energy sources that contribute to global warming.

Global horizontal irradiation
Deep learning
Time-series forecasting
Seasonal forecasting
Hybrid forecasting algorithms