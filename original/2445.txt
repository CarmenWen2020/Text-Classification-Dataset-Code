Recently, many outstanding techniques for Time series forecasting (TSF) have been proposed. These techniques depend on necessary and sufficient data samples, which is the key to train a good predictor. Thus, an Active learning (AL) algorithmic framework based on Support vector regression (SVR) is designed for TSF, with the goal to choose the most valuable samples and reduce the complexity of the training set. To evaluate the quality of samples comprehensively, multiple essential criteria, such as informativeness, representativeness and diversity, are considered in a two clustering-based consecutive stages procedure. In addition, considering the imbalance of time series data, a range of values might be seriously under-represented but extremely important to the user. Thus, it is unreasonable to assign the same prediction cost to each sample. To address this imbalance problem, a multiple criteria cost-sensitive active learning algorithm in the virtue of weight SVR architecture, abbreviated as MAW-SVR, ad hoc for imbalanced TSF, is proposed. By introducing the cost-sensitive scheme, each sample is endowed with a penalty weight, which can be dynamically updated in the AL procedure. The experimental comparisons between MAW-SVR and the other six AL algorithms on a total of thirty time series datasets verify the effectiveness of the proposed algorithm.

Introduction
As a major branch of dynamic data analysis, Time series forecasting (TSF) has received extensive attention in recent decades. Time series have obvious nonlinear and non-stationary characteristics, which makes the prediction results inaccurate. Generally, time series can be classified into two categories: stationary and non-stationary. In statistics, given two sequences ùë•ùë°1,‚ãØ,ùë•ùë°ùëò and ùë•ùë°1+ùúè,‚ãØ,ùë•ùë°ùëò+ùúè in a time series, if the joint statistical distributions of the above two sequences, e.g., expectations, variance, third order and higher, are identical for all ùúè, then the time series is strongly stationary. This definition is considered extremely strict for most applications. Hence, a weaker definition, namely weak stationarity, is also used to analyze time series. Weak stationary time series is a time series that has invariant mean and variance under time translation. In fact, the requirement of both strong stationarity and weak stationarity is hard to be satisfied. On the contrary, most real-world datasets are non-stationary [1, 2], which implies that the patterns in time series evolve over time. In addition, time series data are ubiquitous in our everyday life such as urban temperature, stock price, Internet traffic data flow and so on. Thus, analyzing and forecasting time series data has become a hot research field.

In the early days, many common methods used traditional statistical models for TSF, such as Autoregression integrated moving average (ARIMA) [2,3,4], which needs special differencing transformations before training. As a technique for transforming non-stationary time series into stationary ones, the differencing will lead to the loss of some valuable information in the data when its order is an integer [5]. However, ARIMA with fractional differencing is capable of modeling the long-run behavior of time series. Notwithstanding, these statistical models are not suitable for modeling time series with complex nonlinear features. Thus, they have certain limitations in practical applications.

In recent years, machine learning theory has been gradually applied to TSF tasks, such as Support vector machine (SVM) [6, 7], Artificial neural networks (ANNs) [8] and evolutionary computation [9]. In many methods, ANNs, especially Recurrent neural networks (RNNs), have maintained a leading position in the field of TSF due to their excellent learning ability and function approximation capacity. In addition, RNNs can capture the temporal dependency of time series due to the recurrent structure. Nevertheless, ANNs still have some shortcomings, such as being apt to fall into local optimization and longer training time. It is also difficult to set the learning rates and hidden layer sizes in deep learning models. These problems will inevitably affect the generalization ability of ANNs. However, these issues can be solved by SVM based on the structural risk minimization criteria. SVM possesses strong nonlinear expressiveness and simple geometric interpretation, which is widely applied to classification problems. Support vector regression (SVR) is a special implementation of SVM for predictive data analysis, which shares many of the advantages of SVM. The success of SVR largely depends on its ability to capture nonlinear characteristics. Moreover, SVR can effectively deal with high-dimensional and complex regression problems [10], making it promising for TSF.

Additionally, it is necessary to obtain high-quality data samples for training a good model. On the one hand, if the amount of data in the training set is too large, the information that it delivers to the prediction model may be redundant or unnecessary. As a result, the model is prone to overfitting. On the other hand, insufficient training samples may lead to a decrease in model performance and underfitting phenomenon. To address these issues, Active learning (AL) can be applied to select some valuable samples in this situation [11,12,13]. AL attempts to iteratively select a few samples from a pool ùëà of unlabeled samples according to a certain query strategy, and optimizes the training set ùêø with a minimum number of the most valuable samples. In the field of AL, most previous studies have focused on the researches of classification problems [14, 15].

Recently, approaches based on AL have been used to address regression issues. AL can provide high-quality samples while avoiding redundant and unnecessary label information. The core of any AL algorithm is the query strategy. In [16], two new AL regression methods based on Greedy sampling (GS) were proposed, which aim to achieve the diversity in the input or output space by querying new samples. In [17], authors think that there is room for improvement of single query criterion. Thus, when selecting the most beneficial samples, multiple essential criteria, including informativeness, representativeness and diversity, are considered to propose a pool-based sequential AL for regression. Enlightened by the remarkable success of AL in regression [16,17,18,19,20,21], we introduce AL into the field of TSF for reason that not all time series data have positive significance to improve prediction performance.

In this work, a multiple criteria AL framework in virtue of SVR architecture, specifically for TSF, is constructed. The primary motivation for using SVR is its solid theoretical basis, good generalization property and the ability to process high-dimensional inputs when there are few training samples. Besides, SVR also has the ability to automatically extract key samples, namely support vectors, which are required in the procedure of AL. In summary, our method inherits the merits of AL and tries to reduce the complexity of training samples. Meanwhile, it obtains an accurate predictor that provides superior performance to those predictors trained with all samples, as demonstrated in the experiments conducted in this work.

Nevertheless, in many practical cases of pattern classification, the performance of AL is easily disrupted due to the class imbalance problem. Similarly, the imbalance is also observed in time series data, which affects the performance of our proposed AL algorithm. Such examples may be found in solar irradiance analysis, most of weather in some locations is sunny, while cases of windy days are scarce. However, windy days usually attract more attention of the user. In conclusion, there are two properties of the imbalanced TSF problem: one is that the user has uneven preference across the target variable domain, and the other is that the most preferred values are under-represented. This means that some preferred values are associated with extreme and rare values, which are especially valuable for accurate prediction. For example, accurate prediction of extreme values in meteorology can avoid some catastrophic consequences. Considering this imbalance, we advocate that the predictions for time series data should have differentiated costs. Some values that are more meaningful to the user should have higher prediction costs and greater penalty weights. Thus, an effective cost-sensitive AL algorithmic framework, termed multiple criteria active weighted SVR (MAW-SVR), ad hoc for solving imbalanced TSF problem, is proposed in this paper.

Figure 1 depicts the block diagram of MAW-SVR, which is a closed-loop system. Firstly, inspired by the idea of cost-sensitive learning, we choose weighted SVR as the predictor to endow samples with different costs. Then, a small initial training data is input into the weighted SVR to obtain the set of support vectors (SVs) and the set of non-support vectors (non-SVs). Secondly, a clustering algorithm is applied to the unlabeled samples together with the training samples of non-SVs. Based on the clustering results, we propose a two consecutive stages procedure to select some valuable samples by jointly evaluating multiple criteria. Finally, the selected valuable samples are removed from the unlabeled pool and inserted into the training sample set, which ensures that there are no duplicate samples in the clustering. AL repeats the above process until the stop condition is satisfied. The main contributions of this work are summarized as follows.

Fig. 1
figure 1
Block diagram of the proposed MAW-SVR algorithm

Full size image
Firstly, considering that not all time series data are conductive to prediction, we introduce AL into the field of TSF for the first time. Based on the designed sample selection strategy, we construct an AL algorithmic framework to acquire some valuable samples, which reduces the complexity of the training set and exhibits promising performance.

Secondly, we attempt to select high-quality samples by measuring three criteria of informativeness, representativeness and diversity. Given that SVR has the ability to automatically select key samples, all of these three criteria can be tactfully implemented based on the SVR properties and a clustering-based process.

Thirdly, different from the application of AL in regression, our AL algorithmic framework designed for TSF needs to adapt to the peculiarity of time series data caused by imbalance. The imbalance problem brings serious sample bias, which leads to undesirable performance of AL. To alleviate this problem, predictions for time series data should have differentiated costs instead of the same cost. Thus, it is meaningful to combine the cost-sensitive scheme with the AL procedure for imbalanced TSF tasks.

Related work
However, there is almost no research on AL for TSF. Thus, we first discuss the related works about AL for regression. Afterward, we review the related works on data imbalance problem.

Active learning for regression
Inspired by the superior performance of AL in classification tasks [22], AL for regression has been studied in recent years. AL selects the most significant samples for labeling according to one or more criteria. Various measure criteria can generate several different AL algorithms. In [16], a Greedy sampling (GS) technique considering diversity selects an unlabeled instance in a greedy way, which is far away from the previously labeled instances. In [18], a popular AL approach, named query by committee (QBC), calculates the informativeness of an instance by the disagreement among predictors. Moreover, Cai et al. [19] presented an algorithm considering informativeness called Expected model change maximization (EMCM), which aims to select the instance that causes the largest change in the model.

The above-mentioned three AL approaches only leverage one criterion to accomplish the sample selection. However, it is worth considering that multiple criteria can be combined to form a new query strategy. In [20], a new algorithm called Informative density (ID) considering informativeness and representativeness was proposed. The informativeness of the sample is measured according to the QBC, and then, the informativeness is weighted by its average similarity to all other unlabeled samples. In [21], an AL algorithm for regression that jointly measures the above-mentioned three criteria, called Multiple criteria active learning (MCAL), was developed in the framework of SVR. It can be roughly divided into two parts. In the first part, a clustering method is applied to the unlabeled instances and the non-support vectors, and then, the most informative samples are located. In the second part, a novel measure is defined to choose representative and diverse instances in the relevant clusters. Compared with the above-mentioned approaches, there are some significant differences between them and our proposal.

Firstly, the AL process of studies [16, 18,19,20] is different from that of ours. Our proposal integrates three essential criteria, including informativeness, representativeness and diversity, to choose the most beneficial samples from a large number of unlabeled samples. Each criterion reflects one trait of the sample. While in [16, 18,19,20], one or two of the three criteria are utilized to guide the sample selection. It is reasonable to believe that the richer the assessment criteria, the higher the quality of the selected samples.

Then, MCAL [21] integrates multiple measure criteria and has superior performance in the field of regression. In our work, we construct our own AL algorithmic framework ad hoc for TSF by appropriately transforming the above introduced MCAL framework. In particular, differentiated prediction cost is considered due to the imbalance of time series. Thus, the cost-sensitive scheme is skillfully integrated into our AL procedure in virtue of SVR architecture.

Methods for solving data imbalance problem
Data imbalance problem has been extensively discussed in various fields. There are two research lines to solve the imbalance problem. The first one is resample methods [23, 24], which change the distribution of samples across different classes to achieve class balance. Another research line is cost-sensitive methods [25, 26], which utilize cost matrices as weights of particular samples to balance samples. However, both methods predominantly focus on classification tasks.

As with pattern classification tasks, imbalance problem often exists in the field of TSF, which affects the prediction performance. It occurs when some values are least represented but are particularly important to the user. To alleviate this problem, in [27], authors attempt to decrease the skewness of data by leveraging resampling strategies to change the distribution of rare and normal values. In addition, many works [28,29,30] use cost-sensitive method to effectively solve the imbalance problem, but they are mainly applied to classification tasks. In general, resampling methods tend to exclude some important information from the original datasets, or have a high probability of suffering overfitting when the minority class instances are sampled repeatedly, which may hinder learning. Therefore, considering these defects of resampling methods, we attempt to employ cost-sensitive scheme as a strategy to improve the prediction performance of imbalanced TSF issues. Specifically, although most algorithms focus on the average behavior of data and aim to minimize the total cost, we argue that not all the mispredicted instances have the same cost.

Preview of SVR
We construct our own AL algorithmic framework based on ùúÄ-insensitive SVR [31, 32]. Given a training set ùëá={(ùë•ùë•ùëñ,ùë¶ùëñ),ùëñ=1,‚ãØ,ùëö}, where ùë•ùë•ùëñ‚àà‚Ñúùëú is the ùëñ-th sample, ùë¶ùëñ‚àà‚Ñú is the corresponding target value, and ùëú is the sample dimension. The goal of SVR is to search a function ùëì(ùë•ùë•) acquired by mapping the original feature space (‚Ñúùëú) into a higher-dimensional space (‚Ñúùëë) as:

ùëì(ùë•)=ùë§ùëáùúô(ùë•)+ùëèùë§ùëñùë°‚Ñéùë§‚àà‚Ñúùëëùëè‚àà‚Ñú
(1)
where ùëë is the sample dimension in the mapped higher-dimensional space, ùúô(‚àô) is the mapping function, and ùëè is the bias.

In order to acquire the optimal function ùëì(ùë•ùë•), the solution is to solve the minimization problem:

minùë§ùë§,ùëè{12‚Äñùë§ùë§‚Äñ2}
ùë†ùë¢ùëèùëóùëíùëêùë°ùë°ùëú|ùë¶ùëñ‚àí(ùë§ùë§‚ãÖùúô(ùë•ùë•ùëñ)+ùëè)|‚â§ùúÄùëñ=1,2,‚ãØ,ùëö
(2)
where ùëö denotes the number of samples used for model training and each training sample has a function ùëì(ùë•ùë•) with an accuracy of ùúÄ. Nevertheless, in the practical application, to improve generalization ability, some errors can be tolerable. Hence, the idea of soft margin is proposed and the optimization problem in Eq. (2) evolves into as below:

minùë§ùë§,ùëè,ùúâùëñ,ùúâÀÜùëñ{12‚Äñùë§ùë§‚Äñ2+ùê∂‚àëùëñ=1ùëö(ùúâùëñ+ùúâÀÜùëñ)}
ùë†ùë¢ùëèùëóùëíùëêùë°ùë°ùëú‚éß‚é©‚é®‚é™‚é™ùë¶ùëñ‚àí(ùë§ùë§‚ãÖùúô(ùë•ùë•ùëñ)+ùëè)‚â§ùúÄ+ùúâùëñ(ùë§ùë§‚ãÖùúô(ùë•ùë•ùëñ)+ùëè)‚àíùë¶ùëñ‚â§ùúÄ+ùúâÃÇ ùëñùúâùëñ,ùúâÃÇ ùëñ‚â•0ùëñ=1,2,‚ãØ,ùëö
(3)
where ùúâùëñ and ùúâÀÜùëñ are slack variables, which measure the distance between the training examples outside the ùúÄ-insensitive tube and the tube itself. ùê∂ is a regularization parameter, which is used to adjust the trade-off between the smoothness of ùëì(ùë•ùë•) and the tolerance to empirical errors.

According to the Lagrange duality, the problem in Eq. (3) can be rewritten into the form of dual problem, which can be solved as below:

maxùëéùëé,ùëéùëéÀÜ,{‚àëùëñ=1ùëöùë¶ùëñ(ùëéÀÜùëñ‚àíùëéùëñ)‚àíùúÄ(ùëéÀÜùëñ+ùëéùëñ)‚àí12‚àëùëñ=1ùëö‚àëùëó=1ùëö(ùëéÀÜùëñ‚àíùëéùëñ)(ùëéÀÜùëó‚àíùëéùëó)ùúÖ(ùë•ùë•ùëñ,ùë•ùë•ùëó)}
ùë†ùë¢ùëèùëóùëíùëêùë°ùë°ùëú‚éß‚é©‚é®‚é™‚é™‚é™‚é™‚àëùëñ=1ùëö(ùëéÃÇ ùëñ‚àíùëéùëñ)=00‚â§ùëéùëñ‚â§ùê∂0‚â§ùëéÃÇ ùëñ‚â§ùê∂ùëñ=1,2,‚ãØ,ùëö
(4)
where ùëéùëñ and ùëéÀÜùëñ represent the Lagrange multipliers, and ùúÖ(ùë•ùë•ùëñ,ùë•ùë•ùëó)=ùúô(ùë•ùë•ùëñ)ùúô(ùë•ùë•ùëó) is a kernel function. The final solution of SVR can be described by the following linear expansion of kernel function:

ùëì(ùë•ùë•)=‚àëùëöùëñ=1(ùëéÀÜùëñ‚àíùëéùëñ)ùúÖ(ùë•ùë•ùëñ,ùë•ùë•)+ùëè
(5)
where the bias ùëè is calculated as:

ùëè=averageùëñ{ùë¶ùëñ+ùúÄ‚àôsign(ùëéÀÜùëñ‚àíùëéùëñ)‚àí‚àëùëöùëó=1(ùëéÀÜùëó‚àíùëéùëó)ùúÖ(ùë•ùë•ùëó,ùë•ùë•ùëñ)}
(6)
Methodology
The main purpose of our method is to construct an AL procedure, which can extract some valuable samples to train an accurate TSF model. In the task of TSF, it is assumed that an equidistant sampled set {ùë¶ùëñ}ùëñ=1,2,‚ãØ,ùëá of observed time series signals is available, where ùë¶ùëñ is the value sampled at time ùëñ. It is generally believed that there is a certain degree of correlation between successive values of the series. A form of modeling this correlation consists of using the previous values of the series as predictors of the future value. In our AL procedure, we introduce SVR to establish the nonlinear mapping relationship between the predicted value and the historical values. Thus, the TSF problem can be expressed as:

ùë•ùë•ùëñ=(ùë¶ùëñ‚àíùëö,ùë¶ùëñ‚àíùëö+1,‚ãØ,ùë¶ùëñ‚àí1)
(7)
ùë¶ÃÇ ùëñ=ùëì(ùë•ùë•ùëñ)
(8)
where ùëö denotes the time window size and the function ùëì:‚Ñúùëö‚Üí‚Ñú is named the fitting function. It should be emphasized that one-step-ahead time series prediction is the focus of our attention. Generally, we aim at predicting time series in a rolling forecasting fashion. Therefore, to predict ùë¶ùëñ, we assume that ùë•ùë•ùëñ={ùë¶ùëñ‚àíùëö,ùë¶ùëñ‚àíùëö+1,‚ãØ,ùë¶ùëñ‚àí1} is available. Likewise, to predict the value of next timestamp ùë¶ùëñ+1, we assume that ùë•ùë•ùëñ+1={ùë¶ùëñ‚àíùëö+1,ùë¶ùëñ‚àíùëö+2,‚ãØ,ùë¶ùëñ} is available. We hence formulate a pool of unlabeled samples as {ùë•ùë•1,ùë•ùë•2,‚ãØ,ùë•ùë•ùëõ}. According to the position of the sample ùë•ùë•ùëñ in the observed time series signals, the corresponding target value ùë¶ùëñ can be located, and thus, the sample can be labeled in the AL procedure. That is to say, for the TSF tasks, we do not assign discrete class label to each sample like the pattern classification tasks, but give the ground-truth value of the sample.

In this section, we first elaborate the cost-sensitive SVR method of imbalanced TSF, and then design a guideline for the setting of cost parameter. Afterward, a detailed procedure of AL existing in our method is expounded.

SVR with cost-sensitive method
Considering the imbalance of time series, it is difficult to guarantee the fairness of sample selection in the process of AL. To address this issue, we propose SVR incorporated with the cost-sensitive scheme for TSF. Thus, the weighted SVR [33, 34] is adopted to balance samples by using cost matrices as weights of particular samples. The constrained optimization problem is devised as below:

minùë§ùë§,ùëè,ùúâùëñ,ùúâÀÜùëñ{12‚Äñùë§ùë§‚Äñ2+ùê∂‚àëùëñ=1ùëöùë§‚éØ‚éØ‚éØ‚éØ‚éØùëñ(ùúâùëñ+ùúâÀÜùëñ)}
ùë†ùë¢ùëèùëóùëíùëêùë°ùë°ùëú‚éß‚é©‚é®‚é™‚é™ùë¶ùëñ‚àí(ùë§ùë§‚ãÖùúô(ùë•ùë•ùëñ)+ùëè)‚â§ùúÄ+ùúâùëñ(ùë§ùë§‚ãÖùúô(ùë•ùë•ùëñ)+ùëè)‚àíùë¶ùëñ‚â§ùúÄ+ùúâÃÇ ùëñùúâùëñ,ùúâÃÇ ùëñ‚â•0ùëñ=1,2,‚ãØ,ùëö
(9)
where ùë§‚éØ‚éØ‚éØ‚éØ‚éØùëñ means the mispredicted cost parameter of ùë•ùë•ùëñ. Then, the corresponding dual problem can be rewritten as:

maxùëéùëé,ùëéùëéÀÜ,{‚àëùëñ=1ùëöùë¶ùëñ(ùëéÀÜùëñ‚àíùëéùëñ)‚àíùúÄ(ùëéÀÜùëñ+ùëéùëñ)‚àí12‚àëùëñ=1ùëö‚àëùëó=1ùëö(ùëéÀÜùëñ‚àíùëéùëñ)(ùëéÀÜùëó‚àíùëéùëó)ùúÖ(ùë•ùë•ùëñ,ùë•ùë•ùëó)}
ùë†ùë¢ùëèùëóùëíùëêùë°ùë°ùëú‚éß‚é©‚é®‚é™‚é™‚é™‚é™‚àëùëñ=1ùëö(ùëéÃÇ ùëñ‚àíùëéùëñ)=00‚â§ùëéùëñ‚â§ùë§‚éØ‚éØ‚éØ‚éØ‚éØùëñùê∂0‚â§ùëéÃÇ ùëñ‚â§ùë§‚éØ‚éØ‚éØ‚éØ‚éØùëñùê∂ùëñ=1,2,‚ãØ,ùëö
(10)
Hence, the final solution of the weighted SVR can be acquired using Eqs. (5) and (6).

Guideline for cost parameter
With regard to the setting of the cost parameter ùë§‚éØ‚éØ‚éØ‚éØ‚éØùëñ, a novel weight update rule is designed as a guideline. Firstly, we resort to an automatic technique proposed in [35] for modeling a preference function, which maps the domain of continuous target variable into a [0,1] scale of preference as below:

ùúë(ùëå):‚Ñú‚Üí[0,1]
(11)
where 0 and 1 express minimum and maximum preference, respectively. Specifically, box plot statistic does not need to assume any distribution, but provides a set of crucial statistics of continuous target variable ùëå including the median ùëåÃÉ , the 1st and the 3rd quartiles (ùëÑ1 and ùëÑ3) and two adjacent values (ùëéùëëùëóùêø=ùëÑ1‚àí1.5ùêºùëÑùëÖ and ùëéùëëùëóùêª=ùëÑ3+1.5ùêºùëÑùëÖ), where ùêºùëÑùëÖ=ùëÑ3‚àíùëÑ1. We set all target variable values higher than ùëéùëëùëóùêª or lower than ùëéùëëùëóùêø as extreme values. Thus, two adjacent values are regarded as thresholds for the extremes, and the median is regarded as a centrality value for irrelevance, corresponding to the maximal preference and the minimum preference, respectively. Then, based on these statistics and their corresponding preference scores, a piecewise cubic Hermite interpolation polynomials algorithm [36] is used to derive the preference function ùúë(ùëå) by interpolating a set of preference values. Figure 2 shows a graphical interpretation involved in the derivation of the preference function.

Fig. 2
figure 2
A preference function using box plot statistic

Full size image
After that, a preference threshold ùë°ùëÉ is necessary to set the minimum preference score for some target variable values that are considered valuable. The threshold is a large value between 0 and 1, which is difficult to determine by quantile. Given ùë°ùëÉ, we can get a formal definition of the set of rare and preferred cases ùêøùëÖ, and the set of common and uninteresting cases ùêøùê∂, as follows:

ùêøùëÖ={(ùë•ùë•ùëñùëñ,ùë¶ùëñ)‚ààùêø:ùúë(ùë¶ùëñ)‚â•ùë°ùëÉ}
(12)
ùêøùê∂={(ùë•ùë•ùëñùëñ,ùë¶ùëñ)‚ààùêø:ùúë(ùë¶ùëñ)<ùë°ùëÉ}
(13)
Some values with their preference higher than ùë°ùëÉ are extreme high or low, which are more meaningful to the user, and should have higher prediction cost and greater penalty weight.

Hence, after the sample is inserted into the training set, the cost parameter can be updated as:

ùë§‚éØ‚éØ‚éØ‚éØ‚éØùëñ=‚éß‚é©‚é®‚é™‚é™|ùêøùê∂||ùêøùê∂|+|ùêøùëÖ|ùëñùëìùúë(ùë¶ùëñ)‚â•ùë°ùëÉ|ùêøùëÖ||ùêøùê∂|+|ùêøùëÖ|ùëñùëìùúë(ùë¶ùëñ)<ùë°ùëÉ
(14)
where |‚àô| counts the number of samples in the set. Regardless of the absolute number of samples in each category, the cost parameter depends on the ratio between the number of samples belonging to two different categories: the uninteresting cases and preference cases. When the newly added samples increase gradually, the weight is relatively stable but may fluctuate slightly.

Two-stage sample selection procedure
AL process based on clustering algorithm is divided into two stages, which aims to select some high-quality samples in virtue of SVR architecture. The main motivations of choosing SVR are its good generalization capability and relatively limited computational load in the training phase. More importantly, SVR can assist in identifying the relevant samples in the AL procedure.

Considering the ùúÄ-insensitive nature of SVR, the training set ùêø can be divided into the set ùêøùëÜùëâ of support vectors (SVs) and the set ùêøùëõùëÜùëâ of non-support vectors (non-SVs). SVs outside or on the boundary of the ùúÄ-tube possess informativeness. Because the target value of the sample in ùëà is unknown, a clustering-based process using kernel ùëò-means algorithm [37] is proposed to discern its position relative to the ùúÄ-tube without using its target value.

figure a
Kernel ùëò-means algorithm divides samples in the mapped higher-dimensional feature space instead of the original input space. To find clusters ùëê1,ùëê2,‚ãØ,ùëêùëò, the kernel ùëò-means criterion can be expressed as a minimization of the Euclidean distance between the instance and the centroid of each cluster in the higher-dimensional feature space:

ùê∑({ùëêùë°}ùëòùë°=1)=‚àëùëòùë°=1‚àë‚Ñéùëñ=1ùõø(ùë•ùë•ùëñ,ùëêùë°)‚Äñùúô(ùë•ùë•ùëñ)‚àíùëöùëöùë°‚Äñ2
(15)
where ‚Äñ‚àô‚Äñ denotes the L2 norm, ùúô(ùë•ùë•ùëñ) denotes the transformation of ùë•ùë•ùëñ in the higher-dimensional feature space, ùëöùëöùë° represents the centroid of the cluster ùëêùë°, and the indicator function ùõø(ùë•ùë•ùëñ,ùëêùë°) is used to record whether the sample ùë•ùë•ùëñ is in the cluster ùëêùë°. That is to say, if ùë•ùë•ùëñ‚ààùëêùë°, its value is 1, otherwise it is 0. We apply kernel ùëò-means algorithm to time series datasets for two reasons: (1) compared with other traditional clustering algorithms, it can capture complex nonlinear structure of time series, and distinguish the clusters more accurately by mapping instances to a higher-dimensional space; (2) it computes in the kernel space where the SVR ùúÄ-insensitive tube is represented.

Detailed pseudo-code of the clustering-based process is described in Algorithm 1. Firstly, each sample is randomly assigned to a certain cluster, and the indicator function ùõø(ùë•ùë•ùëñ,ùëêùë°) is used to record the results. Then, to minimize Eq. (15), the distance between the training sample ùë•ùë•ùëñ and the centroid ùëöùëöùë° of a cluster can be expanded as follows:

‚Äñùúô(ùë•ùë•ùëñ)‚àíùëöùëöùë°‚Äñ2=ùêæùëñùëñ‚àí2|ùëêùë°|‚àë‚Ñéùëó=1ùõø(ùë•ùë•ùëó,ùëêùë°)ùêæùëñùëó+1|ùëêùë°|2‚àë‚Ñéùëñ=1‚àë‚Ñéùëó=1ùõø(ùë•ùë•ùëñ,ùëêùë°)ùõø(ùë•ùë•ùëó,ùëêùë°)ùêæùëñùëó
(16)
ùêæùëñùëó=ùúô(ùë•ùë•ùëñ)ùúô(ùë•ùë•ùëó)
(17)
ùëöùëöùë°=1|ùëêùë°|‚àë‚Ñéùëó=1ùõø(ùë•ùë•ùëó,ùëêùë°)ùúô(ùë•ùë•ùëó)
(18)
where ùêæùëñùëó represents kernel function, and |ùëêùë°| denotes the number of samples in cluster ùëêùë°. Eq. (16) realizes the distance calculation in the higher-dimensional space, which only uses kernel computation ùêæùëñùëó and does not require explicit expression ùúô(ùë•ùë•ùëñ). Based on the distance calculation results, the clusters can be updated by reassigning the sample ùë•ùë•ùëñ to the nearest cluster. Finally, the above process is repeated until convergence. Therefore, the kernel ùëò-means clustering algorithm is applied to all instances in ùêøùëõùëÜùëâ and ùëà, and a set of ùëò clusters, denoted as Œ®={ùëê1,ùëê2,‚ãØ,ùëêùëò}, is obtained, where ùëêùë° is the ùë°-th cluster. In this way, the position of the samples in ùëà with respect to the ùúÄ-tube is captured. Next, according to the clustering results, two consecutive stages are used to select the most valuable samples.

The first stage is dedicated to selecting the most informative instances. Figure 3 illustrates this procedure simply and clearly. To this end, the clusters with non-SVs are removed, while those clusters without any non-SVs are reserved. Then, a set of s clusters is generated as follows:

Œ®inf={ùëê1,ùëê2,‚ãØ,ùëêùë†}=Œ®‚àñùëêùë°
ùë§‚Ñéùëíùëüùëíùëêùë°‚à©ùêøùëõùëÜùëâ‚â†‚àÖ,ùë°=1,2,‚ãØ,ùëò
(19)
Fig. 3
figure 3
Visualization of instances in one-dimensional feature space. a Distribution of the unlabeled instances (yellow stars), the non-SVs (gray circles) and the SVs (white circles). b Clustering result of unlabeled instances and non-SVs shows that different colors represent different clusters. c Reserved clusters after removing the clusters with non-SVs

Full size image
Obviously, ùë† must be less than or equal to ùëò. Finally, all samples in the set Œ®ùëñùëõùëì are considered as informative patterns, which can be combined into a set ùëàùëñùëõùëì defined as:

ùëàùëñùëõùëì={ùë•ùë•ùëñ|ùë•ùë•ùëñ‚ààùëêùë°and‚àÄùëêùë°‚äÇŒ®ùëñùëõùëì}
(20)
The second stage is committed to choosing ùë£ instances with representativeness and diversity from ùëàùëñùëõùëì, according to two density measures. Figure 4 displays an example to better illustrate this stage. The density ùúåùëêùë° of each cluster ùëêùë°‚äÇŒ®ùëñùëõùëì is firstly calculated as:

ùúåùëêùë°=|ùê∂ùë°|ùëëùëöùëéùë•ùëêùë°ùë°=1,2,‚ãØ,ùë†
(21)
ùëëùëöùëéùë•ùëêùë°=ùëöùëéùë•ùë•ùë•ùëñ,ùë•ùë•ùëó‚ààùëêùë°{‚Äñùë•ùë•ùëñ‚àíùë•ùë•ùëó‚Äñ2}
(22)
where |ùê∂ùë°| represents the number of instances in cluster ùëêùë°, and ùëëùëöùëéùë•ùëêùë° is a measure of the extension of ùëêùë° in the feature space. In general, a compact cluster with a large number of instances may have a relatively high density.

Fig. 4
figure 4
Visualization of instances in one-dimensional feature space. a Reserved ùë£=2 clusters with the highest density. b Chosen instances (red stars) with the highest density from each cluster

Full size image
After the density ùúåùëêùë°(ùë°=1,2,‚ãØ,ùë†) of each cluster in the set Œ®ùëñùëõùëì is calculated, the top ùë£‚â§ùë† clusters with the highest density are selected for further consideration, while the other clusters are neglected. Then, the instance density associated with each instance position in the feature space is evaluated. Specifically, the density of ùë•ùë•ùëñ in ùëêùë° is measured by the average distance from all other instances within the same cluster:

ùúåùëêùë°ùë•ùë•ùëñ=averageùë•ùë•ùëó‚ààùëêùë°‚àßùë•ùë•ùëó‚â†ùë•ùë•ùëñ{‚Äñùë•ùë•ùëñ‚àíùë•ùë•ùëó‚Äñ2}
(23)
Therefore, a total of ùë£ instances with the highest density in each cluster can be extracted to form set ùëã‚äÇùëàùëñùëõùëì. Because of this choice, the samples that best represent the basic sample distribution in each cluster are identified. Meanwhile, since only one sample is chosen from each cluster, the selected samples distributed in different clusters are implicitly sparse in the feature space, and they are diverse to each other. Finally, these ùë£ instances are eliminated from the unlabeled set ùëà and submitted to the training set as ùêø=ùêø‚à™ùëã. According to the rule of thumb, the number of clusters ùëò is preset as the square root of half of the total number of instances [38].

figure b
The detailed process of the iteration of MAW-SVR is described in Algorithm 2. Firstly, for a given training set, the preference function is fitted based on box plot statistics, and then, the cost parameters of each instance are calculated. Secondly, based on the training instances and their corresponding cost parameters, two types of instances, including Support vectors (SVs) and non-support vectors (non-SVs), are obtained by training weighted SVR. Thirdly, a clustering algorithm is applied to non-SVs and all unlabeled instances. The clusters with non-SVs inside are neglected, and all the unlabeled instances of the remaining clusters are regarded as the most informative instances. Finally, the densities of clusters are estimated and the ùë£ clusters with the highest density are chosen. Then, for each selected cluster, only one sample with the highest density is chosen to be included in the training set.

In conclusion, the AL procedure iterates until the maximum number of the training instances is met. When the procedure finished, the selected instances are trained with the weighted SVR to obtain the final prediction model. It needs to mention that, any clustering technique can be used in the two consecutive clustering-based stages process.

Experiment
Experiment datasets
In this paper, a total of nine time series datasets are used. Four financial time series datasets including Johnson Outdoors Inc. (JOUT), Dow Jones industrial average (DJI), GlaxoSmithKline plc. (GSK) and SP Plus Corporation (SP) are from Yahoo Finance [39], where the daily close price is regarded as the forecast target in our experiment. The remaining four datasets come from Time series data library (TSDL) [40] including daily maximum temperatures in Australia, Radioactivity in the ground at two hourly intervals, Internet traffic data in bits from ISP at five minute intervals, mean daily Saugeen river flows and monthly temperature in England. The detail information about these datasets is listed in Table 1.

Table 1 The details of datasets used in this paper
Full size table
In addition, owing to the different scopes of dataset attribute, each value ùë•ùëñ in the whole dataset is normalized by min‚Äìmax normalization method and mapped into the interval [0,1] in the preprocessing stage to prevent data with larger values from overwhelming the smaller ones:

ùë•ùëñ‚Ä≤=ùë•ùëñ‚àímin(ùë•)max(ùë•)‚àímin(ùë•)
(24)
where min(ùë•) and max(ùë•) denote the minimum value and maximum value of the original time series, respectively. The min‚Äìmax normalization is a kind of linear feature transformation which can scale the numerical range of time series data without changing the data distribution. Thus, the imbalance of time series cannot be eliminated after normalization. In addition, since the change trend of the mean and variance of the preprocessed time series is consistent with that of the original series, the non-stationarity of time series has also been maintained.

Experiment setup
In order to prove the effectiveness of our algorithm, we compare it with several AL algorithms including GS [16], QBC [18], EMCM [19] and ID [20]. Besides, to present the role of individual parts of MAW-SVR, random sample weighted (RSW)-SVR and multiple criteria active (MA)-SVR are also used for comparison. RSW-SVR adopts the cost-sensitive SVR, in which the new incremental samples are extracted randomly in each iteration. MA-SVR jointly measures three criteria to extract the samples without considering the cost-sensitive scheme. The ‚ÄúAll training instances‚Äù is regarded as a baseline algorithm, which adopts all the training samples without cost-sensitive and AL procedure. All algorithms are implemented in Python 3.8, and experiments are carried out with an Intel Core i5 4 cores CPU (main frequency: 2.30 GHz for each core) and 8-GB RAM.

To draw the learning curve of the AL process, the performance of each algorithm at a specific time point is evaluated, and Root mean square error (RMSE) [41] and coefficient of determination (R2) [42] are applied as the estimation measurements of prediction errors in this paper. The RMSE and R2 are calculated as follows:

RMSE=1ùëÅ‚àëùëñ=1ùëÅ(ùë¶ùëñ‚àíùë¶ÀÜùëñ)2‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚Äæ‚ÄæÓÄÅ‚é∑ÓÄÄÓÄÄ
(25)
R2=1‚àí‚àëùëÅùëñ=1(ùë¶ùëñ‚àíùë¶ÀÜùëñ)2‚àëùëÅùëñ=1(ùë¶ùëñ‚àíùë¶‚éØ‚éØ‚éØùëñ)2
(26)
where ùëÅ denotes the size of test dataset, ùë¶ùëñ and ùë¶ÀÜùëñ are true and predicted values, respectively, and ùë¶‚éØ‚éØ‚éØùëñ=1ùëÅ‚àëùëÅùëñ=1ùë¶ùëñ. Generally, for the RMSE, a smaller value represents a better performance, but a larger R2 means a better performance. In the experiment, we adopt an SVR with radial basis function (RBF) kernel or linear kernel. For SVR, the values of the parameter ùúÄ, of the penalty factor ùê∂, of the spread ùõæ of the RBF are determined by trial-and-error, where ùúÄ is tested from {0.001,0.05,0.01,0.1}, ùê∂ is chosen from {1,10,100,1000}, and ùõæ is selected from {0.001,0.05,0.01,0.1}. The minimum preference threshold ùë°ùëÉ in each dataset is chosen from {0.6,0.7,0.8} to obtain the set |ùêøùê∂| and |ùêøùëÖ| in all datasets, which usually leads to a small percentage of samples being considered important. As shown Table 1, we can find that the percentage of rare cases ranges from 17 to 41%. Considering the size of the datasets, the acceptable time window size is set to 10 by trial-and-error. We conduct the experiments by adding ùë£=10 instances at each iteration.

In order to obtain statistically meaningful outcomes, individual experiment is repeated for ten times over each dataset according to ten initial randomly selected training sets. Initial training set is composed of 10% of the samples in the pool. Each experiment dataset is divided into three parts in chronological order: the first 50% for the training set, the next 25% for the validation set and the last 25% for the test set. The validation set is used to find the optimal parameters of the model, and the test set is used to verify the performance of the model. Therefore, in the following experimental results, we report the performance results on the test set to verify the effectiveness of our algorithm.

Experiment results
Comparison of prediction performance
The learning curves of RMSE and R2 for different AL algorithms on the nine datasets with varied numbers of queries are shown in Figs. 5 and Fig. 6, accordingly. These RMSE and R2 values are averaged over ten runs. The X-axis represents the number of training instances queried by the corresponding algorithm in each iteration. In general, when the number of training instances increases, all AL methods achieve better performance (smaller RMSE and larger R2), which is intuitive, since more training instances usually produce a more reliable predictor. We observe that, as the number of instances increases, MAW-SVR convergences and gets the best performance in terms of RMSE and R2 among all AL algorithms. Moreover, our algorithm achieves better performance than the model trained by all instances without exhausting all instances, which shows the practicability of the AL in TSF.

Fig. 5
figure 5
Average (on ten runs) RMSE of different algorithms on the nine datasets

Full size image
Fig. 6
figure 6
Average (on ten runs) R2 of different algorithms on the nine datasets

Full size image
From the learning curves on DJI, SP, daily temperature, Internet traffic and monthly temperature datasets, it is not hard find that MAW-SVR provides the lowest RMSE and highest R2 in most iterations. For instance, in Figs. 5e and  6e, our algorithm achieves an RMSE of 0.099 and an R2 of 0.686 with only 160 queried instances, which is equivalent to the performance of the model with all training instances. However, other algorithms yield higher RMSE and lower R2 with the same number of queried instances. Furthermore, our algorithm achieves the same RMSE and R2 as the other algorithms with a smaller number of training instances, which satisfies the goal of AL to acquire a precise model with a minimum number of training instances. This is considered as a very important advantage. For example, in Figs. 5c, 6c, it is easy to discover that MAW-SVR provides an RMSE of 0.0085 and an R2 of 0.9972 with around 145 queried instances, whereas MA-SVR reaches the similar RMSE with about 180 queried instances and the other algorithms require even more instances.

We can see from JOUT, GSK, radioactivity and river flows datasets that MAW-SVR is not performing well at the beginning of the learning stage. However, with the number of queries increases, MAW-SVR catches up with the other algorithms and achieves decent performance. We attribute the phenomenon to the fact that when there are only a few training instances, the preference values obtained by the distribution of the target variable are inaccurate, so the cost-sensitive scheme cannot bring better performance. Accordingly, MA-SVR, without the cost-sensitive scheme, performs well than MAW-SVR at the beginning of AL procedure with small size training set.

QBC and EMCM are two commonly used AL approaches, which only consider the informativeness of instances. We observe that MA-SVR considering multiple criteria significantly outperforms the two approaches on five of the nine datasets. This phenomenon can be attributed to the fact that single query criterion used for sample selection in QBC and EMCM is relatively simple, while our multiple criteria selection strategy is more effective. GS considers the diversity of instances based on their geometric characteristic. It performs well on some datasets, but works poorly on the others. This may be because the sample selection in GS is independent of the prediction model, but depends on the distance measurement between samples in the feature space. Therefore, GS is not robust to various types of data distributions. ID is an AL approach that jointly assesses the informativeness and representativeness of instances. However, compared with QBC, ID does not show great advantages, which is basically equivalent to the performance of QBC. That is to say, for time series datasets, ID uses the cosine similarity measure to estimate the density of samples, which does not bring significant performance improvement.

Compared with RSW-SVR, MAW-SVR has obviously advantages, which indicates that the multiple criteria selection strategy improves performance faster than random selection. In other words, the result demonstrates the effectiveness of the two consecutive clustering-based stages procedure. When comparing MA-SVR with MAW-SVR, we can find an interesting phenomenon that, in the initial stage of AL, the performance of MA-SVR is similar to that of MAW-SVR, and even better on some datasets, but it is gradually surpassed. We attribute the phenomenon to the proposed guideline for cost parameter. Due to the small number of training set in the early stage of learning process, the penalty weight of each sample obtained from the distribution of the target value is not accurate enough. Fortunately, with the increase in iterations, sufficient samples are enough used to derive the preference function, which can allocate appropriate cost parameter to each sample. Finally, MAW-SVR does not need to exhaust all instances in the pool and achieves an optimal performance. Hence, the cost-sensitive scheme is an effective strategy for the imbalanced TSF, and MAW-SVR is still a better choice than MA-SVR.

For visualization, we also present the predictive values of MAW-SVR on the test set of nine benchmark time series datasets, in which the predictor is trained by using 80% of unlabeled data selected by our AL algorithmic framework. In Fig. 7, the X-axis represents the time unit and the Y-axis indicates the corresponding sequence value. According to the analysis of Fig. 7, it can be seen that the overall variation tendencies of the proposed MAW-SVR algorithm roughly resemble with that of the raw data. In the daily temperature, radioactivity and monthly temperature datasets, some sequence values simulated by MAW-SVR do not show in good agreement with the actual data. However, in most datasets, MAW-SVR provides responses closely fitting the actual data.

Fig. 7
figure 7
Ground truth values and prediction values of the proposed algorithm visualized

Full size image
Statistical analysis
In order to intuitively evaluate the quality of the whole AL process, the values of RMSE and R2 repeated ten times in the form of mean‚Äâ¬±‚Äâstd (standard deviation), with 10%, 20%, 30%, 40%, 50% and 80% of unlabeled data used as the training set, are shown in Tables 2, 3, respectively. For space considerations, each mean and std of RMSE and R2 values shows only four decimal places.

Table 2 Comparison on RMSE (mean‚Äâ¬±‚Äâstd) with different number of instances on the nine datasets
Full size table
Table 3 Comparison on R2 (mean‚Äâ¬±‚Äâstd) with different number of instances on the nine datasets
Full size table
Meanwhile, to ascertain whether the differences between different pairs of algorithms are significant, we also perform ùë° test on the RMSE and R2 values. For each case, the best result and its comparable performances based on paired ùë° test at 95% significance level displayed in boldface are listed in Tables 2, 3, which indicates that the items in boldface significantly improve the predictive performance of the other items in normal font at 5% significance level. For example, on river flows dataset, when 80% of samples in the pool are used as the training set, MAW-SVR obtains R2 values with mean of 0.9116 and standard deviation of 0.0000 on 10 trials, which is more stable with the lowest standard deviation and achieves the best performance compared with the other six algorithms at 5% significance level.

By analyzing Tables 2, 3, we can see that MAW-SVR achieves the best performance in terms of RMSE and R2 in the whole AL process on the DJI, SP, daily temperature and Internet traffic datasets. Besides, MA-SVR also gets the best performance in most AL process, which reflects the effectiveness of our sample selection strategy. For JOUT, GSK, radioactivity, river flows and monthly temperature datasets, MAW-SVR works poorly at the initial stage. However, as the number of queries increases, MAW-SVR achieves decent performance with fast learning speed, and does not need to exhaust all samples in the pool. This again suggests that the cost-sensitive scheme does not work when the training sample set is small.

Furthermore, to further count the results of MAW-SVR versus the other algorithms with varied number of instances based on the ùë° test. Table 4 and Table 5 record the win/tie/loss statistics of MAW-SVR compared with the other algorithms in terms of RMSE and R2, respectively. Through analyzing Table 4, for 157 out of the 324 ùë° tests (48%), MAW-SVR achieves significant improvements over comparative approaches at 5% significance level on RMSE. In addition, we observe that our result has a positive trend between 10 and 50%. That is to say, with the increase in the number of samples, the frequency of win statistics gradually increases and the frequency of loss statistics gradually decreases. For example, in the 10% column, MAW-SVR achieves the best performance at 5% significance level for 10 out of the 54 ùë° tests (19%), but for 7 out of the 54 ùë° tests (13%), the comparative algorithms are significantly better than MAW-SVR we proposed. When half of the samples in the pool are used as training set, MAW-SVR is significantly superior to the other algorithms for 38 out of the 54 ùë° tests (70%), and has no loss statistics.

Table 4 Win/tie/loss statistics of MAW-SVR compared with the other algorithms on RMSE with different number of instances
Full size table
Table 5 Win/tie/loss statistics of MAW-SVR compared with the other algorithms on R2 with different number of instances
Full size table
Then, our result stagnates or decreases from the 50% column. By analyzing the causation, we find that RSW-SVR shows remarkable performance after the 50% column, and it catches up with the performance of MAW-SVR on more datasets. This makes the frequency of win statistics decreases and the frequency of tie statistics increases in our method. We attribute the phenomenon to the fact that, when the number of training samples is enough, the cost-sensitive scheme plays a crucial role in imbalanced time series data, but the effect of sample selection is not obvious. Similar conclusion for R2 can also be obtained on the analysis to Table 5.

Finally, to further confirm whether the proposed MAW-SVR algorithm is significantly superior to the other AL algorithms, it is necessary to perform t tests on more time series datasets. Therefore, on a total of thirty time series datasets, including the previous nine datasets, t test at the 5% significance level is applied to the RMSE results of MAW-SVR and the remaining six algorithms. These new datasets are still from Yahoo Finance [39] and Time series data library (TSDL) [40], respectively. The t test results (i.e., p values) with 80% of unlabeled data used as the training set are recorded in Table 6. The items displayed in boldface manifest that hypothesis H0 is rejected, i.e., the newly proposed MAW-SVR is significantly superior to the other algorithms at 5% significance level in terms of RMSE. As shown in Table 6, for 128 out of the 180 t test (71.1%), the proposed MAW-SVR algorithm achieves significant improvements over comparative algorithms at 5% significance level. At the same time, when applied to the DJI, river flows, silver, FVD1, FVD2, nv515 and shendevbank time series datasets, MAW-SVR is far better than all other compared algorithms. In conclusion, according to the above results, we observe that for most cases, MAW-SVR outperforms the other algorithms significantly.

Table 6 t test results between MAW-SVR and other comparative algorithms on the thirty time series datasets
Full size table
Comparison of running time
As shown in Table 7, the average computational time taken from a single iteration of different algorithms is measured in seconds. We can find that on nine benchmark time series datasets, RSW-SVR usually takes the least computation time benefiting from its random selection strategy. In the single query selection methods, GS is worse than QBC and EMCM in terms of efficiency due to the calculation of distance. Comparing ID with MA-SVR, although both methods are multiple criteria AL methods, ID needs much more time-consuming on most datasets due to training multiple predictors. Compared to MA-SVR, the proposed MAW-SVR has similar running time, which shows that the cost-sensitive scheme does not take too much time. Therefore, it can be verified that our method has considerable performances for TSF and still has acceptable computational complexity.

Table 7 Average running time (seconds)
Full size table
Conclusions
In this paper, we study the application of AL in TSF. Our goal is to select high-quality data samples in TSF issue. However, due to the nonlinear and non-stationary characteristics of time series data [1], TSF is a challenging task. A common problem observed in time series is the imbalanced distribution of the target variable, some of which are very valuable to the user but poorly represented. In this context, an AL algorithmic framework with the goal of adapting it to imbalanced time series data, namely MAW-SVR, is proposed. Our approach adopts the idea of cost-sensitive learning, which makes the prediction cost different. Besides, according to the SVR properties, a two-stage sample selection procedure by the assessment of three criteria, i.e., informativeness, representativeness and diversity, is extended to construct our own AL algorithmic framework. The comparisons between the proposed MAW-SVR and the other six AL algorithms on thirty time series datasets verify the effectiveness of MAW-SVR.

In summary, MAW-SVR provides the first attempt of the cost-sensitive scheme for imbalanced TSF. Different from the same cost of all the mispredicted instances, predictions made for time series data should have differentiated costs. Besides, in the AL process, SVR is conducted for effectively extracting the most valuable samples without having to provide more samples than necessary. However, since SVR considers data as independent and identically distributed, there is a limitation in our proposed MAW-SVR algorithm that, it ignores the temporal dependency structure in the AL procedure, which is an important characteristic of time series data and has to be preserved. This limitation in our AL algorithmic framework is the main focus of our future work. In the future research work, we will seriously consider how to design a more effective AL algorithm reasonably, so that it can take into account the temporal dependency structure of time series data, further improving MAW-SVR.

Keywords
Time series forecasting (TSF)
Data imbalance
Cost-sensitive learning (CSL)
Active learning (AL)