In recent years, studies on chaotic neural networks have been increased to construct a robust and flexible intelligent network resembling the human brain. To increase the chaotic performance and to reduce the time-complexity of conventional chaotic neural networks, this paper presents an innovative chaotic architecture called cascade chaotic neural network (CCNN). Cascade chaotic system is inspired by cascade structures in electronic circuits. Cascade structure is based on a combination of two or more one-dimensional chaotic maps. This combination provides a new chaotic map that has more complicated behavior than its grain maps. The fusion of this structure into the network neurons makes the CCNN more capable of confronting nonlinear problems. In the proposed model, cascade chaotic activation function (CCAF) is introduced and applied. Using the CCAF with inherent chaotic features such as increasing variability, ergodicity, maximum entropy, and free saturation zones can be promising to solve or reduce learning problems in conventional AFs without increasing complexity. The complexity does not increase because no parameter is added to the system in use. The required chaos for neural network is generated by the Li oscillator, and then when using the neural network, parameters are considered as constants. Chaotic behavior of the CCNN is investigated through bifurcation diagram. Also, prediction capability of the proposed model is verified through popular benchmark problems. Simulation and analysis demonstrate that in comparison with outstanding chaotic models, the CCNN provides more accurate and robust results in various conditions.

Introduction
In the past few years, neural networks have been achieved much attention in order to construct robust and flexible intelligent networks using in various areas, from nonlinear modeling and pattern recognition problem [1,2,3] to memory buffers and neural synapses [4]. This dynamic network can also be used in information processing [5], optimization [6, 7], and time series prediction [8, 9]. One of the most important limitations of conventional neural networks such as the standard MLPs (Multi-layer Perceptrons) is their tendency to become trapped in plateau and local minima due to dynamics of gradient descent used in the backpropagation algorithm [10]. More precisely and intuitively, a multilayer feedforward neural network, as one of the important classes of neural networks, is trained by a supervised algorithm. This supervised algorithm includes gradient calculations related to an activation function (AF) in the hidden layer and sometimes in the output layer of the network. The conventional AFs such as tan hyperbolic and sigmoid applied in the learning algorithms of the MLP suffer from the saturation problem. Derivative of the activation function takes a small value around this area. Therefore, the amount of weight update becomes negligible, and the saturated units prevent any significant change in the network weights connected to the units. This leads to increase in the number of training cycles and ultimately a slow convergence rate.

The problem of improving the weight update process has been followed by several researchers. These studies include various approaches, from improving the BP algorithm to presenting new activation functions and from using heuristic strategies to injecting chaos in the AF. In [11], a dynamically varying learning rate based on the error signal is presented. Similar approach was applied in [12] with introducing inertia terms for optimizing the weight values of network. Proposing various cost function and using higher-order derivatives are demonstrated in [10, 13]. Also, introducing popular activation functions such as RELU (Rectified Linear Unit) and Leaky Relu are included in this approach [14]. In [15], an adaptive algorithm called ABPA is introduced for long-term load forecasting. The deviation between trained and future values of datasets is regarded and embedded into new formulation of forecasting. In [16], an approach called magnified gradient function is presented to improve generalization capability and speed up the training process. This approach directly magnifies sigmoid function derivatives, resulting in faster weight upgrades. The above-mentioned algorithms have some merits, but each of them has some shortcomings. Dramatic divergence due to undesirable areas of weight space, large storage requirements, and complex and costly computations of each iteration for the BP modification algorithms are some shortcomings of the above approaches. The last limitation, offset their faster speed of convergence. Dead neurons and restriction of use in complex modeling, due to possessing linearity, are some other drawbacks of the previously presented approaches. On the other hand, none of the mentioned approaches are linked to biologically motivated phenomena. According to the fact that the general structure of a neural network is inspired by the human brain, it is probable that its limitation can also be resolved by integrating the functional properties of the human brain into the structure of NN and its relations.

The researches on neuroscience and electrophysiological experiments have shown that neuron activity is severely affected by chaos dynamic [17, 18]. Neuroscience studies have revealed that the human brain can process an immense amount of information, forthwith, due to the existence of chaotic dynamics [18]. Controlled chaos in the human brain is more than a random event that results from the brain complexity involving thousands of connections. Instead, it could be called the key feature that distinguishes the brain from an AI machine. Neuron chaotic behavior is also observed in microscopic form. For instance, the axon membrane of the squid in response to sinusoidal stimulation has revealed periodic or chaotic behavior proportional to frequency and power stimulation [19].

There are limited studies around the relationship between a supervised algorithm and a biologically inspired approach. In [20], a diagonal recurrent network is considered with different chaos neurons. The Logistic map, the Chebyshev map, and the Sine map are applied for constructing the network and the achieved network is used for image compression. However, the performance of entire network is dependent to type of the chaotic map. Also, another function-dependent approach using Chebyshev map and backpropagation algorithm is used for electrical load forecasting in [12]. In [21], to increase the convergence rate of the MLP network, an algorithm called chaotically adaptive weight and temperature (CAWT) coefficient was proposed. CAWT adds chaos during the learning process of the network. In this algorithm, perturbations of search space are done by chaos, but no supervised pressure exists. Therefore, the algorithm requires connecting to supervised pressure, i.e., error function for updating weights. Hence, the main aim of presented paper is to propose a biologically inspired network to improve the training and generalization capability of the neural network. Injecting chaos into the network is followed by introducing a chaotic activation function, and this phenomenon is reinforced through a cascade structure. In short, the main problems of primary and recent approaches of AFs are as follows:

Low convergence rate due to saturation problem and existence of local minima

Non-symmetric output of some recently presented functions

Problems in complex modeling due to containing linearity

Problem with using batch normalization

High number of system parameters

Not linking to biologically motivated phenomena

A chaotic activation function due to chaos properties such as ergodicity, variability, and entropy could overcome the mentioned drawbacks of the conventional AFs. First of all, chaotic AF is free from saturation. In fact, due to the fractal structure of the chaotic system, a vast amount of differences are generated and exhibited within a range. Each non-uniform cycle of a chaotic function generates a distinct value comparing to its previous cycle in an iterative procedure. So, flat regions are disappeared near the boundaries 0 and 1. The next motivation for using chaotic AFs is the need for an AF gradient computation for supervised algorithms like BP. Most conventional activation functions exhibit saturation behavior to some extent, whereas saturation region does not exist in the gradient of a chaotic activation function. The third and last reason for introducing the presented model is that fusion of chaotic activation functions generates a more robust and flexible network than that of a network with a single chaotic AF. Indeed, the highly nonlinear behavior of the proposed model, resulting from the cascade chaotic activation functions of the neurons, makes it analogous to human brain behavior. This is the main contribution of the presented paper.

The proposed CCNN network applies a cascade chaotic activation function in the hidden layer. The saturation-free properties both in chaotic AF and its gradient, along with greater variability of chaotic dynamic that can be shown in bifurcation behavior, promise prominent features for modeling nonlinearities. All of these features are inspired by the natural neuron behavior. Therefore, the efforts for solving the problems of conventional AFs and neural networks are based on the biological patterns of the brain. Moreover, using the remarkable features of CCS and embedding it to a conventional neural network, the CCNN is generated. This combination is led to providing a more complex behavior. The fusion of cascade chaotic structure into the network neurons makes the CCNN more powerful in nonlinear problems. This capability increases because of its high nonlinearity weights. The weights of nonlinearity in the proposed architecture are analogous to the weights of nonlinearity in the modeled system. This feature is suitable when complex systems such as weather forecasting systems, traffic time series, and encryption systems are modeled. Another noteworthy point is that unlike the models with hyperparameters, presented in the previous researches, there will be no imposed complexity in the proposed system, due to division of the whole procedure into chaos generation and chaos application. This point is discussed completely in the paper. Finally, as reported by numerous researches [22,23,24], chaotic activation function, unlike some recent approaches, is well coordinated with batch normalization techniques.

In short, the notable contributions of this paper are as follows:

1.
Introducing the CCNN model as a general framework of function-independent chaotic neural networks.

2.
Applying the Lee oscillator and excitatory and inhibitory neurons as a chaos generator in NN

3.
Applying the saturation-free chaotic functions in the neurons activation functions.

4.
Reinforcement of the chaotic behavior through the cascade structure of the proposed model.

5.
Analyzing the chaotic performance of the proposed model through the bifurcation diagram

The rest of the paper is organized as follows. Section 2 gives a brief review of the neural network, its inspiration, and the development of CNN and its activation function. Various approaches to chaotic neural networks are studied in this section. Also, problems of NN learning algorithms and strength and weakness of each of these algorithms are investigated. Cascade chaotic system and its analysis based on the Lyapunov exponent are considered in Sect. 3. The proposed model is introduced in Sect. 4. A detailed description of the CCNN is presented in this section. Besides, the bifurcation behavior of the cascade and non-cascade structures (CCNN and CNN) is compared through the Lee oscillator. Section 5 applies the CCNN to the problem of chaotic time series prediction. CCNN is discussed in Sect. 6. Finally, Sect. 7 presents the conclusion and proposes challenges and notes about future works.

Chaotic neural network
An artificial neural network is a mathematical model inspired by the human brain, imitates biological neural processes. Various models are presented for the artificial neural network, where each concentrates on the corner of the brain’s feature [25,26,27]. Combining these models and creating artificial neural networks that their performance is more consistent with the brain capability can improve the processing power of the existing artificial neural network. The latest development in nonlinear dynamics, the emergence of chaos theory and complicated systems, and the advances in laboratory equipment have made humans discover new properties, especially chaos property in the brain function. Therefore, embedding this property into classical models of the neural network can be a way to improve their performance. This point is one of the primary goals of this paper.

One of the most common types of neural networks is feedforward neural networks. These networks have proven to be able to estimate any function with the desired accuracy [28, 29]. In fact, they are general approximators. Due to this fact, the feedforward neural network has been used extensively. In this network, information moves only in a forward direction, and there is no loop. Information starts from input neurons and passes through the hidden layer (if any) to the output neuron. In one feedforward neural network, weights and biases are determined using optimization techniques to minimize the error between the network response and the desired response. For instance, the classical method of backpropagation error [30] is gradient-based methods (first-order class), while the Levenberg–Marquardt [31] is in the class of second-order method.

Gradient computation of the supervised learning algorithms usually requires an activation function in the hidden or sometimes in the output layer of a neural network. Conventional activation functions such as sigmoid and hyperbolic tangent used in this type of neural network suffer from the saturation constraint problem around the intervals − 1 to 1 and 0 to 1. This problem causes the weight update process not to work well and ultimately leads to a reduction of convergence rate [15, 32] along with an increase in system output error compared to the desired output. In recent years, modifications of the gradient function for improving the performance of learning algorithms have attracted the attention of many scholars [12, 16]. Moreover, many advances have been made in error propagation techniques using various cost functions, activation functions, and novel strategies [33, 34].

One of the limitations of the error propagation process is the existence of flat regions due to the saturated behavior of the activation function, leading to a slow convergence rate. According to the critical role of activation function on improving learning algorithms in neural networks, one of the primary aims of this research is to progress the activation function performance considering various approaches.

In general, there are different approaches to improve the activation function performance depending on the type and purpose of the neural network operation [35,36,37]. In [36], a three-term backpropagation algorithm was proposed to overcome the major limitations of the backpropagation algorithm, such as speed convergence and the existence of local minimum. In this algorithm, a new term called proportional factor is added to the standard backpropagation algorithm. To improve the rate of convergence in the backpropagation algorithm, the arc-tangent activation function was presented in [38]. Similarly, in [39], the logarithmic activation function was used instead of the standard sigmoid function. This function, in some cases, has led to an increase in the speed of the training process. Nevertheless, these algorithms face the problem of getting into the local minimum. In [35], a new AF is applied with revised random weights and bias values for crop yield estimation. Moreover, variable number of hidden layer and variable number of nodes in each layer is considered in this study. Introducing and comparing the popular activation function ReLU and its variants such as leaky ReLU and ELU were done in [40]. All of these AFs reduce the vanishing gradient problem of the BP algorithm. However, each of them has its drawbacks. ReLU activation function evaluates gradient problem in the positive interval, and its calculation speed is suitable, but its output is not symmetrical. Moreover, dead neurons or inactivation of neurons in the BP process are among the critical limitations of this algorithm. Also, a bias shift problem resulting from units with nonzero mean activation of the previous layer is another problem of ReLU [41]. The leaky ReLU outperforms the ReLU since it replaces the negative part of the ReLU with a linear function. To solve the dying problem of ReLU, a small negative slope is added so the gradient will be a small nonzero value. Nevertheless, as it contains linearity, it has a problem with complex modeling, and it lags behind the conventional AFs in some cases [42]. Another variant of the ReLU is ELU (Exponential Linear Unit) which adds an exponential term to ReeLU AF instead of a constant slope. The ELU removes the bias shift problem in ReLU, which slows down the training phase of the system. Therefore, the convergence rate is increased. However, ELU and its variants did not report the improved performance using batch normalization [43]. All of the above algorithms utilize a single function as an activator. However, in the study [44], with the combination of several conventional activation functions, a new activation function was introduced, called the combined activation function. Consequently, the convergence rate is improved, although the high number of system parameters is considered as a negative point.

Unfortunately, the algorithms mentioned above have not been linked to bio-inspired phenomena. So far, a limited number of studies have been done on the interface between bio-inspired phenomena and supervised learning algorithms. One of these phenomena is chaos. Chaotic phenomena are present in a variety of systems and applications. Neuroscience is one of the most important fields that chaos exists and neurons, and in a general view, neural networks functions are affected by the chaos phenomena. Besides, some roles of chaos are investigated from the biological point of view. Basically, there are two chaotic optimization approaches. The first approach is to find the optimal total response using a chaotic dynamic neural network, which was first introduced by Aihara [6]. In the second approach, the optimal overall response is sought using chaotic optimization algorithms [45, 46], which mainly relies on chaotic properties such as having pseudo-randomness sequences, ergodicity, and irregularities. In this study, the second approach has been used, and it is attempted to achieve optimal overall responses in a short time with the help of chaotic properties in the activation function. In [47], with combination of deep learning and chaos theory, a recurrent neural network is obtained for load forecasting with different horizons. Concentrating on network structure and embedding various structures of NN, is the important feature of this method. In [21], a chaos-based algorithm called adaptive chaotic weights and temperatures were presented inside the feedforward network. In this model, to improve the convergence rate and generalization capability, chaos is continuously injected during the training process. This model does not have any monitored pressure. In other words, minimizing errors is restricted. Therefore, it requires a connection to the pressure being monitored. However, most of the researches has been concentrated on the sigmoid functions for improving the performance of the activation function in feedforward networks [48, 49]. Since a nonlinear function is usually used as an activation function, so the idea of using chaotic function as an activation function is interesting, and it has been done by various authors [21, 50, 51]. In [51], instead of using conventional nonlinear functions, a new activation function based on the combination of circular and logical functions was presented, and its performance was evaluated in the classification problems. The model yielded acceptable results in terms of convergence rate and generalization capability. The proposed model in this paper has been obtained by replacing the combination of chaotic functions with the cascade chaotic model that will be introduced in the next section.

Cascade chaotic system (CCS)
A dynamical system in mathematics and industrial problem solving is a system in which a state changes with time. In other words, a function describes how a point in geometrical space depends on time [52]. In the past half-century, dynamic systems have received much attention. In particular, chaotic maps, as traditional dynamical systems, are the basis of several studies [53]. Ergodicity and unpredictability are the properties of these maps. The first feature leads to a comprehensive search without considering duplicate points, and the latter generates dissimilar sequences using different initial values or parameters of the chaotic maps. Regarding these remarkable features, chaotic maps are effective tools in various domains such as cryptography [54, 55], and computer science [56]. According to the latest research in chaotic maps, there are two classes of chaotic maps: one-dimensional (1-D) and high-dimensional (HD) chaotic maps [56]. The former are mathematical functions that track the single variable progress upon a discrete phase in time. The latter simulates the progress of two or more than two variables. From the structure point of view, these maps are simple to construct. The chaotic properties of these maps are strong, and a variety of applications can be benefited from these maps. However, there are serious drawbacks in the 1-D chaotic maps, such as limitation of a chaotic span, few parameters, and easy predictability of function output. HD chaotic maps outperform the 1-D chaotic maps in security and application points of view. Their chaotic behavior is complex. However, this complexity leads to an increase in the computational cost of the system, and therefore, some application potentials of the HD chaotic maps, especially in the real-time domain, are restricted. To remove the restricted efficiency of 1-D chaotic map and high-computation cost of HD chaotic map, cascade chaotic system (CCS) proposed in [56]. In general, CCS proposes a 1-D structure that utilizes the features of 1-D and HD chaotic maps by connecting two one-dimensional chaotic functions in series. “The output of the first seed map is linked to the input of the second seed map. The output of the second one is fed back into the input of the first one for recursive iterations, and it is also the output of CCS” [56]. Generation of new and different chaotic maps applying any two 1-D chaotic maps is one of the most promising features of the CSS framework. The newly generated chaotic maps have outstanding chaotic features in comparison with their pure grain maps.

Cascade chaotic structure is inspired by cascade structures in electronic circuits. The architecture of CCS is shown in Fig. 1. G(x) and F(x) denote two-grain maps connected through the CCS. An input vector is fed to the CCS structure, and the output of G(x) is calculated and imported to the F(x). Similarly, the output of F(x) is calculated and feedbacked to the G(x) in the recursive mode. The output of F(x) is the CCS output in recursive and non-recursive mode [56]:

𝑥𝑛+1=Γ(𝑥𝑛)=𝐹(𝐺(𝑥𝑛)).
(1)
Fig. 1
figure 1
The general architecture of cascade chaotic system

Full size image
F(x) and G(x) are two-grain maps and can be any 1-D chaotic map. These two chaotic maps can be the same or different due to problem consideration. If F(x) and G(x) are identical, CCS is substituted with the structure that a chaotic function cascaded with itself:

𝑥𝑛+1=𝐹(𝐹(𝑥𝑛))or𝑥𝑛+1=𝐺(𝐺(𝑥𝑛)).
(2)
If F(x) and G(x) are not identical, CCS is substituted with a new structure and defined by the following [56]:

𝑥𝑛+1=𝐹(𝐺(𝑥𝑛))or𝑥𝑛+1=𝐺(𝐹(𝑥𝑛)).
(3)
Various new chaotic maps (NCMs) are bred using distinct grain maps. This versatility, along with more parameters of the CCS structure, yields a consistent framework with complex chaotic behavior. The CCS structure can be developed into three or more layers using different cascaded grain maps. The result of CCS with more layers has a much more complicated output and also better chaotic behavior. However, this developed cascaded structure confronts serious problems such as time delay and difficulty in hardware implementation. The CCS output has the anatomy of the first map, second map, or both of them. The CCS, as mentioned before, consists of all parameters of its grain maps. Therefore, complex properties of the CCS output are provided. For demonstrating and analyzing the chaotic properties of CCS, Lyapunov exponent (LE) is adopted due to the difficulty of direct analyzing [57]. LE of a dynamical system is the rate of divergence of infinitesimally close trajectories in the phase space. It can be applied as a measure to demonstrate the chaotic behavior of a dynamic system. Suppose that δZ0 is the initial separation of two trajectories in the phase space, the divergence of these two trajectories is calculated after time t by the following equation [56]:

|𝛿𝑍(𝑡)|≈𝑒𝜆𝑡|𝛿𝑍0|
(4)
where λ denotes the Lyapunov exponent. In the previously described CCS equation (Eq. 1), suppose that x0 and y0 are the two near initial CCS values, and xn and yn denote the forenamed path after the nth iteration. Therefore, the LE of Γ(x) is described by:

𝜆Γ(𝑥)=𝜆𝐹(𝑥)+𝜆𝐺(𝑥).
(5)
Hence, based on the above, the LE of cascade chaotic structure is defined based on the summation of LE values of its two-grain maps. If 𝜆Γ(𝑥) is bigger than zero, the trajectories of two initially nearby orbit significantly diverge as the iteration number increments and CCS changes to chaotic behavior. The more positive the LE exponent, the greater the divergence, leading to better chaotic efficiency. In short, chaotic behaviors of cascade chaotic structure are as follows: If 𝜆𝐹(𝑥) and 𝜆𝐺(𝑥) are positive, then 𝜆Γ(𝑥) is positive and bigger than the LE of its two-grain maps. It means that if both grain maps are chaotic, then the CCS is also chaotic and has superior chaotic performance than its grain maps. If 𝜆𝐹(𝑥) and 𝜆𝐺(𝑥) are negative, then 𝜆Γ(𝑥) is negative too, and there is not any chaotic behavior in the CCS output sequences. If 𝜆𝐹(𝑥) is negative and 𝜆𝐺(𝑥) is positive or vice versa, then 𝜆Γ(𝑥) is positive when 𝜆𝐹(𝑥)+𝜆𝐺(𝑥) is positive and 𝜆Γ(𝑥) is negative when 𝜆𝐹(𝑥)+𝜆𝐺(𝑥) is negative. It means that if there is only one chaotic grain map, the chaotic output of CCS depends on the value of 𝜆𝐹(𝑥)+𝜆𝐺(𝑥). If it is positive, the CCS behavior is chaotic, and if it is negative, the CCS behavior is non-chaotic. Briefly, it could be said that the CCS behavior is chaotic when there is at least one chaotic grain map. Also, if both of the grain maps are in the chaotic mode, the chaotic performance of each grain map is dominated by the chaotic performance of the CCS.

Proposed method
This section describes the framework in two steps. In the first step, analyzing the cascade structure is done through the bifurcation diagram of the chaotic Gaussian and the cascade chaotic Gaussian, and the efficiency of the CCS is depicted. The second part demonstrates the design of a chaotic neural network based on the Lee oscillator. In this part, embedding the oscillator dynamics in the activation function is depicted. Also, Equations and relations of the oscillator, along with its communications, are investigated. Problems of existing activation functions and solutions are discussed, and finally, the entire structure of the CCNN is characterized in the layered format.

Bifurcation behavior of CCS
As mentioned in the previous section, nonlinearity features and thus, the chaotic performance of CCS is improved. To evaluate the chaotic properties of CCS, the bifurcation behavior of CCS is studied through the Gaussian map. The Gaussian function is as follows:

𝑓(𝑥)=exp(−(𝑥−𝜇)22𝜎2)
(6)
where 𝜇 and 𝜎 are the mean and standard deviation of the Gaussian function, respectively. To illustrate and compare the chaotic behavior of the Gaussian grain map and the cascade chaotic Gaussian map, the Gaussian map inserted in the chaotic ambiance. This chaotic ambiance is achieved through a chaotic oscillator called the Lee oscillator. Figure 2 depicts the neuronal structure of the LEE oscillator served as an AF of a CCNN. As shown in Fig. 2, the network comprises four neurons, with three neurons interacting with each other having a recursive relation. This interaction and recursive relation produce a chaotic dynamic. Cascading the oscillator enhances this property by adding more parameters to the model. The generated chaos in the LEE oscillator occurs in the inter-communication between the neurons, at a particular input interval and under the stimulation of an input stimulus. As mentioned before, the lee-oscillator comprises four fundamental neural elements u, v, w, and z. The generalized neurotic dynamics of these elemental neurons are given as follows [1]:

𝑢(𝑡+1)=𝑓[𝑎1⋅𝑢(𝑡)+𝑎2⋅𝑣(𝑡)+𝐼(𝑡)−𝜃𝑢]
𝑣(𝑡+1)=𝑓[𝑏1⋅𝑢(𝑡)−𝑏2⋅𝑣(𝑡)−𝜃𝑣]
𝑤(𝑡+1)=𝑓(𝑥)
𝑧(𝑡)=[𝑢(𝑡)−𝑣(𝑡)]⋅𝑒−𝑘𝐼2(𝑡)+𝑤(𝑡)
(7)
Fig. 2
figure 2
Neuronal structure of LEE oscillator

Full size image
In which u(t), v(t), w(t), and z(t) indicate excitatory, inhibitory, input and output neurons, respectively; f(x) is the input function to the network. a1, a2, b1, and b2 are the weights for the neuronal connections. K is a decadence constant and 𝜃𝑢 and 𝜃𝑣 are the thresholds for excitatory and inhibitory neurons. Assume that in Eq. 7, 𝜃𝑢 = 0; 𝜃𝑣 = 0; and f () is a Gaussian function. Other parameters are set as follows:

𝑎1=0.7856;𝑎2=0.6681;𝑏1=−0.2043;𝑏2=0.5078;𝐾=0.4188.
It should be noted that in the presented project, the task of these parameters is just putting the oscillator in the chaotic mode. More details on this topic and various oscillators’ behaviors based on these parameters can be found in [58]. In this study, the chaotic mode has been chosen based on the mentioned reasons, and the above values were selected accordingly. In other words, these four parameters are tuned so that the oscillator exhibits chaotic behavior. After finding the values related to chaos mode, they are regarded as constants and not parameters. So, the above values can be used in the various implementations of the CCNN model without further tuning. Figure 3 shows the 1-D bifurcation diagram of the chaotic Gaussian map based on the Lee oscillator. This structure is inserted in the cascade platform (Fig. 1), and CCS is provided upon a chaotic Gaussian grain map. Indeed, functions F and G in Fig. 1 are oscillators in the cascade structure. The 1-D bifurcation diagram of the Gaussian CCS is shown in Fig. 4. As shown in Figs. 3 and 4, the Gaussian CCS has a wider chaotic span with the same parameters. In other words, the chaotic behavior provided by CCS not only inherits but also boosts the chaotic performance of their grain maps. These findings confirm the issues related to Lyapunov exponents of CCS and its grain maps, raised in the previous section. Based on these results and the similar outcomes from [56], the nonlinear term of CCS is enhanced compared to the chaotic grain map. So, it seems that this structure with high nonlinearity weights can be appropriate for analyzing and modeling highly nonlinear systems. In fact, the weights of nonlinearity in the proposed system are analogous to the weights of nonlinearity in the modeled system. This feature is suitable when complex systems such as weather forecasting systems, traffic time series prediction, and encryption systems are applied.

Fig. 3
figure 3
Bifurcation diagram of chaotic Gaussian function

Full size image
Fig. 4
figure 4
Bifurcation diagram of cascade chaotic Gaussian function

Full size image
Cascade chaotic neural network (CCNN)
Activation functions decide whether a particular neuron in the network is activated or not. Various activation functions have been presented so far, for example, tangent-sigmoid and sigmoid logistics as nonlinear functions and Purelin and Hardlim as linear functions. As mentioned before, the output of an activation function is in the range of 0 and 1, which in these regions and near the boundaries, the derivative or gradient of these functions is close to zero. Therefore, the weight update process performed by the derivation of activation functions does not calculate correctly. The use of chaos and its combination with other conventional activation functions, presented in [51], partially solved the problem. Moreover, the side effects of the ReLU family AFs such as dead neurons, bias shift, and saturation for large negative values are resolved to some extent. The diagram of this method is depicted in Fig. 5. In this network, as shown in Fig. 5, the combination of two AFs, including a circular map and a logistic map, is used. Mathematically, this combination is defined by:

𝑓(𝑥𝑛+1)=𝑟𝑥𝑛(1−𝑥𝑛)+𝑥𝑛+Ω−𝑘2𝜋sin(2𝜋𝑥𝑛)
(8)
where r is a parameter in the range [0, 4] related to the logistic map, k is a constant interpreted as the strength of nonlinearity, and Ω can be interpreted as an externally applied frequency both related to the circular map. The learning process of this network, including weight modification in recursive propagation, requires derivation of Eq. 8. The derivation of the above relation is as follows:

𝑓′(𝑥𝑛+1)=𝑟−2𝑟𝑥𝑛+1−𝑘cos(2𝜋𝑥𝑛)
(9)
Fig. 5
figure 5
Diagram of neural network with hybrid activation function

Full size image
The previously mentioned parameters r and k are tuned through the trial and error process. Based on the results reported in [51] and compared to conventional activation functions, a neural network with chaotic AF combining with other functions has better performance in terms of accuracy and convergence rate. One of the main contributions of this paper is applying the cascade structure in the activation function layer. Using cascade chaotic function with better nonlinear properties and greater variety than low-level chaotic functions, as discussed through bifurcation behavior analysis, promises outstanding results, especially in confronting the chaotic prediction problem. Increasing the inherent features of chaotic mode such as saturation-free dynamic and highly nonlinearities are followed through cascade structure. In short, CCNN is the embedding of cascade chaotic structure in all neurons of the network. Figure 6 indicates the system architecture of CCNN for chaotic time series prediction. As shown in the figure, each node of the network includes a cascade structure in the AF. CCI, CCH, and CCO are the cascade structures in the input, hidden, and output layers, respectively. In the proposed architecture, two chaotic maps connected in a cascade structure make the activation function layer. Therefore, the nonlinearity weights of AFs in each neuron are enhanced, as investigated in the previous section. The detailed structure of each node is shown in Fig. 7. As shown in the figure, the output of the first map (F(x)) is the input of the second map (G(x)), and the output of the second map is the final output of the AF that can be feedbacked to the input of the first map in a recursive manner. According to the chaotic behavior analysis presented in section three, the output of cascade structure (Y) has better chaotic specifications due to higher LE than its two-grain maps F(x) and G(x). Suppose that F(x) and G(x) are the same, and sinusoidal function is applied as a chaotic grain map. The output of activation function in the cascade structure is as follows:

𝑓(𝑥𝑛+1)=𝑅⋅Sin(𝜋⋅𝑄⋅Sin(𝜋𝑥𝑛))
(10)
where R and Q are two constant in [0, 1]. The derivative of Eq. 10 for use in the recursive propagation to modify the weights is as follows:

d𝑓(𝑥𝑛+1)d𝑥𝑛=𝑅⋅𝑄⋅𝜋2⋅cos(𝜋⋅𝑥𝑛)⋅Cos(𝜋⋅𝑄⋅Sin(𝜋⋅𝑥𝑛))
(11)
Fig. 6
figure 6
System architecture of CCNN for chaotic time series prediction

Full size image
Fig. 7
figure 7
Diagram of neural network with cascade chaotic activation function

Full size image
System implementation and experimental results
This section evaluates the proposed structure. Cascade chaotic structure (CCNN) verified through chaotic time series prediction. The conducted experiments are divided into two test categories. In the first category, experiments are done through the predefined structure and parameters (Table 1). The performance of the proposed method is reported in detail based on the first benchmark time series problem. The CCNN is evaluated in terms of training and generalization. Tables 2, 3, 4, 5, 6, and 7 present the result for the Mackey–Glass data set related to this category. In the second category, finding the optimal parameters of each forecaster are followed, and the results are compared. Three popular benchmarks time series problems, including Mackey–Glass, Lorenz, and the Sunspot time series, are considered in this category. Tables 8, 9, 10, and 11 present the results related to this test category in terms of accuracy and convergence rate.

Table 1 Adjustment of forecasters parameters for the first test category
Full size table
First test category
In this category, the prediction of the Mackey–Glass time series is made to evaluate the efficiency of the proposed method. Various experiments are conducted in this part. In the beginning, the CCNN is applied to forecast the MG time series with the fixed parameters both in forecasters and the benchmark problem. Experiments are conducted 20 times, and the results are reported in detail. Then, the highest error rate is used to determine the convergence rate. Afterward, with the parameter change in the MG time series, the proposed method is evaluated in terms of training and generalization. Finally, to compare the proposed model with popular chaotic methods, chaotic sinusoidal activation function and hybrid model described in Sect. 4.2 (Fig. 5) are selected and implemented. These models are then compared with the baseline models applying sigmoid units. All four models are implemented in the same experimental condition, and results are achieved.

The Mackey–Glass chaotic time series
One of the most referred benchmarks in nonlinear time-delay modeling is the Mackey–Glass Differential Equation. This nonlinear time-delay system generates a chaotic time series under specific conditions. The MG equation is as follows:

d𝑥(𝑡)d𝑡=𝛽𝑥(𝑡−𝜏)1+𝑥𝑛(𝑡−𝜏)−𝛾𝑥(𝑡)
(12)
let β = 0.2, γ = 0.1, and n = 10, using these values for the parameters, and Eq. 12 can be rewritten as follows:

d𝑥(𝑡)d𝑡=0.2𝑥(𝑡−𝜏)1+𝑥10(𝑡−𝜏)−0.1𝑥(𝑡).
(13)
If τ > 16.8, time-series exhibit chaotic behavior. This equation is simulated by the fourth-order Runge–Kutta method to calculate the discrete-time equation:

𝑓(𝑥,𝑛)=0.2𝑥(𝑛−𝜏)1+𝑥10(𝑛−𝜏)−0.1𝑥(𝑛),
(14)
where

𝑥(𝑛+1)=𝑥(𝑛)+ℎ𝑓(𝑥,𝑛).
where h is a time step that supposed to be one, and for 𝑛≤𝜏, the initial values of time series are randomly set.

Forecasting chaotic MG time series using CCNN
This section examines the cascade structure injected in the AF of the feedforward neural network, presented in the previous section. As mentioned in Sect. 4, F(x) and G(x) are considered identical, and a sinusoidal function is applied. Therefore, chaotic activation function, namely sin-sin map, is applied to forecast Mackey–Glass time series:

𝑥𝑡+1=𝑅⋅sin(𝑄𝜋×sin(𝜋𝑥𝑡)).
(15)
In which the coefficients R and Q are optimized by the designer. In this study, the values of R and Q were obtained 3.86 and 3.97, respectively. The structure and parameters of the neural network adopted in the first experiment are summarized in Table 1. MG time series is simulated for τ = 17 and 0≤𝑛≤1175. The conventional method for time series forecasting is to generate a map from D sample time series points, sampled every ∆ unit in time, (x (t − (D − 1) ∆),…, x (t − ∆), x(t)) to a forecasted future point x(t + P). Following the typical parameter setting for Mackey–Glass time series forecasting, D and ∆ are considered 4 and 6, respectively. For each t, the input pattern for network training is a four-dimensional vector as follows:

Input:𝑊(𝑡)=[𝑥(𝑡−18)𝑥(𝑡−12)𝑥(𝑡−6)𝑥(𝑡)].
Then, trajectory forecasting is done via output training data:

Output:𝑆(𝑡)=𝑥(𝑡+6).
For each t, the input/output training data are a matrix whose first element is the four-dimensional data. Dividing the generated data to train, validation, and test dataset is done randomly according to the percentage mentioned in Table 1. Experiments are conducted 20 times, and results are obtained. The loss function used in Table 2 is MSE (Eq. 17). Due to space limitation, only the results related to CCAF are reported in detail, and for the other methods, just the average is reported. Then, the highest error rate is used to determine the convergence rate. In Table 2, the lowest, highest, and average errors are shown in bold. The training error for 20 times execution of the first experiment in Table 2.

Table 2 CCNN model error (MSE)
Full size table
Considering the highest error rate in Table 2 (3.0×10−5), the required convergence time based on the learning cycle is listed in Table 3. CCAF simulation results for train, validation and test data set are shown in Fig. 8. The proposed system has reached 2.12 × 10−5 error after 172 epochs. Error histogram and its mu and variance along with correlation diagram between desired output and output of the proposed model and the MSE and RMSE errors are shown in Fig. 9, separately. Also, tracking the timeline of the Mackey–Glass time series by the trained network is depicted in Fig. 9. In Fig. 10, error distribution is shown for training, evaluation, and test data. As can be seen from the figure, the regression for the entire data is 0.9998. This value represents the successful performance of the proposed model. Finally, Fig. 11 shows the final output of the CCAF model compared to the original Mackey–Glass time series. As shown in the figure, the proposed system performance, especially in the maximum and minimum points, is appropriate, and the proposed model has been able to predict the MG time series accurately. Regarding two criteria, including error and convergence rate, the performance of the CCNN is superior to the other methods. Table 4 shows the results of the proposed network confronting with MG time series forecasting problem with different values of τ. The stated errors are based on the mean absolute error (MAE) and mean square error (MSE). MAE computes the average value of errors in a forecasting set, while MSE is a quadratic scoring rule that computes the average value of the error. These two parameters can be applied together to distinguish the error variations in a forecasting set. These two parameters are calculated as follows:

MAE=12∑𝑡=1𝑛|(𝐴𝑡−𝑃𝑡)|
(16)
MSE=12∑𝑡=1𝑛(𝐴𝑡−𝑃𝑡)2.
(17)
Table 3 CCNN model convergence rate (Epoch)
Full size table
Fig. 8
figure 8
MSE error for cascade chaotic model

Full size image
Fig. 9
figure 9
General view of CCAF performance by demonstrating the MSE, RMSE, error histogram, correlation, and MG time-series tracking

Full size image
Fig. 10
figure 10
Representation of regression and dispersion of error for cascade sine activation function

Full size image
Fig. 11
figure 11
Comparison of cascade chaotic NN output with original MG time-series data

Full size image
Table 4 Performance of the CCNN with different values of τ
Full size table
In which At is the desired output, Pt is the network output of tth data in the dataset, and n is the size of the time series. Based on the results reported in Table 4, as the value of τ increases, the forecaster accuracy is decreased. These results are based on both train and test datasets. After presenting the detailed results related to the proposed model, to compare the presented model with outstanding chaotic methods, chaotic sin activation function and hybrid model [44] are selected and implemented. These models are then compared with the baseline model applying sigmoid units. Specifications of these forecasters are mentioned in Table 1. Table 5 shows the forecasting error related to four neural networks for the MG time-series with τ = 17. In test number 6, the hybrid model outperformed the other models with an error rate of 1.4 × 10−5, and in the other test numbers, the proposed model presents the lowest error and best performance. Also, in test number 13, the hybrid model presents the worst performance among the models with an error rate of 1 × 10−4. As seen from Table 5, the performance of the chaotic sinusoidal map is better than the sigmoid model but compared to the hybrid model, the results are not accurate enough. By turning to cascade chaos (CCAF), it is seen that the results are not only better than the tangent sigmoid but also the combination of functions. Simulation results for the sigmoid model, chaotic sin map, and hybrid model are shown in Figs. 12, 13, and 14, respectively, for train, validation, and test datasets. As seen from Figs. 12 to 14, the hybrid model reaches the error 6.5 × 10−5 after 350 epoch, while the chaotic sin map and the sigmoid model reach the error 1.11 × 10−4, and 1.2 × 10−4 after 282 epoch, and 345 epoch, respectively. Better performance of the chaotic approaches is demonstrated in these figures. In the final step of the first test category and to conclude, all the results obtained from the four neural network models are presented together. In these experiments, 20 realizations of the MG chaotic time series are achieved with different values of τ, and the average values of MSEs and MAEs are calculated for each model. The results for train and test datasets are displayed in Tables 6 and 7, respectively. Again, as the value of τ increases, the accuracy of forecasters is decreased. This conclusion is achieved from both the train and test datasets.

Table 5 Performance of different models for predicting MG chaotic time series with τ = 17
Full size table
Fig. 12
figure 12
Required epoch for sigmoid model to reach desired error

Full size image
Fig. 13
figure 13
Required epoch for chaotic sin function to reach desired error

Full size image
Fig. 14
figure 14
Required epoch for hybrid chaotic model to reach desired error

Full size image
Table 6 Performance of different models for train data in MG time series forecasting with different values of τ
Full size table
Table 7 Performance of different models for test data in MG time series forecasting with different values of τ
Full size table
Second test category
In this test category, finding the optimal parameters of each forecaster, including hidden layer size and learning rate, are followed. Three standard time series prediction benchmarks, including the Mackey–Glass chaotic equation, Lorenz system, and the Sunspot numbers, are considered. The MG time series and the Lorenz time series are two simulated time series, while the Sunspot dataset is the real-world problem containing noise and outliers.

The Lorenz chaotic time series
The Lorenz equations are shown as follows [59]:

d𝑥(𝑡)d𝑦(𝑡)=𝑎1[𝑦(𝑡)−𝑥(𝑡)]
d𝑦(𝑡)d𝑡=𝑥(𝑡)[𝑎2−𝑧(𝑡)]−𝑦(𝑡)
d𝑧(𝑡)d𝑡=𝑥(𝑡)𝑦(𝑡)−𝑎3𝑧(𝑡)
(18)
The above equations are matched up with the simplified version of atmosphere convection mathematical model [60], where a1, a2, and a3 are dimensionless parameters. In the case that a1 = 10, a2 = 28, and a3 = 8/3, the system behavior is chaotic. The x-coordinate of the Lorenz time series is selected for forecasting. The embedding dimension D = 4 and time delay T = 2 are chosen for the reconstructed phase space of the original time series as in [61]. The first 1000 values are selected for training, and the next 500 values are chosen for testing. The time series values are scaled to [− 1, 1].

The sunspot time series
The sunspot is one of the main solar activities for solar cycles, which affects the Earth’s climate and magnetic field [49]. Due to this importance, studying and modeling the sunspot are followed in the research. Forecasting solar cycles is difficult due to its complication. The sunspot data used in the simulations are the monthly mean Wolf sunspot numbers [62]. The smoothed sunspot data from November 1834 to June 2001 have been collected from the World Data Center for the Sunspot Index as in the [62]. This period consists of 2000 points in which the first 1000 values are selected for train dataset, and the next 1000 values are chosen for test dataset. The time series values are scaled to [− 1, 1]. The embedding dimension D = 5 and time delay T = 2 are chosen for the reconstructed phase space of the original time series as in [61].

Applying optimized networks to the three benchmark chaotic time series problems
In the first test category, predetermined networks, including a constant number of neurons in the hidden layer and also fixed learning rate, are applied for forecasting MG time series. In this section exploring optimized networks for each benchmark regarding the activation function of each network are followed. Table 8 shows the variations of hidden units along with variations of the learning rate for the proposed method for predicting the MG Time series. The best and worst cases are shown in bold. As can be seen from Table 8, the loss function slightly decreases with the increase of the number of hidden units. This finding is due to more flexibility of the network to map the given function. Also, ignoring a few exceptions, when the learning rate is low, the loss tends to be low too, and increasing it leads to increasing the loss function. These conclusions are achieved through the prediction of three benchmark problems. This experiment is done for all the models and each benchmark, separately, and the results are obtained. To have a fair comparison between the models, the optimized networks are implemented for the final evaluation. The results are depicted in Table 9 for the three benchmark problems. The best answers for each loss function related to the three benchmarks are shown in bold both for the train and test datasets. As seen from Table 9, the presented model outperformed the other methods in most cases. In the MG dataset, both for train and test datasets, the CCAF outperformed chaotic sin, tang-sig, and hybrid models. In the Lorenz dataset, the MAE value of the hybrid model is less than the other methods, and in other cases, the CCAF reported the best answers. In the Sunspot dataset, the hybrid model performance is slightly better than the CCAF model in the test data according to the MSE value, and in the other cases, the CCAF outperformed the other methods. The results related to the Sunspot time series are shown in Fig. 15. Another note about the Sunspot dataset prediction is that although the training error of CCAF is less than the hybrid model, this point does not guarantee the generalization capability of the proposed method. This is maybe due to the presence of noise in the Sunspot dataset based on the nature of original data. Table 10 is related to the time complexity of four implemented models based on the required epoch. This table summarizes the convergence rate of the four models for prediction of the MG chaotic time series to achieve the error of each method separately. Configurations of the models used in this experiment are based on the optimized approach, (Table 8) and each model has its own best-hidden layer size and learning rate. Therefore, just robustness, flexibility, and approximation capability of each method are reflected and reported in the Results. As can be seen from Table 10, the sin map, despite its lower accuracy, is faster than the proposed model, which can be used for issues of higher convergence speed. However, the high error rate of this model should also be taken into account. Nevertheless, the accuracy along with the suitable convergence rate can be improved by combining the sin function with other functions. In the following, the convergence rates of the tang-sig, hybrid model, and the proposed model are investigated. Cascade chaotic model has a lower convergence rate than the other models, but different error rates of each model should also be considered. The next section investigates this issue, and both the error and convergence rates are considered simultaneously.

Table 8 Errors (MSE) of the proposed method with variation of hidden units and learning rate of the network for MG time series forecasting
Full size table
Table 9 Performance of the different models with optimized parameters
Full size table
Fig. 15
figure 15
Comparison of various models with optimized parameters in prediction of Sunspot time series

Full size image
Discussion
According to Table 10, the convergence rates of the models in this table are related to the average error rate of each model, separately. This point justifies the convergence rate of the proposed model in the table. To have a fair comparison between the models based on the convergence speed and error rate, simultaneously, it is better to re-compute the convergence speed of these models with the same target error. Subsequently, three models with better convergence rate are retested with a target error of sigmoid tangent mean error (8.8×10−5), and results are reported in Table 11. As can be seen from Table 11, there is a significant reduction in the convergence rate of the hybrid model compared to the tang-sig model. The convergence rate has decreased more than fifty percent in the proposed model demonstrating the effectiveness of the presented model in predicting chaotic time series. Improving the accuracy and convergence rate simultaneously makes the proposed model suitable for the real-time purposes. The reason for this improvement can be verified from multiple perspectives. Using cascade chaotic function with greater variety and better nonlinear properties than low-level chaotic functions and sigmoid functions presents better results. The sigmoid activation functions, despite their positive characteristics, are monotonic. Therefore, they uniformly approach the saturation boundary. They also suffer from flattening the shape of the function in the boundary areas of − 1, 0, and + 1.

Table 10 Convergence rate of different models for predicting MG time series based on the required epoch with different target error
Full size table
Table 11 Convergence rate of different models for predicting MG time series with unique target error
Full size table
Using the cascade chaotic model inside the activation function resolves this problem, as shown in Fig. 16, and therefore, significant improvement, especially in the convergence rate, will occur. Solving the problem of smoothing the shape of the AF, near the boundaries, removes the tendency of function’s derivation to zero. Therefore, the weight update process, which is accomplished by the derivation of the AF, is performed faster. Besides, the probability of being trapped in the local minimum is also reduced, due to chaotic dynamics of the AF. As a result, it was observed that the network has a much better performance than the previous models in finding the final responses of weights to predict the time series. Also, they have better chaotic properties than pure one-dimensional chaos. Despite the high nonlinearity and diversity of cascade chaotic systems, these models have a lower computational cost and more straightforward hardware implementation than high-order chaos. So, as reported in Table 11, concerns about increasing computational cost are removed.

Fig. 16
figure 16
The waveform of cascade sine activation function

Full size image
Conclusion
This paper has proposed a novel chaotic neural network, namely cascade chaotic neural network (CCNN). The significant contribution of the presented paper is to narrow the gap between the conventional neural network, cascade network, and chaotic systems for solving some important limitations of the conventional neural networks such as MLP. The conventional AFs such as tan hyperbolic and sigmoid used in the MLP learning algorithms suffer from the problem of saturation. This problem leads to increase in the number of training cycles, and ultimately a slow convergence rate. Also, another problem of these networks is their tendency to become trapped into the local minima due to the dynamics of gradient descent used in the backpropagation algorithm. Various approaches have so far been presented to solve or reduce these problems. From introducing new AFs to modifying gradient computation and from improving the BP algorithm to using high order derivatives, and some successful results were achieved. However, each of them has some shortcomings. Dramatic divergence, high storage requirements, complex and costly computations of each iteration for the BP modification algorithms are some shortcomings of the above approaches. Also, dead neurons, bias shift, and restriction of use in complex modeling, due to possessing linearity, are some other drawbacks of the recently presented AFs. On the other hand, none of the chaotic neurons in CNN are inspired by the functional activity of biological neurons in the human brain. The resulting network has the characteristics of the chaotic neural network in modeling nonlinear systems and the features of cascade chaotic systems that enhance chaos capabilities. In other words, injecting chaos into the network is followed by introducing a chaotic activation function, and this phenomenon is reinforced through a cascade structure. The saturation-free properties both in chaotic AF and its gradient, along with greater variability of chaotic dynamic that can be shown in bifurcation behavior, promise prominent features for modeling nonlinearities. These useful features of chaos mode are the main reason for selecting this approach, and other models for comparison are selected and implemented based on this fact. In fact, all of the models that implemented and organized the test environment are in chaotic mode, along with sigmoid AF as the baseline.

Behavioral and fundamental analyses are done to approve the performance of the proposed structure. The bifurcation diagram of the resulting chaotic system confirms the enhanced chaotic behavior of the CCS compare to conventional pure chaotic maps. Besides, the evaluation and comparison results have shown that the new CCNN has more chaotic and nonlinear properties than CNN when confronting the problem of chaotic time series prediction. Another noteworthy point is that in the proposed system, due to division of the whole procedure into chaos generation and chaos application, the role of parameters k, ai, and bi in oscillator equation is just putting the system into chaos mode, and after that, they are regarded as constants. Therefore, unlike the models with hyperparameters, presented in the previous researches, there will be no imposed complexity in the proposed system. Therefore, important limitations of previous studies are theoretically and practically solved to a large extent. The first denotes the pure specifications of chaotic maps such as saturation-free and high nonlinearity and the latter refers to designing two-step model including generation and application of chaos mode to prevent overloaded complexity and hyperparameters model. Furthermore, escaping from local minima and improving the convergence rate can be shown from the results. These points confirm the first claim, mentioned in Sect. 1, that the limitations of NN originated from human brain, can also be resolved by some other features of the brain such as chaotic phenomena. Briefly, introducing the CCNN model as a function-independent chaotic neural network, applying the Lee oscillator as a chaos generator in NN and reinforcement of the generated chaotic behavior through the cascade structure, and finally applying the saturation free chaotic functions in the neurons activation functions, are the most contributions of this paper.

However, one of the challenges of the presented study is to optimize the parameters of the model. In the other words, designing the automated procedure for tuning the parameters is the accurate topic for this issue. For future work, setting the parameters of the cascade network (R and Q in the sinusoidal function) and presenting a training algorithm for the CCNN can be followed.

Keywords
Chaotic neural network
Cascade chaotic system
Bifurcation diagram
Chaotic maps
Benchmark chaotic time series problem
