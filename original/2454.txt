Multi-view clustering methods based on tensor have achieved favorable performance thanks to the powerful capacity of capturing the high-order correlation hidden in multi-view data. However, many existing works only pay attention to exploring the inter-view correlation (i.e., the correlation between views for a same sample) and ignore the intra-view correlation (i.e., the correlation between different samples in a view), such that the high-order information cannot be fully utilized. Toward this issue, we propose an innovative multi-view clustering method in this paper, multi-view clustering with dual tensors (MCDT), which simultaneously exploits the intra-view correlation and the inter-view correlation. Specifically, we first learn a set of specific affinity matrices by using subspace learning in each view. Then, we stack these affinity matrices into a tensor and impose the tensor nuclear norm to exploit the intra-view high-order correlation. Meanwhile, we also rotate this tensor to exploit the inter-view high-order correlation, so as to exploit more comprehensive information hidden in multiple views. Extensive experiments on benchmark datasets demonstrate that the proposed MCDT obtains superior performance in comparison with existing state-of-the-art methods.

Introduction
As a fundamental research topic in computer vision and pattern recognition, clustering is widely used in information retrieval and image segmentation. Its goal is to group data into corresponding clusters based on their similarity [1, 2]. In the past few decades, a large number of clustering methods have been proposed [3,4,5]. Among these methods, the standard spectral clustering (SPC) [3] is the most pervasive method with its fundamental advantages of being easily implemented and efficiently solved. Subsequently, sparse subspace clustering (SSC) [4] exploits the clustering structure with a sparse representation, and low-rank representation (LRR) [5] reveals the real subspace structure by a low-rank representation. Then, both SSC and LRR employ SPC to segment data into corresponding clusters.

In recent years, the means by which humans obtain and process information have become more diverse, and most real-world data are composed of multiple views or feature sets [6,7,8,9]. For example, a webpage can be composed of several different types of data or a news can be described in different languages [10, 11]. So above clustering methods based on single view cannot process these multi-view data. To fully utilize the information from multiple views and improve the clustering performance, a large number of multi-view clustering methods have been proposed [12,13,14,15,16,17,18,19,20,21].

In these multi-view clustering methods, subspace learning methods receive extensive concern because of the effectiveness of mining the subspace structure of multi-view data [22,23,24,25]. For example, multi-view subspace clustering (MVSC) [12] integrates information from multiple views and learns a shared affinity matrix to exploit consistent information among different views. Latent multi-view subspace clustering (LMSC) [13] seeks a shared latent representation to mine potential consistent information among multiple views. Consistent and specific multi-view subspace clustering (CSMSC) [14] exploits the consistent and specific information hidden in multi-view data, so as to obtain a better affinity graph for spectral clustering. Multi-view clustering in latent embedding space (MCLES) [15] seeks a shared latent embedding space and preserves the global structure and learns the clustering indicator matrix in latent embedding space simultaneously.

However, above these methods focus on capturing the clustering information in each individual view and ignore the high-order information hidden in multi-view data. Therefore, it generally cannot obtain optimal clustering performance when omitting correlation in spatial structure. In order to tackle this issue, low-rank tensor constrained multi-view subspace clustering [26] utilizes tensor to capture the complementary information in multi-view data. Nevertheless, this work cannot capture high-order correlation well, since it only utilizes the tensor low rank constraint with unfolding [27] to extend the LLR. Inspired by [28], tensor nuclear norm (t-TNN) derived by tensor singular value decomposition (t-SVD) is introduced to supply a matrix-like multiplication operation for the tensor. t-SVD has a similar structure as the matrix SVD and can capture the high-order structure information of tensor through a t-product operation in the Fourier domain [29]. It is deemed as a better way to exploit the high-order structure information of third-order tensor than the tensor unfolding and decomposition. To perform subspace clustering with tensor on multi-view data, on unifying multi-view self-representations for clustering by tensor multi-rank minimization (t-SVD-MSC) [30] utilizes circulant algebra of the tensor to capture the high-order correlation hidden in multi-view data. Essential tensor learning for multi-view spectral clustering (ETLMSC) [1] unites the Markov chain, the robust principle component analysis, and t-TNN to keep the low-rank property of the learned tensor and exploit the high-order information among multiple views. Low-rank multi-view clustering in third-order tensor space (SCMV-3DT) [31] utilizes the subspace learning and a linear combination with the low-rank and sparse penalty to construct data in third-order tensor space. Multi-view subspace clustering via tensorial t-product representation (SCMV-3DT) [32] proposes a novel construction method by effectively stacking the data matrices of multiple views into third-order tensorial data. Nevertheless, for the above tensor-based methods, they only consider the inter-view high-order correlation, but do not pay attention to the intra-view high-order correlation.

To fully explore the high-order information in multi-view data, we put forward a new multi-view clustering method, termed multi-view clustering with dual tensors (MCDT). To be specific, we first learn a set of specific affinity matrices according to subspace self-expressiveness learning in each view. To exploit the high-order information hidden in multi-view data, we stack these view-specific multiple affinity matrices into a tensor. Furthermore, in order to capture the intra-view high-order correlation (i.e., the correlation between views for a same sample) and the inter-view high-order correlation (i.e., the correlation between different samples in a view), we simultaneously consider tensor rotation and non-rotation and constrain them by t-TNN in tensor space. Finally, we unite the above analysis into a unified framework to learn these optimal affinity matrices and low-rank tensors. Overall, the contributions of this paper are summed up as follows:

In order to effectively exploit the high-order information hidden in multi-view data, a novel multi-view clustering method is proposed, termed MCDT.

Different from existing tensor-based methods, MCDT simultaneously considers the intra-view high-order correlation and the inter-view high-order correlation in tensor space. By comprehensively considering the high-order correlation of tensors, we can better exploit consistent and complementary information hidden in multi-view data.

The performance of MCDT is evaluated via extensive experiments on five commonly used datasets. The results demonstrate that MCDT surpasses these state-of-the-art multi-view clustering methods.

Related work
In this section, we detailedly introduce the definition of notations and simultaneously give the basic preliminaries about tensor used throughout this paper.

Notations
The notations used throughout this paper are shown in Table 1. In addition, there are some other operators about tensor. The fast Fourier transform (FFT) and corresponding inverse FFT operators of tensor  are 𝑓=fft(,[],3) and =ifft(𝑓,[],3), respectively. The block vectorizing and corresponding inverse operation of tensor 𝑛1×𝑛2×𝑛3 are defined as 𝑏𝑣𝑒𝑐()=[M(1);M(2);⋯;M(𝑛3)]∈ ℝ𝑛1𝑛3×𝑛2 and 𝑓𝑜𝑙𝑑(𝑏𝑣𝑒𝑐())=. The block circulant matrix of tensor 𝑛1×𝑛2×𝑛3 is defined as 𝑏𝑐𝑖𝑟𝑐()∈ℝ𝑛1𝑛3×𝑛2𝑛3.

Table 1 The basic notations defined in this paper
Full size table
Preliminaries
In this subsection, some definitions about tensor will be introduced to help reader better understand tensor nuclear norm [30, 33].

Definition 1
(t-Product). The t-product of two tensors with matched dimension, ∈ℝ𝑛1×𝑛2×𝑛3 and ∈ℝ𝑛2×𝑛4×𝑛3, is ∗∈ℝ𝑛1×𝑛4×𝑛3, i.e.,

∗=fold(bcirc() bvec ())
(1)
Definition 2
(Tensor Transpose). The transpose tensor of ∈ℝ𝑛1×𝑛2×𝑛3 is an 𝑛2×𝑛1×𝑛3 tensor, 𝑇, which is obtained by transposing every frontal slice of , and simultaneously reversing the order of frontal slices 2 through 𝑛3 in tensor space.

Definition 3
(Orthogonal Tensor). The tensor ∈ℝ𝑛1×𝑛2×𝑛3 is orthogonal when it satisfies the following condition

𝑇∗=∗𝑇=
(2)
where the notation ∗ stands for the t-product.

Definition 4
(Identity Tensor). It is a identity tensor that tensor ∈ℝ𝑛1×𝑛1×𝑛3 satisfies that the first frontal slice is identity matrix with size 𝑛1×𝑛1, and simultaneously, other frontal slices are zero.

Definition 5
(f-diagonal Tensor). The tensor is f-diagonal tensor when all frontal slices are diagonal matrices. In addition, the t-production with f-diagonal tensors of the size with 𝑛1×𝑛2×𝑛3, i.e., , =∗, is also a f-diagonal tensor with 𝑛1×𝑛2×𝑛3.

Definition 6
(Tensor Multi-Rank). It is a vector ℝ𝑛3×1 about the multi-rank of the tensor ∈ℝ𝑛1×𝑛2×𝑛3, where the k-th element of the vector is the rank of (𝑘).

Definition 7
(Tensor Singular Value Decomposition (t-SVD)). The t-SVD of  is defined as

=∗∗𝑇
(3)
where ∈ℝ𝑛1×𝑛2×𝑛3 and ∈ℝ𝑛1×𝑛2×𝑛3 are orthogonal, respectively. For the tensor ∈ℝ𝑛1×𝑛2×𝑛3, it is f-diagonal tensor.

Definition 8
(Tensor Nuclear Norm based on t-SVD (t-TNN)). For the tensor ∈ℝ𝑛1×𝑛2×𝑛3, its tensor nuclear norm ‖‖⊛ is defined as the sum of singular values with all the frontal slices of 𝑓:

‖‖⊛=∑𝑘=1𝑛3‖‖(𝑘)𝑓‖‖∗=∑𝑖=1min(𝑛1,𝑛2)∑𝑘=1𝑛3∣∣(𝑘)𝑓(𝑖,𝑖)∣∣
(4)
where (𝑘)𝑓 is obtained by (𝑘)𝑓=(𝑘)𝑓(𝑘)𝑓((𝑘)𝑓)𝑇 in the Fourier domain.

Proposed method
In multi-view clustering, given n samples drawn from V different views, 𝐗(𝑣)∈ℝ𝑑𝑣×𝑛 (𝑣=1,2,…,𝑉) denotes the data matrix about the v-th view, where 𝑑𝑣 indicates the feature dimensionality. In this paper, we discuss the LRR based on subspace clustering [5], which constructs affinity matrices according to the correlation between paired samples and exploits low-rank subspace structure hidden in data [34]. Similarly, LRR-based multi-view subspace clustering methods solve the following minimization problem:

min𝐀(𝑣),𝐄(𝑣) s.t. ‖𝐀(𝑣)‖∗+𝛼‖𝐄(𝑣)‖2,1𝐗(𝑣)=𝐗(𝑣)𝐀(𝑣)+𝐄(𝑣),𝑣=1,2,...,𝑉
(5)
where 𝐀(𝑣)∈ℝ𝑛×𝑛 is the affinity matrix learned from v-th view, ‖⋅‖2,1 and ‖⋅‖∗ denote the ℓ2,1-norm and the nuclear norm of a matrix, respectively. After getting 𝐀(𝑣), the final similarity matrix is constructed by integrating all affinity matrices: 𝐇=1𝑉∑𝑉𝑣=1(|𝐀(𝑣)|+(|𝐀(𝑣)|)𝑇)/2. Nevertheless, problem (5) ignores the high-order correlation hidden in multi-view data.

In order to tackle this drawback, some tensor-based multi-view clustering methods [35, 36] are proposed to exploit the high-order information hidden in multi-view data [30]. Inspired by tensor learning [37], we stack the learned affinity matrices 𝐀(𝑣) of all views into a tensor , to exploit the high-order correlation of multi-view data. Since each view represents same objects, different 𝐀(𝑣) should contain some consistent information; meanwhile, the number of clusters is much less than the number of samples. Considering these situations, the tensor  should be low-rank. In this paper, we utilize t-TNN to constrain the intrinsic low-rank structure of the tensor , since t-TNN is the tightest convex relaxation about the ℓ1-norm of the tensor multi-rank [30]. Hence, we have the following objective function

min𝐀(𝑣),𝐄(𝑣) s.t. ‖‖⊛+𝛼‖𝐄‖2,1𝐗(𝑣)=𝐗(𝑣)𝐀(𝑣)+𝐄(𝑣),𝑣=1,…,𝑉,𝐄=[𝐄(1);𝐄(2);…;𝐄(𝑉)],=𝛷(𝐀(1),𝐀(2),…,𝐀(𝑉))
(6)
where 𝛷 is a function that merges the affinity matrices 𝐀(𝑣)(𝑣=1,2,…,𝑉) into a third-order tensor . Conversely, we have the reversible relationship: 𝛷−1(𝑣)()=𝐀(𝑣), and 𝛷−1(𝑣) is the inverse function of 𝛷(⋅). In addition, its subscript (v) denotes the v-th frontal slice of the tensor . In theory, the intra-view special information can be preserved in the singular values, which correspond slices of the f-diagonal tensor . The tensor  is obtained by the t-SVD of tensor  in Fourier domain. Therefore, we can fully exploit intra-view high-order correlation in tensor space.

However, problem (6) ignores the inter-view high-order correlation. To overcome this drawback, we add an extra term to rotate the original tensor ∈ℝ𝑛×𝑛×𝑉 into ˜˜∈ℝ𝑛×𝑉×𝑛, which is illustrated in Fig 1. According to the definition of t-TNN, the t-SVD is performed along each frontal slice of tensor ˜˜∈ℝ𝑛×𝑉×𝑛 after FFT operation along the third dimension. By above operation, we can capture the inter-view high-order correlation among all views. Therefore, problem (5) can further be converted as:

min𝐀(𝑣),𝐄(𝑣) s.t. ‖˜˜‖⊛+𝛼‖𝐄‖2,1𝐗(𝑣)=𝐗(𝑣)𝐀(𝑣)+𝐄(𝑣),𝑣=1,…,𝑉,𝐄=[𝐄(1);𝐄(2);…;𝐄(𝑉)],˜˜=𝛹(𝐀(1),𝐀(2),…,𝐀(𝑉))
(7)
where 𝛹(⋅) is a function, which rotates the tensor  to ˜˜. Conversely, we have the following reversible relationship: 𝛹−1(𝑣)(˜˜)=𝐀(𝑣), where 𝛹−1(𝑣) denotes the corresponding inverse function of 𝛹(⋅). In addition, its subscript (v) is the v-th frontal slice.

Fig. 1
figure 1
Illustration of tensor  and rotated tensor ˜˜. In order to exploit intra-view correlation between samples in a view, the affinity graphs are stacked into a tensor , as shown in left part. For exploiting the inter-view correlation between views for a same sample, tensor  is rotated to tensor ˜˜, as shown in right part

Full size image
To fully capture the complementary and consistent information hidden in multi-view data, we investigate the intra-view correlation learned in problem (6) and the inter-view correlation learned in problem (7) to a unified framework. Then, the final objective function of MCDT can be shown as

min𝐀(𝑣),𝐄(𝑣) s.t. ‖‖⊛+𝛼‖˜˜‖⊛+𝛽‖𝐄‖2,1𝐗(𝑣)=𝐗(𝑣)𝐀(𝑣)+𝐄(𝑣),𝑣=1,…,𝑉,𝐄=[𝐄(1);𝐄(2);…;𝐄(𝑉)],=𝛷(𝐀(1),𝐀(2),…,𝐀(𝑉)),˜˜=𝛹(𝐀(1),𝐀(2),…,𝐀(𝑉))
(8)
where we impose ℓ2,1 norm on 𝐄 to encourage its column to be zero, such that sample-specific corruptions and outliers of original data will be better removed.

By doing so, we can fully capture the high-order information hidden in multi-view data in tensor space, including the intra-view high-order correlation and the inter-view high-order correlation.

Optimization
Algorithm
Problem (8) can be effectively solved by using the alternating direction method of multipliers (ADMM) [15, 38]. By introducing two auxiliary tensors,  and , the corresponding augmented Lagrangian function of (8) is defined as

(𝐀(𝑣),…,𝐀(𝑉);𝐄(1),…,𝐄(𝑉);;)=‖‖⊛+𝛼‖‖⊛+𝛽‖𝐄‖2,1+∑𝑣=1𝑉(⟨𝐘(𝑣),𝐗(𝑣)−𝐗(𝑣)𝐀(𝑣)−𝐄(𝑣)⟩+𝜇2‖‖𝐗(𝑣)−𝐗(𝑣)𝐀(𝑣)−𝐄(𝑣)‖‖2𝐹)+⟨1,−⟩+𝜌2‖−‖2𝐹+⟨2,˜˜−⟩+𝜌2‖˜˜−‖2𝐹
(9)
where matrix 𝐘(𝑣), tensor 1, and tensor 2 are Lagrange multipliers, and 𝜌 and 𝜇 are the penalty parameters. They can be adjusted by utilizing adaptive updating strategy as suggested in [39]. Although the above optimization problem (9) seems hard to solve, we can solve this tricky problem by cleverly updating the variables separately. Specifically, the alternative minimization scheme is adopted to update 𝐀(𝑣), 𝐄(𝑣),  and , respectively. Hence, the detailed procedure can be partitioned into following four steps alternatingly.

▸ Update 𝐀(𝑣) with fixed 𝐄(𝑣),  and 

Since 𝛹−1(𝑣)((𝑣)1)=𝐖(𝑣)1, 𝛷−1(𝑣)((𝑣)2)=𝐖(𝑣)2, 𝛹−1(𝑣)()=𝐌(𝑣), and 𝛷−1(𝑣)()=𝐍(𝑣), we can obtain variable 𝐀(𝑣) by tackling the following minimization problem:

min𝐀(𝑣)⟨𝐘𝑣,𝐗(𝑣)−𝐗(𝑣)𝐀(𝑣)−𝐄(𝑣)⟩+𝜇2‖𝐗(𝑣)−𝐗(𝑣)𝐀(𝑣)−𝐄(𝑣)‖‖‖2𝐹+⟨𝐖(𝑣)1,𝐀(𝑣)−𝐌(𝑣)⟩+𝜌2‖‖‖𝐀(𝑣)−𝐌(𝑣)‖2𝐹+⟨𝐖(𝑣)2,𝐀(𝑣)−𝐍(𝑣)⟩+𝜌2‖𝐀(𝑣)−𝐍(𝑣)‖2𝐹
(10)
By taking the derivative of (10) with respect to 𝐀(𝑣), and setting its derivative to zero, we can be obtained

(𝐀(𝑣))∗=𝜇((𝐗(𝑣))𝑇𝐗(𝑣)+2𝜌𝐈)−1(𝜇(𝐗(𝑣))𝑇𝐊(𝑣)1+𝜌𝐊(𝑣)2+𝜌𝐊(𝑣)3)
(11)
where 𝐊(𝑣)1=𝐗(𝑣)−𝐄(𝑣)+𝐘(𝑣)𝜇, 𝐊(𝑣)2=𝐌(𝑣)−𝐖(𝑣)1𝜌, and 𝐊(𝑣)3=𝐍(𝑣)−𝐖(𝑣)2𝜌.

▸ Update 𝐄(𝑣) with fixed 𝐙(𝑣),  and 

Fixing other variables, we can solve variable 𝐄(𝑣) by tackling the following minimization problem:

=min𝐄(𝑣)𝛽‖𝐄(𝑣)‖2,1+∑𝑣=1𝑉(⟨𝐘𝑣,𝐗(𝑣)−𝐗(𝑣)𝐀(𝑣)−𝐄(𝑣)⟩+𝜇2‖‖𝐗(𝑣)−𝐗(𝑣)𝐀(𝑣)−𝐄(𝑣)‖‖2𝐹)min𝐄(𝑣)𝛽𝜇‖𝐄(𝑣)‖2,1+12‖𝐄(𝑣)−𝐃‖2𝐹
(12)
where 𝐃 is obtained by vertically concatenating these matrices (i.e., 𝐗(𝑣)−𝐗(𝑣)𝐀(𝑣)+1𝜇𝐘(𝑣)) along column. Similar as [40], this minimization problem has the following solution

𝐄∗:,𝑖=⎧⎩⎨⎪⎪‖‖𝐃:,𝑖‖‖2−𝛽𝜇𝐃:,𝑖,‖‖𝐃:,𝑖‖‖2,0,‖‖𝐃:,𝑖‖‖2>𝛽𝜇 otherwise. 
(13)
where 𝐃:,𝑖 represents the i-th column in matrix 𝐃.

▸ Update  with fixed 𝐄(𝑣), 𝐀(𝑣) and 

When 𝐀(𝑣) (𝑣=1,2,⋯,𝑉) are fixed, we can obtain variable  by tackling the following minimization problem:

∗=argmin‖‖⊛+𝜌2‖‖‖−(+1𝜌1)‖‖‖2𝐹
(14)
which is referred to the tensor multi-rank minimization, and can be solved via Theorem 1 [30].

Theorem 1
For a scalar 𝜏 and two tensors ∈ℝ𝑛1×𝑛2×𝑛3, ∈ℝ𝑛1×𝑛2×𝑛3, there is a global optimal solution for the following problem:

min𝜏‖‖⊛+12‖−‖2𝐹
(15)
Its solution can be computed by tensor tubal-shrinkage operator, i.e.,

=𝑛3𝜏()=∗𝑛3𝜏()∗𝑇
(16)
where =∗∗𝑇 and 𝑛3𝜏()=∗, where  is an f-diagonal tensor and corresponding diagonal element is defined as 𝑓(𝑖,𝑖,𝑗)=(1−𝑛3𝜏(𝑗)𝑓(𝑖,𝑖))+ in the Fourier domain.

▸ Update  with fixed 𝐄(𝑣), 𝐀(𝑣) and 

When 𝐀(𝑣)(𝑣=1,2,⋯,𝑉) are fixed, we can obtain variable  by truckling the following minimization problem:

∗=argmin‖‖⊛+𝜌2‖‖‖−(+1𝜌1)‖‖‖2𝐹
(17)
whose solution can be obtained as solving .

▸ Update Lagrange multipliers

The Lagrange multipliers 𝐘(𝑣), 1, and 2 need to be updated as follow:

𝐘(𝑣)=𝐘(𝑣)+𝜇(𝐗(𝑣)−𝐗(𝑣)𝐀(𝑣)−𝐄(𝑣))1=2+𝜌(−)2=2+𝜌(˜˜−)
(18)
Ultimately, the optimization procedure about the proposed MCDT is described in Algorithm 1, and the MATLAB source code will be released in our github page:https://gitee.com/miyong/paper_demo/tree/master/demo_MCDT.

figure a
Computational complexity
In this subsection, we carry out a detailed computational complexity analysis about Algorithm 1, where n is the numbers of sample, and V is the numbers of view, respectively. Overall, Algorithm 1 composes of four sub-problems, including updating , 𝐄,  and . The complexity of updating  is (𝑉𝑛3), which requires matrix inversion operation. However, since (𝐗(𝑣))𝑇𝐗(𝑣)+2𝜌𝐈 is symmetric and positive semidefinite matrix, updating  can be accelerated to (𝑉𝑛2). The complexity of updating 𝐄 is (𝑉𝑛2), which updates each column in turn. The complexity of updating  is (𝑉𝑛2log(𝑛)+𝑛2𝑉2), which takes (𝑉𝑛2log(𝑛)) operations to calculate the 3D FFT and corresponding inverse FFT of a tensor with size 𝑛×𝑉×𝑛, and (𝑉2𝑛2) operations to calculate SVD of these 𝑛×𝑉 matrices in the Fourier domain. The complexity updating  is (𝑉𝑛2log(𝑛)+𝑉𝑛3), which takes (𝑉𝑛2log(𝑛)) operations to calculate the 3D FFT and corresponding inverse FFT of a tensor with size 𝑛×𝑛×𝑉 and (𝑉𝑛3) operations to calculate SVD of 𝑛×𝑛 matrix in the Fourier domain. Moreover, we have 𝑛≫𝑉 and 𝑛≫log(𝑛). Therefore, the main complexity of Algorithm 1 can be approximated as (2𝑡𝑉𝑛3), where t means the number of iterations.

Experiment
In this section, we carry out extensive experiments on five real-word datasets to evaluate the availability of the proposed MCDT.

Datasets
We employ five commonly used multi-view clustering datasets to evaluate the proposed MCDT:

BBCSport [13, 41]: It includes 544 documents about sports news with 5 subject areas on the BBCSport website, and the record time is 2004-2005.

ORL [42, 43]: It is composed of 40 distinct objects, and each of object includes 10 different images under different time, facial expressions, lightings, and facial details.

UCI-digits [44, 45]: It is consisted of 2000 handwritten digit objects of 6 features, including Fourier-coefficient feature, profile-correlation feature, Zernike moment feature, Karhunen-Loeve coefficient feature, intensity-averaged feature, and morphological feature.

Scene-15 [46, 47]: It is built by the scenes about 15 categories, containing kitchen, living room, office, bedroom. There are 210 to 410 images for each category, which includes a wide range about indoor and outdoor scene environments.

Caltech101-20 [46]: It includes 8677 images of objects corresponding to 101 categories, and per category includes 40 to 800 images. We selected 20 commonly used categories as the samples of this experiment.

Compared methods
In order to evaluate the effectiveness of the proposed MCDT, we conduct comparison experiment with two commonly used single-view methods, and a sequence of state-of-the-art multi-view methods. They include

SSC𝑏𝑒𝑠𝑡 [4]: It is the standard spectral clustering, which selects the view with the best clustering performance as final result.

LRR𝑏𝑒𝑠𝑡 [5]: The method is the standard LRR, which selects the view with the best clustering performance as final result.

MVSC [12]: It utilizes the consistent and complementary information across multiple views to obtain a consensus affinity graph.

LMSC [13]: By latent representation learning, LMSC learns a shared latent representation with exploiting the consistent information from all the views.

MCLES [15]: MCLES is also a multi-view clustering method with learning a consensus affinity graph in latent embedding space. In addition, it can directly obtain a final clustering indicator matrix without spectral clustering.

t-SVD-MSC [30]: For promoting the clustering performance, t-SVD-MSC rotates the original tensor to capturing the inter-view correlation in multiple views.

ETLMSC [1]: ETLMSC learns an essential tensor by stacking multiple views representations to exploit the high-order correlation in multi-view data.

Evaluation metrics
To effectively evaluate experimental results adequately, we choose six commonly used clustering metrics in this paper, including normalized mutual information (NMI), accuracy (ACC), AR, F-score, recall and precision, to report the performance of these clustering methods. For the above six metrics, the value of them is higher, and corresponding clustering performance is better.

Experimental results
To avoid randomness, we run each experiment 30 times and then report the average and standard deviation values about six metrics. All parameters of these comparison methods are selected based on the recommendations of these original authors to generate the best results and are listed in Table 2. The clustering performance with different methods on the experiments is shown in Tables 3, 4, 5, 6, 7. Note that the boldface denotes as the best result, while the symbol ’-’ denotes as the running time far more than 30 min in an experiment. From the experimental results shown in Tables, we can observe the following conclusions:

Generally, multi-view based methods (i.e., MVSC [12], MCLES [15], LMSC [13], t-SVD-MSC [30], ETLMSC [1]) work better than single-view methods (i.e., SSC𝑏𝑒𝑠𝑡 [4] and LRR𝑏𝑒𝑠𝑡 [5]). This is mainly because multi-view methods can utilize complementary and consistent information among different views to learn better affinity matrix. It confirms that leveraging the consistent and complementary information of multi-view data is very efficient to improve clustering performance. Therefore, how to mine complementary and consistent information is essential for multi-view clustering.

Tensor-based methods, which include t-SVD-MSC [30], ETLMSC [1] and MCDT, achieve significant improvement, compared with the matrix-based multi-view methods (including MVSC [12], MCLES [15], LMSC [13]). The key factor is that tensor-based methods consider the high-order correlation of multi-view data and utilize tensor low-rank minimization to exploit the high-order correlation. Moreover, the definition of t-TNN endows tensor low-rank with interpretability in the original domain, and can more fully exploit structural information than the general matrix nuclear norm based on certain unfolding.

Compared with t-SVD-MSC [30], ETLMSC [1], which only exploit the inter-view high-order correlation in tensor space, our method comprehensively utilizes the high-order information (including the intra-view high-order correlation and the inter-view high-order correlation) from multiple views. From the experimental results shown in the tables, we can observe that MCDT method achieves significant performance improvement. Therefore, it is novel and effective to comprehensively exploit the high-order information in tensor space for multi-view clustering.

Table 2 Summary of parameters used in each method
Full size table
Table 3 Clustering results on BBCSport dataset
Full size table
Table 4 Clustering results on ORL dataset
Full size table
Table 5 Clustering results on UCI-Digits dataset
Full size table
Table 6 Clustering results on Scene-15 dataset
Full size table
Table 7 Clustering results on Caltech-101 dataset
Full size table
Ablation analysis
In this subsection, we have conducted some ablation experiments for the proposed MCDT. To be specific, MCDT-NV is formed by removing the rotation tensor ˜˜ in MCDT (i.e., the first term of MCDT), which only considers the intra-view high-order correlation and ignores the inter-view high-order correlation. Conversely, MCDT-NS is formed by removing the tensor  in MCDT (i.e., the second term of MCDT), which only considers the inter-view high-order correlation and ignores the intra-view high-order correlation.

Then, we conduct some experiments on the Scene-15 and Caltech-101 datasets to verify the effectiveness of the intra-view and inter-view high-order information. As shown in Table 8, we can observe the following conclusions.

MCDT achieves the significant performance improvement compared with MCDT-NV. For instance, MCDT gains large improvements around 21.6%, 30.8%, 37.6% 32.8%, 12.9% and 32.3% over MCDT-NV in respect of ACC, NMI, AR, F-score, recall and precision on the Scene-15 dataset, respectively. The inter-view high-order correlation corresponds to sample-level correlation across different views, which include the complementary and consistent information in multi-view data. Hence, the inter-view high-order correlation is beneficial to improve the final performance for multi-view clustering.

Compared with MCDT-NS, MCDT achieves better performance. Taking the Caltech-101 dataset, for example, MCDT outperforms MCDT-NS about 6.3%, 8.9%, 3.1%, 6.7%, 7.6% and 1.4% in respect of ACC, NMI, AR, F-score, recall and precision, respectively. This is because MCDT exploits the intra-view high-order correlation among samples in each frontal slice of the learned tensor. Therefore, MCDT can obtain a better affinity matrix and further facilitate the final clustering performance. In addition, MCDT-NS shows better performance than MCDT-NV. This means that the inter-view complementary and consistent information among multiple views is more important than the intra-view relationship between samples for multi-view clustering.

Compared with MCDT-NS and MCDT-NV, MCDT deservedly gains the best performance and achieves the significant improvement. This is because MCDT simultaneously exploits the intra-view high-order correlation and the inter-view high-order correlation in tensor space. It shows that fully exploiting the correlation hidden in multi-view data is favorable to improve clustering performance.

Table 8 Ablation experiment about the proposed method
Full size table
Parameter sensitivity
For the proposed MCDT, there are two trade-off parameters, 𝛼 and 𝛽, which are required to be tuned suitably. By utilizing the grid search technique, the parameters 𝛼 and 𝛽 are tuned in range of {10−5,⋯,103}. Figures  2 and 3 show the influence of different parameter values with respect to ACC and NMI on Scene-15 and UCI-digits datasets. It can be observed that MCDT is relatively stable and can obtain the satisfying performance in a wide value range. Specifically, MCDT is easy to get favorable performance by tuning 𝛼 and 𝛽 from the range [10−2,102] and the range [10−5,10−1], respectively. Experiments show that similar results are found on the other datasets.

Fig. 2
figure 2
Parameter sensitivity on UCI-digits dataset

Full size image
Fig. 3
figure 3
Parameter sensitivity on Scene-15 dataset

Full size image
Convergence analysis
In order to prove the convergence property about Algorithm 1 experimentally, we show the values change of objective function in regard to the number of iterations on the five commonly used datasets, including BBCSport, ORL, UCI digits, Scene-15 and Caltech101-20 datasets, as shown in Fig 4. As can be observed here, the iterator algorithm converges till to the stable point rapidly, and the convergence condition is reached within less than 20 iterations. This confirms the fast convergence property about Algorithm 1.

Fig. 4
figure 4
Convergence curves on five datasets

Full size image
Conclusion
In this paper, we put forward an innovative multi-view clustering method, MCDT. MCDT learns multiple view-specific affinity matrices according to subspace learning and simultaneously extends the learned affinity matrices into the tensor space for exploiting high-order correlation hidden in multiple views. To fully capturing the high-order information, MCDT simultaneously exploits the intra-view and inter-view correlation in the tensor space. Consequently, a better affinity matrix can be learned finally. Extensive experimental results testify the significant performance improvement about MCDT in comparison with the existing state-of-the-art methods. In the future, we will consider how to utilize the subspace self-representation in tensor space to exploit the high-order information of incomplete multi-view data.

Keywords
Multi-view clustering
Tensor learning
Subspace learning
High-order correlation