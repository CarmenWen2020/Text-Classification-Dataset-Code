The development of universal and high-efficiency optimization algorithms is a very important research direction of neural networks. Stochastic Gradient Decent Momentum(SGDM) is one of the most successful optimization algorithms, and easily fall into local extremes minimum. Inspired by the prominent success of Fractional-order Calculus in automatic control, we proposed a method based on Fractional-Order named Fractional-Order Momentum(FracM). As a natural extension of integral calculus, fractional order calculus inherits almost all the characteristics of integral calculus, and have some memorization and nonlocality. FracM performs fractional-order difference of momentum and gradient in SGDM algorithm. FracM can partially solve the problem of traps in the local minimum point and accelerated the train process. The proposed FracM optimization method can compare with the most advanced SGDM and Adam and other advanced optimization algorithm in terms of classification accuracy. The experiments show that FracM outperforms other optimizers on CIFAR10/100 and textual datasets IMDB with transformer-based models.

Introduction
Compared with the ability of humans to process information, the performance of traditional machine learning technology is far from satisfactory. Inspired by human information processing capabilities, the concept of deep learning was introduced at the end of the 20th century. Since Hinton [18] proposed a novel deep learning framework named deep belief network (DBN) in 2006, and a lot of breakthroughs have been made in the field of deep learning. In the past ten years, deep learning technology has developed rapidly, and significant progress has been made in signal and information processing. Compared with traditional machine learning and artificial intelligence methods, deep learning technology has made great progress recently, successfully applied to speech recognition, natural language processing (NLP), information retrieval, computational vision and image analysis [25, 35, 50]. DNN have many different structural changes to adapt to different tasks. For image classification problems, convolutional neural networks (CNN) have better performance in extracting features. CNN can be seen as a feature extraction layer behind the input layer, and finally classified by multi-layer perception (MLP). In recent years, based on CNN many different network structures are derived, such as AlexNet, InceptionNet [15], VGGNet [47], GoogleNet and ResNets [45]. For data timing problem recurrent neural network (RNN) is used to extract time series information and put it after the feature extraction layer. Long short-term Memory network(LSTM) [33, 48] is a special RNN, which is mainly used to solve the problem of gradient disappearance and gradient explosion during Long sequence training. In short, LSTM performs better in longer sequences than normal RNN [2, 13].

Since the development of deep learning technology, computers can design algorithms to improve their ability of learn from data and make decisions or predictions based on data. In the past few decades, deep learning has brought a huge impact on our daily lives, including efficient Internet search, autonomous driving systems, computer vision, and optical character recognition. In addition, through the use of deep learning technology, the ability of artificial intelligence (AI) has also been further improved [26, 40, 54].

Deep learning technology overcomes the shortcomings of traditional algorithms that rely too much on manual design features and has aroused growing research interests. Experimental results show that deep learning fits well with big data analysis. The reason why deep learning can be widely popular in the training process is that DNN can partially solve the overfitting problem with the development of big data analysis technology, and obtain a better local minimum, and a faster convergence speed.

The idea of deep neural network is derived from artificial neural network. The neural network uses the corrected gradient information to continuously update the network parameters through back propagation. Since the update process relies on the local gradient information of the random initial point, it is easy to fall into the local extremum. In addition, if the size of the training data is not large enough, the neural network will face the problem of overfitting. Many high-efficiency training techniques have been proposed to solve the above problem and improve the efficiency and accuracy of deep learning. The training of DNN has long been affected by gradient explosion and gradient disappearance, while parameter initialization [12, 44] and ReLU activation function [19] have alleviated this problem. Dropout technology is used to avoid over fitting [20] and has achieved obvious practical results. The batch normalization technology proposed later further improves the robustness of training to hyperparameters. Simonyan [42] analyzes the impact of convolutional network depth on accuracy in large-scale image recognition problems.

This work was mainly focus on optimization method on DNN. DNN have the characteristics of nonlinear, non-convex, multilayer hidden structure, feature vectorization, and massive model parameters, therefore, the optimization problem of deep learning is quite complicated. The goal of supervised learning is to find a function that improves the accuracy on the training data set and has good generalization ability. The research of optimization problem can be divided into three steps. The first is to make the algorithm run and converge to a reasonable solution, such as a saddle point. The second is to make the algorithm converge as fast as possible. The third step is to ensure that the algorithm converges to a solution with a low Loss value (e.g., the global minimum).

When DNN is used to complete classification and recognition tasks, its performance mainly depends on the model structure and optimization algorithm. Therefore, a good optimization algorithm can improve the accuracy and speed of the deep neural network to complete the task. The current optimization algorithms can be roughly divided into first-order methods and second-order methods. The second-order method uses the Hessian matrix approximation to a certain extent. The first-order method and the second-order method are interchangeable, depending on the amount of second-order information used [43]. In the past ten years, due to the shortcomings of the second-order optimization algorithm, such as large memory usage, the first-order optimization algorithm has been the main optimization algorithm, so this article mainly studies the first-order method. Botou and Bousquet [6] provided some theories to prove why first-order methods are sufficient for large-scale machine learning problems. Some researchers now believe that the second-order method may be overfitting and cannot be compared with the first-order method. SGD is the most basic method to find a suitable solution [4] in deep neural network optimization. It has a certain implicit regularization effect and can obtain good test performance. However, recent second-order optimization methods have also achieved some results. Osawa [36] has achieved good test performance on ImageNet using K-FAC (only 35 epochs can reach 75% on ImageNet top-1 Accuracy). Whether the second-order method can be resurrected in future is an interesting thing.

The second-order optimization algorithm calculates the second-order partial derivative of the loss function. In Newtonâ€™s method, the simplest second-order optimization algorithm, the goal is to find the critical point of the function. At the critical point, the loss function can get the maximum or minimum value, or the saddle point. However, the saddle point is not desirable, and only the smallest point is the optimization goal. In other words, when the Hessian matrix is positive, Newtonâ€™s method can find the minimum value of the optimization objective, and it is likely to stop near the saddle point or the local minimum point. In actual high-dimensional data sets, second-order optimization algorithms consume a lot of memory and are often not feasible.

The rest of this article is arranged as follows: Sect. 2 introduced relevant popular methods in optimization field; Sect. 3 detailed introduction our proposes optimization method; Sect. 4 conducts the convergence problem analysis based on the previous problem analysis; Sect. 5 introduces the data set and basic parameter settings used in the experiment and Sect. 6 introduces the comparative analysis of the experimental results; Sect. 7 summarizes the full text.

Related work
In recent years, many improved optimization algorithms have been proposed on the basis of traditional adaptive optimization algorithms. Below we will introduce commonly used optimization algorithms and the latest improved algorithms. For SGD, in the iterative process, all parameters are updated at the same learning rate ğœ‚ in Eq. (1), where îˆ¸ğ‘¡âˆ’1,ğœƒ is a loss functione, ğ‘”t,i is the computed gradient, ğœƒ is the model parameter.

ğ‘”ğ‘¡âˆ’1,ğ‘–=âˆ‚(îˆ¸ğ‘¡âˆ’1,ğœƒ)âˆ‚(ğœƒğ‘¡âˆ’1,ğ‘–)ğœƒğ‘¡,ğ‘–=ğœƒğ‘¡âˆ’1,ğ‘–âˆ’ğœ‚âˆ—ğ‘”ğ‘¡âˆ’1,ğ‘–
(1)
The gradient-based first-order optimization algorithm has core significance in the field of deep learning. SGD is the most widely used optimization algorithm, which solves many practical problems [5]. SGD calculates the gradient of the current parameter according to the defined loss function and labeled samples, and the parameter is updated in the opposite direction of the calculated gradient until convergence or reach a fixed number of iterations. The negative gradient direction help loss function reduces fastest. According to the number of samples in each batch, gradient descent is divided into stochastic gradient descent, mini-batch gradient descent, and batch gradient descent.

(1)
SGD uses the same learning rate for all dimension parameters in the iterative process, which easily leads to slow optimization or model parameter oscillation [46].

(2)
If the loss function has a local minimum or saddle point (see Figs. 1, 2) [7], SGD is easily stuck in this area, since the gradient is zero.

(3)
The gradient calculation is usually in small batches; therefore, the calculated gradient is noisy.

Due to the shortcomings of SGD, many scholars have proposed some improved optimization algorithms to solve the problem of SGD. The SGD with momentum (SGDM) algorithm [37] borrows the concept of momentum in physics. It simulates the inertia of moving objects. When it is updated, it will keep the previously updated direction within a reasonable range and use the current gradient to fine-tune the final update direction. Compared with traditional methods, the biggest advantage is that it can effectively avoid oscillation.

Another major improvement of the Nesterov Accelerated Gradient (NAG) algorithm. Compared with SGDM, the direction of the gradient is no longer calculated from the current position, but calculated from the â€œexpected positionâ€ after moving in the direction of the first item. Intuitively speaking, the direction of the gradient corrected by this method can â€œpredictâ€ the updated result. In fact, in some cases, Nesterov acceleration gradient does accelerate convergence.

Adagrad introduces the sum of squares of historical gradients as the attenuation factor. The adaptive learning rate for different parameters makes the loss function converge faster. The learning rate decreases with the increase in the gradient, that is, the larger the gradient, the smaller the learning rate, and the smaller the gradient, the greater the learning rate, which avoids the problem of constant learning rate in the SGD method. The algorithm still has some shortcomings. You need to manually specify the initial learning rate. As the denominator continues to accumulate historical gradients, the learning rate will gradually drop to 0. Moreover, if the initial gradient is very large, the initial learning rate of the entire training process will be very small, which leads to a longer training time.

Adadelta is an improvement of the Adagrad algorithm, reducing the radical aspect of Adagrad monotonically reducing the learning rate [56]. Adagrad accumulates all past gradients, while Adadelta limits the accumulation of past gradients and controls the infinite accumulation of gradients in the form of a time window. Changing the accumulated gradient information from the total historical gradient to the accumulation of the window period before the current time is equivalent to the accumulation of historical gradient information multiplied by the attenuation coefficient. Solve the problem that the learning rate drops due to the accumulation of historical gradients.

Adam [23, 34] is the most popular adaptive learning rate algorithm, which uses the online learning framework proposed in [60]. Adam keeps the exponential decay average of past gradients ğ‘št, similar to SGDM. In addition to these, Adam also stores the exponential decay average of the past squared gradient ğ‘£t, which is the same as Adadelta and RMSprop. Adam also has its own problems, the fluctuation of the second-order moment causes the fluctuation of the learning rate. AMSGrad [38] solves this problem, it constantly refreshes the maximum value of the second-order moment in the past. AMSGrad uses the maximum value of the second moment to optimize the iterative direction of the parameters. AMSGrad adopts the idea and bias-corrected method basically similar to Adam algorithm. AMsGrad uses the value of ğ‘£Ì‚ maxt,i instead of just the value of ğ‘£Ì‚ t,i , is defined in Eq. (2):

Fig. 1
figure 1
Saddle point in deep neural network

Full size image
Fig. 2
figure 2
Local min point in deep neural network

Full size image
ğ‘št=ğ›½1ğ‘šğ‘¡âˆ’1+(1âˆ’ğ›½1)ğ‘”t,iğ‘£t=ğ›½2ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½2)(ğ‘”t,i)2ğ‘šÌ‚ t=ğ‘št/(1âˆ’ğ›½t1)ğ‘£Ì‚ t=ğ‘£t/(1âˆ’ğ›½t2)ğ‘£Ì‚ maxt,i=max(ğ‘£Ì‚ maxğ‘¡âˆ’1,i,ğ‘£Ì‚ t,i) 
(2)
ğ‘£Ì‚ maxğ‘¡âˆ’1,i=0 for ğ‘¡=1. The parameter updates are performed using the rules, as Eq. (3):

ğœƒğ‘¡+1,ğ‘–=ğœƒğ‘¡,ğ‘–âˆ’ğœ‚ğ‘¡Ã—ğ‘šÌ‚ ğ‘¡,ğ‘–ğ‘£Ì‚ ğ‘¡,ğ‘–â€¾â€¾â€¾âˆš+ğœ–
(3)
At present, almost all deep neural networks use two methods to improve optimization efficiency and accuracy, the first one is adjusting the learning rate, the second one is a new neural network structure. This paper mainly discusses the optimizer.

Adaptive and Momental Bound (AdaMod) [8] was an optimizer skilled in the control of learning rate bounds, which regulate the learning rate not too big than the historical calculating data, guarantee a proper convergence, and make the optimization process of the entire model more stable. Thus, the parameter update in AdaMod is defined in Eq. (4):

ğ‘šğ‘¡=ğ›½1ğ‘šğ‘¡âˆ’1+(1âˆ’ğ›½1)ğ‘”ğ‘¡,ğ‘–ğ‘£ğ‘¡=ğ›½2ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½2)(ğ‘”ğ‘¡,ğ‘–)2ğ‘šÌ‚ ğ‘¡=ğ‘šğ‘¡/(1âˆ’ğ›½ğ‘¡1)ğ‘£Ì‚ ğ‘¡=ğ‘£ğ‘¡/(1âˆ’ğ›½ğ‘¡2)ğœ‚ğ‘¡=ğ›¼/(ğ‘£Ì‚ ğ‘¡â€¾â€¾âˆš+ğœ–)ğ‘ ğ‘¡=ğ›½3ğ‘ ğ‘¡âˆ’1+(1âˆ’ğ›½3)ğœ‚ğ‘¡ğœ‚Ì‚ ğ‘¡=min(ğœ‚ğ‘¡,ğ‘ ğ‘¡)ğœƒğ‘¡,ğ‘–=ğœƒğ‘¡âˆ’1,ğ‘–âˆ’ğœ‚Ì‚ ğ‘¡ğ‘šÌ‚ ğ‘¡
(4)
AdamP [17] algorithm modifies the radial component of the gradient to achieve limiting the gradient norm to be too large and achieve a better result. This technique can be easily applied to other optimization algorithms, the parameter update in AdamP is defined in Algorithms 1 and 2:

figure a
figure b
DiffGrad [10] is a novel optimizer which adjust the learning rate according to the local change in gradients. Thus, the parameter update in DiffGrad is defined in Eq. (5):

ğ‘šğ‘¡=ğ›½1ğ‘šğ‘¡âˆ’1+(1âˆ’ğ›½1)ğ‘”ğ‘¡,ğ‘–ğ‘£ğ‘¡=ğ›½2ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½2)(ğ‘”ğ‘¡,ğ‘–)2ğ‘šÌ‚ ğ‘¡=ğ‘šğ‘¡/(1âˆ’ğ›½ğ‘¡1)ğ‘£Ì‚ ğ‘¡=ğ‘£ğ‘¡/(1âˆ’ğ›½ğ‘¡2)ğœ‰ğ‘¡,ğ‘–=ğ´ğ‘ğ‘ Sig(ğ›¥ğ‘”ğ‘¡,ğ‘–)ğ´ğ‘ğ‘ Sig(ğ‘¥)=11+ğ‘’âˆ’|ğ‘¥|ğ›¥ğ‘”ğ‘¡,ğ‘–=ğ‘”ğ‘¡âˆ’1,ğ‘–âˆ’ğ‘”ğ‘¡,ğ‘–ğœƒğ‘¡+1,ğ‘–=ğœƒğ‘¡,ğ‘–âˆ’ğœ‚ğ‘¡Ã—ğœ‰ğ‘¡,ğ‘–Ã—ğ‘šÌ‚ ğ‘¡,ğ‘–ğ‘£Ì‚ ğ‘¡,ğ‘–â€¾â€¾â€¾âˆš+ğœ–
(5)
LamB [53] was an optimizer through control the learning rate in large batch settings to accelerate optimization procedure. the learning rate is scaled by some designed function, which is similar to the normalization. Thus, the parameter update in LamB is defined in Eq. (6), where ğœ™(ğ‘§)=min{max{ğ‘§,ğ›¾ğ‘™},ğ›¾ğ‘¢}. ğ›¾ğ‘™,ğ›¾ğ‘¢ was Upper and lower bound.

ğ‘šğ‘¡=ğ›½1ğ‘šğ‘¡âˆ’1+(1âˆ’ğ›½1)ğ‘”ğ‘¡,ğ‘–ğ‘£ğ‘¡=ğ›½2ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½2)ğ‘”2ğ‘¡,ğ‘–ğ‘šÌ‚ ğ‘¡=ğ‘šğ‘¡/(1âˆ’ğ›½ğ‘¡1)ğ‘£Ì‚ ğ‘¡=ğ‘£ğ‘¡/(1âˆ’ğ›½ğ‘¡2) rğ‘¡,ğ‘–=ğ‘šğ‘¡^ğ‘£ğ‘¡^â€¾â€¾âˆš+ğœ–ğ‘¥ğ‘¡+1,ğ‘–=ğ‘¥ğ‘¡,ğ‘–âˆ’ğœ‚ğ‘¡ğœ™(â€–â€–ğ‘¥ğ‘¡,ğ‘–â€–â€–)â€–â€–ğ‘Ÿğ‘¡,ğ‘–+ğœ†ğ‘¥ğ‘¡,ğ‘–â€–â€–(ğ‘Ÿğ‘¡,ğ‘–+ğœ†ğ‘¥ğ‘¡,ğ‘–)
(6)
Lookahead [57] select a better optimization direction help smooth out the oscillations through the parameter interpolation, the better optimization direction was generated by another optimizer with fast weights updates along the low curvature directions. the Lookahead algorithm can improve optimizer stability, also promote a faster convergence. Thus, the parameter update in Lookahead is defined in Eq. (7), where A applying the fast weights updated.

 Synchronize  parameters ğœƒğ‘¡,0â†ğœ™ğ‘¡âˆ’1 for ğ‘–=1,2,â€¦,ğ‘˜ do  sample  minibatch  of  data ğ‘‘âˆ¼îˆ°ğœƒğ‘¡,ğ‘–â†ğœƒğ‘¡,ğ‘–âˆ’1+ğ´(ğ¿,ğœƒğ‘¡,ğ‘–âˆ’1,ğ‘‘) end  for ğœ™ğ‘¡â†ğœ™ğ‘¡âˆ’1+ğœ‚(ğœƒğ‘¡,ğ‘˜âˆ’ğœ™ğ‘¡âˆ’1)
(7)
NovoGrad [11] was an adaptive SGD method, which mainly focus on gradients normalization and 2ğ‘›ğ‘‘ moment with each layers. Thus, the parameter update in NovoGrad is defined in Eq. (8):

ğ‘”ğ‘”ğ‘™ğ‘¡â†âˆ‡ğ‘™îˆ¸ğ‘¡,ğœƒğ‘£ğ‘™ğ‘¡â†ğ›½2â‹…ğ‘£ğ‘™ğ‘¡âˆ’1+(1âˆ’ğ›½2)â‹…â€–â€–ğ‘”ğ‘”ğ‘™ğ‘¡â€–â€–2ğ‘šğ‘™ğ‘¡â†ğ›½1â‹…ğ‘šğ‘šğ‘™ğ‘¡âˆ’1+(ğ‘”ğ‘”ğ‘™ğ‘¡ğ‘£ğ‘™ğ‘¡â€¾â€¾âˆš+ğœ–+ğ‘‘â‹…ğœƒğœƒğ‘™ğ‘¡,ğ‘–)ğœƒğœƒğ‘™ğ‘¡+1,ğ‘–â†ğœƒğœƒğ‘™ğ‘¡,ğ‘–âˆ’ğœ‚ğ‘¡â‹…ğ‘šğ‘šğ‘™ğ‘¡
(8)
PID [1] first reveal the relationship between traditional PID and SGD with Momentum. the optimization algorithm imitate the PID algorithm layout [1]. the optimization algorithm is defined in Eq. (9):

SGD is a P Controller

ğœƒğ‘¡+1=ğœƒğ‘¡âˆ’ğ‘Ÿâˆ‚ğ¿ğ‘¡/âˆ‚ğœƒğ‘¡
(9)
SGD wth momentum optimization is a PI Controller, and defined in Eq. (10)

ğ‘‰ğ‘¡+1=ğœ‚ğ‘‰ğ‘¡âˆ’ğ‘Ÿğ‘”ğ‘¡âˆ’1,ğ‘–ğœƒğ‘¡+1,ğ‘–=ğœƒğ‘¡,ğ‘–+ğ‘‰ğ‘¡+1
(10)
SGD is a PI Controller, and defined in Eq. (11)

ğ‘‰ğ‘¡+1=ğœ‚ğ‘‰ğ‘¡âˆ’ğ‘Ÿğ‘”ğ‘¡âˆ’1,ğ‘–ğ·ğ‘¡+1=ğœ‚ğ·ğ‘¡+(1âˆ’ğœ‚)(ğ‘”ğ‘¡,ğ‘–âˆ’ğ‘”ğ‘¡âˆ’1,ğ‘–)ğœƒğ‘¡+1,ğ‘–=ğœƒğ‘¡,ğ‘–+ğ‘‰ğ‘¡+1+ğ¾ğ‘‘ğ·ğ‘¡+1
(11)
The quasi-hyperbolic momentum algorithm (QHM) [31] and quasi-hyperbolic Adam (QHAdam) [31] can recovers numerous than other optimization algorithms in an efficient and stable manner. Through introduce a hyperparameter ğ›½, QHM can recover many optimizer. Thus, the parameter update in QHM is defined in Eq. (12):

parameterized byğ›¼,ğœ–â‰¥0,ğ›½1,ğ›½2âˆˆ[0,1), and ğ‘£1,ğ‘£2âˆˆğ‘…, uses  the  update  rule: ğ‘”ğ‘¡+1,ğ‘–â†ğ›½â‹…ğ‘”ğ‘¡,ğ‘–+(1âˆ’ğ›½)â‹…âˆ‡îˆ¸ğ‘¡,ğœƒğ‘”â€²ğ‘¡+1â†(1âˆ’ğ›½ğ‘¡+11)âˆ’1â‹…ğ‘”ğ‘¡+1ğ‘ â€²ğ‘¡+1â†(1âˆ’ğ›½ğ‘¡+12)âˆ’1â‹…ğ‘ ğ‘¡+1ğœƒğ‘¡+1,ğ‘–â†ğœƒğ‘¡,ğ‘–âˆ’ğœ‚â¡â£â¢â¢â¢(1âˆ’ğœˆ1)â‹…âˆ‡îˆ¸ğ‘¡,ğœƒ+ğœˆ1â‹…ğ‘šÌ‚ ğ‘¡+1(1âˆ’ğœˆ2)(âˆ‡îˆ¸ğ‘¡,ğœƒ)2+ğœˆ2â‹…ğ‘£Ì‚ ğ‘¡+1â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš+ğœ–â¤â¦â¥â¥â¥
(12)
Shampoo [14] produce a new structure-aware preconditioning algorithm for stochastic optimization over tensor spaces. Shampoo through maintains a set of preconditioning matrices, each of which only operates on a single dimension. Thus, the parameter update in Shampoo is defined in Eq. (13), where ğ¿0=0:

ğ‘”ğ‘¡,ğ‘–=âˆ‡îˆ¸ğ‘¡,ğœƒ{ğºğ‘¡âˆˆâ„ğ‘šÃ—ğ‘›}ğ¿ğ‘¡=ğ¿ğ‘¡âˆ’1+ğ‘”ğ‘¡,ğ‘–ğ‘”âŠ¤ğ‘¡,ğ‘–ğ‘…ğ‘¡=ğ‘…ğ‘¡âˆ’1+ğ‘”âŠ¤ğ‘¡,ğ‘–ğ‘”ğ‘¡,ğ‘–ğœƒğ‘¡+1,ğ‘–=ğœƒğ‘¡,ğ‘–âˆ’ğœ‚ğ¿âˆ’1/4ğ‘¡ğ‘”ğ‘¡,ğ‘–ğ‘…âˆ’1/4ğ‘¡
(13)
Yogi [55] was a optimizer, through the control of effective larning rate, which can obtain a better convergence and accuracy. Thus, the parameter update in Yogi is defined in Eq.(14), where Set ğ‘š0=0, ğ‘£0=0

ğ‘šğ‘¡=ğ›½1ğ‘šğ‘¡âˆ’1+(1âˆ’ğ›½1)ğ‘”ğ‘¡ğ‘£ğ‘¡=ğ‘£ğ‘¡âˆ’1âˆ’(1âˆ’ğ›½2)sign(ğ‘£ğ‘¡âˆ’1âˆ’ğ‘”2ğ‘¡,ğ‘–)ğ‘”2ğ‘¡,ğ‘–ğ‘¥ğ‘¡+1=ğ‘¥ğ‘¡âˆ’ğœ‚ğ‘¡ğ‘šğ‘¡/(ğ‘£ğ‘¡â€¾â€¾âˆš+ğœ–)
(14)
AdaBound [30] and AmsBound [30] was a new variant of ADAM and AmsGrad respectively, which adaptive adjustive learning rate between upper bound and lower bound, which converge to a constant value. At the very beginning, AdaBound was an Adam optimizer, and gradually convert to SGD in the end. Thus, the parameter update in AdaBound is defined in Eq. (15), where clips the learning rate element-wisely such that the output is constrained to be in [ğœ‚ğ‘™,ğœ‚ğ‘¢]

ğ‘”ğ‘¡,ğ‘–=âˆ‡îˆ¸ğ‘¡,ğœƒğ‘šğ‘¡=ğ›½1ğ‘¡ğ‘šğ‘¡âˆ’1+(1âˆ’ğ›½1ğ‘¡)ğ‘”ğ‘¡,ğ‘–ğ‘£ğ‘¡=ğ›½2ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½2)ğ‘”2ğ‘¡,ğ‘– and ğ‘‰ğ‘¡=diag(ğ‘£ğ‘¡)ğœ‚Ì‚ ğ‘¡=Clip(ğœ‚/ğ‘‰ğ‘¡â€¾â€¾âˆš,ğœ‚ğ‘™(ğ‘¡),ğœ‚ğ‘¢(ğ‘¡))ğœ‚ğ‘¡=ğœ‚Ì‚ ğ‘¡/ğ‘¡âˆšğœƒğ‘¡+1,ğ‘–=âˆîˆ²,diag(ğœ‚âˆ’1ğ‘¡)(ğœƒğ‘¡,ğ‘–âˆ’ğœ‚ğ‘¡âŠ™ğ‘šğ‘¡)
(15)
Rectified Adam (RAdam) [27] improves the accuracy of optimization by modifying the variance of the learning rate, and can combine training techniques such as gradient clipping and learning rate smooth with RAdam to achieve a better result. Thus, the parameter update in RAdam is defined in Eq. (16):

ğ‘šğ‘¡=ğ›½1ğ‘šğ‘¡âˆ’1+(1âˆ’ğ›½1)ğ‘”ğ‘¡,ğ‘–ğ‘£ğ‘¡=ğ›½2ğ‘£ğ‘¡âˆ’1+(1âˆ’ğ›½2)(ğ‘”ğ‘¡,ğ‘–)2ğ‘šÌ‚ ğ‘¡=ğ‘šğ‘¡/(1âˆ’ğ›½ğ‘¡1)ğœŒğ‘¡â†ğœŒâˆâˆ’2ğ‘¡ğ›½ğ‘¡2/(1âˆ’ğ›½ğ‘¡2)if the variance is tractable, i.e.,ğœŒğ‘¡>4,thenğ‘™ğ‘¡â†(1âˆ’ğ›½ğ‘¡2)/ğ‘£ğ‘¡â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšğ‘Ÿğ‘¡â†(ğœŒğ‘¡âˆ’4)(ğœŒğ‘¡âˆ’2)ğœŒâˆ(ğœŒâˆâˆ’4)(ğœŒâˆâˆ’2)ğœŒğ‘¡â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšğœƒğ‘¡,ğ‘–â†ğœƒğ‘¡âˆ’1,ğ‘–âˆ’ğ›¼ğ‘¡ğ‘Ÿğ‘¡ğ‘šË†ğ‘¡ğ‘™ğ‘¡ğ‘’ğ‘™ğ‘ ğ‘’ğœƒğ‘¡,ğ‘–â†ğœƒğ‘¡âˆ’1,ğ‘–âˆ’ğœ‚ğ‘šğ‘¡Ë†
(16)
SgdW and AdamW [28] analyzes the difference between L2 regularization and weight decay regularization. For adaptive optimization algorithms, the two are not equivalent. The author can restore the original L2 regularization by decoupling the weight attenuation in the loss function. Thus, the parameter update in SgdW and AdamW is defined in Eq. (17):

ğœƒğœƒğ‘¡â†ğœƒğœƒğ‘¡âˆ’1âˆ’ğ‘šğ‘¡âˆ’ğœ‚ğ‘¡ğœ†ğœƒğœƒğ‘¡âˆ’1ğœƒğœƒğ‘¡,ğ‘–â†ğœƒğœƒğ‘¡âˆ’1,ğ‘–âˆ’ğœ‚ğ‘¡(ğ›¼ğ‘šÌ‚ ğ‘¡/(ğ‘£Ì‚ ğ‘¡â€¾â€¾âˆš+ğœ–)+ğœ†ğœƒğœƒğ‘¡âˆ’1,ğ‘–)
(17)
AdaBelief [59] use the exponential moving average to estimate the gradient, if the difference between the observed gradient and prediction gradient was greater than the threshold, then distrust the current observation and take a small step; if the difference between the observed gradient and prediction gradient was less than the threshold, then trust it and take a large step. Thus, the parameter update in AdaBelief is defined in Eq. (18):

ğ‘šğ‘¡,ğ‘–=ğ›½1ğ‘šğ‘¡âˆ’1,ğ‘–+(1âˆ’ğ›½1)ğ‘”ğ‘¡,ğ‘–ğ‘ ğ‘¡,ğ‘–=ğ›½2ğ‘ ğ‘¡âˆ’1,ğ‘–+(1âˆ’ğ›½2)(ğ‘”ğ‘¡,ğ‘–âˆ’ğ‘šğ‘¡,ğ‘–)2ğ‘šÌ‚ ğ‘¡=ğ‘šğ‘¡,ğ‘–(1âˆ’ğ›½ğ‘¡1)ğ‘ Ì‚ ğ‘¡=ğ‘ ğ‘¡,ğ‘–(1âˆ’ğ›½ğ‘¡2)ğœƒğ‘¡â†âˆîˆ²,ğ‘ ğ‘¡âˆš(ğœƒğ‘¡âˆ’1âˆ’ğœ‚ğ‘šË†ğ‘¡ğ‘ ğ‘¡â€¾â€¾âˆš+ğœ–)
(18)
Apollo [32] dynamically incorporates the curvature of the loss function by approximating the Hessian via a diagonal matrix. This paper main contribution is the storage of approximation Hessianas diagonal matrix efficient as first-order optimization methods with linear complexity of time and memory. Thus, the parameter update in Apollo is defined in Eq. (19), where ğ·ğ‘¡=rectify(ğµğ‘¡,ğœ)=max(|ğµğ‘¡|,ğœ).

ğ‘š0,ğ‘‘0,ğµ0â†0,0,0ğ‘šğ‘¡+1â†ğ›½(1âˆ’ğ›½ğ‘¡)1âˆ’ğ›½ğ‘¡+1ğ‘šğ‘¡+1âˆ’ğ›½1âˆ’ğ›½ğ‘¡+1ğ‘”ğ‘¡+1,ğ‘–ğ›¼â†ğ‘‘ğ‘‡ğ‘¡(ğ‘šğ‘¡+1âˆ’ğ‘šğ‘¡)+ğ‘‘ğ‘‡ğ‘¡ğµğ‘¡ğ‘‘ğ‘¡(â€–ğ‘‘ğ‘¡â€–4+ğœ–)4ğµğ‘¡+1â†ğµğ‘¡âˆ’ğ›¼â‹…Diag(ğ‘‘2ğ‘¡)ğ·ğ‘¡+1â†rectify(ğµğ‘¡+1,1)ğ‘‘ğ‘¡+1â†ğ·âˆ’1ğ‘¡+1ğ‘šğ‘¡+1ğœƒğ‘¡+1â†ğœƒğ‘¡âˆ’ğœ‚ğ‘¡+1ğ‘‘ğ‘¡+1
(19)
AdaHessian [52] through spatial averaging and momentum to precondition the gradient vector, then construct the diagonals of the Hessian matrix. The precondition gradient vector lead to a better descent directions. This paper main contribution is the combination of spatial averaging for Hessian diagonal, which reduce the highly misleading caused by the noisy local Hessian information. Thus, the parameter update in AdaHessian is defined in Eq. (20)

ğ‘”ğ‘¡,ğ‘–=âˆ‡îˆ¸ğ‘¡,ğœƒğ‡=1ğ‘ğµâˆ‘ğ‘–=1ğ‘ğµâˆ‚2îˆ¸ğ‘¡,ğœƒâˆ‚ğœƒ2ğ·ğ·=diag(ğ‡)ğ·ğ·(ğ‘ )[ğ‘–ğ‘+ğ‘—]=âˆ‘ğ‘ğ‘˜=1ğ·ğ·[ğ‘–ğ‘+ğ‘˜]ğ‘, for 1â‰¤ğ‘—â‰¤ğ‘,0â‰¤ğ‘–â‰¤ğ‘‘ğ‘âˆ’1ğ·ğ·â¯â¯â¯â¯â¯ğ‘¡=(1âˆ’ğ›½2)âˆ‘ğ‘¡ğ‘–=1ğ›½ğ‘¡âˆ’ğ‘–2ğ·ğ·(ğ‘ )ğ‘–ğ·ğ·(ğ‘ )ğ‘–1âˆ’ğ›½ğ‘¡2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšğ‘šğ‘¡=(1âˆ’ğ›½1)âˆ‘ğ‘¡ğ‘–=1ğ›½ğ‘¡âˆ’ğ‘–1ğ ğ‘¡,ğ‘–1âˆ’ğ›½ğ‘¡1ğ‘£ğ‘¡=(ğ·ğ·â¯â¯â¯â¯â¯ğ‘¡)ğ‘˜=((1âˆ’ğ›½2)âˆ‘ğ‘¡ğ‘–=1ğ›½ğ‘¡âˆ’ğ‘–2ğ·ğ·(ğ‘ )ğ‘–ğ·ğ·(ğ‘ )ğ‘–1âˆ’ğ›½ğ‘¡2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆš)ğ‘˜ğœƒğ‘¡=ğœƒğ‘¡âˆ’1âˆ’ğœ‚ğ‘šğ‘¡/ğ‘£ğ‘¡
(20)
Adafactor [41] did not modified the first moment of Adam, but modify the second moment with factored and accumulators. Thus, the parameter update in Adafactor is defined in Eq. (21), where ğœ–1=10âˆ’30, ğœ–2=10âˆ’3, ğœŒğ‘¡=min(10âˆ’2,1ğ‘¡âˆš), ğ‘‘=1, ğ›½Ì‚ 2ğ‘¡=1âˆ’ğ‘¡âˆ’0.8.

ğ›¼ğ‘¡=max(ğœ–2,RMS(ğ‘‹ğ‘¡âˆ’1))ğœŒğ‘¡ğ‘”ğ‘¡,ğ‘–=âˆ‡îˆ¸ğ‘¡,ğœƒğ‘£Ì‚ ğ‘¡=ğ›½Ì‚ 2ğ‘£Ì‚ ğ‘¡âˆ’1+(1âˆ’ğ›½Ì‚ 2ğ‘¡)(ğ‘”2ğ‘¡,ğ‘–+ğœ–11ğ‘›)ğ‘ˆğ‘¡=ğ‘”ğ‘¡,ğ‘–/ğ‘£Ì‚ ğ‘¡â€¾â€¾âˆšğ‘ˆÌ‚ ğ‘¡=ğ‘ˆğ‘¡/max(1,RMS(ğ‘ˆğ‘¡)/ğ‘‘)ğœƒğ‘¡=ğœƒğ‘¡âˆ’1âˆ’ğœ‚ğ‘¡ğ‘ˆÌ‚ ğ‘¡
(21)
Aggregated Momentum (AggMo) [29] significantly dampens oscillations caused by the aggressive ğ›½. This paper main contribution is improve stability even with aggressive ğ›½, through modify the gradient descent algorithm and calculate a more stable update velocity. The update velocity average the previous velocity vectors at each optimization step, and used to update the model parameters. Thus, the parameter update in AggMo is defined in Eq. (22), where ğ›¼=0.1, ğ›½(ğ‘–)=1âˆ’ğ‘ğ‘–âˆ’1.

ğ‘£(ğ‘–)ğ‘¡=ğ›½(ğ‘–)ğ‘£(ğ‘–)ğ‘¡âˆ’1âˆ’âˆ‡ğœƒğ‘“(ğœƒğœƒğ‘¡âˆ’1)ğœƒğœƒğ‘¡=ğœƒğœƒğ‘¡âˆ’1+ğœ‚ğ‘¡ğ¾âˆ‘ğ‘–=1ğ¾ğ‘£(ğ‘–)ğ‘¡
(22)
Recently, some novel articles apply fractional order to neural networks. Bao [3] advance a BP neural network with fractional-order, which was determine convergence and avoid overfitting. Kaslik [22] determine the critical value of the fractional order. The global stability analysis of the fractional-order neural network based on LMI conditions is given by Zhang [58]. Yang [51] analyzes the stability of the fractional neural network in the case of no delay and with delay is analyzed. Ding [9] studied global projective synchronization of non-identical fractional neural networks with sliding mode control.

Fractional-order stochastic classical momentum (FOSCM [21]) detailed formula derivation is carried out for the application of fractional order in the optimization. Where ğ›½, and ğ›½1, ğ›½2 is the moment for SGDM and Adam respective, ğ›¾ is the fractional order, and ğ›½<1. the parameter update is defined in Eqs. (23), (24):

ğ‘šğ‘¡,ğ‘–=(ğ›½âˆ’1+ğ›¾)ğ‘šğ‘¡âˆ’1,ğ‘–+âˆ‘ğ‘=2ğ‘˜(âˆ’1)ğ‘+1(ğ›¾ğ‘)ğ‘šğ‘¡âˆ’ğ‘,ğ‘–âˆ’ğœ‚ğ‘”ğ‘¡,ğ‘–
(23)
Adam algorithm with fractional-order momentum, the parameter update is defined as:

ğ‘šğ‘¡,ğ‘–=(ğ›½1âˆ’1+ğ›¾)ğ‘šğ‘¡âˆ’1,ğ‘–+âˆ‘ğ‘=2ğ‘˜(âˆ’1)ğ‘+1(ğ›¾ğ‘)ğ‘šğ‘¡âˆ’ğ‘,ğ‘–+(1âˆ’ğ›½1)ğ‘”ğ‘¡,ğ‘–ğ‘£ğ‘¡,ğ‘–=(ğ›½2âˆ’1+ğ›¾)ğ‘£ğ‘¡âˆ’1,ğ‘–+âˆ‘ğ‘=2ğ‘˜(âˆ’1)ğ‘+1(ğ›¾ğ‘)ğ‘£ğ‘¡âˆ’ğ‘,ğ‘–+(1âˆ’ğ›½2)(ğ‘”ğ‘¡,ğ‘–)2
(24)
In this paper, a gradient-based optimizer is proposed to improve the well-known SGD optimizer. In the iterative process, the gradient is used to improve the iteration direction. The main work of this paper is as follows:

A new method of deep neural network optimization is proposed based on the fractional order and gradient descent method, which improves the performance of the algorithm using fractional order to approximate simulation the current update vector, the coefficient of the past gradient was calculated by fractional order.

We showed how to use fractional order to approximate the update vector direction of the optimization algorithm.

Due to the exponential moving average, Adam has a convergence problem. The gradient may experience large changes, so the update vector may fluctuate instead of monotonically changing.

This method resolves saddle points and oscillations by adjusting the optimization direction in fractional order.

We conducted experimental research on the proposed FracM and observed its improved performance in image classification tasks using ResNet-based CNN architecture. For the sake of comparison, we also conducted experiments on other optimization methods.

Algorithm
According to the discussions in Section II, some existing classic optimization algorithms, such as Adam and other algorithm use exponential moving averages of squared past gradients to modify the iteration direction and update network parameters suffers from the problem of convergence. The exponential moving averages is the accumulation of past gradient within a fixed time window using a decay rate. With the change of the time window, the gradients may encounter great changes, so that momentum may fluctuate, rather than monotonically changing. This may cause a shock of learning rate in the later stage of training, resulting in the failure of convergence of the model. In this section, this paper propose a new method combine Fractional-Order and gradient to avoid the problem of convergence and reflect gradient changes in time named FracM. The proposed Fractional-Order Momentum optimization Algorithm using fractional order to approximate calculate current update vector, the coefficient of the past update vector was calculated by fractional order, the proportion of past update vector has become smaller.

Fractional order [3] has a long history of development. In the following part, we will introduce some preliminary knowledge about discrete fractional calculus(continuous fractional calculus does not apply to this article), which will lay the foundation for the discussion in the following chapters. Starting from the definition of classical calculus, the concept of fractional calculus is intuitively understood. For a continuous function ğ‘¦=ğ‘“(ğ‘¥), the classical integer order derivative formula can be defined as:

ğ‘“â€²(ğ‘¡)=ğ‘‘ğ‘“ğ‘‘ğ‘¡=limâ„â†’0ğ‘“(ğ‘¡)âˆ’ğ‘“(ğ‘¡âˆ’â„)â„
(25)
Its physical meaning can be understood as the rate of change of the above functions, while the second derivative can be expressed as:

ğ‘“â€²â€²(ğ‘¡)=ğ‘‘2ğ‘“ğ‘‘ğ‘¡2=limâ„â†’0ğ‘“â€²(ğ‘¡)âˆ’ğ‘“â€²(ğ‘¡âˆ’â„)â„=limâ„â†’0ğ‘“(ğ‘¡)âˆ’2ğ‘“(ğ‘¡âˆ’â„)+ğ‘“(ğ‘¡âˆ’2â„)â„2
(26)
Its physical meaning can be understood as the curvature of the curve or the acceleration of a physical quantity. It can be known that the third derivative of F(x) is:

ğ‘“â€²â€²â€²(ğ‘¡)=ğ‘‘3ğ‘“ğ‘‘ğ‘¡3=limâ„â†’0ğ‘“(ğ‘¡)âˆ’3ğ‘“(ğ‘¡âˆ’â„)+3ğ‘“(ğ‘¡âˆ’2â„)âˆ’ğ‘“(ğ‘¡âˆ’3â„)â„3
(27)
According to the mathematical induction, the derivative of f(x) should have the following form:

ğ‘“(ğ‘›)(ğ‘¡)=ğ‘‘ğ‘›ğ‘“ğ‘‘ğ‘¡ğ‘›=limâ„â†’01â„ğ‘›âˆ‘ğ‘Ÿ=0ğ‘›(âˆ’1)ğ‘Ÿ(ğ‘›ğ‘Ÿ)ğ‘“(ğ‘¡âˆ’ğ‘Ÿâ„)(ğ‘›ğ‘Ÿ)=ğ‘›(ğ‘›âˆ’1)(ğ‘›âˆ’2)â‹¯(ğ‘›âˆ’ğ‘Ÿ+1)ğ‘Ÿ!
(28)
Fractional order system can be regarded as the generalized form of integral order calculus, we have the following GrÃ¼nwald-Letnikov (abbreviated G-L) definition:

ğºğ¿ğ·ğ›¼ğ‘¡ğ‘“(ğ‘¡)=limâ„â†’01â„ğ›¼âˆ‘ğ‘—=0ğ‘˜(âˆ’1)ğ‘—(ğ›¼ğ‘—)ğ‘“(ğ‘¡âˆ’ğ‘—â„)
(29)
By using the properties of the Gamma function ğ›¤(), the generalized binomial coefficient equation can be simplified as follows:

(ğ›¼ğ‘—)={ğ›¤(ğ›¼+1)ğ›¤(ğ‘—+1)ğ›¤(ğ›¼âˆ’ğ‘—+1)1ğ‘—â‰¥1ğ‘=0
(30)
If the order n can be extended to any rational number, then a definition of fractional derivative is obtained. For SGD, theo update vector is as follows:

ğºğ¿ğ·ğ‘›ğ‘¡ğ‘“(ğ‘¡)=ğ‘“(ğ‘¡)+1â„ğ›¼âˆ‘ğ‘—=1ğ‘˜(âˆ’1)ğ‘—(ğ›¼ğ‘—)ğ‘“(ğ‘¡âˆ’ğ‘—â„)=ğ‘“(ğ‘¡)+ğ›¼1ğ‘“(ğ‘¡âˆ’â„)+ğ›¼1ğ‘“(ğ‘¡âˆ’2â„)+ğ›¼1ğ‘“(ğ‘¡âˆ’3â„)
(31)
ğ‘šğ‘¡+1,ğ‘–=ğ›½âˆ—ğ‘šğ‘¡,ğ‘–+ğ‘”ğ‘¡+1,ğ‘–run fractional orderğºğ¿ğ·ğ›¼ğ‘¡ğ‘šğ‘¡+1,ğ‘–=ğ›½âˆ—ğºğ¿ğ·ğ›¼ğ‘¡ğ‘šğ‘¡,ğ‘–+ğºğ¿ğ·ğ›¼ğ‘¡ğ‘”ğ‘¡+1,ğ‘–=ğ›½(ğ‘šğ‘¡,ğ‘–+ğ›¼1ğ‘šğ‘¡âˆ’2,ğ‘–+ğ›¼2ğ‘šğ‘¡âˆ’4,ğ‘–+ğ›¼3ğ‘šğ‘¡âˆ’6,ğ‘–)+(ğ‘”ğ‘¡+1,ğ‘–+ğ›¼1ğ‘”ğ‘¡âˆ’1,ğ‘–+ğ›¼2ğ‘”ğ‘¡âˆ’3,ğ‘–+ğ›¼3ğ‘”ğ‘¡âˆ’5,ğ‘–)=(ğ‘”ğ‘¡+1,ğ‘–+ğ›½ğ‘”ğ‘¡,ğ‘–+(ğ›½2+ğ›¼1)ğ‘”ğ‘¡âˆ’1,ğ‘–+(ğ›½3+ğ›½ğ›¼1)ğ‘”ğ‘¡âˆ’2,ğ‘–+(ğ›½4+ğ›½2ğ›¼1+ğ›¼2)ğ‘”ğ‘¡âˆ’3,ğ‘–+(ğ›½5+ğ›½3ğ›¼1+ğ›½ğ›¼2)ğ‘”ğ‘¡âˆ’4,ğ‘–+(ğ›½6+ğ›½4ğ›¼1+ğ›½2ğ›¼2+ğ›¼3)ğ‘”ğ‘¡âˆ’5,ğ‘–+(ğ›½7+ğ›½5ğ›¼1+ğ›½3ğ›¼2+ğ›½ğ›¼3)ğ‘”ğ‘¡âˆ’6,ğ‘–+(ğ›½8+ğ›½6ğ›¼1+ğ›½4ğ›¼2+ğ›½2ğ›¼3)ğ‘”ğ‘¡âˆ’7,ğ‘–+...)
(32)
FracM by adding a fractional order coefficient of the update vector of the past time step to the current update vector.

when ğ›¼>0, it is a fractional differential operation, and when ğ›¼<0, it is a fractional integral operation. It can be seen from the above definition that the GL definition can actually be regarded as a limit form, and with the rapid development of computer technology, we can also replace it with a numerical approximation, which means that the GL definition can actually be seen as a discrete form of definition. By a simple function ğ‘¦=1/2âˆ—ğ‘¥2, fractional derivative of a basic power function with ğ›¼=3/2, the analytical solution was ğ‘¦=3.386âˆ—ğ‘¥1/2, and the original function and second derivative and first derivative are compared in Fig. 3, it intuitively explains the relationship between derivative and fractional order.

Fractional-Order Momentum optimization Algorithm can help avoid trap into local minimum. Compare the Eq. (32) with SGDM, the coefficient of the gradient not only depends on ğ›½, but also on ğ›¼. When the optimization falls into a local minimum, the current gradient approaches 0, past gradient also close to 0 due to the coefficient with high power, therefore, it is hard to optimization. When the current gradient is close to 0, the past gradient in FracM is not close to 0 because of the fractional order coefficient.

The coefficient of the past gradient and moment was calculated by fractional order, while not through experience, Fractional coefficients speed up the optimization process and can get better results. Open-source implementation available at https://github.com/yzlicloud/FracM.

Fig. 3
figure 3
A comparison of fractional calculus and second derivative and first derivative and the original function

Full size image
figure c
Convergence analysis
Supervised learning is to find suitable neural network parameters to approximate the data of the training set. The research of optimization problem can be divided into three steps: The first is to make the algorithm run and converge to a reasonable solution, such as a saddle point. The second is to make the algorithm converge as fast as possible. The third step is to ensure that the algorithm converges to a solution ğœƒğ‘¡ with a low Loss value (e.g., the global minimum). The regret bound was the sum of ğ‘“ğ‘¡ğœƒğ‘¡âˆ’ğ‘“ğ‘¡(ğœƒâˆ—) with different t, where ğ‘“ğ‘¡(ğœƒâˆ—) was the best parameter from a feasible set ğœ’. Mathematically, the regret bound is given as:

ğ‘…(ğ‘‡)=âˆ‘ğ‘¡=1ğ‘‡[ğ‘“ğ‘¡(ğœƒğ‘¡)âˆ’ğ‘“ğ‘¡(ğœƒâˆ—)]
where ğœƒâˆ—=argminğœƒâˆˆğœ’âˆ‘ğ‘‡ğ‘¡=1ğ‘“ğ‘¡(ğœƒ). We observe that the regret bound FracM was ğ‘‚(1/ğ‘¡)+ğ‘‚(ğœ‚). The regret bound of FracM can compare with general convex online learning methods. The definition of parameters in the FracM was same as we defined above. The proof process is as follows:

A well-known conclusion in the field of deep learning is that need Lipschitz smoothing gradients. During the iterative process, the iterations are bounded, then an appropriate learning rate SGD will definitely converge. Gradient with Lipschitz constant boundary will produce a gradually decreasing sequence during the optimization process, which will converge stably, but the convergence rate cannot be guaranteed. Even if gradient with Lipschitz constant boundary, its convergence rate may be exponential.

Theorem 1
The convex cost functions ğ‘“(ğœƒ) is ğœ‡âˆ’ğ‘ ğ‘¡ğ‘Ÿğ‘œğ‘›ğ‘”ğ‘™ğ‘¦ convex, and L-smooth, ğœƒğ‘¡,ğ‘– is an unbiased estimate ofâˆ‡ğ‘“(ğœƒğœƒğ‘¡) with input.

Corollary 1
This article hypothesis that the two norm of gradient is bounded â€–â€–ğ‘”ğ‘¡,ğœƒâ€–â€–2â‰¤ğœ2. Bring the previous update vector into the following formula, the update vector satisfies Lipschitz continuity with a value of ğ›½, and the deep neural network is called ğ›½ smoothing.

For all ğœƒ, the expectations of gradient is bounded, as follow:

ğ”¼[â€–ğ‘”ğ‘¡,ğ‘–â€–22]â‰¤ğœ2
(33)
According to the Algorithm 3, the update vector of AdaFM can be derived from the equation above, and combine the past time update vector and current, here we set â„=2. The absolute value of fractional order parameter is getting smaller and smaller, means |ğ›¼1| > |ğ›¼2| > |ğ›¼3|, the other parameter is getting smaller and smaller, so it is omitted, and the update vector of the proposed algorithm can be summarized as:

ğºğ¿ğ·ğ›¼ğ‘¡ğ‘šğ‘¡+1,ğ‘–=(ğ‘”ğ‘¡+1,ğ‘–+ğ›½ğ‘”ğ‘¡,ğ‘–+(ğ›½2+ğ›¼1)ğ‘”ğ‘¡âˆ’1,ğ‘–+(ğ›½3+ğ›½ğ›¼1)ğ‘”ğ‘¡âˆ’2,ğ‘–+(ğ›½4+ğ›½2ğ›¼1+ğ›¼2)ğ‘”ğ‘¡âˆ’3,ğ‘–+(ğ›½5+ğ›½3ğ›¼1+ğ›½ğ›¼2)ğ‘”ğ‘¡âˆ’4,ğ‘–+(ğ›½6+ğ›½4ğ›¼1+ğ›½2ğ›¼2+ğ›¼3)ğ‘”ğ‘¡âˆ’5,ğ‘–+(ğ›½7+ğ›½5ğ›¼1+ğ›½3ğ›¼2+ğ›½ğ›¼3)ğ‘”ğ‘¡âˆ’6,ğ‘–+(ğ›½8+ğ›½6ğ›¼1+ğ›½4ğ›¼2+ğ›½2ğ›¼3)ğ‘”ğ‘¡âˆ’7,ğ‘–+...)
(34)
Using the SGD update rule, we have:

â€–â€–ğœƒğœƒğ‘¡+1,ğ‘–âˆ’ğœƒğœƒâˆ—â€–â€–22=â€–â€–ğœƒğœƒğ‘¡,ğ‘–âˆ’ğœ‚ğºğ¿ğ·ğ›¼ğ‘¡ğ‘šğ‘¡,ğ‘–âˆ’ğœƒğœƒâˆ—â€–â€–22=â€–â€–ğœƒğœƒğ‘¡,ğ‘–âˆ’ğœƒğœƒâˆ—â€–â€–22âˆ’2ğœ‚(ğœƒğœƒğ‘¡,ğ‘–âˆ’ğœƒğœƒâˆ—)âŠ¤ğºğ¿ğ·ğ›¼ğ‘¡ğ‘šğ‘¡,ğ‘–+ğœ‚2â€–â€–ğºğ¿ğ·ğ›¼ğ‘¡ğ‘šğ‘¡,ğ‘–â€–â€–22
(35)
Furthermore, strong convexity gives:

ğœ‡â€–â€–ğœƒğœƒğ‘¡,ğ‘–âˆ’ğœƒğœƒâˆ—â€–â€–22â‰¤ğ‘”ğ‘‡ğ‘¡,ğ‘–(ğœƒğ‘¡,ğ‘–âˆ’ğœƒâˆ—)
(36)
Combine Eqs. (31), (32), (33) and (34) to obtain:

ğ”¼[â€–â€–ğœƒğœƒğ‘¡+1,ğ‘–âˆ’ğœƒğœƒâˆ—â€–â€–22]â‰¤(1âˆ’2ğœ‡ğœ‚((1+ğ›½)(1+ğ›¼2+ğ›¼3)+(ğ›¼1ğ›½4+ğ›¼2ğ›½2)(1+ğ›½+ğ›½2)+ğ›¼3ğ›½2+ğ›½2(2+2ğ›½+ğ›½2+ğ›½3+ğ›½4+ğ›½5+ğ›½6)))âˆ—ğ”¼[â€–â€–ğœƒğœƒğ‘¡,ğ‘–âˆ’ğœƒğœƒâˆ—â€–â€–22]+ğœ‚2((1+ğ›½)(1+ğ›¼2+ğ›¼3)+(ğ›¼1ğ›½4+ğ›¼2ğ›½2)(1+ğ›½+ğ›½2)+ğ›¼3ğ›½2+ğ›½2(2+2ğ›½+ğ›½2+ğ›½3+ğ›½4+ğ›½5+ğ›½6))2ğœ2
(37)
By convexity of f, we have:

ğ”¼[ğ‘“ğ‘¡(ğœƒğ‘¡)âˆ’ğ‘“ğ‘¡(ğœƒâˆ—)]â‰¤ğ”¼[ğ‘”ğ‘‡ğ‘¡,ğ‘–(ğœƒğ‘¡,ğ‘–âˆ’ğœƒâˆ—)]
(38)
This together with Eq. (33) implies:

2ğœ‚((1+ğ›½)(1+ğ›¼2+ğ›¼3)+(ğ›¼1ğ›½4+ğ›¼2ğ›½2)(1+ğ›½+ğ›½2)+ğ›¼3ğ›½2+ğ›½2(2+2ğ›½+ğ›½2+ğ›½3+ğ›½4+ğ›½5+ğ›½6))ğ”¼[ğ‘“ğ‘¡(ğœƒğœƒğ‘¡)âˆ’ğ‘“ğ‘¡(ğœƒğœƒâˆ—)]â‰¤ğ”¼[â€–â€–ğœƒğœƒğ‘¡,ğ‘–âˆ’ğœƒğœƒâˆ—â€–â€–22]âˆ’ğ”¼[â€–â€–ğœƒğœƒğ‘¡+1,ğ‘–âˆ’ğœƒğœƒâˆ—â€–â€–22]+ğœ‚2((1+ğ›½)(1+ğ›¼2+ğ›¼3)+(ğ›¼1ğ›½4+ğ›¼2ğ›½2)(1+ğ›½+ğ›½2)+ğ›¼3ğ›½2+ğ›½2(2+2ğ›½+ğ›½2+ğ›½3+ğ›½4+ğ›½5+ğ›½6))2ğœ2
(39)
Sum over ğ‘˜=0,...,ğ‘¡ to obtain:

âˆ‘ğ‘˜=0ğ‘¡2ğœ‚((1+ğ›½)(1+ğ›¼2+ğ›¼3)+(ğ›¼1ğ›½4+ğ›¼2ğ›½2)(1+ğ›½+ğ›½2)+ğ›¼3ğ›½2+ğ›½2(2+2ğ›½+ğ›½2+ğ›½3+ğ›½4+ğ›½5+ğ›½6))âˆ—ğ”¼[ğ‘“(ğœƒğœƒğ‘¡)âˆ’ğ‘“(ğœƒğœƒâˆ—)]â‰¤ğ”¼[â€–â€–ğœƒğœƒ0âˆ’ğœƒğœƒâˆ—â€–â€–22]âˆ’ğ”¼[â€–â€–ğœƒğœƒğ‘¡+1âˆ’ğœƒğœƒâˆ—â€–â€–22]+ğœ2âˆ‘ğ‘˜=0ğ‘¡ğœ‚2â‰¤ğ”¼[â€–â€–ğœƒğœƒ0âˆ’ğœƒğœƒâˆ—â€–â€–22]+((1+ğ›½)(1+ğ›¼2+ğ›¼3)+(ğ›¼1ğ›½4+ğ›¼2ğ›½2)(1+ğ›½+ğ›½2)+ğ›¼3ğ›½2+ğ›½2(2+2ğ›½+ğ›½2+ğ›½3+ğ›½4+ğ›½5+ğ›½6))2ğœ2âˆ‘ğ‘˜=0ğ‘¡ğœ‚2
(40)
the proposed FracM optimizer shows the follwing guarantee, where ğ‘˜=(1+ğ›½)(1+ğ›¼2+ğ›¼3)+(ğ›¼1ğ›½4+ğ›¼2ğ›½2)(1+ğ›½+ğ›½2)+ğ›¼3ğ›½2+ğ›½2(2+2ğ›½+ğ›½2+ğ›½3+ğ›½4+ğ›½5+ğ›½6):

ğ”¼[ğ‘“(ğœƒğœƒğ‘¡)âˆ’ğ‘“(ğœƒğœƒâˆ—)]â‰¤ğ”¼[â€–ğœƒğœƒ0âˆ’ğœƒğœƒâˆ—â€–22]2ğ‘˜+ğ‘˜ğœ2âˆ‘ğ‘¡ğ‘˜=0ğœ‚22âˆ‘ğ‘¡ğ‘˜=0ğœ‚
(41)
Corollary 1
If ğœ2 = 0, then we could use a constant step-size and would get a O(1/t) rate. Same as regular gradient descent. But due to stochasticity, convergence rate is determined by âˆ‘ğ‘˜ğœ‚2/ğœ‚. convergence rate O(1/t) with diminishing stepsize ğœ‚ğ‘¡â‰ˆ1/ğ‘¡. This article uses constant step-sizes: set ğœ‚ğ‘˜=ğœ‚ for some ğœ‚. Gives âˆ‘ğ‘˜ğœ‚=ğ‘˜ğœ‚ and âˆ‘ğ‘˜ğœ‚2=ğ‘˜ğœ‚2, so the regert bound at t is ğ‘‚(1/ğ‘¡)+ğ‘‚(ğœ‚)

The parameter update of FracM algorithm was same as mini-batch SGD in essential. The proposed FracM just a little more processing on the gradient and moment during the optimization. Thus, the time complexity of FracM is consistent with mini-batch SGD. The comparison of time complexity among classical optimization algorithm SGDM, Adam and proposed FracM is shown in Table 5.

Table 1 The comparison results in terms of the â€˜Test Classification Accuracyâ€™ over the CIFAR10 databases among classical first order optimization algorithm SGDM, AdaGrad, RMSProp, Adam, RAdam, DiffGrad, Adabelief and proposed FracM optimization methods
Full size table
Table 2 The comparison results in terms of the â€˜Test Classification Accuracyâ€™ over the CIFAR100 databases among classical first order optimization algorithm SGDM, RMSProp, Adam, RAdam, DiffGrad, Adabelief and proposed FracM optimization methods
Full size table
Table 3 The comparison results in terms transformer-based models with Natural language processing textual datasets among classical first order optimization algorithm SGDM, Adam and proposed FracM optimization methods
Full size table
Table 4 The video memory occupation in terms of the CIFAR10/100 databases among classical first order optimization algorithm SGDM, Adam, Radam, Adabelief and proposed FracM optimization methods
Full size table
Table 5 The comparison of Time Complexity among classical first-order optimization algorithm SGDM, Adam and proposed FracM optimization methods
Full size table
Experiments
The purpose of this section is to test our proposed algorithm and compare with existing optimization methods. First introduced the network architecture, then the hyperparameter settings, and introduced the data set used.

Deep architecture used
As a part of the field of artificial intelligence research, deep neural network is currently the most popular one is deep convolutional neural network. At present, cnn-based models have achieved great success in many research fields, although the problems faced by these fields are very different. The reason for the success of these tasks is that cnn can automatically learn features from (usually large-scale) data, and can predict well when faced with the same type of unknown data. At present, the combination of CNN and Resnet is the most popular network structure [16], and has achieved good results in many image recognition competitions [39]. The residual block inside the residual network uses jump connection to alleviate the problem of the gradient disappearance caused by the increase in depth in the deep neural network. The experiment in this article uses a residual network, and the depth of the residual network is set to 50 and 110.

Sequence-to-sequence model is a variant of recurrent neural network, including Encoder and Decoder. Seq2Seq is an important model in natural language processing, which can be utilized in machine translation, dialogue systems, and automatic summarization. Transformer [49] was a sequence-to-sequence model in essence, where an encoder on the left reads the input and a decoder on the right gets the output. The core of transformer is attention, and using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.

Hyper-parameter setting
In this article, no matter the optimization algorithm, data processing, network structure, etc., we use the default parameter settings. Since no other techniques are used to improve the accuracy, the training results are not as high as other literatures. The results are all comparative experiments under the same conditions, so the proposed algorithm is meaningful in terms of stability and performance.

The batch size of the experiment mainly includes 32,64,128. The learning rate for the first 60 epochs is ğœ‚=0.2, the learning rate for the next 20 epochs is ğœ‚=0.02, and the learning rate for the last 20 epochs is ğœ‚=0.002. The hyperparameters of the comparison experiment adopt the default parameters, and adopts the same learning rate decay strategy as FracM optimization.

Fig. 4
figure 4
The classification accuracy comparison between the proposed FracM algorithm and some classical algorithm and some recently proposed algorithms about cifar10 dataset

Full size image
Fig. 5
figure 5
The classification loss comparison between the proposed FracM algorithm and some classical algorithm and some recently proposed algorithms about cifar10 dataset

Full size image
Fig. 6
figure 6
The classification accuracy comparison between the proposed FracM algorithm and some classical algorithm and some recently proposed algorithms about cifar100 dataset

Full size image
Fig. 7
figure 7
The classification loss comparison between the proposed FracM algorithm and some classical algorithm and some recently proposed algorithms about cifar100 dataset

Full size image
Fig. 8
figure 8
The classification accuracy comparison between the proposed FracM algorithm and variations of the algorithm (with different parameter ğ›½) about cifar10 dataset

Full size image
Fig. 9
figure 9
The classification loss comparison between the proposed FracM algorithm and variations of the algorithm(with different parameter ğ›½) about cifar10 dataset

Full size image
Fig. 10
figure 10
The classification accuracy comparison between the proposed FracM algorithm and variations of the algorithm (with different parameter ğ›½) about cifar100 dataset

Full size image
Fig. 11
figure 11
The classification loss comparison between the proposed FracM algorithm and variations of the algorithm (with different parameter ğ›½) about cifar100 dataset

Full size image
Fig. 12
figure 12
The classification accuracy comparison between the proposed FracM algorithm and variations of the algorithm (with different fractional order parameter ğ›¼) about cifar10 dataset

Full size image
Fig. 13
figure 13
The classification loss comparison between the proposed FracM algorithm and variations of the algorithm (with different fractional order parameter ğ›¼) about cifar10 dataset

Full size image
Fig. 14
figure 14
The classification accuracy comparison between the proposed FracM algorithm and variations of the algorithm (with different fractional order parameter ğ›¼) about cifar100 dataset

Full size image
Fig. 15
figure 15
The classification loss comparison between the proposed FracM algorithm and variations of the algorithm (with different fractional order parameter ğ›¼) about cifar100 dataset

Full size image
Dataset used
This article uses two classic data sets CIFAR10 and CIFAR100 [24], as the data sets for the verification algorithm. As a test data set for many innovative algorithms, which is used to verify the training speed and training accuracy of the algorithm. In the CIFAR10 data set, all images are divided into 10 categories, while the CIFAR100 data set, all images are divided into 100 categories. The size of all images is 32Ãƒâ€”32Ãƒâ€”3. All images are normalized, which means the mean value of the three RGB channels is zero, and the variance is the unit standard deviation. The image is simply flipped and cropped, and only randomly flipped horizontally with the default probability.

The IMDB Dataset consist of 25,000 movies reviews, which was labeled by sentiment (positive/negative), and each review is encoded as a sequence of word indexes(integers). For convenience, words are indexed by overall frequency in the dataset, integer â€œ3â€ represents the third most frequently used word in the encoded data. This allows for quick filtering operations such as:â€œonly consider the top 10000 most common words, but eliminate the top 20 most common wordsâ€. As a convention,â€œ0â€ does not stand for a specific word. but instead is used to encode any unknown word.

GloVe(glove.42B.300d) is a word representation tool based on count-based and overall statistics. It can express a word as a vector composed of real numbers. These vectors capture some semantic features between words, such as similarity, analogy, etc. It can calculate the semantic similarity between two IMDB dataset words through operations on vectors, such as Euclidean distance or cosine similarity.

Experiments and analysis
The image classification experimental results are shown in Tables 1 and 2. Table 1 describes the classification accuracy of different optimization algorithms on the test set. The experimental results are based on the CIFAR10 dataset based on the 50-layer deep residual network model. Among several optimization algorithms, the best results are shown in bold. It can be seen from Table 1 that the test batch sizes are 32, 64 and 128, respectively. The performance of the FracM optimization algorithm proposed in this paper is significantly better than other optimization algorithms.

Figures 4 and 5 show the detailed validation statistics by the FracM and some classical methods on CIFAR10 with RseNet50. One can see that FracM optimizer converges faster than classical methods with lower loss and higher accuracy.

Table 2 describes the classification accuracy of different optimization algorithms on the CIFAR100 dataset based on the 110-layer deep residual network model. The basic parameters are consistent with Table 1. Our proposed algorithm can reduce the influence of the exponential moving average of the gradient square. Our FracM method can effectively prevent the network from generating noise and oscillation near the minimum. Compared with other optimization algorithms, our algorithm can get more accurate results (Tables 3, 4, 5).

Figures 6 and 7 show the detailed validation statistics by the FracM and some classical methods on CIFAR100 with RseNet50. One can see that FracM optimizer converges faster than classical methods with lower loss and higher accuracy.

Figures 8 and 10 show the detailed validation accuracy by the FracM and variations of the algorithm with different ğ›½ about CIFAR10/100 with RseNet50. Figures 9 and 11 show the detailed validation loss by the FracM and variations of the algorithm with different ğ›½ about CIFAR10/100 with RseNet50. FracM1 stands for ğ›½=0.5, FracM2 stands for ğ›½=0.6, FracM3 stands for ğ›½=3, FracM4 stands for ğ›½=5. It can be noticed from this result that the original FracM performs better over CIFAR10/100 dataset. When the parameter beta gradually increase, validation accuracy will gradually decrease and oscillate. It is clear that original FracM as well as its different fractional coefficients based variants show the promising performance.

Figures 12 and 14 show the detailed validation accuracy by the FracM and variations of the algorithm with different fractional order parameter ğ›¼ about CIFAR10/100 with RseNet50. Figures 13 and 15 show the detailed validation loss by the FracM and variations of the algorithm with different ğ›¼ about CIFAR10/100 with RseNet50. FracM1 stands for ğ›¼=0.2, FracM2 stands for ğ›¼=0.5, FracM3 stands for ğ›¼=2.5, FracM4 stands for ğ›¼=4.2. It can be noticed from this result that the original FracM performs better over CIFAR10/100 dataset. When the fractional order parameter ğ›¼ gradually increase, validation accuracy will gradually increase then stabilize. When the parameter ğ›¼ large, it is easy to oscillate. It is clear that original FracM as well as its different fractional coefficients based variants show the promising performance.

The defect of the algorithm is that the parameters in the FracM optimization algorithm are calculated through the fractional order, which affects the circulation of the algorithm. However, the default parameters of the FracM optimization algorithm show the promising performance.

Conclusion
In this work, we propose a new optimization method FracM based on fractional order. Fractional order calculus inherits almost all the characteristics of integral calculus, and have some memorization and nonlocality. The coefficient of the gradient not only depends on the moment, but also depend on fractional order, which help avoid falls into a local minimum. When the current gradient is close to 0, the past gradient in FracM is not close to 0 because of the fractional order coefficient. When the parameter moment gradually increases, validation accuracy will gradually decrease and oscillate. It means that moment should not occupy a large proportion. When the parameter fractional order gradually increases, validation accuracy will gradually decrease and oscillate. It means that moment should not occupy a large proportion. When the fractional order gradually increases, validation accuracy will gradually increase then stabilize. When a fractional order is large, fractional coefficient will drop rapidly, the past gradient in FracM is almost close to 0, the effect of memorization and nonlocality will gradually fade. The convergence of the algorithm can be proved through the analysis of regret bound. It is clear that original FracM as well as its variants outperform other state of the art optimization methods (such as SGDM, AdaGrad, AdaDelta, RMSProp, AMSGrad, and Adam) on the image classification dataset CIFAR10/100 and textual datasets IMDB with transformer-based models.

Keywords
Deep neural networks
Optimization
Gradient descent
Fractional-order
Residual network
Image classification