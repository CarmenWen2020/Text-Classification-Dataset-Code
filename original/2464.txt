The fast development of video technology and hardware has led to a great amount of video applications in the field of industry, such as the video conference, video surveillance and live video streaming. Most of the applications are facing with the problem of limited network bandwidth while try to keep high video resolution. Perceptual video compression addresses the problem by introducing saliency information to reduce the perceptual redundancy, which retains more information in salient region while compresses the non-salient region as much as possible. In this paper, in order to combine the multi-scale information in video saliency, an advanced video saliency detection model SALDPC is proposed which is built based on deformable pyramid convolution decoder and multi-scale temporal recurrence. In order to better guide video coding through saliency, based on the HEVC video coding standard, a saliency-aware rate-distortion optimization algorithm, SRDO, is proposed using block-based saliency information to change the rate-distortion balance and guide more reasonable bit allocation. Furthermore, a more flexible QP selection method, SAQP, is developed using CU's saliency information adaptively to change the QP of CU to ensure the high quality of the high saliency areas. The final results are available in the following three configurations: SRDO, SRDO + SQP, and SRDO + SAQP. Experimental results show that our method achieves a very high video quality improvement while significantly reducing the video encoding time. Compared to HEVC, the BD-EWPSNR of the SRDO method improves 0.703 dB, and the BD-Rate based on EWPSNR saves 20.822%; the BD-EWPSNR of the SRDO + SAQP is improved up to 1.217 dB, and the BD-Rate based on EWPSNR further saves up to 32.41%. At the same time, in terms of compression time, the proposed method saves up to 29.06% compared to HEVC. Experimental results show the superiority of the proposed method in comparison to the state-of-the-art methods.

Introduction
With the emergence of industrial video applications such as ultrahigh definition television, digital TV, smartphone, and webcast, more high-definition and ultrahigh-definition videos are being transmitted and stored on streaming media [1,2,3]. The quantity of data produced by these video applications is very large. Therefore, it is necessary to develop video compression standards to reduce the volume of video. Based on these requirements, high efficiency video coding (HEVC) has been proposed as an advanced video coding standard.

In comparison to past standards, HEVC significantly improves compression performance. It achieves efficient video compression by reducing statistical redundancy and a small amount of perceptual redundancy in the video [4, 5]. It adopts more complex coding modes in the intra-prediction and inter-prediction to reduce statistical redundancy and then combines the entropy coding to achieve a great improvement from past standards. However, HEVC also brings considerable coding complexity, and it is difficult to further improve video compression performance by using conventional techniques to reduce statistical redundancy [6].

Recently, according to research on the human visual system (HVS), it was found that the human eyes only focus on objects within a limited coverage of angles of view, and the perception of the scene beyond this field of view is blurred, the character of the human eyes is called visual saliency. Such a character of the human eye is not applied to existing video compression standards. The rational use of the visual characteristics of the human eyes can further alleviate the network bandwidth stress in the industrial video applications while improving the perceptual quality of the video [7].

The main contributions of this paper are listed as follows:

(1)
We propose a model SALDPC for video saliency based on multi-scale features and time information for the first time. SALDPC adopts the Res2Net as the encoder and then captures temporal information through the multi-scale temporal recurrence. The feature pyramid network (FPN) is used as the neck to obtain more semantic feature maps. Deformable pyramid convolution (DPC) integrates scale, space, and time for the feature alignment and fusion, enabling accurate saliency detection.

(2)
Based on the obtained video saliency, a saliency-aware rate-distortion optimization algorithm (SRDO) is proposed. A sophisticated logarithmic function is used to guide a bit allocation in video compression, achieving efficiency while considering the perception of the human visual system.

(3)
Based on the adaptive quantization method (AQP) in HEVC, we propose a new saliency-aware QP selection algorithm (SAQP). Our method achieved the outperforming performance when combined with the perceptual rate-distortion optimization algorithm. BD-EWPSNR and BD-Rate based on the EWPSNR perform significantly better than other compared algorithms while greatly saving the video compression time.

The main structure of this paper is as follows:

In Sect. 2, the related works in video saliency detection and perceptual compression are reviewed. In Sect. 3, an overview of the proposed framework is presented. In Sect. 4, the model SALDPC is proposed here and its testing results on HD video are discussed in detail. In Sect. 5, the saliency-aware rate-distortion optimization algorithm (SRDO) and the flexible QP selection algorithm (SAQP) are discussed in detail. In Sect. 6, the experiments on the comparison between the method proposed in this paper with other state-of-the-art methods are introduced in detail. The last Sect. 7 summarizes all the work of this paper.

Related works
The perceptual video compression approach proposed in this paper mainly combines with the research on two aspects: video saliency detection and saliency-aware video compression. The following section is a summary of the research progress in these two directions.


A.
Video Saliency Modeling

Video saliency detection consists of multiple directions, which fall into two main categories: fixation prediction and salient object detection in video. Fixation prediction is based on the fixation points of the multi-person observation image collected by the eye tracker in which the model predicts the probability of the human eyes focusing on each pixel in the image. Salient object detection primarily finds salient objects in an image or video and distinguishes them from the background. Since the perceptual compression focuses on the various emphasis degrees of the human eyes on each region, the fixation prediction is adopted to model the video saliency and is also used to improve video compression. The model can be divided into the following three categories [25].


(1)
Classic saliency models

The classic saliency models, also known as the bottom-up saliency models, mainly use manual features for saliency modeling. The earliest saliency detection model was proposed by Itti et al. [8], and the model extracts the color, edge, and orientation features of the image and extends them to multiple scales to calculate contrast for saliency modeling. Based on the Itti model, Schölkopf et al. [9] used a graph-based approach that gained better feature metrics. Hou et al. [10] proposed a spectral residual method to analyze the logarithmic spectrum of an image and achieved good saliency modeling with only a few lines of code. However, the above-mentioned classic models cannot meet the complex requirements of the video scene due to their limited precision. Most of them are not even as good as a center-prior model [11].


(2)
Deep static models

Deep Gaze I [12] is a deep CNN that uses the pretrained AlexNet on ImageNet for saliency detection. Based on deep Gaze I, deep Gaze II [13] further explores the contribution of high-level and low-level features to saliency. Huang et al. proposed the SALICON [14], which uses two pretrained CNNs to process the images with different scales and then combine them to obtain the final saliency map. DeepFix [15] is the first fully convolutional network for visual saliency detection, it is designed to acquire scene information at multiple scales and combine global scene information with a network layer with a large accepted domain. Pan et al. [16] proposed a shallow CNN and a deep CNN to predict image saliency, achieving high precision. Subsequently, Pan et al. [17] proposed using generative adversarial networks to conduct saliency detection, the generator produces the saliency map after saliency prediction, and the discriminator distinguishes the saliency map from the ground truth used for training. Wang [18] proposed a DVA model, constructed a network using an encoder and a decoder structure, and trained to predict saliency on multiple scales.


(3)
Dynamic models

The dynamic model mainly uses time clues to effectively model video saliency. The traditional dynamic model mainly predicts saliency by directly introducing time information and combining spatial information for saliency prediction. Itti et al. [19] added two temporal features of scintillation and motion to predict the video saliency based on the proposed image saliency model. Subsequently, it used the Bayesian statistical model to model video saliency by comparing the posterior information and the prior information [20]. Guo et al. [21] used two color features of the video, the brightness feature and the motion feature to predict video saliency. Fang et al. [22] proposed a flexible framework for video saliency detection that divides the video saliency into a combination of image saliency and video’s temporal features and uses uncertainty to estimate the confidence of saliency, obtaining a good effect on saliency detection. Xu et al. [23] proposed a novel method that uses the HEVC standard to classify the depth of the CU, the motion vector and bit allocation contained in the compression domain into nine features which can be used to train the support vector machine. Later, with the creation of large-scale video eye-tracking datasets, deep dynamic models gradually emerged. Jiang et al. [24] proposed an object-to-motion CNN model, which contains two subnets, the object and motion information for predicting the video intra-frame saliency, and inter-frame saliency is achieved by the ConvLSTM structure. Wang et al. [25, 26] proposed an attentive CNN-LSTM network to achieve efficient video spatiotemporal saliency modeling.


B.
Saliency-Aware Video Compression

Saliency-aware video compression is mainly divided into two categories: preprocessing before compression and embedded video compression.

The preprocessing method first preprocesses the video frame and then compresses the processed video frame using the compression standard. Itti et al. [27] used a virtual fovea to detect salient objects in the video, and then Gaussian smoothing was conducted in the non-salient background regions to implement preprocessing. Similarly, Cavalano et al. [28] used an algorithm to divide the input frame into the foreground and the background, and the foreground is considered to be salient, the preprocessing was performed by spatially blurring the background region. Zünd et al. [29] proposed a method based on image retargeting, which preprocesses by using nonlinear image scaling to assign different numbers of pixels to salient and non-salient regions. Karlsson et al. [30] conducted filtering using a spatiotemporal filter controlled by a quality map in the background region, the border effects were reduced while achieving compression of the motion vector and reducing its prediction error.

The embedded video compression method mainly and directly integrates the video saliency information into the video encoder to effectively improve the subjective quality of the compressed video. A video compression macroblock-level adaptive bit allocation algorithm based on visual attention was proposed in [31], by adding the weight of the attention to the distortion term, the adjustment strategy of the quantization step (Qstep) was derived. The algorithm in [31] was used by [32], and the bitrate’s decrease was deduced by the formula, and the improved visual saliency detection algorithm obtained better performance. A saliency-aware rate-distortion optimization algorithm was proposed in [33], which combined the saliency deviation before and after the compression into the calculation of the rate-distortion cost, the idea is that the high saliency region allows the saliency to be improved after compression, and the low saliency region allows the saliency to be reduced after compression. The above methods are all based on the H.264 video compression standards.

In the HEVC/H.265 video compression standard, Xu et al. [34] used a new HP model for realizing the facial features in HD session video, which can adaptively divide the CTU for ROI and effectively reduce coding time. Li et al. [35] proposed a perceptual rate control algorithm for session video that detects the face region in the video and uses the bpw to replace the bpp for bit allocation at the CTU level. A saliency-aware QP search algorithm was proposed in [36], which searches the different QP offsets in different CUs and then assigns the different bits to different saliency regions. A saliency-aware rate control scheme was used in [37], in which the saliency combined with feature similarity and the relative detection accuracy was utilized to obtain weights, and weights were added into the original weights for CTU bit allocation to guide the compression of the surveillance video. [38] proposed a novel method: the relative weight of the distortion in the rate-distortion cost is changed by saliency, the distortion weight of the salient region is increased, and then, perceptual compression is performed. The basic ideas of these methods are to assign the higher bits to the salient area to maintain their perceptual high quality.

In the video saliency, although past works have adopted deep learning-based methods to train and extract time information on large datasets, it is better than traditional methods, it lacks the use of multi-level features, that is, multi-level features are extracted and time information is merged at the same time. And empirically speaking, multi-level features are important for video saliency detection. For example, some large objects may be salient, which are captured from the deeper layers with relatively large receptive fields. Some small but moving at a high speed objects are also salient, which are captured from shallower layers holding more low-level information.

In the perceptual video compression. Firstly, because large saliency datasets have only recently appeared, the existing methods do not use high-precision video saliency models to guide video compression, the effect of perceptual compression will be poor. Secondly, most of the existing work uses rate control algorithms to allocate bitrates to salient areas, in this way, the rate control switch needs to be turned on to ensure a certain bitrate during the video compression process. The rate distortion optimization and quantization parameter (QP) are factors that directly affect video compression quality. By reasonably modifying the rate-distortion optimization process according to the rate-distortion curve and setting an appropriate QP value, the entire coding process can be directly modified without rate control, and the efficiency is higher.

Different from the previous method, for the first time, we directly guide video compression through the purely neural network of video saliency. Existing saliency networks [24, 25] usually only use deep features for saliency prediction, our video saliency network builds a feature pyramid and combines multi-scale spatio-temporal features to detect video saliency through temporal recurrence for the first time, and feature alignment is performed through deformable pyramid convolution decoder. SALDPC produces high-precision video saliency maps, through these saliency maps, we design an extremely simple and efficient SRDO algorithm and SAQP algorithm, which greatly improves the video compression effect and reduces the video compression time.

Overview of the proposed framework
The overall block diagram of the method proposed in this paper is shown in Fig. 1. Considering the multi-scale features in video saliency, the video saliency detection network SALDPC is proposed. Firstly, the Res2Net is used to extract the static image features; Secondly, the multi-scale temporal recurrence module is used to fuse the temporal features of video frames at different scales; Thirdly, the semantic features of each feature map are enhanced through the feature pyramid network (FPN); Finally, the interaction of scale, space and time are performed through the deformable pyramid convolution (DPC) decoder.

Fig. 1
figure 1
Overall framework of the proposed saliency-aware video compression algorithm

Full size image
Additionally, a saliency-aware rate-distortion optimization algorithm (SRDO) is proposed, and the compression results exceeded all the current state-of-the-art methods. Furthermore, combined with the previous structure, a QP selection method guided by the saliency (SAQP) is proposed, which can further improve the HD video compression efficiency and is obviously better than HEVC (low delay P (LDP)), its optimization (multiple QP optimization (MQP)) and the latest saliency-aware HEVC perceptual compression algorithm. Using the method proposed in this paper, the BD-EWPSNR is greatly improved, the BD-Rate based on the EWPSNR drops sharply, and considerable video compression time is saved.

Video saliency algorithm based on a neural network

A.
Overall Structure

The common neural networks for video saliency consist of the encoder and decoder. The whole network structure is shown in Fig. 2. Considering that the existing methods lack the use of multi-scale features in video saliency, we combine multi-scale information in our network. In this paper, the pre-trained Res2Net-50 network on ImageNet is used as the encoder to extract the static image features. We combine the multi-scale features obtained by the encoder with the motion information through the temporal recurrence module, and then obtain the feature pyramid through the feature pyramid network (FPN), and the feature pyramid interacts with each other through scale, space and time through the deformable pyramid convolution (DPC) decoder, to simultaneously capture the changes in saliency with space, time and scale.

Fig. 2
figure 2
The overall structure of the proposed video saliency prediction network

Full size image

(1)
Res2Net-50 Backbone

For the image classification tasks, ResNet [39] performs well. In order to enhance the feature extraction ability of the ResNet, Res2Net [40] adds small residual blocks to the original residual unit structure, increasing the size of the receptive field of each layer. The structure of the Res2Net module is shown in Fig. 3.

Fig. 3
figure 3
The structure of the Res2Net module

Full size image
(2)
Temporal Recurrence

Since video saliency is affected by the motion information of the objects in the video, we need to capture the changes in the frames before and after the video. This module is called temporal recurrence. Studies in [41] have shown that exponential moving average can approximately reach the effect or even surpass the effect of the ConvLSTM, and exponential moving average is easier to be trained. Meanwhile, multi-scale features are essential for the task of saliency detection, some large objects may be salient, which are captured from the deeper layers with a relatively large receptive field. Small but moving at a high speed objects are also salient, which are captured from shallower layers holding more low-level information. So, we pass the four feature maps with different scales output by the Res2Net encoder through the temporal recurrence module with shared parameters to simultaneously integrate time information on multi-scale features, temporal recurrence uses exponential moving average. The calculation formulas are set out as follows:

e𝑡𝑙=(1−𝜎(𝛼))⋅e𝑡−1𝐼+𝜎(𝛼)⋅x𝑡𝑙
(1)
where x represents the input of four feature maps, l represents the feature map level, the range is [1, 4], t represents the sequence number of the frame, α is the learnable parameter, e represents the state to be passed to the next layer, σ(·) is the sigmoid activation function. The structure of the exponential moving average is shown in Fig. 4.

(3)
Feature Pyramid Network (FPN)

Fig. 4
figure 4
The structure of the exponential moving average

Full size image
FPN [42] enhances the multi-scale features output by the Res2Net and temporal recurrence, so that each layer of features has deep semantic information, which can facilitate subsequent cross-scale convolution. The structure of the FPN is shown in Fig. 5.

Fig. 5
figure 5
The structure of the FPN

Full size image
(4)
Deformable Pyramid Convolution (DPC) Decoder

The multi-scale semantic feature maps output by the FPN lacks scale interaction. In order to enable the decoder to fully contain multi-scale information, the pyramid convolution is adopted to perform scale and space joint convolution on adjacent feature maps. The calculation is:

𝑦𝑙=𝑤1(𝑠=0.5)∗𝑥𝑙+1+𝑤0(𝑠=1)∗𝑥𝑙+𝑤−1(𝑠=2)∗𝑥𝑙−1
(2)
where, s represents the stride of the convolution operation, and l represents the level of feature maps by the FPN. In order to make the convolution operation of s = 0.5 feasible, convert the convolution of s = 0.5 to the convolution of s = 1 to process first, and then up sample 2 times:

𝑦𝑙=Upsample(𝑤1(𝑠=1)∗𝑥𝑙+1)+𝑤0(𝑠=1)∗𝑥𝑙+𝑤−1(𝑠=2)∗𝑥𝑙−1
(3)
We call a pyramid convolution layer the PC module, which is weight-sharing. In order to accurately locate the salient objects and capture the shape changes of the objects, the convolutions are set here as deformable convolutions [43], so the module is called as deformable pyramid convolution (DPC). Each feature map passes through an independent convolutional layer and an upsampling layer after each DPC, until the feature map is restored to the original image resolution. Finally, all the feature maps being treated by the DPC decoder are added, and the saliency map is obtained through two convolutional layers and a sigmoid layer. The structure of the DPC decoder is shown in Fig. 6, where P1–P5 represent 5 feature maps output by the FPN.

Fig. 6
figure 6
The structure of DPC decoder

Full size image

(5)
Loss Function

The training of the saliency network is a regression problem, and its purpose is to make the predicted saliency map as close as possible to the real saliency map. The evaluation metrics of the saliency map are diversified, and each evaluation metric describes the quality of the saliency modeling from different aspects. Therefore, the weighted sum of the three most commonly used evaluation metrics is used here as the loss function. Assuming that the predicted saliency map is S ∈ [0,1], the binary fixation map is F ∈ {0,1}, and the ground truth saliency map generated by the fixation map is G ∈ [0,1]. Then, the final loss function can be expressed as:

𝐿(𝑆,𝐹,𝐺)=𝐿𝐾𝐿(𝑆,𝐺)+𝛼1𝐿𝐶𝐶(𝑆,𝐺)+𝛼2𝐿𝑁𝑆𝑆(𝑆,𝐹)
(4)
In addition, 𝛼1=0.5, 𝛼2=0.1, 𝐿KL, 𝐿CC and 𝐿NSS represent the Kullback–Leibler (KL) divergence, the linear correlation coefficient (CC), and the normalized scan path saliency (NSS), respectively, their calculation formulas are set out as follows:

𝐿𝐾𝐿(𝑆,𝐺)=∑𝑥𝐺(𝑥)𝑙𝑛(𝐺(𝑥)𝑆(𝑥))
(5)
𝐿𝐶𝐶(𝑆,𝐺)=−𝑐𝑜𝑣(𝑆,𝐺)𝜌(𝑆)𝜌(𝐺)
(6)
𝐿𝑁𝑆𝑆(𝑆,𝐹)=−1𝑁∑𝑥𝑆(𝑥)−𝜇(𝑆(𝑥))𝜌(𝑆(𝑥))𝐹(𝑥)
(7)
where ∑𝑥(∗) represents summing all the pixels, cov(*) represents the covariance, μ(*) represents the mean, ρ(*) represents the variance.

B.
Network Training and Test Results

The network is initialized with pre-trained Res2Net-50 weights on ImageNet, pre-train 28 epochs on the static image saliency dataset SALICON [14] (10,000 images) without temporal recurrence module, then fine-tune 25 epochs on the dynamic video saliency dataset DHF1K [25]. For the SALICON dataset, the batch size is 6, and for the DHF1K dataset, the batch size is 8 videos, and 20 consecutive frames are randomly selected from any period of each video. The network uses Adam optimizer, the initial learning rate is set to 0.0005, every eight epochs reducing by 10 times. Among them, the learning rate of parameter α is fixed to 0.1, and the initial value is 0.25.The final video saliency detection results on HD lossless video saliency dataset [44] are shown in Fig. 7 and Table 1. We also calculated the average value of RMSE (Root Mean Square Error) between the saliency maps of each method in Fig. 7 and ground truth heatmaps, the results are shown in Table 2. We can see that the RMSE value of our method is the smallest, indicating that our saliency map is closest to ground truth.

Fig. 7
figure 7
Video saliency prediction results. a Original frame. b Ground truth heatmap. c Proposed method. d ACL [25]. e Deepvs [24]. f Xu [23]

Full size image
Table 1 Quantitative results on HD lossless video saliency dataset
Full size table
Table 2 The average value of RMSE between the saliency maps of different methods in Fig. 7 and ground truth heatmaps
Full size table
11 videos in 3 different resolutions are selected for detection (Video information is shown in Table 4). We use the 5 most commonly used metrics for evaluation [49], among them, the bigger the AUC, SIM, CC and NSS, the better, and the smaller the KL, the better. Compared with the ground truth and other methods, SALDPC is the best, the network can accurately combine the video spatiotemporal features and scale features to perform high-precision detection on video saliency in HD videos.

C.
Ablation Studies

We design several variants of SALDPC to prove the effectiveness of our structure: (1) Using only deep-level features (Only deep features). (2) Changing the exponential moving average to ConvLSTM (Multi-scale ConvLSTM). The experimental results are shown in Table 3. It can be seen that multi-scale features are important, and the exponential moving average is easier to train under multi-scale, while ConvLSTM with multi-scale is difficult to train and the effect is poor.

Table 3 Experimental results about SALDPC with different architectures
Full size table
In this section, we propose a novel video saliency network SALDPC, which uses Res2Net-50 as an encoder, extracts time information through multi-scale temporal recurrence, and then performs feature fusion and decoding through FPN and DPC decoders. Through a large number of experimental comparisons, we can see that, the final output saliency map achieves the state-of-the-art performances.

Saliency-based HEVC optimization algorithm
The video saliency algorithm used in this paper was introduced in the previous section. This section focuses on how to improve video compression performance by saliency. Since Gaussian blur before compression can cause considerable distortion, this section adopts the idea of embedded video compression.

That is, after calculating the video saliency, the HEVC video compression procedure is modified by the saliency to enhance the video quality for high saliency regions for the purpose of improving the quality of the video, and appropriately reducing the bitrate in the non-salient regions, so that the video bitrate is greatly reduced without affecting the view effect.

Starting from the basic coding unit CU, this section first optimizes the rate-distortion optimization (RDO) algorithm, introduces the result of video saliency into the rate-distortion cost, and seeks the best balance of distortion and bitrate to select the best partition mode and coding mode of CU from the global meaning, which greatly reduces the video bitrate while keeping the visual quality unchanged. On this basis, a QP improvement algorithm based on saliency is further proposed to improve the video compression performance.

The experimental results show that the improved single algorithm and the overall algorithm proposed in this paper are both superior to all the most advanced perceptual compression algorithms and the HEVC standard algorithms.

A.
Saliency-Aware RDO (SRDO)

The HEVC reference software HM adopts the Lagrangian optimization method, which comprehensively considers the influence of distortion and rate and reduces the constraint problem with limited rate and minimum distortion as a cost minimization problem. The Lagrangian multiplier is used as a trade-off factor between distortion and rate, and it represents the ratio of distortion and rate in the cost. The standard RDO uses a fixed Lagrangian multiplier to optimize from the partition of the CTU, the mode of PU and the partition of TU are finally determined, the standard RDO formula is:

min𝐽,𝐽=𝐷+𝜆𝑅
(8)
To make the saliency instruct rate-distortion optimization process in a graceful way and then select the perceptually optimal coding mode in the perceptual rate-distortion optimization process, this paper proposes a set of novel equations to achieve the best performance. First the saliency map is adopted to take the average saliency of the CU as the saliency of the current CU. For a n × n sized CU with depth d, the following formula is used to calculate its average saliency:

𝑆𝑛×𝑛(𝑘)=∑𝑛𝑖=1,𝑗=1𝑆(𝑖,𝑗)𝑛2
(9)
Additionally, the average saliency of the current frame is calculated to classify the degree of saliency of the CU:

𝑆avg=∑𝑖<width,𝑗<height𝑖=0,𝑗=0𝑆(𝑖,𝑗)width×height
(10)
Assuming that the maximum and minimum values of CU saliency are Smax and Smin, respectively, the saliency weighting factor SW can be calculated as:

𝑆𝑊=⎧⎩⎨⎪⎪log2(1+𝑆cu−𝑆avg𝑆max−𝑆avg),𝑆cu≥𝑆avg−log2(1+𝑆avg−𝑆cu𝑆avg−𝑆min),𝑆cu<𝑆avg
(11)
The saliency-aware perceptual priority distortion is calculated as follows:

𝐷𝑠=(ℎ×𝑆𝑊+1)×𝐷
(12)
In addition, ℎ represents the perceptual importance factor, and its calculation formula is:

ℎ={(5−𝑓)×log2(1+𝑓),𝑆𝑊≥0𝑓,𝑆𝑊<0
(13)
where f is a constant, which can be integrated into the HEVC compression standard as a compression parameter. It needs to be manually given by the encoding configuration file, and its range is [0, 1]. The core formulas of our algorithm are (11) and (13) which enable the nonlinear saliency mapping. We find that the nonlinear relationship between rate and distortion from the concave rate-distortion curve is the key to effectively imbed saliency information, which leads to the design of a series of log functions according to our experiments.

For intra-prediction, the above distortion uses the sum of squares error (SSE). For inter-prediction, the above distortion uses the sum of absolute difference (SAD), and the calculation formulas are:

SSE=∑𝑖=0𝑊−1∑𝑗=0𝐻−1∣∣𝑓(𝑖,𝑗)−𝑓′(𝑖,𝑗)∣∣2
(14)
SAD=∑𝑖=0𝑊−1∑𝑗=0𝐻−1∣∣𝑓(𝑖,𝑗)−𝑓′(𝑖,𝑗)∣∣
(15)
The SRDO formula for the final saliency-aware rate-distortion optimization is:

min𝐽𝑠,𝐽𝑠=𝐷𝑠+𝜆𝑅
(16)
If the saliency of the current block is large, Ds increases so that the distortion weight in the rate-distortion cost increases, to find the parameter corresponding to the minimum rate-distortion cost, the encoder tends to reduce the distortion and improve the visual quality. The method treats all small blocks of HEVC and changes the priority order of the RDO completely by using saliency, to optimize the selection process of all coding modes of CU, such as CU’s quad-tree partitioning, prediction mode, motion search, TU partitioning.

The CU partitions of the 20th frame of the SRDO method and HEVC in the Kristen And Sara video are shown in Fig. 8. As seen from the figure, for a salient face area such as an eye and nose, the SRDO method tends to be divided into smaller CUs to improve its quality. For areas that are not salient, such as clothes, the SRDO method tends to be divided into large CUs, while the HEVC standard partition is more complex.

Fig. 8
figure 8
Schematic diagram of CU partition. a Overall CU partition of SRDO. b CU partition of SRDO on clothes. c CU partition of SRDO on the human face. d Overall CU partition of HEVC. e CU partition of HEVC on clothes. f CU partition of HEVC on the human face. g The saliency map

Full size image

B.
Saliency-Aware QP Selection

1.
SQP

[33, 44] used the same QP setting algorithm, and the QP value of the CU can be automatically adjusted according to the saliency. We implement the QP setting algorithm on HEVC. Assuming that QPS is the QP value of the current frame given by the user, the QP value of the CU can be calculated by introducing its salient weights:

𝑄𝑃𝑘=round(𝑄𝑃𝑠𝑤𝑘‾‾‾√)
(17)
𝑤𝑘 is calculated and obtained by the following formula:

𝑤𝑘=𝑏+𝑐1+exp(−𝑎(𝑆(𝑘)−𝑆𝑎𝑣𝑔)𝑆𝑎𝑣𝑔)
(18)
This paper sets b = 0.7, c = 0.6, and a = 4. We call this method the SQP method.

(2)
SAQP

The HEVC standards introduce the concept of QP offset. To adapt to the variable video content, an adaptive quantization method based on CU content (AQP) is used. The basic algorithm cites the TM-5 model proposed by the MPEG-2 standard [36]. In this paper, we use the saliency map to calculate the activity of CU, while the original method treats all the CUs equally and compute the activity based on the pixel variance. The specific method is presented as follows.

For the CU of each depth, the QP of its slice is set to its basic QP. A CU with a size of 2 N × 2 N is divided into quad-tree to obtain four N × N subunits, and the standard deviations of four subunits are calculated. The average activity of the current CU is defined as follows:

𝐴CU=min{𝜎2𝑖}4𝑖=1+1
(19)
The average activity of the current frame is determined by all CUs of depth d:

𝐴⎯⎯⎯⎯𝑑frame=1𝑁∑𝑘=0𝑁−1𝐴𝑑𝐶𝑈𝑘
(20)
In addition, 𝐴𝑑𝐶𝑈𝑘 is the average activity of each CU at depth d, and N is the total number of CUs of depth d that the current frame can be divided. The QP offset from the initial QP of the CU can be calculated as:

Δ𝑄𝑃𝑑𝐶𝑈𝑘=6log2⎛⎝⎜⎜𝐴⎯⎯⎯⎯dframe+𝑅𝐴𝑑𝐶𝑈𝑘𝐴𝑑𝐶𝑈𝑘+𝑅𝐴⎯⎯⎯⎯𝑑frame⎞⎠⎟⎟
(21)
R is a normal number and is expressed as:

𝑅=2Δ𝑄𝑃max6
(22)
Δ𝑄𝑃max is a parameter given by the user in the configuration file, referring to the allowed CU maximum QP offset relative to the slice level QP.

The original intention of this method is to meet the human visual characteristics, which fully considers the local characteristics of each coding block. For the MPEG-2 16 × 16 small blocks, the compression performance can be appropriately improved, but the largest size of the CU in HEVC is 64 × 64. This method, which only considers the sub-block variance, cannot fully describe the difference between the CU and other CUs, and it also does not consider the influence of the saliency.

To make the saliency affect the CU's QP more rationally and improve the above-mentioned AQP method that achieves excellent performance in the past compression standard, the standard deviations of the four subunits of the CU are not adopted to determine the QP, we determine the QP value by the saliency of the CU. Then, the QP offset from the initial QP of the CU is:

Δ𝑄𝑃𝑑CU𝑘=6log2(𝑆cu+𝑅𝑆avg𝑆avg+𝑅𝑆cu)
(23)
where R is still the same as the calculation method in AQP, Δ𝑄𝑃max is set to 7 in the experiment of this paper, and this method is called the SAQP method.

Figure 9 shows the QP distribution map of the AQP, MQP (the range of the QP is 7), SQP and SAQP methods in the Kimono1 video frame. The darker the QP distribution map, the smaller the QP value used, the finer the quantization. The QP distribution of the AQP and MQP is disordered. Both the SQP method and the SAQP method can assign low QP to highly salient regions. The SAQP method has a finer QP setting for the highly salient region and has obvious gradualness.

Fig. 9
figure 9
Original image, QP distribution map and saliency map of Kimono1 60th frame. a QP distribution map of AQP. b QP distribution map of MQP. c QP distribution map of SQP. d QP distribution map of SAQP. e Original image. f Predicted saliency map

Full size image
In this section, we propose the saliency-based HEVC optimization algorithm, and, respectively, introduce the implementation principles and specific formulas of the saliency-aware rate-distortion optimization algorithm and the saliency-aware QP selection algorithm.

Experimental results
A.
Quantitative Evaluation of Video Compression Quality of the Proposed Methods

In recent years, various full reference objective video quality evaluation metrics have been proposed, but few objective evaluation metrics are suitable for the subjective quality of the video. Itti et al. proposed a new objective video quality evaluation metric [31], which uses the weighted distortion of eye-tracking data, based on MSE, and the eye-tracking weighted mean square error (EWMSE) is calculated as follows:

EWMSE=∑𝑊𝑥=1∑𝐻𝑦=1[𝑤(𝑥,𝑦)⋅(𝐿′𝑖(𝑥,𝑦)−𝐿𝑖(𝑥,𝑦))2]∑𝑊𝑥=1∑𝐻𝑦=1𝑤(𝑥,𝑦)
(24)
In addition, (𝑥,𝑦) represents the spatial coordinate of a point of the ith frame of the video, W and H represent the width and height of the video frame, respectively, 𝐿′𝑖(𝑥,𝑦) and 𝐿𝑖(𝑥,𝑦) represent the pixel values of the reconstructed video ith frame and the original video ith frame at (x, y), respectively, and 𝑤(𝑥,𝑦) represents the distortion weight associated with the eye-tracking data point, which can be calculated by the following Gaussian function:

𝑤(𝑥,𝑦)=12𝜋𝜎𝑥𝜎𝑦𝑁∑𝑛=1𝑁e−((𝑥−𝑥en)22𝜎2𝑥+(𝑦−𝑦en)22𝜎2𝑦)
(25)
where N represents the number of observers in the eye-tracking experiment dataset, (xen, yen) represents the fixation data when the nth observer views the video, 𝜎𝑥 and 𝜎𝑦 are two parameters representing the width of the Gaussian function, when the visual fovea’s angle is 2°, both 𝜎𝑥 and 𝜎𝑦 are 64 pixel distances. By mimicking the calculation of the PSNR metric, the eye-tracking weighted PSNR (EWPSNR) can be calculated as:

EWPSNR=10log10(2552EWMSE)
(26)
Since the EWPSNR metric needs definite fixation data, it conforms well with the dataset used in this paper, and it can fully consider the visual characteristics of the human eyes and can well evaluate the perceptual quality of the video. Therefore, BD-EWPSNR and BD-Rate based on EWPSNR are adopted in this paper (bit savings in the case of equivalent EWPSNR to measure the perceptual quality of reconstructed video obtained from the various compression algorithms). Additionally, to ensure the integrity of the experiment, this paper also uses other common metrics, such as BD-PSNR, BD-SSIM, BD-VIFP, BD-VMAF.

In this paper, 11 HD videos with different resolutions in the dataset are used in experiments. In addition, each HD video contains the observer's fixation data, and the specific parameters of the video are shown in Table 4. The saliency-aware HD video compression algorithm proposed in this paper is implemented based on the HM16.8. For convenience, the proposed method has three configurations: the perceptual rate-distortion optimization algorithm is called the SRDO, the SRDO + SQP, and SRDO + SAQP are the combinations of the perceptual rate-distortion optimization algorithms with different saliency-aware QP selection algorithms which are proposed above.

Table 4 Experimental 11 HD video sequences parameter information
Full size table
The proposed three algorithms are contrasted with the LDP standard configuration algorithm of the HM16.8 (using encoder_lowdelay_P_main.cfg), AQP algorithm, MQP algorithm, and the latest perceptual HD video compression algorithm PGVC [44], respectively. To ensure that the other parameters are the same, in the experiment, the GOP size is set to 1, the IPPP low-delay coding structure is used, and the value of the saliency influence factor f is set to 0.8. To obtain the video rate-distortion curve and calculate the BD-EWPSNR and other metrics, the QPs are set to 22, 27, 32, 37 for compression to obtain video with different bitrates and different qualities. Figure 10 shows the EWPSNR-Rate curve results obtained by seven different algorithms for the 11 videos.

Fig. 10
figure 10
EWPSNR-Bitrate curves for different videos

Full size image
As shown in Fig. 10, the higher the EWPSNR value of the curve, the better the video quality at the same bitrate. It can be seen that the proposed methods are obviously superior to other algorithms in general. The proposed three algorithms and PGVC are perceptual compression algorithms, which are superior to the other three non-perceptual compression algorithms, indicating that saliency is important for the guidance of video compression. Additionally, it can be seen that the combination of perceptual rate-distortion optimization and dynamic QP selection is significantly better than that of only perceptual rate-distortion optimization. Our SRDO + SAQP configuration is the best method of all methods.

The FourPeople video is a special case, and the performance of all the algorithms is similar in this video. We consider that the reason is that the four people in the video are slowly transferring paper, the paper attracting attention is small, and the movement is not intense, and the speech communication of the four people is accompanied by slow facial motion. But the facial areas are not all the salient areas in most of the frames, the detection of the facial results is significantly different from the actual value, resulting in a saliency deviation of the saliency prediction and inaccurate compression that performance has not changed significantly. The rest of the videos show a considerable advantage of our algorithms.

For the quantitative analysis, based on the video rate distortion curve, the BD-EWPSNR, BD-PSNR, BD-SSIM and BD-VIFP and BD-Rate based on EWPSNR are calculated based on the three optimization methods proposed in this paper relative to HM (LDP), AQP and MQP, respectively. In which BD-PSNR, BD-SSIM and BD-VIFP do not consider the influence of visual saliency, BD-EWPSNR considers visual saliency based on fixation, for these metrics, the larger BD-PSNR, BD-SSIM, BD-VIFP and BD-EWPSNR, the better, and the smaller BD-Rate, the better. The results are shown in Table 5.

Table 5 Quantitative evaluation results of video compression quality
Full size table
The results of Table 5 are analyzed. Firstly, considering the EWPSNR related to saliency, the proposed three algorithms’ BD-EWPSNR has a considerable gain compared to the HM (LDP), AQP and MQP, and its corresponding BD-Rate savings are all above 20%. Specifically, our optimal algorithm SRDO + SAQP’s BD-Rate saves 32.41% on average compared to HM (LDP), saves 44.58% and 35.38%, respectively, over the AQP and MQP algorithms. This shows that the proposed algorithm gains a significant improvement compared to the HEVC standards.

Secondly, considering the BD-PSNR, BD-SSIM, and BD-VIFP. BD-PSNR assigns the same weight to all distortions, it is necessary that the metric decline under the unequal compression strategy in different regions proposed by this paper, while the degrees for the decline in BD-PSNR and the improvement in the BD-EWPSNR are almost the same. BD-SSIM considers the spatial structure of the image and can describe the visual attention of the human eyes to the image incompletely, because the proposed algorithm and the improved algorithm of HEVC do not significantly improve or destroy this structure in the image, the gaps between the algorithms are few with only a slight loss. BD-VIFP uses natural scene statistics combined with distortion to quantify, it does not consider the gaze characteristics of the human eyes.

It is worth noting that the performance of the AQP algorithm is the worst, which is a good proof of the shortcomings when we introduce this method, it loses too much details when the size of the CTU reaches 64 × 64, which causes its poor performance. Therefore, the AQP method in the TM-5 model that calculates the variances of the subunits are not suitable for larger coding units in HEVC.

In addition to the above metrics, we also calculated our three configuration algorithms compared to HEVC’s BD-VMAF, the results are shown in Table 6; we can see that our method can improve the value of VMAF to a certain extent [50].

Table 6 Quantitative evaluation results of BD-VMAF of the proposed perceptual compression algorithms
Full size table
The results of the proposed algorithm in the paper and the latest perceptual HD video compression method PGVC [44] are shown in Table 7. To make an intuitive comparison, the HM (LDP) is directly used as the baseline to compare the improvement of all methods relative to it.

Table 7 Quantitative evaluation results of perceptual compression algorithms
Full size table
We also choose four frames from different video sequences with different QP to compare their PSNR in salient regions with the global frame PSNR, which shows that the salient area that people tend to focus has a higher PSNR both in low bitrate and high bitrate as is presented in Table 8 and Fig. 11. In such circumstance, we achieve the goal that alleviates the bandwidth stress while keeping the high perceptual quality.

Table 8 Local and global PSNR of the proposed method
Full size table
Fig. 11
figure 11
Salient region PSNR comparison. The blue bounding box indicates the selected salient area predicted by the proposed saliency model

Full size image
As shown in Table 7, three configuration methods proposed in the paper are better than the PGVC method, and the BD-EWPSNR metric is significantly improved, while BD-Rate metric drops sharply. At the same time, the global BD-PSNR degradation of the proposed methods is close to the improvement of BD-EWPSNR, unlike PGVC, the reduction in the former is significantly more than the latter.

The above comparisons are all on HD videos. We also compare with the latest method [45], they experiment on low-definition video dataset in [48]. Our effect on HD video is significantly better than them, and the compression of HD video is more difficult. The results are shown in Table 9.

Table 9 Comparison with the latest methods in low-definition videos
Full size table
The comparison of video compression time is shown in Table 10. Surprisingly, as mentioned before, the method proposed in this paper can not only improve the subjective quality but also obtain a large reduction in video compression time, even if it is not optimized specifically for coding complexity. The possible reason is that the salient region is small, which leads to our SRDO method focusing on the salient region when making CU mode selections, so the rate-distortion mode selection is more efficient and faster.

Table 10 The comparison of video compression time
Full size table
The largest coding time savings achieved by the SRDO + SAQP method is approximately 29.06% compared to HM (LDP), and the maximum time savings for the other two improved methods are also as high as 27.81% and 23.60%. The time savings of the AQP method are very small, the highest is only 6%, and the MQP method is equivalent to approximately 11 times more time. As far as we know, the method proposed in this paper is the only method for improving the compressed video quality while greatly reducing the coding time, but the PGVC [44] requires approximately 2.5 times the encoding time compared to HM.

B.
Comparison of Video Subjective Quality

To compare the subjective quality fairly, the rate control algorithm of HEVC is adopted to ensure the same bitrate of the video, and the target bitrate is set to 1000 kbps. We compare the subjective quality of the SRDO + SAQP method and HM (LDP) in the Kimono1 video. According to Fig. 12, the quality of the salient areas can be significantly improved by the proposed method in this paper, such as face, eyes, and mouth, and the quality of non-salient area is almost the same.

Fig. 12
figure 12
Subjective quality comparison. a Full-frame quality by HM(LDP). b Face’s quality by HM(LDP). c Full-frame quality by SRDO + SAQP method. d Face’s quality by SRDO + SAQP method

Full size image
In order to evaluate the real subjective quality of human observation video, we invited 95 volunteers of whom most know little about video coding according to Fig. 13 to view and evaluate videos compressed by different algorithms, HM, PGVC, AQP and SRDO + SAQP under the same condition. Different videos were played simultaneously on four LCD screens with the resolution of 1920 × 1080 and the background was filled with black in RGB (0, 0, 0). Each interviewee was invited to watch the results of 11 videos and took 30s break between two different videos. After watching four compressed results of different videos, every participant would select the best video with the highest subject quality.

Fig. 13
figure 13
Understanding about video coding

Full size image
As shown in Fig. 14, most respondents believe that the videos compressed by the proposed algorithm possess the best subject quality compared with other compression algorithms.

Fig. 14
figure 14
Subjective evaluation of video quality

Full size image
Conclusion
In this paper, to reduce the perceptual redundancy in HD video, A novel saliency-based HD video compression algorithm is proposed on top of the HEVC video compression standard, which achieves a great improvement of the video quality while greatly reducing the video encoding time. An advanced video saliency model, SALDPC, is proposed based on the deformable pyramid convolution decoder and temporal recurrence. The proposed SALDPC adopts deep learning approaches to achieve excellent performance.

In the HD video compression, the predicted saliency map is adopted to obtain the saliency of the CU, thereby using saliency-aware rate-distortion optimization (SRDO), the BD-Rate saves 20.822% compared to the HM (LDP) method. Further combining the saliency-aware dynamic QP setting algorithms SQP and SAQP, the BD-Rate separately further saves up to 30.618% and 32.41%, the compression time is reduced by up to 29.06%, which greatly exceeds the state-of-the-art method in terms of compression quality and compression efficiency. Using the algorithm proposed in this paper, the perceptual quality of the video can be effectively improved, and the video bitrate can be significantly reduced when the quality is constant.

The perceptual video compression is typically evaluated using the EWPSNR metric. However, the EWPSNR metric has certain defects: the influence of the distortion in the non-salient area is not considered, and the distortion generates saliency deviation before and after the compression. Thus, a study on more effective full-reference video quality assessment metrics can be another research direction in the future.

Further, the proposed saliency-aware perceptual compression algorithm only uses the saliency map as the guidance for video compression, which ignores the integrity information of the object. Future works will also include combining saliency with object contour detection.

Keywords
HEVC
QP
Rate-distortion optimization
Video saliency