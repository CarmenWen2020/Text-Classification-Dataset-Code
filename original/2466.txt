In recent years, RGB and thermal sensors are widely used. There is complementary information from these two types of sensors. A fundamental task which arises in this domain is RGBT tracking. It is a challenging problem to leverage RGB and thermal data. In this paper, we propose an adaptive fusion algorithm based on response map evaluation for RGBT tracking. Specifically, a hierarchical convolutional neural network is employed to extract deep features in RGB and thermal images, respectively. The target is tracked in correlation filter framework with each layer independently in RGB and thermal images. To evaluate response map of tracking status in various conditions, the average sidelobe peak response (ASPR) is proposed. Gaussian regression process is employed to provide adaptive fusion weights based on ASPR. Experimental results on two RGBT tracking datasets demonstrate the success of our method.

Introduction
Visual tracking plays a crucial role in visual applications, e.g., human action recognition, medical image analysis and robotics. Though much progress has been achieved, there still exist challenging problems, such as illumination variation, occlusion and background clutter.

Recently, several types of sensors are incorporated with visual camera to achieve visual applications. For example, Kinect which combines visual camera with depth sensor can compute human action in a short period of time. The thermal sensor is a passive sensor. Target with a temperature above absolute zero emits infrared radiation. It has been widely adopted in military use. In recent years, thermal sensors are used in civilian applications as they are more economically affordable. The thermal sensors are insensitive to illumination changes. Furthermore, thermal can penetrate haze and smog. These properties of thermal sensors are complementary to visible cameras.

Several RGBT tracking methods are proposed [1,2,3] which are based on handcrated features. Convolutional neural networks (CNNs) features based methods attract more and more attentions. In [4], a two-stream network is used to extract features from RGB and thermal images, respectively. CNNs are trained on large scale datasets and have a strong capability to capture semantic information. The higher layers capture semantic information and shallow layers contain local detail information. Therefore, several methods use hierarchical CNNs features to encode target accurately and achieve better results than single CNNs feature [5,6,7].

We note that too little work has been devoted to CNNs based RGBT tracking. Since two data sources (RGB and thermal data) are inputted, it is a challenging task to integrate hierarchical CNNs features from the two data simultaneously. In this paper, we propose a response map evaluation based algorithm for RGBT tracking. First, a hierarchical CNN is used to extract features in RGB and thermal images, respectively. The multiple CNNs features track object in correlation filter framework. An average sidelobe peak response (ASPR) is proposed to depict the response map in RGB and thermal images accurately. A Gaussian regression process (GRP) is employed to predict the ASPR. The final position is determined via the collaboration of these results.

The contributions of this paper are as follows.

Firstly, an ASPR is proposed to depict the response map of each tracker. In our observation, the ASPR is more accurate to depict the response map in challenging conditions.

Secondly, GRP is utilized to provide a prediction of ASPR. As a consequence, the tracking results can be fused accurately and effectively.

Thirdly, hierarchical CNNs features are utilized to extract features from RGB and thermal images. Deep feature based RGBT tracking had been a largely under explored domain. We exploit the representation ability of CNNs features for RGB and thermal images.

The rest of this paper is organized as follows: Section 2 gives related work that most closely to our work. Our tracking algorithm is given in Sect. 3. Experimental results are presented in Sect. 4. Finally, conclusions are drawn in Sect. 5.

Related work
In this section, we only review the most related works with our method. A comprehensive review can be found in [8, 9].

RGB tracking
Correlation filter tracking have attracted great attention in recent years in visual tracking because of its accuracy and efficiency. It is first introduced into visual tracking in the seminal work [10] which runs at hundreds of frames per second (fps). The image intensity which is vulnerable to noise is adopted to depict target.

Then many extensions based on correlation filter tracking framework are developed. For example, the KCF [11] extends the feature to multi-dimensional and uses the kernel trick to improve results. Some methods are proposed to address problems in correlation filter tracking, e.g., scale estimation and boundary effect. Adaptive scale algorithms are proposed [12, 13] in order to deal with scale variation problem. Spatial regularization is introduced in SRDCF [14] to handle the boundary effect which is a big problem in correlation filter tracking. BACF [15] is developed to alleviate boundary effect by incorporating background information. Hard negative mining scheme is used to train correlation filter to improve tracking performance [16]. Other discriminative features are employed for robust representation of the targets. For example, color names are adopted in [17]. CNNs features which have achieved superior performance in image classification are used in visual tracking [5,6,7]. A continuous operation is proposed [18, 19] to integrate multiple CNNs features in a unified correlation filter tracking framework.

RGBT tracking
In recent years, RGBT tracking attracts more and more attention in the computer vision community with the widely use of thermal infrared sensors.

The image features from RGB and thermal images are concatenated into a one-dimensional feature vector which is then represented in a sparse coding form [1]. In [2], RGB and thermal images are jointly represented in sparse coding. The coefficients are fused by using min operation. A Laplacian sparse representation is developed to learn a multi-feature model [3] which can deal with occlusion and capture spatial information. To achieve adaptive fusion of multiple modalities, a fusion algorithm which integrates the modality weights into representation model is proposed [20]. A gray and thermal tracking dataset is released [21]. In addition, it develops an adaptive weight algorithm for each modality image data. The sparse representation is integrated into particle filtering framework to achieve robust tracking. A CNN architecture which includes a two-stream network to achieve adaptive fusion of two modalities for effective RGBT tracking is proposed in [4]. A metric learning scheme is introduced in [22] for RGBT tracking. In [23], a neural network is proposed to handle the modality-shared challenges (e.g., fast motion, scale variation and occlusion) and the modality-specific ones (e.g., illumination variation and thermal crossover) for RGBT tracking. Three kinds of adapters within an end-to-end deep learning framework are design in [24] to effectively represent RGBT data. A novel duality-gated mutual condition network is proposed in [25] to fully exploit the discriminative information of all modalities while suppressing the effects of noise.

Motivated by the works mentioned above, an ASPR is proposed to depict response map in RGB and thermal images, respectively. A GRP is employed to characterize these two values. The predicted ASPR can be used as an indicator of the tracking results.

Tracking algorithm
Figure 1 demonstrates the workflow of our method. Hierarchical CNNs features are first employed to extract features from the region of interest. The target is tracked in correlation filter tracking framework. The ASPR are employed to evaluate response maps. A GRP based fusion algorithm is proposed to evaluate the tracking results of each feature. Finally, model update is carried out.

Fig. 1
figure 1
Demonstration of our workflow

Full size image
CNNs features
Our CNNs features are based on the VGG19-Net [26] which is trained on ImageNet [27]. Recent studies show that the hierarchical CNNs features capture different properties of the target [5,6,7]. Figure 2 shows the visualizations of different CNNs features. As shown in Fig. 2, the high layers and low layers possess different information which encodes various properties of target. In Fig. 2a, there are two people. The features in thermal images focus on the tracked person, while features in RGB images are less differentiable. In Fig. 2b, the features in RGB image contain background information, while features in thermal images capture the tracked person. We use the Conv3-4, and Conv5-4 of VGG19-Net to encode the object in order to improve the accuracy and robustness of object appearance.

Fig. 2
figure 2
Visualizations for different CNNs features of VGG19-Net. The first and third rows are RGB images. The second and fourth rows are thermal images. The first column shows Region of interest (ROI). The second column shows features of Conv3-4. The third column shows features of Conv5-4

Full size image
Correlation filter tracking framework
Correlation filters take advantage of the fast Fourier transform (FFT) to reduce computation complexity and enhance the training speeds. Our correlation filter tracking framework follows the method in [11]. We train a scale correlation filter to achieve adaptive scale estimation which is similar to the work in [12].

Let f be feature map of the object in current frame, ùëì‚ààùëÖùëä√óùêª√óùê∑, where W, H and D are the width, height and dimension of deep features, respectively. The circular shifts of f along the width and height are treated as the training examples. Each shifted example is represented as ùëì(ùë§,‚Ñé),(ùë§,‚Ñé)‚àà{0,1,‚ãØ,ùëä‚àí1}√ó{0,1,‚ãØ,ùêª‚àí1}.

The correlation filter k can be computed by solving the following minimization problem,

ùëò‚àó=argminùëò‚àëùë§,‚Ñé‚à•ùëò‚àóùëì(ùë§,‚Ñé)‚àíùë¶(ùë§,‚Ñé)‚à•2+ùúÜ‚à•ùëò‚à•22,
(1)
where y(w, h) is a label function which is a Gaussian function, ùúÜ is a regularization parameter. Equation (1) can be transformed into frequency domain as follows,

ùêæùëë=ùëå‚®Äùêπùëë¬Ø‚àëùê∑ùëë=1ùêπùëë‚®Äùêπùëë¬Ø+ùúÜ,
(2)
where K, F and Y are the Fourier transformation form of k, f and y, respectively. The bar means complex conjugation. The operator ‚®Ä denotes the element-wise product.

Given an image patch z of ROI in a new frame, the correlation response map can be computed as

ùëÖ(ùëß)=‚Ñ±‚àí1(‚àëùëë=1ùê∑ùêæùëë‚®Äùëßùëë),
(3)
where R represents the correlation response map, ‚Ñ±‚àí1 is the inverse FFT. The position of the target can be located by searching the maximum value from the response map R.

In our work, we use three features, Conv3-4, and Conv5-4 of VGG19-Net and HOG feature, to encode target appearance. The positions of the three features in RGB images {ùëÉ1ùë£,ùëÉ2ùë£,ùëÉ3ùë£} and thermal images {ùëÉ1ùëü,ùëÉ2ùëü,ùëÉ3ùëü} can be obtained by using Eq. (3), respectively. The final position can be obtained through a weighted fusion of these positions,

ùëÉ=‚àëùëó=13ùúÜùëóùëÉùëóùë£+‚àëùëò=13ùúÜùëòùëÉùëñùëü,
(4)
where ùúÜùëó,ùúÜùëò, are the weights of each feature which can be computed in the following subsection.

ASPR computation
It points out that the tracking status can be inferred from peak-to-sidelobe ratio (PSR) which is computed from response map.

PSR=max(ùëÖ)‚àíùúáùúé,
(5)
where ùúá and ùúé are the mean and the standard deviation of the response map, respectively.

In general case, the PSR can be used to accurately infer tracking status. However, in presence of illumination variation and occlusion, the PSR values decrease. In light of these observations, we design the ASPR measure to evaluate the tracking results. The higher ASPR value means the more reliable tracking results. The ASPR is computed as follows,

ASPR=max(ùëÖ)‚àíminùëÖ‚àëùëÖ‚àímax(ùëÖ),
(6)
In Fig. 3, represented frames in three videos are illustrated. In Fig. 3a, the PSR values in RGB and thermal images are 7.8920765 and 8.3236074, respectively. The computed ASPR values in RGB and thermal images are 0.0011521021 and 0.0010277361, respectively. In Fig. 3b, occlusion occurs, the PSR values in RGB and thermal images are 4.6466069 and 3.8966513, respectively. The computed ASPR values in RGB and thermal images are 0.00058759836 and 0.0010613044, respectively. In Fig. 3c, illumination changes, the PSR values in RGB and thermal images are 7.3785429 and 6.9207115, respectively. The computed ASPR values in RGB and thermal images are 0.0010153960 and 0.00099499186, respectively.

Fig. 3
figure 3
Illustration of ASPR values in RGBT tracking. a Sequence man2. b Sequence afterrain. c Sequence rainingwaliking

Full size image
GRP adaptive fusion
A relationship between the PSR and ASPR can be modeled as a GRP. This relationship is formulated as follows,

‚Ñé:ùëé‚ààùëÖ‚Üí‚Ñé(ùëé)‚ààùëÖ,
(7)
where ùëé=PSRùë°ùëñ and ‚Ñé(ùëé)=ASPRùë°ùëñ. The i denotes the i-th feature and t is frame number. The GRP is defined as follows:

‚Ñé(ùëé)‚àºGRP(ùëö(ùëé),‚Ñé(ùëé,ùëé‚Ä≤))
(8)
where m(a) and h(a, a‚Äô) are the mean function and covariance function of this group of functions. Let define ùë¶=‚Ñé(ùëé)+ùúÄ is the noisy observations, ùúÄ is Gaussian noise with variance matrix ùúé2ùêº. The zero-mean distribution of the function y can be formulated with the following prediction,

[ùë¶‚Ñé‚àó]‚àºùëÅ(0,[ùê∫(ùëé,ùëé)+ùúé2ùëõùêºùëî(ùëé‚àó,ùëé)ùëî(ùëé,ùëé‚àó)ùëî(ùëé‚àó,ùëé‚àó)])
(9)
where ùê∫(‚àó) and ùëî(‚àó) represent the covariance matrix and vector of inputs, respectively. ‚Ñé‚àó is normalized values of ASPR. The distribution of the functions can be computed as follows:

‚Ñé‚àó¬Ø=ùëî(ùëé‚àó,ùëé)[ùê∫(ùëé,ùëé)+ùúé2ùêº]‚àí1ùë¶,
(10)
where ‚Ñé‚àó¬Ø is the mean of ‚Ñé‚àó.

In Fig. 3a, the predicted ASPR values in RGB and thermal images are 0.00038 and 0.00015, respectively. In Fig. 3b, occlusion occurs, the predicted ASPR values in RGB and thermal images are 0.0085 and 0.0105, respectively. In Fig. 3c, illumination changes, the predicted ASPR values in RGB and thermal images are 0 and 0.0114, respectively. The comparisons of the ASPR values and the predicted ASPR values are effective to encode the tracking quality. For example, in Fig. 3c, the illumination changes severely. The PSR values cannot reflect the changes as the values are similar to the values in Fig. 3a. The differences between the ASPR values and the predicted ASPR values give a clue to the changes. The computed ASPR values and the predicted ASPR values show that the ASPR values are discriminative to challenging conditions in both RGB and thermal images.

The PSR value-based methods only consider the maximum value of response map while ignoring the global statistical property. Therefore, when the response of the object is smaller than the response of some distractors, these methods will lose the target quickly. While ASPR could not only consider the peak value but also evaluate the statistical property for the response map, which could forecast the tracking status more accurately and guarantee model updating effectively.

Model update
Our model update follows strategy in [11]. The RGB and thermal modules are updated separately. Specifically, our model consists of the learned target appearance and the transformed classifier coefficients. The current appearance is taken into account to update the classifier coefficients by linear interpolation. Figure 4 illustrates our tracking algorithm.

Fig. 4
figure 4
Our tracking algorithm

Full size image
Experiment
Datasets
GTOT dataset [21] consists of 50 RGBT sequences under different situations. The dataset includes seven attributes for analyzing challenge-based performance of the RGB-T tracking algorithm. In GTOT dataset, 13 trackers are compared, including ACFN [28], RPT [29], MUSTer [30], DSST [12], PCOM [31], CN [17], MEEM [32], STC [33], CT [34], SCM [35], KCF [11], Struck [36], TLD [37], BACF [15], ADnet [38] and MIL [39]. In addition, some methods (DSST, CT, KCF, Struck, STC, MIL, TLD, JSR, L1-PF [1], SCM and CN) are used. In particular, features in these trackers are concatenated from gray and thermal images as input.

RGBT234 [9] dataset includes 234 videos. There are 12 attributes for analyzing challenge-based performance of the RGB-T tracking algorithm. In RGBT234 dataset, we compare our tracker with 17 popular algorithms, including C-COT [18], ECO [19], JSR [2], CSR [21], L1-PF [1], DSST [12], CFnet [40], CSRDCF [41], SOWP [42], SRDCF [14] and SAMF [13].

Evaluation metrics
Precision rate (PR) and success rate (SR) are employed to measure the quantitative performance of the compared methods. PR computes the average Euclidean distance between the center locations of the tracked targets and the ground-truth positions of all the frames. The representative PR of the trackers averaged over all sequences using the threshold of 20 pixels. SR computes the overlap of the tracked targets and the ground-truth positions of all the frames. The overlap is defined as ùëÜ=ùëüùë°‚à©ùëü0ùëüùë°‚à™ùëü0, where ùëüùë° and ùëü0 are the tracked region and ground-truth region, respectively. ‚à© and ‚à™ represent the intersection and union operators, respectively. These two metrics are widely used in RGB tracking [8]. One-pass evaluation (OPE) is used in this experiment. In particular, the ground-truth object position in the first frame is initialized. the precision rate and success rate of all the results are recorded.

Quantitative evaluation
RGBT234 dataset
We compare the tracking results on a large dataset RGBT234 against popular tracking algorithms, as shown in Fig. 5. The representative score of PR/SR is presented in the legend. In PR and SR, our method achieves 0.676 and 0.481, respectively. The C-COT, SGT, ECO, and SRDCF in PR achieve 0.715, 0.710, 0.703 and 0.642, respectively. The C-COT, SGT, ECO, and SRDCF in SR achieve 0.513, 0.462, 0.513 and 0.462, respectively. Our method achieves compositable results with state-of-the-art trackers. The appealing results over the rest of methods demonstrate the superiority of the proposed algorithm. We further analyze the tracking results from the following two aspects.

Overall performance. We first compare our method with two RGB tracking algorithms with non-deep features, including CSR-DCF [21], and SRDCF [14]. Our method outperforms these two trackers significantly, i.e., achieving 8.9%/11.6% (PR/SR), and 5.3%/4.1% performance gains over CSR-DCF and SRDCF, respectively. It shows the importance of thermal information and effectiveness of our method Next, we evaluate our method with 6 RGB-T trackers with non-deep features, including CSR [21], SGT [43], JSR [2], L1-PF [1], MEEM [32]+RGBT, and KCF [11]+RGBT.

Our method perform a slightly lower than SGT method. The main reason is that the graph learning is used in SGT to adaptively learn the RGB and thermal information. Our method also achieves appealing performance over the other 5 trackers with a clear margin. Specifically, the superior results over MEEM+RGBT (6.1%/19.1%) may be beneficial from the exploitation of ASPR values and adaptive fusion strategy. The better tracking results than the other trackers demonstrate that our method can effectively integrate multi-spectral feature representation to perform RGBT object tracking. For example, the deep layer contains semantic information of the target and shallow layer captures detail location. The fusion weights are employed to suppress the effects of undesirable features in the object feature representations. Thus the risk of model drifting can be alleviated.

Finally, we compare with deep learning-based methods, C-COT [18], ECO [19], and CFnet [40]+RGBT. The comparison curves show that our method outperforms CFnet+RGBT. It further demonstrates the robustness and effectiveness of our method.

Attribute-based performance. The attribute based evaluation results of PR are presented in Table 1. The attributes include NO (no occlusion), PO (partial occlusion), HO (heavy occlusion), LI (low illumination), LR (low resolution), TC (thermal crossover), DEF (deformation), FM (fast motion), SV (scale variation), MB (motion blur), CM (camera moving), and BC (background clutter). The results show that our method performs comparable with state-of-the-art trackers in most of situations, including PO, HO, LI, LR, MB, CM, and BC, demonstrating the effectiveness of our method. For LI, LR and MB, we achieve comparable results with SGT. Unsatisfying results are generated on TC. It implies that noisy data degrades the tracking results probably due to improper fusing of different types of image data when their qualities vary greatly.

The attribute based evaluation results of SR are presented in Table 2. The results also show that our method performs comparable with state-of-the-art trackers in most of scenarios, including PO, HO, and BC, demonstrating the effectiveness of our method. For LI, we achieve the best result. It implies that our ASPR criterion is effective in low illumination situations.

Fig. 5
figure 5
The evaluation results on the RGBT234 benchmark dataset. The representative score of PR/SR is presented in the legend. The SR of the proposed method is 0.481 and The PR of the proposed method is 0.676

Full size image
Table 1 PR values on different attributes on the RGBT234 dataset
Full size table
Table 2 SR values on different attributes on the RGBT234 dataset
Full size table
GTOT dataset
We present the tracking results of our method on GTOT dataset against state-of-the-art trackers in Fig. 6. These methods are with RGB and thermal inputs used in [21]. The comparison curves demonstrate that our method significantly outperforms most of state-of-the-art algorithms on the GTOT dataset. It indicates that our method which extracts deep features is more robust representative for multiple features tracking.

Fig. 6
figure 6
The evaluation results on the public GTOT benchmark dataset. The representative score of PR/SR is presented in the legend. The SR of the proposed method is 0.481 and The PR of the proposed method is 0.676

Full size image
Ablation study
In this section, we test the effects of three key parts to see how they contribute to enhancing the final results. Our method has three parts: (1) the ASPR; (2) the adaptive fusion scheme and (3) the hierarchical CNNs features. The RGB features are effective while separating two moving objects and the thermal features are insensitive to weathers and lighting conditions. Therefore, to demonstrate the effectiveness of these two features, we also report the results of our method with only RGB or thermal input. On the situations of partial occlusion, low illumination, low resolution and background clutter, the thermal features are helpful to distinguish objects as they are insensitivity to lighting variations. While the RGB features are more effective than thermal one on the situations of thermal crossover, fast motion, scale variation and motion blur, and it suggests that the RGB features include more appearance information of objects which are beneficial to RGBT tracking, especially when the thermal features are unreliable.

Conv3-4 and Conv5-4 of VGG19-Net are shallow and deep features, respectively. The aim of utilizing the two features is that they are complementary to each other. Thus, the tracking results can be complementary and combined to improve the final tracking accuracy. That is, single feature degrades the tracking performance. Thus, we design seven variants of the original method to test the effectiveness on the three parts on the RGBT234 dataset.

Test1: without ASPR part.

Test2: without fusion scheme. RGB and thermal results are fused with equal weights.

Test3: with one deep feature (Conv5-4).

Test4: with one deep feature (Conv3-4).

Test5: with RGB feature only.

Test6: with thermal feature only.

Test7: PSR is used to fuse tracking results.

Table 3 shows the precision scores and success scores for the comparison methods on the RGBT234 dataset. According to the comparison result, all of the variants of our method perform inferior results than the original algorithm. However, most of them achieve comparable results to the other competitive methods as shown in Fig. 5. In summary, the quantitative results show that all these parts are important for constructing a robust tracking algorithm.

Table 3 The evaluation results of the variants of the original method on the public RGBT234 benchmark dataset
Full size table
Quality evaluation
We show the qualitative evaluation in Fig. 7.

In Dog sequence, we can see that our approach can track target robustly even in the challenges of deformation and background clutter. It suggests that our CNNs features are able to learn robust representations from RGB and thermal images to overcome deformation and background effects. ECO tracker can also track the target. The other trackers lost the target when deformation occurs.

In Manlight video, the target undergoes sever illumination changes. We can see that the thermal images are easy to differentiate target, while the RGB images are affected by the lighting condition. This shows the effectiveness of the complementary benefits from RGB and thermal images. When one is unreliable, another one can provide more confidential information to track objects. Our method is robust to illumination variation as the fusion algorithm is effective to integrate RGB and thermal information. CSR-DCF-RGBT and our method track the target throughout the entire video. The other methods lock to the background when illumination changes.

In Night2 sequence, the target undergoes occlusion and background clutter. CSR-DCF-RGBT and our method track the target throughout the entire video. It suggests that our adaptive fusion algorithm is able to effectively incorporate the RGB and thermal information. CSR-DCF-RGBT achieves a little poor accuracy by enlarging its bounding box to include distracting background. The other trackers lost the target at the begging of the video.

In Yellowcar video, the target undergoes scale variation. ECO, SRDCF and our method can track the target. Our tracker can detect the tracking status to choose a suitable scale of target. CSR-DCF-RGBT lost target when scale changes. SGT achieves a little poor accuracy by enlarging its bounding box to include distracting background.

Fig. 7
figure 7
Representative tracking results of our method against five trackers in four challenging sequences

Full size image
Discussions
We analyze the details of our method for better understanding our proposed approach from the following three aspects. First, we evaluate our algorithm on RGBT234 dataset, as illustrated in Fig. 5. We can see that our method achieves appealing performance. The SAMF [13] is based on PSR criterion alone and the tracking results are lower than our results. It demonstrates the robustness and effectiveness of our proposed scheme. Our method achieves superior performance over methods with single image source input, suggesting our algorithm is beneficial from the joint considerations of thermal and RGB data. Next, we compare our algorithm on GTOT dataset, as illustrated in Fig. 6. Our method achieves superior performance over most of the compared methods.

To demonstrate the importance of the main parts, we implement six variants of our method for ablation study. Table 3 presents the evaluation results on RGBT234 dataset. Specifically, we can draw the following conclusions.

(1)
Utilizing the ASPR values to detect tracking status plays a key role; from the observation it shows that results of method without ASPR component is much lower than results of ours.

(2)
Introducing fusion weights into the tracking framework benefits to suppress the effects of background disturbance in tracking.

Conclusion
In this paper, a response map evaluation based RGBT tracking is proposed. Hierarchical CNNs features provide representative features in RGB and thermal images, respectively. The correlation filter tracking framework is adopted. PSR and ASPR are proposed to evaluate response map. The GRP is employed to give fusion weights. To validate the effectiveness of our algorithm, extensive experiments on two RGBT tracking datasets against state of-the-art methods are conducted. Specifically, in GTOT dataset, our method ranks the first and third in SR and PR, respectively. In RGBT234 dataset, our method ranks the fourth and sixth in SR and PR, respectively.

There are two interesting problems to be investigated. The first is to develop a more accurate metric to evaluate tracking quality. The second is to find the relationships in the hierarchical deep features in each layer.

Keywords
RGBT tracking
Gaussian regression process
ASPR