With the increasing demand for real-world scenarios such as robot navigation and autonomous driving, how to achieve a good trade-off between segmentation accuracy, inference speed and model size has become a core issue for real-time semantic segmentation applications. In this paper, we propose a lightweight attention-guided asymmetric network (LAANet), which adopts an asymmetric encoderâ€“decoder architecture. In the encoder, we propose an efficient asymmetric bottleneck (EAB) module to jointly extract local and context information. In the decoder, we propose an attention-guided dilated pyramid pooling (ADPP) module and an attention-guided feature fusion upsampling (AFFU) module, which are used to aggregate multi-scale context information and fuse features from different layers, respectively. LAANet has only 0.67M parameters, while achieving the accuracy of 73.6% and 67.9% mean Intersection over Union (mIoU) at 95.8 and 112.5 Frames Per Second (FPS) on the Cityscapes and CamVid datasets, respectively. The experimental results show that LAANet achieves an optimal trade-off between segmentation accuracy, inference speed, and model size.

Introduction
The task of semantic segmentation is to assign corresponding predefined category labels to each pixel in the input image, which can be regarded as a dense prediction problem. As a key technology in the field of computer vision, semantic segmentation has been widely used in robot navigation [1], autonomous driving [2], augmented reality [3] and other Internet of things applications. However, most of these applications are limited in memory capacity and computational cost, and require high real-time and accuracy. Therefore, how to design a lightweight and efficient real-time semantic segmentation network has become a hot issue of current research.

In response to this issue, many real-time semantic segmentation methods based on Full Convolution Network (FCN) [4] have been proposed in recent years. These methods can be roughly classified into three categories: (1) optimized convolution, for example: ENet [5] uses dilated convolution to expand the receptive field without increasing the number of parameters. ERFNet [6], ShuffleNet [7], and DABNet [8] reduce the computational cost by using asymmetric convolution, depth-wise separable convolution, and depth-wise asymmetric convolution, respectively. (2) Integrating context information, for example: CBAM [9] designs channel attention module and spatial attention module to enhance feature integration between different levels. PSPNet [10], DeepLab V2 [11] and LiteSeg [12] capture multi-scale context information by using pyramid pooling module, atrous spatial pyramid pooling (ASPP) module, and deeper version of atrous spatial pyramid pooling (DASPP) module, respectively. (3) State-of-the-art segmentation architectures, for example: LEDNet [13] uses an asymmetric encoderâ€“decoder architecture. DFANet [14], FDDWNet [15], and EACNet [16] construct lightweight architectures by using lightweight classification networks as the backbone, designing lightweight modules based on optimized convolution, and building a bilateral structure, respectively. Although these real-time semantic segmentation methods have achieved decent results on benchmarks (e.g., Cityscapes [17] and CamVid [18] datasets), there is still much room for improvement in the research on the trade-off between segmentation accuracy, inference speed, and model size.

In this paper, we propose a lightweight attention-guided asymmetric network (LAANet) for real-time semantic segmentation, and its overall architecture is shown in Fig. 1. LAANet consists of two parts: encoder and decoder. In the encoder, we design an efficient asymmetric bottleneck (EAB) module. It uses a two-branch structure, where one branch extracts local information using depth-wise asymmetric convolution and the other branch extracts context information using depth-wise asymmetric dilated convolution. In the decoder, we design two attention-guided modules: one is the attention-guided dilated pyramid pooling (ADPP) module, which introduces the attention mechanism into a deeper version of the ASPP module, thereby guiding the selection of features when integrating multi-scale context information. The other is the attention-guided feature fusion upsampling (AFFU) module, which utilizes the proposed improved spatial attention (ISA) module and improved channel attention (ICA) module to effectively fuse features at different levels during the upsampling process. Without any pre-training and post-processing, LAANet, respectively, achieves the accuracy of 73.6% and 67.9% mIoU on the Cityscapes [17] and CamVid [18] datasets with only 0.67M parameters. Moreover, LAANet is able to process input images of size 512Ã—1024 at 95.8 FPS and 360Ã—480 at 112.5 FPS on a single 1080Ti GPU. The main contributions can be summarized as follows:

(1)
The proposed EAB module captures both local and context information with fewer parameters and computational cost, achieving higher segmentation accuracy and faster segmentation speed.

(2)
The proposed ADPP module integrates context information for segmentation objects at different scales, improving the segmentation accuracy of small targets.

(3)
The proposed AFFU module fuses low-level and high-level features, making the feature map with restored resolution still rich in spatial and semantic information.

(4)
The proposed LAANet effectively solves the intra-class inconsistency and inter-class indistinction problems. Compared with existing state-of-the-art real-time semantic segmentation methods, LAANet achieves the best trade-off between segmentation accuracy, inference speed, and model size.

Fig. 1
figure 1
Overview architecture of the proposed LAANet

Full size image
The remainder of this paper is organized as follows. In Sect. 2, we give some preliminary knowledge of the proposed approach (LAANet). In Sect. 3, we describe the components of LAANet and its complete network architecture in detail. In Sect. 4, we report and compare the experimental results on two benchmark datasets. In Sect. 5, we make a brief conclusion of this paper.

Preliminary nowledge
In this section, we mainly introduce some preliminary knowledge about optimized convolution, pyramid structure and attention mechanism, which are related to our proposed method.

Optimized convolution
Optimized convolution includes two categories: dilated convolution [5, 19, 20] and factorized convolution [21]. Among them, factorization convolution includes asymmetric convolution [6, 22, 23], depth-wise separable convolution [24,25,26], depth-wise separable dilated convolution [14, 27, 28], asymmetric depth-wise separable convolution [8, 29, 30], and asymmetric depth-wise separable dilated convolution [15, 31, 32], etc. In real-time semantic segmentation networks, dilated convolution is often used to expand the receptive field, and factorized convolution is used to reduce the number of parameters and computational cost. In this paper, dilated convolution, asymmetric depth-wise separable convolution and asymmetric depth-wise separable dilated convolution are used.

Dilated convolution is obtained by inserting zeros (or holes) between every two consecutive elements of the convolution kernel. It can effectively expand the receptive field without increasing the number of parameters. Assuming that the size of the input feature map is ğ»Ã—ğ‘ŠÃ—ğ‘, the size of the output feature map is ğ»Ã—ğ‘ŠÃ—ğ‘â€², the size of the convolution kernel is n, and the dilation rate is d, the receptive field of the dilated convolution is:

[(ğ‘›âˆ’1)Ã—ğ‘‘+1]Ã—[(ğ‘›âˆ’1)Ã—ğ‘‘+1]
(1)
The number of parameters of the dilated convolution is:

ğ‘›Ã—ğ‘›Ã—ğ‘Ã—ğ‘â€²
(2)
The computational cost of dilated convolution is:

ğ‘›Ã—ğ‘›Ã—ğ‘Ã—ğ‘â€²Ã—ğ»Ã—ğ‘Š
(3)
Both asymmetric depth-wise separable convolution and asymmetric depth-wise separable dilated convolution are a special type of convolution obtained by factorizing the standard convolution. Asymmetric depth-wise separable convolution factorizes the standard convolution into a set of asymmetric one-dimensional depth-wise convolutions and a point-wise convolution to reduce the number of parameters and computational cost. Using the same notation, the receptive field of the asymmetric depth-wise separable convolution is:

ğ‘›Ã—ğ‘›
(4)
The number of parameters of the asymmetric depth-wise separable convolution is:

2Ã—ğ‘›Ã—ğ‘+ğ‘Ã—ğ‘â€²
(5)
The computational cost of asymmetric depth-wise separable convolution is:

2Ã—ğ‘›Ã—ğ‘Ã—ğ»Ã—ğ‘Š+ğ‘Ã—ğ‘â€²Ã—ğ»Ã—ğ‘Š
(6)
Asymmetric depth-wise separable dilated convolution factorizes the standard convolution into a set of asymmetric one-dimensional depth-wise separable dilated convolutions and a point-wise convolution to expand the receptive field and reduce the number of parameters and computational cost. Using the same notation, the receptive field of the asymmetric depth-wise separable dilated convolution is:

[(ğ‘›âˆ’1)Ã—ğ‘‘+1]Ã—[(ğ‘›âˆ’1)Ã—ğ‘‘+1]
(7)
The number of parameters of the asymmetric depth-wise separable dilated convolution is:

2Ã—ğ‘›Ã—ğ‘+ğ‘Ã—ğ‘â€²
(8)
The computational cost of asymmetric depth-wise separable dilated convolution is:

2Ã—ğ‘›Ã—ğ‘Ã—ğ»Ã—ğ‘Š+ğ‘Ã—ğ‘â€²Ã—ğ»Ã—ğ‘Š
(9)
Pyramid structure
Pyramid structure can extract multi-scale context information, thus solving the problem of large changes in the scale of segmentation objects. To better aggregate information from multiple receptive fields, many advanced semantic segmentation networks have designed a variety of modules with pyramid structure. Such as the pyramid pooling module [10], the atrous spatial pyramid pooling module [11], the distinctive atrous spatial pyramid pooling module [12] and a deeper version of atrous spatial pyramid pooling module [12], etc. In Sect. 3.2, the deeper version of atrous spatial pyramid pooling (DASPP) module is used to design our attention-guided dilated pyramid pooling (ADPP) module.

In LiteSeg [12], the structure of the DASPP module is shown in Fig. 2. The DASPP module is obtained by improving on the ASPP module. Compared with the ASPP module, the DASPP module adds a 3Ã—3 standard convolution to refine the features after the 3Ã—3 dilated convolutions of each layer, thereby obtaining richer context information.

Fig. 2
figure 2
Illustration of DASPP module

Full size image
Attention mechanism
Attention mechanism can selectively emphasize salient features and ignore insignificant features by assigning different weights to image pixels. Several recent works [9, 33,34,35,36] have been devoted to applying attention mechanism to the field of image semantic segmentation, thereby enhancing the feature learning ability of the network and improving the segmentation accuracy of objects. In Sect. 3.3, referring to the two attention modules in CBAM [9], we propose an improved spatial attention (ISA) module and an improved channel attention (ICA) module, and design our attention-guided feature fusion upsampling (AFFU) module based on them.

In CBAM [9], the structure of the channel attention (CA) module is shown in Fig. 3a. First, the input feature map F of size ğ»Ã—ğ‘ŠÃ—ğ‘ is passed through the average pooling and max-pooling operations to obtain two feature maps ğ¹ğ‘ğ‘ğ‘£ğ‘” and ğ¹ğ‘ğ‘šğ‘ğ‘¥ of size 1Ã—1Ã—c. Then, ğ¹ğ‘ğ‘ğ‘£ğ‘” and ğ¹ğ‘ğ‘šğ‘ğ‘¥ are passed through a shared MLP to obtain two channel attention maps of size 1Ã—1Ã—c. Finally, these two channel attention maps are added pixel-by-pixel and processed by the sigmoid function to obtain the final channel attention map ğ‘€ğ‘. In summary, the CA module is calculated as:

ğ‘€ğ‘(ğ¹)=ğœ(ğ‘Š1(ğ‘Š0(ğ¹ğ‘ğ‘ğ‘£ğ‘”))+ğ‘Š1(ğ‘Š0(ğ¹ğ‘ğ‘šğ‘ğ‘¥)))
(10)
where ğœ indicates the sigmoid function, and ğ‘Š0 and ğ‘Š1 indicate the two weights of the shared MLP.

Fig. 3
figure 3
a SA module. b CA module

Full size image
In CBAM [9], the structure of the spatial attention (SA) module is shown in Fig. 3b. First, the feature map ğ¹â€² refined by the channel attention map is applied the average pooling and max-pooling operations along the channel axis to obtain two feature maps ğ¹ğ‘ ğ‘ğ‘£ğ‘” and ğ¹ğ‘ ğ‘šğ‘ğ‘¥ of size ğ»Ã—ğ‘ŠÃ—1. Then, ğ¹ğ‘ ğ‘ğ‘£ğ‘” and ğ¹ğ‘ ğ‘šğ‘ğ‘¥ are concatenated and convolved by a 7Ã—7 standard convolution to obtain a spatial attention map of size ğ»Ã—ğ‘ŠÃ—1. Finally, the obtained spatial attention map is processed by the sigmoid function to obtain the final spatial attention map ğ‘€ğ‘ . In summary, the SA module is calculated as:

ğ‘€ğ‘ (ğ¹)=ğœ(ğ‘“7Ã—7([ğ¹ğ‘ ğ‘ğ‘£ğ‘”;ğ¹ğ‘ ğ‘šğ‘ğ‘¥]))
(11)
where ğœ indicates the sigmoid function, and ğ‘“7Ã—7 indicates a 7Ã—7 standard convolution operation.

Method
In this section, we first introduce how to improve some real-time semantic segmentation methods mentioned in Sect. 2 to propose the three main components of our LAANet: the efficient asymmetric bottleneck (EAB) module, the attention-guided dilated pyramid pooling (ADPP) module, and the attention-guided feature fusion upsampling (AFFU) module. Then, we describe the complete network architecture of our LAANet.

Efficient asymmetric bottleneck module
Recently, many lightweight real-time semantic segmentation networks have applied optimized convolution to the bottleneck module design, which effectively reduces the number of parameters and computational cost. Inspired by the asymmetric convolution in ERFNet [6] (Fig. 4a) and the depth-wise separable convolution in ShuffleNet [7] (Fig. 4b), we design the efficient asymmetric bottleneck (EAB) module. It uses depth-wise asymmetric convolution and depth-wise asymmetric dilated convolution to extract both local and context information with fewer parameters and computational cost.

Our EAB module is shown in Fig. 4c. First, a 3Ã—3 standard convolution is used to capture deeper feature information, and the number of output channels is halved to reduce the computational cost. Next, a two-branch structure is used. The first branch uses a set of depth-wise asymmetric convolutions with kernel 3 to extract local information, and the second branch uses a set of depth-wise asymmetric dilated convolutions with kernel 3 to extract more complex context information. Compared with the standard convolution, the number of parameters and computational cost of these two sets of optimized convolutions is only 6+ğ‘â€²9ğ‘â€² times (ğ‘â€² denotes the number of output channels). In addition, we add feature interaction operations between the two branches to facilitate information sharing between different branches. Then, the outputs of these two branches are added and the number of output channels is recovered by a 1Ã—1 point-wise convolution. The following is a skip connection, which solves the problems of gradient vanish and explosion during training by adding the initial input to the output at this point. Finally, a channel shuffle operation is used to facilitate the information fusion between channels, thereby achieving higher segmentation accuracy.

Fig. 4
figure 4
a Non-bottleneck-1D module. b ShuffleNet module. c Our EAB module. â€œDConvâ€ indicates depth-wise convolution. â€œDDConvâ€ indicates depth-wise dilated convolution. For brevity, the batch normalization and activation functions are not marked

Full size image
Attention-guided dilated pyramid pooling module
The size of objects in urban road scenes varies widely, and to address the multi-scale problem of segmenting objects, we consider how to efficiently integrate context information. Inspired by the deeper version of atrous spatial pyramid pooling (DASPP) module in LiteSeg [12] and channel-wise attention (CA) module in PFAN [37], we design the attention-guided dilated pyramid pooling (ADPP) module. It uses hierarchical U-shaped structure and global average pooling branch to gradually integrate context information from three different pyramid scales and global context information.

Our ADPP module is shown in Fig. 5. First, the max-pooling operation is used to reduce the resolution of the feature map, thereby reducing the computational cost. Next, three 3Ã—3 dilated convolutions with dilation rates of 9, 6, and 3 are used from top to bottom for feature extraction at the corresponding level, and a 3Ã—3 standard convolution is added after each dilated convolution to refine the features. Then, the bilinear interpolation operation is used to recover the resolution of the feature map, and the attention weight map generated by the current layer is added to the result after the convolution operation of the previous layer, thereby fusing the feature information at adjacent scales more accurately. Finally, the output features of the above U-shaped structure and the original features after 1Ã—1 point-wise convolution are multiplied pixel by pixel, and the multiplication results are added with the global average pooling branch results to obtain the final feature representation.

Fig. 5
figure 5
Our ADPP module. The dashed box means the global average pool branch. For brevity, the batch normalization and activation functions are not marked

Full size image
Attention-guided feature fusion upsampling module
In deep neural networks, low-level features are rich in spatial information and high-level features are rich in semantic information. However, it is difficult to fuse them together due to the difference in resolution and channels. Inspired by EGAUD [38], we apply the attention mechanism to upsampling and design our attention-guided feature fusion upsampling (AFFU) module. It uses the spatial attention map generated from the low-level features and the channel attention map generated from the high-level features to effectively fuse the features from different layers and improve the segmentation effect of the network.

Our AFFU module is shown in Fig. 6c. First, inspired by CBAM [9], our improved spatial attention (ISA) module and improved channel attention (ICA) module are designed. As shown in Fig. 6a, our ISA module generates a spatial attention map containing local spatial information of each pixel after averaging pooling along the channel axis and 1Ã—1 convolution operations. As shown in Fig. 6b, our ICA module generates a channel attention map containing global semantic information for each channel after averaging pooling and 1Ã—1 convolution operations. Next, as shown in Fig. 6c, the low-level feature map and the high-level feature map input by the AFFU module are denoted by ğ‘€ğ¿ and ğ‘€ğ», respectively. After 1Ã—1 convolution, the feature maps ğ‘€â€²ğ¿ and ğ‘€â€²ğ» are obtained, respectively. After the ISA module and the ICA module, the feature maps ğ‘€â€³ğ¿ and ğ‘€â€³ğ» are obtained, respectively. Then, the feature map ğ‘€â€²ğ» is upsampled and multiplied pixel-by-pixel with the feature map ğ‘€â€³ğ¿, thereby reweighting each pixel. The low-level feature map ğ‘€â€²ğ¿ is multiplied pixel-by-pixel with the high-level feature map ğ‘€â€³ğ», thereby reweighting each channel. Finally, these two weighted feature maps are added to output the feature map with spatial and semantic information.

Fig. 6
figure 6
a Our ISA module. b Our ICA module. c Our AFFU module. For brevity, the batch normalization and activation functions are not marked

Full size image
Network architecture
Based on the above three modules, we propose a lightweight attention-guided asymmetric network (LAANet) for real-time semantic segmentation, and its overall architecture is shown in Fig. 1. LAANet adopts an asymmetric encoderâ€“decoder architecture, and the details of the architecture are shown in Table 1.

Table 1 Architecture details of LAANet
Full size table
In the encoder, three 3Ã—3 standard convolutions are used to extract the initial features, where the first stride 2 is used to complete the first downsampling. In order to retain more spatial information, only three downsampling operations are used in LAANet, and the total downsampling rate is 8. The latter two downsampling operations are performed using the same downsampling unit as in the initial block of ENet [5], i.e., a cascade of a 3Ã—3 convolution with stride 2 and a 2Ã—2 max-pooling. After the second and third downsampling, 2 and 6 EAB modules are used to extract dense features, respectively. Moreover, in order to extract broader context information, referring to CFPNet [19], the dilation rates in the above EAB modules are set to 2, 2, 4, 4, 8, 8, 16, 16, respectively. In the decoder, the ADPP module is first used to integrate the context information and compensate for the detail information lost in the downsampling process. Then, two AFFU modules are used to fuse the features from different layers and gradually recover the resolution of the feature map. Finally, 1Ã—1 point-by-point convolution and bilinear interpolation operations are used to match the number of channels of the feature map with the number of classes of segmented objects and the resolution of the feature map with the resolution of the input image, respectively.

Experiments
In this section, we first introduce the two benchmark datasets used in the experiments and the implementation details. Then, we perform a series of ablation experiments to verify the effectiveness of each component of LAANet. Finally, we present the performance of LAANet on the Cityscapes and CamVid datasets and compare LAANet with other state-of-the-art real-time semantic segmentation methods.

Datasets
Cityscapes [17] is a large semantic understanding dataset of urban street scenes. It contains 5000 finely annotated images (2975 for training, 500 for validation, 1525 for testing) and 20,000 coarsely annotated images (not used in this experiment). These images have a high resolution of 1024Ã—2048 with 19 semantic categories. Following the conventional settings, we reduce the image resolution to 512Ã—1024 before training.

CamVid [18] is a small road scene dataset extracted from video sequences. It contains 701 images (367 for training, 101 for validation, and 233 for testing). These images have a resolution of 720Ã—960 and 11 semantic categories. Following the conventional settings, we reduce the image resolution to 360Ã—480 before training.

Implementation details
We complete all experiments on the Pytorch platform with a single 1080Ti GPU (11G), CUDA 9.0, and cuDNN V7. We use mini-batch stochastic gradient descent (SGD) algorithm to train our LAANet, where the momentum is 0.9 and the weight decay is 1ğ‘’âˆ’4. The batch size is set to 8 for the Cityscapes dataset and 16 for the CamVid dataset. During training, the â€œployâ€ learning rate strategy is adopted, where the power is 0.9. The initial learning rate is set to 4.5ğ‘’âˆ’2 for the Cityscapes dataset and 1ğ‘’âˆ’3 for the CamVid dataset. To avoid over-fitting, we perform data augmentation by mean subtraction, random level flipping, and random scaling on the input images. The random scale includes {0.75, 1.0, 1.25, 1.5, 1.75, 2.0}. We use mean Intersection over Union (mIoU), frames per second (FPS) and the number of parameters (Params) to evaluate the segmentation accuracy, segmentation speed and model size, respectively.

Ablation study
In this subsection, we successively perform ablation studies on the three main modules (EAB, ADPP and AFFU) that constitute LAANet, thus demonstrating the feasibility and effectiveness of each module. All the ablation experiments are trained on the Cityscapes training set and evaluated on its validation set.

Ablation Study for EAB Module. To verify that our EAB module has a better experimental effect than the other bottleneck modules shown in Fig. 4, we replace the EAB module in LAANet with the Non-bottleneck-1D module and ShuffleNet module, respectively. As shown in Table 2, when the EAB module is used, the model has the least number of parameters, the highest segmentation accuracy, and the higher inference speed. This fully demonstrates the high efficiency of our EAB module. In addition, to verify the validity of the dilation rates set in the EAB modules, we set up two sets of comparative experiments as shown in Table 2. The results show that using different dilation rates has almost no effect on the parameters and inference speed of the model, and the segmentation accuracy obtained by using the dilation rates {2, 2, 4, 4, 8, 8, 16, 16} is the highest, which is 1.7% higher than that obtained by using the dilation rates {2, 2, 4, 4, 4, 4, 4, 4}. In summary, our EAB moduleâ€™s choice of each component outperforms the other choices in terms of trade-offs between segmentation accuracy, inference speed, and model size.

Table 2 Ablation study for EAB module on the Cityscapes validation set
Full size table
Ablation Study for ADPP Module. To verify that it is a correct choice to use the max-pooling operation in our ADPP module, we replace all the max-pooling operations in the ADPP module with the same average pooling operation as in the DASPP module. As shown in Table 3, the segmentation accuracy obtained by using the max-pooling operation in the ADPP module is 0.9% higher than that obtained by using the average pooling operation. Therefore, our CA-PP module is more suitable for using the max-pooling operation. In addition, to verify the validity of the dilation rates we set in the ADPP module, we set up two sets of comparative experiments as shown in Table 3. The results show that using different dilation rates has almost no effect on the parameters and inference speed of the model, and the segmentation accuracy obtained by using the dilation rates {3, 6, 9} is the highest, which is 1.1% higher than that obtained by using the dilation rates {2, 4, 8}. In summary, our ADPP moduleâ€™s choice of each component outperforms the other choices in terms of trade-offs between segmentation accuracy, inference speed, and model size.

Table 3 Ablation study for ADPP module on the Cityscapes validation set
Full size table
Ablation Study for AFFU Module. In our AFFU module, ISA and ICA are used to integrate spatial information into high-level features and semantic information into low-level features, respectively. To verify the efficiency of the scheme, we set up two sets of comparative experiments without attention module and only with ICA module. As shown in Table 4, both ISA and ICA modules improve the performance of the network, and the segmentation accuracy obtained with our AFFU module is 1.4% higher than the segmentation accuracy obtained without the attention module. In addition, in our AFFU module, 1Ã—1 convolution is used to reduce the number of channels of the input feature maps. To verify the rationality of this scheme, we replace the two 1Ã—1 convolutions connecting the inputs in the AFFU module with 3Ã—3 convolutions. As shown in Table 4, although the segmentation accuracy obtained by using 1Ã—1 convolution is 0.2% lower than that obtained by using 3Ã—3 convolution, it reduces the number of parameters by 0.13M, which makes our network more lightweight. In summary, our AFFU moduleâ€™s choice of each component outperforms the other choices in terms of trade-offs between segmentation accuracy, inference speed, and model size.

Table 4 Ablation study for AFFU module on the Cityscapes validation set
Full size table
Performance on cityscapes dataset
In this subsection, we first report the performance of our LAANet on the Cityscapes test set, and then compare and analyze LAANet with other state-of-the-art real-time semantic segmentation methods. For a fair comparison, we test the performance of the other methods using the same experimental environment as LAANet and without any testing techniques.

As can be seen in Table 5, although our LAANet has only 0.67M parameters, it achieves the segmentation accuracy of 73.6% mIoU on the Cityscapes test set. Moreover, LAANet can process input images of size 512Ã—1024 with the inference speed of 95.8FPS on a single 1080Ti GPU. Fig. 7 shows a visual comparison of our LAANet with some state-of-the-art methods in terms of segmentation accuracy, inference speed, and model size. It is clear that LAANet obtains higher segmentation accuracy, faster inference and fewer parameters than most state-of-the-art real-time semantic segmentation methods. Specifically, the segmentation accuracy of LAANet is 2% higher than that of JPANet [43] pre-trained on the ImageNet dataset. Although the segmentation accuracy of LAANet is lower than that of LightSeg [42], LAANet obtains nearly 2 times faster inference speed and nearly 13 times fewer parameters. Compared with MLFFNet [40], LAANet obtains higher segmentation accuracy and faster inference speed with about 11 times fewer parameters. The number of parameters of LAANet is close to LRNNet [28], but the segmentation accuracy is improved by 1.4% and the inference speed is improved by 24.8 FPS in the same experimental setting. In conclusion, compared with other state-of-the-art methods, our LAANet achieves an optimal trade-off between segmentation accuracy, inference speed, and model size.

Table 5 Comparison with other methods on the Cityscapes test set
Full size table
Fig. 7
figure 7
Segmentation accuracy, inference speed and model size on the Cityscapes test set. The size of the bubble is proportional to the number of parameters of the model

Full size image
Table 6 Per-class IoU (%) performance on the Cityscapes test set
Full size table
In Table 6, we present the IoU performance for each class on the Cityscapes test set. Our LAANet obtains good segmentation results on most of the classes, and achieves the best segmentation accuracy on 8 classes, such as Sidewalk, Traffic Light and Sky. In particular, LAANet improves the segmentation accuracy by 7.1% and 17.9% on Bus and Train classes, respectively. This fully illustrates that our LAANet is highly efficient. To show the advantages of our LAANet more intuitively, we show the visualization results of LAANet and several other state-of-the-art real-time semantic segmentation methods on the Cityscapes validation set in Fig. 8. By comparing with the ground truth, it can be seen that our LAANet has a good segmentation effect on both large and small targets. As shown by the dashed box in Fig. 8, the segmentation effect of our LAANet on the classes of traffic signs, cars and riders is clearly better than other methods.

Fig. 8
figure 8
Visualization results on the Cityscapes validation set. From top to down: input images, ground truth, predicted results from ERFNet [6], DABNet [8], FDDWNet [15], LEANet [21] and our LAANet

Full size image
Performance on CamVid dataset
In this subsection, to demonstrate the generality of our LAANet, we test the performance of LAANet on the CamVid dataset. Moreover, the effectiveness of LAANet is demonstrated by comparing LAANet with other state-of-the-art real-time semantic segmentation methods.

Table 7 Comparison with other methods on the CamVid test set
Full size table
Fig. 9
figure 9
Segmentation accuracy, inference speed and model size on the CamVid test set. The size of the bubble is proportional to the number of parameters of the model

Full size image
As can be seen in Table 7 and Fig. 9, our LAANet still achieves good segmentation results on the CamVid test set. Specifically, for an input image of size 360Ã—480, our LAANet achieves the segmentation accuracy of 67.7% mIoU and the segmentation speed of 112.5 FPS with 0.67M parameters. Compared with LRNNet [28] with the same number of parameters, our LAANet obtains higher segmentation accuracy and faster inference speed. In terms of the trade-off between segmentation accuracy, inference speed and model size, our LAANet outperforms most of the state-of-the-art real-time semantic segmentation methods. As can be seen in Table 8, our LAANet has good segmentation results on Sky, Road, Building and other classes. Moreover, LAANet achieves the highest segmentation accuracy on 4 classes, namely Car, Sign, Fence and Pole. In addition, it can be seen more intuitively in Fig. 10 that our LAANet also shows excellent performance on the CamVid dataset.

Table 8 Per-class IoU (%) performance on the CamVid test set
Full size table
Fig. 10
figure 10
Visualization results on the CamVid test set. From top to down: input images, ground truth, predicted results from DABNet [8], LEANet [21] and our LAANet

Full size image
Conclusion
In this paper, the difficulty of the research is how to achieve higher segmentation accuracy and faster inference speed under the premise of reducing the number of parameters. To this end, we propose a LAANet for real-time semantic segmentation, which adopts an asymmetric encoderâ€“decoder architecture. In the encoder, we propose an EAB module to extract local and context information. In the decoder, we propose two attention-guided modules, where the ADPP module is used to aggregate multi-scale context information and the AFFU module is used to fuse features from different layers. To verify that our LAANet is efficient, we test the performance of LAANet on the Cityscapes and CamVid datasets and compared it with other state-of-the-art real-time semantic segmentation methods. Experimental results show that our LAANet achieves an optimal trade-off between segmentation accuracy, inference speed, and model size. In future works, we will work on improving the performance of LAANet on different semantic segmentation datasets (Mapillary Vistas, NYUv2, ADE20K, etc.), thereby making LAANet more general and effective.

Attention mechanism
Asymmetric network
Lightweight network
Real-time semantic segmentation