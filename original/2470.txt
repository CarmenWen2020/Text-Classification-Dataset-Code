Electromechanical actuators (EMAs) are safety-critical components that work under various conditions and loads. Realizing robust and precise fault diagnosis for an EMA increases the overall availability/safety of the whole system. However, the monitoring data of an EMA are collected under different working conditions and consist of numerous unlabeled samples and few unbalanced labeled samples; this severely limits the applications of intelligent data-driven diagnosis approaches. Therefore, this paper provides an extended convolutional adversarial autoencoder (ECAAE) as an adversarial model to achieve end-to-end fault diagnosis for EMAs based only on vibration signals. This approach combines the feature extraction ability of convolutional neural networks (CNNs) with the semisupervised learning and data generation capabilities of adversarial autoencoders (AAEs) by activating different submodels during different training phases and is thus able to utilize both unlabeled and unbalanced labeled samples. With the help of a hyperparameter-free signal conversion method and an imbalance-compensation loss function, the adversarial training process of the model results in a feature extractor that is robust to varying working conditions as well as a sample generator capable of generating samples belonging to a given class. Consequently, after fine-tuning on samples rebalanced by the generator, the classifier of the ECAAE is able to perform robust and precise fault diagnosis even under various working conditions, unbalanced samples and few-shot situations. The proposed method is validated under 12 multicondition data scenarios and achieves a diagnostic accuracy above 90%, even in cases worse than 5-way 5-shot scenarios, thereby revealing its superiority over 3 state-of-the-art models.

Introduction
With the popularity of â€˜all-electricâ€™ and â€˜fly-by-wire controlsâ€™ concepts, electromechanical actuator (EMA) systems have been widely employed in various applications, including industrial process control, robotics, military air/land vehicles and commercial aircraft [1, 2]. In many of these applications, especially in the aerospace industry, actuators are safety-critical components, and an undetected failure might lead to a serious disaster [3]. Although EMAs are attractive for reducing overall weight and improving maintainability, there has not been much experiential research accumulated with regard to their reliability levels and fault modes, such as in the case of hydraulic actuators [4, 5]. Hence, realizing effective and robust fault diagnosis for an EMA that can detect and classify incipient failures is critical for reducing operating and maintenance costs and will increase the overall availability and safety of the whole system.

The basic approaches for the fault diagnosis of EMAs are usually divided into data-driven and model-based approaches. Model-based diagnosis is usually strongly linked to the fundamentals of the given equipment and can identify faults in a straightforward manner from a physical perspective, thereby reaching high levels of accuracy and interpretability [6]. However, most model-based techniques require a specialist modeler to establish a highly accurate mathematical reference model for the system, and this limits its application to a specific machine. The complexity of the model increases with the complexity of the system, resulting in low computational performance and poor portability. In contrast, data-driven methods are good at establishing fault modes without any prior explicit models [7]. They learn and model directly through monitoring data to provide insight into the health condition of the machine. Signals of actuator positions; vibrations; and motor currents, voltages and temperatures contain abundant health information for the EMA, and plenty of approaches have successfully diagnosed EMAs by making full use of these data [8,9,10,11]. However, when applied in actual industrial scenarios, data-driven approaches encounter the following challenges:

(1)
Different and varying working conditions: the state of an EMA in real industrial scenarios varies greatly due to the impacts of uncertain factors, such as different loads and random ambient noise, which make the training samples and testing samples quite different under the same level of system health. Many methods fail to extract features that are immune to the working conditions, resulting in poor diagnostic performance and generalization capabilities.

(2)
Unsatisfactory data: EMAs are usually used in high-reliability and safety-critical equipment, which severely limits the opportunity to obtain enough faulty samples through fault injections or reliability tests. Meanwhile, in real industrial scenarios, obtaining accurate sample tags requires additional costs. Therefore, in the early stage of EMA operation, there are usually many of unlabeled samples along with a few labeled samples. Moreover, these small amounts of labeled samples are usually imbalanced. Most of them are in a normal state, and the number of samples between different failures varies greatly. This few-shot class-imbalanced problem will stop most of the data-driven approaches from functioning well [12].

Several studies have been conducted to solve some of the above-mentioned problems. Ruiz-Carcel et al. [8] diagnosed linear actuators using fault features extracted from current and position signals, and feature combination and dimensionality based on principal component analysis (PCA) were employed to provide consistent health indicators under different operating conditions. Liu et al. [9] employed multifractal detrended fluctuation analysis (MFDFA) and variational mode decomposition (VMD) to extract fault features that are immune to variable working conditions from vibration signals. However, the feature extractions in these traditional data-driven approaches are achieved manually. These processes rely considerably on prior knowledge, diagnostic expertise and complex signal transformation techniques, which are labor-intensive and time-consuming, and the diagnostic performances heavily depend on the chosen fault features.

Deep learning (DL) has emerged as a solution for minimizing the reliance on prior knowledge and realizing intelligent fault diagnosis in an end-to-end manner. Based on the ability to learn high-order representation of samples, DL models can automatically process the original signals and learn dominant features layer by layer [13]. With sufficient training data, these models have demonstrated their superiority in many diagnostic applications for EMAs. Yang et al. [14] applied a modified long short-term memory (LSTM) model for EMA fault detection and isolation; their approach achieved superior performance to traditional methods. Yang N et al. [15] stacked several denoising autoencoders (DAEs) and introduced a greedy algorithm for pre-training to realize accurate fault diagnosis for EMAs. However, both of these methods implement fault diagnosis based on multiple signals, which increases the training burden of the model and introduces interference due to sensor faults. More importantly, these methods can only achieve competitive performance with sufficient and balanced samples, which are luxuries in most fault diagnosis scenarios involving EMAs.

To address the few-shot and unbalanced data problems, the two main solutions are the optimization of the recognition model and the enhancement of the utilized samples. Among basic deep learning algorithms, the convolutional neural network (CNN) is one of the most dominant recognition models, especially when the condition of the given data is poor. The key to the success of a CNN lies in its strong feature extraction and multidimensional fusion capabilities that can be used to exploit the failure symptoms hidden deeply in the monitoring signal. Riaz et al. [10] proposed a 2D-remnant CNN and an optimized softmax model for the diagnosis of ball screw EMAs using motor current signals and achieved improved feature extraction and classification under varying working loads and in cases with unbalanced samples. Peng et al. [16] presented a novel upsampling operation to improve the performance of CNNs and achieved high detection accuracy and real-time performance capability. Siahpour et al. [17] utilized the unsupervised parallel data and realized EMA diagnosis across different sensors through a CNN-based domain adaptation approach. However, none of the above methods have solved the diagnosis issue under small-sample conditions.

With its powerful generation capability, the generative adversarial network (GAN) [18] is a promising way to solve the few-shot and unbalanced class problems in the area of vibration signal-based fault diagnosis [19]. The major drawback of the GAN-based method is that the adversarial training process of the traditional GAN is tricky and often fails to reach global optima. Adversarial autoencoders (AAEs) [20] are used to convert autoencoders into generative models, and they are easier to train than traditional GANs in many scenarios. This approach combines the semisupervised learning ability of autoencoders (AEs) with the generation ability of GANs and has been verified in many pattern recognition applications as being able to capture the data manifold better than other generation approaches. In recent years, several GAN-based models have been proposed in the field of bearing diagnosis. Wang et al. [21] designed a conditional variational autoencoder-based generative adversarial network (CVAE-GAN) to address the imbalanced sample problem under different working conditions. Pan et al. [22] proposed an unsupervised convolutional AE for fault recognition with unseen classes. However, there has not been a study that solves all the above-mentioned problems in an integrated end-to-end model, especially in the field of EMA fault diagnosis.

Therefore, an inspiring approach is to combine the feature extraction ability of CNNs with the semisupervised learning and data generation capabilities of AAEs to realize robust fault diagnosis for an EMA under varying working conditions, unbalanced samples and few-shot situations. In this study, we propose an extended convolutional adversarial autoencoder (ECAAE) model as an end-to-end data-driven approach for diagnosing EMAs using vibration signals only. First, all the vibration signals obtained by a 3-axis acceleration sensor are directly converted into RGB channel images by a concise method without any hyperparameters, making it suitable for a two-dimensional CNN and keeping it end-to-end. Second, different parts of the ECAAE are activated alternatively during the training process and optimized with proper loss functions, making the model switch between the basic AE and GANs and giving it the ability to utilize both the numerous unlabeled samples and the few labeled, unbalanced samples. Thus, a feature extractor that is immune to varying working conditions is acquired, and a sample generator that is able to generate samples belonging to a given class based on various working conditions is obtained. Finally, the classification part of the model is fine-tuned by the samples that are enhanced and rebalanced by the generator, thereby enabling the model to achieve robust and precise diagnostic performance. Experimental analyses are conducted on an open access dataset from the flyable electromechanical actuator (FLEA) testbed [23] to demonstrate the effectiveness and feasibility. The main insights and contributions of this study can be summarized as follows:

(1)
To overcome the data hurdle facing data-driven approaches, the proposed ECAAE model combines the feature extraction ability of CNNs with the semisupervised learning and data generation capabilities of AAEs by activating different submodels during different training phases. An extended fine-tuning phase is designed to fully utilize the sample generation capabilities obtained in the previous three training stages to achieve data augmentation. After fine-tuning based on the rebalanced dataset, the model performs robust and precise fault diagnosis for EMAs under varying working conditions and for unbalanced sample and few-shot situations.

(2)
To keep the ECAAE model as an end-to-end approach, a signal conversion method is introduced that converts all vibration signals directly into RGB channel images without relying on any hyperparameters, thereby realizing the feature fusion of the vibration signals along 3 axes and retains the greatest amount of useful information possible. Moreover, the training procedure and loss functions are well designed to minimize the effects of the newly introduced hyperparameters.

(3)
Experimental validations and comparative analyses are conducted based on twelve data scenarios to demonstrate the superiority of the proposed model and the improvements provided by the corresponding architecture. Visualizations of the learned features and generated samples further illustrate the keys to their success.

Background theory
CNN
A CNN is a multilayer artificial neural network for supervised learning that uses convolution operations instead of matrix multiplication. It has demonstrated its superiority in the applications involving image recognition and classification. The key to its success is its usage of shared weights and local connections, as they help reduce the risk of overfitting and the number of weights as well as make the model easy to optimize. These advantages are more significant if the input of the model is a multidimensional picture, as this type of sample can be directly processed, thereby avoiding the data reconstruction processes and complicated feature extraction in traditional recognition models. Some widely used CNN models are LeNet-5, AlexNet, VGGNet, ResNet and so on [12].

WGAN-GP
A basic GAN consists of a discriminator and a generator. Inspired by the two-player zero-sum game, the two subnetworks are forced to compete with each other to achieve the best generation effect [18]. The generator aims to extract the potential distributions from the given training data and create samples that follow these distributions. Correspondingly, the role of the discriminator is to discriminate whether the samples received are original or generated samples. The goal of the training process for a GAN is to reach Nash equilibrium between the two subnetworks. This zero-sum game can be formalized as:

minğºmaxğ·ğ¿(ğ·,ğº)=ğ¸ğ‘¥âˆ¼ğ‘ğ‘‘ğ‘ğ‘¡ğ‘[log(ğ·(ğ‘¥))]+ğ¸ğ‘§âˆ¼ğ‘ƒğ‘§[log(1âˆ’ğ·(ğº(ğ‘§)))]
In the equations above, ğ¿(ğ·,ğº) is the overall objective function. ğ‘¥âˆ¼ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘ indicates that the input ğ‘¥ originates from the real training set. ğ‘§ is an input vector sampled from distribution ğ‘ƒğ‘§. The two subnetworks are alternately trained by minibatch gradient descent until they reach Nash equilibrium. Finally, the generator learns to generate samples that follow the distribution of the real samples.

The training process of the traditional GAN is tricky and often fails to reach the global optimum. Traditional GANs usually suffer from â€˜mode collapseâ€™ due to their hyperparameter settings, where the model becomes stuck and generates only one specific mode of data. The key reason for this phenomenon lies in the fact that the divergent traditional minimized GANs are potentially not continuous with respect to the generator parameters [24]. The Wasserstein-1 (also called Earth-Mover) distance ğ‘Š(â„™ğ‘Ÿ,â„™ğ‘”) is a promising metric for use in the above problems, and under mild assumptions, it is continuous and differentiable almost everywhere. The distance can be informally interpreted as the minimum â€˜massâ€™ that must be transported to convert distribution â„™ğ‘Ÿ to distribution â„™ğ‘”.

ğ‘Š(â„™ğ‘Ÿ,â„™ğ‘”)=infğ›¾âˆˆÎ (â„™ğ‘Ÿ,â„™ğ‘”)ğ”¼(ğ‘¥,ğ‘¦)âˆ¼ğ›¾[â€–ğ‘¥âˆ’ğ‘¦â€–]
where Î (â„™ğ‘Ÿ,â„™ğ‘”) indicates the set of all joint distributions ğ›¾(ğ‘¥,ğ‘¦) whose marginals are â„™ğ‘Ÿ and â„™ğ‘”. With some necessary transformations, the adversarial training process is able to present with the Kantorovichâ€“Rubinstein duality as

minğºmaxğ·ğ”¼ğ‘¥âˆ¼ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘[ğ·(ğ‘¥)]âˆ’ğ”¼ğ‘§âˆ¼ğ‘ƒğ‘§[ğ·(ğº(ğ‘§))]
in which the discriminator ğ· must be a 1-Lipschitz function. Weight clipping is used in the original WGAN to implement the Lipschitz constraint. However, it may induce optimization difficulties or lead the optimized discriminator to have a pathological value surface. To circumvent these tractability issues, a soft version of the constraint, the gradient penalty approach [25], is widely used as an alternative method. Thus, the final objective of WGAN-GP is

ğ¿=ğ”¼ğ‘§âˆ¼ğ‘ƒğ‘§[ğ·(ğº(ğ‘§))]âˆ’ğ”¼ğ‘¥âˆ¼ğ‘ƒğ‘‘ğ‘ğ‘¡ğ‘[ğ·(ğ‘¥)]+ğœ†ğ”¼ğ‘¥Ì‚ âˆ¼ğ‘ƒğ‘¥Ì‚ [(â€–âˆ‡ğ‘¥Ì‚ ğ·(ğ‘¥Ì‚ )â€–2âˆ’1)2]
where ğœ† is a dimensionless parameter.

Basic AAE
An adversarial autoencoder is a general approach that uses an autoencoder in a generative way; it is similar in spirit to the variational autoencoder (VAE). By forcing the aggregated posterior ğ‘(ğ‘§) to match with an arbitrary prior distribution ğ‘(ğ‘§), a satisfactory generation result is ensured for meaningful samples. However, a GAN is used instead of the KL divergence penalty to perform variational inference, thereby overcoming the drawbacks of VAEs in requiring the exact functional form of the prior distribution. It has been verified for many datasets that AAEs can capture the data manifold better than VAEs [20].

The architecture of a basic AAE is shown in Fig. 1. It is basically a standard autoencoder with a discriminator attached on the hidden code z. The encoder ğ‘(ğ‘§|ğ‘¥) also acts as a generator to form a GAN which aims to force the aggregated posterior to match with a given prior. The model is trained alternatively in two phases. During the first phase, reconstruction, the AAE performs as a standard autoencoder, trained to reconstruct the inputs. During the second phase, regularization, the model performs as a normal GAN, and its discriminator and generator (encoder) are trained until achieving the final Nash equilibrium.

Fig. 1
figure 1
Architecture of a basic AAE

Full size image
ECAAE-based intelligent fault diagnosis method
To overcome the data hurdles and realize end-to-end data-driven EMA fault diagnosis in industrial scenarios, an ECAAE-based method that combines the data generation capabilities of AAEs with the robust feature extraction capabilities of CNNs is proposed and detailed in this section. As shown in Fig. 2, the basic advantage of this method is that it avoids the excessive intervention of domain expert knowledge and can realize diagnosis based only on vibration signals. Moreover, due to the condition and class constraints introduced in the adversarial training process and the sample generation capabilities provided by GANs, the model is able to achieve high-precision fault diagnosis under few-shot class-imbalanced situations. The method is suitable for fault diagnosis of different models of EMAs, especially for those in newly developed equipment, where fault data are limited at the initial stage of operation. Meanwhile, with only vibration signals required, this is a low-cost monitoring approach that is suitable for equipment operating in varying conditions with limited monitoring sensors. The main limitation of the method is the inability to diagnose unknown failure modes. In addition, as a typical adversarial model, the ECAAE-based approach involves a training process that consumes more time and computing resources than most neural networks.

Fig. 2
figure 2
Applicability of the proposed ECAAE model

Full size image
First, the architecture of the entire ECAAE model is presented. Second, the data preprocessing method that transforms vibration signals into images is provided. Thereafter, the designs of different loss functions are detailed. Finally, the training strategy used by the adversarial network is introduced.

Architecture of the ECAAE model
The ECAAE model combines the unsupervised learning capabilities of AEs, the powerful feature extraction capabilities of CNNs and the outstanding data generation capabilities of GANs. The architecture of the entire network is based on a semisupervised AAE and is shown in Fig. 3. It contains four subnetworks: an encoder that is also a generator, a decoder and two discriminators.

Fig. 3
figure 3
Architecture of the proposed ECAAE model

Full size image
The encoder ğ‘(ğ‘§,ğ‘¦|ğ‘¥) is designed to extract the features from the given input image that are converted from the vibration signals and compress them into two independent components, namely a label code ğ‘¦ and a condition code ğ‘§. Label code ğ‘¦ is a latent class variable that represents different fault modes and comes from a categorical prior distribution. Condition code ğ‘§ is a latent continuous variable that reflects the varying working conditions and comes from a continuous prior distribution. The two latent codes are then concatenated as the input of the decoder ğ‘(ğ‘¥|ğ‘§,ğ‘¦) to reconstruct the original figure. In this way, the feature extraction ability is improved by constructing a basic autoencoder to make full use of the large amount of unlabeled data.

Two discriminators are combined with the same generator, the encoder, to form two GANs. The top GAN in Fig. 3 aims to force the aggregated posterior distribution of the latent vector y to match with the given categorical distribution, to ensure that the code y does not contain information that is irrelevant to the category. Meanwhile, the bottom GAN forces the hidden code z to match with the given continuous distribution, which represents the varying working condition. In this way, by splitting the latent code into two independent components, the model acquires a feature extraction ability that is immune to varying working conditions, and its anti-noise ability is also improved. Furthermore, the generation ability of the GAN is effectively exploited so that its feature extraction ability for few-shot learning is improved.

To enhance the feature extraction capabilities, the encoder/generator is built based on a CNN. It stacks several pairs of convolutional and max-pooling layers and concatenates them with 2 branches of dense layers. Correspondingly, the decoder starts with a dense layer and stacks several transposed convolution layers and batch-normalization layers. To avoid modal collapse and stabilize the training process, the two discriminators are constructed with a basic DNN and form two WGAN-GPs.

After training, the decoder ğ‘(ğ‘¥|ğ‘§,ğ‘¦) works as a sample generator to rebalance the labeled training sets, and the ğ‘(ğ‘¦|ğ‘¥) part of the encoder works as the final classifier. The design of the above-mentioned structure guarantees the basic diagnostic performance of the model, but the capability of a specific ECAAE changes to a certain extent according to the settings of the hyperparameters, such as the sizes of the convolution and transposed convolution layers, number of layers, dropout and normalization techniques, and activation functions. In this work, the optimal architecture is obtained through extensive comparative experiments, which is detailed in Sect. 4.4.5.

End-to-end signal conversion method
CNNs have achieved many excellent achievements in the areas of image identification and target tracking, where the input data are two-dimensional. To apply CNNs to fault diagnosis tasks based on one-dimensional vibration signals, there are usually two solutions: using a 1D CNN or continuing to use 2D CNN but with an approach to convert 1D vibration signals into 2D data [12]. A 1D CNN is an end-to-end approach, but for many cases with more padding and normalization techniques, 2D CNNs achieve better diagnostic performances. Therefore, in this paper, to achieve diagnosis based on a 2D CNN, a concise method is used to directly convert all the vibration signals obtained by the 3-axis acceleration sensor into an RGB channel image.

Through the proposed conversion method, the time-domain vibration signals on each axis can be reshaped and normalized into a particular channel and then stacked to form an RGB image, as shown in Fig. 4. Usually, due to the convolution and padding operations, we want the image sent to a CNN to be square. Therefore, we set the length of a given sample to ğ‘2 by cropping the samples or processing them with an ğ‘2-length sliding window. In diagnostic tasks based on vibration signals, the acceleration sensor is usually set with a high sampling frequency. Therefore, a sample has sufficient data points, and abandoning a small number of data points does not affect the diagnostic performance of the model.

Fig. 4
figure 4
Flowchart of the introduced signal conversion method

Full size image
Given a sample ğ‘‹(ğ‘™,ğ‘), ğ‘™=1,2,...,ğ‘2, ğ‘=0,1,2 (stands for ğ—â†’,ğ˜â†’,ğ™â†’ axes), the pixel strength ğ‘™ğ¼(ğ‘–,ğ‘—,ğ‘) of the converted image is as follows: ğ¼(ğ‘–,ğ‘—,ğ‘)=ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘{ğ‘‹((ğ‘–âˆ’1)Ã—ğ‘+ğ‘—,ğ‘)âˆ’ğ‘€ğ‘–ğ‘›(ğ‘‹(ğ‘))ğ‘€ğ‘ğ‘¥(ğ‘‹(ğ‘))âˆ’ğ‘€ğ‘–ğ‘›(ğ‘‹(ğ‘))Ã—255}, where ğ‘–=1,2,...,ğ‘, ğ‘—=1,2,...,ğ‘, ğ‘=0,1,2, and ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘(â‹…) is the rounding function.

This method eliminates the requirement of expert knowledge by avoiding determining the axis on which the vibration signals are used, and it can be calculated without any hyperparameters.

Training procedure and loss function design
After conversion into images, the samples are sent to the ECAAE for training or diagnosis. The whole training procedure includes four phases, as shown in Fig. 5. The first three phases, the reconstruction, the regularization and the semisupervised classification phases, alternate between different minibatches, while the extended fine-tuning phase is executed at last. To overcome the problem of imbalanced data and stabilize the training process, different subnetworks are activated in each phase and optimized with proper loss functions.

Fig. 5
figure 5
Training procedure of the ECAAE

Full size image
The reconstruction phase
In this phase, the model is activated as a basic AE and updated to minimize the reconstruction error. As an unsupervised training process, the class imbalance problem does not need to be considered. Moreover, the purpose of this model is to obtain improved classification results, so there is no need to pursue excessively high reconstruction performance. Therefore, the loss function used in this phase is the binary cross-entropy loss.

îˆ¸ğ´ğ¸=âˆ’âˆ‘ğ‘–=1ğ‘€(ğ‘¥ğ‘–log(ğ‘¥Ë†ğ‘–)+(1âˆ’ğ‘¥ğ‘–)log(1âˆ’ğ‘¥Ë†ğ‘–))
where ğ‘€ is the size of the sample, ğ‘¥ is the input sample, and ğ‘¥Ë† is the corresponding reconstructed sample.

The regularization phase
In this phase, the encoder and the two discriminators, ğ·ğ¶(ğ‘§) and ğ·ğ¿(ğ‘¦), are activated as two separate GANs, with each capturing condition and label information. The two GANs are optimized on a minibatch of unbalanced labeled data. The adversarial network composed of ğ‘(ğ‘¦|ğ‘¥) and ğ·ğ¿(ğ‘¦) is trained to ensure that the label part ğ‘¦ of the latent code does not carry any condition information. In this unbalanced class scenario, ğ‘ƒğ¶ğ‘ğ‘¡ is constructed into the same unbalanced categorical distribution as that of the labeled input samples. Therefore, the feature extraction capabilities that are enhanced in this regularization phase will not be disturbed by the class imbalance problem. Moreover, the other adversarial network, composed of ğ‘(ğ‘§|ğ‘¥) and ğ·ğ¶(ğ‘§), imposes a continuous distribution ğ‘ƒğ‘ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› on the condition component ğ‘§ of the latent code, which makes the feature extraction capabilities more robust under varying working conditions and noise. The continuous distribution ğ‘ƒğ‘ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› can be a priori distribution related to the working conditions, if provided, or simplified as a Gaussian mixture distribution.

Since two GANs are optimized together in this phase, the training process is very unstable. To avoid model collapse and stabilize the training process, the gradient penalty and the Wasserstein distance are introduced, and the loss function and training procedure are modified.

First, the two discriminators are updated with the combined loss function îˆ¸ğ· simultaneously on the labeled minibatch to distinguish the authenticity of each latent code.

îˆ¸ğ·=ğ”¼ğ‘¦Ìƒ âˆ¼ğ‘ƒğ‘¦[ğ·ğ¿(ğ‘¦Ìƒ )]âˆ’ğ”¼ğ‘¦âˆ¼ğ‘ƒğ¶ğ‘ğ‘¡[ğ·ğ¿(ğ‘¦)]+ğœ†ğ”¼ğ‘¦Ì‚ âˆ¼ğ‘ƒğ‘¦Ì‚ [(â€–â€–âˆ‡ğ‘¦Ì‚ ğ·ğ¿(ğ‘¦Ì‚ )â€–â€–2âˆ’1)2]+ğ”¼ğ‘§Ìƒ âˆ¼ğ‘ƒğ‘§[ğ·ğ¶(ğ‘§Ìƒ )]âˆ’ğ”¼ğ‘§âˆ¼ğ‘ƒğ¶ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›[ğ·ğ¶(ğ‘§)]+ğœ†ğ”¼ğ‘§Ì‚ âˆ¼ğ‘ƒğ‘§Ì‚ [(â€–âˆ‡ğ‘§Ì‚ ğ·ğ¶(ğ‘§Ì‚ )â€–2âˆ’1)2]
where ğ‘ƒğ¶ğ‘ğ‘¡ denotes the real categorical distribution, ğ‘ƒğ‘ğ‘œğ‘›ğ‘¡ğ‘–ğ‘‘ğ‘–ğ‘œğ‘› denotes the real continuous distribution, ğœ† is a dimensionless parameter, and ğ‘¦Ì‚  is sampled from the generated code ğ‘¦Ìƒ  and real code ğ‘¦ with ğ›¼ uniformly sampled between 0 and 1, and

ğ‘¦Ì‚ =ğ›¼ğ‘¦Ìƒ +(1âˆ’ğ›¼)ğ‘¦.

Additionally, ğ‘§Ì‚  is obtained in a corresponding way.

Then, the encoder is updated with a combined loss function îˆ¸ğ‘ on the same minibatch to confuse each discriminator.

îˆ¸ğ‘=ğ”¼ğ‘¦âˆ¼ğ‘ƒğ¶ğ‘ğ‘¡[log(1âˆ’ğ·ğ¿(ğ‘¦Ìƒ ))]+ğ”¼ğ‘§âˆ¼ğ‘ƒğ¶ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›[log(1âˆ’ğ·ğ¶(ğ‘§Ìƒ ))]
The semisupervised classification phase
In this phase, ğ‘(ğ‘¦|ğ‘¥), the part of the encoder that generates labels, is updated on the imbalanced and labeled minibatch. Since there are far more normal samples than faulty samples in most cases, the class imbalance problem will overwhelm the training process and degrade the performance. Therefore, the loss function îˆ¸ğ‘(ğ‘¦|ğ‘¥) is modified based on the focal loss [26].

îˆ¸ğ‘(ğ‘¦|ğ‘¥)=âˆ’âˆ‘(1âˆ’ğ‘ğ‘–)ğ›¾log(ğ‘ğ‘–)
where ğ›¾â‰¥0 is a tunable focusing parameter and ğ‘ğ‘– is the estimated probability for each category.

The extended fine-tuning phase
In this phase, the dense layers that act as the classification part of the encoder are fine-tuned on an extended rebalanced minibatch. This minibatch is obtained by mixing the unbalanced minibatch of real data with a fake minibatch generated from the decoder using the rebalanced label code and random conditional code. This fine-tuning procedure exploits the directional generation capability of the decoder and further improves the diagnostic performance under varying working conditions and class imbalance. Since the classes of the extended minibatch are rebalanced, the basic categorical cross-entropy loss is used in this phase.

îˆ¸ğ‘â€²=âˆ’âˆ‘ğ‘¦ğ‘–logğ‘¦ğ‘–Ë†
The adversarial training strategy
Training the proposed ECAAE is much easier than training a traditional AAE, but to obtain optimal results, optimization of the training strategy is still necessary. First, since unlabeled data are generally much larger than labeled data, the model is likely to focus on the reconstruction results and ignore the classification performance of the model. Therefore, both datasets should be divided into minibatches of the same size, and the first three phases should be processed alternately on these minibatches. Second, to stabilize the adversarial training process, the learning rate should be gradually reduced as training progresses. Finally, if the number of labeled samples is rather small, an early stopping criterion should be introduced to avoid overfitting during the extended fine-tuning phase.

Figure 6 shows the overall data flow of the diagnostic approach based on ECAAE. High-performance fault diagnosis is achieved under nonideal data conditions through an iterative training process that integrates the contributions of each subnetwork. First, the proposed signal conversion approach is used to convert vibration data into images for the 2D CNN. Then, the AE part of the model is used to learn distribution information from unlabeled samples, and the GAN parts are used to improve the modelâ€™s immunity to working conditions. Moreover, the proposed extended focal loss is used to reduce interference of the class imbalance problem, and data enhancement and classification performance improvement are achieved through extended fine-tuning. Finally, the CNN-based encoder works as the classifier to diagnose EMA faults.

Fig. 6
figure 6
Flowchart of the ECAAE-based diagnosis

Full size image
Experimental results and analysis
The effectiveness of the ECAAE-based diagnostic approach is verified with the experimental EMA data provided by the NASA Ames Research Center. Specific experimental analyses regarding imbalanced classes, few-shot diagnosis, varying working conditions and hyperparameters are detailed in this section.

Data description
Figure 7 shows the FLEA testbed of the NASA Ames Research Center. The dataset is open access, and further details regarding the testbed can be found on the NASA Ames website [23].

Fig. 7
figure 7
The FLEA testbed and corresponding model

Full size image
The vibration signals used in this paper are acquired from accelerometers installed on the nut of a ball screw mechanism. The sampling frequency is 20 kHz. The experiments are conducted under five health states: ball screw return channel jam, position sensor failure, motor failure, screw surface spall, and normal. The data are collected under 14 working conditions, as detailed in Table 1.

Table 1 Working conditions in the FLEA experiments
Full size table
The data distribution under each working condition is visualized by t-distributed stochastic neighbor embedding (t-SNE), as shown in Fig. 8. Only 100 samples are drawn for each condition, and labels 0, 1, 2, 3 and 4 represent nominal, jam, spall, motor and position, respectively. The figure indicates that the distribution of each failure mode varies significantly under different working conditions, and these distributions are even intertwined under certain working conditions. This means that for this FLEA dataset, data-driven fault diagnosis under varying working conditions is a tricky task.

Fig. 8
figure 8
Data distribution under each working condition visualized by t-SNE

Full size image
Experimental setup
Division of datasets
To achieve rapid fault diagnosis in a way that is suitable for real scenarios, we use 2000 points along each axis (0.1 s) as a sample. To verify the capabilities of the model under varying working conditions, the FLEA data are divided into three scenarios with increasing diagnosis difficulty, as shown in Table 2. Scenario A is the easiest task; although the working conditions of the testing set vary, all have appeared before. The difficulty of scenario B increases because although many working conditions have appeared before, the working conditions of the testing set are all new. Finally, scenario C is the hardest task in which the testing set is acquired under a large number of unknown working conditions. Furthermore, each scenario is divided into four subsets that have different types of unbalanced classes or few-shot issues, as shown in Table 3. The imbalanced classes of the labeled training sets are detailed in the table, where labels 0, 1, 2, 3 and 4 represent nominal, jam, spall, motor and position, respectively. The testing sets are all balanced classes, allowing evaluation of the real diagnosis performance.

Table 2 Division of working conditions
Full size table
Table 3 Number of samples in each dataset
Full size table
Parameters of the ECAAE
As described in Sect. 3.1, the ECAAE consists of four subnetworks: a convolutional encoder, a transposed convolutional decoder and two dense discriminators that share the same structure. The parameters that have certain impacts on the diagnostic performance include the types of CNN models, sizes of the convolution and transposed convolution layers, numbers of layers, dropout and normalization techniques, activation functions and learning rates. The input unit size of the encoder (generator) is determined by the sample size. In addition, the sizes of the input units of the decoder and discriminators are determined by the length of the label code and condition code. Because the encoder is designed to effectively extract local features, the number of convolution kernels should increase as the layer deepens. In contrast, the numbers of transpose convolution kernels of the decoder and discriminators should decrease as the layer deepens. To focus on more subtle features, the size of the kernels will be set to a relatively small value, usually 3*3, 4*4 or 5*5. The specific size should be determined in combination with the strides and padding to ensure that the reconstruction output size is consistent with the input sample. The types of CNN model and the prior continuous distribution of the conditions are the key parameters of the architecture and are obtained through extensive comparative experiments, which are detailed in Sect. 4.4.4. The batch size, learning rates, optimizer and activation functions are determined with the help of an optimization technique called GridSearchCV in Scikit-Learn.

Tables 4, 5 and 6 show the architecture of the ECAAE finally used for the FLEA data. The fundamental architecture of the encoder is set to VGG13 and ends with two branches of dense layers. The prior continuous distribution of the conditions is set to a ten 2D Gaussian mixture distribution. The batch size is 32. The maximum training and fine-tuning epochs is set to 100, and the optimizer is Adam. During the first three phases, the learning rate in the first 50 epochs is 1ğ‘’âˆ’5 and declines to 1ğ‘’âˆ’6 thereafter. During the extended fine-tuning phase, the learning rate of the classifier is 1ğ‘’âˆ’4, and the early stopping technique is used to avoid overfitting. The model and experiments are implemented using the TensorFlow and Keras frameworks.

Table 4 The parameters of the encoder/generator
Full size table
Table 5 The parameters of the decoder
Full size table
Table 6 The parameters of the two discriminators
Full size table
Baseline methods
The following SOTA fault diagnosis methods are utilized in different scenarios for comparison purposes.

The deep neural network (DNN)-based fault diagnosis model was proposed by Lei et al. [27]. The input features are normalized Fourier coefficients. DNNs with 256, 128, 64, 32 and 5 neurons in each hidden layer are trained for EMA fault diagnosis. Additionally, the datasets sent to the model are rebalanced by the synthetic minority oversampling technique (SMOTE) [28].

The VMD-MFDFA-PNN (VMP) approach was proposed by Liu et al. [9]. The input fault features are generalized Hurst exponents, and PCA is introduced to reduce the feature dimensions. The classifier is based on PNN. Similar to the previous method, oversampling based on the SMOTE is used.

The stacked denoising autoencoder (SDA)-based diagnostic model was proposed by Lu et al. [29]. This autoencoder contains 3 hidden layers, which have 400, 200 and 100 neurons. In this EMA fault diagnosis task, the autoencoder is pre-trained for 100 epochs and fine-tuned for another 100 epochs with early stopping.

The stacked autoencoder diagnosis method based on a global optimization GAN (GOGAN-SAE) was proposed by Zhou et al. [30]. Aiming to solve unbalanced data problems, the generator in this method is designed via a global optimization scheme. The autoencoder constructed for the EMA diagnosis task contains 6 hidden dense layers, which have 400, 800, 400, 200, 100 and 5 neurons. Both the generator and discriminator are DNNs and are optimized with a learning rate of 0.001.

Moreover, to demonstrate the contribution of the ECAAE architecture and find the optimal subnetwork, some models that have the same parameters as those in the proposed model are also executed on the FLEA data. For example, a basic VGG-type classification model with the proposed signal conversion method, a 1D convolutional VGG-type classification model that directly processes the vibration signals, and the ECAAE with an LeNet-5 or ResNet encoder are used.

Diagnosis results and comparative analysis
The diagnosis results under different data conditions are presented in this section. Additionally, several comparative experiments are conducted to demonstrate the superiority of the ECAAE model. First, the diagnostic performance is compared based on 12 datasets. The proposed method displays overall superiority in both supervised learning and semisupervised learning tasks. Then, in Sect. 4.3.1, the capability of the model to extract features not influenced by working conditions is clarified through comparisons of diagnostic performance fluctuations under different working condition scenarios and further illustrated through t-SNE visualization. Subsequently, in Sect. 4.3.2, the sample generation ability of the ECAAE is visualized by t-SNE and compared with that of other models. The sample generation quality is further evaluated by four statistical indicators. Then, in Sect. 4.3.3, the signal conversion method is evaluated through comparison analysis. Moreover, in Sect. 4.3.4, the diagnostic performance of several networks with or without the proposed architecture is compared to clarify the improvement provided by the architecture. Finally, the impacts of the hyperparameters are analyzed in Sect. 4.3.5.

Both the proposed ECAAE and the baseline methods are validated on all twelve datasets with fivefold cross-validation. In addition, an extra ECAAE model is implemented under a supervised mode for comparison with three supervised learning models: the DNN, VMP and GOGAN-SAE. In this supervised mode, the input of the ECAAE in the reconstruction phase is also the labeled minibatch, and the focal loss is used as the reconstruction loss. Table 7 shows the average accuracy and the standard deviation of each diagnosis result.

Table 7 Diagnosis accuracies (%) for the FLEA dataset
Full size table
The diagnosis results indicate that the ECAAE outperforms all of the comparison models on all twelve datasets. By comparing the performances of the same model on different datasets, it can be seen that as the data conditions become worse (more changes in working conditions, fewer labeled samples, and more imbalanced categories), the diagnostic performance of the model gradually decreases. For the 12th dataset with large changes in the working conditions, few labeled data, and unbalanced categories, the classification accuracies of the DNN and VMP methods are less than 60%, which means that they are unable to perform fault diagnosis effectively under these data conditions. In contrast, even conducted only on the labeled training set, the proposed ECAAE model can still achieve a diagnostic accuracy of 87.32%. With the help of unlabeled samples, the accuracy can be further improved to 91.77%. Comparing the performances of the ECAAE in the pure supervised mode and semisupervised mode on all 12 datasets, the diagnostic accuracy increases by an average of 2.18%, and the standard deviation is reduced by an average of 1.28%. This proves that the robustness and generalization performance is further improved by making full use of unlabeled samples during the reconstruction phase. Moreover, compared to other SOTA models, the proposed model not only achieves the highest diagnosis accuracy but also maintains low performance fluctuations. The standard deviations of different datasets and each cross-validation result shown in Table 7 indicate the robustness of the model to the different nonideal data conditions. The confusion matrix of the ECAAE obtained with the 12th dataset is shown in Fig. 9. It further confirms that even under harsh data conditions (worse than the 5-way 5-shot scenario), the proposed ECAAE can still effectively perform fault diagnosis.

Fig. 9
figure 9
Confusion matrices of a the supervised ECAAE and b semisupervised ECAAE based on the 12th dataset

Full size image
Feature extraction capability
The generalization performance of each model under varying working conditions can be revealed by comparing the diagnosis results yielded under the same amount of data and the same imbalance condition. A model that uses deep learning to directly extract features from raw signals not only achieves a higher accuracy than a model that uses a manually set feature extraction method but also yields a smaller standard deviation due to the changes in working conditions. For example, on datasets 1â€“3, the standard deviations of the DNN and VMP accuracy changes caused by scenarios with different conditions are 1.45% and 1.07%, while those of the GOGAN-SAE and ECAAE are only 0.31% and 0.54%, respectively. We further use t-SNE to visualize the distributions of the features extracted by the VMD-MFDFA of the VMP, the DNN-based encoder of the GOGAN-SAE and the VGG-based encoder of the ECAAE under different working conditions in scenario C (see Table 2). The results are shown in Fig. 10.

Fig. 10
figure 10
Distributions of the features extracted by a the VMD-MFDFA of the VMP; b the DNN-based encoder of the GOGAN-SAE; c the VGG-based encoder of the ECAAE under different working conditions in scenario C

Full size image
It can be observed that in the learned feature spaces of the VMD-MFDFA and DNN-based encoder, significant distribution discrepancies exist across different working conditions, which means that these methods are less capable of adapting to varying working conditions than other approaches. In contrast, the features extracted by the ECAAE are more concentrated, which means that the samples in the same class under different working conditions are effectively projected into the same regions. This phenomenon indicates that the learned feature extraction knowledge under the source working conditions can be generalized to new conditional domains, resulting in high diagnostic accuracies. The reason is that the adversarial network composed of ğ‘(ğ‘¦|ğ‘¥) and ğ·ğ¿(ğ‘¦) ensures that the label part ğ‘¦ of the latent code is irrelative to condition information, and the continuous distribution ğ‘ƒğ‘ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘› imposed on the conditional part ğ‘§ of the latent code makes the feature extraction capabilities of the model more robust under varying working conditions and noise.

Sample generation ability
Under the conditions of imbalanced classes and few-shot learning, the proposed model still achieves a competitively high accuracy rate, indicating that the model effectively augments data by relying on GANs and architectural advantages. Figure 11 shows the comparison results between the real and generated samples from the 6th dataset. It indicates that the generated samples match rather well with their corresponding real samples in the frequency domain, which indicates that the generator is able to learn the characteristic frequencies that have critical impacts on fault diagnosis. To further illustrate the effectiveness of the generation ability of the proposed method, feature vectors extracted by the VGG-based encoder from the original unbalanced samples and the corresponding samples rebalanced by the SMOTE and ECAAE are visualized by t-SNE and shown in Fig. 12. Comparing subfigures (b) and (c), the samples generated by the ECAAE effectively expand the intraclass changes while avoiding confusion with the interclass changes, thereby improving the diagnostic accuracy and robustness.

Fig. 11
figure 11
The generated samples and the corresponding real samples from the 6th dataset

Full size image
Fig. 12
figure 12
Visualizations of the feature vectors extracted by the VGG-based encoder from the training set derived from the 6th dataset: a original imbalanced data; b rebalanced by the SMOTE; c rebalanced by the ECAAE

Full size image
To further demonstrate the sample generation ability, the generated samples of the WGAN-GP, the basic AAE and the proposed ECAAE are evaluated with several statistical indicators. The average Euclidean distance (ED) is calculated to estimate the similarity between the generated and real samples. The Wasserstein distance (WD), Kullbackâ€“Leibler divergence (Kâ€“L) and maximum mean discrepancy (MMD) are introduced to evaluate those samples as distributions. Moreover, the average Euclidean distance between the intraclass generated samples and between the interclass generated samples is calculated to demonstrate the contributions of these samples for diagnosis. The experiments are conducted on the 4th dataset, and the results are shown in Table 8. The ECAAE achieves the lowest value in the first four metrics, indicating that the samples it generates are more realistic. Furthermore, comparing the results of the last two metrics, the samples generated by the ECAAE have the maximum interclass distance while ensuring a certain intraclass difference, which is very useful for preventing overfitting, especially in the few-shot condition.

Table 8 Statistical features of samples generated by different models
Full size table
Effectiveness of signal conversion
To demonstrate the superiority of the proposed signal conversion approach, 3 additional models are selected for comparison. First, to eliminate the signal conversion method, a 1D convolutional VGG13 model is used as the encoder/generator to form a 1D-ECAAE; therefore, the model can directly process the vibration signals. Second, two signal conversion methods used in the following reference studies are utilized instead of the proposed one. The continuous wavelet transform scalogram used in [31] is utilized to form a CWTS-ECAAE, and the bi-spectrum counter map used in [32] is utilized to form a BCM-ECAAE. The above three models are all run based on twelve datasets. Table 9 shows their average diagnosis accuracies. The proposed signal conversion method outperforms the other models, demonstrating its superiority for diagnosis.

Table 9 Average diagnosis accuracies (%) of models with different signal conversion methods
Full size table
To further demonstrate the improvement achieved with the proposed model, two simple CNN models, a 1D VGG13 that directly processes the vibration signals and a 2D VGG13 with the proposed signal conversion method, are tested for comparison. Figure 13 shows the performance of each model based on each dataset. Obviously, 2D-VGG13 with the proposed signal conversion method yields better results, verifying that the proposed signal conversion method effectively retains the information contained in the data without using any predefined parameters. Furthermore, this simple method provides CNNs with a way to explore 2D features hidden in raw signals so that the advantages of CNNs can be effectively utilized.

Fig. 13
figure 13
Fault diagnosis accuracies of 1D-VGG13 and 2D-VGG13 for each dataset

Full size image
Contribution of the proposed architecture
The proposed overall architecture is the key to modeling success under nonideal data conditions. To further verify the improvement in diagnostic capability brought by the architecture, 4 more models are tested based on the twelve datasets. First, two simple CNN models tested are ResNet18 and LeNet-5. Then, two ECAAE models that use the above CNN models as encoders/generators are tested for comparison. The diagnostic accuracy (average of 5 trials) of each model is shown in Table 10.

Table 10 Diagnosis accuracies (%) of different models
Full size table
The above comparisons reveal that the use of the extended AAE architecture significantly improves the diagnostic performance of the base model. Figure 14 shows the fault diagnosis accuracy improvement achieved for each CNN by using the proposed architecture. The architecture brings improvements to all three CNNs, with the largest increments occurring under harsh data conditions. This means that the generation ability improves the performance of the model under few-shot conditions. At the same time, the architecture effectively uses the generator to balance the samples during the extended fine-tuning phase, thereby obtaining better results.

Fig. 14
figure 14
Fault diagnosis accuracy improvements of three CNNs obtained by using the proposed architecture

Full size image
Influences of key hyperparameters
The parameters of the ECAAE can be divided into two groups: structural or computational. Computational parameters, such as the batch size and learning rate, usually have greater impacts on model diagnosis consistency than structural parameters. Since the ECAAE in this article is applied under few-shot conditions, there are few options for the batch size (4, 8 or 16). The grid search results show that the impact of batch size on accuracy is negligible, but the training time shortens as the batch size increases. The most important computational parameter is the learning rate. Since there are many submodels in the ECAAE, the learning rate needs to be set to a rather small value and should be further reduced as training progresses. A high learning rate will cause the two GANs in the model to fail to converge during the training process. In our experiment, when the learning rate is greater than 0.001, the adversarial training process fails to converge, and the result after the extended fine-tuning phase is also unstable. In the beginning of the first three phases, setting the learning rate to 1ğ‘’âˆ’5 instead of 1ğ‘’âˆ’4 increases the accuracy by 0.03% on average. When smaller than 1ğ‘’âˆ’5, the decrease in the learning rate no longer has an impact on the accuracy but instead leads to a significant increase in training time.

Different from the calculation parameters, structural parameters such as CNN model type, convolution and transposed convolution layer size, and dropout and normalization technique usually have greater impacts on the accuracy of fault diagnosis [33]. The use of dropout and normalization techniques stabilizes the training process of the adversarial model and improves the generalization ability. In the experiment conducted on the 12 datasets, using dropout and normalization techniques reduces the standard deviation of the diagnosis accuracies by 0.7%. In the ECAAE model, the parameter that has the greatest impact on accuracy is the type of CNN. The last three columns of Table 10 show the diagnosis results using different CNNs. VGG achieves the best results. A possible reason for this is that LeNet-5 is too simple to extract features effectively, while ResNetâ€™s representation ability is too strong, commonly resulting in overfitting.

A unique structural parameter of the ECAAE is the number of submodels used in the prior Gaussian mixture distribution for certain conditions, and this parameter also has the greatest impact on diagnostic performance during the experiments. The grid search is used in this experiment. Figure 15 shows the diagnosis result. It indicates that no setting can be optimal on every dataset. In general, the model with a ten 2D Gaussian mixture distribution has the smallest fluctuations and the highest average result. However, compared with the improvements brought by the proposed architecture, the newly added hyperparameter has little impact on diagnostic performance. The results in Table 10 show that the improvement achieved with the proposed architecture is greater than 16% on average, while in Fig. 15, the performance fluctuation associated with the key hyperparameter is less than 3% on average, and the largest fluctuation is only 8.06%. Thus, this comparison indicates the stability of the improvements achieved with the proposed model.

Fig. 15
figure 15
Fault diagnosis accuracies of the ECAAE with different prior Gaussian mixture distributions

Full size image
Conclusions
In this work, a novel adversarial deep learning network called ECAAE, which combines a CNN and an AAE with a cyclic architecture, is proposed to overcome data hurdles and realize end-to-end data-driven EMA-based fault diagnosis using raw vibration signals. With the increasing demands of practical applications in industrial scenarios, the ECAAE aims to train a diagnosis model with few unbalanced and labeled samples while utilizing large amounts of unlabeled data. The diagnostic ability of the ECAAE is demonstrated by experimental validations and comparisons conducted on the FLEA dataset. Three conclusions can be drawn. (1) The ECAAE obtains higher diagnosis accuracies and better generalization performances under unsatisfactory data conditions than several SOTA models. (2) The obtained visualizations of the generated samples and learned features demonstrate the effectiveness of the sample generation procedure and the robustness of the feature extraction process, thereby explaining why the ECAAE achieves the best results. (3) The comparative analysis illustrates that the proposed architecture can effectively improve various CNN models without requiring additional expert knowledge or being affected by excessive hyperparameters. Therefore, in terms of practical industrial applications, the ECAAE provides a feasible solution in which the diagnostic model can be trained on numerous unlabeled samples with few unbalanced and labeled samples collected under certain working conditions, and the model can be applied for monitoring the health statuses of EMAs under other working conditions for which collecting labeled data is difficult or expensive.

The main limitation lies in the inevitable need for labeled samples, especially samples under fault conditions. In real industrial scenarios, obtaining accurate tags for abnormal samples requires manual intervention by experts. Deep active learning is a promising method for reducing this time-consuming and labor-intensive process that utilizes deep learning methods to find samples that are difficult to classify, so that a small amount of manual labeling can achieve a significant performance improvement. The AAE has the potential to perform active learning, and that process will be a focus of future research.

Keywords
Electromechanical actuator
Fault diagnosis
Unbalanced samples
Few-shot learning