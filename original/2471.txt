It has been of great interest in the neuroimaging community to model spatiotemporal brain function and related disorders based on resting state functional magnetic resonance imaging (rfMRI). Although a variety of deep learning models have been proposed for modeling rfMRI, the dominant models are limited in capturing the long-distance dependency (LDD) due to their sequential nature. In this work, we propose a spatiotemporal attention auto-encoder (STAAE) to discover global features that address LDDs in volumetric rfMRI. The unsupervised STAAE framework can spatiotemporally model the rfMRI sequence and decompose the rfMRI into spatial and temporal patterns. The spatial patterns have been extensively explored and are also known as resting state networks (RSNs), yet the temporal patterns are underestimated in last decades. To further explore the application of temporal patterns, we developed a resting state temporal template (RSTT)-based classification framework using the STAAE model and tested it with attention-deficit hyperactivity disorder (ADHD) classification. Five datasets from ADHD-200 were used to evaluate the performance of our method. The results showed that the proposed STAAE outperformed three recent methods in deriving ten well-known RSNs. For ADHD classification, the proposed RSTT-based classification framework outperformed methods in recent studies by achieving a high accuracy of 72.5%. Besides, we found that the RSTTs derived from NYU dataset still work on the other four datasets, but the accuracy on different test datasets decreased with the increase in the age gap to NYU dataset, which likely supports the idea of that there exist age differences of brain activity among ADHD patients.

Introduction
Functional magnetic resonance imaging (fMRI) has become very popular for brain functional activity studies in the past decade [1, 2]. The functional brain networks (FBNs) learned from fMRI provide a powerful tool to model brain functions and disorders even in the absence of an external task [3,4,5,6]. In previous studies, various machine learning methods have been successfully applied on fMRI to exploit FBNs, such as general linear model (GLM) [7, 8], independent component analysis (ICA) [9,10,11,12,13], and sparse dictionary learning (SDL) [14,15,16,17]. GLM is a model-driven method that is widely used in task-based fMRI (tfMRI) analysis. ICA is a data-driven method that is dominant in resting state fMRI (rfMRI) analysis, including identifying resting state networks (RSNs) from rfMRI and related studies on brain diseases. SDL is another data-driven method and it has been widely used in many associated fMRI studies. Although these traditional methods can reconstruct many meaningful FBNs and temporal patterns, their representation power is still limited by their shallow nature.

Recently, deep learning has attracted much attention in the fields of machine learning and data mining. Due to the superior representation power over traditional methods, there have been growing bodies of the literature that adopted deep learning models into fMRI data modeling and associated applications [18,19,20,21,22,23,24,25,26,27,28,29,30]. For instance, Huang et al. proposed a deep convolutional auto-encoder (DCAE) to model fMRI based on temporal courses, and it was shown that the DCAE has superb power on extracting temporal features than traditional shallow models [22]. Dong et al. proposed a volumetric sparse deep belief network (VS-DBN) to learn hierarchical functional brain networks from tfMRI data, which is an early attempt for modeling fMRI on volumes [29]. Wang et al. proposed a recurrent neural network (RNN) to recognize brain states from 7 tasks of tfMRI, and it was proved that the recurrent layers are effective in modeling temporal dynamics of brain activity [26]. Cui et al. proposed a recurrent auto-encoder (RAE) to learn meaningful FBNs with multiple timescales which are overlooked by traditional methods [19]. Li et al. proposed a deep sparse recurrent auto-encoder (DSRAE) to simultaneously learn temporal and spatial patterns from grayordinate tfMRI [18]. These models showed great promises of deep learning in fMRI applications. However, evidences show that multiple regions of brain involve long-distance dependency (LDD), which is very challenging for the existing deep learning models to address [31].

In recent years, CNN and RNN have been successfully applied in fMRI sequence modeling. To utilize the CNN's hierarchical feature abstraction ability, a 1-D temporal convolution architecture was applied on the fMRI time series [22, 32]. This approach can extract local temporal features; however, it cannot deal with the LDDs due to distance limit. In addition, it did not make use of the rich spatial information from fMRI. To incorporate the spatial and temporal information at the same time, recurrent network was applied on the fMRI volumes and preserving temporal features with long short-term memory (LSTM), which is a typical recurrent module [19, 26, 33, 34]. This approach established a unified spatiotemporal framework. However, the inherently sequential nature of RNN/LSTM precludes parallelization, which causes notable time cost especially for high-dimensional data like fMRI. In addition, the long scan time of fMRI imposes great burden on computing and memory resources, and a long memory unit of LSTM may cause gradient vanishing problem. In the machine translation fields, the attention mechanism has gained popularity in sequence modeling and various tasks [35]. Compared to regular CNN and RNN, the attention mechanism models each unit in the input sequence simultaneously and draws global dependencies without regard to their distance. It has also been proved that pure attention mechanism has remarkable representation powers than CNNs and RNNs [35, 36]. Thus, researchers in machine translation fields have turned to attention networks for substitute of RNNs [37,38,39]. Inspired by these researches, we explored the possibility of applying attention mechanism in fMRI sequence modeling.

Considering the unsupervised nature of fMRI, in this work, we propose a spatiotemporal attention auto-encoder (STAAE) to model the fMRI sequence. The STAAE consists of two parts: an encoder and a decoder, with the goal of reconstructing input data. The training data are a volume sequence of a series of fMRI time points. With the attention mechanism, the relation of two volumes in the sequence is captured with an attention score measuring the distance of their embeddings. Thus, the STAAE can model the volumetric inputs not only spatially, but also temporally, making it a suitable spatiotemporal model of fMRI. In the training process, the encoder yields latent vectors and then pass them to the decoder, aiming to reconstruct the inputs. After training, the encoder can be regarded as a high-level feature extractor, which can be used to predict temporal features from fMRI data. We adopt an assumption that is consistent with the spatiotemporal separability assumptions of ICA or SDL; that is, fMRI data can be expressed as the product of time courses and spatial maps. Thus, we applied LASSO (least absolute shrinkage and selection operator) regression on inputs and temporal features to estimate a sparse weight matrix, which can be interpreted and visualized as functional brain networks. With the proposed STAAE model and LASSO regression, the volumetric fMRI data can be decomposed into temporal patterns and spatial patterns.

Attention- deficit hyperactivity disorder (ADHD) is a mental health disorder involves multiple attention-related problems [40,41,42,43,44,45,46,47]. Despite numerous studies on ADHD in the studies, neither a comprehensive pathophysiology model nor a biomarker for clinical practice is established yet. In previous studies, researchers usually used brain functional connectivity (FC) as the main feature to train classifiers. It has been proved that the FC-based classification methods can effectively differentiate ADHD patients from normal controls (NC). However, the performance of these methods is limited by the high dimension of features, and most of them performed not very well on multicenter studies. In this work, based on the STAAE model, we constructed a novel ADHD classification framework. Firstly, we trained the STAAE model and derived RSNs and temporal patterns from whole dataset, ADHD only dataset, and NC only dataset, respectively. Then we averaged all temporal patterns from the ADHD and NC datasets as resting state temporal templates (RSTT). Secondly, we selected a testing subject, and then compared its temporal patterns to these two RSTTs to estimate a possibility of this subject being ADHD or NC. Specifically, if the temporal patterns of the test subject have higher similarity to ADHD RSTT than NC RSTT, this subject is identified as ADHD; otherwise, it is identified as NC.

To quantitatively evaluate the proposed STAAE model and the ADHD classification framework, a series of experiments have been conducted. The results showed that the proposed STAAE model has remarkable performance in deriving 10 well-known RSNs, by taking ICA activation RSN templates as benchmark. For classification study, the proposed RSTT-based method performed excellently on five datasets from ADHD-200 compared with methods in recent studies. The explanations of used abbreviations are given in Table 8 in Appendix.

Proposed approach
Overview
Figure 1 summarizes the proposed STAAE model for decomposition of fMRI. There are three steps: (a). STAAE training. The original 4-D fMRI data are converted to a 2-D group-wise matrix after preprocessing and masking. Then the STAAE is trained on each column of the matrix, aiming to reconstruct the inputs. The encoder of STAAE outputs latent vectors, which is passed to the decoder of STAAE to generate reconstructed data. If the model is trained well, the encoder can be regarded as a high-level feature extractor that can extract temporal features from volumetric fMRI. (b). Feature extraction. After training, we used the encoder of STAAE to predict fMRI data and obtain a series of temporal features, which correspond to the dictionaries in SDL. (c). FBN estimation. As mentioned before, we adopt the assumption that fMRI data can be expressed as the product of time courses and spatial maps. Thus, we used LASSO regression to estimate a sparse weight matrix, which can be interpreted and visualized as FBNs. For rfMRI data, the FBNs are RSNs. After the above three steps, we can decompose group-wise data or single subject into temporal and spatial patterns.

Fig. 1
figure 1
Illustration of fMRI decomposition by STAAE (spatiotemporal attention auto-encoder). a STAAE training. After preprocessing and masking, the four-dimensional fMRI data are converted to a two-dimensional group-wise matrix. Then the STAAE is trained on each column of the matrix, aiming to reconstruct the inputs. The latent vector, outputs of STAAE encoder, can be regarded as high-level feature of inputs, where d is the dimension of latent vector. b Feature extraction. After training, the STAAE encoder can be regarded as a high-level feature extractor, which can be used to predict 2D fMRI data (X, group-wise or single subject) and obtain temporal features (Z). c FBN estimation. As the fMRI data can be expressed as the product of time courses and spatial maps, LASSO regression is applied on input fMRI data (X) and temporal features (Z) to estimate a sparse weight matrix (W), which can be mapped back to the three-dimensional space and visualized as functional brain networks (FBN), where T is the number of time points, n is the number of voxel

Full size image
Figure 2 illustrates the proposed ADHD classification framework based on the spatiotemporal templates derived by STAAE. Firstly, we used STAAE to derive 10 meaningful RSN templates on whole dataset, ADHD dataset, and NC dataset, respectively, then obtained RSN templates named as RSN_ALL, RSN_A, and RSN_N, respectively. Next, we averaged temporal patterns from all ADHD subjects to ADHD temporal template named as RSTT_A, and temporal patterns from all NC subjects to NC temporal template named as RSTT_N. Then we obtained the spatiotemporal templates of the dataset. Secondly, we selected one test subject and applied linear regression on this subject and RSN_ALL to estimate its temporal patterns. Finally, we compared the temporal patterns of this subject to RSTT_A, and RSTT_N. If the temporal patterns have higher similarity to RSTT_A than RSTT_N, this subject is identified as ADHD patient; otherwise, it is identified as NC. The classification framework is very simple and effective. The key is the STAAE model. By introducing attention mechanism into deep auto-encoder, the STAAE can spatiotemporally model the volumetric rfMRI data and decompose it into meaningful temporal and spatial patterns.

Fig. 2
figure 2
Illustration of ADHD classification based on STAAE-derived spatiotemporal templates. a Spatiotemporal templates. According to 10 resting state networks (RSN) derived by ICA, the STAAE derived 10 RSNs on whole dataset (RSN_ALL), ADHD dataset (RSN_A), and NC dataset (RSN_N), respectively. Then the corresponding temporal patterns were averaged as RSTT_A and RSTT_N, representing resting state temporal templates of ADHD subjects and NC (Normal control) subjects, respectively. b Classification on test dataset. Linear regression was applied on test subject and RSN_ALL to estimate temporal patterns of this subject. Then similarities of the temporal patterns to RSTT_A and RSTT_N were calculated and used to identify ADHD patients

Full size image
Spatiotemporal attention auto-encoder (STAAE)
In respect of sequence modeling, especially high-dimensional spatiotemporal sequence data like fMRI, both CNNs and RNNs have been used in the field. However, they can only deal with local dependencies in the sequence. In this paper, we propose an auto-encoder framework with attention mechanism, named spatiotemporal attention auto-encoder (STAAE) to model global features in rfMRI sequence. Most sequence models follow an encoderâ€“decoder paradigm, and we also adopted this structure to our proposed STAAE, considering the intrinsic unsupervised nature of rfMRI since no external stimulus or task is performed. The STAAE model consists of an encoder and a decoder. The encoder aims to extract high-level features, while the decoder aims to reconstruct the exact inputs. As shown in Fig. 3a, the encoder maps an input volume sequence of symbol representations Xâ€‰=â€‰(x1, â€¦, xt) to a sequence of latent vectors Zâ€‰=â€‰(z1, â€¦, zt). Specifically, each xi represents a volume of rfMRI and is embedded with a fully connected network. Given Z, the decoder tries to generate a reconstructed sequence of Xâ€™â€‰=â€‰(x1â€™, â€¦, xtâ€™). There are two fully connected hidden layers in both the encoder and the decoder. The number of nodes in each layer is shown on the right of the corresponding layer.

Fig. 3
figure 3
Structure of STAAE and illustration of self-attention mechanism. a STAAE structure. The encoder of STAAE models the input rfMRI sequence of Xâ€‰=â€‰(x1, â€¦, xt) to a sequence of latent vectors Zâ€‰=â€‰(z1, â€¦, zt). Each xi represents a rfMRI volume and is embedded with a fully connected network. The decoder of STAAE tries to generate a reconstructed sequence of Xâ€™â€‰=â€‰(x1â€™, â€¦, xtâ€™) from input sequence Z. b Self-attention. The self-attention mechanism enables the STAAE to attention sequence information in the training process. There are three matrices in self-attention: queries ğ‘„, keys ğ¾, and values ğ‘‰, and they come from the same input. The product of Q and K is used to calculate the weight vector of self-attention, which is multiplied by V to get a weighted sum, attention score

Full size image
Self-attention mechanism was applied in STAAE to draw global dependencies in rfMRI by capturing the relation of two volumes in the sequence with an attention score measuring the distance of their embeddings. The self-attention mechanism consists of three matrices: queries ğ‘„, keys ğ¾, and values ğ‘‰, and they come from the same input. In the context of rfMRI, a key vector and a query vector are learned for each frame of volume, and the pairs of queryâ€“key are matched across all frame simultaneously. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. If a pair of queryâ€“key matches, it generates a high value as output. As shown in Fig. 3b, we compute each latent vector as:

ğ‘=Attention(ğ‘„,ğ¾,ğ‘‰)=softmax(ğ‘„ğ¾ğ‘‡ğ‘‘â€¾â€¾âˆš)â‹…ğ‘‰
(1)
where d is a scale factor used to limit the attention matrix ğ‘„ğ¾ğ‘‡.

The 3 matrices Q, K, and V are calculated as follows:

ğ‘„=ğ‘‹Ã—ğ‘Šğ‘„
(2)
ğ¾=ğ‘‹Ã—ğ‘Šğ¾
(3)
ğ‘‰=ğ‘‹Ã—ğ‘Šğ‘‰
(4)
The three weight matrices ğ‘Šğ‘„,ğ‘Šğ¾,ğ‘ğ‘›ğ‘‘ğ‘Šğ‘‰ are randomly initialized and updated in the training process. In one word, the STAAE model can attention previous inputs by the attention score; thus, the STAAE can model the fMRI data with sequence information.

With attention mechanism, the STAAE becomes a spatiotemporal model that can simultaneously model the spatial and temporal features from volumetric rfMRI. As an unsupervised data-driven model, the STAAE can automatically capture meaningful temporal features that possibly represent cognitive or physiological variables under resting state, and the corresponding spatial features (RSNs) may reflect related brain activities. If the model is trained well, that means the input rfMRI data are well reconstructed from encoder to decoder of STAAE. Thus, the trained encoder can be regarded as a high-level feature extractor which can extract temporal features from rfMRI with sequence information.

fMRI data decomposition by STAAE
As shown in Fig. 1b, after training, we used the trained encoder to predict input rfMRI data and obtained a series of temporal patterns. According to the spatiotemporal separability assumption of fMRI, we applied LASSO regression on input data and temporal patterns to estimate a sparse coefficient matrix. Each row of this matrix can be mapped back to the brain 3-D space and visualized as an RSN (inverse operation of masking). The dimension of temporal patterns is 100, the same as the number of corresponding RSNs. Thus, the input rfMRI data were decomposed into temporal patterns and spatial patterns. The LASSO regression is as follows:

minğ‘Š12ğ‘‡ğ‘‹âˆ’ğ‘ğ‘Š22+ğœ†ğ‘Š1
(5)
where T represents the total number of time points, Z represents temporal features, X represents input data, and W represents sparse weight matrix. The parameter ğœ† with range of [0, 1] is used to adjust the sparsity of W. In this work, ğœ† was set as 0.1. The input rfMRI data for temporal feature extraction can be group-wise data or single subject. By the same method, we can decompose not only group-wise inputs, but also single subject into spatial and temporal patterns. That means we can obtain spatial and temporal patterns from any input rfMRI data, using the trained STAAE encoder and LASSO regression.

ADHD classification
In order to identify ADHD patients from NC, we tried to explore the differences in RSN patterns among ADHD and NC subjects. However, there was no obvious difference found between ADHD RSNs and NC RSNs on group-wise level. Moreover, even the group-wise differences in RSN cannot be used for single subject identification, due to the inter-subject variability and the influence of noise. Therefore, we explored the possibility of using temporal patterns for classification study. Firstly, we trained the STAAE model on whole dataset, ADHD only dataset, and NC only dataset, respectively, and obtained spatial templates of 10 meaningful RSNs, including â€œvisualâ€ (RSN1, RSN2, and RSN3), â€œdefault mode networkâ€ (RSN4), â€œcerebellum networkâ€ (RSN5) â€œsensorimotorâ€ (RSN6), â€œauditoryâ€ (RSN7), â€œexecutive controlâ€ (RSN8), and â€œfrontoparietalâ€ (RSN9 and RSN10). As shown in Fig. 2a, these RSN templates are named as RSN_ALL, RSN_A, and RSN_N. Next, the corresponding temporal templates from ADHD and NC datasets were simultaneously obtained by averaging all temporal patterns in each dataset. These two resting state temporal templates were named as RSTT_A and RSTT_N. For validation of the RSTT-based method, we selected a test subject and applied linear regression on this subject and RSN_ALL to estimate its temporal patterns (see Fig. 2b. The linear regression is as follows:

minğ‘12ğ‘‡ğ‘‹ğ‘‡âˆ’ğ‘Šğ‘‡ğ‘ğ‘‡22
(6)
where ğ‘‹ğ‘‡, ğ‘Šğ‘‡, and ğ‘ğ‘‡ represent the transpose of the corresponding matrix, respectively. With the similarity defined by Pearson correlation coefficient (PCC), we compared the temporal patterns of the test subject to RSTT_A and RSTT_N. The PCCs to RSTT_A and RSTT_N are named as PCC_A and PCC_N, respectively. If the sum of 10 PCC_As is greater than the sum of 10 PCC_Ns, the test subject is identified as ADHD; otherwise, it is identified as NC. To summarize, in our classification framework, we assume that there exist differences between the 10 resting state temporal patterns from ADHD and NC subjects. Thus, we derived spatiotemporal templates from each dataset by STAAE and conducted classification study based on the 10 resting state temporal templates.

Experiments
Dataset and preprocessing
In this work, we used five independent rfMRI datasets from ADHD-200, including New York University Medical Center (NYU), Peking University (PU), Kennedy Krieger Institute (KKI), NeuroImage (NI), and Oregon Health and Science University (OHSU). All of the used data have already been preprocessed. The details of protocols are available at https://fcon_1000.projects.nitrc.org/indi/adhd200/. The preprocessing pipeline is available at https://preprocessed-connectomes-project.org/adhd200/, including skull striping, slice timing correction, motion correction, resampling to template, detrending, and normalization. All the rfMRI scans are slice timing corrected, motion corrected to the first image of the time series. After the standard preprocessing pipeline, the 4-D rfMRI data (three spatial dimensions and a time dimension) were transformed to a 2-D array (voxel dimension and time dimension) by masking with the Montreal Neurological Institute (MNI)-152 4â€‰Ã—â€‰4â€‰Ã—â€‰4 mm3 standard template space. The number of brain voxels is 28,546 based on the MNI-152 4 mm template. The masking operation was implemented by Nilearn tools (available at https://nilearn.github.io/). Then we aggregated all the subjectsâ€™ arrays to a group-wise matrix as the training set. Table 1 shows the summary of used datasets. The number of time points is the number of volumes in one subject.

Table 1 Summary of used datasets
Full size table
Implementations
In this work, we applied Keras to construct and train our STAAE model. Keras is a popular Python deep learning API that provides great convenience of coding with GPUs. Tensorflow was adopted as the backend of Keras. Our codes were run on two GPU cards of NVIDIA RTX 2080TI. For the model training, the optimization function was selected as â€œAdadelta,â€ and the loss function was selected as â€œbinary_crossentropy.â€ Our experiment consists of three parts as follows:

a)
RSN identification. To quantitatively evaluate the performance of STAAE in deriving meaningful RSNs, we investigated the 10 well-known RSNs derived from whole NYU dataset, NYU ADHD dataset, and NYU NC dataset, respectively, by taking ICA activation RSNs as benchmark. In addition, we also investigated the RSNs derived from single ADHD subjects and single NC subjects.

b)
RSTT estimation and analysis. Based on the 10 RSNs derived by the last step, all corresponding temporal patterns were found. We averaged temporal patterns from all ADHD subjects to RSTT_A, and temporal patterns from all NC subjects to RSTT_N. Then we analyzed the differences between RSTT_A and RSTT_N.

c)
Classification on five ADHD datasets based on RSTTs. We used linear regression on a test subject and RSN_ALL to estimate the temporal patterns of this subject, and compared it to RSTT_A and RSTT_N. Then the subject was identified based on the similarities to these two RSTTs. To verify the proposed template-based classification method, three different experiments were conducted. First, we derived the RSTTs from each dataset, and test them on the same dataset. In addition, we test the RSTTs from NYU dataset on the other four datasets. Second, the five datasets were used as a whole. 80% of subjects were used for training and the left subjects were used for testing. Third, the five datasets were used separately. We conducted fivefold cross-validation on each of the five datasets.

Results and discussions
RSNs derived by STAAE
After training of STAAE, we used the trained encoder to predict temporal patterns of rfMRI. Then we applied LASSO regression on input data and temporal patterns to estimate RSNs. Since the rfMRI data do not contain task designs or any pre-knowledge, there exist no widely recognized temporal templates of rfMRI, and we cannot distinguish which temporal patterns are meaningful. Therefore, we firstly analyzed the RSNs derived by STAAE, taking ICA activation RSNs as benchmark. Then according to the 10 STAAE-derived meaningful RSNs, the corresponding temporal patterns can be found and analyzed. Details of 10 meaningful RSNs derived by ICA can be found in [48]. Overlap rate (OR), defined as follows:

ğ‘‚ğ‘…(ğ‘…ğ‘†ğ‘(1),ğ‘…ğ‘†ğ‘(2))=âˆ‘ğ‘›ğ‘–=1âˆ£âˆ£ğ‘…ğ‘†ğ‘(1)ğ‘–âˆ©ğ‘…ğ‘†ğ‘(2)ğ‘–âˆ£âˆ£âˆ‘ğ‘›ğ‘–=1âˆ£âˆ£ğ‘…ğ‘†ğ‘(1)ğ‘–âˆªğ‘…ğ‘†ğ‘(2)ğ‘–âˆ£âˆ£
(7)
was used to measure the similarity between two RSNs from different methods. n is the number of voxels in one volume. Here n is 28,546. If voxel i is activated in the RSN,RSN(1)ğ‘– is 1. Otherwise, RSN(1)i is 0. With the overlap rate defined above, we compared all of 100 RSNs derived by STAAE to the 10 RSN templates from ICA, including â€œvisualâ€ (RSN1, RSN2, and RSN3), â€œdefault mode networkâ€ (RSN4), â€œcerebellum networkâ€.

(RSN5), â€œsensorimotorâ€ (RSN6), â€œauditoryâ€ (RSN7), â€œexecutive controlâ€ (RSN8), and â€œfrontoparietalâ€ (RSN9 and RSN10). For each meaningful RSN, we found the most similar RSN with highest OR among 100 STAAE-derived RSNs. The RSN templates from ICA and the RSNs on NYU dataset derived by the STAAE are shown in Fig. 4, and the corresponding ORs are listed on the left of the figure (RSNs derived on the other four datasets are given in Appendix). The bright regions in the image are co-activated voxels that form a functional brain network. We can clearly see that the 10 RSNs derived from the two methods are quite similar to each other. The average OR of 10 RSN pairs is as high as 0.485. We also compared the STAAE model with classical AE, well-known SDL and another spatiotemporal model: DSRAE [18]. Table 2 shows ORs between RSNs from the four methods and RSN templates from ICA. It can be seen clearly that the proposed STAAE performed best based on the average OR to the benchmark. Since there exist no widely recognized temporal templates of rfMRI, the temporal patterns derived from these methods cannot be compared. Considering lack of golden standard regarding RSN patterns, the results demonstrated that the proposed STAAE model is effective and competitive for learning meaningful RSNs.

Fig. 4
figure 4
Comparison of RSNs derived by STAAE on NYU dataset with RSN templates derived by ICA (independent component analysis). Each network is visualized with 10 axial slices

Full size image
Table 2 Overlap Rates between RSN templates from ICA (independent component analysis) and RSNs from four methods including STAAE (spatiotemporal attention auto-encoder), AE (auto-encoder), SDL (sparse dictionary learning), and DSRAE (deep sparse recurrent auto-encoder)
Full size table
To investigate whether there exist differences between group-wise RSNs from ADHD patients and NCs, we applied STAAE on NYU ADHD dataset (118 subjects) and NYU NC dataset (98 subjects), respectively, and obtained 10 meaningful RSNs named RSN_A and RSN_N, respectively. As shown in Fig. 5, we did not find considerable differences between RSN_A and RSN_N, except for RSN5, on which the OR is only 0.261. The average OR of RSN_A and RSN_N is as high as 0.494, even higher than the average OR of 10 RSN pairs from ICA and STAAE on whole NYU dataset. Besides, among the 100 RSNs derived by STAAE, we selected 12 RSN patterns and also compared the RSN pairs from ADHD dataset and NC dataset. As shown in Fig. 6, we can see widespread insignificant differences across these RSNs.

Fig. 5
figure 5
Comparison of RSNs derived by STAAE on NYU ADHD and NYU NC (normal control) datasets. Each network is visualized with 10 axial slices

Full size image
Fig. 6
figure 6
12 RSNs derived on NYU ADHD and NYU NC (normal control) datasets. Each network is visualized with 3 most informative orthogonal slices

Full size image
Notably, the group-wise differences probably disappear on the individual level, which is critical for ADHD identification for single subject. To investigate the individual differences of RSNs, we random selected 5 subjects from both NYU ADHD and NYU NC datasets, and derived 10 RSNs from these single subjects. As shown in Fig. 7, we can see that the single subject RSNs are not so similar to the RSNs from group-wise data. Furthermore, it can be seen that the RSN differences between subjects from same dataset and different datasets are almost on the same level. That means it is hard to get use of the individual differences between ADHD and NC for classification study. The results showed that there exist no meaningful RSN differences between individual subjects from ADHD and NC that we can depend on to conduct classification study.

Fig. 7
figure 7
RSNs derived from five single subjects of ADHD and Normal control (NC). Each network is visualized with 3 most informative orthogonal slices

Full size image
To sum up, the experimental results demonstrated that the proposed STAAE has remarkable performance in deriving RSNs from rfMRI. However, it did not capture meaningful individual differences of RSN for potential use of classification, due to considerable inter-subject variability and influence of noise. Therefore, we explored the possibility of using temporal patterns for ADHD classification in the next section.

Temporal templates extracted by STAAE
As compared with 10 meaningful RSNs from ICA, we found 10 similar RSNs among 100 RSNs derived by STAAE. Then the corresponding temporal patterns extracted by the STAAE encoder can be found. We can use the index of RSN to locate the corresponding temporal pattern. For NYU dataset, we trained the STAAE model on NYU ADHD and NYU NC datasets separately, and then used the trained encoder to predict single subject one by one. Thus, we obtained 118 temporal patterns of ADHD subjects and 98 temporal patterns of NC subjects. Each temporal pattern has 10 sub-patterns that correspond to the 10 meaningful RSNs, respectively. Then we averaged the 118 ADHD temporal patterns to RSTT_A, and the 98 NC temporal patterns to RSTT_N. Figure 8 shows comparisons between RSTT_A and RSTT_N from NYU dataset, and the corresponding PCCs are listed at the bottom of the temporal patterns. Figure 8 shows the obvious differences between RSTT_A and RSTT_N. The average of 10 PCCs is as low as 0.032. Except for RSTT_2 and RSTT_3, most temporal patterns from ADHD and NC are quite different from each other. The result demonstrated that there exist significant differences between temporal patterns from ADHD and NC subjects on the group level.

Fig. 8
figure 8
Comparisons of RSTT_A and RSTT_N on NYU dataset. RSTT_A represents average of 118 ADHD subjectsâ€™ temporal patterns, and RSTT_N represents average of 98 NC subjectsâ€™ temporal patterns

Full size image
To investigate the possibility of using individual temporal patterns to identify ADHD from NC, we compared each subjectâ€™s temporal patterns to RSTT_A and RSTT_N, and calculated the PCCs of 10 RSTT pairs, as shown in Table 3. It can be clearly seen that the individual ADHD temporal patterns have higher similarity to RSTT_A than RSTT_N. On the contrary, the individual NC temporal patterns have higher similarity to RSTT_N than RSTT_A. This may suggest that we can take RSTT_A and RSTT_N as temporal templates for ADHD classification study. Furthermore, to find out how many subjects are enough for deriving stable and meaningful RSTTs, we analyzed the RSTT patterns with the increasing number of subjects. We selected RSTT_4 to observe the RSTT_A patterns on 30 subjects, 60 subjects, 90 subjects, and 118 subjects, and RSTT_N patterns on 25 subjects, 50 subjects, 75 subjects, and 98 subjects. Figures 9 and 10 clearly shows that the RSTT patterns show consistent features on different number of subjects. The PCCs in Figs. 9 and 10 show the similarity between corresponding RSTTs and RSTTs on all subjects. The results showed that the RSTTs are stable even derived on part of subjects.

Table 3 Average PCCs of single subjectâ€™s temporal patterns to RSTT_A and RSTT_N on NYU dataset. RSTT_A represents average of 118 ADHD subjectsâ€™ temporal patterns, RSTT_N represents average of 98 NC (normal control) subjectsâ€™ temporal patterns. The 10 RSTTs correspond to 10 resting state networks
Full size table
Fig. 9
figure 9
RSTT_4 patterns from ADHD dataset with the increasing number of subjects

Full size image
Fig. 10
figure 10
RSTT_4 patterns from normal control dataset with the increasing number of subjects

Full size image
In short, according to the 10 meaningful RSNs derived by STAAE, the corresponding temporal patterns can be located. Then we averaged all temporal patterns from ADHD and NC subjects as resting state temporal templates: RSTT_A and RSTT_N. It is found that the temporal patterns from ADHD subjects have higher similarity to RSTT_A than RSTT_N. Thus, it is possible to identify a subject based on the similarities between its temporal patterns and RSTTs.

ADHD classification and analysis
By using the RSTTs derived by STAAE, we constructed a novel ADHD classification framework, as shown in Fig. 2b. We applied linear regression on a test subject and spatial template RSN_ALL to estimate the temporal patterns of this subject. Then we calculated the PCCs of this subjectâ€™s temporal patterns to RSTT_A and RSTT_N, which named as PCC_A and PCC_N, respectively. If the sum of 10 PCC_As is higher than the sum of 10 PCC_Ns, the test subject is identified as ADHD; otherwise, the test subject is identified as NC.

Firstly, we constructed RSTTs from each dataset and test the RSTTs on the same dataset. As shown in Table 4, the identification accuracies based on RSTTs from each dataset are all very high (up to 93.5% of accuracy), demonstrating the effectiveness of the proposed RSTT-based classification framework. Specificity is the percentage of well-classified healthy subjects among all healthy subjects, and sensitivity is the percentage of well-classified ADHD subjects among all ADHD subjects.

Table 4 Classification results Using RSTTs from each dataset and RSTTs from NYU dataset
Full size table
Furthermore, we tested the RSTTs from NYU on the other four datasets. The results are also shown in Table 4. It is noted that the number of time points in each dataset is different, as shown in Table 1. Thus, the temporal templates cannot be directly compared with a test subject from another dataset. Even in the same dataset: KKI and OHSU, there are two different numbers of time points. Therefore, we used linear interpolation to transform the time dimension of the temporal templates to the test subjectâ€™s time dimension. Specifically, the number of time points on NYU is 172, so the time dimension of RSTT_A and RSTT_N is 172 too. The number of time points on PU is 232, so we transformed the time dimension of RSTT_A and RSTT_N to 232. Figure 11 shows an example of singe subject identification on PU dataset, using RSTTs from NYU. We can see that the dimension of RSTTs have already been transformed to 232. The test subject is from PU ADHD dataset. It is shown that the test subject is identified correctly. It can be seen from Table 4 shows that the performance of RSTTs from NYU is still satisfactory on PU dataset (75.8% of accuracy), while the performances on other three datasets decrease considerably. Notably, PU dataset has the closest average age to the training dataset: NYU. As shown in Table 1, the average ages of the 5 datasets NYU, PU, KKI, NI, and OHSU are 11.65, 11.97, 10.24, 17.01, and 8.84 (years), respectively. It can be seen that the classification accuracies on the four test datasets decease gradually with the increase in the age gap to NYU, which suggest that there may exist age differences of brain activity among ADHD patients. Therefore, it is suggested that the identification of ADHD should take age difference into account. In ADHD-200 dataset, there still lack of enough subjects for deriving more stable and meaningful RSTTs at different age stages. With the development of medical data sharing, in the future, we may construct multiple templates for diagnosing brain disorders at different age. Besides, the standardization and balance of data should also be emphasized for multimodal data analysis.

Fig. 11
figure 11
An example of singe subject identification. Sub_1 is from PU ADHD dataset. RSTT_A and RSTT_N are derived on NYU dataset. RSTT_A represents average of 118 ADHD Subsâ€™ temporal patterns, RSTT_N represents average of 98 NC Subsâ€™ temporal patterns. Î£PCC_A represents similarity between Sub_1â€™s temporal patterns and RSTT_A, Î£PCC_N represents similarity between Sub_1â€™s temporal patterns and RSTT_N

Full size image
Secondly, we selected three excellent traditional methods (named as M1, M2, and M3, respectively) in recent studies for comparison of our method. The methods in M1 and M2 used all of eight datasets from ADHD-200. In ADHD-200, the number of NC subjects is much more than the number of ADHD subjects, since there are three datasets which have only NC subjects. For a better balance of training data, the method in M3 removed the three datasets and used all of the remaining five datasets for training, and 20% of subjects for testing. That means all the test subjects have been used for model training. In this work, our method excluded test data from training data based on the M3 dataset. The composition of our used dataset is shown in Table 5. Descriptions and datasets of the compared methods and our method are shown in Table 6, as well as the classification accuracies. It can be seen that our method achieved a high accuracy of 72.5% that outperformed the three traditional methods. Besides, the result based on training on NYU (216 subjects) and testing on PU (194 subjects) also outperformed the three methods by achieving a high accuracy of 75.8%. Even the result based on training on NYU (216 subjects) and testing on others (404 subjects) is still competitive by achieving an accuracy of 67.8%.

Table 5 Composition of used dataset
Full size table
Table 6 Comparison of results with three methods on multiple datasets
Full size table
At last, we selected other four methods in studies to make comparison of our method on each dataset. Our results were achieved based on fivefold cross-validation (CV). All subjects in each dataset were divided into 5 sets, and we implemented 5 times of training and testing on each dataset. Each time we selected four sets for deriving RSTTs and the remaining one set for testing. The average accuracy was regarded as the evaluation of the classification performance. Table 7 shows comparisons of classification accuracies of our method on the five datasets with the results of other four methods. Methods in the literature [43,44,45] are categorized as fMRI-based method (using only fMRI data, named as M4), combined method (using fMRI data and non-fMRI data, named as M5), and non-fMRI-based method (using only non-fMRI data, named as M6), respectively. In addition, we also selected our recent published work in ADHD classification [30] for comparison study (named as M7), in which the results are based on functional connectivity (FC). It can be seen that our results outperformed the fMRI-based method (M4) on 4 datasets, except for OHSU. It is noted that the result of OHSU in M4 is based on test dataset, and the result is 60.61% based on training dataset. Our results also outperformed the combined method (M5) on three datasets, except a little worse on KKI. Predictably, the performance of our RSTT-based method can be further improved by fusion of fMRI data and non-imaging data, which still need to be further studied in the future. We can see that the non-fMRI-based method (M6) achieved high accuracies on NYU and PU. However, our RSTT-based method outperformed it on NYU. For comparison with the FC-based method (M7), we can see that the RSTT-based method has higher accuracies than the FC-based method on three datasets: NYU, PU, and OUSU. On the other two datasets: KKI and NI, the results of FC-based method are better. Notably, the proposed method performed better on NYU and PU than the other three datasets, since the template-based classification framework requires big data to construct stable RSTTs. That may explain why the proposed method performed best on NYU (216 subjects) and worst on NI (48 subjects).

Table 7 Comparison of results with four methods on each dataset
Full size table
Conclusion
In this work, we propose a spatiotemporal attention auto-encoder (STAAE) to model volumetric rfMRI sequence. With attention mechanism, the proposed STAAE can address global dependencies in rfMRI. Thus, the STAAE becomes a spatiotemporal model that can decompose the rfMRI data into meaningful temporal and spatial patterns. Based on the resting state temporal templates (RSTTs) derived by STAAE, we construct a novel classification framework that performed excellent on five datasets from ADHD-200 compared with traditional methods. However, there are still some limitations in this research. The proposed template-based method requires big data to construct stable temporal templates. Therefore, the data size will affect the performance of the method. Besides, other factors such as age and gender may also affect the performance. If enough data are available in the future, it is possible to construct temporal templates based on different age stages or other criteria, which may greatly improve the performance of the proposed method.

To conclude, this work not only provides a spatiotemporal model for learning brain networks, but also contributes a new classification framework based on resting state temporal templates (RSTTs). Predictably, other spatiotemporal model will also work on our classification framework. This platform may inspire researchers to construct their own templates for brain disease identification. Besides, the combination of RSTT and deep learning methods is also worthy of further study. In the future, we will try to explore more meaningful temporal templates and analyze the impact of individual templates and correlations of multiple templates, with the goal of further improving the classification performance on multiple datasets. Besides, the proposed method is expected to be applied to data processing of large-scale biological experiments, such as nanopore detection data results.

Keywords
Resting state fMRI
Temporal pattern analysis
ADHD classification
Spatiotemporal attention auto-encoder