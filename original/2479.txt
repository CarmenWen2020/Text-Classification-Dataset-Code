Image fusion is a method through which an image collection is fused into a composite image by fusing important characteristics from sources. This fused image is more informational, accurate and comprises of all the required information that better enhances human visual perception and machine vision. In this paper a new technique is suggested for the fusion of infrared (IR) and visible (VIS) images. The primary problems for image fusion at feature levels are that artefacts and noise are introduced in the fused picture. The weight map generated by the modified naked mole-rat algorithm (mNMRA) is used to retain important information without using artefacts in a final fused image. The proposed FNMRA fusion method is based on a feature-level fusion after the refinement of weight maps, utilising the WLS approach. This allows the prominent object information from the IR image to be included in the VIS image without any distortion. Experiments on twenty-one image data sets are conducted to verify the fusion performance of the suggested approach. The qualitative and quantitative analysis of fusion results concludes that the suggested technique works well for most image data sets and performs better than some state-of-the-art current methods.

Introduction
The requirement for optimal image processing system performance has prompted the design of a variety of image fusion technology that combines data captured from the many sensors [1,2,3,4]. The objective of image fusion is to fuse the measurable properties of pictures into a fused image reflecting the greatest visual effects [5, 6]. This fused image provides information which cannot be obtained by independently examining several pictures [7,8,9,10]. Details from CCD sensors are in certain applications combined with information acquired by IR sensors and the process is called multi-modal. Multi-modal image fusion is a combination of information obtained from multiple sensors that eventually generates information with less uncertainty and more information compared to each sensor’s individual performance [11,12,13,14,15,16]. Figure 1 of the “Sandpath” image pair provides an example that depicts the same scene through different modalities. In addition to edges and other highlights, the image acquired by the CCD sensor is highly textured with good resolution. At the other end, a clean and monotonous view of the human object is given by the image captured by the IR sensor and the structure features practically disappear. Variation of intra-object intensity is virtually nil, however edges are untouched, and human presence is more importantly recognized. The aim thus is to merge the two pictures to maintain the distinct textures of the scene, but also to emphasise the human being. An ideal theoretical fusion would just extract the human contour from IR and ”paste”, so that the visual image is kept entirely intact, into the detailed background of the camera.

Fig. 1
figure 1
Example of image pair acquired by two different sensors and fusion a Visual image, b IR image and c Fused image

Full size image
Different image fusion techniques have been developed over the last 3 decades to improve the quality of images and make significant use of them in different applications. For converging signals, images, features and symbols, multi-sensor image fusion is widely employed and has various applications in many fields such as robotic, computer vision, military field, remote sensing, satellite cloud images, biological recognition, medical images, target detection, surveillance [15,16,17].

Related work
Image fusion
In the previous several decades’ numerous approaches of image fusion have been suggested that have exhibited some obvious improvements in the outcomes. Before moving on to fusion technique it is important to understand the different levels at which fusion rules can be imposed on an image, which are: pixel level, feature level, and decision level [18,19,20,21]. Pixel level schemes qualify under the spatial domain and have got much exposure because of less complexity and minimum artefacts. At the pixel level, the final image is imparted with quality information as original information from source images is directly merged at the lowest physical level. The widely used algorithms in pixel-level image fusion may be classified as substitution techniques such as Principal Component Analysis (PCA) [22], Independent Component Analysis (ICA) [23], and transform domain techniques such as Pyramid Transform (PT) [24,25,26], Discrete Wavelet Transform (DWT) [27], Curvelet [28], shift-invariant shearlet transform [11], and contourlet transform [24]. These conventional methods may be carried out with very less computation cost but they are not able to preserve adequate details and possess noise [29].

Features are evidently the collection of pixels that are responsible for image texture, corners, edges, and contours [30]. For features level fusion, desired features are extracted from separate sensor images and correspondingly fused [31, 32]. Decision level fusion is a next-level fusion approach in which decisions coming from initial object detection and classification task are fused [11].

IR and visible image fusion
In line with the previous literature review, IR and VIS methods may also be classified as pixel-based fusion and region-based fusion [3, 4, 21, 32,33,34]. The element that detects wavelengths up to 14,000 mm instead of a visible light range of 400–700 nm is used for IR image sensors. Many IR-based civil sector tools have been developed in recent years [35]. The IR cameras are convenient to use in low light ambiance, on the other hand, digital CCD cameras are appropriate for daylight scenes. The information derived from an image captured by an IR camera can be used to match the information at corresponding locations in the image that is derived from VIS sensors.

For more than 2 decades, the fusion and improvement of multiband night time monitoring and navigation imagery IR and VIS image fusion is the area of intensive research. IR images are more resilient against environmental deterioration and varying light levels, also it is prone to targets whose electromagnetic radiation is more visible [36]. On the other hand, the VIS image exhibit superior visual quality with accurate background details. VIS picture is collected by the sensor through the optical reflection of an object, which typically includes more visibly reflective properties, texture details as well as higher pixel pitch. The strategy of image fusion is to derive useful details from various forms of image sensors like IR and VIS within separate modes [37, 38]. The fused image incorporates the benefits of input pictures, and emphasizes object location in the IR image. Moreover, it gives more descriptive information as compared to individual bands.

Several empirical studies have reported that region-based picture fusion exhibits elevated performance to retain spatial and color information. Region-based methods of image fusion segment the source images into a series of clusters or segments using an effective region-based algorithm and then merge them region by region [12, 21, 23, 39]]. In the final fused image, the salient details of the IR picture should be fused explicitly to ensure that it preserves as much knowledge as practicable about the IR. The most important stage of the region-based fusion process is to extract salient features from the IR image. In order to conserve the valuable insights from input images, a region-based novel image fusion approach is proposed in this paper. This is the first instance, to the best of the authors’ understanding, that the meta-heuristic algorithm-inspired segmentation is used to fuse IR and VIS images based on a region-based approach.

The proposed FNMRA approach is based on selecting the correct and distinct features of the low-frequency details through segmentation derived from a modified naked mole rat algorithm (mNMRA), which are further used to compute weight map function for IR and VIS images. A good segmentation based on the objects would naturally have excellent features; as a consequence, the region-based IR and VIS picture fusion approaches will use segmentation to get the salient area of an image.

Segmentation based fusion
Segmentation is a significant task and pre-processing step in computer vision and image processing. It is the simplest, fastest and most effective technique that is capable of discriminating against objects from the background through a set of pixel-level thresholds. In certain examples of image processing, it is required to segregate the foreground object from grey-level pixels of background [40]. It possesses a variety of applications in different fields like in medical, IR images, artificial intelligence, surveillance, remote sensing for specific target recognition, medical imaging [2,3,4,5, 7, 8]. The basic principle behind thresholding is to calculate the optimum threshold value to differentiate the target from the background [2, 8, 41, 43]. The thresholding techniques are divided into two categories, parametric and nonparametric [43]. Both parametric and nonparametric approaches are used to find optimum threshold value, which in turn leads to precise segmentation results. In parametric method estimation of the Probability Density Function (PDF) is required, which itself a complex process. It generally consumes more time and hence more expensive in terms of computational cost. On the other side, the nonparametric methods produce more accurate results with less computational cost [44]. In the nonparametric approach, the calculation of threshold values is done with the help of measurement of entropy, local maxima, between-class variance, entropy, the error rate, and so forth [45]. Instead of using one-dimensional thresholding, nowadays researchers have proposed two-dimensional and even multidimensional methods for thresholding [46,47,48].

Methodology
In this section, a novel segmentation based fusion methodology is proposed. The objective behind the FNMRA fusion technique is to fuse the IR (𝐼IR) and VIS (𝐼V) images without introducing any artefacts. The block diagram of the proposed FNMRA approach is shown in Fig. 2. Each step involved in this algorithm is described in depth in forthcoming sub-sections. In the first step, the segmented image (𝐼S) is computed based on metaheuristic-based segmentation. For weight map computation of IR image, a modified mNMRA is proposed. Further the weight map function for VIS (𝑊1V) is derived from the IR weight (𝑊1IR) using Eq. 22. The resultant weight maps are refined based on weighted least square (WLS) optimization [49]. The refined weighted maps for IR image (𝑊2IR) and VIS image (𝑊2V) are calculated using Eq. 23. The resultant fused image is obtained by fusing the IR (𝐼IR) and VIS (𝐼V) source images using corresponding visible and IR weight maps. Weighted average-based pixel-wise fusion is done to produce final fused image. In order to perform segmentation OTSU as an objective function to perform segmentation is discussed in the forthcoming Sect. 3.1. Section 3.2 conferred NMRA algorithm and proposed modified algorithm mNMRA is presented in Sect. 3.4. Weight map generation and adopted fusion strategy are explained in Sects. 3.5 and 3.6, respectively. The experimental results with comprehensive analysis are presented in Sect. 4.

Fig. 2
figure 2
Illustration of proposed FNMRA fusion algorithm consisting of three principal blocks. For the sake of simplicity the process is explained for two input images. False colours are used to depict segmented regions in IR image. Input images courtesy of Alexander Toet [16]

Full size image
OTSU’s between class variance
Image segmentation on IR images is being used for a separation of the IR target from the background in the proposed FNMRA fusion approach which is used for computing weight maps. Thresholding is the best method to perform segmentation and the most preferred method is OTSU method [50]. It is an automatic, unsupervised, and nonparametric method to obtain the threshold value that aims to maximize the inter-class variance thereby minimizing the intraclass variance between pixels in each class. Pixel intensities within the segment should be close to each other and for different segments, it should be well separated. 𝑏1,𝑏2,…,𝑏𝑛 are different classes of image with different threshold values, which are defined as follows:

𝑏1=𝑃1𝜔0,𝑃2𝜔0…𝑃𝑇𝜔0,     𝑏2=𝑃𝑇+1𝜔1,𝑃𝑇+2𝜔1…𝑃𝐿𝜔1
(1)
where 𝜔0=∑𝑇𝑖=1𝑝𝑖 and 𝜔1=∑𝐿𝑖=𝑇+1𝑝𝑖.

The average levels of 𝜇𝑎 and 𝜇𝑏 for two classes of 𝑏1 and 𝑏2 are as follows:

𝜇𝑎=∑𝑖=0𝑘𝑖𝑝𝑖𝜔0, 𝜇𝑏=∑𝑖=𝑘+1𝐿𝑖𝑝𝑖𝜔1
(2)
If the mean intensity of an image is given by 𝜇𝑇 then

𝜔0𝜇𝑎+𝜔1𝜇𝑏=𝜇𝑇,       𝜔0+𝜔1=1
(3)
Function 𝑓 needs to be maximized to perform thresholding using OTSU as a function

𝑓1 =𝜔0(𝜇𝑎−𝜇𝑇)2+𝜔1(𝜇𝑏−𝜇𝑇)2
(4)
OTSU’s may be further extended from bi-level to multi-level thresholding. Assume an image having 𝐿 grey levels (1,2,…,𝐿),𝑁 number of pixels and having is m thresholds with different 𝑚−1 different classes. Histogram of an image with 𝑓𝑖 as the frequency of grey level, 𝑖 is given by {𝑓0,𝑓1,𝑓2,𝑓3,…𝑓𝐿−1}.

Extended between the class value is given by

𝑓(𝑇)=∑𝑖=0𝑇𝑊𝑖(𝜇𝑖−𝜇𝑇)2
(5)
If between class variance has a maximum value, then within class variable will always have a minimum value, where 𝑓OTSU represents fitness function and maximizing this would correspond to optimal intensity threshold levels.

𝑓OTSU(𝑇)=∅𝑜=max(𝑓(𝑇)),  0≤𝑇≤𝐿−1
(6)
𝑓OTSU(𝑇)==∅𝑜max(𝑓(𝑇)),  0≤𝑇≤𝐿−1,𝑖=1, 2, 3…, 𝑇
(7)
Object tracking in IR images can be achieved simply as it displays unimodal peaks for objects, but in complicated contexts of equivalent intensity values, it becomes a difficult process. In such cases global threshold value does not work, adaptive threshold value-based segmentation is required. Multilevel segmentation of the IR image is illustrated in Fig. 3 including five thresholding values and IR segmented images with pseudo colour.

Fig. 3
figure 3
Segmentation results: a IR source image b Histogram depicting threshold values, c IR segmented image 𝐼𝑠 d IR segmented image with pseudo colours

Full size image
Naked Mole-rat algorithm
Their computational costs grow exponentially when used for multi-level thresholds using the OTSU technique stated in the preceding section. To find 8 threshold values for the OTSU method around 40 years are required and for 9 threshold values time around 10,000 years are required using Intel i7-4770K CPU [51]. In order to overcome this drawback multiple heuristic-based algorithms have been developed, which divide histogram into multiple regions by choosing optimal threshold values. The behavioural characteristics of naked mole rats (NMRs) motivate researchers to develop an algorithm for optimization called NMR algorithm (NMRA) [52]. The algorithm mimics matting patterns of NMRs. The breeders are the most efficient NMR of workers and are intended only for mating with the queen, while the remaining NMRs carry out other tasks. Workers carry out the tasks and the best performer among the workers will be shifted to the breeders’ group. Simply, the workers who perform their task well will become breeders, and those who are not performing well are sent back to the workers’ pool again. The best performer of the breeder’s group will be the queen’s mating partner. The algorithm consists of three steps. The group of NMRs is created initially, then the group of NMRs is divided into workers and breeders. The selection of breeder groups is performed based on breeding probability. Following are the steps of NMRA:

Initialization
Initially, the population of n NMRs are randomly generated, with each NMR in the range (1,2,…,𝑛) represented in a multi-dimensional vector space. The initialization of each NMR takes place in the following manner:

NMR𝑖,𝑗= NMRmin,𝑗+𝑈(0,1)×(NMRmin,𝑗− NMRmax,𝑗)
(8)
where 𝑖∈[1,2,…,𝑛], 𝑗∈[1,2,…,𝐷], NMR𝑖,𝑗 is the ith solution in the jth dimension, NMRmin,𝑗 and NMRmax,𝑗 are the minimum and the maximum bounds of problem and U(0, 1) is a uniform random number. The objective function and its fitness value are calculated after initialization. On the basis of the fitness function, breeders (B) and workers (W) are defined, and the best solution d is determined.

𝑤𝑡+1𝑖= 𝑤𝑡𝑖+ 𝜆(𝑤𝑡𝑗−𝑤𝑡𝑘)
(9)
where 𝑤𝑡𝑖 represents 𝑖th worker in 𝑡th iteration, 𝑤𝑡+1𝑖 represents a new fitness solution and 𝑤𝑡𝑗,𝑤𝑡𝑘 are the random solutions selected from the pool of workers’. The cost of 𝜆 is obtained in the range of (0, 1) from a uniform distribution.

Worker phase
In worker’s phase, they try to improve their potential to become a breeder and have the chance to mate the queen. Hence the latest NMR approach is generated based on its own knowledge of local details. In order to select a new solution current health value of NMR is determined and if the fitness of the previous solution is better than the current solution, then the previous solution is chosen and memorized. The corresponding solution is chosen otherwise. The new solution is generated from the older one using the following equation.

Breeder phase
In order to be selected as a mate or to stay as a breeder, the breeders need to update their NMRs. The NMRs of the breeder are updated according to the breeding probability 𝑏𝑝 with regard to overall best d. This 𝑏𝑝 is any random number chosen in the range of (0, 1). If breeders fail to adopt new fitness value, then they may be demoted to the previous worker’s pool.

𝑏𝑡+1𝑖=(1−𝜆)𝑏𝑡𝑖+𝜆(𝑑−𝑏𝑡𝑖)
(10)
where 𝑏𝑡𝑖 represents 𝑖th breeder in 𝑡th iteration, 𝜆 controls the frequency of mating and provide assistant to identify the new breeder 𝑏𝑡+1𝑖 in the next iteration. Initially, breeder’s probability 𝑏𝑝 is set to 0.5. The whole search procedure is repeated iteratively until the satisfaction of termination criteria. Finally, the possible solution of the problem under study is the best breeder selected from the entire population.

Local neighbourhood search (LNS)
NMRA uses the current solution and local information to improve the search space in the worker phase. In the proposed the mNMRA algorithm, LNS model is introduced to further strengthen the search functionality to increase convergence speed. The main idea is to use the best solution found till now to upgrade the current solution in a small territory of the present solution instead of considering the whole population. For updating the position of the individual, the history of the neighbourhood of an individual is considered, and the graph of their interconnections is called the neighbourhood structure.

Assume that in the current NMR population 𝑋=(𝑋1, 𝑋2, 𝑋3,. . . , 𝑋NP), 𝑋𝑖(𝑖∈[1, NP]) is a vector and its dimension is 𝐷. That vector’s indices are random in a bid to preserve that neighbourhood’s diversity. The neighbourhood of radius (𝑟 is a nonzero integer and 2𝑟+1<𝑁𝑃) is defined for each 𝑋𝑖 vector; that is, 𝑋𝑖 neighbourhood consists of 𝑋𝑖−𝑟, . . . , 𝑋𝑖, . . . , 𝑋𝑖+𝑟. The model for LNS is defined in the following equation.

𝐿𝑖=𝑋𝑖+𝑚∗(𝑋𝑛_opt−𝑋𝑖)+𝑛∗(𝑋𝑝−𝑋𝑞)
(11)
where 𝑋𝑛_opt is the best vector in the 𝑋𝑖 and 𝑝,𝑞∈[𝑖−𝑟,𝑖+𝑟] (𝑝≠𝑞≠𝑖 neighbourhood, and 𝑚,𝑛∈𝑟𝑎𝑛𝑑() are the scaling factors. The new best solution is updated according to Eq. 11 in the improved version of NMRA, and the updated solution performs the worker phase.

𝑥𝑡+1𝑖=𝐿𝑡𝑖+𝑟∗(𝑥𝑡𝑘−𝑥𝑡𝑚)
(12)
where 𝐿𝑖 is the best solution updated by LNS and 𝑥𝑡𝑘 and 𝑥𝑡𝑚 are two random solutions with respect to 𝑘th and 𝑚th NMR. In this case 𝑘≠𝑚, 𝑟 is a scaling factor and 𝑟∈𝑟𝑎𝑛𝑑().

Modified naked mole rat algorithm
NMRA has gained attention among researchers in the recent past due to its linear nature. The revised improved NMRA seeks to achieve enhanced performance by improving its basic exploration and exploitation capabilities both. Exploration has been improved by the introduction of search equations inspired from the Grey Wolf Optimizer (GWO) [2] in basic NMRA, exploitation has been enhanced by LNS driven by a knowledge of the best player so far found in a small territory of the present solution [3] along with differential equations. The concept of LNS is discussed in Sect. 3.3.

The worker phase is improved in the modified NMRA to enhance the performance of conventional NMRA. In this instance, the iterations may be separated into two parts. For the latter part of the iterations, the regular NMRA’s general Eq. 9 is used, and a new search equation is added for the first half. This new search equation is strengthened by taking the mean from the set of search agents of the first three solutions. Such three solutions are created based on the best solution available. The proposed equations in this respect are

𝑤1=𝑤𝑖−𝐴1(𝐶1⋅𝑑−𝑤𝑖),𝑤2=𝑤𝑖−𝐴2(𝐶2⋅𝑑−𝑤𝑖),𝑤3=𝑤𝑖−𝐴3(𝐶3.𝑑−𝑤𝑖)
(13)
𝑤new=𝑤1+𝑤2+𝑤33
(14)
GWO search equations have inspired the basic idea for this change [53, 54]. In GWO, the solution is created by adjusting the parameters to the best search agent location. The same definition is used to get a new solution

𝑤𝑡+1𝑖= 𝑤𝑡new+ 𝜆(𝑤𝑡𝑗−𝑤𝑡𝑘)
(15)
where 𝐴1, 𝐴2, 𝐴3, 𝐶1, 𝐶2, 𝐶3 belongs to 𝐴 and 𝐶 respectively. 𝐴 and 𝐶 are given by

𝐴=2𝑎𝑟1−𝑎, 𝐶=2.𝑟2
(16)
where 𝑎 is a linearly decreasing (0, 2) random number in relation to iterations, 𝑟1 and 𝑟2 are two uniformly distributed random numbers. These random numbers allow the search agents to spread at any location within the search space and thus increase the NMRA’s explorative capabilities.

Furthermore, to improve the exploitation efficiency of NMRA, the population, in this case, is divided into three sections, and the final solution is evaluated using three separate search strategies. The general equation similar to standard NMRA is used as follows for the first part of the population:

𝑤𝑡+1𝑖={𝑤𝑡new+ 𝜆(𝑤𝑡𝑗−𝑤𝑡𝑘),  if 0≥𝑡≥ 𝑡max/2𝑤𝑡𝑖+ 𝜆(𝑤𝑡𝑗−𝑤𝑡𝑘),  if 𝑡max/2+1≥𝑡≥ 𝑡max 
(17)
For the 2nd part of the population, LNS based search equation is used inspired by Eq. 11 as follows:

𝑤𝑡+1𝑖={𝐿𝑡new+ 𝜆(𝑤𝑡𝑘−𝑤𝑡𝑚),   if 0≥𝑡≥ 𝐿𝑡𝑖+ 𝜆(𝑤𝑡𝑘−𝑤𝑡𝑚),   if\ 𝑡max/2+1≥𝑡≥ 𝑡max 
(18)
where 𝐿𝑡𝑖 is the best solution updated by LNS, 𝜆 is a scaling factor and 𝜆∈rand(). Here 𝐿new=𝑤new+𝑚∗(𝑤𝑛_opt−𝑤new)+𝑛∗(𝑤𝑝−𝑤𝑞) using Eq. 11.

For the third part of the population, a new search equation inspired from differential equation is used. This new search equation is given by

𝑤𝑡+1𝑖={𝑤𝑡new+  𝜆((𝑤𝑡𝑗−𝑤𝑡𝑘)+(𝑤𝑡𝑝−𝑤𝑡𝑞)),  if 0≥𝑡≥ 𝑡max/2𝑤𝑡𝑖+  𝜆((𝑤𝑡𝑗−𝑤𝑡𝑘)+(𝑤𝑡𝑝−𝑤𝑡𝑞)),     if 𝑡𝑚𝑎𝑥/2+1≥𝑡≥ 𝑡max 
(19)
Third, in the basic NMRA breeder phase, the search instructions of the upcoming breeders are merely updating procedures within the population by optimum NMR. The entire algorithm quickly gets into premature convergence if the globally optimal individuals collapse against local optimal. Therefore, the leading function of the best individuals potentially residing close to the optimum solution should be employed to minimize the probability of algorithm getting into the local optimum.

The random aspirants close to the optimum solution is deployed to substitute the present optimum solution to direct the search for the algorithm to expand the probability of skipping algorithms from the local optimum. The updated approach for the neighbourhood pursuit of the optimal individual in mNMRA is

𝑏𝑡+1𝑖=(1−𝜆)𝑏𝑡𝑖+𝜆(𝑑(1+𝜇.𝑢𝑛𝑖𝑓𝑟𝑛𝑑(−1,1))−𝑏𝑡𝑖)
(20)
where 𝜇 is the disturbance coefficient and (−1,1) is the uniform distribution number within (−1,1). The present optimum solution is considered as the centre and 𝜇 as the step size in the neighbourhood quest for the optimal individual and the algorithm searches between segment 𝑑(1−𝜇.unifrnd(−1,1)) and 𝑑(1+𝜇.unifrnd(−1,1)). This essentially extends the search direction and raises the probability of algorithm jumping from the local optimal. Algorithm 1 depicts a pseudocode of the proposed mNMRA algorithm.

figure a
The whole search procedure is repeated iteratively until the satisfaction of termination criteria. Finally, the possible solution of the problem under study (the fitness function Eq. 7) i.e. 𝑓OTSU(𝑇)=∅𝑜 is the best breeder (with maximum/optimum threshold value) selected from the entire population.

Weight map computation
In the FNMRA method, values calculated using mNMRA based thresholding are used to obtain a segmented image and corresponding weight map. Let the IR image is operated by mNMRA to produce segmented image, that is to say 𝐼𝑠(𝑥,𝑦)=nNMRA(𝐼IR (𝑥,𝑦)). The weight map for IR and VIS image is given by the following equations

𝑊1IR(𝑥,𝑦)=𝐼𝑠(𝑥,𝑦)
(21)
𝑊1V=max(𝑊1IR(𝑥,𝑦))−𝑊1IR(𝑥,𝑦)
(22)
where 𝑊1IR(𝑥,𝑦) is weight map function of IR image, 𝑊1V(𝑥,𝑦) is a weight map function computed for the VIS image, and 𝐼𝑠(𝑥,𝑦) is a segmented image.

The weight maps computed in Eqs. 21 and 22 are hard and noisy and are not suitable for single scale-weighted average fusion of input images. To eliminate the artefacts in fused image due to fusion process, the weight maps are to be refined using WLS [49]. Nonetheless, attributes of IR and VIS images are very dissimilar; VIS images mostly include fine-scale structural data, whereas IR typically includes coarse-scale structures or other dissenting IR specifics and noise. Direct fusion may consider more insignificant information or noise from the IR image and less fine-scale information from the VIS images. In order to avoid such circumstances, prior to fusion weights are refined using the WLS optimization scheme. In the proposed FNMRA fusion algorithm, WLS filter is utilized to refine the weighted maps of IR and VIS images that are illustrated in Fig. 4 Many researchers have used WLS filter to develop edge-preserving multi-scale decomposition of image [49].

The WLS is an edge preserving filter and smoothing filter, it helps to maintain the edges by finding the best possible balance between blurring and sharpening. It has the greatest potential to progressively sharpen the picture, while preserving the spatial information consistent.

In particular, given an input weight 𝑊1(IR)𝑝 an output weight map 𝑊2(IR)𝑝 having maximum similarity with 𝑊1(IR)𝑝 is required, also it should be as smooth as possible everywhere except around the edges and minimizing the following equation (i.e. Eq.23) would give refined weights.

𝑊2IR=∑𝑝((𝑊2(IR)𝑝−𝑊1(IR)𝑝)2+𝜆(𝑊𝑥,𝑝(𝑊1IR)(∂𝑊2IR∂𝑥)2𝑝+𝑊𝑦,𝑝(𝑊1IR)(∂𝑊2IR∂𝑥)2𝑝)
(23)
In Eq. 23, the spatial coordinate of a pixel is denoted by subscript p and 𝜆 is regularization constraint used to balance sharp and smooth parameters; a higher value will carry out more smoothening operation. 𝑊𝑥,𝑝 and 𝑊𝑦,𝑝 are the horizontal and vertical smoothness weights and (𝑊2(IR)𝑝−𝑊1(IR)𝑝)2ensures maximal similarity between input and output.

Fusion strategy
In this section, final image fusion is computed by considering source input images and their weight maps. After obtaining the weight maps for IR 𝑊2IR and VIS 𝑊2V, the pixel-wise single-scale weighted average composition is performed that is given in Eq. 24. The pixel wise weighted average fusion provides robustness to the fusion algorithm, as it minimizes the information loss.

𝐼𝐹(𝑥,𝑦)=𝑊2IR(𝑥,𝑦)×𝐼IR(𝑥,𝑦)+𝑊2V(𝑥,𝑦)×𝐼V(𝑥,𝑦)
(24)
Fig. 4
figure 4
Depiction of IR and VIS weight maps computed by the proposed FNMRA method: a IR weight map b Refined IR weight map c VIS weight map d Refined VIS weight map

Full size image
Experimental results
The final results of the FNMRA fusion technique are compared with seven existing techniques. The performance analysis is done based on four fusion performance metrics and the MATLAB codes of these metrics are available in the public domain. The techniques used to carry out comparative analysis are fusion method using cross bilateral filter (CBF) and discrete cosine harmonic wavelet transform (DCH) proposed by Shreyamsha Kumar [55, 56], JSR model with saliency detection fusion method (JSR) proposed by C.H. Liu, et.al. [57], and the gradient transfer fusion method (GTF) proposed by Qiheng Zhang et.al. [58]. Furthermore, deep convolutional neural network based method (CNN) [59], convolutional sparse representation (CSR) [60] and latent low-rank representation fusion method (LLR) [61] based fusion methods are also considered for qualitative and quantitative analysis.

Qualitative evaluation of fusion results
For qualitative evaluation, the pairs of three image datasets (“Camp”, “Traffic”, and “Steamboat”) are chosen from TNO data set [16] due to space constraint. Figures 5, 6 and 7 depict pairs of input images and fusion results for visual inspection. The images shown in Figs. 5a, b, 6a, b, and 7a, b are three pairs of IR and VIS images, respectively. In the IR images (see Figs. 5a, and 7a), the human target is visible, while the background details such as trees and building details are visible in the VIS images. In surveillance applications, the comprehensive details present in IR and VIS images are needed. In the “Steamboat” sequence (see Fig. 6a, b, different sections on the boat can be distinguished by their thermal profiles, while the complementary details are present in the VIS image. With the help of image fusion, it is easier to track objects and detect different activities on the boat.

Fig. 5
figure 5
Comparison of results “Traffic”: a IR image, b VIS image, c CBF, d DCH, e JSR, f GTF, g CSR, h CNN, i LLR, and j FNMRA

Full size image
Fig. 6
figure 6
Comparison of results “Steamboat”: a IR image, b VIS image, c CBF, d DCH, e JSR, f GTF, g CSR, h CNN, i LLR, and j FNMRA

Full size image
Fig. 7
figure 7
Comparison of results “camp”: a IR image, b VIS image, c CBF, d DCH, e JSR, f GTF, g CSR, h CNN, i LLR, and j FNMRA

Full size image
As shown in Fig. 5, the fusion results of JSR (Fig. 5e), GTF (Fig. 5f), CSR (Fig. 5g), CNN (Fig. 5h), and LLR (Fig. 5i) produces less information from VIS image of “Traffic”. The fusion results of CBF (Fig. 5c), and DCH (Fig. 5d) are visually distorted. As compared to other methods, the FNMRA is able to transfer details from VIS and IR images into the output fused image (Fig. 5j) that give situational awareness in the scene.

In Figs. 6 and 7, 6c–j and 7c–j depict the fusion results of compared methods and results proposed by the FNMRA for “Steamboat” and “Camp” image data sets, respectively. For the case of images shown in Fig. 6a, b, the ultimate challenge is to inject IR spectral details into VIS image for night-vision context enhancement. Figure 6j depicts better context enhancement and produces lesser artefacts due to the intensity-map difference between VIS and IR imagery. As it can be seen from the fusion results, the FNMRA method preserves background and IR target details accurately. We can observe form Fig. 7j, fences, roof of room and pedestrian are clearly enhanced in the fused image that provides complete information about the battle field. In addition, the salient IR target is perfectly injected into final fused image to locate the exact position of the person.

To sum up, the fusion results produced by FNMRA algorithm and other methods are discussed as follows: CBF and DCH contain the artefacts, and the saliency characteristics are not much flawless. The resultant images collected by JSR, JSR, GTF, CSR, and CNN include several ringing artifacts across the saliency details, as well as the detailed information, which is not obvious either. The results produced by LLR do not contain much detailed information about salient features. Conversely, the fused images produced by the FNMRA algorithm retain more detailed information while keeping saliency characteristics intact. Compared with the above methods of fusion, the fused imagery produced by the FNMRA method of fusion are more acceptable for human perception and has improved efficiency in the qualitative comparison approach.

Objective evaluation
Subjective analysis alone is not sufficient to authenticate the results; more quantitative review is necessary to evaluate the performance of FNMRA algorithm. In order to perform further analysis of fusion result, four widely used quantitative image fusion metrics are used. Based on these fusion quality metrics, quantitative analysis of pairs of twenty-one image datasets are chosen from TNO data set [16]. TNO data set is available to the public at https://doi-org.ezproxy.auckland.ac.nz/10.6084/m9.figshare.1008029.v1. The names of chosen image datasets are listed in Tables 1, 2, 3 and 4.

Edge based similarity index (𝑄𝐴𝐵/𝐹)
It gives the edge preservation detail in the final image from the source input images and given by Eq. 25 higher value of this metric is desired for good results and it ranges between (0-1) [62,63,64,65].

𝑄𝐴𝐵/𝐹=∑𝑀𝑖=1∑𝑁𝑗=1𝑄𝐴/𝐹(𝑖,𝑗)𝑔𝐴(𝑖,𝑗)+𝑄𝐵/𝐹(𝑖,𝑗)𝑔𝐵(𝑖,𝑗)∑𝑀𝑖=1∑𝑁𝑗=1𝑔𝐴(𝑖,𝑗)+𝑔𝐵(𝑖,𝑗)
(25)
where 𝐴, 𝐵 are source and 𝐹 is fused imagery and 𝑄𝐴/𝐹(𝑖,𝑗)\ and\ 𝑄𝐵/𝐹(𝑖,𝑗) are given as

𝑄𝐴/𝐹(𝑖,𝑗)=𝑄𝐴/𝐹𝑔(𝑖,𝑗)𝑄𝐴/𝐹𝛼(𝑖,𝑗),𝑄𝐵/𝐹(𝑖,𝑗)=𝑄𝐵/𝐹𝑔(𝑖,𝑗)𝑄𝐵/𝐹𝛼(𝑖,𝑗)
(26)
where 𝑄𝐴/𝐹𝑔 and 𝑄𝐵/𝐹𝑔 denotes strength of edge, 𝑄𝐴/𝐹𝛼 and 𝑄𝐵/𝐹𝛼 are the parameters representing preservation of orientation values, and 𝑔𝐴(𝑖,𝑗)\ and\ 𝑔𝐵(𝑖,𝑗) are weight values for images A and B, respectively, for pixel position (i,j).

Sum of correlation difference (SCD)
It demonstrates the extent of meaningful data being transmitted from input sources to the final result [14, 21, 33, 61]. Higher value of SCD denotes better fusion results and is given by following equation.

SCD=𝑟(𝐷1,𝐴)+𝑟(𝐷2,𝐵)
(27)
where 𝐷1=F-B and 𝐷2=F-A, F is fused image, A and B are input images and r(.) function calculates the correlation between, 𝐴 and 𝐷1, and 𝐵 and 𝐷2.

Structural similarity index (SSIM)
It is the most consistent technique to find similarity between two images as compared to other existing techniques like peak signal-to-noise ratio (PSNR) and mean square error (MSE). SSIM measures the extent of degradation of fused image as compared to input image [19, 27, 66, 67]. It is mostly preferred in the presence of ground truth but a modified version of 𝑆𝑆𝐼𝑀 is used as given in Eq. 28. The higher value of 𝑆𝑆𝐼𝑀 denotes better fusion results.

SSIM(𝐴,𝐹)=[2𝜇𝐴𝜇𝐹+𝐶1](2𝜎AF+𝐶2)[𝜇2𝐴+𝜇2𝐹+𝐶1](𝜎2𝐴+𝜎2𝐹+𝐶2)
(28)
SSIM(𝐵,𝐹)=[2𝜇𝐵𝜇𝐹+𝐶1](2𝜎BF+𝐶2)[𝜇2𝐵+𝜇2𝐹+𝐶1](𝜎2𝐵+𝜎2𝐹+𝐶2)
(29)
where 𝜇𝐴 , 𝜇𝐵 and 𝜇𝐹 are mean intensities, 𝜎𝐴 , 𝜎𝐵 and 𝜎BF are the standard deviation of two input images 𝐴 and 𝐵, and fused image 𝐶, respectively. 𝜎AF and 𝜎BF are the square root of covariance of two input and fused images, respectively, and 𝐶1 and 𝐶2 are constants. Modified SSIM for two source images in the absence of ground truth is calculated by taking average of two values as given by

SSIM(𝑎)=SSIM(𝐹,𝐴)+SSIM(𝐹,𝐵)2
(30)
Artefact measure 𝑁𝐴𝐵/𝐹
Fusion artefacts reflect visual details that the fusion cycle adds into the fused picture. Fusion objects are essentially false information which directly detracts from the usefulness of the fused picture and can have serious implications for fusion applications [36, 68, 69]. It gives the volume of noise or artefacts added in the fused image during the image fusion process, and its minimum value is preferred. It is preferred for comparison purposes because it gives in-depth evaluation of an image fusion method.

Table 1 Quantitative analysis based on 𝑁𝐴𝐵/𝐹
Full size table
Table 2 Quantitative analysis based on SSIM
Full size table
Table 3 Quantitative analysis based on SCD
Full size table
Table 4 Quantitative analysis based on 𝑄𝐴𝐵/𝐹
Full size table
The values of each measured quality index for the 21 image data sets are given in the tables (see Tables 1, 2, 3, 4). For each calculated quality metric, the best value is shown in bold. From Table 1, we can notice that FNMRA method is performing well for 15 out of 21 image datasets in term of 𝑁𝐴𝐵/𝐹. As shown in Table 2, proposed FNMRA method exhibits highest value of SSIM for 16 out of 21 image datasets. For SCD fusion metric, FNMRA yields larger values for 15 image data sets (See 3). As can be noticed from Table 4, the FNMRA method obtains the largest value of 𝑄𝐴𝐵/𝐹 for 15 out of 21 image datasets. The FNMRA method also gives best averages for all 21 image data sets from the analysis. This implies that the FNMRA based fused image combines more meaningful and accurate information from the source images. By this study we can conclude that, over other approaches, the current image fusion procedure produces higher efficiency.

Analysis of quality metric’s ranks is shown in Fig. 8. Recall that 𝑁𝐴𝐵/𝐹 measures the volume of noise or artefacts added in the fused image and its minimum value is preferred [36]. Rank-1 is given to a minimal value method in comparison to other fusion methods and the fusion method with maximum value of 𝑁𝐴𝐵/𝐹 quality metric is rank-8. The box plot shown in Fig. 8a depicts that FNMRA method possesses a small scatter plot and most rankings are closer to Rank-1 for 21 image data sets. In contrast Rank-1 is offered the technique, which gives the maximum value of quality metrics of SSIM, SCD and 𝑄𝐴𝐵/𝐹 and the fusion method with minimal value of SSIM, SCD, and 𝑄𝐴𝐵/𝐹 quality metrics is rank-8. From the box plots provided in Fig. 8b–d we can notice that the rankings for FNMRA are closer to Rank-1.The statistical analysis of quality measures of 21 ranking data sets is to identify the most consistent and distinguishing approaches to transfer information from input images to fused images without any artefacts. This ranking analysis therefore sets the basis for determining the effectiveness of the fusion process empirically in order to measure the ”quality” of the fused images. Statistical analysis based on ranking shows that the proposed FNMRA is better with respect to other fusion algorithms under comparison.

Fig. 8
figure 8
Box plots of quality metric Ranks for 21 image data sets. Rank-1 depicts good performance and Rank-8 depicts poor performance: a 𝑁𝐴𝐵/𝐹 Ranks, b SSIM Ranks, c SCD Ranks and c 𝑄𝐴𝐵/𝐹 Ranks

Full size image
Parameter selection and analysis of execution time
The proposed mNMRA based fusion approach is implemented in MATLAB 2018a, and the hardware configurations of machine are: 3.70 GHz Intel Core i5 processor and 4 GB RAM. Table 5 lists the CPU running time of compared methods and FNMRA method for three data sets when the FNMRA yields its best fusion results. For compared methods, the related parameters for fusion are set to the values reported by the authors in related publications. The total execution time of FNMRA approach includes the segmentation of IR image based on mNMRA, weight map refinement using WLS filter and fusion process. Since NMRA is a technique of stochastic optimization to be performed at least 10 times to obtain relevant data. The computational complexity analysis for NMRA is provided by 𝑂(𝑛.𝑑.𝑡max) where t corresponds to the entire iteration, d is the dimension size and n is a population size of the problem. The number of iterations selected in the suggested technique is 150, so as to maintain a balance between accuracy and calculation time. Empirically, a value of 𝑛=30 and 𝑑=5 has been proven to produce superior outcomes in IR image segmentation. For “Camp”, “Traffic”, and “Steamboat” the mNMRA consumes 1.21 s, 2.04 s and 1.98 s to compute 5 threshold values, respectively.

In the proposed FNMRA approach, a bottleneck is a weight map refinement based on WLS that consumes the maximum time. For “Camp”, “Traffic”, and “Steamboat” the weight map refinement consumes 1.41 s, 4.13 s and 3.32 s, respectively. The default parameters proposed in [49] are set for weight map refinement of IR image. The preconditioned conjugate gradients (PCG) is used in the WLS optimization. A feasible method to reduce the execution time for real-time applications is implementing PCG [70] on Graphics Processing Unit (GPU), which was proposed by Weber et al. [71]. We believe that an efficient parallel implementation of WLS optimization on GPU can minimize the total implementation time of the weight map refinement process.

Table 5 Comparison of execution time (Unit: seconds)
Full size table
Discussion
We propose a segmentation-based fusion approach that can create a fused image from infrared and visible imagery. The mNMRA is used to distinguish the IR target from the infrared image background that is further applied for weight map computation. Spatial details are often retained by using the proposed process, and infrared targets can be clearly seen in the resulting fused images. We discovered that when this IR image is combined with a visible image, a better image resulting with even tiny details of the background and the concealed targets is acquired. We have observed that the appearance of details in the fused image depends upon thresholding operation applied on IR imagery. The experimental findings indicate that computing weight maps based on mNMRA yielded substantial improvements in the fusion results. This FNMRA method has opened up a potent way to utilize segmentation of IR target in thermal imagery, which would certainly helpful in different fields such as military surveillance and medical imaging. Quantitative and qualitative inspection of the suggested fusion results therefore shows that the IR target and background in the fused image are accurately preserved. Our suggested approach, because of its extra model for weight map refinement based on WLS, is more expensive than previous fusion techniques in terms of computing complexity. Consequently, the approaches must be studied in future in order to satisfy the need of computationally efficient segmentation and weight map refinement.

Conclusion
A novel feature level image fusion algorithm using segmentation based on mNMRA is proposed in this manuscript. Firstly, using mNMRA-based segmentation, the IR image is segmented into different groups. These weights map functions are refined using WLS optimization. The resulting segmented image is used for measuring weight map function used in the process of fusion. Finally, the fused image is reconstructed using pixel-wise weighted average fusion. The efficiency of the FNMRA is measured using both qualitative and quantitative methods. Comparison of the FNMRA algorithm’s experimental results show better performance in terms of different performance metrics which include SSIM, SCD, 𝑄𝐴𝐵/𝐹, and 𝑁𝐴𝐵/𝐹. The proposed fusion results show complete preservation of an object and background details without introducing any undesired artefacts and thus, the FNMRA technique is suitable for the fusion of IR and VIS image data sets.

To enhance segmentation in IR Imagery, we will build an algorithm that allows calculation of threshold value in the near future with more precise selection of parameters. Therefore, the alternative thresholding criteria for improving the performance of the fusion technique will be recommended in the future. Moreover, the expansion of the suggested fusion technique for multiple pictures might be an intriguing topic for future investigation.

Keywords
Infrared (IR)
Visible (VIS) image
Fusion
mNMRA
Segmentation
WLS