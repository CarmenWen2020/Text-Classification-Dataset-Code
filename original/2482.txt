Accurate classification of high-dimensional biomedical data highly depends on the efficient recognition of the data's main features which can be used to assist diagnose related diseases. However, due to the existence of a large number of irrelevant or redundant features in biomedical data, classification approaches struggle to correctly identify patterns in data without a feature selection algorithm. Feature selection approaches seek to eliminate irrelevant and redundant features to maintain or enhance classification accuracy. In this paper, a new wrapper feature selection method is proposed based on the chimp optimization algorithm (ChOA) for biomedical data classification. The ChOA is a newly proposed metaheuristic algorithm whose capability for solving feature selection problems has not been investigated yet. Two binary variants of the ChoA are introduced for the feature selection problem. In the first approach, two transfer functions (S-shaped and V-shaped) are used to convert the continuous version of ChoA to binary. In addition to the transfer function, the crossover operator is utilized in the second approach to improve the ChOA's exploratory behavior. To validate the efficiency of the proposed approaches, five publicly available high-dimensional biomedical datasets, and a few datasets from different domains such as life, text, and image are employed. The proposed approaches were then compared with six well-known wrapper-based feature selection methods, including multi-objective genetic algorithm (GA), particle swarm optimization (PSO), Bat algorithm (BA), ant colony optimization (ACO), firefly algorithm (FA), and flower pollination (FP) algorithm, as well as two standard filter-based feature selection methods using three different classifiers. The experimental results demonstrate that the proposed approaches can effectively remove the least significant features and improve classification accuracy. The suggested wrapper feature selection techniques also outperform the GA, PSO, BA, ACO, FA, FP, and other existing methods in the terms of the number of selected genes, and classification accuracy in most cases.

Introduction
Biological data, such as microarrays, are one of the most essential analytical tools for medical researchers and biologists. Microarray data analysis allows for more accurate diagnosis and prognosis of patient diseases, as well as better clinical decision-making. The main challenging issues associated with microarray data are the curse of dimensionality and complex interaction between features. Microarray data consist of a small number of patient samples with a large number of features (genes), most of which are redundant and irrelevant. The presence of irrelevant and redundant features in biological datasets might obscure the important ones, causing many learning algorithms to perform poorly. To overcome this challenge, feature selection (FS) is required [1]. FS is typically considered as a preprocessing mechanism that aims to choose a subset of significant features to alleviate overfitting, improve the accuracy and interpretability of the learned model, speed up the learning process, and reduce dataset storage memory requirements [2]. From a biological perspective, FS helps molecular biologists to identify the molecular mechanism driving cancer gene expression, interpret the underlying pattern of data to discover new therapeutic targets for those selected features, and reduce clinical costs [3, 4].

The FS process involves two main stages: feature-subset search and feature-subset assessment. In the first stage, a search strategy is needed to explore the search space to find the optimal feature subset, and a learning algorithm is required in the second stage to assess the quality of the selected feature subset [5]. A wide variety of FS methods for classification issues have been proposed, which can be divided into three classes: wrapper-based, filter-based, and hybrid approaches. Filter-based approaches focus solely on the interior characteristics of training data to rank features and are independent of any learning algorithm. Wrapper-based techniques, on the one hand, rely on a specialized learning algorithm (classifier) to determine the optimal set of features. The wrapper approaches typically outperform filter approaches in terms of classification performance, but require high computational time, especially for high-dimensional data due to the frequent use of learning algorithms in their search strategy. The filter-based methods are less time-consuming than the wrapper-based method since no learning algorithm is involved in their search stage. However, the filter-based models suffer in terms of accuracy because they are not iterative models and their search strategy only comprises a single iteration, making them get stuck in local optima easily [6]. A combination of these two approaches has also been proposed as hybrid models to combine their strengths.

Recently swarm intelligence (SI)-based optimization methods have gained a lot of interest due to their high performance in tackling FS problems. Some of most popular SI-based FS methods are Whale optimization algorithm (WOA) [1], ant colony optimization (ACO) [7], bat algorithm (BA) [3, 8], artificial bee colony (ABC) [9, 10], particle swarm optimization (PSO) [11], biogeography based optimization (BBO) [12], genetic algorithm (GA) [13,14,15], harmony search algorithm (HSA) [16], flower pollination (FP) algorithm [17], grasshopper optimization algorithm (GOA) [18], firefly algorithm (FA) [19], and binary dragonfly (BDF) algorithm [20]. However, because of the intricate interactions between features in biomedical data, the large feature search space, and the stochastic nature of the approaches, most of these algorithms are susceptible to the stagnation problem and may suffer from degraded performance [3, 5, 21]. Therefore, the door is still open for more improvement, and a strong search method capable of exploring the search space more complete, eluding local minima, and exploiting the global optimum more reliably is demanded to better address the FS problem.

The Chimp Optimization Algorithm (ChOA) [22] is a recent SI algorithm that mimics the chimpsâ€™ intelligent group hunting (IGH) behavior and their social diversity. For hunting prey, chimps are separated into four groups and undertake four different actions: dividing, chasing, blocking, and attacking. These four groups of the chimps and several operators such as diverse intelligence and sexual motivation were mathematically modeled to create ChOA with high exploration and exploitation ability. The ChOA has been successfully used to train neural network (NN) [23] parameters, and a hybrid version [24] and a modified version of ChOA [25] have also been presented. However, to the best of our knowledge, the effectiveness of ChOA has not been investigated in the feature selection problem.

In this paper, two new wrapper feature selection approaches based on ChOA are proposed to identify the optimal feature subset for biomedical data classification. In addition to using the main operators of the ChOA, some modifications need to be done on the algorithm in order to solve FS problems since the original version of the ChOA was created to address continuous problems. This paper primarily proposes two binary ChOA (BChOA) variants:

In the first version, two transfer functions (S-shaped and V-shaped) are suggested to map the continuous data into binary ones.

In the second version, the crossover operator is integrated with BChOA (BChOA-C) to empower the algorithmâ€™s exploration capabilities.

The fundamental idea is to assign a binary structure to each chimp in the population that indicates whether or not a feature belongs in the final list of features. As a fitness function, the accuracy of two learning algorithms, NaÃ¯ve Bayes (NB) [26, 27] and K-nearest neighbor (K-NN) [26, 28] is used in the proposed algorithms. The learning algorithm is trained with the given features and tenfold cross-validation (CV) is used to evaluate each chimp (candidate solution). Extensive experiments were carried out on five popular microarray datasets (large-scale biological data), as well as various small biological, text, and image datasets. For performance evaluation, the proposed approaches are compared with six previous binary optimization algorithms: PSO, BA, ACO, GA, FA, FP, and other well-regard filtering approaches. The conducted experiments demonstrate that the proposed approaches have better performance compared to the above-mentioned algorithms and several current state-of-the-art methods in the term of accuracy and number of selected features. Furthermore, the results show that incorporating a crossover operator into the BChOA improves the classification accuracy of the model.

The rest of the paper is structured as follows. Section 2 presents a review of recent literature on the FS techniques in biomedical data classification. Section 3 provides all the details about the ChOA. The suggested binary ChOA for feature selection is discussed in Sect. 4. The details of the conducted experiments and achieved results on well-known datasets are given in Sect. 5. The conclusion and future work are presented in Sect. 6.

Related works
FS is an NP-hard search problem [29] that aims to find the optimal number of features (attributes) from the original dataset without sacrificing the performance of the classification. Based on the number of features in the original dataset, the complexity of the problem grows exponentially. Therefore, metaheuristic search (MHS) algorithms have been employed to improve the obtained result and the computational time in large problems. There have been several attempts to review the FS methods [30, 31]. In this section, we briefly review various FS approaches which can be categorized into three classes: filter, wrapper (MHS-Based), and hybrid models.

The filter technique ranks each feature based on its discriminating power between different classes without considering any learning algorithm. In filter methods, various criteria are utilized to find the featuresâ€™ importance such as information theory, cross-entropy, symmetrical uncertainty, correlation, similarity, and statistical measures. Examples include Information Gain (IG) [32], Markov blanket [33], Correlation-based FS (CFS) [11], Fast Correlation Based Filter (FCBF)[34], Fisher score [35], Relief-F [36], Chi-square [37], Random Forest Ranking (RFR) [38], simplified silhouette filter (SSF) [39], Condition Mutual Information Maximization (CMIM) [40], Double Input Symmetrical Relevance (DISR) [41], and Minimum Redundancy Maximum Relevance (mRMR) [42]. The mRMR is a popular filter method in biological data that seeks features with the greatest relevance to the class and the least redundancy between them [8, 43].

The filter methods usually carry out on microarray datasets as a preprocessing step, to reduce the datasetâ€™s high dimensionality by removing redundant and irrelevant features within a reasonable time. The obtained dataset is then fed to wrapper algorithms to find the optimal feature subset. The hybrid model, which is extensively employed in biological data, is based on the idea of sequentially applying both a filter and a wrapper technique. Since the hybrid model supplies a reduced feature set to the wrapper technique, an extended search of feature subsets is avoided [6].

The wrapper technique utilizes a learning algorithm as a fitness function to evaluate the quality of the feature subset. The wrapper model is an iterative search procedure in which the learning algorithm's accuracy is employed to direct search space at each iteration. Various learning algorithms such as Support Vector Machine (SVM) [44], Decision Tree (DT) [45], Artificial Neural Network (ANN) [46], K-NN [47], and NB [48] have been used in wrapper-based FS method for better classification of biomedical data. Generally, wrapper-based methods can be divided into two categories: greedy and random search approaches. Examples of greedy methods include sequential forward selection (SFS) [49], backward selection (SBS) [50], and hill-climbing algorithm [8] in which a single feature is added or removed iteratively in a greedy manner. The random search approaches are mainly based on MHS algorithms. MHS algorithms are nature-inspired optimization algorithm (NIOA) that use randomness in their search strategy to explore a large portion of the search space.

The majority of NIOA's have been introduced for continuous search space and they should be converted to binary form to solve discrete optimization problems like FS [51]. Several transfer functions (TFs) have been utilized for these purposes within the NIOAs such as S-shaped [52], V-shaped [53], U-shaped [54], and X-shaped [55]. The TFs play a key role in the efficiency of binary NIOAs.

Different binary NIOAs have been proposed for FS so far using these TFs, which can be classified into four groups: evolution-based, SI-based, physics-based, and human-related approaches. The most popular evolution-based FS algorithm is a binary genetic algorithm (GA) [13, 14, 26, 38] that simulates Darwinian evolution concepts. Some of the most popular physical-based FS methods which mimic the physical concept in the world are binary black hole algorithm [43, 56,57,58], simulated annealing (SA) [59, 60], and gravitational search algorithm (GSA) [61]. The examples of human-based approaches in FS of biomedical data include Teaching Learning-based optimization (TLBO) algorithm [59], the BrainStorm Optimization (BSO) algorithm [62], and the JAYA algorithm [63] which are inspired by human behaviors in society. SI-based algorithms (SIA) are inspired by animalsâ€™ behavior in herds, flocks, colonies, or schools. SIAs have shown to be quite competitive with the other three types of NIOAs, and have several advantages over them, such as fewer parameters, fewer operators, and the ability to remember search space [22]. Some of the well-known suggested binary SIA for FS are PSO [2, 11], GOA [64], ABC algorithm [9, 10, 65], BA [3], Krill Herd algorithm (BKH) [32], Gray Wolf Optimizer (GWO) [21], Bacterial Foraging Optimization (BFO) algorithm [66], cuckoo search algorithm [67], and Moth Fame Optimization (MFO) algorithm [68]. The following is an analysis of some of the selected literature.

Wang et al. [33] proposed to integrate the Markov blanket filter technique into wrapper-based SFS for feature selection of biomedical data. This model speeds up the FS process by reducing the number of candidate features for the wrapper evaluation. K-NN, NB, and DT classifiers were used in this study as the fitness function for feature subset evaluation.

Dashtban and Balafar [26] introduced a hybrid evolutionary algorithm called an intelligent dynamic genetic algorithm (IDGA) for the FS of microarray data. First, Fisher score was used to reduce dimensionality and provide statistically significant features to the next step. Then, the IDGA method was applied to find the optimal feature subset. Moreover, three classifiers, namely SVM, NB, and K-NN, were utilized to measure the performance of IDGA. Later on, Zhou et al. [14] developed a problem-specific non-dominated sorting genetic algorithm (PS-NSGA), as a multi-objective FS algorithm for high-dimensional data classification. The suggested algorithm included a non-dominated sorting with a preference for accuracy, a rapid bit mutation operator, a mutation-retry operator, and a combination operator to solve the FS issue efficiently. Their study reported the proposed PS-NSGA approach achieves better classification performance and smaller feature subsets in comparison to existing evolutionary and traditional feature selection methods.

Shukla et al. [59] introduced a new hybrid wrapper strategy for determining the optimal feature subsets to predict cancerous-genes., based on the combination of TLBO with the SA algorithm called TLBOSA. First, CFS was utilized to filter the redundant feature from the biological datasets. Then, TLBOSA was used to identify the subset of the most informative features. Also, a new TF was proposed to convert the continuous version of TLBOSA to binary. It was found that TLBOSA outperforms other wrappers in terms of classification accuracy and a small subset of features.

An improved binary krill herd (MBKH) algorithm for FS has been developed by Zhang et al. [32]. The study utilized the IG filter method as a preprocessing step to rank and remove redundant features. Then, MBKH was applied to find out the best feature subset. In the suggested method, the hyperbolic tangent function was employed as the transfer function, and the chaos memory weight factor was introduced into the movement operators of the MBKH algorithm to enhance its local and global search abilities. A new hybrid filter-wrapper strategy was proposed in [21], in which robust mRMR (rmRMR) was used as a filter approach to choose the top-ranked features, and modified GWO (MGWO) with SVM evaluator was utilized as a wrapper approach to seeking the best subset of features. To increase the diversity of the population in the MGWO, TRIZ-inspired optimization operators were introduced in the original GWO, which result in a practical and effective FS tool to select the most informative features.

Although several binary NIOAs for FS have been presented, due to the stochastic nature of the NIOAs according to the No-Lunch theorem, there is still room for more improvements. This is one of the key inspirations for this study, in which two novel binary versions of ChOA, are developed and compared to current well-known discrete NIOAs in the literature for FS of high-dimensional biomedical data.

Chimp optimization algorithm
ChOA is a new SI-based optimization algorithm that was proposed by Khishe and Mosavi in 2020 [22]. The basic idea for ChOA comes from the chimpsâ€™ intelligence and sexual motivation in their group haunting, which differs from that of other social hunters. Due to its simplicity, local optima avoidance, high convergence speed, and low computational overhead, this approach has been widely used to determine the best possible solutions for complex optimization problems [24, 25].

Chimps' hunting behavior is divided into two phases: exploration and exploitation. The exploration entails moving, blocking, and chasing the prey which leads to discovering a wider region of the search space globally, while exploitation entails attacking the prey that provides local search potential across the promising areas discovered during the exploration process. To implement the steps of hunting, four groups of chimps are used: driver, chaser, barrier, and attacker. Each chimp in the population represents a candidate solution in the search space, and attacker, barrier, chaser, and driver chimps represent the best (leader), second best, third best, and fourth-best solutions, respectively. At each iteration, after determining the position of the attacker (ğ‘¥attacker), barrier (ğ‘¥barrier), chaser (ğ‘¥chaser), and driver (ğ‘¥driver) chimps (i.e., the four best chimps), the rest of the chimps (ğ‘¥chimp) are forced to update their positions according to the locations of these four best chimps using the following equations:

ğ‘¥1(ğ‘¡+1)=ğ‘¥attacker(ğ‘¡)âˆ’ğ´1.(ğ·attacker),ğ·attacker=âˆ£âˆ£ğ¶1.ğ‘¥attackerâˆ’ğ‘š.ğ‘¥chimp(ğ‘¡)âˆ£âˆ£ğ‘¥2(ğ‘¡+1)=ğ‘¥barrier(ğ‘¡)âˆ’ğ´2.(ğ·barrier),ğ·barrier=âˆ£âˆ£ğ¶2.ğ‘¥barrierâˆ’ğ‘š.ğ‘¥chimp(ğ‘¡)âˆ£âˆ£ğ‘¥3(ğ‘¡+1)=ğ‘¥chaser(ğ‘¡)âˆ’ğ´3.(ğ·chaser),ğ·chaser=âˆ£âˆ£ğ¶3.ğ‘¥chaserâˆ’ğ‘š.ğ‘¥chimp(ğ‘¡)âˆ£âˆ£ğ‘¥4(ğ‘¡+1)=ğ‘¥driver(ğ‘¡)âˆ’ğ´4.(ğ·driver),ğ·driver=âˆ£âˆ£ğ¶4.ğ‘¥driverâˆ’ğ‘š.ğ‘¥chimp(ğ‘¡)âˆ£âˆ£
(1)
ğ‘¥chimp(ğ‘¡+1)=ğ‘¥1+ğ‘¥2+ğ‘¥3+ğ‘¥44
(2)
where ğ‘¡ is the current iteration's number, ğ‘¥chimp(ğ‘¡) implies the location of each solution in iteration ğ‘¡. ğ´ and ğ¶ indicate coefficient vectors that are formulated in Eqs. (3) and (4).

ğ´1=2.ğ‘“.ğ‘Ÿ11âˆ’ğ‘“,ğ¶1=2.ğ‘Ÿ12ğ´2=2.ğ‘“.ğ‘Ÿ21âˆ’ğ‘“,ğ¶2=2.ğ‘Ÿ22ğ´3=2.ğ‘“.ğ‘Ÿ31âˆ’ğ‘“,ğ¶3=2.ğ‘Ÿ32ğ´4=2.ğ‘“.ğ‘Ÿ41âˆ’ğ‘“,ğ¶4=2.ğ‘Ÿ42
(3)
ğ‘“=2âˆ’ğ‘¡âˆ—(2ğ‘‡)
(4)
where ğ‘“ decreases linearly from 2 to 0, and ğ‘‡ indicates the maximum number of iterations. ğ‘Ÿ1 and ğ‘Ÿ2 are random scaled factors within [0,1] which are calculated as follows:

ğ‘1ğ‘”1=1.95âˆ’(2âˆ—ğ‘¡14ğ‘‡13),ğ‘Ÿ11=ğ‘1ğ‘”1âˆ—ğ‘Ÿğ‘ğ‘›ğ‘‘(),ğ‘2ğ‘”1=2âˆ—ğ‘¡13ğ‘‡13+0.5,ğ‘Ÿ12=ğ‘2ğ‘”1âˆ—ğ‘Ÿğ‘ğ‘›ğ‘‘()ğ‘1ğ‘”2=1.95âˆ’(2âˆ—ğ‘¡13ğ‘‡14),ğ‘Ÿ21=ğ‘1ğ‘”1âˆ—ğ‘Ÿğ‘ğ‘›ğ‘‘(),ğ‘2ğ‘”2=(2âˆ—ğ‘¡3ğ‘‡3)+0.5,ğ‘Ÿ22=ğ‘2ğ‘”1âˆ—ğ‘Ÿğ‘ğ‘›ğ‘‘()ğ‘1ğ‘”3=(âˆ’3âˆ—ğ‘¡3ğ‘‡3)+1.5,ğ‘Ÿ31=ğ‘1ğ‘”3âˆ—ğ‘Ÿğ‘ğ‘›ğ‘‘(),ğ‘2ğ‘”3=2âˆ—ğ‘¡13ğ‘‡13+0.5,ğ‘Ÿ32=ğ‘2ğ‘”3âˆ—ğ‘Ÿğ‘ğ‘›ğ‘‘()ğ‘1ğ‘”4=(âˆ’2âˆ—ğ‘¡3ğ‘‡3)+1.5,ğ‘Ÿ41=ğ‘1ğ‘”4âˆ—ğ‘Ÿğ‘ğ‘›ğ‘‘(),ğ‘2ğ‘”4=(2âˆ—ğ‘¡3ğ‘‡3)+0.5,ğ‘Ÿ42=ğ‘2ğ‘”4âˆ—ğ‘Ÿğ‘ğ‘›ğ‘‘().
(5)
where ğ‘Ÿğ‘ğ‘›ğ‘‘() stands for uniform distribution with a scale of 0 to 1. In Eq. (1) ğ‘š indicates a chaotic value between 0 and 1 derived from one of the chaotic maps mentioned below:

Quadratic:ğ‘¥ğ‘–+1=ğ‘š=ğ‘¥2ğ‘–âˆ’ğ‘,ğ‘=1Gauss/mouse:ğ‘¥ğ‘–+1=ğ‘š={1,ğ‘¥ğ‘–=01mod(ğ‘¥ğ‘–,1),otherwise
Logistic:ğ‘¥ğ‘–+1=ğ‘š=ğ›¼ğ‘¥ğ‘–(1âˆ’ğ‘¥ğ‘–),ğ›¼=4Singer:ğ‘¥ğ‘–+1=ğ‘š=ğœ‡âˆ—(7.86ğ‘¥ğ‘–âˆ’23.31ğ‘¥2ğ‘–+28.75ğ‘¥3ğ‘–âˆ’13.302875ğ‘¥4ğ‘–),ğœ‡=1.07
(6)
Bernoulli:ğ‘¥ğ‘–+1=ğ‘š=2ğ‘¥ğ‘–(mod1)Tent:ğ‘¥ğ‘–+1=ğ‘š={ğ‘¥ğ‘–0.7,ğ‘¥ğ‘–<0.7103(1âˆ’&ğ‘¥ğ‘–),0.7â‰¤ğ‘¥ğ‘–
According to Eqs. (1) and (2), the chimps update their positions according to the populationâ€™s best locations where D is the distance between the chimp (ğ‘¥chimp) and a prey. The A and C values are adjusted to monitor the areas where a solution can be found near the best solution. The ğ‘š value represents the influence of the chimps' sexual motivation, which causes them to behave erratically in the final stages of the hunting process, releasing their hunting responsibilities and desperately attempting to obtain meat. Chimps use this chaotic behavior in the final stage of ChOA to overcome local optima stagnation and slow convergence speed issues when solving complex problems.

To update the chimpsâ€™ position during optimization a probability of %50 is assumed to select between the chaotic model and the normal position updating process. The following equation expresses the model:

ğ‘¥chimp(ğ‘¡+1)=ğ‘“(ğ‘¥)={ğ‘¥1+ğ‘¥2+ğ‘¥3+ğ‘¥44,ğ‘š,ğ‘–ğ‘“(ğ‘<0.5)ğ‘–ğ‘“(ğ‘â‰¥0.5)
(7)
where ğ‘ is a random number in [0,1].

The ChOA's pseudo-code is shown in Fig. 1. The algorithm starts by creating a randomly generated population and setting the positions of the attacker (ğ‘¥attacker), barrier (ğ‘¥barrier), chaser (ğ‘¥chaser), and driver (ğ‘¥driver) to zero vector. The algorithm repeats the steps below until it reaches a termination criterion. First, each solution in the population is evaluated using a fitness function. Second, the algorithm updates the positions of the attacker, barrier, chaser, driver, and their scores. Third, the algorithm updates the values of ğ‘“, ğ‘Ÿ1,ğ‘Ÿ2, and ğ‘š coefficients using Eqs. (4) to (6). Third, using the values of ğ‘Ÿ1,ğ‘Ÿ2, and ğ‘“ parameters, the values of the main coefficients of ğ´ and ğ¶ are determined by Eq. (3). Finally, Eqs. (1), (2), and (7) are used to update the chimpsâ€™ positions. As a result, the best possible solution, i.e., the attackerâ€™s position, is returned.

Fig. 1
figure 1
Pseudo-code of the ChOA

Full size image
The proposed ChOA-based wrapper FS methods
The ChOA has been originally defined to operate in a continuous solution space and has succeeded in tackling a variety of continuous problems. However, the capability of the ChOA in solving binary high-dimensional problems, such as FS has not yet been investigated. This paper aims to generalize the ChOA to discrete settings, and two new binary variants of ChOA are proposed to address the FS problem. In the first approach, two different TFs, including S-shaped and V-shaped [52], are utilized in ChOA (BChOA) to convert continuous search space to a binary one. In the second approach, in addition to TFs, the position of the best solution in BChOA is updated using the crossover operator (BChOA-C) which improves the exploration ability of the algorithm.

Figure 2 explores the methodology of the proposed ChOA-based wrapper FS algorithms. Below steps have been followed to execute the suggested BChOA-C algorithm.

Step 1 The original dataset is utilized to create a population of chimps. Each chimp in the swarm is regarded as a candidate feature subset. The mRMR method is used to filter out noisy and redundant genes before population initialization on microarray datasets.

Step 2 The ChOA is performed. The population is evaluated using a fitness function by employing a classifier.

Step 3 After assigning fitness values to each solution, the four best solutions are selected from the population.

Step 4 The single-point crossover operator is performed between the best and other chimps of the population, and the better offspring is selected as the new position of attacker chimp.

Step 5 The algorithm updates the main coefficients (ğ‘“, ğ‘š, ğ¶, ğ´, and ğ·), and the rest of the chimps are forced to update their location according to the best chimp position.

Step 6 TFs are used to determine the probability of altering the elements of position vectors. Either Eqs. (9â€“10) or Eqs. (11â€“12) are used to update the elements of position vectors in BChOA. Thus, the movements are restricted to 0 and 1 values.

Step 7 The algorithm repeats the above steps until it reaches the value of the maximum iteration.

Step 8 If the termination condition is satisfied, the algorithm will stop and return the best solution, i.e., position of the attacker chimp, in the current population.

Step 9 The algorithm returns to step 2 if the termination condition is not met.

Fig. 2
figure 2
Flowchart of the suggested ChOA-based feature selection wrapper approaches for biomedical data classification. (A) First approach: the standard binary version of ChOA (BChOA). (B) Second approach: an improved version of the BChOA, in which BChOA is hybridized with the crossover operator (BChOA-C)

Full size image
Each of the following subsections describes a component of the proposed methods in detail.

Solution representation
In general, to apply the ChOA as a wrapper-based FS technique, the search space should be modeled by candidate feature subsets or binary solutions. Each candidate solution (chimp) is represented by a one-dimensional binary vector of ğ‘‘ elements, ğ‘ğ‘œğ‘ =(ğ‘“1,ğ‘“2,â€¦,ğ‘“ğ‘‘), where ğ‘‘ indicates the problem dimension (i.e., the number of all features in the original dataset). In the solution vector, each bit ğ‘“ğ‘— has a value of â€œ1â€ or â€œ0.â€ The value 1 indicates that the corresponding feature will be maintained, whereas 0 indicates that it will be discarded. The binary representation of a ChOAâ€™s solution can be seen in Fig. 3.

Fig. 3
figure 3
Binary representation of candidate solutions

Full size image
ChOA initially starts with a set of randomly generated binary solutions or candidate feature subsets. They can be seen as the position of chimps in search space. Only features coded in ones will be considered in the evaluation. The algorithm utilizes an objective (fitness) function to evaluate the effectiveness of each solution during the search.

Fitness function
The fitness function is an important factor to consider when designing any NIOA. Finding the optimal feature subset is a difficult task in the wrapper-based FS methods since it aims to find a subset with the highest accuracy and fewest number of features. The solution is better if it has fewer features and has a higher classification accuracy. An efficient fitness function should take into account these two conflicting objectives and strike a balance between them [3]. In this study, the suggested fitness function combines the number of selected features in the solution with the solutionâ€™s classification accuracy in order to assign a fitness value to each subset using the following equation:

Fitness=ğ›¼Ã—acc(ğ·)+ğ›½Ã—|ğ‘‘|âˆ’|ğ‘…||ğ‘‘|
(8)
where  acc(ğ·) represents the prediction accuracy of a classifier on the training dataset (D) with subset features. The fitness function employs a classifier to evaluate solutions (feature subsets). In wrapper-based FS methods, the process of learning a classifier is concurrent with FS. Two widely used classifiers, NB and K-NN (Kâ€‰=â€‰5) [26] are adopted for the fitness evaluation of solutions using tenfold CV. |ğ‘‘| indicates the total number of features in the original dataset, and |ğ‘…| stands for the number of selected features in the solution. The parameters ğ›¼ and ğ›½ determine the effects of the accuracy and number of selected features on the fitness value, respectively. ğ›¼ is in the interval of [0,1] and ğ›½=(1âˆ’ğ›¼). In this work, ğ›¼ was set to 0.8, since the classification accuracy of the solution is more important than the number of selected features.

It is worth reminding that after finding the best subset, leave-one-out- cross-validation (LOOCV) is used to report the final performance of the proposed binary ChOA on biomedical datasets.

Binary ChOA with transfer functions
The classical ChOA algorithm for continuous problems proceeds in discrete time considering ğ‘› chimps ğ‘1,ğ‘2,â€¦,ğ‘ğ‘›, in which each chimp ğ‘ğ‘– has a position in step ğ‘¡, ğ‘ğ‘œğ‘ ğ‘¡ğ‘–âˆˆâ„ğ‘‘. In discrete problems, however, the search space contains binary position vectors ğ‘ğ‘œğ‘ ğ‘¡ğ‘–âˆˆ{0,1}ğ‘‘. Four best solutions estimate the prey's location, and other chimps in the search space update their positions within the preyâ€™s vicinity. The main challenge in the design of binary ChOA is that how the algorithmâ€™s movement Eq. (7) in real space can be interpreted in discrete domains. The most straightforward technique to convert a continuous search space to a binary one is to use a transfer function. TFs force the chimps to move in a binary space by restricting their movements to 0 and 1 values. The ChOA has been adjusted to fit into the FS problem by employing two distinct TFs from two different families: S-shaped and V-shaped [52].

First, a hyperbolic tangent sigmoid (tansig) TF is employed in ChOA to convert the continuous algorithm to a binary version. It is utilized to modify chimpsâ€™ position based on the following rules:

ğ‘‡ğ‘“(posğ‘¡[ğ‘–,ğ‘‘])=21+ğ‘’âˆ’2âˆ—posğ‘¡[ğ‘–,ğ‘‘]âˆ’1
(9)
posğ‘¡+1[ğ‘–,ğ‘‘]={1,ğ‘‡ğ‘“(posğ‘¡[ğ‘–,ğ‘‘])>0.60,otherwise
(10)
where posğ‘¡+1[ğ‘–,ğ‘‘] represents the bit value of ğ‘‘ğ‘¡â„ dimension of ğ‘–ğ‘¡â„ chimp (position) in the next iteration (ğ‘¡+1). The tansig function generates a value in the range [âˆ’1,1] that specifies the likelihood of changing the elements of a position from 0 to 1 and vice versa. The value of the ğ‘‘ğ‘¡â„ element in the ğ‘–ğ‘¡â„ position vector is set to 0 or 1 based on Eq. (10). The tansig function belongs to the S-shaped transfer function category, and it has been used in the proposed binary ChOA since it has the highest experimental performance among existing S-shaped TFs.

The TF plays a key role in the efficiency of a binary algorithm. TFs strike a balance between exploration and exploitation to reach an appropriate solution. It was shown that the choice of TFs could significantly affect the obtained results of the binary algorithm [53]. The high performance of the V-shaped family of TFs has already been proven in the literature for binary algorithms [51, 52]. So also the rules of a V-shaped function are explored in ChOA to generate the next binary positions. The function is defined as follows:

ğ‘‡ğ‘“(posğ‘¡[ğ‘–,ğ‘‘])=âˆ£âˆ£âˆ£âˆ£âˆ£posğ‘¡[ğ‘–,ğ‘‘]1+(posğ‘¡[ğ‘–,ğ‘‘])2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšâˆ£âˆ£âˆ£âˆ£âˆ£
(11)
posğ‘¡+1[ğ‘–,ğ‘‘]={Â¬posğ‘¡[ğ‘–,ğ‘‘],ğ‘‡ğ‘“(posğ‘¡[ğ‘–,ğ‘‘])>ğ‘Ÿposğ‘¡[ğ‘–,ğ‘‘],ğ‘‡ğ‘“(posğ‘¡[ğ‘–,ğ‘‘])â‰¤ğ‘Ÿ
(12)
where Â¬POS[ğ‘–,ğ‘‘] is the complement operator of POS[ğ‘–,ğ‘‘] and ğ‘Ÿ is a random number in Uâˆ¼(0,1). The value of ğ‘Ÿ has a significant impact on whether or not the single bit value of ğ‘ğ‘œğ‘ ğ‘¡[ğ‘–,ğ‘‘] is filliped for the next position. The flowchart of the ChOA with transfer functions is demonstrated in Fig. 2A, and Fig. 4 illustrates the general steps of the proposed Binary ChOA for FS (See lines 20 and 21).

Fig. 4
figure 4
Pseudo-code of the suggested BChOA for FS

Full size image
It is worth noting that generally a filter-based FS method is applied to biomedical datasets (i.e., microarrays data) as a preprocessing step to provide strong initial data for the wrapper-based FS approach [43]. The search space of the wrapper-based FS approach involves a set of all possible feature subsets (2ğ‘‘), and the size of the set grow exponentially as the number of features (ğ‘‘) increases. So, filter-based FS methods are utilized to effectively alleviate the complexities of the big search space. In this study, mRMR filter approach [42] was first used to reduce the dimensionality of feature space in the microarray dataset, and the features with the highest rank are selected to build a new dataset. Then, the wrapper approach is performed to seek the most informative feature subset.

Binary ChOA with crossover scheme
The crossover operator is integrated into BChOA to offer a new wrapper technique called BChOA-C, which boosts the proposed BChOA performance. The BAOA produces good results on a variety of microarray datasets, but in some datasets, it gets stuck at a sub-optimal solution. So, BChOA's behavior with crossover operator is being examined to improve BChOA exploring capability. The BChOA-C performs the crossover operator just after determining the four best solutions in the population. The crossover operation is done between the best solution ğ‘¥ğ‘ğ‘¡ğ‘¡ğ‘ğ‘ğ‘˜ğ‘’ğ‘Ÿ and current solution ğ‘ƒğ‘‚ğ‘†[ğ‘–,] as shown in Eq. (13).

[ğ‘1,ğ‘2]=Crossover(ğ‘¥attacker,POS[ğ‘–,])
(13)
This paper uses the single-point crossover, in which the two ğ‘¥attacker and ğ‘ƒğ‘‚ğ‘†[ğ‘–,] mating vectors are severed at a random pivot point. Figure 5 shows an example of this technique. The binary bits are exchanged between two solutions as seen in Fig. 5, causing sudden changes in both solutions. Crossover has the potential to change the global best solution and to avoid the algorithm from getting stuck in local optima. In the meantime, the fitness values of the two offsprings produced by the crossover operator are compared with the best solution. The algorithm selects the best offspring as a new candidate solution. If the offspring has a higher fitness value than ğ‘¥attacker, the position of ğ‘¥attacker should be replaced and set to the offspring. Figure 2B shows the flowchart of the proposed BChOA-C with transfer functions and crossover operator, and Fig. 6 represents the pseudo-code of the algorithm.

Fig. 5
figure 5
The crossover process

Full size image
Fig. 6
figure 6
Pseudo-code of the suggested BChOA-C for FS

Full size image
Experimental result
Experimental setup
The R programming language was used to implement the suggested approaches. To evaluate the efficacy of the proposed BChOA-based techniques, five small-sized datasets in the text, life, and image domains, as well as five high-dimensional public microarray datasets with different types of diseases, were employed. The small datasets are obtained from the University of California, Irvine (UCI) Machine Learning Repository and https://jundongl.github.io/scikit-feature/datasets.html. The standard microarray gene expression datasets involving two binary and three multi-class datasets can be downloaded from https://data-mendeley-com.ezproxy.auckland.ac.nz/datasets/fhx5zgx2zj/1. Table 1 shows the characteristics of datasets used in this work. To find the optimal reduct, two wrapper FS techniques based on NB with Gaussian kernel and KNN classifiers (Kâ€‰=â€‰5) are utilized.

Table 1 Main characteristics of the image, text, and biological datasets
Full size table
Tenfold CV is used for the assessment of candidate solutions in the suggested methods. The experiments were carried out in two stages. Five high-dimensional microarray datasets (SRBCT, Prostate cancer, Leukemia, Brain Tumor_1, 11_Tumors) were utilized in the first phase to evaluate the suggested methodologies, and several small datasets in the text, life, and image domains were employed in the second phase. The trials were run on an Intel system with a Core i5 CPU running at 2.2 GHz and 8 GB of RAM. Both the population size and the maximum iteration parameter were set to 50. BChOA-based approaches are compared to state-of-the-art FS algorithms and other NIOAs, based on average classification accuracy and feature count from 20 independent runs.

The Weka program [69] (https://www.cs.waikato.ac.nz/ml/weka/), which is an open-source machine learning platform, was utilized for comparison. Table 2 summarizes all the proposed models and the comparative studies examined.

Table 2 Key to comparative methods
Full size table
Results for microarray datasets
This study employs three commonly used classification algorithms; KNN with kâ€‰=â€‰5, NB, and radial basis function (RBF) neural networks (RBFNet). The performance of these classifiers on five microarray datasets is shown in Table 3. Based on Table 3, we conclude that all three classifiers perform well. Various filter-based algorithms, such as mRMR, CMIM, Chi-square, Relief-F, and DISR, were compared to validate the effectiveness of the recommended approach. The Minâ€“Max technique was used to standardize the microarray expression data before applying filter-based methods. The average classification accuracy of KNN, NB, and RBFNet classifiers for 50 and 100 top genes selected by the above-mentioned filter-based FS algorithms are shown in Table 4 and Fig. 7. Based on Table 4 and Fig. 7, we can infer that the classification performance of the mRMR method with three classifiers is the best. Therefore, mRMR is used to select relevant top genes before employing the NIOA-based feature selection approaches.

Table 3 Percentage of average performance using KNN, NB, and RBFNet classifiers on five microarray datasets
Full size table
Table 4 Classification accuracy using selected genes by filter-based gene selection methods
Full size table
Fig. 7
figure 7
Average classification accuracy of KNN, NB, and RBFNet classifiers for 50 and 100 top genes selected by different filter-based FS algorithms

Full size image
Also from Table 4, the desired number of genes to be selected was considered 50 for the SRBCT and Prostate Tumor datasets, and 100 for the remaining datasets (Leukemia, Brain Tumor_1, and 11_Tumors).

Evaluation of the proposed BChOA without crossover
In this section, the performance of the suggested BChOAs based on the S-shaped Transfer function (BChOA-ST) and V-shaped Transfer function (BChOA-VT) are examined on microarray datasets using two classifiers (NB and KNN). A comparative study was carried out to compare the accuracy of the developed BChOA models. Tables 5 and 6 illustrate the results of these comparisons. Tables 5 and 6 show the performance of the proposed approaches (BChOA-ST and BChOA-VT) in terms of the three objectives (average number of genes, fitness value, and classification accuracy), as well as the computing time for the NB and KNN classifiers. In terms of the average number of selected features and CPU time, BChOA-VT outperformed BChOA-ST in most datasets. In addition, the KNN classifier yielded to a better outcome than NB. The classification accuracies and fitness values presented in the same tables produce almost identical results.

Table 5 Comparison of two proposed BChOA with V-shaped and S-shaped TFs using KNN classifier based on the average number of selected genes, fitness value, classification accuracy (%), and computational time (second)
Full size table
Table 6 Comparison of two proposed BChOA with V-shaped and S-shaped TFs using NB classifier based on the average number of selected genes, fitness value, classification accuracy (%), and computational time (second)
Full size table
Figure 8 demonstrates the average computational results of classification accuracy, fitness function, CPU time, and the number of selected features with error bars for two suggested BChOA with V-shaped and S-shaped transformation functions utilizing NB and KNN classifiers.

Fig. 8
figure 8
Average Classification performance of two proposed BChOA with V-shape and S-shape TFs using NB and KNN classifiers on all datasets

Full size image
Evaluation of the proposed BChOA with crossover
In this section, we assess the performance of BChOA-VT combined with crossover (BChOA-VT-C) and compare its performance to the basic BChOA-VT that has no crossover operator.

Table 7 shows the experimental outcomes for BChOA and BChOA-C in terms of average classification accuracy, fitness value, and the average number of selected genes. Both techniques were tested in ten separate runs. The Wilcoxon signed-rank statistical test between BChOA-C and BChOA is also shown in Table 7. Wilcoxon signed-rank statistical test was performed to reveal a substantial statistical difference between the two approaches. The best performances are highlighted in bold font.

Table 7 Comparison between BChOA and BChOA-C with V-shape (VT) transformation function
Full size table
In Table 7, the Tâ€‰âˆ’â€‰sig row, with a probability range of Î±â€‰â‰¤â€‰0.05, â€˜â€‰âˆ—â€‰â€˜ connotes that the BChOA-C technique produces substantially better results than the BChOA, whereas â€˜â€˜â€‰âˆ’â€‰â€˜ connotes that the BChOA-C method produces results that are not significantly better than the BChOA. To conduct statistical calculations, Wilcoxon signed-rank statistical test uses just the accuracy metric. As shown in Table 7, BChOA-C outperformed BChOA on three out of five datasets in terms of classification accuracy. Both methods achieved the highest classification accuracy in the remaining two datasets (SRBCT and Leukemia) (100%). On all datasets, BChOA-C yields higher fitness values. In Brain Tumor_1 and 11_Tumors datasets, BChOA showed slightly better results than BChOA-C in terms of the average number of selected genes. On two datasets (i.e., Prostate Tumor and 11_Tumors), substantial differences in favor of BChOA-C may be deduced. The convergence behavior of both methods on all datasets is shown in Fig. 9. In terms of fitness value, the convergence behavior trend of BChOA-C is significantly better than BChOA on all datasets.

Fig. 9
figure 9
The convergence behavior of BChOA and BChOA-C for five microarray datasets

Full size image
Comparison with other NIOAs
Table 8 shows that both proposed methods (BChOA and BChOA-C with VT transform function) outperform other NIOAs in terms of accuracy and the number of selected genes. Figures 10 and 11 illustrate the average number of selected genes and the accuracy of the ten approaches, respectively.

Table 8 Average LOOCV classification accuracyâ€‰Â±â€‰STD (in %) and the number of genes over 20 runs of different feature selection methods using KNN (Kâ€‰=â€‰5) classifier
Full size table
Fig. 10
figure 10
The average number of genes of all datasets

Full size image
Fig. 11
figure 11
The average accuracy comparison

Full size image
In this study, we utilized the paired t-test for statistical evaluation of BChOA-C performance. We compared our algorithm's accuracy and the number of selected genes with the other nine methods (Table 9). From Table 9 it can be seen that the p-values produced by the paired t-test among BChOA-C and other algorithms are mostly below the usual significance level of 0.05 for the number of selected genes metric. In other words, the suggested technique outperforms current methods in terms of the number of selected genes and the results are statistically significant. The proposed BChOA-C marginally outperforms the current NIOAs in terms of accuracy, but there is no statistically significant difference between the result of the proposed technique and other metaheuristic algorithms (MHAs). As a consequence, we may infer that the proposed approach has a significant difference in performance and indeed performs better than most of the compared approaches in the term of the number of selected genes.

Table 9 The p-Values of paired t-test of BChOA-C with other algorithms in the terms of accuracy and the number of selected genes
Full size table
Comparison with other state-of-the-art approaches
In this part, the suggested method's results are compared against state-of-the-art gene selection approaches in the literature to further examine its performance. The average classification accuracy and the average of selected genes, which appear between parentheses, are utilized as performance metrics in the assessment (Table 10).

Table 10 Comparing the performance of the proposed approach with the literature methods
Full size table
On most datasets, the BChOA-C provided equal or higher classification performance, as shown in Table 10. Meanwhile, the suggested BChOA-C was able to reduce the number of genes in each dataset while obtaining high classification accuracy. On SRBCT and 11_Tumors datasets, BChOA-C had the best results in terms of classification accuracy and the number of selected genes. On two datasets (Prostate cancer and Brain Tumor 1), BChOA-C chose the fewest number of genes. Furthermore, for Brain Tumor 1, TLBOSA-SVM achieved the best accuracy, whereas BCO-KNN picked the fewest genes for the Leukemia dataset.

In summary for gene selection problems, BChOA-C looks to be competitive and in some cases superior against state-of-art methods.

The biological meaning of the selected genes by the proposed BChOA-C approach
The biological meanings of the best subset of selected genes derived from our suggested approach are presented in this section. The names, prob-IDs, and descriptions of obtained genes as well as their specifications for SRBCT and Leukemia datasets are listed in Table 11. The interpretation of acquired genes from Prostate cancer, Brain Tumor_1, 11_ Tumors datasets is not feasible since no names have been assigned to genes in the datasets.

Table 11 The best subset of selected genes from the gene selection method BChOA-C for binary datasets
Full size table
The biological meanings of the selected genes were obtained using the OMIM (https://omim.org/) and NCBI (https://pubmed-ncbi-nlm-nih-gov.ezproxy.auckland.ac.nz/) websites. Table 11 shows that our suggested method can successfully identify cancer-related genes for each dataset.

For biological interpretation of gene expression data, the heatmap combined with the clustering method was used. The heatmap was created in R using the "gplots" package. In heatmaps (Fig. 12), each column demonstrates a sample and each row demonstrates a gene. Changes in gene expression are represented by the color and intensity of the boxes. Figure 10 depicts the profile of samples for different datasets. It clusters together genes with common expression profiles and confirms that the expression of most of the genes is coordinately down-regulated.

Fig. 12
figure 12
The gene expression level of the best subset of selected genes shown as a heatmap

Full size image
Boxplot was used to assess the validity of selected genes. In Fig. 13, the expression of the best subset of genes was shown as a box plot. Figure 13 shows that the selected genes are able to separate cancer groups by differences in their gene expressions.

Fig. 13
figure 13
Box plot diagrams of gene expression for the best subset of selected genes

Full size image
Results for small-sized datasets
We show and evaluate the results produced by our suggested methods on tiny datasets in this section. Table 12 compares the performance of both proposed methods with three classical (CFS, FCBF, and SSF) and three NIOA-based (Cuckoo, PSO, and NSGAII) feature selection approaches on small datasets. The best results among all feature selection methods have been highlighted and marked in bold type. It is worth mention that for the Yale dataset initially, we used the IG filter-based approach to select the top 100 relevant genes. The kappa2 measurement with the NB classifier from the â€œIrrâ€ package in R was employed as a fitness function in these datasets.

Table 12 Average LOOCV classification accuracyâ€‰Â±â€‰STD (in %) and the number of features over 20 runs of different feature selection methods using NB classifier
Full size table
As shown in Table 12, our method obtained fewer genes than CFS, FCBF, SSF, Cuckoo, PSO, and NSGAII for the majority of datasets. On the other hand, for most datasets, our approach obtains somewhat higher or second higher classification accuracy than other approaches. From this comparison, we conclude that BChOA-ST-NB is a suitable algorithm for FS problems on the different types of datasets.

Conclusion
Feature subset selection plays an important role in classification tasks, as it enhances the general abilities of classifiers, simplifies the learning model, and reduces the computational cost. In this paper, the problem of feature selection in high-dimensional biomedicine data classification has been considered and solved through a novel binary ChOA which extends ChOA from the continuous version to the discrete domain. To the best of our knowledge, this is the first binary variant of ChOA which has been developed for the task of feature selection. An enhanced binary ChOA-based optimizer with a crossover scheme was also presented. Two different transfer functions, S-shaped and V-shaped, were utilized to convert the continuous form of ChOA to binary form. Moreover, the widely used KNN and NB classifiers were served as evaluators of feature subsets in the proposed wrapper BChOA approach.

To verify the effectiveness of the proposed BChOA-based approaches five well-known biomedical datasets and several datasets with different domains were used. The results were compared to two standard filter feature selection methods and six popular wrapper techniques: PSO, BA, ACO, GA, FA, and FP. The experimental results show that the wrapper BChOA-based FS approaches are able to select a small number of the most prominent features whilst achieve a higher classification accuracy. The proposed BChOAs also outperform other current state-of-the-art techniques in the literature in terms of classification accuracy using fewer features. Moreover, the performance of BChOA-C is better than BChOA in the terms of classification accuracy due to the enhancement of the algorithm's exploration capability. In conclusion, both suggested BChOA and BChOA-C can be used as ideal feature selection tools for high-dimensional biomedical data, allowing for better biological data mining in fields of disease diagnosis.

As future work, BChOA may be proposed as a filter feature selection approach and be examined on the classification of biomedical data using a variety of classifiers. Development of binary multi-objective ChOA for feature selection and its performance comparison with the continuous multi-objective ChOA is recommended as well.

Keywords
Chimp optimization algorithm
Feature selection
Biomedical data
Classification
Optimization