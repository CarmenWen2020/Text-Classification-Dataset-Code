The deep forest (DF) model is built using a multilayer ensemble of forest units through decision tree aggregation. DF presents characteristics of an easy-to-understand structure, is suitable for small sample data, and has become an important research direction in the field of deep learning. These attributes are particularly suitable for the modeling of difficult-to-measure parameters in actual industrial process. However, existing methods have mainly focused on the problem of DF classification (DFC) and cannot be directly applied to regression modeling. To overcome these issues, a survey on the DFC algorithm is presented in terms of constructing a small sample data-oriented DF regression (DFR) model for industrial processes. Hence, a survey on the DFC algorithm is presented to construct a small sample size of the data-oriented DF regression (DFR) model for industrial processes. First, principle and properties of DFC are introduced in detail to demonstrate the non-neural network deep learning model. Second, methods of DFC are discussed in terms of feature engineering, representation learning, learner selection, weighting strategy, and hierarchical structure. Furthermore, related studies on decision tree algorithm are reviewed and future investigations on DFR and its relationship with deep learning are discussed and analyzed in detail. Finally, conclusions and the future direction of this study for industrial process modeling are presented. Developing a DFR algorithm with characteristics of dynamic adaptive and interpretation abilities and lightweight structure on the basis of actual industrial domain knowledge will be the focus of our follow-up investigation. Moreover, the existing research results of DFC and deep learning can provide guidance for the future investigations on the DFR model.

Introduction
Industrial processes generally contain multiple important production quantities and environment protection indices while often presenting complex nonlinear relationships with other process variables [1]. These key process parameters should be measured online first for operational optimization of such industrial processes [2, 3]. However, the majority of these parameters, such as dioxin (DXN) emission concentration of municipal solid waste incineration [4] and mill load parameter of mineral grinding [5] processes, are difficult to measure. Therefore, only a small sample size of process modeling data can be obtained due to limitations of the measurement device and complex characteristics of processes. Some researchers have proposed deep neural network (DNN) [6] for industrial process modeling to address these problems [7,8,9,10,11]. However, detailed analysis and theory interpretation with DNN models is difficult due to their learning behavior. A deep forest (DF) algorithm consisting of multigrained scanning (MG-Scanning) and cascade forest (C-Forest) in the deep learning model with a non-neural network structure has been proposed recently [12, 13]. The DF algorithm mainly contains two classic forest algorithms (FAs), that is, random forest (RF) [14] and complete RF (CRF). The DF algorithm demonstrates interpretability potential because decision tree (DT) algorithm is a ‚Äúwhite box model.‚Äù Therefore, DF has become an important research topic.

Many studies have shown that the DF structure is a remarkable alternative to DNN. For example, forward thinking deep random forest (FTDRF) [15] is proposed by replacing DNN neurons with DT, in which cascading layers contain of random and extra random trees [16]. Notably, adaptive weighted deep forest (AWDF) method based on FTDRF has been proposed recently [17], and weighted deep forest (WDF) has also been used by assigning different weights for subforests [18]. A boosting cascade deep forest (BCDF) model is built to train different types of modeling samples separately and increase the weight of interesting instances [19]. Motivated by the unique architecture of DenseNet [20], dense adaptive cascade forest (daForest) with DenseNet-like structure is designed [21]. Furthermore, the authors propose Siamese deep forest (SDF) [22, 23] as an alternative model of Siamese neural network (SNN) [24]. In summary, these approaches modify the structure of the DF model for different backgrounds and applications. These DF methods can be summarized as DF classification (DFC), with representation learning that can be realized using class distribution probability.

Studies on DF regression (DFR) are lacking. DFR can be inferred with excellent performance given the superior aspects of DFC. Main challenges of industrial process modeling involve characteristics of modeling data, such as small sample size, high dimension, and strong collinearity. Realizing the representation learning inside the C-Forest structure is the key issue in establishing the DFR-based process model. A DFR algorithm and its modification version that uses the predicted value of the subforest model is utilized to realize representation learning [25, 26]. Both DFC and DFR present satisfactory interpretability in theory. Therefore, we explore how to modify the DFC algorithm to enhance a small sample size of the data-oriented DFR model. On the basis of existing research results on DFC and deep learning, this study aims to (1) present an overview and discussion of the latest studies on DFC based on the primary DF model, (2) analyze and discuss DFR on the basis of rules from the tree structure to the forest structure and then determine its relationship with deep learning, and (3) present dilemmas of future DFR development and research directions.

The remainder of this paper is organized as follows. Basic principles and related applications of DF are introduced in Sect. 2. The research status in the DFC field is categorized on the basis of feature engineering, representation learning, learner selection, weighting strategy, and hierarchical structure in Sect. 3. DFR for industrial process is analyzed and discussed in Sect. 4. Finally, future research directions of DFR are given in Sect. 5.

Basic DF classification
The DFC structure involves two principal elements of MG-Scanning and C-Forest. The three-classification problem shown in Fig. 1 is taken as an example [12].

Fig. 1
figure 1
Structure diagram of DFC

Full size image
Raw feature of ùëÄ dimension is scanned into (ùëÄ‚àíùë†)+1 feature vector through sliding window with size ùë† in the MG-Scanning module. The transformed feature vector is used as training data in the C-Forest module. Estimated class distribution feature vector (CDFV) with three dimensions is produced with Forests A (based on RF) and B (based on CRF) after selecting two types of FAs, such as CRF and RF algorithms, as the learner for the C-Forest module. Both CDFV and raw feature vectors are connected as the input of the next layer model via the stack strategy [27,28,29] to prevent overfitting effectively while increasing the diversity of learners. In addition, high representation ability of the stack strategy maintains excellent performance even with small training data. Furthermore, inner information can be effectively transmitted because supervision information contributes to each layer. Cross-validation strategy is used to realize the adaptive adjustment of the layer number. Hence, main advantages of the DFC are listed as follows:

(1)
Automatically adjusts the training process in C-Forest layers [12, 25, 26];

(2)
Insensitivity hyperparameters work on different models with the same parameters [12, 25, 26];

(3)
Small computational cost for different types of datasets [12, 30, 31];

(4)
Parallel processing structure [32];

(5)
Theoretical analysis is easier compared with that of the black box model of DNN [12, 15, 22, 23].

Remark 1
RF and CRF algorithms in MG-Scanning and C-Forest are generally classification algorithms, and each FA includes as numerous as 500 DT models. The only difference between RF and CRF is reflected in splitting criteria of DT. RF uses the best Gini value [33] as the basis for tree growth, and CRF applies a completely random growth method. Notably, the output of each FA in DFC is a multi-dimensional class vector according to the characteristic of task categories.

DFC has been extensively applied in different fields. Researchers in the image field basically focus on image object recognition [34,35,36,37,38], search image [39], remote sensing scene image classification [40], flame video recognition [41], image segmentations [42], and image quality evaluation [43]. DFC applications in the field of fault detection and diagnosis include satellite attitude control system [44], railway turnout system [45], rolling bearing [46], health monitoring [47], android malware behavior [48], model of TCM syndrome of chronic gastritis [49], and electricity theft [50]. Buying behavior [51], power system short-term load [52], hospitalization rate of patients [53], stock behavior [54], depression severity [55, 56], and EEG attention recognition [57] are frequently explored in other fields. These studies have proven the broad application range of DFC with high accuracy.

Many studies on memory- and time-overhead problems of the DF structure have been performed. Confidence screening mechanism is added to the DFC to reduce the running time of each C-Forest layer [58]. Instances are divided into easy- and difficult-to-predict subsets. The former directly enters the final predictive stage and the latter is trained layer by layer. The cascade structure is a primary reason for the long training time and weak scalability of DF [32]. A task parallel algorithm based on fine-grained subforest (forest layer) is then proposed by adopting a high-performance distributed execution framework [32]. Similar to the traditional ensemble learning pruning operation, the distributed execution framework is expected to reduce the complexity of the C-Forest by simplifying the learner. Hence, DFC utilizes the feature contribution rate index to prune the tree model. An ordering-based ensemble pruning method is proposed to realize a pruned deep forest (PDF) with high performance and small ensemble size [59]. PDF is based on feature vectorization and quantum walks. However, the results showed that the performance minimally decreases with the simplification of rules and functions [60].

Review on DF classification
DFC methods are classified into five categories of feature engineering, representation learning, learner selection, weighting strategy, and hierarchical structure in this section (Fig. 2).

Fig. 2
figure 2
Classification structure diagram of DFC methods

Full size image
Feature engineering
Preprocessing raw features before C-Forest training is an essential procedure in the classification task given that MG-Scanning can determine the final performance of the C-Forest structure to some extent. Constructing a learner that can achieve excellent performance in differing situations in terms of ‚Äúno free lunch theory‚Äù is difficult because modeling data of various domains present different characteristics and hypotheses [61]. Therefore, extensive preprocessing methods, such as feature extraction, feature selection, and data sampling, are used on the basis of the characteristic of modeling data to address this problem. Preprocessing methods are typically utilized to (1) replace the MS-Scanning module and perform feature processing (2) before or (3) after the MS-scanning module.

The general structure of the first preprocessing method is shown in Fig. 3.

Fig. 3
figure 3
Structural diagram of feature engineering substituting for MG-Scanning

Full size image
Preconditioning of the MG-Scanning module can promote the performance of C-Forest under the assumption that modeling data are true and credible. However, a preprocessing method must be used as a substitute for the MG-Scanning module because modeled data in real-world applications contain noise and present unbalanced distributions. Principal component analysis (PCA) [62] is used to preprocess spectral data and obtain the original feature vector, and the C-Forest model [63] is exploited to classify satellite remote sensing images. Another study showed that t-distributed random adjacent embedding (t-SNE) method is first used to preprocess high-dimensional spectral data before PCA feature extraction, and then, the C-Forest recognition model is also constructed [64]. Multimode LBP [65] operator is utilized to extract features for finger vein recognition [66]. Z-score standardization is used to address the particularity of abstract logic symbol composition in source data and generate original features for detecting software defects [67].

Meanwhile, other methods first apply pretreatment technology to data [68,69,70,71,72] and then utilize the MG-Scanning module to perform feature transformation. Finally, the C-Forest model is trained for final predictions. The common structure of the C-Forest model is shown in Fig. 4.

Fig. 4
figure 4
Feature engineering before MG-Scanning

Full size image
Studies based on the strategy in Fig. 4 are presented as follows. A regional suggestion network is designed to extract a two-dimensional original feature vector from thermal remote sensing images of ships [68]. A feature extraction algorithm incorporating band-pass filter and local color iterative correction is first used to extract text information from images [69], and random fern [73] is applied to generate the CDFV in the MG-Scanning module. Synthetic minority oversampling technique (SMOTE) [74] and Tomek link algorithm [75] are applied to expand and denoise The Cancer Genome Atlas (TCGA) data [70] before building a cancer classification model for unbalanced data. Similarly, SMOTE combined with edited nearest neighbor (ENN) [76] is utilized to address the class imbalance and attention deficit hyperactivity disorder in children [71]. Deep Boltzmann machine (DBM) [77] is used to convert numerical characteristics of industrial processes into binary vector for constructing a fault diagnosis model based on DFC [72]. Different from these processing strategies, average pooling module is introduced [78] to address the space-based HSI classification problem and reduce the dimensionality of the output feature vector of the MG-Scanning module.

The performance of the C-Forest model can be improved using different feature engineering methods. However, determining which singleton feature engineering strategy is suitable for all modeling data is impossible. Hence, determining the quality of these feature engineering methods is difficult when deviation from practical problems occurs. Feature engineering and other related methods are constantly used to observe raw data on the basis of MG-Scanning. Future investigations must consider methods of feature engineering selection on the basis of modeling data properties and expert experience.

Representation learning
The C-Forest structure is trained using a layer-by-layer mechanism with feature vectors from the MG-Scanning module or feature preprocessing. Layer classification feature vector (LCFV) is used for representation learning in each layer of CDFV information. LCFV is then concatenated with raw feature to obtain augmented layer classification feature vector (ALCFV). ALCFV is used as training data of the next layer to restrain the loss of model diversity. The representation learning in C-Forest is basically divided into three types, namely utilization improvement of LCFV, dimension reduction of LCFV, and feature optimization. The cascade layer is illustrated in Fig. 5.

Fig. 5
figure 5
Representation of the cascade layer

Full size image
The three types of representation learning in C-Forest are presented as follows:

(1)
Utilization improvement of LCFV. A layer-by-layer strategy is utilized to represent features in the C-Forest structure of the original DFC. The error will generally present unstable fluctuations with the depth of the C-Forest layer when the adjustment mechanism of adaptive layer number is shielded. This phenomenon called sparse connectivity in the C-Forest structure [21] leads to continuous degradation of the feature information. The DenseNet [79] strategy is applied to avoid this phenomenon and remove the loss of internal representation information effectively. ALCFV of each layer is then modified to connect the raw feature vector with the existing LCFV, and the representation information is infused with training data in the cascade layer. The raw feature vector is combined with the output of the constructed C-Forest layer as the input of the new layer on the basis of stacked generalization theory [27, 80].

(2)
Dimension reduction of LCFV. The remarkable growth of the ALCFV dimension with the increase in depth of C-Forest is due to the use of dense connection results in high time complexity. The average LCFV is determined and then combined with the raw feature vector to obtain the ALCFV and address these limitations [81,82,83]. LCFVs of the same class output and subforest are averaged as the LCFV to realize a maximum dimension reduction of 50% [84]. This process is conducive to reducing the space complexity and improving convergence speed and operation efficiency.

(3)
Feature optimization. The feature optimization module, which measures the importance of the raw feature vector and LCFV, is added to the middle of each layer to reduce the time cost [85]. Distinctive features are then selected in each layer while removing insignificant features to ensure that the calculation time cost is reduced.

The performance of DFC generally improves via density connection or LCFV dimensionality reduction to a certain extent. However, the computational consumption during training and prediction stages increases. The effect of representation learning method can be weakened because feature optimization among layers may result in feature disappearance in the LCFV. For the sake of convenience, ùëá is the number of subforests in the C-Forest layer and ùëá√óùê∂ is the dimension of LCFV generated by the subforest, where ùê∂ is the number of classification types. The time cost is mainly attributed to the raw high-dimensional feature vector when parameters ùëá and ùê∂ are in the normal range. Therefore, the means for reducing the cost of computing time remains an open issue.

Learner selection
The model performance depends on the diversity and accuracy of the learner in ensemble learning [86, 87]. The diversity of subforests in the C-Forest structure exerts a strong effect on the prediction performance of the DF structure. Each subforest of DF is generated via k-fold cross-validation to encourage diversity. Apart from these issues, many researchers have also focused on the type and number of learners, as shown in Fig. 6.

(1)
Change the type of learners. Multi-instance random forest (MIRF) and bag-level randomized forest (BLRF) [88] are used to replace RF and CRF, respectively, in multiple-instance learning approach while considering the diversity of learners [89, 90]. The similar performance of RF and CRF fails to provide diversity in the example of remote sensing image classification. Thus, RF and rotation forests [91] are combined to increase the diversity in the C-Forest structure. Furthermore, four rotation forests are implemented in each C-Forest layer to improve the accuracy of the DF [92]. Similarly, RF and CRF are substituted with four learners based on XGBoost [93] to solve the intrusion detection problem of network attack [94]. Different learner combination methods of the C-Forest layer, such as combination of XGBoost, RF, CRF, and logistic regression [95, 96]; XGBoost, RF, extra trees, and logistic regression [97, 98]; and LightGBM [99], RF, XGBoost, and ExtraTrees [100], subsequently appear.

(2)
Change both the number and type of learners. Many studies have improved the diversity of DFC by reducing or increasing the number of learners. For example, GTC Forest only uses two learners (XGBoost and RF) to build the classification model of grid-structured data [101]. Three types of FA, namely XGBoost, RF, and ExtraTrees, are selected for the fault detection of the industrial process [102]. These studies clearly showed that the performance remains unaffected by the decrease in number of learners. By comparison, RF with information gain ratio, RF with Gini index, CRF with information gain ratio, CRF with Gini index, and extreme random forest are applied to estimate the power system transient stability [103]. The C-Forest model with six flexible neural trees (FNTs) [104] is used to classify the cancer problem [105], and double RF, ExtraTrees, XGBoost, and GBDT [106] are utilized as learners of each layer [107] in the automatic target recognition of radar high-resolution range profile. The results of these studies certified that the increase of learner types can significantly enhance the performance of DFC. Hence, evidence in proving which strategy is effective through the increase or decrease of the number of learners remains unclear. Therefore, other factors, such as characteristic of the application domain, should be considered for the performance improvement of DFC.

Fig. 6
figure 6
Modification of the type of learner

Full size image
These studies have explored diversity from different perspectives although determining the most suitable strategy remains unverified. The following issues must be addressed to determine a suitable strategy. (a) The width of each layer and the depth of the C-Forest structure for dynamic adjustment requires further investigation because increasing the number (width) of subforests in each layer will intensify the time cost. (b) The means for avoiding subjective arbitrariness and analyzing the contribution of the subforest learner for the overall structure need further exploration due to the strong subjective arbitrariness in the selection of the number and type of subforest learner in existing studies.

Weighting strategy
Sample inequality and learner diversity negatively affect the performance of the DFC model, which is prone to the phenomenon of imbalances. The weighting coefficient strategy is commonly used to address this problem. Notably, the weighting strategy based on DFC focuses on the instance (feature) and learner approaches.

1) Weighting instance (feature).

Insufficient fitting and the lack of diversity in datasets with small sample sizes and high dimension are primary issues of DFC. Hence, boosting [106] strategy is introduced to assign weights for instances [19] in which the subforest weight coefficient ùëä=(ùúî1,ùúî2,‚Ä¶,ùúîùëõ) is calculated through the out-of-bagging dataset [108, 109]. Furthermore, weights are assigned for instances ùëã=(ùë•1,ùë•2,‚Ä¶,ùë•ùëõ) of the generated CDFV. The weight distribution is expressed as follows:

ùë•‚Ä≤ùëñ=ùë•ùëñùúîùëñ‚àëùëõùëñ=1ùë•ùëñùúîùëñ
(1)
where ùë•‚Ä≤ùëñ is the normalized CDFV.

The time complexity positively increases when all instances are transferred in each layer. Confidence screening mechanism [58] contributes in discriminating instances and assigning a weight ùúîùëñ for the instance ùë•ùëñ [17]. The instance ùë•ùëñ with weight ùúîùëñ‚â†0 enters the next layer and is assigned according to the average ùêØùëñ of the CDFV generated in the previous layer. The weight ùúîùëñ for ùêØùëñ and ùê®ùëñ is expressed as follows:

ùúîùëñ=ùëì(ùëë(ùêØùëñ,ùê®ùëñ))
(2)
where ùúîùëñ is the weight of ùë•ùëñ, ùê®ùëñ is the label vector, and ùëë(ùêØùëñ,ùê®ùëñ) is the distance between ùêØùëñ and ùê®ùëñ.

The contribution of each feature for the prediction performance is unbalanced, as expected. Therefore, AdaBoost is introduced in the C-Forest layer to solve this unbalance problem through weighted features that exert a serious impact on the performance.

2) Weighting learner.

Weights are assigned according to the contribution of the subforest learner [110]. Area under the curve (AUC) index [111] is used to evaluate the prediction ability of each subforest as follows:

AUC=‚à´10ùëÖùëÇùê∂(ùë¢)ùëëùë¢ùë¢‚àà[0,1]
(3)
However, Eq. (3) is usually complicated. Wilcoxon‚ÄìMann‚ÄìWhitney statistic index [112] is used to facilitate the calculation of AUC as follows:

AUC=‚àëùëöùëñ=1‚àëùëõùëó=11ùë•ùëñ>ùë¶ùëóùëöùëõ
(4)
where ùë•ùëñ and ùë¶ùëñ are outputs of the classifier ùëì.

Assuming that the classifier ùëì and dataset ùëã‚àà‚Ñù contain ùëö and ùëõ negative values, respectively, the AUC value of the learner is used to calculate weights as follows:

ùõº1=AUC1AUC1+AUC2
(5)
ùõº2=AUC2AUC1+AUC2
(6)
where ùõº1 is the weight of RF and ùõº2 denotes the weight of CRF.

Different from these methods, majority voting strategy with simple average works on the basis of the hypothesis that learners contribute identically. However, the contribution of each learner must be considered due to sampling strategies and randomness. Consequently, weakness of the learner may amplify the effect on the final prediction results. Therefore, a weighted average prediction strategy [18] is proposed by calculating the accuracy as follows:

ùëéùëò=ùê¥ùëêùëê(Predict(‚ãÖ),ùëå(‚ãÖ))
(7)
The weight of each tree is calculated according to the prediction accuracy of the subforest learner as follows:

ùëäùëò=log2(ùëéùëò/(1‚àíùëéùëò))‚àëlog2(ùëéùëñ/(1‚àíùëéùëñ))
(8)
Finally, the weighted summation of prediction probability vectors of each learner is obtained to eliminate the influence of the incorrectly predicted result. Therefore, the performance of the DFC model is improved.

The pari-mutuel (PM) [113] model is used for weighting the DT [114] as follows:

ùëùùë°=‚àëùëó=1ùêΩùëùùëò,ùë°ùúîùëó
(9)
where ùëùùë° is the CDFV of the subforest, ùëùùëò,ùë° indicates the CDFV generated during decision, and ùúîùëó represents the weight coefficient of the DT.

Finally, DTs with different weighted coefficients are used to generate the CDFV. The weight is apportioned according to the DT model with consideration for metric learning [22, 23] to reduce the distance among instances of the same class and stretch otherwise. The weight vector is obtained by solving the following loss function:

minùê∞ùêΩùëû(ùê∞)=minùê∞‚àë((1‚àíùëßùëñùëó)ùëë(ùê±ùëñ,ùê±ùëó)+ùëßùëñùëómax(0,ùúè‚àíùëë(ùê±ùëñ,ùê±ùëó)))+ùúÜ‚Äñùê∞‚Äñ2
(10)
where ùëë(ùê±ùëñ,ùê±ùëó) is the distance between two vectors and ùúÜ‚Äñùê∞‚Äñ2 denotes the regularization term.

In summary, introducing the weighting coefficient to the DF structure can weaken the diversity among learners and improve the accuracy. However, a trade-off relationship evidently exists between the complexity and generalization error due to the increase in both number of parameters and model complexity. Overfitting will likely result in a large generalization error and vice versa. Therefore, the trade-off of weighted coefficients and generalization performance or model complexity requires further investigation.

Hierarchical structure
Similar to DNN, DFC inevitably moves toward structure adjustment. The adjusted structure [115, 116] is shown in Fig. 7.

Fig. 7
figure 7
Block diagram of the structural adjustment

Full size image
Four forest models (2RF‚Äâ+‚Äâ2CRF, 4‚Äâ√ó‚Äâ1) of each layer are split into two sublayers (2‚Äâ√ó‚Äâ2) on the basis of the idea that ‚Äúlayers are more important than neurons‚Äù in DNN, thereby indicating the principal difference with the current structure [115, 116]. Although the local structure of the C-Forest is modified to improve the performance, the structure shown in Fig. 7 can be generalized in another form, which is the local density connection from the view of representation feature among layers. Information flow can be weakened due to the similarity. Notably, studies on the DFC structure are limited. Therefore, the relationship between the modification structure and the prediction performance requires further investigation.

Discussion on DFC
Recent studies have shown that the DF algorithm exhibits excellent performance compared with the DNN-like structure [94, 101, 117]. Statistical results on the DF structure are listed in Table 1.

Table 1 Statistical results of DF studies
Full size table
Table 1 shows the following:

(1)
The original DFC presents several characteristics. First, the number of layers can be automatically adjusted. Second, a set of hyperparameters can be used for different models. Third, the small computational cost is suitable for different types of datasets. Finally, the parallel processing structure is beneficial to the distributed implementation.

(2)
DFC based on feature engineering methods mainly includes the categories of (1) replacing the MG-Scanning module and adding (2) before and (3) after the MG-Scanning module. However, discussing the universality of these methods without specific issues is impossible. Recent studies have shown that the performance improvement of the C-Forest model based on feature engineering is very distinct. Methods used typically depend on characteristics of modeling data.

(3)
DFC based on representation learning specifics the feature transform among C-Forest layers. Existing categories, including feature optimization, utilization improvement, and dimension reduction of the LCFV, can be generalized to reinforce the transmitted representation information. For example, the DASF algorithm is an improvement of the DenseNet connection [21]. Thus, the effect of avoiding the loss of hierarchical information lost is realized and the DASF algorithm weakens the swamping phenomenon of raw feature vectors on the LCFV. However, DASF increases the dimensionality of ALCFV that transfers among layers. The LCFV is averaged to address this issue and obtain ALCFV. C-Forest is easily overfitted due to the swamping effect of high-dimensional features on LCFV when the feature information is conveyed and assembled. Feature optimization links are subsequently added to convert the feature information among layers. These methods are preliminary explorations of the representation learning of model internal features.

(4)
The modification of the number and type of learners aims to increase diversity with consideration for DFC based on learner selection. Different types of learners have been used to build the C-Forest module, including tree and non-tree structures from 2018 to 2020. At the same time, methods based on various types combined with different numbers of learners have also been proposed. Notably, a multitude of learners have been used in C-Forest modeling. According to existing studies, the type and number of learners need to be selected on the basis of different application fields.

(5)
The integration of a weighting strategy with the original DFC is proposed. The weighted distribution method of instances (or features) and the learner in the C-Forest layer is put forward to enhance the prediction performance. The inherent nature of inequality in samples and features results in diversity among learners. Importing the weighted coefficient can effectively improve the performance of the DFC model and obtain excellent prediction performance.

(6)
DFC based on hierarchical structure refers to the structure alteration of the C-Forest layer. The internal structure presents high correlation with the final prediction performance in the development process of the neural network. Only one relevant study has modified the structure of four learners in each layer into two sublayers [115]. However, structure analysis and adjustment of DF are interesting approaches due to the plasticity of the internal structure.

Therefore, existing methods have mainly concentrated on the field of classification. The primary DFC structure is composed of two parts, that is, MG-Scanning and C-Forest modules. The former belongs to feature engineering, and the latter contributes to the prediction model. Representation learning within the DF structure is critical to realize the deep learning ability and the true value of the class label because feature representation fails to achieve satisfactory stack generalization given the classification problem [118]. The CDFV works to solve this problem and replace the true label value [118], in which the prediction accuracy and reliability of the classifier are both considered. Accordingly, MG-Scanning with the CDFV can achieve the conversion of the raw feature vector and C-forest with class probability distribution and transformed features in the stack framework and realize representation learning. In summary, existing DFC algorithms are based on these ideas.

Remark 2
This section introduces five categories, i.e., feature engineering, representation learning, learner selection, weighting strategy, and hierarchical structure, to define the current focus of classification-oriented DFC studies adequately. Note that these five types can be explored with individual or combined mode in an article. On the basis of the individual mode for DFC, Liu et al. [64] only investigated feature engineering, Xue et al. [80] only focused on representation learning only, Xu et al. [105] only considered learner selection, Dong et al. [110] only focused on the weighting strategy, and Fan et al. [115] assessed the hierarchical structure [115]. Representation learning and weighting strategy [21, 81]; feature engineering and representation learning [85, 119]; and representation learning, weighting strategy, and hierarchical structure in [21]; are performed on the basis of combined mode for DFC. DFR-related studies are generally not limited to a specific research subject, with directions based on practical problems.

Analysis of DF regression for industrial modeling
The reduction of energy consumption and pollution emission for complex industrial processes must be solved using a control strategy to optimize operation. However, the first principal model that exploits online prediction of key difficult-to-measure parameters as an important factor cannot be established due to the high complexity of the physical/chemistry process. These production qualities and environmental pollution indices mainly obtained through manual timing sampling, offline laboratory analysis, or online estimation of domain experts typically lead to inaccurate and large time lag detection methods that hinder the complex industrial process from achieving operational optimization and feedback control. Therefore, building a soft-sensing model based on easy-to-measure process variables has become an effective method. Widely used soft measurement methods, including artificial neural network, support vector machine, and other machine learning methods, are usually used to address situations wherein only small limited sample data can be obtained. However, these methods are prone to incurring problems, such as overfitting and poor interpretability. A key factor for soft measurement is building an accurate and interpretable industrial model. Therefore, the following subsections review the industrial process modeling-oriented DT, FA, and DF algorithms.

Modeling based on DT
The DT is a basic model in machine learning that consists of internal nodes, leaf nodes, and directed edges [120] (Fig. 8).

Fig. 8
figure 8
Structure of DT

Full size image
The three classic DT algorithms are ID3 with information gain [111], C4.5 with information gain ratio [121], and CART with minimum square error (Gini index). The CART tree model is the most widely used among the three algorithms and its DT construction generally includes the three operations of feature selection, tree growth, and pruning criterion.

The DT algorithm is a typical classification method that can approximate discrete function values. First, the minimum loss function is used to segment the sample space. Second, induction strategy is utilized to create the decision-making model. Finally, the model is used to predict the unseen sample. The DT algorithm is classified as a ‚Äúwhite box model‚Äù because this operating mechanism can be interpreted as a set of rules. However, the split criterion based on the ‚Äúqualitative‚Äù information in the classification problem is unsuitable for the ‚Äúquantitative‚Äù nature of the regression problem. Hence, the CART model has been widely used through the split criterion with both the Gini index and square error to address this limitation. The square error minimization criterion is mainly used to construct a binary DT in the process modeling field.

For simplicity, the training set is denoted ùê∑={(ùê±1,ùë¶1),(ùê±2,ùë¶2),‚Ä¶,(ùê±ùëõ,ùë¶ùëõ)}‚ààùëÖùëÅ√óùëÄ, and the sample space is divided into left and right regions according to the segmentation value ùë† of the segmentation feature ùëó as follows:

{ùëÖL(ùëó,ùë†)={ùë•|ùë•ùëó‚â§ùë†}ùëÖR(ùëó,ùë†)={ùë•|ùë•ùëó>ùë†}
(11)
The minimum square error is defined as

argminùëó,ùë†[minùëêL‚àëùë•ùëñ‚ààùëÖL(ùëó,ùë†)(ùë¶ùëñ‚àíùëêL)2+minùëêR‚àëùë•ùëñ‚ààùëÖR(ùëó,ùë†)(ùë¶ùëñ‚àíùëêR)2]
(12)
where ùëêL and ùëêR are average values of the output in ùëÖL and ùëÖR subregions, respectively, and ùë¶ùëñ denotes true values.

The error of segmentation point is obtained by looping each value of every feature. Thus, the space is segmented into two subregions on the basis of the minimum square error value of the segmentation point. Suppose that the space is divided into ùëÄ subregions and output values of each subregion ùëÖ1,ùëÖ2,‚Ä¶,and ùëÖùëÄ are denoted ùëê1,ùëê2,‚ãØ, and ùëêùëÄ, respectively. Therefore, the regression tree model can be expressed as

ùëì(ùë•)=‚àëùëö=1ùëÄùëêùëöùêº(ùë•‚ààùëÖùëö)
(13)
where ùêº(‚ãÖ) is the indicator function. If ùë•‚ààùëÖùëö, then ùêº(ùë•‚ààùëÖùëö)=1; otherwise, it is equal to 0.

Modeling based on FA
Accuracy of the single-tree model is lower than that of the tree ensemble voting decision. Although the learning strategy of a combination of multiple models has received widespread attention, the diversity of training data has become a major obstacle to the significant development of ensemble learning. Bootstrap [122] sampling, also called bagging forest, is used to address this obstacle, generate subsets from the original dataset, and improve diversity [123]. Although the bagging forest model can obtain excellent generalization performance, bias still exists. Therefore, the performance of FA is still inferior to that of AdaBoost [124] and others. The imitation of the random subspace method [125] in the bagging forest improves the prediction accuracy and leads to the appearance of the classical machine learning method RF. RF has become a representative bagging method due to its powerful ability of noise processing and nonlinear modeling [126, 130]. The FA structure is mainly divided into parallel (PF) and cascade (CF) forests. Structures of PF and CF are shown in Fig. 9.

Fig. 9
figure 9
Structure diagram of the forest algorithm

Full size image
Both PF and CF methods can be used to build a process model with learners on the basis of the CART tree. The RF algorithm is the most representative among PF structure-based methods and has been extensively applied due to its powerful ability to process noise and model nonlinear data [127]. Hence, RF is proposed by combining bagging parallel with the random subspace method, whereby multiple training subsets are extracted through the bootstrap sampling strategy. At the same time, random feature selection is added in each training subset. Thus, these CART trees can be trained with maximized growth. The ‚Äúsimple average‚Äù strategy is then used to address different prediction results produced by each CART tree. The generation process of the training subset can be expressed as

{(ùê±,ùë¶)ùëõ}ùëÅùëõ=1ùê±‚ààùëÖùëÅ√óùëÄùêΩ‚é´‚é≠‚é¨‚é™‚é™‚áí‚éß‚é©‚é®‚é™‚é™‚é™‚é™‚é™‚é™{(ùê±1,ùë¶1)ùëõ}ùëÅùëõ=1‚áí{(ùê±1,ùëÄ1,ùë¶1)ùëõ}ùëÅùëõ=1‚ãØ{(ùë•ùëó,ùë¶ùëó)ùëõ}ùëÅùëõ=1‚áí{(ùê±ùëó,ùëÄùëó,ùë¶ùëó)ùëõ}ùëÅùëõ=1‚ãØ{(ùê±ùêΩ,ùë¶ùêΩ)ùëõ}ùëÅùëõ=1‚áí{(ùê±ùêΩ,ùëÄùêΩ,ùë¶ùêΩ)ùëõ}ùëÅùëõ=1
(14)
where {(ùê±ùëó,ùëÄùëó,ùë¶ùëó)ùëõ}ùëÅùëõ=1 is the ùëóth subset, ùëÅ denotes the number of samples, ùëÄ indicates the number of features, (ùê±ùëó,ùëÄùëó,ùë¶ùëó)ùëõ is the ùëõth input feature and true value pairwise for the ùëóth subset, and ùëÄùëó is the number of input features in the ùëóth subset, where ùëÄùëó<<ùëÄ. The pseudocode of the RF algorithm is shown in the "Appendix".

CF is a type of modeling method based on the boosting strategy [128]. The boosting strategy presents the optimal fitting ability when the relationship between input and output is complicated. Specifically, gradient boosting decision tree (GBDT) is a representative method [106] that consists of an additive model and the forward distribution method. Here, the regression-oriented GBDT model based on the square error loss function is used as an example. The pseudocode of the GBDT algorithm is shown in the "Appendix".

Modeling based on DFR
The representation learning strategy in DFC is implemented through CDFV among C-Forest layers. DFC can only be indirectly adopted to process modeling. Therefore, the representation learning method should be realized according to characteristics of modeling data. Representation learning among C-Forest layers has become a critical issue for the DFR model given that the CDFV can mirror the model performance to a certain extent [118]. Modified strategies from classification to regression problems in terms of DT and FA algorithms can provide effective support on the basis of DFC. The DFR [25] algorithm is proposed for modeling environmental indicators of complex industrial processes, where the ùúÖ-nearest neighbor (k-NN) [129] criterion is used to select the predicted value of FA as the representation feature. The DFR structure is illustrated in Fig. 10.

Fig. 10
figure 10
Structure of deep forest regression

Full size image
Figure 10 shows that the DFR structure mainly includes three forest modules, namely input, middle, and output layers. Among them, the input layer module regards the raw feature vector as the input for training several subforest models. Layer regression feature vector (LRFV) is then obtained by utilizing the k-NN method to select prediction values of subforests. Augmented layer regression feature vector (ALRFV) is obtained by combining LRFV with the raw feature vector. The middle layer module consists of ùêæ‚àí2 layers and regards the ALRFV of the former layer as the input. The output of the second layer is obtained in the same way as the input layer. The modeling process is repeated until the output of the ùêæ‚àí1 layer model is completed. The output layer (ùêæth layer) module considers the ALRFV of the ùêæ‚àí1 layer as the input for training multiple subforest models and obtains the results via simple averaging of prediction outputs of subforests. Consequently, the exploration of the DF structure in the field of regression is initially realized by using the predicted value as the representation feature.

Challenges in DFR
Improved strategies in existing DFC studies are mainly from the perspective of empirical cognition. However, these methods ignore the operating mechanism of the DF internal structure. Thus, the theoretical analysis of generalization and convergence is still unverified. The unsuitability of class distribution probability [118] for regression problems is the central dilemma of DF structure-oriented process modeling despite it being used as the representation learning operator in the C-Forest layer. Converting prediction values to representation features is a feasible solution [25]. Hence, a preliminary attempt on representation learning proves the effectiveness of the DF method in the field of regression modeling.

However, DFR inevitably encounters the predicament of representation learning, that is, swamping effect. The swamping effect weakens not only the representation learning ability but also presents performance deterioration and diversity loss to a certain degree primarily because the raw feature vector dimension is remarkably larger than that of the representation feature. However, an effective or standard way to treat issues, such as proportion between raw feature vector and selected representation feature via k-NN, is unavailable. Although the representation strategy is widely used, realizing high-efficiency representation learning is very hard due to the regression modeling problem. Some improved strategies may prevent the swamping phenomenon in DFR. Hence, feature reduction, feature selection, and ALRFV combination strategies need further investigation.

Obtaining sufficient labeled value samples is difficult in practice when time, economic costs, and other factors are considered [130]. Thus, only a small number of labeled samples can be obtained for process modeling. As expected, this limitation allows DFR to be an alternative method for DNN in terms of regression modeling. The following subsections analyze and discuss impediments of future investigations (Fig. 11).

Fig. 11
figure 11
Challenges in DFR

Full size image
Overcome the issue of insufficient number of training samples for DFR
Compared with DNN that requires sufficient and labeled training data, DF can successfully perform modeling with small sample size data [12]. Improvement of the generalization performance on the basis of the primary DFR has become an important research direction. The following are typical strategies used in existing deep learning algorithms:

(1)
Based on data enhancement. This method generates new samples via translating, rotating, deforming, and scaling operations. Enhancement technology can improve the prediction performance [131]. At present, state-of-the-art algorithms include GAN [132], hallucination sample strategy [133], and encoder‚Äìdecoder architecture [134].

(2)
Based on transfer learning [135]. A large number of unlabeled samples coexist with a small number of labeled samples in actual industrial processes. The Bayesian framework can successfully transfer the knowledge obtained from the source domain to the target domain for a small sample size of data learning [136,137,138].

(3)
Based on metric learning. This strategy simulates the distance distribution among samples. The same kinds of samples are close mutually and vice versa in theory. The two types of metric learning methods for small sample size data are implemented using (1) Euclidean or cosine distance, such as deep triplet ranking [139], matching [140, 141], and prototypical [142,143,144] networks, and (2) DNN to measure the similarity between objects, such as generative adversarial residual pairwise networks [145, 146], relation network [147, 148], and metric-agnostic conditional embedding [149].

Combining existing strategies to modify the DFR structure while ensuring generalization performance and computational efficiency is very challenging.

Internal structure optimization of DFR
The steadily increasing accuracy of DNN without loss of generality is attributed to depth and width structural designs. However, large depths and widths are usually accompanied by high complexity [15]. Existing DFC studies have demonstrated the excellent performance of nondifferential deep learning structure as preliminary explorations. Moreover, DFC structure is very plastic due to the inherent potential interpretability of the DT algorithm and the combination strategy of ensemble learning. The contradiction between model complexity and prediction performance is an inevitable problem. However, reports on the optimization of parameters and structures of the DF-based deep learning model are limited.

Pruning-based optimization strategy [150, 151] can significantly improve the efficiency and generalization of parallel ensemble algorithm [59, 115]. Dropout strategy [152, 153] has been used as a powerful network optimization method for the internal structure of the deep learning model and as a regularization approach for network training. Knowledge transfer has also been utilized for network structure compression [154]. Although CNNs are typically improved by increasing its depth and width [155, 156], unavoidable problems, such as overfitting, hyperparameter increase, and deceleration of training speed, also occur. Batch standardization [157], convolution kernel decomposition [158], and other methods have been used to optimize the DNN structure. In summary, these approaches provide methodological support for the structure optimization of the DFR model.

Adaptive dynamic adjustment of DFR
Actual industrial processes demonstrate dynamic characteristics. The working condition drift is strongly associated with input material fluctuation and equipment abrasion. Therefore, the adaptive mechanism turns into a prerequisite. Recognizing abnormal samples is the first step when they appear. The human brain is a robust adaptive dynamic system. Although the majority of DFC and DFR methods are static, obtaining effective samples in the interaction process with industrial running circumstance is necessary. Hence, the updated model is independent of the limited and old training samples.

Dynamic neural networks, such as Hopfield [159], self-organizing neural [160], and recurrent neural [161] networks, can modify their structure through the self-adaptive mechanism. Thus, DFR should be enhanced to a more high-efficiency, compact, and intelligent version than the current static one to improve its industrial application.

Interpretability of DFR
A mathematical definition for the interpretability of the learning model is unavailable. Hence, interpretability is mainly divided into three nonmathematical illustrates, namely understanding decision-making reasons [162], understanding the model predicted results [163], and the process of model self-explanation [164]. Journals, such as ‚ÄúNature,‚Äù ‚ÄúScience,‚Äù and ‚ÄúMIT Technology Review,‚Äù have declared that interpretability is a necessary condition for deep learning development [165]. The interpretation method for DNN is divided into two types. One is used in CNNs, including representation interpretation for the hidden layer [166, 167], sensitivity analysis [166,167,168,169,170,171], model imitation [172,173,174,175], and multimodal interpretation [176, 177], and the other type focuses on recurrent neural networks, such as its visualization [178,179,180] and neural attention mechanism [181,182,183,184].

The DFR has been successfully applied to a certain extent [25]. However, knowledge on explaining its internal physical mechanism and improving its prediction performance is still finite. Thus, the interpretability of the DFR model can improve its performance and design architecture.

Relationship between deep learning and DFR
The comprehensive relationship between deep learning and DFR for an actual industrial process is demonstrated by combining the existing results of DFC and deep learning, as shown in Fig. 12.

Fig. 12
figure 12
Relations between the existing results and DFR for an actual industrial process

Full size image
Remark 3
A difficult-to-measure parameter for actual industrial processes is used as an example. Dioxin (DXN) emission concentration from the municipal solid waste incineration process is illustrated in Fig. 13. The common procedure for DXN measurement composed of online flue gas sampling and offline laboratory analysis presents disadvantages of high complexity, high cost, and time-consuming. The first factor to realize optimized control of DXN emission reduction obtains the DXN emission concentration in real time. However, solving this challenging issue is impossible due to the complex generation mechanism and emission process of DXN. Accordingly, how to model and forecast the DXN emission concentration based on small high dimensional data is a practical industrial issue. This problem must be solved due to strict environmental policies for pollutant emissions.

Fig. 13
figure 13
Dioxin emission in the municipal solid waste incineration process

Full size image
DF is a product of ensemble learning and DNN algorithms. Therefore, DF inherits not only the excellent performance of ensemble learning but also compensates for disadvantages of DNN [23]. The challenge of DFR can be divided into five stages (Fig. 12), which are described as follows:

Basic DFR model stage DFR modeling should be realized on the basis of the DFC model, ensemble learning theory, and DNN architecture in terms of actual industrial process problems (e.g., soft measurement for the difficult-to-measure parameter);

Overcome the issue of insufficient number of training samples for DFR stage High-precision DFR modeling should be accomplished using data enhancement, meta-learning, and transfer learning algorithms to solve issues of labeled small samples and unlabeled big data;

Internal structure optimization of DFR stage Optimization methods for DFR structure and parameters should be explored in terms of regularization, model compression, and network optimization to achieve the optimal performance of the static DFR model.

Adaptive dynamic adjustment of DFR stage DFR with adaptive dynamic structure organization should be investigated on the basis of human brain recognition mechanisms, dynamic networks, and concept drift detection. Thus, DFR can deal with industrial problems with characteristics of time-series properties and nonlinear dynamics.

Interpretability of DFR stage The interpretability of DFR based on representation learning, model visualization, and imitation interpretation should be the focused problem from the perspective of artificial intelligence application.

The critical point is dependent on the representation learning method of the C-Forest module. Furthermore, research on high-precision modeling is the basis for practical applications when characteristics of small sample size data are considered [115]. The running efficiency and prediction performance can be improved by mining the structure optimization method because of the high memory consumption and time cost of DFR [32]. The adaptive adjustment of the static DFR structure becomes necessary given that the actual practical object is a dynamic time-varying system. Interpretability of the DFR prediction results is necessary to obtain successful industrial application from the cognitive perspective of industrial domain experts and development prospective of artificial intelligent researchers.

Conclusions and prospects
This study reviews state-of-the-art methods of the DFC algorithm to construct an effective and small sample size for data regression modeling. These methods can be classified into five categories on the basis of feature engineering, representation learning, learner selection, weighting strategy, and hierarchical structure. The perspective of regression-oriented DT, FA, and DFR algorithms is analyzed and discussed in terms of industrial process modeling to determine the relationship between deep learning and DFR based on knowledge of industrial processes. Developing a DFR algorithm with characteristics of dynamic adaptive and interpretation ability and lightweight structure is an important research direction. The following aspects should be addressed in a follow-up investigation.

(1)
DFR representation learning method based on ensemble theory. The CDFV is used as the feature representation in DFC to show the uniqueness of the representation learning strategy. The predicted value selected via k-NN as the representation feature is the only method available for DFR. However, adopting the concatenate raw feature vector and representation feature method can easily cause the swamping effect phenomenon. The number, attribute, and selection method of feature representations; learner diversity; and model performance all require further investigation.

(2)
Implementing a small sample size data-oriented DFR modeling method based on fusion strategy. Excellent performance cannot be achieved due to insufficient feature attributes in the training dataset. Hence, different data processing technologies based on the fusion strategy have attracted considerable research attention. The latest data processing technologies for DNNs are mainly divided into scale or pattern expansion and model adaptive data. Many studies have proven the excellent performance of DF as a preliminary investigation on the nondifferential deep learning structure. Therefore, combining the existing small-sample size modeling technology and DFR structure can be the focus of a future investigation to achieve high prediction performance of the DFR model.

(3)
Enhancing diversity within the DFR structure. The performance of a singular excellent learner is usually inferior to the ensemble of many learners with diversity and accuracy. Complementarity among learners is more important than considering accuracy alone to generate not only an accurate learner but also build learners with diversity. However, hyperparameters, such as the forest and tree models and leaf node threshold, use the equal and fixed value in the present study. As a result, the diversity of learner structure cannot be reflected. Therefore, analyzing and resolving the combination diversity of training samples, learners, and their structure can be a research direction in the future.

(4)
DFR structure with high prediction performance and low memory consumption. Bagging parallel and boosting cascade ensembles and stack strategy are mutually coupling and used in DFR. Consequently, deredundancy and accelerated running methods of the DFR model based on structural optimization are also a vital research direction. Defining an optimization evaluation index according to characteristics of DFR is very necessary given that the DNN structure optimization design, such as the evaluation of model accuracy, running time, and model size, has achieved excellent results. The evaluation index can ideally estimate advantages and disadvantages of different DFR structures. In addition, the adaptive dynamic update and interpretability of the DFR-based industrial process model can also be a future research direction.

Keywords
Ensemble learning
DF classification (DFC)
DF regression (DFR)
Small sample size of data