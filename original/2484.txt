We address two key challenges of k-means clustering. In the first part of the paper, we show that: when a dataset is partitioned with an appropriate number of clusters (k), not more than 1/9 of D will exceed twice its standard deviation (2 s.d.), and not more than 4/9 of D will exceed its standard deviation (1 s.d.) (σ), where D is a vector comprising the distance of each point to its cluster centroid. Our bounds assume unimodal symmetrical clusters (a generalization of k-means’ Gaussian assumption). In the second part of the paper, we show that a non-outlier will not be further from its cluster centroid than 14.826 times the median of absolute deviations from the median of D. Interestingly, D is already available from the k-means process. The first insight leads to an enhanced k-means algorithm (named Automatic k-means) which  efficiently estimates k. Unlike popular techniques, ours eliminates the need to supply a search range for k. Meanwhile, since practical datasets may deviate from the ideal distribution, the 1 and 2 s.d. tests may yield different k estimates. Both estimates constitute effective lower and upper bounds. Thus, our algorithm also provides a general way to speed up and automate existing techniques, via automatically determined narrow search range. We demonstrate this by presenting enhanced versions of the popular silhouette and gap statistics techniques (Auto-Silhouette and Auto-Gap). We apply the second theoretical insight to incorporate automatic outlier detection into k-means. Our outlier-aware algorithm (named k-means#) is identical to the standard k-means in the absence of outliers. In the presence of outliers, it is identical to a known outlier-aware algorithm, named k-means-−, except for the crucial difference that k-means−− relies on the user to supply the number of outliers, while our algorithm is automated. Our technique solves a puzzle described by the authors of k-means−− regarding the difficulty of complete automation, which was considered an open problem.

Introduction
There is hardly a modern machine learning software that does not offer an implementation of the k-means clustering algorithm. This is due to its attractive properties and wide range of applications. Well-known examples of such software include R, Weka, Python, MATLAB, Julia and OpenCV, among others. K-means has been successfully applied in a variety of fields such as healthcare [1], market segmentation [2], manufacturing [3], and finance [4].

Given N points in Rd, k-means seeks to find k centers that minimize the total sum of squares [5], when each data point is assigned only to its closest center. Although the problem can be solved by a variety of algorithms, the standard algorithm due to Lloyd [6] has been embraced so widely that it is more popularly known as ‘the k-means algorithm’ or simply ‘k-means’ than any other name. This algorithm starts by selecting k points randomly, taken as initial cluster centers which are then refined iteratively. In each iteration, each center is moved to the position of the mean of all points currently assigned to that center. Following this update, the data points are regrouped using the new centers. This continues until convergence is reached or sufficient iterations have been performed.

Despite its wide acceptance, the k-means algorithm suffers from shortcomings which have attracted significant research concerns. The main issues that have been of interest in recent studies include dealing with outliers which can affect k-means’ results [7,8,9]; speeding up the algorithm for better scalability [10,11,12]; estimating the number of clusters which traditionally, the user must supply [13,14,15]; and how to address the problem of local mimima and selection of initial centroids [16,17,18]. Our paper is concerned with two of these challenges detailed in Sect. 1.1 (Problem Statement).

Problem statement
This paper is concerned with addressing two problems associated with k-means which are reputed to be challenging [19,20,21]. We pursue automation (no dependence on user-supplied parameter) and robustness to outliers, with efficient and easy-to-implement solutions. The two specific problems are described as follows:

(1)
The choice of k (number of clusters) determines whether k-means will yield meaningful clusters or not. In the traditional algorithm, the user must supply this value. Well-known techniques for estimating k, such as elbow, gap statistics [19] and silhouette [22], do so by comparing various values and require the user to correctly guess the search range. This can result in waste of time and computational resources, in situations where the true k is found early in the search space. Also, where the true k is outside the guessed range, the true solution will not be found. The technique proposed in this paper eliminates this problem. Our technique also provides a general way to speed up existing techniques.

(2)
K-means is sensitive to outliers in data. This problem comes rather from the k-means’ formulation itself, not merely the algorithm. Sensitivity to outliers is inherent to least squares optimization which relies on the mean—an outlier-sensitive statistic. Several proposed approaches involve a separate outlier detection module apart from the clustering process. This compromises efficiency significantly. Meanwhile, Chawla and Gionis [21], proposed an impressive algorithm capable of simultaneous clustering and outlier detection, without complicated modification to the original k-means. Although they successfully incorporate outlier detection into k-means, it is assumed that the user knows the number of outliers and must supply it. The authors leave full automation as an open problem, shedding light on the root of its difficulty: The number of outliers depends on the centroids set and vice versa. We address this problem and present a completely automated equivalent.

Contributions of the paper
The contributions of this paper are summarized in the following points:

(1)
Broadly, we show an interesting mathematical relationship between the challenges of automatically estimating k and that of detecting/avoiding outliers. We provide remarkably simple solutions based on Chebyshev’s inequality (CI).

(2)
We show how CI leads to a simple rule: When a dataset is partitioned with an appropriate k, not more than 1/9 of D will exceed twice its standard deviation, or not more than 4/9 of D will exceed its standard deviation, where D is a vector comprising the nearest-centroid distance for each point. These results come from assuming symmetrical unimodal clusters (k-means’ Gaussian assumption is one such distribution).

(3)
Again based on CI, we show that any non-outlying data point will not be further from its cluster centroid than 14.826 times the median of absolute deviations from the median of D.

(4)
We develop an efficient technique for estimating the number of clusters from the first theoretical result (in (2)), with two key advantages. It is very fast, compared to well-known techniques, since D is already available from the clustering process. It also eliminates the need for an expert to guess a range for values of k within which to search. Our technique (named Automatic k-means, or k-means-auto for short) decides a good k without comparing with other values.

(5)
We demonstrate how the technique in (4) provides a general way to speed up and automate existing techniques without losing accuracy. This is applied to the silhouette and gap statistics techniques.

(6)
We apply the second theoretical result (Contribution 3) to incorporate automatic outlier detection into k-means. The modified k-means (named k-means#) is identical to the standard k-means in the absence of outliers. In the presence of outliers, it is identical to a well-known outlier-aware algorithm, named k-means−−, except for the crucial difference that k-means−− relies on the user to supply the number of outliers, while our algorithm is completely automated.

Organization of the paper
In the rest of the paper, we discuss the related literature in Sect. 2 and brief theoretical foundations for our work in Sect. 3. Section 4 presents details of our algorithms for estimating the number of clusters, while Sect. 5 focuses on our approach to incorporating outlier detection into k-means. In Sect. 6, we present experimental results and discussions, before concluding the paper in Sect. 7.

Related works
This section discusses the related literature in two main parts. The first part discusses techniques for estimating the number of clusters in datasets, while the second subsection reviews techniques for dealing with outliers in clustering.

Estimating the number of clusters
The number of clusters (commonly called k) must be supplied by the user. This requirement assumes domain knowledge on the part of the user and is not always feasible. While there is hardly a universal solution [13], some interesting methods have been proposed.

We review these methods under two main categories. As one examines the existing literature, it becomes apparent that designing an algorithm for estimating k, boils down to addressing two questions. One: what property distinguishes a correct clustering from others? Two: how should the space of k values be searched?

Searching the k space
Perhaps, the most popular approach to searching the space of possible k values is to simply vary k from 1 up to a user-specified maximum kmax, performing k-means clustering using each value. The best value is the one that optimizes a specified metric. This approach is straightforward and is employed by many well-known techniques, such as [19, 22]. One drawback is that the user has to guess kmax. On the one hand, if kmax is too large compared to the true value, unnecessary computational expense is incurred. On the other, if kmax is smaller than the true k, then the true solution is never reached. This is one key drawback that is addressed by the technique (Algorithm 1) proposed in this paper. Our technique eliminates the need to guess a range for k. It terminates when the solution has been found.

Some techniques start with an initial value of k and then refine this number recursively, splitting and/or merging centroids according to some specified conditions. In the method of MacQueen [23], an initial value of k is chosen, and initial centroids are selected. If two centroids are closer than a specified value φ they are combined, and each data point is assigned to the nearest of the remaining centroids. Afterward, the centroids are updated, as in k-means. During this process, for each data point, if the distance to its nearest centroid is larger than another parameter ψ that point becomes a separate centroid forming its own cluster and attracting appropriate data points to itself. The algorithm terminates after one pass through the data. In his classic review of these methods [24], Steinley refers to Anderberg’s [25] criticism of this method for lack of clarity as to how to choose values of φ and ψ. The review mentions another similar method [26] that requires the user to specify three parameters: a distance threshold that determines whether a centroid is collapsed or a new one emerges; the minimum number of objects that should make up a cluster and the maximum value of k. The minimum description length (MDL) approach of Bischof et.al. [27] starts with a large value of k, and then, it removes clusters if doing so decreases the MDL. Other splitting-based techniques include the G-means algorithm of Hamerly and Elkan [28] and the dip-means algorithm of Kalogeratos and Likas [14].

Recognizing k
Moving to the question of recognizing a correct solution among other solutions, the various existing approaches can be classified into two main groups.

The first group identifies a measure that is expected to exhibit a predictable behavior at or around the true k, while the other group tests the conformity of clusters to distributional assumptions. One of the oldest approaches in the measure-based category is graphical. For each k, the total within-cluster distortion is plotted. The ‘right’ value of k is the one at which there is significant change in the slope, from steep to shallow. This method, known as the elbow method, is subjective [24]. Tibshirani and his fellow Stanford researchers [19] developed the popular gap statistics method which formalizes the ‘elbow’ heuristic and can be implemented algorithmically. The gap statistic measures how far log(Wk) falls below the expected value computed with null reference distribution of the data which assumes k = 1, where Wk is the pooled distortion. The right k is chosen to be the smallest that satisfies Gap(k) ≥ Gap(k + 1) − sk+1. The silhouette method [22] is another popular method in this category. The silhouette is a measure (− 1 to + 1) computed for each point in the data. A high value indicates the point fits well in its cluster and separates well from other clusters, while a low value indicates otherwise. Thus, the higher the average silhouette across all the points, the better the clustering, hence the more appropriate the underlying k value. Other existing measures include the Calinski and Harabsz index [29], the ratio of Duda and Hart [30], and the C-index of Hubert and Levin [31]. This list is not exhaustive. In their classic comparative study, thirty measures were studied by Miligan and Cooper [32].

Some other approaches test the conformity of clusters to an assumed distribution. One such technique is the earlier mentioned G-means algorithm of Hamerly and Elkan [28]. A statistical test is performed to test the hypothesis that a cluster follows Gaussian distribution at some specified significance level. If the hypothesis is rejected, then the cluster is judged to be non-Gaussian, and the cluster is split into two. The process continues until all clusters satisfy the test. G-means’ test is based on the Anderson–Darling statistic. Thus, the data must first be projected in one dimension. The authors discuss the choice of significance level and specify two different procedures for initializing the two child centers for a center that is to be split. Another algorithm, dip-means [14], also already mentioned, assumes unimodal clusters. It takes each point in a cluster to be a ‘viewer’ and applies the dip test to test for unimodality. These two algorithms, as mentioned earlier, navigate the k space by splitting centers, which may be prone to overfitting or underfitting [24]. They may also be prone to local minima since subsequent solutions are generated from the starting solution.

In the first part of this paper, we present automated and efficient algorithms for estimating k. Unlike popular techniques, ours decides a good k without comparing with other values. This eliminates the need for the user to supply a search range for k. Meanwhile, since practical datasets may deviate from the ideal distribution, the one-standard deviation and two-standard deviation tests may yield different k values. Thus, our technique is more robust when both estimates are used as lower and upper bounds. In this way, our algorithm becomes a general way to speed up and automate existing techniques, giving them the benefit of an automatically determined narrow search range. We demonstrate this with faster and automated versions of the silhouette and gap statistics techniques (Auto-Silhouette and Auto-Gap).

Detecting outliers
In clustering context, an outlier is an observation that does not fit into the overall clustering structure. That is, such a point is far from every cluster. This section discusses techniques that have been proposed to address k-means’ outlier problem.

Algorithms that handle outlier detection and clustering separately
Gan and Ng [33] noted that many existing methods perform outlier detection and clustering as separate processes. Examples include the two-phase algorithm of Jiang et.al. [34], the Clustering-Based Outlier Detection (CBOD) of Jiang and An [34] and the Outlier Removal Clustering (ORC) algorithm of Hautamaki et.al. [35]. A general drawback of the multi-phase approach is that it is time consuming, since k-means is only part of a longer, in some cases iterative, process. In the next subsection, we discuss some algorithms that perform clustering and outlier detection simultaneously. This is the approach of interest in this paper.

Algorithms that handle outlier detection and clustering simultaneously
Some progress has been made toward simultaneous clustering and outlier detection. In the K-means with Outlier Removal (KMOR) algorithm of Gan and Ng [33], the number of outliers is controlled by two parameters: a multiplier γ applied to the average distance and an upper bound n0. Likewise, the Non-exhaustive Overlapping k-means (NEO-k-means) algorithm of Whang et.al. [36] relies on two parameters α and β, although only β is an upper bound on the fraction of the data occupied by outliers, while α deals with the amount of cluster overlap. If α and β are set to zero, then NEO-k-means is identical to k-means. An attempt was made by the authors, toward determining β automatically. While they acknowledged that their algorithm ideally puts the burden of supplying β on the user, they proposed a heuristic to estimate β, which first runs full k-means on the dataset and counts the number of points further than 6 standard deviations from their cluster centers. Afterward, the estimated value of β can then be supplied to NEO-k-means. Besides, the fact that β is only an upper bound on the number of outliers, using the suggested heuristic is time-consuming [33] and it puts their algorithm in the multi-phase category.

In an interesting paper, Chawla and Gionis [21] introduce the k-means−− (k-means minus minus) algorithm which generalizes the k-means formulation into (k, l)-means to accommodate outliers. The parameters k and l are supplied by the user, where l is the number of outliers present in the data and k remains same as in k-means. That is, the goal is to find cluster X after excluding a set L ⊆ X containing outliers. In their algorithm, L consists of the top l points when all points are ranked according to their distance from their respective current cluster centroids. The authors noted that the difficulty of this problem lies in the fact that on the one hand, obtaining the subset L depends on the centers C (L consists of points that are far away from points in C), while on the other hand, C depends on L (if we knew L, then we could remove them from X and simply run standard k-means on X\L to obtain C). Therefore, they resort to taking l to be an additional user-supplied parameter. They note that requiring the user to supply a parameter that controls the number of outliers, is common to all existing methods and is therefore an open problem.

In the second part of our paper, we present a solution to this puzzle. The resulting algorithm (named k-means#) is identical to the standard k-means in the absence of outliers. In the presence of outliers, it is identical to a known outlier-aware algorithm, named k-means––, except for the crucial difference that k-means−− relies on the user to supply the number of outliers, while our algorithm is automated.

Foundations
The inequality of Chebyshev, a key result in probability theory, is foundational to our work. We discuss this theorem briefly in this section to provide necessary background for the sections that follow. An extension of this inequality, which is also of interest in this paper, is presented as well.

Chebyshev’s inequality
Chebyshev’s inequality provides a concentration bound guaranteed for probability distributions having finite variances. The inequality is also known as Bienayme–Chebyshev inequality. Informally, it states that not more than 1/𝑘2 of a distribution can be further from the mean. The inequality is stated formally in Lemma 1 with a short proof.

Lemma 1
Given an integrable random variable X having finite expected value μ and variance σ2 > 0, then for any real number h > 0,

Pr|𝑋−𝜇|≥ℎ𝜎≤1ℎ2
The inequality follows from applying to the random variable 𝑌=(𝑋−𝜇)2 and a bound of 𝑎=(ℎ𝜎)2, Markov’s inequality which states that Pr(|𝑌|>𝑎)≤𝐸[|𝑌|]𝑎.

Camp-Meidell inequality
Chebyshev’s inequality applies to the general distribution. Sharper bounds have been obtained for specific distribution types. Bickel and Krieger [37] provide a compilation of several extensions. One such extension, which is of interest in this paper, is the Camp–Meidell inequality which holds for unimodal symmetrical distributions and is stated as follows:

Pr(|𝑋−𝜇|≥ℎ𝜎)≤1−ℎ3‾√ifℎ≤23‾√,
Pr(|𝑋−𝜇|≥ℎ𝜎)≤49ℎ2ifℎ>23‾√
Automatic estimation of the number of clusters via Chebyshev’s inequality
In this section, we introduce the first of the two algorithms presented in this paper. This algorithm, rooted in the Camp–Meidell’s extension to Chebyshev’s inequality, provides a surprisingly simple solution to the reputedly difficult problem of automatically estimating the number of clusters. First, we discuss the rationale behind the technique and then outline the algorithm.

Detecting true clusters without clusters-based test
Our proposed method is derived from assumptions inherent in the k-means problem. K-means is a special case of the Gaussian mixture model (GMM) [8, 9, 13], in which the mixture components (clusters) have equal variance. First, we show that when this assumption holds, it is also true that the univariate distribution D is also Gaussian, if D is constructed by collecting the distance of each data point to its nearest centroid.

Lemma 2
Given k Gaussians 𝑔1,…,𝑔𝑘 having equal variance 𝜎2 and means 𝜇1,…,𝜇𝑘, the univariate distribution D will also be Gaussian. That is 𝐷={𝑑1,…,𝑑𝑘}→𝑑𝑁(0,𝜎2), where 𝑑𝑗={𝑥−𝜇𝑗:𝑥∈𝑔𝑗}, ∀𝑗∈{1,…,𝑘}.

Proof
Let 𝐷={𝑑1∪…∪𝑑𝑘}, where 𝑑𝑗={𝑥(𝑗)−𝜇𝑗:𝑥(𝑗)∈𝑔𝑗}. Also, let the k Gaussians 𝑔1,…,𝑔𝑘 have equal variance 𝜎2 and different means 𝜇1,…,𝜇𝑘, respectively.

The standard normal random variable z is given by:

𝑧=𝑥(𝑗)−𝜇𝑗𝜎→𝑑𝑁(0,1)∀𝑗∈{1,…,𝑘}
Thus, ∀𝑗∈{1,…,𝑘}:

𝑑𝑗=𝑥(𝑗)−𝜇𝑗=𝑧𝜎→𝑑𝑁(0,𝜎2)
𝐷={𝑑1∪…∪𝑑𝑘}
𝐷=⋃𝑗=1𝑘𝑧𝜎𝑗
𝐷=𝑧𝜎→𝑑𝑁(0,𝜎2).

□

Practical consideration: replacing the Gaussian fit requirement with unimodality and symmetry: Camp–Meidell inequality
Although there are a few simple properties of the Gaussian distribution that can be harnessed for our proposed density-bound test approach, such as the empirical rule (68–95–99.7 rule) and DasGupta’s inequality [38] (the inequality option is more useful since it accommodates uncertainty), we opt for a relaxation of the Gaussian requirement. The reason for this choice is that in practice, clusters are hardly perfectly Gaussian. Moreover, perfect Gaussian fit is not necessary to form meaningful clusters. Therefore, we relax the requirement, checking only for unimodality and symmetry.

Theorem 1
(Our rule for detecting k).

Given unimodal symmetrical clusters, when the number of clusters used to perform clustering is appropriate, at least 88.89% of D is less than twice its standard deviation and at least 55.59% of D is less than its standard deviation, where D is a vector of length N (number of points) comprising the distance of each point to its cluster centroid.

Proof
Since the Gaussian distribution is an example of unimodal and symmetrical distribution, the Camp–Meidell inequality holds for D:

Pr(|𝑋−𝜇|≥ℎ𝜎)≤1−ℎ3‾√ifℎ≤23‾√,
Pr(|𝑋−𝜇|≥ℎ𝜎)≤49ℎ2ifℎ>23‾√
Substituting h = 2 in the inequality gives:

Pr(|𝑋−𝜇|≥2𝜎)≤19
Therefore, at least 88.89% (i.e., 1−19) of D are less than twice its standard deviation. Notice this result agrees with  known results for Gaussian data, like the empirical rule (95% of the data will be within 2 standard deviations from the mean) and DasGupta’s inequality [38] (at least 91.67% fall within 2 standard deviations).

Similarly, by substituting h = 1, the inequality gives:

Pr(|𝑋−𝜇|≥𝜎)≤49
So, at least 55.56% (i.e., 1−49) of D are less than its standard deviation. Again, this result agrees with known results for Gaussian data, like the empirical rule (68% of the data will be within 1 standard deviations from the mean) and DasGupta’s inequality [38] (at least 66.67% fall within 2 standard deviations). □

Proposed k-means with automatic estimation of k
As a summary of the foregoing discussions, the proposed algorithm is now outlined as follows (Pseudocode in Algorithm 1):

1.
INITIALIZE k ← 1.

2.
CLUSTER Perform k-means clustering on data using current k and let D ← {the distance between each point and its nearest centroid} and 𝜎𝜎𝐷𝐷 ← standard deviation of D.

3.
TEST

a.
Option 1 If |{(d D) 2 𝜎𝜎𝐷𝐷}|>89𝑁𝑁, stop and return k (and corresponding clustering). Else, kk+1, go to step 2.

b.
Option 2 If |{(d D) 𝜎𝜎𝐷𝐷}|>59𝑁𝑁, stop and return k (and corresponding clustering). Else, k k+1, go to step 2.

figure a
A general technique for speeding up and automating k estimation algorithms
Since practical datasets may deviate from the ideal distribution, the one-standard deviation and two-standard deviation tests may yield different k values. Thus, our technique is more robust when both estimates are used as lower and upper bounds. In this way, our algorithm provides a general way to speed up and automate existing techniques, giving them the benefit of an automatically determined narrow search range. To demonstrate this concept, in this paper, we presented faster and automated versions of two popular techniques: silhouette and gap statistics (Algorithms 2 and 3 respectively).

figure b
figure c
Analysis and advantages
Theorem 1 leads to an efficient alternative to the approach of testing clusters. Only D needs be tested for Gaussian fit, to ascertain the appropriateness of the underlying value of k. This reduces the number of tests significantly compared to the approach of testing each cluster. If each cluster were to be tested per k value, the total number T of tests, will be 𝑇=1+2+⋯+𝑘′=𝑘′2(1+𝑘′). If we assume that the estimate is equal or close to the true value, i.e., k’ ≈ k, then this approach is O(k2). On the other hand, testing only D per k value yields a total of k’ tests, i.e., O(k). A second advantage is that D is already available as a by-product of the clustering process, so it does not have to be computed afresh by the test. The additional cost incurred by the test. Thirdly, previous methods of testing cluster distributions [14, 28] involve projecting the data to one dimension, which is a requirement of some popular distribution tests. Since D is one-dimensional, an intermediate projection step becomes unnecessary. Nevertheless, motivated by the quest for simplicity, we propose testing D using density-related properties instead of traditional hypothesis testing.

Automatic outlier detection via Chebyshev’s inequality
In this section, we show how Chebyshev’s inequality combined with robust scale estimation provides a surprisingly simple and efficient approach to automatic simultaneous clustering and outlier detection. The modified algorithm, named k-means#, is identical to k-means in the absence of outliers; exhibits speed that is hardly distinguishable from k-means’; and is shown to converge.

An open puzzle solved
The authors of k-means−− [21] described the difficulty of automatic outlier detection explaining that knowing the true centroids depends on knowing the true outliers and vice versa. Thus, they left automatic estimation of the number of outliers, as an open problem. Their algorithm demands that the user supplies this value an input. Our present paper addresses this problem by determining L dynamically using automatically computed cut-off distance in each iteration without the need to know the number of outliers.

Why outliers affect clustering
Outliers can affect the k-means algorithm at two levels:

1.
At initialization: Outliers are bad initial centroids. However, by definition, outliers are rare. So, the probability of having an outlier among the k initial centroids (sampled randomly) is generally small. This is more so because cluster density increases toward the center, giving central points higher weights than border points and outliers.

The phenomenon is supported by the fact that many well-known initialization methods do not ‘worry’ much about outliers. In fact, as an example, k-means++ [39] one of the most popular initialization techniques is rooted in the pursuit of initial centroids that are far apart—a mechanism that assumes these far points are not outliers. In summary, choosing an outlier at initialization is unlikely. Even if this ever happens, the common approach of repeating k-means multiple times addresses the problem. It is therefore, no surprise, that this aspect of the effect of outliers is hardly mentioned among works dealing with development of outlier-robust algorithms.

2.
During centroid update: Outliers contribute disproportionately to the mean (the new location of the centroid). This happens in every iteration. The phenomenon is easily understood with a toy example. Consider an outlier-free dataset: 1, 2, 3, 4, 5. The mean is 3. If a non-outlier (say, 4) is added, the mean will be 3.17. If the newly added value was to be 3, the mean would have remained exactly 3. In all, the mean remains stable (varies slightly around 3), if the newly added value is not an outlier. Now, if we had added an outlier instead (say 100), the mean would jump to 19. This illustrates how outliers lead to misleading cluster center estimates.

Clearly, k-means’ outlier problem lies in the centroid update step. ‘Diagnosing’ the ‘faulty’ part helps us deal with the problem without overhauling the whole algorithm.

Rethinking the centroid update step for outlier-awareness via Chebyshev’s inequality
To introduce automatic outlier detection into the standard k-means algorithm, we only make a simple modification to the centroid update step by solving a generalization of the k-means problem, described as follows:

Proposed generalized k-means formulation to cater for outliers
Given a dataset 𝑋=𝑋′∪𝐿 and the number of clusters k, find simultaneously, k points 𝐶={𝑐1,…,𝑐𝑘} and 𝑋′ (or equivalently, find L), so that the within-cluster sum of squared distances 𝜑(𝑋′,𝐶) is minimized.

Note:

1.
The standard k-means problem is a special case of this generalization, in which 𝐿=∅.

2.
In our formulation, neither L nor |L| is supplied by the user. These must be determined dynamically. We explain our approach to achieving this in the rest of this section.

A similar generalization, named (k, l)-means (where l =|L|), had been presented by Chawla and Gionis [21]. However, there is a key difference: their formulation, hence their modified algorithm (named k-means–-) requires the user to supply the number l of outliers, so that l points having the largest nearest-centroid distances are excluded when the new centroid is to computed for a given cluster. In the approach proposed in this paper, outliers are detected dynamically, so there is no need to supply either l or any other parameter besides the k that k-means requires.

For an arbitrary non-mixture distribution 𝑦𝜖𝑌𝑖, Pr(𝐷(𝑦|𝑐(𝑦))≥ℎ𝜎)≤1ℎ2, where 𝐷(𝑦|𝐶)=|𝑦−𝑐(𝑦)|,𝑐(𝑦) is the mean of 𝑌𝑖, h is an integer constant, and 𝜎 is the standard deviation.

In the context of our problem of interest, Lemma 1 (Chebyshev’s inequality) implies that if for all non-outlying data points, we collect the nearest-centroid distance into a single dataset 𝐷(𝑦|𝑐(𝑦)), not more than 1ℎ2 of these points will be farther than ℎ𝜎, regardless of the distribution of 𝐷(𝑦|𝑐(𝑦)). Thus, using an appropriate value of h and a robust estimate of 𝜎, a cut-off distance can easily be computed to detect outliers in each cluster. Interestingly, since k-means itself assumes clusters of (roughly) uniform scale, there is no need to detect outliers in each cluster separately. The bound can be applied to all the 𝐷(𝑥|𝐶):𝑥𝜖𝑋 where 𝑋=⋃𝑘𝑖=1𝑌𝑖, that is, all the distances combined into a single distribution [40]. This is even advantageous since the distribution will then have a larger number of points than the size of any single cluster.

Robust estimation of the true standard deviation
We note that the true (robust estimate) standard deviation 𝜎𝑟 can neither be computed using the regular standard deviation definition, 𝜎=1|𝑋|−1𝐷(𝑥|𝐶)2 nor the uncorrected version 𝜎=1|𝑋|𝐷(𝑥|𝐶)2. The reason is that 𝜎 is also affected by outliers, so we would be back to the same problem we intend to solve. This phenomenon comes from the involvement of the mean in the two formulae [41]. It is illustrated in Table 1.

Table 1 Advantage of robust estimation of standard deviation via the MAD instead of direct computation from data
Full size table
Computing cut-off threshold for outlier detection
Once we answer the question of robust estimation of the standard deviation, we are left with the question of determining the appropriate value of the factor h in order to construct our cut-off bound. It is tempting to quickly conclude that h = 3 should be appropriate. This would be consistent with common practice in statistics [41,42,43,44,45]. In fact, this value was adopted during the preliminary version of the idea presented in this present paper [8]. Chawla and Sun [46] refer to this concept of “at least three standard deviations from the mean” as the folk definition of outliers. Careful consideration of our context reveals this value will produce an algorithm that overestimates the number of outliers and is hardly ever identical with k-means in the absence of outliers. In other words, h = 3 is too small (See Fig. 1). Lemma 1 implies that for an arbitrary distribution, this overestimate may be up to 𝑁9, where N is the data size. (11 out of 100 inliers are falsely detected as outliers.)

Fig. 1
figure 1
G2-2-10 dataset [48]: Gaussian clusters with some noise but no outliers added. Left: k-means# using h = 3 results in false rejection of many inliers (yellow points); right: h = 10 has no false rejection (100% precision) and is identical with k-means (color figure online)

Full size image
Theorem 2 shows that even if the inliers in 𝐷(𝑥|𝐶) constitute a strictly Gaussian distribution, which in turn means that each cluster is Gaussian, the overestimate may still be up to 𝑁27. (4 out of 100 inliers are falsely detected as outliers.) We also note that while the use of h = 3 is popular in statistics, the choice is subjective and some authors have argued in favor of other values such as h = 2 or 2.5 [41]. Leys et al. [41] stated that the right value depends on the situation. A ‘last resort’ heuristic for the NEO-k-means algorithm (which, by the way, is only an upper bound for the percentage of outliers and does not address the non-robustness of the standard deviation) in [7, 9] suggests a cut-off of 6 standard deviations. Reemphasizing our goal of producing an algorithm that would be identical to k-means in the absence of outliers, we note that on the one hand, h should be much larger than 3, while on the hand, as h grows too large, at some point, we will begin to worsen the recall (wrongly treating an outlier as an inlier).

Theorem 2
(Proposed outlier detection rule).

Any non-outlying point will be within a distance 14.826 MAD(D) from its cluster centroid.

Proof
It has been shown [41, 47] that 𝜎𝑟=𝜃⋅MAD(𝐷(𝑋|𝐶)), where MAD(𝐷(𝑋|𝐶)) is the median absolute deviation from the median of 𝐷(𝑋|𝐶) and 𝜃 is a constant defined as 𝜃=1/𝑄(0.75), Q being the quantile function. Now, following the Gaussian assumption of k-means, 𝑄(0.75)≈0.6745 [41, 47], which yields 𝜃≈1.4826. So, 𝜎𝑟=1.4826 MAD(𝐷(𝑋|𝐶)).

To determine h in a statistically sound fashion, in this paper, we make a distribution-free estimate of an optimal value for h from Lemma 1, by setting:

Pr(𝐷(𝑥|𝐶)≥ℎ𝜎)≤1ℎ2=0.01
This yields h = 10.

Thus cut-off threshold T = ℎ𝜎𝑟=14.826𝑀𝐴𝐷(D). □

Proposed k-means with automatic outlier detection
The ideas discussed so far in this section result in a simple algorithm outlined as follows:

1.
Select initial centroids 𝐶={𝑐1,…,𝑐𝑘}, as in k-means.

2.
Repeat until convergence

a.
Assignment: just like in k-means, compute 𝐷(𝑋|𝐶) to partition the entire dataset 𝑋 into k clusters Y1, Y2, …, Yk.

b.
Centroid update: 𝑐𝑖=mean(𝑌𝑖∩𝑋′).

where 𝑋′={𝑥∈𝑋:𝐷(𝑥|𝐶)≤14.286MAD(𝐷(𝑋|𝐶))}. That is, use only inliers to compute the new centroid. Outliers are therefore 𝐿=𝑋∖𝑋′.

3.
Return C, L.

figure d
Notice that the proposed algorithm (Algorithm 2) does not depart much from the standard k-means in terms of structure. The only difference is that in the iterative part of the algorithm, step 2 (centroid update) is modified such that points assigned to cluster 𝑌𝑖 but which do not belong to the set of inliers 𝑋′ with respect to the current location of centroid 𝑐𝑖 are not used in updating that centroid. In our modified algorithm, the centroid update is preceded by obtaining 𝑋′. (This becomes step 2, while centroid update is performed in step 3.) This modification is all it takes to introduce automatic outlier avoidance and detection.

Analysis
Some key properties of the proposed k-mean# algorithm are analyzed. We focus on proving the algorithms convergence to local minimum, equivalence to k-means in the absence of outliers and robustness to outliers. We also show provable bounds on the performance of k-means# as an outlier detector.

Theorem 3
(outlier detection performance).

Under k-means’ assumption of equal-variance Gaussian clusters, given our choice h = 10, the probability of wrongly treating an inlier as an outlier is at most 0.003.

Proof
The Lemma follows from two known results in the literature, highlighted as follows:

1.
If the clusters are Gaussian with equal variance 𝜎, then 𝐷(𝑥|𝐶) is Gaussian with variance 𝜎 [40].

2.
For the Gaussian case, Chebyshev’s bound (Lemma 2.1) is tightened with the right-hand side reduced by a factor of 3 (1ℎ2 is replaced with 13ℎ2) [38].

Thus, Pr(𝐷(𝑥|𝐶)≥ℎ𝜎)≤13ℎ2. Substituting h = 10 yields an upper bound probability 0.003 for wrong rejection of an inlier. □

Figure 1 illustrates the comparison between the false positive rate of our proposed technique when h is taken to be 3 versus when it is taken to be our derived value of 10, on the G2-2-10 dataset [48].

Theorem 4
(convergence guarantee).

Algorithm 4 (k-means#)
is guaranteed to converge to local optimum.

Proof
The key to proving this lies in establishing that each operation in every iteration reduces the cost. Let us consider each of these steps in sequence.

Assignment step This is the first step of an iteration. C is provided as input to this step. The goal is to assign each point x to a cluster 𝑌𝑖, such that: 𝑌𝑖←{𝑥∈𝑋:𝐷(𝑥|𝑐𝑖)2≤𝐷(𝑥|𝑐𝑗)2,𝑖≠𝑗} and 𝑌𝑖∩𝑌𝑖,𝑖≠𝑗.

That is, for each x: 𝐷(𝑥|𝑐(pre)𝑖)2≤𝐷(𝑥|𝑐(post)𝑖)2, where 𝑐(pre)𝑖 and 𝑐(post)𝑖 are centroids with which x is grouped before and after the assignment, respectively. Since, 𝜑(pre)(𝑋,𝐶)=∑𝑥∈𝑋𝐷(𝑥|𝐶(pre))2 and 𝜑(post)(𝑋,𝐶)=∑𝑥∈𝑋𝐷(𝑥|𝐶(post))2. Clearly, 𝜑(pre)(𝑋,𝐶)<𝜑(post)(𝑋,𝐶) holds. That is, the assignment step always reduces the cost.

Exclusion step Now, let us consider the step in which we exclude outliers. It is easy to see that 𝜑(𝑋∖𝐿,𝐶)<𝜑(𝑋,𝐶). In the first place, excluding any point 𝑥∈𝑋:𝑥≠𝑐(𝑥|𝐶) reduces the cost; much more when the points excluded are outliers, which are the top contributors to the cost.

Update step It is well-known that the expression 𝜑(𝑌𝑖,𝐴)=∑𝑎,𝑦∈𝑌𝑖(𝑦−𝑎) is minimum at 𝑎=mean(𝑌𝑖)=𝑐𝑖(verify by setting ∂𝜑(𝑌𝑖,𝐴)∂𝑎=0). This implies that the update step also reduces the cost.

Combining the fact that each step in the algorithm reduces the cost yields the conclusion that 𝜑(𝑡)(𝑋,𝐶)<𝜑(𝑡−1)(𝑋,𝐶). Indeed, the same steps prove k-means’ convergence; the only step that does not apply to k-means is the exclusion step. □

Theorem 4
(Equivalence to k-means in absence of outliers).

Almost surely, 𝐶𝑘(𝑋′)=𝐶#(𝑋′), where 𝐶𝑘(𝑋′) and 𝐶#(𝑋′) are the cluster centroids estimated by k-means and k-means#, respectively, given an outlier-free dataset. That is, when there are no outliers, k-means# is identical to k-means.

Proof
If there are no outliers, that is, 𝐿=∅ then 𝑋=𝑋′.

So, 𝑌𝑖∩𝑋′=𝑌𝑖∩𝑋=𝑌𝑖∩𝑋′ (since 𝑌𝑖⊆𝑋).

Thus, 𝑐𝑖=mean(𝑌𝑖∩𝑋′)=mean(𝑌𝑖), and the theorem follows. □

Theorem 5
(Robustness to outliers).

With high probability, 𝐶𝑘(𝑋)=𝐶#(𝑋′).

(That is, when outliers are introduced into an otherwise outlier-free dataset X to form a new dataset X’, k-means# recovers the solution k-means’ produced on X from the contaminated data X’).

Proof
Basically, k-means is fed with 𝑋∖𝐿=𝑋′, while k-means# is fed with 𝑋=𝑋′∪𝐿.

In k-means, 𝑐𝑖=mean(𝑌𝑖).

In k-means# 𝑐𝑖=mean(𝑌𝑖∩𝑋′)=mean(𝑌𝑖).

Theorem 5 formally establishes that k-means is a special case of k-means#, in which the absence of outliers is assumed. It also means the solution k-means# returns is the solution k-means would have returned if the outliers were first removed.

We proceed to derive bounds on outlier detection performance metrics. While the bounds derived (Theorem 6) are interesting, the analysis is remarkably simple. This is another aspect of simplicity of the original k-means which our algorithm preserves.

Theorem 6
With regard to classifying points as inliers and outliers, k-means# is characterized by the following performance bounds:

a.
0.99≤Accuracy≤1.

b.
𝐿0.99𝐿+0.01𝑁≤Precision≤1, where N is the number of points in the data.

c.
Recall=1.

And if the clusters are Gaussian, these bounds improve to:

a.
0.997≤Accuracy≤1.

b.
𝐿0.997𝐿+0.003𝑁≤Precision≤1.

c.
Recall = 1.

Proof
The results follow from Theorems 2 and 3. Clearly, outliers will be correctly detected (Recall = 1), and Theorem 3 provides upper bounds on the number of inliers that can be incorrectly classified as outliers. Since we seek to evaluate the performance of outlier detection, we take outliers as the positive class and inliers as the negative. Thus,

True positives TP = L,

False positives FP≤0.01 for arbitrary distribution (Lemma 1) or FP≤0.003 if k-means’ Gaussian assumption holds (Theorem 2),

True negatives 0.99𝑁−𝐿≤TN≤𝑁−𝐿,

False negatives FN = 0.

Accuracy = TP+TN𝑁. So, 0.99≤Accuracy≤1.

Precision = TPTP+FP. So, 𝐿𝐿+0.01(𝑁−𝐿)≤Precision≤1.

Recall = TPTP+FN. So, 𝐿𝐿≤Recall≤1; Recall = 1.

The Gaussian cases can be easily verified by following the same steps. □

Experiments
This experiment section is divided into two main parts. The first part evaluates our algorithms for estimating k—Auto, Auto-Silhouette and Auto-Gap. The second part of our experiments evaluates the performance of our outlier-robust k-means (k-means#).

Estimating number of clusters
This section reports results of experimental performance evaluation on standard (15 clustering benchmark and 3 real-world) datasets.

Method
We compare our 3 proposed algorithms with two well-known techniques (gap statistics [19] and silhouette [22]) are also tested and compared with ours. We are interested in the accuracy of the estimates of k and the efficiency of the algorithms. To measure accuracy, we record the mode of the estimate over 10 runs for each algorithm (Table 1) and compare this value to the true k (Table 2).

Table 2 Estimated k: mode over 10 runs per dataset
Full size table
In our implementation of each algorithm, k-means++ is employed in performing k-means clustering. This is preferred to the standard k-means because it is more stable and more accurate. For the gap method, which requires generating reference distributions, our implementation generates 30 distributions. To prevent the traditional algorithms from running for an impractically long time per dataset, we limit the search range to 1–50, given that none of the datasets studied exceeds 50 clusters. This range is sufficient to demonstrate the efficiency margin between the traditional algorithms and ours. We have used the k-means +  + , gap, and silhouette implementations of the Statistical and Machine Learning Toolbox of MATLAB 2016a. All code is implemented in this MATLAB environment, running on Windows 10 operating system, on a 4 GB (3.7 GB usable) RAM, intel (R) core (TM) i5–3320 M CPU @ 2.60 Hz 2.60 Hz computer.

Datasets
The datasets studied are as follows:

1.
Iris This is, perhaps, one of the most widely studied datasets in clustering and classification literature. It consists of real-life data comprising species of the Iris flower. Each flower is represented by four numerical features (dimensionality d = 4). Three species are represented. However, two of the species overlap. Thus, there are 3 classes or 2 well-separated classes (k = 2 or 3; see Fig. 1). Number of flowers N is 150.

2.
Penguin Another real-world dataset consisting of male and female penguins belonging to three species. N = 334, d = 8, k = 2 or 3.

3.
Breast Cancer Diagnostic Winconsin breast cancer database. N = 699, d = 9, k = 2.

4.
Ruspini This is another widely studied dataset, synthetically created from 75 points (N = 75) in four distinct classes (k = 4). A careful examination of the dataset (Fig. 1) reveals that the dataset can still be partitioned into 5 to 7 groups.

5.
Dim set [48] Synthetic high dimensional datasets consisting of Gaussian clusters. Each dataset has 1024 points in 16 clusters. The dimensionalities are 32, 64, 128, 256 and 512.

6.
S set [48] Synthetic 2-dimensional data having 5000 points in 15 clusters. S1, S2 and S3 have cluster overlaps of 9%, 22% and 41%, respectively.

7.
A set [48] Three datasets A1, A2, A3, containing spherical clusters. K = 20, 35 and 50, respectively, and N = 3000, 5250 and 7500, respectively, while cluster size (150), deviation (1402), overlap (20%) and dimensionality (2) are kept constant.

8.
G2 set [48] Gaussian clusters with varying overlap. N = 2048, k = 2, d = 2, variance = {10, 20, 40}.

Preprocessing
Majority of the datasets are employed in the literature for benchmarking clustering algorithms “as is.” Only the Penguin dataset required processing: We removed two cases having missing data.

Results and discussion
Tables 2, 3 and 4 show that our proposed technique competes favorably with the known methods. Our proposed automatic silhouette algorithm correctly predicts the number of clusters in nearly every reported case. This is also true for the standard silhouette, but ours is drastically faster. Perhaps, more importantly, ours is automated, while the standard silhouette relies on the user to guess a search range for k. Compared to the traditional gap statistics method, our automatic gap algorithm is much faster, more accurate and automated.

Table 3 Estimation error = Difference between estimated and true k
Full size table
Table 4 Mean CPU time (seconds) over 10 runs
Full size table
The wide range of datasets studied helps to observe how the performances of the algorithms are affected by increasing data size, number of clusters, dimensionality and cluster overlap. The general observation is that the margin by which our algorithms surpass the traditional algorithms in efficiency increases with data size and dimensionality. The main factor that makes accurate estimation challenging for the algorithms is cluster overlap. In such cases, gap statistics produces estimates that are very poor. The impact on our technique is that the estimated interval for k is widened. Yet, our automated versions of silhouette and gap still produce impressive estimates.

In summary, our experiments show that estimating the number of clusters by measuring intra-cluster cohesion and inter-cluster separation proves to be more robust than distribution-based measure like the gap statistics. Meanwhile, both silhouette and gap consume a lot of time because of the intricate computations involved. Our simple concentration-based test proves to be drastically faster and highly scalable, essentially involving only the computation of standard deviation. Yet, it proves accurate and can be used as a stand-alone algorithm for neatly clustered datasets. Our technique becomes robust, accurate, yet still much faster than the traditional algorithms, when hybridized with silhouette to constitute our Auto-Silhouette algorithm. An important advantage of our proposed algorithms is that they are automated, relying no user input, unlike the traditional techniques.

The experimental results (Tables 2, 3, 4, 5) show that when used as a stand-alone algorithm, our proposed technique correctly estimates k in 11 out of the 18 case. The 1σ test produced the correct estimate in 4 cases, while the 2σ test produced 8 of the cases (both of them yielded the correct value for one case: the Iris dataset). Only in 3 out of the 18 cases, do we have a situation where both tests yield absolute error that is greater than 1, at the same time. Moreover, the algorithm is very fast: its running time ranged between 0.01 s and 2.85 compared to the running times of the popular silhouette and gap statistics techniques which have range 0.75–232.43 s.

Table 5 Performance of the proposed technique when used as a stand-alone algorithm
Full size table
The experimental results (Tables 2, 3, 4, 6) also show that without loss of accuracy, our technique speeds up silhouette giving running time range of 0.03–127.86 s. Similarly, our technique speeds up gap statistics from range 17.10–242.55 s to 0.50–134.37 s (Tables 3, 4, 7).

Table 6 Comparing the standard silhouette with our fast and automated version (Auto-Silhouette)
Full size table
Table 7 Comparing the performances of the standard gap statistics method with our fast and automated version (Auto-Gap)
Full size table
Simultaneous clustering and outlier detection
This section focuses on empirical evaluation of the proposed k-means modification.

Method and metrics
In the experiments, k-means# is compared with the two most related existing algorithms: k-means and k-means−− [21]. The k-means−− algorithm must be supplied with the true number of outliers, while the proposed k-means# is not supplied with this value. This is already an advantage, but there is need to compare their accuracies. Six versions of the G2-2-10 dataset and six versions of the Iris datasets are created by adding varying amounts of outliers: 0%, 2% and 4%. G2-2-10 is part of the benchmark of set of Franti and Sieranoja [48], while Iris is a popular real-world flower species dataset. G2-2-10 has 2048 points in 2 dimensions and 2 clusters, while Iris has 150 points in 4 dimensions and 3 clusters. These datasets are known to be clusterable by k-means and ground truth centroids are available for these datasets. The goal is to measure the extent to which the algorithms can recover the results they produce when there are no outliers, after outliers are added.

Metrics The metrics of interest are described as follows:

RN This is simlar to the ratio 𝐷(𝑥|𝐶)/𝐷(𝑥|𝐶∗) used in [21], where C are the estimated cluster centers, and C* are the ground truth outliers. We sum the contribution (squared Euclidean distance) of all the points considered to be non-outliers to the estimated cost (using estimated centers) and divide by the same sum computed using ground truth centers. Thus, the smaller the value of RN, the better the performance.

RO this is the same ratio as RN but computed over outliers only instead of non-outliers. The larger this value, the more the ‘outlyingness’ of the detected outliers. Other metrics are as defined already.

Initialization For each dataset, the three algorithms are initialized identically. We sample k points uniformly and use them as initial centroids for each algorithm.

Results and discussion
The experimental results (Tables 8, 9, 10, 11, 12, 13 and Figs. 2, 3, 4, 5, 6, 7) show that all three algorithms produce identical results when there are no outliers. In the presence of outliers, the proposed k-means# produces the same results as k-means−−. Looking at all presented tables, the two algorithms record exactly same values for all the metrics. There is one seeming exception: in the case G2-2-20 dataset with 4%, k-mean−− records RN = 0.9958 while k-means# has 0.9961; k-means−− had RO of 1.0023, while k-means# had 1.0010. Firstly, the differences are negligible. Secondly, note that, this could be due to the fact that slightly different centers can yield the same clustering. In fact, the algorithms estimated the same value for one of the centers, while the differences between each coordinate of their estimates for the second center are within 000.2. Furthermore, the two algorithms had the same values of accuracy, precision, recall and F1 score.

Table 8 G2-2-10. 0% outliers: The algorithms are identical
Full size table
Table 9 G2-2-10. 2% outliers
Full size table
Table 10 G2-2-10. 4% outliers
Full size table
Table 11 Iris dataset. 0% outliers: The algorithms are identical
Full size table
Table 12 Iris dataset. 2% outliers
Full size table
Table 13 Iris dataset. 4% outliers
Full size table
Fig. 2
figure 2
G2-2-10 dataset, 0% outliers. K-means, k-means−− and k-means# all produced exactly same cluster centroids

Full size image
Fig. 3
figure 3
G2-2-10 dataset, 2% outliers. K-means’ center (blue) has shifted slightly in the direction of the outliers, while k-means−−  (green) and k-means# (red) remained robust (color figure online)

Full size image
Fig. 4
figure 4
G2-2-10 dataset, 4% outliers. K-means’ center (blue) shifts further, while k-means−− (green) and k-means# (red) remain robust. K-means# produces exactly the centroid as k-means (color figure online)

Full size image
Fig. 5
figure 5
Iris dataset, 0% outliers. K-means, k-means−−  and k-means# produce exactly same centroids

Full size image
Fig. 6
figure 6
Iris dataset, 2% outliers. K-means’ center (blue) shifts toward the outliers, while k-means−− (green) and k-means# (red) remain robust (color figure online)

Full size image
Fig. 7
figure 7
Iris dataset, 4% outliers. K-means (blue) estimates the wrong centers in this case, while k-means−− (green) and k-means# (red) remain robust (color figure online)

Full size image
While k-means has no capacity to detect outliers, throughout the experiments, the other two algorithms record 100% accuracy, precision, recall and F1 score. These confirm the effectiveness of our proposed generalization of the k-means formulation. The results further speak to the effectiveness of the proposed automatic detection. Thus, the riddle of Chawla and Gionis [21] is effectively solved. Throughout the experiments, k-means# correctly estimates the exact number |L| of outliers.

In terms of speed, no clear trend is observed in the comparison among the algorithms. The three algorithms exhibit comparable speed. This means that their outlier detection mechanisms do not increase k-means’ speed significantly. Such preservation of k-means’ attractive properties is emphasized in this paper.

The observations regarding speed accuracy show that the work of Chawla and Gionis [21] is an impressive solution to k-means’ outlier problem. Indeed, their work is a solution in the right direction. A further implication of all these experimental results is that the gap left, of full automation, is filled by this paper, without observable speed trade-off.

Summary and conclusion
We have presented automated and highly efficient solutions to two key challenges of k-means clustering. First, we showed how Chebyshev’s inequality leads to a simple, fast and automated way to estimate the number of clusters. We demonstrate the use of this technique as a stand-alone algorithm, and as a general way to drastically speed up existing algorithms. We demonstrated the latter with faster and automated versions of the popular silhouette and gap statistics methods.

Secondly, we incorporate automatic outlier detection into k-means, yielding an algorithm that is identical to the standard k-means in the absence of outliers. In the presence of outliers, it is identical to a well-known outlier-aware algorithm, named k-means−−, except for the crucial difference that k-means−− relies on the user to supply the number of outliers, while our algorithm is completely automated. Our technique solves a puzzle described by the authors of k-means−− [21] regarding the difficulty of complete automation, which was considered an open problem.

It might be an interesting direction for future work to explore incorporating similar techniques into other clustering algorithms.

Keywords
k-means clustering
Number of clusters
Outliers
Chebyshev