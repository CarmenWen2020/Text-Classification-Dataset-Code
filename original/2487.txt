Retinal image analysis is an integral and fundamental step towards the identification and classification of ocular diseases like glaucoma, diabetic retinopathy, macular edema, and cardiovascular diseases through computer-aided diagnosis systems. Various abnormalities are observed through retinal image modalities like fundus, fluorescein angiography, and optical coherence tomography by ophthalmologists, and computer science professionals. Retinal image analysis has gained a lot of importance in recent years due to advances in computational, storage, and image acquisition technologies. Better computational capabilities lead to a rise in the implementation of deep learning-based methods for ocular disease detection. Although deep learning promises better performance in this field, some issues like lack of well-labeled datasets, unavailability of large enough datasets, class imbalance, and model generalizability are yet to be addressed. Also, the real-time implementation of detection methods on new devices or existing hardware is an untouched area. This article highlights the development of retinal image analysis and related issues due to the introduction of AI-based methods. The methods are analyzed in terms of standard performance metrics on various publicly and privately available datasets.

Introduction
A report “The Global Burden of Vision Impairment” by Vision Atlas in 2015 mentioned that 253 million people are visually impaired in the world [1]. The major causes of visual impairment include age-related macular degeneration (AMD), cataract, corneal opacity, uncorrected refractive error (URE), diabetic retinopathy (DR), trachoma, glaucoma, and other abnormalities as shown in Fig. 1. In the case of timely detection and treatment, 75% of visual impairments is preventable [1, 2]. Another report by WHO in 2021 mentioned that the number of visually impaired patients are 2.2 billion, among which around 1 billion cases are preventable [3]. Early detection of ocular diseases is crucial for the prevention of such cases and improve the quality of life of the patients. Therefore, a need for computer-aided diagnosis (CAD) arises for the early detection of ocular diseases.

Fig. 1
figure 1
Causes of visual impairment and their distribution

Full size image
Various ophthalmological diseases are caused due to abnormalities manifested near anatomical structures of the human retina, which are visualized by retinal images. Retinal image analysis is an important step for the identification of ocular diseases. In recent years, retinal image analysis gained traction due to improvement in medical image acquisition and low-cost computer storage capacity. Ocular disease detection in clinical setup is done with the help of various retinal image modalities. The patient’s retina is initially observed using fundus images. Nowadays, fundus image acquisition devices are available in tabletop and hand-held configurations. With the development of hand-held fundus cameras, it is possible to acquire a large amount of data from urban and remote areas. Other modalities used to investigate the retinal further are fluorescein angiography and optical coherence tomography (OCT). OCT image acquisition devices are recently developed. These devices are expensive in comparison to fundus cameras. Due to the availability of low-cost high computational facilities in recent years, artificial intelligence (AI) can play an important role in medical image analysis. AI aims to develop models that can be integrated with image acquisition devices for automated disease diagnosis. Several systematic and detailed reviews on various steps used in retinal image analysis and ocular disease detection are available in the literature [4,5,6,7,8,9,10,11]. The motivation behind this work is to discuss challenges associated with the introduction of AI in retinal image analysis and explore related methods. The key objectives of this work are:

1.
Exploration of state-of-the-art methods in retinal image analysis using conventional and deep learning-based methods.

2.
Discussion on CAD models for ocular disease diagnosis and screening.

3.
Analysis of key challenges related to retinal image datasets and models arises by introducing AI for the ocular disease detection.

4.
Understanding the real-life clinical challenges associated with acceptability and deployment of AI models.

Retinal images and associated ocular diseases are discussed in Sect. 2. The retinal image dataset and evaluation parameters used in state-of-the-art methods are discussed in Sect. 3. Existing state-of-the-art conventional and deep learning methods are described in Sect. 4. The CAD systems for ocular disease diagnosis are discussed in Sect. 5. The technical and clinical challenges of AI in the field of retinal image analysis are discussed in Sect. 6. This includes issues related to dataset size, quality, and class imbalance, variation in classification systems and reference standards, deployment and acceptance of AI models in the real-life clinical setup in the case of retinal image analysis. Finally, the work is concluded in Sect. 7.

Retinal imaging and ocular diseases
Fig. 2
figure 2
Anatomy of the retina [12]

Full size image
The retina is a thin layer of tissue that lines the back of the eye. It converts the incident light into neural signals, which are processed by the visual cortex of the brain for recognition of objects [13]. The study of retinal tissue is important for the maintenance of the overall health of a person. As shown in Fig. 2, the eye contains various anatomical structures responsible for image formation. The light reflected from an object enters through a transparent tissue called the cornea. The light then passes through the pupil, and the lens further focuses the image on the retina. The retina is a highly metabolically active tissue with a double blood supply that allows non-invasive observation of circulation. The blood supply of the retina comes from the choroid and retinal vasculature. It is also connected to the brain through the optic nerve. The optic disc represents the beginning of the optic nerve. It is the direct connection to the brain where the image is formed. The choroid is further divided into various layers, which are used for the diagnosis of different diseases. Ophthalmological diseases of the eye, as well as diseases that affect the circulation and the brain, can be manifested in the retina. The detection of ocular diseases like macular degeneration, glaucoma, diabetes, hypertension, and some cardiovascular diseases is done through retinal image modalities. Fundus, fluorescein angiography, and OCT image modalities are used to visualize various anatomical structures like blood vessels, optic disc, fovea, and macula. The shape, size, and color changes in anatomical structures indicate the presence of abnormalities related to diseases.

Fundus imaging
The invention of the fundus image camera in 1910 by Gullstrand was a significant step towards the visualization of retinal structures of the human eye [15]. Fundus images are a 2D representation of the 3D retinal tissues captured using reflected light [16]. The fundus images are captured using a specialized low power microscope with an attached camera which works on the principle of indirect ophthalmoscopy. The fundus cameras are described by the field of view (FOV) which ranges from 30∘ to 140∘ [17]. The fundus imaging includes a broad set of modalities like red-free photography, colored fundus photography (CFP), stereo fundus photography, and fluorescein angiography [13]. The colored fundus photographs are commonly used in the real-life clinical setup. The CFP comprises three color channels, namely, red, green, and blue. Each channel highlights a different anatomical part of the retina, which is helpful in segmentation and detection tasks. Fundus images are used as a primary modality by ophthalmological experts for the detection of diseases like diabetic retinopathy, glaucoma, AMD, cataract, hypertension, and myopia [18]. In contrast to expensive OCT imaging technology, fundus imaging is a non-invasive and cost-effective way of screening and detection of ocular diseases.

Fundus images with various abnormalities and associated ocular diseases are shown in Fig. 3. Conventionally, fundus image analysis focuses on the segmentation and detection of anatomical structures and other changes near and inside them. The primary step of ocular disease detection is the segmentation of anatomical parts like fovea, macula, optic disc, and vasculature present in the retina as shown in Fig. 3a. The vessel structure of the retina includes major and fine blood vessels. The abnormalities appear due to change in the vascular structure cause diabetic retinopathy, hypertension, and AMD diseases. The leakage near vessels constitutes microaneurysms and hemorrhages, as shown in Fig. 3b. The segmented parts are used for conventional feature extraction and classification using machine learning algorithms. The segmentation of optic disc and cup is also a crucial step in the identification of glaucoma where the cup to disc ratio is an important indicator [19]. The optic cup of the patient suffering from glaucoma has a large cup to disc ratio as shown in Fig. 3c. Blood vessel related abnormalities like neovascularization and geographic atrophy can be seen in AMD as shown in Fig. 3d. Hypertension changes the morphological properties of the blood vessels and results in AV nicking and arteriolar narrowing as shown in Fig. 3e. The basic anatomical structures become blurry or completely vanish from the fundus image in the case of cataract (as shown in Fig. 3f). Peri-papillary atrophy occurs near the optic disc of the retina in myopia patients (Fig. 3g). Macular vitreous degeneration, pigment epithelium proliferation, and epiretinal membrane are some of the other abnormalities which affect fundus images (Fig. 3h). The advantage of using fundus images in clinical setup lies in the single image capture of the back of the retina. According to the FOV of a fundus camera, various parts of the retina can be captured in a single image. In a real-life clinical setup, pupillary dilation is usually needed to capture a fundus image [20], which is a major disadvantage of this retinal image modality. Various non-mydriatic fundus cameras are recently made available, which do not require any dilation but may introduce some artifacts due to exposure. Non-mydriatic cameras need patients to be attentive and cooperative, which might not be possible during intensive conditions [21].

Fig. 3
figure 3
Fundus images from ODIR dataset showing anatomical structures and abnormalities due to various ophthalmological diseases: a normal image; b glaucoma; c diabetic retinopathy, d AMD, e hypertension, f cataract, g myopia, and h other abnormalities [14]

Full size image
Fluorescein angiography
The development of fluorescein angiography imaging allowed the understanding of the functional state of retinal circulation [22]. These images are captured by injecting fluorescein dye in the patient’s circulation where each image pixel intensity represents the number of emitted photons from the dye [13]. Fluorescein angiography images are used to observe abnormalities caused due to ocular diseases like diabetic retinopathy and macular edema. The retinal neovascularization causes fluorescein dye leakage, which is indicated near the optic disc and blood vessels [23]. The publicly available datasets for fluorescein angiography usually contain ground truth for blood vessel segmentation as shown in Fig. 4. The fluorescein angiography images capture a wider area of the retinal and choroidal vasculature in comparison to fundus or OCT image modalities. The ultra-wide fluorescein angiography images usually range up to 200∘ FOV, which is a major advantage of this retinal modality in clinical practice. The dye used during the capture of fluorescein angiography images ensures fewer artifacts and is easier to interpret. However, this modality is not popular due to the potential side effects like nausea and allergic reaction to the injected dye [24]. Some pathological conditions require long-term treatment and repeated follow-ups. The invasive nature of angiography impedes its practical use during routine treatment [25].

Fig. 4
figure 4
Fluorescein angiography image from RECOVERY-FA19 dataset [26]: a ultra-wide fluorescein angiography image and b ground truth for vessel segmentation

Full size image
Fig. 5
figure 5
OCT image captured by centering on fovea depicting its various layers [27]

Full size image
Fig. 6
figure 6
OCT images from Duke dataset: a normal image showing fovea, b abnormality in retinal layers, and c–d noisy images corresponding to images shown in a and b

Full size image
OCT imaging
OCT imaging is a relatively new technology for retinal image acquisition. OCT is a non-invasive imaging technique based on the principle of low coherence interferometry to capture cross-sectional images of ocular tissues [28]. OCT images are mainly captured through time-domain (TD-OCT) and spectral-domain (SD-OCT) imaging technologies. HD-OCT and 3D-OCT are SD-OCT images with added features [29]. The 3D volumetric scans of the retina are acquired through OCT imaging by focusing on an anatomical retinal structure for example fovea as shown in Figs. 5 and 6 [30, 31]. This correspondence can be used to construct fundus images of the anatomical retinal structure. The cross-sectional visualization is an important tool for the identification and assessment of abnormalities related to ocular diseases. The OCT images mainly consist of 11 layers as shown in Fig. 5, where a dip is observed in the retinal layers showing fovea. A normal image is shown in Fig. 6a. Ocular diseases affect different layers of OCT image differently. Macula, the central portion of the retina, is observed through the outer plexiform layer (OPL). The complication of diabetes affects fovea as fluid accumulation in OPL and is termed as DME. Drusen are focal deposits located between the RPE layer and the Bruchs membrane, as shown in Fig. 5. Patients suffering from drusen have an increased risk of AMD. When drusen start accumulating between the RPE layer and choroid, it results in AMD development, as shown in Fig. 6b. It is manifested in dome-shaped atrophy and causes severe vision damage. The development of new blood vessels in the choroid layer results in an abnormality called CNV. This abnormality can be seen near the RPE layer in the form of pocket-like atrophy.

The abnormal thickness of the retinal layer is an indicator of abnormalities caused due to diseases. Layer segmentation based and feature based methods are used for the identification of ocular disease [13, 17]. The OCT images provide a faster way of capturing retinal structures with a higher number of scans. The greater number of scans provide high-resolution images resulting in a broader range of operations. However, OCT images suffer from speckle noise during the acquisition process as the imaging light is scattered and coherently superimposed by the ocular tissues (as shown in Figs. 6c and 6d). This noise interferes with layer segmentation and feature extraction in later stages and needs to be removed before processing. The OCT imaging technology is also comparatively expensive than other retinal image modalities.

Retinal image datasets and evaluation parameters
Retinal image data acquisition is a prerequisite in the construction of good deep learning-based models. The retina datasets are majorly categorized into pre-processing, segmentation, and disease classification for each imaging modality. Various publicly available datasets are explored in the literature for optic disc and blood vessel segmentation as summarized in Table 1a and b respectively. Vessel segmentation is also explored using fluorescein angiography images along with classification problems. A limited number of datasets publicly available for fluorescein angiography images for segmentation and classification are mentioned in Tables 1c and 2b, respectively. The OCT images are used for ocular disease detection by following three major steps, namely denoising, layer segmentation, and classification. Datasets available for denoising and layer segmentation of OCT images are summarized in Table 1d. Classification datasets for fundus and OCT images are mentioned in Table 2a and c, respectively.

The conventional and deep learning methods are usually implemented on MATLAB and Python environments. The simulation scenario for ocular disease classification comprises of dividing the dataset into training, validation, and testing sets. The models are trained on the training set and validated on the validation set. The validated model is evaluated on an unseen testing set. In conventional machine learning methods, the models are cross-validated using methods like leave p-out, holdout, k-fold, and Monte Carlo. The conventional methods can be implemented on systems with limited storage and computational resources. The deep learning model is trained for several epochs and validated at each step. Hyperparameters for the deep learning methods are selected based on dataset type and domain requirement. The deep learning methods need more storage space and GPU-enabled systems for training.

Some of the popular performance metrics used for the performance evaluation of retinal image analysis methods in the literature are discussed below. Segmentation and classification methods discussed in this work are mainly evaluated on the performance parameters based on the confusion matrix. The confusion matrix is constructed using the ground truth and the predicted labels for any task [32]. In the case of segmentation, each pixel of an image is assumed as one sample and the foreground and background are treated as classes. The number of true positives (TP) is counted when the positive labels are correctly classified in the positive class; true negatives (TN) are counted when the negative labels are correctly classified in the negative class; false positives (FP) are counted when the negative label is classified as a positive class, and false negatives (FN) are counted when the positive label is classified as a negative class. These four parameters are used to create various performance metrics for the evaluation of segmentation and classification methods in retinal image analysis. Accuracy (ACC) is defined as:

𝐴𝐶𝐶=𝑇𝑃+𝑇𝑁𝑇𝑃+𝐹𝑃+𝐹𝑁+𝑇𝑁
(1)
𝑇𝑃𝑅=𝑇𝑃𝑇𝑃+𝐹𝑁
(2)
𝐹𝑃𝑅=1−𝑇𝑁𝑇𝑁+𝐹𝑃
(3)
𝑃𝑅=𝑇𝑃𝑇𝑃+𝐹𝑃
(4)
𝐹1−𝑆𝑐𝑜𝑟𝑒=2∗𝑃𝑅∗𝑇𝑃𝑅𝑃𝑅+𝑇𝑃𝑅
(5)
Area under ROC curve (AUC) is the plot of true positive rate (TPR) (Eq. 2), i.e., sensitivity (SENS) against false positive rate (FPR) (Eq. 3), i.e., 1-specificity (SPEC). F1-Score is calculated by the harmonic mean of TPR and precision (PR) parameters. Some other parameters like disc similarity coefficient (DSC), overlapping error, and Euclidean distance are also used in the literature. Parameters like peak signal to noise ratio (PSNR) [33] and structural similarity index (SSIM) [34] are used to evaluate denoising algorithms.

𝑃𝑆𝑁𝑅=20log10[𝐾𝑀𝑆𝐸‾‾‾‾‾‾√]
(6)
𝑀𝑆𝐸=1𝑚𝑛∑𝑖=0𝑚−1∑𝑗=0𝑛−1[𝐼𝐷(𝑖,𝑗)−𝐼𝐺𝑇(𝑖,𝑗)]2
(7)
𝑆𝑆𝐼𝑀(𝐼𝐷,𝐼𝐺𝑇)=𝑓[𝑙(𝐼𝐷,𝐼𝐺𝑇),𝑐(𝐼𝐷,𝐼𝐺𝑇),𝑠(𝐼𝐷,𝐼𝐺𝑇)]
(8)
where K is the maximum pixel intensity value and MSE (Eq. 7) is mean squared error between denoised image (𝐼𝐷) and ground truth (𝐼𝐺𝑇) image of size 𝑚×𝑛. SSIM (Eq. 8) is a parameter for quantitative measurement of similarity of denoised image with respect to ground truth image. This similarity measurement is a function of luminance (l), contrast (c), and structure(s).

Table 1 Publicly available datasets for denoising and segmentation of retinal images
Full size table
Table 2 Publicly available datasets for classification of retinal images
Full size table
Retinal image analysis methods
Image pre-processing, segmentation [67,68,69], and classification [19, 70] are general steps followed in the literature for fundus, fluorescein angiography, and OCT images. In this section, state-of-the-art methods implemented using conventional and deep learning-based methods are discussed. The evolution of ocular disease detection with the introduction of AI is also highlighted.

Conventional methods for retinal image analysis
Conventional methods for the detection of ocular diseases are broadly categorized into image processing and machine learning-based methods. Image processing and machine learning methods mainly rely on feature engineering. The machine learning models are trained using the handcrafted features selected based on domain knowledge. Good features are chosen based on the researcher’s prior domain knowledge from technical as well as clinical literature and practicality of implementation. Conventional methods are classified into supervised, semi-supervised, and unsupervised methods. In supervised methods, the ground truth labels are available for segmentation or classification tasks in contrast to unsupervised methods where the models are trained only using input data.

In fundus image segmentation, Awan et al. [71] extracted region-based features from the segmented vascular pattern followed by feature selection. Support Vector Machine (SVM) classifier is used to categorize each region as a true vessel and a false vessel. Staal et al. [46] implemented a method to extract image ridges that approximately coincide with vessel centerlines. Line elements are calculated on the basis of image centerlines and used to extract feature vectors for each pixel. The feature vectors are classified using k-nearest neighbor classifier. Perez et al. [72] implemented a steerable filter for automatic vessel segmentation of ultra-wide fluorescein angiography. The high vessel response in the filtered image is selected in a penalization stage to improve the detection of peripheral vessels in-turn the reduction of false positives. An infinite active contour-based method was proposed by Zhao et al. [73] for segmentation of fluorescein angiography images. The method utilizes hybrid region information like a combination of intensity information which gives accurate feature segmentation and local phase-based enhancement map for preserving vessel edges. Ding et al. [74] decomposed the fluorescein angiography image into multiple resolutions and the vessels are segmented at each scale. The morphological top-hat filter is applied using a total of 9 linear structuring elements. The maximum value of all the images is selected as the final segmented vessels. Multi-scale super-pixel segmentation and feature extraction are used by Tan et al. [75] to segment optic disc and cup of ORIGA dataset. Sarathi et al. [76] implemented a blood vessel in-painting technique combined with a region growing technique to segment the optic disc region of datasets like MESSIDOR and DRIVE.

Conventionally, clinical and image-based feature extraction methods are proposed for glaucoma detection in the literature. Some of the clinical features are cup-to-disc ratio (CDR), inferior superior nasal temporal (ISNT) rule, disk damage likelihood scale (DDLS), and glaucoma risk measurements (GRI). Local binary fitting clustering and spatially weighted fuzzy c-means (SWFCM) segmentation is used by Mittapalli and Kande [77] to calculate CDR for glaucoma classification. Gour and Khanna [78] classified the concatenated pyramid histogram of oriented gradients (PHOG) and GIST features using SVM and ensemble classifiers. A wavelet transform feature extraction and multi-class discriminant analysis method is used for categorizing fundus images into different grades of cataract diseases [79]. Acharya et al. [80] used a discrete wavelet transform (DWT) and discrete cosine transform (DCT) to extract features for DME diagnosis in fundus images. Multiscale amplitude-modulation-frequency-modulation (AM-FM) methods are used for discriminating normal and pathological retinal images [81]. Microaneurysms, exudates, neovascularization, hemorrhages, normal retinal background, and normal vessel patterns are analyzed for automatic diabetic retinopathy screening.

Speckle noise in OCT images is estimated using a wavelet domain-based method [82], where wavelet decomposition of frames for local noise estimation is done. A statistical model [83] using a numerical optimization framework based on maximum a-posteriori estimates denoised the OCT image. Fang et al. [51] proposed a multi-scale sparsity-based method for denoising SD-OCT images. The method uses compressive sensing for the reconstruction of irregularly sampled tomographic data. A sparse dictionary is learned for the representation of OCT images with a high signal-to-noise ratio (SNR) parameter. Chong and Zhu [84] implemented a modified block-matching 3D (BM3D) filter using morlet wavelet composition for OCT denoising. Xu et al. [85] implemented a computationally efficient DWT based method for removal of speckle noise from OCT image. Du et al. [86] implemented a shrinkage filter on wave atoms transform for speckle noise reduction in OCT images. The OCT images contain oscillatory patterns and textures which are traditionally represented using wavelet and curvelet transforms. The wave atoms transform proposed by the authors is a multi-scale geometric analysis method that offers better representation and more sparse expansion in comparison to wavelet and curvelet transforms. Xia et al. [87] also implemented an anisotropic diffusion model for removal of speckle noise in OCT images. The anisotropic diffusion filters are combined with synthetic noise estimation criteria based on local noise estimator and distance median value for noise reduction in phase OCT images. The method accurately identifies the noise and signal pixels and diffuses them with different coefficients to achieve efficient noise reduction.

Hussain et al. [88] used features extracted from segmented retinal layers and changes manifested due to abnormalities. The calculated features are classified using random forest classifier into AMD, DME, and Normal classes. AA local binary pattern-based method was proposed by Lemaitre et al. [89] in combination with the bag of word technique for classification using the SVM classifier. Srinivasan et al. [62] proposed an image processing and machine learning-based method for the classification of SD-OCT images into AMD, DME, and normal classes. The OCT images are first denoised using BM3D methods and the retinal curvature is flattened by calculating the convex-hull of the retinal pigment epithelium (RPE) layer. Venhuizen et al. [90] extracted interest points by thresholding top 3% values of first-order vertical Gaussian gradient of OCT images of Duke dataset [62]. Principal component analysis (PCA) is used to reduce the dimensionality of the features. Unsupervised learning is used to classify the final feature vector into disease class labels.

Road from conventional to deep learning methods
The conventional methods are light models which need limited resources and small dataset size. The construction of CAD models relies on the extraction of handcrafted features to train the models. The handcrafted features are extracted by considering color, texture, and shape changes in the retinal images. A limited number of retinal images contribute largely to the selection of appropriate features. Although conventional methods have these advantages, the performance achieved through them is not state-of-the-art. These methods cannot be deployed in the real-life clinical setup. Human intervention to select relevant handcrafted features needs the special skill of researchers and inputs from clinical experts.

Deep learning methods have replaced conventional image processing techniques and exhibited superior performance in recent years. In comparison to conventional methods for ocular disease detection, deep learning methods have many advantages. The biggest advantage of the implementation of deep learning methods is the elimination of the need for feature engineering. In conventional methods, the features are handcrafted, which require expert domain knowledge about a certain problem. Deep learning methods automatically extract features that correlate with the problem and combine them for faster learning. In conventional approaches, the problem is broken down into different steps to achieve the goal of disease diagnosis. In contrast, the deep learning techniques automated the process of learning and provided an end-to-end solution to a problem. The ability of deep learning methods to process high-dimensional data is another advantage. This leads to the development of automated systems with the ability to deliver high-quality results in comparison to conventional methods. The performance achieved through deep learning-based methods is significantly higher in comparison to conventional methods. These merits of deep learning can lead to transformational outcomes in the healthcare sector [91].

Deep learning methods
Various convolutional neural networks (CNNs) and other deep learning architectures are developed for fundus, fluorescein angiography, and OCT images for pre-processing, segmentation, and classification. Deep learning-based methods are mainly categorized on the way the models are trained for a particular task. Deep learning models are trained form the scratch or transfer learning-based methods are mainly used for retinal image analysis.

Training from scratch The recent development in image acquisition techniques for retinal images paved way for the construction of larger datasets. The deep learning architectures are trained on large datasets for retinal image analysis tasks. The method of learning the training parameters with random initialization is training from scratch. Architectures like autoencoder, Generative adversarial networks (GANs), and CNN are trained from the scratch for segmentation, synthetic image generation, and classification respectively. Hybrid architectures are a combination of deep learning-based models or conventional and deep learning models. These hybrid architectures made it possible to implement deep learning models for retinal image data.

Transfer learning During the introduction of AI in computer vision AlexNet model gave a state-of-the-art performance in object detection on the ImageNet dataset. The error reported by the deep learning-based method was low in comparison to methods based on conventional techniques. The pre-trained CNN models are fine-tuned on retinal image data using the transfer learning concept. Transfer learning provides an efficient solution when the datasets available for retinal image analysis are smaller.

In the next subsections, methods including both training from scratch and transfer learning for retinal image enhancement, segmentation, classification are reviewed for fundus, fluorescein angiography, and OCT image modalities. This work includes some of the methods that produced a state-of-the-art performance and portray the development of the field of AI in retinal image analysis.

Fundus image segmentation
The anatomical structures of fundus images are mainly segmented using CNN for pixel-wise classification and autoencoder-based U-Net architectures. The blood vessels and optic disc segmentation methods are explored in [92]. Various methods based on U-Net architecture are proposed for the segmentation of various retinal parts. Liskowski et al. [93] implemented a patch-based CNN architecture for the classification of fundus image pixels into a vessel or non-vessel pixels. The fundus images of DRIVE, STARE, and CHASE datasets are converted into patches of size 3×27×27. Li et al. [94] proposed a U-Net-like autoencoder architecture for full patch prediction and reconstruction. Yan et al. [95] implemented a U-Net like autoencoder with the combination of segment-level and pixel-wise losses for segmentation of blood vessels of DRIVE, STARE, and CHASE datasets. Synthetic fundus images are constructed using a recurrent neural network-based GAN (R-sGAN) technique which looks realistic while maintaining vessel structure. This method attempts to solve the dataset size issue. The synthesized images are further segmented using methods like the deep retinal image understanding (DRIU) [96] model which is a transfer learning (TL) model based on VGG architecture. Akram et al. [97] proposed another adversarial learning model with multiscale features and kernel factorization. Zhao et al. [98] focused on the segmentation of retinal vessels using dissimilar datasets like DRIVE, STARE, HRF, Kaggle, and a mobile fundus imaging dataset. Such methods can be used to resolve the issue of variation in reference standards in retinal image analysis.

Various deep learning techniques are also proposed for optic disc segmentation required in glaucoma detection. Fu et al. [99] proposed a multi-scale U-net architecture for the segmentation of optic disc and cup simultaneously with a multi-label loss function. A novel method called DeepDisc [100] was implemented to segment optic disc using atrous convolution and spatial pyramid pooling. The method is evaluated on ORIGA and MESSIDOR datasets. A disc aware ensemble network [101] for glaucoma screening was proposed where a combination of four different networks is used for the segmentation of optic disc. The combination of four different networks is used to combine the results of multiple architectures assuming insights given by a single network is not enough. Al-Bander et al. [102] proposed a deep multiscale CNN architecture for the simultaneous segmentation of optic disc and fovea of fundus images. The method is evaluated on the publicly available MESSIDOR and Kaggle datasets. A combination of Yolo and U-Net architectures (UOLO) is implemented by Araújo et al. [103] on three publicly available datasets MESSIDOR, IDRiD, and DRIVE. These datasets are small, and the results should be evaluated both inter and intra-dataset-wise. The results reported by other methods give no such analysis. Huang et al. [104] implemented a region proposal network to generate multiple optic disc regions. A region of interest containing fovea is applied to three-level cascaded CNN to locate the fovea. The experimental analysis of some of the fundus image segmentation approaches is summarized in Table 3.

Fundus image classification
Graham et al. [105] proposed the classification of fundus images using sparse convolutional neural network (SparseConvNet) during a global competition, Diabetic Retinopathy Detection Challenge”, hosted by “Kaggle” in 2015. They achieved a final Kappa score of 0.84958. Gulshan et al. [106] in Google AI developed a deep learning-based algorithm for the detection and grading of diabetic retinopathy. Grader variability and the importance of reference standards for evaluating machine learning models for diabetic retinopathy are also discussed by them [107]. Gangwar and Ravi [108] used transfer learning on pre-trained Inception-ResNet-v2 and evaluated on MESSIDOR-1 and Kaggle dataset. They achieved a test accuracy of 72.33% and 82.18% on MESSIDOR-1 and Kaggle datasets, respectively. Qummar et al. [109] used up sampling and down sampling approaches for class imbalance and transfer learning on five CNN architectures (Resnet50, Inceptionv3, Xception, Dense121, and Dense169) for a small Kaggle dataset to classify fundus images into various diabetic retinopathy classes. Diaz-Pinto et al. [110] classified fundus image datasets using pre-trained networks like VGG16, VGG19, InceptionV3, ResNet50, and Xception for glaucoma classification. Another transfer learning-based model was implemented by Joshi et al. [111] using YOLO-v3 model which uses DarkNet-53 architecture for classification of fundus images into glaucoma and normal category. A combination of three different datasets is used for training and testing purposes. Li et al. [112] proposed an attention-based CNN for glaucoma detection (AG-CNN) on large-scale attention-based glaucoma (LAG) private dataset and RIM-ONE public dataset. Muhammad et al. [113] sampled fundus images using stratified sampling to ensure equal distribution of glaucoma and normal class for optic disc segmentation and classification using regions with CNN (RCNN) architecture.

Recently, a multi-label fundus dataset was released through a global grand challenge named international competition on ocular disease intelligent recognition (ODIR). It comprises left and right eye images of a patient with eight ocular disease classes. The patients can belong to more than one disease class. Islam et al. [114] implemented a shallow CNN architecture for the ODIR dataset by training from scratch on fundus images of ODIR dataset. A similar model was proposed by Jordi et al. [115] where two pre-trained architectures VGG16 and InceptionV3 are fine-tuned for the classification of fundus images of the ODIR dataset. Li et al. [116] proposed a transfer learning-based approach using ResNet architecture and implemented a dense correlation network (DCNet) to exploit the dense spatial correlations between the pair of color fundus images. The experimental analysis of some of the fundus image classification approaches is summarized in Table 4.

Fluorescein angiography segmentation and classification
Pan et al. [60] proposed a transfer learning-based method for multi-label classification of fluorescein angiography images using DenseNet, ResNet50, and VGG16 networks. The private dataset used in this approach has a class-imbalance problem that is not addressed by the authors. Jin et al. [117] used a subset of the same private dataset for the segmentation of abnormalities like non-perfusion regions, microaneurysm, and leakage using U-net architecture. Li et al. [61] proposed an automated quality assessment and image selection of ultra-widefield fluorescein angiography images through deep learning. The images are categorized into poor, good, best, and ungradable categories for quality assessment. The method was implemented on a private dataset for both unbalanced and balanced training datasets. The fluorescein angiography images are classified into quality assessment categories and the performance of the balanced model is compared with the unbalanced model. A CycleGAN is implemented by Schiffers et al. [50] to synthesize fluorescein angiography images using fundus images. As angiographic imaging poses risks for the eyes of the patient, the objective of this method is to generate synthetic fluorescein angiography images. This also solves the problem of dataset size and class imbalance for the retinal images especially rarely captured modality like fluorescein angiography. A similar method implemented by Li et al. [26], generates cross-modality fluorescein angiography training data by aligning vessel maps from fundus images. The ground truth generated in this process is matched with the available ground truth to tune the performance. Finally, ground truth for unlabeled fluorescein angiography is generated using neural network architectures like U-Net and GAN. Experimental analysis of some of the fluorescein angiography segmentation and classification approaches is summarized in Tables 3 and 4, respectively.

OCT image denoising
Chen et al. [118] proposed a GAN for speckle noise removal while preserving the texture details. The method was evaluated on 36 images of a private dataset. A CNN-based method [119] uses residual learning, shortcut connection, batch normalization, and leaky rectified linear units to achieve good de-speckling performance on private datasets. Qui et al. [120] proposed an end-to-end CNN architecture with a perceptually sensitive loss function for the OCT denoising. The CNN is combined with SSIM loss function for removal of speckle-noise from the OCT images of a private dataset. Gour and Khanna [121] proposed an efficient OCT denoising technique using residual convolutional neural network on two publicly available datasets namely, Duke and Topcon. Experimental analysis of some of the OCT image denoising approaches is summarized in Table 3.

OCT image segmentation
Segmentation of retinal layers in OCT images is important for the detection of various abnormalities. Mishra et al. [122] implemented a deep learning method combined with the shortest path (DL-SP) algorithm to segment 11 major retinal layers in SD-OCT images. Fully convolutional networks (FCN) take tensor-like input and produce pixel-wise or voxel-wise estimates for a particular class. This network is used in a DenseNet-FCN with 103 layers for the segmentation of retinal layers [123]. Recently, Ngo et al. [124] proposed an automated layer segmentation method based on the feature-learning regression network. Intensity, gradient, and adaptive normalized intensity score (ANIS) features are used for the prediction of retinal layer boundaries. The experimental analysis of some of the OCT image classification approaches is summarized in Table 3.

OCT Image Classification
Zhang et al. [63] first presented a transfer learning-based method using Inception-V3 architecture. Li et al. [125] also proposed a VGG16 transfer learning-based method. A layer-guided CNN was implemented by Huang et al. [126] for the classification of OCT images. The proposed method is a lightweight CNN having a smaller number of training parameters in comparison to the method given by Zhang et al. [63]. Rasti et al. [127] implemented a wavelet-based CNN and Rong et al. [128] proposed a surrogate specialized CNN architecture for the classification of OCT images of the Duke dataset. Tsuji et al. [129] gave a capsule network-based architecture where the capsules are groups of neurons representing different properties of the same object. The proposed method is evaluated on the ZhangLab dataset. The experimental analysis of some of the OCT image classification approaches is summarized in Table 4.

CAD systems for ocular diseases
Various AI-based CAD systems are developed worldwide for diagnosis of ocular diseases [8]. Odaibo et al. [130] acquired a US patent for a cloud-based AI platform named “Retina-AI” for analysis of retinal images of various modalities, including fundus, OCT, fluorescein angiography, etc. Although it is a multi-modality CAD system, Retina-AI does not have integrated image acquisition devices. The retinal images are classified using weighted-ensemble supervised-learning methods according to disease type, state, and stage. The supervised models are ranked according to their performance on the testing dataset, and weights are calculated based on this ranking. These ranked-based weights are used to compute class scores in weighted-ensemble models for retinal image classification. Another cloud-based system was developed in collaboration between India-based Sankara Eye Foundation and Singapore-based Leben Care [131]. The software named Netra.AI is also integrated with existing fundus image cameras for the analysis of images in the presence of ocular diseases like diabetic retinopathy, glaucoma, and AMD. Image quality assessment, lesion annotation, and blood vessel segmentation are some of the operations included in Netra-AI software. The system is not intended to diagnose any retinal disease but to be used as an ancillary tool by qualified eye care professionals. Pegasus-disc software [132] by Visulytix Ltd, London, proposed an AI-based support tool for the analysis of retinal images. It has shown performance similar to the opinion of two human experts for detection of glaucoma using optic disc from fundus images. Medios AI is a clinically validated software available in the market which runs on Remidio Non-Mydriatic Fundus On Phone for detection of referable diabetic retinopathy (RDR) lesions in captured images [133]. Most of the CAD systems do not contain integrated image acquisition, but Medios AI analyses the fundus images captured by the attached smartphone. The software is specifically deployed for the detection of diabetic retinopathy and does not involve other ocular diseases. Healthvisors made available a diagnostic support algorithm for automated early diagnosis of diabetic retinopathy in early stages of pathology [134]. The software named IDx-DR is standardized on multi-ethnic Hispanic and African American patients. The software includes image quality assessment and diagnosis modules for diabetic retinopathy. IDx-DR is the first software approved by the United States Food and Drug Administration. Apart from these CAD systems, various screening programs are deployed globally for the detection of diabetic retinopathy using AI [135]. Singapore implemented a national-level telemedicine-based screening program named Singapore integrated diabetic retinopathy program (SiDRP) for screening of diabetic retinopathy from fundus images [136]. National health screening (NHS) diabetic eye screening programme was commenced in the UK in 2003 for systematic population screening of diabetic retinopathy [137]. A Scottish software (iGradingM) was developed under NHS to evaluate fundus images with 45∘ FOV for diabetic retinopathy screening [138].

Technical and clinical challenges in deep learning-based retinal image analysis
Conventional trained models generalize well for the given data if appropriate features are selected for the task. The features which are thought appropriate by researchers may not be always suitable and fail to capture important characteristics of data. AI-based models which automatically learn features are more desirable in this case [139]. AI-based ocular disease identification systems present good results in comparison to conventional methods. The previous section provides an exhaustive insight into the existing state-of-the-art in retinal image analysis. The methods presented in the literature clearly indicate that deep learning-based approaches improved the performance for fundus and OCT imaging-based CAD systems and their sub-modules. Deep learning models depend upon hyper-parameters and datasets which must be extensively validated. In the real-life scenario, various existing issues that hamper the performance of CAD systems are yet to be addressed. This section discusses real-life technical and clinical challenges related to AI in ocular disease identification.

Retinal image datasets and related issues
Medical image data acquisition is a fundamental prerequisite in the construction of good deep learning-based models. Heavy CNNs and auto-encoders require a large amount of data to achieve proper generalization. The quantity of data depends upon the type of task for which the models are implemented. The segmentation of blood vessels and optic disc does not require a large number of images. The features on and around the desired region of interest are quite distinctive and a single image can provide a lot of information for the segmentation task. On the other hand, the classification task depends upon the identification of various abnormalities present in the image. The region of interest having abnormalities usually covers a small area in the image. To generalize a model for the classification task, deep learning models require a large amount of image data. Unavailability of enough data may lead to model over-fitting. Although retinal image data has increased in recent years due to the development of better acquisition devices, still enough data is not available. A large number of fundus and OCT image datasets are publicly available online for different applications of retinal image analysis (Tables 1, 2).

Dataset size
The scarcity of retinal image data can be seen in some of the classification datasets which are published in the literature as summarized in Table 2a, b, and c. The image processing-based techniques for segmentation require a limited amount of data whereas deep learning methods with better performance generally require a large dataset. Datasets like STARE, INSPIRE-AVR4, ONHSD, IDRiD, REVIEW, HRF, DRIVE, and CHASE_DB1 for segmentation are not suitable for training a deep learning model from scratch. To combat this issue, researchers use pre-trained models trained on image data of other domains and fine-tune them on retinal image data. The pre-trained models are used as feature extractors based on the assumption that primary layers of the models give fundamental features that are relevant irrespective of the domain. Another alternative approach implemented by researchers is the generation of synthetic retinal image data [140]. GANs and autoencoders are used to synthesize synthetic retinal images. The original data are combined with a synthetic dataset to improve the performance of the training models [141]. The transfer learning, GAN, and autoencoder-based techniques for retinal image analysis are reported in this work.

Dataset quality
Another important aspect of working with image data is maintaining its quality and uniformity throughout the dataset. The retinal dataset is constructed by collecting images from different hospitals and clinics. It is difficult to ensure the quality of the captured retinal image to be similar to the actual retina. The fundus or OCT cameras may fail to capture important features responsible for disease identification. The images are also of different resolution and angles. In the ODIR dataset (as shown in Fig. 7), the fundus images of different patients are of heterogeneous sizes. The models trained on such data may fail in categorizing images belonging to the same class. ODIR dataset also contains out-of-focus blurred images and has artifacts that interfere with the training images as shown in Fig. 7. These images do not contribute or play a decisive role in determining a patient’s disease. Data collected from patients of different ethnicity may also differ in appearance and may lead to bias towards a particular ethnicity.

Fig. 7
figure 7
Fundus images of ODIR dataset with artifacts or incorrect capture and having varying dimension: a low image quality (2592 × 1728), b optic disc photographically invisible (2736 × 1824), c lens dust (1936 × 1296), and d image offset (3456 × 2304)

Full size image
Dataset heterogeneity
Another challenge in AI models trained on various modalities of the retina is heterogeneity in the images of the datasets used. The fundus images captured using different cameras may vary in terms of resolution and FOV. The datasets can also be heterogeneous in terms of the age, gender, and ethnicity of the patients. According to studies, the fundus images are comparatively lightly pigmented in the case of Caucasian patients as compared to African or Asian patients [142, 143]. The neural networks are highly sensitive to the image details/variations and dataset heterogeneity might lead to false learning. Ting et al. [143] proposed a deep learning model for diabetic retinopathy from multi-ethnic populations. The initial training was done on the fundus images of patients from Singapore. The trained model is validated on fundus images from 10 different multi-ethnic cohorts, namely, Malay, Indian, Chinese, African American, Australia, Mexican, Hong Kong, and Guangdong. Lee et al. [144] analyzed the variations in optic disc size in fundus images. The fundus images of patients of Caucasian, Chinese, Filipino, African, and Hispanic ethnicity are collected, and the general size of the optic disc was calculated. According to this study, African, Hispanic, Filipino, and Chinese patients have a similar-sized optic disc in comparison to Caucasian patients with a relatively smaller optic disc. These types of differences should be taken into consideration in the development of diagnostic systems for the diagnosis of diabetic retinopathy and glaucoma. Yang et al. [145] implemented a deep learning-based model for ethnicity detection for Chinese, Malay, and Indian patients. Bhatia et al. [146] evaluated Pegasus-OCT, a commercially available AI-based system for the detection of ocular diseases using OCT images. The evaluation is done for individual datasets of five countries, namely, USA, China, Iran, Greece, and the UK.

Dataset class imbalance
Computer vision usually deals with the problem of class imbalance which is quite prevalent in the field of medical image processing. An imbalanced dataset typically has the number of samples of one class significantly higher than the number of samples of the rest of the classes. As mentioned earlier in this section, a large retinal dataset is important for a good deep learning model. In the case of an imbalanced dataset even if the overall sample size is large enough, the trained model may become biased towards the majority class. In real life scenario, normal patients treated by ophthalmologists are significantly higher than diseased. This creates an insufficient number of diseased samples in retinal datasets in comparison to normal image samples. Various methods are implemented by researchers to address the class imbalance issue. The majority class is under-sampled or the minority class is oversampled to create a well-balanced training set [147].

In this work, the prevalence of class imbalance in retinal datasets available in the literature is depicted through histogram as shown in Fig. 8a and b. The most over-represented and under-represented classes of the datasets are selected and their deviation from the ideal average sample size is calculated using the following formula:

𝑆𝑎𝑣𝑔=𝑆𝑡𝑜𝑡𝑎𝑙#𝐶
(9)
𝐶𝑜𝑣𝑒𝑟=#𝐶𝑜𝑣𝑒𝑟𝑆𝑎𝑣𝑔
(10)
𝐶𝑢𝑛𝑑𝑒𝑟=#𝐶𝑢𝑛𝑑𝑒𝑟𝑆𝑎𝑣𝑔
(11)
where, 𝑆𝑎𝑣𝑔 is desired number of images each class for balanced dataset, 𝑆𝑡𝑜𝑡𝑎𝑙 total number of images in the dataset, and #𝐶 is the number of classes. In an ideal balanced dataset, all classes must have equal number of samples. Prevalence of imbalance in the retinal datasets can be seen through deviation of the most over-represented class, 𝐶𝑜𝑣𝑒𝑟 (Eq. 10)) and most under represented class, 𝐶𝑢𝑛𝑑𝑒𝑟 (Eq. 11)). The plotted histograms for over-represented and under-represented classes for fundus, fluorescein angiography, and OCT image classification problem are shown in Fig. 8a, b, and c respectively.

In fundus images, HRF dataset has well-balanced images whereas RIM-ONE dataset is having the highest imbalance. In the case of fluorescein angiography datasets, the dataset used by Li et al. [61] has a high-class imbalance whereas Hajeb et al. [49] used a well-balanced dataset. However, the well-balanced datasets have the disadvantage of less number of samples for each class. For OCT images, ZhangLab and EUDENDA datasets are having a high-class imbalance.

Fig. 8
figure 8
Prevalence of class imbalance in retinal image datasets for ocular disease classification

Full size image
Model generalizability
Conventional methods are handcrafted for a task and may not perform well for other similar tasks of retinal image analysis. For example, a model trained for the classification of fundus images into glaucoma and normal images may not generalize well for fundus images of glaucoma taken in different acquisition settings or diabetic retinopathy classification problems. Due to the development of efficient processors with parallel computing, availability of medical data, and advances in neural network architectures in reducing training time and complexity, the use of deep learning significantly improved the performance of retinal CAD systems. However, generalizability on unseen data is a practical concern in deep learning methods too. The medical datasets reflect true distribution in the real-life clinical setup. Deep learning methods are stochastic. The trained model may become biased towards the minority class. GANs are used to synthesize retinal images to balance the distribution of classes. Mode collapse is a phenomenon in GAN where the trained network focuses on a particular subset of data. Minibatch discrimination [148] is used for GAN to solve the problem of mode collapse. The discriminator is trained such that it focuses on image examples in combination and avoids generator collapse. The problem of generalizability is also mitigated by training multiple models for all classes or different models for each class and combining their performance. Implementing such ensemble models improves the overall performance of the model.

Challenges in real-life clinical setup
AI methods are widely accepted in various fields of computer vision but not completely adopted in real-life clinical setup for retinal image analysis. The ophthalmological experts have apprehension towards deep learning models due to lack of interpretability, difference in reference standards, model deployment issues in real life clinical setup which are discussed in this section.

Variation in classification systems and reference standards
The first concern is variation in ocular diseases and their reference standards used for diagnosis in different countries. The ground truth labels are constructed by retinal specialists and can vary for a different dataset. The AI models trained on different datasets related to the same disease learn differently and such models cannot be standardized for clinical setup. Even for the same dataset, different specialists have a disagreement in the correct ground truth for segmentation and classification. The usual approach adopted by researchers is majority voting for classification using available ground truths by different specialists.

Deep learning model acceptance in clinical setup
The deep learning-based model works on the principle of automatically learning representations from retinal images for various tasks mentioned earlier. The end-to-end deep learning models can be seen like a “black box” and it is difficult to interpret its inner workings [149]. In a real-life clinical setup, medical experts are reluctant to accept the performance produced by deep learning models in the absence of proper interpretability. Various solutions are proposed in the literature to visualize the deep learning models like the construction of saliency maps and class activation maps [150]. The image regions which contribute most significantly to the final classification are highlighted by these methods.

CAD systems deployment in clinical setup
Deep neural networks need high computational resources for training and testing. The multiple ensemble models are more extensive than the single deep learning model. In the clinical setting, patients are tested for multiple ocular diseases and require different models. The time taken to analyze each image for different models increases even when computers with GPUs are available. Due to privacy and security concerns in the clinics and hospitals, online servers cannot be used and need independent systems installed. Deployment in such conditions can be difficult, costly, and infeasible in developing countries and rural areas. Various existing CAD systems, as mentioned in Sect. 5 are deployed in real-life clinical conditions but have multiple limitations. The systems are yet to be developed, having an integrated image acquisition module capable of screening multiple ocular diseases. The CAD systems designed in a developed countries might be expensive to deploy in developing countries.

Table 3 Experimental observations for pre-processing and segmentation of different retinal image modalities
Full size table
Table 4 Experimental observations for classification of different retinal image modalities
Full size table
Conclusion
In recent years, retinal image analysis is improved greatly due to the introduction of AI methods in the field of biomedical data processing. The performance of CAD systems has improved and state-of-the-art performance is achieved. This review discusses the methods which produced high-performance and efficient solutions in ocular disease identification. Three retinal imaging modalities, i.e., fundus, fluorescein angiography, and OCT are used to demonstrate the effect of AI in ocular disease identification. Although AI promises better performance, it raises some concerns too. This review mainly raises concerns in two categories, namely retinal datasets related and real-life clinical setup related. Some of the recent methods which attempt to solve these issues are summarized and analyzed for different problems of ocular disease identification. The review showed that even though a lot of research has been done in developing methods for ocular disease identification, some issues are yet to be addressed. The future work includes the development of efficient retinal disease identification systems while addressing the issues discussed in this paper. Retinal image segmentation and classification methods are supposed to be developed by addressing issues like class imbalance, model generalizability, model interpretability, and real-life clinical challenges. The future work also includes exploration of technologies like IoT and blockchain for smart and remote healthcare monitoring [153, 154] in the field of retinal image analysis.

Keywords
Ocular diseases
Artificial intelligence
Retinal datasets
Deep learning