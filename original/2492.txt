In this paper, a hybrid cooperative differential evolution with the perturbation of the Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) with the local search of Limited-Memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (LBFGS) mechanism, named jSO_CMA-ES_LBFGS, is proposed to solve the complex continuous problems. In the proposed algorithm, jSO, as a variant of Differential Evolution (DE), is used as a global search operator to explore the entire solution space. When the population falls into stagnation, a relatively reliable initial solution for the local search operator is generated by the CMA-ES, which is activated to perturb the optimal candidates in the solution space. The LBFGS utilized as the local search strategy is embedded in CMA-ES to obtain the potential local optimal solutions. A cooperative co-evolutionary dynamic system is formed by jSO and CMA-ES with a local search operator. The proposed jSO_CMA-ES_LBFGS is tested on the CEC2017 benchmark test suite and compared with eleven state-of-the-art algorithms. Further, two practical engineering problems are investigated utilizing the proposed method. The experimental results reveal the effectiveness and efficiency of the jSO_CMA-ES_LBFGS.

Introduction
As an important branch of artificial intelligence, evolutionary computation, which has the characteristics of self-adaptation, self-organization, and self-learning, is a kind of model and algorithm for simulating the behavior of â€œthe survival of the fittestâ€ in natural biological and ecological systems, and the algorithm is used to solving various complicated problems [1, 2]. The continuous optimization problem from the real world is an important application field of evolutionary computation [3, 4]. It has been an important issue and has been widely studied in recent years. Classical evolutionary algorithms mainly include genetic algorithm (GA) [5], differential evolution [6], artificial bee colony (ABC) [7], particle swarm optimization (PSO) [8], whale optimization algorithm (WOA) [9], and the other typical hybrid evolutionary computation algorithms.

The DE is proposed to find the global optimum solution for complex optimization problems [10]. The basic idea of DE stems from Darwin's theory of biological evolution. Owing to the efficiency and effectiveness of solving optimization problems, DE is applied to solve numerous real-world problems from diverse domains of engineering optimization, e.g., electrical and power systems [11], manufacturing science, and operation research [12, 13]. It is difficult to accurately classify the methods of DE since certain variants of DE are combinations of multiple mechanisms. The modifications of DE are roughly divided into three categories, and the details are shown as follows.

(i) DE with certain strategies.

A variant of DE with multi-population and mutation strategies (EDEV) is proposed by Wu et al. [14], which utilized the combination of different evolution strategies and different subpopulations to produce the next generation solution in different evolution stages. The surrogate is embedded as a strategy in DE to predict the optimal solution and guide the search direction of differential evolution [15, 16]. The experimental results verify that the surrogate helps speed up the convergence of DE. In [17], the stochastic mutation strategy and information sharing mechanism are introduced to enhance the search capability of DE. In addition, incorporating a local search heuristic is often useful in enhancing the search performance of DE [18].

(ii) DE with Adaptation of the Parameters.

The performance of DE depends on its control parameters and search operators. In DE, the values of two parameters are automatically configured and only one mutation strategy is used throughout the evolution process [19]. The evolution process of the algorithm is recognized as a closed-loop control system. In [20], two sinusoidal waves are used to adapt to the scaling factor. A performance adaptation scheme based on earlier success is used to choose one of the sinusoidal waves. Moreover, the covariance matrix learning strategy is used for the crossover operator to solve problems with a high correlation between the variables. In [21], a parameter with the adaptive learning mechanism (PALM) is introduced to tackle the ill interaction of control parameters in certain DE variants, and a timestamp mechanism is employed to update inferior solutions in the external archive.

(iii) Hybridization of DE with other algorithms.

In the domain of optimization, hybridization refers to the combination of different algorithms to achieve the effect of "chemical reaction" according to the optimization characteristics of the algorithm, rather than the simple flattening of the different operating mechanisms of the algorithms. In [22], the combination of ABC and DE, which is named HABCDE, is proposed to develop a more effective algorithm than ABC and DE in terms of convergence and explorationâ€“exploitation. Moreover, in [23], DE is used as a local search process in the moth search algorithm to enhance local searchability. The experimental performance analysis confirms that the hybrid mechanism is effective for solving practical optimization problems. In [24], a hybrid system of DE and bat evolution algorithm (BA), in which DE and BA are in a competitive relationship, is introduced to balance the exploration and exploitation capabilities of BA.

According to the literature, since Zhang and Sanderson [25] proposed a variant of DE by implementing the mutation strategy â€œDE/currentâˆ’toâˆ’pbestâ€ with the external archive ğ´ğ´ to improve optimization performance (JADE), there has been a variety of researches focusing on adaptation of the control parameters of DE. The feedback from the process of evolutionary search is employed to dynamically tune the value of control parameters. By adopting certain self-learning mechanism, the control parameters of DE are automatically adjusted to appropriate values. An improved JADE, which is named as SHADE [22], is proposed to improve the robustness of JADE.

A hybridization framework of LSHADE-SPACMA based on the LSHADE and CMA-ES is introduced to improve the optimization performance of L-SHADE algorithm [26]. DE as a local search operator is introduced into the Grey Wolf Optimizer (GWO) to improve the exploitation of the GWO (COGWO2D) [27]. The experimental results proved that COGWO2D is effective to find the optimal solutions of the global optimization problem. Adaptive neighborhood operators are borrowed from DE to promote exploration and exploitation of Migrating Birds Optimization (MBO) [28].

In summary, DE has been extensively studied on the operating mechanism of the algorithm regarding whether it is a single DE algorithm or a hybrid DE algorithm in recent years. However, DE tends to trap into the local optima and lose the diversity of the population in the evolution process. First, the size of the external archive is fixed in the classic evolution strategy of DE. As the evolution of the population progresses, the solutions in the external archive are constantly replaced by updated candidates with optimal fitness. The population falls into local optimal solutions, the external archive stores various suboptimal solutions rather than worse solutions. In other words, the perturbation strength is insufficient currently. Second, according to the evolutionary state of the population, the strength of the disturbance is automatically adjusted rather than a pre-determined single value. Third, to enhance the local search ability of the hybrid algorithm for DE, the population of DE will be degraded by injecting results of local optimization.

Inspired by the cooperative co-evolutionary framework, a hybrid cooperative jSO [19] (a optimization algorithm for solving single-objective real parameter problems) with the perturbation of CMA-ES with a local search of LBFGS, named jSO_CMA-ES_LBFGS, is proposed to solve the single objective numerical optimization problems. In the jSO_CMA-ES_LBFGS, the jSO provides a reliable initial solution for CMA-ES with LBFGS, and CMA-ES with LBFGS provides a potentially local optimal solution for jSO, which makes jSO have a wide range of learning capabilities. Therefore, a cooperative co-evolution between jSO and CMA-ES with LBFGS is achieved. The contributions of this paper are generalized as follows.

(1)
In this paper, a hybrid cooperative differential evolution algorithm is designed to solve the single-objective real-parameter numerical optimization problem, which effectively combines evolutionary algorithm (jSO), an evolutionary strategy (CMA-ES), and mathematical method (LBFGS). A de-randomized and anti-rotation CMA-ES [29] is used as a perturbation operator to provide a relatively reliable initial solution for the local search operator (LBFGS).

(2)
The Markov model of jSO_CMA-ES_LBFGS is presented to analyze the convergence performance of the jSO_CMA-ES_LBFGS mathematically. The evolutionary process of the jSO_CMA-ES_LBFGS is mapped to the state transition process in the Markov model, and then the convergence performance of the jSO_CMA-ES_LBFGS is proved.

(3)
The proposed jSO_CMA-ES_LBFGS is tested on CEC 2017 benchmarks and compared with the seven variants of DE and other evolutionary algorithms. The experimental results imply that the jSO_CMA-ES_LBFGS obtain more efficient and accurate solutions than that of the other algorithms. Furthermore, a desirable combination of parameters is also analyzed by executing the Taguchi method [30].

The remainder of the paper is organized as follows. The framework of the original DE and CMA-ES is summarized in Sect. 2. The proposed methodology is described and analyzed in Sect. 3. The experimental research and statistical analysis are provided in Sect. 4. In Sect. 5, two practical engineering problems including continuous and discrete decision space are investigated using the proposed jSO_CMA-ES_LBFGS. Finally, the conclusions and future work are presented in Sect. 6.

Basic algorithm and operators
In this section, jSO and CMA-ES are briefly introduced. The notation is described as follows.

jSO algorithm
The basic operations of the DE include mutation, crossover, and selection. When the population of DE is initialized, three basic operations are repeated until a certain termination criterion (e.g., exhaustion of maximum functional evaluations) is satisfied. In the DE, the population is a set of real value vectors ğ‘¥ğ‘–=(ğ‘¥1,â€¦,ğ‘¥ğ·), ğ‘–=1,â€¦,ğ‘ğ‘ƒ, where ğ· is the dimension of the objective function, and ğ‘ğ‘ƒ is the size of the population. The population is initialized randomly.

The mutation operator:

ğ‘£ğ‘–,ğ‘”=ğ‘¥ğ‘–,ğ‘”+ğ¹ğ‘¤(ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”âˆ’ğ‘¥ğ‘–,ğ‘”)+ğ¹(ğ‘¥ğ‘Ÿ1,ğ‘”âˆ’ğ‘¥ğ‘Ÿ2,ğ‘”)
(1)
where ğ¹ is scale factor and ğ¹ğ‘¤ is calculated as follows:

ğ¹ğ‘¤=â§â©â¨âªâª0.7âˆ—ğ¹,ğ‘›ğ‘“ğ‘’ğ‘ <0.2ğ‘€ğ‘ğ‘¥_ğ¹ğ¸ğ‘ ,0.8âˆ—ğ¹,ğ‘›ğ‘“ğ‘’ğ‘ <0.4ğ‘€ğ‘ğ‘¥_ğ¹ğ¸ğ‘ ,1.2âˆ—ğ¹,ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’
(2)
The indexes ğ‘Ÿ1,ğ‘Ÿ2 are chosen from [1:ğ‘+ğ‘ğ‘ƒ] randomly, where ğ‘ is the size of the external archive ğ´ğ´ and ğ‘Ÿ1â‰ ğ‘Ÿ2â‰ ğ‘–. ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘” are the dominant individuals in the population. The parameter ğ¹ğœ–[0:1] is the scaling factor of the i-th individual, which controls the size of the difference term. If the donor vector goes beyond the feasible region of the problem, the following operation is performed:

ğ‘£ğ‘–,ğ‘—,ğ‘”={(ğ‘¥ğ‘šğ‘–ğ‘›ğ‘—+ğ‘¥ğ‘–,ğ‘—,ğ‘”)/2ğ‘–ğ‘“ğ‘£ğ‘–,ğ‘—,ğ‘”<ğ‘¥ğ‘šğ‘–ğ‘›ğ‘—(ğ‘¥ğ‘šğ‘ğ‘¥ğ‘—+ğ‘¥ğ‘–,ğ‘—,ğ‘”)/2ğ‘–ğ‘“ğ‘£ğ‘–,ğ‘—,ğ‘”<ğ‘¥ğ‘šğ‘ğ‘¥ğ‘—
(3)
The crossover operator:

ğœ‡ğ‘–,ğ‘—,ğ‘”={ğ‘£ğ‘–,ğ‘—,ğ‘”ğ‘–ğ‘“ğ‘Ÿğ‘ğ‘›ğ‘‘[0,1)>ğ¶ğ‘…ğ‘œğ‘Ÿğ‘—=ğ‘—ğ‘Ÿğ‘ğ‘›ğ‘‘,ğ‘¥ğ‘–,ğ‘—,ğ‘”ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’.
(4)
The selection operator:

ğ‘¥ğ‘–,ğ‘”={ğ‘¢ğ‘–,ğ‘”ğ‘–ğ‘“ğ‘“(ğ‘¢ğ‘–,ğº)â‰¤ğ‘“(ğ‘¥ğ‘–,ğº)ğ‘¥ğ‘–,ğ‘”ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’
(5)
The jSO algorithm is one of the most advanced variants of the DE family [19, 31]. The jSO, with the new weighted version of the mutation strategy, inherits the evolutionary strategy and parameter control mechanism of the L-SHADE. The parameter control mechanism of jSO is an adaptive closed-loop control system. The values of the control parameters of jSO are different at different stages of evolution, which embodies a search process from global to local.

CMA-ES
A de-randomized evolutionary strategy with the covariance matrix adaptation (CMA-ES) is used as a perturbation operator to avoid using a pre-determined single value for the perturbation strength [29]. A standard CMA-ES consists of the three control parameters, including mean vector ğ‘š, covariance matrix ğ¶, and step-size ğœ. ğ‘š is the favorite solution. ğœ is the control step length. ğ¶ is the shape of the distribution ellipsoid. The adaptive update process for these three parameters is as follows.

ğ‘ğ‘ƒ	The population size of the jSO
D	Dimension size
ğ‘“(ğ‘‹)	The object function
ğ‘”	Generation counter
ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”	ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘” is randomly chosen as one of the top 100 p% individuals in the current population with ğ‘ƒâˆˆ[0.25,0.125]
ğ´ğ´	External archive
ğ‘‹ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ğ‘ğ‘’ğ‘ ğ‘¡	The best solution of current population
ğ‘›ğ‘¢ğ‘šğ‘”	The number of times the ğ‘”-th generation population has stagnated
ğœ	The so-called step-size ğœâˆˆğ‘…+ controls the step length
ğ‘š	The mean vector ğ‘šâˆˆğ‘…ğ‘› represents the favorite solution
ğ¶	The covariance matrix ğ¶âˆˆğ‘…ğ‘›Ã—ğ‘› determines the shape of the distribution ellipsoid
ğ‘ğ‘–	The multi-variate normal distribution
ğ‘ğ‘	The evolution path of ğ¶
ğ‘ğœ	The evolution path of ğœ
ğœ†	The population size of CMA-ES, default value:ğœ†=4+ğ‘Ÿğ‘œğ‘¢ğ‘›ğ‘‘(3.0âˆ—log(ğ·))
ğ‘‹ğ‘ğ‘’ğ‘ ğ‘¡	The best solution obtained by the proposed algorithm
ğ»ğ‘˜	Hessian matrix of the objective function in the ğ‘˜-th iteration of LBFGS
âˆ‡ğ‘“ğ‘˜	The first derivative of the objective function in the ğ‘˜-th iteration of LBFGS
ğ¸ğ¹ğ‘ 	The number of evaluations
The search solutions are sampled by a normally distributed.

ğ‘¥ğ‘–=ğ‘š+ğœğ‘¦ğ‘–,ğ‘¦ğ‘–âˆ¼ğ‘ğ‘–(0,ğ¶)
(6)
The mean is updated.

ğ‘šâ†âˆ‘ğ‘–=1ğœ‡ğ‘¤ğ‘–ğ‘¥ğ‘–:ğœ†=ğ‘š+ğœğ‘¦ğ‘¤
(7)
where ğ‘¦ğ‘¤=âˆ‘ğœ‡ğ‘–=1ğ‘¤ğ‘–ğ‘¦ğ‘–:ğœ†. The evolutionary path of ğ¶ is constructed and accumulated.

ğ‘ğ‘â†(1âˆ’ğ‘ğ‘)ğ‘ğ‘+1âˆ’(1âˆ’ğ‘ğ‘)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšğœ‡ğ‘¤â€¾â€¾â€¾âˆšğ‘¦ğ‘¤
(8)
The evolutionary path of Ïƒ is constructed and accumulated.

ğ‘ğœâ†(1âˆ’ğ‘ğœ)ğ‘ğœ+1âˆ’(1âˆ’ğ‘ğœ)2â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾âˆšğœ‡ğ‘¤â€¾â€¾â€¾âˆšğ¶âˆ’12ğ‘¦ğ‘¤.
(9)
The covariance matrix ğ¶ is updated.

ğ¶â†(1âˆ’ğ‘1âˆ’ğ‘ğœ‡)ğ¶+ğ‘1ğ‘ğ‘ğ‘ğ‘‡ğ‘+ğ‘ğœ‡âˆ‘ğ‘–=1ğœ‡ğ‘¤ğ‘–ğ‘¦ğ‘–:ğœ†ğ‘¦ğ‘‡ğ‘–:ğœ†.
(10)
The step length ğœ is updated.

ğœâ†ğœÃ—ğ‘’ğ‘¥ğ‘(ğ‘ğœğ‘‘ğœ(||ğ‘ğœ||ğ¸||ğ‘(0,ğ¼)||âˆ’1))
(11)
In summary, the main characteristics of CMA-ES are as follows: (i) Multivariate normal distribution is adopted to generate search solutions and conforms to the maximum entropy principle, in which the least possible assumptions on target function are used in the distribution shape. (ii) A rank-based selection strategy implies rotational invariance. (iii) The convergence rate is improved by a step-size control mechanism. (iv) Covariance matrix adaptation (CMA) increases the likelihood of previously successful steps, which reduces any convex-quadratic function to the sphere model without using derivatives. Please refer to the mentioned literature which includes a further description of the CMA-ES.

The description of the proposed algorithm
The three problems mentioned earlier in Sect. 1 about DE are critical reasons that restrict the improvement of the algorithm. Firstly, for the problem of disturbance intensity mentioned in the first problem and the second problem, CMA-ES is utilized as a disturbance operator, so that the parameters in the evolution process of the algorithm change adaptively and dynamically. Secondly, the designed stagnation detection operator is used to detect the state of the population all the time, and different operations are activated according to different states, to ensure the self-cooperation between algorithm operations as far as possible. For the third problem, a mathematical method (LBFGS) is employed to fuse in the local search stage of the evolutionary algorithm to conduct in-depth exploitation. In Sects. 3.1 and 3.2, the importance of the LBFGS in the jSO_CMA-ES_LBFGS is proved by detailed experiments. In addition, the effect of ensemble operators has been proved by experiments in Sect. 4.4.

As shown in Fig. 1, firstly, the jSO searches the entire search space as a global search operator. In the process, the stagnation detection operator is performed to detect the state of the population. The perturbation operator is activated when the population falls into a locally optimal solution (red pentagram). Second, the local optimal solution generated by jSO is considered as the mean vector parameter of the perturbation operator (CMA-ES). New Î» search points, which distributes around the local optimal solution (red line), are generated by a normally distributed (gray spot). Furthermore, a local search (LBFGS) is performed on the Î» search points to obtain the potential local optimal solutions (green pentagram). Finally, these local optimal solutions guide the population of jSO out of the current attraction basin to explore another area containing a potential local optimal solution.

Fig. 1
figure 1
The operating mechanism of jSO_CMA-ES_LBFGS

Full size image
The jSO provides the CMA-ES_LBFGS with a reliable initial solution, while CMA-ES_LBFGS returns multiple local optimal solutions to jSO. Then, jSO learns from these multiple local optimal solutions, thus jumping out of the current attraction basin. Therefore, the cooperative evolution between jSO and CMA-ES_LBFGS is achieved.

figure a
The pseudocode of jSO_CMA-ES_LBFGS is presented in Algorithms 1â€“3. As the LBFGS performs a local search on a copy of the population of CMA-ES, LBFGS does not affect the adaptability and convergence of CMA-ES. At the same time, the local optimization results are not directly injected into the jSO. The adaptability and convergence of jSO are not affected by CMA-ES and LBFGS. The size of the perturbation is provided by the CMA-ES, which explores the neighborhood of the current optimal solution for the jSO.

figure b
figure c
Stagnation detection operator
To measure the diversity of the population of jSO_CMA-ES_LBFGS, the diversity metric ğ·ğ‘ƒ is defined as follows:

ğ·ğ‘ƒğ‘”=1ğ‘ğ‘ƒâ‹…(âˆ‘ğ‘–=1ğ‘ğ‘ƒâˆ£âˆ£âˆ£âˆ£ğ‘¥ğ‘–,ğ‘”âˆ’1ğ‘ğ‘ƒâ‹…âˆ‘ğ‘—=1ğ‘ğ‘ƒğ‘¥ğ‘—,ğ‘”âˆ£âˆ£âˆ£âˆ£),
(12)
ğ‘›ğ‘¢ğ‘šğ‘”+1={ğ‘›ğ‘¢ğ‘šğ‘”+1ğ‘–ğ‘“âˆ£âˆ£ğ·ğ‘ƒğ‘”+1âˆ’ğ·ğ‘ƒğ‘”âˆ£âˆ£ğœâ‹…ğ·0Otherwise
(13)
where ğ·ğ‘ƒğ‘” is a metric that is used to measure the diversity of the ğ‘”-generation population. ğ· is the dimension size. The ğœ is the step size parameter in CMA-ES. The ğ‘›ğ‘¢ğ‘šğº is an indicator to detect whether the ğ‘”-generation population is in a state of stagnation. ğ‘›ğ‘¢ğ‘š1=0, if ğ‘›ğ‘¢ğ‘šğ‘”>ğ‘‡, the population of jSO_CMA-ES_LBFGS is considered to be in a stagnant state and the perturbation operator (CMA-ES) is activated. Here, ğ‘‡=10, the stagnation detection operator is executed to detect the state of the population of jSO. As shown in Figs. 2 and 3. The dynamics of jSO_CMA-ES_LBFGS in terms of population state (stagnation, explorative, and exploitative) are studied for ğ‘“4 and ğ‘“11 functions to show the role of the stagnation detection operator.

Fig. 2
figure 2
The diversity of the population of jSO_CMA-ES_LBFGS (ğ‘“4, ğ·=30)

Full size image
Fig. 3
figure 3
The diversity of the population of jSO_CMA-ES_LBFGS (ğ‘“11, ğ·=30)

Full size image
Local search
The local search methods for solving unconstrained numerical optimization problems are mainly divided into two categories: iterative gradient decrement algorithm and random local search algorithm. Random local search algorithms mainly include Multi-Trajectory Local Search algorithm (Mtsls1), Solis and Wets' Algorithm (SW), etc.

The SW is a local search algorithm proposed by Solis and Wets [32]. The basic principle is to give an initial solution and randomly generate a possible solution in a normal distribution. In the search process, according to the increase or decrease in the value of the objective function, the direction and step size of the search are adjusted to find the optimal solution. A candidate solution requires 3 times functional evaluations in the SW algorithm.

The Mtsls1 is a well-known local search algorithm [33]. As shown in Fig. 4, the Mtsls1 searches the solution space by dimension. A solution starts from the first dimension and ends with the last dimension. Furthermore, it is updated one by one.

Fig. 4
figure 4
Mtsls1 search along one dimension from the first dimension to the last dimension (Adapted from [31])

Full size image
Move step size ğ‘  along one dimension, if the fitness value is reduced, then obtained solution is kept for the next dimension (from solution ğ‘ to solution ğ‘). If the fitness value increases, the Mtsls1 returns to the starting solution and moves by 0.5Ã—ğ‘  along the reverse direction (from solution ğ‘ to solution ğ‘). If the fitness of solution ğ‘ is smaller than that of ğ‘, solution ğ‘ is kept as the starting solution in the next dimension. Therefore, the update process of the Mtsls1 is greedy.

The LBFGS belongs to an iterative gradient decrement algorithm. LBFGS is known for the fast convergence for solving the convex optimization problem and achieves higher accuracy with less number of function evaluations [34]. The core idea behinds the Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (BFGS), without using second-order partial derivatives of the function, is to construct a positive definite symmetric matrix that approximates the Hessian matrix. The objective function is optimized by BFGS under the condition of "quasi-Newton".

Each step of the BFGS method has the form,

ğ‘‘ğ‘˜=âˆ’ğ»ğ‘˜âˆ‡ğ‘“ğ‘˜
(14)
ğ‘¥ğ‘˜+1=ğ‘¥ğ‘˜+ğ›¼ğ‘˜ğ‘‘ğ‘˜
(15)
where ğ›¼ğ‘˜ is the step length and Î±k satisfies the Wolfe conditions:

ğ‘“(ğ‘¥ğ‘˜+ğ›¼ğ‘˜ğ‘‘ğ‘˜)â‰¤ğ‘“(ğ‘¥ğ‘˜)+ğ›½â€²ğ›¼ğ‘˜(âˆ‡ğ‘“ğ‘˜)ğ‘‡ğ‘‘ğ‘˜,
(16)
(âˆ‡ğ‘“ğ‘˜+1)ğ‘‡ğ‘‘ğ‘˜â‰¥ğ›½(âˆ‡ğ‘“ğ‘˜)ğ‘‡ğ‘‘ğ‘˜,
(17)
0<ğ›½â€²<12,ğ›½â€²<ğ›½<1.
(18)
Moreover, if ğ›¼ğ‘˜=1 satisfies Eq. (16), Eq. (17), and Eq. (18), ğ›¼ğ‘˜=1. ğ»ğ‘˜ is updated at every iteration utilizing the formula,

ğ»ğ‘˜+1=ğ‘‰ğ‘‡ğ‘˜ğ»ğ‘˜ğ‘‰ğ‘˜+ğœŒğ‘˜ğ‘ ğ‘˜ğ‘ ğ‘‡ğ‘˜
(19)
where

ğœŒğ‘˜=1ğ‘¦ğ‘‡ğ‘˜ğ‘ ğ‘˜,
(20)
ğ‘‰ğ‘˜=ğ¼âˆ’ğœŒğ‘˜ğ‘¦ğ‘˜ğ‘ ğ‘‡ğ‘˜,
(21)
ğ‘ ğ‘˜=ğ‘¥ğ‘˜+1âˆ’ğ‘¥ğ‘˜,
(22)
ğ‘¦ğ‘˜=âˆ‡ğ‘“ğ‘˜+1âˆ’âˆ‡ğ‘“ğ‘˜.
(23)
The information from the recent iterations is utilized to construct the approximate Hessian in the LBFGS [35,36,37]. The termination condition of LBFGS is satisfied when the candidates move one step in the search space. According to the reference manual of ALGLIB and the user guidance, LBFGS uses a 4-point central formula for differentiation. Therefore, one gradient calculation requires 4Ã—ğ· times function evaluations, where ğ· is dimension size. Because the gradient is the fastest direction in which the function value decreases, LBFGS is adopted as a local search operator to achieve the fast convergence speed of the proposed algorithm.

Principle and convergence analysis of jSO_CMA-ES_LBFGS
In this paper, the results of local optimization are not directly injected into jSO. Therefore, the adaptability and convergence of jSO are not affected by CMA-ES and LBFGS. The convergence of jSO proved to be equivalent to the convergence of jSO_CMA-ES_LBFGS.

As a population-based stochastic optimization algorithm, the evolutionary process of jSO is considered to be a stochastic process. Let ğ‘†=ğ‘…ğ· be the individual solution space, ğ‘†ğ‘ğ‘ƒ denote the population space, and ğ‘“:ğ‘†â†’ğ‘…+ be the fitness function, where ğ· is the dimension of the problem and ğ‘ğ‘ƒ is the population size. The basic operators of DE are described as follows.

The stochastic mapping of the mutation operator:

ğ‘‡ğ‘š:ğ‘†ğ‘ğ‘ƒâ†’ğ‘†
(24)
The probability distribution of the mutation operator:

ğ‘ƒ{ğ‘‡ğ‘š(ğ‘‹)=ğ‘£ğ‘–}=âˆ‘ğ‘ƒ(ğ‘‡1ğ‘š(ğ‘¥){ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”})â‹…ğ‘ƒ(ğ‘‡2ğ‘š(ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”)=ğ‘£ğ‘–)=âˆ‘ğ‘ƒ(ğ‘‡1ğ‘š(ğ‘¥)={ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”}),{ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”}âˆˆğ‘†4.
(25)
The stochastic mapping of the crossover operator:

ğ‘‡ğ‘:ğ‘†2â†’ğ‘†
(26)
The probability distribution of the crossover operator:

P{ğ‘‡ğ‘(ğ‘¥ğ‘–,ğ‘£ğ‘–)=ğ‘¢ğ‘–}=â§â©â¨âªâª0,ğ‘¢ğ‘–=ğ‘¥ğ‘–ğ‘šâ‹…ğ¶ğ‘˜ğ·ğ¶ğ‘…ğ‘˜(1âˆ’ğ¶ğ‘…)ğ·âˆ’ğ‘˜,ğ‘’ğ‘™ğ‘ ğ‘’ğ¶ğ‘…ğ‘+1ğ‘ğ‘ƒ,ğ‘¢ğ‘–=ğ‘£ğ‘–
(27)
The stochastic mapping of the selection operator:

ğ‘‡ğ‘ :ğ‘†2â†’ğ‘†.
(28)
The probability distribution of the selection operator:

ğ‘ƒ{ğ‘‡ğ‘ (ğ‘¥ğ‘–,ğ‘¢ğ‘–)=ğ‘¢ğ‘–}={1,ğ‘“(ğ‘¢ğ‘–)â‰¤ğ‘“(ğ‘¥ğ‘–)0,ğ‘’ğ‘™ğ‘ ğ‘’
(29)
From the above analysis, jSO algorithm is written as

ğ‘‹(ğ‘›+1)={ğ‘¥ğ‘–(ğ‘›+1)=ğ‘‡âˆ˜ğ‘ ğ‘‡âˆ˜ğ‘ğ‘‡ğ‘š(ğ‘‹(ğ‘›)),ğ‘–=1,â‹¯ğ‘ğ‘ƒ}
(30)
Definition 1
(Convergence in Probability) [38] Let {ğ‘‹(ğ‘›),ğ‘›=0,1,2â€¦} be a population sequence generated by a population-based stochastic algorithm, the stochastic sequence {ğ‘‹(ğ‘›)} weakly converges in probability to the global optimum. If and only if:

ğ‘™ğ‘–ğ‘šğ‘›â†’âˆğ‘ƒ{ğ‘‹(ğ‘›)âˆ©ğµâˆ—â‰ âˆ…}=1
(31)
where ğµâˆ— is a set of global optimum of an optimization problem.

Lemma 1
In the jSO algorithm, the evolutional direction of the population is monotonically non-increasing, i.e., ğ¹(ğ‘‹(ğ‘›+1))â‰¤ğ¹(ğ‘‹(ğ‘›)) .

Lemma 2
The population sequences of DE algorithm ğ‘‹(ğ‘›),ğ‘›âˆˆğ‘+ form a Markov chain process.

Proof
The population sequences of jSO are.

ğ‘‹(ğ‘›+1)=ğ‘‡(ğ‘‹(ğ‘›))=ğ‘‡âˆ˜ğ‘ ğ‘‡âˆ˜ğ‘ğ‘‡ğ‘š(ğ‘‹(ğ‘›)),
(32)
where the ğ‘‡ğ‘ , ğ‘‡ğ‘, and ğ‘‡ğ‘š are irrelevant to the iteration ğ‘› and ğ‘‹(ğ‘›+1) only depends on ğ‘‹(ğ‘›). The transition probability of states is calculated by

ğ‘ƒ{ğ‘‡(ğ‘‹(ğ‘›))ğ‘–=ğ‘¥ğ‘–(ğ‘›+1)}=âˆ‘âˆ‘âˆ‘ğ‘ƒ{ğ‘‡1ğ‘š(ğ‘‹(ğ‘›))={ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”}}â‹…ğ‘ƒ{ğ‘‡ğ‘(ğ‘¥ğ‘–(ğ‘›),ğ‘£ğ‘–)=ğ‘¢ğ‘–}â‹…ğ‘ƒ{ğ‘‡ğ‘ (ğ‘¥ğ‘–(ğ‘›),ğ‘¢ğ‘–)=ğ‘¥ğ‘–(ğ‘›+1)},ğ‘¢ğ‘–âˆˆğ‘†,ğ‘£ğ‘–âˆˆğ‘†,{ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”}âˆˆğ‘†4.
(33)
It is obvious that âˆ€ğ‘‹(ğ‘›)âˆˆğ‘†ğ‘ğ‘ƒ,âˆƒ{ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”}âˆˆğ‘†ğ‘ğ‘ƒ and ğ‘£ğ‘–,ğ‘¢ğ‘–âˆˆğ‘†ğ‘ğ‘ƒ, subject to

ğ‘ƒ{ğ‘‡1ğ‘š(ğ‘‹(ğ‘›))={ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”}}>0
(34)
ğ‘ƒ{ğ‘‡2ğ‘š(ğ‘¥ğ‘Ÿ1,ğ‘¥ğ‘Ÿ2,ğ‘¥ğ‘–,ğº,ğ‘¥ğ‘ğµğ‘’ğ‘ ğ‘¡,ğ‘”)=ğ‘£ğ‘–}>0,
(35)
ğ‘ƒ{ğ‘‡ğ‘(ğ‘¥ğ‘–(ğ‘›),ğ‘£ğ‘–)=ğ‘¢ğ‘–}>0,
(36)
ğ‘ƒ{ğ‘‡ğ‘ (ğ‘¥ğ‘–(ğ‘›),ğ‘¢ğ‘–)=ğ‘¥ğ‘–(ğ‘›+1)}>0
(37)
So,

ğ‘ƒ{ğ‘‡(ğ‘‹(ğ‘›))ğ‘–=ğ‘¥ğ‘–(ğ‘›+1)}>0,
(38)
not depending on ğ‘›. Thus, the transition probability of states is given as

ğ‘ƒ{ğ‘‡(ğ‘‹(ğ‘›))=ğ‘‹(ğ‘›+1)}=âˆğ‘–=1ğ‘ğ‘ƒğ‘ƒ{ğ‘‡(ğ‘‹(ğ‘›))ğ‘–=ğ‘¥ğ‘–(ğ‘›+1)}>0.
(39)
This transition probability is also independent of ğ‘›. Therefore, the population sequence of DE is a homogeneous irreducible aperiodic Markov chain. Markov population sequence of jSO is represented by

ğ‘ƒ{ğ‘‹,ğ‘Œ}=ğ‘ƒ{ğ‘‹(ğ‘›+1)=ğ‘Œ|ğ‘‹(ğ‘›)=ğ‘‹}
(40)
â–¡

Theorem 1
Suppose that {ğ‘¥(ğ‘¡),ğ‘¡=0,1,2,â€¦} is the population sequence generated by DE, then, {ğ‘¥(ğ‘¡),ğ‘¡=0,1,2,â€¦} converges to the subset ğµâˆ—0={ğ‘Œ=(ğ‘¦1,â‹¯ğ‘¦ğ‘ğ‘ƒ);ğ‘¦ğ‘–âˆˆğµâˆ—} of global optimum ğµâˆ— with probability one, i.e.

limğ‘›â†’âˆğ‘ƒ{ğ‘‹(ğ‘›)âˆˆğµâˆ—0|ğ‘‹(0)=ğ‘‹0}=1
(41)
Proof
Suppose ğ‘¥âˆ— is the unique optimum satisfaction solution. The following properties are derived according to Eq. (25) and Eq. (27).

(1)
if X, Yâˆˆğµâˆ—0, then ğ‘ƒ{ğ‘‹,ğ‘Œ}>0,ğ‘ƒ{ğ‘Œ,ğ‘‹}>0, in which these two states are interconnected; i.e.,ğ‘‹â†”ğ‘Œ.

(2)
if Xâˆˆğµâˆ—0 and Yâˆ‰ğµâˆ—0 then ğ‘ƒ{ğ‘‹,ğ‘Œ}=0, i.e.,ğ‘‹â†®ğ‘Œ.

Hence ğµâˆ—0 is a positive recurrent, irreducible, aperiodic, and closed set. According to the properties of the aperiodic, homogeneous Markov chain [39], the sequence {ğ‘¥(ğ‘¡),ğ‘¡=1,2,â€¦,} exists a limiting distribution ğœ‹(ğ‘Œ),

ğ‘™ğ‘–ğ‘šnâ†’âˆğ‘ƒ{ğ‘‹(ğ‘›)=ğ‘Œ|ğ‘‹(0)=ğ‘‹0}={ğœ‹(ğ‘Œ),0,ğ‘Œâˆˆğµâˆ—0ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’.
(42)
Then,

ğ‘™ğ‘–ğ‘štâ†’âˆğ‘ƒ{ğ‘‹(ğ‘›)=ğµâˆ—0}=1
(43)
So,

ğ‘™ğ‘–ğ‘šğ‘›â†’âˆğ‘ƒ{ğ‘‹(ğ‘›)âˆ©ğµâˆ—â‰ âˆ…}=1
(44)
According to Definition 1, we can obtain that jSO converges to the global optimum in probability.

â—»
Experiments and comparisons
The performance of the proposed algorithm and the comparison algorithm is evaluated in this section. Firstly, a brief introduction of the CEC2017 benchmark function is shown as follows.

The CEC2017 benchmark test suite contains 30 test functions, ğ‘“1âˆ’ğ‘“2 are unimodal functions, ğ‘“4âˆ’ğ‘“10 are simple multimodal functions, ğ‘“11âˆ’ğ‘“20 are hybrid functions, and ğ‘“21âˆ’ğ‘“30 are composition functions. These functions are classic, representative single-objective continuous optimization functions, which are all parameter optimization functions. They are published by an expert group of the special theme of the annual CEC conference to test the current optimization methods (especially the Meta-heuristic optimization method). The summary of the CEC2017 test function is shown in Table 1. The last column in the table represents the optimal value of each function. ğ‘“2 has been excluded because it shows unstable behavior especially for higher dimensions. There are 20 basic functions [40] defined to construct the CEC2017 test function. The unimodal functions and simple multimodal functions are obtained by adding rotation and translation operations to the basic functions. The hybrid functions and composition functions are obtained by embedding different basic functions into different subcomponents of variables, where ğ‘ğ¹ is the number of basic functions.

Table 1 Summary of the CEC2017 test function
Full size table
In Sect. 4.1, the time complexity of the jSO_CMA-ES_LBFGS is analyzed. In Sect. 4.2, the experimental condition and parameters setting are described when the simulation is run. The role of the local search algorithm is tested in Sect. 4.3. The integration effect of different operators is provided in Sect. 4.4. The comparison of jSO_CMA-ES_LBFGS with certain representative algorithms is described in Sect. 4.5. Moreover, owing to limited space, interested readers obtain the experimental results of all algorithms from the online supplemental material.

jSO_CMA-ES_LBFGS time complexity
This subsection deals with the time complexity of the jSO_CMA-ES_LBFGS. The running time obtained by evaluating the benchmark function ğ‘“18 is compared with the running time of the test program.

The computing time of the test program is denoted as T0. Let us assume that variable T1 presents the time required for evaluating the benchmark function ğ‘“18 and variable T2 the time of jSO_CMA-ES_LBFGS execution for function ğ‘“18 within 200,000 evaluations for each dimension. Variable T2Ë† is an average of T2 values obtained in five independent runs. Table 2 shows the algorithm complexities relationship with dimension. The computational complexity of the algorithm jSO_CMA-ES_LBFGS is reflected by the measured and calculated variables T0, T1, T2Ë†, and (T2Ë†âˆ’T1)/T0 for each of the observed dimensions ğ·={10,30,50}. This calculation is independent of the computing system and programming language in which the measured algorithm is implemented. According to Table 2, it is easy to conclude that more computational cost is required with the increasing number of dimensions for the benchmark functions.

Table 2 The computational complexity of the algorithm jSO_CMA-ES_LBFGS (all times are in seconds)
Full size table
Experimental conditions and parameters setting
For a fair comparison among all the algorithms, they are executed using the same maximum number of function evaluations (ğ¸ğ¹ğ‘ =ğ·Ã—10000), where the ğ· is the size of the dimension. All the algorithms are executed independently 51 times on each test function. The experiments are run in the Windows Server 2012 R2 under the hardware environment of Intel (R) Core (TM) i7-6700 CPU @ 3.40 GHz processor and 8.0 GB of RAM. The proposed algorithm is implemented using the Câ€‰+â€‰â€‰+â€‰programming language. It is necessary to point out that a numerical analysis and processing library, ALGLIB (http://www.alglib.net/), is utilized to implement the process of LBFGS. For each comparison algorithm, the values of the control parameter are configured as recommended in the corresponding literature.

The value of the parameters has a significant impact on the performance of the jSO_CMA-ES_LBFGS. There are three crucial parameters: (1) stagnation threshold ğ‘‡; (2) memory size ğ»; (3) the step length ğœ in the jSO_CMA-ES_LBFGS. The Taguchi method of design is adopted to calibrate the value of each parameter in jSO_CMA-ES_LBFGS. In the experiment, the proposed algorithm is calibrated using the 30-dimensional CEC2017 benchmark function [41]. The various values of ğ‘‡, ğ», and ğœ are listed in Table 3. The parameter combinations and average error values yielded by jSO_CMA-ES_LBFGS are listed in Table 4. According to Table 4, the significance rank of each parameter is listed in Table 5. Meanwhile, the tendency of the parameters is described in Fig. 5.

Table 3 The parameters for different levels
Full size table
Table 4 Parameter combinations
Full size table
Table 5 Response table for means
Full size table
Fig. 5
figure 5
Main effects plot of all parameters

Full size image
From Table 5, it is observed that ğ» is the most significant one among all the parameters, which implies that the memory size ğ» is an important influential factor to jSO_CMA-ES_BFGS. The large ğ» records more search history information. The external storage failure is caused by abnormal ğ» with a too large or too small value. The ğœ ranks the second place, which illustrates that it is also an important factor in jSO_CMA-ES_LBFGS. A small ğœ would cause the population easily to fall into the local optimum. A large ğœ leads the algorithm to search in the global space and wastes a lot of function evaluations, similar to the random search. The stagnation threshold parameter ğ‘‡ ranks third place. ğ‘‡ is the best combination point of the global search operator and the local search operator to achieve a trade-off between exploration and exploitation.

According to the results of the Taguchi method, the parameters in jSO_CMA-ES_LBFGS are suggested as follows: ğ‘‡=10,ğ»=5,ğœ=0.1.

Selection of local search operators
The two local search algorithms, Solis&Wets algorithm (SW) and the Multi-Trajectory Local Search (Mtsls1) are tested in the paper. Although various adjustments in the parameter settings are combined to solve the problem in this paper, the essences of these comparison algorithms (e.g., the framework, encoding, operating mechanism) are not changed in the implementation process. According to the single factor principle, the parameters of jSO and CMA-ES remain unchanged. The parameter values used by SW and MTSLS1 are shown in Table 6. The experimental results of LBFGS, SW, and MTSLS1 are shown in Tables 7, 8, 9.

Table 6 The parameter values of SW and MTSLS1
Full size table
Table 7 Results of the multiple-problem Wilcoxonâ€™s test
Full size table
Table 8 The mean error of different local search algorithm combinations on 10 dimensions
Full size table
Table 9 The mean error of different local search algorithm combinations on 30 dimensions
Full size table
The SW algorithm belongs to the estimation of distribution algorithm (EDA), the number of parameters of the SW algorithm is more than that of MTSLS1 and LBFGS. There is no prior knowledge to find the best combination of parameters of the SW algorithm. The Mtsls1 algorithm searches from one dimension to another without considering the dependencies between variables. LBFGS belongs to the iterative gradient decrement algorithm. LBFGS is known for the fast convergence for solving convex optimization problems and achieves higher accuracy with a fewer number of function evaluations.

From Table 7, the LBFGS is significantly better than SW and Mtsls1 on 29 benchmark functions with Î±=0.1 and Î±=0.05 for 30 dimensions. Therefore, the LBFGS is employed as a local search operator in the jSO_CMA-ES_LBFGS rather than the SW or MTSLS1.

The effect of ensemble operators
As shown in Figs. 6 and 7. If there is no LBFGS operator in jSO_CMA-ES_LBFGS, jSO_CMA-ES_LBFGS is degraded to jSO_CMA-ES. The diversity of the algorithm in the process of solving is increased effectively by the gray points in Fig. 6 (the solutions generated by CMA-ES). The mean values of jSO, jSO_CMA-ES, and jSO_LBFGS on 30 dimensions and 50 dimensions are showed in Table 10. From Table 11, it is found that jSO_CMA-ES is significantly better than jSO on 29 benchmark functions with Î±=0.1 for 30 dimensions. Therefore, CMA-ES is embedded in jSO and has at least no side effects on the performance of jSO. According to the literature [42,43,44], CMA-ES has the characteristics of de-randomization and rotational invariance. New search points are sampled by normal distribution as perturbations of the mean (redpoint). The standard CMA-ES belongs to the estimation of the distribution algorithm, which has a strong global search ability and insufficient local searchability. It is not suitable for solving high-dimensional optimization problems.

Fig. 6
figure 6
The operating mechanism of jSO_CMA-ES

Full size image
Fig. 7
figure 7
The operating mechanism of jSO_LBFGS

Full size image
Table 10 The mean values of jSO, jSO_CMA-ES, and jSO_LBFGS
Full size table
Here, CMA-ES only as an agent provides a relatively reliable initial solution for LBFGS. As shown in Table 11, a significant difference between jSO_CMA-ES and jSO with ğ›¼=0.1 is not observed for 50-dimensional optimization problems, which demonstrates the rationality of the hypothesis proposed in this paper. As shown in Fig. 7. If there is no CMA-ES operator in jSO_CMA-ES_LBFGS, jSO_CMA-ES_LBFGS is degraded to jSO_LBFGS. It makes the search area limited to a locally optimal solution to fall into stagnation. As shown in Table 11, a significant difference between jSO_LBFGS and jSO with Î±=0.1 is also not observed. Therefore, the disturbance strength of jSO_LBFGS is insufficient. However, as shown in Table 12, jSO_CMA-ES_LBFGS is significantly better than jSO with Î±=0.1 and Î±=0.05 for 30 and 50 dimensions.

Table 11 The results of the Wilcoxonâ€™s test for jSO, jSO_CMA-ES, and jSO_LBFGS
Full size table
Table 12 The results of the Wilcoxonâ€™s test for jSO_CMA-ES_LBFGS, and jSO
Full size table
The mean plot with a 95% confidence interval for the different operators is shown in Fig. 8. The abscissa represents three different kinds of functions including simple multimodal functions (ğ‘“4âˆ’ğ‘“10), hybrid functions (ğ‘“11âˆ’ğ‘“20), and composition functions (ğ‘“21âˆ’ğ‘“30) [19]. The horizontal axis is the different operators. The vertical axis is the normalized value of 51 independent tests by Eq. (45). To make the effect clear, the values of 51 independent tests were normalized as

Normalized Values=log10(Mean of51 independent tests)
(45)
Fig. 8
figure 8
Mean plot with a 95% confidence interval for the different operators

Full size image
The points in the figure below reflect the excellent performance of the operators, and the horizontal lines on both sides of each point represent the stability of the operators. The results show that each strategy contributes to the jSO_CMA-ES_LBFGS. In particular, the results have a significant improvement compared with the original jSO in hybrid functions and composition functions. Therefore, jSO, CMA-ES, and LBFGS are inseparable and indispensable organic whole, which achieves the effect of "1â€‰+â€‰1â€‰+â€‰1â€‰>â€‰3" instead of "1â€‰+â€‰1â€‰>â€‰2".

Comparison of jSO_CMA-ES_LBFGS with certain state-of-the-art algorithms
In this section, the proposed jSO_CMA-ES_LBFGS is compared with eleven state-of-the-art algorithms, including jSO [19], LSHADE_cnEpSin [20], AMECoDEs [45], ELSHADE [46], EBLSHADE [46], EBOwithCMAR [47], LSHADE_SPACMA [26], LSHADE-EpSin [48], CMA-ES [44], EWWO [49], and OLPSO [50]. The jSO, LSHADE_cnEpSin, AMECoDEs, ELSHADE, EBLSHADE, LSHADE_SPACMA, and LSHADE-EpSin are typical variants of DE. Significantly, jSO, LSHADE-cnEpSin, and LSHADE_SPACMA are on the second, third, and fourth place on CEC2017 competition on single objective real-parameter optimization. The EBOwithCMAR is in the first place on CEC2017. The EWWO is an improved water wave optimization algorithm enhanced by CMA-ES. The value of the control parameter of each compared algorithm is set to the value recommended in the original paper. The simulation experiment is carried out to evaluate the performance of the proposed jSO_CMA-ES_LBFGS on the CEC2017 test suite. All algorithms are executed independently 51 times on each test function, and the mean value and standard deviation (std) metrics are calculated.

The parameters of the compared algorithms use the same settings as in their original papers, which are listed in Table 13. For jSO_CMA-ES_LBFGS, the parameters used in these comparison experiments are set experimentally. Detailed parameter analysis about the jSO_CMA-ES_LBFGS is given in Sect. 4.2.

Table 13 Parameter settings
Full size table
Meanwhile, the jSO_CMA-ES_LBFGS and other comparison algorithms are compared on the convergence rate. The convergence plots of the ğ‘“1, ğ‘“4, ğ‘“7, ğ‘“11, ğ‘“12, ğ‘“26, and ğ‘“28 which include the unimodal, simple multimodal, hybrid, and composition functions are described in Fig. 9. The reason why these functions are selected for analysis is that they are the best reflection of the performance and operating mechanism of jSO_CMA-ES_LBFGS on all test functions. The mean value of  jSO_CMA-ES_LBFGS and jSO are listed in Table 14.

Fig. 9
figure 9
Convergence plots of jSO_CMA-ES_LBFGS and other compared algorithms

Full size image
Table 14 The mean value of jSO_CMA-ES_LBFGS and jSO
Full size table
As shown in Fig. 9, although the proposed algorithm does not converge at the fastest speed, it maintains a continuous downward trend and obtains a higher precision solution than that of the comparison algorithm, which is attributed to the role of perturbation operator and local search operator. In addition, a distinct separation between jSO and jSO_CMA-ES_LBFGS on the convergence rate for the late stage of population evolution is obvious from Fig. 9, which implicitly indicates that the operating mechanism of the proposed algorithm is effective.

The box plots for ğ‘“1, ğ‘“4, ğ‘“11, and ğ‘“28 are shown in Fig. 10 to further illustrate the stability and performance of the proposed algorithm. Box plots use the data of five statistics: minimum, first quartile, median, and the third quartile and the maximum value to describe a method of data, it can also roughly see whether data have symmetry, and the distribution of the dispersed degree of information, such as special, can be used for the comparison of several samples. The sides of the two sides of boxes will correspond to the top and bottom quartile of the data batch. The red line in the boxes is the position of the median value. An extension line is created between the top quartile and the maximum, which is called "whicker". Similarly, build an extension line between the bottom quartile and the minimum. The points outside the extension line represent outliers. Therefore, the narrower the box and the fewer outliers mean that the data tend to be stable. On the other hand, the ordinate of the box plots represents the normalized error value, so the closer the box is to the bottom, the better the performance of the algorithm for the considered minimization problems. The stability of the algorithm is indicated from the box plots of ğ‘“1(ğ·=30) and ğ‘“28(ğ·=50) from Fig. 9. Although the narrowest box is not shown from the two box plots of ğ‘“4(ğ·=30) and ğ‘“11(ğ·=30), the box position of the proposed algorithm is the closest to the bottom, so the performance of the proposed algorithm is better than other comparison algorithms.

Fig. 10
figure 10
Box plots of jSO_CMA-ES_LBFGS and other compared algorithms on some typical benchmark functions

Full size image
To testify the performance of the jSO_CMA-ES_LBFGS, Wilcoxonâ€™s test [51] is performed to check the behaviors of the six algorithms which are introduced as compared algorithms. The statistical analysis results are summarized in Table 15, considering jSO_CMA-ES_LBFGS as a control algorithm. The "yes" in bold means that jSO_CMA-ES_LBFGS is significantly better than the another algorithm. From Table 15, it is found that jSO_CMA-ES_LBFGS is significantly better than jSO on 29 benchmark functions with ğ›¼=0.1 for 30, 50, 100 dimensions and significantly better than AMECoDEs with Î±=0.05 and Î±=0.1 for 30, 50, and 100 dimensions. The jSO_CMA-ES_LBFGS is significantly better than ELSHADE with ğ›¼=0.1 for 50 and 100 dimensions, and significantly better than EBLSHADE with ğ›¼=0.1 for 10 and 100 dimensions. Although the significant differences are not observed between jSO_CMA-ES_LBFGS and the comparison algorithms in a certain case, the value of ğ‘…âˆ’ is better than the value of ğ‘…+. Namely, the jSO_CMA-ES_LBFGS obtains better solutions than other comparison algorithms in the above cases.

Table 15 Results of the Wilcoxonâ€™s test for jSO_CMA-ES_LBFGS and other compared algorithms
Full size table
Table 16 Results achieved by Friedman Test
Full size table
Friedmanâ€™s test [51] is carried out to further test the significant differences between jSO_CMA-ES_LBFGS and the eleven competitors. The results are listed in Table 16. As shown in Fig. 11, there is a significant difference between jSO_CMA-ES_LBFGS and AMECoDEs with Î±=0.1 and Î±=0.05 for 30 dimensions. As shown in Fig. 12, a significant difference among jSO_CMA-ES_LBFGS, AMECoDEs, and ELSHADE with Î±â€‰=â€‰0.1 and Î±â€‰=â€‰0.05 is observed on 50 dimensions. Although there is no significant difference between jSO_CMA-ES_LBFGS and other compared algorithms in other cases, the rank of jSO_CMA-ES_LBFGS is the smallest in all comparison algorithms.

Fig. 11
figure 11
Rankings for ğ·=30

Full size image
Fig. 12
figure 12
Rankings for ğ·=50

Full size image
In summary, the results of the statistical analysis imply that the performance of the proposed jSO_CMA-ES_LBFGS is significantly better than that of the other comparison algorithms for test problems with 30 and 50 dimensions, whereas with the increase in problem dimensions, the performance of jSO_CMA-ES_LBFGS and the comparison algorithms decreases. On the one hand, the growing functional dimensions make the number of decision variables expand dramatically. On the other hand, in the CEC2017 benchmark test suite, the number of evaluations specified for functions with different dimensions is limited. Especially in high-dimensional functions, the algorithm cannot effectively find an approximate optimal solution within the limited number of evaluations. With the increase in function dimensions, the complexity of the problems to be solved increases sharply, which affects the performance of the algorithm.

Application to the engineering problems
The proposed algorithm is tested on two important engineering problems including continuous and discrete decision space. In addition, the performance of the jSO_CMA-ES_LBFGS is compared with canonical methods to analysis the advantages of the jSO_CMA-ES_LBFGS in addressing engineering optimization problems. For each engineering problem, the algorithms are run independently 31 times.

Gear train engineering design problem
The gear train engineering design problem is utilized to verify the performance of the jSO_CMA-ES_LBFGS in addressing engineering problems. Gear train design aims to minimize the gear ration of the gear train as shown in Fig. 13. There are four types of parameters in the gear train engineering design problem. The details of the mathematical model about this problem are described as follows [52].

Fig. 13
figure 13
Geat train design problem

Full size image
Decision variable:

ğ‘”âƒ— =[ğ‘”1,ğ‘”2,ğ‘”3,ğ‘”4]=[ğ‘€ğ´,ğ‘€ğµ,ğ‘€ğ¶,ğ‘€ğ·]
(46)
Objective:

ğ‘“(ğ‘”âƒ— )=(16.931âˆ’ğ‘”2ğ‘”3ğ‘”1ğ‘”4)2
(47)
Subject to:

12â‰¤ğ‘”1,ğ‘”2,ğ‘”3,ğ‘”4â‰¤60
(48)
The jSO_CMA-ES_LBFGS is run in 1000 fitness evaluations. The obtained statistical results for gear train engineering design problem are compared in Table 17. More details about Sandgren, GeneAS, and ABC are described in [52]. It is observed from Table 17 that the jSO_CMA-ES_LBFGS outperforms other algorithms.

Table 17 The results of five algorithms
Full size table
The blocking flow shop scheduling problem
The blocking flow shop scheduling problem (BFSP) [53], which is one of the most important scheduling types, is widespread in modern industries. In the BFSP, there are no buffers between machines, and the job remains in the current machine until the next machine is available for processing. With the increase of the scheduling scale, the difficulty and computation time of solving the problem increased exponentially. Existing experiments and literature have shown that BFSP with more than two machines is a typical NP-hard problem [53].

The definitions of the variables and parameters mentioned in this section are recorded in Table 18. The details of the mathematical model of BFSP are described as follows.

Table 18 The definitions of the notations
Full size table
Decision variable:

ğ‘¥ğ‘–,ğ‘˜âˆˆ{0,1},(ğ‘–,ğ‘˜=1,2,â€¦,ğ‘›)
(49)
Objective:

minğ¶ğ‘šğ‘ğ‘¥=maxğ‘˜=1,2,â€¦,ğ‘›(ğ·ğ‘˜,ğ‘š)
(50)
Subject to:

âˆ‘ğ‘˜=1ğ‘›ğ‘¥ğ‘–,ğ‘˜=1,ğ‘–âˆˆ{1,2,â€¦,ğ‘›}
(51)
âˆ‘ğ‘–=1ğ‘›ğ‘¥ğ‘–,ğ‘˜=1,ğ‘˜âˆˆ{1,2,â€¦,ğ‘›}
(52)
ğ·1,0â‰¥0
(53)
ğ·ğ‘˜,0â‰¥ğ·ğ‘˜âˆ’1,1,(ğ‘˜=2,3,â€¦,ğ‘›)
(54)
ğ·ğ‘˜,ğ‘—â‰¥ğ·ğ‘˜,ğ‘—âˆ’1+âˆ‘ğ‘–=1ğ‘›ğ‘¥ğ‘–,ğ‘˜â‹…ğ‘ƒğ‘–,ğ‘—,(ğ‘˜=1,2,â€¦,ğ‘›),(ğ‘—=1,2,â€¦,ğ‘š)
(55)
ğ·ğ‘˜,ğ‘—â‰¥ğ·ğ‘˜âˆ’1,ğ‘—+1,(ğ‘˜=2,3,â€¦,ğ‘›),(ğ‘—=1,2,â€¦,ğ‘šâˆ’1)
(56)
Mathematical model of BFSP
In this study, the objective is to minimize the makespan criterion. The makespan of the schedule ğœ‹ is ğ¶ğ‘šğ‘ğ‘¥=ğ·ğ‘›,ğ‘š. The computation complexity of this task is ğ‘‚(ğ‘›âˆ—ğ‘š).

[ğ‘ƒğ‘–,ğ‘—]3Ã—3=â¡â£â¢â¢â¢ğ‘ƒ1,1ğ‘ƒ2,1ğ‘ƒ3,1ğ‘ƒ1,2ğ‘ƒ2,2ğ‘ƒ3,2ğ‘ƒ1,3ğ‘ƒ2,3ğ‘ƒ3,3â¤â¦â¥â¥â¥=â¡â£â¢â¢312231123â¤â¦â¥â¥
(57)
Here, an example is presented to show how the decision variables reflect the solution by considering a problem with three jobs (ğ‘›=3) and three machines (ğ‘š=3). The processing times are given in Eq. (57). The scheduling Gantt is shown in Fig. 14. The makespan of schedule sets ğœ‹1={ğ½1,ğ½2,ğ½3} in BFSP is 13.

Fig. 14
figure 14
Gantt chart for a solution to the example problem

Full size image
Experimental settings and analysis
The traditional jSO algorithm and its variants cannot be directly utilized to solve the combinational optimization problem with discrete characteristics. Therefore, the coding scheme and decoding rule are utilized to help the algorithms work directly in the discrete domain by representing individuals as discrete job permutations. In this paper, the LOV rule is utilized to represented individuals as discrete job permutations. More details about LOV rule can be found in [54]. The average relative percentage deviation (ARPD) index is utilized to measure the results, and the calculate method is shown in Eq. (58), where R is the number of runs and ğ¶ğ‘– is the solution generated by a specific algorithm in the ith experiment for a given instance. ğ¶ğ‘œğ‘ğ‘¡ is the minimum makespan found by all algorithms. The algorithm with minimum ARPD outperforms other algorithms.

ğ´ğ‘…ğ‘ƒğ·=1ğ‘…â‹…âˆ‘ğ‘–=1ğ‘…ğ¶ğ‘–âˆ’ğ¶ğ‘œğ‘ğ‘¡ğ¶ğ‘œğ‘ğ‘¡â‹…100%
(58)
The well-known standard benchmark set of Taillard [55] is used for evaluating the performance of jSO_CMA-ES_LBFGS. This benchmark is composed of 120 different problem instances. The instances are categorized into 12 subsets of different combinations of n (number of jobs) and m (number of machines). These combinations range from 20 jobs and 5 machines up to 500 jobs and 20 machines. The results of jSO, OLPSO, EWWO, CMA-ES, EBLSHADE, LSHADE-cnEpsin, and jSO_CMA-ES_LBFGS are shown in Table 19. Each algorithm is run in 10mâ‹…n milliseconds (ms). The parameters of the comparison algorithms are consistent with the previous ones.

Table 19 The ARPD of all compared algorithms
Full size table
The number of jobs and machines have a significant impact on all the compared algorithms from Table 19. The experiment results show the excellent performance of the jSO_CMA-ES_LBFGS for solving the BFSP. The effectiveness of jSO_CMA-ES_LBFGS compared with jSO is that the local search capability of proposed algorithm is enhanced via LBFGS. On the other hand, the LBFGS is embedded in CMA-ES to perturb the optimal candidates in the solution space when the population is falling into stagnation. Therefore, jSO_CMA-ES_LBFGS is a competitive algorithm for solving BFSP.

Conclusions and future research
The paper presented a hybrid algorithm (jSO_CMA-ES_LBFGS) based on DE, CMA-ES, and LBFGS to solve the continuous optimization problems. The various experimental results imply the following conclusions: (1) LBFGS, as a local search operator, plays an important role in the proposed algorithm to enhance the local search capability of DE. (2) In the jSO_CMA-ES_LBFGS, a relatively reliable initial solution for the local search operator is generated by the CMA-ES, which is activated to perturb the optimal candidates in the solution space when the population is falling into stagnation. The LBFGS is embedded in CMA-ES as the local search strategy to obtain the potential local optimal solutions. (3) The search performance of jSO_CMA-ES_LBFGS is better than that of the other comparison algorithms on a certain confidence level. Further, jSO_CMA-ES_LBFGS is an effective algorithm to solve the continued optimization problems including the CEC2017 benchmarks and the gear train engineering design problem. On the other hand, the proposed jSO_CMA-ES_LBFGS is applied to solve BFSP effectively. The results for the benchmark set of Taillard demonstrate that the proposed algorithm is with good performance.

In future work, the search performance of jSO_CMA-ES_LBFGS is further improved, and the improved jSO_CMA-ES_LBFGS is applied to solve certain complex scheduling problems, e.g., the flow shop scheduling problems, the job shop scheduling problems, the lot-streaming flow shop scheduling problems. Moreover, DE is combined with other meta-heuristics for the multi-objective scheduling problems.

Differential Evolution (DE)
Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES)
Limited-Memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno (LBFGS)
Cooperative coevolution
Numerical optimization