Underrating the evergrowing effectiveness of cryptanalysis tools is likely to be a big mistake. Although available protection techniques are effective and may dissipate our worries about information safety, we should not ignore the formidable advancements in hacking tools. Such advancements are likely to soon challenge our protection techniques. Unfortunately, although this challenge is probable, there is not much advancement in the way the protection tools function. Encryption techniques still use roughly the same computation principles and the same level of dependency on the encryption key. This paper goes beyond these encryption techniques and proffers an encryption technique with a new computation model. This model uses a fuzzy neural network to generate highly complicated hiding codes from the encryption key. The computation model uses also substitution and distortion methods that depend on plaintext and chaotic noises to induce enormous confusion in the ciphertext. This combination along with other confusion boosting actions such as interval scattering and chaotic locking establishes a very effective encryption technique. Experiments on the proof-of-concept prototype showed that the output (ciphertext) of the proposed technique passed rigorous randomness tests.

Introduction
Evergrowing security challenges necessitate the reconsideration of our encryption techniques. Cryptanalysts are producing more effective hacking techniques because they are gaining a deeper understanding of how conventional encryption techniques work. If we do not act proactively, these hacking techniques will seriously threaten the security of the information. Current techniques (e.g. [1,2,3,4,5,6,7]) suffer from intrinsic issues: (1) their dependency on the encryption key and (2) their dependency on the complexity of the mathematical computations to secure the information. (The complexity of the mathematical computations is achieved using different means. Examples include the computations within unconventional fields such as the Galois field [8], chaotic/dynamic manipulations [9,10,11,12,13,14,15], and DNA-like sequences for hiding information [16,17,18,19,20,21,22].)

To the best of our knowledge, all encryption techniques partially depend on the key. The rationale is that more dependency on the key may increase its (the key) predictability. For instance, AES [23, 24] and DES [25, 26] use the key only in the key round, which mixes the expanded key with the ciphertext using an XOR operation. The likelihood that the adversaries can predict the encryption key increases only if this key is used improperly (e.g. after expanding it using a simple process). However, if the key is properly expanded by a highly effective process, it is unlikely to be susceptible to prediction. That is because the key is embedded in an enormously complicated code sequence, making it impossible for the hacking tools to analyze this code sequence and predict the key.

This paper proffers an encryption technique that induces substantial confusion by exploiting plaintext, encryption key, and chaotic noises produced by a key-controlled chaotic system. Such dynamics in which the encryption technique depends on three facets (plaintext, key, chaotic noises) make the computations of the encryption technique extremely complicated and thus unpredictable. The functionality of the proposed technique is based on several operations that ensure deep transformation and high confusion in the resulting ciphertext. These operations include symbol encoding scheme, distortion operations, block shuffling operation, and a highly complicated key sequence generated using a fuzzy neural network model. The technique also employs a chaotic system, which produces key-driven chaotic noises for boosting the effectiveness of encryption operations.

The proposed encryption technique has many features that make it unique and different from other techniques. First, it uses the encryption key in a quite different way. The key is processed using a “one-way" bit-mixing operation that deeply transforms this key to a significantly different output (a sequence of symbols with the same length as the key). Second, its operations are highly sensitive to both the key and plaintext. These operations can detect the variations of the key and plaintext and adjust their functionality while processing the input. This feature makes the proposed technique very different from other techniques (e.g. AES and DES), where some of their operations are only sensitive to the key and some are only sensitive to the plaintext, but none of their operations is sensitive to both.Footnote1 Third, the encryption technique uses a fuzzy neural network to expand the key and generate a hiding code sequence of arbitrary length (the key-code sequence is used for embedding further confusion in the ciphertext). The key-code sequence has almost no trace of the key because it is generated by a fuzzy neural network whose computational behavior is controlled by chaotic weights and updated using a fuzzy update mechanism. The fuzzy neural network assures that the key trace is hidden within the complexity of the computations of the network. Fourth, the encryption technique utilizes chaotic noises generated by the chaotic system whose state is fully driven by the encryption key. These chaotic noises not only increase the effectiveness of the encryption operations but also enable the operations to adjust their state based on the variations of the key.

We present our contributions as follows. Section 2 describes the substitution operation and its inverse. Section 3 presents the technical details of the deep bit mixing operation and Sect. 4 introduces the chaotic system and its use. Sections 5 and 6 present the details of the block manipulation operations. Section 7 presents the details of the fuzzy neural network model for generating key-based hiding codes. Section 8 presents the symbol encoding scheme and Sect. 9 describes how to orchestrate the different operations for building the cipher and its inverse. Section 10 presents the performance analysis results. We conclude and give directions for future work in Sect. 11.

The substitution operation
The substitution operation, SUB(.), substitutes a symbol 𝑏𝑖 with a new symbol using a substitution space. The substitution space (M-TAB) is a 2𝑝2×2𝑝2 box that contains all the possible permutations of p bits. The entries of M-TAB are organized as specified by AES encryption technique [23]. The substitution method substitutes a symbol 𝑏𝑖 with p bits by splitting its bits into a left half l and a right half r, where each half is 𝑝2 bits. The substitution SUB(l, r) returns the content of the cell at the index (l, r) of the M-TAB. The substitution operation has an inverse operation SUB−1(.), which recovers the input symbol. The inverse operation SUB−1(.) substitutes its input exactly as SUB(.) except that it uses the substitution space inverse Inv-M-TAB. The substitution space inverse is created as specified by AES technique.

Deep bit mixing operation
The mixing operation detects any variation in the input block (even a single bit). It effectively propagates this variation to impact every bit in the input block and consequently cause drastic changes to the output. Figure 1a presents the algorithmic steps of the mixing operation.

Fig. 1
figure 1
Deep bit mixing operation (a) and its inverse (b)

Full size image
The operation DeepBitMixing(.) mixes the bits of the input block 𝑘1...𝑘𝑛 and generates a very different output block 𝑘″1...𝑘″𝑛. The logic of this operation consists of two passes: forward-pass and backward-pass. The forward-pass processes the symbols of the input block left-to-right. Each symbol 𝑘𝑖 is first XORed (⨂) with the outcome of substituting the previous symbol 𝑘𝑖−1 and the result of the XOR operation is substituted using the substitution operation SUB(.). The output of the forward-pass is a new block 𝑘′1...𝑘′𝑛. The backward-pass does essentially the same steps except that it processes the input right-to-left. Due to the dual-pass processing, any variation in the input is detected and propagated back-and-forth to affect every bit, causing large variations to the output.

The mixing operation DeepBitMixing(.) has an inverse operation DeepBitMixing-Inv(.) that restores the original input. Figure 1b presents the logic of the inverse operation. The inverse operation processes the input using the same logic as the original operation except (1) the backward-pass is executed first then the forward-pass and (2) the substitution is done using the SUB−1(.) operation (not SUB(.)).

Chirikov chaotic system
The discretized version of the standard Chirikov 2D chaotic system is defined by the following two equations [27].

𝑥𝑖+1=(𝑥𝑖+𝑦𝑖) Mod 𝑁𝑦𝑖+1=(𝑦𝑖+𝐾𝑆𝑖𝑛2𝜋𝑥𝑖+1𝑁) Mod 𝑁
(1)
where N, K are positive integers, 𝑥𝑖,𝑦𝑖∈[0,𝑁), and Mod is the division remainder operation. To operate the chaotic system, we must bind the control variable K and the pair (𝑥0, 𝑦0) to initial values from their proper ranges. From a cryptography perspective, an effective use of Chirikov chaotic system requires a secret and random initialization of the parameters K, 𝑥0, and 𝑦0 with values from their corresponding effective ranges. As reported in the literature [28, 29], the Chirikov system is very sensitive to the initial values of its parameters: a very significant feature that can be exploited to generate chaotic numbers for secure encryption. To leverage this property, the paper proposes a simple but effective method that uses the encryption key for initializing the parameters K, 𝑥0, and 𝑦0. Suppose the key has n symbols 𝑐1𝑐2...𝑐𝑛. The following Eqs. (2), (3) and (4) use the key to compute values for Chirikov’s parameters.

𝑥0=⎡⎣⎢⎢⎛⎝⎜⎜∑𝑖=1𝑛2𝑐′𝑖×Log𝑒⟨[2𝑝]𝑛−𝑖⟩⎞⎠⎟⎟×1014⎤⎦⎥⎥ Mod 𝑁
(2)
𝑦0=⎡⎣⎢⎢⎛⎝⎜⎜∑𝑖=𝑛2+1𝑛𝑐′𝑖×Log𝑒⟨[2𝑝]𝑛−𝑖⟩⎞⎠⎟⎟×1014⎤⎦⎥⎥ Mod 𝑁
(3)
𝐾=Floor((∑𝑖=1𝑛𝑐′𝑖×Log𝑒⟨[2𝑝]𝑛−𝑖⟩)×10𝑚)
(4)
where 𝑐′𝑖(𝑖=1...𝑛) are the symbols that result from processing the key symbols 𝑐𝑖(𝑖=1...𝑛) using the deep bit mixing operation, Log𝑒 is the logarithm of base e, p is the maximum number of bits that represent the used symbols (e.g. 𝑝=8 if we use the symbols from 0...255), and m is the number of decimal digits that constitute the control variable K. The operator Floor(x) returns the closest integer to x.

The Initialize(.) operation (see Fig. 2) illustrates how we exactly use the equations  (2, 3, and 4) to initialize the parameters of the Chirikov chaotic system. The logic of the initialization operation Initialize(.) is straightforward. It processes the encryption key using the deep bit mixing operation DeepBitMixing(.) and uses the appropriate equation to generate an initial value for each parameter. Therefore, it processes the encryption key and uses Eq. (2) to compute an initial value for 𝑥0, then retransforms the new version of the key and uses Eq. (3) to compute an initial value for 𝑦0, and so on.

Fig. 2
figure 2
Routine for initializing the parameters of Chirikov chaotic system

Full size image
After initializing the parameters of the Chirikov chaotic system, the chaotic random numbers (𝑥𝑟,𝑦𝑟) are generated using the algorithmic steps illustrated in Fig. 3.

Fig. 3
figure 3
Chaotic numbers generation

Full size image
Block distortion operation
The block distortion provokes additional confusion to the ciphertext. The computational model of the method is outlined in Fig. 4. The method utilizes two data lists H and V each of size 2𝑝 entries, where p is the maximum number of bits that represent the used symbols. Both lists are filled with the ASCII symbols 0...2𝑝−1 and randomly scattered using two sequences of random numbers. These two lists create a virtual lattice of 2𝑝×2𝑝 entries, where H denotes the rows and V denotes the columns of the lattice.

Fig. 4
figure 4
The block distortion operation logic

Full size image
The block distortion processes a block of symbols 𝑏𝑖(𝑖=1...𝑛) using M rounds. Each round executes two operations: Distort (.) and BlockMix (.). The Distort (.) operation uses the substitution method SUB(.), which substitutes its input as discussed in Sect. 2. It also uses the variable Q (initialized to the outcome of substituting the symbol 𝑏1 of the input block), which serves as a “memory" to carry the impact of the previous symbols to the next ones. The Distort (.) operation processes the input symbols as follows. The first symbol 𝑏1 of the input block is unprocessed. For the remaining symbols 𝑏𝑖(𝑖>1), it selects distortion values from H and V using a data-dependent selection method. To select the distortion values, it uses two indexes 𝐻𝑖 for indexing H and 𝑉𝑖 for indexing V. Both indexes 𝐻𝑖 and 𝑉𝑖 are initialized to 𝑏1, but updated before using them. 𝐻𝑖 and 𝑉𝑖 are updated using the contents of the left, right, bottom, and top cells that are surrounding the cell identified by Q in the substitution space. The cell identified by Q is the cell at (𝑄/2𝑝2,Mod(𝑄,2𝑝2)), where Mod is the division remainder operator. The content L of the left surrounding cell (of Q) is obtained by substituting Q using SUB(𝑄/2𝑝2,Mod(𝑄,2𝑝2−1)) as shown in Fig. 4. The contents R, T, B of respectively the right, top, and bottom surrounding cells are obtained likewise. Using the contents of the four surrounding cells, 𝐻𝑖 is updated by adding to it (to 𝐻𝑖) the content of the right surrounding cell R and then subtracting the content of the left surrounding cell L. Observe that we are trying to capture the intuitive meaning of the left and right surrounding: the right surrounding is toward the higher indexed cell therefore it is added (+) and the left neighbor is toward the lower indexed cell therefore it is subtracted (−). The value 𝑉𝑖 is updated likewise but using the content of the top and bottom surrounding cells T and B.

The input symbol 𝑏𝑖 is distorted by XORing (⨂) it with either the symbol 𝐻[𝐻𝑖] or 𝑉[𝑉𝑖]. The choice depends on the condition “Cond". For this paper, the condition “Cond" is simple but data-dependent: “Cond" is true if the ASCII value of the symbol 𝑏𝑖−1 is even. Therefore, if the symbol 𝑏𝑖−1 is even, 𝑏𝑖 is distorted by XORing it with the symbol 𝐻[𝐻𝑖]. Otherwise, 𝑏𝑖 is distorted by XORing it with the symbol 𝑉[𝑉𝑖].

The value of Q is updated after processing each input symbol (see Distort (.)). The update is performed by XORing the previous value of Q with the input symbol 𝑏𝑖−1 and then substituting the outcome using the substitution operation SUB(.,.). This update of Q is data-dependent and makes Q transfer the effect of the previous input symbols to the following symbols (i.e. it is a memory that transfers the impact of the previous to the next).

To intensify the effectiveness of the distortion, the block is processed by the BlockMix(.) action. This action uses a data-dependent method to change the order of the block symbols. The execution of this action consists of n iterations (n is the size of the block). Each iteration performs two swaps and one cyclic left-shifting. The swaps are executed using the first and second symbols (𝐹=𝑠1, 𝑆=𝑠2) of the input block. The symbols F and S are used to create four values 𝐹1, 𝐹2, 𝑆1, and 𝑆2. 𝐹1 is the decimal value of the leftmost p/2 bits of F and 𝐹2 is the decimal value of the rightmost p/2 bits of F. The values 𝑆1 and 𝑆2 are computed likewise but using the second input symbol S. BlockMix(.) then swaps the symbols at the indexes 𝐹1 and 𝐹2 and also swaps the symbols at the 𝑆1 and 𝑆2. This swap is followed by a cyclic left-shifting for the input block by 2 positions. Note, by changing the order of the block symbols, the outcome of the substitution changes, which highly affects the values for the parameters (e.g. L, R, Q, etc.) of the Distort(.). As a result, the distortion value changes causing a different distortion impact.

Fig. 5
figure 5
The block distortion inverse operation logic

Full size image
The effect of the block distortion operation is reversible. Figure 5 illustrates the logic of the inverse of the distortion operation. The inverse operation executes roughly the same steps with few changes (highlighted in Fig. 5). The order of the operation calls is reversed: first call BlockMix-Inv(.) and then call Distort–Inv(.). The BlockMix-Inv(.) operation differs in the order of the processing steps and the shift direction. The input is right-shifted instead of left-shifted and the order of swapping actions is also different.

Interval shuffling with chaotic locking
This operation shuffles the block symbols and therefore further confuses the ciphertext. Figure 6 illustrates the operation’s processing steps. It maintains three 𝑚×2 boxes for supporting its functionality (the three listed are denoted by INTERVAL, SWAP, and C-INTERLEAVE). Each box is filled with m pairs of chaotic numbers. The interval shuffling operation shuffles the input block using m rounds, where each round executes three operations: Interleave(.), Swap(.) and ChaoticLock(.).

Fig. 6
figure 6
The interval shuffling operation

Full size image
The operation Interleave(.) extracts a subsequence, EXTRACT, designated by two indices INTERVAL[i][0] and INTERVAL[i][1] and interleaves the extracted symbols within the remaining block symbols REMAIN. The interleaving is performed by inserting the symbols of the extracted subsequence in the odd positions of the remaining subsequence. If the extracted subsequence EXTRACT is longer than the remaining symbols, the rest of the extracted subsequence is appended as a suffix to the resulting block. The Swap(.) operation swaps two symbols designated by the indexes SWAP[i][0] and SWAP[i][1].

To break this pattern of repositioning, the shuffling operation uses a chaotic locking method ChaoticLock(.), which performs two steps. First, it uses the rightmost symbol in the block to swap the leftmost symbol as follows. Let 𝑏𝑛 be the rightmost symbol of the block. The operation creates an index 𝑘=𝑀𝑜𝑑(𝑏𝑛,𝑛) and then swaps the leftmost symbol 𝑏1 with the symbol at the index k. Second, the operation swaps the leftmost symbol 𝑏1 with the symbol at the chaotic index q, which is computed from two chaotic values C-INTERLEAVE[i][0] and C-INTERLEAVE[i][1] using q=(C-INTERLEAVE[i][0] ⨂ C-INTERLEAVE[i][1]) Mod n.

Although the operation illusively scatters the symbols of the block, it is reversible. The original block can be straightforwardly recovered using the operation delineated in Fig. 7. The recovery requires reversing the order of executing the internal methods (Interleave(.), Swap(.), and ChaoticLock(.)) and some modifications to Interleave(.) and ChaoticLock(.)—the modified versions are termed Interleave-Inv(.) and ChaoticLock-Inv(.). Also, the shuffling inverse uses the three lists (INTERVAL, SWAP, C-INTERLEAVE) backward—from the last entry down to the first.

Fig. 7
figure 7
The interval shuffling operation inverse

Full size image
Neural network code generation module
Critical to the encryption methods is how the impact of the key is produced and embedded.Footnote2 An effective way to embed the key impact is through an XOR operation as adopted by many encryption methods (e.g. AES). In this way, each symbol of the ciphertext is XORed with a symbol of the key impact. The process of producing the key impact is also very critical for two reasons. First, this process must be effective to hide the identity of the key. Second and equally important, the generated key impact must have a high avalanche effect so that it boosts the security of the resulting ciphertext. This section presents the details of a process that uses the key to generate a sequence of key codes (key impact) of an arbitrary length. The process adopts a novel computational model that is based on the fuzzy use of the neural network. The model hides the key through enormously complicated neural network computations and effective diffusion. The output of the process is a key impact, which is an arbitrarily long sequence of hiding codes.

Neural network deep transformation
The neural network is a set of neurons that are interconnected to form a network. It typically consists of an input layer, which could be one or more input variables, one or more hidden layers, and an output layer. The interconnections are designated with signals (or weights) that specify the importance of a neuron in a previous layer i to a neuron in the next layer i+1.

All neurons perform pretty the same calculations. They produce their output 𝑂𝑖,𝑗 through the summation defined by equation (5).

𝑂𝑖,𝑗=∑𝑘=1𝑛𝑤𝑖,𝑗𝑘,𝑗−1×𝑃𝑘,𝑗−1
(5)
where 𝑂𝑖,𝑗 is the output of the neuron i in the layer j, 𝑤𝑖,𝑗𝑘,𝑗−1 are the signals that are passed from neurons k in the previous layer 𝑗−1 to the neuron i in the next layer j, and 𝑃𝑘,𝑗−1 is the current value of the neuron k in the previous layer 𝑗−1.

Although most of the neural network use is for dealing with either classification problems or mirroring [30,31,32], we use the neural network for a totally different goal (see Fig. 8). Namely, we use it for performing deep transformations that shift a sequence of input symbols 𝑥𝑖’s from some domain to an entirely different domain in which the relation to the input symbols is untraceable. Regardless of its application, the neural network undergoes a set of updates to the interconnection-signals until achieving “Good behavior". Good behavior is measured by the ability of the neural network to process the input and produce the desired output, where the desired output depends on the application. For instance, in classification problems, the desired output is to have the neural network correctly classify a high percentage of the input objects while, in a mirroring neural network, the desired output is to have the output exactly match the input. That is, the desired output is pretty binary. We use the neural network similarly, but the coveted output is defined in a rather fuzzy way. In particular, the desired output is determined based on the transformation intensity value 𝑇𝑖, which represents the discrepancy between the input 𝑥𝑖 and the code symbol 𝐶𝐻𝑖, and is computed using 𝑇𝑖=|𝑥𝑖−𝐶𝐻𝑖|2𝑝, where |x| is the absolute value for x. (Note based on the formula : 0≤𝑇𝑖≤1.) Accordingly, the output of the neural network 𝐶𝐻𝑖 is considered to be good to a degree 𝑇𝑖 if the transformation intensity 𝑇𝑖 is in the interval [𝜀 , 1], where 𝜀>0. When the transformation intensity 𝑇𝑖 falls below the threshold 𝜀, the neural network is considered incapable of delivering the desired transformation intensity.

Fig. 8
figure 8
The fuzzy neural network hiding code generation process

Full size image
The interconnection signals (or weights) may be updated after processing each input symbol 𝑥𝑖 as follows. If the network is incapable (i.e. 𝑇𝑖<𝜀), all of the signals must be updated to new chaotic values. If the transformation intensity is within the desired range (i.e. the neural network is effective), the signals’ update is stochastic with a probability 𝜓. (We typically set 𝜓 to a small value, say 0.1, just to introduce more confusion to the computations of the neural network.) The stochastic update is implemented using a Boolean list STOCH of 2𝑝 entries. The first 𝑧=𝜓×2𝑝 entries of the list are filled with TRUE (stochastic update should occur) and the rest with FALSE (no update). We randomly select one of the neurons in either of the hidden layers and use its computed value, say 𝛼, to compute two different values 𝜅1 and 𝜅2 as specified in equation (6). The value 𝜅1 indexes one of the list STOCH’s entries. The stochastic update should execute only if the indexed entry is a TRUE value; no update if the indexed entry is a FALSE value.

𝜅1=Mod (𝛼,2𝑝)𝜅2=Mod [floor(𝛼×107),2𝑝]
(6)
when the indexed entry is TRUE, two actions are executed. First, some entries of the STOCH list are reordered. (Note this means that the order of the entries of the list is changed after every stochastic update invocation.) Namely, the entries STOCH[𝜅1] and STOCH[𝜅2] are swapped. Second, some signals of the neural network are updated. The number of signals to update is inversely proportional to the transformation intensity 𝑇𝑖: higher intensity entails fewer signals to update. The intuition is that if the transformation intensity is high, the neural network is effective (capable) and thus there is no need to perform drastic changes to the current signals. The number of signals to update is calculated using 𝑛=𝑚×(1−𝑇𝑖), where m is the total number of signals in the neural network and 𝑇𝑖 is the transformation intensity. The n signals are selected randomly and updated to new values obtained from the chaotic system.

Hiding code generation
The initial configuration of the neural network interconnection is defined by a set of real numbers obtained from the chaotic system. The initial input to the neural network is the encryption key (n symbols) after being processed by the deep bit mixing operation. Processing the key by the deep bit mixing operation before using it is very fundamental. It hides the identity of the key due to the deep manipulation. It also propagates the bit variations (if any) to impact every symbol in the output, causing the output to greatly vary for even a minor bit-variation.

When the neural network processes an input symbol 𝑥𝑖, the output is real numbers 𝐷𝑇. The output values 𝐷𝑇 are used to create hiding code symbols 𝐶𝐻𝑖 using: 𝐶𝐻𝑖=floor[Mod(𝐷𝑇,2𝑝)]. (The symbols 𝐶𝐻𝑖’s are what we called the key impact.) For the sake of producing a longer sequence of hiding codes if needed, the neural network computes the integer numbers 𝑢𝑖 by taking the fraction part of the output values 𝐷𝑇 (without the decimal point) module 2𝑝. That is, 𝑢𝑖=Mod[frac(𝐷𝑇),2𝑃)]. Each integer 𝑢𝑖 is XORed with an additional value 𝑣𝑖, where 𝑣𝑖 is the value of the neuron that is randomly picked from the neurons of the second hidden layer. Note that the involvement of the value 𝑣𝑖 in computing the input symbols 𝑥𝑖 adds extra confusion to the computation of the new inputs. The outcome of the XOR operation is a new sequence of n values 𝑥𝑖, which can be used for generating a longer sequence of hiding codes (if needed).

Before we leave this section, we emphasize that the computation of the neural network is fuzzy. As discussed, the neural network computation model may perform two updates: either a total update (all the signals are updated) or a partial update (only some randomly selected signals are updated). When the neural network is incapable, the update is only total—no partial update. When the network is still capable, there may be a partial update depending on the stochastic process. When the partial update is triggered, it updates some of the signals and changes the probability model that controls the stochastic process (represented by the list STOCH). Due to the changes in the stochastic probability model, whether the same symbol triggers the partial update or not depends on the current state of the STOCH and the current values of the neurons (neural network state). This type of computation has two impacts. First, when the neural network processes a symbol x, the computational behavior of the neural network is not definite because the outcome of the processing depends on whether the neural network is updated or not. Second, the same input symbol may lead the neural network to a total update or not depending on the state of the neural network. As such, the computations of the neural network are fuzzy because they may not produce the same outcome even for the same input symbol.

Symbol encoding scheme
The encoding scheme uses data-dependent computations to encode plaintext input symbols 𝑏1𝑏2...𝑏𝑛 and shift them to a different symbol space 𝑐1𝑐2...𝑐𝑛. Figure 9 outlines the logic of the encoding scheme. The scheme uses encoders to encode each symbol 𝑏𝑖 by mixing it with a distortion value 𝑘𝑖.

Encoders
Encoders are operations that perform distinct impacts on their input. Table 1 shows the used encoders. The encoder 𝑋𝑂𝑅(𝑏𝑖,𝑘𝑖) performs an XOR operation between the input symbol 𝑏𝑖 and the distortion symbol 𝑘𝑖. The encoder 𝐿𝑅𝑋[𝑚](𝑏𝑖,𝑘𝑖) left rotates the input symbol 𝑏𝑖 by m positions and then XORes the outcome with the distortion symbol 𝑘𝑖. (m=1, 2, ..., p–1, where p is the number of bits that represent a symbol.) The encoder ⨀(𝑏𝑖,𝑘𝑖) multiplies 𝑏𝑖 and 𝑘𝑖 using the multiplication under Galois field 𝐺𝐹(2𝑝). The encoder 𝑇𝑋[𝑠,𝑙,𝑗,𝑄](𝑏𝑖,𝑘𝑖) breaks the structure of the input symbol 𝑏𝑖 by extracting a selected subsequence of its bits and appending it as a prefix or a suffix to the remaining bits (of the original symbol). The encoder extracts l bits starting from the index s. It may further process the selected subsequence based on the directives defined in Q. In particular, the encoder flips the bits of the selected subsequence if the directive is “Flip", reverses the order of the bits of the subsequence if the directive is “Reverse", or leaves the subsequence unprocessed if the directive is “NoOp". Once the subsequence is processed, the encoder appends the subsequence to the remaining bits as a suffix or a prefix based on the value of j, where 𝑗∈{Suffix, Prefix}. Finally, the processed symbol is XORed with the distortion symbol 𝑘𝑖. Figure 10 provides an illustration of the 𝑇𝑋 functionality.

Fig. 9
figure 9
Symbol encoding scheme

Full size image
Table 1 The fusing encoders
Full size table
Fig. 10
figure 10
An example of the 𝑇𝑋 operator computations

Full size image
Each encoder in Table 1 has a decoder that reverses its impact and restores the original symbol. The encoder 𝐿𝑅𝑋[𝑚](r, k) effect is reversed by first XORing the input symbol r with the symbol k and then right rotate the outcome of the XOR by m positions. The encoder XOR (r, k) impact is reversed by XORing the input symbol r with k. The Galois multiplication encoder ⨀(𝑟,𝑘) can reversed by dividing the input symbol r by k using the Galois division under the field 𝐺𝐹(2𝑝). The encoder 𝑇𝑋[s, l, j, Q](r, k) impact is more complicated and is reversed by the algorithmic steps described in Fig. 11.

Fig. 11
figure 11
Inverse of 𝑇𝑋 encoder

Full size image
Fusing module
The fundamental step of the fusing module is how the distortion symbol 𝑘𝑖 is computed and the encoder is selected. Referring to Fig. 9, the interceptor controls the input to the substitution method using a 2–1 multiplexer. It maintains a Mutex list and the state variables IR, 𝜓, and Ω all initialized to chaotic values. The Mutex list consists of 2𝑝 entries, where each entry is populated with either “B" or “I" alternatively, until it is filled up. The variable Ω is updated by XORing it with a value 𝛽𝑖 (computed as specified next). The variable IR is updated by XORing it with the value Ω. The variable 𝜓 is updated by adding to it the value of the rightmost 3 bits of Ω. The interceptor controls the functionality of the multiplexer through the enabling variable EN, which receives a value (“B" or “I") by indexing Mutex list using the value “Ω+𝜓". The multiplexer allows the input symbol 𝑏𝑖−1 to pass to the output variable 𝑑𝑖 if EN received “B" and allows the current value of the variable IR to pass if EN received “I".

The output of the multiplexer 𝑑𝑖 is mapped to the substitution space (M-TAB) in a typical way. The outcome of the mapping is a triple (𝛼𝑖, 𝛽𝑖, 𝛾𝑖). The value 𝛼𝑖 is the content of the cell indexed by 𝑑𝑖 while 𝛽𝑖 and 𝛾𝑖 are the contents of respectively the left and bottom cells of the indexed cell. The value 𝛼𝑖 is used to index and retrieve an encoder. The value 𝛽𝑖 is used to update the variable Ω using an XOR operation. The value 𝛾𝑖 is used to update the variable G, which will be used for producing the distortion value 𝑘𝑖. Updating the variable G is done using the operator 𝐿𝑗 ⨂ (G, 𝛾𝑖), where ⨂ is an XOR operator and j is the value of the rightmost three bits of 𝛽𝑖. The operator 𝐿𝑗 ⨂ (G, 𝛾𝑖) functions as follows: left shift G by j positions and then XOR the result with the value 𝛾𝑖. The updated G is used to produce the distortion value 𝑘𝑖 using the formula “Mod(G, 2𝑝)", where Mod is the division remainder.

The fusing module executes the retrieved encoder on the input symbols 𝑏𝑖 and the distortion value 𝑘𝑖 to produce the coded symbol 𝑐𝑖. When encoding the first input symbol 𝑏1 in the plaintext, the symbol 𝑏𝑖−1 (input to the multiplexer) is NULL. Encoding the other input symbols 𝑏𝑖 (𝑖≥1), however, may be impacted by the previous input symbol 𝑏𝑖.

The fusing model can also restore the original block 𝑏1𝑏2...𝑏𝑛 from the encoded one 𝑐1𝑐2...𝑐𝑛. In this case, the fusing model uses the inverse encoders for decoding rather than the encoders themselves.

Before concluding this section, we emphasize that the computations of the encoding scheme are data-dependent. First, the input symbol 𝑏𝑖−1 influences the behavior of the interceptor because 𝑏𝑖−1 is used to produce the triple (𝛼𝑖, 𝛽𝑖, 𝛾𝑖), which is used to compute the parameters of the interceptor. Second, the input symbol 𝑏𝑖−1 influences the computation of the distortion symbol 𝑘𝑖 because this computation relies on the triple, which is influenced by the input symbol 𝑏𝑖−1. Third, the input symbol 𝑏𝑖−1 influences the selection of the encoders. As such, since the input symbol 𝑏𝑖−1 influences the selection of the encoders and the distortion values, it necessarily affects the computation of the output symbols 𝑐𝑖. As such, the variations in the plaintext necessarily result in variations in the functional behavior of the encoding scheme and thus variations in its final output.

Encryption/decryption process
The encryption process is outlined in Fig. 12. The chaotic system uses the key to generate two streams of chaotic numbers. The Y-Stream is directed to the hiding code generation operation. The X-Stream is directed to support the functionality of the symbol encoding scheme, block distortion, and interval shuffling operations.Footnote3 The hiding code generation receives the encryption key and the Y-Stream and uses the fuzzy neural network module to produce the key-based hiding codes. Plaintext blocks are processed in W rounds. In each round, the encryption process executes four operations: (1) deep bit mixing, (2) symbol encoding scheme, (3) block distortion, and (4) interval shuffling. These four operations work collectively to highly secure the ciphertext. Every operation adds more confusion to the ciphertext to increase its ability to withstand hacking techniques. The “bit mixing operation" increases the confusion in the ciphertext by increasing the avalanche effect. The “symbol encoding scheme" is a nonlinear operation that transforms the symbols to a different space (thus complicating the relation to the original input—high confusion). The “block distortion" confuses the ciphertext by impacting the bits of each input symbol through XORing the input symbol with a symbol that is computed using a complicated process. The “interval shuffling operation" induces additional confusion at the macro-level (i.e. by changing the order of the block’s symbols using a data-dependent process). The output of the W rounds is pre-ciphertext. This pre-ciphertext receives additional confusion by the “hiding code-mixing" operation. This operation XORes the symbols of the pre-ciphertext with the symbols of the hiding codes (generated using the fuzzy neural network).

Fig. 12
figure 12
The encryption process

Full size image
The decryption process is illustrated in Fig. 13. It uses almost identical logic except that the ciphertext is processed first by the “hiding code-mixing" operation. The output of the hiding code-mixing is a pre-ciphertext, which is then processed in W rounds. In each round, the decryption process executes the inverse of the encryption operations in a reversed order: (1) interval shuffling inverse, (2) block distortion inverse, (3) symbol encoding scheme inverse, and (4) deep bit mixing inverse. The output of the W rounds is the plaintext.

Fig. 13
figure 13
The decryption process

Full size image
Performance analysis
This section analyzes the performance of the proposed encryption technique. We firstly analyze the performance of two fundamental components that highly impact the performance of the encryption technique. The deep bit mixing operation greatly boosts the ciphertext confusion (Sect. 10.1). The neural network hiding code module props up the ciphertext security by generating complicated codes to embed the symbols of the ciphertext (Sect. 10.2). We secondly analyze the performance of the encryption technique using standard testing methods (Sect. 10.3). We finally estimate the time complexity of the proposed encryption technique and compare its performance with other encryption techniques (Sect. 10.4).

Deep bit mixing process
As discussed earlier, the deep bit mixing is a major source of confusion. Due to its importance to the encryption process, we analyzed the confusion that it could induce measured by the avalanche effect [33]. Effective confusion technique must have a high avalanche effect: changing a bit or more in the input must cause huge changes to the output. We used 128-bit and 256-bit sequences of all zeros (low entropy).Footnote4 These two sequences are the base input. We derived perturbed sequences by flipping bits of the base input (flip zero to one). Due to the prohibitively large number of possible perturbed sequences, we flipped only a specific number of bits and in random positions of the base input. Table 2 presents the number of flipped bits “f-bits", the number of perturbed sequences of sizes 128 “p(128)" and 256 “p(256)" that were derived by flipping the corresponding number of bits in random positions of the base input. For instance, we derived 128 and 256 sequences by flipping only one bit in respectively the 128-bit and 256-bit base inputs.

Table 2 Perturbed sequences created from the base input
Full size table
We then processed the two base inputs and each perturbed sequence of both sizes (128 and 256) using the deep bit mixing operation. When processing each input, the deep bit mixing performed only two rounds. The number of different bits between the output of processing the base input and the output of processing each of its respective processed perturbed sequences were recorded. Figure 14a, b show the number of bit differences in terms of min, max, and average. As the figures show, the average number of bit differences is more than half of the bits in both 128 bits and 256 bits input. The minimum number of bit difference exceeds 49 bits while the maximum exceeds 77 bits for 128 bits input. In the case of 256 bits input, the minimum number of bit difference exceeds 126 and the maximum exceeds 174 bits. Also, Fig. 14a, b show that the bit difference between the base input and its respective perturbed sequences becomes ever more significant when the size of the input is larger. This sounds logical because more bits enable the deep bit mixing technique to induce sharper changes. As clearly stated in [34], based on the average number of bit differences, the proposed deep bit mixing has a secure and large avalanche effect.

Fig. 14
figure 14
Avalanche effect of the deep bit mixing technique

Full size image
We further investigated the impact of increasing the number of rounds on the avalanche effect. For this purpose, we randomly selected 100 sequences from each set of perturbed sequences. The selected sequences along with the base input are processed by executing the deep bit mixing operation a specific number of rounds. Figure 15a, b show the number of bit differences in terms of min, average, and max as the number of rounds increases. These figures provide no significant evidence that increasing the number of rounds beyond 2 will improve the avalanche effect. Although the figures show that one round may produce a reasonable bit difference, more than one round is highly recommended to induce a higher avalanche effect. The need for more than one round is well justified by the entropy. Figure 16 shows that the average entropy value increases as the number of rounds increase. Starting from the second round, the entropy value starts reaching the optimal value for 256-bit sequences (the optimal value is 8 because we use the ASCII’s within the range [0...255]). The average entropy for 128-bit sequences starts reaching the optimal value at round 3, however.

Fig. 15
figure 15
Impact of increasing the number of rounds on the bit differences for both 128 bits and 256 bits inputs

Full size image
Fig. 16
figure 16
Average entropy trend as a function of number of rounds

Full size image
Neural network code generation module
We randomly selected 512 sequences from those processed by the deep bit mixing operation using two rounds (Sect. 10.1). These 512 sequences include 256 sequences of length 128 bits and 256 sequences of length 256 bits. Each sequence is passed to the neural network to generate hiding code sequences of 1024 bits. Since the length of the input sequences is less than 1024 bits, the neural network used the sequences computed during producing the output sequence (as described in Sect. 7) to produce the desired output sequences (1024 bits). For instance, to generate a sequence of 1024 bits from each 128-bit input sequence, the neural network created 7 additional sequences each of size 128 bits. Each of the original sequence and the 7 created sequences is passed as an input to the neural network to generate an output sequence of 1024 bits hiding code.

Fig. 17
figure 17
The avalanche effect of the neural network hiding code generation module

Full size image
The effectiveness of the neural network code is measured by the avalanche effect (the number of bit-differences between the 1024-bit input sequence and the 1024-bit output sequence). Figure 17a shows the bit-difference between an input and the resulting output (hiding code), for both 128 and 256 bits. The average bit difference is above 800 bits (out of 1024) while the minimum is greater than 790 bits. This is quite a large avalanche effect. Figure 17b shows more statistics about the performance. Besides the average, it shows the standard deviation, 95% confidence interval around the average, and the average entropy. Note that out of 100 sequences, at least 95% of the input, sequences will have a bit-difference (between each input and its resulting hiding code) within the designated confidence interval. Additionally, the average entropy is close to 1 (the ideal value). All these numbers provide evidence that the neural network hiding code has a really large avalanche effect and thus it largely props up the security of the ultimate ciphertext. This effectiveness is attributed to the fact that the neural network uses a fuzzy way to update its state. As described in Sect. 7, the neural network continuously observes its output and performs chaotic updates to its signals if the transformation becomes weak (measured by the difference between the input and the output). Even more, regardless of the quality of the transformation, the neural network may perform stochastic updates with a given probability (please see Sect. 7). These stochastic updates induce sharp noises to the output.

Encryption technique performance analysis
We used the NIST [35] and ENT [36] batteries of randomness tests for assessing the performance of the proposed technique. In our evaluation, we strictly adhered to the NIST guidelines when preparing the data sets. As stated in [37], the data set must include low entropy, high entropy, and random plaintexts/ keys. Table 3 shows the initial configuration of the different parameters of the encryption technique operations. Since we are not aware of any analytical method that can determine proper values for the parameters, we had to estimate them using simulations. We simulated the encryption technique over 1000 times using different values for the parameters. During the simulation, we found the values in Table 3 good choices because they enable the encryption technique to produce random ciphertexts and do not have a significant impact on the execution time.

Table 3 The parameter initialization of the encryption technique parameters
Full size table
Data set preparation
Based on the NIST’s framework guidelines [37], a comprehensive evaluation must include the following data sets.

1.
Key avalanche data set How key’s variations influence the randomness of the ciphertexts for a fixed plaintext.

2.
Plaintext avalanche data set How plaintext’s variations influence the ciphertexts for a fixed key.

3.
Plaintext/ciphertext correlation data set Measures correlations that could appear between plaintext/ciphertext pairs (high correlation=bad security).

The key avalanche data set includes 1000 sequences of size 1,048,576 bits each. Each sequence was created using low entropy plaintext (8,192 bits) of all zeros and 1000 keys each of size 128 bits.Footnote5 Each sequence is the concatenation of 128 derived blocks created as follows. Each derived block is constructed by XORing the ciphertext created using the fixed plaintext and the 128-bit key with the ciphertext created using the fixed plaintext and the perturbed 128-bit key with the ith bit was modified, for 1≤𝑖≤128. The plaintext avalanche data set includes 1000 sequences of size 1,048,576 bits each. These sequences were created using low entropy 128-bit key of all zeros and 1000 plaintexts of size 8192 bits.Footnote6 Each sequence was created by concatenating 8,192 derived blocks constructed using the same procedure we used to create the previous set with the difference that we perturbed the bits of the plaintext instead of the key (the key is fixed). The plaintext/ciphertext correlation data set includes 250 sequences of size 1,048,576 bits per a sequence, which are created using 128 plaintext blocks (8192 bits each) and 250 random 128-bit keys. (The blocks and keys are chosen from the 1000 blocks/keys previously used.) Each sequence is created by concatenating 128 derived blocks of 8192 bits each. A derived block is created by XORing the plaintext block and its respective ciphertext block. Using the same 128 plaintext blocks, the process is repeated 249 times (one time for every additional random 128-bit key).

Randomness test results
Tables 4, 5, and 6 present the results of NIST’s randomness tests for plaintext, key, and plaintext/ciphertext correlation data sets. In all tables, the results are presented in terms of number/ratio of success “Success(%)", number of low entropy sequences compared to the total number of sequences that failed a particular test “LeF/TF", and the maximum expected number of sequences that may fail a test given a 0.05 significant level “Max Fail".Footnote7

Referring to the tables, the success ratio in all tables exceeds 91%. Also, the sequences that actually failed a test are fewer than the expected in all cases but one case in Table 5. For this case, a total of 78 sequences failed “Spectral" test, which exceeds the maximum expected to number 70.7. Interestingly, the number of low entropy sequences that failed any randomness test is not remarkably large as it may be expected. (We mean by low entropy sequences those sequences that were created using either low entropy plaintext or low entropy key.) In most of the cases, the ratio of the low entropy sequences that failed a test to the total number of sequences that failed a test is less 40%.

Table 4 NIST’s random test figures: plaintext Avalanche
Full size table
Table 5 NIST’s random test figures: key Avalanche
Full size table
Table 6 NIST’s random test figures: plaintext/Cipheredtext Correlation
Full size table
For the sake of further evaluating the proposed technique, we conducted ENT randomness tests on the same bit sequences used for NIST test. Tables 7, 8, and 9 show the result of five important ENT randomness tests. The numbers in the tables represent the average. As the tables show, the entropy is close to 1 (the ideal values since we use bit sequence), the Chi-square values indicate that the sequences are random, the estimation for 𝜋 is close to the actual value with some error (please see [36] for ENT test values interpretation). The serial correlation coefficient is really small and the arithmetic mean is close to the desired value 0.5. These ENT’s test results indicate that the ciphertexts do not deviate from randomness.

Table 7 ENT’s randomness tests: plaintext Avalanche
Full size table
Table 8 ENT’s randomness tests: key Avalanche
Full size table
Table 9 ENT’s randomness tests: plaintext/Ciphertext Correlation
Full size table
Time complexity analysis
The encryption technique must have a low time complexity (i.e. fast) to be usable. All the encryption operations are shown in Fig. 12 use simple processing operators. The most used operation is the XOR logical operator, which is known for its time simplicity. The substitution operation is a lookup table, which can do the substitution in O(1) operation (Big-O). The fuzzy neural network uses linear computations (mainly additions and multiplications), which all result in linear time complexity. The interval shuffling operation is also a simple operation because it performs simple symbol interleaving and swapping—both are executed on specific indices. The symbol encoding scheme involves simple table lookup and XOR operations.

To provide a concrete estimation of the running time, we encrypted 13 plaintexts of sizes ranging from 256 to 512,000 bytes. The encryption was executed on Intel i5 with 4 GB memory. Figure 18 depicts the execution time as a function of the input size. The figure shows a roughly linear but slow increase in the execution time. Table 10 compares between the proposed technique and some encryption techniques. We compared the proposed technique against these encryption techniques because they had a superior execution time based on the comparison in [38, 39]. As Table 10 illustrates, the execution time of the proposed technique is comparable to the execution time of the other techniques. The proposed technique has an even better execution time in some cases. This means that the proposed technique has a very practical execution time and is usable.

Fig. 18
figure 18
The increase in the execution time as a function of the input size

Full size image
Table 10 Comparing time complexity of the proposed technique versus other encryption techniques (time in milliseconds)
Full size table
Discussion
Based on NIST framework, an encryption technique is effective and secure if it passes the NIST’s battery of randomness tests. This framework was used to evaluate the standard encryption algorithm such as AES and DES. According to this framework and the results in previous Sects. 10.1, 10.2, and 10.3, the proposed encryption technique is effective and secure. A high percentage of the ciphertext sequences passed the standard randomness tests (NIST and ENT). The number of sequences that failed a particular randomness test is less than the maximum expected number. Furthermore, the proposed technique outperformed the performance of the many encryption techniques in the literature. Authors [43], reported the performance for many techniques including DES, AES, and [44]. The performance is reported in terms of entropy, serial correlation, and arithmetic mean. The entropy, serial correlation, and arithmetic mean values in Tables 7, 8, and 9 are better than the reported values. For instance, AES has an entropy of 0.99436 and a serial correlation of 0.002100. The minimum entropy for the proposed technique was 0.9999991 (closer to the ideal value 1) and a serial correlation of 0.00041 (closer to the ideal value 0).

This effectiveness can be attributed to the robustness of the different operations that collectively produce the final ciphertext and the way these operations process the input. These operations brutally transform the input by imposing deep micro/macro changes along with space shifting. Along with this brutal transformation, the encryption technique adopts a lookback mechanism that makes variations of the previous blocks influence the processing of the next blocks. The block distortion and symbol encoding operations not only use the plaintext and key to influence the encryption, but these operations are highly sensitive to the plaintext’s/key’s changes. Accordingly, minor changes to plaintext/key would absolutely result in greatly induced confusion (reflected by sharp changes to the output). The fuzzy update mechanism for signals and the complexity of the computation of the neural network hiding code module highly contributed to the randomness (and therefore to the security) of the ciphertext. The deep bit mixing operation also contributed to the effectiveness of the encryption technique by boosting the avalanche effect and entropy.

Although other encryption techniques passed randomness tests, the proposed technique has several innovative computations that tip the balance in its favor. First, it is a light-weighted process, which is a very important feature, especially for network communications. Second, the indirect dependency on the encryption key has many security advantages. These advantages include preserving the privacy of the key and making changes to the key greatly affect the output of the encryption. Third, in addition to the impact of the key, the encryption technique depends on plaintext. The encryption operations are so sensitive to the changes of the plaintext. This means that small changes to any previous plaintext symbol would cause tremendous changes in the state of the operations and consequently would cause drastic variations to the output. Fourth, using the chaotic system as a source of noise absolutely complicates the relation of the ciphertext to both the encryption key and the plaintext. Hence, it is infeasible to predict the key from the output because the trace of the key is hidden in enormously complicated computations. Fifth, even though the neural network has been exploited in encryption, the proposed technique uses this complicated computational model quite differently and effectively. It uses a fuzzy model for updating the signals. Such a type of update induces large noises in the already-complicated computation of the neural network since the internal neuron computations are never deterministic and vary based on the state of the input (input symbols). As a consequence, the hiding code (output of the neural network) is enormously complicated, which ensures highly secure codes to hide the ciphertext.

Concluding notes and future work
The paper proposed a technique that exploits all the input facets (keys and plaintext) along with the chaotic noises to highly secure the ciphertext. The state of the technique is key-driven and plaintext-driven and is very sensitive to the variations in their bits. The key also impacts the chaotic system, which in turn highly affects the encryption operations and the fuzzy neural network module. In such key/plaintext-driven dynamics, the state of the encryption technique is too complicated and unlikely to be predictable.

The sensitivity of the encryption operations to the key, plaintext, and chaotic noises and how the key is used make the proposed technique new and innovative. None of the encryption techniques has this degree of dependency on the key/plaintext like ours. None of them uses chaotic but key-controlled noises to fine-tune the computations of the technique during processing the input.

The security evaluation using rigorous and effective testing tools (NIST and ENT randomness tests) showed that the technique is effective. The time complexity analysis showed that the technique has a very acceptable execution time that is comparable to the execution time of the state-of-the-art techniques (the proposed technique has even a better execution time in some cases).

Although the proposed technique showed a high performance, more testing is needed. We are planning to use larger sets of keys and plaintexts. We are also planning to embed chaotic noises to the computational model of the deep bit mixing operation and evaluate their impact on the avalanche effect and the entropy. These important tasks are left for our future work, however.

Notes
In AES, for instance, the key round operation is sensitive only to the key while the transformation operations are sensitive only to the plaintext.

We mean by key impact a sequence of symbols generated from the key using a specific method.

Recall the chaotic system (Fig. 3) generates two chaotic numbers 𝑥𝑟 and 𝑦𝑟 in each iteration. The X-Stream denotes 𝑥𝑟’s and Y-Stream denotes 𝑦𝑟’s.

The input is intentionally low entropy because inducing high confusion in such an input is very challenging.

The keys are (1) 256 keys each with 16 identical ASCII codes from 0 to 255, (2) 238 keys created by our students, and (3) 506 randomly generated.

To cover the variations of the plaintext cases, these plaintexts include: (1) 256 plaintext each with 8192 identical ASCII codes from 0 to 255, (2) 256 copied from different articles in Wikipedia, and (3) 488 randomly generated.

The maximum expected number of sequences that may fail a random test given some significant level 𝛼 is computed based on the NIST’s recommendation using the formula: Max Fail =𝑆.(𝛼+3.𝛼(1−𝛼)𝑆‾‾‾‾‾‾√). Here S is the total number of tested sequences and 𝛼 is the significance level.

Keywords
Encryption
Key-based induced dynamics
Neural network
Fuzzy behavior
Distortion techniques
Dynamic symbol coding
Chaotic systems