Real-world graphs are generally generated from highly entangled latent factors. However, existing deep learning methods for graph-structured data often ignore such entanglement and simply denote the heterogeneous relations between entities as binary edges. In this paper, we propose a novel Multi-level Disentanglement Graph Neural Network (MD-GNN), a unified framework that simultaneously implements edge-level, attribute-level, and node-level disentanglement in an end-to-end manner. MD-GNN takes the original graph structure and node attributes as input and outputs multiple disentangled relation graphs and disentangled node representations. Specifically, MD-GNN first disentangles the original graph structure into multiple relation graphs, each of which corresponds to a latent and disentangled relation among entities. The input node attributes are then propagated in the corresponding relation graph through a multi-hop diffusion mechanism to capture long-range dependencies between entities, and finally the disentangled node representations are obtained through information aggregation and merging. Extensive experiments on synthetic and real-world datasets have shown qualitatively and quantitatively that MD-GNN yields truly encouraging results in terms of disentanglement and also serves well as a general GNN framework for downstream tasks. Code has been made available at: https://github.com/LirongWu/MD-GNN.

Introduction
The purpose of disentanglement is to decompose an entity, such as a feature vector, into several interpretable components to better understand and explain the behavior of a learned model. Recently, there have been many methods proposed to solve the disentanglement problem with promising results. For example, Variational AutoEncoder (VAE) [13] constrains the distribution of latent features to be Gaussian and generates disentangled representations. However, most previous efforts focused on the disentanglement for Convolutional Neural Networks (CNNs) [2, 7], and few endeavors have been made on the disentanglement of irregular non-Euclidean domains, such as structural graph data.

In many real-world applications, including chemical molecules, social networks, and citation networks, data can be naturally modeled as graphs. Recently, Graph Neural Networks (GNNs), especially Graph Convolutional Networks (GCN) [14], have demonstrated their powerful potential to solve graph-related problems, such as community detection [20, 32] and anomaly detection [25]. DisenGCN [24], a pioneering work in graph disentanglement, focuses on producing independent latent features for node-level disentanglement, but without treatment of the underlying relations between entities. These relations are in many cases heterogeneous, but entangled together and denoted merely as a single bare binary-valued edge. However, edges often contain rich relation information, not just binary indicators of structural connectivity, which motivates us to implicitly uncover latent relations between entities via edge-level disentanglement. More importantly, the attributes of each entity are also highly entangled and usually aggregated and transformed as a whole. However, the attributes are generally associated with different relations to indicate what type of proximity exists between entities, which helps us to explain the interactions between entities for better attribute-level disentanglement.

Fig. 1
figure 1
A motivating example of our work. We take common interests between users as relations (marked by lines with different colors and widths) and the interaction frequency of textual content as attributes (visualized as point clouds)

Full size image
Figure 1 shows a motivating example to illustrate the concept of disentanglement. It can be seen that all four users share common interests in different aspects of Artificial Intelligence (AI), but their interactions have distinctly different focuses. For example, the interaction between John and Ally is mainly about “Detection, Tracking, ImageNet", reflecting their common interest in the topic of “Computer Vision", while John and Ben interact more on “Reinforcement Learning". Without such fine-grained relations and attributes, it is difficult to accurately characterize nodes solely from binary graph structure, and this hinders both model interpretability and performance on downstream tasks.

In this paper, we propose a novel Multi-level Disentanglement Graph Neural Network (MD-GNN) framework to simultaneously achieve edge-level, attribute-level, and node-level disentanglement in an end-to-end manner. MD-GNN decomposes each binary-valued edge into a feature vector, with each dimension corresponding to a disentangled relation subspace. Meanwhile, the attention mechanisms and orthogonal constraints are applied to help locate the attributes associated with each relation space in the input. Moreover, to capture long-range dependencies between nodes, we introduce a relation diffusion mechanism to expand the receptive field to multi-hop neighbors in each relation space within a single layer. Finally, features are aggregated and transformed on individual relation space to produce new features for each node, and all derived features from each relation space are concatenated to produce block-wise disentangled node features.

Our main contributions are summarized as follows: (1) Multi-level Disentanglement. We are the first to provide explicit mathematical definitions for graph disentanglement, including edge-level, attribute-level, and node-level disentanglement, and propose a unified framework to implement all three levels of disentanglement. (2) Relation-based Diffusion. Most previous efforts [16, 34] on multi-hop message-passing are based on a given binary structure or attention mechanism. In contrast, MD-GNN incorporates the multi-hop diffusion mechanism for message passing based on the disentangled relation spaces, which provides a novel perspective for message-passing. (3) Comparative Evaluation. The proposed MD-GNN is evaluated on both synthetic and real-world datasets and numerous visualizations are provided to demonstrate the excellent disentanglement performance of MD-GNN.

Related work
Graph neural networks
Graph neural networks (GNN) are a family of neural networks that are widely used for graph representation learning. A general GNN framework involves two key computations for each node 𝑣𝑖 at every layer: (1) AGGREGATE operation: aggregating messages from neighborhood 𝑖; (2) UPDATE operation: updating node representation from its representation in the previous layer and aggregated messages. Considering a L-layer GNN, the formulation of the l-th layer is as follows:

𝐦(𝑙)𝑖𝐡(𝑙)𝑖=AGGREGATE(𝑙)({𝐡(𝑙−1)𝑗:𝑣𝑗∈𝑖})=UPDATE(𝑙)(𝐡(𝑙−1)𝑖,𝐦(𝑙)𝑖)
(1)
where 1≤𝑙≤𝐿, 𝐡(𝑙)𝑖 is the embedding of node 𝑣𝑖 in the l-th layer, and 𝐡(0)𝑖=𝐱𝑖 is the input feature. For node-level tasks, the node representation 𝐡(𝐿)𝑖 can be directly used for downstream tasks. However, for graph-level tasks, an extra READOUT function is usually required to aggregate node features to obtain a graph-level representation 𝐡𝑔, as follows:

𝐡𝑔=READOUT({𝐡(𝐿)𝑖∣𝑣𝑖∈})=1||∑𝑖∈𝕍𝐡(𝐿)𝑖
(2)
Recent years have witnessed a surge of interest in handling graph-related tasks with Graph Neural Networks (GNNs). There are two categories of GNNs: Spectral GNNs and Spatial GNNs. The spectral GNNs define convolution kernels in the spectral domain based on the graph signal processing theory [31]. For example, GCN-Cheby [5] uses the polynomial of the Laplacian matrix as the convolution kernel, and Graph Convolutional Networks (GCN) [14] can be considered as a first-order approximation of GCN-Cheby with a self-loop mechanism. Besides, GraphHeat [38] designs a more powerful low-pass filter through heat kernel. Moreover, GWNN [39] replaces the eigenvectors with wavelet bases so as to further improve the efficiency of the model. Generally, spectral methods have good interpretability for graph signal processing, but lack generalization [9].

The spatial GNNs focus on the design of aggregation functions. For example, GraphSAGE [9] employs a generalized induction framework to efficiently generate node embeddings for previously unseen data by using known node feature information. Graph Attention Networks (GAT) [14] extends the idea of GCN by introducing the attention mechanism. Unlike APPNP [16], which incorporates personalized PageRank in the aggregation function, GPRGNN [4] proposes a new Generalized PageRank GNN that adaptively learns edge weights to jointly optimize the feature extraction and topological information regardless of the level of graph heterophily. Moreover, MAGNA [35] proposes a principled way to incorporate multi-hop context information into every layer of GNN attention computation by diffusing the attention scores across the network. Some other classical GNN variants include CensNet [11], AdaLNet [18] and KrylovNetc [23]. For more detailed reviews on GNNs, please refer to the survey [44].

Recently, many neural networks have been proposed with expressive power beyond the 1-WL test [3, 17, 26, 42]. However, these papers introduce extra and domain-specific components beyond standard message-passing GNNs. For example, the learned embeddings of P-GNN [42] are tied with random anchor-sets, and thus are not applicable to node/graph level tasks that require deterministic node embeddings. To address this problem, ID-GNNs [41] develops a class of message passing GNNs and show that GNNs, after incorporating inductive identity information, can surpass the expressive power of the 1-WL test while maintaining benefits of efficiency, simplicity, and broad applicability.

Disentanglement learning
The task of disentanglement learning has recently been an important research topic toward interpretable AI. The purpose of disentanglement is to decompose an entity, such as a feature vector, into several interpretable components to better understand and explain the behavior of a learned model. In recent years, many approaches have been proposed for learning disentangled representations based on deep neural networks and have achieved promising results with better robustness and interpretability [22]. In contrast to earlier attempts that relied on hand-crafted variables [36, 37], most recent works are based on the autoencoder [7, 10, 13, 29] or generative model [2]. The autoencoder-based methods generally constrain the latent feature generated from the encoder to make it independent in each dimension. For example, Variational AutoEncoder (VAE) [13] constrains the distribution of latent features to be Gaussian. In contrast, the work of [29] disentangles latent features by ensuring that each block of latent features cannot be predicted from the rest. Besides, DSD [7] swaps some of the latent features twice to achieve semi-supervised disentanglement. For the generative model, extra information is introduced during the generation. For example, InfoGAN [2] adds the class code to the model and maximizes the mutual information between the generated data and the class code. Despite many previous efforts in CNN-based disentanglement, disentanglement learning poses great challenges and there are still many unexplored problems in the GNN domain. More importantly, it’s not easy to apply existing strategies directly to GNN due to its non-Euclidean property.

Table 1 Functional capability of different methods
Full size table
Graph disentanglement learning
Numerous methods have been proposed to deal with the heterogeneous relations between nodes. For example, the works of DisenGCN [24] and IPGDN [21], as pioneering attempts, achieve node-level disentanglement through neighbor routines that divide the neighbors of a node into several mutually exclusive parts. Following the idea of neighbor routines, ADGCN [43] further proposes an adversarial regularizer that improves the separability between different neighbor components, thus restricting interdependence among components. In particular, GAT applies a multi-head attention mechanism to prune irrelevant neighbors and discover intrinsic relations in the graph, which can also be considered as a special kind of edge-level disentanglement. The closest work to us is FactorGCN [40], which performs edge-level disentanglement by taking into account global-level topological semantics, such as higher-order relations. However, our advantages over FctorGNN are: (1) attribute-level disentanglement; (2) a unified end-to-end framework for all three levels of disentanglement; (3) explicit mathematical definitions; (4) replacing multi-layer disentanglement with one-layer relation diffusion. Another work similar to ours is GCAT [19], which proposes a channel-aware attention mechanism enabled by edge textual contents when aggregating information and implements this mechanism in a graph autoencoder framework. However, the textual content between nodes is usually hardly accessible in practice, i.e., the relational topological semantics of the graph is underlying, which limits the application of GCAT, which is exactly the problem that graph disentanglement aims to address. From a practical point of view, relation learning with and without textual contents are two completely different research directions. While similar in motivation, the experimental setup, datasets, and evaluation protocols of this paper are completely different from those of GCAT.

Despite the promising results of the previous works, there are still many problems left to be solved. First, the current relation disentanglement is performed with all input attributes without locating the attributes corresponding to specific relations, i.e., attribute-level disentanglement has not yet been fully explored. Second, the three closely related tasks of edge-level, attribute-level, and node-level disentanglement have never been tackled in a unified framework. Last, despite being repeatedly mentioned, the mathematical definitions of graph disentanglement are still ambiguous and have not been clarified so far. The distinctive features of MD-GNN in comparison with related methods are summarized in Table 1, where VAE can be seen as a special version of node-level disentanglement (without graph structure). MD-GNN enables all three levels of disentanglement, for which no other methods can achieve.

Fig. 2
figure 2
Illustration of the architecture of the proposed MD-GNN. The blue, green and red in the figure represent different relation graphs and their corresponding node attributes and hidden features

Full size image
Methodology
Preliminary
Let =(,,) be a given attribute graph, where  is the set of N nodes, ⊆× is the set of edges, and  is the set of N node attributes. Each node 𝑣∈ is associated with an attribute vector 𝐱𝑣∈, where 𝐱𝑣∈ℝ𝑑𝑖𝑛 with 𝑑𝑖𝑛 being the dimension of input attributes. Besides, each edge 𝑒𝑢,𝑣∈ denotes a connection between node u and node v. Next, we will state the mathematical definition on all three levels of graph disentanglement and then devise a well-thought-out instantiation for it. Unless particularly specified, the notations used in this paper are illustrated in Table 2.

Table 2 Notations used in this paper
Full size table
The assumptions basis for this paper is that the disentangled relations between entities should be node-independent without hierarchical properties, which is more compatible with real-world graph data. For example, if two people are “friends", they should be disentangled as “friends" at different hierarchical levels of the relation graph. A complete graph disentanglement includes three levels: edge-level, attribute-level, and node-level disentanglement. We first define three mapping functions: 𝜓:→, 𝛺:→ and 𝜙:→ for them. (1) The edge-level disentanglement can be defined as a mapping 𝜓:→, where 𝐑𝑢,𝑣=𝜓(𝑒𝑢,𝑣)∈ℝ𝐾 is the disentangled relation vector for edge 𝑒𝑢,𝑣, i.e., there are K latent relations to be disentangled. (2) The attribute-level disentanglement can be defined as a mapping 𝛺:→, where 𝐌𝑢=𝛺(𝐱𝑢)=[𝐦𝑢,1;𝐦𝑢,2;…;𝐦𝑢,𝐾]∈ℝ𝑑𝑖𝑛×𝐾 denotes the disentanglement for attribute 𝐱𝑢, and (𝐌𝑢)𝑖,𝑘 describes the correlation of i-th element on the attribute vector 𝐱𝑢 with relation k (1≤𝑘≤𝐾). In practice, it is the global consistency (rather than local node-specific correlations) between relations and attributes that we want to revealFootnote1, which means there exists a global attribute mask matrix 𝐌=[𝐦1;𝐦2;…;𝐦𝐾] holding for all nodes. (3) The node-level disentanglement can be defined as 𝜙:→, where 𝐡𝑢=𝜙(𝑢)∈ℝ𝑑𝑜𝑢𝑡 is the disentangled features for node u, with 𝑑𝑜𝑢𝑡 being the dimension of 𝐡𝑢. Specifically, we would like 𝐡𝑢 to be composed of K independent components, i.e., 𝐡𝑢=[𝐡𝑢,1,𝐡𝑢,2,…,𝐡𝑢,𝐾]. The 𝑘𝑡ℎ component 𝐡𝑢,𝑘∈ℝ𝑑𝑜𝑢𝑡𝐾 describes the aspect of node u about relation k.

In this paper, we propose a novel instantiation, Multi-level Disentanglement Graph Neural Network, to achieve all three levels of graph disentanglement in a unified framework in an end-to-end manner. As shown in Fig. 2, MD-GNN guides the information aggregation and merging among nodes via latent relations revealed by relation learning and diffusion steps.

Relation learning step
Given an attributed graph =(,,), we aim to learn a relation vector 𝐑𝑢,𝑣=𝑔(𝐱𝑢,𝐱𝑣)∈ℝ𝐾 between nodes u and v if there exists an edge 𝑒𝑢,𝑣∈. To solve this problem, we propose to conduct relation learning in a low-dimensional subspace, done by multiplying the input attributes of nodes with linear transformation matrices {𝐖𝑘}𝐾𝑘=1 where 𝐖𝑘∈ℝ𝑑𝑖𝑛×𝐹 and F is the dimension of the subspace. In addition, in order to locate the relation-specific attributes in the input, we design a set of learnable attribute maskers parameterized by {𝐦𝑘}𝐾𝑘=1, where 𝐦𝑘∈ℝ𝑑𝑖𝑛 masks out the node attributes irrelevant to relation k, as follows:

𝐡′𝑘,𝑖=(𝐦𝑘⊙𝐱𝑖)𝐖𝑘
(3)
Then, the learning of relation graphs is formulated as

𝐑𝑘,𝑖,𝑗=11+𝑒−𝐚𝑇𝑘[𝐡′𝑘,𝑖‖𝐡′𝑘,𝑗]
(4)
where 1≤𝑖,𝑗≤𝑁, 1≤𝑘≤𝐾, and ‖ denotes the concatenation operation. 𝐚𝑘∈ℝ1×2𝐹 is the attention coefficient with respect to relation k, similar to that of GAT [33]. However, different from most previous forms of attention-based GNNs that normalize the attention coefficients among all the neighbors of nodes, our proposed model directly normalizes the attention score to [0, 1] with activation function 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(⋅). The motivation behind this is twofold: (1) it endows the relation learning process with more flexibility, making it free from the constraints of neighborhood connectivity and helping to learn more essential and intrinsic relation graphs; (2) normalization prevents too large or small attention scores from dominating the optimization process, which may lead to gradient explosion or vanishing.

In our framwork, {𝐦𝑘}𝐾𝑘=1 are defined as a set of learnable parameters, with each 𝐦𝑘 corresponding to one specific relation k. However, without other constraints, the learned {𝐦𝑘}𝐾𝑘=1 may fail to locate relation-specific attributes due to non-sufficient distinguishability. Therefore, we impose an orthogonality constraint [1] to guide them to focus on different aspects:

𝑜=‖‖𝐌⊤𝐌⊙(𝟙−𝐈𝑁)‖‖2F
(5)
where 𝐌=[𝐦1;𝐦2;…;𝐦𝐾], 𝐈𝑁 is a identity matrix, and 𝟙 is a matrix composed of values 1. The underlying motivation behind this constraint is that, first, a certain relation is generally related to only some specific attributes; for example, words such as “vocabulary" and “language" may be more relevant to natural language processing and less relevant to computer vision. Second, the difference between the attributes corresponding to different relations is greater than the similarity, and the definition of word “attributes" inherently implies the intention to distinguish different things.

Once all the relation graphs are computed, one relation k now can be represented by a specific relation graph 𝐑𝑘. However, without any other constraints, some generated relation graphs may contain similar structures, degrading the disentanglement performance and capacity of the model. More importantly, it is not easy to directly maximize the gap between various relation graphs due to the non-Euclidean property of graph structure. Therefore, we first derive a graph descriptor 𝐃𝑘 for each relation graph 𝐑𝑘 and ensure that the descriptor is related only to the graph structure and not to the node features, and then we impose constraints by maximizing the gap between graph descriptors. First, we obtain the graph descriptors by the following:

𝐃𝑘=𝑓(((𝐑𝑘,𝐙)))
(6)
where 𝐙=(𝐳1;𝐳2;⋯;𝐳𝑁)∈ℝ𝑁×𝐾𝐹 and 𝐳𝑖=‖𝐾𝑘=1ℎ′𝑖,𝑘 (1≤𝑖≤𝑁) with ‖ being the concatenation operation.  is a two-layer graph autoencoder [15] which takes 𝐙 and relation graph 𝐑𝑘 as inputs, and generates new features for each node, (⋅) is a READOUT function defined in Eq.2 that performs global average pooling for all nodes, and 𝑓(⋅) is a fully connected layer used to generate graph-specific descriptor 𝐃𝑘. Note that all the relation graphs {𝐑𝑘}𝐾𝑘=1 share the same input node features 𝐙, thus the distinguishability of descriptors depends only on the distinction of relation graph structures. The following discriminative loss is then designed to maximize the differences between different descriptors, given by

𝑑=−∑𝑖=1𝐾−1∑𝑗=𝑖+1𝐾||𝐃𝑖−𝐃𝑗||22
(7)
Relation diffusion step
Different from FactorGCN [40], which stacks multiple disentanglement layers to enlarge the receptive field to multi-hop neighbors, we propose a relation diffusion mechanism that expands the receptive field to multi-hop neighbors in each relation space within a single layer to capture the long-range dependencies between nodes. First, we extend the learned one-hop relation matrix 𝐑𝑘 into a multi-hop relation matrix 𝐇𝑘 by:

𝐇𝑘=∑𝑖=0∞𝜃𝑖𝐑𝑖𝑘
(8)
where 𝜃𝑖=𝛼(1−𝛼)𝑖 with teleport probability 𝛼∈(0,1] satisfies ∑∞𝑖=0𝜃𝑖=1, 𝜃𝑖>0, and 𝜃𝑖>𝜃𝑖+1. The powers of relation matrix, 𝐑𝑖𝑘, give us the number of relation paths from node u to node v of length up to i in the relation graph 𝐑𝑘. In practice, the computation of Eq. 8 is not trivial, as it involves the powers of the matrix. To solve this problem, we approximate 𝐇𝑘=∑∞𝑖=0𝜃𝑖𝐑𝑖𝑘 by a sequence of iterations, as follows:

𝐀(𝑆+1)𝑘=(1−𝛼)𝐑𝑘𝐀(𝑆)𝑘+𝛼𝐀(0)𝑘,𝐀(0)𝑘=𝐈𝑁
(9)
Theorem 1
lim𝑆→∞𝐀(𝑆)𝑘=𝐇𝑘.

Proof
Let 𝑆>0 be the total iteration steps, and the result of the S-th iteration is as follows:

𝐀(𝑆)𝑘=(1−𝛼)𝐑𝑘𝐀(𝑆−1)𝑘+𝛼𝐀(0)𝑘=(1−𝛼)2𝐑2𝑘𝐀(𝑆−2)𝑘+(1−𝛼)𝛼𝐑𝑘𝐀(0)𝑘+𝛼𝐀(0)𝑘=⋯=(1−𝛼)𝑆𝐑𝑆𝑘𝐀(0)𝑘+𝛼∑𝑖=0𝑆−1(1−𝛼)𝑖𝐑𝑖𝑘𝐀(0)𝑘=((1−𝛼)𝑆𝐑𝑆𝑘+𝛼∑𝑖=0𝑆−1(1−𝛼)𝑖𝐑𝑖𝑘)𝐀(0)𝑘=(1−𝛼)𝑆𝐑𝑆𝑘+𝛼∑𝑖=0𝑆−1(1−𝛼)𝑖𝐑𝑖𝑘
Since 𝛼∈(0,1] and 𝐑𝑘,𝑖,𝑗∈(0,1), we are therefore able to prove that (1−𝛼)𝑆𝐑𝑆𝑘 converges to 0 when 𝑆→∞. Finally, we can get

lim𝑆→∞𝐀(𝑆)𝑘=lim𝑆→∞(1−𝛼)𝑆𝐑𝑆𝑘+𝛼∑𝑖=0𝑆−1(1−𝛼)𝑖𝐑𝑖𝑘=lim𝑆→∞𝛼∑𝑖=0𝑆−1(1−𝛼)𝑖𝐑𝑖𝑘=∑𝑖=0∞𝛼(1−𝛼)𝑖𝐑𝑖𝑘
where 𝛼(1−𝛼)𝑖=𝜃𝑖, thus

lim𝑆→∞𝐀(𝑆)𝑘=∑𝑖=0∞𝛼(1−𝛼)𝑖𝐑𝑖𝑘=∑𝑖=0∞𝜃𝑖𝐑𝑖𝑘=𝐇𝑘
We have proved theoretically that 𝐀(𝑆)𝑘 converges to the value of 𝐇𝑘=∑∞𝑖=0𝜃𝑖𝐑𝑖𝑘 as the total iteration step 𝑆→∞. The strategy for taking values of S is given in Sect. 4.4. Note that while the idea of personalized PageRank has been adopted in graph neural networks by APPNP [16], we would like to emphasize our differences: (1) we perform relation-based diffusion rather than based on binary graph structure like APPNP; (2) three associated tasks of relation learning, relation diffusion and transformation are completely separate in our framework, rather than entangled together like APPNP; (3) we exploit the multi-hop mechanism for better exploration on the inherent relations embedded in the graph, not just to help with feature extraction. ◻

Feature aggregation and merging step
Once the relation diffusion is completed, we can obtain a number of diffused relation graphs {𝐀(𝑆)𝑘}𝐾𝑘=1. Then, in the feature aggregation step, we aggregate the features in each diffused relation graph 𝐀(𝑆)𝑘 accordingly to learn relation-specific representations. Specifically, the new node representations are generated by taking the weighted sum of its neighbors, formulated as:

𝐡(𝑙+1)𝑖,𝑘=𝜎(∑𝑗∈𝑖,𝑘𝐀(𝑆)𝑘,𝑖,𝑗𝐡(𝑙)𝑗|𝑖,𝑘||𝑗,𝑘|‾‾‾‾‾‾‾‾‾‾‾√𝐖(𝑙,𝑘))
(10)
where 𝐡(𝑙+1)𝑖,𝑘 represents the representation of node i that are pertinent to relation k in (𝑙+1) layer. In the diffused relation graph 𝐀(𝑆)𝑘, 𝑖,𝑘 is the S-hop neighbors of node i, 𝐀(𝑆)𝑘,𝑖,𝑗 is the weighting coefficient from node i to j, and 𝐖(𝑙,𝑘) is a linear transformation matrix.

The learned features from different relation space is merged to produce block-wise disentangled features:

𝐡(𝑙+1)𝑖=‖𝐾𝑘=1𝐡(𝑙+1)𝑖,𝑘
(11)
where 𝐡(𝑙+1)𝑖 is the disentangled feature of node i in (𝑙+1) layer. In Sect. 4.2, we demonstrate the effectiveness of node-level disentanglement through the correlation analysis of the learned disentangled features.

figure a
Table 3 Dataset statistic information
Full size table
Architecture
FactorGCN stacks multiple disentanglement layers and sets the number of relation graphs at different layers as hyperparameters to achieve hierarchical disentanglement. However, this may lead to three potential problems: (1) Excessive hyperparameters affect the generality and scalability, and how to set the different number of relation graphs for each layer requires further exploration. (2) Learning the relation graph for each layer separately brings an excessive computational burden. (3) The assumptions basis for hierarchical disentanglement does not always hold true in the real world, because the relations between entities are constant without hierarchical properties. For example, if two people are “friends," they should be disentangled as “friends" in different levels of the relation graph. To solve these problems, we perform relation learning and diffusion only once, and then perform L-layer information aggregation and merging based on the diffused relation graph {𝐀(𝑆)𝑘}𝐾𝑘=1. The total loss of MD-GNN is defined as

=𝑡+𝛽∗𝑜+𝜆∗𝑑
(12)
where 𝑜 and 𝑑 is the orthogonal loss and discriminate loss proposed in Eqs. 5 and 7. 𝛽 and 𝜆 are the weights to balance these two losses. 𝑡 is the task-specific loss, which may be taken to be binary cross-entropy for the multi-label graph classification task, Mean Absolute Error (MAE) for the graph regression task, and cross-entropy for the multi-class node classification. The pseudo-code of the proposed MD-GNN is summarized in Algorithm 1. Omitting the dimensionality to simplify the notation, the computational burden of the model mainly comes from three parts: (1) relation learning ((𝐾𝑁)); (2) relation diffusion ((𝐾𝑆𝐸)); and (3) information aggregation ((𝐾𝐸)), with a total complexity of (𝐾(𝑁+𝐸+𝑆𝐸)). Since K and S is usually <10 in practice, the complexity is linearly related to the number of nodes || and edges ||, in the same order as other GCN and GAT variants.

Experiments
Experimental setups
In this section, we show the effectiveness of the proposed MD-GNN qualitatively and quantitatively on both synthetic and real-world datasets, and provide an ablation study on its various components.

Datasets
The effectiveness of the proposed MD-GNN is evaluated on five datasets. The first one is a synthetic dataset containing a fixed number of predefined graphs as ground-truth relation graphs. The second one is ZINC dataset [12] built from molecular graphs. The other three are widely used citation datasets including Cora [30], Citeseer [8], and PubMed [27]. A brief description of datasets is given in Table 3. Next, we introduce these datasets in detail, especially how to generate the synthetic dataset.

Synthetic dataset The synthetic dataset contains 10000 graphs, with 7000 for training, 1000 for validation, and 2000 for testing. The task for this dataset is multi-label graph classification. To generate this synthetic dataset, we first generate K=6 predefined graphs that are well-known graphs like grid-2d graph, balanced-tree graph, and hypercube graph, from which we select 3 ground-truth relation graphs and merge them as one sample (mixed graph) as shown in Fig. 3. The type of the ground-truth graph that the mixed graph generates from is taken as the graph label. Each node in the mixed graph is associated with an D-dimensional attribute vector 𝐶=[𝑐1,𝑐2,…,𝑐𝐷], with the attribute 𝑐𝑖 (𝐷𝐾∗(𝑘−1)<𝑖≤𝐷𝐾∗𝑘) corresponding to a specific relation k (1≤𝑘≤𝐾). For each attribute 𝑐𝑖 (1≤𝑖≤𝐷), if 𝑐𝑖 is associated with relation k, its value will be sampled from the Gaussian distribution 𝑁(𝑘,𝜎2). In the paper, we have D = 30 and 𝜎 = 5.0.

ZINC dataset The ZINC dataset contains 12000 graphs, with 10000 for training, 1000 for validation, and 1000 for testing. The task of this dataset is graph regression, where we regress the constrained solubility properties of molecular graphs. The types of bonds (edges) between atoms (nodes) are provided as ground truths to evaluate the disentanglement performance.

Cora, Citeseet, and PubMed datasets These three real-world datasets are commonly used for node classification. We use them to demonstrate that MN-GNN may well-serve as a general GNN framework, even putting aside its excellent disentanglement capability. In addition to the dataset splitting consistent with the [14] (denoted as pub), we follow the splitting strategy in [18] to experiment with fewer and harder label rates. We evaluate with 3%, 1% and 0.5% labeled data in training set on Cora, 1%, 0.5% and 0.3% labeled data on Citeseer, and 0.1%, 0.05% and 0.03% labeled data on PubMed. For these three datasets, we randomly select 50% samples for validation and the rest for testing.

Fig. 3
figure 3
Examples of the ground-truth and disentangled relation graphs on the synthetic dataset

Full size image
Baselines
To demonstrate the powerful disentanglement capability of MD-GNN, we compare it with three state-of-the-art disentangled GNNs, including DisenGCN, FactorGCN, and ADGCN on the synthetic. Besides, MLP, GCN, and GAT, as three representative methods, are also included for comparison. For the ZINC dataset, we add MoNet [28] and GatedGCN [6] as baselines. On the Cora, Citeseer, and PubMed datasets, we compare MD-GNN with GraphSAGE, AdaLNet, APPNP, GPRGNN, CensNet, and KrylovNet to demonstrate that MD-GNN serves well as a general GNN framework even putting side its excellent disentanglement capability.

Hyperparameters
The following hyperparameters are set for the synthetic dataset: Adam optimizer with learning rate lr = 0.005 and weight decay decay = 5e-4; Epoch E = 200; Layer number L = 2 with middle dimension 𝑑𝑚 = 18; Relation number K = 6, subspace dimension F = 18; Iteration steps S = 3, teleport probability 𝛼 = 0.2; Loss weights 𝛽 = 𝜆 = 1.0. The experimental settings of the other four datasets are the same as above, but with several dataset-specific hyperparameters determined by a toolkit - NNI, including relation number K, iteration steps S, teleport probability 𝛼, and loss weights 𝛽, 𝜆.

Evaluation protocol
For the downstream tasks, we adopt micro-F1 for the multi-label graph classification on the synthetic dataset, MAE for the graph regression on the ZINC dataset, and classification accuracy for the multi-class node classification on the Cora, Citeseer, and PubMed datasets. Furthermore, we use two metrics—𝐺𝐸𝐷𝐸 and C-Score—proposed by [40] to evaluate the disentanglement performance. The first one is Graph Edit Distance on Edge (𝐺𝐸𝐷𝐸), which restricts the traditional GED by only allowing adding and removing the edges, and thus obtains a score by Hungarian match between the generated relation graphs and the ground truths. Besides 𝐺𝐸𝐷𝐸, we also care about the consistency of generated relation graphs. In other words, the best-matched pairs between the generated factor graphs and the ground truths, optimally, should be identical across all samples. We, therefore, use the second metric named consistency score (C-Score), which is computed as the average percentage of the most frequently matched relation graphs. In this paper, each set of experiments is run five times with different random seeds, and the mean and standard deviation are reported as the final metric.

Qualitative evaluation
We first provide the qualitative evaluation results on the performance of the edge-level, attribute-level, and node-level disentanglement, corresponding to the visualization of the disentangled relation graphs, relation-related attributes, and correlation analysis of the learned disentangled features in the first layer.

Fig. 4
figure 4
Feature correlation analysis of 108-dimensional latent features on the synthetic dataset

Full size image
Table 4 Graph classification and disentanglement performance on the synthetic dataset
Full size table
Table 5 Graph regression and disentanglement performance on the ZINC dataset
Full size table
Disentangled relation graphs
Some disentangled relation graphs are provided in Fig. 3 to give an intuitive understanding of the edge-level disentanglement. We visualize the best-matched relation graphs with ground truths. In particular, the redundant and missing edges in the relation graphs are marked in red and blue, respectively. It is found that only MD-GNN yields highly consistent disentangled graphs with ground truths, while the disentangled graphs of FactorGNN show lots of misidentified edges.

Correlation of disentangled features
Figure 4 shows the correlation analysis of 108-dimensional latent features with 𝐾=6 relations on the synthetic dataset. We can see that only the correlation map of MD-GNN exhibits six clear diagonal blocks, indicating that it can extract highly independent hidden features with excellent node-level disentanglement performance. None of the compared methods, except FactorGCN and ADGCN, can capture the mutual exclusion information. Nevertheless, FactorGCN and ADGCN still lag far behind the proposed MD-GNN in terms of node-level disentanglement performance.

Relation-related attributes
An essential criterion for relation graphs is that they must be interpretable, i.e., locating the attributes associated with each relation graph and enabling attribute-level disentanglement. We show the ground truth and disentangled attributes corresponding to each relation graph in Fig. 5. Specifically, for each relation graph, we show only the highest scoring attributes to make the number of corresponding attributes equal to the true ones. It is clear that MD-GNN is able to locate the attributes associated with each relation and accomplish the task of attribute-level disentanglement, whereas no other compared methods possess this capability.

Fig. 5
figure 5
Ground-truth (upper row) and disentangled (lower row) attributes corresponding to each relation (a total of six relations) in the input on the synthetic dataset

Full size image
Table 6 Node classification accuracy (%) on the Cora, Citeseer, and PubMed datasets with four different dataset splits
Full size table
Quantitative evaluation
Evaluation on the synthetic dataset
The graph classification and disentanglement performance on the synthetic dataset is reported in Table 4, where we mark the disentanglement metrics of MLP and GCN as “-" since they are not capable of graph disentanglement. In terms of classification performance evaluated by Micro-F1, MD-GNN performs much better than other baselines, which demonstrates that despite the powerful disentanglement capability, it does not prevent MD-GNN from being a general-purpose GNN that still outperforms conventional models such as GCN and GAT on downstream tasks. For example, the performance of MD-GNN is 4.4% and 2.3% higher than GCN and GAT, respectively. Moreover, MD-GNN achieves the best performance with respect to the disentanglement performance evaluated by 𝐺𝐸𝐷𝐸 and C-Score. Compared with the advanced method ADGCN, MD-GNN has a 5.513 reduction in the 𝐺𝐸𝐷𝐸 metric and improves the C-Score metric by 0.294.

Evaluation on the ZINC dataset
For the ZINC dataset, the type information of edges is hidden during the training process and served as the ground truth to evaluate the disentanglement performance. As shown in Table 5, MD-GNN achieves the best performance on both the disentanglement and downstream tasks. We also show the performance of GatedGCN on this dataset as a baseline for comparison, which utilizes the type information of edges during the training process. In terms of graph regression performance evaluated by MAE, MD-GNN performs much better than other baselines, for example, its MAE performance is 0.031 and 0.010 lower than ADGCN and FactorGCN, respectively. Even when compared with the state-of-the-art method GatedGCN, MD-GNN still shows advantages. In terms of disentanglement performance, MD-GNN is the best in the metric C-score, and second only to FactorGNN in the metric 𝐺𝐸𝐷𝐸.

Evaluation on three citation datasets
The focus of this paper is to explore disentanglement on heterogeneous graphs rather than to design more powerful GNNs to achieve state-of-the-art performance for all tasks or efficiently deal with large-scale graphs. Therefore, we also evaluate MD-GNN on three widely used node classification datasets with four different data splits to see the performance of MD-GNN as a general GCN framework. Since there is no ground truth relation between nodes, we only report the classification accuracy in Table 6. It can be observed MD-GNN achieves the best overall performance, showing the potential of MD-GNN as a general GNN framework, even putting aside its excellent disentanglement capability. More importantly, although DisenGCN and FactorGCN can be considered as the disentangled versions of GCN, their performance is not always better than that of GCN, and in some settings, even worse than GCN. For example, on the Cora dataset with 3% labels, the classification accuracy of DisenGCN and FactorGNN are 1.8% and 0.7% lower than that of GCN, respectively. Furthermore, while ADGCN shows better performance than the classical GNN model for all datasets and settings, it still lags behind MD-GNN, especially when labeled data is severely limited. For example, the performance of MD-GNN improves ADGCN by 6.8% and 8.4% on the Citeseer dataset with 0.3% labels and the PubMed dataset with 0.03% labels, respectively.

Fig. 6
figure 6
Ablation study. a shows the accuracy on the Cora, Citeseer, and PubMed datasets. b shows the three evaluation metrics on the synthetic dataset

Full size image
Fig. 7
figure 7
Sensitivity analysis of key hyperparameters like relation number K and iteration step S. a, b is the performance of downstream task (Accuracy and Micro-F1) with different K and S on the four datasets. c, d shows the disentanglement performance (𝐺𝐸𝐷𝐸 and C-Score) with different K and S on the synthetic dataset

Full size image
Ablation study & sensitivity analysis
Ablation study
This evaluates the effectiveness of various components in the proposed MD-GNN framework through five sets of experiments: the model without (A) Relation Learning (w/o RL); (B) Multi-hop Relation Diffusion (w/o Diffusion); (C) Discriminative Loss (w/o 𝑑); (D) Orthogonal Loss (w/o 𝑜), and the (E) the full model. Limited by space, only the results on synthetic, Cora (0.5%), Citeseer (0.3%), and PubMed (0.03%) datasets are provided in the main paper. After analyzing the reported results as shown in Fig. 6, we can draw the following conclusions: (1) Relation learning and diffusion contribute to achieving better performance on downstream tasks. Besides, the absence of multi-hop relation diffusion deteriorates the disentanglement performance slightly, which demonstrates the benefit and importance of long-range dependencies between nodes for disentanglement tasks. In addition, the absence of relation learning makes the model lose its ability to disentangle, so the disentanglement-related metrics (𝐺𝐸𝐷𝐸 and C-score) are not reported. (2) The discriminative loss 𝑑 and the orthogonal loss 𝑜 help to achieve better edge-level and attribute-level disentanglement, and more importantly, the introduction of these two losses does not deteriorate (and even improve) the performance of downstream tasks thus not affecting the potential of MD-GNN as a general GNN framework.

Sensitivity analysis
Figure 7 shows the sensitivity analysis with respect to the key hyperparameters like assumed relation number K and multi-hop iteration step S. When varying K, the iteration step S is set to a fixed value; when varying S, the assumed relation number K is fixed. In the figure, the best performance is circled. The performance gain of MD-GNN becomes larger as the number of assumed relation number K and iteration step S increase. However, when the number of K and S become too large, the disentanglement and downstream tasks become more challenging, which in turn yields lower performance gains. The performance gain depends heavily on the properties of the graph data. Besides, since the synthetic dataset is a dataset specifically for evaluating disentanglement, its performance deteriorates severely when K deviates from the ground truth. In practice, empirical results show that taking 3≤𝐾≤8 generally yields better results. However, since how to precisely select K is a problem about estimating the intrinsic property (complexity) of graphs, we use the validation set to determine the empirically optimal K.

Discussion
In the proposed MD-GNN framework, the three closely related tasks of edge-level, attribute-level, and node-level disentanglement are tackled in a unified end-to-end framework. Next, we explain how this is achieved from the following three aspects: (1) edge-level disentanglement is achieved through relation learning defined in Eq. 4, which produces multiple relation graphs, each corresponding to one latent relation between nodes; (2) we define a learnable mask for each relation as in Eq. 3 and then achieve attribute-level disentanglement through the orthogonality constraint defined in Eq. 5; (3) node-level disentanglement is achieved by propagating and aggregating the input node attributes in each relation graph to obtain disentangled representations.

In terms of disentanglement performance, extensive qualitative and quantitative experiments have demonstrated that MD-GNN outperforms existing methods, including the state-of-the-art methods FactorGCN and ADGCN. Even putting aside its excellent disentanglement performance, MD-GNN still achieves much better performance than classical GNN models, such as GCN and GAT, which shows the potential of MD-GNN as a general GNN framework. Note that we have not tested the graph classification performance on any public dataset for the following three reasons: (1) The focus of this paper is to explore disentanglement on heterogeneous graphs rather than to design more powerful GNNs to achieve state-of-the-art performance for all graph-related tasks. (2) The reason we evaluated the micro-F1 metric on the synthetic dataset is to show that MD-GNN is endowed with strong disentanglement capabilities without compromising its performance as a general-purpose GNN for downstream tasks. (3) All related works on graph disentanglement, including DisenGNN, FactorGNN, and ADGCN, all mainly focus on the node classification task, and this paper just follows their experimental settings for a fair comparison.

Despite the great progress, some challenging problems on graph disentanglement are still left for future work: (1) If the relation number K is unknown, how to estimate it from the given graph data? (2) How to implement user-defined graph disentanglement, i.e., given the specific semantics, learning the corresponding representations? (3) How to combine disentanglement with graph explainability for analyzing the learned model.

Conclusion
The proposed MD-GNN framework implements all three levels of disentanglement simultaneously in a unified framework in an end-to-end manner. The MD-GNN model learns several interpretable relations from an input binary structure, each representing a latent and disentangled relation between entities. More importantly, we also locate the input attributes associated with each relation. The learned relations are then diffused in each relation space through a multi-hop diffusion mechanism to capture long-range dependencies and produce disentangled features through information aggregation and merging. Extensive experiments on synthetic and real-world datasets have shown that the MD-GNN outperforms other leading methods on both disentanglement and downstream tasks, indicating the proposed MD-GNN framework can serve as a general GCN framework with the capability of multi-level graph disentanglement.

Keywords
Graph neural networks
Disentanglement
Relation learning
Semi-supervised learning