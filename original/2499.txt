A kinematics analysis model for the beam pumping unit (BPU) based on deep learning is proposed in this study. It takes the real-time BPU video stream as the input, and outputs the sequence of kinematic parameters. The model is composed of two parts. The first is the motion detection model based on modified Yolov4, which can output the components classes and positions of the targeted BPU in the input video. Experimental results on the test dataset reveal that the average precision of the modified model is superior to original Yolov4 and other methods. The second part is the BPU kinematics analysis model based on motion detection, which can analyze and output the kinematics parameters of BPU in real time according to the input motion detection results. Finally, we deployed this model on mobile inspection robot and run in oilfield. The experiment results show that the approach proposed in this study can help achieve accurate kinematics analysis on targeted BPU in real time. In general, this research provides a novel approach for BPU kinematics analysis, which is a very important part of the intelligent oilfield construction.

Introduction
Beam pumping unit (referred to as â€œBPUâ€ below) is the most important engineering machinery in petroleum equipment industry, and widely used in oilfields of the world [1]. Kinematic analysis of the BPU is the basis for BPU research.

Currently there are two main ways for kinematic analysis of beam pumping unit: analytical methods based on mathematical models and analytical methods based on software modeling. The former type of methods [2,3,4,5,6,7] build mathematical and physical models to do kinematic analysis. The latter type of methods [1, 8, 9] utilize 3D modeling software, such as Adams, Pro/E, and SolidWorks to establish dynamic models for BPU. These two types of methods are for the kinematic analysis of BPU before its deployment in oilfield. The real-time kinematic analysis for BPU working in real oilfields is very important for its optimization design, fault detection, etc. However, there is a lack of research work on real-time kinematics analysis of BPU.

With the rapid development of intelligent oilfields, the live working process video stream of BPU can be obtained by the camera installed in fixed positions or mobile robots. Meanwhile, many works are researching human motion detection using the video stream [10,11,12,13], especially the paper [11] did human kinematic analysis for underwater running. These research works inspire us to explore the real-time kinematics analysis of BPU based on live video stream.

In this study, by using deep learning approach, a method for kinematic analysis of BPU based on video stream is proposed. In this method, the live video stream of BPU is used as input, and the deep-learning-based object detection is used to obtain the coordinates and categories of components. We designed a kinematic analysis model for BPU according to the output of deep learning model. Therefore, we can obtain the kinematics parameter such as angle, velocity and acceleration from the model in real time. To the best of our knowledge, it is the first to propose BPU kinematic analysis based on deep learning and video stream.

In summary, our contributions include:

A BPU dataset is established for BPU object detection, which includes 5000 images collected from Xinjiang Karamay Oilfield. Later, we annotated and augmented the dataset to 20,000 images in order to train the BPU object detection model. The dataset is named as BPU-dataset.

Inspired by Yolov4, an end-to-end convolutional neural network named Yolo_BPUnet is proposed for BPU object detection. Dilated Convolution, DenseNet and Spatial Attention Module are combined with the Yolov4 network to improve the performance of BPU object detection.

A kinematic analysis model of BPU was proposed. Based on the above Yolo_BPUnet, this model takes real-time continuous video stream as its input, and outputs BPU kinematic parameters like angles, velocity, angular velocity, accelerated velocity, and angular acceleration, etc.

Both the Yolo_BPUnet and the kinematic analysis model were loaded on the mobile inspection robot to monitor the BPU in Xinjiang Karamay Oilfield, and obtained real-time kinematic parameters effectively.

The remaining of the paper is organized as follows. In Sect.  2, we summarize the related work done in the research. Section 3 is the system overview of this paper. Then we describe the BPU-dataset in Sect. 4, and introduce the designed model, Yolo_BPUnet, including the network architecture in detail in Sect. 5. After that in Sect. 6, a model of BPU kinematic analysis depend on Yolo_BPUnet is proposed, while in Sect. 7 the training results of Yolo_BPUnet are presented and the kinematic analysis model in oilfield is evaluated. Finally, Sect. 8 shows the conclusions of this study.

Related work
We firstly reviewed the BPU kinematic analysis method based on the mathematical model and software modeling. Then we further reviewed the application of deep-learning-based video detection technology in kinematic analysis nowadays, which is an era with rapid development of deep learning.

Existing kinematic analysis methods for BPU
In this part, we introduce the existing BPU kinematics analysis methods are as follows:

BPU kinematic analysis methods based on mathematical model In the year of 1963, [2], for the very first time, proposed to conduct the overall analysis by kinematic method on the pumping system which takes the sucker rod as the main part. It focuses on relative kinematic parameters like the polished rod position, velocity, accelerated velocity, and crank rotation angle, etc. After that, [3] proposed a comprehensive geometrical analysis method for the BPU based on the four-bar linkage mechanism, which can deduce the equations of the polished bar position, velocity, and accelerated velocity. Reference [4], on the basis of previous kinematic analysis on the BPU, proposed a general kinetic equation, which analyzed the contributions of all parts of the conventional BPU to its crank torque, including the Counterweight. Moreover by this equation, the method to calculate the crank torque accurately was figured out based on relative kinematic parameters, such as the velocity and the accelerated velocity. [5] proposed a calculation method for plane analysis of BPUs based on Cartesian coordinate system. It can build up local coordinate system by the corners of the part to figure out the position, velocity, and accelerated velocity of the connecting rod at any moment. Reference [6] integrated the modular analysis and parametric analysis of the BPUs to analyze the overall performance of the BPU. Reference [7] used kinematic analysis to optimize the calculation of variable speed of BPU, thereby improving the comprehensive performance of the pumping well system.

BPU kinematic analysis method based on modeling software [8] simplified the BPU into the corresponding model of crank-rocker mechanism, calculated and simulated the BPU kinematic laws by MATLAB software, and depicted its angular displacement, angular velocity, and angular acceleration curves. By doing so, the main mechanical properties and the main moving part faults were analyzed and studied. Reference [1, 9] adopted Adams and other necessary software to make 3D modeling for the BPU, developed sucker rod of the BPU by theoretical analysis and calculation, combining with connecting methods of bushing and contact pieces. And then, the virtual sample BPU was obtained successfully to simulate the actual motion laws of the horse head.

Video motion detection based on deep learning
Deep learning technology has become the best analysis method for image and semantic recognition [14, 15]. It can take use of the convolutional neural networks to learn features from dataset automatically, update the parameters of the convolution kernel through forward and back propagation, and learn feature maps of different scales according to the network layer [16].

Deep learning approach is widely used in motion detection based on video. But as far as we know, there is no deep-learning-based motion detection method for BPU video. During the research, we found that most motion detection studies are about humans. Moreover, the detection of motion process of BPU is similar to some human motions (such as running and swimming). In addition, the key parts of BPU play an important role in its movement, just like human joints. So, we summarized the research of human motion detection based on deep learning to enlighten ours.

[10] proposed an approach for recognizing human actions in RGB-D video by deep learning. This paper proved that convolutional neural networks can learn the features of moving objects in RGB videos. Reference [11] implemented a CNN to obtain the position of important joints in the human body. The information of position is used to calculate the kinematic parameters during running. [12] proposed a multi-dimensional data model of motion recognition and motion capture in video images based on a deep learning framework. This paper illustrated that deep CNN has advantages in small-scale feature extraction. Reference [13] proposed a real-time detection positioning and recognition network based on multi-scale feature fusion (IMFF-SSD). This paper proved that one-stage algorithms (SSD, Yolo, etc.) can meet the requirements of object detection and recognition.

By summarizing the above researches, we found that scholars have applied deep learning for motion detection and kenimatic analysis. Therefore, we believe that applying deep learning to the kinematic analysis of BPU can achieve good results. Moreover, compared with RNN, CNN can design a deeper network level and has a better recognition effect in natural scenes.

Summarizing
The existing methods for kinematic analysis of beam pumping unit are based on simulative data, which is hard to reflect the complexed oilfield condition. And it is also not easy to fully capture the BPU motion data via the sensors (such as inertial element and gyroscope). With the intelligent oilfield construction, we can get the video for the BPU from the fixed camera or the mobile robotics easily, which make the real-time BPU motion detection possibly. As to the video-based motion detection, the deep learning method is proved effectively for human motion detection. However, there is no video-based motion detection work for the real-time kinematic analysis of beam pumping unit.

System overview
Fig. 1
figure 1
The outline of our method. This system collected the real-time BPU video stream which were input into Yolo_BPUnet for motion detection. Yolo_BPUnet output the continuous BPU component class and position sequence to the kinematic analysis model, to analyze and output the BPU kinematic parameter curve in real time

Full size image
The outline of the method is shown in Fig. 1. By the data from a large BPU database (see Sect. 4), a convolutional neural network named Yolo_BPUnet is trained to detect the sequence of components class and location from the video input (green box in Fig. 1, see Sect.  5). After that, we designed and implemented a set of kinematic analysis model according to the BPU components classes and positions obtained by the above CNN network, thus to analyze the BPU kinematic parameters in real time (blue box in Fig. 1, see Sect. 6). Finally, both the CNN network and the kinematic analysis model were deployed on the mobile inspection robot to test its effectiveness in actual oilfield environment (red box in Fig. 1, see Sect. 7).

Data preparation
In this section, we describe our construction of BPU-dataset suitable for training deep learning model. To improve the accuracy and robustness of BPU detection, the quality of dataset should be high enough. To the best of our knowledge, no BPU dataset is available now. In this regard, actual data of this experiment, including videos and images, were collected from Karamay Oilfield in Xinjiang, China. The data were annotated and augmented as well. For business reason, we do not want to public this dataset for now. If you want these data for your research, send email to my email.

Data acquisition
By setting oilfield inspection robots and manual shooting, we collected data from over 20 BPUs in Karamay Oilfield of Xinjiang. For the robustness of deep learning model, these data were taken from several angles. The collection was mainly conducted from 8:00 to 24:00, and the conditions included sunny and cloudy weather as well as nightfall conditions, which enriched the light intensity and blurriness of images in the dataset. We collected over 5000 images with 1920âˆ—1080 pixel resolution.

Data annotation
We annotated key components including the horse head, fulcrum (the joint point of the walking beam and the support), the rear end of the walking beam, the crank, and the whole pumping unit [5, 8]. In the images, the occlusion areas of objects over 50% as well as small objects that could not be seen clearly, were not annotated. Annotation format is shown in Fig. 2.

Fig. 2
figure 2
Annotation to key components of BPU-dataset. The dark green box is the entire BPU, the light green box is the horse head, the red box is the fulcrum, the blue box is the rear end of the walking beam, and the yellow box is the crank

Full size image
Data augmentation
The Mixup algorithm [17] showed good effects on augmenting dataset, which were collected from natural scenes. So, in our research, the Mixup algorithm was applied to augment the BPU-dataset shot in natural scenes. On this basis, the datasetâ€™s size of our experiment was expanded from 5000 to 20,000 images. Among these images, 16,000 are for training purpose. And the rest 4000 are for verification. The Mixup algorithm adopted in our research is as shown in Eqs. 1âˆ¼3.

Suppose ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¥ğ‘– is a batch sample, to which, the ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¦ğ‘– is its corresponding label; and suppose ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¥ğ‘— is another batch sample, and the ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¦ğ‘— is its corresponding label. ğœ† is the mixing coefficient calculated by beta distribution which takes ğ›¼ and ğ›½ as the parameters.

ğœ†âˆ¼ğµğ‘’ğ‘¡ğ‘(ğ›¼,ğ›½)
(1)
ğ‘€_ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¥=ğœ†âˆ—ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¥ğ‘–+(1âˆ’ğœ†)âˆ—ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¥ğ‘—
(2)
ğ‘€_ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¦=ğœ†âˆ—ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¦ğ‘–+(1âˆ’ğœ†)âˆ—ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¦ğ‘—
(3)
In the formula, Beta refers to the beta distribution, ğ‘€_ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¥ is the post-mixing batch sample, and ğ‘€_ğ‘ğ‘ğ‘¡ğ‘â„ğ‘¦ is the label corresponding to the batch sample after mixing.

For the image of Image1 that is input to be tested, we fused it with a randomly selected Image2. The fusion ratio ğœ† is a random real number between [0,1]. The fusion strategy of adding the pixel-levels of the two images was applied to get a mixed image of Image3. The application of Mixup algorithm in our research is as shown in Fig. 3, where, Fig. 3a is the input image, Fig. 3b is the random image, ğœ†=0.6, and the mixed image is as shown in Fig. 3c, containing 5 labels and 10 label boxes. Then, the mixed image was transferred to the model to get the output tensor. For the outputs, the loss calculations were carried out for the labels of the two images respectively, thus to conduct weighted summation finally. It is presented as Algorithm 1.

Fig. 3
figure 3
Demonstration of Mixup in graphical representation

Full size image
figure a
Model design for BPU motion detection
Yolov4 is an end-to-end object detection algorithm. Its most significant feature is that it not only has good real-time performance, but also guarantees high level accuracy [18]. At the beginning, Yolov4 was applied in this experiment to conduct BPU object detection. However according to the experimental results, the method of using the original Yolov4 network in the BPU-dataset intensively showed unsatisfactory recall or accuracy (for detailed experiments, please see Sect. 7). Therefore, we improved the Yolov4 network in this chapter, in order to make its structure more suitable for BPU object detection. It was then named as Yolo_BPUnet.

Model overview
The deep learning network and its input and output structure in this experiment are as shown in Fig. 4. The content of input is a continuous BPU video stream, which would be processed by Yolo_BPUnet network, and output into a continuous BPU component class and coordinate sequence finally.

Three main parts of Yolo_BPUnet:

Modified CSPDarknet53 The feature extraction network of Yolo_BPUnet. It is mainly improved based on CSPDarknet53 in original Yolov4, for which, the Dilated Convolution, DenseNet block and SAM (Spatial Attention Module) were added to enhance the object detection accuracy of the feature extraction network to BPU. The specific network structure and the improvement plan are introduced in Sect. 5.2.

SPP block The feature fusion network of the original Yolov4 [19]. In Yolo_BPUnet, the SPP block performs 5âˆ—5, 9âˆ—9, and 13âˆ—13 max pooling for the input, respectively, and then outputs the feature map after concatenate.

PANnet PANnet added a top-down bypass connection on the basis of FPN in order to add high-level semantics to features, so to make the classification easier, accelerate the information fusion, and shorten the information path between low and high level features[20]. Since FPN contains the upsampling process, which can cause Checkerboard Artifacts [21]. To solve this problem, we use bilinear interpolation to resize each input layer in FPN, make the feature maps of input layer and output layer the same size. In Yolo_BPUnet, the PANnet achieves fusion of three feature maps with different scales. Finally, Yolo_BPUnet uses Yolov3 Head[22] to extract multiple-layer (three layers in total) features to conduct object detection. The depth calculation formula of the prediction-layer feature map is 3âˆ—(5+ğ¶), among which, 5 represents the width, height, horizontal and vertical coordinates of the center point of the prediction box, and grid confidence, while C represents the total number of classes. For object detection of BPU, ğ¶=5. So the shape of the final output feature map is (19, 19, 30), (38, 38, 30), (76, 76, 30).

Fig. 4
figure 4
Detection process of Yolo_BPUnet in our research

Full size image
Yolo_BPUnet structure
The original Yolov4 network was tried to be applied for BPU object detection. But the results proved that it has unsatisfactory recall or accuracy by the experiment results. According to our inferring, it is the low extraction effects of the CSPDarknet53 network to BPU components that result in such unsatisfactory Yolov4 performance in this experiment. Because all BPU data were taken from a long-range view in order to get the whole picture of the BPU. So accordingly, relative BPU component would be smaller and blurred in the data. In this regard, we added the Dilated Convolution module [23] in CSPDarknet53, aiming to increase the receptive field of the convolution kernel, and effectively expand the receptive field of the feature maps and capture richer context information without increasing the volume of network calculation or losing the resolution of the feature map. By doing so, we hope to further improve the detection of small-scale targets.

The dilated convolution kernel sometimes fails to gather the local features of small objects due to the enlargement of the weight interval. This eventually leads to increased detection accuracy, but decreased classification accuracy. It can also been seen in [24], to which, a solving method was proposed as well: to propose a new Local Feature Extraction (LFE) module for aggregating local features. Its core idea is to directly connect all layers under the premise of ensuring the maximum information transmission between layers in the network. In our research, in order to solve the above problem, we introduced DenseNet Block in a specific position of the model to solve the problem of local feature loss caused by dilated convolution [25].

Since the algorithm can be affected greatly by external environment in actual project development, we made decision to add Spatial Attention Module[26] to the Yolo_BPUnet. This is an attention mechanism based on convolution operation proposed during recent years. After adding the SAM module, the network achieves focusing on important features and suppressing unnecessary one. It can effectively ease the interference caused by external conditions and make the trained model more robust.

The overall structure of Yolo_BPUnet is as shown in Fig. 5, among which, the red rectangle shows the convolution operation with added Dilated Convolution, and Resblock-body and Resblock-body with DenseNet are the feature extraction network units used in Yolo_BPUnet. SAM is the Spatial Attention Module. We will then explain the principles and the network structures of the above-mentioned parts separately.

Fig. 5
figure 5
Network structures of Yolo_BPUnet. SPP Block and PAN Net(green and gray square) are shown in Fig. 4

Full size image
Dilated Convolution The formula to calculate the enlarged receptive field is as shown in Eqs. 4âˆ¼5. In the formula, ğ¹ğ‘˜ is the size of the dilated convolution kernel, F is the size of the original convolution kernel, D is the dilated convolution coefficient, ğ‘…ğ‘› is the size of the receptive field of the nth layer after the dilated convolution operation, ğ‘…ğ‘›âˆ’1 is the receptive field size of No. ğ‘›âˆ’1 layers, and ğ‘ ğ‘– is the stride size of the ith layer.

ğ¹ğ‘˜=ğ¹âˆ—ğ·âˆ’ğ·+1
(4)
ğ‘…ğ‘›=ğ‘…ğ‘›âˆ’1+[(ğ¹ğ‘˜âˆ’1)âˆ—âˆğ‘–=1ğ‘›âˆ’1ğ‘ ğ‘–]
(5)
The storage structure of the convolution kernel in the Yolo network is NCHW [27]. The purpose is to continuously access memory during convolution operation. In this structure, it is difficult to store the Dilated Convolution kernels sequentially. Equations 6âˆ¼9 realizes the mapping of the convolution kernel from original position to the position after dilated convolution expansion. In these equations, x represents the original position, y represents the output position, floor represents the integer quotient operation, ğ¹ğ‘˜ is the size of the dilated convolution kernel, F is the size of the original convolution kernel and d represents dilated convolution coefficient. We calculate the space occupied by the blank line and the non-blank line before y. Then, we find out where y is in its row. The sum of the above is the output position after dilation. Equation 6 calculate the total space of non-blank line before y. The quotient of original position and original convolution kernel size is the number of non-blank line.

ğ‘ğ‘œğ‘›ğ¸ğ‘šğ‘ğ‘¡ğ‘¦ğ‘¦=ğ‘“ğ‘™ğ‘œğ‘œğ‘Ÿ(ğ‘¥ğ¹)âˆ—ğ¹ğ‘˜
(6)
Equation 7 calculate the total space of blank line before y.

ğ¸ğ‘šğ‘ğ‘¡ğ‘¦ğ‘¦=[ğ‘“ğ‘™ğ‘œğ‘œğ‘Ÿ(ğ‘¥ğ¹)âˆ—(ğ‘‘âˆ’1)]âˆ—ğ¹ğ‘˜
(7)
Equation 8 calculate the position of y in its row.

ğ‘ƒğ‘Ÿğ‘’ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘¦=ğ‘¥%ğ¹âˆ—ğ‘‘+1
(8)
Equation 9 calculate the value of y. We minus one because computer memory starts at 0.

ğ‘¦=ğ¸ğ‘šğ‘ğ‘¡ğ‘¦ğ‘¦+ğ‘ğ‘œğ‘›ğ¸ğ‘šğ‘ğ‘¡ğ‘¦ğ‘¦+ğ‘ƒğ‘Ÿğ‘’ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘¦âˆ’1
(9)
For the dilated convolution module, we designed relative strategy as below:

In each Resblock-body block, change the first 3âˆ—3 convolution operation to ğ‘‘=2 dilated convolution.

In each Resblock-body block, add a layer of ğ‘‘=4 dilated convolution before 1âˆ—1 convolution operation of another bypass that is to be conducted with concatenate operation.

The comparison between the Resblock-body of the original Yolov4 network and the Resblock-body of Yolo_BPUnet is as shown in Fig. 6.

DenseNet Block The structure is as shown in Fig. 7. Compared with ResNet, its advantages are:

Reduce vanishing-gradient

Enhance transmission of the feature

Make utilize of the feature more efficient

For the DenseNet structure, our strategy was to introduce the DenseNet structure in the 3rd, 4th, and 5th Resblock-body, thereby reducing the feature loss of small target objects caused by the dilated convolution. And then a Transition convolutional layer was added after each DenseNet structure to achieve consistent dimensions of feature maps. The specific implementation is as shown in Fig. 8.

Spatial Attention Module The specific network structure is as shown in Fig. 9. In our research, the average pooling and max pooling were both applied to process the feature maps generated by convolution operation, and generate two new feature maps. After the 1âˆ—1 convolutional dimensionality reduction and activation function (Sigmoid), the two feature maps were taken as the spatial attention weight, which was further multiplied with previous feature maps. And the attention was added to the feature map as well.

Fig. 6
figure 6
Original Resblock-body structure Fig. 6a and Resblock-body structure with dilated convolution Fig. 6b

Full size image
Fig. 7
figure 7
Network structure of DenseNet Block

Full size image
Fig. 8
figure 8
The implementation structure of DenseNet Block in Yolo_BPUnet. We replaced the second Resblock-body with a 4-part DenseNet structure, like Fig. 7, to solve the problem of feature loss caused by dilated convolution

Full size image
Fig. 9
figure 9
SAM structure in Yolo_BPUnet

Full size image
Kinematic analysis model
We aimed at carrying out real-time kinematics analysis on BPU based on deep learning technology and input video stream. And according to the Yolo_BPUnet constructed by Sect. 5, we can obtain the position of each component of the BPU in real time. With these positions, we designed and implemented a BPU kinematics analysis model.

Abstract model in BPU We can abstract the BPU into a relatively simple geometric structure in order to analyze its kinematic characteristics conveniently. Its structure is as shown in Fig. 10. By summarizing the existing research experience, we concluded eight angles that are important for the kinematic analysis of BPUs, including: âˆ ğ»ğ‘†ğ¶âˆ ğ»ğ‘†ğ¶    âˆ ğ»ğ‘†ğ¸âˆ ğ»ğ‘†ğ¸    âˆ ğ»ğµğ¸âˆ ğ»ğµğ¸    âˆ ğ¸ğ¶ğ‘†âˆ ğ¸ğ¶ğ‘† âˆ ğµğ¶ğ¸âˆ ğµğ¶ğ¸    âˆ ğµğ¸ğ¶âˆ ğµğ¸ğ¶    âˆ ğ»ğµğ¶âˆ ğ»ğµğ¶    âˆ ğ¶ğ¸ğ‘†âˆ ğ¶ğ¸ğ‘† We could conduct further kinematic analysis on BPUs with these eight characteristic angles.

Kinematic parameter analysis We can further calculate kinematic characteristic parameters according to the above-mentioned abstract model and characteristic angles of BPU. We hereby summarized previous research experience, and mainly analyzed five kinematic characteristic parameters that are important to BPUs:Horse head displacement, Horse head velocity, Horse head angular velocity, Horse head acceleration, Horse head angular acceleration, Horse head displacement can be calculated by âˆ ğ»ğ‘†ğ¶ and the distance between point H and point S. Since point S and point C in âˆ ğ»ğ‘†ğ¶ are relatively static, only point H moves, so Horse head displacement can be calculated by Eq. 10, among which, âˆ ğ»ğ‘†ğ¶ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘› represents the initial position of Horse head, and ğ‘‘ğ»ğ‘† represents the distance between point H and point S.

ğ·â„=ğ‘ ğ‘–ğ‘›(âˆ ğ»ğ‘†ğ¶âˆ’âˆ ğ»ğ‘†ğ¶ğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›)âˆ—ğ‘‘ğ»ğ‘†
(10)
Horse head waving angular velocity can be solved by instantaneous center method [28]. Figure 11 shows the abstracted instantaneous center model for calculating the angular velocity, among which, ğ‘‰â„, ğ‘‰ğ‘, and ğ‘‰ğ‘’ represent the linear velocities of points H, B, and E, respectively, while point P is the instantaneous center of ğ‘‰ğ‘ and ğ‘‰ğ‘’ velocities. According to the instantaneous center method, we can get Eq. 11.

ğœ”ğµğ¸=ğ‘‰ğ‘ğ‘ƒğµâ¯â¯â¯â¯â¯â¯â¯â¯=ğ‘‰ğ‘’ğ‘ƒğ¸â¯â¯â¯â¯â¯â¯â¯â¯â¯âŸ¹ğ‘‰ğ‘=ğ‘‰ğ‘’âˆ—ğ‘ƒğµâ¯â¯â¯â¯â¯â¯â¯â¯ğ‘ƒğ¸â¯â¯â¯â¯â¯â¯â¯â¯â¯
(11)
According to the law of sines, we can get Eq. 12.

ğ‘ƒğµâ¯â¯â¯â¯â¯â¯â¯â¯ğ‘ƒğ¸â¯â¯â¯â¯â¯â¯â¯â¯â¯=ğ‘ ğ‘–ğ‘›(ğœ‹âˆ’âˆ ğµğ¸ğ¶)ğ‘ ğ‘–ğ‘›(ğœ‹âˆ’âˆ ğ»ğµğ¸)=ğ‘ ğ‘–ğ‘›(âˆ ğµğ¸ğ¶)ğ‘ ğ‘–ğ‘›(âˆ ğ»ğµğ¸)
(12)
The angular velocity formula Eq. 13 of point H can be obtained by Eqs. 11 and 12, among which, n is the BPU strokes.

ğœ”ğ»=ğ¶ğ¸â¯â¯â¯â¯â¯â¯â¯â¯â¯ğµğ‘†â¯â¯â¯â¯â¯â¯â¯â¯âˆ—ğœ‹âˆ—ğ‘›30âˆ—ğ‘ ğ‘–ğ‘›(âˆ ğµğ¸ğ¶)ğ‘ ğ‘–ğ‘›(âˆ ğ»ğµğ¸)
(13)
Horse head velocity   We can get Eq. 14 by using angular velocity of H point.

ğ‘‰ğ»=ğœ”ğ»âˆ—ğ»ğ‘†â¯â¯â¯â¯â¯â¯â¯â¯â¯âˆ—ğœ‹
(14)
Horse head acceleration, Horse head angular acceleration can be obtained by deriving ğ‘‰ğ» and ğœ”ğ» with respect to the time. The formula is as shown in Eq. 15, where ğ‘ğ» is the acceleration at point H, and ğœ–ğ» is the angular accelerated velocity at point H.

ğ‘ğ»=ğ‘‘ğ‘‰ğ»ğ‘‘ğ‘¡ğœ–ğ»=ğ‘‘ğœ”ğ»ğ‘‘ğ‘¡
(15)
At this point, the kinematics analysis model for the BPU based on deep learning has been well built.

Experimental results
Now some experimental results of training and kinematic analysis would be introduced. Letâ€™s begin with the Yolo_BPUnet training results that we designed in Sect. 5. For this training, we used the BPU-dataset of Sect. 4 that were collected from the field. After that, we combined the trained model with the kinematic analysis model that was proposed in Sect. 6. We then loaded on the mobile inspection robot to analyze a video stream sample that we selected, and gave out real-time feedback about its kinematic parameters.

Yolo_BPUnet training
In this study, the designed model was evaluated under an experimental environment, which consists of a Ubuntu PC with two Intel Xeon processors, a CPU at 3.5 GHz, 128 G DDR4, and two NVIDIA GeForce GTX 1080ti with 11 GB memory.

During the training, the input images from the dataset in Sect. 4 with sizes of 1920âˆ—1080 were resized to 608âˆ—608 pixels.

Loss function The original Yolov4 network takes CIoU as the loss function. But in this experiment, we applied DIoU loss[29]. That is because the special structure of the BPU object detection model that all detection component boxes are within the overall BPU detection frame would affect the accuracy of object detection. However in DIoU, the normalized distance between the anchor box and the target box could be modeled. The calculation method is as shown in Eqs. 16âˆ¼17, where, ğœŒ(ğ¶,ğ¶ğ‘”ğ‘¡) represents the Euclidean distance between the center of the prediction box and the center of the ground-truth box, and d represents the diagonal distance of the smallest closure area that can contain both the prediction box and the ground-truth box. As shown in Fig. 12, the method of using DIoU loss as the loss function can effectively solve the problems of box inclusion and excessive box overlap. Compared with CIoU, it is more suitable for Yolo_BPUnet.

ğ¼ğ‘œğ‘ˆ=|ğ´âˆ©ğµ||ğ´âˆªğµ|
(16)
ğ·ğ¼ğ‘œğ‘ˆ=ğ¼ğ‘œğ‘ˆâˆ’ğœŒ2(ğ¶,ğ¶ğ‘”ğ‘¡)ğ‘‘2
(17)
Results of different loss function We carried out training by all the 16,000 BPU images that have been well annotated. Figure 13 shows the loss curve that was drawn by taking the number of iterations as the abscissa while taking loss as the ordinate. The blue line is the loss curve of the original Yolov4 (Yolo + CIoU), the orange line is the loss curve of Yolo_BPUnet+CIoU, and the green line is the loss curve of Yolo_BPUnet + DIoU. We trained 150,000 epochs, which we thought that all the curves had converged. It can be seen from the figure that the convergence speed of Yolo_BPUnet + CIoU is faster than that of the original Yolov4, which proves that the modified CSPDarknet53 in Yolo_BPUnet has better effects than the original CSPDarknet53 in BPU-dataset. The convergence speed of Yolo_BPUnet + DIoU is slightly faster than Yolo_BPUnet + CIoU, which proves that DIoU loss is more suitable for BPU-dataset.

Results of Spatial Attention Module The Grad-cam method [30] was taken to carry out for attention visualization in this study. Thatâ€™s because it can detect the model sensitivity to each pixel in the image that is to be tested, and output in form of thermodynamic chart. So, using this method, it can be deemed that the higher heat the pixel in the thermodynamic chart, the more attentions the model pay to. In order to obtain the area purely with highlighted attention, we took four to-be-detected parts (horse-head, fulcrum, rear point, crank), and analyzed the Yolo_BPUnet network with and without SAM. The results are as shown in Table 1. By comparing Yolo_BPUnet with SAM and Yolo_BPUnet without SAM, we figured out that Yolo_BPUnet with SAM can localize the component of BPU more completely and accurately, which proved that adding SAM can make Yolo_BPUnet focus more on BPU component and improve the accuracy of network detection.

Comparison of different models To evaluate the performance of Yolo_BPUnet, the designed model was conducted on the test dataset and compared with the Yolov4, Yolov3, DETR and DETR-R101[31]. The accuracy of the different models is shown in Table 2. When the IOU threshold is 0.6, the mAP value of the Yolo_BPUnet was 93.27%, which was higher than the original Yolov4 (91.05%), YOLOv3 (91.13%), DETR (87.91%) and DETR-R101 (87.91%). Furthermore, when the IOU threshold rise to 0.75, the mAP value of the Yolo_BPUnet was 83.32%, which was also higher than the original Yolov4 (73.06%), YOLOv3 (64.92%), DETR (68.27%) and DETR-R101 (71.67%). The result revealing that the performance of Yolo_BPUnet was better than other models. Meanwhile when the IOU threshold increased, mAP value of the Yolo_BPUnet showed the smallest reduction, which also proves that our model has stronger robustness and better effects in actual oilfield scenarios.

Detection results showing We adopted the model trained by Yolo_BPUnet to detect the pictures in the validation set. Part of the results are as shown in Fig. 14. It can be seen that the model is featured in good detection effects on one hand, and on the other hand, can maintain comparatively high detection rate even in different BPU, angles, and weather conditions.

Fig. 10
figure 10
The BPU is simplified into a geometric construction. C represents crank center; E represents crank end; B represents the rear end of the walking beam; S represents fulcrum (the joint point of the walking beam and the support); H represents horse head

Full size image
Fig. 11
figure 11
Abstracted of instantaneous center model

Full size image
Fig. 12
figure 12
A schematic of DIoU loss function. Blue box represents detection result. Green box represents ground-truth. Red line represents ğœŒ(ğ¶,ğ¶ğ‘”ğ‘¡). Gray line represents d

Full size image
Fig. 13
figure 13
Curves of three different strategies

Full size image
Table 1 Network visualization in different strategies
Full size table
Table 2 Comparison of the accuracy of different model on BPU-dataset
Full size table
Fig. 14
figure 14
Detection results of Yolo_BPUnet. Including different BPU and weather

Full size image
Experiment of kinematic analysis
In order to verify the object detection algorithm and the kinematic analysis algorithm, we loaded the deep learning model and the kinematic analysis model on the automatic inspection robot. The robot can move to the BPU location by automatic pathfinding, and conduct video monitoring. Figure 15a briefly introduces the structure of the oilfield inspection robot used in this study, including: the camera for shooting, the pan-tilt used for adjusting the camera angle, and the crawler chassis. It can find the target BPU by the automatic pathfinding algorithm, like Fig. 15b, and then start to run the kinematic analysis program.

Fig. 15
figure 15
Structure and working Environment of automatic inspection robot

Full size image
We conducted real-time monitoring for about 30s. The changing curves of the eight feature angles of corresponding BPU are as shown in Fig. 16, and the changing curves of five kinematic parameter data are as shown in Fig. 17.

Fig. 16
figure 16
Changing curves of eight angles

Full size image
Fig. 17
figure 17
Changing curves of five kinematic parameters data in real time

Full size image
So far, we have proved the feasibility of the BPU kinematics analysis model based on deep learning, and verified it in natural scenarios. This model can be used to achieve accurate object detection and kinematic analysis in the oilfield environment, reaching our expected performance results.

Conclusion
In this study, we proposed a BPU kinematics analysis model based on the deep learning method and live video stream in actual scenario. And we deployed this model in actual oilfield scenario to verify it.

In order to train the motion detection model based on deep learning, we conducted data collection, annotating, and augmentation on the BPU in Karamay Oilfield in Xinjiang, and created BPU-dataset. The Yolo_BPUnet, which is the modified model, integrates Yolov4 with Dilated Convolution, DenseNet and Spatial Attention Module to capture richer information. This enhances the feature reuse, focuses on important features, and improves performance. Based on performance of BPU-dataset, experimental results with the compared models showed that the mAP values of the Yolo_BPUnet was superior to the original Yolov4 and other object detection methods. The model of BPU kinematic analysis use the class and coordinate of BPU components, which obtained from Yolo_BPUnet, and compute the kinematics parameters such as angle, velocity, accelerated velocity. Finally, we disposed this model into an intelligent robot for inspection, and demonstrated that it can effectively and immediately analyze the kinematics parameters of BPU in an oilfield.

For future study, the kinematic model of BPU will be improved to obtain more kinematics parameters or motion information. For the object detection based on video, it has strong relevance between adjacent frames. And the weakness of this research is the relevance are not well used to further improve the accuracy. So we are considering using the method proposed by [32] to enhance the application of relevance between adjacent frames. In data, we utilize Mixup for data augmentation. Considering about there are some difficulty in the data acquisition of BPU, we may apply GAN(Generative Adversarial Networks) for data augmentation and fusion, such as [33]. In addition, just as [34] said, only calculate the mAP is insufficient to evaluate the performance of deep learning models. Therefore, we would add more outstanding models and evaluation method for comparison.

Keywords
Beam pumping unit
Deep learning
Object detection
Real-time kinematic analysis