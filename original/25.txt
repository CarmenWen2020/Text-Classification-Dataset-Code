Abstract
Privacy-by-Design (PbD) suggests designing the fundamental architecture and features of computing systems with privacy in mind. Although widely adopted by regulatory frameworks, a growing number of critics have questioned whether PbD's focus on compliance with privacy regulation may prevent it from addressing users' specific privacy attitudes and expectations. Motivated to enhance user-centered privacy-by-design processes, we examine what are the consequences of the way privacy questions are framed to crowd users, and how personal characteristics of the crowd users impact their responses. We recruited a total of 665 participants, of which 456 were recruited via Amazon Mechanical Turk (AMT), and 209 were university students. We show that the framing of computing systems' features using data flows results in features' evaluations that are less critical, compared to using descriptions of personal experiences. We also found, based on the student sample, that students with professional engineering experience are less critical than those with no work experience when assessing the features' appropriateness. We discuss how our results can be used to enhance privacy-by-design processes and encourage user-centered privacy engineering.

Previous
Next 
Keywords
Privacy-by-design

User-centered design

Privacy

Framing

Vignette study

1. INTRODUCTION
Systems that violate privacy norms can deter users from using a system (Felt, Egelman, & Wagner, 2012; Lin et al., 2012) or push them to choose alternative systems (Egelman, Felt, & Wagner, 2013; Poikela & Toch, 2017). To avoid incidents in which individual privacy is violated, Privacy-by-Design (PbD) methods propose a design and development framework that helps in the production of privacy-respectful systems (Cavoukian, 2009; Langheinrich, 2001). The U.S. FTC's acknowledgment of PbD (FTC, 2012) and its adoption in the EU General Data Protection Regulation (GDPR) drew considerable attention to PbD and its challenges in implementing it. At the same time, turning PbD from an abstract concept into a concrete engineering reality is not a straightforward task. Critics have pointed to serious flaws in PbD, such as mainly focusing on the technologies' “back end” while overlooking the end-users’ privacy expectations (Hartzog and Stutzman, 2012) and its stark differences from engineering mindsets (Birnhack, Toch, & Hadar, 2014).

While PbD had been primarily explored from a legal perspective, the “design” part has a considerable impact on the experiences of users, requiring multifaceted explorations. Studies have shown that when making decisions about privacy design, developers consult with other developers (Balebako, Marsh, et al., 2014; Hadar et al., 2017) or Chief Privacy Officers (CPOs) (Balebako, Marsh, et al., 2014; Bamberger & Mulligan, 2011), while the experiences of users are often neglected (Wong & Mulligan, 2019). Taking these experiences into account at the early stages of the product development can help to avoid privacy infringements that might occur in later stages of the product life cycle. This process can potentially reduce the commercial and legal risks. This type of end-user participation has been shown to reduce legal and organizational risks in developing healthcare systems, a domain rich with these risks (Vimarlund and Timpka, 2002). An illustrative case study is the privacy design of Facebook Messenger in 2015, which tracked the message sender's location and shared it with the message recipient by default (King, 2015). In response to public outcry, Facebook updated the application design to share the location upon the user's action, rather than automatically (Chowdhry, 2015).

PbD engineering frameworks incorporate legal codes into the design, mainly through notice and choice (Schaub et al., 2015) and data minimization (Nist, 2015), even if the conceptualization of people might be different (Wong & Mulligan, 2019). Several papers advocate involving user feedback in PbD processes (Ayalon and Toch, 2019; European Union Agency for Fundamental Rights and EDPS, 2018; Report, 2016; Senarath et al., 2017; Wong and Mulligan, 2019). However, while regulatory frameworks sometimes advise that users should be involved, this is not a requirement nor a fundamental principle (Kuner et al., 2018). More importantly, neither existing research nor regulation specifies how to collect user feedback and choose those users that should be involved.

User-centered design (UCD) is based on the premise that users should be involved in every step of the development process (Abras, Maloney-Krichmar, & Preece, 2004). UCD's criticism of traditional software engineering can be applied to the way PbD addresses users’ privacy expectations. This study's motivation is to create large-scale tools that can help us obtain a better understating of the users’ privacy perceptions of a developed computing system's feature. We use the term crowd users to refer to chosen collections of end-users that are invited to take part in the development process of new features. The study's concrete goals are to understand two fundamental questions at the heart of user-oriented engineering processes: how feature designs are presented to crowd users and how personal characteristics may impact the crowd users’ responses. First, we consider how the framing of privacy designs can affect the crowd users’ perceptions of the feature designs' appropriateness. Second, we explore if and how the crowd users' personal characteristics impact their evaluation of the feature's appropriateness.

To answer the research questions, we report the results of two vignette studies. We followed a methodology used in behavioral economics to assess the effects of information framing and professional experience on tested outcomes (Czap et al., 2012; Rubinstein, 2006). In the studies, we used an online experiment in which participants were presented with scenarios and answered questionnaires; these included 665 crowd users: 456 participants were recruited via Amazon Mechanical Turk (AMT), and 209 participants were university students. We found that framing design issues using personified scenarios, in which the human perspective is prominent, results in a perception of a less appropriate feature, reflecting a higher critical perception of the feature's design. Moreover, we found that some of our explored personal variables also significantly affect the feature's appropriateness. Participants with engineering work experience found the feature more appropriate compared to inexperienced participants. The same assessment was found for personally perceived privacy: users with a greater perception of having privacy evaluated the feature as more appropriate.

In this paper, we aim to discuss the role of user-centered design in privacy by design. To this end, we deliver an experimental and evaluation of the effects of using personified scenarios and of explored demographics on appropriateness evaluation. We discuss how our findings can be used to improve privacy-by-design processes and how the outcome of these processes can be affected by its framing. We conclude the paper by suggesting new avenues for user-centered privacy-by-design.

2. BACKGROUND
2.1. Privacy-by-Design
Privacy-by-Design applies design principles and processes to analyze and improve privacy of information systems and procedures. It advocates considering data protection and privacy from the very beginning rather than by adding layers of privacy-enhancing technologies after the fact, when it can be too late to solve inherent privacy problems (Cavoukian, 2009; Langheinrich, 2001). PbD received meaningful recognition in 2010, when the International Conference of Data Protection and Privacy Commissioner published a resolution on PbD. The resolution recognized “Privacy by Design as an essential component of fundamental privacy protection”, translating privacy rights and data protection principles from abstract theoretical requirements to concrete design decisions (Cavoukian, 2011; Data Protection and Privacy Commissioners, 2010). Several privacy theories provide theoretical basis for the motivation of PbD processes. PbD addresses many of the privacy harms enumerated in Solove's Taxonomy of Privacy, such as the harms related to surveillance and unwanted exposure, and to preserve confidentiality (Solove, 2008). It aims to preserve the balance individualistic and collective privacy rights and dignity of users (Mantelero, 2017; Regan, 2000). From the point of view of Nissenbaum's Contextual Integrity theory, we can view PbD processes as trying to minimize the way information systems harm the appropriateness of information flow norms (Nissenbaum, 1998). PbD is now included in regulations and laws throughout the world. Article 25 of the EU General Data Protection Regulation (“GDPR”) (“EUGDPR,” 2016) regulates “Data protection by design and by default,” in which data controllers are required to “implement appropriate technical and organizational measures”. In the U.S., the Federal Trade Commission proposes that companies implement PbD as part of a broader approach to privacy (FTC, 2012).

With its growing acceptance, PbD has also been criticized for being too abstract (Rubinstein and Good, 2012), not providing engineering advice (Davies, 2010), and primarily taking a legal perspective (Koops and Leenes, 2014). One stream of studies has proposed engineering guidelines to be used by the organizations, suggesting principles to follow, including data minimization and personal information processes separation (Gürses et al., 2011; Hoepman, 2018; Nist, 2015). From a technical perspective, other studies have suggested tools or frameworks for implementing PbD in systems’ architecture (Antignac and Métayer, 2014; Bringer et al., 2018; Schupp, 2019). However, several studies have shown that in many important cases, engineering and legal perspectives are not compatible (Rubinstein and Good, 2012; Wong and Mulligan, 2019). Other studies found that designers mostly view regulation as a compliance issue, rather than as guidelines for design, and suggested supporting them by raising their awareness to privacy regulations (Luger et al., 2015; Urquhart, 2017). Wong and Mulligan criticized PbD for taking a legal perspective for the “design” in privacy-by-design, setting the objectives of the design process without taking specific and contextualized user feedback.

From a legal perspective, there is a limited, nonmandatory, and non-specific suggestion to involve users in the design process. The main tool to evaluate the extent to which organizations are following PbD principles is to conduct Data Protection Impact Assessment (DPIA) (GDPR Article 35) (Kuner et al., 2018). DPIA is a process that aims to help organizations identify and minimize the data protection risks of a particular project. According to DPIA guidelines, the data subjects should be involved in the development process to a certain extent: “Where appropriate, the controller shall seek the views of data subjects or their representatives on the intended processing.” (GDPR Article 35). However, the language requesting the involvement of the data subjects is limited and should only take place based on the controller's necessity evaluation. Several sources aim to help organizations with legal compliance and suggest taking a user-centered approach to conducting comprehensive PbD processes (European Union Agency for Fundamental Rights and EDPS, 2018; Report, 2016). These sources include recommendations to promote ‘usable privacy’ and to involve the users in the assessment processes; however, they do not provide explicit suggestions regarding how to involve them in practice.

From an academic perspective, several recent studies have suggested mechanisms to involve users in PbD processes. Senarath et al. proposed a detailed workflow for implementing a user-centric privacy design (Senarath et al., 2017). Ayalon and Toch investigated the applicability of A/B testing method to evaluate privacy designs and found that it primarily supports the evaluation of social privacy aspects (Ayalon and Toch, 2019). Crabtree et al. argue, based on the results from a qualitative study, that people's perceptions of privacy are rooted in the larger framework of managing relationships in a networked world, and that people were mostly concerned with day-to-day digital risk management with other house members, and not with risks related to external agents as unauthorized users. (Crabtree et al., 2017). However, more empirical studies are required to explore which other tools are applicable to involve the users. In this study, we aim to narrow this gap by investigating how we can reach the users’ privacy-related perspectives. We turn to user-centered design as a tool for mediating privacy issues to the users.

2.2. User-Centered Design
User-Centered Design (UCD) was defined by Norman in the 1980s as a design process that takes into considerations the users’ perspective by involving them in all the stages of the design process (Norman, 1988; Norman and Draper, 1986) and became widely used in subsequent years (Abras et al., 2004).

2.2.1. Framing
User-centered design involves users in all the steps of the development process. However, due to the abstract nature of privacy (Wong & Mulligan, 2019), the data protection aspects of information systems need to be made concrete and framed to users. Framing is widely used as an analytical framework in a variety of fields, including psychology, sociology, and communication (Scheufele, 1999). Scholars from different fields define framing differently; for example, a leading definition from the field of journalism is Entman's: “To frame is to select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described” (Entman, 1993, p. 52). In Behavioral Economics, the Framing Effect refers to people's tendency to decide differently depending on whether the choice is presented as a loss or a gain (Plous, 1993). In this study, we are more closely related to the journalism definition. A wide range of studies has used different types of framing devices to test the participants’ outcomes. In the context of privacy, researchers have explored how to better communicate privacy notices and policies using succinct text, graphics and even comics (Kumaraguru et al., 2010; Tabassum et al., 2018). Studying economics, Rubinstein found that students tend to make decisions that maximized a company's profits when the background scenario's details were framed using mathematical equations (A. Rubinstein, 2006). In a laboratory experiment, Czap et al. found that when describing a problem using an empathic framing, the participants made decisions that favored positive environmental implications (Czap, Czap, Khachaturyan, Lynne, & Burbach, 2012). We explore how these studies’ findings can be applied to privacy engineering and seek to explore how we can ask questions related to privacy in the context of a given feature's design. We will use framing analyses in our study to learn if different types of information presentations arouse a more critical view of privacy design.

To explore different levels of framing, we implemented practices taken from advertising studies. For example, images are used in charitable advertisements to increase donations. Thornton et al. (Thornton et al., 1991) found that when a photograph was present in a charity advertisement, people contributed more compared to a non-photograph condition. Other practices are taken from UCD studies that explore personas. Personas are models of end-users (Cooper, 2004) that are meant to create a personal connection between designers and users, enabling the designers to understand what matters to users in order for them to develop empathy with users (Adlin and Pruitt, 2010). Building upon these theories, we framed features’ privacy designs using an increasing level of detailed personified scenarios.

2.2.2. Selecting Users for User-Centered Design
The fundamental principle of the UCD approach is that users are involved during the development process, and therefore an essential consideration is the characteristics of the representative users. Kujala and Kauppinen gathered types of characteristics that were suggested in the literature, including personal (education, attitudes such as those toward using technology, and skills), task-related (usage, training and experience, and goals and motivation), and geographic and social characteristics (location, social connections and organizations) (Kujala and Kauppinen, 2005). As researchers are unable to find users with all possible users’ intersectional characteristics, they focus on those most relevant to the study conducted. In the context of privacy, a possibly relevant personal characteristic is personal privacy attitudes. Previous studies have shown that personal perceptions of privacy affect a system's evaluation (Knijnenburg and Kobsa, 2013; Kobsa et al., 2016). For example, Knijnenburg and Kobsa found that people who were generally more concerned about information collection by online companies perceived the studied system as more privacy threatening (Knijnenburg and Kobsa, 2013).

In terms of task-related characteristics, the users’ profession can affect privacy-related outcomes. In a study conducted in the field of economics, Rubinstein showed that students who studied economics were more profit-oriented compared to students from other disciplines, ignoring other variables such as the workers’ well-being (Rubinstein, 2006). Similarly, the users’ profession may affect their evaluation of the appropriateness of a feature's design. For example, various professions differ in the extent to which they are more system-oriented or more human-oriented.

2.3. Appropriateness and Privacy in Context
Developing a model for evaluating privacy designs requires us to tie system feature designs to the opinions, feelings, and attitudes of users. Nissenbaum's theory of privacy as Contextual Integrity (CI) provides this theoretical link (Nissenbaum, 1998). In Contextual Integrity, the analysis of respecting or violating privacy is based on the appropriateness of information flows according to beliefs and norms, and is specific to social context (Benthall et al., 2017). For example, when a social media application publishes information in unexpected ways to the user's contacts, the appropriateness of the induced information flow is questioned. People's privacy expectations depend on contextual informational norms, which “map onto people's privacy expectations” (Benthall et al., 2017). We say that a particular system design corresponds to a contextual informational norm, in which the user's data (the sender) flows to the system's operator (the recipient) under some transmission principle. We consider a “good” design to be a design that reflects the appropriateness of the information flow, given the contextual ends, purposes, and values which “are useful in evaluating the ethical legitimacy of given flows” (Benthall et al., 2017).

2.4. Research Model
In this study, we investigate two central aspects in involving crowd users in PbD processes: how the framing of privacy designs and the crowd users’ personal characteristics affect a system's feature's evaluation. Based on Contextual Integrity Theory, we measure the “perceived feature's appropriateness” as the measured attitude of users toward the appropriateness of a given feature design. As users perceive a feature as less appropriate, they are more critical of privacy.

Investigating the effect of framing, our study takes a user-centered design approach: we do not ask how either policymakers or users frame privacy, as done in previous privacy-related studies (Epstein et al., 2019; Quinn and Epstein, 2018), but rather how framing privacy might affect the way users perceive privacy in different variations of feature design. Positioning our study in the literature of framing in communication, our study aims at answering a similar question, as suggested by Scheufele (1999): “What kinds of media frames influence the audience's perception of certain issues, and how does this process work?”

Vignette studies are commonly used in experimental studies, including in the domain of usable privacy (Bloom et al., 2017; Naeini et al., 2017) and ethics (Detert et al., 2008). Scenarios are used as a method to present the participants with case-studies and can be used as a measure (Detert et al., 2008). To explore the effect of framing on users’ evaluation of a feature's privacy design, we are taking a scenario-based approach. We developed framing with personified scenarios by looking at studies in advertising (Chang and Lee, 2009; Thornton et al., 1991). We expect that framing a privacy design feature using personified scenarios will affect the feature's appropriateness negatively. We plan to compare personified scenarios to the standard system model used in privacy-by-design processes, which is based on data flows. When presented with a privacy-violating design while using personified scenarios, the users are expected to have a greater understanding and empathic consideration of the design presented; therefore, we expect them to perceive the design as less appropriate. This expectation leads us to introduce our first hypothesis:

H1. The feature design will be perceived as less appropriate when it is framed using personified scenarios.

To further explore the effect of personified scenarios on features’ evaluations, we followed the methodology of studies that explored how to conduct a development process with a user-centered design approach (Adlin and Pruitt, 2010; Cooper, 2004). These studies explored the outcomes of adding personal details to personas, such as using private names and profile photos. Based on these studies, we hypothesize that there will be a significant effect of using personified scenarios that are more detailed. Our second hypothesis is the following:

H2. Features designs are perceived as less appropriate when the personified scenarios are more detailed.

Involving users in development processes requires exploring relevant users’ characteristics. The users selected to participate ought to represent a much greater target audience, hence the need to explore whether relevant characteristics have a crucial impact on the study's outcomes. In our study, we investigate how we can involve users in PbD processes. Accordingly, and based on previous studies (Knijnenburg and Kobsa, 2013; Kobsa et al., 2016), we explore whether personal privacy perceptions affect the participants’ evaluation of a feature. Our third hypothesis is the following:

H3. Features appropriateness is positively correlated to the users’ personally perceived privacy.

Other relevant user characteristics refer to their professional characteristics, and we formulate two distinct hypotheses in this context. In the first hypothesis, we explore the effect of academic background on features’ evaluation. In his study, Rubinstein explored the impact of the students’ discipline on solving an economic dilemma (Rubinstein, 2006). Taking a similar approach, we explore whether engineering students evaluate features differently, as they might be more systems-oriented, showing less consideration for the end-users’ possible privacy implications compared to students from other disciplines. Our fourth hypothesis is the following:

H4. Users with academic backgrounds in engineering and computer science would perceive features designs as more appropriate.

As privacy-by-design pushes privacy to the design process, it is currently unclear what would be the effect of developers’ professional identity on the outcome of their design. Previous studies exploring the developers’ aspects in the context of privacy found that developers’ personally perceptions of privacy affect their privacy practices (Ayalon et al., 2017; Hadar et al., 2017; Spiekermann et al., 2018). However, there remains a gap in our understanding of whether their profession affects their system's appropriateness evaluations.

To narrow this gap, we refer to the users’ work experience that is aligned with their academic studies. We hypothesize that users with experience in engineering-related work might consider the features’ perspective as primary constrains and benefits when evaluating the feature. Our fifth hypothesis is the following:

H5. Users with work experience in system development would perceive features designs as more appropriate.

The next two sections describe the user studies that aim to test our research hypotheses.

3. STUDY 1: EXPLORING FRAMING AND APPROPRIATENESS
In this study, we aimed to test the study's first three hypotheses: exploring the effects of different types of information framings, and of personal perceived privacy on a feature's appropriateness.

3.1. Methodology
3.1.1. Experimental design
We designed a between-subject user study using an online experiment that included a questionnaire. The main section of the questionnaire was designed to elicit the feature's appropriateness. The questionnaire also included two other sections: 1) personally perceived privacy, and 2) demographics. Except for the demographics section, the questions presented statements and the participants were asked about the extent to which they agreed with each statement. We used a seven-point Likert scale, where 1 represented low agreement and 7 high agreement. The study was authorized by the institutional ethics review board (IRB) and was carried out in January 2017.

The primary goal of the experiment was to compare the effects of framing on the perceived appropriateness of the feature. Accordingly, the participants were randomly assigned to one of three condition groups. We developed three questionnaires with different types of framings, in which we had two major types of framing: information-flows versus personified scenarios. A further division of types differentiates between two types of the personified scenarios, differing in the extent to which the scenario is detailed and represents the end-users’ perspective more prominently. The three conditions are termed “data-flows,” “general scenarios,” and “detailed scenarios.”

The questionnaire opened with a general explanation of a company named WeCreateApps, which develops apps. The participants were asked to imagine they are working in this company and are asked to make decisions about application designs. We chose to ask the participants to take the developers’ perspective to direct their considerations from the possible benefits as end-users and to focus on judging the appropriateness of the design. Next, only for general and detailed scenarios conditions, additional information referring to interviews that were held with end-users was shown. It was explained that the interviews were conducted to help the team develop a stronger understanding of end-users’ behaviors and views on the new applications.

Next, the participants were presented with five randomly ordered scenarios based on the applications’ descriptions. Each scenario comprised five sections: 1) application name; 2) screenshot; 3) short explanation about the application's main functionality; 4) specific case study; 5) question referring to the privacy design issue to elicit the system's appropriateness. We took an approach similar to Detert and Sweitzer (Detert et al., 2008) and averaged the scenarios’ appropriateness measure per each participant.

The scenarios were based on five fictitious mobile applications, focusing at specific feature for each application. The applications were chosen based on a pilot study based on Mechanical Turk (n=287), in which we eliminated applications that did not have sufficient variation in the appropriateness measure. The applications’ names were invented, but we based the applications’ functionalities on existing applications. The five applications used were 1) WeMail, which enables users to manage their emails; 2) Photo Album Creator, which allows users to create photo albums using photos stored on a device's memory card; 3) BiP, an online social network; 4) WeFit, which enables users to track their sports activities; and 5) Emoji Keyboard, which allows users to send messages with special emojis.

After viewing a screenshot (section 2) and reading a short description of the application's main goal (section 3), the participants were presented with a specific case study, representing the explored feature (section 4). Lastly, after they were presented with the case study details, the participants were asked a relevant question, thus eliciting their perceived system's appropriateness (section 5). For example, in the “WeFit” scenario, the question was: “As a team member of the app company, I think it is okay that once a user shared his/her fitness activity information with another user, the app shares future activities by default.”

The difference between the conditions is found in the fourth section of the application description, in which a specific case study was presented. The data-flows condition was presented as an information flow description. When designing the personified scenarios conditions, we were inspired by advertising and personas literature (Adlin and Pruitt, 2010; Cooper, 2004; Thornton et al., 1991). For the general scenarios and detailed scenarios conditions, the design was represented using a user's quote given under a fictitious end-user name. For the detailed scenarios condition, additional information about the end-user was presented, including a picture and a short description. It could be understood that the quotes and details referred to the end-users who were mentioned earlier as interviewees. To minimize the differences between the presented scenarios, thus avoiding biased answers based on the given end-user's details, they were all defined as undergrad female students from Tucson, AZ. Table 1 is an example of the Photo Album Creator scenario.


Table 1. Measuring privacy appropriateness using three conditions that differ in the framing types. For all of the conditions, the mobile app’s presentation began with the presentation of the app’s name and a screenshot, followed by a description of the specific case study given (the outlined part).


The rest of the questionnaire elicited information on other independent variables. We referred to the participant's perceived levels of privacy. The participants were asked to contemplate the degrees of access that websites and apps have to their personal information and to answer several questions drawn from Dinev et al. (Dinev et al., 2013). Another personal aspect that we measured was that of empathy based on two of Davis’ (Davis, 1980) four recommended empathy measurements: empathic concern and perspective-taking. Finally, the questionnaire closed with demographic questions. See Appendix for the phrasing used in all the scenarios and conditions and the other measures used.

3.1.2. Recruitment
We used Amazon Mechanical Turk (AMT) to recruit 469 adult participants. The use of AMT as a platform to recruit participant is common in usable privacy studies, and it was found that the Mturks’ responses regarding these issues can be generalized to a broader population (Redmiles et al., 2017). Participants were required to be 18 years of age or older and to reside in the U.S. to ensure English proficiency. The study presentation did not include a mention of privacy to avoid biasing our participant base by attracting people who were more sensitive to privacy concerns (Gross & Acquisti, 2005).

Qualified participants followed a link that randomly assigned each participant to one of three links to the questionnaires. The questionnaire was built using the Qualtrics commercial web survey service. The participants completed an IRB-approved consent form on participation limitations. The questionnaire took approximately 6.5 minutes to complete, and participants were compensated with $0.3 per Human Intelligence Task (HIT), which results in hourly wage of approximately $2.8/h, close to the average hourly wage of Mturkers and higher than the median wage (Hara et al., 2018).

Following Goodman et al.’s (Goodman, Cryder, & Cheema, 2013) study on AMT workers, we phrased a question to identify participants who would not follow the survey's instructions (Oppenheimer, Meyvis, & Davidenko, 2009). The participants were presented with a reading comprehension test, which involved reading a short paragraph related to the survey content and answering a question about it (see Appendix). We excluded participants’ records if they answered the screening question incorrectly.

After filtering out participants who completed the screening task incorrectly, we removed 13 responses out of 469 and were left with 456 responses. Two hundred and thirty participants were female (50%), 224 were male (49%) and two participants did not reveal their gender (1%). The age distribution of our participants was as follows: 65 were between the ages of 18 and 24 (14%); 207 were between the ages of 25 and 34 (46%); 100 were between the ages of 35 and 44 (22%); 43 were between the ages of 45 and 54 (9%); 32 were between the ages of 55 and 64 (7%); and 9 were 65 or older (2%). Since the 65+ age group was relatively small, we created a new age group of “55 or older” to increase the size group similarity.

3.1.3. Data analysis
We used Cronbach's α measure to determine the reliability (Straub et al., 2004) of each construct according to our designed questionnaire. Our measure of a feature's appropriateness was computed using all five scenarios. However, following our analysis, we removed the item related to the Emoji Keyboard scenario. Removing this item increased the Cronbach's α value from 0.75 to 0.76. A possible explanation could be that the emoji case study reflected an institutional privacy aspect (user-system interaction), while all other case studies reflected a social privacy aspect (user-user interaction) (Raynes-Goldie, 2010). The final Cronbach's α and number of items for each construct are respectively: Feature's appropriateness: 4 items, Cronbach's α: 0.76; Perceived privacy: 3, 0.92; Empathic concern: 7, 0.88; Perspective taking: 7, 0.92. Next, we performed a Herman single-factor test to control for the effects of Common Method Variance (CMV). A single factor explains 24% of the variance; therefore, our data are not exposed to CMV bias (Podsakoff & Organ, 1986). Normality analysis revealed that the system's appropriateness variable was not normally distributed (Shapiro-Wilk, p < 0.001); therefore, we used nonparametric tests to analyze the results, including the Kruskal-Wallis test and Wilcoxon test using Bonferroni correction for multiple comparisons. Lastly, we performed a linear regression analysis and verified that the studentized residuals are normally distributed.

3.2. Results
3.2.1. Descriptive Statistics
We began our analysis by comparing the feature's appropriateness variable between the study conditions. Figure 1 shows the differences in the median feature's appropriateness scores among the framing conditions. When personified scenarios were used, the features were perceived as less appropriate. A Kruskal-Wallis analysis showed a significant difference between the three conditions (H = 12.887, p = 0.002). To further explore the differences between the condition, we conducted multiple Wilcoxon tests using Bonferroni correction. We found a significant difference between the personified scenarios conditions and data-flows, showing significantly lower appropriateness scores for personified scenarios which supports our first hypothesis (p-value: detailed scenarios vs. data-flows: 0.025, general scenarios vs. data-flows: 0.002). However, the difference between the personified scenarios conditions was found to be insignificant, rejecting our second hypothesis. The mean score of the feature's appropriateness was the highest for the data-flows condition (mean = 3.46, SE = 0.11, median = 3.5), and the detailed and general scenarios conditions received lower scores (detailed scenarios: mean = 3.06, SE = 0.12, median = 2.75; general scenarios: mean = 2.94, SE = 0.12, median = 2.75).

Figure 1
Download : Download high-res image (196KB)
Download : Download full-size image
Figure 1. Feature's appropriateness versus the type of information framing. A lower appropriateness score reflects a higher critical perception of the feature design. We used multiple Wilcoxon tests and Bonferroni correction to compare the feature's appropriateness score of the different framing conditions. p < 0.05 *, p < 0.01 **.

3.2.2. Model Validation
Next, we further examined our hypotheses by conducting a linear regression analysis for predicting the feature's appropriateness. We used our proposed model and a stepwise technique to define the model and determine which predicting variables to include. The final regression comprised of six variables and latent variables. We also included interaction variables to study the effect of framing for individuals with varying degrees of perceived privacy (see Table 2). Tests for multicollinearity indicated that a low level of multicollinearity was present (VIF = 1.432 for empathic concern, 1.429 for perspective-taking, and 1.005 for perceived privacy). The regression model (adjusted R2 = 0.324) pointed to two significant predictors affecting the feature's appropriateness: the framing types and the participants’ perceived levels of privacy. We found that usage of personified scenarios affected the feature's appropriateness under both general and detailed scenarios conditions: (a) general scenarios compared to data-flows (Estimated coefficient = -0.527, p < 0.001) and (b) detailed scenarios compared to data-flows (Estimated coefficient = -0.323, p = 0.022). The results show that using personified scenarios spurred the perception of a less appropriate feature, confirming our first hypothesis. We further analyzed the difference between the two conditions of personified scenarios (general and detailed scenarios). We performed a regression wherein the general scenarios condition was the baseline to which the two other conditions were compared to, and we did not find a significant difference between general and detailed scenarios conditions (Estimated coefficient = 0.23, p = 0.095). Therefore, this result rejects our second hypothesis in which features would be perceived as less appropriate when the personified scenarios are more detailed. The results supported our third hypothesis, as we found a positive significant effect of personally perceived privacy on the feature's appropriateness (Estimated coefficient = 0.402, p < 0.001). This direction of effect is the opposite of that of the negative effect of personified scenarios on the feature's appropriateness. The stronger the participant's perception of having privacy, the more appropriate the feature was perceived to be. For the interaction effect, we found no significant interaction between participants’ perceived privacy and the framing types.


Table 2. Regression model predicting system's appropriateness. Study 1: Adjusted R2 = 0.314, F(13,442)=17.01, p < 0.001. Study 2: Adjusted R2 = 0.152, F(17,191) = 3.185, p < 0.001.

Study 1				Study 2			
Estimated coefficient	Std. Error	t value	Pr (>|t|)	Estimated coefficient	Std. Error	t value	Pr (>|t|)
(Intercept)	2.946	0.442	6.671	<0.001	3.738	0.970	3.853	<0.001
Detailed scenario	-0.323	0.140	-2.303	0.022	-0.934	0.192	-4.863	<0.001
General scenario	-0.527	0.138	-3.828	<0.001	-0.799	0.189	-4.232	<0.001
Perceived privacy	0.402	0.064	6.308	<0.001	0.312	0.103	3.027	0.003
Empathic concern	-0.065	0.060	-1.076	0.282	-0.189	0.105	-1.8	0.073
Perspective-taking	-0.092	0.072	-1.273	0.204	-0.028	0.098	-0.288	0.774
Gender: no answer	0.945	0.867	1.090	0.276	-0.724	0.800	-0.905	0.367
Gender: male	0.085	0.120	0.709	0.479	0.098	0.177	0.553	0.581
Age: 25-34	-0.116	0.172	-0.673	0.502				
Age: 35-44	-0.213	0.195	-1.094	0.275				
Age: 45-54	-0.153	0.241	-0.633	0.527				
Age: 55+	-0.228	0.246	-0.927	0.355				
Detailed scenario * Perceived privacy	0.148	0.087	1.698	0.090	-0.051	0.159	-0.323	0.747
General scenario * Perceived privacy	0.031	0.087	0.357	0.721	-0.351	0.142	-2.466	0.015
Major field: Other					0.198	0.218	0.907	0.366
Experience: no					-0.516	0.237	-2.172	0.031
Experience: yes, other relevant					-0.594	0.320	-1.858	0.065
Year: 2nd					0.239	0.412	0.579	0.563
Year: 3rd					0.270	0.412	0.656	0.513
Year: 4th					0.352	0.443	0.794	0.428
Year: 5th or higher					-0.525	0.690	-0.761	0.447
Age (continuous)					-0.014	0.018	-0.765	0.446
Other latent variables were found to be nonsignificant and were used as our control. We found that the constructs representing personal empathic elements, empathic concern, and perspective-taking had no significant effect on the feature's appropriateness. Effects, therefore, resulted from increasing empathy through personified scenarios usage and not as a result of being generally more empathic. Lastly, both age and gender were found to be nonsignificant variables.

4. STUDY 2. EFFECT OF PROFESSIONAL CHARACTERISTICS
The next study aimed at testing the remaining two hypotheses. We explored the effects of personal characteristics, including academic background and the existence of engineering experience, on a system's appropriateness. To conduct this study, we recruited students from various disciplines.

4.1. Methodology
4.1.1. Experimental design
The experiment design closely followed the design of the first study, except for the demographics selection and reporting. We removed questions that had many possible answers, related to the participants’ education level and academic major. Instead, knowing that the participants are students from Tel Aviv University, we asked more specific questions. The participants were asked at which faculty are they studying, whether they are currently undergrad or grad students, and at what stage of the studies are they (on what year). We then asked them whether they are working or have worked in the field of their studies and if their answer was positive, they were asked for how long.

4.1.2. Recruitment
We recruited 246 participants using an internal process of Tel Aviv University (Arad, 2021), and by attending classes and asking students to participate. We wanted to distinguish between participants from engineering and non-engineering fields. Fields of study that were not differentiated enough and could not be associated with one of the two groups were excluded. Therefore, we did not recruit students whose major was economics. The students were in their second year of studies, or higher, and most of them were undergraduates. The questionnaire took approximately 13.4 minutes to complete. The participants were compensated by participating in a raffle of 100 NIS (approximately $30) with approximately 10% chances of winning. After filtering out participants who completed the screening task incorrectly, we removed 37 responses and were left with 209 responses. One hundred and nineteen participants were female (57%), 88 were male (42%) and two participants did not reveal their gender (1%). The average age was 25.6 years.

4.1.3. Data analysis
We followed the first study's results analyses and averaged the scores given to four scenarios (wefit, wemail, bip, album creator) to create the appropriateness construct. Similar to study 1, we removed the item related to the Emoji Keyboard scenario. Moreover, for analysis purposes, we defined levels for some of the variables. We divided the students according to their major field of studies into two groups: 1) engineering and exact sciences and 2) all other major fields. For their experience, we divided these into three groups: 1) yes – relevant to engineering or exact sciences; 2) yes – relevant to other major fields; and 3) no. For the analysis specified in this study, we used the Scheirer-Ray-Hare test. This nonparametric analysis measures the effect of two categorical independent variables on the dependent variable, similar to the parametric multivariate analysis of variance (MANOVA) test. We used it to explore the influence of both the information framing and the major field of studies.

4.2. Results
We used the Scheirer-Ray-Hare test to analyze the influence of both the information framing and the major field of studies on a system's appropriateness. The analysis showed a significant difference among the three framings’ conditions only (p < 0.001). We were surprised to see that the difference among the major fields of studies was insignificant (p = 0.34), as was the interaction effect between the framing type and the major field (p = 0.368). A post hoc analysis for the information framing type reproduced the same results as in the first study, showing significant differences between the personified scenarios conditions to the data-flows condition, but not between the personified scenarios conditions (p-value: detailed scenarios vs. data-flows; and general scenarios vs. data-flows: p < 0.001). No differences in gender or other demographic properties were found.

Lastly, we conducted a linear regression analysis to predict the feature's appropriateness. The regression consisted of nine variables, latent variables, and interaction variables (see Table 2). Within the shared variables of the first and second studies, the regression model (adjusted R2 = 0.15) reproduced the same results as in the first study. We observed a lower perception of the feature's appropriateness when using personified scenarios and higher perceptions of the feature's appropriateness with a higher perception of privacy. For the variables that were investigated only in the second study, we found that engineering work experience had a significant positive influence on the feature's appropriateness. Participants with engineering work experience perceived the feature as more appropriate compared to students without any work experience (Estimated coefficient = -0.516, p = 0.031). We did not find a significant difference between participants with work experience in engineering and participants with work experience related to their other academic backgrounds (Estimated coefficient = -0.594, p = 0.065). Therefore, the results partially supported our fifth hypothesis, in which we hypothesized that crowd users with engineering-related work experience would perceive features designs as more appropriate. Echoing the results found using the Scheirer-Ray-Hare test, the results did not support our fourth hypothesis, in which we hypothesized that crowd users with engineering-related academic backgrounds would perceive features designs as more appropriate. The regression results showed that the students’ major field of studies was insignificant predictor of the feature's appropriateness. For the interaction effect, we found a significant interaction between participants’ perceived privacy and framing. For general scenarios, a higher perception of privacy led to lower perceptions of the feature's appropriateness (Estimated coefficient = -0.351, p = 0.015). For detailed scenarios, we found no significant interaction effect: a higher perception of privacy resulted in a higher perception of the system's appropriateness. See Figure 2 to see the visualization of the interaction effects. We also analyzed the interaction effect when referring to “detailed scenarios” as the baseline and found no significant interaction effect when general scenarios are compared to detailed scenarios (Estimated coefficient = -0.299, p = 0.06). See Table 2 for a comparison of the first and second studies.

Figure 2
Download : Download high-res image (336KB)
Download : Download full-size image
Figure 2. Feature's appropriateness versus the participant's perceived privacy, comparing different types of information framing. In General scenarios condition, compared to Data-flows condition, a higher perception of privacy led to lower perceptions of the feature's appropriateness (Estimated coefficient = -0.351, p = 0.015).

5. DISCUSSION
This study aimed to evaluate the effect of framing privacy design issues with personified scenarios on the perceived system's features’ appropriateness. We used an experimental setting that included a comparison between types of information framings. Our findings show that the usage of personified scenarios has an adverse influence on the feature's appropriateness, confirming our first hypothesis. Perceiving the feature as less appropriate points to an increase in the participants’ critical judgment of the feature's design. We further explored different levels of detailed personified scenarios but found no significant difference in the feature's appropriateness when comparing detailed levels, thus rejecting our second hypothesis. Taking into account a personal characteristic that could affect the feature's evaluation, we explored personally perceived privacy. We found that the more the participants had a general perception of having privacy, the more they perceived the feature as more appropriate, supporting our third hypothesis.

In the second set of hypotheses, we explored two professional characteristics: academic background and engineering-related work experience. We explored whether engineering students differ from non-engineering students in their perceptions of the feature's appropriateness. We found that work experience significantly influenced the feature's appropriateness. Participants with experience in engineering-related work significantly perceived the feature as more appropriate, compared to students with no work experience, partially supporting our fifth hypothesis. Our fourth hypothesis was rejected, as we did not find that the participants’ major field of studies significantly influences their perceived feature's appropriateness. Lastly, we found that these results hold even when we control for other personal related variables, including individual differences in empathy. We measured the participants’ empathy concerns and perspective-taking constructs and found that these variables did not influence the feature's appropriateness.

5.1. Theoretical Implications
5.1.1. Users’ involvement
At its core, the concept of privacy-by-design is rather simple and reasonable: to develop technologies that respect privacy – and there is a need to develop them accordingly from the very beginning. However, the transfer from idea to implementation is fraught with difficulties. On the one hand, PbD supports organizations by providing engineers and managers with motivation and inspiration to develop respective privacy technologies (Levin, 2018). On the other hand, it is lacking in engineering support (Levin, 2018) and is often reduced from its intentionally holistic approach to self-regulatory measures in the industry (Hartzog, 2018).

The PbD's seventh principle is about developing technology while respecting the user and “keep[ing] it user-centric” (Cavoukian, 2009). However, after reading guidelines as to how to conduct such design in practice, we found what Wong and Mulligan referred to as “viewing design as a solutionism lens” (Wong and Mulligan, 2019). For example, in ICO's guidelines, they suggest one should “offer strong privacy defaults, user-friendly options and controls, and respect user preferences” (ICO (Information Commissioner's Office), 2021). Wong and Mulligan criticize the PbD approach for referring to privacy design mostly as solving predefined problems, preventing the rise of exploratory and speculative design practices that will also solve future problems (Wong and Mulligan, 2019).

To fully adopt a user-centric approach, users should be involved in the development process, which is UCD's basic call for designers. Usable privacy studies take a UCD approach and explore users’ preferences and understanding of a system's privacy, looking for ways to improve users’ understanding and ability to control their privacy in systems (Jeffrey Warshaw et al., 2016; Mazzia et al., 2012; Naeini et al., 2017). Our study also belongs to usable privacy studies; however, we are asking users to become involved in the PbD context and exploring how to conduct this involvement. Therefore, we used the term crowd users. These individuals do not use the feature directly as system users. Instead, they are involved in its development process. Our study is a starting point to developing appropriate methods for involving crowd users in features development processes, specifically in the context of privacy design. We point to a small set of essential questions that need to be asked when developing such a methodology: how to frame the scenarios for crowd users and how personal characteristics may impact their responses.

Our study results support the argument whereby PbD should take an HCI approach and specifically involve users. Previous privacy incidents, for example, the previously described initial design of Facebook Messenger (King, 2015), point to the possible consequences of designing privacy without involving users and leaning on internal consultations only. Our results empirically show that it is required to involve the users during the development process to design systems that meet their privacy expectations. Engineers, as our results are pointing, are significantly different from the general population when evaluating the system's appropriateness. We compared the system's appropriateness among three groups: 1) students with engineering-related work experience, 2) students with work experience related to other fields (art, education, environmental studies, humanities, law, life sciences, management, medicine, neuroscience, social science, and social work), and 3) students with no work experience. We found that students with engineering-related work experience were significantly less critical in their system's evaluation compared to inexperienced students. Often, engineers and designers are not good representatives of a product's intended users. When users are not involved in the development process, and the designers consider themselves as representative users, the resulting outcome may fit the designers’ abilities and beliefs rather than to those of the intended users (Oudshoorn et al., 2004). Without involving users, without making an effort to understand their privacy expectations, engineers and designers develop systems based on their understandings and expectations. This conduction can partly explain the existence of privacy designs that surprise users. The designers, engineers, and other stakeholders in the organization have different perceptions than those of users, and if these users are not involved, the design may reflect the engineers’ perceptions. For this reason, methodologies for taking a user-centric perspective were suggested.

5.2. Design Implications
5.2.1. Emphasizing the users’ perspective
To involve the users in PbD processes, and as a result, to extend internal organizational consultations, one must understand how to mediate the problem the organization is considering. Using a between-subject study design, we were able to compare different types of information framings: data-flows versus personified scenarios, and to compare different levels of detailed personified scenarios: general versus detailed. Our results suggest that this type of involvement is nuanced. Merely asking about an information flow will not provoke users’ sense of inappropriateness. The context and possible privacy-violation implications will be better understood if explained from a human perspective. However, unlike common practices that are used in advertisements, more details about the presented user in the scenario are not required. The effect of adding details, such as a short bio and a photo, on the perceived appropriateness was insignificant. There are several possible explanations for this, one of them being the selected user. All the users in the scenarios were young females, possibly reducing empathy to a certain extent. Our results also point to the resistance of general scenarios framing to the effect of personally perceived privacy on appropriateness evaluation. In the general scenarios condition, higher perception of privacy did not result in higher perception of the feature's appropriateness. The impact of framing point to the relative ease of manipulating the results of evaluations that aim to evaluate the appropriateness of systems. Therefore, our findings can be useful in creating evaluation metrics that consider possible manipulations and to effectively control for them.

5.2.2. Which users to involve and when?
Involving users in a development process raises various questions that should be considered, including which methods to use, how many participants are required, and which participants to recruit, based on their personal characteristics. We found two characteristics that affected the crowd users’ feature's appropriateness: their personal privacy perceptions and professional work experience. We found that the more the participants had a general perception of having privacy, the less critical was their judgment of the system's appropriateness. The same direction of result was observed when the participants had an engineering-related work experience. Our findings confirm several previous results that are referring to personal perceived privacy. Dinev et al. found that higher perceptions of privacy are associated with lower perceptions of risk (Dinev et al., 2013). Other studies have shown that lower perceived information control is associated with higher privacy concerns (Acquisiti et al., 2006; Hoadley et al., 2010; Xu, 2007) and that higher perceived information control resulted in greater privacy risky behavior (Brandimarte et al., 2013). As for the impact of engineering-related work experience on the feature's evaluation, Sheth et al. also found that developers and end-users have different privacy perceptions (Sheth et al., 2014). The current study results highlight two distinct but related aspects of a representative-sample recruit. First, consulting only internal organizational stockholders as developers or privacy experts can result in a biased sample, as they might have similar privacy perceptions. Second, also when involving the users, their personally perceived privacy and their professional work experience affects their system's design evaluations; these personal characteristics should be considered when choosing representative users.

5.3. Limitations and Future Work
Our study is subject to several limitations. First, for the first study, we did not limit the crowd used to specific workers who may be relevant to a specific application. Future studies might test the possibility of selecting specific types of crowd workers. Second, we only examined privacy violations that are visible and detectable by users. Data uses that occur in the background were not within our study's scope. Third, concerning the study's dependent measures, we used the same direction in all scenarios, in which choosing a lower score (from 1 to 7) represented a lower perception of the system's appropriateness. Fourth, for the detailed scenario condition, we made an effort to use representative users who were as similar as possible. However, there is still a chance that the participants answered in a certain way due to considerations referring to a particular user's details. However, as the SE of the mean of the detailed scenario score is similar to that of the two other conditions, we can assume that even if this did occur, it did not occur in most cases.

Our scenarios were based on privacy designs that included a social aspect. We have not tested scenarios with an institutional aspect (user-user interaction versus user-system interaction). As a result, the current study reflects the potential of personified scenarios to mediate privacy design issues to end-users mostly in social aspects. Although social privacy aspects are very common, given the extensive use of social networks, further research is required to understand how we can collect feedback about aspects of institutional privacy.

6. CONCLUSIONS
This paper investigates how to involve users in privacy design processes under the normative assumption of promoting a privacy-respecting feature design. Our study explores how personified scenarios can be used to increase users’ critical judgment of the feature's privacy design. Using an online experimental design (n=665), we found that framing privacy design dilemmas while representing the end-users’ perspectives, and not solely as a matter of “data,” reduced the extent to which crowd users perceived the feature as appropriate, pointing to increased critical judgment.

Our findings highlight the necessity of involving users during the development process and specifically in the context of privacy. Students with engineering-related work experience perceived the systems as more appropriate than students with no work experience. The study findings show that involving users in privacy-by-design processes is important, as we have shown that different perceptions, backgrounds, and framings, affect the feature's evaluated appropriateness and are necessary for reducing privacy risks. We thus conclude that organizations should consider involving users for providing a comprehensive view of varying privacy aspects. Our study provides a starting point for an understanding of how to involve users in practice.

Funding
This work was supported by the Shulamit Aloni Scholarship by the Israeli Ministry of Science and Technology, grant number 314575, and by the ICRC – Blavatnik Interdisciplinary Cyber Research Center, grant number 590713.

Declaration of Competing Interest
None.

Acknowledgments
We would also like to thank Luiza Jarovsky for helping us with finalizing the scenarios.

APPENDIX. SURVEY INSTRUMENT
Appropriateness measure

Please imagine that you are working for a company called WeCreateApps, a software company that develops different applications. As part of your job you make decisions about the application's design.

[advanced and basic personas versions only:] Your team works on creating new mobile applications. As part of the design, your team is interested in understanding how the users are using the applications, their feelings about them and etc. Therefore, interviews were conducted with several end-users.

The following are short descriptions of the applications that are currently being developed. [advanced and basic personas versions only:] In addition, quotes from the users about future applications are also presented. Please read the descriptions and quotes and answer the following questions.

[Each scenario was followed by question, on which the participants answered using a scale from 1 to 7 (1 - Strongly disagree, 7 - Strongly agree]

1. “WeMail” (free application)
The app enables the user to manage his/her emails (send, receive, etc.).

[data:] The app requests for permission to constantly access the user's contacts list. The information is also used in order to send emails to the user's contacts in order to attract new users.

[advanced and basic personas versions:] Lisa Johnson, end-user: “From my own experience, similar apps are requesting to constantly access my contacts list. Few hours after installing the app, several friends of mine told me that they received an email from WeMail, suggesting them to install the app as well.”

Question: As a team member of the app company, I think it is okay for the app to use the contacts list information.

2. “Photo Album Creator” (free application)
The app enables the user to create photo albums using photos that are located on the device's memory card (for example, photos that were taken by the device's camera or photos that were sent via other apps, such as WhatsApp).

[data:] The app shares the albums with other users that are defined as the user's "Friends" by default. The sharing settings can be changed by the user.

[advanced and basic personas versions:] Kathy Griffin, end-user: "After creating the first album one of my friends told me she enjoyed looking at it. I only then recalled that although I can change the settings, the albums are constantly shared by default with other users that are defined as "Friends".

Question: As a team member of the app company, I think it is okay to share the albums with friends by default.

3. “WeFit” (free application)
The app enables the user to track his/her sports activities, providing information such as calorie count, speed, and location tracking.

[data:] Once a user shares with another user his/her activity information, the app automatically shares future activity information, including current location.

[advanced and basic personas versions:] Amy Smith, end-user: “Me and some of my friends downloaded the app. After finishing the first run, I shared with my friend, Julie, my running pace and all other measurements (average speed and etc.). A few days later, Julie surprised me with a bottle of juice at one point along my route. She explained that once I shared with another user my activity information, the app automatically shares future activity information, including current location.”

Question: As a team member of the app company, I think it is okay that once a user shared his/her fitness activity information with another user, the app shares future activities by default.

4. “BiP” (free application)

The app enables the user to share different kinds of information with other users, similar to the Facebook app.

[data:] Every week the app re-shares a post that was published in the previous week, it is not explained why specific posts are chosen to be re-shared.

[advanced and basic personas versions:] Karen Baker, end-user: "I was asked (by WeCreateApps team) to download the app and to use it for a month. I noticed that every week one of the posts I shared in the previous week was re-posted on my wall. I don't know why specific posts were chosen to be re-shared by the app.”

Question: As a team member of the app company, I think it is okay to re-share previous posts by default.

(*) “Emoji Keyboard” (free application)

The app enables the user to send messages with special emojis.

[data:] The app requests for permission to constantly access the user's exact location.

[advanced and basic personas versions:] Donna Miller, end-user:  "Usually, before I download a new app, I check which permissions are required. For example, the required permissions included a constant access to my exact location. I accepted the required permissions and it provided me with some very cool emojis for my outgoing messages (for Snapchat, WhatsApp and so on).

Question: As a team member of the app company, I think it is okay for the app to request the location information.

(*) was removed due to low EFA loading

Perceived privacy measure

The following three questions are about your personal privacy, as an end-user. When you answer the questions, please think about the limited access the Web sites and apps have to your personal information.

1
I feel I have enough privacy when I use these Web sites and apps.

2
I am comfortable with the amount of privacy I have.

3
I think my online privacy is preserved when I use these Web sites and apps.

Empathic concern measure

1
When I see someone being taken advantage of, I feel kind of protective toward them.

2
When I see someone being treated unfairly, I sometimes don't feel very much pity for them.

3
I often have tender, concerned feelings for people less fortunate than me.

4
I would describe myself as a pretty soft-hearted person.

5
Sometimes I don't feel sorry for other people when they are having problems.

6
Other people's misfortunes do not usually disturb me a great deal.

7
I am often quite touched by things that I see happen. Personal.

Perspective-taking measure

1
Before criticizing somebody, I try to imagine how I would feel if I were in their place.

2
If I'm sure I'm right about something, I don't waste much time listening to other people's arguments. (*)

3
I sometimes try to understand my friends better by imagining how things look from their perspective.

4
I believe that there are two sides to every question and try to look at them both.

5
I sometimes find it difficult to see things from the "other guy's" point of view. (*)

6
I try to look at everybody's side of a disagreement before I make a decision.

7
When I'm upset at someone, I usually try to "put myself in his shoes" for a while.

(*) was removed due to a lowered Cronbach's α value

Screening task

Former studies in the field of decision making show that people, when making decisions and answering questions, are not always paying attention and are minimizing their effort as much as possible. A few studies show that over 50% of people don't carefully read questions. If you are reading this paragraph and the following question, and have read all the other questions, please select the box marked ‘other’ and type ‘Mobile decision-making’ in the box below. Do not select anything else. Thank you for participating and taking the time to read through the questions carefully!

What was this study about?

() Behavioral prediction; () Making decisions about mobile apps; () Restaurants and movies; () Other

