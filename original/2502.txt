Backbreak is a rock fracture problem that exceeds the limits of the last row of holes in an explosion operation. Excessive backbreak increases operational costs and also poses a threat to mine safety. In this regard, a new hybrid intelligence approach based on random forest (RF) and particle swarm optimization (PSO) is proposed for predicting backbreak with high accuracy to reduce the unsolicited phenomenon induced by backbreak in open-pit blasting. A data set of 234 samples with six input parameters including special drilling (SD), spacing (S), burden (B), hole length (L), stemming (T) and powder factor (PF) and one output parameter backbreak (BB) is set up in this study. Seven input combinations (one with six parameters, six with five parameters) are built to generate the optimal prediction model. The PSO algorithm is integrated with the RF algorithm to find the optimal hyper-parameters of each model and the fitness function, which is the mean absolute error (MAE) of ten cross-validations. The performance capacities of the optimal models are assessed using MAE, root-mean-square error (RMSE), Pearson correlation coefficient (R2) and mean absolute percentage error (MAPE). Findings demonstrated that the PSO–RF model combining L–S–B–T–PF with MAE of 0.0132 and 0.0568, RMSE of 0.0811 and 0.1686, R2 of 0.9990 and 0.9961 and MAPE of 0.0027 and 0.0116 in training and testing phases, respectively, has optimal prediction performance. The optimal PSO–RF models were compared with the classical artificial neural network, RF, genetic programming, support vector machine and convolutional neural network models and show that the PSO–RF model has superiority in predicting backbreak. The Gini index of each input variable has also been calculated in the RF model, which was 31.2 (L), 23.1 (S), 27.4 (B), 36.6 (T), 23.4 (PF) and 16.9 (SD), respectively.

Introduction
Explosives are widely used to break hard rock mass in open-pit mining due to their low costs. However, the explosive energy is poorly utilized, with nearly 70 to 80 percent of the explosive energy dissipating in the ground, which may cause several detrimental influences [5, 81], i.e., blasting fume, ground vibration, noise, backbreak, flyrock. Particularly, backbreak, as part of side effects of the explosion, is a rock fracture phenomenon that exceeds the limits of the last row of holes in an explosion operation [30], which has various undesirable impacts, such as an increase in the stripping ratio, falling down the mining machinery, instability in mine walls, reduction in efficiency of drilling and lower in the overall slope angle [18, 34, 58, 79]. Therefore, accurate estimation of backbreak before a blasting operation is of great significance to minimize the harmful impact of backbreak.

Figure 1 represents an open-pit bench terminology. Figure 1 shows that explosive properties, blast design parameters and rock mass properties have certain effects on backbreak. Controllable factors, namely explosive properties and blast design parameters and uncontrollable factors, namely rock mass properties have been selected by numerous researchers to predict backbreak [15, 51, 53, 79]. Lundborg [31] and Roth [57] have attempted to predict backbreak with some empirical models. Nevertheless, these empirical models are capable of predicting backbreak under certain geo-mining conditions and are based on only a few influencing factors. Models featuring wider adaptability between backbreak, and the influencing parameters are needed to minimize production cost vis-à-vis to enhance the safety and stability of an open-pit mine.

Fig. 1
figure 1
The appearance of open-pit bench

Full size image
Currently, various artificial intelligence (AI) techniques including fuzzy set theory [27, 62], ANN [7, 17, 64], SVM [21, 42, 73, 80], GP [4, 45], CNN [25] and neuro-genetic approach [1] utilize capturing nonlinear relationships between multi-dimensional variables which have made a great success in plenty of geotechnical engineering applications, and have been showing good performance in the field of predicting rockbursts [66, 74, 75], blast vibrations [2, 26, 28, 38, 67] and ground settlement [22, 47, 68, 78]. Regarding backbreak prediction, Table 1 summarizes some published literature on backbreak prediction. It has been found that ANN is mostly used to predict backbreak; however, other models, such as SVM, GP, ANFIS, etc., have also been used. As a branch of ensemble learning, a random forest algorithm shows good prediction performance in a large number of databases, less over-fitting phenomenon, fast training speed, and the importance of each feature can also be evaluated internally [56, 68]. In addition, as a widely used swarm intelligence algorithm, PSO has been proved to have the advantages of fewer parameters to be adjusted, easy implementation, the use of individual local information and global information of the group to guide the search and better hyper-parameter selection ability compared with other algorithms [55, 71, 80, 82, 83]. Based on this, a hybrid artificial model combining PSO and RF, namely PSO-RF is presented in this study to predict backbreak. To compare the proposed PSO-RF model prediction capability, various other AI algorithms, such as ANN, RF, GP, SVM and CNN, which are popular or potential in predicting backbreak, are also adopted in this study.

Table 1 Current literature on backbreak prediction applying AI techniques
Full size table
In this article, first, the background of the proposed methodologies has been presented. Then, the framework of the proposed model is illustrated, and the establishment of datasets are introduced. After searching the optimum hyper-parameters of different combinations of PSO-RF models, the optimal combination of PSO-RF will be determined. Moreover, the optimal PSO-RF models will be compared with classical models. Finally, the Gini index will be calculated internally in the RF model to investigate the most important input variables in estimating backbreak.

Methodology
Random forest algorithm
Random forest [9] is a supervised algorithm composed of an independent decision tree (DT) and bagging framework. Here, the Bootstrap sampling method is utilized to stochastically extract a certain amount of data from the training set N to form a bootstrap training set Nt. Accordingly, DTs for each bootstrap training set Nt are built. Based on bootstrap training sets, out-of-bag (OOB) predictors (about a third of N) is built, which contain non-existing samples in the Nt. In the OOB error estimation process, OOB predictors play a test set role, so there is no need to create another test set. The essence of random forest is the integration of DTs [76], which forms multiple DTs through the randomization of column variables and row values of the dataset, and eventually averages the results of the DTs as per Eq. (1). Random feature selection is carried out after random data selection. The double randomness reduces the correlation between DTs, decreases the phenomenon of over-fitting, and has good anti-noise ability. When constructing the DT, the procedure of pruning is not implemented to avoid inhibiting the growth of the tree. Each tree is composed of randomly selecting column variables and row observations. Single DT is difficult to predict correctly, but all DTs form a forest, making the aggregated results integrate the results of all DTs, so the overall prediction is more accurate [77]. Alongside, the mean decrease in the Gini index was also calculated showing the variable importance within the model as per Eq. (2).

𝑦ˆ(𝑥)=1𝐵∑𝑏=1𝐵𝑇(𝑥,𝐸𝑏)
(1)
where 𝑦ˆ(𝑥) is the result of a combined prediction model, B represents the overall number of DTs, and 𝑇(𝑥,𝐸𝑏) is the results of all DTs generated from bootstrapped training samples.

𝐺𝑖𝑛𝑖(𝑋𝑖)=∑𝑗−1𝐽𝑃(𝑋𝑖=𝑌𝑗)(1−𝑃(𝑋𝑖=𝑌𝑗))=1−∑𝑗𝐽𝑃(𝑋𝑖=𝑌𝑗)2
(2)
where 𝐺𝑖𝑛𝑖(𝑋𝑖) is the Gini index, 𝑃(𝑋𝑖=𝑌𝑗) is probabilities and 𝑋𝑖=𝑌𝑗 is estimated values. The flowchart of building the random forest is shown in Fig. 2.

Fig. 2
figure 2
The flowchart of building the random forest

Full size image
Particle swarm optimization
The PSO algorithm was presented for solving unconstrained optimization problems [12], which simulates the behavior of bird swarms or fish swarms. Depending on its simplicity and remarkable search efficiency, the success of PSO has been verified in many fields, such as function optimization, vehicle routing optimization, geodesy and image processing [6, 11, 29, 46, 60, 65]. The architecture of the PSO algorithm is shown in Fig. 3. The algorithm starts by randomly locating N particles in the search space. Each swarm particle has its unique position vector 𝑥𝑖𝑑(𝑡) and velocity vector, 𝑣𝑖𝑑(𝑡). The position and velocity of the ith particle in each iteration are updated as follows:

𝑐1𝑟1[𝑝𝑖𝑑(𝑡)−𝑥𝑖𝑑(𝑡)]+𝑐2𝑟2[𝑝𝑔𝑑(𝑡)−𝑥𝑖𝑑(𝑡)]+𝑣𝑖𝑑(𝑡)=𝑣𝑖𝑑(𝑡+1)
(3)
𝑣𝑖𝑑(𝑡+1)+𝑥𝑖𝑑(𝑡)=𝑥𝑖𝑑(𝑡+1)
(4)
Fig. 3
figure 3
The architecture of particle swarm optimization

Full size image
where d is the dth dimension, 𝑥𝑖𝑑(𝑡) and 𝑣𝑖𝑑(𝑡) are the position and velocity of the ith particle at the tth iteration; 𝑝𝑖𝑑(𝑡) = historical best position found by the ith particle; and 𝑝𝑔𝑑(𝑡) = historical global optimal position found by all particles; 𝑐1 and 𝑐2 are constant called acceleration coefficients, both 𝑐1 and 𝑐2 are equal to 1.49. 𝑟1 and 𝑟2 are two generated random numbers in the interval [0, 1]. The size of the population is set as 20, which is sufficient to earn the optimum position vector.

Herein, the optimal position is the optimal hyper-parameter that occurs at the position, where fitness is minimized and maintained constant. The maximum generation is set to 100 to obtain the optimal results. Table 2 presents the values of parameters in PSO-RF algorithms.

Table 2 Parameters in PSO algorithm
Full size table
The hybrid PSO-RF model
Particle swarm optimization algorithm has the advantages of independent problem information, strong universality of the algorithm, few parameters to be adjusted, simple principle and easy implementation [12]. Therefore, PSO is adopted to optimize the hyper-parameters 𝑚𝑡𝑟𝑦 and 𝑛𝑡𝑟𝑒𝑒 of the RF algorithm. The PSO-RF model framework is shown in Fig. 4, which has three stages: data processing, RF model training and testing.

Fig. 4
figure 4
Flow chart of the proposed PSO-RF model

Full size image
In the data processing stage, the data set consists of several selected influential factors, and then the data are divided into seven different parameter combinations by feature selection method to obtain the best combination. After that, 80% of data is randomly assigned to train the model, while the rest of the data are used to test the model [69, 81]. The data in this paper were all measured in practice, and there are no outliers, duplicate values and missing values, so there is no data cleaning, and all data are used in this paper. All data were normalized into [−1,1] to increase the computational efficiency and enhance the performance of the model.

In the training stage, the optimal RF models are determined when the optimal hyper-parameters of the models are searched by the PSO algorithm. Firstly, the initial location and speed of the particles are randomly assigned, and the corresponding hyper-parameters of the RF model are specified. To acquire the fitness of each model, an approach called tenfold CV is favored. After the fitness of each round has been calculated, the local optimal position and global optimal position of the particle swarm are determined. Because the position and velocity of particles are dynamic, the best RF model can be obtained when the number of iterations reaches the maximum and the fitness value does not change.

In the testing stage, the seven optimal models were evaluated by their respective test sets. Better performance models have higher overall scores in indicators (MAE, RMSE, R2 and MAPE).

There are several ways to validate models, including the holdout method, cross-validation and bootstrap [10, 39, 72]. Since the data used in this paper is limited, if most of the data are used for training models, it will easily lead to model overfitting, so a cross-validation method is used [40, 42, 63]. In this study, a tenfold CV method which is widely used and has proven to have good performance is proposed to improve the credibility of the hybrid models, and the MAE of ten datasets is employed as a fitness function to quantificationally evaluate the accuracy of the hybrid model, as shown in Eq. (5) [69].

𝐹𝑖𝑡𝑛𝑒𝑠𝑠=110∑𝑖=110𝑀𝐴𝐸𝑖
(5)
where 𝑀𝐴𝐸𝑖 refers mean decrease error of ith validation set.

Grey relational analysis
As aforementioned above, BB is affected by many factors, and the correct selection of influencing factors has great effects on the prediction efficiency and accuracy of the model. The correlations among variables have been evaluated by grey correlation analysis [32, 41, 70], which is adopted to calculate the correlation degree between BB and the six selected factors. The following are the calculation steps of grey relational analysis:

1.
Reference variable 𝛿𝑂=𝛿𝑂[𝛿𝑂(1),𝛿𝑂(2),…,𝛿𝑂(𝑛)] and compared variable 𝛿𝑡=𝛿𝑡[𝛿𝑡(1),𝛿𝑡(2),…,𝛿𝑡(𝑛)] are given.

2.
Reducing the error of correlation analysis, interval transformation is used to make all variables dimensionless. The method is as follows:

𝛿𝑡(𝑢)=𝛿𝑡(𝑢)−min𝑢𝛿𝑡(𝑢)max𝑢𝛿𝑡(𝑢)−min𝑢𝛿𝑡(𝑢)
(6)
where t = 1, 2, …, m; u = 1, 2, …, n.

3.
The following is the calculation formula of grey relational degree between variables:

𝛾(𝛿𝑂(𝑢),𝛿𝑡(𝑢))=min𝑡min𝑢|Δ𝛿|+𝜒max𝑡max𝑢|Δ𝛿||Δ𝛿|+𝜒max𝑡max𝑢|Δ𝛿|
(7)
where Δ𝛿=𝛿𝑂(𝑢)−𝛿𝑡(𝑢), and the smaller it is, the bigger the correlation is. 𝜒 = the resolving coefficient, was set to be 0.5. min𝑡min𝑢|Δ𝛿| and max𝑡max𝑢|Δ𝛿| denotes minimum and a maximum deviation of δO and δt, respectively, and their addition prevents grey relational degrees from being identical to 0 when one of them is 0.

4.
The grey relational grade between the variables can thus be obtained by:

𝛾(𝛿𝑂,𝛿𝑡)=1𝑛∑𝑢=1𝑛𝛾(𝛿𝑂(𝑢),𝛿𝑡(𝑢))
(8)
where a large grey relational grade suggests a strong correlation between variables 𝛿𝑂 and 𝛿𝑡.

Evaluation indicators
Four evaluation indicators, namely mean absolute error (MAE), root-mean-square error (RMSE), mean absolute percentage error (MAPE) and Pearson correlation coefficient (R2), are applied in this work to evaluate the performance of the hybrid RF models. Scale-dependent indicator MAE and RMSE and scale-independent indicator R2 and MAPE are applied to indicate the error between the measured and predicted value. MAPE can be used as a prediction index when the predicted value is not 0. If the error dispersion is high, that is, the maximum deviation is large, then the RMSE will increase because the RMSE is the square of the deviation. The range of R2 is 0–1, the larger R2 means the greater the correlation degree, and vice versa. The following equations depict the definition of MAE, RMSE, MAPE and R2 [3, 69, 77, 82, 83]:

𝑀𝐴𝐸=1𝑛∑𝑖=1𝑛∣∣𝐵𝐵𝑖−𝐵𝐵′𝑖∣∣
(9)
𝑅𝑀𝑆𝐸=1𝑛∑𝑖=1𝑛(𝐵𝐵𝑖−𝐵𝐵′𝑖)2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾⎷
(10)
𝑀𝐴𝑃𝐸=1𝑛∑𝑛𝑖=1∣∣∣𝐵𝐵𝑖−𝐵𝐵′𝑖𝐵𝐵𝑖∣∣∣×100%
(11)
𝑅2=1−∑𝑛𝑖=1(𝐵𝐵𝑖−𝐵𝐵′𝑖)2∑𝑛𝑖=1(𝐵𝐵𝑖−𝐵𝐵⎯⎯⎯⎯⎯⎯⎯⎯𝑖)
(12)
where 𝐵𝐵 denotes the actual backbreak distance, 𝐵𝐵′ = predicted backbreak distance, n is the number of samples, and 𝐵𝐵⎯⎯⎯⎯⎯⎯⎯⎯ refers to the average of actual backbreak distance.

Backbreak database and its description
The datasets used in this study were conducted by Khandelwal and Monjezi [33] and composed of 234 blasting datasets of Sungun Copper Mine, Iran. As given in Table 1, backbreak is affected by many factors, but only the changeable parameters during the blasting operation in Sungun Copper Mine are considered in this study, and other parameters such as rock properties, hole diameter and bench height that affect BB remained constant, and due to that such constant parameters were not considered in this study. This database includes the following input parameters, namely burden (B), hole length (L), stemming (T), spacing (S), powder factor (PF) and special drilling (SD). Note that these input parameters have been widely used in previous models with better predictive performance [16, 19, 24, 33, 53, 54], and thus it is promising to build a superior model to predict the backbreak in the Sungun Copper mine. All parameters are recorded before each blasting operation. Because the bench crest is an uneven edge, several points on the bench crest are usually selected to measure the horizontal distance between them and the critical cracks, and the average value is regarded as backbreak distance. The descriptive statistics of the parameters are given in Table 3. To visualize the distribution of input parameters is normalized to [0,1] and plotted in a violin diagram in Fig. 5. The datasets are randomly divided into 190 training datasets (approximately 80%) and 44 (approximately 20%) testing datasets, to develop the prediction model and evaluate the generation ability of the model.

Table 3 The descriptive statistics of the parameters
Full size table
Fig. 5
figure 5
Violin plots of input parameters for BB prediction

Full size image
As aforementioned above, backbreak is affected by many factors, and the correct selection of influencing factors has great effects on the prediction efficiency and accuracy of the model. The correlations among variables have been evaluated by grey correlation analysis [32, 41], which is adopted to calculate the correlation degree between backbreak and the six selected factors.

Figure 6 shows that the grey relational grade between backbreak and each influential factor is in the range of 0.74–0.83, S has the highest degree of correlation between backbreak, and SD has the least degree of correlation between backbreak. Grey correlation analysis shows that the inputs and output have a high correlation, suggesting that the influential factors selected in this article are suitable for predicting backbreak.

Fig. 6
figure 6
The grey relational grade between variables

Full size image
Result
Optimal hyper-parameters
As aforementioned above, the PSO algorithm is applied to optimize the hyper-parameters (i.e., mtry and ntree) of RF models, and MAE is applied as a fitness function to determine the optimum two hyper-parameters. Table 4 summarizes optimal hyper-parameters in each PSO-model. The fitness of seven models is obtained by using 100 generations as the stop criteria. As shown in Fig. 7, the fitness values of the seven models remained constant after 22 generations, suggesting that the optimal model could be obtained before 100 generations. The fitness values of RF models range from 0.6842 to 0.1026. The lowest fitness value, 0.6842, appears in the RF model combining L–S–B–T–SD, even better than the RF model combining all variables, suggesting a better performance in CV sets of the model. The biggest fitness value appears in the RF model combining L–S–B–T–PF and keeps constant from the first generation, indicating that the model was relatively worse in the CV sets.

Table 4 Optimal hyper-parameters in each PSO-model
Full size table
Fig. 7
figure 7
Evolution of fitness value in all proposed RF models

Full size image
Prediction of backbreak using PSO–RF
If each model’s hyper-parameters can be identified positively, seven optimal RF models can be established, and their performance can be evaluated. The statistical indices, MAE, RMSE, R2 and MAPE are adopted to evaluate the accuracy level of each constructed model. It is difficult to evaluate the accuracy of the model only by these index values. Therefore, to explain the optimal model, statistical index values of the training sets and testing sets were carried out and ranked synthetically. Tables 5 and 6, respectively, summarize the statistical index values of the predicted backbreak for training sets and testing sets in seven optimal models, and the ranking of each model is shown in Table 7. Marginal histograms of the predicted results of seven optimal models are shown in Fig. 8, which can visually see the contribution of the data on the X and Y axes.

Table 5 Comparison of the training set performance of RF-based hybrid models
Full size table
Table 6 Comparison of the test set performance of RF-based hybrid models
Full size table
Table 7 Comparison of performance of RF-based hybrid models
Full size table
Fig. 8
figure 8
Marginal histograms of the predicted results of seven optimal models

Full size image
Tables 5, 6, 7 and Fig. 8 show that the predicted backbreak for the training sets and testing sets are next to the P = M line. The prediction error of each optimal model is fairly small, demonstrating that seven optimal RF models developed by the PSO algorithm are promising in performance. It is worth noting that the model combining L–S–B–T–PF outperforms the remaining models; however, the model combining L–B–T–PF–SD provides the lowest performance. The remaining models also perform quite well, as shown in Fig. 8. In conclusion, the combinations integrate more variables and are more robust in prediction. Therefore, the combinations L–S–B–T–PF (PSO–RF1) and L–S–B–T–PF–SD (PSO–RF2) are recommended as the optimal models for predicting backbreak in engineering practice.

Discussion
Comparison with different combinations of models
Figure 9 presents the distributions of MAE values in each RF model, with lower mean MAE value (fitness value) appearing in outperformed RF models. According to the fitness values, the RF model combining L–S–B–T–SD performs best in CV sets, a little better than the RF model combining all parameters. Therefore, for the performance of CV sets, the RF model which contains all variables and the RF model combining L–S–B–T–SD are better than that of the remaining models, indicating that the backbreak prediction models combining more variables outperform those with fewer.

Fig. 9
figure 9
Distribution of MAE values in ten CV sets

Full size image
Comparison with different classical models
By comparing five classical regression algorithms, namely SVM, GP, RF, ANN and CNN [17, 21, 45], we can verify the advanced algorithm. Based on the conclusion that the combinations integrating more variables are more robust in predicting backbreak, all variables are adopted in these models. The scatter plots of the predicted backbreak using these models are shown in Fig. 10. It is not difficult to find that the prediction accuracy of all models is roughly the same, but the PSO-RF models outperform slightly from Figs. 8 and 10. Additionally, the accuracy of classical models is shown in Table 8, and the comprehensive prediction score is shown in Fig. 11 to visually compare the differences between models. As shown in Fig. 11, the PSO-RF model combining L–S–B–T–PF achieve the highest score, indicating the most outstanding performance (MAE = 0.0132, RMSE = 0.0811, R2 = 0.9990, MAPE = 0.0027 on the training dataset; MAE = 0.0568, RMSE = 0.1686, R2 = 0.9961, MAPE = 0.0116 on the test dataset) of it. What is worth noting is the PSO-RF model has better prediction accuracy than the remaining models. To sum up, the models proposed in this study perform commonly well in predicting and evaluating backbreak in open-pit mines, especially the proposed PSO-RF model. Therefore, the PSO-RF is recommended for predicting backbreak in engineering practice.

Fig. 10
figure 10
Marginal histograms of the predicted results of four classical models. a SVM; b GP; c RF; d ANN; e CNN

Full size image
Table 8 The accuracy of classical models
Full size table
Fig. 11
figure 11
Comprehensive sorted stacked graph for different models

Full size image
In previous research, Khandelwal and Monjezi [33] developed multivariate regression analysis (MVRA) and SVM models for forecasting backbreak with the same dataset. They employed R2 and MAE as evaluation indicators, which are R2 = 0.987, MAE = 0.29 and R2 = 0.89, MAE = 1.07 in the MVRA and SVM models, respectively. Compared with the results obtained by [33], the PSO–RF model shows more outstanding performance in predicting backbreak. Therefore, it is recommended to use the PSO–RF model to predict backbreak in practice.

Sensitive analysis
The importance of the input variables largely determines the accuracy of the model. In previous studies, the importance of input variables was not analyzed. Therefore, based on the proposed PSO–RF method, the importance of input variables is studied in this paper. As the introduction explains, the Gini index of each input variable is computed internally using a random forest model to investigate the importance. Breiman et al. [8] demonstrated variables that are more sensitive to the backbreak have a higher Gini index. The sensitivity analysis of each variable is shown in Fig. 12. It can be clearly seen that T is the most sensitive parameter to backbreak, followed by L, B, PF, S and SD. Their importance scores were 36.6, 31.2, 27.4, 23.4, 23.1 and 16.9, respectively. Some research results also show that T, L and B can greatly affect backbreak [14, 33, 51, 58]. In addition, Eskandar et al. [14] reduce backbreak impact by decreasing B and T, and Sari et al. [58] decrease backbreak distance by reducing L. In practical operation, through reasonably adjusting these sensitive parameters, and using PSO-RF model to test backbreak after adjusted blasting design, backbreak can be effectively reduced under the condition of meeting engineering requirements. Moreover, sensitivity analysis can provide a reference for selecting more important input parameters to establish a model in the future.

Fig. 12
figure 12
The relative importance of the influenced variables

Full size image
Conclusions
A novel hybrid artificial intelligence approach using RF with PSO algorithm is presented for estimating backbreak modeling in open-pit blasting. A data set of 234 samples with six inputs and one output was built to build a prediction model. The grey relational grade between backbreak and each influential factor is in the range of 0.74–0.83, indicating the selected variables are capable of predicting backbreak. Seven input combinations were set up to obtain the optimal prediction model. The PSO algorithm was integrated with the RF algorithm for finding the optimal hyper-parameters of each model, whose fitness function is the MAE of tenfold CV. After the prediction results of each optimal model were calculated, MAE, RMSE, R2 and MAPE were employed as the statistical indicators. Then, the comprehensive performance of each model was evaluated by the overall score. The results indicated that the combinations of more variables are more robust in prediction. Thus, the results of this study suggested that the combinations L–S–B–T–PF and L–S–B–T–PF–SD are the optimal models for predicting backbreak in engineering practice. The optimal models recommended in the result section were compared with the proposed classical models (SVM, GP, RF, ANN and CNN), and the results showed that PSO-RF model had good performance in predicting backbreak. Finally, the RF algorithm was used to calculate the Gini index of each input variable internally, which were 31.2 (L), 23.1 (S), 27.4 (B), 36.6 (T), 23.4 (PF) and 16.9 (SD), respectively. This method can evaluate the sensitivity of input variables. The variables employed in this study are commonly sensitive to backbreak, suggesting that the parameter selection is reasonable. What’s more, because the model is inseparable from the input parameters, the optimal models determined in this article are only recommended to predict backbreak under the same condition. Additionally, based on the advantages of the RF algorithm in predicting backbreak, it is recommended to use the RF algorithm to predict backbreak in other cases.

Keywords
Backbreak
Blasting
Random forest
PSO algorithm
Predictive model