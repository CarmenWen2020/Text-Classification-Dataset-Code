Traditional single-label classification methods can not be effectively applied in multi-label classification due to the semantic correlation. Conventional methods using the attention mechanism or prior knowledge, lacks deep semantic correlations, resulting in degradation for detection performance. Considering the hippocampal circuit and memory mechanism of human brain, a brain-inspired Memory Graph Convolutional Network (M-GCN) is proposed. M-GCN presents crucial short-term and long-term memory modules to interact attention and prior knowledge, learning complex semantic enhancement, and suppression. We evaluate the effectiveness of our method on public benchmarks (Microsoft COCO and PASCAL VOC). Extensive experiments demonstrate that M-GCN outperforms general state-of-the-art methods and shows the advantages in semantic correlation and complexity comparing with traditional memory models.

Introduction
A real-world image generally contains diverse semantic correlations between multiple objects. Predicting a series of objects in a single image is an essential problem in computer vision. In recent years, the establishment of large image datasets such as MS-COCO [23], Pascal VOC [10], ImageNet [4] greatly reduces the manual labeling effort, and certain progress has been made in image recognition.

Benefited from the rapid development of single-label classification methods on public datasets, the attention mechanism is introduced to determine whether each object exists in isolation. The performance for early multi-label recognition methods is greatly limited by ignoring the complex semantics between objects. So the semantics between multiple objects is mainly discussed, and traditional memory models, and graph models are used to discover the semantic correlation between multiple objects.

Semantics exert a significant impact on solving computer vision tasks. Methods for extracting and modeling semantics have been the subject of many studies. Wang et al. [20] considered the common memory model (RNN), using semantic redundancy and co-occurrence dependence to perform multi-label classification. Chen et al [1] further proposed LSTM [18] to model label dependencies and proposed a recursive framework to iteratively discover attention and semantic regions, modeling the long-term semantic relationship. However, RNN/LSTM models the region/label dependencies in a linear order, and the direct correlation between objects is not sufficiently utilized. Meanwhile, these methods are deficient in explicit modeling of label co-occurrence, which is considered critically for multi-label image recognition. Graph models for modeling label relationships are introduced, such as conditional random field [13], dependency network [15], co-occurrence matrix [35], etc. ML-GCN [3] and SSGRL [2] represented the label relationship by performing the statistics for the prior probability of label co-occurrence in the dataset. However, these previous models focus simply on semantic co-occurrence relationships, and the inhibition relationship of semantics in human memory is ignored. Moreover, the methods in [2, 3] calculated pre-statistical label co-occurrence to express the prior probability from some specific datasets but both [30, 31] suggested that most datasets have probability bias problems, inevitably leading to inadequate generalization. F-GCN [32] and MS-CMA [36] aggregate word vectors and image features to extract visual features. They implemented a rough cross-modal fusion with GCN, but the attention and prior knowledge are not balanced.

The attention mechanism of target location is more frequently discussed in traditional methods. Visual attention networks [33, 37] mainly consider areas of great attention. Some deep convolution networks contribute to construct a more perfect attention mechanism with a multi-layer structure [11, 28]. MCAR [11] designs different attention streams to discover the global representation and possible object areas respectively. Meanwhile, MSRN [28] generates label-specific and group embeddings that capture local and global semantics with multiple layers of a convolutional neural network, and then learn label and group shared semantic representations by attention mechanism. But problems are still posed for semantic generation by using attention mechanism independently. As shown in Fig. 1, we discuss the attention mechanism and prior knowledge via this “driving” image. In Fig. 1a, prior knowledge guides the conclusion that the image has driving semantics composed of “people” and “car”, when people and parts of the car body appear in the image. Conversely, “driving” and “dog” lack a prior knowledge, so the temporary objects are ignored. As shown in Fig. 1b, the attention mechanism efficiently recognized the semantic regions of people and dogs, and the object “car” however is missing due to occlusion and viewpoint. Therefore, the previous models generally have the problems derived from the above analysis that using independently prior knowledge or the attention mechanism leads inevitably to bias. The attention technique lacks the accuracy of detecting multiple objects, and the prior knowledge will miss the temporary objects.

Fig. 1
figure 1
The multi-label image recognition in the “driving” image. The black faded area indicates the non-conforming part, and the bright area indicates a certain part. a The semantic consistency of prior knowledge determines that the driving “person” is driving the “car”. b The attention mechanism demonstrates that “people” and “dogs” are detected effortlessly

Full size image
To address the above limitations of current methods, we discuss short-term and long-term synaptic plasticity in the hippocampus and medial temporal lobe based on the neurological theory of the semantic learning mechanism of the human brain. Depending on the synaptic circuit mechanism and memory mechanism in the hippocampus, the brain-inspired short-term and long-term memory mechanisms are considered a better solution to the problems. In this paper, we propose a multi-label image recognition model framework for semantic memory based on the bionic memory mechanism of brain-inspired neurology. The model combines the efficient semantic recognition mechanism of the CNN network and the direct semantic correlation constructed by the GNN, realizing the description of the short-term and long-term memory of the human brain. The model imitates brain mechanism for short-term and long-term memory in the nervous system, and complex semantic correlations between multiple objects are efficiently recognized. In addition, our memory module can self-learn to generate enhanced and inhibited semantic correlations, which is different from the previous specific prior annotations.

The main contributions of the proposed method are as follows:

(1)
A multi-label image recognition framework based on the hippocampal synaptic circuit mechanism and memory mechanism is established.

(2)
We propose a memory branch module containing short and long-term memory, which learn the complex semantic enhancement and inhibition correlations between multiple objects. The robustness of the model through relational memory mechanism is improved, solving the problem of imperfect semantic learning.

(3)
Unlike previous methods of prior statistics for specific datasets, we achieve self-learning deep semantic correlations for objects in the image.

(4)
We demonstrate the effectiveness and generalization ability of our method by quantitative and qualitative experiments on MS-COCO , VOC 2007, and VOC 2012 datasets.

Related work
Short and long term memory
Memory is a conscious recollection of events and objects. We create an intricate network, constituting a record of our personal experiences. The memory involving semantic knowledge is called declarative memory [6].

Declarative memory is mainly produced in the hippocampal region [6] and occurs at synapses of neurons [25]. Neurons can adjust the properties of synapses to reduce the difference between synaptic weights and values. The specific patterns of neural activity give rise to changes for insynaptic efficacy, and the persistent changes that events trigger, are called synaptic plasticity [25]. Synaptic plasticity can be divided into short-term plasticity and long-term plasticity according to the length of memory time.

Declarative memory does not permanently store all information. Short-term memory [9] lasts from a few seconds to a few hours. Main objects are kept by storing attention information temporarily, which helps the brain filter out redundant information. Long-term memory [19] is formed by repeated training on short-term memory and lasts several days or longer. Long-term memory is often an important event in experience, and thus considered as the basis of recall to make certain decisions for judgments, and semantic knowledge is constituted [6].

How is the declarative memory formed? From the perspective of biological mechanisms, researchers believe that memory stems from changes in synapses [25], representing in the whole brain, and thus synaptic modification becomes the research area of memory. The neurotransmitter receptor NMDA on the synapse plays a key role in the formation of declarative memory [14].

Fig. 2
figure 2
The gating effect of NMDA receptors [5]

Full size image
As shown in Fig. 2, the NMDA receptor on the synapse has an ion of Mg2+ as a gate switch [5] and Mg2+ blocks the channel into the cell. Coincident neuronal activity removes Mg2+ in the process of triggering declarative memory, opening a channel directly into the cell that easily causes excitement and enhances synaptic transmission. On the contrary, the gating effect of Mg2+ will also reduce the efficiency of synaptic transmission and achieve inhibition, in some environments with weak coincidence. Specifically, coincident neuronal activity refers to the simultaneous coincidence of input stimuli and output stimuli. For example, the input image is an input stimulus, and the real label of the image is an output stimulus. The coincident stimulus of the input and output will cause the gate unit to turn on, manifested in the formation of memory to provide the specific channel signal [5] for the same input next time.

The gating function of NMDA receptors in synaptic modification provides the material basis for memory formation. The auxiliary decision-making of memory and the construction of semantic framework is mainly reflected in the hippocampal structure.

Hippocampus and relational memory
NMDA receptors regulate the synaptic plasticity of the hippocampus and form memory which is utilized in the hippocampus for further decision-making [24]. The hippocampal region is considered the core area where declarative memory is generated, and essentially, it combines declarative memory and attention to make decisions [6]. The input information is often visual attention objects generated by the visual nerve. Information reaches the output neuron through two pathways after entering the hippocampus: the direct pathway and indirect pathway. The input information in the direct path is directly connected to the output neuron. In the indirect pathway, the information is projected through the collateral pathway and finally returns to the dendritic part of the output neuron, forming a circular auxiliary circuit to feedback the direct pathway. Most of the experiments on long-term memory are performed in the pyramidal neuron of sector CA1 on the indirect pathway [38]. Long-term memory is updated and the semantic knowledge is formed on the indirect pathway, using the direct pathway’s information, and then fed back to the direct pathway for decision.

Fig. 3
figure 3
The direct pathway and indirect pathway [21]

Full size image
We have discussed the formation of memory and how memory participates in decision-making. The memory function in the hippocampus does not just store the memory for a single object. Neal Cohen and Howard Eichenbaum further believed that the hippocampus is also involved in relational memory [8]. The information entering the hippocampus is stored with relational events or objections. The sequence of events or objections are encoded by the hippocampus and linked together through shared elements [6]. This kind of memory related to multiple objects is relational memory.

Relational memory has emerged from several models of declarative memory [7, 26, 27], suggesting that individual episodic experiences do not exist in isolation but rather in extended networks linked by common features. The advantages of relational memory are adaptability and the capability to use memory for reasoning in facing new situations. Firstly, the common features in different memories will be extracted to form “timeless” semantic elements which are not bound to any particular events. Then, elements of common characteristics are coded (represented by C in Fig. 4), linking memories with each other, so that people can compare the memories and make inferences between indirectly related events. The hippocampus automatically encodes the new memory based on the old memory. Various memory traces are accumulated, and the cerebral cortex forms a generalized memory from these traces, which is the semantic knowledge framework.

Fig. 4
figure 4
A simple schematic diagram of relational memory [7]. The current memory is composed of two episodic memories (A and B), each composed of a series of elements (1-5) that represent the occurrence of events. C is an element that contains the same characteristics, and D represents some other unrelated elements

Full size image
In the task of multi-label recognition, relational memory can effectively process the associated semantics of multiple objects. Long-term memory based on indirect pathways realizes a semantic knowledge framework between multiple objects and assists direct pathways in accurate decision-making.

Memory GCN
Memory in brain and multi-label image recognition
The hippocampus receives visual information and multimodal sensations to produce memories and the human brain encodes multiple object relationships through relational memory. The multi-label image recognition task is efficiently processed in the hippocampal region through two excitatory pathways (memory and visual attention).

Memory is subdivided into short-term memory and long-term memory in the image recognition task. Short-term memory filters redundant information and affects the human brain’s ability to extract objects in instantaneous characteristics. The transformation from short-term memory to long-term memory is formed by repeating training. Long-term memory encodes the common parts of the event sequence, connecting objects. Multi-label images are recognized by the interactive mode of short-term and long-term memory. Therefore, we expect to imitate the output structure of the hippocampus and model the semantic knowledge from multi-label objects.

A brain-inspired framework is established based on the mechanism of relational memory and multi-label recognition. We propose a brain-inspired memory branch for multi-label recognition, imitating the neural mechanism of short- and long-term memory.

Overview of the memory GCN
We simulate the synaptic circuit in the hippocampus using the two pathway mode to build our network model. The final recognition results are jointly decided by memory and attention. The network architecture for the dual pathway model is illustrated in Fig. 5. The M-GCN model we proposed is an end-to-end model, including a direct pathway and an indirect pathway. Given input image, a fully convolutional network (we choose the ResNet-101 [17] network as backbone) is used to compute its feature map. In the direct pathway, the attention mechanism decouples semantic regions of each class, and the attention scores for each class are obtained by these regions. Meanwhile, the semantic regions are used by the memory module in the indirect pathway. There are two different memory modules in the indirect pathway: short-term memory module and long-term memory module. The short-term memory module uses semantic regions to generate short-term memory. The long-term memory module is a GCN (Graph Convolution Network) model, and long-term memory is formed by the supervision of the semantic regions and short-term memory. Long-term memory will interactive with short-term memory to calculate memory scores. Attention score and memory score jointly achieve the ultimate decision. We describe each component of our method in the rest of this section.

Fig. 5
figure 5
Network architecture for M-GCN model. The image through a fully convolutional network (backbone) and is sent into two pathways: the direct and indirect pathway. The direct pathway is mainly the attention mechanism, and the indirect pathway is mainly the complex semantic memory module containing a gate unit

Full size image
Perception module simulating short-term memory
Short-term memory is the first information stored rationally in human vision, which helps the brain filter redundant information. Short-term memory temporarily stores attention information. As shown in the memory process of Fig. 5, we first simulate the visual attention mechanism and short-term memory to filter and screen a given image. Attention mechanism can extract a set of content-aware classes from the feature map 𝑋∈𝑅𝐻×𝑊×𝐷, where H, W, and D respectively represent the height, width and the channel of the feature map after a fully convolutional network. The feature map X, extracted from the given image E through the fully convolutional network, is defined as,

𝑋=𝑓𝑐𝑛𝑛(𝐸)
(1)
where 𝑓𝑐𝑛𝑛(⋅) denotes a fully convolutional network. The learned mask 𝑀∈𝑅𝐻×𝑊×𝐶 for filtering information is computed by the feature map X through Muti-Layer Perception (MLP).

𝑀=𝑀𝐿𝑃(𝑋)
(2)
Meanwhile, we resize X to 𝑋′∈𝑅𝑁×𝐷 (𝑁=𝐻×𝑊) and resize M to 𝑀′∈𝑅𝑁×𝐶 =[𝑚1,𝑚2,⋯,𝑚𝑐]. The attention feature 𝐼∈𝑅𝐶×𝐷 for c classes is formed by element-wise multiplication of the masks 𝑀′ and the features 𝑋′ to obtain each class of visual attention:

𝐼=[𝑖1,𝑖2,⋯,𝑖𝑐]𝑖𝑛=𝑚𝑇𝑛𝑋′
(3)
The short-term memory module is designed to learn the preliminary relational semantics after the effective detection of the attention mechanism. With the attention features I, the short-term memory module applies average pooling to estimate a global semantic 𝐺∈𝑅𝐶𝐷, and short-term memory S is further formed by concatenation and convolution of attention features I and global semantics G.

𝐺=𝐿𝑅𝑒𝐿𝑈(𝑓𝑎𝑣𝑒𝑝𝑜𝑜𝑙(𝐼))𝑆=𝑓𝑐𝑜𝑛𝑣(𝑐𝑎𝑡(𝐼,𝐺))
(4)
Short-term memory S tends to focus on simple global semantic relations and attention features I further focus on individual explicit differences. The output of the short-term memory module is utilized as the input of the long-term memory module. The attention features I and short-term memory S are sent to the gate unit of the long-term memory to generate the long-term memory. The gate unit based on the NMDA receptor in Fig. 3, is used to change the excitation threshold of different labels.

Fig. 6
figure 6
The Short-term memory module

Full size image
Perception module simulating long-term memory
Long-term memory is a persistent memory formed after repeated training on the short-term memory, corresponding to the lasting change of synaptic weight. Long-term memory stimulates relational memory in neurons due to the gating action of NMDA receptors providing specific signals that enhance synaptic efficacy.

To facilitate modeling the relationship of labels in the image, long-term memory shares semantic relationships by graph convolution, using the vertices of the graph 𝑉=[𝑣1,𝑣2,𝑣3,⋯,𝑣𝑐] to represent c classes of labels. The adjacency matrix A and the weight matrix W of the graph jointly reflect the relational memory between the labels.

Long-term memory depends on the gating and switching role of NMDA receptors, so in M-GCN a NMDA-like gating unit Gate is proposed. The output of the short-term memory module (attention features I and short-term memory S) adjusts the excitation threshold in the Gate unit, thus supervising the generation of long-term memory.

The excitation threshold is a specific memory mechanism present in neurons. Effective stimulation induces excitation by altering the gating switch of NMDA receptors on the cell membrane. Inspired by the gate effect of NMDA, we consider that for the input 𝑥∈𝑅𝑐, there is an excitation threshold 𝜏, and the excitation caused by 𝑝∈𝑅𝑐. The input 𝑥=(𝐼+𝑆) represents the input of the Gate unit in the long-term memory module. When a specific type of stimulus is higher than the threshold, each 𝑥𝑖 has a corresponding 𝑝𝑖:

𝑥=[𝑥1,𝑥2,⋯,𝑥𝑐]𝑝𝑖={0,𝑥𝑖<𝜏1,𝑥𝑖<𝜏
(5)
Here x generally cannot cause excitation for the specific class 𝐶1 if 𝑥1<𝜏 but 𝑥2>𝜏. However, 𝐶1 is highly susceptible to excitation when perceived 𝐶2, assuming the existence of relational memory for specific classes 𝐶1 and 𝐶2. The human brain achieves gating of excitation thresholds primarily by detecting synchronous pre- and postsynaptic excitation through NMDA receptors on the cell membrane. We learn a gating detection parameter v to detect the relationship between input and current long-term memory by simulating NMDA receptors. The excitation threshold 𝜏 is altered by the influence of long-term memory. Since predicting the excitation threshold 𝜏 for each class poses a difficulty because of the complexity of brain mechanism, we change x to achieve the same effect of changing the excitation threshold 𝜏.

We denote 𝐴𝑡−1 and 𝑊𝑡−1 as the adjacency and weight of long-term memory at the 𝑡−1 previous moment, and 𝑖𝑡,𝑠𝑡 are the attention and short-term memory of the new input at t time. 𝑓𝑎,𝑓𝑤 generate new adjacency and weight with non-linear mapping.

𝐴𝑡−1=𝑓𝑎(𝑖𝑡)𝑊𝑡−1=𝑓𝑤(𝑠𝑡)
(6)
𝑣𝑎=tanh(𝑊𝑎⋅[𝐴𝑡−1,𝑖𝑡])𝑣𝑤=tanh(𝑊𝑤⋅[𝑊𝑡−1,𝑠𝑡])
(7)
Where the output gating detection parameters 𝑣𝑎,𝑣𝑤 in the Gate unit represent whether 𝑖𝑡,𝑠𝑡 are active or inhibited in the current gating state, leading to the generation of reliable semantic relations.

𝑣∈[−1,1] is the gating detection parameter, whose magnitude represents the strength of the contact memory. To summarize, M-GCN chooses to trust memory more than attention under extreme conditions (lim|𝑣|→1), so that a negative v indicates exerting an inhibitory effect to represent an ambivalent relationship, while a positive v exerts a reinforcing effect to represent a correlation. Moreover, the gating detection parameter v also supports the self-learning for deep semantic correlations. The memory is not completely static, the positive and negative represents the correlation, and the value represents the difficulty of change. The relational memory may change after each acceptance of new content.

Fig. 7
figure 7
The Long-term memory module

Full size image
The true stimulus degree after the influence of relational memory is formulated by the new representation z.

𝑧𝑖=𝑖𝑡+𝑣𝑎∗𝐴𝑡−1𝑧𝑠=𝑠𝑡+𝑣𝑤∗𝑊𝑡−1
(8)
In the example, 𝑥1<𝜏 cannot independently trigger excitation, but the presence of relational memory 𝑥2>𝜏 for 𝐶1,𝐶2 can reason the relation for 𝑧1>𝜏, allowing the input 𝑥1 to be optimized to a value consistent with the memory stimulus. The Gate gating module is designed to modulate the excitation threshold of the input x to the memory representation z, and subsequently, supervise the generation of a new adjacency 𝐴𝑡 and weights 𝑊𝑡 in long-term memory.

𝐴𝑡=𝑓𝑎(𝑧𝑡𝑖)𝑊𝑡=𝑓𝑤(𝑧𝑡𝑠)
(9)
The long-term memory module shares the learned relational memory in direct pathway targets for semantic reasoning, assisting in calculating the memory score.

𝑆𝑚𝑒𝑚𝑜𝑟𝑦=𝑆𝑖𝑔𝑚𝑜𝑖𝑑(𝐴𝑡⋅(𝑖𝑡+𝑠𝑡)⋅𝑊𝑡)
(10)
Simulate the interaction for direct and indirect pathway
The hippocampus processes input information through a dual-pathway structure, enabling effective interaction between long-term memory and attentional information for decision making. Long-term memory contains semantically consistent associations and corrections, while attention captures objects that appear temporarily, both of which is believed significant for multi-target detection.

𝑆𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛=𝑓𝑐(𝐼)
(11)
where the fc(.) denotes the fully connected layers.

Traditional models focused only on one-sided decisions, making the final recognition effect lack of robustness. Therefore, the mutual decision-making between the direct path and the indirect path in our model is performed for final result. The decision score 𝑆∈𝑅𝑐=[𝑠1,𝑠2,𝑠3,⋯,𝑠𝑐] is achieved from weighted summation for the memory score 𝑆𝑚𝑒𝑚𝑜𝑟𝑦 of the indirect pathway and the attention score 𝑆𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 of the direct pathway, which is similar to the dual pathway structure in the human brain. We use the hyper-parameter 𝜆 to control the decision weights between attention and memory.

𝑆=𝜆⋅𝑆𝑚𝑒𝑚𝑜𝑟𝑦+(1−𝜆)⋅𝑆𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛.
(12)
The algorithm of the proposed model is shown below:

figure a
The multi-label classification loss [3] is used to supervise our Memory-GCN. Where 𝜎(⋅) is a sigmoid function.The number of all classes is 𝑛𝑐, and 𝑦𝑐,𝑠𝑐 denote the truth label and predicted score for class c.

𝐿(𝑦,𝑠)=∑𝑐=1𝑛𝑐𝑦𝑐⋅𝑙𝑜𝑔(𝜎(𝑠𝑐))+(1−𝑦𝑐)⋅𝑙𝑜𝑔(1−𝜎(𝑠𝑐))
(13)
Experiments
Datasets and experiment setup
Datasets and evaluation metrics: To evaluate our method, we use MS-COCO [23], VOC 2007 and VOC2012 [10] datasets on multi-label image recognition tasks. Microsoft COCO [23] initially served for object detection and recently it has been widely used for multi-label recognition. MS-COCO [23] consists of 82,081 training images, and 40,137 images for validation. A total of 80 object classes are contained, with an average of 2.9 objects per image. We draw on previous works [2, 3] to evaluate our model on the validation set, due to the lack of test labels in the dataset. Pascal VOC 2007 [10] is widely used for multi-label image recognition tasks, and it contains 20 classes of objects and 9963 images. VOC2007 [10] was divided into the common training set, validation set, and test set. VOC 2012 [10] consists of 11,540 images as trainval set and 10,991 as test set. However, no ground truth labels are provided in the test set. The result of VOC2012 testing set is submitted to Evaluation Server for evaluation. For a fair comparison, we draw on previous work [2, 3] and use the validation set and the test set together for testing. We used evaluation metrics from previous work [22, 37]: overall/per-class precision (OP/CP), overall/per-class recall (OR/CR), overall/per-class F1-score (OF1/CF1), and the mean Average Precision (mAP). The OF1/CF1, mAP metrics are more meaningful than other metrics generally.

Implementation details: Table 1 shows the hardware and software configuration and all experiments use the same hardware and software configuration.

Table 1 Software and hardware environment for experiments
Full size table
To guarantee the fairness of the experiments in this paper, all the experiments in this paper use the following parameter configurations. For data augmentation, the same methods in [3] are selected to random horizontal flip and crop images, and then resize them to the size of 448×448. For the backbone CNN, we use Resnet101 pre-trained on ImageNet [17] with output dimensionality of 1024. The global average pooling for short-term memory extraction contains the nonlinear activation function LeakyReLU with the negative slope of 0.2 [3]. In the evaluation of the VOC dataset, fast convergence was performed with the COCO pre-trained model [2]. We trained all the models with SGD optimiser using a weight decay of 10-4 and momentums of 0.9, with a batch size of 16 [3]. On all datasets, the learning rate is initialized as 0.5 for memory module and 0.05 for backbone, divided by 10 after 15 epochs and 25 epochs [3]. We stop training in 40 epochs. We implement our methods with Pytorch.

Ablation study
In order to analyze the influence of different components in the framework, the model was investigated on COCO 2014 [23] for three aspects: the short-term memory module, the long-term memory module, and the effect performed by different hyperparameters 𝜆.

A.
Evaluation of the hyperparameter 𝜆:

The hyper-parameter 𝜆 controls the decision weight between attention and memory. We evaluate the parameter 𝜆 with the mean average precision (mAP) in Fig. 8. We started the experiment from 𝜆=0, which obtains the mAP similar to baseline. Adding a memory module and increasing the hyper-parameter 𝜆 significantly improves the accuracy of multi-label recognition. The best performance is achieved when 𝜆=0.5. However, the limited semantic consistency causes a decrease in accuracy when we rely too much on memory to make decisions. Finally, the mAP score decreases approximate to the baseline when 𝜆=1. Therefore, we select 0.5 as the best value of 𝜆.

Fig. 8
figure 8
Accuracy comparisons with different values of 𝜆

Full size image
B.
Evaluation of the memory module:

Fig. 9
figure 9
Evaluation of Memory module on MS-COCO

Full size image

Baseline: We use the Resnet101 pre-trained on ImageNet [17] as our baseline and evaluate it on the COCO dataset.

No Long memory (similarity to 𝜆= 0 ): After the combination of short-term memory and attention, we used the linear layer to replace the long-term memory module to assess whether long-term memory worked. Results show that mAP for removing the long-term memory module are similar to baseline. Meanwhile, the accuracy is decreased slightly for short-term memory because of the shorter time training and the lack of pre-training.


No Short memory (similarity to 𝜆 = 1 ): Both attention and short-term memory are used to generate semantic long-term memory in the proposed model. We utilize attention as the only input to study the role of short-term memory, which implies that both our adjacency and weighting relations are directly generated by attentional supervision. Results show that we still improve baseline from 79.7% to 83.6% and have significant improvements in both OF1 and CF1. It is suggested that the long-term memory plays a critical role in the absence of short-term memory.


Short + Long memory (𝜆=0.5): The combined short and long-term memory achieved the best results, with an increase of 5.6% on mAP. The OF1 and CF1 outperform the baseline by 3.7% and 5.9%, respectively. All the results are visually illustrated in Fig. 9.

Comparison with State-of-the-arts
In this section, we present comparisons with state-of-the-art methods on MS-COCO [23], VOC 2007, and VOC 2012 [10] respectively, showing the effectiveness and generalization of our model.


Results on MS-COCO: The results of MS-COCO are presented in Table 2. In particular, We focus on deep convolutional networks with traditional memory models CNN-RNN [20], Atten-Reinforce [1], RNN-Attention [33], Multi-Evidence [12], ResNet-SRN [37], ResNet-101 [17] and the latest graph neural network models ML-GCN [3], SSGRL [2] , F-GCN [32] and MS-CMA [36]. Additionally, MCAR [11] and MSRN [28] perform multi-Label recognition using the attention mechanism independently. The parameter configuration of our proposed method is the same as the experiment in Sect.  4.1, and the experimental configuration of the comparison methods is completely consistent with the original paper. According to previous work [2, 3], the test image size used is 576*576 in this comparative experiment.

Table 2 Comparison of our M-GCN and other state-of-the-art methods on MSCOCO
Full size table
Table 2 presents the comparison results on MS-COCO. Results show proposed model has a clear advantage compared to the traditional memory models RNN/LSTM. M-GCN is leading in the mAP with 85.3% and overall-class metrics, but slightly lower than MSRN in the TOP-3 metrics. This situation shows that M-GCN has a better overall detection effect on all classes through the two pathway mechanism, while the attention-focused MSRN has a better performance in detecting key classes. Our brain-inspired model is more effective than the latest papers [11, 28] which focus on the attention mechanism. It shows more meaningful to add a memory pathway instead of mining attention information alone. ML-GCN [3] and SSGRL [2] determine the adjacency relationship through artificial data calibration. M-GCN still surpassed ML-GCN [3] by 2.3% on mAP and 1.5% on SSGRL [2], indicating that our self-learning memory module can learn more complex and complete semantic correlations. F-GCN [32] and MS-CMA [36] utilize the label word embedding vectors to generate a label graph, aggregating word vectors, and image features to extract visual features. The lack of balance between attention and prior knowledge even makes them weaker than the perfect attention mechanism [11]. The SSGRL [2] model is higher than M-GCN on CP, mainly due to its design of a deeper semantic decoupling network, which obtain high accuracy in the initial framing target stage compared to common attention mechanisms. M-GCN outperforms the other models on the CR metric, and a better CR reflects that most of the samples can be correctly detected by the memory module, indicating the effectiveness of the M-GCN memory module. Results of the SOTA memory models perform similarly to the ResNet-101 pre-trained by ImageNet [17] but our memory model can surpass the latest results.


Results on VOC2007: A comparison of our model with other methods on the VOC2007 is shown in Table 3. M-GCN achieves the highest accuracy rate on 75% of detection classes and 96.1% on mAP, which improves 1.1% on the latest SOTA SSGRL [2] on mAP. Furthermore, we also achieve accurate detection for small objects, such as “bottle”, and accuracy increases by 3.4% comparing with previous SOTA. These objects are generally difficult to detect, but their strong correlation with other objects makes our memory model work well. In addition, the traditional memory model has even lower detection accuracy for many classes than the original CNN due to the lack of an effective combination of both memory and attention. Considering the long and short memory characteristics, our model achieves the accuracy improvement for almost all classes compared with baseline.

Results on VOC2012: A comparison of our model with other methods on the VOC2012 is shown in Table 4. The classes of VOC 2012 are same as VOC 2007 and the overall results show the same trend. Our M-GCN achieves 94.9% on mAP. However, SSGRL and MCAR outperforms well in the detection of animals. For these slightly independent detection classes, they focuses on the attention mechanism to make animals easier to detect. M-GCN achieves the highest accuracy rate on most detection classes. Our proposed M-GCN is still very powerful in detecting people, furniture and vehicles, benefiting from their correlation.

Table 3 Comparison of our M-GCN and other state-of-the-art methods on Pascal VOC 2007 dataset
Full size table
Table 4 Comparison of our M-GCN and other state-of-the-art methods on Pascal VOC 2012 dataset
Full size table
Visualization
In this section, we visualize some examples of multi-label recognition to illustrate the interaction of memory and attention mechanisms, and the semantic relationships learned through relational memory. Visualizing the recognition performance on voc2007 measures the role of the memory module is intuitively. Specifically, we use the Resnet101 [17] with the attention mechanism as the baseline. The visualization example is shown in Fig. 10 a, b. The recognition performance of memory module network on voc2007 is discussed through visualization.

Fig. 10
figure 10
a shows the performance of input images under semantic enhancement. Green is the correct recognition with common missed labels, and red is the wrong recognition. b shows the performance of input images under semantic suppression. Red is the wrong recognition with redundant labels and black is the correct recognition

Full size image
The memory module has a advantage over the attention mechanism in the multi-label image recognition task. As shown in Fig. 8a, baseline misses specific targets due to occlusion or incomplete display. However, the semantic enhancement of relational memory is performed in M-GCN, and the correct recognition of missed targets such as “bicycle”, “person” and “chair” was achieved, demonstrating the advantages of the relational memory module in semantic enhancement. As shown in Fig. 8b, baseline detects redundant objects under complex background. For the proposed M-GCN, the contradictory suppression of relational memory is performed and the misdetected “train” and “airplane” are eliminated, indicating that the memory module can further learn the complex semantic relations to semantic suppression. The recognition results of our method better match the image correct labels. Furthermore, it is found that the memory module can complement the recognition of objects that are difficult to detect directly owing to viewpoint and lighting problems. M-GCN is also allowed to suppress decisions for recognized objects that have large contradictory semantics, thus achieving the optimization of semantic consistency of recognition results.

Conclusion
We use brain-inspired methods to solve the problem of complex semantic learning in visual tasks and our model is more interpretable based on neuroscience theory. The proposed Memory-GCN network achieves short-term and long-term memory interaction, self-learning enhances and inhibit semantic correlations among multiple labels. Furthermore, M-GCN is a generalized model, which is suitable for solving many computer vision problems that depend on attention mechanism and prior knowledge. Extensive public benchmark experiments (MS-COCO, Pascal VOC 2007&VOC2012) demonstrate that M-GCN outperforms general state-of-the-art methods, and achieves the mAP of 85.3%, 96.1% and 94.9% respectively. It is verified that our method success fully explore complex semantic relations, which is superior to the existing methods for multi-label image recognition.

Keywords
Multi-label classification
Brain-inspired
Graph Conventional Network
Memory network