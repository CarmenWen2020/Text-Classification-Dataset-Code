Recently, the discriminative correlation filters (DCF)-based methods have performed excellent precision and speed in object tracking. Due to the continuous change and expansion of the search region, the problem of insufficient training samples is solved by the periodicity hypothesis, which inevitably introduces boundary effects that can lead to severe failures in the detection stage. In this paper, we firstly add a background penalty factor into the correlation filter and propose a novel spatial regularization term by using the saliency detection method. Based on the above two points, a background-aware correlation filter model with saliency-aware regularization is established. Secondly, in order to solve the model better and faster, we introduce an energy function for the solution of the spatial weight and apply the alternating direction method of multipliers (ADMM) method and deduce the closed-form solution of each subproblem of the objective function efficiently. Thirdly, we propose an adaptive updating mechanism based on the variation of target appearance and the reliability of tracking results, which can update the model online by adjusting the spatial weight distribution for precisely tracking in the spatio-temporal domain. Finally, we apply two BAASR models to estimate the position and the scale of the target, respectively. One model adopts hand-crafted features at multiple scales to select the optimal scale, while the other model predicts the optimal position by fusing hand-crafted features with deep features extracted from the trained network models. Extensive experiments are carried out on the following five datasets, OTB-2013, OTB-2015, UAV123, UAV20L, and TC128. Experimental results demonstrate that our tracker has superior robustness and performance.

Introduction
Background
Video object tracking is one of the hottest topics in the field of computer vision. By estimating the location, shape, or region of the object in continuous image sequences and determining the motion information of the object such as the speed, direction, and trajectory, we can analyze and understand the behavior of the moving object to fulfil more advanced and more complex tasks. Object tracking has great significance and broad application prospect in both civil safety and military defense, such as intelligent video surveillance, video-based humanâ€“computer interaction, intelligent transportation systems, medical diagnostics, visual navigation, missile guidance, drone surveillance, and flight control. Although breakthroughs have been made in this field after decades of research, there are still several challenges from two aspects. On the one hand is the complex external environment, such as occlusion, background clutter, similar objects, illumination variation. The other comes from the target itself, such as scale variation, deformation, and rotation.

From the perspective of modeling, object tracking algorithms can typically be divided into generative methods [1,2,3] and discriminative methods [4,5,6,7,8]. Generative methods start by extracting features from the target and constructing a model, followed by finding a region similar to the model in the next frame and determining it as the prediction region of the target. Discriminative methods regard the tracking task as a binary problem and focus on how to distinguish target from background. Comparing these two methods, the second one can better cope with complex challenges such as background variation. In particular, tracking algorithms based on the discriminative correlation filters achieve matrix diagonalization by cyclic shift and apply fast Fourier transform (FFT) to transform the filtering process from spatial domain to frequency domain, which simplifies the process and improves the speed of calculation. There have been a number of improvements and technical breakthroughs to the discriminative methods. Recently, researchers have continuously improved algorithms from multi-dimensional feature representation [9, 10], scale estimation [11, 12], adaptive response map [13], adaptive update [14], spatio-temporal regularization [15,16,17,18,19], multi-kernel models [20, 21], localâ€“global appearance model [22, 23] etc., making trackers more adaptable to the complex and variable environment [24]. With the rapid advancement of deep learning, many researchers have combined deep learning with discriminative methods for further exploration. Some pre-trained neural network models have been applied in several domains such as object recognition, object detection, classification and have achieved excellent performance in some works [25,26,27].

In recent years, some benchmarks and evaluation metrics have been proposed to accommodate the research development of object tracking. Here we list some dedicated tracking platforms, OTB-2013 [28], LaSOT [29], UAV123 [30], Temple-Color 128 [31], which are large datasets with higher quality and more challenges. Therefore, these datasets not only increase the difficulty of object tracking, but also place higher demands on the performance of the trackers.

Motivation
As the expression ability of gray features is not enough to deal with the complex background or objects close to the background, Henriques et al. [32] extended one-channel features to the multi-channel directional histogram of gradient features. The paper [10] expands the grayscale to multi-channel CN features and includes an adaptive dimension reduction mechanism to boost the tracking robustness and reduce the computational cost. HOG features and color features show clear advantages in describing objects, so our tracker chooses to introduce HOG features for experiments. With the extensive popularization and application of deep learning, many DCF-based trackers incorporating deep convolutional features have been developed. The structure of the correlation filters can effectively improve the speed of the filter training and detection. At the same time, the deep features are learned from a large number of training samples that have strong expression ability of the target information and contain rich semantic information. In our algorithm, we combine these two types of features to create two BAASR models. The model 1 is built on the hand-crafted features at five scales to estimate the target scale. And the model 2 fuses the hand-crafted features and deep features at one scale to achieve the estimation of object location.

The advantages of DCF methods are circular shift and fast calculation, but it is precisely because of the use of circular shift that the boundary effect appears. Generally, the way of adding padding boxes is applied to reduce the influence of boundary effects. Spatially regularized discriminative correlation filters (SRDCF) [15] adopt a larger detection region, set padding to 4, and add a spatial weight to punish the boundary region. In practice, the weight map used in SRDCF is a fixed value and does not change over time after the initialization of the first frame. In real tracking scenarios, the object appearance is usually asymmetric and irregular, and it changes frequently. Background-aware correlation filters (BACF) [33] add a padding box of the same size as the image and then adds a binary matrix for background clipping to the ridge regression term to extract the target region. In this paper, we introduce background-aware and saliency-aware into the algorithm to effectively alleviate the errors caused by boundary effects and target appearance changes. The filter is refreshed in real time by describing the saliency information such as the size and shape, which can more accurately to grasp the target changes and ensure the robustness of the tracker. At the same time, we adopt the ADMM method to obtain the solution to the objective function, which not only optimizes the solution steps and quickly solves the filter, but also greatly reduces the computational cost of the algorithm and improves the overall performance. In Sect. 5.2.2 of the ablation experiment, we can clearly understand the effect of each contribution on the performance of the tracker.

Contributions
In this paper, a ridge regression term with background penalty and an adaptive spatial regularization term are proposed to obtain precision and robust tracking results. Our main contributions are as below:

(1)
We propose a novel background-aware correlation filter with adaptive saliency-aware regularization (BAASR) for visual tracking. Our tracker combines the real background information and the adaptive spatial regularization into the traditional DCF formula. The shape, scale and change information of the target is integrated into a new spatial weight through saliency detection method.

(2)
To cope with the variation of object appearance over time, we propose a new energy function to obtain the optimal solution of the spatial weight, and apply the alternating direction method of multipliers (ADMM) for joint optimization to solve the subproblem.

(3)
An adaptive updating mechanism is proposed, which combines the saliency map with the response map of the tracking object. By judging the reliability of the tracking results, a new spatial weight which changes adaptively over time is obtained to ensure the robustness of the appearance model.

(4)
We present two BAASR models to estimate the location and scale, respectively. Only HOG features at five scales are applied to ascertain the object scale. Deep feature norm1 and conv4-3, which are extracted from two pre-trained network structure models VGG-m and VGG-16. We apply the fusion features of these deep features and hand-crafted features to determine the object position.

(5)
We have conducted a numerous performance evaluation of our tracker and other state-of-the-art trackers on OTB-2013, OTB-2015, UAV123, UAV20L, and TC128 datasets, and our algorithm exhibits excellent tracking robustness. The precision and success rate of OTB-2013 are 94.0% and 71.6%, respectively. On OTB-2015, the precision of our tracker reaches 91.8%, and the success rate reaches 69.0%. Besides, our tracking algorithm has also achieved excellent performance on TC128 and UAV datasets.

The remainder of the paper is organized as follows. In Sect. 2, we revisit the work related to object tracking. Section 3 lists some classic tracking models. Our proposed BAASR model and optimal solutions are elaborated on Sect. 4. The experimental results and analysis are discussed in Sect. 5. Finally, we conclude our paper in Sect. 6.

Related work
Correlation filter-based tracking
In 2010, Bolme et al. firstly proposed the Minimum Output Sum of Square Error (MOSSE) [34] filters for object tracking. The MOSSE adopts an adaptive training strategy to extract grayscale features and estimates the target center with the highest response map score. General speaking, the more training samples a correlation filter uses, the better the tracking performance will be, but if too many or too large training samples, the problem of computational overloading may occur. The CSK model [32] takes advantage of the short moving distance between frames of targets and adopts cyclic sampling technique to obtain training samples, which not only gets more samples to train the filter, but also dramatically reduces the computational cost. Subsequently, Henriques et al. introduce the Kernel Correlation Filter (KCF) [9], which maps the ridge regression of linear space to the high-dimensional nonlinear feature space through the kernel function and extends the single-channel grayscale features to the histogram of oriented gradient (HOG) features with 31 channels. Danellian et al. proposed the discriminative scale space tracking (DSST) [11] models on the scale change problem in the tracking. In this model, the feature scaled pyramid method is applied, which greatly improves the tracking effect of the model in some complex scenarios. On the basis of the previous research, the scale adaptive multiple features (SAMF) [12] algorithm was proposed. The algorithm takes the scale variation into account and adopts a variety of features, which improves the tracking effect to a certain extent.

Convolutional features-based tracking
Ma et al. combined the correlation filtering algorithm with the deep learning model and proposed a hierarchical convolutional features tracking (HCFT) [35]. The study found that the shallow features extracted from the deep network contain spatial location information, while the deep features contain more semantic information. Danelljan et al. introduce deep features on the basis of SRDCF to form DeepSRDCF (Deep Features for SRDCF). It proposes that the expression of first layer convolutional features is better than the deep layer features, and the shallow layer feature extraction is faster, which is conducive to the speed improvement of the whole algorithm. In [36], the continuous convolutional operators tracking (C-COT) was proposed. This method uses VGG-net to extract features, uses implicit interpolation to interpolate feature maps of different resolutions into the continuous spatial domain, and applies the Hessian matrix to obtain the target position with sub-pixel accuracy. Danellian et al. propose an Efficient Convolution Operators (ECO) [37] to improve the tracking speed of the C-COT algorithm. ECO is a fusion of traditional hand-crafted features and convolutional features, removing redundant operations from feature dimensions, samples, and template updates. Cui et al. propose the Recurrently Target-Attending Tracking (RTT) [38] algorithm, which uses the RNN networks to extract the saliency map. The saliency map has a higher value in the target part, and it is added to the correlation filter coefficient to enhance the anti-interference ability and make filter better focus on the parts that are useful to the tracking results.

End-to-end learning for tracking
In the studies currently, visual tracking algorithm on the basis of Siamese networks has attracted the attention on researchers due to its high performance. Siamese networks connect two input branches to generate one output and measure the similarity between two input image blocks through a shared convolutional layer to find out whether there are the same objects in the input image. The fully convolutional Siamese network (SiamFC) proposed in [39] performs the tracking task by evaluating the similarity between the object region and the candidate region. Li et al. propose a Siamese region proposal network (SiamRPN) [40] and introduced the RPN structure into the tracking framework. Two branch task models are designed in the RPN structure: one for classifying the target and background and the other for regressing the regions of the candidate box. Subsequently, Li et al. [41] modified the shortcomings of the SiamRPN model and formulated the SiamRPN++ model. Based on the original model, the impacts of spatial invariance on deep neural networks were analyzed, and a new sampling strategy was proposed to overcome these drawbacks. Later Guo et al. proposed a SiamCAR [42], consisting of two sub-networks for classification and regression at the pixel level, which is both proposal and anchor free. Siamese corner networks proposed in [43] introduced a modified corner pooling layer, using a pair of corner predictions to replace the estimation of the bounding box, which can avoid the need to design the anchor boxes. They are end-to-end offline training on images.

Revisit classic correlation filter trackers
Discriminative correlation filters (DCF) have been successfully applied to ensure the robustness and precision of the object tracking. Generally, the DCF-based tracking algorithms essentially train the filter based on the first frame of the tracking, adopt the filter to search for the target region, smooth the boundary region by using the cosine window, determine the target positions based on the coordinate of the maximum response score, and continuously update the filter during the tracking process. Figure 1 shows the main steps of DCF tracking.

Fig. 1
figure 1
The DCF framework of visual object tracking

Full size image
Traditional correlation filters (CF)
To achieve the multi-channel CF learning in the spatial domain, learning correlation filter h can be posed as solving a ridge regression problem as

ğœ€(â„)=12â€–â€–â€–â€–ğ‘¦âˆ’âˆ‘ğ‘‘=1ğ·ğ‘¥ğ‘‘âˆ—â„ğ‘‘â€–â€–â€–â€–22+ğœ†2âˆ‘ğ‘‘=1ğ·â€–â„ğ‘‘â€–22,
(1)
where ğ‘¦âˆˆâ„ğ‘‡Ã—1 represents the desired response ground-truth with the Gaussian-shaped. ğ‘¥ğ‘‘âˆˆâ„ğ‘‡Ã—1 and â„ğ‘‘âˆˆâ„ğ‘‡Ã—1 represent the vectorized feature and the filter for the ğ‘‘th channel of image, respectively. ğ· denotes the number of channels. âˆ— is the spatial correlation operator. ğœ† is a regularization parameter.

Spatially regularized discriminative correlation filters (SRDCF)
To address the performance degradation of the tracking model caused by boundary effects, a spatial regularization is proposed to the filter learning, which penalizes the filter coefficients according to the spatial position.

A correlation filter h is learned by minimizing the following objective:

ğœ€(â„)=12â€–â€–â€–â€–ğ‘¦âˆ’âˆ‘ğ‘‘=1ğ·ğ‘¥ğ‘‘âˆ—â„ğ‘‘â€–â€–â€–â€–22+ğœ†2âˆ‘ğ‘‘=1ğ·â€–ğ‘¤SRâŠ™â„ğ‘‘â€–22.
(2)
The spatial weight ğ‘¤SR is a negative Gaussian-shaped. The operator âŠ™ is the element-wise product. Its principle is to suppress the filter learning by applying higher weight to the filter coefficients in the background region. On the contrary, a lower weight is given to the filter coefficients in the target region to ensure that region has a higher response score. ğ‘¤SR is calculated by the below formula:

ğ‘¤SR(ğ‘–,ğ‘—)=ğ‘+ğ‘(ğ‘–âˆ’ğ‘€2ğ‘¤2)2+ğ‘(ğ‘—âˆ’ğ‘2â„2)2,
(3)
where ğ‘¤SR(ğ‘–,ğ‘—) denotes the corresponding value of ğ‘¤SR in (ğ‘–,ğ‘—), and ğ‘–âˆˆ1,â€¦,ğ‘€, ğ‘—âˆˆ1,â€¦,ğ‘. a and b denote two predefined coefficients. w and h denote the width and height of the object bounding box, respectively.

Background-aware correlation filters (BACF)
Subsequently, aiming at the problems of high cost and slow speed of SRDCF objective function optimization, BACF is proposed. The negative samples used by the tracker are generated through the real background, which include a larger search region, rather than the cyclic shift of the positive samples in the traditional CF method.

Therefore, it can effectively model according to the object changes of the background and foreground over time. The objective function of BACF is as follows:

ğœ€(â„)=12â€–â€–â€–â€–ğ‘¦âˆ’âˆ‘ğ‘‘=1ğ·ğ‘¥ğ‘‘âˆ—(ğ‘ƒğ‘‡â„ğ‘‘)â€–â€–â€–â€–22+ğœ†2âˆ‘ğ‘‘=1ğ·â€–â„ğ‘‘â€–22.
(4)
ğ‘ƒ represents a diagonal binary matrix. The foreground and real background information can be directly applied to the cyclic shift calculation by using the correlation operator.

The proposed tracker
Our tracker framework can be summarized as shown in Fig. 2. In the BAASR tracker, we firstly introduce background information into the ridge regression term so that the training samples contain more real negative samples. Considering the importance of spatial information, we add a saliency-aware spatial weight component to the objective function and adopt the saliency map to describe the change of target appearance. Through the analysis of the current frame response map, the reliability of the tracking results can be determined. Based on the above, a new spatial regularization is generated adaptively in the Fourier domain to guide the filter training.

Fig. 2
figure 2
The various components in the BAASR tracker. The purple dotted box is the initialization of the filter, which describes the combination process of a saliency result and a spatial weight. The orange dotted box shows the feature extraction, where the second dark blue layer and the penultimate red layer are the deep feature Norm1 and Conv4-3, respectively, which is combined to better characterize the target. The blue dashed box corresponds to the background-aware part, in which the response region of the red box is the background-aware region. The green dotted box is the saliency-aware part of realizing the adaptive change of the spatial weight. Section 4.5 illustrates the overall flow of the algorithm (color figure online)

Full size image
Saliency-aware detection mechanism
In the original DCF model, we generally solve a ridge regression problem to get a correlation filter and take the periodic samples obtained by cyclic shifting as training samples. Usually, the values of spatial weight ğ‘¤ are the same, which means that both the real samples and the cyclic shift samples are of the equal importance to filter training. However, the real scene is not denoted by the generated samples which are far away from the center of the target, the importance of which is also overestimated to some extent and thus leads to large errors. SRDCF uses a weight map ğ‘¤SR in Eq. (3), defined by a fixed rectangular box, which may ignore the changing appearance information, for example, the irregular object variations of shape and scale. Thus, we develop a novel spatial weight ğ‘¤Ìƒ  in Eq. (6), which contains some saliency information and can change dynamically over time.

Saliency-aware detection is applied to the solution of this new spatial weight, and it can capture the shape and size of the target well and can describe the appearance more accurately to achieve reliable tracking. Saliency detection mechanism detects the target by binarizing the image and represents their shapes, so it can be regarded as a binary segmentation problem. Specifically, the single-layer cellular automata (SCA) [44] is selected to detect the saliency of the cropped region. In SCA, averages of the color features and coordinates of pixels are applied to describe each superpixel. Each superpixel represents a cell, and the saliency value of each superpixel is used as its state, which is a continuous value between 0 and 1. As can be seen in Fig. 3, the first row shows that the red rectangular box is the bounding box. The center of the target bounding box is enlarged by k times to get the blue rectangular box region (the cropped region), which regarded as the saliency detection box. In the actual detection, we discard the saliency results outside the red box and in the blue box to suppress the background interference and then calculate the saliency results to get the saliency map S.

Fig. 3
figure 3
The first row shows the whole process of saliency detection, which can obtain saliency map through binarization. Following two lines list the saliency results of six challenging cases (color figure online)

Full size image
For online object tracking, we calculate a new spatial weight in the first frame according to the following method. We  directly substitute ğ‘† for ğ‘¤SR which will easily lead to the loss of the target, because ğ‘† pays more attention to the content of the target and ignores the effect of the context to the filter learning. The context information plays a vital role in the precise localization of the target. As shown in Fig. 3, the value of ğ‘† is smaller in the foreground (object) area, but larger in the background area, which indicates that the introduction of intensive penalty in the target context can easily affect the tracking performance. Meanwhile, ğ‘¤SR is a continuous function by which the background in the vicinity of the target can also be involved in learning to discriminative filter and get a precise localization of the target. Therefore, the saliency map ğ‘† is multiplied by the weight map ğ‘¤SR to obtain a new weight ğ‘¤Ìƒ .

ğ‘¤Ìƒ =ğ‘†âŠ™ğ‘¤SR.
(5)
Since the appearance of the target is continuously changing in the tracking process, it is necessary to change our target weight map accordingly. Therefore, how the weight map realizes the subsequent dynamic change is described as follows.

We propose a new energy function Eq. (7) to solve the new spatial weight ğ‘¤Ìƒ  to ensure the value change dynamically over time. This method combines response map c with saliency map S to expand the spatial weight. The first two terms of the function introduce the shape and change information on objects of saliency maps into the weight map to deal with the irregular change of the object. When the first term contains the saliency map of the background region or the second term contains the saliency map of the foreground region, this value becomes very large. The other terms of the function introduce response maps to reflect the reliability of tracking results into the spatial weight to address the low saliency caused by challenges such as target occlusion. The core idea is to make ğ‘¤Ìƒ  close to ğ‘¤SR when the tracking result is reliable, and to make ğ‘¤Ìƒ  close to ğ‘¤SRâ€² when the tracking results are unreliable. Our main goal is to make the value of the function as small as possible, and optimize it to obtain the final spatial weight.

Background-aware correlation filter with adaptive saliency-aware regularization (BAASR)
Based on the above discussion, we combine the background information with the spatial information to obtain a new background-aware ridge regression term and an adaptive saliency-aware spatial regularization term to achieve more effective learning of multi-channel DCF. In our work, the objective function is expressed as

ğœ€(â„)=12âˆ‘ğ‘—=1ğ‘‡â€–â€–â€–â€–ğ‘¦(ğ‘—)âˆ’âˆ‘ğ‘‘=1ğ·â„ğ‘‡ğ‘‘ğµğ‘¥ğ‘‘[Î”ğœğ‘—]â€–â€–â€–â€–22+ğœ†2âˆ‘ğ‘‘=1ğ·â€–ğ‘¤Ìƒ âŠ™â„ğ‘‘â€–22.
(6)
ğµ is a ğ‘ƒÃ—ğ‘‡ binary matrix, which crops the middle ğ‘ƒ elements of signal ğ‘¥ğ‘‘, and ğ‘‡>>ğ‘ƒ. The ğ‘¦(ğ‘—) denotes the jth element of y. Î”ğœğ‘— is the circular shift operator, and ğ‘¥ğ‘‘[Î”ğœğ‘—] applies a j-step discrete circular shift to the signal ğ‘¥ğ‘‘. ğ‘¤Ìƒ  denotes the adaptive spatial weight. We introduce a new energy function to solve the new spatial weight ğ‘¤Ìƒ  optimally. The main idea is to make ğ‘¤Ìƒ  close to ğ‘¤SR when the object tracking result is reliable, and to make ğ‘¤Ìƒ  close to ğ‘¤SRâ€² when the tracking results are unreliable. We solve the spatial weight in the following energy function:

ğœ€(ğ‘¤Ìƒ ,ğœ‡fore,ğœ‡back)=âˆ‘(ğ‘–,ğ‘—)âˆˆğ›ºfore(ğ‘†(ğ‘–,ğ‘—)âˆ’ğœ‡fore)2+âˆ‘(ğ‘–,ğ‘—)âˆˆğ›ºback(ğ‘†(ğ‘–,ğ‘—)âˆ’ğœ‡back)2+ğœŒâ€–ğ‘¤Ìƒ âˆ’ğ‘¤SRâ€–2+(1âˆ’ğœŒ)â€–ğ‘¤Ìƒ âˆ’ğ‘¤SRâ€²â€–2,
(7)
where ğ›ºfore and ğ›ºback represent the region of the foreground and the background, which are calculated by Eq. (8). ğ‘†(ğ‘–,ğ‘—) represents the saliency result S of the corresponding pixel in row i and column j and is described specifically in Sect. 4.1. ğœ‡fore and ğœ‡back are the averages of S in ğ›ºfore and ğ›ºback, respectively. ğ‘¤SR and ğ‘¤â€²SR are calculated by Eqs. (3) and (9), respectively, which means that the foreground region is given a higher penalty and the background region is given a lower value. ğœŒ determines the adaptive change of spatial weight, and the selection scheme is shown in Sect. 4.3.

The foreground and background regions are defined by

{ğ›ºfore={(ğ‘–,ğ‘—)|ğ‘¤Ìƒ (ğ‘–,ğ‘—)â‰¤ğœ}ğ›ºback={(ğ‘–,ğ‘—)|ğ‘¤Ìƒ (ğ‘–,ğ‘—)>ğœ},
(8)
where Î¶ denotes the threshold, and Î¶â€‰>â€‰0. We apply it to divide the considered ğ›º into foreground and background region. When ğ‘¤Ìƒ  at (ğ‘–,ğ‘—) is less than or equal to the threshold plane Î¶, we regard it as the foreground region, otherwise regard it as the background region.

ğ‘¤â€²SR can be solved by the below formula:

ğ‘¤â€²SR=Mw+mwâˆ’ğ‘¤SR,
(9)
where Mw and mw denote the maximum and minimum values of ğ‘¤SR, respectively.

Optimization
To improve the training efficiency, we transform the objective function (6) in frequency domain into the following form of equality constrained optimization:

ğœ€(â„,ğ‘”Ì‚ )=12â€–â€–â€–â€–ğ‘¦Ì‚ âˆ’âˆ‘ğ‘‘=1ğ·ğ‘¥Ì‚ ğ‘‘âŠ™ğ‘”Ì‚ ğ‘‘â€–â€–â€–â€–22+ğœ†2âˆ‘ğ‘‘=1ğ·â€–ğ‘¤Ìƒ âŠ™â„ğ‘‘â€–22.s.t.ğ‘”Ì‚ ğ‘‘=ğ‘‡â€¾â€¾âˆšğ¹ğµğ‘‡â„ğ‘‘,ğ‘‘=1,2,â€¦,ğ·
(10)
Using the Parseval theorem, we introduce an auxiliary variable ğ‘”Ë†ğ‘‘ by setting ğ‘”Ë†ğ‘‘=ğ‘‡â€¾â€¾âˆšğ¹ğµğ‘‡â„ğ‘‘. F denotes an orthogonal Tâ€‰Ã—â€‰T matrix, which maps T-dimensional vectorized signal to the Fourier domain. The symbol Ë† indicates the discrete Fourier transform (DFT) of a given variable.

We adopt the alternating direction method of multipliers (ADMM) method to minimize the Eq. (10) and obtain the optimal local solution. The augmented Lagrangian form can be rewritten as

ğ¿(â„,ğ‘”Ì‚ ,ğœ‘Ì‚ )=12â€–â€–ğ‘¦Ì‚ âˆ’ğ‘‹Ì‚ ğ‘”Ì‚ â€–â€–22+ğœ†2â€–ğ‘¤Ìƒ âŠ™â„â€–22+ğœ‘Ì‚ ğ‘‡(ğ‘”Ì‚ âˆ’ğ‘‡â€¾â€¾âˆšğ¹ğµğ‘‡â„)+ğ›¾2â€–â€–ğ‘”Ì‚ âˆ’ğ‘‡â€¾â€¾âˆšğ¹ğµğ‘‡â„â€–â€–22,
(11)
where ğœ‘Ë† is the Lagrange multiplier in Fourier transform, and ğœ‘Ë†=[ğœ‘Ë†1,ğœ‘Ë†2,â€¦,ğœ‘Ë†ğ·]. ğ›¾ represents the step size regularization parameter.

By assigning ğ›½=1ğ›¾ğœ‘, Eq. (11) can be expressed by

ğ¿(â„,ğ‘”Ì‚ ,ğœ‘Ì‚ )=12â€–â€–ğ‘¦Ì‚ âˆ’ğ‘‹Ì‚ ğ‘”Ì‚ â€–â€–22+ğœ†2â€–ğ‘¤Ìƒ âŠ™â„â€–22+ğ›¾2â€–â€–ğ‘”Ì‚ âˆ’ğ‘‡â€¾â€¾âˆšğ¹ğµğ‘‡â„+ğ›½Ì‚ â€–â€–22.
(12)
Finally, we apply the ADMM optimization algorithm to solve the below sub-problems.

Solutions of sub-problems
Subproblem ğ‘¤Ìƒ : Given ğ‘†, ğœ‡fore, ğœ‡back, ğ‘¤SR and ğ‘¤SRâ€², a new spatial weight ğ‘¤Ìƒ  can be obtained by solving the following sub-problem

ğ‘¤Ìƒ âˆ—=argminğ‘¤Ìƒ {âˆ‘(ğ‘–,ğ‘—)âˆˆğ›ºfore(ğ‘†(ğ‘–,ğ‘—)âˆ’ğœ‡fore)2ğ»(ğ‘¤Ìƒ (ğ‘–,ğ‘—))+âˆ‘(ğ‘–,ğ‘—)âˆˆğ›ºback(ğ‘†(ğ‘–,ğ‘—)âˆ’ğœ‡back)2(1âˆ’ğ»(ğ‘¤Ìƒ (ğ‘–,ğ‘—)))+ğœŒâ€–ğ‘¤Ìƒ âˆ’ğ‘¤SRâ€–2+(1âˆ’ğœŒ)â€–â€–ğ‘¤Ìƒ âˆ’ğ‘¤â€²SRâ€–â€–2}.
(13)
Heaviside step function H(Â·) can be defined as

ğ»(ğ‘¤Ìƒ )=12âˆ’1ğœ‹arctan(ğ‘¤Ìƒ âˆ’ğœğœ).
(14)
The final spatial weight value can be obtained by calculating the following gradients:

âˆ‚ğ‘¤Ìƒ âˆ‚ğœ=ğ›¿(ğ‘¤Ìƒ )[âˆ’(ğ‘†âˆ’ğœ‡fore)2+(ğ‘†âˆ’ğœ‡back)2]âˆ’2[ğœŒ(ğ‘¤Ìƒ âˆ’ğ‘¤SR)+(1âˆ’ğœŒ)(ğ‘¤Ìƒ âˆ’ğ‘¤SRâ€²)],
(15)
where ğ›¿(ğ‘¤Ìƒ ) is the derivative of ğ»(ğ‘¤Ìƒ ).

Subproblem h: Given ğ‘”Ë† and ğœ‘Ë†, the optimal â„âˆ— can be obtained by

â„âˆ—=argminâ„{ğœ†2â€–ğ‘¤Ìƒ âŠ™â„â€–22+ğ›¾2â€–â€–ğ‘”Ì‚ âˆ’ğ‘‡â€¾â€¾âˆšğ¹ğµğ‘‡â„+ğ›½Ì‚ â€–â€–22}=ğ›¾ğ‘‡ğµâŠ™(ğ›½+ğ‘”)ğœ†(ğ‘¤Ìƒ âŠ™ğ‘¤Ìƒ )+ğ›¾ğ‘‡ğµ.
(16)
Subproblem ğ‘”Ë†: Given â„ and ğœ‘Ë†, the optimal ğ‘”Ë†âˆ— can be obtained by

ğ‘”Ë†âˆ—=argminğ‘”Ë†{12â€–ğ‘¦Ë†âˆ’ğ‘‹Ë†ğ‘”Ë†â€–22+ğ›¾2â€–ğ‘”Ë†âˆ’ğ‘‡â€¾â€¾âˆšğ¹ğµğ‘‡â„+ğ›½Ë†â€–22}=1ğ›¾ğ‘‡(ğ¼âˆ’ğ‘‹Ë†ğ‘‹Ë†ğ‘‡ğ›¾ğ‘‡+ğ‘‹Ë†ğ‘‡ğ‘‹Ë†)(ğ‘¦Ë†ğ‘‹Ë†+ğ›¾(ğ‘‡â€¾â€¾âˆšğµğ‘‡â„)âˆ’ğ›¾ğ›½Ë†).
(17)
Through the Sherman Morrison formula (ğ´+ğ‘¢ğ‘£ğ‘‡)âˆ’1=ğ´âˆ’1âˆ’ğ´âˆ’1ğ‘¢(ğ¼ğ‘š+ğ‘£ğ‘‡ğ´âˆ’1ğ‘¢)âˆ’1ğ‘£ğ‘‡ğ´âˆ’1 (where ğ´âˆˆâ„ğ‘›Ã—ğ‘› is a invertible matrix, ğ‘¢âˆˆâ„ğ‘› and ğ‘£âˆˆâ„ğ‘› are column vectors, ğ‘¢ğ‘£ğ‘‡ is a rank-one matrix), the result of the second step is derived from the first step.

Update
Update the Lagrangian parameters according to the following formula:

ğœ‘Ë†(ğ‘–+1)=ğœ‘(ğ‘–)+ğ›¾(ğ‘”Ë†ğ‘–+1âˆ’â„Ë†ğ‘–+1),
(18)
where ğœ‘Ë†(ğ‘–) is the Lagrangian Fourier transform in the previous frame. ğ‘”Ë†ğ‘–+1 and â„Ë†ğ‘–+1 denote the current solutions to the two sub-problems in the (iâ€‰+â€‰1)th iteration.

Same as other DCF-based trackers, we apply an online adaptive template strategy to train our filter. The online updating method of filter model is as below:

ğ‘¥Ë†ğ‘¡model=(1âˆ’ğ›¼)ğ‘¥Ë†ğ‘¡âˆ’1model+ğ›¼ğ‘¥Ë†âˆ—,
(19)
where ğ‘¥Ë†ğ‘¡model and ğ‘¥Ë†ğ‘¡âˆ’1model denote the temple model at tth frame and (tâ€‰âˆ’â€‰1)th frame, respectively. ğ›¼ is a constant which represents the online update rate, and ğ‘¥Ë†âˆ— represents the current observation model.

Adaptive updating strategy of spatial weight
In the classic algorithms, the target location usually corresponds to the coordinate of the maximum response map score ğ‘max. Naturally, the larger value of ğ‘ is given to sample that is more similar to the candidate region. However, if there are some challenges such as occlusion, illumination, the multi-peak situation will occur to the response map. In this case, the position of the peak in the response map may be the background region or other similar region to the target, which will result in degradation of the tracking performance. Our algorithm employs the average peak-to-correlation energy (APCE) method to estimate the reliability of the tracking results. For the response map ğ‘, the APCE score is calculated by

APCE=|ğ‘maxâˆ’ğ‘min|2mean[âˆ‘ğ‘¤,â„(ğ‘ğ‘¤,â„âˆ’ğ‘min)2],
(20)
where ğ‘min and ğ‘ğ‘¤,â„ represent the minimum and wth row hth column pixels of the response score, respectively. It is usual that the higher value of the APCE confers to the more reliable tracking result. To ensure the speed and precision of tracking and to prevent the introduction of redundancy, we update ğœŒ over time. The necessary and sufficient condition for ğœŒ= 1 to be the tracking results reliable is that there exists the value which satisfies the following properties:

ğœŒ={10ifğ‘max>ğœ1ğ‘maxâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯andAPCE>ğœ2APCEâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯others,
(21)
where ğ‘maxâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ and APCEâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ are the average values of ğ‘max and APCE. ğœ1, ğœ2>0 are two pre-given coefficients. If both conditions are satisfied, the tracking is reliable and ğœŒ takes 1. Otherwise, ğœŒ takes 0.

Object location and scale estimation
For online object tracking, the object location in the Fourier domain can be formulated as

ğ‘Ì‚ =îˆ²âˆ’1(âˆ‘ğ‘‘=1ğ·ğ‘¥Ì‚ ğ‘‘âŠ™ğ‘”Ì‚ ğ‘‘),
(22)
posğ‘¡=argmaxğ‘š,ğ‘›(ğ‘(ğ‘š,ğ‘›)),
(23)
where ğ‘Ë† denotes the response map in Fourier transform, posğ‘¡=(ğ‘¥ğ‘¡,ğ‘¦ğ‘¡). The optimal target position can be obtained at the peak of the response map.

In our experiments, we fuse hand-crafted features and deep features on one scale and apply them to form a BAASR model to achieve target localization. For scale estimation, to avoid too much calculation due to the extensive applies to deep features, this paper adopts only HOG features to estimate the scale on five scales and then selects the one with the maximum response score as the optimal scale. The overall tracking process of the two BAASR models is shown in Fig. 4. The first branch is the BAASR model 1 for predicting target location, and the other branch is the BAASR model 2 to estimate the target scale. The ablation study in Sect. 5.2.2 verifies the effectiveness of this method.

Fig. 4
figure 4
The tracking framework of two BAASR models

Full size image
Tracking pipeline
Algorithm 1 presents a simple tracking pipeline of our tracker in detail.

figure a
Experiments
Experiment setup
Implementation details
Our algorithm is implemented on a computer with an Intel I7-9750H 8.00 GHz CPU and a GeForce GTX1650Ti 4G GPU, and on Matlab 2016b platform. Deep features Norm1 and Conv4-3 are, respectively, extracted from VGG-M and VGG-16 using the MatConvnet Toolkit, and combined them with HOG features to model the appearance of the object. The following are the specific settings of our algorithm in the experiment. The spatial regularization parameter Î» is set to 0.2. The parameter ğœ is set to 3.1, and ğœ is set to 0.01. The center of the target is cropped kâ€‰=â€‰1.5 times as the saliency detection region, and the learning rate ğœŒ of the model is dynamically calculated by Eq. (21). We set the iterations and the penalty factor Î³ of ADMM optimization to 2 and 1 and then updated by ğ›¾(ğ‘–+1)=min(ğ›¾max,ğœ‡ğ›¾ğ‘–), where ğœ‡ = 10 and ğ›¾max = 104.

Experiment datasets
To fully verify the performance of our algorithm in object tracking, we perform a series of comparison experiments on three datasets, OTB-2013, OTB-2015, and TC128, which contain 51, 100, and 128 sequences, respectively. These three datasets contain 11 common complex tracking scenarios, including illumination variation (IV), out-of-plane rotation (OPR), scale variation (SV), occlusion (OCC), deformation (DEF), motion blur (MB), fast motion (FM), in-plane rotation (IPR), out of view (OV), background clutter (BC), low resolution (LR). Meanwhile, to verify the generalizability of the tracker, we tested the experimental effects on the UAV123 and UAV20L datasets captured by the UAV. UAV123 dataset can be divided into three subsets, firstly 103 video sequences with UAV-stabilized cameras, secondly 12 video sequences captured by UAV-unstabilized cameras, and thirdly, 8 video sequences synthesized by the UAV simulator. UAV20L is a dataset used for long-sequence aerial tracking, which is more difficult to track. They contain the following 12 challenging scenarios: Scale Variation (SV), Aspect Ratio Change (ARC), Low Resolution (LR), Fast Motion (FM), Full Occlusion (FOC), Partial Occlusion (POC), Out-of-View (OV), Background Clutter (BC), Illumination Variation (IV), Viewpoint Change (VC), Camera Motion (CM), Similar Object (SOB).

Evaluation metrics
For the evaluation of our tracker, we employ two performance indexes: precision and success rate. We calculate the distance between the center of the estimated bounding box and the center of the ground-truth, and compare the percentage of video frames whose length is less than a given threshold to get the final tracking precision. The success rate is measured by the intersection-over-union (IoU) between the bounding box (b) and the ground-truth (g), and its calculation formula is expressed as IoU=|ğ‘âˆ©ğ‘”||ğ‘âˆªğ‘”|, where |â‹…| is the number of pixels in the region. If the IoU calculated in the current frame is greater than a given threshold, the tracking of this frame is considered to be successful, and the percentage of the successful frames to the total number of frames can be determined as its success rate.

Effectiveness analysis of our approach
Parameters selection
Selection of ğ›¾, ğœ‡, ADMM iterations values: to verify the influence of different iterations on the success rate score and precision score of the ADMM, we adjusted the corresponding parameters according to the settings in Table 1 and carried out experiments, respectively, and finally, the corresponding values with the best testing results are chosen to solve the objective function.

Table 1 The success rates score and precision score are based on different ğ›¾, ğœ‡, ADMM iterations in OTB-2015
Full size table
Selection of k value: Since the value of k determines the size of the saliency detection region, we set k to four values for comparison in our experiments, kâ€‰=â€‰1, 1.5, 2, 2.5. Fix other parameters, conduct test experiments on OTB-2013, and select the best k value for tracking precision. The tracking effect of different k is shown in Fig. 5. We finally set k to 1.5. If the value is large, it means that the background region of saliency detection is large, and the effect of detection is relatively poor, which will reduce the online learning efficiency of the filter. Similarly, if the k is small, the target will fill the whole cropping region. For example, when k taking 1, the main boundary of the target will be ignored, which will make little contribution to the spatial regularization. It can be seen from the experimental results that although the accuracy and success rate of the algorithm are different, it has made significant progress compared with BACF. This experiment verifies the effectiveness of adding spatial regularization term and introducing saliency detection.

Fig. 5
figure 5
The success rates scores and precision scores of different ğ‘˜ in OTB-2015

Full size image
Selection of ğœ1,ğœ2 values: In Eq. (21), ğœ1 and ğœ2 as the weights of ğ‘maxâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ and APCEâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯ to determine the value of ğœŒ. The modified formula is an essential basis to judge whether the tracking is robust. Figure 6 compares the tracking results in OTB-2015 under different settings of ğœ1, ğœ2. According to the final experimental results, we choose the value that can make the success rate score and precision score reach the highest, set ğœ1 to 0.3 and set ğœ2 to 0.4.

Fig. 6
figure 6
The success rates scores and precision scores of different ğœ1, ğœ2 in OTB-2015

Full size image
Ablation study
We here analyze the effectiveness of the innovations proposed by our tracker in turn. Ablation experiments are carried out on OTB-2015 to prove the availability of the critical components of the algorithm, and the verification results are listed in Table 2. The basic descriptions are as follows. (1) â€˜Baselineâ€™ is the BACF tracker which does not use deep features for location and nor does it introduce adaptive spatial regularization. (2) â€˜Baselineâ€‰+â€‰STâ€™ indicates a tracker that introduces saliency maps into the baseline, and the effectiveness of saliency detection can be seen by comparing the data. (3) â€˜Baselineâ€‰+â€‰DLâ€™ refers to using the fusion features of hand-crafted features and deep features to estimate the target position, instead of the multi-scale estimation method in the original feature space. (4) â€˜Baselineâ€‰+â€‰DYâ€™ introduces a new spatial weight composed of the results of response map and saliency map and realizes its adaptive update in the tracking process. (5) â€˜Baselineâ€‰+â€‰DLâ€‰+â€‰DYâ€™ is the composition of our final tracker, which combines the baseline method with saliency map, response map, an adaptive spatial weight and the location estimation of hand-crafted features and deep features. As we can see in Table 2, our contributions dramatically improve the performance of the tracker. In particular, our tracker improves 9.4% and 6.9% in success rate and precision, respectively, compared to the baseline tracker.

Table 2 Validate each innovation of the proposed algorithm on OTB-2013, OTB-2015
Full size table
State-of-the-art trackers comparison
Quantitative evaluation
We evaluate the proposed tracker on some datasets comparing with 14 state-of-the-art trackers and classify them into two broad groups. (1) DCF-based tracker: KCF [9], DSST [11], SAMF [12], MCPF [7], Staple [45], SRDCF [15], BACF [33], STRCF [18], ECO-HC [37]. (2) CNN-based tracker: HCF [35], SiamFC [39], HDT [46], DeepSRDCF [47], C-COT [36]. We have done a lot of experiments on the datasets OTB-2013, OTB-2015, and TC128 with the above trackers and compared them. The evaluation results of all the trackers on OTB-2013 and OTB-2015 are shown in Figs. 7 and 8. As can be seen, compared with many advanced tracking methods, our tracker performs well in terms of precision and success rate. Specifically, our algorithm achieves the highest precision of 94.0% and the highest success rate of 71.6% in OTB-2013. Although the performance will decrease in OTB-2015 with more videos and more difficult challenges, our algorithm still keeps the leading performance in precision and success rate, which are 91.8% and 69.0%, respectively. Besides, the comparison results of our tracker on TC128 are shown in Table 3, which also achieves excellent effect.

Fig. 7
figure 7
Comparison of our tracker and other 14 excellent trackers in terms of precision and success rate on OTB-2013

Full size image
Fig. 8
figure 8
Comparison of our tracker and other 14 excellent trackers in terms of precision and success rate on OTB-2015

Full size image
Table 3 We choose BACF, SRDCF, MCPF, ECO-HC, C-COT, and STRCF to compare with our tracker on TC128
Full size table
In order to verify the universality of the algorithm, comparative experiments are also conducted on UAV123 and UAV20L datasets captured by drones. We applied 9 excellent trackers to compare with our proposed tracker to obtain the evaluation results; they are BACF [33], STRCF [18], ARCF-H [48], ECO-HC [37], SRDCF [15], CSR-DCF [49], MCCT-H [50], STAPLE_CA [45], AutoTrack [51]. In the same way, we compare the tracker results on the UAV123 and UAV20L datasets, as shown in Figs. 9 and 10. Despite the more difficult UAV datasets, our tracker also shows relatively excellent performance.

Fig. 9
figure 9
Comparison of our tracker and other 8 excellent trackers in terms of precision and success rate on UAV123

Full size image
Fig. 10
figure 10
Comparison of our tracker and other 8 excellent trackers in terms of precision and success rate on UAV20L

Full size image
Attribute-based evaluation
In order to extensively evaluate the performance of the BAASR tracker in dealing with different complex challenges, we compare the DCF-based tracker and the CNN-based tracker on OTB-2015 and, respectively, list the tracking results they obtained under the 12 challenges, as shown in Table 4 and Fig. 11. By observing the data in these two results, we conclude that our tracker has better performance than other methods in terms of FM and MB challenges. In particular, our tracker outperforms other trackers in handling BC, DEF, and IV challenges. Due to the introduction of real background samples, the tracker has an excellent ability to deal with the background clutter. To cope with target deformation, we employ the saliency detection method to add a description of the target shape and size change. Compared with the second tracker, our tracking success rate increased by 4.6%, and the accuracy improved by 3.3%. The adaptive update strategy for subsequent frames also plays an essential role in the expression of the appearance model.

Table 4 The 11 attribute-based analyses of our tracker and other 5 deep feature-based trackers on OTB-2015
Full size table
Fig. 11
figure 11
The 11 attributes-based analyses of our tracker and other 7 hand-crafted feature-based trackers on OTB-2015. We draw the red line to express the score of our proposed algorithm (color figure online)

Full size image
For the 11 attributes-based tracking results precision plots and success rate plots on OTB-2015, as described in Figs. 12 and 13. Most of the challenges in these attributes are well handled by our tracker. For example, our tracking accuracy reaches 93.2%, which is 5.7% higher than the basic algorithm, and the success rate reaches 69.7% for the background clutter. Experimental results show that our tracker can effectively solve the various situations, such as illumination variation, background clutter, rotation, and deformation. Also, the tracker still has some shortcomings; for example, its performance is not stable enough under low resolution interference.

Fig. 12
figure 12
The 11 attributes-based precision comparisons of our tracker and other existing state-of-the-art trackers on OTB-2015

Full size image
Fig. 13
figure 13
The 11 attributes-based success rate comparisons of our tracker and other existing state-of-the-art trackers on OTB-2015

Full size image
Qualitative evaluation
We select 10 challenging sequences in the OTB-2015. Figure 14 presents the tracking results of our trackers and a total of five trackers from BACF [33], Staple [45], DSST [11], HDT [46], and SRDCF [15] on the series of sequences. The object changes rapidly in appearance from frames 50 to 95 of the Matrix sequence, and other trackers miss the target in both frames. However, our tracker still accurately captures the target. Trellis and Shaking sequences have LR, BC, OV, OCC, OPR challenges. Since we employ multiple deep features for target localization and fusion features to achieve target scale estimation, our tracker has a better performance. Out-of-rotation occurs in frame 27 of the DragonBaby video sequence and in frame 87 of the Biker video sequence, because our tracker tracks the target most reliably by adding a saliency map to the objective function, which dynamically changes the spatial regularization term according to the shape of the target. The BACF tracker loses the object. Staple and our algorithm always track the target. They ensured tracking accuracy when there was complex occlusion in the Soccer and Ironman sequences, and BACF, SRDCF, and HDT could not find the target in subsequent frames. In addition, we have considerable improvements in success rate and accuracy compared to the baseline tracker for processing BC, DEF, and IV, and more notable experimental results on sequences with other challenging aspects so that our algorithm is well suited for current tracking.

Fig. 14
figure 14
Our tracker compares the results of the bounding boxes with the other five trackers on ten challenging image sequences (from left to right and top to down are DragonBaby, Coupon, Trellis, Woman, Soccer, Shaking, Matrix, Ironman, Biker, Couple)

Full size image
Conclusions
In this paper, the traditional DCF tracking is improved by introducing background-aware information and a novel spatial regularization into the objective function. Saliency maps reflecting the target appearance information and response maps reflecting the tracking reliability are combined to the spatial regularization and establish an adaptive model updating mechanism. We propose a simple and effective energy function to obtain the solution of the spatial weight and solve the new objective function optimally by the ADMM algorithm. In the tracking process, two separate BAASR models are built with different features to achieve localization and scale estimation of the target. We evaluate our tracker on OTB-2013, OTB-2015, TC128, UAV123, and UAV20L datasets, which proves its effectiveness and generalization capability. In particular, BAASR achieves a 71.6% AUC on OTB-2013 and a 57.8% AUC on TC128, while runs only 6.89 fps and cannot effectively adapt low resolution. In future work, we will further enhance the tracking speed and precision on the premise of ensuring robustness.

Keywords
Background-aware
Saliency detection
Correlation filter
Adaptive spatial weight
Convolutional neural network
Visual object tracking