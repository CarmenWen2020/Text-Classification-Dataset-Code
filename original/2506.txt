Learning using privileged information (LUPI) paradigm, which pioneered teacherâ€“student interaction mechanism, makes the learning models use additional information in the training stage. This paper is the first to propose an incremental learning algorithm with LUPI paradigm for random vector functional-link (RVFL) networks, named IRVFL+â€‰. This novel algorithm can leverage privileged information into incremental RVFL (IRVFL) networks in the training stage, which provides a new constructive method to train IRVFL networks. In order to solve two scenarios that require fast speed of modeling but low-accuracy requirements and high accuracy but slow speed of modeling requirements, two algorithmic implementations of IRVFL+â€‰, respectively, based on local update and global update strategies are presented for data classification and regression problems in this paper. Specifically, the first algorithm, named IRVFL-I+â€‰, calculates the output weights of the newly added hidden nodes, while the input and output parameters of all the existing hidden nodes are fixed. In contrast to IRVFL-I+â€‰, the second one named IRVFL-IIâ€‰+â€‰can update all the parameters of all the existing hidden nodes and newly added hidden nodes. Moreover, the convergences of two implementations have been studied in this paper. Finally, experimental results indicate that IRVFL+â€‰indeed performs favorably.

Introduction
As an active borderline cross-science, neural networks have become a hot spot in related majors. It has been around for many decades and the history can be traced back to the 1940s. In 1943, McCulloch and Pitts proposed a formal model of neurons [1], which became the pioneering work of neural networks. Since then, the theoretical study of neural networks models has been initiated. Although the development of neural networks is not all smooth sailing, it has finally made great progress in signal processing, pattern recognition, image processing, nonlinear system modeling and control, big data processing, knowledge processing, etc.

So far, gradient algorithms [2] are the most widely used in neural network learnings. However, the traditional gradient algorithms have some difficulties to solve, such as slow convergence, easy to fall into local minimum, strong dependence on the setting of initial parameters [3]. Especially for deep neural networks and recurrent neural networks, gradient algorithms have the problems of gradient disappearance and gradient explosion, which makes it difficult for neural networks to exert the powerful learning ability [4]. On the other hand, through the anatomical reconstruction of the brain, people found that random connections exist in some brain regions and play an important role in the process of neural representation [5]. Based on this biological basis, some scholars have proposed randomized neural networks (RNNs), which calculate the networks weights connected to the output layer nodes by solving simple linear regression problems, while other networks weights and thresholds are randomly generated in a specific interval based on a given probability distribution [6]. This type of algorithm does not need to iterate repeatedly, which overcomes the bottleneck problem encountered by traditional gradient algorithms to a certain extent.

RNNs were first proposed in the 1990s. In 1994, Pao et al. [7] presented random vector functional-link (RVFL) networks, which added a direct link between the input and the output layers in the single-layer feedforward neural networks. The weights and biases between the input nodes and enhancement nodes in RVFL networks are randomly generated, and the direct link helps to keep the complexity of the model low [8,9,10]. In [11] Igelnik and Pao presented a theoretical justification for RVFL networks that showed RVFL network was a universal approximator with continuous functions on hounded finite-dimensional sets and closed-form solution. Henceforth, many scholars have proposed derivatives of RVFL networks, including quantum version of RVFL networks [12], recurrent RVFL networks [13], orthogonal polynomial expanded RVFL networks [14], exponentially expanded robust RVFL networks [15], etc. Today, RVFL networks have become one of the most popular single-layer feedforward neural networks due to its advantages of training speed, generalization performance and universal approximation ability.

Currently, a novel version of RVFL networks named RVFL+â€‰was proposed [16]. RVFL+â€‰incorporates learning using privileged information (LUPI) paradigm that can leverage additional source of information, which offers an alternative way to train the RVFL networks. The LUPI paradigm was first proposed by Vapnik and Vashist, whose motivation is to improve generalization performance of learning models [17]. They provided a support vector machine algorithm with privileged information termed SVM+â€‰. LUPI paradigm fastens the learning attention on some elements of human teaching. It considers that a teacher who can provide students with explanations plays a significant role in human learning. In contrast to the traditional learning paradigm, LUPI paradigm can provide a set of additional information for the training data during the training process. That is to say, the privileged information is available in the training stage but unavailable in the test stage. By employing additional information, LUPI paradigm solves an open problem of how the data modalities being only available during training time help the learning model achieve better prediction. The new learning paradigm is incorporated into other algorithms, such as weighted SVM [18], metric learning [19, 20], learning to rank [21], multi-view transfer learning [22] and self-paced learning [23]. In addition, for solving practical problems, LUPI paradigm has been applied to visual tracking object [24], photoaesthetic assessment [25], pedestrian detection [26], etc. RVFL+â€‰uses this idea to introduce the regularization term and correction function of privilege information into the training objective function, which improves the generalization of RVFL networks.

However, RVFL+â€‰still has the limitation of RVFL networks that is the optimal network architecture is difficult to build. The number of hidden nodes plays a critical role in determining the learning performance of the network [27]. It is obvious that neural networks with too many nodes will easy to cause time-consuming training and overfitting problems, but too little will cause underfitting problem [28]. In other words, it is feasible only when the network architecture is chosen correctly. Therefore, algorithms that can select an appropriate network architecture automatically are highly desirable. There is no generally accepted theory concerning how to select the optimal number of hidden layer nodes. So far, most proposed methods to determine the number of hidden layer nodes only consider the case of any number of training samples and the most unfavorable situation, which is difficult to meet in general engineering practice. In fact, the number of hidden layer nodes obtained by various calculation formulas sometimes differs several times or even hundreds of times.

The most basic principle for determining the number of hidden layer nodes is to take as compact structure as possible under the premise of meeting accuracy requirements. In general, there are two methods to tackle this problem: pruning algorithms [29] and constructive algorithms [30,31,32]. The former first uses a larger than needed network to train and then remove some hidden layer nodes that are no longer effectively used. The latter starts with a small network architecture and then grows additional hidden units and weights until finds an acceptable solution. Comparing with pruning algorithms, constructive algorithms have more advantages [33]. First, the constructive algorithms directly define a small initial network, but the pruning algorithms do not know how large the initial network is. Second, constructive algorithms always search for small solutions, while pruning algorithms spend most of their training time on networks larger than the optimal solution, which is more time-consuming. Moreover, pruning solutions are largely dependent on the initial networks, but building a high-quality initial network is time-consuming and difficult in general. On the contrary, constructive algorithms usually grow additional hidden nodes and easily control the quality of nodes. Therefore, constructive algorithms are likely to find smaller network solutions than pruning algorithms. In this paper, the authors will mainly focus on constructive algorithms. Incremental RVFL (IRVFL) networks algorithm approximates the output by adding hidden nodes to the network one by one. An error tolerance can be predefined in an IRVFL network, which will terminate training when the training error meets the condition. This constructive method is able to obtain a more compact model while ensuring that the desired effect can be achieved. The authors note that incremental learning algorithms could solve the problem of selecting an optimal number of hidden nodes in RVFL networks.

This paper introduces the LUPI paradigm into IRVFL networks called IRVFL+â€‰, which provides a constructive randomized approach with LUPI paradigm for flat networks. IRVFL+â€‰can make use of additional information into constructive process to obtain a network model. The main contributions of this paper are summarized as follows.

Different from other advanced neural networks with LUPI paradigm that only consider the fixed network structures, the proposed IRVFL+â€‰is an incremental learning method, which could solve the problem of constructing an appropriate network architecture.

This paper develops two algorithmic implementations with local updating and global updating strategies. Concretely, the first one just calculates the output weights when going on new grown (iteration), while the input and output parameters of all the existing hidden nodes are fixed; the second one will update all the parameters of all the existing hidden nodes and newly added nodes.

The convergence of two algorithmic implementations has been proved in this paper, which provides a strong theoretical support.

The remainder of this paper is organized as follows: The related work will be briefly introduced in Sect. 2. Section 3 details IRVFL+â€‰, consisting of theoretical analysis, algorithmic description and implementations. The authors have carried out several sets of experiments to test the validity of IRVFL+â€‰in Sect. 4, and Sect. 5 draws our concluding remarks.

Related work
Random vector functional-link networks
The architecture of a RVFL network is shown in Fig. 1. Given a set of labeled data {(xi,yi) | xiâˆˆRn, i =1, â€¦, N}, a RVFL network with P enhancement nodes (hidden nodes) can be formulated as

ğ»ğ›½=ğ‘Œ
(1)
ğ»=[ğ»1ğ»2]
ğ»1=â¡â£â¢â¢â¢ğ‘¥11â‹®ğ‘¥ğ‘1â‹¯â‹±â‹¯ğ‘¥1ğ‘›â‹®ğ‘¥ğ‘ğ‘›â¤â¦â¥â¥â¥
ğ»2=â¡â£â¢â¢â¢ğº(âŸ¨ğœ”1,ğ‘¥1âŸ©+ğ‘1)â‹®ğº(âŸ¨ğœ”1,ğ‘¥ğ‘âŸ©+ğ‘1)â‹¯â‹±â‹¯ğº(âŸ¨ğœ”ğ‘ƒ,ğ‘¥1âŸ©+ğ‘ğ‘ƒ)â‹®ğº(âŸ¨ğœ”ğ‘ƒ,ğ‘¥ğ‘âŸ©+ğ‘ğ‘ƒ)â¤â¦â¥â¥â¥
(2)
ğ›½=â¡â£â¢â¢â¢ğ›½T1â‹®ğ›½Tğ‘›+ğ‘ƒâ¤â¦â¥â¥â¥=â¡â£â¢â¢â¢ğ›½1,1â‹®ğ›½ğ‘›+ğ‘ƒ,1â‹¯â‹±â‹¯ğ›½1,ğ‘šâ‹®ğ›½ğ‘›+ğ‘ƒ,ğ‘šâ¤â¦â¥â¥â¥
(3)
where ğœ”j and bj (jâ€‰=â€‰1, 2, â€¦, P) are the weights and bias between the input layer and the hidden nodes, âŸ¨ğœ”j, xiâŸ© denotes the inner product of vectors ğœ”j and xi, and G(Â·) is a nonlinear activation function.

Fig. 1
figure 1
Architecture of RVFL networks

Full size image
From (1), we can easily calculate the output weights by the Mooreâ€“Penrose pseudo-inverse or the ridge regression as

ğ›½=ğ»â€ ğ‘Œ
(4)
ğ›½=(ğ»Tğ»+ğ¼ğ¶)âˆ’1ğ»Tğ‘Œ
(5)
where â€  is the Mooreâ€“Penrose pseudo-inverse, I is an identity matrix, and C is a trading-off parameter.

Random vector functional-link networks with privileged information
The LUPI paradigm can be described as follows: Given a set of triplets (ğ‘¥1, ğ‘¥Ìƒ 1, ğ‘¦1), â€¦, (ğ‘¥ğ‘, ğ‘¥Ìƒ ğ‘, ğ‘¦ğ‘), ğ‘¥Ìƒ ğ‘–âˆˆğ‘‹Ìƒ , ğ‘¥ğ‘–âˆˆğ‘‹ generated according to a fixed but unknown probability measure P(x, ğ‘¥Ìƒ , y) finds among a given set of functions f(x, ğ›¼),ğ›¼âˆˆÎ›, the function y= f(x, ğ›¼âˆ¼) that guarantees the smallest probability of incorrect classification or regression. Generally speaking, the additional information xâˆ¼âˆˆXâˆ¼ belongs to the space Xâˆ¼ which is different from the space X. In other words, this is a new learning paradigm that the additional information is available at the training stage, but it is not available for the test set.

Given a set of additional privileged information {ğ‘¥âˆ¼iâˆˆRd, iâ€‰=â€‰1, â€¦, N} in the training phase, the training data become {(xi,xâˆ¼i,yi) |xiâˆˆRn,ğ‘¥âˆ¼iâˆˆRd,yiâˆˆRm, iâ€‰=â€‰1, â€¦, N}. In the new training dataset, xâˆˆX is in the original feature space and xâˆ¼âˆˆXâˆ¼ belongs to a new privileged feature space Xâˆ¼ that is different fromX.

Then the RVFL+â€‰can be written as

minğ›½,ğ›½Ìƒ ,ğœ12â€–ğ›½â€–22+ğ›¾2â€–â€–ğ›½Ìƒ â€–â€–22+ğ¶ğœğ‘–(ğ›½Ìƒ ,ğ»Ìƒ )
ğ‘ .ğ‘¡.ğ»ğ›½=ğ‘¦ğ‘–âˆ’ğœğ‘–(ğ›½Ìƒ ,ğ»Ìƒ )
(6)
where Î³ is a regularization coefficient. H and Hâˆ¼ are enhanced layer output matrices corresponding to the original feature xi and privileged feature xâˆ¼i. ğœi(ğ›½âˆ¼, Hâˆ¼) is the correcting function (or slack function) in the privileged feature space, and ğ›½âˆ¼ is an output weight matrix for the correcting function. Also, the correcting function can be written as

ğœğ‘–(ğ›½Ìƒ ,ğ»Ìƒ )=ğ»Ìƒ ğ›½Ìƒ 
(7)
In the light of (6) and (7), the RVFL+â€‰can be rewritten as

minğ›½,ğ›½Ìƒ ,ğœ12â€–ğ›½â€–22+ğ›¾2â€–â€–ğ›½Ìƒ â€–â€–22+ğ¶ğ»Ìƒ ğ›½Ìƒ 
ğ‘ .ğ‘¡.ğ»ğ›½=ğ‘¦ğ‘–âˆ’ğ»Ìƒ ğ›½Ìƒ 
(8)
The output weight matrix Î² can be calculated by

ğ›½=ğ»T(ğ»ğ»T+1ğ›¾ğ»Ìƒ ğ»Ìƒ T+ğ¼ğ¶)âˆ’1(ğ‘Œâˆ’ğ»Ìƒ ğ»Ìƒ Tğ¶ğ‘™ğ›¾)
(9)
where ğ‘™T=â¡â£â¢â¢â¢1â‹®0â‹¯â‹±â‹¯0â‹®1â‹¯â‹±â‹¯0â‹®0â¤â¦â¥â¥â¥ğ‘šÃ—ğ‘ is an identity matrix. Consequently, the output function of the RVFL+â€‰can be defined as

ğ‘“(ğ‘¥)=ğ»ğ›½=ğ»ğ»T(ğ»ğ»T+1ğ›¾ğ»Ìƒ ğ»Ìƒ T+ğ¼ğ¶)âˆ’1(ğ‘Œâˆ’ğ»Ìƒ ğ»Ìƒ Tğ¶ğ‘™ğ›¾)
(10)
Incremental random vector functional-link networks with privileged information
Algorithm description
It is obvious that existing neural network algorithms with LUPI paradigm are difficult to determine the size of the network structures. To tackle this problem, this paper proposes an incremental learning algorithm with LUPI paradigm to build an IRVFL network in constructive way. The proposed learning algorithm is named as IRVFL+, and its learning process is shown in Fig. 2. In IRVFL+â€‰, the definitions ofx, xâˆ¼ and y are same as RVFL+â€‰, and the privileged information xâˆ¼ is only used in the training stage. Usually, IRVFL networks start with the direct link part and then gradually increase the hidden nodes until an acceptable performance is met. The initial residual error ğ‘’0=ğ‘“âˆ’ğ‘“0, where f0=ğ»0ğ›½0, and grown residual error eL=fâˆ’fL=fâˆ’(fL - 1+ğ»ğ¿ğ›½ğ¿+ğ»Ìƒ ğ¿ğ›½Ìƒ ğ¿)=eL - 1âˆ’(ğ»ğ¿ğ›½ğ¿+ğ»Ìƒ ğ¿ğ›½Ìƒ ğ¿) in the Lth (Lâ€‰=â€‰1,2, â€¦, Lmax) grown (where Î²L= [ğ›½ğ‘›+ğ¿,1, â€¦, ğ›½ğ‘›+ğ¿,ğ‘š] and ğ›½Ìƒ L= [ğ›½Ìƒ ğ‘›+ğ¿,1, â€¦, ğ›½Ìƒ ğ‘›+ğ¿,ğ‘š] are the output weight vectors of newly added hidden nodes corresponding to the original feature x and privileged featurexâˆ¼, respectively, in the Lth grown. The hidden layer output vectors.

Fig. 2
figure 2
Architecture of IRVFL+â€‰networks

Full size image
ğ»ğ¿=â¡â£â¢â¢â¢ğº(âŸ¨ğœ”ğ¿,ğ‘¥1âŸ©+ğ‘ğ¿)â‹®ğº(âŸ¨ğœ”ğ¿,ğ‘¥ğ‘âŸ©+ğ‘ğ¿)â¤â¦â¥â¥â¥ and ğ»Ìƒ ğ¿=â¡â£â¢â¢â¢ğº(âŸ¨ğœ”Ìƒ ğ¿,ğ‘¥Ìƒ 1âŸ©+ğ‘Ìƒ ğ¿)â‹®ğº(âŸ¨ğœ”Ìƒ ğ¿,ğ‘¥Ìƒ ğ‘âŸ©+ğ‘Ìƒ ğ¿)â¤â¦â¥â¥â¥.

are corresponding to the original feature and privileged feature after the Lth grown).

How to obtain the output weights is a crucial part of the constructive process. For two scenarios that require fast-speed but low-accuracy requirements and high-accuracy but low-speed requirements, this paper presents two algorithm implementations based on local update and global update strategies, namely IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰. The details are shown in the following sections.

IRVFL-I+â€‰: learning with local update strategy
IRVFL-I+â€‰is a speed priority algorithm, which only updates the output weights when going on a new grown while keeping the previously constructed structure and parameters fixed. This method of updating parameters is suitable for scenarios that require high model construction speed but low-accuracy requirements. The training target function of IRVFL-I+â€‰can be formulated as

minğ›½ğ¿,ğ›½Ìƒ ğ¿ğ‘“=12[ğ›½Tğ›½Tğ¿][ğ›½ğ›½ğ¿]+ğ›¾2[ğ›½Ìƒ Tğ›½Ìƒ Tğ¿][ğ›½Ìƒ ğ›½Ìƒ ğ¿]+ğ¶[ğ»Ìƒ ğ»Ìƒ ğ¿][ğ›½Ìƒ ğ›½Ìƒ ğ¿]+12â€–ğ‘’ğ¿â€–2=12â€–ğ›½â€–2+12â€–ğ›½ğ¿â€–2+ğ›¾2â€–â€–ğ›½Ìƒ â€–â€–2+ğ›¾2â€–â€–ğ›½Ìƒ ğ¿â€–â€–2+ğ¶ğ»Ìƒ ğ›½Ìƒ +ğ¶ğ»Ìƒ ğ¿ğ›½Ìƒ ğ¿+12â€–ğ‘’ğ¿â€–2
ğ‘ .ğ‘¡.ğ»ğ›½+ğ»ğ¿ğ›½ğ¿=ğ‘Œâˆ’(ğ»Ìƒ ğ›½Ìƒ +ğ»Ìƒ ğ¿ğ›½Ìƒ ğ¿)
(11)
where.

ğ›½=[ğ›½0,1â‹¯ğ›½0,ğ‘›ğ›½ğ‘›+1â‹¯ğ›½ğ‘›+ğ¿âˆ’1]T,

ğ›½Ìƒ =[ğ›½Ìƒ 0,1â‹¯ğ›½Ìƒ 0,ğ‘›ğ›½Ìƒ ğ‘›+1â‹¯ğ›½Ìƒ ğ‘›+ğ¿âˆ’1]T,

ğ»=â¡â£â¢â¢â¢ğ‘¥11â‹®ğ‘¥ğ‘1â‹¯â‹±â‹¯ğ‘¥1ğ‘›â‹®ğ‘¥ğ‘ğ‘›ğº(âŸ¨ğœ”1,ğ‘¥1âŸ©+ğ‘1)â‹®ğº(âŸ¨ğœ”1,ğ‘¥ğ‘âŸ©+ğ‘1)â‹¯â‹±â‹¯ğº(âŸ¨ğœ”ğ¿âˆ’1,ğ‘¥1âŸ©+ğ‘ğ¿âˆ’1)â‹®ğº(âŸ¨ğœ”ğ¿âˆ’1,ğ‘¥ğ‘âŸ©+ğ‘ğ¿âˆ’1)â¤â¦â¥â¥â¥
and

ğ»Ìƒ =â¡â£â¢â¢â¢ğ‘¥Ìƒ 11â‹®ğ‘¥Ìƒ ğ‘1â‹¯â‹±â‹¯ğ‘¥Ìƒ 1ğ‘›â‹®ğ‘¥Ìƒ ğ‘ğ‘›ğº(âŸ¨ğœ”Ìƒ 1,ğ‘¥Ìƒ 1âŸ©+ğ‘Ìƒ 1)â‹®ğº(âŸ¨ğœ”Ìƒ 1,ğ‘¥Ìƒ ğ‘âŸ©+ğ‘Ìƒ 1)â‹¯â‹±â‹¯ğº(âŸ¨ğœ”Ìƒ ğ¿âˆ’1,ğ‘¥Ìƒ 1âŸ©+ğ‘Ìƒ ğ¿âˆ’1)â‹®ğº(âŸ¨ğœ”Ìƒ ğ¿âˆ’1,ğ‘¥Ìƒ ğ‘âŸ©+ğ‘Ìƒ ğ¿âˆ’1)â¤â¦â¥â¥â¥
are constructed and fixed before the L-th grown. Cğ»Ìƒ ğ›½âˆ¼ and Cğ»Ìƒ ğ¿ğ›½âˆ¼ğ¿ are slack functions (or correction functions), and Î³ is a regularization coefficient.

Taking the partial derivative of the target function with respect to ğ›½ğ¿ and ğ›½âˆ¼ğ¿, we have

âˆ‚ğ‘“âˆ‚ğ›½ğ¿=ğ›½ğ¿+ğ»Tğ¿(ğ»ğ¿ğ›½ğ¿+ğ»Ìƒ ğ¿ğ›½Ìƒ ğ¿âˆ’ğ‘’ğ¿âˆ’1)=0
(12)
âˆ‚ğ‘“âˆ‚ğ›½Ìƒ ğ¿=ğ›¾ğ›½Ìƒ ğ¿+ğ»Ìƒ Tğ¿(ğ»ğ¿ğ›½ğ¿+ğ»Ìƒ ğ¿ğ›½Ìƒ ğ¿âˆ’ğ‘’ğ¿âˆ’1)+ğ¶ğ»Ìƒ Tğ¿ğ‘™=0
(13)
Combining (12) and (13), the output weights can be calculated as

ğ›½ğ¿=((ğ›¾+ğ»Ìƒ Tğ¿ğ»Ìƒ ğ¿)ğ»Tğ¿âˆ’ğ»Tğ¿ğ»Ìƒ ğ¿ğ»Ìƒ Tğ¿)ğ‘’ğ¿âˆ’1+ğ¶ğ»Tğ¿ğ»Ìƒ ğ¿ğ»Ìƒ Tğ¿ğ‘™(1+ğ»Tğ¿ğ»ğ¿)(ğ›¾+ğ»Ìƒ Tğ¿ğ»Ìƒ ğ¿)âˆ’ğ»Tğ¿ğ»Ìƒ ğ¿ğ»Ìƒ Tğ¿ğ»ğ¿
(14)
ğ›½Ìƒ ğ¿=(ğ»Ìƒ Tğ¿ğ»ğ¿ğ»Tğ¿âˆ’(1+ğ»Tğ¿ğ»ğ¿)ğ»Ìƒ Tğ¿)ğ‘’ğ¿âˆ’1+ğ¶(1+ğ»Tğ¿ğ»ğ¿)ğ»Ìƒ Tğ¿ğ‘™ğ»Ìƒ Tğ¿ğ»ğ¿ğ»Tğ¿ğ»Ìƒ ğ¿âˆ’(1+ğ»Tğ¿ğ»ğ¿)(ğ›¾+ğ»Ìƒ Tğ¿ğ»Ìƒ ğ¿)
(15)
where ğ‘™T=â¡â£â¢â¢â¢1â‹®0â‹¯â‹±â‹¯0â‹®1â‹¯â‹±â‹¯0â‹®0â¤â¦â¥â¥â¥ğ‘šÃ—ğ‘ is an identity matrix. Furthermore, we can obtain the output function ftest(z)=H(z)ğ›½ in the test stage when using the test data z.

IRVFL-IIâ€‰+â€‰: learning with global update strategy
IRVFL-IIâ€‰+â€‰is a performance priority algorithm that would update all input and output parameters when a new round of grown is coming in the training stage. This method of updating parameters is suitable for scenarios that require high model accuracy but low construction speed. The training target function of IRVFL-IIâ€‰+â€‰can be formulated as

minğ›½,ğ›½Ìƒ ğ‘¡=12â€–ğ›½â€–2+ğ›¾2â€–â€–ğ›½Ìƒ â€–â€–2+ğ¶ğ»Ìƒ ğ›½Ìƒ 
ğ‘ .ğ‘¡.ğ»ğ›½=ğ‘Œâˆ’ğ»Ìƒ ğ›½Ìƒ 
(16)
The output weights can be easily calculated according to [16]

ğ›½=ğ»T(ğ»ğ»T+1ğ›¾ğ»Ìƒ ğ»Ìƒ T+ğ¼ğ¶)âˆ’1(ğ‘Œâˆ’ğ»Ìƒ ğ»Ìƒ Tğ¶ğ‘™ğ›¾)
(17)
ğ›½Ìƒ =1ğ›¾(ğ»Ìƒ T(ğ»ğ»T+1ğ›¾ğ»Ìƒ ğ»Ìƒ T+ğ¼ğ¶)âˆ’1(ğ‘Œâˆ’ğ»Ìƒ ğ»Ìƒ Tğ¶ğ‘™ğ›¾)âˆ’ğ»Ìƒ Tğ¶ğ‘™)
(18)
Convergence analysis
For ease of reading and calculation, we simplify the target function of IRVFL-I+â€‰as

minÎ”ğ›½ğ¿ğ‘“=12Î”ğ›½Tğ´Î”ğ›½+12Î”ğ›½Tğ¿ğ´Î”ğ›½ğ¿+Î”ğ»ğ¿ğµÎ”ğ›½ğ¿+12(Î”ğ»ğ¿Î”ğ›½ğ¿âˆ’ğ‘’ğ¿âˆ’1)T(Î”ğ»ğ¿Î”ğ›½ğ¿âˆ’ğ‘’ğ¿âˆ’1)
(19)
where Î”ğ›½=[ğ›½ğ›½Ìƒ ], Î”ğ›½ğ¿=[ğ›½ğ¿ğ›½Ìƒ ğ¿], ğ´=[100ğ›¾], ğµ=[000ğ¶] and Î”ğ»ğ¿=[ğ»ğ¿ğ»Ìƒ ğ¿].

In order to verify that the simplified equation is consistent with the original equation, take the derivative of (19) with respect to Î”ğ›½ğ¿, we have

âˆ‚ğ‘“âˆ‚Î”ğ›½ğ¿=ğ´Î”ğ›½ğ¿+ğµÎ”ğ»Ìƒ Tğ¿ğ‘™+Î”ğ»Tğ¿(Î”ğ»ğ¿Î”ğ›½ğ¿âˆ’ğ‘’ğ¿âˆ’1)=0
(20)
Converting (20) to another form, we have

[ğ›½ğ¿ğ›¾ğ›½Ìƒ ğ¿]+[ğ»Tğ¿(ğ»ğ¿ğ›½ğ¿+ğ»Ìƒ ğ¿ğ›½Ìƒ ğ¿âˆ’ğ‘’ğ¿âˆ’1)ğ»Ìƒ Tğ¿(ğ»ğ¿ğ›½ğ¿+ğ»Ìƒ ğ¿ğ›½Ìƒ ğ¿âˆ’ğ‘’ğ¿âˆ’1)+ğ¶ğ»Ìƒ Tğ¿]=[00]
(21)
Obviously (21) is equal to (12) and (13). Therefore, we can obtain the output weights as

Î”ğ›½ğ¿=(ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ (Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)
(22)
â€–ğ‘’ğ¿â€–2âˆ’â€–â€–ğ‘’ğ¿âˆ’1â€–â€–2=â€–â€–ğ‘’ğ¿âˆ’1âˆ’Î”ğ»ğ¿Î”ğ›½ğ¿â€–â€–2âˆ’â€–â€–ğ‘’ğ¿âˆ’1â€–â€–2=âŸ¨Î”ğ»ğ¿Î”ğ›½ğ¿,Î”ğ»ğ¿Î”ğ›½ğ¿âŸ©âˆ’2âŸ¨ğ‘’ğ¿âˆ’1,Î”ğ»ğ¿Î”ğ›½ğ¿âŸ©=(Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)T((ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ )TÃ—(Î”ğ»Tğ¿Î”ğ»ğ¿)T(ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ (Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)âˆ’2ğ‘’Tğ¿âˆ’1Î”ğ»ğ¿(ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ (Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)<ğ‘’Tğ¿âˆ’1Î”ğ»ğ¿((ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ )T(ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)TÃ—(ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ (Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)âˆ’2ğ‘’Tğ¿âˆ’1Î”ğ»ğ¿(ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ (Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)=ğ‘’Tğ¿âˆ’1Î”ğ»ğ¿(ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ (Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)âˆ’2ğ‘’Tğ¿âˆ’1Î”ğ»ğ¿(ğ¶+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ (Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)=âˆ’ğ‘’Tğ¿âˆ’1Î”ğ»ğ¿(ğ´+Î”ğ»Tğ¿Î”ğ»ğ¿)â€ (Î”ğ»Tğ¿ğ‘’ğ¿âˆ’1âˆ’ğµÎ”ğ»Tğ¿ğ‘™)=âˆ’âŸ¨ğ‘’ğ¿âˆ’1,Î”ğ»ğ¿Î”ğ›½ğ¿âŸ©=âˆ’â€–Î”ğ»ğ¿Î”ğ›½ğ¿â€–2â‰¤0
(23)
Consequently, we can obtain an inequality: â€–eLâ€–<â€–eLâˆ’1â€–, which proves the convergence of IRVFL-I+â€‰.

Below is the proof of IRVFL-II+â€‰.

Define Î”ğ›½Ì‚ L=(A+Î”Hğ‘‡LÎ”HL)â€ (Î”Hğ‘‡Leâˆ—L - 1âˆ’BÎ”Hğ‘‡Ll) as intermediate values, Î”ğ›½âˆ—L as the output weights of IRVFL-IIâ€‰+â€‰, and then calculate the corresponding eË†L=eâˆ—Lâˆ’1âˆ’Î”HLÎ”ğ›½Ì‚ L.

â€–â€–ğ‘’âˆ—ğ¿â€–â€–2âˆ’â€–â€–ğ‘’âˆ—ğ¿âˆ’1â€–â€–2â‰¤â€–ğ‘’Ì‚ ğ¿â€–2âˆ’â€–â€–ğ‘’âˆ—ğ¿âˆ’1â€–â€–2=â€–â€–ğ‘’âˆ—ğ¿âˆ’1âˆ’Î”ğ»ğ¿Î”ğ›½Ì‚ ğ¿â€–â€–2âˆ’â€–â€–ğ‘’âˆ—ğ¿âˆ’1â€–â€–2=âŸ¨Î”ğ»ğ¿Î”ğ›½Ì‚ ğ¿,Î”ğ»ğ¿Î”ğ›½Ì‚ ğ¿âŸ©âˆ’2âŸ¨ğ‘’âˆ—ğ¿âˆ’1,Î”ğ»ğ¿Î”ğ›½Ì‚ ğ¿âŸ©
	(24)
Then, the convergence of IRVFL-II+â€‰can be proven.

Algorithm implementation
The detailed implementation procedures of IRVFL+â€‰are summarized as follows.

figure a
figure b
Experiments and results
Evaluation on classification and regression datasets
This section compares with IRVFL-I+â€‰, IRVFL-IIâ€‰+â€‰and IRVFL on five real-world multi-class classification datasets and five regression datasets from a machine learning repository named KEEL. All datasets are available: http://www.keel.es/. The statistics of the KEEL datasets are illustrated in Tables 1 and 2, including the number of training and test data, input attributes, the number of the normal features, the number of the privileged features, classes, and output. The authors randomly split attributes of each dataset mentioned above in half and select one part as normal ones and the others as privileged. All samples will be preprocessed with normalization and the best values of experimental result in each table will be bolded.

Table 1 Statistics of KEEL classification datasets
Full size table
Table 2 Statistics of KEEL regression datasets
Full size table
There is no exact theoretical basis to guide the selection of hyperparameters C and Î³, which can be only set empirically or repeat the experiment. The authors will experiment with IRVFL-I+â€‰on dataset Wine as an example to select parameters. The hyperparameters C, Î³ and u should be pre-defined by users. The maximum number of hidden layer nodes Lmax will be set as 50, and parameters C and Î³ are chosen from a random search within [11] and [10, 104], respectively. Figure 3 shows a part of results of selecting the parameters. As shown in Fig. 3, the IRVFL-I+â€‰can achieve the best performance when C set as 0.1 and Î³ set as 1000. From the figure the authors can summarize that IRVFL-I+â€‰will achieve the best performance, while C and Î³ set as a small number and big number, respectively. On the remaining datasets, the same method could be used to determine the optimal parameters for IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰. Due to space limitations, the description of this process will be omitted later.

Fig. 3
figure 3
Performance of IRVFL-I+â€‰with different user-defined parameters C and Î³, in which the IRVFL-I+â€‰uses the sigmoid function as the activation function, when Lmaxâ€‰=â€‰50

Full size image
The expressivity of a network model is typically characterized by the internal neuron behavior, which is associated with activation functions. Identify and define a suitable AF for the specified tasks to boost up the performance of a network model. Therefore, we should select an appropriate activation function for IRVFL+â€‰before comparison experiments. Table 3 reports the performance of the IRVFL+â€‰with different activation functions in the light of accuracy on Wine. As Table 3 shows, both IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰can achieve the best performance on Wine when choosing sigmoid function as activation function. Other datasets could be determined the activation functions in the same way.

Table 3 Performance of the IRVFL+â€‰with different activation functions, when Lmaxâ€‰=â€‰50
Full size table
For classification datasets, all experiments will fix the maximum hidden layer nodes Lmax to compare the performance of each algorithm. As shown in Table 4, the training accuracy and test accuracy of IRVFL IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰are given. Besides, experiments on datasets Wine, Contraceptive, Flare, Iris and Texture fixed the maximum hidden layer nodes as 50, 50, 50, 100 and 50, respectively. The authors can obtain that the performance of IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰is better remarkably than that of IRVFL, which means that the privilege information of IRVFL+â€‰can improve the generalization of the model. More importantly, IRVFL-IIâ€‰+â€‰can always achieve the best performance on all above classification datasets. Figure 4 shows the training and testing accuracy curves of IRVFL, IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰on different classification datasets.

Table 4 Comparisons with three approaches on classification datasets in terms of training accuracy and testing accuracy
Full size table
Fig. 4
figure 4
Performance of IRVFL, IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰on classification datasets, where (a) and (b) represent the performance on Wine, (c) and (d) represent the performance on Contraceptive, (e) and (f) represent the performance on Flare, (g) and (h) represent the performance on Iris, (i) and (j) represent the performance on Texture

Full size image
Like the results in Table 4, the training and testing accuracy of IRVFL+â€‰is always better than IRVFL. The above experimental results indicate that privileged information is effective in improving model performance on classification cases.

For regression datasets, all experiments will fix the error tolerance É› to compare the performance of each algorithm. Table 5 shows the training RMSE, the test RMSE and the number of hidden layer nodes of each algorithm on different regression datasets under the condition of achieving the same expected error. The error tolerance of datasets Mortgage, Treasury, Abalone, Laser and W-Izmir is set to 0.15, 0.15, 0.2, 0.225 and 0.15, respectively. As shown in Table 5, IRVFL-IIâ€‰+â€‰can achieve best performance. More importantly, under the premise of achieving the same error tolerance, the hidden layer nodes required by IRVFL-I+â€‰on Mortgage are 4.1% less than IRVFL, while IRVFL-IIâ€‰+â€‰is 79.2% less than IRVFL; on Treasury, the hidden nodes number of IRVFL-I+â€‰is 15.1% less than IRVFL, and IRVFL-IIâ€‰+â€‰is 84.3% less than IRVFL; on Abalone, the hidden nodes number of IRVFL-I+â€‰is 26.8% less than IRVFL, and IRVFL-IIâ€‰+â€‰is 87.8% less than IRVFL; on Laser, the hidden nodes number of IRVFL-I+â€‰is 10.4% less than IRVFL, and IRVFL-IIâ€‰+â€‰is 87.3% less than IRVFL; on W-Izmir, the hidden nodes number of IRVFL-I+â€‰is 17.2% less than IRVFL, and IRVFL-IIâ€‰+â€‰is 74.8% less than IRVFL. It can be seen that the algorithms with LUPI paradigm can always get much smaller network structures than IRVFL under the condition of achieving the same error tolerance, which reduces the size of the models. Figure 5 shows the curves of the training RMSE and testing RMSE of the three algorithms on different datasets. The authors can obtain that the IRVFL+â€‰always converges faster than IRVFL. Besides, under the premise of achieving the same desired solution, IRVFL-IIâ€‰+â€‰requires fewer hidden nodes than IRVFL-I+â€‰, while IRVFL requires more than IRVFL+â€‰algorithms. The above illustrates that IRVFL+â€‰performs effectively on regression cases.

Table 5 Comparisons with three approaches on regression datasets in terms of training nodes (L), training RMSE and testing RMSE
Full size table
Fig. 5
figure 5
Performance of IRVFL, IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰on regression datasets, where (a) and (b) represent the performance on Mortgage, (c) and (d) represent the performance on Treasury, (e) and (f) represent the performance on Abalone, (g) and (h) represent the performance on Laser, (i) and (j) represent the performance on W-Izmir

Full size image
Discussion
The authors conduct various experiments on different classification and regression datasets. The experimental results illustrate that IRVFL+â€‰can always perform better than IRVFL which proves that leveraging different privileged information for different classification and regression tasks is useful. Not IRVFL-I+â€‰but also IRVFL-IIâ€‰+â€‰have their own advantages and disadvantages. The former is faster but less accurate, and the latter is more accurate but slower.

Table 6 reports the complexity of IRVFL, IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰[34]. Obviously, the highest orders of IRVFL-IIâ€‰+â€‰reach O(N2) but others just O(N) for the number of data N, which explains why IRVFL-IIâ€‰+â€‰is slower to train on large-scale data than IRVFL-I+â€‰. The complexity of IRVFL-I+â€‰is slightly more complicated than that of IRVFL that lead IRVFL trains faster than IRVFL-I+â€‰. Nevertheless, IRVFL-I+ still performs better than IRVFL. The complexity of IRVFL-II+â€‰seems more complicated, but it can achieve the best performance compared with IRVFL and IRVFL-I+â€‰. Finally, the authors recommend using IRVFL-I+â€‰-based strategy when aging is required and the accuracy requirement is not high, and IRVFL-IIâ€‰+â€‰-based strategy when higher accuracy is required and the aging requirement is not high. How to improve the computational efficiency of IRVFL+â€‰is the focus of our future research.

Table 6 Comparison of the complexity of different algorithms
Full size table
Conclusion
By using LUPI paradigm, this paper proposes an incremental learning paradigm based on RVFL networks, named IRVFL+â€‰. IRVFL+â€‰leverages privileged information into the IRVFL networks in the training stage. Two algorithmic implementations of IRVFL+â€‰have been developed with local updating and global updating strategies. IRVFL-I+â€‰is a speed priority algorithm, while IRVFL-IIâ€‰+â€‰is a performance priority one. This paper has analyzed the convergence of two algorithmic implementations, which provides a strong theoretical guarantee. The fully discussions on why IRVFL-I+â€‰and IRVFL-IIâ€‰+â€‰can achieve rapidity and accuracy, respectively, have been made through complexity analysis. This newly derived IRVFL+â€‰can merge the incremental learning algorithm with the LUPI paradigm effectively, which provides a way of thinking for the application of LUPI paradigm in incremental learning.

Keywords
Learning using privileged information
Random vector functional-link networks
Incremental learning
Constructive method