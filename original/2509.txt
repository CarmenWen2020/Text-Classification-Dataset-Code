Pedestrians are often occluded by various obstacles in public places, which is a big challenge for person re-identification. To alleviate the occlusion problem, we propose a Pose-drive Attention Fusion Mechanism (PAFM) that jointly fuses the discriminative features with pose-driven attention and spatial attention in an end-to-end framework. To simultaneously use global and local features, a multi-task network is constructed to realize multi-granularity feature representation. After anchoring the region of interest to the un-occluded spatial semantic information in the image through the spatial attention mechanism, some key points of the pedestrian’s body are extracted using pose estimation and then fused with the spatial attention map to eliminate the harm of occlusion to the re-identification. Besides, the identification granularity is increased by matching the local features. We test and verify the effectiveness of the PAFM on Occluded-DukeMTMC, Occluded-REID and Partial-REID. The experimental results show that the proposed method has achieved competitive performance to the state-of-the-art methods.

Introduction
Person re-identification (Re-ID) [3, 18, 26, 34] can be understood as retrieving and identifying the same pedestrian from non-overlapping cameras at different times and spaces. With the wide application of monitoring equipment, Re-ID has become an important topic in computer vision. Pedestrians under surveillance are often occluded by trees, vehicles, buildings and other pedestrians around, so it is significant to study the subject of re-identification of occluded pedestrians.

Fig. 1
figure 1
The figure shows the result of incorrect re-identification by the traditional partitioning method. The red box indicates failure in identification. When pedestrians in probe and gallery are both occluded, the traditional blocking method is easily affected by occlusion noise, and the identification results are not ideal

Full size image
The existing methods on Re-ID [19, 22, 33, 35, 39] usually perform well when pedestrians are not covered. However, due to the complexity and diversity of the person’s environment, many influencing factors can cause errors in the final matching results. For example, light, and different perspectives cause different degrees of deformation in pedestrian images. Most importantly, people are often occluded by trees, vehicles, buildings and other pedestrians. Therefore, it is significant to study the subject of occluded Re-ID. As shown in Fig.1, the target person taken under different cameras is occluded by natural scenes. As seen in the Re-ID results, the partial models cannot effectively identify the occluded person.

Occluded pedestrian images contain more complex and disturbing information, which makes the person Re-ID more challenging. A very intuitive solution is to remove the occluded parts and then perform feature matching. Some existing works [8, 11, 37] for occluded person Re-ID manually crop occluded regions and then match the un-occluded part of the images. These methods are very inefficient and time-consuming on some large datasets, and this manual cropping process may introduce noise.

Recently, attention mechanisms [5, 13, 36] have been widely used in different computer vision tasks. Attention aims to focus on important features and suppress irrelevant features. By focusing on local regions, the attention mechanism can enhance features and improve recognition accuracy. In addition, some person Re-ID methods [1, 2, 9, 28, 35] use pose information to facilitate person Re-ID models. Pose estimation can accurately anchor the key points of a pedestrian’s skeleton, which can avoid the alignment process of the matching process and filter the occluded region.

There are two common methods for overcoming the change of person pose and perspective types. The first method evenly segments person images and extract local features. Another method extracts images with local invariant features as the overall features. Directly using the pedestrian image uniform segmentation introduces too much background noise, and extracting local invariant features relies on labor, poor stability and a heavy workload. Therefore, a pedestrian recognition method is needed, which can accurately recommend local regions and overcome occlusion.

Inspired by this, we propose a novel method that fuses the pose information with the spatial attention feature map. The model incorporates a spatial attention mechanism, which can focus more on the un-occluded regions that contain more identifying features. After the pose estimation model extracts the key point features of pedestrians, a removed occluded pose-driven attention map is obtained. Local feature matching adds more fine-grained identification features. In constructing Re-ID features, the Pose-drive Attention Fusion Mechanism (PAFM) model proposed in this paper focuses on un-occluded regions, which can effectively ignore the information of occluded regions in different scenes and effectively alleviate the impact of data deviation on identification accuracy. In summary, we make several important contributions in this paper:

∙ We propose a Pose-drive Attention Fusion Mechanism (PAFM), which is the first to apply a spatial attention mechanism to the occluded person Re-ID task.

∙ We improve the pose estimation model and construct a feature map with more robust recognition by combining the pose-guided attention map and spatial attention map, which guides the model to focus on the un-occluded human body region and alleviates the occlusion problem.

∙ We construct a multi-task network that combines global and local features to achieve multi-granularity feature representation, thus enabling the network to obtain all potential salience features. In addition, PAFM is an end-to-end structure that supports potential improvements.

∙ The experimental results show that the mean average precision (mAP) evaluation index of PAFM is significantly improved, and the most advanced effect is achieved on Occluded-DukeMTMC, Occluded-REID and Partial-REID.

Related work
Occluded person Re-ID
The existing research on person Re-ID [22, 33, 35, 39] has made some progress. However, some previous methods have ignored the occlusion problem. More recently, Zhuo et al. [40] propose an attention framework of the person body (AFPB) to solve this problem from randomly increasing occlusion. The model uses the occlusion simulator to randomly add background blocks to the target person and automatically generates a large number of artificially occluded person images. Moreover, the multi-task loss is integrated into the framework to force the network to distinguish simulated and un-occluded samples to learn a more robust feature representation against occlusion. He et al. [12] propose a model without alignment, which obtained the pyramidal features after different pooling of the spatial features collected by the fully convolutional network through spatial pyramid pooling. The similarity metric function based on foreground aware pyramid reconstruction can improve the accuracy in the case of severe occlusion. Wang et al. [31] adopt a graph neural network for modeling. The adaptive direction graph convolutional network (ADGC) can learn high-order relationship features dynamically to suppress irrelevant information and eliminate occlusion noise.

The methods mentioned above [12, 31, 40] directly extract features from the entire occluded images to match the person images and avoid cropping the image, but this cannot effectively eliminate the occlusion noise. Moreover, since there is no regional occlusion label for person images, these methods are all focused on matching entire images, which cannot be well recognized for occlusion images, which limits its applicability in actual monitoring scene.

Attention-based person Re-ID
The attention mechanism is similar to how humans focus on a small number of crucial regions when looking at an object, and it is widely used in the field of computer vision [21, 32]. The main application for Re-ID [4, 5, 13, 30] is that it can usually reset the weight of the convolution response map to highlight the important part of the image. The attention mechanism is used to enhance the model’s attention to regions with high recognition for Re-ID. Chen et al. [4] propose an attentive but diverse network, that learns attention masks directly from data and context and avoids excessive correlation and redundant attention characteristics by combining an attention module and diversity regularization as a mutual supplement. Tay et al. [30] combine the attention mechanism with attribute information. They proposed an attribute attention network, which unifies the local features of the image, as well as the appearance attributes of pedestrian clothing colors, hair, and backpacks into a single frame, which combine to learn an attribute attention feature with high discrimination. Fu et al. [5] design the residual dual attention module aggregation feature, which is embedded in the salience-guided cascaded suppression network and can mine diversified salient features and improve the network’s capacity for salient features.

The above-mentioned different attention models have increased the model’s attention to human body regions. It is also prone to excessive attention to details that are not related to the human body. The attention mechanism proposed in this paper transforms the original image features into another space and retains the key information. The irrelevant features can be effectively suppressed by establishing the spatial pair relation between feature nodes to characterize the correlation between the features and the key features.

Pose-drive person Re-ID
Pose-drive person Re-ID can effectively solve inaccurate identification caused by excessive changes in the person pose [1, 9, 17, 35]. However, pose estimation also has the problem of small pose samples, and it is challenging to transform the pose of severely distorted pedestrian samples. Therefore, Liu et al. [17] propose a pose transfer framework that uses generative adversarial networks and pose skeletons for joint learning to generate new pose variants to enhance data samples. This pose transfer model can provide sufficient discriminative features. Artacho et al. [1] propose a unified pedestrian pose estimation framework based on a waterfall, and single-pose estimation combines with context segmentation can effectively locate pedestrian poses at one stage. The waterfall-style pedestrian pose estimation structure has the feature of high accuracy and does not rely on statistical post-processing methods. Sarfraz et al. [25] construct the pose sensitive embedding (PSE) architecture. The calculated perspective probability value is used as the weight of the corresponding unit to obtain the final weighted fusion feature. The method combines end-to-end person perspective information and has a good effect on scenes with varied perspectives.

However, the effect of these models is not good when they are used in an occluded scene. Our model integrates the key point information with the spatial attention feature map, and pose estimation is used to guide feature matching. The pose information of the un-occluded region guides the model to ignore the influence of the occluded region.

Fig. 2
figure 2
A schematic overview of PAFM. It consists of Horizontally partition subnetwork (HPN), Spatial Attention subnetwork (SAN) and Pose Attention subnetwork (PAN), where HPN is to generate features of horizontal partition, SAN is to generate spatial attention map, and PAN is to predict un-occluded key points in person pose. The outputs of the three subnetworks are combined by homotopic uncertainty learning to predict person Re-ID

Full size image
The proposed method
In order to better solve the impact of occlusion on Re-ID and extract more discriminative features from occluded person images, we propose an end-to-end occluded Re-ID model that integrates spatial attention and pose estimation. Figure 2 shows our PAFM architecture, which consists of Spatial Attention subnetwork (SAN), Horizontally Partition subnetwork (HPN) and Pose Attention subnetwork (PAN). Given an image, SAN contains a spatial attention module, which is used to calculate the relationship between pixels of output features of the backbone network and enhance the feature expression of regions with rich semantic information. PAN can predict pedestrian key points and set thresholds to eliminate occluded regions. HPN fuses the spatial attention feature map and then fused it with the pose feature map to eliminate as much of the influence of occlusion on the matching results as possible. This section elaborates on the various modules of PAFM and finally introduces the loss function used in this section.

Spatial attention subnetwork
The purpose of spatial attention is to establish a paired relationship between pixels in the spatial domain, to capture and aggregate the pixels with higher semantic correlation so that the model can focus more on the un-occluded regions in the images. The orange part in Fig. 2 indicates the Spatial Attention Subnetwork (SAN). The details of spatial attention (SA) are shown in Fig. 3.

The convolutional feature map extracted from the backbone network 𝐴𝐴∈ℝ𝐶×𝐻×𝑊 can be used as the input of SA, where C is the total number of channels and 𝐻×𝑊 is the size of the feature map. The convolutional feature map 𝐴𝐴 can be expressed as an 𝑁×𝐶 2-dim matrix after deformation, where 𝑁=𝐻∗𝑊 represents the number of pixels in the convolutional feature map, and the matrix can be regarded as containing N C-dim feature vectors. In this paper, these feature vectors (i.e., pixels) are expressed as 𝐴𝐴𝑖, where 𝑖=1,⋯,𝑁. The attention map generated by the spatial attention module is:

𝐹𝐹𝑠=[g(𝛿(𝜏(𝑊𝑊1𝐴𝐴)))]©[𝛿(𝜏(𝑊𝑊2𝑆𝑆𝑖))]
(1)
where 𝑊𝑊1 and 𝑊𝑊2 are the parameters of the 1×1 spatial convolution layer, and the sizes of the convolution kernel are 𝐶×(𝐶/𝑟)×1×1 and 2𝑁×(2𝑁/𝑟)×1×1, respectively. r is a pre-defined positive integer that controls the dimensionality reduction ratio. 𝜏 represents the Batch Normalization (BN) layer and 𝛿 represents the ReLU activation function. g(⋅) is the global average pooling operation along the channel dimension. After pooling, the channel dimension is reduced to 1. © denotes the connection operation. The original feature after the dimensionality reduction and the feature containing the global relationship are embedded and then connected into an attention map. Where 𝐹𝐹𝑠 contains both the original feature and global scope feature, it has better identification.

The global relationship feature 𝑆𝑆𝑖∈ℝ2𝑁 can be expressed as:

𝑆𝑆𝑖=𝑀𝑀(𝑖,:)©𝑀𝑀(:,𝑖)
(2)
where 𝑀𝑀(𝑖,:) is the element at the 𝑖𝑡ℎ row of the incidence matrix 𝑀𝑀∈ℝ𝑁×𝑁 between pixels, and 𝑀𝑀(:,𝑖) represents the 𝑖𝑡ℎ column. Therefore, the calculation of global relation features 𝑆𝑆𝑖 can be understood as connecting the row relation features and column relation features obtained through deforming of the incidence matrix between pixels. The calculation formula of the incidence matrix 𝑆𝑆 between pixels is as follows:

MM𝑖𝑗=[𝛿(𝜏(𝑊𝑊3𝐴𝐴𝑖))]𝑇[𝛿(𝜏(𝑊𝑊4𝐴𝐴𝑗))]
(3)
where 𝑀𝑀𝑖𝑗 is the 𝑖𝑡ℎ row and 𝑗𝑡ℎ column of the incidence matrix 𝑀𝑀 and indicats the effect of the 𝑖𝑡ℎ pixel on the 𝑗𝑡ℎ pixel, where 𝑖,𝑗∈[1,𝑁]. 𝑊𝑊3 and 𝑊𝑊4 are the parameters of the 1×1 spatial convolution layer, and the size of the convolution kernel is 𝐶×(𝐶/𝑟)×1×1.

Fig. 3
figure 3
Diagram of Spatial Attention (SA). The input of the module is the feature map from the backbone network, and its output is the spatial attention map. When computing the attention at a feature position, in order to fusion the relevance global pixels and local original information, we stack the pairwise relation items, and the feature of this position, for learning the attention with convolutional operations

Full size image
The attention map obtained through the above stages is used as the input of global average pooling. Then, a three-layer micro-network structure, including a convolutional layer (V), BN layer (B), and ReLU layer (R), is used to reduce the channel dimension to increase the network depth further and learn deeper semantic features.

𝐹𝐹𝑜𝑢𝑡=𝛿[𝜏(𝑊𝑊5(𝑔(𝐹𝐹𝑠)))]
(4)
where 𝑊𝑊5 is the parameter of the convolution layer. After obtaining the output of SAN, softmax is used to predict the identity of each incoming image. The training stage uses cross-entropy loss.

𝐿𝑠=𝐶𝐸(𝑦⌢,𝑦)
(5)
where CE denotes the cross-entropy loss, 𝑦⌢ denotes the prediction and y denotes the ground truth ID.

Horizontally partition subnetwork
Inspired by the Part-based Convolutional Baseline(PCB) [29] network, it is easy to lose many detailed features if the global features extracted from the backbone network are pooled and classified. The comprehensive representation of pedestrians by global features and local features can effectively improve the representation ability of features. To refine the network structure and further improve the characterization capability of the features, HPN is added in this paper, whose structure is shown in the upper part of Fig. 2.

In the HPN, we are not simply horizontally partitioning the global features extracted from the backbone network. The approach of this paper is to fuse the attention features output by the SAN with the global features and then partition them. Because the noise in parts with severe occlusion can easily lead to incorrect matching results, the global feature map fused with spatial attention can guide the model to focus on un-occluded parts and minimize the influence of occlusion noise on matching performance. Specifically, we extract the attention aware feature map by taking the outer product of the output by the backbone network and the attention map 𝐹𝐹𝑠 of the person images. Next, the attention feature map is horizontally divided into p parts, and each part is represented as 𝐷𝐷𝑝, where p=1,⋯,𝑃. Then global average pooling is performed for each part.

During training, the feature vectors are sent to the fully connection layer for classification. After obtaining the classification result of each part, the labels that are the same as the global feature classification of person are used to calculate the cross-entropy loss. The cross-entropy loss function of multiple tasks is shown as:

𝐿ℎ=∑𝑝=1𝑃−logexp(𝑊𝑊𝑇𝑦𝑝𝐷𝐷𝑝+𝑏𝑏𝑦𝑝)∑𝑘=1𝐾exp(𝑊𝑊𝑇𝑘𝐷𝐷𝑝+𝑏𝑏𝑘)
(6)
where 𝑦𝑝 represents the pedestrian identity, k is the total number of person identities in the training set, and {𝑊𝑊,𝑏𝑏} denotes the weight and bias of the classification layer respectively. The total loss of a partial characteristic subnetwork with horizontal partitioning is denoted by 𝐿ℎ.

Pose attention subnetwork
Pose estimation has shown good performance in the application of person Re-ID. For specific problems such as occlusion, effectively eliminating occlusions can suppress noise interference. Inspired by this, the PAN aims to construct global features that contain pose information. After inputting a person image, the pose estimation model can extract key points and obtain the confidence maps of the key point regions. Each key point corresponding to the visible region has a Gaussian peak, and the key points with occlusion have low confidence.

The datasets of Re-ID lack the labels for pose key points. We use the pre-trained AlphaPose model on the COCO [16] dataset as the pose estimator to generate pose key points, which predicts a total of 18 key points. To reduce the complexity of the pose estimation model and prevent the model from paying too much attention to some details of pedestrians, we fused the key points of the head region and finally obtained 14 key points, including the head, neck, shoulders, elbows, wrists, hips, knees, and ankles. Next, a threshold 𝜆 is set to filter out the key points with lower confidence to eliminate the occluded region.

𝐿𝑀𝑞={(𝑋𝑞,𝑌𝑞)0𝑐𝑜𝑛𝑓𝑞≥𝜆𝑒𝑙𝑠𝑒(𝑞=1,…,𝑄)
(7)
where 𝐿𝑀𝑞 is the symbol of the 𝑞th key point, (𝑋𝑞,𝑌𝑞) represents the corresponding coordinate, and 𝑐𝑜𝑛𝑓𝑞 is the confidence degree of the key point.

Then, we fuse the heatmaps and the spatial attention map. If a simple summation or pooling operation is used when fusing features, it is easy to introduce feature noise interference. We connect the features of the sub-networks and then perform convolution operations through the fusion module. First, heatmaps are multiplied by the spatial attention map to obtain a feature map containing pose information. Since each heatmap clearly marks the occluded region of the image, the feature map containing pose information can make the model pay more attention to the un-occluded region, which is effective in suppressing the occlusion noise. Then, connect the feature map containing the pose information 𝐹𝐹𝑝𝑜𝑠𝑒 with the spatial attention map 𝐹𝐹𝑠. The convolutional neural network here uses deep convolution and point convolution, which can obtain more fine-grained fusion features, and can effectively reduce the parameters of the neural network. The fused feature map is obtained and classified after being fully connected. The whole fusion process can be expressed as:

𝐹𝐹𝑓𝑢𝑠𝑒=𝑊𝑊7[𝑊𝑊6(𝐹𝐹𝑝𝑜𝑠𝑒⊗𝐹𝐹𝑠)©𝐹𝐹𝑠)]
(8)
where 𝑊𝑊6 and 𝑊𝑊7 are implemented by deep convolution and point convolution. ⊗ is the outer product operation. During the training of the subnetwork, similar to the above network structure, softmax is used to predict the identity of each input image, and cross-entropy is used as the loss function.

Loss function
Our framework contains three subtask networks, which can be regarded as multi-task networks. Each subtask has different contributions to the whole model. If the weight of each subtask loss is set to the same value, the final identification accuracy will be affected. The optimal weight of each subtask depends not only on the measurement scale but also on the noise level of each task. Therefore, in the work of this paper, homotopic uncertainty learning [14, 15] is used to combine the loss of multiple subtasks.

The different weights of multiple tasks can be optimized by using Bayesian modeling for homotopic uncertainty. When the loss of one of the subtasks increases, the weight parameter decreases, and vice versa. We assume that the prediction error satisfies the Gaussian distribution. We define the Bayesian probabilistic model classification likelihood output as:

Γ(𝑦|𝐹𝐹𝑎,𝜇)=Softmax(1𝜇2𝐹𝐹𝑎)
(9)
where 𝐹𝐹a is the output of the neural network, and a is one of the three subnetwork. 𝜇 is the observation noise. The log likelihood for this output is given by

𝑙𝑜𝑔(Γ(𝑦=𝑛|𝐹𝐹𝑎,𝜇))=1𝜇2𝐹𝐹𝑎(𝑛)−𝑙𝑜𝑔∑𝑛′exp(1𝜇2𝐹𝐹𝑎(𝑛′))
(10)
where 𝐹𝐹a(n) is the 𝑛𝑡ℎ elem of 𝐹𝐹a. ∑𝑛′(⋅) is the total likelihood of all classes. Then, we model with a softmax likelihood. The loss is given as:

𝐿(𝜇)=−𝑙𝑜𝑔(Γ(𝑦=𝑛|𝐹𝐹𝑎,𝜇))
(11)
What we need is the cross-entropy loss of the non-scaled y, which is defined as:

𝐿𝑎=−𝑙𝑜𝑔(Softmax(𝑦,𝐹𝐹𝑎))
(12)
The loss function can be simplified to

𝐿(𝜇)≈1𝜎2𝐿𝑎+𝑙𝑜𝑔𝜇
(13)
By applying the above loss function to our PAFM, the total task loss of this network structure is:

𝐿𝑡𝑜𝑡𝑎𝑙≈1𝜇21𝐿𝑠+1𝜇22𝐿ℎ+1𝜇23𝐿𝑝+𝑙𝑜𝑔𝜇1𝜇2𝜇3
(14)
where 𝐿𝑠,𝐿ℎ, and 𝐿𝑝 for SAN, HPN and PAN loss of three tasks, respectively. 𝜇1,𝜇2,𝜇3 are their noise factor, and they are determined by multi-task learning in training.

Experiments
Dataset and evaluation metrics
Datasets In order to verify the effectiveness of our model to occluded person Re-ID, experiments are performed on Occluded-DukeMTMC [20], Occluded-REID [40] and Partial-REID [37]. In addition, we evaluate the effectiveness of the proposed model on two large-scale person Re-ID datasets Market-1501 [36] and DukeMTMC-reID [23].

Occluded DukeMTMC is a large occluded dataset from DukeMTMC-reID. The training set contains 15618 images of 702 pedestrians, and the test set contains 1110 identities. Among them, there are 17661 images in the gallery and 2210 images in the query set.

Occluded-REID contains images of pedestrians captured by mobile cameras on campus, which contains 2000 annotated images of 200 occluded persons. Among them, each person’s identity contains five full-body and five differently occluded images.

Partial-REID is the first partial person Re-ID dataset. Images are collected from different viewpoints, backgrounds and different types of severe occlusion. It includes 900 images of 60 identities. Each of them contains five full-body images, five occlusion images, and five hand-cut half-length images. We only use full-body images and half-length images for performance evaluation.

Market-1501 contains 1501 images of different identities taken from 6 cameras, all of which are of fixed size. It includes 19732 gallery images and 12936 training images. The dataset contains fewer occlusions or partial images.

DukeMTMC-reID contains 36411 images of different sizes taken by 8 cameras. There are 1404 identities, 16522 training images, 2228 images to be queried, and a total of 17661 gallery images.

Evaluation We use two evaluation indicators, the cumulative matching characteristic(CMC) and the mean average precision(mAP), to measure the performance of the model. The CMC curve mainly reflects the accuracy of the entire model, often representing the first n matching results in the form of Rank-n; mAP is the average accuracy of all returned results of a certain category.

Implementation details
The GPU of the hardware platform used in the experiment is a 32G Tesla V100, and the operating system used is Ubuntu 18.04. The deep learning framework used in the experiment is PyTorch1.0, the Python version is 3.7, and the CUDA version is 10.1.

We take ResNet50 [10] pre-trained on ImageNet [6] as the backbone network, and fine-tune the network: the pooling layer and the fully connected layer of the last layer of the network are removed. In order to obtain richer feature information, set the step size of conv4_1 to 1. In the training stage, the size of all input images is resized to 384×128, and the data are enhanced by random horizontal flipping and random erasing. The batch size is set to 32, and the number of training epochs of the model is set as 120. In the SAN, the predefined positive integer r controlling the dimensionality reduction ratio is set to 8. In HPN, the feature map is horizontally divided into p parts, here p is set to 3. In the PAN, the confidence threshold 𝜆 is 0.2. For the Occluded-DukeMTMC and Occluded-REID datasets, the initial learning rate is set to 0.1, and after 40 epochs it is reduced 0.01 via the Adam optimize. For the Partial-REID dataset, the initial learning rate is set to 0.02.

Table 1 Performance(%) comparison with the attention methods, pose estimation methods and partial methods in the three datasets. The 1/2 best result is red/blue
Full size table
Comparison with SOTA
Table 1 is divided into three groups, and compared with the attention methods, partial methods and pose estimation methods on the three datasets of Occluded-DukeMTMC, Occluded-REID and Partial-REID.


Comparison with attention methods The comparison results with some attention methods [4, 5, 30] are illustrated in the first group of Table 1. In these methods, ABD-NET [4] introduces a bi-attentional mechanism and regularization to complement each other. AANET [30] proposes an attribute attention mechanism combining attributes and attention. SCSN [5] designs the aggregation characteristics of the residual dual attention module. It is easy to see that the proposed model performs well on the three datasets. Our accuracy of Rank-1 is approximately 10% higher than that of the second-best attention method. This comparative experiment shows that the attention mechanism that lacks occlusion processing does not perform well in re-identifying occluded persons, so it is necessary to design a special occlusion processing mechanism.


Comparison with partial methods The second group in Table 1 shows the experimental results of the partial methods [11, 12, 27, 29]. Local matching may lead to misalignment of the image, which may lead to a misalignment. When segmented images are segmented with occlusion, there may be serious noise interference in each segmented image, which may increase the difficulty of local matching and reduce the matching performance. Compared to the existing partial-based person Re-ID methods, our model shows better applicability to occluded persons. Since the feature map of attention perception is divided into blocks in this paper, the feature map after integrating the attention mechanism enhances the attention to the uncovered region, and the uncovered region can be more accurately matched after partitioning.


Comparison with pose estimation methods The third group in Table 1 shows the performance of the pose estimation methods [20, 24, 31]. It is easy to see that our model achieves 55.1%, 76.4% and 82.5% Rank-1 accuracy on the three datasets, which are higher those that of the three pose estimation methods. The performance improvement in this paper is mainly due to partial matching being more suitable for occluded person Re-ID tasks than global feature learning. Compared with simply fusing features with heat maps of pose key points, pose fusion features are more effective for recognition accuracy.

Table 2 Performance(%) comparison with the existing state-to-the-art model on Market-1501 and DukeMTMC-reID. The 1/2 best result is red/blue
Full size table

Performance on holistic datasets Although some of the existing occluded models have achieved good matching results on occluded and partial datasets, due to the influence of noise in the feature learning and alignment process, holistic datasets may also face model overfitting. The problem often fails to achieve satisfactory performance. As shown in Table 2, the experimental results on the Market-1501 and DukeMTMTC-reID holistic datasets are presented. The corresponding result of “+Aug” is that when training the model in our work, random occlusion image enhancement is added to solve the problem of data imbalance in the training set. It can be observed from the table that the PAFM model has achieved the start-of-the-art, indicating that our model is also effective for the holistic dataset. It also shows that the method in this paper is not limited to blocking pedestrians and has better universality.

Table 3 Performance(%) comparison of different task loss combinations on the Occluded-DukeMTMC dataset
Full size table
Table 4 Experimental results(%) of placing the attention module (SA) in different positions
Full size table
Ablation studies
First, the effect of three subnetworks on the discernibility is verified in the Occluded-DukeMTMC dataset. The experimental results are shown in Table 3. When these related subtasks are added to the network, the network’s overall recognition accuracy will increase, which verifies that each subtask has a specific contribution to the model’s overall performance. Besides, compared with setting the same weight for each subnetwork, the homotopic uncertainty learning we use can effectively optimize the learning objectives and effectively improve performance.

Then, we analyze the impact of the spatial attention (SA) module at different layers of the backbone network ResNet-50. Since the SA module can be plug-and-play, we add the proposed SA module after all residual blocks (including conv2_x, conv3_x, conv4_x, and conv5_x). Table 4 shows the experimental results on the Occluded-DukeMTMC and Partial-REID datasets. It can be seen from the data in the table that mAP and Rank-1 are improved after adding the SA module after different residual blocks, and the performance of the SA module after conv5_x can reach the best performance.

Next, we attempt to use three different pose estimation algorithms in the model: OpenPose [2], HR-Net [28] and AlphaPose [7]. The model was tested on two different datasets Occluded-DukeMTMC and Partial-REID. The experience results are shown in Table 5. It can be seen that the three algorithms have reached similar accuracy. In the end, we use the better performance AlphaPose.

Table 5 Performance(%) comparison of different pose estimation algorithms on Occluded-DukeMTMC and Partial-REID
Full size table
Fig. 4
figure 4
The influence of setting the confidence threshold 𝜆 of attitude estimation on the accuracy of different datasets

Full size image
As shown in Formula 7, a threshold 𝜆 is set in this paper to filter out key points with low confidence to eliminate occluded regions. Figure 4 shows the effect of different threshold settings on the identification accuracy on different datasets. When 𝜆 is too small or too large, the performance is poor. When it is too small (for example, 0), the model selects all detected markers, which does not achieve the purpose of using key points to eliminate occlusion. When the information with the occlusion region is used for matching, noise information will inevitably be created. When 𝜆 is too high, many flags are discarded. The corresponding regions of these discarded signs, although they may not be covered in any way, are unnecessarily discarded. It can be seen from the experimental results that when the threshold value is 0.2, the accuracy of the three datasets reaches the optimal level.

Fig. 5
figure 5
The influence of different setting of horizontal block p on the accuracy of different datasets

Full size image
Figure 5 shows the effect of different settings of horizontal block p on the accuracy of different datasets. The size of p has a great influence on the granularity of the block feature. When p = 1, the learned feature is a global feature, and the performance is always worse than the accuracy when p>1, which proves the necessity of blocking the features extracted by the convolutional neural network. When it is increased to 3, the performance of the model reaches the best. When it is greater than 3, the performance starts to decline slowly. When the number of parts is too large, some un-occluded parts may not contain any key points. The corresponding confidence of these regions in Formula 7 is 0, and these regions are filtered out during matching.

Fig. 6
figure 6
Comparison of PCB, SCSN and PAFM. The green and red rectangles represent correct and incorrect retrieval results, respectively

Full size image
Visualization
Figure 6 shows some retrieval examples of PCB, SCSN and PAFM on the Occluded-REID dataset. The search results show that PCB and SCSN easily mix the information of the target person and the obstacle, resulting in the retrieval of the wrong person with similar obstacles. In contrast, PAFM integrates the pose information to effectively mark the occlusion regions and works well on occluded person Re-ID.

Fig. 7
figure 7
Visualization results of using the attention mechanism(SA) alone and the overall model (PAFM)

Full size image
Figure 7 shows the visualization results of using the attention mechanism alone and the overall model using gradient response in this paper. The gradient response can identify regions that the network model considers relatively essential. In the figure, “Baseline” uses the Resnet50 network model, and “SA” indicates that we use the spatial attention mechanism model alone. As seen in Fig. 7, this model focuses on the un-occluded regions more accurately, showing better effects compared with the baseline model.

Conclusion
In this paper, we propose a multi-task network to solve occluded person Re-ID. The spatial attention mechanism and pose estimation can mine recognizable fine-grained features from global features to eliminate occlusions. Moreover, local feature alignment increases the granularity of the network. Our model has good recognition accuracy, better robustness, and generalization ability than other advanced methods through abundant experimental analysis and verification. In future work, we will further study occlusion pedestrian identification combined with pose estimation. For example, we will design a background interference suppression mechanism or use graph convolution to mine deep semantic information to improve the accuracy of occluded person Re-ID.

Keywords
Attention mechanism
Pose estimation
Multitask network
Person re-identification