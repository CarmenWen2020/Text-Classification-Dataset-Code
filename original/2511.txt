Over the past few years, deep learning methods have proved to be of great interest for the computational fluid dynamics community, especially when used as surrogate models, either for flow reconstruction, turbulence modeling, or for the prediction of aerodynamic coefficients. Overall exceptional levels of accuracy have been obtained but the robustness and reliability of the proposed approaches remain to be explored, particularly outside the confidence region defined by the training dataset. In this contribution, we present an autoencoder architecture with twin decoder for incompressible laminar flow reconstruction with uncertainty estimation around 2D obstacles. The proposed architecture is trained over a dataset composed of numerically-computed laminar flows around 12,000 random shapes, and naturally enforces a quasi-linear relation between a geometric reconstruction branch and the flow prediction decoder. Based on this feature, two uncertainty estimation processes are proposed, allowing either a binary decision (accept or reject prediction), or proposing a confidence interval along with the flow quantities prediction (u, v, p). Results over dataset samples as well as unseen shapes show a strong positive correlation of this reconstruction score to the mean-squared error of the flow prediction. Such approaches offer the possibility to warn the user of trained models when provided input shows too large deviation from the training data, making the produced surrogate model conservative for fast and reliable flow prediction.

Introduction
During the last few years, the computational fluid dynamics (CFD) community has largely benefited from the fast-paced development of the machine learning (ML) field, and more specifically from that of the neural networks (NN) domain. In many cases, a part of the usual numerical resolution process is replaced with a trained NN, in order to reduce its computational cost. Examples for these applications are the prediction of closure terms in RANS [1, 2] or LES [3] computations. In other situations, a supervised network is trained to directly predict a flow profile: in [4], an autoencoder is used to obtain steady state flow predictions around elementary and real-life shapes; in [5], a fusion convolutional neural network (CNN) is trained to predict velocity snapshots around a cylinder in weakly turbulent flows, using the time history of pressure around the cylinder as an input; in [6], a neural network is trained to predict unsteady flow around a circular cylinder, by minimizing a physical loss function composed of regression error and conservation laws. CNNs were also directly applied to predict lift and drag coefficients of 2D airfoils [7] or arbitrary shapes [8].

Still, in most of the proposed works, the question of the reliability of the predictions produced by the trained models is left out. Indeed, the topological complexity of the input space can make it hard to determine whether not a given element, provided by an external user, lies within the boundaries of the dataset used during training. While very few approaches were proposed to tackle such issues in the context of NN-assisted CFD, several outlier detection techniques have been proposed in other domains. Among them, unsupervised methods have attracted much attention, as they do not require labeled data. In particular, several autoencoder (AE)-based techniques were developed for medical and industrial applications: in [9], a fully connected AE with three hidden layers is applied to breast cancer detection; in [10], a convolutional AE (CAE) is used to detect cracking and spalling defects on concrete structures; in [11], CAE is used to detect miss-printed logo images on mobile phones, and in [12], the authors compared several variants of AE on anomaly segmentation in brain magnetic resonance images.

Autoencoder is feedforward neural networks that are widely applied to dimension reduction and feature extraction of high-dimensional data [13]. As shown in the sketch of Fig. 1a, AEs are composed of a contractive path, named encoder, whose role is to compress input data to a space of reduced dimension called latent space. The latent space representation of the input variables is obtained at the bottleneck of the structure, which is followed by a decoder branch, mirror of the encoder one, responsible for the reconstruction of the input. Autoencoders can be trained both in unsupervised or supervised way, depending on the application scenario. In the case of unsupervised learning, AEs are usually exploited to infer the latent space structure of a given dataset. In the CFD community, this functionality makes AE potential candidate tools for model reduction. In [14], AEs are used in conjunction with convolutional layers to learn low-dimensional features of fluid systems. In [15] and [16], the authors combine recurrent neural networks with CAE to learn the dynamics of the extracted low dimensional features. Adversely, in the case of supervised learning, AEs are exploited to perform various full-field flow prediction tasks [4,5,6]. Among the multiple variations of AE structures, the special case of U-net [17] must be mentioned. As sketched in Fig. 1b, U-nets structures contain skip connections from the encoder to the decoder, the role of which is to concatenate low-level features from the contractive path to the expansion path (here, concatenation means stacking tensors together along the channel axis). By allowing the mixing of low-level features with the high-level latent-space representation, U-nets usually achieve excellent performance levels on segmentation [17] and regression tasks: in [18], the authors exploit U-nets to infer the velocity and pressure fields of turbulent flow around airfoils computed in a Reynolds-averaged Navier-Stokes framework; in [19], a U-net-like architecture is used to reconstruct turbulent flows from extremely coarse flow field images with remarkable accuracy; in [20], a recurrent U-net architecture is trained to predict the instationary velocity and pressure fields in porous membranes.

Fig. 1
figure 1
Sketch of autoencoder architectures. Standard autoencoders (left) are composed of an encoder and a decoder paths, and can be exploited either for end-to-end regression tasks (in a supervised way, with labels), or for the inference of latent space representations (in an unsupervised way, without labels). U-net autoencoders are a specific class of AE, in which skip connections are added from the encoder branch to the decoder one in order to mix high-level features from the latent space with low-level one from the contractive path. They usually present a superior level of performance on regression tasks

Full size image
In the present paper, we introduce an autoencoder architecture with a twin-decoder as a possible tool for outlier detection in the context of fluid flow predictions. New contributions of this work include:

A novel twin-decoder architecture displaying a strong correlation between the input reconstruction and the flow prediction error levels by taking advantage of proper skip connections between the two decoder branches. We find that this correlation is almost linear at the expense of a slightly lower flow prediction accuracy than that of a U-net with similar structure;

Two uncertainty estimation procedures taking advantage of the latter property: (i) a qualitative procedure based on a user-provided error threshold level, providing binary decisions regarding the prediction (accept or reject), and (ii) a quantitative procedure, providing the user with a error level interval on top of the flow prediction. Results over the considered dataset as well as unseen shapes proved these methods to be efficient to detect the applicability limits of the trained model. Both methods rely on simple concepts, and can be easily applied to other end-to-end prediction tasks.

The paper is organized as follows: the problem settings and dataset construction are presented in Sect. 2. Insights about the proposed twin-decoder architecture and its training procedure are given in Sect. 3. The concepts of both the qualitative and quantitative trust-level methods are then described. In Sect. 4, the performance of the method is first explored through a hyper-parameter calibration, then the best architecture is selected based on a cost-to-accuracy ratio. Finally, the correlation levels of the trained model are presented, and the two trust-level methods are put into practice. Finally, future perspectives are given. The base code used in this paper is available at https://github.com/jviquerat/twin_autoencoder (see Sect. 1 for additional details).

Dataset construction
This section provides insights on the random shape dataset generation used to train networks in the next sections. This dataset was initially used in [8] (section 3.5), thus only the main lines are sketched here. For more details, the reader is referred to [8]. First, we describe the steps to generate arbitrary shapes by means of connected Bezier curves. Then, solving of the Navier-Stokes equations with an immersed method is presented. Finally, details about the dataset are given.

Random shape generation
In the first step, 𝑛𝑠 random points are drawn in [0,1]2, and translated so their center of mass is positioned in (0, 0). An ascending trigonometric angle sort is then performed (see Fig. 2a), and the angles between consecutive random points are then computed. An average angle is then computed around each point (see Fig. 2b) using:

𝜃∗𝑖=𝛼𝜃𝑖−1,𝑖+(1−𝛼)𝜃𝑖,𝑖+1,
with 𝛼∈[0,1]. The averaging parameter 𝛼 allows to alter the sharpness of the curve locally, maximum smoothness being obtained for 𝛼=0.5. Then, each pair of points is joined using a cubic Bézier curve, defined by four points: the first and last points, 𝑝𝑖 and 𝑝𝑖+1, are part of the curve, while the second and third ones, 𝑝∗𝑖 and 𝑝∗∗𝑖, are control points that define the tangent of the curve at 𝑝𝑖 and 𝑝𝑖+1. The tangents at 𝑝𝑖 and 𝑝𝑖+1 are respectively controlled by 𝜃∗𝑖 and 𝜃∗𝑖+1 (see Fig. 2c). A final sampling of the successive Bézier curves leads to a boundary description of the shape (Fig. 2d). Using this method, a wide variety of shapes can be attained, as shown in Fig. 3.

Fig. 2
figure 2
Random shape generation with cubic Bézier curves

Full size image
Fig. 3
figure 3
Shape examples drawn from the dataset. A wide variety of shape is obtained using a restrained number of points (𝑛𝑠∈[4,6]), as well as a local curvature r and averaging parameter 𝛼

Full size image
Numerical resolution of the Navier-Stokes equations
The flow motion of incompressible newtonian fluids is described by the Navier-Stokes (NS) equations:

{𝜌 (∂𝑡𝐯+𝐯⋅∇𝐯)−∇⋅(2𝜂𝜖𝜖(𝐯)−𝑝𝐈)∇⋅𝐯=𝐟,=0,
(1)
where 𝑡∈[0,𝑇] is the time, 𝐯(𝑥,𝑡) the velocity, p(x, t) the pressure, 𝜌 the fluid density, 𝜂 the dynamic viscosity and 𝐈 the identity tensor. In order to efficiently construct the dataset, an immersed volume method is used for resolution instead of the usual body-fitted method, avoiding a systematic re-meshing of the whole domain for each shape. This method rely on a unified fluid-solid Eulerian formulation based on level-set description of the geometry [21], and leads to the following set of modified equations:

{𝜌∗(∂𝑡𝐯+𝐯⋅∇𝐯)−∇⋅(2𝜂𝜖𝜖(𝐯)+𝜏𝜏−𝑝𝐈)∇⋅𝐯=𝐟,=0,
(2)
where we have introduced the following mixed quantities:

𝜏𝜏=𝐻(𝛼)𝜏𝜏s,𝜌∗=𝐻(𝛼)𝜌s+(1−𝐻(𝛼))𝜌f,
where the subscripts f and s respectively refer to the fluid and the solid, and 𝐻(𝛼) is the Heaviside function:

𝐻(𝛼)={10 if 𝛼>0, if 𝛼<0.
(3)
The reader is referred to [22] for additional details about formulation (2). Eventually, the modified Eq. (2) are cast into a stabilized finite element formulation, and solved using a variational multi-scale (VMS) solver [23,24,25,26,27].

Dataset
The dataset is composed of 12.000 shapes, along with their steady-state velocity and pressure fields at Re=10 (see Fig. 4). All the labels were computed using CimLib-CFD [22], following the methods exposed in Sect. 2.2. The input fields are resized to 2D 100×150 arrays before being provided to the network. As is customary in neural networks training, a channel-wise normalization is applied, mapping all the pixels’ value into [0, 1]. In following sections we use a color scale to visualize the velocity and pressure fields, but each of them is always a 2D array. For additional details about the distribution of the elements in the dataset, the reader is referred to [8] (section 3.5).

Fig. 4
figure 4
Network input, velocity field, and pressure field for a dataset element. The shape is shown in its computational domain (upper left), along with the computed velocity field (top right, lower left), and pressure field (lower right)

Full size image
Network architecture and training
Twin-decoder architecture
The general autoencoder architecture with twin-decoder proposed in this contribution is shown in Fig. 5. Its input consists in a boolean 1-channel 2D tensor containing the computing domain and the obstacle. Its outputs are (i) a 1-channel 2D tensor containing the reconstructed input, and (ii) a 3-channels tensor containing the predicted velocity components and pressure fields. The encoder branch consists in stacked convolution-convolution-max-pooling blocks using 3×3 kernel size, stride size equal to 1, and zero-padding. The convolutional layers exploit rectified linear unit (ReLU) as activation functions. After every pooling operation, the number of kernels used for convolutional layers is doubled, until reaching the bottleneck.

The decoder is composed of two branches, hereafter denoted by “shape decoder” and “flow decoder”. Both decoder branches are composed of deconvolution-convolution-convolution blocks, and share similar structures. The deconvolution layers use 2×2 kernel size, stride size equal to 2, zero padding, and ReLU activation. Symmetrically to the encoder branch, the number of kernels is halved after each block in the decoder branches. Finally, a convolutional layer with a 1×1 kernel is applied to set the final number of channels (1 for the shape decoder, and 3 for the flow decoder). The output of our network hence contains 1-channel tensor representing reconstructed input, and a 3-channel tensor representing the velocity and pressure.

The key ingredient of the proposed architecture lies in the skip connections that link the shape decoder and the flow decoder. The output of each shape decoder block is concatenated (along the channel axis, to preserve shape) to the output of the deconvolution layer of each flow decoder block. This idea is similar to that of U-net, except that low-level features do not originate from the encoder branch but from the reconstruction of the input. Forcing such dependence between the two decoder branches is expected to induce a strong correlation between their performance levels. In essence, by enforcing a relation between the shape reconstruction error and the flow prediction error, the proposed method allows to reject possible outliers based on the reconstruction error. As the latter can be computed for an arbitrary input, the end-user can be warned whether the network prediction can be trusted or not. More details on the acceptance/rejection procedure are provided in Sect. 3.3.

Fig. 5
figure 5
Proposed twin-decoder architecture. The encoder is based on a pattern made of two convolutional layers followed by a max-pooling layer. At each occurence of the pattern, the image size is divided by two, while the number of filters, noted m, is doubled. In both decoder paths, a transposed convolution step is first applied to the input, while the number of filters is halved. The output of this layer in the flow decoder is then concatenated with its mirror counterpart in the shape decoder. Finally, two convolution layers are applied. At the end of the last layer, a 1×1 convolution is applied on each decoder to obtain a final 3D tensor with 4 channels. Every channel has the same dimension as the input

Full size image
Training procedure
The loss function used to train the twin-decoder architecture is a weighted sum of the shape and the flow decoder losses. Both decoders use the regular mean squared error (MSE) as loss function:

𝐿=1ℎ𝑤𝑛𝑐∑𝑑,𝑖,𝑗(𝑦𝑑,𝑖,𝑗−𝑦̂ 𝑑,𝑖,𝑗)2,
(4)
where h, w and 𝑛𝑐 represent respectively channel height, width and the number of channels. Expression (4) represents the average squared error over all the pixels of a 3D tensor. The final loss function used for training is:

𝐿twin=𝐿flow+𝛽𝐿shape,
(5)
where 𝛽 is a weighting parameter that remains to be tuned (see Sect. 4). The network is trained with the Adam optimizer using an initial learning rate of 1×10−3 , which is reduced to 1×10−4 after 600 epochs. To prevent overfitting, the validation loss is monitored, and early stopping is used to determine the end of the training. The network parameters are initialized using a truncated Guassian distribution, following [17]. Training is performed using a Tesla V100 GPU, using mini-batches of size 128 to limit the required computational resources. As different models are evaluated and caompared, their training times are given in Sect. 4.

Trust level based on shape reconstruction
Given a trained twin-decoder neural network, it is feasible to evaluate the trust level of flow prediction by input reconstruction. As the error levels of the twin decoders are strongly correlated, a qualitative and a quantitative trust-level methods are proposed, both based on the reconstruction error. Their general concepts are proposed in the following sub-sections, while their applications on trained networks are presented in Sect. 4.4. In the following, the MSE for flow and shape reconstructions is respectively denoted 𝑒𝑓 and 𝑒𝑠.

Qualitative method
In this case, the end-user provides an acceptable MSE level 𝑒∗𝑓 on the flow prediction. The method consists in selecting the associated shape reconstruction error 𝑒∗𝑠 that minimizes the probability of taking wrong decisions when supposing that the two error levels are linearly correlated. The threshold shape reconstruction error 𝑒∗𝑠 is the solution to the following minimization problem:

𝑒∗𝑠=1𝑁min𝑒[card(𝑒𝑠<𝑒 and 𝑒𝑓>𝑒∗𝑓)+card(𝑒𝑠>𝑒 and 𝑒𝑓<𝑒∗𝑓)],
(6)
where N is the number of MSE scatter points (𝑒𝑠,𝑒𝑓) taken into account. The numerator sums the amount of wrong decisions as the error of both decoders are assumed to be positively correlated. Such a formulation prevents the end-user from making mistakes when accepting or rejecting predictions. An illustration of the method is shown in Fig. 6a.

Fig. 6
figure 6
Description of the qualitative and quantitative methods on the scatter plots of 𝑒𝑓 versus 𝑒𝑠 on training and validation sets for a reference twin-decoder architecture. (Left) Qualitative method: given a threshold 𝑒∗𝑓, the corresponding optimal 𝑒∗𝑠 is found by solving problem (6). Then, the end-user rejects predictions that produce shape reconstruction errors superior to 𝑒∗𝑠. Only the predictions falling within the bottom left quarter (in orange) are accepted. (Right) Quantitative method: 𝑒𝑓 is modeled as an affine function of 𝑒𝑠 with an uncertainty interval, as shown in relation (7). Based on 𝑒𝑠, the method provides an estimated 𝑒𝑓 level, along with a confidence interval for the prediction

Full size image
Quantitative method
With the quantitative method, 𝑒𝑓 and an associated confidence interval 𝛿𝑓 are directly estimated from 𝑒𝑠 using the following relation:

𝑒𝑓=𝑎𝑒𝑠+𝑏+𝜖,
(7)
where 𝜖 follows a normal law of the form:

𝜖∼(0,(𝑐𝑒𝑠)2).
(8)
The assumption is supported by the linear pattern of error scatter plots. In essence, indexing the standard deviation on 𝑒𝑠 in (8) represents the growing uncertainty on flow prediction as shape reconstruction turns worse. Under this formulation, the likelihood of 𝑒𝑓 on N scatter points is:

∏𝑖=1𝑁𝑝(𝑒𝑖𝑓|𝑒𝑖𝑠;𝑎,𝑏,𝑐)=∏𝑖=1𝑁12𝜋(𝑐𝑒𝑖𝑠)2‾‾‾‾‾‾‾‾√exp(−(𝑒𝑖𝑓−𝑎𝑒𝑖𝑠−𝑏)22(𝑐𝑒𝑖𝑠)2)
(9)
The optimal parameters 𝑎∗, 𝑏∗ and 𝑐∗ are obtained by minimizing the negative log likelihood:

(𝑎∗,𝑏∗,𝑐∗)=min𝑎,𝑏,𝑐12∑𝑖(𝑎𝑒𝑖𝑠𝑒𝑖𝑓−𝑏𝑐𝑒𝑖𝑠)2+∑𝑖log(𝑐𝑒𝑖𝑠)+𝑁2log(2𝜋),
(10)
which is achieved using a gradient descent algorithm. An illustration of the method is shown in Fig. 6b. The minimum is determined by the scatter points of the training and validation error, then evaluated on the test set. To the difference of the qualitative method, in which the predictions are either plainly accepted or rejected, such formulation allows one to estimate 𝑒𝑓 with a confidence interval 𝛿𝑓, without a need of pre-selecting a threshold accuracy, thus providing an additional flexibility. As an example, given a shape reconstruction error 𝑒𝑠, the associated flow prediction accuracy level 𝑒𝑓 would fall into [(𝑎−2𝑐)𝑒𝑠+𝑏,(𝑎+2𝑐)𝑒𝑠+𝑏] with 95% probability.

Results
In this section, the performance of the twin-decoder architecture is explored. First, a minimal hyper-parameter calibration is presented that highlights the impact of the loss weighting parameter 𝛽, the number of convolutional blocks used in the decoder structure, and the number of kernels used in the convolutional layers. The resulting architecture is evaluated on the test set, showing good performance on unseen shapes. Then, the qualitative and quantitative methods respectively presented in Sects. 3.3.1 and 3.3.2 are put into practice. Finally, a comparison with two other AE-based architectures is proposed, revealing the contribution of skip connections between shape and flow decoders.

Hyper-parameter calibration
In this section, the impact of three different parameters is explored: (i) the weighting parameter 𝛽, (ii) the number of convolutional blocks, and (iii) the number of convolutional kernels. In total, 30 configurations were tested, the network being evaluated not only on its flow field prediction performance, but also on the correlation coefficient between 𝑒𝑓 and 𝑒𝑠 on the validation set. As the different explorations are detailed below, the corresponding results can be found in Fig. 7.

Convolution blocks
The number of convolution blocks in the encoder (or equivalently the number of deconvolution blocks in the decoders) proved to be determinant regarding the performance of the flow decoder. Based on previous experiments (not shown here), only architectures with 5 or 6 blocks were considered (less blocks showing low performance, while more blocks being too costly to train). Architectures with 5 blocks proved to outperform deeper ones in terms of flow prediction, probably due to a too high compression rate in the latent space when using 6 blocks. Adversely, architectures with 6 blocks presented a slightly better correlation coefficient between the two decoder errors.

Convolution kernels
The flow prediction accuracy was highly dependent on the number of convolution kernels used in each blocks. Since the architecture is symmetric and scalable, we use the number of kernels in the first convolutional layer to represent this hyper-parameter. Results show a clear advantage of using 8 kernels over 4, while increasing from 8 to 12 is not as beneficial. Hence, it is not necessary to use too many kernels, as a CNN complexity scales with the square of the kernel number. Similar conclusions hold for the correlation coefficient.

Weighting parameter
The 𝛽 parameter in the loss function (5) proved to have a crucial impact on correlation of the two decoder errors. Five different values were considered (from 0.1 to 5), with larger values giving more weight to the shape decoder during the training process. We found that small 𝛽 values were very beneficial to the correlation level, while alleviating the differences caused by different number of convolutional blocks. With 𝛽=0.1, a 5-block architecture with 12 kernels in the first convolutional layer obtained a correlation level of 93%, which represents a strong linear relation. The impact of 𝛽 on the flow prediction accuracy is not as clear according to the obtained results.

Fig. 7
figure 7
Hyper-parameter calibration. The performance is evaluated on the validation set. To compare the accuracy of flow prediction between different architectures, the MSE is averaged over the entire dataset

Full size image
Prediction cost and accuracy
In order to evaluate the computational cost of the training, we provide the best performance attained by each combination with respect to the number of learnable parameters in Table 1. Each point represents the smallest 𝑒𝑓 value obtained by tuning 𝛽. By using the 5-block architecture with 12 kernels, 𝑒𝑓 values as low as 1.4×10−5 can be reached, requiring 1.4 million parameters, making it architecture choice with the best cost-accuracy ratio. Learning time on a Tesla V100 GPU is approximately 0.7 hours. The training curves for the training and validation subsets are presented in Fig. 8.

Fig. 8
figure 8
Training history of the model. The training is ceased when the validation loss does not decrease for 10 successive epochs. A slight overfitting can be observed on the trained model

Full size image
Table 1 Flow prediction performance obtained for architectures of various complexities
Full size table
On the test set, the proposed model reaches a flow reconstruction accuracy 𝑒𝑓=1.38×10−5, thus showing good generalization capabilities on unseen data. In Fig. 9, a prediction example from the test set is shown, along with the associated exact solution and a pixel-wise error map. As can be seen, the velocity and pressure fields are well recovered by the network, the error being concentrated in the vicinity of the obstacle, i.e.  in the area of large pressure and velocity gradients. As the obstacle itself only represents a small portion of the predicted field, a second metric is proposed: for each element of the test set, the mean pixel-wise relative error is computed on a smaller area surrounding the obstacle. The corresponding error distributions are plotted in Fig. 10. Overall, the average relative error is 3.92% for horizontal velocity, 3.57% for vertical velocity, and 3.55% for pressure, with very few elements exceeding 6% of relative error, showing again decent generalization capabilities.

Fig. 9
figure 9
Flow and shape predictions around an obstacle from the test set. On this instance, the flow prediction error is 𝑒𝑓=1.57×10−5, with most of the error concentrated on the boundary of the shape, i.e.  in the area of large pressure and velocity gradients. For the u, v and p predictions, the pixels’ value range is still [0, 1]. An RGB colormap is used for better visualization

Full size image
Fig. 10
figure 10
Relative error for flow predictions over test set. The black rectangle around the obstacle indicates the area on which the u, v and p relative errors are computed. The histograms indicate the error levels obtained when comparing predictions to labels on the 1200 elements of the test set

Full size image
Correlation levels
In this section, the correlation levels obtained between the shape reconstruction and the flow prediction errors are commented. To further show the interest of the autoencoder architecture with twin-decoder (twin-AE), two close network architectures are considered, namely the dual autoencoder (dual-AE), and the U-net dual autoencoder (U-dual-AE). The dual-AE architecture is simply obtained by removing the skip connections of the twin-AE, while the U-dual-AE exploits skip connections coming from the encoder path instead of the reconstruction path. To ensure a fair comparison, the same configuration is used for all three architectures. A concatenation with constant tensor is applied to the flow decoder of the dual-AE, so its number of parameters is equal to that of the U-dual-AE and the twin-AE.

The scatter plots obtained with the twin-AE on the training, the validation and the test sets are respectively shown in Fig. 11a, b, and c. Associated correlation levels are 0.772, 0.931, and 0.954, meaning that strong linear relations are observed on the validation and test set. Regarding the training set, the weaker correlation is interpreted as a consequence of the slight overfitting observed during training (see Fig. 8). In comparison, the obtained correlation level of the dual AE on the test set is 0.830, which is significantly weaker than that of the twin AE, thus proving the interest of the skip connections between the two decoder branches (see Fig. 11d). The twin-AE also has slightly lower relative errors than its dual-AE counterpart (3.92%, 3.57% and 3.55% respectively for u, v, p, against 4.30%, 4.05%, and 4.00%). Finally, the U-dual-AE architecture exhibits almost no correlation between 𝑒𝑠 and 𝑒𝑓, with a computed correlation level of 0.385 (see Fig. 11e). Adversely, the relative error levels of the U-dual-AE are significantly lower (3.01%, 1.94%, and 1.94%), which is in line with results from the literature [28].

Fig. 11
figure 11
Comparison of scatter plots of 𝑒𝑓 versus 𝑒𝑠 for different sets and architectures. Top row: the correlation levels obtained with the twin AE on the training, validation, and test sets are respectively 0.772, 0.931, and 0.954. Bottom row: dual AE and U-dual AE architectures show weaker correlation levels on the test set, with respective levels of 0.830 and 0.385

Full size image
Trust level based on input reconstruction
This section presents the application of the trust level methods detailed in Sect. 3.3. Again, the underlying concept is to take advantage of the strong correlation between shape and flow reconstruction error levels (see Sect. 4.3) to propose an uncertainty estimation (either qualitative or quantitative) along with the flow prediction.

Qualitative method
A threshold mean squared error tolerance for the flow reconstruction is provided by the user, and is here chosen to be 𝑒∗𝑓=5×10−5. The corresponding threshold shape reconstruction error 𝑒∗𝑠 is obtained by solving the minimization problem (6). As the dataset size is relatively limited, the problem is solved by an exhaustive search, the results of which are shown in Fig. 12a. One observes that choosing 𝑒∗𝑠=1.9×10−4 minimizes the risks of accepting bad predictions and rejecting good predictions. When testing the procedure on the elements of the validation and testing sets, it is observed that the mistake rate is close to 1% in both cases. In Fig. 12b, the false classification rate on the three subsets is plotted as a function of 𝑒∗𝑓, showing that stricter 𝑒∗𝑓 choices inevitably lead to worse performances with this method. The area of accepted predictions is plotted in Fig. 14a, along with a representation of the elements of the test set.

Fig. 12
figure 12
Trust level identification based on the 𝑒𝑠 indicator. (Left) The optimal threshold is obtained by minimizing the mistaken classification rate on the training and the validation sets. (Right) The mistake rate rises significantly on all three subsets when decreasing the threshold 𝑒∗𝑓 value. For the optimal 𝑒∗𝑠 value, the mistake rate on the validation and test sets is approximately 1%

Full size image
Quantitative method
A gradient descent algorithm is used to minimize the negative log-likelihood problem (10) over the training set, in order to obtain the optimal parameter set (𝑎∗,𝑏∗,𝑐∗). With an initial value (𝑎0,𝑏0,𝑐0)=(0.1,0,0.1), the algorithm converges after 6 iterations (see Fig. 13). The optimal parameters retained are (𝑎∗,𝑏∗,𝑐∗)=(0.257157,2.24820×10−6,0.105841), which minimize the negative log likelihood over the validation set. Hence, 𝑒𝑓 can be estimated from 𝑒𝑠 as:

𝑒𝑓=0.257157𝑒𝑠+2.24820×10−6+(0,(0.105841𝑒𝑠)2).
(11)
As shown in Fig. 14b, the regression line and its 1𝜎 confidence interval matches well with the distribution of the test set, with only a handful of (𝑒𝑓,𝑒𝑠) couples falling outside of the range. The fact that the confidence interval widens with larger values of 𝑒𝑠 translates the increasing scarcity of samples in the test set when 𝑒𝑠 rises. For the majority of predictions, though, it provides a good grasp of the flow prediction quality. The 1𝜎 (68% probability) and 2𝜎 (95% probability) confidence intervals for sampled 𝑒𝑠 values are provided in Table 2.

Fig. 13
figure 13
Minimization of the negative log-likelihood problem (10) using the BFGS algorithm

Full size image
Fig. 14
figure 14
Representations of the qualitative and quantitative methods along with the test set scatter plot

Full size image
Table 2 Estimating 𝑒𝑓 for given 𝑒𝑠 values
Full size table
Flow prediction on outliers
In this section, the capabilities of the qualitative and quantitative methods to detect invalid inputs and outliers are evaluated. To do so, multiple shapes are generated that do not fit within the dataset, including polygons with sharp edges (see Fig. 15), shapes included in the dataset but misplaced in the input domain (i.e.  moved away from the position used in the dataset), and enlarged shapes from the dataset. In total, 120 outliers are tested. In Table 3, one can see that the relative errors on the different classes of outliers are systematically larger than those obtained on the dataset shapes. While the prediction errors on polygons are only slightly higher that those from the test set, field prediction errors for misplaced shapes are systematically superior to 10%. Enlarged shapes present extremely high reconstruction and prediction errors, higher than 100%. An example of prediction on enlarged Bezier shape is shown in Fig. 17, illustrating the interest of incorporating uncertainty estimation and outlier detection processes in neural network architectures.

In Fig. 16b, the (𝑒𝑠,𝑒𝑓) scatter plot of the test set is shown, along with the position of the 120 outliers, both for the qualitative and the quantitative methods. As can be seen, most polygons are located in the accepted region of the qualitative method, indicating that the higher average relative error shown in Table 3 is caused by a few polygon outliers with large relative errors. When inspecting the polygons set, it was found out that those with largest 𝑒𝑓 errors were presenting edges features that were considerably sharper than the others. Almost all the misplaced and enlarged shapes fall into the rejected area of the qualitative method, while also matching well the ±2𝜎 interval of the quantitative method (27 enlarged shapes out of 30 are not shown due to their out-of-range MSE.). Still, a handful of outliers presenting low 𝑒𝑠 with large 𝑒𝑓 are noticed, indicating that the proposed methods are still missing a small amount of outliers. Overall, the qualitative method efficiently detects most of the outliers, with only 10 out of 120 bad predictions not detected (i.e.  91.6% of outliers detected). The ±1𝜎 interval of the quantitative method covers 85 out of 120 (70.8%) outliers, while ±2𝜎 interval covers 106 out of 120 (88.3%) outlier predictions. The results of Fig. 16b also indicate that the uncertainty levels of misplaced and enlarged inputs are partly under-estimated.

Overall, the qualitative method efficiently diminishes the risk of mis-use of the trained model, by catching more than 90% of the bad inputs. Still, it remains a limited, binary method, and by construction approximately discards 1% of the dataset points. Conversely, the quantitative method also provides adequate confidence range for almost all the elements from the test set, and for nearly 90% of the outliers. However, the provided error intervals for inputs leading to very large 𝑒𝑓 errors are underestimated. Finally, similarly to the qualitative one, the quantitative method cannot account for a handful of points presenting large 𝑒𝑓 values in conjunction with small 𝑒𝑠 values.

Fig. 15
figure 15
Outlier examples for model evaluation. Polygons are generated with fewer sampling points and sharper edges than the dataset shapes. Misplaced and enlarged shapes have the same curve characteristics as the original data set but are ill-positioned, or significantly larger than those of the dataset

Full size image
Table 3 Model performance on test set and polygons
Full size table
Fig. 16
figure 16
Prediction and reconstruction error on the outliers. The qualitative and quantitative methods are illustrated in a and b

Full size image
Fig. 17
figure 17
Flow and shape predictions around an enlarged Bezier shape. As this input shape is an outlier, the shape reconstruction is poor, and is associated with large prediction errors (𝑒𝑓=1.316×10−2). For the u, v and p predictions, the color scales are the same as for Fig. 4

Full size image
Conclusion
In the present contribution, a twin-AE architecture for 2D incompressible laminar flow prediction with embedded uncertainty estimation was presented. The underlying motivation was to propose a method to naturally incorporate outlier detection and uncertainty estimation in the training procedure, in order to provide a decisional tool to the potential end-user. The embedded uncertainty estimation relies on the coupling of the autoencoder for flow prediction with a second autoencoder for input reconstruction, using well-chosen skip connections. Doing so naturally enforces a quasi-linear relation between the flow prediction error and the input reconstruction error. Building on this particular trait, simple yet effective qualitative and quantitative techniques were proposed to detect outliers and provide uncertainty prediction on any input provided to the trained network.

The proposed architecture was trained on a dataset of 12000 laminar flows around random 2D shapes, generated using Bezier curves. After hyper-parameter calibration, the correlation coefficient between the reconstruction error and flow prediction error reached 0.95 on the test set. The two methods were then tested on true outliers presenting different flaws (polygonal shapes that did not belong to the dataset, shapes from the dataset misplaced in the input domain, shapes significantly larger than those of the dataset), and proved efficient to either reject shapes with high flow prediction errors, or provide adequate uncertainty range. Still, a handful of outliers remained undetected, or their associated uncertainty range was under-estimated. Possible improvements could be brought to these methods, either by improving the network architecture to improve the flow error/reconstruction error correlation level, or by gaining more control on the dataset generation, in order to avoid the inclusion of possible outliers.

These results underline the potential of the proposed approach. Indeed, the implementation of such methods in prediction tasks can significantly lower the risk of the end-user taking decisions based on network predictions using inadequate inputs. Efforts shall be pursued for more accurate input reconstruction. From the experiments on U-Dual-AE, low-level features from its encoder bring great benefits to flow prediction. If the shape decoder of twin-AE provides equivalently beneficial features, we expect an improved flow prediction while keeping the strong correlation between its twin decoders.

Keywords
Neural networks
Autoencoders
Anomaly detection
Computational fluid dynamics
Surrogate model