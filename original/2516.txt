The extreme learning machine (ELM) is a method to train single-layer feed-forward neural networks that became popular because it uses a fast closed-form expression for training that minimizes the training error with good generalization ability to new data. The ELM requires the tuning of the hidden layer size and the calculation of the pseudo-inverse of the hidden layer activation matrix for the whole training set. With large-scale classification problems, the computational overload caused by tuning becomes not affordable, and the activation matrix is extremely large, so the pseudo-inversion is very slow and eventually the matrix will not fit in memory. The quick extreme learning machine (QELM), proposed in the current paper, is able to manage large classification datasets because it: (1) avoids the tuning by using a bounded estimation of the hidden layer size from the data population; and (2) replaces the training patterns in the activation matrix by a reduced set of prototypes in order to avoid the storage and pseudo-inversion of large matrices. While ELM or even the linear SVM cannot be applied to large datasets, QELM can be executed on datasets up to 31 million data, 30,000 inputs and 131 classes, spending reasonable times (less than 1 h) in general purpose computers without special software nor hardware requirements and achieving performances similar to ELM.

Introduction
The extreme learning machine (ELM) is a neural network model [21] related to the SVM [2, 31] that has been widely applied for classification, regression and time series prediction. Several variants of ELM have been proposed in the literature, such as twin ELM [42] for classification, that uses two non-parallel hyperplanes that simultaneously minimize the distance to one class while keeps away from the other class. The online sequential ELM (OS-ELM) allows to learn pattern-by-pattern or chunk-by-chunk [30] and has also been combined in ensembles [27]. In [50], the augmented OS-ELM is used for classification and regression of noncircular quaternion signals that provide a convenient way to represent 3D and 4D signals. Other ensembles of ELM have been proposed in [39], using negative correlation learning, and [34], a committee of voting ELMs trained with different bootstrap training samples for road lane landmark detection. The problems of ELM with low column rank data are solved by the effective ELM [44] that has been successfully applied to image classification [5]. Low-rank matrix factorization is also used in the ELM autoencoder [38] to learn optimal low-dimensional features for the same application. The weighted ELM [51] is oriented to classification of imbalanced data that has been applied in [20] for discriminative data clustering alongside with linear discriminant analysis and K-means, a clustering method that was used with ELM in [32] to forecast sales of computer servers. The ELM has also been combined with other paradigms like fuzzy logic, used in [17] to find the optimal ELM hyper-parameters (size of the hidden layer), and particle swarm optimization (PSO) for feature selection and hidden layer size estimation in the sleep stage classification over electrocardiogram signal [41]. The ELM and PSO are also combined in [28] with boosting for electric consumption time series forecasting. In [25], the ELM is applied instead of genetic algorithms for symbolic regression in system identification. Deep neural networks have also been combined with ELM by using the recursive deep arc-cosine kernel [1]. Regarding hardware implementations, [37] describes a neuromemristive circuit architecture for ELM, and [29] compares experimentally several implementations on different hardware devices.

The input weights of ELM, connecting the input and hidden layers, are set randomly, while the output weights are calculated by multiplying the target matrix (class labels in classification problems) by the pseudo-inverse of the matrix with the outputs of the hidden neurons. The number of hidden neurons is relevant because the more training patterns, the more hidden neurons are required to achieve a good performance. This number is a hyper-parameter that must be tuned in order to achieve a good performance. Often, its value is selected from a collection of pre-defined values as the one with the best ELM performance (grid-search approach), although alternative strategies such as fuzzy logic [17] and particle swarm optimization [41] have been also proposed to select the optimal hidden layer size. The simplicity makes the ELM very fast in small and medium datasets, but it is slow or unable to classify large datasets with many training patterns, because of two reasons: (1) the tuning of the hidden layer size by grid-search requires to train and test the ELM for each hyper-parameter value, either using grid-search or other optimization methods; and (2) the calculation of the pseudo-inverse of the hidden layer activity matrix may be very slow for large-scale datasets, in fact this matrix eventually may not fit in the available memory. In order to solve these problems, the current paper proposes the quick extreme learning machine (QELM) for classification problems that is able to train and test using large-scale datasets. The paper is organized as follows: Sect. 2 lists the research about ELM for large-scale datasets. Section 3 describes the materials and methods, including the ELM network (Sect. 3.1), the estimation of the hidden layer size in QELM (Sects. 3.2 and 3.3), the output weight calculation (Sect. 3.4) and the whole QELM algorithm (Sect. 3.5). Results are reported and discussed in Sect. 4, and conclusions are compiled in Sect. 5.

Related work
The literature reports several works that use ELM with large-scale datasets [6]. The study [43] uses the bag of little bootstraps technique to reduce the size of the training datasets and alleviate the computational overhead of a bagging ensemble of ELMs for large-scale datasets. In [46], the symmetric ELM cluster is evaluated for traffic congestion prediction, transforming a large-scale problem (up to 5 million training patterns) in several sub-problems of small- and medium-size datasets. In [15], feature selection is used to reduce the data size (up to 58,000 patterns and 60 features), identifying the most relevant features by ranking them with the coefficient obtained through ELM divided by the variation coefficient. The regularized ELM [22] was applied to large-scale image classification problems up to 250,000 training patterns, 1770 inputs and 340 classes, spending 65 s. for training on a 4-core computer with CPU i7-3630, 2.4 GHz and 8 GB RAM in 2015. In [23], the approximated kernel ELM was evaluated on middle-sized datasets (up to 14,000 training patterns).

The fast singular value decomposition (SVD)-hidden-nodes-based ELM (FSVD-H-ELM) replaces the random initialization of the input weights by the use of SVD to multiple random subsets sampled from the original dataset [12]. The SVD is much slower than random generation, in fact is approximately so slow as matrix pseudo-inverse calculation. Besides, SVD requires more memory than pseudo-inverse, and thus, it can only be applied to small matrices. Finally, the matrix pseudo-inverse continues to be calculated, so globally FSVD-H-ELM is more complex than ELM. The overload introduced by SVD is only compensated by the random selection of training patterns that keeps small the matrices to which SVD is applied. This work uses a fast divide-and-conquer scheme to keep tractable the computational complexity on high volume data, up to 20 million patterns and 30 million inputs (dataset KDD2010), spending 1769 s per fold (about 30 min) on a powerful computer with Intel Xeon E5-2650 2 GHz CPU (32 cores), 256GB RAM and Matlab R2013a in 2016. However, in dataset Webspam, with only 350,000 patterns and 16 million inputs, the training time is 8,154 s./fold (about 2.5 h), so the time does not only depend on the dataset size, but also on other properties of the dataset.

The review [19] reports recent applications of ELM in the big data context. The papers [14] and [18] describe parallel implementations of ELM based on MapReduce and Spark, respectively, the latter being able to classify 38 million patterns with 8 inputs on a computer cluster of 10–35 nodes. The elastic ELM [45] uses MapReduce to speed up the matrix products through incremental, decremental and correctional calculations, being applied to synthetic datasets until 10 million patterns but only 50 inputs. Another MapReduce-based approach [49] uses an ELM ensemble for large-scale imbalanced classification datasets with two classes up to 300,000 patterns and class imbalance ratios up to 2000. The work [4] presents a complete ELM toolbox for big data applications on Matlab and Python. An ELM trained with 19,000 hidden nodes on this toolbox spent 15 days and 5 h in year 2015 to process a dataset of 500 million patterns with 147 inputs, on a powerful workstation of 4-core 4 GHz CPU with 256–512 GB RAM per core and specific hardware (GPU acceleration). In [33], a ELM with rank-reduced matrix is proposed to decrease the size of the hidden layer, speeding the training while raising the performance on datasets up to 29,000 training patterns. The E-OS-ELM [48] avoids the oscillations of OS-ELM in different trials spending about 1 h to process large-scale datasets until 4.8 million patterns and 54 inputs. The feature-bagged ELM [24] is another variant that uses a combination of ensembles and feature bagging to run over computer systems with severe memory constraints.

The previous approaches apply ELM on large-scale datasets, but in some cases the datasets are not so large, and in others the gain on speed is mainly based on the use of big data technologies (MapReduce and Spark) or specific hardware (parallelization, GPU acceleration and powerful computing capabilities). The goal of the current paper is to propose an ELM-based method than can be executed on classification datasets with arbitrarily large populations over general-purpose computers with no specific hardware nor software, such as big data technologies. Thus, our contribution is designed to allow the execution of ELM on large datasets while keeping its simplicity and speed, as described in Sect. 3.

Fig. 1
figure 1
Diagram of the ELM neural network for classification during training. During test, 𝐓𝐶×𝑁 is removed, while 𝐗𝑁×𝐼 and 𝐕𝐶×𝑁 are replaced by 𝐒𝑃×𝐼 and 𝐕𝐶×𝑃, respectively

Full size image
Materials and methods
First, we introduce the notation associated to the ELM network (refer to Fig. 1). Let Q be the total number of patterns, being N and P the numbers of training and test patters, respectively, so that 𝑄=𝑁+𝑃. Let {𝐱𝑛}𝑁𝑛=1 be the N training patterns of dimension I (number of inputs of the ELM network) composing the matrix 𝐗, of order 𝑁×𝐼. Given the number of classes C of the classification problem, let 𝑤𝑛∈{1,…,𝐶}, with 𝑛=1,…,𝑁, be the true class label of 𝐱𝑛, composing the N-dimensional vector 𝐰=(𝑤1,…,𝑤𝑁). Let H be the number of neurons in the hidden layer of the ELM, and {𝑎ℎ𝑖}𝐻,𝐼ℎ𝑖=1 the weights of the hidden neurons (input weights)Footnote1, composing the matrix 𝐀𝐻×𝐼. Let {𝑢ℎ𝑛}𝐻,𝑁ℎ𝑛=1 be the inputs of the hidden neurons, composing the matrix 𝐔𝐻×𝑁, and {𝑦ℎ𝑛}𝐻,𝑁ℎ𝑛=1 the activities (or outputs) of the hidden neurons, composing the matrix 𝐘𝐻×𝑁. Let {𝐛𝑐ℎ}𝐶,𝐻𝑐ℎ=1 be the weights connecting the hidden and output layers (output weights), composing the matrix 𝐁𝐶×𝐻, where the output layer of the ELM has C neurons, one for each class. The desired outputs for the output neurons are {𝑡𝑐𝑛}𝐶,𝑁𝑐𝑛=1 given by 𝑡𝑐𝑛=𝛿(𝑐,𝑤𝑛)=1, where 𝛿(𝑐,𝑤𝑛)=1 for 𝑐=𝑤𝑛 and 𝛿(𝑐,𝑤𝑛)=0 otherwise, composing the matrix 𝐓𝐶×𝑁. The P test patterns, of dimensionality I, are {𝐬𝑝}𝑃𝑝=1, and compose the matrix 𝐒𝑃×𝐼. Let 𝐕𝐶×𝑃 be the output of the last layer, and {𝑧𝑝}𝑃𝑝=1 the class labels predicted by the ELM for the test pattern 𝐬𝑝, composing the P-dimensional vector 𝐳=(𝑧1,…,𝑧𝑃).

Extreme learning machine
Given a training pattern 𝐱𝑛=(𝑥𝑛1,…,𝑥𝑛𝐼) with inputs 𝑥𝑛𝑖∼(0,1), with 𝑖=1,…,𝐼, i.e., standardized with zero mean and standard deviation one, the input 𝑢ℎ𝑛 of a hidden neuron h is 𝑢ℎ𝑛=∑𝐼𝑖=1𝑎ℎ𝑖𝑥𝑛𝑖, or in matrix form 𝐔=𝐀𝐗𝑇, where 𝐗𝑇 denotes the transposed of matrix 𝐗. The output of this neuron is given by 𝑦ℎ𝑛=𝑔(𝑢ℎ𝑛), being 𝑔(𝑢)=(1+e−𝑢)−1 the activation functionFootnote2. The matrix 𝐀 is defined with random numbers, whose values, given the activation function g(u), should verify |𝑎ℎ𝑖|<1/𝐼 in order to guarantee that 𝑢ℎ𝑛 is in an environment of 𝑢=0 for 𝑥𝑛𝑖∼(0,1). The output weight matrix 𝐁𝐶×𝐻 is calculated as 𝐁=𝐓𝐘†, where 𝐘† is the pseudo-inverse matrix of 𝐘. The trained ELM neural network is defined by the weight matrices 𝐀 and 𝐁, of sizes 𝐻×𝐼 and C × H, respectively, so the number of weights, and the network size, is 𝐻(𝐼+𝐶). Since I and C are given by the dataset, the size of the ELM neural network is completely determined only by the number H of hidden neurons. Algorithm 1 reports the pseudocode of the ELM network for classification.

figure a
Number of hidden neurons
The number H of hidden neurons is often tuned using the grid-search method, i.e., to use K-fold cross-validation with three datasets per fold: training, validation and test sets. For each fold, the ELM is trained using a value of H and its performance is evaluated on the corresponding validation set. The performance for this value of H is averaged over the K folds. The training–test cycle must be repeated for each value of H from a pre-specified collection, and the one with the best average performance is selected for testing. Then, for each fold the ELM using this value of H is trained over the training and validation sets, and tested on the test set. The process is repeated for the K folds, and the final test performance is the average over the K folds.

Alternative methods for model selection in ELM include [47] that uses pruning of the less significant hidden neurons from an initially large network, but this approach is relatively slow, spending about 5 minutes on the largest datasets considered (5600 patterns and 64 inputs). The paper [13], instead of tuning the network size, uses a large fixed value of H, pruning those neurons that are non-relevant or similar to other non-relevant neurons. The pruning is based on simulated annealing, and stops when the desired network size is achieved, without adjusting weights nor reducing performance. The complexity of this approach is high, spending about 300–900 s (5–15 min) on dataset adult with 39,047 training patterns. The method U-ELM [36] adds incrementally neurons to the hidden layer using a PSO-based multi-objective function that minimizes the uncertainty using a Riemann metric in order to select the optimal input weights. The addition of new neurons stops when a maximum number of iterations is reached or uncertainty does not change appreciably. This method is also relatively slow, spending about 20 s on a small dataset with 350 training patterns.

Selection of H in QELM
With large datasets, to use grid-search for hyper-parameter tuning is very slow because the ELM must be executed several times, depending on K (number of folds) and the number of H values tried. The literature (see, e.g., Tables 2 and 3 in [47] and Table 2 in [36]) has shown that the optimal H raises with the number N of training patterns. Since H is the number of rows of the hidden activity matrix 𝐘, large datasets with high N require matrices 𝐘 with many rows, that slows down the calculation of the pseudo-inverse of 𝐘. On the other hand, this calculation requires to store in memory the whole matrix 𝐘. However, increasing N, and consequently H, the matrix 𝐘 will eventually not fit in memory. Our proposal is to calculate H as an increasing function of N with an upper bound to keep limited the number of rows 𝐘 for large datasets. In order to estimate this function, we evaluated the behavior of the ELM performance with H over the small datasets of our collection (see Sect. 4) varying H from 𝐻=1 to 𝐻=𝑁 (100 values). The input weights {𝑎ℎ𝑖}𝐻,𝐼ℎ𝑖=1 are set to random values, so the classification performance has a certain degree of randomness. In order to reduce this random component, we use exactly the same weight matrix 𝐀0 for all the H values. This matrix has random values and is of order 𝑁∗×𝐼, where 𝑁∗ is the largest N over the small datasets. For each value of H, the matrix 𝐀 uses only the first H rows of 𝐀0, with H ranging from 𝐻=1 to 𝐻=𝑁, with 𝑁≤𝑁∗. In this way, the variability due to randomness is almost removed.

Fig. 2
figure 2
Value of H (in logarithmic scale) that achieved the best kappa over the small datasets (𝑄< 15,000 patterns) of the collection described in Sect. 4 versus the number N of training patterns (left panel) and vs. the number I of inputs (right panel), both in logarithmic scales

Full size image
Fig. 3
figure 3
Training (continuous red line) and test (dashed blue line) kappa achieved by ELM vs. H/N for three datasets that are illustrative of the performance behavior of ELM depending on H/N (upper and lower left panels). The lower right panel plots the average kappa over the small datasets (𝑄< 15,000 patterns) of the collection described in Sect. 4

Full size image
Figure 2 plots the best values of H against the number of training patterns and inputs. The left panel reports an increasing dependence between H and N; in fact, their correlation coefficient is 0.71, a value “moderate to good” according to the Colton scale [9]. However, the right panel does not reflect a clear dependence of H on I, with a “bad to moderate” correlation (0.32). Therefore, the value of H seems to depend more on N than on I. Figure 3 plots the classification performance, measured by the Cohen kappa score [7], for three datasets with slightly different behaviors for the performance in terms of H/N, and the average (lower right panel) over a wide dataset collection (see Sect. 4). The training performance raises with H/N until perfect classification, but the test kappa raises only for H/N below 0.2 and decreases or keeps relatively constant for larger H/N, suggesting that the highest performance is expected for 0.1≤𝐻/𝑁≤0.2. Note that 𝐻/𝑁≤1 in Fig. 3 because in our experiments the performance reduced even more for 𝐻>𝑁. Therefore, we propose that QELM uses 𝐻=𝜂𝑁 with 𝜂=0.15, because in Fig. 3 (lower right panel) the highest average test performance is achieved for 𝐻/𝑁=𝜂∼ 0.15. Since H must be upper bounded, we set a threshold 𝑁0 for N so that 𝐻=𝜂𝑁 for 𝑁<𝑁0 (small datasets) and 𝐻=𝜂𝑁0 for 𝑁>𝑁0 (large datasets). We set 𝑁0=15,000, so the maximum H is 𝜂𝑁0=0.15×15,000=2,250 hidden neurons. Thus, H is given by:

𝐻=⌊𝜂min(𝑁,𝑁0)⌋,𝜂=0.15,𝑁0=15,000
(1)
This approach allows to estimate a value for H directly from the number N of training patterns, which is an intrinsic property of the dataset, without the need to repeat the ELM training or test stages, nor to require complex calculations over the original training set, that might be slow for large datasets. The value of H proposed by Eq. 1 is proportional to N for small datasets with 𝑁<𝑁0 = 15,000 patterns, reaching its highest value H = 2250 for 𝑁0 = 15,000 patterns and remaining constant for large datasets with higher N. Since H does not raise for large N and H is the number of rows in matrices 𝐘 and 𝐀, and the number of columns of 𝐁, the ELM training raises slowly with the dataset size. Besides, the H value in Eq. 1 is not expected to be very far from optimality, because the ELM performance (see Fig. 3) achieves its maximum for low values (below 0.2) of H/N. Finally, larger values of H with respect to N, e.g., 𝐻=0.9N, are not required because our experiments report that: (1) performance keeps constant or reduces for 𝐻>𝑁 or 𝐻>0.4N, see lower right panel of Fig. 3; and (2) the ELM training becomes too slow or even not possible for 𝐻> 15,000 hidden neurons.

Calculation of the output weights
The output weights of an ELM network are calculated using the pseudo-inverse of matrix 𝐘, of order 𝐻×𝑁. This is an expensive task either in terms of memory, because it requires to store a matrix that may be large, and time, due to the computational complexity of the matrix pseudo-inversion, that is similar to matrix inversion. The complexity of the n-order square matrix inversion [26] is (𝑛3) using the standard Gauss–Jordan elimination, (𝑛2.807) using the Strassen algorithm [40], (𝑛2.376) using the Coppersmith–Winograd (CW) algorithm [10], used, e.g., in manifold learning ELM (ML-ELM) for matrix inversion [35], and (𝑛2.373) using CW-inspired methods. Considering specifically the matrix pseudo-inversion, given a matrix 𝐆 of order 𝑚×𝑛 with 𝑚≥𝑛, the method geinv proposed in [11] uses a full rank Cholesky factorization, followed by the inversion of a symmetric matrix, to calculate the pseudo-inverse of 𝐆. The factorization and inversion are of orders (𝑛3) and (𝑟), respectively, being r the rank of 𝐆𝑇𝐆, but on parallel architectures with enough processors these complexities can be reduced to (𝑛) and (log𝑟), respectively. However, even these optimized approaches are very slow to calculate the pseudo-inverse of matrix 𝐘, of size 𝐻×𝑁, for large datasets with high N, and consequently high H (numbers of columns and rows of 𝐘, respectively). The previous section proposed to keep H bounded with N. Analogously, the number M of columns of 𝐘 must also be bounded when the size of the training dataset raises, so for large datasets it must be 𝑀≤𝑁 instead of 𝑀=𝑁 as in ELM. Note that an acceptable value for M will be conditioned by N, but not by the number I of inputs. Our proposal is to set an upper bound 𝑀0≤𝑁 on M so that 𝑀≤𝑀0. For small datasets 𝑀0=𝑁, but 𝑀<𝑁 for large datasets, so that a smaller training set is used instead the original N training patterns. Therefore, a threshold 𝑁1=5,000 patterns is set on N, to separate both cases. This value, that defines the maximum number of columns of matrix 𝐘, was selected to achieve a reasonable size for this matrix. Thus, we have:

𝑀0=min(𝑁,𝑁1),𝑁1=5000
(2)
Specifically, we propose to replace the N original training patterns that ELM uses as columns in 𝐘 by M prototypes created from the original training patterns [3]. A set of prototypes {𝐩𝑐𝑙} of the different classes is defined, with 𝑐=1,…,𝐶 and 𝑙=1,…,𝑙𝑐, being 𝑙𝑐 the number of prototypes of class c. Each column of matrix 𝐘 is a prototype, so the number M of columns of matrix 𝐘, that must be lower than 𝑀0, is given by:

𝑀=∑𝑐=1𝐶𝑙𝑐≤𝑀0
(3)
In datasets where 𝑁<𝑁1, we have 𝑀0=𝑁 and 𝑀=𝑀0, so all the training patterns can be stored by columns in 𝐘 as in ELM. In datasets where 𝑁>𝑁1, we have 𝑀0=𝑁1 by Eq. 2, and an upper bound 𝐿𝑐 on the number 𝑙𝑐 of prototypes of each class is set, so that 𝑙𝑐≤𝐿𝑐. This bound 𝐿𝑐 is class-dependent because classes may be unbalanced, so the most populated classes should have 𝐿𝑐 higher than the low populated ones. This suggests that 𝐿𝑐∼𝑁𝑐𝑀0/𝑁, where 𝑁𝑐 is the number of training patterns of class c. Note that for large datasets, 𝑁>𝑁1 and 𝑀0=𝑁1<𝑁 so that it may be 𝑁𝑐𝑀0/𝑁<1 and class c would have zero prototypes. In order to avoid this, a lower bound 𝐿1 is set for 𝐿𝑐, so that a class always has prototypes. In order to guarantee a minimum number of prototypes per class for large datasets, an acceptable value for the maximum number of prototypes per class would be 𝐿1= 100. However, in datasets with high C where 𝑁𝑐>𝐿𝑐 for all the classes, the number M of prototypes would be 𝑀=𝐿1𝐶. The constraint 𝑀≤𝑀0 requires 𝐿1≤𝑀0/𝐶. When C > M0 we have 𝑀0/𝐶<1, so 𝐿1 would be zero and classes would have no prototypes. In order to avoid this possibility, we propose to use:

𝐿1=min{𝐿0,max(1,⌊𝑀0𝐶⌋)},𝐿0=100
(4)
so that 𝐿1=𝐿0=100 unless 𝑀0/𝐶<𝐿0, or equivalently 𝐶>𝑀0/𝐿0, in which case 𝐿1 is 1 if ⌊𝑀0/𝐶⌋=0 and ⌊𝑀0/𝐶⌋ otherwise. Finally, 𝐿𝑐 is given by:

figure b
𝐿𝑐=max(𝐿1,⌊𝑁𝑐𝑀0𝑁⌋)
(5)
Since 𝑙𝑐≤𝐿𝑐, the classes with 𝑁𝑐<𝐿𝑐 training patterns will have 𝑙𝑐=𝑁𝑐<𝐿𝑐 prototypes, while classes with 𝑁𝑐≥𝐿𝑐 will have 𝑙𝑐=𝐿𝑐. The prototypes 𝐩𝑐𝑙, with 𝑐=1,…,𝐶 and 𝑙=1,…,𝑙𝑐, are calculated in a simple way in order to be efficient for large datasets. The first 𝐿𝑐 training patterns of class c are selected as initial prototypes {𝐩𝑐𝑙}𝐿𝑐𝑙=1. The following training patterns of this class (if they exist) update their nearest prototypes from 𝐩𝑐𝑙(𝑡) to 𝐩𝑐𝑙(𝑡+1) according to:

𝐩𝑐𝑙(𝑡+1)=𝑐=[1−1𝑁𝑐𝑙(𝑡)]𝐩𝑐𝑙(𝑡)+𝐱𝑛𝑁𝑐𝑙(𝑡)𝑤𝑛,𝑙=argmin𝑗=1,…,𝑙𝑐{|𝐩𝑐𝑗−𝐱𝑛|},𝑁𝑐𝑙(𝑡+1)=𝑁𝑐𝑙(𝑡)+1
(6)
where 𝐩𝑐𝑙(𝑡+1) and 𝐩𝑐𝑙(𝑡) are the new and old versions, respectively, of the l-th prototype of class 𝑐=𝑤𝑛, i.e., the class label of training pattern 𝐱𝑛. Besides, |𝐱−𝐲|=∑𝐼𝑖=1|𝑥𝑖−𝑦𝑖|, while 𝑁𝑐𝑙(𝑡) is the number of training patterns of class label c for which prototype 𝐩𝑐𝑙(𝑡) was the nearest one until now. When class c has 𝑁𝑐<𝐿𝑐 training patterns, its l-th prototype 𝐩𝑐𝑙 is the l-th training pattern 𝐱𝑛 of class 𝑐=𝑤𝑛 for 𝑙=1,…,𝑁𝑐. This updating method, that is an efficient online version of the simple K-means clustering algorithm, allows to create a collection of class prototypes in a fast way without excessive memory requirements, because the total number of prototypes 𝑀=∑𝐶𝑐=1𝑙𝑐 for large datasets with 𝑁𝑐𝑀0/𝑁>𝐿1, using Eqs. 2 and 5, is upper bounded by 𝑁1:

𝑀==∑𝑐=1𝐶𝑙𝑐≤∑𝑐=1𝐶𝐿𝑐=∑𝑐=1𝐶max(𝐿1,⌊𝑁𝑐𝑀0𝑁⌋)∑𝑐=1𝐶⌊𝑁𝑐𝑀0𝑁⌋≤𝑀0𝑁∑𝑐=1𝐶𝑁𝑐=𝑀0𝑁𝑁=𝑀0≤𝑁1
(7)
The QELM algorithm
Algorithm 2 reports the pseudocode of QELM, where the number H of hidden neurons is given by Eq. 1 and class prototypes are updated according to Eq. 6. Table 1 list the names, values and meanings of the hyper-parameters of QELM. With these values, 𝐻≤2250 and 𝑀≤𝑁1=5000, so the size 𝐻×𝑀 of the matrix 𝐘 to be pseudo-inverted cannot overcome 2250×5000, while matrices 𝐀 and 𝐁 cannot overcome sizes 2250×𝐼 and 𝐶×2250, respectively, being C and I the numbers of classes and inputs, respectively. The values of hyper-parameters 𝑁0,𝜂,𝑁1 and 𝐿0 are selected in order to limit the computational cost of QELM for large datasets. The value 𝑁0=15,000 is selected to keep H acceptably low when N is large and to avoid slow training. The value 𝜂=0.15 is selected to provide a good performance (see Fig. 3) while keeping H low. Similarly, the value 𝑁1=5000 is selected to avoid an excessive number of prototypes that would slow down QELM in large datasets. Finally, the value 𝐿0=100 is selected to guarantee a minimum number of prototypes for low populated classes in unbalanced datasets, but not too high in order to avoid an excessive number of prototypes when many classes are present. The performance of QELM is expected to be not very sensitive to these hyper-parameters in order to avoid their tuning, that would hinder its application to large datasets.

Table 1 Hyper-parameters of QELM
Full size table
The classical ELM has been formally proven [21] to be a universal approximator that can exactly learn any training dataset provided a H large enough is used. However, the ELM cannot be applied to datasets with high N because it requires to perform pseudo-inversion of matrix 𝐘 of size 𝐻×𝑁. The QELM extends ELM for large datasets with two key contributions. First: QELM estimates an adequate number H of rows of matrix 𝐘 directly from the dataset properties, without train nor test the network. Our preliminary exploration of the ELM behavior depending on H (see Sect. 3.3) reports that the best performance is achieved for low H/N values, specifically 0.1≤𝐻/𝑁≤ 0.2. Thus, we propose to use 𝐻=𝜂𝑁 with 𝜂= 0.15, bounded for large datasets with 𝑁>𝑁0. Second: in the calculation of matrix 𝐘, QELM replaces the original training patterns by a set of class prototypes, whose size is limited for large datasets in order to keep low the number of columns of 𝐘. An efficient online version of the well-known K-means clustering algorithm is used to create a representative collection of class prototypes. Due to the K-means properties, this collection is a reduced but significant version of the original (large) training set. Combined together, both contributions are oriented to achieve a matrix 𝐘 of size large enough to achieve a good performance, but small enough to allow its storing in memory and the efficient pseudo-inverse calculation. Since the value estimated for H by QELM is high enough for a good performance on large datasets, and since the class prototypes are guaranteed to be a small but accurate representation of the original training set, the mathematical properties of the ELM guarantee that the classification problem will be learnt with a high level of performance.

Table 2 List of small datasets (𝑄≤ 15,000 patterns) with the number of total (Q) and training (N) patterns, inputs (I) and classes (C), sorted by increasing N
Full size table
Table 3 List of large datasets (𝑄> 15,000 patterns) sorted by increasing N
Full size table
Results and discussion
We tested the QELM on a collection of 28 small (𝑄≤ 15,000 patterns) and 22 large (𝑄> 15,000 patterns) classification problems (see Tables 2 and 3), including datasets up to 34 million patterns, 30,000 inputs and 131 classes. Most of them are selected from the Machine Learning Repository of the University of California at Irving (UCI).Footnote3 Dataset msa is the well-known artificial dataset with 6 nested spirals, and fruits comes from the KaggleFootnote4 repository. The table lists, for each dataset, its original name, the short name used in this paper, the total number of patterns (Q) and the number of training patterns (N), inputs (I) and classes (C). On the small datasets, QELM is compared to the classical ELM and the support vector machine with radial basis function (RBF) kernel, henceforth named SVC, implemented by the LibsvmFootnote5 library [8]. On the large datasets, the QELM is compared to ELM and to the linear kernel support vector machine, henceforth named LSVC, implemented by the LiblinearFootnote6 library [16]. The reason of using LSVC instead of SVC for large datasets is that the latter (i.e., with RBF kernel) is very slow and has two hyper-parameters (see below) whose tuning is not practical in these cases, while LSVC is designed for large datasets and can be executed without no hyper-parameter tuning.

Table 4 Kappa (in %) and time (in seconds) per fold of QELM, ELM and SVC on the small datasets
Full size table
The experiments used fourfold cross-validation, excepting the small dataset imseg and the large datasets shuttle, mnist, fruits, poker, physical and hepmass, that already provide two separated train and test sets. On the small datasets, both ELM and SVC perform a grid-search-based hyper-parameter tuning, so there is a training set (including 50% of the patterns of each class, randomly selected), a validation and a test set (each one including 25% of the patterns of each class, randomly selected) for each trial. The grid search proceeds by training the classifier over the 4 training sets and evaluating its performance over the 4 validation sets, for each combination of hyper-parameter values. The selected combination is the one with the highest average performance over the 4 validation sets. Then, the classifier is trained, using this selected combination, over the 4 train and validation sets, and the final performance is averaged over the 4 test sets. In the dataset imseg, that has separated original train and test sets, the original training set is split in a training set, including 2/3 of the patterns of each class, randomly selected, and a validation set, including the remaining 1/3 of the original training patterns of each class. The selected combination of hyper-parameter values is the one that achieves the best performance over the validation set. The final performance is the one achieved by the classifier over the test set after training over the train and validation set with the best combination. The ELM tuned the hyper-parameter H using values from 10 to min(𝑁,500) with step 10. Therefore, 𝐻≤𝑁 because the ELM performance reduces for 𝐻>𝑁 (see our previous experiments in Sect. 3.3), and 𝐻≤500 to avoid excessively large values that would slow down ELM. The SVC tuned the hyper-parameters with values in the range {2𝑖}, with 𝑖=−5 to 15 step 2 for the regularization hyper-parameter 𝜆 and 𝑖=−15 to 3 step 2 for the inverse 𝛾 of the RBF kernel spread. On the contrary, no hyper-parameter tuning was performed on the large datasets in order to avoid too slow experiments, so there is only one training and one test set per fold. The ELM used 𝐻=500, and the LSVC used 𝜆=1. The performance measurement used is the Cohen kappa [7] that takes into account the class unbalancing. The algorithms were programmed in the Octave scientific programing languageFootnote7 and executed on a computer with Intel Core i7-9700K CPU (8 cores) at 3.6GHz with 64GB RAM under Kubuntu 20.04.

Table 4 reports the kappa and time (left and right parts of the table, respectively) achieved by QELM, ELM and SVC on the small datasets. In the majority of the datasets, SVC achieves the best kappa, overcoming QELM and ELM, with the highest average value (81.7%), 5.2 points above ELM, which is expectable because SVC is a state-of-the-art classifier. A Wilcoxon sign-rank sum test comparing the kappa of SVC and ELM reports a p-value of 0.085, so the difference which is not statistically significant. Comparing ELM and QELM, the difference is small (2.1 points) and far from being significant (p-value 0.517), in fact only in two datasets (synthetic and energy) the difference slightly overcomes 10 points, so QELM follows adequately the performance of ELM. In the small datasets, the number of training patterns 𝑁𝑐 of each class is lower than the upper bound 𝐿𝑐 on the number of prototypes, so QELM uses all the training patterns as prototypes, and the columns of matrix 𝐘 are the own training patterns, as in ELM. Thus, the only difference between QELM and ELM is that the former does not perform tuning of the hyper-parameter H, so the difference in performance between them measures the impact of this lack of tuning. Since this difference is fairly low (2.1 points in average), the strategy used by QELM to estimate H (Eq. 1) can be considered quite successful.

Regarding the time required per fold (right part of Table 4), QELM is systematically faster than ELM in the small datasets, being the latter between 2 and 142 times slower than the former depending on the dataset. Since the average of times has no statistical sense, because only the larger datasets would contribute to the mean, the right part of the last row reports the average, over all the datasets, of the time of ELM divided by the time of QELM, and the analogous for SVC. Thus, the value in the QELM column is empty (—). This average value reports that ELM is in average 36 times slower than QELM that performs hyper-parameter tuning by grid-search and therefore repeats the training and test several times. Comparing SVC and QELM, the former is about 154 times slower than the latter, ranging from 9 to 580 times slower depending on the dataset. The three classifiers are slower in dataset isolet, compared to the other datasets, due to its high number of classes. In dataset nursery, QELM spends 4 s/fold, while ELM spends 17 s/fold and SVC about 7 minutes/fold.

Table 5 Kappa (in %) and time (in seconds) per fold of QELM, QELM with 𝐻=500 (column QELM∗), ELM and LSVC on the large datasets
Full size table
Table 5 reports the kappa and time of QELM, ELM and LSVC on the large datasets. The QELM is able to run in all the datasets, because the size of the matrices 𝐀, 𝐁 are 𝐻×𝐼 and 𝐶×𝐻, respectively, while 𝐘 is of size 𝐻×𝑀, with 𝑀≤𝑁1, so none of them scales with the number N of training patterns. However, the ELM fails in 9 of 22 datasets, while LSVC, which is designed for large-scale datasets, fails in 3 of the largest datasets. Considering performances, QELM also follows ELM in the large datasets, and ELM overcomes QELM by more than 10 points in 2 datasets (magic and adult), although QELM overcomes ELM by more than 10 points in datasets chess and ijcnn1. Comparing average values over the datasets where ELM does not fail, ELM overcomes QELM by only 2 points, and the p-value of the Wilcoxon test is 0.74, very far from statistical significance.

The difference between them in large datasets is caused by two reasons. (1) Following Eq. 1, QELM uses 𝐻=2,250 neurons on the large datasets, while ELM uses 𝐻=500, so it is speeded up. (2) The QELM uses prototypes, while ELM uses training patterns. In order to evaluate the impact of these second reason (prototyping) over performance, we developed an experiment comparing QELM with 𝐻=500 and ELM, that also uses 𝐻=500, so prototyping is the only difference between them. The column labeled QELM∗ in the left part of Table 5 reports the kappa of QELM∗ for the large datasets where ELM did not fail. In the majority of the datasets, excepting devanagari, QELM∗ performs similarly to ELM and the average kappa of QELM∗ is 3 points below ELM, near to the average difference between QELM and ELM (2 points). Thus, the prototyping increases the average performance from 54.6 (QELM∗) to 55.6% (QELM).

Fig. 4
figure 4
Times required by QELM for prototype calculation, train and test in the large datasets sorted by increasing N

Full size image
The performance of LSVC is clearly poorer than QELM and ELM, being 13.3 and 15.3 points below them with p-values (Wilcoxon test) of 0.285 and 0.214, respectively. The results of LSVC are very irregular, e.g., in dataset record LSVC achieves 1.8% while QELM achieves 73.3%. Other cases where LSVC performs poorly are letter, chess, shuttle, wisdm and ijcnn1. However, in hepmass LSVC achieves 67.2% and QELM 22.9%, and in adult LSVC also outperforms QELM. In the largest datasets (numbers 15–22 in Table 5), both LSVC and QELM perform poorly, which suggests that they are really difficult datasets.

The times in the right part of Table 5 report that QELM is twice slower than ELM. This is caused by: (1) ELM no longer performs hyper-parameter tuning, but uses 𝐻=500, what speeds it up; and (2) QELM must calculate prototypes, while ELM uses directly the original training patterns, so it fails in datasets 14–22. On the other hand, the LSVC is about 15 times slower than QELM, because despite of using a fast linear kernel, LSVC still uses the iterative SVC algorithm to solve the classification process, that is slow for large-scale datasets. Comparing LSVC and QELM by datasets, in some cases LSVC is faster than QELM, but in other cases the former is much slower than the latter, e.g., arabic, devanagari and baiot, with high many inputs, failing in dataset fruits due to the high 𝐼=30,000 and 𝐶=131. Note also that times of QELM raise slowly with N from 6 minutes in kddcup (4 million patterns) to 19 min in wesad (31 millions). This is caused by the limitation on the matrix sizes, although increasing on N also raises the time spent by the prototype calculation. There are also some peaks in kitsune and fruits, due to their high I (115 and 30,000, respectively).

Fig. 5
figure 5
Kappa and time, both sorted increasingly, achieved by QELM in large datasets varying 𝑁1

Full size image
Figure 4 plots the times spent by QELM to calculate prototypes, to perform training (i.e., to calculate the pseudo-inverse of the 𝐘 matrix) and test on the large datasets. In the left part, that corresponds to the smaller datasets, the times are lower, and raise when we shift to the right because the dataset size increases, with some peaks in datasets with many inputs (arabic, adult, devanagari and fruits). However, on the right part the three times are relatively stable thanks to the bounding on the dimensions 𝐻≤2250 and 𝑀≤5000 of the 𝐘 matrix. There is a slow increasing in the prototype time (continuous blue line) because more patterns must be used in the prototype calculation. Once prototypes are created, the train time (dashed red line) also remain stable over all the large datasets. However, the test time (dotted green line) raises because the time required to read the test patterns increases with the size of the test set. Note also the peaks on the test time in datasets arabic, fruits and devanagari, due to the high I, and the peak of poker, because this dataset has a reduced training set (N = 25,000) and a large test set (P = 1,000,000).

Figure 5 plots the kappa and time of QELM on the large datasets with maximum number of prototypes 𝑁1=5000 (value used in the previous experiments), 7000 and 10,000. The kappa is very similar with the different values of 𝑁1, with average values of 43.9%, 46.9% and 48.8% with 𝑁1=5000, 7000 and 10,000, respectively. Since increasing 𝑁1 more prototypes are used, the times increase in average 3.13 and 3.57 times for 𝑁1=7000 and 𝑁1= 10,000, respectively, compared to 𝑁1=5000, with some exceptions. The conclusion is that increasing 𝑁1 above 5000 only slows QELM down without raising kappa too much, so no tuning of 𝑁1 is required to achieve a good performance.

Table 6 Kappa (in %) of QELM for the large datasets varying the hyper-parameters H, 𝑁0, 𝜂 and 𝐿0, and average values
Full size table
Table 6 reports the kappa achieved by QELM on the large datasets using several values of the hyper-parameters H (number of hidden neurons), 𝑁0 (maximum N for which 𝐻=𝜂𝑁), 𝜂 (fraction of N used for H when 𝑁<𝑁0) and 𝐿0 (lower limit to the number of prototypes of a class when 𝐶<𝑀0/𝐿0, see Sect. 3.4). The missing value for 𝐿0=500 and dataset fruits is because this large value and the high number of classes (131) drive QELM out of memory. The lower row reports the average kappa, and the asterisks identify the column achieved with the default values in Table 1 (H given by Eq. 1, 𝑁0= 15,000 𝜂=0.15 and 𝐿0=100). The influence of H is very high (columns 2–4), but Eq. 1 reports the highest kappa that confirms the performance reduction for 𝐻>0.2, as in Fig. 3, while QELM is slower because H is higher (not shown in Table 6). Regarding 𝑁0 (columns 5–7), performance reduces increasing its value (see the average kappa), while driving QELM also slower. To change the 𝜂 value between 0.1 and 0.2 (columns 8–10) has also low influence over performance, although 𝜂 = 0.1 seems to perform slightly better. Similarly, to increase 𝐿0 (columns 11–13) slows down QELM, because more prototypes must be calculated, but does not raise appreciably performance with respect to 𝐿0=100. Overall, these results show that QELM is not very sensitive to the choice of 𝑁1 (see Fig. 5), 𝑁0, 𝜂 and 𝐿0, so their values do not need to be tuned. The QELM is sensitive to H, but raising its value compared to the value set by Eq. 1 only reduces performance.

Conclusion
This paper presents the quick extreme learning machine (QELM), a version of ELM designed for large-scale classification problems with many patterns (up to 31 millions) in general purpose computers without special software (such as pre-trained neural networks or big data platforms) nor hardware (such as graphic processing units, GPUs), something that is not currently available in the literature and therefore exhibits a high practical interest. In order to achieve this goal, QELM replaces the hyper-parameter tuning of the number H of hidden neurons, that usually represents an important overload because it requires to train and test the network several times, by an bounded estimation from the number N of training patterns. On the other hand, the ELM requires to calculate the pseudo-inverse of the hidden layer activity matrix 𝐘, of size 𝐻×𝑁, a process that is not scalable for datasets with large N. Instead, QELM uses an activity matrix of size bounded both on H and N, avoiding and indefinite growing of the matrix with the dataset size. First, the number H of rows of 𝐘 is 𝐻≤𝜂𝑁0=2250 neurons, so it is upper bounded for large N. Second, the columns of 𝐘 are 𝑀≤𝑁1=5000 prototypes, created using an efficient online version of the K-means clustering algorithm from the training patterns, instead of the original training patterns, so 𝐘 has at most 𝑁1= 5000 columns when 𝑁≥𝑁1, independently on N. Thus, the maximum size of 𝐘 is 𝜂𝑁0×𝑁1, or 2250×5000. This allows QELM to process datasets with millions of patterns, because only 5000 prototypes are stored, although the time required by the prototype calculation raises with N. Consequently, QELM is able to classify datasets up to 31 millions of patterns (23 millions of training patterns), 30,000 inputs and 131 classes, while the standard ELM can only manage datasets up to 5 million patterns (3,7 millions of training patterns) and even the linear SVM cannot execute in three of the largest datasets. The QELM performance is 2 and 7 points below ELM and radial SVM, respectively, in small datasets. In large datasets, QELM is only 2 points below ELM and 13 points above the linear SVM. Considering speed, QELM is 36 and 154 times faster than ELM and radial SVM on the small datasets. In the large datasets, QELM is so fast as ELM (that uses 𝐻=500 and fails in 40% of the datasets) and 14 times faster than linear SVC. Moreover, the time spent by QELM is relatively stable when the size of the training set increases, due to the limitation on the matrix sizes. The sensitivity of QELM with respect to its hyper-parameters (e.g., the maximum number 𝑁1 of prototypes) is low, so no tuning is required for a good performance. Future work includes to extend the capability to datasets with even larger number of inputs.

Keywords
Extreme learning machine
Classification
Large-scale datasets
Model selection