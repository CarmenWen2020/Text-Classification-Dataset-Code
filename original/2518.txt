Person reidentification (ReID) aims to associate the person with the given identity across different cameras, which has wide application in the field of intelligent video. In this work, an efficient method is proposed to improve ReID performance. First, a global-guided feature joint network is designed, which consists of a multiple feature extraction network and a global-guided feature fusion network. The former can align different body regions and extract global feature and local features. The latter utilizes the global feature to guide the adaptive fusion of the local features, which can evaluate the importance of different local features dynamically. Secondly, a Bi-relevance TriHard loss (TriBR) is designed to penalize the loss dynamically. TriBR combines Euclidean distance and angle information, which considers the self-relevance of the intra-class samples and the cross-relevance of the inter-class samples simultaneously. Besides, TriBR can adaptively adjust the distance margin and the angle margin to optimize the network. The proposed TriBR is advantageous to learn more discriminative features. The method achieves 89.2% mAP (mean Average Precision) with 95.8% Rank-1 on Market-1501, 79.7% mAP with 89.3% Rank-1 on DukeMTMC. The proposed method also performs excellently on the datasets for the occluded person reidentification problem.

Introduction
As an important automated retrieval technology, ReID refers to retrieving the given person’s image from the images taken by other cameras, which has a wide application. In consideration of the potential value, it has attracted noticeable attention successfully.

ReID technologies can be divided into two categories, the traditional image processing methods and the deep learning methods. The traditional methods require manually designed feature descriptors to represent the images and then ascertain the indistinguishable information between different features [1,2,3,4]. The deep learning methods are the end-to-end model optimized by the neural networks, which combine the process of designing feature descriptors and measuring feature similarity. Deep learning methods can be divided into representation learning and metric learning.

Representation learning aims to extract features from the person images. Most of the researchers utilize convolution neural networks to acquire information on the global features [5, 6]. The global feature means: take a complete image as input, do not perform other processing (such as horizontal cut, and posture estimation) on the image, and directly use the convolutional neural network to extract the feature of the whole image. Recently, researchers consider detailed information and attempt to extract local features from different body regions [7, 8]. The local feature means: divide the complete image into different sub-regions, then using the convolutional neural network to extract the features from these sub-regions. In this work, a global-guided feature joint network (GFJN) is proposed, which consists of a multiple feature extraction network (MFEN) and a global-guided feature fusion network (GFFN). In MFEN, a pose estimation module is embedded to track down the skeleton landmarks and extract three body regions with strong alignment. Simultaneously, a global feature and three local features could be generated straight through the subsequent pipeline. Next, GFFN fuses four features to the final feature and utilizes the generated feature to describe the person image. It is noteworthy to record that GFFN evaluates the association between the global feature and the local features. GFFN takes full advantage of the global feature to guide the adaptive fusion of the local features, which can evaluate the effectiveness of different local features dynamically. GFJN can enhance feature information and improve feature discrimination efficiently.

Metric learning focuses on optimizing the sample distance, which is considered to be a very useful approach to improve the performance. Contrastive loss [9], triplet loss [10], TriHard loss [11], and quadruplet loss [12] are widely used losses. In this work, a Bi-relevance TriHard (TriBR) loss is proposed to optimize the designed network. TriBR can combine Euclidean distance and angle information. TriBR considers the self-relevance of the intra-class samples and the cross-relevance of the inter-class samples. Therefore, TriBR can penalize the hardest samples dynamically. Besides, the distance margin and the angle margin can be adaptively adjusted in the pipeline. The proposed TriBR can guide the network to learn more discriminating features in the embedding space.

The proposed method achieves competitive performance in comparison with the state-of-the-art methods. The contributions of this work can be summarized as follows: (1) A global-guided feature fusion strategy is proposed to fuse the global feature and the local features dynamically based on the feature points from the feature map, which can distinguish the difference in importance of the features. (2) A Bi-relevance TriHard loss considering the self-relevance and the cross-relevance is proposed to optimize the network, which can penalize the metric loss and guide the network to learn more discriminating features dynamically. Compared with other loss variants, the proposed loss achieves higher performance while bringing less additional calculation and complexity.

Related work
In deep learning, representation learning focuses on utilizing the neural network to extract image features. Li et al. [5] proposed a FPNN (filter pairing neural network) to divide the images horizontally and then designed a block matching layer to obtain a more effective feature. Varior et al. [13] proposed a JLCF (Jointly Learned Color Features) algorithm to learn image features by transforming the pixel feature into a stable embedding space. Ding et al. [14] proposed a feature mask network, which used a ResNet to perform feature mapping and to achieve selective feature extraction. Zheng et al. [15] proposed a PAN (Pedestrain Alignment Network), which used a body structure to enhance the pairing features, and the extracted features were used to achieve adaptive location and alignment. Zhao et al. [16] used the skeleton information to locate and extract body regions, proposed a Spindle Net to fuse region features, and achieved multi-scale and multi-integrated image features. Zheng et al. [17] estimated the human body posture and the skeleton landmarks and then used an affine transformation to achieve feature extraction. Li et al. [18] proposed a cross-modal model assisted by X-mode, which matched person images under infrared light and visible light and utilized different color channels in different modalities to achieve cross-modal matching. Li et al. [19] proposed a deep learning model of HA-CNN (Harmonious Attention Convolutional Neural Network), which used a convolutional network to learn soft and hard features simultaneously, to maximize the information. These researches show that combining global feature and local features can improve the reidentification performance. Therefore, in this work, we also propose a new method to dynamically fuse the global feature and the local features, which corresponds to the first contribution in our work.

Different from representation learning, metric learning focuses on learning the similarity of image pairs. The loss makes the distance between the positive sample pairs as small as possible, and the distance between the negative sample pairs as large as possible. Varior et al. [9] designed a twin network and proposed contrastive loss to constrain the network, and this loss could improve the performance dynamically. Schroff et al. [10] proposed a triplet loss by taking three images as input, which considered the distance between different samples. Hermans et al. [11] improved the triplet loss to construct a TriHard loss, which trained the network with more difficult samples to guide the network to learn more discriminative features. This strategy can improve the generalization ability and the robustness of the network. Chen et al. [12] designed a quadruple loss, which took four different images as input. The absolute distance was considered. Xiao et al. [20] proposed a new metric learning loss, called margin sample mining loss, which chose the hardest samples to calculate the loss. It considered the relative distance and the absolute distance simultaneously. Ma et al. [21] proposed a paired constraint analysis method based on distance weighting, which utilized the weighted distance to measure the feature similarity. Liu et al. [22] designed a nonlinear metric learning loss and a set-label model based on a deep belief network to measure the correlation of the image pairs. Chen et al. [23] proposed a novel similarity loss. The intermediate features were used to describe the matching of the sub-regions, and all intermediate features were integrated into an unified model to calculate the similarity from different regions. Wang et al. [24] proposed an angular loss for deep metric learning based on the angle relationship of the negative points, which could improve the robustness of features and achieve better convergence. Wang et al. [25] designed an angular loss with hard sample mining; the negative point’s angle could be constrained to learn more discriminative and robust features. These researches show that improving the triplet loss and introducing the angle information can also increase the performance. Therefore, in this work, we also explore a strategy to improve the loss to promote the reidentification performance, which corresponds to the second contribution in our work.

Global-guided feature joint network
A global-guided feature joint network (GFJN) is proposed in this work as shown in Fig. 1, which consists of a multiple feature extraction network (MFEN) and a global-guided feature fusion network (GFFN). First, the image is input to MFEN, and the region extract module (REM) is utilized to detect different skeleton landmarks and to extract three local regions with strong alignment. Then, neural networks are used to extract multiple features including a global feature and three local features. GFFN is proposed to fuse four features to the final feature descriptor. The correlation between global feature and local features is considered. It utilizes the global feature to guide the adaptive fusion of the local features, which can evaluate the significance of the local features dynamically. GFJN can enhance feature information and improve feature discrimination efficiently. The details are explained in the following sections.

Fig. 1
figure 1
The pipeline of GFJN. It consists of a multiple feature extraction network (MFEN) and a global-guided feature fusion network (GFFN). Here, REM refers to region extraction module, RMM refers to region mapping module, FWM refers to feature weight module, and FFM refers to feature fusion module

Full size image
Multiple feature extraction network
After analyzing and identifying the differences and the similarities between the previous researches, we infer that the local feature can give a boost. Horizontal division and pose estimation are two prevailing strategies, but the former may cause image misalignment. Therefore, the advantage of the latter is retained to achieve the extraction of different local regions.

Convolutional pose machine (CPM) [26] is a mainstream method of pose estimation. Based on this, the region extraction module (REM) is designed to extract three local regions. The pipeline of REM is illustrated in Fig. 2.

Fig. 2
figure 2
The pipeline of REM. Three local regions are generated by fourteen landmarks

Full size image
For an input image, REM utilizes a convolutional pose machine to detect fourteen skeleton landmarks. Then, the input image is divided into three local regions: head, body, and leg. Different regions contain different landmarks. The specific relationship can be summarized as follows:

𝑅ℎ=[𝐿1,𝐿2,𝐿3,𝐿6]
(1)
𝑅𝑏=[𝐿3,𝐿4,𝐿5,𝐿6,𝐿7,𝐿8,𝐿9,𝐿12]
(2)
𝑅𝑙=[𝐿9,𝐿10,𝐿11,𝐿12,𝐿13,𝐿14]
(3)
where 𝑅ℎ is the head region, 𝑅𝑏 is the body region, 𝑅𝑙 is the leg region, and 𝐿𝑖 is the 𝑖𝑡ℎ landmark.

After generating three local regions, MFEN can extract a global feature and three local features. As illustrated in Fig. 1, MFEN consists of four branches.

From top to bottom, the first branch can extract the local feature of the head region, the second branch can extract the local feature of the body region, and the third branch can extract the local feature of the leg region. Apart from the above three branches, the last branch takes the original image as input to extract the global feature. As shown in Fig. 1, in the branches of extracting local features, the pipeline of feature extraction is ResNet50. In the branch of extracting global feature, ResNet50 is split into Backbone_1 and BackBone_2. The purpose is to extract an intermediate feature map which is used for feature fusion.

Therefore, for an input image, MFEN can extract four features including a global feature and three local features. Owing to the use of pose estimation, MFEN can achieve efficient alignment for different local regions.

In conclusion, MFEN based on REM aims to extract three local regions with strong alignment and learn the global feature as well as the local features. The pipeline can be divided into four branches. With MFEN, the input image can generate four features. This can be summarized as follows:

𝑓𝑠=[ 𝑓𝑔, 𝑓ℎ, 𝑓𝑏, 𝑓𝑙 ]
(4)
where 𝑓𝑠 is the set of four features, 𝑓𝑔 is the global feature, 𝑓ℎ is the local feature of the head region, 𝑓𝑏 is the local feature of the body region, and 𝑓𝑙 is the local feature of the leg region.

Global-guided feature fusion network
With the pipeline of MFEN, a global feature and three local features can be generated. Afterward, combining these features into an efficient feature descriptor is an important and challenging task. Most of the works directly perform simple feature fusion, not considering the fact that different features should have different importance. To resolve this problem, we propose a global-guided feature fusion network (GFFN) as illustrated in Fig. 1. GFFN utilizes the global information to adaptively join a global feature and three local features. GFFN consists of a region mapping module, a feature weight module, and a feature fusion module. The feature weight module is the core of GFFN. It generates different weights for local features based on the importance of the local features and then weights these features to generate new local features. Finally, the feature fusion module is embedded in the pipeline to fuse a global feature and three weighted local features. The fused feature is considered as the new feature descriptor. The following content explains the working mechanism of each module.

To extract image features, the feature map needs to be calculated continuously with the subsequent neural units such as convolution kernel and pooling layer. The feature map is composed of a series of feature points. If the value of the feature point is larger, the calculated result will be larger and the influence on the subsequent network will be greater. Based on this, the distribution of the feature points on the feature map is utilized to evaluate the importance of different features.

As mentioned earlier, three local regions are extracted from the original image. Therefore, the importance of the local features is evaluated according to the global feature map and the region mapping relationship.

As illustrated in Fig. 1, the feature extraction module in the global branch is divided into two parts: BackBone_1 and BackBone_2. They are all parts of ResNet50. The specific structure is illustrated in Fig. 3.

Fig. 3
figure 3
The pipeline of ResNet50 in the global branch. Here, C refers to Conv, BN refers to Batch Norm, R refers to ReLU, MP refers to Max Pooling, CB refers to Conv Block, IB refers to ID Block, AP refers to Avg Pooling, F refers to Flatten, and FC refers to Full Connect

Full size image
The purpose of this segmentation is to conveniently extract the feature map of the intermediate stage. After generating the feature map from the global branch, the region mapping module (RMM) is utilized to construct the correlation between the local regions and the global feature map as illustrated in Fig. 4.

Fig. 4
figure 4
Mapping three local regions to the global feature map. H, B, and L refer to head, body, and leg

Full size image
The region mapping module takes the locations of three local regions as input, and it can locate the corresponding sub-regions on the global feature map based on the correlation between the local regions and the original image. The three sub-regions are defined as the local feature maps.

After determining three local feature maps, the feature weight module can calculate the weights of different features based on the distribution of the feature points on the feature map. And then, three local features can be weighted by the generated weights.

As mentioned above, the feature map consists of numerical values situated in different points, we will call them feature points. The larger value will lay a greater influence on the subsequent pipeline. Inspired by this, considering the distribution of the feature points, taking the global feature map as the benchmark, the local features are regarded as the supplementary information of the global feature. First, set a threshold T, see Eq. (6), then calculate the number of feature points which are greater than the set threshold T on the local feature map and the global feature map. The ratio of these two numbers is defined as the weight of the local feature. It can be formulated as follows:

𝑤𝐿=∑𝐶𝑖=1∑𝐻𝐿𝑗=1∑𝑊𝐿𝑘=1𝛿𝐿(𝑖,𝑗,𝑘)∑𝐶𝑢=1∑𝐻𝐺𝑡=1∑𝑊𝐺𝑟=1𝛿𝐺(𝑢,𝑡,𝑟)
(5)
where C is the number of the feature channels, 𝐻𝐿 and 𝑊𝐿 is the height and the width of the local feature map, 𝐻𝐺 and 𝑊𝐺 is the height and width of the global feature map. The calculation method of 𝛿𝐿 and 𝛿𝐺 is defined as follows:

𝛿(𝑐,ℎ,𝑤)={1,0,𝑣(𝑐,ℎ,𝑤)>𝑇𝑣(𝑐,ℎ,𝑤)≤𝑇
(6)
where v(c,h,w) is the value of the feature point, and T is the threshold.

Figure  5 shows an example of generating the weights of the local features based on the global feature map. The values of feature points on the feature map are greater or equal to zero with ReLU.

Fig. 5
figure 5
An example of generating weights of the local features based on the global feature map. Assume the size of the global feature map is 1×16×8, z refers to zero, and v refers to a value greater than zero. The threshold T is set to zero. There are 100 points whose value is greater than the threshold on the global feature map. There are 15, 45, and 30 points which are greater than the threshold on three local feature maps. Therefore, three weights of three local features are calculated as 0.15, 0.45, and 0.30

Full size image
After employing the global information to guide the generation of weights for three local features, the feature weight module can weight these local features with the generated weights. It can be formulated as follows:

𝑓𝐿=[ 𝑓ℎ  𝑓𝑏  𝑓𝑙]𝐓[𝑤ℎ  𝑤𝑏  𝑤𝑙]
(7)
where 𝑓ℎ, 𝑓𝑏, and 𝑓𝑙 are three local features. 𝑤ℎ, 𝑤𝑏, and 𝑤𝑙 are the weights for three local features.

The feature fusion module is embedded in the proposed GFFN to fuse the global feature and the weighted local features. It can generate a new feature descriptor as follows:

𝑓=𝑐𝑜𝑛𝑐𝑎𝑡(𝑓𝑔,𝑓𝐿)
(8)
where 𝑓𝑔 is the global feature, 𝑓𝐿 is the weighted local features, and concat is the fusion function. concat is a function which can cascade two feature vectors 𝑓𝑔 and 𝑓𝐿. In this work, 𝑓𝑔 is a 256-dimensional vector, and 𝑓𝐿 is a 768-dimensional vector. After processing with concat function, the new feature f is a 1024-dimensional vector. For two images to be compared, we extract two cascaded features from them and compare the similarity between these two vectors, that is, the similarity between the two images.

The fused feature is considered as a new feature descriptor for the person image. For the image pair to be compared, two corresponding descriptors can be obtained by this method. Then, the segmented similarity calculation method is utilized to calculate the similarity between two features. It can be formulated as follows:

𝑆=𝑠(𝑓𝑔𝑝,𝑓𝑔𝑔)+𝑠(𝑓ℎ𝑝,𝑓ℎ𝑔)+𝑠(𝑓𝑏𝑝,𝑓𝑏𝑔)+𝑠(𝑓𝑙𝑝,𝑓𝑙𝑔)
(9)
where 𝑠(𝑓𝑔𝑝,𝑓𝑔𝑔) stands for the similarity between the global feature from the probe image and the gallery image. 𝑠(𝑓ℎ𝑝,𝑓ℎ𝑔), 𝑠(𝑓𝑏𝑝,𝑓𝑏𝑔), and 𝑠(𝑓𝑙𝑝,𝑓𝑙𝑔) are three similarities from three local features. s is a function to calculate the similarity between two vectors. In this work, Euclidean distance is applied to measure this similarity. The greater Euclidean distance between feature vectors is, the lower similarity between the corresponding images is. The smaller Euclidean distance between feature vectors is, the greater similarity between the corresponding images is. Finally, all images are ranked in descending order based on this similarity to complete the person reidentification task.

The fusion similarity generated by Eq. (9) is considered to evaluate the similarity between the probe image and the gallery image. It can fuse a global feature and three local features efficiently to improve the performance.

Bi-relevance trihard loss
To train the mentioned feature extraction networks in the global-guided feature joint network efficiently, a Bi-relevance TriHard (TriBR) loss architecture is proposed in this work as shown in Fig. 6. Here, TriBR loss is derived from TriHard loss.

Fig. 6
figure 6
The pipeline of the proposed Bi-relevance TriHard loss

Full size image
First, TriBR considers the self-relevance of the intra-class samples based on sample outliers. Secondly, it also considers the cross-relevance of the inter-class samples based on sample differences. Therefore, TriBR can penalize the hardest samples dynamically. In addition, the adaptive adjustment strategy for distance margin and angle margin is embedded in the proposed pipeline. TriBR can guide network to learn more discriminative features.

TriHard loss
Triplet loss and derivatives are widely applied in the person reidentification. They can constrain the network training efficiently and guide the network to learn image features with higher discrimination and robustness. The triple images consist of an anchor sample, a positive sample, and a negative sample. Triplet loss makes the distance between positive sample pairs as small as possible, and the distance between negative sample pairs as large as possible. It can be formulated as follows:

𝐿𝑡𝑟𝑖=1𝑁∑𝑖=1𝑁[𝑑(𝑎,𝑝)−𝑑(𝑎,𝑛)+𝑚]+
(10)
where a is the anchor, p is the positive sample, n is the negative sample, a and p are the positive sample pair, a and n are the negative sample pair, and m is the distance margin.

TriHard loss (hard-mining triplet loss) is an improved version of triplet loss. The traditional triples randomly select three images. Although this method is relatively simple, most of the samples are sample pairs that are easy to distinguish. If a large number of training sample pairs are simple, then this is not conducive for the network learning a discriminating model representation. Therefore, TriHard loss utilizes more difficult samples to constrain the network.

For each training batch, TriHard loss selects P persons randomly, and each person selects K different images randomly. That is a batch containing P×K images. Then, for each anchor in the batch, TriHard loss can select a hardest positive sample and a hardest negative sample to form a triple. This can be formulated as follows:

𝐿𝑇𝑟𝑖𝐻𝑎𝑟𝑑=1𝑃×𝐾∑𝑖=1𝑃×𝐾[max(𝑑(𝑎,𝑝))−𝑚𝑖𝑛(𝑑(𝑎,𝑛))+𝑚]+
(11)
Although TriHard loss has been established itself confirmed to be a powerful loss to improve the performance efficiently. It merely considers the hardest positive sample and the hardest negative sample, which deems that the hardest samples possess the same importance. Without considering the remaining samples except for the hardest samples, it also ignores the relevance of the difference between the hardest positive sample and the hardest negative sample. Therefore, a novel loss named Bi-relevance TriHard (TriBR) loss is proposed in this work. It combines Euclidean distance and angle information. Besides, both the self-relevance of the intra-class samples and the cross-relevance of the inter-class samples are considered simultaneously.

Self-relevance penalty on trihard loss
Traditional TriHard loss only utilizes the hardest samples to calculate the loss, which ignores the remaining samples. Therefore, we propose the concept of the self-relevance considering the distribution of the intra-class samples to penalize the hardest samples dynamically.

Figure  7 illustrates the mechanism of the self-relevance penalty based on the Euclidean distance of the intra-class samples. If the hardest positive sample or the hardest negative sample has a greater outlier degree compared with the remaining positive samples or negative samples, it indicates that the original loss cannot achieve the desired performance, the penalty of the network needs to be further strengthened.

Fig. 7
figure 7
The mechanism of the self-relevance penalty based on the outlier degree of the Euclidean distance. a is the anchor, 𝑝1 and 𝑝2 are the hardest positive samples in two batches, and 𝑛1 and 𝑛2 are the hardest negative samples in two batches. The green circle is the distribution of the remaining positive samples, and the red circle is the distribution of the remaining negative samples. The positive samples should be closer to the green circle, and 𝑝2 has a higher outlier degree than 𝑝1 compared with other intra-class samples. Therefore, the greater penalty should be assigned on 𝑝2. Similarly, the negative samples should be near to the red circle, and 𝑛1 has a higher outlier degree than 𝑛2 compared with other intra-class samples. Therefore, the greater penalty should be assigned on 𝑛1. The number of arrows indicates the penalty degree

Full size image
The self-relevance penalty of the Euclidean distance is imposed on TriHard loss as follows:

𝐿𝑠𝑟−Euclidean=1𝑃×𝐾∑𝑖=1𝑃×𝐾[𝑤𝑠𝑒𝑝max(𝑑(𝑎,𝑝))−𝑤𝑠𝑒𝑛min(𝑑(𝑎,𝑛))+𝑚𝑒]+
(12)
where 𝑤𝑠𝑒𝑝 and 𝑤𝑠𝑒𝑛 are two penalty weights for the hardest samples. The former has a positive correlation with the sample outlier degree, and the latter has a negative correlation with the sample outlier degree.

In this work, the ratio between the hardest distance with the average distance is utilized to describe the outlier degree. It is considered as the penalty coefficient of the hardest distance simultaneously. These can be formulated as follows:

𝑤𝑠𝑒𝑝=max(𝑑(𝑎,𝑝))𝑎𝑣𝑒(𝑑(𝑎,𝑝))
(13)
𝑤𝑠𝑒𝑛=min(𝑑(𝑎,𝑛))𝑎𝑣𝑒(𝑑(𝑎,𝑛))
(14)
TriHard loss only considers Euclidean distance. However, angle information usually provides additional help in a retrieve task. Therefore, the angle information is embedded in the self-relevance penalty. Figure  8 illustrates the mechanism of the self-relevance penalty based on the sample angle. If the angle of the hardest sample has a greater outlier degree compared with the remaining intra-class samples, the loss needs to be further strengthened.

Fig. 8
figure 8
The mechanism of the self-relevance penalty based on the outlier degree of the angle. a is the anchor, 𝑝1 and 𝑝2 are the hardest positive samples in two batches, and 𝑛1 and 𝑛2 are the hardest negative samples in two batches. Here, 𝑝1 has a higher outlier degree than 𝑝2. Therefore, the greater penalty should be assigned on 𝑝1. Similarly, 𝑛1 has a higher outlier degree than 𝑛2. Therefore, the greater penalty should be assigned on 𝑛1. The number of arrows indicates the penalty degree

Full size image
The self-relevance penalty of the angle information is imposed on TriHard loss as follows:

𝐿𝑠𝑟−angle=1𝑃×𝐾∑𝑖=1𝑃×𝐾[𝑤𝑠𝑎𝑝max(𝜃(𝑎,𝑝))−𝑤𝑠𝑎𝑛min(𝜃(𝑎,𝑛))+𝑚𝜃]+
(15)
where 𝑤𝑠𝑎𝑝 and 𝑤𝑠𝑎𝑛 are two penalty weights for the hardest samples. They can be formulated as follows:

𝑤𝑠𝑎𝑝=max(𝜃(𝑎,𝑝))𝑎𝑣𝑒(𝜃(𝑎,𝑝))
(16)
𝑤𝑠𝑎𝑛=min(𝜃(𝑎,𝑛))𝑎𝑣𝑒(𝜃(𝑎,𝑛))
(17)
Therefore, the self-relevance penalty on TriHard loss can be formulated as follows:

𝐿𝑠𝑟=𝐿𝑠𝑟−Euclidean+𝐿𝑠𝑟−angle
(18)
Both Euclidean distance and angle information are embedded in the self-relevance penalty on TriHard loss. Besides, it considers the distribution of the remaining intra-class samples and improves the performance.

Cross-relevance penalty on trihard loss
Except for the fact that the distribution of the intra-class samples should be considered, and the difference between the hardest positive sample and the hardest negative sample is also supposed to be discussed. Therefore, the cross-relevance penalty on TriHard loss is embedded in the proposed loss, which can consider the difference of the inter-class samples.

Figure  9 illustrates the mechanism of the cross-relevance penalty based on the Euclidean distance of the inter-class samples. If the hardest positive sample has a considerable difference compared to the hardest negative sample. In other words, the distance of the hardest positive sample exceeds the hardest negative sample a lot. This indicates that the network can be further optimized by strengthening the penalty.

Fig. 9
figure 9
The mechanism of the cross-relevance penalty based on the sample difference of the Euclidean distance. a is the anchor, 𝑝1 and 𝑛1 are the hardest samples in the first batch, and 𝑝2 and 𝑛2 are the hardest samples in the second batch. Compared to the first batch, the difference of the hardest inter-class samples from the second batch is larger. Therefore, a greater penalty should be assigned to the second batch. The number of arrows indicates the penalty degree

Full size image
The cross-relevance penalty of the Euclidean distance is imposed on TriHard loss as follows:

𝐿𝑐𝑟−Euclidean=1𝑃×𝐾∑𝑖=1𝑃×𝐾[𝑤𝑐𝑒𝑝max(𝑑(𝑎,𝑝))−𝑤𝑐𝑒𝑛min(𝑑(𝑎,𝑛))+𝑚𝑒]+
(19)
where 𝑤𝑐𝑒𝑝 and 𝑤𝑠𝑎𝑛 are two penalty weights for the hardest samples.

In this work, the ratio between the hardest positive distance with the hardest negative distance is utilized to describe the difference of the inter-class samples. It is considered as the penalty coefficient of the hardest distance simultaneously. However, it should be noted that we do not impose a penalty always, but only when the difference of the inter-class samples satisfies the set condition. These can be formulated as follows:

𝑤𝑐𝑒𝑝={max(𝑑(𝑎,𝑝))/min(𝑑(𝑎,𝑛)),1,max(𝑑(𝑎,𝑝))/min(𝑑(𝑎,𝑛))max(𝑑(𝑎,𝑝))/min(𝑑(𝑎,𝑛))>𝑇𝑒⩽𝑇𝑒 
(20)
𝑤𝑐𝑒𝑛={min(𝑑(𝑎,𝑝))/max(𝑑(𝑎,𝑛)),1,max(𝑑(𝑎,𝑝))/min(𝑑(𝑎,𝑛))max(𝑑(𝑎,𝑝))/min(𝑑(𝑎,𝑛))>𝑇𝑒⩽𝑇𝑒 
(21)
where 𝑇𝑒 is the set threshold of the distance difference.

Fig. 10
figure 10
The mechanism of the cross-relevance penalty based on the sample difference of the angle information. a is the anchor, 𝑝1 and 𝑛1 are the hardest samples in the first batch, and 𝑝2 and 𝑛2 are the hardest samples in the second batch. Compared to the first batch, the angle difference of the hardest inter-class samples from the second batch is smaller. Therefore, a greater penalty should be assigned to the second batch. The number of arrows indicates the penalty degree

Full size image
Similar to the self-relevance penalty, the angle information is also embedded in the cross-relevance penalty. Figure  10 illustrates the mechanism of the cross-relevance penalty based on the angle of samples. The smaller difference in the angle of the hardest inter-class samples indicates that the loss of the network needs to be further increased.

The cross-relevance penalty of the angle information is imposed on TriHard loss as follows:

𝐿𝑐𝑟−angle=1𝑃×𝐾∑𝑖=1𝑃×𝐾[𝑤𝑐𝑎𝑝max(𝜃(𝑎,𝑝))−𝑤𝑐𝑎𝑛min(𝜃(𝑎,𝑛))+𝑚𝜃]+
(22)
where 𝑤𝑐𝑎𝑝 and 𝑤𝑐𝑎𝑛 are two penalty weights for the hardest samples.

In this work, the angle discrepancy between the hardest positive sample with the hardest negative sample is used to describe the difference of the inter-class samples. And the penalty coefficient of the hardest angle is calculated based on this. It can be formulated as follows:

𝑤𝑐𝑎𝑝={(𝜋+𝑇𝑎)/(𝜋+𝜃𝑑),1,𝜃𝑑<𝑇𝑎𝜃𝑑≥𝑇𝑎
(23)
𝑤𝑐𝑎𝑛={(𝜋+𝜃𝑑)/(𝜋+𝑇𝑎),1,𝜃𝑑<𝑇𝑎𝜃𝑑≥𝑇𝑎
(24)
where 𝑇𝑎 is the set threshold of the angle discrepancy, 𝜃𝑑 is the angle discrepancy, it can be formulated as follows:

𝜃𝑑=𝑎𝑏𝑠(max(𝜃(𝑎,𝑝))−min(𝜃(𝑎,𝑛)))
(25)
Therefore, the cross-relevance penalty on TriHard loss can be formulated as follows:

𝐿𝑐𝑟=𝐿𝑐𝑟−Euclidean+𝐿𝑐𝑟−angle
(26)
Combining Eq. (18) and Eq. (26), the Bi-relevance TriHard (TriBR) loss considering the self-relevance penalty and the cross-relevance penalty can be formulated as follows:

𝐿𝑇𝑟𝑖𝐵𝑅=1𝑃×𝐾∑𝑖=1𝑃×𝐾[(𝑤𝑠𝑒𝑝𝑤𝑐𝑒𝑝max(𝑑(𝑎,𝑝))−𝑤𝑠𝑒𝑛𝑤𝑐𝑒𝑛min(𝑑(𝑎,𝑛))+𝑚𝑒)+(𝑤𝑠𝑎𝑝𝑤𝑐𝑎𝑝max(𝜃(𝑎,𝑝))−𝑤𝑠𝑎𝑛𝑤𝑐𝑎𝑛min(𝜃(𝑎,𝑛))+𝑚𝜃)]+
(27)
when combining Eq. (18) and Eq. (26), we are not simply adding them together, but multiplicatively combining the weight coefficients of the same items. For example, for the item max(d(a, p)), the weight coefficients in the two formulas are 𝑤𝑠𝑒𝑝 and 𝑤𝑐𝑒𝑝, respectively. We multiply these two weights together to get 𝑤𝑠𝑒𝑝 𝑤𝑐𝑒𝑝 and then arrange it in Eq. (27) as the new weight of item max(d(a, p)). The new weights of the other three terms are also obtained according to this method, so final Eq. (27) can be obtained. In addition to multiplicative combination, additive combination is also an effective way. And both of these methods do not require complex calculations. But compared with the additive combination, the performance of the multiplicative combination is more effective. Therefore, the multiplicative combination strategy is still used when combining the weights of the same items in this work.

By the definition of Bi-relevance TriHard loss, we can find that TriBR loss can always be greater than traditional TriHard loss, which is effective in the early stage of the training, but a larger loss in the later stage of the training may cause network under-fitting. Therefore, a simple strategy is proposed to adaptively adjust the distance margin and the angle margin. It can be summarized as follows:

𝑤𝑚𝑒=𝑤𝑠𝑒𝑛𝑤𝑐𝑒𝑛𝑤𝑠𝑒𝑝𝑤𝑐𝑒𝑝
(28)
𝑤𝑚𝜃=𝑤𝑠𝑎𝑛𝑤𝑐𝑎𝑛𝑤𝑠𝑎𝑝𝑤𝑐𝑎𝑝
(29)
where 𝑤𝑚𝑒 and 𝑤𝑚𝜃 are the weight coefficient for the distance margin and the angle margin.

Therefore, the final TriBR loss can be formulated as follows:

𝐿𝑇𝑟𝑖𝐵𝑅=1𝑃×𝐾∑𝑖=1𝑃×𝐾[(𝑤𝑠𝑒𝑝𝑤𝑐𝑒𝑝max(𝑑(𝑎,𝑝))−𝑤𝑠𝑒𝑛𝑤𝑐𝑒𝑛min(𝑑(𝑎,𝑛))+𝑤𝑠𝑒𝑛𝑤𝑐𝑒𝑛𝑤𝑠𝑒𝑝𝑤𝑐𝑒𝑝𝑚𝑒)+(𝑤𝑠𝑎𝑝𝑤𝑐𝑎𝑝max(𝜃(𝑎,𝑝))−𝑤𝑠𝑎𝑛𝑤𝑐𝑎𝑛min(𝜃(𝑎,𝑛))+𝑤𝑠𝑎𝑛𝑤𝑐𝑎𝑛𝑤𝑠𝑎𝑝𝑤𝑐𝑎𝑝𝑚𝜃)]+
(30)
TriBR can combine Euclidean distance and angle information simultaneously. Besides, both the self-relevances of the intra-class samples based on the sample outlier degree and the cross-relevance of the inter-class samples based on the sample difference are embedded in the proposed loss. Therefore, TriBR can dynamically penalize the hardest samples. And the distance margin and the angle margin can be adaptively adjusted. The proposed TriBR can guide the network to learn more discriminative features.

Experiments
Datasets
Market-1501 [27] utilizes six cameras to collect 32668 images of 1501 identities. The training set includes 12936 images of 751 identities, and the testing set includes a query set containing 3368 images and a gallery set containing 19732 images.

DukeMTMC [28] utilizes eight cameras to collect 36411 images of 1404 identities. The training set includes 16522 images of 702 identities, and the testing set includes a query set containing 2228 images and a gallery set containing 17661 images.

Occluded-DukeMTMC [29] is an occluded dataset, which utilizes eight cameras to collect 35489 images of 1404 identities. The training set includes 15618 images of 702 identities, and the testing set includes a query set containing 2210 images and a gallery set containing 17661 images.

Partial iLIDS [30] is another occluded dataset, which includes 238 images of 119 identities. Each identity consists of an occluded image and a non-occluded image.

In the used datasets, all persons in the images are upright, so we only consider this upright situation.

Implementation details
Our network is initialized with the pre-trained ImageNet [31] model. We resize the input image to 256×128, and a random erasing strategy [32] and a random flipping strategy are applied in the experiment. Each training batch consists of sixteen identities, and each identity contains four images. We train the network with 160 epoch and utilize a warm-up learning rate strategy in the training progress. The warm-up learning rate is a variable learning rate strategy. In the first 10 training epochs, the learning rate gradually increases from 3.5×10−6 to 3.5×10−5. Then, it rises to 3.5×10−4 in the 10th epoch, then drops to 3.5×10−5 in the 40th epoch, and then drops to 3.5×10−6 in the 70th epoch, and keeps the same in the subsequent training. The experiment is implemented with GTX 2080Ti GPU. We use Market1501 as an example to analyze time complexity. When training the designed GFJN with the proposed TriBR loss, each epoch requires 186 iterations. Each iteration takes approximately 0.36s, so each epoch takes approximately 66.9s. It takes approximately 2.97h to train for 160 epochs. When testing our method, it takes about 94s to extract the features of the entire testing set. Calculating the query-gallery distance, the result is a 3368×15913 matrix, which takes about 4s. It takes about 10s to calculate CMC and mAP.

Comparison with the state of the art
The result comparing the proposed GFJN method with state-of-the-art methods is illustrated in Table 1. GFJN achieves 89.2% mAP with 95.8% Rank-1 on Market-1501, 79.7% mAP with 89.3% Rank-1 on DukeMTMC, which outperforms the related published results. Compared with the best method BAT [33], our method has 1.8% mAP with 0.7% Rank-1 gains on Market-1501, 2.4% mAP with 1.6% Rank-1 gains on DukeMTMC.

Table 1 Comparison with the state of the art on Market1501 and DukeMTMC
Full size table
Table 2 Comparison with the state of the art on Occluded-DukeMTMC
Full size table
Table 3 Comparison with the state of the art on Partial-iLIDS
Full size table
In order to further prove the reliability of GFJN, the experiment based on an occluded environment is conducted in Table 2 and Table 3.

As shown in Table 2, compared with the optimal method IVPPE [47], our method can achieve 9.3% mAP gain, 5.4% Rank-1 gain, 3% Rank-5 gain, and 3.1% Rank-10 gain on Occluded-DukeMTMC.

As shown in Table 3, compared with the optimal method HO [42], our method can achieve 9.9% Rank-1 gain and 3.5% Rank-3 gain on Partial-iLIDS.

From Tables 1, 2, 3, it appears that GFJN achieves an increased performance under a non-occluded environment and an occluded environment. It proves the effectiveness of our method.

Evaluation of feature fusion
GFJN can extract and join a global feature as well as three local features. A comparative experiment based on different features has been conducted to verify the effectiveness of the proposed fusion strategy. The performance on Market-1501 and DukeMTMC is illustrated in Fig. 11 “Global” represents a global feature. “Head”, “Body”, and “Leg” stand for three local features, respectively.

Fig. 11
figure 11
Evaluation of feature fusion. a Market-1501. b DukeMTMC

Full size image
Compared to the single global feature, our method achieves 1% mAP with 0.4% Rank-1 gains on Market-1501 and 1.3% mAP with 0.7% Rank-1 gains on DukeMTMC. With the introduction of more local features, the performance can be further improved. When all features are merged, the best performance can be obtained. This verifies the effectiveness of the proposed fusion strategy.

Evaluation of feature threshold
When calculating the weights for three local features, the feature threshold T needs to be set in Eq. (6). To evaluate the influence of different threshold values T on performance, we conducted an experiment comparing different thresholds. The result is shown in Table 4. “Direct fusion” represents fusing global feature and local features without global guidance, which means that the weights of the local features are not adjusted. This indicates the weight w𝐿 of all local features is set to 1. In this case, the final result of specifying or not specifying the threshold is the same, because the weight of the local feature has been set to a fixed value. “Median” represents the median number of feature points on the feature map. “Average” represents the average of feature points on the feature map.

Table 4 Evaluation of different feature threshold
Full size table
As shown in Table 4, different feature thresholds can achieve different performances. Direct fusion achieves 85.8% mAP with 94.5% Rank-1 and 75.2% mAP with 86.5% Rank-1 on Market-1501 and DukeMTMC, which has 3.4% mAP with 1.3% Rank-1 decline and 4.5% mAP with 2.8% Rank-1 decline compared to the best performance. Compared to direct fusion, no matter what kind of threshold is utilized to generate the weights for the local features, different performance improvements can always be obtained. This verifies the correctness of the proposed weight generation mechanism. The feature threshold T of zero corresponds to the best performance. This is because of the use of ReLU, each non-zero feature point has a contribution to the subsequent network.

Evaluation of bi-relevance
Different from TriHard loss, Bi-relevance TriHard (TriBR) loss combines Euclidean distance and angle information. Besides, both the self-relevance and the cross-relevance are considered, and the distance margin and the angle margin can be adaptively adjusted. To demonstrate the effectiveness of the proposed strategy, the result from comparative experiments based on TriBR is illustrated in Table 5.

Here, TriHard is the traditional loss, which corresponds to Eq. (11). “+SRE” denotes adding the self-relevance of the Euclidean distance, which corresponds to Eq. (31). “+SRA” denotes adding the self-relevance of the angle information, which corresponds to Eq. (32). “+CRE” denotes adding the cross-relevance of the Euclidean distance, which corresponds to Eq. (33). “+CRA” denotes adding the cross-relevance of the angle information, which corresponds to Eq. (34). “+DM” denotes adding the dynamic margin for the distance margin and the angle margin, which corresponds to Eq. (30).

Loss=1𝑃×𝐾∑𝑖=1𝑃×𝐾[(𝑤𝑠𝑒𝑝max(𝑑(𝑎,𝑝))−𝑤𝑠𝑒𝑛min(𝑑(𝑎,𝑛))+𝑚𝑒)+(max(𝜃(𝑎,𝑝))−min(𝜃(𝑎,𝑛))+𝑚𝜃)]+
(31)
Loss=1𝑃×𝐾∑𝑖=1𝑃×𝐾[(𝑤𝑠𝑒𝑝max(𝑑(𝑎,𝑝))−𝑤𝑠𝑒𝑛min(𝑑(𝑎,𝑛))+𝑚𝑒)+(𝑤𝑠𝑎𝑝max(𝜃(𝑎,𝑝))−𝑤𝑠𝑎𝑛min(𝜃(𝑎,𝑛))+𝑚𝜃)]+
(32)
Loss=1𝑃×𝐾∑𝑖=1𝑃×𝐾[(𝑤𝑠𝑒𝑝𝑤𝑐𝑒𝑝max(𝑑(𝑎,𝑝))−𝑤𝑠𝑒𝑛𝑤𝑐𝑒𝑛min(𝑑(𝑎,𝑛))+𝑚𝑒)+(𝑤𝑠𝑎𝑝max(𝜃(𝑎,𝑝))−𝑤𝑠𝑎𝑛min(𝜃(𝑎,𝑛))+𝑚𝜃)]+
(33)
Loss=1𝑃×𝐾∑𝑖=1𝑃×𝐾[(𝑤𝑠𝑒𝑝𝑤𝑐𝑒𝑝max(𝑑(𝑎,𝑝))−𝑤𝑠𝑒𝑛𝑤𝑐𝑒𝑛min(𝑑(𝑎,𝑛))+𝑚𝑒)+(𝑤𝑠𝑎𝑝𝑤𝑐𝑎𝑝max(𝜃(𝑎,𝑝))−𝑤𝑠𝑎𝑛𝑤𝑐𝑎𝑛min(𝜃(𝑎,𝑛))+𝑚𝜃)]+
(34)
Table 5 Evaluation of Bi-Relevance
Full size table
As shown in Table 5, traditional TriHard loss without dynamic penalty only obtains 86.4% mAP with 94.4% Rank-1 on Market-1501 and 75.9% mAP with 86.2% Rank-1 on DukeMTMC, which is lower than our proposed TriBR loss. Neither single self-relevance nor single cross-relevance can achieve optimal performance. With the introduction of the self-relevance and the cross-relevance, the performance can be improved gradually. Dynamically adjusting the distance margin and the angle margin obtain even further improvements on Market-1501 and DukeMTMC.

To further prove the effectiveness of the loss proposed in this paper, different loss variants are chosen to train our network (GFJN). The experimental result is illustrated in Table 6. TriHard is triplet loss based on hard-mining [11]. Quar is quadruplet loss [12]. MSML is margin sample mining loss [20]. AL is angular loss [24]. ALHSM is angular loss with hard sample mining [25].

Table 6 Evaluation of different loss variants
Full size table
It can be seen that the loss proposed in this work can achieve the best performance improvement. And compared with some loss variants in the list, our calculation is simpler and does not need to introduce more additional computational complexity, which proves the effectiveness of our designed loss.

Evaluation of difference threshold
When imposing a dynamical cross-relevance penalty on TriHard loss, the penalty weights will be generated based on the difference threshold 𝑇𝑒 and 𝑇𝑎 in Eq. (20), Eq. (21), Eq. (23), and Eq. (24). To evaluate the influence of the difference threshold on performance, an experiment based on the variable threshold has been conducted. The performance based on different distance thresholds and angle thresholds is shown in Fig. 12 and Fig. 13.

Fig. 12
figure 12
Evaluation of distance threshold. (a) Market-1501. (b) DukeMTMC

Full size image
Fig. 13
figure 13
Evaluation of angle threshold. a Market-1501. b DukeMTMC

Full size image
As shown in Fig. 12, the best performance can be obtained when 𝑇𝑒= 1.2 on Market-1501 and DukeMTMC. When 𝑇𝑒= 1, it means that as long as the distance of the hardest positive sample is greater than the distance of the hardest negative sample, the cross-relevance penalty is performed, which can obtain 88.6% mAP with 95.3% Rank-1 and 79.1% mAP with 88.9% Rank-1 on Market-1501 and DukeMTMC. Compared with the single self-relevance penalty, the cross-relevance penalty based on an arbitrary distance threshold can promote the performance. It proves the correctness of the cross-relevance penalty. As shown in Fig. 13, the best performance can be obtained with angle margin 𝑇𝑎=0.4𝜋 and 𝑇𝑎=0.3𝜋 on Market-1501 and DukeMTMC.

As can be seen from Figs. 12 and 13, the performance curves have the similar trends. With the increase of the distance threshold and the angle threshold, the performance first rises and then gradually declines and tends to converge.

Conclusions
A global-guided feature joint network consisting of a multiple feature extraction network, and a global-guided feature fusion network is designed in this work. The former can align body regions and extract a global feature as well as three local features. The latter utilizes the global feature to guide the adaptive fusion of the local features, which can evaluate the importance of different features dynamically to generate a more efficient feature descriptor. A Bi-relevance TriHard loss is designed to penalize the loss dynamically. TriBR combines Euclidean distance and angle information simultaneously, which considers the self-relevance of the intra-class samples and the cross-relevance of the inter-class samples. Besides, TriBR can adjust the distance margin and the angle margin adaptively to optimize the network. Hopefully, more discriminative features can be learned by the proposed TriBR to improve the performance. In summary, when combining the proposed joint network by global guidance and Bi-relevance TriHard loss, we can achieve 89.2% mAP with 95.8% Rank-1 on Market-1501, 79.7% mAP with 89.3% Rank-1 on DukeMTMC, 52.8% mAP and 61.7% Rank-1 on Occluded-DukeMTMC, 82.5% Rank-1 and 89.9% Rank-3 on Partial-iLIDS. Our method is superior to most person reidentification methods, which further verifies the correctness and the effectiveness of the proposed strategy.

Keywords
Person reidentification
Global guidance
Feature joint-network
Bi-relevance TriHard loss