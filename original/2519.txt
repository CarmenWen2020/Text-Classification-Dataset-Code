Localization has become one of the important techniques for Internet of Things (IoT). However, most existing localization methods need a central controller and operate on an off-line manner, which cannot satisfy the requirements of real-time IoT applications. In order to address this issue, a novel distributed localization scheme based on multi-agent reinforcement learning (MARL) is proposed. The localization problem is first reformulated as a stochastic game for maximizing the sum of the negative localization error. Each non-anchor node is then modeled as an intelligent agent, where its action space corresponds to possible locations. After that, we invoke a MARL framework on the basis of conventional Q-learning framework to learn the optimal policy, and to maximize the long-term expected reward. The novel strategy is also proposed to reduce the localization error. Extensive simulations demonstrate that the proposed localization method is superior to game theoretic-based distributed localization algorithm and virtual force-based distributed localization algorithm in terms of both localization accuracy and convergence speed, and is suitable for on-line localization scenarios.

Introduction
Internet of Things (IoT) is a network to enable the physical objects being smart [1], and can be applied in many real applications, such as smart shopping [2], home office automation [3], industrial control and monitoring. Node localization is a key issue for many IoT applications. For example, the sensing data in a fire-alarm application are meaningless without an accurate location [4]. In order to provide an accurate localization, many approaches have been proposed for IoT applications [5, 6]. However, more existing localization methods work on the assumption that the network is static, whereas off-line localization can be adopted. Therefore, it is still a significant challenge to locate the nodes of dynamic or mobile IoT applications with high localization accuracy [7].

Some researchers have resorted to the global localization system (GPS) to locate IoT nodes in outdoor scenarios. However, owing to the high cost and huge energy consumption, adding GPS infrastructure for each IoT node is still impractical. Therefore, GPS-free localization algorithms have been proposed. Typically, these algorithms can be classified into two types: range-free and range-based localization algorithms. For the former, nodes can be localized by estimating the hops between beacons and unknown nodes, whereas the distance of each hop is approximated by some other methods [8]. However, the distance estimation of each hop may import additional localization errors, and the localization accuracy may be very low [9]. For the latter, the nodes can be divided into two types: anchor nodes and non-anchor nodes. The anchor nodes are endowed with their ground truth locations through manual placement while the remaining nodes are called non-anchor nodes. The distances among the neighbor nodes are calculated by some additional techniques. For example, the distance can be estimated by Time of Arrival (TOA) [10], Time Difference of Arrival (TDOA) [11], and Received Signal Strength Indicator (RSSI) [12], etc. Among them, the TOA measures the distance between nodes by transmission time, which can provide an accurate distance estimation. However, it needs an extra hardware for accurate time synchronization. TDOA uses time difference to locate nodes. It also can provide a high accuracy of the distance estimation. However, time synchronization is still required. On the contrary, the RSSI-based distance estimation does not require additional hardware, and is easy to implement. However, a high distance estimation error may be employed due to the propagation model. How to improve the localization accuracy is still a challenge problem.

Trying to solve the problems raised above, numerous localization algorithms have been designed for IoT networks. For example, some researchers applied the evolutionary algorithms [13, 14] for wireless sensor networks (WSNs) to reduce the localization error. However, these methods are still energy consuming. In [15], authors proposed minimum weight low energy adaptive clustering hierarchy (MW-LEACH) to improve the energy efficiency of localization. However, an additional localization latency was imported. Different from traditional centralized localization methods, the localization problem was transformed into a distributed potential game, and a novel game theoretic-based distributed localization algorithm (GTDLA) was proposed in [16]. In [17], a virtual force-based distributed localization algorithm (VFDLA) was proposed, where the least square method was utilized to estimate the preliminary location, and distributed interaction based on virtual force was designed to save localization overhead. However, these above algorithms still can not be applied for dynamic real-time applications.

How to design dynamic real-time localization methods has become an important issue for many IoT applications. Considering that the dynamic optimization problems are usually more complex than conventional problems, some researchers tried to discretize the solution space [18], or simplify the secular solution to the periodic solution [19]. Markov chain is such a stochastic process that utilizes the discretization idea. Specifically, Markov chain is a discrete stochastic process with Markov property where the probability distribution of the next state can only be determined by the current state. Markov chains have been well applied in many studies in the past decade [20,21,22,23]. Similarly, Markov Decision Process (MDP) also utilizes the Markov property to represent the dynamic control problem, where the agent perceives the current system state and takes an action based on its learning strategy. As an important part of machine learning, reinforcement learning (RL) optimization problems are usually to solve problems represented by MDP [24].

Q-Learning is one of the typical RL algorithms and can find the optimal solutions for MDPs with small action and state space [24]. An important element of Q-learning is the learning strategy, which can be used to select an optimal action in correspondence with the state. In [25], authors proposed a novel action selection method called cuckoo action-selection (CAS) method, which can optimize the estimated value (Q-value) of each possible combination at current state, thus to increase the possibilities of finding better policies. In [26], the authors proposed the cooperative Q-value updating strategies based on standard Q-learning, where Q-value sharing strategies between reinforcement learners was presented to accelerate the learning process. In [27], a variant of the Q-learning named Deep Q Network (DQN) was proposed to solve the problem of excessive state space. In [28], a distributed hierarchical learning model-based cooperative Q-learning named QA-learning algorithm was proposed to accelerate the convergence speed. In [29], the Double Delayed Q-learning (2D Q-learning) is proposed to speed up the convergence of the algorithm. However, the 2D Q-learning may slightly underestimate the action values in the learning process.

Motivation and contributions
As discussed above, machine learning is promising to solve dynamic optimization problems. Several machine learning-based localization algorithms have been proposed [30,31,32], recently. However, most existing researches focus on the centralized approaches, which may result in a high time complexity for networks with large action space. As a contrast, MARL can provide a distributed perspective on each intelligent agent. It has been widely applied for several dynamic optimization of wireless networks. In [33, 34], authors studied the applications of MARL for cognitive radio networks. A MARL-based channel and power level selection algorithm for device-to-device (D2D) pairs in heterogeneous cellular networks is proposed in [35]. We therefore apply the MARL for distributed localization of IoT networks, where only local information is required.

Motivated by the promising potentials of MARL, we aim to develop a distributed and on-line localization framework based on MARL for IoT networks. In the considered IoT nodes network, each non-anchor node can be localized by interacting only with its nearby neighbor nodes, thus to improve the localization robustness and reduce the localization overhead. As for reinforcement learning algorithms, though deep reinforcement learning can be applied for problems with sufficient large action and state space, high computation resources is required. Considering the constrained computing capability and energy supply of IoT nodes, the conventional Q-learning is thus employed and executed at each non-anchor node. Moreover, by carefully designing the state space and action space, and constructing a Q-table for each non-anchor node, the proposed Q-learning algorithm can learn to the optimal policy with limited computation resources. To the best of our knowledge, this is the first work of IoT localization from the perspective of MARL, and the main contributions of this paper can be summarized as follows.

We formulate a stochastic game theory based MARL framework to model the dynamic localization of IoT, where each non-anchor node is viewed as an intelligent agent, and its action space corresponds to possible locations.

We develop a MARL-based distributed node localization algorithm to solve the stochastic game. In the proposed algorithm, each non-anchor node operate a Q-learning algorithm independently, and the localization results only be exchanged with their neighbors, thus to reduce energy consumed in the localization.

Simulation results demonstrate the proposed MARL-based algorithm performs better than the existing localization methods in term of both localization accuracy and convergence speed. Moreover, our algorithm can provide the localization results in real-time, and can be implemented for dynamic localization applications.

Organization
The rest of this paper is organized as follows. In Sect. 2, the network model for localization problem is proposed. We formulate the localization problem and present a stochastic game framework for the dynamic node localization in Sect. 3. In Section 4, a multi-agent Q-learning algorithm is proposed. Simulation results are shown in Sect. 5 and the conclusion is presented in Sect. 6.

Network model
We assumed there is a IoT network with N IoT nodes randomly deployed in a two-dimensional area with the size of 𝑅×𝑅. We denote the set of IoT nodes by ={𝑣1,𝑣2,…,𝑣𝑁}, where there are K non-anchor nodes and 𝑁−𝐾 anchor nodes. We denote the set of non-anchor nodes by ={𝑣1,𝑣2,…,𝑣𝐾}, and the set of anchor nodes by ={𝑣𝐾+1,𝑣𝐾+2,…,𝑣𝑁}, respectively. The anchor node is the node whose location is known a priori while the non-anchor node is the node whose location is unknown. We denote the location of node 𝑣𝑖∈ at time slot t by 𝑝𝑖(𝑡), where 𝑝𝑖(𝑡)=(𝑥𝑖(𝑡),𝑦𝑖(𝑡))∈𝑅2. We assume the communication radius of all nodes is the same, and denoted by 𝑅𝑐. After deployment, each node can determine its neighbor nodes, and we denote the set of neighbor nodes of 𝑣𝑖 by 𝑁𝑖(𝑡), which can be given by

𝑁𝑖(𝑡)={𝑣𝑗∈,𝑗≠𝑖:∥𝑟𝑖𝑗∥≤𝑅𝑐},
(1)
where ∥𝑟𝑖𝑗∥ = ∥𝑝𝑖(𝑡)−𝑝𝑗(𝑡)∥ denotes the Euclidean distance between 𝑣𝑖 and 𝑣𝑗, and ∥⋅∥ is the 2-norm operator. Generally, the non-anchor node 𝑣𝑖∈, can be localized directly when the number of anchor nodes in 𝑁𝑖 is greater than or equal to three. In this paper, we focus on the dynamic design of localization for IoT when the number of anchor nodes in 𝑁𝑖 is less than three. We assume the fading model is utilized, the distance measurement d between any two neighbor nodes at time slot t can be obtained by transforming its received RSSI. The distance measurement is given by

𝑑(𝑡)=10(∣𝑢−𝑅𝑆𝑆𝐼∣/10𝑧),
(2)
where u is the value of RSSI at 1m from the base station [36] and z is the index of pass loss. In this paper, we use 𝑑𝑖𝑗(𝑡) to denote the distance between 𝑣𝑖 and 𝑣𝑗 and 𝑒𝑖𝑗(𝑡) denotes the measurement error, at time slot t. Hence, at time slot t, the measurement distance is given by

𝑑𝑖𝑗(𝑡)=𝑟𝑖𝑗(𝑡)+𝑒𝑖𝑗(𝑡),
(3)
where the distance measurements are asymmetrical at each time slot t, such as 𝑑𝑖𝑗(𝑡)=𝑑𝑗𝑖(𝑡), for all 𝑣𝑖,𝑣𝑗∈. We assume that the measurement errors in the network are independent and follow the Gaussian distribution, 𝑒𝑖𝑗∼𝑁(0,𝜎2) when the average measurement error is 0 and the variance is 𝜎2.

The estimated location of each non-anchor node derived by the proposed localization algorithm at time slot t is represented by 𝑝ˆ𝑖(𝑡)=(𝑥ˆ𝑖(𝑡),𝑦ˆ𝑖(𝑡)). According to the estimated location, the corresponding estimated distance 𝑑ˆ𝑖𝑗(t) between 𝑣𝑖 and 𝑣𝑗 is given by

𝑑ˆ𝑖𝑗(𝑡)={∥𝑝ˆ𝑖(𝑡)−𝑝ˆ𝑗(𝑡)∥,∥𝑝ˆ𝑖(𝑡)−𝑝𝑗(𝑡)∥,∀𝑣𝑗∈,∀𝑣𝑗∈.
(4)
The square of the difference between the distance measurement and the estimated distance is defined as localization error at time slot t, which is denoted as 𝜎𝑖𝑗(𝑡) and it can be expressed as

𝜎𝑖𝑗(𝑡)=(𝑑𝑖𝑗(𝑡)−𝑑ˆ𝑖𝑗(𝑡))2.
(5)
We assume that the process of localization operates on the discrete time horizon and the partitions of time axis are called time slots which are equalization and non-overlapping. The symbol t denotes each time slot and symbol 𝑇𝑠 denotes the duration of the localization. During the time slot t, the localized non-anchor node 𝑣𝑖 computes its localization error according to the location selected at time slot 𝑡−1 and it should make a decision on its final location by the terminal time slot 𝑡+𝑇𝑠. We also assume the non-anchor nodes do not know the accurate localization duration. These features motivate us to develop a dynamic algorithm to solve the online and dynamic localization problem for IoT.

Stochastic game framework for dynamic localization
In this section, we first describe the investigated localization problem. Then, we formulate the problem of the node localization to be a stochastic game.

Problem formulation
Notice that, each non-anchor node in the IoT network only can directly communicate with its neighbor nodes. Therefore, we consider to divide the overall localization problem into several subproblems, and thus the localization problem of each non-anchor node is regarded as a subproblem. In each subproblem, non-anchor node 𝑣𝑖 executes its localization process independently. Hence, the localization problem of all non-anchor nodes from any time slot t can be formulated as

𝐦𝐢𝐧:∑𝑖=1𝑘∑𝑣𝑗∈𝑁𝑖𝜎𝑖𝑗(𝑡),∀𝑣𝑖∈.
(6)
As discussed in [24], the reward function of the learning problem indicates whether the decision made by the agent is good or bad for itself. Hence it is reasonable to define the negative value of localization error of 𝑣𝑖 as its reward function, and thus the reward function 𝑅𝑖(𝑡) of non-anchor node 𝑣𝑖 at time slot t can be expressed as

𝑅𝑖(𝑡)=−∑𝑣𝑗∈𝑁𝑖𝜎𝑖𝑗(𝑡),∀𝑣𝑖∈.
(7)
It can be observed from (7) that the instantaneous reward of non-anchor node 𝑣𝑖 relies on the location selection. In each subproblem, the higher the localization error, the smaller the reward for the localized non-anchor node. Next, we consider to maximize the long-term reward and we utilize the future discounted reward [37] as the measurement for each non-anchor node. Specifically, the future discounted reward is the sum of the reward in the current time slot, plus the discounted future reward. Hence, the long-term reward of non-anchor node 𝑣𝑖 can be expressed as

𝑈𝑖(𝑡)=∑𝜏=0+∞𝛾𝜏𝑅𝑖(𝑡+𝜏+1),
(8)
where 𝛾 denotes the discount factor and 𝛾∈[0,1). The long-term reward 𝑈𝑖(t) indicates the discounted sum of the reward from time slot t, which is used to evaluate the action carried out by the non-anchor node 𝑣𝑖.

Next, we denote the the action space of agent 𝑣𝑖∈ by 𝑖, which corresponds to all possible locations of non-anchor node 𝑣𝑖. Therefore, the goal of each non-anchor node is to maximize its long-term reward in (8) by choosing an optimal location 𝑎∗𝑖 (t) at each time slot t. Accordingly, the localization problem of each non-anchor node 𝑣𝑖, can be formulated as

𝑎∗𝑖(𝑡)=argmax𝑎𝑖∈𝑖𝑅𝑖(𝑡).
(9)
In order to solve the optimization problem (9), we try to formulate the localization problem to a non-cooperative stochastic game in the next subsection.

Stochastic game formulation
In this subsection, we try to use stochastic game to model the localization problem in (9). And we put the index t to the superscript for the symbol conciseness in the following of this paper.

In the process of localization, each non-anchor node is considered as an agent to learn its location by interacting with the environment. Therefore, the localization problem can be considered as a multi-agent game, in which multiple non-anchor nodes learn their optimal location independently.

The localization processes of all non-anchor nodes satisfy the properties of Markov chain, which means the reward of each non-anchor node only depends on the current action and state. In RL, the function of Markov chain is applied to describe the dynamics of the states of a stochastic game [38]. Specifically, the definition of Markov chain is given as follows.

Definition 1
A Markov chain with finite state is a discrete process, where the finite state is denoted by ={𝑠1,𝑠2,...,𝑠𝑒} and the 𝑒×𝑒 transition matrix is denoted by 𝐓. Each element in matrix 𝐓 satisfies 0≤𝑇𝑖,𝑧≤1 and ∑𝑒𝑧=1𝑇𝑖,𝑧=1 for any 1≤𝑖≤𝑒. The discrete process starts in a certain state and moves to another state successively. The probability of transition from state 𝑠𝑖 at time solt t to the state 𝑠𝑧 at time slot 𝑡+1 is given by

Pr{𝑠𝑡+1=𝑠𝑧|𝑠𝑡=𝑠𝑖}=𝑇𝑖,𝑧.
(10)
This transition only depends on the current state and not depend on the previous states, which is also called as Markov property.

Markov Decision Process (MDP) also has the Markov property, and multi-agent MDPs can also be generalized by stochastic game. In the proposed localization process, each non-anchor first takes an action according to the learning strategy, then receives a reward, i.e, the negative value of the total localization error. After that, it transits to a new state with the taken actions of its non-anchor neighbor nodes. Therefore, we can formulate the IoT localization problem as a stochastic game, and given in the following definition.

Definition 2
The stochastic game of each non-anchor node can be defined as a tuple G = {,,,T,} where

 denotes the set of players, i.e., the non-anchor nodes;

 is the state set with  = 1×2 ⋯×𝑘 and 𝑖 is the state set of each player 𝑣𝑖, for all 𝑣𝑖∈;

 is the action set with  = 1×2⋯×𝑘 and 𝑖 is the action set of player non-anchor node 𝑣𝑖;

T is the transition probability function and 𝑇(𝑠𝑡𝑖,𝑎𝑖,𝑠𝑡+1𝑖) denotes the transition probability of non-anchor node 𝑣𝑖 from state 𝑠𝑡𝑖 move to the next state 𝑠𝑡+1𝑖 by taking joint action a with 𝑎={𝑎1,𝑎2,⋯,𝑎𝑘}∈;

={𝑅1,…,𝑅𝑖} denotes the player’s reward function where 𝑅𝑖 is the reward function of non-anchor node 𝑣𝑖;

In the formulated stochastic game, 𝑖 is the action space of 𝑣𝑖, and the alternative action of each node 𝑣𝑖 at time slot t are conducted by randomly selecting n location within a certain area 𝑡𝑖, and 𝑡𝑖 is given by

𝑡𝑖=⎧⎩⎨⎪⎪𝑡𝑖,1,𝑡𝑖,2,𝑡𝑖,3,𝑁𝑖∩={∅},𝑁𝑖∩={𝑣𝑚},𝑁𝑖∩={𝑣𝑚,𝑣𝑧},
(11)
where 𝑡𝑖,1 denotes the whole area of IoT network; 𝑡𝑖,2 denotes a circular area with 𝑣𝑚 as the center and 𝑅𝑐 as the radius; 𝑡𝑖,3 denotes the overlapping area of two circular area with 𝑣𝑚 and 𝑣𝑧 as the center respectively and 𝑅𝑐 as the radius. For each agent 𝑣𝑖∈, the actions taken by other non-anchor nodes in 𝑁𝑖 at time slot t is denoted by 𝑎𝑡−𝑖, i.e., 𝑎𝑡−𝑖∈∖𝑖, =∏𝑣𝑖∈∩𝑁𝑖𝑖. After taking an action 𝑎𝑡𝑖 at time slot t, the state of node 𝑣𝑖 can be denote by 𝑠𝑡+1𝑖. Hence, the state 𝑠𝑡+1𝑖 observed by non-anchor node 𝑣𝑖 can be defined as

𝑠𝑡+1𝑖=𝑎𝑡𝑖
(12)
Moreover, the neighbor set of 𝑣𝑖 can be divided into two parts: (1) anchor nodes set, i.e., ∩𝑁𝑖; (2) non-anchor nodes set, i.e., ∩𝑁𝑖. Then, the reward function of each non-anchor node 𝑣𝑖 in (7), can be reformulated as

𝑟𝑡𝑖=𝑅𝑖(𝑎𝑡𝑖,𝑎𝑡−𝑖,𝑠𝑡𝑖)=−∑𝑣𝑚∈∩𝑁𝑖(𝑑𝑖𝑚−∥𝑎𝑡𝑖−𝑝𝑡𝑚∥)2−∑𝑣𝑗∈∩𝑁𝑖(𝑑𝑖𝑗−∥𝑎𝑡𝑖−𝑠𝑡𝑗∥)2,∀𝑣𝑖∈,
(13)
where 𝑣𝑗 and 𝑣𝑚 denote the anchor node neighbor and non-anchor node neighbor respectively. Here, the reward obtained by node 𝑣𝑖 depends on the 𝑠𝑡𝑖 and the actions (𝑎𝑡𝑖, 𝑎𝑡−𝑖) at any time slot t. Since the action selection of each node satisfy the Markov property, the probability of 𝑣𝑖 moves to the next state 𝑠𝑡+1𝑖 only depended on the current state 𝑠𝑡𝑖 and the actions (𝑎𝑡𝑖,𝑎𝑡−𝑖). In the localization process, non-anchor node 𝑣𝑖 can observe its own state 𝑠𝑡𝑖 and the action 𝑎𝑡𝑖 at any time slot t, while the actions taken by other players are not known.

In the stochastic game, the strategy 𝜋𝑖 denotes the mapping from the state set 𝑖 to the action set 𝑖. We use 𝜋𝑖( 𝑠𝑖) = {𝜋𝑖(𝑠𝑖,𝑎𝑖)|𝑎𝑖∈𝑖} to denote the strategy for non-anchor node 𝑣𝑖 in the state 𝑠𝑖. Specifically, each the strategy 𝜋𝑖(𝑠𝑖,𝑎𝑖) is the probability of taking the action 𝑎𝑖 in the state 𝑠𝑖 and 𝜋𝑖(𝑠𝑖,𝑎𝑖)=Pr(𝑎𝑡𝑖=𝑎𝑖∣𝑠𝑡𝑖 = 𝑠𝑖)∈[0,1]. 𝜋 = {𝜋1(𝑠1),𝜋2(𝑠2),⋯,𝜋𝑘(𝑠𝑘)} is the vector of strategies for k agents. Based on the above discussions, the goal of each non-anchor node in the strategy game is to maximize its expected reward. Hence, the problem (8) can be reformulated as

𝑉𝑖(𝑠,𝜋)=𝐸𝜋{∑𝜏=0+∞𝛾𝜏𝑟𝑡+𝜏+1𝑖∣𝑠𝑡𝑖=𝑠,𝜋},
(14)
where t represents the current time slot. 𝑟𝑡+𝜏+1 denotes the immediate reward at time slot 𝑡+𝜏+1. 𝛾 ∈ [0, 1] denotes the discount factor of reward. 𝐸{⋅} is the expectation operation and it represents the state transition by taking strategy 𝜋 from state s. In the proposed stochastic game, agents have individual expected rewards which could be affected by the strategy of other agents in their neighbor sets. Therefore, we cannot simply expect all agent to obtain their maximal expected reward at the same time. In general, Nash Equilibrium (NE) is a solution for a stochastic game. The definition of NE is given as follows.

Definition 3
If the collection of strategies 𝜋∗={𝜋∗1,𝜋∗2,...,𝜋∗𝑘} is a Nash Equilibrium [39], the state value function for each player 𝑣𝑖 satisfies

𝑉𝑖(𝜋∗𝑖,𝜋−𝑖)⩾𝑉𝑖(𝜋′𝑖,𝜋−𝑖),∀𝜋′𝑖,
(15)
where 𝜋′𝑖 denotes any possible strategy taken by 𝑣𝑖.

As shown in (15), each player can obtain the maximal expected reward in a Nash Equilibrium. Therefore, the goal of each player 𝑣𝑖 is to find a Nash Equilibrium for any state 𝑠𝑖 to ensure each player and its non-anchor neighbors obtain their accurate locations. In next section, we propose a framework for maximizing the expected reward in (14) of each agent.

Distribute localization based on multi-agent Q-learning algorithm
In this section, we establish a MARL framework firstly. Then, a Q-learning-based node localization algorithm proposed to maximize the expectation of long-term reward of each non-anchor node.

Multi-agent reinforcement learning framework
The MARL framework is shown in Fig. 1. In this figure, each node 𝑣𝑖 only communicates with the nodes in its neighbor set 𝑁𝑖 and observes its local state 𝑠𝑖 and reward 𝑟𝑖, then selecting the action 𝑎𝑖 according to the strategy 𝜋𝑖. As mentioned in the previous section, the state 𝑠𝑡+1𝑖 at time slot 𝑡+1 only depends on the state 𝑠𝑡𝑖 and the selected action 𝑎𝑡𝑖. The decision faced by an agent when the other agents choose a fixed strategy is the same as a MDP. As discussed above, we use Markov property to model the dynamic environment, MDP for non-anchor node 𝑣𝑖 consists of: (1) a discrete state set 𝑖, (2) a discrete action set 𝑖, (3) the state transition probabilities 𝑇(𝑠𝑡𝑖,𝑎𝑖,𝑠𝑡+1𝑖) for all 𝑎𝑖∈𝑖 and 𝑠𝑡𝑖,𝑠𝑡+1𝑖∈𝑖; (4) a reward function 𝑅𝑖, which denotes the next expected reward for non-anchor node 𝑣𝑖. We use Q-learning algorithm to solve the MDPs in this paper. In this case, each non-anchor node can be regarded as a learning agent, where its reward and transition function can not be obtained directly [24]. Next, we introduce the Q-learning algorithm for solving the MDP of a non-achor node in detail.

Fig. 1
figure 1
Multi-agent reinforcement learning framework

Full size image
Proposition 1
Each non-anchor node 𝑣𝑖∈ selects the strategy independently to maximize its own expected discounted reward, which can be defined as the state value function. For any state 𝑠𝑖 and strategy 𝜋, we have

𝑉𝑖(𝑠𝑖,𝜋)=∑𝑠′𝑖∈𝑖𝑇(𝑠𝑖,𝑎,𝑠′𝑖)∑𝑎𝑖∈𝑖∏𝑣𝑗∈∩𝑁𝑖𝜋𝑗(𝑠𝑗,𝑎𝑗)×[𝑅𝑖(𝑠𝑖,𝑎,𝑠′𝑖)+𝛾𝑉(𝑠′𝑖,𝜋)],
(16)
where 𝑠𝑡𝑖=𝑠𝑖 and 𝑠𝑡+1𝑖 = 𝑠′𝑖, with 𝑠𝑖, 𝑠′𝑖∈𝑖. 𝜋𝑗(𝑠𝑗,𝑎𝑗) denotes the probability of the selecting the action 𝑎𝑗 in the state 𝑠𝑗.

Proof
According to (14), we have

𝑉𝑖(𝑠𝑖,𝜋)=𝐸{𝑟𝑡+1𝑖∣𝑠𝑡𝑖=𝑠𝑖}+𝛾𝐸{∑𝜏=0+∞𝛾𝜏𝑟𝑡+𝜏+2𝑖∣𝑠𝑡𝑖=𝑠𝑖},
(17)
where the first part denotes the expected value and the second part denotes the state value function at time slot 𝑡+1. Let 𝑠𝑡𝑖 = 𝑠𝑖, 𝑎𝑡𝑖 = 𝑎𝑖 and 𝑠𝑡+1𝑖 = 𝑠′𝑖, the expected value can be expressed as

𝐸{𝑟𝑡+1𝑖∣𝑠𝑡𝑖=𝑠𝑖}=∑𝑠′𝑖∈𝑖𝑇(𝑠𝑖,𝑎,𝑠′𝑖)∑𝑎𝑖∈𝑖∏𝑣𝑗∈∩𝑁𝑖𝜋𝑗(𝑠𝑗,𝑎𝑗)×𝐸{𝑟𝑡+1𝑖∣𝑠𝑡𝑖=𝑠𝑖,𝑎𝑡𝑖=𝑎𝑖,𝑠𝑡+1𝑖=𝑠′𝑖}=∑𝑠′𝑖∈𝑖𝑇(𝑠𝑖,𝑎,𝑠′𝑖)∑𝑎𝑖∈𝑖∏𝑣𝑗∈∩𝑁𝑖𝜋𝑗(𝑠𝑗,𝑎𝑗)𝑅𝑖(𝑠𝑖,𝑎,𝑠′𝑖),
(18)
where 𝜋𝑗(𝑠𝑗,𝑎𝑗) denotes the probability of selecting the action 𝑎𝑗 in the state 𝑠𝑗 and 𝑣𝑗 is one of the non-anchor nodes in 𝑁𝑖. Similarly, the second part in (17) can be rewritten into

𝐸{∑𝜏=0+∞𝛾𝜏𝑟𝑡+𝜏+2𝑖∣𝑠𝑡𝑖=𝑠𝑖}=∑𝑠′𝑖∈𝑖𝑇(𝑠𝑖,𝑎,𝑠′𝑖)∑𝑎𝑖∈𝑖∏𝑣𝑗∈∩𝑁𝑖𝜋𝑗(𝑠𝑗,𝑎𝑗)×𝐸{∑𝜏=0+∞𝛾𝜏𝑟𝑡+𝜏+2𝑖∣𝑠𝑡𝑖=𝑠𝑖,𝑎𝑡𝑖=𝑎𝑖,𝑠𝑡+1𝑖=𝑠′𝑖}=∑𝑠′𝑖∈𝑖𝑇(𝑠𝑖,𝑎,𝑠′𝑖)∑𝑎𝑖∈𝑖∏𝑣𝑗∈∩𝑁𝑖𝜋𝑗(𝑠𝑗,𝑎𝑗)𝑉(𝑠′𝑖,𝜋).
(19)
Substituting (18) and (19) into (17), we can get (16). ◻

Therefore, the state value function in (16) denotes the expected reward when taking the strategy 𝜋 from the state 𝑠𝑖. Based on (16), we can derive the action value function. i.e., Q-function, which is the cumulative expected reward under the policy 𝜋, taking action 𝑎𝑖, from the state 𝑠𝑖, and it can be expressed as

𝑄𝑖(𝑠𝑖,𝑎𝑖,𝜋)=𝐸{∑𝜏=0+∞𝛾𝜏𝑟𝑡+𝜏+1∣𝑠𝑡𝑖=𝑠,𝑎𝑡𝑖=𝑎𝑖}=𝐸{𝑟𝑡+1𝑖+𝛾∑𝜏=0+∞𝛾𝜏𝑟𝑡+𝜏+2𝑖∣𝑠𝑡𝑖=𝑠𝑖,𝑎𝑡𝑖=𝑎𝑖,𝑠𝑡+1𝑖=𝑠′𝑖}=∑𝑠′𝑖∈𝑖𝑇(𝑠𝑖,𝑎,𝑠′𝑖)∑𝑎−𝑖∈−𝑖∏𝑣𝑗∈∩𝑁𝑖𝜋𝑗(𝑠𝑗,𝑎𝑗)×[𝑅𝑖(𝑠𝑖,𝑎,𝑠′𝑖)+𝛾𝑉𝑖(𝑠′𝑖,𝜋)],
(20)
where the value of (20) is called Q-value. In the action value function (20), the Q-value depends on the actions of all non-anchor nodes in its neighbor set 𝑁𝑖. And the condition in (20) holds between two consistency state 𝑠𝑡𝑖 and 𝑠𝑡+1𝑖. As discussed above, we can conclude the relationship between state values and Q-values:

𝑉𝑖(𝑠𝑖,𝜋)=∑𝑎𝑖∈𝑖𝜋𝑖(𝑠𝑖,𝑎𝑖)𝑄𝑖(𝑠𝑖,𝑎𝑖,𝜋).
(21)
Note that the goal of each agent in MDP is to find a strategy to maximize its expected reward. That is, the goal of each non-anchor node in MDP is to find a strategy to minimize its localization error. Under the optimal strategy 𝜋∗, the state value function for non-anchor node 𝑣𝑖 at state 𝑠𝑖 can be defined as

𝑉∗𝑖(𝑠𝑖)=max𝜋i𝑉𝑖(𝑠𝑖,𝜋),𝑠𝑖∈𝑖.
(22)
And the optimal action value function also can be defined as

𝑄∗𝑖(𝑠𝑖,𝑎𝑖)=max𝜋i𝑄𝑖(𝑠𝑖,𝑎𝑖,𝜋),𝑠𝑖∈𝑖,𝑎𝑖∈𝑖.
(23)
Substituting (21)–(22), the Eq. (22) can be rewritten as

𝑉∗𝑖(𝑠𝑖)=max𝑎𝑖𝑄∗𝑖(𝑠𝑖,𝑎𝑖).
(24)
Based on (24), we can combining it with (16) to derive the Bellman optimality equation for state value function, which can be expressed as

𝑉∗𝑖(𝑠𝑖)=∑𝑎−𝑖∈−𝑖∏𝑣𝑗∈∩𝑁𝑖𝜋𝑗(𝑠𝑗,𝑎𝑗)×max𝑎𝑖∑𝑠′𝑖𝑇(𝑠𝑖,𝑎,𝑠′𝑖)[𝑅𝑖(𝑠𝑖,𝑎𝑖,𝑠′𝑖)+𝛾𝑉∗𝑖(𝑠′𝑖)].
(25)
And by the combining (24) with (20), we also can derive the Bellman optimality equation for action value function, which can be expressed as

𝑄∗𝑖(𝑠𝑖,𝑎𝑖)=∑𝑎−𝑖∈−𝑖∏𝑣𝑗∈∩𝑁𝑖𝜋𝑗(𝑠𝑗,𝑎𝑗)×∑𝑠′𝑖𝑇(𝑠𝑖,𝑎,𝑠′𝑖)[𝑅𝑖(𝑠𝑖,𝑎𝑖,𝑠′𝑖)+𝛾max𝑎′𝑖𝑄∗(𝑠′𝑖,𝑎′𝑖)].
(26)
It can be observed in (26), under the optimal strategy, each non-anchor node 𝑣𝑖 selects the optimal action to maximize the value of Q-function at current state. However, the optimal Q-function of each non-anchor node 𝑣𝑖 depends on the actions and the policy taken by its non-anchor neighbors, which makes it difficult to find the optimal strategy [40]. Trying to solve the complex problem, we assume non-anchor nodes are independent learners (ILs), which means the non-anchor nodes do not known the actions and the rewards of other non-anchor nodes.

Distributed localization based on Q-learning
In this subsection, Q-learning is applied to solve the MDPs and a Q-learning-based distributed node localization algorithm is proposed to solve the node localization problem.

Q-learning is an effective reinforcement learning method, which is first proposed in Ref. [41], and the stability of the algorithm based on random approximation was proved in Ref. [42]. Based on these advantages of Q-learning, we use it to solve the MDPs. In the learning process, each non-anchor node runs the Q-learning independently to learn its optimal Q-value and to find the optimal policy for the MDP. Specifically, the action selection strategy in any time slot t depends on the Q-function of state 𝑠𝑡 and 𝑠𝑡+1. Therefore, the Q-value indicates the future quality of the actions in next state. In the proposed algorithm, the update rule for Q-value of Q-learning [24] is given by

𝑄𝑡+1𝑖(𝑠𝑖,𝑎𝑖)=(1−𝛼𝑡)𝑄𝑡𝑖(𝑠𝑖,𝑎𝑖)+𝛼𝑡[𝑟𝑡𝑖+𝛾max𝑎′𝑖∈𝑖𝑄𝑡𝑖(𝑠′𝑖,𝑎′𝑖)],
(27)
with 𝑠𝑖 = 𝑠𝑡𝑖, 𝑎𝑖 = 𝑎𝑡𝑖 and 𝑠′𝑖 = 𝑠𝑡+1𝑖, 𝑎′𝑖 = 𝑎𝑡+1𝑖. 𝛼𝑡 is the learning rate at time slot t. To ensure the convergence of Q-learning, we set 𝛼𝑡 as in [42], which is given by

𝛼𝑡=1𝑡+𝑢𝛼𝜉𝛼,
(28)
where 𝑢𝛼>0, 𝜉𝛼∈ (12, 1].

Moreover, another important element of Q-learning is strategy 𝜋, which is used to select an action for the agent during the learning process and 𝜋∈[0,1]. In this paper, we proposed a new strategy to find the optimal policy. Note that, the action selection of the non-anchor node 𝑣𝑖 is only related to the nodes in its neighbor set 𝑁𝑖. Therefore, we consider to use the weighted localization errors to represent different influence of anchor neighbors and non-anchor neighbors on 𝑣𝑖. In this case, a higher weight is assigned to the anchor neighbors and a lower weight is assigned to the non-anchor neighbors, denoted by 𝑤1 and 𝑤2 respectively. As mentioned above, n denotes the number of the alternative actions at time slot t. Let 𝑎𝑡𝑖,𝑥,𝑥∈[1,𝑛], denote the alternative action of 𝑣𝑖 at time slot t. Let 𝑒𝑟𝑟𝑡𝑖,𝑥 denote the weighted localization error. Hence, the weighted localization error can be expressed as

𝑒𝑟𝑟𝑡𝑖,𝑥=𝑤1∑𝑣𝑚∈∩𝑁𝑖(𝑑𝑖𝑚−∥𝑎𝑡𝑖,𝑥−𝑝𝑡𝑚∥)2+𝑤2∑𝑣𝑗∈∩𝑁𝑖(𝑑𝑖𝑗−∥𝑎𝑡𝑖,𝑥−𝑠𝑡𝑗∥)2,∀𝑣𝑖∈,
(29)
where 𝑝𝑡𝑚 denotes the location of anchor node 𝑣𝑚. 𝑑𝑖𝑚 denotes the distance measurement between 𝑣𝑖 and 𝑣𝑚. 𝑑𝑖𝑗 denotes the distance measurement between 𝑣𝑖 and 𝑣𝑗. Based on (29), at time slot t, the probability of selecting the action 𝑎𝑡𝑖,𝑥 at state 𝑠𝑡𝑖 can be expressed as

𝜋𝑖(𝑠𝑡𝑖,𝑎𝑡𝑖,𝑥)=∑𝑛𝑠=1𝑒𝑟𝑟𝑡𝑖,𝑠−𝑒𝑟𝑟𝑡𝑖,𝑥(𝑛−1)∗∑𝑛𝑠=1𝑒𝑟𝑟𝑡𝑖,𝑠.
(30)
It can be observed in (29) and (30), at any time slot t, the lower weighted localization error is, the higher the probability of selecting the action 𝑎𝑡𝑖,𝑥.

In the proposed localization algorithm, each non-anchor node learns its optimal location by running the Q-learning algorithm independently and maintains its own Q-table which is the set of Q-values. Since the non-anchor nodes have no prior knowledge of the initial state, each non-anchor node randomly selects an action from the region 𝑡𝑖 at first. The process of the Q-learning-based distributed node localization algorithm is shown in Algorithm 1.

figure a
Convergence performance
In this subsection, we investigate the convergence of the proposed localization algorithm. The convergence of the algorithm is concluded in Proposition 2.

Proposition 2
In the proposed Algorithm 1, the Q-learning procedure for each agent is converged to its optimal Q-value.

We proof Proposition 2 in the following contents. Notice that, each agent execute its individual Q-learning independently, the convergence of the MARL algorithm is determined by the convergence of the Q-learning. Therefore, we only need to proof the convergence for the Q-learning in the proposed Algorithm 1.

Theorem 1
The Q-learning algorithm in Algorithm 1 converges to the optimal Q-value 𝑄∗𝑖(𝑠𝑖,𝑎𝑖) if

1.
The state and action space for each agent is finite;

2.
∑+∞𝑡=0𝛼𝑡=∞, ∑+∞𝑡=0(𝛼𝑡)2<∞ with probability 1;

3.
The value of 𝑟𝑡𝑖 is bounded;

Proof
See Appendix A. ◻

Numerical results
Experiment results
In order to evaluate the performance of the proposed MARL-based algorithm, we invoke the mean localization error (MLE) as the localization evaluation, which is given by

MLE=1𝐾∑𝑖=1𝐾∥𝑝𝑖ˆ−𝑝𝑖∥.
(31)
At first, we randomly deploy 150 non-anchor nodes and 15 anchor nodes in a 100×100 m area. The communication radius 𝑅𝑐 is set as 20m. We assume that the measurement error 𝑒𝑖𝑗 is randomly generated by a Gaussian distribution with a mean of 0 and a variance of 𝛿2. Here, we set the mean value of measurement error 𝑒𝑖𝑗 as 0, and the variance 𝛿2 as 0.5. Additionally, we set 𝑢𝛼 as 0.5, 𝜉𝛼 as 0.7 and 𝛾 as 0.9, respectively.

Fig. 2
figure 2
Initial distribution of nodes and final localization results

Full size image
Figure 2a plots the initial deployment of the network and the connection between non-anchor nodes and anchor nodes, where the anchor nodes and the ground truth locations of non-anchor nodes are shown by asterisks and circles, respectively, the connections between them are shown by blue lines. Figure 2b plots the final localization results which are calculated by the proposed MARL-based localization algorithm, where the anchor nodes and the ground truth locations of non-anchor nodes are shown by asterisks and circles respectively, the estimated locations of non-anchor nodes and the localization error are shown by plus and pink lines. From these two figures, it can be observed that almost all non-anchor nodes can be accurately localized.

Figure 3 plots the performance comparison of system localization error versus different number of anchor nodes with fixed number of non-anchor nodes. Here, the system localization error refers to the accumulative localization error of all non-anchor nodes. It can be observed that the system localization error decreases with the increase of time slot and finally converges to the optimal solution within 40 iterations for all scenarios. This can be explained by the fact that the MARL operates with a distributed manner, thus is more suitable for large scale localization. Though better performance can be obtained with large number of anchor nodes initially, the final converged performance of all these scenarios all almost the same. This also showcase that it does not require more anchor nodes to locate the non-anchor nodes.

Figure 4 plots the MLE versus different communication radius. Here, we set the number of anchor nodes as 15, and the number of non-anchor nodes as 150, and four different communication radii are tested. It can be observed that the MLE converges to the minimum when the communication radius is set to 30 m. Moreover, it also can be observed that the algorithm is more stable when the communication radius is set to 30 m. We thus can conclude that increasing the communication radius is helpful to improve the localization accuracy, and obtain a more stable localization result. This can be explained by the fact that more connectives are obtained with a large communication radius.

Fig. 3
figure 3
Comparisons of system localization error among different number of anchor nodes

Full size image
Fig. 4
figure 4
The box plots of Average localization error under different communication radii

Full size image
Performance comparison
In this subsection, we further compare the performance of the proposed MARL-based algorithm with VFDLA and GTDLA with the same network topology, where the size of the network is set to 100×100 m, 𝑅𝑐 is set to 20 m, and the proportion of anchor nodes is set to 10% of the number of non-anchor nodes..

Figure 5 plots the converge performance of these three algorithms. It can be observed that MARL-based localization algorithm outperforms other two algorithms, while VFDLA performs the worst. Another observation is that MARL-based algorithm converges faster than the other two algorithms. In order to further test the performance of energy consumption of these three algorithms, the number of communications required for localization is also compared. Here, four network typologies with different numbers of IoT nodes are tested. The performance comparison is shown in Table 1. From Table 1, we can observe that the number of communications increase with increasing the number of nodes. Moreover, the number of communications of our algorithm is obviously less than the other two algorithms. Since the energy is mainly consumed in communications between IoT nodes, it can be concluded that our algorithm is more energy efficient than the other two algorithms.

Figure 6 plots the MLE comparison of these three algorithms versus different communication radius. It can be observed the three algorithms can obtain accurate localization with a large communication radius. This can be explained by the fact that more node connectives can be obtained with a large the communication radius. Moreover, comparing with VFDLA and GTDLA, the MLE of MARL-based algorithm is the smallest for all communication radii.

Fig. 5
figure 5
Converge curve of different localization algorithms

Full size image
Table 1 Comparison of the number of communications
Full size table
Fig. 6
figure 6
Comparison of MLE under different communication radii

Full size image
Fig. 7
figure 7
Comparison of MLE under different anchor nodes

Full size image
Fig. 8
figure 8
Comparison of MLE under different range errors

Full size image
Figure 7 plots the MLE comparison of these three algorithms versus different anchor nodes. Here the number of non-anchor nodes is ranged from 110 to 220. It can be observed that the MLE decreases with increasing the number of anchor nodes. Since the proportion between anchor and non-anchor nodes is fixed, the increase of the anchor nodes brings about an increase of non-anchor nodes. This means that more non-anchor node which are already localized can be further utilized as the anchor nodes, thus to further improve the localization accuracy. Moreover, it also can be observed that the performance of the proposed MARL-based algorithm outperforms GTDLA and VFDLA, respectively.

Figure 8 plots the MLE comparison of these three algorithms versus different measurement error. As shown in Fig. 8, the MLE of the three algorithms increase with increasing the measurement error. However, the MLE curve of the proposed MARL-based algorithm is relative smooth. It also can be observed that the performance of the proposed MARL-based algorithm is similar to that of the VFDLA algorithm with small measurement error. However, when the measurement error continues to increase, the proposed MARL-based algorithm performs better than the other two algorithms.

Conclusion
This paper mainly investigated the distributed localization problem in IoT. We established the distributed node localization model and proposed the updating formula for this model-based on Q-learning. Moreover, we proposed a dynamic strategy so that the optimal policy can be carried out without taking more exploratory actions. Then, we proposed a distributed node localization algorithm based on MARL to solve the formulated stochastic game and the simulation verified that the localization error can converge to a low value in a short time. Finally, the simulation results of several comparative experiments shown that the localization accuracy of the proposed algorithm is better than the existing node localization algorithms in different aspects. Since the localization can further be enhanced by node coordination, our future work will consider the joint coordinated based localization method, where the trade-off between coordination and the signal overhead will be jointly optimized.

Keywords
Distributed localization
Q-learning
Internet of things (IoT)
Multi-agent reinforcement learning