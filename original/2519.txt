Localization has become one of the important techniques for Internet of Things (IoT). However, most existing localization methods need a central controller and operate on an off-line manner, which cannot satisfy the requirements of real-time IoT applications. In order to address this issue, a novel distributed localization scheme based on multi-agent reinforcement learning (MARL) is proposed. The localization problem is first reformulated as a stochastic game for maximizing the sum of the negative localization error. Each non-anchor node is then modeled as an intelligent agent, where its action space corresponds to possible locations. After that, we invoke a MARL framework on the basis of conventional Q-learning framework to learn the optimal policy, and to maximize the long-term expected reward. The novel strategy is also proposed to reduce the localization error. Extensive simulations demonstrate that the proposed localization method is superior to game theoretic-based distributed localization algorithm and virtual force-based distributed localization algorithm in terms of both localization accuracy and convergence speed, and is suitable for on-line localization scenarios.

Introduction
Internet of Things (IoT) is a network to enable the physical objects being smart [1], and can be applied in many real applications, such as smart shopping [2], home office automation [3], industrial control and monitoring. Node localization is a key issue for many IoT applications. For example, the sensing data in a fire-alarm application are meaningless without an accurate location [4]. In order to provide an accurate localization, many approaches have been proposed for IoT applications [5, 6]. However, more existing localization methods work on the assumption that the network is static, whereas off-line localization can be adopted. Therefore, it is still a significant challenge to locate the nodes of dynamic or mobile IoT applications with high localization accuracy [7].

Some researchers have resorted to the global localization system (GPS) to locate IoT nodes in outdoor scenarios. However, owing to the high cost and huge energy consumption, adding GPS infrastructure for each IoT node is still impractical. Therefore, GPS-free localization algorithms have been proposed. Typically, these algorithms can be classified into two types: range-free and range-based localization algorithms. For the former, nodes can be localized by estimating the hops between beacons and unknown nodes, whereas the distance of each hop is approximated by some other methods [8]. However, the distance estimation of each hop may import additional localization errors, and the localization accuracy may be very low [9]. For the latter, the nodes can be divided into two types: anchor nodes and non-anchor nodes. The anchor nodes are endowed with their ground truth locations through manual placement while the remaining nodes are called non-anchor nodes. The distances among the neighbor nodes are calculated by some additional techniques. For example, the distance can be estimated by Time of Arrival (TOA) [10], Time Difference of Arrival (TDOA) [11], and Received Signal Strength Indicator (RSSI) [12], etc. Among them, the TOA measures the distance between nodes by transmission time, which can provide an accurate distance estimation. However, it needs an extra hardware for accurate time synchronization. TDOA uses time difference to locate nodes. It also can provide a high accuracy of the distance estimation. However, time synchronization is still required. On the contrary, the RSSI-based distance estimation does not require additional hardware, and is easy to implement. However, a high distance estimation error may be employed due to the propagation model. How to improve the localization accuracy is still a challenge problem.

Trying to solve the problems raised above, numerous localization algorithms have been designed for IoT networks. For example, some researchers applied the evolutionary algorithms [13, 14] for wireless sensor networks (WSNs) to reduce the localization error. However, these methods are still energy consuming. In [15], authors proposed minimum weight low energy adaptive clustering hierarchy (MW-LEACH) to improve the energy efficiency of localization. However, an additional localization latency was imported. Different from traditional centralized localization methods, the localization problem was transformed into a distributed potential game, and a novel game theoretic-based distributed localization algorithm (GTDLA) was proposed in [16]. In [17], a virtual force-based distributed localization algorithm (VFDLA) was proposed, where the least square method was utilized to estimate the preliminary location, and distributed interaction based on virtual force was designed to save localization overhead. However, these above algorithms still can not be applied for dynamic real-time applications.

How to design dynamic real-time localization methods has become an important issue for many IoT applications. Considering that the dynamic optimization problems are usually more complex than conventional problems, some researchers tried to discretize the solution space [18], or simplify the secular solution to the periodic solution [19]. Markov chain is such a stochastic process that utilizes the discretization idea. Specifically, Markov chain is a discrete stochastic process with Markov property where the probability distribution of the next state can only be determined by the current state. Markov chains have been well applied in many studies in the past decade [20,21,22,23]. Similarly, Markov Decision Process (MDP) also utilizes the Markov property to represent the dynamic control problem, where the agent perceives the current system state and takes an action based on its learning strategy. As an important part of machine learning, reinforcement learning (RL) optimization problems are usually to solve problems represented by MDP [24].

Q-Learning is one of the typical RL algorithms and can find the optimal solutions for MDPs with small action and state space [24]. An important element of Q-learning is the learning strategy, which can be used to select an optimal action in correspondence with the state. In [25], authors proposed a novel action selection method called cuckoo action-selection (CAS) method, which can optimize the estimated value (Q-value) of each possible combination at current state, thus to increase the possibilities of finding better policies. In [26], the authors proposed the cooperative Q-value updating strategies based on standard Q-learning, where Q-value sharing strategies between reinforcement learners was presented to accelerate the learning process. In [27], a variant of the Q-learning named Deep Q Network (DQN) was proposed to solve the problem of excessive state space. In [28], a distributed hierarchical learning model-based cooperative Q-learning named QA-learning algorithm was proposed to accelerate the convergence speed. In [29], the Double Delayed Q-learning (2D Q-learning) is proposed to speed up the convergence of the algorithm. However, the 2D Q-learning may slightly underestimate the action values in the learning process.

Motivation and contributions
As discussed above, machine learning is promising to solve dynamic optimization problems. Several machine learning-based localization algorithms have been proposed [30,31,32], recently. However, most existing researches focus on the centralized approaches, which may result in a high time complexity for networks with large action space. As a contrast, MARL can provide a distributed perspective on each intelligent agent. It has been widely applied for several dynamic optimization of wireless networks. In [33, 34], authors studied the applications of MARL for cognitive radio networks. A MARL-based channel and power level selection algorithm for device-to-device (D2D) pairs in heterogeneous cellular networks is proposed in [35]. We therefore apply the MARL for distributed localization of IoT networks, where only local information is required.

Motivated by the promising potentials of MARL, we aim to develop a distributed and on-line localization framework based on MARL for IoT networks. In the considered IoT nodes network, each non-anchor node can be localized by interacting only with its nearby neighbor nodes, thus to improve the localization robustness and reduce the localization overhead. As for reinforcement learning algorithms, though deep reinforcement learning can be applied for problems with sufficient large action and state space, high computation resources is required. Considering the constrained computing capability and energy supply of IoT nodes, the conventional Q-learning is thus employed and executed at each non-anchor node. Moreover, by carefully designing the state space and action space, and constructing a Q-table for each non-anchor node, the proposed Q-learning algorithm can learn to the optimal policy with limited computation resources. To the best of our knowledge, this is the first work of IoT localization from the perspective of MARL, and the main contributions of this paper can be summarized as follows.

We formulate a stochastic game theory based MARL framework to model the dynamic localization of IoT, where each non-anchor node is viewed as an intelligent agent, and its action space corresponds to possible locations.

We develop a MARL-based distributed node localization algorithm to solve the stochastic game. In the proposed algorithm, each non-anchor node operate a Q-learning algorithm independently, and the localization results only be exchanged with their neighbors, thus to reduce energy consumed in the localization.

Simulation results demonstrate the proposed MARL-based algorithm performs better than the existing localization methods in term of both localization accuracy and convergence speed. Moreover, our algorithm can provide the localization results in real-time, and can be implemented for dynamic localization applications.

Organization
The rest of this paper is organized as follows. In Sect. 2, the network model for localization problem is proposed. We formulate the localization problem and present a stochastic game framework for the dynamic node localization in Sect. 3. In Section 4, a multi-agent Q-learning algorithm is proposed. Simulation results are shown in Sect. 5 and the conclusion is presented in Sect. 6.

Network model
We assumed there is a IoT network with N IoT nodes randomly deployed in a two-dimensional area with the size of ğ‘…Ã—ğ‘…. We denote the set of IoT nodes by î‰‚={ğ‘£1,ğ‘£2,â€¦,ğ‘£ğ‘}, where there are K non-anchor nodes and ğ‘âˆ’ğ¾ anchor nodes. We denote the set of non-anchor nodes by îˆ·={ğ‘£1,ğ‘£2,â€¦,ğ‘£ğ¾}, and the set of anchor nodes by îˆ¹={ğ‘£ğ¾+1,ğ‘£ğ¾+2,â€¦,ğ‘£ğ‘}, respectively. The anchor node is the node whose location is known a priori while the non-anchor node is the node whose location is unknown. We denote the location of node ğ‘£ğ‘–âˆˆî‰‚ at time slot t by ğ‘ğ‘–(ğ‘¡), where ğ‘ğ‘–(ğ‘¡)=(ğ‘¥ğ‘–(ğ‘¡),ğ‘¦ğ‘–(ğ‘¡))âˆˆğ‘…2. We assume the communication radius of all nodes is the same, and denoted by ğ‘…ğ‘. After deployment, each node can determine its neighbor nodes, and we denote the set of neighbor nodes of ğ‘£ğ‘– by ğ‘ğ‘–(ğ‘¡), which can be given by

ğ‘ğ‘–(ğ‘¡)={ğ‘£ğ‘—âˆˆî‰‚,ğ‘—â‰ ğ‘–:âˆ¥ğ‘Ÿğ‘–ğ‘—âˆ¥â‰¤ğ‘…ğ‘},
(1)
where âˆ¥ğ‘Ÿğ‘–ğ‘—âˆ¥ = âˆ¥ğ‘ğ‘–(ğ‘¡)âˆ’ğ‘ğ‘—(ğ‘¡)âˆ¥ denotes the Euclidean distance between ğ‘£ğ‘– and ğ‘£ğ‘—, and âˆ¥â‹…âˆ¥ is the 2-norm operator. Generally, the non-anchor node ğ‘£ğ‘–âˆˆîˆ·, can be localized directly when the number of anchor nodes in ğ‘ğ‘– is greater than or equal to three. In this paper, we focus on the dynamic design of localization for IoT when the number of anchor nodes in ğ‘ğ‘– is less than three. We assume the fading model is utilized, the distance measurement d between any two neighbor nodes at time slot t can be obtained by transforming its received RSSI. The distance measurement is given by

ğ‘‘(ğ‘¡)=10(âˆ£ğ‘¢âˆ’ğ‘…ğ‘†ğ‘†ğ¼âˆ£/10ğ‘§),
(2)
where u is the value of RSSI at 1m from the base station [36] and z is the index of pass loss. In this paper, we use ğ‘‘ğ‘–ğ‘—(ğ‘¡) to denote the distance between ğ‘£ğ‘– and ğ‘£ğ‘— and ğ‘’ğ‘–ğ‘—(ğ‘¡) denotes the measurement error, at time slot t. Hence, at time slot t, the measurement distance is given by

ğ‘‘ğ‘–ğ‘—(ğ‘¡)=ğ‘Ÿğ‘–ğ‘—(ğ‘¡)+ğ‘’ğ‘–ğ‘—(ğ‘¡),
(3)
where the distance measurements are asymmetrical at each time slot t, such as ğ‘‘ğ‘–ğ‘—(ğ‘¡)=ğ‘‘ğ‘—ğ‘–(ğ‘¡), for all ğ‘£ğ‘–,ğ‘£ğ‘—âˆˆî‰‚. We assume that the measurement errors in the network are independent and follow the Gaussian distribution, ğ‘’ğ‘–ğ‘—âˆ¼ğ‘(0,ğœ2) when the average measurement error is 0 and the variance is ğœ2.

The estimated location of each non-anchor node derived by the proposed localization algorithm at time slot t is represented by ğ‘Ë†ğ‘–(ğ‘¡)=(ğ‘¥Ë†ğ‘–(ğ‘¡),ğ‘¦Ë†ğ‘–(ğ‘¡)). According to the estimated location, the corresponding estimated distance ğ‘‘Ë†ğ‘–ğ‘—(t) between ğ‘£ğ‘– and ğ‘£ğ‘— is given by

ğ‘‘Ë†ğ‘–ğ‘—(ğ‘¡)={âˆ¥ğ‘Ë†ğ‘–(ğ‘¡)âˆ’ğ‘Ë†ğ‘—(ğ‘¡)âˆ¥,âˆ¥ğ‘Ë†ğ‘–(ğ‘¡)âˆ’ğ‘ğ‘—(ğ‘¡)âˆ¥,âˆ€ğ‘£ğ‘—âˆˆîˆ·,âˆ€ğ‘£ğ‘—âˆˆîˆ¹.
(4)
The square of the difference between the distance measurement and the estimated distance is defined as localization error at time slot t, which is denoted as ğœğ‘–ğ‘—(ğ‘¡) and it can be expressed as

ğœğ‘–ğ‘—(ğ‘¡)=(ğ‘‘ğ‘–ğ‘—(ğ‘¡)âˆ’ğ‘‘Ë†ğ‘–ğ‘—(ğ‘¡))2.
(5)
We assume that the process of localization operates on the discrete time horizon and the partitions of time axis are called time slots which are equalization and non-overlapping. The symbol t denotes each time slot and symbol ğ‘‡ğ‘  denotes the duration of the localization. During the time slot t, the localized non-anchor node ğ‘£ğ‘– computes its localization error according to the location selected at time slot ğ‘¡âˆ’1 and it should make a decision on its final location by the terminal time slot ğ‘¡+ğ‘‡ğ‘ . We also assume the non-anchor nodes do not know the accurate localization duration. These features motivate us to develop a dynamic algorithm to solve the online and dynamic localization problem for IoT.

Stochastic game framework for dynamic localization
In this section, we first describe the investigated localization problem. Then, we formulate the problem of the node localization to be a stochastic game.

Problem formulation
Notice that, each non-anchor node in the IoT network only can directly communicate with its neighbor nodes. Therefore, we consider to divide the overall localization problem into several subproblems, and thus the localization problem of each non-anchor node is regarded as a subproblem. In each subproblem, non-anchor node ğ‘£ğ‘– executes its localization process independently. Hence, the localization problem of all non-anchor nodes from any time slot t can be formulated as

ğ¦ğ¢ğ§:âˆ‘ğ‘–=1ğ‘˜âˆ‘ğ‘£ğ‘—âˆˆğ‘ğ‘–ğœğ‘–ğ‘—(ğ‘¡),âˆ€ğ‘£ğ‘–âˆˆîˆ·.
(6)
As discussed in [24], the reward function of the learning problem indicates whether the decision made by the agent is good or bad for itself. Hence it is reasonable to define the negative value of localization error of ğ‘£ğ‘– as its reward function, and thus the reward function ğ‘…ğ‘–(ğ‘¡) of non-anchor node ğ‘£ğ‘– at time slot t can be expressed as

ğ‘…ğ‘–(ğ‘¡)=âˆ’âˆ‘ğ‘£ğ‘—âˆˆğ‘ğ‘–ğœğ‘–ğ‘—(ğ‘¡),âˆ€ğ‘£ğ‘–âˆˆîˆ·.
(7)
It can be observed from (7) that the instantaneous reward of non-anchor node ğ‘£ğ‘– relies on the location selection. In each subproblem, the higher the localization error, the smaller the reward for the localized non-anchor node. Next, we consider to maximize the long-term reward and we utilize the future discounted reward [37] as the measurement for each non-anchor node. Specifically, the future discounted reward is the sum of the reward in the current time slot, plus the discounted future reward. Hence, the long-term reward of non-anchor node ğ‘£ğ‘– can be expressed as

ğ‘ˆğ‘–(ğ‘¡)=âˆ‘ğœ=0+âˆğ›¾ğœğ‘…ğ‘–(ğ‘¡+ğœ+1),
(8)
where ğ›¾ denotes the discount factor and ğ›¾âˆˆ[0,1). The long-term reward ğ‘ˆğ‘–(t) indicates the discounted sum of the reward from time slot t, which is used to evaluate the action carried out by the non-anchor node ğ‘£ğ‘–.

Next, we denote the the action space of agent ğ‘£ğ‘–âˆˆîˆ· by îˆ­ğ‘–, which corresponds to all possible locations of non-anchor node ğ‘£ğ‘–. Therefore, the goal of each non-anchor node is to maximize its long-term reward in (8) by choosing an optimal location ğ‘âˆ—ğ‘– (t) at each time slot t. Accordingly, the localization problem of each non-anchor node ğ‘£ğ‘–, can be formulated as

ğ‘âˆ—ğ‘–(ğ‘¡)=argmaxğ‘ğ‘–âˆˆîˆ­ğ‘–ğ‘…ğ‘–(ğ‘¡).
(9)
In order to solve the optimization problem (9), we try to formulate the localization problem to a non-cooperative stochastic game in the next subsection.

Stochastic game formulation
In this subsection, we try to use stochastic game to model the localization problem in (9). And we put the index t to the superscript for the symbol conciseness in the following of this paper.

In the process of localization, each non-anchor node is considered as an agent to learn its location by interacting with the environment. Therefore, the localization problem can be considered as a multi-agent game, in which multiple non-anchor nodes learn their optimal location independently.

The localization processes of all non-anchor nodes satisfy the properties of Markov chain, which means the reward of each non-anchor node only depends on the current action and state. In RL, the function of Markov chain is applied to describe the dynamics of the states of a stochastic game [38]. Specifically, the definition of Markov chain is given as follows.

Definition 1
A Markov chain with finite state is a discrete process, where the finite state is denoted by îˆ¿={ğ‘ 1,ğ‘ 2,...,ğ‘ ğ‘’} and the ğ‘’Ã—ğ‘’ transition matrix is denoted by ğ“. Each element in matrix ğ“ satisfies 0â‰¤ğ‘‡ğ‘–,ğ‘§â‰¤1 and âˆ‘ğ‘’ğ‘§=1ğ‘‡ğ‘–,ğ‘§=1 for any 1â‰¤ğ‘–â‰¤ğ‘’. The discrete process starts in a certain state and moves to another state successively. The probability of transition from state ğ‘ ğ‘– at time solt t to the state ğ‘ ğ‘§ at time slot ğ‘¡+1 is given by

Pr{ğ‘ ğ‘¡+1=ğ‘ ğ‘§|ğ‘ ğ‘¡=ğ‘ ğ‘–}=ğ‘‡ğ‘–,ğ‘§.
(10)
This transition only depends on the current state and not depend on the previous states, which is also called as Markov property.

Markov Decision Process (MDP) also has the Markov property, and multi-agent MDPs can also be generalized by stochastic game. In the proposed localization process, each non-anchor first takes an action according to the learning strategy, then receives a reward, i.e, the negative value of the total localization error. After that, it transits to a new state with the taken actions of its non-anchor neighbor nodes. Therefore, we can formulate the IoT localization problem as a stochastic game, and given in the following definition.

Definition 2
The stochastic game of each non-anchor node can be defined as a tuple G = {îˆ·,îˆ¿,îˆ­,T,îˆ¾} where

îˆ· denotes the set of players, i.e., the non-anchor nodes;

îˆ¿ is the state set with îˆ¿ = îˆ¿1Ã—îˆ¿2 â‹¯Ã—îˆ¿ğ‘˜ and îˆ¿ğ‘– is the state set of each player ğ‘£ğ‘–, for all ğ‘£ğ‘–âˆˆîˆ·;

îˆ­ is the action set with îˆ­ = îˆ­1Ã—îˆ­2â‹¯Ã—îˆ­ğ‘˜ and îˆ­ğ‘– is the action set of player non-anchor node ğ‘£ğ‘–;

T is the transition probability function and ğ‘‡(ğ‘ ğ‘¡ğ‘–,ğ‘ğ‘–,ğ‘ ğ‘¡+1ğ‘–) denotes the transition probability of non-anchor node ğ‘£ğ‘– from state ğ‘ ğ‘¡ğ‘– move to the next state ğ‘ ğ‘¡+1ğ‘– by taking joint action a with ğ‘={ğ‘1,ğ‘2,â‹¯,ğ‘ğ‘˜}âˆˆîˆ­;

îˆ¾={ğ‘…1,â€¦,ğ‘…ğ‘–} denotes the playerâ€™s reward function where ğ‘…ğ‘– is the reward function of non-anchor node ğ‘£ğ‘–;

In the formulated stochastic game, îˆ­ğ‘– is the action space of ğ‘£ğ‘–, and the alternative action of each node ğ‘£ğ‘– at time slot t are conducted by randomly selecting n location within a certain area îˆ¯ğ‘¡ğ‘–, and îˆ¯ğ‘¡ğ‘– is given by

îˆ¯ğ‘¡ğ‘–=â§â©â¨âªâªîˆ¯ğ‘¡ğ‘–,1,îˆ¯ğ‘¡ğ‘–,2,îˆ¯ğ‘¡ğ‘–,3,ğ‘ğ‘–âˆ©îˆ¹={âˆ…},ğ‘ğ‘–âˆ©îˆ¹={ğ‘£ğ‘š},ğ‘ğ‘–âˆ©îˆ¹={ğ‘£ğ‘š,ğ‘£ğ‘§},
(11)
where îˆ¯ğ‘¡ğ‘–,1 denotes the whole area of IoT network; îˆ¯ğ‘¡ğ‘–,2 denotes a circular area with ğ‘£ğ‘š as the center and ğ‘…ğ‘ as the radius; îˆ¯ğ‘¡ğ‘–,3 denotes the overlapping area of two circular area with ğ‘£ğ‘š and ğ‘£ğ‘§ as the center respectively and ğ‘…ğ‘ as the radius. For each agent ğ‘£ğ‘–âˆˆîˆ·, the actions taken by other non-anchor nodes in ğ‘ğ‘– at time slot t is denoted by ğ‘ğ‘¡âˆ’ğ‘–, i.e., ğ‘ğ‘¡âˆ’ğ‘–âˆˆîˆ­âˆ–îˆ­ğ‘–, îˆ­=âˆğ‘£ğ‘–âˆˆîˆ·âˆ©ğ‘ğ‘–îˆ­ğ‘–. After taking an action ğ‘ğ‘¡ğ‘– at time slot t, the state of node ğ‘£ğ‘– can be denote by ğ‘ ğ‘¡+1ğ‘–. Hence, the state ğ‘ ğ‘¡+1ğ‘– observed by non-anchor node ğ‘£ğ‘– can be defined as

ğ‘ ğ‘¡+1ğ‘–=ğ‘ğ‘¡ğ‘–
(12)
Moreover, the neighbor set of ğ‘£ğ‘– can be divided into two parts: (1) anchor nodes set, i.e., îˆ¹âˆ©ğ‘ğ‘–; (2) non-anchor nodes set, i.e., îˆ·âˆ©ğ‘ğ‘–. Then, the reward function of each non-anchor node ğ‘£ğ‘– in (7), can be reformulated as

ğ‘Ÿğ‘¡ğ‘–=ğ‘…ğ‘–(ğ‘ğ‘¡ğ‘–,ğ‘ğ‘¡âˆ’ğ‘–,ğ‘ ğ‘¡ğ‘–)=âˆ’âˆ‘ğ‘£ğ‘šâˆˆîˆ¹âˆ©ğ‘ğ‘–(ğ‘‘ğ‘–ğ‘šâˆ’âˆ¥ğ‘ğ‘¡ğ‘–âˆ’ğ‘ğ‘¡ğ‘šâˆ¥)2âˆ’âˆ‘ğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–(ğ‘‘ğ‘–ğ‘—âˆ’âˆ¥ğ‘ğ‘¡ğ‘–âˆ’ğ‘ ğ‘¡ğ‘—âˆ¥)2,âˆ€ğ‘£ğ‘–âˆˆîˆ·,
(13)
where ğ‘£ğ‘— and ğ‘£ğ‘š denote the anchor node neighbor and non-anchor node neighbor respectively. Here, the reward obtained by node ğ‘£ğ‘– depends on the ğ‘ ğ‘¡ğ‘– and the actions (ğ‘ğ‘¡ğ‘–, ğ‘ğ‘¡âˆ’ğ‘–) at any time slot t. Since the action selection of each node satisfy the Markov property, the probability of ğ‘£ğ‘– moves to the next state ğ‘ ğ‘¡+1ğ‘– only depended on the current state ğ‘ ğ‘¡ğ‘– and the actions (ğ‘ğ‘¡ğ‘–,ğ‘ğ‘¡âˆ’ğ‘–). In the localization process, non-anchor node ğ‘£ğ‘– can observe its own state ğ‘ ğ‘¡ğ‘– and the action ğ‘ğ‘¡ğ‘– at any time slot t, while the actions taken by other players are not known.

In the stochastic game, the strategy ğœ‹ğ‘– denotes the mapping from the state set îˆ¿ğ‘– to the action set îˆ­ğ‘–. We use ğœ‹ğ‘–( ğ‘ ğ‘–) = {ğœ‹ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–)|ğ‘ğ‘–âˆˆîˆ­ğ‘–} to denote the strategy for non-anchor node ğ‘£ğ‘– in the state ğ‘ ğ‘–. Specifically, each the strategy ğœ‹ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–) is the probability of taking the action ğ‘ğ‘– in the state ğ‘ ğ‘– and ğœ‹ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–)=Pr(ğ‘ğ‘¡ğ‘–=ğ‘ğ‘–âˆ£ğ‘ ğ‘¡ğ‘– = ğ‘ ğ‘–)âˆˆ[0,1]. ğœ‹ = {ğœ‹1(ğ‘ 1),ğœ‹2(ğ‘ 2),â‹¯,ğœ‹ğ‘˜(ğ‘ ğ‘˜)} is the vector of strategies for k agents. Based on the above discussions, the goal of each non-anchor node in the strategy game is to maximize its expected reward. Hence, the problem (8) can be reformulated as

ğ‘‰ğ‘–(ğ‘ ,ğœ‹)=ğ¸ğœ‹{âˆ‘ğœ=0+âˆğ›¾ğœğ‘Ÿğ‘¡+ğœ+1ğ‘–âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ,ğœ‹},
(14)
where t represents the current time slot. ğ‘Ÿğ‘¡+ğœ+1 denotes the immediate reward at time slot ğ‘¡+ğœ+1. ğ›¾ âˆˆ [0, 1] denotes the discount factor of reward. ğ¸{â‹…} is the expectation operation and it represents the state transition by taking strategy ğœ‹ from state s. In the proposed stochastic game, agents have individual expected rewards which could be affected by the strategy of other agents in their neighbor sets. Therefore, we cannot simply expect all agent to obtain their maximal expected reward at the same time. In general, Nash Equilibrium (NE) is a solution for a stochastic game. The definition of NE is given as follows.

Definition 3
If the collection of strategies ğœ‹âˆ—={ğœ‹âˆ—1,ğœ‹âˆ—2,...,ğœ‹âˆ—ğ‘˜} is a Nash Equilibrium [39], the state value function for each player ğ‘£ğ‘– satisfies

ğ‘‰ğ‘–(ğœ‹âˆ—ğ‘–,ğœ‹âˆ’ğ‘–)â©¾ğ‘‰ğ‘–(ğœ‹â€²ğ‘–,ğœ‹âˆ’ğ‘–),âˆ€ğœ‹â€²ğ‘–,
(15)
where ğœ‹â€²ğ‘– denotes any possible strategy taken by ğ‘£ğ‘–.

As shown in (15), each player can obtain the maximal expected reward in a Nash Equilibrium. Therefore, the goal of each player ğ‘£ğ‘– is to find a Nash Equilibrium for any state ğ‘ ğ‘– to ensure each player and its non-anchor neighbors obtain their accurate locations. In next section, we propose a framework for maximizing the expected reward in (14) of each agent.

Distribute localization based on multi-agent Q-learning algorithm
In this section, we establish a MARL framework firstly. Then, a Q-learning-based node localization algorithm proposed to maximize the expectation of long-term reward of each non-anchor node.

Multi-agent reinforcement learning framework
The MARL framework is shown in Fig. 1. In this figure, each node ğ‘£ğ‘– only communicates with the nodes in its neighbor set ğ‘ğ‘– and observes its local state ğ‘ ğ‘– and reward ğ‘Ÿğ‘–, then selecting the action ğ‘ğ‘– according to the strategy ğœ‹ğ‘–. As mentioned in the previous section, the state ğ‘ ğ‘¡+1ğ‘– at time slot ğ‘¡+1 only depends on the state ğ‘ ğ‘¡ğ‘– and the selected action ğ‘ğ‘¡ğ‘–. The decision faced by an agent when the other agents choose a fixed strategy is the same as a MDP. As discussed above, we use Markov property to model the dynamic environment, MDP for non-anchor node ğ‘£ğ‘– consists of: (1) a discrete state set îˆ¿ğ‘–, (2) a discrete action set îˆ­ğ‘–, (3) the state transition probabilities ğ‘‡(ğ‘ ğ‘¡ğ‘–,ğ‘ğ‘–,ğ‘ ğ‘¡+1ğ‘–) for all ğ‘ğ‘–âˆˆîˆ­ğ‘– and ğ‘ ğ‘¡ğ‘–,ğ‘ ğ‘¡+1ğ‘–âˆˆîˆ¿ğ‘–; (4) a reward function ğ‘…ğ‘–, which denotes the next expected reward for non-anchor node ğ‘£ğ‘–. We use Q-learning algorithm to solve the MDPs in this paper. In this case, each non-anchor node can be regarded as a learning agent, where its reward and transition function can not be obtained directly [24]. Next, we introduce the Q-learning algorithm for solving the MDP of a non-achor node in detail.

Fig. 1
figure 1
Multi-agent reinforcement learning framework

Full size image
Proposition 1
Each non-anchor node ğ‘£ğ‘–âˆˆîˆ· selects the strategy independently to maximize its own expected discounted reward, which can be defined as the state value function. For any state ğ‘ ğ‘– and strategy ğœ‹, we have

ğ‘‰ğ‘–(ğ‘ ğ‘–,ğœ‹)=âˆ‘ğ‘ â€²ğ‘–âˆˆîˆ¿ğ‘–ğ‘‡(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)âˆ‘ğ‘ğ‘–âˆˆîˆ­ğ‘–âˆğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—)Ã—[ğ‘…ğ‘–(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)+ğ›¾ğ‘‰(ğ‘ â€²ğ‘–,ğœ‹)],
(16)
where ğ‘ ğ‘¡ğ‘–=ğ‘ ğ‘– and ğ‘ ğ‘¡+1ğ‘– = ğ‘ â€²ğ‘–, with ğ‘ ğ‘–, ğ‘ â€²ğ‘–âˆˆîˆ¿ğ‘–. ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—) denotes the probability of the selecting the action ğ‘ğ‘— in the state ğ‘ ğ‘—.

Proof
According to (14), we have

ğ‘‰ğ‘–(ğ‘ ğ‘–,ğœ‹)=ğ¸{ğ‘Ÿğ‘¡+1ğ‘–âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ğ‘–}+ğ›¾ğ¸{âˆ‘ğœ=0+âˆğ›¾ğœğ‘Ÿğ‘¡+ğœ+2ğ‘–âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ğ‘–},
(17)
where the first part denotes the expected value and the second part denotes the state value function at time slot ğ‘¡+1. Let ğ‘ ğ‘¡ğ‘– = ğ‘ ğ‘–, ğ‘ğ‘¡ğ‘– = ğ‘ğ‘– and ğ‘ ğ‘¡+1ğ‘– = ğ‘ â€²ğ‘–, the expected value can be expressed as

ğ¸{ğ‘Ÿğ‘¡+1ğ‘–âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ğ‘–}=âˆ‘ğ‘ â€²ğ‘–âˆˆîˆ¿ğ‘–ğ‘‡(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)âˆ‘ğ‘ğ‘–âˆˆîˆ­ğ‘–âˆğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—)Ã—ğ¸{ğ‘Ÿğ‘¡+1ğ‘–âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ğ‘–,ğ‘ğ‘¡ğ‘–=ğ‘ğ‘–,ğ‘ ğ‘¡+1ğ‘–=ğ‘ â€²ğ‘–}=âˆ‘ğ‘ â€²ğ‘–âˆˆîˆ¿ğ‘–ğ‘‡(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)âˆ‘ğ‘ğ‘–âˆˆîˆ­ğ‘–âˆğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—)ğ‘…ğ‘–(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–),
(18)
where ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—) denotes the probability of selecting the action ğ‘ğ‘— in the state ğ‘ ğ‘— and ğ‘£ğ‘— is one of the non-anchor nodes in ğ‘ğ‘–. Similarly, the second part in (17) can be rewritten into

ğ¸{âˆ‘ğœ=0+âˆğ›¾ğœğ‘Ÿğ‘¡+ğœ+2ğ‘–âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ğ‘–}=âˆ‘ğ‘ â€²ğ‘–âˆˆîˆ¿ğ‘–ğ‘‡(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)âˆ‘ğ‘ğ‘–âˆˆîˆ­ğ‘–âˆğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—)Ã—ğ¸{âˆ‘ğœ=0+âˆğ›¾ğœğ‘Ÿğ‘¡+ğœ+2ğ‘–âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ğ‘–,ğ‘ğ‘¡ğ‘–=ğ‘ğ‘–,ğ‘ ğ‘¡+1ğ‘–=ğ‘ â€²ğ‘–}=âˆ‘ğ‘ â€²ğ‘–âˆˆîˆ¿ğ‘–ğ‘‡(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)âˆ‘ğ‘ğ‘–âˆˆîˆ­ğ‘–âˆğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—)ğ‘‰(ğ‘ â€²ğ‘–,ğœ‹).
(19)
Substituting (18) and (19) into (17), we can get (16). â—»

Therefore, the state value function in (16) denotes the expected reward when taking the strategy ğœ‹ from the state ğ‘ ğ‘–. Based on (16), we can derive the action value function. i.e., Q-function, which is the cumulative expected reward under the policy ğœ‹, taking action ğ‘ğ‘–, from the state ğ‘ ğ‘–, and it can be expressed as

ğ‘„ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–,ğœ‹)=ğ¸{âˆ‘ğœ=0+âˆğ›¾ğœğ‘Ÿğ‘¡+ğœ+1âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ,ğ‘ğ‘¡ğ‘–=ğ‘ğ‘–}=ğ¸{ğ‘Ÿğ‘¡+1ğ‘–+ğ›¾âˆ‘ğœ=0+âˆğ›¾ğœğ‘Ÿğ‘¡+ğœ+2ğ‘–âˆ£ğ‘ ğ‘¡ğ‘–=ğ‘ ğ‘–,ğ‘ğ‘¡ğ‘–=ğ‘ğ‘–,ğ‘ ğ‘¡+1ğ‘–=ğ‘ â€²ğ‘–}=âˆ‘ğ‘ â€²ğ‘–âˆˆîˆ¿ğ‘–ğ‘‡(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)âˆ‘ğ‘âˆ’ğ‘–âˆˆîˆ­âˆ’ğ‘–âˆğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—)Ã—[ğ‘…ğ‘–(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)+ğ›¾ğ‘‰ğ‘–(ğ‘ â€²ğ‘–,ğœ‹)],
(20)
where the value of (20) is called Q-value. In the action value function (20), the Q-value depends on the actions of all non-anchor nodes in its neighbor set ğ‘ğ‘–. And the condition in (20) holds between two consistency state ğ‘ ğ‘¡ğ‘– and ğ‘ ğ‘¡+1ğ‘–. As discussed above, we can conclude the relationship between state values and Q-values:

ğ‘‰ğ‘–(ğ‘ ğ‘–,ğœ‹)=âˆ‘ğ‘ğ‘–âˆˆîˆ­ğ‘–ğœ‹ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–)ğ‘„ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–,ğœ‹).
(21)
Note that the goal of each agent in MDP is to find a strategy to maximize its expected reward. That is, the goal of each non-anchor node in MDP is to find a strategy to minimize its localization error. Under the optimal strategy ğœ‹âˆ—, the state value function for non-anchor node ğ‘£ğ‘– at state ğ‘ ğ‘– can be defined as

ğ‘‰âˆ—ğ‘–(ğ‘ ğ‘–)=maxğœ‹iğ‘‰ğ‘–(ğ‘ ğ‘–,ğœ‹),ğ‘ ğ‘–âˆˆîˆ¿ğ‘–.
(22)
And the optimal action value function also can be defined as

ğ‘„âˆ—ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–)=maxğœ‹iğ‘„ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–,ğœ‹),ğ‘ ğ‘–âˆˆîˆ¿ğ‘–,ğ‘ğ‘–âˆˆîˆ­ğ‘–.
(23)
Substituting (21)â€“(22), the Eq. (22) can be rewritten as

ğ‘‰âˆ—ğ‘–(ğ‘ ğ‘–)=maxğ‘ğ‘–ğ‘„âˆ—ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–).
(24)
Based on (24), we can combining it with (16) to derive the Bellman optimality equation for state value function, which can be expressed as

ğ‘‰âˆ—ğ‘–(ğ‘ ğ‘–)=âˆ‘ğ‘âˆ’ğ‘–âˆˆîˆ­âˆ’ğ‘–âˆğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—)Ã—maxğ‘ğ‘–âˆ‘ğ‘ â€²ğ‘–ğ‘‡(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)[ğ‘…ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–,ğ‘ â€²ğ‘–)+ğ›¾ğ‘‰âˆ—ğ‘–(ğ‘ â€²ğ‘–)].
(25)
And by the combining (24) with (20), we also can derive the Bellman optimality equation for action value function, which can be expressed as

ğ‘„âˆ—ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–)=âˆ‘ğ‘âˆ’ğ‘–âˆˆîˆ­âˆ’ğ‘–âˆğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–ğœ‹ğ‘—(ğ‘ ğ‘—,ğ‘ğ‘—)Ã—âˆ‘ğ‘ â€²ğ‘–ğ‘‡(ğ‘ ğ‘–,ğ‘,ğ‘ â€²ğ‘–)[ğ‘…ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–,ğ‘ â€²ğ‘–)+ğ›¾maxğ‘â€²ğ‘–ğ‘„âˆ—(ğ‘ â€²ğ‘–,ğ‘â€²ğ‘–)].
(26)
It can be observed in (26), under the optimal strategy, each non-anchor node ğ‘£ğ‘– selects the optimal action to maximize the value of Q-function at current state. However, the optimal Q-function of each non-anchor node ğ‘£ğ‘– depends on the actions and the policy taken by its non-anchor neighbors, which makes it difficult to find the optimal strategy [40]. Trying to solve the complex problem, we assume non-anchor nodes are independent learners (ILs), which means the non-anchor nodes do not known the actions and the rewards of other non-anchor nodes.

Distributed localization based on Q-learning
In this subsection, Q-learning is applied to solve the MDPs and a Q-learning-based distributed node localization algorithm is proposed to solve the node localization problem.

Q-learning is an effective reinforcement learning method, which is first proposed in Ref. [41], and the stability of the algorithm based on random approximation was proved in Ref. [42]. Based on these advantages of Q-learning, we use it to solve the MDPs. In the learning process, each non-anchor node runs the Q-learning independently to learn its optimal Q-value and to find the optimal policy for the MDP. Specifically, the action selection strategy in any time slot t depends on the Q-function of state ğ‘ ğ‘¡ and ğ‘ ğ‘¡+1. Therefore, the Q-value indicates the future quality of the actions in next state. In the proposed algorithm, the update rule for Q-value of Q-learning [24] is given by

ğ‘„ğ‘¡+1ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–)=(1âˆ’ğ›¼ğ‘¡)ğ‘„ğ‘¡ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–)+ğ›¼ğ‘¡[ğ‘Ÿğ‘¡ğ‘–+ğ›¾maxğ‘â€²ğ‘–âˆˆîˆ­ğ‘–ğ‘„ğ‘¡ğ‘–(ğ‘ â€²ğ‘–,ğ‘â€²ğ‘–)],
(27)
with ğ‘ ğ‘– = ğ‘ ğ‘¡ğ‘–, ğ‘ğ‘– = ğ‘ğ‘¡ğ‘– and ğ‘ â€²ğ‘– = ğ‘ ğ‘¡+1ğ‘–, ğ‘â€²ğ‘– = ğ‘ğ‘¡+1ğ‘–. ğ›¼ğ‘¡ is the learning rate at time slot t. To ensure the convergence of Q-learning, we set ğ›¼ğ‘¡ as in [42], which is given by

ğ›¼ğ‘¡=1ğ‘¡+ğ‘¢ğ›¼ğœ‰ğ›¼,
(28)
where ğ‘¢ğ›¼>0, ğœ‰ğ›¼âˆˆ (12, 1].

Moreover, another important element of Q-learning is strategy ğœ‹, which is used to select an action for the agent during the learning process and ğœ‹âˆˆ[0,1]. In this paper, we proposed a new strategy to find the optimal policy. Note that, the action selection of the non-anchor node ğ‘£ğ‘– is only related to the nodes in its neighbor set ğ‘ğ‘–. Therefore, we consider to use the weighted localization errors to represent different influence of anchor neighbors and non-anchor neighbors on ğ‘£ğ‘–. In this case, a higher weight is assigned to the anchor neighbors and a lower weight is assigned to the non-anchor neighbors, denoted by ğ‘¤1 and ğ‘¤2 respectively. As mentioned above, n denotes the number of the alternative actions at time slot t. Let ğ‘ğ‘¡ğ‘–,ğ‘¥,ğ‘¥âˆˆ[1,ğ‘›], denote the alternative action of ğ‘£ğ‘– at time slot t. Let ğ‘’ğ‘Ÿğ‘Ÿğ‘¡ğ‘–,ğ‘¥ denote the weighted localization error. Hence, the weighted localization error can be expressed as

ğ‘’ğ‘Ÿğ‘Ÿğ‘¡ğ‘–,ğ‘¥=ğ‘¤1âˆ‘ğ‘£ğ‘šâˆˆîˆ¹âˆ©ğ‘ğ‘–(ğ‘‘ğ‘–ğ‘šâˆ’âˆ¥ğ‘ğ‘¡ğ‘–,ğ‘¥âˆ’ğ‘ğ‘¡ğ‘šâˆ¥)2+ğ‘¤2âˆ‘ğ‘£ğ‘—âˆˆîˆ·âˆ©ğ‘ğ‘–(ğ‘‘ğ‘–ğ‘—âˆ’âˆ¥ğ‘ğ‘¡ğ‘–,ğ‘¥âˆ’ğ‘ ğ‘¡ğ‘—âˆ¥)2,âˆ€ğ‘£ğ‘–âˆˆîˆ·,
(29)
where ğ‘ğ‘¡ğ‘š denotes the location of anchor node ğ‘£ğ‘š. ğ‘‘ğ‘–ğ‘š denotes the distance measurement between ğ‘£ğ‘– and ğ‘£ğ‘š. ğ‘‘ğ‘–ğ‘— denotes the distance measurement between ğ‘£ğ‘– and ğ‘£ğ‘—. Based on (29), at time slot t, the probability of selecting the action ğ‘ğ‘¡ğ‘–,ğ‘¥ at state ğ‘ ğ‘¡ğ‘– can be expressed as

ğœ‹ğ‘–(ğ‘ ğ‘¡ğ‘–,ğ‘ğ‘¡ğ‘–,ğ‘¥)=âˆ‘ğ‘›ğ‘ =1ğ‘’ğ‘Ÿğ‘Ÿğ‘¡ğ‘–,ğ‘ âˆ’ğ‘’ğ‘Ÿğ‘Ÿğ‘¡ğ‘–,ğ‘¥(ğ‘›âˆ’1)âˆ—âˆ‘ğ‘›ğ‘ =1ğ‘’ğ‘Ÿğ‘Ÿğ‘¡ğ‘–,ğ‘ .
(30)
It can be observed in (29) and (30), at any time slot t, the lower weighted localization error is, the higher the probability of selecting the action ğ‘ğ‘¡ğ‘–,ğ‘¥.

In the proposed localization algorithm, each non-anchor node learns its optimal location by running the Q-learning algorithm independently and maintains its own Q-table which is the set of Q-values. Since the non-anchor nodes have no prior knowledge of the initial state, each non-anchor node randomly selects an action from the region îˆ¯ğ‘¡ğ‘– at first. The process of the Q-learning-based distributed node localization algorithm is shown in Algorithm 1.

figure a
Convergence performance
In this subsection, we investigate the convergence of the proposed localization algorithm. The convergence of the algorithm is concluded in Proposition 2.

Proposition 2
In the proposed Algorithm 1, the Q-learning procedure for each agent is converged to its optimal Q-value.

We proof Proposition 2 in the following contents. Notice that, each agent execute its individual Q-learning independently, the convergence of the MARL algorithm is determined by the convergence of the Q-learning. Therefore, we only need to proof the convergence for the Q-learning in the proposed Algorithm 1.

Theorem 1
The Q-learning algorithm in Algorithm 1 converges to the optimal Q-value ğ‘„âˆ—ğ‘–(ğ‘ ğ‘–,ğ‘ğ‘–) if

1.
The state and action space for each agent is finite;

2.
âˆ‘+âˆğ‘¡=0ğ›¼ğ‘¡=âˆ, âˆ‘+âˆğ‘¡=0(ğ›¼ğ‘¡)2<âˆ with probability 1;

3.
The value of ğ‘Ÿğ‘¡ğ‘– is bounded;

Proof
See Appendix A. â—»

Numerical results
Experiment results
In order to evaluate the performance of the proposed MARL-based algorithm, we invoke the mean localization error (MLE) as the localization evaluation, which is given by

MLE=1ğ¾âˆ‘ğ‘–=1ğ¾âˆ¥ğ‘ğ‘–Ë†âˆ’ğ‘ğ‘–âˆ¥.
(31)
At first, we randomly deploy 150 non-anchor nodes and 15 anchor nodes in a 100Ã—100 m area. The communication radius ğ‘…ğ‘ is set as 20m. We assume that the measurement error ğ‘’ğ‘–ğ‘— is randomly generated by a Gaussian distribution with a mean of 0 and a variance of ğ›¿2. Here, we set the mean value of measurement error ğ‘’ğ‘–ğ‘— as 0, and the variance ğ›¿2 as 0.5. Additionally, we set ğ‘¢ğ›¼ as 0.5, ğœ‰ğ›¼ as 0.7 and ğ›¾ as 0.9, respectively.

Fig. 2
figure 2
Initial distribution of nodes and final localization results

Full size image
Figure 2a plots the initial deployment of the network and the connection between non-anchor nodes and anchor nodes, where the anchor nodes and the ground truth locations of non-anchor nodes are shown by asterisks and circles, respectively, the connections between them are shown by blue lines. Figure 2b plots the final localization results which are calculated by the proposed MARL-based localization algorithm, where the anchor nodes and the ground truth locations of non-anchor nodes are shown by asterisks and circles respectively, the estimated locations of non-anchor nodes and the localization error are shown by plus and pink lines. From these two figures, it can be observed that almost all non-anchor nodes can be accurately localized.

Figure 3 plots the performance comparison of system localization error versus different number of anchor nodes with fixed number of non-anchor nodes. Here, the system localization error refers to the accumulative localization error of all non-anchor nodes. It can be observed that the system localization error decreases with the increase of time slot and finally converges to the optimal solution within 40 iterations for all scenarios. This can be explained by the fact that the MARL operates with a distributed manner, thus is more suitable for large scale localization. Though better performance can be obtained with large number of anchor nodes initially, the final converged performance of all these scenarios all almost the same. This also showcase that it does not require more anchor nodes to locate the non-anchor nodes.

Figure 4 plots the MLE versus different communication radius. Here, we set the number of anchor nodes as 15, and the number of non-anchor nodes as 150, and four different communication radii are tested. It can be observed that the MLE converges to the minimum when the communication radius is set to 30 m. Moreover, it also can be observed that the algorithm is more stable when the communication radius is set to 30 m. We thus can conclude that increasing the communication radius is helpful to improve the localization accuracy, and obtain a more stable localization result. This can be explained by the fact that more connectives are obtained with a large communication radius.

Fig. 3
figure 3
Comparisons of system localization error among different number of anchor nodes

Full size image
Fig. 4
figure 4
The box plots of Average localization error under different communication radii

Full size image
Performance comparison
In this subsection, we further compare the performance of the proposed MARL-based algorithm with VFDLA and GTDLA with the same network topology, where the size of the network is set to 100Ã—100 m, ğ‘…ğ‘ is set to 20 m, and the proportion of anchor nodes is set to 10% of the number of non-anchor nodes..

Figure 5 plots the converge performance of these three algorithms. It can be observed that MARL-based localization algorithm outperforms other two algorithms, while VFDLA performs the worst. Another observation is that MARL-based algorithm converges faster than the other two algorithms. In order to further test the performance of energy consumption of these three algorithms, the number of communications required for localization is also compared. Here, four network typologies with different numbers of IoT nodes are tested. The performance comparison is shown in Table 1. From Table 1, we can observe that the number of communications increase with increasing the number of nodes. Moreover, the number of communications of our algorithm is obviously less than the other two algorithms. Since the energy is mainly consumed in communications between IoT nodes, it can be concluded that our algorithm is more energy efficient than the other two algorithms.

Figure 6 plots the MLE comparison of these three algorithms versus different communication radius. It can be observed the three algorithms can obtain accurate localization with a large communication radius. This can be explained by the fact that more node connectives can be obtained with a large the communication radius. Moreover, comparing with VFDLA and GTDLA, the MLE of MARL-based algorithm is the smallest for all communication radii.

Fig. 5
figure 5
Converge curve of different localization algorithms

Full size image
Table 1 Comparison of the number of communications
Full size table
Fig. 6
figure 6
Comparison of MLE under different communication radii

Full size image
Fig. 7
figure 7
Comparison of MLE under different anchor nodes

Full size image
Fig. 8
figure 8
Comparison of MLE under different range errors

Full size image
Figure 7 plots the MLE comparison of these three algorithms versus different anchor nodes. Here the number of non-anchor nodes is ranged from 110 to 220. It can be observed that the MLE decreases with increasing the number of anchor nodes. Since the proportion between anchor and non-anchor nodes is fixed, the increase of the anchor nodes brings about an increase of non-anchor nodes. This means that more non-anchor node which are already localized can be further utilized as the anchor nodes, thus to further improve the localization accuracy. Moreover, it also can be observed that the performance of the proposed MARL-based algorithm outperforms GTDLA and VFDLA, respectively.

Figure 8 plots the MLE comparison of these three algorithms versus different measurement error. As shown in Fig. 8, the MLE of the three algorithms increase with increasing the measurement error. However, the MLE curve of the proposed MARL-based algorithm is relative smooth. It also can be observed that the performance of the proposed MARL-based algorithm is similar to that of the VFDLA algorithm with small measurement error. However, when the measurement error continues to increase, the proposed MARL-based algorithm performs better than the other two algorithms.

Conclusion
This paper mainly investigated the distributed localization problem in IoT. We established the distributed node localization model and proposed the updating formula for this model-based on Q-learning. Moreover, we proposed a dynamic strategy so that the optimal policy can be carried out without taking more exploratory actions. Then, we proposed a distributed node localization algorithm based on MARL to solve the formulated stochastic game and the simulation verified that the localization error can converge to a low value in a short time. Finally, the simulation results of several comparative experiments shown that the localization accuracy of the proposed algorithm is better than the existing node localization algorithms in different aspects. Since the localization can further be enhanced by node coordination, our future work will consider the joint coordinated based localization method, where the trade-off between coordination and the signal overhead will be jointly optimized.

Keywords
Distributed localization
Q-learning
Internet of things (IoT)
Multi-agent reinforcement learning