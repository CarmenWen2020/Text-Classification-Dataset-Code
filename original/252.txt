Network scanning is one of the first steps in gathering information about a target before launching attacks. It is used to scan for vulnerable devices and exposed services in order to exploit them. Such exploits can result in data breaches or network disruption, which can be very costly for organizations. There are many factors, including technical and non-technical, affecting the volume of scanning activities. In this paper, we study the impact of vulnerability disclosure on the volume of scans over time and propose a machine learning-based approach to predict this impact. We conducted a comprehensive data collection of network scans from two network telescopes hosted in different countries, as well as the disclosed vulnerabilities from 2014 to 2019. We then designed a set of features to characterize the disclosed vulnerabilities and used several classifiers to predict whether a vulnerability will impact the volume of daily scans. The resulting classifier achieves over 85% accuracy in predicting the impact. In addition, we performed an analysis of the key characteristics of vulnerabilities that directly affect scanning activities. Our findings show that this approach is able to classify vulnerabilities that have an impact on network scans. The implementation of our model and validation tests proved the efficiency of the selected features, as well as the robustness of our model to classify vulnerabilities’ impact on scans.

Previous
Next 
Keywords
Network scans

Classification algorithm

Time series

NVD

CVE

Machine learning

1. Introduction
According to a Gartner research study (Elizabeth et al., 2018), the global information security market is forecasted to be worth USD 170 billion by 2022. That is due to the ever-increasing number of attacks and data breaches. More than 37 billion records were compromised in 2020 alone (Riskbasedsecurity, 2021), estimated by IBM (IBM, 2020) to cost USD 3.86 million per breach on average. In addition, WannaCry, ransomware that spread in 2017, was responsible for losses estimated at USD 4 billion, affecting roughly 230,000 machines in 150 countries around the globe by exploiting a vulnerability in the Windows operating system (Kaspersky, 2020).

Multiple protection systems are used by security practitioners to block attacks, such as intrusion detection and prevention systems (IDS/IPS), firewalls, and unified threat management systems (UTM). These systems contain implementations of models to block complex attacks such as advanced persistent threats (APT) (Milajerdi et al., 2019), to detect and filter malicious traffic (Shafiq et al., 2021), in addition to policy enforcement for authorization management and access control (Qiu et al., 2020). Despite using all these mechanisms, the existence of one critical vulnerability can render these protection systems useless, and cannot stand in front of attackers with vulnerabilities’ exploits, tools and knowledge.

A software vulnerability is a weakness induced by security-related errors found in the software design or implementation. It is exploited by attackers to harm systems (get unauthorized access, execute a DoS attack, etc.). Because of the great importance of vulnerability information, it was standardized and managed by Mitre corporation (Mitre, 2020). Whenever a new vulnerability is discovered, system administrators are urged to patch the vulnerability as fast as possible. On the other hand, attackers create exploits for the vulnerability to compromise as many systems as they can before the patch release. Unfortunately, systems administrators cannot patch all vulnerabilities in a timely manner because of the cost and complexity of patching, which gives time for attackers to take more victims.

It has been found that the number of exploits and attacks increases after the public disclosure of a vulnerability (Bilge and Dumitras, 2012). In the 150 days following the disclosure, the increase of magnitude was up to five times. Thus, a vulnerability disclosure may trigger a change in the behavior of attackers, especially network scans, as they are the first step to identify the target vulnerable systems.

Scanning is one of the techniques used in the reconnaissance phase, where attackers gather information about targets before starting the weaponization phase. Analyzing the evolution of scans targeting a network may indicate underlying event triggers such as technical ones like the spread of a new worm, vulnerability, exploit or even socio-political events where entities like companies or states could become a target. Thus, understanding scans and their evolution over time helps understand attacks and mitigate their impact. Unfortunately, little is known about scans’ evolution and the impact of each technical and social event on their volume. A large amount of work is done about software vulnerabilities but is still not used to understand the attack behavior, including network scans, despite the fact that previous works (Fachkha and Debbabi, 2016, Arora et al., 2007, Arora et al., 2010) have shown that the vulnerability disclosure has an impact on different aspects of cybersecurity, such as the speed of patching, of exploit release, and volume of attacks and scans. This work, instead, focuses on studying the impact of vulnerabilities on network scans, providing more details about the impact and building models to predict it.

The aim of this work is to study the vulnerabilities’ disclosure impact on scans in an automatic manner, which can be learned from scans and vulnerabilities history. Instead of relying on a single feature to determine the significance and severity of the vulnerability and then quantify its expected impact on the scans, we will use all the features of the vulnerability. We define the impact of vulnerability on scans as a change in the statistical properties of the scans over time. We feed them to a model based on machine learning that classifies vulnerabilities; whether or not they affect the scans.

The network scans’ data we use in this paper is collected using a system called the Network Telescope, which is composed of a logging system and a large set of non-allocated IP addresses. Network telescopes are used as a threat intelligence to capture large-scale scans, network-spreading worms, etc. (Fachkha and Debbabi, 2016). The telescope network data is composed of packets targeting different ports through TCP (Transmission Control Protocol) and UDP (User Datagram Protocol) protocols. We extract time series of scans targeting each port and then find the vulnerabilities related to the same port.

The purpose of carrying out this study is to answer the following research questions:

•
Does the disclosure of vulnerabilities have an impact on scans?

•
Do all vulnerabilities have an impact on the scans’ data?

•
What are the features of vulnerabilities having an impact on scans?

•
To what extent can we identify a vulnerability, whether it has an impact?

The rest of the paper is organized as follows. We start with the related work in Section 2. Section 3 provides a detailed description of our research methodology, including data collection and algorithms we used. We present the proposed model and the achieved results in Section 4. Then we discuss and show some limitations of this work in Section 5, and we finally summarize the entire work in Section 6.

2. Related work
In this section, we present the work related to this study. We start by introducing the different techniques used in scanning networks and the methods that were developed to detect scans. Then, we present the work done on software vulnerabilities and their impact on the cybersecurity space.

2.1. Network scanning activities
Network scans are the first step to enumerate targets and search for vulnerable devices. Receiving scans is an indication that the network is a target for coming attacks. Network scanners developed multiple techniques for scanning networks (Bou-Harb et al., 2014). We can identify two types of scans based on their nature. Active scans which are used to actively scan targets by sending packets in order to do applications and operating systems fingerprinting (Song et al., 2019), network maintenance, quality of service checks, and other measurements and research. On the other hand, passive scans, instead, use monitoring tools on network devices for passive measurements including vulnerability detection. Scans can be launched from the inside of the local network itself, or from a remote network. We can differentiate them (i) by the aim of the scan: wide scans or scans targeting organizations, (ii) by their scan rate or (iii) by their scans method: a single or a distributed (Griffioen and Doerr, 2020) source of scan.

Scan detection methods were developed for both distributed and single-source scans. They can be applied at a flow level (Satheesh et al., 2020) or a packet level. They extract the traffic information needed such as the source and destination IP addresses, source and destination ports, protocols used, etc. The most known are threshold-based. They detect scans if a parameter exceeds a specified threshold such as the number of contacted IP addresses or requests to unavailable resources, the number of invalid flags on TCP packets, the number of the SYN (synchronized) packets sent to different hosts/ports within a small time window or the number of destinations (IP, port) contacted by each source (IP, port) in a specified amount of time.

Other methods use an algorithmic approach to separate scans from the legitimate network traffic. They can be graph-based algorithms (Abid and Jemili, 2020) or anomaly detection-based (Clotet et al., 2018). They can use other techniques such as fuzzy logics (Saidi et al., 2020, Chiba et al., 2019) or artificial neural networks (ANNs) (Raman et al., 2017, Chiba et al., 2019, Viet et al., 2018). Rule-based approaches set rules to differentiate malicious from legitimate traffic. Parameters of rules can be learned from normal traffic, then applied to classify future coming traffic. Variables in the rules can include Ethernet packets fields and protocols fields (Ullah, 2016). In addition, fuzzy rules can be used instead of explicit rules. Visual approaches are used to present packets and flows of data to users to help them recognize scans (Jiawan et al., 2008). Other detection techniques can be found in a detailed survey (Fernandes et al., 2019).

Scanners use many techniques to avoid detection (Nisa and Kifayat, 2020). They distribute the scanning task on multiple scanner hosts with different IP addresses to avoid reaching the thresholds specified in the IDS/IPS. Many approaches have been developed to tackle this problem and detect distributed and coordinated scans. Clustering approaches can be used to group probing activities from single hosts to form sets of distributed scans. The probing similarity can be based on packet’s information, session time, proximity of IP addresses, or similarity of the target (IP/port) for multiple scanners within a small time window (Griffioen and Doerr, 2020).

With the spread of Software Defined Networks (SDN) (Birkinshaw et al., 2019, Satheesh et al., 2020), which renders the scan’s detection a complex task, new types of intrusion detection systems (IDS) are emerging. In addition, the complexity of attacks is evolving which makes simple methods incapable of detecting them. This drove security practitioners to leverage the power of other exogenous data to improve the detection models’ performance.

2.2. Software vulnerability impact on cybersecurity space
The software vulnerability is of important interest in cyberspace because it can be exploited to gain access and attack information systems. Many studies are done to analyze the spread, exploitability, and severity of software vulnerability. A summary of the software vulnerability-related work is presented in Table 1.

Vulnerabilities can be detected by experts before the release of the software to the market. However, it is a time-consuming task, and multiple automated models have been proposed for that. Niu et al. used deep learning models for taint analysis for the discovery and location of software vulnerability in C/C++ programs (Niu et al., 2020). They used a Bidirectional Long Short Term Memory (BLSTM) based static taint analysis approach. They achieved high accuracy, around 97%, outperforming previously proposed models. In a comparative study, Zheng et al. (2020) went deeper and analyzed the impact of factors on the performance of multiple machine learning-based vulnerability detection model such as deep learning including BLSTM, CounterVectorizer, Random Forest and others. As a results, they found that deep learning models achieve better performance.


Table 1. Summary of vulnerability related work.

Category	Paper	Description	Models used
Vulnerability detection	Niu et al. (2020)	Discover and locate vulnerabilities in programs	Bidirectional Long Short Term Memory (BLSTM)
Zheng et al. (2020)	Analyzed the impact factors on the performance of multiple machine learning-based vulnerability detection model	Deep learning including BLSTM, CounterVectorizer, Random Forest and others
Vulnerability prediction	Movahedi et al. (2019)	Predicting vulnerabilities that may be discovered in a product	Artificial Neural Networks (ANNs)
Yasasin et al. (2020)	Predicting time series of vulnerabilities	Croton’s methodology, ARIMA, ANNs
Kerem and Fatih (2021)	Focus on Android related vulnerabilities	Convolutional LSTM (ConvLSTM), CNN-LSTM, Long Short Term Memory (LSTM), Multilayer Perceptron (MLP), Time series and convolutional neural network (CNN)
Vulnerability scoring	Zhang et al. (2019)	Vulnerability scores through the results of popular search engines	n-gram-based approach, Support vector machines (SVM)
Zou et al. (2019)	Assess the severity of vulnerabilities based on the attack process	–
Han et al. (2017)	Predict multi-class severity based on vulnerability description	Word embedding and a single-layer shallow convolutional neural network (CNN)
Chen et al. (2020)	Classify the severity of vulnerabilities based on vulnerability description	Term frequency-inverse gravity moment (TF-IGM)
Spanos and Angelis (2018)	Predict vulnerability scores based on vulnerability description	Text analysis
Target classification
Exploitability	Sabottke et al. (2015)	Twitter-based exploit detector	SVM
Yin et al. (2020)	Vulnerability description to predict the exploitability	Bidirectional Encoder Representations from Transformers (BERT)
Vulnerability disclosure and attacks	Syed et al. (2018)	Analyzed the factors that impact the retweets of software vulnerabilities	Negative binomial regression
Arora et al. (2007)	Increase of attacks after a vulnerability disclosure	–
Jumratjaroenvanit and Teng-Amnuay (2008)	Analyzed factors of vulnerability that contribute to attack increase	–
Software companies look forward to predicting vulnerabilities for multiple reasons such as resource allocation. Roumani et al. used time series models to predict vulnerabilities in five major browsers (Roumani et al., 2015). They accordingly found that the model can be useful for prediction. Instead, Movahedi et al. used Artificial Neural Networks (ANNs) for predicting vulnerabilities that may be discovered in a product (Movahedi et al., 2019). They found that the ANNs achieve better results compared to other Vulnerability Discovery Models (VDMs). In another study,  Yasasin et al. (2020) applied prediction algorithms on time series of vulnerabilities such as Croton’s methodology, ARIMA (autoregressive integrated moving average) and ANNs. They found that the optimal forecasting methodology depends on a target system or software. However, Kerem and Fatih (2021) point out the importance of the focus on Android-related vulnerabilities because of their spread of use. They trained multiple models such as Convolutional LSTM (ConvLSTM), CNN-LSTM, long short term memory (LSTM), multilayer perceptron (MLP), ARIMA and convolutional neural network (CNN). They found that LSTM based algorithms’ performance is good compared to classical time series models.

Each newly discovered vulnerability has a text description related to it in addition to other features and scores assigned later by security experts. Features and scores are important to be assigned quickly to use new vulnerabilities on other models. Multiple studies attempted to automatically score vulnerabilities. Zhang et al. designed an automated approach that uses only the search results of vulnerabilities on popular search engines (Zhang et al., 2019). The designed approach gives vulnerability scores without the need for a human expert. The approach is faster in forecasting and able to capture the continuous evolution (new exploits) of vulnerabilities. Zou et al. (2019) proposed an approach to automatically assess the severity of vulnerabilities based on the attack process, called the Automatic Common Vulnerability Scoring System (AutoCVSS). They found that automatic assessment of the severity of vulnerability through the AutoCVSS is essentially the same as that manually assessed by security experts in the National Vulnerability Database (NVD). Han et al. (2017) developed an efficient and reliable deep-learning approach to predict multi-class severity levels of software vulnerabilities using only the vulnerability description. The approach uses word embedding and a single-layer shallow convolutional neural network (CNN) to automatically capture the discriminatory characteristics of words and phrases in vulnerability descriptions. Chen et al. (2020) used a framework base on term frequency-inverse gravity moment (TF-IGM) to classify the severity of vulnerabilities, while  Russo et al. (2019) categorized vulnerabilities according to a defined taxonomy by extracting and classifying information of vulnerability descriptions.  Spanos and Angelis (2018) also used the vulnerability description, but in this case to predict the scores of vulnerabilities using a developed model that combines text analysis and target classification techniques.

A study was done about the ratio of vulnerabilities exploited in the real world by Nayak et al. (2014). They found that none of the products they studied had more than 35% of their disclosed vulnerabilities exploited in the wild. Therefore, many studies about the exploitability of vulnerabilities have been conducted. Sabottke et al. (2015) described the design of a Twitter-based exploit detector that has fewer false positives than a common vulnerability scoring system (CVSS) based detector to prioritize which vulnerabilities are likely to be exploited. They conducted a quantitative and qualitative exploration of the vulnerability-related information disseminated on Twitter as this latter’s data provide early warnings for the existence of real-world exploits. Bullough et al. (2017) replicated and compared key parts of previous work on predicting vulnerability exploitation and identified several methodological considerations that need to be taken into account when building and evaluating predictive models. However, they conclude that the software vulnerability exploitation models based on the NVD database and social media alone are unlikely to have sufficient predictive power to be useful in practice. Recently, Yin et al. (2020) used vulnerability description to predict the vulnerability exploitability. They designed a framework named ExBERT which is an improved Bidirectional Encoder Representations from Transformers (BERT). Their results were around 91% of precision and accuracy, outperforming similar models.

Besides the exploitability, the vulnerability disclosure was also of interest to researchers. Attackers take advantage of the information about the disclosure of newly discovered vulnerabilities shared in security databases and platforms. They start attacking devices before they get new patches, which may increase the rate of cyber incidents. Syed et al. (2018) analyzed the factors that impact the retweets of software vulnerability tweets and defined the major content categories of tweets. They also used negative binomial regression and found that multiple factors related to the vulnerability features, tweet source, etc., impact the retweeting. Arora et al. (2007) have done a study about the increase in attacks after a vulnerability disclosure. They empirically explored the impact of vulnerability information disclosure and availability of patches on attacks targeting the vulnerability. They found that the secret vulnerabilities get exploited fewer times than patched vulnerabilities while published vulnerabilities without patches get exploited more often. Jumratjaroenvanit and Teng-Amnuay (2008) found, by analyzing 439 vulnerabilities, that various factors such as the availability of patches and exploit codes, contribute to the probability of an attack. Another study by Farhang et al. (2019) Focused on Android vulnerabilities and their implications on the consumers, the public vulnerability disclosure and the Android ecosystem together.

This large amount of work about software vulnerability is explained by its importance as one of the widely used attack vectors. A decent number of recent works we presented about vulnerability analysis used machine learning-based approaches to solve detection and prediction tasks. They mostly agree on the efficiency of machine learning algorithms in achieving good performance. Our work combines network scans with software vulnerability using machine learning-based approaches.

As for studies that analyzed the relationship between scans and vulnerabilities, there is a study (Durumeric et al., 2014) where Durumeric et al. extensively analyzed scans traffic in terms of the targeted services and countries, scan sources, scanning tools used, and information about the scans rates. They found that new high-speed scanning tools offer complete IPv4 address space scans in a reasonable amount of time. Therefore, they included in their study a case study of three vulnerability disclosures affecting Linksys routers, OpenSSL, and network time protocol (NTP). They found that scans started from bullet-proof hosting providers within two days of the vulnerability disclosure. Similarly, when the Heartbleed vulnerability was discovered, there was an increase in the volume of scans targeting the 443/TCP port (Lee et al., 2014).

The previous work we presented is either analyzing vulnerabilities or studying scans separately. To the best of our knowledge, this is the first work to propose a model that assesses the impact of vulnerability disclosure on network scans. Our model focuses on using recent approaches, which are machine learning-based, to not only show the existence of an overall impact of vulnerabilities on scans, but also to create a framework that is capable of assessing that impact of each new vulnerability. In addition, we opted to use telescope network data because of its wide use as a source of cyber threat intelligence (Fachkha and Debbabi, 2016) in multiple studies to infer denial of service (DoS) attacks, distributed denial of service (DDoS) attacks, worm activities, probing activities (Galluscio et al., 2017) and botnets (Antonakakis et al., 2017).

3. Methodology
In this section, we present the data and tools we used in this work. Fig. 1 summarizes the workflow of data processing alongside algorithms used in each step. We explain them in further detail in this paper. In summary, we clean and pre-process the vulnerability and telescope network datasets. Then, we extract the scan’s time series from the telescope network data, and we extract the vulnerability disclosures as timed events. We use an algorithm to detect the impact of each vulnerability disclosure on time series of scans. The result is a table of vulnerabilities described by their features and a label of the impact of each vulnerability. That table is then used to train machine learning-based classifiers.

3.1. Data collection & processing
To study the effect of vulnerabilities on scans, we collected a dataset of network scans and aggregated them by the target port. This dataset contains the number of scans targeting a specific port over time. We also collected a dataset about all vulnerabilities with their publication date. Finally, we collected the dataset of information about port numbers. We use it to map each vulnerability to the corresponding port it may affect.

3.1.1. Network scans traffic
A network telescope (darknet, black hole) is a sensor logging the traffic received by a set of passive unallocated network addresses. Network telescopes are used to catch port scans, worm activities, traffic of DoS activities replaying on spoofed IP addresses, etc.

The darknet provides an overview of events in the global IP space, especially the information about malicious and misconfiguration traffic. It can be deployed in many organizations, such as Internet Service providers ISPs, universities and research labs. Attackers do not have a reliable technique to distinguish darknets from the rest of the used address space (Cooke et al., 2006). Thus, the received traffic on the darknet represents to some extent the traffic targeting the rest of the address space.

The traffic in organizations can contain malicious traffic from sources such as network scanners, DoS/DDoS attacks, worms, misconfigured devices, etc., as well as regular and benign traffic generated by users on the organization’s network. The network traffic we use is collected from a network telescope which does not have any users traffic to filter out, nor does it contain hosted services to be targeted by attackers. Thus, the network is not related to an organization to be a target of a DoS/DDoS attack, which makes scans easy to be extracted from the traffic, especially that we only select TCP SYN packets.

We use private data that is collected by two /20 (4096 hosts) network telescopes in different countries. The first telescope network is hosted at INRIA Nancy-Grand Est in France (Beck and Festor, 2008), and the second is hosted by NICT (2021) in Japan. The traffic was recorded from the 11th of November, 2014, to the 1st of December, 2019.

We collect traffic coming from different sources, through the traffic logger, and store it in the database. Fig. 2 shows how the traffic is collected. First, the generators of traffic such as network scanners, hosts infected with malware, and hosts that are victims of DoS attacks send traffic to all the IPv4 address space. Second, each of the two address blocs (INRIA and NICT) is composed of 4096 public IPs and catches the traffic targeting it and sends it to the monitoring/logging system. The received traffic in form of packets is stored as packet captures (PCAP file format) then stored in the system. Finally, a script is run to extract important information from the packet captures, then to store it in comma-separated values (CSV) files, or to send it to a system like ElastiSearch Logstash Kibana (ELK). Information extracted from the packet captures is described in detail in Table 2.

For each packet, we save the important information such as the source and destination IP addresses, the protocol used (TCP, UDP, …), and other information like the port number in the case of UDP and TCP protocols. We focus mainly on TCP scans and we select only TCP packets with the SYN flag because it is the most used scan type. Fig. 3 represents the steps we follow to process the received packets to extract time series of scans. We filter out all the other protocols to select only TCP packets with the SYN flag. Next, we put together the scans targeting the same port.


Download : Download high-res image (468KB)
Download : Download full-size image
Fig. 2. Telescope network.


Table 2. Information extracted from network telescope captured packets.

Information	Description
Timestamp	The Unix timestamp at which the packet was captured
Date	Human readable date at which the packet was captured
Source IP	The source IP address in the packet
Destination IP	The destination IP address in the packet (which is one of our 4096 public IP addresses)
Protocol number	The number of the protocol used for the transport layer (TCP:6, UDP:17, etc.)
Source TCP port	The source port used if the protocol is TCP
Source UDP port	The source port used if the protocol is UDP
Destination TCP port	The destination port used if the protocol is TCP
Destination UDP port	The destination port used if the protocol is UDP
TCP flags	The TCP flags used in the packet if the protocol is TCP
ICMP type	The name of the ICMP message type if the protocol used is ICMP
3.1.2. Ports data
The data about port registration is managed by Internet Assigned Numbers Authority (IANA). In addition to the official list published by IANA, we use other datasets containing information about port numbers. These datasets may contain ports that are widely used in the real world without being registered by IANA. In our study, we used the following datasets:

3.1.2.1. IANA data.
Internet assigned numbers authority (IANA) is a standard organization that is responsible for the allocation of globally unique numbers and names on the internet, such as IP addresses, Autonomous System (AS) numbers, official assignments of port numbers for specific uses, etc. We use a list of registered ports published on IANA’s website (IANA, 2020). We have for each port: the name of the service, the transport protocol (TCP, UDP, …), and a description of the port service.

3.1.2.2. Linux services names.
Most UNIX-based operating systems have a file stored at /etc/services which contains a list of well-known ports. It is used by applications to translate human-readable service names into port numbers (Debian, 2017). The information in this file is collected from Internet Requests for Comments (RFCs) and other sources. Table 3 shows an example of the Kerberos service which runs on port 88 of both transport protocols UDP and TCP, and has 3 aliases.

3.1.2.3. Nmap services names.
The well-known ports scanner Nmap (2020) maintains a list of port numbers and their related service names. It is used to convert the port number of scans to human-readable names. The list is derived from IANA data and other sources (nmap, 2020).


Table 3. Example of Linux services names data.

Service name	Port/protocol	Aliases	Comments
kerberos	88/tcp	kerberos5 krb5 kerberos-sec	# Kerberos v5
kerberos	88/udp	kerberos5 krb5 kerberos-sec	# Kerberos v5
3.1.2.4. Wikipedia ports list.
Wikipedia (2020) has a crowdsourced updated list of port numbers and the related services to them. The data is derived from IANA and other sources.

3.1.3. Vulnerability data
Data about vulnerabilities is collected, preprocessed, and mapped to the related port numbers. More details about the preprocessing and mapping can be found in Section 4.1.3. The features of each record are described in detail as listed in Table 4.

We collected vulnerabilities that appeared on the same date as the telescope network’s probes from the NVD database. The US National Vulnerability Database (NVD) (NVD, 2020) data is a vulnerability database built upon and fully synchronized with the CVE list from MITRE. Mitre Corporation (Mitre, 2020) has been maintaining a list of publicly disclosed software vulnerabilities known as Common Vulnerabilities and Exposures (CVE) list since 1999. The vulnerabilities’ data we used is collected from the NVD website in JSON format. In addition to MITRE’s CVEs list, NVD data includes fixes, severity scores, and other information for identifiers on the CVE list.

Each CVE entry has many attributes, including:

•
CVE ID: A unique identifier of the vulnerability

•
Publication date: The date when the vulnerability was made public

•
Description: A descriptive text of the vulnerability

•
CVSS scores: Vulnerabilities’ severity as scored using Common Vulnerability Scoring System (CVSS) (FIRST, 2020).

•
Vendors/products: The list of the affected products and their vendors.

We pre-process all the data we collected. We clean it to remove duplicates, fill missing values and handle incorrect ones. Then, we store the data to apply algorithms which will be explained in the following.


Table 4. Table of features.

Feature name	Description
CVSS v3	CVSS-attackComplexity	Has two values: High and Low. A vulnerability with high attack complexity means that there are conditions beyond the attacker’s control that the success of the attack depends on.
CVSS-attackVector	Reflects where it is possible to exploit the vulnerability. It has four values: Network, Adjacent, Local and Physical. It has a higher impact the more remote the attacker can be.
CVSS-baseScore	It scores the overall severity of the vulnerability. It is computed based on the CVSS-impactScore, CVSS-exploitabilityScore, and the Scope. It has a value ranging from 0 to 10
CVSS-base Severity	It is a textual representation of the CVSS-baseScore. It sets four levels of severity: None, Low, Medium, High and Critical
CVSS-confidentialityImpact	Describes the impact of a successful exploit of the vulnerability on confidentiality. It has three values: None, Low and High
CVSS-integrityImpact	Describes the impact of a successful exploit of the vulnerability on integrity. It has three values: None, Low and High
CVSS-availabilityImpact	Describes the impact of a successful exploit of the vulnerability on availability. It has three values: None, Low and High
CVSS-privilegesRequired	To successfully exploit the vulnerability, an attacker should be required to have some privileges. It has three values: None, Low and High. Higher required privileges mean less vulnerability base score.
CVSS-scope	This metric captures the change that happens in the impact scope after the vulnerability exploit. Its value is “Changed” If an exploit of the vulnerability affects resources out of the security scope of the vulnerable component unlike when its value is “Unchanged”. A change in the affected scope means a higher base score of the vulnerability.
CVSS-userInteraction	Has two values: None or Required. “Required” means that a human user action is required to complete the attack. The base score of a vulnerability is higher when no user interaction is required
CVSS-exploitabilityScore	It scores the complexity of exploiting the vulnerability. It is computed based on CVSS-attackVector, CVSS-attackComplexity, CVSS-privilegesRequired, and CVSS-userInteraction
CVSS-impactScore	It gives a score of the general impact of the vulnerability. It is computed based on the CVSS-availabilityImpact, CVSS-integrityImpact, CVSS-confidentialityImpact, and CVSS-scope
Added features	Number of the affected port	A vulnerability can be found to be related to more than one port (after the vulnerability port mapping). This feature represents the number of ports that were found to be related to the vulnerability.
Port class	After a port is mapped to the vulnerability, we assign a class to it depending on the port number assigned. Ports are put into three classes: well-known ports [0-1023], registered ports [1024-49151] and dynamic ports [49152,65535]
Port darknet score	This information shows how much the port is important in our dataset. We get statistics about highly targeted ports in the network telescope, then we assign a score from 0 to 1 to each port. A score close to zero means that the port is not very important, whereas a score close to 1 means that the port is highly targeted by attackers and a vulnerability related to that port is important.
Product score	The score we compute for each product based on its spread of use. See 4.1.7 for more details.
Publication date	The date when the vulnerability was made public
System type	This feature is extracted from the information about the vulnerability. It describes the type of the vulnerable product. The type of the system could be:
 • “o” : an operating system,
 • “a” : an application
 • “h” : a hardware product
Daily disclosures	The number of vulnerabilities that are published in the same day. For each new disclosed vulnerability, we count the number of disclosed vulnerabilities in the same day and add that number as a feature to the vulnerability.
Vendor score	The score we compute for each vendor based on the spread of its products. See 4.1.7 for more details.
Port number	The port number that is found to be related to the vulnerable product. The assigned port number is the results of the mapping between ports and vulnerabilities we did 4.1.5.
Registration status	Contains a boolean that indicates whether the affected port is registered by IANA or not.
3.2. Analysis of historical vulnerability disclosure impact on scans time series
Many tools have been developed to analyze correlations between time series and correlations between timed events. However, few studies have developed hmethods for assessing the correlation between time series and sequences of events. Because the occurrence of an event may not be a point-to-point correlation but associated with a change in a given period of time, applying traditional correlation analysis, such as Pearson or Spearman, often fails to produce satisfactory results when applied to time series and events data. Luo et al. (2014), proposed an approach to evaluate the correlation between time series and timed event data. The proposed approach studied the changes in sets of sub-time series before and after each event. They could evaluate the existence of correlation, temporal order, and monotonic effect. A similar approach was proposed by Xun et al. (2016), but using multi-type time series and multi-type events instead of single time series. In our work, we applied the method proposed by Luo et al. (2014) because it is the most suitable for our work.

We study the impact of vulnerabilities, transformed to time events, on scans evolution which is represented as time series. We chose to use the method proposed by Luo et al. (2014). It studies three aspects of the time series and events dependency. First, they study the existence of dependency between the time series and events. Second, if the dependency exists, they study the temporal order of the effect which can determine if the events are triggering the time series change, or it is the other way around. Finally, they study the effect type, in which they determine the result of the impact whether it is an increase or a decrease in the time series.

The method proposed by Luo et al. (2014) consists of dividing a time series into small sub-series of length k near each event. For each event, we extract the sub-series of length k before the event and a sub-series of the same length after the event. We then compare the two sub-series to detect whether there is a change in the statistical features of the time-series when the event occurs.

The method is capable of assessing three aspects of the relation between events and time series:

•
Dependency existence: Shows the existence of a correlation between an event sequence and a time series without concluding a causality relationship.

•
Dependency temporal order: Describes the directions of the correlation which can help in the causality analysis. If a significant change in a time series always follows the occurrence of an event, we can assume and verify the event’s occurrence causing a change in the time series.

•
Dependency monotonic effect: The occurrence of an event may be related to an increase or a decrease in time series values. Information about the monotonic effect of events on the time series can also help in the causality study of the correlation.

The method is depicted in Fig. 4. It takes a time series and an event sequence then checks if the two are correlated. Given a time series 
 with an even sampling interval and an event sequence  with timestamps 
. From the time series S, we extract 2 sub-series of length k for each event 
 in . A sub-series before the occurrence of 
 is denoted as 
 and a sub-series after the occurrence of 
 is denoted as 
. We create the first set for front sub-series of all events from  is denoted as 
, the second set for rear sub-series for all events from  is denoted as 
, and the third set for randomly sampled sub-series from S is denoted as .


Download : Download high-res image (647KB)
Download : Download full-size image
Fig. 4. Impact detection of events on time series.

We say that the event sequence is correlated with the time series  if 
 rear is statistically different from the randomly sampled time series , or 
 is statistically different from . If the event occurrence is related to a significant increase (or decrease) in the values of the time series we denote it as 
 (or 
). A multivariate two-sample test is used to compare the two sets of sub-time series, 
 with  and 
 with , to check if they are from the same distribution. Given two sets of sub-time series  and  which can be considered as samples from different distributions 
 and 
. We use the following test statistic: (1) 
 

If 
 is true, this means that the two distributions are statistically different. As a result, the events and time series are correlated. If 
 is true, then the events and time series may not be correlated.

To assess the monotonic effect of the events on time series, they used a t-test with the following equation for calculating the 
: (2)
 
 

Where 
 and 
 are the mean values of 
 and 
, 
 and 
 are the variances of 
 and 
 and  is the number of events.

In this work, we study only the existence of the positive effect of events on scans time series. The nature of the data we use makes the temporal order always from events to time series.

Publication of the vulnerabilities can lead to a change in the volume of scans, but a change in the volume of scans cannot lead to the publication of a new vulnerability. The second aspect which is the existence of the dependency cannot be studied because vulnerabilities have different features and severities. As a result, the event of “the publication of a vulnerability” does not always have the same effect because of the importance and severity of each vulnerability.

We assume that the effect lasts for some time interval, which is the case in vulnerability data. Because when the vulnerabilities are published, their patches are released, and that takes some time to be applied by the majority of the product’s users. After some time interval, the majority of users patch their systems which makes the vulnerability obsolete and not interesting for attackers to look for vulnerable devices containing it.

3.3. Vulnerability feature selection
Selecting important features helps reduce computation time to produce fast models and can sometimes improve the models’ performance. We used multiple feature selection methods:

•
Variance: We eliminate all features having a variance below a specified threshold. As a result, it will remove all constant features. The value of the variance represents the expected deviation of the values of a random variable from its mean.

•
Correlation matrix: We compute the Pearson’s correlation matrix of all features, then we keep only the features that are highly correlated and eliminate the others based on a threshold.

•
Decision Tree: We train a decision tree model on the data to select the important features based on the node impurity.

3.4. Prediction of vulnerability disclosure impact on scans time series
We predict whether a new vulnerability can have an impact on scans time series using a classification model. We construct a dataset that consists of the features of the vulnerabilities and tells whether they have an impact on scans time series during their publication.

Furthermore, we train the following models on data to predict the impact of vulnerabilities:

•
Random forest: It is a model composed of multiple decision trees. It fits the decision trees on various sub-samples of datasets. It then uses the average decision of trees in order to improve performance and control over-fitting. Random forest generally performs better than the decision tree, but it is slower.

•
The Naïve Bayes algorithm: It is based on the Bayes theorem. It computes the probability that a data point belongs to a particular category, assuming independence between pairs of features. Given a dataset with  classes 
 and  features 
, we assign to a new instance 
 the class with the highest probability. Naïve Bayes classifiers are fast, and they work well in many situations and require little data to learn, but they are generally known to be bad estimators.

•
SVM: A support vector machine represents the data in space and then partitions the classes by a hyperplane. It classifies new instances by mapping them to the same space and determining which side of the hyperplane they belong to. The simple case is the one of a linear discriminant function, obtained by the linear combination of the input vector 
, with a weight vector 
. It is then decided that  is of class 1 if  and of class 1 otherwise. For non-linearly separable data, a transformation is applied to map the data to a higher dimensional space, then a kernelized version of the SVM is used. In this case, the common kernels that can be used are Polynomial kernel, Radial Basic Function (RBF) kernel, and Sigmoid kernel. SVM is efficient in high-dimensional spaces but has poor performance when the number of features is larger than the number of samples.

•
Decision trees: They generate sequences of classification rules for numerical and categorical data that are easy to understand and visualize. Tree construction can be done using different algorithms (Sharma and Kumar, 2016) such as CART (Timofeev, 2004), ID3, C4.5 and C5.0. Their disadvantage is the generation of complex trees, and they can be unstable if there are small variations in the training data, leading to large changes in the generated tree.

After presenting all the algorithms we used to process data and build the classifiers, we show how we apply the methodology to implement the proposed model specified for the classification of software vulnerability impact on network scans.

4. Proposed framework for forecasting vulnerability disclosure impact on scans time series
In this section, we present the process we followed to go from the scans and vulnerabilities datasets to a classifier that is able to tell whether new vulnerabilities can have an impact on the scans’ evolution over time. We also present the results of each model.

The proposed model is composed of three main parts: data processing, vulnerability impact detection, and classifier training. We present details of each step in the following.

4.1. Scans and vulnerability data processing
The data sources we use were cleaned from duplicates and missing values. The next step consists of transforming the data to be used to train the vulnerability classifier.

4.1.1. Scans time series extraction
Let P be the set of (
) ports 
,  the set of  ports scans 
, and 
 be a port from . We create 
 sets of scans by ports: 
. We note that the 
 and 
. In other words, the sets of scans by ports are pairwise disjoint and cover S, which means that every scan is targeting only one port.

We use two methods to transform scan packets into time series representing scans. The first is to count the number of received packets by port each day as the scan rate on that port. The second is to count the number of distinct IP addresses trying to scan the target port in each new day. The second method has the advantage of focusing on the evolution of the number of scanners of the system. Especially, if a vulnerability is discovered, it is more likely that we have many scanners trying to get into the system, rather than one attacker sending multiple packets. In the rest of the paper, the first method will be called scans count by packets and the second will be called scans count by scanner. To deal with incorrect values, we delete packets that contain incorrect or missing values. In addition, the telescope network stopped multiple time capturing packets during time intervals less than a day. We corrected these values by averaging the values of the traffic before and after the missing time interval.

4.1.2. EDA: darknet data
Our proposed method focuses on ports and time series of scans. We started with an exploration of the dataset to understand the distribution of scan traffic over ports and over time.

4.1.2.1. Traffic over time .
Plotting the evolution of scans over time shows that they do not follow a regular pattern. Fig. 5, Fig. 6 describe the evolution of the received traffic over time. Fig. 5 shows the evolution of the number of attackers by day for the INRIA and the NICT darknet datasets. Fig. 6 shows the number of the daily packets received by each network telescope. Despite the difference in geographic locations, in both figures, the number of packets and scanners are close to each other in both datasets. It also shows a similar evolution over time. That could be explained by the nature of the traffic caught by the telescope networks which contains scans that are location agnostic and are generally run in a worldwide manner. We also observe that the shapes of the two time series in Fig. 5, Fig. 6 are different. That means that the mean number of packets sent by each attacker is not constant but changes over time.

4.1.2.2. Top target ports.
In this study, we are mostly interested in ports. Therefore, it is important to get more statistics about their distribution. We aggregate all the traffic received in the darknet by port numbers, then we count the total traffic targeting each port and divide it by the total amount of the traffic. This percentage represents the interest of attackers in the services provided by that port. Fig. 7 represents the percentage of each of the top 15 ports by the darknet dataset source. The ratio represents the portion of unique attackers that are interested in that port. The results of the figure show that the two datasets, INRIA and NICT, have nearly the same distribution of traffic over ports. Moreover, attackers mainly target remote access services, such as Secure shell protocol (SSH) (port 22), Telnet (port 23), or TR-069 protocol used for remote management of common platform enumerations (CPEs) on port 7547, where more than half of the attackers were interested in the Telnet (port 23) service and its alternative port 2323. Other services are Database management systems, web services, and Microsoft services.


Download : Download high-res image (252KB)
Download : Download full-size image
Fig. 5. Daily number of attackers time series.


Download : Download high-res image (262KB)
Download : Download full-size image
Fig. 6. Daily number of packets time series.

4.1.3. Vulnerabilities data preprocessing
In the following, we will describe the steps of processing the vulnerabilities dataset. The vulnerabilities data we use are stored in JSON (JavaScript Object Notation) files. Each one contains all published vulnerabilities in one year. We merge all the files into one file, then we do the following for each vulnerability:

•
We filter out rejected vulnerabilities, which are labeled with the string “**REJECTED**” in the description. More details about rejected vulnerabilities could be found in MITRE (2021).

•
We extract information such as the vulnerability category: Common Weakness Enumeration (CWE).

•
We extract the affected product name from affected configurations. Each product is identified by its common platform enumeration (CPE).

In this study, we select only vulnerabilities that can be exploited over the network, as they can be related to the network scan rate changes. Exploitability over the network can be found in the attack vector attribute of the vulnerability which is one of the four vulnerability exploitability metrics (Attack vector, Attack complexity, Privileges required and User interaction). The exploitability metrics of a vulnerability describe the complexity to exploit it.

The attack vector reflects where it is possible to exploit the vulnerability. A higher score is given to vulnerabilities that can be exploited remotely rather than the ones requiring to have physical access. This is explained by the number of attackers potentially having access to that vulnerability. This metric can have four possible values:

•
Network: Whether the vulnerability can be exploited through the Internet

•
Adjacent: The attack is limited to the same shared physical or logical network

•
Local: The vulnerability can only be exploited locally, or it may require a user interaction

•
Physical: The attacker should have physical access to the vulnerable component

4.1.4. EDA: Vulnerabilities data
The vulnerability dataset contains 50320 vulnerabilities. Fig. 9 shows the evolution of vulnerabilities over time. The rolling mean with a window of 60 days shows that after 2017 there was an increase to roughly 50 vulnerabilities published by day.

4.1.5. Mapping vulnerabilities and ports
In order to study the impact of vulnerability disclosure on network scans, we should select only vulnerabilities that can be exploited over the network, then map them to the ports that they can affect. Thus, for each time series of scans targeting a port, we select the associated vulnerabilities affecting the same port. The dataset about the ports information (see 3.1.2) will help identify the port that is usually opened by a vulnerable product. The open port will be the target of attackers, and its scan rate is likely to increase. The whole mapping process is shown in Fig. 10. We combine the ports datasets (IANA, Nmap, and Linux services) into one dataset that contains information about ports: the port number, a text description about the port, and the name of the service. Afterward, we use the created dataset to look for the product name of each vulnerability. If the product name is found in the dataset, we assign the port related to the product name to the vulnerability.


Download : Download high-res image (275KB)
Download : Download full-size image
Fig. 10. Vulnerabilities port mapping: A port number is assigned to each row of the dataset. A row contains a vulnerability described with its features 
 to 
.

Let P be the set of (
) ports 
,  the set of  vulnerabilities 
, and 
 be a port from . We create a set of vulnerabilities by ports which results in 
 sets of vulnerabilities: 
. For the vulnerabilities sets, we have 
 which means that there are vulnerabilities that are not related to any port, and 


, which means that one vulnerability can be related to more than one port.
The two features of vulnerabilities that can be used to infer the related ports are the text description and the product name. Each vulnerability has a description field that contains a brief description of the vulnerability. The authors of vulnerabilities sometimes mention the ports that can be related to and open by the affected product. Some examples of these mentions are in Table 5.

The ports can also be inferred from the affected product, but in this case, we should use another dataset to find ports usually open by the product. We propose to use four datasets described in the ports data section. We merge those four datasets, as shown in Fig. 10 to get a table where each port has a list of products that usually use it. We query the table of ports for the product name. If the product is found, we assign all the found ports to the vulnerability. This process results in four sets of vulnerabilities. The first set has no ports related to it. The second set contains vulnerabilities labeled with the ports that can be affected if the vulnerability is published. We note that some vulnerabilities can have multiple ports related to them.


Table 5. Examples of port extraction from vulnerability description.

Vulnerability description	Extracted TCP ports
“... ports 23/tcp and 22/tcp ...”	22,23
“... ports 8080/tcp and 9000/udp ...”	8080
4.1.6. Results of mapping vulnerabilities and ports
The process of vulnerability port mapping resulted in a ratio of 18.83% of vulnerabilities having a port related to them. Extraction of the port number using the description and the product name gives us a wider view of ports related to the vulnerability. Table 6 shows some examples of vulnerabilities labeling. The vulnerability with ID CVE-2014-2976 was discovered in SixView Namanager, we extracted the related port 18081 from the description. The second vulnerability has the ID CVE-2017-15535, and it was discovered in the MongoDB database management system. We extract all the ports related to the product name. The third vulnerability was found in MySQL and MariaDB database management systems. We extract all the ports related to the product which are mostly services used with MySQL. The related ports are:

•
7307: Zimbra mysql

•
33060: MySQL Database Extended Interface (mysqlx)

•
1862: MySQL Cluster Manager Agent (mysql-cm-agent)

•
2273: MySQL Instance Manager (mysql-im)

•
1186: MySQL Cluster Manager (mysql-cluster)

•
6446: MySQL Proxy (mysql-proxy)


Table 6. Examples of port extraction from vulnerabilities.

CVE ID	Vulnerability description	Vendor	Product	Extracted TCP ports
CVE-2014-2976	”Directory traversal vulnerability in Sixnet SixView Manager 2.4.1 allows remote attackers to read arbitrary files via a .. (dot dot) in an HTTP GET request to TCP port 18081.”	[’sixnet’]	[’sixview manager’]	[’18081’]
CVE-2017-15535	”MongoDB 3.4.x before 3.4.10, and 3.5.x-development, has a disabled-by-default configuration setting, networkMessageCompressors (aka wire protocol compression), which exposes a vulnerability when enabled that could be exploited by a malicious attacker to deny service or modify memory.”	[’mongodb’]	[’mongodb’]	[’27017’, ’27019’, ’28017’, ’27018’]
CVE-2016-9156	”A vulnerability in Siemens SICAM PAS (all versions before V8.09) could allow a remote attacker to upload, download, or delete files in certain parts of the file system by sending specially crafted packets to port 19235/TCP.”	[’siemens’]	[’sicam_pas’]	[’19235’]
CVE-2016-3492	”Unspecified vulnerability in Oracle MySQL 5.5.51 and earlier, 5.6.32 and earlier, and 5.7.14 and earlier allows remote authenticated users to affect availability via vectors related to Server: Optimizer.”	[’mariadb’, ’oracle’]	[’mariadb’, ’mysql’]	[’6446’, ’7307’, ’1186’, ’2273’, ’33060’, ’1862’, ’7306’, ’3306’]
4.1.7. Feature space augmentation
The discovery of vulnerabilities in well-known products is more likely to generate a larger volume of attacks than a discovery of a vulnerability in a non-popular product. That is because the popular products are generally related to high usage numbers, and one discovered vulnerability will reach the largest segment of targets.

Adding such information to the vulnerabilities data could help the impact assessment model detect the most important vulnerabilities. As we know, every product has vulnerabilities that are not yet discovered, also known as zero days. Popular and important products attract more attackers and security researchers to find them, and this leads to more vulnerabilities discovery in those products. Therefore, the number of discovered vulnerabilities may be an indicator of the product’s popularity. For each vulnerability, we have the name of the affected product and the name of the vendor of that product. We plot the top products and vendors in terms of the number of published vulnerabilities. Fig. 11, Fig. 12 show that the popular vendors and products have the biggest number of vulnerabilities. Thus, adding this information can help the model to detect the vulnerabilities of important products. We assigned two scores to vulnerabilities. The first is the number of vulnerabilities found in the product. The second is the number of vulnerabilities discovered in all the products of one vendor.

4.2. Detection of vulnerability impact on scans
The data processing resulted in a time series of network scans and timed events of vulnerability disclosure related to each port. The question is how to check if a vulnerability has a positive impact on the scan’s volume. The typical scenario is that when a vulnerability is discovered, attackers tend to scan ports related to the vulnerable product. Thus, our study should focus on the effect of the vulnerabilities related to each port on the scans targeting the same port.

An example of the algorithm we use is presented in Fig. 13. It gets two inputs: a scans time series and a timed event sequence for the same port. Then, it extracts sub-series before and after each vulnerability disclosure event. After the execution of the test statistic, it detects that vulnerabilities 1 and 3 had an impact on the scans time series, while vulnerability 2 had no impact. Finally, the result is saved to be used to train the classifier model.

We study the impact of each port’s vulnerabilities on the scans time series targeting the same port. For ,  and  as defined earlier, and a given port 
, we study the impact of the set of vulnerabilities 
 on the set of scans 
. The set of scans 
 contains all the scans targeting the port 
. Each scan is a time series representing the evolution of the intensity of the scan over time. We note 
 the result of combining all the scans time series in 
 which will be a time series representing the evolution of scans intensity for the port 
 over all the period of the study. For each vulnerability in 
, with 
 the number of vulnerabilities related to port 
. We extract the corresponding front and rear sub-series of length , with  representing the size of the window used to extract sub-series before and after each vulnerability disclosure. We note 
 the sub-series of length  before the publication of the vulnerability 
, and we note 
 the sub-series of length  after the publication of the vulnerability 
.


Download : Download high-res image (362KB)
Download : Download full-size image
Fig. 13. Impact detection of vulnerabilities on scans.

We can now create a set of pairs of sub-series for each vulnerability publication related to port 
 as follows: 
. We create a set  composed of pairs of all the sub-series for all ports in 
. We use a two-sample t-test to check if the front and rear sub-series are statistically different. We formulate our hypothesis for the th vulnerability 
 of port 
 and the scans time series of the same port 
 as follows: (3)
 

Where 
 is the mean of the front sub-series before publication of the vulnerability 
 and 
 is the mean of the rear sub-series after publication of the vulnerability 
. If the mean of the rear sub-series is significantly larger than the mean of the front sub-series, we reject the null hypothesis 
, and we conclude that the publication of the vulnerability 
 increased the volume of scans for port 
. If the two means are not significantly different, we fail to reject the null hypothesis, and we conclude that the publication of the vulnerability does not affect the volume of scans.

The result of testing all the pairs in the set  allows to label each vulnerability in the vulnerabilities dataset whether it has a positive impact on the volumes of scans or not.

4.2.1. Ratios of impacted scans by vulnerability publication
The comparison of sub-time series before and after the publication of vulnerabilities resulted in two sets of vulnerabilities: Vulnerabilities with impact on the scans rates and vulnerabilities without impact. In the following, we study the impact of the parameters we choose on the ratio of vulnerabilities having an impact on the scans. The parameters are the source of the dataset we use, the scans count method, and the window size .

4.2.1.1. Window parameter.
We remind that the window size defines the length of the sub-time series used in the impact detection phase. Table 7 shows that the vulnerabilities mean impact ratio changes when we change the window size (by days). Roughly 20% of vulnerabilities have a positive impact on scans.

4.2.1.2. Dataset and scans count method.
Table 8 presents the percentage of vulnerabilities found to have an impact on the scans. We observe that the impact ratio changes by changing the dataset or the scans count method and ranges from 10% and 27%.


Table 7. Impact ratio by window size.

Window	25	30	35	40	45	50	55	60
Impact ratio	21.18	20.17	16.91	16.75	16.95	17.41	18.21	18.84
We applied the impact detection algorithm, and it resulted in a dataset of vulnerabilities with their impact on scans time series. We found that the parameters we set, such as the window size, do not have a big impact on the ratios of the vulnerabilities found to have an impact. Whereas the count method and the dataset used for scans can change the ratio up to 17%. Before training a classification model on the dataset we created, we propose to apply some feature-selection algorithms to it.


Table 8. Impact ratio by dataset.

Data source	Count method	Impact ratio
INRIA	Attackers	27.18
Packets	16.65
NICT	Attackers	10.30
Packets	13.53
4.3. Selection of important vulnerability features
Our goal is to identify the important features to use to classify vulnerabilities with impact on scans. We extracted the important features resulting from the feature selection process. We used feature selection algorithms described previously to select the best features to use in training the models. Multiple datasets for training are created depending on the combination of parameters we chose. These parameters are the source of the scans dataset, the scans count method, and the window size parameter used in the impact detection algorithm. Fig. 14 is a heat map representation of the contribution percentage of important features. The percentages are the result of one of the feature selection models which is the decision tree.

It shows generally that the first 8 features are the most important ones with a high mean percentage of contribution in the model. The models with different types of scans count methods applied in different datasets give high importance roughly to the same set of features. It is to note that there is a difference between the important features of the two scan count methods, such as the CVSS-exploitabilityScore. The feature “Number of affected ports” has a high percentage of importance for different scan count methods and different datasets. That could be because mapping a vulnerability to a long list of ports means either the vulnerability is affecting multiple products or the mapping is not good enough that it links a vulnerability to that long list of ports. In either case, the vulnerability is not guaranteed to impact the scan rate on all ports mapped to it compared to a vulnerability that is mapped to only one port. By training the models, they learn that predicting a low impact for that type of vulnerability improves their score. The feature “CVSS-exploitabilityScore” comes second, but that is only because it has high percentage of contribution to the models trained on scans counted by the number of daily packets. This means that the feature selection algorithms detected that this feature is helpful to detect the impact only on time series of scans representing the number of packets rather than the number of scanners. An assumption that could be verified is that attackers send more packets when the vulnerability is easy to exploit, where complex vulnerabilities receive fewer packets from attackers. The high rate of packets by attackers can also be explained by the use of automation tools that focus on exploiting easier vulnerabilities. We also notice that the features we added to the dataset like the product score and the vendor score, daily discovered vulnerabilities, etc., are found to be important.

4.4. Prediction of vulnerability impact on scans
Not all vulnerabilities are expected to have a positive impact on the scan’s volume. The goal is to know, in advance, the vulnerabilities that are most likely to have an impact on scans.

The vulnerabilities have multiple features, like their score, access vector, etc. In addition to the result of the impact detection algorithm, we train multiple machine-learning classification models to be able to detect whether a vulnerability can have an impact based on its features. We also take advantage of feature selection, feature interaction, and other machine-learning techniques to improve the performance of the models.

4.4.1. Impact classification
We described in the previous sections how we create a dataset containing vulnerabilities. Each vulnerability is described by its features and the value of its impact as a Boolean. We train multiple classifiers to differentiate vulnerabilities that have an impact from the ones that do not.

4.4.1.1. Classification parameters.
We trained multiple models on the dataset. Table 9 shows the parameters we use for classifications. We created multiple datasets for each combination of the following parameters: window size, source darknet and scans count method. We then trained multiple classification models, with 5-fold cross-validation. In addition, we performed a grid-search on each model type and parameter to find the best parameters combination. We also used feature selection and feature interaction, and we performed an under-sampling to balance the data. For the execution environment used to train and test the models, we used a system with the following configuration:

•
Processor: Intel(R) Core(TM) i9-7980XE CPU @ 2.60 GHz up to 4.20 GHz , 18 Cores (36 Threads)

•
Operation system: Linux debian 4.9.0-7-amd64

•
Random Access Memory (RAM): DDR4 128 GB

We train all the models for each of the 7 windows sizes, for each of the two datasets and for each of the two scans count methods, which makes 28 (7*2*2) iterations to train and test each model. The execution time for training one model depends on the combinations of parameters used for the grid-search. Usually, training and testing with grid-search parameters for all iterations take 2 to 3 h, bearing in mind that the models are run in parallel.

It is hard to set a window for each vulnerability where it can have an impact on network scans because that window can range from several days to several months. The best we can do for the moment is to train the models with different windows and select the window size that was used by the classifier giving the best results.

4.4.1.2. Classification results.
The classification results of both datasets and the window size fixed to 45 days are presented in Table 10. We found that the Random Forest scores well in all metrics except for recall where it is less than the decision tree by 0.10%. Nevertheless, The Decision Tree is very close to Random Forest in performance, while Naive Bayes and Support Vector Machine (SVM) fail to achieve good scores. A plausible explanation of this could be due to the nature of the features which are mostly categorical.


Table 9. Classification parameters.

Parameter	Values
Window size (days)	25, 30, 35 ,40, 45, 50, 55 and 60
Scans datasets	INRIA and NICT
Scans count methods	By attackers and by packets
Models	Random Forest, SVM, Logistic regression, Naive Bayes, and Decision Tree
Cross-validation	5 folds

Table 10. Classification results by model.

Classification model	Accuracy	Recall	F1 Score	Precision	ROC AUC
Decision tree	81.58	86.54	82.45	78.73	87.39
Logistic regression	73.01	72.63	72.90	73.19	79.33
Naive bayes	58.58	57.94	58.29	58.67	66.19
Random Forest	82.68	86.44	83.31	80.39	89.14
SVM	57.09	53.98	55.71	57.56	52.77
4.4.2. Effect of parameters on classification results
We study the effect of changing the experiment’s parameters on the results. Three parameters to study are the sub-time series window size , the source dataset and the darknet time series scans count method. We also studied the effect of feature selection and feature interaction on improving the performance of the classification models. It has achieved a very small to no improvement in the model’s performance. As a result, we selected models using a small number of features for fast training and predictions.

4.4.2.1. Window size.
From Table 11, we see that the models trained with a window size of 30 have achieved the best results compared to the others.

4.4.2.2. Scans dataset and scans count method .
Table 12 shows the classification results grouped by dataset and darknet scans count method. It shows that the best results are achieved by models trained on the INRIA dataset with time series representing the number of the attackers. The model that achieved that score is the random forest. It achieved the best results in all classification metrics.


Table 11. Effect of window parameter.

Window	Accuracy	Recall	F1 Score	Precision	ROC AUC	Model
25	84.38	84.75	84.43	84.13	91.56	Random Forest
30	85.18	85.74	85.26	84.80	92.42	Random Forest
35	83.55	86.65	84.04	81.60	90.55	Random Forest
40	83.72	86.94	84.23	81.69	90.30	Random Forest
45	82.56	86.60	83.24	80.13	89.33	Random Forest
50	82.53	87.06	83.28	79.83	89.01	Random Forest
55	83.10	87.23	83.77	80.59	90.15	Random Forest
60	82.60	87.57	83.42	79.65	89.93	Random Forest
The results of the classification show that we can classify vulnerabilities impact with good metrics. The random forest algorithms showed higher performance compared to other algorithms such as Naive Bayes and SVM. As for the parameter of the model, we observed that the window parameter  did not have a huge impact on the performance of the classifier, while the performance of the model decreased when we used “packets” of the count method parameter.


Table 12. Effect of the dataset and scans count method.

Data source	Count method	Accuracy	Recall	F1 Score	Precision	ROC AUC	Model
INRIA	Attacker	85.18	85.74	85.26	84.80	92.42	Random Forest
Packets	76.43	79.51	77.13	74.91	83.65	Random Forest
NICT	Attacker	84.14	86.17	84.45	82.81	91.04	Random Forest
Packets	75.70	77.78	76.19	74.68	83.70	Random Forest
5. Discussion
Our implementation of the impact detection algorithm of vulnerability disclosure events on scans’ time series shows that the ratio of vulnerabilities having an impact on the scan rates is around 20%. The results mean that not all vulnerability disclosures have an impact on scan rates, but only 20% of them were followed by an increase in the scan rate. That led us to believe in the existence of an impact of vulnerability disclosure on scan rates because not all vulnerabilities have the same features, and their impact on scan rates can be related to the individual features of each vulnerability.

The next step was the use of machine learning-based models to classify vulnerabilities related to an increase in scans’ traffic from those which are not. The results of the models show high performance. We could say that even if only 20% of vulnerabilities are related to an increase in scans’ traffic, the trained models could detect them. We can conclude that they have some common features that make them impact scans. Fig. 14 used for feature selection shows that the common features of vulnerabilities that have an impact on scans’ rates are generally related to the product and the vendor, the vulnerability exploitability and the vulnerability confidentiality impact. The importance of these features sounds reasonable, taking into consideration the goal of scanners which is to find a vulnerability that is easily exploitable and enables them to impact the confidentiality of the target by doing malicious acts such as breaching data. The affected vendors or products are also of major importance, as they are widespread and trusted by customers to manage their data and infrastructure.

To implement our model in an organization architecture that does not have access to telescope network data, the traffic received by the organization should be preprocessed and classified to filter it and select only scans, then be passed to the model. The dataset we used is collected from real network traffic that targets public IP addresses. If a scanner is scanning the entire Internet, the probability that it scans hosts of an organization is roughly equal to the probability of scanning an IP address of a telescope network.

It is true that not all vulnerabilities’ researchers follow the procedure of responsible disclosure and never disclose technical details before the enterprises release the patch and security update. However, we first believe that the number of these vulnerabilities is very low compared to the number of daily disclosed vulnerabilities. Second, we propose in our model another scans’ count method in which we count the number of scanners that are scanning the network rather than the number of packets sent to the network. That will be helpful to catch the increase in the number of scanners, which is an indication of the prevalence of the vulnerability. Finally, we propose for future work to use social platforms such as Twitter and others to catch the non-official disclosures of vulnerabilities (Sabottke et al., 2015).

Our work is exposed to some limitations. First, this work was performed on a network telescope, which is not probably the best data source if we try to apply the methodology to attacks targeting a real network of a particular enterprise. However, network telescopes are better suited for examining malicious traffic because we skip the stage of filtering malicious traffic from the legitimate one. Second, multiple vulnerabilities can be disclosed on the same day, and that makes it hard for the model to learn the vulnerability that increased the scan’s rate. Furthermore, the disclosure of a vulnerability may occur concurrently with another event of a different nature, such as a social or political ones. Finally, the disclosure of a vulnerability may not generate a stream of scans, because attackers nowadays can use vulnerabilities and open port search engines, such as Censys (Durumeric et al., 2015) or Shodan (2020) in order to obtain the results of a scan without being caught by telescope networks. However, not all attackers rely on vulnerability search engines, either because their results are incomplete, outdated or because attackers have to perform very specific port scans. In addition, spiders of vulnerability search engines will start showing interest in ports related to products with severe vulnerability as a response to users’ requests for those ports.

Other limitations about the method we used include mapping vulnerabilities with ports. It is hard to assess the robustness of the mapping because we do not have a ground truth data to check against. Instead, we relied on our expertise to manually check the results of the mapping, and some examples of the results are shown in Table 6. In addition, the test statistic of the means of sub-time series done in Eq. (3) is sensible for some time series characteristics, such as a trend.

Scans and brute-force attacks always occur, and they are triggered by various factors, including technical and social events such as problems between countries and organizations, as well as some other random factors that cannot be assessed. The proposed model does only studied the impact of a vulnerability publication on the “increase” of scans without claiming that all scans are originated or triggered by vulnerability publications. In addition, we study the increase over a time window of at least 25 days in order to avoid any random event making an increase in one or two days. We also believe that the impact of a vulnerability will generally last more than a week.

The ubiquity of attacks in the cyber world incites constant development and improvement of detection and prevention tools. The development of this model has theoretical and practical implications.

The proposed model aims at using exogenous data to help improve cybersecurity techniques, especially attack detection and prediction. Attacks are of different types and forms, which require different methods and data to deal with each type. Our proposed framework intends to extract information from different sources that can have an impact on an increase in attacks. That can help the community to focus and develop new methods to use different sources of data related to attacks instead of relying only on one data source. It is an important step toward studying and using software vulnerability-related information as a tool to predict attacks’ evolution over time.

The results of our work show that our framework can help detect network scans’ evolution over time. The framework described in this paper can be used in broad types of attack detection and prevention tools. Technically, this model can be implemented and used in intrusion detection systems (IDS) or intrusion prevention systems(IPS). Depending on the chosen IDS/IPS design, it is possible to add the described model as part of the detection engine of an IDS/IPS, or it can be added as a separate layer of the detection process. Future work is still needed to check the reliability of the proposed system in other environments to make it more efficient. For example by using multiple data sources, such as data about vulnerability disclosure in the dark web and the existence of exploits for new vulnerabilities, as well as testing other new and robust machine learning models.

6. Conclusion
In this work, we presented a machine learning approach to creating models that classify vulnerabilities whether they will have an impact on network scans. We collected data about software vulnerability from NVD, and added data about ports to map vulnerabilities with ports that will be likely affected, while data about network scans are collected from two telescope networks. The proposed approach is used to extract information about the impact on the volumes of network scans after the publication of each vulnerability. The impact is detected by comparing the distribution of values of the time series of scans before and after the disclosure of each vulnerability. A classifier is then trained on that knowledge data to be used for predicting the impact of newly published vulnerabilities. We found that roughly 20% of vulnerabilities related to products exposing services over the network have an impact on the scan’s volume on the darknet. This ratio changes slightly if we change some parameters, such as the size of the window used for the impact detection algorithm. However, the representation of network scans by the number of scanners made significant changes in the results. We trained multiple classifiers and found that Random Forest gives the best results with 89.14% in AUC and 83.31% in F1 Score. In addition, we extracted the most important features in the classifier and found that CVSS exploitation metrics, the popularity of the vendor and product are the main features used to classify vulnerabilities with likely impact. The presented framework is a contribution to improving cybersecurity techniques for attack detection and prevention by leveraging the power of exogenous data. It can be implemented as a layer in security equipments to enhance its detection performance. Our work is subject to some limitations, such as the size of the telescope network and the use of vulnerability search engines by attackers. These limitations can be addressed in future research by using other exogenous datasets, such as data from social platforms, or using larger telescope networks.

