Visual Question Answering (VQA) is a new and popular research direction. Dealing with language prior problems has become a hot topic in VQA in the past two years. With the development of technologies relating to VQA, related scholars have realized that in VQA tasks, the generation of answers relies too much on language priors and considers less with visual content. Some of the previous methods to alleviate language priors only focus on processing the question, while the methods to increase visual acuity only concentrate on finding the correct region. To better overcome the language prior problem of VQA, we propose a method that will improve visual content further to enhance the impact of visual content on answers. Our method consists of three parts: the base model branch, the question-only model branch, and the visual model branch. Many experiments have been carried out on the three datasets VQA-CP v1, VQA-CP v2, and VQA v2, which proves the effectiveness of our method and further improves the accuracy of the different models. Our code is available in GitHub (https://github.com/shonnon-zxs/AddingVisualModule).

Introduction
With the successful application of deep learning in natural language processing and image processing, multimodal fusion VQA tasks have developed rapidly. At present, the methods of VQA can be divided into base modal fusion methods [1,2,3,4,5], attention model methods [6,7,8,9], based on external knowledge model methods [10,11,12], relational reasoning model methods [13,14,15], etc. Most of the methods have achieved high accuracy on the VQA v2 [16] dataset. However, many articles [17,18,19,20,21] pointed out that these models had strong language prior; that is, the generation of answers relies more on the relationship to the question and less on the image. For example, for the question â€œWhat color is the sky?â€. Since most answers are â€œblueâ€, even if the sky in the image is â€œgrayâ€, the answer is more likely to be predicted as â€œblueâ€ than â€œgrayâ€.

The earlier method of dealing with language priors is to enhance visual sensitivity. For example, Hint [22] and SCR [23] introduced additional annotations/cues such as human attention maps. They proposed punishing the sensitivity of wrong answers to essential regions to improve the modelâ€™s judgment on critical regions. Shrestha et al. [24] proved that these methods of enhancing visual acuities did not enhance the visual basis but used a regularization effect to prevent over-fitting to the prior language.

Recent research methods have proposed fusing the base model and the question-only model to deal with the language prior problems in modal fusion. For example, LMH [25] proposed to combine the predicted answer of the question-only model with the predicted answer of the base model as the final predicted answer distribution. Although these methods have significantly improved the modelâ€™s accuracy, they only focus on reprocessing the question rather than the image.

Fig. 1
figure 1
Adding the visual branch can help us further find the correct answer

Full size image
Different modalities should have different degrees of effect on the answer prediction [26]. To further balance the relationship between the image and the language and improve the robustness of the model, we propose a method that will increase the visual grounding and enhance the impact of the visual grounding in answering questions. As shown in Fig. 1, the result of the model is improved by adding visual branches. Our contributions are as follows:

1.
We propose adding a visual branch, which is used to balance the relationship between images and questions, and our method is implemented in different models.

2.
We improve two regularized loss functions and analyze their impact.

3.
Our method is versatile and improves the accuracy of most models on the VQA-CP v2 [27], VQA-CP v1 [27], and VQA v2 [16] datasets, especially the accuracy of RUBi on VQA v2 by 9.35%.

Related work
Language prior problem in VQA
Recent research [28,29,30,31,32,33,34,35,36,37] pointed out that VQA had language prior problems and proposed different processing methods. The language priors in VQA mean that in the multimodal fusion task of vision and language, the answer generation mainly depends on the language rather than the image. For example, for the question type â€œhow many...â€, answer â€œ2â€ accounts for 31% in VQA v1 [38] and 27% in VQA v2, which is much larger than other answer distributions. Therefore, the predicted answer is more inclined to answer â€œ2â€ directly, not based on the imageâ€™s content. Guo [31] proposed a metric to analyze this prior situation of language quantitatively.

Debiasing VQA
At present, the primary methods to alleviate the language prior are mainly divided into two categories: one is to deal with question-only models to reduce language bias, and the other is to enhance visual sensibility. Some researchers [32, 33] proposed methods of expanding counterfactual samples and achieved good results. These data expansion methods do not conflict with the basic methods of reducing language priors, such as the method of generating counterfactual samples proposed by CSS [33] is based on the LMH [25] model that reduces language priors.

There are many ways to increase the question-only model to reduce language priors, through adversarial [34, 35], fusion-based [36], ensemble-based [25]. Other different strategies are to merge with the basic VQA model. The adversarial method defaults that only the prediction of the question-only model is wrong, but this part is not always bad. The RUBi model [36] that adopts the fusion-based method also joins the question model and dynamically adjusts the loss to compensate for the influence of this bias. Ensemble-based methodâ€“LMH proposes that the question-only model uses 65 types of question classification labels as input and is trained on the same multi-label target as the primary model. The new strategy combines the results of the basic model and the question-only model as the predicted answer and compares the ground-truth answer. The performance of these methods is better than the previous models that enhance visual sensitivity, but these models only focus on the processing of questions while reducing the language prior; they do not further enhance the importance of vision in the task.

Enhanced vision can balance the relationship between image and question. The SCR model [23] focuses on essential regions of the image based on HINT [22], proposes a self-criticism method, penalizes the sensitivity of incorrect answers to essential regions, and further improves the modelâ€™s performance. In addition, some recently proposed ways to increase negative samples [32, 33, 37, 41] also increase the visual sample to some extent. Although these methods can help the image find the key regions, they are not enough to increase the weight of the overall vision in predicting the answer. In other words, even if the correct key region is found, the answer is still based on text content.

For counterfactual methods, generating counterfactual samples is proposed, which improves the modelâ€™s visual interpretation ability and sensitivity to question simultaneously. Niu et al. [26] start from the perspective of causal reasoning and point out that different modalities have different effects on the answer. Hence, the relationship between vision and language needs to be further balanced. Inspired by this, we proposed a method to increase the branch of the visual model to improve the generalization ability of the model.

Fig. 2
figure 2
The overall model architecture of the proposed adding visual branches for visual question answering. The model consists of three branches: a base model, a question model, and a visual model. Finally, the prediction results of the three models are combined by embedding. ğ‘„âˆ— represents the first k words(ğ‘˜=1 in this figure) of the question, ğ‘‰âˆ— represents the combination of visual features V and ğ‘„âˆ—. The dotted line indicates that the question model does not backpropagate in some methods such as LMH

Full size image
Method
In this section, we will introduce our method implementation. Our method consists of three parts: (1) base model, (2) question-only model module, which is used to deal with language priors, (3) the visual model module is used to balance the visual weight further.

As shown in Fig. 2, the general VQA method combines language with image content to predict the answer. The specific fusion method is the key to different base model methods. The models that deal with the language priors of VQA are all processed by the basic model plus the question-only model branch, and the combination of the question-only model and the basic model is the key to processing language prior. Our method is to add a visual component based on the fusion of the basic model and the question-only model, further balance the relationship between language and vision, and enhance the role of the visual foundation in the VQA task.

Baseline VQA model
VQA tasks are generally considered multi-classification problems. Given a set consisting of N triplets of images ğ¼ğ‘–âˆˆğ¼, question ğ‘„ğ‘–âˆˆğ‘„ and answer ğ‘ğ‘–âˆˆğ´, we denote as ğ·={ğ¼ğ‘–,ğ‘„ğ‘–,ğ‘ğ‘–}. The task aims to learn a mapping function ğ‘“ğ‘£ğ‘ğ‘:ğ¼Ã—ğ‘„â†’[0,1]|ğ´|, producing an answer distribution of the given image and question. In the following sections, we will omit the subscript i for simplicity.

Unless otherwise stated, the methods mentioned in this article are based on UpDn [39] as the baseline model. For a given image V and question Q, a top-down attention mechanism is used to obtain a predicted answer P:

ğ‘ƒ=ğ‘(ğ´|ğ‘‰,ğ‘„)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘“ğ‘£ğ‘ğ‘(ğ‘‰,ğ‘„))
(1)
where Q is the question feature encoded with GRU [40], and V is the visual feature with attention.

All models use bilinear cross-entropy as the loss function without considering other regularization:

ğ‘™ğ‘œğ‘ ğ‘ ğ‘ğ‘ğ‘’=âˆ’1ğ‘âˆ‘ğ‘–=1ğ‘[ğ‘log(ğ›¿(ğ‘ƒ))+(1âˆ’ğ‘)log(1âˆ’ğ›¿(ğ‘ƒ))]
(2)
where a is the ground truth, and ğ›¿(â‹…) is the activation function.

Question-only model
The question-only model processing can reduce the language prior [42]. Different methods deal with the question-only results differently, and the fusion results are also different.

For example, RUBi and LMH, respectively, use the following fusion strategies:

â„(ğ‘„,ğ‘ƒ)=ğ‘ƒâŠ™ğ›¿(ğ‘(ğ´|ğ‘„))
(3)
â„(ğ‘„,ğ‘ƒ)=logğ›¿(ğ‘ƒ)+ğ‘”(ğ‘¥)logğ›¿(ğ‘(ğ´|ğ‘„))
(4)
where g(x) is the learnable function, and x is the last layer of the product of V and Q. The predicted answer h(Q, P) of the basic model and the question-only model is obtained through the above method.

Visual model
We get the prediction result h(Q, P) of the fusion of the question-only model and the basic model in Sect. 3.2. This section, h(Q, V, P), is obtained by adding a visual module. We first introduce the method of constructing the branch of the visual model and then present the method of embedding the result of the visual component and the removal of the language prior.

Visual model construction
On the one hand, the image is not as a prior as the question, and we cannot rely on the image alone to predict the answer. On the other hand, each image in the dataset has multiple different questions corresponding to it. If only images are used, this will lead to no difference in the prediction results of the visual model when predicting different questions of the same image. Based on the above two reasons, we use the combination of image and question to construct the visual branch. In order to avoid the influence of the language prior, the question only uses part of the first k words that do not contain keywords which are the object noun in the questions in VQA datasets. For example, to answer this question â€œwhat color is a banana?â€, we construct the visual branch by combining the image and the first 2 words â€œwhat colorâ€(i.e., ğ‘˜=2 in this example). This prevents the visual branch from answering the question based on the term â€œbananaâ€.

Table 1 Question-type length statistics
Full size table
For the selection of the first k words, there are two specific methods: First, only 65 question types are used in combination with images, and the word length of each question type is different. Second, we use the fixed first k words combined with the image to constructing the visual branch. In Table 1, we have counted the results of 65 question types, and the average length of all question types is about 2.6 words.

Visual branch fusion method
We use an ensemble-based approach to increase the visual branch. Specifically, we add the predicted answer of the visual model to the predicted answer of the base model and the question-only model instead of separately comparing the visual branch with the ground truth and adjusting it through the loss function. The following, respectively, introduce our methods of adding visual content to models such as UpDn, RUBi, SSL [32], LMH, and CSS.

UpDn+V The method of UpDn to increase the visual model is to directly add the predicted answer of the visual feature to the predicted answer of the basic model, and there is no question model module. The way to increase is:

â„(ğ‘„,ğ‘‰,ğ‘ƒ)=ğ‘ƒ+ğ‘(ğ´|ğ‘‰âˆ—)
(5)
where ğ‘‰âˆ— represents the combination of visual features V and the first k words.

RUBi+V For RUBi and LMH models, the question-only model is obtained in different ways. The question-only model of LMH only obtains the predicted answer according to the question types in 65, while RUBi obtains the predicted answer through the entire question. The method of adding a visual model based on RUBi:

â„(ğ‘„,ğ‘‰,ğ‘ƒ)=ğ‘ƒâŠ™ğ›¿(ğ‘(ğ´|ğ‘„))+ğ›¿(ğ‘(ğ´|ğ‘‰âˆ—))
(6)
SSL+V Because SSL obtains the predicted answer of the basic model through UpDn, in order to improve the performance of the model, we use a method similar to RUBi to increase the visual model and the question-only model at the same time, and the method to increase the visual model based on SSL:

â„(ğ‘„,ğ‘‰,ğ‘ƒ)=(ğ‘ƒ+ğ‘(ğ´|ğ‘‰âˆ—))âŠ™ğ›¿(ğ‘(ğ´|ğ‘„)
(7)
LMH (CSS) + V Since CSS is an improvement made by LMH, we have the same method of adding visual models to LMH and CSS, as follows:

â„(ğ‘„,ğ‘‰,ğ‘ƒ)=(2âˆ’ğœ†ğ‘)logğ›¿(ğ‘ƒ)+ğœ†ğ‘ğ‘”(ğ‘¥)logğ›¿(ğ‘(ğ´|ğ‘„)+ğœ†ğ‘£logğ›¿(ğ‘(ğ´|ğ‘‰âˆ—))
(8)
Only through the model it may not be able to learn better prediction results; for the weight of vision question in the model needs to set hyperparameters. Therefore, to balance the role of images in the final predicted answer, we use dynamic g(x), and some fixed constant c for the visual branch as the weight ğœ†ğ‘£, where g(x) is the implementation is the same as formula 4.

Experiment
Experimental setup
Dataset To alleviate the imbalance between language and vision, Goyal et al. [16] proposed a new dataset VQA v2 based on VQA v1, supplemented with pictures for each question. Agrawal et al. [27] further presented a dataset VQA-CP v2 specifically for processing language priors. Re-splitting VQA v2 obtains VQA-CP v2. The distribution of answers for each question type in the test set is as different as possible from its distribution in the training set. Similarly, re-splitting the VQA v1 obtains VQA-CP v1 dataset. In addition, neither VQA-CP v1 nor VQA-CP v2 has a validation set, only a training set, and a test set. To show the effectiveness and completeness of our experimental method, we have done main experiments on VQA-CP v2 and VQA v2 and supplemented the experimental results on the VQA-CP v1 dataset.

Setting We use UpDn [39], RUBi [36], SSL [32], LMH [25], and CSS [33] as comparison models and show the results of these models after adding visual branches. Among them, models such as UpDn, RUBi, LMH, and CSS are implemented using 30 epochs, and the SSL model is implemented using 40 epochs. Other feature extraction methods and data preprocessing, such as some hyperparameter settings, are the same as the original paper. We train on NVIDIA 2080ti(11G).

Experimental results
Performance on VQA-CP v2 and VQA v2
We add visual content to some previous methods of dealing with language priors and compare them with the results of some previous advanced models on VQA-CP v2 and VQA v2. These models are UpDn, RUBi, CSS, SSL, etc., among which SSL is only compared on the VQA-CP v1 and VQA-CP v2 datasets.

Table 2 The best results (%) of different models on the VQA-CP v2 test set and VQA v2 validation set after adjusting with the visual branch
Full size table
It can be seen from Table 2 that, in general, by adding visual model modules, it is possible to improve the accuracy of the VQA v2 dataset without reducing the ability of processing language prior on VQA-CP v2. The result of the RUBi model on VQA v2 has increased the accuracy by 9.35%, and on the VQA-CP v2 dataset, the CSS model increased by 1.01%, reaching the best result among the compared models. Moreover, on the VQA v2 dataset, the accuracy of all models has been dramatically improved, further reducing the gap with UpDn.

Performance on VQA-CP v1
For the completeness of the experiment, we further experiment with the model on VQA-CP v1. The comparison is still based on UpDn, RUBi, LMH, CSS, and SSL. The experimental results in Table 3 show that adding visual content improves the accuracy of all the above models. Among them, RUBi increased by 3.91%. The CSS increased by 0.46% to reach the highest accuracy of 61.37% among all comparison models.

Fig. 3
figure 3
a The results of the different number of words and question model weights when the visual weight is 1. b Different word counts and visual weights when the question-only model weight is 0.7. The above results are the results of the CSS model on the VQA-CP v2 dataset. ğ¿ğ‘ğ‘¡ğ‘¦ğ‘ğ‘’ represents the length of the question type

Full size image
Table 3 Accuracy(%) of different models on the VQA-CP v1 test set; all results are based on our re-implementations
Full size table
Ablation experiment
Setting of the number of words As mentioned in Sect. 3.3.1, we compared experiments result while only images, where the number of words is constant k (where ğ‘˜={1,2,3,4}) and only the question type is combined, respectively, with visual content. Note that when ğ‘˜=0, it means that only the image is used. In the experiments, the questionsâ€™ length is changed from 14 to k for the first k words, while the questionsâ€™ length is still padded to 14 for the question type.

Weight of the question-only model and the visual model Although the model can still learn, adding the question-only model can find the optimal value as much as possible. However, the model cannot fundamentally adjust the weight between vision and question. In formula 5, we increase the weight of the image and question to balance the relationship between those two further.

Results The number of different words and the weight setting results of the question-only model and the visual model is referred to in Fig. 3. Figure 3a shows the effects of different word number and question-only model weights when the visual weight is 1. It shows that when the weight of the question branch is 0.7, the model result is the best. Figure 3b shows the effects of different word counts and visual weights when the question-only model weight is 0.7. On the whole, it is best when the visual model parameter is 1. And experiments show that the constant c determined for the hyperparameter is better than the dynamic g(x).

Fig. 4
figure 4
Qualitative comparison between the outputs of CSS and CSS+V(Ours) on VQA-CP v2 test set. The green/red bounding boxes indicate the most important regions resulting from Ours/CSS. These examples are from the VQA-CP v2 dataset (Color figure online)

Full size image
Table 4 Ablation study
Full size table
In addition, to prove the effectiveness of increasing the visual model, in Fig. 4, we quantitatively compare some examples of CSS models with visual branches to the CSS model. The ablation experiments in Table 4 show that for the LMH and CSS models, if the visual branch is not added, only reducing the weight of the language basis can also improve the modelâ€™s accuracy. However, the accuracy of the model can still be further enhanced when the visual content is added. For the SSL model, after adding visual content, its accuracy has also been better improved.

Supplementary experiment
Shrestha et al. [24] pointed out that some effects come from regularization. We used the contrast loss function [28] and the semantic spatial relationship loss function [43] for reference and added two regularization methods. Based on the CSS model, we further studied the effect of regularization on the experimental results.

Comparison of loss functions
A self-supervised comparative learning mechanism of counterfactual samples is introduced [28]. Based on this, we use this loss function to conduct experiments on VQA-CP v2, narrow the relationship between the final prediction and the answer label, and alienate the relationship between the predicted answer of the question model and the answer label. Similarly, to express the relationship between the original sample and the original sample and the answer label, we use the cosine similarity function as the similarity function:

ğ‘ (ğ‘,ğ‘)=ğ‘ğ‘‡â‹…ğ‘ƒâ€–ğ‘â€–â‹…â€–ğ‘ƒâ€–
(9)
where s(a, P) is the similarity score between the ground truth and the predicted answer. Similarly, the similarity score between the answer label and the question-only predicted answer is s(a, b), where b is the question-only predicted answer. Then, the comparison learning loss is:

ğ‘™ğ‘œğ‘ ğ‘ ğ‘™ğ‘=âˆ’1ğ‘âˆ‘ğ‘–=1|ğ´|log(ğ‘’ğ‘ (ğ‘,ğ‘ƒ)ğ‘’ğ‘ (ğ‘,ğ‘ƒ)+ğ‘’ğ‘ (ğ‘,ğ‘))
(10)
For each tripartite sample, the distance between the original sample and the answer can be narrowed. Still, the distance between the predicted answer and the answer can be pushed farther.

Table 5 Comparison of the results of the regularized loss function
Full size table
Cosine similarity loss function
Based on the method of narrowing the relationship between the answer semantics [43], we use the following cosine similarity function as the loss function.

ğ‘™ğ‘œğ‘ ğ‘ ğ‘ğ‘œğ‘ =1âˆ’ğ‘ğ‘‡â‹…ğ‘ƒâ€–ğ‘â€–â‹…â€–ğ‘ƒâ€–
(11)
In order to express ğ‘™ğ‘œğ‘ ğ‘ ğ‘™ğ‘ and ğ‘™ğ‘œğ‘ ğ‘ ğ‘ğ‘œğ‘  uniformly, the above two regularization losses are collectively called the regularization function ğ‘™ğ‘œğ‘ ğ‘ ğ‘§ğ‘§. The total loss function is expressed as:

ğ‘™ğ‘œğ‘ ğ‘ =ğ‘™ğ‘œğ‘ ğ‘ ğ‘ğ‘ğ‘’+ğ‘™ğ‘œğ‘ ğ‘ ğ‘§ğ‘§+ğ‘…
(12)
where R is the loss function proposed in LMH:

ğ‘…=ğ‘¤ğ»(ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘”(ğ‘¥ğ‘–)log(ğ‘ğ‘–)))
(13)
The effect of regularization
The experimental results in Table 5 show that a suitable regularization method can appropriately improve the modelâ€™s accuracy, but the improved ability is limited. In the following experiment, when adding the visual branch, the parameters are selected as the number of words is 1, and the question weight is 0.7.

Conclusion
In this paper, we propose a method to increase visual importance by adding visual branches, creatively designed a visual branch, and applied it to VQA tasks to deal with language priors. Many experiments conducted on three data sets, VQA-CP v1, VQA-CP v2, and VQA v2, show that our method can be integrated into multiple previous models and improve the modelâ€™s accuracy. Among them, we achieved the highest accuracy of 59.48% on VQA-CP v2. Experimental results show the feasibility of adding visual branches based on question-only model and provide new ideas for dealing with language prior questions. Our method can be extended to other multimodal fusion tasks, fusing different modal branches separately [44,45,46], whereas we use a more straightforward way to increase the visual branch in this paper. In the future, we will explore the relationship between images and questions, and incorporate this relationship network graph into visual features, or fuse the relationship network as a separate modality with other modalities.

Keywords
Visual Question Answering
Language prior
Visual branches
Visual importance