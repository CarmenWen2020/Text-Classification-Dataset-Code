We develop a deep learning architecture for inverse design of a self-oscillating sheet propelled by an embedded chemical reaction. The dynamics of our problems are nonlinear and exhibit chaotic behavior, a challenging setting for existing deep-learning-based inverse design approaches. The aim is to explore data-driven design of soft robots using a novel locomotion mechanism. We train the architecture using a forward model of the locomotion mechanism developed recently by Alben et al. (J Comput Phys 399:108952, 2019). The architecture is shown to successfully map a snapshot of target motions of the gel into geometric and reaction parameters. The final architecture consists of a multi-layer perceptron (MLP) classifier for discrete parameters, followed by a stacked MLP regressor (SMLPR) for continuous parameters. Our inverse design setting is unique in that it considers both discrete and continuous outputs, requiring an architecture capable of classification and regression. We are able to recover parameters within 2.87% accuracy. We also compare the simulated motion of the sheets at the recovered parameters. Because the motion has a chaotic quality, our demonstration is able to show quantitative agreement for a small time horizon and qualitative agreement over longer time horizons. We also demonstrate agreement of Lyapunov exponents up to 6.78% accuracy for suitable motions.

Introduction
Conventional robots with rigid bodies can be designed and programmed for accomplishing specific tasks efficiently [1, 2]. However, this efficiency comes with a price in adaptability. On the other hand, soft robots that are inspired by structures existing in nature offer flexibility to different tasks [3] and promise increased collaboration opportunities between man and machine [4]. As attention in this subfield of robotics increases, successful adoption of these approaches becomes more frequent across areas including surgical tools [5], marine exploration [6], wearables [7, 8] and assistive devices [9].

One of the main challenges to deploying soft robots in real-life conditions is providing energy or signal to actuators to initiate or sustain locomotion. A variety of actuation possibilities have emerged in recent years: Mosadegh et al. [10] uses pressurized air flow through special channels within the robot to actuate finger-like soft structures, Calisti et al. [11] uses various cables to mimic the pushing-based locomotion and object grasping of octopi, Hoang et al. [12] uses hydraulic pressure to create a helical gripper that mimics snakes and elephant trunks and [13] uses dielectric elastomer actuators to mimic the crawling motion of inchworms. However, all of those options share a common problem: they either need to be connected to external power/signal sources or are required to have onboard power supply. Furthermore, the control circuitry and actuators determine a lower limit on the size of the robots, which may pose a problem for operations in confined spaces.

A variety of propulsion methodologies have been proposed for smaller scale robots without those limitations: Tottori et al. [14] proposes propulsion through altering the surrounding magnetic field to micromachines, Qiu et al. [15] utilizes acoustic waves to actuate an untethered device and control its movement, and Zeng et al. [16] uses structured light to trigger liquid crystal polymers to motion. In this paper, we consider an alternative approach based on the motion of chemical-reaction driven sheets of gels.

The idea of inducing motion of gels using oscillatory chemical reactions was introduced to soft robotics by Yoshida et al. [17]. Yoshida and collaborators have since shown various transduction schemes; a C-shaped gel that propels itself across a toothed surface as the reaction in the active gel causes it to expand and contract [18]; artificial cilia [19, 20]; surface waves that induce rolling and translation of a cylinder [21], and transport of a bubble within a tube [22]. The oscillatory motion resulting from the chemical reactions was investigated and modeled in [23]. On the other hand, instead of having the polymer gels react to their environment autonomously, Dayal et al. [24] used simulations to investigate using light to guide the response of self-oscillating gels. That non-invasive method of triggering the BZ reaction was promising, since it allowed the reaction (and therefore the resultant motion) within the soft material to be controlled in a purpose-driven way. Being able to control the motion of gels without a physical actuator will contribute greatly to completely untethered, self-sufficient robots for the soft robotics field.

By extending previous applications on the luminously triggered gels to sheet form, Levin et al. [25] demonstrates how useful light can be in this context by creating and modifying the pattern of the BZ reactions and therefore implying the controllability of this process. Furthermore, in the same work, a non-Euclidean elasticity approach was presented and verified with experiments and simulations. In [26], the authors used a semi-implicit method to simulate self-oscillating gel, and the results of that paper form the basis of the present work.

Though the behavior of these self-oscillating gels can be accurately and efficiently simulated for a fixed set of parameters, a robust approach to find parameters that induces a target motion does not exist. Since these parameters are typically continuous, the motion of the sheet is sensitive to the choice of parameters, and the motion of the sheet is nonlinear and exhibits chaos; brute force tuning of parameters may not yield precise results.

This paper provides a systematic case study for the development of an inverse design tool that maps sheet motion to sheet parameters. Inverse design is an in silico technique adopted by different engineering disciplines that aims to accelerate the search process in multidimensional continuous spaces. Siakavara [27] uses an MLP-based inverse design architecture in design of microstrip antennas, Chen and Gu [28] uses inverse design with generative deep neural networks to discover and design novel materials, Sekar et al. [29] uses convolutional neural networks to design airfoils, and Li et al. [30] uses inverse design to determine the sample structure information from measured spectrum.

Machine learning-based inverse design is also used in the soft robotics world; Massari et al. [31] uses an FEM-based machine learning architecture to obtain the inverse sensor function. We adopt a similar method to [32] for constructing the training and validation data in this work: instead of collecting real-life data of oscillating gels, we use the simulation of system proposed in [26] to create our data set, which makes our inverse design architecture a simulation-based inverse design.

The contribution of this paper is a deep learning architecture that is able to effectively perform inverse design for determining geometric parameters and parameters of the chemical reaction to induce a target motion of the self-oscillating sheet. We step through a comprehensive set of hyper-parameter tuning activities and we finally demonstrate verification simulations. For at least 50% of the feasible domain of parameters, we are able to predict parameter settings for motions that are not in the test set within 2% with this architecture.

Comparison with existing approaches
In this section we compare and contrast other inverse design approaches with our proposed architecture and application. Inverse design is an emerging tool in the soft robotics world with very few examples and to our knowledge, this work is the first work bridging deep learning powered inverse design methods with autonomously actuated soft robotics. Inverse design has predominately been used in materials and crystals research, which exhibit different problem structures and can exhibit different types of complexity. Below we review several inverse design approaches, where we see a common theme that our application and approach is distinguished by the fact that (1) we consider a nonlinear, dynamical rather than a static, system; (2) we consider an architecture that does both regression and classification; and (3) we provide an extensive hyperparameter tuning procedure that also gives an insight into architecture robustness.

In [33], the authors investigated inverse design in context of nanoparticle light scattering spectra. Similar to our approach, they create training set through exhaustive numerical simulations on a sparse subset of the simulation domain. Then, they train an MLP NN to replace a forward simulation approximating Maxwell interactions. By reversing the MLP network and training it in the “reverse” direction to fine-tune the inverse mapping, they obtain a framework calculating the necessary layer thicknesses for the nanoparticle that would give the prescribed scattering profile. In contrast to our proposed framework that provides a single architecture including both classification and regression, the authors assume the discrete parameter (number of layers of the nanoparticle) is known and train separate networks for each one.

In [34], the authors develop crystal graph CNNs framework to learn material properties from the connection of atoms in the crystal. Instead of simulated data or images, they use crystal graphs that encode atomic information and atomic configuration in the crystals. By passing that feature vectors through the CNN framework, they obtain both discrete and continuous properties of a given crystal. In contrast to this example, we were able to achieve our target accuracy without resorting to complicated architectures such as CNNs. However, in order to achieve our stringent target accuracy we needed to include a stacking network on top of the first MLPR. Another major difference between the two approaches is the fact that we are considering the dynamics of our system of interest, whereas [34] does not.

Inverse design has also been used for predicting airfoil/wing shapes and parameters from aerodynamical parameters. In [35], the authors train various types of NNs (including MLP) using a small dataset comprised of different airfoil types to obtain geometrical parameters of the requested airfoil/wing to accelerate aerodynamic design processes tailored to given criteria. In contrast to our application, the authors use self-organizing maps as multi-class classification to help their framework decide which airfoil type to choose and then determine the geometrical parameters of the selected airfoil type. By adopting this method, they alleviate the adverse effects of having a small training set and high-dimensional target. In [29], the authors considered a similar application, but instead they use 2D 𝐶𝑝 distribution images from simulations and various CNN architectures to estimate the airfoil shape responsible for the given 𝐶𝑝 distribution. They have conducted a more concise version of network hyperparameter tuning procedure that resembles to the one we have conducted in this work. However, they are using “static” images of the 𝐶𝑝 distribution whereas we are considering a dynamical system of the oscillation of the gel sheet in our framework.

In [27] the author uses an MLP-based inverse design framework to obtain structural parameters of microstrip circular antennas that would give a prescribed frequency response. Similar to our approach, the author uses a forward simulation including PDEs to generate the training data. However, the input to their inverse design framework is in frequency domain, whereas our architecture works with inputs in time domain. Additionally, the methodology in [27] lacks the classification stage that is essential to our method.

In [36], the authors use a supervised autoencoder (SAE)-based inverse design architecture, where they feed in vectorized images of graphene kirigami structures and reduce to a 10-dimensional latent space including the ultimate stress and strain as supervised parameters. The authors embed inverse design into an architecture mainly used for dimensionality reduction and in addition, they force two elements of the latent space to be physical properties such as stress and strain. Another noteworthy achievement in [36] is the ability of the framework’s ability of interpolation between two distinct cut orientations (horizontal, vertical) to get shapes that have diagonal slits. Since we do not investigate superposition of multiple wave types in our analysis, we refrain from using interpolation methods between classes in our framework. Moreover, we separate the inverse design stages for the distinct classes using a MLP classifier to make the regression afterward easier.

In [37], the authors use an encoder-decoder architecture comprised of 1D convolutional layers to obtain geometrical parameters of an acoustic sink using given a sound absorption coefficient profile. Using the simulated parameter-absorption profile pairs, the authors train the encoder and decoder portions of the proposed framework separately. For some selected profiles, they manufacture the acoustic sinks using additive manufacturing and test the sound absorption profile using an experimental setup. Finally, they compare the sound absorption coefficient profiles of the prescribed, the estimated (using the decoder) and the built acoustic sinks. We do not seek to replace the forward simulation developed by [26]; therefore, we do not need anything similar to the decoder stage in [37]. Instead, we introduce stacking to the regression stage to account for the errors in the predicted parameters due to their position in the 3 dimensional coordinate space (ℙ⊂ℝ3). Finally, [37] also lacks the classification stage that plays a crucial role in our framework.

To summarize, the existing approaches consider methods ranging from autoencoders to different CNN setups. Even though there exists similar MLP-based inverse design approaches in the literature, none of the aforementioned works (except [35] which implicitly does multi-class classification through self-organizing maps) provide classification and regression within a single architecture. Additionally, none of the listed works do inverse design on a dynamical system. Last but not least, despite the differences in applications and methodologies followed, only [29] follows a hyperparameter tuning procedure such as we do in the sections that follow. The rigorous procedure we follow in our work helps us justify the robustness of our methodology and provides explanation for the particular hyperparameter setting that we decide at the end.

The rest of this paper is organized as follows: Sect. 2 provides detail about the forward simulation by introducing the underlying equations and discretization, Sect. 3 describes the inverse design problem and introduces the proposed two-staged architecture, Sect. 4 explains the experimental procedure along with the obtained results for both classification and regression as well as with the verification simulations, and Sect. 5 summarizes our findings.

Forward simulation
In this section, the simulation model used as the forward simulation for generating data is described. This simulation model is based on a semi-implicit solver developed by Alben et al. [26].

Equations for the dynamics of flexible sheets
This section describes the equation used to simulate a hexagonal self-oscillating gel sheet defined via a triangular lattice (for example, left panel of Fig. 1). The lattice has stretching springs between neighboring vertices, and bending springs with energy proportional to the square of the (dihedral) angle between neighboring triangular faces. As the lattice spacing tends to zero, the energy closely approximates that of a continuum isotropic elastic sheet [26, 38, 39]. The motion of the lattice is driven by time- and space-varying distributions of the rest lengths of the stretching springs. In the motivating experiments on thin gel sheets [25], there are chemical waves, radial or spiral in form, that induce local swelling of the sheets. As a simple model, we study radial or planar (unidirectional) traveling waves in the corresponding simulations:

𝑅𝑎𝑑𝑖𝑎𝑙𝑤𝑎𝑣𝑒: 𝜂(𝑥,𝑦,𝑡;𝐴,𝑘)=1+𝐴sin(2𝜋(𝑘𝑥2+𝑦2‾‾‾‾‾‾‾√−𝑡))
(1)
𝑃𝑙𝑎𝑛𝑎𝑟𝑤𝑎𝑣𝑒: 𝜂(𝑥,𝑡;𝐴,𝑘)=1+𝐴sin(2𝜋(𝑘𝑥−𝑡))
(2)
Here 𝜂 is the relative change in rest length of the stretching springs, and is a sinusoidal traveling wave function of the (x, y) location in the undeformed sheet and time t, with wave amplitude A, and wavenumber k. The sheet moves by overdamped dynamics

𝜇∂𝐫∂𝑡=𝐟𝑆(𝐫;𝐾𝑠,𝜂)+𝐟𝐵(𝐫).
(3)
with stretching force 𝐟𝑆, bending force 𝐟𝐵, and damping constant 𝜇. We nondimensionalize quantities with dimensions of length by the radius of the hexagonal sheets, and time by the period of 𝜂 (assumed to be time-periodic). 𝐟𝑆 is proportional to the stretching stiffness 𝐾𝑠 (nondimensionalized by bending stiffness, and thus, this constant is proportional to sheet thickness to the −2 power [26]). The rest strain function 𝜂 enters 𝐟𝑆 explicitly, but not 𝐟𝐵. If the sheet is initially nearly planar, with a small out-of-plane perturbation, and 𝜂 is spatially nonuniform, 𝐟𝑆 can cause the perturbation to grow, i.e., buckling. Buckling occurs above a critical threshold of the rest strain amplitude A. The threshold decreases as 𝐾𝑠 increases, i.e., when the stretching force becomes more dominant over the bending force, which resists buckling.

To solve Eq. (3), we apply a semi-implicit method with a second-order-in-time backward differentiation discretization, developed in [26]:

𝜇𝐴𝑝3𝐫𝑛+1−4𝐫𝑛+𝐫𝑛−12Δ𝑡=𝐾𝑠𝐋𝐫𝑛+1+2𝐟𝑆𝐸(𝐫𝑛)−𝐟𝑆𝐸(𝐫𝑛−1)+𝐁𝐫𝑛+1−2𝐁𝐫𝑛+𝐁𝐫𝑛−1+2𝐟𝐵(𝐫𝑛)−𝐟𝐵(𝐫𝑛−1)
(4)
The superscripts denote time steps. On the right-hand side of Eq. (4), the first three terms represent the stretching force. Of these, the first, depending on 𝐫𝑛+1, is implicit and linear (with 𝐋 a discrete Laplacian matrix), and the second and third (denoted 𝐟𝑆𝐸) are explicit and nonlinear, but bounded in norm. The remaining terms are the bending force, again with a linear implicit term (involving 𝐁, a discrete biharmonic matrix) that approximates the bending force at time step 𝑛+1, while the remaining terms approximate the remainder of the bending force with second-order temporal accuracy.

Examples of flexible sheet dynamics
We spatially discretize an initially flat hexagon of radius 1 with an equilateral triangular lattice mesh, with initially uniform mesh spacing 1/33, giving 3367 mesh points. We apply a small out-of-plane perturbation, and evolve the sheet forward in time with Eq. (4). For large enough wave amplitude A, the sheet rapidly buckles into shapes with time-varying distributions of curvature, large in magnitude. Each simulation is run for 𝑡∈[0,20] to allow a sufficient amount of time for the sheet to evolve beyond the transient initial buckling motion. Figure 1 shows a snapshot of the sheet (left panel) during an example of the dynamics. The right panel shows the corresponding time traces of the norms of the stretching force (red) and bending force vectors (blue), denoted 𝐹𝑠𝑡𝑟𝑒𝑡𝑐ℎ and, 𝐹𝑏𝑒𝑛𝑑.

Fig. 1
figure 1
A snapshot of the flexible sheet (left) and stretching and bending force norms (right) at 𝑡=20. Sheet parameters: 𝐴=0.1, 𝐾𝑠=7500, 𝑘=2, 𝑇=Radial

Full size image
A set of snapshots of the motion over time for varying A is shown in Fig. 2. Note that with increasing wave amplitude the sheet experiences different patterns of curvature resulting from outward-moving circumferential bands of dilation and contraction. The range of the simulation parameters for which this occurs is discussed in Sect. 4.1. We take the sheet position 𝐫—x, y and z coordinates at each of the 3367 lattice points, and for each time step—as the training features. We also use the stretching and bending force vectors, which have the same dimensions as the position. The stretching and bending forces involve the second- and fourth-order derivatives, respectively, of the lattice positions. These forces can be written in terms of the sheet curvature and its spatial derivatives, and hence give measures of the sheet shape.

Fig. 2
figure 2
Snapshots of the sheet with various values for A, listed at left, between 𝑡=19 and 𝑡=20 (0.2 time units between each image). Here we have a radial wave for 𝜂 in (1) with 𝑘=1; and we take 𝐾𝑠=1.15×104 [26]

Full size image
In Fig. 1 right panel, the force vector norms evolve nonperiodically in time, reflecting nonperiodic dynamics of the sheet position. Nonetheless, the sheet shape is often smooth, with certain approximate spatial symmetries, such as a bilateral symmetry for the snapshot in the left panel.

Inverse design for the flexible sheet analysis
In this section, we describe the inverse design problem—the task of mapping motion to parameters. Motion is explicitly defined by a discrete sequence of snapshots of the shape of a three-dimensional lattice and associated derived quantities such as bending and stretching forces.

The inverse design problem is solved using data created by the simulation described in Sect. 2. The parameters, inputs, and simulation outputs used in this design problem are provided in Fig. 3. The sheet parameters are amplitude A, stretching stiffness constant 𝐾𝑠, wavenumber k, and wave type T. The amplitude A, sheet stiffness 𝐾𝑠 and wavenumber k are all continuous parameters. The type T is a binary variable specifying wave form type—0 for radial travelling wave and 1 for planar travelling wave. The ranges of the continuous parameters are chosen to avoid sheet deformations which are not physically possible, as described in Sect. 4.1.

Fig. 3
figure 3
Inverse engineering diagram for the flexible sheet analysis

Full size image
Our contribution is a learning architecture that converts target motion into sheet/reaction parameters. This architecture has two stages: the first stage is a classifier that determines wave types from lattice snapshots, and the second stage consists of two regressors—one for each wave-type—to identify the continuous parameters. Principal component analysis (PCA) is used as the main preprocessing step for the inputs in both stages.

Standardization of data before and/or after applying PCA is done for multitude of reasons: (1) to neutralize the effects of outlier in the data [40], (2) to make the units of the variables comparable [41] and (3) to neutralize the difference in the orders of magnitude between data [42]. A schematic of this architecture is shown in Fig. 4. In the following sections, the components of this architecture are discussed in detail.

Fig. 4
figure 4
Structure of the inverse mapping architecture. MLP classifier (red) is used to determine the motion type on the gel. According to the motion type, the architecture selects one of the specialized MLP regressors (green) and predicts the continuous sheet parameters (Color figure online)

Full size image
Stage 1: MLP classifier with PCA preprocessing
The first stage is a multi-layer perceptron (MLP) classifier [43] that distinguishes between reaction wave types. Since there are different types of symmetries on the sheet for different types of motions (radial symmetry for radial waves and plane symmetry for planar waves), we found that using only the x coordinates of the lattice points are sufficient to determine the motion type.

We also find that preprocessing with principal component analysis (PCA) both accelerates the learning algorithm and increases accuracy by up to 50% (for single snapshot training) and 3% (for 10 Snapshot training), as will be discussed in Sect. 4. For each training snapshot, the raw features used as inputs to PCA are the x coordinates of 3367 lattice points obtained from the simulations. We found it is not necessary to use the time dependencies of the snapshots to obtain accurate predictions of reaction wave type.

PCA preprocessing extracts linearly uncorrelated features from the lattice positions and reparameterizes the data along new orthogonal axes with decreasing variance [44, 45]. It can be computed using the singular value decomposition (SVD) [46] as follows. Let us consider n data points, each with m features. SVD decomposes the data set organized in an 𝑚×𝑛 matrix, 𝑋∈𝑅𝑚×𝑛, into three factors

𝑋=𝑈Σ𝑉𝑇 where 𝑈∈ℝ𝑚×𝑚,Σ∈ℝ𝑚×𝑛, 𝑉∈ℝ𝑛×𝑛.
(5)
The columns of 𝑈̃ =𝑈Σ represent principal directions onto which X is projected. The projected values are called principal components. The projection of X onto the first p principal directions (𝑢̃ 1 to 𝑢̃ 𝑝 with 𝑢̃ 𝑖 sorted in decreasing singular value order) creates the new data matrix Y, whose coordinates are defined by the columns of U

𝑌=[𝑢̃ 1,…,𝑢̃ 𝑝]𝑇𝑋, where 𝑌∈ℝ𝑝×𝑛, 𝑝≤𝑚
(6)
Using this formulation, the size of the feature space is reduced to p.

PCA preprocessing enables a reduction in the number inputs into a neural network, and thereby enables a more compact network architecture [47]. Furthermore, evidence suggests that the resulting PCA-NN architecture can achieve better performance in some cases [48], with multiple successes found in the literature [49,50,51].

Though PCA offers reduction of dimension for data sets, the size of the data sets used in learning can still hinder learning algorithms due to large memory requirements for standard PCA. To prevent this problem we used a memory efficient version of PCA, called Incremental PCA (iPCA) [52, 53]. Incremental PCA determines principle components by using a sequence of mini-batches of the initial data set.

Following iPCA and renormalization, the new features passed through a classifier that is comprised of fully connected layers, of which at least one is a hidden layer. ReLU activation functions are used in the hidden layers. The output layer uses a sigmoid function to obtain a value between [0, 1]. The output is then rounded to the nearest integer to indicate one of the labels for the motion of the gel.

Stage 2: MLP regressor with PCA preprocessing
The second stage of the architecture is a stacked set of regressors, one stacked regressor for each wave type. These regressors aim to identify the three continuous sheet parameters: amplitude, stiffness, and wavenumber (𝐴,𝐾𝑠,𝑘, respectively).

The features used for this portion of the architecture are the y and z coordinates of the lattice points and the resultant bending and stretching forces on the gel sheet (𝑓𝑏 and 𝑓𝑠, respectively). The force components are nonlinear combinations of higher order derivatives of the lattice positions. We found that using this physics-inspired quantity avoids the need to incorporate convolutional layers that also essentially extract derivatives.

Since the regressor networks are specialized to single reaction wave type (either radial or planar), only the simulations of one wave type is used as training set. For each snapshot, the raw features used as inputs to another PCA preprocessing procedure (see Fig. 5) are the y and z coordinates of 3367 lattice points as well as 10 bending, and 10 stretching force norms that are calculated between each snapshot (a sum of 6754=2×3367+10+10 total features). Similar to the classification stage, the time dependencies of the training snapshots are not taken into consideration.

To keep the analysis concise, we are using a fixed dimension reduction via PCA transformation from 10 to 7 for both bending and stretching force data. We want to keep the number of the principal components for force components as high as possible so that the force components are not outnumbered by the coordinate components at the input layer of the networks. The performance may increase by changing this coordinate transformation but we keep it out of our analysis because we were able to achieve excellent results even by fixing it at 7.

This regressor structure outputs the three continuous sheet parameters. Unlike the classification stage, this stage consists of two stacked networks as shown in Fig. 5.

Fig. 5
figure 5
Stacked MLP Regressor (SMLPR) architecture consists of two components (green boxes). First layer MLPR outputs the sheet parameters and those sheet parameters are fed into the second MLPR stage along with force and coordinate components. The second stage outputs the sheet parameters more accurately (yellow box) (Color figure online)

Full size image
This parameter estimation problem can be visualized as following: given simulation’s data correspond to a unique coordinate in the feasible parameter space (ℙ⊂ℝ3) and our aim is to find that coordinate in ℙ using the proposed SMLPR structure. The first regressor network stage makes an initial prediction about the coordinate of given simulation data in ℙ. However, this estimation has low robustness (high percentile values over the test set). The second regressor network uses the outputs of the first network (preliminary coordinate predictions of the three continuous sheet parameters in ℙ) on top of the inputs used at the first network (preprocessed x, y coordinates and 𝑓𝑏,𝑓𝑠) as input and outputs the final predictions for the three continuous sheet parameters. Adding the preliminary coordinate estimates from the first network to the coordinate and force components reduces the distance between the actual and estimated parameters in the parameter space ℙ.

Furthermore by including the initial predictions of the input data in the second network’s inputs, we aim to eliminate the underlying effects of the location of the parameters in the 3D parameter space on the estimation accuracy.

Similar to the classification portion, ReLU activation function is used for all fully connected layers with the exception of output layers of each network due to regression purposes.

Experiments and results
In this section we describe the data generation and training procedures; present our approach to architecture determination; and demonstrate inverse design performance.

Scikit-Learn [54] is used for all preprocessing and PyTorch [55] is used for all neural network training. Computers used for training the networks are equipped either with an NVIDIA Quadro P1000 GPU with 4GB memoryFootnote1 or with an NVIDIA GeForce GTX 1050 Ti with Max-Q Design with 4GB memory.Footnote2 All of the graphics card drivers are up to date at the time of the trainings.Footnote3 The classifiers are trained using binary cross-entropy loss. The regressors are trained using squared loss. The optimization procedure uses the Adaptive Moment Estimation (ADAM) stochastic gradient descent [56]. Hyperparameters for

ADAM are set as in [56]: 𝛼=0.001, 𝛽1=0.9, 𝛽2=0.999 and 𝜖=10−8. In addition to the hyperparameters, the sizes of the batches of each training method are adjusted so that there are 26 batches for all classification case studies to enable reasonable memory usage on our hardware and provide a fair ground to evaluate the classification accuracy by fixing the number of updates on the weights.

We use a fixed number of 20 training epochs for classification case studies to keep the training time short and avoid overfitting of the network. For regression we use 200 epochs as baseline epoch count. The number of batches for regression studies are limited to 256 to limit the memory usage on our hardware. We did not find significant sensitivity to the number of epochs in the classification case. The effect of the number of epochs for the regressors is investigated in Sect. 4.3.

Data generation
In this section, we identify the parameter domain, training data, and validation data.

The parameter bounds are determined through random and targeted sampling across the simulation space to identify a rectangular region containing feasible motions. This procedure yields 𝐴∈[0.01,0.22] for the amplitude; 𝐾𝑠∈[103,104] for the stretching stiffness; and 𝑘∈[0.1,10] for the wave number.

The parameter domain for these continuous variables is then obtained as a tensor product space .

To generate the training data, we discretize  into a 16×20×20 grid of equidistant points. For each parameter combination 𝑝(𝑖)∈ we generate a simulation for both wave types—yielding 6400=16×20×20 simulations per wave type.

Each simulation results in a solution 𝑟(𝑖)(𝑡) for 𝑡∈[0,20]. Our networks are trained at certain snapshots of this solution, and these snapshots are selected from 𝑡∈[6.1,7.0]. Figure 1 shows that this time is after the initial transient phase and still provides us with times in the future with which we can test extrapolation.

We generate two different types of training data sets from these trajectories. In the first version, which we call single-snapshot-per-simulation, we choose only a single time instance for the training data. In this case we will use ten different training data sets to train ten different networks. Specifically, each of the data sets corresponds to snapshots from 𝑡=6.1,6.2,…,7.0. In the second version, multi-snapshot-per-simulation, we use a single larger training set with all ten-snapshots-per-simulation in the training data.

Both networks will be tested on a validation set . This validation set is obtained by generating 15000 random uniformly distributed parameters across the input domain  and then obtaining simulation results for each wave-type. To test extrapolation performance, a total of 20 validation snapshots are taken from each of these simulation from the time span 𝑡∈[7.1,9.0].

Classification
In this section we perform a parametric study over network architectures to determine a classifier. This study is a single feed-forward process by which we first optimize the number of hidden units, then the number of hidden layers, and then the number of principle components. We show that this process leads to sufficient accuracy for the classification network. The baseline architecture from which we begin has 1 hidden layer and 10 hidden units. This process is performed for both the single- and multi-snapshot training sets outlined above. The single-snapshot results are provided in Sect. 4.2.1, and the multi-snapshot results are provided in Sect. 4.2.2.

Metrics
These studies consider three performance metrics: mean classification accuracy (MCA), worst case classification accuracy (WCA), and training time.

For one snapshot training, 𝑀𝐶𝐴1 is the average classification accuracy that networks trained using a single 𝑟(𝑖)(𝑡) from 𝑡∈[6.1,7.0] achieve over  (Eq. 7).

𝑀𝐶𝐴1=1||∑𝑖=1||120∑𝑘=120110∑𝑗=110𝛿(𝑇(𝑖),𝑇̂ (𝑖)(𝑡𝑘;𝑡𝑡𝑟𝑎𝑖𝑛,𝑗))
(7)
where 𝑇(𝑖) is the true type of the i-th simulation in the validation set ; 𝑇̂ (𝑖) is the predicted type for the i-th simulation in the validation set where the prediction is based on a snapshot in  from time 𝑡𝑘∈{7.1,7.2,…,9.0} (20 total); 𝑡𝑡𝑟𝑎𝑖𝑛,𝑗 is the time at which snapshots for the training data were taken; and finally 𝛿 is the delta function (1 if arguments are equal, and 0 otherwise). The sum over the time of the training snapshot is used to obtain to the average performance of a single-snapshot-trained network.

𝑊𝐶𝐴1 is the minimum average classification accuracy that networks trained using a single 𝑟(𝑖)(𝑡) from 𝑡∈[6.1,7.0] achieve over  (Eq. 8).

𝑊𝐶𝐴1=min𝑗∈[1,10],𝑘∈[1,20]1||∑𝑖=1||𝛿(𝑇(𝑖),𝑇̂ (𝑖)(𝑡𝑘;𝑡𝑡𝑟𝑎𝑖𝑛,𝑗))
(8)
For the ten-snapshot training, we do not average over different training data sets because the training is performed using all ten snapshots of 𝑡∈[6.1,7.0]. As a result, the 𝑀𝐶𝐴10 becomes:

𝑀𝐶𝐴10=1||∑𝑖=1||120∑𝑘=120𝛿(𝑇(𝑖),𝑇̂ (𝑖)(𝑡𝑘)),
(9)
and the 𝑊𝐶𝐴10 becomes:

𝑊𝐶𝐴10=min𝑘∈[1,20]1||∑𝑖=1||𝛿(𝑇(𝑖),𝑇̂ (𝑖)(𝑡𝑘)).
(10)
MCA [57, 58] and WCA [59, 60] are used widely in the learning literature to provide a more holistic view by accounting for both the overall performance and the reliability of the proposed networks.

Training time is used to differentiate between networks with similar MCA and WCA values. However, in other applications, this timing can have a higher impact factor [30, 33]. If all three performance metrics yield similar results, the network with the minimum dimensions (number of hidden layers, number of units or number of principal components) is preferred. Our aim is to have both MCA and WCA ≥99% for classification.

Procedure
The analysis proceeds by first choosing the number of hidden units, then the number of hidden layers, and finally refining the number of principal components. In the first two steps we also search over a coarse grid of principal components, ranging between 10 and 2000, so that the hyperparameter search is not entirely a greedy search. Results for each network combination are provided in Figs. 7, 8, 9, 10 and 11 and Figs. S1–S3.

Figures 7, 8 and 9 and Figs. S1–S3 (provided among supplementary materials for more detailed analysis) correspond to single snapshot-per-simulation training. Each column represents training with a different time 𝑡𝑘, where 𝑡𝑘 is denoted at the top of the figures. Each row of these figures represents testing on different time-intervals of the validation set: the first row uses t between 7.1−8.0 and the second row uses t between 8.1−9.0. Both rows demonstrate extrapolation in time.

Figure 11 corresponds to the ten snapshot training studies, recall that this approach trains a single network on multiple snapshots (compared to ten networks at single snapshot training). Each column of these figures represents testing on different snapshots in the validation set 𝑣(𝑖)(𝑡)∈. The first and third columns of Fig. 11a use t between 7.1 and 8.0, and the second and fourth columns of Fig. 11a use t between 8.1 and 9.0. For Fig. 11b, first column uses t between 7.1 and 8.0 and second column uses t between 8.1 and 9.0.

We now describe hyper-parameter tuning results in detail.

Single snapshot-per-simulation training
We first investigate whether training on a single snapshot-per-simulation is sufficient. Figure 6 summarizes the results of the parametric studies done using single snapshot along with the selected network parameters. Upon completion of the pass we achieve a network with 𝑀𝐶𝐴1 of 95.3% and 𝑊𝐶𝐴1 of 84.6%.

Fig. 6
figure 6
Flow diagram for classification architecture determination using single snapshot-per-simulation training. Each box represents a sequential step in the parametric study of this section. The architectural choice is written above each arrow and the performance is written below

Full size image
Figures S1–S3 are provided among supplementary materials for more detailed results.

Number of Hidden Units
In this subsection we study the effect of the number of hidden units on classification accuracy. The worst performance is observed with 10 hidden units (87.2% 𝑀𝐶𝐴1, 72.3% 𝑊𝐶𝐴1), and the best performance is observed with 300 and 100 hidden units (95.3% 𝑀𝐶𝐴1, 84.9% 𝑊𝐶𝐴1, respectively).

Pointwise classification errors—per training validation simulation and time snapshot—are shown in Figs S1 for 10 units,  S2 for one hundred units, and S3 for three hundred units.

From these figures, we calculate that 𝑊𝐶𝐴1 and 𝑀𝐶𝐴1 become better as the number of hidden units increases from 10 to 100 (Fig. S2 94.3% 𝑀𝐶𝐴1 at 30 PCs, 84.9% 𝑊𝐶𝐴1 at 30 PCs). However, beyond 100 hidden units, the performance of the MLP classifiers starts to saturate. Specifically, as the number of hidden units increases from 100 to 300, 𝑀𝐶𝐴1 increases up to 95.3% for 30 PCs, and we obtain a 𝑊𝐶𝐴1 of 84.6% at 30 PCs (Fig. S3). Though the 𝑊𝐶𝐴1 seems to deteriorate slightly, we choose 300 hidden units as the result of our analysis.

Number of hidden layers
Next we increase the number of hidden layers. Following this process we find an improved three-layer architecture with 97.0% 𝑀𝐶𝐴1 and 84.2% 𝑊𝐶𝐴1.

Pointwise classification errors—per training validation simulation and time snapshot—are shown in Figs. 7 (for 2 layers) and 8(for 3 layers).

Fig. 7
figure 7
Classification accuracies of 2-layer neural networks with 𝑁𝑢𝑛𝑖𝑡𝑠=300 on snapshots from the validation set (top row: 𝑡∈[7.1,8], bottom row: 𝑡∈[8.1,9].). Columns correspond to networks trained with single-snapshots obtained at the indicated training time (TR). Summary: highest 𝑀𝐶𝐴1=96.3% obtained with PC=30; highest 𝑊𝐶𝐴1=84.4% obtained with PC=30

Full size image
Fig. 8
figure 8
Classification accuracies of 3-layer neural networks with 𝑁𝑢𝑛𝑖𝑡𝑠=300 on snapshots from the validation set (top row: 𝑡∈[7.1,8], bottom row: 𝑡∈[8.1,9].). Columns correspond to networks trained with single-snapshots obtained at the indicated training time (TR). Summary: highest 𝑀𝐶𝐴1=97.0% obtained with PC=30; highest 𝑊𝐶𝐴1=84.2% obtained with PC=30

Full size image
From these figures we calculate that 𝑊𝐶𝐴1 and 𝑀𝐶𝐴1 becomes better as the number of hidden layers increases to 2 (Fig. 796.3% 𝑀𝐶𝐴1 at 30 PCs, 84.4% 𝑊𝐶𝐴1 at 30 PCs). However, beyond 2 hidden layers, the performance of the MLP classifiers starts to saturate. Specifically, as the number of hidden layers increases from 2 to 3, 𝑀𝐶𝐴1 increases up to 97.0% for 10 PCs, and we obtain a 𝑊𝐶𝐴1 of 84.2% (Fig. 8). Though the 𝑊𝐶𝐴1 seems to deteriorate slightly, we choose 3 hidden layers as the result of our analysis. For further examples of networks that are not included in this work, the bitbucket repositoryFootnote4 can be visited.

Number of principal components
Next we consider the number of principal components. The worst performance is observed under 5 PCs (91.2% 𝑀𝐶𝐴1, 70.6% 𝑊𝐶𝐴1), and the best performance is observed under 15 PCs (97.3% 𝑀𝐶𝐴1, 87.5% 𝑊𝐶𝐴1).

Pointwise classification errors—per training validation simulation and time snapshot—are shown in Fig. 9 for all of the analyses mentioned in this section.

Figure 8 demonstrates peak performance when the number of principal components is ≤30. Therefore, we refine our analysis in this region, and the results are shown in Fig. 9. From these figures, we calculate that the 𝑀𝐶𝐴1 are >96.7% for 10, 15, 20 and 25 principal components. We also find that their training times are similar (1.95–2.25 s). As a result, we choose 15 PCs for the final architecture.

Fig. 9
figure 9
Classification accuracies of 3-layer neural networks with low to no PCs and 𝑁𝑢𝑛𝑖𝑡𝑠=300 on snapshots from the validation set (top row: 𝑡∈[7.1,8], bottom row: 𝑡∈[8.1,9].). Columns correspond to networks trained with single-snapshots obtained at the indicated training time (TR). Summary: highest 𝑀𝐶𝐴1=97.3% obtained with PC=15; highest 𝑊𝐶𝐴1=87.5% obtained with PC=15. The figures are zoomed in to provide more detailed overview and the shifted lower bounds of the figure axes are denoted in red (Color figure online)

Full size image
Single-snapshot summary
Our final architecture is an MLP classifier structure with 3 hidden layers, 300 hidden units and 15 PCs. A comparison of the pointwise classification error of this architecture with a neural network lacking PCA processing is shown in Fig. 9. We observe that without PCA preprocessing, all of the performance metrics become significantly worse. The 𝑀𝐶𝐴1 drops down to 73.7% and training time (which is around 2 s for the network with 3 hidden layers and 300 hidden units) almost doubles itself and jumps to 3.8 s.

Figure S1 shows that the accuracy peaks at the validation snapshots from the same temporal phase as the training data for a given network (e.g., network trained with snapshots from 𝑡=6.7 shows its peak performance at validation snapshots from 𝑡=8.7, 9.7 etc.). We see that all the single-snapshot analyses indicate periodic patterns in the classification accuracy—see Figs. 9 and S3 for the more obvious cases. This sinusoidal trend in the classification accuracy hints that a single time snapshot cannot capture the full range of motion. The recurring high accuracy is caused by the cyclic nature of the underlying physical motion and therefore we conclude that multi-snapshot training is required to achieve better accuracy.

Ten snapshots-per-simulation training
We now repeat the above experiments for the case where ten snapshots are gathered from each training simulation (i.e., entire 𝑟(𝑖)(𝑡)). Figure 10 indicates a significant improvement, eventually obtaining greater than 99% 𝑀𝐶𝐴10 and 𝑊𝐶𝐴10 accuracy. Since we achieve the desired classification performance with a single layer MLPC network, we skip tuning the number of layers as shown in Fig. 10.

Fig. 10
figure 10
Flow diagram for classification architecture determination using ten snapshots-per-simulation training. Each box represent a sequential step in the parametric study of this section. The architectural choice is written above each arrow and the performance is written below

Full size image
This improvement comes with significantly increased training data size. The ten snapshot training data occupy 1.6 GB of memory compared to 160MB for the single-snapshot training. Nevertheless, we find the training times of both networks have the same order of magnitude.

Number of hidden units
In this subsection we study the effect of the number of hidden units on classification accuracy. The worst performance is observed under 10 hidden units (99.1% 𝑀𝐶𝐴10, 98.5% 𝑊𝐶𝐴10) and the best performance is observed under 30 hidden units (99.7% 𝑀𝐶𝐴10, 99.5% 𝑊𝐶𝐴10).

Pointwise classification errors—per training validation simulation and time snapshot—are shown in Fig. 11a for 10 units and 30 units.

From Fig. 11a, we calculate that the 𝑊𝐶𝐴10 and the 𝑀𝐶𝐴10 becomes slightly better as the number of hidden units increases from 10 (99.1% 𝑀𝐶𝐴10 and 98.5% 𝑊𝐶𝐴10 at 1000 PCs) to 30 (99.7% 𝑀𝐶𝐴10 and 99.5% 𝑊𝐶𝐴10 at 300 PCs). As the number of units increase to 30, both of the criteria are met.

Unlike in the single snapshot training cases, the networks using many principal components are performing close to the desired level of accuracy—≥99% for all simulations in the validation set  among the networks with 30 hidden units (𝑃𝐶≥100). Due to memory efficient computing concerns, the focus is to achieve the highest possible accuracy with the simplest network possible and therefore we select the network with 30 hidden units as the result of this section.

Number of principal components
Next we refine the number of principal components. The worst performance is observed with 50 principal components (97.2% 𝑀𝐶𝐴10, 96.0% 𝑊𝐶𝐴10), and the best performance is observed under 200 principal components (99.7% 𝑀𝐶𝐴10, 99.4% 𝑊𝐶𝐴10). Furthermore, a comparison between the best performing PCA hybridized MLP classifier and non-PCA version of the same network shows that the PCA-based classifier is better by 3.7% 𝑀𝐶𝐴10 and 3.6% 𝑊𝐶𝐴10

Pointwise classification errors—per training validation simulation and time snapshot—are shown in Fig. 11b for all of the analyses mentioned in this section.

Figure 11a demonstrates peak performance when the number of principal components is between 100 and 300 PCs. Therefore, we refine our analysis in this region and focus on the region between 50and250 PCs. The results of this analysis are shown in Fig. 11b.

Fig. 11
figure 11
Ten snapshot classification results with coarse principal component search (Fig. 11a). Ten snapshot classification results with fine principal component search and no-PCA comparison (Fig. 11b). Networks are trained using entire 𝑟(𝑖)(𝑡) with 𝑡∈[6.1,7.0]. The figures are zoomed in to provide more detailed overview and the shifted lower bounds of the figure axes are denoted in red. Time of the validation snapshots are denoted along the x-axis (Color figure online)

Full size image
As can be seen in Fig. 11b, all of the networks except the ones with 50 and 100 principal components achieve comparable and satisfactory 𝑀𝐶𝐴10 that is above 99.5%. Since the training times and the 𝑊𝐶𝐴10 accuracy performances of the networks are not different from each other, it can be concluded that the MLPC with 1 layer and 30 units having 150 principal components is the best performing combination for this architecture.

Ten-snapshots summary
Our final architecture is an MLP classifier structure with 1 layer and 30 units and 150 PCs. A comparison of the pointwise classification error of this architecture with a neural network lacking PCA processing is shown in Fig. 11b. We observe that without PCA preprocessing, networks perform comparably but still worse. The 𝑀𝐶𝐴10 drops down to 96.2% and the 𝑊𝐶𝐴10 drops down to 95.8%. These results give the impression that the desired specifications may be achieved using a more complex network architecture. However, the training time grows substantially with the absence of PCA preprocessing. The MLP classifier takes around 1.3s on average to train with PCA preprocessing whereas without the PCA preprocessing, the training takes around 13s.

Classification summary
Though 𝑀𝐶𝐴1 of the networks trained with single-snapshot are close to the desired level, networks trained with ten-snapshots outperform their single-snapshot counterparts by a large margin. Furthermore, that increase in both MCA and WCA are achieved with a much more compact MLP classifier. Therefore, our inverse design architecture is equipped with a MLP classifier with PCA preprocessing trained using ten-snapshots per simulation.

Regression
In this section we perform a parametric study over the network architecture of the regressor stage described in Sect. 3.2. We focus only on training with ten-snapshots-per-simulation.

This study is a single forward process by which we first optimize the number of hidden units, then the number of hidden layers, then narrow in on the number of principle components, and finally the number of epochs as shown in Fig. 12. As with the classification procedure, the initial phases also consider parametric dependence on the number of principal components.

Fig. 12
figure 12
Flow diagram for regression architecture determination using ten snapshots-per-simulation training. Each box represents a sequential stage in the parametric study of this section. The best architectural choice following a particular stage is written above each arrow and the performance is written below

Full size image
The baseline architecture has 1 hidden layer and 30 hidden units.

Metrics
Two types of performance metrics are considered: (1) mean 𝛼− quantiles (𝑀𝑄𝛼) of percent error, and (2) mean maximum (MMax) percent error. The 𝑀𝑄𝛼 percent error is defined as

𝑀𝑄𝛼=120∑𝑘=120𝚚𝚞𝚊𝚗𝚝(𝛼,𝑃𝐸𝑘),𝑃𝐸𝑘=100(𝑝(𝑖)−𝑝̂ (𝑖)(𝑡𝑘)𝑝(𝑖)).
(11)
In particular, this is the average (over 20 snapshots in the testing regime 𝑡𝑘∈[7.1,9.0]) median percentage errors achieved by the stacked regressor networks trained using a ten-snapshot sequence 𝑟(𝑖)(𝑡). The function 𝚚𝚞𝚊𝚗𝚝(𝛼,𝑃𝐸𝑘) returns the 𝛼-th quantile values for a set of data points 𝑃𝐸𝑘. Here 𝑃𝐸𝑘 is a vector of percentage errors in recovering some parameter 𝑝(𝑖) in the testing set of parameters using snapshot 𝑟(𝑡𝑘) for 𝑡𝑘∈[7.1,9.0] (the testing time-regime). Tables 1, 2, 3, 4 and 5 show the 25th, 50th, and 75th quantiles.

Similarly, the average maximum error is defined as

𝑀𝑀𝑎𝑥=120∑𝑘=120max(𝑃𝐸𝑘)
(12)
However, since the extreme outliers can shift the mean substantially, we prefer median as an unbiased metric for the regression studies.

The aim of the case studies is to obtain a neural network that has small median PE. We also want the final network to be robust — having 𝑀𝑄75≤5%. Unless there is a substantial difference between two MMax values, it will not play a decisive role in the analysis.

Procedure
The analysis proceeds by first choosing the number of hidden units, then the number of hidden layers, then refining the number of principal components and finally the number of epochs on the training set. In the first two steps we also search over a coarse grid of principal components, ranging between 20 to 640, so that the hyperparameter search is not entirely a greedy search.

For the simplicity of the analysis, both MLPs within the stacked MLP regressors (SMLPR), defined in Sect. 3.2, have the same network shape, i.e., if a regressor network is described as having 5 layers and 100 units, then the neural network consists of two MLP regressors having 5 hidden layers with 100 neurons each. Results for each network combination are provided in Tables 1, 2, 3, 4 and 5, Figs. 13, 14 and 15 and Figs. S4–S8. (Figures S4-S8 are provided in the supplementary material for the interested reader.)

Fig. 13
figure 13
Regression metrics of 1-layer SMLPR network with 𝑁𝑢𝑛𝑖𝑡𝑠=300 (trained using entire 𝑟(𝑖)(𝑡)∈[6.1,7.0]) on 20-snapshots from the validation set (𝑡∈[7.1,9]). Columns correspond to different continuous sheet parameters, rows correspond to different metrics. Summary: lowest 𝑀𝑄50=1.38% obtained with PC=80 for stiffness; lowest 𝑀𝑄75=3.48% obtained with PC=80 for stiffness

Full size image
Fig. 14
figure 14
Regression metrics of 4-layer SMLPR network with 𝑁𝑢𝑛𝑖𝑡𝑠=300 (trained using entire 𝑟(𝑖)(𝑡)∈[6.1,7.0]) on 20-snapshots from the validation set (𝑡∈[7.1,9]). Columns correspond to different continuous sheet parameters, rows correspond to different metrics. Summary: lowest 𝑀𝑄50=1.11% obtained with PC=160 for stiffness; lowest 𝑀𝑄75=3.22% obtained with PC=40 for stiffness

Full size image
Fig. 15
figure 15
Regression metrics of 4-layer SMLPR network with 𝑁𝑢𝑛𝑖𝑡𝑠=300 (trained using entire 𝑟(𝑖)(𝑡)∈[6.1,7.0]) on 20-snapshots from the validation set (𝑡∈[7.1,9]) for fine principal component search and no-PCA comparison. Columns correspond to different continuous sheet parameters, rows correspond to different metrics. Summary: lowest 𝑀𝑄50=1.06% obtained with PC=70 for stiffness; lowest 𝑀𝑄75=3.04% obtained with PC=70 for stiffness

Full size image
Tables 1, 2, 3, 4 and 5 provide detailed results for the error metrics of each network configuration in the analyses. The columns are divided into three groups, one for each sheet parameters. The rows are divided into subgroups according to the number of different parameter options investigated under each study and for Tables 1, 2, 3 and 4, regression metrics of the networks with different numbers of principal components are listed. Minimum values of each column are written in bold and the selected value for that study is italicized.

In Figs. 13, 14 and 15 and Figs. S4–S8, each column represents the regression metrics for a different continuous variable (amplitude, stiffness, wavenumber, respectively) and each row gives the performance of the trained network in terms of one of the three regression metrics (25th quantile, median, and 75th quantile, respectively).

Number of hidden units
In this subsection, we study the effect of the number of hidden units on regression performance. Following this analysis step, the highest 𝑀𝑄50 (8.97%) is achieved with 30 hidden units (for wavenumber, 20 PCs) and the lowest 𝑀𝑄50 under this analysis (1.23%) is obtained with 1000 hidden units (for stiffness, 40 PCs) (see Table 1).

Pointwise regression results—per training validation simulation and time snapshot—are shown in Fig.S4 for 30 units, Fig. 13) for three hundred units, and S5) for one thousand units. Then Table 1, the derived regression metrics from Figs. S4-S5 are presented.

Our baseline architecture (1 hidden layer 30 hidden units—Fig. S4) achieves highest and lowest 𝑀𝑄50 recorded 8.97% (for wavenumber, 20 PCs) and 1.96% (for stiffness, 80 PCs), respectively. Increasing the number of hidden units to 300 decreases the 𝑀𝑄50 for all three parameters. The highest and the lowest 𝑀𝑄50 recorded among networks with 300 hidden units are 5.71% (for wavenumber, 20 PCs) and 1.38% (for stiffness, 80 PCs) respectively (Fig. 13).

Further increasing the number of hidden units to 1000 does not result in significant decrease of 𝑀𝑄50. The maximum 𝑀𝑄50 observed among the networks with 1000 hidden units are 5.97% (640 PCs, for wavenumber) and 1.23% (40 PCs, for stiffness) respectively (Fig. S5).

In contrast to 𝑀𝑄50, increasing the number of units does not result in a consistent reduction in MMax as shown in Table 1. This is caused by the random occurring extreme outliers shifting the mean. It is also noteworthy that this phenomenon becomes apparent for the networks trained with higher numbers of principal components.

Though most of the minimum values of each metric appear among networks with 𝑁𝑢𝑛𝑖𝑡𝑠=1000, most of the observed metrics under the networks with 𝑁𝑢𝑛𝑖𝑡𝑠=300 have less difference than 0.5% from the minimum metrics observed in Table 1. Since both networks can achieve comparable MMax values we proceed with 𝑁𝑢𝑛𝑖𝑡𝑠=300 because of smaller network size (as highlighted in Table 1).

Number of hidden layers
In this subsection we seek to improve accuracy by increasing the depth of the networks with layers of 300 hidden units. The highest 𝑀𝑄50 (5.71%) is obtained with 1 hidden layer and 20 PC) and the lowest 𝑀𝑄50 within this analysis (1.0%) is obtained with 3 hidden layers and 80 PCs, according to Table 2.

Pointwise regression results—per training validation simulation and time snapshot—are shown in Fig. S6 for 2 hidden layers, Fig. S7) for three hidden layers, Fig. 14) for four hidden layers, and Fig. S8 for five hidden layers. Then Table 2, the derived regression metrics from Figs. S6–S8 are presented.

We observe that adding the second hidden layer results in a median PE drop up to 50% for all of the parameters as it can be seen in Table 2. Additionally, the MaxPE also drops significantly for amplitude and wavenumber as the number of hidden layers increase to 2. The highest and lowest 𝑀𝑄50 recorded with this network combination are 2.93% (for amplitude, 640 PCs) and 1.07% (for stiffness, 80 PCs) respectively.

Introduction of the third layer does not have a substantial effect on the regression performance other than reducing MMax for all of the parameters.

As the number of layers increases from 2 to 3, the metrics for 𝑁𝑙𝑎𝑦𝑒𝑟𝑠=3 (in Fig.S7) have slightly better values than 𝑁𝑢𝑛𝑖𝑡𝑠=2 (in Fig.S6). The highest 𝑀𝑄50 recorded among networks with 3 hidden layers is 2.43% (for amplitude, 80 PCs) whereas the lowest 𝑀𝑄50 is 1.0% (for stiffness, 80 PCs).

As the number of hidden layers increases to 4, the performance metrics get close to the desired values. Figure 14 demonstrates that the metrics for 80 principal components exhibit results that comply with the goal metrics almost for all test snapshots. The highest and lowest 𝑀𝑄50 recorded for this network combination are 3.11% (for amplitude, 20 PCs) and 1.11% (for stiffness, 160 PCs) respectively.

At this point of the analysis, each additional layer to the SMLPR architecture with 300 hidden units adds between 30 and 50 s of total training time depending on the number of principal components used and number of epochs selected to train the network.

Adding the fifth layer brings almost no improvement to the regression metrics. In Fig. S8, the regression metrics of the network with 5 hidden layers with 300 units can be seen. The maximum and minimum 𝑀𝑄50 recorded among networks with this combination are 3.1% (for amplitude, 640 PCs) and 1.23% (for stiffness, 80 PCs) respectively. This range is similar to the results of the SMLPR with 4 hidden layers. Therefore, SMLPR networks with 4 hidden are selected as the result of this study.

Number of principal components
Next we tune the number of principal components. The highest 𝑀𝑄50 is achieved with 70 principal components (for amplitude,

2.11%) and the lowest 𝑀𝑄50 is achieved with 70 principal components (for stiffness, 1.06%). Table 2 indicates that the best PCA values are between 40 and 80, so we zoom into this region in this study.

Pointwise regression results—per training validation simulation and time snapshot—are shown in Fig. 15 for all the cases. Then Table 3, the derived regression metrics from Fig. 15 are presented.

Table 3 summarizes the results of our analysis. All of the trained networks except the one with 70 principal components satisfy our criterion for 𝑀𝑄50≤2%. However, the 0.75th quantile ≤5% criterion is just satisfied at the networks with 60 and 80 principal components. Since there is not a significant difference between the MMax values or the mean of 0.25th quantiles of PE of two networks, the network with 60 principal components is selected due to its smaller network size.

Number of epochs
In this study, the effect of training epochs on the regression performance is investigated. Furthermore, a comparison between the PCA hybridized and non-PCA version of the same network architecture is done to investigate the benefits of PCA hybridization. The worst performance within this section is observed under 100 epochs (3.44% 𝑀𝑄50 for amplitude), and the best performance is observed under 300 epochs (0.98% 𝑀𝑄50 for stiffness).

Pointwise regression results—per training validation simulation and time snapshot—are only shown for 200 epochs in Fig. 15. Results of 100 and 300 epochs are only included in Table 4 along with the derived regression metrics from Fig. 15 for epoch case studies. The regression metrics for the Non-PCA comparison are listed in Table 5.

The final network parameter of interest is the number of epochs completed to train the SMLPR networks. Please note that the numbers given here represent the number of epochs completed by each individual stage of the SMLPR networks. Though we select a specific value for the dimension of the coordinate data after principle component transformation in the previous section, we investigate the [30, 80] principle component range in this case study again to check if changing the number of epochs have any effect on the best performing PC choice in Table 4.

To investigate the possibility of overfitting, we modify the number of epochs used for training. In particular, we test using hundred more and hundred fewer epochs. Table 4 shows that changing the number of epochs increases the 𝑀𝑄75 for amplitude by at least 0.5 and pushes the error in amplitude prediction beyond our admissible limits. For stiffness and wavenumber, the performance becomes slightly better by increasing the number of epochs. However, the best SMLPR network combination that satisfies our criteria remains as the last section (4 layers 300 units with 60 PCs and 200 epochs).

Regression summary
Finally, we again compare performance of our network to that which lacks PCA preprocessing. Table 4 and Fig. 15 show that removing PCA hybridization from the coordinate components increases the error in stiffness estimation to limit of our predefined admissible values. In our analyses, the SMLPR network with 60 principal components completes the training in 514s whereas the Non-PCA version of the same network completes the training in 638s. As PCA preprocessing proves itself beneficial, we equip our inverse design architecture with a SMLP regressor with 4 layers 300 units 60 PCs that is trained using the ten-snapshot-per-simulation data over 200 epochs and conclude our investigation.

Simulated performance of recovered parameters
In this section, we provide examples of the motion recovered by the resulting inverse mapping architecture in comparison to known actual parameters.

Figures 16, 17, 18 and 19 show the performance for four cases with parameters chosen to display notable vertical displacements (two with planar and two with radial motion). In the top subfigures we provide the motion with the true parameters, and the bottom subfigures provide the motion with the inverted parameters. The selected samples and the respective outputs of the inverse design architecture are listed in Table 6. Inverse design radial motion continuous parameters were recovered with ≤2.87% error, whereas the continuous parameters in the planar motion case were recovered with ≤2.47% error. For all cases, the correct wave type was predicted. Videos for the motions can be found in supplementary material.Footnote5

Fig. 16
figure 16
Comparison of true and estimated motions. Each subfigure shows the vertical displacement (top left); the lattice springs’ rest lengths at 𝑡=3 (top right); time series plot of vertical displacement (bottom left); and time series of the force terms (bottom right) for 𝑡=[0,20]

Full size image
Fig. 17
figure 17
Comparison of true and estimated motions. Each subfigure shows the vertical displacement (top left); the lattice springs’ rest lengths at 𝑡=4 (top right); time series plot of vertical displacement (bottom left); and time series of the force terms (bottom right) for 𝑡=[0,20]

Full size image
Fig. 18
figure 18
Comparison of true and estimated motions. Each subfigure shows the vertical displacement (top left); the lattice springs’ rest lengths at 𝑡=15 (top right); time series plot of vertical displacement (bottom left); and time series of the force terms (bottom right) for 𝑡=[0,20]

Full size image
Fig. 19
figure 19
Comparison of true and estimated motions. Each subfigure shows the vertical displacement (top left); the lattice springs’ rest lengths at 𝑡=11 (top right); time series plot of vertical displacement (bottom left); and time series of the force terms (bottom right) for 𝑡=[0,20]

Full size image
Figures 16 and 17 provide both the shape of the gel at a time instance and a time series of the vertical displacement and forces for the planar waves. We note the general agreement of the time series data.

The simulations with radial waves (Figs. 18, 19) indicate further discrepancy between shapes at a fixed timestep. However, we see that these motions are fairly chaotic, and such qualitative mismatch at a fixed time does not necessarily indicate a significant mismatch in motion type. Indeed the time series of the vertical displacement and forces still indicate qualitative similarity. To obtain a more quantitative comparison, we compare Lyapunov exponents using a long-time simulation over 300 periods using the approach provided in [61]. Here, we find at most 6.78% error in their dominant Lyapunov exponents.

Note, we do not perform a similar comparison for the planar cases because (1) they already indicate strong agreement and (2) since they have one high frequency and one low frequency oscillations, they are unsuitable for Lyapunov exponent estimation using our chosen toolFootnote6 prepared according to [61].

Conclusion
We have successfully created an inverse design mapping to learn sheet parameters of a self-oscillating gel to obtain a target motion. Our final architecture consists of several integrated neural networks—a single classifier that determines a discrete parameter and two regressors for determining continuous parameters. The optimal classifier uses 150 principal components for PCA preprocessing of the input into an MLP with 1 hidden layer and 30 units with 150 principal components. The regressor is a stacked two-network architecture, each having 4 hidden layers and 300 units with 60 principal components. For both classification and regression, the networks are trained using snapshots from 10 successive steps of a forward simulation across a wide range of parameter settings.

We also demonstrated the efficacy of the developed architectures by comparing the simulations of the sheets with the recovered parameters and the true parameters both qualitatively through inspection of the force and vertical displacement trajectories and quantitatively through comparing the Lyapunov exponents of the resulting chaotic systems.

Future work will seek to verify our approach using experimental data on a gel.

Keywords
Inverse design
Soft robotics
Self-oscillating gel
Simulation-based design
Stacked multi-layer perceptron