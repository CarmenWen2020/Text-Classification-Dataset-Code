Although modern deep learning approaches have achieved astounding results in most visual pattern recognition tasks, they do it using large datasets of labeled data. Besides the fact that, in many applications, such labels are costly to obtain, the need for them is not observed in a biologically intelligent machine like the human brain. â€œWhat-Whereâ€ sets were proposed as a way to represent visual patterns in a manipulatable manner, where two-dimensional geometric transformations can be exploited to increase invariance, and thus reduce the need for large amounts of training data. However, the cornerstone of classification using these sets is a similarity measure that implicates a time-consuming computation due to the unstructured nature of sets. In this work, we propose a grid-based coding strategy to represent the sets as sparse binary vectors. By doing so, we achieve three main advantages: first, leveraging pointer-coding of active bits, we reduce the time complexity of the similarity computation from quadratic to linear in the number of elements of the smaller set being compared; second, we use the theoretical framework of sparse representations to justify the classification robustness exhibited in the original work; third, we bring the model under the widely accepted biological constraint that populations of neurons in the brain code sparse representations.

Introduction
Deep learning models like convolutional neural networks (CNNs) have achieved tremendous success on vision tasks [16,17,18]. Despite the fact that these models were originally based in biologically constrained models [7,8,9, 11,12,13,14, 21, 25], nowadays they have branched out almost entirely [24] to a problem-solving focus. Although this freedom can be good to address tasks at hand while we do not understand the brain completely, it is also the main reason behind the inability of the systems to exhibit desirable properties that the brain possess. A typical example is energy efficiency. Yet, in this work, we are more interested in data efficiency.

In fact, we as humans seem to be able to learn from little (to sometimes none) labeled examples. At the same time, these CNNs require many labeled examples even for relatively simple tasks [6].

In [22, 23], following the biological principle [3, 20] of separating â€œwhatâ€ one is seeing from â€œwhereâ€ one is seeing it [27, 28], we proposed a simple model that represents visual patterns as what we refer to as â€œWhat-Whereâ€ sets.

These sets represent the list of visual features that constitute a pattern. For each feature, its kind (â€œwhatâ€) and its position (â€œwhereâ€) are stored.

Using this logic, one can exploit the two dimensional nature of a pattern and apply geometric transformations such that each feature is coded in an object dependent reference frame [5].

Incorporating this knowledge yields a representation that is invariant to many changes and noises that may appear in different elements of the same class and, thus, makes it possible to learn from fewer labeled examples.

Despite this advantage, the model has a major drawback. The cornerstone for the proposed classification mechanism was similarity measure that allowed the application of nearest neighbor. However, due to the lack of structure that sets provide, this similarity measure requires an enumeration of both sets under comparison, to evaluate the similarity between each possible pair of features.

In this work, we propose a simple change to the model that will convert the original sets to sparse distributed vector representations while losing negligible precision. From this we get three key things:

1.
A close replication of the results achieved by the original model with a speed-up from quadratic to linear due to the exploitation of fast structures for sparse vectors;

2.
A sparse distributed representations theoretical framework that helps to understand why the model performs robustly in classification;

3.
A model that is more consistent to biologyâ€™s principles of intelligence.

Background: â€œWhat-Whereâ€ sets for classification
The â€œWhat-Whereâ€ set representation of a pattern holds the occurrences, in the pattern, of previously learned features of a dictionary. Each element codes an occurrence with a pair (â€œwhatâ€, â€œwhereâ€) such that â€œwhereâ€ describes where the feature appears and â€œwhatâ€ describes which feature of the dictionary appears.

We can define an input visual pattern as a function that maps positions to pixel gray values ğ‘ƒ:ğ‘‰â†’[0,1] where

ğ‘‰={[ğ‘–ğ‘—]âˆˆ{0,â€¦,ğ¼âˆ’1}Ã—{0,â€¦,ğ½âˆ’1}}
(1)
is the set of positions in an ğ¼Ã—ğ½ matrix.

The first step towards representing P as a â€œWhat-Whereâ€ set is to learn a dictionary D of codebook vectors (i.e., common visual features) to code the pattern with. In the original work, this was done through k-means clustering of image patches of size ğ‘“Ã—ğ‘“.

ğ·={ğ0,ğ1,...,ğ|ğƒ|âˆ’1}
(2)
Having the dictionary, one can measure the distance of each possible image patch to each centroid. We define this process as a convolution operation that replaces the dot product with the Euclidean distance and denote it by âˆ—euclidean. With that said, we can denote the set of extracted features by specifying the characteristic function:

ğœ‡ret(ğ‘˜,ğ¯)=ğ›©[ğ‘‡whatâˆ’ğ‘ƒ(ğ¯)âˆ—euclideanğğ¤(ğ¯)]
(3)
where ğ‘‡what specifies a tolerance threshold for inexact matching of the features and the subscript â€œretâ€ is meant to denote that positions are vectors on the retinotopic fixed coordinate system. It is worth noting that this threshold can depend on the obtained Euclidean distances, thus making the extraction of a feature depend not only on how similar a given patch is to an element of the dictionary, but also on the extracted neighboring patches. This behavior emulates the winner-takes-all mechanism [4] and is also described in biologically plausible work as lateral inhibition.

From the retinotopic set, we build the invariant â€œWhat-Whereâ€ set by taking advantage of the objectâ€™s center

ğ‚ğğ§=âˆ‘ğ¯âˆˆğ‘‰ğ¯maxğ‘˜ğœ‡ret(ğ‘˜,ğ¯)âˆ‘ğ¯âˆˆğ‘‰maxğ‘˜ğœ‡ret(ğ‘˜,ğ¯)
(4)
and radius

ğ‘…ğ‘ğ‘‘=maxğ¯âˆˆğ‘‰,ğ‘˜âˆˆ{0,â€¦,|ğ·|âˆ’1}â€–ğ¯âˆ’ğ‚ğğ§â€–2ğœ‡ret(ğ‘˜,ğ¯)
(5)
to map all feature positions to an object-dependent coordinate system that is normalized for size and translation:

ğœ‡obj(ğ‘˜,ğ¯)=ğœ‡ret(ğ‘˜,ğ¯ğ‘…ğ‘ğ‘‘+ğ‚ğğ§)
(6)
This process is illustrated in Fig. 1 for a representative of the handwritten digit zero category and an input pattern that should be recognized as belonging to that category.

Fig. 1
figure 1
The representative of class â€œhandwritten zeroâ€ (blue pattern) and the input pattern (red pattern) go through a feature extraction step, where previously learned features of a dictionary are pinpointed in fixed two-dimensional retinotopic coordinate system. From this space, we compute the center and the radius of both patterns to map each feature position to an object-dependent referential that approaches invariance to size and translation

Full size image
To perform classification, the original work defined a similarity measure between two sets and applied nearest neighbor. More specifically, given a representative of a class ğ‘†ğ‘¥ and a test pattern ğ‘†ğ‘¦, we define similarity as the number of observed features in the input that are accurately explained by the representative:

sim(ğ‘ ğ‘¥,ğ‘ ğ‘¦;ğ‘‡where)=âˆ‘(ğ‘˜,ğ¯ğ²)âˆˆğ‘†ğ‘¦ğ›©[ğ‘‡whereâˆ’min(ğ‘˜,ğ¯ğ±)âˆˆğ‘†ğ‘¥{â€–â€–ğ¯ğ²âˆ’ğ¯ğ±â€–â€–2ğ•€[ğ‘˜ğ‘¦=ğ‘˜ğ‘¥]}]
(7)
where ğ‘‡where works analogously to ğ‘‡what in that it allows for inexact matching but for positions. Note that, the â€œwhatsâ€ are matched exactly through an indicator function since tolerance was already achieved at feature extraction time. This inexact matching of feature positions is illustrated in Fig. 2.

Fig. 2
figure 2
Each one of the representativeâ€™s features induces a tolerance region of radius ğ‘‡where. Afterwards, similarity is equal to the number of input features that fall inside on of these regions

Full size image
The lack of symmetry in the proposed measure is consistent with research into human cognition [26].

All in all the proposed model achieved the intended steep learning curve on two typical visual pattern recognition datasets of handwritten digits (MNIST and ETL-1). More specifically [22]:

Using small training data sets of around 200 examples, it achieved less than 2% error on ETL-1 and less than 5% on MNIST.

Using a bit larger datasets of between 10 and 20%, the model scored close to perfect accuracy on independent test sets.

From this description of the previously proposed model the coding strategy we propose in the next section will, hopefully, become apparent.

From sets to sparse vectors
As it was described in the previous section, the similarity between two sets requires an enumeration of both of them. This computation is somewhat time-consuming with a quadratic complexity of îˆ»(|ğ‘†ğ‘¥|âˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£) (where, once again, ğ‘†ğ‘¥ is the representative set and ğ‘†ğ‘¦ is the input set). This fact can become a problem specially because computing similarity is the cornerstone of the employed classification mechanism.

In this section, we will argue for a strategy that allows to code these sets as pointer coding representations of sparse binary vectors. The coding process requires a discretization of continuous two-dimensional space, so it will come at a cost of precision. However, we hypothesize that we can increase precision up to a point where the practical differences are insignificant. Furthermore, and back to the original problem, working with sparse vectors allows us to reduce the complexity of similarity computation to that of a dot product between sparse vectors. Having these vectors pointer-coded, the dot product is extremely fast as it requires only one pass through the number of active bits in one of the vectors [10].

Besides achieving advantages in speed while keeping the disadvantages under control, describing â€œWhat-Whereâ€ sets as sparse distributed representations approximates the model to well-known biological constraints [1, 2, 19] and provides a theoretical framework that helps to justify the modelâ€™s good results [22].

Grid encoding
Let us start with the aforementioned coding strategy. Departing from the bottom of Fig. 1 where occurrences of all kinds (â€œwhatsâ€) of features are marked in a two-dimensional space (â€œwheresâ€), we can discretize this space by placing a ğ‘„Ã—ğ‘„ grid over each one of the |ğ·|âˆ’1 spaces. Each position of each grid will correspond to a dimension in the output vector. In general, if a given grid position contains the occurrence of a feature then its corresponding dimension will hold a 1. Otherwise it will contain a 0. We denote this procedure as the function grid(ğ‘†;ğ‘„):îˆ¿â†’ğ‘„Ã—ğ‘„Ã—|ğ·|, where îˆ¿ is the set of all possible â€œWhat-Whereâ€ sets. This process is easier to understand through an illustration. To that end, we provide Fig. 3.

Fig. 3
figure 3
A ğ‘„Ã—ğ‘„ gird is placed on top of the continuous space to perform discretization

Full size image
Keeping up with our previous notation, what the Figure illustrates is the conversion of the representative and inputâ€™s â€œWhat-Whereâ€ sets to their respective sparse representations:

ğ‘¢ğ‘¥=grid(ğ‘†ğ‘¥;ğ‘„)
(8)
ğ‘¢ğ‘¦=grid(ğ‘†ğ‘¦;ğ‘„)
(9)
Inexact matching of â€œWhereâ€
Instead of sets, we now have vectors. However, there is still no clear similarity measure between them that emulates the tolerance regions induced by ğ‘‡where (recall Fig. 2). One further step is necessary to achieve this. For training representatives specifically, a grid position will not only be active for feature occurrences, but also for all positions inside the neighborhoods around those features that are induced by the ğ‘‡where threshold. Mathematically, this can be approximated by convolving the tensor that is constituted by the grids with a binary kernel that approximates the tolerance regions. Eq. 10 defines this kernel.

ğ‘Š(ğ¯;ğ‘‡where)=ğ›©[ğ‘‡whereâˆ’â€–ğ¯â€–2]
(10)
With that in mind, we update the representative as follows.

ğ‘¢â€²ğ‘¥=ğ›©[ğ‘¢ğ‘¥âˆ—ğ‘Š]=ğ›©[grid(ğ‘†ğ‘¥;ğ‘„)âˆ—ğ‘Š]
(11)
Having defined the key steps, we write Eq. 12 to define the new way to compute our similarity measure.

sim(ğ‘¢â€²ğ‘¥,ğ‘¢ğ‘¦)=ğ‘¢â€²ğ‘¥â‹…ğ‘¢ğ‘¦
(12)
We sum up the aforementioned steps in Fig. 4.

Fig. 4
figure 4
For each pattern, the set of grids becomes a sparse vector where each grid position is a dimension that has a 1 if a feature is present. Afterwards, the representative is convolved with a kernel that is defined by the tolerance radius ğ‘‡where. Finally, similarity is computed with a simple dot product between two vectors

Full size image
Why sparse representations?
Having understood how we can generate sparse representations from â€œWhat-Whereâ€ sets we can focus on the advantages that they imply. In general, three aspects are key.

Foremost, as we described before, if for each â€œWhat-Whereâ€ vector we represent only the dimensions with active bits, a technique which is commonly used with sparse data, the similarity computation will go from a quadratic complexity of îˆ»(|ğ‘†ğ‘¥|âˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£) to a linear one of îˆ»(âˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£).

Secondly, under the theory of sparse distributed representations [10, 15] we can begin to understand the modelâ€™s high performance. Taking into account the high levels of sparseness, we can assume that the probability that two vectors share active bits by accident is extremely low. More specifically, for a sparse representation of a representative ğ‘¢ğ‘¥, with dimensionality ğ‘„2|ğ·|Ã—1 and |ğ‘†ğ‘¥| active bits, Eq. 13 counts the number of possible input vectors ğ‘¢ğ‘¦ that overlap with the representative on b bits.

#overlapping(ğ‘¢ğ‘¥,ğ‘¢ğ‘¦;ğ‘)=(|ğ‘†ğ‘¥|ğ‘)(ğ‘„2|ğ·|âˆ’|ğ‘†ğ‘¥|âˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£âˆ’ğ‘)
(13)
Now, for a recognition threshold ğœƒâˆˆ[0,1] that defines the fraction of overlapping bits that is required for a match to occur, we can compute the probability of a false positive using Eq. 14.

ğ‘“ğ‘(ğ‘¢ğ‘¥,ğ‘¢ğ‘¦;ğœƒ)=âˆ‘âˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£ğ‘=âŒˆğœƒâˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£âŒ‰#overlapping(ğ‘¢ğ‘¥,ğ‘¢ğ‘¦;ğ‘)(ğ‘„2|ğ·|âˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£)
(14)
This reasoning provides the intuition behind the classification robustness of the original model. For a good representative of a class, the probability of a false positive at classification time is rather low. Concretely, on a MNIST dataset quick experiment in the digit â€œoneâ€ class, with the following quantities:

ğ‘„=41

Average âˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£=88

Average |ğ‘†ğ‘¥|=1842

Average overlap with nearest neighbor 86, which implies a threshold ğœƒ=8688

the false positive probability as defined in Eq. 14 is on the order of 10âˆ’121.

Finally, it is commonly accepted among neuroscience that populations of neurons in the brain exhibit sparse firing [1, 2, 19]. Focusing on the typical abstraction of population coding, it is commonplace to say that the brain works with these sparse representations. For this reason, we find it specially interesting to propose an adjustment to the model that not only brings other advantages, but also allows it to be under these biological constraints.

Experiments
Arguably, the most important practical advantage that was described in the previous section is the potential speed-up. For that reason, we dedicate a subsection to analyze the experiments we performed to validate this idea. However, even the best speed-up in the world is only useful if we can maintain the performance of the original version. So, we dedicate an experiment to measuring how different the two versions of the model really are. Let us start with that.

Reproducing set-based results
Taking into account that the feature extraction part was left unchanged by our proposal the only parameters that can cause differences in the two versions of the model are the size of the discretization grid Q and the positional tolerance threshold ğ‘‡where.

For that reason, we put together an experiment that, for different values of these parameters, runs the two models in parallel on a randomly chosen set of MNISTFootnote1 images. For each possible pair of images, we treat one as the representative and the other as the input and measure similarity using both models. Then, we normalize both values and report the absolute difference. Figure 5 reports, for several thresholds, how Q affects the difference. As it could be expected, the larger the Q the more precise the discretization is and, thus, the smaller the difference. Moreover, we observe from the error bars that even relatively small values of Q yield statistical insignificant differences between both models.

Fig. 5
figure 5
The absolute difference in the fraction of recognized input features by both versions of the model is plotted against increasing levels of Q. As it was expected, the larger the Q, the more precise is the discretization and, thus, the smaller the difference between the set-based version and the vector-based version

Full size image
Having showed that the new proposal approximates the original version well, we can now focus on measuring speed gains.

Temporal speed-up
To validate the temporal complexity reduction we conduct an experiment where the two versions of the model compute similarity values between randomly generated pairs of â€œWhat-Whereâ€ sets with increasing numbers of features |ğ‘†ğ‘¥| and âˆ£âˆ£ğ‘†ğ‘¦âˆ£âˆ£. While on the original version the pairs are represented as normal sets, on the proposed version the pairs are represented as pointer-coded sparse vectors.

As the original work was applied to the MNIST dataset where images are of 28Ã—28=784, we restrict the possible numbers of features to an integer between 1 and 784. Having said that, for each run, we generate a pair for each possible feature size, where both elements have an equal number of features, and apply both models. Measuring all comparison times across 30 runs, we can build two curves (see Fig. 6) where the mean (and standard deviation) time taken to compute similarity is plotted against the size of the sets.

Fig. 6
figure 6
Across 30 runs, we measure the mean (and standard deviation) time it takes to compute similarity, for both versions, between patterns with increasing number of extracted features. As it was expected the speed-up from the original version to the proposed one is clear

Full size image
Analyzing both curves, we can see that, as expected, set comparisons are much more time-consuming than their sparse vector counterparts. Namely, while the former looks like the right part of a parabola, the latter seems like a small sloped line (almost constant).

Conclusions
Although modern deep learning approaches have achieved astounding results in most visual pattern recognition tasks, they do it using large datasets of labeled data. Besides the fact that, in many applications, such labels are costly to obtain, the need for them is not observed in a biologically intelligent machine like the human brain.

â€œWhat-Whereâ€ sets were proposed as a way to represent visual patterns in a manipulatable manner, where two-dimensional geometric transformations can be exploited to increase invariance, and thus reduce the need for large amounts of training data.

However, the cornerstone of classification using these sets is similarity measure that implicates a time-consuming computation.

The main reason behind this is the unstructured nature of sets, which requires analyzing each possible pair of elements between two sets to measure their similarity.

Using a grid, we can discretize a group of continuous two-dimensional sets and code the sets as sparse binary vectors.

The advantages behind these sparse distributed representations are threefold. First, leveraging pointer-coding of active bits, we can reduce the time complexity of the similarity computation from quadratic to linear in the number of elements of the smaller set being compared. Second, using the fact that in two big sparse vectors the probability of common active bits is rather low, we can bring about a theoretical framework that helps justify the classification robustness exhibited in the original work. Third, sparse distributed representations are the brainâ€™s tool to represent information, so, we managed to bring the model under widely accepted biological constraints.

Keywords
Brain inspired architectures
Sparse distributed representations
â€œWhat-Whereâ€ sets
Steep learning curves