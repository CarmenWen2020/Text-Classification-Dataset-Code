This study aimed at improving the performance of classifiers when trained to identify signatures of unknown attacks. Furthermore, this paper addresses the following objectives: (1) To establish and examine most commonly used classifiers in the implementation of IDSs (KNN and Bayes); (2) To evaluate the performance of the individual classifiers independently; and (3) To model a hybrid classifier based on the strengths of the two classifiers. This study adopted a quantitative methodology of collecting and interpreting data. The study had used the NSL-KDD and the original KDD 1999 datasets. This paper evaluated the devised mechanisms over virtualised networked environments and traffic workloads. SVM was used for detecting cycle numbers whereas coefficients and signal shifts were used for completing period detection. Also, this paper has presented rare data for detecting anomalies. Anticipated events that have not occurred and unanticipated events can be detected at various sampling frequencies based on a hybrid approach since no one has proposed a hybrid approach for detecting anomalies. This paper has ranked features from a network traffic database based on a combination of feature selection wrappers and filers and determined that 16 features showed a strong contribution to the anomaly detection task.

Introduction
Anomaly detection is considered as the problem to find patterns in data that do not imitate anticipated behavior. These non-imitating patterns are usually classified as anomalies, discordant observations, aberrations, peculiarities, exceptions, surprises, outliers, or contaminants in various application domains [1]. Anomaly detection has been mostly classified with outliers and anomalies and; thus, can be found in various industrial realms including cyber security, safety‚Äìcritical, insurance or health care, and military surveillance. The aim of anomaly detection is to detect abnormal patterns opposing from the rest of the data which is referred to as anomalies or outliers. The high dimensions make it difficult to detect anomaly because as there is an increase in the number of features, the data needed to generalize accurately also increases, which results in the sparsity of data due to which data points are more isolated and scattered. The sparsity occurs due to unnecessary variables or the high noise level of a number of irrelevant attributes that cover the true anomalies. It is a hurdle for a number of anomaly detection techniques, which address high dimensionality that fails to hold the efficiency of conventional approaches like distance-based, density-based, and clustering-based techniques.[2].

Network traffic anomalies are rate, and substantial modifications are observed in the traffic of a network. For instance, it encompasses both legitimate activities, including transient changes in the flash crowds and customer demand, and illegitimate activities including port scans, viruses and worms, and DDoS ports [3]. Currently, an important role is being played by networks in social and economic infrastructures. Network security becomes essential, and network traffic anomaly detection comprises an essential aspect of network security [4].

There are no better survey or tutorial papers on the subject of network anomaly detection, specifically using the hybrid approach despite its manifest significance. It is assumed that this study will remove this lacking by offering a model on network traffic anomaly detection. Network anomaly detection will become more important in the prospect events, and the information about it will become more beneficial. Signature and non-signature-based anomalies can be vividly found in IP addresses. The issue of prior approaches was that it requires the signature to be predefined, and it cannot identify new anomalies. Furthermore, anomalies are exposed through different channels including byte and flow metrics, downflow data by packet, SNMP to IP flow data, and variations in average size of packets.

Data, in an anomaly, are exposed when leaning forward toward its source. However, in the presence of mitigated traffic aggregation, there are consequently additional paradoxes in the ambient traffic for isolating the anomaly signal. This paper emphasizes anomaly detection exclusively. The determinant characteristics for anomaly detection are not important the same as the attributes considered to identify the type of anomaly. Activities of a certain system are inspected through intrusion detection system for suspicious behavior patterns, leading toward system misuse or attack. The information collated is inspected in anomaly detection and then compared with normal baseline service as it has the competency for detecting unidentified network attacks. Misuse detection is appropriate for making comparison in the presence of attack database signature. There exists a low false-positive rate within the use of misuse detection, but it cannot detect novel unidentified attacks. Thus, there exists the challenge of categorizing new kinds of traffic in a network as either ‚Äúgood‚Äù or ‚Äúbad‚Äù.

This calls for a near-perfect classifier that will be able to take up the challenge so as to minimize on false positives and false negatives. Researchers are venturing into this area and good results are being registered. But still, as mentioned earlier, a near-perfect classifier is required and we are not yet there. Therefore, this study aims at improving the performance of classifiers when trained to identify signatures of unknown attacks. Furthermore, this paper addresses the following objectives: (1) To establish and examine most commonly used classifiers in the implementation of IDSs (KNN and Bayes); (2) To evaluate the performance of the individual classifiers independently; and (3) To model a hybrid classifier based on the strengths of the two classifiers.

This research‚Äôs overarching objective is to design a network-wide anomaly detection framework that will be able to operate on (and integrate) partial data, work in short timescales, and detect previously unseen anomalies. The work will evaluate the devised mechanisms over virtualised networked environments and traffic workloads. Recent advances such as software-defined networks (SDN) and network function virtualisation (NFV) enable dynamic configuration and routing of flows throughout a network without physical layout or wiring changes. This new paradigm presents a potentially feasible platform for distributed machine analysis and handling of anomalous traffic.

This paper has rigorously devised and performed the experiments undertaking the recent arguments and the disorder classified to an appreciable aspect of the related literature. These arguments compare previous approaches, discuss the causes behind errors and drifts, and install precautions for addressing such drawbacks. Lastly, this paper showed that benchmarks relied on the selected subsets and predefined classification models.

Related work
Current techniques for statistical, network-wide anomaly detection are offline and static, relying on the classical machine learning paradigm of collecting a corpus of training data to train the system [4]. Thus, there is no ability to adapt to changing network and traffic characteristics without collecting a new corpus and re-training the system [5]. Assumptions as to the characteristics of the data are crude: assuming measured features are independent through a Na√Øve Bayes classifier, or that projections that maximize the variance within the features (PCA) will naturally reveal anomalies [6]. Moreover, there is currently no framework for profiling the evolving normal behavior of networked infrastructures and identifying anomalies as deviations from such normality. A comprehensive survey has compared a significant number of network IDS. Anomalies occur in the presence of malfunctioning devices, network attacks and intrusions, and network overloads. An anomaly is defined as any condition that makes network traffic vary from normal behavior [7].

Network traffic anomalies are significant and unusual modifications at the traffic level of a network. Internet security has been jeopardized by intrusions such as zombie networks and DDoS attacks, whereas malfunctions and network jams have an unsatisfactory effect on service quality [8]. Thereby, it is important for detecting and locating network anomalies for both end-users and network operators. It is a stimulating task for detecting and locating them because an individual must extract and explain anomalous trends of noisy background traffic data [9].

There are a massive number of researches on anomaly detection. A computing system's internals is monitored and analyzed through a host-based anomaly detection system to integrate data mining of the audit records and system logs [10]. A previous study [11] has integrated a single-variable time series to measure data performance on the basis of end-to-end round-trip time and packet loss probability. However, machine learning and signal analysis can be used for detecting network anomalies on traffic measurements. These approaches have their limitations as they only emphasize on the information and limited detected realm [12]. The majority of the network anomalies usually portray strong network-wide characteristics when the network's scale expands and the data transfer rate speeds up. Their effect might usually spread to different connections, whereas their local attributes might not be that apparent [13]. It is complicated in conducting network-wide analysis with the methods above, and thus their accuracy cannot be guaranteed.

Lakhina, Crovella & Diot [14] previously came up with network-wide anomaly detection based on subspace construction through PCA. This approach integrates network traffic of most origin‚Äìdestination (OD) flows for establishing a model of normal behavior in detecting anomalies by estimating deviations from that model. Similarly, [15] has enriched network-wide anomaly detection by conducting real-time and robust processing, which allows them to use whole-network traffic data over a single point. Moreover, [16] has implemented the similar model to facilitate in the development of anomalous feature library. Thereby, it can be integrated for detecting identified anomalies and unknown anomalies, and it can be widely used.

Detection performance is enhanced through network-wide anomaly detection by instigating wider and multidimensional network information, but it further experiences some real issues when integrated into high-speed and large-scale backbone networks [17]. Initially, it might not be applicable if gathered data were lost throughout the collection or transfer process due to its additional collection equipment, faster network speed, and wider collection area. Secondly, traffic flows continue to expand in complexity and volume in the backbone network [18]. In this regard, the anomaly detection algorithms' performance can be degraded by hidden noise such as hidden noise. However, some of the attacks might even pollute the detectors. Thirdly, when anomalies occur, the anomaly above detection methods can merely reveal, but they still have some defects to locate these anomalies [19].

Flash crowd events were comprised within the first group of the dataset. The low-frequency representation was used to effectively expose flash crowd anomalies in the system. The second group of anomalies comprises network attacks, failures, and other events and were short-lived [20]. These short-lived anomalies were more complicated in exposing data because of their similarity to normal network behavior. Data from the high and mid-frequency levels should be merged to expose these signals [21].

A previous study [22] has conducted different traffic signals to inspect the integration of general wavelet filters for isolating signal attributes via time-bounded representation. The wavelet analysis was tested using several different wavelet systems to traffic signals for determining the effectively exposed characteristics of anomalies witnessed therein. The constraint was accepted that this flow and simple network management protocol (SNMP) data was gathered at five-minute intervals to preclude analysis on finer timescales [21]. Nonetheless, a wavelet system has been selected for developing algorithms that efficiently expose the important characteristics of both strange and ambient traffic. Not surprisingly, the analysis reveals apparent daily and weekly traffic cycles. Such attributes can be exposed by segregating them into two different clusters on the basis of their predefined time period [22, 23].

In time series, anomaly detection is a majorly studied area of machine learning and data science [24]. The majority of the anomaly detection approaches are present, both supervised and unsupervised; hitherto, most of them are for processing data in batches and inappropriate for real-time streaming applications [25]. The robust principal component analysis method and EGADS are the prime examples, both of which need investigating the entire dataset [26, 27]. Similarly, decomposition of the entire time series for generating symbols before anomaly detection is encompassed within symbolic aggregate approximation [28, 29]. These techniques are conventional batch methods, and the emphasis of this article is on methods for online anomaly detection even though they may work effectively in specific conditions.

The partial activation of some of the anomaly detection algorithms either rely on lookahead to flag anomalous data or have an introductory phase of offline learning [30,31,32]. Majority of the clustering-based tactics are marked within such frameworks; for instance, they involve online drift detection algorithm, multi-class learning algorithm for data streams, and distributed matching-based grouping algorithm [33,34,35]. Moreover, self-adaptive and dynamic k-means utilized training data for learning weights before anomaly detection [36]. It has been witnessed that the principle of no lookahead is further violated by kernel-based recursive least squares since it resolves temporarily flagged data exemplars a few time steps after deciding for whether they are anomalous or not [37].

On the contrary, some kernel methods follow the criteria of real-time anomaly detection. Statistical techniques are mostly in practice that is computationally lightweight to stream anomaly detection. These techniques encompass outlier tests, including k-sigma, extreme studentized deviate, and sliding thresholds along with exponential smoothing, changepoint detection, and statistical hypotheses testing [38, 39]. The effectiveness of typicality and eccentricity does not require any user-specific metrics. The usefulness of such techniques in temporal dependencies is restricted in the presence of spatial anomalies [40]. Additional advanced time-series modeling and forecasting models can detect temporal anomalies in complicated conditions [41].

Also, ARIMA is a general-objective technique for modeling temporal data with seasonality [42]. It is efficient to detect anomalies in data with weekly or regular patterns. The automatic determination of seasonality is facilitated through ARIMA extensions for specific applications [43]. Relative entropy can also be used for handling temporal anomalies [40]. The explicitness of domain knowledge is required within model-based approaches as these approaches encompass of cloud data-center temperatures, aircraft engine measurements, and ATM fraud detection [44, 45]. Furthermore, residual error and domain knowledge frameworks are needed within Kalman filtering as they are computationally sound, but are restricted toward general streamlined applications [46, 47].

Proposed model for network anomaly detection
Anomaly detection also referred to as outlier detection is the identification of events, which occur unexpectedly, observations, or items that differ extensively from the norm. It is often applied to the unlabeled data by the data scientists in a procedure called unsupervised detection of anomaly, any type of detection is based on the following two assumptions:

Anomaly in data can occur rarely.

The features of data anomalies are extremely different from the normal instances.

An anomalies data is associated with any sort of problem or a rare event like hacking, bank fraud, structural defects, textual errors, malfunctioning of equipment, and infrastructure failures. This is the reason as to why identifying actual anomalies rather than the false positives or data noise is necessary from the perspective of business [48].

Therefore, this study proposed different techniques are covered in the proposed model for detecting network anomalies of network traffic. An anomaly was considered where no vector was labeled as normal traffic in the database since this paper aims to work with labeled data such as type of attack. Data were collected as previously described in Ahmad et al. [27]. Anomaly detection and identification were differentiated along with concepts that sometimes presume to be integrated into the literature.

Feature extraction
Apparent benefits are brought with the reduction in several features concerning computational resources, including faster processing, easier data extraction, and less stored data. Also, dimensionality reduction profits through data mining and machine learning in additionally complicated approaches. In this regard, feature selection methods were presented along with their advantages for classification and their implications to deal with network anomaly detection databases. The determinant is the type of data for dealing with and deciding the adequate feature selection method. The original dataset comprises mixed types, in the aimed case, some features were nominal such as the protocol type, and the rest of the features belong to numerical ratio scales.

Afterward, the MRMR filter conducts a step forward in WMR. A sequential forward exploration was performed by MRMR by adding features iteratively to the subset by taking into account the following elements: (1) the minimum redundancy concerning the feature subset selected, and (2) a new feature comprising of the maximum relevance about the label. Fisher F-test scores, mutual information measurements, and Pearson‚Äôs correlation were used for computing relevance and redundancy assessments for the WMR case. The quotient of redundancy and relevance ascertained the relevance redundancy association for the parameterization in the conducted tests.

Classification
Degradation in the performance of classifiers is not encompassed for describing a phenomenon from a theoretical perspective. On the contrary, kNN and SVM models are extremely affected by irrelevant and noisy variables. Also, some linear models, such as LAR are very sensitive to noise data and associated variables. A total of three different classification exemplars were used for validation for reducing the bias instigated by particular classifiers and up to five classifications for assessment. Moreover, a LASSO-LAR-based classifier was integrated when utilizing this particular method for feature extraction.

Methods
This study has adopted a quantitative methodology of collecting and interpreting data. This is due to the fact that the study executed a number of simulations and experiments. At first, the simulations were run on the individual classifiers and their performance was assessed independently. The same experiments were done on the new model and the results were compared.

The study had used the NSL-KDD and the original KDD 1999 datasets. Live network traffic, simulations were run on the University private network and other sample virtual networks. The KDD Cup 99 dataset, launched by the Defense Advanced Research Projects Agency (DARPA) was used for differentiating between valid and unauthentic connections at the knowledge discovery and data mining conference for a computer network. Approximately 4900,000 single vectors are comprised within the dataset encompassing 41 attributes and marked within two different categories. If it is classified as an attack, it will accurately describe the type of attack, such as denial-of-service. In order for this dataset to be implemented in an intrusion detection system, these algorithms need training datasets to properly function, due to the ever-changing cyber-attacks.

Because there is a lack of common datasets used for IDS, KDD Cup 99 is widely used. It is critical that organization‚Äôs systems share information, somewhat like the automated information sharing (AIS) with NCCIC or a STIX/TAXII. When data packets are traveling throughout the network, these packets can be misused and prone to attack by threat actors. Networks deal with large amounts of data, and any unauthorized access should raise an alarm to system administrators. The KDD Cup 99 is a standard dataset that will evaluate the accuracy of intrusion detection systems.

Dataset
There have been recent studies on KDD Cup 99 in regards to its significant role in intrusion detection systems. Many organizations will test a product or method before putting it into live production on the network by using their very own testing environment. However, when it comes to intrusion detection systems, it can be very costly for an organization to implement an experimental intrusion detection system or prevention system. Another inherent weakness with KDD Cup 99 is that it uses TCPdump, which will drop a vast amount of traffic packets that can overload the network. It is critical that an organization implements this dataset into their current intrusion detection or prevention system and tests before going into production. However, it is believed this type of dataset is not a one size fits all approach and may be challenging to implement into a current environment.

Inspection can be conducted against data representatives because of different attacks, networks, and traffic profiles. Thereby, it becomes difficult to explore adequate, labeled datasets. Iglesias & Zseby [49] have proposed NSL-KDD dataset for resolving the massive number of redundant records along with the duplicated records. In this regard, Figs. 1, 2, 3 showed the occurrence of labeled risks within the dataset.

Fig. 1
figure 1
Data implementation

Full size image
Fig. 2
figure 2
Selection of discrete filter

Full size image
Fig. 3
figure 3
Application of filter

Full size image
Experimental analysis and results
Experiments have been conducted with RapidMinerFootnote1 or MATLABFootnote2 On a machine with the following attributes: Windows 7‚Äì64 bits; 8 Gb RAM, Intel¬Æ Core ‚Ñ¢ i5-2405 s 2.50 GHz. This paper emphasized the investigation and selection of anomaly detection attribute to postpone anomaly identification to corresponding work. Thereby, there were two likely labels for attack or normal vector [50].

Every feature's contribution was weighted by filers based on its inherent metrics and indices for the anomaly identification task. Filter weights were normalized based on the maximum and minimum values and reclassified features in ascending order. Afterward, nominal classifications were visually stated based on pronounced modifications in the weight slops acquired by each filer, such as SAM and WMR cases. The validation of the final subset extractions is presented in Table 1. Some surprising elements can be inferred by comparing performance outcomes:

The high scores acquired by the extracted classifiers confirmed their appropriateness for feature extraction validation, irrespective of the feature subset.

The removal of negligible features enhanced the classification of up to 30 features.

The performance degradation was substantially low from 30 to 16 features; therefore, the contribution for anomaly detection of the rejected features is too low (Fig. 4).

Table 1 Confusion matrix (5-fold test)
Full size table
Fig. 4
figure 4
Dataset

Full size image
5-Fold test
Correctly Classified: 13439+11692=25131

Incorrectly Classified: 51+10=61

Accuracy=2513125131+61=99.7579%

Detection Rate:ùê∑ùëÖ=1343913439+11692=53.4757%

False Alarm Rate:ùêπùê¥ùëÖ=1011692+10=0.0854

Mathew Correlation Coefficient:ùëÄùê∂ùê∂=99.5139%

Precision Rate (Normal):1343913439+10=99.9256%

Precision Rate (Anomaly):1169211692+51=99.5656%

Precision Rate (Avg):99.7456\% 

Recall Rate (Normal):1343913439+51=99.6219%

Recall Rate (Anomaly):1169211692+10=99.9145%

Recall Rate (Avg):99.7682%

F-Measure (Avg):2‚àó99.7458‚àó99.768299.7458+99.7682=99.7659%

Removing the less authentic Bayes classifier (Table 2), feature subsets can be compared based on the provided indices (Table 3). Table 3 provides such a comparison. Accuracy and AUC are global evaluators, while a better recall performance is often undertaken for anomaly detection compared to precision. Precisely, acquired consequences recommend that classifiers act in less noisy space, are simpler, and enhance criteria and mitigate randomness in classification. However, there is minimal information loss that can be beneficial for detecting particular attacks (Fig. 5).

Table 2 Validation of subsets after stability selection and 5-fold cross-validation
Full size table
Table 3 Validation of significant subsets after 5-fold cross-validation
Full size table
Fig. 5
figure 5
5-fold cross-validation

Full size image
Findings have indicated that the original dataset comprises irrelevant and redundant features. Some tests were performed for validating this statement, such as evaluating classifiers acting with the extracted 41, 30, or 16 features and comparing outcomes (Table 3).

10-Fold test
Correctly Classified: 13436+11697=25133

Incorrectly Classified: 46 +13=59

Accuracy=2513325133+59=99.7658%

Detection Rate: ùê∑ùëÖ=1343613436+11697=53.4595%

False Alarm Rate:ùêπùê¥ùëÖ=1311697+13=0.1110

Mathew Correlation Coefficient: ùëÄùê∂ùê∂=99.5297%

Precision Rate (Normal):1343613436+13=99.9033%

Precision Rate (Anomaly):1169711697+46=99.6082%

Precision Rate (Avg):99.7558%

Recall Rate (Normal):1343613436+46=99.6588%

Recall Rate (Anomaly):1169711697+13=99.8889%

Recall Rate (Avg):99.7388%

F-Measure (Avg):2‚àó99.7388‚àó99.758899.7388+99.7588=99.7502%

15-Fold test
Correctly Classified: 13434+11695=25129

Incorrectly Classified: 48 +15=63

Accuracy=2512925129+63=99.7499%

Detection Rate: ùê∑ùëÖ=1343413434+11695=53.4601%

False Alarm Rate:ùêπùê¥ùëÖ=1511695+15=0.1280

Mathew Correlation Coefficient: ùëÄùê∂ùê∂=99.4977%

Precision Rate (Normal):1343413434+15=99.8884%

Precision Rate (Anomaly):1169511695+48=99.5912%

Precision Rate (Avg):99.7398%

Recall Rate (Normal):1343413434+48=99.6439%

Recall Rate (Anomaly):1169511695+15=99.8719%

Recall Rate (Avg):99.7579%

F-Measure (Avg):2‚àó99.7398‚àó99.757999.7398+99.7579=99.7488%

False positives were mitigated with 16 features but lost power for determining some anomalies. It does not induce that missing risks were differentiated with 41 features. Performance indices recommend that the Na√Øve Bayes classifier was casually segregating the input space and thus incompetent for diving at an in-depth level for classifying the data (Table 4). SVM was optimal for binary issues that encompass high dimensional input spaces and several samples. Especially sensitive samples were shown by SVM for modifications in the parametrization, which request a new adjustment for every different subset extraction (Fig. 6).

Table 4 Confusion matrix (10-fold test)
Full size table
Fig. 6
figure 6
10-fold cross-validation

Full size image
As the table shows, each test has a very high percentage in each test value (almost 100%), making random forest a very good choice in this case. The 10-fold test held the best result in class correctly classified, accuracy, MCC, and precision rate (Table 5). In this case, the 10-fold test yields the best result, but we cannot confirm that 10-fold will work the best because there will be many different cases and different datasets; it works the best in this case (Table 6). Table 7 presents parameter optimization with minimum number of characteristics, which significantly increase the number of dimensions and feature sparsity (Fig. 7).

Table 5 Confusion matrix (15-fold test)
Full size table
Table 6 Comparison between 5, 10, and 15-fold test classification test
Full size table
Table 7 Evaluation of significant subsets with the original train or test division
Full size table
Fig. 7
figure 7
15-fold cross-validation

Full size image
In recent years, denial-of-service attacks have greatly diminished, but they have become more advanced. It is imperative that organizations have proper security measures in place and secure ports to mitigate these attacks. Previous studies discussed the most common types of DoS attacks on the internet and using an algorithm known as CAPTCHA. CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart. One type of denial-of-service attack that CAPTCHA can help prevent is flood attacks. Cybercriminals will try to overwhelm the server with many fake requests so that a mass amount of traffic is generated. The authors explain CAPTCHA as a program that protects websites against automated software and bots/botnets. Think of CAPTCHA as a form of artificial intelligence; it is separating humans and computers. Per the article, CAPTCHAs tests must be able to fulfill three basic properties. These tests must be made for humans to easily pass, easy for a tester machine to generate and score, and difficult for a robot to pass. The authors further explain their proposed concept of CAPTCHA and how it mitigates types of denial-of-service attacks as well as its benefits.

Conclusions and future work
This paper has proposed a hybrid model for real-time adaptive network intrusion detection for streaming data. The model has ranked features from a network traffic database based on a combination of feature selection wrappers and filers and determined that 16 features showed a strong contribution to the anomaly detection task. The experiments indicated irrelevant features and a high redundancy among some features and inter-dependencies that refer to a significant group of anomalies.

Future studies should focus on implementing wavelet transform coefficients for the enhancement of spectral densities. This approach can be effective in mitigating the variance of spectral power estimation for different noises. The enhancement of speech signal can be used from the consistent estimation of the spectral subtraction method. By considering this approach, the power estimation of the spectral signal of speech recognition can be mitigated through the lower variance of power spectral density. The by-product of the current spectral subtraction method can be effective in exploring the speech signal of the spectrum. Furthermore, single-channel methods of speech improvement can be used to threshold the wavelet transforms coefficients and the anticipation of power spectral density.

Keywords
Anomalies
Na√Øve bayes
Network intrusion
Traffic data
SVM