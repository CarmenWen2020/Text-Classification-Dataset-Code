Both academia and industry have directed tremendous interest toward the combination of Cyber Physical
Systems and Cloud Computing, which enables a new breed of applications and services. However, due to the
relative long distance between remote cloud and end nodes, Cloud Computing cannot provide effective and
direct management for end nodes, which leads to security vulnerabilities. In this article, we first propose a
novel trust evaluation mechanism using crowdsourcing and Intelligent Mobile Edge Computing. The mobile
edge users with relatively strong computation and storage ability are exploited to provide direct management
for end nodes. Through close access to end nodes, mobile edge users can obtain various information of the
end nodes and determine whether the node is trustworthy. Then, two incentive mechanisms, i.e., Trustworthy
Incentive and Quality-Aware Trustworthy Incentive Mechanisms, are proposed for motivating mobile edge
users to conduct trust evaluation. The first one aims to motivate edge users to upload their real information
about their capability and costs. The purpose of the second one is to motivate edge users to make trustworthy
effort to conduct tasks and report results. Detailed theoretical analysis demonstrates the validity of QualityAware Trustworthy Incentive Mechanism from data trustfulness, effort trustfulness, and quality trustfulness,
respectively. Extensive experiments are carried out to validate the proposed trust evaluation and incentive
mechanisms. The results corroborate that the proposed mechanisms can efficiently stimulate mobile edge
users to perform evaluation task and improve the accuracy of trust evaluation.
CCS Concepts: • Networks → Network economics; Mobile and wireless security; Sensor networks;
Network performance analysis; Network reliability; Mobile networks;
Additional Key Words and Phrases: Crowdsourcing, mobile edge computing, artificial intelligence, trust
evaluation
1 INTRODUCTION
Cyber Physical and Cloud Systems (CPCS) has become increasingly popular with many applications, including industrial process control, video surveillance, structural health monitoring, and
smart city [2, 8, 20, 35]. CPCS consists of numerous devices and sensors that can be endowed with
different levels/forms of intelligence and even capable of thinking [10, 15, 33]. During this process,
identifying untrustworthy nodes is critical for the safety of CPCS. On the one hand, the growing communication capability and increasing use of end devices connect users, remote cloud, and
other CPCS more closely [13]. On the other hand, it makes CPCS naturally vulnerable to network
failures and malicious interference [16, 22, 31]. One compromised or malicious node in CPCS can
result in disruption, damage, or even loss of life. For example, considering highway vehicular platooning in Vehicular Ad Hoc network, malicious and badly behaved platoon head vehicles may
imperil the system by transferring low-quality messages or providing fake information to put user
vehicles in danger [32].
To cope with the problem, researchers consider applying trust evaluation mechanism in CPCS
[3, 21]. Trust evaluation mechanism is an Artificial Intelligence (AI) technology that enables machines with awareness, reasoning, and learning. For instance, in cognitive radio systems, the term
“awareness” refers to the process of extracting meaningful information from surrounding environment [34]. Reasoning is defined as finding an appropriate action using information provided
by awareness to achieve a particular goal, and learning is the process of accumulating knowledge
according to the observed impacts based on the applied action [26]. In trust evaluation, which is
viewed as a part of AI, awareness can be defined as the process of collecting information about
nodes [14, 27]. Reasoning refers to the step that acquires trust value of a particular node according
to the collected data. Learning causes the changes in communication behavior based on the results
of reasoning.
The existing trust evaluation mechanism is mainly oriented to sensor networks, which can be
classified as centralized and decentralized [6, 7]. In centralized trust evaluation mechanism, a computing center evaluates and stores the trust value of the end nodes in whole network [5, 25]. In
decentralized trust evaluation mechanism, end nodes evaluate their neighbors’ trust value and
store the trust value by themselves [28, 36]. However, those two modes cannot be directly applied
to CPCS due to the following reasons: For centralized trust evaluation mechanisms, the poor scalability makes it hard to meet the requirement of self-organization [24, 38]. For distributed trust
evaluation mechanisms, the relative long distance between underlying node networks and cloud
makes it hard for cloud to obtain the fine-grained information of each end node [9, 29, 30]. Therefore, designing a scalable mechanism that can provide detailed and real-time trust value for end
nodes remains a challenge.
To solve the challenge, in this article, we exploited Mobile Edge Users (MEUs), e.g., smartphones,
laptops, smart watches, and so on, with relative strong computing and storage ability to participate
in trust evaluation by directly accessing end nodes and obtaining their overall trust information.
Our Mobile Crowdsourcing (MCS)-based mechanism aims to provide fine-grained and detailed
trust evaluation for each end node and allows the remote cloud center to get real-time trust information. Meanwhile, the proposed incentive mechanism motivates MEUs to accurately upload
relevant information. To the best of our knowledge, there is currently no research considering
recruiting MEUs to conduct trust evaluation for the end nodes.
In summary, we first introduce MCS-based trust evaluation mechanism, including incentive
mechanism, task assignment, trust evaluation, and result aggregation. Then, we design a Trustworthy Incentive Mechanism to stimulate MEUs to honestly upload their cost for conducting the
trust evaluation. Next, we combine both incentive mechanism and data aggregation mechanism
together to ensure the trustfulness of MEUs when they perform the task.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
Crowdsourcing Mechanism for Trust Evaluation in CPCS 62:3
The main contributions of this article are as follows:
• Novel trust evaluation mechanism based on crowdsourcing: To the best of our knowledge,
we are the first to study the combination of trust evaluation and mobile crowdsourcing.
Compared with traditional trust evaluation mechanism, MCS-based trust evaluation owns
the advantages of low cost, better scalability, and finer granularity.
• Hierarchical trust management scheme based on mobile edge node: We consider a scenario
where MEUs visit certain nodes and their neighbors through moving and judge whether
these nodes are malicious or not with various information by reasoning.
• Ecosystem in proposed mechanism: We design two online incentive mechanisms based on
the reverse auction model. The first one aims to ensure the bid trustfulness and individual
rationality, while the second one guarantees the data trustfulness, effort trustfulness, quality
trustfulness, bid trustfulness, and individual rationality.
The remainder of this article is organized as follows: We introduce MCS-based trust evaluation mechanism in Section 2. Section 3 introduces the Trustworthy Incentive Mechanism. The
Quality-Aware Trustworthy Incentive Mechanism is proposed in Section 4. We conduct extensive
experiments in Section 5, and the conclusion is presented in Section 6.
2 MCS-BASED TRUST EVALUATION MECHANISM
In this section, we describe the MCS-based trust evaluation mechanism in detail. The proposed
mechanism consists of four parts, including: trust evaluation mechanism, incentive mechanism,
task assignment, and result aggregation. The first one is conducted by MEUs and the remaining
three are executed by cloud.
2.1 Overview
First, the cloud publishes the task to all available MEUs. The publication includes network topology
of sensor network and location of each sensor node. After receiving the publication, interested
MEUs report their evaluation quality, moving area, and expected price to the cloud. We name the
triple composed of them as bids. Second, according to the bids of each MEU, cloud decides the
winner set and assigns task to each winner. Third, the selected MEUs use their mobile capability
to collect sensor information and reason whether object nodes are malicious or not. Next, based on
the reported results of MEU, cloud executes the result aggregation mechanism to obtain the final
trust evaluation of each sensor. The payment is given to MEUs afterwards. Figure 1 illustrates the
structure of MCS-based trust evaluation mechanism.
2.2 Trust Evaluation Mechanism
The trust evaluation mechanism is the key to ensure the reliability of lower sensor nodes [15].
Here, we propose a hierarchical trust evaluation scheme based on MEUs. We consider the communication behavior, energy status, and data content of sensor nodes as trust evidence [11]. Edge
users reason the trust value using subjective logic method [1]. In our trust evaluation mechanism,
sensor nodes monitor the communication behaviors of their neighbors and upload them to MEUs,
which directly obtain the energy status and data content of target nodes. According to the communication behavior, energy status, and data content, MEUs can reason the trust value of the whole
network. The trust evaluation mechanism is illustrated in Figure 2.
The communication behavior of a sensor node relates to the number of packets sent by it and the
communication success rate. Usually, the greater the interaction between two nodes, the higher
the trust value. However, when the number of packets sent by the sensor node within a certain
time exceeds a threshold, the trust value of the node should be lowered to prevent flooding attack.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
62:4 T. Wang et al.
Fig. 1. The MCS-based trust evaluation mechanism.
Fig. 2. The structure of trust evaluation.
Meanwhile, due to the packet number sent by sensor nodes having correlation [37], we assume
that the packet number sent by the receiver’s neighbor follows the normal distribution. Therefore,
when the packet number sent by a sensor node within a certain time exceeds the threshold, the
difference between that number and the average packet number sent by the receiver’s neighbors
represents the degree of deviation. The trust value of one node to the other on packet number is
defined as:
Nij (Δt) =
⎧⎪
⎨
⎪
⎩
numi j
max(numi ) i, j ∈ N, numij ≤ λμ,
exp
− |numi j−μ |
θ
 i, j ∈ N, numij > λμ, (1)
where the variable numij is the packet number sent by node j to node i, max(numi ) is the maximum
packet number received by node i from its neighbors, μ is the average packet number sent by the
receiver’s neighbors, and λ and θ are the significant factors used to adjust threshold and decline
rate. The equation describes the trust value of node i to node j on packet number in a certain
period of time.
Communication success rate is another important criterion for evaluating whether a sensor
node is trustworthy or not. When a sensor node communicates with the target node, the higher
the success rate of communication, the higher the trust value of the target node and vice versa.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.  
Crowdsourcing Mechanism for Trust Evaluation in CPCS 62:5
According to Reference [36], the subjective logic framework can be used to describe the communication trust. The trust of one node toward another on communication success rate can be described
as:
Sij (Δt) = 2b + u
2 , (2)
where b = s
s+f +1 , u = 1
s+f +1 , s is the number of successful communication, and f is the number of
unsuccessful communication.
After obtaining these two parameters, sensor nodes are able to reason their neighbors’ trust
value. The communication trust of node i to node j is presented by Equation (3):
Cij (Δt) = ω1 × Nij (Δt) + ω2 × Sij (Δt) , (3)
where ω is weight of each part and ω1 + ω2 = 1.
When MEUs visit the sensor node, edge users are able to obtain the overall information about
the node, including comprehensive communication trust, current energy status, and data content.
The equation of the comprehensive communication trust is as follows:
Cj (Δt) = C1j + C2j + ··· + Cij
i , (4)
where C1j,C2j,...,Cij is the evaluation of the 1th, 2th,...,i
th neighbor toward node j, respectively. i is the number of neighbors of node j.
The energy status of a sensor node can be represented from two aspects: residual energy and
energy consumption rate. When residual energy is less than a threshold, the energy trust is zero.
When residual energy is greater than the threshold, MEUs consider the energy consumption rate
to compute the energy trust of the sensor node. Same as the packet number trust, the energy
consumption rate of adjacent sensor nodes is correlated. Thus, the equation of energy trust is as
follows:
Ej (Δt) =
 0 j ∈ N, Eres < Emin,
exp
− |Erate−μe |
δe
 j ∈ N, Eres ≥ Emin, (5)
where Eres is the current residual energy, Emin is the energy threshold, Erate is the energy consumption rate of sensor node within Δt, μe is the average energy consumption rate of the node
and its neighbors, and δe is the variance.
The data quality of sensor nodes reflects their sensing ability. According to Reference [36], the
data trust can be defined as follows:
Dj (Δt) = 2* 
0.5 −
 vd
μd
f (x) dx
= 2
 ∞
vd
f (x) dx, (6)
where f (x) = 1
δ
√
2π e
− (x−μd )
2
2δ 2 , vd is the average data value collected by the node with Δt, μd is the
average data value collected by the node and its neighbor with Δt, and δ is the variance.
Thus, the overall trust value of the evaluated node is as follows:
Tj (Δt) = ω3 × Cj (Δt) + ω4 × Ej (Δt) + ω5 × Dj (Δt) , (7)
where ω3 + ω4 + ω5 = 1.
2.3 Incentive Mechanism
Intuitively, a naive way to provide reward for the selected MEUs is paying the payment according
to their uploaded bids. However, some MEUs may exaggerate their cost to gain additional rewards.
Meanwhile, during the process of task taking, MEUs may not channel effort or lower their evaluation ability. Some MEUs may even report fake result to gain advantage over the cloud [4]. To
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.  
62:6 T. Wang et al.
cope with these problems, we propose two incentive mechanisms; the first one aims to solve the
problem that MEUs exaggerate their costs, while the second one focuses on ensuring the reliability
of MEUs during the process of trust evaluation.
2.4 Task Assignment
The task assignment mechanism decides the trajectory of selected MEUs. Based on the requirement of trust evaluation task, every sensor in network may need to be evaluated several times.
Meanwhile, the moving distance of MEUs is limited and the evaluation capability of each MEU
is different. Thus, designing an effective task assignment mechanism is a worthy consideration.
However, the focus of this article is the incentive mechanism. We will discuss the task assignment
problem as future work.
2.5 Result Aggregation
The data aggregation mechanism aims at obtaining the trust value of each sensor according to
the reported results of MEUs. To ensure the accuracy, one sensor node needs to be evaluated
several times with different MEUs. According to MEUs’ evaluation capability, the cloud conducts
comprehensive calculations to obtain the final evaluation results. Meanwhile, we will consider
to adopt privacy preservation methods such as those in References [19, 23] to protect privacy
information of MEUs in the future.
3 TRUSTWORTHY INCENTIVE MECHANISM
In this section, we design a Trustworthy Incentive Mechanism to attract MEUs to conduct trust
evaluation for sensor nodes in CPCS system. This mechanism is to maximize the overall evaluation
ability of entire selected MEUs and minimize the costs under the conditions of individual rationality (to attract selfish MEUs to conduct the task) and bid trustfulness (to assure selfish MEUs
upload real information about their evaluation ability and cost). The notations through the article
are presented in Table 1.
3.1 Preliminaries
We assume that there are n MEUs who want to participate in evaluation tasks. EU =
{user1,user2,...,usern } is the set of applicants. Q = {q1,q2,...,qn } represents each MEU’s real
evaluation quality, where qi ∈ [0, 1]. Suppose the ground truth of a sensor node is d0, and di is the
result obtained by useri . Here, we define d0 ∈ {−1, 1}, d0 = −1 indicates that the node is malicious
and d0 = 1 represents the node is normal. qi is the possibility that d0 = di , which is donated by
Pr(di = d0). Here, we suppose that evaluation quality is relating to the intrinsic characteristic of
MEUs. Thus, for any MEUs, qi is a constant unrelated to the evaluated sensor. The higher the value
of qi , the greater the evaluation quality of useri . AR = {area1, area2,..., arean } denotes the MEUs’
evaluation area, where areai is a two-tuple consisting of the current location of useri and the radius ri . For simplicity, the evaluation area of MEUs can be represented by a circle. Based on their
location, MEUs are required to evaluate the sensor nodes within the radius. Figure 3 illustrates the
MEU’s evaluation area.
Both evaluation quality and moving range decide MEU’s ability for performing the task. Thus,
the ability of each MEU can be defined as follows:
ai = F (qi,ri ) , (8)
where ai ∈ [0, 1] and for both qi and ri , function F is monotonically increasing:
• if qi = qj ,ri > rj , then F (qi,ri ) > F (qj,rj),
• if qi > qj,ri = rj , then F (qi,ri ) > F (qj,rj).
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
Crowdsourcing Mechanism for Trust Evaluation in CPCS 62:7
Table 1. Notations
Notation Meaning
EU The set of MEUs
qi The real evaluation quality of useri
areai The evaluation area of useri
AR The set of evaluation area
ri The moving radius of useri
ai The evaluation ability of useri
bidi The bidding information of useri
bi The bidding price of useri
pi The payment of useri
ci The cost of useri
ui The utility of useri
Ucloud The utility of the cloud
ei The effort of useri
ni Sensor node
ˆdi The evaluation results of useri
di,j The evaluation result for useri to node nj
di The evaluation result for useri to a particular node
Fig. 3. Evaluation area of MEU.
When MEUs bid, they are required to submit the bid bidi = (qi, areai,bi ), which is a triple containing the MEU’s evaluation quality, evaluation area, and the bidding prices for executing the
task.
From the perspective of cloud, P = {p1,p2,...,pn } is the set of payment clouds offering to each
MEU. When an MEU is not selected to conduct the task, pi = 0. Thus, we can define the utility of
an MEU as follows:
ui = pi (ai,bi ) − ci, (9)
where ci represents the costs each MEU exerts to execute the task.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
62:8 T. Wang et al.
We can also define the cost of cloud as the total payment that cloud has to pay for MEUs:
Ccloud =
n
i=1
pi . (10)
3.2 Design Objective
In this subsection, the design objective of the mechanism and some properties are introduced.
Considering both evaluation ability and bidding price, the goal of the mechanism is to maximize the
overall evaluation ability of entire selected MEUs while minimizing the costs. The formal definition
is as follows:
max
S ∈EU

i ∈S
ai
bi
, (11)
where S is the set of selected MEUs.
Meanwhile, the basic requirement of the evaluation task can be defined as the number of times
each sensor node is evaluated. In this section, we temporarily define the number of times as 1,
which indicates that the evaluation area of all selected MEUs needs to cover the whole network at
least once. We will discuss more general situations in Section 4.
We also aim to ensure the proposed mechanism has the following properties:
Definition 1 (Bidding Trustfulness [12]). A mechanism is of bidding trustfulness if, given any report strategy by other participators, the optimal strategy for each MEU to maximize their expected
utility is to trustfully upload their bid according their ability, i.e.,
E[pi (ai,bi )] − ci ≥ E[pi (ai,b
i )] − ci ∀i,b
i ,
where E[pi (ai,bi )] − ci is the utility when MEU is acting trustfully, and E[pi (ai,b
i )] − ci represents the utility when MEU is reporting a false bidding price.
Definition 2 (Individual Rationality). A mechanism is individually rational if for an MEU who
trustfully uploads their evaluation ability and bid, the expectation of the utility is nonnegative, i.e.,
E [pi (ai,bi ) − ci] ≥ 0 ∀i
3.3 Trustworthy Incentive Mechanism
In this part, we use Myerson’s conclusion [18] to design our Trustworthy Incentive Mechanism.
Theorem 1. A mechanism owning the following properties is of bidding trustfulness:
• If user i wins the auction by uploading bid bi , then it will still win by uploading bid bj ≤ bi
(sub-modularity);
• The mechanism pays each MEU the maximum declared bid.
The proposed algorithm is Algorithm 1. The input of the algorithm includes the set of all MEUs
EU and their bids BID. The output of the algorithm is the winner set S and their payment set P.
First, it initializes the winner set S, payment set P, winner set without certain MEU S as ∅, and
EU  as EU (lines 1–3). Then the algorithm uses greedy method to find the MEU with the maximum
performance-price ratio. D(S) are the nodes that can be accessed by MEU in set S. After all the nodes
can be accessed by MEU in S, loop over (lines 4–12). Next, for each MEU in winner set S, the algorithm
removes the MEU from S and continues to select other MEUs in EU to join S until all the nodes can
be accessed (lines 13–23). Finally, according to the element in S
, the algorithm gets the payment of
each MEU.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
Crowdsourcing Mechanism for Trust Evaluation in CPCS 62:9
ALGORITHM 1: Trustworthy Incentive Mechanism
Require: EU, BID
Ensure: S, P
1: S, P = ∅
2: S = ∅
3: EU  = EU
4: while D(S) cannot cover the whole network do
5: u = maxuseri ∈EU  ai
bi
6: if D(u) ⊆ D(S) then
7: EU  = EU  \ u
8: else
9: S = S ∪ {u}
10: EU = EU \ u
11: end if
12: end while //Find MEU with the maximum performance-price ratio
13: for each useri ∈ S do
14: S = S \ {useri}
15: while D(S
) cannot cover the whole network do
16: u = maxuseri ∈EU  ai
bi
17: if D(u) ⊆ D(S
) then
18: EU  = EU  \ u
19: else
20: S = S ∪ {u}
21: EU = EU \ u
22: end if
23: end while //Determine winner set without certain MEU
24: pi = maxj ∈S ai
aj ∗ bj
25: P = P ∪ pi
26: end for //Calculate payment
27: return S, P
3.4 Mechanism Analysis
Lemma 1. The proposed mechanism is of bidding trustfulness.
Proof. First, we need to prove the equivalence between sub-modularity of ai
bi and a1
b1 ≥ a2
b2 ≥
··· ≥ an
bn . We use reduction to absurdity to prove that sub-modularity of ai
bi implies a1
b1 ≥ a2
b2 ≥ ··· ≥ an
bn . Suppose thatuseri anduserj are selected by the cloud and i < j. Assuming that ai
bi < aj
bj
, we aim
to find a contradiction toward the assumption. After the cloud selects the last useri−1 with ai−1
bi−1 >
ai
bi
, the mechanism gets i = arдmaxk ∈EU −EUi−1
ak
bk . Thus, ai|EUi−1
bi > aj |EUi−1
bj . Given EUi−1 ⊂ EUj−1,
the definition of sub-modularity indicates that aj |EUi−1
bj > aj |EUj−1
bj , so ai|EUi−1
bi > aj |EUi−1
bj > aj |EUj−1
bj .
Thus, we prove that the way we select the winner guarantees the sub-modularity.
Next, we need to determine the payment that the cloud should pay for each selected MEU. The
key point is to find the maximum bid that the MEU can upload and still win. For each selected MEU,
we use greedy algorithm to get S when EU = EU \ {useri} and the payment pi = maxj ∈S ai
aj ∗ bj .
Next, we just need to prove that the payment pi = ai
aj ∗ bj is greater than the original bidding
price bi of useri . Since the cloud selects MEUs using greedy algorithm, we have ai
bi > aj
bj
, thus ai
aj ∗ bj > bi . The cloud pays MEUs the maximum declared bid.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
62:10 T. Wang et al.
According to Theorem 1, the proposed mechanism is of bidding trustfulness.
Lemma 2. The proposed mechanism is of individual rationality.
Proof. If an MEU is selected by the mechanism, then the utility is equal to ui = ai
aj ∗ bj − ci .
Given that ai
bi > aj
bj
, ai
aj ∗ bj − ci > ai
ai ∗ bi − ci = bi − ci > 0, if a user is not selected by the mechanism, then the utility is zero. Thus, the proposed mechanism is of individual rationality.
Lemma 3. The proposed mechanism is of O(n3) time complexity, where n is the cardinality of EU .
Proof. In the first loop, MEUs are selected by greedy function, which is O(n2). In the second
loop, each MEU’s marginal utility is determined by the same greedy function. Therefore, the time
complexity of the second loop is O(n3). So, time complexity of the proposed algorithm is O(n3) +
O(n3) = O(n3).
4 QUALITY-AWARE TRUSTWORTHY INCENTIVE MECHANISM
In this section, we extend the above mechanism to Quality-Aware Trustworthy Incentive Mechanism. We consider the situation where MEUs do not make efforts or lower the evaluation quality
after accepting the task. To ensure that all involved users honestly conduct evaluation task and
report result, we combine the incentive mechanism and data aggregation mechanism together and
decide payment for each MEU according to the result they uploaded. The proposed mechanism ensures bidding trustfulness, individual rationality, data trustfulness (to assure selfish MEUs report
evaluation results honestly), effort trustfulness (to assure selfish MEUs exert efforts to conduct the
task), and quality trustfulness (to assure selfish MEUs conduct task with reported ability).
4.1 Preliminaries
For each MEU in S = {user1,user2,...,userk }, E = {e1, e2,..., ek } denotes efforts that users make
to conduct the task, where ei ∈ {1, 0}. ei = 1 represents that useri exerts efforts to execute the task,
while ei = 0 denotes that useri does not make any effort to execute the task and just guesses a
result and uploads it to cloud [4, 17]. Thus, the evaluation quality of useri can be represented by:
qi = eiqi + (1 − ei ) ∗ 0.5. (12)
When an MEU conducts the task, ei = 1, and qi indicates the possibility of getting the ground
truth; that is, evaluation quality of the MEU. When an MEU does not make any effort to carry out
a task, ei = 0, and qi denotes the possibility the MEU gets the ground truth by guessing, which is
defined as 0.5 due to the binary property of the evaluation result.
The costs of an MEU to conduct a task can also be redefined as:
ci = eici . (13)
When ei = 0, the cost of an MEU to conduct a task is equal to 0, because the user does not need
to move and execute evaluation of sensors, while the cost of carrying on a task relates to MEU’s
ability (evaluation quality and area) when the user exerts efforts on the task.
4.2 Design Objective
We aim to ensure that the proposed Quality-Aware Trustworthy Incentive Mechanism has the
following desirable properties:
Definition 3 (Data Trustfulness). A mechanism is of data trustfulness if the optimal strategy for
each MEU to maximize their expected utility is to honestly upload the data about their evaluation
result, i.e.,
E[pi (ai,bi, ei, ˆdi )] − ci ≥ E[pi (ai,bi, ei, ˆd
i )] − ci ∀i,
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
Crowdsourcing Mechanism for Trust Evaluation in CPCS 62:11
where E[pi (ai,bi, ei, ˆdi )] − ci is the utility when an MEU honestly uploads the evaluation result,
and E[pi (ai,bi, ei, ˆd
i )] − ci represents the utility when the MEU reports false evaluation result.
Definition 4 (Effort Trustfulness). A mechanism is of effort trustfulness if the optimal strategy
for each MEU to maximize their expected utility is to exert efforts to conduct the task, i.e.,
E[pi (ai,bi, ei, ˆdi )] − ci ≥ E[pi (ai,bi, e
i , ˆdi )] − c
i ∀i,
where E[pi (ai,bi, ei, ˆdi )] − ci is the utility when an MEU exerts efforts to conduct trust evaluation
task, and E[pi (ai,bi, ei, ˆd
i )] − c
i represents the utility when the MEU does not make effort to
execute the task.
Definition 5 (Quality Trustfulness). A mechanism is of quality trustfulness if the optimal strategy
for each MEU to maximize their expected utility is to honestly execute task with reported ability
(evaluation quality and area), i.e.,
E[pi (ai,bi, ei, ˆdi )] − ci ≥ E[pi (a
i,bi, ei, ˆdi )] − ci ∀i, a
i,
where E[pi (ai,bi, ei, ˆdi )] − ci is the utility when an MEU honestly conducts the task with reported
ability, and E[pi (ai,bi, ei, ˆd
i )] − ci represents the utility when the MEU conducts task using inferior evaluation ability.
Meanwhile, the proposed mechanism also needs to satisfy Trustfulness and Individual Rationality mentioned above.
4.3 Quality-aware Trustworthy Incentive Mechanism
The proposed Quality-aware Trustworthy Incentive Mechanism can be divided into three steps:
First, the cloud chooses the winner set from all the MEUs according to their uploaded bids
(qi, areai,bi ) and informs them to execute the task. Next, after all the selected MEUs finish their
evaluation, the data should be uploaded to the cloud and the cloud executes a data aggregation
mechanism to gain the final results. Finally, according to the reported bids and data, the cloud
decides payments of each participating MEU.
Algorithm 2 describes the process of choosing the winner set.
ALGORITHM 2: Selection of Winner Set
Require: EU, BID, x
Ensure: S
1: S = ∅
2: while D(S) cannot cover the whole network x times do
3: u = maxuseri ∈EU ai
bi
4: if u’s margin utility equal to 0 then
5: EU = EU \ u
6: else
7: S = S ∪ {u}
8: EU = EU \ u
9: end if
10: end while // Find MEU with the maximum price-performance ratio
11: return S
The inputs of Algorithm 2 are the set of all MEUs EU , their bids BID, and evaluation times of
each node x. The output is the winner set S. Similar with the winner set determination in Trustworthy Incentive Mechanism, the process of Quality-aware Trustworthy Incentive Mechanism
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
62:12 T. Wang et al.
also considers the maximum performance-price ratio. However, this algorithm relaxes the limitation on the number of evaluations for sensor. Each sensor can be evaluated many times by MEUs
to ensure the accuracy of overall trust evaluation.
The margin utility in Algorithm 2 represents the contribution of an MEU to the entire trust
evaluation based on the selected MEUs. Particularly, the margin utility is zero when visiting area
of the MEU does not exceed the existing area.
After the selection of task-taking MEUs, each user conducts the evaluation and reports the results to the cloud. According to the reported data, the cloud runs the data aggregation mechanism
to obtain the final evaluation results of each sensor. Algorithm 3 presents the process of data aggregation.
ALGORITHM 3: Data Aggregation
Require: BID, S, D = ( ˆd1, ˆd2,..., ˆdk )
Ensure: (d1,d2,...,dk )
1: for each j s.t. nj ∈ N do
2: dc,j = siд(
	
i:useri ∈S,nj ∈N (qidi,j ))
3: end for // Calculate final evaluation result of nodes
4: return (dc,1,dc,2,...,dc,y )
The input of the Data Aggregation algorithm includes all MEUs’ bids BID, winner set S, and the
evaluation result D. The output is the evaluation result obtained by the cloud. For each MEU, the
uploaded evaluation area not only represents the moving range of the user, but also the sensors
within the area that required to be evaluated. The evaluation result gained by useri can be defined
as ˆdi , where ˆdi = (di,1,di,2,...,di,y ) and di,j is denoted as the evaluation result of useri to nodej .
N = {n1,n2 ...,ny } is set of sensor nodes in whole network and y is the total number of sensor
nodes. Whenuseri determines that a sensor nj is malicious or untrustworthy, thendi,j = −1. When
useri believes that a sensor is trustworthy, di,j = 1. di,j = 0 indicates that useri does not evaluate
sensor nj . According to each MEU’s reported evaluation quality qi and uploaded data, the cloud
calculates the comprehensive trust evaluation result (dc,1,dc,2,...,dc,y ).
Based on the above result, the cloud is able to determine the payment of each participated MEU.
The proposed payment determination algorithm is as follows:
The input of Payment Determination algorithm includes the set of all MEUs EU , their bids BID,
winner set S, set of MEUs’ evaluation results D, and the result obtained by the cloud. The output
is each MEU’s payment. For participating MEUs, cloud compares their reported result with the
aggregative results. Intuitively, the higher the MEU’s accuracy, the more payments are awarded.
Thus, the payment function can be represented as:
pi =
⎧⎪⎪⎪⎪
⎨
⎪⎪⎪⎪
⎩
ai
aj ∗ bj aci ≥ qi, 
 2
2qi1−1aci + 1
1−2qi
 ai
aj ∗ bj 1/2 < aci < qi,
0 aci ≤ 1/2,
(14)
where aci is the correct rate of useri and ai
aj ∗ bj is the payment of MEU without considering data
quality. When the actual accuracy is greater than qi , the cloud pays ai
aj ∗ bj to the user. The payment decreases along with the decrease of actual evaluation correct rate of MEU. When aci ≤ 1/2,
the actual accuracy of user is equal to or less than the accuracy without making effort and the payment of MEU is equal to 0. Therefore, this function can effectively ensure that the MEU gains the
maximum payment when they honestly report the trust evaluation ability and results. Meanwhile,
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
Crowdsourcing Mechanism for Trust Evaluation in CPCS 62:13
ALGORITHM 4: Payment Determination
Require: EU, BID, S, D, (dc,1,dc,2,...,dc,y )
Ensure: (p1,p2,...,pk )
1: for each i s.t. useri ∈ S do
2: run Algorithm 2 with EU \ useri
3: S = the winner set with EU \ useri
4: pˆi = maxj ∈S ai
aj
bj
5: aci =
	
j:di,j =dc,j,di,j ∈dˆ
i |di,j |
	
j:di,j 0,di,j ∈dˆ
i |di,j |
6: if aci ≥ qi then
7: pi = pˆi
8: else if qi > aci > 1/2 then
9: pi = ( 2 2qi1−1aci + 1 1−2qi )pˆi
10: else
11: pi = 0
12: end if
13: end for // Calculate payment considering accuracy
14: return (p1,p2,...,pk )
the payment of the MEUs will decrease if they report false evaluation ability, evaluation results,
or do not make effort to conduct the trust evaluation task.
4.4 Mechanism Analysis
In this section, we aim to prove that the proposed Quality-Aware Trustworthy Incentive Mechanism has the following desirable properties. First, we need to determine an assumption that most
of participating MEUs are of overall trustfulness, which indicates that most users honestly upload
their evaluation results. Thus, Pr(dc = dд ) > 0.5 > Pr(dc  dд ), where dc is the result obtained by
cloud and the dд is the ground value.
Lemma 4. The proposed mechanism is of data trustfulness.
Proof. Based on the premise qi > 0.5, Pr(di = dд ) > 0.5. Thus, if the MEU chooses to honestly
upload the evaluation result, the possibility that the user and the cloud get the same result is
Pr(di = dc )
= Pr(dc = dд )Pr(di = dд ) + Pr(dc  dд )Pr(di  dд ).
If the MEU does not report evaluation result honestly, then the possibility that the user and the
cloud get the same result is
Pr(di = dc )
= Pr(dc = dд )Pr(di = dд ) + Pr(dc  dд )Pr(di  dд )
= Pr(dc = dд )Pr(di  dд ) + Pr(dc  dд )Pr(di = dд ).
Therefore, we can get that
Pr(di = dc ) − Pr(di = dc )
= Pr(dc = dд )Pr(di = dд ) + Pr(dc  dд )Pr(di  dд )
− Pr(dc = dд )Pr(di  dд ) − Pr(dc  dд )Pr(di = dд )
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.            
62:14 T. Wang et al.
= Pr(dc = dд )[Pr(di = dд ) − Pr(di  dд )]
+ Pr(dc  dд )[Pr(di  dд ) − Pr(di = dд )]
= [Pr(dc = dд ) − Pr(dc  dд )][Pr(di = dд )
− Pr(di  dд )] > 0.
Supposinguseri is required to evaluatem sensors, the expectation number of correctly evaluated
sensors when MEUs honestly reports the results is E[Pr(di = dc )m] ≥ E[Pr(di = dc )m]. According
to the increasing monotonicity of the payment function,
E[pi (Pr(di = dc )m)] ≥ E[pi (Pr(di = dc )m)]
and
E(ui | di = dc ) = E[pi (Pr(di = dc )m) − ci]
≥ E[pi (Pr(di = dc )m) − ci] = E(ui | di = dc ).
Thus, data trustfulness holds.
Lemma 5. The proposed mechanism is of effort trustfulness.
Proof. When useri does not make any effort on the evaluation task, qi = 0.5 indicates that
Pr(di = dд ) = 0.5. Thus, the possibility that useri and the cloud get the same result equal to
Pr(di = dc )
= Pr(di = dд )Pr(dc = dд ) + Pr(di = dд )Pr(dc = dд )
= 0.5Pr(dc = dд ) + 0.5Pr(dc = dд ) = 0.5.
So the expectation number of correctly evaluated sensors is E(0.5m) and the expectation of payment is E(pi (0.5m)) = 0. Accordingly, the expectation of utility is E(ui | ei = 0) = E(pi (0.5m)) −
ci = 0, where ci = 0. Due to individual rationality of this mechanism, when useri makes efforts to
conduct the task, the expectation of utility is E(ui | ei = 1) = E(pi (qim)) − ci ≥ 0, where ci > 0.
Therefore, E(ui | ei = 1) ≥ E(ui | ei = 0) and effort trustfulness holds.
Lemma 6. The proposed mechanism is of quality trustfulness.
Proof. When useri furtively lowers the evaluation quality, we get p
i < pi indicating Pr
(di =
dд ) < Pr(di = dд ). The possibility that useri and the cloud get the same result equal to Pr
(di =
dд )Pr(dc = dд ) + Pr
(di  dд )Pr(dc  dд ). Thus,
Pr(di = dc ) − Pr
(di = dc )
= [Pr(di = dд ) − Pr
(di = dд )]Pr(dc = dд )
+ [Pr(di  dд ) − Pr
(di  dд )]Pr(dc  dд ) > 0.
Hence, we can get Pr(di = dc ) > Pr
(di = dc ) and E(Pr(di = dc )m) > E(Pr
(di = dc )m). Therefore,
E[pi (Pr (di = dc )m)] > E[pi (Pr (di = dc )m)]
and
E (ui | qi ) = E[pi (Pr (di = dc )m)] − ci
> E[pi (Pr (di = dc )m)] − ci = E 
ui | q
i

.
Quality trustfulness holds.
Lemma 7. The proposed mechanism is of individual rationality.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.           
Crowdsourcing Mechanism for Trust Evaluation in CPCS 62:15
Table 2. Experimental Parameters
Parameter Values
size of area (m2) 100 × 100
number of sensors 1,000
number of edge users 5,000
evaluation radius (m) [10, 30]
bid range [1, 100]
Proof. When useri makes efforts to conduct the task with reported evaluation ability and honestly reports evaluated results to cloud, the expectation of payment is E(pi ) = ai
aj
bi . According to
Lemma 2, the proposed mechanism is of individual rationality.
Lemma 8. The proposed mechanism is of bidding trustfulness.
Proof. The proof of Lemma 7 is the same as Lemma 1.
Lemma 9. The time complexity of the proposed mechanism is O(max(n3,m)), where n is the cardinality of EU and m is the cardinality of N.
Proof. According to Lemma 3, we can conclude that the time complexity of Algorithm 2 is
O(n2) and the time complexity of Algorithm 4 is O(n3). Meanwhile, the time complexity of Algorithm 3 is O(m). Therefore, the time complexity of the proposed mechanism is O(max(n3,m)).
5 PERFORMANCE EVALUATION
This section is divided into four parts. The first part introduces the experimental setting. Then, we
study cloud’s total payment under different scenarios. The effectiveness of the proposed QualityAware Trustworthy Incentive Mechanism is validated in the third part. Finally, we study the proposed mechanism from the perspectives of security and users’ utility.
5.1 Experimental Setting
The experiment is conducted in MATLAB 2017. We consider the network is deployed in a 100 ×
100m2 area. The experimental parameters are presented in Table 2. Each MEU randomly owns
evaluation radius ranging from 10 to 30 and chooses their bidding prices ranging from 1 to 100.
We normalize each MEU’s trust evaluation quality and trust evaluation area and use the linear
weighted sum method to get the evaluation ability of each user.
5.2 Cloud’s Total Cost
This part mainly shows the influence—namely, the change of different parameters—on the cloud’s
total cost. Figure 4(a) illustrates the impact of the number of MEUs on cloud’s total cost. The blue
line indicates the cloud’s total cost with different numbers of MEUs using Trustworthy Incentive
Mechanism (TIM), while the red line represents the cloud’s total cost with different numbers of
MEUs using Quality-Aware Trustworthy Incentive Mechanism (QTIM). Both the red line and blue
line decrease as the number of MEUs rises. The reason is that with the increasing number of MEUs,
cloud can select more MEUs with high cost-effective and spends less on the same evaluation ability.
Figure 4(b) illustrates the impact of the number of sensor nodes on the cloud’s total cost. Both
TIM and QTIM increase as the number of sensor nodes rises. Obviously, as the number of sensor
nodes increases, the cloud is required to recruit more MEUs to conduct evaluation task, thus the
total cost increases. Meanwhile, the increasing rate of cloud’s total cost decreases as the number
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019. 
62:16 T. Wang et al.
Fig. 4. The influence of different parameters on cloud’s total cost: (a) Influence of the number of MEUs,
(b) Influence of the number of nodes, (c) Influence of the number of times each sensor node evaluated.
Fig. 5. The influence of different behavior on MEU’s payment: (a) Influence of MEU’s true evaluation quality
and reported evaluation quality; (b) Influence of making effort or not; (c) Influence of reporting true data or
not.
of sensor nodes increases. The reason is that all the sensor nodes are confined in a certain area,
and the possibility that the new nodes are not in the evaluation area of selected MEUs minimizes
as the number of sensor nodes rises. So, the increasing rate of the cloud’s cost decreases.
Figure 4(c) represents the impact of the number of times each node is evaluated on cloud’s total
cost. Both TIM and QTIM increase as the number of times each node is evaluated rises. Apparently,
the increase of evaluation times for each sensor node indicates the growth of the number of selected
MEUs. As the number of selected MEUs rises, cloud’s total cost increases accordingly.
5.3 MEU’s Payment
Some experiments are conducted to study the impact of different behaviors on MEU’s payment. Figure 5(a) studies the influence of MEU’s true evaluation quality and reported evaluation quality on payment. X-axis indicates the difference between the reported evaluation quality (q
) and the true evaluation quality (q) on MEU’s payment. Each line represents the situation
where q = 0.55,q = 0.6,q = 0.7,q = 0.8,q = 0.9, and q = 1.0, respectively. As the differences
between q and q rise, all the MEUs’ payments decrease. Notably, the payment is influenced by
both p and p rather than only by p. For example, according to Figure 7, an MEU with p = 0.6 and
p = 0.7 will receive more payment than MEU with p = 0.6 and p = 0.8. The result indicates that
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
Crowdsourcing Mechanism for Trust Evaluation in CPCS 62:17
Fig. 6. (a) Influence of the number of times each sensor nodes evaluated on the evaluation error rate;
(b) Influence of MEU’s cost on utility; (c) Influence of introduction of MEUs on trust evaluation.
the proposed QTIM can effectively distinguish the behavior of lowering quality and providing low
payment accordingly.
Figure 5(b) illustrates the differences of payment between making efforts or not, and Figure 5(c)
indicates the difference of payment between reporting true data or not. Obviously, MEUs will
receive much more payment by making efforts and reporting true data than making no efforts and
reporting false data. The result indicates that the proposed QTIM can correctly distinguish MEUs
that are not making effort and reporting false data, and provide them with low payment.
5.4 Security and Utility
This subsection studies the proposed MCS-based Trust Evaluation Mechanism from the perspectives of security and MEU’s utility.
Figure 6(a) shows the impact of the number of times each sensor node is evaluated on cloud’s
evaluation error rate. The term cloud’s evaluation error rate indicates the differences between
evaluation results obtained by the cloud and the ground truth. According to Figure 10, the ratio
increases as the proportion of malicious MEUs rises, and decreases as the number of times each
sensor node is evaluated rises. The result indicates that increasing the number of times each sensor
node was evaluated can effectively minimize the cloud’s evaluation error rate.
Figure 6(b) indicates the impact of cost on the MEU’s utility. When the MEU honestly conducts
the evaluation task, the utility of both TIM and QTIM are stable and greater than the utility with
lower quality, false data, and zero effort. Thus, the result indicates that the proposed QTIM can
effectively embolden the MEUs to make efforts, report true data, and conduct task using the same
evaluation quality as reported.
The influence of introduction of MEUs is presented in Figure 6(c), where NN indicates the normal
nodes and MN represents the malicious nodes. We can see that the difference between NN and MN
is enlarged after the introduction of MEU. The reason is that, in experiments, underlying sensor
nodes determine the trust value of their neighbors by monitoring their communication behavior,
while MEUs determine the trust value of sensor nodes according to their communication behavior,
energy consumption, and data. Some nodes with normal communication behavior but insufficient
energy or abnormal data cannot be detected by the lower sensor nodes. The results illustrate that
the introduction of MEU can effectively improve the performance of trust evaluation.
6 CONCLUSION
CPCS has been emerging as the combination of cyber physical systems and cloud computing and
gradually become a research hotpot. As the connectivity among nodes, cloud computing centers,
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 6, Article 62. Publication date: October 2019.
62:18 T. Wang et al.
and users increases, the damage caused by untrustworthy nodes is magnified. Fortunately, the appearance of Intelligent Edge Computing enables people to handle this problem in a cost-effective
manner. In this article, we first proposed MCS-based Trust Evaluation Mechanism, where cloud
recruits mobile edge users to evaluate end nodes using intelligent method. Then, two incentive
mechanisms are designed to ensure the trustfulness of mobile edge users. We demonstrate the
trustfulness and effectiveness of our proposed Trustworthy Incentive Mechanism from bid trustfulness, data trustfulness, effort trustfulness, and quality trustfulness separately. We experimentally evaluated our proposed mechanisms, and the result demonstrated that compared with users
who do not honestly conduct the trust evaluation task, the trustworthy users earn eight times
more than them. Further studies are needed in the future. For example, how to protect the data of
end nodes when mobile edge users evaluate the trust of them.