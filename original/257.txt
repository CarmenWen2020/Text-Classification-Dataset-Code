By amalgamating recent communication and control technologies, computing and data analytics techniques, and modular manufacturing, Industry 4.0 promotes integrating cyber–physical worlds through cyber–physical systems (CPS) and digital twin (DT) for monitoring, optimization, and prognostics of industrial processes. A DT enables interaction with the digital image of the industrial physical objects/processes to simulate, analyze, and control their real-time operation. DT is rapidly diffusing in numerous industries with the interdisciplinary advances in the industrial Internet of things (IIoT), edge and cloud computing, machine learning, artificial intelligence, and advanced data analytics. However, the existing literature lacks in identifying and discussing the role and requirements of these technologies in DT-enabled industries from the communication and computing perspective. In this article, we first present the functional aspects, appeal, and innovative use of DT in smart industries. Then, we elaborate on this perspective by systematically reviewing and reflecting on recent research trends in next-generation (NextG) wireless technologies (e.g., 5G-and-Beyond networks) and design tools, and current computational intelligence paradigms (e.g., edge and cloud computing-enabled data analytics, federated learning). Moreover, we discuss the DT deployment strategies at different communication layers to meet the monitoring and control requirements of industrial applications. We also outline several key reflections and future research challenges and directions to facilitate industrial DT’s adoption.

Previous
Next 
Keywords
Industry 4.0

Digital twin

Industrial Internet of things

Cyber–physical systems

Machine learning

Artificial intelligence

Computational intelligence

Multi-access edge computing

5G-and-Beyond/6G

Green communication

Age of information

1. Introduction
The fourth industrial revolution, termed Industry 4.0, targets digital transformation of various sectors, such as intelligent manufacturing, automation, and aerospace (Kourtis et al., 2019, Malik and Brem, 2021). In this transformation, the intelligent factory, also known as the factory of the future (FoF), depends on ubiquitous industrial Internet of things (IIoT) connectivity to achieve the goal of flexible, efficient, and versatile production systems. On the other hand, the emerging architectures, such as cyber–physical systems (CPS) and industrial digital twin (DT), together with the intelligent computation-enabled next-generation (NextG) wireless networks (i.e., 5G-and-Beyond networks), are envisioned to play a prominent role in reshaping the digital landscape of FoF. In the following subsections, we give a brief overview of Industry 4.0 enablers, followed by the need for adopting emerging communication and computation paradigms for realizing Industrial DT, and define the structure of our survey.

1.1. Enablers of industry 4.0 and their symbiotic relationship
Industrial IoT. IoT is a revolutionary concept of building an intelligent digital ecosystem by connecting all physical assets, empowered to interact or communicate through the Internet infrastructure and NextG wireless networks (Mahmood et al., 2021). Meanwhile, integration of Industry 4.0 with IoT in the products’ manufacturing process has given surge to IIoT—a child IoT technology designed explicitly for mission-critical industrial applications (Munirathinam, 2020). The connected industrial assets are machines, actuators, control systems, and robots, performing mission-oriented automation tasks. An IIoT network differs from a typical ad-hoc IoT network; it is primarily data analytics-enabled cloud-based structured network that supports machine-to-machine (M2M) wireless connectivity having stringent latency and reliability requirements in a dynamic industrial environment (Sisinni et al., 2018).

CPS and Industrial DT. CPS brings together the physical and networked resources with emerging computation paradigm, enabling the intelligence in machines and robots to perform collaborative mission-critical tasks (Zhang et al., 2018, Teng et al., 2021). Meanwhile, DT is a living virtual or digital image/softwarized model that can be built for robots, machines, or the physical process of the entire manufacturing plant, which interacts with the physical assets of the plant using actuators and control planes to optimize the production (He et al., 2018, Villalonga et al., 2021).

Symbiotic Role of IIoT and CPS towards Industrial DT: A DT is an emerging but conceptually different construct than CPS and IIoT. Much like CPS, DT relies on communication (wired/wireless) to create a highly-consistent, synchronized digital mirror image/representation of the objects or physical processes. However, DT, in addition, uses built-in softwarized models on this precise image to simulate, analyze, predict, and optimize their real-time operation using feedback. Fig. 1 illustrates the conceptual relation and difference among IIoT, CPS, and DT for physical entities on a factory floor. Essentially, industrial DT is a digital tool that recreates an intelligent virtual image of the machines in the edge or cloud based on the incoming IIoT data from field devices, associated with real-time physical attributes of a CPS. This implies that a DT can be implemented at various levels of the layered communication pyramid, i.e., at the edge close to the data sources or the cloud close to the application (Leng et al., 2021). In industrial automation, the monitoring applications are typically not affected by the delay and jitter in packets, and the tolerable latency is in the order of seconds. However, critical processes such as closed-loop control and interlocking have stringent latency requirements of 1 ms to 100 ms and ultra-reliability of more than 99.999% (Mahmood et al., 2019). In a nutshell, Industry 4.0 is the product of an amalgamation of two splendid paradigms, IIoT and the CPS, which is further aided by DT (Niederer et al., 2021, Aazam et al., 2018). The headpin of this globally adopted industrial revolution is the unprecedented implementation of intelligent services using the emerging technologies of Industry 4.0 critical enablers.

1.2. Accelerated adoption of new ICT paradigms for industrial DT
Thanks to the advances in communication and sensing technologies, virtualization, and computing power, many industries are opting for the DT-driven customization and optimization of the factory operations (Wang et al., 2021, Stark et al., 2019). By 2025, more than six billion IoT-enabled devices will be online through cellular access, which currently stands at 1.5 billion connections, and the generated cellular traffic will reach  bytes (Ericsson, 2020, Piran et al., 2020). The COVID-19 pandemic has affected the high forecast of connections and online traffic predicted in previous technical reports (Ericsson, 2020). Nevertheless, little has changed as it has increased the demand for the acquisition of intelligent services that can be managed and controlled remotely through information and communication technologies (ICT) (Piran and Suh, 2019, Chamola et al., 2020, Allam and Jones, 2021). This strengthens the importance of emerging trending technologies and techniques in NextG wireless networks and computational intelligence (CI) paradigms as their nexus will provide the baseline for developing industrial DTs.

NextG Wireless Networks. The design and deployment of the fifth-generation (5G) and beyond (B5G) wireless networks is primarily focused on supporting diverse services with heterogeneous communication attributes of mission-critical applications (Chettri and Bera, 2019). These communication attributes are (Lin et al., 2021, AlAhmad et al., 2021, Abbas et al., 2021): (1) ultra-reliability and low latency, (2) support for high data rates, (3) massive machine connectivity, (4) secure data-driven mobile computation services, (5) dynamic and optimized over-the-air resource allocations, and (6) age-optimal energy efficient communication. Collectively, these enabling attributes provided by the NextG wireless networks form the building foundation for two-way communication between industrial DT and the physical assets.

Computational Intelligence. The concurrent deployment of the enhanced networked communication infrastructure, high-performance data analytics (HPDA) techniques and high power computing (HPC) capabilities at the cloud/edge is ushering new computational intelligence paradigms that can provide the customized services to on-demand industrial applications, e.g., anomalies detection, fault prognosis, and increased digital hyperconnectivity (Tang et al., 2021, Xiao et al., 2020). One of the new computational intelligence (CI) paradigms called “federated learning” combines the data analytics and computing models at the edge of network to provide intelligent services (data offloading, efficient computations) for the end-devices, e.g., IIoT-connected robots and machines (Khan et al., 2021). Similarly, data fusion and streaming analytics with HPC capabilities can provide real-time analysis of IIoT data.

Industrial DT at the Nexus. The nexus of NextG wireless networks and emerging CI paradigms in tandem is expected to play an essential role in realizing the true potential of Industrial DTs and bridging the cyber-space and physical space comprising multiple robots and machines. Many factory assets are expected to continuously transmit an ample amount of machine data to the HPC-enabled edge or cloud servers, utilizing NextG wireless networks’ resources. Similarly, HPDA-based algorithms assist in realizing the softwarization of physical space based on the incurred IIoT data at the HPC-enabled edge or cloud servers to model the industrial DT. Once the industrial DT is modeled, it monitors, controls, and optimizes the industrial process with NextG wireless networks. This increases the significance of discussing the roles and requirements of the emerging computational and communication enablers in both NextG wireless networks and computational intelligence paradigms.

1.3. Our survey structure
The rest of the article is organized as follows. Section 2 identifies the research trends in Industry 4.0 enablers, existing gaps in surveys and our motivation/contribution. Section 3 gives an overview of the DT for smart industries, followed by Section 4 that discusses the role and requirements of emerging DT-enablers and technologies related to NextG wireless networks and emerging computational intelligence schemes. Section 5 discusses the lessons learned, future opportunities and challenges, and Section 6 gives concluding remarks. The overall structure of the article is given in Fig. 2.


Table 1. Methodology on screening papers.

Index of searching	Content of evaluation
Search Time-period	From: January 2003, To: July 2021
Article Database	Scopus, Science Direct, Google Scholar, and IEEE Xplore.
Articles Type	Published peer-reviewed technical conferences and journals
Screening Procedures	The relevance with the research topic as judged by the content
written in the abstract, introduction and conclusion section of each paper.
Search Strings	“Industrial IoT”, “IoT for Industry 4.0”, “cyber–physical systems”,
“digital twin”, “digital twin manufacturing”, “digital twin and Industry 4.0”, etc.

Download : Download high-res image (766KB)
Download : Download full-size image
Fig. 2. Structure and overview of our survey.

2. Research trends, gaps in existing surveys, and our contributions
This section discusses the market statistics and current research trends in critical enablers of Industry 4.0 (IIoT, CPS, and DT), research methodology for collecting and evaluating literature, summary of existing surveys and review works on digital twins in various industries, and motivation and contributions of our review work.

2.1. Market statistics and research trends
The trend to incorporate digitization and robotization in the manufacturing and aerospace sectors is growing rapidly to enhance agility and efficiency of the production processes (International Federation of Robotics, 2021). This is apparent from the increasing density of robots on the factory floors; in developed countries, such as China, South Korea, and Germany, more than 500 industrial robots exist on average per 10000 employees. Meanwhile, the International Federation of Robotics (IFR) records show that the worldwide number of operating robots is 2.7 million, an increase of 12% from the previous year. In this emerging scenario, the FoF demand the networked interaction of collaborating multiple robots to perform isochronous and intelligent operations. The critical nature of these collaborative operations is becoming possible with the IIoT connectivity technologies together with the emerging CPS/DT-based synchronized digital breathing replicas (ManufacturingGlobal, 2021, Department, 2021). These trends and technological advances have been drivers behind the global market increase in factory automation. According to the market statistics (c.f.  Fig. 3), the factory automation market is projected to grow exponentially at the compound annual growth rate of 8.8% during the 2017–2025 time period with a forecasted value of 368 billion USD (Anandan, 2020).

Meanwhile, it is apparent from Fig. 4 that significant growth has been observed in research publications every year for both CPS and DT during 2011–2021. The screening methodology to obtain Fig. 4 is summarized in Table 1. Note that we repeated the screening process through three different independent campaigns and compiled the findings from 2003–2021 for CPS, IIoT, and DT to bring reliability to the publication screening process. However, before final processing, the relevance of the compiled data with the area of interest needs to be established, i.e., it should be based on the abstract, introduction, and conclusion of the papers. Moreover, numerous articles in the search database included the keyword “digital” or “twin” in the abstract or title, which does not mean the “digital twin” or “virtual image” of the process as a whole. The same holds for “cyber” or “physical systems”, and “industrial” or “IoT” while searching data for CPS and IIoT. Such types of articles were excluded from the final database used to plot the trend of publication count in Fig. 4.


Download : Download high-res image (197KB)
Download : Download full-size image
Fig. 3. Annual expected size projection of the worldwide business by factory automation (2017–2025) (Anandan, 2020).

We carefully studied all the incorporated papers in our search database and developed the common grounds and proposition towards CPS, IIoT, and DT in the industrial ecosystem. During 2003–2011, there was significant adoption and development in IoT, sensor technology, machine analytics, simulation, and communication technologies, which provided a baseline for further work in the areas of CPS and IIoT. However, the technological foundations were not mature enough to support DT deployment in industrial applications. Since 2011, there is a significant shift in focus towards the DT research and development in tandem with IIoT and CPS, as evident from Fig. 4. However, fewer attempts have been made to rigorously evaluate the DT applications in the industry. Based on the facts mentioned above, DT is expected to open up novel opportunities for research and development in the foreseeable future.

2.2. Existing surveys and review works
Completeness is the priority of any review work. Numerous surveys and review works on various case studies primarily reviewed DT for control and management processes in industrial applications (Tao et al., 2019, Qi et al., 2019, Cimino et al., 2019, Yi et al., 2021a, Liu et al., 2020, Jones et al., 2020, Tao and Zhang, 2017a, Qi and Tao, 2018, Rasheed et al., 2020, Fuller et al., 2020, Wanasinghe et al., 2020, Khajavi et al., 2019, Barricelli et al., 2019, Hasan et al., 2020, Moyne et al., 2020, Minerva et al., 2020, Rathore et al., 2021, Zheng et al., 2019, Wu et al., 2021). The closely related works to this article are summarized in Table 2 with the necessary emerging computation and communication enablers identified and marked for either they are covered in DT review work or not. We use (✓) if the enabler technology is discussed and explored from the DT’s factory usage perspective and (✗) otherwise.


Table 2. Summary of existing surveys and case reviews on digital twin with their primary research focus.



The idea of survey work of authors in Liu et al., 2020, Jones et al., 2020, Rasheed et al., 2020, Fuller et al., 2020 primarily centers around: (1) DT concepts and characterization, (2) DT construction and modeling methodologies, (3) various applications of DT usage, (4) DT business value, and lastly, (5) reporting the research gaps findings in DT literature and/or providing future research directions. Similarly, Cimino et al. (2019) explored the DT use cases in manufacturing sectors and identified the expected critical DT services on the factory management level. Khajavi et al. discussed the benefits and shortcomings of DT for building management, and developed the DT model for building using numerous IoT sensors and installed devices to manage the building life cycle (Khajavi et al., 2019). Furthermore, the authors of Zheng et al. (2019) reviewed the DT concepts and developed the DT-based management model for the product life cycle. Moyne et al. (2020) identified the requirements of DT usage and developed a model based on the recommended requirements towards the practical implementation of DT. Minerva et al. (2020) surveyed the DT features to enable softwarization and virtualization of physical objects and achieve true hyper-connectivity in application-specific environments, such as manufacturing industries. Tao et al., 2019, Qi et al., 2019, Rathore et al., 2021 and Qi and Tao (2018) reviewed the DT usage for innovative factory applications. These studies: (1) explore the DT research carried out for the industrial use cases, (2) identify critical DT enablers for implementation, i.e., cloud computation, big data, data fusion, ML, etc., and (3) review the interplay of ML, AI, and big data and their role in DT-based smart manufacturing. More review details are given in the remarks column of Table 2. Hasan et al. (2020) reviewed blockchain technology to implement the DT process and considered using a blockchain-based DT case study for securing the data transaction, logs, and other essential processes data. Likewise, Wanasinghe et al. (2020) performed a literature review for the use of DT technology in the oil and gas industry and explored its benefits, lapses, and future research directions.

From observing the review work in Table 2, it is evident that the most of the studies are only focused on exploring the computing enablers for DT. However, emerging technologies and techniques in communication and computing have not been explored together in the literature. Our review work focuses on industrial DT with respect to emerging state-of-the-art technologies in both computation and communication domains since both will jointly play an essential role in realizing the DT in smart industries.

2.3. Our motivation and contributions
During the 2003–2011 time period, there was limited research on DT development due to the aforementioned reasons and technological constraints. Consequently, less number of publications are available on DT at the 2003–2011 timeline. On the other hand, other communication and computation enablers technologies, such as big data analytics, cloud computing, ML, and AI, continue seeing advances and exponential growth in their utilization in smart manufacturing. Moreover, the concept of DT was largely underestimated because of lag in the vision for DT significance, its adaptation, and long-term influence on real-time industrial applications. Nevertheless, this lag of vision changed when NASA in 2010 practically demonstrated the superiority of DT’s adaptation in space flight shuttle program. Since then, many DT applications in various fields have emerged, and both the industry and academia have focused on it together with IIoT and CPS (as evident from Fig. 4) due to many technological advancements in communication, sensing, and computation technologies. Keeping in view the current research trends and research gaps in DT adaptation, it can be envisioned that future research on DT and its practical deployment in the smart factories will experience exponential growth in the next 2–6 years.

DT has already been adopted by various smart industries, complementing the vision of Industry 4.0. However, as the industry ecosystem’s digital landscape embraces emerging technologies and tools, which include, but are not limited to, cloud and edge computing, ML and AI, advanced data optimization techniques, and beyond-5G (B5G) network services, it brings up some critical questions. Especially, what is the role of various emerging technologies in enhancing futuristic smart industries’ performance, and how these emerging technologies will reshape DT’s usage in smart industries? Similarly, what are the vital requirements of different use cases that have to be fulfilled by DT in conjunction with these emerging technologies for realizing the Industry 4.0 vision?

To the best of our knowledge, there is no prior work on DT for smart factories keeping in view the role and requirements of emerging technologies at various layers of the factory communication stack. Our key contributions in this review paper can be summarized as follows:

•
We review the recent research on the use of DT in smart industries, elaborate upon functional aspects of DT, and highlight its appeal for smart industries. Moreover, we provide the taxonomy for DT usage in various industrial applications and identify the impending challenges in terms of communication and computation requirements for industrial DT.

•
We discuss the current state-of-the-art developments in emerging technologies, especially the role of hybrid cloud/edge computing, advanced ML and data optimization techniques, federated learning, immunocomputing, B5G/6G networks, green communication, and age of information (AoI), and their implications and significance on the performance of DTs.

•
We discuss the DT placement strategies at different industrial communication layers to address the identified critical requirements. For instance, migrating DT capabilities from the cloud to the edge layer can address security, computation, and stringent quality of service (QoS) targets of factory floor applications.

•
Finally, we summarize the lesson learned from our thorough review work and outline the possible future research opportunities and challenges in emerging technologies to facilitate DT’s adoption in industries.

3. DT in smart industries
This section provides an overview of DT fundamentals and the impactful role DT plays in tandem with IIoT and CPS inside the factory ecosystem to change the digital landscape. Furthermore, we classify the DT usage and its significance in numerous innovative industries and identify the critical challenges for the adoption of industrial DT.

3.1. Fundamentals of DT systems
A DT system of a smart industry forms a virtual image of physical objects in a factory environment, i.e., it depicts a living digital simulation model of the physical counterparts in a factory, as shown in Fig. 5. A DT model is often confused with digital shadow or digital model; however, in the latter approaches, there are no automated exchange of control data between the image created in the virtual space and the physical objects to alter the industrial processes (Kritzinger et al., 2018, Ladj et al., 2021). In contrast, the DT system of a single robotic machine or entire physical space of a factory continuously updates and evolves in real-time together with its physical counterpart to show the operating status, health conditions, and collaborating positions (Madni et al., 2019, Tao and Zhang, 2017b).

To create a twin model of an object, integration of numerous communication technologies, cloud services, data analytics, and learning techniques is required (Verdouw et al., 2021). In this respect, the data sources for analytics and learning can be, for instance, individual sensors, similar machines in different systems, recorded data of faulty machines, and input of technical experts (Camposano et al., 2021, Biesinger et al., 2019). The inflow of information from all the sources significantly contributes to the development of agile and fast DT models, while the information is often stored in the cloud using dedicated network infrastructure.

3.2. The role of DT across industries
The integration of IIoT and CPS with DT is critical in realizing intelligent factory machines/processes since the high-value real-time data is generated throughout their working cycle (Guiffo Kaigom and Rossmann, 2020). Also, it enables machines/processes to interact and evolve synchronously with other machines/processes in cyberspace; thus allowing to assist and optimize various mission-critical applications in manufacturing and automation (Seebo, 2019). Moreover, DT recreates the factory ecosystem’s physical space, enabling them to interact and evolve synchronously with other machines/processes to assist and optimize various mission-critical applications in manufacturing and process automation (Seebo, 2019).

In Fig. 6, we classify and reference the latest case studies of DT usage reported in the literature for different innovative industries that come under the vision of Industry 4.0, e.g., manufacturing (Zhang et al., 2019, Liu et al., 2021b, Xia et al., 2021, Yi et al., 2021b), automobile (Sidorov et al., 2021, Yujun et al., 2021, Qin et al., 2021, Son et al., 2021), aerospace (Liu et al., 2021a, Cunbo et al., 2018, Lee et al., 2021, Xiong et al., 2021), windfarm (Pawar et al., 2021, Moghadam and Nejad, 2022, Moghadam et al., 2021, Branlard et al., 2020), and healthcare (Laamarti et al., 2020, Elayan et al., 2021, Zhang et al., 2020, Croatti et al., 2020). Moreover, Fig. 6 explicates the impact of valuable essential services provided by industrial DT in classified enabling application domain. We provide the current state-of-the-art research work details on DT implementation and features along with its solution and services in each identified broad industrial area in Table 3. In the subsequent subsections, we explore the vital impacts of industrial DT.


Table 3. Existing research work on industrial DT’s implementation and service provisioning in numerous classified innovative industries (Legend: ✓ means that the feature has been considered and implemented in the DT application while × means otherwise).

Industrial
Category	Authors
(Case Studies)	Features and Solutions of Industrial DT Implementation	Industrial DT
Services
Scope	Dataset for
Implementation	Technology & Tools	Control of Real-
System through DT
Manufacturing	Liu et al. (2021b)	Hollow Glass
Production Line
System	Perceptual Sensors,
Machine Process,
Control Feedbacks	SCADA and PLC-
enabled Machines,
Cloud/edge computing
& Storage databases,
qcadoo MES (Java)	✓	Configuration Design,
Motion Planning,
Control Development,
Optimization Decoupling
Zhang et al. (2019)	Single Machine
(Assembly Process)	Control & Machine
Performance, Historical
Knowledge	Siemens PLC, Software
I/O, SQL server,
Protégé (Java)	×	Machine Failure Analysis,
Prediction & Maintenance
Automobile	Yujun et al. (2021)	Constant Velocity
Joint ( Process)	Sensors Data (Vibration,
Temperature, Noise, &
Oil Sample)	CATIA (Dassault
Systèmes), Data Fusion	✓	Product Lifecycle,
Predictive Maintenance
Son et al. (2021)	Automative
Production Line	Information Data Classes
(Product, Process, and
Plant Resources)	VREDI, IIoT, CPS,
Orcacle Database 12c,
Visual Studio 2019	✓	Defective Product
Monitoring & Prediction
Aerospace	Lee et al. (2021)	Single Machine (Aero-
engine Components)	Measured Geometric
Data (Gas Turbine Rotor)	MISES, Hydra
& Communication
Modules	×	Virtual Simulation,
Performance Prediction
Xiong et al. (2021)	Single Machine
(Aircraft Aero-engine)	NASA’s Turbofan Dataset
(Maintenance, Sensors, & Running Environment)	Implicit DT model
& AI Methods	×	Intelligent Predictive
Maintenance
Windfarm	Moghadam et al. (2021)	Wind Turbine Drive-
trains (Machine Process)	Gearboxes Data (Sensor,
Control & Monitoring)	SIMO, RIFLEX,
AeroDyn	×	Predictive Maintenance
(Useful Lifetime)
Branlard et al. (2020)	Land-based Wind
Turbine (Machine Process)	Wind speed and thrust,
Tower loads and position,
Rotor speed, and Generator
torque and pitch	Augmented Kalman Filter,
OpenFAST, and NREL
5-MW turbine	×	Real-time Process
Simulation and Monitoring
Healthcare	Elayan et al. (2021)	Digital Healthcare and
Operations System	MIT-BIH Arrhythmia
Database (ECG Recordings)	IoT Wearables Sensors,
Data Fusion and Storage,
ML and DL Methods	✓	Intelligent context-aware
Health-Ecosystem,
Patients Monitoring
Zhang et al. (2020)	Medical IoT-based
Health Process	Real-world Patient Data
(Age, Thoracic CT findings, Leukocyte, Pathological type
and location, and ECG)	Pico G2 (VR headset),
Unity engine, Python,
Android OS, C++/C#,
and Neural Networks	×	Patients Monitoring,
Visualization, and Software Vulnerability Detection
3.2.1. Data visualization
In industries, the processes are advanced and complex, thus making it nontrivial for technical and management teams to take decisive actions from the data in raw data-sheets and figures (Munirathinam, 2020, Tong et al., 2019). The DT bridges this gap by integrating the visualization of live data from machines in the virtual image or digital model. Besides, any data redundancy can be removed from visualization to develop clear insight into complex factory processes (Zheng et al., 2019). Moreover, each deployed machine or robot’s physical parameters, e.g., temperature, rusting, failure rate, and working conditions can be accessed. For example, a joint project by Altair, MX3D, and ABB showed a working DT model with visual settings for a 3D printed customized manufacturing robot (Glabeke et al., 2020). The DT model of the robot and visual access to its time-series data has increased the robot’s performance, which could be exploited to achieve higher precision and isochronous operation in smart factories.

3.2.2. Collaboration at management levels
Another crucial role of DT is to increase collaboration between the stakeholders, management authorities, expert teams, and the ground staff to actively monitor the output of a facility and weigh in if any input is required (Lim et al., 2019). This collaboration provides a deep insight into the complex processes of a manufacturing facility to data scientists, field engineers, designers, and product managers  (Macchi et al., 2018). Also, it gives a better comprehension of working knowledge, which helps design new prototype systems and test them quickly with increased efficiency. For example, ThyssenKrupp, a leading elevator manufacturer, collaborated with Microsoft and Willow to built an intelligent cloud-enabled DT model for a 246-meter innovation test tower in Rottweil, Germany (Azure-DigitalTwins, 2018). The collected data from hundreds of sensors, installed across the building, are integrated to create the building’s digital replica in the cloud, giving a unique visual insight to perform asset and resource management in real-time.

3.3. Impact of DT on smart industries
The impact circle of DT on the critical factories can be identified as, (Gunasegaram et al., 2021, Lu et al., 2020c, Tao and Qi, 2019):

(1)
Product manufacturing and designing: The availability of machines’ DT enables accurate prediction of failure in the production process before affecting a plant’s output targets. If system enhancement is desired, performance parameters can be adjusted and simulated in DT without imperiling the operation of the entire production.

(2)
Field products: DT enhances the access and analysis of deployed field products to enable remote commissioning and diagnostics. It lowers service costs by remotely configuring faulty parts of a product, which can be ordered and replaced accordingly for new customers.

(3)
Future products: DT can predict machines’ faulty behavior in complex systems, design newer and better systems from the learned history of machine operating conditions, and optimize a facility’s efficiency and output.

By catering to customer satisfaction and efficient working of smart factories, these DT-based end-services can undoubtedly increase the profit margin and market share of factory owners. For example, American electric power (AEP), which supplies electricity to more than 5 million customers, is developing a DT model of the US’s most significant power transmission network with the specialized modeling and analysis software PSS® ODMS from Siemens. It tightly integrates the electrical grid network with its virtual twin model (Siemens, 2019). Otherwise, the grid network planning and provisioning of services to customers were becoming complicated with traditional (manual) methods of sharing the technical data among the various utility systems. Similarly, ABB’s state-of-the-art electromagnetic (EM) flow measurement products integrate DT technology to build up the predictive model of EM flow during production processes using multiphysics finite element analysis (FEA) techniques (ABB-DigitalTwin, 2019). In particular, DT usage mimics the virtual EM flow process, giving visual insights to acquire performance complexities.

3.4. Impending challenges in industrial DT
The initial coined idea of DT was in the context of increasing the product life cycle of an industrial machine and learning from the anomalies and malfunction over time, which tends to design it better. However, the simultaneous interplay of industrial twin with all emerging communication and computation technologies in large-scale factory scenarios inherits significant challenges and hurdles (c.f.  Fig. 6). For example,

•
A large amount of data from numerous factory floors needs to be transmitted for mapping a large number of industrial devices with their virtual counterpart in the cloud or possibly at the edge, while the communication resources are limited.

•
The communication burden caused by this frequent real-time interaction of factory floor machines with the DT residing in the cloud may lead to intolerable delays for time-critical applications.

•
The integration of edge-based computing architecture with the cloud brings new roles and adjustments to a digital twin’s deployment strategies to address the requirements on performance metrics, such as big data management, communication latency, reliability, packet loss ratio (PLR), data update, data size, security, and privacy.

•
The massive inflow of incurred machine data from the factory manufacturing floor using communication infrastructure requires enhanced raw data preprocessing and the latest computation-efficient data analytics and learning techniques to build up the industrial DT.

•
The energy constraints and AoI requirements set by the applications’ requests limit the collected data update rates in meeting the goal of energy-efficient green communication for industrial devices while satisfying the information freshness at the industrial DT.

4. Role and requirements of emerging technologies for industrial DT
The integration of DT with the emerging technologies, i.e., hybrid edge and cloud layer architectures, B5G network services, state-of-the-art ML and AI frameworks, can open up many new potential use cases of DT and accelerate the digital transformation of smart industries. Table 4 summarizes the various critical requirements of industrial use cases, which must be maintained by the DT of a smart factory. Note that the generated data of each use case in Table 4 has data and big data class, which is not mentioned in the table. Moreover, the data update time (msec) applies to the periodic updates of event-based or sporadic data traffic generated. These emerging technologies are explored in subsequent subsections for their adoption in DTs, and their roles and needs are also discussed at each layer of the communication stack, as identified in the smart factory scenario in Fig. 7.


Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 7. Illustration of B5G and cloud/edge-based DT layered architecture for smart industries. Main ideas: a) a part of CDT is shifted to the edge layer to make local learning and make decisions quickly (federated learning), b) EDTs are developed at the edge layer, i.e., at 5G gNodeB (gNB) or edge server, which takes the inflow of data from numerous sources, computes and locally learn, and c) 5G gNB provides the computation-enabled NextG network services to provide efficient and reliable wireless connectivity for the factory devices.

4.1. Cloud/edge computing and industrial DT deployment
Cloud computing and edge computing (cloud/edge computing) are a critical component of Industry 4.0 to ensure on-demand availability of high computing resources, e.g., as shown for aerospace manufacturing industry in Caesarendra et al. (2019) and vehicular intelligence towards connected smart vehicles in Zhang et al. (2021). The vital strengths of high computing power, massive data storage capacity, data analytics, service-oriented architecture with a sizeable autonomous structure have led to a massive adoption of cloud computing in today’s smart industries (Coelho et al., 2021). Numerous CPS- and IIoT-based machines generate a large amount of data during the intricate manufacturing process, which has to be transferred and stored in the cloud (Sinha and Roy, 2020). By this, the industries can reduce the cost of dedicated data centers, which also brings global access and management to factories (Patel et al., 2018).

4.1.1. Cloud-based digital twins
The creation of a virtual digital image of a factory from the inflow of data from heterogeneous sources in the cloud leads to a significant class of twins, termed as cloud-based digital twins (CDTs) (Shahriar et al., 2018). Fig. 7 shows the cloud-native CDT service closely integrated with the upper factory management layers. In the cloud, necessary operations, e.g., pre-processing of machine data and big data analytics, are applied for efficient data management and utilization (Qi and Tao, 2019). By using that, CDT brings more possibilities; it enhances the collaboration and visualization for intelligent decision making, in addition to the advantages discussed in Section 3.2. Moreover, CDT allows the training of a complex network of all industrial assets with high power computing (HPC), deep learning (DL) and AI.


Table 4. A Summary of smart industries requirements in Industry 4.0 (based on  (Shahzad et al., 2020, Ho et al., 2019, Schulz et al., 2017, Yan et al., 2017, 5G ACIA, 2019)).



4.1.2. Emergence of edge-based digital twins
CDTs have certain inherent limitations of cloud architecture for stringent time-critical industrial communications, e.g., high round-trip time (RTT) with regular periodic data updates and end-to-end (E2E) latencies to the cloud (Szabó et al., 2019). Similarly, factory machines’ reliability factor can drastically reduce with the outdated decisions for the critical sporadic events happening at the factory floor (Cinque et al., 2018). What if the DT in the cloud is deployed or shifted towards the factory network’s edge layer, i.e., at the factory gateways, industrial controllers, cluster of machines, 5G gNodeB (gNB). This emerging new cloud computing architecture named “edge computing” can address these drawbacks and brings new novel analytics and control strategies at the network edge.

4.1.3. DT deployment at the edge for critical communications
The edge servers at the factory network can take data readings from physical entities locally, store and pre-process it, make advanced computations, and have cloud-assisted analytics and real-time control (Mahmud et al., 2020). Moreover, the computational power of edge network’s end devices (i.e., IIoT and CPS-based machines) is increasing with time (Shih et al., 2016). These computing resources at the underlaying edge architecture can bridge the gaps for a new class of smart vertical industries in tandem with cloud computing. The local edge-based digital twin (EDT) can be independently created from the heterogeneous streams of incoming data, or a copy of the CDT model can be provided at the network edge. The CDT continuously gets updates from the local EDTs that is running close to the factory physical layer, as shown in Fig. 7. In either case, EDT brings flexibility and agility to the decision-making process for critical events, i.e., insight for performance optimization in machine processes, abrupt anomalies, and disaster situations.

Table 4 shows that security, latency, and reliability are critical requirements for smart manufacturing, smart grids, and intelligent vehicular domains. Bringing DT capabilities from the cloud layer to the edge devices or servers indeed reduces the impact of latency and decision reliability as it lessens the cloud dependency by making the critical decisions locally at the EDTs. Moreover, while continuous transmission of big data from factory to cloud can be costly and vulnerable to data breaches, edge-based pre-processing can reduce such concerns. In Lu et al. (2020a), the authors addressed the security requirements of users’ data in edge computing by integrating the DT and blockchain technology at the edge layer, which increases the robustness of the IoT networks. The computation of sensitive factory data can be performed at the edge layer to facilitate EDT. In the event of disconnection from the cloud, analytics can still run at the edge device, keeping the real-time continuous self-learning and evolving EDT with time. EDT can update the CDT once the connection restores, thus increasing the resilience of the smart industry network.

4.2. Data-driven computational intelligence paradigms
Computational intelligence represents a collection of mathematical/computational models and various tools/algorithms from data science and optimization theory. CI aims to learn and approximate a mathematical model based on the set of provided sample data, collected from industrial process and machines. It offers many advantages for industrial DTs, e.g., (1) it can help in predicting disaster events and anomalies while bringing intelligence to various DT-based applications (Louridas and Ebert, 2016, Yang et al., 2021), (2) cloud/edge-based CI in numerous ML/AI-based big data analytics techniques can pre-process and analyze the incoming multi-heterogeneous and complex raw data of industrial assets (Chen et al., 2019), and (3) it leads to enhanced insight discovery, self-optimization, and decision making in the industrial process. In the following subsections, we shed some light on recent works and trends in the CI paradigms that can aid the Industrial DTs implementation.

4.2.1. Data sources, pre-processing, and data fusion for analytics
Application of data analytics framework on a continuous stream of incoming time-series factory data plays an essential role in the perpetual update of the DT at both cloud and edge (Zehnder and Riemer, 2018). In smart industries, generated data can be classified into two categories based on the source of their origination at the physical layer, i.e., factory field data and factory management data (Meski et al., 2019, Qi and Tao, 2018).

1.
Factory field data, composed of multiple data inflows from the physical layer of an operating factory. For example, environmental data related to air quality, temperature, humidity, and other essential data linked with machine performance is collected from IIoT and CPS.

2.
Factory management data, carrying information on product planning, design schematics, service management, and finance, originate from the numerous information and computer-aided systems, such as manufacturing execution system (MES), enterprise resource planning (ERP), computer-aided design (CAD), and computer-aided engineering (CAE).

This inflow of data at both the edge and cloud layers forms the building block for realizing and updating CDTs and EDTs. However, the underlying physical layer’s raw data is barely useful because of the multi-source and multi-scale, heterogeneous, and highly noisy data nature (Wan et al., 2020). Hence, pre-processing of the data is needed before any ML-based analytics operation is applied to extract the valuable information for efficient simulation of DT at the edge and cloud layer. Moreover, data fusion techniques can be applied during the pre-processing step, where data from multiple data sources are fused for constructing accurate and reliable insights (Xiang et al., 2018).

Streaming Analytics for EDTs. Traditional data analytics store the data first and then analyze it to extract insightful data patterns. In the new streaming analytics model, incoming time-series data are continuously analyzed while the machine processes are still in progress at the factory floor (Hill et al., 2017). Afterward, the processed data is stored for batch analysis. Moreover, traditional analytics at cloud and edge needs to store data first before any further analysis. However, as discussed in Section 4.1.2, CDTs can induce large RTT latencies. The synergy of both streaming data analytics and edge architecture increases the agility in EDTs to address the stringent low-latency and mission-critical events (Rehman et al., 2018). Moreover, this synergic mode leads to better and faster insights at the EDTs to act locally on critical events and make the all-important decisions.

4.2.2. Emerging trends in ML, AI, and data optimization algorithms
Modern trends in the field of ML and AI can enhance the operation of CDTs and EDTs across multiple industries. ML/DL-based frameworks are typically built in the cloud and edge layers to model and classify the performance parameters from industrial data, which are used to update the industrial DT. However, the nature of time-series industrial data, originating from various machine processes, is different; it has large volume and dimensionality, and varying degrees of correlation and sensitivity depending on the time cycle (Min et al., 2019). Hence, conventional ML techniques, such as regression and classifying techniques, cannot be applied. Therefore, new emerging ML approaches need to be explored for meeting the EDT requirements of low computation and better control on insights, while ensuring security and communication needs (see Table 4).

Artificial Neural Networks (ANN). The ANN algorithms, which are based on interconnected collection of artificial neuron units (i.e., computation units), are used extensively in numerous industrial applications. For example, for anomalies detection in collaborative machines and safety precautions in factories, visual perception sensors like industry-grade video cameras are installed, which continually produce a time-series visual data (Ho et al., 2019). In such computer-vision-related tasks, convolutional neural networks (a class of ANN) has extensive usage, and perform significantly better, especially on a cameras-originated perceptual data class that has an inherent property of local relationships among spatial dimensions inside images (Lemley et al., 2017, Coelho et al., 2021).

Generally, CNNs had two parts: (1) feature extractors that learn features from raw data, and (2) trainable multilayer perceptron (MLP), which performs classifications based on input from learned features. However, the traditional ANN models lag the support for performing spatio-temporal analysis on time-series as they do not use the past historical observations and information acquired in the previous steps of the learning/training process. For this purpose, various causal convolutional filters of CNN units are designed and utilized to use past information for learning long-term correlation in time-series data for accurate prediction (Brownlee, 2018 Chap. 3). CNN applied on the perceptual data at the edge has the potential of continuously updating the EDT in real-time to detect and respond to anomalies appropriately. Similarly, recurrent neural networks (RNNs) and their extensions, i.e., gated recurrent unit (GRU) and long short-term memory (LSTM)-based neural networks, has also an inherent property of modeling the past historical observations and spatio-temporal analysis on incurred time-series machine data for prognosis and forecasting applications (Ma and Mao, 2020).

Deep Reinforcement Learning. Reinforcement learning (RL), a subset area of machine learning, has two components, RL agent and RL environment. RL agent can be a neural network or policy-based agent that can interpret and perceive changes in the state of the RL environment (i.e., factory process, machine) (Polydoros and Nalpantidis, 2017). Based on the type of changes in the environment (i.e., desired state or undesired state), it can assign positive or negative rewards, take actions in the RL environment, and learn through trial and error. Generally, there are four categories of algorithms in RL systems (Arulkumaran et al., 2017): (1) policy-based algorithms, which learn policies by creating the different path of states to reach an objective of optimization, e.g., REINFORCE, (2) value-based algorithms learns to evaluate function value (i.e., Q-values) of states and actions, e.g., Q-learning, (3) model-based algorithms predict the state of the environment given the current state and action, e.g., Monte-Carlo Tree Search (MTCS), and (4) hybrid algorithms to overcome the shortcoming of individual algorithms, e.g., Actor–Critic algorithms such as deterministic policy gradients (DPGs), which combine the strengths of policy-based and value-based functions. The combination of deep learning (e.g., CNN or ANN) with RL leads to powerful Deep RL (DRL) techniques that enable solving complex problems. The examples of DRL techniques are trust region policy optimization (TRPO), Deep Q-Network (DQN), Double-DQN (DDQN), Deep recurrent QN (DRQN), Deep DPGs (DDPG) Deep attention RQN (DARQN) and asynchronous advantage actor–critic (A3C) (Luong et al., 2019).

DRL has also seen an exponential rise in its usage for a wide range of industrial DT-driven data processing and resource provisioning applications. For example, Adhikari et al. (2021) used Deep RL-based edge-cybertwin framework for designing intelligent resource provisioning strategies and supporting dynamic service requirements in 6G-enabled Internet-of-Everything (IoE) applications. The collected data near the edge is stored and processed using the Support Vector Machine (SVM) classifier model to aid DT implementation further. The proposed framework decreases the average energy consumption and communication delays by 15% over the considered baseline algorithms (i.e., Local Execution, Random Execution, and Server Execution). It increases the accuracy of feature extraction and predictive analysis on collected data at the edge server by 12%.

Swarm Intelligence (SI). The SI algorithms provide practical AI tools based on nature-inspired swarm behavior to solve complex problems (e.g., optimization, scheduling, classification). SI-based tools are widely applied in diverse applications, e.g., wireless communication, robotics, data mining, cluster analysis, medical applications, process optimization, and automation industry (Yang, 2020). The U.S. military, NASA, and European Space Agency consider SI techniques for controlling unmanned aerial vehicles (UAVs) and orbital swarm for next-generation satellites and planetary mapping. The algorithms are designed on the collective swarm behavior of self-organized and decentralized autonomous systems comprising of artificial local agents, and they interact/exchange heuristic information among themselves and with the environment. It leads to the adaptive search behavior in complex optimization problems and converges towards a global solution. Using this swarming behavior, a myriad of swarm intelligence algorithms are proposed over time, which can be beneficial for implementing industrial DT to tackle complex problems. Some of the SI-based algorithms are: (1) stochastic diffusion search (SDS), which is a probabilistic optimization technique suitable to find global search in multi-objective problems (Maroufpoor et al., 2020), (2) ant colony optimization (ACO) algorithm and its variants are also a probabilistic technique, which takes inspiration from ant colony principles to find better paths using graphs. ACOs are better suited for solving combinational optimization problems, i.e., traveling salesman problem and quadratic assignment problem (Alarifi and Alwadain, 2021), (3) firefly algorithms (FA), which are based on a swarm of fireflies lighting pattern (RM et al., 2020), and (4) particle swarm optimization (PSO), which is a global optimization algorithm used in real-parameter optimization problems and it has better performance over other global minimization strategies, e.g., simulated annealing, to avoid local minima problems (Piotrowski et al., 2020).

The latest development in swarm intelligence is the arrival of artificial SI or human swarm methods that is based on using the collective intelligence of connected human beings to a real-systems (Rosenberg and Willcox, 2019). The connected human beings to real-systems acts as a dynamic swarm for finding and converging towards the final solution in complex problems. Similarly, a new swarm-based algorithm, Harris hawks optimization (HHO), has also seen widespread usage in many critical applications. HHO finds its roots in the natural phenomenon of hawks groups working in collaboration to hunt and prey, enabling efficient and quick solutions (Alabool et al., 2021). For example, Pham et al. (2020b) tried to solve the sum-rate maximization problem for the visible light communications (VLCs) integrated with non-orthogonal multiple access (NOMA)-enabled UAV communication using the nexus of HHO method and feedforward neural network (FNN). The proposed HHO-FNN trainer avoids the local minima trap as compared to the conventional trainers. The overall gain in sum-rate is achieved by jointly optimizing NOMA power allocation and UAV placement in real-time using the proposed HHO-FNN trainer compared to independently optimizing power and UAV placement.

Evolutionary Algorithms. In CI, evolutionary algorithms (EA) are a subset of evolutionary computation-based learning, derived from the Darwinian evolution principles (i.e., reproduction, mutation, recombination, and selection) (Slowik and Kwasnicka, 2020, Maier et al., 2019). Some prominent EA are differential evolution (DE), evolution strategies (ESs), genetic algorithms (GAs), and estimation of distribution algorithms (EDAs). These EA algorithms are essentially heuristic search methods, with the desirable properties of high efficiency, robustness, and flexibility to achieve the optimal global solutions, which can be beneficial for industrial DT applications (Bozorg-Haddad et al., 2017, Mirjalili, 2019). Moreover, EAs are essential for future asynchronous grid computing (parallel computing) environments for their ability of, (1) easy data processing in parallel, (2) operating without fitness gradient information, (3) high probability of quickly finding near-optimal solution in the complex optimization process, and (4) ability to avoid local minima where deterministic optimization generally fails or renders inapplicable.


Download : Download high-res image (680KB)
Download : Download full-size image
Fig. 8. Federated learning methods based on: (a) centralized aggregating and computing approach, (b) decentralized aggregating and computing approach, and (c) fully-distributed aggregating and computing approach.

Among new variants, the multi-objective (MO) EA framework based on decomposition method (MOEA-D) is gaining attention due to its efficient problem-solving performance in complex data optimization problems (i.e., Pareto optimization) and intelligent decision-making applications (Coello et al., 2020). In the MOEA-D method, the set of pre-defined weight vectors is used to decompose application-specific multi-objective optimization problems (MOP) into several single-objective subproblems, which are solved cooperatively in parallel using a selected EA framework. The configuration of weight vectors plays a critical part in finding the final solution set since they determine the search directions in MOEA-D methods. Many strategies have been introduced in the literature to counter this issue, e.g., pre-defined uniformly distributed weights, adjusting weights on random, adaptive weights, or using the simplex method to adjust vector weights. Ma et al. (2020) provides a comprehensive survey on various adaptation strategies proposed for weight vectors in MOEA-D methods, their benchmark performance, suitable test applications, and current limitations and challenges are discussed.

Artificial Immune Systems (AIS). The AIS techniques are composed of computationally intelligent and adaptive algorithms based on rule-based ML systems. AIS working process and principles are stimulated from the human vertebrate immune systems (Dasgupta et al., 2011). AIS algorithms are generally modeled after the three types of immune systems, i.e., innate, adaptive, and passive, which use the system’s characteristics of memory and learning for solving complex data optimization problems (Fernandes et al., 2017). Some of the examples of AIS algorithms are immune network algorithms (INA), clonal selection algorithm (CSA), negative selection classification algorithm (NSCA), and dendritic cell algorithms (DCA). The inherent property of AIS-based algorithms is their robustness against unwanted viral attacks and malicious access to collected data, which can be exploited to address the challenges faced by IIoT-based DT communication (Aldhaheri et al., 2020). Gao et al. (2020) addressed the large-scale problem in cloud-based microservices architecture to address the QoS requirements of end-users using the proposed hybrid AIS algorithm based on integrating INA and CSA frameworks. Microservices are easier to deploy, manage and scale using orchestrating tools (e.g., Kubernetes, OpenStack), leveraging the cloud-native computing environment to provide customized services. Thus, the large-scale problems in the complicated composition of inter- or intra-related container-based microservices inherit the characteristics of complex multi-objective NP-hard optimization. The authors in Gao et al. (2020) showed that their proposed metaheuristic AIS algorithm’s combined strength outperforms the selected variants of single AIS algorithms since it uses the hybrid advantages of monoclone, multi-clone, and co-evolution properties of immune systems. Moreover, their algorithm efficiently solves the complex problems for the multi-cloud environment, which they numerically verified and compared against each selected AIS algorithm.

4.2.3. Emerging trends in computing techniques
ML/AI algorithms running on a single computing machine with a centralized computing infrastructure are insufficient for the multi-heterogeneous and enormous volume of data generated in factories (Khayyam et al., 2020). Moreover, increasing resources on a single computation machine or complexity of DL frameworks by adding more fully connected hidden layers of neurons to learn all the performance parameters is not a go-to option for industrial big data. A different hierarchical approach of computing-based ML ecosystem can be adopted, which varies on the type and degree of hierarchical distribution. The possible approaches include:

•
Decentralized ML Computing approach, in which various computing edge servers share the data set with other connected edge servers (so called peers) for computations, wherein no single master node (e.g., cloud) has centralized control over ML computations.

•
Fully Distributed ML Computing approach, where a master node performs the big data-based ML computational task by sharing the data set among the connected peers while having a control over them (Al-Gumaei et al., 2019).

The discussion in Section 4.1.2 leads to a vital lesson that moving DT from the cloud to edge can bring many novel applications in smart industries. However, the lack of an efficient ML framework and a large volume of generated data inflow incurs high computational demands requiring distributed ML approaches.

Data Parallelism-based Distributed ML. To handle this issue, data parallelization technique called, data parallelism, can be applied to the partitioned machine data (Fan et al., 2019). In data parallelism, the industrial data is initially split into multiple data cells (e.g., at a cloud). Then, each data cell is shared to the connected computational nodes through a communication network. Further, each node learns the optimized parameters and shares its output with the peers to update their learned parameters until they reach consensus on the learned parameters and submit it back to the cloud. Data parallelism-based distributed ML approach addresses the efficient management of large ML computations at the cloud and edge. However, the sharing of sensitive data by the master node (cloud) to computational devices on edge servers at the factory layers incurs particular challenges in data parallelism-based distributed ML approach, e.g., (1) potential risks of data breaches and data poisoning, and (2) inducing large latencies remains large which renders slow updates to the EDT and CDT (Chen et al., 2021, Qu et al., 2018).

Federated Learning Approach. To address the above mentioned issues, another ML approach, called “federated learning” can be used, which utilizes a model parallelism technique instead of data parallelism. In model parallelism, the learned ML model or framework is shared by the master node with the computational devices (Qu et al., 2020). During training, the master chooses the ML framework and transmits it to the devices, each learning separately on the locally generated data. The devices (i.e., edge or field devices) train without exchanging any local data. Afterward, all the devices share the optimized trained ML frameworks with the master, which pools the received trained models and selects the best global model for further inference. FL approach can address the data-security related issues and provide the real-time continuous learning evolution at the twins at both the cloud and edge layers.

Fig. 8 shows the various federated learning methods for CDT and EDT implementation based on the hybrid approach of model aggregation and computing techniques. Using federated learning, a DT-based edge architecture is proposed and analyzed for IoT network in Lu et al. (2020b), which develops and trains the twin model on the devices’ data. Results from Lu et al. (2020b) show that real-time optimization of resource allocation to network is achievable using federated learning, even without uploading data to the cloud.

Immunocomputing Approach. It provides a foundational basis for AIS-based computations (Tarakanov and Skormin, 2002). This new computing approach is different from the traditional computing architectures since the vision of immunocomputing information processing hinges on the fundamental computing unit of “formal protien” (FPs) and “formal cells” (FCs) in immunocomputers (Tarakanov et al., 2003). It is mathematically modeled against its biological prototype. The goal of immunocomputing is to provide a new kind of computing hardware based on an enhanced mathematical framework that solves the user-defined complex problems in smart systems (e.g., pattern recognition, vision applications, knowledge-based learning) while simultaneously protecting against intrusion, errors, malware, and viruses. Thus, it can provide a secure and robust new computing paradigm that can be utilized for complex problems in industrial DT implementations. One of the AIS-based INA algorithms, formal immune networks (FINs), are designed based on the idiotypic network theory proposed by Niels Kaj Jerne, and they can perform supervised and unsupervised learning and solve complex optimization problems. Bataineh and Kaur (2021) used the immunocomputing-based approach using the CSA algorithm to find the best hyperparameters to optimize the network topology of the LSTM network instead of adopting gradient configuration (trial and error approach). Thus it reduces the time consumption in finding the best LSTM topology for knowledge-based semantic analysis and removing the human intervention factor. Similarly, Said and Mostafa (2020) proposed a hybrid immune algorithm by integrating NSCA with the danger theory model providing a self-learning mechanism that enhances the database security in terms of data integrity, confidentiality, and availability while efficiently detecting intruders with low false detection rates. By adopting immunocomputing-approach in AIS-based intelligent system for the implementation of data-driven industrial DT, it can provide enhanced and secure DT services for different industrial areas identified in Table 3.

4.3. 5G-and-beyond/6G networks and AoI-aware Green communication
Future digital industries need a service-based B5G/6G wireless network capable of: (a) satisfying the stringent mission-critical communication requirements of Table 4, and (b) optimizing the radio and core network resource allocations for the diverse factory floor services (Morocho-Cayamcela et al., 2019, Saad et al., 2019). Integrating DT with 5G networks leads to a simulated end-to-end software replica of the underlying industrial network (Sun et al., 2020, Nguyen et al., 2021). It can ensure the critical communication requirements for factory floor of Fig. 7 (Physical Resource Layer) by having continuous analysis, predictions, and recommendations to provide hybrid 5G network services.

4.3.1. Industrial B5G/6G wireless connectivity and services
5G-and-Beyond wireless networks are constantly evolving to provide service-based wireless access to the futuristic industries with services as ultra-reliable and low latency communication (URLLC), enhanced mobile broadband (eMBB), and massive machine-type communication (mMTC) (Cheng et al., 2018).


Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 9. Features expected from B5G/6G in terms of industrial DT usage, objective and what technology enables these features.

Industry and Standardization Bodies. Unique to 5G-and-Beyond networks is the all-out effort from telecom standardization bodies (e.g., ETSI, 3GPP), regulators, service providers, operational technology (OT) companies, and manufacturers to transfer the technological advancements to industrial domain. In particular, key industry bodies from the manufacturing sector, which are the market representative to 3GPP, like 5G automotive association (5GAA), 5G alliance for connected industries and automation (5G-ACIA), and the critical communication association (TCCA) proffer regular inputs to the 3GPP. The main objective is to break historic silos between industrial and wireless communities in designing the beyond 5G networks according to the industrial needs (Ho et al., 2019). Many industries have opted for 5G for OT connectivity to achieve secure and safe manufacturing, productivity and efficiency (IET, 2019).

Private 5G Networks. From Brown, 2019, IET, 2019, it is evident that private 5G networks are provisionally designed for industrial use cases, that can provide industries standalone dedicated resources and services rather than conventional mobile networks. It creates an opportunity for employing DTs at the 5G radio access network (RAN) layer with dedicated computational resources available and closer to the factory floor to learn, predict and make the decisions locally while communicating with the CDT in the cloud (Fig. 7, Cloud Layer). Moreover, the vital advantage for enterprise users in this new approach is designing the private and reliable mobile network according to their needs, which can satisfy the broad-scale coverage, stringent latency and reliability requirements, and security of industrial communication Mahmood et al., 2021, Brown, 2019.

Significance of EDTs in Beyond 5G. The EDTs can also facilitate the B5G network performance by optimizing the resources for: (1) a single service, e.g., high bandwidth allocation at millimeter-Wave (mmWave) bands or employing digital beamforming at multi-input multi-output (MIMO) antennas to provide eMBB service to camera operated remote control operations, or (2) dynamically optimize network slicing to simultaneous support traffic of all three 5G network services, i.e., eMBB, URLLC, and mMTC (Sun et al., 2020). Meeting these requirements by twin-enabled 5G networks is fundamental in realizing the new era of mission-critical applications. Similarly, in multi-hop IIoT-based sensors networks, the approach of cooperative communication can be used to disseminate the new factory data information (Vitturi et al., 2019). This approach dramatically improves information packets’ reliability by reaching the sensor network’s gateway (i.e., 5G gNB) in any direct communication failure. For this approach to work, M2M timing synchronization (M2M sync) between the multiple collaborating machines (shown in Fig. 7, Physical Resource Layer) is crucial (Mahmood et al., 2018). Three types of M2M sync are to be achieved among factory machines, as shown in Fig. 7. The propagation delays of signals from gNB to devices can be adjusted using a timing advance (TA) mechanism to estimate over-the-air propagation delays (Mahmood et al., 2018). EDTs can also efficiently distribute standard time, e.g., universal time coordinated (UTC) information, in TA mechanism to all the factory machines for accurate M2M sync in M2M communication. As a result, the reduction in latency and increase in the reliability due to these approaches increases the successful dissemination of new and periodic data from the machines to the nearest edge server running the industrial DT (Dong et al., 2019).

4.3.2. Computational intelligence in B5G/6G networks
5G/B5G are rapidly introducing capacity and reliability enablers such as mmWave, massive MIMO, and others, to provide service-based massive wireless connectivity. Simultaneously, integrating data-driven CI will allow 5G-based communication systems to support enhanced AI-native service-based architecture built upon the collected network data.

CI-native B5G/6G Networks. The futuristic vision of 6G network design has been stepping forth based on the amalgamation of trending ICT and data-driven technologies in tandem with the cloud-native computing model to the 5G core network, increasing the prospects of new industrial services (Zeb et al., 2021, Zaidi et al., 2020). Moreover, CI-native intelligent network functions (i.e., intelligent network slicing and traffic routing) at various computing and networked communication layers of Fig. 7 enables the dynamic reconfiguration of network resources and provides a higher degree of enhancements in industrial DT-based services (c.f.  Table 3). Similarly, the nexus of various CI enablers and their usage, discussed in Section 4.2.2, i.e., ANN, DRL, swarm intelligence, EA, and AIS techniques, facilitate the efficient solution of numerous application-specific multi-objective optimization problems in 5G/B5G-based systems (Ji et al., 2021). In Table 5, we summarize the research work in terms of application scope, objective, methods and outcomes of CI-based techniques in solving various challenges of 5G/B5G-based systems. This brings a significant level of advantages for the implementation of industrial CDTs and EDTs, which are poised to give enhanced industrial services in terms of massive industrial connectivity, real-time communication control, and monitoring feedback (c.f.  Fig. 9). Similarly, new features and enablers expected from B5G/6G networks for industrial DT are identified and summarized in Fig. 9.


Table 5. Comparison of various application scenarios for usage of CI in 5G/B5G.

Articles	Scope	Objectives	Methods Used	Results
Jia et al. (2020)	Nexus of Satellite & Terrestrial networks towards B5G wireless system	Intelligent resource management and allocation hierarchical framework utilizing spectrum sensing and prediction feature to ensure high spectral efficiency and low co-channel interference	SVM classifier
& CNN Model	Results demonstrate that the system achieved lower
error detection probability in spectrum occupation while better spectrum utilization efficiency using proposed intelligent resource allocation scheme.
Sakib et al. (2021)	Multiband B5G-
enabled Massive IoT	Effective and lightweight predictive
channel assignment technique based on signal-to-interference-plus-noise (SINR).	Shallow & Deep-
CNN Models	The simulation results shows the accurate predictions
of channel conditions & band selection with low co-channel interference for multiband relay systems in comparison with other benchmark channel estimation strategies.
Shehzad et al. (2021)	Terrestrial small-cell base
stations (TSBSs) with the UAVs-based Aerial Network	Joint positioning of UAVs &
the TSBSs association such that the total system sum-rate is maximized within communication-related constraints (i.e., bandwidth, UAV heights, power limit, and backhaul data rates).	Genetic Algorithm (GA)
& k-means clustering	In comparison with k-means algorithm, the simulation
results reveals that the GA-based technique outperformed k-means in maximizing system sum-rates throughout all the evaluation parameters.
Ari et al. (2019)	Resource Allocation Frame-
work for 5G Cloud-RAN (CRAN)	Logical joint mapping among the
end-user and the remote radio head (RRH) and additionally between the RRH and the baseband processing unit (BBU), while ensuring user QoS and QoE and lowering overall network expenses.	Artificial Bee Colony
& Ant Colony Optimization	The computational results shows that the proposed
Bee-Ant-CRAN framework decreased the wastage of computational BBU resources while greatly improving the spectral efficiency and throughput.
Abedin et al. (2021)	Elastic Open RAN Slicing
for the IIoT Networks	Formulating the O-RAN slicing problem for
real-time industrial monitoring and control, while satisfying AoI and energy consumption constraints of IIoT devices, additionally maintaining the O-RAN slice isolation.	Distributed Matching
Game & Deep RL Algorithm (DDPG)	In comparison to the baseline techniques, the
simulation results shows that the recommended solution served an average of 50% and 43.64% extra IIoT devices, enhancing the required performance gain for IIoT services.
Kumar et al. (2021)	Security and privacy-
aware Artificial Intrusion Detection System for B5G	Developing a privacy-preserving technique for
secure edge intelligence through integrating AIS with Federated ML in B5G networks, utilizing the strategies of Paillier Homomorphic encryption and differential privacy techniques	Federated ML &
Clonal Selection Algorithm	The experimental results shows that the proposed
scheme outperforms the traditional ones in terms of selected performance metrics (i.e., computation overhead, communication efficiency, and accuracy).
Multi-access Edge Computing. The significance of edge computing architectures in CI from the DT’s perspective is explored and discussed in Section 4.1.2. Meanwhile, the emerging trend in B5G networks is “multi-access edge computing (MEC)”, which is a part of edge computing techniques. In MEC paradigm, 5G gNBs are integrated with the hyperconverged computation infrastructure (HCI) comprising of the high-power data analytics (HPDA) capabilities and data-storage resources (Pham et al., 2020a). MEC’s inclusion in the B5G/6G networks helps in moving the cloud computing capabilities towards 5G RAN; thus, decreasing the application latency, traffic congestion at backhaul, and improving the end-users’ quality of experience (QoE) and QoS. Fig. 7 (Edge Layer) shows the MEC concept in DT-enabled smart industry for offloading, storing, and computing the factory data at the MEC located at 5G RAN layer. Combining MEC with ML and AI models (shared by the cloud using federated learning approach) and local analytics are essential in developing the new agile class of EDTs for the smart industries. As a result, EDTs can learn and render the simulation of the entire local manufacturing facility through the inflow of large volume of data at 5G gNB (Dong et al., 2019). As discussed in Section 4.3.1, these new classes of EDTs at 5G RAN, trained on real-time data of the local facility, can provide better network services and fulfill various constrained requirements of use cases mentioned in Table 4.

Security Concerns, Challenges, and Countermeasures. Introducing CI in the 5G/B5G network paves the way for the building block towards the futuristic vision of CI-native B5G/6G networks that brings exciting new features. However, their union incurs new security challenges in securing sensitive data and core network pieces of equipment. In CI-enabled B5G/6G networks, data is an integral part of developing intelligent systems, and compromising the data integrity affects the network performance since the model development process is altered. Moreover, the development and training of AI-based models entirely relies on the continuous access to massive collected data from sensitive equipment and utilize the critical information (Yang et al., 2020). However, the cybersecurity aspect of any traditional system involves the restriction of cyber access to sensitive data and components. Also, The data retrieval, pre-processing, and dataset labeling for accurate model development are particularly resource-intensive. The AI model training and usage are frequently performed outside the organization, which poses the most significant security threat since it increases the chances of attacks, i.e., data poisoning in adversarial attacks, during the data supply chain. This increases the importance of secure and efficient data sharing protocols, ensuring the seamless integration of data sharing mechanisms with AI systems. Ilahi et al. (2021) explores the adversarial attacks on DRL-based systems in various real-life critical systems of 5G/B5G applications, e.g., smart grids, traffic controls, and autonomous vehicles, and provides an overview on challenges and countermeasures techniques, i.e., adversarial training, robust learning, adversarial detection, defensive distillation, and game-theoretic approaches. On the other hand, the trend in softwarization of B5G networks increases the risks of software flaws and backdoor access which increases the sensitivity of specific types of equipment in the B5G network, e.g., 5G gNBs, network management and control functions (Samdanis and Taleb, 2020). The nexus of federated learning and private 5G networks can provide the on-premise softwarized services, enabling total control of a secure 5G network within the enterprise organization. On this aspect, the ETSI Industry Specification Group on Securing Artificial Intelligence (ISG SAI) is currently working towards the rapid expansion of AI usage in the new industries, creating new 5G standards, and using AI to enhance security by mitigating attacks and preserving the AI model development process (ETSI, 2021).

4.3.3. Green Communication
Smart factory is tied to denser and wide-scale monitoring and control of sensors and actuators through low-complexity IIoT devices, often deployed in harsh and inaccessible locations without grid power (Zeb et al., 2020b, Zeb et al., 2020c). In such scenarios, communication of battery-operated devices needs to be carefully optimized since communications are typically the most energy-draining operation. Additionally, offloading of computations workload from end-users (machines) to edge layers at 5G RAN or dedicated standalone edge server can save industrial sensors’ power consumption, extending their battery life (Mumtaz et al., 2017). However, solutions based on battery-operated devices suffer from various concerns such as network lifetime, and environment unfriendly and costly battery recycling and replacements, respectively. To overcome the battery-related challenges, different energy harvesting (EH), wireless power transfer, and backscattering-based wireless networks are being investigated (Zeb et al., 2020a, Jameel et al., 2020, Zeb et al., 2019, Nazar et al., 2021). The challenge remains on the refresh rate of EDTs and CTD, which, depending on the application, affects the querying rate of devices and might mismatch with their energy renewal rate.

4.3.4. Age of information
Numerous industrial applications rely on the updated data collection over time, while the data must possess the property of having new and fresh information, i.e., the minimum AoI (Abbas et al., 2020, Abbas et al., 2021). Table 4 shows the update time of generated periodic traffic in various industrial use cases. AoI of the collected machine data forms the important performance metric for critical decision-making processes as the data value reduces with the elapsed time (Sinha and Roy, 2019). Hence, minimum AoI is desired for reliable and agile decisions. For this purpose, one strategy is to cache the (periodic and sporadic) data updates from IIoTs at the edge servers, which are readily accessible to the upper layer applications, resulting in minimum AoI (Wu et al., 2017). This problem introduces the tradeoff for cloud and edge-based strategies in providing data updates with either minimum data AoI, achieved by frequently polling each machine and sensing device, or with aged information from cached data at the server. As discussed in Section 4.1.2, EDTs and CDTs have access to the incoming periodic data traffic stored in the cache with different AoI. The authors in Corneo et al. (2019) proposed the AoI-aware scheduling policy together with learning at EDT and CDT, which can dynamically address the tradeoff between minimum AoI of data and cached data from field devices in a large wireless network.

On the other hand, because of EH constraints (or green communications) of IIoTs and scarce radio resources, not being able to entertain and reply to all sensing (application) requests from the factory terminal layer (illustrated in Fig. 10) leads to deterioration in the AoI metric of sensed data. Therefore, it is crucial to develop intelligent network slicing schemes with multi-objective resource allocation criteria. Each objective must capture the requirements and requests of industrial applications realistically using critical metrics as service rate, scheduling and isolation, information freshness, and energy efficiency. In this direction, in Abedin et al. (2021), using distributed game theory and machine learning, the authors developed an elastic network slice policy to satisfy time-varying resource allocation demands for three different industrial traffic classes. In the slice configuration policy, the authors mainly aimed to balance AoI and energy efficiency while maximizing the service rate.

5. Lessons learned, challenges, and future directions
In this section, we discuss various important observations, recommendations, and open future research challenges/problems associated with the practical utilization of industrial DT in conjunction with the emerging communication and computation technologies.

5.1. Lessons learned and challenges
Based on the systematic review presented in the previous sections, the key practical lessons and recommendations are as follows.

•
It is clear from the summarized review (c.f.  Section 3) that industrial DT has mostly been explored for the factory processes associated with robotic production, prognosis, and devices health management applications. The inclusion of DT technology in such applications brings significant gains over traditional optimization methods (e.g., physical modeling, geometrical modeling) since it allows to incorporate the command and management/behavior modeling aspects using living softwarized models. Implementing and simulating these DT-driven softwarized models utilizes the fused (physical and virtual) data, past historical data, real-time data, and simulation data, resulting in an accurate depiction of real situations. It enables support for back and forth real-time control and monitoring feedback between factory floor machines (physical entities) and softwarized replicas, leading to an agile decision-making process. Nevertheless, contemporary research on DT focuses more on a single machine and/or equipment. However, this limits the scope of industrial DT applicability to the entire manufacturing floor covered with multiple collaborating robots and operational machines, which requires exploring trending ICT techniques to facilitate the industrial DT’s build-up. Moreover, there is no consensus on the general design framework for industrial DT; therefore, a unified research and development effort is needed towards DT implementation.

•
The integration of industrial DT technology with trending cloud/edge-based data computing methods and enhanced core network connectivity through software-defined networking (SDN) and network function virtualization (NFV) technologies are paving the way to move cloud DTs (CTDs) closer to the factory manufacturing floor with edge DTs (EDTs). This move will deliver exciting new enhanced security and privacy features, and provide high reliability and low-latency towards increasing the agility of decision-making processes. Moreover, providing the cloud computing capabilities at the edge layer can provide the deployment infrastructure for cloud-based microservices, which is easily accessible by EDTs to enhance their operational capabilities in processing IIoT data from heterogeneous sources, performance monitoring, and optimization. However, the computation capabilities and networked connectivity (wireless and wired) at the edge and cloud layers to support the CDT and EDT-driven factory operations is quite challenging because of: (1) insufficient computation resources, (2) non-optimized network architecture to support high data traffic flows, (3) complexities in software and hardware configurations of networking infrastructure, (4) the lack of DT-aware network communication protocol, and (5) the geographical distribution of clouds.

•
Data fusion, acquisition, and mining will play an essential part in giving true meaning to CDT and EDT functions realization. Altogether, these techniques will effectively link the cyber and physical spaces of the manufacturing floor by simultaneously processing and fusing the multiple features of acquired time-series machine data from multiple heterogeneous sources (physical space) with past data records, behavior, and simulation data (cyber-space). However, data fusion and mining at such a massive scale for CDT and EDT incite the availability and implementation challenges of robust, computationally efficient, and resilient CI techniques that can accurately model and fuse the DT data.

•
The emergence of novel ML/AI frameworks with a de-facto hybrid cloud/edge-native computing architectures will play an integral part in stimulating the numerous functional aspects (i.e., prognosis, simulation) of CDTs and EDTs. Especially the use of streaming data analytics and distributed federated learning-based computation techniques will significantly improve the security, privacy, low latency, and reliability aspects at CDT and EDT. All in all, these paradigms will undoubtedly enhance the data computations of the incurred data, accurate prognosis, and agile decisions in DT-driven industrial processes. However, many challenges exist for the complete adoption of ML/AI frameworks for industrial DT applications:, such as, (1) the need for large high-quality labeled data for training ML/AI algorithms, which increases the importance of preprocessing raw data and data fusion techniques, (2) current networking infrastructure lags the computational support for AI-based service deployment and solutions, (3) malicious exchanges of learning model updates during the federated learning process can affect the integrity of DT models, and (4) local and global aggregation of the learned models at the EDT and CDT varies from application to application.

•
From the discussion in Section. 4.3 and Fig. 9, it is clear that B5G networks will play an essential role in providing diverse wireless connectivity services (e.g., URLLC, eMBB, and mMTC) between industrial DT and physical space (manufacturing floor). Moreover, the innovative edge intelligence vision of 6G is all about integrating state-of-the-art AI services and hybrid edge-native computing models to provide on-demand services to various applications, which will extend the service-based architecture of B5G to support the objectives of Industrial DTs, e.g., softwarization of industrial processes, support for data computations near the edge layer (MECs), optimized resources for wireless data transfer in a harsh indoor multipath-riched industrial environment, AR/VR support for visualization. However, there are critical challenges in developing standards and fully adopting the emerging technologies and enablers (SDN/NFV, mmWave bands, MIMO, MEC, etc.) in B5G/6G cellular networks to build a universal B5G-based wireless ecosystem for industries. So far, these technologies are not technologically matured enough to provide industrial-grade connectivity.

•
In reaching the objectives of industrial DTs, various compenents such as sensing, communication, and computing will require adopting a holistic approach towards energy-efficient, green infrastructure. As dense sensing leads towards massive battery-operative and energy harvesting (EH) devices, new hardware, communication infrastructure, and algorithmic approaches are needed. Meanwhile, due to the critical nature of industrial DTs, the QoS must be objectively captured and maintained for industrial DTs. Therefore, radio access, core network, and computing resources have to be jointly optimized while balancing the trade-off between energy efficiency and QoS. At radio access, energy-efficient schemes include (but are not limited to) link adaptation and topology, radio resource, and network management techniques. At the core network, energy efficiency can be enhanced by dynamic resource activation and virtualization. Meanwhile, the green computing solutions range from deployment optimization (e.g., at cloud or edge) to virtualization of computing resources. Although cloud computing lowers the energy cost, distributed edge computing can meet industrial QoS targets and facilitates UEs’ energy efficiency by computation offloading. Importantly, AoI, which objectively captures the value of information for industrial control and monitoring applications, should be considered in various tasks, such as communication and computing resource allocation, device polling and scheduling, and energy arrivals.

5.2. Future research directions
In the previous sections, we identified the critical challenges, i.e., security, privacy, big data computations, data updates, and communication constraints for complete DT adoption in industries. Moreover, we discussed the role and requirements of emerging technologies with respect to catering and facilitating the seamless integration of DT at each layer of the factory communication stack in Fig. 7. The provision of emerging technologies to meet the demands of performance metrics in the industrial DT landscape creates a plethora of research directions, which must be explored in the future. In Fig. 11, we give an overview and mapping between factory communication stack, discussed state-of-the-art features and enablers, and challenges/future trends for Industrial DT. This section highlights the foreseeable open research problems in DT for the smart industry at edge and cloud layers, 5G adaptation, data-related issues, advanced computation, and classification problems.

5.2.1. Privacy and security issues; blockchain technology
Integration of communication technologies on factory floor, such as B5G for IIoT-enabled factory machines, have made it relatively easy to sinew a range of industrial devices communicate wirelessly and enable them to share data ubiquitously even from distant locations. The computation servers at cloud and edge layers, where DT resides, are inundated with high-value and large-volume periodic and sporadic data from complex industrial processes. Therefore, there is a strong need to defend against the security and privacy issues arising from the perpetrators, unauthorized machine access, remote attacks, and rival intruders at each factory communication stack layer. Protecting these machine communication carrying high-value data is challenging.

Blockchain technology can be an effective solution for DT in smart industries to maintain industrial data integrity in virtual cyberspace, bringing future research opportunities. A chain of blocks, rechristened as blockchain, is a growing list of records called a block linked with all other blocks using cryptography. A motif of these sophisticated blocks is intrinsically disciplined to resist any possible data modification. As a result, any tampering to industrial data in these blocks is inherently inviolable. Ledgers of the blockchains are designed altogether ingeniously and holistically, that if any addition is once made, it can never be edited or deleted and the hash of the block acts as DNA. Thus, it makes the blockchain technology a promising candidate to ensure the integrity of data supply chain to CDT and EDT models more secure and intact.

5.2.2. Challenges in adaptation of B5G for industrial DT
Integrating B5G networks with DT technology for smart industries opens up a new avenue of futuristic opportunities. However, several research challenges exist in adapting B5G networks to meet the time-critical applications’ communication constraints that hinder the DT’s deployment. Some of these open research challenges are:

1.
SDN and NFV are the key enablers in realizing the B5G network slicing architecture to fulfill the applications’ diverse needs and requirements. However, there is a need to explore SDN and NFV functionality for B5G network slicing techniques to support the DT requirements.

2.
The interplay of mmWave and MIMO technology along with the availability of a dedicated licensed spectrum for industries is needed to provide the demanding URLLC and eMBB services.

3.
Currently, most of the literature primarily focuses on addressing the network side concerns of B5G in a smart factory. However, there is a limited progress on the B5G connectivity chipset modules for industrial devices, and their seamless compatibility with other industrial communication technologies, such as Ethernet and field buses.

5.2.3. Modeling problems in anomalies classification
The prognosis of anomalies or fault detection in machines, involved in complex manufacturing processes, is typically classified as logistic regression problems that predict the class of malignant event occurrence in machines. However, the frequency class of faulty events is less in a real-time factory environment, which prevents the formation of a logistic classifier that can accurately predict the faulty events from provided machine data. Consequently, the classifier becomes more biased towards predicting the majority class of benign events, i.e., machines’ regular operation, with greater accuracy while the less frequent class corresponding to critical fault or anomaly in machines is misclassified or ignored. This recurrent problem of misclassifying anomalies detection brings the wrong insights to DT for automated decision-making. To address the class imbalance in training data set of machines data, preprocessing methods such as class undersampling and oversampling techniques, or embedded modifications in the model of the ML framework can be explored from the DT’s perspective.

5.2.4. Challenges in multi-source data fusion for DT
Data fusion techniques will play an integral part in facilitating EDT and CDT to handle and fuse the multi-heterogeneous big industrial data so that the fused data gives an optimized insight to the factory twin. Besides data fusion, aggregated knowledge of the machines from past historical data accumulated over the years from complex industrial processes and technical knowledge from experts is also instrumental in building an accurate prognosis model at EDT and CDT. There is a limited research on the integration of all these factors, meant to expedite DT building process in smart factories.

5.2.5. Quantum-enhanced Machine Learning (QML)
Recent advances in the computation field has led to new QML architecture. QML has emerged from merging two interdisciplinary research areas: ML and quantum physics. It deals with executing state of the art ML algorithms on classical data using the quantum computer. QML can increase the computation power on big data of smart factories by intelligently analyzing data in the realm of quantum states. The integration of QML algorithms at cloud or edge will give fast and accurate updates to CDT and EDT. Moreover, a hybrid processing mechanisms can be explored for DTs in which complicated subroutines of computation processes are assigned to the quantum devices for faster execution, while at the same time, the rest is fed to the conventional computational server machines.

6. Conclusion
In this paper, we reviewed the emerging innovative uses of digital twins (DT) in various industries while primarily focusing on the new DT enablers, which lie at the nexus of new vision of wireless networks and computational intelligence (CI). We first differentiate between CPS and DT in terms of their functional aspects and establish the symbiotic relationship among IIoT, CPS, and DT towards realizing intelligent services in Industry 4.0. Second, we provided a systematic review of industrial DTs, as well as their utilization, value, and impact in various industrial domains, which is complemented with real deployment examples. Afterward, we identified the critical role and requirements of emerging technologies, including, cloud and edge computing, ML/AI and data optimization algorithms, age-optimal energy-efficient communication, and beyond-5G (B5G) networks, in the DTs for smart industries. We also discussed the various advances and concepts within these technologies that factories of the future (FoF) can exploit to realize the new class of DTs, i.e., edge DT and/or cloud DT. Despite the implementation and operational cost of DT enabling technologies/infrastructures, the role out of industrial DTs depends on the business viability and return on investment. Nevertheless, the nexus is still poised to play a critical role in realizing the industrial DT for increasing the value of FoFs. In the end, we reflected upon the key findings of our survey work and provided insights, recommendations, and future research directions pertaining to the DTs implementation using the nexus of CI and B5G. In particular, the main challenges stem from developing a DT of an entire factory floor, where flexible softwarized B5G connectivity with the support for advanced CI techniques to deploy DTs at the edge and/or cloud is needed. The challenging demands on CI are related to further enhancements on computer vision, anomalies classification, multi-source data fusion, seasonal forecasting, and enhanced machine learning. Meanwhile, B5G/6G networks need to address various issues such as multi-objective network and service optimization, age of information-aware scheduler design at various network levels. Above all, security and privacy issues are accentuated in the B5G-CI nexus with the increase in possible attack surfaces, wherein the current research trends in non-public B5G, blockchain, and federated learning, are expected to play a critical role towards industrial DTs.

