Multi-label classification is defined as the problem of identifying the multiple labels or categories of new
observations based on labeled training data. Multi-labeled data has several challenges, including class imbalance, label correlation, incomplete multi-label matrices, and noisy and irrelevant features. In this article, we
propose an integrated multi-label classification approach with incomplete label space and class imbalance
(ML-CIB) for simultaneously training the multi-label classification model and addressing the aforementioned
challenges. The model learns a new label matrix and captures new label correlations, because it is difficult
to find a complete label vector for each instance in real-world data. We also propose a label regularization to
handle the imbalanced multi-labeled issue in the new label, and l1 regularization norm is incorporated in the
objective function to select the relevant sparse features. A multi-label feature selection (ML-CIB-FS) method
is presented as a variant of the proposed ML-CIB to show the efficacy of the proposed method in selecting
the relevant features. ML-CIB is formulated as a constrained objective function. We use the accelerated proximal gradient method to solve the proposed optimisation problem. Last, extensive experiments are conducted
on 19 regular-scale and large-scale imbalanced multi-labeled datasets. The promising results show that our
method significantly outperforms the state-of-the-art.
CCS Concepts: • Computing methodologies → Machine learning; Supervised learning; Supervised
learning by classification; Learning paradigms;
Additional Key Words and Phrases: Multi-label classification, class imbalance, label correlation, multi-label
feature selection
1 INTRODUCTION
Multi-label classification (MLC) is the problem of simultaneously classifying instances into multiple categories or classes. For example, in text categorization, a document can belong to multiple
topics [30, 35]; similarly in genomics, a gene or protein is related to multiple functional labels [1].
It should not be confused with multi-class classification, where one class is predicted for each
instance.
In many previous works, the correlated labels have been exploited to improve label inference and
assignment [21, 32, 41]. In image annotation, for instance, as shown in Figure 1, it is not a simple
task to predict whether the label of the image should be “sea turtle” or “land turtle,” because they
both look very similar. However, if the image in Figure 1(a) is tagged as “fish” and the image in
Figure 1(b) is tagged as “rabbit,” the correlation information of the labels can be exploited. It is then
likely that the “turtle” in Figure 1(a) will be correctly annotated as “sea turtle” and the “turtle” in
Figure 1(b) will be annotated as “land turtle” or “tortoise.” Label correlation has been widely used
in multi-label learning studies over the past few years, and it significantly contributes to label
prediction in MLC. However, integrating label correlation into multi-label learning may not be
effective if the MLC method does not take into account important challenges such as incomplete
and noisy label space and the imbalanced class problem.
In real applications, it is particularly difficult to find a complete label vector for each instance due
to the unavailability of all labels in some applications such as genomics [26]. Furthermore, the label
matrix may contain noisy labels during manual labeling. Therefore, constructing a label correlation
matrix from an incomplete and noisy label matrix is not appropriate and may not represent the
real dependencies between the labels. The other important challenge is the structure of the label
matrix as Boolean data. Several previous studies have applied existing similarity metrics, including
Manhattan distance and Euclidean distance, which are mainly built for continuous data [17, 21].
Using existing categorical similarity metrics such as overlap or a frequency-based measure is too
simplistic for capturing the dependencies between the categorical labels. Therefore, proposing
a new predictive continuous label matrix has a twofold benefit. First, the method converts the
categorical data to continuous data, which specifically defines the contribution of the label in the
example. For instance, in Figure 1, the turtle in Figure 1(a) is bigger than the turtle in Figure 1(b),
and this will be reflected as a continuous value in the example. In the case of categorical labels,
however, the label shows whether present or absent in the example. Second, it generally captures
the global correlations between all labels in the new complete label matrix.
The most challenging case in MLC is the class imbalance problem. In standard binary and multiclass learning models, the class imbalance is ubiquitous in real machine learning applications,
which degrades classification accuracy [28]. The class imbalance issue is even greater in MLC due
to the existence of the incomplete label matrix. Typically, there are two different types of class
imbalance in MLC, which we refer to as instance-class imbalance and label-class imbalance. In the
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:3
first, the number of instances assigned a positive label is much less than the number assigned a
negative label, which is similar to the imbalance problem in binary classification. The second is
the label-class imbalance problem in which the number of labels assigned to different instances is
significantly disparate. Surprisingly, the class imbalance issue has not been extensively studied in
the MLC context [37]. Finally, in the context of multi-label feature selection, the class imbalance
problem and incomplete label matrix lead to the selection of biased features to the majority class
label, which degrades the performance of the MLC.
In this article, we propose an integrated multi-label approach (ML-CIB) that addresses the three
aforementioned challenges: the class imbalance problem, the incomplete label matrix, and label
correlation discovery from a multi-label Boolean matrix. This approach learns a new label matrix
and captures the new label correlations while simultaneously training the multi-label model. In
addition, we impose the l1 regularization norm on the feature matrix to retain the relevant features. A new continuous label matrix is constructed by considering three factors: (a) the new label
matrix should be consistent with the original label matrix, (b) the imbalanced class problem should
be handled in the original label matrix, and (c) the new label matrix uses the semantic instance
assumption, which means if the two instances xi and xj are similar, then their labels are correlated. Motivated by the work in Reference [36], we propose a label regularization to handle the
imbalanced label issue in the new predicted label matrix. The new predicted label matrix captures
richer information than the original matrix because it is complete and handles the imbalanced class
problem. We formulate the integrated approach that combines these components as a constrained
optimization problem. Our proposed objective function is non-smooth due to the existence of the
l1 norm regularization term. We use the accelerated proximal gradient method to solve our proposed optimization problem, and present a variant of our proposed method ML-CIB-FS to achieve
a supervised feature selection on high-dimensional multi-label datasets, taking into account the
imbalanced class problem and incomplete label matrix. We consider this to be the first attempt to
handle the imbalanced class problem using a multi-label feature selection approach. Extensive experiments are conducted on regular-scale and large-scale imbalanced multi-label datasets, which
clearly verify that the results of our proposed approach favorably outperform the state-of-the-art
algorithms.
In summary, our main contributions are:
(1) to provide an integrated multi-label classification approach that aims to:
(a) handle the class imbalance issue in multi-labeled datasets by proposing a novel label
regularization parameter;
(b) learn a new label matrix to address the missing label problem in multi-labeled data;
(c) generate label correlations from a multi-label Boolean matrix by exploring the multilabel manifold.
(2) to propose a novel multi-label feature selection algorithm from the proposed multi-label
classification approach by integrating l1-norm on the feature matrix.
(3) to develop a constrained optimization problem and iterative solution using the accelerated
proximal gradient method.
(4) to demonstrate the advantages of our proposed method by comprehensively evaluating it
against state-of-the-art methods on a high-dimensional imbalanced multi-label dataset.
The rest of this article is organized as follows: Section 2 discusses previous work. Section 3
presents the details of the proposed method. Section 4 details the optimization method for
ML-CIB. Section 5 conducts the experiments and reports the results of the compared algorithms.
The conclusion and future works are presented in Section 6.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
56:4 A. Braytee et al.
2 RELATED WORK
In this section, we briefly review the existing approaches related to MLC methods that handle the
noisy and incomplete label matrix, and the imbalanced class problem. Several approaches integrate label correlation with the feature selection process [17, 23, 33, 40], and a number of methods
extract the low-dimensional space shared among all labels by projecting the original data matrix
with the correlated labels [21, 22]. However, these approaches exploit the label correlations as prior
knowledge, which is typically constructed from an incomplete original label matrix. In contrast, a
small number of projection approaches that tackle the noisy label problem by projecting the original label matrix to low-dimensional space have been proposed [20, 42], but the major drawback
of these approaches is that the label projection process is achieved before it can be embedded in
the model training steps. A recent study has been proposed to learn a new supplementary label
matrix [38], but the new label matrix depends only on the original label correlations, and it does
not handle the imbalanced class problem.
Class imbalance in multi-label learning has not been extensively handled in the literature, as
stated recently by Reference [37]. Typically, the methods proposed in previous studies to handle
class imbalance can be divided into three main categories: data sampling strategy, cost-sensitive
strategy, and algorithmic adaptations. These three categories are inspired by the class imbalance
problem in single-label learning.
2.1 Data Sampling
Briefly, the data sampling strategy has attracted research attention due to its simplicity. It aims to
modify the distribution of the dataset to make it balanced, and this is achieved by using undersampling and oversampling techniques. The former reduces the number of majority class samples,
while the latter increases the number of minor class samples [19].
A simple under-sampling algorithm for multi-label learning was proposed by Reference [12] to
classify multi-label text datasets. The authors considered the problem as a binary class problem by
handling each label separately. Instances that belong to a certain label are considered as positive
labels and others are regarded as negative labels. The number of majority samples is then reduced
to lower the bias in each label. Another under-sampling method was presented in Reference [7],
which randomly reduces the number of majority samples. An inverse random under-sampling
(BR-IRUS) technique that was originally proposed for single-label classification was applied to
multi-label datasets [31]. The idea of this technique is to train multiple classifiers on balanced data
by using all the minority samples and a subset of the majority samples. However, these techniques
may exclude some important information by randomly removing the majority samples. Also, they
are sensitive to the number of retained samples, which may affect the generalization of the method.
Two techniques based on random oversampling have been proposed, namely, LP-ROS and MLROS [13]. Both techniques randomly duplicate the instances associated with the minority class,
but random oversampling is more prone to over-fitting. Several more advanced over-sampling
algorithms have been proposed, such as SMOTEUG [15] and MLSMOTE [8], which are based on
the classic SMOTE method in single-label learning. Briefly, the SMOTE method creates synthetic
minor instances to balance the dataset. SMOTEUG is a method to identify the seed samples to clone
them using the SMOTE algorithm. However, only one minority label is considered in SMOTEUG
to find the seed samples. In MLSMOTE, the set of samples from each minority label is processed
instead of synthetic samples from one minority label being generated. Typically, over-sampling
techniques increase the model training time due to the addition of more samples, which may not
be appropriate in the case of large datasets. Over-sampling techniques modify the distribution
of the dataset, which may degrade the performance of the classifiers. Furthermore, they do not
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:5
incorporate the correlation between labels, which helps to predict new labels and identifies missing
and noisy labels.
2.2 Cost-sensitive Learning
In contrast to single-label learning, cost-sensitive learning studies in MLC are very few. There
is one study proposed by Reference [10] that maximizes the imbalance-specific metric macro-F1.
However, this algorithm is dependent on the specific evaluation metric. Also, it is difficult in costsensitive learning strategies to set the values of the cost matrix [18].
2.3 Algorithmic Adaptations
Several algorithmic adaptation methods have been proposed recently to deal with the imbalanced
class problem in multi-label learning. Of these, Reference [34] introduces a method of forming a
smaller subset of balanced data from the imbalanced multi-label training data, which was used to
train a single artificial neural net (ANN) classifier. The subset of data is initialized by clustering the
imbalanced data into multiple clusters, and an equal number of data are collected from each cluster.
During the training phase of the ANN model, samples are incrementally added and removed to
improve training performance. This method is closer to being an ensemble method, because it does
not explicitly modify the learning model. Also, it is necessary to set a predefined parameter for
the number of clusters, which is not considered as a trivial task. Another method based on ANN
has been proposed, called IMIMLRBF [24]. IMIMLRBF takes into account the number of samples
per label when it selects the number of units in the hidden layer, and the weights associated with
the hidden layer are adjusted according to the individual bias of each label. In the proposal by
Reference [9], the authors used min-max modular classifiers that divide the problem randomly
into several smaller binary SVM classifiers. This decomposition is achieved by a clustering method
or by using principal component analysis. The proposed methods in the algorithmic adaptation
strategy are designed to be algorithm-dependent as mentioned above. A recent method is COCOA
[39], in which the classification decision is predicted by combining a binary-class imbalance of the
current class with a multi-class classifier for other classes. However, the authors do not consider
the extreme imbalance ratio in their study; also, the noisy and mislabelled classes are ignored.
A very recent work, entitled “Constrained Submodular Minimization for Missing Labels and
Class Imbalance in Multi-label Learning” (MMIB), was proposed by Reference [37] to jointly handle the class imbalance problem and missing labels in multi-label learning. However, the MMIB
method is conducted separately from the model training steps, which implies a high possibility
that the new label representations may not align with the multi-label trained model. Also, the
computational complexity of the training model will be high as a result of handling the missing
labels and class imbalance separately from the training phase.
It can be seen from this review that the existing multi-label feature selection algorithms in the
literature do not take the imbalanced class problem into consideration, which leads to the selection of features that favor the majority label. In the popular multi-label feature selection methods
“Multi-label informed feature selection” (MIFS) [23] and “Web image annotation via subspacesparsity collaborated feature selection” (SFUS) [25], the former exploits the label correlations to
extract the common features among multiple labels, and the latter discovers the shared feature
space in multi-label learning with feature selection. The common drawback is that neither takes
into account the imbalanced class problem. They both use the original noisy and incomplete labels
to exploit the label correlations.
Our proposed integrated multi-label approach, ML-CIB, simultaneously handles the incomplete
label and imbalanced class problem in MLC and predicts a new label matrix that accurately defines
the contribution of the label, as shown in the example in Figure 1. Furthermore, our method can
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
56:6 A. Braytee et al.
Table 1. Notations
Symbol Description
X Training data matrix
Xt s Testing data matrix
Y Training label matrix
Yt s Testing label matrix
Yˆ New training label matrix
n Number of samples
l Number of labels per sample
d Number of features in the data matrix
Cr Correlation matrix of the data matrix
yˆi The label vector for a certain sample
L Correlation coefficients
W Label-specific feature matrix
V Label regularization matrix
I Indicator function
S Score matrix
τ Classification threshold
α Control the contribution of the new label matrix manifold
β Control the difference between the new and original label matrix
γ Control the sparsity of the feature matrix W
ε Non-zero small number
ImR Label imbalance ratio
D+
j The number of positive samples for a certain label j
D−
j The number of negative samples for a certain label j
P% Missing labels proportion
be used as a multi-label feature selection method that reduces the dimensions of large datasets by
selecting balanced features to improve the accuracy of the MLC models.
3 PROBLEM FORMULATION
In this section, we present a multi-label classification method that simultaneously estimates the
label-specific features W using linear regression by taking into consideration the accurate label
correlations and class imbalance problem. We optimize our method ML-CIB using the accelerated approximal optimization method. The notations used throughout the article are introduced
in Table 1.
3.1 Inferring Accurate Label Correlations
Previous studies, as shown in Section 2, have observed that if the labels in MLC are strongly correlated, this indicates that they share common discriminative features, which is significant for
improving the classification accuracy of MLC models. However, it is not appropriate to capture
the label correlations from incomplete, incorrect, and flawed labels. Moreover, the original label
space has logical labels, which restricts the existing MLC algorithms from capturing the accurate
label correlations. To tackle this problem, we develop a new label matrix Yˆ ∈ Rn×l
, estimated during the training process, in which n is the number of instances and l are the labels. The new label
space Yˆ is extended to Euclidean space, where the Boolean label vector in each instance is changed
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:7
to a real value. The new label space has several benefits, such as allowing the discovery of more
semantic information about the labels, precisely depicting the contribution of the labels in each
instance by using continuous values and capturing accurate label correlations by exploring the
label manifold using the traditional similarity metrics in Euclidean space.
The new label matrix is predicted using two assumptions: First, the smoothness assumption on
the instance level [6], which assumes that instances close to each other are highly likely to have
similar labels. Consequently, we extract the local topological structure of the original data matrix
X ∈ Rn×d in the feature space by computing the correlation matrix Cr ∈ Rn×n; n and d are the
number of samples and features in X, respectively.
Cr = XXT . (1)
Computationally, we assume that each label point yˆi can be constructed using the topological
structure of X, for instance if the value of yˆi is labeled and yˆj contains a missing label, therefore,
yˆj will be predicted similar to yˆi if xi and xj are similar. The approximation of the Yˆ matrix can be
solved by minimizing
min
Yˆ YˆT LYˆ,
where L = 1 − Crij represents the correlation coefficients between xi and xj . The other factor in
constructing the new label matrix is label consistency, since we assume that the proposed new label
matrix Yˆ must be consistent with the original label matrix Y ∈ Rn×l by controlling the difference
between the two matricesYˆ andY. In our proposed method ML-CIB, we adopt the least squares loss
function to learn the label-specific featuresW by inducing sparsity onW using the l1 regularization
norm. Interestingly, the features in W are influenced by more accurate correlations between all
labels, which contributes to finding better predictive features for MLC.
The assumptions above are formulated as:
min
W,Yˆ
F (W ,Yˆ) = 1
2
XW − Yˆ 2
F + αTr(YˆT LYˆ) + β Yˆ − Y 2
F +γ W 1. (2)
3.2 Label Regularization for Class Imbalance
The class imbalance problem is considered to be a significant challenge in multi-label learning
that degrades the classification accuracy. The original label matrix Y consists of jth label vectors
yˆj that are assigned to the instances in the training data X, and the number of negative instances
in each jth label vector is greatly larger than the number of positive instances. In the case of linear
regression, for instance, the feature values of W in Equation (2) are influenced by the incorporation of the label correlation as noted above. Therefore, the features generated in W will be biased
towards predicting the majority labels due to the class imbalance problem among the instances in
each binary label vector yˆj . Our ML-CIB can be used as a feature selection method by imposing
the l1-norm regularization term on feature matrixW . The values inW can be used to rank the features, and the largest value features will be selected to train the jth binary classifiers for each label
vector yˆj . Thus, integrating the label correlations without regularizing the binary class imbalance
problem in each classifier will degrade the classification accuracy of the generated classifiers.
Motivated by the work in Reference [36], we introduce the matrix V ∈ Rl×l
, which is a label
regularization to control the influence of the majority labels on the instances. The matrix V is
constructed from the current label matrix Y as
Vjk =
n
j,k,i=1 IYj i (Yk i )
n for j = 1 ···l, k = j + 1, (3)
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.       
56:8 A. Braytee et al.
where
IYj i (Yk i ) =

1 if Yji = Yk i
0 if Yji  Yk i
(4)
and Yji is the label to a certain sample.
The proposed label regularization is used to normalize the new label matrix Yˆ, which is defined
as Yˆ = YV . The higher values of the label regularization matrix make a larger contribution to the
construction of the new label matrix Yˆ, and it plays a role in estimating the balanced predictive
features in W . Also, the impact of different labels is balanced even if the existing class labels are
imbalanced. Our objective function is modified as follows:
min
W,Yˆ
F (W ,Yˆ) = 1
2
XW − Yˆ 2
F + αTr(YˆT LYˆ) + β Yˆ − YV 2
F +γ W 1. (5)
By optimizing Equation (5), ML-CIB jointly trains a multi-label classification model and exploits
the accurate label correlations through a new predicted label matrix by exploring the label manifold
and controlling the class imbalance problem.
3.3 Predicting Labels for Testing Samples
Our ML-CIB predicts the labels of the testing samples using a linear regression method based
on a threshold τ without using a specific classifier. Assume a test data Xt s ∈ Rk×d , where k is the
number of testing samples. We compute the score matrix S ∈ Rk×l of the test samples by S = Xt sW ;
and we classify the test samples by proposing Yt s ∈ Rk×l as follows:
Yt s =

0 if S < τ
1 Otherwise .
3.4 Multi-label Feature Selection for Class Imbalance and Incomplete Label Space
(ML-CIB-FS)
In machine learning, feature selection and feature extraction techniques are affected by the class
imbalance problem. Feature selection, including the wrapper and embedded strategies, gives more
weight to the features that predict the majority class, because they depend on the performance of
the classification models, which are sensitive to the class imbalance problem. Feature extraction
techniques such as NMF and principal component analysis generate biased features that favor the
majority class [5]. In this subsection, we propose the first multi-label feature selection algorithm
(ML-CIB-FS) to handle the multi-label imbalance problem and incomplete label matrix. ML-CIBFS is a variant of our proposed ML-CIB method. It sorts all the features in factorized matrix W
according to wi 2 in descending order and returns the top-ranked features.
4 OPTIMIZATION PROCEDURE FOR ML-CIB
The minimization problem in Equation (5) is non-convex. Our objective function is partially convex with respect to each variable by fixing the other variables. It alternately minimizes F w.r.t. W
and Yˆ, one variable at a time, while fixing the other variable at the most recent value. Furthermore,
our objective function is non-smooth due to the presence of l1 norm regularization. In our problem, we use the accelerated proximal gradient (APG) method to solve the objective function for
two reasons: First, the l1 regularization norm makes our optimization function non-smooth. Second, the APG method generally has a faster convergence than other methods such as alternating
nonnegative least squares methods (ANLS) [2].
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.           
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:9
The proximal gradient method as proposed by Reference [27] is formulated as follows. Consider
the following convex optimization problem:
min
a ∈χ
F (a) = f (a), (6)
where χ is a convex set and f is a smooth function with Lipschitz continuous gradient Lf ,

∇f (a) − ∇f (b)
2 ≤ Lf a − b2 , for all a,b ∈ χ .
The projected gradient method for Equation (6) is
at+1 = 
at − αt∇f (at )

,
where αt is the step size at a specific iteration, and the new iteration at+1 is a minimizer of the
linearized proximal regularization of function f at point at ,
at+1 = min
x ∈χ
f (at ) +

∇f (at ), a − at

+
Lf
2 a − at 2
2 .
We can update the iteration as follows:
at+1 = min
x ∈χ
f (bt ) +

∇f (bt ), a − bt

+
Lf
2 a − bt 2
2 , (7)
where bt is a point at which f is linearized. The closed form of Equation (7) is
at+1 =

bt − 1
Lf
∇f (bt )
	
.
This method is guaranteed to solve Equation (6) with the appropriate value of bt . The optimal
solution is achieved by letting at = bt . For more details refer to Reference [27].
Following the method above, we solve our objective function in Equation (5) by updating both
the W -subproblem and the Yˆ-subproblem at each iteration by
W t+1 = W t − 1
LW t
∇W F (W t
,Yˆt ) + д(W ),
Yˆt+1 = Yˆt − 1
LYˆ t
∇Yˆ F (W t+1
,Yˆt ),
where LW t and LYˆ t are Lipschitz constants of gradient ∇W F (W ,Yˆt ) and ∇Yˆ F (W t+1,Yˆ) by F as
a function w.r.t. W and Yˆ, respectively, д(W ) = γ W 1. In our algorithm, LW t = XXT and LYˆ t =
I + αL + βI.
The proximal gradient method realizes a promising convergence result, but it converges relatively slowly. We use a Nesterov-type acceleration technique to speed up the algorithm. The
acceleration technique defines a positive extrapolation weight ω. The idea behind this technique
is that if the current iteration of at approaches a more optimal point a∗ than the previous iteration
at−1, then ωt is closer to a∗ than at . ω is defined as ωt = kt−1−1
kt , where kt = (1+
√1+4k2
t−1 )
2 . Therefore,
the setting of our subproblems W and Yˆ is changed as follows:
Zt
W = W t + ωt (W t −W t−1),
Zt
Yˆ = Yˆt + ωt (Yˆt − Yˆt−1).
The updating rules are modified as follows:
W t+1 = Zt
W − 1
LW t
∇W F

Zt
W ,Yˆt

+γ W 1, (8)
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.              
56:10 A. Braytee et al.
Yˆt+1 = Zˆt
Yˆ − 1
LYˆ t
∇Yˆ F

W t+1
,Zˆt
Yˆ

, (9)
where ∇W F (Zt
W ,Yˆt ) = XXTW − XYT and ∇Yˆ F (W t+1,Zˆt
Yˆ ) = Yˆ(I + αL + βI) − XW − βYV by taking the partial derivative w.r.t. W and Yˆ.
In the updating rule Equation (8), д(W ) is the l1 norm regularization term, then in every iteration
W t+1 is solved using the soft-thresholding operator Sγ , where γ > 0. W t+1 is defined as
W t+1 = Sγ (W t+1) = Zt
W − 1
LW t
∇W F

Zt
W ,Yˆt

+γ W 1, (10)
where the soft-thresholding operation Sγ is as follows:
Sγ [wij] =
⎧⎪⎪
⎨
⎪⎪
⎩
wij −γ if wij > γ
wij +γ if wij < −γ
0 otherwise
⎫⎪⎪
⎬
⎪⎪
⎭
.
Algorithm 1 summarizes the overall optimization process of our objective function using the
accelerated proximal gradient algorithm.
ALGORITHM 1: ML-CIB algorithm
Input: Training data X ∈ Rn×d
label matrix Y ∈ Rn×l
label regularization V ∈ Rl×l
Parameters: α, β, γ and MaxIteration
Initialization:
ε > 0: to avoid divide by zero
Yˆ
1 = YV
W1 = (XT Yˆ)/(L + ε)
k0, k1 = 1 and t = 0
while MaxIteration> t do
ωt = kt−1−1
kt ;
Zt
W = W t + ωt (W t −W t−1);
W t+1 = Zt
W − 1
LW ∇W F (Zt
W ,Yˆt );
W t+1 = Sγ (W t+1);
Zt
Yˆ = Yˆt + ωt (Yˆt − Yˆt−1);
Yˆt+1 = Zˆt
Yˆ − 1
LYˆ ∇Yˆ F (W t+1,Zˆt
Yˆ );
kt+1 = (1+


1+4k2
t )
2 ;
t = t + 1;
end
Output: Coefficient matrix W ∈ Rd×l
New label matrix Yˆ ∈ Rn×l
5 EXPERIMENTS AND DATASETS
In this section, extensive experiments are conducted on multi-label datasets from different applications to demonstrate the efficacy of the proposed method. The experiments are structured in
two phases based on the assessments of the proposed method: First, we introduced ML-CIB as a
multi-label classification method that uses linear regression to classify the test points. Second, the
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.  
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:11
Table 2. Characteristics of the Evaluated Datasets
Dataset Domain #Features #Labels #Instances MinImR MaxImR AvдImR
Bibtex Text 1,836 159 7,395 6.09 144 87.69
Medical Text 1,449 45 978 2.67 977 328.06
Enron Text 1,001 53 1,702 1.00 1,701 136.86
Genebase Biology 1,185 27 662 2.87 661 143.45
Image Images 294 5 2,000 2.44 3.89 3.11
MIMLtext 243 7 7,119 2.15 11.08 5.71
CAL500 Music 68 174 502 1.04 99.40 22.34
Corel5k Images 499 374 5,000 3.46 4,999 845.28
Emotions Music 72 6 593 1.24 3 2.32
Education(web) Text 550 33 5,000 2.17 4,999 568.77
Languagelog Text 1,004 75 1,459 7.53 1,458 334.11
MediaMill Video 120 101 43,907 1.74 1,415.4 331.43
RCV1V2 (S1) Text 23,679 103 6,000 1.12 5,999 436.9
RCV1V2 (S2) Text 23,571 103 6,000 1.12 5,999 436.9
RCV1V2 (S3) Text 23,631 103 6,000 1.12 5,999 436.9
Education Text 20,782 14 11,817 2.16 92.04 25.35
Health Text 18,430 14 9,109 1.06 78.90 22.15
Bookmarks Text 2,150 208 5,000 3.72 207 148.38
Social Text 1,047 39 5,000 1.31 113.55 58.60
proposed method is evaluated as a feature selection method. Several state-of-the-art multi-label
learning methods for imbalanced datasets and multi-label feature selection methods are selected
for comparison. This section is divided into three subsections: datasets, experimental configuration, experimental results and parameter sensitivity. The evaluation metrics are described in
appendix A.
5.1 Datasets
The experiments are conducted on 19 multi-label benchmark datasets. These datasets are collected
from various domains to empirically show the generalization of the proposed method and to serve
as a solid basis for the results analysis. The characteristics of the datasets are summarized in Table 2,
which shows that these datasets cover a broad range of applications, including music, images,
text, biology, and video. These are available online from the Mulan1 and LAMDA2 repositories.
The average class imbalance ratio AvдImR is computed for the datasets by l
j=1 AvдImRj , and
ImRj = max(D+
j ,D−
j )/ min(D+
j ,D−
j ), where D+
j and D−
j are the number of positive and negative
instances respectively w.r.t. label j [39]. In addition, MinImR and MaxImR are the minimum and
maximum imbalance ratio, respectively, of all the labels.
5.2 Experimental Setting and Compared Algorithms
Extensive experiments are conducted in this section to demonstrate the benefits of the ML-CIB
method. The experimental setting is divided into two stages: First, MLC is undertaken using a
linear regression method based on a threshold τ without using a specific classifier. Second, ML-CIB
1http://mulan.sourceforge.net/datasets-mlc.html. 2http://lamda.nju.edu.cn/Data.ashx.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019. 
56:12 A. Braytee et al.
is used as a supervised feature selection method (ML-CIB-FS), which reduces the dimensionality of
the large-scale datasets by performing feature selection on matrixW . To ensure a fair comparison
between the methods, the classification results of every metric are averaged based on five-fold
cross-validation. The evaluation metrics used are: Hamming loss, one-error, coverage, ranking
loss, average precision, subset accuracy, and micro-F1. For the first four, the smaller the value, the
better the performance. However, for the other four evaluation metrics, the reverse is true.
In the first stage of MLC, we set τ = 0.5. The experiments are designed to demonstrate the contribution of the proposed and compared methods to handle the class imbalance and missing labels
problems. First, the MLC performance of our ML-CIB algorithm is compared with four state-of-theart methods that are specifically proposed to handle the class imbalance problem in MLC. These
methods are: MLSMOTE [8], BR-IRUS [31], COCOA [39], and MMIB [37]. Then, we compared the
results of two methods, namely, MMIB [37] and GLOCAL [44] in relation to handling a differing
amount of missing labels. We also utilize a baseline binary relevance (BR) method [4]. A linear
SVM classifier is used for binary classification for BR, CC, and MLSMOTE.
(1) BR: Independently trains one binary classifier for each label.
(2) MLSMOTE3: Multilabel Synthetic Minority Over-sampling Technique, which generates
synthetic samples for imbalanced multi-label datasets.
(3) BR-IRUS: Inverse random under-sampling technique, which trains an ensemble of classifiers in each label using a random subset of the majority samples along with all minority
samples.
(4) COCOA4: Cross-Coupling Aggregation method, which proposes a predictive model that
combines one binary-class imbalance learner for a certain label with several multi-class
imbalance learning models.
(5) MMIB5: Proposes a constrained sub-modular minimization optimization function to handle the missing labels and class imbalance problem in multi-label learning.
(6) GLOCAL: Multi-label approach learning a Laplacian matrix to learn the missing labels.
In the second stage, ML-CIB-FS is presented as a supervised feature selection method. We evaluate the effectiveness of the selected features from the compared algorithms by checking the classification performance. To ensure a fair comparison between all algorithms, we use the binary
relevance (BR) and classifier chains (CC) strategies to decompose the multi-label classification
problem into binary sub-problem classifiers. Without loss of generality, we use a linear SVM to
train the binary sub-problem classifiers. The results, based on a different number of features, vary
from 5% to 20%. ML-CIB-FS is benchmarked against the following state-of-the-art multi-label feature selection methods:
(1) MIFS: Multi-Label Informed Feature Selection, which exploits the label correlations to
extract the common features of multiple labels.
(2) SFUS: Sub-Feature Uncovering with Sparsity, which jointly discovers the shared feature
space in multi-label learning with feature selection.
5.3 Results and Analysis
5.3.1 MLC Results for Handling the Imbalance Problem. Tables 3 and 4 report the classification
results of the algorithms using the different evaluation metrics. These results are the average values of each compared algorithm using five-fold cross-validation. Several datasets with different
3The Java application is available at http://simidat.ujaen.es/papers/MLSMOTE. 4The source code is obtained from http://cse.seu.edu.cn/people/zhangml/Resources.htm. 5The source code is obtained from https://sites.google.com/site/baoyuanwu2015/Publications.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:13
Table 3. Multi-label Classification Results of All Compared Algorithms (Mean) on the Regular-scale
Multi-label Datasets
Evaluation
criteria Algorithm Bibtex Medical Enron Genebase Image MIMLText CAL500 Corel5k Emotions
Hamming
loss ↓
MLCIB 0.005 (1) 0.007 (1) 0.054 (2) 0.003 (3) 0.142 (1) 0.039 (1) 0.324 (6) 0.005 (1) 0.097 (1)
BR (Baseline) 0.015 (4) 0.010 (3) 0.059 (4) 0.001 (1) 0.176 (5) 0.051 (3) 0.137 (3) 0.011 (3) 0.199 (5)
MLSMOTE 0.014 (2) 0.008 (2) 0.058 (3) 0.001 (1) 0.175 (4) 0.046 (2) 0.135 (2) 0.008 (2) 0.197 (4)
BR-IRUS 0.031 (6) 0.020 (6) 0.110 (6) 0.006 (5) 0.224 (6) 0.067 (6) 0.190 (5) 0.022 (6) 0.227 (6)
COCOA 0.017 (5) 0.015 (5) 0.074 (5) 0.002 (2) 0.160 (3) 0.059 (5) 0.188 (4) 0.014 (4) 0.176 (3)
MMIB 0.014 (3) 0.012 (4) 0.051 (1) 0.005 (4) 0.153 (2) 0.055 (4) 0.098 (1) 0.018 (5) 0.174 (2)
One-error ↓
MLCIB 0.199 (3) 0.043 (1) 0.086 (1) 0.000 (1) 0.140 (2) 0.027 (1) 0.106 (4) 0.077 (1) 0.103 (1)
BR (Baseline) 0.232 (5) 0.072 (3) 0.112 (4) 0.001 (2) 0.155 (5) 0.040 (3) 0.001 (1) 0.469 (5) 0.129 (5)
MLSMOTE 0.229 (4) 0.068 (2) 0.110 (3) 0.001 (2) 0.153 (4) 0.037 (2) 0.001 (1) 0.467 (4) 0.128 (4)
BR-IRUS 0.448 (6) 0.134 (6) 0.207 (6) 0.001 (2) 0.197 (6) 0.052 (6) 0.002 (2) 0.787 (6) 0.149 (6)
COCOA 0.193 (2) 0.087 (5) 0.135 (5) 0.001 (2) 0.141 (3) 0.049 (5) 0.004 (3) 0.429 (3) 0.113 (2)
MMIB 0.172 (1) 0.076 (4) 0.095 (2) 0.003 (3) 0.135 (1) 0.046 (4) 0.002 (2) 0.154 (2) 0.115 (3)
Ranking
loss ↓
MLCIB 0.101 (1) 0.030 (1) 0.092 (1) 0.000 (1) 0.174 (1) 0.035 (1) 0.175 (1) 0.324 (2) 0.216 (1)
BR (Baseline) 0.597 (5) 0.214 (3) 0.489 (4) 0.009 (3) 0.503 (5) 0.159 (3) 0.772 (4) 0.885 (5) 0.454 (5)
MLSMOTE 0.594 (4) 0.211 (2) 0.486 (3) 0.006 (2) 0.502 (4) 0.155 (2) 0.771 (3) 0.882 (4) 0.450 (4)
BR-IRUS 0.99 (6) 0.396 (6) 0.889 (6) 0.013 (5) 0.631 (6) 0.201 (6) 0.99 (5) 0.979 (6) 0.507 (6)
COCOA 0.489 (3) 0.248 (5) 0.580 (5) 0.013 (5) 0.450 (3) 0.177 (5) 0.996 (6) 0.810 (3) 0.396 (3)
MMIB 0.438 (2) 0.222 (4) 0.405 (2) 0.010 (4) 0.434 (2) 0.169 (4) 0.545 (2) 0.293 (1) 0.394 (2)
Coverage ↓
MLCIB 0.199 (1) 0.039 (1) 0.250 (1) 0.008 (1) 0.195 (1) 0.070 (1) 0.737 (2) 0.562 (5) 0.350 (3)
BR (Baseline) 0.477 (5) 0.147 (3) 0.453 (4) 0.0183 (3) 0.276 (5) 0.116 (3) 0.871 (4) 0.439 (4) 0.359 (5)
MLSMOTE 0.474 (4) 0.143 (2) 0.448 (3) 0.014 (2) 0.275 (4) 0.114 (2) 0.867 (3) 0.437 (3) 0.355 (4)
BR-IRUS 0.917 (6) 0.273 (6) 0.821 (6) 0.026 (6) 0.347 (6) 0.149 (6) 0.982 (5) 0.736 (6) 0.404 (6)
COCOA 0.391 (3) 0.170 (5) 0.537 (5) 0.022 (4) 0.248 (3) 0.128 (5) 0.994 (6) 0.401 (2) 0.314 (2)
MMIB 0.352 (2) 0.151 (4) 0.377 (2) 0.022 (5) 0.242 (2) 0.122 (4) 0.615 (1) 0.145 (1) 0.310 (1)
Subset
accuracy ↑
MLCIB 0.240 (1) 0.540 (4) 0.145 (1) 0.999 (1) 0.282 (5) 0.502 (6) 0.195 (1) 0.191 (1) 0.283 (3)
BR (Baseline) 0.143 (5) 0.662 (2) 0.108 (4) 0.975 (3) 0.384 (4) 0.725 (2) 0.000 (6) 0.007 (6) 0.263 (5)
MLSMOTE 0.153 (4) 0.671 (1) 0.117 (3) 0.985 (2) 0.393 (3) 0.733 (1) 0.008 (4) 0.015 (4) 0.270 (4)
BR-IRUS 0.002 (6) 0.107 (6) 0.012 (6) 0.641 (6) 0.279 (6) 0.534 (5) 0.010 (3) 0.112 (2) 0.223 (6)
COCOA 0.163 (3) 0.554 (5) 0.081 (5) 0.970 (4) 0.416 (2) 0.655 (4) 0.015 (2) 0.091 (3) 0.292 (1)
MMIB 0.173 (2) 0.647 (3) 0.118 (2) 0.968 (5) 0.430 (1) 0.691 (3) 0.008 (5) 0.008 (5) 0.291 (2)
Average
precision ↑
MLCIB 0.670 (1) 0.970 (1) 0.738 (1) 0.999 (1) 0.880 (1) 0.990 (1) 0.679 (1) 0.399 (1) 0.951 (1)
BR (Baseline) 0.372 (5) 0.783 (3) 0.457 (4) 0.990 (2) 0.720 (5) 0.887 (3) 0.292 (4) 0.098 (5) 0.740 (5)
MLSMOTE 0.379 (4) 0.792 (2) 0.466 (3) 0.999 (1) 0.727 (4) 0.895 (2) 0.300 (3) 0.106 (3) 0.747 (4)
BR-IRUS 0.021 (6) 0.126 (6) 0.079 (6) 0.650 (5) 0.531 (6) 0.656 (6) 0.180 (5) 0.023 (6) 0.647 (6)
COCOA 0.432 (3) 0.658 (5) 0.366 (5) 0.984 (3) 0.789 (3) 0.803 (5) 0.180 (5) 0.099 (4) 0.833 (3)
MMIB 0.463 (2) 0.765 (4) 0.529 (2) 0.980 (4) 0.813 (2) 0.848 (4) 0.372 (2) 0.220 (2) 0.836 (2)
Micro-F1 ↑
MLCIB 0.453 (3) 0.870 (1) 0.671 (1) 0.964 (1) 0.757 (1) 0.760 (5) 0.358 (2) 0.439 (1) 0.778 (1)
BR (Baseline) 0.416 (5) 0.804 (3) 0.513 (4) 0.950 (5) 0.577 (5) 0.853 (2) 0.330 (4) 0.157 (5) 0.646 (4)
MLSMOTE 0.424 (4) 0.812 (2) 0.521 (3) 0.959 (4) 0.585 (4) 0.862 (1) 0.338 (3) 0.165 (3) 0.655 (3)
BR-IRUS 0.024 (6) 0.132 (6) 0.089 (6) 0.651 (6) 0.425 (6) 0.632 (6) 0.202 (6) 0.043 (6) 0.563 (5)
COCOA 0.483 (2) 0.675 (5) 0.414 (5) 0.963 (2) 0.631 (3) 0.771 (4) 0.205 (5) 0.163 (4) 0.728 (2)
MMIB 0.519 (1) 0.785 (4) 0.597 (2) 0.961 (3) 0.650 (2) 0.814 (3) 0.420 (1) 0.357 (2) 0.728 (2)
Rank between (.). The best results are highlighted in bold.
characteristics are used in the experiments to investigate the generalization of the algorithms. As
shown in Table 2, we used regular and large-scale datasets in terms of the number of instances and
dimensions. We also used datasets that have a small, regular, and extreme imbalance ratio. From
the experimental results in Tables 3 and 4, we make the following observations:
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
56:14 A. Braytee et al.
Table 4. Multi-label Classification Results of All Compared Algorithms (Mean) on the Large-scale
Multi-label Datasets
Evaluation
criteria Algorithm Education(web) Languagelog MediaMill RCV1V2(S1) RCV1V2(S2) RCV1V2(S3) Avg. rank Friedman
Hamming
loss ↓
MLCIB 0.030 (1) 0.011 (1) 0.006 (1) 0.008 (1) 0.011 (2) 0.013 (3) 1.73 Base
BR (Baseline) 0.037 (4) 0.018 (4) 0.031 (4) 0.013 (3) 0.012 (3) 0.012 (2) 3.4 0.020
MLSMOTE 0.032 (2) 0.014 (3) 0.028 (3) 0.009 (2) 0.008 (1) 0.011 (1) 2.26 0.070
BR-IRUS 0.047 (5) 0.024 (5) 0.061 (5) 0.019 (5) 0.017 (6) 0.018 (5) 5.53 0.004
COCOA 0.035 (3) 0.012 (2) 0.013 (2) 0.013 (3) 0.016 (5) 0.013 (3) 3.6 0.020
MMIB 0.032 (2) 0.014 (3) 0.028 (3) 0.016 (4) 0.015 (4) 0.017 (4) 3.06 0.070
One-error ↓
MLCIB 0.051 (1) 0.313 (3) 0.024 (1) 0.025 (3) 0.016 (1) 0.019 (1) 1.66 Base
BR (Baseline) 0.076 (4) 0.417 (5) 0.107 (5) 0.023 (2) 0.025 (3) 0.024 (3) 3.66 0.0045
MLSMOTE 0.071 (3) 0.415 (4) 0.106 (4) 0.019 (1) 0.021 (2) 0.021 (2) 2.8 0.0045
BR-IRUS 0.092 (5) 0.473 (6) 0.206 (6) 0.030 (5) 0.033 (5) 0.031 (6) 5.26 0.0008
COCOA 0.065 (2) 0.203 (1) 0.036 (2) 0.026 (4) 0.025 (3) 0.027 (4) 3.06 0.07
MMIB 0.065 (2) 0.257 (2) 0.090 (3) 0.023 (2) 0.026 (4) 0.029 (5) 2.66 0.438
Ranking
loss ↓
MLCIB 0.222 (1) 0.205 (1) 0.169 (1) 0.294 (1) 0.289 (1) 0.279 (1) 1.06 Base
BR (Baseline) 0.764 (5) 0.718 (5) 0.536 (5) 0.316 (5) 0.314 (5) 0.306 (4) 4.4 0.0001
MLSMOTE 0.762 (4) 0.714 (4) 0.533 (4) 0.315 (4) 0.309 (4) 0.304 (3) 3.4 0.0001
BR-IRUS 0.912 (6) 0.810 (6) 0.988 (6) 0.354 (6) 0.367 (6) 0.329 (6) 5.53 0.0001
COCOA 0.629 (3) 0.347 (3) 0.173 (2) 0.299 (2) 0.294 (2) 0.283 (2) 3.46 0.0001
MMIB 0.628 (2) 0.442 (2) 0.433 (3) 0.303 (3) 0.313 (3) 0.323 (5) 2.73 0.0008
Coverage ↓
MLCIB 0.285 (2) 0.151 (1) 0.195 (1) 0.253 (1) 0.256 (1) 0.249 (1) 1.53 Base
BR (Baseline) 0.300 (4) 0.359 (5) 0.625 (5) 0.277 (5) 0.277 (4) 0.274 (4) 4.2 0.0008
MLSMOTE 0.298 (3) 0.358 (4) 0.622 (4) 0.276 (4) 0.274 (3) 0.273 (3) 3.2 0.0008
BR-IRUS 0.358 (5) 0.408 (6) 0.976 (6) 0.312 (6) 0.324 (6) 0.295 (6) 5.86 0.0001
COCOA 0.249 (1) 0.174 (2) 0.201 (2) 0.261 (2) 0.260 (2) 0.254 (2) 3.06 0.0201
MMIB 0.249 (1) 0.222 (3) 0.507 (3) 0.264 (3) 0.278 (5) 0.289 (5) 2.8 0.070
Subset
accuracy ↑
MLCIB 0.297 (1) 0.384 (1) 0.298 (1) 0.644 (1) 0.643 (1) 0.666 (1) 1.93 Base
BR (Baseline) 0.195 (5) 0.225 (5) 0.070 (5) 0.437 (5) 0.436 (4) 0.439 (4) 4.33 0.020
MLSMOTE 0.203 (4) 0.233 (4) 0.077 (3) 0.445 (4) 0.445 (3) 0.448 (3) 3.13 0.020
BR-IRUS 0.150 (6) 0.189 (6) 0.005 (6) 0.379 (6) 0.357 (6) 0.404 (6) 5.46 0.0008
COCOA 0.220 (3) 0.334 (2) 0.110 (2) 0.456 (2) 0.460 (2) 0.470 (2) 3.26 0.07
MMIB 0.221 (2) 0.306 (3) 0.075 (4) 0.452 (3) 0.433 (5) 0.412 (5) 3.33 0.07
Average
precision ↑
MLCIB 0.729 (1) 0.469 (1) 0.878 (1) 0.843 (1) 0.881 (1) 0.851 (1) 1 Base
BR (Baseline) 0.297 (4) 0.168 (5) 0.476 (5) 0.709 (5) 0.708 (4) 0.715 (4) 4.46 0.0001
MLSMOTE 0.305 (3) 0.177 (4) 0.485 (4) 0.718 (4) 0.718 (3) 0.722 (3) 3.13 0.0001
BR-IRUS 0.232 (5) 0.138 (6) 0.045 (6) 0.621 (6) 0.585 (5) 0.661 (6) 5.73 0.0001
COCOA 0.342 (2) 0.247 (2) 0.793 (2) 0.748 (2) 0.753 (2) 0.769 (2) 3.2 0.0001
MMIB 0.342 (2) 0.224 (3) 0.561 (3) 0.739 (3) 0.708 (4) 0.679 (5) 2.93 0.0001
Micro-F1 ↑
MLCIB 0.412 (1) 0.376 (1) 0.882 (1) 0.883 (1) 0.793 (2) 0.853 (1) 1.53 Base
BR (Baseline) 0.338 (5) 0.205 (5) 0.518 (5) 0.752 (5) 0.755 (5) 0.756 (4) 4.4 0.0045
MLSMOTE 0.346 (4) 0.213 (4) 0.525 (4) 0.760 (4) 0.764 (3) 0.765 (3) 3.26 0.0045
BR-IRUS 0.267 (6) 0.172 (6) 0.048 (6) 0.659 (6) 0.624 (6) 0.702 (6) 5.53 0.0001
COCOA 0.391 (3) 0.303 (2) 0.861 (2) 0.792 (2) 0.803 (1) 0.813 (2) 2.93 0.1967
MMIB 0.392 (2) 0.275 (3) 0.612 (3) 0.784 (3) 0.757 (4) 0.718 (5) 2.66 0.1967
Rank between (.). The best results are highlighted in bold.
• First, the state-of-the-art algorithms perform better than the baseline BR in most of the evaluation metrics. This indicates the importance of handling class imbalance and the incomplete label matrix to improve the performance of MLC. Note that the baseline BR achieved
better results than BR-IRUS due to the nature of the method. First, random undersampling is
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.                          
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:15
the simplest way to deal with the class imbalance problem due to the high chance of losing
information by removing random samples. Second, IRUS was not originally proposed to deal
with class imbalance in multi-label datasets, but it is used as an application in combination
with BR. Finally, from the model perspective, BR-IRUS makes the majority negative classes
as minority samples; therefore, in the case of high class imbalance and high-dimensional
data, the model suffers from the small number of samples in the training process, which
leads to overfitting.
• Second, the results of the ML-CIB algorithm detailed in Tables 3 and 4 are compared with
the results obtained by the-state-of-the-art algorithms, showing that the proposed method
outperforms the compared algorithms for most benchmark datasets using various evaluation metrics.
• Third, theoretically, the missing labels for the multi-label matrix increase the imbalance
ratio of the datasets. The results show that addressing the problem of missing labels along
with the class imbalance problem improves the performance of MLC, and this is shown in
the results of ML-CIB and MMIB against the results of the other algorithms.
• Fourth, the ranking loss and one-error metrics rank the irrelevant labels higher than the
relevant labels by definition [41]. As shown in Tables 3 and 4, we note that the algorithm
achieves the best results in the ranking loss metric, because it handles the incomplete label
matrix by predicting the relevant labels using the label manifold during the training process.
• Fifth, the micro-F1 used the F1-score, which is appropriate in the presence of the class imbalance problem. The reported results for these two metrics show that ML-CIB achieves better
performance than the other algorithms for most benchmark datasets, which indicates the
superiority of the proposed algorithm in handling the class imbalance problem.
• Finally, the proposed algorithms work efficiently on the extreme levels of class imbalance
ratio (AvдImR > 50) without being restricted by the exclusion of any class label, as in
Reference [39]. The results in Tables 3 and 4 show that ML-CIB achieves better performance than the others on extreme imbalance ratio datasets such as Languagelog, MediaMill,
RCV1V2(S1), and Corel5k.
5.3.2 MLC Results for Handling Missing Labels. In this experiment, to investigate the potential
contribution of the compared methods in relation to handling different levels of missing labels,
we randomly removed different proportions of labels from the samples from moderate to extreme:
20%, 40%, 60%, and 80%. The proportion of missing labels is determined as follows:
P% = |Yt r == −1|
n × l ,
where Yt r == −1 is the number of missing labels in the training set. Figure 2 compares the results
of the algorithms with different proportions of missing labels. The datasets that we used in this experiment allow us to evaluate the compared methods in relation to handling the different amounts
of missing labels, learning labels in the presence of an extreme amount of missing labels and the
imbalance problem using the Medical and RCV1V2 (S3) datasets, and evaluating the performance
of the methods in relation to dealing with the previous challenges and the high-dimensional multilabel dataset, namely, RCV1V2 (S3). Importantly, we found that the ML-CIB algorithm achieved
consistent improvement over the MMIB and GLOCAL methods. The success of ML-CIB is due to
several reasons. (1) In contrast to the other methods, ML-CIB takes into consideration the existence
of the Boolean label matrix and learns a new continuous label matrix during the training process,
which allows accurate label correlations to be captured in Euclidean space. (2) ML-CIB simultaneously optimizes the incomplete label matrix, class imbalance problem, and the label correlation
discovery from a multi-label Boolean matrix. (3) As shown in Figures 2(b), 2(c), 2(e), and 2(f), when
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
56:16 A. Braytee et al.
Fig. 2. Results on three datasets with different missing label proportions.
the proportion missing labels increases, the impact of class imbalance is further observed. Therefore, ML-CIB integrates label regularization to reduce the influence of the majority labels, which
plays a significant role in ML-CIB to alleviate the negative impact of the class imbalance problem on the classification results. Taken together, our proposed ML-CIB algorithm was superior in
handling the challenges of the multi-label datasets such as incomplete label matrices and the class
imbalance problem. To further investigate the performance of ML-CIB on an extreme label dataset,
we evaluated ML-CIB on large-scale datasets, namely, Wiki10-31K [3], which contains 101, 938 features and 30, 938 labels. In our analysis, ML-CIB achieved a good average precision and Hamming
loss measures of 0.85 and 0.031, respectively. In future work, ML-CIB will be implemented in C++
to speed up the evaluation time on extreme labels and large-scale datasets.
We conducted a Friedman test, which is suitable for statistically demonstrating the difference in
performance between multiple learning methods in various datasets. We performed the Friedman
test with 95% confidence, under the hypothesis that the evaluation metric results between the two
types of methods would not be significantly different. As shown in Table 4, the p-value is lower than
0.05 in most cases, which rejects the hypothesis and indicates that the proposed method outperforms the other methods. We also report the ranking of each dataset of the compared algorithms,
and a () sign for the Friedman test suggests that the methods being compared are significantly
different. As shown in the results in Table 4, the ML-CIB method (“Base”) is significantly better
than the compared methods BR, MLSMOTE, BR-IRUS, COCOA, and MMIB. As highlighted earlier, the ML-CIB algorithm improves the performance of multi-label classification for two reasons:
First, it tackles the class imbalance problem, including the high level of imbalanced ratios. Second, it predicts a new label matrix from the existing incomplete label matrix, which alleviates the
negative effects of the rare appearance of many class labels.
5.3.3 ML-CIB-FS Feature Selection Results. We introduce ML-CIB-FS as a feature selection
method for multi-label learning, which uses ML-CIB to rank the features. As previously mentioned,
ML-CIB handles the class imbalance and incomplete label matrix to improve the multi-label classification performance. Also, ML-CIB imposes l1-norm regularization on the multi-label feature
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019. 
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:17
Fig. 3. ML-CIB-FS average classification results on Bibtex dataset.
matrix W as defined in Equation (5). Therefore, the top-ranked features in W are those features
that make a greater contribution to classifying the multi-label sample. The features with the largest
values are selected as the most predictive balanced features, which are able to lessen the inherent
bias of the majority labels due to the class imbalance problem and incomplete label matrix. The
ML-CIB-FS algorithm sorts all the features inW according to wi  in descending order and returns
the top-ranked features.
We conducted several experiments to assess the importance of the predicted features selected
by the compared algorithms to enhance the multi-label classification. The features are selected
from the top 5% to 20% of features for all algorithms. We evaluated the selected features on highdimensional multi-label datasets using two evaluation metrics, micro-F1 and Hamming loss, to
evaluate the advantages of addressing the class imbalance problem and incomplete label matrix,
respectively, in feature selection multi-label methods. As mentioned, micro-F1 is based on F1-
measure and is used in the class imbalance problem, and Hamming loss is used to check the percentage of wrongly predicted labels. The results are reported in Figures 3 through 7. It is observed
that the ML-CIB-FS algorithm outperforms the two compared algorithms for all the benchmarked
datasets. This indicates the importance of handling the class imbalance problem in feature selection multi-label learning methods.
Furthermore, the performance of the MIFS and SFUS algorithms is very similar, but ML-CIB-FS
clearly achieves better performance, as shown in the figures. The results in Figures 6 and 7 are extremely promising, as ML-CIB-FS significantly outperforms MIFS and SFUS on high-dimensional
datasets such as RCV1V2 (S1) and RCV1V2 (S2) where the number of dimensions of each is > 40K.
Moreover, as shown in the results, the proposed algorithm achieves better results using the Hamming loss metric, which indicates the importance of selecting features based on a new complete
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.  
56:18 A. Braytee et al.
Fig. 4. ML-CIB-FS average classification results on Enron dataset.
label matrix Yˆ. The last observation in relation to the reported results is the stability of the results
after 10% of the features have been selected, which demonstrates the ability of the ML-CIB-FS
algorithm to achieve better results with fewer features. Using fewer features will reduce the computational time of training the classification model.
We perform a statistical comparative analysis using the Nemenyi test [11] to check the performance of the proposed algorithm ML-CIB-FS against the other algorithms. According to the
Nemenyi test, the two methods perform differently if they vary by at least one critical difference
(CD). This is an important statistical test across different algorithms over multiple datasets. Based
on the results of the Friedman mean ranks for all algorithms, Figure 8 shows that the rank results
of the ML-CIB-FS algorithm performed differently to the SFUS and MIFS algorithms, because their
ranks are located outside the interval of CD. Furthermore, it can be observed that SFUS and MIFS in
all cases achieve equivalent performance due to their connection by a colored line, which indicates
that there is no significant difference between the connected algorithms. As a result, ML-CIB-FS
is superior across different types of high-dimensional datasets in different domains. The results
indicate the importance of handling the class imbalance problem in multi-label feature selection.
5.3.4 Comparison with Other Methods. To compare ML-CIB-FS with the commonly used
CMLFS [17] and the recent MLMLFS [43], we further investigated four datasets; namely, Education,
Health, Bookmarks, and Social, as presented in Table 2. Due to the unavailability of the source code
of the two methods, we investigated the high-dimensional datasets using our ML-CIB-FS. We used
the same experiment settings as those used in the previous two experiments. The first experiment
is carried out on two datasets, Education and Health. We pre-processed the datasets by deleting
categories with less than 100 documents as shown in the original study [17] and AUC is used
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:19
Fig. 5. ML-CIB-FS average classification results on Medical dataset.
as a classification performance measure. Figure 9 depicts the classification results of ML-CIB-FS
with respect to the increasing number of features. In line with this experiment, we further investigated the performance of our ML-CIB-FS on the Bookmarks and Social datasets using the same
experiment settings in MLMLFS. MLMLFS is proposed to handle the multi-label high-dimensional
datasets in the presence of missing labels. However, the benchmark datasets used in the original
dataset are not very high-dimensional, with the Bookmarks dataset having the largest number of
features, namely, 2,150 features. MLMLFS achieved the best classification score without explicitly
mentioning the number of selected features in the results. The experiments are performed with 0%,
25%, 50%, and 80% of missing labels, and the results are recorded in Table 5. Taken together, the
results suggest that our proposed ML-CIB-FS algorithm consistently improves the classification
performance on four datasets over the CMLFS and MLMLFS methods.
5.3.5 Time Complexity Analysis. In this subsection, we report the computation time of each
method on all datasets in Table 6 to demonstrate the efficiency of the proposed method compared
to the state-of-the-art methods. This is the time taken by each method to converge to the optimal
solution. To ensure a fair comparison, we conducted the experiments on a five-core Linux server
with 3.10GHZ cores and a total memory of 126GB. The running time in Table 6 is for the optimal
parameter setting for each method. We observe that the proposed algorithm is the fastest compared
to the state-of-the-art algorithms over most datasets. This indicates the efficiency of the proposed
optimization method in converging quickly to the optimal solution.
ML-CIB performs matrix multiplication and matrix inversion computation at initialization, requiring O(d3 + dnl) to computeW and O(l
2
n) to compute Yˆ. At each iteration, it needs O(d2
l) for
∇W and O(ndl + nl 2) for ∇Yˆ .
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
56:20 A. Braytee et al.
Fig. 6. ML-CIB-FS average classification results on RCV1V2(S1) dataset.
Table 5. Evaluation of ML-CIB-FS with Different Proportions of Missing Labels
Datasets Measures 0% 25% 50% 80%
Hamming loss 0.0193 0.0191 0.0208 0.0222
Bookmarks Average precision 0.6762 0.6603 0.6238 0.5886
Macro-F1 0.5778 0.5240 0.5031 0.4691
Hamming loss 0.0148 0.0152 0.0211 0.0249
Social Average precision 0.8226 0.8156 0.7923 0.7523
Macro-F1 0.6261 0.6130 0.5976 0.4896
5.3.6 Convergence Analysis and Parameter Sensitivity. The ML-CIB algorithm uses the accelerated proximal gradient, which monotonically decreases its objective function in Equation (5) until
convergence. Based on the convergence curves in Figure 10, it can be seen that the optimization
function rapidly converges after 15 to 20 iterations, which demonstrates the efficacy of the proposed algorithm.
This section examines the sensitivity of the parameters in the proposed algorithm on four
datasets, namely, Medical, Enron, Emotions, and MIMLtext. ML-CIB has three important parameters: α, β, and γ . These parameters control the contribution of the following components: the new
label matrix manifold, the difference between the new and original label matrix, and the sparsity
of the feature matrix W , respectively. They are tuned using a “grid-search” strategy from 0.1 to
1 with step size 0.1. Due to space limitations, we only report the results in terms of the micro-F1
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:21
Fig. 7. ML-CIB-FS average classification results on RCV1V2(S2) dataset.
Fig. 8. Comparison of ML-CIB-FS with state-of-the-art algorithms using Nemenyi test.
metric over the Medical, Enron, Emotions, and MIMLtext datasets, averaged over five-fold crossvalidation. The experiment results are shown in Figure 11. The results evaluate all the combinations
of the grid search line for the three parameters; however, to enable the results to be visualized, we
fix one parameter and show the results of the remainder. We observe the following results: (1) the
ML-CIB algorithm is not sensitive to its parameters with wide ranges; (2) in most cases, the classification accuracy is increased by increasing the value of parameter γ , which can be interpreted
as a result of imposing sparsity over feature matrix W ; (3) increasing the values of parameters α
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
56:22 A. Braytee et al.
Fig. 9. AUC of ML-CIB-FS with respect to the number of features on the two datasets.
Table 6. The Computation Time (Sec) of Different Algorithms
Dataset MLCIB BR MLSMOTE BR-IRUS COCOA MMIB
Bibtex 576.710 633.51 789.21 711.54 945.23 1,659.23
Medical 6.745 90.23 268.25 68.25 241.23 299.39
Enron 13.531 14.12 20.21 13.13 16.98 21.54
Genebase 2.977 4.56 12.15 9.66 8.52 9.56
Image 5.948 6.15 16.58 9.41 9.58 12.13
MIMLtext 196.860 190.23 269.26 199.87 313.55 295.45
CAL500 15.341 26.35 44.51 18.21 56.54 48.73
Corel5k 539.325 501.24 874.12 485.31 699.54 581.29
Emotions 1.190 12.81 16.41 13.91 14.72 11.57
Education(web) 72.319 174.21 283.29 95.29 89.81 299.19
Languagelog 73.547 108.82 194.19 81.13 95.31 106.91
MediaMill 25,520.38 36,841.14 66,541.21 34,215.51 46,439.11 76,520.38
RCV1V2 (S1) 19,541.54 25,142.21 43,259.19 31,451.17 61,254.15 53,546.56
RCV1V2 (S2) 18,649.54 24,518.88 43,494.08 30,999.01 62,351.19 53,875.22
RCV1V2 (S3) 19,549.59 24,894.30 44,001.58 32,019.16 62,111.07 52,896.48
and β leads to an improvement in classification performance, which indicates the importance of
the contribution in predicting a new label matrix during the training phase.
6 CONCLUSIONS
This study presents an integrated multi-label classification approach that handles three challenges:
namely, class imbalance, incomplete multi-label matrix, and the generation of label correlations
from a Boolean label matrix. The proposed method is able to learn a new multi-label matrix and
simultaneously capture the new label correlations while training the multi-label model. It presents
a label regularization to handle the class imbalance problem, and the l1 regularization norm is
incorporated in the model to select the relevant features. A variant of our proposed method is
presented to evaluate multi-label classification performance using the selected features. Extensive
experiments are conducted on regular-scale and large-scale imbalanced multi-label data, which
clearly show the superiority of our proposed approach over the state-of-the-art methods.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
Correlated Multi-label Classification with Incomplete Label Space and Class Imbalance 56:23
Fig. 10. Convergence analysis for the proposed ML-CIB algorithm.
Fig. 11. Micro-F1 measure results of ML-CIB on the MIMLtext dataset w.r.t. different parameter values.
APPENDIX
A.1 Evaluation Metrics
Evaluating the classification performance of the multi-label learning models using standard evaluation metrics is not appropriate. A number of evaluation metrics have been proposed specifically
for multi-label learning [41]. We use seven evaluation metrics in this study: Hamming loss, oneerror, coverage, ranking loss, average precision, subset accuracy, and micro-F1 [14, 16, 29].
Given a multi-label test set S = {(xi,Yi )}
n
i=1, where Yi ∈ {0, 1}
l is the ground truth labels of the
test point xi , n is the number of test points, |Yi | is the size label set for instance xi , and h(xi ) is the
multi-label classifier. In this section, we define the evaluation metrics:
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 5, Article 56. Publication date: September 2019.
56:24 A. Braytee et al.
Subset accuracy evaluates the correctly classified examples among all labels.
Subset accuracy = 1
n
n
i=1
I 
h(xi ) = Yi,
where I 
. is the indicator function.
Hamming loss evaluates the frequency of an example-label pair being misclassified, i.e., a nonrelevant label is predicted for the example, or relevant is missed.
Hamming loss = 1
n
n
i=1
1
l

l
j=1
I


h(xi )j  Yij
.
Average precision is the average of the proportion of relevant labels ranked higher than a particular
labels.
Average precision = 1
n
n
i=1
1



Yl
i




yj ∈Yl
i




yq ∈ Yl
i : Ri (yq ≤ Ri (yj))


Ri (yj) ,
where Ri (yj) is the predicted rank of the class label yj for an instance xi .
One-error is the fraction of the examples whose most ranked label is irrelevant.
One-error = 1
n
n
i=1
I

min
yj ∈Yi
Ri (yj  Yi )

.
Coverage is the metric that on average computes the number of steps are needed to cover all the
relevant labels for the instance.
Coverage = 1
n
n
i=1
max
y ∈Y
Ri (y) − 1.
Ranking loss computes the fraction of the incorrectly ordered label pairs.
Ranking loss = 1
n
n
i=1




(ya,yb ) : Ri (ya ) > Ri (yb ), (ya,yb ) ∈ Yl
i × Yl
i







Yl
i







Yl
i




,
where Y
i = Y \ Yi , and (ya,yb ) is the pair class label for instance xi .
Micro-F1 considers class imbalance issue. It evaluates every label separately using F 1 measure
and averages over all labels.
Micro − F1 = 2
l
j=1
n
i=1 yijh(xi,yj)
l
j=1
n
i=1 yij + l
j=1
n
i=1 h(xi,yj)
.        