In data center networks (DCNs), flows with different objectives coexist and compete for limited network resources (such as bandwidth and buffer space). Without harmonious resource planning, chaotic competition among these flows would lead to severe performance degradation. Furthermore, low latency is critical for many emerging applications such as augmented reality(AR), virtual reality(VR) and telepresence, making the network control problem even more challenging. To address the above issues, this paper novelly proposes a receiver-driven two-layer control framework called HOPASS, which incorporates a slow control layer and a fast control layer to strike a balance among multiple network sharing objectives and achieve low latency. The slow control layer ensures bandwidth guarantee in an aggregated flow level by solving a multi-objective network utility maximization (NUM) problem using an online learning approach. Then the results will be dispatched to the data plane by configuring weights in the switches with weighted fair queue functionality. Under the configuration dictated by the slow control layer, the fast control layer leverages the token packets sent by receivers to dynamically probe and reserve network capacity, so that it can proactively prevent network congestion and guarantee low latency data delivery. To evaluate the proposed framework, we have implemented HOPASS in ns-3 and conduct extensive experiments under various network scenarios. The simulation results show that HOPASS achieves near-optimal performance in terms of bandwidth allocation in multi-objective scenarios and also guarantees low end-to-end delay. Moreover, it outperforms DCTCP and NewReno in terms of average bandwidth utilization and global total network utility at the aggregated flow level. Therefore, we conclude that HOPASS provides an effective framework for DCNs when considering both multi-objective optimization and a low latency network.

Previous
Next 
Keywords
Data center

Multi-objective optimization

Receiver-driven protocol

Flow scheduling

Bandwidth allocation

1. Introduction
Due to the development of the emerging applications with various service requirements in today’s data center networks (DCNs), flows with different performance objectives coexist and compete for the limited network resources. Infrastructure-as-a-Service clouds has introduce multiple kinds of virtual machine (VMs) (Masdari and Zangakani, 2019, Hosseinzadeh et al., 2020), which could have different amounts of various resources. Through the Vms, customers could achieve their different level of Quality of Service (QoS), which brings a significant challenge for the network resource allocation and flow scheduling. For applications like on-demand high-definition video streaming, the associated flows require high throughput. Meanwhile, some real-time applications, like live telecast and network conference, need low end-to-end latency for flows. Moreover, some applications demand high throughput and low end-to-end delay for flows, such as augmented reality (AR), virtual reality (VR) and telepresence. As for applications such as email, they only require best-effort services. It is evident that some of these objectives may conflict with each other. Thus, a multi-objective network utility maximization (NUM) problem needs to be solved to strike a balance among these conflicting objectives.

There has been a tremendous amount of effort in solutions to the NUM problem. However, as far as we know, most of the existing solutions only try to optimize network performance with a single objective. For example, the solutions in Alizadeh et al., 2010, Alizadeh et al., 2013, Munir et al., 2013, Nagaraj et al., 2016, Perry et al., 2014, Vamanan et al., 2012, Zats et al., 2012, Frohlich and Gelenbe, 2020 and Fröhlich et al. (2021) only focus on optimizing a single performance objective such as maximum throughput, minimum congestion, or minimum latency. Note that the single objective optimization may lead to the degradation of other performance metrics because some objectives such as throughput and delay naturally contradict each other. Simply applying a single-objective solution can only satisfy some applications’ requirements while hurting the performance of applications with a different objective. It is desirable to consider the trade off among different performance objectives to achieve relative fairness among different flows.

However, even given a multi-objective resource allocation solution, there still needs an objective-oriented scheduling among flows under various constraints to achieve the performance balance and to address the congestion in the network data plane. Such a congestion control problem is very challenging because of the well-known incast problem incurred by massive bursty and concurrent traffic from parallel-computing (Zaharia et al., 2012) and distributed deep learning (Abadi et al., 2016, Li et al., 2014) applications. Traditional sender-driven transport layer protocols such as TCP CUBIC (Ha et al., 2008) and DCTCP (Alizadeh et al., 2010) cannot ensure low queuing delay, zero data loss and high throughput under the incast scenario. Moreover, in DCNs, queuing delay is the main contributing factor for latency since propagation delay is low. Therefore, considering the commonly used many-to-one communication pattern in DCNs, we propose to adopt receiver-driven congestion control to resolve this issue.

Received-driven protocols (Gao et al., 2015, Cho et al., 2017, Handley et al., 2017, Montazeri et al., 2018) have been quite popular recently to solve congestion issues in DCNs due to incast traffic. The idea of receiver-driven control is that the senders can learn the guaranteed amount of traffic to be sent based on the number of tokens received from the receiver. Such a control scheme reacts to the network congestion in a proactive way instead of the passive manner used in the sender-driven control schemes.

In this paper, we propose HOPASS, a two-layer control framework for DCNs. With the cooperation of the two control layers, HOPASS can not only achieve the near-optimal performance for the global multi-objective NUM problem but also guarantees low latency for small flows. Specifically, HOPASS consists of two layers: (i) a slow control layer (SCL) that allocates bandwidth among aggregated flows with different objectives and works at a coarse-grained mode, and (ii) a fast control layer (FCL) that works at a fine-grained mode and acts as a received-driven protocol to schedule the flows under the allocated bandwidth decided by the SCL and to do the congestion control function.

There are several challenges to designing such a two-layer control framework. Firstly, different objectives may contradict each other and how to design a scheme to ensure fairness across flows is very challenging. Secondly, enforcing the solution of the multi-objective optimization in a distributed manner is desirable for practical implementation. Thirdly, ensuring low latency for small flows while preventing the large flows from starving. Our proposed framework, HOPASS, addresses the above challenges and the contributions are summarized as follows:

•
A two-layer framework for solving multi-objective NUM and ensuring low latency: This framework includes two layers that focus on solving a multi-objective NUM problem and resolve congestion, respectively. The two layers work together to balance the performances of flows with different objectives and ensure a low latency in the data plane.

•
An online learning based approach to solving the multi-objective NUM problem in the slow control layer: The learning based approach can solve NUM problems with a wide variety of utility functions. What is more, our proposed method converges fast and the convergent result is near-optimal.

•
An objective-oriented scheduling among flows based on a receiver-driven approach: Receivers dynamically adjust the token sending rate based on the estimated network state (e.g., congestion level and packet drop rate), and schedule small flows and large flows at different frequencies. Then the senders can only send data after receiving tokens. Such a mechanism can positively address the possible congestion and meet the requirements from both latency-sensitive mice flows and bandwidth-hungry elephant flows.

The rest of the paper is organized as follows. Section 2 introduces the related work and the motivation. Section 3 describes the design details of HOPASS. Section 4 presents the modeling process and the theoretic analysis of the SCL and FCL, respectively. Section 5 evaluates HOPASS against the serval classic method and shows the performance of HOPASS. Finally, Section 6 makes a conclusion.

2. Background and motivation
2.1. Multi-objective network utility maximization
In DCNs, flows associated with different applications may have different performance requirements, such as flow completion time and end-to-end throughput. Most of the existing solutions only focus on optimizing a single performance objective, such as minimizing the end-to-end delay or maximizing the throughput. NUMFabric (Nagaraj et al., 2016) converted the bandwidth allocation problem into a Network Utility Maximization (NUM) problem and solved it in a distributed manner. However, the solution in NUMFabric cannot be extended to solve the multi-objective NUM problem. DeTail (Zats et al., 2012),  (Munir et al., 2013), pHost (Gao et al., 2015), pFabric (Alizadeh et al., 2013), PIAS (Bai et al., 2015) and Homa (Montazeri et al., 2018) are typical works with the objective of minimizing the flow completion time (FCT). Even though some works studied the coexistence of multiple utilities in the network, none of them can give a complete and effective solution.

On the other hand, existing works that consider multiple objectives either fail to clarify how to allocate resources among flows with different objectives, or require too much time to converge to the optimal allocation. SCC (Tian et al., 2017) proposed a two-layer control method to solve a multi-tenant multi-objective bandwidth allocation problem, but it only solves this problem with given weights. SCC does not give a solution on how to get the weights that determine how many resources each type of flow is allocated with. BwE (Kumar et al., 2015) proposed a hierarchical bandwidth allocation framework, which optimizes the performance of flows with different requirements in terms of throughput and fairness. However, BwE is a purely centralized solution that requires global information from the network and induces a very long convergence time.

Besides, some methods based on learning approach have been applied to solving bandwidth allocation problems recently and are quite useful when dealing with highly dynamic networks. The authors in Xu et al. (2018) proposed a centralized flow scheduling strategy based on deep reinforcement learning (DRL). The authors in Dong et al. (2018) also consider using DRL approach to adjust the flow transmission rate to adapt to the dynamics of the network. However, even these solutions can modify the reward function to realize different objectives, they are not able to achieve multiple objectives at the same time.

2.2. Receiver-driven protocol with congestion control
Typical applications in DCN, such as MapReduce (Dean and Ghemawat, 2004), web search, and distributed machine learning, often employ the many-to-one communication pattern. This leads to bursty flow arrivals and a large number of concurrent flows. Such incast traffic essentially makes most network congestion happening at the receiver side. Hence, the traditional sender-based congestion control schemes (Ha et al., 2008, Alizadeh et al., 2010, Wilson et al., 2011, Vamanan et al., 2012) fail to perform well since the congestion signal delivered from the far end are pretty delayed and the senders can only react to the delayed signal to avoid congestion in a very passive way.

Instead, the receiver-driven protocol enables proactive congestion control using token packets (also known as credits Cho et al., 2017 or message grants Montazeri et al., 2018). When a sender initiates a new flow, it first sends a request-to-start (RTS) message to the receiver. Once receiving an RTS message, the receiver determines how many data packets can be sent by each sender based on its available network capacity. The receiver then issues the associated number of tokens to the senders per maximum transmission unit (MTU) time. On receiving each token from the receiver, the sender sends one packet. Therefore, the number of tokens received strictly determines the amount of data traffic the sender pours into the network. Then the sender can have a rough estimate on the available network capacity, and the receiver can also avoid network congestion and bursty traffic pouring into the network by adjusting the token sending rate.

However, existing receiver-driven protocols have mainly focused on minimizing flow completion time (FCT) for latency-sensitive mice flows. Bandwidth-hungry elephant flows either become starved due to low priority (Gao et al., 2015, Montazeri et al., 2018), or only achieve fair sharing (Cho et al., 2017, Handley et al., 2017) without differentiated QoS. Furthermore, congestion can happen within the core network. As shown in Fig. 1, we measure the queue lengths of the switches in the network core under three different production traces and the same topology as in pHost’s paper (Gao et al., 2015), the average queuing delay is as large as 895 ns (180  715 under IMC10). Given that the per-link propagation delay is only 200 ns, the queuing delay is crucial in end-to-end latency. We compared the characteristics of FCL with some similar solutions, the comparisons of FCL against these solutions are summarized in Table 1. In pHost (Gao et al., 2015) and Homa (Montazeri et al., 2018), the authors assume congestion only happens in the edge network and the core network is congestion-free. Hence, as the table shown, the proposed receiver-driven approaches cannot be applied to solve any in-network congestion. ExpressPass (Cho et al., 2017) tackles the in-network congestion issue by actively limiting the token sending rate at switches and enforcing symmetric routes for token and data packets. However, it requires non-trivial modifications of switches which is highly undesirable for commodity switches. Although FCL takes switch support into the design to achieve better performance with the cooperation of SCL, the operations in switches are quite simple and can be soft-implemented.

2.3. Motivation and challenges
Nowadays, optimization of network performance pays more attention to the efficiency of network resources such as throughput, bandwidth utilization, delay and so on. Under the trend of cloudification for applications, data center networks should satisfy the differentiated requirements from different kinds of users and concentrate more on applications’ efficiency. That is why we are trying to solve the multi-objective NUM problem in data center networks under limited network computing and transmission resources. The reward for networks or customers may be different for flows of different services while obtaining the same network resources. Utility functions, which map the obtained bandwidth to the expected reward, are introduced to quantify the network utility. Diversified or user-defined optimization objectives can be supported by changing the form or parameters of the utility functions. At present, the objectives that data center networks focus on mainly include but are not limited to those listed in Table 2.

In Table 2, different utility functions represent different requirements of bandwidth. -fairness (Kelly, 1997) focuses on the fairness among the bandwidth allocation of different users. Proportional fairness is a special case of -fairness. When , according to the L’Hospital’s rule, the utility function of -fairness can convert to utility function of proportional fairness. The third utility function concentrates on a performance metric named flow completion time (Munir et al., 2013). In this functions,  represents the flow size.


Table 2. Common optimization objectives and corresponding utility functions.

Objective of flows	Utility function
Proportional fairness	
-fairness	
Minimize flow completion time	
Remark: To ensure that the utility functions are concave,  ranges from 0 to 1.

In the scenario of multi-objective coexistence, flow control method should allocate the limited network resources to different flows reasonably to maximize the total utility of the data center networks. With the help of utility functions, flow control problem in data center networks can be modeled as a multi-objective NUM problem. Its mathematical expression can be listed as follows: (1)Here,  represents the number of utility functions coexisting in the networks.  represents the number of flows for th objective.  represents the utility function of th objective.  represents the bandwidth capacity of the link.  represents the bandwidth allocation results.  represents the bandwidth allocation for the th flows with objective  and .  is a  matrix and  denotes the number of links of the network.  in the matrix is 1 when the th flow gets through the th link, otherwise it is 0. The constraint condition of this problem mainly comes from limited link capacity.

If we constrain the utility functions to concave functions, the multi-objective NUM problem will be transformed into a convex optimization problem. This problem can be solved mathematically by the conditions of Lagrange Multiplier and KKT (Karush–Kuhn–Tucker) condition. Relying on the centralized control architecture, the mathematical solution mentioned above is theoretically feasible, but there are still many challenges in practical applications. As the scale of traffic in data center networks continues to increase, the multi-objective NUM problem’s scale is growing larger and larger. Although centralized control architecture can guarantee a globally optimal solution, it also introduces long communication and calculation time. Long communication latency is caused by the extensive information exchange between the central controller and other network devices, such as hosts and switches. Long calculation time comes from the centralized solution to large-scale convex optimization problems. However, the traffic in data center networks has obvious heavy-tailed characteristics. Most of the flows are short flows, but the long flows occupy most of the traffic. Most of the time, the short flows have finished transmitting before the optimal bandwidth allocation results are figured out. On the contrary, the distributed control architecture can provide fast control for packets, but it cannot guarantee convergence to the global optimal point because the decisions were made based on local information. Neither the centralized control architecture nor the distributed control architecture can simultaneously take the needs of both fast control and accurate control into consideration. To address this problem, it is inevitable to re-design the control framework.

2.4. A two-layer control framework
Based on the above analysis, it is complicated to either solve the multi-objective NUM problem or achieve multi-objective performance with a low-latency guarantee for small flows and reasonable congestion control in a distributed manner. It is far more challenging if one wants to achieve both at the same time.

Some researchers do this problem from a macro perspective. They aim to find request routing paths or a mapping between requests and services through machine learning methods. CRE (Francois and Gelenbe, 2016) is a logically centralized cognitive routing engine based on random Neural Networks with Reinforcement Learning. When the overlay nodes use the public Internet as the communication means, it tries to find the optimal overlay paths with minimal monitoring overhead. Reinforcement Learning is also used in paper (Frohlich et al., 2020) to help an SDN controller to direct user requests at the edge toward appropriate servers where the requests can be satisfied. Paper (Wang and Gelenbe, 2018) designs three online QoS aware adaptive task allocation schemes to provide a lower response time by assigning tasks to sub-systems and splitting the task arrival stream into sub-streams at rates computed from the hosts’ processing capabilities. Instead of leading requests to the instantiated services, paper (Frohlich and Gelenbe, 2020) focuses on the services placement optimization by using the SDN and machine learning means.

Different from the above methods, this paper focus on finer granularity. Suppose that the flows’ destinations have been determined, when the multi-objective flows go through the same path, how to guarantee a fair and stable bandwidth allocation and low latency simultaneously? We propose a two-layer control framework that incorporates a slow control layer and a fast control layer to address this. Specifically, the slow control layer works in a coarse-time granularity and allocates the network resources at a macroscopic scale, that is, aggregated flow level. The slow control layer is adaptive to the static network states (such as network topology) and guarantees the bandwidth allocation for aggregated flows. As for the fast control layer, it operates in a decentralized way, and it is adaptive to the dynamic information of the network (such as non-persistent congestion). Also, this layer considers both latency-sensitive mice flows and bandwidth-hungry elephant flows. We illustrate such an design idea in Fig. 2.

Under the above design of the two-layer control framework, we observe that both layers are critical to ensure good end-to-end performance. Suppose there is only a slow control layer, it will be hard to guarantee low latency network and effective weighted bandwidth allocation without the network congestions control in the fast control layer. On the contrary, if there is no global performance guarantee without the slow control layer, the overall network performance would be severely affected. What is more, the two-layer control combined with fast and slow control can balance the contradiction between accurate bandwidth allocation and fast scheduling for packets.

3. Design of HOPASS
In this section, we illustrate the design ideas behind HOPASS. Note that our goal is to achieve multi-objective NUM and provide a low-latency network infrastructure based on reasonable congestion control. It is difficult to ultimately achieve the above goals if innovations are only made in algorithm design. In order to strike a balance between control accuracy and efficiency, changes in the control framework are necessary. This paper profoundly studies the traffic control architecture of data center network from the two dimensions of control framework and algorithm.

3.1. Overview of the architecture
The nervous system of humans has important enlightenment for the design of the control system. It is mainly composed of the brain and spinal cord. The brain is responsible for regulating some complex activities such as thinking, learning and so on. The spinal cord is responsible for some simple reflex activities, such as knee jerk reflex. Inspired by the nervous system, John C. Doyle proposed fast and slow neural control theory, emphasizing the importance of combining centralized slow control and distributed fast control. Based on the above theory, we proposed a coordinated distributed flow control framework. This framework is still a distributed control framework in essence to guarantee low control delay and transmission delay. However, it allows control nodes to fully coordinate with each other or collect information on a longer time scale. And then, an optimal control result will be calculated to adjust the fast control for each packet. The proposed control framework can be regarded as a compromise between centralized control and distributed control. It is expected that this framework is able to provide accurate flow control results while ensuring the timeliness of control.

Based on the proposed coordinated distributed control framework, this paper design a hierarchical flow control method named HOPASS to solve the multi-objective NUM problem in data center networks. Unlike the flat control paradigm, such as traditional pure distributed control or centralized control, a hierarchical control structure makes it possible to leverage the idea of divide and conquer to solve the large-scale NUM flow control problem. The multi-objective flow control problem can be decomposed into the following two sub-problems:

•
Coarse-grained bandwidth allocation problem focuses on how to allocate bandwidth among different objectives to maximize the overall utility of the network. This problem can be defined as Eq. (2). (2)where  represents the number of utility functions coexisting in the networks.  represents the utility function of th objective.  represents the bandwidth capacity of the link .  represents the bandwidth reserved for the th objective on link .

•
Fine-grained packets scheduling problem, defined as Eq. (3), mainly focuses on solving the single-objective NUM problem and realizes objective-oriented packet scheduling under the specific bandwidth constraints. Besides, it is also an important problem that how to build a low-latency DCN through fast packet scheduling. (3)where  represents the number of flows for th objective.  represents the transmission rate for the th flows with objective .  indicates whether the th flows with objective  passes through the link . When the flow passes through link , this value will be set as 1, otherwise 0.

HOPASS consists of a Slow Control Layer (SCL) and a Fast Control Layer (FCL). The SCL is responsible for the macro control for the network resources. It allocates the network resources in a relatively long timescale (usually every few microseconds) and searches for a near-optimal bandwidth allocation for different objectives to maximize the overall network utility. The FCL focuses on micro scheduling at the packet level. Taking the bandwidth pre-allocated by the SCL for different objectives as input, the FCL uses specific packet control strategies to further allocate bandwidth pre-allocated to flows with the same objective. Another target for FCL is providing a best-effort performance guarantee for both latency-sensitive mice flows and bandwidth-hungry elephant flows. In HOPASS, the combination of SCL and FCL can not only ensure the fast scheduling for packets on a small time scale, but also calculate an optimal bandwidth allocation scheme on a large time scale to improve the overall utility of the DCNs. More details of HOPASS are shown in Fig. 2.

Deployed only on the switch, the SCL prefers to leverage a learning-based approach to solve the bandwidth allocation problem among different objectives. As a result, in SCL, the unit of scheduling is aggregated flow. For a specific link, the flows with the same utility function will be classified into an aggregated flow. Since SCL needs to collect more local information in a coarse-time granularity to support an optimal decision, the control cycle of the SCL is relatively long.

In SCL, a packet is the basic unit of scheduling. Therefore, compared with SCL, the control cycle of FCL is significantly smaller. The available packets scheduling methods of FCL are diversified, such as the adjustment for transmission window or rate, queue management strategy, etc. As a result, the deployment of FCL can be divided into two parts. The first part is the queue management strategy deployed on the switch to ensure that the network traffic approximates the near-optimal results calculated by SCL. The second part is deployed on the end-hosts to provide transmission control and objective-orient packets scheduling. In design, the FCL is compatible with existing or redesigned single-objective flow control methods. In our previous work (Lei et al., 2019), we made necessary modifications to an existing method NUMFabric (Nagaraj et al., 2016), and then used it as the fast control layer algorithm, which verified the feasibility of this design. In this paper, we redesign a receiver-driven protocol with congestion control for FCL. Proactive congestion control in FCL helps to meet the requirements of weighted bandwidth sharing and low-latency.

3.2. Design of slow control layer
In this section, we will illustrate the design of SCL in details. While solving the coarse-grained bandwidth allocation problem shown in Eq. (2), the SCL algorithm’s design is faced with two challenges. On the one hand, the requirements of upper-layer applications and users keep changing, which puts forward the new demand for the service quality provided by the under-layer data center networks. As a result, the expressions and parameters of utility functions are diversified and will continue to expand. On the other hand, traffic in DCNs is highly dynamic because of the difference and uncertainty in flow size, objectives, priority and start time. Besides, burst traffic and network failures are inevitable and unpredictable. Most of the traditional flow control methods in DCNs were designed based on the network’s mathematical model. To guarantee that the method works in a specific network, it is necessary to build an accurate model for the network environment and user requirements.

To deal with the two challenges mentioned above, HOPASS applies an online convex optimization approach in the design of SCL. The online learning approach has the following two advantages. Firstly, it is independent of any specific model. Secondly, compared with the traditional model-based approach, this approach is applicable to a wider range of utility functions. So the restrictions of utility functions are limited. To ensure a near-optimal solution, the utility functions are required to be smooth and strictly concave.

Leveraging the idea of Online Gradient Descent (ODD) in online convex optimization (Shalev-Shwartz, 2011), the SCL can dynamically search for an optimal or near-optimal bandwidth allocation scheme through periodic exploration and decision. The SCL algorithm includes two key steps. First, SCL explore the rewards for different bandwidth allocation. Second, SCL would make decisions based on several historical decisions’ feedback.


Download : Download high-res image (212KB)
Download : Download full-size image
Fig. 3. Control cycle of slow control layer.

In SCL, time is divided into continuous slots of equal size. The bandwidth allocation will be updated every time slot. As shown in Fig. 3, each control cycle consists of two phases, i.e., an exploration phase and a decision phase. Each exploration phase contains several time slots. The number of time slots in the study phase is equal to the number of objectives coexisting in the network. In the exploration phase, each time slot is used to explore the reward of a specific bandwidth adjustment strategy. The reward is estimated in terms of the increment of the total utility value of all the objectives. Each decision phase contains only one time slot. According to the result of the exploration phase, the decision maker will calculate a bandwidth allocation result with a potential largest reward for aggregated flows.

Denote  as the number of aggregated flows coexisting in a link. 
 represents the ratio of capacity that the th utility shares on the th link. Since we want to ensure a high utilization of bandwidth and avoid wasting the resources, the constraint shown as Eq. (4) is set for 
: (4)
Therefore, the allocated bandwidth on link  for aggregated flow  can be calculated using (5)

In exploration phase, each time slot is used to explore the reward of a bandwidth allocation adjustment direction. In each bandwidth adjustment direction, the bandwidth of a utility is increased by a fixed step length 
, and the reward of bandwidth of other utilities are reduced in equal proportion. At the end of each time slot in the exploration stage, according to the results of network measurement, the decision maker uses the corresponding utility function to quantify the transmission performance of the time slice into numerical values.

The bandwidth allocation strategy is calculated based on the bandwidth allocation results 
 of the previous control cycle. At the th slot within the exploration phase, the th bandwidth allocation is increased by 
 while the others are all decreased by 
 
. This rule maintains the constraint in Eq. (4). The expressions for bandwidth update are summarized as follows: (6) 
 
 where 
 represents the updated allocated bandwidth value for the th aggregate flow on the th link.

The value of the step size 
 have an impact on the convergence of the algorithm of SCL. If the step size is too small, it will induce a long convergence time, which is undesirable in highly dynamic networks. It is also inappropriate to set it too large because a large step size induces a coarse searching process in the feasible set of problem (2), which may lead to suboptimal solutions.

In decision phase, the decision maker evaluates the bandwidth allocation strategies that has been explored in the exploration phase and find out a bandwidth adjustment direction which has the largest potential reward. Then the decision maker will compares this reward with the reward of previous decision phase. If this reward is larger, the decision maker will update the bandwidth allocation according to . Otherwise, it means that the total network utility cannot be further improve by any adjustment of bandwidth allocation. The bandwidth allocation will remain the same as the previous control cycle. (7) 
 
 where 
 is index of time slot that gained the largest total network utility. As shown in Eq. (8), 
 is the increment step size in decision phase. If the step size is too large, the risk of overshooting the optimal solution will increase. So we set an upper boundary 
 for it. (8)
 
 
Here,  is a sensitive variable which measures how aggressive the bandwidth strategy is updated.

3.3. Design of fast control layer
In this section, we will discuss the design of the FCL. As mentioned above, the implementation of FCL includes two parts, i.e., the queue management strategy on the switches and the transmission control protocol on the end hosts.

3.3.1. Queue management strategy on switches
To avoid the mutual interference among packets with different objectives while scheduling, the SCL requires the underlayer to guarantee performance isolation among different objectives. As shown in Fig. 5, an isolated queue mechanism is designed in the FCL. Multiple isolated queues are used to replace the single queue to manage packets. Packets with different optimization objectives are processed in different queues. To ensure that the actual network traffic is approximate to the bandwidth allocation result calculated by the SCL, the round-robin mechanism is used as the inter-queue scheduling strategy in FCL. The packets management strategy inside the queue depends on the transmission control protocol. The macro control results for bandwidth allocation calculated by the SCL are used to update the queue management strategy’s parameters periodically. In HOPASS, end-hosts rely on the Explicit Congestion Notification (ECN) mechanism (Leung and Muppala, 2002) to realize proactive window adjustment according to the congestion level. The packets dequeue according to a first-in–first-out (FIFO) manner. Besides, the packets will be marked with an ECN label once queue’s length exceeds a certain threshold. The marking threshold will be updated periodically based on the output of the SCL.

3.3.2. Transmission control protocol on end-hosts
Specifically, the FCL gives a receiver-driven congestion control with three participants as shown in Fig. 6, i.e., senders, switches, and receivers. The implement of fast control layer algorithm mainly includes two parts: sending and receiving packets by end host and queue management of switch. The realization of the function of sending data packets by the end host mainly includes checking the sending window and setting the packet header parameters of the data packets. The behavior of the end host receiving data packets mainly includes updating the measured values and reading the information exchanged with the intermediate nodes.

As shown in algorithm 1, when a flow starts, the sender will send an RTS message and several pioneering data packets to the receiver. The RTS message contains some information such as flow size, flow weight and so on. Pioneering data packets transmission without waiting for the first batch of tokens from the receiver mitigates the end-to-end delay and reduces the latency of small flows. For subsequent data transfer, the sender will send data in the arrival order of corresponding tokens. As for switches, they need to support ECN marking firstly. When in-network congestion occurs and the queue length exceeds the marking threshold, data packets will be marked by switches. Such congestion information will be passed to receivers to handle. Secondly, to enhance the cooperation with SCL, the part of FCL, which is deployed on the switch, is designed to take weighted-round-robin scheduling for flows with different objectives.


Download : Download high-res image (315KB)
Download : Download full-size image
Fig. 6. Fast control layer overview.

Receivers are responsible for scheduling flows, updating token rate and handling in-network congestion, as shown in algorithm 2. Each receiver maintains one queue for active small flows and one queue for active large flows. When a receiver receives an RTS, it puts the flow into the corresponding queue according to its size. To achieve the near-optimal performance of latency-sensitive small flows, each receiver in HOPASS just applies shortest-remaining-processing-time (SRPT) first scheduling inside the queue of active small flows, since SRPT is known as the optimal algorithm for minimizing FCT. However, for the in-queue scheduling of large flows, if we just use the same way, it is obvious that there will be some large flows that get starved. So we introduce a new metric 
 to assign each large flow an ordering, to alleviate the conflict between the flows that are relatively “small” and flows that are relatively “large” but have been waiting for a long time. It is defined as follows: 
 
where  is a constant value to protect the flows that are relatively small from being preempted too easily by flows that are relatively large and just wait for a little while. And OPT means the optimal FCT of flow (that is, the flow completion time if it is the only flow in the network). Specifically, the tokens of large flows from the same receiver will be sent to the sender in the increasing order of 
, namely, the flow with the smallest 
 is scheduled first at receivers. For example, there are three large flows F1, F2 and F3, with a same receiver. F1 is relative large whose OPT is 1000 and starts at time 0. F2 and F3 start at time 300 whose OPT are 200 and 900 respectively. Assume  equals 100. At time 300, F2 will be scheduled first since its 
 equals 2 and is smaller than F1’s 
 (2.5) and F3’s 
 (9). Then F1 will be scheduled as it has been waiting for a long time, and finally F3 will get its turn to be scheduled.


Download : Download high-res image (272KB)
Download : Download full-size image
As for the scheduling between large flows and small flows, the receiver simply adopts Weighted-Round-Robin (WRR) scheduling. So small flows will be assigned tokens more frequently than large flows, which means that the small flows have a higher priority to be sent and will not experience head-of-line blocking due to large-sized flows. When there are only small flows or large flows, the active flows will be scheduled in every slot to avoid bandwidth waste. This scheduling scheme aims to improve the throughput of large flows with a small compromise for the FCT of small flows.

However, only WRR scheduling cannot handle in-network congestion. The leading cause of in-network congestion is the traffic from large flows. Therefore, the large flow should back off when congestion occurs. Besides, to achieve weighted bandwidth allocation, large flows’ token rates should be decreased based on their weights.

ECN is proven to be efficient for congestion control in data center networks (Alizadeh et al., 2010, Vamanan et al., 2012, Mittal et al., 2015), which is available in commodity switches. We use ECN in our design as the signal of congestion to adjust token sending rate, which can react to congestion from the receiver side. Furthermore, to meet the weighted-fair bandwidth allocation, we play a small trick on the back-off size.

DCTCP (Alizadeh et al., 2010) introduces  to decrease rate sizes. When congestion happens, every flow has the same back-off size, which is not reasonable as the large flows with different objectives have different weights. We embed flow weights in the token control loop (see the formula in line 15 of algorithm 2). The larger the weight is, the smaller the back-off size is. We will prove this token control loop can achieve weighted sharing in Section 4.

As flows have different objectives, the data packets of a flow will enter the corresponding logical weighted queue (which is soft-implemented) inside each switch along the path and get a weighted round-robin scheduling. The slow control layer has configured a ratio of capacity, 
 for each queue. If the aggregated data rate of flows belonging to the same slice exceeds the allocated capacity, 
, a persistent queue will be built up. Once the queue length exceeds the ECN threshold, all subsequent data packets will be marked. Receivers will then adjust the token sending rate accordingly. In this way, the fast control layer can gradually learn the capacity allocated by the slow control layer, and flows with the same objective will fairly compete for the allocated bandwidth.


Download : Download high-res image (600KB)
Download : Download full-size image
4. Modeling and theoretic analysis
In this section, we give theoretic analysis for the two control layers of HOPASS respectively. The analysis for SCL is to verify the correctness and the efficiency of the multi-objective NUM problem solving. And the analysis for FCL, we mainly focus on the part deployed on end-hosts which acts as a receiver-driven protocol.

4.1. Analysis of slow control layer
In this section, we will explore the conditions to ensure that the SCL algorithm can converge to a near-optimal solution, and the associated proofs will be provided.

It has been established in paper (Zinkevich, 2003) that gradient descent is an effective method for the online convex programming problems. The average regret can be guaranteed to approach zero, while applying gradient descent algorithm to solve the convex optimization problem.

When the objective function is concave, it has only one local maximum. The only local maximum value found by the gradient descent algorithm is also the global maximum. Reduction to absurdity will be applied to prove this principle as follows: Assuming that the bandwidth allocation vector 
 calculated by the SCL is only a local maximum instead of a global maximum, then there must be another bandwidth allocation vector 
 satisfying the linear constraints in Eq. (2), so that 
. We can construct the following vector  (9)

According to the property of concave function, we have the following inequality: (10)

When ,  approaches 
, which means  become a point in the neighborhood of 
. However, Eq. (10) shows that 
. It contradicts with the assumption that 
 is the local maximum solution. This is because if 
 is the local maximum solution, inequality  always holds for any point  in its neighborhood. Therefore, such an assumption is invalid. The local maximum value is also the global maximum value in convex optimization problem.

In the next step, it will be proved that the problem described in formula (2) is a convex optimization problem, so the principle proved above can be applied on this problem. The proof is described as follow. Firstly, the feasible region of formula (2) is a convex set, as the constraints are linear. Secondly, the sum of all utility functions is concave. Specifically, to verify the concavity condition, we calculate the second-order derivative of the objective function of the NUM problem and we have (11)
 
 
 
 
 

Since  is positive and bounded and , 
 always holds, meaning the utility functions are all concave. Therefore, the objective function is concave.

4.2. Analysis of fast control layer
In this section, we analyze the properties of FCL from a theoretic perspective. We first develop a fluid model for large flows’ control feedback loop of token rate. On top of the model, we further derive how the classification threshold will influence the bandwidth sharing among large flows and the queuing delay experienced by the small flows.

Before the detailed illustration of the fluid model, let us review the update process of large flows’ token rates in congestion. The receiver maintains a running estimate of the fraction of marked data packets. This estimate, , is updated once roughly each round-trip time: (12)where F is the fraction of marked data packets in the most recent , and  is a fixed parameter using for the exponentially weighted moving average estimation. And the token rate of a large flow with a normalized weight 
 (the smallest weight equal to one) is updated as follows: (13)
 
It is clearly that only the token rate of the large flow with the smallest normalized weight  will be nearly reduced by half, when  is close to  (heavy congestion).

4.2.1. Fluid model for weighted sharing among large flows
To analyze the behavior of our token control loop, we develop a fluid model for large flows with different predefined weights.  long-lived flows traverse a single bottleneck switch with capacity . The bottleneck only resides in the path for data packets, whereas the token packets are sent to senders without experiencing any congestion. The following non-linear, delay-differential equations describe the dynamics of token window 
, the running estimate of the fraction of marked data packets , and the queue size at the switch, : (14)
 
 
 
(15)
 
 
(16)
 
 
 Here  is the round-trip time (), where  is the propagation delay (assumed to be equal for all flows). 
 is the approximate fixed value for the delay. 
 indicates the packet marking process at the bottleneck switch; 
 is the normalized weight such that the smallest weight equal to one.

Eq. (14) models the evolution of token rate.  is the standard additive increase term when there is no congestion, and 
 is the multiplicative decrease term which models the reduction of token rate by a factor 
 when the congestion occurs. Eq. (15) is a continuous approximation of (12). And Eq. (16) models the queue evolution : 
 is the net input rate and C is the bandwidth capacity.

Through numerical analysis, we plot the trajectory for the token window and queue lengths when three flows compete through one bottleneck switch in Fig. 7. It is clear to see that HOPASS is periodic stable, which can be proved using the same method in Alizadeh et al. (2011). We can see from the results that the token window of flows with different weights stabilize around its own weighted share. Since senders in HOPASS immediately sends a data packet once it receives a token, the throughput of each flow is essentially proportional to the number of tokens it received in every round trip. Since all flows share the common bottleneck, they will experience the same increase phase of token window, and then use their own weights to backoff once receiving ECN markings. It follows that in the stable state, the decrease phase of different flows’ token rates satisfies: (17)
 
 
where 
 and 
 are the maximal token window before entering the decrease phase. Therefore, we have 
. In other words, the peak token rates achieve strictly weighted sharing.

4.2.2. Influence of classification threshold
We now analyze the influence of the classification threshold for our WRR scheduling. Suppose the threshold for classifying flows into large flows with low scheduling frequency is . Flow arrivals form a Poisson process with rate . It has been observed that in DCNs, the flow size distribution is heavy-tailed (Montazeri et al., 2018). We assume the flow size distribution follows a Pareto distribution with the tail index (or Pareto index) as . Let  be the probability density function for flow size distribution. We have 
 
.

For flows with sizes smaller than the threshold, they will be classified as small flows, enter the queue with higher scheduling frequency, get sufficient tokens and finish. The average size of small flows in the queue with high scheduling frequency can thus be computed as: (18)
(19)
 
 
 Since  for production data traces (e.g., 
 satisfies the “80-20 rule”),  is an increasing function with the threshold . Since these small flows can get enough tokens in one round-trip, their data packets will directly enter the network and complete transfer before reacting to the token control loop. Therefore, the amount of data packets in the bottleneck queue that comes from these flows is .

For the large flows, they will send tokens assigned by receivers until the bottleneck queue length reaches the ECN marking threshold . At this point, we denote the token window for each large flow as 
. We have (20)
(21)
 Since 
 is the maximal token window, and is proportional to the weight in steady state, we have (22)
 

Since  is increasing with the threshold , it follows that a larger threshold will harm the bandwidth of each large flow in the queue with low scheduling frequency. We may increase  to compensate the damage for weighted bandwidth sharing among large flows. However, since the maximal queuing delay is approximated as , increasing  will harm the delay experienced by the small flows.

5. Evaluation
In this section, we present the evaluation of HOPASS by analyzing the simulation results on ns-3 (Henderson et al., 2008) which is used to evaluate pFabric (Alizadeh et al., 2013). We evaluate our approach under different network topologies and different kinds of utility functions. We have compared HOPASS with existing approaches to prove its effectiveness. The evaluation is divided into three parts:

•
The overall performance of HOPASS with both fast and slow control layer: HOPASS is implemented HOPASS in ns-3 and compared with DCTCP (Alizadeh et al., 2010) and TCP NewReno on the performance metrics such as average bandwidth utilization and overall network utility.

•
Optimality of the slow control: As the slow control layer of HOPASS aims to solve the multi-objective NUM problem in aggregate flow level, we evaluate the feasibility of the slow control layer through a scenario where flows with three different objectives coexist. We use the CVX toolbox of MATLAB (Higham and Higham, 2016) to calculate the optimal solution of the multi-objective NUM problem and compare it with the result of HOPASS to verify the solution’s accuracy.

•
Performance of the fast control: We conducted extensive simulations and evaluated the performance of the fast control layer over a wide range of topologies, workloads, traffic models, and performance metrics. We compare the performance with pHost (Gao et al., 2015) and pFabric (Alizadeh et al., 2013).

5.1. Overall performance of HOPASS
To evaluate the overall performance of HOPASS, we performed a simulation in ns-3 with large-scale topology and complex workload in multiple objective coexistence scenarios.

Topology: Two-tier multi-rooted tree. It is a typical topology in data center networks, for the simulations in ns-3. In the two-tier multi-rooted tree topology, each core switch is connected with all the aggregation switches. The links among them are core links. The links that connect end hosts with aggregation switches are edge links.

We used a two-tier multi-rooted tree with four core switches and eight aggregation switches, and each aggregation switch is connected to sixteen end hosts. The capacity of all the edge links is set to 10 Gbps, and the capacity of core links is 40 Gbps. The propagation delay between any two end hosts is μ. The workload we used is the same as that in NUMFabric (Nagaraj et al., 2016).

Workload: Workload is based on the measurements of applications from large enterprises and web searches, whose services’ obvious feature is heavy-tailed, which means that most of the flows are small-sized flows and they contribute most of the total workload. The interarrival time of flows obeys the exponential distribution.

Performance metrics: In this part of the evaluation, we continually build a three-objective coexistence scenario. As Table 2 indicated, the utility functions are 
, 
 
 and 
 
, respectively with  and 
.

Setup: To verify the performance of HOPASS, we also compare HOPASS with DCTCP (Alizadeh et al., 2010), a widely deployed congestion control algorithm, and TCP NewReno, a traditional Transport Control Protocol. These two algorithms are both implemented in ns-3 simulator and use the same topology and workload to conduct the simulation. The performance metrics used to evaluate these three approaches include: (a) average bandwidth utilization of all the links; (b) global total network utility of all the aggregated flows.

Fig. 9 shows that HOPASS is able to achieve higher global total network utility of all the aggregated flows compared with DCTCP and Newreno, which proves the effectiveness of the slow control layer. The learning-based method helps update the bandwidth allocation strategy so that the network has a higher probability of achieving a more considerable global total utility of all the aggregated flows.

5.2. Optimality of slow control layer
5.2.1. Experimental setup
To evaluate the performance of the learning-based method in the slow control layer, we conducted packet-level simulations in ns-3 and calculated the optimal bandwidth allocation using the CVX toolbox of MATLAB. In this evaluation, we mainly focus on the convergence time and optimality of the slow control layer decisions.

Topology: We chose a simple topology for the simulation. This simple topology (Fig. 10) is a small asymmetric two-tier multi-rooted tree with two core switches, two aggregation switches, and four end hosts. The capacity of all the edge links is set to 10 Gbps with μ propagation delay. The core links’ capacity is 40 Gbps and the propagation delay of core links is the same as the edge links. The buffer size of each port on the switches is 1 MB.


Download : Download high-res image (202KB)
Download : Download full-size image
Fig. 10. Simple topology.

Setup: The slow control layer focuses on the multi-objective NUM problem of aggregated flows on each link. We use three long-lived flows to simulate three aggregated flows with different utility functions. These three long flows are generated from sender  to receivers , respectively. All these three flows share the same bottleneck link . The flows among three source–destination pairs have different performance objectives: proportional fairness, -fairness, and minimum flow completion time. The utility functions are the same as Section 5.1.

5.2.2. The optimality of HOPASS
The first experiment is used to evaluate the optimality of HOPASS on the multi-objective NUM problem. We compare the bandwidth allocation of each objective and the total utility of the bottleneck link with the optimal solution calculated by MATLAB’s CVX toolbox, which is a convex optimization problem solver. In order to ensure the network has enough time to converge to a stable bandwidth allocation, we set a large flow size (500 MB in the experiment) for these three flows.

performance metrics: To verify that HOPASS is suitable for a wide range of objectives, we change the network’s preference for different objectives by changing the parameters of utility functions and conducted multiple sets of experiments. The results are shown in Table 3. By comparing the bandwidth allocation result of HOPASS with the optimal solution calculated by MATLAB, we verify the optimality of HOPASS over different objectives. We define a formula to measure the difference between the results of HOPASS and the optimal solution: (23)
 
where,  denotes the normalized error between the result of HOPASS and the optimal solution; 
 denotes the total maximum utility of the optimal bandwidth allocation; 
 denotes the total utility under the bandwidth allocation of HOPASS.


Table 3. Comparison between bandwidth allocation result of HOPASS and the solution calculated by Matlab.

Utility function	HOPASS	Ideal solution	Error (%)
Proportional fairness	-fairness	Minimize FCT	Total utility	Proportional fairness	-fairness	Minimize FCT	Total utility	
  1000, 
  0.7	1952.1 Mbps	359.7 Mbps	8353.3 Mbps	12 635.5	1912.7 Mbps	655.9 Mbps	8096.4 Mpbs	12 640.2	0.037
  1000, 
  1.1	3138.0 Mbps	6960.5 Mbps	561.8 Mbps	11 515.0	2432.5 Mbps	7252.8 Mbps	975.0 Mbps	11 555.0	0.35
  1200, 
  1.0	3665.8 Mbps	5364.4 Mbps	1626.4 Mbps	13 150.0	2853.8 Mbps	5785.6 Mbps	2017.3 Mbps	13 193.4	0.33
  800 , 
  1.0	2588.2 Mbps	6428.1 Mbps	1642.8 Mbps	10 042.4	1924.1 Mbps	6476.7 Mbps	2258.3 Mbps	10 085.2	0.4
experimental result analysis: As shown in Table 3, even though in some cases the bandwidth allocation results of HOPASS are different from that of the ideal solution, the error of utilities between HOPASS and the ideal solution is less than 0.4%. So these solutions can be regarded as near-optimal. The error is inevitable because of the limited precision of the convergence step size in the slow control layer algorithm. HOPASS is able to provide an optimal or near-optimal solution to the multi-objective network utility optimization problem.

5.2.3. Learning-based method in SCL
The second experiment evaluates the convergence time of the learning-based method used in the slow control layer. To simulate various flow patterns, we set the sizes of these three long flows to 100 MB, 500 MB and 1 GB, respectively. In the measurement of the simulation, the first sample point was measured 0.005 s after the start of the simulation. Then the rate of these three aggregated flows is measured every 0.02 s.

Fig. 11 shows that the network reaches a stable bandwidth allocation in 0.08 s after the start of the simulation. The converged bandwidth allocation is 2588.2 Mbps, 6428.1 Mbps, 1642.3 Mbps, which is a near-optimal bandwidth allocation according to Table 3. Besides, when one of the flow stops injecting traffic into the network, a new convergence can be reached within only 0.02 s. This can be attributed to the fact that the slow control layer can quickly respond to the dynamics of flows.


Download : Download high-res image (269KB)
Download : Download full-size image
Fig. 11. Throughput and the rate of aggregated flow on the bottleneck link in multiple objective coexistence scenario.

Besides, this experiment can also prove that HOPASS can achieve high utilization of bandwidth. As shown in Fig. 11, the throughput of the bottleneck link reaches its capacity in 0.005 s, which means HOPASS ensures the link converges to a high bandwidth utilization quickly. We owe the high utilization of bandwidth to both the SCL and FCL algorithms of HOPASS. In the slow control layer, as (4) shown, the sum of the bandwidth allocation ratio is always equal to 1, which means that there is no waste of bandwidth in any control cycle. Besides, the fast control layer algorithm strictly controls the aggregated rate of each objective to the bandwidth constraint given by the slow control layer, which also contributes to the high utilization of bandwidth. We further verify the performance of the fast control layer in the next part.

5.3. Experimental in fast control layer
In this section, we provide simulation results and evaluate the performance of FCL over a wide range of topologies, workloads, traffic models, and performance metrics, and compare its performance with pHost (Gao et al., 2015) and pFabric (Alizadeh et al., 2013).

Topology: Two-tier multi-rooted tree. We use the same tree topology as in pHost (Gao et al., 2015), pFabric (Alizadeh et al., 2013), and PIAS (Bai et al., 2015), shown in Fig. 12. The two-tier multi-rooted tree topology consists of three levels of components. The top layer has four core switches. Each core switch has nine aggregation switches (at the second layer), and each aggregation switch has 16 hosts (at the bottom layer). The link of the core-aggregation switch is 40 Gbps, and the link of host-aggregation is 10 Gbps. Each link has a μ propagation delay. Network switches implement cut-through routing and each switch port has a 36 kB queue buffer (same as pFabric). Dumbbell. Fig. 12 shows the dumbbell topology that consists of two switches, two senders and two receivers. Each link is 10 Gbps, μ propagation delay, with 36 kB buffer at the switch.


Download : Download high-res image (375KB)
Download : Download full-size image
Fig. 12. Topology.

Workload: the naive workload produces approximately 100 MB large flow and 30 kB small flow, mainly used to verify the weighted bandwidth sharing. Then we compare the performance of FCL against pHost and pFabric over Three traces: IMC10, Data Mining, and Web Search, same as in pHost. The IMC10 and Data Mining workloads have a larger fraction of small flows than Web Search.

Traffic model: we generate the flows under three models. Concurrent traffic has concurrent large flows, and the small flows are generated according to a Poisson distribution. Default traffic means many-to-many traffic, same as in pHost, each host can be the sender or receiver, and the flow arrival times are subject to a Poisson distribution. Incast traffic means each receiver gets flows from a specified number of data sources.

Performance metrics: slowdown. We use the slowdown to evaluate the flow completion time as in pHost and pFabric, OPT(i) denotes the optimal FCT of flow i, ACT(i) denotes the actual FCT in observation. The slowdown denotes the ratio of the ACT(i) and OPT(i). The closer that the slowdown is to 1, the better performance.


Download : Download high-res image (285KB)
Download : Download full-size image
Fig. 13. Dumbbell topology. For small flows, the slowdown under FCL is similar to pFabric, which is optimal due to the shortest remaining processing time first strategy. Compared to pHost, the 80% queuing delay under FCL is less than μ, which is 63.6% lower than pHost (μ). For large flows, FCL’s mean throughput is higher than pFabric, and is only 16.7% lower than pHost.


Download : Download high-res image (433KB)
Download : Download full-size image
Fig. 14. Four large flows with different weights on the Dumbbell topology. pFabric excessively suppresses the throughput of large flows when there are still small flows (before sample  40). pHost’s two receivers schedule one large flow at a time. When f0 and f3 finishes, they will schedule f1 and f2. FCL achieves weighted bandwidth sharing for large flows.


Download : Download high-res image (344KB)
Download : Download full-size image
Fig. 15. FatTree topology. Fast control layer’s mean small flows’ slowdown is similar to pFabric and pHost. 99% small flows’ queuing delay is less than μ, which is 75% lower than pHost (μ). For large flows, the mean throughput is consistently higher than pFabric, and is higher than pHost under the data mining trace.

5.3.1. Concurrent flows in dumbbell topology
We use the naive workload and concurrent traffic model to measure the performance of FCL under the dumbbell topology. There are four large flows (f0, f1, f2, f3), their weights are 1:1:2:4, generated from sender1 and sender2 to receiver1 and receiver2 respectively. There are 5000 small flows generated randomly, and their arrival time is subject to Poisson distribution.

Fig. 13 shows the performance of pFabric, pHost and FCL. We set the best parameters for pFabric and pHost, to evaluate the small flow’s mean slowdown, the large flow’s mean throughput and CDF of the small flow’s queuing delay. FCL shows similar FCT of small flows with pFabric (both better than pHost), and 80% of small flows’ queuing delay is under μ, 14.3% lower than pFabric (μ) and 63.6% lower than pHost (μ). FCL schedules the small flows more frequently, and it is one of the mechanisms to minimize the FCT. The second mechanism is that the receiver takes rate control when in-network congestion occurs. Specifically, the large flows would back off according to different weights to give way to the small flows. Thus, the small flow’s FCT and queuing delay are better, at the price of acceptable throughput degradation for large flows. pHost only degrades flow when the expired token exceeds a BDP, and the congestion may have occurred for a period, which is hysteretic and not adaptive. This will cause high throughput, high FCT and long queuing delays. pFabric’s priority scheduling and priority-based packet drop play a significant role in the FCT. By using rate control, it prevents spurious packet drops to approximate SRPT. However, it also leads to the lowest mean throughput for large flows.

Fig. 14 shows the large flow’s time series diagram of the three protocols. pFabric suppresses the throughput of large flows at the switch in the beginning, because the small flow gets a high priority than the large flow. When small flow finishes (after sample  40), it schedules the large flows one by one at a switch, which may make other flows starve for a long time. pHost schedules the large flow one by one at a receiver, and switch fairly allocates the bandwidth for the incoming flows, which cannot guarantee the bandwidth for different applications. FCL approaches the weighted bandwidth allocation by the weighted back-off mechanism, and will not make one flow or receiver wait for a long time. In addition, it makes the best use of the link capacity, when f3 and f2 are finished, the remaining flows weighted share the rest bandwidth. It does not absolutely obey the weight ratio, because there is small flows’ interference. Also, the rate increase phase does not take the weighted step.

5.3.2. Default traffic in two-tier multi-rooted tree topology
To show our scheme’s scalability, we use the topology as in Fig. 12(a), default traffic and Three traces to evaluate three protocols’ performance. Fig. 15 indicates that FCL achieves a similar mean FCT for small flow with pFabric and pHost, and improves the large flow’s throughput. Besides, 99% small flows’ queuing delay of FCL is less than μ, 75% lower than pHost (μ), while pFabric’s queuing delay is close to 0. pFabric achieves global optimization for small flows’ FCT, but it needs the switch’s help to schedule and drop the data. Moreover, it ignores the objective for large flow’s throughput. pHost just handles the congestion by stopping sending tokens for a time, hence its ”on-off”behavior causes the unstable throughput. FCL achieves both low latency for the small flows and high throughput for the large flows, because it employs the WRR scheduling and weighted back-off at the receivers. WRR scheduling makes the small flows be scheduled more frequently, and the weighted back-off can guarantee weighted bandwidth sharing among the large flows.

6. Conclusion
This paper innovatively proposes a two-layer network resource allocation framework named HOPASS for DCNs to solve a multi-objective NUM problem and ensure a low-latency network with reasonable congestion control. HOPASS consists of a slow control layer (SCL) and fast control layer (FCL), and these two layers are designed to solve different problems: at the SCL, we propose a learning-based approach to solve the multi-objective NUM at an aggregate flow level. The FCL adopts a receiver-driven approach that gives a performance guarantee for both latency-sensitive mice flows and bandwidth-hungry elephant flows and takes proactive congestion control in a distributed manner to meet the requirements of weighted bandwidth sharing.

The simulation results in ns-3 show that HOPASS can achieve the near-optimal solution to the multi-objective NUM problem in multi-objective scenarios. In the simple topology, the bottleneck link bandwidth utilization of the proposed method is 99.56%, the convergence time of the algorithm is less than 0.4 s, the error between the control result and the theoretical optimal control result solved by Matlab is less than 0.2%. In addition, with limited additional overhead, the proposed flow control scheme achieves less vibration and 2.03 times the overall network utility of the typical data center flow control method DCTCP, which fully verifies the superiority of HOPASS. Finally, by leveraging the learning-based method, HOPASS is flexible and applicable to a wide range of network utilities.