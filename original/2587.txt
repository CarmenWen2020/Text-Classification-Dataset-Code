Question retrieval, which aims to find similar versions of a given question, is playing a pivotal role in various
question answering (QA) systems. This task is quite challenging, mainly in regard to five aspects: synonymy,
polysemy, word order, question length, and data sparsity. In this article, we propose a unified framework
to simultaneously handle these five problems. We use the word combined with corresponding concept information to handle the synonymy problem and the polysemous problem. Concept embedding and word
embedding are learned at the same time from both the context-dependent and context-independent views.
To handle the word-order problem, we propose a high-level feature-embedded convolutional semantic model
to learn question embedding by inputting concept embedding and word embedding. Due to the fact that the
lengths of some questions are long, we propose a value-based convolutional attentional method to enhance
the proposed high-level feature-embedded convolutional semantic model in learning the key parts of the
question and the answer. The proposed high-level feature-embedded convolutional semantic model nicely
represents the hierarchical structures of word information and concept information in sentences with their
layer-by-layer convolution and pooling. Finally, to resolve data sparsity, we propose using the multi-view
learning method to train the attention-based convolutional semantic model on question–answer pairs. To
the best of our knowledge, we are the first to propose simultaneously handling the above five problems in
question retrieval using one framework. Experiments on three real question-answering datasets show that
the proposed framework significantly outperforms the state-of-the-art solutions.
CCS Concepts: • Information systems → Information retrieval; Question answering; • Computing
methodologies → Natural language processing;
Additional Key Words and Phrases: Question retrieval, concept embedding, question embedding, value-based
convolutional attentional method
1 INTRODUCTION
Over the few past decades, large-scale question answering (QA) archives have become important resources for various intelligent services. However, users may have to wait for a long time
for answers after asking a question in manual QA platforms. Inspired by this, many automatic QA
systems have been proposed [8, 29]. Among various challenges in automatic QA systems, retrieval
of similar questions [12, 40, 41, 46, 47] is attracting much attention in the domain of answering frequently asked questions in QA platforms, e.g., Stack Overflow, Yahoo Answers, and Baidu Zhidao.
However, this task is quite challenging in regard to five aspects:
• Synonymy: Synonymy means that words having different forms may share the same
meaning. This will lead to two questions having the same meaning being predicted as two
different questions that use some key word–based retrieval methods, e.g., “how to get rid
of stuffy nose? ” and “how to prevent a cold? ” share the same meaning though they contain
different words. Many traditional question-retrieval methods, such as language model (LM)
[39] and vector space model (VSM) [23], suffer from synonymy.
• Polysemy: Polysemy means that a word may reveal different senses as the context changes,
e.g., the word “apple” may refer to an IT company or a type of fruit according to its context.
Many retrieval methods, such as traditional methods [12, 47] and a recent word distributed
representation–based method [18], suffer from this problem.
• Word order: Sometimes, two questions express completely different meanings though they
have the same words, e.g., “does a cat run faster than a rat? ” and “does a rat run faster than
a cat? ” Many retrieval methods [18, 47] ignore this information.
• Question length: There are some questions that have higher length values, e.g., “I would
like to know what is the voltage input the surface book supports. usually is from 110v to 240v.
anyone can confirm it? as an eu citizen is important.” Generally, it is hard to match the whole
question due to the length of the question. However, the above question can be rewritten
to include only what it focuses on: “what is the voltage input the surface book supports?”
• Data sparsity: One of the biggest challenges is that many problems do not come with
adequate training examples. Training data is very critical for modeling the problem.
To solve the synonymy problem, several methods have been proposed, such as translation-based
methods [12, 16, 33, 43], topic model–based methods [13, 40], and continuous word embedding–
based methods [18]. In comparison, the word embedding–based methods have been proven to be
more efficient than other methods. However, they still suffer from the polysemous problem and the
word-order problem. Recently, some methods have been proposed to handle the polysemous problem, such as the creation of sense-specific embedding [20] and concept embedding [5]. Some other
methods [10, 15, 24] have been proposed to obtain the sentence embedding representation directly
so that the syntactic information can be kept. To the best of our knowledge, there is no method
that can resolve the above five problems in one framework, especially the data sparsity problem.
In our previous work [27], we propose a concept-embedded convolutional semantic model to
handle the first three problems: synonymy, polysemy, and word order. The concept information
can help confirm the exact meaning of a word possessing multi-senses. Thus, to handle the polysemy and the synonymy problems, we propose a concept-powered model that encodes concept
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:3
embedding and word embedding from both context-dependent and context-independent views
(see Section 3.2). The context-dependent view focuses on leveraging the concept information within the context and the context-independent view focuses on leveraging the concept
information within the knowledge base. Then, we propose several models, the high-level featureembedded convolutional semantic model (HCSM) and its variants, to handle the word-order
problem in our previous work. These models learn question embedding from concept embedding
and word embedding instead of using the raw text feature [10, 24]. The proposed models consider
the word order and n-gram information of the word sequence.
In this article, we extend our previous work by taking into account the other two problems: the
question-length problem and data-sparsity problem. When humans answer a long question, they
always focus only on key parts of the question and ignore some unimportant words. In addition,
the answer always has a relation with only these key parts of the question. In the question-retrieval
task, it is usually hard to match a similar question for a given very long question. Moreover, the
mutual information between the question and the answer plays a key role in evaluating the key
parts of the question and answer. Thus, to handle the question-length problem, we propose a
value-based convolutional attention method operating on mutual information to enhance HCSM
models. Our enhanced model can efficiently learn the key parts of questions and answers by leveraging mutual information. The generated question vector representation can handle the first four
problems at the same time.
Many learning-based models depend to a great extent on manually labeled training data. To
solve the data-sparsity problem, we propose using a multi-view learning framework for adding
training data from multi-views in a semisupervised way. Because the convolutional semantic
model takes the embedding of question-and-answer pairs as input, our goal is to generate more
question–answer pairs. Note that similar question–question pairs can add additional question–
answer pairs since similar questions can potentially share their corresponding answers. Further,
the similar evaluation of two questions can be estimated from two perspectives: one is the word
content in the question, such as the fact that two questions may be similar if they have similar semantics and syntaxes; the other is the relation between the question and answer, such as the fact
that two questions are likely to be similar if they have similar answers. Inspired by this, we propose
using the multi-view learning method, which is designed to add training data from multi-views,
to train our convolutional semantic model (see Section 4).
To summarize, the contributions of this article are as follows:
• We are the first who propose to simultaneously handle the five challenges in question retrieval using one framework.
• We encode concept embedding and word embedding from both context-dependent and
context-independent views.
• We directly learn the question vector representation using a HCSM, which takes the contextual word embedding as input instead of raw text features.
• We propose a value-based convolutional attention method to enhance the HCSM.
• We propose a multi-view learning-based method to train the convolutional semantic model
to solve the data-sparsity problem.
2 OUR UNIFIED FRAMEWORK
As shown in Figure 1, the proposed model consists of three steps:
(1) Word and concept learning: This step is to learn the semantic relations between words
for resolving the synonymy problem and to learn concept embedding for resolving the
polysemous problem. The output of this step consists of word embeddings of words and
concept embeddings of concepts (see Section 3).
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
41:4 P. Wang et al.
Fig. 1. The flowchart of the unified framework.
(2) Question and answer embedding learning: This step is to learn sentence embedding
representation to obtain the syntactic information and key parts from a question and answer (see Section 4).
(3) Multi-view learning: This step is to resolve the data-sparsity problem to better train the
model obtained from step (2) (see Section 5).
In Section 3.1, we first briefly introduce the Skip-gram model, the foundation of the proposed concept-powered model. For completeness, we then introduce the concept-powered model
(Section 3.2) and the HCSM (Section 4.1) proposed in our previous work [27]. Finally, we introduce our new methods in this work, which are used to handle the question-length problem and
the data-sparsity problem.
3 WORD AND CONCEPT LEARNING
3.1 Neural Word Embedding Model
Generally, word embedding is learned from a given text corpus without supervision by predicting
the context of each word or predicting the current word given its context [1, 6, 18]. In this work,
we consider the Skip-gram model [18] for learning distributed representations of words, known as
word embedding, because it is much more efficient than other word-embedding models. Ultimately,
words are represented as vectors and their meanings are distributed across the dimensions of a
semantic space [5]. The training goal of the Skip-gram model is to find word representations that
are useful for predicting the surrounding words in a sentence or a document. Mathematically, given
a sequence of training words w1, w2,...,wT , the objective of the Skip-gram model is to maximize
the average log probability:
J (θ ) = 1
T

T
t=1

−l ≤j ≤l,j0
loдp(wt+j |wt ), (1)
wherewt andwt+j represent the target word and contextual word, respectively, and l is the size of
context window centered at the target word wt . θ is the embedding of the words that we should
learn in this sequence. The conditional probability p(wt+j |wt ) is computed as
p(wt+j |wt ) = exp(wT
t+jwt )
N
k=1 exp(wT
kwt )
, (2)
where wt+j , wt , wk denote the d-dimensional word-embedding representations of the wt+j , wt ,
wk . N is the number of words in the vocabulary.
3.2 Concept Powered Model (CPM)
In this section, we describe how we encode concept information into the embedding model. Previous work [5] shows that conceptual information of words can efficiently handle the polysemous
nature of human language. In the work of Cheng et al. [5], each word is associated with an intrinsic
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.    
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:5
Fig. 2. Two different concept models.
vector that maintains the unique features of the word. Each concept is assigned a vector that delivers an unambiguous meaning. The contextual representation of a word is obtained by combining
its intrinsic vector and the most context-appropriate concept vector. For example, considering two
queries [python zoo] and [python string], the embedding of the first “python” is obtained by combining the intrinsic vector of “python” and the concept vector of “animal category.” In contrast, the
embedding of the second “python” is obtained by combining the intrinsic vector of “python” and
the concept vector of “programming language.” Before describing our proposed concept-powered
model, we briefly introduce the GWCS-1 model, which achieves the best results in the work of
Cheng et al. [5].
As shown in Figure 2(a), the task of choosing a word to fit in the context can be reduced to two
steps in the GWCS-1 model: locating the right concept and then searching for a word underneath
the chosen concept. The resulting objective function is
J (θin, θcn ) = 1
T

T
t=1

−l ≤j ≤l,j0
loдp(wt+j |wt ) (3)
p(wt+j |wt ) = p(et+j |wt )p(wt+j |wt, et+j), (4)
where et+j denotes the concept of wt+j in the given context, the θin denotes the word-intrinsic
vectors to be learned, and θcn denotes the concept vectors to be learned.
However, the GWCS-1 model considers just the context-dependent relation in the context. The
context-independent relation from the knowledge base is ignored. We believe that the contextindependent relation can enhance the embedding learning via two methods:
• Certainty: As shown in Figure 2(a), GWCS-1 must select the right concept ec1 ofwc1 before
calculating the probability p(wc1 |wt ). It will introduce an error if a wrong concept name of
wc1 is selected, for example, the concept of “company” is selected as “apple” in the sentence
“the company provides apple every afternoon” using GWCS-1. From the context-independent
view—namely, without context—the embedding of the w1, w2, ec will be enhanced if the w1
and w2 really share the concept ec in a knowledge base.
• Directness: Due to data sparsity, two words that share the same concept may not co-occur
in a given context. In this case, the semantic relationship between the two words will be
caught by an indirect method, such as by another word, which has the relation with the two
words respectively. Even worse, the semantic relation between the two words will not be
learned if there is no word that co-occurs with both of them. For example, the relation between “sofa” and “bed ” will be missed if they do not co-occur in a context window. However,
if two words share the same concept in an external knowledge base, they will be enhanced
directly from the context-independent view.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.   
41:6 P. Wang et al.
Inspired by this, a regularization function derived from the concept information is added to the
Skip-gram objective function in a context-independent way. The regularization term focuses on
leveraging the concept information regarding the knowledge base without considering context.
As shown in Figure 2(b), if two words share the same concept in a knowledge base, we will model
the concept in addition to the context to achieve certainty and directness. Let sm (wt,wi ) be the
similarity score between wt and wi toward the mth concept em in the whole concept space, and
the following heuristic is used to constrain the similar score:
sm (wt,wi ) =

1 ifem ∈ (C(wt ) ∩C(wi ))
0 otherwise , (5)
where C(wt ) and C(wi ) denote the concept sets of word wt and wi . In other words, if the central
word wt and word wi share a same concept em, their similarity score of the mth concept will be
1; otherwise, the score will be 0. Then, we encode the concept information using a regularization
function Rc :
Rc =

N
t=1

N
i=1


1
M

M
m=1
sm (wt,wi )d(wm
t ,wm
i )

, (6)
where M denotes the number of whole concepts in the knowledge base and M
denotes the number
of concepts that wt and wi share. In practice, we can just loop through the intersection concept
set of wt and wi instead of the whole set. wm
t and wm
i are the word wt and wi combining with the
mth shared concept name. d(wm
t ,wm
i ) is the distance between the words in the contextual word
embedding space and sm (wt,wi ) serves as a weight function. For similarity, we denote d(wm
t ,wm
i )
as the cosine similarity between wm
t and wm
i . We then use the following equation to combine the
word vector and the concept vector:
wm
t = (1 − λ)wt + λcm, (7)
where wm
t denotes the combination embedding representation of wm
t , wt denotes the embedding
representation of wt , cm denotes the embedding representation of the mth shared concept cm,
and λ controls the relative importance between the two types of embeddings, which is set to 0.45
according to the validation set. In the case of selecting the right concept of a word to update, we
use the same method as GWCS-1, the conceptualization method.
Then, we obtain the objective function Jc that encodes concept information from the contextindependent view:
Jc = J (θin, θcn ) + βRc , (8)
where β is the combination coefficient, which is set to 0.05 according to the validation set. Our goal
is to maximize the combined objective function Jc . This can be optimized by using back propagation neural networks. According to the validation dataset, the dimension of the concept embedding
and word embedding is set to 300, the windows size l in Skip-gram part is set to 5, the learning
rate is set to 0.02, and β is set to 0.05 in this work. We call this model the concept-powered model
and denote it as CPM for easy reference.
4 SENTENCE REPRESENTATION LEARNING
4.1 High-Level Feature Embedded Convolutional Semantic Model (HCSM)
Distributed representations have been proven successful at word level. However, the embedding
representations of large text—e.g., sentence or document—are more evident in natural-language
processing applications. This is because the sentence or paragraph embedding can keep more
syntactic information compared with words. After obtaining the embedding of the question, we
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.     
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:7
can evaluate the cosine similarity of two questions using the following equation:
sim(q1,q2) = q1 · q2
 q1  q2 
, (9)
where q1, q2 denote two questions, q1, q2 denote the embedding representation of q1, q2, “·” denotes
the dot product, and “” denotes the module of a vector. If two questions are similar, the cosine
similarity will be close to 1. Otherwise, the cosine similarity will be close to -1. Thus, we use
Equation (9) to rank candidate questions in the question-retrieval task.
To encode the concept information and syntactic information, especially the word-order information, into the sentence1 embedding, we propose several models, the high-level feature
embedded convolutional semantic model (HCSM) and its variants, to generate the embedding representations of questions and answers. Given a QA dataset P = {(qi, ai )|1 ≤ i ≤ N}, where
(qi, ai ) is the i
th question–answer pair in the dataset, qi denotes the question and ai denotes the
answer of this pair, and N is the number of question–answer pairs in this dataset. The question
qi of the i
th pair is composed of a sequence of words qi = {wq
ij, 1 ≤ j ≤ Nq
i }, where Nq
i denotes
the length of qi . Similarly, the answer ai of the i
th pair is composed of a sequence of words
ai = {wa
ij, 1 ≤ j ≤ Na
i }, where Na
i denotes the length of ai .
Once word embedding and concept embedding are learned, the qi (or ai ) can be represented by
a word-embedding matrix and concept-embedding matrix, respectively. With regard to the wordembedding matrix, the qi and ai are represented as Ew
qi = {wq
ij , 1 ≤ j ≤ Nq
i } and Ew
ai = {wa
ij , 1 ≤
j ≤ Na
i }, where the bold letters wq
ij ∈ Rd and wa
ij ∈ Rd are the d-dimensional word-embedding
representations ofwq
ij andwa
ij . With regard to the concept-embedding matrix, the qi and ai are represented as Ec
qi = {c
q
ij, 1 ≤ j ≤ Nq
i } and Ec
ai = {ca
ij, 1 ≤ j ≤ Na
i }, where the bold letters c
q
ij ∈ Rd and
ca
ij ∈ Rd are the d-dimensional concept-embedding representations of thewq
ij concept andwa
ij concept. Thus, the goal is to obtain the vector representation qi and ai of the questionqi and answer ai .
Many previous works [10, 14, 18, 24, 25] have explored the compositional operations of words.
Among these methods, the simplest one is to obtain the vector representation qi of question qi by
averaging the sum of all the word representations wq
ij :
qi = 1
Nq
i
N q
i
j=1
wq
ij . (10)
We observe that two questions, if having similar answers, could have semantic relations. Thus,
we use such relations as the supervised information to train our model. We model the sentence
matrix using a convolutional architecture that alternates wide convolutional layers and pooling
layers. Next, we briefly introduce the idea of wide convolution and pooling.
Wide convolution: A convolutional layer in the network is obtained by convolving a convolution kernel m ∈ Rd×m with the source input matrix. d is the dimension of the input word vector
andm is the hyper-parameter of the network. We use the wide one-dimensional convolution in this
step. Wide convolution means adding zero-padding to the source input matrix. One-dimensional
convolution means that each row mr in m, where 1 ≤ r ≤ d, is operated only on the corresponding
rth dimensional (row) vector of Ew
qi or Ec
qi
.
For example, the second layer is obtained by applying a convolutional kernel m to the input
word-embedding question matrix Ew
qi ∈ Rd×N q
i and has dimensions d × (Nq
i + m − 1). Thus, after
the wide one-dimensional convolutions, the number of rows of the resulting matrix is still d. In
1The sentence denotes a question or answer in this article.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018. 
41:8 P. Wang et al.
Fig. 3. The structure of the HCSM-1 used in learning question vector.
addition, a bias b ∈ Rd and a nonlinear function д are applied component wise to the convolved
matrix after convolving. We use RELU as д in our experiments. In our experiments, we use the
same network parameters in both question and answer parts.
Pooling: The average-pooling operator is applied in the network after the topmost convolutional layer. This guarantees that the generated question or answer vector can keep more important information. In addition, we use the dynamic k-max pooling [14] after each intermediate
convolutional layer. We use the following function to calculate the k:
kl =

L − l
L s

, (11)
where l is the number of the current convolutional layer to which the pooling is applied, L is the
total number of convolutional layers, and s is the length of input sentence. For example, for an
input sentence s =8 and L=2, the pooling parameter k1 at the first layer is 4.
HCSM-1: In the first variant, we emphasize three types of relation: the relation between concepts, the relation between words, and the relation between concept and word. As shown in
Figure 3, two different convolution kernels mc
1 and mw
1 , which encode the concepts relation and
the words relation, respectively, are used to convolve the input word-embedding question matrix
Ew
qi and concept-embedding question matrix Ec
qi
. The Cq and Ca denote the concept lists of the
word in question q and answer a. The first convolution layer is obtained by using the following
equation to combine the convolutional results from Ew
qi and Ec
qi
:
Y = (1 − λ)Yw + λYc (12)
Yw = Ew
qi ∗ mw
1 Yc = Ec
qi ∗ mc
1, (13)
whereY is the first convolution layer, “*” is the wide convolution symbol, and λ controls the relative
importance between the two types of results. The value of λ can be decided by validation set. As
shown in Figure 3, after the first special convolution layer, the convolutional layers and pooling
layers alternately appear in the architecture. This variant of the High-Level Feature Embedded
Convolutional Semantic Model is called HCSM-1.
HCSM-2: In the second variant of HCSM (HCSM-2), we put more emphasis on the relation
between concepts and the relation between words. We consider the relation between concept and
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:9
Fig. 4. The structure of the HCSM-2 used in the learning question vector.
word in the end of the architecture. As shown in Figure 4, we build two distinct networks for
the word-embedding question matrix and concept-embedding question matrix. In other words,
the convolution kernels of the two networks are different. We name the two networks as word
network and concept network. On the top of the last pooling layer (average-pooling layer), we
add a functional layer. The meaning of each layer in Figure 4 is same as the meanings of layers
in Figure 3 except for the functional layer. The functional layer uses the following equation to
combine the two pooling results from the two networks:
Y = (1 − λ)Aw + λAc , (14)
where Y is the output of the functional layer and Aw and Ac are the average-pooling layer results
of the word network and concept network. λ controls the relative importance between the two
types of results. The value of λ can be decided by the validation set.
HCSM-3: We can also directly combine the word-embedding question matrix and conceptembedding question matrix before inputting them to the network using the following equation:
Eqi = (1 − λ)Ew
qi + λEa
qi
, (15)
where Eqi is the combination result. Then, the convolutional network takes Eqi as input. We name
this model variant HCSM-3.
We just show the three model variants by inputting the question matrix. The mechanism is the
same for inputting the answer matrix. Finally, the output of the three model variants is a question
or answer vector in a low-dimensional semantic feature space.
4.2 Value-Based Convolutional Attention Method
The mutual influence between the question and answer is ignored in the above models. When humans answer a question, they always focus only on key parts of the question and ignore unimportant words in terms of the answer, especially in long questions. Thus, mutual information between
the question and answer should be used to decide which parts of the question and answer are more
important. Inspired by this, we propose a value-based convolutional attention method to enhance
our HCSM models.
As shown in Figure 5, the inputs of the proposed method are the representation feature map of
the question and answer. The outputs are the attention feature map of the question (red matrix)
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
41:10 P. Wang et al.
Fig. 5. The structure of the attention model.
Fig. 6. Two different convolutional methods.
and answer (yellow matrix). We first obtain the mutual matrix A, namely, the mutual information,
from the representation feature map of the question and answer. Then, we propose a value-based
convolutional attention method to obtain the attention feature map of question and answer from
the mutual matrix A. Finally, we take the representation feature map and attention feature map of
the question (or answer) as the input to generate the convolution layer, as shown in Figure 5.
Mutual Information Generation: After obtaining the word representation map Ew
q =
{wq
i , 1 ≤ i ≤ Nq } of a question q and the word representation map Ew
a = {wa
i , 1 ≤ i ≤ Na } of the
corresponding answer a, we compute the mutual matrix A ∈ RN q×N a
as follows:
Ai,j = f (Ew
q [:,i], Ew
a [:, j]), (16)
where f (, ) denotes a matching function between the representations of the i
th item of q and the j
th
item of a. Cosine similarity is used in our work. The row i in A denotes the attention distribution
of the i
th word of the question with respect to each word in the answer. The column j in A denotes
the attention distribution of the j
th word of the answer with respect to each word in the question.
Attention Feature Map Generation: In this subsection, we try to transform A into two new
corresponding attention maps: the question attention map Gq (the red matrix) and the answer
attention map Ga (the yellow matrix), as shown in Figure 5. Yin et al. [37] use just two weight
matrices to simply transform the A into two attention maps. Thus, the n-gram information existing
is missing. In this work, we propose using the value-based convolutional model to generate the
two attention maps from A.
As shown in Figure 6(a), we first introduce the general convolution operation on A, namely, the
position-based shared filter. W ∈ Rk×N q×d denotes the filter. k is the windows size and d denotes
the number of the filter map. We define Aj = [A1j ,...,AN q j] as the j
th column vector of A and
ft
i = [W t
1i,...,W t
N qi
] as the i
th column vector of the tth filter map ft ∈ Rk×N q
. For simplicity, we
set k as 3 for the following illustration. In the general convolution operation, the convolution result
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:11
дjt is calculated as follows:
ft
i · Aj =
N
q
a=1
AajW t
ai (17)
дjt = ft
1 · Aj−1 + ft
2 · Aj + ft
3 · Aj+1, (18)
where “·” denotes the inner product. Thus, there are 3xNq parameters in one filter map.
However, the key difference between the computer vision and natural language processing is that
the relative position information for pixels is very important in computer vision tasks. However,
there is no necessary position relation between the question words and answer words due to the
flexibility of the sentence [7, 34], such as the first word of the answer may have a relation with the
last word of the question. Note that the mutual matrix values between the question and answer
in different ranges play different roles in catching the semantic relation between the question and
answer.
Inspired by this, we propose using the value-based shared filter method to generate the attention
map. We split the value in mutual matrix A into different score levels and the values of A in the
same score level share the same weight. As shown in the example of Figure 6(b), since the Aij is
within the interval [−1,1], we split the interval into a set of ordered bins. The bin size in the figure
is fixed as 0.25. We will obtain 9 bins [−1,−0.75),[−0.75,−0.5), . . . ,[0.75,1),[1,1] and each bin has a
corresponding weight. Thus, we will have the filters W ∈ Rk×9×d . Similarly, ft
i = [W t
1i,...,W t
9i] is
the i
th column vector of the tth filter map. The number of parameters in one filter map is 3 × 9.
As shown in Figure 5 and Figure 6(b), the items in mutual matrix A, which have the same color,
belong to a same value interval (bin) and share the same filter weight.
Mathematically, the value-based convolutional operation can be represented as follows:
ft
i ⊗ Aj =
N
q
a=1
Aaj m
b=1
W t
biI(Aaj ∈ Inter(W t
bi )), I(x) =

1 if x is True
0 if x is False (19)
дjt = ft
1 ⊗ Aj−1 + ft
2 ⊗ Aj + ft
3 ⊗ Aj+1, (20)
where m denotes the number of the bins (m is 9 in the above example) and the Inter(W t
bi ) denotes
the corresponding value interval of the W t
bi .
After obtaining the attention map Gq and Ga, we stack the representation feature map Eq (Ea)
and the attention feature map Gq (Ga) as an order 3-way tensor and induct the convolution operation on it. The same value-based convolution attention operation is constructed on the concept
feature map between the question and answer. Currently, we add just the attention layer before
the first convolutional layer in HCSM.
4.3 Training
The question–answer pairs act as the labeled data in our model. Thus, to model parameters, we
propose a training method that maximizes the conditional likelihood of the correct answer given
the question. First, we compute the posterior probability of an answer given a question from the
semantic relevance score between them through a softmax function:
P (a|q) = exp(γ R(q, a))

a
∈M exp(γ R(q, a
)) , R(q, a) = q · a
 q  a 
, (21)
where γ is a smoothing factor in the softmax function, which is set empirically on the validation
set. R(q, a) denotes the semantic relevance score between the question q and the answer a. q and
a is the learned vector representation of the q and the a. M denotes the set of candidate answers
to be ranked. Ideally, M should contain all of the answers in the question–answer pairs P. In
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.    
41:12 P. Wang et al.
practice, for each question–answer pair (q, a+), where q denotes the question and a+ denotes the
corresponding answer, we construct M by including a+ and four randomly selected other answers
from P. In training, we minimize the following loss function to estimate the model parameters:
L(θ ) = −loд 
(q,a+)
P (a+|q), (22)
where θ denotes the model parameters; the model is trained by a gradient-based numerical optimization algorithm.
During the experiment, we find that a model with only two convolutional layers and pooling
layers performs better than previous methods. The widths of the two matrix filters at two convolutional layers are 3 and 2, respectively. The length d of the two matrix filters are the same with
the given word-embedding dimension. The dynamic k-max pooling and average pooling are used
in the first pooling layer and second pooling layer, respectively.
5 MULTI-VIEW LEARNING
Convolutional neural networks need sufficient training data to obtain good performance. In our
task, sufficient question–answer pairs are needed to train the HCSM model. Thus, we propose
leveraging the multi-view learning method to solve the data sparsity problem. In the multi-view
learning method, the data can be represented in many views, and each view can train a model. In
each iteration, some new data with a high degree of confidence can be predicted by each model.
This new data can be used to train other models.
In this work, our goal is to retrieve similar questions given a question. Thus, one view of judging if two questions are similar is whether they are explicitly learned from the labeled similar question–question pairs, and the other view is whether they are implicitly learned from the
question–answer pairs, assuming that the questions with similar answers are potentially similar.
When we generate similar question–question pairs from question–answer pairs via the second
view, we only consider the question in which the corresponding answer is more than five words.
In addition, we omit some answers that contain only meaningless words, such as some stop words.
This would avoid a case in which a very simple answer whose questions are different, such as
“Where is this computer made?” and “Which country does support the warranty policy?” have
the same answer “USA” but are totally different questions. In our framework, we train two distinct HCSMs. One HCSM, called the question–question–based model (QQM), is fed labeled similar
question–question pairs. The other HCSM, called the question–answer–based model (QAM), is fed
question–answer pairs. Then, each model uses the other model’s new labeled data and its original
training data as its new training data.
Algorithm 1 summarizes the proposed algorithm. It takes the labeled question–question pairs
Dqq and question–answer pairs Dqa as input, and outputs the QQM parameters Mqq and QAM
parameters Mqa, which can generate the question vector. In each iteration, we first use Dqq and
Dqa to train the QQM mqq and QAM mqa. Second, the model mqq is used to predict the similar
question–question pairs Pqq
1 ; the model mqa is used to predict the similar question–question pairs
Pqq
2 with high degree of confidence (see Section 5.1). Third, we use the new predicted data to
enlarge the original training data. The Pqq
2 predicted by model mqa can directly merge into Dqq.
However, the model mqa takes the question–answer pairs as input. Thus, we take the method
described in Section 5.2 to obtain the question–answer pairs Pqa
3 from Pqq
1 . Then, the Pqa
3 can
merge into Dqa. Finally, the ultimate model parameters Mqa and Mqq can be obtained when the
algorithm converges.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:13
Fig. 7. The method used to generate question–answer pairs using similar question pairs.
ALGORITHM 1: Multi-View Learning Framework
Input: Question–question pairs Dqq, question–answer pairs Dqa
Output: Mqa, Mqq
1: loop
2: mqq ← HCSM(Dqq), mqa ← HCSM(Dqa);
3: Pqq
1 ← Predict(mqq), Pqq
2 ← Predict(mqa);
4: Pqa
3 ← GenerateQA(Pqq
1 );
5: Dqq ← Pqq
2 ∪ Dqq, Dqa ← Pqa
3 ∪ Dqa;
6: Mqa ← mqa, Mqq ← mqq;
7: end loop
5.1 PredictQQ
After getting the model parametersmqq andmqa, a question vector representation can be obtained
given a question. Thus, we can calculate the cosine similarity of each possible question–question
pair in which the question comes from whole data P. However, the cost of this calculation is too
high. In our work, we first use the k-means to cluster the question set based on the vector representation of questions. Then, we need to calculate only the cosine similarities of the possible
question–question pairs in each cluster. In addition, different clusters can be processed in a parallel
manner. Finally, only the question–question pairs with cosine similarity greater than the threshold
τ are selected as the predicted data. τ is selected as 0.7 in three datasets according to the validation
datasets.
5.2 GenerateQA
Figure 7 depicts the method to generate new question–answer pairs from the question–question
pairs. In the figure, pairs linked by solid lines represent the question–answer pair in the original
dataset. Questions may share answers if they are similar. Thus, many new question–answer pairs
(represented by dotted-line links) may be generated.
6 EXPERIMENTS
6.1 Data Collection and Evaluation Metrics
Amazon Product Customer Questions & Answers: We collect this dataset from the Amazon Product
Customer Questions & Answers section with 20,120 question–answer pairs (“Clothing” and “Electronics” category). The dataset is not sufficient for a supervised learning method, making it a good
candidate for trying the proposed model. We label 2500 similar question pairs. We follow the same
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
41:14 P. Wang et al.
method used in previous works [44–46] to generate the evaluation data. The evaluation dataset
contains 350 questions sampled from the whole dataset, and the validation dataset contains 200
questions. Each original question is associated with 15 other target questions, which are retrieved
by the VSM [23] from the questions in the whole dataset. Each question pair has a relevance label.
This label is generated by humans and is on a 3-level relevance scale, 1 to 3,; level 3 means that the
retrieved question is similar to the source question (good) and level 1 means they are dissimilar
(bad). In the experiments, the 15 target questions will be ranked according to the similarity to the
original question.
Microsoft Community Questions & Answers: We collect this dataset from the Microsoft Community. It consists of 31,000 question–answer pairs (“Windows” and “Office” category). We label 3,000
similar question pairs. The evaluation dataset contains 400 questions sampled from the whole
dataset, and the validation dataset contains 300 questions. On average, each question is still associated with 15 other questions using the same method as for the Amazon data. The labeling and
usage of the evaluation data are the same as for the Amazon dataset. The preprocessing is also the
same as for the Amazon dataset.
Yahoo! Answers: This dataset is released in Yahoo! Research Language data. We select top 3 categories from the L6 Comprehensive Questions and Answers dataset: “Family & Relationships,”
“Entertainment & Music,” and “Society & Culture.” It consists of 1,150,223 question–answer pairs.
We label 7,000 similar question pairs. The evaluation dataset contains 1,000 questions sampled from
the whole dataset and the validation dataset contains 1,000 questions. On average, each question
is still associated with 15 other questions using the same method as for the Amazon data. The labeling and usage of the evaluation data are the same as for the Amazon dataset. The preprocessing
is also the same as for the Amazon dataset.
Evaluation Metrics: The performance of models is measured by mean normalized discounted
cumulative gain (nDCG) [11] and Precision@n. The nDCG is calculated as
nDCGp = DCGp
IDCGp
, DCGp = rel1 +

p
i=2
reli
loд2 (i + 1)
, (23)
where reli is the graded relevance of the result at position i, reli ∈ {1, 2, 3} in our experiments and
p is the rank list length. The IDCGp is calculated as DCGp except for the fact that reli is set to
3 in our experiment. The Precision@N reports the fraction of the top-N questions retrieved that
are relevant. Question pairs, whose relevance score are 2 or 3, are considered as relevant. We also
conduct the paired t-test. Differences are considered statistically significant when the p-value is
less than 0.05.
6.2 Results
6.2.1 Concept-Powered Model. We show the concept-powered model results in Table 1. Probase
[30] is selected as the knowledge base in this step. Given a word, Probase will provide the concepts
associated with the word. More usages of Probase can be found in its official website.
In Table 1, the first two methods calculate the relevance score using the bag-of-words (BOW)
method and the LM, which is the baseline model in this work [39]. Then, we compare our CPM
with three groups of methods that also can resolve the synonymy problem. The first group is
translation models: word-based translation model [12], word-based translation language model
[33], and phrase-based translation model [43]. In these translation models, we take the training
question–answer pairs as the parallel corpus. The second group is the topic-based model [13] and
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018. 
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:15
Table 1. Comparison of Different Approaches
Amazon Data Microsoft Data Yahoo Data
Model nDCG@10 P@10 nDCG@10 P@10 nDCG@10 P@10
BOW 0.5924 0.4911 0.5614 0.4715 0.3618 0.2715
LM 0.6003 0.5043 0.5647 0.4640 0.3874 0.2854
Jeon et al. 2005 [12] 0.6312 0.5240 0.5814 0.4940 0.4044 0.3197
Xue et al. 2008 [33] 0.6502 0.5314 0.5978 0.5013 0.4396 0.3509
Zhou et al. 2011 [43] 0.6711 0.5377 0.6114 0.5078 0.4561 0.3617
Ji et al. 2012 [13] 0.6645 0.5514 0.6189 0.4998 0.4789 0.3881
Zhang et al. 2014 [40] 0.6789 0.5689 0.6245 0.5015 0.5020 0.3974
Zhang et al. 2015 [41] 0.6901 0.5877 0.6589 0.5178 0.5205 0.4217
Skip-gram 0.6989 0.5914 0.6621 0.5398 0.5459 0.4673
GWCS-1 0.7021 0.6074 0.6787 0.5548 0.5796 0.4863
CPM 0.7187 0.6214 0.6914 0.5745 0.6016 0.5114
supervised question–answer topic model [40]. The third group is the paraphrasing-based model
[41].
From the results we observe that the following. (1) The translation-based approaches, topicbased approaches and paraphrasing-based approaches can outperform the baseline methods.
(2) The question retrieval methods based on continuous word-embedding representations (Skipgram, GWCS-1, CPM) can outperform the translation-based models and topic-based models. The
paraphrasing-based model can achieve comparable performance with the Skip-gram models. Here,
we use the average combination method to obtain the question vector from word-embedding representations. (3) The performances of the CPM and GWCS-1 are better than the Skip-gram model.
Thus, incorporating the concept information can increase the retrieval performance. (4) By considering the concept information from the context-dependent and context-independent views, our
proposed CPM can outperform the GWCS-1 model.
6.2.2 Sentence Vector Representation Generation Model. In this part, we compare several sentence vector representation generation models with our proposed HCSM models. The results are
summarized in Table 2. We first show three methods, which directly obtain the question vector
representation from the raw text data. The DSSM [10] and CLSM [24] originally are used in web
search and obtain the query vector representation. Here, we use the question–answer pairs instead
of query–document pairs to train the two models. The Para.-vector [15] directly obtains the paragraph vector by leveraging the paragraph vector and word vectors from a paragraph to predict
the following word in a given context. The paragraph can be a paragraph or a sentence. Then,
we show the results of the methods that take the word embedding as input: average combining
method, representation-focused methods, interaction-focused methods, and our proposed HCSM
methods.
The second group contains average combining methods. The AverageW 2V denotes that the
sentence vector is combined with the word embedding of Skip-gram [18] using Equation (10).
AverageGW CS−1 and AverageCPM denote that the sentence vectors are also generated by Equation (10), where the word embedding is obtained by GWCS-1 [5] and CPM using Equation (7). The
HCSM-3W 2V denotes that the input matrixes are constructed by word embedding of Skip-gram.
The third group contains representation-focused methods: ARC-I [9], CNTN [22], BI-CNN [35],
and MultiGranCNN [36]. The representation-focused methods focus on representing text to a
dense vector without interaction between the question and answer. The fourth group contains
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
41:16 P. Wang et al.
Table 2. Comparison of Different Sentence Vector Representation Approaches
Amazon Data Microsoft Data Yahoo Data
Model nDCG@10 P@10 nDCG@10 P@10 nDCG@10 P@10
DSSM 0.7201 0.6254 0.6814 0.5715 0.5778 0.4908
CLSM 0.7324 0.6346 0.7145 0.5843 0.5827 0.4999
Para.-vector 0.7014 0.6023 0.6701 0.5412 0.5401 0.4659
AverageW 2V 0.6989 0.5914 0.6621 0.5398 0.5459 0.4673
AverageGW CS−1 0.7021 0.6074 0.6787 0.5548 0.5796 0.4863
AverageCPM 0.7187 0.6214 0.6914 0.5745 0.6016 0.5114
ARC-I 0.7145 0.6160 0.6579 0.5415 0.5796 0.5014
CNTN 0.7605 0.6414 0.7345 0.5963 0.6240 0.5483
BICNN 0.7687 0.6423 0.7398 0.5998 0.6137 0.5401
MultiGranCNN 0.7915 0.6714 0.751 0.6215 0.6349 0.5631
ARC-II 0.7341 0.6277 0.6749 0.5545 0.5908 0.5197
AI-CNN 0.7596 0.6409 0.7103 0.5805 0.6187 0.5419
MatchPyramid 0.8148 0.686 0.7659 0.6483 0.6541 0.5883
ABCNN-1 0.8124 0.7046 0.7915 0.6710 0.6887 0.6295
Match-SRNN 0.8179 0.7143 0.8014 0.6625 0.6933 0.6287
HCSM-3W 2V 0.7506 0.6423 0.7315 0.6013 0.6227 0.5415
HCSM-3GW CS−1 0.7714 0.6557 0.7401 0.6290 0.6541 0.5748
HCSM-3CPM 0.8189 0.6711 0.7821 0.6468 0.6662 0.5815
HCSM-1CPM 0.8214 0.7106 0.7945 0.6683 0.6879 0.6214
HCSM-2CPM 0.8459 0.7457 0.8269 0.7015 0.7095 0.6557
HCSM-2CPM +proj 0.8503 0.7514 0.8314 0.7093 0.7119 0.6634
HCSM-2CPM +pos-conv 0.8577 0.7597 0.8392 0.7160 0.7213 0.6688
HCSM-2CPM +val-conv 0.8652 0.7629 0.8498 0.7250 0.7459 0.6854
interaction-focused methods: ARC-II [9], AI-CNN[42], MatchPyramid [21], ABCNN [37], and
Match-SRNN [26]. The interaction-focused methods directly model the interaction relation of
two texts. Word embedding used in some baseline deep learning models is the same as our
word embedding. The batch size used in our experiments and baselines is set to 64. The other
parameters for the baseline methods are set by taking the values from the original papers.
The fifth group contains our proposed HCSM-3 model with different inputs. The HCSM-3GW CS−1
and HCSM-3CPM denote that the input matrixes of HCSM-3 are constructed by word embedding
and concept embedding generated by GWCS-1 [5] and CPM. The HCSM-1CPM and HCSM-2CPM
denote that the input matrixes of HCSM-1 and HCSM-2 are constructed by word embedding and
concept embedding generated by CPM. We also show the results of the HCSM-2CPM model enhanced by three kinds of attention methods: ordinary projection [37], position-based convolution
(posi-conv), and value-based convolution (val-conv).
From the results, we can see the following. (1) The result of Para.-vector is better than that
of AverageW 2V . It shows that directly considering the sentence vector can achieve better performance than the word vector. (2) The results of DSSM and CLSM outperform those of average
combination methods. This verifies that the results can be improved considering the supervised
information, e.g., the relation between the question and answer. In addition, the result of CLSM is
better than that of the DSSM since the CLSM considers the word order information. (3) The results
of HCSM-3 combining methods outperform the result of CLSM, which feeds the neural network
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:17
Table 3. Multi-View Learning Results
Amazon Data Microsoft Data Yahoo Data
Model nDCG@10 P@10 nDCG@10 P@10 nDCG@10 P@10
HCSM-2CPM−QA +val-conv 0.8652 0.7629 0.8498 0.7250 0.7459 0.6854
HCSM-2CPM−QQ +val-conv 0.4467 0.4137 0.3969 0.3420 0.2417 0.2286
Multi-View 0.9017 0.8031 0.8814 0.7668 0.7859 0.7364
Table 4. The nDCG@10 of with or without Multi-view Learning
Amazon Data Microsoft Data Yahoo Data
Model QAbased QQbased Comb. QAbased QQbased Comb. QAbased QQbased Comb.
Supervised 0.8652 0.4467 0.8698 0.8498 0.3969 0.8573 0.7459 0.2417 0.7488
Multi-View 0.8873 0.7591 0.9017 0.8703 0.7849 0.8814 0.7797 0.6870 0.7859
raw text feature instead of high-level feature, e.g., word vector and concept vector in our work.
(4) The interaction-focused methods can achieve better results compared with representationfocused methods. (5) The result of HCSM-3CPM outperforms the HCSM-3GW CS−1 , which once again
validates that our proposed CPM method can outperform the GWCS-1 model. (6) The results of
HCSM-1CPM and HCSM-2CPM outperform most of the baselines and, among them, HCSM-2CPM
shows superior performance. (7) The attentional methods improve the performance; the valuebased convolutional attention method achieves the best result. This shows that our proposed CPM
and HCSM combining method can efficiently learn the deep semantics of the questions. Moreover,
the value-based attentional method can efficiently learn the key parts of the questions and answers
and achieve the best performance.
6.2.3 Multi-View Learning Framework. In this section, we verify whether the multi-view learning framework can help to increase the performance. The results are summarized in Table 3.
HCSM-2CPM−QA represents the HCSM-2CPM trained on the question–answer pairs. HCSM-2CPM−QQ
represents the HCSM-2CPM trained on the question–question pairs. We use the value-based convolutional attention method to enhance the HCSM model in the following experiments. The multiview row in the table represents the multi-view training algorithm applied on the two distinct
HCSM-2 models. As we can see from Table 3, the multi-view training algorithm achieves a better
result than the HCSM-2CPM−QA and HCSM-2CPM−QQ . Here, the result of the multi-view algorithm is
combining the results of the question–answer–based model and question–question–based model,
which have been trained for several iterations.
To clearly show the results of scenarios with or without multi-view learning, we summarize the
main results in Table 4, in which the supervised row denotes the models that are trained only once
using the original data, e.g., question–answer pairs and labeled question–question pairs. The multiview row denotes the models that are trained using the multi-view learning method. The results of
the multi-view row are obtained when the algorithm converges. In Table 4, the “QA based,” “QQ
based,” and “Combined” denote the model HCSM-2CPM−QA trained on question–answer pairs, the
model HCSM-2CPM−QQ trained on question–question pairs, and the model combining these two
models, respectively. The evaluation in Table 4 is nDCG@10. From the result, we can see that
for all three learning models (QA based, QQ based, Combined), the multi-view learning methods
outperform the supervised training methods.
6.2.4 Case Studies. Some new and similar question–question pairs are generated using the
multi-view learning method, as shown in Table 5. We show two parts of the pairs: one part
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
41:18 P. Wang et al.
Table 5. Show Cases
iter5
Amazon How long is the band of this watch in inches?
How long is the watch band in inches?
Microsoft Installing Windows 10
How to install Windows 10
Yahoo Which God is the real god?
Which one is the real god?
iter35
Amazon How long is the watch band in inches?
What is the length of the watch band?
Microsoft Install Windows 10 from USB on a laptop
Clean Install Windows 10 on a laptop?
Yahoo Does God Really Exist?
Are the Gods real?
Fig. 8. Input attention visualization.
is generated at iteration 5 and the other part is generated at iteration 35. We can see that the
question pairs generated later have the same meaning with different words, which indicates that
our method can leverage the information from answers.
6.2.5 Visual Analysis. Figure 8 shows the mutual matrix (a) of an input question–answer pair
and the question attention score (b). The question and answer are “How much is shipping overnight?
I am wanting to purchase this today” and “overnight shipping cost is 0$” (we replace the number with
0). In the Figure 8(a), the i
th row denotes attention values between the i
th word in the question
and each word in the answer. Similarly, the j
th column denotes attention values between the j
th
word in the answer and each word in the question. Darker shades of green indicate stronger attention values. From Figure 8(a), we can see that if two words in the question and answer have a
semantical relation, they will get a high attention value, e.g., “shipping” and “overnight,” “much”
and “cost.” After obtaining the mutual matrix, we use our proposed value-based convolutional attention method to generate the question attention. Here, we average the whole dimension value
of one word and show the result in Figure 8(b). From Figure 8(b), we can see that some important words regarding the answer get high attention values while some unimportant words get low
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:19
Fig. 9. Plot for nDCG@10 against λ.
Fig. 10. nDCG@10 versus number of iterations for one run of multi-view training experiment.
values. For example, the words “much,” “shipping,” “overnight,” and “purchase” get high values
while the words “I,” “wanting,” “this,” and “today” get low values. From this, we can see that our
model can efficiently learn the key parts in question.
6.2.6 Parameter Analysis. We perform a grid search to decide the optimum value of λ in
Equations (12), (14), and (15); the results are shown in Figure 9. The three HCSM variants take
the word and concept embedding generated by CPM as input. From Figure 9, we can see that the
HCSM-2 generally outperforms other variants. In the three datasets, the nDCG@10 increases as
the conceptual features started to be injected into word embeddings. However, a further increase
of λ will lead to some performance degradation in three cases. One interpretation is that the concept information is input as just supplementary information to the word information. A great
deal of useful information would be missing considering too much concept information. For the
Amazon dataset, the optimum values of λ of the HCSM-1, HCSM-2, and HCSM-3 are 0.35, 0.45,
and 0.3, respectively. For the Microsoft dataset, the optimum values of λ of the HCSM-1, HCSM-2,
and HCSM-3 are 0.45, 0.4, and 0.3, respectively. For the Yahoo dataset, the optimum values of λ of
the HCSM-1, HCSM-2, and HCSM-3 are 0.25, 0.2, and 0.25, respectively. Figure 10 gives a plot of
nDCG@10 versus number of iterations in the multi-view training algorithm. We show the result
of HCSM-2CPM enhanced by the value-based convolutional attention method. It verifies that our
proposed method can help augment the unlabeled data to the small training data.
Figure 11 gives a plot of nDCG@10 versus the size of the initial question–answer pairs training
data. We show the result of HCSM-2CPM enhanced by the value-based convolutional attention
method. In this step, we keep the size of the training data of question–question pairs fixed in
order to observe the sensitivity of the result to the size of the initial training data of question–
answer pairs. In terms of Figure 11, Figure 12 gives a plot of nDCG@10 versus the size of the
initial question–question pairs training data. Similarly, we keep the size of the training data of
question–answer pairs fixed in order to observe the sensitivity of the result to the size of initial
training data of question–question pairs. Each one of the experiments runs 40 iterations. From the
results it is clear that the size of the initial training data has a great influence on its corresponding
learning model (e.g. HCSM). From Figure 11, we can see that both of the models have significantly
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
41:20 P. Wang et al.
Fig. 11. nDCG@10 versus the size of the initial question–answer pairs training data.
Fig. 12. nDCG@10 versus the size of the initial question–question pairs training data.
increased results. In comparison, only the QQ-based model result has a significant increase in
Figure 12. This is due to the fact that the question–answer pair data is much larger in quantity
than the question–question pair data. Thus, if we keep the training data size of question-answer
pairs fixed, the QA-based model increases slowly.
7 DISCUSSION
All of the variants of HCSM outperform the average method. This verifies that the high-level semantic information existing in the sentences can be efficiently learned using our proposed HCSM
models in a layer-by-layer convolution and pooling manner. The difference among the three variants is the layer in which the concept information and word information should be merged. HCSM3 merges them before inputting them into the neural network. HCSM-1 merges them after the first
convolutional operation. HCSM-2 merges them after the topmost pooling layer. Among the three
variants, HCSM-2 yields the highest performance in the three datasets. This indicates that learning the hierarchical structure information of the concept and word separately is more effective
than learning the information of merged concept and word. This is because HCSM-2 uses more
parameters to fit the real concept-level function and word-level function. However, the training
time of HCSM-2 nearly doubles compared with the two other models. Thus, the trade-off between
efficiency and performance should be considered in the real-life situation.
8 RELATED WORK
8.1 Question-Retrieval Methods
The majority of traditional question-retrieval studies use unsupervised approaches, such as the LM
[39] and VSM [23]. However, they cannot capture the semantics in short texts. The same meaning
can be represented by different words, which would be regarded as two independent words in
traditional models.
Recently, word-based translation models [12, 16, 33] have been proposed to solve the word mismatch problem. Their principle approaches capture word translation probability by the IBM model
1 [3] and consider the word translation probability into the LM. These methods can outperform
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
Concept and Attention-Based CNN for Question Retrieval in Multi-View Learning 41:21
traditional models (e.g., VSM and LM). In addition, phrase-based translation models [43] have
been proven to outperform the word-based translation models by considering phrase information.
Phrase information can bring more accurate semantic meaning than the information gained by
just using word information. Some other studies model the semantic relations between questions
and answers with topic model [13, 40] or key concept paraphrasing–based language translation
[17, 41]. Recently, some work [4, 44, 45] exploited the category metadata within cQA pages to
further improve performance.
To better capture word semantics and efficiently calculate similarity, Mikolov et al. [18] introduce a Skip-gram model, which is an efficient word-embedding method for learning high-quality
representations of words from large amounts of unstructured text data. To solve the missing capture of syntactically or semantically similar words and the problem of noisy data, some works [31,
38] propose using prior knowledge or knowledge graphs to advance the learning of word embeddings. To solve polysemy, two works [5, 20] propose using multi-sense or concept information to
enhance the learning of word embeddings.
8.2 Sentence Vector Generation Methods
Compared with word representation generation methods, two works [14, 15] propose directly
learning the sentence vector representation from text. Le et al. [15] introduce an unsupervised
model that leverages the paragraph vector and word vectors from a paragraph to predict the following word in the given context. Kalchbrenner [14] proposes using a convolutional neural network to learn the sentence vector employing labeled data.
Due to the fact that the training sample is the question–answer pair in the question-retrieval
task, deep matching models can be applied to the retrieval task so that the sentence representation
can be learned in an end-to-end manner. Recently, the deep matching models were categorized into
two types: representation-focused models and interaction-focused models. The representationfocused models focus on representing text to a dense vector without interaction between the
question and answer, such as DSSM [10], CLSM [24], ARC-I [9], CNTN [22], BI-CNN [35], and
MultiGranCNN [36]. However, the drawback of representation-focused models is that separately
modeling the two sentences is unable to capture the complex sentence interactions. Thus, a variety of interaction-focused models are proposed, such as ARC-II [9], AI-CNN [42], MatchPyramid
[21], ABCNN [37], Match-SRNN [26], and BiMPM [28]. In addition, Mitra et al. [19] proposed using local and distributed representations to handle this problem. The interaction-focused models
build interactions between two sentences. After that, the interactions information can be used in
enhancing the sentence representation process. In addition, the interactions information can also
be used in a deep neural network to learn hierarchial interaction patterns for matching.
8.3 Multi-View Machine Learning
Multi-view learning is concerned with the problem of machine learning from data represented by
multiple distinct feature sets. The recent emergence of this learning mechanism is largely motivated by the property of data from real applications in which each example is described by different
feature sets or different “views.” Moreover, a noteworthy fact for multi-view learning is that when
a natural feature split does not exist, performance improvements can still be observed using manufactured splits [32]. Therefore, multi-view learning is a very promising topic with widespread
applicability.
A related set of research uses labeled and unlabeled data in problem domains in which the
features naturally divide into two disjoint sets. For example, Blum and Mitchell [2] present an
algorithm called co-training to classify webpages using two classifiers. They assume that either
view of the example would be sufficient for learning if they have enough training data, but the
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 4, Article 41. Publication date: January 2018.
41:22 P. Wang et al.
goal is to use both views together to allow inexpensive unlabeled data to augment a smaller set of
labeled examples. They suggest that two learning algorithms are trained separately on each view,
then each algorithm’s prediction on new unlabeled examples are used to enlarge the training set
of the other.
9 CONCLUSIONS
In this article, we propose a unified framework that can simultaneously handle five challenging
problems existing in the question-retrieval task: synonymy, polysemy, word order, question length,
and data sparsity. The question vector generation model enhanced by the value-based convolutional attention method encodes the word embedding and the concept embedding of a question
into a question vector, which solves the first four problems. To solve the last problem, this question vector generation model is trained in a multi-view learning manner. Experimental results
show that our proposed framework outperforms previous models in the question-retrieval and
information-retrieval tasks.
We focus only on question retrieval in the current work. In future works, we will directly apply
our method to the answer retrieval given an input question, which is more meaningful than the
question retrieval in QA systems. In addition, we will try to encode more relationships that exist
in the knowledge base to enhance the learning of word representation. Finally, we will try to use
a semisupervised method to find some question–question pairs, which will be taken as training
data directly.