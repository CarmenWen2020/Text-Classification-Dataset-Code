In this article, we study the problem of online heterogeneous transfer learning, where the objective is to make
predictions for a target data sequence arriving in an online fashion, and some offline labeled instances from
a heterogeneous source domain are provided as auxiliary data. The feature spaces of the source and target
domains are completely different, thus the source data cannot be used directly to assist the learning task in the
target domain. To address this issue, we take advantage of unlabeled co-occurrence instances as intermediate
supplementary data to connect the source and target domains, and perform knowledge transition from the
source domain into the target domain. We propose a novel online heterogeneous transfer learning algorithm
called Online Heterogeneous Knowledge Transition (OHKT) for this purpose. In OHKT, we first seek to
generate pseudo labels for the co-occurrence data based on the labeled source data, and then develop an online
learning algorithm to classify the target sequence by leveraging the co-occurrence data with pseudo labels.
Experimental results on real-world data sets demonstrate the effectiveness and efficiency of the proposed
algorithm.
CCS Concepts: • Computing methodologies → Transfer learning; Online learning settings; Information extraction;
Additional Key Words and Phrases: Transitive transfer learning, online learning, heterogeneous transfer
learning, co-occurrence data
1 INTRODUCTION
Transfer learning [20] is devoted to leveraging knowledge from a source domain to enhance the
learning performance in a target domain, where the data distributions or representations of the two
domains can be different. It has been established as one of the most important learning paradigms
for the tasks where training data in a target domain of interest are limited or too expensive to
collect. Over the last decade, various transfer learning algorithms have been developed and proven
to be effective in many applications, such as text categorization [11, 12], sentiment analysis [45],
image classification [44], recommendation systems [20], etc.
Most of the existing transfer learning approaches focused on homogeneous settings, in which
the source and target domains share a common feature space. Recently, various researchers [10,
28, 37, 38] have considered the case of heterogeneous transfer learning (HTL). Unlike the homogeneous transfer learning, the source and target domains in HTL are from two different feature
spaces. For example, the target learning task can be image classification using SIFT-descriptor feature, and only a few labeled training images are available, while a set of labeled text documents
using bag-of-word features is given as auxiliary source data. HTL task is very challenging since
the two domains, e.g., image data and text data, share no overlap feature spaces. On the other hand,
from the perspective of human learning via transitivity [4], two indirectly related concepts can be
connected with the aid of using auxiliary data as intermediate bridges. For example, after reading
an article about a particular type of bird, we may find it easier to identify the images of this kind
of bird if we have consulted an encyclopedia of birds that includes both a text introduction and
picture samples.
There have been some existing works considering knowledge transfer from a heterogeneous
source domain to a target domain. Yang et al. [43] constructed a graphical probabilistic model
based on two heterogeneous domains, i.e., text and image, to solve an image clustering problem.
Zhu et al. [47] performed heterogeneous images transfer using text-image co-occurrence data. In
[19, 34], researchers modeled the relationship between text and image data as a transition probability matrix, and conducted binary classification tasks for image and text data, respectively. These
algorithms apply the so-called transitive transfer learning idea, i.e., to leverage some auxiliary intermediate data to connect heterogeneous feature spaces, so that knowledge in the source domain
can be transited to the target domain through the intermediate domain.
The concept of transitive transfer learning is proposed in [27]. Take a text-image transfer learning problem as an example, where the objective is to classify images by exploiting knowledge
extracted from text data. There is no direct relationship between text and image data, traditional
knowledge transfer may not be effective in this setting [27]. To solve this issue, transitive transfer learning utilizes intermediate data to bridge target and source domains. In this example, we
use co-occurrence data as the intermediate data. Co-occurrence data can be a set of text-image
pairs represented in two different feature spaces (domains). The unlabeled co-occurrence data is
usually cheap and can be easily collected from web pages or social networks [47]. For instance,
we can easily obtain some images associated with their text descriptions with low cost, and treat
them as co-occurrence data for the text-image transfer learning task. The website, Flickr,1 contains
a tremendous amount of images with tags, and some social networks include a large number of
pictures with text comments posted by users (see Figure 1). Through the intermediate text-image
co-occurrence data, we can transfer knowledge from the text domain to the image domain more
effectively.
1https://www.flickr.com.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
Online Heterogeneous Transfer Learning by Knowledge Transition 26:3
Fig. 1. Text-image co-occurrence data collected from Flickr.
While the effectiveness of HTL has been demonstrated by some works [19, 27, 34, 37, 38, 41,
42, 47], most existing studies are focused on an offline/batch learning fashion, by assuming that
training data from the target domain is accessible/available in advance. However, this assumption
may not be valid in some real-world applications where the target instances arrive sequentially,
e.g., online social platforms or stock pricing systems. Online learning is required in these applications to handle the data stream and respond immediately in real time. Therefore, performing a
batch learning algorithm on the target data stream is not suitable for this kind of online learning
case, and an efficient online transfer learning algorithm must be developed. In the literature, Zhao
et al. studied online heterogeneous transfer learning in [45, 46]. Wu et al. applied Hedge strategy
to deal with online heterogeneous transfer learning problem [35]. The major limitation of these
approaches is that the feature space of the source domain has to be a subset of that of the target
domain.
To address the above-mentioned limitation, we develop a new learning algorithm called Online
Heterogeneous Knowledge Transition (OHKT) by transiting knowledge from a source domain to
a target domain through an intermediate domain including co-occurrence data. Figure 2 shows a
general framework of the proposed method. We first transfer knowledge from the source domain to
the intermediate domain by training a classifier on the labeled source data to classify the unlabeled
co-occurrence data. After that, the co-occurrence data with pseudo labels can be used to train a
classifier to assist the learning task in the target domain. As a result, labeled information extracted
from the labeled source data is transited to the target task through the co-occurrence data. For the
online learning task in the target domain, we propose an online learning model involving both
co-occurrence and target data, and design an algorithm to solve the resulting optimization model.
The major contributions of this article are as follows:
(1) We propose an online heterogeneous transfer learning algorithm that transits knowledge
from a heterogeneous source domain to a target domain through auxiliary co-occurrence
data. With the aid of co-occurrence data, we can connect two different feature spaces, so
that label information extracted from labeled source data can be leveraged to enhance the
learning performance on the target task.
(2) We propose a new online learning model by incorporating an offline learning model using
co-occurrence data with pseudo labels, and develop an optimization algorithm to solve it
analytically.
(3) We perform extensive experiments on real-world data sets to demonstrate the effectiveness and efficiency of our proposed algorithm.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
26:4 H. Wu et al.
Fig. 2. Knowledge Transition from a text source domain into an image target domain.
This article is organized as follows: We review some related works in Section 2, and provide
the definition of our problem in Section 3. Section 4 presents the details of our proposed method.
Section 5 shows the experimental results and Section 6 concludes our work.
2 RELATED WORK
2.1 Online Learning
Online learning has been extensively studied in the community of machine learning [5, 18]. Online learning is able to handle a classification task on a data sequence arriving in an online fashion,
and make a prediction once an instance arrives. One of the early algorithms of online learning is
the Perceptron [21], which simply updates a linear classifier based on the gradient information
when a new instance is predicted incorrectly. Freund and Schapire [16] propose a hedge strategy
to dynamically update the weights of multiple classifiers according to their performance. Crammer
et al. [8] and Shalev-Shwartz et al. [23] adopt the criterion of maximum margin to model the online learning problem as a constrained convex optimization problem, which enjoys a closed-form
solution. Shalev-Shwartz et al. [24] and Zinkevich [48] first update the classifier by the gradientbased approach, and then project the classifier into a constrained space. In recent years, some
algorithms have been proposed to make use of the second-order information. [9, 14, 31, 32] adopt
confidence-weighted strategy to control the update process of the classifier.
2.2 Transfer Learning
Transfer learning aims to leverage knowledge extracted from a source domain to enhance a task
in a target domain [20]. Based on the relationship between the source and target feature spaces,
we can divide transfer learning into two classes: homogeneous transfer learning or heterogeneous transfer learning. Homogeneous transfer learning, which addresses the situation where the
source and target instances are represented in the same feature space, has been successfully applied in some applications like text mining [11–13] and image classification [22, 33]. To solve
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
Online Heterogeneous Transfer Learning by Knowledge Transition 26:5
homogeneous transfer learning problem, Daume [13] proposes a solution based on multi-task
learning and discusses a generalized model using regularization term. Saito et al. [22] propose
a novel asymmetric tri-training method involved pseudo labels to solve the homogeneous transfer learning problem, and provided the error bound according to the theory work [2]. In contrast,
heterogeneous transfer learning deals with problems in which the feature spaces of the source and
target domains are different. Feuz and Cooke [15] propose to handle the activity recognition problem using multi-view knowledge transfer, which treats the target data as multi-view instances and
uses one view of them to connect with the source data. Recently, various heterogeneous transfer
learning algorithms have been proposed to deal with image classification tasks by using text source
data [30, 41, 43, 47]. Besides, Ng et al. [19], Tan et al. [28], and Wu et al. [34] construct a transition
probability matrix based on co-occurrence data to perform the classification problem in the target domain. Tan et al. [27] studies a learning problem called transitive transfer learning, in which
one or multiple intermediate domains are selected to connect the source and target domains. Yang
et al. [42] proposes to evaluate the relatedness among the domains through transferred weights,
which can be used to determine how much should we transferred from a source domain to a target
domain.
The above works study offline transfer learning problems where the training data are given in
advance. By far, there are only a few studies considering the problem of online transfer learning
[17, 29, 39, 40, 45, 46]. Yan et al. [39, 40] handles the online heterogeneous transfer problem by
updating weights of offline and online classifiers based on the Hedge weighting strategy [16].
Zhao and Hoi [45] and Zhao et al. [46] deal with the problem of homogeneous online transfer
based on the ensemble learning. Zhao and Hoi [45] and Zhao et al. [46] also address the problem
of online heterogeneous transfer with the assumption that the feature space of the source domain
is a subset of that of the target domain. However, in our problem, the feature spaces of the source
and target domains are different, e.g., text and image data.
The problem of online heterogeneous transfer studied in this article is different from multi-view
learning [26, 36]. In multi-view learning problems, one data instance consists of multiple views.
In other words, the instances from one view will have correspondences in other views as well.
In our problem, the co-occurrence data, which can be regarded as multi-view instances, are used
to connect two different feature spaces. However, different from [15], the target data here arrive
in an online fashion and they are single-view instances, e.g., image data; and the source data are
represented in another different view, e.g., text data. Our goal is to transfer knowledge from a
single view to another single view.
3 PROBLEM DEFINITION
Let Xs × Ys be the data space of a source domain, where Xs = Rds , Ys = {+1, −1}, s denotes the
source domain and ds is the feature dimension of source data. Some labeled source instances
{xs
i ,ys
i }
ns
i=1 ∈ Xs × Ys are given beforehand as the training data, ns denotes the number of source
instances. Let X×Y be the data space of a target domain, where X = Rd and Y = {+1, −1}, d
is the feature dimension of target data. In our study problem, we follow the settings in [39, 40,
45, 46], where no target data are provided in advance, and target instances {xt,yt }
T
t=1 arrive in
an online fashion, T denotes the number of target instances. The label spaces of the source and
target domains are identical with each other, i.e., Y = Ys . In other words, the same label in two
different domains indicates the same category. For example, in a binary image classification task
whose goal is to distinguish between the car and boat classes, +1 represents the car class and −1
represents the boat class in both source and target domains, while the feature spaces of the source
and target domains are totally diverse, i.e., X∩Xs = ∅. For example, source instances could be
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
26:6 H. Wu et al.
some text data, while target instances could be some images. Note that in [35, 45, 46] the feature
space of a source domain has to be a subset of that of the target domain, i.e., Xs ⊂ X. Thus, the
online heterogeneous learning algorithms proposed in [35, 45, 46] are not fit for our considered
problem.
The objective of online heterogeneous transfer learning is to train a classifier f (xt ) to make
predictions for the target instances that arrive in an online fashion. Specifically, at the tth round,
the classifier receives a target instance xt and provides a predicted class labelyˆt immediately. Then,
the classifier obtains the true class labelyt and adjusts itself according to a predefined loss function.
Without loss of generality, we adopt the linear prediction function as the classifier, i.e., f (xt ) =
siдn(w · xt ). Through the kernel trick, we are able to handle nonlinear separable problems.
—Co-occurrence data. In order to connect the different feature spaces of the source and target
domains, we use the co-occurrence data as a bridge to transfer knowledge from the source
domain to the target domain. The unlabeled co-occurrence data {(x˜ s
i , x˜i )}
nc
i=1 ∈ Xs × X are
collected in advance, where nc denotes the number of co-occurrence instances, x˜ s
i ∈ Xs and
x˜i ∈ X. For example, in Flickr, users can post and add some tags to describe them. As a result,
each image is associated with some tags, which can be used as the co-occurrence data to
connect the texts and images. Figure 1 shows the text-image co-occurrence data that are
collected from Flickr. In this example, x˜ s
i represents the text part of the ith co-occurrence
instance, while x˜i represents the image part of the ith co-occurrence instance.
4 METHODOLOGY
In this section, we first introduce how to use co-occurrence data to connect the source and target feature spaces, so that knowledge extracted from the source domain can be transited to the
target domain through co-occurrence data. After that, we present the proposed OHKT learning
algorithm.
4.1 Knowledge Transition
We use a text-image application to illustrate the knowledge transition mechanism of our approach.
In this application, the knowledge extracted from text data is leveraged to assist to classify target
images based on text-image pairs. The co-occurrence text-image pairs can be regarded as multiview data. Based on the assumptions in multi-view learning [3, 25, 36], we are able to use the
labeled text source data to generate predicted labels for the co-occurrence data, and then use these
co-occurrence data with pseudo labels to build a classifier in the target feature space, which can
be used to assist the learning task on the target data. As a result, label information involved in the
labeled text data is transited from the source domain to the target domain.
We first train a classifier on the labeled text data to make predictions for the text-view of
the co-occurrence data. To this end, we refer to the labeled text source instances {xs
i ,ys
i }
ns
i=1
as the training data to train a linear SVM classifier, and then make predictions for the text parts of
the co-occurrence data {x˜ s
i }
nc
i=1 to generate labels {yˆ
c
i }
nc
i=1.
In multi-view learning, the compatibility assumption states that the objective classifier in each
view provides the same labels for most multi-view instances [25]. We employ a stronger assumption that two objective classifiers in the text and image views are consistent with each other on
the labels of all the given instances. Based on this assumption, the predicted labels {yˆ
c
i }
nc
i=1 given by
the text classifier are regarded as the labels of the corresponding image parts of the co-occurrence
data {x˜i}
nc
i=1. Consequently, we obtain the labeled image instances {x˜i,yˆ
c
i }
nc
i=1, which can be used
to train a classifier in the target feature space.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
Online Heterogeneous Transfer Learning by Knowledge Transition 26:7
The sufficiency assumption in multi-view learning states that every view is sufficient for classification on its own [36]. Therefore, either text or image view is sufficient to train a classifier
for the co-occurrence instances. Therefore, we can train a linear SVM classifier w˜ on the image
parts of the co-occurrence data with pseudo labels, and then propose an online learning algorithm
that leverages w˜ to assist the learning task on the target data. The proposed algorithm will be
introduced in detail in Section 4.3.
Figure 2 illustrates an example of knowledge transition from a text source domain into an image target domain. We use the labeled source text data to construct a supervised learning problem,
which generates the predicted labels for the text parts of the co-occurrence data. After that, these
predicted labels are assigned to their corresponding image parts, so that the labeled images are
obtained to train a classifier, which is helpful to address the online learning task to make predictions for the target images. The overall learning algorithm is referred to as OHKT, which stands
for Online Heterogeneous Knowledge Transition. Algorithm 1 summarizes our proposed OHKT algorithm. It is worthwhile noting that steps 1-4 can be conducted in an offline stage since the source
and co-occurrence data are given in advance. We only need to perform Algorithm 2 in an online
fashion, which significantly reduces the computational cost of online learning procedure.
ALGORITHM 1: Online Heterogeneous Knowledge Transition (OHKT)
Input: The labeled source instances {xs
i ,ys
i }
ns
i=1,
the unlabeled co-occurrence instances{(x˜ s
i , x˜i )}
nc
i=1.
1: Train a classifier on {xs
i ,ys
i }
ns
i=1,
2: Make predictions for {x˜ s
i }
nc
i=1 to obtain {yˆ
c
i }
nc
i=1,
3: Assign the predicted labels {yˆ
c
i }
nc
i=1 to the correspondences {x˜i}
nc
i=1,
4: Obtain w˜ on {x˜i,yˆ
c
i }
nc
i=1 by linear SVM,
5: Perform the online learning task on the target instances (Algorithm 2).
4.2 Learning on Source and Co-occurrence Data
In order to extract the knowledge from the labeled source data, we use the labeled source instances
{xs
i ,ys
i }
ns
i=1 to train a linear SVM classifier. The goal is to make predictions for the instances {x˜ s
i }
nc
i=1
in the co-occurrence data, so that a classifier can be trained on these instances with generated
pseudo labels. By performing linear SVM on the labeled source data, we obtain a parameter vector
v, and then we can generate the predicted labels of {x˜ s
i }
nc
i=1 by
yˆ
c
i = sign(v · x˜
s
i ). (1)
Based on the compatibility assumption in multi-view learning, we assign the pseudo labels
{yˆ
c
i }
nc
i=1 to instances {x˜i}
nc
i=1, which is another part of the co-occurrence data.
Given the co-occurrence data that have already been labeled, now we can use the labeled instances {x˜i,yˆ
c
i }
nc
i=1 to train a classifier to assist the online learning task on target data. Specifically,
we apply linear SVM to {x˜i,yˆ
c
i }
nc
i=1 to train w˜ .
4.3 Online Learning on Target Data
In our problem settings, target data arrive in an online fashion, thus the weight vector should be updated in each round. To handle this issue and motivated by [8], we seek to develop a weight update
model, which formalizes the tradeoff between the amount of progress made on each round and the
amount of previous information. Here, the previous information means the trained weights from
the previous rounds and the trained parameters w˜ from the source domain through co-occurrence
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
26:8 H. Wu et al.
data. Specifically, we regularize the weight vector based on not only the weights from previous
rounds, but also the source domain model. Concretely, we propose to solve the following minimization problem to obtain wt+1
min
w,ξt
η
2
||w−wt ||2 + η˜
2
||w − w˜ ||2 + Cξt,
s.t.1 − yt (w · xt ) ≤ ξt,
ξt ≥ 0,
(2)
where we set η + η˜ = 1. On the one hand, our update requires wt+1 to correctly classify the current
instance with a sufficiently large margin. On the other hand, we design two regularization terms
to make wt+1 remain as close as possible to wt and w˜ , in which the former one involves the
knowledge learned from the previous rounds and the latter one is constructed based on the cooccurrence data with the pseudo labels, thus the previous information can be retained. As a result,
we can make a prediction for the next instance xt+1 by
f (xt+1) = sign(wt+1 · xt+1). (3)
The solution to Problem (2) is given by Proposition 1.
Proposition 1. For the minimization problem (2), the solution is expressed as
wt+1 = ηwt + η˜w˜ + τtyt xt, (4)
where τt = min{C, [1−yt (ηwt +η˜w˜ )·xt ]+
| |xt | |2 }, and [·]+ = max{·, 0}.
Proof. To solve the constrained optimization problem (2), we introduce the non-negative Lagrangian multipliers τt and λt to achieve the Lagrangian
L(w,ξt, τt, λt ) = η
2
||w − wt ||2 + η˜
2
||w − w˜ ||2 + Cξt + τt (1 − yt (w · xt ) − ξt ) + (−λtξt ). (5)
The KKT conditions are
∂
∂wL(w,ξt , τt, λt ) = η(w − wt ) + η˜(w − w˜ ) − τtyt xt
= 0
⇒ w = ηwt + η˜w˜ + τtyt xt, (6)
∂
∂ξt
L(w,ξt , τt, λt ) = C − τt − λt = 0
⇒ τt = C − λt ≤ C, (7)
τt ≥ 0, (8)
λt ≥ 0, (9)
τt (1 − yt (w · xt ) − ξt ) = 0, (10)
λtξt = 0. (11)
By substituting these results into Equation (5), we rewrite the Lagrangian as
L(τt ) = η
2
||ηwt + η˜w˜ + τtyt xt − wt ||2 + η˜
2
||ηwt + η˜w˜ + τtyt xt − w˜ ||2
+ τt (1 − yt ((ηwt + η˜w˜ + τtyt xt ) · xt )).
(12)
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
Online Heterogeneous Transfer Learning by Knowledge Transition 26:9
Therefore, we obtain the dual form of the problem (2) as
min
τt
1
2
||xt ||2
τ 2
t + ytτt (ηwt + η˜w˜ ) · xt − τt,
s.t. 0 ≤ τt ≤ C.
(13)
By setting the partial derivative w.r.t. τt to 0 without considering the inequality constraints, we
get
∂L(τt )
∂τt
= ||xt ||2
τt + yt (ηwt + η˜w˜ ) · xt − 1 = 0
⇒ τt = 1 − yt (ηwt + η˜w˜ ) · xt
||xt ||2 .
(14)
Combining the fact that 0 ≤ τt ≤ C, we thus obtain
τt = min
C,
[1 − yt (ηwt + η˜w˜ ) · xt]+
||xt ||2

, (15)
where [·]+ = max{·, 0}.
Combining Equation (6), we complete the proof of Proposition 1.
Algorithm 2 presents the procedure of the learning on target data.
ALGORITHM 2: Algorithm of learning on target data.
Input: Trade-off parameters C > 0, η, η˜, where η + η˜ = 1.
Initialize: w1 = 0
1: Train w˜ over {x˜i,yˆ
c
i }
nc
i=1 based on SVM
2: for t = 1 to T do
3: Receive instance: xt ∈ X
4: Predict: yˆt = sign(wt · xt )
5: Receive the true label yt ∈ Y and compute the error rate
6: Compute learning rate:τt = min
C, [1−yt (ηwt +η˜w˜ )·xt ]+
| |xt | |2

7: Update: w = ηwt + η˜w˜ + τtyt xt
8: end for
4.3.1 Kernel Method. Now we present how to extend Algorithm 2 to handle non-linearly separable problems by the kernel method. The main idea of the kernel method is to utilize a function
ϕ(·) to map an original feature vector x into a higher-dimensional space, in which the instances
with different labels can be separated linearly; and the inner product of two mapped instances of
xi and xj can be represented by a kernel function, i.e.,
K (xi, xj) = ϕ(xi ) · ϕ(xj). (16)
We can simply replace the inner product operation between two instances in the linear predictors by a predefined kernel function, even without knowing the formula of the mapping function
ϕ(·).
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.     
26:10 H. Wu et al.
In order to apply the kernel method, we rewrite the linear classifier used at the (t + 1)-th round
as
wt+1 = ηwt + η˜w˜ + τtyt xt = η(ηwt−1 + η˜w˜ + τt−1yt−1xt−1) + η˜w˜ + τtyt xt
= η(··· (η(η˜w˜ + τ1y1x1) + η˜w˜ + τ2y2x2) ··· ) + η˜w˜ + τtyt xt
= (1 + η + ··· + ηt−1)η˜w˜ + ηt−1
τ1y1x1 + ηt−2
τ2y2x2 + ··· + τtyt xt
= 1 − ηt
1 − η
η˜w˜ +
t
i=1
ηt−i
τiyixi .
(17)
Remember that for the linear SVM on the co-occurrence data, we have
w˜ =
nc
i=1
αc
i yˆ
c
i x˜i, (18)
where αc
i is the dual variable for instance (x˜i,yˆ
c
i ), i = {1,...,nc } [6]. Combining Equations (17)
and (18), we can make prediction for instance xt+1 by
wt+1 · xt+1 =

1 − ηt
1 − η
η˜
nc
i=1
αc
i yˆ
c
i x˜i +
t
i=1
ηt−i
τiyixi


· xt+1
= 1 − ηt
1 − η
η˜
nc
i=1
αc
i yˆ
c
i (x˜i · xt+1) +
t
i=1
ηt−i
τiyi (xi · xt+1).
(19)
Last, we replace the inner product operation between two instances by the kernel function to
obtain the nonlinear prediction function
f (xt+1) = 1 − ηt
1 − η
η˜
nc
i=1
αc
i yˆ
c
i K (x˜i, xt+1) +
t
i=1
ηt−i
τiyiK (xi, xt+1). (20)
Therefore, instead of updating the linear classifier w, we save the learning rate τt , the true
label yt , and the current instance xt , which are used to calculate the kernel function and make
predictions for the future instances in Equation (20). In this article, we adopt the Gaussian kernel,
i.e., K (xi, xt+1) = exp(− 	xi−xt+1 	2
2σ 2 ), where σ > 0 is the bandwidth.
5 EXPERIMENTS
In this section, we perform experiments on two real-world data sets to show the effectiveness and
efficiency of the proposed algorithm. We first describe the data sets used in our experiments, and
then present the compared baseline methods and the experimental setting. After that, we discuss
the results and the effects of parameters.
5.1 Datasets
—Text-Image Dataset. We use the benchmark data set NUS-WIDE [7] to generate online classification tasks. The NUS-WIDE data set is collected from the Flickr website. It includes image
instances and associated text tags, and has been widely used in studies of heterogeneous
transfer learning. We randomly pick up 10 categories (bird, boat, flower, food, rock, sun,
tower, toy, tree, vehicle) to construct (
10
2 ) = 45 binary classification tasks, each of which
takes one category as the positive class and another one as the negative class. For each
task, we randomly select 600 image instances as the target data, 1,200 labeled text instances
as the source data, and 1,600 unlabeled text-image pairs as the co-occurrence data.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.  
Online Heterogeneous Transfer Learning by Knowledge Transition 26:11
Table 1. Statistics of the Datasets in Terms of #instances × #features
Text-Image Dataset Multi-Language Dataset
target data 600 × 500 O(103) × O(104)
source data 1,200 × 1,000 O(103) × O(104)
co-occurrence data 1,600 × 1,500 O(103) × O(104)
—Multi-Language Dataset. We use the Multi-Language dataset [1] to construct online learning
tasks. The dataset contains documents written in different languages and translated documents from one language to another. The documents are associated with six categories: C15,
CCAT, E21, ECAT, GCAT, and M11. We use four languages (English, French, German, and
Spanish) and the six classes to construct ( 4
2 ) × ( 6
2 ) = 90 two-language binary classification
tasks, each of which takes two languages as the source and target domains, respectively.
Every task involves thousands of source instances, target instances, and co-occurrence instances. A co-occurrence instance includes a document and its translated version.
Table 1 lists the statistical information of the datasets in terms of #instances × #features, e.g.,
600 × 500 in the table means 600 instances, each of which is represented by a 500-dimensional
feature vector.
5.2 Baseline Methods
—PA [8]. The PA algorithm is a first-order online learning algorithm. PA models online learning as a constrained optimization problem, which minimizes the distance between the new
classifier and the current classifier with the constraint that the hinge loss made by the new
classifier should be zero. We conduct PA as a baseline method without knowledge transfer.
—SCW [31]. The SCW method is a second-order online learning algorithm. SCW assumes a
Gaussian distribution of weights, where the weight distribution can be updated by minimizing the Kullback-Leibler divergence between the new weight distribution and the old
one. SCW is considered as the state-of-the-art online learning algorithm, and we conduct it
as a baseline method without knowledge transfer.
—HET-PA. We develop a knowledge transfer method called HET-PA based on k Nearest Neighbors (kNN) as baseline, which transfers knowledge from auxiliary unlabeled co-occurrence
data to the target task. Specifically, for each target instance, HET-PA finds the k nearest
instances in the co-occurrence data, and then aggregates the heterogeneous parts of the
neighbors as the heterogeneous features. Then the PA algorithm is conducted on the new
features. We adopt cosine similarity to search for nearest neighbors, and k is set to be 1
10nc .
—PCA-PA. We also develop a knowledge transfer method called PCA-PA based on Principal
Component Analysis (PCA) as baseline, which transfers knowledge from auxiliary unlabeled data represented in target feature space (i.e., target-view parts of co-occurrence data)
to the target task. We perform PCA on the target-view parts of co-occurrence data to find
the latent factors for the target data, so that the target data can be represented by the latent factors. Then the PA algorithm is used to learn an online learning classifier on the new
representations.
—HTLIC [47]. The HTLIC (heterogeneous transfer learning for image classification) algorithm
is an offline heterogeneous transfer learning algorithm. HTLIC performs matrix factorization on co-occurrence and source data to find latent semantic representations for target data,
and then uses SVM to train a classifier on the new representations to make predictions for
target data. In order to fit the setting of online learning, we periodically train the classifier
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
26:12 H. Wu et al.
Fig. 3. Results of different algorithms using linear kernel on all 45 tasks on the Text-Image dataset.
based on SVM once T
20 target instances arrive, and use the trained classifier to make predictions for the next T
20 instances, where T is the total number of target data. LIBSVM [6] is
used as the implementation of SVM.
—OHTWC [39, 40]. The OHTWC approach builds an offline classifier based on heterogeneous similarity constructed from labeled source data and unlabeled co-occurrence data,
and learns an online classifier from the target data, and then combines the offline and online classifiers using the Hedge weighting strategy to update their weights for final prediction. OHTWC is considered as the state-of-the-art online heterogeneous transfer learning
algorithm.
We conduct experiments by adopting linear kernel for all the compared algorithms. The aggressiveness parameter C = 1 in OHKT, SCW, OHTWC, and PA-based algorithms, i.e., PA, HET-PA,
and PCA-PA. In OHKT, we set η = η˜ = 1
2 . In addition, we extend our OHKT method using the kernel function, and make comparisons to several methods adopted kernel function. Specifically, we
use the Gaussian kernel with the bandwidth σ = 2. For simplicity, we denote k as the algorithms
adopted Gaussian kernel, e.g., PA-k represents the PA algorithm using Gaussian kernel. To obtain stable results, we draw 20 times of random permutation of target instances and evaluate the
performances of the algorithms based on the average rate of mistakes.
5.3 Experimental Results
5.3.1 Results on the Text-Image Dataset. In this experiment, we use 1,600 unlabeled cooccurrence instances and 1,200 labeled text source instances to assist the target learning task.
Figure 3 shows the average rates of mistakes of different algorithms using linear kernel on all the
45 tasks. The y-axis of the figure refers to the error rate, and the x-axis refers to the 45 tasks. Table 2
shows the mean values and standard deviations of mistake of all the compared algorithms. Due
to the space limitation, we randomly pick up several representative tasks and present the results
of them, and list the means of all the 45 tasks at the bottom of Table 2. On most tasks, our proposed algorithm achieves the best performance among all the tested algorithms. We draw several
interesting observations from the results as follows.
• Algorithms that adopted transfer learning outperform algorithms without knowledge transfer, which indicates that leveraging knowledge from auxiliary data can effectively improve
the performance of the target learning task.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
Online Heterogeneous Transfer Learning by Knowledge Transition 26:13
Table 2. Results (%) of Different Algorithms Using Linear Kernel on Representative
Tasks on the Text-Image Dataset
Task PA SCW HET-PA PCA-PA HTLIC OHTWC OHKT
1 34.01 ± 1.39 30.77 ± 1.81 30.22 ± 1.08 33.66 ± 1.56 37.11 ± 1.12 28.85 ± 0.79 24.58 ± 0.21
3 34.28 ± 1.82 32.59 ± 1.55 30.95 ± 1.22 33.88 ± 1.14 34.67 ± 1.38 29.69 ± 1.25 24.32 ± 0.13
5 30.31 ± 1.58 28.81 ± 1.11 29.18 ± 1.46 30.77 ± 1.72 31.13 ± 0.79 24.64 ± 0.92 22.13 ± 0.18
7 37.27 ± 1.42 34.95 ± 1.56 41.24 ± 1.75 36.42 ± 1.52 34.98 ± 1.08 35.18 ± 1.27 30.39 ± 0.20
9 27.18 ± 1.20 25.57 ± 1.07 26.22 ± 1.38 26.61 ± 0.98 25.42 ± 1.18 24.11 ± 0.90 19.70 ± 0.15
11 27.28 ± 1.28 25.20 ± 1.35 27.14 ± 1.51 27.34 ± 1.27 27.67 ± 1.02 23.28 ± 0.87 20.29 ± 0.23
13 25.21 ± 1.06 22.41 ± 1.20 25.93 ± 1.25 24.18 ± 1.21 19.19 ± 0.71 21.73 ± 0.94 19.19 ± 0.20
15 29.85 ± 1.37 26.96 ± 1.35 27.16 ± 1.25 28.75 ± 1.40 31.49 ± 1.11 23.34 ± 0.91 19.67 ± 0.17
17 26.39 ± 1.20 24.17 ± 1.13 27.72 ± 0.84 26.88 ± 1.18 20.69 ± 0.84 22.35 ± 0.89 18.03 ± 0.13
19 26.64 ± 1.26 23.98 ± 0.85 22.32 ± 0.97 25.48 ± 1.20 21.55 ± 0.86 22.28 ± 0.84 17.57 ± 0.19
21 32.07 ± 1.33 28.67 ± 1.29 36.49 ± 1.62 30.75 ± 0.95 39.78 ± 1.00 30.86 ± 1.30 31.33 ± 0.16
23 25.74 ± 1.07 23.56 ± 0.81 31.92 ± 1.74 24.88 ± 1.14 27.65 ± 1.18 24.56 ± 1.02 20.34 ± 0.16
25 21.33 ± 1.02 19.40 ± 0.82 21.16 ± 1.16 20.63 ± 1.17 24.97 ± 1.14 19.61 ± 0.82 17.76 ± 0.09
27 28.94 ± 1.51 25.78 ± 1.02 29.45 ± 1.27 27.79 ± 1.58 34.26 ± 1.47 29.45 ± 1.22 19.71 ± 0.15
29 37.43 ± 1.69 34.72 ± 1.46 39.20 ± 0.78 36.39 ± 1.57 35.16 ± 0.75 35.82 ± 1.37 31.58 ± 0.13
31 42.54 ± 1.48 42.63 ± 1.57 40.05 ± 1.45 43.87 ± 1.15 38.02 ± 1.71 35.47 ± 1.02 36.18 ± 0.13
33 27.34 ± 1.74 25.04 ± 1.24 27.23 ± 1.25 27.32 ± 1.22 30.94 ± 0.76 24.66 ± 1.27 18.39 ± 0.11
35 23.07 ± 1.48 20.40 ± 0.83 22.14 ± 1.10 22.18 ± 1.52 19.17 ± 0.84 19.22 ± 0.88 15.88 ± 0.10
37 35.19 ± 1.52 31.61 ± 1.36 35.45 ± 1.32 33.92 ± 1.41 34.94 ± 0.89 31.60 ± 1.28 27.77 ± 0.21
39 26.42 ± 1.27 24.99 ± 1.42 23.21 ± 0.73 26.08 ± 0.97 22.75 ± 0.70 26.54 ± 0.89 19.35 ± 0.17
41 27.53 ± 1.06 24.69 ± 1.08 23.76 ± 0.73 27.17 ± 1.20 23.47 ± 0.97 24.93 ± 0.69 18.07 ± 0.10
43 35.62 ± 1.34 33.90 ± 1.57 31.58 ± 1.19 34.77 ± 1.21 32.60 ± 1.17 32.16 ± 1.04 29.68 ± 0.11
45 28.72 ± 1.60 26.57 ± 1.30 28.92 ± 1.48 27.43 ± 1.48 29.72 ± 1.53 27.82 ± 1.44 23.90 ± 0.13
Avg. 30.01 27.94 29.82 29.50 29.56 26.98 23.07
• HET-PA and PCA-PA usually obtain comparable results to PA, which implies that the simple
methods of using co-occurrence data in HET-PA and PCA-PA cannot effectively exploit
auxiliary data. Therefore, a more refined method to leverage co-occurrence data is needed
to enhance the performance of the target task.
• SCW outperforms PA on most tasks, which demonstrates the effectiveness of the secondorder method. OHKT performs better than PA and SCW, which validates that transfer learning with knowledge transition is beneficial for improving performance on target domain.
• OHKT outperforms all the baseline methods and achieves the lowest standard deviations on
most tasks. Label information of the source data is transited to the target domain through
co-occurrence data, leading to the improvement of performance on the target classification
task. This observation validates that our proposed algorithm is able to stably achieve better
performance than other baseline methods.
In addition, we extend OHKT using Gaussian kernel, i.e., OHKT-k, and make comparisons to PAk and SCW-k. The results on all the 45 tasks are presented in Figure 4. We randomly pick up several
representative tasks and present the results of them in Table 3. In general, algorithms adopted
Gaussian kernel obtain better performance than that adopted linear kernel. Similar observations
can be drawn as in experiments using linear kernel.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
26:14 H. Wu et al.
Fig. 4. Results of different algorithms using Gaussian kernel on all 45 tasks on the Text-Image dataset.
Table 3. Results (%) of Different Algorithms Using
Gaussian Kernel on Representative Tasks on the
Text-Image Dataset
Task PA-k SCW-k OHKT-k
5 28.23 ± 1.04 25.29 ± 1.12 16.36 ± 0.62
15 28.01 ± 1.14 23.52 ± 1.07 15.07 ± 0.56
25 20.43 ± 0.89 17.17 ± 0.95 11.51 ± 0.59
35 21.94 ± 0.93 18.58 ± 0.81 10.37 ± 0.53
45 27.33 ± 1.19 23.96 ± 1.00 19.79 ± 0.79
Avg. 28.33 24.94 18.28
Fig. 5. Results of different algorithms using linear kernel of all 90 tasks on the Multi-Language dataset.
5.3.2 Results on the Multi-Language Dataset. The feature dimension of Multi-Language dataset
is much larger than that of the Text-Image dataset, leading to the increasing excessive running
times to perform SCW and HTLIC methods. Also, the algorithms that adopted Gaussian kernel
need too much time to finish the learning tasks. Therefore, in this experiment, we use linear kernel for all the tested algorithms, and compare OHKT with PA, HET-PA, PCA-PA, and OHTWC
algorithms. Figure 5 shows the average mistake rates of different algorithms on each task. The
y-axis of the figure refers to the error rate, and the x-axis relates to the 90 tasks. Table 4 presents
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
Online Heterogeneous Transfer Learning by Knowledge Transition 26:15
Table 4. Results (%) of Different Algorithms Using Linear Kernel
on Representative Tasks on the Multi-Language Dataset
Task PA HET-PA PCA-PA OHTWC OHKT
1 44.86 ± 0.61 49.71 ± 0.63 42.20 ± 0.40 42.15 ± 0.72 23.36 ± 0.62
4 33.94 ± 0.62 48.87 ± 0.69 37.08 ± 0.35 29.21 ± 0.56 14.22 ± 0.52
39 37.80 ± 0.83 40.42 ± 0.77 31.90 ± 0.51 28.89 ± 1.27 25.21 ± 0.51
49 32.86 ± 0.75 49.39 ± 0.60 33.47 ± 0.36 26.06 ± 0.55 14.47 ± 0.53
75 26.36 ± 0.54 35.16 ± 0.45 27.67 ± 0.39 20.13 ± 0.37 13.32 ± 0.27
84 38.80 ± 0.52 36.18 ± 0.52 32.00 ± 0.55 29.57 ± 0.20 21.38 ± 0.61
88 36.96 ± 0.97 48.15 ± 1.19 43.82 ± 0.80 21.38 ± 0.68 16.62 ± 0.80
90 24.88 ± 0.66 32.97 ± 0.57 29.53 ± 0.51 19.87 ± 0.40 14.99 ± 0.42
Avg. 36.45 43.73 37.28 28.21 21.61
the numerical results of all the compared algorithms on several randomly selected tasks and the
average results across all the 90 tasks. We observe that PA outperforms HET-PA on almost all the
tasks, which indicates that the utilization of the co-occurrence data in HET-PA is too trivial to
help improve the performance. PCA-PA shows comparable results to PA, but obtains a high error rate compared to OHKT. On most tasks, OHKT achieves better performance than PA, which
demonstrates that knowledge transferred from the source domain is helpful for the target task.
OHKT obtains the best result on average, which validates that exploiting knowledge extracted
from the source and co-occurrence data can lead to better performance.
5.4 Efficiency Evaluation
We also empirically investigate the efficiency of our proposed algorithm on a representative task
of the Text-Image dataset. In our proposed OHKT algorithm, only Algorithm 2 is run in an online
fashion, while the other procedures can be conducted in an offline stage. Therefore, we just evaluate the running time of Algorithm 2. For HTLIC, we can also use an offline stage to perform matrix
factorization on the source and co-occurrence data. So the running time of HTLIC is tested on the
target learning task without considering the cost of matrix factorization. In addition, as HET-PA
and PCA-PA algorithms are conducted based on PA, we only report the running time of PA as a
benchmark. We use the linear kernel for the tested algorithms and report the running times on a
representative task in Figure 6. We see that PA is the most efficient one in the compared methods
since it does not utilize any auxiliary information. HTLIC takes much more time because of the
periodic training of SVM. OHKT needs more time than PA to update the classifier by using the
classifier that is trained on the co-occurrence data. In general, compared to PA, OHKT takes comparable online computational time and it can achieve better performance. Similar observations can
be drawn from the other tasks.
5.5 Parameter Sensitivity
5.5.1 Effect on Parameters nc and ns . We evaluate the effects of different numbers of the cooccurrence and heterogeneous source instances on the Text-Image dataset. We use linear kernel
method to conduct this experiment. The number of co-occurrence data nc is set to be 400, 600, 800,
1,000, and 1,600, respectively. The number of text source data ns is set to be 20, 40, 60, 80, 100, 200,
300, 400, 600, and 1,200, respectively. We vary one of them in the range space while fix the other
and test every combination of nc and ns by conducting 5 × 10 = 50 groups of experiments, in each
of which the average error rate over all 45 tasks are presented in Figure 7. Two observations can be
drawn from this experiment. First, for a fixed value of nc (ns ), a larger value of ns (nc ) is beneficial
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
26:16 H. Wu et al.
Fig. 6. Time cost of a representative task on the Text-Image dataset.
Fig. 7. Average error rates over all tasks on the Text-Image dataset with different values of ns and nc .
to achieve better performance. Second, in general, varying values of nc make larger effect on the
performance compared with different values of ns .
5.5.2 Effect on the Parameter σ. For the Gaussian kernel method in our proposed model, i.e.,
OHKT-k, we study the effect of the bandwidth parameter σ. Specifically, we take all the learning
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
Online Heterogeneous Transfer Learning by Knowledge Transition 26:17
Fig. 8. Average error rates over all tasks on the Text-Image dataset with different values of σ.
tasks on the Text-Image data set as examples, and compare OHKT-k with PA-k and SCW-k. We
set σ ∈ {20,..., 26}. Figure 8 shows the results of average error rates over all tasks with different
values of σ. We observe that as σ increases, the average error rate of OHKT-k decreases and then
increases. Moreover, OHKT-k performs better than PA-k and SCW-k when σ ≤ 22, and obtains
best performance when σ = 2.
6 CONCLUSION
In this article, we study the problem of online heterogeneous transfer learning (OHTL), in which
some labeled source data and unlabeled co-occurrence data are given in advance to assist the
learning task on the target instances arriving in an online fashion. We connect the heterogeneous
source and target domains using auxiliary co-occurrence data as an intermediate bridge, so that
the knowledge extracted from the source domain can be transited to the target domain through the
intermediate data. To efficiently handle the target data sequence, we design a new online learning model that introduces the co-occurrence data with pseudo labels as a regularization term.
We propose an online algorithm to solve it and obtain a closed-form solution. We develop two
methods involved linear kernel and Gaussian kernel, and conduct extensive experiments on realworld datasets. We compare our proposed OHKT algorithm to several state-of-the-art algorithms.
Experimental results validate the effectiveness and efficiency of the proposed model and further
demonstrate that our proposed model enhances the learning performance of online heterogeneous
transfer learning.
It is obvious that the experiments on the Text-Image dataset can be conducted in two directions,
i.e., text to image and image to text. Apparently, the learning performance on text data is much
better, which is the basis of knowledge transfer to improve the learning performance on image
data. Thus, the text-to-image tasks are much easier than the image-to-text tasks. However, the
learning on image data is much harder; brute force applying the predictions on image data to cooccurrence data using multi-view assumption may lead to negative transfer. Thus, we remain the
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 26. Publication date: May 2019.
26:18 H. Wu et al.
image-to-text tasks as our future work. In addition, due to the two regularization terms in the
proposed model, it is difficult to determine the upper bound of the loss value, and we will work
on this in the future. Besides, for the online learning procedure, the confidence-weighted strategy
could also benefit the proposed model. However, such strategy assumes a Gaussian distribution
with two additional parameters. Introducing OHKT with confidence-weighted strategy can lead
to a ternary first-order problem, which is complex to solve. Thus, we reserve this solution as our
future work.