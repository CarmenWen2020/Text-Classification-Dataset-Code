Hashing techniques have recently gained increasing research interest in multimedia studies. Most existing
hashing methods only employ single features for hash code learning. Multiview data with each view corresponding to a type of feature generally provides more comprehensive information. How to efficiently integrate multiple views for learning compact hash codes still remains challenging. In this article, we propose
a novel unsupervised hashing method, dubbed multiview discrete hashing (MvDH), by effectively exploring
multiview data. Specifically, MvDH performs matrix factorization to generate the hash codes as the latent
representations shared by multiple views, during which spectral clustering is performed simultaneously. The
joint learning of hash codes and cluster labels enables that MvDH can generate more discriminative hash
codes, which are optimal for classification. An efficient alternating algorithm is developed to solve the proposed optimization problem with guaranteed convergence and low computational complexity. The binary
codes are optimized via the discrete cyclic coordinate descent (DCC) method to reduce the quantization errors. Extensive experimental results on three large-scale benchmark datasets demonstrate the superiority of
the proposed method over several state-of-the-art methods in terms of both accuracy and scalability.
CCS Concepts: • Computer systems organization → Embedded systems; Redundancy; Robotics; • Networks → Network reliability;
Additional Key Words and Phrases: Hashing, multi-view, multimedia search
1 INTRODUCTION
Hashing techniques have been widely applied in large-scale multimedia applications, such as multimedia search (Wang and Hua 2011), classification (Shen et al. 2017b), and near-duplicate retrieval
(Song et al. 2013a; Zhou et al. 2017), due to the significant advantages in terms of computation and
storage (Wang et al. 2014). Hashing (Wang et al. 2014) aims to map the original data, such as images and texts, into a low-dimensional Hamming space, where the binary codes with similarity
preservation are used for the effective similarity search.
Locality-Sensitive Hashing (LSH) (Gionis et al. 1999) is one of the most famous hashing methods.
LSH generates hash functions using random projections, which embed similar data into similar
binary codes with high probabilities. LSH is independent from data, and often requires long hash
codes to achieve high performance. Later, many data-dependent methods (Gong et al. 2013; Liu et al.
2014, 2011; Shen et al. 2015, 2013; Wang et al. 2012; Weiss et al. 2009) were proposed to leverage
machine-learning techniques to produce more compact binary codes. This category includes some
representative works, such as Spectral Hashing (SH) (Weiss et al. 2009), PCA hashing (PCAH)
(Wang et al. 2012), Anchor Graph Hashing (AGH) (Liu et al. 2011), Iterative Quantization (ITQ)
(Gong et al. 2013), Discrete Graph Hashing (Liu et al. 2014), and Supervised Discrete Hashing
(Shen et al. 2015). Nevertheless, the foregoing methods can only learn hash codes from data with a
single feature. Generally speaking, these hashing methods cannot directly deal with data described
with different features.
The objects in the multimedia applications are often characterized by multiple features (Xu et al.
2015). For example, in image search, each image can be described by different kinds of features,
such as SIFT feature, RGB feature, or texture feature. This kind of data is referred to as multiview
data1 (Xu et al. 2015), and each view corresponds to a type of feature, reflecting some specific
characteristics of the data. Multiview data generally offers more comprehensive information than
single-view data, and thus learning on multiview data has received increasing attention (Liu et al.
2015b; Luo et al. 2015; Xie et al. 2017; Xu et al. 2015; Zhai et al. 2012). Much of the literature (Xie
et al. 2017; Xu et al. 2015) has shown that fusing multiple views usually has better performance
than using only a single view. Consequently, it is desirable to use multiple views for learning more
compact hash codes.
Recently, some efforts (Kim et al. 2012; Liu et al. 2015, 2014; Song et al. 2013a; Zhang et al. 2011)
have been made to yield hash codes from multiview data. Some representative works include Composite Hashing with Multiple Information Sources (CHMIS) (Zhang et al. 2011), Multiple Feature
Hashing (MFH) (Song et al. 2013a), Multiple Feature Kernel Hashing (MFKH) (Liu et al. 2014), Multiview Spectral Hashing (SU-MVSH) (Kim et al. 2012), and Multiview Alignment Hashing (MAH)
(Liu et al. 2015). However, these methods may have one or two of the following limitations. First,
they directly solve a simple problem by dropping the binary constraint, and threshold the continuous features to obtain the hash codes. Nevertheless, some research (Liu et al. 2014; Shen et al. 2015)
has shown that the simple scheme would induce a large quantization error, leading to low-quality
hash codes. Second, they have quadratic or cubic training time complexity, which is obviously high
if the size of the dataset is large. It is infeasible for them to be applied to the large-scale applications. Therefore, how to generate hash codes effectively and efficiently for large-scale multiview
data is still a challenging research topic.
Based on the above considerations, in this article, we propose an unsupervised multiview hashing method, termed Multiview Discrete Hashing (MvDH), to address the foregoing problems.
MvDH aims to learn the compact hash codes by exploring the complementarity of multiple views,
1Here “view” can be replaced by “feature” and “modality.”
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.
Multiview Discrete Hashing for Scalable Multimedia Search 53:3
Fig. 1. Illustration of the proposed Multiview Discrete Hashing (MvDH). MvDH embeds the multiview data
{X(m)
}M
m=1 into compact hash codes B as latent representations in terms of an underlying basis set U. The
weights {θm }
M
m=1 that can be adaptively learned are imposed on each view to reflect its relative importance
in the learning process. MvDH performs the nonnegative spectral analysis to obtain the cluster labels F, then
enforces the consistency between generated hash codes and cluster labels.
which also should be consistent with the cluster labels generated by nonnegative spectral analysis.
The proposed MvDH is illustrated in Figure 1. It is worthwhile to highlight the main contributions
of this work:
• We propose a novel unsupervised hashing method to learn compact hash codes from multiview data. The proposed method jointly learns the hash codes and cluster labels via factorization techniques and spectral analysis, respectively, such that the hash codes are consistent with cluster labels. The joint learning ensures that the generated hash codes not only
reflect the underlying semantics from multiple views but also enjoy high discrimination.
• We develop an efficient alternating algorithm to optimize the proposed model. We optimize
the binary codes via the discrete cyclic coordinate descent (DCC) method, where each bit
admits a closed-form solution. The convergence of the proposed algorithm is strictly guaranteed. The proposed method has low computational complexity and is suitable for the
large-scale multimedia search.
• We evaluate the proposed method on CIFAR-10, Caltech-256, and NUS-WIDE datasets.
The experiments demonstrate that the proposed method outperforms some state-of-theart methods in terms of accuracy and efficiency.
This article is an extended version of our previous conference paper (Shen et al. 2015). Compared with the previous version, substantial contributions include the newly developed model that
jointly learns hash codes and cluster labels, a detailed analysis of the computational complexity
and convergence, and more experimental results on the large-scale datasets.
The organization of the remaining part is given as follows. Section 2 reviews some related research. Section 3 introduces the proposed method. Section 4 presents extensive experimental evaluations. Finally, Section 5 concludes this article.
2 RELATED WORK
In this section, we briefly review some related hashing methods.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.
53:4 X. Shen et al.
2.1 Hashing on Single-View Data
Single-view hashing generates hash codes from single-view data. It includes data-independent and
data-dependent categories. LSH (Gionis et al. 1999; Kulis and Grauman 2012) is one of the most
famous data-independent hashing methods.
Many data-dependent hashing methods (Gong et al. 2013; Liu et al. 2014, 2011, 2015; Shen
et al. 2015, 2013, 2017a; Wang et al. 2012, 2014; Weiss et al. 2009) have been proposed to leverage machine-learning techniques to produce more effective hash codes. SH (Weiss et al. 2009) is
one of the representative works in the category. SH explores the data distribution and requires the
codes to be balanced and uncorrelated. AGH (Liu et al. 2011) uses anchor graphs for hash code
learning. Recent research attention has been paid to discrete hashing, which can reduce the quantization error. Discrete Graph Hashing (DGH) (Liu et al. 2014) and Supervised Discrete Hashing
(SDH) (Shen et al. 2015) have demonstrated significant performance gains. By jointly learning with
some loss functions, such as 2 loss and hinge loss (Gu et al. 2017), DGH can generate the optimal
binary hash codes.
2.2 Hashing on Multi-View Data
Recently some hashing methods have also been applied to multiview data. We divide these methods
into the following two categories.
The first category is cross-view hashing, which is developed for cross-view retrieval (Tian and
Chen 2017; Wei et al. 2016). For example, in the popular image-text retrieval application, one is
able to find the most relevant images given a text document or find the words that best describe an
image. Another example is the cross-heterogeneous age estimation (Tian and Chen 2017), which
establishes a single age estimator through correlation representation learning. Until now many
cross-view hashing methods (Bronstein et al. 2010; Ding et al. 2014; Kumar and Udupa 2011; Shen
et al. 2017; Song et al. 2013b; Zhu et al. 2013) have been proposed. To name a few, Bronstein
et al. (2010) proposed cross-modality search hashing (CMSSH) to learn hash functions on multiple
views. Spectral hashing is applied to the multiview setting by Kumar and Udupa (2011). Intermedia
Hashing (IMH) (Song et al. 2013b) learns a hash function by preserving both intraview similarity
and interview consistency. Collective Matrix Factorization Hashing (CMFH) (Ding et al. 2014) uses
the matrix factorization model to yield the unified binary codes.
The second category is multiview hashing (Kim et al. 2012; Liu et al. 2015, 2014, 2016, 2015a;
Shen et al. 2015; Song et al. 2013a; Wang et al. 2015; Zhang et al. 2011), which generates integrated
compact binary codes from multiple views. Note that the proposed MvDH belongs to this category.
To our knowledge, the first work in this area is CHMIS (Zhang et al. 2011). CHMIS establishes
a graph for each view and then combines them to learn the linear hash function. MFH (Song
et al. 2013a) learns a set of hash functions by considering both the within-view local structure and
the cross-view alignment. Later on, MFKH (Liu et al. 2014) generates hash codes via a multiple
kernel learning technique, which assigns different weights to combine multiple views. Kim et al.
(2012) propose Multi-View Spectral Hashing (SU-MVSH). SU-MVSH first obtains the α-averaged
similarity matrix, and then learns the hash function via a sequential manner. Liu et al. (2015a) first
present a novel multiview complementary hash table method that learns complementary hash
tables from the data with multiple views. Liu et al. (2016) propose a novel and generic approach to
building multiple hash tables with multiple views and generating fine-grained ranking results at
bitwise and tablewise levels. MAH (Liu et al. 2015) used the regularized kernel nonnegative matrix
factorization to fuse multiple views. However, one drawback is that these methods directly discard
the discrete constraints and then solve a relaxed problem in the continuous space. Studies (Liu
et al. 2014; Shen et al. 2015) have shown that such simple optimization will reduce the quality of
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018. 
Multiview Discrete Hashing for Scalable Multimedia Search 53:5
Table 1. Important Notations Used in This Article
Notation Description
X(m) data matrix of the mth view
U(m) basis matrix of the mth view
B latent hash code matrix
F cluster label matrix
P projection matrix
L graph Laplacian matrix
Z truncated similarity matrix in anchor graph
θm learning weight of the mth view
dm dimensionality of the mth view
M the number of views
N the number of objects
T the number of anchors
l the number of clusters
c the length of hash code
the binary codes. Another drawback is that most of these methods suffer from high computational
complexity. For example, the training complexities of CHMIS and MFH are around O(N3), as they
do eigenvalue decomposition of a matrix of N × N size; MAH requires around O(N2) for updating
the parameters, where N is the size of the dataset. Basically, it is infeasible to apply them to the
large-scale applications.
3 MULTIVIEW DISCRETE HASHING
3.1 Notations and Problem Statement
Throughout this article, we write matrices as boldface uppercase letters and vectors as boldface
lowercase letters. Given a matrix M = (mij), its ith row and jth column are denoted as mi · and
m·j , respectively. The important notations in this article are summarized in Table 1.
Suppose that O = {oi}N
i=1 is a set of objects, and we are given its corresponding features
{X(m) = [x(m)
·1 ,..., x(m)
·N ] ∈ Rdm×N }M
m=1, where dm is the dimension of the mth view, M is the
number of views, and N is the number of objects. We also denote the latent hash code matrix
B = [b·1,..., b·N ] ∈ {−1, 1}
c×N , where b·i ∈ {−1, 1}
c×1 is the hash code associated with oi , and c
is the code length. MvDH aims to learn B to preserve the similarity structure of the objects.
3.2 Formulation
1) Latent hash coding scheme: Multiview data are composed of different views, which often provide
information complementary to each other. To integrate information from multiple views, we aim
to learn latent hash codes that can exploit the consistent information from multiview data. We
use the matrix factorization technique (Singh and Gordon 2008) to generate the hash codes. It
decomposes multiview data into compact integrated binary codes, which are composed of latent
factors shared by multiple views.
Accordingly, we first define the reconstruction error of the ith object during matrix factorization:

M
m=1
θm


x(m)
·i − U(m)
b·i



2
2 , (1)
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.       
53:6 X. Shen et al.
where U(m) ∈ Rdm×c is the basis set in the mth view representing a set of semantic concepts, b·i is
the binary code of the ith object, and θm is the variable for weighting the reconstruction error of
the mth view.
By considering the total reconstruction errors of all the training objects, we have the following
minimization problem in a matrix form:
min
θm,U(m)
,B

M
m=1
θm


X(m) − U(m)
B


2
F (2)
s.t. B ∈ {−1, 1}
c×N ,
and
M
m=1
θm = 1, θm ≥ 0,m = 1,..., M,
where B = [b·1, b·2,..., b·N ] is the target binary code matrix.
2) Discrimination via nonnegative spectral clustering: Existing research (Shen et al. 2015) proposes
to incorporate classifier design, such as the least squares classifier (Gu et al. 2015) or support vector
machine (Gu and Sheng 2017), into hash learning and shows that the discrimination of hash codes
can be greatly improved. In essence, the above method is applied to the supervised scenario, but
our setting is unsupervised, where class labels are not available. Spectral clustering is a powerful
technique to discover the clusters of data, and has already received significant research attention
(Ng et al. 2001). This motivates us to use spectral clustering to learn the pseudo class labels, which
are employed to guide the process of inferring hash codes.
Similar to Shen et al. (2015), we assume that the generated hash codes should be consistent
with the pseudo class label. Then, we further assume that there exists a linear transformation
between them. Specifically, we propose to learn the scaled cluster indicator matrix F ∈ Rl×N and
the transformation matrix P ∈ Rc×l simultaneously. The joint learning problem can be formulated
as
min
F,P
PB − F2
F + βJ (F) (3)
s.t. FF = I, F ≥ 0,
where J (F) is the spectral clustering criterion and β is a regularization parameter for J (F). Note
that the explicit nonnegative constraint is imposed on the cluster indicator matrix to ensure that
the cluster label is nonnegative (Yang et al. 2011). Following many existing works (Belkin and
Niyogi 2001; Ng et al. 2001), J (F) can be defined as
J (F) = Tr
FLF
, (4)
where L ∈ RN ×N is the normalized graph Laplcaian matrix computed according to the data local
structure using different strategies. A common strategy is to define the kNN graph, which requires
the time complexity of O(dN2) (d is the dimensionality of the features, N the number of samples).
However, the complexity is very high for the large-scale applications. In this work, we adopt the
recently proposed anchor graph (Liu et al. 2010) due to its computation efficiency and powerful
approximation to the true kNN graph.
The basic idea of the anchor graph is to use a small set of samples called anchors to approximate
the neighborhood structure. We first concatenate all the views of the training data as {xi}N
i=1, and
then use them to generate T (T  N) anchor points {μt }
T
t=1 by scalable K-means clustering (Chen
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.         
Multiview Discrete Hashing for Scalable Multimedia Search 53:7
and Cai 2011). The truncated similarity matrix Z ∈ RN ×T between training data and anchors can
be defined as follows:
Zij = exp
D2 (xi, μj)/σ


j ∈[i] exp
D2 (xi, μj)/σ
 , (5)
where [i] denotes the indices of s (s  T ) nearest anchors of xi , D2 (xi, μj) = xi − μj 2
2 , and σ is
the predefined bandwidth parameter. Thus, each row in Z contains only s nonzero entries, which
sum to 1. With the help of Z, the anchor graph defines the similarity matrix as
S = ZΛ−1
Z, (6)
where Λ = diag 
Z1
 ∈ RT ×T . The graph Laplacian can be calculated as
L = I − S = I − ZΛ−1
Z. (7)
By combining Equations (3), (4), and (7), we have
min
F,P
PB − F2
F + βTr(FLF) (8)
s.t. FF = I, F ≥ 0.
3) Overall objective function: To this end, combining Equations (2) and (8), we have the final
objective function
min
θm,U(m)
,
B,P,F

M
m=1
θm


X(m) − U(m)
B


2
F
+ α PB − F2
F
+ βTr
FLF (9)
s.t. B ∈ {−1, 1}
c×N , and FF = I, F ≥ 0,
and
M
m=1
θm = 1, θm ≥ 0,m = 1,..., M,
where α is a regularization parameter for the linear regression term. Our final objective function is
formulated to jointly learn hash codes and cluster labels via matrix factorization and nonnegative
spectral analysis, respectively, such that the obtained hash codes are also optimal for classification.
3.3 Optimization
Equation (9) is nonconvex, and thus hard to minimize. In this section, we alternatively solve the
subproblems with respect to each variable.
1) Update U(m)
: By dropping some irrelevant terms to U(m)
, we have
min
U(m)



X(m) − U(m)
B


2
F
+ δ


U(m)


2
F . (10)
Here δ U(m)
2
F is added as a regularization term to avoid the overfitting of U(m)
, and δ is a regularization parameter, which is set to 0.01 in this work. Clearly, it is a regularized least squares
minimization problem. Let the derivative of Equation (10) with respect to U(m) equal 0, and then
we have
U(m) = X(m)
B(BB + δI)
−1
. (11)
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.                       
53:8 X. Shen et al.
2) Update B: The subproblem with respect to B is defined as
min
B
X − UB2
F + α PB − F2
F
s.t. B ∈ {−1, 1}
c×N , (12)
where U = [
√
θ1U(1),..., √
θM U(M)]
 ∈ RD×c , X = [
√
θ1X(1),..., √
θM X(M)]
 ∈ RD×N , and
D = M
m=1 dm. Note that Equation (12) is NP-hard for the binary constraint. Inspired by the recent discrete optimization (Shen et al. 2015), we generate the hash codes B using the discrete cyclic
coordinate descent (DCC) method. We first give a lemma about optimization of binary code.
Lemma 3.1. For the binary optimization problem of b: minb hb with the binary constraint
b ∈ {−1, 1}N ×1, h ∈ RN ×1 is the constant vector, and we have the closed-form solution of b as
b = −sign(h), where sign(·) is the sign function.
The proof is easy to obtain. Let bi · ∈ {−1, 1}
1×N denote the ith row of B, which represents the
ith bit of B. Based on the above lemma, we have the following theorem for the solution of bi .
Theorem 3.2. In the binary optimization of B, with the other c − 1 rows fixed, the closed-form
solution of bi ·, i.e., the ith row of B is obtained as follows:
bi · = sign
g
·i (Q − G¯ B¯)

, (13)
where Q = [X,
√
αF]
, G = [U,
√
αP]; B¯ ∈ {−1, 1}(c−1)×N denotes the other rows in B excluding
bi ·; g·i ∈ R(D+l)×1 is the ith column of G; and G¯ ∈ R(D+l)×(c−1) denotes the other columns in G excluding g·i .
Proof. The proof can be found in Appendix A.
3) Update P: Let us move to solve the projection matrix P. We obtain the following objective
function with respect to P:
min
P
PB − F2
F + δ P2
F . (14)
Similar to Equation (10), a regularization term with respect to P is added, and δ is a regularization
parameter. We can easily obtain the optimal solution of P as
P = (BB + δI)
−1
BF. (15)
4) Update F: The optimization problem with respect to F is defined as
min
F
α PB − F2
F + βTr(FLF) (16)
s.t. FF = I, F ≥ 0.
By relaxing the orthogonal constraint, we rewrite the above objective function as
min
F
Tr(FLF) +
α
β PB − F2
F + γ
2β FF − I2
F (17)
s.t. F ≥ 0,
where γ > 0 is a parameter to control the orthogonality condition. In practice, γ should be large
enough to ensure the orthogonality, which is set to 106 in this work. The multiplicative updating
rule can be introduced to solve the above problem. We have the following theorem about the
updating rule.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.  
Multiview Discrete Hashing for Scalable Multimedia Search 53:9
Theorem 3.3. For the above optimization problem, we can update F by using the following updating rule:
Fij ← Fij
 γ
β F

ij

FM + γ
β FFF

ij
(18)
to reach the local optimal solution of F, where
M = L +
α
β

I − B(BB + δI)
−1
B

. (19)
Proof. The proof can be found in Appendix B.
5) Update θ: Finally, we focus on solving the weight of each view θm in the learning process.
The objective function in Equation (9) can be reduced to
min
θ

M
m=1
θm


X(m) − U(m)
B


2
F
+ λ θ 2
2 (20)
s.t.
M
m=1
θm = 1, θm ≥ 0,m = 1,..., M,
where θ = [θ1, θ2,..., θM ]
. In Equation (20), θ 2
F is used to discover the complementarity among
all views, and λ > 0 controls the smoothness of θ. This subproblem can be effectively solved via the
quadratic programming solvers, among which the coordinate descent algorithm (Bertsekas 1999)
is adopted in this work. In each iteration, the ith and jth elements in θ, i.e., θi , θj , are selected to
be updated while the others are fixed. Using the Lagrangian method for the sum constraint, we
obtain the following updating rule:
⎧⎪
⎨
⎪
⎩
θ ∗
i = λ(θi+θj )+(hj−hi )
2λ
θ ∗
j = θi + θj − θ ∗
i , (21)
where hi = X(i) − U(i)
B2
F . By further taking the nonnegative constraint into consideration, we
have

θ ∗
i = 0, θ ∗
j = θi + θj, if λ(θi + θj) + (hj − hi ) ≤ 0
θ ∗
j = 0, θ ∗
i = θi + θj, if λ(θi + θj) + (hi − hj) ≤ 0. (22)
The flow chart of MvDH is illustrated in Algorithm 1. Following the optimization procedure, we alternatively update one variable while fixing other variables until the objective function converges.
In this work, the iterations will end when the difference of the objective function values is less than
a small threshold or the number of iterations reaches a maximum value.
3.4 Out-of-Sample Extension
After the hash codes of the training set are learned, we next address the out-of-sample extension
of MvDH. It is impractical to retrain MvDH for obtaining hash codes of the unseen samples. Thus,
it is desirable to learn the hash code of a new sample online.
Given a new sample, i.e., {x(m)
q }M
m=1, we focus on minimizing the following optimization problem:
min
b

M
m=1
θm


x(m)
q − U(m)
b


2
F (23)
s.t. b ∈ {−1, 1}
c×1
.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.                   
53:10 X. Shen et al.
ALGORITHM 1: Multiview Discrete Hashing
Input: Training set {X(m) ∈ Rdm×N }M
m=1; code length c; cluster number l; parameters α, β, γ .
Output: binary codes B.
1: Initialize B as {−1, 1}
c×N randomly;
2: Initialize F via Landmark-based Spectral Clustering (LSC) (Chen and Cai 2011);
3: Initialize θ = [1/M,..., 1/M]
;
4: repeat
5: Update U(m) using Equation (11);
6: Update P via Equation (15);
7: Update B bit by bit via Equation (13);
8: Update F via Equation (18);
9: Update θ using Equations (21) and (22);
10: until convergence
Basically, the above problem can be seen as the binary code learning subproblem in Equation (12).
Accordingly, we can also apply the discrete optimization algorithm to obtain the binary code. The
updating rule for the ith bit of b, i.e., bi , is as follows:
bi = sign
u
·i (xq − U¯¯
b)

, (24)
where xq = [
√
θ1x(1) q ,..., √
θM x(M) q ]
, U = [
√
θ1U(1),..., √
θM U(M)]
, u·i is the ith column of
U, U¯ denotes other columns in U except u·i , and θm is the optimal weight obtained by Algorithm 1.
3.5 Convergence and Computational Complexity Analysis
The convergence property of discrete optimization on hash codes is presented as follows.
Lemma 3.4. When fixing other variables, the sequence of binary bits {b·i}
c
i=1 generated by the
discrete optimization, i.e., Equation (13), monotonically decreases the objective function value of the
hash code subproblem.
Proof. The proof is easy to obtain. In each step, the solution of b·i optimized via Equation (13) can always lead to a lower (or equal) objective function value of Equation (12). Therefore,
the sequence of binary bits monotonically decreases the objective function value of the hash code
subproblem.
Based on the above lemma, the convergence of MvDH is given as follows.
Theorem 3.5. The alternate updating rules in Algorithm 1 monotonically decrease the objective
function value of Equation (9) in each iteration, and Algorithm 1 will converge to a local minimum
of Equation (9).
Proof. The proof can be referred to Appendix C.
Furthermore, in the experimental section, we will empirically show that the proposed method
converges very fast in the real-world applications.
The training complexity of MvDH has the following parts. For simplicity, we assume that N  l,
N  T , N  dm, and dm > c (m = 1,..., M) in the large-scale applications. It takes O(cDN) for
updating U, where D = M
m=1 dm. Updating each bit of B requires O ((D + l + c)cN); accordingly, it
takes O ((D + l + c)cNT1) for updating B, where T1 is the number of iterations. Besides, the complexity of updating P is O (d(d + c)N). In the step of updating F, we do not precompute L, and
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.   
Multiview Discrete Hashing for Scalable Multimedia Search 53:11
Table 2. Statistics of Three Benchmark Datasets
Datasets # Dataset # Training # DataBase # Query # Dim. of Each View
CIFAR-10 60,000 59,000 59,000 1,000 384/300
Caltech-256 28,780 27,780 27,780 1,000 512/500
NUS-WIDE 196,776 194,809 194,809 1,967 228/125
instead use Z in Equation (7) to reduce the computation cost. By changing the order of calculation, updating F requires O((lT + lc + c2 + l
2)N). Updating θ requires O(M3 + DcN). The outer
iteration converges less than 10 times in our experiments. For the query stage, it takes O(DcT1) to
generate the hash code of a new query. The above analysis reveals that the computational complexity of MvDH is linear with the size of the dataset, and thus suitable for the large-scale multimedia
retrieval.
4 EXPERIMENTS
This section compares the proposed MvDH with some state-of-the-art methods on three benchmark datasets, and evaluates their performance in large-scale multimedia retrieval applications.
4.1 Datasets
We adopt three benchmark datasets, i.e., CIFAR-10 (Krizhevsky 2009), Caltech-256 (Griffin et al.
2007), and NUS-WIDE (Chua et al. 2009). Without loss of generality, similar to existing works (Kim
et al. 2012; Liu et al. 2014; Song et al. 2013a; Zhang et al. 2011), we adopt two visual features for
each dataset to construct the multiview data. The statistics of the three datasets are summarized
in Table 2, and the detailed descriptions are as follows:
• CIFAR-102: The CIFAR-10 dataset consists of sixty thousand 32 × 32 color images of 10
classes and 6,000 images in each class. For each image, we extract the 384-dimensional Gist
vector and 300-dimensional vector quantized from dense Sift features as the global and local
feature, respectively. Here we randomly sample 1,000 samples as the query set, and use the
remaining 59,000 samples for training.
• Caltech-2563: The Caltech-256 dataset contains a total of 29,780 images of 256 object categories with more than 80 images per category. Similar to the CIFAR-10 dataset, the 512-
dimensional Gist vector and 500-dimensional Sift vector are extracted for each image as two
views. We take 1,000 samples and the rest as the query set and the training set, respectively.
• NUS-WIDE4: The NUS-WIDE dataset consists of 269,648 images from 81 ground-truth concepts with a total number of 5,018 unique tags. Only the top 25 most frequent labels and
the corresponding 196,776 annotated samples are kept. As the multiple visual features have
been already provided, we arbitrarily select two features: 128-dimensional wavelet texture
and 255-dimensional block-wise color moments. We randomly select 1% of the dataset and
the remaining 99% as the query set and the training set, respectively.
4.2 Experimental Setting
To validate the effectiveness of the proposed MvDH, we compare it with various state-of-theart methods, which belong to two categories: (1) single-view hashing, including SH (Weiss et al.
2http://www.cs.toronto.edu/∼kriz/cifar.html. 3http://authors.library.caltech.edu/7694/. 4http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.
53:12 X. Shen et al.
Table 3. Mean Average Precision Comparison
with Respect to Different Number of Bits
on CIFAR-10 Dataset
Method 16 32 64 128
SH 0.1271 0.1236 0.1214 0.1218
AGH 0.1332 0.1315 0.1250 0.1213
ITQ 0.1473 0.1508 0.1539 0.1576
CHMIS 0.1335 0.1278 0.1239 0.1183
MFH 0.1338 0.1288 0.1236 0.1186
MFKH 0.1449 0.1437 0.1394 0.1334
SU-MVSH 0.1479 0.1455 0.1455 0.1478
MAH 0.1572 0.1616 0.1652 0.1677
MvDH 0.1724 0.1782 0.1864 0.1915
2009), AGH (Liu et al. 2011), and ITQ (Gong et al. 2013), and (2) multiview hashing, including
CHMIS (Zhang et al. 2011), MFH (Song et al. 2013a), MFKH (Liu et al. 2014), SU-MVSH (Kim et al.
2012), and MAH (Liu et al. 2015). Multiple features are first concatenated in single-view hashing
methods. It is infeasible to train CHMIS, MFH, and MAH on the whole CIFAR-10 and NUS-WIDE,
and a subset of 30,000 samples is used to train the three methods. Source codes of SH, AGH, CHMIS,
MFH, and MFKH are publicly available, and the other methods are implemented by ourselves.
In MvDH, γ and l are empirically set as 106 and 50, respectively; similar to Liu et al. (2011),
s and T in the anchor graph are simply set as 5 and 100, respectively; α and β range from
[10−3, 10−1, 101, 103, 105] and finally chosen by the cross-validation technique. In AGH and MFKH,
the number of landmarks is set as 1,000. We carefully tune the parameters of all the comparisons
and report the best results.
To evaluate the retrieval performance, two criteria, i.e., mean Average Precision (mAP) and
Precision-Recall Curve, are adopted. Given a query q, average precision (AP) is defined as
AP (q) = 1
Lq

R
r=1
Pq (r)δq (r), (25)
where Lq is the number of the true neighbors in the retrieved list, Pq (r) denotes the precision of the
top r retrieved results, and δq (r) = 1 if the rth result is the true neighbor and 0 otherwise. For all
the queries, we first calculate their APs and then use the average value as mAP. In the experiment,
we set R as the number of objects in the database.
4.3 Performance Evaluation
This section evaluates the performance of MvDH by comparing it with eight state-of-the-art methods on the three datasets. The mAP results of all the methods on CIFAR-10, Caltech-256, and NUSWIDE are reported in Tables 3, 4, and 5, respectively. In addition, the Precision-Recall Curves with
different code lengths, i.e., 16, 32, 64, and 128 bits, are also shown in Figures 2 to 4. From these
results, we find the following interesting observations:
• The proposed MvDH outperforms all the comparisons on all the datasets. Among 12 cases
from Tables 3 to 5, MvDH has the best mAP results 11 times (except the case of 16=bit on
Caltech-256). Similar results can be also observed from Figures 2 to 4. From the figures,
we clearly observe that the Precision-Recall Curves of MvDH are consistently above the
others, resulting in the large Area Under the Curve (AUC), which is an important criterion
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018. 
Multiview Discrete Hashing for Scalable Multimedia Search 53:13
Table 4. Mean Average Precision Comparison
with Respect to Different Number of Bits
on Caltech-256 Dataset
Method 16 32 64 128
SH 0.0243 0.0323 0.0374 0.0402
AGH 0.0368 0.0443 0.0497 0.0460
ITQ 0.0253 0.0431 0.0574 0.0680
CHMIS 0.0356 0.0391 0.0394 0.0381
MFH 0.0441 0.0428 0.0428 0.0385
MFKH 0.0309 0.0414 0.0425 0.0438
SU-MVSH 0.0204 0.0334 0.0543 0.0614
MAH 0.0291 0.0375 0.0441 0.0552
MvDH 0.0412 0.0639 0.0719 0.0802
Table 5. Mean Average Precision Comparison
with Respect to Different Number of Bits
on NUS-WIDE Dataset
Method 16 32 64 128
SH 0.3496 0.3404 0.3368 0.3357
AGH 0.3442 0.3398 0.3336 0.3294
ITQ 0.3466 0.3499 0.3527 0.3547
CHMIS 0.3542 0.3461 0.3382 0.3327
MFH 0.3545 0.3460 0.3373 0.3321
MFKH 0.3420 0.3394 0.3344 0.3305
SU-MVSH 0.3559 0.3650 0.3623 0.3736
MAH 0.3624 0.3700 0.3750 0.3822
MvDH 0.3712 0.3863 0.3940 0.3950
Fig. 2. Precision-recall curves on CIFAR-10 dataset with respect to different number of bits: (a) 16-bit,
(b) 32-bit, (c) 64-bit, and (d) 128-bit.
in information retrieval. These results clearly validate the superiorities of MvDH over some
state-of-the-art methods in large-scale multimedia retrieval.
• It is interesting to note that the performance of MvDH improves as the code length increases, which indicates that more discrimination can effectively be exploited with longer
codes. In contrast, some comparisons, e.g., AGH, CHMIS, MFKH, and MFH, cannot work
well when the code length is large. This is because the orthogonality constraint in these
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.
53:14 X. Shen et al.
Fig. 3. Precision-recall curves on Caltech-256 dataset with respect to different number of bits: (a) 16-bit,
(b) 32-bit, (c) 64-bit, and (d) 128-bit.
Fig. 4. Precision-recall curves on NUS-WIDE dataset with respect to different number of bits: (a) 16-bit,
(b) 32-bit, (c) 64-bit, and (d) 128-bit.
Fig. 5. Performance comparison between MvDH with multiple views and only single view on different
datasets.
methods results in very low variances for the latter bits, reducing the quality of the entire
hash code (Wang et al. 2012).
• Among all the multiview comparisons, MAH generally achieves the best performance, especially in the case of long codes. SU-MVSH takes second place. It has better performance
with long codes, due to its sequential learning strategy (Kim et al. 2012). MFH and CHMIS
generally have similar performances. For the three single-view methods, ITQ generally performs the best. AGH outperforms SH on CIFAR-10 and Caltech-256, while SH outperforms
AGH on NUS-WIDE. The three single-view methods have better performance than some
multiview comparisons, e.g., CHMIS and MFH, on several cases.
4.4 Comparison with MvDH Using Single View
We next compare the performances of MvDH using multiple views and single view. In the singleview setting, either of the two views is used for learning hash codes. Figure 5 shows the mAP
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.
Multiview Discrete Hashing for Scalable Multimedia Search 53:15
Fig. 6. Performance comparison between the proposed discrete optimization and the continuous relaxation
one on different datasets.
results of MvDH using two views and single view on three datasets. From Figure 5, we can clearly
see that MvDH with both views obviously outperforms either of the two views on all the cases.
MvDH can effectively fuse the complementary information among the two views and thus learn
more discriminative hash codes, leading to superior and robust performance.
4.5 Discrete Optimization Analysis
We evaluate the effectiveness of the discrete optimization in MvDH. We consider a relaxed model,
which discards the discrete constraint in Equation (12) and then solves a least squares minimization
problem to obtain the hash codes.
The mAPs with respect to two optimization manners are plotted in Figure 6, where the proposed discrete optimization is shown to outperform the relaxed one. We can conclude that discrete optimization yields better-quality hash codes than the conventional relaxed optimization by
considering discrete nature.
4.6 Efficiency and Convergence Analysis
We conduct a comparison of computation efficiency. The experiment is conducted on a 64-bit
Linux server with 96GB memory. Table 6 lists the training and query time of all the methods with
32-bit code. Note that for CHMIS, MFH, and MAH, the training time on CIFAR-10 and NUS-WIDE
is reported on a subset of 30,000 samples, due to their large memory cost.
From Table 6, we observe the following: (1) In the single-view category, SH is the fastest, followed by ITQ. AGH is the slowest in terms of both training and query. (2) In the multiview category,
the proposed MvDH is the most efficient in training, which is followed by SU-MVSH, MFKH, and
MAH. CHMIS and MFH are very time consuming in training. MFH and SU-MVSH are the fastest
in query, followed by CHMIS, MvDH, and MFKH. MAH is the slowest in query, because computing the kernel matrix related to a new sample is time consuming in the large-scale problem. The
above results demonstrate that, compared with other multiview hashing methods, MvDH is more
efficient in training and competitive in query on large-scale multimedia retrieval applications.
We next empirically study the convergence. The convergence curves on three datasets are plotted in Figure 7, where we observe that MvDH converges very quickly and converges to a local
minimum within around 10 iterations.
4.7 Parameter Analysis
Three parameters in the proposed MvDH, i.e., regularization parameters α and β and the number
of clusters l, are empirically studied. We analyze their effects on the mAP with 32-bit code. In
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.
53:16 X. Shen et al.
Table 6. Comparison of Training and Query Time on Three Benchmark Datasets
Method
CIFAR-10 Caltech-256 NUS-WIDE
Training (s) Query (μs) Training (s) Query (μs) Training (s) Query (μs)
SH 0.66 5.30 1.26 11.76 0.84 4.80
AGH 71.36 39.12 27.72 49.37 264.59 29.98
ITQ 3.43 6.21 3.04 12.83 9.89 5.69
CHMIS 5.18×103  11.99 3.22×103 11.38 6.38×103  11.62
MFH 5.45×103  4.44 4.66×103 3.71 4.78×103  3.67
MFKH 94.81 40.74 51.17 50.02 414.19 34.31
SU-MVSH 32.69 2.90 21.27 3.97 121.33 1.47
MAH 381.91  958.15 430.01 1.17×103 355.62  623.06
MvDH 22.46 22.32 15.13 26.76 53.98 14.68
denotes the training time on a subset of 30,000 samples.
Fig. 7. Convergence analysis of the proposed MvDH on (a) CIFAR-10, (b) Caltech-256, and (c) NUS-WIDE.
Fig. 8. Parameter analysis with respect to α and β in the proposed MvDH on (a) CIFAR-10, (b) Caltech-256,
and (c) NUS-WIDE.
the experiment, α and β are varied from the range of [10−3, 10−1, 101, 103, 105], and l ranges from
[10, 20, 50, 100, 200, 500].
Figure 8 shows mAP results with varying α and β on three datasets. From Figure 8, we see
that mAP slightly changes with these two parameters. With the increase of α and β, mAP generally improves to different degrees on different datasets. Empirically, α, β = 10 is a good choice
for MvDH to obtain superior performance on these three datasets. The results demonstrate that
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.       
Multiview Discrete Hashing for Scalable Multimedia Search 53:17
Fig. 9. Parameter analysis with respect to the number of clusters l in the proposed MvDH.
the class consistency term can help improve the performance. Besides, Figure 9 shows the mAP
results with different numbers of clusters, where we see that mAP is stable with respect to l.
5 CONCLUSION
This work focused on efficiently learning compact hash code from large-scale multimedia data.
We proposed a novel unsupervised hashing method, dubbed multiview discrete hashing (MvDH),
to effectively integrate multiple views. MvDH jointly performed matrix factorization and nonnegative spectral clustering, such that the hash code could not only reflect the underlying semantics
from multiple views but also enjoy high discrimination. We developed an efficient alternating algorithm to solve the formulated problem, where the key hash code was optimized in a discrete
manner to ensure its high quality. Besides, we rigorously proved convergence of the optimization
algorithm and theoretically showed that its computational complexity was linear with the training size. MvDH was validated on three benchmark datasets, and it yielded promising accuracy and
scalability.
Recent research has shown that a high-dimensional feature is useful for some recognition tasks.
The proposed MvDH less considers the problem of learning hash codes from the data with multiple
high-dimensional views. Thus, MvDH may suffer from very high computation and storage costs
in such scenarios. In addition, the way of MvDH to use the additional label information to boost
the learning performance in the semisupervised setting deserves further research. In the future,
we will explore the extensions of MvDH to address these challenging tasks.
APPENDIXES
A PROOF OF THEOREM 3.2
Proof. Equation (12) can be rewritten into the following compact form:
Q − GB2
F = Q2
F − 2Tr
QGB
+ GB2
F (26)
= −2Tr
QGB
+ GB2
F + const,
where Q = [X,
√
αF]
, G = [U,
√
αP]
. Let bi · ∈ {−1, 1}
1×N denote the ith row of B, which
represents the ith bit of B. B¯ ∈ {−1, 1}(c−1)×N denotes the other rows in B excluding bi ·, g·i ∈
R(D+l)×1 is the ith column of G, and G¯ ∈ R(D+l)×(c−1) denotes the other columns in G excluding
g·i . Besides, we have
Tr(QGB) = Tr
Q(g·ibi · + G¯ B¯)
 (27)
= Tr
Qg·ibi ·

+ const.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.    
53:18 X. Shen et al.
Similarly,
GB2
F = g·ibi · + G¯ B¯ 2
F (28)
=
g·ibi ·


2
F + 2Tr
b
i ·
g
·iG¯ B¯

+ G¯ B¯ 2
F
= 2Tr
b
i ·
g
·iG¯ B¯

+ const.
Here g·ibi · 2
F = Tr(g·ibi ·b
i ·
g
·i ) = Ng
·ig·i = const.
Substituting Equations (27) and (28) into Equation (26), we have
min
bi·
bi ·

B¯ G¯ g·i − Qg·i
 (29)
s.t. bi · ∈ {−1, 1}
1×N .
According to Lemma 3.1, Equation (29) has a closed-form solution:
bi · = sign
g
·i (Q − G¯ B¯)

. (30)
The proof is complete.
B PROOF OF THEOREM 3.3
Proof. Substituting the solution of P into Equation (17), the optimization of F can be rewritten
as
min
F
Tr(FMF) + γ
2β FF − I2
F (31)
s.t. F ≥ 0,
where M = L + α
β (I − B(BB + δI)
−1B). Let ϕij be the Lagrange multiplier for constraint Fij ≥ 0
and Φ = [ϕij] ∈ Rl×N . The Lagrangian function L is
L = Tr(FMF) + γ
2β FF − I2
F + Tr(ΦF). (32)
Setting its derivative with respect to F to 0, we have
2
	
FM + γ
β
FFF − γ
β
F


+ Φ = 0. (33)
Using the Karush-Kuhn-Tucker (KKT) conditions, we have the complementary slackness constraint ϕij Fij = 0,∀i, j. Multiplying Fij on the corresponding entry of Equation (33), we have
(FM + FFF − F)ij Fij = 0. (34)
Similar to Nonnegative Matrix Factorization (NMF) (Lee and Seung 2001), we obtain the updating
rule
Fij ← Fij
 γ
β F

ij

FM + γ
β FFF

ij
. (35)
Therefore, Theorem 3.3 can be proved.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 53. Publication date: May 2018.            
Multiview Discrete Hashing for Scalable Multimedia Search 53:19
C PROOF OF THEOREM 3.5
Proof. Let {U, B, P, F, θ } be the solution in the th iteration generated by Algorithm 1,
and ϱ = ϱ(U, B, P, F, θ ) be the corresponding objective function value. By Algorithm 1, at
the ( + 1)-th iteration, since each subproblem is solved exactly, we have
ϱ (U, B, P, F, θ ) ≥ ϱ (U+1, B, P, F, θ ) (36)
≥ ϱ (U+1, B+1, P, F, θ )
≥ ϱ (U+1, B+1, P+1, F, θ )
≥ ϱ (U+1, B+1, P+1, F+1, θ )
≥ ϱ (U+1, B+1, P+1, F+1, θ+1) .
Thus, the sequence 
ϱ
 is monotonically decreasing. In addition, the objective function
ϱ(U, B, P, F, θ ) is lower-bounded by zero. Therefore, Algorithm 1 is guaranteed to converge
to a local minimum of MvDH.                                                 