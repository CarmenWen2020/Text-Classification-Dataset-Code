Abstract
Augmented Reality (AR) combines real-world and computer-generated data, allowing for a dynamic presentation of visual contents while moving in the physical surroundings. Though promising for the investigation of visuospatial processing in natural scenarios, experimental research exploiting AR in this context is scarce. In this study, we aimed at testing the behavioral consequences of multitasking when walking in different directions to close landmarks while responding to AR holograms outdoors. Participants were engaged in i) a visual single-task for discriminating augmented peripheral targets, ii) a navigation single-task consisting of a sequence of short goal-directed walking periods to close augmented landmarks and iii) a dual-task combining the latter tasks. We evaluated the cost of dual-tasking on cognitive and motor performance in comparison to the single-tasks, along with a subjective assessment of mental load. Cognitive-Motor Interference (CMI) was highlighted by performance costs in both the cognitive and motor domains under dual-task. Interestingly, a discrepancy between subjective and objective measures of mental load under dual-task was observed. We conclude that the attentional load induced by multitasking can have important consequences when navigating the dynamic real world, and thus needs to be addressed in a variety of daily-living contexts. In this perspective, AR is a suitable research tool for simulating dynamic tasks outdoors, enhancing the ecological validity of cognitive investigations without sacrificing the experimental rigor of laboratory research. Additionally, it provides insights into the possible impact on attention and behavior when using wearable mobile technologies that overlap virtual data to the physical environment.

Previous
Next 
Keywords
Augmented Reality

Dual task

Spatial cognition

Cognitive research

Cyber research

Abbreviations
CMI

Cognitive Motor Interference

ISI

Inter Stimulus Interval

STE

Exploration Single-Task

1. Introduction
When navigating a distracting environment, attentional capabilities are challenged by an increase in workload (Marois and Ivanoff, 2005). Dividing attention between the cognitive multisensorial exploration of the surroundings and the accompanying motor coordination may indeed cause a performance decrease in the cognitive and/or motor behavior, particularly when the executed tasks share the same resources (for a review, Yogev-Seligmann et al., 2008). This phenomenon was named Cognitive-Motor Interference (hereafter CMI, Woollacott and Shumway-Cook, 2002; Al-Yahya et al., 2011) and a common way to assess it is through dual-task paradigms. CMI likely occurs in many daily scenarios. For instance, when walking to the bus stop while paying attention to the crossing bicycles, or even at home, when walking to the kitchen by stepping over the kid's toys. The interference between cognitive and motor tasks is likely to become even more challenging with the introduction of wearable and mobile technologies in daily life, which bring additional load to the user that is concurrently performing other tasks. A typical example is given by walking and texting or reading emails (see Feld and Plummer, 2019; Lim et al., 2017; Prupetkaew et al., 2019), but similar challenges can arise when using smart glasses, smartwatches or smart sport equipment. These technologies have the potential to be integrated into everyone's life in the near future - see the manifold applications of smart glasses in the educational domain (Bacca et al., 2014; Freina and Ott, 2015), industry (Berg and Vange, 2017; Fraga-Lamas et al., 2018), and medicine (Pelargos et al., 2017) - but before that, it is fundamental to fully understand the attentional implications of receiving virtual stimuli that overlap the physical world, especially when navigating the dynamic surroundings.

From a methodological point of view, CMI has been investigated rarely in physical environments with high variability of spatial movements and more often through research tools like treadmills that reduce the demand for physical navigation. Moreover, a number of studies investigating attentional and navigation abilities were conducted via desktop computers, thereby not involving vestibular and kinesthetic modalities at all. In contrast, research that has started to avail of mobile virtual technologies such as Virtual or Augmented Reality - respectively VR and AR - is gaining impetus but has still room to grow. Using computer-generated environments like VR or AR in cognitive-motor research has manifold advantages (Dünser et al., 2006). First, researchers can program environments where the unpredictable dangers that potentially occur in the real world are not an option (i.e., suddenly approaching cars). Secondly, these systems’ portability allows researchers to conduct studies in simulated environments with freely moving agents (i.e., during active navigation), still properly controlling the experimental settings. Thirdly, the most recent virtual and augmented devices offer the possibility to automatically log data gained from both explicit and implicit participants’ behaviors. For those virtual and augmented systems that offer the combination with eye tracking, it is even possible to gain the finest information about gaze direction and eye movement in the scene (Meißner et al., 2019). As a specific feature of AR systems, the possibility of controlling visual input is provided while moving in the physical surroundings, reducing the risk of motion sickness and making the visual content more ecological. This is possible because AR is equipped with spatial understanding: thanks to computational geometry and computer-aided engineering, it maps the physical space by creating a mesh that lays over the physical environment and is frequently updated. In this way, if people or objects move around, they are tracked in real-time. These significant features make AR a powerful tool with technical advantages that can be leveraged in many physical scenarios, including applied cognitive-motor research. However, it is still unclear how the principles of attention formulated through traditional desktop experiments can fit novel interfaces like AR. Prospectively, a systematic and interdisciplinary assessment of attention- and workload-related performance when using AR in such dynamic situations will help understanding its implications and promote its deployment in many contexts of life.

In the present study, we thus tackle issues concerning attention, workload and human motion in participants walking from landmark to landmark in a physical environment using AR. Particularly, we chose to simulate a chaotic movement characterized by a series of short periods of walking and directional changes in order to test a more complex motor behavior as compared to previous research analyzing continuous walking. In this context, we analyzed the cognitive states that can lead to the misperception of virtual objects placed in the near peripheral field, as well as to the alteration of the own motor behavior. In the next subsection, we provide a short literature review both on the attentional effects given by the concurrent execution of cognitive and motor tasks and on the implications of using mobile virtual interfaces on attention. Thereafter, we present the motivations and hypotheses of the present study.

1.1. Related work
Whether attentional mechanisms are altered during motion is a relevant question that covers a wide range of everyday behaviors. Prupetkaew et al. (2019) examined the effect of cognitive, visual and gross motor demands while using a phone during gait in both a laboratory and a free-living environment. As compared to walking without additional tasks, they observed alterations of the gait pattern and velocity when walking and concurrently texting, answering questions, and even just looking at the phone in front of them. Likewise, visual scanning behavior was observed to differ between walking and texting, walking while performing a verbal fluency task and simply walking, indicating an important reduction in overt visual attention to the walking path as well as to the surroundings (Feld and Plummer, 2019). In active scenarios involving human motion, paying attention to events happening in the peripheral visual field can be extremely important. For instance, walking through a crowded urban environment requires dividing attention over different portions of the visual field. In this regard, attentional load has been shown to affect cognitive processing in the visual periphery (i.e., Webster and Haselrud, 1964; Chen and Spence, 2017; Romeo et al., 2019) and it is tracked by neurophysiological signatures such as pupil dilation (Lisi et al., 2015) and modulation of early event-related brain potentials (Bonato et al., 2015; Romeo et al., 2019). In a study measuring the ability to respond to peripheral visual stimuli presented at 10° in eccentricity during cycling, RT increased only when exceeding the ventilatory threshold during cycling at high workloads compared to rest (Ando et al., 2010). Recently, slower RT and lower accuracy were observed when discriminating between peripheral visual stimuli as compared to more central ones, both when standing and walking in VR (Nenna et al., 2020). There is also evidence for walking to increase the contrast sensitivity in the periphery compared to the center of the visual field (Cao and Handel, 2018). The same study also demonstrated a decreasing detection rate of lateralized visual stimuli for increasing walking speed. Similarly, when walking while texting and attending to peripheral visual stimuli, nearly half of the visual cues was not detected and, for the more lateralized stimuli, the performance decrease in terms of time and rate of detection was more pronounced (Lim et al., 2017). These findings show that the eccentricity effects are not only due to the neurophysiological differences between foveal and peripheral vision (Carrasco and Frieder, 1995). A central bias in the allocation of attentional resources is usually observed (Wolfe et al., 1998) both in static and physically moving participants. Therefore, multitasking activities involving the visual field periphery in parallel with active movement increase the attentional load and induce perceptual and behavioral changes that can be crucial when acting in the real world.

However, most of the studies on active behaviors involving visuospatial abilities were conducted via desktop-based tasks, thus not providing for a “real or virtual movement of subjects within large-scale environments, geometrically defined and with landmarks” (Castelli et al., 2008). Static desktop-based evaluations are valuable for studying spatial abilities because of their systematic recording of behavioral measures and highly controllable setting. However, they do not involve vestibular and kinesthetic modalities (Waller, 2000), leaving open the question of whether the cognitive mechanisms underlying spatial tasks conducted via desktop are predictive of the performance in the real environment. When assessing walking agents, a number of studies have been conducted through fixed treadmills, and only rarely during overground walking (Ladouce et al., 2019; Nenna et al., 2020; Plummer et al., 2015; Reiser et al., 2019). Treadmills are suitable tools for detailed analyses of gait kinematics during the continuous act of walking (e.g., Gwin et al., 2011; Kao, Pierro and Booras, 2018). However, they only allow walking straight, performing a mechanical and continuous motor act that does not require the same cautious examination of the surroundings usually needed when moving in everyday environments. In everyday life, we always walk from point A to point B, constantly directed towards a physical object or location in order to accomplish a specific action (Herwig et al., 2013). For reaching point B without colliding with external objects or agents, we need to orient, plan and adapt our motor behavior to the external circumstances. In this interplay between the body and the environment, not only the visual input plays a relevant role (Imai et al., 2001), but broad situational awareness, spatial orientation and opportunity to freely move in the three-dimensional space are also fundamental (Dourish, 2004). However, these aspects become negligible when walking on a treadmill, making it less suitable for testing cognitive factors underlying navigational abilities in an ecologically valid way. Therefore, moving out of the laboratory into natural living is a necessary step that needs to be taken for a deeper understanding of visuospatial attention and navigational abilities and wider applicability of this knowledge to real-life situations (Ladouce et al., 2017).


Table 1. Means and standard deviations for all measures both in the visual discrimination and in the navigation task as a function of Task, Target and Hemifield.

Task	Target	Hemifield
Single-task (M ± SD)	Dual-task (M ± SD)	Green (M ± SD)	Red (M ± SD)	Left (M ± SD)	Right (M ± SD)
Visual discrimination	Reaction time (s)	.425 ± .172	.513 ± .196	.466 ± .186	.463 ± .194	.469 ± .188	.460 ± .188
Missed (%)	0.89 ± 2.34	2.00 ± 4.12	1.43 ± 3.36	1.46 ± 3.44	1.55 ± 3.66	1.34 ± 3.11
Incorrect (%)	4.97 ± 7.74	5.09 ± 6.31	5.10 ± 7.07	4.96 ± 7.06	5.40 ± 7.56	4.66 ± 6.51
Navigation	Exploration time (s)	2.62 ± 2.24	3.31 ± 2.56				
Walking velocity (m/s)	0.66 ± 0.19	0.57 ± 0.18				
One way to ensure experimental control while allowing broader freedom of movement is to employ virtual technologies. For instance, Haarman and colleagues (2017) used an AR setting in healthy subjects performing a visuo-motor walking task for testing performance and learning capacities while walking on an instrumented treadmill. One example of an applied attention-based study involving AR comes from Bonanni and colleagues (2005). They followed the principles of attention theory to prototype five intelligent and mobile AR kitchen systems requiring the lowest attentional demand. However, traditional attention paradigms are not necessarily applicable in 3D contexts. Schmitz and colleagues (2020) measured the impact of central and peripheral visual cues in guiding attention within two panoramic videos projected into a head-mounted display. Contrary to desktop-based attention studies, they showed how the peripheral cues engaged both voluntary and involuntary attention. In a study involving a wide field of view AR display, participants who had to search visual targets in the surroundings while performing a puzzle showed lower discovery rates for targets appearing in the peripheral vision (Kishishita et al., 2014). However, when providing participants with different fields of view in VR during a searching task and a walking task, targets’ localization was slower in the visual field periphery, but the time on both tasks decreased for the increasing field of view (Arthur, 2000). This suggests that a wide field of view is essential for active behaviors like searching a target or walking because it allows to visually access information that serves as a guide for producing better overall navigation strategies. Another interpretation is given by (Ball and North, 2008), who asked whether performance improvement in searching and navigation tasks is potentially due to peripheral vision or physical movements or the combination of the two. When participants had to search a target, they were faster in the condition allowing physical movements in front of a 4.4m x 1.7m display than when executing the task through desktop, mouse and keyboard, but there was no effect of peripheral vision. More broadly, with respect to HCI applications, results obtained through 2D and static interfaces are not necessarily transferable to those involving physical presence and interactions. Therefore, further research in this area is beneficial both for widening the knowledge about how human attention is impacted during dynamic actions, but also for exploring the alterations of performance when using virtual and mobile interfaces in 3D environments.

1.2. The present study
The present study aims to exploit AR to investigate how attentional demands modulate the behavioral performance of participants freely moving from landmark to landmark in a real outdoor environment. CMI is typically indexed by a performance drop in the cognitive and/or motor domain (Woollacott and Shumway-Cook, 2002; Al-Yahya et al., 2011). However, this phenomenon has been extensively studied in laboratory situations that fall short from reproducing the natural conditions in which CMI would have practical significance. For this reason, we chose to avail of AR because of its capacity to concurrently manage virtual features and real ones, its reduced risk of motion sickness compared to VR devices, and a higher ecological validity of the behavior under investigation. Using AR in a research context does not allow for fully ecological research, but it ensures accurate experimental control on the variables under investigation when assessing participants acting outdoors and without physical constraints. Therefore, with reference to the ‘real-world or the lab’-dilemma (Holleman et al., 2020), the outdoor nature of the present research context and the freedom of action of the behavior under investigation move towards greater naturality and complexity, while the nature of the stimuli remains artificial like in laboratory research. Conversely, this research context provides insights into the impact of using AR as wearable and mobile technology when physically moving in the surroundings. Indeed, behaviors involving cognitive and motor resources are connected to the design of mobile and wearable interfaces like smart glasses, smartwatches or smart sport equipment. The more features are incorporated in wearable and mobile technologies, the more the cognitive resources will be divided between the data virtually presented by the technology and the natural stimuli coming from the physical environment. This orchestration of mental resources is likely to cause a performance drop in cognitive or motor performance that can be particularly dangerous in dynamic situations.

To explore these issues, we created three different task conditions through AR: i) visual discrimination of augmented peripheral targets while standing still, ii) spatial navigation that required walking to augmented visual landmarks, and iii) the concurrent performance of both visual and navigation tasks. The visual discrimination task had already been employed in similar studies (Nenna et al., 2020), and was chosen in the present investigation for the proven effect on attention and workload in dual-task scenarios. The navigation task was specifically designed for the present study with the purpose of testing short periods of goal-oriented walking, simulating the everyday behavior of walking short distances from landmark to landmark. We thus evaluated the cost of dual-tasking on both cognitive and motor performance in comparison to the single tasks, along with the subjective assessment of mental load. We hypothesize that CMI mechanisms evidenced in laboratory-based experiments should be the same when moving into a more dynamic scenario and in natural conditions. By implication, they should be replicated in our experiment involving AR and short goal-directed walking outdoors. Specifically, we expect to observe a performance drop in the dual-task condition at least in one of the two tasks. Moreover, we hypothesize the same effect on the perceived workload, which is expected to be higher in the dual-task as compared to the single-task conditions. Finally, we expect the lateralized augmented visual information to impact attention and workload, which has potential implications for the design of AR mobile interfaces.

2. Methods
2.1. Subjects
An a priori power analysis conducted on Gpower (Erdfelder et al., 1996) indicated that a total sample of 36 people was needed to detect medium effect size (d = 0.5) with 95% power. The experimental sample consisted of 45 young adults (university students), 21 females (Mage= 24.28, SDage= 2.22) and 24 males (Mage= 24, SDage= 2.62), who volunteered to participate in the study (without compensation) and signed an informed consent. None of the participants had current or past neurological or psychiatric problems. All the participants had normal or corrected-to-normal visual acuity and reported to have normal color vision. The experimental protocol was approved by the local ethics committee and the study was conducted following the principles of the Declaration of Helsinki.

2.2. Technical set-up
As depicted in Fig. 1, the participants were provided with Microsoft HoloLens 1st generation smart glasses (OS Windows 10, CPU Intel 32-bit 1GHz, memory 2GB RAM and 1GB HPU RAM, 2.3 megapixel widescreen head-mounted display, field of view 30 × 17°, mass 579g). As a fully untethered holographic computer, neither cables nor further devices were needed for executing the experiment. The augmented environment was programmed ad-hoc in Unity (version 2017.4.18f1) for the present investigation. Participants interacted with the augmented objects by physical collision and by giving responses through a wireless Xbox One controller compatible with HoloLens. Behavioral data was automatically saved on the HoloLens internal storage (64 GB flash memory) at the end of each experimental session. Sun brightness was kept under control via photometer: during sunniest days (>700 lx), a tinted visor for outdoor use of the AR was used to augment hologram brightness.

Figure 1
Download : Download high-res image (242KB)
Download : Download full-size image
Fig. 1. Technical set-up. For the experiment, participants wore the Microsoft HoloLens headset for interacting with the augmented contents, with an additional tinted visor for augmenting the hologram's brightness in sunniest conditions (> 700 lx). In addition, they used a wireless Xbox One controller for responding to the visual task. As the AR device is a fully untethered holographic computer and the opted controller was connected to the AR device via Bluetooth, neither cables nor further devices were needed.

2.3. Procedure
Participants were tested outdoors in the local university campus back square, which was chosen because it is protected from direct sunlight by the university buildings. The field of action (Fig. 2) was about 25 × 7m, whereby participants were free to walk and interact with the AR stimuli. Before starting the experimental testing, participants carried out a training session in the augmented environment, based on the same tasks used afterward in the testing phase. Instructions were presented to the participants through the AR system in a videogame fashion (Fig. 2). The participant was instructed to consult the experimenter only in case additional explanations to the instructions presented in AR were needed. The aim of this practice was to familiarize with the augmented stimuli and minimize the incidence of individual differences in the ability to use the experimental apparatus. Therefore, the training was considered concluded only when the participant confirmed to have correctly understood all the tasks and accepted to move to the experimental session.

Figure 2
Download : Download high-res image (511KB)
Download : Download full-size image
Fig. 2. On the left, the university campus square where the experiment took place. Participants were free to move within a field of action without slopes that was about 25m long and 7m wide. On the right, a picture taken from the HoloLens camera during the training session. The instruction panel is shown to guide the participant through the tasks. In the depicted example it instructs to reach the green gem, which is the starting point of the session.

During the experimental session, all subjects executed three tasks in a random order: a Visual Single-Task, a Navigation Single-Task and a Dual-Task combining the first two (Fig. 3). Since human behavior involving motion demonstrates high variability particularly in natural contexts, a within-subject design was chosen, whereby participants serve as their own baseline and the error associated with individual differences is thus reduced.

Figure 3
Download : Download high-res image (687KB)
Download : Download full-size image
Fig. 3. Experimental tasks and measurements. All participants performed a Visual Single-Task, a Navigation Single-Task - which comprised an exploration phase (STE) for finding a virtual balloon, and a walking phase (STw) for reaching it -, and a Dual-Task (DT) whereby the single visual and navigation tasks were performed simultaneously. As for the Navigation Single-Task, the DT was divided into sub-tasks as well for analyzing respectively the exploration time at the DTE and the walking velocity at the DTw. Abbreviations: DTE = Exploration Dual-Task; DTw = Walking Dual-Task; RT = reaction time; STE = Exploration Single-Task; STw = Walking Single-Task; TLX = task load index.

Specifically, the Visual Single-Task was a discrimination task. A green or red augmented butterfly could appear in the left or in the right hemifield at 15° in eccentricity, which is the maximum eccentricity allowed by the HoloLens field of view (30°x17.5°) and ensures a good color perception (Abramov et al., 1991). In order to make the stimuli always visible to the participant, even when in motion, their position was relative to the AR headset position. The stimulus onset time (SOT) was 300ms, and an interstimulus interval (ISI) randomly ranging between 2 and 3s intercurred between the stimuli. Each participant was asked to press the right (or left) trigger on the Xbox 360 controller whenever a green butterfly appeared, regardless of the hemifield where it appeared. On the opposite, the left (or right) trigger had to be pressed whenever a red butterfly appeared, regardless of the hemifield. Participants were asked to be as fast and accurate as possible. The color and lateralization of the stimuli as well as the associated controller buttons were randomized and counterbalanced. If participants did not respond within 1.5s after the stimulus presentation, the response was registered as missed. If they did respond within the 1.5s time window and pressed the corresponding button, the response was registered as correct, otherwise as incorrect.

Further, during the Navigation Single-Task, an augmented blue balloon appeared in front of the participant (0°), on the left (at -90°) or right (at 90°) side. Each participant was thus instructed to first stay on the spot, visually inspecting the surrounding looking for the blue balloon and, once detected, to walk towards the balloon for reaching it. When the participant physically stepped on the balloon spot, the balloon burst, the next balloon randomly appeared in one of the three positions (-90°, 0°, 90°) and a new trial started. When searching for the balloon, participants were instructed to stand still on the spot without stepping around, but they were allowed to move their head and chest. Moreover, we recommended participants to be as fast as possible in visually detecting the balloon position, and then to walk straight to it without deviating the path. This task was thus divided in two sub-tasks: the Exploration Single-Task (STE) for detecting the balloon position, and the Walking Single-Task (STw) for physically reaching the same balloon. Specifically, the visual exploration time was measured from the balloon onset until the same balloon entered the field of view of the HoloLens device. In that instant, the balloon was considered as visually detected by the subject. Right afterward, the walking activity initiated until the physical collision with the same balloon was registered. Each balloon always appeared 3m away from the previous one and was provided with an invisible collider, with 50 cm of diameter. The walking velocity was thus calculated by dividing the distance covered from the position of one balloon to the edge of the other balloon collider (2.5m) by the time elapsed between the balloon detection and the collision. The collisions between the headset and the virtual balloons were detected by the Unity application, which registered the headset position in the space over time by taking advantage of the accelerometer and gyroscope provided by HoloLens. Each Navigation Single-Task phase comprised 50 trials, therefore 50 balloons appeared one after the other creating a navigation path. Diverse navigation paths were thus created and randomly assigned to the participants. In order to avoid possible learning effects, the navigation path performed in the Navigation Single-Task phase was never the same as the Dual-Task path.

Finally, during the Dual-Task (DT) participants were instructed to simultaneously perform the visual and the navigation tasks as fast and accurately as possible. None of the tasks was asked to be prioritized over the other. The number of trials in the navigation task was kept constant to 50, whilst the stimuli of the visual task continuously appeared until the 50th balloon was reached. As for the Navigation Single-Task, the DT was divided into two sub-tasks for analyzing respectively the exploration time in the DTE and the walking velocity in the DTw in direct comparison with the same measurements in the Single-Task conditions (STE and STW). The whole experimental procedure, from the participant's arrival to the end of the experiment, lasted 40 minutes on average.

2.4. Measurements
Performance in the visual task was quantified through Reaction Time (RT) over the correct trials and the percentage of incorrect and missed trials as a measure of task accuracy. During the navigation tasks, the exploration time was measured from the balloon onset until the same balloon entered the field of view of the HoloLens device. Moreover, taking advantage of the AR motion tracking, the walking velocity was additionally measured in conditions requiring motion. After each task, an Italian translation of the NASA TLX was used for assessing perceived workload (Hart and Steveland, 1988), which was thus measured once after the Visual Single-Task, once after the Navigation Single-Task, and also after the Dual-Task. For both behavioral and subjective measures, a Dual-Task cost was computed by subtracting the performance/score registered at the Visual Single-Task and at the Navigation Single-Task from the performance/score at the Dual-Task. The obtained Dual-Task costs were standardized by subtracting the mean of the variable and dividing the result by the variable's standard deviation. Additional information about height, body weight and gender were also collected, as well as information about technology proficiency and about AR/VR past experience. More specifically, we asked participants to respond to three multiple-choice questions asking how they would judge their technology proficiency (low, medium, high) and if they had past experiences with VR (yes/no) and/or AR (yes/no).

2.5. Analysis
All data were analyzed through linear mixed-effect models (LMMs from lme4 package, Bates et al., 2014) in RStudio (Team, 2015). According to the main hypotheses, we expected the multitasking condition to reveal a performance drop in terms of RT and accuracy of responses in the visual task, and/or in terms of exploration time and walking velocity in the navigation task as compared to the single-task conditions. Moreover, we expected the perceived workload to be higher in the dual-task as compared to the single-task conditions. These hypotheses were tested through LMMs with Participant as a random effect (Fitzmaurice et al., 2012) in order to investigate Task (Single-Task, Dual-Task) differences on all the dependent measurements. When analyzing the performance at the visual task, the predictors Target (Green, Red) and Hemifield (Left, Right) were incorporated as well, in order to control for possible effects of the target color and/or lateralization of the stimuli on the task performance. Given the possible influence of height and body weight on walking speed (Blanke and Hageman et al., 1989; Bohannon et al., 1997; Samson et al., 2000), they were additionally considered as random effects in the model for analyzing the walking velocity. The analysis of the NASA TLX questionnaire score was conducted over Task (Visual Single-Task, Navigation Single-Task, Dual-Task) and Items (Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, Frustration) in order to investigate differences between Single- and Dual-Task at each of the dimensions of the questionnaire. In addition, if the performance significantly differed for Task (Single-Task, Dual-Task), possible associations between subjectively perceived mental workload and objective performance were assessed. With this aim, the Pearson's linear correlation test was conducted between the Dual-Task cost computed on NASA TLX overall score and the same cost obtained from each of the behavioral measures that yielded a significant difference between Single- and Dual-Task. Post hoc contrasts were performed on each of the significant interactions with the application of the Bonferroni correction for multiple comparisons (Bonferroni et al., 1936). Finally, additional exploratory analyses were conducted for assessing whether the level of technology proficiency and/or the AR/VR past experiences significantly influenced the results and for investigating possible gender differences. For the latters, three LMMs were computed over Task (Single-Task, Dual-Task) and respectively Technology proficiency (low, medium, high), AR/VR experience (AR, VR, both, none) and Gender (male, female) with Participant as a random effect (Fitzmaurice, Laird and Ware, 2012).

3. Results
Four participants (two female and two male) were considered outliers since their performance was 3SD over the averaged time or error rate of the sample. Thus, the analysis was conducted on 41 participants, 19 female and 22 male. General descriptive statistics at the visual and navigation tasks are reported in Table 1

3.1. Visual task
The analyses of RT, percentage of incorrect and missed responses at the visual task were conducted on the factors Task, Target and Hemifield (repeated measures 2 × 2 × 2). All statistics are reported in Table 2. The analysis of RTs yielded significant main effects of Task and Hemifield but not of Target. Increased RT was observed for the Dual- compared to the Single-Task, and for stimuli presented in the Left compared to the Right hemifield. No interactions were found to be significant.


Table 2. Main results from the LMMs analyzing RT, percentage of incorrect and missed responses in the visual discrimination task, and exploration time and walking velocity in the navigation task.

Visual discrimination	Navigation
Reaction time (s)	Incorrect (%)	Missed (%)	Exploration time (s)	Walking velocity (m/s)
Task (Single-task, Dual-task)	X2 = 560.94	X2 = 0.03	X2 = 10.72	X2 = 111	X2 = 317.62
***	p = 0.84	**	***	***
Target (Green, Red)	X2 = 0.8	X2 = 0.04	X2 = 0.01		
p = 0.37	p = 0.82	p = 0.91		
Hemifield (Left, Right)	X2 = 9.24	X2 = 1.32	X2 =0.41		
**	p = 0.25	p = 0.52		
Stars indicate the significance level of the statistical test (*: p ≤ 0.05; **: p ≤ 0.01; ***: p ≤ 0.001)

The analysis of the incorrect response rate did not yield significant main effects. The interaction between Target and Hemifield was significant (X2 (2, N = 41) = 3.88, p = .048); however, when applying the Bonferroni correction to the post-hoc results, none of the contrast reached the significance threshold. In contrast, for the percentage of missed responses at the visual task, the main effect of Task was significant, with a higher percentage of misses observed in the Dual-Task as compared to the Single-Task. The factors Target and Hemifield did not yield significant effects on the percentage of misses. Performance differences in terms of RTs and percentage of missed responses are depicted in Fig. 4 and summarized in Table 2.

Figure 4
Download : Download high-res image (383KB)
Download : Download full-size image
Fig. 4. Mean reaction time (in s) and percentage of missed responses in the Visual discrimination task as a function of Single- vs. Dual-Task conditions. In each plot and for each condition, a boxplot is depicted next to a half violin plot showing the data distribution. Additionally, each dot corresponds to the averaged data of one participant. Data collected during the Single-Task are color-coded in green, while data from the Dual-Task are in yellow.

Finally, no significant difference was found from the exploratory analysis of performance differences based on Gender, Technology proficiency and AR/VR past experience, indicating that none of these predictors significantly influenced the performance at the visual task.

3.2. Navigation task
Within the navigation task, the time spent searching for balloons appearing in front of the participant (0°) was always 0 and thus not informative. Therefore, those trials (3.5% of the total trials) were deleted only for the analysis of exploration time. The latter analysis indicated a significant main effect of Task. Specifically, the exploration time was significantly longer during the Dual-Task as compared to the Single-Task. Similarly, a significant effect of Task was also observed when analyzing the walking velocity, with a significant decrease of walking velocity in the Dual-Task as compared to the Single-Task. These results are shown in Fig. 5 and resumed in Table 2.

Figure 5
Download : Download high-res image (489KB)
Download : Download full-size image
Fig. 5. Navigation task performance in the exploration task and walking velocity as a function of Single- vs. Dual-Task condition. In each plot and for each condition, a boxplot is depicted next to a half violin plot showing the subjects’ distribution of the data. Additionally, each dot corresponds to the averaged data of one participant. Data collected during the Single-Task are shown in blue, while data performance of the Dual-Task in yellow.

The exploratory analysis of exploration time including the factors Task, Gender (M = 22; F = 19) and Technology proficiency on exploration time revealed a main effect of Task (X2 (1, N = 41) = 111.79, p < .0001) and Gender (X2 (1, N = 41) = 14.91, p < .001), but not of Technology proficiency (X2 (2, N = 41) = 0.71, p > .05). Particularly for the Gender effect, females demonstrated longer exploration time as compared to males. In addition, a significant interaction effect was observed between Gender and Technology proficiency (X2 (2, N = 41) = 4.28, p = .038). As demonstrated by the post hoc test, the exploration time significantly differed for Gender only when comparing males and females who reported a medium (p < .001) but not a high (p = .45) level of Technology proficiency. Post hoc contrasts on the low level of Technology proficiency was not assessable because no males reported a low technology proficiency and only two females did. The number and percentage of males and females who reported low, medium or high technology proficiency, and AR/VR past experience are shown in Table 3. Similarly, the exploratory analysis including AR and VR past experience yielded main effects of Task (X2 (1, N = 41) = 111.47, p < .0001) and Gender (X2 (1, N = 41) = 14.39, p < .001), but not of AR (X2 (2, N = 41) = 0.16, p > .05) and VR past experience (X2 (2, N = 41) = 0.41, p > .05). Moreover, the interactions between Task and Gender (X2 (1, N = 41) = 6.75, p = .009) and between Task, Gender and AR past experience (X2 (1, N = 41) = 5.19, p = .022) were significant. Even though the main effect of Gender suggested longer exploration time in females compared to males, the post hoc test revealed that it did not significantly differ for Gender when covarying for the AR past experience, neither in the Single- nor in the Dual-Task conditions (p-values > .05 for all contrasts).


Table 3. Percentage and number of male and female participants reporting low, medium or high Technology proficiency, AR past experience and VR past experience.

Technology proficiency	AR past experience	VR past experience
Low	Medium	High
Gender	M (n = 22)	0% (n = 0)	27.7%(n = 6)	72.72%(n = 16)	31.8%(n = 7)	59.1%(n = 13)
F (n = 19)	10.5% (n = 2)	42.1%(n = 8)	47.36%(n = 9)	15.79%(n = 3)	21.05%(n = 4)
For walking velocity, the exploratory analysis on the factors Task, Gender and Technology proficiency with participants’ Height and Weights as random effects, indicated main effects for Task (X2 (1, N = 41) = 319.76, p < .0001) and Gender (X2 (1, N = 41) = 3.86, p = .049) but not for Technology proficiency (X2 (1, N = 41) = 0.33, p = .84). A significant interaction was observed only between Task and Gender (X2 (1, N = 41) = 7.44, p < .01). Participants walked faster during the Single-Task compared to the Dual-Task condition, and the walking velocity (m/sec) of males (M = 0.66; SD = 0.19) was significantly higher as compared to females (M = 0.57; SD = 0.19). Particularly, post-hoc on the interaction between Task and Gender revealed that males walked faster than females only in the Dual-task (p < .0001) but not in the Single-task condition (p = 0.18). Additionally, the exploratory analysis on the walking velocity accounting for the AR and VR past experience yielded main effects of Task (X2 (1, N = 41) = 318.9, p < .0001) and Gender (X2 (1, N = 41) = 11.68, p < .001) but not of the AR (X2 (1, N = 41) = 1.11, p = .29) or VR past experiences (X2 (1, N = 41) = 2.35, p = .12). Significant interactions were observed between Task and Gender (X2 (1, N = 41) = 13.97, p < .001), Task and VR past experience (X2 (1, N = 41) = 5.82, p = .015), and Task, Gender and VR past experience (X2 (1, N = 41) = 4.49, p = .03). As revealed by the post hoc tests, the differences in walking velocity between Single- and Dual-Task remained statistically significant when considering for Gender (p-values < .0001) and VR (p-values < .0001). Also in this case, as indicated by the post hoc contrasts for the interaction between Task and Gender, the walking velocity of females was significantly slower as compared to males only in the Dual-task (p = .005). Similarly, the walking velocity did not significantly differ between males and females when covarying for VR past experience in the Single-task (p-values > .05 for all contrasts), but it did in the Dual-Task. Particularly, males and females that already have had experiences with VR did not differ significantly in their Dual-task walking performance (p=.74). However, when considering participants without previous experiences with VR in the Dual-task, males walked faster than females (p = .007).

3.3. NASA TLX questionnaire
The NASA TLX questionnaire was analyzed through a LMM over Task (Visual Single-Task, Navigation Single-Task, Dual-Task) and Items (Mental Demand, Physical Demand, Temporal Demand, Performance, Effort, Frustration), and revealed significant main effects for both Task (X2 (2, N = 41) = 174.64, p <.0001) and Item (X2 (5, N = 41) = 134.12, p <.0001). Post hoc tests on the factor Task showed that the overall score of perceived workload for the Navigation task (M = 3.95; SD = 3.1) was significantly lower than the overall score for both the Visual task (M = 7.23; SD = 4.9; p < .0001) and the Dual-Task (M = 7.34; SD = 4.34; p < .0001). The latter two scores did not differ. Additionally, the interaction between Task and Item reached significance (X2 (10, N = 41) = 115.26, p <.0001), with post-hoc tests demonstrating higher Mental Demand perceived during the Dual-Task (p < .0001) and during the Visual task (p < .0001) both compared to the Navigation task. Similarly, the perceived Effort resulted to be higher during the Dual-Task (p < .0001) and during the Visual task (p < .0001) in comparison to the Navigation task. A significantly higher score at the item Performance was also given to the Visual task (p < .001) and to the Dual-Task (p = .005) as compared to the Navigation task, and significantly higher Frustration was registered for the Visual task (p < .0001) and the Dual-Task (p = .016) as compared to the Navigation task. Finally, higher Physical demand was reported for the Dual-Task (p < .001) compared to the Visual task. Means and standard deviations of the NASA scores for each task and item are depicted in Fig. 6. In addition, no significant difference was found from the exploratory analysis of perceived workload as a function of Gender, Technology proficiency and AR/VR past experience.

Figure 6
Download : Download high-res image (447KB)
Download : Download full-size image
Fig. 6. Means of subjective NASA TLX scores for each Item as a function of Task. Error bars depict the standard error.

Further, a Pearson's linear correlation test was conducted between the Dual-Task cost obtained from the overall NASA TLX score and the Dual-Task cost obtained from each of the behavioral measures that yield a significant difference between Single- and Dual-Task (RT and percentage of missed responses at the visual task, and exploration time and walking velocity at the navigation task). The results of the four correlation tests did not reveal significant relationships between the Dual-Task cost at the NASA TLX questionnaire and any of the Dual-Task costs calculated on the behavioral measurements.

4. Discussion
In the present study, we exploited AR to create a realistic outdoor scenario in which freely moving participants engaged in either or both tasks of spatial navigation (walking to close augmented landmarks) and visual discrimination of augmented targets appearing in the near periphery. We evaluated the cost of dual-tasking on cognitive and motor performance in comparison to the single-tasks. Overall, we expected to observe a CMI in the Dual-Task condition, reflected by a performance cost in the visual task (i.e., increased RTs and lower accuracy) and/or in the navigation task (slower walking velocity and increased exploration time). Moreover, the subjectively perceived workload was expected to reflect the objective modulation of behavioral performance. For the visual task, possible effects of the target color and/or lateralization on task performance were assessed as well for controlling unwanted perceptual differences given by the experimental design features. As an additional exploratory analysis, the potential impact of Gender, Technology proficiency and AR/VR past experiences on both subjective and performance measures were investigated.

4.1. Task performance
4.1.1. Visual task
Significant differences in the visual task performance between the Single- and Dual-Task conditions were observed in terms of RT and percentage of missed responses, revealing that participants became slower in responding (by more than 80 ms) and more prone to miss the target while navigating (Dual-Task) as compared to standing still in the campus square (Single-Task). In the present investigation, the Visual Single-Task was static and thus purely cognitive, while the Dual-Task had an important motor component both in the exploration phase (whereby participants turned their head and chest for visually inspecting the environment), and in the walking phase. These results are in line with the hypothesis that the cognitive-motor dual-task demands triggered a CMI (Al-Yahya et al. 2011; Patel et al., 2014; Plummer et al., 2015; Beurskens et al., 2016).

A small (9 ms) RT difference was also found as a function of hemifield, indexing slower responses to targets appearing in the left as compared to the right. This is consistent with the notion that non-spatial information is processed more efficiently in the right visual field (Karim and Kojima, 2010, for review). Moreover, the processing of peripheral stimuli is better in the right than in the left hemispace especially under load (Chen and Spence, 2017).

Additionally, we note that the eccentricity of the visual targets was set at 15° because of the relatively small field of view provided by HoloLens 1 (30°x17.5°), which implies that they appeared in near-peripheral vision. This leaves open the question of whether more lateralized stimuli could have the same impact on attentional and navigational performance. Future experiments may investigate how delivering visual stimuli at different and higher eccentricities through AR would impact attention and navigation. However, given the scarce capability of the human eye to discriminate colors in far peripheral vision(Abramov, Gordon and Chan, 1991), a different task should be designed. This would also allow to further investigate the impact of different visual augmented tasks on the navigation performance.

4.1.2. Navigation task
Dual-tasking also affected the navigation task, both in terms of longer exploration time and slower walking velocity as compared to performing the Navigation Single-Task. The increase in exploration time suggests that the additional visual task required resources which in turn were subtracted from the exploration task, leading to a delay in finding the landmark. Many situations in everyday life require our attentional resources to be distributed over wide areas of the visual field for searching and detecting target items while ignoring distractors. Usually, similar mechanisms have been assessed through static visual search experiments investigating the detection of a target between multiple distractor elements (i.e. Olk et al., 2018) and rarely allowing a multi-sensorial involvement of self-movement (i.e. Jungnickel and Gramann, 2016). However, in daily situations, we usually search for target objects by dynamically exploring and moving through the surroundings. In the present experiment, we emphasized the active component of this behavior, placing the target objects of the navigation task beyond the functional visual field of the participant and allowing higher degrees of movement during the exploration of the environment.

About the walking activity, there is a large number of studies demonstrating that gait pattern is negatively affected during dual-task walking in young adults (Beauchet et al., 2005; Beurskens et al., 2011, 2016; Feld and Plummer, 2019; Prupetkaew et al., 2019). More specifically, Al-Yahya et al. (2011) reviewed gait speed differences observed in a variety of tasks (RT task, discrimination and decision making, mental tracking, working memory and verbal fluency tasks) and in different populations, including healthy young subjects. In light of this evidence, slowing down under dual-tasking suggests that walking involved high-order cognitive systems that were also recruited by the cognitive task, thereby reaching capacity limits and affecting motor performance. However, it is important to mention that our participants only walked 3 meters between one virtual balloon and the other. Moreover, unlike previous dual-task walking studies, the walking activity in our paradigm was object-oriented rather than walking per se. In other words, it was instrumental to the navigation task and thus part of the spatial navigation task itself. In this perspective, the observed walking velocity modulation was even more strongly related to the attentional task-load rather than to purely kinematic aspects.

4.2. Task load self report
The NASA TLX questionnaire was used for evaluating the subjectively perceived workload related to the three experimental tasks. Among the most interesting effects for the focus of the present study, higher mental demand and effort were perceived at the Dual-Task as compared to the Navigation Single-Task. Moreover, higher mental demand and effort at the Navigation Single-Task compared to the Visual Single-Task were also reported, showing how standing still and focusing exclusively on the visual task was perceived as more demanding and effortful as compared to exploring and actively navigating the square without further tasks. Despite the evidence for a higher cognitive load reflected by RT, accuracy of responses, walking velocity and exploration time in the Dual-Task, participants did not perceive differences in mental load or effort between the execution of the visual task while standing still or during the Navigation Single-Task. Similarly, they self-reported their performance as similar at the Dual-Task and at the Visual Single-Task. This speaks for an incongruence between perceived and objective Dual-Task load, which was underestimated by our participants. Furthermore, despite a clear trend for a progressive increase of perceived physical demand with increasing physical load, the former was perceived as significantly higher only in the Dual-Task with respect to the static Visual Single-Task. Even in this case, although participants demonstrated slower walking velocity at the navigation task while performing the visual task, it was not subjectively perceived as significantly more demanding as compared to the Navigation Single-Task. The inconsistency between the objectively registered performance and the subjectively self-reported workload was also evidenced by the absence of correlation between the Dual-Task cost computed on the NASA TLX score and the Dual-Task cost computed on the task performance indices. One possible explanation is that participants did not perceive a significantly higher mental load in the dual-task because they actually compensated for the increased load by decreasing their performance. This can be particularly true when considering motor performance. Bloem and colleagues (2000) compared the “stop walking when talking” effect in older adults with motor or cognitive impairments and showed that impaired performance in this task predicts falls in those having cognitive but not motor impairments. In the present study, it is thus conceivable that participants did not feel higher mental and physical fatigue in the dual-task condition because they spent longer time searching for the landmark, and walked slower towards it. However, from a different perspective, it is also true that similar dissociations in workload measurements have been observed and addressed widely (Horrey et al., 2009; Hancock and Matthews, 2019). In this regard, future studies might consider exploring whether physiological data lead to workload measures that diverge from the subjective rated workload as well. Exploring the suitability of each methodology (self-report, behavioral, physiological) in workload assessment would be beneficial for a deeper understanding of psychophysiological responses to higher cognitive load, particularly during motion.

4.3. Gender and individual technology proficiency
Our results seemed to suggest that males performed better than females in the navigation task, both in terms of exploration time and walking velocity. However, the difference in exploration time disappeared by covarying for the past AR experience. Similarly, when accounting for the past VR experience, gender differences disappeared also for walking velocity but only in the Single-task. This may suggest that the past experience with virtual technologies smooths the gender differences in low-workload conditions, but is not able to compensate for those differences under higher workload. In the present study, only the 15.8% of females compared to the 31.8% of males already had experience with AR, and the difference between males (59.1%) and females (21.05%) who reported previous experiences with VR devices was even bigger. Especially for experimental settings involving virtual environments, literature shows how the gender effect was observed to be more likely mediated by third factors rather than being a primary predictor of visuospatial performance (Waller, 2000). Particularly, the ability in the use of interfaces and other individual differences in HCI are important factors for explaining gender differences in experimental contexts involving virtual environments. Similarly, in the present investigation, the previous experience with virtual/augmented technologies did impact the performance at the visuospatial tasks.

These factors influenced the performance only when actively navigating the environment (navigation task), but not when executing the static task (visual task). This may be a relevant point in favor of the use of AR as an ecologically valid method for studying spatial cognition, which does not suffer from individual differences in previous experience with immersive technologies when restricting its use to a static setting. Differently, when requiring participants to physically navigate a virtual environment, it becomes essential to control for individual differences given by the past experience with immersive technologies as well as more specific information, such as the familiarity with technological devices for navigation or gaming abilities. Particularly, the possible advantage of video game players in computer-simulated environments has been proposed as an additional possible explanation of such a gender effect in visuospatial tasks (Waller, 2000). In the present experiment, we did not collect information about gaming habits. Nonetheless, the individual previous experience with immersive technologies did affect the results.

4.4. Practical implications
As a very crucial aspect of this research, results obtained in traditional dual-task laboratory-based paradigms were here reproduced extending the CMI effect to both searching and physically reaching a close landmark with AR, suggesting that AR can be a suitable method for capturing attentional changes during everyday navigation tasks outdoors. This opens a new range of possibilities for basic research investigations on mobile cognition, enhancing the ecological validity of these studies without impacting the experimental rigor. One of the possible applications of such a portable and easy-to-use paradigm fits the clinical settings for diagnosis and rehabilitation of cognitive impaired targets. Developing an augmented tool that detects the very well-known attentional variations usually recorded in traditional dual-tasks might help for a quick examination of cognitive-motor coordination impairments in specific clinical populations. For example, multitasking has been shown to unveil even subtle attentional biases in stroke patients, which typically remain undetected in diagnostic tests in which patients are free to compensate for their deficits through strategic processes (Blini et al., 2016; Bonato et al., 2013; Bonato, 2015). At the same time, it might be the key for enhancing the engagement and portability of traditional dual-task trainings which are known to be effective in chronic stroke, dementia, Parkinson and Alzheimer diseases (Mirelman et al., 2011; Schwenk et al., 2010; Segev-Jacubovski et al., 2011; Yogev-Seligmann et al., 2012; Yang et al., 2007) and even in healthy older adults (Silsupadol et al., 2009).

AR-based assessment of attentional load effects might be also relevant within working environments, where high workload situations are an everyday occurrence. Information overload can overwhelm workers even in workplaces free from physical risks, undermining productivity and contributing to low employee engagement as well as health impairments (De Beer et al., 2016). On the other hand, jobs in the modern transportation industry, surveillance jobs, and all man-machine complexes requiring high and sustained vigilance such as assembly and production lines, are all characterized by safety risks depending on the worker's attention. In these contexts, a systematic evaluation of workload on the field may help workers to manage their attentional load during the execution of tasks, particularly when they require sustained vigilance and implies risks for their health.

5. Contributions, Limitations and Future Work
In summary, healthy young adults showed a performance drop at both tasks when freely navigating from landmark to landmark while detecting peripheral augmented objects in AR. Participants were significantly slower in searching for and walking through landmarks (navigation task) while simultaneously discriminating between target objects suddenly appearing in their near peripheral visual field (visual task). Moreover, the discrimination of the lateralized visual targets slowed down significantly and was more prone to missed responses when it was concurrent with the physical navigation. These results replicate the performance drop observed in previous cognitive-motor dual-tasks, extending the CMI effect to both searching and physically reaching a close landmark. Moreover, they point towards AR being a valuable tool for detecting workload-related behavioral differences in real environments. In turn, they offer insights into the potential impact of AR interfaces that overlap augmented visual contents on the lateralized visual field during everyday motion. A wide visual perspective on the surrounding is beneficial for orienting, predicting and adapting the own motor behavior during navigation, but the presentation of relevant lateralized information through AR does affect motion itself. At the same time, participants showed to subjectively underestimate the dual-task workload objectively quantified by their performance.

Our study has some limitations. Firstly, three meters is a short distance to make inferences about an agent's walking behavior and particularly about the walking speed. Indeed, our results are relevant for situations in which short periods of walking allow one to reach a landmark in a distracting space. Still, they cannot be directly compared with those of previous studies that assessed continuous walking (e.g., treadmill studies). Notably, our purpose was not to measure continuous and mechanical walking but a more dynamic and goal-oriented motor behavior, simulating the everyday navigation of a chaotic environment characterized by sudden directional changes. Nonetheless, one further reason for the 3m distance between landmarks is attributable to the technical specifics of HoloLens, which does not allow to place objects far in the space while keeping the adequate resolution of the holograms. Future research should avail of the HoloLens 2, which are equipped with increased resolution and would allow further assessment of finalized walking to landmarks placed farther in the space. In this way, it would also be possible to additionally measure gait kinematics and make precise inferences about the walking behavior. For instance, given the short distance between landmarks, the navigation task became quite continuous, with participants dynamically moving from one landmark to the next one. This made it impossible to find a stable acceleration threshold for separating the searching phase from the walking one and led us to choose the moment when the landmark entered the participant's field of view as the onset of the walking activity for each trial. In future studies, especially if allowing walking longer distances from landmark to landmark, headset acceleration might be used to detect the agents' walking onset.

From a different perspective, it is known that video game players can have an advantage in navigation tasks, particularly if executed in computer-simulated environments, and we did not measure this aspect in our study. However, all participants completed a training session for familiarizing themselves with the AR paradigm before starting the experiment, which likely made them more homogeneous in terms of task execution ability. Future experiments involving 3D environments might consider collecting more specific information about gaming habits and the familiarity with virtual devices. Additionally, none of the participants asked for a break during the experiment or complained about discomfort due to the AR headset use. However, this information was not systematically collected in the present study, and headset wearing time was relatively short (i.e., total time on task was about 40 minutes). Future studies might further explore technical challenges and possible issues that come with AR headsets' prolonged use. Finally, it is also important to mention that our study involved only young adults. If and how the present findings would translate to older adults is a relevant question that may be addressed in future investigations.

Overall, in this work we measured the CMI in a novel experimental outdoor scenario, highlighting the consequences of conducting multitasking behaviors involving cognition and motion through AR. Our results widen the knowledge about how human attention is impacted in a hybrid real-virtual environment and point towards AR as a relevant tool for conducting cognitive-motor research within the context of everyday tasks outdoors and in motion.