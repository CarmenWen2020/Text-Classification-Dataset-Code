The elderly living in smart homes can have their daily movement recorded and analyzed. As different elders
can have their own living habits, a methodology that can automatically identify their daily activities and
discover their daily routines will be useful for better elderly care and support. In this article, we focus on
automatic detection of behavioral patterns from the trajectory data of an individual for activity identification
as well as daily routine discovery. The underlying challenges lie in the need to consider longer-range dependency of the sensor triggering events and spatiotemporal variations of the behavioral patterns exhibited by
humans. We propose to represent the trajectory data using a behavior-aware flow graph that is a probabilistic finite state automaton with its nodes and edges attributed with some local behavior-aware features. We
identify the underlying subflows as the behavioral patterns using the kernel k-means algorithm. Given the
identified activities, we propose a novel nominal matrix factorization method under a Bayesian framework
with Lasso to extract highly interpretable daily routines. For empirical evaluation, the proposed methodology has been compared with a number of existing methods based on both synthetic and publicly available
real smart home datasets with promising results obtained. We also discuss how the proposed unsupervised
methodology can be used to support exploratory behavior analysis for elderly care.
CCS Concepts: • Applied computing → Health informatics; • Computing methodologies → Probabilistic
reasoning; • Information systems → Clustering;
Additional Key Words and Phrases: Nominal matrix factorization, probabilistic hierarchical model, Bayesian
inference, routine pattern discovery
1 INTRODUCTION
The advent of ubiquitous computing and sensor technologies has enabled new opportunities for
human activity analysis. Related applications include daily activity pattern detection in cities
(Jiang et al. 2012; Schneider et al. 2013), activity recognition (Kwapisz et al. 2011), and assisted living (Tacconi et al. 2008; Park et al. 2010; Lin et al. 2013). In particular, for assisted living, the elderly
Fig. 1. Trajectories of two different activities: frequent sequential patterns (left) and our proposed method
(right).
living in a smart home equipped with sensors can have their daily indoor movement logged and
analyzed. Analyzing the logs can help detect potential abnormal behaviors that could be caused by
unfavorable health situations (Tacconi et al. 2008; Park et al. 2010). For related studies, daily routines are often extracted to provide decision support for better elderly care (Lin et al. 2013). In the
literature, different computational tools have been developed for analysis. In particular, a number
of supervised learning methods have been adopted and found effective for activity analysis and
recognition (Brdiczka et al. 2009; Doukas et al. 2007). However, there are limitations when putting
them into practice. Manually creating labeled data for the training is time-consuming. Also, analyzing human activities is often of an exploratory nature, where the activity labels are simply
unknown in advance. Thus, unsupervised learning methods have been explored for automatic activity identification (Nazerfard et al. 2010; Rashidi et al. 2011).
In this article, we focus on elderly mobility and daily routine analysis using unsupervised learning methods in a smart home setting. This means that we do not assume any prior knowledge on
the activities and daily routines to be identified. The only assumption we make is that a person
when performing an activity at home (e.g., preparing a meal) should move around according to
some regularity of his or her own. We call this kind of regularity a behavioral pattern. Our first goal
is to detect behavioral patterns from the observed sensor-triggering events to identify the underlying activities. Furthermore, how such behavioral patterns (activities) appear within a day should
follow some routines (e.g., routines for ordinary days and routines for weekends). Our second goal
is to discover such daily routines from the data.
The activity identification task (Goal 1) is challenging as different activities of the same person are performed in the same smart home. The trajectory segments, even though corresponding
to different activities, are highly likely to have some short patterns of sensor triggering sharing
among them. In order to differentiate the trajectory segments and detect behavioral patterns that
are more activity specific, one needs to characterize each sensor-triggering event by considering
also the triggering of its nearby sensors over a longer time window before and after its triggering
(or, in other words, a longer range).
Also, a person seldom moves in exactly the same way to perform the same activity, and spatiotemporal variations of the behavioral pattern are unavoidable (e.g., one may occasionally stay
at different positions in the kitchen for occasionally a longer or shorter time). All these make the
conventional frequent pattern-based method, which represents a particular activity as a cluster of
similar discrete frequent patterns, not directly applicable. As an illustration, Figure 1 shows a few
sequences of sensor-triggering events of two different activities. Representations obtained based
on the frequent sequential pattern method are shown on the lower left. It is obvious that such representations of the two activities are hard to be distinguished. The key problem is that the ordering
information of the appearance of the sequential patterns is discarded.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:3
To alleviate the aforementioned challenges, we propose to infer from the indoor trajectory data
(timestamped sensor-triggering events of an individual) a behavior-aware mobility flow graph. We
represent the mobility flow graph as a probabilistic finite state automaton (PDFA) where its nodes
are attributed with features characterizing some local movement features. Behavioral patterns are
represented as subflows in the flow graph. Activities are identified by detecting subflows embedded
in the flow graph. Our conjecture is that an activity can be characterized by a set of states (local
movement patterns) where, once one of them is reached, there is a high chance of staying within
and transiting among the states for a while before leaving. To detect the subflows, we adopt a
weighted kernel k-means algorithm. As illustrated in Figure 1, using our proposed method, the
flow graphs inferred for the two activities can retain the ordering information and obviously end
up with two distinct enough representations.
The daily routine discovery task (Goal 2) is also nontrivial even though most of the people simply apply matrix factorization techniques. This task carries the errors from the preceding activity
identification task. Also, the assumption that the basis vectors extracted by existing matrix factorization methods are corresponding to some interpretable daily routines may not be valid. Also,
different parts of a daily routine may not appear at exactly the same time each day. So, in general,
a more robust factorization technique is needed. And the problem will be further complicated if
interleaving activities are considered and the environment is dynamic (say, with moving objects
or other individuals).
In this work, we propose a probabilistic nominal matrix factorization method. In particular, after
detecting the subflows as activities, we tag each sensor-triggering event in the trajectory data
with an activity label as detected. A matrix with each column being a sequence of daily activity
labels ordered by the time within a day is first formed and will be factorized into basis factors
as daily routines. While it is common to decompose each sequence of labels (nominal data) into
multiple sequences of binary data (one for each label), as in (Eagle and Pentland 2009), to convert
the matrix to binary, we factorize the nominal data matrix directly. We avoid multiplying the
size of the data. Also, we want to capture the correlation of the activities, which is hard to do
after converting it into multiple binary matrices. In the literature, most of the matrix factorization
techniques deal with matrices with continuous real values (with few exceptions (Lin et al. 2005,
Paquet et al. 2012)). To perform nominal matrix factorization directly, we assume that the similarity
of detected subflows can be estimated. We embed the discrete labels (or nominal values) onto a
d-dimensional continuous space accordingly. Then, a hierarchical probabilistic model is proposed
for the factorization, which is important as the activity labels identified for the sensor-triggering
events are noisy. A Bayesian Lasso (Least Absolute Shrinkage and Selection Operator) is introduced
to ensure the sparsity of the coefficient matrix, and thus the interpretability of the discovered
basis factors (i.e., daily routines). We consider interpretability to be a highly important aspect for
applications like elderly care. To carry out the model inference, we adopt Gibbs sampling.
We evaluate the effectiveness of the proposed methodology using several publicly available
smart home datasets that contain movement trajectories of an elder living in a smart home. Our
experimental results show that our proposed approach can detect subflows that are more specific
in terms of their correspondence to activities when compared with an existing frequent patternclustering approach (Rashidi et al. 2011). Also, we benchmark the performance of the proposed
nominal matrix factorization method for routine discovery with a number of existing matrix factorization methods using both synthetic and benchmarking datasets. Highly promising results in
terms of the accuracy of the extracted basis vectors of discrete labels are obtained.
The remaining article is organized as follows. Section 2 describes the related work. Section 3
presents the proposed methodology for inferring the behavior-aware mobility flow graph and the
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
54:4 C. Li et al.
daily routines. Experimental results and related discussion can be found in Section 4. Section 5
concludes the article with possible future extensions.
2 RELATED WORK
In the literature, there have been quite a few studies on human mobility analysis using data mining methods. For instance, macroscopic human mobility patterns of humans were extracted from
trajectories of mobile phones to support better urban planning and infection control (Gonzalez
et al. 2008, Song et al. 2010). Algorithms for better organizing trajectory data have been developed
so that important movement trends can be visualized and tracked (Parent et al. 2013). With the
recent advent of location-aware social networks, data about people’s whereabouts have become
much more accessible. That further triggers new applications like detecting anomalous trajectory
patterns (Fanaswala and Krishnamurthy 2013), discovering interesting places (Yan et al. 2013),
gathering pattern analysis (Zheng et al. 2014), inferring social ties (Xiao et al. 2014), and discovering urban functional zones (Yuan et al. 2015), among others. The aforementioned projects focus on
outdoor activities. Indoor trajectory data can also be collected in a smart home setting (Schneider
et al. 2013, Prentow et al. 2015). Related applications include detection of behavioral deviations
(Saives et al. 2015), daily routine analysis (Zhao et al. 2014), and so forth. In this article, we focus
on developing data mining methodologies for indoor mobility analysis.
As discussed in Section 1, modeling longer-range dependency among the sensor-triggering
events is an important issue to address for modeling behavioral patterns. (Pastra and Aloimonos
2012) explained the similarity between human activities and languages with respect to the sequence representations and the grammatical structures they share. In the literature, probabilistic
grammar models have been widely used for representing languages so that variations as well as
longer-range dependency in the observed sentences can be properly modeled and captured. They
have also been applied to human activity analysis, including activity recognition (Peng et al. 2011,
Kuehne et al. 2014), gesture recognition (Hong et al. 2000), maneuver recognition (Hülnhagen
et al. 2010), activity segmentation (Pirsiavash and Ramanan 2014), and activity prediction (Li and
Fu 2014). Also, different inference methods (Carrasco and Oncina 1994, Thollard et al. 2000) have
been proposed for the model estimation. In this article, we extend probabilistic finite state automaton (Vidal et al. 2005) so that they can better model indoor human activities.
For related work on daily routine discovery, approaches like matrix factorization (MF) (Eagle
and Pentland 2009, Zheng et al. 2013) and topic modeling (Sun et al. 2014) have been proposed.
In particular, MF is an effective data analysis approach that has been studied for decades.
Some recently proposed ones include maximum-margin matrix factorization (Xu et al. 2012),
probabilistic matrix factorization (Schmidt et al. 2009), sparse probabilistic matrix factorization
(Jing et al. 2015), and variants of nonnegative matrix factorization (NMF) (Li et al. 2010, Cai et al.
2011, Huang et al. 2014, Gillis and Vavasis 2014). Most of them can only work for matrices with
continuous values. In our case, we need to factorize nominal matrices with activity labels. In the
literature, there exist few exceptions where factorization of matrices with noncontinuous values
is studied. Boolean matrix factorization (Miettinen 2012) introduces the Boolean operation to deal
with binary valued matrices. It was then extended to handle ordinal cases by introducing a new
set of operations (Belohlavek and Krmelova 2013). Ternary matrix factorization (TMF) (Maurus
and Plant 2014) uses three-valued logic recursively to approximate the discrete valued matrix
with hard constraints such that each coefficient should be either zero or one. Ordinal matrix
factorization (OMF) (Paquet et al. 2012) can be formulated under a hierarchical probabilistic
framework to model matrices, with their elements taking a finite ordered set of values. The
methodology proposed in this article is inspired by OMF. We leverage on a reasonable assumption
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:5
Fig. 2. The flow chart of the proposed methodology.
that the similarity among the discrete labels is known (or can be estimated) so that the underlying
nominal matrix can be factorized under a Bayesian framework.
3 PROBLEM FORMULATION
3.1 An Overview of the Proposed Methodology
Figure 2 shows a flow chart summarizing the key steps of the methodology proposed in this article. In a nutshell, we first infer the mobility flow graph to summarize the mobility traces of an
individual. A set of “behavior-aware” features that characterize the local movement patterns are
proposed to guide the graph inference so as to infer a compact flow graph and at the same time to
preserve the specificity of the “behavior-aware” features per state as far as possible. A weighted
kernel k-means algorithm is then utilized to detect subflows in the flow graph for activity identification. With the labels of the identified activities marked on the trajectory data, we rearrange the
data in a matrix form and propose a novel probabilistic nominal matrix factorization method for
discovering daily routines with high interpretability. In this section, we present the key concepts
and mathematical formulations of the different steps. Table 1 summarizes the notations used in
the article.
3.2 Detecting Behavioral Patterns for Representing Human Activities
Considering sequences of sensor-triggering events as strings of alphabets generated by a stochastic
sequence model, we infer a sequence model so that the probability distribution over the event
sequences can be optimized. In principle, with the model inferred, tasks like identifying the most
probable movement in the next step given a location can be supported. Among different sequence
models, the probabilistic automaton is one of the representative ones and adopted in this article.
Definition 3.1. A Deterministic Finite Automaton (DFA) A is a 5-tuple, (Q, E, δ,q0, F ), where
Q is a finite set of states, E is an alphabet, q0 ∈ Q is the initial state, δ : Q × E → Q is a transition
function, and F ⊆ Q is the set of final states. A Prefix Tree Acceptor (PTA) is a tree-like DFA
generated by all the prefixes of the observed strings as states, which can only accept the observed
strings.
Definition 3.2. A Probabilistic Deterministic Finite Automaton (PDFA) is a 5-tuple A =
(Q, E, δ, π,q0, F ), where Q is a finite set of states, E is an alphabet, δ : Q × E → Q is a transition
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
54:6 C. Li et al.
Table 1. Table of Notations
Notation Description
A Probabilistic deterministic finite automaton (PDFA)
Q Finite set of states
E Alphabet
θ Threshold for state merging
D The observed data
M The number of sensor-triggering events per day
N The number of observed days
X An observed sequence
Xˆ An observed segment
Y The set of distinct sensor labels
fq The FBLM feature for state q
fr
q The RFBLM feature for state q
t The number of subflows or activity labels
L The set of activity labels
d The dimensionality of embedded space
DA The nominal matrix transformed from D
Dμ The embedded matrix of DA
K The number of basis vectors
U The basis vector matrix
V The coefficient matrix
function, π : Q × E → [0, 1] is the probability of the next symbol given a state, q0 ∈ Q is the initial
state, and F : Q → [0, 1] is the end of the string probability function.
Among the existing PDFA inference algorithms, we adopt the ALERGIA algorithm (Carrasco
and Oncina 1994). We first build a PTA from each observed sequence. To generalize for strings
other than those observed, the algorithm introduces a merge operation. Let ni denote the number
of strings arriving at state qi ; fi (a) the number of strings following edge δi (a), where a is the
edge’s symbol; and fi (#) the number of strings ending at state qi . The probabilities for the string
terminating at or leaving state qi can be computed as fi (#)/ni and fi (a)/ni , respectively. According
to ALERGIA, for each pair of states (qi,qj) with common outgoing edges, they are compatible
for merging if the probabilities of leaving them are close enough as controlled by a threshold
parameter θ as the confidence of the test. In the following sections, we explain how we modify
ALERGIA for our application.
3.2.1 Inferring the Behavior-Aware Mobility Flow Graph. To infer a mobility flow graph from
the trajectory data, we consider the temporally ordered sensor-triggering events over a fixed time
interval per day as an observed sequence, X = {x1, x2,..., xM }, where each element corresponds
to a sensor ID. Given N day observations, N observed sequences denoted as D = {X1,X2,...,X N }.
We then infer a set of corresponding PTAs and the corresponding PDFA. As an illustration, given
X = {a,b, a,b, a,b} as visualized in Figure 3(a), we first define for each of these sensor-triggering
events a particular edge. Then, we add a state denoted as q = (SIDq, Iq,Oq, FIq , FOq ) with a unique
state ID (SID) between each consecutive pair of the edges to link them up using the incoming
and outgoing edges stored in q as (Iq,Oq ) to form an initial PTA. Initially, the frequencies of encountering Iq and Oq (FIq FOq ) are both set to be one, which are to be updated during the flow
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:7
Fig. 3. An illustrated example of the proposed behavior-aware feature set. (a) A sensor-triggering sequence
involving sensors a, b, and c. (b) Three PTAs for three segments of sensor-triggering events. (c) Frequencies of
the unigrams in PTA 2 and PTA 3. (d, e) Frequencies of the bigrams in PTA 2 and PTA 3. (f ) The data structure
of a PTA.
graph inference. The corresponding PTA is depicted as PTA 3 in Figure 3(b). Then, in principle,
we can proceed with the state-merging operation of the standard ALERGIA algorithm to obtain
the PDFA. However, this will end up with a PDFA containing states that are not location specific,
which is not desirable. The main reason is that the ALERGIA algorithm pairs up and merges states
with the same outgoing edge labels. In our application, a state change happens when the sensor
event associated with an incoming edge is triggered. To maintain the states after being merged
to be location specific, states are paired up with the same incoming edges labeled Iq instead. In
addition, we further propose a set of features characterizing the local movement to attribute each
state, and only the states with their features close enough are to be merged. For the design of
such “behavior-aware” features, spatiotemporal properties of local movement are considered, as
detailed in the next section.
3.2.2 Behavior-Aware Features as State Attributes. Merging states by considering only identical incoming edge labels will end up with trajectories of different activities that cross each other
to be “tied” up. We propose a set of “behavior-aware” features to differentiate the states based
on the spatiotemporal properties of their local movements. To compute that, we model a trajectory as a sequence of segments, where each segment is a locally maximal subsequence of triggering
events associated with T different labels. A subsequence islocally maximal if any further sequential
extension of the subsequence will end up with more than T different labels. To allow the local behavioral context (sensor-triggering events before and after one particular sensor-triggering event)
to be smoothed over time, we use overlapping segments. For instance, if there is a subsequence
“aaabbbccc,” the two overlapping segments will be “aaabbb” and “bbbccc.” If the subsequence is
“abababcccbc,” the two overlapping segments will be “ababab” and “bcccbc.”
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
54:8 C. Li et al.
Figure 3 presents three segments of sensor-triggering events represented by a PTAs with T = 2.
Referring to states q12 in PTA 1 and q34 in PTA 3, their incoming edges are both labeled with
sensor a. Considering merely their incoming edges, the two states could be merged. If we look at
the sensor-triggering events along the whole segment instead, q12 is a state corresponding to the
situation of staying still near sensor b, whereas q34 is corresponding to the situation of frequently
hopping between sensors a and b. The two situations are in fact quite different. We need to define
some “behavior-aware” features so that states like q12 and q34 should not be merged.
The key idea is to make use of some statistics of the time spent near each of the nearby sensors as the behavior-aware features. In particular, we first compute the occurrence frequency of
each sensor as the proxy of the estimated time staying near the sensor. Then, the frequency value
will be discounted if the corresponding triggering events do not occur consecutively. For instance,
Figure 3(c) shows the occurrence frequencies of sensors a and b in PTA 2 and PTA 3, and their unigram statistics are identical. We then consider length-2 sequential patterns, or bigrams, as shown
in Figures 3(d) and 3(e). We use the bigram statistics to discount the unigram statistics for more
accurate estimation of the corresponding portion of time staying close to each of the nearby sensors. For instance, seeing more instances of pattern(a,b) will end up with more of a discount on
the time span near sensor a, while seeing more instances of pattern (a, a) should imply less of a
discount.1
The mathematical formulation of the proposed behavior-aware feature set is defined as follows.
Let Xˆ = (x1, x2,..., xn ) denote an observed segment represented as an ordered set of
sensor-triggering events; Y = (y1,y2,...,ym ) the set of distinct sensor labels within Xˆ; SP =
(yi ∈ Y,yj ≥i ∈ Y ) the set of distinct 2-length sequential patterns of sensor IDs, where (yi,yj) and
(yj,yi ) are considered equivalent and grouped;2 SPa the subset of SP containing the sequential
patterns in SP with at least one of the events labeled as a; Fyi the occurrence frequency of yi in Xˆ;
and F (SPij) the occurrence frequency of the order-invariant sequential pattern (yi,yj) in Xˆ.
For each sequential pattern SPij = (yi,yj), we define ρyx (SPij) as the portion of time staying
near yx ∈ Y within SPij , given as
ρyx (SPij) = |{yx ∩· SPij}|
lenдth(SPij)
,
where yx ∩· SP denotes that the element yx performs an element-wise intersection with the set
SP.
So given a particular state q with the label of the incoming edge Iq ∈ Y, the portion of time
staying near yx averaged over Xˆ is estimated as
ϱq (yx ) =

S Pi j ∈S PIq
ρyx (SPij)P (SPij |SPIq )
=

S Pi j ∈S PIq
ρyx (SPij)
F (SPij)

S Pi j ∈S PIq F (SPij)
. (1)
Definition 3.3. The Frequency-Based Local Mobility (FBLM) feature set for state q is defined
as
fq =
(y1, fq (y1)),..., (y|Y |, fq (y|Y |))
, (2)
1And in general, one can consider n−gram statistics. 2This is because we are only interested in formulating the discounting factor.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.     
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:9
Fig. 4. Visualization of (a) the FBLM features at different states (upper: coarse grained; lower: fine grained)
and (b) the RFBLM features at different states (upper: obtained from two overlapped segments Xˆ1 and Xˆ2;
lower: combined ones).
where
fq (yi ) = ϱq (yi )F (yi )

yl ∈Y ϱq (yl )F (yl )
.
Note that this formulation implies that the states within a segment will share the same feature
set if they have the same incoming edge label. For more fine-grained modeling of the local context
within one segment, ϱq (yx ) in Equation (1) can be computed over a moving window with reference
to state q instead of SPIq . Figure 4(a) shows the two versions of the FBLM feature sets assigned
to different states within a segment. The upper ones correspond to the less fine-grained modeling
version and the lower ones correspond to the more fine-grained one.
Definition 3.3 can be further extended by leveraging the prior knowledge on the spatial arrangement of the installed sensors so that nearby sensors (with different labels) can still be matched
where the missing feature values can be estimated via “smoothing.” For instance, in PTA 2 of Figure 3(b), we extend the feature set from {a,b} to {a,b,c}. More specifically, if sensorc is the common
neighbor of sensors a and b, we set the feature value fq (c) = Δ for state q ∈ {q22,q23,... q27}. If c
is only the neighbor of b, we set fq (c) = Δ exp(−d(c,b)) for only {q25,q26,q27}, where d(c,b) is the
shortest path length between c and b. For 1-hop neighbors, d = 1.
Also, overlapping segments can be considered so as to allow a longer range of local behavioral
context propagation. As each state will have two FBLM feature sets defined, we combine them via
a weighted average based on the length of the overlapping segments as shown in Figure 4(b). This
version of feature set can give more robust results, with the formal definition given as follows.
Definition 3.4. Robust Frequency-Based Local Mobility (RFBLM): Let C = {ci} denote the
set of common neighbors of the sensors in Y andC = {c
i} denote the set of the neighbors of sensor
Iq. We further define the Robust Frequency-Based Local Mobility (RFBLM) feature set for state q
that exists in both segments Xˆi and Xˆi+1 as
fr
q = { (y1, f r
q (y1)),..., (y|Y |, f r
q (y|Y |)),
(c1, f r c
q (c1)),..., (c|C |, f r c
q (c|C |)),
(c
1, f r c
q (c
1)),..., (c
|C |
, f r c
q (c
|C |
)) },
where
f r
q (yi ) = |Xˆi | ∗ fq (yi )|
Xˆi + |Xˆi+1 | ∗ fq (yi )|
Xˆi+1
|Xˆi | + |Xˆi+1 |
f r c
q (ci ) = Δ f r c
q (c
i ) = Δ ∗ exp(−d(c
i, Iq )),
where Δ is the smoothing constant.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018. 
54:10 C. Li et al.
In this article, we consider only the 1-hop neighborhood. Depending on the spatial arrangement
of the sensors, a wider neighborhood can also be exploited. Also, the feature values in the feature
set are normalized for each state before the next step of processing.
ALGORITHM 1: The Overall State-Merging Algorithm
Input: a set of observed sequences D, θ > 0
Output: a PDFA A
A ← BuildinдPTA(D);
Red ← {q0}, Blue ← {qa : qa ∈ δ (q0, E)};
while (qb ∈ Blue) is not empty do
if ∃qr ∈ Red : Iqr = Iqb &&|fr
qr − fr
qb |2 > θ
then
A ← StateMerдe (A,qr,qb );
else
Red ← Red ∪ {qb };
end
Blue ← {qa ∈ δ (qb , E) ∩ qb ∈ Red}\Red;
end
ALGORITHM 2: Building Prefix Tree Acceptor
(PTA)
Input: a set of observed sequences D
Output: a PTA A=(Q, E,q0, δ)
Q ← q0, E ← ∅,QID ← 1;
for xi,j ∈ D do
QID ← QID + 1;
if j == 1 then
δ (q0, xi,j ) ← qQID
end
δ (qQID−1, xi,j ) ← qQID ;
Q ← Q ∪ qQID ;
E ← E ∪ xi,j ;
end
Calculate RFBLM for q ∈ Q\q0 using Equation (3);
The overall procedure of computing the behavior-aware flow graph is summarized in Algorithms 1 and 2. We order the obtained sensor-triggering events by time and merge them in a
depth-first manner so that the resulting mobility flow graph is unique.
The time complexity of the algorithm is O(n2) and the space complexity is O(n), which is linear
with the number of records in the dataset.
3.2.3 Detecting Subflows as Activities Using the Weighted Kernel k-Means. The PDFA obtained
as explained in the previous section summarizes the observed sequences as a directed flow graph.
With the conjecture that the mobility pattern of an activity can be represented as a subflow in the
flow graph, we propose to identify them by applying a graph partitioning method. Here we define
a subflow as a subgraph where the number of edges within the subgraph is relatively higher than
the number of edges going in and out. In the context of activity modeling, a subflow corresponds to
a group of states where an individual, once getting in, will have a higher chance to move according
to the state transitions modeled by the subflow before moving out.
To perform the graph partitioning, we extend a weighted kernel k-means algorithm (Dhillon
et al. 2007) to work on the directed flow graph. As compared to the spectral clustering implementation, the weighted kernel k-means algorithm is more desirable as the high computational cost
to compute the eigenvectors for a large matrix for obtaining the minimum k-cut can be avoided.
Let Q be the finite set of states of the inferred PDFA A and (Q1,Q2,...,Qt ) the set of t disjoint
subflows where their union is Q. The k-cut can be obtained as
kCut(A) = min
Q1,...,Qt
t
c=1
links(Qc ,Q\Qc ) + links(Q\Qc ,Qc )
deд+(Qc ) + deд−(Qc ) , (3)
where links(Qu,Qv ) are the sum of the frequency counts on the transitions between Qu and Qv ,
deд+(Qc ) is the sum of the out-degree of the states in the clusterQc , and deд−(Qc ) is the sum of the
in-degree of the states in the cluster Qc . Given that links(Qc ,Q\Qc ) = deд+(Qc ) − links(Qc ,Qc )
and links(Q\Qc ,Qc ) = deд−(Qc ) − links(Qc ,Qc ), the k-cut can thus be obtained by maximizing
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018. 
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:11
Fig. 5. A subflow discovered from a smart home dataset.
with respect to (Q1,Q2,...,Qt ) the criterion:
t
c=1
links(Qc ,Qc )
deд+(Qc ) + deд−(Qc ) =
t
c=1
x
c Mxc
x
c D+xc + x
c D−xc
=
t
c=1
x
c Mxc
x
c Dxc
=
t
c=1
x˜

c Mx˜ c ,
where M is the adjacency matrix storing the transition frequencies of the states in A, xc is an
indicator vector with its ith element taking the value 1 if cluster c contains state i or 0 otherwise,
D+ is a diagonal matrix with D+
i i = n
j=1 Mij , D− is a diagonal matrix with D−
i i = n
j=1 Mji , D =
D+ + D−, and x˜ c = xc /(x
c Dxc )
1/2.
According to (Dhillon et al. 2007), it can be shown that the weighted kernel k-mean algorithm
can be formulated as a trace maximization problem as
max
D1/2X˜
trace ((D1/2
X˜ )

D1/2
ϕϕD1/2 (D1/2
X˜ )) + constant, (4)
where ϕϕ is the kernel matrix for the data points, x˜ c is the cth column of X˜ , and D is a diagonal
matrix. Thus, one can just use M to replace ϕϕ in Equation (4) and the subflow extraction can
readily be solved using the weighted kernel k-means algorithm.
Figure 5 shows a subflow extracted from a real trajectory dataset (upper) and the corresponding
movement pattern in the smart home (lower). The locations of the sensors are marked with M0XX
in the floor plan, and each edge in the subflow is associated with a sensor label and the transition
probability value. It is obvious that the discovered subflow is corresponding to the activity of meal
preparation.
3.3 Discovering Daily Routines via Bayesian Matrix Factorization
A daily routine refers to a sequential pattern of activities that a person follows within a day. To
discover daily routines, we first make use of the identified activity subflows and detect their occurrence in the data. The data DM×N is then converted to traces of occurrence of t distinct activity
labels in L = (L1,..., Lt ). We denote the converted data as DM×N
A , where each column corresponds
to a sequence of subflow labels for one particular day. To do the conversion, for each column, we
first find the states corresponding to the location of the first sensor-triggering event. Then, states
that can generate the longest subsequence based on the transition function δ would have their subflow labels assigned to the sensor-triggering events. We continue this process until every record in
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.                 
54:12 C. Li et al.
Fig. 6. Probabilistic Nominal Matrix Factorization for daily routine discovery.
DM×N is assigned with a subflow label. We notice that this label assignment process can occasionally end up with occasional transient jumps between two activity subflows due to the relatively
strong assumption of hard-partitioning for the subflow identification. We eliminate these transient
jumps by prohibiting jumps if they are shorter than 1 minute.
After that, each element in DA will take one of the labels in L as its value, forming a nominal
matrix. We assume that DA could have noise and missing labels as discussed before. Thus, a true
label in DA could be replaced by another one. We further assume that the noise is not uniformly
distributed over all the possible labels but follows a distribution so that those more similar to the
true one will have a higher probability to be the replacement. The research problem here is how to
factorize the nominal matrix DM×N
A into K(<< min(M, N)) basis vectors (or daily routines) UK×M
with each element of the vectors corresponding to one of the labels in L as its value, so that matrix
DA can be “well” reconstructed by combining the basis vectors via a coefficient matrix VK×N .
3.3.1 Probabilistic Nominal Matrix Factorization. As presented in Section 2, there exist a lot of
algorithms for matrix factorization (Salakhutdinov and Mnih 2007, Lee and Seung 1999, Schmidt
et al. 2009). However, work on factorizing matrices with nominal values is rare. While one can just
convert each nominal label to a numerical value and then factorize the matrix, this is invalid as the
multiplication and addition operations on labels are undefined. With the assumption that the similarity among the labels is known or can be estimated, we can embed the labels onto a d-dimensional
continuous vector space so that at least some ordering of the labels can be recovered. The value
of d needed is to be set so that the similarity relations of the labels can be preserved. A higher
value of d can keep the similarity information better in general. With the embedding performed,
we can replace all the value of labels in the nominal matrix DA by the corresponding vectors in the
embedding space and factorize it. The problem becomes similar to the ordinal matrix factorization
problem where solutions exist. Our ultimate goal is to come up with a factorization that can ensure
that (1) the basis vectors obtained are robust to noise caused by the imperfect activity identification
step and (2) each row of the matrix DA can be well represented by a small number of basis vectors
as sparse as possible for enhancing the interpretability of basis vectors. The former suggests the
use of a probabilistic model and computes the factorization by arg maxU,V p(DA |U,V ). The latter
suggests that special attention is needed to handle how the basis vectors of the continuous values
should be converted back to the labels with the interpretability considered. We propose a probabilistic nominal matrix factorization with a Bayesian Lasso adopted. The flowchart is illustrated in
Figure 6.
3.3.2 Label Embedding. To embed the labels onto a vector space, we start with the inferred
mobility flow graph and shrink the nodes in each activity subflow to form a metanode. The
set of metanodes will give a new graph A
, where the nodes correspond to the t subflows
(Q := Q1,Q2,...,Qt ) and the edges correspond to links among the subflows that are weighted
by their frequencies. We apply the node2vec algorithm (Grover and Leskovec 2016) to embed the
t distinct labels onto a d-dimensional continuous space. The resulting embeddings of the labels
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:13
can encapsulate local and global structural relationships among the nodes (for our case the similarity of the labels) as far as possible. It then gives a matrix μ := [μL1 ; ...; μLt ] of size t × d, where
μLi : Qi → Rd .
Then, we replace each element in DA by the corresponding d-dimensional vector μ(m,n), resulting in a new matrix Dμ = {μ(m,n) ∈ Rd }, with each element being a vector as visualized in Figure
6. By further unfolding Dμ to [Dμ1 ;Dμ2 ; ... ;Dμd ], a matrix Dunfold of dimension M × d × N will
result. This unfolding step allows most of the existing matrix-related techniques to be applicable
again. However, it also implies that the d dimensions of the vector representation are considered
independently during the factorization. In general, it is not straightforward to ensure that the basis
vectors obtained by the factorization can be “folded” back to give a valid label representation.
3.3.3 An Extended Hierarchical Model. We use a hierarchical model for the factorization. By assuming that the d dimensions of the vector representations of the label are independent, we model
the (soft) range of values of the jth dimension of μLi , denoted as r Li
j (i = 1,...,t, j = 1,...,d),
using a normal distribution N (μ
Li
j , σ Li
j ), where μ
Li
j is the projected value of label Li on the jth
dimension, and σ Li
j indicates the standard deviation of the range corresponding to Li . By defining
дj (m,n) as a latent variable to indicate the particular range that Dμj (m,n) falls into, the conditional
probability of rj (m,n) = Li given дj (m,n) is
p(rj (m,n) = Li |дj (m,n)) = N (дj (m,n); μ
Li
j , σ Li
j )

i N (дj (m,n); μ
Li
j , σ Li
j )
, (5)
where the uncertainty of дj (m,n) is modeled as
p(дj (m,n)|hj (m,n)) = N (hj (m,n), 1). (6)
hj (m,n) can be interpreted as the mean value ofдj (m,n). We then recover the basis vectors from the
matrix h. Specifically, hj (m,n) is modeled as a linear combination of matrices Uj and V with rank
K, given as hj (m,n) = Uj (·,m)
V (·,n) + εj (m,n), where X (·,y) represents the whole yth column
of matrix X, and εj (m,n) ∼ N (0,γ −1) is zero-mean Gaussian noise with γ −1
j being the standard
deviation, i.e.:
hj (m,n) ∼ N (uj (·,m)
v(·,n),γ −1
j ). (7)
3.3.4 Bayesian Lasso. Matrix factorization methods typically allow the observed vectors to be
reconstructed by some linear combinations of the basis vectors. However, for our case, as the elements of the basis vectors correspond to different labels, addition of “labels” is generally an undefined operation under the matrix factorization setting. We adopt the Bayesian Lasso (Park and
Casella 2008) to avoid such addition as far as possible.
It has been pointed out in Tibshirani (1996) that robust Lasso regression in the Bayesian setting
can be achieved by assuming the coefficients v(·,n) to follow:
p(v(·,n)|σ2) =

k
i=1
λ
2
√
σ2 exp 
−λ |v(·,ni )|
√
σ2

.
We adopt this to achieve a sparse structure of the coefficient matrix. According to (Tibshirani
1996) and (Park and Casella 2008), this conditional Laplace prior is equivalent to integrating out
a hierarchical model for vn with addition variables τ 2
1 , τ 2
2 ,..., τ 2
k , where vn is modeled as a zeromean Gaussian with a diagonal covariance matrix scaled by σ and τ , given as
p(v(·,n)|σ2
, τ 2
1 , τ 2
2 ,..., τ 2
k ) ∼ N (0, σ2
Dτ ),
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.   
54:14 C. Li et al.
Fig. 7. The graphical model for factorization (daily routine discovery).
where
Dτ = diaд(τ 2
1 , τ 2
2 ,..., τ 2
k ) σ2 ∼ 1
σ 2dσ2
τ 2
1 , τ 2
2 ,..., τ 2
k ∼

p
j=1
λ2
2 e−λ2τ 2
i /2
dτ 2
i , τ 2
1 , τ 2
2 ,..., τ 2
k > 0,
with τ 2
1 , τ 2
2 ,..., τ 2
k and σ2 assumed independent among them.
For modeling Uj (·,m), γ , and σ Li
j for each of the d dimensions, we simply use the exponential
conjugate families, given as
p(uj (·,m)|μj ,ψj) = N (uj (·,m); μj ,ψj)
p(μj,ψj) = NWi (μj,ψj |μ0j ,κj , νj ,Tj)
= N (μj |μ0j , (κjψj)
−1)Wiνj (ψj |Tj)
p(σ Li
j |α
Li
j , βLi
j ) = Γ(α
Li
j , βLi
j ) ∝ γ α Li
j −1 exp



−
σ Li
j
βLi
j


	
p(γj |αj , βj) = Γ(αj , βj) ∝ γ αj−1
j exp 
−
γj
βj

.
Figure 7 shows the graphical model for the proposed matrix factorization.
3.3.5 Inference. We infer the model using Gibbs sampling. To update the model parameters
{uj (·,m), v(·,n)}, the hyperparameters {μj , ψj , σ2, 1
τ 2 , σ Li
j , γj }, and the latent variables {hj (m,n)}
and {дj (m,n)} on each of the d dimensions, we make reference to (Paquet et al. 2012, Park and
Casella 2008, and DeGroot 2005). The corresponding conditional probabilities for the sampling
steps are listed as follows.
For sampling uj (·,m), v(·,n),
uj (·,m) ∼ N (Σm (ψj μj +γj

hj (m,n)v(·,n)), Σmj ) (8)
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018. 
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:15
Σmj = (ψj +γj

v(·,n)v(·,n)
))−1
vn ∼ N (B−1
U H, σ2
B−1) B = U U + D−1
τ . (9)
For sampling μj , ψj , σ2, 1
τ 2 , σ Li
j , γj ,
μj ,ψj ∼ N (μj, (κjψj)
−1)W iνj (ψj |Tj) (10)
σ2 ∼ Γ−1

n − 1
2
+
k
2
,
(H − Uvn )
(H − Uvn )
2
+ v
nD−1
τ vn
2

(11)
1
τ 2
i
∼ N−1 (μ
, λ
) μ =

λ2σ2
v2
n
λ = λ2 (12)
γj ∼ Γ(αj , βj) σ Li
j ∼ Γ(α
Li
j , βLi
j ). (13)
For sampling h,д and r 3, we first sample д from p(д|r, χ,γ ), where χ = uj (·,m)
v(·,n). In particular, p(д|r, χ,γ ) is derived based on Equations (5) and (6), given as
p(д|ri, χ,γ ) = p(ri |д)p(д|χ,γ )
p(ri |χ,γ ) ∝ p(ri |д)p(f |χ,γ ) = N (д; μLi , σ Li )

i N (д; μLi , σ Li )
N (χ, 1 +γ −1
.
Then, with reference to Equations (6) and (7), we sample h from p(h|д, χ,γ ), given as
p(h|д, χ,γ ) ∝ p(д|h)p(h|χ,γ ) = N (h, 1)N (χ,γ −1) ∝ N 
д +γ χ
1 +γ , (1 +γ )
−1

. (14)
3.4 Folding Up the d Dimensions
After the model inference step, we fold Dunfold back such that each element is represented by a
d-dimensional vector. The predicted value of r(m,n) is the label that gives the highest probability
of generating д in different dimensions of the projected space, given as
r(m,n) = arg max
r(m,n)

d
p(rd (m,n)|fd (m,n)).
4 EXPERIMENTS
In this section, we present the empirical results on evaluating the performance of the proposed
methodology using several publicly available real datasets and illustrate how it can be used to
identify activities and discover the daily activity routines in an unsupervised fashion. The programming code for computing the mobility flow graph, extracting the activity subflows, and
identifying the daily routine analysis have been made available at https://github.com/igeneli/
Smart-house-mining.
4.1 Real Indoor Trajectory Datasets
We apply the proposed methodology to two publicly available datasets containing the trajectories
of a volunteer collected in a smart home setting (Cook 2012). The two datasets are partially labeled
with activity labels like meal preparation, work, sleep, and so forth. In this article, we use the labels
only for evaluation. Statistics of the two datasets are summarized in Table 2. For preprocessing,
we first chop the whole dataset into subsequences by day. Since the dataset also contains traces
of occasional visitors, we consider them as noise. To remove those sensor-triggering events, we
3In the sequel, we denote hj (m, n), дj (m, n), and r Li
j as h, д, and ri , respectively, for the sake of clarity.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.        
54:16 C. Li et al.
Table 2. Statistics of Real Trajectory Datasets
# of Records # of Sensors # of Activities Duration (day) Sampling Interval (sec)
Dataset1 716,798 35 11 200 10
Dataset2 598,988 27 17 52 5
precompute a nearest-neighbor graph of the sensors. If the geodesic distance between two consecutive events is larger than two (i.e., more than two sensors apart), we detect a new visitor and
start tracing the triggering events of the nearby sensors as outliers.
ALGORITHM 3: Gibbs sampling
Input: U,V, μ,ψ, σ,Dτ ,γ , nonempty rmn
repeat
for items m = 1, . . . ,M do
sample hmn |rmn,um,vn,γ (Equation (14));
sample um |hmn,vn,γ , μ,ψ (Equation (8));
end
for items n = 1, . . . ,N do
sample hmn |rmn,um,vn,γ (Equation (14));
sample vn |hmn,um,γ , σ (Equation (9));
end
sample μ|U,ψ and ψ |U Equation (10);
sample 1
τ 2 |V, σ2 and σ2 |V (Equations(11)) and (12));
sample γ |H,U,V Equation (13));
until sufficient samples have been taken;
4.2 Performance on Activity Identification
To measure the effectiveness of the proposed methodology for identifying activities, we make
reference to the ground-truth activity labels and compute the entropy value as the performance
metric:
Entropy =
t
i=1
ni
n E(Qi ) E(Qi ) = − 1
log t
t
j=1
nj
i
ni
log nj
i
ni
,
where t is the number of distinct activity labels, nj
i is the number of records in the subflow Qi with
the ground-truth label of activity j, and ni is the number of records in Qi . The overall entropy
value is the sum of the individual subflow entropies weighted by the subflow size. If almost all the
nodes in a detected subflow are associated with the same activity label, it means that the subflow
is specifically corresponding to one activity and will give the lowest entropy value. If the labels of
the nodes in a subflow are evenly distributed among the possible activities, it will be hard to say
if the subflow is representing an activity or not, and the entropy value will be high.
In our experiments, we use 90% of the data for training the PDFA and the remaining 10% for
testing (i.e., computing the entropy value). As there are some parameters that need to be set
for the proposed methodology, we conduct the experiments to determine the optimal parameter
setting. We first evaluate the use of different orders of statistics in defining the RFBLM feature
set. According to Figure 9(a), we found that as higher-order n-grams are used, the corresponding
RFBLM feature can give a significantly lower entropy value in general. It is consistent with our
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.  
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:17
Fig. 8. (a) Sensitivity of subflow detection quality on different number of subflows. (b) Size of the resulting
PDFA given different threshold values for state merging. (c) Sensitivity of subflow detection quality on given
different θ.
Fig. 9. (a) Performance comparison on subflow detection quality given different n-gram statistics adopted
in RFBLM. (b) Performance of PNoMF on dataset 1 with different numbers of basis vectors. (c) Performance
of PNoMF on dataset 2 with different numbers of basis vectors.
understanding that considering a longer range of sensor-triggering events for representing behavioral patterns can give a more accurate model, which inevitably incurs additional computational
cost. In addition, we conduct experiments to test the effect of different numbers of subflows (t) on
the sensitivity of the subflow detection quality (Figure 8(a)), and the effect of different merging
threshold values (θ) on the size of the inferred flow graph (Figure 8(c)) and the subflow detection
quality (Figure 8(a)). According to Figure 8(a), the entropy value decreases until the number of
subflows is sufficiently large (t = 13). As shown in Figure 8(c), a higher value of θ results in a
smaller PDFA, which is obvious as the algorithm will accept more state-merging tests. For the
entropy value, we see that it fluctuates within a small range given different values of θ as shown
in Figure 8(a). In the sequel, we set n = 4, t = 13, and θ = 0.08.
For performance comparison, we implemented a frequent pattern clustering approach (FP)
(Rashidi et al. 2011) as the baseline. We first demonstrate the effectiveness of adopting the proposed
feature set (RFBLM) for representing each state to infer the flow graph and then the embedded
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
54:18 C. Li et al.
Table 3. Resulting Entropies of Real
Trajectory Datasets
Entropy (FP) Entropy (RFBLM)
Dataset1 0.521 0.398
Dataset2 0.760 0.514
Table 4. The Portion of Activity Labels in Each Identified Subflow of Dataset 1
SF1 SF2 SF3 SF4 SF5 SF6 SF7
Eating 0.01 0.04 0.62 0.10 0.02 0.25
Leave_Home 0.62 0.67 0.06
Meal_Preparation 0.01 0.17 0.09 0.01 0.01 0.69
Relax 0.31 0.11 0.29 0.90 0.84 0.33
Resperate
Sleeping 0.05 0.08
Work 0.01 0.05 0.66
Table 5. The Portion of Activity Labels in Each Identified Subflow of Dataset 2
SF1 SF2 SF3 SF4 SF5 SF6 SF7 SF8 SF9 SF10 SF11 SF12 SF13
Bathe 0.10 0.09 0.05 0.02
Bed_Toilet_Transition 0.01
Cook 0.05 0.40 0.01
Dress 0.04
Eat 0.50 0.30 0.03
Groom 0.23 0.02 0.10 0.13 0.19
Leave_Home 0.98 0.11 0.84 0.98 0.04
Morning_Meds 0.01 0.01
Personal_Hygiene 0.47 0.03 0.54 0.46 0.01 0.29
Read 0.15 0.20 0.11 0.03 0.23
Relax 0.01 0.01 0.18 0.18 0.08 0.06 0.42
Sleep 0.68 0.01 0.28 0.32 0.02 0.02 0.12
Take_Medicine 0.01
Toilet 0.16 0.22 0.15 0.71 0.02 0.05 0.01
Wash_Dishes 0.02 0.13
Watch_TV 0.26 0.09
Work 0.02 0.07 0.01 0.19 0.48 0.05 0.01 0.02 0.01 0.07 0.0
subflows. The entropy values of the clusters and subflows identified using FP and RFBLM, respectively, are shown in Table 3. Tables 4 and 5 show the portions of the ground truth of activity labels
mapped to each subflow based on the PDFA trained using dataset 1 and dataset 2, respectively. We
observed that each identified subflow is mainly mapped to one or two activity labels. In particular,
the identified subflows SF7, SF8, and SF12 in Table 5 are more or less characterizing specifically
the activities “Eat,” “Cook,” and “Leave Home,” respectively. While there are some subflows corresponding to the same activity (e.g., SF8, SF10, and SF11 in Table 5), they are in fact capturing three
different patterns of leaving home via three different doors of the home. In the dataset, they are all
labelled as “Leave Home.” If we keep increasing the value of t, some trivial subflows that are not
corresponding to any ground-truth activity will result. We also notice that some activities cannot
be distinguished by our proposed method as they share very similar movement information. To
verify that, we compute the similarities (based on the KL divergence) among activities according
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:19
Fig. 10. Similarity among activities computed based on the KL divergence of the ground-truth labels. (Cells
in deep blue color indicate pairs of similar activities.)
to the ground-truth labels as shown in Figure 10. For example, in dataset 2, we find that the activities “Bathe,” “Personal Hygiene,” and “Groom” are very similar with respect to the distribution of
sensors triggered by them.
While we can use the identified activity labels as inputs for the subsequent daily routine discovery task, we group SF8, SF10, and SF11 in Table 5 to be all labeled as “Leave Home” for ease
of the latter interpretation. In principle, we can also label SF8, SF10, and SF11 as different ways of
“Leave Home.” We do similar groupings for the two datasets. Finally, we identified five and eight
activities in dataset 1 and dataset 2, respectively, for the subsequent daily routine discovery.
4.3 Evaluation of Daily Activity Routine Discovery
We apply our proposed nominal matrix factorization method to extract daily routines as explained
in Section 3.3. Since we do not have ground-truth information for daily activity routines, we first
made use of a synthetic dataset specially created with ground-truth labels for evaluating the effectiveness of the proposed methods. Then, the two real trajectory datasets were further tested.
4.3.1 Evaluation Metrics. Mean absolute error (MAE) is used to measure the percentage of incorrectly recovered labels with reference to the ground-truth labels. For synthetic data, both the
ground-truth matrix and the ground-truth basis vectors are known. The mean absolute error of
recovering D (data reconstruction MAE) is defined as
MAE = 1
M × N

M
i=1

N
j=1
Dist(rˆij,rij),
where Dist(·) is a distance function defined as Dist(rˆij,rij) = 0 if rˆij = rij or 1 otherwise. Similarly, we can also measure the basis vector reconstruction MAE. We argue that even if the data
reconstruction MAE is small, it does not necessarily mean that the latent basis vectors can be discovered. And if a low MAE for the basis vectors can be obtained with respect to the ground truth,
we consider the discovery to be accurate for human interpretation.
4.3.2 Performance Evaluation. We implemented the proposed method and four related matrix
factorization methods for performance comparison:
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.  
54:20 C. Li et al.
Fig. 11. Visualization of label embedding of synthetic data.
—Ternary Matrix Factorization (TMF) (Maurus and Plant 2014): Nominal matrix is first transformed into a binary matrix by mapping each label to a binary vector. The binary matrix is
then recursively approximated where hard constraints are imposed to make the low-rank
matrices binary.
—Bayesian Nonnegative Matrix Factorization (P_NMF_order) (Schmidt et al. 2009): Labels are
ranked based on the node2vec (d = 1) results and then replaced in order by integers.
—Probabilistic Ordinal Matrix Factorization (POMF) (Paquet et al. 2012): The labels are embedded using node2vec (d = 1) to both positive and negative regions on the real number
line, with the prior on vn similar to that of um.
—Probabilistic Nonnegative Ordinal Matrix Factorization with Lasso (+POMF_Lasso): The labels are embedded to only the positive real number line (d = 1) and Lasso is applied on
vn.
—Probabilistic Nominal Matrix Factorization (PNoMF): This is the method proposed in this
article where the labels are embedded to only the positive side for all the d dimensions, and
the Lasso is applied on vn. Also, the noise model for labels is considered.
4.3.3 Results on Synthetic Data. We generate synthetic datasets with eight labels
{A, B,C,D, E, F,G,H}. We further group the labels into subsets ({A, B}, {C,D}, {E, F }, {G,H}),
where labels within the same subset are supposed to be close to each other. Proximity information
of the labels can be expressed by a weighted adjacency matrix as shown in Figure 11(a). We then
randomly pick one label from each subset and use these four labels to generate four ground-truth
basis vectors. Repeating those basis vectors gives the noise-free data matrix. For evaluation, we
generate three noise-free data matrices (SD1, SD2, SD3) with 384, 576, and 768 elements from
basis vectors of dimension 12, 18, and 24, respectively. We set the number of basis vectors K to
four and check if the four ground-truth basis vectors can be recovered. Also, we add noise to the
data matrices at different levels by randomly replacing labels based on the adjacency matrix to
test their robustness to noise.
Figure 12 shows the experimental results based on SD3. PNoMF(2D) outperforms significantly
all the state-of-the-art factorization methods at all noise levels in terms of MAE for basis vector
discovery while keeping a good performance in data reconstruction. For all the other factorization
methods, even though the data reconstruction MAE is low, the basis vector reconstruction MAE
is large. That is, the discovered latent factors are inaccurate. Similar results are obtained for SD1
and SD2.
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:21
Fig. 12. Results based on synthetic datasets.
Table 6. Mean MAE(±SD) of Smart Home Data
PNoMF (1D) PNoMF (2D) PNoMF (3D) POMF +POMF_Lasso P_NMF_order TMF
Dataset 1 0.429(0.004) 0.359(0.011) 0.304(0.005) 0.462(0.002) 0.548(0.006) 0.635(0.011) 0.307(0.002)
Dataset 2 0.580(0.006) 0.525(0.015) 0.451(0.005) 0.802(0.010) 0.794(0.009) 0.877(0.003) 0.419(0.001)
Table 7. Mean MAE(±SD) of Subsets of Dataset 1
PNoMF (1D) PNoMF (2D) PNoMF (3D) POMF +POMF_Lasso P_NMF_order TMF
DEC 2010 0.500(0.002) 0.201(0.001) 0.235(0.013) 0.458(0.001) 0.517(0.030) 0.838(0.006) 0.179(0.001)
JAN 2011 0.252(0.006) 0.118(0.001) 0.111(0.029) 0.189(0.047) 0.223(0.030) 0.851(0.001) 0.104(0.001)
FEB 2011 0.291(0.004) 0.133(0.012) 0.109(0.002) 0.242(0.006) 0.375(0.001) 0.314(0.003) 0.099(0.003)
MAR 2011 0.406(0.002) 0.270(0.009) 0.262(0.018) 0.377(0.002) 0.415(0.002) 0.508(0.001) 0.277(0.008)
APR 2011 0.299(0.007) 0.167(0.001) 0.161(0.008) 0.331(0.000) 0.351(0.003) 0.598(0.008) 0.166(0.001)
4.3.4 Results on Smart Home Data. We apply PNoMF to two real indoor trajectory datasets. In
our experiment, we set the number of basis vectors as six and four for dataset 1 and dataset 2,
respectively, according to Figures 9(b) and 9(c). For dataset 1, we also split it into five different
subsets, covering the sensor readings collected in December 2010, January 2011, February 2011,
March 2011, and April 2011 for daily routine analysis per month.
As the ground-truth basis vectors are not available, we can only compare the data reconstruction MAE. According to Tables 6 and 7, PNoMF gives the best or close to the best result
as compared to the other methods for data reconstruction. Also, the use of label embedding of
a higher-dimensional space will lower the MAE, which is consistent with our understanding
explained earlier. As PNoMF is designed to find highly interpretable basis vectors without
sacrificing much the matrix reconstruction accuracy, Figure 13 shows the daily routines and the
corresponding coefficient matrices obtained based on the December, January, February, March,
and April data subsets. To ease our interpretation of the daily activity routines obtained for the
5 months, we put a red asterisk with the coefficient of the highest value for each day per month.
Each corresponds to the most representative daily routine for the particular day. For example, by
referring to Figure 13(a), routine #5 can be interpreted as having a day with eating and relaxing
for most of the time with some working hours at around 4 to 6 p.m. And according to Figure 13(b),
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
54:22 C. Li et al.
Fig. 13. Visualization of daily routines and the coefficient matrix obtained from the data subsets of dataset
1 using PNoMF.
this routine happens for most of the days in December 2010 except for some particular days where
they follow other basis vectors, with more time being spent in the kitchen at different periods
of time. Also, we compared the daily routines obtained for different months and found that they
are quite different. For example, there is quite a long period of working time in the afternoon for
routine #5 of December 2010 as compared to a much shorter one around noon for routine #2 of
March 2011. Both are the most representative daily routines for the corresponding months. As
ACM Transactions on Intelligent Systems and Technology, Vol. 9, No. 5, Article 54. Publication date: May 2018.
Automatic Extraction of Behavioral Patterns for Elderly Mobility 54:23
Fig. 14. Visualization of daily routines and the coefficient matrix obtained from dataset 2 using PNoMF.
shown in Figure 14, routine #2 happens for most of the days in dataset 2, which indicates that
the resident wakes up at around 9 a.m., relaxes for most of the time during the day, and has a
tea break at around 3 p.m. As discussed in (Lee and Dey 2011), allowing the elderly to be able to
discuss their own daily routines and exceptions can help them reflect more their daily activities,
which in turn may provide better support to their awareness of the potential deterioration of
their functional capabilities. We believe the methodology developed in this article can provide
more information to the elderly for more in-depth reflection in general.
5 CONCLUSION
In this article, an unsupervised learning methodology is proposed to detect behavioral patterns
for activity identification and daily routine discovery. Our experimental results show that the subflows obtained by applying a weighted kernel k-means to the proposed behavior-aware mobility flow graph can identify more activity-specific behavioral patterns than the existing frequent
pattern-clustering approach. Also, the daily routines discovered based on the proposed probabilistic nominal matrix factorization were empirically shown to be more accurate via experiments on
both synthetic and real human trajectory data. We also discuss how the proposed methodology
can be applied to support human mobility behavior understanding, and the resulting routine patterns can support applications such as anomalous detection (Park et al. 2010) and assisted living
(Lin et al. 2013).
For future work, a more accurate modeling on the local movement patterns can lead to better
subflow detection performance, and the one we proposed is by no means optimal. For daily routine
discovery, the way to label the trajectory data using the detected subflows will affect the subsequent daily routine discovery task. A more robust way for the labeling could be considered. In
addition, some other possible enhancements include (1) how to determine the optimal number of
subflows for a given PDFA, (2) how to model overlapping subflows, and (3) how to carry out the
activity discovery and routine pattern discovery jointly under a unified framework.