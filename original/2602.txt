Multi-source domain adaptation has received considerable attention due to its effectiveness of leveraging the
knowledge from multiple related sources with different distributions to enhance the learning performance.
One of the fundamental challenges in multi-source domain adaptation is how to determine the amount of
knowledge transferred from each source domain to the target domain. To address this issue, we propose
a new algorithm, called Domain-attention Conditional Wasserstein Distance (DCWD), to learn transferred
weights for evaluating the relatedness across the source and target domains. In DCWD, we design a new
conditional Wasserstein distance objective function by taking the label information into consideration to
measure the distance between a given source domain and the target domain. We also develop an attention
scheme to compute the transferred weights of different source domains based on their conditional Wasserstein
distances to the target domain. After that, the transferred weights can be used to reweight the source data
to determine their importance in knowledge transfer. We conduct comprehensive experiments on several
real-world data sets, and the results demonstrate the effectiveness and efficiency of the proposed method.
CCS Concepts: • Computing methodologies → Object recognition; Transfer learning; • Information
systems → Data mining;
Additional Key Words and Phrases: Domain adaptation, multiple sources, optimal transport, attention
1 INTRODUCTION
In many real-world applications, it is difficult to build effective classification models when the
labeled training data are limited. To deal with the problem of lack of labeled data, domain
Fig. 1. An example of multi-source domain adaptation problem: transfer knowledge from source domains
Amazon, Caltech256, and DSLR to the target domain Webcam.
adaptation techniques have attracted considerable attention in recent years [29]. Domain adaptation aims to improve the learning performance on a target domain by leveraging knowledge
from one or multiple source domains, which contain sufficient labeled data with different
distributions compared with the target domain. Domain adaptation has shown superiority in
terms of employing the source domain data to enhance the learning performance of the target
domain. For this reason, it has been applied to various real-world applications, such as object
recognition [45, 52], image classification [24, 42], text classification [41, 46, 47, 54], and so on.
According to the number of source domains for knowledge transfer, domain adaptation can be
summarized into two categories, i.e., single-source domain adaptation [35, 42] and multi-source
domain adaptation [43, 44, 48]. Single-source domain adaptation approaches focus on extracting
knowledge from one single source domain. However, knowledge involved in one source domain is
limited, which restricts the performance in the target domain. Furthermore, if the source domain
is unrelated to the target domain, brute force performing knowledge transfer may hamper the
performance, which is referred to as negative transfer [29]. In contrast, multiple source domains
provide more comprehensive knowledge regarding the target domain. Therefore, it is beneficial
for the target domain to leverage knowledge from multiple source domains. In this article, we
focus on multi-source domain adaptation where the source domains have sufficient labeled data
for training, while the target domain has only a few labeled data [1, 48, 49, 52].
To effectively transfer the knowledge from different domains, the relatedness between domains
should be considered. Figure 1 shows pictures from different domains, where the domains
Amazon, Caltech256, DSLR, and Webcam contain images from Amazon website, Google Images,
digital SLR camera, and web camera, respectively. To classify the category, e.g., calculator, in
the Webcam, which is regarded as a target domain, we can use training data from Amazon, Caltech256, and DSLR, each of which is regarded as a source domain. The relatedness across different
domains differs from each other. Simply training a classifier on all the source data and performing
predictions on the target data may not achieve promising performance, since this method neglects
the different amount of contribution made by each source domain to the target domain.
Various researchers have carried out preliminary investigations on determining whether a
source domain is useful in transferring knowledge to a target domain. Reference [36] proposes
to weigh data samples based on marginal probability differences, and then weigh sources based
on conditional probability differences. References [1, 48] adapt auxiliary classifiers to the target
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
Domain-attention Conditional Wasserstein Distance 44:3
domain, then select the most effective classifiers for knowledge transfer. The selected classifiers
can be considered as the most relevant ones to the target domain.
On the one hand, it has been shown that the performance can be significantly enhanced if the attention mechanism [2] is considered to exploit the relevant parts of the data in the learning tasks
[40]. The application of attention varies in real-world, such as natural language processing [2,
25], image classification [53], and action recognition [22]. Hierarchical attention transfer network
(HATN) [25] transfers attentions for emotions in both word and sentence levels across domains
by automatically capturing pivots and non-pivots. Ji et al. proposed an attention transfer network
called ANT to solve view-invariant action recognition problem [22]. In addition, Liu et al. used
attention mechanism to determine the importance of the shared features for each task and solved
the multi-task problem in an end-to-end scheme [26]. On the other hand, recent studies of optimal
transport [6, 34] have shown that optimal transport is effective in measuring the distance between
different domains. Optimal transport aims to find a way to transport something from one distribution to another with minimal cost. This optimal cost is called the Wasserstein distance [33], which
measures the distance between two distributions.
Motivated by the recent progress in attention learning and optimal transport, in this article, we
propose a novel method, called Domain-attention Conditional Wasserstein Distance (DCWD), to
learn the transferred weights to evaluate the relatedness across the source and target domains.
Specifically, for each source domain and the target domain, we design a conditional Wasserstein
distance objective function by taking the label information into consideration to measure the distance between each source domain and the target domain. We also develop an attention scheme to
learn the transferred weights of different source domains based on their conditional Wasserstein
distances to the target domain. Such transferred weights are used to reweight the source data so
that their importance can be well leveraged. Then, we train an SVM [5] classifier on the weighted
source data and labeled target data to make predictions for the unlabeled target data.
We highlight the contributions of this article as follows:
• We propose a new conditional Wasserstein distance, which takes the label information into
consideration to measure the distance between the source and target domains.
• We develop an attention scheme to compute the transferred weights based on the conditional Wasserstein distances among different domains.
• We conduct extensive experiments on several real-world data sets to demonstrate the effectiveness and efficiency of the proposed method.
The rest of this article is organized as follows: Section 2 reviews some related studies. We provide
detailed descriptions of the proposed method in Section 3, and conduct extensive experiments in
Section 4. Section 5 concludes the work.
2 RELATED STUDIES
2.1 Domain Adaptation
According to the availability of labeled data in the target domain, we divide domain adaptation
problems into two categories: i.e., semi-supervised domain adaptation, where a few labeled target
data are available for learning model [45, 46, 52]; and unsupervised domain adaptation, where no
labeled target data are provided during model training [27, 42, 50].
Most domain adaptation methods adopt two learning strategies: instance reweighting and
feature representation matching. Instance reweighting assigns new weights to the source domain
samples, so that the marginal distribution difference between the source and target domains can
be minimized [7, 8, 21]. For example, Chu et al. learned an SVM classifier by adapting the source
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
44:4 H. Wu et al.
and target marginal distributions [7, 8]. However, this kind of method makes an assumption that
the conditional distributions of the source and target domains are the same [29, 50]. To relax this
assumption, several works turn to feature representation matching strategy, which seeks to map
the source and target data into a shared subspace while preserving the commonalities between
the source and target domains [27, 35]. Geometric Knowledge Embedding (GKE) [42] minimizes
the Maximum Mean Discrepancy (MMD) in a graph convolutional network. Gong et al. proposed
to learn conditional transferable components for domain adaptation, in which an MMD reduction
was included [18]. Correlation Alignment (CORAL) [35] seeks to align the second-order statistics
of the source and target distributions.
The above-mentioned approaches are proposed to solve domain adaptation problems with one
source domain. Recently, knowledge transferred from multiple source domains has been proven to
be effective for boosting domain adaptation performance [36, 37, 51, 52]. This kind of learning paradigm is referred to as multi-source domain adaptation. For instance, Zhang et al. used causal models
to represent the relationship between the features and class and then discovered the most appropriate knowledge extracted from source domains to transfer under different causal assumptions
[51]. Leveraging knowledge from multiple source domains to enhance the learning performance
of the target domain can be considered as a strategy to avoid negative transfer [29]. Recent years
have seen a rapid development of multi-source domain adaptation, and lots of methods have been
proposed. Most of these methods seek to make use of the pre-learned classifiers obtained from the
source domains and target domain [1, 14–16, 48]. Specifically, once the classifiers are trained using some standard learning models, such as SVM [5], one can weight these classifiers, or select the
most effective ones, to obtain a final classifier for the target domain. For example, adaptive SVM
(ASVM) [48] regularizes the distance between the learned model and the source model to learn
from the source model. Duan et al. leveraged a set of pre-learned classifiers by introducing a datadependent regularizer, which is based on a smoothness assumption that the target classifier shares
similar decision values with the relevant base classifiers on the unlabeled target data [14–16].
Another strategy adopted in multi-source domain adaptation methods is to adjust the weights
across domain data, thus the source domains can distribute similarly as the target domain [36, 37,
49]. Yao et al. proposed a boosting-based method, which contains two learning phases: first extract
the knowledge from multiple source domains, and then transfer knowledge into the target domain
[49]. Sun et al. proposed a two-stage multiple sources domain adaptation methodology that first
weighted data samples based on marginal probability differences and then weighted sources based
on conditional probability differences [36]. Moreover, Zhu et al. proposed to adapt both features
and classifier to solve the multi-source domain adaptation problem [55].
2.2 Optimal Transport
Optimal transport aims to move samples from one distribution to another distribution with minimal transport cost, which is also known as the Wasserstein distance [33]. In recent years, optimal
transport [34] has been studied heavily and shown promising performance in many real-world
applications, such as image processing [4, 17, 30], computational fluid mechanics [3], and visual
recognition [10, 46]. Reference [17] proposes a generalization of the discrete optimal transport
that enables to relax the mass conversation constraint and to regularize the transport map. Reference [10] solves the unsupervised domain adaptation by proposing an optimal transport based
framework, which encodes class-structure in the source domain during a transportation plan and
enforces the samples of the same class must undergo a similar transformation. Reference [46] proposes to tackle the semi-supervised heterogeneous domain adaptation by leveraging the entropic
Gromov-Wasserstein discrepancy in an optimal transportation procedure, thus the difference between the metric matrices of two heterogeneous domains can be minimized.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
Domain-attention Conditional Wasserstein Distance 44:5
Table 1. Notations and Descriptions Used in This Article
Notation Description Notation Description
DSk kth source domain DT target domain
XSk kth source feature space XT target feature space
YSk kth source label space YT target label space
XSk kth source data XT target data
XTL labeled target data XTU unlabeled target data
xSk,i a sample of kth source data xT,i a sample of target data
ySk label vector of kth source domain yTL label vector of labeled target domain
NSk sample number of kth source domain NT sample number of target domain
NTL sample number of labeled target
domain
NTU sample number of unlabeled target
domain
P(X) marginal distribution probability P(y|X) conditional distribution probability
M number of classes K number of source domains
a transpose of vector a 1 a vector with all elements one
3 METHODOLOGY
In this section, we begin with the problem definition in this article and then introduce the conditional Wasserstein distance and the attention mechanism based on this distance for learning
transferred weights. Last, we summarize our proposed algorithm and provide the computational
complexity analysis.
3.1 Problem Definition
In this part, we first provide the definitions of terminologies and then describe the notations. For
clarity, Table 1 lists the notations and their descriptions used in this article.
Definition 1 (Domain). A domain D consists of two components: a feature space X = RD where
D is the number of features, and a marginal probability distribution P(x) where x ∈ X.
Definition 2 (Task). Given a specific domain D = {X, P(x)}, a task T consists of two components: a label space Y = {1,..., M} where M is the number of classes, and a classifier f (x) that
can be modeled as a conditional probability distribution P(y|x), where x ∈ X and y ∈ Y.
Let us denote the feature dimension of the source and target domains data by D, and the
number of classes by M. Given a source domain DSk , where k ∈ {1,...,K} represents the kth
source domain, and K is the number of source domains. Denote the data of DSk by XSk =
[xSk,1,..., xSk,NSk ]
 ∈ RNSk ×D , where NSk is the number of samples in XSk . Denote ySk =
[ySk,1,...,ySk,NSk ]
 ∈ RNSk as the corresponding labels of XSk . Similarly, given a target domain
DT , whose data can be represented as XT = [xT,1,..., xT,NT ]
 ∈ RNT ×D , where NT is the number of samples in XT . Following the semi-supervised setting of multi-source domain adaptation
in References [1, 48, 49, 52], we are provided with a few labeled target data and lots of unlabeled
target data, denoted by {XTL , yTL } and {XTU }, respectively. Let NTL and NTU be the number of
samples in XTL and XTU , respectively. Obviously, we have NTL  NTU and NT = NTL + NTU . In
addition, in this problem, both the feature and label spaces are the same in these domains, i.e.,
XSk = XT and YSk = YT . While both the marginal and conditional probability distributions of
these domains are different, i.e., P(XSk )  P(XT ) and P(ySk |XSk )  P(yT |XT ). The goal of our
problem is to make use of the given source and target domains to improve the prediction performance on unlabeled target data XTU . In other words, given K source domains {DSk }
K
k=1 and their
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.      
44:6 H. Wu et al.
learning tasks {TSk }
K
k=1, and a target domain DT and its learning task TT with some labeled target
data {XTL , yTL }, our goal is to improve the prediction performance on unlabeled target data XTU
using the knowledge in domains DSk and DT .
3.2 The Wasserstein Distance
The Wasserstein distance is also known as the Earth Mover’s Distance (EMD) [31]. For example,
we treat the distributions as different heaps of a certain amount of earth, the minimal total amount
of work it takes to transform one heap into the other is denoted as the EMD. And the work here is
defined as the amount of earth in a chunk times the distance it was moved. As a result, computing
the Wasserstein distance means the calculation of the work.
Domain differences are caused by a variety of factors that are difficult to control [6], such as
light conditions, acquisition devices, or even from different image formats, e.g., RGB and HHA.
MMD and f-divergence-based methods fail to adapt between domains once their distributions do
not have significant overlap [6]. However, optimal transport-based methods provide meaningful
distances even when the supports of the distributions do not overlap by exploiting the geometry of
the underlying metric space [9, 10]. In other words, optimal transport-based methods can alleviate
the need of such assumption. Therefore, in this article, to estimate the distance between the source
and target domains, we adopt the Wasserstein distance instead of MMD or f-divergence. Specifically, given the kth source data XSk and the target data XT , we express the empirical probability
distributions of them as
P(XSk ) =
NSk
i=1
P(xSk,i )ϕxSk ,i , P(XT ) =

NT
i=1
P(xT,i )ϕxT,i , (1)
where ϕx is the Dirac function at location x. P(xSk,i ) and P(xT,i ) are probability distributions of
the ith sample from XSk and XT , respectively. NSk
i=1 P(xSk,i ) = NT
i=1 P(xT,i ) = 1. Optimal transport aims to transform data XSk to XT using a way that takes least work; that is to say, to find an
optimal transportation plan from P(xSk,i ) to P(xT,i ).
Kantorovitch [23] proposed a formulation of this problem. Let Θ be the set of probabilistic
couplings between the two empirical distributions and we have
Θ = {θk ∈ (R+)
NSk ×NT | θk 1NT = P(XSk ), θ
k 1NSk = P(XT )}, (2)
where 1NSk = [1,..., 1] ∈ RNSk , 1NT = [1,..., 1] ∈ RNT , and θk is the transportation plan. As
a result, the Wasserstein distance between XSk and XT can be computed as
ρk = min
θ k ∈ Θ
θk , Ψk F , (3)
where ·, ·F is the Frobenius dot product. Ψk ≥ 0 is the cost function matrix and Ψk (i, j) =
ψk (xSk,i, xT,j) denotes the cost to move a probability distribution from xSk,i to xT,j . In this article, we use the squared Euclidean distance to calculate this cost, i.e., Ψk (i, j) = ||xSk,i − xT,j ||2
2 .
3.3 Conditional Wasserstein Distance
The Wasserstein distance can be treated as the distance between two distributions, thus it can
be adapted to evaluate the relationship of two domains. Our goal is to find the optimal transport
distance of the data points with the same labels from the source domain to the target domain. To
achieve this, we design a new conditional Wasserstein distance objective function by taking the
label information into consideration. Figure 2 shows an explanation of the idea above. Define Θm
k
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.       
Domain-attention Conditional Wasserstein Distance 44:7
Fig. 2. An explanation of the conditional Wasserstein distance. The superscripts “1” and “2” indicate the
classes “1” and “2,” the subscripts “1” and “2” represent the source domains “1” and “2,” e.g., η2
1 means the
conditional Wasserstein distance between 1st source domain and the target domain on label 2.
as the set of probabilistic couplings between the distributions of the kth source domain and the
target domain with label m, then we have
Θm
k = {θm
k ∈ (R+)
N m
Sk ×N m
T | θm
k 1N m
T = P(Xm
Sk ), (θm
k )
1N m
Sk
= P(Xm
T )}, (4)
where 1N m
Sk
= [1,..., 1] ∈ RN m
Sk and 1N m
T = [1,..., 1] ∈ RN m
T . m is a label index, N m
Sk and N m
T
represent the number of samples of label m in the source and target domains, respectively. Therefore, the conditional Wasserstein distance between a source domain and a target domain can be
computed as follows:
ηk = 1
M

M
m=1
ρm
k = 1
M

M
m=1
min
θm
k ∈ Θm
k
θm
k , Ψm
k F , (5)
where Ψm
k denotes the cost function matrix of label m.
In multi-source domain adaptation, we usually have only a few labeled data in the target domain.
To compute the term ηk in Equation (5), we utilize the provided labeled target data to train an SVM
classifier and then make predictions for the unlabeled target data. The predicted labels are then
treated as pseudo labels for the computation of Equation (5).
3.4 Domain-attention Conditional Wasserstein Distance
Attention mechanism was first introduced in Reference [2]. The advantage of the attention mechanism is that it provides the ability to focus on the most significant part of inputs to make a good
decision. To perform the attention mechanism, we need to calculate the important coefficients
through a function between the query and each input. It has been shown that the deep learning
performance on various tasks, such as machine translation [12, 39] and image recognition [53],
can be significantly improved if the attention mechanism is utilized.
With the above defined conditional Wasserstein distance, we can design an attention scheme to
learn the transferred weights to leverage the importance of different source domains. Specifically,
we compute the attention coefficients as follows:
ek = −ωη2
k , (6)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.     
44:8 H. Wu et al.
where ω is a hyperparameter to control the spread of η2
k . Then, we normalize the attention coefficients using the softmax function
α = [α1,..., αK ] =
⎡
⎢
⎢
⎢
⎢
⎣
exp(e1)
K
k=1 exp(ek )
,..., exp(eK )
K
k=1 exp(ek )
⎤
⎥
⎥
⎥
⎥
⎦
, (7)
where αk is the transferred weight of kth source domain.
The normalized attention coefficients are taken as the transferred weights to control the importance of different source domains, as well as to combine the data of all the source domains together.
We concatenate all the reweighted source data and the labeled target data to form training data as
follows:
X = [α1XS1 ; α2XS2 ; ... ; αKXSK ; XTL ], y = [yS1 ; ... ; ySK ; yTL ], (8)
where {X, y} represents the combined training data. We learn an SVM classifier on the training
data and make predictions for the unlabeled target data XTU . Our proposed DCWD algorithm is
summarized in Algorithm 1.
ALGORITHM 1: Domain-attention Conditional Wasserstein Distance (DCWD).
Input: XTL , yTL , XTU , XSk , ySk , k ∈ {1,...,K}
1: Train SVM on the labeled target data {XTL , yTL } and make predictions for the unlabeled target
data XTU , the predicted labels are treated as pseudo labels for the calculation of the conditional
Wasserstein distance.
2: for k = 1 to K do
3: Calculate the conditional Wasserstein distance between XSk and XT :
ηk = 1
M
M
m=1 minθm
k ∈Θm
k
θm
k , Ψm
k F .
4: Compute the attention coefficients:
ek = −ωη2
k .
5: end for
6: Normalize the attention coefficients using the softmax function:
α = [α1,..., αK ] =
 exp(e1 )
K
k=1 exp(ek )
,..., exp(eK )
K
k=1 exp(ek )
	
.
7: Use Equation (8) to reweight the source data and combine them with labeled target data to
form training data{X, y}.
8: Apply SVM on the training data to learn a classifier for the unlabeled target data XTU .
3.5 Complexity Analysis
Equation (3) is a linear program, which is a classic problem in optimization and operations research
area. Thus, it can be solved with many existing algorithms, such as the famous simplex algorithm
proposed by Reference [28]. However, the simplex-like methods need to cycle through complex
conditional statements. In this article, we use the algorithm proposed in Reference [11], which
only relies on matrix-vector products. As a result, the computational complexity of Equation (3) is
O(NSkNT (D + t)), where t means the few iterations to converge. Regarding the attention scheme
based on the conditional Wasserstein distance, the computational complexity is O(KMN m
SkN m
T (D +
t)).
4 EXPERIMENT
In this section, we conduct extensive experiments on several real-world data sets. We first provide detailed information about the used data sets and the compared baseline methods. Then, we
report the experimental results to demonstrate the effectiveness and efficiency of the proposed
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.     
Domain-attention Conditional Wasserstein Distance 44:9
Fig. 3. Some example images from the Office, Caltech-256, ImageNet, and Sun data sets.
method. We also provide the parameters sensitivity analysis in this section to show the effects of
the parameters involved in the proposed method.
4.1 Data Sets
We perform experiments on three real-world data sets: Office, Office-Caltech, and Testbed, which
have been widely used to evaluate domain adaptation algorithms.
• Office
The Office data set [32] contains images of 31 categories over three object domains: Amazon
(A) are images downloaded from the Amazon website, DSLR (D) are high-resolution images
obtained from a digital SLR camera, and Webcam (W) are low-resolution images taken from
a web camera. We use the DeCAF6 [13] and ResNet50 [20] features with 4,096 and 2,048
dimensions, respectively. In experiments, we randomly select 20 samples per class from
Amazon and 8 samples per class from DSLR and Webcam if they are source domains, while
3 samples per class are selected if they are target domains, and the rest data in target domain
are used for testing.
• Office-Caltech
The Caltech-256 (C) data set [19] includes 256 categories, among which 10 categories are
overlapped with the Office data set. We train the four domains (A, D, W, and C), which are
drawn from the Office and Caltech data sets, in a convolutional neural network. Then, we
extract the output of the seventh layer, i.e., DeCAF7. The feature dimension after training
is 4,096. In experiments, we randomly select 20 samples per category from Amazon and 8
samples per category from DSLR, Webcam, and Caltech if they are source domains, while
3 samples per category are selected if they are target domains, and the rest data in target
domain are used for testing.
• Testbed
The Testbed data set [38] contains 10,473 images with 40 categories. This data set is collected from three domains: Caltech256 (C) contains 256 categories with a minimum of 80
and a maximum of 827 images, ImageNet (I) contains around 21K object classes organized
according to the Wordnet hierarchy, Sun (S) contains a total of 142,165 pictures, and it was
created as a comprehensive collection of annotated images covering a large variety of environmental scenes, places, and objects. We use the shared 40 classes in these three domains
and use the publicly available DeCAF7 features with 4,096 dimensions [35]. Specifically, we
randomly select 10 samples per class from C, I, and S if they are source domains, while 3
samples per class are selected if they are target domains, and the rest data in target domain
are used for testing.
We list the statistical information of the data sets in Table 2 and show some example images
collected from these data sets in Figure 3. Specifically, we construct learning tasks by treating one
domain as the target domain and the rest domains as the source domains. For example, on the
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
44:10 H. Wu et al.
Table 2. Statistical in Formation of the Data Sets
Data set Domain #Samples #Features #Classes NSk /M NTL /M
Office Amazon (A) 2,817 4, 096/2, 048 31 20 3
DSLR (D) 498 8
Webcam (W) 795 8
Office-Caltech Amazon (A) 958 4,096 10 20 3
DSLR (D) 157 8
Webcam (W) 295 8
Caltech (C) 1,299 8
Testbed Caltech (C) 3,847 4,096 40 10 3
ImageNet (I) 4,000
Sun(S) 2,626
Office-Caltech data set, task TA means domain A is the target domain, domains C, D, and W are
the source domains. As a result, we obtain 13 learning tasks by applying this way to the three data
sets.
4.2 Baseline Methods
—SVM [5]. We directly train an SVM classifier on the labeled target data and then make predictions for the unlabeled target data, thus SVM is a baseline method without considering
knowledge transfer.
—SVM-T [5]. We combine all the source data with labeled target data to form one source
domain and learn an SVM classifier on it. Then, we use this classifier to predict labels for
the unlabeled target data. This is a knowledge transfer method that treats all the domain
data equally.
—TJM [27]. Transfer Joint Matching (TJM) jointly reweights source data and learns a new
feature representation to minimize the domain difference, and then trains a classifier on the
new representation. We combine all the source domains into one domain and perform TJM
to make predictions.
—ASVM [48]. Adaptive SVM (ASVM) adapts auxiliary classifiers learned from source domains,
where the most effective classifiers are selected to obtain the predictions.
—MSTrAdaBoost [49]. MultiSourceTrAdaBoost (MSTrAdaBoost) learns a candidate weak classifier in an iterative procedure for each source independently, and the final weak classifier
is chosen from the source that minimizes the target classification error.
—PMTSVM [1]. Projective Model Transfer SVM (PMTSVM) minimizes the projection of models onto a separating hyperplane orthogonal and selects the most effective classifiers to
make predictions.
—LSDT [52]. Latent Sparse Domain Transfer (LSDT) simultaneously learns a sparse coefficient
matrix and a low-dimensional latent space projection, and then an optimal subspace is obtained to learn a classifier for the target domain. LSDT is considered as a state-of-the-art
method for multi-source domain adaptation.
For all the above baseline methods, we use the default parameters recommended by their original
papers and run 20 random trials to obtain their mean accuracy. We adopt SVM [5] as the base
classifier and set the trade-off parameter C = 1 for all the compared domain adaptation methods.
Specifically, in DCWD, we empirically set ω = 1000 for the Office data set with ResNet50 features,
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
Domain-attention Conditional Wasserstein Distance 44:11
Table 3. Results (Mean (%) ± Standard Deviations) on the Office Data Set with DeCAF6 Features
Methods TA TD TW Average
SVM 53.67±2.06 78.52±2.17 78.08±1.87 70.09
SVM-T 50.36±1.77 78.56±2.78 80.21±1.79 69.71
TJM 39.03±1.02 77.46±1.51 71.65±2.02 62.71
ASVM 52.68±2.44 80.27±3.83 80.58±2.90 71.18
MSTrAdaBoost 51.87±2.20 78.31±2.34 82.19±2.06 70.79
PMTSVM 53.11±2.04 79.26±2.82 80.11±2.19 70.83
LSDT 55.22±1.31 84.16±1.73 85.58±1.41 74.99
DCWD 55.40±1.23 88.09±2.02 88.85±1.09 77.45
Table 4. Results (Mean (%) ± Standard Deviations) on the Office Data Set with ResNet50 Features
Method TA TD TW Average
SVM 83.15±0.83 93.60±1.80 92.92±1.19 89.89
SVM-T 75.35±0.97 91.05±1.75 89.32±1.30 85.24
TJM 72.38±1.33 95.01±1.11 93.58±1.00 86.99
ASVM 84.56±0.81 95.16±1.90 94.21±1.18 91.31
MSTrAdaBoost 83.74±0.85 95.67±1.27 94.57±1.42 91.33
PMTSVM 84.97±0.80 96.43±1.51 95.34±1.17 92.25
LSDT 83.34±0.37 96.62±0.94 95.43±0.59 91.80
DCWD 84.13±1.03 97.78±0.66 96.75±0.80 92.89
Table 5. Results (Mean (%) ± Standard Deviations) on the Office-Caltech Data Set
Methods TA TC TD TW Average
SVM 88.46±1.61 79.01±3.01 90.71±4.57 90.28±3.52 87.12
SVM-T 89.40±1.30 82.69±1.43 92.64±3.23 93.21±2.57 89.49
TJM 89.83±1.20 82.08±1.60 97.13±1.74 96.02±1.48 91.26
ASVM 89.86±1.22 82.05±2.52 92.64±4.18 92.23±3.14 89.19
MSTrAdaBoost 88.90±1.46 84.93±1.18 93.23±3.15 94.23±2.62 90.32
PMTSVM 89.77±1.33 81.40±2.64 92.48±4.37 92.40±3.06 89.01
LSDT 92.30±0.61 88.15±0.74 96.81±1.54 96.28±1.24 93.39
DCWD 92.18±0.67 86.78±1.04 98.78±1.29 98.34±1.30 94.02
ω = 100 for the Testbed data set and Office data set with DeCAF6 features, and ω = 10 for the
Office-Caltech data set.
4.3 Effectiveness Evaluation
We use the term mean and standard deviations to evaluate the performance of all the compared
algorithms. Tables 3, 4, 5, and 6 present the results on the Office with DeCAF6 and ResNet50 features, Office-Caltech and Testbed data sets, respectively. We also list the average accuracy of all
the compared algorithms in the tables. For a clear perspective, we plot the results in Figure 4. On
most tasks, our proposed algorithm achieves better or comparable performance compared with all
the tested algorithms. We draw several interesting observations as follows:
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
44:12 H. Wu et al.
Table 6. Results (Mean (%) ± Standard Deviations) on the Testbed Data Set
Methods TC TI TS Average
SVM 53.27±1.57 43.64±1.91 19.73±1.57 38.88
SVM-T 59.75±0.84 46.25±1.62 19.09±0.96 41.70
TJM 45.24±1.97 41.83±1.44 18.92±1.05 35.33
ASVM 62.13±1.69 53.76±1.72 22.07±1.46 45.99
MSTrAdaBoost 59.36±3.38 50.11±4.00 21.80±1.95 43.76
PMTSVM 59.37±1.71 49.93±1.88 20.82±1.42 43.37
LSDT 68.57±1.16 63.04±0.87 25.98±0.84 52.53
DCWD 70.85±1.25 63.32±0.90 26.10±1.02 53.42
Fig. 4. Visualization of accuracy (%) of different methods: (a) presents the results on the Office data set with
DeCAF6 features; (b) presents the results on the Office data set with ResNet50 features; (c) and (d) report
the results on the Office-Caltech and Testbed data sets, respectively.
• All the methods except SVM take domain adaptation into consideration and outperform
SVM on average on most tasks, which indicates that leveraging knowledge from auxiliary
data can exactly improve the learning performance on the target tasks. Moreover, we observe that SVM-T performs worse than SVM on a few tasks; the reason is that SVM-T performs knowledge transfer without considering domain relatedness. The source domain that
has less relatedness to the target domain may harm the performance, which is also known
as the negative transfer.
• DCWD obtains better or comparable results compared with TJM and LSDT, which demonstrates that the use of attention mechanism is able to learn effective transferred weights,
thus the importance of each source domain to the target domain is well revealed and
leveraged.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
Domain-attention Conditional Wasserstein Distance 44:13
Table 7. Results (Mean (%) ± Standard Deviations) with Different Numbers of Source Domain on Task TD
on the Office-Caltech Data Set
Method {A, C} → D {A, W} → D {C, W} → D {A, C, W} → D
SVM 92.64±3.23 92.64±3.23 92.64±3.23 92.64±3.23
SVM-T 81.02±4.43 91.34±2.03 91.65±1.71 90.71±4.57
TJM 89.06±3.64 95.04±2.62 95.98±2.15 97.13±1.74
ASVM 91.81±4.17 92.95±4.02 92.83±4.16 92.64±4.18
MSTrAdaBoost 91.42±4.44 93.11±4.28 92.95±4.21 93.23±3.15
PMTSVM 87.40±4.58 91.46±4.11 94.06±2.43 92.48±4.37
LSDT 91.73±0.93 96.46±1.24 96.14±0.87 96.81±1.54
DCWD 92.68±1.86 97.80±1.10 97.87±1.23 98.78±1.29
• Compared to ASVM, MSTrAdaBoost, and PMTSVM, DCWD consistently achieves better accuracy, which means that learning transferred weights to determine the amount of knowledge transferred from each source domain to the target domain is beneficial to domain
adaptation.
• DCWD achieves better or comparable performance in comparison with all the baseline
methods, which reveals that applying attention mechanism to the proposed conditional
Wasserstein distance can boost the domain adaptation performance.
4.4 Effect on the Number of Source Domain
To investigate the effect of different numbers of source domain on the learning performance, we
take the learning task TD on the Office-Caltech data set as an example and report the results in
Table 7. Specifically, {A, C} → D means domain D is the target domain and domains A and C are
the source domains. From Table 7, we can draw several observations as follows:
• Domains D and W distribute very similarly while both of them distribute significantly different to domains A and C [42], which can also be observed from images in Figure 3. Therefore,
the performance of this task is good when domain W is involved as a source domain.
• In general, for the domain adaptation methods, performance on task adopted more source
domains is better than that on task using less source domains, which means domain adaptation is useful in extracting knowledge from source domains. However, for SVM-T, the
performance on tasks {A, W} → D and {C, W} → D is better than that on task {A, C, W}
→ D, which indicates that equally treating all the source domains can cause the negative
transfer issue.
• Compared to all the baseline methods, DCWD consistently achieves the best results, which
demonstrates the effectiveness of DCWD. In addition, in some baseline methods such as
ASVM, MSTrAdaBoost, and LSDT, the accuracy on tasks {A, W} → D, {C, W} → D, and {A,
C, W} → D are comparable. However, in DCWD, the performance on task {A, C, W} → D
is better than that on tasks {A, W} → D and {C, W} → D, which indicates that DCWD can
extract more informative knowledge from the given source domains than baseline methods.
This demonstrates the effectiveness of the proposed attention scheme and further validates
the motivation of transferring knowledge from multiple source domains.
4.5 Efficiency Evaluation
In this experiment, we empirically investigate the efficiency of our proposed DCWD. We take
the learning tasks on the Office-Caltech data set as examples and conduct the evaluation on a
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
44:14 H. Wu et al.
Table 8. Running Time (s) of all the Learning Tasks on the Office-Caltech Data Set
Methods TA TC TD TW
SVM 1.47 1.90 0.33 0.47
SVM-T 2.73 4.37 2.12 2.29
TJM 24.34 42.93 6.74 9.66
ASVM 153.13 289.60 279.52 278.01
MSTrAdaBoost 95.75 179.21 130.15 135.36
PMTSVM 4367.08 4456.51 4450.57 4462.11
LSDT 19.91 25.21 16.80 17.05
DCWD 4.59 5.73 2.12 2.54
Fig. 5. Running time(s) with different NSk /M and M of the proposed method.
workstation with Xeon 3.40 GHz CPU and 16 GB of RAM. Specifically, we run one trial to obtain
the running time of all the compared algorithms and report the results in Table 8. We observe that
SVM achieves the shortest running time, because its procedure only involves target data. ASVM
and PMTSVM take much more time, because they have to solve quadratic programming problems
with heavy computations. In addition, DCWD achieves comparable running time compared with
SVM-T, but obtains better or comparable results compared to all the baseline methods, which
demonstrates the efficiency of the proposed method.
To go deeply for evaluating the efficiency of the proposed method, we generate simulated data,
in which the settings are as follows: the number of source data K is set to be 3, the dimension
of feature D is set to be 100, the numbers of labeled and unlabeled target data per category are
set to be 3 and 30 (i.e., NTL  NTU ), respectively. Then, we vary the number of each source data
per category NSk /M in the range {10, 20,..., 100}, and the number of category M in the range
{5, 10, 15,..., 50}. We vary one of them in the range space while fix the other and run one trial to
obtain the CPU running times, which are reported in Figure 5. We observe that both NSk and M
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
Domain-attention Conditional Wasserstein Distance 44:15
Table 9. Results of Different Cost Function Matrices on the Office-Caltech Data Set
Tasks Squared Euclidean Jensen-Shannon Cosine
TA 92.18±0.67 91.33±1.01 92.16±0.67
TC 86.78±1.04 82.72±1.97 86.77±1.05
TD 98.78±1.29 98.78±1.29 98.78±1.29
TW 98.34±1.30 98.34±1.35 98.34±1.30
influence the running time of the proposed method significantly, and given a fixed value of NSk
(M), a larger value of M (NSk ) can lead to the increasing time-consumption.
4.6 Comparison to Other Distance
In this article, for the cost function matrix in Equation (5), we use the squared Euclidean distance. In this part, for full comparison, we study the effect on this cost function matrix by replacing the squared Euclidean distance with the Jensen-Shannon distance and the cosine distance,
which can be represented as Ψm
k (i, j) = (P(xm
Sk,i ) log(
P(xm
Sk ,i )
P(xm
T,j ) )) + (P(xm
T,j) log(
P(xm
T,j )
P(xm
Sk ,i ) )) and
xm
Sk ,i • xm
T,j
| |xm
Sk ,i ||||xm
T,j | | , respectively. Specifically, for the cosine distance, we here define it as Ψm
k (i, j) =
1 − xm
Sk ,i • xm
T,j
| |xm
Sk ,i ||||xm
T,j | | . We use the learning tasks on the Office-Caltech data set as examples and report
the results in Table 9. We observe that different cost function matrices have different effects on the
performance. Specifically, the results of squared Euclidean distance show roughly similar to that
of the cosine distance, and show different to that of the Jensen-Shannon distance in a few tasks.
4.7 Effect on Different NSk and NTL
We evaluate the effects of different sample number of the source data and the labeled target data
on task TC on the Testbed data set. The sample number of each source data NSk is set to be 1, 5, 10,
15, and 20 per category, respectively. The sample number of labeled target data NTL is set to be 1, 3,
6, 9, and 12 per category, respectively. We vary one of them in the range space while fix the other,
and test every combination of NSk /M and NTL /M by conducting one trial in each experiment. We
report the error rate of the proposed method in Figure 6. Two observations can be drawn from this
experiment. First, for a fixed value of NSk (NTL ), a larger value of NTL (NSk ) is beneficial to achieve
better performance. Second, in general, as the sample number of the source data and the labeled
target data increases, the performance increases and reaches a stable result.
4.8 Parameters Sensitivity Analysis
4.8.1 Effect on the Parameter ω. We use the learning tasks on the Office-Caltech data set as
examples to study the sensitivity of parameter ω. Specifically, we vary ω ∈ {0.01, 0.1, 1, 10, 100}.
Figure 7(a) presents the accuracy w.r.t. different values of ω. The results show that different tasks
obtain their best performance with different values of ω, but achieve roughly the same curve trend.
For instance, as ω increases, the accuracy of task TA (respectively, TC) increases and obtains the
best performance when ω = 10 (respectively, 1), and then decreases. We have similar observations
on other data sets.
4.8.2 Effect on the Parameter C. For the trade-off parameter C, we use the learning task TD on
the Office data set as an example. Specifically, we vary C ∈ {2−5, 2−4,..., 25} and run one trial to
obtain the results, which are presented in Figure 7(b). To make a clear perspective, we also plot the
results of the second-best method, i.e., LSDT. We observe that the accuracy of DCWD and LSDT
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.  
44:16 H. Wu et al.
Fig. 6. Error rate (%) of DCWD with different NSk and NTL on task TC on the Testbed data set.
Fig. 7. Sensitivity analysis w.r.t. parameters ω and C on the Office-Caltech data set.
change significantly with differentC, and the curves of these two methods follow a similar trend. In
addition, the stable accuracy curves ensure that DCWD can outperform LSDT consistently. Similar
observations can be drawn on other tasks.
5 CONCLUSION
In this article, we study the problem of multi-source domain adaptation. The key challenge in
multi-source domain adaptation is how to determine the amount of knowledge transferred from
each source domain to the target domain. We proposed a new algorithm, called Domain-attention
Conditional Wasserstein Distance (DCWD), for this issue. DCWD models the attention on different
source domains as the transferred weights based on their conditional Wasserstein distances to the
target domain. These weights are then treated as the ensemble weights to estimate the relatedness
across different domains. We weigh the source data using transferred weights and concatenate
them with the labeled target data to form training data, on which we learn the classifier for the
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 4, Article 44. Publication date: May 2020.
Domain-attention Conditional Wasserstein Distance 44:17
unlabeled target data. We conduct comprehensive experiments on several real-world data sets, and
the results demonstrate the effectiveness and efficiency of the proposed method.
Currently, the proposed method adopts a two-step learning paradigm, which learns the weights
and trains the classifier separately. This may result in suboptimal solution. In the future, we will
investigate how to perform these two tasks in a jointly learning model. It is worth noting that
the attention scheme is used mostly in Deep Neural Networks (DNNs). However, it is challenging
to introduce the Wasserstein distance into the attention mechanism in DNNs, thus, we leave this
work for our future direction. Another interesting topic is to process target data in an incremental
manner, which is also related to the problem of online transfer learning. We leave the study on
this learning paradigm for our future direction.