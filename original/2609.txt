Multimodality in dialogue systems has opened up new frontiers for the creation of robust conversational
agents. Any multimodal system aims at bridging the gap between language and vision by leveraging diverse
and often complementary information from image, audio, and video, as well as text. For every task-oriented
dialog system, different aspects of the product or service are crucial for satisfying the user’s demands. Based
upon the aspect, the user decides upon selecting the product or service. The ability to generate responses
with the specified aspects in a goal-oriented dialogue setup facilitates user satisfaction by fulfilling the user’s
goals. Therefore, in our current work, we propose the task of aspect controlled response generation in a
multimodal task-oriented dialog system. We employ a multimodal hierarchical memory network for generating responses that utilize information from both text and images. As there was no readily available data for
building such multimodal systems, we create a Multi-Domain Multi-Modal Dialog (MDMMD++) dataset. The
dataset comprises the conversations having both text and images belonging to the four different domains,
such as hotels, restaurants, electronics, and furniture. Quantitative and qualitative analysis on the newly
created MDMMD++ dataset shows that the proposed methodology outperforms the baseline models for the
proposed task of aspect controlled response generation.
CCS Concepts: • Computer systems organization → Embedded systems; Redundancy; Robotics; • Networks → Network reliability;
Additional Key Words and Phrases: Multimodal dialogue system, response generation, memory network
1 INTRODUCTION
Advancement in Artificial Intelligence (AI) has opened up new frontiers in conversational agents.
Human-machine interaction is an essential application of AI helping humans in their day-to-day
lives. Progress in AI has led to the creation of personal assistants like Apple’s Siri, Amazon’s Alexa
and Microsoft’s Cortana that assist humans in their everyday work. The machines’ capability to
comprehend and complete the user’s goals has empowered researchers to build advanced dialogue
systems. With the progress in visual question answering [25, 65] and image captioning [23, 67],
the use of different modalities in dialogue agents has shown remarkable performance bringing the
different areas of Computer Vision (CV) and Natural Language Processing (NLP) together. Hence,
a multimodal dialogue system bridges the gap between vision and language, ensuring interdisciplinary research. Integration of information from different modalities such as text, image, audio,
and video has been known to provide complete details for building effective end-to-end dialogue
systems [27, 47].
Lately, several works on multimodal dialogue systems [7, 14, 28] have encouraged research in
this direction by combining information from different modalities, such as texts, audios, videos,
and images. Multimodal conversational systems provide completeness to the existing dialogue systems by providing necessary information that lacks in unimodal systems. The visual information
(extracted from the image and video) and the audio help build a robust system.
1.1 Problem Definition
Dialogue systems are grouped into two broad categories, viz. open-domain conversational agents
[29, 52] and goal-oriented dialogue systems [44, 59]. Response generation or Natural Language
Generation (NLG) is an important component of these systems, handling the task of presenting
the information to the user. One of the AI objectives is to combine vision and language in order
to develop a powerful dialogue system. Task-oriented conversational agents are majorly based
on the unimodal (textual) information. Growing requirements in various fields such as travel, entertainment, retail, and the like require conversational agents to communicate by incorporating
information from the different modalities to build a robust system. Our current work focuses on
generating responses in a multimodal setup by employing information from both textual and visual
features. In response generation, we take a step ahead by proposing the task of aspect controlled
response generation for multimodal dialogue systems. Aspect denotes an attribute or a property of
the task-oriented systems. For example, a customer interested in purchasing a laptop considers all
the product elements, such as the price, weight, storage, color, and so on. Hence, a conversational
agent having the capability of generating responses conditioned on different aspect information
is essential for building a robust goal-oriented multimodal system.
1.2 Motivation and Contribution
Generation of responses is one of the primary tasks of every dialogue system. In the case of a goaloriented conversational agent, it is essential to satisfy the users’ goals or objectives. To fulfill the
user’s goals or objectives, it is crucial to achieve the detailed information of the service required
by the user. For example, for the restaurant domain, the conversational agent needs to know about
the price range, cuisine information, number of people, and so on, for suggesting the best available
option to the user. Similarly, in case of different domains, such as electronics, furniture, and hotels, aspect information of each domain is a crucial characteristic for generating informative and
more engaging responses. Hence, it can be established that aspect information is significant for
generating correct responses and achieving user objectives.
Multimodal dialogue systems utilize information from the complementary sources, such as
videos and images along with textual information to generate the responses. In Ref. [47], the authors proposed a multimodal dialogue dataset having textual and image information for the fashion domain. From the dataset, it is clear that image information is necessary for selecting the right
clothes and accessories for different individuals. In other domains, visual information is equally
essential for facilitating a proper understanding of the product and need of the user. From visual
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:3
Fig. 1. Domain-wise examples from the MDMMD++ dataset.
information in the form of images, the user can decide upon the food it wants to eat, the room it
wants to book, or the furniture it intends to order.
Due to the unavailability of a multimodal dataset for different domains having aspect information, we firstly create a Multi-Domain Multi-Modal Dialogue (MDMMD++) dataset. The primary
objective of creating this multi-domain conversational dataset is due to the sole reason that there
is no multimodal dataset comprising information from different domains, representing real-life
scenarios.
In Figure 1, we provide some examples of the newly created MDMMD++ dataset having conversations belonging to the four domains, such as restaurants, hotels, electronics, furniture, and so on.
The figure shows that images are equally important in any task-oriented system, like textual information. In Example 1 of the figure belonging to the restaurant domain, pictures of different food
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:4 M. Firdaus et al.
Fig. 2. Cross-domain examples from the MDMMD++ dataset.
items are essential as, many times, by the name of the items, it is not clear what the item is. Also,
the visual features provide a strong impact and make the conversation more interesting and clear.
Similarly, visual information is essential for the other related domains, as shown in the different
examples of the given figure. The dataset even comprises cross-domain conversations, as shown in
Figure 2. These conversations provide inter-domain diversity, making the dataset natural, exciting,
and challenging for response generation.
Aspect information varies from domain to domain. Every domain has domain-specific aspects
along with some elements common to the different domains. In case of the restaurant domain,
the cuisine is a domain-specific aspect. In Example 1 of the figure, breakfast is a critical aspect
for suggesting the correct food items. Similarly, in the hotel domain, the user wants to stay in
“5-star rating hotels.” Here, the hotel rating is an essential aspect for selecting the best hotels. In
Examples 3 and 4, belonging to the electronics and furniture domain, the user inquires about the
product’s color, which is one of the aspects in this case. From the above examples, it is evident
that aspect information is vital in every goal-oriented dialogue system. The ability to generate
responses according to the specified aspects is imperative to achieve the users’ desired goals. This
framework focuses explicitly on the use of aspect information, which formulates as prior information to the decoder. This aspect information directs the agent in shaping interesting and insightful
conversational responses. Given an input message, conversational history, and the current aspect
information, the task is to generate the best possible response.
In brief, the contributions of our present work can be summarized as below:
—We propose the task of aspect controlled response generation in a multi-domain taskoriented multimodal dialogue system. To the best of our knowledge, this is the very first
attempt for aspect-controlled NLG.
—We create a large MDMMD++ dataset comprising natural conversations with text and images. The dataset consists of four domains, viz. restaurant, hotel, electronics, and furniture,
with every conversation labeled with its corresponding aspect information. The dataset also
comprises cross-domain conversations.
—We propose a multimodal hierarchical memory network based framework for generating
the responses conditioned on the aspects while providing the aspect information directly
to the decoder.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:5
—We design human evaluation metrics to assess the effectiveness of our proposed framework
on the newly created MDMMD++ dataset.
—Experimental results on both automatic and human evaluation metrics prove that our proposed framework outperforms the baseline methods for generating responses with specified
aspects in a multimodal setup.
The rest of the article is structured as follows. In Section 2, we present a brief review of the
existing literature. In Section 3, we provide the details of data creation and present the detailed
statistics. We provide the details of the baseline and the proposed methodology in Section 4. Implementation details, along with the evaluation metrics, are discussed in Section 5. Experimental
results are presented in Section 6 along with detailed analysis, including error analysis. Finally, in
Section 7, we conclude along with future directions of research.
2 RELATED WORK
NLG is a classical problem in every dialogue system. In recent times, with the rapid growth of AI,
there is a trend to build multimodal dialogue systems by incorporating images, audio, and video
modalities along with text. Below we present a brief overview of some of the works carried out in
unimodal conversational agents, followed by the multimodal dialogue systems.
2.1 Unimodal Dialogue Systems
Deep learning’s efficacy has shown notable improvement in the generation of dialogue. As shown
in Refs [51] and [59], deep neural models are quite successful in modeling the conversations. To
capture the context of the previous queries by the users, the authors in Ref. [54] proposed a hierarchical framework capable of preserving the past information. Similarly, to preserve the dependencies among the utterances in a dialogue, the hierarchical encoder-decoder framework was investigated in Refs [48] and [49]. The authors in Ref. [66] extended the hierarchical encoder-decoder
framework by adding a latent variable for understanding the intentions of the conversations in a
task-oriented dialog system. Lately, memory networks [35] have been intensely investigated for
capturing the contextual information in dialogues for the generation of responses infusing pointer
networks.
Hierarchical pointer networks [42] have also been employed for response generation in taskoriented dialogues. The authors in Ref. [64] incorporated a global encoder and a local decoder to
share external knowledge in a task-oriented dialogue setup. The ability to infuse knowledge in
responses was achieved by using a Bag-of-sequence memory unit [43] for generating the coherent
responses in goal-oriented dialog systems. The authors in Ref. [46] proposed a multi-level memory framework for task-oriented dialogues. A memory-augmented framework with the ability to
extract meaningful information during training for better response generation has been explored
in Ref. [58].
With the release of MultiWoz, a task-oriented dialog dataset, several works have focused on
multi-domain dialogue generation. The authors in Ref. [5] used a pre-trained language model for
dialog generation. A hierarchical graph framework employing the dialogue acts of the utterances
was investigated for dialogue generation in Ref. [10]. The meta-learning approach [36, 41] has been
applied on different datasets to increase the domain adaptability for generating the responses. To
increase the ability to memorize the dialogue context, the authors in Ref. [63] used a memoryto-sequence framework and the pointer generator for response generation. A multi-task framework to enhance the performance of natural language generation was investigated in Ref. [68]. In
Ref. [11], working memory was employed for dialog generation. The working memory interacts
with two long-term memories that capture the dialog history and the knowledge base tuples for the
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:6 M. Firdaus et al.
informative response generation. Recently, heterogeneous memory network [33] has been explored for response generation having the capability to simultaneously use the dialog context, user
utterance, and the knowledge base for response generation. Dynamic fusion technique has been
employed in Ref. [42] to share domain features across different domains for a better generation.
Our current work differs from these existing works on task-oriented dialogue systems as we
focus on generating aspect controlled responses in a multimodal setup by utilizing the information
present in both text and images.
2.2 Multimodal Dialogue Systems
Research in the dialogue system has recently shifted toward incorporating different sources of
information, such as images, audio, video, and text in order to make a robust system. The research
reported in Refs [14], [15], [17], [20], and [37] has been useful in narrowing the gap between vision
and language. In Ref. [37], an Image Grounded Conversations (IGC) task was proposed, where
conversations are natural and focused upon a shared image. Similarly, the authors in Ref. [14]
introduced the task of visual dialog, which requires an AI agent to hold a meaningful dialogue
with humans in natural, conversational language about the visual content.
Recently, video and textual modalities were investigated with the release of the Dialog State
Tracking Challenge (DSTC7) dataset in Ref. [28] that used a multimodal transformer network to
encode videos and incorporate information from the different modalities. Similarly in Refs [3], [27],
and [32], DSTC7 dataset has been used for generations by incorporating audio and visual features.
The release of Multimodal dialog (MMD) dataset [47], having conversations on the fashion domain
in both text and images, has facilitated response generation in multimodal setup. Several works on
the MMD dataset reported in Refs [1], [2], and [30] used the hierarchical encoder-decoder model to
generate responses by capturing information from text, images, and the knowledge base. Recently,
Ref. [7] proposed attribute-aware and position-aware attention for generating textual responses.
The authors in Ref. [13] used a hierarchical attention mechanism for generating responses on
the MMD dataset. The dataset reported in Ref. [18] is extended in MDMMD++ by adding more
domains, cross-domain conversations, more diversity, as well as by improving its quality.
Our newly created multi-domain multimodal dialog dataset differs from the existing datasets
as it comprises the conversations belonging to the four different domains. The dataset comprises
multi-domain conversations belonging to multiple domains as well. This is one of the first datasets
that includes conversations belonging to multiple domains with textual and image information.
Our present work distinguishes from the prior works on multimodal dialog in a sense that we
focus here on the task of generating responses conditioned on a particular aspect of the product
or service, and in accordance with the conversational history. To the best of our knowledge, we
are the first to propose the task of aspect controlled dialog generation in a multimodal setup.
3 DATASET CREATION
As mentioned in the earlier section, one of the critical contributions of this work is a large-scale
MDMMD++ dataset.1 The dataset has dyadic dialogues utilizing both textual and visual information in the utterances and context that also exhibit domain-specific knowledge in the sequence
of interactions. This dataset will facilitate developing robust multimodal goal-oriented dialogue
systems. A few examples from our newly created dataset are provided in Figures 1 and 2 in the
introduction section. In the following section, we first describe the process of corpus creation,
its statistics and quality analysis, followed by comparisons to the existing goal-oriented dialog
datasets to establish the usefulness and significance of our work.
1The MDMMD++ dataset is available at https://www.iitp.ac.in/∼ai-nlp-ml/resources.html#mdmmd++.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:7
3.1 Process for Data Creation
The first step of data creation was to decide the different domains on which the conversations were
to be made. After closer analysis and thorough discussions, we come up with the following two
top-level criteria for domain selection: (i) It covers a large group of industries or organizations that
use task-oriented systems and are likely to create user interfaces; (ii) the domains require visual
information for better understanding and clarity of the products. The domains that generally need
customer support are well suited for these criteria. Besides, there is no multimodal dialogue corpus
available in the selected domains to encourage research in the area of multimodal goal-oriented
dialogue systems. Therefore, in our newly created large-scale MDMMD++ dataset, we choose to
curate conversations belonging to the four different domains, viz. restaurant, hotel, electronics,
and furniture.
The collection of multimodal dyadic conversations was done with the coordination of a devoted
team of 15 domain experts belonging to each domain. Multimodal information in the domain selected plays a significant role. As the available conversational flows were limited, this dataset was
built in close coordination with the experts in the respective domain. The experts from each domain explained many conversation flows during the order and purchase of a particular product
concerning the different aspect of a product. The significance of different aspects in selecting a
product was identified, after which, in various chat sessions, the domain information was incorporated to make the dialogues natural and free-flowing. The significant steps followed for the data
creation consists of: (i) Data collection strategy, (ii) Creating a large-scale multimodal conversation consisting of text and images while incorporating the domain knowledge in the interactions,
(iii) annotation of the aspect in the multimodal conversations. In the following section, we explain
the detailed processes involved in the above three steps.
3.1.1 Data Collection Strategy. Interactions with the domain experts led to the discovery of
much intricate information needed for creating the dialogues. As a consequence of the experts’
interactions, we recognize the complexities of various types in a natural conversation for every
domain, driven by the background knowledge that both the domain expert and the customer employ in their conversation. The domain expert’s knowledge is varied ranging from “Which food
goes best with which beverage?” to What are the different cooking methods employed for different foods?” to Which electronics are best suited for which application according to the usage?” to
“Which furniture looks best according to the place?” to different aspects involved in each product
and so on. This detailed information belonging to each domain is crucial for creating the dialogues.
Hence, the first step was to gather this domain knowledge from unstructured multimodal content
available on the web. The necessary steps followed in this process are stated below.
—We crawled approximately 2 million products belonging to the different domains, such as
food items, restaurants, hotels, rooms, electronics, furniture from the different websites together with the images of the products, and semi/un(-structured) information.
—The domain experts manually inspected the unstructured data according to the domain
information and parsed the free text in a structured format.
—Each domain selected was strictly observed first; then the aspect categories were listed
to mark the aspect information. The different aspect categories, along with the associated
aspect terms belonging to the different domains, are listed in Table 1.
—After detailed discussions with the domain experts, we identify common aspects belonging
to the different domains. The list of common aspects categories is also listed in Table 1.
—The domain experts helped recognize different states for building conversations with respect to the identified aspect categories, as provided in Table 2.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:8 M. Firdaus et al.
Table 1. Aspect Information per Domain
Domain Aspect Category Aspect Terms
Restaurant
Quantity, Cuisine_Type,
Restaurant_ Type,
Meal_Type, Cooking_Type,
Flavor, Course_Type,
Meal_Course, Food_Service,
Dietary_Preference,
Beverage_Type,
Dessert_Type
One serving, two servings, large, small; Indian, Italian,
French, Chinese, Spanish; fast food, casual dining, fine
dining, ethnic; breakfast, lunch, dinner, snacks; steamed,
fried, pan-fried, grilled, baked; sweet, savory, sour, spicy,
bland; 2-course, 3-course, 4-course; appetizers, starters,
main course, dessert; plate, cart, platter, buffet, family
style; vegetarian, non-vegetarian, vegan, halal; juice, soft
drinks, energy drinks, alcoholic drinks; cookies, cakes,
chocolates, puddings, sweets;
Hotel Hotel_Type, Hotel_Rating,
Bed_Type, Room Amenities
Apartments, Villas, Bungalow, Farm Stay, Nature Lodge,
Earth House; 5-star, 4-star, 3-star; Single bed, Double
bed, Conference room, Banquet, Hall, Restaurant, single,
double, triple; Balcony, Computer with the Internet,
Electric Kettle, Fridge, Hairdryer, Microwave, Satellite,
Washing machine, Air conditioner, Television
Electronics
Model_Type, Shapes, Weight,
Ease of Carrying,
Dimensions, Brand, Color,
Operating System,
Network_Type,
Varies in each product (mobiles, laptop, AC, TV, fridge,
washing machine); rectangle, circle, square, cylinder,
oval; portable, non-portable; length, breadth, height;
Samsung, Apple, LG, Asus, JBL; red, black, silver, white,
blue; Linux, android, iOS, Windows; 4G, 3G, 2G;
Furniture
Furniture_Style, Brand,
Room_Type, Material,
Living_Type,
Furniture_Usage,
Lighting_Type
Contemporary, Modern, Mid-century Modern,
Industrial, Traditional; CasaCraft, Amberville,
Mudramark, Bohemiana, Clouddio; Living Room,
Bedroom, Study Room, Kids Room, Washroom;
Plywood, Veneer, Plastic, Stainless Steel, Copper,
Wrought Iron, Wood; Sofa, Chair, Table, Cabinetry;
bedding, lighting, furnishing; Ceiling light, Wall lights,
Outdoor lights, Festive lights; storage, seating, decor,
Across Domain
Price, Rating, Payment,
Discount_Offer,
Delivery_Status, Location,
Budget
cheap, medium, expensive, costly; customer rating,
product review rating; EMI, cash on delivery, net
banking, wallets; Bank offer, Exchange offer, No cost
EMI, 30%, 20%, etc.; Deliver_on_time, Cancelled, Return,
Refund;
This table lists some of the aspect categories along with the corresponding aspect terms for each domain, respectively.
This also shows the list of aspects common to more than one domain.
We collect a large amount of data from the various sources available, as stated earlier. Although
the task of creating a two-party conversation has to be done manually, knowledge curation is still
a difficult task. Firstly, each domain exhibits various aspect information, at which a user looks
when they choose a domain. Previously, it has been discussed through the examples in Figure 1
the significance of images for answering the user’s queries. Therefore, we have collected several
images (pictures) for each domain from the various sites.
Below, we describe the domains in our data:
(1) Restaurant—In the restaurant domain, dialogues focus on restaurant bookings and food
ordering. In reserving a restaurant, the information such as the restaurant’s view, the ambience inside the restaurant, and the restaurant’s parking facility are some of the important aspects that the user looks into before booking a restaurant. The images showcasing
these aspects assist the user in selecting the restaurants of their choice. Similarly, while
ordering food, the food items’ images help decide the types of food the user wants to order. For example, in certain food items, it is essential to see the images to understand the
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:9
Table 2. State Type Information
Seq. No. State Type Description
1 Greeting/ Acknowledgment User or the System greets each other.
2 User - information User describes his requirements.
3 Give—images System displays responses based on the previous context and
current query.
4 Give—images description System displays description of the images based on the visual
features available.
5 Specific items/common
items like/dislike, tell-more
User describes whether they are liking/ disliking the description
of the item shown.
6 Give—orientation
System displays the orientation of the liked image by the user.
Food—images of food, the ambience of the the restaurant in
different directions.
Hotel—rooms, washrooms from different views.
Electronics/Furniture—images of watches, mobiles, etc., from
various angles.
7 Give—alike
System displays items that are similar to the one asked by the
user.
Food—In this domain, either it can be a similar food type or
restaurant type.
Hotel—In this, a similar room type can be displayed.
Electronics/Furniture—similar wearables or products may be
displayed.
8 Goes—with System suggests which food/ingredients goes best with the item
asked by the user.
9 Ask—attributes/
ingredients/amenities
User asks for particular attributes from the
Electronics/Furniture; or they ask for various ingredients, or
displays information about amenities present at the hotel.
10 Sort—result System sorts the images or textual response in a particular order.
11 Buy User buys the items they have selected.
12 Exit User exits the conversation system; here, the chat ends.
13 Ask—suggestion User asks for system’s advice on what they should try now.
Lists the state information identified by the domain experts. These state types, in turn, have sub-types upon which the
chats are generated.
item properly. Like in the case of pasta, different colors signify the different types of pasta,
which are difficult to understand only by the name. Hence, the visual image of the food is
needed for selecting the correct one to order.
(2) Hotel—In the hotel domain, the conversations correspond to the customers’ bookings of
the hotels for different occasions, such as trips, conferences, parties, and so on. The images
of the rooms are required for assisting the customer in selecting the best one. Different
room amenities such as television, air conditioner, and so on are essential while choosing
a room. This information is easily visible through the images that can help the customer
select the appropriate rooms for themselves.
(3) Electronics—In the electronics domain, the conversations focus on purchasing different
items or browsing various electronic products. The color, shape, size, and brand are an
essential aspect of buying an item. Therefore, visual information is inevitable for selecting
an electronics item.
(4) Furniture—In the case of furniture, the conversation revolves around purchasing furniture. Like the electronics domain, in furniture, the size, shape, and color are essential for
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:10 M. Firdaus et al.
deciding the correct item. Images play a significant role in selecting the right furniture,
and hence, users prefer visual information of the products as well.
3.1.2 Building User-Agent Conversations. The domain experts with thorough knowledge about
the respective domains and crowd-sourced workers were employed to create the goal-oriented
multimodal conversations using a Wizard-of-Oz approach. For every conversation belonging to a
particular domain, the domain experts assumed a system agent’s role while the workers act as the
customer agents. The wizard (domain expert) is asked to perform a system function by supplying
user-requested information.
As described by the domain experts, we first pen down all the available states, then follow every
time a pair of two domain experts and crowd-sourced workers performed the task of preparing
the utterance-response pairs. For natural and robust conversation building, hundreds of texts and
images are browsed, and the data was collected step-by-step each day. During the data collection
process, various phases of the online sales process were identified. For instance, a dialog between
the sales agent (i.e., system) and a user visiting an online hotel reservation website with the goal
of either booking or browsing one or more hotel rooms begins with the shopper providing the
agent with their shopping needs. The system then browses the collected data and returns with a
multimodal answer (i.e., a set of images that meet the user’s constraints and/or some related text).
Now, the user provides feedback using this response or updates their requirements. The session
continues until the user chooses to order (or purchase or reserve) with one or more items or leave
without purchasing anything.
To create high-quality multimodal goal-oriented conversations between the system agent and
the customer agent, every domain expert follows a list of states, as mentioned in Table 2. To create
informative utterances for satisfying the users and help achieve the end goal, we provide a list of
different aspect categories having different aspect terms for creating conversations based on the
various aspects. Different criteria for creating the conversations, such as minimum conversation
length, number of aspect categories, number of images in response, number of goals, number of
complex requests, and so on, were specified to increase the conversation diversity.
In addition, we requested the domain experts and the workers not to provide any personally
identifiable information, such as name, address, or number in the conversations. At the implementation level for dialogue creation, we establish a web interface for the experts and the workers that
displays the instructions (state types for completion of dialogue) and different aspect categories
along with aspect terms belonging to a particular domain next to the ongoing dialogue creation.
This helps the participants create good conversations while referring to the guidelines and the
different aspect information pertaining to a domain without stopping the conversation. Though
we follow a known approach (Wizard-of-Oz) for data creation, as observed in the present works
[6, 40, 47], our MDMMD++ dataset constitutes of more varied responses belonging to multiple
domains and having both textual and visual modalities.
To the best of our knowledge, this dataset is novel in the sense that it is created in full supervision
of the experts, and we explicitly monitor and guide the workers to participate in the process to
create engaging, informative, and diverse conversations while focusing on the different aspects
of a particular product/service. For example, in the Restaurant domain, participants were advised
to pretend that they were interested in ordering food or looking for a fine place to dine. The
different aspects associated with this domain, like the type of cuisine (Chinese, Italian, Indian,
etc.), type of restaurant (cafes, lounges, etc.), ambience, the meal type (dinner, breakfast, etc.), type
of food (desserts, snacks, appetizers, etc.) are provided for creating diverse conversations. They
were asked to change their preferences between the conversations (like Chinese, they could shift
into Italian foods) to make it more challenging and complex. Similarly, in the other domains, the
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:11
Fig. 3. Percentage of aspect categories in each domain.
participants were instructed to follow the guidelines and use different aspect categories to create
diverse, entertaining, and engaging responses.
3.1.3 Aspect Annotation. Our dataset has two kinds of annotation, viz. aspect category (AC)
and aspect terms (AT). We have intentionally decoupled aspect category and aspect terms in the
annotation process to create fine-grained aspect aware responses. Intuitively, the aspect category
for a particular domain can be constant for a group of utterances, but the aspect terms in every
utterance may or may not be consistent. For example, the meal_type is the aspect category, but
breakfast is the aspect term that, according to the user, could change into dinner or lunch in the
remaining utterances of a particular dialogue. Therefore, the labeling of both the aspect category
and aspect term is essential for the generation of aspect controlled responses to learning the subtle
differences between the different aspect terms within the same category. In Figure 3, we present
the percentage of aspect categories belonging to each domain along with some aspect categories
common to all the domains.
In the restaurant domain, there are 23 aspect categories, such ascuisine, meal_type, cooking_type,
color, ambience, and establishment_type. There are quite a few aspect terms associated with each
aspect category, such as in the case of cuisine—the different aspect terms are Chinese, Italian, Continental, Mexican, Indian, Thai, and Japanese. In contrast, in case of establishment_type, the associated aspect terms are casual dining, bakeries, cafes, sweet shops, and ice cream parlors. There
are around 15 aspect categories for the hotel domain like accommodation_type, room_type, and
room_amenities. There are many aspect terms related to each aspect category, like in the case of
accommodation_type, the related aspect terms are guest house, lodge, villa, and apartment. Similarly,
in case of room_amenities, the related aspect terms are table, chair, electric kettle, air conditioner,
room heater, and the like. The list of aspect categories, along with the aspect terms, are provided in
Table 1 for each domain. Some aspect categories are common to one or more domains. The list of
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:12 M. Firdaus et al.
Table 3. Dataset Statistics of the MDMMD++ Dataset
DATASET STATISTICS TRAIN VALID TEST
Number of modalities T+I T+I T+I
Number of utterances 3,599,438 514,205 771,308
Number of textual responses 2,485,651 355,093 532,639
Number of image responses 1,451,321 207,331 310,997
Number of dialogues 149,813 21,081 32,105
Avg. turn per dialogue 24.9 23.6 24.7
Avg. word in textual response 12.07 11.7 11.74
Total Aspect category 97 32 55
Aspect Terms 342 137 175
Vocabulary size 49,453 - -
common categories, such as price, location, rating, payment_method, and so on, are also provided
in the table.
For annotation, we make a predefined list of aspect categories by browsing the different online
sites used for crawling the data. The aspect terms for a particular group are listed for every domain. In the interface provided for the creation of dialogues, the crowd workers and experts were
requested to mark the aspect categories for every utterance from the predefined list. The corresponding aspect terms were also labeled. In case of utterances with no aspect information like the
starting and ending utterances of the dialogues are marked with a None label to signify the absence
of aspect in them. The workers were instructed to label the utterances with more than one aspect
category or terms present in a particular utterance. We conducted a comparison between the expert and crowd workers’ annotation on a subset of the data. A group of five annotators was selected
to verify the annotations done by the experts and crowd workers on a set of 2,500 dialogues. We
observe the multi-rater Kappa agreement ratio of approximately 80%, which may be considered as
reliable. Hence, from the survey, it can be concluded that the experts and crowd workers’ annotation for both aspect category and aspect terms are correct. Hence, this particular dataset may be
considered of good quality and can be used for aspect controlled response generation.
3.2 Dataset Statistics
The statistics of the complete dataset having all the four domains are provided in Table 3. In the
table, we provide statistics on the total number of utterances in the entire dataset comprising both
textual and image response. The dataset is divided into train, test, and validation with 75%, 15%,
and 10% conversations in each, respectively. The average turn per dialogue, along with the total
vocabulary size, is also provided. The total number of aspect categories, together with the number
of aspect terms, is also shown in the table.
The domain-wise statistics of the dataset are provided in Table 4. The number of dialogues
belonging to each domain is provided along with the number of textual and image responses.
The average dialogue length, number of aspect categories, and aspect terms associated with each
domain are also stated. The details of cross-domain conversations are given in the table. The crossdomain conversation corresponds to the conversations having more than one domain.
3.3 Quality Survey
Clean and good quality of data is essential to build a robust model for any particular application. To
guarantee that the dataset is descriptive and not skewed for a particular domain having different
aspects, we organize a survey on the dataset. For the study, a distinct group of 10 domain experts
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:13
Table 4. Domain-Wise Statistics of MDMMD++ Dataset
Domain #Dialogues #Utterances #Utterances
with Text
#Utterances
with Images
#Utterances
with Aspect
Terms
Avg.
Utterances
Length
#Aspect
Category
#Aspect
Terms Vocabulary
Size
Restaurant 50k 1,250,386 829,150 421,236 1,162,139 25 23 121 45,734
Hotel 45k 990,125 693,210 378,524 956,126 22 15 63 23,617
Electronics 51k 1,122,473 765,221 578,261 1,079,651 24 25 78 15,964
Furniture 43k 903,692 591,245 369,417 810,263 21 24 55 17,285
Cross Domain 25k 875,379 672,105 325,879 639,348 35 10 25 21,679
Table 5. Qualitative Survey by the Domain Experts
Percentage of Dialogues Rating Rating Description
43.5% 5 the dialogues are genuine and correct
31.7% 4 less than 1.5% secondary errors
18.8% 3 less than 1.5% primary and secondary errors
4.2% 2 less than 2% secondary errors and 1.5% primary errors
1.8% 1 more than 2% primary and secondary errors
from each domain was hired to check the quality of the data created. The experts were given the
following instructions: (i) asked to ensure that the dialogues’ textual and visual portions were substantial, natural, and meaningful; (ii) the aspect annotations of each utterance in the dialogue were
correct, and also, the associated images were contextually suitable. We surveyed 2,000 conversations randomly drawn from the chat sessions (500 from each domain). The domain experts were
requested to provide an overall rating to the dialogues between the range of 1 to 5 (with 5 being
highest in quality).
In this survey, mainly two types of errors were reported (i) primary error—these consisted
of major mistakes in logic, such as the user’s intent was wrongly understood and the response
was given, images provided were incorrect, the aspect was not correctly annotated, and so on;
(ii) secondary error—this error mainly comprises the grammatical correctness of the responses and
the image quality of the responses (i.e., the images are not clear and slightly blurry). In Table 5,
we present the results of the survey. From the results, it is evident that the average rating attained
was 4 out of 5, signifying that the dialogues created have a few conversational errors belonging
to the different domains. Although there are conversations that comprise some minute errors, as
we have performed the manual annotation, the text is of high quality, and because of which, the
user retention is still on point. On average, if we consider the entire dataset, the quality persists.
Nonetheless, the dataset contains some noise inherited from the crawled information from the
different websites or because of some confusion in deciphering the intents of the utterances. Even
though such error exists, they are less in quantity, and these types of errors are expected in any
data creation process done on such a large scale. It can be concluded from the data quality survey
that the mistakes are minimal in most of the dialogues. Therefore, the quality of the conversations
in the MDMMD++ dataset is good and natural.
3.4 Dataset Comparison
Goal-oriented dialogue systems tend to fulfill user’s goals and objectives by assisting them in their
day-to-day lives. In Table 6, we represent the comparison of our proposed MDMMD++ dataset
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:14 M. Firdaus et al.
Table 6. Comparison of Our Proposed MDMMD++ Dataset with the Existing
Goal-Oriented Dialogue Datasets
Dataset DSTC 2 WOZ 2.0 FRAMES KVRET M2M MultiWOZ MultiDoGo MMD MDMMD++
Source AMT AMT Human Human Human CS CS Semi-automated Human
Modality T T T T T T T T+I T+I
Avg. Dialogue
Length 14.49 7.45 14.60 5.25 9.86 15.91 20.06 40 25.1
Total Dialogues 1,612 600 1,396 2,425 3,000 8,438 40,576 150,629 100,521
No. of Domains 1 1 1 3 2 7 6 1 4
No. of Tasks 1 1 1 2 2 2 3 1 3
Aspect
Information - - - - - - - - Yes
with the existing goal-oriented dialogue datasets. As discussed earlier, there are quite a significant
number of datasets available for building conversational systems. As it is evident from the table,
most of the datasets are unimodal. Moreover, these majorly comprise a single domain, such as
DSTC2 [22] and WOZ 2.0 [61] that contain conversations on restaurant reservations. Similarly,
the FRAMES dataset [4] contains two-party dialogues belonging to the trip bookings, whereas
M2M [50] comprises two domains with 1,500 conversations, each for movie ticket booking and
restaurant reservation. The KVRET corpus [16] constitutes three different domains, such as calendar scheduling, weather information retrieval, and point-of-interest navigation, having multiturn conversations. The recently released dataset, MultiWOZ [6], is an extension of WOZ2.0 that
is composed of seven domains. Similarly, the MultiDoGo corpus [40] has been released containing six different domains with a maximum number of dialogues compared to the other unimodal
datasets. Our present dataset differs from the existing datasets in that our dataset is multimodal
and uses both textual and visual information for conversations in different domains.
Recent research in goal-oriented dialogues has shown improvement by incorporating multimodal information in the dialogue systems. The release of the MMD [47], having goal-oriented dialogues, has encouraged research in building multimodal systems capable of utilizing information
from complementary sources like images and text. The MMD dataset contains dialogues belonging to the fashion domains having both text and images. Our present dataset is slightly similar
to the MMD dataset, as it also includes conversations having texts and images bringing language
and vision together. Our proposed MDMMD++ dataset contains multiple domains instead of the
MMD dataset, thereby having a higher number of conversations. Also, our MDMMD++ dataset is
annotated with aspect information for generating focused and informative responses. As per our
knowledge, our proposed MDMMD++ dataset is the largest multimodal dataset available that is
both multimodal and multi-domain in nature. This dataset can be utilized for promoting research
in goal-oriented multimodal systems.
3.5 Task
The proposed MDMMD++ dataset consists of multiple domains and multimodal conversations
between the user and the system (dyadic conversations). It can, therefore, be used to evaluate a
wide range of tasks. In this section, we define each of these tasks and illustrate their technical
challenges.
(1) Textual Response: The task is to generate the next textual response for a given context of
k turns.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:15
(2) Image response: For a context of k turns, the task is to generate the image response, i.e., to
respond with the relevant images according to the conversational history. This task can be
achieved through two approaches: (i) Image Retrieval—For a given context of k turns and a
database of images, the task is to retrieve the relevant images and rank them according to
the conversational history; (ii) Image Generation—The task is to generate the appropriate
images for a given context of k turns.
(3) Aspect Controlled Response: As the dataset is labeled with aspect categories and the corresponding aspect terms, we propose the task of aspect controlled response generation
where the desired aspects are specified for generating exciting and coherent responses.
(4) Utilizing Domain Knowledge: To understand the significance of domain knowledge, we
propose the task of employing unstructured domain information for generating responses
as defined in Tasks 1, 2, and 3.
In our current work, we focus on Tasks 1 and 3, i.e., aspect controlled textual response generation
while utilizing the information present in the context having knowledge of both the modalities.
4 METHODOLOGY
In the present section, we first define the problem and then present our proposed method. For
the proposed task, we assume that the aspect term information will be provided to generate the
response. As different aspects are incredibly subjective in a goal-oriented system, hence, the responses are majorly dependent upon the respondent. Therefore, there can be several suitable responses possible for a given input. Because of this subjectivity in goal-oriented systems, we like
to focus on solving the task of generating responses with the desired aspect information. In this
section, our baseline for the proposed task, as shown in Figure 4, is discussed in details.
Finally, we propose an aspect controlled generation framework, employing a Multimodal Hierarchical Variational Memory Network (MHVMN) with the aspect, as shown in Figure 5. Primarily
most of the previous works, such as Refs [1], [2], [7], and [47] on multimodal dialogue systems
have extended the existing unimodal architecture for multimodal dialogue setting. Hierarchical
encoder-decoder framework [48, 49] is one of the strong baselines employed for dialogue generation. Extension of this architecture to incorporate multimodality has been investigated in the
existing frameworks as one of the baselines. In our case, our framework’s novelty lies upon the
fact that we employ a hierarchical memory network in a multimodal setup. Besides, to accomplish
the proposed task, we integrate an aspect controlled decoder that assists in generating the aspect
guided responses.
4.1 Formal Definition
Our current work addresses the task of aspect controlled response generation in a multimodal goaloriented dialog system conditioned on the conversational history, having both textual and visual
information. The dialogue consists of textual utterance along with multiple images and a context
history. We address the task of generating the next response that is coherent and in accordance
with the specified aspects providing better human-machine interaction.
From the above example in Figure 1, it is evident that traditional multimodal conversations
consist of system-user chat pairs. At any instant of time, an utterance-response pair between
the user and system may include text only, or both text and image. Our task here is to generate the most appropriate and feasible response for a given context of k turns, consisting of any
of the following cases: (i) there may be various images that provide a context; (ii) all such context
images may change over time during the conversation; (iii) the response in each step may be text
or both (text and image); and (iv) at each turn, we also provide the aspect information.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:16 M. Firdaus et al.
Fig. 4. Baseline models with aspect information. Figure 4(a) represents the unimodal hierarchical encoderdecoder framework with aspect controlled decoder. Figure 4(b) represents the multimodal hierarchical
encoder-decoder framework with aspect information, and Figure 4(c) displays the unimodal hierarchical
variational network with aspect information.
To be more specific, given an utterance Uk = (wk,1,wk,2,...,wk,n ), a set of images Ik =
(ik,1,ik,2,...,ik,j), and a conversational history Hk = ((U1, I1), (U2, I2),..., (Uk−1, Ik−1)), and the
aspect term Va, the task is to generate the next textual response Y = (y1,y2, . . . ..,yn ), where n
and n are the given input utterance and response length, respectively.
We would like to mention that the dataset has been designed so that the image responses are
always accompanied by textual information. Hence, in all cases, there is a possibility of textual
response generation. As the current work proposes the task of aspect aware response generation,
this work’s primary focus is textual response generation only. In every case, there is a groundtruth textual response available to evaluate the generated response. Only text generation takes
place for the utterances that have both text and images in our current architecture. Image retrieval
or generation is one of the possible future extensions of our current work.
4.2 Multimodal Hierarchical Encoder Decoder
As shown in Figure 4(b), we build a response generation framework for generating aspect controlled responses. Our framework is an extension of the recently introduced Hierarchical EncoderDecoder (HRED) architecture [48, 49]. In contrast to the standard seq2seq models [56], the dialogue
context among the utterances is captured by adding utterance-level Recurrent Neural Network
(RNN) over the word-level RNN, increasing the efficacy of the encoder to capture the hierarchy
in dialogue. The MHRED is constructed upon the HRED to incorporate both text and image information in a single framework. The primary and essential components of our baseline MHRED
framework are the image encoder, utterance-level encoder, context-level encoder, and finally, the
decoder.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.  
Aspect-Aware Response Generation for Multimodal Dialogue System 15:17
Fig. 5. Multimodal Hierarchical Variational Memory Network with Aspect. Here, the Text RNN encoder
encodes the textual utterance while Image VGG-16 encoder captures the image features. To capture the
hierarchy in dialogue, the memory network encodes both the textual and visual representation. Here, z
represents the latent variable fm to ensure diversity in responses. Finally, the RNN decoder is initialized
by the hidden context representation of the memory network and the aspect information for generation of
aspect-aware responses.
4.2.1 Utterance Encoder. For a given utterance Uk , we employ bidirectional Gated Recurrent
Units (BiGRU) [12] to encode each word wk,i , where i ∈ (1, 2, 3, . . . ..n) having d-dimensional embedding vectors into the hidden representation hU,k,i . We concatenate the last hidden representation from both unidirectional GRU to form the final hidden representation of a given utterance as
follows: −−−−→ hU,k,i = GRUU,f (wk,i,
−−−−−−→ hU,k,i−1) (1)
←−−−− hU,k,i = GRUU,b (wk,i,
←−−−−−− hU,k,i−1) (2)
htxt
U,k,i = [
−−−−→ hU,k,i,
←−−−− hU,k,i] (3)
4.2.2 Image Encoder. A pre-trained VGG-16 [53], having a 16-layer deep convolutional neural
network (CNN) trained on more than millions of images present in the ImageNet dataset, is used
for encoding the images. It can classify images into 1,000 object categories such as dress, shoes,
animals, keyboard, mouse, and so on. As a result, the network can learn rich features from a wide
range of images. Here, it is also used to extract the “local” image representation for all the pictures
in the dialogue turns and concatenate them together. The concatenated image vector is passed
through the linear layer to form the global image context representation as given below:
Ik,i = VGG(Ik,i ) (4)
Tk = Concat(Tk,1,Tk,2,...,Tk,j) (5)
himд
I,k = ReLU (WITk + bI ), (6)
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:18 M. Firdaus et al.
whereWI and bI are the weight matrix and biases, respectively, which are the trainable parameters.
In every turn, the maximum number of images i ≤ 6, so, in case of only text, vectors of zeros are
considered in place of image representation.
4.2.3 Context Encoder. As shown in Figure 4(b), the final hidden representations from both
image and text encoders are combined for each turn and are given as input to the context level
GRU. A hierarchical encoder is built to model the conversational history placed over the text and
image encoders. The decoder GRU is initialized by the final hidden state of the context encoder.
hctx
c,k = GRUc

htxt
U,k,i ;himд
I,k

,hc,k−1

, (7)
where hctx
c,k is the final hidden representation of the context for a given turn.
4.2.4 Decoder. In the decoding section, we build another GRU for generating the response sequentially based on the context hidden representation of the hierarchical encoder (context GRU)
and the words decoded previously. We use input feeding decoding and the attention [34] mechanism for enhancing the performance of the model. Using the decoder state hdec
d,t as the query
vector, we apply self-attention on the hidden representation of the context-level encoder. The decoder state and the context vector are concatenated and used to calculate a final distribution of
probability over the output tokens.
hdec
d,t = GRUd (yk,t−1,hd,t−1) (8)
ct =

k
i=1
αt,ihctx
c,k , (9)
αt,i = so f tmax
hctx
c,k
T
Wf hd,t
 (10)
˜
ht = tanh(Wh˜[hd,t ;ct]) (11)
P (yt /y<t ) = so f tmax (WV ˜
ht ), (12)
where Wf , WS , and Wh˜ are the trainable weight matrices.
For generating responses with the specified aspects as shown in Figure 4(b), we provide the
aspect term embedding Va as input during decoding at every decoder timestep. In order to include
the aspect vector in the decoder, there is a slight change in Equation (8), and the new equation is
as follows:
hdec
d,t = GRUd (yk,t−1,[hd,t−1,Va]) (13)
4.3 Proposed Approach
In this section, we propose an MHVMN approach for generating aspect controlled responses. Dialogue generation systems aim at generating responses that are natural-sounding and grammatically correct. The traditional encoder-decoder mechanism lacks in providing variations and capturing long-term dependency. Our proposed framework is an extension of the HVMN [8] to infuse
the image information for generating consistent and informative responses. The memory network
can tackle two problems that encoder-decoder frameworks cannot handle, such as (i) Generic
responses—given a context history and the current utterance, basic encoder-decoder networks
like sequence-to-sequence models are incapable of generating a diverse and interactive response.
These networks generate generic and boring responses, such as “I don’t know,” “Okay,” and so on.
(ii) Memory decay—any conversation session consists of numerous dialogue turns, which require
modeling of several utterances while learning the long-term dependencies. The basic encoderdecoder approach is incapable of modeling long contextual information leading to the loss of information. Hence, in our proposed method, we employ a memory network that is competent in
dealing with the issues mentioned above in the existing encoder-decoder architectures.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.      
Aspect-Aware Response Generation for Multimodal Dialogue System 15:19
The proposed framework is built upon the hierarchical encoder that models the conversation
at the utterance and dialogue levels. A latent variable and a memory block are employed by the
variational memory network to capture the long-term utterance dependencies along with the intricate details present in the utterances during the modeling of a dialogue. The utterance encoder,
image encoder, and the context encoder are similar, as discussed in Section 4.2. In the variational
memory block, a latent variable is sampled to retrieve the memory cells mi . Simultaneously, the
memory is also updated with hctx
c,k to cumulatively memorize the current utterance.
4.3.1 Variational Memory Reading Mechanism. To correctly retrieve the information from the
memory cells, it is essential to read the cells’ information. A continuous stochastic latent variable
fm is employed to draw rm from memory M as follows:
rm =

T
t
Mt
m  f t
m, (14)
where, Mm ∈ Rdt xdf , fm ∈ Rdt , and  is the Hadamard product function.
Here, the latent variable fm is conditioned on all the previously observed tokens of the context
encoder hctx
c,k and is calculated for every utterance:
P (fm |U<k ) = N 


μprior (U<k ),

prior
(U<k )


	
,
where N (μ,
) is the multivariate normal distribution.
4.3.2 Variational Memory Updating. The central idea behind variational memory updating is to
capture long-term dependencies. Certainly, for this task, we use the mechanism similar to LSTM,
which mainly focuses on forget and update mechanisms. In the same way, the forget mechanism
tells what information we have to forget, and the update mechanism keeps track of the memory
that should be updated. For every utterance, the memory Mm is updated with the context encoder
hctx
c,k to capture intricate details in the conversational history. The memory is updated as follows:
Mm = Fm  Mm−1 + Um  hupdate
m , (15)
where Fm and Um are the forget and update gate, respectively. The gates are computed in the
following manner:
Fm = siдmoid
hctx
c,k ,rm, Mm
 (16)
Um = siдmoid
hctx
c,k ,rm, Mm
 (17)
Finally, hupdate
m is denoted by:
hupdate
m = σ

hctx
c,k ,rm

, (18)
where σ is a non-linear activation function.
4.3.3 Aspect Controlled Decoder. For aspect controlled dialogue generation, the output of the
context encoder hctx
c,k capturing information from both the utterance encoder and the image encoder is fed as input to the decoder. Along with the contextual information, the latent variable fm,
the previous decoded word ewt−1 , and the aspect embedding Va are also provided as input to the
decoder RNN. Formally, the decoder RNN can be defined as:
hdec
d,t = GRUd

yk,t−1,

hdec
d,t−1, ewt−1 , fm,Va
 (19)
To achieve aspect embeddings Va, we use pre-trained 300-dimensional Glove embeddings [39]
to represent every aspect present in the MDMMD++ dataset. The aspect embedding encourages
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.          
15:20 M. Firdaus et al.
the generation of aspect controlled responses in a similar manner as Ref. [24], where emotion
embeddings are provided directly to the decoder for generating emotional responses. Inspired by
this existing approach, we adapt our architecture to generate the aspect-aware responses. Since the
decoder gets the aspect information directly at every timestep, hence, it facilitates the generation
of the responses toward the specified aspect.
4.4 Training and Inference
We employ commonly used teacher forcing [62] algorithm at every decoding step to minimize the
negative log-likelihood on the model distribution. We define y∗ = {y∗
1,y∗
2,...,y∗
m } as the groundtruth output sequence for a given input by:
Lml = −
m
t=1
logp(y∗
t |y∗
1,...,y∗
t−1) (20)
We apply uniform label smoothing [57] to alleviate the common issue of low diversity in dialogue systems, as suggested in Ref. [26].
4.5 Baseline Models
We implement the following models as baselines for our experiment:
Model 1 (HRED): The first baseline is a simple hierarchical encoder-decoder framework using
only textual information for generating the responses.
Model 2 (MHRED): The second baseline model is the extension of the HRED framework with
the incorporation of the multimodal information, i.e., the images for the generation of coherent
responses.
Model 3 (HRED + Aspect): In this model, at the decoder side, instead of only textual conversational information, we add the desired aspect at the decoder side for generating aspect controlled
responses as shown in the Figure 4(a).
Model 4 (MHRED + Aspect): To learn the aspect information at the decoder, we provide the
aspect information to the decoder along with the text and the visual representation, as explained
in Section 4.2 and shown in Figure 4(b).
Model 5 (HVMN): The HVMN model serves as one of our baseline models. The encoded context
history having only the textual responses is fed into the memory network, and the representation
from the memory network is given as the input to the decoder.
Model 6 (MHVMN): This baseline is the extension of the HVMN framework with the multimodal information to incorporate the images as well for better response generation.
Model 7 (HVMN + Aspect): In this baseline model, we at the decoder side feed in the aspect
information for generating aspect controlled responses, as shown in Figure 4(c).
5 EXPERIMENTS
In this section, we present the implementation details, along with the evaluation metrics used to
evaluate the model’s output (both automatic and human evaluation).
5.1 Implementation Details
All the implementations are done using the PyTorch2 framework. For all the models, including
baselines, the batch size is set to 32. The utterance encoder is a bidirectional GRU with 600 hidden
2https://pytorch.org/.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:21
units in each direction. We use the dropout [55] with probability 0.45. During decoding, we use
a beam search with beam size 10. We initialize the model parameters randomly using a Gaussian
distribution with the Xavier scheme [21]. The hidden size for all the layers is 512. We employ
AMSGrad [45] as the optimizer for model training to mitigate the slow convergence issues. We
use uniform label smoothing with ϵ = 0.1 and perform gradient clipping when the gradient norm
is above 5. To reduce data sparsity, all the numbers and names are replaced with <number> and
<person>. The out-of-vocabulary (OOV) words are replaced with the <UNK> token. We use 300-
dimensional word-embedding initialized with Glove [39] embedding pre-trained on Twitter. The
previous three turns are considered for the dialogue history, and the maximum utterance length
is set to 50. We ran 15 epochs, and the proposed model took about three days on a Titan X GPU
machine. For image representation, FC6 (4,096 dimension) layer representation of the VGG-19 [53],
pre-trained on ImageNet, is used.
5.2 Automatic Evaluation
To evaluate the model at the content and grammatical level, we report the results using the standard automatic metrics. To evaluate our proposed framework at the content level, we report perplexity [9]. Lesser perplexity scores signify that the generated responses are grammatically correct
and fluent. We also report the results using standard metrics like BLEU-4 [38] and Rouge-L [31]
to measure the ability of the generated response for capturing the correct information. To capture the semantics of the dialog and to assess the ability to generate aspect relevant responses, we
also provide Entity F1 [35] as one of the evaluation metrics. In our case, the entity corresponds
to the different aspects present in the responses. We report Distinct-1 and Distinct-2 metrics that
measure the distinct n-grams in the generated responses and are scaled with respect to the total
number of generated tokens to avoid repetitive and boring responses [29].
5.3 Human Evaluation
To analyze the response quality of the generated responses, we use human evaluation to study
the efficiency of the different baselines and the proposed models. From the generated responses,
we randomly take 700 responses from the test dataset for qualitative evaluation. We employed
human annotators to evaluate the efficacy of the generated responses similarly to the previous
works [7, 11, 13, 51, 58]. For a given input and aspect information, three annotators with postgraduate exposure were assigned to evaluate the correctness, relevance, and aspect consistency of
the generated responses by the different approaches for the following four metrics:
(1) Fluency (F): This metric is used to measure the grammatical correctness of the generated
response. It checks that the response is fluent and does not contain any errors.
(2) Relevance (R): It is used to judge whether the generated response is relevant to the conversational history.
(3) Aspect Appropriateness (AP): For this metric, we take care of the fact that the response
generated is consonant to the specified aspect (e.g., cuisine, color, type) and is also coherent to the conversational history.
(4) Domain Consistency (DC): This metric is used to measure the consistency of the generated
response according to the domain being discussed.
The scoring scheme for the human evaluation metrics in case of fluency is measured as follows
where “-0” means incomplete or incorrect response; “1” means moderately correct response; and
“2” is a correct response. The scoring scheme for relevance, aspect appropriateness, and domain
consistency is defined as follows: “0-” for the absence of aspect in the reply and the reply is inconsistent to the domain and “1-” for the presence of aspect in the response and the consistency
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:22 M. Firdaus et al.
Table 7. Results of Different Baselines and Proposed Model on the MDMMD++ Dataset
Model Description Perplexity BLEU-4 Rouge-L Distinct-1 Distinct-2 Entity F1
Baseline
Approaches
HRED 1.0293 0.5206 0.5473 0.0125 0.0464 56.83
HVMN 1.0189 0.5453 0.5618 0.0131 0.0472 60.26
MHRED 1.0185 0.5668 0.6122 0.0155 0.0534 63.89
MHVMN 1.0145 0.6011 0.6421 0.0163 0.0581 66.47
HRED + Aspect 1.0113 0.5569 0.6077 0.0162 0.0562 62.70
HVMN + Aspect 1.0095 0.5715 0.6391 0.0172 0.0613 65.11
MHRED + Aspect 1.0085 0.6221 0.6685 0.0189 0.0678 68.41
Proposed
Approach MHVMN + Aspect 1.0069 0.6581 0.6974 0.0210 0.0823 72.38
of the response with the domain information. For the human evaluation metrics, we calculate the
Fleiss’s kappa [19] to determine the inter-rater consistency. For fluency and relevance, the kappa
score is 0.75, and for aspect appropriateness and domain consistency, it is 0.77, indicating “substantial agreement.”
6 RESULTS AND DISCUSSION
This section presents the experimental results (both automatic and human) and the necessary
analysis of the generated responses by all the baseline models and the proposed methodology.
6.1 Automatic Evaluation Results
The results by the automatic evaluation metrics are provided in Table 7. From the table, it is clear
that the proposed approach outperforms all the baseline models, and the results are statistically
significant.3 The lower the perplexity, the better the generated responses; hence, it is visible that
the perplexity scores of the proposed MHVMN + Aspect model are the lowest among all the baseline models. The HRED model, having the highest perplexity scores, is unable to generate better
responses compared to the other frameworks. As opposed to the text-based models, multimodal
frameworks such as MHRED and MHVMN have lower scores for the perplexity exhibiting improvement in performance. The reduction in perplexity scores for the aspect controlled models,
both text-based and multimodal frameworks, further ensure these models’ capability for generating better responses.
In the case of the BLEU-4 metric, we see that the proposed model MHVMN + Aspect, having
the ability to generate responses according to the specified aspect information, achieves higher
scores with an improvement of 3.6% from the MHRED + Aspect baseline model. The superior performance establishes that the proposed model generates correct responses while preserving the
information present in the ground-truth response as BLEU-4 compares the generated response to
the ground-truth. Similarly, in case of Rouge-L metric, we see an increase of 2.89% in comparison
to the MHRED + Aspect based baseline. By adding the information present in the images and the
textual information in the multimodal frameworks, such as MHRED and MHVMN, we see enhancement in the performance of these models. In contrast to HRED and HVMN, there is an increment
of 4.62% and 5.58% in the corresponding multimodal frameworks, MHRED and MHVMN, in BLEU
scores, respectively. This demonstrates the fact that knowledge/information available in the images are significant for the generation. Similarly, in the case of Rouge-L, there is an increase of
3We perform a statistical significance t-test [60], and it is conducted at a 5% (0.05) significance level.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:23
6.49% and 8.03%, respectively, in the multimodal frameworks. The huge jump in the performance
entitles the fact that images play a crucial role in generating contextually correct responses. As our
research focuses on aspect controlled response generation in multimodal dialogue systems, we see
that the frameworks having aspect information outperform the other baseline frameworks. The
responses with the specified aspects, such as MHRED + Aspect, shows remarkable performance
with an increase of 5.53% and 5.63% in BLEU-4 and Rouge-L metrics, respectively, in comparison
to the MHRED counterpart. Finally, the proposed approach having both textual and image along
with aspect information yields exceedingly better performance than all the different variants for
the task of response generation.
As our current work aims to ensure aspect aware responses for improving the quality of generation, we measure the Entity F1 score of the generated responses. In comparison to the baselines
without having aspect knowledge, there is an improvement of about 5% in all the aspect-aware
frameworks. This proves that providing aspect information assists in generating better responses.
The proposed framework having multimodal information of both text and images shows a performance gain of around 4% in the F1 score, which validates that aspects information from both
the complementary sources (text and images) is crucial for generating intelligible, relevant, and
consistent responses.
Interactive and exciting responses are essential for every conversational system. Dull and repetitive responses are not encouraged in dialogue systems. Therefore, we measure the diversity of
our generated responses to ensure that the responses are interesting and diverse. The results in
Table 7 for Distinct-1 and Distinct-2 metrics show that the proposed approach is capable of generating varied responses. The main reasons behind the improvement are due to the variational
memory network framework and the aspect information provided to the model. It is evident from
the results that multimodal information is also essential and crucial in making the responses interesting as there is an improvement in these models compared to their unimodal counterparts.
From the results, it can be concluded that the proposed framework is not only capable of generating contextually correct responses but also has diverseness in the generated responses. It can
also be confirmed from the automatic evaluation that the multimodality information (both text
and images) is significant for the task.
6.2 Ablation Study
For a thorough understanding of every component in the proposed framework, we provide a detailed ablation study showcasing every unit’s importance in the proposed architecture. Since hierarchy is crucial for capturing the dialogue flow, we present the evaluation results of Seq2Seq and
Memory network that lack the hierarchical information to demonstrate the efficacy of hierarchy
in the dialogue. From Table 8, it is visible that the Seq2Seq network performs poorly in comparison to the HRED network. Though Memory network performs better than the Seq2Seq model, it
is unable to outperform the hierarchical networks. The major reason for the low performance of
these frameworks, even though having the aspect information, is that the previous conversational
history is not provided to build a better context for the next textual response.
With regard to the HRED framework, we provide the different results illustrating the significance of context and aspect information. Having both the context and aspect knowledge increases
the BLEU-4 score by 5% from the HRED framework that does not have either context or aspect
information. Similarly, it is evident in the HVMN framework that there is a gain of almost 8% in
Entity F1 score, showcasing the importance of aspect. The essence of this work is its contribution
toward multimodality, and to validate its effectiveness, we present the comparison of multimodal
networks that have visual information. As opposed to the unimodal HRED system, the MHRED
and MHVMN frameworks, having both textual and visual information, outperform the unimodal
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:24 M. Firdaus et al.
Table 8. Results of Ablation Study with Context and Aspect Information for Different Architectures
Model Context Aspect Perplexity BLEU-4 Rouge-L Entity F1 Distinct-1 Distinct-2
Seq2Seq - - 1.0341 0.4378 0.4723 51.59 0.0083 0.0298
- √ 1.0196 0.4624 0.5269 55.63 0.0120 0.0386
Memory - - 1.0308 0.4880 0.4973 53.76 0.0109 0.0402
Network - √ 1.0164 0.5133 0.5541 58.48 0.0134 0.0532
HRED
- - 1.0301 0.5077 0.5122 55.91 0.0119 0.0413
√ - 1.0293 0.5206 0.5473 56.83 0.0125 0.0464
- √ 1.0158 0.5346 0.5733 59.78 0.0147 0.0541
√ √ 1.0113 0.5569 0.6077 62.70 0.0162 0.0562
HVMN
- - 1.0247 0.5149 0.5319 57.84 0.0109 0.0451
√ - 1.0189 0.5453 0.5618 60.26 0.0131 0.0472
- √ 1.0180 0.5591 0.6068 63.79 0.0159 0.0587
√ √ 1.0095 0.5715 0.6391 65.11 0.0172 0.0613
MHRED
- - 1.0239 0.5278 0.5817 60.33 0.0133 0.0488
√ - 1.0185 0.5668 0.6122 63.89 0.0155 0.0534
- √ 1.0176 0.5973 0.6325 66.75 0.0173 0.0636
√ √ 1.0085 0.6221 0.6685 68.41 0.0189 0.0678
MHVMN
- - 1.0173 0.5732 0.6145 64.87 0.0142 0.0499
√ - 1.0145 0.6011 0.6421 66.47 0.0163 0.0581
- √ 1.0092 0.6207 0.6593 69.71 0.0185 0.0611
√ √ 1.0069 0.6581 0.6974 72.38 0.0210 0.0823
networks in case of all the metrics. This proves that both modalities are essential for enhancing the
quality of the generated response. By including the visual aid in the form of images, the proposed
approach, MHVMN, shows an improvement of more than 8% from the unimodal HVMN framework
having the contextual and aspect knowledge. It is revealed from the detailed ablation study that
dialogue context plays a vital role in generating relevant responses. In contrast, aspect information
makes the responses more interesting and in accordance with the user demands. Finally, it can be
stated that multimodal details are crucial for improving the architecture’s overall performance,
thereby generating better responses.
6.3 Comparison to the Existing Approaches
To prove the efficacy of our proposed network, we present the comparisons with some of the
recent state-of-the-art approaches in reference to the goal-oriented dialogue system in Table 9.
For a comprehensive analysis, we compare our proposed method with both unimodal and multimodal approaches. As Ref. [64] proposes a global to local memory pointer framework, we employ
the available implementation4 on our newly created dataset. Evaluation results reveal that our
proposed framework outperforms the existing approach with respect to all the metrics. Due to
the unavailability of multimodal information, the local decoder proposed in Ref. [64] is unable
to generate the specific and relevant responses. Similarly, in comparison to Ref. [11], our proposed multimodal hierarchical variational memory network shows an improvement of 9% in the
BLEU-4 score. The ability to generate the specified aspects and the image representation helps enhance the quality of responses compared to the working memory model. Finally, we compare our
4https://github.com/jasonwu0731/GLMP.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:25
Table 9. Results of the Existing Approaches and the Proposed Framework
Model Description Modality Perplexity BLEU-4 ROUGE-L Entity F1 Distinct 1 Distinct 2
Existing
Approaches
Global-to-Local [64] U 1.0169 0.5785 0.5947 64.29 0.0169 0.0569
Working Memory Model [11] U 1.0173 0.5692 0.6072 63.78 0.0161 0.0541
Dynamic Fusion Network [41] U 1.0165 0.5937 0.6183 65.36 0.0178 0.0586
Knowledge Grounded [2] M 1.0112 0.6281 0.6428 68.71 0.0186 0.0693
M-GLMP M 1.0081 0.6386 0.6597 70.85 0.0199 0.0737
Position and Attribute Aware [7] M 1.0076 0.6399 0.6675 70.53 0.0197 0.0754
Proposed Model MHVMN + Aspect M 1.0069 0.6581 0.6974 72.38 0.0210 0.0823
Here, U means unimodal and M stands for multimodal information.
proposed approach with the unimodal approach [41] that fuses information of different domains
for a better generation using the available implementation.5 The table shows that the dynamic
fusion technique outperforms all the other unimodal approaches on our MDMMD++ dataset. The
reason behind the improvement is that all the information extracted from the different domains is
effectively fused to encourage accurate and consistent responses. Our proposed multimodal framework exhibits an increase of 7% in Entity F1 score, proving the effectiveness of multimodality and
aspect in our methodology.
In comparison to the existing multimodal approaches, we see that our proposed MHVMN +
Aspect outperforms both the approaches presented in Refs [2] and [7]. The primary reason for
the degradation in performance is that the existing approaches [2, 7] employ a hierarchical RNN
framework for generating the responses while we use a variational memory network that has a
higher ability to capture dependencies for a better generation. Also, the aspect controlled decoder
in our work helps generate responses according to the user’s demands that inherently help in
generating specific and coherent responses. On our MDMMD++ dataset, we see the improvements
of almost 3% and 2% BLEU-4 scores from the existing multimodal approaches reported in Refs [2]
and [7], respectively.
From Table 9, it is evident that the M-GLMP outperforms the unimodal baselines as it has the
image information for coherent response generation. By using multimodal information, we see
improvement in the M-GLMP framework in comparison to the MHRED framework proposed in
Ref. [2]. This is possibly because the memory network provides better contextual information in
comparison to the RNN network. In contrast, the performance of M-GLMP slightly decreases compared to the Position and Attribute Aware [7] approach. The feasible explanation is that in Ref. [7],
the author’s employed position and attribute-aware attention mechanism to focus on the correct
image with respect to the current input is not performed in M-GLMP. However, the Entity F1 score
is slightly higher with its ability to copy the correct entities with the help of the pointer network.
Finally, our proposed framework MHVMN + Aspect outperforms the M-GLMP architecture in
terms of all the metrics, mainly because it can focus on the correct entity as the information is
provided directly to the decoder. Also, the hierarchy in dialogue captured by the memory network
in our proposed framework provides a stronger conversational context than the M-GLMP network.
Through this analysis, it can be established that our proposed framework outperforms both the
existing unimodal and multimodal approaches with its ability to generate aspect-centric responses
that accomplish the user needs that assist in overall task completion in a multimodal dialogue
setting.
5https://github.com/LooperXX/DF-Net.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:26 M. Firdaus et al.
Table 10. Results of Human Evaluation
Aspect Domain
Model Description Fluency Relevance Appropriateness Consistency
012 0 1 0 1 0 1
Baseline
Approaches
HRED 34.36 35.83 29.81 55.93 44.07 62.20 37.80 57.73 42.27
HVMN 32.11 36.71 31.18 52.56 47.44 60.14 39.86 54.14 45.86
MHRED 31.85 35.43 32.72 54.33 45.67 59.88 40.12 54.39 45.61
MHVMN 30.77 35.60 33.63 51.81 48.19 57.64 42.36 52.28 47.72
HRED + Aspect 29.55 36.88 33.57 53.49 46.51 51.31 48.69 53.64 46.36
HVMN + Aspect 29.05 35.63 35.32 49.47 50.53 49.17 50.83 50.17 49.83
MHRED + Aspect 24.92 37.77 37.31 50.09 49.91 47.44 52.56 49.95 50.05
Proposed
Approaches MHVMN + Aspect 18.64 39.65 41.71 45.72 54.28 44.85 55.15 46.85 53.15
6.4 Human Evaluation Results
Along with the automatic evaluation, human evaluation is also essential for assessing the quality of
the responses. Hence, we evaluate our baseline and proposed approach with the human evaluation
metrics for our specified task of generating responses in a multimodal setup. In Table 10, we present
the results of human evaluation for all the baselines and the proposed model. Fluency ensuring
the grammatical correctness of responses is an important metric to evaluate the responses. The
fluency scores of the baseline HRED model are the lowest for grammatically correct replies due to
repetition and incomplete responses. In case of MHRED framework, there is an increment of 2.91%
in fluent responses compared to the unimodal counterpart. The images help provide complete
knowledge and, thereby, help generate better responses that are grammatically correct.
As it is evident from the table, the proposed MHVMN + Aspect framework achieves the highest scores for the fluent responses with an increase of 4.4% in contrast to the baseline MHRED +
Aspect model. The scores for moderately correct responses are also higher, whereas the grammatically incorrect responses are low in the proposed approach. This indicates that the methodology
employed for generating aspect controlled generation can generate fluent and grammatically correct responses. Similarly, the proposed framework shows an increment of 4.37% over the MHRED +
Aspect baseline method for the relevance metric. This suggests that the proposed framework generates contextually correct responses. Also, from the table, it is visible that the relevance scores of
the memory network baselines are higher than the hierarchical encoder-decoder baselines. This
signifies that the memory network is capable of retaining the context information better than the
encoder-decoder approach.
As the current work revolves around the aspect, the generated responses are assessed according to the specified aspects. It is evident from the results that the proposed framework generates
responses that are appropriate to the specified aspects with an improvement of 2.59% from the
MHRED + Aspect baseline. The use of aspect information in the frameworks shows better performance compared to the others. The HRED + Aspect baseline has an improvement of 10.89% from the
HRED framework, indicating the fact that the aspect information specified directly to the decoder
helps in better generation of the responses. Similarly, it is visible that the HVMN + Aspect and
MHRED + Aspect baselines have an increment of 10.97% and 12.44%, respectively, from the HVMN
and MHRED baselines.
In the case of domain consistency, it is evident that the memory network-based approaches
outperform the hierarchical frameworks. This is major because the memory retention capability
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:27
Fig. 6. Generated examples from different models.
of these networks is superior and can generate responses consistent with the domain. The multimodal information also improves performance by providing additional knowledge of the domain,
thereby constraining the response generation to the domain being discussed. Therefore, our proposed framework with the multimodal information and memory network shows superior performance compared to the baseline methods. From the human evaluation, it can be concluded that
the generated responses are not only fluent and relevant but also consistent with the domain and
the specified aspect information.
6.5 Case Study
In Figure 6, we provide examples of the generated responses from different methods. The figure
shows that the response generated by the proposed framework is consistent with the specified
aspect. From the provided examples, it is visible that the MHRED and MHVMN baseline methods
generate responses that are generic, as they do not have the aspect information. This makes the
conversation less interactive and sometimes leads to the failure of the user’s objectives. In case of
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
15:28 M. Firdaus et al.
the MHRED + Aspect baseline, we see that though this model is capable of generating responses
conditioned on the aspect information, it lacks the participation required in the communication
and thereby makes the responses a little boring. The ability to capture the correct information from
the images and generate responses makes the proposed framework a robust framework compared
to the baselines. Also, the aspect information used by the MHVMN + Aspect framework helps in
taking the conversation forward and reaching the user’s goal. The generated responses are not
only contextually (in case of both text and images) correct and in accordance with the aspect, but
also diverse, making the proposed approach interesting and interactive.
6.6 Error Analysis
After a thorough analysis of the automatic and human evaluation results, we observe the generated
outputs from the baseline and the proposed frameworks to perform a detailed qualitative analysis
of the generated responses. Some of the common errors made by the baseline models and our
proposed approach are listed below.
—Incomplete responses: The baseline HRED framework generates incomplete responses.
Gold: Sorry, the color is not available in the second image; Predicted: Sorry, it is not present.
This is because the HRED framework is incapable of memorizing the contextual information
compared to the memory network.
—Loss of information: The unimodal baselines such as HRED and HVMN, generate responses that lack complete information. Gold: Here are the chairs in yellow color as in the
third image but not in round shapes as in the fifth image; Predicted: The chairs are here but
<unk> not in the shape. This error indicates that the unavailability of multimodal information (in this case, images) leads to the loss of information in the generated response.
—Contextually wrong domain: In some case, the baseline and the proposed framework
generate responses that are contextually incorrect with the domain. For example, with the
aspect color, the response generated belongs to the electronics domain, but the actual domain in the discussion is a restaurant. This type of error occurs due to the higher number
of utterances, with the color aspect belonging to the electronics domain compared to the
restaurant domain.
—Cross-domain aspect inappropriateness: In conversations with multiple domains, the
proposed and baseline models generate the responses with incorrect aspect information due
to the confusion between the domains and because of the common attributes belonging to
both the domains, such as price, color, number of people, and so on.
—Extra information: The proposed framework sometime generates responses that contain extra information. For example, Gold: There is a discount available for this room; Predicted:There is a discount available for you in this room. Hope you like it! These errors occur
possibly because most of the training samples have such phrases hope you like it in them that
inevitably gets generated in cases that do not have such expressions in the ground-truth.
—Mistakes in image identification: The baseline and proposed frameworks, in some cases,
confuse the images being discussed, leading to generating incorrect responses. For example,
Gold:I have beverages to go with the second image, but it is similar to the fourth one; Predicted:
I have got you beverages to go with the fourth image but nothing like the third one. This
indicates the model’s inability to capture the correct positional information of the images.
Also, the mention of different images in the contextual information confuses the model in
selecting the right images. As a series of images accompany the conversation, this leads to
the incorrect identification of the image that needs to be focused upon in some of the cases
with the generation of textual responses.
ACM Transactions on Intelligent Systems and Technology, Vol. 12, No. 2, Article 15. Publication date: February 2021.
Aspect-Aware Response Generation for Multimodal Dialogue System 15:29
—Incorrect products: The model generates responses with the products different from those
discussed in the conversational history as many products have common aspects. For example,color, pattern is an important aspect of a product. In few cases, the conversation revolves
around laptops while the generated response is related to headphones. The discrepancy in
products can be related to the fact that both products have common aspects and belong to
the same domain, leading to confusion between different products.
—Unknown tokens: Sometimes, the baseline and the proposed frameworks generate the
responses with unknown tokens. This occurs mainly in the case of names, brands, numbers,
and so on, due to the inefficiency of memorizing them and fewer training examples. For
example, restaurant names such as Delhi Darbar and Tung Fong are generated as unknown
tokens, mainly due to the framework’s inability to memorize these tokens as they occur very
few times in the training corpus. Similarly, an unknown token gets generated in different
furniture and electronic brands, leading to errors in responses.
7 CONCLUSION AND FUTURE WORK
With the progress in artificial intelligence, dialogue systems have reached new paradigms. Narrowing the gap between vision and language, multimodal conversational systems have gained
immense popularity. Complementary information in the form of images, audios, or videos to the
unimodal (text) systems has helped build robust systems. Task-oriented dialog systems focus on
assisting humans by helping them to achieve their desired goals. Response generation is a crucial component in every dialogue system. Our current work emphasizes the task of generating
responses in a multimodal dialogue system. In this work, we have proposed the task of aspect
controlled dialogue generation in a multimodal setup utilizing the information present in both
text and images. As in a goal-oriented system, the different aspects of a product are considered before purchasing, such as color, price, shape, and so on. Therefore, we design a response generation
framework with the specified aspects.
We have established strong baselines for our task, both unimodal and multimodal, based upon
the hierarchical encoder-decoder framework. Our proposed methodology employs a multimodal
hierarchical memory network with aspect information provided to the decoder to generate responses. We have created a large-scale task-oriented MDMMD++ dataset comprising 1.50M dyadic
conversations for our proposed task. The dataset consists of four different domains, such as restaurant, hotel, electronics, and furniture. Experimental results on the newly created dataset show
that our proposed technique outperforms the baseline models in automatic and human evaluation
metrics.
In the future, along with enhancing our proposed methodology’s architectural design, we would
like to investigate methods for image retrieval for complete multimodal response generation. To
handle the errors made by the current work, we would like to infuse a multi-attention network that
can focus on the text and images simultaneously to ensure the correct image selection according
to the given textual utterance. Concurrently, we would also like to incorporate fusion techniques
between the modalities to assure non-linear interaction to facilitate in the generation of better
responses that are complete, contextually correct, and have all the information. In addition, to
solve the errors due to incorrect domain, we would investigate domain-specific features that help
capture every domain’s accurate information, thereby promoting responses that are relevant and
coherent to the given domain. Moreover, the prediction of aspect from the dialogue history and its
incorporation at the decoder for the generation of aspect-aware responses is our future objective
for building an end-to-end framework. Furthermore, we would extend our framework for multiaspect based response generation.