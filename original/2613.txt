Many real-world human behaviors can be modeled and characterized as sequential decision-making processes, such as a taxi driver’s choices of working regions and times. Each driver possesses unique preferences
on the sequential choices over time and improves the driver’s working efficiency. Understanding the dynamics of such preferences helps accelerate the learning process of taxi drivers. Prior works on taxi operation
management mostly focus on finding optimal driving strategies or routes, lacking in-depth analysis on what
the drivers learned during the process and how they affect the performance of the driver. In this work, we
make the first attempt to establish Dynamic Human Preference Analytics. We inversely learn the taxi drivers’
preferences from data and characterize the dynamics of such preferences over time. We extract two types of
features (i.e., profile features and habit features) to model the decision space of drivers. Then through inverse
reinforcement learning, we learn the preferences of drivers with respect to these features. The results illustrate that self-improving drivers tend to keep adjusting their preferences to habit features to increase their
earning efficiency while keeping the preferences to profile features invariant. However, experienced drivers
have stable preferences over time. The exploring drivers tend to randomly adjust the preferences over time.
CCS Concepts: • Computing methodologies → Spatial and physical reasoning;
Additional Key Words and Phrases: Urban computing, inverse reinforcement learning, preference dynamics
1 INTRODUCTION
Taxi service is a vital part of transportation systems in large cities. Improving taxi operation efficiency is a crucial urban management problem, as it helps improve the transportation efficiency of
the city and at the same time improves the income of taxi drivers. In the same city, taxi operation
efficiency might differ significantly. Figure 1(a) shows the earning efficiency (total amount earned
normalized by total working time) of different taxi drivers in Shenzhen, China. The top drivers
earn three to four times more money than the bottom drivers.
A major cause of such difference is the difference in working experiences. Figure 1(b) shows
the growth of earning efficiency of new drivers over years.1 From March 2014 to December 2016,
the new drivers became more experienced and had much higher earning efficiency. During the
same time as shown in Figure 1(c), there is no obvious change to the local economy or market, as
the average earning efficiency of all drivers is pretty stable. This shows that drivers are trying to
improve their own strategies of looking for passengers based on their increasing knowledge of the
city.
However, each driver might have gained different knowledge during the learning process, which
in turn developed different preferences when making decisions. For instance, some drivers tend to
look for passengers around regions near their homes, whereas others might prefer to take passengers from city hubs (e.g., train station, airport). These preferences might be unique to individual
drivers and ultimately lead to differences in earning efficiency. Figure 1(b) shows that the “smart”
drivers (in blue) improve their earning efficiency faster than “average” drivers and reach a higher
level of earning efficiency eventually. Finding what adaptation strategies these smart drivers carry
could help us understand the learning process of successful drivers and therefore help new drivers
become more successful.
The passenger-seeking behavior of taxi drivers can be modeled as a Markov decision process
(MDP). Prior works on taxi operation management focused on recommending the optimal policy
or routes to maximize the chance of finding passengers or making profit [14, 18, 24, 25]. However,
these works only studied how to find the “best” strategies based on data rather than fundamentally
understanding how the drivers learned these strategies over time.
In this work, we make the first attempt to establish Dynamic Human Preference Analytics
(DHPA). We inversely learn the taxi drivers’ decision-making preferences, which lead to their
choices while looking for passengers. We also study how these preferences evolve over time and
how they help improve earning efficiency. The results shed lights on “how” the successful drivers
became successful and suggests “smarter” actionable strategies to improve taxi drivers’ performances. Our main contributions are as follows:
(1) We are the first to employ inverse reinforcement learning (IRL) to infer the taxi drivers’
preferences based on an MDP model.
(2) We extract various kinds of interpretable features to represent the potential factors that
affect the decisions of taxi drivers.
(3) We infer and analyze the preference dynamics of three groups of taxi drivers: self-improving
drivers, stabilized drivers, and exploring drivers.
(4) We analyze the preference trend of different groups of taxi drivers.
(5) We conduct experiments with taxi trajectories from more than 17k drivers over different
time spans. The results verify that each driver has unique preferences to various profile
and habit features. The preferences to profile features tend to be stable over time, and the
preferences on habit features change over time, which leads to higher earning efficiency.
1The dataset we have contains the records from March and November of 2014 and July through December of 2016.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
DHPA: Dynamic Human Preference Analytics Framework 8:3
Fig. 1. Dynamics of taxi drivers’ earning efficiency.
Fig. 2. Mean earning efficiency
of experienced drivers over
months in 2016.
Fig. 3. Shenzhen map data.
The rest of the article is organized as follows. Section 2 motivates and defines the problem.
Section 3 details the methodology. Section 4 presents evaluation results. Related works are discussed in Section 5, and the article concludes in Section 6.
2 OVERVIEW
In this section, we introduce the motivation of the proposed study and formally define the problem.
2.1 Motivation
It is a common perception that new drivers gradually learn how to make smart choices with regard
to time and can improve their working efficiency over time. We verify this perception through data
analysis. In Figure 1(b), the average earning efficiency of new drivers who joined in March 2014
increased by up to 100% in 2 years, whereas in Figure 2, the same measure of experienced drivers
in 2016 did not change much. This can be explained by the fact that experienced drivers learned
enough knowledge to make nearly optimal decisions.
We further noticed that drivers have very different learning curves, which ultimately affects
earning improvements they can achieve. As mentioned previously, in Figure 1(b), the two colors
represent two subgroups of new drivers who joined in March 2014. One group (in blue) includes
those who became “top” drivers after 2 years with higher earning efficiency, and the other (gray)
includes the rest of the drivers. Apparently, the former gained more useful knowledge, which
contributed to their earning improvement.
Little is known about what specific knowledge the drivers gained and which pieces are contributing the most to the earning improvement. Answering these questions would potentially
guide and train new drivers to become quick learners.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
8:4 M. Pan et al.
Fig. 4. DHPA framework.
We consider such “knowledge” as a series of preferences of a driver when making each decision,
such as “how frequent to visit the train station” and “how far away from home to go when seeking
passengers.” Specifically, we extract features from the data to represent such decisions a taxi driver
might face while working. To achieve the aforementioned goal, in this study we aim to answer two
questions: (1) how to recover the preferences of taxi drivers when making these choices and (2)
how these preferences change over time for different groups of drivers.
Problem definition. In a time interval T0 (i.e., 1 month), given a taxi driver’s trajectory data T˜ ,
and k environmental features [f0, f1,..., fk ], which influence drivers’ decision-making process
over time, we aim to learn the driver’s preference θ = [θ0, θ1,..., θk ] (i.e., weights to features
when the driver makes decisions). Second, for a long time horizon, with multiple time intervals
[T0,T1,...,Tm], we analyze the evolution pattern of the driver’s preferences over time.
2.2 Data Description
Our analytical framework takes two urban data sources as input, including (1) taxi trajectory data
and (2) road map data. For consistency, both datasets were collected in Shenzhen, China, in 2014
and 2016.
The taxi trajectory data contain GPS records collected from taxis in Shenzhen, China, during
March and November in 2014, and July to December in 2016. In total, 17,877 taxis were equipped
with GPS sets, where each GPS set generated a GPS point every 40 seconds on average. Overall, a
total of 51,485,760 GPS records were collected on each day, and each record contained five key data
fields, including taxi ID, timestamp, passenger indicator, and latitude and longitude. The passenger
indicator field is a binary value, indicating if a passenger is aboard or not.
The road map data of Shenzhen covers the area defined between 22.44◦ to 22.87◦ in latitude and
113.75◦ to 114.63◦ in longitude. The data is from OpenStreetMap [1] and has 21,000 roads of six
levels. Figure 3(a) shows the road map in Shenzhen.
3 METHODOLOGY
Figure 4 outlines our DHPA framework, which takes two sources of urban data as inputs and
contains three key analytical stages: (1) data preprocessing, (2) inverse preference learning, and
(3) preference dynamic analysis.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
DHPA: Dynamic Human Preference Analytics Framework 8:5
Fig. 5. Statistical distributions of P1, H1, H2, and H3.
3.1 Data Preprocessing
3.1.1 Map and Time Quantization. We use a standard quantization trick to reduce the size of
the location space. Specifically, we divide the study area into equally sized grid cells with a given
side-length s in latitude and longitude. Our method has two advantages: (1) we have the flexibility
to adjust the side length to achieve different granularities, and (2) it is easy to implement and
highly scalable in practice [10, 11]. Figure 3(b) shows the actual grid in Shenzhen, China, with a
side-length l = 0.01◦ in latitude and longitude. Eliminating cells in the ocean, those unreachable
from the city, and other irrelevant cells gives a total of 1,158 valid cells.
We divide each day into 5-minute intervals for a total of 288 intervals per day. A spatio-temporal
region r is a pair of a grid cells and a time interval t. The trajectories of drivers then can be mapped
to sequences of spatio-temporal regions.
3.1.2 Feature Extraction. Taxi drivers make hundreds of decisions throughout their work shifts
(e.g., where to find the next passenger, and when to start and finish working in a day). When
making a decision, they instinctively evaluate multiple factors (i.e., features) related to their current
states and the environment (i.e., the current spatio-temporal region). For example, after dropping
off a passenger, a driver may choose to go back to an area that she is more familiar with or a nearby
transport station (e.g., airport, train station). Here, we extract key features the drivers use to make
their decisions.
Note in our framework that each feature is defined as a numeric characteristic of a specific
spatio-temporal region, which may or may not change from driver to driver. For example, let fr
represent the average number of taxi pickups in history in location s during time slott. Apparently,
the value of feature fr is the same for every driver. However, another feature дr at r could be the
distance from s to the home of the driver. The value of this feature varies from driver to driver,
depending on their home locations. However, it does not change over time. The features we extract
can be roughly categorized by profile features and habit features, as detailed next.
Profile features. Each driver has unique personal (or profile) characteristics, such as home location, daily working schedule (time duration), and preferred geographic area. For each spatiotemporal region, we build the profile features. Here, we extract four profile features:
P1: Visitation Frequency. This group of features represents the numbers of daily visits to different regions of a driver as extracted from the historical data. Figure 5(a) shows the
distribution of visitation frequency to different regions of an arbitrarily chosen driver.
Here, visitation frequencies vary significantly across regions.
P2: Distance to Home. Each taxi driver has a home location, which can be extracted from the
driver’s GPS records. This feature characterizes the distance (in miles on the road network) from the current location to the driver’s home location. Different drivers may have
different preferences in regard to working close to their homes or not.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
8:6 M. Pan et al.
P3 & P4: Time from Start & Time to Finish. Taxi drivers typically work according to consistent
starting and finishing times. We construct two features to characterize the differences of
the current time from the regular starting and finishing time.
Habit features. The habit features represent the habits of the drivers, which typically are governed by experience (e.g., remaining near the train station instead of traveling around to find
passengers). We extract six habit features:
H1: Number of Pickups. This feature characterizes the demands in a cell during a time interval, and is extracted and estimated using the historical trajectories from all drivers. The
distribution on the numbers of pickups is shown in Figure 5(b).
H2 & H3: Average Trip Distance & Time. These features represent the average distance and the
travel time of passenger trips starting from a particular spatio-temporal region. Different
spatio-temporal regions can have different expected trip distances and travel time. For
example, the passengers picked up near the airport probably have longer trip distances
than the passengers picked up near the train station since the airport is farther away
from the downtown area than the train station. A driver’s preferences to these features
characterize to what extent the driver prefers long versus short distance trips and how
well the driver gains knowledge on the lengths of trips over the spatio-temporal regions.
The distribution of these features across spatio-temporal regions are showed in Figure 5(c)
and Figure 5(d), respectively:
H4: Traffic Condition. This feature captures the average traffic condition based on the time
spent by a driver in each spatio-temporal region. A long travel time implies traffic congestion. The preference of drivers over this feature represents how much drivers would
like avoid the traffic.
H5 & H6: Distance to Train Station & Airport. These features reflect the distances from the
current cell to Shenzhen train station and airport, respectively.
3.1.3 Driver Selection. Different drivers have different earning efficiencies, as shown in Figure 1(a). In the following, we describe the criteria we use to select drivers.
We estimate the earning efficiency of each driver in different time periods from their historical
data. The estimated earnings E of a driver in the whole sampling span (e.g., per month) is calculated
from the distancedo that the taxi is occupied with passenger. The factors we take into consideration
include the taxi fare in Shenzhen in 2014 and 2016, and the expense for gas. The taxi fare is 11 CNY
for the first 2 km, and the charge for each additional kilometer is 2.4 CNY. The estimated expense
for gas is 0.5 CNY per kilometer. The calculation of E is as follows:
E =
 11 − 0.5 ∗ do if do < 2
11 + (do − 2) ∗ 2.4 − 0.5 ∗ do else. (1)
Note that our model is easy to extend to other definitions of ”earnings.” Given the data we have,
and without losing much accuracy in regard to calculated earnings in terms of representing the
driver’s profits, we employ Equation (1) to estimate the earnings of each driver.
The earning efficiency re is defined as the average per-hour income (i.e., in Equation (2)).
re = E
tw
, (2)
where E is the income in the whole sampling time span, span (e.g., per month), and tw represents
the driver’s working time.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020. 
DHPA: Dynamic Human Preference Analytics Framework 8:7
Driver selection criterion. We select drivers with the highest earnings because the preference
learning algorithms require the input data to be generated by the converged policy (see more
details in Section 3.2). We note that drivers with high earning efficiencies are likely the most experienced (i.e., they use converged policies to make decisions).
3.2 Inverse Preference Learning
This section explains our inverse learning algorithm for extracting drivers’ decision-making process. We use an MDP to model drivers’ sequential decision making and relative entropy IRL to
learn their decision-making preferences.
3.2.1 Markov Decision Process. A Markov decision process (MDP) [4] is defined by a 5-tuple
S,A,T,γ , μ0, R so that
• S is a finite set of states and A is a finite set of actions,
• T is the probabilistic transition function withT (s
|s, a) as the probability of arriving at state
s by executing action a at state s,
• γ ∈ (0, 1] is the discount factor,2
• μ0 : S → [0, 1] is the initial distribution, and
• R : S × A → is the reward function.
A randomized, memoryless policy is a function that specifies a probability distribution on the
action to be executed in each state, defined as π : S × A → [0, 1].
We use τ = [(s0, a0), (s1, a1),..., (sL, aL )] to denote a trajectory generated by the MDP. Here, L
is the length of trajectory. We model the decision-making process of taxi drivers with the MDP as
follows:
• State: A spatio-temporal region specified by a geographical cell and a time slot.
• Action: Traveling from the current cell to one of the eight neighboring cells, or staying in
the same cell.
• Reward: The inner product of the preference function (as a vector) θ and the feature vector
f on each state-action pair.
Note that in the MDP settings, the reward of each state-action pair is the inner product of
the preference function and the feature vector. The preferences are the weights of each feature.
The driver aims to maximize the accumulated reward when making decisions. To interpret each
of the preferences recovered, we can consider two factors: the sign and the magnitude of the
preference (weights of each feature). A positive preference means that the driver prefers to go
to the regions where the corresponding feature value is higher than other locations, and larger
magnitude can imply that the driver pays more attention to the corresponding feature, and vice
versa for negative sign and smaller magnitude. In this work, we design two categories of features:
profile features and habit features. The intuition is that the preferences for some features can be
time invariant because these features are closely related to the drivers’ profiles, such as home
location and working schedule, which usually do not change.
The preferences to some other features can be time variant because these features are related
to the habits of drivers, such as the number of pickups and the distance to train stations. The
preferences for the habit features are considered as the habits of the drivers. The drivers can learn
to change their habits (i.e., preferences to habit features) to improve their earning efficiencies.
Figure 6 shows an example of trajectory in the MDP: a driver starts in state s0 with the taxi
idle, and takes the action a0 to travel to the neighboring cell S1. After two steps, the driver reaches
2Without loss of generality, we assume γ = 1 in this work, and it is straightforward to generalize our results to γ  1.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020. 
8:8 M. Pan et al.
Fig. 6. MDP of a taxi driver’s decision-making process.
state S2, where she meets a passenger. The destination of the new trip is cell S3. The trip with the
passenger is a transition in the MDP from S2 to S3.
3.2.2 Inverse Preference Learning. Given the observed trajectory set T˜ of a driver and the features extracted on each state-action pair (s, a), the inverse preference learning stage aims to recover
a reward function (i.e., preference vector θ) under which the observed trajectories have the highest likelihood to be generated [15]. Various IRL approaches, such as apprenticeship learning [3],
maximum entropy IRL [31], Bayesian IRL [17], and relative entropy IRL [5], have been proposed
in the literature.
Our problem possesses two salient characteristics. First, the state space is large. We have 1,158
cells and 288 time intervals. Therefore, the total number of states is 1,158 × 288 ≈ 330k. Second,
the transition probability is hard to measure, partly because of the large state space issue.
Therefore, we adopt a model-free IRL approach, namely relative entropy IRL [5], that does not
require estimating transition probabilities and is more scalable than other alternatives.
The optimization problem. Let T denote the set of all possible trajectories of the driver decisionmaking MDP, outlined in Section 3.2.1. For any τ ∈ T , denote P (τ ) as the trajectory distribution
induced by the taxi driver’s ground-truth policy and Q(τ ) as the trajectory distribution induced
by a base policy. The relative entropy between P (τ ) and Q(τ ) (in Equation (3)) characterizes the
distribution difference between P (τ ) and Q(τ ):
H(P 
Q) =

τ ∈T
P (τ ) ln P (τ )
Q(τ )
. (3)
The driver’s trajectory distribution is governed by the driver’s preference θ and thus is a function
of θ (i.e., P (τ |θ )). The relative entropy IRL aims to find a reward function θ that minimizes the
relative entropy in Equation (3) and matches the trajectory distribution to the observed trajectory
data.
P1: Relative Entropy IRL Problem. The relative entropy IRL problem is presented as
follows:
min
θ : H(P (θ )
Q) =

τ ∈T
P (τ |θ ) ln P (τ |θ )
Q(τ ) , (4)
s.t.:







τ ∈T
P (τ |θ )f τ
i − ˆ
fi






≤ ϵi,∀i ∈ {1,..., k}, (5)

τ ∈T
P (τ |θ ) = 1, (6)
P (τ |θ ) ≥ 0, ∀τ ∈ T , (7)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.                
DHPA: Dynamic Human Preference Analytics Framework 8:9
where i is the feature index, f τ
i is the i’s feature count in trajectory τ , and ˆ
fi = 
τ ∈T˜ f τ
i /|T | ˜ is the
feature expectation over all observed trajectories in T˜ . ϵi is a confidence interval parameter, which
can be determined by the sample complexity (the number of trajectories) via applying a Hoeffding’s
bound. The constraint Equation (5) ensures that the recovered policy matches the observed data.
The constraints (Equations (6) and (7)) guarantee that the P (τ |θ )’s are non-negative probabilities
and thus sum up to 1.
Solving P1. The function Q(τ ) and P (τ |θ ) can be decomposed as
Q(τ ) = T (τ )U (τ ) and P (τ |θ ) = T (τ )V (τ |θ ),
where T (τ ) = μ0 (s0)
K
t=1T (st |st−1, at−1) is the joint probability of the state transitions in τ , for
τ = [(s0, a0), (s1, a1),..., (sK, aK )], with μ0 (s0) as the initial state distribution. U (τ ) (resp.ectively,
V (τ |θ )) is the joint probability of the actions conditioned on the states in τ under driver’s policy
πθ (respectively, a base policy πq). As a result, Equation (4) can be written as follows:
H(P (θ )
Q) =

τ ∈T
P (τ |θ ) ln V (τ |θ )
U (τ ) . (8)
Moreover, when πq (a|s) at each state s is uniform distribution (e.g., πq (a|s) = 1/|As |, withAs as the
set of actions at state s), the problem P1 is equivalent to maximizing the causal entropy of P (τ |θ )
(i.e., 
τ ∈T P (τ |θ ) lnV (τ |θ )) while matching P (τ |θ ) to the observed data [30]. Following the similar
process outlined in Boularias et al. [5], P1 can be solved by a gradient descent approach, with the
stepwise updating gradient as follows:
∇д(θ ) = ˆ
fi −

τ ∈T π U (τ )
π (τ ) exp(
k
j=1 θi f τ
j )

τ ∈T π U (τ )
π (τ ) exp(
k
j=1 θi ) − αiϵi, (9)
where αi = 1 if θi ≤ 0 and αi = −1 otherwise. T π is a set of trajectories sampled from T˜ by an
executing a given policy π.U (τ ) is the joint probability of taking actions conditioned on the states
in a observed trajectory τ , induced by uniform policy πq (a|s) = 1/|As |.
See Algorithm 1 for our IRL algorithm.
ALGORITHM 1: Relative Entropy IRL
Input: Demonstrated trajectories T˜ , feature matrix F , threshold vector ϵ, learning rate α, and
executing policy π.
Output: Preference vector θ.
1: Randomly initialize preference vector θ.
2: Sample a set of trajectories. T π using π.
3: Calculate feature expectation vector ˆ
f .
4: repeat
5: Calculate each feature count f τ
i .
6: Calculate gradient ∇д(θ ) using Equation (9).
7: Update θ ← θ + α∇д(θ ).
8: until ∇д(θ ) < ϵ.
3.3 Preference Dynamic Analysis
Using Algorithm 1, we can inversely learn the preference θ for each driver, during each time interval (e.g., a month) over time, and obtain a sequence of preference vectors {θ1,..., θN }. For each
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020. 
8:10 M. Pan et al.
driver, we can conduct hypothesis testing to examine if the change of the preference vectors over
months is significant or not. We denote the preference vector learned for taxi driver p in period
Ti as θp
i and that in period Tj as θp
j . Then, we can obtain two preference vector sample sets in i-th
and j-th months as Si and Sj over a group of n drivers as follows:
Si =
θ 1
i , θ 2
i ,..., θn
i

, (10)
Sj =
θ 1
j , θ 2
j ,..., θn
j

. (11)
With Si and Sj , we will examine if the entries in preference vectors changed significantly or not
from the i-th to j-th month using a paired sample t-test [22]. For each feature fm, the null hypothesis is that the difference between the m-th entry of each θp
i in Si and θp
j in Sj equals 0, which
means drivers’ preference to feature fm does not change significantly from the i-th month to the
j-th month. Otherwise, the alternative hypothesis indicates a significant change. Taking the difference between Si and Sj as ΔSij = {Δθ 1
ij, Δθ 2
ij ,..., Δθn
ij} = {θ 1
i − θ 1
j , θ 2
i − θ 2
j ,..., θn
i − θn
j }.
The t-test statistics of the m-th entry is as follows:
tij (m) = Z
s = Δθij (m) − μ
δ/
√
n , (12)
where μ is the sample mean, n is the sample size, and δ is the sample square error. The t-distribution
for the test can be determined given the degree of freedom n − 1. Given a significance value 0 <
α < 1, we can get a threshold of the t-value tα in the t-distribution. Then, if tij (k) > tα , the null
hypothesis should be rejected with significance α; otherwise, we can accept the null hypothesis
with significance α. Usually, we set α = 0.05, which also means that the confidence of the test is
1 − α = 0.95.
3.4 Preference Trend Analysis
In Section 3.3, we employ the hypothesis test to examine if the change of the preference over
months for each driver is significant or not. In this section, we investigate the trends of the preferences for different groups of drivers regarding the significantly changed preferences. For the
significantly changed preference to feature fs , we can obtain the preference of driver k in the i-th
and j-th (i < j) months as θk
i (s) and θk
j (s). Then, the set of preferences to feature fs over a group
of n drivers in the i-th month can be denoted as follows:
Si (s) =
θ 1
i (s), θ 2
i (s),..., θn
i (s)

, (13)
Sj (s) =
θ 1
j (s), θ 2
j (s),..., θn
j (s)

. (14)
We want to investigate in detail how the preferences change (i.e., some preferences trend up over
time, whereas others trend down). Here, we define the positive trend rate rp in Equation (15) to
characterize the increasing trend of the each preference for each group of drivers, and the negative
trend rate rn to characterize the decreasing trend, which equals 1 − rp :
rp =
n
k=1 I s
ij (k)
n , (15)
where n is the number of drivers in the group and
I
s
ij (k) =
⎧⎪
⎨
⎪
⎩
1 if θk
i (s) < θk
j (s)
0 if θk
i (s) > θk
j (s). (16)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.    
DHPA: Dynamic Human Preference Analytics Framework 8:11
Fig. 7. Earning efficiency gap distribution.
4 EXPERIMENTS
In this section, we conduct experiments with real-world taxi trajectory data to learn the preferences of different groups of taxi drivers and analyze the preference evolution patterns for each
group.
4.1 Experiment Settings
When analyzing the temporal dynamics of the drivers’ decision-making preferences, the null hypothesis is that the difference between the preferences in two time periods is not significant. The
alternative hypothesis is that the temporal preference difference is significant. We choose the t-test
significance value α = 0.05.
Driver group selection. We aim to analyze how taxi drivers’ decision making preferences evolve
over time. For each month, we select 3,000 drivers with the highest earning efficiency. The reason
we select these drivers is that they are likely more experienced drivers, thus with near-optimal
policies, which is required by the maximum entropy principle [31] to ensure precise preferences
recovered by IRL from the demonstrations. To evaluate the preference change across 2 months (i.e.,
the i-th and j-th months), we find those drivers from those experience drivers, who also show up in
both months for our study. For example, in July and December of 2016, there are 2,151 experienced
drivers in common.
Then, we calculate the difference of earning efficiency of each driver in the 2 months. Figure 7
shows the gap distribution in July and December of 2016. We will choose three groups of drivers
for preference dynamics analytics based on the drivers’ earning efficiency gaps:
• Group #1 (self-improving drivers): 200 drivers whose earning efficiencies increase the most.
• Group #2 (stabilized drivers): 200 drivers whose earning efficiency gaps are small (i.e., close
to 0).
• Group #3 (exploring drivers): 200 drivers whose earning efficiencies decrease the most.
The self-improving drivers are more likely to have learned a lot during the time span from July to
December of 2016. By analyzing their preference dynamics, we can get a sense of how they learned
over time. The stabilized drivers are those whose earning efficiencies did not change much from
July to December of 2016, and we want to validate if their preferences were also stable during the
time span to cross validate how taxi drivers gain knowledge over time. The exploring drivers are
those whose earning efficiencies decrease the most from July to December of 2016, and we want
to figure out why this happened to these drivers by analyzing their learning curve via our DHPA
framework. As the first attempt to analyze the learning curve of taxi drivers, in this work we do
not explore deeper to individual taxi drivers. In our future work, we will explore the learning curve
of individual taxi drivers.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
8:12 M. Pan et al.
Fig. 8. Preference dynamics between July and each
of the following 5 months of Group #1 drivers.
Fig. 9. Preference dynamics between July and each
of the following 5 months of Group #2 drivers.
Experiment plan. We use 12 months’ trajectory data across 3 years for our study (i.e., July–
December 2014 and July–December 2016). We evaluate the preference dynamics across month
pairs. First, we set up the month July of 2016 as the base month and compare the preferences of
drivers in Group #1 and Group #2 with that of 5 subsequent months (August–December 2016),
respectively, to examine the dynamics of potential habits’ preferences in a short period and a long
period. We define the short period as July and August of 2016 and July through September of 2016,
and the long period as July through November of 2016 and July through December of 2016. We
apply familywise t-tests with Bonferroni correction to avoid an inflation of false positives.
4.2 Preference Dynamics Analysis
Now we present the results of the analysis of the preference dynamics of two driver groups over
time.
4.2.1 Results for Group #1. The table in Figure 8 shows the t-values obtained for comparing
preferences (with respect to each feature) in July 2016 to that of August through December of
2016, respectively. For these self-improving drivers, the boxes of failed tests are marked with red
color and the corresponding t-values. Note that these tests are conducted individually without
comparisons among them because we want to examine whether there exists a significant preference change for individual features in a specific month compared with July. First, with a time
span of less than 3 months, the preferences do not show any significant change. However, when
the time space is larger than 3 months, preferences to some habit features change significantly if
viewed as individual tests, including H1: Number of Pickups, H3: Average Trip Time, and H4: Traffic
Condition. This makes sense, as over time the self-improving drivers tend to gain the knowledge
of where the demands, low traffic, and long trip orders are. However, the preferences to all four
profile features and other habit features stay unchanged over half a year.
According to the results of the preceding individual tests, we notice that the preferences to three
habit features (H1, H3, H5) might change significantly over a long period (i.e., after 3 months). To
validate these preference dynamics over a long period, we conduct familywise hypothesis tests to
examine if the preferences to these features change significantly after a long period. We consider
July to August and July to September as the short period, and July to November and July to December as the long period. Since only the preferences to the three habit features potentially change
significantly, we have six tests in total. After Bonferroni correction, the results are presented in
Figure 10. We observe that, after a long period, the preferences to H1: Number of Pickups and H4:
Traffic Condition change significantly, whereas the preference to H3: Average Trip Time does not
show a significant change. In addition, preferences to these three habit features do not change
after a short period.
4.2.2 Results for Group #2. The table in Figure 9 shows the t-values obtained for preference
comparison of drivers in Group #2.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
DHPA: Dynamic Human Preference Analytics Framework 8:13
Fig. 10. Validation on the long-term preference dynamics.
Fig. 11. Preference dynamics between July and
each of the following 5 months of Group #3 drivers.
Fig. 12. Positive rates of each preference in
Group #3.
Fig. 13. Positive rate of each preference in Groups #1 and #2.
Clearly, the preferences to all profile and habit features stay unchanged over half a year, which
means that these stabilized drivers have kept the same strategy of finding passengers over half
a year. This is consistent with their unchanged earning efficiencies over time. The reason the
stabilized drivers maintained stable preferences is either because they were very experienced and
already obtained the optimal strategies given the profiles they had, which means that they had
reached the upper bound of earning efficiency for the drivers who have similar profiles, or they
had found a near optimal strategy and still have potential space for improvement but were not
motivated to find a better one, or some other potential reasons. As the first attempt to analyze the
preference dynamics in this work, we do not dig that deep to figure out the exact reasons these
drivers maintained stabilized preferences, but we will investigate this problem in future work.
4.2.3 Results for Group #3. The table in Figure 11 shows the t-values obtained for preference
comparison of drivers in Group #3. The preference of this group of drivers changed a lot over
months, most habit features changed, and the preferences to one or two profile features changed
in November and December. Group #3 drivers are the exploring drivers, whose earning efficiencies
drop over these months, and the results reveal that they try changing their preferences significantly
over time to explore new strategies, but the attempts do not work for the growth of their earning
efficiencies.
4.3 Preference Trend Analysis
In this section, we present the results of the preference trend analysis over three driver groups.
Results for Group #1. The results of the preference trend analysis for Group #1 are shown in Figure 13. The values in the table are rp ’s for each preference between July and December. The values
in red indicate that most drivers have a positive trend on the preference, which is consistent with
the result of the preference dynamics analysis, since the most significantly changed preferences are
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
8:14 M. Pan et al.
Fig. 14. Box plots of the preferences in December 2016.
the same (i.e., the preferences to H1: Number of Pickups, H3: Average Trip Time, and H4: Traffic Condition). For example, the preference to feature H1 has rn = 0.62, which is prominently larger than
rp = 0.38. This indicates that most of the self-improving drivers tend to reduce their preference to
feature H1 (i.e., the number of pickups). This is reasonable because the regions with a large number
of pickups are usually crowded (e.g., downtown area). To complete a service trip is time consuming,
which can damage the earning efficiency of taxi drivers. As for the significantly changed preferences to features H3: Average Trip Time and H4: Traffic Condition, the rp ’s are prominently larger
than the rn’s, respectively, which indicates that the number of self-improving drivers who increase
their preferences on feature H3 and H4 is prominently greater than those who decrease.
Results for Group #2. The results of the preference trend analysis for Group #2 are shown in
Figure 13. We notice that the rp ’s are close to 0.50 regarding the preference to each of the features,
which is consistent with the results of the preference dynamics analysis in Section 4.2.
Results for Group #3. The results of the preference trend analysis for Group #3 are shown in
Figure 12. The values in the table are the rp ’s calculated between July and each of the following months. The values in red indicate that most drivers have a positive trend on the preference
compared with July, and the blue ones indicate that most drivers have a negative trend on the preference compared with July. We find that the exploring drivers change their preferences randomly.
Taking the preference to feature H5: Distance to Train Station as an example, a significant positive
trend is found in August and October, whereas in December it switches to a negative trend. Similar
patterns can be found in the preferences to features H1, H3, and P3.
4.4 Preference Distribution Analysis
To explore the different features that different groups of drivers pay attention to, we analyze
the distribution of the preferences in December for self-improving (Figure 14(b)), stabilized
(Figure 14(c)), and exploring (Figure 14(d)) groups, as well as the entire taxi driver population
(Figure 14(a)). We find that in Figure 14(a), the preference median of profile features are all close to
0, and the preference medians to H1: Number of Pickups, H2: Average Trip Distance, and H3: Average Trip Time are relatively higher than the other three habit features, which implies that overall,
taxi drivers prefer a higher number of pickups and longer trips. From Figure 14(b) through (d),
we find that the self-improving drivers and the stabilized drivers have higher preferences to H1:
Number of Pickups, which indicates that they gained sufficient knowledge on which regions have
high demands. In contrast, the exploring drivers have lower preferences to H1: Number of Pickups.
This reveals that they are still learning the distribution pattern of travel demands. Moreover, the
preference to H2: Average Trip Distance of the exploring drivers is higher than that of other groups,
which implies that the exploring drivers paid excessive attention to the long distance trips. This
may be one of the negative effects leading to a decreasing earning efficiency trend, because the
longer the trip distance, the more time it costs.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
DHPA: Dynamic Human Preference Analytics Framework 8:15
Fig. 15. The decision-making preferences of John. Fig. 16. Heatmap of trajectory of John.
Fig. 17. Heatmap of pickups of John. Fig. 18. The decision-making preferences of Mike in July, October, and December.
4.5 Case Study
4.5.1 Case of Preference Dynamics. To further understand the preference dynamics, we look
into individual drivers to showcase how the preference and working behaviors evolve over time.
Here we show one randomly selected driver from Group #1. Let us call him “John.” John’s earning efficiency grew from 41.84 CNY/hour to 52.24 CNY/hour from July to December of 2016. His
preferences in the months of July and December are listed in Figure 15. Clearly, the preferences
to the profile features remain unchanged, whereas the preferences to some habit features, such as
H1: Number of Pickups and H5 & H6: Distance to Train Station & Airport, changed. When we look
into John’s driving behaviors, it matches the preference change perfectly.
Preference change to H1. The preference change to feature H1 indicates that John increased
his preference to areas with a high volume of pickup demands. Figure 16(a) and (b) show the
distribution of trajectories when the taxi was idle in the morning rush hours in July 2016 and
December 2016, respectively. Figure 17(a) and (b) show the all-taxi pickup demand distributions
in the morning rush hours in July and December of 2016, respectively. The citywide demand
distribution does not change. However, during the morning rush hours, John changed his strategy
from July to December of 2016 (i.e., to look for passengers from the high-demand areas). This is
consistent to the preference change to feature H1 (number of pickups).
Preference change to H5. The preference change to feature H5 (i.e., distance to train station) is
also significant. The negative preference indicates that John prefers to be closer to the train station
to look for passengers. Over time, this preference became stronger. To explain this phenomenon,
we highlighted the train station in Figure 16. The statistics we obtained from John’s trajectory data
showed that the percentage of orders received near the train station increased from 11.93% in July
2016 to 14.21% in December 2016, which is consistent with the preference change.
The results of preference dynamics analysis in Section 4.2.1 show the overall pattern of Group #1,
which illustrate that the self-improving drivers showed significant dynamics on their preferences
to features H1: Number of Pickups and H4: Traffic Condition, and the preference to H1 tends to
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
8:16 M. Pan et al.
Fig. 19. Heatmap of trajectory of Mike.
decrease in the group. However, in the case study, “John” is an individual driver randomly selected
from the group, who happened to have a reversed trend regarding the preference to feature H1.
This is not a contradiction since the results in Section 4.2.1 are for the overall group, whereas in
the case study, the randomly selected driver is an individual in the group, who might not maintain
the same preference dynamics as the whole group.
4.5.2 Case of Preference Trend. To further understand the preference trend, we also look into
an individual driver’s showcase to investigate how exactly the preference changed over time from
July to December. We randomly select a driver from Group #3. Let’s call him “Mike.” Mike’s earning
efficiency dropped from 46.15 CNY/hour to 41.34 CNY/hour from July to December in 2016. His
preferences in July, October, and December of 2016 are shown in Figure 18. The preferences to
some features changed randomly (e.g., H5). H5 is the habit feature: distance to the train station.
The negative values indicate that Mike prefers to be closer to the train station. Compared with
the preference to H5 in July, the preference becomes weaker in October and becomes stronger in
December. To explain this, we visualize the distributions of the trajectories of Mike in Figure 19.
The distribution near the train station becomes more scattered in October and more concentrated
in December compared with that in July.
4.6 Takeaways and Discussions
From our studies on a large amount of taxi trajectory data spanning for 3 years, we made the firstever report on how real-world taxi drivers make decisions when looking for passengers and how
their preferences evolve over time. Overall, three key takeaways are summarized as follows:
1. Each driver has unique preferences to the driver’s profile features, which tend to be stable
over time.
2. While learning the environments, drivers may change their preferences to habit features.
3. While exploring the environments, drivers may change their preferences to profile and
habit features randomly.
Our findings can potentially be utilized to assist and guide taxi drivers to improving their earning
efficiencies. For example, for those slow-learning drivers, by learning their preferences, especially
the preferences to habit features, we can diagnose which knowledge in terms of the features they
are lacking (e.g., not familiar with the high-demand regions). As a result, some guiding messages
may be sent directly to the drivers about such information to assist the drivers to improve a better
policy faster. In addition, our proposed DHPA framework can easily adapt to different time interval
analyses (e.g. over months, over days, over time in a day). One only needs to change the trajectory
extraction in Stage 1 according to the different settings of time intervals. Due to space limitations,
we may not present the results of all possible settings of time intervals in this article.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 1, Article 8. Publication date: January 2020.
DHPA: Dynamic Human Preference Analytics Framework 8:17
5 RELATED WORKS
Taxi operating strategies (e.g., dispatching, passenger seeking) and driver behavior analysis have
been studied extensively in recent years due to the emergence of the ride-sharing business model
and urban intelligence. However, to the best of our knowledge, we make the first attempt to employ
IRL to analyze the preference dynamics of taxi drivers. Works related to our study are summarized
next.
Urban computing, transportation, and geo-informatics are general research areas that integrate
urban sensing, data management, and data analytics together as a unified process to explore, analyze, and solve crucial problems related to people’s everyday life [6, 7, 9, 10, 12–14, 20, 21, 23, 26,
27, 29]. In particular, several works study taxi operation management, such as dispatching [8, 19]
and passenger seeking [24, 25, 28], aiming at finding an optimal actionable solution to improve the
performance/revenue of individual taxi drivers or the entire fleet.
Rong et al. [18] solved the passenger seeking problem by giving direction recommendations to
drivers. However, all of these works focus on finding “what” are the best driving strategies (as an
optimization problem) rather than finding “why” and “how” good drivers make these decisions.
By contrast, our work focuses on analyzing the evolving preferences of good drivers that helped
them make better and more profitable decisions.
IRL aims to recover the reward function under which the expert’s policy is optimal from the
observed trajectories of an expert. There are various IRL methods. For example, Ng and Russell
[15] found that there is a class of reward functions that can lead to the same optimal policy, and
it proposed a strategy to select a reward function. However, this method is not proper for analyzing human behaviors because it uses the deterministic policy in the MDP, whereas human
decisions tend to be non-deterministic. Ziebart et al. [31] proposed an IRL method by maximizing
the entropy of the distribution on state actions under the learned policy. Although this method
can employ stochastic policy, the computation efficiency is not friendly to large-scale state space,
and it requires the information of the model. In this article, we employ relative entropy IRL [5],
which is model free and employs softmax policy. Our work, compared with the preceding related
work, is the first to apply IRL to study the evolving driving preferences of taxi drivers.
6 CONCLUSION
In this article, we made the first attempt to employ IRL to analyze the preferences of taxi drivers
when making sequences of decisions to look for passengers. We further studied how the drivers’
preferences evolve over time, during the learning processes. This problem is critical to helping
new drivers improve performance quickly. We extracted different types of interpretable features
to represent the potential factors that affect the decisions of taxi drivers and inversely learned
the preferences of different groups of drivers. We conducted experiments using large-scale taxi
trajectory datasets, and the results demonstrated that drivers tend to improve their preferences to
habit features to gain more knowledge in the learning phase and keep the preferences to profile
features stable over time.