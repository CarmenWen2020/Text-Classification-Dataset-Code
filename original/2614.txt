In this article, we address the multiple-choice machine comprehension (MC) problem in natural language processing. Existing approaches for MC are usually designed for general cases; however, we specially develop
a novel method for solving the multiple-choice MC problem. We take the inspiration generative adversarial networks (GANs) and first propose an adversarial framework for multiple-choice oriented MC, named
McGAN. Specifically, our approach is designed as a GAN-based method that unifies both generative and
discriminative MC models. Working together, the generative model focuses on predicting relevant answer
given a passage (text) and a question; the discriminative model focuses on predicting their relevancy given
an answer-passage-question set. Based on the competition via adversarial training in a minimize-maximize
game, the proposed method takes advantages from both models. To evaluate the performance, we test our
McGAN model on three well-known datasets for multiple-choice MC. Our results show that McGAN can
achieve a significant increase in accuracy compared to existing models based on all three datasets, and it
consistently outperforms all tested baselines, including state-of-the-art techniques.
CCS Concepts: ‚Ä¢ Computing methodologies‚ÜíNatural language processing;Information extraction;
Additional Key Words and Phrases: Generative adversarial networks, recurrent neural networks, machine
comprehension, GAN, RNN, MC, RACE
1 INTRODUCTION
The objective of machine comprehension (MC) is to automatically provide answers to questions related to a given short text or passage. As a fundamental task and research topic in natural language
processing (NLP), the MC problem has been widely studied. It has been a common agreement in
the NLP community that machine-learning-based text comprehension can be considered as an essential step for a broad class of applications of text mining and text analysis. The quality of MC can
be easily measured based on the accuracy of the automatically provided solutions to the questions
related to the given text. For research purposes, there are extensive data sources for MC that are
publicly available for researchers to use. Generally, the existing MC corpus falls into three major
categories: cloze style [12], multiple choice [17], and question answering based [14, 23]. Related
MC datasets offer researchers unified resources to evaluate their MC methods. Although people
have been working on all three types of MC tasks mentioned previously, the multiple-choice MC
task is the only one that has not been well addressed. For example, when dealing with the wellknown multiple-choice dataset RACE [17], the state-of-the-art method can only achieve about half
of the performance of human performance [37, 42].
Our work focuses on addressing the multiple-choice MC problem by considering its special
characteristics in our models. Different from the other two types of questions, multiple-choice
questions follow a standard format consisting of a passage, questions, and candidate answers for
each question. We show an example from the RACE dataset in Appendix A (see Table A.1). In
addition, we also use two other datasets, Textbook Question Answering (TQA) [16] and SciQ [31],
which are also released for multiple-choice MC tasks. We will provide more detailed description
of these datasets in Section 4. Generally, there are two characteristics of the datasets for multiplechoice MC learning tasks. On the one hand, comparing with other MC datasets, the questions
and candidate answers of multiple-choice MC do not have to strictly follow the original texts. On
the other hand, the passages of multiple-choice MC cover different types of articles and are from
various domains (e.g., philosophy, biography, stories, news). The comprehensiveness of passages
and related questions with a multiple-choice format result in a challenging MC task.
In the recent literature on MC tasks, attention-based recurrent neural architectures [4, 17, 22, 37,
42] have shown great advantages, although they generally suffer from an important issue. In other
words, increasingly complex systems have been conceived for the MC tasks without comparing
to simpler architecture baseline systems. Some complicated questions might be answerable using
simple heuristics, and hence it is unnecessary to develop complex architectures with deep interaction layers for all questions. Another important observation is the fact that during training they just
train one ‚Äúdiscriminative‚Äù (regression/classification) model. To address the issue, some researchers
have used generative adversarial networks (GANs) [10] to handle the MC tasks. Different from the
attention-based recurrent neural architecture, which usually builds a complex top-down process
architecture aiming to model word-by-word interaction between different layers, GANs simultaneously train two sub-models and let them compete against each other. The two sub-models include
a generative model (G) from which artificial data examples can be sampled, and a discriminative
model (D) that classifies real data examples and simulated ones obtained from the generator. Due
to the advantages of GANs in benefiting from both G and D models, there is extensive literature
focusing on applying GAN and its extensions to MC problems. For example, Yu et al. [39] applied
GAN to discrete sequence generation by directly optimizing the discrete discriminator‚Äôs rewards.
Li et al. [18] presented a novel GAN model (called DAN) to open-domain dialogue generation and
generate higher-quality responses. Wang et al. [27] proposed a conditional GAN model (called
CGAN) that adopts a sentence-document gated recurrent unit (GRU) model with discriminative
adversarial training to enhance the semantic-inference ability. Che et al. [3] proposed a novel GAN
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
Unified GANS for Multiple-Choice Oriented MC 25:3
model (called MaliGAN) that derives a novel and low-variance objective using the discriminator‚Äôs
output that corresponds to the log-likelihood to stably model the discrete sequences.
However, aforementioned studies regarding GANs mainly focus on the task of text generation,
whereas the multiple-choice MC problem has not been well addressed and original GANs cannot
be applied directly. To this end, we take the inspiration from the GAN‚Äôs architecture and propose
an adversarial framework for the multiple-choice oriented MC. Specifically, McGAN is designed
as a GAN-based method that unifies both generative and discriminative MC models. Working
together, the generative model G focuses on predicting a relevant answer given a passage (text)
and a question, and the discriminative model D focuses on predicting their relevancy given an
answer-passage-question set. Such design is different from the original GAN, in which it generates
continuous data, and the input to the generator is only a random noise. In our McGAN framework,
which focuses on multiple-choice MC, we modify the generator based on stochastic sampling over
discrete candidate answers instead of using random noises. Moreover, we design the generator and
the discriminator with special settings as follows. Our generator aims to optimize the objective
that tries to generate relevant answers from candidate answers for a given question and passage.
Our discriminator models the joint probability distribution of the answer-passage-question and
determines whether they are correlated. We simultaneously consider two major types of models
for multiple-choice MC by unifying them with an adversarial training process.
To sum up, our work differs from existing models and contributes to the literature in four ways:
‚Ä¢ First, we develop a novel methodology, named McGAN, based on the characteristics of the
multiple-choice MC problem. We simultaneously consider two types of models for multiplechoice MC in our methodology by unifying them with an adversarial training process. The
first type of model assumes that the connection between the answer and its necessary information is an independent stochastic process, and the second one takes the advantages
of machine learning to capture hidden patterns of the relationship between answers and
passages.
‚Ä¢ Second, the generative model in our McGAN follows stochastic sampling over discrete candidate answers, whereas the original GAN has the deterministic generation based on the
sampled noise signal. By directly designing a unique GAN model with the generative and
discriminative MC models, the MC task can be completed with the idea of confrontation
training. Our methodology takes advantages from both supervised and unsupervised learning models and leads to significant improvement in handling the multiple-choice oriented
MC task.
‚Ä¢ Third, inspired by Yu et al. [39], we apply the policy gradient method in reinforcement learning (RL) to handle the discrete data in the MC task. Considering that traditional GANs only
work effectively for continuous data, our work serves as a successful example of extended
GANs for discrete data in NLP.
‚Ä¢ Last, we conduct extensive experiments on three famous datasets for multiple-choice MC.
The results demonstrate the superiority of our McGAN over all tested benchmarks, including the state-of-the-art ones.
2 PRELIMINARIES
2.1 Attention-Based Neural Networks
Attention-based neural networks have already been widely applied in MC models. For example,
many deep learning models were developed to address MC problems with a focus on span prediction (searching the most relevant content for given a question) [25, 30]. In other words, the span
prediction model searches the relevant content by locating a start index and an ending index in
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
25:4 Z. Liu et al.
Fig. 1. Illustration of the basic architecture that underlies most existing attention-based neural networks in
MC models.
the passage, and then uses the selected content to generate answers. Researchers also developed
similar models for cloze-style questions [4, 26, 32]. Most existing MC models share a very similar
framework, which contains the following five components:
‚Ä¢ Embedder layer: The embedder is responsible for embedding a sequence of tokens into a
sequence of n-dimensional states. We convert all words to respective embeddings (wordlevel or char-level embedding, pre-processing feature vectors).
‚Ä¢ Encoder layer: Encoding the question, the passage and the candidate answers. Embedded
tokens are further encoded by some form of composition function. A prominent type of
encoder is the bi-directional recurrent neural network (biRNN).
‚Ä¢ Interaction layer: Many studies focused on the interaction layer, which is responsible for
word-by-word interaction between context and question. Different ideas have been explored, such as co-attention [34], bi-directional attention flow [25], multi-perspective context matching [35], and fine-grained gating [38]. For multiple-choice MC, Zhu et al. [42]
present a hierarchical attention model, which adequately leverages candidate options to
model the interactions among questions, passages, and candidate options.
‚Ä¢ Elimination layer: The objective of the elimination layer is to refine the passage representation so that it does not focus on portions corresponding to irrelevant options. Parikh et al.
[22] proposed a model that mimics the human approach. The model takes a soft decision as
to whether an option should be eliminated or not.
‚Ä¢ Answer layer: A decoder to predict (generate or select) an answer. Most models divide the
prediction of the start and the end by separate networks. The complexity varies depending
on different network structures, which can be a single fully connected layer [25], convolutional neural networks (CNNs), or recurrent deep Highway networks [34].
An illustration of such systems is shown in Figure 1. The differences between the models in
the literature arise from the specific choice of encoder, decoder, interaction function, and iteration
mechanism.
2.2 Generative Adversarial Nets
An illustration of standard GANS is shown in Figure 2. In GANs, two models are trained simultaneously in an adversarial manner: a generator G that recovers the data distribution pdata from a
random noise z and generates highly realistic data, and a discriminator D(x) that learns to discriminate whether a given sample is real or not. The generative model D and the discriminative model
G are parameterized via two neural networks, and are learned by solving the minimize-maximize
optimization as follows:
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
Unified GANS for Multiple-Choice Oriented MC 25:5
Fig. 2. An illustration of standard GANs.
min
G max
D
J (G,D) = Ex‚àºpdata (x )[lo–¥(D(x))] + Ez‚àºpŒ∏ (z)[lo–¥(1 ‚àí D(G(z)))].
3 THE PROPOSED MODEL: MCGAN
In this section, we provide an unified framework to jointly apply a discriminative model (D) and
a generative model (G) in an adversarial network. Similar to the standard training strategy [10,
28, 36], we simultaneously train the two models. On the one hand, G computes the probability
distribution over answers by the reward from D. It focuses on predicting relevant answers given
a question and a passage. On the other hand, D exploits the unlabeled data selected (or generated)
from G and predicts the relevancy given a question-passage-answer pair. G tries to select (or generate) the more relevant answers and affect D. Meanwhile, D learns to draw a distinction between
the generated answers and the correct ones. Formally, the objective of G and D, JG‚àó,D‚àó
, is defined
as
J
G‚àó,D‚àó
= min
Œ∏ max œÜ

N
n=1

Ed‚àºptrue (a |p,qn )[lo–¥D(a|p,qn )]
+ Ed‚àºpŒ∏ (a |p,qn )[lo–¥(1 ‚àí D(a|p,qn ))]

,
(1)
where the generative model G is written as pŒ∏ (a|p,qn ), and the discriminative model D computes
the probability of passage p relevant to question q, which is given by the logistic sigmoid activation
function of the D score:
D(a|p,q) = sigmoid
fœÜ (p,q, a)

. (2)
Encoder. Considering a questionQ = {wQ
t }
qlen
t=1 and a passage P = {wP
t }
plen
t=1 , we convert all words
to their respective embeddings (word-level, char-level embedding, pre-processing feature vectors).
The char-level representations are generated by using the final hidden states of a bi-directional
gated recurrent unit (BiGRU) to apply to representations of characters. Then we take a BiGRU to
produce new embeddinguP
1 ,...,uP
plen,uQ
1 ,...,uQ
qlen anduO
1 ,...,uO
olen of the words in the passage,
question, and candidate answers, respectively:
uQ
t = BiGRUQ

uQ
t‚àí1,

e
Q
t ,charQ
t

, (3)
uP
t = BiGRUP

uP
t‚àí1,

eP
t ,char P
t

, (4)
uO
t = BiGRUO

uO
t‚àí1,

eO
t ,charO
t

. (5)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.          
25:6 Z. Liu et al.
Given passage and question representation uP
t and uQ
t , sentence-pair embedding vP
t by soft alignment of each word in the passage and question is obtained by the following equation:
vP
t = GRU 
vP
t‚àí1,c
Q
t

, (6)
where c
Q
t = att(uQ ,[uP
t ,vP
t‚àí1]) is an attention pooling representation of uQ .
The discriminative model. The discriminator aims to maximize the log-likelihood of correctly
distinguishing generated relevant answers from the true ones. fœÜ (q,p, a) is used to discriminate
well-matched passage-question-answer pairs (p,q, a) from ill-matched pairs, in which the benefit
of matching given by fœÜ (q,p, a) mainly depends on the relevance of (p,q) to a. In other words, the
goal here is to distinguish between the relevant answers and non-relevant answers for question q.
Indeed, it is just a binary classifier, and we will set 1 to label the passage-question-answer tuples
that truly match. If not, set it as 0. Here, for the discriminative model, we optimize the optimal
parameters as follows:
œÜ‚àó = arg max
œÜ

N
n=1

Ed‚àºptrue (a |p,qn )[lo–¥(œÉ[fœÜ (p,qn, a)])]
+ Ed‚àºpŒ∏ (a |p,qn )[lo–¥(1 ‚àí œÉ[fœÜ (p,qn, a)])]

,
(7)
where if the function fœÜ is fully differentiable, it is typically solved by stochastic gradient descent
(SGD).
The generative model. The generative model pŒ∏ (a|p,q) aims to minimize the objective, which
tries to generate relevant answers, from the candidate answers for the given question and passage
(p,q). It aims to approximate the true relevance probability distribution over answers ptrue (a|p,q).
G returns the relevant answer a for q and p. D determines whether q,p, and a are correlated, which
can be considered as a classification problem. It can be seen that G involves feature extraction of
q,p, and a, and comparison of similarity between a and q and p, but generally it cannot use the
correlation data between a. D models joint probability distribution of q,p, and a, and is able to
learn from labeled data, but it is unable to extract features effectively from unlabeled data. Here
we optimize the optimal parameters for the generative model as follows:
Œ∏ ‚àó = arg minŒ∏

N
n=1

Ed‚àºptrue (a |p,qn )[lo–¥(œÉ (fœÜ (p,qn, a))]
+ Ed‚àºpŒ∏ (a |p,qn )[lo–¥(1 ‚àí œÉ (fœÜ (p,qn, a)))]

= arg minŒ∏

N
n=1

Ed‚àºpŒ∏ (a |p,qn )

lo–¥ 
1 ‚àí œÉ

fœÜ (p,qn, a)
 
= arg minŒ∏

N
n=1
‚éß‚é™‚é™
‚é®
‚é™‚é™
‚é©
Ed‚àºpŒ∏ (a |p,qn )
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
lo–¥ 


1 ‚àí exp 
fœÜ (p,qn, a)

1 + exp 
fœÜ (p,qn, a)




‚é§
‚é•
‚é•
‚é•
‚é•
‚é•
‚é¶
‚é´‚é™‚é™
‚é¨
‚é™‚é™
‚é≠
= arg minŒ∏

N
n=1
‚éß‚é™
‚é®
‚é™
‚é©
Ed‚àºpŒ∏ (a |p,qn )
‚é°
‚é¢
‚é¢
‚é¢
‚é¢
‚é£
lo–¥

1
1 + exp(fœÜ (p,qn, a))


‚é§
‚é•
‚é•
‚é•
‚é•
‚é¶
‚é´‚é™
‚é¨
‚é™
‚é≠
= arg max Œ∏

N
n=1

Ed‚àºpŒ∏ (a |p,qn )[lo–¥(1 + exp(fœÜ (p,qn, a)))]

,
(8)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.             
Unified GANS for Multiple-Choice Oriented MC 25:7
where the sampling a is a discrete set, so we cannot directly use the mini-batch SGD methods as
in the basic GAN formulation. Therefore, motivated by the common approach based on RL, we
apply the policy gradient method in RL [7, 11, 39] for handling the discrete data in the MC task.
The policy gradient method is as follows:
‚àáŒ∏ (Objective f unction o f the –¥enerator)
= ‚àáŒ∏ Ed‚àºpŒ∏ (a |p,qn )[lo–¥(1 + exp(fœÜ (p,qn, a)))]
= Ed‚àºpŒ∏ (a |p,qn )

‚àáŒ∏ lo–¥pŒ∏ (a|p,qn )lo–¥(1 + exp(fœÜ (p,qn, a)))
 ‚àáŒ∏ lo–¥pŒ∏ (a|p,qn )lo–¥ 
1 + exp 
fœÜ (p,qn, a)
 
 ‚àáŒ∏ lo–¥pŒ∏ (a|p,qn )(2 ‚àó si–¥moid(fœÜ (p,qn, a)) ‚àí 1),
(9)
where we adopt a sampling approximation to reduce the high variance of the policy gradient estimator in the last steps in which answers are sampled from the generator pŒ∏ (a|p,qn ). In accordance
with RL terminology, lo–¥(1 + exp(fœÜ (p,qn, a))) acts as the reward signal for policy pŒ∏ (a|p,qn ),
which takes an action a in the environment qn. The overall logic of the proposed McGAN model is
summarized in Algorithm 1. During the adversarial training, the discriminator and generator are
trained alternatively via Equation (9) and Equation (7).
ALGORITHM 1: The overall logic of the McGAN solution.
Require:
generator pŒ∏ (a|p,q);
discriminator fœÜ (xq );
training dataset S = {x};
Ensure:
pŒ∏ (a|p,q), fœÜ (p,q, a) are initialized random Gaussians with standard deviation 0.01 and mean
0;
Pre-train pŒ∏ (a|p,q), fœÜ (p,q, a) using S;
1: repeat
2: while generator-steps do
3: pŒ∏ (a|p,q) generates answer for each question q
4: Train generator by policy gradient Equation (7)
5: end while //end G
6: while discriminator-steps do
7: Use pŒ∏ (a|p,q) to generate the competitive negative examples then combine with given
positive examples
8: Train fœÜ (p,q, a) via Equation (9)
9: end while //end D
10: until McGAN converges //end Repeat
4 DATA
In this section, we discuss the datasets used in our experiments. We evaluate our McGAN model on
three large-scale multiple-choice MC datasets, including RACE1 [17], TQA2 [16], and SciQ3 [31].
1RACE is available at http://www.cs.cmu.edu/glai1/data/race/. 2TQA is available at http://textbookqa.org. 3SciQ is available at https://allenai.org/data/sciq/.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.  
25:8 Z. Liu et al.
Fig. 3. Distribution of all question categories in RACE.
4.1 RACE Dataset
RACE is the first large-scale multiple-choice MC dataset. The data observations are collected from
English examinations in China, which is designed for middle school students (12 to 15 years old)
and high school students (15 to 18 years old). Comparing with the two other multiple-choice
datasets, which we will introduce later, RACE is more challenging. In the RACE dataset, RACE-M
and RACE-H represent the sub-samples of middle school and high school difficulty levels, respectively. All questions contain four candidate options with only one correct option.
RACE contains 27,933 passages and 97,687 questions in total, 5% as the dev set and 5% as the test
set. We show an example from RACE-M in Appendix A (see Table A.1). The statistics for RACE-H
and RACE-M are reported in Appendix A (see Table A.2 and Table A.3). There is a wide variety of
question types in RACE datasets, which may result in increasing problem complexity. For example,
some questions ask for the best title for the passage. Such tasks require a strong language understanding capacity for comprehending the entire passage. In addition, there are quantity questions,
such as ‚ÄúHow many people were presented?‚Äù Some questions ask for the meaning in the context of
a passage, such as ‚ÄúWhat does the word ‚ÄòPOS‚Äô in last passage refer to?‚Äù In addition, there are standard Wh-type questions, such as ‚ÄúWhat color is the car?‚Äù and ‚ÄúWhere does Lucy intend to go this
summer?‚Äù Given the variety of questions, we summarize all questions in the test dataset into 13
categories. In Figure 3, we show the distribution of each of the categories in RACE-H and RACE-M.
4.2 TQA and SciQ Datasets
The other two datasets we use in this article are TQA and SciQ. Kembhavi et al. [16] initially
released TQA, which includes 1,076 lessons and 26,260 multiple-choice questions for middle
school science tests. SciQ [31] is a question answering dataset containing 13.7k multiple-choice
science examination questions. In SciQ, most instances come with the document used to formulate
the question.
5 EXPERIMENTS AND VALIDATION
In this section, we compare our McGAN with existing multiple-choice MC models and present
the results along with experimental details. We further conduct ablation studies to validate the
effectiveness of our proposed components.
5.1 Experimental Settings
The experimental settings of our proposed McGAN are as follows. All passages, questions, and
candidate answers are lowercased and stemmed. We pre-process each word using the library of
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
Unified GANS for Multiple-Choice Oriented MC 25:9
Table 1. Accuracy (%) Comparison on RACE Datasets
Model RACE-M RACE-H RACE
Human Performance [17] 95.4 94.2 94.5
Random [17] 24.6 25.0 24.9
Sliding Window [17] 37.3 30.4 32.2
GA Reader (100D) [17] 43.7 44.2 44.1
GA Reader (300D) [8, 17] 42.4 44.5 43.9
Stanford AR (100D) [4, 17] 44.2 43.0 43.3
Stanford AR (300D) [4, 17] 44.9 43.7 44.1
HAF Reader (100D) [42] 46.2 44.1 44.7
HAF Reader (300D)[42] 45.0 46.4 46.0
ElimiNet (100D) [22] - - 44.5
CSA Model (100D) [5] 51.0 47.3 48.4
MUSIC (100D) [37] 51.5 45.7 47.4
Ours: McGAN (100D) 56.7 53.7 55.2
Ours: McGAN (300D) 56.2 54.3 56.1
Note: Results on the RACE test set. The result that performs as the state of the
-art is depicted in bold.
 indicates the results that we obtained by running the published code [17] of
GA Reader [8, 17] and Stanford AR [4, 17]. All tests are with 300D pre-trained
GloVe word embeddings.
Stanford CoreNLP4 [20] and do word embedding based on GloVe5 with a size of (100, 300). For
out-of-vocabulary (OOV) tokens, we initialize the embeddings randomly. The GRU weights were
initialized based on a random Gaussian distribution with a standard deviation of 0.01 and a mean
of zero. We set all GRU hidden state sizes to 150. We use the SGD optimizer with a learning rate of
0.01 for RL training, and we set the batch size to 64 for optimization. A small dropout rate of 0.2
is applied to each layer. We implemented our models using TensorFlow6 [1], and our source code
and trained models are publicly released.7
5.2 Key Results
5.2.1 Overall Performance. We report the accuracy of our McGAN model and several baseline models on the test set. Our model outperforms previous best baselines, on both the RACE-M
and RACE-H subset, and yields state-of-the-art overall accuracy. Table 1 reports the overall performance of our method along with the benchmark approaches. All models were trained with
sub-samples randomly selected from the whole RACE dataset. The results show that our proposed
model obtains significant improvements on RACE. According to the published records of the accuracy rate on RACE, our McGAN achieves 5.2%, 7.9%, and 8.7% improvements on the accuracy in
comparison with state-of-the-art methods for RACE-M, RACE-H, and RACE, respectively.
For a robustness check, we test our method based on both 100D and 300D word embeddings.
The results show that McGAN outperforms baseline models consistently under the same word
embedding size. Moreover, our McGAN model with 100D word embeddings even outperforms
300D neural baselines in overall accuracy. Note that the results we report for McGAN are based
on 18 independent experiments and under different regularization processes, but they all lead to
4http://stanfordnlp.github.io/CoreNLP/coref.html. 5https://nlp.stanford.edu/projects/glove/. 6http://tensorflow.org/. Software is available from tensorflow.org. 7https://github.com/tifoit/McGAN.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.   
25:10 Z. Liu et al.
Table 2. Accuracy for
Multiple-Choice Questions on the
Validation Set of the TQA Dataset
Model Accuracy
GRUbl  [2] 34.6%
CNN2,3,4
 [2] 35.5%
McGAN (ours) 46.9%
Table 3. Accuracy of the
Models on the Validation Set
of the SciQ Dataset
Model Accuracy
GRUbl  [2] 68.2%
CNN2,3,4
 [2] 87.8%
McGAN (ours) 92.0%
better performance for the case of 100D. We believe that the superiority of the 100D version of
McGAN is due to the data characteristics after carefully analyzing RACE-M and RACE-H. RACEM (middle school) is much smaller than RACE-H (high school) and contains more short and simple
sentences in the questions. Based on our experience on other NLP tasks (e.g., semantic matching,
cloze-style MC), word embeddings with 50 to 150 dimensions usually lead to the optimal result.
When the dimension is too large (say above 150), there is no significant benefit with an increased
dimension. In some cases, a higher dimension may lead to worse results.
We also test our proposed McGAN framework on TQA [16] and SciQ [31] question answering
datasets. We compare our results with the reported accuracy in Kembhavi et al. [16], which is
considered as the best benchmark for both datasets. As can be seen in Table 2, for the TQA dataset,
our McGAN obtains a 46.9% accuracy rate, which is about an 11% improvement compared to GRU
and CNN models. In Table 3, we report the results of our McGAN along with GRU and CNN
based on the SciQ validation set. Our McGAN achieves 92.0% accuracy, which is more than a 4%
improvement compared to the benchmark.
To sum up, based on the experiments on RACE, TQA, and SciQ, our McGAN consistently behaves as the best method in handling the multiple-choice MC task among existing benchmarks.
In addition, our results show that McGAN significantly improves the accuracy compared with the
recurrent neural network (RNN)-based baselines and CNN-based approaches.
5.2.2 Categorical Analysis for RACE. Given the categorical information offered in RACE, we
analyze the categorical performance of our model and compare it with two baselines. Since the
code of the MUSIC [37] model has not been released, we compare McGAN with ElimiNet [22] and
GA Reader [17] as two very strong neural models leading to good performance.
The analysis is based on 13 question categories in RACE, RACE-H, and RACE-M datasets. As
summarized in Figure 4, on the full sample of RACE-full (the top panel), our McGAN outperforms
GA Reader and ElimiNet in 11 out of 13 categories. On Quantity and Meaning in context of passage
style questions, McGAN is 1.3% and 0.5% worse than ElimiNet. Similarly, on RACE-M (the middle panel), our McGAN is superior to other two baselines for 10 out of 13 categories. The three
categories in which we fail are Quantity, Meaning in context of passage, and What. On RACE-H
(the bottom panel), we observe that McGAN outperforms GA Reader and ElimiNet in 12 out of 13
categories.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.    
Unified GANS for Multiple-Choice Oriented MC 25:11
Fig. 4. Performance of McGAN and baseline models (ElimiNet [22], GA Reader [17]) on all question categories.
As can be seen, our model outperforms consistently in most of the categories in RACE, but it performs slightly worse than the baselines for Quantity and Meaning in context of passage questions.
Our explanations are as follows. For these two types of questions, the answer options are usually
short and simple sentences. Thus, the interactions among passages, questions, and answers may
not be fully exploited by our model. In other words, McGAN may not significantly improve at the
beginning of training process, and the generator may not be able to effectively select (or generate)
competitive negative examples. There is a semantic distance between the negative target samples
and the real one. A small such distance in the test set may result in a failure of the discriminator,
which aims to differentiate the negative target samples and the real one.
5.3 Ablation Studies
We conduct ablation studies to examine the impacts of important parameters and components of
our model on the overall performance.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
25:12 Z. Liu et al.
Fig. 5. Performance of our McGAN on TQA (the left panel) and RACE (the right panel) in the training process.
The dark red curve represents the normalized cosine similarity between the generated answers and the truth
answers.
5.3.1 Performance Ablation. We conduct the following two-step hyper-parameter tuning process. In the first step, a coarse-grained selection of hyper-parameters is conducted for locating the
range of optimal parameters. In the second step, a fine-grained selection process for parameters is
processed based on the identified range in the first step.
To study the sensitivity of our model given different parameters, we test our model with different batch sizes (16, 32, 64, 128, 256), learning rates (1e-4, 1e-3, 1e-2, 2e-4, 2e-3, 2-e2), dimensions
for word embeddings (64, 100, 128, 200, 300), and dropout rate (0.1, 0.2, 0.3, 0.5). Based on our
preliminary experiments, the dimension of word embeddings is the only parameter that has relatively high impact on the final performance. The 100D word embedding can achieve the best result
when handling RACE-M, whereas the 300D word embedding performs the best for RACE-H (see
Table 1).
Furthermore, to quantitatively analyze the performance improvement obtained from our McGAN, we demonstrate the semantic similarity between generated target answers and the true
ground truth in Figure 5 for the training process. We only show TQA (the left panel) and RACE
(the right panel) in the figure because TQA and SciQ are very similar datasets, and SciQ can be
considered as a simple version of TQA. To compare semantic similarity (coherence), we use the
normalized cosine similarity metric (i.e., cosine distance). A cosine similarity closer to 1 indicates
stronger similarity. As can be seen, at the beginning stage of the training, the McGAN model does
not improve significantly because the generative model G does not learn very well to generate the
answers. We believe that the sampled negative target answers have a big semantic distance to the
real target, especially for the discriminator trained based on random samples. Thus, the boundary
of the discriminative model D may be ineffective. Especially in the test set, the negative answers
are relatively close to the real target in the semantic space, and hence D can easily fail in making
good determinations. While the training process goes on, the generative model G would generate
target answers with an increasingly higher quality, and D would improve accordingly.
5.3.2 Model Comparison and Discussion for TQA. Kembhavi et al. [16] established three baselines, including a random model, the TextOnly model, and the BiDAF-based model [25]. The TextOnly model and the BiDAF-based model are the variant of RNN using the attention mechanism.
As plotted in Figure 6, the CNN2,3,4 model achieves about 3.6% improvement compared with the
BiDAF-based model. The results also confirm the superiority of our model over all benchmarks,
and we provide the following two reasons for this. On the one hand, the existing models (e.g.,
the RNN-based BiDAF model and CNN-based CNN2,3,4) rely heavily on a complex multi-layer
attention-based neural network, which lacks flexibility for specific problems. On the other hand,
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
Unified GANS for Multiple-Choice Oriented MC 25:13
Fig. 6. Comparison of accuracy on the TQA test set (multiple choice).
Fig. 7. The ablation experimental results on the TQA dataset.
our model simultaneously considers two types of models for the multiple-choice MC problem by
benefiting from the advantages of machine learning in capturing hidden patterns of the relationship between answers and passages.
To isolate the detailed training process of our model, in Figure 7 we separately demonstrate the
training curve of the generative model G and the discriminative model D along with two baselines:
BiDAF [16] and CNN2,3,4 [2]. The performance of D in McGAN shows better performance than
the baselines, whereas G maintains unfavorable performance. A reason could be the sparsity of the
distribution of the answer data. For example, each question usually has only one correct answer.
Such sparsity in data may result in a failure of G in getting positive feedback from D. Given that the
true distribution is unobserved, how G and D can be effectively merged is still an open problem.
5.3.3 Feature Representation Ablation Study. We conduct an ablation study on the feature representation of text tokens. The feature representation we consider includes two major types.
Fine-tune. We pre-process each word using the library of Stanford CoreNLP [20] and exploit the
popular pre-trained GloVe word embeddings. We keep most of the pre-trained word embeddings
and question words Ff ine-tune because the representations of key words such as which, what, and
when are important terms for MC systems.
Token features. We also add several handcrafted features for enhancing the embedding representations. The additional features include three components, which can be summarized as follows:
‚Ä¢ Named entity recognition and part-of-speech: We use the Stanford CoreNLP toolkit [20] to
tokenize the named entity recognition (NER) tagger and part-of-speech (POS) tagger. These
simple features are extremely helpful, as we will show in Table 4.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
25:14 Z. Liu et al.
Table 4. Features Ablation Analysis of the McGAN Model on the Dev Set of RACE
Model RACE-M RACE-H
Full model (100D embeddings) 56.7 53.7
w/o F EATU REf ine-tune 56.3 (‚Äì0.4) 53.4 (‚Äì0.3)
w/o F EATU REtoken-f eatures 55.6 (‚Äì1.1) 52.9 (‚Äì0.8)
w/o F EATU REf ine-tune and F EATU REtoken-f eatures 54.1 (‚Äì2.6) 51.2 (‚Äì2.5)
Model RACE-M RACE-H
Full model (300D embeddings) 56.2 54.3
w/o F EATU REf ine-tune 55.6 (‚Äì0.6) 53.7 (‚Äì0.6)
w/o F EATU REtoken-f eatures 54.7 (‚Äì1.5) 52.6 (‚Äì1.7)
w/o F EATU REf ine-tune and F EATU REtoken-f eatures 53.3 (‚Äì2.9) 51.2 (‚Äì3.1)
Note: The numbers indicate the accuracy (%) after we exclude each feature from the full model, and
thus the lower the number, the more important the feature. These tests are with 100D and 300D
pre-trained word embeddings, respectively.
Table 5. Comparison of Computing Efficiency
(Experiments on RACE)
Training Time Testing Time
GA Reader 19 hours 2.770 minutes
McGAN (ours) 15 hours 1.512 minutes
Speedup 1.20√ó 1.83√ó
‚Ä¢ Exact matching: We use a simple binary feature, indicating whether the passage token can
be exactly matched to one question or answers. Taking the text as an example, if the word
appears in the question or answers, we will set the feature to 1. If not, set it to 0. In this way,
we can also add the exact word matching feature to the question and answers.
‚Ä¢ Fuzzy matching: Similar to the ‚Äúexact matching‚Äù feature. However, we use a stemmer or
fuzzy partial matching to loosen the matching criteria. For example, we regard ‚Äúeffective‚Äù
and ‚Äúeffect‚Äù as fuzzy term matching, because the string ‚Äúeffective‚Äù is partially matched to
‚Äúeffect.‚Äù
As reported in Table 4, all features contribute to the performance of the McGAN model. Removing the feature representations would weaken the performance of our model, but our McGAN
model is still able to achieve the best performance compared to other baselines. Both features play
a similar but complementary role in the feature representation related to the paraphrased nature
of a question and the context for its answer.
5.3.4 Computing Efficiency. In this section, we show the feasibility of using our model in addressing the multiple-choice MC problem in terms of computational efficiency. Table 5 reports the
running time of our model along with GA Reader [17], which is a classic attention-based neural network model. The experiments are conducted on the same hardware we use in studies (an
NVIDIA Titan X GPU). We adopt the default settings in the original code in which the batch sizes
for training and testing are both 64.
As reported in Table 5, GA Reader takes 19 hours to complete the training for RACE, whereas
our McGAN only spends 15 hours, which is a 1.2√ó speedup. Based on the results, our McGAN can
complete the training process for RACE (25,137 passages and 87,866 questions) in 15 hours, which
means 2.14 seconds per passage or 0.61 second per question. After training, the testing or model
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
Unified GANS for Multiple-Choice Oriented MC 25:15
implementation is even more efficient. Table 5 shows that our model can finish processing the
test set of RACE (1,407 passages and 4,934 questions) in 1.512 minutes. Although computational
efficiency is not the main focus of our work, the results show that McGAN is exceptionally efficient
in handling the multiple-choice MC task.
6 RELATED WORK
We summarize the related work into three categories: (i) large-scale MC datasets, (ii) multiplechoice reading comprehension models, and (iii) attention mechanism.
6.1 Large-Scale Datasets
The recent progress in MC is largely due to the introduction of large-scale datasets. Large-scale
datasets have enabled MC researchers to make substantial advances. According to whether the
answers are restricted to an exact match span of the reference passage, we can classify existing
datasets into two categories. Who Did What (WDW) [21], CNN/Daily Mail8 [12], and Children‚Äô
s Book Test (CBT)9 [13] are automatically generated cloze-style large-scale datasets in which the
answer is a word (or a named entity) of the passage. The answer in SQuAD10 [23] is a continuous
span, labeled by a human to guarantee high quality, instead of a word. TQA [16] and SciQ [31]
are multiple-choice reading comprehension datasets. In TQA and SciQ, an instance consists of
a question and four answer options. Most instances come with the document used to formulate
the question. RACE [17] is a popular and large-scale dataset that fall into another category in
which the answers may not appear in the reference passage. This is closer to the setting in human
oriented reading comprehension. In addition, RACE is a multiple-choice reading comprehension
dataset in which the answer is one of four additional candidate options. RACE focuses primarily on
developing multiple-choice oriented MC models with near-human capability. Questions in RACE
come from real English examinations designed specifically to test human comprehension.
6.2 Multiple-Choice Reading Comprehension
Multiple-choice questions are common in language examinations. MCTest [24] is a smaller-scale
and multiple-choice reading comprehension dataset of high quality, and its difficulty is restricted
to 7-year-old children. It contains 500 stories and 2,000 questions by crowd-sourcing, in which the
answer is one of four additional candidate options. But the size is too small to efficiently train an
end-to-end neural network model. Thus, previous methods on MCTest are almost feature engineering models [6, 15, 40, 41]. These works heavily rely on lexical, syntactic, and frame semantic
features extracted by various NLP tools. These works achieved good performance on the sparse
data, although they lay heavy burden on humans.
RACE is in the same format as MCTest, but it has much larger size and higher complexity. In
the RACE dataset, all questions and candidate answers are generated by experts, intentionally
designed to test reading comprehension ability of 12- to 18-year-old students. This makes the
RACE dataset a relatively accurate indicator to reflect the text reading comprehension ability of
MC systems. Compared with previous MC datasets, the questions in RACE are substantially more
difficult in terms of the large portion of questions involving reasoning. In addition, it is sufficiently
large and suitable for training powerful deep neural networks. Unlike most existing MC datasets,
candidate answers in the RACE dataset are generated by humans, which may not be found in
the original text. In the meantime, it makes the task more challenging due to the wide variety
8CNN and Daily Mail datasets are available at http://cs.nyu.edu/%7ekcho/DMQA. 9CBTest datasets are available at http://www.thespermwhale.com/jaseweston/babi/. 10SQuAD dataset are available at https://rajpurkar.github.io/SQuAD-explorer/.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
25:16 Z. Liu et al.
of questions, such as Why, How, and Key ideas. Chen et al. [4] built a rule-based baseline with a
sliding window algorithm and adapted the Stanford AR [4] and GA Reader [8] to RACE as strong
neural baselines. The GA Reader [8] model was initially proposed and a state-of-the-art model for
cloze-style MC. The authors of the RACE dataset [17] adapted GA Reader [8] for multiple-choice
machine reading comprehension by replacing the output layer with a layer that computes the
bi-linear similarity between the candidate option representation and passage representation. Zhu
et al. [42] present a hierarchical attention model, which adequately leverages candidate options to
model the interactions among questions, passages, and candidate options. Different from all prior
approaches, we approach the problems by unifying generative and discriminative MC models. We
first propose an adversarial framework for the MC task.
6.3 Attention Mechanism in MC
The attention mechanism [4, 8, 9, 14, 15, 19, 25, 29, 30, 33] in MC models is mainly used to directly
model interactions and to predict the answer. Chen et al. [4] used a single question representation
to passage representation. Instead of representing the question with single representation, Xiong
et al. [34] and Gardner and Clark [9] utilized each word of the question to interact between passages. In Hu et al. [14], question words are aligned with the word in each timestep of the passage. In
Seo et al. [25], the attentions between passage and question are computed via bi-directional LSTMs
(Bi-LSTMs). Furthermore, Dhingra et al. [8] proposed gated attention to select the relevant part of
a passage with a single question representation via multi-hop, and Wang et al. [30] introduced a
gated self-attention for matching the passage and question to locate answer span.
7 CONCLUSION
We adapted the GAN to address the challenging multiple-choice reading comprehension task. Our
method, named McGAN, extends existing GAN-based models and achieves the best performance in
comparison with state-of-the-art techniques for the multiple-choice MC problem. The experiments
on three well-known datasets for multiple-choice MC confirm the superiority of our model over
all tested benchmarks. In summary, the advantages of our McGAN are twofold. First, the generative model is guided by the signals obtained from the discriminative model, leading to better performance than unsupervised learning methods and traditional maximum likelihood estimations.
Second, the discriminative model is enhanced so that the relevancy of each question-passage pair
can be effectively predicted. Although there is still a long distance between our achieved performance and the human capability, our work serves as the first successful attempt in machine-based
multiple-choice reading comprehension. Our model and results should be able to provide guidance
for potential future improvement on related problems.
A APPENDIX
RACE is a challenging and widely used multiple-choice oriented MC dataset. In the RACE dataset,
RACE-M and RACE-H correspond to middle school and high school difficulty levels, respectively.
All questions contain four candidate options with only one correct option. In Table A.1, we provide
an example of the multiple-choice reading comprehension dataset (RACE). It is an example from
the RACE-M test set. In RACE datasets, it contains 27,933 passages and 97,687 questions in total,
5% as the dev set and 5% as the test set. The statistics for the RACE-M dataset and RACE-H dataset
are reported in Table A.2 and Table A.3, respectively.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 3, Article 25. Publication date: March 2020.
Unified GANS for Multiple-Choice Oriented MC 25:17
Table A.1. An Example from the RACE Dataset
Passage:
Scientists say there are seven kinds of food that people should eat every day. They are:
(1) green and yellow vegetables of all kind; (2) citrus fruits and tomatoes; (3) potatoes and
other fruits and vegetables; (4) meat of all kinds, fish and eggs; (5) milk and foods made of
milk, like cheese and ice-cream; (6) bread or cereal, rice is also in this kind of foods; (7) butter,
or something like butter, with fat. People in different countries in the world eat different kinds
of things. They also eat in different kinds the day. In some place people eat once or twice a
day; in other countries, people eat three or four times a day. Scientists say that these
differences are not important. It doesn‚Äôt matter if a person eats dinner at 4 o‚Äôclock in the
afternoon or at eleven o‚Äôclock at night. The important thing is that every day a person must
eat something from each of the seven kind of food. ...
Question:
It is important that people should _ .
Candidates:
A. eat three times a day.
B. eat cooked food.
C. eat dinner at 4 o‚Äôclock.
D. eat something from each of the seven kinds of food every day.
Note: An example from RACE-M randomly sampled from the test set. We highlight the correct answer in brown color.
Table A.2. Data Statistics of the RACE-M,
RACE-H, and RACE Datasets
RACE-M RACE-H RACE
#w/p 249.9 374.9 342.9
#s/p 17.2 19.2 18.7
#w/q 10.1 11.4 11.0
#w/o 4.9 6.8 6.3
Note: #w/p and #s/p represent the average number
of words and sentences in the passage. #w/q and
#w/o are the average length of the question and
option. Training, development, and test sets share
similar statistics.
Table A.3. Statistics of the Training, Development, and Test Sets of the RACE-M,
RACE-H, and RACE Datasets
RACE-M RACE-H RACE
Train Dev Test Train Dev Test Train Dev Test All
# passages 6,409 368 362 18,728 1,021 1,045 25,137 1,389 1,407 27,933
# questions 25,421 1,436 1,436 62,445 3,451 3,498 87,866 4887 4,934 97,687
Vocab. size 32,811 125,120 136,629