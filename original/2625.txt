Multi-modal hashing methods could support efficient multimedia retrieval by combining multi-modal features for binary hash learning at the both offline training and online query stages. However, existing multimodal methods cannot binarize the queries, when only one or part of modalities are provided. In this article,
we propose a novel Flexible Multi-modal Hashing (FMH) method to address this problem. FMH learns multiple modality-specific hash codes and multi-modal collaborative hash codes simultaneously within a single
model. The hash codes are flexibly generated according to the newly coming queries, which provide any one
or combination of modality features. Besides, the hashing learning procedure is efficiently supervised by the
pair-wise semantic matrix to enhance the discriminative capability. It could successfully avoid the challenging symmetric semantic matrix factorization and O(n2) storage cost of semantic matrix. Finally, we design
a fast discrete optimization to learn hash codes directly with simple operations. Experiments validate the
superiority of the proposed approach.
CCS Concepts: • Information systems → Information systems applications; Multimedia information systems; Nearest-neighbor search;
Additional Key Words and Phrases: Multi-modal hashing, efficient discrete optimization
1 INTRODUCTION
Hashing-based methods [33] could transform the high-dimensional data in the original feature
space into binary codes in low-dimensional Hamming space. They can significantly improve retrieval efficiency and reduce storage costs. Currently, much attention has been paid to uni-modal
hashing [7, 26, 37, 41] and cross-modal hashing [10, 12, 21, 39], and less efforts have been made on
multi-modal hashing [13, 23, 30, 40, 43].
With the growth of multimedia data, the multimedia retrieval task has attracted wide attention.
In real-world multimedia retrieval tasks, both the query and database samples are usually described
by heterogeneous multi-modal features. Multiple modality features possess their own characteristics, and they can characterize the data samples from different aspects [6, 34]. To support efficient
multimedia retrieval, multi-modal hashing is studied by combining multi-modal features at both
offline hash learning and online query stages. It is essentially different from uni-modal and crossmodal hashing that only one of the modalities is provided at query (as shown in Figure 1). With
this research field, several multi-modal hashing methods [13, 14, 22, 23, 27, 28, 30, 36, 40] have
been proposed with impressive performance.
However, existing multi-modal hashing methods suffer from three important problems:
(1) Modality-missing problem. Existing multi-modal hashing methods require multi-modal features
as input at both offline training and online query stage. When only one or part of modalities is
provided in queries, they cannot work well anymore. (2) High computation and storage cost. Most
multi-modal hashing methods perform hash learning on the pre-constructed graph, which is a
n × n matrix (n is the number of training samples) to characterize relationship between data samples. The time cost for constructing such graph is O(n2) [16, 17]. Therefore, these methods will
generate great computation overhead, and thus cannot scale well for large-scale multimedia retrieval. (3) Relaxed optimization or inefficient discrete optimization. Essentially, hashing learning is
a discrete optimization problem. Nevertheless, most existing multi-modal hashing methods adopt
the two-step “relaxing+rounding” hash optimization strategy, which first relaxes the discrete constraints and then calculates binary codes by thresholding. This relaxed hash optimization strategy
could bring significant quantization errors and lead to suboptimal solutions [27]. Notice that existing discrete multi-modal hashing methods [27, 38] are mainly performed on the discrete cyclic
coordinate descent (DCC) [26]. It means that those methods must learn the hash codes bit by bit,
which is still time-consuming when dealing with large-scale data.
In this article, we propose a novel Flexible Multi-modal Hashing (FMH) method to deal with these
problems. FMH simultaneously learns multiple modality-specific hash codes to preserve the innermodality characteristics, and multi-modal collaborative hash codes to combine different modalities
and exploit their complementarity. Furthermore, binary hash codes are directly learned with the
supervision of discriminative pair-wise semantics and efficient (both computation and storage) discrete optimization. At the online query stage, the hash codes could be flexibly generated according
to the newly coming queries, which provide any one or part of modalities. The main contributions
of this article are:
• We propose a novel flexible multi-modal hashing model, which could simultaneously generate multiple modality-specific and multi-modal collaborative hash codes. It could adaptively
accommodate the newly coming queries that only provide any one or part of modalities. To
our best knowledge, there is no existing model with such advantages.
• We develop an efficient asymmetric collaborative supervised learning module to enhance
the discriminative capability of hash codes with pair-wise semantics, and meanwhile avoid
the challenging symmetric semantic matrix factorization and storage cost of semantic
graph.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
Flexible Multi-modal Hashing for Scalable Multimedia Retrieval 14:3
Fig. 1. Difference between uni-modal hashing, cross-modal hashing, and multi-modal hashing.
• A discrete hash code optimization method is proposed to directly solve the binary hash
codes and thus avoid relaxing quantization errors. Further, the hash codes are learned in a
fast mode with simple operation, achieving high efficiency (computation and storage) and
retrieval accuracy. Experimental results demonstrate the state-of-the-art performance of
the proposed method from various aspects.
The rest of this article is arranged as follows. Section 2 reviews the related work of existing
multi-modal hashing methods. The details of the proposed method are introduced in Section 3.
Section 4 presents the experiments. Finally, Section 5 concludes the article.
2 RELATED WORK
According to the explored features in hash learning, existing hashing methods are categorized into
three major families: uni-modal hashing [7, 37, 41], cross-modal hashing [10, 12, 21], and multimodal hashing [13, 23, 40].
2.1 Uni-modal Hashing
Uni-modal hashing [7, 19, 24, 26, 35, 37, 41] performs hash learning and retrieval for uni-modal
data, which is represented by one kind of features. Iterative Quantization (ITQ) [7] is a unsupervised uni-modal hashing method, which decreases the quantization error by learning an orthogonal rotation to project training data. However, it relies on pre-computed mappings such as
Principal Component Analysis (PCA) [44] or Canonical Correlation Analysis (CCA) [8]. Latent
Factor Hashing (LFH) [41] is a supervised hashing method that learns similarity-preserving binary codes with latent factor models. One of the challenges confronted by hashing methods is
that the discrete constraints of binary codes lead to NP-hard optimization problem. Most hashing
methods adopt the simple “relaxing+rounding” hash optimization strategy, which leads to significant quantization error. It is preferable that the binary hash codes can be directly learned without
any relaxation. Supervised Discrete Hashing (SDH) [26] is a supervised discrete hashing method
to learn hash codes for linear classification. Scalable Supervised Discrete Hashing (SSDH) [24]
can discretely and efficiently learn the hash codes by making full use of all training samples and
semantic information. Deep Supervised Discrete Hashing (DSDH) [19] is a supervised discrete
hashing algorithm based on deep-learning framework, where the pairwise label information and
the classification information are used to learn the hash codes.
These uni-modal hashing methods are particularly designed for uni-modal data. To support
multimedia search, multi-modal features should be first concatenated into a feature vector and
then imported into the uni-modal hashing model. Under such circumstance, the complementary
semantic association of different modal-specific features are ignored, the inter-modal redundancies
are involved, and thus sub-optimal performance may be obtained accordingly.
2.2 Cross-modal Hashing
Cross-modal hashing [5, 10, 12, 21, 31, 32, 39] aims at retrieving the most relevant objects represented by the other modalities for a given query instance represented by one modality. Collective
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
14:4 L. Zhu et al.
Matrix Factorization Hashing (CMFH) [5] learns the unified hash codes by collective matrix factorization from different modalities of one instance. Alternating Co-Quantization (ACQ) [12] iteratively generates hash codes for each modality by connecting with other modality. It simultaneously
reduces quantization errors and preserves the data similarity. Semantic Correlation Maximization
(SCM) [39] integrates semantic labels into the adopt hash learning to enhance the discriminative
capability of hash codes. Semantics-Preserving Hashing (SePH) [21] transforms the supervised semantic affinities into a probability distribution and approximates it with to-be-learnt hash codes
in Hamming space by minimizing the Kullback-Leibler divergence.
The main objective of cross-modal hashing is to discover the shared Hamming space between
different modalities so that the search process across the heterogeneous modalities can be achieved
accordingly. Similar to uni-modal hashing, only one kind of features is provided for search. It
cannot support the multimedia search when multi-modal features are all provided at the query
stage.
2.3 Multi-modal Hashing
Multi-modal hashing [13, 14, 22, 23, 27, 28, 30, 36, 40] fuses multi-modal features for hash learning. Most of the existing multi-modal hashing methods adopt unsupervised learning to learn hash
codes. A common approach is to construct graph to describe the relationship between data samples
in each modality, and subsequently perform hash learning on this basis. Composite Hashing with
Multiple Information Sources (CHMIS) [40] is one of the pioneers adopting such an approach. It
constructs a graph with local similarity preserving in each modality to preserve the similarity between samples in the learned hash codes. Multiple Feature Hashing (MFH) [30] not only preserves
the local structural information of each modality but also globally considers all the local structures
during the optimization. Multi-view Anchor Graph Hashing (MVAGH) [14] constructs multi-view
anchor graph to keep the averaged similarity in a low-rank form, where the eigenvectors can be
computed efficiently. Multi-view Alignment Hashing (MAH) [22] formulates a multi-graph regularized nonnegative matrix factorization framework, where hash codes are learned by uncovering
the hidden semantics and capturing the joint probability distribution of data. Multi-view Discrete
Hashing (MvDH) [27] performs spectral clustering to learn pseudo class labels and applies them to
generate discriminative hash codes. Multi-view Latent Hashing (MVLH) [28] learns binary codes
shared by multiple modalities from an unified kernel feature space. The aforementioned unsupervised multi-modal hashing methods are independent of semantic labels, thus they still suffer from
the limited discriminative capability.
Few supervised multi-modal hashing methods adopt semantic labels as supervision in the hash
learning process. Compact Kernel Hashing with Multiple Features (MFKH) [23] preserves the semantic similarity between different samples and learns hash code with optimal linearly-combined
multiple kernels. Discrete Multi-view Hashing (DMVH) [36] is a discrete supervised multi-modal
hashing method. It constructs similarity graph based on Locally Linear Embedding (LLE) [9, 25],
which can not only preserve local similarity structure but also keep the semantic similarity between data pairs.
However, all existing multi-modal hashing methods cannot work for query samples containing
only a single modality or part of modalities. Different from them, in this article, we propose a
novel Flexible Multi-modal Hashing (FMH), which directly learns discriminative discrete multiple
modality-specific hash codes and multi-modal hash codes simultaneously, to dynamically adapt
to the new sample, whether it contains only one single modality or any combination of multiple
modalities.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
Flexible Multi-modal Hashing for Scalable Multimedia Retrieval 14:5
Table 1. Important Notations Used in This Article
Notation Dimension Description
Xm dm × n the feature matrix of the mth modality data
S n × n the pair-wise semantic matrix
φm (Xm ) P × N nonlinear embedding of the mth modality data
Bm r × n the modality-specific hash code matrix of the mth modality data
[B1; ... ; BM ] (M × r) × n the multi-modal collaborative hash code matrix
Wm r × p the projection matrix of the mth modality data
D (M × r) × n the auxiliary variable of the multi-modal hash code matrix
L c × n the matrix to store the label information
μm weight of the mth view
dm feature dimension for the mth modality data
n number of training samples
M number of modalities
p number of anchor points
r hash code length
c number of semantic categories
3 THE PROPOSED METHODOLOGY
3.1 Notation
Throughout this article, we utilize bold uppercase letters to represent matrices and bold lowercase
letters to represent vectors. For any matrix A, Aij denotes the (i, j)-element of A and tr(A) is the
trace of A if A is square. AT denotes the transposed matrix of A. The Frobenius norm of a matrix
A ∈ Rm×n is defined as A2
F = m
i=1
n
j=1 A2
ij = tr(ATA). The important notations in this article
are summarized in Table 1.
Supports that O = {oi}
n
i=1 is the training dataset, which contains n training samples represented with M different modality features. The mth modality feature is {Xm = [xm1,..., xmn] ∈
Rdm×n }M
m=1, where dm is the dimensionality of the mth modality, M is the number of modalities.
Different modalities of one sample oi belong to the same category. S ∈ {−1, 1}
n×n is a pair-wise
semantic matrix, where the element in the ith row and jth column is defined as
sij =
 1, if xi and xj belong to the same category,
−1, vice versa.
Our method aims to learn multiple modality-specific hash codes Bm ∈ {−1, 1}
r×n and multimodal hash code [B1; ... ; BM ] ∈ {−1, 1}M×r×n to represent multimedia instances, where r is the
length of hash code. The basic framework of the proposed FMH is illustrated in Figure 2.
3.2 Nonlinear Projective Binary Mapping
The projected binary multi-modal hash codes should comprehensively preserve the multi-modal
feature information. Most existing multi-modal methods [14, 22, 30, 40] construct graph to accomplish this task. The graph construction process costs O(n2) computation and storage complexity,
which is practically unacceptable for large-scale multimedia retrieval. In this article, we propose
an efficient nonlinear projective binary mapping to reduce the complexity to O(n).
Given the mth modality feature Xm = [xm1,..., xmn] ∈ Rdm×n, we first obtain the nonlinearly transformed representation φ(xmi ) as [exp(
xmi−am1 2
F
2σ 2
m
),..., exp(
xmi−amp 2
F
2σ 2
m
)]
T where
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.          
14:6 L. Zhu et al.
Fig. 2. The basic framework of the proposed FMH. In the offline training process, FMH first performs nonlinear mapping on both CNN feature of image modality and BoW feature of text modality, and obtains
nonlinear embedding of each modality. Then, the corresponding modality-specific hash code is generated
with binary projection. Next, asymmetric collaborative supervised learning is performed on multi-modal collaborative hash code with pair-wise semantic supervision. FMH directly optimizes the discrete hash codes
with enhanced linear computation and storage efficiency. In the online hashing process, given a new multimodal query sample, after performing nonlinear mapping on the multiple features, the modality-specific
hash code and multi-modal collaborative hash code are generated for different retrieval tasks.
{am1,..., amp } are p anchors that are randomly selected from the training samples in the mth
modality, σm is the corresponding Gaussian kernel parameter. Under such circumstance, φ(Xm ) =
[φ(xm1),...,φ(xmn )] ∈ Rp×n preserves the modality-specific sample correlations by simply characterizing the correlations between the samples and anchors. The computation complexity of this
part is O(Mnp). With φ(Xm )|
M
m=1, we then compute the corresponding modality-specific hash code
Bm |
M
m=1 ∈ {−1, 1}
r×n with binary projection, which is formulated as
min
μm,Wm,Bm |
M
m=1

M
m=1
μm Bm − Wmφ(Xm )2
F + δ Wm 2
F,
s.t. Bm ∈ {−1, 1}
r×n,m = 1,..., M,

M
m=1
μm = 1, μm ≥ 0,
(1)
where Wm ∈ Rr×p is the projection matrix of themth modality, μm is the weight of themth modality and it measures the importance of modality feature, δ plays the trade-off between regularization terms. The idea behind Equation (1) is that discriminative feature will be assigned with larger
weights, and thus better projection can be achieved by minimizing the corresponding regularization loss.
3.3 Asymmetric Collaborative Supervised Learning
Different from uni-modal and cross-modal hashing, the complementarity of multi-modal data [1, 3,
42] is important for multi-modal hash learning. In this article, instead of learning a fused hash codes
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.    
Flexible Multi-modal Hashing for Scalable Multimedia Retrieval 14:7
as existing methods, we concatenate the modality-specific hash code Bm |
M
m=1 into a collaborative
representation of multi-modal data [B1; ... ; BM ]. This strategy not only effectively exploits the
complementarity of multi-modal data but also avoids damaging the structures of modality-specific
hash codes. It can thus support the flexible query hash generation (as shown below).
With the collaborative binary representation, we further leverage the pair-wise semantic S as
guidance to enhance the discriminative capability. This part can be formulated as
min
Bm |
M
m=1
rS − [B1; ... ; BM ]
T[B1; ... ; BM ]2
F,
s.t. Bm ∈ {−1, 1}
r×n,m = 1,..., M.
(2)
However, directly solving binary Bm |
M
m=1 in Equation (2) is very challenging due to the discrete
symmetric factorization, not to mention the fast hash optimization. Furthermore, storing the elements of S will consume O(n2), which is unacceptable in large-scale multimedia retrieval. In this
article, we develop an asymmetric supervised learning module that transfers semantics from pairwise semantic matrix S to hash codes while avoiding these problems. Specifically, we substitute
one of [B1; ... ; BM ] with an auxiliary variable D ∈ R(M×r)×n and keep their consistency during
the hashing learning process. The formula is
min
Bm |
M
m=1,D
rS − [B1; ... ; BM ]
TD2
F +γ [B1; ... ; BM ] − D2
F,
s.t. Bm ∈ {−1, 1}
r×n,m = 1,..., M.
(3)
In this formulation, the symmetric matrix factorization can be obviously avoided. Only one of
the decomposed variables is imposed with discrete constraint. The second regularization term
can guarantee the acceptable information loss. As shown below, with the support of asymmetric
hashing learning, the hash codes can be learned with a simple sgn(·) operation instead of bit-bybit discrete optimization as existing discrete multi-modal hashing methods. In addition, the O(n2)
storage cost brought by S can be reduced to O(n) when representing S with the label matrix (as
shown below).
3.4 Overall Objective Formulation
By integrating the above two parts into a unified learning framework, we derive the overall objective function of FMH as
min
μm,Wm,Bm |
M
m=1,D

M
m=1
(μm Bm − Wmφ(Xm )2
F + δ Wm 2
F )
+ β rS − [B1; ... ; BM ]
TD2
F +γ [B1; ... ; BM ] − D2
F,
s.t. Bm ∈ {−1, 1}
r×n,m = 1, 2,..., M,

M
m=1
μm = 1, μm ≥ 0,
(4)
where β, δ, and γ are balance parameters. The first two terms perform multiple modality-specific
hashing learning with nonlinear projective binary mapping, while the last two terms perform
asymmetric supervised multi-modal hashing learning.
3.5 Efficient Discrete Hash Optimization
Solving hash codes is actually a NP-hard problem due to the discrete constraint. It is always a
challenging problem from the birth of this technique. Most existing multi-modal hashing methods
[14, 22, 23, 30, 40] adopt two-step “relaxing+rounding” optimization strategy. They basically solve
the relaxed continuous solutions first and then calculate the binary hash codes by thresholding.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.              
14:8 L. Zhu et al.
However, this simplified strategy will lead to significant quantization loss. Although there exist
discrete multi-modal hash methods [27], they learn the hash codes bit-by-bit with DCC, which is
still time-consuming. In this article, with the support of objective formulation Equation (4), a discrete optimization strategy is proposed to directly solve the binary hash codes. Moreover, different
from existing multi-modal hashing methods, we avoid explicitly computing the similarity matrix
S, which can achieve linear computation and storage efficiency. In the following, we introduce the
optimization steps in details.
Step 1: Update µm |
M
m=1
. We denote hm = Bm − Wmφ(Xm )2
F + δ Wm 2
F. In some cases, μm =
1 corresponds to the minimum hm over all modalities, and μm = 0 otherwise, leading to only one
modality be selected but other modalities ignored. Thus, we have μm ← μt
m, t > 1. The reformulated objective function is minμm ≥0,
M
m=1 μm=1
M
m=1 μt
mhm. The μm can be calculated as
μm = (1/hm )
1/(t−1)
M
m=1 (1/hm )1/(t−1)
. (5)
Step 2: Update Wm |
M
m=1
. By calculating the derivative of Equation (4) with respect to Wm and
setting it to zero, the updating formula for Wm can be derived as
Wm = μmBmφT (Xm )(μmφ(Xm )φT (Xm ) + δIp )
−1
. (6)
Step 3: Update Bm |
M
m=1
. By fixing the other variables, the optimization formula for Bm can be
derived as
min
Bm ∈{−1,1}r×n μm Bm − Wmφ(Xm )2
F + β rS − BT
mDm 2
F +γ Bm − Dm 2
F, (7)
where Dm ∈ Rr×n is the part of D ∈ RMr×n corresponding to the mth modality. Equation (7) is
equivalent to
min
Bm ∈{−1,1}r×n Tr(−2μmBT
mWmφ(Xm ) + β (−2rBT
mDmST + BT
mDmDT
mBm ) − 2γBT
mDm ). (8)
We introduce auxiliary variable Zm ∈ {−1, 1}
r×n to substitute one of Bm in BT
mDmDT
mBm and keep
their consistency during hash code optimization. Equation (8) can be transformed as
min
Bm ∈{−1,1}r×n Tr(−2μmBT
mWmφ(Xm ) + β (−2rBT
mDmST + BT
mDmDT
mZm ) − 2γBT
mDm )
+ ρ
2





Bm − Zm +
Gm
ρ





2
F
.
(9)
Bm can be calculated as the following closed form:
Bm = sgn(2μmWmφ(Xm ) + β2rDmST − βDmDT
mZm + 2γDm + ρZm − Gm ). (10)
The hash codes Bm are directly solved with a single hash code-solving operation, which is faster
than iterative hash code-solving steps in DCC.
Note that the S ∈ Rn×n is included in the term DmST when updating Bm. If we compute S explicitly, then the computational complexity is O(n2). In this article, we utilize c × n matrix L˜ (c is
the number of semantic categories) to store the label information instead of directly calculating S,
and reduce the computational complexity to O(n). Let L˜ k i = lk i
li 2
, as the element at the kth row
and the ith column in the matrix L˜. Then, we can get the similarity matrix S˜ = L˜ TL˜. The semantic
similarity matrix S can be calculated as
S = 2S˜ − E = 2L˜ TL˜ − 1n1T
n, (11)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.                         
Flexible Multi-modal Hashing for Scalable Multimedia Retrieval 14:9
where 1n is an all-one column vector with length n, and E is a matrix with all elements as 1. Then,
we can get
DmST = 2DmLTL − Dm1n1T
n,
which consumes O(Mrcn).
Step 4: Update D. By calculating the derivative of Equation (4) with respect to D and setting it
to zero, the updating formula for D can be derived as
D = (β[B1; ... ; BM ][B1; ... ; BM ]
T +γ IMr )
−1 (r β[B1; ... ; BM ]S +γ [B1; ... ; BM ]), (12)
where S is also transformed using Equation (11), then we have
[B1; ... ; BM ]S = 2[B1; ... ; BM ]LTL − [B1; ... ; BM ]1n1T
n . (13)
The time complexity of computing [B1; ... ; BM ]S is reduced to O(Mrcn).
Step 5: Update Zm |
M
m=1
. Zm can be solved with the following closed form:
Zm = sgn(−βDmDT
mBm + ρBm + Gm ). (14)
Step 6: Update Gm |
M
m=1
. The update rule is
Gm = Gm − ρ(Bm − Zm ). (15)
The convergence criterion is that (Ot − Ot−1) < ξ, whereOt is the value of the objective function
in the tth iteration, convergence error ξ is usually set to 1e-4. Algorithm 1 summarizes the basic
procedure of the discrete hash code optimization.
ALGORITHM 1: Key steps of discrete hash code optimization.
Input: Training set {Xm ∈ Rdm×n }M
m=1, pair-wise semantic S ∈ Rn×n, hash code length r, the number of
anchor points p, parameters β, γ , δ, and ρ.
Output: Projection matrices Wm |
M
m=1 and modality weight μm |
M
m=1.
1: Initialize μm = 1
M ; Initialize Bm and Zm as {−1, 1}
r×n matrices; Initialize Gm = Bm − Zm; Initialize Dm
as r × n variable;
2: Randomly select the anchor point set in each modality.
3: Construct the nonlinearly transformed representation φ(Xm ).
4: repeat
5: for m = 1,..., M do
6: Update μm by Equation (5).
7: Update Wm by Equation (6).
8: Update Bm by Equation (10).
9: Update Zm and Gm by Equations (14) and (15), respectively.
10: end for
11: Update [B1; ... ; BM ]. Update D by Equation (12).
12: until Convergence
3.6 Online Hashing for New Queries
After the hash codes of the training set are learned, we next address the out-of-sample extension
of FMH. It is impractical to retrain FMH for obtaining hash codes of the unseen samples. Thus,
in online hashing stage, we aim to project the newly coming queries into binary hash codes for
retrieval.
When only one single modality is provided with Xquery
m , we can obtain the uni-modal hash code
Bquery
m with the learned projection matrix Wm as
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.    
14:10 L. Zhu et al.
Table 2. General Statistics of Three Datasets
Dataset Training Size Retrieval Size Query Size Categories Visual Modality Text Modality
MIR Flickr [11] 5,000 17,772 2,243 24 CNN (4,096-D) BoW (1,386-D)
MS COCO [20] 20,000 115,525 7,762 80 CNN (4,096-D) BoW (2,000-D)
NUS-WIDE [2] 21,000 193,749 2,085 21 CNN (4,096-D) BoW (1,000-D)
CNN feature is obtained by Caffe implementation of VGG Net.
Bquery
m = sgn(Wmφ(Xquery
m )). (16)
When any combination of multiple modalities is provided in the newly sample Xquery
m |
M
m=1,
we first obtain the multiple modality-specific hash codes for each modality according to Equation (16), and then get the multi-modal hash code with the combination of corresponding hash
codes [Bquery
1 ; ... ; Bquery
M ]. The multi-modal hash codes can be flexibly generated according to
the specific provided query modalities.
3.7 Discussions
3.7.1 Complexity Analysis. We give the complexity analysis of the proposed FMH in this subsection. We assume that n  p, n  iter, p  r, p  c, where iter is the number of iteration. The
time complexity of constructing the nonlinear feature mapping φ(Xm ) of each modality is O(np).
It takes O(prn) for updating μ(m)
. Updating W(m) costs O(p2
n). Updating Bm requires O(prn). Updating D costs O((Mr + c)Mrn). Updating Zm costs O(r 2
n). Then the computational complexity
of the whole optimization process is O(iter × prn), which scales linearly with n. At online multimedia retrieval process, it takes O(iter × prn) to generate multiple binary hash codes for a new
query.
Besides, we avoid explicitly computing the pair-wise similarity matrix S by substituting it with
the expression label matrix L˜. The complexity of the proposed method is reduced to O(n). Thus,
both the computational and the space complexity of FMH are linear with the size of the dataset.
Therefore, the proposed method is applicable for large-scale multimedia retrieval.
3.7.2 Convergence Analysis. The objective function in Equation (4) is minimized by iteratively
updating the variables, that is, updating one variable by fixing the others. In Algorithm 1, the
optimization of one variable in each step can lead to a lower or equal value of the objective function.
Therefore, each iteration process of the proposed optimization method will monotonously reduce
the value of the objective function. After several iterations, the optimization process finally reaches
a local minimum. Moreover, we verify the convergence of the proposed FMH on three benchmark
datasets in the experiments.
4 EXPERIMENTS
4.1 Evaluation Datasets
Our method can flexibly generate hash codes according to the provided query modalities. In experiments, we perform multimedia retrieval tasks on MIR Flickr [11], NUS-WIDE [2], and MS COCO
[20]. These datasets have been widely used for evaluating the multimedia retrieval performance
[4, 32]. Without loss of generality, in each dataset, we take visual feature from image modality and
corresponding textual feature from text modality to construct multi-modal data as existing works
[15, 18]. The statistics of three datasets used in experiments are summarized in Table 2, and their
detailed descriptions are as follows:
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
Flexible Multi-modal Hashing for Scalable Multimedia Retrieval 14:11
• MIR Flickr1 is composed of 25,000 images collected from Flickr website.2 Each image is
annotated with several user-assigned tags, which are from 24 provided unique labels. We
randomly select 100 samples from each category to construct the query set. After removing
the repeated images without textual tags or manually annotated labels, there are 20,015
image-text instances. Each image is represented as a 4,096-dimensional CNN visual feature
extracted by the Caffe implementation of VGG Net [29], while text is represented as a 1,386-
dimensional bag-of-words feature. Finally, we have 2,243 image-text instances as the query
set and the remaining instances are treated as the retrieval set. Within the retrieval set,
5,000 image-text instances are randomly selected for training.
• MS COCO3 contains more than 300,000 images for object detection, segmentation, and
captioning. We adopt MS COCO 2017 dataset, which contains 118,287 training images and
5,000 validation images. Each image is associated with at least one of 80 object categories.
In our experiments, each image is represented as a 4,096-dimensional CNN visual feature
extracted by the Caffe implementation of VGG Net and each text is represented as a 2,000-
dimensional bag-of-words feature. In our experiments, we select the 115,525 images as the
retrieval set and randomly select 20,000 images within them as the training set. Besides,
we randomly select 100 images for each class from the validation images to form the query
set. After pruning the images without any label or tag information, we obtain the query set
with 7,762 images.
• NUS-WIDE4 consists of 269,648 images with 81 ground-truth semantic concepts, which are
also downloaded from Flickr. Each image is represented as a 4,096-dimensional CNN visual
feature extracted by the Caffe implementation of VGG Net, and text is represented as a 1,000-
dimensional bag-of-words feature. The 21 most common concepts and the corresponding
193,749 images are selected for experiment. We randomly select 100 images for each class
and the corresponding 2,085 image-text instances to form the query set. The remaining
193,749 image-text instances are determined as the retrieval set, and 21,000 instances from
them are randomly selected for training.
4.2 Evaluation Metrics
Two standard evaluation metrics, Mean Average Precision (MAP) [5, 31] and topK-precision [27],
are adopted for retrieval performance evaluation. In the experiments, we adopt Hamming distance
to measure the similarity between the binary codes of the query instances and and that of the
retrieved instances in the multimedia database. Two instances are considered to be semantically
similar when they share at least one semantic tag. Given a query sample q, average precision (AP)
is defined as
AP (q) = 1
Lq

R
r=1
Pq (r)δq (r), (17)
where Lq is the number of the true neighbors in the retrieved list, Pq (r) denotes the precision of the
top r retrieved results, and δq (r) = 1 if the rth instance is the true neighbor and 0 otherwise. For
all the queries, we first calculate their APs and then use the average value as mAP. In this article, to
compute MAP, we consider the top 100 returned neighbors. For both evaluation protocols, larger
value indicates better performance.
1http://lear.inrialpes.fr/people/guillaumin/data.php. 2https://www.flickr.com/. 3http://cocodataset.org/. 4http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
14:12 L. Zhu et al.
4.3 Evaluation Baselines
In experiments, when only one of the image or text modality is provided in the queries, the retrieval problem degenerates to content-based image retrieval or text retrieval problem. Under such
circumstance, existing multi-modal hashing methods cannot work anymore, we compare the proposed method with four state-of-the-art uni-modal hashing methods, including Iterative Quantization (ITQ) [7], Latent Factor Hashing (LFH) [41], Supervised Discrete Hashing (SDH) [26], Scalable
Supervised Discrete Hashing (SSDH) [24], Deep Supervised Discrete Hashing (DSDH) [19].
When both image and text modalities are provided in the queries, we compare the proposed
method with the above mentioned uni-modal hashing methods, and six state-of-the-art multimodal hashing methods, including Multiple Feature Hashing (MFH) [30], Multiple Feature Kernel
Hashing (MFKH) [23], Multi-view Alignment Hashing (MAH) [22], Multi-view Latent Hashing
(MVLH) [28], Discrete Multi-view Hashing (DMVH) [36], Multi-view Discrete Hashing (MVDH)
[27]. Those methods have been discussed in Section 2.
5 For uni-modal hashing methods, we first
concatenate multiple features as a unified vector input and then import it into the uni-modal hashing models. For multi-modal hashing methods, we input multiple features from different modalities
into the multi-view hashing models. Source codes of ITQ, LFH, SDH, SSDH, MFH, and MFKH are
publicly available, and the other methods are implemented by ourselves.
4.4 Implementation Details
There are five parameters involved in the proposed FMH, including β, γ , and δ in Equation (4), ρ in
discrete hash code optimization, and the number of anchors p. In the experiments, the best performance is achieved when {β = 10−5,γ = 10−5, δ = 10−3, ρ = 10−5,p = 500}, {β = 10−5,γ = 10−5, δ =
10−3, ρ = 10−5,p = 500}, and {β = 105,γ = 105, δ = 10−1, ρ = 101,p = 1000} on MIR Flickr, NUSWIDE, and MS COCO, respectively. We carefully tune the parameters of all the baselines and finally
report their best results for performance comparison. On three datasets, we conduct five successive experiments with different randomly partitioned datasets and report the average results. All
our experiments are conducted on a workstation with a 3.40 GHz Intel(R) Core(TM) i7-6700 CPU
and 64 GB RAM.
4.5 Comparison Results
We record the MAP values of compared methods varying with different hash code lengths (16
bits, 32 bits, 64 bits, and 128 bits) on three datasets (MIR Flickr, NUS-WIDE, and MS COCO) in
Tables 3, 4, and 5, respectively. Please note that, 16 bits, 32 bits, 64 bits, and 128 bits refer to the
code length of multi-modal collaborative hash code. From these results, we can find that: (1) When
only one of the image or text modality is provided in query samples, existing multi-modal hashing
methods cannot work anymore. But our proposed FMH can still obtains better retrieval accuracy
than uni-modal hashing baselines. For example, when only image modality is provided, the average retrieval accuracy of FMH is 1.71%, 1.57%, and 1.11% higher than that of DSDH, the second-best
method, on MIR Flickr, NUS-WIDE, and MS COCO, respectively. These results validate that the
multiple modality-specific hash codes can effective alleviate the modality-missing problem in multimedia retrieval. (2) When both image and text modalities are provided in query samples, FMH
outperforms all the multi-modal hashing baselines and uni-modal hashing baselines with concatenated features. The average retrieval accuracy of FMH is 1.87%, 4.08%, and 1.64% higher than that
of DSDH, the second-best uni-modal hashing method, on MIR Flickr, NUS-WIDE, and MS COCO,
5We do not compare with the cross-modal hashing methods. Because only uni-modal feature is used as input in cross-modal
hashing, which is completely different from the application scenario of multi-modal hashing that multiple modality-specific
features should all provided as input.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
Flexible Multi-modal Hashing for Scalable Multimedia Retrieval 14:13
Table 3. MAP Comparison Results When Only Image Modality Is Provided
Methods
Image-only
MIR Flickr NUS-WIDE MS COCO
16 bits 32 bits 64 bits 128 bits 16 bits 32 bits 64 bits 128 bits 16 bits 32 bits 64 bits 128 bits
ITQ 0.7065 0.6944 0.6950 0.6998 0.5127 0.5126 0.5186 0.5257 0.3942 0.3954 0.3957 0.3964
LFH 0.6886 0.6942 0.7143 0.7308 0.5169 0.5339 0.5742 0.5700 0.3932 0.3941 0.3942 0.3959
SDH 0.7603 0.7765 0.7904 0.8209 0.6823 0.6634 0.7071 0.7234 0.3979 0.3935 0.4004 0.4007
SSDH 0.7578 0.7623 0.7329 0.7264 0.6289 0.6750 0.6384 0.6184 0.3941 0.3953 0.3973 0.3991
DSDH 0.7640 0.7832 0.7951 0.8173 0.6874 0.6997 0.7077 0.7141 0.3988 0.3929 0.3997 0.4025
FMH-map 0.6318 0.6419 0.6480 0.6667 0.6273 0.6614 0.6569 0.6797 0.3960 0.3932 0.3944 0.3928
FMH-supe 0.5979 0.6007 0.6126 0.6154 0.3640 0.3658 0.3699 0.3737 0.3917 0.3905 0.3911 0.3915
FMH-relax 0.7312 0.7486 0.7483 0.7427 0.6480 0.6537 0.6831 0.6942 0.3963 0.3994 0.3967 0.3978
FMH-dcc 0.7416 0.7483 0.7521 0.7586 0.6646 0.6799 0.6823 0.6845 0.3959 0.3981 0.3971 0.3976
Ours 0.7797 0.8002 0.8133 0.8347 0.7002 0.7068 0.7272 0.7375 0.4008 0.4091 0.4107 0.4186
Table 4. MAP Comparison Results When Only Text Modality Is Provided
Methods
Text-only
MIR Flickr NUS-WIDE MS COCO
16 bits 32 bits 64 bits 128 bits 16 bits 32 bits 64 bits 128 bits 16 bits 32 bits 64 bits 128 bits
ITQ 0.6454 0.6408 0.6309 0.6195 0.4474 0.4376 0.4309 0.4308 0.4511 0.4508 0.4570 0.4678
LFH 0.6608 0.6692 0.6839 0.6868 0.4494 0.4825 0.4723 0.4801 0.5509 0.5725 0.5967 0.6080
SDH 0.6371 0.6445 0.6514 0.6558 0.5841 0.5868 0.6189 0.6322 0.5913 0.6309 0.6446 0.6604
SSDH 0.6598 0.6321 0.6146 0.6143 0.5630 0.5762 0.5301 0.5123 0.5727 0.5554 0.5450 0.4918
DSDH 0.6577 0.6593 0.6642 0.6699 0.5874 0.5903 0.6200 0.6279 0.6107 0.6354 0.6448 0.6577
FMH-map 0.6574 0.6643 0.6818 0.6873 0.5463 0.5688 0.5997 0.6021 0.5562 0.5810 0.5825 0.6119
FMH-supe 0.5811 0.5822 0.5815 0.5827 0.3705 0.3713 0.3804 0.3862 0.4146 0.4222 0.4400 0.4548
FMH-relax 0.6433 0.6325 0.6366 0.6371 0.5169 0.5521 0.5687 0.5713 0.5692 0.6061 0.6281 0.6319
FMH-dcc 0.6248 0.6321 0.6375 0.6460 0.5229 0.5547 0.5613 0.5699 0.5672 0.5664 0.5936 0.6023
Ours 0.6622 0.6719 0.6898 0.6937 0.6371 0.6516 0.6532 0.6673 0.6297 0.6483 0.6581 0.6663
respectively. The average retrieval accuracy of FMH is 5.43%, 12.52%, and 5.60% higher than that of
DMVH, the second-best multi-modal hashing method, on MIR Flickr, NUS-WIDE, and MS COCO,
respectively. The superior performance is mainly because our proposed FMH exploits the complementarity of multi-modal features and the generated collaborative hash codes will have stronger
semantic representation capability.
Furthermore, Figure 3 shows the corresponding topK-precision curves with the increasing number of the retrieved samples on three datasets when the code length is fixed to 32 bits. These curves
demonstrate that no matter which modality is provided in query samples, FMH could achieve
higher or comparable retrieval precision with the number of retrieved samples increasing.
4.6 Effects of Nonlinear Projective Binary Mapping
In this article, FMH adopts an efficient nonlinear projective binary mapping to project multi-modal
features and learn binary hash code with high efficiency. To validate the effects of the nonlinear
projective binary mapping, we design a variant of the proposed method, named FMH-map. It simply imports the original features into the hash model. We conduct experiments on three datasets
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
14:14 L. Zhu et al.
Table 5. MAP Comparison Results When Both Image and Text Modalities Are Provided
Methods
Image+Text
MIR Flickr NUS-WIDE MS COCO
16 bits 32 bits 64 bits 128 bits 16 bits 32 bits 64 bits 128 bits 16 bits 32 bits 64 bits 128 bits
ITQ 0.7079 0.6987 0.6969 0.7004 0.5175 0.5116 0.5165 0.5250 0.3957 0.3953 0.3995 0.4002
LFH 0.6756 0.6774 0.7009 0.7121 0.5211 0.5697 0.5566 0.5354 0.4927 0.4547 0.4869 0.5078
SDH 0.7407 0.7559 0.7765 0.7822 0.6680 0.6752 0.7050 0.7283 0.5476 0.5647 0.5831 0.5842
SSDH 0.7565 0.7170 0.7160 0.7336 0.6467 0.6606 0.6432 0.6274 0.5642 0.5656 0.5438 0.5407
DSDH 0.7577 0.7806 0.7834 0.7901 0.6702 0.6779 0.6844 0.6980 0.5672 0.5673 0.5801 0.5847
MFH 0.5795 0.5824 0.5831 0.5836 0.3603 0.3611 0.3625 0.3629 0.3924 0.3927 0.3931 0.3935
MFKH 0.6369 0.6128 0.5985 0.5807 0.4768 0.4359 0.4342 0.3956 0.4211 0.4185 0.4021 0.3988
MAH 0.6488 0.6649 0.6990 0.7114 0.4633 0.3945 0.5381 0.5476 0.3870 0.3890 0.3899 0.4002
MVLH 0.6541 0.6421 0.6044 0.5982 0.4182 0.4092 0.3789 0.3897 0.4140 0.4020 0.4048 0.3945
MVDH 0.6828 0.7210 0.7344 0.7527 0.4947 0.5661 0.5789 0.6122 0.3889 0.3891 0.3894 0.3997
DMVH 0.7231 0.7326 0.7495 0.7641 0.5676 0.5883 0.6092 0.6279 0.5268 0.5233 0.5397 0.5512
FMH-map 0.6729 0.6789 0.6890 0.7113 0.6297 0.6718 0.6829 0.6952 0.5210 0.5585 0.5468 0.5650
FMH-supe 0.5953 0.5967 0.6028 0.6046 0.3691 0.3705 0.3791 0.3854 0.4038 0.4048 0.4154 0.4219
FMH-relax 0.7228 0.7369 0.7378 0.7335 0.6325 0.6547 0.6763 0.6804 0.5559 0.5692 0.5780 0.5897
FMH-dcc 0.7477 0.7512 0.7583 0.7639 0.6563 0.6755 0.6792 0.6844 0.5193 0.5326 0.5524 0.5578
Ours 0.7789 0.7891 0.8018 0.8166 0.7006 0.7102 0.7375 0.7453 0.5768 0.5868 0.5986 0.6025
with the code length varying from 32 bits, 64 bits, and 128 bits to compare the performance of
the proposed method and FMH-map. Tables 3, 4, and 5 show the comparison results on retrieval
accuracy. Table 7 shows the training time comparison of the proposed method and FMH-map on
three datasets when the code length is fixed to 128 bits. We can observe that the proposed method
obtains higher retrieval precision, and consumes less training time compared with FMH-map. The
superior performance of the proposed FMH is attributed to the nonlinearly projective binary mapping that can effectively characterize the sample correlations.
4.7 Effects of Asymmetric Collaborative Supervised Learning
In this article, FMH leverages the pair-wise semantic matrix as supervision to enhance the discriminative capability. To validate the effects of the pair-wise semantic supervision, we design
a variant of the proposed method, named FMH-supe. It learns hash code without the pair-wise
semantic supervision. We conduct experiments on three datasets with the code length varying
from 32 bits, 64 bits, and 128 bits to compare the performance of the proposed method and FMHsupe. Tables 3, 4, and 5 show the comparison results on retrieval accuracy. We can find from the
tables that the proposed method outperforms FMH-supe by a large margin. Because the supervision information is preserved in the pair-wise semantic matrix, it can significantly enhance the
discriminative capability of hash codes.
4.8 Effects of Efficient Discrete Hash Optimization
In this article, we propose an efficient discrete hash optimization to directly solve the binary hash
codes and thus avoid relaxing quantization errors. The hash codes are learned in a fast mode
with simple operation. To validate the effects of the discrete hash optimization, we develop two
variants of the proposed method: (1) FMH-relax first relaxes the discrete constraints to learn hash
codes and then obtains binary codes by mean-thresholding. (2) FMH-dcc directly solves hash code
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
Flexible Multi-modal Hashing for Scalable Multimedia Retrieval 14:15
Fig. 3. topK-precision curves on MIR Flickr, NUS-WIDE, and MS COCO when the code length is fixed to 32
bits.
bit by bit based on DCC [26]. We conduct experiments on all three datasets with the code length
varying from 32 bits, 64 bits, and 128 bits to compare the performance of the proposed method and
the variants. Tables 3, 4, and 5 show the comparison results on retrieval accuracy. Table 7 shows
the comparison training time on three datasets when the code length is fixed to 128 bits. We can
observe from the tables that, the retrieval accuracy of the proposed method is obviously higher
than that of the designed variants. Besides, compared with the baselines, the proposed method
consumes less or comparable training time. These results validate that our proposed discrete hash
optimization method efficiently reduces the quantization errors and achieves superior retrieval
performance.
4.9 Hash Code Length Analysis
In this article, we we simply set the same code length for different modalities when we learn multimodal collaborative hash code. In this subsection, we conduct the experiments when different hash
code lengths are set. Specifically, we fix the code length of multi-modal collaborative hash codes
to 128 bits and observe the performance variations by varying the hash code lengths of image and
text. Table 6 presents the main results. From this table, we can find that the multimedia retrieval
performance changes with the code length of image and text. This is because different modality
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
14:16 L. Zhu et al.
Table 6. MAP Comparison Results When Different Code
Length for Image and Text Are Set
Code Length
(image+text)
Image+Text
MIR Flickr NUS-WIDE MS COCO
8 bits + 120 bits 0.6762 0.6526 0.5588
16 bits + 112 bits 0.7238 0.6849 0.5636
32 bits + 96 bits 0.7513 0.7191 0.5820
64 bits + 64 bits 0.8166 0.7453 0.6025
96 bits + 32 bits 0.8186 0.7688 0.5441
112 bits + 16 bits 0.8248 0.7749 0.5026
120 bits + 8 bits 0.8276 0.7706 0.4958
features have different discriminative capability. Under such circumstance, discriminative modality should be assigned with more bits for better performance, and vice versa.
4.10 Efficiency Analysis
In this subsection, we conduct experiments to compare the training efficiency between FMH and
the baselines. We fix the code length to 128 bits and present the comparison training time when
any one modality or multiple modalities are provided in query samples in Table 7. The results
show that, when both image and text modalities are provided, the training efficiency of FMH is
significantly higher than that of multi-modal hashing baselines, and that of the uni-modal hashing
baselines with concatenated feature. When only one of these modalities is provided, the training
efficiency of FMH is better or comparable to uni-modal hashing baselines.
Furthermore, we evaluate the training time variations of FMH with the increase of training size,
and record the results in Figure 4. In the three retrieval tasks, only image modality is provided,
only text modality is provided, and both image and text modalities are provided; the training time
increases almost linearly with training size. This result validates that the linear computation complexity of FMH.
4.11 Parameter Analysis
In this subsection, we conduct parameter analysis on the involved parameters β, γ , δ, ρ, and
the number of anchors p. Similar to other works [27, 30], we determine the best parameters via grid search on MIR Flickr when the code length is fixed to 32 bits. Specifically, the
search range of p is {100, 200, 300, 400, 500, 600, 700, 800, 900, 1000}, that of β, γ , δ, and ρ is
{10−5, 10−3, 10−1, 10, 103, 105}. Holding the parameters γ and ρ at {γ = 105, ρ = 10−5}, we perform a grid search over β and δ within the range of {10−5, 10−3, 10−1, 10, 103, 105}. Then, we hold
{δ = 10−5, ρ = 10−5}, and sweep β andγ in the range of {10−5, 10−3, 10−1, 10, 103, 105}. Next, we hold
{δ = 105,γ = 10−5} and sweep β and ρ in the range of {10−5, 10−3, 10−1, 10, 103, 105}. Parameters δ
and γ are tuned via grid search in the range of {10−5, 10−3, 10−1, 10, 103, 105} when {β = 10, ρ =
10−5}. Parameters δ and ρ are tuned via grid search in the range of {10−5, 10−3, 10−1, 10, 103, 105}
when {β = 10,γ = 10−5}. Parameters γ and ρ are tuned via grid search in the range of
{10−5, 10−3, 10−1, 10, 103, 105} when {β = 10, δ = 10−5}. Finally, we hold {β = 10, δ = 10−5,γ =
105, ρ = 10−5} and sweep p in the range of {100, 200, 300, 400, 500, 600, 700, 800, 900, 1000}. Graphical experimental results are presented in Figure 5. These figures show that the performance is relatively stable when β is in the range of {10−5, 10−3, 10−1, 101}, δ is in the range
of {10−5, 10−3, 10−1, 101}, γ is in the range of {10−5, 10−3, 10−1, 101}, ρ is in the range of
{10−5, 10−3, 10−1, 101}, and p is in the range of {400, 500, 600, 700, 800}.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
Flexible Multi-modal Hashing for Scalable Multimedia Retrieval 14:17
Table 7. Comparison of Training Time (seconds) in Different Retrieval Tasks
When the Hash Code Length Is Fixed to 128 bits
Methods Image-only Text-only
MIR Flickr NUS-WIDE MS COCO MIR Flickr NUS-WIDE MS COCO
ITQ 10.21 125.42 46.43 2.88 18.09 16.17
LFH 21.33 159.01 87.52 3.64 19.42 27.52
SDH 14.44 170.11 131.08 13.74 105.19 119.83
SSDH 344.21 3674.80 3286.43 332.71 3543.77 3043.55
DSDH 947.32 8339.98 8179.32 925.37 8420.53 8311.89
FMH-map 30.03 259.28 344.60 29.79 258.18 350.79
FMH-relax 6.13 96.21 101.68 6.13 93.76 99.26
FMH-dcc 1107.24 9224.51 29647.55 1105.21 9216.08 29550.86
Ours 1.33 49.94 55.19 1.32 49.98 54.52
Methods Image+Text
MIR Flickr NUS-WIDE MS COCO
ITQ 18.13 139.92 89.11
LFH 39.37 283.03 175.61
SDH 16.01 226.26 157.98
SSDH 359.88 3714.09 3102.65
DSDH 927.15 8418.57 8233.44
MFH 20.56 689.56 702.30
MFKH 855.12 7472.85 7771.35
MAH 34.85 9163.59 9415.25
MVLH 15.46 107.78 113.18
MvDH 212.28 845.90 885.39
DMVH 891.43 9677.87 12287.87
FMH-map 30.81 260.00 351.89
FMH-relax 6.10 92.53 102.59
FMH-dcc 1109.39 9223.96 29679.56
Ours 1.34 78.61 81.12
Fig. 4. Variations of training time with data size when the code length is fixed to 32 bits.
4.12 Convergence Analysis
In this subsection, we conduct experiments on MIR Flickr, NUS-WIDE, and MS COCO with the
code length is fixed to 32 bits to show convergence of the proposed FMH. We record the objective
function value variations with the number of iterations. The convergency curves are shown in
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 14. Publication date: January 2020.
14:18 L. Zhu et al.
Fig. 5. Parameter analysis on MIR Flickr when the code length is fixed to 32 bits.
Fig. 6. Convergence analysis on three datasets when the code length is fixed to 32 bits.
Figure 6. We can observe from the figures that, the updating of variables monotonically decreases
the objective function value and eventually reaches a local minimum at each iteration.
5 CONCLUSION
In this article, we propose a novel supervised multi-modal hashing method, named Flexible Multimodal Hashing (FMH), to fuse multi-modal data and support efficient large-scale multimedia retrieval. FMH could adaptively generate hash codes according to the specific query types, and it
could deal with the modality-missing problem in multimedia retrieval. Furthermore, we develop
an efficient asymmetric collaborative supervised learning module to enhance the discriminative
capability of hash codes with pair-wise semantics, and meanwhile avoid the challenging symmetric semantic matrix factorization and storage cost of semantic graph. A discrete hash code
optimization method is proposed to directly solve the binary hash codes and thus avoid relaxing
quantization errors. Experiments on several public multimedia retrieval datasets demonstrate the
superiority of the proposed approach.