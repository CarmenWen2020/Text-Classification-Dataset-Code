As an e-commerce feature, the personalized recommendation is invariably highly-valued by both consumers
and merchants. The e-tourism has become one of the hottest industries with the adoption of recommendation
systems. Several lines of evidence have confirmed the travel-product recommendation is quite different from
traditional recommendations. Travel products are usually browsed and purchased relatively infrequently
compared with other traditional products (e.g., books and food), which gives rise to the extreme sparsity
of travel data. Meanwhile, the choice of a suitable travel product is affected by an army of factors such as
departure, destination, and financial and time budgets. To address these challenging problems, in this article,
we propose a Probabilistic Matrix Factorization with Multi-Auxiliary Information (PMF-MAI) model in the
context of the travel-product recommendation. In particular, PMF-MAI is able to fuse the probabilistic matrix
factorization on the user-item interaction matrix with the linear regression on a suite of features constructed
by the multiple auxiliary information. In order to fit the sparse data, PMF-MAI is built by a whole-data based
learning approach that utilizes unobserved data to increase the coupling between probabilistic matrix factorization and linear regression. Extensive experiments are conducted on a real-world dataset provided by
a large tourism e-commerce company. PMF-MAI shows an overwhelming superiority over all competitive
baselines on the recommendation performance. Also, the importance of features is examined to reveal the
crucial auxiliary information having a great impact on the adoption of travel products.
CCS Concepts: • Information systems → Association rules; Personalization; Electronic commerce;
• Computer systems organization → Distributed architectures;
Additional Key Words and Phrases: Travel product recommendation, probabilistic matrix factorization, linear
regression, multiple auxiliary information, recommender systems
1 INTRODUCTION
The tourism industry has experienced steady growth almost every year worldwide. Sensing these
huge business opportunities, more and more online travel agencies (OTA) keep popping up all
across the world, e.g., TripAdvisor, Expedia, Trip.com, and Tuniu. These online travel agencies
are able to provide services including transportation ticketing, packaged tours, accommodation
reservation, and corporate travel management, which are usually packaged as various travel products [30]. In this context, the usage of online information has become a major trend among travelers [21], along with online reservations for travel products becoming an important application.
The iResearch data1 shows that the online travel booking rate has now reached over 40% with an
OTA market growing to $7.69 billion in China. In response, many OTA platforms have adopted
recommender systems as a marketing communication tool so as to facilitate travelers learning
and purchasing their products [17]. Both OTA platforms and travelers benefit from such recommender systems. Prospective travelers can quickly locate the travel products satisfying their personalized requirements. Recommender systems also help OTA improve services, attract and retain
customers, and eventually increase conversions from browsers to buyers.
In the literature, plenty of studies on tourism-oriented recommendation have been devoted to
identify points of interest (POI) by regarding user’s attributes and constructing the personalized
itinerary for recommendation [15, 19, 25], through mining various types of data, e.g., GPS trajectories [47], check-in records [46], travelogues [18], and geo-tagged photos [25]. Most of the research
has considered “Where, When, Who” issues to model user mobility. In fact, this domain closely
relates to the fields of location-based social network services and urban computing [52]. Nevertheless, our study is highly-related to another research stream of literature that explores the intelligent
recommendation for travel products (sometimes termed as travel packages) [9, 30, 31, 41, 53]. These
studies have repeatedly verified that the recommendation of travel products is remarkably different from that of traditional items, e.g., movies, books, or groceries. Specifically, the user-item
interaction matrix in the context of tourism is extremely sparse, since the relatively expensive
travel-products lead to infrequent browsing and purchasing. This implies that the tourism recommendation needs to exploit other rich information for enhancing its performance. Furthermore,
compared with the recommendation of traditional items, the recommendation of travel products
is greatly affected by contextual factors such as the departure city, the landscapes of destination,
travel seasons, financial and time budget, and so on.
The auxiliary information used in recommender systems generally involves user-specific features, item-specific features, and global features [1, 2, 33, 35, 44, 49]. User/item-specific features
provide an additional description of users or items, e.g., item information and user demographics.
The global features are also known as dyadic features, which denote a similarity degree between
a user and a product [49]. To work around the conundrum of travel-product recommendation,
studies have attempted to incorporate partial auxiliary information to alleviate the data sparseness problem and thus to improve the recommendation accuracy. For example, Liu et al. introduced a tourist-area-season topic model to integrate the descriptive text of travel products with the
1http://www.iresearchchina.com/.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:3
collaborative filtering model [30, 31]. Ge et al. [9] developed the cost-aware latent factor model to
take both financial and time costs into account. He et al. [11] extended the topic model to incorporate the social influence besides the descriptive text of travel products. Liu et al. [28] proposed
a multiple factors model for estimating passengers’ future air travel pattern. The auxiliary information used in these studies are almost item-specific features. However, the global features act a
fat part in the tourism-oriented recommendation because many features make sense when they
link up users with products. For instance, the city that a consumer lives in has little effect on
modeling preferences, but it becomes meaningful when it relates to the departure city of a travel
product. Hence, the pressing concern for travel product recommendation is the ability for fusing
all-round knowledge. That is, the recommendation model should not be restricted to some specific
type of auxiliary information, whereas it calls for a flexible and universal framework that is able
to effectively incorporate all kinds of auxiliary information that is available from data.
In this article, we propose a Probabilistic Matrix Factorization with Multi-Auxiliary Information
(PMF-MAI) model for the travel-product recommendation. PMF-MAI is capable of fusing multiple
auxiliary information into matrix factorization and utilizing plenty of unobserved values to improve the recommendation accuracy. This article makes three key contributions to the literature
of recommender systems as well as travel recommendations:
(1) PMF-MAI is able to jointly model the multiple auxiliary information affecting the adoption
of travel products and user-product interaction matrix explicitly expressing users’ preferences. Although PMF-MAI is used for travel product recommendation throughout this
article, the model itself is truly universal to other recommendation or prediction problems
fed with both interaction matrix and auxiliary information.
(2) PMF-MAI adopts a whole-data based learning framework working on both observed and
unobserved samples. By sufficiently exploiting the unobserved data, PMF-MAI is conducive to alleviating the sparsity of user-product interaction data.
(3) PMF-MAI is evaluated on a real-life dataset obtained from a large tourism e-commerce
company in China. A set of dyadic features are constructed in the context of e-tourism.
Experimental results not only demonstrate that PMF-MAI outperforms several competitive baselines for the travel product recommendation, but also validates the effectiveness
of the proposed dyadic features.
Organization: The remainder of this article is organized as follows. In Section 2, we summarize
the related work in detail. In Section 3, we begin by describing the problem that we study in this
article, and then present our proposed recommendation framework PMF-MAI. Section 4 introduces
the real-life dataset and the construction of features by using the multi-auxiliary information. We
exhibit the experimental results in Section 5 and finally conclude this article in Section 6.
2 RELATED WORK
In this section, we survey the relevant literature in two streams of research: tourism-oriented
recommendations and matrix factorization (MF) based recommendation methods.
2.1 Tourism-Oriented Recommendations
Here, we discuss two substreams related to the study of tourism-oriented recommendations. The
first one relates to predicting next location (i.e., POI) by regarding interest preferences, and further
to generating itinerary as a sequence of locations under trip constraints (e.g., time limits, start and
end points). Since a sequence of POI visits is naturally interpreted as a session [36], the sessionbased recommendation methods are adopted for predicting the next POI [7, 48]. These methods
learn user transactional behavioral patterns (e.g., sequential patterns) and the user preference shift
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
22:4 L. Chen et al.
from one transaction to another for recommendations. In addition to the user-location data, plenty
of contextual information has been exploited to improve the recommendation accuracy. Cheng
et al. [3] proposed a Gaussian mixture model for taking social influence and geographical information into consideration. Similarly, Liu et al. [27] modeled the geographical influences and some
other factors by a general geographical probabilistic factor model. On the other hand, tour recommendation and itinerary planning depend on the combination of various factors such as POI
popularity and category, trip constraints, and interest preferences, which are usually approached
as an optimization problem [19, 25, 26, 46]. For instance, the variants of the traveling salesman and the orienteering problem are two widely-used optimization models in the field of tour
recommendations.
Although a wide array of studies fall within the aforementioned field, our work is highly-related
to the second substream: travel product or package recommendations. Much of the available literature on this research substream used a dataset provided by an offline travel company, consisting
of tens of thousands of expense records between users and travel packages [8, 9, 11, 30, 31, 41].
Compared with traditional products, a notable feature of the travel-package recommendation is
that the user-product interaction matrix is overly sparse and there exists amounts of auxiliary information that is potentially useful to an effective recommendation. Along this line, information
about area and season is extracted from the descriptive text of each travel package, and a hybrid
recommendation method that has the ability to combine many constraints is developed [30, 31].
Likewise, Ge et al. [8, 9] examined the effect of both financial and time cost on travel product purchases and presented two kinds of cost-aware recommendation models. He et al. [11] considered
the social influence of co-travelers to enhance the representation of travel interests. Recently, Liu
et al. [28] presented a topic model fusing multiple factors such as gender, age, and the customer
similarity graph to predict customer airline travel preferences. However, previous studies on this
regard focused on exploiting some specific type of factor to improve the recommendation quality,
and little work has considered designing a systematic and flexible framework to incorporate allround knowledge for travel-product recommendations, leaving an open field worthy of research.
Furthermore, our work is one of few studies to investigate the travel-product recommendation
problem on the real-life data sourced from an OTA platform, which is particularly important for
practitioners as the OTA platform has become common among online retailers.
2.2 MF-based Recommendation Methods
Matrix factorization (MF) aims to find two or more matrices such that their product can wellapproximate the original data matrix [34], and it has been successfully applied to handle a bank
of recommendation problems. Historically, many researchers have studied how to effectively
leverage auxiliary information (e.g., social relations among users [3, 6, 42] and geographical
information [24, 27, 51]) and incorporate them into MF-based recommendation models. From
a technical perspective, two principled methods are noteworthy. The first one is the so-called
matrix co-factorization [3, 27, 42, 51] that simultaneously decomposes two or three matrices with
the share of the latent factor matrices. Although this approach is very insightful, the extension
to incorporate multiple kinds of auxiliary information usually represented as multiple matrices is
largely neglected. The other one is the regression-based latent factor model, where multiple auxiliary information can be encoded as features and fed together with user-item interaction matrix by
using the linear or non-linear regression models [1, 6, 24, 33, 39]. Compared with co-factorization,
the regression-based latent factor method is more reasonable and flexible to fuse multiple auxiliary
information. Agarwal and Chen [1] assumed that the latent factor matrices are generated from the
side information via linear regression and their product should be approximated with the original
data matrix. Park et al. [33] proposed the Bayesian matrix factorization approach to alleviate
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:5
overfitting the problem of the traditional regression-based latent factor models. Chen et al. [2]
simultaneously incorporated features and past user-item interactions through a generalized linear
model and developed a machine learning toolkit for feature-based matrix factorization. However,
these above methods either ignore global features or only regard global features as the bias term.
Global features have a great influence on the tourism-oriented recommendation, which directly
affects users’ preferences for products. Different from the existing models, we fuse the global
features via linear regression and minimize the deviation between matrix factorization and linear
regression. Meanwhile, we treat the regressions of global features as the calibration of factorization on unobserved values. In fact, our PMF-MAI model provides a reasonable and effective way
to extend the regression-based MF model to learn on both observed and unobserved data.
Recent research has further improved MF-based recommendation models by focusing on the
usage of the missing data, i.e., unobserved values [4, 13, 24, 43]. For instance, Volkovs et al. [43]
presented the SVD block-factorization approach that enables SVD to handle the missing data.
Devooght et al. [4] offered an interesting approach where the unobserved ratings are modeled
as a prior estimate that is dealt with separately from the observed ratings. They showed that to
make MF-based models, learning on unobserved values is very critical to enhance the recommendation performance on the sparse data. Nevertheless, much of the research up to now regards the
unobserved values as the negative feedback, which is not always consistent with reality.
Despite previous works having made significant improvements on recommendation performances, the newly-proposed PMF-MAI model has its distinctive characteristics and advantages.
Firstly, PMF-MAI is a systematic and scalable approach that is capable of fusing multiple sources
of auxiliary information, which is superior to most current models that fail to fully fuse global
features. Secondly, PMF-MAI fully exploits unobserved values as the calibration of probabilistic
matrix factorization with linear regression, which is more suitable for handing highly sparse data.
3 THE PMF-MAI MODEL
In this section, we first outline the travel-product recommendation problem to be studied. Then,
we present the recommendation model named PMF-MAI, which provides an integrated framework
for fusing the probabilistic matrix factorization on user-item interaction matrix and the linear
regression on a set of features constructed by multi-auxiliary information.
Throughout the article, lowercase symbols (such as a, b) denote scalars, bold lowercase symbols (such as a, b) represent vectors, bold uppercase symbols (such as A, B) denote matrices, and
calligraphy symbols (such as A, B) represent tensors. For a better illustration, Table 1 lists all
mathematical notations used in this article.
3.1 Problem Statement
In the literature, the recommendation task is usually specified as: given an N × M matrix X describing the preferences of N users over M items, we aim to recommend each user with a set of new
items that this user might be interested in but has never been keen on before. The matrix X has
various definitions in different scenarios. For example, X can represent the browsing behavior or
consumption behavior on e-commerce sites [22], and can also denote the five-grade ratings in the
classic movie recommendation. The recommendation task is then equivalent to the prediction for
missing values of X, and thus the recommended items are generated by the ranking of predicted
values.
In many real applications, there is multiple auxiliary information producing effects on users’
interests. Taking the application to be addressed throughout this article as an example: the recommendation of travel products on an OTA platform. We can collect the frequency that a user
has clicked, i.e., browsed, a web page about one travel product, which is naturally placed into
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
22:6 L. Chen et al.
Table 1. Mathematical Notations
Notation Description
N number of instances, e.g., users, sessions
M number of items, i.e., travel products
K dimension of latent factors
D number of features constructed by auxiliary information
X user-product browse frequency matrix
H (H¯) indicator matrix of observed (unobserved) values in X
U (ui ) user latent factors (for ith user)
V (vj) product latent factors (for jth product)
Y feature tensor
yij (yijd ) (i, j) fiber of tensor Y (for dth feature)
β (βd ) regression coefficients vector (for dth feature)
the matrix X. This matrix only indicates users’ interests against the destinations or landscapes of
different travel products. However, besides the users’ historical interests, multiple other auxiliary
information is very important to determine whether a user prefers one travel product. For instance,
most users expect to travel from home, i.e., the starting place should be as near as the user’s city.
Furthermore, most tourists will not frequently browse some travel products of which the financial
costs they are unable to afford. In other words, both time and price of a travel product should be
in line with users’ estimates.
Based on the above analysis, the problem discussed in this article is how to fuse all-around
auxiliary information for enhancing the prediction of missing values inside the user-item matrix
X. In other words, we target at developing a novel recommendation model that can jointly combine
the interest expressed by X and various auxiliary information affecting the users’ interests. With
notations shown in Table 1, the problem discussed in this article is described as follows:
Definition 3.1 (Problem Statement). Given the partially observed preference matrix H  X and
the feature tensor Y, we target at estimating the unknown preference of every instance to each
product, such that we can make recommendations to the users.
3.2 Probabilistic Matrix Factorization on User-Item Matrix
Let X = [xij]N ×M be the matrix representing the interest of every user over all items, and U ∈
RK×N and V ∈ RK×M be two projection matrices on the latent space for users and items respectively, with column vectors ui and vj representing the K-dimensional user-specific and itemspecific latent vectors. We decompose X as the product of two matrices on the joint latent space
with dimension K  min(N, M) by using the probabilistic matrix factorization:
X = UV + E1, (1)
where E1 is an error matrix of which each element is often modeled as a Gaussian observation
noise [37], denoted as N (0, σ2
X1). Since the matrix X is usually very sparse, an indicator matrix
H ∈ RN ×M is adopted to indicate the observed values, of which each element hij = 1 indicates
the observed values; otherwise, hij = 0 for unobserved values. We then define the conditional
distribution with respect to all observed values of X as
P

X|U, V, σ2
X1

=

N
i=1

M
j=1

N
xij



u
i vj , σ2
X1
hi j
. (2)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.         
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:7
We also place zero-mean spherical Gaussian priors [37] on user and item latent vectors,
respectively:
P

U


σ2
U

=

N
i=1
N
ui |0, σ2
U I

, P

V


σ2
V

=

M
j=1
N
vj |0, σ2
V I

. (3)
With the Bayesian theorem, we have:
P

U, V|X, σ2
X1, σ2
U , σ2
V

∝P

X|U, V, σ2
X1

P

U


σ2
U

P

V


σ2
V

. (4)
By putting Equations (2) and (3) into Equation (4), we can obtain the log of the posterior distribution over user and item latent vectors with respect to the prior variance and observation noise
variance.
log P

U, V|X, σ2
X1, σ2
U , σ2
V

∝ − 1
2σ2
X1

N
i=1

M
j=1
hij
xij − u
i vj
2
− 1
2σ2
U

N
i=1
u
i ui − 1
2σ2
V

M
j=1
v
j vj . (5)
According to Equation (5), to compute the maximum a posteriori (MAP) estimation of U and V
is equivalent to minimizing the following objective function denoted as J1.
J1 = 1
σ2
X1
||H  (X − UV)||2
F +
1
σ2
U
||U||2
F +
1
σ2
V
||V||2
F , (6)
where || · ||2
F denotes the Frobenius norm and  is the Hardamard product.
Remark. The SVD-based models are the most widely used in recommender systems [20, 32].
They minimize the Frobenius norm between the preference matrix and the SVD approximation,
whereas PMF only optimizes the reconstruction error, which makes it far more flexible. For this
reason, we select PMF as the basis model, which is easily extended to incorporate multi-auxiliary
information and to support whole-data based learning on unobserved values.
3.3 Linear Regression for Multi-Auxiliary Information
The auxiliary information that affects the recommendation is data-specific. Nevertheless, we propose to model this auxiliary information as a set of pre-defined features associated with each
element xij ∈ X to be predicted. For instance, features constructed by auxiliary information associated with a paired user and travel-product, i.e., xij , may contain the distance between the user’s
departure city and landscapes included in this product, and the price utility of the user with respect to this product. The feature construction in the case of travel products recommendations
will be introduced in Section 4.2. Without loss of generality, for any xij ∈ X, we assume it is associated with a (D − 1)-dimensional feature vector yij = [yijd ](D−1)×1. Then, if we regard xij as the
response and yij as the control variables, the regression function is
xij = βyij + βD, (7)
where β = [βd ](D−1)×1 denotes the regression coefficients corresponding to every feature. For simplicity, if we set yijD = 1, Equation (7) can be written as xij = βyij = D
d=1 βdyijd .
We regard yij as the (i, j) fiber of tensor Y; then, the Equation (7) can be further rewritten as:
X = Y×dβ + E2,
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.                                  
22:8 L. Chen et al.
where ×d is the d-mode product between tensor Y and vector β. Again, we adopt a Gaussian
observation noise with variance σ2
X2 to model the error matrix E2. Similar to Equation (2), the
conditional distribution over observed values in X is
P

X|β, σ2
X2

=

N
i=1

M
j=1

N
xij |βyij , σ2
X2
hi j
. (8)
Likewise, we exploit zero-mean spherical Gaussian prior on the regression weight vector:
P

β


σ2
B1

=

D
d=1
N
βd |0, σ2
B1I

.
According to the Bayesian theorem, we have
P

β|X, σ2
X2, σ2
B1

∝ P

X|β, σ2
X2

P

β


σ2
B1

. (9)
The log of the posterior distribution in Equation (9) is given by
log P

β|X, σ2
X2, σ2
B1

∝ − 1
2σ2
X2

N
i=1

M
j=1
hij (xij − βyij)
2 − 1
2σ2
B1

D
d=1
β2
d . (10)
Therefore, the MAP estimation of β is equivalent to minimize the objective function J2 as
follows.
J2 = 1
σ2
X2
||H  (X − Y×dβ)||2
F +
1
σ2
B1
||β||2
F . (11)
Remark. Much of prior work [9, 24] only considered one type of feature constructed by auxiliary
information that was usually represented as a matrix. Thus, the matrix factorization approach
can be directly utilized to obtain the user/item latent vector on the feature matrix. In this article,
we scale the feature matrices to a tensor for incorporating richer auxiliary information. Hence,
we adopt the regression model to handle such a complex case, because the matrix factorization
is ineffective to multi-dimensional features. If only one feature is considered, i.e., D − 1 = 1, our
model is approximately reduced to many of models in previous studies [9, 24], except that the
linear regression instead of matrix factorization is utilized in our model.
3.4 Modeling Unobserved Values
Up to now, we have considered the observed value in X, constrained by the indicator variable Hij ,
in both the probabilistic matrix factorization model and the linear regression model. However,
lack of consideration for unobserved data is likely to increase the bias of the maximum likelihood
inference [4]. Therefore, we propose to use the auxiliary information to calibrate the PMF model
on unobserved values. To be specific, we attempt to minimize the bias between the PMF model
and the linear regression on tensor of features constructed by auxiliary information:
UV = Y×dβ + E3.
Along this line, the conditional distribution to indicate the Gaussian noise over unobserved
values is
P

U, V|β, σ2
B2

=

N
i=1

M
j=1

N
u
i vj |βyij , σ2
B2
h¯
i j
, (12)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.                             
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:9
Fig. 1. Graphical representation of the PMF-MAI model. Figure (a) shows the basic version of PMF-MAI;
Figure (b) shows the variant of PMF-MAI incorporating user/item-specific features.
where ¯
hij is the negation of hi,j , i.e., ¯
hij = 1 − hij . The log of the posterior probability can be
computed as follows:
log P

U, V|β, σ2
B2

∝ − 1
2σ2
B2

N
i=1

M
j=1
¯
hij
u
i vj − βyij2
. (13)
Thus, the MAP estimation is to minimize objective function J3 as:
J3 = 1
σ2
B2
||H¯  (Y×dβ − UV)||2
F . (14)
Remark. Since the user-item interaction matrices would be very sparse in practice, the matrix
factorization must be performed despite missing data (i.e., unobserved values), a problem which
has seen significant research attention. Most existing techniques [4, 13, 24] treated unobserved
values as negative examples and utilized the unknown rating, which was usually set as the worst
rating (e.g., 0), to guide the matrix factorization on missing data. However, we argue that the
unobserved values are not always appearing in products that users dislike. That is why we propose
to optimize the error between the output of PMF, i.e., UV, and the linear regression on features,
i.e., Y×dβ.
3.5 Integrated Model: PMF-MAI
In order to provide a more accurate and efficient model, we integrate the above-mentioned three
objective functions, i.e., J1, J2, and J3, to get the joint model PMF-MAI. Specifically, we utilize
the linear weighting method to convert the problem of multi-objective optimization into a monoobjective one. To better understand PMF-MAI, we display the graphical representation of PMFMAI in Figure 1(a). In the graphical representation, white and gray circles represent hidden and
seen circles, respectively. The directed edges between variables indicate dependencies between
the variables. Each plate represents a group of variables. In our model, the top plate, middle plate,
and bottom plate represent all the variables related to a specific product, user, and feature. And we
repeat the generation process by M, N, and D times, respectively. As we can see, based on whether
the data is observed or not, xij is divided into two categories. The unobserved xij is used as the
calibration of the regression model with matrix factorization. The objective function of PMF-MAI
is defined as
J = α1J1 + α2J2 + α3J3,
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.     
22:10 L. Chen et al.
ALGORITHM 1: PMF-MAI: The Unified Procedure
Require: Matrices X, H; Tensor Y; Dimension of latent factors K; Hyperparameters λX1, λU , λV , λB2, λX2,
λB1;
Ensure: Predicted matrix X;
1: procedure PMF-MAI(X, H, Y,K)
2: Initialize U, V and β with random numbers within the range (0, 1);
3: Perform random sampling on unobserved values of X;  optional step
4: while not converged do
5: Update U ← U − ξ ∂J
∂U with Equation (16);
6: Update V ← V − ξ ∂J
∂V with Equation (17);
7: Update βd ← βd − ξ ∂J
∂βd with Equation (18);
8: end while
9: return X = UV;
10: end procedure
where α1, α2, and α3 are weights for balancing three objective functions, and α1 + α2 + α3 = 1. By
putting Equations (6), (11), and (14) together, we have:
J = λX1
2 ||H  (X − UV)||2
F +
λX2
2 ||H  (X − Y×dβ)||2
F
+
λB2
2 ||H¯  (Y×dβ − UV)||2
F +
λU
2 ||U||2
F
+
λV
2 ||V||2
F +
λ2
B1
2 ||β||2
F , (15)
where λX1 = 2α1
σ2
X1
, λX2 = 2α2
σ2
X2
, λB2 = 2α3
σ2
B2
, λU = 2α1
σ2
U
, λV = 2α1
σ2
V
, λB1 = 2α2
σ2
B1
.
A local minimum of the objective function given by Equation (15) can be obtained by performing
gradient descent in U, V, and βd , respectively.
∂J
∂U = λX1[V(H  (VU − X))] + λB2[V(H¯   ((Y×dβ)
 − VU))] + λU U, (16)
∂J
∂V = λX1[U(H  (UV − X))] + λB2[U(H¯  (Y×dβ − UV))] + λV V, (17)
∂J
∂βd
= λX2

N
i=1

M
j=1

hij
y
ijβ − xij
yijd 
+ λB2

N
i=1

M
j=1

H¯ij
u
i vj − y
ijβ

yijd 
+ λB1βd . (18)
With Equations (16), (17), and (18), we can employ the gradient descent method to solve our
PMF-MAI model. For each iteration, we update U = U − ξ ∂J
∂U , V = V − ξ ∂J
∂V , and βd = βd − ξ ∂J
∂βd ,
where the step size ξ is set to 0.01 in our experiments. Finally, we obtain the predicted values by
X = UV. To better understand PMF-MAI, we summarize the computational procedure of PMFMAI in Algorithm 1.
Complexity: Since our PMF-MAI has taken all unobserved values into account, the cost of
updating U and V by Equations (16) and (17) is O((K + D)NM), and the cost of updating β by
Equation (18) isO(KDNM). Hence, the total computational complexity of PMF-MAI isO(KDNM)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.              
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:11
for each iteration. Intuitively, the number of observed values |X| is very small in comparison to
the number of unobserved values (NM − |X|). To reduce the computational cost of PMF-MAI, we
can perform the random sampling, a commonly-used technique in handling missing data [4, 43],
on unobserved instances before training gradient against variables (see line 3 of Algorithm 1). As
a result, if we denote γ ∈ [0, 1] as the sampling ratio, the total cost of PMF-MAI can be reduced to
O(KD(|X| +γ (NM − |X|))). We will show the effect of γ in the experimental section and demonstrate when γ = 0.3 PMF-MAI can achieve satisfactory performance.
3.6 Connections to Existing Models
Here, we show the connections between PMF-MAI and previous models taking user/item-specific
features into account, and the distinctions between PMF-MAI and previous models taking global
features into account. When given auxiliary information in the form of feature vectors related
with user and item, there exist a number of matrix factorization models making the user of the
user/item feature vectors to derive latent vectors. Mathematically, let F = [f1,..., fN ] and G =
[д1,...,дM ] denote feature matrices, where fi and дj are feature vectors related with user i and
item j, respectively. The first class of methods is to use the matrix co-factorization to compute the
latent vectors [40]:
F = AU + EF , X = UV + EX , G = BV + EG , (19)
where EF , EX , EG are the Gaussian noise. The second class is the regression-based latent factor
model (RLFM) [1]
U = AF + EU , V = BG + EV , X = UV. (20)
Many studies have adopted other generative models as the alternative of the linear regression
used by RLFM such as Bayesian matrix factorization with side information (BMFSI) [35] and hierarchical BMFSI [33]. Thus far, PMF-MAI does not consider user/item-specific features and its basic
version employs the probabilistic matrix factorization to obtain user/item latent vectors as shown
in Equation (1). Nevertheless, our PMF-MAI can readily be extended to incorporate user/itemspecific features by replacing the basic probabilistic matrix factorization with aforementioned
latent factor models. This upgrade is unlikely to alter the way that we handle global features.
Figure 1(b) shows the graphical representation of the extended PMF-MAI model incorporating
user/item-specific features.
Several studies [1, 2] have considered a fraction of global features such as day-of-week about
rating time, last purchase frequency associated with user and product, and so on. Since bits of
global features are unlikely to have a great impact to predicted values, existing methods such
as RLFM [1] and SVDFeature [2] have modeled the global features as the bias term of the score
function. In contrast to these methods, our PMF-MAI approach minimizes the loss between the
latent factor model and the linear regression of global features over both observed and unobserved
values, and it in fact amplifies the effect of global features. This treatment is potentially effective to
cope with the challenging problem raised from the tourism domain: the user-product preference
matrix is very sparse, and the contextual information is extremely vital to preference modeling.
4 DATA AND FEATURES
In this section, we first introduce a real-life dataset provided by an e-travel company in China and
then describe several features constructed by auxiliary information. The set of features is used to
constitute the feature vector yij as introduced in Section 3.3.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
22:12 L. Chen et al.
Fig. 2. Characteristics of Tuniu data. Figures (a) and (b) show the comparison of data sparseness among three
real-world datasets. Figure (c) shows the distribution of financial costs of products with the same time span.
4.1 Data Description
Our dataset is provided by Tuniu2, a large tourism e-commerce company in China. This dataset
is mainly made up of web server logs for recording a series of page views of users, where only
pages associated with travel products are maintained. Each page contains the following auxiliary information: Page_ID, Departure, Destination, Price, Time_Span, where Departure (Destination)
represents the starting (destination) city of a travel product, and Price (Time_Span) denotes the
financial cost (time cost) of this travel product.
A sequence of successive records of a user is divided into a number of sessions of which each
includes the sequential web pages clicked by a user during a certain period. To enrich the auxiliary
information of every session, we collect sessions that arrive at the website through advertising
campaigns, and internal and external search engines. For these sessions, we can obtain a Keyword
attribute that usually contains the intended destination, which will facilitate the construction of
several features. In addition, the IP address of each session is also analyzed to get the location of
this session denoted as IP_City.
We finally extracted 2,033 sessions over 15,491 pages from the logs from July to August 2013.
Then, a 2,033×15,491 matrix X is constructed, where each element xij denotes the count of when
the ith session has visited the jth page. The click count xij is in the range [1, 26] with the average
value 1.25. Moreover, the matrix X has only 50,533 non-zero elements, with a very low density
0.128%, which is much lower than those of traditional datasets used in recommendation. Figure 2(a)
and (b) compare the data sparseness between Tuniu dataset and two typical datasets used in the
recommendation area. One is the standard MovieLens-10K, and the other one is Tmall [54]. The
cumulative distribution function (CDF) is the probability that takes a value less or equal to the corresponding x-value. We can see from Figure 2(a) that over 90% of users in Tuniu data have clicked
less than 50 items, whereas roughly 40% of users of both MovieLens and Tmall have clicked and
rated less than 50 items. A similar observation can be seen from Figure 2(b): the number of users
that clicked each item is much more scarce in the Tuniu dataset. In addition, Figure 2(c) shows
the boxplots of a group of travel products with the same time span but different financial cost.
As can be seen, the product with longer time span tends to be sold at a higher price, but the price
fluctuation of products with the same time span is remarkably wild. This implies that both time
cost and financial cost should be considered simultaneously for the user preferences modeling.
4.2 Feature Construction
Here, we delineate the construction of features as a part of the input for our PMF-MAI model. As
described above, xij represents the relationship between a session and a page. So each feature yijd
2http://www.tuniu.com/.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:13
Fig. 3. A part of hierarchical structure for place names extracted from UNG.
associated with xij should also be constructed between the session and the page. In what follows,
we will construct two classes of features, including six features in total.
The first class of intuitive features attempts to characterize the distance of departure city and
destination city. Specifically, the locations of each session indicated by IP_City and the Departure
attribute of each page are used for computing the distance of departure cities. Meanwhile, the
intended destination of each session extracted from Keyword and the Destination attribute of each
page are responsible for computing the distance of destination cities. We employ two kinds of
methods to measure the distance between two place names: Geographical Similarity and Semantic
Similarity as follows:
(1) Geographical Similarity. Given a pair of place names, we use Google Maps API3 to compute the distance on the Earth’s surface, denoted as Dist(pi,pj) where pi and pj are two
place names. Then, we utilize the Min-Max normalization to transform the distance to a
geographical similarity:
S1 (pi,pj) = 1 − Dist(pi,pj) − min
max − min , (21)
where S1 (pi,pj) represents the geographical similarity and max (min) is the maximum
(minimum) value of Dist(pi,pj).
(2) Semantic Similarity. In fact, the place names can be organized as a hierarchical structure,
i.e., a tree. For instance, one possible path of such tree is: “China→East China→Jiangsu
Province→Nanjing.” Thus, we use the structural similarity upon the tree to define the semantic relationship between two place names. In detail, we utilize the hierarchical structure from United Nations Geoscheme (UNG)4. Figure 3 shows an illustrative example of
this hierarchical structure. Then, we define the semantic similarity S2 (pi,pj) as:
S2 (pi,pj) = 2Depth(pi ∩ pj)
Depth(pi ) + Depth(pj)
, (22)
where pi ∩ pj denotes the last common ancestor of pi and pj , and Depth(·) gives the depth
of a node in the tree, i.e., the number of nodes from the root to this node. For example, if a
tourist’s intentional destination is Thailand, according to Figure 3, S2 (Thailand, Phuket) =
0.889 and S2 (Thailand, Jeju) = 0.444. This means this tourist has a higher preference in
destination Phuket than Jeju.
3https://developers.google.com/maps/. 4https://en.wikipedia.org/wiki/United_Nations_geoscheme.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
22:14 L. Chen et al.
By using the above two similarity measures, we can construct four features to indicate the distance of both departure and destination cities.
The price and time of different travel products vary wildly. For example, travel products in our
dataset are sold for dozens of dollars to thousands of dollars, and take 1 day to more than 10 days.
Hence, another class of features is designed for characterizing the preference on the financial and
time cost of tourists. We adopt the method proposed by Ref. [9] to model the users’ preference to
financial and time cost as the Gaussian prior. Specifically, we first utilize the Min-Max method to
normalize the prices of all products, i.e., the Price attribute, and compute the average value and the
standard deviation of all pages contained in a session. Then, we can obtain the probability function
on price for every session by assuming the price follows 1-dimension Gaussian distribution. For
each xij , the utility on price is easily given by the probability function with the price of the jth
page. The same process is taken on the time cost by using the Time_Span of every page. As a result,
we construct two features to indicate both price and time preference.
5 EXPERIMENTAL RESULTS
In the following, we present our experimental setup and results. Specifically, we demonstrate:
(1) the performance comparisons between PMF-MAI and other benchmark methods; (2) the understanding of features in the context of e-tourism; and (3) an analysis of parameters inside our
PMF-MAI model.
5.1 Experimental Setup
All experiments were conducted on the real-world dataset as described in Section 4.1. For the
2,033×15,491 sparse matrix X, we divided each row into a training set and a test set by randomly
extracting a certain percentage of the elements to be part of the training set and the remaining
ones to be part of the test set.
5.1.1 Performance Metrics. Let xˆij and xij denote an estimated value and a true value, respectively, for a test instance. Two commonly used metrics indicating the estimated error are Mean
Absolute Error (MAE) and Root Mean Square Error (RMSE). Here, we define the average MAE and
RMSE as
MAE =

i,j |xij − xˆij |
nt
, RMSE =

i,j (xij − xˆij)
2
nt
, (23)
where nt is the number of test instances. Obviously, the small value of MAE (RMSE) indicates the
better performance.
Besides metrics for estimated errors, we further employ Recall, F-measure, and Normalized Discounted Cumulative Gain (NDCG) to characterize the ranking accuracy of recommendation results. Since these measures have been widely used in the literature of recommender systems [9,
30, 38, 46], we provide a very brief introduction of their calculations. In detail, the travel products
in the test data are regarded as the truly relevant items, denoted as Ti for the ith session (i.e., the
ith row of X). Then, the recommendation list generated by various recommendation methods is
denoted as Ri . Recall measures the ratio of the number of hits to the size of each session’s test data:
Recall = 1
N

i
|Ti ∩ Ri |
|Ti | . (24)
F-measure, an overall accuracy metric, is defined by the harmonic mean of precision.
F-measure = 2 · Recall · Precision
Recall + Precision , where Precision = 1
N

i
|Ti ∩ Ri |
|Ri | . (25)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:15
NDCG [38] is the normalized position-discounted precision score:
NDCG = 1
N

i
|Ri 
|
j=1
2I (Ri j ∈Ti ) − 1
log1+j
2
, (26)
where the indicator function I (·) = 1 if Rij ∈ Ti , otherwise for 0. We further adopt a diversity metric
Coverage =



N
i=1 Ri



M , (27)
and a higher coverage value indicates that the recommendation method can encompass a wider
range of interests. In total, six evaluation measures are used thereafter.
5.1.2 Algorithms Compared. We consider neighborhood-based approaches (User-based Collaborative Filtering (UCF) and Item-based Collaborative Filtering (ICF)), matrix factorization methods
without side information (PMF and Singular Value Decomposition (SVD)), a matrix factorization
method using an unobserved value (element-wise Alternating Least Squares (eALS)), a sequential
pattern-based recommendation method (Max Confidence Algorithm (MCA)) and matrix factorization methods with side information (RLFM, SVDFeature, and BMFSI). The comparison methods
are given below:
—UCF [16]. The standard UCF using Pearson correlation coefficient (PCC) as the similarity
measure is used here. The number of nearest neighbors is set to 50.
—ICF [12]. Similar to UCF, the ICF also utilizes PCC as the similarity measure, while the
number of nearest neighbors is set to 200 because the number of product pages is far larger
than that of sessions.
—PMF [29, 37, 45]. It is a standard latent factor model that is widely used in recommender
systems. This can be regarded as the basic version of our PMF-MAI that does not fuse any
features; that is, the estimation is performed purely based on the user-item matrix.
—SVD [50]. The Singular Value Decomposition (SVD) method is another famous recommendation model using the matrix factorization technique. PMF model is proposed by introducing Gaussian noise to observed value while SVD finds the matrix Rˆ = U VT of the given
rank, which minimizes the sum-squared distance to the target matrix R.
—eALS [14]. eALS is a matrix factorization method that weights unobserved data based on
the popularity of products. This method does not require any auxiliary information.
—MCA [36]. MCA is a sequential pattern-based recommendation approach. It first mines a
collection of sequential patterns and then recommends the remaining items after the occurrence of the prior items.
For methods with side information, we shall extract user/item-specific features and global features from Tuniu dataset. In particular, the features constructed in Section 4.2 are taken as global
features. We further add the membership of customers as one user-specific feature, and price, time
costs, as well as travel product types as three item-specific features.
—RLFM [1]. RLFM is a regression-based latent factor model for gaussian response. Here,
we fuse user-specific features, item-specific features, and global features into the matrix
factorization.
—SVDFeature [2]. SVDFeature is a model for feature-based collaborative filtering. Similar to
RLFM, we use the same user-specific features, item-specific features, and global features as
model inputs.
—BMFSI [35]. BMFSI is a Bayesian matrix factorization method that utilizes auxiliary information. However, this model excludes global features. So we only use user-specific features
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.      
22:16 L. Chen et al.
Table 2. Performance Comparisons (MAE and RMSE)
Method 50% 40% 30% 20% 10%
MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE
PMF-MAI 0.389 0.738 0.407 0.747 0.412 0.753 0.418 0.754 0.473 0.765
UCF 0.509 0.828 0.538 0.866 0.595 0.883 0.684 0.992 0.785 1.110
ICF 0.684 1.097 0.733 1.144 0.795 1.181 0.876 1.233 0.994 1.298
PMF 0.453 0.788 0.468 0.795 0.479 0.801 0.508 0.810 0.548 0.847
SVD 0.496 0.817 0.519 0.857 0.545 0.859 0.606 0.901 0.722 0.910
eALS 0.432 0.765 0.446 0.769 0.459 0.782 0.467 0.789 0.512 0.816
RLFM 0.422 0.750 0.433 0.753 0.449 0.770 0.457 0.776 0.509 0.806
SVDFeature 0.428 0.762 0.437 0.768 0.458 0.779 0.463 0.786 0.517 0.812
BMFSI 0.451 0.771 0.459 0.774 0.461 0.789 0.482 0.793 0.531 0.821
and item-specific features as raw features to feed into BMFSI. For a fair comparison, we further decompose global features into user (product) attributes to complement user-specific
(item-specific) features, e.g., user geolocation, user intended destination, and departure and
destination of travel product.
The dimension of latent factors, i.e., K, is set to 10 by default for PMF, SVD, eALS, RLFM,
SVDFeature, and BMFSI, and the proposed PMF-MAI. For our PMF-MAI, the sampling ratio γ
is set to 0.3, and the weights in Equation (15) are set equally by default, i.e., λX1 = λX2 = λB1 =
λU = λV = λB2 = 0.05. The impact of γ and the other six weights will be discussed in Section 5.4.
All experiments were done on the server with one quad-core E5-2650v2 processors and 128GB of
main memory. PMF-MAI, PMF, and MCA were implemented in Python by our team. We used the
Mahout Java machine learning framework5 to implement SVD, UCF, and ICF. For eALS, RLFM,
SVDFeature, and BMFSI, the source codes were available from Github.
5.2 The Overall Comparison
First of all, we present a performance comparison between PMF-MAI and baseline methods. To
this end, we randomly split the elements in matrix X into training data and test data, and decrease training set ratio gradually from 50% to 10%. For each ratio of training data, we repeat the
experiments 10 times on different random splits, and then report the average values of two performance metrics. The comparison results in terms of MAE and RMSE are shown in Table 2, where
MCA is not reported since it directly generates recommender items. In Table 2, the best results
are set to be bold. According to the results, there are several observations. First, our PMF-MAI
method consistently gives rise to the lowest estimated errors in all splits, followed in decreasing
order by RLFM, SVDFeature, eALS, BMFSI, PMF, SVD, UCF, and ICF. As the decrease in the number of training instances, PMF-MAI exhibits more superior performance. For example, PMF-MAI
improves 1.6% on RMSE over RLFM with 50% training data, but it achieves 5.1% improvement on
RMSE over RLFM with 10% training data. This clearly shows that PMF-MAI can better alleviate
the data sparsity problem compared with other methods. Second, PMF-MAI, RLFM, SVDFeature,
and BMFSI perform better than PMF and SVD, demonstrating the effectiveness of incorporating
auxiliary information. Furthermore, PMF-MAI, RLFM, and SVDFeature beat BMFSI. The major
difference among these methods is that BMFSI considers more user/item features, while other
methods utilize three kinds of features including global features, suggesting the benefit of jointing
global features. Third, eALS achieves better performance than PMF and SVD because the use of the
5https://mahout.apache.org/.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:17
Fig. 4. The overall comparison results with the latent dimension K.
unobserved values. Unobserved values largely alleviate the data sparsity in the matrix factorization model and make the prediction more accurate. Last but not least, both UCF and ICF perform
much worse than the models based on matrix factorization, which indicates that the neighborhood
methods cannot work effectively on the extremely sparse user-product interaction matrix.
Figure 4 shows the experimental results of various models with the varying parameter of the
latent dimension size K. First of all, we can observe that PMF-MAI works well among these baselines in all cases. An intuitive explanation is that PMF-MAI leverages both auxiliary information
and unobserved values to alleviate the data sparsity problem, which implies the excellence of utilizing auxiliary information and unobserved values in learning latent factor of spare user-product
interaction matrix for each user and item. Second, as K increases, PMF-MAI, RLFM, SVDFeature,
and BMFSI perform much more stable than PMF and SVD. This is largely owed to the auxiliary information integrated by these methods. Third, the performance improvement for all latent-based
models is from K = 5 to 10, and the prediction accuracy increases slowly and even decreases when
the latent dimension further increases. This implies that the default setting K = 10 is fair to our
method as well as its competitors and is reasonable for datasets with the varying ratio of the training set.
Figure 5 shows the comparison results among 10 methods in terms of Recall, F-measure,
NDCG, and Coverage, respectively. We set the training set size as 50% and repeat the experiments
10 times on random splits. Then, the average values of each evaluation measure are adopted as
the final results. We can observe several patterns from the results. First and foremost, our PMFMAI significantly outperforms benchmark approaches indicated by all of the evaluation measures.
The improvements of PMF-MAI achieved, on average, 13.8%, 13.1%, 2.7%, and 11.8% compared with
the second-best performer RLFM in Recall, F-measure, NDCG, and Coverage, respectively. Second,
PMF-MAI, RLFM, SVDFeature, and BMFSI outperform PMF and SVD in terms of Recall, F-measure,
NDCG, and Coverage, again verifying the superiority of the frameworks for jointing auxiliary information and matrix factorization. Third, we observe that, by fusing only user-specific and itemspecific features, BMFSI can only gain marginal improvements in terms of top-N recommendation
performances. Compared with PMF without auxiliary information, BMFSI can only gain around
1.2%, 4.8%, and 0.2% improvement in terms of top-20 Recall, F-measure, and NDCG, respectively.
We argue that user and product attributes are not fine-grained enough to discriminate user preferences. Quite differently, PMF-MAI, RLFM, and SVDFeature leverage the constructed global features
to better profile user interest preferences. Fourth, eALS is better than PMF and SVD. This is mainly
because eALS imposes an item popularity-aware weighting strategy on unobserved values while
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
22:18 L. Chen et al.
Fig. 5. Performance comparisons in terms of Recall, F-measure, NDCG, and Coverage. The training set size
is set to 50%.
PMF and SVD simply assign zeros to all missing values. Fifth, MCA dose not perform as well as
PMF, especially in terms of Coverage. Because MCA is frequency-based only, it is easy to filter out
infrequent but significant items or patterns. That is, the sequential pattern-based approach is suitable for the relation discovery between frequent items in simple datasets. However, it may easily
fail to model complex dependency in complex datasets for session-based recommendation. Sixth,
UCF performs worse than MF-based methods on MAE and RMSE, but UCF obtains better overall accuracy than PMF and SVD as shown by Figure 5(b). Closer inspection of Figure 5(d) shows
that the diversity of products within UCF’s recommendation list is quite low. This observation
suggests UCF tends to recommend popular products. Our PMF-MAI inherits the advantage of MFbased approaches that are able to recommend products having high diversity and yet improves
the recommendation accuracy. Finally, both UCF and ICF appeared to be unaffected by the length
of the recommendation list because every nearest neighbor has browsed few travel-products (i.e.,
the data is extremely sparse) and thus the recommended products are limited.
5.3 Evaluation of Features Constructed by Auxiliary Information
The most striking characteristic of PMF-MAI is its ability of fusing a set of features and the matrix
factorization. Six features in the case of the e-travel scenario have been defined and used by PMFMAI. We focus on evaluating these features, that is, to understand which features are more or
less important to the prediction performance. We design two strategies for the evaluation. First,
the standardized coefficients of the regression model can be used for evaluating the importance
of every feature. Second, we shall fall back on some impurity measure commonly used in the
classification model [5, 23] to evaluate the discriminative power of every feature. To this end, all
elements xij ∈ X are regarded as samples, and the integer value of xij is treated as the class label.
As mentioned in Section 4.1, xij ∈ [1, 26], and thus 26 classes are generated over all samples. Then,
we select Fisher score [5] as the impurity measure for the evaluation of features. For each feature,
the Fisher score is defined as
Fr =
26
c=1 nc (μi − μ)
2
26
c=1 ncσ2
c
, (28)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:19
Table 3. Effect of Implicit Features on Prediction Accuracy
Type Departure Destination Cost
Feature Geo. Sim. Sem. Sim. Geo. Sim. Sem. Sim. Price Time
Fisher Score 0.0015 0.0008 0.0057 0.0020 0.0077 0.0054
Ranking 5 6 2 4 1 3
Std. Coefficients 0.020∗∗ 0.008 0.029∗∗∗ 0.011∗ 0.047∗∗∗ 0.039∗∗∗
Ranking 5 6 3 4 1 2
Note: ∗p < 0.05; ∗∗p < 0.01; ∗∗∗p < 0.001.
where nc is the number of data samples in class c, μi is the average feature value in class c, σc is
the standard deviation of the feature value in class c, and μ is the average feature value of all data
samples. A higher Fisher score value indicates the feature has strong discriminative power.
Table 3 shows evaluation results of six features by using Fisher score and standardized coefficients. We can observe that the orders of features w.r.t. both metrics are almost consistent, except
the order between the geographical similarity of destination and the time cost. However, the values
of both metrics of the time cost correspond closely to those of the geographical similarity of destination. The top three features are price, geographical similarity of destination, and time, where the
p-value < 0.001 of their standardized coefficients indicates a very high significance level. Furthermore, Table 3 conveys the interesting information that tourists are more concerned with cost and
destination but relatively care less about departure, when they are choosing travel products. As
life quality rises, people increasingly prefer traveling individually and even by self-driving, rather
than the traditional package tour. So, tourists are more inclined to purchase travel products on the
destination, such as tickets, hotels, and the like. Nevertheless, both financial and time costs are still
the most important factors that tourists care about.
5.4 Effects of Parameters inside PMF-MAI
Here, we demonstrate the effects of parameters inside PMF-MAI, including the dimension of latent
vector K, the usage of random sampling on unobserved data, and six weights of loss terms and
regularization terms in Equation (15).
5.4.1 Sampling Ratio. After utilizing a random sampling technique, the computational complexity can be reduced to O(KD(|X| +γ (NM − |X|))), where γ ∈ [0, 1] is the sampling ratio. It is
a tradeoff between the performance and the runtime to select the appropriate γ . The runtime per
iteration is displayed in Figure 6(a). Here, we use the same 10-dimensional latent features and the
50% training set ratio. It can be seen that the runtime is linearly increasing as the sampling ratio increases from 0 to 1 with 0.1 as the interval. On the other hand, Figure 6(b)–6(f) depict the
recommendation performances. We observe that the results of RMSE, Precision, Recall, NDCG,
and Coverage exhibit similar patterns. As sampling rate γ increases, the recommendation performances increase quickly at first. But when γ increases further, the recommendation performances
increase slowly and even decrease. This phenomenon indicates that a very high sampling rate
cannot help to improve the recommendation performances. Considering the runtime and recommendation performances together, the appropriate γ is within [0.3,0.4]. This shows the default
setting γ = 0.3, highlighted by a gray dash line in each figure, is reasonable. Compared with results learned on all unobserved data (i.e., γ = 1), the runtime of each iteration is greatly reduced by
60% at the cost of increasing RMSE by 0.3%. Furthermore, it is noteworthy that the values at which
the index curves intersect the y-axis represent the results of PMF-MAI without fusing unobserved
values, i.e., γ = 0. We can observe that Recall, F-measure, NDCG, and Coverage of PMF-MAI without fusing unobserved values, on average, decrease by 20.8%, 20.7%, 6.6%, and 47.7%. The major
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
22:20 L. Chen et al.
Fig. 6. Effect of sampling ratio on PMF-MAI performance.
reason lies in extreme sparsity of user-product interaction matrix. Unlike other methods that assign zeros or weights to all unobserved values, PMF-MAI provides a new guide for incorporating
unobserved values into the matrix factorization model. The experimental results demonstrate the
effectiveness of the way to incorporate unobserved values.
Remark. Our PMF-MAI directly employs the batch gradient descent as the optimization algorithm that calculates the error over all samples. A limitation of this approach is that when the input
matrix X grows larger, it requires the entire data in memory and the iterative optimization may
become slower. Fortunately, there exist a number of variants of the gradient descent algorithm [10]
such as the Stochastic Gradient Descent (SGD), the mini-batch gradient descent, and the Alternating Least Square (ALS) techniques, which are developed for accelerating the optimization on big
data.
5.4.2 Weights of Loss Terms and Regularization Terms. When introducing the PMF-MAI model
above, we set the parameters λX1, λU , λV , λB2, λX2, λB1 to be equal, i.e., λX1 = λU = λV = λB2 =
λX2 = λB1 = 0.05. We here verify its effectiveness by using the so-called traversal method. In this
method, we alternately traverse the value of each parameter while keeping other parameters fixed.
Figure 7 exhibits the effects of six weights on the dataset of which the training set ratio is set
to 50%. As can be seen, the default settings for six weights marked by the gray dash lines have
achieved pretty good performance. As indicated by Figures 7(a), (c), and (e), λX1 = λV = λX2 = 0.05
is the best choice. In Figures 7(b) and (d), when setting λU and λB2 to be the default value, the
performance is just slightly worse than the optimal performance. As shown in Figure 7(f), PMFMAI has its best results for λB1 = 0.01, which has a little difference from the default setting.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 22. Publication date: January 2020.
Travel Recommendation via Fusing Multi-Auxiliary Information into Matrix Factorization 22:21
Fig. 7. Impact of six weights of loss terms and regularization terms.
6 CONCLUSION
To tackle the travel-product recommendation problem, we presented a novel framework called
PMF-MAI in this article. Different from existing methods, PMF-MAI was generated by jointly
considering user-item interaction matrix and multi-auxiliary information. Meanwhile, PMF-MAI
could be viewed as a whole-data based learning framework that utilized unobserved click volumes
as the calibration of probabilistic matrix factorization with linear regression. Experiments on a
real-world online travel dataset have demonstrated PMF-MAI outperforms competitive baselines
in terms of six evaluation measures, which is mainly attributed to fusing features constructed by
auxiliary information. Up to now, the simple random sampling technique is employed by PMFMAI to speed up its update on the large unobserved data. It will be important that future research
integrates the learning of ranked unobserved values as a part of the optimization objective in order to identify relevant unobserved instances for improving the performance. We also plan to seek
more complex factorization models that can potentially lead to better latent representations.