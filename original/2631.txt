Recommender Systems are tools designed to help users find relevant information from the myriad of content
available online. They work by actively suggesting items that are relevant to users according to their historical
preferences or observed actions. Among recommender systems, top-N recommenders work by suggesting a
ranking of N items that can be of interest to a user. Although a significant number of top-N recommenders
have been proposed in the literature, they often disagree in their returned rankings, offering an opportunity
for improving the final recommendation ranking by aggregating the outputs of different algorithms.
Rank aggregation was successfully used in a significant number of areas, but only a few rank aggregation
methods have been proposed in the recommender systems literature. Furthermore, there is a lack of studies
regarding rankings’ characteristics and their possible impacts on the improvements achieved through rank
aggregation. This work presents an extensive two-phase experimental analysis of rank aggregation in recommender systems. In the first phase, we investigate the characteristics of rankings recommended by 15
different top-N recommender algorithms regarding agreement and diversity. In the second phase, we look at
the results of 19 rank aggregation methods and identify different scenarios where they perform best or worst
according to the input rankings’ characteristics.
Our results show that supervised rank aggregation methods provide improvements in the results of the
recommended rankings in six out of seven datasets. These methods provide robustness even in the presence
of a big set of weak recommendation rankings. However, in cases where there was a set of non-diverse highquality input rankings, supervised and unsupervised algorithms produced similar results. In these cases, we
can avoid the cost of the former in favor of the latter.
CCS Concepts: • Information systems → Rank aggregation; Combination, fusion and federated
search; Recommender systems; Learning to rank;
Additional Key Words and Phrases: Rank aggregation, recommender systems, machine learning
1 INTRODUCTION
Recommender Systems (RS) are tools designed to help users find relevant information from the
myriad of content available online. They work by actively suggesting items that are relevant to
users according to their profile. The user profile is defined by looking at their explicit declared
preferences (e.g., the number of stars the user gives to an item in Amazon) or implicitly observed
actions (e.g., the number of times a user listened to music on a streaming service) [50].
RS are built upon recommendation algorithms. These algorithms are usually represented by
a function f (ui, xj) that learns the relevance of item xj to user ui , where xj ∈ I, ui ∈ U, and
I and U are, respectively, the set of items and the set of users in the system. The relevance of
an item xj can be defined according to: (i) the preferences that users similar to ui expressed to
xj (collaborative filtering approach) or (ii) using other items’ side-information, such as actors or
genre in the case of movies recommendation (content-based approach). Regardless of the approach
followed, recommendation algorithms can be used either to predict the rating a user ui will assign
to an item xj (rating prediction problem) or to identify a set of N items that will likely be of interest
to user ui (top-N recommendation problem). In the former, the function f (ui, xj) acts as a rating
function that predicts the rating given by user ui to item xj . In the latter, the function f (ui, xj) acts
as a ranking function that assigns a relevance score to item xj [50] and returns to user ui a ranking
of items sorted by their relative relevance score.
A great number of top-N recommendation functions, which are the focus of this article, have
been previously proposed in the literature [6, 27, 30, 43, 44]. An analysis of the results generated
by these functions shows they often disagree on the items that should appear at the top of the
recommended rankings (see Section 5). In cases where there is a relevant level of disagreement
between different ranking functions, instead of trying to find the best one to solve the problem
at hand, methods for rank aggregation (or rank fusion) can be good alternatives to combine the
rankings’ best features.
In recommender systems, rank aggregation methods are designed to account for the opinion of
different ranking functions to reach a consensus on which permutation of the ranked items (i.e.,
a new ranking composed by a subset of the items in different input rankings) should be recommended to a user. More formally, assume T = {τ 1, τ 2,..., τ k } as a set of k input rankings containing the top-N items returned by k different recommendation algorithms (ranking functions) to a
target user. The rank aggregation problem consists of combining the rankings in T to obtain a
“better” or “consensus” ranking τ ∗ according to a given evaluation metric.
Although rank aggregation has been successfully applied to different tasks, such as crowdsourcing [40], bioinformatics [31], and information retrieval and metasearch [17, 59, 61, 62], it is still
underexplored in RS. In this context, they can be useful methods to: (i) provide more accurate
item suggestions to users by taking into account the various biases of recommenders [9, 41, 57],
(ii) improve the diversity of the recommendation rankings [49], and (iii) reduce the impact of items
incorrectly placed at high rank positions by an individual recommender [7].
However, to the best of our knowledge, there is no systematic investigation on which scenarios
or rankings characteristics can affect rank aggregation results. This led us to important questions,
such as: How often will rank aggregation improve the results of the rankings being aggregated?
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:3
How are the results of these methods affected by scenarios where the input rankings are very
different from or very similar to each other? Which kind of rank aggregation algorithm is more
suitable for each scenario? Identifying these ranking characteristics and scenarios is not straightforward. In this direction, this work presents an extensive empirical study on using rank aggregation methods in RS, and is divided in two parts. We first investigate the characteristics of the top-N
rankings generated by 15 collaborative filtering algorithms in seven datasets, while accounting for
the datasets’ characteristics with a focus on sparsity. We then use 19 rank aggregation methods
to combine the previously generated rankings. As a result, we get further insights on the characteristics of the input rankings where it is worth exploring rank aggregation methods. The main
contributions of this article are:
• By characterizing the rankings generated by different collaborative filtering algorithms, we
show that: (i) The majority of the items shared by different rankings are irrelevant; (ii) As
expected, relevant items are concentrated in the first positions of the rankings, but there is
a considerable number of them at much lower positions (e.g., considering the top-100 items
recommended to the users in the dataset ML10M, on average, 60% of the relevant items are
placed below position 20);
• By analyzing the rankings generated by 15 recommendation algorithms for seven different datasets, we identified three main scenarios where rank aggregation methods differ.
These scenarios depend on the rankings overlap (which we also call ranking diversity) and
rankings overall quality. They are: (1) Very low ranking overlap and poor-quality input
rankings; (2) High ranking overlap and high-quality input rankings, and (3) Low ranking
overlap (concentrated in pairwise rankings relations) and high-quality input rankings.
• By analyzing the results of 19 rank aggregation methods for the seven datasets, we showed
how the identified scenarios impact rank aggregation methods. In short: (i) Rank aggregation achieves the biggest improvements in scenarios with high-quality input rankings and
high diversity (scenario 3), as expected; (ii) Unsupervised rank aggregation methods should
be avoided in scenarios with poor-quality input rankings (scenario 1); (iii) The use of supervised or unsupervised methods achieve similar results in scenarios with high-quality input
rankings but low diversity (scenario 2).
The remainder of this article is organized as follows. Section 2 defines the rank aggregation problem for the recommendation scenario. Section 3 brings an extensive but not exhaustive overview of
related work in rank aggregation. Section 4 describes the recommendation algorithms and datasets
that will be considered in the characterization of the rankings generated with respect to agreement
and diversity in Section 5. Following, Section 6 describes the rank aggregation methods considered
and the experimental setup. In Section 7, we discuss the results of rank aggregation obtained along
with a discussion of the results and their relation with the input rankings characteristics. Section 8
presents conclusions and future perspectives.
2 PROBLEM DEFINITION
This section defines the problem of rank aggregation in recommender systems and introduces the
basic notation that will be used throughout this article. We start with the concept of ranking. We
define a ranking τ = [xj ≥ xh ≥ ··· ≥ xz ] as an ordered list of items, where the items’ relevance is
given by their position, i.e., the item in the first position of τ is more relevant than the item in the
second position and so on. The term x is used to generically represent an item and specific items
are represented by a subscript, e.g., xj , xh, xz . We denote by τ (xj) the position (or rank) of item xj
under ranking τ . Two items xj and xh ∈ τ can be compared using their positions in τ . We say that
xj is “better ranked” than xh if τ (xj) < τ (xh ).
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
16:4 S. E. L. Oliveira et al.
Fig. 1. Example of input for a rank aggregation method, considering three users and five recommendation
algorithms. Each column in the user matrix represents a personalized recommendation ranking returned by
the corresponding recommendation algorithm.
The set of rankings T = {τ 1, τ 2 ..., τ k } is composed by rankings generated by the algorithms
S = {S1,S2,...,Sk }, where k denotes the number of algorithms and consequently the number
of rankings in the set T . There is a correspondence between the set of rankings and algorithms.
Therefore, the ranking τ r is generated by the algorithm Sr , where r ∈ {1, 2,..., k}.
Since we are dealing with rank aggregation for recommender systems, the algorithms in S
are top-N recommendation algorithms. These algorithms receive as input a rating matrix Mm×n.
Each row in M represents a user ui ∈ U, where U is the set of users (|U| = m), and each column
represents an item xj ∈ I, where I is the set of items in the dataset (|I| = n). In this matrix, each
value Mi,j corresponds to the rating given by user ui to item xj .
Each of these algorithms returns a ranking of size N for each user u ∈ U. Similarly to the notation used to represent the items, specific users are represented by subscripts. Therefore, the set of
k rankings returned by the algorithms in S for a specific user ui is denoted by Ti = {τ 1
i , τ 2
i ,..., τ k
i }.
Given a matrix M of users per items and a set of k recommendation algorithms S =
{S1,S2,...,Sk }, a recommendation rank aggregation input R is constructed by providing M
as input to each of the recommendation algorithms in S, and then combining their results. As
we are dealing with personalized recommendations, for each user ui ∈ U = {u1,u2,...,um }, each
algorithm Sr ∈ S returns a potentially different ranking τ r
i . Hence, the set of items recommended
to user ui is represented by Ii =
τ r
i ∈Ti τ r
i , i.e., the union of all items in the set of input rankings
Ti . Also note that when describing the rank aggregation task, the set I is restricted to the items
present in T , i.e., I =
τ r ∈T τ r .
Finally, the input R given to a ranking aggregation method corresponds to R = {T1, T2,..., Tm }.
Figure 1 shows an example of R, while Table 1 summarizes the basic notations used in the remainder of this article.
3 RELATED WORK
As previously explained, given a set of rankings T = {τ 1, τ 2 ..., τ k }, where the union of all items
contained in the rankings of T is the set I, the rank aggregation problem aims to reach a consensus
on which permutation of I should be considered as the consensus ranking τ ∗. Since we are dealing
with top-N recommendation, the consensus ranking τ ∗ will be composed only by a subset of items
I∗ ⊂ I.
The consensus ranking is expected to be better than the individual rankings, and the concept of
better can have different interpretations. One of the most common definitions of “better” is based
on the distance between the aggregated ranking and the individual rankings. Using this interpretation, a common approach followed by rank aggregation methods consists of using heuristics to
find an aggregated ranking that minimizes the Kendall tau distance from the input rankings [13,
14, 16, 45]. However, in the RS literature, as well as in information retrieval problems, one can
decide that one ranking is better than another based on well-known ranking evaluation metrics.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.  
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:5
Table 1. Basic Notations
Symbol Meaning
u, ui Generic user, specific user i
U The set of all users in the system
m The number of users |U|
x, xj Generic item, specific item j
I The set of all items in the system or the set of all items in the rankings to be aggregated
n The number of items |I|
Ii The set of all items recommended to user ui
S Set of k recommendation algorithms S = {S1,S2,...,Sk }
τ , T Generic ranking, set of generic rankings
τ r
i Ranking recommended to user ui by algorithm Sr , where r ∈ {1, 2,..., k}
Ti Set of rankings recommended to user ui by the algorithms in S
τ (xj) The position of item xj in ranking τ
τ ∗
i The aggregated ranking for user ui
I∗
i The set of items used to build τ ∗
i (I∗
i ⊂ Ii )
Ψ Aggregation function
R The union of all sets of rankings {T1, T2,... Tm } recommended to U
Commonly used metrics include precision, mean average precision, and normalized discounted
cumulative gain [50] (see Section 6 for definitions of these metrics).
Once one defines the metric (distance) to be used, formally, the rank aggregation problem consists in finding an aggregation function or procedure Ψ that receives a set of rankings T and
produces an aggregated ranking that optimizes a chosen metric (distance), i.e.:
Ψ : {τ 1
, τ 2
,..., τ k } → τ ∗
. (1)
Depending on the availability of ground truth data, rank aggregation methods can follow a supervised or unsupervised approach. Supervised methods search for the aggregated ranking that
optimizes a given metric computed over ground-truth data. Unsupervised methods, in contrast,
have no access to ground truth data and rely only on metrics that can be computed using, exclusively, the rankings in T (e.g., Kendall tau distance). Regardless of the availability of ground truth,
rank aggregation methods are usually divided into two main groups: score-based and permutationbased. The main differences between methods in these categories consists of the strategies that
each group employs to produce the aggregated ranking. Score-based methods construct the aggregated ranking by sorting the items based on a score value received from an aggregation function f . Permutation-based methods, in turn, search for an aggregated ranking directly in the space
induced by all the possible permutations of items in I.
One of the difficulties of using permutation-based methods in real-world applications is the size
of the search space (usually |I|!, which is the size of the set of all permutations of items in I).
However, score-based methods do not suffer from this problem and have been successfully used
in many contexts. For these reasons, we focus our analyses on score-based methods. The next
sections discuss unsupervised and supervised score-based methods and their application to the
recommendation scenario.
3.1 Unsupervised Rank Aggregation Methods
There are two main types of unsupervised score-based rank aggregation methods: positional and
majoritarian. Both methods attribute a score to each item in the set I. However, when calculating
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
16:6 S. E. L. Oliveira et al.
the value of this score, positional methods consider the absolute position of the item in the ranking,
while majoritarian methods perform pairwise comparisons between items.
Positional methods receive as input the set of rankings T and use an aggregation function
f : U×I→ R and a procedure to combine the item scores. Their simplicity makes them easy to
use in any rank aggregation scenario. These methods include Borda Count [1, 10], median rank
aggregation (MRA) [16], and the Comb* family [18]. Since these methods are considered in our
experiments, they are described in detail in Section 6.1.
In contrast with positional methods, majoritarian score-based methods compute the score of an
item by performing pairwise comparisons to other items in the ranking. For instance, the Outrank
[17] algorithm performs contests between pairs of items and uses the number of wins and losses
in these contests to compute the items’ scores. It represents the first of the majoritarian methods
in our experimental analysis, and is also described in detail in Section 6.1.
Among the majoritarian methods are also those based on Markov Chain (MC) [14, 34], where the
union of items from all rankings forms the state space. In RS, each state sj in the MC corresponds
to an item xj and the transition matrix M—used to control the transitions between the states of the
MC—depends on all the input rankings. A transition matrix is then constructed in a way that its
stationary distribution will have larger probabilities for states that are ranked higher. Therefore,
the consensus ranking is determined by the stationary probability of each state. MC methods for
rank aggregation differ on how they build the matrix M, as detailed in Section 6.1. These methods
are considered majoritarian, because they use pairwise comparisons to build the state transition
matrix.
Other methods based on item pairwise comparison include those proposed in References [12,
20, 29, 58, 60]. The authors in Reference [20] handled the rank aggregation problem as a matrix
completion one. Their method first builds an item pairwise preference matrix Yn×n. Since Y is
incomplete, the second step of their method uses a Singular Value Decomposition (SVD)-based
approach to build a complete matrix that approximates Y. The complete matrix is then used to
compute scores to the items, which are used to create the aggregated ranking.
The authors in Reference [12] proposed a method that builds directed graphs of preferences
between the items in the input rankings. Their method creates one graph for each input ranking
and then merges these individual graphs. The in-degree of each item in the aggregated graph is
used as a score to sort the items. Finally, Reference [58] proposed a genetic algorithm that searches
for the combination of input rankings generated by majoritarian ranking aggregation methods
while optimizing an effectiveness estimation measure. Although they used majoritarian methods,
they that highlight any other type of method could have been considered.
Regardless of the type of the unsupervised rank aggregation method proposed, current methods
have also explored additional items information to improve the performance of basic unsupervised
rank aggregation methods. In Reference [33], for example, the authors first aggregate the input
rankings using CombSUM (or CombMNZ) and then use items’ characteristics to build a manifold. Next, item relevance scores are modified considering other item scores in the same intrinsic
structure. Also aiming to use additional items information, Reference [2] proposed a method with
two alternated interdependent steps. At each step the method first learns a LETOR model that
maps items’ features to rank scores. It then tries to aggregate the input rankings by minimizing
the divergence between the aggregated ranking and the ranking scores generated in the LETOR
step.
3.2 Supervised Rank Aggregation Methods
Unsupervised rank aggregation methods are broadly used in the literature, mainly due to their
simplicity, efficiency (especially for score-based ones) and flexibility to be modified and adapted
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:7
for a large range of scenarios. However, when ground truth data are available, supervised ranking
methods may be a more effective approach.
Score-based supervised methods usually follow a Learning to Rank (L2R) approach [36] to build
aggregated rankings. In these methods, the items to be ranked are represented by a set of features,
which are used to build a rank model—usually a machine learning model—by optimizing the aggregation function with respect to a chosen metric. Common metrics include precision, MAP, and
nDCG. It is important to emphasize that the input of a rank aggregation problem consists only of
the rankings themselves—i.e., the order of the items—and by definition does not include the scores
used to produce the ranking. In this sense, any feature representing the items needs to be extracted
from the rankings.
As will be discussed in the next section, we found a single L2R aggregation method for recommender systems. Hence, here, we discuss methods proposed in other contexts that can be adapted
to the recommendation task. These methods usually extract a set of appropriate meta-features for
the domain of interest and then use popular L2R algorithms, including LambdaRank [65], AdaRank
[66], Coordinate (CA(L2R)) [38], and Random Forests (RF(L2R)) [4]. For example, in Reference [60]
the authors proposed an L2R method to perform rank aggregation in metasearch. They used features extracted from matrices of item pairwise preferences in the input rankings. The construction
of the items’ pairwise preference matrices follows a matrix completion method similar to the one
proposed in Reference [20]. However, instead of using an SVD to obtain an approximate complete
matrix, in Reference [60] the latent factors returned by SVD are used as items’ features to the
LambdaRank algorithm [5].
Still in the metasearch context, the authors in Reference [59] proposed a genetic programming
algorithm, called GP-Agg, to aggregate the results of rank aggregation methods. GP-Agg follows
a meta-level rank aggregation approach, i.e., the aggregation function generated by GP-Agg is
a composition of other rank aggregation methods. For example, given a set of rank aggregation
methods {f1, f2, f3,..., fn } and a set of input rankings {τ 1, τ 2,..., τ k } one possible aggregation
function generated by GP-Agg is Ψ = f1 (f3 (τ 3, τ 1), f4 (τ 5, τ 2)). This function first uses the rank
aggregation method f3 to aggregate the rankings τ 3 and τ 1, then uses f4 to aggregate τ 5 and τ 2
and, finally, the rank aggregation method f1 is used to aggregate the outputs of f3 and f4.
3.3 Rank Aggregation Applied to Recommendation
Although there are only a few methods described as rank aggregation in the RS literature, note
that some classes of hybrid recommendation methods actually perform rank aggregation. In particular, the methods proposed by References [49, 53, 56], which are introduced as mixed hybrids,
do perform rank aggregation. In Reference [49], the authors presented a hybrid recommendation
algorithm that aggregates the results of different input recommendation algorithms to improve the
accuracy, novelty, and diversity of the ranks. Aggregation is performed using a weighted linear
combination of the items returned by the recommendation algorithms used as input. The weights
are optimized by an evolutionary algorithm following a multi-objective Pareto-Efficient setting,
which considers the accuracy, novelty, and diversity of the aggregated rankings.
Following evolutionary approaches, the works of References [9] and [41] proposed rank aggregation methods to combine the rankings produced by top-N recommendation algorithms. Reference [9] used a Genetic Algorithm (GA) to combine different rankings coming from memory-based
collaborative filtering. The proposed GA builds a structure that defines how many items should be
selected from the top of each input rankings to compose the aggregated ranking. ERA (Evolutionary Rank Aggregation) [41], in contrast, uses genetic programming (GP) to evolve a population
of aggregation functions (individuals). These functions are used to attribute scores to the items
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
16:8 S. E. L. Oliveira et al.
Table 2. Selected Recommendation Algorithms
Algorithm Acronym Code Strategy Type Reference
BPRSLIM BPRSLIM MML IIS top-N [39]
[48]
ItemKNN ItemKNN MML IIS top-N -
LeastSquareSLIM LSSLIM MML IIS top-N [39]
Most Popular MP MML Most popular top-N -
BPRMF BPRMF MML MF top-N [48]
SoftMarginRankingMF SMRMF MML MF top-N [46]
[64]
WRMF WRMF MML MF top-N [27]
[44]
FISMauc FISM librec IIS - MF top-N [28]
LDA LDA librec PGM RP [23]
Hybrid Hybrid librec Hybrid top-N [67]
RankALS RALS librec MF top-N [55]
CofiRank Cofi author1 MF/L2R top-N [63]
libFM libFM author2 FM RP [47]
Poisson Factorization PF author3 PF top-N [22]
CoFactor CF author4 MF/L2R top-N [32]
present in the input rankings and then produce an aggregated ranking by sorting the items with
respect to their scores.
4 EXPERIMENTAL METHODOLOGY
This section introduces the experimental methodology followed to characterize the rankings generated by a set of collaborative filtering algorithms. It describes the algorithms and datasets included in our study, and also details the experimental setup.
4.1 Recommendation Algorithms
One of the objectives of this article is to perform an analysis of RS algorithms regarding how much
they agree in the set of items that should be at the ranking and how diverse are the rankings recommended. For that, we selected a set S of 15 well-known recommendation algorithms to generate the
input rankings, listed in Table 2. Note that we selected algorithms that employ distinct strategies, as
we expected them to have a higher probability of providing diverse recommendation rankings. The
selected algorithms use strategies that go from the commonly used matrix factorization (MF) and
item-item similarity (IIS) to probabilistic graphical models (PGM), hybrid methods, learning to rank
(L2R), factorization machines (FM), and Poisson factorization (PF). It is important to point out that
two algorithms (LDA and libFM) perform rating prediction (RP) instead of top-N recommendation.
For these algorithms, we generate the rankings by sorting the items by their predicted rating.
We used implementations provided by the algorithms’ authors along with implementations of
two recommender systems libraries: My Media Lite (MML) [19] and Librec [24].
1https://github.com/markusweimer/cofirank. 2https://github.com/srendle/libfm. 3https://github.com/premgopalan/hgaprec. 4https://github.com/dawenl/cofactor.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:9
Table 3. Recommendation Datasets
Dataset Acronym Users Items Ratings Ratings scale Items type Sparsity Avg. Items/User
MovieLens 1M ML1M 6K 4K 1M 1-5 movie 0.955 165.6
MovieLens 10M ML10M 70K 11K 10M 1-5 movie 0.987 143.1
MovieLens 20M Ml20M 138K 27K 20M 1-5 movie 0.995 144.4
Jester Jester 73K 100 4M −10-10 jokes 0.436 56.3
FilmTrust FT 1.5K 2K 35K 0.5-4.0 movies 0.988 23.5
Brookcrossing BX 105K 340K 1M 0-10 books 0.999 10.9
Yelp (Scottsdale) Yelp 25K 3K 87K 1-5 reviews 0.999 3.4
4.2 Datasets
The selected algorithms will be used to generate rankings for seven datasets belonging to four
different recommendation domains. The datasets main characteristics are listed in Table 3. MovieLens 1M, 10M, 20M [26], and Filmtrust [25] deal with movie recommendation, Jester [21] with joke
recommendation, Bookcrossing (BX) [68] with book recommendation, while Yelp recommends local businesses. With respect to the Yelp dataset, as the objective is to recommend local businesses,
we chose one city from the dataset to restrict the users and items (the local business) to a single
physical location. The chosen city was Scottsdale, Arizona, the fourth largest city in the dataset in
number of ratings (available from the sixth round of the Yelp Dataset Challenge).
For all datasets, we perform a two-step pre-processing before giving them as input to the recommendation algorithms: (i) Binarization of the ratings of the datasets, as the items are considered
only as relevant or irrelevant in the top-N recommendation task, (ii) Removal of users and items
that do not reach a predefined threshold value regarding frequency of ratings.
The binarization step considered that an item xj is relevant to a user ui if its rating Mi,j is
greater than the median of the ratings given by user ui . This approach has some advantages when
compared to the common approach of defining a fixed rating value as the relevance threshold
[37]. First, we do not need to scale the ratings of all datasets to the same range of values, nor
define a different threshold to each dataset. Second, this approach makes the definition of relevance
personalized to each user. Notice that, for the rating prediction algorithms (i.e., LDA and libFM), we
keep the predicted rating value of the items considered relevant. Despite the different approach
employed by these algorithms, we did not notice any major difference in their results or in the
rankings built from their predicted ratings when compared to the ranking-based algorithms.
Next, since our focus was not on the cold-start problem, we removed from the datasets infrequent items and users that rated very few items. Items rated by less than 10 users were removed,
together with users that had rated less than 10 items.
4.3 Experimental Setup
For each of the seven datasets, we first run all recommendation algorithms in the set S =
{S1, S2,..., Sk }, where k = 15. The set of output rankings T generated by these algorithms was
used to generate inputs R = {T1, T2,..., Tm } (wherem is the number of users of the dataset), which
were given to the rank aggregation methods. All the recommendation algorithms were run with
their default parameters. However, to assess the impact parameter tuning would have in our characterization results, Appendix D presents an analysis with a subset of recommendation algorithms
that had their parameters tuned, and shows these results present little variation regarding the experiments with default parameters.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
16:10 S. E. L. Oliveira et al.
For statistical validity, all experiments reported were performed using a 5-fold cross-validation
procedure. Hence, the items evaluated by each user were divided into training, validation, and test
sets, i.e., the rating matrix M was split into matrices Mtrain, Mval , and Mtest , respectively. Each
of these matrices contains the same users and items and, consequently, the same dimensionm × n.
The difference between them is on the items each user rated. Taking the original matrix M, in
each cross-validation fold, we use 60% of the ratings of each user for Mtrain, and the remaining
ratings are equally split among Mval (20%) and Mtest (20%). In Section 5, we perform our analyses
of rankings agreement using only the matrices Mtrain and Mtest . The matrix Mval is used in the
process of supervised learning aggregation functions, described in Section 6. In this way, based on
the past preferences of the user (the ratings in Mtrain), each recommendation algorithm predicts
which other items in the system may be relevant in the future to each user in the dataset. These
predictions are evaluated using the ratings in Mtest .
5 RECOMMENDATION ALGORITHMS SIMILARITY ANALYSES
As stated before, rank aggregation methods are a good choice when the rankings being aggregated
have a “limited but non-trivial overlap” [14]. In other words, the rankings being aggregated should
be diverse with respect to their items but, at the same time, have some level of agreement to help
identify which items are relevant to the user. When the set of input rankings has no overlap, it
becomes difficult to differentiate the items’ relevance. In this case, the only information available
to distinguish an item from another is its position in each individual ranking. However, when a
set of input rankings has a big overlap (i.e., there is no diversity between the rank items), the
aggregated ranking will be very similar to the input ones, bringing little to no improvement.
This section shows a characterization of the outputs of the 15 recommendation algorithms listed
in Table 2 to analyze whether their output rankings have the agreement and diversity required by
rank aggregation methods. We analyze:
(1) The similarity between rankings generated by different recommendation algorithms;
(2) The agreement between different sets of rankings considering all items, as well as only
hits (a hit denotes the occurrence of a relevant item in a ranking);
(3) The cumulative distribution of hits over the ranking positions for all algorithms.
The next sections present the results of these analyses for three out of the seven datasets:
ML10M, BX, and Jester. We chose to present these results because, due to the similarity of the
datasets in terms of the number of ratings and/or sparsity, most analyses are quite similar. BX
presents characteristics similar to Yelp, Jester is similar to FT, and ML10M to ML1M and ML20M.
We chose the datasets that are more representative for a detailed analyses, and at the end of this
section, we present a general discussion over all datasets.
5.1 Rankings Similarity
This first analysis measures how similar are the rankings recommended to users by different recommendation algorithms. For that, we compute pairwise distances of rankings using the Kendall
tau distance for partial rankings (top-N) with a penalty parameter p [15]. Let τ1 and τ2 be two
partial rankings of items in I and p be a fixed parameter, with 0 ≤ p ≤ 1. Let P(τ1, τ2) = I1 ∪ I2
be the set of unordered pairs of distinct items in rankings τ1 and τ2. We define a penalty K¯(p)
h,j for
each pair xh, xj ∈ P(τ1, τ2) considering four scenarios:
• xh, xj appear in both rankings. If xh, xj are in the same order (τ1 (h) > τ1 (j) andτ2 (h) > τ2 (j)),
then the penalty K¯(p)
h,j (τ1, τ2) = 0. If xh, xj are in the opposite order in τ1 and τ2, then the
penalty K¯(p)
h,j (τ1, τ2) = 1.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:11
Fig. 2. Distance heatmaps: Each cell represents the distance between the rankings returned by the algorithm
in the row and the algorithm in the column. Lighter colors indicate closer rankings.
• xh, xj both appear in one ranking (say τ1) and exactly one of xh or xj (say xh) appear in the
other ranking (τ2). If xh is ahead of xj in τ1, K¯(p)
h,j (τ1, τ2) = 0, and K¯(p)
h,j (τ1, τ2) = 1 otherwise.
• xh but not xj appear in one ranking, xj but not xh appear in the other ranking. Then
K¯(p)
h,j (τ1, τ2) = 1.
• xh and xj both appear in one ranking, but neither appear in the other. In this case,
K¯(p)
h,j (τ1, τ2) = p. We set p = 1.
Based on these cases, K(p) is defined as in Equation (2). Its values range from 0 (when the rankings are equal) to 1. Two rankings can have a distance of 1 in two cases: (i) when there is no
intersection between the rankings’ items or (ii) when the rankings have the same items placed in
reverse order.
K(p)
(τ1, τ2) =

{h,j }∈P(τ1,τ2 )
K¯(p)
h,j (τ1, τ2). (2)
The results of this analysis are presented in the heatmaps in Figure 2. Each cell of the heatmap
represents the distance between the rankings in the given row and column. The lighter the cell
color, the closer (less distant) are the rankings generated by these algorithms. The algorithms in
each heatmap are ordered (in ascending order) by the average distance of their rankings to the
other algorithms’ rankings. This analysis considers rankings of size N = 10.
Observe that there are clusters of algorithms (the lighter areas in the superior left corners of
the heatmaps) in the heatmaps of BX and Jester. The rankings in these clusters share two characteristics: (i) they have a high intersection between their items and (ii) their items’ relative order is
similar. This does not occur for ML10M, which means that either: few of the item pairs that are in
the intersection between similar rankings returned by a pair of algorithms, say Sa and Sb , are also
present in the rankings returned by a third algorithm Sc ; or although the item pairs intersection
is high, the items appear in a different relative order. If the majority of item pairs in Sa ∩ Sb were
also present in Sc in the same order, the pairwise similarity between these three rankings would
be similar and, therefore, the colors of the cells that represent them in the heatmap would have
similar tones.
As the algorithms within the clusters in Figures 2(a) and 2(b) are similar according to the Kendall
tau distance, they also have similar values of P@10 (see results in Appendix B). This indicates that,
when a pair of items (xj, xh ) appears in the rankings of two of these algorithms, their order tends
to be the same in both rankings. However, their similarity is less evident when considering the
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020. 
16:12 S. E. L. Oliveira et al.
Fig. 3. Agreement of recommendation rankings in BX.
Fig. 4. Agreement of recommendation rankings in Jester.
values of MAP. This is because the Kendall tau distance only accounts for the relative order of
item pairs, and the absolute position of the items in the rankings has no effect on the distance.
The same is valid for the computation of P@10 but not for MAP. In the latter, relevant items in
higher rank positions have a bigger weight than the same items in lower positions. This explains
the distinct behavior of both metrics in the algorithms within the cluster.
5.2 Rankings Agreement
Looking further at the similarity of items in different rankings, this section analyzes how much
rankings generated by distinct recommendation algorithms agree among themselves. We define
ranking agreement as the number of items two rankings of size N, where N 
 n, have in common,
i.e.,the size of the rankings intersection |τh ∩ τj |. In this section, we use recommendation rankings
of size 10 (i.e., N equals 10). We also performed this analysis with rankings of size 20 and found
similar results.
These analyses are shown in Figures 3 to 5. We calculate the agreement of different combinations
of rankings considering different numbers of ranking in each combination. For example, here, we
have 15 algorithms that generate 15 rankings. We first calculate the agreement of pairs of rankings,
and then the ranking agreement considering 3, 4, and up to 15 rankings. However, notice that
there exists a value of ranking agreement for each user in the dataset. To summarize these results
of agreement per user without losing too much information, for each possible number of items in
agreement—which varies from 0 to 10 and where each combination of rankings can agree in none
up to all items—we compute the percentage of users in the dataset for each agreement value.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:13
Fig. 5. Agreement of recommendation rankings in ML10M.
In these graphs in Figures 3 to 5, the x axis corresponds to the number of rankings combined,
and the y axis corresponds to the percentage of users where rankings agree. For each combination
size (x axis), there is a histogram of the rankings’ agreement values. Notice that, for each size of
ranking combinations, we first compute the agreement considering all possible ranking combinations. For instance, consider the agreement of sets of three rankings. It would generate 
15
3

= 455
possible ranking combinations. We count for how many users on average the 3 rankings in the
combinations agree on 0, 1, 2,..., N items, and report it in terms of percentage. Since the agreement in combinations greater than 5 is small for all datasets, for the sake of simplicity, we omit
these results.
Each Figure presents two graphs: the one on the left considers the agreement between all the
items recommended to the users, and the one on the right considers only the items recommended
that are relevant to the users (i.e., the items relevant to the users in the test set). Note that the graphs
on the right also contain the agreement between a single method; that is effectively a histogram
of mean hits among all algorithms and serves as a baseline of comparison for the agreement size.
To elucidate these results, let us consider the graph in Figure 5(a). When we consider the average agreement of any two rankings (the first histogram of the graph, labeled 2), almost 30% of the
users have no intersections between the 10 items recommended to them by two algorithms, about
15% have one item in the intersection, around 14% have two items in the intersection, and so on.
The same interpretation is valid for the other histogram columns (the number of items in the intersection) and to the other values in the x axis (the number of rankings taken into consideration).
Considering Figures 3(a) to 5(a), we observe that, except for the BX dataset, the input rankings
for the majority of users share one or more items, mainly when considering 2 or 3 rankings. As the
number of rankings in the combination increases, so does the number of users whose rankings did
not agree in any items. The exception is BX, for which there is almost no intersection between the
rankings for the majority of the users regardless of the number of algorithms combined. This effect
may be related to data sparsity: BX is the sparsest dataset, leading to less accurate recommendations (see column 1 in Figure 3(b), where the recommenders produce rankings with no relevant
items for more than 80% of the users).
The biggest ranking agreement, however, occurs in the rankings returned to the users of Jester.
This occurs both when considering all items and only hits. In contrast with BX, Jester is the leastsparse dataset and the one where the recommenders produced the best rankings according to the
evaluated metrics (detailed later in Section 7 and Appendix B).
Finally, the agreement in ML10M is lower than in Jester but bigger than in BX. For example,
when considering two rankings in ML10M (column labeled 2 in Figure 5(a)), at least 70% of the
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
16:14 S. E. L. Oliveira et al.
Fig. 6. CDF of hits for recommendation algorithms.
users’ rankings have one or more items in common (the sum of all columns except for the column
that represents zero items in the agreement). For Jester, this value reaches almost 90%, and for BX
this percentage is lower than 50% of the users. This behavior does not change when we consider
only hits.
Comparing the graphs on the right to their counterparts on the left, we notice that, when we
take into consideration only the hits, the number of items for which the rankings agree decreases
considerably. This observation shows that, despite the concordance of two or more rankings on
the importance of an item, the agreement can involve non-relevant items to the users, i.e., there
is a non-negligible probability that two or more rankings miss-retrieve the same item. This will
certainly influence the results of rank aggregation methods.
5.3 Distribution of Relevant Items
An important aspect of the rankings returned by top-N recommendation algorithms is that items
that have a higher probability of being a hit should be placed at the top positions of the ranking.
However, we observed that the number of hits in the top rankings vary substantially from one
algorithm to another. For this reason, we investigate the distribution of hits over the rankings’
positions. In this particular experiment, we chose ranks of size N = 100 to show the distribution
of items in lower-ranking positions.
Figure 6 shows the cumulative distribution function (CDF) of hits in the top-100 rankings positions returned by the recommendation algorithms described in Table 2. The x-axis represents
the position in the ranking (up to 100), and the y-axis represents the cumulative probability of a
hit up to that ranking position (i.e., P (hit rankinд position) ≤ x). For the sake of simplicity, we
only present the CDF of the rankings returned by the algorithms that obtain (i) the best value
of mean average precision (MAP—see Section 7 for details on this metric), (ii) the worst MAP
value, (iii) and the average of all the 15 algorithms’ hits cumulative probability. Note that when we
say average of the hits cumulative probability, we have an abuse of terminology. This average is
calculated as follows: For all rankings returned by the recommendation algorithms, we calculate
the percentage of hits they have in each position of the ranking. Suppose we have four rankings
τ1 = {00010}, τ2 = {00100}, τ3 = {00100}, τ4 = {01010}, with a total of five hits altogether.
The values for the CDF of each position would be 0, 20 (one hit out of 5 from τ4), 60, 100, and 100%.
Figure 6 also shows the CDF for the most popular (MP) algorithm—i.e., the one that recommends
the most popular items, which serves as a baseline for non-personalized recommendation.
Apart from the curves representing the worst algorithms for each dataset, in all graphs, we
notice a concentration of hits in the first rankings positions. This is illustrated by the higher slope in
the beginning of the CDFs. As we previously stated, this behavior is expected for top-N algorithms.
However, it is interesting to notice the different slopes of the CDFs and the distances between
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:15
them on different datasets. As expected, after the analyses performed in Section 5.2, the rankings
returned for the Jester dataset are the ones where the CDFs have the steepest slopes. This happens
because the rankings for Jester have, on average, a better quality than the rankings returned to
the users in BX and ML10M. For BX and ML10M, the shapes of the CDFs that describe the average
behavior of the rankings are similar. However, the CDF of the best algorithm is closer to the average
in ML10M than in BX.
Finally, notice that despite the concentration of relevant items in the first ranking positions,
there is a considerable number of relevant items much lower in the rankings. For example, for
ML10M (Figure 6(c)), in the rankings generated by WRMF, only 40% of the relevant items in the
rankings are concentrated in the top 20 positions. This represents an interesting scenario for ranking aggregation methods, which can move items from lower to higher positions in the ranking.
5.4 Discussion
This section presented the characterization results for the rankings of three out of the seven
datasets considered in our experiments. However, we emphasize that the BX dataset presents characteristics similar to Yelp, the Jester dataset to FT, and the dataset ML10M to ML1M and ML20M.
Due to this similarity, the results of the analyses performed were also similar for datasets with
similar characteristics.
BX and Yelp are the sparsest datasets, leading to the rankings with the lowest values of MAP,
precision, and NDCG (see Section 7). Looking at the characterization of BX, we notice that it
has the lowest levels of agreement between the rankings (Figures 3(a) and 3(b)), and also that its
rankings are the least similar among themselves (Figure 2(a)). Finally, despite the similar shape of
the CDFs showing the distribution of relevant items (Figure 6) of BX and ML10M, it is important
to notice that they reflect the distribution of the hits over the positions in the rankings. However,
the average number of hits in BX is small, being one order of magnitude smaller than the ones in
ML10.
Jester and FT are the least sparse datasets and have the highest values of MAP, precision, and
NDCG and the biggest agreement between items in the recommended rankings (Figure 4). The
high quality of the rankings generated for these datasets justifies the fact that the hit per position
CDFs for Jester and FT have the steepest slopes among them all. Jester presents a cluster of five
similar algorithms (Figure 2(b)) that represent one-third of the recommendation algorithms used,
and, in general, its rankings more similar among themselves than the rankings returned to the
other datasets.
Finally, all MovieLens recommendation datasets have similar characteristics. In the rankings
recommended to the users of these datasets, the CDFs of hits of the best algorithm are similar to
the CDFs of the average of all algorithms (Figure 6(c)), a behavior that does not happen neither in
Jester nor in BX. The absence of a clear cluster when analyzing the algorithms’ rankings distances
(Figure 2(c)) also indicates a high diversity among the rankings.
6 RANKING AGGREGATION METHODS COMPARED
In the previous section, we detailed how much the rankings generated by the recommendation
algorithms agree and how diverse they are. This section uses 19 different rank aggregation methods
to combine the rankings generated by the 15 recommendation algorithms analyzed earlier.
6.1 Unsupervised Methods
We have tested 14 unsupervised score-based ranking aggregation methods, including eight
positional—Borda Count [10], the Comb* family [18] (CombSum, CombMed, CombMax, CombMin,
CombMNZ, CombANZ), Median Rank Aggregation (MRA) [16]—and six majoritarian—Outrank
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
16:16 S. E. L. Oliveira et al.
Table 4. Combination Algorithms Proposed in Reference [18]
CombMIN min
τ r ∈T sc(xj, τ r )
CombMAX max
τ r ∈T sc(xj, τ r )
CombSUM 
τ r ∈T
sc(xj, τ r )
CombMED 1
|T | ∗ CombSU M(xj, τ r )
CombANZ 1
| {τ r |τ r ∈T :xj ∈τ r } | ∗ CombSU M(xj, τ r )
CombMNZ CombSU M(xj, τ r ) ∗ |{τ r |τ r ∈ T : xj ∈ τ r }|
[17], three versions of Markov Chain Methods [35] (namely, MC1, MC2, and MC3), Stuart [54],
and Robust Rank Aggregation (RRA) [29].
Recall that positional methods require the set of rankings T , an aggregation function f : U ×
I → R, and a procedure to combine the item scores. When using Borda Count, each item xj in the
rankings receives a score xj borda = 
{τ r |∀τ r ∈T,xj ∈τ r } |τ r | − τ r (xj) + 1. The items are then sorted
in descending order of their scores, and the aggregated ranking is composed by the top-N items
of interest.
The Comb family of methods, in turn, proposed by Reference [18], uses the score function
sc(xj, τ r ) = 1 − τ r (xj )−1
|τ r | to attribute a score to an item xj given its position in a ranking τ r . After the scores are computed, they are combined using a variety of methods, presented in Table 4.
The last positional method, MRA [16], sorts the items by the median of the positions they received
in the rankings τ r ∈ T .
The Outrank method [17], which follows the majoritarian approach, compares items in a pairwise fashion and, for each ordered pair of items xj and xh, computes a concordance and a discordance condition with respect to the order xj ≺τ r xh (i.e., xj appears before xh in a specific ranking
τ r ). The concordance condition is used to verify if the majority of the rankings agrees that xj ≺ xh.
The discordance condition verifies if no ranking strongly disagrees with xj ≺ xh, i.e., if xj never appears far behind xh for all rankings, where far behind is a user-defined parameter. After these two
conditions are computed, items are aggregated in a consensus ranking, where items with higher
concordance and lower discordance values appear first.
The second of the majoritarian methods, Robust Rank Aggregation (RRA) [29], generates consensus rankings by comparing the position of an item in each ranked list with a null model, i.e.,
random lists of items. The idea is to find out how probable it is to obtain τ1 (xj) ≤ τ2 (xj) when
τ1 (xj) is generated by the null model. A numerical score is assigned to each item based on the reference distributions of order statistics, i.e., beta distributions. P-values are computed based on the
Bonferroni correction of the numerical scores to avoid intensive computation required to obtain
exact P-values. The final aggregated rank is obtained by sorting P-values.
The third majoritarian method, Stuart, was proposed in Reference [54] in the context of bioinformatics to identify pairs of genes that are co-expressed from experiments in multiple organisms.
The method first identifies pairs of genes (in our case, items) whose expression is significantly correlated in multiple organisms (multiple rankings) using Pearson correlation. Each item receives a
linear score according to its position in the ranking. Items are then ordered by their correlation.
Next, the authors use a probabilistic method based on order statistics to evaluate the probability
(P value) of observing a particular configuration of ranks across the different organisms by chance.
Finally, we consider three methods based on Markov Chains that differ in the way they build
the transition matrix, used to control the transitions between the states (recall that each state corresponds to an item) [11, 34]. They are: (i) MC1: The next state is generated uniformly from the set
of all states that are ranked at least as high as the current state by at least one base ranker; (ii) MC2:
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:17
The next state is generated uniformly from the set of all states that are ranked at least as high as
the current state by at least half of rankings; (iii) MC3: The probability of moving to a certain state
is proportional to the number of rankings that rank this state higher than the current state.
6.2 Supervised Methods
We evaluated the performance of supervised rank aggregation using five methods. The first is
ERA [41], which is an L2R algorithm proposed for the rank aggregation task in the recommendation context. We also used four other general L2R algorithms: (i) LambdaRank (LR(L2R)) [65],
(ii) AdaRank (AR(L2R)) [66], (iii) Coordinate Ascent (CA(L2R)) [38], and (iv) Random Forests
(RF(L2R)) [4]. Four out of these five algorithms use the L2R list-wise approach, RF(L2R) being
the exception, as it follows a point-wise approach. Methods based on a list-wise approach optimize a ranking metric in the learning phase (such as MAP or NDCG) and, therefore, evaluate the
rankings that are being produced. Methods that use a point-wise approach, however, optimize the
output of the rankings with respect to individual items, e.g., the accuracy with respect to the items’
relevance. We use the implementations in RankLib’s5 L2R library for the general L2R algorithms,
and the authors’ implementation6 for ERA.
As previously explained, the supervised algorithms construct their models using a set of features
extracted from the rankings that will be aggregated. These features provide a feature space for
computing items’ scores, which will be used in the rank aggregation itself. In this work, we used
the same set of features used by ERA [41] (listed in Appendix C), but any other set of features
could have been used. The methods in References [62] and [60] also extract features to use in an
L2R approach, such as latent factors obtained by factorizing an item by item matrix with SVD,
or pairwise features (e.g., the distance between the rank positions of all pairs of items) extracted
from the same item by item matrix. However, the features used in Reference [41] are simpler
and cheaper to be computed. As detailed in Appendix C, they come from positional methods,
majoritarian methods, and also include features based on the items’ frequency over the rankings,
which covers distinct items and rankings’ characteristics.
6.3 Experimental Setup for Rank Aggregation Methods
Similarly to the experimental setup presented in Section 4.3, in the ranking aggregation experiments, we also use a 5-fold cross-validation procedure. However, the experiments conducted in this
section are performed in two phases, namely, a training phase (only necessary for the supervised
algorithms) and a test phase.
Figure 7 describes the training phase of the supervised rank aggregation methods, where the
output of the algorithm is an aggregation function Ψ. First, the recommendation algorithms undergo a training phase, where the input rankings are generated using only the training partition
(Mtrain) of the recommendation datasets. After, a set of features F(ui,xj ) is extracted for each pair
(user,item) in the input rankings and given as input to the rank aggregation supervised algorithms. During the training phase, the aggregated rankings generated by the aggregation function
being learned are evaluated in the validation dataset (Mval). Finally, the algorithm returns the
aggregation function generated. Note that unsupervised algorithms can be seen as an aggregation
function themselves, and do not need to undergo any learning phase.
Having the rank aggregation function, the test phase follows the procedure described in Figure 8.
First, we combine the matrices Mtrain and Mval into a single matrix, rerun the recommendation
algorithms, and extract the features F(ui,xj ) (this step is not necessary for the unsupervised algorithms). Finally, we use Ψ to attribute a score to each item xj ∈ Ii , and then we sort the items in
5https://sourceforge.net/p/lemur/wiki/RankLib/. 6https://github.com/samuevan/ERA.
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
16:18 S. E. L. Oliveira et al.
Fig. 7. Process of learning the aggregation function for supervised algorithms.
Fig. 8. Process of using an aggregation function Ψ to aggregate rankings.
descending order to form one aggregated ranking τ ∗
i for each user ui . In all experiments, we use
the default parameters for the rank aggregation methods.
6.4 Evaluation Metrics
We used the precision, Mean Average Precision (MAP), and Normalized Discounted Cumulative
Gain (NDCG) metrics to evaluate both the input and the aggregated ranking. Precision (P@K,
where K means we are considering the positions up to K to compute the metric) measures the
percentage of relevant items among all items in the ranking. Equation (3) shows the computation
of precision at positionK, where Rel(ui ) is the set of relevant items to userui , and τ r
i @K represents
the first K items of ranking τ r
i :
P@K

τ r
i

=



Rel(ui )
τ r
i @K





τ r
i @K


. (3)
The Average Precision (AP) and MAP metrics, in contrast with precision, consider the position
at which the relevant items occur in the ranking [51, 52]. Therefore, AP is a ranked precision
metric that puts emphasis on highly ranked correct predictions. Note that in the recommendation
scenario, we have a ranking for each user in the dataset, and consequently MAP is the mean AP
over all users. Equation (4) shows how to compute the values of AP for a given ranking τ r
i , and
Equation (5) shows the MAP computation for all users of the dataset:
AP@K

τ r
i

=

K
z=1
P@z

τ r
i

×relui (xz )
min
|Rel(ui )|,



τ r
i



 , (4)
MAP@K = 1
|U|

|U |
i=1
AP@K(τi ), (5)
where relui (xz ) is the relevance of item xz to the user ui .
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.                    
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:19
Finally, we also assess the rankings quality through NDCG, a ranking metric commonly used in
information retrieval. NDCG is the normalized value of DCG, which measures the normalized gain
of an item given its relevance and the position in the ranking being evaluated. Given a ranking τ i
k
(i.e., a ranking recommended to the user ui ), the DCG is defined as in Equation (6):
NDCG
τ r
i

= DCG
τ r
i

DCG∗

τ r
i
 , (6)
DCG
τ r
i

=

xj ∈τ r
i
relui (xj)
loд2

τ r
i (xj) + 1
 , (7)
where DCG∗ is the maximum DCG possible for ranking τ r
i (which occurs when the items xj ∈ τ r
i
are sorted in decreasing order considering their relevance).
7 RESULTS AND DISCUSSION
This section presents the experimental results when comparing the selected recommendation algorithms and the supervised and unsupervised ranking aggregation methods. As we are dealing with
a recommendation task, we consider that users only evaluate a few recommended items. Therefore, in our experiments the rank aggregation methods always return an aggregated ranking of
size 10. With respect to the size of the rankings used as input to the rank aggregation methods, we
used rankings of size 20. In previous work, the authors in Reference [41] tested input rankings of
sizes 10 and 20 to produce aggregated rankings of size 10 and noticed that when using 20 items the
rank aggregation methods were able to produce better aggregated rankings. This is intuitive, as
when one increases the input rankings size, it also increases the chances of retrieving items placed
in lower positions of the ranking.
Due to the great number of recommendation algorithms, rank aggregation methods, and
datasets, we chose to present only the results of the best algorithms (with respect to MAP) in
each category. The complete set of results can be found in Appendix B. Table 5 shows the results
of (i) the two best recommendation algorithms, (ii) the three best unsupervised rank aggregation
methods (Borda Count and MRA), (iii) the two best general L2R algorithms (Coordinate Ascent—
CA(L2R)—and Random Forest RF(L2R)) and, finally, (iv) the L2R rank aggregation method ERA, as
it was originally proposed for rank aggregation.
We evaluated the rankings using MAP@10, P@1, P@10, and NDCG@10. Due to the nondeterministic nature of the supervised algorithms, we have performed 5 runs for each fold of the
5-fold cross validation procedure, resulting in 25 executions for each algorithm and dataset. To obtain the statistical relevance of the results, we apply a Friedman’s test followed by Wilcoxon’s test
(with 95% confidence) with Bonferroni correction as a post hoc procedure to make the pairwise
comparison between the algorithms’ results. Table 5 also presents the results of this test, where
the symbols  and  are used to denote if the results of the best rank aggregation method are
statistically better or worse than the results of the best individual recommendation algorithm.
First, notice that there is a pattern in the results of the recommendation algorithms. In general,
WRMF is among the top-two algorithms in five out of seven datasets. The Hybrid algorithm also
presents good results for the sparser datasets (BX and Yelp), while LDA is among the best for the
least sparse datasets (FT and Jester). Finally, RALS and CF present the overall best results for the
MovieLens datasets.
The results presented in Table 5 also provide evidence on how using rank aggregation can improve the results of individual recommenders. Note that in six out of seven datasets at least one
of the rank aggregation methods was better than individual recommendation algorithms. The
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.   
16:20 S. E. L. Oliveira et al.
Table 5. Evaluation Metrics of the Best Recommendation Algorithms and Rank Aggregation Methods
Dataset Algorithms MAP@10 P@1 P@10 NDCG@10
Yelp
Recomm. Alg Hybrid 0.0372 0.0686 0.0446 0.3400
WRMF 0.0380 0.0704 0.0429 0.3336
Unsup. Agg
MC1 0.0376 0.0710 0.0398 0.3156
CombSUM 0.0374 0.0699 0.0420 0.3236
CombMED 0.0372 0.0696 0.0419 0.3243
Sup. Agg
CA(L2R) 0.0408 0.0707 0.0455 0.3450
RF(L2R) 0.0399 0.0687 0.0457 0.3503
ERA 0.0392 0.0712 0.0439 0.3357
BX
Recomm. Alg BPRSLIM 0.0390 0.0737 0.0315 0.2392
Hybrid 0.0485 0.0939 0.0422 0.2748
Unsup. Agg
MC1 0.0425 0.0780 0.0358 0.2671
Stuart 0.0281 0.0479 0.0293 0.2339
RRA 0.0277 0.0471 0.0289 0.2316
Sup. Agg
CA(L2R) 0.0479 0.0827 0.0422 0.2946
RF(L2R) 0.0506 0.0949 0.0421 0.2977
ERA 0.0465 0.0842 0.0396 0.2772
Filmtrust
Recomm. Alg LDA 0.3306 0.4145 0.2929 0.896
LSSLIM 0.3426 0.4287 0.2953 0.8926
Unsup. Agg
MC2 0.3630 0.4673 0.3053 0.9045
MC3 0.3605 0.4645 0.3031 0.9028
Stuart 0.3585 0.3573 0.3015 0.9030
Sup. Agg
CA(L2R) 0.3572 0.4559 0.3028 0.9022
RF(L2R) 0.3309 0.4084 0.2925 0.9036
ERA 0.3468 0.4349 0.2992 0.9014
Jester
Recomm. Alg LDA 0.2849 0.4041 0.2704 0.8431
WRMF 0.2823 0.4007 0.2730 0.8476
Unsup. Agg
MC2 0.3000 0.4248 0.2820 0.8497
MC3 0.2996 0.4210 0.2828 0.8509
BordaCount 0.2989 0.3431 0.2817 0.8525
Sup. Agg
CA(L2R) 0.3077 0.4410 0.2923 0.8542
RF(L2R) 0.3033 0.4301 0.2907 0.8529
ERA 0.3018 0.4282 0.2828 0.8526
ML1M
Recomm. Alg CF 0.1843 0.3754 0.2620 0.8358
WRMF 0.1812 0.3850 0.2503 0.7828
Unsup. Agg
MC3 0.1983 0.4220 0.2643 0.8179
MRA 0.1985 0.4167 0.2679 0.8197
Stuart 0.1977 0.3215 0.2637 0.8182
Sup. Agg
CA(L2R) 0.2157 0.4296 0.2866 0.8446
RF(L2R) 0.2097 0.4235 0.2813 0.8408
ERA 0.2139 0.4323 0.2839 0.8424
ML10M
Recomm. Alg RALS 0.1825 0.3756 0.2329 0.7515
WRMF 0.1877 0.3776 0.2401 0.7536
Unsup. Agg
RRA 0.2016 0.4021 0.2510 0.7751
Stuart 0.2035 0.4085 0.2537 0.7786
MRA 0.1986 0.3922 0.2516 0.7754
Sup. Agg
CA(L2R) 0.2196 0.4213 0.2781 0.8137
RF(L2R) 0.2272 0.4372 0.2794 0.8125
ERA 0.2015 0.3981 0.2581 0.7968
(Continued)
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.                            
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:21
Table 5. Continued
Dataset Algorithms MAP@10 P@1 P@10 NDCG@10
ML20M
Recomm. Alg RALS 0.1675 0.3514 0.2181 0.7206
WRMF 0.1748 0.3586 0.2262 0.7232
Unsup. Agg
RRA 0.1851 0.3760 0.2364 0.7459
Stuart 0.1897 0.3911 0.2425 0.7542
MRA 0.1841 0.3684 0.2368 0.7452
Sup. Agg
CA(L2R) 0.2080 0.4071 0.2642 0.7925
RF(L2R) 0.2121 0.4155 0.2661 0.7925
ERA 0.2132 0.4170 0.2659 0.7934
The symbols  and  indicate if the results of the rank aggregation methods are better or worse than the results of
the best recommendation algorithm.
exception was Yelp. Besides, the best overall results were achieved by the supervised ranking
aggregation methods. The ranking aggregation methods were better than the individual recommendation algorithms in 32 cases. From these 32, 18 represented supervised aggregation methods
and 14 unsupervised aggregation methods. We expect supervised aggregation methods to perform
better, as they have more information to work with than the unsupervised methods. However, the
unsupervised methods also presented a good performance. The magnitude of the improvements
of supervised rank aggregation methods over the recommendation algorithms (and even over the
unsupervised rank aggregation methods) is interestingly correlated with the rankings characteristics explored in Section 5. To elucidate these correlations, the following sections discuss the main
results by grouping the rankings generated for each dataset according to their characteristics.
7.1 Rankings Characteristics and Rank Aggregation
The results presented in this section show that the effectiveness of rank aggregation, as expected,
is intrinsically correlated with the characteristics of the rankings used as input. However, the ranking characteristics, in turn, are highly correlated with the dataset characteristics, such as sparsity,
which have a high impact on: (i) the concentration of relevant items in the first rankings positions (which reflects in the rankings quality); (ii) the agreement between the input rankings; and
(iii) the distance (and consequently the similarity) between the input rankings.
We analyze our results considering three scenarios: (1) Very low ranking overlap and poor input
rankings quality; (2) High ranking overlap and good input rankings quality; and (3) Low ranking
overlap (concentrated in pairwise rankings relations) and good rankings quality. By poor (good)
input rankings quality, we refer to the set of input rankings whose MAP is further than (closer
to) the best MAP obtained for that dataset. These scenarios reflect the characteristics of the input
rankings considered.
In scenario 1, which includes BX and Yelp, our results showed that the very small overlap between the rankings (see Figure 3) makes it difficult to improve the results of the recommendation
algorithms by using rank aggregation. Additionally, the aggregation task becomes even more difficult if the majority of the input rankings has no or very few relevant items (see Figures 3(a), 6(a),
and Tables 6 and 7 in Appendix B). In this scenario, the unsupervised rank aggregation methods
produced rankings that are even worse than the ones produced by the best recommendation algorithms (WRMF and Hybrid) for both datasets. However, the supervised algorithms were able to
produce slightly better rankings for BX dataset (RF(L2R) obtained 4.5% of improvement in MAP
and 7.8% in NDCG) and rankings with results not statistically significant different from the ones
produced by WRMF for Yelp. In this scenario, the rank aggregation methods can be misled by
irrelevant items in top rank positions (poor quality rankings) and by the rankings agreement in
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.          
16:22 S. E. L. Oliveira et al.
irrelevant items. Since unsupervised methods have no way to distinguish irrelevant items from
the relevant ones, they present worse results in this scenario.
However, in scenario 2, which includes Jester and FT, we have very similar good quality input
rankings (see Figure 6(b)), i.e., rankings with a big overlap (see Figures 4 and 2(b)), which leaves
a small margin for improvement for rank aggregation (when compared to scenario 3). This effect
happens because there is not enough information to build a ranking that is, at the same time, better
than the input rankings and different from these rankings individually. Thereby, for Jester both the
supervised and unsupervised rank aggregation methods presented improvements when compared
with the results of the best recommendation algorithm (CA(L2R) obtained 8% of improvement in
MAP and P@10 and 9% in P@1). For FT both supervised and unsupervised methods achieved
improvements (5.9% and 4% in MAP obtained by MC2 and CA(L2R), respectively, and 9% in P@1
obtained by MC2). Notice that improvements in P@1 and MAP followed by no improvements in
P@10 indicate that the rank aggregation methods rearranged the relevant items in the rankings
while keeping the number of relevant items unchanged. It is interesting to notice that the difference
of the results of the supervised and unsupervised algorithms is very small for both datasets. In this
scenario, the big overlap among ranking items along with the good quality input rankings make
it easier for the unsupervised algorithms to obtain results that are closer to the ones obtained by
the supervised algorithms.
Rank aggregation methods achieved the biggest improvements in datasets that fit scenario 3.
These datasets have good quality input rankings that are diverse with respect to the items recommended to the users, i.e., the overlap among the input rankings is small (see Figures 5 and 2(c)).
This happens for the MovieLens datasets. In all three datasets, there is a set of recommendation
algorithms for which the quality of the rankings produced is similar but the items in the rankings differ significantly. Because of that, the quality of the individual rankings is inferior to the
quality of the consensus rankings obtained by the rank aggregation methods. Additionally, there
are no clusters of similar algorithms, i.e., the rankings are diverse in items recommended to users.
Compared to the previous scenarios, this one gives the biggest margin for improvement for rank
aggregation methods.
Different from what happens for Jester and FT, there are considerable distinctions between the
results of the supervised and unsupervised approaches. For example, for ML20M the improvement
achieved by the unsupervised algorithms reaches 8.5% in MAP (for MRA), while the improvements
achieved by the supervised algorithm are of up to 21% for the same metric (for ERA). This happens
because, while the unsupervised algorithms considered in our experiment give all input rankings
the same weight, the supervised methods are able to learn which features will produce the best
rankings. However, it is important to mention that in Reference [12], the authors proposed an
unsupervised method for rank aggregation in metasearch that can attribute different weights to
different input rankings to avoid scenarios where most rankings did not perform well.
7.2 Should I Use Rank Aggregation Methods?
Our experimental results show improvements in the recommended ranking by using rank aggregation in all scenarios, which indicates they should be used in the recommendation scenario.
However, the extent of these improvements as well as the choice of the methods used to perform the aggregation are correlated to the characteristics of the set of rankings to be aggregated,
which in turn depends on the characteristics of the recommendation datasets. The characteristics
of the ranking can be highly influenced by the recommendation algorithms used as input, and
selecting how many and which methods to use in a more informed manner is a natural direction
of future work, but is out of the scope of this article. The authors in Reference [8], for example,
present an extensive review on the task of selecting collaborative filtering algorithms. The work in
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
Is Rank Aggregation Effective in Recommender Systems? An Experimental Analysis 16:23
Reference [42] proposes a set of heuristic methods to select an appropriate number and set of recommendation algorithms for rank aggregation.
Having said that, based on the results of our experiments, we can draw some guidelines to help
choose the most effective rank aggregation methods based on the distance of the input rankings:
• If you have have no a priori information about the rankings quality, agreement, or distance,
then supervised rank aggregation methods would be your choice. We recommend to start
with CA(L2R), ERA, and RF(L2R). If you have no label information, then we recommend
Stuart, RRA, and MRA.
• If you know that the set of input rankings are distant from each other, then again supervised
rank aggregation, such as CA(L2R) and RF(L2R), will provide the best results. In our experiments, the average Kendall tau distance between rankings where supervised methods
excelled was 0.88 (0.83 in the first quartile [Q1], 0.91 in the second [Q2], and 0.96 in the third
[Q3]) in the case of BX and 0.83 for Yelp (Q1:0.76, Q2: 0.83, Q3: 0.88). These values for Jester
and ML10M, in contrast, are 0.62 (Q1: 0.51, Q2: 0.57, Q3: 0.69) and 0.79 (Q1: 0.73, Q2: 0.77, Q3:
0.83), respectively. Although the average of ML10 is not far from the average of Yelp, note
the Q3 of ML10M is equal the average of Yelp. In this same scenario, if there was no labeled
data available, the MC1 would be our recommended method.
• In cases where the set of input rankings are close to each other, we noticed little improvement
in using supervised instead of unsupervised rank aggregation methods. As unsupervised
methods are faster, we recommend using MC2. This was the case for Jester with an average
Kendall tau distance of 0.62 (Q1: 0.51, Q2: 0.57, Q3: 0.69) and Filmtrust with 0.63 (Q1: 0.54,
Q2:0.58,Q3: 0.71).
Finally, the heuristics proposed by Reference [42] could be used to help select a subset of rankings to be used as input to the rank aggregation methods compared in this article. In a similar
fashion, the genetic algorithm proposed in Reference [58] can search for combinations of rankings
that, when given to unsupervised aggregation methods, optimize unsupervised metrics—such as
rank distances or supervised ones—including MAP or NDCG.
8 CONCLUSIONS AND FUTURE WORK
In this work, we conducted an extensive analysis of rank aggregation for RS. This analysis included
the characterization of the agreement and diversity of rankings generated by 15 recommendation
algorithms as well as a joint analysis between the input rankings and dataset characteristics and
the comparison of 19 supervised and unsupervised rank aggregation methods.
The characterization phase showed interesting characteristics of the rankings generated by distinct recommendation algorithms. The rankings agreement analyses showed that the presence of
an item in more than one ranking cannot be used as a strong evidence of its relevance. When we
analyze the distribution of the relevant items over the positions in the rankings, we noticed that
there is, in fact, a concentration of relevant items in the first rank positions. However, when we
consider lower positions (e.g., the ones after the 20th position), there is still a considerable percentage of relevant items. The study of the potential of rank aggregation methods in retrieving
these items and bringing them to the top positions of the ranking is left for future analysis, as the
improvements obtained when using these methods may be simply due to the diversity of rankings
returned by different algorithms. As we showed, the distance between the rankings can be used as
a measure of rankings diversity and, along with the rankings agreement in relevant items, is paramount to help rank aggregation methods achieve improvements over individual input rankings.
Our results show improvements in the recommended rankings by using rank aggregation in all
the studied scenarios. Supervised rank aggregation methods (particularly CA(L2R), RF(L2R), and
ACM Transactions on Intelligent Systems and Technology, Vol. 11, No. 2, Article 16. Publication date: January 2020.
16:24 S. E. L. Oliveira et al.
ERA), in turn, provide the biggest improvements in the results of the rankings used as input. Unsupervised rank aggregation only presents similar results to the ones obtained by their supervised
counterpart for the Jester and Filmtrust datasets. However, in the absence of labels, the unsupervised methods that presented the best results were MC1 for BX and Yelp (scenario 1), MC2 for
Filmtrust and Jester (scenario 2), and MRA and Stuart for the MovieLens datasets (scenario 3).
The results of rank aggregation encourage future research on the use of meta-learning algorithms [3] to help in the choice of the better rank aggregation method for each case. Meta-learning
refers to use of meta-knowledge to learn how to combine or choose the most suitable method to
deal with a problem. We can bring this concept to our context by representing the characteristics
of a set of rankings and the rank aggregation improvements obtained by different rank aggregation
methods as meta-knowledge information. This meta-knowledge can be used to learn a model that
recommends rank aggregation algorithms to distinct scenarios and would be able to systematize
the experimental analyses performed in this work.