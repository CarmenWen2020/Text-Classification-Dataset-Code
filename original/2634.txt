Recently, co-saliency detection, which aims to automatically discover common and salient objects appeared
in several relevant images, has attracted increased interest in the computer vision community. In this article, we present a novel graph-matching based model for co-saliency detection in image pairs. A solution
of graph matching is proposed to integrate the visual appearance, saliency coherence, and spatial structural
continuity for detecting co-saliency collaboratively. Since the saliency and the visual similarity have been
seamlessly integrated, such a joint inference schema is able to produce more accurate and reliable results.
More concretely, the proposed model first computes the intra-saliency for each image by aggregating multiple saliency cues. The common and salient regions across multiple images are thus discovered via a graph
matching procedure. Then, a graph reconstruction scheme is proposed to refine the intra-saliency iteratively.
Compared to existing co-saliency detection methods that only utilize visual appearance cues, our proposed
model can effectively exploit both visual appearance and structure information to better guide co-saliency
detection. Extensive experiments on several challenging image pair databases demonstrate that our model
outperforms state-of-the-art baselines significantly.
CCS Concepts: • Computing methodologies → Artificial intelligence; Computer vision; Computer
vision tasks; Computer vision problems;
Additional Key Words and Phrases: Computer vision, Co-saliency detection, graph matching model, graph
reconstruction, image understanding
1 INTRODUCTION
Visual saliency detection aims at automatically highlighting salient object(s) that attracts most attention for humans in a scene. Driven by its broad applications, such as image retrieval [54], visual
Fig. 1. Examples of detected co-saliency maps with different methods, including SACS [1], CBCS [8], IPCS
[19], IPSD [3], LDW [50], and IPDIM [49]. Results of the proposed method are shown as OURS, and GT
represents the ground truth.
tracking [21], object segmentation [20, 44], and video summarization [34], many computational
saliency detection models have been investigated in the past several years.
Most of existing saliency models [16, 28, 47, 51, 56] focus on detecting the salient object for
a single image and have achieved satisfactory performance on several public benchmarks. However, with the rapid growth of photo-sharing websites such as Instagram and Facebook, detecting
common salient objects from multiple images that share the same salient objects, i.e., co-saliency
detection, has been introduced in many computer vision tasks, including image co-segmentation
[15] and image matching [38]. Several methods, such as SACS [1], CBCS [8], IPCS [19], IPSD [3],
LDW [50], and IPDIM [49] have been proposed to solve the co-saliency detection problem. However, it is still challenging due to the complex scenarios, such as diverse illumination conditions,
different backgrounds, and different viewpoints in the practical group images. Figure 1 shows two
representative examples, where (b)–(g) are co-saliency maps generated by existing co-saliency
methods. For those methods, the unsupervised ones (i.e., (b)–(e) in Figure 1) generally compute
the co-saliency maps by feature matching [3, 8, 19] or multiple cues fusion [1, 24, 40] based on
visual appearance. However, due to lack of spatial correlation preservation as well as saliency coherence guidance in finding the co-salient regions, these methods obtain suboptimal performance
under the complex image contents, as shown in Figure 1. Several supervised methods (i.e., (f)–(g)
in Figure 1) get promotion of consistent salient results by metric learning [9], self-paced learning
[52], or deep learning [43, 49, 50, 55], considering spatial and appearance features jointly. However,
such learning-based methods require large-scale manual supervision in the form of pixel-level human annotations, which is highly labor-intensive and time-consuming.
More recently, several graphical methods [11, 27, 39] have been developed for better preserving
spatial consistency in co-saliency detection. Although showing a good compromise between the
performance and the annotation requirement, these methods have three major limitations when
exploring common salient regions. First, their graphical models only consider consistency measurement on node features and ignore the continuity of edge structures among graphs and, hence,
fail to preserve object details when computing the co-salient regions. Second, intra-saliency and
inter-saliency in these graphical models are detected individually, which is incapable of capturing well the intrinsic interactive relationship between common salient regions appearing across
images. Finally, to obtain a better performance, some post-processing techniques, such as selfpaced learning and CRFs, are used separately, reducing the ability of generalization in co-saliency
detection.
Motivated by the aforementioned limitations, we propose a novel unsupervised method that
effectively identifies the consistency of the co-salient object in a graph matching manner and
iteratively refines the co-saliency via a graph reconstruction optimization. More elaborately, the
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
Co-saliency Detection with Graph Matching 22:3
Fig. 2. Framework of the proposed co-saliency detection approach for image pairs. It contains two main
stages, one is the intra-saliency map generation with single saliency cues (i.e., (a), (b), (c)) color contrast,
spatial information, and connectivity cues. The other is multi-image saliency detection with graph matching
scheme (i.e., (d), (e)) Corresponding cues are extracted for intra-image at the initial graph matching stage
and the graph reconstruction stage, respectively.
proposed graph matching–based method detects common salient regions considering spatial structural continuity and saliency coherence for given images jointly and enables achieving better
performance in modeling pairwise relationships without annotation and post-processing requirement. The flowchart of our proposed method is shown in Figure 2. Specifically, we first organize
the regions of each image into a graph (where each node denotes one region) and compute the
initial intra-saliency for each image via weighted fusion of three types of cues, i.e., boundary prior
[56], contrast cues [4], and spatial cues [45]. Next, we cast the co-saliency detection as a graph
matching problem, where the common salient object for multiple graphs is identified via integrating feature correspondence, intra-saliency coherence, and spatial continuity simultaneously, and
the initial intra-saliency is iteratively refined via seeking optimal graph reconstruction. Finally, a
logistical fusion strategy is investigated to generate the final co-saliency map based on the graph
matching results, where a smooth regularization is embedded to constrain the coherence between
salient regions in the image pair. Some co-saliency maps produced by our proposed method are
shown in Figure 1. We can see that our method is capable of capturing the co-salient object entirely, especially in complex scenarios. For thorough evaluation, we collect a large and challenging
dataset, named iCom2017 dataset, for benchmarking co-saliency detection on pairwise images. We
then verify effectiveness of the proposed method on the pairwise image dataset: the Image pair
dataset [19] and the iCom2017 dataset. Experimental results demonstrate that our proposed cosaliency method significantly outperforms state-of-the-art co-saliency detection methods on both
datasets.
In summary, the major contribution of this article lies in the new graph matching–based method
for co-saliency detection and can be summarized by the following three aspects: (1) We propose a
graph-based framework to model the visual appearance, saliency coherence, and spatial structural
continuity simultaneously. Such a graph-based framework facilitates localization of corresponding
salient regions through graph matching; (2) We design a novel strategy for predicting co-salient
regions and saliency corresponding and introduce a graph reconstruction into graph matching for
iteratively refining the intra-saliency; (3) We rearrange a large scale of image pair dataset with an
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
22:4 Z. Li et al.
existing co-saliency detection database, which is used for evaluating visual co-saliency detection
algorithms in pairwise images.
The rest of this article is organized as follows: In Section 2, we summarize the related works of
our study. Section 3.1 describes the proposed framework and its main components. Afterwards,
details and implementation of the proposed method are discussed in Section 3; and then comprehensive experimental evaluations are presented in Section 4. Finally, the conclusion of this article
is depicted in Section 6.
2 RELATED WORK
2.1 Single-image Saliency Detection
Single-image saliency detection aims to automatically distinguish salient objects from the background in each image. Solving such relatively mature research area, a huge number of methods
[4, 10, 16, 26, 28, 31, 45–47, 51, 53, 56] have been proposed from different theoretical perspectives. Among which, graphical models [10, 26, 31, 46, 47, 53], by fully connected graph or sparse
graph, achieved a promising performance for single image saliency detection. These approaches
can handle well salient regions by considering color appearance, spatial structures, as well as various higher-level priors within an image. However, they were designed for the task of single image
detection and might fail when multiple common salient objects are presented with group images.
To mitigate the above issue, co-saliency detection [6] is expected to find out the common and
salient objects appearing in multiple images. Compared with traditional saliency detection, cosaliency detection is required to explore the consistent information among multiple images rather
than a single image. However, the appearance variations of common objects as well as the more
complex scenes across multiple images make co-saliency detection a more challenging task. By
exploiting co-occurrence patterns in a set of relevant images, existing co-saliency methods can be
roughly classified into unsupervised and supervised models.
2.2 Co-saliency Detection with Unsupervised Models
In the past several years, various traditional unsupervised co-saliency methods [1, 2, 8, 19, 24,
35, 40] have been presented to capture intra-image visual stimulus as well as inter-image correspondences. These methods usually discover co-occurrence salient regions via different bottom-up
principles, including feature matching and multiple cues fusion.
For a feature matching–based method, Chen et al. [2] detected the common salient objects in
pairwise images via a sparse distribution-based representation; however, only the low-level attention mechanisms were considered. A low-resolution co-saliency map was generated via this
method. Considering the multiple features in a pair of images, Li and Ngan et al. [18, 19] modeled
the co-saliency in a co-multiplayer graph manner and explored the similar regions using SimRank
algorithm. Following the SimRank, Tan et al. [35] estimated the consistent saliency using feature
similarities of image regions and generated co-saliency maps with pairwise SimRank theory. Recently, Ye et al. [48] proposed matching saliency region by propagating the intra-saliency in a
local and global manner, respectively. Different from the above methods, Fu et al. [8] built the
global corresponding relation by clustering with contrast cue, spatial cue, and corresponding cue.
As co-salient regions generally share high homogeneity in visual features, each of these feature
matching–based co-saliency detection methods works well for some images, but in the case of
practical complex image scenarios, they are hard to detect complete co-salient objects, as they fail
to take into account the continuity of spatial structures among co-salient objects.
The multiple cues fusion–based approaches aim to mine the co-saliency maps driven from single
and multiple saliency cues. Liu et al. [25] modeled corresponding saliency information by fusing
local and global similarities between pairwise images. Afterwards, Huang et al. [12] predicted
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
Co-saliency Detection with Graph Matching 22:5
co-saliency prior via a GMM-based model and generated co-saliency maps combining multi-scale
single-image saliency maps. Li et al. [24] designed a ranking model to obtain the co-saliency maps
guided by the initial intra-saliency as different queries. Rather than generating the similar information from the original image sets, Cao et al. [1] mined the consistency property by self-adaptive
weighted fusing the existing saliency maps. More recently, Tsai et al. [39, 40] detected co-salient
objects via adaptively fusing region-wise saliency maps generated from existing saliency methods.
Despite expected co-saliency results that could be achieved with these unsupervised fusion-based
methods, the performance might drop dramatically once most of the adopted (co-)saliency techniques lose their power.
2.3 Co-saliency Detection with Supervised Models
For promoting the performance of co-saliency detection, several supervised methods [9, 14, 39, 43,
49, 50, 52, 55] enable learning the consistency property by considering spatial and appearance features jointly. Typically, in [52], Zhang et al. proposed to learn the co-salient objects in an iterative
self-learning scheme and detected co-saliency via a self-paced multiple-instance learning method.
Recently, the development of deep learning technology has resulted in significant progress in cosaliency detection. For instance, Zhang et al. [50] introduced the deep convolutional features to
explore the regional similarities among multiple related images. Furthermore, in Reference [49],
Zhang et al. also proposed a transfer-learning-based method by exploiting stacked denoising autoencoders (SDAE) to learn intra-saliency prior and inter-image coherent foreground representations. Despite the impressive results achievable with these supervised methods, the separation of
feature extraction and co-saliency detection make the capture of co-salient regions costly. Recently,
Wei et al. in References [43] and [55] had proposed end-to-end learning-based methods using
fully convolutional network. Although the similar salient regions can be well learned with these
learning-based methods, the performance comes at the cost of requiring pixel-wise co-saliency annotations to generate training data, which is laborious and time-consuming to annotate pixel-wise
co-saliency labels in a group of images.
Rather than focusing on supervised methods, in this article, we propose a simple but effective
unsupervised graph matching–based framework inspired by the aforementioned observations. Although several graphical methods [11, 27, 39, 55] have been showing a good compromise between
the performance and the annotation requirement for co-saliency detection, they have a low capability in the complex scenarios and rely on some post-processing procedures, as depicted in
Section 1. In this article, the key idea is to design graph matching together with graph reconstruction optimization to solve the co-saliency problem. Visual appearance, spatial continuity, as well as
intra-saliency coherence of multiple images are collaboratively considered while processing graph
matching framework, which makes a better performance in modeling pairwise relationships without post-processing.
3 THE PROPOSED APPROACH
In this section, we describe the details of our proposed graph matching–based co-saliency detection and show how to iteratively output high-quality co-saliency maps with the help of graph
reconstruction.
3.1 Problem Formulation
Given an image pair {I1, I2}, we first segment each image into Nm (m = 1, 2) regions using the
regional segmentation method [7], where the ith region in the image Im is denoted as ri
m (i =
1, 2,..., Nm ). The objective of co-saliency detection is to automatically extract the common salient
objects in an image pair and simultaneously assign high saliency values to the salient regions in
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
22:6 Z. Li et al.
each image. Therefore, for image pair {I1, I2}, we propose to solve the problem of co-saliency
detection by maximizing the following energy function:
max
S1,S2

Ω(S1 |I1) + Ω(S2 |I2)

· L(S1, S2 |I1, I2)

,
s.t., 0N1  S1  IN1 , S1 ∈ RN1 ,
0N2  S2  IN2 , S2 ∈ RN2 , (1)
where Sm (m = 1, 2) is an Nm-dimensional saliency score vector. Each element in Sm denotes the
intra-saliency of the corresponding region in Im. Ω(·) measures the intra-saliency for each image,
and L(·) scales how consistent the saliency detection results are for an image pair. Since it is difficult
to directly solve the joint optimization problem in Equation (1), we initialize the intra-saliency
value Sm by weighted integrating multiple saliency cues first and then iteratively refine Sm with
the help of consistent measure function L(·).
3.2 Feature-based Intra-image Saliency Detection
Solving the optimization problem in Equation (1) requires the initial intra-saliency values S1 and S2
to be predicted at first. Some existing saliency detection methods [4, 47] can be deployed to solve
this problem. However, those methods measure saliency of each image only by exploring singlemodality saliency cue (i.e., color contrast) and do not perform so well due to lack of comprehensive
saliency information. In this article, inspired by References [56] and [16], we propose to compute
the intra-saliency by integrating multiple complementary saliency cues, including color contrast,
spatial proximity, and boundary connectivity with region entity. Formally, for each region ri
m in
image Im , its intra-saliency value is computed by learning to aggregate the involved saliency cues
as follows:
S (ri
m ) =

3
t=1
Ct (ri
m )ωt, (2)
where Ct (ri
m ) denotes the tth saliency cues of region ri
m, ωt is the corresponding learned weight
for the tth saliency cue.
To compute Ct (ri
m ) in Equation (2), three kinds of image features are first extracted for ri
m,
including the mean color in the lab space [13] of each member pixel ci = [li ai bi], the average of
all the element coordinates in an region vi = [xi yi], and the outer perimeter of each region prmi .
Next, we give the implementation details of each saliency cue Ct (ri
m ).
(1) Color contrast. As reported in Reference [16], a salient region usually shows high contrast against its contextual regions and background regions in color characteristics. Thus,
for each region ri
m, we first calculate its color histogram Hri
m (k) and then normalize such
color histogram with K
k=1 Hri
m (k) = 1. Note that K represents the total number of the
color quantized with all the color bins, and k describes the index in K. Then, the contrast
cue for region ri
m is defined as the chi-squared distance between current region and its
surrounding regions, i.e.,
Cϕ (ri
m ) = 1
2 | Bi
m |

K
k=1
Bi
m
r z
m=1
⎡
⎢
⎢
⎢
⎢
⎣
Hri
m (k) − Hr z
m (k)
Hri
m (k) + Hr z
m (k)
⎤
⎥
⎥
⎥
⎥
⎦
2
r z
m ∈ Bi
m, (3)
where Bi
m denotes the surrounding region set of ri
m, and | Bi
m | represents the number of
surrounding regions for ri
m.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.       
Co-saliency Detection with Graph Matching 22:7
(2) Spatial proximity. Recently, a series of works [4, 46, 53] have shown that salient objects
tend to appear near the center of an image and away from the image boundary. In this
article, we calculate the spatial proximity for ri
m as
Cφ (ri
m ) = e−γ ρ (ri
m )
, (4)
where γ is a constant to control the strength of the length of the path. We set γ = 0.5
empirically. ρ(ri
m ) denotes the center prior forri
m, which is denoted as the spatial position
Euclidean distance between ri
m and the center region Pm in image Im:
ρ(ri
m ) = ||vri
m − vPm ||
=

(xri
m − xPm )2 + (yri
m − yPm )2. (5)
(3) Connectivity. As demonstrated in Reference [45], the connectivity property of boundary regions is a good index to measure the salient regions. In this article, inspired by the
method in Reference [56], we first construct an adjacent graph by connecting all adjacent segmentation regions and then compute the similarity between connected node pair
(ri
m,r
j
m ) as
ϖ(ri
m,r
j
m ) =
⎧⎪
⎨
⎪
⎩
e
− ||ci −cj ||
σ 2 , Nдh (ri
m, r
j
m ) = 1, j  i,
0 , otherwise, (6)
where ||ci − cj || =

(li − lj)2 + (ai − aj)2 + (bi − bj)2 denotes the color distance between
nodes vi and vj . Nдh (ri
m, r
j
m ) = 1 represents that there is a connectivity between node i
and j. σ is a trade-off parameter that controls the strength of the weight.
Since the similarity ϖ(ri
m,r
j
m ) is defined by the color distance, the shortest path on the
adjacent graph essentially corresponds to the smoothest path on the image. Accordingly,
the connectivity can be obtained by computing the length of shortest smooth path that
satisfies the following optimization criteria:
Cτ (ri
m,r
j
m ) = |Path(ri
m,r
j
m )|,
= arg min
path(ri
m,rj
m )

Dm
rj
m=1
ϖ(rh
m,rh
m ),
s.t.
	
Nдh (rh
m, rh
m ) = 1,r
j
m ∈ Dm
h,h ∈ h = [i,... j] , (7)
wherepath(ri
m,r
j
m ) denotes the path meeting the above constraint.Dm denotes the boundary regions of image Im; h is an index vector, elements from which are the subscripts of
the regions on the path from ri
m to r
j
m.
Once completing each saliency cue, we generate the intra-saliency in Equation (2) by
setting the weight as ωt = 1 and fusing the saliency cues as
S (ri
m ) = Cϕ (ri
m ) × Cφ (ri
m ) × Cτ (ri
m,r
j
m ). (8)
3.3 Co-saliency Based on Graph Matching
To solve the co-saliency detection problem in Equation (1), we first compute the cross-image consistency score by evaluating the function L(·) and then iteratively refine the initial intra-saliency
based on the pairwise saliency information. Thus, the main problem in our work is to explore an
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019. 
22:8 Z. Li et al.
effective way to formulate the function L(·) in Equation (1). For simplicity, we define the consistency function L(·) to find the correspondence between a pair of images I1 and I2. A popular way
is to apply the feature matching algorithm (such as References [19, 27, 35]) to model L(·) based on
appearance feature similarity. However, as described in Section 1, due to lack of structure continuity inference and saliency coherence guidelines, these methods usually fail to preserve object
details when finding the co-saliency regions. In this article, we overcome this limitation by a graph
matching procedure that has proven a success in object tracking [36]. The region-level saliency
consistency is viewed as a graph matching [42] problem; both image properties and intra-saliency
coherence are integrated in the proposed graph-based framework. Instead of representing the image as the orderless collection of local regions, we represent the regions of each image as an undirected graph. Given the image graph G1 and G2, our goal is to find the optimal correspondence
between them and then estimate the co-saliency by iteratively refining initial intra-saliency S1
and S2 with the help of correspondence results. We describe such a proposed method in details as
follows:
3.3.1 The Graph Construction. An undirected graph of n nodes can be represented as G =
(V, E), where V = {v1,v2,...vn } is the set of nodes and E ⊆ V ×V denotes the undirected edges.
For each image Im with Nm regions in the image pair {Im }
2
m=1, we construct a graph Gm = (V, E)
as follows:
Node generation. In this article, we build the graph via modeling each local region as a node
and represent the nodes with feature descriptors extracted as in Section 3.2. Since there is oneto-one mapping between the nodes and local regions, the ith node in the graph Gm (i.e., vi
m) also
implies the region ri
m.
Edge generation. There are several methods for edge construction, one of them is to construct the fully connected graph. This graph contains comprehensive structure information and is
widely used in saliency detection [47], but it is not suitable for our co-saliency framework due to
its high costs on storage space and computational time. In this article, we adopt region adjacent
graph (RAG) to build the graph edges, because it not only saves storage space but also reduces the
complexity of the proposed algorithm.
For each image Im, we construct a graph model Gm using the same way and then formulate the
problem of L(·) in a graph matching framework.
3.3.2 Consistent Saliency Measurement. Given an image pair {I1, I2} with {N1, N2} regions, two
undirect graphs G1 = (V1, E1) and G2 = (V2, E2) are built to represent corresponding pairwise images. For Equation (1), we represent the correspondence between the intra-saliency S1 and S2 by
an affinity matrix X ∈ {0, 1}N1×N2 . Here, Xi,j = 1 means that the ith region ri
1 in the image I1 corresponds to the jth region r
j
2 in the other image I2, otherwise Xi,j = 0. The problem of saliency
consistency evaluation (i.e., computing maxS1,S2 L(S1, S2 |I1, I2) in Equation (1)) is formulated to
find an optimal affinity matrix X  that maximizes the consistent measure function L(X):
X  = arg max X
L(X),
s.t. X1N2  1N1 ,XT 1N1  1N2 , (9)
where 1N2 denotes all-one column vector of dimensionality N2. The constraints are imposed to
guarantee that each local region can be matched at most once.
Generally, Equation (9) can be solved by only adopting the feature matching. However, as discussed in Section 1, due to complex scenarios (i.e., diverse illumination conditions, complex backgrounds, and viewpoint variations) in realistic images, results are often unreliable when simply
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.    
Co-saliency Detection with Graph Matching 22:9
considering the appearance information among corresponding regions. In this article, we propose to incorporate structural spatial continuity, visual appearance, and saliency coherence from
pairwise images and reformulate the consistency function L(X) in terms of the following global
consistency:
X  = arg max X
L(X),
=

i,t
ci,tXi,t +

i,j,t,h
di,j,t,hXi,tXj,h, (10)
where ci,t measures the consistency between the ith node in G1 and the tth node in G2. di,j,t,h
denotes the coherence between the edge Ei,j in G1 and the edge Et,h in G2. The correspondence
matrix X is a {0,1}-valued matrix with size of N1 × N2, which defines the matching results between
two graphs. Here, Xi,t = 1 if and only if vi
m ∈ V1 corresponds to vt
m ∈ V2.
As for the co-saliency detection task, the corresponding information measured by L(·) is viewed
as a pairwise geometric relationship between the matched regions. Both image properties and
saliency cues in the corresponding salient regions play an essential role to guide the matching
result. In this article, we simultaneously combine the appearance, structural information, and intrasaliency information into graph matching and effectively measure the consistency for an image
pair via proposed graph matching framework. Intuitively, the function L(·) in Equation (10) is
specified as
L(X) = tr(CTX) + α ||A1 − XA2XT ||2
F ,
s.t. X = (Xij) = {0, 1},

N
j=1
Xij = 1,∀i, j. (11)
Here, α ≥ 0 is the weight parameter to balance the comparisons between nodes and edges; || · ||F
is the Frobenius norm; C = (ci,t ) ∈ RN1×N2 is the node consistency matrix. Different from other
methods that simply learn the matching between graph nodes with locally regional features, here
we take visual appearance and intrinsic saliency into consideration. In particular, we are interested
in studying the node consistency between two graphs under regional features and intra-saliency
values. Therefore, element ci,t in the node consistency matrix C of Equation (11) is defined as
ci,t =

1 − 





S (r 1
i ) − S (r 2
t )





2

exp
−χ2 (fi,ft )

,
=

1 − 





S (r 1
i ) − S (r 2
t )





2

exp 

−1
2

K
k=1

Hi (k) − Ht (k)
Hi (k) + Ht (k)
 2


, (12)
where f = [c, v,prm], and its elements fi and ft are appearance descriptors of vi
1 and vt
2, respectively. Hi (k), K, and k are similar to the definition in Equation (3). χ2 (fi,ft ) is the chi-square
feature distance between vi
1 and vt
2. Consistency score ci,t will be large enough when two matches
vi
1 and vt
2 indicate similar appearance and intrinsic saliency, which encourages a high possibility
for ri
1 and rt
2 to be co-saliency regions. In contrast, the opposite counter would appear with a low
potential for ri
1 and rt
2 to be co-salient.
In Equation (11), Am = (Aij) ∈ RNm×Nm is an affinity matrix encoding edge information, which
represents the spatial structural relation of edges inside the graph Gm. For each node pair (i, j) in
a graph, we define element Aij in Am as
Aij = exp
−||vi − vj ||
, (13)
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.     
22:10 Z. Li et al.
where vi and vj denote the spatial coordinates of nodesvi
m andvj
m, respectively. In particular, such
a proposed approach in Equation (11) performs graph matching by taking both image properties
and intra-saliency into consideration, which generally provides robust and accurate results for
co-saliency detection.
3.3.3 Optimization. As the variable X in Equation (11) takes integer values, it is difficult to
solve this NP-hard problem and no known efficient algorithms offer global optimal guarantee.
In the general case, existing methods, such as the ones in References [5, 17, 33, 37, 41], usually
search a path formed by local minima of some interpolated and relaxed formulations, and solve
the problem of Equation (11) by relaxing some hard constraints. Among the algorithms for graph
matching, the Fast computation of Bipartite graph matching (FBP) [33] algorithm achieves a good
balance in terms of matching accuracy and computational efficiency. Thus, we adopt FBP [33] as
a component to solve the problem in Equation (11).
Fast computation of bipartite graph matching (FBP). FBP defines the edit cost as a distance
function. It initially implements the Munkres [29] algorithm to find the permutation of a quadratic
matrix and then computes the minimum edit distance through this permutation matrix. For two
graphs G1 and G2 with size of N1 and N2, respectively, the matching result X can be obtained from
the permutation of minimum edit cost. Computation details of FBP are presented in Algorithm 1.
ALGORITHM 1: Fast computation of Bipartite (FBP)
CQ1 =Computation_Cost(G1,G2)
// C is the initial cost matrix.
// CQ1 is N1 × N2 first quadrant of cost matrix C
.
P =Munkres(CQ1
)
// P is the N1 × N2 permutation matrix.
EditCost =Sum(Sum(P. ∗ CQ1
))
// .∗ represents the multiplication element by element.
EditCost =EditCost + Cve_d i
// Cve_d i is the edit cost that focuses on the costs of deleting nodes and edges from G1 and inserting
nodes and edges from G2.
X =Munkres(EditCost
)
// final N1 × N2 permutation matrix generated by Munkres algorithm.
End
3.3.4 Co-saliency Prediction with Region Corresponding. For two regions ri
1 and rt
2 in an image
pair {I1, I2}, after obtaining the initial intra-saliency Si
1, St
2 and the matching result Xi,t = 1, we
first predict the inter-saliency Qi,t for such consistency region as
Qi,t =

N2
t=1
I(Xi,t  0)(Si
1 + St
2 )ζ (Xi,t ), (14)
where I(·) is an indicator function that outputs 1 when statement in the I(·) is true, and 0 otherwise.
ζ (x) is a fitting function that measures the common saliency value in two corresponding salient
regions.
To predict the inter-saliency among consistent regions, we need to retain the matched foreground regions close to high values and ensure the matched background regions close to lower
or even zero saliency values. Unlike the existing methods that estimate co-saliency with linearly
combining visual similarities, we adopt a logistical fusion strategy to capture the co-salient regions
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019. 
Co-saliency Detection with Graph Matching 22:11
with corresponding cues as
ζ (Xi,t ) = M
1 + e−a((Si
1+St
2 )Xi, t −b)
, (15)
where the constant a shows power of the steepness in intra-saliency of pairwise images, b is the
mid-value of the sum of two intra-saliency values, and M is the parameter that controls the max
inter-saliency value when intra-saliency value of each image reaches maximum. Such logistical
function takes into consideration the overall agreements between the corresponding salient regions and outputs a series of smooth values to fit the inter-saliency results.
Once obtaining the intra- and inter-saliency values for each region, we generate the co-saliency
map by refining the initial intra-saliency iteratively. Meanwhile, considering the fact that if two
adjacent image regions are similar in terms of their visual appearance, then their saliency values
should be close to each other, and vice versa. Motivated by this, we introduce a smoothness term
to ensure that the co-saliency map is uniform in the flat regions of each image. Specifically, for
each region ri
1 in an image pair {I1, I2}, we calculate its co-saliency value Ti
1 by refining the intrasaliency value Si
1 as
Ti
1 =

N1
j=1,ij
Si
1 · Qi,t + λ||Si
1 − Sj
1 ||2
2 ϖij . (16)
Here, the indexes of i and j meet the condition of Nдh (ri
1, r
j
1 ) = 1, which represents that there is
a connectivity between region ri
1 and r
j
1; λ is the positive tradeoff parameter to ensure the local
smoothness of co-saliency map, and ϖij is defined in Equation (6).
3.4 Co-saliency Refinement with Graph Reconstruction
At the previous stage, we predict the co-saliency map by refining the initial intra-saliency with
logistical fusion strategy. Even though we are able to detect the major salient regions, the irrelevant
noisy regions may not be entirely eliminated. We propose to further reconstruct the original graph
by removing unreliable nodes (i.e., irrelevant noise regions) and gradually refine the initial intrasaliency to predict final co-saliency values.
For each graph Gm, we first remove the node set Um exploiting the current co-saliency constraints by
Um =
	
{vi
m }, Ti
m ≤ η ∗ mean(T1 +T2)
∅, otherwise, (17)
where ∅ represents an empty set. Each element in ∅ means that the current node vi
m belongs to the
potential common salient region; mean(·) is a mean function that defines the mean saliency over
the previous intra-saliency maps; η > 0 controls the extraction of the unreliable matching regions.
Then the intra-saliency of each region ri
m is updated as
Si
m =
	
0, vi
m ∈ Um
Ti
m, otherwise;
(18)
and the node consistency matrix C is condensed by removing the corresponding row and column for each node vi
m ∈ Um. Similarly, we reduce the corresponding matrix Am by removing the
edges connected to node set Um. After removing the redundant nodes and edges, we reconstruct
the original graph and re-predict the matching result X using the proposed graph matching framework. The final co-saliency map is estimated by refining the original intra-saliency gradually with
the help of matching result X. This refinement is iterated until the best performance is achieved.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019. 
22:12 Z. Li et al.
Fig. 3. Co-saliency maps and mean absolute errors (the lower, the better) with numbers of iterations (i.e.,
iter): (a) Original image; (b) Initial intra-saliency map (MAE = 0.1426); (c) iter = 1 (MAE = 0.1382); (d) iter =
2 (MAE = 0.1207); and (e) iter = 3 (MAE = 0.1067).
Figure 3 shows comparison of some co-saliency maps with graph reconstruction strategy. We
can see that the common saliency regions and irrelevant noise regions are distinguished gradually
with intra-saliency iteration, indicting the effectiveness of graph reconstruction strategy.
4 EXPERIMENTS
4.1 Experimental Settings
Datasets. We evaluate the performance of the proposed algorithm on the widely used image
pair dataset [19]. This dataset consists of simply 105 image pairs with a total of 210 images, which
has become less challenging for co-saliency detection. To facilitate research and evaluation of cosaliency models on image pairs, we collect a large-scale dataset where multiple co-salient objects
and complex scenarios are composed. This dataset is named iCom2017 dataset, in which 1,000 image
pairs (i.e., 2,000 images) and their corresponding labeled ground truth data are collected from two
group image datasets: iCOSEG [30] and Cosal2015 [50]. Images in iCom2017 dataset contain one
or more common salient objects with complex image scenarios, including different shapes, diverse
illumination conditions, views, or complex backgrounds, which is a great challenge to discover the
useful information among the image pairs.
Comparative Models. In our experiment, we compared the proposed algorithm with classical and recently proposed state-of-the-art co-saliency detection approaches, including IPCS [19],
CBCS [8], CSHS [25], IPSD [3], SACS [1], IPDIM [49], and LDW [50]. Note that, since neither cosaliency maps nor codes are provided by the authors, LDW on image pair dataset and IPDIM on
iCom2017 dataset are not evaluated in our experiments.
Evaluation Details. For performance evaluation, the proposed algorithm is measured with
some widely used criteria: precision, recall, F-measure, mean absolute (MAE), and overlapping
ratio (OR).
Given a continuous co-saliency map S and its corresponding binary ground truthG, their precision and recall are computed by comparing each binary mask S withG. Details of their definitions
are described as
precision = | S ∩ G |
| S | ,recall = | S ∩ G |
| G | , (19)
where S is obtained by binarizing S using a certain threshold varying from 0 to 255.
In particular, the precision-recall (PR) curve is usually plotted with the mean precision and recall
values under a certain threshold—the higher the curve fitting, the better for the performance of
co-saliency detection. Thus, to reflect the high performance of co-saliency detection intuitively,
F-measure is put forward to indicate the weighted harmonic of precision and recall, which is
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
Co-saliency Detection with Graph Matching 22:13
Table 1. Parameter Setting of η at Each Iteration
Number of iterations 12 3
Filter factor η 1 0.5 0.2
defined as follows:
Fβ = (1 + β2) × precision × recall
β2 × precision × recall , (20)
where β2 is set to be 0.3 to stress precision more than recall.
Additionally, considering the effect of correct assignment for non-salient pixels, we also introduce the MAE and OR metrics. MAE [32] is defined as the mean absolute difference between S
and G : MAE= mean(|S − G|); OR [22] is defined as the overlapping ratio between the segmented
object mask S and ground truth G: OR= |S ∩ G|/|S ∪ G|.
In summary, these evaluation metrics reflect the performance of algorithms from different viewpoints, which are universally agreed and easy to understand in co-saliency detection.
4.2 Parameter Setting
In our experiments, the parameters σ in Equation (6) and α in Equation (11) are set to 1 empirically. The local propagation λ in Equation (16) is set to 0.3. Besides these parameters, we observe
that performance of co-saliency detection is sensitive to the parameter η in Equation (17). Thus,
we analyze the relationship between the factor η and iteration numbers and set values of these
two parameters following Table 1. From the analysis to the experiments, we discover that best
performance of co-saliency is reached at iter = 3, which demonstrates a fast convergence of our
approach.
4.3 Overall Performance Comparison
We report both qualitative and quantitative comparisons of our approach with state-of-the-art
approaches.
Qualitative comparison. Figures 4 and 5 provide visual comparison of different methods on
two datasets. As can be seen, when co-salient objects appear in different shapes (i.e., the first and
fourth image pairs in the image pair dataset) or touch the image borders (i.e., the second and fifth
image pairs in the iCom2017 dataset) or occupy a tiny area (i.e., the last image pair in the iCom2017
dataset), most baseline methods do not generate full-resolution co-saliency maps; moreover, the
noisy non-co-salient regions are also detected with baseline methods. Our approach (OURS) comprehensively solves the aforementioned drawbacks and produces co-saliency maps that are consistent with ground truth. Furthermore, our approach can deal well with the challenging cases
where the background is cluttered (i.e., the first and last pairwise images in the iCom2017 dataset,
the last pairwise image in the image pair dataset). These visual results clearly demonstrate the
superiority of the designed graph matching framework in co-saliency detection.
However, an interesting thing is that IPCS [19] generates a coarse co-saliency map in complex image scenarios. This is mainly because IPCS [19] obtains the corresponding information via
simple feature matching and generates the co-saliency by fusing the intra- and inter-saliency linearly. By contrast, our method performs graph matching by considering the interaction of visual
appearance, structure continuity, and intrinsic saliency cue; it effectively suppresses the complex
background and gradually refines the co-saliency map. In addition, thanks to the non-linear fusion
strategy, the smoothing common saliency maps are accurately highlighted.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
22:14 Z. Li et al.
Fig. 4. Visual comparison of co-saliency maps on the image pair dataset. From top down: original image
pairs, the corresponding ground truth, co-saliency maps generated by our proposed method, and SACS [1],
IPSD [3], CSHS [25], IPDIM [49], CBCS [8], and IPCS [19]. Our method consistently produces co-saliency
maps closest to the ground truth.
Fig. 5. Visual results between our proposed method and the baseline methods on the iCom2017 dataset.
From top down: original image pairs, the corresponding ground truth, co-saliency maps generated by our
proposed method, and SACS [1], IPSD [3], CSHS [25], LDW [50], CBCS [8], and IPCS [19]. Our final result
focuses on the main co-salient object as shown in the ground truth map.
Quantitative comparison. To provide quantitative comparisons, we first plot the PR and Fmeasure curves for each baseline approach and then calculate the average scores of precision,
recall, and F-measure on the two datasets. The quantitative comparison is shown in Figure 6 and
Table 2. As can be seen, our approach (OURS) consistently outperforms the baseline methods in
terms of PR and F-measure curves and even all the evaluation scores.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
Co-saliency Detection with Graph Matching 22:15
Fig. 6. Comparison of quantitative results, including PR curves, F-measure curves, and averages of precision,
recall, as well as F-measure, on the image pair (i.e., the first row) and iCom2017 (i.e., the second row) datasets.
The proposed method consistently outperforms other methods across all the testing datasets.
Table 2. The Performance (MAE, maxF, and OR) of Baseline Methods
on the Image Pair and iCom2017 Datasets
Dataset Metrics
Methods
SASC IPSD CSHS IPDIM LDW CBCS IPCS OURS
MAE 0.1687 0.2094 0.1472 0.2561 – 0.1564 0.1734 0.1411
Image Pair maxF 0.8571 0.5548 0.8559 0.8671 – 0.7993 0.7612 0.8958
OR 0.6231 0.4114 0.5438 0.5690 – 0.5820 0.5497 0.6713
MAE 0.1612 0.1984 0.1722 – 0.1701 0.1635 0.3389 0.1270
iCom2017 maxF 0.7847 0.4909 0.7639 – 0.7929 0.7203 0.4510 0.8021
OR 0.5801 0.3541 0.5689 – 0.5682 0.5289 0.2308 0.6232
The best and second best results are shown in bold font.
Specifically, according to PR and F-measure curves in Figure 6, our method is capable of achieving the highest precision/recall rates in the widest ranges of recall/false positive rates; it also obtains the highest AP, AR, and average F-measure scores on the image pair dataset. Similarly, on the
iCom2017 dataset, our algorithm achieves a max of 93% precision rate with high recall, which performs favorably compared with all the other algorithms. For the F-measure curves, performance in
Figure 6 reflects that our method still yields the best performance than the state-of-the-art methods. In addition, although some recently proposed models such as SACS [1] and LDW [50] perform
slightly close to our model in terms of PR curves, their max scores of F-measure are slightly worse
than our method. Meanwhile, when comparing the proposed method with the recent learningbased methods (i.e., IPDIM [49], LDW [50]), our method congruously shows the highest PR curve
and F-measure curve and F-measure scores on two datasets, which significantly demonstrates the
effectiveness and robustness of the proposed method.
Table 2 lists results of MAE, maxF, and OR metrics derived from our method and the baseline
methods. Our method exhibits the best performance with respect to these metrics. Specifically,
on the iCom2017 dataset, the recently proposed method of LDW [50] and the classical method
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
22:16 Z. Li et al.
Table 3. Quantitative Evaluations of Co-saliency Results with Different Initial Intra Saliency Methods
on the iCom2017 Dataset and Image Pair Dataset
iCom2017 dataset image pair dataset
Type maxF MAE OR Type maxF MAE OR
DRFI intra saliency 0.7802 0.1506 0.5832 intra saliency 0.8543 0.1671 0.6438
co-saliency 0.8019 0.1301 0.6218 co-saliency 0.8887 0.1423 0.6702
RBD intra saliency 0.7629 0.1612 0.5730 intra saliency 0.8417 0.1702 0.6306
co-saliency 0.7958 0.1306 0.6149 co-saliency 0.8843 0.1478 0.6632
GBMR intra saliency 0.7403 0.1786 0.5779 intra saliency 0.8503 0.1748 0.6271
co-saliency 0.8015 0.1312 0.6219 co-saliency 0.8812 0.1502 0.6657
GS intra saliency 0.7346 0.2042 0.5432 intra saliency 0.8376 0.1978 0.5842
co-saliency 0.7994 0.1297 0.6174 co-saliency 0.8796 0.1462 0.6678
RC intra saliency 0.7104 0.2504 0.4817 intra saliency 0.7623 0.2173 0.4574
co-saliency 0.7912 0.1346 0.6096 co-saliency 0.8646 0.1574 0.6532
DSR intra saliency 0.7277 0.2273 0.5012 intra saliency 0.7854 0.2042 0.5018
co-saliency 0.7967 0.1322 0.6150 co-saliency 0.8702 0.1493 0.6601
Proposed intra saliency 0.7798 0.1623 0.6004 intra saliency 0.8514 0.1707 0.6512
co-saliency 0.8021 0.1270 0.6232 co-saliency 0.8958 0.1411 0.6713
The bold fonts show the best and second best results in term of comparative metric.
of SACS [1] are the top performers. LDW [50] has the highest maxF score (0.7929) and OR score
(0.5689), and SACS [1] has the lowest MAE score (0.1612). However, our method outperforms these
two algorithms with maxF = 0.8021, OR = 0.6232, and MAE = 0.1270, which are 0.92%, 0.54%,
and 0.34% better than LDW [50], SACS [1], respectively. Similarly, for the image pair dataset, our
method achieves a maxF of 0.8958 and an MAE of 0.1411, which is higher than that of the bestperforming baseline method (i.e., IPDIM [50]) with a corresponding margin of 0.0287 and 0.0061,
respectively. These results clearly illustrate the strong effectiveness and robustness of our method.
4.4 Illustration of Initial Intra-Saliency
In this article, we focus on detecting co-saliency by iteratively refining the initial intra-saliency
in a graph matching manner. Therefore, the intra-saliency acts as an initial value, and the overall performance of the proposed method is not sensitive to the initial intra-saliency. To demonstrate such insensitivity of the initial intra-saliency, we adopt the proposed intra-saliency detection method and traditional top six state-of-the-art single saliency methods, including DRFI
[16], RBD [56], GBMR [47], GS [45], RC [4], and DSR [23] as initial intra-saliency separately, and
compare their intra-saliency and co-saliency performance based on maxF and OR values on two
datasets. Quantitative measures including the maxF, MAE, and OR scores are reported in Table 3.
From this table, regarding initial intra-saliency produced by any existing method, the maxF scores
are improved through the proposed co-saliency method, and the MAE scores as well as the OR
scores also achieve better performances compared with the previous intra-saliency results. These
results clearly demonstrate the proposed model can effectively improve the performance of the
initial intra-saliency. For each dataset, when using different saliency methods as the initialized
intra-saliency, the proposed method shows relatively stable co-saliency results, demonstrating the
insensitivity of initial intra-saliency for performing the proposed co-saliency model. However,
performance with the proposed initialization method well outperforms other ones, which clearly
illustrates the effectiveness of the proposed framework by using multi-cue fusion method as the
initialization.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
Co-saliency Detection with Graph Matching 22:17
Fig. 7. Some examples of detected saliency maps from different methods. The first column is the original
images, followed by RC [45], RBD [56], DSR [23], GBMR [47], GS [45], DRFI [16], and our proposed multiple
cues fusion method. The ground truth is provided in the last column.
In our experiment, to illustrate the effectiveness of the proposed feature-based intra-saliency detection method, we further compare the proposed method with the aforementioned single saliency
detection methods qualitatively. Some visual comparisons of different methods are illustrated in
Figure 7. We can clearly see that even though the images own complex backgrounds or the salient
objects exhibit tiny shapes, the proposed intra-saliency detection method effectively highlights
salient objects, and it also produces more accurate saliency maps than other methods.
4.5 Graph Reconstruction and Convergence Analysis
To illustrate the effectiveness of iterative solution in our method, we first investigate the contribution of graph reconstruction for co-saliency refinement and then analize the convergence of the
proposed method based on the graph reconstruction procedure. These experiments are performed
on the iCom2017 dataset. The reason is that iCom2017 dataset contains more image pairs that can
be used for more comprehensive analysis.
Graph reconstruction is proposed to iteratively refine the initial saliency. To demonstrate its
effectiveness, at each iteration, we set η varying from 0.1 to 1 and investigate the overall performance based on the MAE, F-measure values. Figure 8 reports the comparison results from different
parameter settings. When the iteration number of graph matching increases, a better performance
is achieved with a small η; that means with the deletion of unreliable nodes for each iteration, the
irrelevant noisy regions are eliminated gradually, and more common objects are precisely matched
through the strategy of graph reconstruction.
With graph reconstruction in the proposed graph matching–based method, the final co-saliency
maps are predicted in an iteration manner. Thus, the convergence of the proposed method is mainly
determined by the iteration numbers in graph reconstruction. Results of the last column in Figure 8
further illustrate such convergence. We can clearly see that our method can converge to a better
performance when iterating three times, demonstrating that the proposed iteration solution not
only converges quickly but also helps to refine the initial intra-saliency effectively.
4.6 Contribution of Structural Spatial Information
It is also worth illustrating the contribution of structural spatial information in our work. As defined in Equation (11), α controls the balance of visual nodes and structure edges consistency in the
proposed method. Thus, we set α varying from 0 to 2.0 on two datasets and analyzed the effects
of structural information to the overall performance based on MAE, OR, and F-measure values.
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
22:18 Z. Li et al.
Fig. 8. Illustration of the co-saliency refinement based on iteration number and graph reconstruction among
corresponding regions.
Table 4. The Performance (MAE, maxF, and OR) with Different Values of α
on the Image Pair and iCom2017 Datasets
Dataset Metrics
Values of α
α = 0 α = 0.1 α = 0.2 α = 0.5 α = 0.8 α = 1 α = 1.5 α = 2
MAE 0.2042 0.1524 0.1476 0.1439 0.1423 0.1411 0.1503 0.1547
Image Pair maxF 0.7522 0.8698 0.8703 0.8725 0.8854 0.8958 0.8524 0.8049
OR 0.4823 0.6412 0.6489 0.6503 0.6602 0.6713 0.6532 0.6417
MAE 0.5456 0.1402 0.1398 0.1346 0.1298 0.1301 0.1304 0.1389
iCom2017 maxF 0.6523 0.7904 0.7987 0.8017 0.8019 0.8021 0.7854 0.7822
OR 0.2012 0.6989 0.6197 0.6204 0.6217 0.6232 0.6103 0.6149
Best co-saliency results can be achieved at α = 1, and we empirically set α as 1 in our work.
Table 5. Average Processing Time(s) for Each Image Using State-of-the-art Methods
SASC IPSD CSHS IPDIM LDW CBCS IPCS OURS
Image pair 34.8 43.28 15.73 – – 1.65 468.12 14.32
iCom2017 61.7 64.32 29.26 – – 2.32 543.81 28.96
As shown in Table 4, when α = 0, the performance is the least promising, which verified the contribution of structural information in our work. Moreover, we observe that when α ranges from
0.2 to 1.5, the performance of our work stays relatively stable, which illustrates that structural
information is one of the key elements to perform the proposed graph matching framework.
4.7 Computational Cost and Runtime Comparison
We also analyze the computational time on two datasets to further illustrate the advantages of the
proposed algorithm. Given an image pair, the average processing time for each image over different
approaches is listed in Table 5. In this article, we implement the experiments with source code
provided by the authors and uniformly perform the models using the Matlab2015b environment
on an Intel(R) Core(TM) i5 CPU @ 3.30GHz. As is shown in Table 5, computational cost of our
method is inferior to CBCS [8]; the CBCS [8] has the highest computational efficiency. This is
mostly because more uncertain foreground salient regions would be selected at the first iteration of
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
Co-saliency Detection with Graph Matching 22:19
Fig. 9. Some co-saliency maps in group images generated by our method. The odd rows are original images,
and the even rows are co-saliency maps detected by cross-match operation on any pairwise images.
the proposed graph matching framework. But at the latter iterative procedure, our method exhibits
high computational cost with removing the irrelevant noisy regions gradually. To this end, for cosaliency detection, the proposed algorithm has the moderate computational complexity with a
better performance.
5 DISCUSSION
Our method solves co-saliency problem via graph matching, and it is mainly focusing on the pairwise images co-saliency detection. It is worth noting that the proposed approach is also applicable to detecting co-salient regions in group images when we perform cross-wise graph matching
among any two images. To achieve this, we first fix an image and then perform pairwise graph
matching as well as graph reconstruction with the fixed image and the other remaining image in
the current image group. Thus, for the fixed image, a set of co-saliency maps can be obtained after operation of cross-wise graph matching, and its final co-saliency is computed by averaging of
these cross-wise co-saliency maps. Figure 9 shows some examples of cross-wise graph matching
within group images. Our method is capable of uniformly highlighting the group co-salient regions
by a cross-match procedure. However, due to the fact that the computation time of the proposed
method is determined by the candidate matches in graph matching, it may be time-consuming for
detecting group images with our method. For example, to obtain salient objects “fish” in Figure 9,
we will do pairwise graph matching approximately ten times, which seems less efficient compared
with pairwise image co-saliency detection.
Besides, for pairwise image co-saliency, our method exploits a graph reconstruction scheme to
suppress irrelevant noisy regions; thus, it may be difficult to reserve very precise co-salient objects
with distinctive appearances, as shown in the second and fourth pairwise images in Figure 5. The
underlying reason is that those un-salient regions are filtered determined by weighting threshold
method, and this may not be enough to control a uniform saliency threshold to support graph
reconstruction. Thus, exploring a more effective and automatic threshold generation method may
be the future elaboration to alleviate the above problem.
6 CONCLUSION
In this article, we proposed a novel graph matching–based computational framework for cosaliency detection. By introducing a new method of undirected graphs, we first computed the initial intra-saliency via combining multiple saliency cues and then modeled the consistent saliency
as geometric graph matching problems, which adopted a matching prediction strategy to flexibly
optimize the graph matching. Finally, the co-saliency maps were generated by iteratively refining
the initial intra-saliency with graph reconstruction optimization. Comprehensive evaluations over
ACM Transactions on Intelligent Systems and Technology, Vol. 10, No. 3, Article 22. Publication date: April 2019.
22:20 Z. Li et al.
several benchmarks revealed that graph matching–based co-saliency detection method gained superior performance compared to state-of-the-art approaches.