Traditional recommender systems (RS) only consider homogeneous data and cannot fully model heterogeneous information of complex objects and relations. Recent advances in the study of Heterogeneous Information Network (HIN) have shed some light on how to leverage heterogeneous information in RS. However, existing HIN-based recommendation models assume HIN is invariable and merely use HIN as a data source for assisting recommendation, which limits their performance. In this paper, we propose a multi-task learning framework, called MTRec, for recommendation over HIN. MTRec relies on self-attention mechanism to learn the semantics of meta-paths in HIN and jointly optimizes the tasks of both recommendation and link prediction. Using a Bayesian task weight learner, MTRec is able to achieve the balance of two tasks during optimization automatically. Moreover, MTRec provides good interpretabilities of recommendation through a “translation” mechanism which is used to model the three-way interactions among users, items and the meta-paths connecting them. Experimental results demonstrate the superiority of MTRec over state-of-the-art HIN-based recommendation models, and the case studies we provide illustrate that MTRec enhances the explainability of RS.

SECTION 1Introduction
Traditional recommender systems (RS) rely on collaborative filtering (CF) methods, especially matrix factorization (MF) [1], to characterize the historical user-item interactions (i.e., ratings) and offer recommendations. However, CF-based approaches suffer from cold-start and data sparsity problems. The system lacks rich interaction information for many users/items, which downgrades its performance. With the development of web services, various types of data (e.g., text, image, location and social network) become available in RS [1] and researchers start to consider incorporating such auxiliary information into recommendation models in order to alleviate the cold-start and data sparsity problems. On the other hand, the rich side information raises new challenges for RS. Because of the heterogeneous nature of the real world, auxiliary data typically contains different objects with multiple types of interactions among them. Traditional recommendation approaches only consider homogeneous data and cannot fully model heterogeneous information in modern RS.

Due to the importance of heterogeneity in real-world applications, researchers have paid extensive attention to the study of heterogeneous information network (HIN). In HIN, objects are of different types, and links among objects represent different relations (i.e., interactions). Based on the fusion of heterogeneous information from HIN, various data mining tasks can be exploited, e.g., similarity search, clustering, classification and link prediction [2]. Recent advances in the study of HIN [2] have also shed some light on how to utilize heterogeneous data in RS and HIN-based RS have attracted much attention [3], [4], [5], [6], [7]. To better depict user preferences and item properties in RS, HIN-based recommendation approaches learn the semantic relations behind different types of interactions using meta-path [8] or meta-graph [9] (also called meta-structure [10]). The advantages of HIN-based recommendation models over traditional methods are twofold. First, HIN-based recommendation methods are able to model the complex interactions between different objects. Characterizing and using such information is the key to improve the quality of recommendation since interactions among different objects affect users’ behaviors, especially when users make purchasing decisions. Moreover, HIN-based recommendation approaches typically harness similarities based on meta-path/meta-graph [11] or the attention mechanism [6] to distinguish different parts of HIN (e.g., path or subgraph) according to their importance to users’ behaviors. Consequently, HIN-based recommendation approaches provide a way to understand which piece of information affects recommendation results and are of great value to Explainable Recommender Systems [12].

Although HIN-based approaches have exceeded traditional models on both performance and interpretability, there are still some limitations of current HIN-based RS. A tremendous amount of work relies on leveraging meta-path/meta-graph based similarity [3], [11], [13], [14], [15] to improve the quality of recommendation. To obtain such similarity, HIN has to be represented as a graph structure, and then substructures like meta-path or meta-graph can be extracted from it, for the computation of the similarity. However, the high computational complexity [16] of similarity computation hinders these methods from being fully deployed in real large-scale HIN [7]. A compromise (e.g., limiting the length of meta-path [8]) has to be struck at the sacrifice of inaccurate similarity. Moreover, similarity based on meta-path/meta-graph may not fully reflect the latent features of users and items in HIN [7].

With the surge of research on network embeddings [16], some researchers start to consider utilizing node embeddings to overcome the limitations of similarity-based methods. Embedding based methods efficiently represent HIN in a low-dimensional space while preserving the semantic meanings. The node embeddings are either learned independently for subsequent use in MF [7] or constitute the representations in a neural network based model [6]. However, embedding based methods also have their limitations: (1) HIN is supposed to be invariable in these methods and the information flow between HIN and the recommendation module is monodirectional. In other words, HIN will not get updated in the lifecycle of RS, and it is solely used as a data source for assisting recommendation. (2) Moreover, HIN is keeping growing. The increase of the data in RS brings new user-item interactions and item-item relations to HIN. The enhanced HIN can further improve the recommendation. Therefore, there exists a mutual effect between HIN and recommendation. Nevertheless, existing approaches merely optimize recommendations, ignoring that there are other relevant tasks (e.g., modeling the dynamic growth of HIN) and learning multiple tasks jointly may further improve the generalization performance of all the tasks.

Aiming at solving the issues above, we propose a Multi-Task learning framework for RECommendation over HIN (abridged as MTRec) in this paper. Our contributions are summarized as follows:

We design a meta-path based recommendation model which mostly relies on the attention mechanism. By utilizing self-attention, the importance of each ingredient in the meta-path can be identified during the fine-grained learning.

We model the meta-path as the “translation” from a user to an item in a heterogeneous way to depict the three-way interactions among users, items and the meta-paths connecting them. The translation based three-way modeling improves not only the performance but also the interpretability of the recommendation.

For the first time, multi-task learning is introduced into HIN-based RS. We select link prediction, one task for modeling the dynamics of the network, as the auxiliary task and design a multi-task learning framework which optimizes both the recommendation task and the link prediction task in HIN.

The balance of main and auxiliary tasks is automatically achieved using a Bayesian task weight learner during joint optimization. The Bayesian task weight learner can benefit not only the HIN-based recommendation task in this paper but also other tasks involving multi-task learning and negative sampling.

We conduct extensive experiments on real public data sets which show that MTRec has a superior performance compared to the state-of-the-art models on the task of HIN-based recommendation. Further, MTRec enhances the interpretability of recommendation via its “translation” mechanism and helps us better understand the recommendation results. Moreover, MTRec is able to provide predictions for the missing links in HIN, which is one task for modeling the dynamic growth of HIN. Therefore, MTRec is of high practical value compared to previous HIN-based recommendation methods which merely consider the recommendation task.

The rest of this paper is organized as follows. Section 2 provides the background. We present the details of MTRec in Section 3. Experiments on real data sets that demonstrate the effectiveness and the interpretability of MTRec are reported in Section 4. Section 5 discusses the related work. Section 6 concludes the paper and provides directions for future work.

Notation. We use lower-case fonts for scalars, bold lower-case fonts for vectors and bold upper-case font for matrices. For example, p is a scalar, p is a vector and P is a matrix.

SECTION 2Preliminaries
In this section, we introduce relevant concepts used in this paper. A recommender system consists of m users and n items and their interactions are represented by a user-item interaction matrix R∈Rm×n with each non-zero element ri,j=1 indicating an observed interaction between user i and item j (e.g., i has rated or viewed j). Heterogeneous Information Network is defined as follows [8]:

2Definition 1 (Heterogeneous Information Network).
HIN is denoted as a directed graph G={V,E} where V is an object set and E is a link set. A HIN is also associated with an object type mapping function ϕ:V→A and a link type mapping function ψ:E→R. A and R are sets of object types and link types where |A+R|>2.

To describe the complex HIN, a network schema is provided for better understanding the structures and relations from meta level:

2Definition 2 (Network Schema).
Network schema, defined as S={A,R}, is a meta template for a HIN G={V,E} with object and link mapping functions ϕ and ψ.

In HIN, two objects can be linked through different semantic paths which are defined as meta-paths:

2Definition 3 (Meta-path).
A meta-path ρ is a path defined on a network schema S={A,R}, and is denoted in the form of A1⟶R1A2R2⟶⋯Rl⟶Al+1, which defines a composite relation R=R1∘R2∘⋯∘Rl between object types A1 and Al+1, where ∘ represents the composition operator on relations.

Since there are multiple objects for each object type, a meta-path ρ can produce multiple meta-path instances. We use h∈ρ to indicate that h is an instance of meta-path ρ. A meta-path (or meta-path instance) with a length of l indicates there are l+1 object types (or objects) and l links.

By incorporating the idea of HIN into the recommendation, RS can better depict user preferences and item properties. We redefine the recommendation task in the setting of HIN:

2Definition 4 (HIN-based Recommendation Problem).
Given the user-item interaction matrix R and the corresponding HIN G in the system, for each user u, HIN-based RS aims to offer a recommendation list of k items that u is most interested in.

Some recent works have considered the rating prediction task in the setting of HIN-based RS [5]. Since top-k recommendation is more important and practical, we target at solving the problem defined in Definition 4.

Link prediction, which is a traditional data mining task, is introduced as an auxiliary task into MTRec in order to enhance the recommendation task under a multi-task architecture. Link prediction for HIN can be modeled from a perspective of sequential modeling, based on meta-paths. We give the definition of the auxiliary task as follows:

2Definition 5 (Link Prediction for HIN).
Each instance h of meta-path ρ, which has a length of l, can be segmented into l links: h={⟨sρ,h,1,sρ,h,2⟩,⟨sρ,h,2,sρ,h,3⟩,…,⟨sρ,h,l,sρ,h,l+1⟩}, where sρ,h,b is the bth object in h. The link prediction problem for HIN can then be defined as a prediction problem of the oth object in a meta-path instance, given its preceding o−1 objects.

SECTION 3MTRec
In this section, we introduce the proposed framework MTRec for multi-task learning in HIN-based RS.

3.1 Overview of Our Model
HIN is extremely spare, and there are many missing links (i.e., edges between different types of objects) in HIN-based RS for two reasons:

There is a large number of items in the system. It is unlikely for one user to interact with most items. Consequently, a tremendous amount of links from users to other objects in HIN, which represent user interactions, are missing.

Furthermore, a large part of the information for new items has not been collected in RS and the relations (i.e., links) from the new item to other objects do not appear in HIN.

The sparsity of HIN prevents it from assisting the recommendation task. Additionally, HIN is growing daily as users interact with items (e.g., view a webpage, buy a product or listen to music) and more information is collected by the system (e.g., product attributes). Assuming HIN is invariable when modeling HIN-based RS is unreasonable. Nevertheless, the sparsity and dynamics of HIN in HIN-based RS are commonly ignored in previous works.

To address these problems, we introduce an auxiliary task, link prediction for HIN, into MTRec and design the architecture of MTRec using multi-task learning. Link prediction of the network is a traditional data mining task. HIN-based recommendation task and link prediction for HIN task are relevant as they share some features of meta-paths. MTRec employs multi-task learning which introduces several benefits to HIN-based recommendation:

Link prediction models the dynamic growth of HIN and it not only helps enrich the information of HIN (thus it alleviates the sparsity problem and information from meta-paths which do not bridge the target user and candidate items are also considered in MTRec), but also increases the performance of the recommendation task through sharing features and joint optimization in a multi-task manner.

Multi-task learning helps the model to focus on most essential features, as the auxiliary task will provide additional evidence for the relevance or irrelevance of those features.

Multi-task learning enhances the generalization ability of MTRec as it biases the model to prefer features that both the main task and the auxiliary task prefer.

Fig. 1 illustrates the overall architecture of MTRec that consists of two components. The left component (Section 3.2) is the self-attentive recommendation module, which provides top-k recommendations to users based on the meta-paths connecting users and candidate items. The right component (Section 3.3) is designed for the auxiliary task, link prediction in HIN, which is one of the tasks modeling the dynamics of HIN. The two modules share the underlying object embedding layer. Additionally, each component maintains one private feature space and shares a common feature space of meta-paths (Section 3.4.1) for multi-task learning. The private features in each component and the shared features are learned through the private self-attention layer in each component and a public self-attention layer, respectively. Finally, the two tasks are automatically balanced using a Bayesian task weight learner during the joint optimization (Section 3.4.2).


Fig. 1.
Architecture of MTRec. “⊗” is the operator of Hadamard product and “⊙” denotes the operator of concatenation. “U” and “I” represent users and items, respectively. “A”, “B”, “C” and “D” are four different object types.

Show All

3.2 Main Task: HIN-Based Recommendation
In this section, we will elaborate on the self-attentive recommendation module in MTRec.

3.2.1 Self-Attentive Meta-Path Modeling
Unlike previous embedding based approaches that leverage neural networks like CNN to learn the representations of meta-paths [6], MTRec mostly relies on a new concept, self-attention [17], to capture the complex semantics in meta-paths. Like other attention mechanisms which compute a weight distribution on the input sequence and assigns higher values to more relevant elements in the sequence [18], self-attention endows the model with the ability to focus on the most important parts of the sequential data and enhances the interpretation of the model. Differently, self-attention mechanism learns the attention weights by matching a sequence against itself. Inspired by the success of self-attention in various NLP tasks, especially machine translation [17], we adopt self-attention to model meta-paths in HIN-based RS which requires a sophisticated design as HIN-based recommendation is quite different from the machine translation task. Compared to most neural network based recommendation methods which use the attention mechanism as a supplemental component to enhance the original neural architecture (e.g., RNN or CNN), MTRec uses self-attention as its backbone in most part of the learning procedure.

The self-attentive meta-path modeling method used in MTRec can be decomposed into the following parts.

Object Embedding Layer. Since this paper focuses on the pure HIN-based CF setting, we only use the identity (i.e., ID) of each object in HIN as the input features. The input feature (e.g., the id of the object i with object type t) is represented as a binarized sparse vector g(t)i with one-hot encoding. Above the input layers are the embedding layers for all object types. Each embedding layer is a fully connected layer that projects the sparse one-hot representation g(t)i to a dense d-dimensional vector s(t)i to represent the corresponding object i with type t in HIN.

Meta-Path Embedding Layer. As introduced in Section 2, each meta-path consists of a sequence of object types. We represent each meta-path instance by concatenating the embeddings of the objects in the sequence. Given an instance h of meta-path ρ with length l−1, its embedding Pρ,h∈Rd×l is the stack of the embeddings for the objects in h
Pρ,h=⎡⎣⎢⎢⎢⎢s(1)ρ,h,1⋮s(d)ρ,h,1s(1)ρ,h,2⋮s(d)ρ,h,2⋯⋯⋯s(1)ρ,h,l⋮s(d)ρ,h,l⎤⎦⎥⎥⎥⎥,(1)
View Sourcewhere s(f)ρ,h,o indicates the fth dimension of the oth object in the meta-path instance h. Note that s is used to indicate object embedding in this paper with two usages: 1) s(f)ρ,h,o will be used when depicting its position in a meta-path; 2) When emphasizing the object id and object type, s(t)i will be used.

Self-Attention Layer. A typical attention technique [18] maps a sequence of d-dimensional vectors, i.e., the key K, to the attention weights A. In this process, another input element Q called query is used as a reference and the attention technique computes the compatibility between the key and the query to give emphasis to the elements in the keys which are relevant to the query. Then, the compatibility is passed through a distribution function to obtain the attention weights A. Finally, the attention weight is applied on another input element V, the value, to obtain the refined representations of the key. In self-attention, the key, query and value refer to the same input sequence and this is how the name self-attention comes from.

In our model, the inputs of the key K, the query Q and the value V are the same and compose of the meta-path instance embeddings Pρ,h. They are first projected to the same space with shared parameters
Kρ,h=WKPρ,h,Qρ,h=WQPρ,h,Vρ,h=WVPρ,h,(2)
View Sourcewhere WK,WQ,WV∈Rd×d are learnable weight matrices for K, Q and V, respectively.

Then, the affinity values Aρ,h∈Rl×l (i.e., attention weights) can be obtained as follows:
Aρ,h=softmax(KTρ,h⋅Qρ,hd−−√),(3)
View Sourcewhere d−−√ is used as the scaling factors to avoid the negative effects of small gradients. Each element ac,e∈A reflects the relevance of the eth object in the key sequence to the cth object in the query sequence. It is worth noting that the key and the query is the same sequence in our model, i.e., one meta-path embedding. Consequently, the attention weight in Eq. (3) is computed by the self-matching of an input sequence.

After that, the refined instance embedding vector p^Tρ,h∈Rd of instance h for meta-path ρ can be obtained by combining the value V and attention weights A
p^(1)ρ,hp^(2)ρ,h=flatten(Vρ,hAρ,h)=tanh(Wpp^(1)ρ,h+bp),(4)
View Sourcewhere “flatten(⋅)” is the flattening operator which reshapes the d×l inputed matrix to (d×l)-dimensional vector, Wp and bp denote the weight matrix and the bias vector, and we use the hyperbolic tangent function as the activation function.

Our model further applies a mean pooling operation over the embeddings of all the meta-path instances for the meta-path ρ to derive the corresponding meta-path embedding m^ρ∈Rd
m^ρ=mean(∑h∈ρp^ρ,h),(5)
View Sourcewhere “mean(⋅)” is the mean pooling operation that extracts the average value of each dimension for embeddings of all meta-path instances.

After reviewing the whole process in self-attention layer, we can find that the position of each object in the meta-path instance does not affect the formulation for the refined meta-path embedding vector m^ρ. In other words, permuting the order of objects in one meta-path and the resulted meta-path will have the same representation as the original one. To avoid such unreasonable representations, we inject a learnable position embedding G∈Rd×l into Pρ,h before projecting it to Kρ,h, Qρ,h and Vρ,h in Eq. (2) to help our model retain the sequential patterns of meta-path instances
Pρ,h=Pρ,h+G,(6)
View Sourcewhere each element in the ith row of G is initialized with the value i. Through the injected positional embedding G, our model is aware of the position of each object in the meta-path. Unlike [17] where a fixed position embedding is used, we find that applying a learnable position embedding makes the results more robust.

3.2.2 Modeling Three-Way Interactions in Recommender
Most HIN-based RS, which rely on meta-paths, only characterize two-way user-item interactions, and seldom consider the mutual effect between the meta-path and the involved user-item pair in an interaction. Features learned from meta-paths are used to enhance user and item representations in recommendation models. Though these methods achieve performance improvements to some extent, they are unable to provide good interpretations about why meta-path and which meta-path helps improve the quality of recommendation due to the lack of modeling interactions among users, items and meta-paths connecting them [6].

To model the probability that a user i will accept an item j given the meta-paths connecting them, MTRec adopts the concept of “translation” which is prevalently used in knowledge graph embedding [19], [20]. In a knowledge graph, an edge can be represented as a triple ⟨head entity,relation,tail entity⟩ where head and tail entities are the nodes, and relation is the edge between them. Translation-based knowledge graph embedding [21] models the relation embedding r as the translation from head embedding h to tail embedding t: h+r≈t. Inspired by the success of translation-based knowledge graph embedding approaches, we design a prediction layer for HIN-based recommendation with the idea that the meta-path can be viewed as the translation from the user to the item. The prediction layer for recommendation is defined as follows:
ei,ρ,j=relu(We(s(user)i+m^ρ−s(item)j)+be)ri,j=σ(−∑ρ∈Path(i,j)||ei,ρ,j||2),(7)
View Sourcewhere We and be are the learnable parameters, Path(i,j) denotes all the meta-paths connecting user i and item j, “||⋅||2” indicates the vector norm, ri,j indicates the likelihood that user i will interact with item j, and σ(⋅) is the sigmoid function.

If a meta-path ρ connecting a user i and an item j indicates a high likelihood that i will interact with j, then s(item)j should be close to s(user)i+m^ρ. Different from translation-based knowledge graph embedding where the embeddings of head and tail entities belong to the same space, user embedding s(user)i, meta-path embedding m^ρ and item embedding s(item)j in Eq. (7) belong to three different vector spaces in MTRec. Therefore our model employs a more heterogeneous translation mechanism for modeling three-way interactions compared to translation-based knowledge graph embedding approaches.

One by-product of harnessing “translation” to model three-way interactions is the reinforcement for the interpretability of HIN-based recommendation. Observed from Eq. (7) that the L2 distance between the representation of a meta-path ρ (i.e., m^ρ) and s(item)j−s(user)i, where j is an item that user i is very likely to accept (i.e., ri,j is large), should be small if ρ plays a vital role in marking j as a positive candidate for the recommendation. Therefore, we can calculate the L2 distance between the representation of each meta-path connecting a ground-truth user-item pair and the corresponding representation s(item)−s(user) (we call it reference representation in this paper) to explain which meta-path contributes to the recommendation most.

3.2.3 Loss Function of HIN-Based Recommendation
We adopt negative sampling to train our model for the HIN-based recommendation task. Specifically, we uniformly sample nrec items for each user i (i.e., negrec(i)) in the training set in each epoch. User i has not interacted with any item in negrec(i) in the training set. We modify the binary cross entropy loss as the objective function
Lrec(θrec)=−∑⟨i,j⟩∈Trec(logri,j+∑j′∈negrec(i)log(1−ri,j′)),(8)
View Sourcewhere Trec indicates the training data for the recommendation task, θrec is the parameters for the model, and ⟨i,j⟩ denotes that user i has interacted with item j.

3.3 Auxiliary Task: Link Prediction for HIN
Link prediction of a network is a tradition data mining task and various methods, either catered for HIN or not, can be adapted in our multi-task framework. Since we already harness self-attention mechanism to modeling the representations of meta-paths in HIN-based recommendation, we go along this direction in the task of link prediction for HIN (illustrated in Definition 5) for ease of exploration.

Similar to the recommendation module, we first obtain the meta-path instance embedding Pρ,h for instance h of meta-path ρ and enhance it using the learnable positional embedding G as shown in Eqs. (1) and (6). After that, Pρ,h is passed through a self-attention layer to retrieve the refined embedding matrix P^ρ,h∈Rd×l of the meta-path instance h
Aρ,hP^ρ,h=softmax(KTρ,h⋅Qρ,hd−−√)=Vρ,hAρ,h=⎡⎣⎢⎢⎢⎢s^(1)ρ,h,1⋮s^(d)ρ,h,1s^(1)ρ,h,2⋮s^(d)ρ,h,2⋯⋯⋯s^(1)ρ,h,l⋮s^(d)ρ,h,l⎤⎦⎥⎥⎥⎥,(9)
View Sourcewhere s^(f)ρ,h,o is the fth dimension of the output representation for the oth object. However, directly using s^ρ,h,o as the features for predicting the oth link will be erroneous [22]. Due to the nature of sequences, the model should consider only the first o objects when predicting the oth link. However, we can observe that the oth output of the self-attention layer contains embeddings of subsequent objects. Therefore, we modified the attention mechanism in Eq. (9) for link prediction module by using masks to forbid all the links from Qρ,h,o to Kρ,h,f where f>o. This way, s^ρ,h,o can be viewed as the features learned based on first o objects in an instance.

To predict the oth link in a meta-path instance, we estimate the likelihood of each candidate object c for the (o+1)th object based on the first o objects in the instance h. MTRec employs a product layer for the estimation
qh,o+1,c=σ(s^Tρ,h,o⋅sc),(10)
View Sourcewhere sc is the embedding of the candidate object c that has the object type for the (o+1)th object in the meta-path schema for ρ, and σ(⋅) is the sigmoid function.

3.3.1 Loss Function of Link Prediction
Similar to the recommendation task, we employ negative sampling to cope with the optimization of the link prediction task. For each meta-path instance h∈ρ, we uniformly sample npath objects (i.e., negpath(ρ,h,t)) for each step t in h in each epoch. During sampling, any negative sample in negpath(ρ,h,t) should not be the direct subsequent object for the (t−1)th object sρ,h,t−1 in any instance of ρ. We modify the binary cross entropy loss as the objective function
Lpath(θpath)=−∑ρ∈Tpath∑h∈ρ∑2≤t≤|ρ|(logqh,t,ct+∑j′∈negpath(ρ,h,t)log(1−qh,t,j′)),(11)
View Sourcewhere θpath are the parameters of the model for link prediction, Tpath is the training data of link prediction task, |Tpath| is the number of all meta-path instances, ct is the id of the ground-truth object at tth step of h, and |ρ| indicates the number of object types contained in the meta-path ρ.

3.4 Multi-Task Learning for HIN-Based Recommender
In this section, we explain how MTRec can be optimized in a multi-task manner.

3.4.1 Feature Sharing
One key idea of multi-task learning is that features learned by different tasks should be divided into private and shared spaces, depending on whether parameters of some components should be shared. Fig. 2 depicts the private-shared model we use for parameter sharing between the HIN based recommendation task and the link prediction task.


Fig. 2.
Private-shared model for multi-task learning. “⊙” denotes the operator of concatenation.

Show All

For any meta-path instances h used either in recommendation task or link prediction task, its private representation and shared representation can be learned by passing it through a private self-attention layer in each task and a shared self-attention layer between two modules, respectively. Each self-attention layer is defined in Section 3.2. The final features for each task are the concatenation of private representation and shared representation.

3.4.2 Bayesian Task Weight Learner
Multi-task learning concerns the problem of optimizing a model with respect to multiple objectives. A common way to combine loss objectives of the two tasks, i.e., HIN based recommendation and link prediction, is to perform a weighted sum of the individual loss objective
L=wrecLrec+wpathLpath.(12)
View Source

The harmony of the two tasks can be achieved through setting different weight values of wrec and wpath for the two tasks. A naive but prevalent way is to set the weights to be equal for individual tasks. However, this workaround is only valid when the tasks do not compete, which is rarely the case. Different tasks need to be properly balanced so that network parameters converge to robust shared features that are useful across all tasks. Task imbalances impede proper training because they manifest as imbalances between backpropagated gradients. A task that is too dominant during training, for example, will express that dominance by inducing gradients which have relatively large magnitudes. Searching and dynamically updating optimal weights during optimization is a difficult and expensive process.

Inspired by the recent study which uses uncertainty to weigh losses in multi-task learning [23], [24], we leverage Bayesian modeling to derive the joint multi-task loss function and design a Bayesian task weight learner which can automatically achieve the balance between the two tasks.

Similar to Kendall et al. [24], we introduce an assumption 1λ2(exp(xλ2)+1)≈(exp(x)+1)1λ2 which becomes an equality when λ→1. Based on the assumption, we can derive the following approximations for the sigmoid function σ(⋅):
σ(xλ2)=exp(xλ2)exp(xλ2)+1≈1λ2(exp(x)exp(x)+1)1λ2=1λ2(σ(x))1λ21−σ(xλ2)=1exp(xλ2)+1≈1λ2(1exp(x)+1)1λ2=1λ2(1−σ(x))1λ2.(13)
View Source

Now let us investigate the loss function shown in Eq. (8) for the HIN-based recommendation. Let x(i,j) be a user-item interaction ⟨i,j⟩ and y(i,j) represents the label for x(i,j) (1 if ⟨i,j⟩ is positive, otherwise 0). R(x) indicates the outputs of MTRec for all x using Eq. (7) and rx(i,j) is the probability of a specific interaction ⟨i,j⟩ being positive. Then the binary classification likelihood can be defined as follows:
Pr(y|R(x))=∏⟨i,j⟩∈TrecPr(y(i,j)∣∣rx(i,j))=∏⟨i,j⟩∈Trec(σ(rx(i,j))∏j′∈negrec(i)(1−σ(rx(i,j′)))).(14)
View Source

Following Kendall et al. [24], we introduce a scalar α into Eq. (14) to get a scaled version of the model output
Pr(y∣∣R(x),α)=∏⟨i,j⟩∈Trec(σ(rx(i,j)α2)∏j′∈negrec(i)(1−σ(rx(i,j′)α2))),(15)
View Sourcewhich can be interpreted as a Boltzmann distribution (i.e., Gibbs distribution) [24]. The input is scaled by α2 (often referred to as temperature). Then the log likelihood can be written as
=≈=logPr(y∣∣R(x),α)∑⟨i,j⟩∈Trec(log(σ(rx(i,j)α2))+∑j′∈negrec(i)log(1−σ(rx(i,j′)α2)))∑⟨i,j⟩∈Trec(1α2log(σ(rx(i,j)))+1α2∑j′∈negrec(i)log(1−σ(rx(i,j′)))−2(nrec+1)logα)−1α2Lrec(θrec)−2(nrec+1)⋅|Trec|⋅logα,(16)
View Sourcewhere we introduce the approximations in Eq. (13) to the penultimate transition.

Similarly, we can obtain the log likelihood for the link prediction task with a scalar β, since the objective for link prediction in Eq. (11) has a similar form as the objective of recommendation in Eq. (8)
logPr(y∣∣Q(x),β)≈−1β2Lpath(θpath)−2(npath+1)⋅|Tpath|⋅l¯⋅logβ,(17)
View Sourcewhere Q(x) indicate the outputs of MTRec on the inputs x to the link prediction task using Eq. (10), y is the labels, |Tpath| is the number of all meta-path instances, and l¯ is the average length of meta-paths.

We maximize the log likelihood of MTRec in maximum likelihood inference, while we minimize the joint objective during optimization. Thus, the joint loss L can be formulated as
L(θrec,θpath)===−logPr(y∣∣U(x))−log(Pr(yrec∣∣R(xrec),α)⋅Pr(ypath∣∣Q(xpath),β))1α2Lrec(θrec)+1β2Lpath(θpath)+2(nrec+1)⋅|Trec|⋅logα+2(npath+1)⋅|Tpath|⋅l¯⋅logβ,(18)
View Sourcewhere U(x)={R(xrec),Q(xpath)} is the outputs of MTRec on the inputs x={xrec,xpath}, and y={yrec,ypath} is the labels.

The joint objective can be seen as learning the relative weights of the two losses. Small value of α will decrease the contribution of Lrec, whereas large value will increase its contribution. β has a similar impact on the contribution of Lpath. Scales are regulated by the last two terms in Eq. (18). This way, we provide an automatic mechanism to balance the two tasks. Unlike Eq. (12) where the relative weights have to be set manually, the weights α and β in the joint loss of Eq. (18) are automatically learned during optimization.

The Bayesian task weight learner is different compared to previous works [23], [24], which introduce the idea of uncertainty to automatically learn the task weights to balance a regression task and a classification task without the negative sampling strategy. It is nontrivial to design such a task weight learner without the approximations we proposed. Negative sampling strategy is a common training method used in many machine learning tasks including general recommendation [25] and knowledge graph embedding [19], [20]. The Bayesian task weight learner can benefit not only the HIN-based recommendation task in this paper but also other tasks involving multi-task learning and negative sampling.

SECTION 4Experiments
In this section, we conduct an experimental study using real data sets to show the effectiveness of MTRec.

4.1 Experimental Settings
4.1.1 Data
We use data sets MovieLens,1 LastFM2 and Yelp3 which are widely used in previous studies about HIN-based recommendation [6].

For the recommendation task, our evaluation focuses on implicit feedbacks. LastFM contains users’ listening records which can be transformed as implicit feedbacks and directly used for our evaluation. For data sets MovieLens and Yelp, we follow [6], [25] and treat a user-item rating as an interaction record. We select the same meta-paths as [6]. These meta-paths contain at most 4 hops since long meta-paths are likely to introduce noisy semantics [8]. Table 1 explains the meaning of the notations used for these meta-paths. For example, relations “User-Movie” and “Movie-Genre” are denoted as “um” and “mg” in the following, respectively. Thus, the meta-path “umgm” in MovieLens indicates “User-Movie-Genre-Movie”.

TABLE 1 Statistics of the Data

For assessing the quality of HIN-based recommendation task, we randomly selected 80 percent user-item interactions in each data set to be used for training; the remaining 20 percent interactions are held out for testing. We then use the methodology in [6] to select meta-path instances connecting users and items:

First, user-item interaction matrix is factorized by CCDPP4 [26], a matrix factorization method, to obtain user and item feature vectors.

Based on user and item feature vectors, we compute the top-50 most similar users/items for each user/item and generate {uu, mm}, {uu, aa} and {uu} relations for MovieLens, LastFM and Yelp, respectively. We adopt Peason's coefficient as the similarity measure. Other relations exist in the original data. Given these relations (i.e., edges), HIN can be constructed.

We leverage HIN2Vec5 [27], a representation learning method for HINs, to retrieve representations for each object in HIN.

Priority based random walk [6] is used to retrieve meta-paths connecting user-item pairs which have interaction records in the training data. The next object to visit is selected based on the similarity between the representations of the current object and all the out-going similar objects. Similarity threshold is set to be 0.9 for all data. Additionally, we use a threshold of 0.8 to generate a larger version of Yelp data which includes more meta-path instances. We denote it by Yelp-L.

For evaluating the link prediction task, we also adopt the priority based random walk to sample meta-path. Differently, the start object (user) and the end object (item) of the sampled instance are not required to constitute a user-item interaction in the data. Then, 80 percent meta-path instances are randomly sampled for training and the remaining instances are used for evaluation.

In addition to the explanation of notations, Table 1 shows the statistics for all data sets used in both main task and auxiliary task. It is worth noticing that the meta-path instances used in the main task is not the subset of the instances used in the auxiliary task, though they are overlapping. Meta-path instances for the auxiliary task may connect some user-item pairs in which the user has not interacted with the item, while instances for the main task connecting user-item interactions.

4.1.2 Baselines
We compare the performance of the following approaches:

BPR [28] is the Bayesian Personalized Ranking model that minimizes the pairwise ranking loss for implicit feedback

NeuCF [25] is the Neural Collaborative Filtering which utilizes implicit feedback for top-k recommendation.

NGCF [29] is the Neural Graph Collaborative Filtering method which exploits the user-item graph structure by propagating embeddings on it.

VAES [30] is the Variational Autoencoders based Collaborative Filtering method, which generalizes linear latent-factor models to a non-linear probabilistic latent-variable model.

NeuACF [31] is the Aspect-Level Deep Collaborative Filtering approach which learns the aspect-level latent factors through different meta-paths and fuse them with an attention mechanism for top-k recommendation.

FMG [3] is a meta-graph based method for rating prediction in recommender system. Similar to [6], we modify its optimization objective as pairwise ranking loss used in BPR [28] for top-k recommendation.

HERec [7] is a HIN based recommendation method which adopts a meta-path based random walk strategy to generate meaningful object sequences for network embedding. The embeddings are then fused and integrated into a MF method for recommendation.

MCRec [6] is a meta-path based model, which constructs a three-way neural interaction architecture for recommendation. CNN is used to extract meta-path embeddings and a co-attention mechanism is leveraged for modeling interactions among users, items and meta-paths.

MTRecrec is our proposed approach for recommendation task in HIN. It only contains the method introduced in Section 3.2.

MTReclp is the method introduced in Section 3.3 for the link prediction task in HIN.

MTRec is the complete framework for multi-task recommendation in HIN. It optimizes both the tasks of HIN-based recommendation and link prediction in HIN. Feature sharing and weighted loss illustrated in Section 3.4 are used in the framework.

MTRecn is similar to MTRec, except that a naive weighted sum loss (Eq. (12)) is used for multi-task learning. wrec and wpath are set to be equal.

The learned representations (i.e., the output from Step 3 when selecting meta-path instances for evaluating HIN-based recommendation) for each object in HIN by HIN2Vec is utilized as the pre-trained features for MCRec. Our approach and BPR were implemented using PyTorch. Implementations for other methods are provided by their authors.

4.1.3 Evaluation
For a fair comparison, we follow the evaluation settings in [6] for the recommendation task. For each user-item pair in the test set, we randomly sample nrec negative test items. The target user has not interacted with these negative items before. For each user in the test set, we use all the items he/she has interacted with in the test set and the corresponding negative items to construct a test list. Then we rank the list and the results are evaluated using Precision at rank k (Prec@k), Recall at rank k (Recall@k) and Normalized Discounted Cumulative Gain at rank k (NDCG@k).

For the auxiliary task, link prediction, we use a similar evaluation protocol as the recommendation task. For each meta-path instance with length l−1 in the test set, we predict the lth object in the sequence based on the first l−1 objects. For each positive last object in an instance h∈ρ, we sample npath negative objects which can not be the direct subsequent objects for the (l−1)th object (i.e., sρ,h,l−1) in any instance of ρ. For each user in the test set, we use all the meta-path instances starting from him/her in the test set and the corresponding negative objects to construct a test list. We adopt Hit Ratio at rank k (HR@k), NDCG@k and Mean Reciprocal at rank k (MRR@k) which are the measures commonly used for sequential analysis [32] for evaluating link prediction task.

Hyperparameters. For all methods, we set learning rate, dimensionality d, training batch size, nrec and npath to 0.001, 128, 256, 20 and 100, respectively. Hypeparameters of each baselines are set as suggested by their authors. All methods are optimized using Adaptive Moment Estimation (Adam) [33] and trained until convergence.

4.2 Experimental Results
4.2.1 Performance for HIN Based Recommendation
Overall Performance. Table 2 illustrates the performance of all methods for the recommendation on the four data sets when evaluating using rank k=10. The first row of ‘Improve’ shows the improvement percentage of our model MTRec over other methods (except MTRecrec and MTRecn) with the best performance on each evaluation measure, the second row demonstrates the improvement percentage of our model MTRecrec over other methods (except MTRec and MTRecn) with the best performance on each evaluation measure, the third row depicts the improvement percentage of MTRec over MTRecrec, and the forth row shows the improvement percentage of MTRec over MTRecn.

TABLE 2 Performance of Different Methods for the Recommendation Task on Four Data Sets

From Table 2, we can observe that MTRec significantly outperforms existing recommendation approaches, either leveraging HIN or not, for the recommendation task. Compared to methods which do not utilize HIN (i.e., BPR, NeuCF, NGCF and VAES), approaches using HIN (i.e., NeuACF, FMG, HERec, MCRec, MTRecrec, MTRecn and MTRec) show better performance in most cases, which demonstrates the usefulness of HIN in improving the quality of recommendation.

To investigate whether each component of MTRec contributes to the improvement of performance, we evaluate the performance of MTRecrec. As shown in Table 2, it achieves a noticeable improvement compared to baselines. In the worst case, MTRecrec shows a comparable performance to the best baseline. In most cases, MTRecrec outperforms all the baselines significantly. From the results, we can conclude that the self-attentive meta-path modeling and three-way interaction modeling, as introduced in Section 3.2, play a vital role in increasing the performance of the recommender. On the other hand, the better results of MTRecrec shows that a pure attention based recommendation model can yield a comparable or even better recommendation compared to previous methods which rely on convolutional neural network (MCRec), multi-layer perceptron (NeuACF), graph neural network (NGCF) and variational autoencoder (VAES).

From Table 2, we can also find that introducing multi-task learning into MTRec help improve its performance for the recommendation task, since MTRec largely exceeds MTRecrec. This is consistent with our motivation of MTRec which is illustrated in Section 1: considering the auxiliary task in HIN during optimization can enhance the performance of the model on the recommendation task.

Performance on Different Rank k. To assess the robustness of our models, we report the Prec@k of HIN-based recommendation models (FMG, MCRec, MTRecrec and MTRec) on three data sets using k={1,2,5,10} in Fig. 3. For other measures and Yelp-L data, similar trends can be observed. From Fig. 3, we can conclude that MTRec consistently outperforms other HIN-based recommendation methods which shows the robustness of MTRec.


Fig. 3.
Performance of different methods on Prec@k (k={1,2,5,10}) for the recommendation task.

Show All

4.2.2 Performance for Link Prediction in HIN
Overall Performance. The focus of this paper is improving the performance of recommendation and link prediction is the auxiliary task which assists the main task. For ease of exposition, we only compare the performance of MTReclp and MTRec to show that the auxiliary task also benefits from the multi-task learning.

We report the results of link prediction on four data sets in Table 3 when evaluating using rank k=10. The row of ‘Improve’ shows the improvement percentage of MTRec over MTReclp which only considers the link prediction task.

TABLE 3 Performance of MTReclplp, MTRecnn and MTRec for the Link Prediction Task on Four Data Sets

From Table 3 we can observe that our model MTRec, which considers both recommendation task and link prediction task, significantly surpasses its downgraded version MTReclp. From the results, we can draw the conclusion that the multi-task mechanism we design not only improves the performance on the main task but also benefits the auxiliary task.

Performance on Different Rank k. We also report the results of MTReclp and MTRec using k={1,2,5,10} for HR@k on three data sets to evaluate the robustness of our models. For other measures and Yelp-L data, the two models show similar trends. Fig. 4 illustrates the results. From Fig. 4, we can find that the performance of MTRec surpasses MTReclp for different values of k on HR@k. Hence, both MTReclp and MTRec are robust for the auxiliary task.


Fig. 4.
Performance of different methods on HR@k (k={1,2,5,10}) for the link prediction task.

Show All

4.2.3 Effectiveness of the Bayesian Task Weight Learner
We also compare the performance of using naive joint loss (MTRecn) and employing Bayesian weighted loss (MTRec). The evaluation results for recommendation task and link prediction task can be found in Tables 2 and 3, respectively. From the results, we can observe that MTRec surpasses MTRecn in both tasks. Additionally, Bayesian task weighter learner helps improve the performance when MTRecn does not significantly exceed MTRecrec (e.g., Prec@10 for LastFM in Table 2) or MTReclp (e.g., MRR@10 for LastFM in Table 3) due to the improper setting of task weights. This demonstrates the effectiveness of our proposed Bayesian task weight learner in increasing the performance, in addition to its benefit that we do not need to manually set the weights for two tasks in the joint loss.

4.2.4 Interpretability of the Recommendation
We now conduct a detailed analysis of MTRec on its interpretability.

First, we investigate the overall impact of different meta-paths in the recommendation results of MTRec. As illustrated in Section 3.2.2, the L2 distance between the representation of each meta-path connecting a ground-truth user-item pair and the reference representation s(item)−s(user) depicts the importance of each meta-path. Therefore, we calculate the average L2 distance between the embeddings of each meta-path and the corresponding reference for all ground-truth user-item pairs.

From Table 4, we can observe that “umgm” and “uata” are most important meta-path for MTRec when it makes a recommendation in MovieLens and LastFM, respectively. For Yelp and Yelp-L, both “ubcb” and “ubib” play a vital role. This observation is consistent with the previous study [6].

TABLE 4 Average L2 Distance Between Each Meta-Path Representation and Reference Representation

We further take two real examples to illustrate how the “translation” mechanism used in MTRec can help us understand the recommendations provided by MTRec and enhance the interpretability of RS. Fig. 5 demonstrates the two examples. We examine the recommendation results of MTRec to a user with an anonymous id “joIzw_aUiNvBTuGoytrH7g” in Yelp data set. We call this anonymous user Bob in the sequel.


Fig. 5.
Two examples for the interpretability of MTRec.

Show All

Example 1 (ubcb).
MTRec recommends one bar named “Tailgaters Sports Bar & Grill” to Bob, which correctly hits the ground-truth user-item interaction, i.e., Bob has indeed interacted with “Tailgaters Sports Bar & Grill”. Then, we investigate the L2 distance between relevant meta-path representations and reference representations. We find the meta-path “ubcb” contributes most to the recommendation. The object type “c” (Category) in the meta-path “ubcb” gives us the hint. By inspecting into the data, it is found that Bob has visited 21 local businesses in the category of “Beer, Wine & Spirits” which explains why “ubcb” leads MTRec to recommend “Tailgaters Sports Bar & Grill”: Bob frequently goes to lcoal bars. These 21 bars form the first segment (i.e., “ub”) of “ubcb”. “Tailgaters Sports Bar & Grill” and these 21 bars belong to the same category “Beer, Wine & Spirits”, which provides the third segment (i.e., “cb”) and the second segment (i.e., “bc”) in ubcb. Finally, “ubcb” leads MTRec to find the correct recommendation target “Tailgaters Sports Bar & Grill”.

Example 2 (uub).
MTRec also correctly recommends one restaurant named “Oregano's Pizza Bistro” to Bob. We investigate the L2 distance between relevant meta-path representations and reference representations and find the meta-path “uub” contributes most to this recommendation. We investigate the most similar users of Bob, which indicates that there are edges of “uu” connecting these users to Bob in HIN. We found that the top-4 most similar users have all visited “Oregano's Pizza Bistro” before, which means there are edges “ub” connecting these 4 users to the restaurant “Oregano's Pizza Bistro”. This explains why meta-path “uub” plays a key role in this successful recommendation.

The above two examples show that MTRec is able to provide good interpretabilities for the recommendation results through using the the “translation” mechanism introduced in Section 3.2.2.

SECTION 5Related Work
In this section, we elaborate on three directions of previous works which are related to MTRec.

5.1 Heterogeneous Network Embedding
Heterogeneous network embedding, which studies the embedding problem of HIN through preserving the structural information of meta-path and meta-graph, is one research direction of network embedding. Dong et al. [34] proposed metapath2vec and metapath2vec++, which adopt meta-path based random walks, skip-gram model [35] and heterogeneous negative sampling to learn heterogeneous representations. Fan et al. [36] learns representations based on meta-graph instead of meta-path and proposed metagraph2vec for malware detection. HIN2Vec [27] carries out multiple prediction training tasks jointly to learn latent vectors of objects and meta-paths in HIN. Hussein et al. [37] found that adopting random walks with a jump and stay strategy can better learn the embeddings of HIN and thus selecting and learning on meta-path is not necessary. HERR [38] transcribes the rich and potentially incompatible information from HIN to the embeddings using edge representations. PME [39] utilizes metric learning to capture both first-order and second-order proximities of objects in HIN. Shi et al. [40] introduced the concept of aspects into HIN with each aspect being a unit representing one underlying semantic facet. Instead of preserving information of the network in one semantic space, they proposed ASPEM which encapsulates information regarding each aspect individually. SHNE [41] captures both heterogeneous structural information and unstructured semantic information (e.g., text) of objects. Wang et al. [42] used hyperbolic spaces instead of euclidean spaces as the proximity measurement for learning HIN embedding. RHINE [43] distinguishes relations into two categories (i.e., affiliation relations and interaction relations) and uses them to model different relations. Wang et al. [44] introduced the attention mechanism into heterogeneous network embedding. The importance between an object and its meta-path based neighborhood and the importance of different meta-paths are learned by object-level attention and semantic-level attention, respectively.

5.2 Traditional Recommendation
Traditional recommender systems typically rely on collaborative filtering methods, especially matrix factorization [1], to harness historical user-item interactions for the recommendation. MF models user preferences and item properties by factorizing the user-item interaction matrix into two low-dimensional latent matrices. MF has been successfully deployed in the industry (e.g., Amazon and eBay [1]), due to its effectiveness when handling large-scale data [45]. The cold-start problem, where historical data is not available for new users or items, is one of the most challenging issues in RS. In order to alleviate the cold-start problem, another line of work is to incorporate additional context information, which is also called auxiliary data or side information (e.g., social network [46], [47], review text [48], [49], user grouping data [50], [51], relationships in a graph [52], location [53], image [54] and time-series information [55]), into recommendation models [1].

Recently, the advance of deep learning has fostered the development of RS. Multi-layer perceptron (MLP) [25], Graph Neural Networks (GNN) [29], Variational Autoencoder [30] and other deep neural architectures have been introduced to model the recommendation task and show promising results. Readers can refer to Zhang et al. [56] for a detailed survey.

However, traditional RS only consider homogeneous information and cannot fully model the heterogeneous information which can be widely observed in real life, i.e., complex objects with different types and rich interactions.

5.3 Recommendations Over HIN
Recent decades have witnessed a massive increase of auxiliary data in RS. However, it is difficult to manage and utilize this heterogeneous and complex information using traditional methods for network analysis. HIN provides a flexible way to model the data heterogeneity and characterize the useful structural and semantic information contained in auxiliary data for the recommendation task [7].

Feng and Wang [57] proposed OptRank to utilize heterogeneous information and alleviate the cold-start problem in social tagging systems. Yu et al. [11] measured the similarity of all item pairs along one meta-path and the different preferences on different meta-path semantics were distinguished by linear regression. Then, an unified MF model was used to take advantages of both rating data and meta-path semantics for the recommendation task. Yu et al. [13] introduced the diffusion of user preferences based on similarity matrices defined by different meta-paths and user/item latent features are learned using nonnegative MF on the diffused matrix. With the latent features, they defined a recommendation model and optimized it with Bayesian Ranking. Subsequently, they improved the model by considering personalization [58]. Luo et al. [14] adopted PathSim [8] to measure the relations between users, items and user-item pairs. Finally, they used a unified MF model to incorporate heterogeneous information into social recommendation task. Shi et al. [59] designed SemRec which considers attribute values of links in HIN and personalized weights of different meta-paths for each user in RS. Pham et al. [60] proposed HeteRS, which represents HIN as multiple transition matrices, each of which corresponds to a relation from one to another type of objects. HeteRS transforms the recommendation problem into node proximity calculation problems w.r.t. some query nodes, and then uses the multivariate Markov chain to solve it. Fang et al. [61] harnessed a combination of Bayesian Personalized Ranking model and meta-path based representation learning for music recommendation. FMG [3] adopts MF to learn user/item latent features from user-item similarity matrix, which is obtained from meta-graph. Then FMG feeds the latent features into factorization machine (FM) [62] with Group lasso for the recommendation task.

Previous HIN-based recommendation models mainly rely on meta-path or meta-graph based similarity which suffers from the high computational complexity [16] and may not fully reflect the latent features of users and items in HIN [7]. Recently, there are some attempts to utilize the heterogeneous network embedding in order to mine user and item features in HIN based RS efficiently. Yu et al. [4] proposed to learn user embeddings through meta-path and then compute cosine similarity over user embeddings to identify implicit friends (i.e., users with high similarity) in social RS. NeuACF [31] extracts aspect-level similarity matrices of users and items through different meta-paths and then feeds them into deep neural network with attention to learn aspect-level latent factors in RS. Jiang et al. [63] learned representations for publications from HIN which is constructed from multilingual repositories and then offered recommendations for cross-language citation. Shi et al. [7] proposed HERec, which adopts random walks to generate object sequences and learn the embeddings of objects. Then, the object embeddings are transformed and integrated into an extended MF approach. Chen et al. [5] took advantage of meta-path, neural network, hierarchical attention mechanism and FM to predict ratings in HIN-based recommender. Hu et al. [6] first learned the embeddings of meta-path in addition to users and items in HIN-based RS, and considered the interplay between the meta-path and the involved user-item pair in an interaction. They also proposed a co-attention mechanism to mutually improve the representations for meta-path based context, users and items.

SECTION 6Conclusion
In this paper, we propose a multi-task learning framework, called MTRec, for recommendation over HIN. MTRec mostly relies on self-attention, to capture the complex semantics in meta-paths. It jointly optimizes tasks of both recommendation and link prediction, and the balance is automatically achieved via using a Bayesian task weight learner. What is more, MTRec provides good interpretabilities of recommendation through a “translation” mechanism which is used to model the three-way interactions among users, items and the meta-paths connecting them.

However, MTRec does not achieve a very high precision in some cases (e.g., around 0.2 for Prec@10 in Yelp), though it already exceeds existing methods. This observation indicates that HIN-based recommendation problem is far from being addressed completely. Moreover, MTRec does not explicitly leverage users’ new coming interactions and perform incremental learning (i.e., data sample-level learning). The bidirectional information flow between HIN and RS in current MTRec is implicit, i.e., the parameters of two components are updated according to the information flow between them (i.e., feature-level learning). In the future, we plan to study if the precision of MTRec can be further improved or not, by considering data sample-level information flow.